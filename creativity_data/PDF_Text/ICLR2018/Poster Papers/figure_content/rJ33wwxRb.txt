Figure 1: Classifying MNIST images with over-parameterized networks. The setting of Section 5is implemented (e.g., SGD with batch of size 1, only first layer is trained, Leaky ReLU activations)and SGD is initialized according to the initialization defined in Eq. 6. The linearly separable data setconsists of 4000 MNIST images with digits 3 and 5, each of dimension 784. The size of the trainingset is 3000 and the remaining 1000 points form the test set. Three experiments are performed whichdiffer only in the number of hidden neurons, 10, 100 and 1000. In the latter two, the networks areover-parameterized. For each number of hidden neurons, 40 different runs of SGD are performedand their results are averaged. (a) shows that in all experiments SGD converges to a global minimum.
Figure 2: Classifying MNIST images with over-parameterized networks and training both layers.
Figure 3: Classifying MNIST images with over-parameterized networks, training both layers andchoosing an appropriate learning rate. The setting of Figure 2 is implemented, but here a differentlearning rate is chosen for each network size, in order to satisfy the conditions of the proof in Section9.1.2. Figures (a) and (b) are train and test errors of MNIST classification for different network sizesand the chosen learning rates. In this setting, SGD exhibits similar training and generalizationperformance as in Figure 2. Figure (c) shows the minimal and maximal value of the second layerweights divided by their initial value (denoted as c, C respectively in Section 9.1.2). It can beseen that these values remain above zero, which implies that the weights do not flip signs duringthe training process (namely they satisfy the sign condition in Section 9.1.2) and that they behavesimilarly for different network sizes.
