Figure 1: Structure of a hierarchical sub-policy agent. θ represents the master policy, which selectsa sub-policy to be active. In the diagram, φ3 is the active sub-policy, and actions are taken accordingto its output.
Figure 2: Unrolled structure for a master policy action lasting N = 3 timesteps. Left: Whentraining the master policy, the update only depends on the master policy’s action and total reward(blue region), treating the individual actions and rewards as part of the environment transition (redregion). Right: When training sub-policies, the update considers the master policy’s action as partof the observation (blue region), ignoring actions in other timesteps (red region)4Published as a conference paper at ICLR 20185	RationaleWe will now provide intuition for why this framework leads to a set of sub-policies φ which allowagents to quickly reach high reward when learning θ on a new task. In metalearning methods, it iscommon to optimize for reward over an entire inner loop (in the case of MLSH, training θ for Titerations). However, we instead choose to optimize φ towards maximizing reward within a singleepisode. Our argument relies on the assumption that the warmup period of θ will learn an optimalmaster policy, given a set of fixed sub-polices φ. As such, the optimal φ at θfinal is equivalent to theoptimal φ for training θ from scratch. While this assumption is at some times false, such as when agradient update overshoots the optimal θ policy, we empirically find the assumption accurate enoughfor training purposes.
Figure 3: Sampled tasks from 2D moving bandits. Small green dot represents the agent, while blueand yellow dots represent potential goal points. Right: Blue/red arrows correspond to movementswhen taking sub-policies 1 and 2 respectively.
Figure 4: Learning curves for 2D Moving Bandits and Four Rooms.
Figure 5: Top: Ant Twowalk. Ant must maneuver towards red goal point, either towards the top ortowards the right. Bottom Left: Walking. Humanoid must move horizontally while maintaining anupright stance. Bottom Right: Crawling. Humanoid must move horizontally while a height-limitingobstacle is present.
Figure 6: Top: Distribution of mazes. Red blocks are impassable tiles, and green blocks representthe goal. Bottom: Sub-policies learned from mazes to move up, right, and down.
Figure 7: Learning curves for Twowalk and Walk/Crawl tasks0Figure 8: Ant Obstacle course task. Agent must navigate to the green square in the top right corner.
Figure 8: Ant Obstacle course task. Agent must navigate to the green square in the top right corner.
