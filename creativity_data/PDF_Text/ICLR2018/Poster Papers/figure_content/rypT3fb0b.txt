Figure 1: A DNN is first trained with GrOWL regularization to simultaneously identify the sparsebut significant connectivities and the correlated cluster information of the selected features. We thenretrain the neural network only in terms of the selected connectivities while enforcing parametersharing within each cluster.
Figure 2: GrOWLâ€™s regularization effect on DNNs. Fully connected layers (Left): for layer l,GrOWL clusters the input features from the previous layer, l - 1, into different groups, e.g., blueand green. Within each neuron of layer l , the weights associated with the input features from thesame cluster (input arrows marked with the same color) share the same parameter value. The neuronsin layer l - 1 corresponding to zero-valued rows of Wl have zero input to layer l , hence get removedautomatically. Convolutional layers (right): each group (row) is predefined as the filters associatedwith the same input channel; parameter sharing is enforced among the filters within each neuron thatcorresponds with the same cluster (marked as blue with different effects) of input channels.
Figure 3: Regularization effect ofGrOWL for different p values (Eq. (9)).
Figure 4: MNIST: comparison of the data correlation and the pairwise similarity maps (Eq (10)) ofthe parameter rows obtained by training the neural network with GrOWL, GrOWL+'2, group-Lasso,group-Lasso+'2 and weight decay ('2).
Figure 5: MNIST: sparsity pattern of the trained fully connected layer, for 5 training runs, usinggroup-Lasso, GrOWL, group-Lasso+'2, GrOWL+'2.
Figure 6: Output channel cosine similarity histogram obtained with different regularizers. Labels:GO:GrOWL, GOL:GrOWL+'2, GL:group-Lasso, GLL:group-Lasso+'2, WD:Weight decay.
