Figure 1: We do inference on the learned comPressed rePresentation (middle), without decoding.
Figure 2: We perform infer-ence of some variable y fromthe compressed representa-tion Z instead of the decodedRGB X. The grey blocks de-note encoders/decoders of alearned compression networkand the white block an infer-ence network.
Figure 3: Top-5 accuracy on the validation set for different architectures and input types at eachoperating point. Results are shown for ResNet-50 (where reconstructed/decoded RGB images areused as input) and for cResNet-51 and cResNet-39 (where compressed representations are used asinput).
Figure 4: mIoU performance on the validation set for different architectures and input types at eachoperating point. Results shown for ResNet-50-d (where reconstructed/decoded RGB images areused as input), and for cResNet-51-d and cResNet-39-d (where compressed representations are usedas input).
Figure 5: Top: Reconstructed/decoded RGB images at different compression operating points. Mid-dle: Predicted segmentation mask starting from reconstructed/decoded RGB images using ResNet-50-d architecture. Bottom: Predicted segmentation mask starting from compressed representationusing cResNet-51-d architecture. Left: Original RGB image and the ground truth segmentationmask.
Figure 6: Inference performance at the 0.0983bpp operating point at different computationalcomplexities, for both compressed representa-tions and RGB images. We report the compu-tational cost of the inference networks only andfor reconstructed RGB images we also show theinference cost along with the decoding cost. Forruntime benchmarks see Appendix A.94FLOPS [∙109]Classification〔琛〕?EJnOOE S dojFigure 7: Showing how classification and seg-mentation performance improves by finetuning(ft.) the compression network only and thecompression network and the classification net-work jointly. The dots show how the perfor-mance “moves up” from the baseline perfor-mance when finetuning. The baseline is ob-tained using fixed compression operating points.
Figure 7: Showing how classification and seg-mentation performance improves by finetuning(ft.) the compression network only and thecompression network and the classification net-work jointly. The dots show how the perfor-mance “moves up” from the baseline perfor-mance when finetuning. The baseline is ob-tained using fixed compression operating points.
Figure 8: MS-SSIM, SSIM and PSNR as a function of rate in bpp. Shown for JPEG 2000, JPEGand the reported Deep Compression operating points. Higher is better.
Figure 9: For each operating point we show the reconstructed/decoded image along with the 4 high-est entropy channels of the compressed representation. The original RGB image is shown on theleft for comparison. The channels of the compressed representation look like quantized downscaledversions of the original image, which motivates doing inference based on them instead of the recon-structed RGB images.
Figure 10: Top: Reconstructed/decoded RGB images at different compression operating points.
Figure 11:	Showing how the selected metrics move from the original compression operating pointto a different point after finetuning. We show this change when finetuning the compression networkonly, and then when finetuning the compression network and the classification architecture jointly.
Figure 12:	Inference performance at the 0.0983 bpp operating point for different architectures, forboth compressed representations and reconstructed RGB images. We report the computational run-time (per image) of the inference networks only and for the reconstructed RGB images we also showthe runtime for the inference network along with the decoding runtime.
