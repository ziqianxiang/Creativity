Figure 1: Notation overview. For an L-layer feed-forward network we denote the explicit layer-wiseactivation variables as zl and al. The transfer functions are denoted as φ and σ. Layer l is of size nl .
Figure 2: Exact and inexact solvers for ProxProp compared with BackProp. Left: A more precisesolution of the proximal subproblem leads to overall faster convergence, while even a very inexactsolution (only 3 CG iterations) already outperforms classical backpropagation. Center & Right:While the run time is comparable between the methods, the proposed ProxProp updates have bettergeneralization performance (≈ 54% for BackProp and ≈ 56% for ours on the test set).
Figure 3: ProxProp as a first-order oracle in combination with the Adam optimizer. The proposedmethod leads to faster decrease of the full batch loss in epochs and to an overall higher accuracy onthe validation set. The plots on the right hand side show data for a fixed runtime, which correspondsto a varying number of epochs for the different optimizers.
