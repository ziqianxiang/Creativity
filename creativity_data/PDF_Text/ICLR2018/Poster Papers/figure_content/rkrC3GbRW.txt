Figure 1: The recurrent model used to approx-imate the Q-function. A hypothetical logisticfunction activation is shown for each char-acter in C. Here the set of characters is theSMILES alphabet and we use the first 3 char-acters of the molecule in figure 3 as the inputexample. The initial character is predictedfrom the first hidden state, and the LSTMcontinues until the end of the sequence.
Figure 2: Experiments with length 25 Python expressions. (Left) Area under validity-entropy curveas training progresses, 10-90 percentiles shaded. Active learning converges faster and reaches ahigher maximum. (Right) Entropy versus validity for median active and median passive model after200k training sequences. Both models have learnt a high entropy distribution over valid sequences.
Figure 3: Predictions y(xtâˆ£x<t, W) of the agent at each step t for the valid test molecule shown inthe top left figure, for a subset of possible actions (selecting as next character C, F, ) ,or ]). Eachcolumn shows which actions the trained agent believes are valid at each t, given the characters x<tpreceding it. We see that the validity model has learned basic valence constraints: for example theoxygen atom O at position 10 can form at most 2 bonds, and since it is preceded by a double bond,the model knows that neither carbon C nor fluorine F can immediately follow it at position 11; Wesee the same after the bromine Br at position 18, which can only form a single bond. The modelalso correctly identifies that closing branch symbols ) cannot immediately follow opening branches(after positions 6, 8, and 17), as well as that closing brackets ] cannot occur until an open bracket hasbeen followed by at least one atom (at positions 32-35). The full output heatmap for this examplemolecule is shown in Figure 4 in the appendix.
Figure 4: Full heatmap showing predictions y(xt|x<t, w) for the molecule in Figure 3.
