Figure 1: Layer Removal Policy removes layers of Teacher network architecture (stage-1 candidates)then Layer Shrinkage Policy reduces parameters (stage-2 candidates).
Figure 2: a) Layer removal policy network, b) Layer shrinkage policy network2014) represents the state s âˆˆ S (the initial state) and by removing one convolutional filter from thefirst layer we obtain a new network architecture s0 .
Figure 3: Student learning on MNIST. Reward, Accuracy, Compression vs Iteration (Top: Stage 1,Bottom: Stage 2)(b) VGG-13MNIST To evaluate the compression performance we use (1) a Conv4 network consisting of 4convolutional layers and (2) a high capacity VGG-13 network.
Figure 4: Student learning on CIFAR-10. Reward, Accuracy, Compression vs Iteration (Top: Stage1, Bottom: Stage 2)CIFAR-10 On the CIFAR-10 dataset we ran experiments using the following teacher networks:(1) VGG-19, (2) ResNet-18 and (3) ResNet-34 networks. The experimental results are shown inFigure 4. It is interesting to note that on CIFAR-10, our learned student networks perform almost aswell or better the teacher networks despite a 10x compression rate.
Figure 5: Student learning on CIFAR-100.
Figure 6: MNIST Left: Actor-critic Right: REINFORCE, averaged over 3 runsFigure 7: CIFAR-10 Left: Actor-critic Right: REINFORCE, averaged over 3 runsFor the MNIST dataset, our results show that there is a slight improvement in stability, although theyboth converge at a similar rate.
Figure 7: CIFAR-10 Left: Actor-critic Right: REINFORCE, averaged over 3 runsFor the MNIST dataset, our results show that there is a slight improvement in stability, although theyboth converge at a similar rate.
Figure 8: Average reward over 3 runs for various learning rates on the MNIST dataset7.2	Batch sizeSimilarly we performed a grid search to determine the optimal batch size over 1, 5, 10. A batch sizeof 1 was too unstable while a batch size of 10 offered no substantial improvements to justify theadditional computation. Thus we observed that a batch size of 5 worked the best.
Figure 9: Average reward over 3 runs for batch sizes Left: 1, Middle: 5, Right: 10 on the MNISTdataset8	Transfer learning experimentsBelow are the results of the transfer learning experiments, as observed, the pretrained policies startoff with a high reward unlike the policies trained from scratch.
Figure 10: Transfer learning experiments9 Additional experimentsThe following section contains results about additional compression experiments that were con-ducted.
Figure 11: ResNet-18 experiments on SVHN, (Left: Stage 1, Right: Stage 2)Figure 12: ResNet-18 experiments on Caltech, (Left: Stage 1, Right: Stage 2)10 Implementation detailsThe following section contains the implementation details required to replicate the experiments. Allof the experiments were implemented in PyTorch with 1 NVIDIA TitanX GPU.
Figure 12: ResNet-18 experiments on Caltech, (Left: Stage 1, Right: Stage 2)10 Implementation detailsThe following section contains the implementation details required to replicate the experiments. Allof the experiments were implemented in PyTorch with 1 NVIDIA TitanX GPU.
Figure 13: Stage1 ResNet-34 experiments on ImageNet32x3210.1	PoliciesRemoval policy The removal policy was implemented with 2 hidden layers and 30 hidden units andtrained with the Adam optimizer and a learning rate of 0.003. The shrinkage policy was implementedwith 2 hidden layers and 50 hidden units and trained with the Adam optimizer and with a learningrate of 0.1. These policies were each trained for at least 100 epochs for each experiment. Batch sizeof 5 rollouts was used.
Figure 14: Reward manifold of naive reward vs. our rewarda naive reward function is symmetric while our reward function returns a lower reward for lowaccuracy, high compression models compared to high accuracy, low compression models. Bothfunctions are monotonically increasing.
