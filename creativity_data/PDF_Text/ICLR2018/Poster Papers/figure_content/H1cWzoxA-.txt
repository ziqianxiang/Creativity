Figure 1: A comparison of Bi-BloSAN andother RNN/CNN/SAN in terms of train-ing time, training memory consumptionand test accuracy on SNLI (Bowman et al.,2015). The details of all the models are pro-vided in Section 4.
Figure 2: Masked self-attention mechanism. fij de-notes f (xi , xj ) in Eq.(9).
Figure 3: Masked block self-attention (mBloSA) mechanism.
Figure 4: Bi-directional block self-attention network (Bi-BloSAN) for sequence encoding.
Figure 6: (a) Inference time cost and (b) GPU memory consumption of the sequence encoding models vs. thesequence length with the batch size of 64 and the features number of 300.
Figure 5: Validation accuracy vs. training time (second) of Bi-LSTM, CNN, multi-head attention, DiSAN andBi-BloSAN for 800 training steps on CR dataset. (The Bi-LSTM for 800 steps consumes 279s in total.)(m≡)uoaulnsuoo XJOE8≡ΘOU9J8JU-Sequence Length(b)both by a large margin on prediction quality in previous experiments. Moreover, Bi-BloSAN ismuch faster than the RNN models (Bi-LSTM, Bi-GRU, BI-SRU). In addition, although DiSANrequires less training time than the RNN models in the experiments above, it is much slower duringthe inference phase because the large memory allocation consumes a great amount of time. Bycontrast, the block structure of Bi-BloSAN significantly reduces the inference time.
Figure 7: The structure of a neural network for machine comprehension. The candidates of the context fusionlayer include Bi-LSTM, Bi-GRU, Bi-SRU, multi-CNN, multi-head attention and Bi-BloSA. Unlike the originalmulti-CNN for sentence embedding, we use padding and remove the max-pooling along the time axis to obtainan output of the same length as input. DiSA is not considered due to memory limitation.
