Figure 1: The schematic of Skim-RNN on a sample sentence from Stanford Sentiment Treebank: “intelligentand invigorating film”. At time step 1, Skim-RNN makes the decision to read or skim x1 by using Equation 1on h0 and x1. Since ‘intelligent’ is an important word for sentiment, it decides to read (blue diamond) byobtaining a full-size hidden state with the big RNN and updating the entire previous hidden state. At time step 2,Skim-RNN decides to skim (empty diamond) the word ‘and’ by updating the first few dimensions of the hiddenstate using small RNN.
Figure 2: Analyzing the effect of small hidden state size, d’ (left) and γ (right) on skim rate; (d = 100, d0 = 10,and γ = 0.02 are default values).
Figure 3: Skim rate of LSTMs in LSTM+Att model.
Figure 5: Trade-off between F1 score and Flop-Robtained by adjusting the threshold for the skim (orskip) decision. Blue line is a skimming model withd0 = 10, and red line is a skipping model (d0 = 0).
Figure 4: F1 score of standard LSTM with varyingconfigurations (Blue) and Skim LSTM with varyingconfigurations (Red), both sorted together in ascendingorder by the inverse of Flop-R (Orange). d = 100 bydefault. Numbers inside B refer to d, and numbersinside S refer to d0 , γ.
Figure 6: Reading (red) and skimming (white) decisions in four LSTM layers (two for forward and two forbackward) of Skim-LSTM+Attention model. We see that the second layer skims more, implying that the secondlayer is more confident about which tokens are important.
Figure 7: Speed up rate of Skim-LSTM (vs LSTM) with varying skimming rates and hidden state sizes.
Figure 8: F1 score and skim rate when using differ-ent pretraining schemas. Models with half-pretrainedmodel (Yellow) outperforms models with no pretrainedmodel (Blue) or fully pretrained model (Green), bothin F1 score and skim rate.
Figure 9: Skim rate in different layers of LSTM, whenusing different pretrained models. All models have hid-den size of 100 and small hidden size of 20. Numberinside parenthesis indicates γ. Models with no pre-trained model (Red) have unstable skim rates.
Figure 10: Reading (red) and skimming (white) on SQuAD, with LSTM+Attention model. The top two areskimming models with different values of γ. The bottom one is skimming models with different values of skimdecision threshold (whose default is 0.5). An increase in γ and a decrease in threshold lead to more skimming.
