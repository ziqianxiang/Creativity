Figure 1: Generating adversarial examples with spatial transformation: the blue point denotes thecoordinate of a pixel in an output adversarial image and the green point is its corresponding pixel inan input image. The flow field in red represents the displacement from the pixels in the adversarialimage to the pixels in the input image.
Figure 2: Adversarial examples generated by stAdv against different models on MNIST. The groundtruth images are shown in the diagonal and the rest are adversarial examples that are misclassifiedto the target classes shown on the top.
Figure 3: Adversarial examples generated by stAdv against different models on CIFAR-10. Theground truth images are shown in the diagonal while the adversarial examples on each column areclassified into the same class as the ground truth image within that column.
Figure 4: Comparison of adversarial examples generated by FGSM, C&W and stAdv. (Left:MNIST, right: CIFAR-10) The target class for MNIST is “0” and “air plane” for CIFAR-10. Wegenerate adversarial examples by FGSM and C&W with perturbation bounded in terms of L∞ as0.3 on MNIST and 8 on CIFAR-10.
Figure 5: Flow visualization on MNIST. A digit “0” is misclassified as “2”.
Figure 6: Flow visualization on CIFAR-10. An “airplane” image is misclassified as “bird”.
Figure 7: Flow visualization on ImageNet. (a): the original image, (b)-(c): images are misclassifiedinto goldfish, dog and cat, respectively. Note that to display the flows more clearly, we fade out thecolor of the original image.
Figure 8: CAM attention visualizations for ImageNet inception_v3 model. (a) the original image and(b)-(d) stAdv adversarial examples targeting different classes. The second row shows the attentionvisualizations for the corresponding images displayed above.
Figure 9: CAM attention visualizations for ImageNet inception_v3 model. The first column showsthe CAM maps corresponding to the original images. Column 2-4 show the adversarial examplesgenerated by different methods. The visualizations are drawn for Row 1 (inception_v3 model) andRow 2 (adversarial trained inception_v3 model). (a) and (e)-(g) are labeled as the ground truth“cinema”, while (b)-(d) and (h) are labeled as the adversarial target “missile.”We also analyze the attack success rate of these examples under existing defense methods anddemonstrate they are harder to defend against, which opens new directions for developing morerobust defense algorithms. Finally, we visualize the attention regions of DNNs on our adversarialexamples to better understand this new attack.
Figure 10: Examples from an ImageNet-compatible set. Left: original image; right: adversarialimage generated by stAdv against inception_v3.
Figure 11: Adversarial examples generated by stAdv against Model B on MNIST. The originalimages are shown in the diagonal; the rest are adversarial examples that are classified into the sameclass as the original image within that column.
Figure 12: Adversarial examples generated by stAdv against a ResNet-32 on CIFAR-10. The orig-inal images are shown in the diagonal; the rest are adversarial examples that are classified into thesame class as the original image within that column.
