Figure 1: Visual pattern recognition with sparse networks during training. Sample trainingimages (top), test classification accuracy after training for various connectivity levels (middle) andexample test accuracy evolution during training (bottom) for a standard feed forward network trainedon MNIST (A) and a CNN trained on CIFAR-10 (B). Accuracies are shown for various algorithms.
Figure 2: Rewiring in recurrent neural networks. Network performance for one example run(A) and at various connectivity levels (B) as in Fig. 1 for an LSTM network trained on the TIMITdataset with DEEP R (green), soft-DEEP R (red) and a network with fixed random connectivity(blue). Dotted line: fully connected LSTM trained without regularization as reported in Greff et al.
Figure 3: Efficient network solutions under strict sparsity constraints. Accuracy and connectiv-ity obtained by DEEP R and soft-DEEP R in comparison to those achieved by pruning (Han et al.,2015b) and `1 -shrinkage (Tibshirani, 1996; Collins & Kohli, 2014). A, B) Accuracy against theconnectivity for MNIST (A) and CIFAR-10 (B). For each algorithm, one network with a decentcompromise between accuracy and sparsity is chosen (small gray boxes) and its connectivity acrosstraining iterations is shown below. C) Performance on the TIMIT dataset. D) Phoneme error ratesand connectivities across iteration number for representative training sessions.
Figure 4: Transfer learning with DEEP R. The target labels of the MNIST data set were shuffledafter every epoch. A) Network accuracy vs. training epoch. The increase of network performanceacross tasks (epochs) indicates a transfer of knowledge between tasks. B) Correlation betweenweight matrices of subsequent epochs for each network layer. C) Correlation between neural activityvectors of subsequent epochs for each network layer. The transfer is most visible in the first hiddenlayer, since weights and outputs of this layer are correlated across tasks. Shaded areas in B) andC) represent standard deviation across 5 random seeds, influencing network initialization, noisyparameter updates, and shuffling of the outputs.
Figure 5: Hyper-parameter search for the pruning algorithm according to Han et al. (2015b).
Figure 6: Rewiring behavior of DEEP R. A) Network performance versus training iteration (sameas green line in Fig. 1A bottom, but for a network constrained to 1% connectivity). B) Absolutenumber of newly activated connections Kn(el)w to layer l = 1 (brown), l = 2 (orange), and the outputlayer (l = 3, gray) per iteration. Note that these layers have quite different numbers of potentialconnections K(l) . C) Same as panel B but the number of newly activated connections are shownrelative to the number of potential connections in the layer (values in panel C are smoothed with aboxcar filter over X iterations).
