Figure 1: 2D Environment a) Let the agent (blue square) operate in a 2D environment. The goalregion is represented by the red square and the orange square represents the agents observation b)Agents observation. The gray area is not observable. c) It is possible to plan on this locally observedspace since it is a MDP.
Figure 2: Environment with local minima. The agents observation when entering the tunnel toexplore it and when backtracking after seeing the dead end are the same. Using a reactive policy forsuch environments leads to the agent getting stuck near the dead end .
Figure 3: MACN Architecture. The architecture proposed uses convolutional layers to extractfeatures from the environment. The value maps are generated with these features. The controllernetwork uses the value maps and low level features to emit read and write heads in addition to doingits own planning computation.
Figure 4: Performance on grid world environment . In the map the blue square represents the startposition while the red square represents the goal. The red dotted line is the ground truth and the bluedash line represents the agents path. The maps shown here are from the test set and have not beenseen before by the agent during training.
Figure 5: Grid world environment with local minima. Left: In the full map the blue squarerepresents the current position while the red square represents the goal. Center-left: The partial maprepresents a stitched together version of all states explored by the agent. Since the agent does notknow if the tunnel culminates in a dead end, it must explore it all the way to the end. Center-right:The sensor input is the information available to the agent. Right: The full map that we test our agenton after being trained on smaller maps. The dimensions of the map as well as the tunnel are larger.
Figure 6: T-sne visualization on 2D grid worlds with tunnels. a) T-sne visualization of the rawimages fed into the network. Most of the input images for going into the tunnel and exiting thetunnel are the same but have different action labels. b) T-sne visualization from the outputs of the preplanning module. While it has done some separation, it is still not completely separable. c) Finaloutput from the MACN. The actions are now clearly separable.
Figure 7: 9 Node GraphSearch. Blue is start and Redis goal.
Figure 8: Navigation in a 3D environment on a continuous control robot. a) The robot is spawnedin a 3d simulated environment. b) Only a small portion of the entire map is visible at any given pointto the robot c) The green line denotes ground truth and red line indicates the output of MACN.
Figure 9: Performance on simulated environment. a) We report a plot of the number of steps leftto the goal as the agent executes the learned policy in the environment (Lower is better). In this plot,the agent always starts at a position 40 steps away from the goal. b) The biggest advantage of MACNover other architectures is its ability to scale. We observe that as the distance to the goal increases,MACN still beats other baselines at computing a trajectory to the goal.(Higher success % is better)Memory Size(SdSS Jo J(υqEnN) A=njssə:DnS p(υLPB ①--BOg ISwteLL.
Figure 10: Effect of Memory in Robot World MACN scales well to larger environments in therobot world when memory is increased suitably.
Figure 11: Testing Error on grid world (32 x 32)16Published as a conference paper at ICLR 2018B	Environments with TunnelsWe setup the network the same way as we did for the grid world experiments with blob shapedobstacles. Due to the relatively simple structure of the environment, we observe that we do not reallyneed to train our networks with curriculum. Additionally, the read and write heads are both set to 1for this experiment.
Figure 12: Learning optimal policy with Deep Q networks. In this case the input to the DQNis the sensor input. State after 1 million iterations. The agent gets stuck along the wall (left wallbetween 40 and 50)B.3	Visualizing the Memory StateFor the tunnel task, we use an external memory with 32 rows (N = 32) and a word size of 8 (W = 8).
Figure 13: Shift in memory states just before and after the end of the tunnel is observed. Once theagent has turned around, the memory state stays constant till it reaches the end of the tunnel.
Figure 14: a) A random connected undirected graph with a the goal given by the star-shaped node,and the current state given by the blue node b) Shows the corresponding adjacency matrix wherewhite indicates a connection and black indicates no connection. The goal is given by the row shadedin green, and the current state is given by the row shaded in blue.
Figure 15: Simulated ground robotHuman Friendly Navigation (HFN) (Guzzi et al., 2013) which generates reactive collision avoidancepaths to the goal. Given a start and goal position, the HFN algorithm generates way points that aresent to the controller. For our experiment, we generate a tuple of (x, y, θ) associated with everyobservation. To train the network, a m × n matrix (environment matrix) corresponding to the m × nenvironment is initialized. A corresponding reward array (reward matrix) also of size m × n with a 1at the goal position and zero elsewhere is concatenated with the environment matrix. The observationscorresponding to the laser scan are converted to a j × k matrix (observation matrix) where j < mand k < n. The values at the indices in the environment array corresponding to the local observationare updated with the values from the observation matrix. At every iteration, the environment matrixis reset to zero to ensure that the MACN only has access to the partially observable environment.
