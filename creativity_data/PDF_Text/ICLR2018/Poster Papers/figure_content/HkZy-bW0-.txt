Figure 1: An example signal xt (blue), encoded with kp varying across rows and kd varying acrosscolumns. st (black) is the quantized signal produced by the successive application of encoding(Equation 4) and quantization (Equation 3, where N indicates the total number of spikes. Xt (orange)is the reconstruction of xt produced by applying Equation 5 to st . One might, after a careful lookat this figure, ask why we bother with the proportional (kp) term at all? Figure 2 anticipates thisquestion and answers it visually.
Figure 2: The problem with only sending changes in activation (i.e. kp = 0) is that during training,weights change over time. Top: we generate random signals for a single scalar activation xt and scalarweight wt . Row 2: We efficiently approximate zt by taking the temporal difference, multiplying bywt then temporally integrating, to produce Zt, as described in Section 2.4. As the weight Wt changesover time, our estimate Z diverges from the correct value. Rows 3, 4: Introducing kp allows US tobring our reconstruction back in line with the correct signal.
Figure 3: A visualization of our efficient update schemes from Section 2.6. Top: A scalar signalrepresenting a PresynaPtic neuron activation Xt = hι-ι(zι - 1), its quantized version, Xt = (Q ◦enc)(xt), and its reconstruction Xt = dec(Xt). Middle: Another signal, representing the postsynapticgradient of the error e = ∂∂ZL, along with its quantized (e) and reconstructed (^) variants. Bottom:∂ L	d∂ LThe true weight gradient ∂∂L and the reconstruction gradient ∂∂L. At the time of the spike in et, wehave two schemes for efficiently computing the weight gradient that will be used to increment weight(see Section 2.6). The PaSt scheme computes the area under X ∙ e since the last spike, and the futurescheme computes the total future additional area due to the current spike.
Figure 4: Left: Our STDP rule, when both the input and error spikes have the same sign. Middle:Our STDP rule, when the input and error spikes have opposite signs. Right: The classic STDP ruleMarkram et al. (2012), where the weight update is positive when a presynaptic spike preceeds apostsynaptic spike, and negative otherwise.
Figure 5: Top Row: Results on MNIST. Bottom Row: Re-sults on Temporal MNIST. Left Column: the training andtest scores as a function of epoch. Middle: We now put thenumber of computational operations on the x-axis. We seethat as a result our PDNN shifts to the left. Right: Becauseour network computes primarily with additions rather thanmultiplications. When we multiply our operation countswith the estimates of Horowitz (2014) for the computationalcosts of arithmethic operations (0.1pJ for 32-bit fixed-pointaddition vs 3.2pJ for multiplication), we can see that ouralgorithm would be at an advantage on any hardware wherearithmetic operations were the computational bottleneck.
Figure 6: Some sam-ples from the Temporal-MNIST dataset. Eachcolumn shows a snippetof adjacent frames.
Figure 7: Left: Learning Curves on the YoutubeDataset. Right: Learning curves with respectto computational energy using the conversion ofHorowitz (2014). The spiking network slightlyoutperforms the non-spiking baseline - we suspectthat this is because the added noise of spiking actsas a regularizer.
Figure 8: We simulate different frame-rates byselecting every n’th frame. This plot shows ournetwork’s mean computation over several snippetsof video, at varying frame rates. As our framerate increases, the computation per-frame of ourspiking network goes down, while with a normalnetwork, it remains fixed.
Figure 9: In Section 2.6 and 2.7, we described 4 different update rules (“Reconstruction”, “Past”,“Future”, and “STDP”), and stated that while they do not necessarily produce the same updates atthe same times, they produce the same result in the end. Here we demonstrate this empirically. Wegenerate two random spike-trains representing the presynaptic input and the postsynaptic error signalto a single synapse and observe how the weight changes according to the different update rules. Top:A randomly generated PresynaPtic quantized signal x, along with its reconstruction x. Middle: Arandomly generated postsynaptic quantized error signal e, along with its reconstruction e. Bottom:The cumulative weight update arising from our four updates methods. "recon" is just PT =1 Xτ^τ,“past” and “future” are described in Section 2.6 and “STDP” is described in Section 2.7. Note that bythe end all methods arrive at the same final-weight value.
Figure 10: Top Left: A time varying signal xt, the quantized signal Q(enc(xt)), and the time-varying“weight” wt. Bottom Left: Compare the true product of these signals xt∙wt with the dec(enc(xt )∙wj,which shows the effects of the non-stationary weight approximation, and dec(Q(enc(xt)) ∙ W) whichshows both approximations. Top Middle: The Cosine distance between the “true” signal x wand the approximation due to the nonstationary w, scanned over a grid of kp , kd values. Top Right:The cosine distance between the “true” signal and the approximation due to the quantization ofx. Bottom Middle: The Cosine Distance between the “true” signal and the full approximationdescribed in Equation 7. This shows why we need both kp and kd to be nonzero. Bottom Right: TheNumber of spikes in the encoded signal. In a neural network this would correspond to the number ofweight-lookups required to compute the next layer’s activation: dec(Q(enc(x)) W).
Figure 11:	16 Frames from the Youtube-BB dataset. Each video annotated as having one of 24 objectsin it. It also comes with annotated bounding-boxes, which we do not use in this study.
Figure 12:	The average relative change in layer activation between frames, as frame-rate increases.
