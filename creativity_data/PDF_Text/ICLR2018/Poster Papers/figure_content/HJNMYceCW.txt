Figure 1: Residual Loss Prediction: the system assigns a part-of-speech tag sequence to thesentence “International Conference for Learning Representations”. Each state represents a partial la-beling. The end state e = [Noun, Noun, Preposition, Verb, Noun]. The end state e is associated withan episodic loss `(e), which is the total hamming loss in comparison to the optimal output structuree* = [Adjective, Noun, Preposition, Noun, Noun]. We emphasize that our algorithm doesn't assumeaccess to neither the optimal output structure, nor the hamming loss for every time step. Only thetotal hamming loss is observed in this case (`(e) = 2).
Figure 2: Average loss during learning on the four RL problems. Shaded regions are empiricalquartiles over the experimental replicates with different random seeds.
Figure 3:	Average loss during learning for three bandit structured prediction problems. Also in-cluded are supervised learning results with DAgger.
Figure 4:	Empirical effect of additive vs non-additive loss functions. Performance is better whenthe loss is additive (blue) vs non-additive (green). The x-axis shows the number of episodes and they-axis measures the incremental loss using the true loss function (light colors) and using Reslope(dark colors). If Reslope worked perfectly, these would coincide.
Figure 5: An example for a search space defined by a Learning to Search (L2S) algorithm. A searchspace is defined in terms of the set of states X, and the set of actions A. The agent starts at the initialstate S, and queries the roll-in policy πin twice, next, at state R, the agent considers all three actionsas possible one-step deviations. The agent queries the roll-out policy πout to generate three differenttrajectories from the set of possible output structures Y .
Figure 6: Reinforcement Learning Tasks(c) Cart Pole(d) Grid WorldE Proof of Theorem 2Proof of Theorem 2. The proof follows a combination of the proof of Theorem 1 with the LOLSanalysis. Using the same notation as before, additionally let πnout be the mixture of πn with πref forrollout.
Figure 7: Example inputs for part of speech tagging and dependency parsing.
Figure 8: Average loss during learning on the four RL problems, including PPO with minibatching.
Figure 9:	Average loss (top) and heldout loss (bottom) during learning for three bandit structuredEnalish ParsinaChinese POSIO1 IO2 IO3 IO4	IO1 IO2 IO3 IO4	IO1 IO2 IO3 IO4 IO5Num ber of exa m pies	Numberofexamples	NUmber OfeXamPleSFigure 10:	The empirical effect of multiple deviations for different algorithms.
Figure 10:	The empirical effect of multiple deviations for different algorithms.
Figure 11: Empirical effect of additive Vs non-additive loss functions. Performance is better whenthe loss is additive (blue) vs non-additive (green). The x-axis shows the number of episodes and they-axis measures the incremental loss using the true loss function (light colors) and using Reslope(dark colors). If Reslope worked perfectly, these would coincide.
