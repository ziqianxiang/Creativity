Figure 1: A compositional abstraction hierarchy for faces, derived from 3 attributes: hair color, smiling or not,and gender. We show a set of sample images generated by our model, when trained on CelebA, for differentnodes in this hierarchy.
Figure 2: Illustration of the product of experts inference network. Each expert votes for a part of latent spaceimplied by its observed attribute. The final posterior is the intersection of these regions. When all attributes areobserved, the posterior will be a narrowly defined Gaussian, but when some attributes are missing, the posteriorwill be broader. Right: we illustrate how inclusion of the “universal expert” p(z) in the product ensures that theposterior is always well-conditioned (close to spherical), even when we are missing some attributes.
Figure 3: Samples from attribute vectors seen at training time, generated by the 3 different models. We plotthe posterior mean of each pixel, E [x|zs], where Zs 〜qφ (z|y). The caption at the top of each little imageis the predicted attribute values. The border of the generated image is red if any of the attributes are predictedincorrectly. (The observation classifier is fed sampled images, not the mean image that we are showing here.)orientation, and 100% for location. Consequently, it is a reliable way to assess the quality of samplesfrom various generative models (see Appendix A.5 for details). We then compute correctness andcoverage on the iid dataset, and coverage on the comp dataset.
Figure 4: (a) We show quantitaive results on the 3C’s on MNIST-A. (b) Qualitative results on MNIST-A forvarious queries. For refined/fully specified queries, we can see that both TELBO and JMVAE produce goodcorrectness, i.e., the images produced follow constraints placed by the specified attributes. When the attribute‘orientation’ is unspecified, we see that TELBO produces upright and counter clockwise digits, while JMVAEproduces clockwise and upright digits. Finally, when we leave the digit unspecified (top), we see that TELBOappears to generate a more diverse set of digits (9, 3, 8, 6) while JMVAE produces 0 and 3.
Figure 5: Sample CelebA results. Left: we show the attributes specified to be present or absent when generatingimages. Middle: we show 10 samples each generated from TELBO, JMVAE and BiVCCA. We see that TELBOand JMVAE genreate better samples than BiVCCA which collapses to the mean. Middle, bottom: We showfive samples from TELBO and JMVAE in response to queries with unspecified attributes, and see that bothapproaches generate a mix in the samples, generalizing meaningfully across unspecified attributes.
Figure 6: A qualitative illustration of some of the examples from concept naming models. Top-left: an exampleof a sample that is correctly named by a Concept-NB model. However, the Concept-NB model is not thatstrong and often gets simple concepts such as digits incorrect, making mistakes between 6 and 0, for example(bottom-left). This is likely because the only way in which the Concept-NB approach reasons about the set is notvia a "meaningful" low dimensional latent variable but via a sampling distribution on a high dimensional spaceof images. The Concept-Latent model is able to do better on the same set of images, and classify the set as theconcept “6”. Finally, we show a failure case of the model where it incorrectly classifies the digits as being large(there is a small digit in the set), and ignores the fact that all of the digits are in the top-left.
Figure 7: Example binary images from our MNIST-A dataset.
Figure 8: Visualization of the benefit of semantic annotations for learning a good latent space. Each small digitis a single sample generated from p(x|z) from the corresponding point z in latent space. (a) β-VAE fit to imageswithout annotations. The color of a point z is inferred from looking at the attributes of the training image thatmaps to this point of space using q(z|x). Note that the red region (corresponding to the concept of large and evendigits) is almost non existent. (b) Joint-VAE fit to images with annotations. The color of a point z is inferredfrom p(y|z).
Figure 9: Architecture for the q(z|x, y) network in our JVAE models for MNIST-A. Images are (64x64x1),class has 10 possible values, scale has 2 possible values, orientation has 3 possible values, and location has 4possible values.
Figure 10:	Archtectures for the single input inference networks for MNIST-A.
Figure 11:	Randomly sampled images from the TELBO model when fed randomly sampled concepts from theiid training set. We also show the outputs of the observation classifier for the images. Note that we visualizemean images above (since they tend to be more human interpretable) but the classifier is fed samples from themodel. Figure best viewed by zooming in.
Figure 12:	Compositional generalization on MNIST-A. Models are given the unseen compositional query shownat the top and each of the three columns shows the mean of the image distribution generated by the models.
Figure 13: Set of all 9 images labelled as bald=1 and male=0 in the CelebA dataset. We can see that in allthe cases the labels are inaccurate for the image, probably due to annotator error.
Figure 14: TELBO creates more diverse images than JMVAE. At the top we show the set of attributes whichare present and absent in the input query. Below, we show the results of generation with all the attributesspecified, drawing 10 samples each. We see that both TELBO and JMVAE create accurate images satisfying theconstraints. Note that the concept “male” is set to “absent” in the query, which in CelebA means that “female” ispresent. Next, we unspecify whether the image should contain a male or a female. We see that in this setting,TELBO has a better mixing of male and female images (fourth, sixth, eighth and ninth images in the third roware male), than JMVAE which just produces a single male image (the ninth image in the fourth row).
