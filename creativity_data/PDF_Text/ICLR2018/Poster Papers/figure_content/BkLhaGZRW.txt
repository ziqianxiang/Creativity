Figure 1: Capacity usage of the rectifier discriminator D in different scenarios. D (a rectifier net)cuts the input space into different linear regions, since rectifier nets compute piece-wise linear func-tions. left: D uniformly spreads its capacity in the input space, but does not have enough capacity todistinguish all subtle variations within a data distribution. middle: D uses its capacity in the regionwith no data; while real and fake data are correctly separated, variations within real data distribu-tion are not represented by D , so cannot possibly be communicated to G if this degeneracy persiststhrough training; meanwhile all fake points in the same linear region passes the same gradient infor-mation to G, even if they are visually distinct. right: D spends most capacity on real and fake data,but also in regions where G might move its mass to in future iterations.
Figure 2: Activation vector sk of a sample xk ona layer L immediately before nonlinearity.
Figure 3: Notations for defining RBRE.
Figure 4: Fitting 2D Mixture of Gaussian (MoG): h1-h4 show the input space linear region definedby different binary activation patterns on each layer; each colour corresponds to one unique binarypattern; the last column shows probability of being real according to D ; BRE is applied on h2and h3. Experimental details and more visualization in Appendix A and D, including one set ofcomparison for fitting an imbalanced mixture in Fig. 16-17.
Figure 5: Even with stable DCGAN architecture, Figure 6: Samples for DCGAN-ReLU without BREBRE makes convergence faster.	(left) vs with BRE (right)4.3	Improved Semi-supervised Learning on CIFAR10 and SVHNBRE regularization is not only compatible with semi-supervised learning using GANâ€™s, but alsoimproves classification accuracy.
Figure 7: Inception scores and regularizer values during training: (left column, i.e. (a) and (c))default optimization setting; (right column, i.e. (b) and (d)) more aggressive optimization. Detailsin Appendix A. (top row, i.e. (a) and (b)) Inception scores during training; (bottom row, i.e. (c) and(d)) RAC term of BRE on fake, real, and interpolation inbetween. Even though BRE is not appliedon real, model still allocates enough capacity when BRE is applied on fake and interpolation.
Figure 8: Improved semi-supervised learning on SVHN: each curve corresponds to a different ran-dom seeding; we repeat the same set of seeds for runs without BRE (Top row), and with BRE(Bottom column); both the random seed for selecting labeled examples and random seeds for modelparameter initialization are varied.
Figure 9: Rastered t-SNE visualization of DCGAN-ReLU CIFAR10 samples. Images with redframes are generated without BRE and images with blue frames are generated with BRE. Locationsroughly indicates similarity between images in the pixel value space.
Figure 10: Improved semi-supervised learning on CIFAR-10: BRE regularizer placed on every othersecond layer, starting from the 2nd until 4 layers before the classification layer. Regularizer weightis .01 and not decayed.
Figure 11: (Thresholded) Activity correlation (AC) values (top left) and samples at iteration 10K:(top right) DCGAN unstable run (lr = .01 and 3 D update steps for each G update); (lower right)DCGAN stable run (lr = 2e - 3 and 1 D update steps for each G update); (lower left) BRE-DCGAN, DCGAN training with BRE regularizer same hyperparameters as DCGAN stable run inlower right plot. BRE-DCGAN results are visibly more diverse.
Figure 12: More Results on Fitting 2D Mixture of Gaussian on the control group. See Figure 4 fordetailed description.
Figure 13: More Results ondetailed description.
Figure 14: More Results on Fitting 2D Mixture of Gaussian on the control group. See Figure 4 fordetailed description.
Figure 15: More Results on Fitting 2D Mixture of Gaussian on the treat group. See Figure 4 fordetailed description.
Figure 16: More Results on Fitting imbalanced 2D Mixture of Gaussian (probabilities [.1, .3, .3, .3])on the control group. See Figure 4 for detailed description.
Figure 17: More Results on Fitting imbalanced 2D Mixture of Gaussian (probabilities [.1, .3, .3, .3])on the treat group. See Figure 4 for detailed description.
