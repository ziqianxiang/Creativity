Figure 1: (a) A graphical model describing the relationship between the latent scene structure{St}, motion {Mt}, and the observed images of a sequence. We describe a method for learning arepresentation M of the motion space M from observed image sequences {It}. (b) By recomposingsequences of images to satisfy the group properties of associativity and invertibility, we constructpairs of image sequences with equivalent motion. We use these properties to learn an approximategroup homomorphism Φ ∈ M between motion in the world and in an embedding.
Figure 2: (a) Network structure. The RNN output at the final step of the sequence is treated as thesequence embedding. During training, the distance between sequence embeddings is adjusted usingan embedding loss. (b) We recompose sequences to enforce associativity and invertibility. Sequenceswith equivalent motion (e.g. 1-2-4 and 1-3-4) serve as positive examples, while sequences withinequivalent motions (e.g. 1-2-4 and 4-3-1) serve as negative examples.
Figure 3: (a) An example test sequence from MNIST and the corresponding saliency maps. Salienciesshow the gradient backpropagated from the final RNN timestep. Each column represents an imagepair passed to one of the CNNs. (b)-(d) tSNE of the network embedding on the test set, with pointslabeled by (b) the magnitude of translation in pixels, (c) the translation direction in degrees, and(d) the digit label (0-9). The representation clusters sequences by both translation magnitude anddirection but not identity.
Figure 4: (a) Natural image motion is a subspace of the space of all image transformations, and aparticular motion can be viewed as a path in the latent space of natural images. Although [I1, IK, IT]has a total transformation equivalent to [I1, IT] for any value of K, only [I1, Im, IT] can be producedby natural image motion. (b) In interpolation experiments, we compare the distance between theembedding of [I1, IT] with the embedding of this sequence after inserting either a true middle frame(Im) or another frame (IIN or IOUT). (c) Images with lowest relative error taken from the sequence orfrom the whole dataset, for each distance measure. Errors are relative to that of the true middle framein the corresponding measure: high relative errors (	1) indicate the distance distinguishes realisticmotion from unrealistic motion. Images other than true middle frame produce dramatically highererrors when using the embedding but not when using a Euclidean distance.
Figure 5: Saliency results on a test sequence from KITTI tracking with both camera and independentmotion. The network focuses on areas that are relevant to determining motion in 3D, not simplyregions with large temporal image gradients.
Figure 6: Error on egomotion regression from self-supervised flow PCA as a function of the numberof principal components included. Horizontal lines reflect our method (latent, shown in red) and achance baseline (show in green).
Figure 7: Cumulative percent variance explained of the optical flow in KITTI odometry as a functionof the number of principal components included. 67% of the variance is explained by the first 5components; 90% of the variance is explained by the first 40 principal components.
Figure 8: Representative principal components of optical flow on the KITTI odometry dataset. Thefirst few components capture the dominant motions (forward and left/right turning) and reflect thestereotypical depth structure of KITTI.
