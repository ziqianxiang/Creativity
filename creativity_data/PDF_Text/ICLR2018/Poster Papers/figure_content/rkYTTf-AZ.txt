Figure 1: Toy illustration of the principles guiding the design of our objective function. Left (auto-encoding): the model is trained to reconstruct a sentence from a noisy version of it. x is the target,C(x) is the noisy input, X is the reconstruction. Right (translation): the model is trained to translatea sentence in the other domain. The input is a noisy translation (in this case, from source-to-target)produced by the model itself, M, at the previous iteration (t), y = M(t) (x). The model is symmet-ric, and we repeat the same process in the other language. See text for more details.
Figure 2: Illustration of the proposed architecture and training objectives. The architecture is asequence to sequence model, with both encoder and decoder operating on two languages dependingon an input language identifier that swaps lookup tables. Top (auto-encoding): the model learns todenoise sentences in each domain. Bottom (translation): like before, except that we encode fromanother language, using as input the translation produced by the model at the previous iteration (lightblue box). The green ellipses indicate terms in the loss function.
Figure 3: Unsupervised model selection.
Figure 4: Left: BLEU as a function of the number of iterations of our algorithm on the Multi30k-Task1 datasets. Right: The curves show BLEU as a function of the amount of parallel data on WMTdatasets. The unsupervised method which leverages about 15 million monolingual sentences in eachlanguage, achieves performance (see horizontal lines) close to what we would obtain by employing100,000 parallel sentences.
