Figure 1: (a) Architecture of the network We are considering. Given input X, We extract its patches{Zi} and send them to a shared weight vector w. The outputs are then sent to ReLU and thensummed to yield the final label (and its estimation). (b)-(c) Two conditions we proposed for con-vergence. We want the data to be (b) highly correlated and (c) concentrated more on the directionaligned with the ground truth vector w*.
Figure 2: (a) The four regions considered in our analysis. (b) Illustration of L (φ), Y(φ) andL-w* (φ) defined in Definition 2.1 and Assumption 2.1.
Figure 3: Convergence rates of SGD (a) with different smoothness where larger σ is smoother; (b)with different closeness of patches where smaller σ2 is closer; (c) for a learning a random filter withdifferent initialization on MNIST data; (d) for a learning a Gabor filter with different initializationon MNIST data.
Figure 4: Visualization of true and learned filters. For each pair, the left one is the underlying truthand the right is the filter learned by SGD.
Figure 5: Loss of linear interpolation between learned filter and the true filter.
