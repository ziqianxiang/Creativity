Figure 1: (left) Illustration of parameter vectors for direct optimization in the D = 3 case. (middle)Illustration of parameter vectors and a possible random subspace for the D = 3,d = 2 case. (right)Plot of performance vs. subspace dimension for the toy example of toy example of Sec. 2. Theproblem becomes both 90% solvable and 100% solvable at random subspace dimension 10, sodint90 and dint100 are 10.
Figure 2: Performance (validation accuracy) vs. subspace dimension d for two networks trainedon MNIST: (left) a 784-200-200-10 fully-connected (FC) network (D = 199,210) and (right) aconvolutional network, LeNet (D = 44,426). The solid line shows performance of a well-traineddirect (FC or conv) model, and the dashed line shows the 90% threshold we use to define dint90 . Thestandard derivation of validation accuracy and measured dint90 are visualized as the blue vertical andred horizontal error bars. We oversample the region around the threshold to estimate the dimensionof crossing more exactly. We use one-run measurements for dint90 of 750 and 290, respectively.
Figure 3: Measured intrinsic dimension dint90 vs number of parameters D for 20 FC models ofvarying width (from 50 to 400) and depth (number of hidden layers from 1 to 5) trained on MNIST.
Figure 4: Performance vs. number of trainable parameters for (left) FC networks and (right) convo-lutional networks trained on MNIST. Randomly generated direct networks are shown (gray circles)alongside all random subspace training results (blue circles) from the sweep shown in Fig. S6. FCnetworks show a persistent gap in dimension, suggesting general parameter inefficiency of FC mod-els. The parameter efficiency of convolutional networks varies, as the gray points can be significantlyto the right of or close to the blue manifold.
Figure 5: Results using the policy-based ES algorithm to train agents on (left column)InvertedPendulum-v1, (middle column) Humanoid-v1, and (right column) Pong-v0. Theintrinsic dimensions found are 4, 700, and 6k. This places the walking humanoid task on a similarlevel of difficulty as modeling MNIST with a FC network (far less than modeling CIFAR-10 with aconvnet), and Pong on the same order of modeling CIFAR-10.
Figure S6: A sweep of FC networks on MNIST. Each column contains networks of the same depth,and each row those of the same number of hidden nodes in each of its layers. Mean and variance ateach d is shown by blue dots and blue bars. dint90 is found by dark blue dots, and the variance of itis indicated by red bars spanning in the d axis.
Figure S7: Measured intrinsic dimension dint90 vs number of parameters D for 20 models of varyingwidth (from 50 to 400) and depth (number of hidden layers from 1 to 5) trained on MNIST. Thevertical red interval is the standard derivation of measured dint90 . As opposed to Fig. 3, which useda global, shared baseline across all models, here a per-model baseline is used. The number of nativeparameters varies by a factor of 24.1, but dint90 varies by only 1.42. The per-model baseline resultsin higher measured dint90 for larger models because they have a higher baseline performance thanthe shallower models.
Figure S8: Training accuracy vs. subspace dimension d for a FC networks (W =400, L=5) trainedon a shuffled label version of MNIST containing 100%, 50%, and 10% of the dataset.
Figure S9: Results of subspace training versus the number of layers in a fully connected networktrained on MNIST. The direct method always fail to converge when L > 5, while subspace trainingyields stable performance across all depths.
