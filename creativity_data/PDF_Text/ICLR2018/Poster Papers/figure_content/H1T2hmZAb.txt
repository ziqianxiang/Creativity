Figure 1: Complex convolution and residual network implementation details.
Figure 2: Precision-recall curve15Published as a conference paper at ICLR 201860 l025	50	75	100	125	150	175FrameFigure 3: Predictions (Top) vs. ground truth (Bottom) for a music segment from the test set.
Figure 3: Predictions (Top) vs. ground truth (Bottom) for a music segment from the test set.
Figure 4: Learning curve for speech spectrum prediction from dev set.
Figure 5: Depiction of Complex Standardization in Deep Complex Networks. At left, Naive Com-plex Standardization (division by complex standard deviation); At right, Complex Standardization(left-multiplication by inverse square root of covariance matrix between < and =). The 250 inputcomplex scalars are at the bottom, with <(v) plotted on x (red axis) and =(v) plotted on y (greenaxis). Deeper representations correspond to greater z (blue axis). The gray ellipse encloses the inputscalars within 1 standard deviation of the mean. Red ellipses enclose all scalars within 1 standarddeviation of the mean after “standardization”. Blue ellipses enclose all scalars within 1 standarddeviation of the mean after left-multiplying all the scalars by a random 2 × 2 linear transformationmatrix. With the naive standardization, the distribution becomes progressively more elliptical withevery layer, eventually collapsing to a line. This ill-conditioning manifests itself as NaNs in theforward pass or backward pass. With the complex standardization, the points’ distribution is alwayssuccessfully re-circularized.
Figure 6: Phase information encoding for each of the activation functions tested for the Deep Com-plex Network. The x-axis represents the real part and the y-axis axis represents the imaginary part;The bottom figure corresponds to the case where b < 0 for modReLU. The radius of the white circleis equal to |b|. In case where b ≥ 0, the whole complex plane would be preserving both phaseand magnitude information and the whole plane would have been colored with orange. Differentcolors represents different encoding of the complex information in the plane. We can see the forboth zReLU and modReLU, the complex representation is discriminated into two regions, i.e, theone that preserves the whole complex information (colored in orange) and the one that cancels it(colored in white). However, CReLU discriminates the complex information into 4 regions where intwo of which, phase information is projected and not canceled. This allows CReLU to discriminateinformation easier with respect to phase information than the other activation functions. For bothzReLU and modReLU, we can see that phase information may be preserved explicitly through anumber of layers when these activation functions are operating in their linear regime, prior to a layerfurther up in a network where the phase of an input lies in a zero region. CReLU has more flexibilitymanipulating phase as it can either set it to zero or π∕2, or even delete the phase information (whenboth real and imaginary parts are canceled) at a given level of depth in the network.
