Figure 1: a) Example neural data showing different kinds of neural correlates underlying spatial nav-igation in EC. All figures are replotted from previous publications. From left to right: a “grid cell”recorded when an animal navigates in a square environment, replotted from Krupic et al. (2012),with the heat map representing the firing rate of this neuron as a function of the animal’s location(red corresponds to high firing rate); a “band-like” cell from Krupic et al. (2012); a border cell fromSolstad et al. (2008); an irregular spatially tuned cell from Diehl et al. (2017); a “speed cell” fromKropff et al. (2015), which exhibits roughly linear dependence on the rodent’s running speed; a“heading direction cell” from Sargolini et al. (2006), which shows systematic change of firing ratedepending on animal’s heading direction. b) The network consists of N = 100 recurrently con-nected units (or neurons) which receive two external inputs, representing the animal’s speed andheading direction. The two outputs linearly weight the neurons in the RNN. The goal of training isto make the responses of the two output neurons accurately represent the animal’s physical location.
Figure 2: Different types of spatial selective responses of units in the trained RNN. Example sim-ulation results for three different environments (square, triangular, hexagon) are presented. Blue(yellow) represents low (high) activity. a) Grid-like responses. b) Band-like responses; c) Border-related responses; d) Spatially irregular responses. These responses can be spatially selective butthey do not form a regular pattern defined in the conventional sense.
Figure 3: Direction tuning and speed tuning for nine example units in an RNN trained in a triangulararena. For each unit, we show the spatial tuning, (head) directional tuning, speed tuning respectively,from left to right. a,b,c) The three model neurons show strong directional tuning, but the spatialtuning is weak and irregular. The three neurons also exhibit linear speed tuning. d,e,f) The threeneurons exhibit grid-like firing patterns, and clear speed tuning. The strength of their directiontuning differ. g,h) Border cells exhibit weak and a bit complex directional tuning and almost nospeed tuning. i) This band cell shows weak directional tuning, but strong speed tuning.
Figure 4: Development of border cells and grid-like cells. Early during training all responses aresimilar to the border related responses, and only as training continues do the grid-like cells emerge.
Figure 5:	Complete set of spatial response profiles for 100 neurons in a RNN trained in a squareenvironment. a) Without proper regularization, complex and periodic spatial response patterns donot emerge. b) With proper regularization, a rich set of periodic response patterns emerge, includinggrid-like responses. Regularization can also be adjusted to achieve spatial profiles intermediatebetween these two examples.
Figure 6:	Error-correction happens at the boundary and the error is stable over time. At the boundary,the direction is resampled to avoid input velocities that lead to a path extending beyond the boundaryof the environment. These changing input statistics at the boundary, termed a boundary interaction,are the only cue the RNN receives about the boundary. We find that the RNN uses the boundaryinteractions to correct the accumulated error between the true integrated input and its predictionbased on the linear readout of equation (2). Panel a), the mean squared error increases when thereare no boundary interactions, but then decreases after a boundary interaction, with more boundaryinteractions leading to greater error reduction. In the absence of further boundary interaction, thesquared error would gradually increase again (blue curve) at roughly a constant rate. b) The networkwas trained using mini-batches of 500 timesteps but has stable error over a duration at least fourorders of magnitude larger. The error of the RNN output (mean and standard deviation shown inblack, computed based on 10000 timesteps) is compared to the error that would be achieved by anRNN outputting the best constant values (red).
