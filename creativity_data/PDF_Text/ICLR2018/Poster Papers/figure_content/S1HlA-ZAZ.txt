Figure 1: The probabilistic graphical model for the Kanerva Machine. Left: the generative model;Central: reading inference model. Right: writing inference model; Dotted lines show approximateinference and dashed lines represent exact inference.
Figure 2: The negative variational lower bound (left), reconstruction loss (central), and KL-Divergence(right) during learning. The dip in the KL-divergence suggests that our model has learned to use thememory.
Figure 3: Left: reconstruction of inputs and the weights used in reconstruction, where each binrepresents the weight over one memory slot. Weights are widely distributed across memory slots.
Figure 4:	One-shot generation given a batch of examples. The first panel shows reference samplesfrom the matched VAE. Samples from our model conditioned on 12 random examples from thespecified number of classes. Conditioning examples are shown above the samples. The 5 columnsshow samples after 0, 2, 4, 6, and 8 iterations.
Figure 5:	Comparison of samples from CIFAR. The 24 conditioning images (top-right) are randomlysampled from the entire CIFAR dataset, so they contains a mix of many classes. Samples from thematched VAE are blurred and lack meaningful local structure. On the other hand, samples from theKanerva Machine have clear local structures, despite using the same encoder and decoder as the VAE.
Figure 6: Left: the training curves of DNC and Kanerva machine both shows 6 instances with the besthyperparameter configuration for each model found via grid search. DNCs were more sensitive torandom initilisation, slower, and plateaued with larger error. Right: the test variational lower-boundsof a DNC (dashed lines) and a Kanerva Machine as a function of different episode sizes and differentsample classes.
Figure 7: Interpolation for Omniglot and CIFAR images. The first and last column show 2 randomimages from the data. Between them are linear interpolations in the space of memory accessingweights wt .
Figure 8: Iteratively sampled priors from VAE, for both Omniglot (left) and Cifar (right). In bothpanels, the columns show samples after 0, 2, 4, 6, 8 and 10 iterations, mirroring the procedureproducing figure 4 and 5.
Figure 9: The architecture of the VAE and the Kanerva Machine used in our experiments. conv/deconv:convolutional and transposed convolutions neural networks. MLP: multiplayer perceptron. concat:vector concatenation. The blue arrows show memory writing as exact inference.
Figure 10: Covariance between memory rows is important. The two curves shows the test loss(negative variational lower bound) as a function of iterations. Four models using full K Ã— Kcovariance matrix U are shown by red curves and four models using diagonal covariance matrix areshown in blue. All other settings for these 8 models are the same (as described in section 4). These 8models are trained on machines with similar setup. The models using full covariance matrices wereslightly slower per-iteration, but the test loss decreased far more quickly.
Figure 11: The KL-divergence between yt (left) and zt (right) during training.
Figure 12: The negative variational lower bound, reconstruction loss, and total KL-divergence duringCIFAR training. Although the difference between the lower bound objective is smaller than thatduring Omniglot training, the general patterns of these curves are similar to those in Fig. 2. Therelatively small difference in KL-divergence significantly influences sample quality. Notice at thetime of our submission, the training is continuing and the advantage of the Kanerva Machine over theVAE is increasing.
