Figure 1: An illustration of a GC Layer.
Figure 2: An illustration of a Generalized Convolutional Network.
Figure 3: Illustrating the total receptive field andtotal stride attributes for the L’th layer, whichcould be seen as the projected receptive field andstride with respect to the input layer. Together,they capture the overlapping degree of a network.
Figure 4: A network architectures beginning with large local receptive fields greater than N/2 and atleast M output channels. According to theorem 1, for almost all choice of parameters we obtain afunction that cannot be approximated by a non-overlapping architecture, if the number of channelsH2in its next to last layer is less than M F.
Figure 5: The common network architecture of alternating B ×B “conv” and 2×2 “pooling” layers.
Figure 6: Training accuracies of standard ConvNets on CIFAR-10 with data augmentations, wherethe results of spatial augmentations presented at the top row, and color augmentations at the bottomrow. Each network follows the architecture of proposition 2, with with receptive field B and usingthe same number of channels across all layers, as specified by the horizontal axis of left plot. Weplot the same results with respect to the total number of parameters in the right plot.
Figure 7: The original Convolutional Arithmetic Circuits as presented by Cohen et al. (2016a).
