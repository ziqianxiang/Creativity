Figure 1: Block Diagram of GANITE (y is sampled from G after G has been fully trained).
Figure 2: Performance comparison between GANITE and state-of-the-art methods as the selectionbias is varied (Kullback-Leibler divergence of treated with respect to controlled distributions)Table 1 shows the performance of the GANITE architecture using different combinations of the fourlosses in Section 4. For each generator component, we can use 3 different combinations of loss:(1) Supervised loss (S loss) only (this reduces the corresponding component to a standard neuralnetwork), (2) GAN loss only, (3) both S loss and GAN loss. The top-left most entry correspondsto simply using a standard neural network to first impute the counterfactuals and then using anotherstandard neural network to learn an ITE estimator from the imputed dataset. As can be seen, thisalready performs well, but by adding the GAN losses for both the imputation step and the estimationstep, a significant gain is shown (15.6%) (the bottom-right entry).
Figure 3: (a) Underlying distribution of potential outcomes (Y), (b) Underlying distribution oftreatment assignments P (T |X), (c) Training data (factual outcomes) sampled from distributionsexplained in (a) and (b), (d) Potential outcomes sampled from trained ITE generator (I).
