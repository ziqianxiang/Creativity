Figure 1: This is an adversarial example crafted for VGG (Simonyan & Zisserman, 2015). Theleft image is classified correctly as king penguin, the center image is the adversarial perturbation(magnified by 10 and enlarged by 128 for better visualization), and the right image is the adversarialexample misclassfied as chihuahua.
Figure 2: The pipeline of our randomization-based defense mechanism. The input image Xn firstgoes through the random resizing layer with a random scale applied. Then the random paddinglayer pads the resized image Xn0 in a random manner. The resulting padded image Xn00 is used forclassification.
Figure 5: Top-1 classificationaccuracy on the adversarial ex-amples generated under theensemble-pattern attack scen-rio.
Figure 4: Top-1 classifica-tion accuracy on the adversar-ial examples generated underthe single-pattern attack scen-rio.
Figure 3: Top-1 classificationaccuracy on the clean imagesand the adversarial examplesgenerated under the vanilla at-tack scenrio.
