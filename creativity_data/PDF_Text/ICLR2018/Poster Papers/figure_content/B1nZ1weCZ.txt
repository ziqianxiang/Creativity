Figure 1: Multi Tasking Instance MT7 (21 tasks). A playlist of game-play for MT7 is at: https://goo.gl/GBXfWD. It verifies our claim; tasks are visually different & difficult.
Figure 2: A visualization of our Active-sampling based Multi-Task Learning FrameworkIn this section, we introduce our framework for MTL by first describing a naive framework for on-lineMTL in the first subsection and then presenting our approach as extension to this framework in thesecond subsection. To avoid the computational costs of training single-task expert networks, weassume that the MTA does not have access to expert networks’ predictions. Previous approaches toMTL: (Parisotto et al., 2016; Rusu et al., 2016a) have been off-line in nature. Before we describe theframeworks, we outline how an on-line algorithm for MTL takes inputs from different tasks.
Figure 3: Evolution of game-play performance scores for A5C,EA4C,UA4C and BA3C agents. Alltraining curves for all the multi-tasking instances have been presented in Appendix D .
Figure 4: Training curve for DUA4C wherein doubling of target estimates is performed and thus notarget scores whatsoever are used.
Figure 5: Understanding abstract LSTM features in our proposed methods by analyzing firing patternsLet fij denote the fraction of time steps for which neuron j fires when tested on task i. Neuron j firesfor the task i if fij ≥ 0.01. We chose this low threshold because there could be important neuronsthat detect rare events. Figure 5 demonstrates that for A4C, a large fraction of the neurons fire for alarge subset of tasks and are not task-specific. It plots neuron index versus the set of fraction of timesteps that neuron fires in, for each task. The neurons have been sorted first by |{i : fij ≥ 0.01}| andthen by Pi fij. Neurons to the left of the figure fire for many tasks whereas those towards the rightare task-specific. The piece-wise constant line in the figure counts the number of tasks in which aparticular neuron fires with the leftmost part signifying 6 tasks and the rightmost part signifying zerotasks. Appendix H contains the analysis for all MTIs and methods.
Figure 6: Turn Off analysis heap-maps for the all agents. For BA3C since the agent scored 0 on oneof the games, normalization along the neuron was done only across the other 5 games.
Figure 7: Comparison of performance of BA3C, A5C, UA4C, EA4C and FA4C agents along withtask-specific A3C agents for MT1 (6 tasks). Agents in these experiments were trained for 300 milliontime steps and required half the data and computation that would be required to train the task-specificagents (STA3C) for all the tasks.
Figure 8: Comparison of performance of BA3C, A5C, UA4C, EA4C and FA4C agents along withtask-specific A3C agents for MT2 (6 tasks). Agents in these experiments were trained for 300 milliontime steps and required half the data and computation that would be required to train the task-specificagents (STA3C) for all the tasks.
Figure 9: Comparison of performance of BA3C, A5C, UA4C, EA4C and FA4C agents along withtask-specific A3C agents for MT3 (6 tasks). Agents in these experiments were trained for 300 milliontime steps and required half the data and computation that would be required to train the task-specificagents (STA3C) for all the tasks.
Figure 10: Comparison of performance of BA3C, A5C, UA4C, EA4C and FA4C agents along withtask-specific A3C agents for MT4 (8 tasks). Agents in these experiments were trained for 400 milliontime steps and required half the data and computation that would be required to train the task-specificagents (STA3C) for all the tasks.
Figure 11: Comparison of performance of BA3C, A5C, UA4C, EA4C and FA4C agents along withtask-specific A3C agents for MT5 (12 tasks). Agents in these experiments were trained for 600million time steps and required half the data and computation that would be required to train thetask-specific agents (STA3C) for all the tasks.
Figure 12: Comparison of performance of BA3C, A5C, UA4C, EA4C and FA4C agents along withtask-specific A3C agents for MT6 (12 tasks). Agents in these experiments were trained for 600million time steps and required half the data and computation that would be required to train thetask-specific agents (STA3C) for all the tasks.
Figure 13: Comparison of performance of BA3C, A5C, UA4C, EA4C and FA4C agents along withtask-specific A3C agents for MT7 (21 tasks). Agents in these experiments were trained for 1.05billion time steps and required half the data and computation that would be required to train thetask-specific agents (STA3C) for all the tasks.
Figure 14: Visual depiction of A5CUA4Cepisode?X ` 「，'，∙ ∙「I Compuxc X —. 'ιlH ； X （'川ItFCl 7 "） ∣∣Next State-lMeta Task-DeciderBaSelineS（瓦）∣ ∣ Latest Episodic Reward of ThSkS（OJDeeP NeUral NetvtfOrk» Sample taskAccumulateDiscountedEpisode RewardEnvironmentM ult i -Task i ng. Aqent个工tPolicyFigure 15: A Visual depiction of UA4C32Published as a conference paper at ICLR 2018
Figure 15: A Visual depiction of UA4C32Published as a conference paper at ICLR 2018EA4CFigure 16: A Visual depiction of EA4CFA4CFigure 17: A Visual depiction of FA4C33Published as a conference paper at ICLR 2018Appendix G:	Robustness of our proposed framework (A4C) toTARGET SCORESOn the robustness of A4C to target scoresTo demonstrate that our framework is robust to the use of different target scores, we performed twotargeted experiments.
Figure 16: A Visual depiction of EA4CFA4CFigure 17: A Visual depiction of FA4C33Published as a conference paper at ICLR 2018Appendix G:	Robustness of our proposed framework (A4C) toTARGET SCORESOn the robustness of A4C to target scoresTo demonstrate that our framework is robust to the use of different target scores, we performed twotargeted experiments.
Figure 17: A Visual depiction of FA4C33Published as a conference paper at ICLR 2018Appendix G:	Robustness of our proposed framework (A4C) toTARGET SCORESOn the robustness of A4C to target scoresTo demonstrate that our framework is robust to the use of different target scores, we performed twotargeted experiments.
Figure 18: Training curve for HUA4C: when human scores are used as target for calculating therewards.
Figure 19: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT136Published as a conference paper at ICLR 2018Neuron-Firing Analysis on MT2:UA4C-MT2 ：ι.o0.80.60.40.20.00	50	100	150	200	2500	50	100	150	200	2500	50	100	150	200	2501.00.80.60.40.2
Figure 20: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT2Neuron-Firing Analysis on MT3:Figure 21: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT337Published as a conference paper at ICLR 2018Neuron-Firing Analysis on MT4:Figure 22: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT4Neuron-Firing Analysis on MT5:UA4C-MT51.00.80.60.40.20.00	50	100	150	200	250
Figure 21: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT337Published as a conference paper at ICLR 2018Neuron-Firing Analysis on MT4:Figure 22: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT4Neuron-Firing Analysis on MT5:UA4C-MT51.00.80.60.40.20.00	50	100	150	200	2500	50	100	150	200	2500	50	100	150	200	250Figure 23: Understanding abstract LSTM features in our proposed methods by analyzing firing
Figure 22: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT4Neuron-Firing Analysis on MT5:UA4C-MT51.00.80.60.40.20.00	50	100	150	200	2500	50	100	150	200	2500	50	100	150	200	250Figure 23: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT50	50	100	150	200	25038Published as a conference paper at ICLR 2018Neuron-Firing Analysis on MT6:
Figure 23: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT50	50	100	150	200	25038Published as a conference paper at ICLR 2018Neuron-Firing Analysis on MT6:O 50 IOO 150	200	2500	50	100	150	200	250O 50 IOO 150	200	250EA4C-MT6Figure 24: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT60	50	100	150	200	25039Published as a conference paper at ICLR 2018Neuron-Firing Analysis on MT7:UA4C-MT7•	Space Invaders•	SeaquestA5C-MT7
Figure 24: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT60	50	100	150	200	25039Published as a conference paper at ICLR 2018Neuron-Firing Analysis on MT7:UA4C-MT7•	Space Invaders•	SeaquestA5C-MT7•	Space Invaders•	SeaqUeSt1.00.80.60.40.20.0• Crazy Climber• Demon Attack
Figure 25: Understanding abstract LSTM features in our proposed methods by analyzing firingpatterns on MT740Published as a conference paper at ICLR 2018Appendix I:	Different output heads for different tasksSome works in multi-task learning (Parisotto et al., 2016) have experimented with baselines whereinall but one layer of the MTN are shared across tasks (to enable the learning of task-agnostic commonrepresentation of the different state spaces) but the final layer is task-specific. In fact (Rusu et al.,2016a) used different output heads for their main method, the Policy-distillation based MTA aswell. We wanted to understand whether the use of different output heads helps with the problem ofmulti-tasking or not. The reason that this section is in the appendix because we answer that questionin the negative.
Figure 26: Training Curves for the DUA4C agent on MT1 (6 tasks). The horizontal line representsthe Single Task Agent’s score. Agents in these experiments were trained for 300 million time stepsand required half the data and computation that would be required to train the task-specific agents(STA3C) for all the tasks.
Figure 27: Training Curves for the DUA4C agent on MT2 (6 tasks). The horizontal line representsthe Single Task Agent’s score. Agents in these experiments were trained for 300 million time stepsand required half the data and computation that would be required to train the task-specific agents(STA3C) for all the tasks.
Figure 28: Training Curves for the DUA4C agent on MT4 (8 tasks). The horizontal line representsthe Single Task Agent’s score. Agents in these experiments were trained for 400 million time stepsand required half the data and computation that would be required to train the task-specific agents(STA3C) for all the tasks.
Figure 29: Training Curves for the DUA4C agent on MT5 (12 tasks). The horizontal line representsthe Single Task Agent’s score. Agents in these experiments were trained for 600 million time stepsand required half the data and computation that would be required to train the task-specific agents(STA3C) for all the tasks.
