Figure 1: Empirical variance of gradients with respect to mini-batch size for several architectures. (a) FCon MNIST; FC1 denotes the first layer of the FC network. (b) ConVGG on CIFAR-10; Conv1 denotes thefirst convolutional layer. (c) LSTM on Penn Treebank; the variance is shown for the hidden-to-hidden weightmatrices in the first LSTM layer: Wf, Wi, Wo, and Wc are the weights for the forget, input and output gates,and the candidate cell update, respectively. Dotted: shared perturbations. Solid: flipout. Dashed: LRT.
Figure 2: Large batch training and ES. a) Training loss per iteration using Bayes By Backprop with batch size8192 on the FC and ConvLe networks. b) Error rate of the FC network on MNIST using ES with 1,600 samplesper update; there is no drop in performance compared to ideal ES. c) Error rate of FC on MNIST, comparingFlipES (with either 5,000 or 1,600 samples per update) with backpropagation. (This figure does not imply thatFlipES is more efficient than backprop; FlipES was around 60 times more expensive than backprop per update.)d) The same as (c), except run on ConvLe.
Figure 3: Empirical variance of the gradients when training on multiple GPUs. Solid: flipout. Dotted: sharedperturbations.
Figure 5: Per-update time comparison between FlipES and 40-core cpuES (5,000 samples) w.r.t. themodel size. We scale the FC network by modifying the number of hidden units, and we scale theConv network by modifying the number of filters (1.0 stands for 32 filters in the first convolutionallayer and 64 filters for the second one).
Figure 7: Training curves for WD andWD+Flipout, with batch size 8192.
Figure 6: The variance reduction offered by flipoutcompared to the WD model (Merity et al., 2017).
