Figure 1: Images from the VisDA-17 domain adaptation challenge2	Related workIn this section we will cover self-ensembling based semi-supervised methods that form the basis ofour approach and domain adaptation techniques to which our work can be compared.
Figure 2: The network structures of the original mean teacher model and our model. Dashed lines inthe mean teacher model indicate that ground truth labels - and therefore cross-entropy classificationloss - are only available for labeled samples.
Figure 3: Small image domain adaptation example imagesQ O Âè∑ B yQ O ? 3 &We addressed this problem by introducing a class balance loss term that penalises the network formaking predictions that exhibit large class imbalance. For each target domain mini-batch we com-pute the mean of the predicted sample class probabilities over the sample dimension, resulting in themini-batch mean per-class probability. The loss is computed as the binary cross entropy between themean class probability vector and a uniform probability vector. We balance the strength of the classbalance loss with that of the self-ensembling loss by multiplying the class balance loss by the aver-age of the confidence threshold mask (e.g. if 75% of samples in a mini-batch pass the confidencethreshold, then the class balance loss is multiplied by 0.75).2We would like to note the similarity between our class balance loss and the entropy maximisationloss in the IMSAT clustering model of Hu et al. (2017); IMSAT employs entropy maximisationto encourage uniform cluster sizes and entropy minimisation to encourage unambiguous clusterassignments.
