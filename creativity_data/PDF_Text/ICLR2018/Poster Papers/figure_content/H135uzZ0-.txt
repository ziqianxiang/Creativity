Figure 1: Snapshot of precision and dynamic range capabilities of a) IEEE-754 float b) IEEE-754half-float, and c) Dynamic Fixed Point (DFP-16) data formats.
Figure 2: High-level data flow diagram for mixed precision training. Operators FPl, BPl and WUlindicate convolution layers, while Qa, Qw, Qe are quantization operators for activations, weightsand back propagated errors. Please note, the weight gradients (âˆ†wl) are not quantized before SGD,the updated weights are quantized for the next iteration.
Figure 3: Convergence plots for DFP-16b training vs. reference baseline FP32 results for ResNet-50,GoogLeNet-v1, VGG-16 and AlexNet trained for ImageNet-1K classification taskIt can be seen from Figure.3 that DFP16, closely tracks the full precision training. For some modelslike GoogLeNet-v1 and AlexNet, we observe the initially DFP16 training lags the baseline, howeverthis gaps is closed with subsequent epochs especially after the learning rate changes. Further, weobserve that compared to baseline run - with DFP16 the validation/test loss tracks much closer to thetraining loss. We believe this is the effect of the additional noise introduced from reduced precisioncomputation/storage, which is results in better generalization with reduced training-testing gap andbetter accuracies.
Figure 4: Performance breakdown of mixed precision DFP16training vs. baseline FP32to faster compute. Furthermore, such memory bandwidth optimizations are becoming more criticalwith the growing disparity between compute capabilities and memory bandwidth with advent ofspecialized compute accelerators.
