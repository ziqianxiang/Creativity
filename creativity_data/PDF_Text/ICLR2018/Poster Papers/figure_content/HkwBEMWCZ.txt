Figure 1: Singularities in a fully connected layer and how skip connections break them. (a) Inelimination singularities, zero incoming weights, J = 0, eliminate units and make outgoing weights,w, non-identifiable (red). Skip connections (blue) ensure units are active at least sometimes, so theoutgoing weights are identifiable (green). The reverse holds for zero outgoing weights, w = 0: skipconnections recover identifiability for J. (b) In overlap singularities, overlapping incoming weights,Ja = Jb, make outgoing weights non-identifiable; skip connections again break the degeneracy. (c)In linear dependence singularities, a subset of the hidden units become linearly dependent, makingtheir outgoing weights non-identifiable; skip connections break the linear dependence.
Figure 2: Why singularities are harmful for gradient-based learning. (a) Diagram of the analyzednetwork and parameter reduction performed in the analysis in Wei et al. (2008). h = 0 correspondsto the overlap singularity and z = ±1 correspond to the elimination singularities. (b) The gradientflow field for the two-dimensional reduced system. The gradient norm is indicated by color. Thesegment marked by the thick solid line is stable in this example; its basin of attraction is shadedin gray. (c) Near-singularity plateau. Trajectory of learning dynamics starting from the black dotindicated in b. Analysis and plots adapted from Wei et al. (2008). (d) Illustration of a linear depen-dence manifold in a toy model: the new coordinate m represents the distance to a particular lineardependence manifold. (e) Gradient flow field for the toy model shown in d.
Figure 3: Model degeneracy increases training difficulty. (a) Training accuracy of different archi-tectures. (b) Estimated fraction of degenerate eigenvalues during training. Error bars are standarderrors over 50 independent runs of the simulations. (c) Estimated fraction of negative eigenvaluesduring training. (d) Example fitted spectra after 3 training epochs.
Figure 4: Training accuracy is correlated with distance from degenerate manifolds. (a) Training ac-curacies of the best 10 and the worst 10 plain networks trained on CIFAR-100. (b) Estimated fractionof degenerate eigenvalues throughout training. Error bars are standard errors over 10 networks. (c)Mean norm of the incoming weight vectors of hidden units. (d) Mean overlap of the weight vectorsof hidden units. (e) Linear dependence between hidden units in the same layer, measured by thefraction of variance explained by the top three eigenmodes of their covariance matrix. These valuesare averaged over the 30 layers of the network, yielding a single linear dependence score for eachnetwork. For a replication of the results shown here for two other datasets, see Figure S7.
Figure 5: Singularity elimination through bias regularization improves training. (a-b) Training ac-curacy of 30-layer networks on the CIFAR-10 and CIFAR-100 benchmarks. Error bars represent±1 SEM over 50 independent runs. (c) Estimated fraction of degenerate eigenvalues in the plain,residual and BiasReg networks.
Figure 6: Skip connections effectively deal with the vanishing gradients problem. (a) Mean gradientnorms with respect to layer activities at the beginning of the first three epochs. Note that the meangradient norms of Plain and BiasReg networks are almost identical initially. (b) Training accuracyof the networks on the CIFAR-100 dataset. The BiasReg network with a single batch normalizationlayer inserted at layer 15 (BiasReg+BN) is shown in yellow. Its performance approaches the perfor-mance of the residual network. The results shown are averages over 50 independent runs. Standarderrors are small, hence are not shown for clarity.
Figure 7:	Random dense orthogonal skip connectivity matrices work slightly better than identityskip connections. (a) Increasing the non-orthogonality of the skip connectivity matrix reduces theperformance (represented by lighter shades of gray). The results shown are averages over 10 in-dependent runs of the simulations. (b) Probability of zero responses for residual networks withidentity skip connections (blue) and dense random orthogonal skip connections (black), averagedover all hidden units and all training examples.
Figure 8:	Success of orthogonal skip connections cannot be explained by their ability to deal withvanishing gradients only. (a) Eigenvalues of the covariance matrices with different τ values: τ = 0corresponds to orthogonal skip connectivity matrices, larger τ values correspond to less orthogonalmatrices. Note that these eigenvalues are the eigenvalues of the covariance matrix of the skip con-nectivity vectors. The eigenvalue spectra of the skip connectivity matrices are always fixed to beon the unit circle, hence equivalent to that of an orthogonal matrix. (b) Mean gradient norms withrespect to layer activations at the beginning of training. Gradients do not vanish in less orthogonalskip connectivity matrices. If anything, gradient norms are typically larger with such matrices. (c-d)Training accuracies on CIFAR-100 and CIFAR-10.
Figure S1: Validation of the results with 10-layer plain and residual networks trained on CIFAR-100.
Figure S2: Validation of the results with 400 10-layer plain networks with 16 hidden units in eachlayer (4852 parameters total) trained on CIFAR-100. We compare the best 40 networks with theworst 40 networks, as in Figure 4. (a-b) Training and test accuracy. (c-d) Fraction of negativeand degenerate eigenvalues throughout training. Better performing networks are less degenerate andhave more negative eigenvalues. (e) Mean norms of the incoming weight vectors of the hidden units.
Figure S3: For 10-layer plain networks with 32 hidden units in each layer (14292 parameters total),estimates obtained from the mixture model slightly underestimate the fraction of degenerate eigen-values, and overestimate the fraction of negative eigenvalues; however, there is a highly significantlinear relationship between the actual values and the estimates. (a) Actual vs. estimated fraction ofdegenerate eigenvalues. (b) Actual vs. estimated fraction of negative eigenvalues for the same net-works. Dashed line shows the identity line. Dots and errorbars represent means and standard errorsof estimates in different bins; the solid lines and the shaded regions represent the linear regressionfits and the 95% confidence intervals.
Figure S4: Evolution of a1 and a2 in linear plain and residual networks (evolution of b1 and b2proceeds similarly). The weights converge faster in residual networks. Simulation details are asfollows: the number of hidden units is 2 (the two solid lines for each color represent the weightsassociated with the two hidden nodes, e.g. a11 and a12 on the left), the singular values are s1 = 3.0,s2 = 1.5. For the residual network, uι = vι = [1/√2,1/√2]> and u = v2 = [1/√2, -1/√2]>.
Figure S5: (a) Phase portraits for three-layer plain, residual and hyper-residual linear networks. (b)Evolution of u = QlN=l1-1 al for 10-layer plain, residual and hyper-residual linear networks. In theplain network, u did not converge to its asymptotic value s within the simulated time window.
Figure S6:	Main results hold for two-layer shallow nets trained on CIFAR-100. (a-b) Training andtest accuracies. Residual nets perform slightly better. (c-d) Fraction of negative and degenerateeigenvalues. Residual nets are less degenerate. (e) Mean gradient norms with respect to the twolayer activations throughout training. (f) Mean overlap for the second hidden layer units, measuredas the mean correlation between the incoming weights of the hidden units. Results in (a-e) areaverages over 16 independent runs; error bars are small, hence not shown for clarity. In (f), errorbars represent standard errors.
Figure S7:	Replication of the results reported in Figure 4 for (a) the Street View House Numbers(SVHN) dataset and (b) the STL-10 dataset.
