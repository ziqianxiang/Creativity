Figure 1: Three types of GAN architectures. Left: shallow. Middle: semi-shallow. Right: deep.
Figure 2: Mode Collapse on a Gaussian Mixture. We show heat maps of the generator distribution over time, aswell as the target data distribution in the last column. Standard GAN updates (top row) cause mode collapse,whereas CHEKHOV GAN using K = 10 past steps (bottom row) spreads its mass over all the modes of thetarget distribution.
Figure 3: chekhov GAN consistently outperforms WGAN across both metrics: (left) Inception score, (right)Frechet Inception Distance (FID). The shaded area denotes the standard deviation.
Figure 4: Comparison of FID: (left) WGAN-based variants, (right) GAN-based variants.The shaded areadenotes the standard deviation.
Figure 5: Both GAN and CHEKHOV GAN converge to the true data distribution when the dimensionality of thenoise vector is 2of 0.01 and means equally spaced around a unit circle.
Figure 6: CHEKHOV GAN with concave discriminator is able to learn the toy data distribution with K = 10.
Figure 7: Mode Collapse on a Gaussian Mixture. Comparison between a) vanilla GANs; b) CHEKHOVGAN with K = 10, increase=1 and regularization of 0.0005; c) Unrolled GAN with 10 unrolling steps and d)Unrolled GAN with 5 unrolling steps.
Figure 8:	Comparison of CHEKHOV GAN for K = 10 when generating from the most current and all of thegenerators at the final step of training.
Figure 9:	Random batch of generated images from GAN after training for 30 epochs (on the left) andChekhov GAN (K=25) after training for 30 epochs (on the right)D.3.1 RESULTS ON CIFAR 1 0We train for 30 epochs, which we find to be the optimal number of training steps for vanilla GAN interms of MSE on images from the validation set. Table 10 includes comparison to other baselines. Thefirst set of baselines (given with purple color) consist of GAN where the updates in the inner loop (forthe discriminator), the outer loop (for the generator), or both are performed 25 times. The baselinesshown with green color are regularized versions of GANs, where we apply the same regularization asin our Chekhov GAN in order to show that the gain is not due to the regularization only. Figure 9presents two randomly sampled batches from the generator trained with GAN and Chekhov GAN .
Figure 10: Random batch of generated images from GAN (left) and CHEKHOV GAN (K = 5) (right)after training for 10 epochs.
Figure 11: Comparison of Inception score: (left) WGAN-based variants, (right) GAN-based variants.Theshaded area denotes the standard deviation.
