Figure 1: Overview of our simple neural attentive learner (SNAIL); in this example, two blocks ofTC layers (orange) are interleaved with two causal attention layers (green). The same class of modelarchitectures can be applied to both supervised and reinforcement learning.
Figure 3: Learning curves of SNAIL (red) and LSTM (blue) on the random MDP task for differentvalues of N . The horizontal axis is the TRPO iteration, and the vertical is average reward.
Figure 4: Test-time adaptation curves on simulated locomotion tasks for SNAIL, LSTM, and MAML(which was unrolled for three policy gradient updates). Since SNAIL incorporates experience throughits hidden state, it can exploit common task structure to perform optimally within a few timesteps.
Figure 5: From left to right: (a) A (higher-resolution) example of the observations the agent receives.
Figure 6: (a) A residual block within our mini-Imagenet embedding. (b) The embedding, a smallerversion of ResNet (He et al., 2016), uses several of the residual blocks depicted in (a).
