Figure 1: After setting the hidden-layer targets T1 of a deep hard-threshold network, the network decomposesinto independent perceptrons, which can then be learned with standard methods.
Figure 2: Figures (a)-(c) show different per-layer loss functions (solid blue line) and their derivatives (dashedred line). Figure (d) shows the quantized ReLU activation (solid blue line), which is a sum of step functions, itscorresponding sum of saturated-hinge-loss derivatives (dashed red line), and the soft-hinge-loss approximationto this sum that was found to work best (dotted yellow line).
Figure 3: The top-1 train (thin dashed lines) and test (thicker solid lines) accuracies for AlexNet with differentactivation functions on ImageNet. The inset figures show the test accuracy for the final 25 epochs in detail. Inboth figures, ftprop-mb with soft hinge (FTP-SH, red) outperforms the saturated straight-through estimator(SSTE, blue). The left figure shows the network with sign activations. The right figure shows that the 2-bitquantized ReLU (qReLU) trained with our method (FTP-SH) performs nearly as well as the full-precision ReLU.
Figure 4: The top-1 test accuracies for the 4-layer convolutional network with different activation functions onCIFAR-10. The inset figures show the test accuracy for the final 100 epochs in detail. The left figure showsthe network with sign activations. The right figure shows the network with 2-bit quantized ReLU (qReLU)activations and with the full-precision baselines. Best viewed in color.
Figure 5: The top-1 test accuracies for the 8-layer convolutional network with different activation functions onCIFAR-10. The inset figures show the test accuracy for the final 100 epochs in detail. The left figure showsthe network with sign activations. The right figure shows the network with 2-bit quantized ReLU (qReLU)activations and with the full-precision baselines. Best viewed in color.
Figure 6: The top-1 train (thin dashed lines) and test (thicker solid lines) accuracies for AlexNet with differentactivation functions on ImageNet. The inset figures show the test accuracy for the final 25 epochs in detail. Theleft figure shows the network with sign activations. The right figure shows the network with 2-bit quantizedReLU (qReLU) activations and with the full-precision baselines. Best viewed in color.
Figure 7: The top-1 train (thin dashed lines) and test (thicker solid lines) accuracies for ResNet-18 withdifferent activation functions on ImageNet. The inset figures show the test accuracy for the final 60 epochs indetail. The left figure shows the network with sign activations. The right figure shows the network with 3-bitquantized ReLU (qReLU) activations and with the full-precision baselines. Best viewed in color.
