Figure 1: An illustration of xworld and the two language use cases. (a) and (b): A mixed trainingof NAV and QA. (c): Testing ZS1 sentences contain a new combination of words (“east” and “av-ocado”) that never appear together in any training sentence. (d): Testing ZS2 sentences contain anew word (“watermelon”) that never appears in any training sentence but is learned from a traininganswer. This figure is only a conceptual illustration of language generalization; in practice it mighttake many training sessions before the agent can generalize. (Due to space limitations, the maps areonly partially shown.)1)	interpolation, new combinations of previously seen words for the same use case, or2)	extrapolation, new words transferred from other use cases and models.
Figure 2: An overview of the model. We process e by always placing the agent at the center viazero padding. This helps the agent learn navigation actions by reducing the variety of target repre-sentations. c, a, and V are the predicted answer, the navigation action, and the critic value for policygradient, respectively. φ denotes the concept detection function shared by language grounding andprediction. MA generates a compact representation from xi∩c and h for navigation (Appendix C).
Figure 3: An illustration of the attention cube XCUbe “ xioc ∙ Xfeatl, where xioc attends to imageregions and xfeat selects feature maps. In this example, xloc is computed from “northeast.” In orderfor the agent to correctly answer “red” (color) instead of “watermelon” (object name), xfeat has tobe computed from the sentence pattern “What ... color ...?”use for the scoring. Intuitively, each score on χ indicates the detection response of the feature vectorin that location. A higher score represents a higher detection response.
Figure 4: A symbolic example of the 2D convolution for transforming attention maps. A 2D con-volution can be decomposed into two steps: flipping and cross correlation. The attention map of“northwest” is treated as an offset filter to translate that of “apple." Note that in practice, the atten-tion is continuous and noisy, and the interpreter has to learn to find out the words (if any) to performthis convolution.
Figure 5: The three types of language data and their statistics.
Figure 6: The basic evaluation. (a) Training reward curves. The shown reward is the accumulateddiscounted reward per session, averaged over every 8k training time steps. The shaded area of eachcurve denotes the variance among 4 random initializations of the model parameters. The reasonwhy the curves tend to drop in the beginning is that the map difficulty increases according to ourcurriculum (Appendix F). (b) Navigation success rates in the test. (c) The accuracies of the answersin the test (NAVA is excluded because it does not train QA).
Figure 7: The test results of language generalization with a varying held-out portion of X %, whereX “ 0 represents the basic evaluation in Section 4.3. (a-c) ZS1. (d-f) ZS2. For either ZS1 orZS2, from top to bottom, the three rows represent the average navigation reward per session, theaverage navigation success rate per session, and the average QA accuracy, respectively. (The plotsof nav-obj in (a) and (b) are empty because there is no ZS1 sentence of this type by definition.)4.5	How does it adapt to 3D?We discuss the possibility of adapting our model to an agent with similar language abilities in a 3Dworld (e.g., Beattie et al. (2016); Johnson et al. (2016)). This is our goal for the future, but herewe would like to share some preliminary thoughts. Generally speaking, a 3D world will pose a12Published as a conference paper at ICLR 2018greater challenge for vision-related computations. The key element of our model is the attentioncube xcube that is intended for an explicit language grounding, including the channel mask xfeat andthe attention map xloc. The channel mask only depends on the sentence, and thus is expected to workregardless of the world’s dimensionality. The interpreter depends on a sequence of score maps χwhich for now are computed as multiplying a word embedding with the feature cube (Eq. 3). A moresophisticated definition of φ will be needed to detect objects in a 3D environment. Additionally, theinterpreter models the spatial transform of attention as a 2D convolution (Eq. 7). This assumptionwill be not valid for reasoning 3D spatial relations on 2D images, and a better transform methodthat accounts for perspective distortion is required. Lastly, the surrounding environment is only
Figure 8: The Euclidean distance matrix of the 134 question groups where each group is representedby a word label. Each row (column) represents the sampled questions that have the word label asthe answer. A matrix entry indicates the empirical expectation of the distance between the channelmasks of the sentences from two question groups. The labels are arranged into three topics: color,object, and spatial relation. A small distance indicates that the two channel masks are similar. (Zoomin on the screen for a better view.)AppendicesA Visualization and analysisIn this section, we visualize and analyze some intermediate results of our trained model.
Figure 9: Visualizations of the computation of word attention. (a) Word context vectors Wl. (b) Theword attentions ol of several example questions. Each attention vector, represented by a color bar,shows the attention accumulated over I interpretation steps.
Figure 10: The first example showing how xloc is computed.
Figure 11: The second example showing how xloc is computed.
Figure 12: The third example showing how xloc is computed.
Figure 13: The fourth example showing how xloc is computed.
Figure 14: The fifth example showing how xloc is computed.
Figure 15: The sixth example showing how xloc is computed.
Figure 16: All the 119 X 3 = 357 object instances plus the agent (second-to-last) and the wall (last).
Figure 17: An overview of the baseline VL. The computations of NAV and QA only differ in thelast MLPs.
Figure 18: An overview of the baseline CE. The computations of NAV and QA only differ in thelast MLPs.
