Figure 1: Cross-entropy loss on adversarial examples during training. The plots show how theadversarial loss on training examples evolves during training the MNIST and CIFAR10 networksagainst a PGD adversary. The sharp drops in the CIFAR10 plot correspond to decreases in traininglearning rate. These plots illustrate that we can consistently reduce the value of the inner problem ofthe saddle point formulation (2.1), thus producing an increasingly robust classifier.
Figure 2: Performance of our adversarially trained networks against PGD adversaries of differentstrength. The MNIST and CIFAR10 networks were trained against ε = 0.3 and ε = 8 PGD '∞adversaries respectively (the training ε is denoted with a red dashed lines in the '∞ plots). We noticethat for ε less or equal to the value used during training, the performance is equal or better.
Figure 3: Cross-entropy loss values while creating an adversarial example from the MNIST andCIFAR10 evaluation datasets. The plots show how the loss evolves during 20 runs of projectedgradient descent (PGD). Each run starts at a uniformly random point in the '∞-ball around the samenatural example (additional plots for different examples appear in Figure 11). The adversarial lossplateaus after a small number of iterations. The optimization trajectories and final loss values arealso fairly clustered, especially on CIFAR10. Moreover, the final loss values on adversarially trainednetworks are significantly smaller than on their naturally trained counterparts.
Figure 4: Values of the local maxima given by the cross-entropy loss for five examples from theMNIST and CIFAR10 evaluation datasets. For each example, We start projected gradient descent(PGD) from 105 uniformly random points in the '∞ -ball around the example and iterate PGD untilthe loss plateaus. The blue histogram corresponds to the loss on a naturally trained network, while thered histogram corresponds to the adversarially trained counterpart. The loss is significantly smallerfor the adversarially trained networks, and the final loss values are very concentrated without anyoutliers.
Figure 5: A conceptual illustration of “natural” vs. “adversarial” decision boundaries. Left: A setof points that can be easily separated with a simple (in this case, linear) decision boundary. Middle:The simple decision boundary does not separate the '∞-balls (here, squares) around the data points.
Figure 6: The effect of network capacity on the performance of the network. We trained MNIST andCIFAR10 networks of varying capacity on: (a) natural examples, (b) with FGSM-made adversarialexamples, (c) with PGD-made adversarial examples. In the first three plots/tables of each dataset, weshow how the natural and adversarial accuracy changes with respect to capacity for each trainingregime. In the final plot/table, we show the value of the cross-entropy loss on the adversarial examplesthe networks were trained on. This corresponds to the value of our saddle point formulation (2.1) fordifferent sets of allowed perturbations.
Figure 7: CIFAR10: change of loss function in the direction of white-box and black-box FGSMand PGD examples with ε = 8 for the same five natural examples. Each line shows how the losschanges as we move from the natural example to the corresponding adversarial example. Top: simplenaturally trained model. Bottom: wide PGD trained model. We plot the loss of the original networkin the direction of the FGSM example for the original network (red lines), 5 PGD examples for theoriginal network obtained from 5 random starting points (blue lines), the FGSM example for anindependently trained copy network (green lines) and 5 PGD examples for the copy network obtainedfrom 5 random starting points (black lines). All PGD attacks use 100 steps with step size 0.3.
Figure 8: Transferability experiments for four different instances (naturally trained large and verylarge networks, and FGSM-trained large and very large networks, respectively). For each instance weran the same training algorithm twice, starting from different initializations. Tables on the left showthe accuracy of the networks against three types of input (clean, perturbed with FGSM, perturbedwith PGD ran for 40 steps); the first column shows the resilience of the first network against examplesproduced using its own gradients, the second column shows resilience of the second network againstexamples transferred from the former network. The histograms reflect angles between pairs ofgradients corresponding to the same inputs versus the baseline consisting of angles between gradientsfrom random pairs of points. Images on the right hand side reflect how the loss functions of the nativeand the transfer network change when moving in the direction of the perturbation; the perturbation isat 1 on the horizontal axis. Plots in the top row are for FGSM perturbations, plots in the bottom roware for PGD perturbations produced over 40 iterations.
Figure 9: Visualizing a sample of the convolutional filters. For the natural model (a,b) we visualizerandom filters, since there is no observable difference in any of them. For the first layer of robustnetworks we make sure to include the 3 non-zero filters. For the second layer, the first three columnsrepresent convolutional filters that utilize the 3 non-zero channels, and we choose the most interestingones (larger range of values). We observe that adversarially trained networks have significantly moreconcentrated weights. Moreover, the first convolutional layer degrades into a few thresholding filters.
Figure 10:	Softmax layer examination. For each network we create a histogram of the layer’s weightsand plot the per-class bias. We observe that while weights are similar (slightly more concentrated forthe natural one) the biases are far from uniform and with a similar pattern for the two adversariallytrained networks.
Figure 11:	Loss function value over PGD iterations for 20 random restarts on random examples. The1st and 3rd rows correspond to naturally trained networks, while the 2nd and 4th to adversariallytrained ones.
Figure 12:	Sample adversarial examples with `2 norm bounded by 4. The perturbations are significantenough to cause misclassification by humans too.
