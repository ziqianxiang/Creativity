Figure 1: The interactive process between teacherand learner.
Figure 2: Test accuracy curves of different teaching strategies on MNIST(a), CIFAR-10(b) andIMDB(c). Different hyper-parameter settings are included: The numbers in L2T-τ and SPL-Erespectively represent the two hyper-parameters in L2T and SPL introduced in Subsection 5.1.3.
Figure 3: The number of instances filtered by L2T teacher in each training epoch of MNIST(a),CIFAR-10(b) and IMDB(c). Different curves denote the number of filtered data corresponding todifferent hardness levels, as indicated by the ranks of loss on that filtered data instance within itsmini-batch. Concretely speaking, We evenly partition all the rank values {1, 2,…，M}, where Mis the batch size, into five buckets. Bucket 1 denotes the hardest data whose loss values are largestamong the instances in each mini-batch, while bucket 5 is the easiest.
Figure 4: (a):Apply the teacher trained based on ResNet32 to teach ResNet110 on CIFAR-10. (b):Apply the teacher trained based on MLP for MNIST to train CNN for CIFAR-10. (c):Apply theteacher trained based on CNN for CIFAR-10 to train MLP for MNIST.
Figure 5: Learning curves w.r.t. wall-clock time of training ResNet32 student model on CIFAR-10under different teaching strategies.
Figure 6: Feature analysis for L2T on MNIST dataset. The learning curves of NoTeach, and L2Twith all the three parts of features remained, are also included.
Figure 7: (a):The reward curves in each episode of teacher model training. (b): The L2 norm ofteacher model weight changes (∆θ) in each episode of teacher model training.
