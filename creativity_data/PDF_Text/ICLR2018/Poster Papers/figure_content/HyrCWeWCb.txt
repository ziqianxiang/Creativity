Figure 1: The results of TrUst-PCL against a TRPO baseline. Each plot shows average greedyreward with single standard deviation error intervals capped at the min and max across 4 best of 5randomly seeded training runs after choosing best hyperparameters. The x-axis shows millions ofenvironment steps. We observe that Trust-PCL is consistently able to match and, in many cases,beat TRPO,s performance both in terms of reward and sample efficiency.
Figure 2: The results of Trust-PCL across several values of e, defining the size of the trust re-gion. Each plot shows average greedy reward across 4 best of 5 randomly seeded training runs afterchoosing best hyperparameters. The x-axis shows millions of environment steps. We observe thatinstability increases with e, thus concluding that the use of trust region is crucial.
Figure 3: The results of Trust-PCL varying the degree of on/off-policy. We see that Trust-PCL(on-policy) has a behavior similar to TRPO, achieving good final reward but requiring an exorbitantnumber of experience collection. When collecting less experience per training step in Trust-PCL(off-policy), we are able to improve sample efficiency while still achieving a competitive final re-ward.
