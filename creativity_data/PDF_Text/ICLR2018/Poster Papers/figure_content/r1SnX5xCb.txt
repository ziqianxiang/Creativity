Figure 1: Deep Sensing ParadigmIt is important to note that CT +ι might be empty; i.e., there might be no measurements for which theinformation gain exceeds the cost. Because of this, Deep Sensing answers both the question ”whento sample?” as well as the question ”what to sample?” At each time T, Deep Sensing asks whetherthere are any measurements to be made at time T +1 for which the benefit outweighs the cost. If theanswer is“yes"(i.e. CT +ι = 0) then Deep Sensing recommends that those measurements should bemade at time T + 1. If the answer is ”no” (i.e. CT +ι = 0) then Deep Sensing asks whether there areany measurements to be made at time T + 2 for which the benefit outweighs the cost, and so forth.
Figure 2: Block diagram of Deep Sensinga measurement that is truly missing in the dataset. Instead we fix a measurement that was actuallymade, remove that measurement, form an estimate for the measurement using only the data setD - xtd (i.e. the data set with xtd removed), and then compute the error between the estimate and theactual measurement (that was deleted). If Xd is an actual measurement and Xd is the estimate formedwhen Xd is removed then the loss can be defined as the mean squared error (MSE) l(Xd Xd)=(Xd - xd)2. The loss for the entire dataset D is defined asNL({Xd MD = Xn=1PTnI PdtI mdS) × (Xds) — XKn))I2PTnI PD=I md(n)	.
Figure 3: Diagram of the neural networks for M-RNNavoids overfitting and leads to significant performance improvements as compared to a standardBi-RNN. (See the Interpolation part of Fig. 3.)Imputation: The objective of the imputation block is to construct an imputation function Ψ thatoperates across streams. Again, We abuse notation and write Xd = Ψ(D - xd). KeeP in mind thatnow we are using only data at time stamp t, not data from other time stamps. We construct thefunction Ψ to be independent of the time stamp t; so we construct it using fully connected layers(FC); see Imputation part of Fig 3:ot= Wht+co,ht = U xt + V Xt + Qmt + Chwhere Ot = Xt and the block-diagonal entries of U are zero because we do not use Xd to estimateXd. We use multiple deeply stacked FC layers using linear activation functions.
Figure 4: Active Sensing: AUC vs Cost for Different Solutions with MIMIC-III dataset (Lab testcost = 5× Vital sign cost)random sampling, and two benchmarks based on the method of Che et al. (2016) for prediction withmissing data (sampling either using the method of Deep Sensing or randomly).
Figure 5: The operation of Deep Sensing in runtime17Published as a conference paper at ICLR 2018Pseudo-codes of Deep SensingAlgorithm 1 Deep Sensing - Training StageInput: Dataset D = {(Xn, Yn, Sn)}nN=1, multiple representations U = {u1 , u2, ..., uR}For each representation ur ∈ UInitialization:Φι, Ψι, Ωι, Γι J Xavier Initialization, Di JDΦr J— Φr-1, Ψr J— Ψr-1, Ωr J— Ωr-1, Γr J— Γr-1 and Dr J— Dr -1Interpolation and Imputation: Using M-RNN and FC layersΦr,亚；=argminφ,ψ L({φ({xT, ψ({xd,mdr,δd}τ =1：T),md}d=i：D),xT})Error Estimation: Using M-RNN and FC layersΓr = argminr L({r({mT}d=i：D, {δτd,mτd}τ=1:T , eτd})Prediction: Using RNN with GRUΩr = argminΩ L({Ω({xd,xT,δd}τ =i：T,d=i：D),%})Adaptive Sampling:For all τ , s, nIf yU,l (n) — ^d,l (n) < UrDr J Dr - xτs (n)
