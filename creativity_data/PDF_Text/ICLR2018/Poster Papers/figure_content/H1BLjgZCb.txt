Figure 1:	Adversarial examples. Given an instance (a), existing FGSM approach (Goodfellow et al.,2015) adds small perturbations in (b), that change the prediction of the model (to be “2”, in this case).
Figure 2:	Training Architecture with a GAN and an Inverter. Loss of the inverter combinesreconstruction error of x with divergence between Gaussian distribution Z and IY (Gθ (Z)).
Figure 3:	Natural Adversary Generation. Given an instance X, our framework generates naturaladversaries by perturbing inverted z0 and decoding perturbations Z via Gθ to query the classifier f.
Figure 4: Classifier accuracy and average ∆z of their adversaries. In (a) and (b) we vary thenumber of neurons and dropout rate, respectively. In (c) we present the correlation between accuracyand average ∆z for 80 different classifiers. (d) shows adversaries for an input image, against a set ofclassifiers with a single hidden layer, but varying number of neurons.
Figure 5: Illustration with synthetic data. With training data that lies on a complex manifold (a),the inverter maps input to compact gaussian latent z0 = Iγ (x) in (b), while the generator reconstructsthe data via Gθ (Iγ (x)) in (c). Given f as a binary classifier with decision boundary as the horizontalline in (c), for an input x, our approach returns x* on the left as natural adversary that lies on themanifold, while existing approaches may find x* on the right as the adversary, which is the nearestbut impossible.
Figure 6: Model Architecture for Text. Our model incorporates in the adversarially regularizedautoencoder (ARAE) (Zhao et al., 2017) for encoding discrete X into continuous code C and decodingcontinuous c into discrete X when generating samples.
