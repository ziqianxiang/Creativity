Figure 1: The APe-X architecture in a nutshell: multiple actors, each With its own instance of the environment,generate experience, add it to a shared experience replay memory, and compute initial priorities for the data.
Figure 2: Left: Atari results aggregated across 57 games, evaluated from random no-op starts. Right: Ataritraining curves for selected games, against baselines. Blue: Ape-X DQN with 360 actors; Orange: A3C;Purple: Rainbow; Green: DQN. See appendix for longer runs over all games.
Figure 3: Performance of Ape-X DPG on four continuous control tasks, as a function of Wall clock time.
Figure 4:	Scaling the number of actors. Performance consistently improves as we scale the number of actorsfrom 8 to 256, note that the number of learning updates performed does not depend on the number of actors.
Figure 5:	Varying the capacity of the replay. Agents with larger replay memories perform better on mostgames. Each curve corresponds to a single run, smoothed over 20 points. The curve for Wizard Of Wor withreplay size 250K is incomplete because training diverged; we did not observe this with the other replay sizes.
Figure 6:	Testing whether improved performance iscaused by recency alone: n denotes the number ofactors, k the number of times each transition is repli-cated in the replay. The data in the run with n = 32,k = 8 is therefore as recent as the data in the run withn = 256, k = 1, but performance is not as good.
Figure 7:	Varying the data-generating policies: Red:fixed set of 6 values for . Blue: full range of val-ues for . In both cases, the curve plotted is froma separate actor that does not add data to the replaymemory, and which follows an -greedy policy with= 0.00164.
Figure 8:	Continuous control domains considered for benchmarking Ape-X DPG: (a) Humanoid,and (b) Manipulator. All tasks simulated in the MuJoCo physics simulator (Todorov et al. (2012)).
Figure 9:	Training curves for 57 Atari games (performance against wall clock time). Green: DQN baseline.
Figure 10:	Training curves for 57 Atari games (performance against environment frames). Only the firstbillion frames are shown, corresponding to 5-6 hours of training for Ape-X. Green: DQN baseline. Purple:Rainbow baseline. Blue: ApeX-DQN with 360 actors, 1 replay server and 1 Tesla P100 GPU learner.
Figure 11: Speed of data generation scales linearly with the number of actors.
Figure 12: Training curves showing performance against wall clock time for various numbers of actors ona selection of Atari games. Blue: prioritized replay, with learning rate 0.00025 / 4. Red: uniform replay,with learning rate 0.00025. For both prioritized and uniform, we tried both of these learning rates and selectedthe best. Both variants benefit from larger numbers of actors, but prioritized can better take advantage of theincreased amount of data. In the 256-actor run, prioritized is equal or better in 7 of 9 games.
