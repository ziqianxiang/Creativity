Figure 1: Comparison of embedding computations between the conventional approach (a) and com-positional coding approach (b) for constructing embedding vectorsan integer number in [1, K]. Ideally, similar words should have similar codes. For example, we maydesire Cdog = (3, 2, 4, 1) and Cdogs = (3, 2, 4, 2). Once we have obtained such compact codes forall words in the vocabulary, we use embedding vectors to represent the codes rather than the uniquewords. More specifically, we create M codebooks E1, E2, ..., EM, each containing K codewordvectors. The embedding of a word is computed by summing up the codewords corresponding to allthe components in the code asME(Cw) = XEi(Cwi ),	(1)i=1(2)where Ei(Cwi ) is the Cwi -th codeword in the codebook Ei. In this way, the number of vectors in theembedding matrix will be M Ã— K, which is usually much smaller than the vocabulary size. Fig. 1gives an intuitive comparison between the compositional approach and the conventional approach(assigning unique IDs). The codes of all the words can be stored in an integer matrix, denoted by C.
Figure 2: The network architecture for learning compositional compact codes. The Gumbel-softmaxcomputation is marked with dashed lines.
Figure 3: Visualization of code balance for different coding scheme. Each cell in the heat map showsthe count of words containing a specific subcode. The results show that any codeword is assigned tomore than 1000 words without wasting.
