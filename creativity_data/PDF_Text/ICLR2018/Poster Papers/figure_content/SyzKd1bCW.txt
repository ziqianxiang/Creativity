Figure 1: Left: Training curves comparing different gradient estimators on a toy problem:L(θ) = Ep(b∣θ)[(b - 0.499)2] Right: Log-variance of each estimator's gradient.
Figure 2: Histograms of samples from the gradient estimators that create LAX. Samples generatedfrom our one-layer VAE experiments (Section 6.2).
Figure 3: The optimal relaxation for a toy lossfunction, using different gradient estimators. Be-cause REBAR uses the concrete relaxation of f,which happens to be implemented as a quadraticfunction, the optimal relaxation is constrained tobe a warped quadratic. In contrast, RELAX canchoose a free-form relaxation.
Figure 4: Training curves for the VAE Experiments with the one-layer linear model. The horizontaldashed line indicates the lowest validation error obtained by REBAR.
Figure 5: Top row: Reward curves. Bottom row: Log-variance of policy gradients. In each curve, thecenter line indicates the mean reward over 5 random seeds. The opaque bars in the top row indicatethe 25th and 75th percentiles. The opaque bars in the bottom row indicate 1 standard deviation. Sincethe gradient estimator is defined at the end of each episode, we display log-variance per episode.
Figure 6: Training curves for the VAE Experiments with the two-layer linear model. The horizontaldashed line indicates the lowest validation error obtained by REBAR.
Figure 7: Training curves for the VAE Experiments with the one-layer nonlinear model. Thehorizontal dashed line indicates the lowest validation error obtained by REBAR.
