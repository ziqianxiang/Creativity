Figure 1: Architecture of the Deep Wasserstein Embedding: two samples are drawn from the datadistribution and set as input of the same network (Ï†) that computes the embedding. The embeddingis learnt such that the squared Euclidean distance in the embedding mimics the Wasserstein distance.
Figure 2: Prediction performance on the MNIST dataset. (Figure) The test performance are as follows:MSE=0.41, Relative MSE=0.003 and Correlation=0.995. (Table) Computational performance of W22and DWE given as average number of W22 computation per seconds for different configurations.
Figure 3: Barycenter estimation on each class of the MNIST dataset for squared Euclidean distance(L2) and Deep Wasserstein Embedding (DWE).
Figure 4: Principal Geodesic Analysis for classes 0,1 and 4 from the MNIST dataset for squaredEuclidean distance (L2) and Deep Wasserstein Embedding (DWE). For each class and method weshow the variation from the barycenter along one of the first 3 principal modes of variation.
Figure 5: Comparison of the interpolation with L2 Euclidean distance (top), LP Wasserstein interpo-lation (top middle) regularized Wasserstein Barycenter (down middle) and DWE (down).
Figure 6: W22 validation MSE along the number of epoChs for the MNIST dataset (DWE).
Figure 7:	Examples drawn from the 3 Google Doodle datasets (left) cat dataset, (center) Crab dataset(right) Face dataset.
Figure 8:	Interpolation between four samples of each datasets using DWE. (left) cat dataset, (center)Crab dataset (right) Face dataset.
Figure 9:	Performance of DWE on the Cat (left) Crab (center) and Face (right) doodle datsets.
Figure 10:	Nearest neighbor walk along the 3 datasets when using L2 or DWE for specifying theneighborhood. (up) Cat dataset, (middle) Crab dataset (down) Face dataset.
