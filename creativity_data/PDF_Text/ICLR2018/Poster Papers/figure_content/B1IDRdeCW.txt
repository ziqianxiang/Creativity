Figure 1: Review of the Courbariaux et al. (2016) BNN Training Algorithm: (a) A binary neuralnetwork is composed of binary convolution transformers (dashed green box). Each oval correspondsto a tensor and the derivative of the cost with respect to that tensor. Rectangles correspond totransformers that specify forward and backward propagation functions. Associated with each binaryweight, wb , is a continuous weight, wc, that is used to accumulate gradients. k denotes the kthlayer of the network. (b) Each binarize transformer has a forward function and a backward function.
Figure 2: Binarization of High-Dimensional Vectors Approximately Preserves their Direction inTheory and Practice: (a) Distribution of angles between two random vectors (blue), and between avector and its binarized version (red), for a rotationally invariant distribution of dimension d. Thered distribution is peaked near the d → ∞ limit of arccos √2∕∏ ≈ 37° (SL Sec. 1). While 37°may seem like a large angle, that angle is small as compared to the angle between two randomvectors in moderately high dimensions (i.e. the blue and red curves are well-separated). (b) Angledistribution between continuous and binary weight vectors by layer for a binary CNN trained onCIFAR-10. For the higher layers, there is a close correspondence to the theory. There is a small, butsystematic deviation towards large angles (SI, Fig. 6). d is the dimension of the filters at each layer.
Figure 3: Binarization Preserves Dot Products: Each subplot shows a 2D histogram of the dotproducts between the binarized weights and the activations (horizontal axis) and the dot productsbetween the continuous weights and the activations (vertical axis) for different layers of a networktrained on CIFAR10. Surprisingly, the dot products are highly correlated (r is the Pearson correlationcoefficient). Thus replacing wb with wc changes the overall constant in front of the dot products,while still preserving whether the dot product is zero or not zero. This overall constant is divided outby the subsequent batch norm layer. The shaded quadrants correspond to dot products where the signchanges when replacing the binary weights with the continuous weights. Notice that for all but thefirst layer, a very small fraction of the dot products lie in these off diagonal quadrants. The top leftfigure (labeled as Layer 1) corresponds to the input and the first convolution. Note that the correlationis weaker in the first layer.
Figure 4: Left: Random Rotation Improves Angle Preservation for a Non-Isotropic Gaussian.
Figure 5: Activation Binarization Preserves Dot Products: Left: Each panel shows a 2D histogram ofthe dot products between the binarized weights and binarized activations (vertical axis) and post-batchnorm (but pre-activation binarization) activations (horizontal axis). The binarization transformer doeslittle to corrupt the dot products between weights and activations. Right: Dot products between thebinary weights and binary activations (horizontal axis) compared against the dot products between thecontinuous weights and continuous activations (vertical axis). The dot products are not significantlyimpacted by removing binarization.
Figure 6: Angle distribution between continuous and binary weight vectors by layer for a binaryCNN trained on CIFAR-10 (same plot as in Fig. 2b except zoomed in). Notice that there is a smallbut systematic deviation towards larger angles relative to the theoretical expectation (vertical dottedline). As the dimension of the vectors in the layer goes up, the distribution gets sharper. The theorypredicts that the standard deviation of these distributions scales as 1 / √d. This relationship is shownto approximately hold in Fig. 2c.
Figure 7: Ternarization of High-Dimensional Vectors Preserves their Direction in Theory andPractice: (a) Histogram of the components of the continuous weights at each layer for a ternaryCNN trained on CIFAR-10. The distribution is approximately Gaussian for all but the first layer.
