Figure 1: Sampling from Attention PixelCNN. Support images are overlaid in red to indicate theattention weights. The support sets can be viewed as small training sets, illustrating the connectionbetween sample-conditional density estimation and learning to learn distributions.
Figure 2: The PixelCNN attention mechanism.
Figure 4: Typical Omniglot samples from PixelCNN, Attention PixelCNN, and Meta PixelCNN.
Figure 5: Stanford online products. Samples from Attention PixelCNN tend to match textures andcolors from the support set, which is less apparent in samples from the non-attentive model.
Figure 6: Flipping 24Ã—24 images, comparing global-conditional, attention-conditional and gradient-conditional (i.e. MAML) PixelCNN.
Figure 7: Comparison to ConvDRAW in 4-shot learning.
