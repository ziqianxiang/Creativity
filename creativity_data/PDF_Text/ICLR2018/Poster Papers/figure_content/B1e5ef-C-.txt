Figure 1: Average F1 -score of 200 recoveredBoW vectors from SST (left) and IMDB (right)compared to dimension. Pretrained word embed-dings (SN trained on Amazon reviews) need halfthe dimensionality of normalized Rademachervectors to achieve near-perfect recovery. Notethat IMDB documents are on average more thanten times longer than SST documents.
Figure 2: F1 -score of 1000 recovered BoWscompared to number of unique words. Real doc-uments (left) are drawn from the SST and IMDBcorpora; random signals (right) are created bypicking words at random. For d = 200, pre-trained embeddings are better than Rademachervectors as sensing vectors for natural languageBoW but are worse for random sparse signals.
Figure 3: Proportion of 500 randomly sampled documents from SST (top) and IMDB (bottom) thatare perfectly recovered from linear measurements.
Figure 4: Time needed to initializemodel, construct document representa-tions, and train a linear classifier on a16-core compute node.
Figure 5: IMDB performance of unigram (left) and bigram(right) DisC embeddings compared to the original dimension.
Figure 6: IMDB performance com-pared to training sample size.
Figure 7: Efficiency of pretrained embeddings as sensing vectors at d = 300 dimensions, measuredvia the F1 -score of the original BoW. 200 documents from each dataset were compressed andrecovered in this experiment. For fairness, the number of words V is the same for all embeddingsso all documents are required to be subsets of the vocabulary of all corpora. word2vec embeddingstrained on Google News and GloVe vectors trained on Common Crawl were obtained from publicrepositories (Mikolov et al., 2013; Pennington et al., 2014) while Amazon and Wikipedia embeddingswere trained for 100 iterations using a symmetric window of size 10, amin count of 100, for SN/GloVea cooccurrence cutoff of 1000, and for word2vec a down-sampling frequency cutoff of 10-5 and anegative example setting of 3. 300-dimensional normalized random vectors are used as a baseline.
