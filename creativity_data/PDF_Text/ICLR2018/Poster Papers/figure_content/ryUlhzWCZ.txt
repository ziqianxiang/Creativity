Figure 1: ReWard versus batch iterations of THOR With different k and TRPO-GAE (blue) forMountain car, Sparse ReWard (SR) CartPole, and Acrobot With different horizon. Average reWardsacross 25 runs are shoWn in solid lines and averages + std are shoWn in dotted lines.
Figure 2: Reward versus batch iterations of THOR with different k and TRPO-GAE (blue) forSparse Reward (SR) Inverted Pendulum, Sparse Reward Inverted-Double Pendulum, Swimmer andHopper. Average rewards across 25 runs are shown in solid lines and averages + std are shown indotted lines.
Figure 3: The special MDP we constructed for theorem 3.1Proof. We prove the theorem by constructing a special MDP shown in Fig 3, where H = âˆž. TheMDP has deterministic transition, 2H + 2 states, and each state has two actions a1 and a2 as shownin Fig. 3. Every episode starts at state s0 . For state si (states on the top line), we have c(si) = 0 andfor state s0i (states at the bottom line) we have c(si) = 1.
