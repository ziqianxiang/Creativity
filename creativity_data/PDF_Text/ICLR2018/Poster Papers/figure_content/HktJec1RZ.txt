Figure 1: (a) The overall architecture of NPMT. (b) An illustration of using NPMT in German-English trans-lation. Ideally, phrases in the source sentence (German) are first reordered. Given the new order, phrases canbe translated one by one to the target phrases. These translated phrases then compose the target sentence (En-glish). Phrase boundaries in the target language are not predefined, but automatically discovered by the model.
Figure 2: Courtesy to Wang et al. (2017a). Symbol $ indicates the end ofa segment. Given a sequence of inputsx1, . . . , x5, which is from the outputs from the bi-directional RNN of Figure 1(a), SWAN emits one particularsegmentation of yi：3 = n(ai：5), where {aι = {yι, $},a2 = {$},a3 = {$},a4 = {y2,y3, $},a5 = {$}}.
Figure 3: (a) Example of a local reordering layer of window size 5 (τ = 2) to compute ht. Here σt-2+i ,σ(wiT [et-2; et-1; et; et+1; et+2]), i = 0, . . . , 4, are the gates that decides how much information ht shouldaccept from those elements from this input window. Note that all information available in this input windowhelps decides each gate. (b) An illustration of the reordering layer that swaps information between e2 and e3and contributes to h3 and h2, respectively.
Figure 4: An example of NPMT greedy decoding output for German-English translation. The example corre-sponds to the first example of Table 2. Note that for illustrating the input and output segments, we do not takeinto account of the behavior of the reordering layer and bi-directional RNN—the index mappings from sourceto target assumes monotonic alignments so some of them might be inaccurate.
Figure 5: Visualizing reordering gates in the NPMT English-German translation model.
