Figure 1: Left: Tree MDP, with k leaves. Tree: log1-α E (s, start) as function of visit cycles, fordifferent trees of k leaves (color coded). For each k, a cycle consists of visiting all leaves, hence kvisits of the start action. log1-α E behaves as a generalized counter, where each cycle contributesapproximately one generalized visit.
Figure 3: MSE between Q and Q* on optimal policy per episode.
Figure 4: MSE between Q and Q* on optimal policy per episode. Convergence measure of allagents, long bridge environment (k = 15). E-values agents are the first to converge, suggestingtheir superior learning abilities.
Figure 5: Convergence of Q to Q* for individual state-action pairs (each denoted by a differentcolor), with respect to counters (left) and generalized counters (right). Results obtained from E-Value LLL Softmax on the short bridge environment (k = 5). Triangle markers indicate pairs with”east” actions, which constitute the optimal policy of crossing the bridge. Circle markers indicatestate-action pairs that are not part of the optimal policy. Generalized counters are a useful measureof the amount of missing knowledge.
Figure 6: Results on Freeway game. All agents used -greedy action-selection rule without explo-ration bonus (DQN, blue), with a bonus term based on density model counters (Density, orange)added to the reward, or with bonus term based on E-values (black).
Figure 7: Normalized MSE between Q and Q* on optimal policy per episode. Convergence ofE-value LLL and Delayed Q-Learning on, normalized bridge environment (k = 15). MSE wasnoramlized for each agent to enable comparison.
Figure 8: Empirical visits histogram (left) and learned CE (right) after training, γE = 0.
Figure 9: Empirical visits histogram (left) and learned CE (right) in the first 10 training episodes,γE = 0.
Figure 10: Difference in empirical visits histogram (left) and learned CE (right) in the last 10 train-ing episodes, γE = 0.
Figure 11: Difference in empirical visits histogram (left) and learned CE (right) in the last 10 train-ing episodes, γE = 0.99. Note that the results are based on a different simulation than those inFigure 10.
Figure 12: Histogram of correlation coefficients between empirical visit counters and CE throughouttraining, per state (γE = 0).
Figure 13: Probability of reaching goal on MountainCar (computed by averaging over 50 simula-tions of each agent), as a function of training episodes. While Softmax exploration fails to solve theproblem within 1000 episodes, LLL E-values agents with generalized counters (γE > 0) quicklyreach high success rates.
