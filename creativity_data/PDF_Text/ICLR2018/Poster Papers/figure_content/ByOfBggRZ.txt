Figure 1: An illustration of an interaction withina fully connected feedforward neural network,where the box contains later layers in the network.
Figure 2: Standard feedforward neu-ral network for interaction detection,with optional univariate networks6Published as a conference paper at ICLR 2018Table 1: Test suite of data-generating functionsF1(x)	∏x1x2 √2X3 - sin-l(x4) + lθg(x3 + X5)- x9-∖ ∕x7 - X2X7 	X10 V X8	F2(x)	∏x1x2p2∣x3∣ - sin-1 (0.5x4)+log(∣X3 + X5∣ + 1) + -r^x9~P/ x7 ∣ -胃2胃7 	 1 + ∣χ10∣ V 1 + ∣χ81	F3(x)	exp ∣X1 — X2∣ + ∣X2X3∣ — X3∣ 4 + l0g(x4 + X2 + X7 + x2) + X9 + 1+~2~F4(x)	exp ∣x1 - x2∣ + ∣x2x3∣ - x3lx4l + (x1x4)2 + log(x4 + X2 + x2 + x8) + x9 + - χ2F5(x)	1 + χ2 + χ2 + χ2 + PeXP(x4 + X5) + ∣X6 + X7∣ + X8X9X10F6(x)	exp (∣X1X2∣ + 1) - exp(∣x3 + X4∣ + 1) + cos(x5 + X6 - X8) + yx2 + X2 + X10F7(x)	,	,.	,	1	( XI λ5 以 (arctan(XI) + arctan(X2)) +maχ(X3X4 + χ6,0) - 1 + (χ4χ5χ6χ7χ8)2 + (jτ^ ) + X XiF8(x)	X1X2 + 2x3+x5+x6 + 2x3+x4+x5+x7 + sin(X7 sin(X8 + X9)) + arccos(0.9X1o)F9(x)	tanh(X1X2 + X3X4) V∖X5∖ + exp(X5 + X6) + log ((X6X7X8)2 + 1)+ X9X10 +	——T 	 1 + ∖x10∖	F10(x)	sinh (x1 + X2) + arccos (tanh(X3 + X5 + Xγ)) + cos(X4 + X5) + Sec(X7X9)pKcK(x) =	gi(xi) +	gi0(xI),i=1	i=1where gi(∙) captures the main effects, gi(∙) captures the interactions, and both gi and gi are small
Figure 3: A comparison of aver-aging functions by the total num-ber of correct interactions ranked be-fore any false positives, evaluated onthe test suite (Table 1) over 10 tri-als. x-axis labels are maximum, rootmean square, arithmetic mean, ge-ometric mean, harmonic mean, andminimum.
Figure 4:	Heat maps of pairwise interaction strengths proposed by our NID framework on MLP-Mfor datasets generated by functions F1-F10 (Table 1). Cross-marks indicate ground truth interactions.
Figure 5:	Heat maps of pairwise interaction strengths proposed by our NID framework on MLP-Mfor real-world datasets.
Figure 6: MLP-Cutoff error with added top-ranked interactions (along x-axis) of F1-F10 (Table 1),where the interaction rankings were generated by the NID framework applied to MLP-M. Red cross-marks indicate ground truth interactions, and 0 denotes MLP-Cutoff without any interactions. SUb-set interactions become redundant when their true superset interactions are found.
Figure 7: MLP-Cutof error with added top-ranked interactions (along x-axis) of real-world datasets(Table 1), where the interaction rankings were generated by the NID framework on MLP-M. 0denotes MLP-Cutoff without any interactions.
Figure 8: Comparisons between AG and NID in higher-order interaction detection. (a) Comparisonof top-ranked recall at different noise levels on the synthetic test suite (Table 1), (b) comparison ofruntimes, where NID runtime with and without cutoff are both measured. NID detects interactionswith top-rank recall close to the state-of-the-art AG while running orders of magnitude times faster.
Figure 9: ROC curves of NID-MLP-M corresponding to Table 2seH əA--Sod BnJ1-0.0 0.2 0.4 0.6 0.8 1.0False Positive Rate0.0 0.2 0.4 0.6 0.8 1.0False Positive RateseH əA--Sod BnJ1-0.0 0.2 0.4 0.6 0.8 1.0False Positive RateF8	F9	F10F LARGE p EXPERIMENTWe evaluate our approach in a large p setting with pair-wise interactions using the same synthetic function as in Pu-rushotham et al. (2014). Specifically, we generate a datasetofn samples andp features { X(i), y(i) } using the functiony(i) = β>X(i) + X(i)>WX(i) +(i),where X(i) ∈ Rp is the ith instance of the design matrixX ∈ Rp×n, y(i) ∈ R is the ith instance of the responsevariable y ∈ Rn×1 , W ∈ Rp×p contains the weights ofpairwise interactions, β ∈ Rp contains the weights of main
Figure 10: ROC curve for large p ex-In this experiment, we set p = 1000, n = 1e4, and K = 5. perimentX is normally distributed with mean 0 and variance 1, and(i) is normally distributed with mean 0 and variance 0.1.
Figure 11: Response plots of an MLP-M’s univariate networks corresponding to variables x8, x9,and x10. The MLP-M was trained on data generated from synthetic function F6 (Table 2). Note thatthe plots are subject to different levels of bias from the MLP-M’s main multivariate network.
Figure 12: A heatmap showing relative housing prices in California based on longitude and latitude.
