Figure 1: Our model disentangles environment-specific information (e.g. transition dynamics) andtask-specific knowledge (e.g. task rewards) for training efficiency and interpretability.
Figure 3: Lava WorldPath function The set of states in lava world is definite, we directly userandomly generated pairs of starting state and final state to train the path function. During this trainingphase, the agent understands how to move as well as how to go from one room to another.
Figure 4: Experiments showing the performance of the Path function.
Figure 5: The rate of successfully reachingthe final position in game Reachability. Theuniversal agent model is trained through back-propagation.
Figure 6: Different feature spaces affect the learning of τ functionKnowledge transfer via Path function We study two types of knowledge transfer: knowledgefrom exploration and knowledge from tasks. We implemented Pathak et al. (2017) as a no-reward RLexplorer in unknown environments. The experience of the agents is collected for the Path training.
Figure 7: Knowledge transfer via the Path function.
Figure 8: Comparison between A3C, A3C with pre-trained model on the original task, and UA withpre-trained Path function. The translucent regions indicate the standard deviations of multiple runs.
Figure 9: Comparison between A3C, A3C with pre-trained model on one of source tasks, and UAwith pre-trained Path function from multiple source tasks. The translucent regions indicate thestandard deviations of multiple runs. All environments are based on the latest versions of Gym Atari.
Figure 10: Visualization of the goal state composed by universal agentGiven the feature space of all states, we can visualize the goal state generated by τ function by link itto the nearest real state in feature space. The shadow agent in Figure 10 shows the correspondinggoal position when the agent is at its real position. The universal agent model can thus dramaticallyincrease the interpretability of the agent’s play in certain games.
Figure 11: The probability of successfully reaching the final position during testing time in gameReachability by pure imitation learning.
Figure 12: Lunar LanderThe Lunar Lander is a typical task where reward function is hard to design, especially for specialtasks. To show the generalization ability, we design three new tasks for imitation learning: hover inthe center, fly away from left-middle, and swing near the center. We provide the agent with a small setof demonstration plays (1 for all tasks) generated by heuristic players. The imitation learning baselineis a CNN model trained by minimizing the KL-divergence between policy π(a0∣s) with one-hotpolicy a for all demonstration pairs (s, a). Shown in Figure 13, universal agent with pre-trainedPath function outperforms pure imitation learning model in generalization and robustness.
Figure 13: Learn from only demonstration in Lunar Lander. “Copy” represents agents which directlycopy demonstration,s actions as baseline.
Figure 14: Comparison between A3C, jointly trained UA, and UA with pre-trained Path function.
Figure 15: Knowledge transfer from exploration in Atari gamesIn Figure 15, with Path function learned during exploration, the agents showed faster learning speedon the given task initially. However, their performance (score) is typically worse than the A3C model.
Figure 16: Visualizations of the saliency maps for the Path function w.r.t. both inputs in the gameRiverraid Scene #1. Columns (left-to-right): input state, the saliency map, the Jacobian. First row:current state. Second row: next state (with certain distances).
Figure 17: Visualizations of the saliency maps for the Path function w.r.t. both inputs in the gameRiverraid Scene #2. Columns (left-to-right): input state, the saliency map, the Jacobian. First row:current state. Second row: next state (with certain distances).
Figure 18: Visualizations of the saliency maps for the Path function w.r.t. both inputs in the gameAlien Scene #1. Columns (left-to-right): input state, the saliency map, the Jacobian. First row: currentstate. Second row: next state (with certain distances).
Figure 19: Visualizations of the saliency maps for the Path function w.r.t. both inputs in the gameAlien Scene #2. Columns (left-to-right): input state, the saliency map, the Jacobian. First row: currentstate. Second row: next state (with certain distances).
