Figure 1: Quantization of LeNet-5 model trained on MNIST dataset(b) Convergence of quantization. Iterations refers toiterations of the loop in algorithm 2.
Figure 2: Distributions of parameter values and quantization widths for the CIFAR-10 network.
Figure 3: Trade-off between accuracy and error rate for benchmark datasets. The optimal pointsfor MNIST, CIFAR-10, and SVHN (highlighted red) achieve 64×, 35×, and 14× compression andcorrespond to 0.12%, -0.02%, and 0.7% decrease in accuracy, respectively. BNN-2048, BNN-24096,BNN-Theano are from (Courbariaux & Bengio, 2016); BinaryConnect is from (Courbariaux et al.,2015); and Deep Compression is from (Han et al., 2015)assuming the same quantization precision for all parameters in the network or all parameters in a layer.
