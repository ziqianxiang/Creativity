Figure 2: One optimization step of the parameter θ through Eq.(6) at point θ0. The posterior qr (x∖y) is amixture of pe。(x∖y = 0) (blue) and p®。(x∖y = 1) (red in the left panel) with the mixing weights inducedfrom qφo(y∖x). Minimizing the KLD drives pe(x∖y = 0) towards the respective mixture qr(x∖y = 0)(green), resulting in a new state where penew (x∖y = 0) = pg§new (x) (red in the right panel) gets closer topθo (x∖y = 1) = Pdata (x). Due to the asymmetry of KLD, pgθnew (x) missed the smaller mode of the mixtureqr (x∖y = 0) which is a mode of Pdata (x).
Figure 3: Symmetric view of generation and inference. There is little difference of the two processesin terms of formulation: with implicit distribution modeling, both processes only need to performsimulation through black-box neural transformations between the latent and visible spaces.
Figure 4: Left: Graphical model of InfoGAN. Right: Graphical model of Adversarial AUtoencoder(AAE), which is obtained by swapping data X and code Z in InfoGAN.
