Figure 1: Contraction of aSimple TN.
Figure 2: The Convolutional Arithmetic Circuit (ConvAC) network (Cohen et al., 2016b).
Figure 3: TN of the weights tensor Ayd ...dNwhich correspond to internal edges, leaving the external edges which correspond to d1 , ..., dN , yopen. As mentioned above, a TN is a weighted graph, and the weights marked next to each edgein this TN are equal by construction to the number of channels in the corresponding ConvAC layerl, denoted rl . This last equivalence will allow us to draw a direct relation between the number ofchannels in each layer of a deep ConvAC and the functions it is able to model. Accordingly, it willallow us to provide prescriptions regarding the layer widths for the design of a network that is meantto support known input correlations.
Figure 4: Applying ReLU networks with maxpooling to the global and local classification tasks.
Figure 5: A quick introduction to Tensor Networks. (a) Tensors in the TN are represented bynodes. The degree of the node corresponds to the order of the tensor represented by it. (b) A matrixmultiplying a vector in TN notation. The contracted indices are denoted by k and are summed upon.
Figure 6: The original Convolutional Arithmetic Circuits network as presented by Cohen et al.
Figure 8: The TN equivalent to the HT decomposition with a same channel pooling scheme cor-responding to the calculation of a deep ConvAC given in fig. 6 with N = 8. Further details inappendix D.2.
Figure 9: A recursive building block of the deep ConvAC TN. This scheme is the TN equivalentof two feature vectors in the l - 1 level being operated on with the conv→pool sequence of a deepConvAC shown in fig. 6, as is demonstrated below.
Figure 10: The components comprising a ‘ConvAC-weights TN’ φ that describes the weights tensorAy of a ConvAC, are an undirected graph G(V, E) and a bond dimensions function c. The bonddimension is specified next to each edge e ∈ E, and is given by the function c(e). As shown inappendix D.2, the bond dimension of the edges in each layer of this TN is equal to the number ofchannels in the corresponding layer in the ConvAC. The node set in the graph G(V, E) presentedabove decomposes to V = Vtn ∪ Vinputs, where Vtn (grey) are vertices which correspond to tensorsin the ConvAC TN and V inputs (blue) are degree 1 vertices which correspond to the N open edges inthe ConvAC TN. The vertices in V inputs are ‘virtual’ — were added for completeness, so G can beviewed as a legal graph. The open edge emanating from the top-most tensor (marked by a dashedline) is omitted from the graph, as it does not effect our analysis below — no flow between any twoinput groups can pass through it.
Figure 11: An example for the minimal multiplicative cut between A and B in a simple TN.
Figure 12: Accompanying illustrations for the proof of claim 1. (a) An example for an arbitrarilyinter-connected TN with N external indices, arranged such that the indices corresponding to groupA are on the left and indices corresponding to group B are on the right. The cut marked in pinkin the middle separates between A and B . (b) A contraction of all the internal indices to the leftand to the right of the cut results in two higher order tensors, each with external indices only fromgroup A or B, connected to each other by the edges of the cut. (c) Finally coalescing the indicesinto three groups, results in a matrix that on one hand is equal to the matricization w.r.t. (A, B) ofthe tensor represented by a TN in (a), and on the other is equal to a multiplication of matrices, therank of which is upper bounded by Q|iC=|1 cki, thus proving claim 1.
Figure 13: An example for the construction of the TN φ* out of the original TN φ which representsa deep ConvAC (section. D.2), in the case where all of the bond dimensions are powers of someinteger number p. ne edges with bond dimension P are placed in φ* in the position of each edge e inφ that has a bond dimension pne. This construction preserves the value of the minimal multiplicativecut between any two groups of external indices, (A, B) in φ (here chosen as the left-right partitionfor example) which correspond to (A*, B *) in φ*.
Figure 14: (a) An example for the tensor in φ* which corresponds to a δ tensor T(V ∈ Rp2 ×p ×pin φ. According to the construction of φ* presented in fig. 13, each edge is split into ne = 2 edgesof bond dimension p. The δ tensor structure in φ translates into this τ*v tensor holding a non-zeroentry only when the indices corresponding to all of the edges that are marked by the same color areequal to each other (eq. 27). Additionally, paths crossing this τδ*v tensor must only contain edgesof the same color in order to be called δ restricted edge disjoint paths. (b) There are L guaranteededge disjoint paths between V A* and V B*. In a flow directed from V A* to V B* (w.l.o.g), we arguethat one can choose these paths such that they have the same flow direction in all edges in φ* thatoriginate from a certain edge in φ.
Figure 15: An example for the effect that a δ tensor has on the upper bound on the rank of theʌzmatricization of the overall tensor represented by a TN. minC WC is defined in eq. 32 and shownin claim 3 to be the upper bound on the rank of the matricization of the conv-weights tensor of aConvAC represented by a TN. In this example, the upper bound is reduced upon changing a singlegeneral tensor in the TN shown in (a) (identical to fig. 11), whose entries are free to be equal anyvalue, with a δ tensor in the TN shown in (b) which obeys the constraint given in eq. 12. Thecentrality of the δ tensor in the TN compliant with a shallow ConvAC (that is depicted in fig. 7,is in effect the element which limits the expressiveness of the shallow network, as is discussed inappendix F.
