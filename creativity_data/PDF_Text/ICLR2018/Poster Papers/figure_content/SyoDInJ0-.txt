Figure 1: RL framework: after performing actiona(t), the agent perceives observation o(t + 1) andreceives reward r(t + 1).
Figure 2: Algorithm selection for reinforcement learning flow diagram3Published as a conference paper at ICLR 2018expectations. The outside expectation Eσ assumes the meta-algorithm σ fixed and averages overthe trajectory set generation and the corresponding algorithms policies. The inside expectation Eμassumes the policy fixed and averages over its possible trajectories in the stochastic environment.
Figure 3: The figures on the top plot the performance over time. The figures on the bottom show theESBAS selection ratios over the epochs.
Figure 5: ratios (averaged over 3000 runs) obtained with SSBAS on the gridworld task.
Figure 4: gridworldThe selection ratios displayed in 5 show that SSBAS selected the algorithm with the highest (0.5)learning rate in the first stages, enabling to propagate efficiently the reward signal through the visitedstates, then, over time preferentially chooses the algorithm with a learning rate of 0.01, which isless sensible to the reward noise, finally, SSBAS favours the algorithm with the finest learning rate(0.001). After 1 million episodes, SSBAS enables to save half a transition per episode on averageas compared to the best fixed learning rate value (0.1), and two transitions against the worst fixedlearning rate in the portfolio (0.001).
Figure 7: Comparative performance over time of SSBAS versus the algorithms in its portfolio onQ*bert (1 run).
Figure 6: Q*bert7 reveals that SSBAS experiences a slight delay keeping in touch with the best setting performanceduring the initial learning phase, but, surprisingly, finds a better policy than the single algorithmsin its portfolio and than the ones reported in the previous DQN articles. We observe that the largesetting is surprisingly by far the worst one on the Q*bert task, implying the difficulty to predict whichmodel is the most efficient for a new task. SSBAS allows to select online the best one.
