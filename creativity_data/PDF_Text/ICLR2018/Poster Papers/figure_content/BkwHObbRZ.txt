Figure 1: Data are generated by a network with ReLU activation without noise. The training modeluses the same architecture. Left: the estimated population risk doesn’t converge to zero. Right: theparameter error using the surrogate in equation (4.1).
Figure 2: Data are generated by a network with sigmoid activation without noise. The training modeluses the same architecture. Left: the estimated population risk doesn’t converge to zero. Right: theparameter error using the surrogate in equation (4.1).
Figure 3: The labels are generated from a network with ReLU activation. We learn with σ2 h2 + σ4 h4activation. Left: the test loss subtracted by the theoretical global minimum value. Right: the errorin parameter space measured by equation (4.1)0.8J 0.62IlJ⅛50.4⅛'iBCL 0.23×1051	2Iterations00verified empirically the conjecture that SGD would successfully recover B? using the activationfunctions Y(Z) = σ2h2(z) + σ4h4(z),11 even if the data were generated via a model with ReLU
Figure 4: Learning with objective function G(∙). Left: the test loss. Right: the error in parameterspace measured by equation (4.1)factor of 4 every 5000 number of iterations after the error plateaus at 10000 iterations. For the final5000 iterations, the step-size is less than 10-9, so we can be confident that the non-zero objectivevalue is not due to the variance of SGD. We see that none of the five runs of SGD converged to aglobal minimum. Figure 2 shows the result for sigmoid activation which is quantitatively similar.
