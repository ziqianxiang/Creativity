Figure 1: Model architecture of the proposed Skip RNN. (a) Complete Skip RNN architecture,where the computation graph at time step t is conditioned on ut . (b) Architecture when the stateis updated, i.e. ut = 1. (c) Architecture when the update step is skipped and the previous state iscopied, i.e. Ut = 0. (d) In practice, redundant computation is avoided by propagating ∆Ut betweentime steps when ut = 0.
Figure 2: Accuracy evolution during training on the validation set of MNIST. The Skip GRU exhibitslower variance and faster convergence than the baseline GRU. A similar behavior is observed forLSTM and Skip LSTM, but omitted for clarity. Shading shows maximum and minimum over 4 runs,while dark lines indicate the mean.
Figure 3: Sample usage examples for the Skip LSTM with λ = 10-4 on the test set of MNIST. Redpixels are used, whereas blue ones are skipped.
Figure 4: Accuracy evolution during the first 300 training epochs on the validation set of UCF-101(split 1). Skip LSTM models converge much faster than the baseline LSTM.
Figure 5: Sample usage examples for the Skip GRU with λ = 10-5 on the adding task. Red dotsindicate used samples, whereas blue ones are skipped.
Figure 6: Sample usage examples for the Skip LSTM with λ = 10-4 on the frequency discrimina-tion task with Ts = 0.5ms. Red dots indicate used samples, whereas blue ones are skipped. Thenetwork learns that using the first samples is enough to classify the frequency of the sine waves, incontrast to a uniform downsampling that may result in aliasing.
