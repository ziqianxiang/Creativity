Figure 1: The tasks in our experiments: (a) reaching target locations, (b) pushing a puck to a randomtarget, (c) training the cheetah to run at target velocities, (d) training an ant to run to a target positionor a target position and velocity, and (e) reaching target locations (real-world Sawyer robot).
Figure 2: The comparison of TDM with the baseline methods in model-free (DDPG), model-based,and goal-conditioned value functions (HER - Dense) on various tasks. All plots show the finaldistance to the goal versus 1000 environment steps (not rollouts). The bold line shows the meanacross 3 random seeds, and the shaded region show one standard deviation. Our method, whichuses model-free learning, is generally more sample-efficient than model-free alternatives includingDDPG and HER and improves upon the best model-based performance.
Figure 3: Ablation experiments for (a) scalar vs. vectorized TDMs on 7-DoF simulated reacher taskand (b) different Ï„max on pusher task. The vectorized variant performs substantially better, whilethe horizon effectively interpolates between model-based and model-free learning.
Figure 4: TDMs with different number of updates per step I on ant target position task. The max-imum distance was set to 5 rather than 6 for this experiment, so the numbers should be lower thanthe ones reported in the paper.
