Figure 1: Control Suite domains used for benchmarking. Top: Acrobot, Ball-in-cup, Cart-pole,Cheetah, Finger, Fish, Hopper. Bottom: Humanoid, Manipulator, Pendulum, Point-mass, Reacher,Swimmers (6 and 15 links), Walker.
Figure 2: Ablation study of the MPO algorithm and comparison to common baselines from the liter-ature on three domains from the control suite. We plot the median performance over 10 experimentswith different random seeds.
Figure 3: MPO on high-dimensional control problems (Parkour Walker2D and Humanoid walkingtask_name=run, domain_name=humanoid----agent=DDPG	---- agent=MPOagent=EPG + retrace + entropy agent=PPO(optimized)----agent=MPO (parametric)5.3	Discrete controlAs a proof of concept - showcasing the robustness of our algorithm and its hyperparameters - Weperformed an experiment on a subset of the games contained contained in the "Arcade LearningEnvironment" (ALE) where we used the same hyperparameter settings for the KL constraints as forthe continuous control experiments. The results of this experiment can be found in the Appendix.
Figure 4: Complete comparison of results for the control suite. We plot the median performanceover 10 random seeds together with 5 and 95 % quantiles (shaded area). Note that for DDPG weonly plot the median to avoid clutter in the plots. For DDPG and PPO final performance is markedby a star).
