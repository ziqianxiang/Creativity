Figure 1: L2HMC mixes faster than well-tuned HMC, and than A-NICE-MC, on a ColleCtion of toy distribu-tions.
Figure 2: Training and held-out log-likelihood for models trained with L2HMC, HMC, and the ELBO (VAE).
Figure 3: Demonstrations of the value ofa more expressive posterior approximation.
Figure 4: Diagram of our L2HMC-DGLM model. Nodes are functions of their parents. Roundnodes are deterministic, diamond nodes are stochastic and the doubly-circled node is observed.
Figure 5: L2HMC-DGLM decoder produces sharper mean activations.
