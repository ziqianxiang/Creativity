Figure 1: Illustration of explanation approaches. Function and signal approximators visualize theexplanation using the original color channels. The attribution is visualized as a heat map of pixel-wise contributions to the outputOForward ReLU(activated)Layer-wise Relevance Propagation (Bach et al., 2015, LRP) and the Deep Taylor Decomposition(Montavon et al., 2017, DTD), Integrated Gradients (Sundararajan et al., 2017) and SmoothGrad(Smilkov et al., 2017).
Figure 2: For linear models, i.e., a simple neural network, the weight vector does not explain thesignal it detects Haufe et al. (2014). The data x = yas + ad is color-coded w.r.t. the outputy = wTx. Only the signal s = yas contributes to y. The weight vector w does not agree with thesignal direction, since its primary objective is canceling the distractor. Therefore, rotations of thebasis vector ad of the distractor with constant signal s lead to rotations of the weight vector (right).
Figure 3: Evaluating ρ(S) for VGG-16 on ImageNet. Higher values are better. The gradient (Sw),linear estimator (Sa) and nonlinear estimator (Sa+- ) are compared. An estimator using randomdirections is the baseline. The network has 5 blocks with 2/3 convolutional layers and 1 max-poolinglayer each, followed by 3 dense layers.
Figure 4: Image degradation experiment on all 50.000 images in the ImageNet validation set. Theeffect on the classifier output is measured. A steeper decrease is better.
Figure 5: Top: signal. Bottom: attribution. For the trivial estimator Sx the original input is thesignal. This is not informative w.r.t. how the network operates.
Figure 6: Visualization of random images from ImageNet (validation set). In the leftmost showscolumn the ground truth, the predicted label and the classifier’s confidence. Methods should only becompared within their group. PatternNet, Guided Backprop, DeConvNet and the Gradient (saliencymap) are back-projections to input space with the original color channels. They are normalized usingXnOrm = 2mXχ∣x∣ + 1 to maximize contrast. LRP and PatternAttribution are heat maps showingpixel-wise contributions.
Figure 7: Visualization of random images from ImageNet (validation set). In the leftmost showscolumn the ground truth, the predicted label and the classifier’s confidence. Comparison betweenthe proposed methods PatternNet and PatternAttribution to the Prediction-Differences approach byZintgraf et al. (2017).
