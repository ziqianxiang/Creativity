Figure 1: Empirical cross-entropy loss (left) and Rademacherregularizer (right) as a function of retain rates. We observethat the empirical loss and Rademacher regularizer changeroughly in a monotonic way as a function of retain rateson training data. The experiments are evaluated on mnistdataset with a hidden layer of 128 ReLU units. We applydropout on the hidden layer only, and keep the retain ratesfixed throughout training. We optimize with the empiricalloss Loss(S, fl(∙; W, θ)), i.e., without any regularize] for200 epochs with minibatch fo 100. All Rademacher regu-larizers are computed after every epoch in post-hoc manner,under the settings of p = ∞, q = 1. We plot the samplesfrom last 20 epochs under each settings, with initial learningrate of 0.01, and decay by half every 40 epochs.
Figure 2: Changes in the true objectives in “stochastic” modeand their “deterministic” approximations against trainingepochs under different network architectures. The optimiza-tion objectives are reported on the training set of mnistdataset, with Rademacher regularizer. We use minibatch sizeof 100, initial learning rate of 0.01, and decay it by half every200 epochs. The network structures we evaluated includes 1hidden layer with 1024 units, 2 hidden layers with 800 unitseach, and 3 hidden layers with 1024 units each. The regular-izer weights are set to 1e-3, 1e-4 and 1e-5 respectively. Allneurons are ReLU units. Empirically, we find that optimizingthe “deterministic” objective leads to similar improvementson the true “stochastic” objective as in Eqn. (2).
Figure 3: Changes in retain rates With Rademacher regularization on MNIST dataset. Top-Left:5101520255	10	15	20	25changes in retain rate histograms for input layer (784 gray scale pixels) through training. The retainrates get diffused over time, and only a handful of pixels have retain rates close to 1. Top-Right:changes in retain rate histograms for hidden layer (1024 ReLU units) through training process.
Figure 4: Changes in retain rates With RademaCher regularization on CIFAR10 dataset. Top-Left:changes in retain rate histograms for input layer (32 × 32 × 3 RGB pixels) through training. Top-Middle: Changes in retain rate histograms for first Convolutional layer (32 × 15 × 15 units) throughtraining proCess. Top-Right: Changes in retain rate histograms for seCond Convolutional layer(64 × 7 × 7 units) through training proCess. Bottom-Left: Changes in retain rate histograms for fully-ConneCted layer (1024 ReLU units) through training proCess. Bottom-Middle: sample images fromcifar10 dataset. Bottom-Right: retain rates for Corresponding input pixels in both superposition andindividual RGB Channels upon model ConvergenCe. Unlike mnist datasets, there is no Clear patternfrom the retain rates out of these Channel pixels, sinCe they are all informational toWards prediCtion.
Figure 6: Histograms of the hidden layer dropout retain rates upon model convergence on 10 differentruns under different Rademacher p = ∞, q = 1 regularization settings. The neural network containsone hidden layer of 1024 ReLU units, and is trained on MNIST dataset with different regularizerweight of 1e-3 (Left) and 1e-4 (Right). We train the neural network for 200 epochs, with a minibatchsize of 100, initial learning rate of 0.01, and decay it by half every 40 epochs. The experiments arerun with same configurations and experimental settings, except different initializations on the networkweight coefficients.
