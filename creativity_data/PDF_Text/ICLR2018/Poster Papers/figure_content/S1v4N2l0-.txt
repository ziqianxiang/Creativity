Figure 1: Images rotated by random multiples of 90 degrees (e.g., 0, 90, 180, or 270 degrees). Thecore intuition of our self-supervised feature learning approach is that if someone is not aware of theconcepts of the objects depicted in the images, he cannot recognize the rotation that was applied tothem.
Figure 2: Illustration of the self-supervised task that we propose for semantic feature learning.
Figure 3: Attention maps generated by an AlexNet model trained (a) to recognize objects (super-vised), and (b) to recognize image rotations (self-supervised). In order to generate the attention mapof a conv. layer we first compute the feature maps of this layer, then we raise each feature activationon the power p, and finally we sum the activations at each location of the feature map. For the conv.
Figure 4: First layer filters learned by a AlexNet model trained on (a) the supervised object recogni-tion task and (b) the self-supervised task of recognizing rotated images. We observe that the filterslearned by the self-supervised task are mostly oriented edge filters on various frequencies and, re-markably, they seem to have more variety than those learned on the supervised task.
Figure 5: (a) Plot with the rotation prediction accuracy and object recognition accuracy as a functionof the training epochs used for solving the rotation prediction task. The red curve is the objectrecognition accuracy of a fully supervised model (a NIN model), which is independent from thetraining epochs on the rotation prediction task. The yellow curve is the object recognition accuracyof an object classifier trained on top of feature maps learned by a RotNet model at different snapshotsof the training procedure. (b) Accuracy as a function of the number of training examples per categoryin CIFAR-10. Ours semi-supervised is a NIN model that the first 2 conv. blocks are RotNet modelthat was trained in a self-supervised way on the entire training set of CIFAR-10 and the 3rd conv.
Figure 6: Attention maps of the Conv3 and Conv5 feature maps generated by an AlexNet modeltrained on the self-supervised task of recognizing image rotations. Here we present the attentionmaps generated for all the 4 rotated copies of an image.
