Figure 2:	Illustration of the decision boundary models considered in this paper. (a): For the flatdecision boundary model, the set {v : |r(x)T v| ≤ kr(x)k22} is illustrated (stripe). Note that for vʌ ʌ ʌ ʌtaken outside the stripe (i.e., in the grayed area), we have k(x + v) 6= k(x) or k(x - v) 6= k(x) inthe ρ neighborhood. (b): For the curved decision boundary model, the any vector v chosen in theʌgrayed area is classified differently from k(x).
Figure 3:	Link between robustness and curvature of the decision boundary. When the decisionboundary is positively curved (left), small universal perturbations are more likely to fool the classifier.
Figure 4: Left: Normal section U of the decision boundary, along the plane spanned by the normalvector r(x) and v. Right: Geometric interpretation of the assumption in Theorem 2. Theorem 2assumes that the decision boundary along normal sections (r(x), v) is locally (in a ρ neighborhood)located inside a disk of radius 1∕κ. Note the difference with respect to traditional notions of curvature,which express the curvature in terms of the osculating circle at x + r(x). The assumption we usehere is more “global”.
Figure 5: Visualization of normal cross-sections of the decision boundary, for CIFAR-10 (Left:LeNet, Right: ResNet-18). Top: Normal cross-sections along (r(x), v), where v is the universalperturbation computed using the algorithm in Moosavi-Dezfooli et al. (2017). Bottom: Normalcross-sections along (r(x), v), where v is a random vector uniformly sampled from the unit spherein Rd .
Figure 6: Visualization of normal cross-sections of the decision boundary, for ImageNet (Left:ResNet-152, and Right: CaffeNet) Top: Normal cross-sections along (r(x), v), where v is theuniversal perturbation computed using the algorithm in Moosavi-Dezfooli et al. (2017). Bottom:Normal cross-sections along (r(x), v), where v is a random vector uniformly sampled from the unitsphere in Rd .
Figure 7: (a) Average curvature KS, averaged over 1000 validation datapoints, as a function ofthe subspace dimension. (b) Fooling rate of universal perturbations (on an unseen validation set)computed using random perturbations in 1) Sc : the subspace of positively curved directions, and 2)Sf : the subspace collecting normal vectors r(x). The dotted line corresponds to the fooling rateusing the algorithm in Moosavi-Dezfooli et al. (2017). Sf corresponds to the largest singular vectorscorresponding to the matrix gathering the normal vectors r(x) in the training set (similar to theapproach in Moosavi-Dezfooli et al. (2017)).
Figure 8: Left column: Universal perturbation computed through random sampling from Sc . Secondcolumn to end: All images are (incorrectly) classified as “bubble”. The CaffeNet architecture is used.
Figure 10: Cosine of princi-pal angles between ScLeNet andScNiN. For comparison, cosineof angles between two randomsubspaces is also shown.
Figure 9: Diversity of universal perturbations randomly sampled from the subspace Sc . The normal-ized inner product between two perturbations is less than 0.1.
Figure 11: Same experiment as Fig. 7 (b) performed on VGG-16 architecture (CIFAR-10 dataset).
Figure 12: Same experiment as Fig. 7 (b) performed on ResNet-18 architecture (CIFAR-10 dataset).
Figure 13: Transferability of the subspace Sc across different networks. The first row shows normalcross sections along a fixed direction in Sc for VGG-16, with a subspace Sc computed with CaffeNet.
