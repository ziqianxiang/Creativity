Figure 1: Schematics of our approach. We train the agent in a multi-task setup, where the task id isgiven as a one-hot input to the embedding network (bottom-left). The embedding network generatesan embedding distribution that is sampled and concatenated with the current observation to serveas an input to the policy. After interaction with the environment, a segment of states is collectedand fed into the inference network (bottom-right). The inference network is trained to classify whatembedding vector the segment of states was generated from.
Figure 2: Bottom: resulting trajectories for different 3D embedding values with (right) and without(left) variational-inference-based regularization. The contours depict the reward gained by the agent.
Figure 3: Left, middle: resulting trajectories that were generated by different distributions used forthe skill-embedding space: multivariate Bernoulli (left), Gaussian (middle). The contours depictthe reward gained by the agent. Note that there is no reward outside the goal region. Right: KL-divergence between the embedding distributions produced by task 1 and the other three tasks. Task1 and 3 have different task ids but they are exactly the same tasks. Our method is able to discoverthat task 1 and 3 can be covered by the same embedding, which corresponds to the minimal KL-divergence between their embeddings.
Figure 4: Left: visualization of the sequence of manipulation tasks we consider. Top row: spring-wall, middle row: L-wall, bottom row: rail-push. The left two columns depict the two initial skillsthat are learned jointly, the rightmost column (in the left part of the figure) depicts the transfer taskthat should be solved using the previously acquired skills. Right: trajectories of the block in theplane as manipulated by the robot. The trajectories are produced by sampling a random embeddingvector trained with (red) and without (black) the inference network from the marginal distributionover the L-wall pre-training tasks every 50 steps and following the policy. Dots denote points atwhich the block was lifted.
Figure 5: Comparison of our method against different training strategies for our manipulation tasks:spring-wall, L-wall, and rail-push.
Figure 6: Final policy for all three tasks: spring-wall (top), L-wall (middle), rail-push (bottom).
