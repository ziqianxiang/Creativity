Figure 1: An overview of the QANet architecture (left) which has several Encoder Blocks. Weuse the same Encoder Block (right) throughout the model, only varying the number of convolutionallayers for each block. We use layernorm and residual connection between every layer in the EncoderBlock. We also share weights of the context and question encoder, and of the three output encoders.
Figure 2: An illustration of the data augmentation process with French as a pivotal language. k isthe beam width, which is the number of translations generated by the NMT system.
