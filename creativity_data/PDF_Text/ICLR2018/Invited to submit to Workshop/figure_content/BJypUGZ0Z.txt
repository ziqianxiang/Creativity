Figure 1: Example Learning Curves: Example learning curves from experiments considered in thispaper. Note the diversity in convergence times and overall learning curve shapes.
Figure 2: Predicted vs True Values of Final Performance: We show the shape of the predictivedistribution on three experiments: MetaQNN models, Deep Resnets, and LSTMs. Each ν-SVR modelis trained with 100 configurations with data from 25% of the learning curve. We predict validationset classification accuracy for MetaQNN and Deep ResNets, and perplexity for LSTMs.
Figure 3: Performance Prediction Results: We plot the performance of each method versus thepercent of learning curve observed. For BNN and ν-SVR, we sample 10 different training sets,plot the mean R2, and shade the corresponding standard error. We compare our method againstBNN (Klein et al., 2017), LCE (Domhan et al., 2015), and a “last seen value” heuristic (Li et al.,2017). Absent results for a model indicate that it did not achieve a positive R2 . The results forCuda-Convnet on the SVHN dataset are shown in Appendix Figure 11.
Figure 4: Simulated Speedup in MetaQNN Search Space: We compare the three variants of theearly stopping algorithm presented in Section 4 for both a ν-SVR and BLR SRM. Each SRM istrained using the first 100 learning curves, and each algorithm is tested on 10 independent orderingsof the model configurations. Triangles indicate an algorithm that successfully recovered the optimalmodel for more than half of the 10 orderings, and X’s indicate those that did not.
Figure 5: MetaQNN on CIFAR-10 withEarly Stopping: A full run of the MetaQNNalgorithm (Baker et al., 2017) on the CIFAR-10 dataset with early stopping. We use theν-SVR SRM with a probability threshold∆ = 0.99. Light blue bars indicate the aver-age model accuracy per decrease in , whichrepresents the shift to a more greedy policy.
Figure 6: Simulated Max Accuracy vs SGD Iterations for Hyperband: We show the trajectoriesof the maximum performance so far versus total computational resources used for 40 consecutiveHyperband runs with η = 4.0 and ∆ = 0.95. Small vertical lines indicate the point at which eachf-Hyperband run has searched over the same number of models as vanilla Hyperband. On Cifar-10,both BLR and ν-SVR f-Hyperbands outperform vanilla Hyperband, but on SVHN only ν-SVRf-Hyperbands do. Each triangle marks the completion of full Hyperband algorithm.
Figure 7: f-Hyperband with SVR Acquisition Function: We compare f-Hyperband with an SVRacquisition function to f-Hyperband with the uniform acquisition function (this is the standard versiondescribed in the main text) and to vanilla Hyperband. The results shown are inconclusive; it seemsthat there could be some benefit of incorporating the SVR acquisition function, but it isn’t significantenough to make a strong statement.
Figure 8: Example Error Distributions: Here we compare example error distributions betweentraining and validation from a ν-SVR SRM. From left to right we see plots for models trainedwith increasing amounts of the learning curve. The blue line shows the Gaussian probabilitydensity function estimated from the training set with LOOCV. The orange histogram shows the errordistribution of the held out validation set.
Figure 9: Error Log Likelihood: In orange we show the mean log likelihood of prediction errorsmeasured from validation sets being drawn from the Gaussian distribution parameterized by the meanand variance of the training error, again measured with LOOCV. For a baseline, we show the meanlog likelihood for 1,000,000 samples drawn from the same Gaussian. We calculate these metrics over10 data splits and report the mean and standard error.
Figure 10:	Time Series Weights: We show the trends for weights for time-series features, alongwith first- and second-order differences.
Figure 11:	Cuda Convnet SVHN Performance Prediction Results: We plot the performance ofeach method versus the percent of learning curve observed for the Cuda Convnet SVHN experiment.
Figure 12: Simulated Speedup on Hyperband vs Hyperband Iteration: We show the speedupusing the f-Hyperband algorithm over Hyperband on 40 consecutive runs with η = 4.0 and ∆ = 0.95.
Figure 13: Simulated f-Hyperband with Pretrained SRMs: Each f-Hyperband model is initializedwith a pretrained SRM, which simulates the case where one reinitializes an experiment. Every triangleindicates a full run of Hyperband, and small vertical lines indicate the point at which the f-Hyperbandruns have completed the same number of full Hyperband runs as shown for vanilla Hyperband. Top1 indicates only training the best model to ri iterations, and δ represents an offset off the thresholdused for early stopping (See Section 4).More aggressive f-Hyperband settings lead to suboptimalperformance, indicating that there is a limit to how much speedup f-Hyperband can achieve.
