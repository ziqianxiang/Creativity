Figure 1: Sample frames from Torcs (left) and GTA (right).
Figure 2: Performances on the Torcs game. The x-axis shows the training iterations. The y-axisshows the average total rewards. Solid lines are average values over 10 random seeds. Shadedregions correspond to one standard deviation. The left figure shows the performance for each agentwhen they only learn from demonstrations, while the right one shows the performance for eachagent when they interact with the environments after learning from demonstrations. Our methodconsistently outperforms other methods in both cases.
Figure 3: Performance on GTA (left) and performance on Torcs with human demonstrations (right)Figure 4: Left: Learning from imperfect data when the imperfectness is 30%. Our NAC methoddoes not clone suboptimal behaviors and thus outperforms DQfD and behavior cloning. Right:Learning from a limit amount of demonstrations. Even with only 30 minutes (10k transitions) ofexperience, our method could still learn a policy that is comparable with supervise learning method.
Figure 4: Left: Learning from imperfect data when the imperfectness is 30%. Our NAC methoddoes not clone suboptimal behaviors and thus outperforms DQfD and behavior cloning. Right:Learning from a limit amount of demonstrations. Even with only 30 minutes (10k transitions) ofexperience, our method could still learn a policy that is comparable with supervise learning method.
Figure 5: More results when introducing imperfect demonstrations. Left figure shows when thereare 50% imperfect actions and the right one shows the case for 80%. Our NAC method is highlyrobust to noisy demonstrations.
Figure 6: More results when varying the amount of demonstrations. The left and right figuresshow when there are 150k and 300k transitions respectively. Our NAC method achieves superiorperformance with a large amount of demonstrations and is comparable to supervise methods withsmaller amount of demonstrations.
