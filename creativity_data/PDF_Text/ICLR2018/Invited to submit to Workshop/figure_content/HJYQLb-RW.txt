Figure 1: A selection of different GAN behaviors. In all plots the true distribution was G** withμ* = (-0.5,0.5), and step size was taken to be 0.1. The solid lines represent the two coordinates ofb, and the dotted lines represent the discriminator intervals. In order: (a) first order dynamics withinitial conditions that converge to the true distribution. (b) First order dynamics with initial conditionsthat exhibit wild oscillation before mode collapse. (c) Optimal discriminator dynamics. (d) Firstorder dynamics that exhibit vanishing gradients and converge to the wrong distribution. Observe thatthe optimal discriminator dynamics converge, and then the discriminator varies wildly, because theobjective function is not differentiable at optimality. Despite this it remains roughly at optimalityfrom step to step.
Figure 2: Heatmap of success probability for random discriminator initialization for regular GANtraining, unrolled GAN training, and optimal discriminator dynamics.
Figure 3: Example of Discriminator Collapse. The initial configuration has μ* = {-2,2}, b ={-1, 2.5}, left discriminator [-1, 0.2], and right discriminator [-1, 2.5]. The (multiplicative) stepsize used to generate (c), (d), and (e) was 0.3.
Figure 4: Heatmap of success probability for random discriminator initialization for regular GANtraining, unrolled GAN training with dynamics induced by 12C.1 Why does discriminator collape still happen?It might be somewhat surprising that even with absolute values discriminator collapse occurs. Orig-inally the discriminator collapse occurred because if an interval was stuck in a negative region, italways subtracts from the value of the loss function, and so the discriminator is incentivized to makeit disappear. Now, since the value of the loss is always nonnegative, it is not so clear that this stillhappens.
