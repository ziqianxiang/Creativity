Figure 1: (a) A small graph G with 6 vertices and its adjacency matrix. (b) An alternative form G0 ofthe same graph, derived from G by renumbering the vertices by a permutation σ : {1, 2, . . . , 6} 7→{1, 2, . . . , 6}. The adjacency matrices of G and G0 are different, but topologically they represent thesame graph. Therefore, we expect the feature map φ to satisfy φ(G) = φ(G0).
Figure 2: (a) A composition scheme for an object G is a DAG in which the leaves correspond toatoms, the internal nodes correspond to sets of atoms, and the root corresponds to the entire object.
Figure 3: A minimal requirement for composition schemes is that they be invariant to permutation,i.e. that if the numbering of the atoms is changed by a permutation σ, then we must get an isomorphicDAG. Any node in the new DAG that corresponds to {e0i , . . . , e0i } must have a corrresponding nodein the old DAG corresponding to {eσ-1(i1) , . . . , eσ-1(ik)}.
Figure 4: In convolutional neural networks if the input image is translated by some amount (t1, t2),What used to fall in the receptive field of neuron n',j is moved to the receptive field of n'+t1,j+t2.
Figure 5: Top left: At level ` = 1 n3 aggregates information from {n4 , n5 } and n2 aggregatesinformation {n5, n6}. At ` = 2, n1 collects this summary information from n3 and n2. Bottom left:This graph is not isomorphic to the top one, but the activations ofn3 and n2 at` = 1 will be identical.
Figure 6: The activations of vertices in the receptive field Pv = {w1,w2,w3} of vertex V at level`-th are stacked into a 3rd order tensor and undergo a tensor product operation with the restrictedadjacency matrix, and then contracted in different ways. In this figure, we only consider singlechannel, each channel is represented by a 5th order tensor. In the general case of multi channels, theresulting tensor would have 6th order, but we contract on each channel separately.
