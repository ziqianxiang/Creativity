Figure 1: Challenges of disentangling. We disentangle the feature into two parts, one representingthe viewpoint, the other the car type. We use the features for attribute transfer. For all subfiguresthe viewpoint feature is taken from the leftmost column and the car type feature is taken from thetopmost row. (a) ideal solution: the viewpoint and the car type are transferred correctly. (b) shortcutproblem: the car type is not transferred. (c) reference ambiguity: compared to the others the viewpointorientation is flipped for the blue car.
Figure 2: Learning to disentangle factors of variation. The scheme above shows how the encoder(Enc), the decoder (Dec) and the discriminator (Dsc) are trained with input triplets. The componentswith the same name share weights.
Figure 3: Feature transfer on Shapenet. (a) synthesized images with LAE , where the top row showsimages from which the car type is taken. The second, third and fourth row show the decoder renderingsusing 2, 16 and 128 dimensions for the feature Nv. (b) images synthesized with LAE + LGAN. Thesetting for the inputs and feature dimensions are the same as in (a).
Figure 4: The effect of dimensions and objective function on Nv features. (a), (b), (c) t-SNEembeddings on Nv features. Colors correspond to the ground truth viewpoint. The objectivefunctions and the Nv dimensions are: (a) LAE 2 dim, (b) LAE 128 dim, (c) LAE + LGAN 128 dim.
Figure 5: Renderings of transferred features. In all figures the variable factor is transferred from theleft column and the common factor from the top row. (a) MNIST Mathieu et al. (2016); (b) MNIST(ours); (c) Sprites Mathieu et al. (2016); (d) Sprites (ours).
Figure 6: Attribute transfer on ShapeNet. For both subfigures the viewpoint is taken from the leftmostcolumn and the car/chair type is taken from the first row. (a) Cars: the factors are transferred correctly.
