Figure 1: Comparing double deep Q-learning (orange) and ARM (blue) on Pong.
Figure 2: Top row: Doom screenshots from (left) “HealthGathering” and (right) “MyWayHome.”Bottom row: Minecraft screenshots from (leftmost) “L1” through (rightmost) “L5”DoomHeaIthGathenng200015001000OO5出①POS-de croφE0.5	1.0sin∩ steps le6-0.20DoomiviyWayHome1.0-8-6-42∙0Ooooo出①POS-de croφE1	2
Figure 3: Evaluating double deep Q-learning (orange), dueling double DQN (red), A3C (purple),TRPO (green), ARM (blue), and ARM with off-policy data (cyan) on two ViZDoom scenarios.
Figure 4: Evaluating double deep Q-learning (orange), dueling double DQN (red), TRPO (green),and ARM (blue) on a Minecraft curriculum learning protocol. The simulator step counts at whicheach level begins are labeled and demarcated with dashed vertical lines.
Figure 5: Comparing double deep Q-learning (orange), double deep fitted Q-iteration (red), andARM (blue) on a suite of seven Atari games from the Arcade Learning Environment. For eachmethod, we plot the mean across 3 trials along with standard error bars.
Figure 6: Comparing A2C with a feedforward convolutional network (blue) and a recurrentconvolutional-LSTM network (orange) on the ViZDoom scenario MyWayHome.
