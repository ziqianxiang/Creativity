Figure 1: Purchase history data split. Consumption history is divided into three pieces X, Y andZ, by date split dx, dxy, dyz and dz . p is index of the product purchased at YYYY/MM/DD. Forexample, P = 1 — product 1 purchased at 2012/11/15, P = 2 — product 2 purchased at 2013/06/21.
Figure 2: Neural Network topologiesThe recommender model for L layers is defined by the function at each layer, and the loss function,as shown in Figure 2a and illustrated below using the equations corresponding to each layer:Ao = X + ξ; Aj = Drop(Act(Wj ∙ Aj-ι + Bj)); Y = Sigm(Wl ∙ Al-i + Bl);L	L-1loss = COSt(Y, Y) + λ X |W∙ ∣2 + β X KLS, pj)；	(3)j=1	j=1where weighted cross-entropy is used as the cost function:P-1Cost(Y,1Y) = - X (wι ∙ yp ∙ log(yp) + wο ∙ (1 - yp) ∙ log(1 - yp))p=0with, X, Y - training input and output data in Figure 1; X, Y ∈ {0, 1}P, P - number of rec-ommended products; Y - output scores of the NN; L - number of layers in the NN; Wj-, Bj -weight matrix and bias to be learnt. j = 1,...,L; Aj is the activation at layer j; Act(*)-4Workshop track - ICLR 2018the nonlinear activation function (Relu, Sigmoid, (Nair & Hinton, 2010) etc.); Drop(*) - dropout function (Srivastava et al., 2014); ξ - noise added to input data, for increasing robustnessof the model (Vincent et al., 2008); w1, w0 - constant weights for purchased and non-purchasedproducts respectively, introduced for balancing number of purchased vs non-purchased products;
Figure 3: Accuracy metrics of different approaches on AIV purchase historyPredictor modelProduction CFLSTM is well applied on sequences like text, speech etc. These sequences has strong grammaticalrules, which are well captured by LSTM. We explain lower accuracy of LSTM (On Figure 3 ) byour data properties (or lack of strong grammatical rules in sequences of purchases in our data).
Figure 4: Shape of time decay function (4), (5). Parameter d belongs to two years range:0, . . . , 365 × 2.
Figure 5: Different weighted combinations of predictor and auto-encoder modelsdecay applied. Shape of the time decay functions applied on input data and cost function (8) isshown on Figure 5b. It uses past purchases X as training input data and predicts both past X andfuture purchases Y . We can expect higher PCC in comparison to predictor model because auto-encoder and predictor model can predict products which belong to both input X and output Y datasets.
Figure 6: Accuracy metrics on different categories4.3	Model scalabilityIt is important for recommendation algorithms to be easily operated across different categories,especially at Amazon scale. In this section, we present how our approach can be trained on allcategories of digital products without additional feature engineering.
Figure 7: Properties of predictor model on AIV data. (a)(left) Convergence of metrics over epochs.
Figure 8: Metrics in October and ChristmasWe also measure accuracy metrics to validate the performance of NN models quantitatively. Preci-sion@K and PCC@K are calculated in October and Christmas week and shown on Figure 8. As canbe seen, Christmas recommendations has higher Precision@K and lower PCC@K, in comparisonwith October recommendations. There is an acceptable temporal variation in the accuracy metrics,so we can confirm that out NN model is adapting to seasonality changes. At the time of this test ourmodel was not in production yet, so our recommendations did not impact customer consumptionpattern.
Figure 9: Production pipelineOur pipeline is similar with (Cheng et al., 2016), the main difference is that we pre-generate rec-ommendations offline. As a result, our online recommender remains simple and supports multiplecategories with no modifications. Also Data pre-processing, Model training and Recommendationsgenerations pipeline can be used by multiple category recommenders sequentially or in parallel ifthere is an extra hardware available. The side effect of our approach is offline recommendationsgeneration is delaying the propagation of recent purchases into our model.
Figure 11: Accuracy metricsPredictor model has the same PCC@K With soft split and loWer precision@K. Both of these modelshave higher accuracy metrics than fastXML. We observe similar difference in precision betWeenpredictor model and fastXML on Figure. 3, but fastXML has higher PCC@K. These results canbe used only as an approximation of a performance on real implicit feedbacks (purchase history),because in this section We Were using ratings converted to implicit feedbacks.
