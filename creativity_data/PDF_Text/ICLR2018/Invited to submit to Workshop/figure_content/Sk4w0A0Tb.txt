Figure 1: Rotation is a universal differentiable operation that enables the advantages of theRUM architecture. (a) The rotation R(a, b) in the plane defined by a = ε and b = T acts on thehidden state h. (b) The RUM cell, in which Rotation encodes the kernel R. The matrix Rt acts onht-1 and thus keeps the norm of the hidden state.
Figure 2: The orthogonal operation Rotation enables RUM to solve the Copying Memory Task.
Figure 3: The kernel generating the target memory for RUM is following a diagonal activationpattern, which signifies the sequential learning of the model. (a) A temperature map of the valuesof the variables when the model is learned. The task is Associative Recall, T = 50, and the model isRUM, λ = 1, with Nh = 50 and without time normalization. (b) An interpretation of the function ofthe diagonal and off-diagonal activations of RUM’s Whh kernel on NLP tasks. The task is CharacterLevel Penn Treebank and the model is λ = 0 RUM, Nh = 2000, η = 1.0. See section E foradditional examples.
Figure 4: The associative memory provided by rotational operation Rotation enables RUM tosolve the Associative Recall Task. The input sequences is 50 . For all models Nh = 50. For thetraining of all models we use RMSProp optimization with a learning rate of 0.001 and a decay rateof 0.9; the batch size Nb is 128. We observe that it is necessary to tune in the associative memoryvia λ = 1 since λ = 0 RUM does not learn the task.
Figure 5:	The FS-RUM-2 high-level architecture. In our experiments we combine a slow RUMcell with fast LSTM cells in the FS-RNN model.
Figure 6:	Examples of validation BPC vs Epochs for each model in the PTB task. The FS-LSTM-2(B) model is the previous state-of-the-art by Mujika et al. (2017), which we take as anLSTM baseline. The FS-RUM-2(B) model is the FS-RUM-2 model, descibred in table 5, whichserves as our RUM baseline. FS-RUM-3 has the same hyper-parameters as the baseline FS-RUM-2(B), except that it has 3 fast cells LSTM, each with a hidden state of 500. FS-RUM-2(B)+TBPTTis the same as FS-RUM-2(B) except that the T length is 200. FS-RUM-2(B)+DZ is the same asFS-RUM-2(B) except that the (non-recurrent dropout, cell zoneout, hidden zoneout) is (0.4, 0.5,0.2). FS-RUM-2(B)+Norm is the same as FS-RUM-2(B) except that the time normalization normη is 1.3. FS-RUM-2(B)+(S)800-1100 is the same as FS-RUM-2(B) except that the fast cells’ sizeis 800 and the slow cell’s size is 1100. FS-RUM-2(B)+(S)1000-1000 is the same as FS-RUM-2(B)except that the fast cells’ size is 1000 and the slow cell’s size is 1000. FS-RUM-2(B)+(S)900-1200is the same as FS-RUM-2(B) except that the fast cells' size is 900 and the slow cell's size is 1200.
Figure 7: The collection of kernels for λ = 1 RUM, Nh = 100, η = NlA for the Copying task,T = 500.
Figure 8: The collection ofkernels for λ = 0 RUM, Nh = 256, η = N/A for the Question AnsweringbAbI Task.
