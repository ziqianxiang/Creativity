Figure 1: Direct connections (dashed) to a single time step t and example shortest paths (solid) fromtime t 一 T to time t for various architectures. Typical RNN connections (blue) impede gradient flowthrough matrix multiplications and nonlinearities. LSTM facilitates gradient flow through additionalpaths between adjacent time steps with less resistance (orange). NARX RNNs facilitate gradientflow through additional paths that span multiple time steps.
Figure 2: Gradient norms ∣∣ ∂h+lt k averaged over a batch of examples during permuted MNIST∂ ht-τtraining. Unlike Clockwork RNNs and MIST RNNs, simple RNNs and LSTM capture essentiallyno learning signal from inputs that are far from the loss.
Figure 3: Validation curves for the copy problem with copy delays of 50 (upper left), 100 (upperright), 200 (lower left), and 400 (lower right).
