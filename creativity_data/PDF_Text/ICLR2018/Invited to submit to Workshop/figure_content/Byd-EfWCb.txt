Figure 1: Unrolling a RNN decoder at inference time. The initial hidden state for the decoder is typically theencoder output, either the recurrent cell final state for a RNN encoder, or the sum of the input word embeddingsfor a BOW encoder. At the first time step, a learned <GO> token is presented as the input. In subsequent timesteps, a probability-weighted sum over word vectors is used. The decoder is then unrolled for a fixed numberof steps. The hidden states are then concatenated to produce the unrolled decoder embedding. In the modelsevaluated in Section 4, this process is performed for the RNN corresponding to the previous and next sentences.
Figure 2: Performance on the STS tasks depending on the number of unrolled hidden states of the decoders,using dot product as the similarity measure. The top row presents results for the RNN encoder and the bottomrow for the BOW encoder. Red: Raw encoder output with BOW decoder. Green: Raw encoder output withRNN decoder. Blue: Unrolled RNN decoder output. Independent of the encoder architecture, unrolling evena single state of the decoder always outperforms the raw encoder output with RNN decoder, and almost alwaysoutperforms the raw encoder output with BOW decoder for some number of unrolls.
Figure 3: Performance on the STS tasks depending on the number of unrolled hidden states of the decoders,using cosine similarity as the similarity measure. The top row presents results for the RNN encoder and thebottom row for the BOW encoder. Red: Raw encoder output with BOW decoder. Green: Raw encoder outputwith RNN decoder. Blue: Unrolled RNN decoder output. For both RNN and BOW encoders, unrolling thedecoder strictly outperforms *-RNN for almost every number of unroll steps, and perform nearly as well as orbetter than *-B OW.
