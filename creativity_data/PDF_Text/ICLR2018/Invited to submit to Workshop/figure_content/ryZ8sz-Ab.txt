Figure 1: Outline of the proposed model.
Figure 2: Comparison of partial reading, early stopping and our model on the IMDB dataset. Curvesare obtained by changing the computational budget for each method. For Ours and Early Stopping,we adjust the parameter α.
Figure 3: Comparisons on character-level topic classification on two datasets: The x-axis and y-axisare representing FLOPs and accuracy, respectively. The red line denotes the partial reading baseline.
Figure 4: Comparison of the partial reading model and our model on the Yelp challenge dataset.
Figure 5: Comparison between different action combination settings to demonstrate the effective-ness of each mechanism: the blue line denotes our model with all actions, the green line denotesthe model with only an early-stopping module. Between these two lines, the red one represents themodel with rereading and early-stopping (no skimming) while the yellow one represents the modelwith skimming and early-stopping (no rereading).
Figure 6: Sentiment analysis example: The color scheme shows the spectrum from positiveness(green) to negativeness (red) output by the LSTM model. Our model is able to read the first twosentences and get the correct prediction (positive), while the full-reading LSTM model gets confusedand outputs the wrong label (negative).
Figure 7: Topic classification example: Our model skips irrelevant chunks and reread an importantphrase with “stake.” It also early stops after reading “collaboration deal” and outputs the rightclassification (Business), but the full-reading model is fooled to output a wrong label (Technology).
Figure 8: Comparison between different chunk sizes: Here the x-axis and y-axis are the same asprevious figures. The red curve denotes the partial reading baseline, while the grey, blue, purplecurves denote our models with chunk size 8, 20, 40, respectively. We found that our model is robustto different chunk sizes.
