Figure 1: Examples of the observationBased on this observation, we extract the topic from the given answer A according to its questionQ. A and Q are represented as a sequence of tokens [a1, ..., an] and [q1, ..., qm] respectively. In theprocess of topic extraction, we consider all the tokens in A as candidates for the topic, and considerall the tokens in Q as voters voting for the candidates similar to qi . Formally, after voting, candidateai obtains votes Si = Pjm=i Pij ∙ sim(ai, qj), where Pij can be calculated by:1, if sim(ai, qj ) > λ0, otherwise(2)where sim(ai, qj) is the cosine similarity between ai and qj calculated using word embeddings ofthe two words. Candidate ai is selected as part of the topic T if the average vote Si = Si/ Pjm=I Pijis greater than a threshold K .
Figure 2: Illustration for Distribution MappingAs illustration 2 shows, our model uses two internal representations (can be assumed as sampledfrom 夕(Al) and 夕(ZS) respectively) to generate a question (can be assumed as sampled from夕(Qsl)) which learns a distribution mapping method P(夕(Qsl)∣夕(Al),夕(ZS)) from 夕(Al) and夕(Zs) to 夕(Qsl). When given an answer a% and a tqt Zi which never appeared as a pair in the train-ing data, benefited from the distribution mapping, our model can generate a question similar to qjbecause a% and a§ are from the same distribution 夕(Al) and, Zi and Zj are from the same distribu-tion 夕(Zs). However, this method requires enough training data to learn each distribution and themapping relationship between them.
Figure 3: Architecture of the Topic-Specific Question Generation ModelTQT Encoder: We also call it topic encoder. It encodes the topic information into an internal vectorrepresentation. For question types in tqt (topic and question type), we divide them into 7 categoriesas We discussed previously, and each category is represented by a type embedding e_ti which canbe learned in the training process. The question type encoder uses a full connection function totransduce e_ti to an internal vector representation qti = FC1 (e_ti). We use a bidirectional LSTMto encode the topic,-→bt = L--S-T-M→(tct,b--t-→1)b>t = ⅛M(tct, C1)(5)where bt is the hidden state at time step t for the forward pass LSTM and bt for the backward pass.
