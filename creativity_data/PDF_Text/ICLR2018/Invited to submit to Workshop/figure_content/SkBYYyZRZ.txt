Figure 1: An example activation function structure. The activation function is composed of multiplerepetitions of the “core unit”, which consists of two inputs, two unary functions, and one binaryfunction. Unary functions take in a single scalar input and return a single scalar output, such u(x) =x2 or u(x) = σ(x). Binary functions take in two scalar inputs and return a single scalar output, suchas b(xι, x2) = xι ∙ x2 or b(x1,x2) = exp(-(xι — x2)2).
Figure 2: The RNN controller used to search over large spaces. At each step, it predicts a singlecomponent of the activation function. The prediction is fed back as input to the next timestep inan autoregressive fashion. The controller keeps predicting until every component of the activationfunction has been chosen. The controller is trained with reinforcement learning.
Figure 3: The top novel activation functions found by the searches. Separated into two diagrams forvisual clarity. Best viewed in color.
Figure 4: The Swish activation function.	Figure 5: First derivatives of Swish.
Figure 6: Preactivation distribution aftertraining of Swish with β = 1 on ResNet-32.
Figure 7: Distribution of trained β values of Swishon Mobile NASNet-A.
Figure 8: Training curves of Mobile NASNet-Aon ImageNet. Best viewed in colorModel	Top-IAcc.(%)			Top-5Acc. (%)		LReLU	73.8	73.9	74.2	91.6	91.9	91.9PReLU	74.6	74.7	74.7	92.4	92.3	92.3Softplus	74.0	74.2	74.2	91.6	91.8	91.9ELU	74.1	74.2	74.2	91.8	91.8	91.8SELU	73.6	73.7	73.7	91.6	91.7	91.7GELU	74.6	-	-	92.0	-	-ReLU	73.5	73.6	73.8	91.4	91.5	91.6Swish-1	74.6	74.7	74.7	92.1	92.0	92.0Swish	74.9	74.9	75.2	92.3	92.4	92.4Table 6: Mobile NASNet-A on ImageNet, with3 different runs ordered by top-1 accuracy. Theadditional 2 GELU experiments are still trainingat the time of submission.
