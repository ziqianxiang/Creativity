Figure 1: Gaps in Inference1Under review as a conference paper at ICLR 2018Term	Definition	VAE FormulationInference	logp(x) - L[q]	KL (q(z∣x)l∣p(z∣x))Approximation	log p(x) - L[q*]	KL (q*(ZIx)IIp(ZIx))Amortization	L[q*] - L[q]	KL (q(ZIx)IIp(ZIx)) - KL (q*(ZIx)IIp(ZIx))Table 1: Summary of Gap Terms. The middle column refers to the general case where our variationalobjective is a lower bound on the marginal log-likelihood (not necessarily the ELBO). The right mostcolumn demonstrates the specific case in VAEs. q*(z∣x) refers to the optimal approximation withina family Q, i.e. q* (z|x) = argmi%∈Q KL (q(z∣x)∣∣p(z∣x)).
Figure 2: True Posterior and Approximate Distributions of a VAE with 2D latent space. Columns:4 different datapoints. FFG: FUIly-factorized Gaussian. Flow: Using a flexible approximate dis-tribution. Amortized: Using amortized parameters. Optimal: Parameters optimized for individualdatapoints. The green distributions are the true posterior distributions, highlighting the mismatchwith the approximation.
Figure 3: Training curves for a FFG and a Flow inference model on MNIST. AIS provides thetightest lower bound and is independent of encoder overfitting. There is little difference betweenFFG and Flow models trained on the 1000 datapoints since inference is nearly equivalent.
