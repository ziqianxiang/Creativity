Figure 1: Using WV to control the gradient explosion of a tanh resnet. Shades indicate standard deviation(taken in normal scale, but possibly displayed in log scale), while solid lines indicate the corresponding mean.
Figure 2: Zigzagging through β∙-space with ReLU resnet. We trained a grid of ReLU residual networks,pinning all σ.s at 1, but varying the β∙s as follows: The zig: fix βv = βa = 0; fix βw = βb and increase bothfrom 0 to 2 (making Vr = βw + βv go from 0 to 2 as well); The zag: fix βv = 0, βw = βb = 2; increase βafrom 0 to 2 (increasing Ur from 0 to 2 as well). For each setting of the hyperparameters, we train a network onMNIST with those hyperparameters for 30 epochs. We then report the accuracy that the network achieved onthe training and test sets. The plots above are, in order from left to right, (a) zig/test, (b) zag/test, (c) zig/train,(d) zag/train. In the zig, we have overlaid a contour plot of s (computed from Thm C.2), which is almostidentical to the contour plots of P and χ(0)∕χ(l); numbers indicate log 1 + log s. The dashed line is givenby I--V l1-Vr = C for C ≈ 10. In the zag, We have overlaid a contour plot of χV1) /Xvl) (computed fromThm C.3). Numbers indicate log χv1)∕χvl).
Figure 3: Sweeping through VV phases of tanh resnet. In all experiments here, We Pin σ.s all to 1. From left toright: (a) and (b). We sweep Ut : 0 % 1 in to ways, testing to what extent Ut determines the final performance.
