Figure 1: ARAE architecture. The model can be used as an autoencoder, where a structure x is encoded anddecoded to produce x, and as a GAN (ARAE-GAN), where a sample Z is passed though a generator gÎ¸ toproduce a code vector, which is similarly decoded to x. The critic function fw is only used at training to helpapproximate W .
Figure 2: Left: '2 norm of encoder code C and generator code c during ARAE training. The encoder C isnormalized by the model, whereas the generator learns to match this as training progresses. Middle: Sum of thedimension-wise variances of the encoder codes Pr and generator codes Pg compared to that of the standard AE.
Figure 3:	Sample interpolations from the ARAE-GAN. Constructed by linearly interpolating in the latent spaceand decoding to the output space. Word changes are highlighted in black. Results of the ARAE. The top blockshows output generation of the decoder taking fake hidden codes generated by the GAN; the bottom block showssample interpolation results.
Figure 4:	Left. Quantitative evaluation of transformations. Match % refers to the % of samples where at leastone decoder samples (per 100) had the desired transformation in the output, while Prec. measures the averageprecision of the output against the original sentence. Right. Examples (out of 100 decoder samples per sentence)where the offset vectors produced successful transformations of the original sentence. See Appendix 11 for fullmethodology.
Figure 5: Text samples generated from ARAE-GAN, a simple AE, and from a baseline LM trained on the samedata. To generate from an AE we fit a multivariate Gaussian to the learned code space and generate code vectorsfrom this Gaussian.
