Figure 1: One turn in an ESS Attacker-Defender game. The attacker proposes a partition A, B of the currentgame state, and the defender chooses one set to destroy (in this case A). Pieces in the remaining set (B) thenmove up a level to form the next game state.
Figure 2: Training a linear network to play as the defender agent with PPO, A2C and DQN. A linear model istheoretically expressive enough to learn the optimal policy for the defender agent. In practice, we see that formany difficulty settings and algorithms, RL struggles to learn the optimal policy and performs more poorly thanwhen using deeper models (compare to Figure 3). An exception to this is DQN which performs relatively wellon all difficulty settings.
Figure 3: Training defender agent with PPO, A2C and DQN for varying values of potentials and two differentchoices of K with a deep network. Overall, we see significant improvements over using a linear model. Forsmaller K, DQN performs relatively consistently across different potential values, though not quite matchingPPO-left and right panes, row 2. A2C tends to fare worse than both PPO and DQN.
Figure 4: Training defender agent with PPO, A2C and DQN for varying values of K and two different choicesof potential (top and bottom row) with a deep network. All three algorithms show a noticeable variation inperformance over different difficulty settings, though we note that PPO seems to be more robust to larger K(which corresponds to longer episodes). A2C tends to fare worse than both PPO and DQN.
Figure 5: Plots comparing the performance of Supervised Learning and RL on the Attacker Defender Gamefor different choices of K. The left pane shows the proportion of moves correct for supervised learning and RL(according to the ground truth). Unsurprisingly, we see that supervised learning is better on average at getting theground truth correct move. However, RL is better at playing the game: a policy trained through RL significantlyoutperforms a policy trained through supervised learning (right pane), with the difference growing for larger K .
Figure 6: Figure showing: (1) (left pane) the number of fatal mistakes (defender moves from winning state(less than 1.0 potential) to losing state (greater than 1.0 potential)) made by supervised learning compared to RL.
Figure 7: Plot showing overfitting to opponent strategies. A defender agent is trained on the optimal attacker,and then tested on (a) another optimal attacker environment (b) the disjoint support attacker environment. Theleft pane shows the resulting performance drop when switching to testing on the same opponent strategy as intraining to a different opponent strategy. The right pane shows the result of testing on an optimal attacker vs adisjoint support attacker during training. We see that performance on the disjoint support attacker converges to asignificantly lower level than the optimal attacker.
Figure 8: Performance of PPO and A2C on training the attacker agent for different difficulty settings. DQNperformance was very poor (reward < -0.8 at K = 5 with best hyperparams). We see much greater variation ofperformance with changing K, which now affects the sparseness of the reward as well as the size of the actionspace. There is less variation with potential, but we see a very high performance variance (top right pane) withlower (harder) potentials.
Figure 9: Performance of attacker and defender agents when learning in a multiagent setting. In the top panes,solid lines denote attacker performance. In the bottom panes, solid lines are defender performance. The sharpchanges in performance correspond to the times we switch which agent is training. We note that the defenderperforms much better in the multiagent setting: comparing the top and bottom left panes, we see far morevariance and lower performance of the attacker compared to the defender performance below. Furthermore,the attacker loses to the defender for potential 1.1 at K = 15, despite winning against the optimal defender inFigure 8. We also see (right panes) that the attacker has higher variance and sharper changes in its performanceeven under conditions when it is guaranteed to win.
Figure 10: Results for generalizing to different attacker strategies with single agent defender and multiagentdefender. The figure single agent defender trained on the optimal attacker and then tested on the disjoint supportattacker and a multiagent defender also tested on the disjoint support attacker for different values of K. We seethat multiagent defender generalizes better to this unseen strategy than the single agent defender.
Figure 11: On the left we train on different potentials and test on potential 0.99. We find thattraining on harder games leads to better performance, with the agent trained on the easiest potentialgeneralizing worst and the agent trained on a harder potential generalizing best. This result isconsistent across different choices of test potentials. The right pane shows the effect of training on alarger K and testing on smaller K . We see that performance appears to be inversely proportional tothe difference between the train K and test K .
Figure 12: Defender agent demonstrating catastrophic forgetting when trained on environment 1 withsmaller K and environment 2 with larger K.
Figure 13: Figure showing proportion of sets of form [0, ..., 0, 1, 0, ..., 0] that are valued less than thenull set. Out of the K + 1 possible one hot sets, we determine the proportion that are not pickedwhen paired with the null (zero) set, and plot this value for different K.
Figure 14: Confidence as a function of potential difference between states. The top figure shows truepotential differences and model confidences; green dots are moves where the model prefers to makethe right prediction, while red moves are moves where it prefers to make the wrong prediction. Theright shows the same data, plotting the absolute potential difference and absolute model confidencein its preferred move. Remarkably, an increase in the potential difference associated with an increasein model confidence over a wide range, even when the model is wrong.
Figure 15: Change in performance when testing on different state distributionson an ‘easy’ start state distribution (one where most of the states seen are very similar to one another)results in a significant performance drop when switching distribution.
Figure 16: Effect of model size on performance for K = 10. Hidden layers have 300 units.
