Figure 1: The loss surfaces of ResNet-56 with/without skip connections. The vertical axis islogarithmic to show dynamic range. The proposed filter normalization scheme is used to enablecomparisons of sharpness/flatness between the two figures.
Figure 2: 1D linear interpolation of solutions obtained by small-batch and large-batch methods forVGG9. The blue lines are loss values and the red lines are accuracies. The solid lines are trainingcurves and the dashed lines are for testing. Small batch is at abscissa 0, and large batch is at abscissa 1.
Figure 3: Histogram of weights. With zero weight decay, small-batch methods produce large weights.
Figure 4: The shape of minima obtained using different optimization algorithms, with varying batchsize and weight decay. The title of each subfigure contains the optimizer, batch size, and test error.
Figure 5: 2D visualization of solutions obtained by SGD with small-batch and large-batch. Similar toFigure 4, the first row uses zero weight decay and the second row sets weight decay to 5e-4.
Figure 6: 2D viSualization of the SolutionS of different networkS.
Figure 7: Wide-ResNet-56 (WRN-56) on CIFAR-10 both with shortcut connections (top) and without(bottom). The label k = 2 means twice as many filters per layer, k = 4 means 4 times, etc. Test erroris reported below each figure.
Figure 8: Ineffective visualizations of optimizer trajectories. These visualizations suffer from theorthogonality of random directions in high dimensions.
Figure 9: Projected learning trajectories use normalized PCA directions for VGG-9. The left plot ineach subfigure uses batch size 128, and the right one uses batch size 8192.
Figure 10: 1D linear interpolation of solutions obtained by small-batch and large-batch methods forResNet56. The blue lines are loss values and the red lines are error. The solid lines are training curvesand the dashed lines are for testing.
Figure 11: The shape of minima obtained via different optimization algorithms for ResNet-56, withvarying batch size and weight decay. Similar to Figure 4, the first row uses zero weight decay and thesecond row uses 5e-4 weight decay.
Figure 12: 2D visualization of solutions of ResNet-56 obtained by SGD/Adam with small-batch andlarge-batch. Similar to Figure 11, the first row uses zero weight decay and the second row sets weightdecay to 5e-4.
Figure 13: The loss landscape for DenseNet-121 trained on CIFAR-10. The final training error is0.002 and the testing error is 4.3716Under review as a conference paper at ICLR 2018m30≈s≈0*s*0累)」E0	50	1∞	150	200	250	300Epochs(d)	ResNet-CIFARm30≈s≈0*s*0(次)」E3O 50	1∞	150	200	250	300Epochs(e)	ResNet-CIFAR-noshortm30≈s≈0*s*0(次)」E30	50	1∞	150	200	250	3∞Epochs(f)	ResNet-ImageNetFigure 14: Convergence curves for different architectures. The first row is for training loss and the
Figure 14: Convergence curves for different architectures. The first row is for training loss and thesecond row are training and testing error curves.
Figure 15: Training and testing loss curves for VGG-9. Dashed lines are for testing, solid for training.
