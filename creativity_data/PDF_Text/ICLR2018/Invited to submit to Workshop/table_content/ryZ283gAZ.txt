Table 1: In this table, we list a few popular deep networks, their associated ODEs and the numericalschemes that are connected to the architecture of the networks.
Table 2: Comparisons of LM-ResNet/LM-ResNeXt with other networks on CIFARModel	Layer	Error	Params	DatasetResNet (He et al. (2015b))	20	8.75	0.27M	CIFAR10ResNet (He et al. (2015b))	32	7.51	0.46M	CIFAR10ResNet (He et al. (2015b))	44	7.17	0.66M	CIFAR10ResNet (He et al. (2015b))	56	6.97	0.85M	CIFAR10ResNet (He et al. (2016))	110, pre-act	6.37	1.7M	CIFAR10LM-ResNet (Ours)	20, pre-act	8.33	0.27M	CIFAR10LM-ResNet (Ours)	32, pre-act	7.18	0.46M	CIFAR10LM-ResNet (Ours)	44, pre-act	6.66	0.66M	CIFAR10LM-ResNet (Ours)	56, pre-act	6.31	0.85M	CIFAR10ResNet (Huang et al. (2016b))	110, pre-act	27.76	1.7M	CIFAR100ResNet (He et al. (2016))	164, pre-act	24.33	2.55M	CIFAR100ResNet (He et al. (2016))	1001, pre-act	22.71	18.88M	CIFAR100FractalNet (Larsson et al. (2016))	^^0	23.30	38.6M	CIFAR100FractalNet (Larsson et al. (2016))	40	22.49	22.9M	CIFAR100DenseNet (Huang et al., 2016a)	^T00	19.25	27.2M	CIFAR100DenseNet-BC (Huang et al., 2016a)	190	17.18	25.6M	CIFAR100ResNeXt (Xie et al. (2017))	29(8×64d)	17.77	34.4M	CIFAR100ResNeXt (Xie et al. (2017))	29(16×64d)	17.31	68.1M	CIFAR100
Table 3: Single-crop error rate on ImageNet (validation set)Model	Layer top-1 top-5ResNet (He et al. (2015b))	50	24.7	7.8ResNet (He et al. (2015b))	101	23.6	7.1ResNet (He et al. (2015b))	152	23.0	6.7LM-ResNet (Ours)	50, pre-act	23.8	7.0LM-ResNet (Ours)	101, pre-act	22.6	6.4Figure 3: Training and testing curves of ResNext29 (16x64d, pre-act) and and LM-ResNet29(16x64d, pre-act) on CIFAR100, which shows that the LM-ResNeXt can achieve higher accura-cy than ResNeXt.
Table 4: Test on stochastic training strategy on CIFAR10Model	Layer	Training Strategy	ErrorResNet(He et al. (2015b))	110	Original	6.61ResNet(He et al. (2016))	110,pre-act	Orignial	6.37ResNet(Huang et al. (2016b))	56	Stochastic depth	5.66ResNet(Our Implement)	56,pre-act	Stochastic depth	5.55ResNet(Huang et al. (2016b))	110	Stochastic depth	5.25ResNet(Huang et al. (2016b))	1202	Stochastic depth	4.91LM-ResNet(Ours)	56,pre-act	Stochastic depth	5.14LM-ResNet(Ours)	110,pre-act	Stochastic depth	4.809Under review as a conference paper at ICLR 20183.2	Stochastic Training for Networks with LM-architectureIn this section, we extend the stochastic depth training strategy to networks with the proposed LM-architecture. In order to apply the theory of Ito process, We consider the 2nd order X + g(t)X =f(X) (which is related to the modified equation ofthe LM-structure (3)) and rewrite it as a 1st orderODE system, ∙ ,.
