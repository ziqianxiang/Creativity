Table 1: Key metrics for architectures in their randomly initialized state evaluated on Gaussiannoise. In the ‘Normalization’ column, ‘layer’ refers to layer normalization, ‘batch’ refers to batchnormalization, ‘LOlayer’ refers to length-only layer normalization and ‘none’ refers to an absenceof a normalization layer. In the ‘Matrix type’ column, ‘Gaussian’ refers to matrices where eachentry is drawn from an independent Gaussian distribution with mean zero and a standard deviationthat is constant across all entries. ‘orthogonal’ refers to a uniformly random orthogonal matrix and‘looks-linear’ refers to the initialization scheme proposed by Balduzzi et al. (2017) and expoundedin section H. In the ‘Skip type’ column, ‘identity’ refers to identity skip connections and ‘Gaussian’refers to skip connections that multiply the incoming value with a matrix where each entry is drawnfrom an independent Gaussian distribution with mean zero and a standard deviation that is constantacross all entries. ‘none’ refers to an absence of skip connections. In the ‘Width’ column, ‘100/200’refers to linear layers having widths alternating between 100 and 200. ‘St. Dev.’ refers to pre-activation standard deviation. ‘Sign Div.’ refers to pre-activation sign diversity. For details anddefinitions, see section I. Red values indicate gradient explosion or pseudo-linearity.
Table 2: Training classificaion error for architectures trained on CIFAR10. In the ‘Normalization’column, ‘layer’ refers to layer normalization, ‘batch’ refers to batch normalization and ‘none’ refersto an absence of a normalization layer. In the ‘Matrix type’ column, ‘Gaussian’ refers to matriceswhere each entry is drawn from an independent Gaussian distribution with mean zero and a standarddeviation that is constant across all entries. ‘looks-linear’ refers to the looks-linear initializationscheme proposed by Balduzzi et al. (2017) and expounded in section H. In the ‘Skip type’ column,‘identity’ refers to identity skip connections and ‘none’ refers to an absence of skip connections. Inthe two rightmost columns, we show the training classification error achieved when using a singlestep size and when using a custom step size for each layer. Whichever error value is lower is shownin bold. For further methodological details, see section I. For a detailed breakdown of these results,see figures 2, 4, 5 and 6.
Table 3: Key metrics for architectures derived from dynamical systems theory.
