Table 2: Face verification accuracy (%) on LFW.
Table 3: ACCuracy(%) Comparison of different λ.
Table 4: Accuracy(%) Comparison by Scaling the Feature.
Table 5: Accuracy(%) Comparison of different μ on CIFAR10. μ = 0.0005 is the best choice amongall of them, thus We choose this setting in all the other experiments.
Table 6: Average L2-norm and the corresponding number of examples on test set of CIFAR10, e.g., 253.2 /9141 represents 9141 examples are correctly classified and their average L2-norm is 253.2. Error-fixed repre-sents the examples mis-classified by Softmax but correctly-classified by RN + Softmax. Error-added representsthe examples correctly-classified by Softmax but mis-classified by RN + Softmax.
Table 7: The CNN architectures used for MNIST/CIFAR10/CIFAR100. The count of the Conv1.x, Conv2.xand Conv3.x closely follows the settings in Liu et al. (2016). All the pooling layers are with window size 2 × 2and stride of 2._________________________________________________________Layer	MNIST	CIFAR10	CIFAR100Conv0.x	[3 × 3, 64] × 1	[3 X 3, 64] X 1	[3 X 3,128] X 1Conv1.x	[3 × 3, 64] × 3	[3 X 3, 64] X 4	[3 X 3,128] X 4Conv2.x	[3 × 3, 64] × 3	[3 X 3,128] X 4	[3 X 3, 256] X 4Conv3.x	[3 X 3, 64] X 3	[3 X 3, 256] X 4	[3 X 3, 512] X 4Fully Connected	256	—	512	5126.1	LEMMAHere is the Lemma proposed by Ranjan et al. (2017) and is used in the proof of Property 3 andProperty 4.
Table 8: Results of comparison experiments for three feature incay on CIFAR10, where LN repre-Sents the linear form, LogN represents the log form and RN represents the reciprocal form.
