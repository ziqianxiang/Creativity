Table 1: NotationsD,N,M Input, hidden and output dimensionsxt ∈ RD or CD, ht ∈ CD Input and hidden state at time tVt ∈ RM or CM, ^t ∈ RM or CM Prediction targets and RNN predictions at time tU ∈ CN×D, W ∈ CN×N, V ∈ CM×N Input, recurrent amd output weight matricesb ∈ RN or CN, c ∈ RM or CM Hidden and output biasσ(.), L(y, y) Point-wise non-linear activation function and the loss function2Under review as a conference paper at ICLR 20182	Recurrent neural network formalismTable 1 summarizes some notations that we use in the paper. We consider the field to be complexrather than real numbers. We will motivate the choice of complex numbers later in this section.
Table 2: KRU achieves state of the art performance on pixel by pixel permuted MNIST while havingup to four orders of magnitude less parameters than other models.
Table 3: Performance in BPC of KRU variants and other models for character level language modelingon Penn TreeBank data-set. KRU has fewer parameters in the recurrent matrix which significantlybring down training and inference time.
Table 4: Average negative log-likelihood of KRU and KRU-LSTM compared to the baseline models.
