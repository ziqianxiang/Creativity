Table 1: Statistics on the first six graph datasets	Nodes	Edges	Edges/Node	# ClassesFacebook1 (McAuley & Leskovec, 2012)	-4,039^	88,234	21.84	-Astro1 (Leskovec et al., 2007)	18,772	198,110	10.55	-PPI1,2 (Breitkreutz et al., 2008)	3,890	38,739	9.95	49Wikipedia1,2 (Mahoney, 2011)	4,777	92,517	19.36	39Blog1,2 (Zafarani & Liu, 2009)	10,312	333,983	32.38	39DBLP1 (Yang & Leskovec, 2012)	317,080	1,049,866	3.31	-1 used in Link Prediction2 used in Node Classificationbrief description of each dataset can be found in Appendix A and an analysis of their assortativityproperties in Appendix B.1. We present results for the link prediction problem in Section 4.1 and forthe node classification problem in Section 4.2. For all experiments we used sentences of size k = 5and embeddings of size d = 128, while the number of permutations was run for n ∈ {1, 5, 10}.
Table 2: Link prediction results	Facebook		Astro			PPI			AUC	Training Time	AUC	Training Time	AUC	Training TimeNBNE	0.9688	0m11s	0.8328	0m07s	0.8462	-0m02s-DeepWalk	0.9730	2m26s	0.7548*	6m55s	0.7741*	2m30sNode2vec	0.9762	69m33s	0.7738*	182m16s	0.7841*	66m37sGain	-0.76%	12.96x	7.62%	59.06x	7.91%	77.43x 二		369.85x		1555.80x		2061.67x	Wikipedia			Blog				DBLP			AUC	Training Time	AUC	Training Time	AUC	Training TimeNBNE	0.6853	0m02s	0.9375	1m11s	0.9335*	-14m30sDeepWalk	0.6534*	7m38s	0.9098*	28m13s	0.9242t	164m34sNode2Vec	0.6547*	236m60s	0.9202*	838m41s	0.9322^	3,285m59sGain	4.67%	194.86x	1.88%	23.86x	0.13%	-Π34X-		6049.77x		709.24x		226.52xf average of 10 fold resu I no statistical tests were		ts run, due to the time necessary to run a single fold				In DBLP, NBNE again presents the best AUC score, although this difference was small and itsstatistical significance could not be verified due to the large training times of the baselines. Thisdataset contains the largest graph analyzed in this work (317,080 nodes and 1,049,866 edges) andin it, to train a single fold, Node2Vec took 3,285m59s (more than 54 hours) and DeepWalk took
Table 3: Node classification results		Blog				PPI			Wikipedia		Macro F1	Training Time	Macro F1	Training Time	Macro F1	Training TimeNBNE	0.2004	1m57s	-0.0978-	0m16s	0.0727	0m41sDeepWalk	0.1451*	31m31s	0.0991	3m04s	0.0679	13m04sNode2vec	0.1637*	959m12s	0.0971	83m02s	00689	408m00sGain	22.45%	16.18x 492.57x	-1.35%	11.82x 319.78x	5.56%	19.04x 594.62x5	Further Analysis5.1	NUMBER OF PERMUTATIONS (n)The quality of NBNE’s embeddings depends on both the size of the embeddings (d) and the numberof permutations (n). For highly connected graphs, larger numbers of permutations should be chosen(n ∈ [10, 1000]) to better represent distributions, while for sparser graphs, smaller values can beused (n ∈ [1, 10]).
Table 4: Link Prediction results for varying n with NBNEn	PPI (9.95t)			Facebook (21.84t)			Blog (32.38t)			Precision		AUC	Precision		AUC	Precision		AUC	Train	Test		Train	Test		Train	Test	10	0.7071	0.7108	0.7795	0.8453’	0.9061	0.9642	0.8771"	0.8627	0.9348"5	0.7280	0.7305	0.8071	0.8408"	0.9070	0.9688	0.8775"	0.8681	0.9375"1	0.7822	0.7751	0.8462	0.8036	0.8410	0.9150	0.8115	0.8374	0.9146t Edges per node	^ No statistical difference5.2 Time ComplexitySkipGram’s time complexity is linear on the number of sentences, embedding size (d) and logarith-mic on the size of the vocabulary (Mikolov et al., 2013a). Since the number of sentences is linearon the number of permutations (n), branching factor of the graph (b) and on the number of nodes,7Under review as a conference paper at ICLR 2018Training Time vs Embedding Size (Facebook)ιo54 3 2 1OoooIlll3EF
Table 5: Datasets homophily information	Assortativity		Degree1	Label1Facebook	0.0635^^	-Astro	0.2051	-PPI	-0.0930	0.0533Wikipedia	-0.2372	-0.0252Blog	-0.2541	0.0515DBLP	0.2665	-1 Calculated as in (Newman, 2003)We also analyze graphs with both positive and negative label assortativity in our label classificationtask. While PPI and Blog datasets present positive label assortativity, with connected nodes morefrequently sharing classes, Wikipedia has a negative assortativity, with its connected nodes beingmore likely to have different classes.
Table 6: Link prediction results with SDNE	Facebook		Astro			PPI			AUC	Training Time	AUC	Training Time	AUC	Training TimeNBNE SDNE	0.9688 0.9510*	^^0m11s^ 20m34st 242m10st	0.8328 0.8157*	0m07s*- 234m24st 5,237m59st	0.8462 0.8751*	~~0m02st 16m10st 232m01stGain	1.87%	112.21x =	2.10%	2,009.17占	-3.30%	485.10x		1,320.91x		44,896.96x		6,960.34x	Wikipedia			Blog			DBLP			AUC	Training Time	AUC	Training Time	AUC	Training TimeNBNE SDNE	0.6853 0.6781	^^0m02s^ 22m23st 337m47st	0.9375 0.9462*	1m11st 81m33st 1,492m47st	0.9335 -	-14m30s* - -Gain	1.06%	671.59x =	-0.92%	68.92x =		-		10,133.46x		1,261.51x	-	-^ Training time on CPUt Training time on GPUTable 7 shows the results of running NBNE and SDNE on the Node Classification task. On this taskNBNE gave statistically better results on two datasets, with an impressive gain of 29.27% on PPIand 46.94% on Blog, only losing on Wikipedia with an also large gain of -20.20%. We can againsee that NBNE has a more than an order of magnitude faster training time than SDNE on a GPU inthis dataset, being more than two orders of magnitude faster when SDNE is trained on a CPU.
Table 7: Node classifications results with SDNE		Blog				PPI			Wikipedia		Macro F1	Training Time	Macro F1	Training Time	Macro F1	Training TimeNBNE SDNE	0.2005 0.1364*	1m57st 96m48st 1,476m33st	0.0978 0.0757*	0m16s* 16m52st 231m04st	-0.0727^^ 0.0911*	^^0m41st 19m60st 338m40stGain	46.94%	49.64x = 757.20x	29.27%	63.24x = 866.48x	-20.20%	29.26x 495.60x^ Training time on CPUt Training time on GPUAnalyzing both these tables we can also see that the largest gains in training time occur when usingNBNE on a large but sparse network, such as Astro. This agrees with our theoretical expectations,since SDNE’s time complexity grows quadratically with the number of nodes O(|V 2|) and NBNE’sgrows with O(|V | ∙ log(∣V |) ∙ b), which is close to linear on the number of nodes for large graphs.
Table 8: Link prediction complete resultsFacebook	Precision		AUC	Training Time	Train	Test		NBNE	0.8408	0.9070	0.9688	0m11sDeepWalk	0.8770*	0.9218	0.9730	2m26sNode2vec	0.8844*	0.9251	0.9762	69m33sGain	-4.93%	-1.95%	-0.76%	(12.96x,369.85x)-Astro	Precision		AUC	Training Times	Train	Test		NBNE	0.7640	0.7552	0.8328	0m07sDeepWalk	0.6957*	0.6836*	0.7548*	6m55sNode2vec	0.7223*	0.7163*	0.7738*	182m16sGain	5.78%	5.43%	7.62%	(59.06x, 1555.80x)PPI	Precision		AUC	Training Times	Train	Test		NBNE	0.7822	0.7751	0.8462	0m02sDeepWalk	0.7124*	0.7078*	0.7741*	2m30sNode2Vec	0.7332*	0.7253*	0.7841*	66m37sGain	6.69%	6.86%	7.91%	(77.43x, 2061.67x)Wikipedia	Precision		AUC	Training Times
Table 9: Node classification complete resultsBlog	Precision		Macro F1	Training Times	Train	Test		NBNE	0.4235	0.3290	0.2004	1m57sDeepWalk	0.3726*	0.3108*	0.1451*	31m31sNode2vec	0.4022*	0.3257	0.1637*	959m12sGain	5.28%	1.00%	22.45%	(16.18x, 492.57x)PPI	Precision		Macro F1	Training Times	Train	Test		NBNE	0.3930	0.1436	0.0978	0m16sDeepWalk	0.4143*	0.1457	0.0991	3m04sNode2Vec	0.4599*	0.1371	0.0971	83m02sGain	-14.54%	-1.45%	-1.35%	(11.82x,319.78x)Wikipedia	Precision		Macro F1	Training Times	Train	Test		NBNE	0.5853	0.4938	0.0727	0m41sDeepWalk	0.5595*	0.5078*	0.0679	13m04sNode2Vec	0.5796*	0.5002	0.0689	408m00sGain	0.99%	-2.76%	5.56%	(19.04x, 594.62x)F	Author Name Disambiguation
Table 10: DBLP-ambiguous (DBLP-amb) Dataset DetailsName	# Authors	Nodes	EdgesJingLi	4~	105,746	-589,367JingWang	16	108,913	581,457JunLiu	2	106,533	590,032JunWang	21	121,511	691,705JunZhang	16	116,497	631,738LeiZhang	38	118,798	664,898LiZhang	14	122,403	693,916WeiLi	57	157,427	887,727WeiWang	85	183,962	1,103,702WeiZhang	52	131,200	722,272XiaodongWang	3	50,854	284,733XinWang	14	107,920	578,084YangLiu	33	130,319	740,501YuZhang	9	131,683	734,21410Link to used DBLP crawler and the dataset will be made available in final publication.
Table 11: Author name disambiguation resultsName	# Authors	Algorithm ∣ PreCision	Jing Li	4	NBNE	0.9415		Baseline	0.9415JingWang	16	NBNE	一	0.8791		Baseline	0.8512JunLiu	2	NBNE	0.9709		Baseline	0.9651JunWang	21	NBNE	0.8357		Baseline	0.7821JunZhang	16	NBNE	一	0.8206		Baseline	0.8130LeiZhang	38	NBNE	0.8843		Baseline	0.8309LiZhang	14	NBNE	0.8661		Baseline	0.8201WeiLi	57	NBNE	0.8221		Baseline	0.7822WeiWang	85	NBNE	0.8143		Baseline	0.8070
