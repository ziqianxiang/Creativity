Table 1: Baseline-1: configurations of the baseline network used on MusicDet200K. Each convolu-tional layer is followed by a nonlinearity layer (i.e. ReLU), batch normalization layer and poolinglayer, which are omitted in the table for brevity. The strides of all pooling layers are 2. The paddingstrategies adopted for both convolutional layers and fully connected layers are all “size preserving”.
Table 2: Baseline-2: configuration of the baseline network used on ESC-50, UrbanSound8K andDCASE. This baseline is adapted from SoundNet (Aytar et al., 2016) as detailed in Section 4.1. Forbrevity, the nonlinearity layer (i.e. ReLU), batch normalization layer and pooling layer followingeach convolutional layer are omitted. The kernel sizes for pooling layers following conv1-conv4 andconv5-conv7 are 8 and 4 respectively. The stride of every pooling layers is 2.
Table 3: Ablative study of the effects of different settings of WSNet on the model size, computationcost (in terms of #mult-adds) and classification accuracy on MusicDet200K. For clear description,We name WSNets with different settings by the combination of symbols S/C/SC^/D/Q. “S" denotesthe weight sampling along spatial dimension; “C” denotes the weight sampling along the channeldimension. “SC*" denotes the weight sampling of fully connected layers whose parameters can beseen as flattened vectors with channel of 1. “D” denotes denser filter sampling. “Q” denotes weightquantization. With a symbol occurred in the name, the corresponding component is used in WSNet.
Table 4: The configurations of the WSNet used on ESC-50, UrbanSound8K and DCASE. Pleaserefer to Table 3 for the meaning of symbols S/C/D. Since the input lengths for the baseline aredifferent in each dataset, we only provide the #Mult-Adds for UrbanSound8K. Note that since weuse the ratio of baseline’s #Mult-Adds versus WSNet’s #Mult-Adds for one WSNet, the numberscorresponding to WSNets in the column of #MUlt-Adds are the same for all dataset.
Table 5: Comparison with state-of-the-arts on ESC-50. All results of WSNet are obtained by 10-folder validation. Please refer to Table 3 for the meaning of symbols S/C/D/Q. The baseline usedhere is a simple modification of SoundNet with 8 convolution layers (refer to Section 4.1 for details),thus they have the same model size.
Table 6: Comparison with state-of-the-arts on UrbanSound8K. All results of WSNet are obtainedby 5-folder validation. Please refer to Table 3 for the meaning of symbols S/C/D/Q.
Table 7: Comparison with state-of-the-arts on DCASE. Note there are only 100 samples in testingset. Please refer to Table 3 for the meaning of symbols S/C/D/Q.
Table 8: Studies on the effects of different settings of WSNet on the model size, computation cost (interms of #mult-adds) and classification accuracy on ESC-50. Please refer to Table 3 for the meaningof symbols S/C/D/Q.
Table 9: Configurations of the baseline network Chen et al. (2016) used on CIFAR10. Each convolu-tional layer is followed by a nonlinearity layer (i.e. ReLU). There are max-pooling layers (with sizeof 2 and stride of 2) and drop out layers following conv2, conv4 and conv5. The nonlinearity layers,max-pooling layers and dropout layers are omitted in the table for brevity. The padding strategiesare all “size preserving”.
Table 10: Test error rates (in %) of WSNet and HashNet on MNIST. The model size is provided forthe baseline. For the model size of WSNet/HashNet, we provide the ratio of the baseline’s modelsize versus the model size of WSNet/HashNet. For the hidden fully connected layer in WSNet, weconduct weight sampling along the spatial dimension as introduced in Section 3.2.2 and the resultedWSNet has compactness of 8 and 64, respectively.
Table 11: Test error rates (in %) of WSNet and HashNet on CIFAR10. The model size is pro-vided for the baseline. For the model size of WSNet/HashNet, we provide the ratio of the baseline’smodel size versus the model size of WSNet/HashNet. For each convolutional layer in WSNet withcompactness of 16, we set its compactness along spatial/channel dimension to be 4/4, respectively.
