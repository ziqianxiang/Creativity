Table 1: Summary of Gap Terms. The middle column refers to the general case where our variationalobjective is a lower bound on the marginal log-likelihood (not necessarily the ELBO). The right mostcolumn demonstrates the specific case in VAEs. q*(z∣x) refers to the optimal approximation withina family Q, i.e. q* (z|x) = argmi%∈Q KL (q(z∣x)∣∣p(z∣x)).
Table 2: Inference Gaps. The columns qFFG and qF low refer to the variational distribution used fortraining the model. All numbers are in nats.
Table 3: Larger Encoder. The columns qFFG and qF low refer to the variational distribution used fortraining the model. All numbers are in nats.
Table 4: Influence of Flows on the Amortization Gap. The parameters used to increase the flexibilityof the approximate distribution also reduce the amortization gap. See Section 5.2.1 for details of theexperiment.
Table 5: Increased decoder capacity reduces approximation gap. All numbers are in nats.
Table 6: Models trained without entropy annealing. The columns qFF G and qF low refer to thevariational distribution used for training the model. All numbers are in nats.
Table 7: Neural net architecture for MNIST/Fashion-MNIST experiments.
Table 8:	Flow setting for MNIST/Fashion-MNIST experiments. q(vT, zT |v0, z0) consists of twonormalizing flows given in the second tabular.
Table 9:	Network architecture for CIFAR-10 experiments. For the generator, one of the MLPsimmediately after the input layer of the generator outputs channel-wise scales for the discretizedlogistic likelihood model. BN stands for batch-normalization.
Table 10:	Gaussian latents trained with full covariance.
