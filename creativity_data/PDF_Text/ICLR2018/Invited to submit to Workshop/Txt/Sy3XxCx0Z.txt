Under review as a conference paper at ICLR 2018
Natural Language Inference with External
Knowledge
Anonymous authors
Paper under double-blind review
Ab stract
Modeling informal inference in natural language is very challenging. With the
recent availability of large annotated data, it has become feasible to train com-
plex models such as neural networks to perform natural language inference (NLI),
which have achieved state-of-the-art performance. Although there exist relatively
large annotated data, can machines learn all knowledge needed to perform NLI
from the data? If not, how can NLI models benefit from external knowledge
and how to build NLI models to leverage it? In this paper, we aim to answer
these questions by enriching the state-of-the-art neural natural language inference
models with external knowledge. We demonstrate that the proposed models with
external knowledge further improve the state of the art on the Stanford Natural
Language Inference (SNLI) dataset.
1	Introduction
Reasoning and inference are central to both human and artificial intelligence. Natural language infer-
ence (NLI) is concerned with determining whether a natural-language hypothesis h can be inferred
from a natural-language premise p. Modeling inference in human language is very challenging but
is a basic problem towards true natural language understanding — NLI is regarded as a necessary (if
not sufficient) condition for true natural language understanding (MacCartney & Manning, 2007).
The most recent years have seen advances in modeling natural language inference. An important
contribution is the creation of much larger annotated datasets such as SNLI (Bowman et al., 2015)
and MultiNLI (Williams et al., 2017). This makes it feasible to train more complex inference models.
Neural network models, which often need relatively large amounts of annotated data to estimate their
parameters, have shown to achieve the state of the art on SNLI and MultiNLI (Bowman et al., 2015;
2016; Munkhdalai & Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016; Chen et al.,
2017a;b).
While these neural networks have shown to be very effective in estimating the underlying inference
functions by leveraging large training data to achieve the best results, they have focused on end-to-
end training, where all inference knowledge is assumed to be learnable from the provided training
data. In this paper, we relax this assumption, by exploring whether external knowledge can further
help the best reported models, for which we propose models to leverage external knowledge in major
components of NLI. Consider an example from the SNLI dataset:
•	p: An African person standing in a wheat field.
•	h: A person standing in a corn field.
If the machine cannot learn useful or plenty information to distinguish the relationship between
wheat and corn from the large annotated data, it is difficult for a model to predict that the premise
contradicts the hypothesis.
In this paper, we propose neural network-based NLI models that can benefit from external knowl-
edge. Although in many tasks learning tabula rasa achieved state-of-the-art performance, we believe
complicated NLP problems such as NLI would benefit from leveraging knowledge accumulated by
humans, at least in a foreseeable future when machines are unable to learn that with limited data.
1
Under review as a conference paper at ICLR 2018
A typical neural-network-based NLI model consists of roughly four components — encoding the
input sentences, performing co-attention across premise and hypothesis, collecting and computing
local inference, and performing sentence-level inference judgment by aggregating or composing lo-
cal information information. In this paper, we propose models that are capable of leveraging external
knowledge in co-attention, local inference collection, and inference composition components. We
demonstrate that utilizing external knowledge in neural network models outperforms the previously
reported best models. The advantage of using external knowledge is more significant when the size
of training data is restricted, suggesting that if more knowledge can be obtained, it may yielding
more benefit. Specifically, this study shows that external semantic knowledge helps mostly in at-
taining more accurate local inference information, but also benefits co-attention and aggregation of
local inference.
2	Related Work
Early work on natural language inference (also called recognizing textual textual) has been per-
formed on quite small datasets with conventional methods, such as shallow methods (Glick-
man et al., 2005), natural logic methods (MacCartney & Manning, 2007), among others. These
work already shows the usefulness of external knowledge, such as WordNet (Miller, 1995),
FrameNet (Baker et al., 1998), and so on.
More recently, the large-scale dataset SNLI was made available, which made it possible to train more
complicated neural networks. These models fall into two kind of approaches: sentence encoding-
based models and inter-sentence attention-based models. Sentence encoding-based models use
Siamese architecture (Bromley et al., 1993) — the parameter-tied neural networks are applied to
encode both the premise and hypothesis. Then a neural network classifier (i.e., multilayer percep-
tron) is applied to decide the relationship between the two sentence representations. Different neural
networks have been utilized as sentence encoders, such as LSTM (Bowman et al., 2015), GRU (Ven-
drov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016; Lin et al.,
2017; Chen et al., 2017b), and more complicated neural networks (Bowman et al., 2016; Munkhdalai
& Yu, 2016a;b). The advantage of encoding-based models is that the encoders transform sentences
into fixed-length vector representations, which can help a wide range of transfer tasks (Conneau
et al., 2017). However, this architecture ignores the local interaction between two sentences, which
is necessary in traditional natural language inference procedure (MacCartney & Manning, 2007).
Therefore, inter-sentence attention-based models were proposed to relieve this problem. In this
framework, local inference information is collected by the attention mechanism and then fed into
neural networks to compose as fixed-sized vectors before the final classification. Many related works
follow this route (Rocktaschel et al., 2015; Wang & Jiang, 2016; Cheng et al., 20l6; Parikh et al.,
2016; Chen et al., 2017a). Among them, Rocktaschel et al. (2015) were the first to propose neural
attention-based models for NLI. Chen et al. (2017a) proposed an enhanced sequential inference
model (ESIM), which is one of the best models so far and regarded as the baseline in this paper.
In general, external knowledge have been shown to be effective in a wide range of NLP tasks,
including machine translation (Shi et al., 2016; Zhang et al., 2017), language modeling (Ahn et al.,
2016), and dialogue system (Chen et al., 2016). For NLI, to the best of our knowledge, we are the
first to utilize external knowledge together with neural networks. In this paper, we first show that
a neural network equipped with external knowledge obtains further improvement over the already
strong baseline, and achieves an accuracy of 88.6% on the SNLI benchmark. Furthermore, we show
that the gain is more significant when using less training samples.
3	Methods
3.1	Representation of External Knowledge
External knowledge needs to be converted to a numerical representation for enriching natural lan-
guage inference model. One of approaches to represent external knowledge is using knowledge
graph embeddings, such as TransE (Bordes et al., 2013), TransH (Wang et al., 2014), TransG (Xiao
et al., 2016), and so on. However, these kind of approaches usually need to train a knowledge-graph
embedding beforehand.
2
Under review as a conference paper at ICLR 2018
In this paper, we propose to use relation features to describe relationship between the words in any
word pair, which can be easily obtained from various knowledge graphs, such as WordNet (Miller,
1995), and Freebase (Bollacker et al., 2008). Specifically, we use WordNet to measure the semantic
relatedness of the word in a pair using various relation types, including synonymy, antonymy, hy-
pernymy, and so on. Each of these features is a real number on the interval [0,1]. The definition
and instances of pair features derived from WordNet are indicated in Table 1. The setting of fea-
tures refers to MacCartney (2009), but we add a new feature same hypernym, which improve the
result significantly in our experiments. Intuitively, the synonymy, hypernymy and hyponymy features
help model the entailment of word pairs; the antonymy and same hypernym features help model
contradiction in word pairs.
We regard the vector r ∈ RDr as the relation feature derived from external knowledge, where Dr
is 5 in our experiments. The r will be enriched in the neural inference model to capture external
semantic knowledge. Table 2 reports some key statistics of the relation features from WordNet.
Table 1: The definition and instances of relation features from WordNet
TYPE	DEFINrnON	INSTANCES
Synonymy	It takes the value 1 if the words in the pair are synonyms in WordNet (i.e., belong to the same synset), and 0 otherwise. Specifically, it takes value 1 when two words are same.	[felicitous, good] = 1 [dog, wolf] = 0
Antonymy	It takes the value 1 if the words in the pair are antonyms in WordNet, and 0 otherwise.	[wet, dry] = 1
Hypernymy	It takes the value 1-n/8 if one word is a (direct or indirect) hypernym of the other word in WordNet, where n is the number of edges between the two words in hierarchies, and 0 otherwise.	[dog, canid] = 0.875 [wolf, canid] = 0.875 [dog, carnivore] = 0.75 [canid, dog] = 0
Hyponymy	It takes the value 1 - n/8 if a word is a (direct or indirect) hyponym of the other word in WordNet, where n is the number of edges between the two words in hierarchies, and 0 otherwise.	[canid, dog] = 0.875 [canid, wolf] = 0.875 [carnivore, dog] = 0.75 [dog, canid] = 0
Same Hypernym It takes the value 1 if the two words have the [dog, wolf] = 1
same hypernym but they do not belong to the same
synset, and 0 otherwise.
Table 2: The key statistics of relation features from WordNet.
TYPE	#WORDS	#PAIRS
Synonymy	84,487	237,937
Antonymy	6,161	6,617
Hypernymy	57,475	753,086
Hyponymy	57,475	753,086
Same Hypernym	53,281	3,674,700
3.2	Neural Inference Models
We present here our natural inference models which are composed of the following major compo-
nents: input encoding, knowledge enriched co-attention, knowledge enriched local inference col-
lection, and knowledge enriched inference composition. Figure 1 shows a high-level view of the
3
Under review as a conference paper at ICLR 2018
architecture. First, the premise and hypothesis are encoded by the input encoding components as
context-dependent representations. Second, co-attention is calculated to obtain word-level soft-
alignment between two sentences. Third, local inference information is collected to prepare for final
prediction. Fourth, the inference composition component applies aggregation of the whole sentences
and makes final prediction based on the fixed-size vector. Among them, external knowledge is re-
gard as the auxiliary component to improve the ability of (1) calculating co-attention, (2) collecting
local inference information and (3) composing inference.
Figure 1: A high-level view of our neural inference networks. Given two sentence, i.e., the premise
“The child is getting a pedicure”, and the hypothesis “The kid is getting a manicure”, the model
needs to predict the relationship among them: entailment, contradiction, or neutral.
3.2.1	Input Encoding
Given the word sequences of the premise a = (a1, . . . , aM) and the hypothesis b = (b1 , . . . , bN),
where M and N are the lengths of the sentences, the final objective is to predict a label y that
indicates the logic relationship between a and b. The formula is
y? = arg max Pr (y|a, b) ,	(1)
y
Specifically, “<BOS>” and “<EOS>” are inserted as the first and last token, respectively. First,
a and b are embedded into a De-dimensional vectors [E(a1), . . . , E(aM)] and [E(b1), . . . , E(bN)]
using an embedding matrix E ∈ RDe ×V , where V is the vocabulary size and E can be initialized
with some pre-trained word embeddings from a universal corpus.
To represent the words of the premise and hypothesis in a context-dependent way, the two sentences
are fed into the encoders to obtain context-dependent hidden states as and bs . The formula is
ais = Encoder(E(a), i) , bjs = Encoder(E(b), j) .	(2)
We employ bidirectional LSTMs (BiLSTMs) (Hochreiter & Schmidhuber, 1997) as encoders, which
is a common choice for natural language. A BiLSTM runs a forward and a backward LSTM on a
sequence starting from the left and the right end, respectively. The hidden states generated by
these two LSTMs at each time step are concatenated to represent that time step and its context:
ht = [h→;拉丁].The hidden states of the unidirectional LSTM (h→ or h厂)is calculated as follows:
it=σ(Wixt+Uiht-1+bi),	(3)
ft=σ(Wfxt+Ufht-1+bf),	(4)
ut =	tanh(Wuxt + Uuht-1	+ bu) ,	(5)
ot =	σ(Woxt + Uoht-1 +	bo) ,	(6)
ct =	ft ct-1 + it ut ,	(7)
ht =	ot tanh(ct) ,	(8)
4
Under review as a conference paper at ICLR 2018
where σ is the sigmoid function, is the element-wise multiplication of two vectors.
Wi,Wf,Wu,Wo ∈ RD×De, Ui,Uf,Uu,Uo ∈ RD×D and bi,bf,bu,bo ∈ RD are parame-
ters to be learned. D is the dimension of the hidden states in the LSTM. The LSTM utilizes a set of
gating functions for each input vector xt, i.e., the input gate it, forget gate ft, and output gate ot,
together with a memory cell ct to generate a hidden state ht .
3.2.2	Knowledge-Enriched Co-attention
In this component, we acquire soft-alignment of word pairs between the premise and hypothesis
based on our knowledge-enriched co-attention mechanism. Given the relation features rij ∈ RDr
between the premise’s i-th word and the hypothesis’s j-th word from the external knowledge, the
co-attention is calculated as
eij = (ais)Tbjs +F(rij) .	(9)
The function F can be any non-linear or linear function. Here We use F(rj) = λl(rj), where λ
is a hyper-parameter tuned on the development set and 1 is the indication function.
1	if rij is not zero vector ;
l(rij) = <	J
0	if rij is zero vector .
(10)
Intuitively, the word pairs with semantic relationship in various features are probably aligned to-
gether. Soft-alignment is determined by the co-attention matrix e ∈ RM ×N computed in Equa-
tion (9), which is used to obtain the local relevance between the premise and hypothesis. For the
hidden state of a word in a premise, i.e., ais (already encoding the word itself and its context), the
relevant semantics in the hypothesis is identified into a context vector aic using eij , more specifically
with Equation (11).
__ exp(eij)
j	PN=I exP(eik)
e	exP(eij)
βij = M^m	；―7
k=1 exp(ekj )
N
aic = αij bjs ,
j=1
M
bjc = X βij ais ,
i=1
(11)
(12)
where α ∈ RM ×N and β ∈ RM ×N are the normalized attention weight matrices with respect to the
2-axis and 1-axis. The same calculation is performed for each word in the hypothesis, i.e., bjs, with
Equation (12) to obtain the context vector bjc .
3.2.3	Knowledge-Enriched Local Inference Collection
By way of comparing the relationship between as and ac (or bs and bc), we can model local infer-
ence between aligned word pairs. In this component, we further collect knowledge-enriched local
inference information. The formula is
N
am = G([as; ac； as - ac； as G ac； X αijrij]),	(13)
j=1
M
bm = G([bj,苏；bj -bj； bj © bj； X βijrji]),	(14)
i=1
where a heuristic matching trick with difference and element-wise product is used (Mou et al., 2016;
Chen et al., 2017a). The last term in Equation (13)(14) aims to obtain the local inference relationship
between the original vectors (ais or bjs) and the context vectors (aic, or bjc) derived from the external
semantic knowledge rij. G is a non-linear mapping function to reduce dimensionality. Specifically,
we use a 1-layer feed-forward neural network with the ReLU activation function with a shortcut
connection from input PjN=1 αij rij (and PiM=1 βij rji). Intuitively, we use a weighted version of
the relation vectors between the premise and hypothesis, because only semantic relations of aligned
word pairs make an important impact on the whole sentence inference relationship.
5
Under review as a conference paper at ICLR 2018
3.2.4	Knowledge-Enriched Inference Composition
In this component, we introduce knowledge-enriched inference composition. To determine the over-
all inference relationship between a premise and a hypothesis, we need to explore a composition
layer to compose the local inference vectors (am and bm) collected above. The formula is
aiv = Composition(am, i) , bjv = Composition(bm, j) .	(15)
Here, we also use BiLSTMs as building blocks for the composition layer. The BiLSTMs read local
inference vectors (am and bm ) and learn to judge the type of local inference relationship and distin-
guish crucial local inference vectors for overall sentence-level inference relationship. The respon-
sibility of BiLSTMs in the inference composition layer is completely different from the BiLSTMs
in the input encoding layer. Our inference model converts the output hidden vectors of BiLSTMs
to a fixed-length vector with pooling operations and puts it into the final classifier to determine
the overall inference class. Particularly, besides using mean pooling and max pooling similarly to
ESIM (Chen et al., 2017a), we propose to use weighted pooling based on external knowledge to ob-
tain a fixed-length vector as in Equation (16). Intuitively, the final prediction is mostly determined
by those word pairs appearing in the external knowledge. Chen et al. (2017b) uses a similar idea
called gated-attention but they do not use external knowledge.
aw
X	exp(H (Pj=I αi ri )	a bw = XX	exp(H (PM=I Bij rji))
=PM=1 exp(H (Pi= αi ri)) i ,	一 ± P=EHP=Ej)
(16)
In our experiments, we regard the function H as a 1-layer feed-forward neural network with ReLU
activation function. We concatenate all pooling vectors, i.e., mean, max, and weighted pooling, into
a fixed-length vector and then put the vector into a final multilayer perceptron (MLP) classifier. The
MLP has a hidden layer with tanh activation and softmax output layer in our experiments. The entire
model is trained end-to-end, through minimizing the cross-entropy loss.
4	Experimental Setup
4.1	Data
The Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) focuses on three
basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis
(entailment), they contradict each other (contradiction), or they are not related (neutral). We use
the same data split as in previous work, and use classification accuracy as the evaluation metric, as
in related work. WordNet 3.0 (Miller, 1995) is used to extract semantic relation features between
words, as described in Section 3.1. The words are lemmatized using Stanford CoreNLP 3.7.0 (Man-
ning et al., 2014) to match words in WordNet, but the input word sequences for the input encoding
layer are only tokenized, without lemmatization.
4.2	Training
We release our code at [xxx] to make it replicatibility purposes. The models are selected on the
development set. Some of our training details are as follows: the dimension of the hidden states
of LSTMs and word embeddings are 300. The word embeddings are initialized by 300D GloVe
840B (Pennington et al., 2014), and out-of-vocabulary words among them are initialized randomly.
All word embeddings are updated during training. Adam (Kingma & Ba, 2014) is used for optimiza-
tion with an initial learning rate 0.0004. The mini-batch size is set to 32. Dropout with a keep rate of
0.5 and early stopping with patience of 7 are used to avoid overfitting. The gradient is clipped with
a maximum L2-norm 10. The trade-off λ for calculating co-attention in Equation (9) is selected in
[0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50] based on the development set.
6
Under review as a conference paper at ICLR 2018
Table 3: Accuracies of the different models on SNLI. The proposed model KIM achieves an accuracy
of 88.6% on the test set, which is the best result so far.
MODEL
TEST
LSTM (Bowman et al., 2015)	80.6
GRU (Vendrov et al., 2015)	81.4
Tree CNN (Mou et al., 2016)	82.1
SPINN-PI (Bowman et al., 2016)	83.2
NTI (Munkhdalai & Yu, 2016b)	83.4
Intra-Att BiLSTM (Liu et al., 2016)	84.2
Self-Att BiLSTM (Lin et al., 2017)	84.2
NSE (Munkhdalai & Yu, 2016a)	84.6
Gated-Att BiLSTM (Chen et al., 2017b)	85.5
DiSAN (Shen et al.,2017)	85.6
LSTM Att (Rocktaschel et al., 2015)	83.5
mLSTM (Wang & Jiang, 2016)	86.1
LSTMN (Cheng et al., 2016)	86.3
Decomposable Att (Parikh et al., 2016)	86.8
NTI (Munkhdalai & Yu, 2016b)	87.3
Re-read LSTM (Sha et al., 2016)	87.5
BiMPM (Wang et al., 2017)	87.5
btree-LSTM (Paria et al., 2016)	87.6
DIM (Gong et al., 2017)	88.0
ESIM (Chen et al., 2017a)	88.0
KIM	88.6
HIM (ESIM+Syntactic TreeLSTM)(Chen et al., 2017a)	88.6
BiMPM (Ensemble) (Wang et al., 2017)	88.8
DIIN (Ensemble) (Wang et al., 2017)	88.9
KIM (Ensemble)	89.1
5	Results
5.1	Overall Performance
Table 3 shows the results of different models on the SNLI dataset. The first group of models use
sentence-encoding based approaches. Bowman et al. (2015) employs LSTMs as encoders for both
the premise and hypothesis into two fixed-size sentence vectors. Then the sentence representation
is put into a MLP classifier to predict the final inference relationship. The accuracy on the test
set is 80.6%. Many related works follow this framework, using different neural networks as en-
coders. Their performances are also listed in the first group in Table 3. Among them, gated-Att
BiLSTM (Chen et al., 2017b) achieves an accuracy of 85.5%, which is state of the art for sentence-
encoding based approaches.
The second group of models uses a cross-sentence attention mechanism, which can obtain soft-
alignment information between cross-sentence word pairs. Wang & Jiang (2016) proposes a
matching-LSTM to compare the inference information of locally-aligned words, and obtains a higher
accuracy of 86.1%, even better than the state-of-the-art sentence-encoding models. Other related
models are also listed in the second group in Table 3. Among them, ESIM (Chen et al., 2017a) is the
previous state-of-the-art system, whose accuracy in test set is 88.0%. The proposed model, namely
Knowledge-based Inference Model (KIM), which enriches ESIM with external knowledge, obtains
an accuracy of 88.6%. The difference between ESIM and KIM is statistically significant under the
one-tailed paired t-test at the 99% significance level. To be best of our knowledge, this is a new state
of the art. Our ensemble model, which averages the probability distributions from ten individual
single KIMs with different initialization, achieves an even higher accuracy, 89.1%.
7
Under review as a conference paper at ICLR 2018
Figure 2: Accuracies of the models using
different components of external knowledge,
varies the training set size among 0.8%, 4%,
20% and the whole training set.
Figure 3: Accuracies of the models using dif-
ferent size of external knowledge. More ex-
ternal knowledge, higher accuracies.
5.2	Ablation analysis
Figure 2 displays the ablation analysis of different components when using the external knowledge.
To compare the importance of external knowledge under different training data scales, we randomly
sample different ratio of the whole training set, i.e., 0.8%, 4%, 20% and 100%. “A” indicates
adding external knowledge in calculating the co-attention matrix as in Equation (9), “I” indicates
adding external knowledge in collecting local inference information as in Equation (13)(14), and
“C” indicates adding external knowledge in composing inference as in Equation (16). When we only
have restricted training data, i.e., 0.8% training set (about 4,000 samples), our baseline ESIM has a
poor accuracy of 62.4%. When we only add external knowledge in calculating co-attention (“A”),
the accuracy increases to 66.6% (+ absolute 4.2%). When we only utilize external knowledge in
collecting local inference information (“I”), the accuracy has a significant gain, to 70.3% (+ absolute
7.9%). When we only add external knowledge in inference composition (“C”), the accuracy gets a
smaller gain to 63.4% (+ absolute 1.0%). The comparison indicates that “I” plays the most important
role among the three components in using external knowledge. Moreover, when we compose the
three components (“A,I,C”), we obtain the best result of 72.6% (+ absolute 10.2%). When we use
more training data, i.e., 4%, 20%, 100% of the training set, only utilizing external knowledge in
local inference information collected (“I”) achieves a significant gain, but “A” or “C” do not bring
any significant improvement. The results indicate that external semantic knowledge only helps in
co-attention and composition when there is limited training data, but always helps in collecting
local inference information. Meanwhile, for less training data, λ is usually set to a larger value.
For example, the optimal λ tuned on the development set is 20 for 0.8% training set, 2 for the 4%
training set, 1 for the 20% training set and 0.2 for the 100% training set.
Figure 3 displays the results of using different ratio of external knowledge for different training
data size. Note that here we only use external knowledge in collecting local inference information,
because it always works well for different scale of the training set. Better accuracies are achieved
when using more external knowledge. Especially under the condition of restricted training data
(0.8%), the model obtains a large gain when using more than half of the external knowledge.
6	Conclusions
Our enriched neural network-based model for natural language inference with external knowledge,
namely KIM, achieves a new state-of-the-art accuracy on the SNLI dataset. The model is equipped
with external knowledge in the major informal inference components, specifically, in calculating
8
Under review as a conference paper at ICLR 2018
co-attention, collecting local inference, and composing inference. The proposed models of infusing
neural networks with external knowledge may also help shed some light on tasks other than NLI,
such as question answering and machine translation.
References
Sungjin Ahn, HeeyoUl Choi, Tanel Parnamaa, and Yoshua Bengio. A neural knowledge language
model. arXiv preprint arXiv:1608.00318, 2016.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The Berkeley FrameNet project. In
COLING-ACL, 1998.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collab-
oratively created graph database for structuring human knowledge. In Proceedings of the 2008
ACM SIGMOD international conference on Management of data ,pp. 1247-1250. AcM, 2008.
Antoine Bordes, Nicolas Usunier, Alberto Garcla-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In NIPS, 2013.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anno-
tated corpus for learning natural language inference. In EMNLP, 2015.
Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. A fast unified model for parsing and sentence understanding. In ACL, 2016.
Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sackinger, and Roopak Shah. Signature Verifi-
cation using a ”siamese” time delay neural network. In NIPS, 1993.
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced LSTM
for natural language inference. In ACL, 2017a.
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Recurrent neu-
ral network-based sentence encoder with gated attention for natural language inference. CoRR,
abs/1708.01353, 2017b.
Yun-Nung Chen, Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz, Jianfeng Gao, and Li Deng.
Knowledge as a teacher: Knowledge-guided structural attention networks. arXiv preprint
arXiv:1609.03286, 2016.
Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In EMNLP, 2016.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised
learning of universal sentence representations from natural language inference data. In EMNLP,
2017.
Oren Glickman, Ido Dagan, and Moshe Koppel. Web based probabilistic textual entailment. 2005.
Yichen Gong, Heng Luo, and Jian Zhang. Natural language inference over interaction space. arXiv
preprint arXiv:1709.04348, 2017.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014.
Zhouhan Lin, Minwei Feng, Clcero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. A structured self-attentive sentence embedding. CoRR, abs/1703.03130, 2017.
Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural language inference using
bidirectional LSTM model and inner-attention. CoRR, abs/1605.09090, 2016.
Bill MacCartney. Natural Language Inference. PhD thesis, Stanford University, 2009.
9
Under review as a conference paper at ICLR 2018
Bill MacCartney and Christopher D Manning. Natural logic for textual inference. In Proceedings
ofthe ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,pp. 193-200, 2007.
Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David
McClosky. The Stanford CoreNLP natural language processing toolkit. In ACL System Demon-
strations, pp. 55-60, 2014.
George A. Miller. WordNet: A lexical database for english. Commun. ACM, 38(11):39-41, Novem-
ber 1995.
Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. Natural language inference by
tree-based convolution and heuristic matching. In ACL(Volume 2: Short Papers), 2016.
Tsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. CoRR, abs/1607.04315, 2016a.
Tsendsuren Munkhdalai and Hong Yu. Neural tree indexers for text understanding. CoRR,
abs/1607.04492, 2016b.
Biswajit Paria, K. M. Annervaz, Ambedkar Dukkipati, Ankush Chatterjee, and Sanjay Podder.
A neural architecture mimicking humans end-to-end for natural language inference. CoRR,
abs/1611.04741, 2016.
AnkUr Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model for natural language inference. In EMNLP, 2016.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In EMNLP, 2014.
Tim RocktaScheL Edward Grefenstette, Karl Moritz Hermann, TomaS Kocisky, and Phil Blunsom.
Reasoning aboUt entailment with neUral attention. CoRR, abs/1509.06664, 2015.
Lei Sha, Baobao Chang, Zhifang Sui, and Sujian Li. Reading and thinking: Re-read LSTM unit for
textual entailment recognition. In COLING, 2016.
Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang. Disan:
Directional self-attention network for rnn/cnn-free language understanding. arXiv preprint
arXiv:1709.04696, 2017.
Chen Shi, Shujie Liu, Shuo Ren, Shi Feng, Mu Li, Ming Zhou, Xu Sun, and Houfeng Wang.
Knowledge-based semantic embedding for machine translation. In ACL, 2016.
Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. CoRR, abs/1511.06361, 2015.
Shuohang Wang and Jing Jiang. Learning natural language inference with LSTM. In NAACL, 2016.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by trans-
lating on hyperplanes. In AAAI, 2014.
Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural
language sentences. CoRR, abs/1702.03814, 2017.
Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. CoRR, abs/1704.05426, 2017.
Han Xiao, Minlie Huang, and Xiaoyan Zhu. TransG : A generative model for knowledge graph
embedding. In ACL, 2016.
Shiyue Zhang, Gulnigar Mahmut, Dong Wang, and Askar Hamdulla. Memory-augmented chinese-
uyghur neural machine translation. arXiv preprint arXiv:1706.08683, 2017.
10