Under review as a conference paper at ICLR 2018
Exponentially vanishing sub-optimal local
MINIMA IN MULTILAYER NEURAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska
et al. (2015)) suggest that local minima with high error are exponentially rare in
high dimensions. However, to prove low error guarantees for Multilayer Neural
Networks (MNNs), previous works so far required either a heavily modified MNN
model or training method, strong assumptions on the labels (e.g., “near” linear
separability), or an unrealistically wide hidden layer with Ω (N) units.
Results: We examine a MNN with one hidden layer of piecewise linear units, a
single output, and a quadratic loss. We prove that, with high probability in the
limit of N → ∞ datapoints, the volume of differentiable regions of the empiric
loss containing sub-optimal differentiable local minima is exponentially vanishing
in comparison with the same volume of global minima, given standard normal
input of dimension do = Ω (√N), and a more realistic number of di = Ω (N/do)
hidden units. We demonstrate our results numerically: for example, 0% binary
classification training error on CIFAR with only N/do ≈ 16 hidden neurons.
1	Introduction
Motivation. Multilayer Neural Networks (MNNs), trained with simple variants of stochastic gra-
dient descent (SGD), have achieved state-of-the-art performances in many areas of machine learning
(LeCun et al., 2015). However, theoretical explanations seem to lag far behind this empirical success
(though many hardness results exist, e.g., (Sima, 2002; Shamir, 2016)). For example, as a common
rule-of-the-thumb, a MNN should have at least as many parameters as training samples. However, it
is unclear why such over-parameterized MNNs often exhibit remarkably small generalization error
(i.e., difference between “training error” and “test error”), even without explicit regularization (Zhang
et al., 2017a).
Moreover, it has long been a mystery why MNNs often achieve low training error (Dauphin et al.,
2014). SGD is only guaranteed to converge to critical points in which the gradient of the expected
loss is zero (Bottou, 1998), and, specifically, to local minima (Pemantle, 1990) (this is true also for
regular gradient descent (Lee et al., 2016)). Since loss functions parameterized by MNN weights are
non-convex, it is unclear why does SGD often work well - rather than converging to sub-optimal local
minima with high training error, which are known to exist (Fukumizu & Amari, 2000; Swirszcz et al.,
2016). Understanding this behavior is especially relevant in important cases where SGD does get
stuck (He et al., 2016) - where training error may be a bottleneck in further improving performance.
Ideally, we would like to quantify the probability to converge to a local minimum as a function
of the error at this minimum, where the probability is taken with the respect to the randomness
of the initialization of the weights, the data and SGD. Specifically, we would like to know, under
which conditions this probability is very small if the error is high, as was observed empirically (e.g.,
(Dauphin et al., 2014; Goodfellow et al., 2015)). However, this seems to be a daunting task for
realistic MNNs, since it requires a characterization of the sizes and distributions of the basins of
attraction for all local minima.
Previous works (Dauphin et al., 2014; Choromanska et al., 2015), based on statistical physics
analogies, suggested a simpler property of MNNs: that with high probability, local minima with
high error diminish exponentially with the number of parameters. Though proving such a geometric
property with realistic assumptions would not guarantee convergence to global minima, it appears to
1
Under review as a conference paper at ICLR 2018
be a necessary first step in this direction (see discussion on section 6). It was therefore pointed out
as an open problem at the Conference of Learning Theory (COLT) 2015. However, one has to be
careful and use realistic MMN architectures, or this problem becomes “too easy”.
For example, one can easily achieve zero training error (Nilsson, 1965; Baum, 1988) - if the MNN's
last hidden layer has more neurons than training samples. Such extremely wide MNNs are easy
to optimize (Yu, 1992; Huang et al., 2006; Livni et al., 2014; Haeffele & Vidal, 2015; Shen, 2016;
Nguyen & Hein, 2017). In this case, the hidden layer becomes linearly separable in classification
tasks, with high probability over the random initialization of the weights. Thus, by training the last
layer we get to a global minimum (zero training error). However, such extremely wide layers are
not very useful, since they result in a huge number of weights, and serious overfitting issues. Also,
training only the last layer seems to take little advantage of the inherently non-linear nature of MNNs.
Therefore, in this paper we are interested to understand the properties of local and global minima,
but at a more practical number of parameters - and when at least two weight layers are trained. For
example, Alexnet (Krizhevsky, 2014) is trained using about 1.2 million ImageNet examples, and has
about 60 million parameters - 16 million of these in the two last weight layers. Suppose we now
train the last two weight layers in such an over-parameterized MNN. When do the sub-optimal local
minima become exponentially rare in comparison to the global minima?
Main contributions. We focus on MNNs with a single hidden layer and piecewise linear units,
optimized using the Mean Square Error (MSE) in a supervised binary classification task (Section
2). We define N as the number of training samples, dl as the width of the l-th activation layer, and
g (x) <h (x) as an asymptotic inequality in the leading order (formally: limχ→∞ l：g 双)< 1). We
examine Differentiable Local Minima (DLMs) of the MSE: sub-optimal DLMs where at least a
fraction of > 0 of the training samples are classified incorrectly, and global minima where all
samples are classified correctly.
Our main result, Theorem 10, states that, with high probability, the total volume of the differentiable
regions of the MSE containing sub-optimal DLMs is exponentially vanishing in comparison to the
same volume of global minima, given that:
Assumption 1. The datapoints (MNN inputs) are sampled from a standard normal distribution.
Assumption 2. N → ∞, d0 (N) and d1 (N) increase with N, while ∈ (0, 1) is a constant1.
Assumption 3. The input dimension scales as √N< d0≤ N.
Assumption 4. The hidden layer width scales as
Nlog4N
——g— < dι< N.
d0
(1.1)
Importantly, we use a standard, unmodified, MNN model, and make no assumptions on the target
function. Moreover, as the number of parameters in the MNN is approximately d0 d1 , we require
only “asymptotically mild” over-parameterization: dod∖>N log4 N from eq. (1.1). For example, if
do H N, We only require di >log4 N neurons. This improves over previously known results (Yu,
1992; Huang et al., 2006; Livni et al., 2014; Shen, 2016; Nguyen & Hein, 2017) - which require an
extremely wide hidden layer with d1 ≥ N neurons (and thus Nd0 parameters) to remove sub-optimal
local minima with high probability.
In section 5 we validate our results numerically. We show that indeed the training error becomes low
when the number of parameters is close to N. For example, with binary classification on CIFAR
and ImageNet, with only 16 and 105 hidden neurons (about N/d0 ), respectively, we obtain less then
0.1% training error. Additionally, we find that convergence to non-differentiable critical points does
not appear to be very common.
Lastly, in section 6 we discuss our results might be extended, such as how to apply them to “mildly”
non-differentiable critical points.
Plausibility of assumptions. Assumption 1 is common in this type of analysis (Andoni et al., 2014;
Choromanska et al., 2015; Xie et al., 2016; Tian, 2017; Brutzkus & Globerson, 2017). At first it may
1For brevity we will usually keep implicit the N dependencies of d0 and d1 .
2
Under review as a conference paper at ICLR 2018
appear rather unrealistic, especially since the inputs are correlated in typical datasets. However, this
no-correlation part of the assumption may seem more justified if we recall that datasets are many
times whitened before being used as inputs. Alternatively, if, as in our motivating question, we
consider the input to the our simple MNN to be the output of the previous layers of a deep MNN
with fixed random weights, this also tends to de-correlate inputs (Poole et al., 2016, Figure 3). The
remaining part of assumption 1, that the distribution is normal, is indeed strong, but might be relaxed
in the future, e.g. using central limit theorem type arguments.
In assumption 2 we use this asymptotic limit to simplify our proofs and final results. Multiplicative
constants and finite (yet large) N results can be found by inspection of the proofs. We assume a
constant error since typically the limit → 0 is avoided to prevent overfitting.
In assumption 3, for simplicity we have d0≤N, since in the case d0 ≥ N the input is generically
linearly separable, and sub-optimal local minima are not a problem (Gori & Tesi, 1992; Safran &
Shamir, 2016). Additionally, we have √N< d0, which seems very reasonable, since for example,
d0/N ≈ 0.016, 0.061 and 0.055 MNIST, CIFAR and ImageNet, respectively.
In assumption 4, for simplicity We have dι<N, since, as mentioned earlier, if di ≥ N the hidden
layer is linearly separable with high probability, which removes sub-optimal local minima. The other
bound N log4 N<d°di is our main innovation - a large over-parameterization which is nevertheless
asymptotically mild and improves previous results.
Previous work. So far, general low (training or test) error guarantees for MNNs could not be
found - unless the underlying model (MNN) or learning method (SGD or its variants) have been
significantly modified. For example, (Dauphin et al., 2014) made an analogy with high-dimensional
random Gaussian functions, local minima with high error are exponentially rare in high dimensions;
(Choromanska et al., 2015; Kawaguchi, 2016) replaced the units (activation functions) with indepen-
dent random variables; (Pennington & Bahri, 2017) replaces the weights and error residuals with
independent random variables; (Baldi, 1989; Saxe et al., 2014; Hardt & Ma, 2017; Lu & Kawaguchi,
2017; Zhou & Feng, 2017) used linear units; (Zhang et al., 2017b) used unconventional units (e.g.,
polynomials) and very large hidden layers (d1 = poly (d0), typically N); (Brutzkus & Globerson,
2017; Du et al., 2017; Shalev-Shwartz et al., 2017) used a modified convnet model with less then d0
parameters (therefore, not a universal approximator (Cybenko, 1989; Hornik, 1991)); (Tian, 2017;
Soltanolkotabi et al., 2017; Li & Yuan, 2017) assume the weights are initialized very close to those of
the teacher generating the labels; and (Janzamin et al., 2015; Zhong et al., 2017) use a non-standard
tensor method during training. Such approaches fall short of explaining the widespread success of
standard MNN models and training practices.
Other works placed strong assumptions on the target functions. For example, to prove convergence of
the training error near the global minimum, (Gori & Tesi, 1992) assumed linearly separable datasets,
while (Safran & Shamir, 2016) assumed strong clustering of the targets (“near” linear-separability).
Also, (Andoni et al., 2014) showed a p-degree polynomial is learnable by a MNN, if the hidden layer
is very large (di = Ω (d0p), typically》N) so learning the last weight layer is sufficient. However,
these are not the typical regimes in which MNNs are required or used. In contrast, we make no
assumption on the target function. Other closely related results (Soudry & Carmon, 2016; Xie et al.,
2016) also used unrealistic assumptions, are discussed in section 6, in regards to the details of our
main results.
Therefore, in contrast to previous works, the assumptions in this paper are applicable in some
situations (e.g., Gaussian input) where a MNN trained using SGD might be used and be useful (e.g.,
have a lower test error then a linear classier).
2	Preliminaries and notation
Model. We examine a Multilayer Neural Network (MNN) with a single hidden layer and a
scalar output. The MNN is trained on a finite training set of N datapoints (features) X ,
x(i), . . . , x(N) ∈ Rd0×N with their target labels y , y(i), . . . , y(N)> ∈ {0, 1} N - each
datapoint-label pair x(n), y(n) is independently sampled from some joint distribution PX,Y . We
3
Under review as a conference paper at ICLR 2018
define W = [w1, . . . , wd1]> ∈ Rd1×d0 and z ∈ Rd1 as the first and second weight layers (bias
terms are ignored for simplicity), respectively, and f (∙) as the common leaky rectifier linear unit
(LReLU (Maas et al., 2013))
f (u) , ua (u) with a (u) ,	,	, u >	,
ρ , if u < 0
(2.1)
for some ρ 6= 1 (so the MNN is non-linear) , where both functions f and a operate component-wise
(e.g., for any matrix M: (f (M))ij = f (Mij)). Thus, the output of the MNN on the entire dataset
can be written as
f (WX)> z ∈ RN.	(2.2)
We use the mean square error (MSE) loss for optimization
MSE，N kek 2 with e，y - f (WX)> Z ,	(2.3)
where ∣∣∙k is the standard euclidean norm. Also, We measure the empiric performance as the fraction
of samples that are classified correctly using a decision threshold at y = 0.5, and denote this as the
mean classification error, or MCE2. Note that the variables e, MSE, MCE and other related variables
(e.g., their derivatives) all depend on W, Z, X, y and ρ, but we keep this dependency implicit, to
avoid cumbersome notation.
Additional Notation. We define g (x) <h (x) if and only if limχ→∞ l：g h(：) < 1 (and similarly ≤
and =). We denote “M 〜N” when M is a matrix with entries drawn independently from a standard
normal distribution (i.e., ∀i, j: Mij 〜N (0,1)). The Khatari-rao product (cf. (Allman et al., 2009))
of two matrices, A = a(1), . . . , a(N) ∈ Rd1 ×N and X = x(1), . . . , x(N) ∈ Rd0×N is defined as
A ◦ X，Ia(I)乳 x(1),..., a(N) 0 X(N)] ∈ Rd0d1×N ,	(2.4)
where a 0 x = a1x>, . . . , ad1x>>is the Kronecker product.
3	Basic Properties of Differentiable Local minima
MNNs are typically trained by minimizing the loss over the training set, using Stochastic Gradient
Descent (SGD), or one of its variants (e.g., Adam (Kingma & Ba, 2014)). Under rather mild
conditions (Pemantle, 1990; Bottou, 1998), SGD asymptotically converges to local minima of the
loss. For simplicity, we focus on differentiable local minima (DLMs) of the MSE (eq. (2.3)). In
section 4 we will show that sub-optimal DLMs are exponentially rare in comparison to global minima.
Non-differentiable critical points, in which some neural input (pre-activation) is exactly zero, are
shown to be numerically rare in section 5, and are left for future work, as discussed in section 6.
Before we can provide our results, in this section we formalize a few necessary notions. For example,
one has to define how to measure the amount of DLMs in the over-parameterized regime: there
is an infinite number of such points, but they typically occupy only a measure zero volume in the
weight space. Fortunately, using the differentiable regions of the MSE (definition 1), the DLMs can
partitioned to a finite number of equivalence groups, so all DLMs in each region have the same error
(Lemma 2). Therefore, we use the volume of these regions (definition 3) as the relevant measure in
our theorems.
Differentiable regions of the MSE. The MSE is a piecewise differentiable function of W, with at
most 2d1N differentiable regions, defined as follows.
Definition 1. For any A ∈ {ρ, 1}d1 ×N we define the corresponding differentiable region
DA (X) , {W|a (WX) = A} ⊂ Rd1×d0 .	(3.1)
Also, any DLM (W, Z), for which W ∈ DA (X) is denoted as “in DA (X)”.
2Formally (this expression is not needed later): MCE，备 PN=ι ∣^1 +(1 — 2y(n)) sign (e(n) — 1)].
4
Under review as a conference paper at ICLR 2018
Note that DA (X) is an open set, since a (0) is undefined (from eq. 2.1). Clearly, for all W ∈ DA (X)
the MSE is differentiable, so any local minimum can be non-differentiable only if it is not in any
differentiable region. Also, all DLMs in a differentiable region are equivalent, as we prove on
appendix section 7:
Lemma 2. At all DLMs in DA (X) the residual error e is identical, and furthermore
(A ◦ X) e = 0.	(3.2)
The proof is directly derived from the first order necessary condition of DLMs (VMSE = 0) and
their stability. Note that Lemma 2 constrains the residual error e in the over-parameterized regime:
d0d1 ≥ N. In this case eq. (3.2) implies e = 0, if rank (A ◦ X) = N. Therefore, we must have
rank (A ◦ X) < N for sub-optimal DLMs to exist. Later, we use similar rank-based constraints to
bound the volume of differentiable regions which contain DLMs with high error. Next, we define this
volume formally.
Angular Volume. From its definition (eq. (3.1)) each region DA (X) has an infinite volume in
Rd1 ×d0: if we multiply a row of W by a positive scalar, we remain in the same region. Only by
rotating the rows of W can we move between regions. We measure this “angular volume” of a
region in a probabilistic way: we randomly sample the rows of W from an isotropic distribution, e.g.,
standard Gaussian: W ~ N, and measure the probability to fall in DA (X), arriving to the following
Definition 3. For any region R ⊂ Rd1 ×d0. The angular volume of R is
V (R)，Pw~n (W ∈R) .	(3.3)
4 Main Results
Some of the DLMs are global minima, in which e = 0 and so, MCE = MSE = 0, while other DLMs
are sub-optimal local minima in which MCE > > 0. We would like to compare the angular volume
(definition 3) corresponding to both types of DLMs. Thus, we make the following definitions.
Definition 4. We define3 L ⊂ Rd1 ×d0 as the union of differentiable regions containing sub-optimal
DLMs with MCE > , and G ⊂ Rd1 ×d0 as the union of differentiable regions containing global
minima with MCE = 0.
Definition 5. We define the constant γ as γ , 0.23 max [limN→∞ (d0 (N) /N) , ]3/4 if ρ 6=
{0, 1}, and γ , 0.233/4 ifρ = 0.
In this section, we use assumptions 1-4 (stated in section 1) to bound the angular volume of the region
L encapsulating all sub-optimal DLMs, the region G, encapsulating all global minima, and the ratio
between the two.
Angular volume of sub-optimal DLMs. First, in appendix section 8 we prove the following upper
bound in expectation
Theorem 6. Given assumptions 1-4, the expected angular volume of sub-optimal DLMs, with
MCE > > 0, is exponentially vanishing in N as
Ex~NV (Le (X, y)) ≤ exp (-YeN3/4 [did。]”).
and, using Markov inequality, its immediate probabilistic corollary
Corollary 7. Given assumptions 1-4, for any δ > 0 (possibly a vanishing function of N), we have,
with probability 1 - δ, that the angular volume of sub-optimal DLMs, with MCE >	> 0, is
exponentially vanishing in N as
V (Le (X, y)) ≤ 1exp (-YeN3/4 [did。]1/4)
3More formally: if A (X, y, ) is the set of A ∈ {ρ, 1}d1 ×N for which DA(X) contains a DLM with
MCE=,then∀>0,L(X,y),S0≥ hSA∈A(X,y,0) DA (X)i and G (X, y), SA∈A(X,y,0) DA(X).
5
Under review as a conference paper at ICLR 2018
Proof idea of Theorem 6: we first show that in differentiable regions with MCE >	> 0, the
condition in Lemma 2, (A ◦ X) e = 0, implies that A = a (WX) must have a low rank. Then, we
show that, when X 〜 N and W 〜 N, the matrix A = a (WX) has a low rank with exponentially
low probability. Combining both facts, we obtain the bound.
Existence of global minima. Next, to compare the volume of sub-optimal DLMs with that of
global minima, in appendix section 9 we show first that, generically, global minima do exist (using a
variant of the proof of (Baum, 1988, Theorem 1)):
Theorem 8. For any y ∈ {0,1} N and X ∈ Rd0 ×N almost every^where4 Wefind matrices W* ∈
Rdι ×d0 and z* ∈ RdL such that y = f (W*X)> z* , where d*，4 ∖N/ (2do - 2)] and ∀i, n :
wi>x(n) 6= 0. Therefore, every MNN with d1 ≥ d*1 has a DLM which achieves zero error e = 0.
Recently (Zhang et al., 2017a, Theorem 1) similarly proved that a 2-layer MNN with approximately
2N parameters can achieve zero error. However, that proof required N neurons (similarly to (Nilsson,
1965; Baum, 1988; Yu, 1992; Huang et al., 2006; Livni et al., 2014; Shen, 2016)), while Theorem
8 here requires much less: approximately d*1 ≈ 2N/d0. Also, (Hardt & Ma, 2017, Theorem 3.2)
showed a deep residual network with N log N parameters can achieve zero error. In contrast, here
we require just one hidden layer with 2N parameters.
Note the construction in Theorem 8 here achieves zero training error by overfitting to the data
realization, so it is not expected to be a “good” solution in terms of generalization. To get good
generalization, one needs to add additional assumptions on the data (X and y). Such a possible
(common yet insufficient for MNNs) assumption is that the problem is “realizable”, i.e., there exist a
small “solution MNN”, which achieves low error. For example, in the zero error case:
Assumption 5. (Optional) The labels are generated by some teacher y = f (W*X)> z* with
weight matrices W* ∈ Rdi ×d0 and z* ∈ Rdi independent of X,forsome d*<N/d0.
This assumption is not required for our main result (Theorem 10) - it is merely helpful in improving
the following lower bound on V (G).
Angular volume of global minima. We prove in appendix section 10:
Theorem 9. Given assumptions 1-3, we set δ== ^nd-1/ + 2d0∕2√log d°/N and d* = 2N∕do , or
if assumption 5 holds, we set d1* as in this assumption. Then, with probability 1 - δ, the angular
volume of global minima is lower bounded as,
V (G (X, y)) > exp (-d*d° log N) ≥ exp (-2Nlog N).
Proof idea: First, we lower bound V (G) with the angular volume of a single differentiable region
of one global minimum (W* , z* ) - either from Theorem 8, or from assumption 5. Then we show
that this angular volume is lower bounded when W 〜N, given a certain angular margin between
the datapoints in X and the rows of W* . We then calculate the probability of obtaining this margin
when X 〜N. Combining both results, we obtain the final bound.
Main result: angular volume ratio. Finally, combining Theorems 6 and 9 it is straightforward to
prove our main result in this paper, as we do in appendix section 11:
Theorem 10. Given assumptions 1-3, we set δ = ∏d0d-1/2 + 2d*∕2√log do/N. Then, with
probability 1 - δ, the angular volume of sub-optimal DLMs, with MCE >	> 0, is exponentially
vanishing in N, in comparison to the angular volume of global minima with MCE = 0
V (Le (X, y))
V (G (X, y))
≤exp (-γeN3/4 [d*do]1∕4) ≤exp(-γeNlogN).
5 Numerical Experiments
Theorem 10 implies that, with “asymptotically mild” over-parameterization (i.e. in which #parameters
=Ω (N)), differentiable regions in weight space containing sub-optimal DLMS (with high MCE) are
4i.e., the set of entries of X, for which the following statement does not hold, has zero measure (Lebesgue).
6
Under review as a conference paper at ICLR 2018
rand: two hidden layers
d=25
d=50
d=100
d=200
rand: one hidden layer
5 2 5 1 5 0
20.IS,0
Ooo
wɔn) JoJj ə UWJ
d2∕N (≈ #parameters/#samples)
。d=25
d= 50
φ d=100
* d=200
2	3
d2∕N (≈ 2#parameters/#samples)
Figure 5.1: Gaussian data: final training error (mean±std, 30 repetitions) in the over-
parameterized regime is low (right of the dashed black line). We trained MNNs with one and two
hiddens layer (with widths equal to d = d0) on a synthetic random dataset in which ∀n = 1, . . . , N,
x(n) was drawn from a normal distribution N (0, 1), and y(n) = ±1 with probability 0.5.
	MCE	do	di	N	#Parameters/N
MNIST	0%	784	-89-	7 ∙ 104	0.999
CIFAR	0%	3072	-16-	5 ∙ 104	0983
ImageNet (downsampled to 64 X 64)	0.1%	12288		128 ∙ 104	1.008
Table 1: Binary classification of MNIST, CIFAR and ImageNet: 1-hidden layer achieves very
low training error (MCE) with a few hidden neurons, so that #parameters ≈ d0d1 ≈ N . In
ImageNet we downsampled the images to allow input whitening.
exponentially small in comparison with the same regions for global minima. Since these results
are asymptotic in N → ∞, in this section we examine it numerically for a finite number of
samples and parameters. We perform experiments on random data, MNIST, CIFAR10 and ImageNet-
ILSVRC2012. In each experiment, we used ReLU activations (ρ = 0), a binary classification target
(we divided the original classes to two groups), MSE loss for optimization (eq. (2.3)), and MCE to
determine classification error. Additional implementation details are given in appendix part III.
First, on the small synthetic Gaussian random data (matching our assumptions) we perform a scan
on various networks and dataset sizes. With either one or two hidden layers (Figure 5.1) , the
error goes to zero when the number of non-redundant parameters (approximately d0d1) is greater
than the number of samples, as suggested by our asymptotic results. Second, on the non-syntehtic
datasets, MNIST, CIFAR and ImageNet (In ImageNet we downsampled the images to size 64 × 64,
to allow input whitening) we only perform a simulation with a single 1-hidden layer MNN for which
#parameters ≈ N , and again find (Table 1) that the final error is zero (for MNIST and CIFAR) or
very low (ImageNet).
Lastly, in Figure 5.2 we find that, on the Gaussian dataset, the inputs to the hidden neurons converge
to a distinctly non-zero value. ThiS indicates We converged to differentiable critical points - since non-
differentiable critical points must have zero neural inputs. Note that occasionally, during optimization,
We could find some neural inputs With very loW values near numerical precision level, so convergence
to non-differentiable minima may be possible. HoWever, as explained in the next section, as long as
the number of neural inputs equal to zero are not too large, our bounds also hold for these minima.
6 Discussion
In this paper We examine Differentiable Local Minima (DLMs) of the empiric loss of Multilayer
Neural NetWorks (MNNs) With one hidden layer, scalar output, and LReLU nonlinearities (section 2).
We prove (Theorem 10) that With high probability the angular volume (definition 3) of sub-optimal
DLMs is exponentially vanishing in comparison to the angular volume of global minima (definition
4), under assumptions 1-4. This results from an upper bound on sub-optimal DLMs (Theorem 6) and
a loWer bound on global minima (Theorem 9).
7
Under review as a conference paper at ICLR 2018
Figure 5.2: Gaussian data: convergence of the
MSE to differentiable critical points, as indica-
ted by the convergence of the neural inputs to
distinctly non-zero values. We trained MNNs
with one hidden layer on the Gaussian dataset from
Figure 5.1, with various widths d = d0 = d1 and
N = d2/5 for 1000 epochs, then decreased the
learning rate exponentially for another 1000 epo-
chs. This was repeated 30 times. For all d and
repeats, we see that (left) the final absolute value
of the minimal neural input (i.e., mini,n wi>x(n) )
in the range of 10-3 - 100, which is much larger
then (right) the final MSE error for all d and all
repeats - in the range 10-31 - 10-7.
Convergence of SGD to DLMs. These re-
sults suggest a mechanism through which low
training error is obtained in such MNNs. Ho-
wever, they do not guarantee it. One issue is
that sub-optimal DLMs may have exponentially
large basins of attraction. We see two possi-
ble paths that might address this issue in future
work, using additional assumptions on y . One
approach is to show that, with high probability,
no sub optimal DLM falls within the vanishingly
small differentiable regions we bounded in The-
orem 6. Another approach would be to bound
the size of these basins of attraction, by showing
that sufficiently large of number of differentia-
ble regions near the DLM are also vanishingly
small (other methods might also help here (Free-
man & Bruna, 2016)). Another issue is that
SGD might get stuck near differentiable saddle
points, if their Hessian does not have strictly
negative eigenvalues (i.e., the strict saddle pro-
perty (Sun et al., 2015)). It should be straig-
htforward to show that such points also have
exponentially vanishing angular volume, similar
to sub-optimal DLMs. Lastly, SGD might also
converge to non-differentiable critical points, which we discuss next.
Non-differentiable critical points. The proof of Theorem 6 stems from a first order necessary
condition (Lemma 2): (A ◦ X) e = 0, which is true for any DLM. However, non-differentiable
critical points, in which some neural inputs are exactly zero, may also exist (though, numerically, they
don’t seem very common - see Figure 5.2). In this case, to derive a similar bound, we can replace the
condition with P (A ◦ X) e = 0, where P is a projection matrix to the subspace orthogonal to the
non-differentiable directions. As long as there are not too many zero neural inputs, we should be able
to obtain similar results. For example, if only a constant ratio r of the neural inputs are zero, we can
simply choose P to remove all rows of (A ◦ X) corresponding to those neurons, and proceed with
exactly the same proof as before, with d1 replaced with (1 - r) d1. It remains a theoretical challenge
to find reasonable assumptions under which the number of non-differentiable directions (i.e., zero
neural inputs) does not become too large.
Related results. Two works have also derived related results using the (A ◦ X) e = 0 condition
from Lemma 2. In (Soudry & Carmon, 2016), it was noticed that an infinitesimal perturbation of A
makes the matrix A ◦ X full rank with probability 1 (Allman et al., 2009, Lemma 13) - which entails
that e = 0 at all DLMs. Though a simple and intuitive approach, such an infinitesimal perturbation is
problematic: from continuity, it cannot change the original MSE at sub-optimal DLMs - unless the
weights go to infinity, or the DLM becomes non-differentiable - which are both undesirable results.
An extension of this analysis was also done to constrain e using the singular values of A ◦ X (Xie et al.,
2016), deriving bounds that are easier to combine with generalization bounds. Though a promising
approach, the size of the sub-optimal regions (where the error is high) does not vanish exponentially
in the derived bounds. More importantly, these bounds require assumptions on the activation kernel
spectrum γm, which do not appear to hold in practice (e.g., (Xie et al., 2016, Theorems 1,3) require
mγm 1 to hold with high probability, while mγm < 10-2 in (Xie et al., 2016, Figure 1)).
Modifications and extensions. There are many relatively simple extensions of these results: the
Gaussian assumption could be relaxed to other near-isotropic distributions (e.g., sparse-land model,
(Elad, 2010, Section 9.2)) and other convex loss functions are possible instead of the quadratic loss.
More challenging directions are extending our results to MNNs with multi-output and multiple hidden
layers, or combining our training error results with novel generalization bounds which might be better
suited for MNNs (e.g., (Feng et al., 2016; Sokolic et al., 2016; Dziugaite & Roy, 2017)) than previous
approaches (Zhang et al., 2017a).
8
Under review as a conference paper at ICLR 2018
References
Elizabeth S. Allman, Catherine Matias, and John A. Rhodes. Identifiability of parameters in latent structure
models with many observed variables. Annals ofStatistics, 37(6 A):3099-3132, 2009. ISSN 00905364. doi:
10.1214/09-AOS689.
A Andoni, R Panigrahy, G Valiant, and L Zhang. Learning Polynomials with Neural Networks. In ICML, 2014.
Pierre Baldi. Linear Learning: Landscapes and Algorithms. Advances in Neural Information Processing Systems
1, (1):65-72, 1989.
EriC B. Baum. On the capabilities of multilayer perceptrons. Journal OfComplexity, 4(3):193-215, 1988. ISSN
10902708. doi: 10.1016/0885-064X(88)90020-9.
L Bottou. Online learning and stochastic approximations. In On-line learning in neural networks, pp. 9-42.
1998. ISBN 978-0521117913.
Alon Brutzkus and Amir Globerson. Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs.
arXiv, 2017.
Ronald W. Butler. Saddlepoint Approximations with Applications. 2007. ISBN 9780511619083. doi: 10.1017/
CBO9780511619083.
Yingtong Chen and Jigen Peng. Influences of preconditioning on the mutual coherence and the restricted
isometry property of Gaussian/Bernoulli measurement matrices. Linear and Multilinear Algebra, 64(9):
1750-1759, 2016. ISSN 0308-1087. doi: 10.1080/03081087.2015.1116495.
Anna Choromanska, Mikael Henaff, Michael Mathieu, G6rard Ben Arous, and Y LeCun. The Loss Surfaces of
Multilayer Networks. AISTATS15, 38, 2015.
T M Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern
recognition. Electronic Computers, IEEE Transactions on, (3):326-334, 1965.
G Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and
Systems (MCSS), 2:303-314, 1989.
Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio.
Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. NIPS, pp.
1-9, 2014. ISSN 10495258.
Simon S. Du, Jason D. Lee, and Yuandong Tian. When is a Convolutional Filter Easy To Learn? arXiv, sep
2017.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing Nonvacuous Generalization Bounds for Deep
(Stochastic) Neural Networks with Many More Parameters than Training Data. ArXiv, 2017.
Michael Elad. Sparse and redundant representations: from theory to applications in signal and image processing.
Springer New York, New York, NY, 2010.
Jiashi Feng, Tom Zahavy, Bingyi Kang, Huan Xu, and Shie Mannor. Ensemble Robustness of Deep Learning
Algorithms. ArXiv, feb 2016.
C.	Daniel Freeman and Joan Bruna. Topology and Geometry of Deep Rectified Network Optimization Landsca-
pes. ArXiv: 1611.01540, 2016.
K. Fukumizu and S. Amari. Local minima and plateaus in hierarchical structures of multilayer perceptrons.
Neural Networks, 13:317-327, 2000. ISSN 08936080. doi: 10.1016/S0893-6080(00)00009-5.
Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe. Qualitatively characterizing neural network optimization
problems. In ICLR, 2015.
Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 14(1):76-86, 1992. ISSN 01628828. doi: 10.1109/34.107014.
B D Haeffele and R Vidal. Global Optimality in Tensor Factorization, Deep Learning, and Beyond.
ArXiv:1506.07540, (1):7, 2015.
Moritz Hardt and Tengyu Ma. Identity Matters in Deep Learning. ICLR, pp. 1-19, 2017.
K He, X Zhang, S Ren, and J. Sun. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectifiers: Surpassing Human-
Level Performance on ImageNet Classification. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 1026-1034, 2015. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV.2015.123.
K Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(1989):251-257,
1991.
Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. Extreme learning machine: Theory and applications.
Neurocomputing, 70(1-3):489-501, 2006. ISSN 09252312. doi: 10.1016/j.neucom.2005.12.126.
M Janzamin, H Sedghi, and A Anandkumar. Beating the Perils of Non-Convexity: Guaranteed Training of
Neural Networks using Tensor Methods. ArXiv:1506.08473, pp. 1-25, 2015.
Kenji Kawaguchi. Deep Learning without Poor Local Minima. In NIPS, 2016.
Diederik P Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimization. arXiv preprint
arXiv:1412.6980, pp. 1-13, 2014.
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv:1404.5997, 2014.
9
Under review as a conference paper at ICLR 2018
YLeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444, 2015. ISSN
0028-0836. doi: 10.1038/nature14539.
Jason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient Descent Converges to
Minimizers. Conference on Learning Theory, 2016.
Yuanzhi Li and Yang Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU Activation. arXiv,
may 2017.
Roi Livni, S Shalev-Shwartz, and Ohad Shamir. On the Computational Efficiency of Training Neural Networks.
NIPS, 2014.
Haihao Lu and Kenji Kawaguchi. Depth Creates No Bad Local Minima. ArXiv, (2014):1-9, 2017.
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic
models. In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing, pp. 6, 2013.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. Arxiv, 2017.
Nils J. Nilsson. Learning machines. McGraw-Hill New York, 1965.
R Pemantle. Nonconvergence to unstable points in urn models and stochastic approximations. The Annals of
Probability, 18(2):698-712, 1990.
Jeffrey Pennington and Yasaman Bahri. Geometry of Neural Network Loss Surfaces via Random Matrix
Theory. Proceedings of the 34th International Conference on Machine Learning, 70:2798-2806, 2017. ISSN
1938-7228.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity
in deep neural networks through transient chaos. In NIPS, 2016.
Mark Rudelson and Roman Vershynin. Non-asymptotic Theory of Random Matrices: Extreme Singular
Values. Proceedings of the International Congress of Mathematicians, pp. 1576-1602, 2010. doi: 10.1142/
9789814324359_0111.
Itay Safran and Ohad Shamir. On the Quality of the Initial Basin in Overspecified Neural Networks. In ICML,
2016.
A M Saxe, J L. McClelland, and S Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear
neural networks. ICLR, 2014.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Weight Sharing is Crucial to Succesful Optimization.
jun 2017.
Ohad Shamir. Distribution Specific Hardness of Learning Neural Networks. arXiv preprint arXiv:1609.01037,
pp. 1-26, 2016.
Hao Shen. Designing and Training Feedforward Neural Networks: A Smooth Optimisation Perspective. ArXiv,
(i):1-19, 2016.
Jir^ Sima. Training a single sigmoidal neuron is hard. Neural computation, 14(11):2709-28, 2002. ISSN
0899-7667. doi: 10.1162/089976602760408035.
D Slepian. The One Sided Problem for Gaussian Noise. Bell System Technical Journal, 1962.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust Large Margin Deep Neural
Networks, 2016.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization landscape
of over-parameterized shallow neural networks. arXiv, jul 2017.
D.	Soudry and Y Carmon. No bad local minima: Data independent training error guarantees for multilayer
neural networks. In arXiv:1605.08361, 2016.
Ju Sun, Qing Qu, and John Wright. When Are Nonconvex Problems Not Scary? arXiv:1510.06096 [cs, math,
stat], pp. 1-6, 2015.
Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of deep
networks. arXiv:1611.06310, pp. 1-13, 2016.
Yuandong Tian. Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with
ReLU nonlinearity. Submitted to ICLR, 2017.
L. Welch. Lower bounds on the maximum cross correlation of signals. IEEE Transactions on Information
Theory, 20(3):397-399, may 1974. ISSN 0018-9448. doi: 10.1109/TIT.1974.1055219.
Bo Xie, Yingyu Liang, and Le Song. Diversity Leads to Generalization in Neural Networks. pp. 1-23, 2016.
Xiao Hu Yu. Can Backpropagation Error Surface Not Have Local Minima. IEEE Transactions on Neural
Networks, 3(6):1019-1021, 1992. ISSN 19410093. doi: 10.1109/72.165604.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
requires rethinking generalization. In ICLR, 2017a.
Qiuyi Zhang, Rina Panigrahy, Sushant Sachdeva, and Ali Rahimi. Electron-Proton Dynamics in Deep Learning.
arXiv:1702.00458, pp. 1-31, 2017b.
Kai Zhong, Ut-Austin Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery Guarantees
for One-hidden-layer Neural Networks. ICML, jun 2017.
Pan Zhou and Jiashi Feng. The Landscape of Deep Learning Algorithms. may 2017.
10
Under review as a conference paper at ICLR 2018
Supplementary information - Appendix
The appendix is divided into three parts. In part I we prove all the main theorems mentioned in the
paper. Some of these rely on other technical results, which we prove later in part II. Lastly, in part III
we give additional numerical details and results. First, however, we define additional notation (some
already defined in the main paper) and mention some known results, which we will use in our proofs.
Extended Preliminaries
• The indicator function I (A) ,
10
, if A
, else ,
for any event A.
•	Kronecker’s delta δij , I (i = j).
•	The Matrix Id as the identity matrix in Rd×d, and Id×k is the relevant Rd×k upper left
sub-matrix of the identity matrix.
•	[L] , {1,2,...,L}
•	The vector mn as the n’th column of a matrix M, unless defined otherwise (then mn will
be a row of M).
•	M > 0 implies that ∀i, j : Mij > 0.
•	MS is the matrix composed of the columns of M that are in the index set S.
•	A property holds “M-almost everywhere” (a.e. for short), if the set of entries of M for
which the property does not hold has zero measure (Lebesgue).
•	kvk0 = Pid=1 I (vi > 0) is the L0 “norm” that counts the number of non-zero values in
v ∈ Rd.
•	If X 〜N (μ, Σ) the X is random Gaussian vector.
•	φ (x)，√2∏ exp (-2x2) as the univariate Gaussian probability density function.
•	Φ (x) , -x∞ φ (u) du as the Gaussian cumulative distribution function.
•	B (x, y) as the beta function.
Lastly, we recall the well known Markov Inequality:
Fact 11. (Markov Inequality) For any random variable X ≥ 0, we have ∀η > 0
EX
P(X ≥ η) ≤ 7
Part I
Proofs of the main results
7 FIRST ORDER CONDITION: PROOF OF LEMMA 2
Lemma 12. (Lemma 2 restated) At all DLMs in DA (X) the residual error e is identical, and
furthermore
(A ◦ X)e = 0.	(7.1)
Proof. Let W = [w1,..., wd1]> ∈ DA (X), G，A ◦ X ∈ Rd0dι×N, W = diag(z) W =
[Wι,..., Wd∕> and W, vec (W >) ∈ Rd0d1 , where diag (v) is the diagonal matrix with v in its
11
Under review as a conference paper at ICLR 2018
diagonal, and vec (M) is vector obtained by stacking the columns of the matrix M on top of one
another. Then, we can re-write the MSE (eq. (2.3)) as
MSE= Ndy - G>w∣∣2 = N kek 2,	(7.2)
where G>W is the output of the MNN. Now, if (W, z) is a DLM of the MSE in eq. (2.3), then there
is no infinitesimal perturbation of (W, z) which reduces this MSE.
Next, for each row i, we will show that ∂MSE∕∂Wi = 0, since otherwise we can find an infinitesimal
perturbation of (W, z) which decreases the MSE, contradicting the assumption that (W, z) is a local
minimum. For each row i, we divide into two cases:
First, we consider the case Zi = 0. In this case, any infinitesimal perturbation qi in Wi can be
produced by an infinitesimal perturbation in Wi: Wi + qi = (Wi + qi∕zi)zi. Therefore, unless the
gradient ∂MSE∕∂W% is equal to zero, we can choose an infinitesimal perturbation qi in the opposite
direction to this gradient, which will decrease the MSE.
Second, we consider the case zi = 0. In this case, the MSE is not affected by changes made
exclusively to wi. Therefore, all Wi derivatives of the MSE are equal to zero (∂k MSE∕∂k wi, to
any order k). Also, since we are at a differentiable local minimum, ∂MSE∕∂z% = 0. Thus, using a
Taylor expansion, if we perturb (wi, Zi) by (Wi,Zi) then the MSE is perturbed by
∂∂
ZiW >V钎 MSE + O(^2)
∂wi ∂Zi
Therefore, unless ∂2MSE/ (∂w%∂z%) = 0 we can choose Wi and a sufficiently small Z such that the
MSE is decreased. Lastly, using the chain rule
∂	∂
∂Zi ∂Wi
MSE
∂
百WiMSE.
Thus, ∂MSE∕∂Wi = 0. This implies that W is also a DLM5 of eq. (7.2), which entails
0= - N 旦 MSE= G (y - G>W) .	(7.3)
2 ∂w i	'	'
Since G = A ◦ X and e = y - G> W this proves eq. (7.1). Now, for any two solutions W1 and W2
of eq. (7.3), we have
0 = G (y - G>Wι) - G (y - G>Wι) = GG> (W2 - Wι).
Multiplying by (W2 - Wι)>from the left we obtain
∣∣G> (W2 — Wι)∣∣2 = 0 ⇒ G> (W2 — Wι) = 0 .
Therefore, the MNN output and the residual error e are equal for all DLMs in DA (X).	口
8 Sub-optimal differentiable local minima: Proof of Theorem 6
AND ITS COROLLARY
Theorem 13. (Theorem 6 restated) Given assumptions 1-4, the expected angular volume of sub-
optimal DLMs, with MCE > > 0, is exponentially vanishing in N as
Ex~NV (Le (X, y)) ≤ exp (-YeN3/4 [did。]”),
where γ , 0.23 max [limN →∞ (d0 (N) /N) , ]3/4 if ρ 6= {0, 1}, and γ , 0.233/4 if ρ = 0.
To prove this theorem we upper bound the angular volume of Le (definition 4), i.e., differentiable
regions in which there exist DLMs with MCE > > 0. Our proof uses the first order necessary
condition for DLMs from Lemma 2, (A ◦ X) e = 0, to find which configurations of A allow for
5Note that the converse argument is not true - a DLM in W might not be a DLM in (W, Z).
12
Under review as a conference paper at ICLR 2018
a high residual error e with MCE > > 0. In these configurations A ◦ X cannot have full rank,
and therefore, as we show (Lemma 14 below), A = a (WX) must have a low rank. However,
A = a (WX) has a low rank with exponentially low probability when X 〜N and W 〜N
(Lemmas 15 and 16 below). Thus, we derive an upper bound on EX〜NV (Le (X, y)).
Before we begin, let us recall some notation: [L] , {1, 2, . . . , L},M > 0 implies that ∀i, j : Mij >
0, MS is the matrix composed of the columns ofM that are in the index set S, kvk0 as the L0 “norm”
that counts the number of non-zero values in v. First we consider the case ρ 6= 0. Also, we denote
Kr , max [N, rd0] .
First we consider the case ρ 6= 0.
From definition 3 of the angular volume
EX〜NV (Le (X, y))=P(X,y)〜Px,y,W〜N (W ∈ Le (X, y))
≤)P(x,y)〜Px,y W 〜N (∃A ∈{ρ, 1}d1×N , W ∈ Da (X), V ∈ RN : (A ◦ X) V = 0 ,N ≤ k vk0)
=)Pχ 〜N ,w 〜N (∃A ∈ {ρ, 1}d1×N , W ∈ Da (X), V ∈ RN : (A ◦ X) V = 0 ,N ≤ k v∣Q
(3)
≤PX〜N,w〜N (∃S ⊂ [N] : |S| ≥ max [Ne, rank (a (WXS)) do + 1])
=EX〜N [Pw〜N (∃S ⊂ [N]: |S| ≥ max [Ne, rank (a (WXS)) do + 1] [X)]
(4)	ΓN∕do	一
≤EX〜N EPW〜N (∃S ⊂ [N] : |S| = Kr, rank(a (WXS)) = r|X)
r=1
(5)	N/d0
≤ EX 〜NIE E	PW〜N (rank (a (WXS)) = r|X) I ,	(8.1)
r=1 S:|S|=Kr
where
1.	If we are at DLM a in DA (X), then Lemma 2 implies (A ◦ X) e = 0. Also, if e(n) = 0 on
some sample, we necessarily classify it correctly, and therefore MCE ≤ keko /N. Since
MCE > e in Le this implies that Ne < keko . Thus, this inequality holds for V = e.
2.	We apply assumption 1, that X 〜 N.
3.	Assumption 4 implies dodι>N log4 N ≥ N. Thus, we can apply the following Lemma,
proven in appendix section 12.1:
Lemma 14. Let X ∈ Rd0×N, A ∈ {ρ, 1}d1 ×N, S ⊂ [N] and dod1 ≥ N. Then, simultane-
ously for every possible A and S such that
|S| ≤ rank (AS) do ,
we have that, X-a.e., @V ∈ RN such that vn 6= 0 ∀n ∈ S and (A ◦ X) V = 0 .
4.	Recall that Kr , max [Ne, rdo]. We use the union bound over all possible ranks r ≥ 1:
we ignore the r = 0 case since for ρ 6= 0 (see eq. (2.1)) there is zero probability that
rank (a (WXS)) = 0 for some non-empty S. For each rank r ≥ 1, it is required that
|S| > Kr = max [Ne, rdo], so |S| = Kr is a relaxation of the original condition, and thus
its probability is not lower.
5.	We again use the union bound over all possible subsets S of size Kr .
13
Under review as a conference paper at ICLR 2018
Thus, from eq. (8.1), we have
Ex~nV (Le (X, y))
N/d0
≤ X X Ex~N [Pw~N (rank(a (WXS)) = r|X)]
r=i S:|S|=Kr
N/d0
(=)X ( K ) PX~N,W~N (rank (a (WXKr])) = r)
r=i	r
(2)	N/d0	N	2
≤ X ( N) 2Kr +rd0(logd1+logKr)+r Px~n,W~N (WX[Kr∕2] > 0)
r=i	r
≤) X0 ( N ) 2Kr +rd0(log dι+log Kr )+Mxp (-0.2Κr (2哈)
(4)	N/d0
≤ X 2n log NeXP (-0.23N3/4 [dido]i/4 max [e, rdo∕N]3/4)
r=i
(5)
≤ exp (-YeN3/4 [dido]1/4).
(8.2)
1.
2.
3.
4.
5.
Since we take the expectation over X, the location of S does not affect the probability.
Therefore, we can set without loss of generality S = [Kr].
Note that r ≤ N∕d0< min [do, di] from assumptions 3 and 4. Thus, with k = Kr ≥ do, We
apply the following Lemma, proven in appendix section 12.2:
Lemma 15. Let X ∈ Rd0 ×k be a random matrix with independent and identically distribu-
ted columns, and W ∈ Rd1×d0 an independent standard random Gaussian matrix. Then, in
the limit min [k, do, di] >r,
P (rank (a (WX)) = r) ≤2k+rd0(log d1+log k)+r2P (WX[[k∕2∕ > 0).
Note that Kr ≥ Ne=N > 2di, and min [Kr, do, di] >dodi/K >1 from assumptions 2
and 4. Thus, we apply the following Lemma (with C = X> ,B = W>, M = do, L = di
and N = Kr/2), proven in appendix section 12.3:
Lemma 16. Let C ∈ RN×M and B ∈ RM×L be two independent standard random
Gaussian matrices. Without loss of generality, assume N ≥ L, and denote α , M L/N.
Then, in the regime M ≤ N and in the limit min [N, M, L] >a>1, we have
P (CB > 0) ≤ exp (-0.4Nai/4).
We use rdo ≤ N, (N ) ≤ 2n ,K ≤ N, and di< N (from assumption 4) and r2 ≤
N2∕d2<N (from assumption (3)) to simplify the Combintaorial expressions.
First, note that r = 1 is the maximal term in the sum, so we can neglect the other, exponenti-
ally smaller, terms. Second, from assumption 3 we have do≤N, so
lim	0.23 max	[e,	do	(N)	/N]3/4	= 0.23 max	he,	lim	do (N) /Ni	= γ .
N→∞	N→∞
Third, from assumption 4 we have N log4 N< dodi ,so the 2n log N term is negligible.
Thus,
Ex~NV (Le (X, y)) ≤exp (-YeN3/4 [did0]i/4).
which proves the Theorem for the case ρ 6= 0.
(8.3)
Next, we consider the case ρ = 0. In this case, we need to change transition (4) in eq. (8.1), so the
sum starts from r = 0, since now we can have rank (a (WXS)) = 0. Following exactly the same
14
Under review as a conference paper at ICLR 2018
logic (except the modification to the sum), We only need to modify transition (5)in eq. (8.2) - since
now the maximal term in the sum is at r = 0. This entails γ = 0.233/4.
Corollary 17. (Corollary 7 restated) Given assumptions 1-4, for any δ > 0 (possibly a vanishing
function of N), we have, with probability 1 - δ, that the angular volume of sub-optimal DLMs, with
MCE > > 0, is exponentially vanishing in N as
V (Le (X, y)) ≤ 1 exp (-YeN3/4 [dιdo]1∕4)
Proof. Since V (L (X, y)) ≥ 0 We can use Markov’s Theorem (Fact 11) ∀η > 0:
PX-N IV(Le (X,y …> 1 - EX-N V (Le (X, y))
η
denoting η = ∣Ex~nV (Le (X, y)), and using Theorem (6) we prove the corollary.
1 - δ < Px-N (V (Le (X, y)) < δEχ-NV (Le (X, y)))
< PX-N (V (Le (X, y)) ≤δ exp (-YeN3/4 [dido]1/4))
where we note that replacing a regular inequality< with inequality in the leading order≤ only removes
constraints, and therefore increases the probability.	口
9 Construction of global minima: Proof of Theorem 8:
Recall the LReLU non-linearity
f(x) , ρx , ifx<0
f (x)	x , if x ≥ 0
in eq. (2.1), where ρ 6= 1.
Theorem 18. (Theorem 8 restated) For any y ∈ {0, 1} N and X ∈ Rd0×N almost everywhere
we find matrices W* ∈ Rdι ×d0 and z* ∈ RdL such that y = f (W*X)> z*, where dɪ ，
4 \N/ (2do 一 2)] and ∀i,n : w>x(n) = 0. Therefore, every MNN with di ≥ d* has a DLM
which achieves zero error e = 0.
We prove the existence of a solution (W*,z*), by explicitly constructing it. This construction is
a variant of (Baum, 1988, Theorem 1), except we use LReLU without bias and MSE - instead of
threshold units with bias and MCE. First, we note that for any i > 2 > 0, the following trapezoid
function can be written as a scaled sum of four LReLU:
I 0	, if |x| > €i
τ (x) ,	1	, if |x| ≤ 2	(9.1)
〔e-2	, if e? < |x| ≤ ei
=----------；---[f (X + EI) - f (X + S) - f (X -⑴ + f (X - EI)].
i - 2 1 - ρ
Next, we examine the set of data points which are classified to 1: S+ , n ∈ [N] |y(n) = 1 .
Without loss of generality, assume |S+1 ≤ N. We partition S+ to
K
-d -
do - 1
≤
N
2 (do - I)
subsets {S+ }K=i, each with no more than do _ 1 samples. For almost any dataset we can find K
hyperplanes passing through the origin, with normals {Wi}K=ι such that each hyperplane contains all
do - 1 points in subset Si+, i.e.,
W >Xs+ =0,	(9.2)
i
15
Under review as a conference paper at ICLR 2018
but no other point, so ∀n / S+ : W> x(n) = 0,
If €1, €2 in eq. (9.1) are sufficiently small (∀n / S+ : ∣W>x(n) ∣ > ∈ι) then We have
T (W>χ(n))={0 ,e; /S+.
Then We have
, if n / S +
, else
(9.3)
Which gives the correct classification on all the data points. Thus, from eq. (9.1), We can construct a
MNN With
d1 =4K
hidden neurons Which achieves zero error. This is straightforWard to do if We have a bias in each
neuron. To construct this MNN even without bias, we first find a vector w^i such that
W> [Xs+ , W,] =[1,∙∙∙, 1,1,0].	(9.4)
Note that this is possible since [x$+ , WJ has full rank X-a.e. (the matrix X$+ / Rd0×d0-1 has,
X-a.e., one zero left eigenvector, which is W,, according to eq. (9.2)). Additionally, we can set
kWik = kWik ,
(9.5)
since changing the scale of W, would not affect the validity of eq. (9.2). Then, we denote
(1)	(2)
Wi , Wi + €1Wi ； Wi , Wi + €2Wi
(3)	(4)
Wi ，Wi — €2Wi ； Wi ，Wi — €1Wi .
Note, from eqs. (9.2) and (9.4) that this choice satisfies
卜 1	, if j = 1
∀n /Si+ : W(j)TXS)=卜2	, ifj = 2 .	(9.6)
i i	I —€2	, if j = 3
I—€1 , ifj=4
Also, to ensure that ∀n // Si+ the sign of Wi(j) X(n) does not change for different j, for some β, γ < 1
we define
_ m minn∈S+ lW>x(n)l	_
€1 = β	pɪ—(^yy , €2 = Y€1,
maxn∈S+ |W>X(n)|
where with probability 1, mi□n∈s+ ∣W>x(n) ∣ > 0 and maXn/s+ [W >x(n)| > 0. Defining
Wi , hWi(1),Wi(2),Wi(3),Wi(4)i> / R4K×d0
zi , [1, —1, —1, 1] / R4
(9.7)
(9.8)
and combining all the above facts, we have
f (Wix(n)>zi
1	1
€1 — €2 1 — ρ
f Wi(1)>x(n)	— f Wi(2)> x(n)	— f Wi(3)>x(n) + f Wi(3)>x(n)
€1 — €21—ρ [f (W>x(n) + €1W>x(n)) — f (W>X + €2W>x(n))
—f (W>x(n) — €2W>x(n)) + f (W>x(n) — €1W>x(n))]
1	, ifn/Si+
0 , else
16
Under review as a conference paper at ICLR 2018
Thus, for
W*
z*
W1> , . . . , WK> > ∈ R4×d0
士 E ∙ κ..., zK ] ∈ R4K
we obtain a MNN that implements
f W*x(n)> z* =	10	,,ieflsne∈S+
and thus achieves zero error. Clearly, from this construction, if wi is a row of W*, then ∀n ∈ Si+,∀i :
wi>x(n) ≥ 2, and with probability 1 ∀n ∈/ Si+,∀i : wi>x(n) > 0, so this construction does not
touch any non-differentiable region of the MSE.
10	Global minima: Proof of Theorem 9
Theorem 19. (Theorem 9 restated). Given assumptions 1-3, we Set δ= ^8d-1/2 +
2d1/2 √log do /N and d* = 2N∕do , or if assumption 5 holds, we set d； as in this assumption.
Then, with probability 1 - δ, the angular volume of global minima is lower bounded as,
V (G (X, y)): exp (-d；do log N) ≥ exp (-2Nlog N).
In this section we lower bound the angular volume of G (definition 4), i.e., differentiable regions
in which there exist DLMs with MCE = 0. We lower bound V (G) using the angular volume
corresponding to the differentiable region containing a single global minimum.
From assumption 4, We have dodι>N, so We can apply Theorem 8 and say that the labels are
generated using a (X, y) -dependent MNN: y = f (W*X)> z* with target weights W* =
[w*>,..., w*> ∈ Rdι×d0 and z* ∈ Rd1. If, in addition, assumption 5 holds then we can
assume W* and z* are independent from (X, y). In both cases, the folloWing differentiable region
G(X, W*)，{W ∈ Rd1 ×d0 ∣∀i ≤ d* : sign (w>X) =Sign (w*>X)} ,	(10.1)
also contains a differentiable global minimum (just set wi = wi*, zi = zi* ∀i ≤ d1*, and zi = 0
∀i > d*1), and therefore ∀X, y and their corresponding W*, We have
~ , ..
G (X, y) ⊃G(X, W*)
(10.2)
Also, We Will make use of the folloWing definition.
Definition 20. Let X have an angular margin α from W* if all datapoints (columns in X) are at an
angle of at least α from all the Weight hyperplanes (roWs of W*) , i.e., X is in the set
(	x(n)>w*
X∈Rd0×Nl∀i,n: R)>⅛ >sinɑ∫
(10.3)
Using the definitions in eqs. (10.3) and (10.1), We prove the Theorem using the folloWing three
Lemmas.
First, In appendix section 13.2 We prove
Lemma 21. For any α, if W* is independent from W then, in the limit N → ∞, ∀X ∈ Mα (W*)
with log sin a> d-1 log do
V ⑹=PW〜N (W ∈G(X, W*))≥exp(dod1logsin a).
Second, in appendix section 13.3 We prove
17
Under review as a conference paper at ICLR 2018
Lemma 22. Let W* ∈ Rd1 ×d0 a fixed matrix independent of X. Then, in the limit N → ∞ with
d↑≤≤ do≤≤ N, the probability of not having an angular margin Sin a = 1/ (d;d°N) (eq. (10.3)) is
upper bounded by
P(X ∈ Mα(W*)) ≤ Jnd-1/2
Lastly, in appendix section 13.4 we prove
Lemma 23. Let X ∈ Rd0×N be a standard random Gaussian matrix of datapoints. Then we
can find, with probability 1, (X, y)-dependent matrices W* and z* as in Theorem 8 (where d1* ,
4 d N/ (2do 一 2)e). Moreover, in the limit N → ∞, where N∕do≤do ≤N ,for any y, we can bound
the probability of not having an angular margin (eq. (10.3)) with sin α = 1/ (d1*d0N) by
P(X ∈ Mα (w*))≤ r/8 d-1/2 + 2d0∕2√log d0
πN
Recall that ∀X, y and their corresponding W*, We have G (X, y) ⊂ G (X, W*) (eq. (10.2)). Thus,
combining Lemmas 21 with sin α = 1/ (d*1d0N) together with either Lemma 22 or 23, we prove the
first (left) inequality of Theorem 9:
V (G (X, y)) ≥exp(-d1do log N)
Next, if d* = 2N∕do or d*<N/do (is assumption 5 holds), we obtain the second (right) inequality
, - - _____. , ________ ___.
exp (—d1do log N) ≥exp(-2Nlog N).
11 Volume ratio of global and local minima: Proof of Theorem 10
Theorem 24. (Theorem 10 restated) Given assumptions 1-3, we set δ = ∏d0d-1/2 +
2d0/2 √log do /N. Then, with probability 1 — δ, the angular volume of sub-optimal DLMs, with
MCE >	> 0, is exponentially vanishing in N, in comparison to the angular volume of global
minima with MCE = 0
V (Le (X, y))
V (G (X, y))
≤exp (-γeN3/4 [dido]1") ≤exp(-γeNlog N).
To prove this theorem we first calculate the expectation of the angular volume ratio given the X-event
that the bound in Theorem 9 holds (given assumptions 1-3), i.e., V (G (X, y)) ≥ exp (-2Nlog N).
Denoting this event6 as M, we find:
Eχ~N
-V (Le (X, y))
一 V (G (X, y))
(≤)Ex~n [V (Le (X, y)) |M] (≤)
—	exp (-2N log N)	一
Ex~N [V (Le (X, y))]	(3 exp I-YeN3/4 [dido]1/4) (4
Pχ~N (M)exp(-2N log N) ≤ Pχ~N (M)exp(-2N log N) ≤
exp -γeN 3/4 [dido]i/4 (5)	i 4
—' / on] W )≤ exp (-YeN3/4 [dido]V4)	(11.1)
exp (-2N log N)
where
1.	We apply Theorem 9.
2.	We use the following fact
6This event was previously denoted as X ∈ Mɑ (W*) in the proof of Theorem 9, but this is not important
for this proof, so we simplified the notation.
18
Under review as a conference paper at ICLR 2018
Fact 25. For any variable X ≥ 0 and event A (where A is its complement)
E [X] = E [X|A] P (A) + E [X|A] (1 - P (A)) ≥ E [X|A] P (A)
3.	We apply Theorem 6.
4.	We apply Theorem 9.
5.	We use assumption 4, which implies YeN3/4 [dido]1/4 >2N log N.
For simplicity, in the reminder of the proof we denote
R(X)
V (Le (X, y))
V (G (X, y)).
From Markov inequality (Fact 11), since R (X) ≥ 0, we have ∀η (N) > 0:
PX〜N [R (X) ≥ η (N) |M] ≤ Ex~N)RNX) |M]
On the other hand, from fact 25, we have
1	- PX〜N [R (X) < η (N) |M] ≥ 1 - Px~N)[R (X)<(N)].
PX〜N (M)
Combining Eqs. (11.2)-(11.3) we obtain
EX〜N [R (X) [M] 、 P	PX〜N [R (X) < η (N)]
≥ 1
η (N)	≥	PX 〜N (M)	,
and so
PX〜N (M) - PX〜N (M) EX~N)RNX) |M] ≤ PX〜N [R (X) < η (N)].
We choose
η (N) = NPX〜N (M)EX〜N [R (X) ∣M]=exp (-γeN3/4 [dido] 1/4)
so that
PX〜N (M) - N ≤ PX〜N [r (X) ≤exp (-YeN3/4 [dιdo]1∕4)i .
Then, from Theorem 9 we have
1 - PX 〜N (M)≤ r d-1/2+2do/2 严.
πN
(11.2)
(11.3)
(11.4)
so we obtain the first (left) inequality in the Theorem (10)
∕8 ,-1/2 , 2d0"√lθg do ,	ΓV (Le (X, y))j	/	^3/4 u J 11/4^"|
Vndo	+ -N- ≥1- Px~N [v (G (X, y)) ≤exp(-YeN / [dldo]/)].
Lastly, we note that assumption 4 implies γeN3/4 [d1d0]1/4 >N log N, which proves the second
(right) inequality of the theorem.
19
Under review as a conference paper at ICLR 2018
Part II
Proofs of technical results
In this part we prove the technical results used in part I.
12 Upper bounding the angular volume of sub-optimal
differentiable local minima: Proofs of Lemmas used in Section
8
12.1	Proof of Lemma 14
In this section we will prove Lemma 14 in subsection 12.3.3. Recall the following definition
Definition 26. Let
A = [a1, . . . ,aN] ; X = [x1,. . . ,xN] ,
where X ∈ Rd0×N and A ∈ Rd1 ×N. The Khatari-Rao product between the two matrices is defined
as
A ◦ X ， [aι 0 xι, a2 0 x2,…aN 0 XN]	(12.1)
a11x1 a12x2	. . .
.
=	a21X1 a22X2	..	.
∖	....	...)
Lemma 27. (Lemma 14 restated) Let X ∈ Rd0 ×N, A ∈ {ρ, 1}d1 ×N, S ⊂ [N] and d0d1 ≥ N. Then,
simultaneously for every possible A and S such that
|S| ≤ rank(AS)d0,
we have that, X-a.e., @v ∈ RN such that vn 6= 0 ∀n ∈ S and (A ◦ X) v = 0 .
Proof. We examine specific A ∈ {ρ, 1}d1×N and S ⊂ [N], and such that |S| ≤ dSd0, where we
defined dS , rank (AS). We assume that dS ≥ 1, since otherwise the proof is trivial. Also, we
assume by contradiction that ∃v ∈ RN such that vi 6= 0 ∀i ∈ S and (A ◦ X) v = 0 . Without loss
of generality, assume that S = {1, 2, ..., |S|} and that a1, a2, ..., adS are linearly independent. Then
|S|
(A ◦ X) V = Evnak,nXn =0	(12.2)
n=1
for every 1 ≤ k ≤ d1. From the definition of S we must have vn 6= 0 for every 1 ≤ n ≤ |S|. Since
a1, a2, ..., adS are linearly independent, the rows of AdS = [a1, a2, ..., adS] span a dS -dimensional
space. Therefore, it is possible to find a matrix R such that RAdS = [IdS ×dS, 0dS×(d1-dS)]>, where
0i×j is the all zeros matrix with i columns and j rows. Consider now AS ◦ XS, i.e., the matrix
composed of the columns of A ◦ X in S. Applying R0 = R 0 Id0 to AS ◦ XS, turns (12.2) into
d0dS equations in the variables v1, ..., v|S|, of the form
|S|
vkxk +): vnak,nxn = 0	(12.3)
n=dS +1
for every 1 ≤ k ≤ dS . We prove by induction that for every 1 ≤ d ≤ dS, the first d0d equations
are linearly independent, except for a set of matrices X of measure 0. This will immediately imply
|S| > dSd0, or else eq. 12.2 cannot be true for V 6= 0. which will contradict our assumption,
as required. The induction can be viewed as carrying out Gaussian elimination of the system of
equations described by (12.3), where in each elimination step we characterize the set of matrices X
that for which that step is impossible, and show it has measure 0.
20
Under review as a conference paper at ICLR 2018
For d = 1, the first do equations read v〔xi + PnS= ds+1vnaι,nXn = 0, and since vι = 0, We
must have xι ∈ Span {a-^s++ιXds+ι,…，aι,∣s∣x∣s∣}. However, except for a set of measure 0
With respect to x1 (a linear subspace of Rd0 With dimension less than d0), this can only happen if
dim Span aa1,d++1xd++1, ..., aa1,|S|x|S| = d0, which implies |S| ≥ dS - 1 + d0 > d0 and also
that the first d0 rows are linearly independent (since there are d0 independent columns).
For a general d, we begin by performing Gaussian elimination on the first (d - 1) d0 equations,
resulting in a new set of rd equations, such that every new equation contains one variable that appears
in no other new equation. Let C be the set of the indices (equivalently, columns) of these variables rd
variables. From (12.3) it is clear none of the variables vd, vd+1, ..., vd+ appear in the first (d - 1) d0
equations, and therefore C ⊆ S0 = S \ {d, d + 1, ..., dS}. By our induction assumptions, except for
a set of measure 0, the first (d - 1) d0 are independent, which means that |C| = rd = (d - 1) d0.
We now extend the Gaussian elimination to the next d0 equations, and eliminate all the variables in C
from them. The result of the elimination can be written down as,
vdxd +	vn (aad,nId0 - Y) xn = 0 ,	(12.4)
n∈S0∖C
where Y is a square matrix of size d0 whose coefficients depend only on {aak,n }n∈C,d>k≥1 and on
{xn}n∈c , and in particular do not depend on Xd and {xn}n∈so∖c.
Now set	xan	=	(aad,nId0	-	Y)xn	for n ∈	S0	\ C. As in the case of d =	1,	since	vd	6=	0,
Xd ∈ Span{Xn}n∈so∖c. Therefore, for all values of Xd ∈ Rd0 but a set of measure zero (linear
subspace of with dimension less than do), we must have dimSpan{Xn}n∈so∖c = do. From the
independence of {Xn}n∈so∖c on Xd it follows that dim Span{Xn}n∈so∖c = do holds a.e. with
respect to the Lebesgue measure over X.
Whenever dimSpan{Xn}n∈so∖c = do we must have ∣S0 \ C| ≥ do and therefore
|S| > |S0| = |C|+|S0\C| ≥ (d- 1)do+do =dod.	(12.5)
Moreover, dimSpan{Xn}n∈so∖c = do implies that the do equations VdXd + Pn∈so∖c VnXn = 0
are independent. Thus, we may perform another step of Gaussian elimination on these do equations,
forming do new equations each with a variable unique to it. Denoting by C0 the set of these do
variables, it is seen from (12.4) that C0 ⊆ (S0 ∪ {d}) \ C and in particular C0 is disjoint from C.
Thus, considering the first (d - 1) do equations together with the new do equations, we see that there
is a set C ∪ C0 of dod variables, such that each variable in C ∪ C0 appears only in one of the dod
equations, and each of the dod contains only a single variable in C ∪ C0. This means that the first
dod must be linearly independent for all values of X except for a set of Lebesgue measure zero,
completing the induction.
Thus, we have proven, that for some A ∈ {ρ, 1}d1 ×N and S ⊂ [N] such that |S| ≤ rank (AS) do
the event
E (A,S) = {X ∈ Rd0×N∣∃v ∈ RN : (A ◦ X) V = 0andVn = 0, ∀n ∈ S}
has zero measure. The event discussed in the theorem is a union of these events:
Eo，	U	U	E (A,S),
A∈{ρ,1}dι ×N -S⊂[N]"S∣≤rank(A+ )d°	_
and it also has zero measure, since it is a finite union of zero measure events.	口
For completeness we note the following corollary, which is not necessary for a our main results.
Corollary 28. If N ≤ d1do, then rank (A ◦ X) = N, X-a.e., if and only if,
∀S ⊆ [N] : |S| ≤ rank (AS) do .
Proof. We define dS , rank (AS ) and A ◦ X. The necessity of the condition |S| ≤ dodS holds for
every X, as can be seen from the following counting argument. Since the matrix AS has rank dS ,
21
Under review as a conference paper at ICLR 2018
there exists an invertible row transformation matrix R, such that RAS has only dS non-zero rows.
Consider now GS = AS ◦ XS, i.e., the matrix composed of the columns of G in S. We have
GS = (RAS) ◦ XS = R0 (AS ◦ XS) = R0Gs ,	(12.6)
where R0 = R 0 L。is also an invertible row transformation matrix, which applies R separately on
the d0 sub-matrices of GS that are constructed by taking one every d0 rows. Since G0S has at most
d0dS non-zero rows, the rank of GS cannot exceed d0dS. Therefore, if |S| > d0dS, GS will not
have full column rank, and hence neither will G. To demonstrate sufficiency a.e., suppose G does
not have full column rank. Let S be the minimum set of columns of G which are linearly dependent.
Since the columns of GS are assumed linearly dependent there exists v ∈ R|S| such kvk0 = |S| and
GSV = 0. Using Lemma 28 we complete the proof.	口
12.2 Proof of Lemma 15
In this section we will prove Lemma 15 in subsection 12.3.3. This proof relies on two rather basic
results, which we first prove in subsections 12.2.1 and 12.2.2.
12.2.1	Number of dichotomies induced by a hyperplane
Fact 29. A hyperplane w ∈ d0 can separate a given set of points X = x(1), . . . , x(N) ∈ Rd0 ×N
into several different dichotomies, i.e., different results for sign w>X . The number of dichotomies
is upper bounded as follows:
X I (∃w : sign (w>X)= h>) ≤ 2 X ( Nk 1 ≤ ≤ 2Nd0 .	(12.7)
h∈{-1,1}N	k=0
Proof. See (Cover, 1965, Theorem 1) for a proof of the left inequality as equality (the Schlafli
Theorem) in the case that the columns of X are in “general position” (which holds X-a.e, see
definition in (Cover, 1965)) . If X is not in general position then this result becomes an upper bound,
since some dichotomies might not be possible.
Next, we prove the right inequality. For N = 1 and N = 2 the inequality trivially holds. For N ≥ 3,
we have
2 X1 (	N- 1 )≤) 2 XI(N	- 1)k	(≤)	2(N - 1)d0- 1	≤ 2N do	.
k=0 V k)一 k= (	)	≤ N -2	≤
where in (1) we used the bound ( N ) ≤ Nk , in (2) we used the sum of a geometric series. 口
12.2.2	A basic probabilistic bound
Lemma 30. Let H = h1>, . . . , hd> > ∈ {-1, 1}d1 ×k be a deterministic binary matrix, W =
w1>, . . . , wd> > ∈ Rd1×d0 be an independent standard random Gaussian matrix, and X ∈ Rd0 ×k
be a random matrix with independent and identically distributed columns.
P(Sign(WX) = H) ≤ b [Ri ) P (WX[bk∕2C] > °).
22
Under review as a conference paper at ICLR 2018
i=1
(2)	d1	(3)
≤ EnP(W>Xs(hi)>0|X)≤ E
i=1
Proof. By direct calculation
d1
P (sign (WX) = H) = E [P (sign (WX) = H|X)] = E Y P (sign (w>X)= h> |X)
d1
∏p (w>Xs* > 0|X)
i=1
E	P(WXS > 0|X)
S⊂[k]"S∣=[k∕2C
X	E [P (WXs > 0|X)] (=) ( b k∕2 c ) P (WX[bk∕2C] > 0).
S⊂[k]"S∣ = [k∕2C	× L J ×
(4)	(5)
=) E [P (WXs* > 0|X)] ≤ E
where
1.	We used the independence of the Wi .
2.	We define S± (h) ， {S ⊂ [k] : ±h> > 0} as the sets in which h is always posi-
tive/negative, and S (h) as the maximal set between these two. Note that Wi has a stan-
dard normal distribution which is symmetric to sign flips, so ∀S : P Wi> XS > 0|X =
P (w>Xs < 0|x).
3.	Note that ∖s (h)∣ ≥ [k/2j. Therefore, we define S* =	argmax	P (w>Xs > 0∣X).
1	1	S⊂[k]"S∣ = [k∕2C
4.	We used the independence of the wi .
5.	The maximum is a single term in the following sum of non-negative terms.
6.	Taking the expectation over X, since the columns of X are independent and identically
distributed, the location of S does not affect the probability. Therefore, we can set without
loss of generality S = [bk/2c].
□
12.2.3	Main proof: Bound on the number of configurations for a binary matrix
WITH CERTAIN RANK
Recall the function a (∙) from eq. (2.1):
a(u) , 1
ρ
, if , u > 0
, if u < 0
where ρ 6= 1.
Lemma 31. (Lemma 15 restated). Let X ∈ Rd0×k be a random matrix with independent and
identically distributed columns, and W ∈ Rd1 ×d0 an independent standard random Gaussian matrix.
Then, in the limit min [k, do, di] >r,
P (rank (a (WX))= r) ≤2k+rd0(log d1+log k)+r2P (WX[[k∕2∕ > 0).
Proof. We denote A = a (WX) ∈ {ρ, 1}d1 ×k. For any such A for which rank (A) = r, we have a
collection of r rows that span the remaining rows. There are dri possible locations for these r
spanning rows. In these rows there exist a collection of r columns that span the remaining columns.
There are kr possible locations for these r spanning columns. At the intersection of the spanning
23
Under review as a conference paper at ICLR 2018
1	1	.1	∙	Γ∙ 11	IF	∙ ɪʌ 11 T 1	, T	.1	A 1 ∙	1
rows and columns, there exist a full rank sub-matrix D. We denote A as the matrix A which rows
and columns are permuted so that D is the lower right block
W W1X1 W1X2
aI W2X1 W2X2
(12.8)
where D is an invertible r × r matrix, and we divided X and W to the corresponding block matrices
W , [W>,W>]τ ,X , [X1,X2],
with W2 ∈ Rr×d0 rows and X2 ∈ Rd0×r.
Since rank (A) = r, the first d1 — r rows are contained in the span of the last r rows. Therefore,
there exists a matrix Q such that QC = Z and QD = B. Since D is invertible, this implies that
Q = BD-1 and therefore
Z = BD-1C ,	(12.9)
i.e., B, C and D uniquely determine Z.
Using the union bound over all possible permutations from A to A, and eq. (12.9), we have
P (rank (A) = r)
≤ (d1)(k) Mrank (A)=r)
≤ ( d1 )( ： ) P (Z = BDTC)
=d d1 )( k )p (a (W1X2)[a (W2X2)]-1 a (W2X1) = a (W1X1))
(12.10)
(d1 )( k ) X	P(a (W1X2)[α (W2X2)]-1 a (W2 X1) = a (H) ISign(W1X1) = H)P(Sign(W1X1) = H)
∖ H H	,H∈{-1,1}(d1-r)×(k-r)
Using Lemma 30, we have
P (sign (W1X1 ) = H) ≤ ( b(kk-~r/2J ) P (W1X[L(fc-r)∕2j] > 0),	(12.11)
an upper bound which does not depend on H. So all that remains is to compute the sum:
X P (a (W1X2) [a (W2X2)]-1 a (W2X1) = a (H) ∣sign (W1X1) = H)
H∈{-1,1}(d1-r)×(k-r)
=	X E [p (a (W1X2) [a (W2X2)]-1 a (W2X1) = a (H) ∣W1, X1) ∣sign (W1X1) = h]
H∈{-1,1}(d1 - r) × (k-r)
(1)
≤E
(2)
≤E
≤E
≤E
X Ie (W2, X2) : a (W1X2) [a (W2X2)]-1 a (W2X1)
sign (W1X1) = H
(12.12)	'
2r2
2r2
E I (3X2 : sign(W1X2)= H)
_HE{ —1,1}(dι-r)×r
E	I (3W2 : sign (W2X1) = H) I I sign (W1X1) = H
.H∈{-1,1}r×(k-r)	_| I	.
X I (3x :sign (W1x) = h)	^X	I (∃w : sign (wτX1) = hτ)
h∈{-1,1}(dι-r)	h	1h∈{-1,1}(k-r)
^r2 gdo log(dι —r)+r £do log(k-r)+r
∣sign(W1X1) = h]
r	-
sign (W1X1) = H
a，Z B
⑶
=2rdo[log(dι-r) + log(k-r)]+r2 + 2r
(12.13)
where
24
Under review as a conference paper at ICLR 2018
1.	Given (W1, X1), and eq. (12.8), the indicator function in eq. (12.12) is equal to zero only
ifP a(W1X2) [a (W2X2)]-1 a (W2X1) = A|W1, X1 = 0, and one otherwise.
2.	This sum counts the number of values of H consistent with W1 and X1. Conditioned on
(W1,X1), D = [a (W2X2)]-1,B = a(W1X2) andC = a(W2X1) can have multiple
values, depending on W2 and X2. Also, any single value for (D, B, C) results in a single
value of H. Therefore, the number of possible values of H in eq. (12.12) is upper bounded
by the product of the number of possible values of D, B and C, which is product in the
following equation.
3.	The function Ph∈{-1,1}(k-r) I ∃w : sign w>X1 = h> counts the number of dichoto-
mies that can be induced by the linear classifier w on X1. Using eq. (12.7) we can bound
this number by 2 (k - r)d0. Similarly, the other sum can be bounded by 2 (d1 - r)r.
Combining eqs. (12.10), (12.11) and (12.13) we obtain
P (rank (A) = r) ≤
(d1 ) ( k )( b(k"r∕2c ) 2rd0[log(dLr)+log(I)]+r2 +2rP (WιX[b(k-r)∕2C] > 0).
Next, we take the log. To upper bound	Nk	, for small k we use	Nk	≤ Nk, while for
k = N/2, we use	NN/2	≤ 2N . Thus, we obtain
log P (rank (A) = r) ≤ (rd。(log (dɪ — r) + log (k — r)) + r2 + 2r) log 2	(12.14)
+r log d1 + r logk + (k - r)log2 +logP W1 X[b(k-r)∕2c] > 0 .
Recalling that W1 ∈ R(d1-r)×d0 while W ∈ Rd1 ×d0, we obtain from Jensen’s inequality
b(k — r) /2c bd — rc
log P (W1X[b(k-r)∕2C] > 0) ≤ b(	[k∕2ccbdjC log P (WX[bk/2」]> 0).
(12.15)
Taking the limit min [k, d。, di] >r on eqs. (12.14) and (12.15) We obtain
P (rank (A)= r)≤2k+rd0(Iog d1+log k)+t2P (WX^/时 > 0).
□
12.3 Proof of Lemma 16
In this section We Will prove Lemma 16 in subsection 12.3.3. This proof relies on more elementary
results, Which We first prove in subsections 12.3.1 and 12.3.2.
12.3.1	Orthant probability of a random Gaussian vector
Recall that φ (x) and Φ (x) are, respectively, the probability density function and cumulative distribu-
tion function for a scalar standard normal random variable.
Definition 32. We define the folloWing functions ∀x ≥ 0
xΦ (x)
g (X) , φ R ,	(12.16)
ψ (x) , (g 2Xx)) — log (Φ (g-i (x))) ,	(12.17)
Where the inverse function g-i (x) : [0, ∞) → [0, ∞) is Well defined since g (x) monotonically
increase from 0 to ∞, for X ≥ 0.
25
Under review as a conference paper at ICLR 2018
Lemma 33. Let Z ~ N (0, Σ) be a random Gaussian vector in RK, with a Covariance matrix
∑ij =(1 一 θKT) δmn + θKT where K》θ > 0. Then, recalling ψ (θ) in eq. (12.17), we have
logP (∀i : zi > 0) ≤ -Kψ (θ) + O (log K) .
Proof. Note that we can write Z = u+η, where U ~ N(0,(1 — θK T) IK ),and η 〜N(0, θK-1).
Using this notation, we have
P (∀i : zi > 0)
Z dη ]γ Z	du江(pi 一 θkTUi + √θkTη > 0) φ (Ui)
z∞∞ dη φ (rT-KK-，φ ⑺
J Zld ”'exp (一 丁 )
S 2∏(K-θ) Zldexp (ξ2)exp [K (logφ(ξ) 一 22)]，
φ (η)
(12.18)
where in ⑴ we changed the variable of integration to ξ = ,θ/ (K 一 θ)η. We denote, for a fixed θ,
ξ2
q(ξ)，^①⑹一 ⅛
2θ
h (ξ), S2π (K 一 θ)exp (ξ2
(12.19)
(12.20)
and ξ0 as its global maximum. Since q is twice differentiable, we can use Laplace’s method (e.g.,
(Butler, 2007)) to simplify eq. (12.18)
log
∞ h(ξ)
-∞
exp(Kq(ξ)) dξ = Kq(ξ0) + O (logK) .
(12.21)
To find ξ0, we differentiate q (ξ) and equate to zero to obtain
which implies (recall eq. (12.16))
q0 (ξ) =
φ(ξ)
Φ^B
0.
(12.22)
…ξΦ(ξ)	A
g⑹,WT = θ.
(12.23)
—
θξ
This is a monotonically increasing function from 0 to ∞ in the range ξ ≥ 0. Its inverse function can
also be defined in that range g-1 (θ) : [0, ∞] → [0, ∞]. This implies that this equation has only one
solution, ξo = g-1 (θ). Since limξ→∞ q (ξ) = -g, this ξo is indeed the global maximum of q (ξ).
Substituting this solution into q (ξ), we get (recall eq. (12.17))
∀θ > 0 ： q(ξo)= 一砂(θ) = q (g-1 (θ)) = log (Φ (g-1 (θ))) -
(12.24)
Using eq. (12.18), (12.21) and (12.24) we obtain:
log P (∀i : zi > 0)
log
∞
-∞
dξ exp
K (…)-2θ)
+ O (log K)
—K@ (θ) + O (log K).
□
26
Under review as a conference paper at ICLR 2018
Next, we generalize the previous Lemma to a general covariance matrix.
Corollary 34. Let u ~ N (0, Σ) be a random Gaussian vector in RK for which ∀n : ∑nn = 1, and
θ ≥ K maxn,m: n6=m Σnm > 0 . Then, again, for large K
log P (∀i : ui > 0) ≤ -Kψ (θ) +O(logK) .
Proof. We define U ~ N(0, ∑), with Σmn =(1 - θKT) δmn + θK-1. Note that ∀n : ∑nn =
Σnn = 1 and ∀m 6= n: Σmn ≤ Σmn. Therefore, from Slepian’s Lemma (Slepian, 1962, Lemma 1),
P (∀n : Un > 0) ≥ P (∀n : Un > 0).
Using Lemma 33 on U completes the proof.	口
12.3.2	Mutual coherence bounds
Definition 35. We define the mutual coherence of the columns of a matrix A = [aι,…,aN] ∈
RM ×N as the maximal angle between different columns
Y (A)，max	Iai ：」.
i,j:i6=j kai k kaj k
Note that Y (A) ≤ 1 and from (Welch, 1974), for N ≥ M, Y (A) ≥ JMN-I).
Lemma 36. Let A = [aι, ∙∙∙ , aN] ∈ RM×N be a standard random Gaussian matrix, and Y (A) is
the mutual coherence of it columns (see definition 35). Then
P (Y (A) > ) ≤ 2N2 exp
Proof. In this case, we have from (Chen & Peng, 2016, Appendix 1):
P(Y(A) > ) ≤ N(N- 1) exp
Ma22	M	2
4(1 + e/2) )+exp (-Z(I- a)
—
for any a ∈ (0, 1). Setting a = 1 - /2
P (Y (A) > ) ≤ N (N - 1) exp
M(1 - /2)2 2
4(1+/2)
M2
+ eχp(- 16 e
(1)
≤ N (N - 1) exp
—
≤ 2N2 exp
M2
+ exp(- 16 £
where in (1) we can assume that ≤ 1, since for ≥ 1, we have P (Y (A) > ) = 0 (recall
Y (A) ≤ 1).	口
Lemma 37. Let B = [bi, ∙∙∙ , 6L]∈ RM×L be a standard random Gaussian matrix and mutual
coherence Y as in definition 35. Then, ∀ > 0 and ∀K ∈ [L]:
P
min
S⊂[N ]:|S|=K
Y (BS) >
≤ exp
2 log (2K)
Proof. We upper bound this probability by partitioning the set of column vectors into bL/Kc subsets
Si of size |Si | = K and require that in each subset the mutual coherence is lower bounded by .
27
Under review as a conference paper at ICLR 2018
Since the columns are independent, we have
P min γ (BS) >
∖s⊂[N ]：|S| = K	J
bL/Kc
≤ Y P(∀S = {1 + (i- 1) K, 2+(1 -i) K,...,iK} : Y (BS) >e)
i=1
(1)
≤
L/K-1
Y 2K2
i=1
exp
≤ exp
2log(2K)
where in (1) we used the bound from Lemma 36.
□
12.3.3	Main proof: Orthant probability of a product Gaussian matrices
Lemma 38. (Lemma 16 restated). Let C = [ci,…，CN]> ∈ RN×M and B ∈ RM乂L be two
independent random Gaussian matrices. Without loss of generality, assume N ≥ L, and denote
a，ML/N. Then, in the regime M ≤ N and in the limit min [N, M, L] >a>>1, we have
P (CB > 0) ≤ exp (-0.4Να").
Proof. For some θ > 0, and subset S such that |S| = K < L, we have
P(CB > 0)
≤P (CBS > 0∣Y (BS) ≤ E) P (Y (BS) ≤ e) + P (CBS > 0∣Y (BS) > E) P	(Y	(BS)	>	E)
≤P (CBS > 0∣Y (BS) ≤ e) + P (y (BS) > E)
=E [[P (c>Bs > 0|Bs, Y (BS) ≤ e)] NIY (BS) ≤ Ei + P (γ (BS) > E),
where in the last equality we used the fact that the rows of C are independent	and identically
distributed.
We choose a specific subset
S* = argminSu[L]:|S|=KY (BS)
to minimize the second term and then upper bound it using Lemma 37 with θ = KE; additionally, we
apply Corollary 34 on the first term with the components of the vector u being
Ui = (BS/，(B>BS% ∈ RK ,
which is a Gaussian random vector with mean zero and covariance Σ for which ∀i : Σii = 1 and
∀i 6= j : Σij ≤ E = θK-1. Thus, we obtain
P(CB > 0) ≤ exp(-NKψ(θ) +O(NlogK)) +exp
(2K)2 - 2MK) (K - 1),
(12.25)
where we recall ψ (θ) is defined in eq. (12.17).
Next, we wish to select good values for θ and K, which minimize this bound for large (M, N, L, K).
Thus, keeping only the first order terms in each exponent (assuming L K 1), we aim to
minimize the function as much as possible
Mθ2L
f(K,θ) , eχp(-NKψ (θ)) + eχp -折
(12.26)
Note that the first term is decreasing in K, while the second term increases. Therefore, for any θ the
minimum of this function in K would be approximately achieved when both terms are equal, i.e.,
NKψ (θ) =
Mθ2L
24K3 ,
28
Under review as a conference paper at ICLR 2018
so we choose
K ~(242MLn )：	(12.27)
Substituting K (θ) into f (K, θ) yields
f (K (θ) ,θ) = 2exp (-N [ ψ3≡NML J)
To minimize this function in θ, we need to maximize the function ψ3 (θ) θ2 (which has a single
maximum). Doing this numerically gives us
θ* ≈ 23.25 ; ψ (θ*) ≈ 0.1062; ψ3 (θ*) θ2 ≈ 0.6478 .	(12.28)
Substituting eqs. (12.27) and (12.28) into eq. (12.25), we obtain
P (CB > 0)
ML	1/4
≤ exP(-N [37≡nJ	+ O (N log K))
+ exp "-N [-ML-] 1/4 + 2L j + M2 - log (2K/
+ P	[37.05NJ	+ K + 24K2 g'	)
≤ exp (-N [ ML ]1/4 + O (Nlog (ML))],
—Pi	[37.05NJ	+ V gl N))广
where in the last line We used N ≥ L,N ≥ M and min [N, M, L] >a>1. Taking the log, and
denoting α , ML/N, we thus obtain
log P (CB > 0) ≤ -0.4Na1/4 + O (N log a),
Therefore, in the limit that N → ∞ and a (N) → ∞, with α (N) <N, We have
P(CB > 0) ≤exp (-0.4Nα1∕4).
□
13 Lower bounding the angular volume of global minima: Proof
of Lemmas used in section 10
13.1	Angles between random Gaussian vectors
To prove the results in the next appendix sections, we will rely on the following basic Lemma.
Lemma 39. For any vector y and X ~ N (0, Id。), we have
P
X> y
kχkkyk
> cos ()
2sin(e)d0-1
(d0 - 1) B (1, d02^1)
(13.1)
χ>y	2u
P .....: “ < u ≤ —z1 . 1x ,	(13.2)
IkXkkyk	) - B (2, ⅛1)
where we recall that B (x, y) is the beta function.
Proof. Since N (0, Id0) is spherically symmetric, we can set y = [1, 0 . . . , 0]>, without loss of
generality. Therefore,
χ>y	2 = χ2	B (1 do - 1 ʌ
画面=χ2 + P= 2 χ2 〜【2,	「
29
Under review as a conference paper at ICLR 2018
the Beta distribution, since x2 〜X2 (1) and Pd= 2 χ2 〜 X2 (do - 1) are independent chi-square
random variables.
Suppose Z 〜B (α, β), α ∈ (0, 1), and β > 1 .
P (Z >u) = RU XaT(I - x)β-1	dx	≥	RU	1α-1(1 - x)β-1 dx	=	R01-u	XeTdx	=	(1 - Uf
B (α,β)	—	B (α, β)	B (α,β)	βB (α,β)
Therefore, for > 0,
/	d	d	do — 1
P 八 x>y	> cos2(e)! ≥ 2 "cos2 ⑹)F =	2sin(e)d0T_
UkXkkyk	>	( )) ≥(do- 1)B(1,⅛1)	(do- 1)B(1,⅛1),
which proves eq. (13.1).
Similarly, for α ∈ (0, 1) and β > 1
RUXα-1 (1 - X)β-1 dX	R U Xα-11β-1dX	Uα
P (Z < U) = J0≤ J0	=	.
B (α, β)	B (α, β)	αB (α,β)
Therefore, for > 0,
P fl X>y 2 <u2∖< _2u_
UkXkkyk ) < B (1, ⅛1),
which proves eq. (13.2).	□
13.2	Proof of Lemma 21:
Given three matrices: datapoints, X = X(1), . . . , X(N)	∈ Rd0×N, weights W =
[w>,..., w> ]> ∈ Rd1×d0, and target weights W* = [w],..., w“]	∈ Rd"d0, with
d↑ < dι,we recall the following definitions:
(l X(n)>w* l
X ∈ Rd0×Nl∀i,η: W⅛ > sinɑ∫
and
(13.3)
G (X, W*) , {W ∈ Rd1 ×d0 ∣∀i ≤ d* : sign (w>X)= sign (w*>X)} .	(13.4)
Using these definitions, in this section we prove the following Lemma.
Lemma 40. (Lemma 21 restated). For any α, if W* is independent from W then, in the limit
N → ∞, ∀X ∈ Ma (W*) with log sin a>d-1 log do
PW〜N (W ∈ G (X, W*))≥exp(d°d* log sin a).
Proof. To lower bound PW〜N (W ∈ C (X, W*)) ∀X ∈ Ma (W*), we define the event that all
weight hyperplanes (with normals wi ) have an angle of at least α from the corresponding target
hyperplanes (with normals wi* ).
Ga(W*) = {w ∈ Rd1×d0llk⅛⅛l< cos(α)}.
In order that sign wi>X(n) 6= sign w1*>X(n) , wi must be rotated in respect to wi* by an angle
greater then the angular margin α, which is the minimal the angle between X(n) and the solution
hyperplanes (with normals wi*). Therefore, we have that, given X ∈ Ma (W*),
d;
∀α : ∩Ga (W*) ⊂G(X, W*) .	(13.5)
i=1
30
Under review as a conference paper at ICLR 2018
And so, ∀X ∈Mα (W*):
PW 〜N (W ∈G (X, W*)) ≥) PW 〜N (W ∈ \ Ca (Wj	(13.6)
=)YPW~N (w ∈ Ca (W*)) ≥)"	-s1n(Cj)#1,
i=1	L (d0	1) B ∖2, 2 ) _
d	dd^
where in (1) we used eq. (13.5), in (2) we used the independence of {wi}i=11 and in (3) we used eq.
(13.1) from Lemma 39. Lastly, to simplify this equation We use the asymptotic expansion of the beta
function B (1 ,x)= <不Ix + O (x-3/2) for large x:
log PW〜N (w ∈ C (X, W*)) ≥ dod* log sin α + O (d； log do).
We obtain the Lemma in the limit N → ∞ when log Sin a>d-1 log do.	□
13.3	Proof of Lemma 22:
Lemma 41. (Lemma 22 restated). Let W* = [w>,..., WM ] ∈ Rdι ×d0 a fixed matrix indepen-
dent of X. Then, in the limit N → ∞ with d1*≤do≤N, the probability of not having an angular
margin sin α = 1/ (d1*doN) (eq. (13.3)) is upper bounded by
P (X ∈ Mα (W*)) ≤,2d-1/2
Proof. We define
(	x(n)>w*
X ∈ Rd0×Nl 假叫 kw*k > sin3 卜
and Ma (W*)，Td= I Mα,i (W*). Since M (W*) = TN=I Ma (W*), we have
NN
P(X ∈ Ma (W*)) (=1) Y P(X ∈ Mna (W*)) = Y [1 -P(X ∈/ Mna (W*))]
n=1	n=1
(≥2) YN
n=1
d；
ι - EP (X ∈ Mα,i (w*))
i=1
(3)
≥
1 - d*1
2 sin (α)
BTpO-IJ
N
where in (1) we used the independence of {x(n) %=；, in (2) we use the union bound, and in (3) we
use eq. (13.2) from Lemma 39. Taking the log andwe using the asymptotic expansion of the beta
function B (ɪ,x) =、/不/x + O (x-3/2) for large x, we get
logP(X ∈ Ma (W*)) ≥ Nlo
—dod* sin α + O
π
—
2d-1/2 + O (d-3/2/N + d-1N-2)
where in the last line we recalled sin α = 1/N. Recalling that d1* ≤do ≤N , we find
. ... ♦
P (X ∈ Ma (W*)) ≥1 — exp
□
31
Under review as a conference paper at ICLR 2018
13.4	Proof of Lemma 23:
Lemma 42. (Lemma 23 restated). Let X ∈ Rd0×N be a standard random Gaussian matrix of
datapoints. Then we can find, with probability 1, (X, y)-dependent matrices W* and z* as in
Theorem 8 (Where d* , 4dN/ (2d。- 2)]). Moreover, in the limit N → ∞, where N∕d°≤do≤N,
for any y, we can bound the probability of not having an angular margin (eq. (13.3)) with sinα =
1∕ (d*1d0N) by
p(x ∈ Mα (w*))≤r/8d-1/2 + 2d0/ √logd0
πN
Proof. In this proof we heavily rely on the notation and results from the proof of in appendix section
9. Without loss of generality We assume S+ = [do — 1]. Unfortunately, We can't use Lemma 41 -
this proof is significantly more complicated since the constructed solution W* depends on X (we
keep this dependence implicit, for brevity). Similarly to the proof of Lemma 41, We define,
(	x(n)> w*
X ∈ Rd0×Nl Fn)>⅛ > sin(α)∫
and Ma (W*)，TN=I Man (W*), so M (W*) = Td= 1 Ma (W*). Wehave
(1)	d1
P(X ∈ Ma (W*)) = 1 -P(X ∈∕ Ma (W*)) ≥ 1 - P(X ∈∕ Mia (W*))
i=1
(=2) 1 - d1*P(X ∈∕ M1a (W*)) = 1 -d*1 (1 -P(X ∈ M1a(W*))) , (13.7)
Where in (1) We used the union bound, and in (2) We used the fact that, from symmetry, ∀i :
P (X ∈∕ Mia (W*)) = P (X ∈∕ M1a (W*)). Next, We examine the minimal angular margin in M1a,n:
separately for ∀n < do and ∀n ≥ do. Recalling the construction of W in appendix section 9, We
have, for ∀n < do :
min
i,n<d0
χ(n)>w*	l(ʌv 1 土 62W^l)> X(n)
M--M——i- = min  ------------------------∏-τ
||x(n)|| kw*k	n<d0,± IIW1 ± €2Wιk ||x(n)|
=)min ___________!2___________=)	γs∕，1 + Y2€2
n<d0 ,± ∣W 1 ± €2Wιk ∣lx(n)∣l	IIWιk maxn<do 卜⑺11
(13.8)
where in (1) we used ∀n < d0: x(n)>Wι = 1 and x(n)>Wι = 0 , from the construction of Wι and
Wι (eqs. (9.2), (9.5), and (9.4)), and in (2) we used the fact that W>W1 = 0 from eq. (9.4) together
with ∣W 1 k = ∣W 1 k from eq. (9.5), and €2 = Y€1 from eq. (9.7).
For ∀n ≥ do :
x(n)>Wi*
min --------τ.----
i,n≥d0 ||x(n)||kW*k
min
n≥d0 ,±
(W 1 ± €1W 1)> x(n)
(1 - γβ) €1
∣W 1 ± €1W1k||x(n)| ≥ YePf
min
+ €1 n≥d0
∣w >x(n)|
kW 1k |x(n)||,
(13.9)
where we used the fact that ∀n ≥ do : €2 ∣W>x(n) ∣ ≤ γβ ∣W>x(n)|, from eq. (9.7), and also that
W> W1 = 0 from eq. (9.4).
We substitute eqs. (13.8) and (13.9) into P (X ∈ M1a (W*)):
P (X ∈ M1a (W*))
≥ P (	Y€1/" + Y|2€1 山 > sinα, (I- - min
∖∣∣W1∣∣ maxn<do !x(n)|	Ye√1 + eι n≥d0
[W > x(n)
kW 1k ||x(n)|1
> sin α
(1)
≥P
γκ
∣∣W1∣∣ maxn<do Ux(n)|1
> Sin ɑ, 一 κ min
Ye	n≥d0 |x(n) |
€1
> Sin α, —/	C > K
V1 + €1
(13.10)
V P ( YK > IlW 1k ,η > max ||x(n)||) P ( (1γβ)
η Sin α	n<d0	γβ
κ min |
n≥d0 |x
x(n)
(n)
€1
π > Sin α, J > > K
H V1 + €1
32
Under review as a conference paper at ICLR 2018
where in (1) We rotate the axes so that w^ 1 8 [1,0,0..., 0] axes Wι 8 [0,1,0,0 ..., 0] - this is
possible due to the spherical symmetry of x(n), and the fact that w^ι and Wι are functions of x(n) for
n < d0 (from eqs. (9.4) and (9.2)), and as such, they are independent from x(n) for n ≥ d0, in (2)
we use that fact that kWιk and maxn<d0 ||x(n) ∣∣ are functions of x(n) for n < do , and as such, they
are independent from x(n) for n ≥ d0 . Thus,
P (X ∈Mα (W*))
≥ 1 - P A
η sin α
≤ kw 1k
or η ≤ max
n<d0
1 - P-
κmin ∣
n≥d0 ∣x
(n)
≤ sin α or
1
≤
2
1
κ
(≥)(1- P (η≡α ≤kw1k)- P 卜 ≤ maχ∣∣x(n)
1-P
(1 - γβ)
γβ
K min /(4 ≤ sin α] - p(
n≥d0∣∣x叫—	∖√T+e
21 ≤κ
P η > max
n<d0
x(n)∣∣)- P (η≡ ≤kw ιk
(13.11)
P , K
min ∣∣x1、n > sinα ] — P ( —/ '1	≤ κ ]]
n≥d0∣∣X⑺ ∣∣	)	(√i+^ ≤ ))
where in (1) we use the union bound on both probability terms.
All that remains is to calculate each remaining probability term in eq. (13.11). First, we have
P J ≤ K
∖√i+^i
1 - P ( √=^ < £1
1 - K2
(=1) 1 - P
W >x(n)
min T-r-÷
n≥do IW>X(n) I
K 1	(2)	I x(2n) I K 1
/	—	= 1 — P min	1 —2^1	>	/	—
√1 -	κ2 β J	yn≥do	i χ1n) i √1	- κ2 β
=) 1 -卜(图〉√τ⅛
N-d0-1
1 !#
(4)
≤1-
1 — - arctan
π
K
-K
N
,
(13.12)
>
1
亍β
where in (1) we used eq. (9.7), in (2) we recall that in eq. (13.10) we rotated the axes so that
Wi 8 [1,0,0 ..., 0] axes Wι 8 [0,1,0,0..., 0], in (3) we used the independence of different
x(n), and in (4) we used the fact that the ratio of two independent Gaussian variables is distributed
according to the symmetric Cauchy distribution, which has the cumulative distribution function
P (X > x) = 1 - ∏1 arctan (x), and therefore P (|X| > x) = 1 - 2 arctan (x).
Second, we use eq. (13.2)
P
γβ sin α
〉(1 - Yβ) K
>
2γβ sin α
(1 - γβ) κB (2, d02-1 )
N
(13.13)
Third,∣∣x(n) ∣∣2 is distributed according to the chi-square distribution of order d0, so for η2 > d0,
P (∣∣χ(n)∣∣ ≥ η八 ≤ (η2 exp (1 - η2∕do) /do)d0/2
Therefore,
P nm<adx0∣∣∣x(n)∣∣∣2 <η2	>
[1 - (η2 exp (1 - η2∕do) /do)d0/2] 0	.
(13.14)
33
Under review as a conference paper at ICLR 2018
Lastly, We bound kW11∣ = ∣∣W 11∣ (from eq. (9.5)). From eq. (9.4), We have
W>X[d0-i] = [1,...,1,1] ,	(13.15)
Where X[d0-1] has a singular value decomposition
d0
X[d0-1] =	σiuivi> ,
i=1
With σi being the singular values, and ui and vi being the singular vectors. The singular values are
ordered from smallest to largest, and σι = 0 with uι = Wι, from eq. (9.2). With probability 1, the
other d0 - 1 singular value are non-zero: they are the square roots of the eigenvalues of the random
matrix X[>d -1]X[d0-1] ∈ Rd0-1×d0-1. Taking the squared norm of eq. (13.15), We have
d0
do - 1 = W >X[do-1] X>do -1]W1 = X σ2 (u>wl)2 ≥ σ2 kw 1k2 ,	(13.16)
i=1
where the last inequality stems from the fact that u>W 1 = W>W 1 = 0 (from eq. (9.4)), so the
minimal possible value is attained when u>W 1 = ∣∣W 1 k. The minimal nonzero singular value, σ2,
can be bounded using the following result from (Rudelson & Vershynin, 2010, eq. (3.2))
PCm⅛ UXMrU ≤ ηd-1/2) ≤ 斗
Since
σ2 = mdin-1 UX>d0-1]rU ≥ mind UX>d0]rU
r∈Rd0 -1	r∈Rd0
we have,
P 3 < ηd-1/2) ≤ η.
Combining this with eq. (13.16) we get
P(上
η sin α
< kW1 k
sin α.
(13.17)
Lastly, combining eqs. (13.12), (13.13), (13.14) and (13.17) into eqs. (13.7) and (13.11), we get, for
η2 > d0,
P(X ∈Mα(W*))
≥ 1 - d1
1 — (η2 exp(1 — η2/do) /do)d0/2]	— η^° Sin a
N
N
1-
2γβ sin α
(I - Ye) KB (1, d02^)
1 — - arctan
π
-κ
≥ 1 — d1 (1 — 11 — (log do exp(1 — log do))d0∕2i 0
2d0/2,log do
d1N
1 - V8 d⅛N + o(n⅛
N
- 0.45N
—
κ
1
亍β
—
where in the last line we take β = Y = K = 1∕√2, η = d0/2,log do, sinα = 1/ (d1doN). Using
the asymptotic expansion of the beta function B (ɪ, x) =、/不/x + O (x-3/2) for large x, we obtain,
34
Under review as a conference paper at ICLR 2018
for Sin a = 1/ (d；d°N)
1 - P (X ∈ Ma (W*))
Thus, taking the log, and using log (1 - X) = -X + O (x2), We obtain, forsin a = 1/ (d;d°N)
logP(X ∈ Mα (W*))
2d1/2,log do + o
N +
d1/4
+ d*N + d*2-N + d0 exp
格 1	2dY2√oid0
VndlN — + O
d1/4
+ d*N + d*2-N + d0exp
Recall that d；，4「N/ (2d。— 2)] = N/do. Taking the limit N → ∞, do → ∞ with d*≤≤do≤≤N, we
have
P (X ∈ Mα (W*)) ≤1 - exp f-J8 d-1/2 -初"*10! ≤ T8 d-1/2 + 2d0∕2√logd0
π	Nπ	N
□
Part III
Numerical Experiments - implementation
details
Code and trained models for CIFAR and ImageNet results is available here https://github.
com/MNNsMinima/Paper. In MNIST, CIFAR and ImageNet we performed binary classification
on between the original odd and even class numbers. In we performed this binary classification
between digits 0 - 4 and 5 - 9. Weights were initialized to be uniform with mean zero and variance
2/d, where d is fan-in (here the width of the previous neuron layer), as suggested in (He et al.,
2015). In each epoch we randomly permuted the dataset and used the Adam (Kingma & Ba, 2014)
optimization method (a variant of SGD) with β1 = 0.9, β2 = 0.99, ε = 10-8. Different learning
rates and mini-batch sizes were selected for each dataset and architecture. In CIFAR10 and ImageNet
we used a learning-rate of α = 10-3 and a mini-batch size of 1024; also, ZCA whitening of the
training samples was done to remove correlations between the input dimensions, allowing faster
convergence. We define L as the number of weight layers. For the random dataset we use a mini-batch
size of bmin (N/2, d/2)c with learning rate α = 0.1 and 0.05, for L = 2 and 3, respectively. In the
random data parameter scans the training was done for no more than 4000 epochs - we stopped if
MCE = 0 was reached.
35