Under review as a conference paper at ICLR 2018
Deep Mean Field Theory: Layerwise Variance
and Width Variation as Methods to Control
Gradient Explosion
Anonymous authors
Paper under double-blind review
Ab stract
A recent line of work has studied the statistical properties of neural networks to
great success from a mean field theory perspective, making and verifying very
precise predictions of neural network behavior and test time performance. In
this paper, we build upon these works to explore two methods for taming the
behaviors of random residual networks (with only fully connected layers and no
batchnorm). The first method is width variation (WV), i.e. varying the widths
of layers as a function of depth. We show that width decay reduces gradient
explosion without affecting the mean forward dynamics of the random network.
The second method is variance variation (VV), i.e. changing the initialization
variances of weights and biases over depth. We show VV, used appropriately,
can reduce gradient explosion of tanh and ReLU resnets from exp(Θ(√L)) and
exp(Θ(L)) respectively to constant Θ(1). A complete phase-diagram is derived
for how variance decay affects different dynamics, such as those of gradient and
activation norms. In particular, we show the existence of many phase transitions
where these dynamics switch between exponential, polynomial, logarithmic, and
even constant behaviors. Using the obtained mean field theory, we are able to
track surprisingly well how VV at initialization time affects training and test time
performance on MNIST after a set number of epochs: the level sets of test/train set
accuracies coincide with the level sets of the expectations of certain gradient norms
or of metric expressivity (as defined in Yang and Schoenholz (2017)), a measure of
expansion in a random neural network. Based on insights from past works in deep
mean field theory and information geometry, we also provide a new perspective
on the gradient explosion/vanishing problems: they lead to ill-conditioning of the
Fisher information matrix, causing optimization troubles.
1	Introduction
Deep mean field theory studies how random neural networks behave with increasing depth, as the
width goes to infinity. In this limit, several pieces of seminal work used statistical physics (Derrida and
Pomeau, 1986; Sompolinsky et al., 1988) and Gaussian Processes (Neal, 2012) to show that neural
networks exhibit remarkable regularity. Mean field theory also has a substantial history studying
Boltzmann machines (Ackley et al., 1985) and sigmoid belief networks (Saul et al., 1996).
Recently, a number of results have revitalized the use of mean field theory in deep learning, with a
focus on addressing practical design questions. In Poole et al. (2016), mean field theory is combined
with Riemannian geometry to quantify the expressivity of random neural networks. In Schoenholz
et al. (2017) and Yang and Schoenholz (2017), a study of the critical phenomena of mean field neural
networks and residual networks1 is leveraged to theoretically predict test time relative performance of
differential initialization schemes. Additionally, Choromanska et al. (2015) and Pennington and Bahri
(2017) have used related techniques to investigate properties of the loss landscape of deep networks.
Together these results have helped a large number of experimental observations onto more rigorous
footing (Montfar et al., 2014; Glorot and Bengio, 2010; Bertschinger et al., 2005). Finally, deep mean
field theory has proven to be a necessary underpinning for studies using random matrix theory to
1without batchnorm and with only fully connected layers
1
Under review as a conference paper at ICLR 2018
understand dynamical isometry in random neural networks (Pennington et al., 2017; Pennington and
Worah, 2017). Overall, a program is emerging toward building a mean field theory for state-of-the-art
neural architectures as used in the wild, so as to provide optimal initialization parameters quickly for
any deep learning practitioner.
In this paper, we contribute to this program by studying how width variation (WV), as practiced
commonly, can change the behavior of quantities mentioned above, with gradient norm being of
central concern. We find that WV can dramatically reduce gradient explosion without affecting the
mean dynamics of forward computation, such as the activation norms, although possibly increasing
deviation from the mean in the process (Section 6).
We also study a second method, variance variation (VV), for manipulating the mean field dynamics
of a random neural network (Section 7 and Appendix B). In this paper, we focus on its application
to tanh and ReLU residual networks, where we show that VV can dramatically ameliorate gradient
explosion, and in the case of ReLU resnet, activation explosion 2. Affirming the results of Yang
and Schoenholz (2017) and predicted by our theory, VV improves performances of tanh and ReLU
resnets through these means.
Previous works (Poole et al., 2016; Schoenholz et al., 2017; Yang and Schoenholz, 2017) have
focused on how network architecture and activation functions affect the dynamics of mean field
quantities, subject to the constraint that initialization variances and widths are constant across layers.
In each combination of (architecture, activation), the mean field dynamics have the same kinds of
asymptotics regardless of the variances. For example, tanh feedforward networks have exp(Θ(l))
forward and backward dynamics, while tanh residual networks have poly(l) forward and exp(Θ(√7))
backward dynamics. Such asymptotics were considered characteristics of the (architecture, activation)
combination (Yang and Schoenholz, 2017). We show by counterexample that this perception is
erroneous. In fact, as discussed above, WV can control the gradient dynamics arbitrarily and VV
can control forward and backward dynamics jointly, all without changing the network architecture or
activation. To the best of our knowledge, this is the first time methods for reducing gradient explosion
or vanishing have been proposed that vary initialization variance and/or width across layers.
With regard to ReLU resnets, we find that gradient norms and “metric expressivity” (as introduced
in Yang and Schoenholz (2017), also defined in Defn 4.2), make surprisingly good predictors,
respectively in two separate phases, of how VV at initialization affects performance after a fixed
amount of training time (Section 7.1). However, in one of these phases, larger gradient explosion
seems to cause better performance, with no alternative course of explanation. In this paper we have
no answer for why this occurs but hope to elucidate it for future work. With regard to tanh resnets, we
find that, just as in Yang and Schoenholz (2017), the optimal initialization balances trainability and
expressivity: Decaying the variance too little means we suffer from gradient explosion, but decaying
the variance too much means we suffer from not enough metric expressivity.
We want to stress that in this work, by “performance” we do not mean absolute performance but
rather relative performance between different initialization schemes. For example, we do not claim
to know what initialization scheme is needed to make a particular neural network architecture solve
ImageNet, but rather, conditioned on the architecture, whether one initialization is better than another
in terms of test set accuracy after the same amount of training iterations.
Before we begin the mean field analysis, we present a perspective on gradient explosion/vanishing
problem from a combination of mean field theory and information geometry, which posits that such
problem manifests in the ill-conditioning of the Fisher information matrix.
2	Gradient Explosion/Vanishing: An Information Geometric
Perspective
Given a parametric family of probability distributions on Rm, P := {Pθ}θ with θ = (θ1, . . . , θn)
in Rn, its Fisher information matrix is defined as F(θ) := [Ez〜p@ (∂i logPθ(z))(∂j logPθ(z))]nj=ι
(here ∂i is partial derivative against θi ). It is known from information geometry that, under regularity
conditions, P forms a Riemannian manifold with θ 7→ Pθ as its coordinate map and F(θ) as
2This is the phenomenon where in deep ReLU resnet, the activation vectors blow up exponentially with
depth, invalidating the computation with infs; see Yang and Schoenholz (2017)
2
Under review as a conference paper at ICLR 2018
its Riemannian metric tensor (a fortiori it is positive definite) (Amari, 2016). This fact is most
famously used in the natural gradient method, which, akin to second order methods, computes from
a gradient vector ∂E∕∂θ a "natural direction of greatest descent” F(θ)-1∂E∕∂θ that is invariant
to reparametrization θ → θ0 (Amari, 1998). This method and related ideas have been applied to
great success in supervised, unsupervised, and reinforcement learning (for example, Pascanu and
Bengio (2013); Desjardins et al. (2015); Martens and Grosse (2015); Grosse and Martens (2016);
Wu et al. (2017)). An F(θ) with eigenvalues all approximately equal means that the neighborhood
around Pθ is isotropically curved and the gradient is approximately just the natural gradient up to a
multiplicative constant. Conversely, an F(θ) with a large condition number κ(F (θ)) (the ratio of the
largest over the smallest eigenvalue) means that the gradient is a poor proxy for the natural gradient
and thus is much less efficient. From another angle, F(θ) is also the Hessian of the KL divergence
τ 7→ KL(Pθ kPτ) at τ = θ. If we were simply to minimize this KL divergence through gradient
descent, then the number of iterations to convergence is proportional to K(F(θ)) (in general, there
is a lower bound of Ω( vzκ(F(θ))) for first order methods satisfying a mild condition) (Nesterov,
2004).
For a random deep network (residual or not) suffering from gradient explosion in the mean, we show
heuristically in this section that the condition number of its Fisher information matrix is exponentially
large in depth with high probability 3. First partition θ into groups of parameters according to layer,
θ = (θ11, θ12, . . . , θ1k1, θ21, . . . ,θ2k2, . . . , θL1, . . . , θLkL), with θlj denoting parameter j of layer
l. We can then partition the Fisher information matrix F(θ) into blocks, with the diagonal blocks
having sizes k1 × k1 , k2 × k2 , . . . , kL × kL .
According to the Hermitian min-max theorem, the largest eigenvalue of F(θ) is given by
maxkxk=1 xT F (θ)x and the smallest eigenvalue is given by minkxk=1 xTF(θ)x (both are posi-
tive as F(θ) is positive definite under our regularity assumptions). Thus κ(F(θ)) equals to their ratio
and is lower bounded by a ratio of extremal diagonal terms maxlj F (θ)lj,lj / minlj F (θ)lj,lj. Let
hY (θ)i be the expectation of Y (θ) with respect to random initialization of θ in some fixed method.
Suppose there is gradient explosion such that hEz(∂lj log Pθ(z))2i ∈ [exp(c0l), exp(C0l)] for univer-
sal constants c0 , C0 > 0 independent of j (this is true, for example, for feedforward tanh networks
initialized in the chaotic region Schoenholz et al. (2017)). By concentration of measure phenomenon
(as seen in Daniely et al. (2016); Poole et al. (2016); Schoenholz et al. (2017); Yang and Schoenholz
(2017) and this work), over randomization of parameters, Ez(∂lj logPθ(z))2 will in fact concentrate
around its mean as width goes to infinity. Thus we have, with high probability, that diagonal entries
F (θ)lj,lj = Ez(∂lj log Pθ(z))2 ∈ [exp(cl), exp(Cl)] for some new constants 0 < c < c0 < C0 < C.
Then the ratio maxlj F (θ)lj,lj / minlj F (θ)lj,lj is at least exp(cL)/ exp(C1) = exp(cL - C), so
that the κ(F (θ)) is exponential in L. The argument can be easily modified to accommodate gradient
vanishing and other rates of gradient explosion/vanishing like exp(Θ(lα)).
Thus such gradient dynamical problems cause the gradient to deviate from the natural gradient
exponentially in an appropriate sense and violate the information geometry of the information
manifold Pθ. For the case of minimizing KL divergence from a specific distribution, they directly
cause the number of gradient descent iterations to diverge exponentially. These issues cannot be
solved by just adjusting the learning rate (though it can somewhat ameliorate the problem by taking
conservative steps in such ill-conditioned regions).
3	Background
The desire to understand how initialization can affect final performances of a deep neural network has
led to a resurgence of mean field techniques, this time applied to deep learning. A series of papers
(Poole et al., 2016; Schoenholz et al., 2017; Yang and Schoenholz, 2017; Pennington et al., 2017) have
established the depth-wise dynamics of random neural networks (i.e. networks at initialization time).
For example, Poole et al. (2016) showed that, in a random tanh classical feedforward neural network,
activation norm converges exponentially fast in depth to a constant value, and so does the angle
between images of two different input vectors at successive depths, which the authors proposed as a
measure of expressivity that Yang and Schoenholz (2017) called “angular expressivity.” Schoenholz
et al. (2017) then showed that the gradient norm of such a random network suffers from exponential
explosion or vanishing during the course of backpropagation. But when the initialization variances
3This can be made rigorous in a straightforward, though somewhat tedious, way, but we will not do so here
3
Under review as a conference paper at ICLR 2018
lie on a “critical curve,” the gradient is neither vanishing nor exploding, and, more importantly, the
networks initialized on this “critical line” has the best test time performance after training for a fixed
number of iterations. The mean field framework was extended to residual networks (with only fully
connected layers and no batchnorm) in Yang and Schoenholz (2017). There the authors showed that
just by adding a skip connection to the feedforward network, the dynamics of a tanh network becomes
subexponential. More crucially, they investigated both tanh and ReLU residual networks, and found
that whereas gradient dynamics controls the test time performances of tanh resnets, “expressivity”
controls those of ReLU resnets. This expressivity is, roughly speaking, how much distance a random
network on average puts between two different input vectors; it was aptly named “metric expressivity.”
On the other hand, the “angular expressivity” proposed in Poole et al. (2016) (how much angle the
network puts between two input vectors, as explained above) was not found to be predictive of relative
test time performance of either tanh or ReLU resnets.
More precisely, the optimal initialization scheme for tanh resnet seems to strike a delicate balance
between trainability and expressivity, in that weight variance too large causes too much gradient
explosion and causes training to fail, whereas weight variance too small causes the typical network
to collapse to a constant function Yang and Schoenholz (2017). The optimal variance σw2 satisfies
σw2 L = const where L is depth. On the other hand, ReLU resnets have completely different
behavior with respect to initialization variance; here the best initialization scheme is obtained by
maximizing the weight variance (and as a consequence also maximizing the metric expressivity)
without overflowing activation values of deeper layers into numerical infs. Indeed, trainability
seems to not be a problem at all, as the gradient norm of weight parameters at each layer stays
constant within O(1) over the course of backpropagation.
In this paper, we extend the results of Yang and Schoenholz (2017) to include depthwise variation
of widths and of variances. We show that they can be used to great effect to reduce gradient
explosion as well as manipulating the expressivity (metric or angular) of the random network.
Corroborating Yang and Schoenholz (2017), we find that they improve tanh resnet performance
by taming gradient dynamics and improve ReLU resnet performance by preventing activations
from numerically overflowing while maximizing metric expressivity. However, in certain regimes,
worsening gradient explosion can mysteriously make ReLU resnet perform better, and we currently
do not know how to explain this phenomenon.
4	Preliminaries
Notations and Settings. We adopt the notations of Yang and Schoenholz (2017) and review them
briefly. Consider a vanilla feedforward neural network of L layers, with each layer l having N (l)
neurons; here layer 0 is the input layer. Let x(0) = (x(10), . . . , x(N0()0) ) be the input vector to the
network, and let x(l) for l > 0 be the activation of layer l. Then a neural network is given by
the equations xi(l) = φ(hi(l)), hi(l) = PjN=(l1-1) wi(jl)x(jl-1) + bi(l) where (i) h(l) is the pre-activation
at layer l, (ii) w(l) is the weight matrix, (iii) b(l) is the bias vector, and (iv) φ is a nonlinearity,
for example tanh or ReLU, which is applied coordinatewise to its input. To lighten up notation,
We suppress the explicit layer numbers l and write Xi = φ(hi), hi = PN=I WijXj + b where •
implicitly denotes ∙(l), and • denotes •(IT) (and analogously, • denotes ∙(l+1)). When the widths
are constant N(l) = N(m), ∀m, l, residual network (He et al., 2016a;b) adds an identity connection
or skip shortcut that “jumps ahead” every couple layers. We adopt one of the simplified residual
architectures defined in Yang and Schoenholz (2017) for ease of analysis 4, in which every residual
block is given by
M
Xi = Evij φ(hj) + ai + Xi,
j=1
N
hi = Ewij Xj + bi.
j=1
where M(l) is the width of the “hidden layer” of the residual block, (vi(jl))iN,j(=l)1,M(l) is a new set of
weights and (ai(l))iN=(1l) is a new set of biases for every layer l. If we were to change the width of a
4It is called full residual network by Yang and Schoenholz (2017), but in this paper, we will simply assume
this architecture whenever we say residual network.
4
Under review as a conference paper at ICLR 2018
residual network, as is done in practice, we need to insert “projection” residual blocks (He et al.,
2016a;b) every couple layers. We assume the following simplified projection residual block in this
paper, for the ease of presentation 5 6:
M	NN
Xi = Evij Φ(hj) + y + ai,	hi = Ewij Xj + bi,	y = Enij Xj.
j=1	j=1	j=1
where y = (yi)N=ι is a "projection" of X = (Xi)N=I 6, and (njINjNI is the “projection" matrix. Note
that we only consider fully-connected affine layers instead of convolutional layers.
Deep mean field theory is interested in the “average behavior" of these network when the weights and
biases, wi(jl) , bi(l) , πi(jl) , vi(jl), and ai(l) are sampled i.i.d. from Gaussian distributions. Following standard
practice, they respectively have standard deviations σW)/√N(l-1), σ(l), σ∏l)∕√N(l-1), σVl/√M(l),
and σa(l); here we normalize the variance of weight parameters so that, for example, the variance
of each h is σW, assuming each Xj is fixed. While previous works have all focused on fixing σfl)
to be constant across depth, in this paper We are interested in studying varying σfl). In particular,
other than σπ(l) which we fix at 1 across depth (so that “projection" doesn’t act like an “expansion"
or “contraction"), we let σfl) = σ∙l-β∙ for each • = v,w, a, b, where σ.,β. are constant w.r.t. l.
Hereafter the bar notation •, •, • do not apply to σs, so that, by σo, for example, we always mean the
constant σa.
We make the same statistical assumptions as in Yang and Schoenholz (2017). In the interest of space,
we relegate their discussion to Appendix A.
Mean Field Quantities. Now we define the central quantities studied in this paper. Inevitably, as
deep mean field theory analyzes neural networks closer and closer to those used in practice, the
variables and notations become more and more complex; our paper is no different. We have however
included a glossary of symbols (Table A.1) that will hopefully reduce notation confusions to the first
time reader.
Definition 4.1. Fix an input X(0). Define the length quantities q(l) := hkh(l) k2 i/M (l) = h(h(1l))2i
and p(l) := hkX(l) k2i/N (l) = h(X(1l))2i for l > 0 and p(0) = kX(0) k2/N. Here (and in the following
definitions) the expectations ⑻ are taken over all random initialization of weights and biases for
all layers l, as N(l), M(l) → ∞ (large width limit). Note that in our definition, the index 1 can be
replaced by any other index by Axiom 1. Thus p(l) is the typical magnitude (squared) of a neuronal
activation at layer l.
Definition 4.2. Fix two inputs X(0) and X(0)0. We write •0 to denote a quantity • with respect to
the input X(0)0. Then define the correlation quantities Y(I) := hh(l ∙ h(l)0i/M (l) = hh(1l)h(1l)0i and
λ(l) :=(X(l) ∙ X(I)0i/N(I) =(X,/，)0〉for l > 0 and λ(0) = X(O) ∙ x(0)o/N(0). Again, here the index
1 does not matter by Axiom 1. By metric expressivity, we mean S(I):= 击(kX(l) 一 X(l)0k2i =
2N(h∣∣X(l)∣∣2i + h∣∣X(l)0∣∣2i - 2(X(l) ∙ X(I)Oi) = 1 (P(I) + P(I)O) - γ(l). Additionally, define the cosine
distance quantities e(l) := Y(l)/ʌ/p(I)P(I)O and C(I) := λ(l)/Vzq(I)q(l)0, and we will also call e(l)
angular expressivity.
Following Yang and Schoenholz (2017), we assume P(0) = P(0)O for the ease of presentation, but
this is a nonessential assumption. Then, as we will see, P(l) = P(l)O, q(l) = q(l)O for all l, and as a
result, e(l) = Y(l)/P(l) and s(l) = P(l) 一 Y(l) = (1 一 e(l))P(l).
Definition 4.3. Fix an input X(O) and a gradient vector (∂E∕∂X(L ) of some loss function E with
respect to the last layer X(L). Then define the gradient quantities χ(l) := h(∂E∕∂Xll))2i, χ∙l) :=
h(∂E∕∂∙1l))2i for • = a, b, and χ∙l) := ((∂E∕∂∙1l))2i for • = w,v. Here the expectations are
5If the reader is disappointed by our simplifications, he or she is encouraged to consult He et al. (2016b),
which empirically explored many variations of residual blocks.
6“projection" is in quotes because the dimension N(l) can be less than or greater than N(l-1); but we will
follow the traditional wording.
5
Under review as a conference paper at ICLR 2018
taken with Axiom 2 in mind, over both random initialization of forward and backward weights and
biases, as N → ∞ (large width limit). Again, the index 1 or 11 does not matter by Axiom 1.
5	Overview of Results and Techniques
Just as in previous works in deep mean field theory (Poole et al., 2016; Schoenholz et al., 2017; Yang
and Schoenholz, 2017), the primary tool for investigating behaviors of large width networks is the
central limit theorem. Every time the activations of the previous layer pass through an affine layer
whose weights are sampled i.i.d., the output is a sum of a large number of random variables, and
thus follows an approximately Gaussian law. The output of the next nonlinearity is then a nonlinear
transform of a Gaussian variable, with computable mean and variance. Repeating this logic gives
us a depthwise dynamical system of the activation random variables. The gradient dynamics can be
similarly derived, assuming Axiom 2. Table A.2 summarizes the equations governing the dynamics
of various mean field quantities when we vary variances and/or widths.
We are interested in the asymptotics of such dynamics, for example, how does the gradient explode?
(the dynamics of χ). They can be derived via extensions of the techniques used by Yang and
Schoenholz (2017) as well; intuitively any nonlinear function can be asymptotically approximated
by linear ones, and each difference equation can be approximated by simple differential equations.
The asymptotics of those differential equations can be shown to coincide with the asymptotics of the
original difference equations.
Theoretically and empirically, WV does not change the mean dynamics of forward quantities like
activation norm p but can be used to control gradient dynamics χ. Intuitively, this is because each
neuron at a width-changing layer “receives messages” from different numbers of neurons in the
forward and backward computations. If for example N(l) = N (l-1)/2, then on the backward pass,
the neuron receives half as many messages on the backward pass than in the forward, so we expect
that its gradient should be half of what it would be when N(l) = N(l-1). See Section 6.
On the other hand, VV will usually change both the forward and backward dynamics of mean field
quantities. The phase transitions are many and complicated, but the overall trend is that, as we
dampen the variance with depth, both forward and backward dynamics will dampen as well; the only
major exception is weight gradients in ReLU resnets (see Appendix B.1). In contrast to WV which
works the same for any nonlinearity, the phase diagram for VV is controlled by different quantities
depending on the nonlinearity. We show through experiments that all of the complexities involved in
VV theory are reflected in the practice of training neural networks: we can predict the contour lines
of test time accuracy using only our mean field theory (Section 7 and Appendix B)
Expressivity vs Trainability Tradeoff. Yang and Schoenholz (2017) made the observation that
the optimal initialization scheme for tanh resnets makes an optimal tradeoff between expressivity
and trainability: if the initialization variances are too big, then the random network will suffer from
gradient explosion with high probability; if they are too small, then the random network will be
approximately constant (i.e. has low metric expressivity) with high probability. They posited that
such tradeoff between expressivity and trainability in ReLU resnets is not observed because the
gradient against weight parameters w and v are bounded w.r.t. depth (so that there is no gradient
explosion)7, while (metric) expressivity is exponential, thus strongly dominating the effect on final
performance.
We confirm this behavior in tanh resnets when decaying their initialization variances with depth:
When there is no decay, gradient explosion bottlenecks the test set accuracy after training; when we
impose strong decay, gradient dynamics is mollified but now (metric) expressivity, being strongly
constrained, bottlenecks performance (Section 7.2). Indeed, we can predict test set accuracy by
level curves of gradient norm ratio χW0)/Xw) in the region of small variance decay, while We can do
the same with level curves of metric expressivity s when in the region of large decay (Fig. 3). The
performance peaks at the intersection of these two regions.
7There is gradient explosions in the bias parameters χa and χb which follow the same dynamics as activation
and metric expressivity dynamics p and s. However, as performance increases when such explosions worsen, it
seems that they do not bottleneck performance.
6
Under review as a conference paper at ICLR 2018
Figure 1: Using WV to control the gradient explosion of a tanh resnet. Shades indicate standard deviation
(taken in normal scale, but possibly displayed in log scale), while solid lines indicate the corresponding mean.
The left two plots show that mean forward dynamics are more or less preserved, albeit variance explodes toward
the deeper layers, where WV is applied. The last plot show that the gradient dynamics is essentially suppressed
to be a constant compared to the exp( VZL) dynamics of tanh resnet without width decay. Dashed lines indicate
theoretical estimates in all three plot; solid, simulated data, which is generated from random residual networks
with 100 layers and N(0) = 2048, and we half the widths at layers l = m2 for m = 4, 5, . . . , 9.
Also corroborating Yang and Schoenholz (2017), we did not observe a tradeoff in ReLU resnets VV.
In the regime with small to moderate variance decay, VV exerts its effect through metric expressivity,
not gradient dynamics (Section 7.1) 8. However, when we impose strong decay, gradient explosion,
but not metric expressivity, predicts performance, in the unexpected way that worse gradient explosion
correlates with better performance; that is, expressivity and trainability (as measured by gradient
explosion) are both worsening, yet the performance increases! We currently have no explanation for
this phenomenon but hope to find one in future work.
6	Width Variation
Width variation is first passingly mentioned in Schoenholz et al. (2017) as a potential way to guide
gradient dynamics for feedforward networks. We develop a complete mean field theory of WV for
residual networks.
Via Table A.2 and Thm C.3, we see that width variation (WV) has two kinds of effects on the mean
gradient norm: when compared to no width variation, it can multiply the squared gradient norm of
biases bi or weights wij by N/M (which doesn’t “stack”, i.e. does not affect the squared gradient
norm of lower layers), or it can multiply the squared gradient norm of Xi by N/N (which “stacks”,
in the sense above, through the dynamics of χ). We will focus on the latter “stacking” effect and
assume N(l) = M (l)
Suppose from layer l to layer m, χ(m) rises to r times χ(l) . If we vary the width
so that N (m-1) is rN (m) , then this gradient expansion is canceled, and χ(m-1)	=
(N(m)/N(m-1))(σ2σWl-βv-βw V(φ(q(m)) + 1)χ(m) = (σv2σWl-βv-βw V(φ(q(m)) + I)X(I) so that
it is as if we restarted backpropagation at layer m.
Remarkably, changing the width does not change the mean field forward dynamics (for example the
recurrences for p, q, γ, λ remain the same) (Thm C.2). But, as always, if we reduce the width as
part of WV (say, keeping N(0) the same but reducing the widths of later layers), the variance of the
sampled dynamics will also increase; if we increase the width as part of WV (say, keeping N(L) the
same but increasing the widths of earlier layers), the variance of the sampled dynamics will decrease.
We can apply this theory of WV to tanh residual networks (φ = tanh in Table A.2) without VV.
By Yang and Schoenholz (2017), tanh residual networks with all β∙ = 0 have gradient dynamics
log(X(m)/X(I)) = A(√l - √m) + Θ(logl - logm) where A = 3∖P2 √σσw . If We place
σv+σa
projection blocks projecting N (l-1) 7→ N(l) / exp(A) at layer l = n2, for n = 1, 2, . . ., then the
gradient norms would be bounded (above and below) across layers, as reasoned above. Indeed, this
is what we see in Fig. 1. The rightmost subfigure compares, with log scale y-axis, the gradient
8As observed theoretically and empiricaly by Yang and Schoenholz (2017), there is gradient explosion in the
bias parameters, but they do not seem to hold back performance
7
Under review as a conference paper at ICLR 2018
Figure 2: Zigzagging through β∙-space with ReLU resnet. We trained a grid of ReLU residual networks,
pinning all σ.s at 1, but varying the β∙s as follows: The zig: fix βv = βa = 0; fix βw = βb and increase both
from 0 to 2 (making Vr = βw + βv go from 0 to 2 as well); The zag: fix βv = 0, βw = βb = 2; increase βa
from 0 to 2 (increasing Ur from 0 to 2 as well). For each setting of the hyperparameters, we train a network on
MNIST with those hyperparameters for 30 epochs. We then report the accuracy that the network achieved on
the training and test sets. The plots above are, in order from left to right, (a) zig/test, (b) zag/test, (c) zig/train,
(d) zag/train. In the zig, we have overlaid a contour plot of s (computed from Thm C.2), which is almost
identical to the contour plots of P and χ(0)∕χ(l); numbers indicate log 1 + log s. The dashed line is given
by I--V l1-Vr = C for C ≈ 10. In the zag, We have overlaid a contour plot of χV1) /Xvl) (computed from
Thm C.3). Numbers indicate log χv1)∕χvl).
βw = Jb
dynamics with no WV to that with WV as described above. We see that our theory tracks the mean
gradient dynamics remarkably precisely for both the WV and the no WV cases, and indeed, WV
effectively caps the gradient norm for l ≥ 16 (where WV is applied). The left two figures show the
forward dynamics of p and e, and we see that the WV does not affect the mean dynamics as predicted
by theory. However, we also see dramatic increase in deviation from the mean dynamics at every
projection layer in the forward case. The backward dynamics (rightmost figure) similarly sees large
deviations (1 standard deviation below mean is negative for χa and χw ), although the deviations for
χ is more tamed but still much larger than without WV.
Therefore, width variation is unique in a few ways among all the techniques discussed in the mean
field networks literature so far, including variance decay as studied below, adding skip connections,
or changing activation functions: It can ameliorate or suppress altogether gradient explosion (or
vanishing) problems without affecting the mean forward dynamics of p, q, λ, γ , c, e. To do so, it
has to choose a trade-off from the following spectrum: At one end, we truncate neurons from the
original network (say, keeping N (0) the same), so that we have fewer parameters, less compute, but
larger deviations from the mean dynamics. At the other, we add neuron to the original network (say,
keeping N (L) the same), so that we have more parameters, more compute, and smaller deviations
from the mean dynamics.
7	Variance Variation
7.1	ReLU
A zigzag of parameters controls various asymptotics of ReLU resnet: “zigging” Vr := min(βv +
βb, βa) from 0 to > 1, and then “zagging” Ur := βv + βw from 0 to > 1. During the zig, the
asymptotics of p is subdued from exp(poly(l)) to poly(l). During the zag, it is further reduced to
Θ(log l) (at Ur = 1) and Θ(1) (when Ur > 1). On the contrary, the gradient dynamics of weight
parameters become more explosive along the zigzag. During the zig, XwI)/χW) increases from Θ(1)
to poly(l) while χV1)∕χVl) increases from Θ(lβv) to a bigger poly(l). During the zig, both quantities
increase the exponents of their polynomial dependence in l. In the interest of space, we stop here
our sketch of VV dynamics for ReLU resnet, but refer the reader to Appendix B.1 for more detailed
descriptions and Appendix C for proofs.
To test our theory, we sweep through these two macro-phases of the parameter space in our experi-
ments and train an array of randomly initialized ReLU resnets; results are demonstrated in Fig. 2. The
figure caption gives the experimental details involved. In addition, we provide heatmap and contour
plots of various quantities of interest such as p, e, and χV1)∕χVl) in both the zig and the zag regimes
in Fig. A.1 and Fig. A.2 respectively.
8
Under review as a conference paper at ICLR 2018
300
250
200
150
100
50
300
250
200
150
100
50
βw = βv
0.4 0.6 0.8
0.4 0.6 0.8 1.0
Figure 3: Sweeping through VV phases of tanh resnet. In all experiments here, We Pin σ.s all to 1. From left to
right: (a) and (b). We sweep Ut : 0 % 1 in to ways, testing to what extent Ut determines the final performance.
In the first way (a), we set all β.s equal to a common β and increase β : 0 % 1. In the second way (b), we clamp
βb = βa = 1 and set βw = βv, increasing their common values from 0 to 1. The heatmaps are produced from
final test set performance as in Fig. 2. As we can easily see, these two sweeps produce almost identical heatmaps.
In both plots, there is a visible peak in the heatmaps in the upper left. On each of (a) and (b), we overlay the
contours of XwI)/χW) (in blue) to the left of the peak and those of P (in green) to the right of the peak, the latter
being very similar to those of s. The blue numbers indicate log X(O)/χW) while the green numbers indicate
log p. (c). We fix Ut = 1 by fixing βv = βa = 1 and sweep βw = βb from 0 to 1, thereby sweeping Vt from 1
to 2. The heatmap is obtained again with the same procedure as in Fig. 2 from the test set after training. Overlaid
on top is a contour plot of s, and the numbers indicate log s.
Discussion. First notice that training fails in the upper left corner of the zig. This happens because
of numerical instability caused by exploding P and χ(0)∕χ(l), which grow like exp(IW^l1-Vr +
o(l1-Vr)) (Thms C.9 and C.11). Indeed, one of the contour lines of p traces out almost exactly where
training fails. The dashed line is a level set of the dominant term of asymptotic expansion of log P,
and we see it agrees with the contour of P very well. By increasing βw = βb, we effectively solved
the activation explosion problem observed by Yang and Schoenholz (2017) without changing the
activation function.
Second, observe that performance actually dips in the direction where χ(0)∕χ(l) decreases, quite
counterintuitively9. This can be explained (as in Yang and Schoenholz (2017)) by noting that gradient
against weights, χw and χv , in fact respectively remain bounded and polynomial in l (and changes
rather mildly with Vr; see Fig. A.1); gradient against biases do experience the same behavior as
χ, but in general they are much less important than the weights, as parameters go. In addition,
the performance is also dipping in the direction where s decreases (exponentially) in (Vr, L)-space.
This is the quantity that essentially underlies the exponential expressivity result (as told from an
extrinsic curvature perspective) of Poole et al. (2016); as s decreases dramatically, it gets harder and
harder for a linear functional in the final layer to tell apart two input vectors. This exponential loss
in expressivity dominates the effect on performance more than a polynomial reduction in gradient
explosion.
Third, it is remarkable that in the zag regime, the level curves of χ(v1)∕χ(vl) (but not those ofP, s, e, χ,
or χw !) accurately predict the contour of the test set performance, in such a counterintuitive way
that greater gradient explosion χ(v1)∕χ(vl) correlates with better performance (Fig. 2(b)). Especially
when βa (and thus also Ur) is large, the weight gradient dynamics are much more explosive than
that of metric expressivity, so according to prevailing theory, gradient explosion should bottleneck
performance, but instead the reality is the exact opposite. It is currently unclear if in certain situations
like this, larger gradient expansion is actually beneficial, or if there is a quantity yet undiscovered
which has the same level curves and can explain away this seeming paradox (like how s explains
away χ(0)∕χ(l) above, in the zig regime). Of the quantities that appear in Fig. A.2, none foots this
bill.
9
Under review as a conference paper at ICLR 2018
7.2	TANH
As in the previous section, we briefly sketch the VV dynamics of tanh resnets, but defer a more
detailed discussion to Appendix B.2 and the proofs to Appendix C. We are concerned with the
scenario that q → ∞ with l, as otherwise higher layers become essentially unused by the network.
The major phases of tanh resnet VV dynamics are then determined by when Ut := min(βv, βa) < 1
and when Ut = 1 (Thms C.5 and C.6). Within the former, the gradient dynamics is controlled
by Wt := 1 - βw + Ut - 2βv ; as Wt starts positive, decrease to 0, and then becomes negative,
the gradient ratio χ(0) /χ(l) starts out as exp(poly(l)), turns into polynomial, and finally becomes
bounded. When Ut = 1, χ(0)/χ(l) is always subpolynomial, with Vt := βv + βw making it bounded
as Vt increases past 1. On the other hand, the dynamics of p is quite simple, with p = Θ(l1-Ut )
when Ut < 1 and p = Θ(log l) when Ut = 1.
This theory enables us to predict and optimize an VV initialization scheme for tanh resnets. We
sweep through the the two phases described above, train the corresponding random networks, and
exhibit the results in Fig. 3. The figure caption details the experimental setup.
Discussions. Fig. 3(a) and Fig. 3(b) sweep through Ut from 0 to 1 in two different ways while
obtaining almost identical test set performance heatmap, showing indeed that the hyperparameters β∙
exert their effect through Ut = min(βv, βa). In each of these two plots, peak accuracies happen in the
upper left. To the left of such a peak, gradient norm χW1)∕χW) predicts the accuracy, while to the right
of such a peak, metric expressivity s (and in this case p as well because they induce similar contours)
does. But χW1 )∕χW) would not do well in the large β region because the slopes of its contours are too
steep; conversely s would not predict well in the small β region because its contour slopes are not
steep enough. Indeed, one sees that the slopes of the heatmap level set boundaries decrease as the
accuracy levels increase (and as β decreases from 1), but when the level peaks, the slope suddenly
becomes much steeper (compare the left boundary of the peak to its right boundary). Our observation
here reaffirms the trainability vs expressivity tradeoff studied in Yang and Schoenholz (2017).
In Fig. 3(c), we study the Ut = 1 phase. Here s alone predicts performance (though in the large
βw = βb region, final accuracy becomes more random and the prediction is not as great). This is
expected, as χ∙0)∕χ∙l) is now subpolynomial for all • = v,w,a,b (Thm C.6), so that trainability is
not an issue.
8	Conclusion
In this paper, we derived the mean field theory of width and variance variation and showed that they
are powerful methods to control forward (VV) and backward (VV + WV) dynamics. We proved that
even with a fixed architecture and activation function, the mean field dynamics of a residual neural
network can still be manipulated at will by these two methods. Extraordinarily, the mean field theory
we developed allowed us to accurately predict the performances of trained MNIST models relative to
different initializations, but one puzzling aspect remains where test set accuracy seems to increase as
gradient explosion worsens in one regime of random ReLU resnets.
Open Problems. We solved a small part, width variation, of the program to construct mean field
theories of state-of-the-art neural networks used in practice. Many open problems still remain,
and the most important of them include but is not limited to 1. batchnorm, 2. convolution layers,
and 3. recurrent layers. In addition, more work is needed to mathematically justify our “physical”
assumptions Axiom 1 and Axiom 2 to a “math” problem. We hope readers will take note and
contribute toward deep mean field theory.
9Even though Fig. 2(a) has overlaid on top the contours of s, they are similar enough to those of χ(0)∕χ(l)
that we are identifying them here.
10
Under review as a conference paper at ICLR 2018
References
David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A Learning Algorithm for
Boltzmann Machines*. Cognitive Science, 9(1):147-169, 1985. ISSN 1551-6709. doi: 10.1207/
s15516709cog090L7. URL http://dx.doi.org/10.12 07/s155167 0 9cog0901_7.
Shun-ichi Amari. Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2):251-
276, February 1998. ISSN 0899-7667, 1530-888X. doi: 10.1162/089976698300017746. URL
http://www.mitpressjournals.org/doi/10.1162/089976698300017746.
Shun’ichi Amari. Information geometry and its applications. Number volume 194 in Applied
mathematical sciences. Springer, Japan, 2016. ISBN 978-4-431-55977-1. OCLC: ocn930997330.
Nils Bertschinger, Thomas Natschlger, and Robert A. Legenstein. At the Edge of Chaos:
Real-time Computations and Self-Organized Criticality in Recurrent Neural Networks. In
L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing
Systems 17, pages 145-152. MIT Press, 2005. URL http://papers.nips.cc/paper/
2671-at-the-edge-of-chaos- real-time-computations-and-self-organized-criticality-in
pdf.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in neural
information processing systems, pages 342-350, 2009. URL http://papers.nips.cc/
paper/3628- kernel- methods- for- deep- learning.
Anna Choromanska, Yann LeCun, and Grard Ben Arous. Open Problem: The landscape of the loss
surfaces of multilayer networks. In COLT, pages 1756-1760, 2015. URL http://www.jmlr.
org/proceedings/papers/v40/Choromanska15.pdf.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks:
The Power of Initialization and a Dual View on Expressivity. In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29,
pages 2253-2261. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6427- toward- deeper- understanding- of- neural- networks- the- power- of- initialization- an
pdf.
B. Derrida and Y. Pomeau. Random Networks of Automata: A Simple Annealed Approximation.
EPL (Europhysics Letters), 1(2):45, 1986. URL http://stacks.iop.org/0295-5075/
1/i=2/a=001.
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, and koray kavukcuoglu. Natural Neural
Networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28, pages 2071-2079. Curran Associates, Inc.,
2015. URL http://papers.nips.cc/paper/5953- natural- neural- networks.
pdf.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward
neural networks. In PMLR, pages 249-256, March 2010. URL http://proceedings.mlr.
press/v9/glorot10a.html.
Roger Grosse and James Martens. A Kronecker-factored approximate Fisher matrix for convolution
layers. In PMLR, pages 573-582, June 2016. URL http://proceedings.mlr.press/
v48/grosse16.html.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for
Image Recognition. pages 770-778, 2016a. URL https://www.cv-foundation.
org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_
CVPR_2016_paper.html.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pages 630-645. Springer, 2016b.
11
Under review as a conference paper at ICLR 2018
Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random synaptic
feedback weights support error backpropagation for deep learning. Nature Communications, 7:
ncomms13276, November 2016. ISSN 2041-1723. doi: 10.1038/ncomms13276. URL https:
//www.nature.com/articles/ncomms13276.
James Martens and Roger Grosse. Optimizing Neural Networks with Kronecker-factored Approxi-
mate Curvature. arXiv:1503.05671 [cs, stat], March 2015. URL http://arxiv.org/abs/
1503.05671. arXiv: 1503.05671.
Guido Montfar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the Number of Linear
Regions of Deep Neural Networks. In Proceedings of the 27th International Conference on Neural
Information Processing Systems - Volume 2, NIPS'14, pages 2924-2932, Cambridge, MA, USA,
2014. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969033.2969153.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
IU. E Nesterov. Introductory lectures on convex optimization: a basic course. 2004. ISBN 978-
1-4419-8853-9. URL http://dx.doi.org/10.1007/978-1-4419-8853-9. OCLC:
883391994.
Razvan Pascanu and Yoshua Bengio. Revisiting Natural Gradient for Deep Networks. December
2013. URL https://openreview.net/forum?id=vz8AumxkAfz5U.
Jeffrey Pennington and Yasaman Bahri. Geometry of Neural Network Loss Surfaces via Random
Matrix Theory. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pages 2798-2806, International Convention Centre, Sydney, Australia, August 2017. PMLR. URL
http://proceedings.mlr.press/v70/pennington17a.html.
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learn-
ing. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages
2634-2643. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
6857-nonlinear-random-matrix-theory- for-deep-learning.pdf.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid
in deep learning through dynamical isometry: theory and practice. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems 30, pages 4788-
4798. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7064- resurrecting- the- sigmoid- in- deep- learning- through- dynamical- isometry- theory- a
pdf.
Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Ex-
ponential expressivity in deep neural networks through transient chaos. In Advances In Neural
Information Processing Systems, pages 3360-3368, 2016.
Lawrence K Saul, Tommi Jaakkola, and Michael I Jordan. Mean field theory for sigmoid belief
networks. Journal of artificial intelligence research, 4:61-76, 1996.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information
Propagation. 2017. URL https://openreview.net/pdf?id=H1W1UN9gg.
H. Sompolinsky, A. Crisanti, and H. J. Sommers. Chaos in Random Neural Networks. Phys. Rev.
Lett., 61(3):259-262, July 1988. doi: 10.1103/PhysRevLett.61.259. URL https://link.aps.
org/doi/10.1103/PhysRevLett.61.259.
Yuhuai Wu, Elman Mansimov, Shun Liao, Roger Grosse, and Jimmy Ba. Scalable trust-region method
for deep reinforcement learning using Kronecker-factored approximation. arXiv:1708.05144 [cs],
August 2017. URL http://arxiv.org/abs/1708.05144. arXiv: 1708.05144.
Greg Yang and Samuel S. Schoenholz. Meanfield Residual Network: On the Edge of Chaos. In
Advances in neural information processing systems, 2017.
12
Under review as a conference paper at ICLR 2018
Table A.1: Glossary of Symbols. “Mean normalized” is abbreviated “m.n.”
Symbol Meaning
Ref
σ∙
x(l)
h(l)
N(l)
M(l)
p(l)
q(l)
γ(l)
λ(l)
s(l)
e(l)
e*
c(l)
X V WwtwvVvr4 W
standard deviation of trainable parameter •
activation vector/input vector
hidden vector
width of layer l activation x(l)
width of layer l hidden vector h(l)
m.n. squared length of activation vector x(l)
m.n. squared length of hidden vector h(l)
m.n. dot product x(l) ∙ x(l)0
m.n. dot product h(l) ∙ h(l)0
m.n. squared distance ∣∣x(l) - x(l)0k2
cosine distance Y(l)/，p(I)P(I)0
limit value of e(l) as l → ∞
cosine distance λ(l)/Azq(I)q(I)O
m.n. gradient squared norm w.r.t. x(l)
m.n. gradient squared norm w.r.t. trainable parameter •
variable nonlinearity R → R
variance integral transform
covariance integral transform
min(βv, βa); controls tanh resnet VV dynamics
1	- βw + Ut - 2βv ; controls tanh resnet VV dynamics
βv + βw; controls tanh resnet VV dynamics
βv + βw controls ReLU resnet VV dynamics
min(βv + βb, βa); controls ReLU resnet VV dynamics
2	σVσW; controls ReLU resnet VV dynamics
4.1
4.1
4.2
4.2
4.2
4.2
4.2
4.3
4.3
C.1
C.1
C.4
C.4
C.4
C.8
C.8
C.8
Table A.2: Mean field dynamics with VV and WV
Forward (Thm C.2)	Backward (Thm C.3)
q = σW l-βw p + σ2l-βb	y/y = J (N/N)(σ2σWl-βv-βwV‹φ(q) + I) X/X = l(σVσW LeLe Vφ(q) + 1)
∆p = σVl-βv Vφ(q) + σ2l-βa	Xb = (N/M )σ2l-βv χV<Φ(q)
λ = σw2 l-βw λ + σb2l-βb	Xw = (N/M )σ2i-βv χV<Φ(q)p
∆γ = σv2l-βv Wφ(q, λ) + σa2l-βa .	Xv = χVφ(q)	Xa = χ Xn = χp.
The two cases for χ∕χ are resp. for a projection and a normal residual block, assuming σ∏ = L The V and W
operators are defined in Defn C.1.
Appendices
A Mean Field Assumptions
We make several key “mean field” assumptions, which were formulated in their entirety first in Yang
and Schoenholz (2017) (though Axiom 2(a) has been stated first by Schoenholz et al. (2017)). While
these assumptions may be mathematically unsatisfying, identifying them and discovering that they
lead to highly precise prediction is in fact one of the most important contributions of deep mean field
theory.
13
Under review as a conference paper at ICLR 2018
0gl+2
Og1+l(g3 - Y)
Og'1— Y /P)
logl+log' (0/'l
Figure A.1: Heatmap and contour plots of various quantities in the “zig” regime of Section 7.1. Note that we
squashed things like p with large range so that the colors are meaningful.
zag: βa :10-2,a = 0,β∙ = ,, = 2
loχ (0)χVl
Figure A.2: Heatmap and contour plots of various quantities in the “zag” regime of Section 7.1. Note that we
squashed things like p with large range so that the colors are meaningful.
14
Under review as a conference paper at ICLR 2018
Axiom 1 (Symmetry of activations and gradients). (a) We assume h(hi(l))2i = h(h(jl))2i and
h(x(0))2i = h(xj0))2i for any i,j,l. (b) Wealso assume that the gradient ∂E∕∂x(l) with respect to
the lossfunction E satisfies h(∂E∕∂x(l))2i = h(∂E∕∂x(l))2i forany i,j,l.
Axiom 2 (Gradient independence). (a) We assume the we use a different set of weights for backpropa-
gation than those used to compute the network outputs, but sampled i.i.d. from the same distributions.
(b) For any loss function E, we assume that the gradient at layer l, ∂E∕∂x(l, is independentfrom
all activations hj(l) and x(jl-1) from the previous layer.
One can see that Axiom 1(a) is satisfied if the input x(0) ∈ {±1}N and Axiom 1(b) is satisfied if
Axiom 2 below is true and the gradient at the last layer ∂E∕∂xL ∈ {±1}N. Axiom 2(a) was first
made in Schoenholz et al. (2017) for computing the mean field theory of gradients for feedforward
tanh networks. This is similar to the practice of feedback alignment (Lillicrap et al., 2016).
B Overview: Dynamics under Variance Variation
As discuss in Section 6, WV is essentially a post-hoc technique to tweak an existing gradient dynamic
without changing the forward dynamics. Thus in this section we assume all widths are constant,
N (l) = N (m) = M(n) for any m, l, n, so that WV can be applied as a “touch-up” if necessary. We
will overview the phases and transitions due to VV, but defer all proofs to later sections.
B.1	Residual Network with ReLU
It was shown in Yang and Schoenholz (2017) that, with no variance decay, in an ReLU resnet, both
the mean squared activation norm (p) and the mean squared gradient norm (χ) explode exponentially
with depth, and this causes training to fail for even 100 layer networks. We show that this problem is
in fact extremely easy to fix, requiring no architectural changes at all, only that β∙ be increased from
0 so that the randomization variances decay across depth (Thms C.9 and C.11).
Gradient quantities. The main driver of this gradient mollification is Vr := βv + βw . When
Vr ∈ [0,1), the gradient norm varies like χ(0)∕χ(l) = exp(Θ(l1-Vr)); when Vr = 0 this recapitulate
the exponential behavior derived in Yang and Schoenholz (2017). When Vr = 1, it experiences a sharp
phase transition, where now X(O)/x(l) = poly(l). As Vr becomes larger than 1, χ(0)∕χ(l) = Θ(1),
all bounded! Fig. B.3 verifies this result empirically, and in fact show that our computed asymptotic
expansions in Thm C.11 are highly accurate predictors. It is both interesting and important to note
that the gradient norms for actual trainable parameters, such as χw and χv, are affected differently
by Vr. In fact, χW1)/Xw) is bounded (!) with l when Vr < 1 (the Vr = 0 case is already observed in
Yang and Schoenholz (2017)) but phase transitions to poly(l) for Vr ≥ 1, while XvI)/χVl) is already
poly(l) when Vr < 1, and remains so as Vr is increased to > 1. Curiously, greater gradient explosion
in Xv predicts better performance in the Vr > 1 regime, and we currently do not know if this is
intrinsic or there are confounding variables; see Section 7.1.
Length quantities. Similarly, Vr is the primary conduit for mollifying the behavior of squared
activation norms p and q (Thm C.9). Like the gradient dynamics, p = exp(Θ(l1-Vr)) when Vr < 1;
when Vr = 0 this recapitulates the results of Yang and Schoenholz (2017). As Vr rise to 1 and above,
p experiences a phase transition into polynomial dynamics, but unlike the case of X, it is not constant
when Vr > 1. Instead, a different parameter, Ur := min(βv + βb, βa) drives the asymptotics of p
in the Vr > 1 regime. When Ur ∈ [0, 1) is small, p grows like Θ(l1-Ur). The instant Ur hits 1, p
is just logarithmic, p = Θ(log l). As Ur shoots past 1, p becomes constant. Thus the dynamics of
P is governed by a zigzag through (β∙)∙=v,w,a space. Fig. B.4 goes through each of the five cases
discussed above and verifies that our asymptotics are correct.
Cosine distance. e = γ∕p measures how well the input space geometry (angles, in this case) is
preserved as the input space is propagated through each layer. Its dynamics are much simpler than
those of P and X above. If e(0) = 1, then e(l) = 1 for all l trivially. If e(0) < 1, then we have one of
the following two cases, • If Vr ≤ 1 or Ur ≤ 1, then e → 1 irrespective of initial data P(0) and γ(0).
15
Under review as a conference paper at ICLR 2018
Figure B.3: We verify the asymptotic characterizations of χ given by Thm C.11. Here solid lines indicate
computations done via the recurrences Table A.2, while dashed lines indicate asymptotics proved in Thm C.11.
To facilitate visualization, all green (but not blue) dashed lines are shifted vertically to match the end point of
the corresponding solid lines. From left to right: (a) log-log plot of the log of χ(0)∕χ(l), against the predicted
leading term IW^l1-Vr, when Vr = .2; thus χ(0)∕χ(l) is superpolynomial. As in Thm C.11, Wr = 1 σVσW.
(b) log-log plot of χ(0) /χ(l) when Vr = 1, showing it is polynomial. (c) χ(0)∕χ(l) is bounded above when
Vr = 1.6.
β = 1,0, α =0.5
βv = βw =0,5, βa =0,5, βb=0.5
100	101	102	103
layerl
Figure B.4: We verify the asymptotic characterizations of p given by Thm C.9. Here solid lines indicate
computations done via the recurrences Table A.2, while dashed lines indicate asymptotics proved in Thm C.9.
In all but the leftmost plot, We show both P and ∆p = P — P (possibly adjusted for log factors) and their
asymptotics. Ib facilitate visualization, all dashed lines except the red one in the leftmost plot are shifted
vertically to match the end point of the corresponding solid lines. In the leftmost plot, the red lines are
respectively log P (solid) and the leading term IWVrl1-Vr in its asymptotic expansion (dashed). The green lines
in the same plot are the remainder (solid) and its polynomial asymptotic form, ignoring the leading coefficient
(dashed). From left to right gives one example for each of the 5 cases discussed in the main text: (a) Vr < 1
and P = exp(poly(l)). (b) Vr = 1 and P phase transitions into polynomial dynamics. (c) Vr > 1, Ur < 1
and P = poly(l). (d) Vr > 1, Ur = 1 and P = Θ(log l). (e) Vr > 1,Ur > 1 and P becomes bounded but
∆P = Θ(l-Ur ).
• If Vr > 1 and Ur > 1, then e converges to a fixed point e* < 1 which depends on the initial data
p(0) and γ(0), at a rate of Θ(l1-Ur). Thus ReLU very much likes to collapse the input space into a
single point (e = 1 means every two input vectors get mapped to the same output vector), and the
only way to prevent this from happening is to make the β∙s so high, that higher layers barely modifies
the computation done by lower layers at all. Indeed, the second condition Vr > 1 and Ur > 1 ensures
that p = Θ(1) as discussed above (Thm C.9), so that as l → ∞, layer l’s residual adds to x(l-1) only
a vector of vanishing size compared to the size of x(l-1).
B.2	Residual Network with Tanh
While ReLU resnet depends heavily on Vr = βv+βw and Ur = min(βv + βb, βa), Ut := min(βv, βa)
and Wt := 1 - βw + Ut - 2βv are the key quantities determining the dynamics in the case of tanh
resnet, with Vt = βv + βw = Vr playing a minor role. We will study tanh resnets in the setting where
q → ∞ as l → ∞; otherwise, p is bounded, meaning that higher layers become essentially unused
by the neural network (similar to the discussion made in Appendix B.1(Cosine distance) above). In
this setting, it can be shown that Ut ≤ 1 (Thm C.5).
Gradient quantities. By Thm C.6, as long as Ut stays below 1, the asymptotics of χ is entirely
governed by Wt, which is 1 when all β∙s are 0 and most of the time decreases as β∙s are increased.
16
Under review as a conference paper at ICLR 2018
When Wt > 0, χ(0)∕χ(l) = exp(Θ(l 1 Wt ));the results of Yang and Schoenholz (2017) are recovered
by setting all β∙s to 0, thus Wt = 1 and χ(0)∕χ(l) = exp(Θ(l2)). When Wt hits 0, χ(0)∕χ(l)
becomes polynomial, and as Wt dips below 0, gradient expansion becomes bounded.
If Ut = 1, χ(0)∕χ(l) is automatically suppressed to be subpolynomial. The only minor phase
transition here is going from Vr = 1 to Vr > 1 (and Vr cannot be less than 1 by our assumption that
q → ∞). In the former case, the gradient expansion is exp(Θ(√log l)), while in the latter case it is
bounded.
Length quantities have simpler asymptotics determined by Ut: either Ut < 1 and p = Θ(l1-Ut ),
or Ut = 1 and p = Θ(log l) (Thm C.5). Cosine distance, unlike the case of ReLU resnets, can
be controlled effectively by βa and βv (Thm C.7). When βa > βv , the magnitude of ai(l) drops
much more quickly with depth than that of vi(jl), so that higher layers experience the chaotic phase
(Schoenholz et al., 2017; Yang and Schoenholz, 2017), driving e(l) toward the limit point e* = 0. On
the other end, when βa < βv, vi(jl) vanishes in comparison to ai(l) with large l, so that the higher layers
experience the stability phase (Schoenholz et al., 2017; Yang and Schoenholz, 2017), collapsing all
inputs to the same output vector, sending e(l) → 1. Only when βa = βv could the fixed point e* be
controlled explicitly by σv and σo, with e* given by the equation e*
case with no VV (Yang and Schoenholz, 2017).
σ2 ∏2 arcsin3D+σ2
σV+σ2
as in the
C Main Theorems
Asymptotic notations. The expressions f = O(g) ^⇒ g = Ω(f) have their typical meanings,
.	_ ,	、	.	_ ,	,	_ , ..	. ,	-T ,	,	~ ,.,
and f = Θ(g) iff f = O(g),g = O(f). Wetake f (x) = O(g(χ)) ^⇒ g(x) = Ω(f(x)) to mean
k
f(x) = O(g logk x) for some k ∈ Z (this is slightly different from the standard usage of O), and
f = Θ(g) ^⇒ f = O(g) & g = O(f). All asymptotic notations are sign-less, i.e. can indicate
either positive or negative quantities, unless stated otherwise.
We recall integral transforms from Yang and Schoenholz (2017):
Definition C.1. Define the transforms V and W by Vφ(ρ) := E[φ(z)2 : Z 〜N(0,ρ)] and
Wφ(ρ, V) := E[φ(z)φ(z0) : (z, z0)〜N(0, (P ρ))].
Table A.2 summarizes the derived recurrences.
C.1 Recurrences
Yang and Schoenholz (2017) gave recurrences for mean field quantities p, q, γ, λ, χ under the
assumption of constant initialization variances across depth. The proofs there carry over straightfor-
wardly when variance varies from layer to layer. Schoenholz et al. (2017) also derived backward
dynamics for when the width of the network is not constant. Generalizing to the residual network case
requires some careful justifications of independences, so we provide proof for gradient dynamics; but
we omit the proof for the forward dynamics as it is not affected by nonconstant width and is almost
identical to the constant variance case.
Theorem C.2. For any nonlinearity φ in an FRN, regardless of whether widths vary across layers,
q(l) = σw2 l-βw p(l-1) + σb2l-βb
p(l) = σv2l-βv Vφ(q(l)) + σa2l-βa + p(l-1)
λ(l) = σw2 l-βw λ(l-1) + σb2l-βb
γ(l) = σv2l-βv Wφ(q(l), λ(l)) +σa2l-βa + γ(l-1).
(L) N(l)
Theorem C.3. Suppose a random residual network receives a fixed gradient vector ∂E∕∂xi
with respect to some cost function E, at its last layer. For any nonlinearity φ in an FRN, under
17
Under review as a conference paper at ICLR 2018
Axiom 1 and Axiom 2, whenever φ(Z )2 has finite variance for any Gaussian variable ζ,
((n∕n )(σ2σW l-βv -βw Vφ(q) + σ∏ )χ if layer l has projection connection
X	l(σV2σW l-βv-βw Vφ(q) + 1)χ	if layer l has identity connection
Xb = (N/M )σ2l-βv χV<Φ(q)
Xw = (N/M )σ2l-βv χV<Φ(q)p
Xv = XVφ(q)
Xa = X
χ∏ = χp.
Proof. We will show the proof for the projection connection case; the identity connection case is
similar but easier.
Write η(l) = -dE. We have the following derivative computations:
i	∂xi
∂xi ——= dXj	∂hk	∂xi	∂hi	∂hi	∂hi =πij + χvikφ(hk)西,	西=Vijφ(hj),	∂Xj	=	Wij,	∂Wij	=	xj,福=1, ∂xi	∂xi	∂xi =	=φ(hk),	=—	=	1,	=	 =	Xj ∂vik	∂ai	∂∏ij	J
Then	N	M	∂h ηj = Y^ηi(πij + HVik φ(hk) ∂χk) NM =fηi(∏ij + EVik φ(hk )Wkj) i=1	k=1
Thus,	NM hη2i = hE Higj + EVik φ(hk)wkj )]2i i=1	k=1 NM = h	ηi2[(πij +	Vikφ(hk )Wkj)] i i=1	k=1 MM + 2 Ehnini0 (∏ij + EVikφ(hk)Wkj)(∏i0j + £ Vi0kφ(hk)Wkj)i i<i0	k=1	k=1 M =N hn2 i(hπ2ιi + h[X Vik φ(hk Iwkj ]2i) k=1 MM + 2 y^hnini0 ih(∏ij + ^Vikφ>(hk )wkj )(∏i0j + fjVi0kφ(hk )wkj )i i<i0	k=1	k=1
where in the second equality, we expanded algebraically, and in the third equality, we use the symmetry
assumption Axiom 1 and the independence assumption Axiom 2. Now, h[PM=ι vikφ(hk)wkj]2i =
PM hv2 φ(肌)2w2 i	=	PM σ2l-βv vΦ(q) σWl-βw	=	σ2l-βv vΦ(q) σWl-βw	bv	the indenen-
乙 k=1hv1k φ(hk) Wkj i	=	λ=k = l M v φ(q) N	=	σvl v φ(q) N	by	UIeindePen
dence of {vik}i,k ∪ {hk}k ∪ {wkj}k,j (by our independence assumptions Axiom 2). Similarly,
because {πij}j ∪ {πi0 j}j ∪ {vik}k ∪ {vi0k}k for i 6= i0 is mutually independent by our assumptions,
one can easily Seethath(∏j + PM=I Vikφ(hk)wkj)(∏i0j + PM=I vi,kφ(hk)wkj)i = 0.
18
Under review as a conference paper at ICLR 2018
Therefore,
σ2	σ2 l-βw
X = N χ(σ + σVl-βv Vφ(q) -WL)
-	N	N
=N X(-2 + -V-W ITLe vφ(Q)).
For the other gradients, we have (where we apply Axiom 2 implicitly)
∂E --= dbj	∂E ∂xi ∂hj = ⅛∂χ-西西=Nnivj φ(hj)	
Xb	N	-2 l-βv =([£nivijφ(h)]2i = Nχ TM	V(φ(q) = i=1	二 MN-2l-βv χvφm);
∂E N ∂E ∂xi ∂hj	N
∂Wji = ⅛ ∂Xi 西 ∂Wji = Nnivj CKj )xj
N	-2 l-βv	N
XW =(£nivijφ>(hj)xji = Nχ M V(Xq)P =而-Vl βχVφ(q)p
MM	M	— M
i=1
∂E	∂E ∂xi	= ni φ(hk ) =⇒ XV = h[ni φ(hk)] i	XVφ(q)
∂vik	∂Xi ∂vik		
∂E	∂E ∂xi _	∂E 二 ∂Xi =⇒ Xa = X	
∂ai	∂xi ∂ai		
∂E	∂E ∂xi	=mxj =⇒ χ∏ = χp	
∂∏ij	∂xi ∂πij		
□
C.2 Residual Networks with Tanh
In this section we derive the asymptotics of various mean field quantities for tanh resnet. The main
proof technique is to bound the dynamics in question with known dynamics of difference equations
(as in Yang and Schoenholz (2017)).
Definition C.4. Let Ut := min(βV,βa),Wt := 1 -βW +Ut -2βV,Vt := βV +βW.
Theorem C.5. Suppose φ = tanh, and q(l) → ∞ as l → ∞.
1.	IfUt < 1, then 1 > βW + Ut and
P(l) = Θ(l1-Ut )
q(l) = Θ(l1-βw-Ut ).
More specifically,
f(-Vl1-βv ,-V-WI—)
(PQ q(l))〜{(-al1-βa,-2-W IITaTw )
l((-2+-V)lι-βv, (-a+-V)-W Iii)
if βV < βa
if βa < βV
if βa = βV.
2.	IfUt = 1, then βW = 0, and P(l) = Θ(log l) = q(l). More specifically,
((-V ,-W-V log l)	if βν <βa
(P(I), q(I))〜((-a log l,-W-a log l)	if Bv >βa
l((-2+-V)log l,-W (-a+-V)log l) ifβ=βa.
19
Under review as a conference paper at ICLR 2018
3.	Ut cannot be greater than 1.
Proof. Claim 1. We have Vφ(q) = 1 - ∏∏q-1/2 + Θ(q-3/2) by LemmaD.5. Thus
P - p = σVl-βv (1 - √2∕∏q-1/2 + Θ(q-3/2)) + σ2l-βa
=σVl-βv + σ2l-βa - o(l-βv)
fσVl1-βv	if βv <βα
P 〜σ σ,2l1-βa	if βα < βv
l(σa2 + σV)l1-βv if βα = βv
where we used the assumption 1 - Ut > 0. Thus p(l) = Θ(l1-Ut ) and goes to infinity with l.
Then q = σWl-βwP + σ2l-βb. Because We assumed q → ∞, the first term necessarily dominates
the second, and 1 -Ut - βw > 0. The possible asymptotics of q are then
(σVσWl1-βv-βw	if βv < βa
q 〜σ。脑Wl1-βa-βw	if βv > βa
Sa+σV)σWι1-βv-βw if Ba=βv
Which gives q(l) = Θ(l1-βw-Ut ).
Claim 2. If Ut = 1, then
(σv2 log l	if βv < βa
P 〜σ σa log l	if βv > βa
l(σa + σv)log l if βv = βa.
Then for q to go to infinity, βw has to be 0, so that q = Θ(log l) as Well, and
(σw2 σv2 log l	if βv < βa
q 〜σ σW σ2 log l	if βv > βa
[σW (σ2 + 蟾)log l if βv = βa.
Claim 3. If Ut > 1, then P = Θ(1) =⇒ q = Θ(1), a contradiction.
□
Theorem C.6. Suppose φ = tanh, and q(l) → ∞ with l. Recall Wt = 1 - βw + Ut - 2βv and
Vt = βv + βw .Then log(χ ⑼/χ(l)) is
If Ut < 1
If Wt > 0
Θ(l 2 Wt)
If Wt = 0
Θ(log l)
If Wt < 0
Θ(1)
If Ut = 1
If Vt < 1
This case cannot happen
If Vt = 1
Θ(√Iog7)
If Vt > 1
Θ(1)
20
Under review as a conference paper at ICLR 2018
Proof. By Thm C.3 and Lemma D.4, we have
log(χ∕χ) = log(1 + σVσW l-βv-βw Vφ(q))
=log(1 + σVσW l-βv-βw (C q-1/2 + Θ(q-3/2)))
=σVσW I--% C q-1∕2 + 0(1-2%-2%)
where C = 2 ^∏. Since q(I) has different growth rates depending on the hyperparameters, We need
to consider different cases:
• IfUt < 1, then Thm C.5 implies βw +Ut < 1 and q = Θ(l1-βw-Ut). So l-βv-βw q-1/2
l-βv -βw Θ(l- 2 (I-βw-Ut)) = Θ(l1 ( —1-βw+Ut-2βv ) )
◦	If 1 - βw + Ut - 2βv > 0, logx(0)/X(I) = Pm=I Θ(mI(T-βw+Ut-2βv))
Θ(l1 (I-βw +Ut -2βv ) )
◦	If 1 — βw + Ut — 2βv = 0, then l-βv-βwq-1/2 = Θ(l-1). Thus log χ(0)/X(I)
Plm=1 Θ(m-1) = Θ(log l).
◦	If 1 - 8w + Ut - 2βv < 0, then 1 ( —1 - 8w + Ut- 2βv ) > 1, so that log X(O)/x(l)
Θ(1).
• If Ut = 1, then by Thm C.5, βw = 0 and q = Θ(logl). So l-βv-βwq-1/2
Θ(l-βv-βw / √log l).
◦	If βv	+ βw	< 1, then βv < 1 =⇒ Ut < 1, contradiction.
◦	If βv	+ βw	= 1, then log X(0)/X(I) = Pm=I e(l-1/Mlog l)	=	Θ(√log7).
◦	If βv	+ βw	> 1, then log X(0)/X(l) = Θ(1).
□
Theorem C.7. Suppose φ = tanh and q → ∞. If e(0) < 1, then e(l)
given by the equations
converges to a fixed point e*,
f0
e* _ σ σV ∏ arcsin(e*) + σ2
e = ∖ ɪ~σv+σa
if βa > βv
if βa = βv
if βa < βv.
Note that in the case βa = βv, we recover the fixed point of tanh residual network without variance
decay.
Proof. We have
σvi-βv Wφ(q, cq) + o2lTe = Y - Y
=ep - ep
=ep - ep + ep - ep
=(e - e)p + e(p - p)
=(e + (e - e) —p- )(p - p)
P - p 一
σvi-βv Wφ(q, Cq) +。ɑ1-诙 _	_ ) P
σ2l-βvVφ(q)+ σi2l-βa	~	- p - P
Using Lemma D.1 and Lemma D.5, we can see that the LHS is monotonic (increasing or decreasing)
for large enough l. Therefore e(l) is a bounded monotonic sequence for large enough l, a fortiori it
has a fixed point.
21
Under review as a conference paper at ICLR 2018
If We express e = e* + e, Y(l) = (e* + e(l))p(l), then
σVl-βvWφ(q,cq + σ2l-βa = e* + + ( _ ) P
σVl-βv Vφ(q) + σi2l-βa	'	'ZP — P
RHS It,s easy to verify via Thm C.5 that either p/(p-p) = Θ(l log l) (when Ut = 1) or p/(p-p)=
Θ(l) (all other CaSeS). If E — C = Ω((llogl)-1), then E = Ω(loglog l) (by Euler-MacLaurin formula),
which would be a contradiction to E = o(1). Thus (E — C) p—p = o(1). 10 Hence the RHS goes to 0
with l.
LHS If βv > βa, then the LHS converges to 1. So e* = 1.
If βv < βa, then the LHS 〜Wφ(qqq). As l → ∞, C 〜e → e*, and Wφ(q, e*q) → ∏ arcsin(e*),
and Vφ(q) → 1. Therefore, e* = ∏ arcsin(e*), for which there are 2 solutions, 0, the stable fixed
point, and 1, the unstable fixed point. In particular, for all e(0) < 1, e(l) → 0.
If βv = βa, then taking limits l → ∞, we get
σV2 arcsin(e*) + σ22	*
---π................ -	Q*
σV + σ
□
C.3 Residual Network with ReLU
The asymptotics of ReLU resnet depends on the following values:
Definition C.8.
Vr :=	βv + βw,	Ur	:= min(βv	+	βb,βa)	= βv	+min(βb, βa	- βv),	Wr	:= 1 σVσW
Theorem C.9. Let φ= ReLU. Then we have the following asymptotics of P and q:
• If Vr > 1
◦	If Ur ∈ [0, 1)
P = Θ(l1-Ur) andq = Θ(lmax(1-Ur-βw,-βb)).
◦	If Ur = 1
Θ(l-βw log l) if βw ≤ βb
P = Θ(log l) andq=	-β	w b
Θ(l-βb )	otherwise.
◦	If Ur > 1
.	P = Θ(1), p — p = Θ(l-Ur) and q = Θ(l-min(βw,βb)).
• If Vr = 1
◦	If Wr 6= 1 — Ur
. P = Θ(lmax(Wr,1-Ur)), and q = Θ(lmax(max(Wr,1-Ur)-βw,-βb)).
◦	Otherwise
.P
Θ(lWr log l) and q
Θ(lWr-βw log l) ifβb ≥ βw — Wr
Θ(l-βb)	otherwise
• If Vr < 1
10 In fact, since e must be o(log log ∙∙∙ log l) for any chain of logs, e — e = o((l log l log log l ∙∙∙ log(k) l)-1)
for any k, where log(k) is k-wise composition of log; so (e — e) p-p = o((log log l ∙∙∙ log(k) l)-1) for any k.
22
Under review as a conference paper at ICLR 2018
◦ p, q	=	exp(IW^l1-Vr + Θ(lmaX(0,1-2Vr))).	In particular, P =
exp( 1W⅛l1-Vr + R(l; Wr, Vr) +。⑴),R is the same R as in Lemma D.7,
depending on only l, Wr, and Vr, with
(Θ(l1-2Vr ) if Vr < 1/2
R= Θ(log l)	ifVr = 1/2
[θ(1)	if Vr > 1/2.
and q = exp(I-V；l1-Vr + R(l; Wr, Vr) - βw log l + O(1)).
In particular, p, q = poly(l) ifVr ≥ 1.
Proof. Since Vφ(q) = 2q, We have
P - P = σ2l-βv Vφ(q) + σ,2l-βa
=2 σV l-βv (σW l-βw p + σb2l-βb) + …
=1 σV σW l-βv-βw p + 1 σVσ2l-βv-βb + ”.
2	2
We apply Lemma D.8 with β = βv + βw, δ = 1 σνσW, and α = min(βv + βb,βa) = βv +
min(βb, βa - βv) to upper and loWer bound our dynamics here, With appropriately chosen C. With a
little bit of case work, we obtain the desired results.	□
Theorem C.10. Let φ = ReLU. Suppose e(0) < 1.
•	IfVr ≤ 1 or Ur ≤ 1, then liml→∞ e(l) = 1.
•	If Vr > 1 and Ur > 1, then e(l) converges to a fixed point e* < 1, dependent on the initial
data P(0) andγ(0), at a rate of Θ(l-Ur+1).
Proof. By Cho and Saul (2009); Yang and Schoenholz (2017), we have Wφ(q, Cq) = Vφ(q)Jι(c)
2 q ∙ ∏1 (√1 — c2 + (π — arccos(c))c). As in the proof of Thm C.7, we have
σ2l-βv q∕2J1(c) + σ2l-βa
σVl-βv q∕2 + σ2l-βa
-e =(e-e) p⅞
But %；:qV2J+σ+ ；-/“ ≥ Jι(c) ≥ C ≥ e, where the last inequality holds for all large l, by
Lemma D.1. So the LHS is nonnegative for large l, which implies e(l) is a bounded monotonically
increasing sequence for large enough ls and thus has a limit e*.
Writing e = e* + e, we have
e* + e +(e - e)
σVl-βv q∕2J1(c) + σl2l-βa
σ2l-βv q∕2 + σ2l-βa
(?)
P
P - P
Suppose Vr ≤ 1 or Ur ≤ 1.
LHS. By Thm C.9, P = ω(1) (assuming σv, σw > 0), so that p∕(p - P)= O(l). As in the proof of
Thm C.7, e - e cannot be ΩΩ(l-1), or else e → ∞. Thus (e - e)p∕(p - p) = o(1), and the LHS in
the limit l → ∞ becomes e*.
RHS. In all cases, we will find e* = 1.
If q = o(lβv-βa ), in the RHS σa2l-βa dominates, and the RHS converges to 1.
If q = ω(lβv-βa), then in the RHS 1 σ2l-β,jq dominates. By LemmaD.1:
•	If p = ω(lβw-βb), then C 〜e → e*, so that in the limit l → ∞, e* = Jι(e*). This
equation has solution e* = 1.
23
Under review as a conference paper at ICLR 2018
•	If P = o(lβw-βb), then C → 1, so that e* = Jι(1) = 1.
•	If p 〜Clβw-βb, then C → *2^：^ , and We have an equation e* = Jι().
Note that since e* ∈ [0,1],《2^*2 =屐；阖∙ e* + σ2C+竭∙ 1 ≥ e*, With equality
iff e* = 1. Since Ji is monotonic, e* = Jι( σw2CeJσb) iff the equality condition above is
σwC+σb
satisifed, i.e. e* = 1.
If q 〜Dlew -βb for some D, then the RHS is
2 σVDJι(C) + σ2	⑴
2 σVD + σ	+ O(I)
by the same logic as in the proof of Lemma D.1. As in the above case, Lemma D.1 yields the
folloWing results:
•	If p = ω(lβw-βb), then C 〜 e → e*, so that in the limit l → ∞, e* = 2%;"+,+金.
Since the RHS of this equation is a convex combination ofJ1(e*) and 1, and J1(e*) ≥ e*
by the monotonicity ofJ1, the equality can hold only ifJ1(e*) = e*. The only such e* is 1.
•	If p = o(lβw-βb), then C → 1, so that e* = 上飞鲁)+寸。= 上
•	If p 〜 Clew-βb, then C →	⅛⅛2, and We have an	equation	e*	=
σw C+σb
12 1 I 2 (1 σ2DJι(σw2Ce;σb) + σ2). By the monotonicity of Ji,	the	RHS	is	at least
2 σ2 D+σ2 2 2 V 八 σw C+σj / a J	1	1,
ι 2 L 2 (1 σ2D( σwCe ∣+σb) + σ2), which is a convex combination of e* and 1. Since
2 σ2D+σ2 '2 V σ σwC+σb2 ! a ”
e* ≤ 1, the equality can hold only if e* = 1.
Suppose Vr ≤ 1 or Ur ≤ 1.
By Thm C.9, p = Θ(1) and therefore γ = Θ(1). Both converge to fixed points p* and γ* (possibly
dependent on initial data p(0) and γ(0)) because they are monotonically increasing sequences. Thus
e* = γ* /p*.
Unwinding the proof of Thm C.9, we see that P - P = l-Ur, so that p/(p - p) = Θ(lUr). Since the
RHS of Eq. (?) cannot blow up, it must be the case that (E - e) = O(l-Ur) =⇒ E = O(l-Ur+1). If
(E - e)p/(p - p) = F + o(1) for some constant F, then the LHS becomes e* + F in the limit. Yet,
unless γ(0) = p(0), e* < 1. Therefore, F > 0 (or else, like in the case of Vr ≤ 1 or Ur ≤ 1, e* = 1)
whenever γ(0) < p(0), and E = Θ(l-Ur+i).
□
Theorem C.11. Suppose φ = ReLU.
• If Vr > 1, then χ(0)∕χ(l) = Θ(1).
◦	IfUr ∈ [0, 1)
Xw7χw) = Θ(lurτ+ev)	xb1)/Xbl)= Θ(lβv)
χV1)∕χVl) = Θ(l- max(1-ur-ew ,-βb))	XaI/χι = Θ(1).
◦	If Ur = 1
If βw ≤ βb
Xwl)∕χw) = Θ(lβv / log l)	xb1)∕χbl) = Θ(lβv)
χι∕χl = Θ(lew / log l)	χa1)/Xal) = θ(1)
24
Under review as a conference paper at ICLR 2018
Otherwise
◦ If Ur > 1
xW1)/Xw)= Θ(lβv/log l)
xV1)/Xvl)= Θ(lβb)
χW1)∕χWl) = Θ(iβv)
X(v1)/X(vl) = Θ(lmin(βw ,βb) )
X(b1)/X(bl) = Θ(lβv )
X(a1)/X(al) = Θ(1)
X(b1)/X(bl) = Θ(lβv )
X(a1)/X(al) = Θ(1)
•	If Vr = 1 ,then χ(0) ∕χ(l) = Θ(lWr).
◦	If Wr 6= 1 - Ur
X(w1)/X(l) = Θ(lWr+βv-max(Wr,1-Ur))
X(v1)/X(l) = Θ(lWr-max(max(Wr,1-Ur)-βw ,-βb) )
X(b1)/X(bl) = Θ(lWr+βv )
X(a1)/X(al) = Θ(lWr )
◦	Otherwise
If βb ≥ βw - Wr
X(w1)/X(wl) = Θ(lβv / log l)
X(v1)/X(vl) = Θ(lβw / log l)
Otherwise
X(w1)/X(wl) = Θ(lβv / log l)
X(v1)/X(vl) = Θ(lWr+βb )
X(b1)/X(bl) = Θ(lWr+βv )
X(a1)/X(al) = Θ(lWr )
X(b1)/X(bl) = Θ(lWr+βv )
X(a1)/X(al) = Θ(lWr )
•	If Vr < 1 ,then χ(0)∕χ(l) = exp( IWVr l1-Vr + Θ(lmaX(I-2βv-2βw,O))).
X(w1)/X(wl) = Θ(1)
X(b1)/X(bl) = exp(
Wr
1 - Vr
P-Vr + Θ(imax(1-2βv-2βw,0)))
X(v1)/X(vl) = Θ(lβw )
X(a1)/X(al) = exp(
Wr
1 - Vr
p-Vr + Θ(imax(1-2βv-2βw,0)))
Proof. Using Thm C.3 and the fact that Vφ(q) = 2q, Vφ(q) = 1, We get
so
X =(2 σvσwI-Vr+ 1)χ,
Xw = 2 σVl-βv χp,
Xb = 2 σVl βv χ,
1
XV = 2 χq,
Xa = X,
XwI) =	2σVχ(I)P(O)	= lβv P(O) X(O)
Xw)	1 σVl-βv X(l)p(l-1)	p(l-1) Xal
XV1) _ 2X(I)q(I) _ q(O) X(O)
--T-	—：----：---	---：-：---T-
XVl)	2X(l)q(lT)	q(l-1) X(I)
x!1' _ X(I)
--:~~- 	 ---
Ml).........χ(l).
Xa	X
Suppose Vr > 1. Then χX(0)- = Θ(1) by LemmaD.7. By Thm C.9:
25
Under review as a conference paper at ICLR 2018
• If Vr > 1, then χX(0)- = Θ(1) by LemmaD.7.		
◦ IfUr ∈ [0, 1)		
P = Θ(l1-Ur) and q = Θ(lmax(1-Ur-βw,-βb)). So		
χ(w1)∕χ(wl) = Θ(lUr-1+βv )	χ(b1)∕χ(bl	= Θ(lβv)
χ(v1)∕χ(l) = Θ(l- max(1-Ur-βw,-βb))	χ(a1)∕χ(al	= Θ(1).
◦ IfUr = 1, then P = Θ(log l) and		
Ifβw ≤ βb		
q = Θ(l-βw log l) so that		
χ(w1)∕χ(wl) = Θ(lβv ∕ log l)	χ(b1)∕χ(bl) =	Θ(lβv)
χ(v1)∕χ(vl) = Θ(lβw ∕ log l)	χ(a1)∕χ(al) =	Θ(1)
Otherwise		
q = Θ(l-βb) so that		
χ(w1)∕χ(wl) = Θ(lβv ∕ log l)	χ(b1)∕χ(bl) =	Θ(lβv)
χ(v1)∕χ(vl) = Θ(lβb )	χ(a1)∕χ(al) =	Θ(1)
◦ If Ur > 1		
P = Θ(1) and q = Θ(l- min(βw,βb)). So		
χ(w1)∕χ(wl) = Θ(lβv )	χ(b1)∕χ(bl) =	Θ(lβv)
χ(v1)∕χ(vl) = Θ(lmin(βw,βb))	χ(a1)∕χ(al) =	Θ(1)
• If Vr = 1, then by Lemma D.7 χ(0)∕χ(l) = Θ(l 1 σvσw) = Θ(lWr).
◦	If Wr 6= 1 - Ur
p = Θ(lmax(Wr,1-Ur)), and q = Θ(lmax(max(Wr,1-Ur)-βw,-βb)). So
χ
χ
(w1)∕χ(l) = Θ(lWr+βv-max(Wr,1-Ur))	χ(b1)∕χ(l) = Θ(lWr+βv )
(v1)∕χ(l) = Θ(lWr-max(max(Wr,1-Ur)-βw,-βb))	χ(a1)∕χ(l) = Θ(lWr)
◦	Otherwise, p = Θ(lWr log l), and
Ifβb ≥βw -Wr
q = Θ(lWr-βw log l), so
χ(w1)∕χ(wl) = Θ(lWr+βv-Wr ∕ log l) = Θ(lβv/ log l)
xV1)∕χVl) = Θ(lWr + βw-Wr / log l) = Θ(lβ / log l )
Otherwise
q = Θ(l-βb), so
χ(w1)∕χ(wl) = Θ(lWr+βv-Wr∕ log l) = Θ(lβv ∕ log l)
χ(v1)∕χ(vl) = Θ(lWr+βb )
χ(b1)∕χ(bl) = Θ(lWr+βv )
χ(a1)∕χ(al) = Θ(lWr )
χ(b1)∕χ(bl) = Θ(lWr+βv )
χ(a1)∕χ(al) = Θ(lWr )
• If Vr < 1, then by Lemma D.7, χ(0)∕χ(l) = exp( 1WVl1-Vr + R(l; Wr, Vr)), where
R = Θ(lmax(I-2Vr,01)) is as in Lemma D.7.
◦ We have P = exp(IWVl1-Vr + R(l; Wr, Vr) + O(1)), where R is the same as above,
and q = exp(IWel1-Vr + R(l; Wr, Vr) — βw log l + O(1)). Thus
χ(w1)∕χ(wl) = exp(O(1)) = Θ(1)
xb1)∕χbl) = exp( TWrl1-Vr + R + βv log l)
1 — Vr
χ(v1)∕χ(vl) = exp(βw log l + O(1)) =Θ(lβw)
χ1)/Xal) = exp(ιWV ι1-Vr + R).
26
Under review as a conference paper at ICLR 2018
□
D Lemmas
In this section, we present many lemmas used in the proofs of the main theorems. In all cases,
the lemmas here have already appeared in some form in Yang and Schoenholz (2017), and for
completeness, we either include them and their proofs here or improve upon and extend them, with
the blessing of the authors.
Lemma D.1. Asymptotically,
σW l-βw p + σt2l-βb
1 — σW σ-2lβb-βw p(1 — Yzp) + Θ(l2(βb-βw )p2(1 — Yzp))
=1 + O(lβb-βw p)	if P = o(lβw-βb)
Y/p + σ2σ-2lβw-βb PT(YZp — 1) + Θ(l2(βw-βb)p-2(γ∕p — 1))
=γ∕p + O(lβw -βb PT)	if P = ω(lβw-βb)
σσC 黑；+ E Rlβb-βw (γ∕p — 1) + Θ((Rlβb-βw )2(γ∕p — 1))
σw	； CG+σ2	if p = Clβw-βb + R for
= 展2 C：；2b +。(1)	some constant C and re-
W b	mainder R = o(lβw-βb).
where E = (σ⅞w⅜2 ∙
Proof. If P = o(lβw-βb), then
(σ2l-βb + σW l-βw P)T= σ-2 lβb (1 + σW σ-2lβb-βw P)T
=σ-2lβb (1 — σW σ-2lβb-βw p + (σ2, σ-2lβb-βw p)2 ——)
；Wl；P + ；2；二=(σWl-βwY + σ2l-βb)σ-2lβb(1 — σ2,σ-2lβb-βwp + (σWσjlβb-βwp)2 —…)
=(σW σ-2lβb-βw Y + 1)(1 — σW σ-2lβb-βw p + (σ2, σ-2lβb-βw p)2 ——)
=1 — σW σ-2lβb-βw (p — Y) + Θ((lβb-βw )2p(p — Y))
=1 + O(lβb-βw p)∙
If p = ω(lβw-Bb), then
(σW l-βw P + σ2l-βb )-1 = (σW l-βw P)T(1 + σ2σ-2lβw-βb PT)T
=(σW ITW p)-1(1 — σ2σ-2lβw-βb PT + (σ2σ-2lβw-βb p-1)2 ——)
I? " P + I2" = (Y/P + σ2σ-2lβw-βb PT)(I — σ2σ-2lβw-βb p-1 + (σ2σ-2lβw-βb PT)2 ——)
=Y/p + σ2σ-2lβw-βb p-2(Y — p) + Θ(l2(βw-βb)p-3(Y — p))
=Y/p + O(lβw-βb P-1).
27
Under review as a conference paper at ICLR 2018
If P 〜Clew-βb for some C and R = P — Clβw-βb = o(lβw-βb), then
σWl-βwp + σ2l-βb 二	2 l-βb 「 l-βw P(σW + σ⅛) =i p(σw + C + RlLew) l-βw p =C + RlLw (σW(C + Rlβb-ew)+ σh)
σWl-βwY + σ2l-βb 二 σW l-βw γ + σ2l-βb σW l-βw p + σ2l-βb	=C + RWp-ew (σW YZp(C + RiBLe) + σ2) _ σWγ∕p(C + RleLew) + σh σW (C + RIeb-ew) + σh =(σWYZpC + σ2 + σWγ∕pRleb-ew )(1 _ σWRlebTa + …) σW C + σ2	σW C + σ2	σW C + σ2 =σWYZpC + σ2 + Rleb-ew ( σWYZp _ σWYZpC + σ2	σW	) + σW C + σh +	(σW C + σh - σW C + σh σW C + σh ) + … =σw y⅛c+σ+Rle-w FlJ (YZp - σw y⅛c+σ)+… σW C + σ2	σW C + σ2	σW C + σ2 =¾c⅛2+…σ⅛ ⅞⅛⅛ + θ((Rleb-ew )2(YZP-1)) σW YZpC + σ2 + σW C + σ-	+ O(I) □
For any function f that is (k + 1)-times differentiable in a neighborhood of 0, we have the asymptotic
expansion
n=0
Since
d(W q1/2v0(q)
q→∞
≡ ZI Φ2 (z)z2n dz
whenever the RHS is integrable, we have
Lemma D.2. Suppose φ2(z)z2n is integrable over z ∈ R for all 0 ≤ n ≤ N + 1. Then Vφ(q)
q-1/2(PnN=0 Cnq-n + O(q-N-1)) as q → ∞, where
2nn!√2π
Z∞
φ2(z)z2n dz.
∞
Note that Sechd(Z) = θ(e-dlzl) for Z → ∞ as long as d > 0, so that Cn from the above result
converges when φ = sechd. Therefore
Lemma D.3. Let d > 0. We have V sechd(q) ' q-1/2 Pn≥0 Cnq-n, where
Cn = 2n⅛‰ Zihh…dz.
As corollaries, we obtain the following asymptotics.
LemmaD.4. Vtanh(q) = 3Jq- q-1∕2 + Θ(q-3∕2) as q → ∞.
28
Under review as a conference paper at ICLR 2018
Proof. Use Lemma D.3 along with the fact that tanh(z) = sech2 (Z) and J sech4 Z dz = 2 tanh Z +
1 sech2 z tanh z.	□
Lemma D.5. 1 - V tanh(q) = ∏q- q-1/2 + Θ(qT2) asq → ∞.
Proof. Use Lemma D.3 along with the fact that 1 - tanh2(z) = sech2(z) and sech2 z dz =
tanh z.	□
Lemma D.6. Letd ∈ Rand1 < M < N with N - M ∈ Z≥0. SetΣ(M,N,d) := PaN=Mad. If
we fix M and let N → ∞,
'Θ⑴	ifd< -1
log N + O(1)	if d = —1
Σ(M,N,d)= * +。⑴	if — 1 <d< 0
N — M + 1	if d = 0
.d+1 Nd+1 + 1Nd + O(Nmax(O,dT)) if d > 0
Proof. Euler-MacLaurin formula.	□
Lemma D.7. Suppose (l) satisfies the recurrence
e(I) = e(IT)(1 + lβ).
for some nonzero constant δ ∈ R independent ofl.
•	Ifβ > 1, then (l) = Θ(1).
•	Ifβ = 1, then (l) = Θ(lδ).
•	If 0 < β < 1, then E(I) = exp(1-^l1-β + <Θ(∕max(0,1-2β))). In particular, E(I)=
exp(ι-βl1-β + R), where remainder R is
(Θ(l1-2β) if β < 1/2
R= Θ(log l)	ifβ= 1/2
.Θ(1) ifβ> 1/2
Proof. We have
log E(I) = log E(IT) + log(1 + δ∕lβ)
=log E(IT) + δ∕lβ + Θ(δ2∕l2β)
for large l. Ifβ > 1, then Pl l-β converges, and
log E(l) = log E(0) — Θ(1)
E(l) = Θ(1).
If β = 1, then
log E(l) = log E(0) + δlogl + Θ(1)
E(l) = Θ(lδ).
If β < 1, then
log E(I) = log E(O) +-δ--l1-β + R
1 — β
for a remainder R that is
(Θ(l1-2β) if β < 1/2
Θ(log l) ifβ = 1∕2
.Θ(1) ifβ > 1/2
Exponentiating gives the desired result.	□
29
Under review as a conference paper at ICLR 2018
Lemma D.8. Suppose e(l) = Cl-α + e(l-1)(1 + δ∕lβ) for α ∈ R, C = 0, and δ = 0. Then
• If β > 1, then
◦	(l) = Θ(l1-α) ifα ∈ [0, 1);
◦	(l) = Θ(log l) ifα = 1;
◦	(l) = Θ(1) ifα > 1.
•	If β = 1, then
◦	(l) = Θ(lmax(δ,1-α)) if1 - δ 6= α.
◦	(l) = Θ(lδ log l) if1 - δ = α.
•	If β < 1, then
◦ e()= exp(ι-βl1-β + (Θ(lmax(0,1-2β))). In particular, e(l) = exp(ɪ-el1-β +
R(l; δ, β) + O(1)), where R is the same R as in Lemma D.7, depending on only
l, δ, and β but not on α or C, with
(Θ(l1-2β) ifβ < 1/2
R= Θ(log l)	ifβ= 1/2
(θ⑴	if β> 1/2.
Furthermore, for β = —δ = 1: Ε⑴ 〜l-1 if α > 2, e()〜l1-α if α < 2, and e(l)〜lδ log l if
α = 2.
Proof. We can unwind the recurrence to get
加=C χχ m-α YI(1+n)+M Yl(1+n)
m=1	n=m+1	n=1
Suppose β > 1. By Lemma D.7, we get
l
(l) = θ(1) X m-α + (0)θ(1)
m=1
(θ(l1-α) if α ∈ [0,1)
= θ(log l) ifα = 1
[θ(1) if α > 1.
Now suppose β = 1. By Lemma D.7, we get
l
(l) = X m-αθ(m-δlδ) + (0)θ(lδ)
m=1
where the constants hidden inside the θ are the same in every term of the sum. If α > 1 — δ, then
m-δ-α = o(m-1), so that Plm=1 m-δ-α = θ(1), and
(l) = θ(lδ) + (0)θ(lδ)
= θ(lδ).
On the other hand, if α < 1 — δ, then Plm=1 m-δ-α = θ(l1-δ-α). So
(l) = θ(l1-α) + (0)θ(lδ)
= θ(l1-α).
30
Under review as a conference paper at ICLR 2018
x If α = 1 - δ, then Plm=1 m-δ-α = Θ(log l). So
(l) = Θ(lδ log l) + (0)Θ(lδ)
= Θ(lδ log l).
Finally, if β ∈ (0, 1), then by Lemma D.7,
l
萨=Ceι-βl1-β+R X m-αeι-δβm1-β +θ(lmax(0,1-2β)) + eι⅛l1-β+R,
m=1
―l	— δ m 1 — β +Θ(∕max(0,1 — 2β))
where R is the remainder as in Lemma D.7. The sUm Em=I m-αeKm +θ(l	) Canbe
upper and lower bounded by integrals of the form R x-ae-μx1 β dx for appropriate μs, which are
finite (they are bounded by values of incomplete Gamma functions). Thus & = Θ(1)e 1-β" +R
where
(Θ(l1-2β) if β < 1/2
R0 =R+O(1) = Θ(log l)	ifβ= 1/2
[θ(1) if β > 1/2.
.. l∣∖	δ ι1-β + Θ(lmax(O，1 -2G))
A fortiori, e(l) = e 1-β	+θ(l	).
For our “furthermore” claim: the case of δ = -1 telescopes, so that the upper and lower constants
hidden in Θ can both be taken to be 1.	□
31