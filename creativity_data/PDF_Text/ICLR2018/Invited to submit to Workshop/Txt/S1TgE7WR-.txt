Workshop track - ICLR 2018
Covariant Compositional Networks	For
Learning Graphs
Risi Kondor, Truong Son Hy, Horace Pan & Brandon M. Anderson
Department of Computer Science
The University of Chicago
Chicago, IL - 60637
{risi,hytruongson,hopan,brandona}@cs.uchicago.edu
Shubhendu Trivedi
Toyota Technological Institute
Chicago, IL - 60637
shubhendu@ttic.edu
Ab stract
Most existing neural networks for learning graphs address permutation invariance
by conceiving of the network as a message passing scheme, where each node sums
the feature vectors coming from its neighbors. We argue that this imposes a limita-
tion on their representation power, and instead propose a new general architecture
for representing objects consisting of a hierarchy of parts, which we call covari-
ant compositional networks (CCNs). Here, covariance means that the activation
of each neuron must transform in a specific way under permutations, similarly
to steerability in CNNs. We achieve covariance by making each activation trans-
form according to a tensor representation of the permutation group, and derive
the corresponding tensor aggregation rules that each neuron must implement. Ex-
periments show that CCNs can outperform competing methods on standard graph
learning benchmarks.
1	Introduction
Learning on graphs has a long history in the kernels literature, including approaches based on ran-
dom walks (Gartner, 2002; BorgWardt & Kriegel, 2005; Feragen et al., 2013), counting subgraphs
(Shervashidze et al., 2009), spectral ideas (Vishwanathan et al., 2010), label propagation schemes
with hashing (Shervashidze et al., 2011; Neumann et al., 2016), and even algebraic ideas (Kondor
& Borgwardt, 2008). Many of these papers address moderate size problems in chemo- and bioinfor-
matics, and the way they represent graphs is essentially fixed.
Recently, with the advent of deep learning and much larger datasets, a sequence of neural net-
work based approaches have appeared to address the same problem, starting with (Scarselli et al.,
2009). In contrast to the kernels framework, neural networks effectively integrate the classification
or regression problem at hand with learning the graph representation itself, in a single, end-to-end
system. In the last few years, there has been a veritable explosion in research activity in this area.
Some of the proposed graph learning architectures (Duvenaud et al., 2015; Kearnes et al., 2016;
Niepert et al., 2016) directly seek inspiration from the type of classical CNNs that are used for im-
age recognition (LeCun et al., 1998; Krizhevsky et al., 2012). These methods involve first fixing a
vertex ordering, then moving a filter across vertices while doing some computation as a function of
the local neighborhood to generate a representation. This process is then repeated multiple times
like in classical CNNs to build a deep graph representation. Other notable works on graph neural
networks include (Li et al., 2016; Schutt et al., 2017; Battaglia et al., 2016; Kipf & Welling, 2017).
Very recently, (Gilmer et al., 2017) showed that many of these approaches can be seen to be spe-
cific instances of a general message passing formalism, and coined the term message passing neural
networks (MPNNs) to refer to them collectively.
1
Workshop track - ICLR 2018
While MPNNs have been very successful in applications and are an active field of research, they
differ from classical CNNs in a fundamental way: the internal feature representations in CNNs are
equivariant to such transformations of the inputs as translation and rotations (Cohen & Welling,
2016; 2017), the internal representations in MPNNs are fully invariant. This is a direct result of
the fact that MPNNs deal with the permutation invariance issue in graphs simply by summing the
messages coming from each neighbor. In this paper we argue that this is a serious limitation that
restricts the representation power of MPNNs.
MPNNs are ultimately compositional (part-based) models, that build up the representation of the
graph from the representations of a hierarchy of subgraphs. To address the covariance issue, we
study the covariance behavior of such networks in general, introducing a new general class of neural
network architectures, which we call compositional networks (comp-nets). One advantage of this
generalization is that instead of focusing attention on the mechanics of how information propagates
from node to node, it emphasizes the connection to convolutional networks, in particular, it shows
that what is missing from MPNNs is essentially the analog of steerability.
Steerability implies that the activations (feature vectors) at a given neuron must transform according
to a specific representation (in the algebraic sense) of the symmetry group of its receptive field, in our
case, the group of permutations, Sm. In this paper we only consider the defining representation and
its tensor products, leading to first, second, third etc. order tensor activations. We derive the general
form of covariant tensor propagation in comp-nets, and find that each “channel” in the network
corresponds to a specific way of contracting a higher order tensor to a lower order one. Note that
here by tensor activations we mean not just that each activation is expressed as a multidimensional
array of numbers (as the word is usually used in the neural networks literature), but also that it
transforms in a specific way under permutations, which is a more stringent criterion. The parameters
of our covariant comp-nets are the entries of the mixing matrix that prescribe how these channels
communicate with each other at each node. Our experiments show that this new architecture can
beat scalar message passing neural networks on several standard datasets.
2	Learning graphs
Graph learning encompasses a broad range of problems where the inputs are graphs and the outputs
are class labels (classification), real valued quantities (regression) or more general, possibly combi-
natorial, objects. In the standard supervised learning setting this means that the training set consists
of m input/output pairs {(G1, y1), (G2, y2), . . . , (Gm, ym)}, where each Gi is a graph and yi is the
corresponding label, and the goal is to learn a function h : G → y that will successfully predict the
labels of further graphs that were not in the training set.
By way of fixing our notation, in the following we assume the each graph G is a pair (V, E),
where V is the vertex set of G and E ⊆ V × V is its edge set. For simplicity, we assume that
V = {1, 2, . . . , n}. We also assume that G has no self-loops ((i, i) 6∈ E for any i ∈ V ) and that
G is symmetric, i.e., (i, j) ∈ E ⇒ (j, i) ∈ E1. We will, however, allow each edge (i, j) to have a
corresponding weight wi,j , and each vertex i to have a corresponding feature vector (vertex label)
li ∈ Rd . The latter, in particular, is important in many scientific applications, where li might encode,
for example, what type of atom occupies a particular site in a molecule, or the identity of a protein
in a biochemical interaction network. All the topological information about G can be summarized
in an adjacency matrix A ∈ Rn×n, where Ai,j = wi,j if i and j are connected by an edge, and
otherwise Ai,j = 0. When dealing with labeled graphs, we also have to provide (l1, . . . , ln) to fully
specify G.
One of the most fascinating aspects of graphs, but also what makes graph learning challenging, is
that they involve structure at multiple different scales. In the case when G is the graph of a protein,
for example, an ideal graph learning algorithm would represent G in a manner that simultaneously
captures structure at the level of individual atoms, functional groups, interactions between functional
groups, subunits of the protein, and the protein’s overall shape.
The other major requirement for graph learning algorithms relates to the fact that the usual ways
to store and present graphs to learning algorithms have a critical spurious symmetry: If we were to
1Our framework has natural generalizations to non-symmetric graphs and graphs with self-loops, but in the
interest of keeping our discussion as simple as possible, we will not discuss these cases in the present paper.
2
Workshop track - ICLR 2018
Figure 1: (a) A small graph G with 6 vertices and its adjacency matrix. (b) An alternative form G0 of
the same graph, derived from G by renumbering the vertices by a permutation σ : {1, 2, . . . , 6} 7→
{1, 2, . . . , 6}. The adjacency matrices of G and G0 are different, but topologically they represent the
same graph. Therefore, we expect the feature map φ to satisfy φ(G) = φ(G0).
permute the vertices of G by any permutation σ : {1, 2, . . . , n} → {1, 2, . . . , n} (in other words,
rename vertex 1 as σ(1), vertex 2 as σ(2), etc.), then the adjacency matrix would change to
A0i,j = Aσ-1(i),σ-1(j) ,
and simultaneously the vertex labels would change to (l10 , . . . , ln0 ), where l0i = lσ-1(i). However,
G0 = (A0, l10 , . . . , ln0 ) would still represent exactly the same graph as G = (A, l1, . . . , ln). In partic-
ular, (a) in training, whether G or G0 is presented to the algorithm must not make a difference to the
final hypothesis h that it returns, (b) h itself must satisfy h(G) = h(G0) for any labeled graph and
its permuted variant.
Most learning algorithms for combinatorial objects hinge on some sort of fixed or learned internal
representation of data, called the feature map, which, in our case we denote φ(G). The set of all n!
possible permutations of {1, 2, . . . , n} forms a group called the symmetric group of order n, denoted
Sn. The permutation invariance criterion can then be formulated as follows (Figure 1).
Definition 1. Let A be a graph learning algorithm that uses a feature map G 7→ φ(G). We say
that the feature map φ (and consequently the algorithm A) is permutation invariant if, given any
n ∈ N, any n vertex labeled graph G = (A, l1, . . . , ln), and any permutation σ ∈ Sn, letting
G0 = (A0, l10 , . . . , ln0 ), where A0i,j = Aσ-1(i),σ-1(j) and li0 = lσ-1(i), we have that φ(G) = φ(G0).
Capturing multiscale structure and respecting permutation invariance are the two the key constraints
around which most of the graph learning literature revolves. In kernel based learning, for exam-
ple, invariant kernels have been constructed by counting random walks (Gartner, 20θ2), matching
eigenvalues of the graph Laplacian (Vishwanathan et al., 2010) and using algebraic ideas (Kondor
& Borgwardt, 2008).
3	Compositional networks
Many recent graph learning papers, whether or not they make this explicit, employ a compositional
approach to modeling graphs, building up the representation ofG from representations of subgraphs.
At a conceptual level, this is similar to part-based modeling, which has a long history in machine
learning (Fischler & Elschlager, 1973; Ohta et al., 1978; Tu et al., 2005; Felzenszwalb & Hutten-
locher, 2005; Zhu & Mumford, 2006; Felzenszwalb et al., 2010). In this section we introduce a
general, abstract architecture called compositional networks (comp-nets) for representing com-
plex objects as a combination of their parts, and show that several exisiting graph neural networks
can be seen as special cases of this framework.
Definition 2. Let G be a compound object with n elementary parts (atoms) E = {e1, . . . , en}. A
composition scheme for G is a directed acyclic graph (DAG) M in which each node ni is associated
with some subset Pi of E (these subsets are called the parts of G) in such a way that
1.	If ni is a leaf node, then Pi contains a single atom eξ(i)2.
2.	M has a unique root node nr, which corresponds to the entire set {e1 , . . . , en}.
3.	For any two nodes ni and nj, if ni is a descendant of nj, then Pi ⊂ Pj.
We define a compositional network as a composition scheme in which each node ni also carries a
feature vector fi that provides a representation of the corresponding part (Figure 2). When we want
2Here ξ is just a function that establishes the mapping between each leaf node and the corresponding atom.
3
Workshop track - ICLR 2018
Figure 2: (a) A composition scheme for an object G is a DAG in which the leaves correspond to
atoms, the internal nodes correspond to sets of atoms, and the root corresponds to the entire object.
(b) A compositional network is a composition scheme in which each node ni also carries a feature
vector fi . The feature vector at ni is computed from the feature vectors of the children of ni .
Figure 3: A minimal requirement for composition schemes is that they be invariant to permutation,
i.e. that if the numbering of the atoms is changed by a permutation σ, then we must get an isomorphic
DAG. Any node in the new DAG that corresponds to {e0i , . . . , e0i } must have a corrresponding node
in the old DAG corresponding to {eσ-1(i1) , . . . , eσ-1(ik)}.
to emphasize the connection to more classical neural architectures, we will refer to ni as the i’th
neuron, Pi as its receptive field3, and fi as its activation.
Definition 3. Let G be a compound object in which each atom ei carries a label li, and M a
composition scheme for G. The corresponding compositional network N is a DAG with the same
structure as M in which each node ni also has an associated feature vector fi such that
1. If ni is a leaf node, then fi = lξ(i).
2. Ifni is a non-leaf node, and its children are nc1 , . . . , nck, then fi = Φ(fc1 , fc2 , . . . , fck) for some
aggregation function Φ. (Note: in general, Φ can also depend on the relationships between the
subparts, but for now, to keep the discussion as simple as possible, we ignore this possibility.)
The representation φ(G) afforded by the comp-net is given by the feature vector fr of the root.
Note that while, for the sake of concreteness, we call the fi’s “feature vectors”, there is no reason
a priori why they need to be vectors rather than some other type of mathematical object. In fact, in
the second half of the paper we make a point of treating the fi’s as tensors, because that is what will
make it the easiest to describe the specific way that they transform with respect to permutations.
3Here and in the following by the “receptive field” of a neuron ni in a feed-forward network we mean the
set of all input neurons from which information can propagate to ni .
4
Workshop track - ICLR 2018
In compositional networks for graphs, the atoms will usually be the vertices, and the Pi parts will
correspond to clusters of nodes or neighborhoods of given radii. Comp-nets are particularly attrac-
tive in this domain because they can combine information from the graph at different scales. The
comp-net formalism also suggests a natural way to satisfy the permutation invariance criterion of
Definition 1.
Definition 4. LetM be the composition scheme ofan object G with n atoms and M0 the composition
scheme of another object that is equivalent in structure to G, except that its atoms have been per-
muted by some permutation σ ∈ Sn (ei = eσ-i(i) and ' = 'σ-i(i)). We s^y that M (more precisely,
the algorithm generating M) is permutation invariant if there is a bijection ψ : M → M0 taking
each na ∈ M to some n0b ∈ M0 such that if Pa = {ei1 , . . . , eik }, then Pb0 = {e0σ(i ), . . . , e0σ(i )}.
Proposition 1. Let φ(G) be the output of a comp-net based on a composition scheme M. Assume
1. M is permutation invariant in the sense of Definition 4.
2. The aggregation function Φ(fc1 , fc2 , . . . , fck) used to compute the feature vector of each node
from the feature vectors of its children is invariant to the permutations of its arguments.
Then the overall representation φ(G) is invariant to permutations of the atoms. In particular, if G is
a graph and the atoms are its vertices, then φ is a permutation invariant graph representation.
3.1 Message passing neural networks as a special case of comp-nets
Graph learning is not the only domain where invariance and multiscale structure are important: the
most commonly cited reasons for the success of convolutional neural networks (CNNs) in image
tasks is their ability to address exactly these two criteria in the vision context. Furthermore, each
neuron ni in a CNN aggregates information from a small set of neurons from the previous layer,
therefore its receptive field, corresponding to Pi, is the union of the receptive fields of its “children”,
so we have a hierarchical structure very similar to that described in the previous section. In this sense,
CNNs are a specific kind of compositional network, where the atoms are pixels. This connection
has inspired several authors to frame graph learning as a generalization of convolutional nets to
the graph domain (Bruna et al., 2014; Henaff et al., 2015; Duvenaud et al., 2015; Defferrard et al.,
2016; Kipf & Welling, 2017). While in mathematics convolution has a fairly specific meaning that
is side-stepped by this analogy, the CNN analogy does suggest that a natural way to define the Φ
aggregation functions is to let Φ(fc1 , fc2 , . . . , fck) be a linear function of fc1 , fc2, . . . , fck followed
by a pointwise nonlinearity, such as a ReLU operation.
To define a comp-net for graphs we also need to specify the composition scheme M. Many algo-
rithms define M in layers, where each layer (except the last) has one node for each vertex of G:
M1. In layer `= 0 each node ni0 represents the single vertex Pi0 = {i}.
M2. In layers ` = 1, 2, . . . , L, node ni` is connected to all nodes from the previous level that are
neighbors of i in G, i.e., the children of ni` are
ch(n') = {n'-1 | j ∈N⑶},
where N(i) denotes the set of neighbors of i in G. Therefore, Pi = Uj∈N(i) P'-1.
M3. In layer L+1 we have a single node nr that represents the entire graph and collects information
from all nodes at level L.
Since this construction only depends on topological information about G, the resulting composition
scheme is guaranteed to be permutation invariant in the sense of Definition 4.
A further important consequence of this way of defining M is that the resulting comp-net can be
equivalently interpreted as label propagation algorithm, where in each round `= 1, 2, . . . , L, each
vertex aggregates information from its neighbors and then updates its own label.
5
Workshop track - ICLR 2018
Algorithm 1 The label propagation algorithm corresponding to M1-M3
for each vertex i
fi J li
for ' = 1 to L
for each vertex i
fi J Φ(f-1,…，ft1) where Nm = {iι,...,ik}
φ(G) ≡ fr J Φ(f1L,...,fnL)
Many authors choose to describe graph neural networks exclusively in terms of label propagation,
without mentioning the compositional aspect of the model. Gilmer et al. (2017) call this general
approach message passing neural networks, and point out that a range of different graph learning ar-
ChitectUres are special cases of it. More broadly, the classic Weisfeiler-Lehman test of isomorphism
also follows the same logic (Weisfeiler & Lehman, 1968; Read & Corneil, 1977; Cai et al., 1992),
and so does the related Weisfeiler-Lehman kernel, arguably the most successful kernel-based ap-
proach to graph learning (Shervashidze et al., 2011). Note also that in label propagation or message
passing algorithms there is a clear notion of the source domain of vertex i at round `, as the set of
vertices that can influence f' and this corresponds exactly to the receptive field Pi of “neuron” n'
in the comp-net picture.
The following proposition is immediate from the form of Algorithm 1 and reassures us that message
passing neural networks, as special cases of comp-nets, do indeed produce permutation invariant
representations of graphs.
Proposition 2. Any label propagation scheme in which the aggregation function Φ is invariant to
the permutations of its arguments is invariant to permutations in the sense of Definition 1.
In the next section we argue that invariant message passing networks are limited in their represen-
tation power, however, and describe a generalization via comp-nets that overcomes some of these
limitations.
4 C ovariant compositional networks
One of the messages of the present paper is that invariant message passing algorithms, of the form
described in the previous section, are not the most general possible compositional models for pro-
ducing permutation invariant representations of graphs (or of compound objects, in general).
Once again, an analogy with image recognition is helpful. Classical CNNs face two types of basic
image transformations: translations and rotations. With respect to translations (barring pooling,
edge effects and other complications), CNNs behave in a quasi-invariant way, in the sense that if the
input image is translated by any integer amount (tx, ty), the activations in each layer ` = 1, 2, . . . L
translate the same way: the activation of any neuron ni,j is simply transferred to neuron ni+tι ,j+t2,
i.e., f 0i+t1,j+t2= fij. This is the simplest manifestation of a well studied property of CNNs called
equivariance (Cohen & Welling, 2016; Worrall et al., 2017).
With respect to rotations, however, the situation is more complicated: if we rotate the input image
by, e.g., 90 degrees, not only will the part of the image that fell in the receptive field ofa particular
neuron ni`,j move to the receptive field ofa different neuron n`j,-i, but the orientation of the receptive
field will also change (Figure 4). Consequently, features which were, for example, previously picked
up by horizontal filters will now be picked up by vertical filters. Therefore, in general, fj,- = fj
It can be shown that one cannot construct a CNN for images that behaves in a quasi-invariant way
with respect to both translations and rotations unless every filter is directionless.
It is, however, possible to construct a CNN in which the activations transform in a predictable and re-
versible way, in particular, fj,- = R(fj) for some fixed invertible function R. This phenomenon
is called steerability, and has a significant literature in both classical signal processing (Freeman &
Adelson, 1991; Simoncelli et al., 1992; Perona, 1995; Teo & Hel-Or, 1998; Manduchi et al., 1998)
and the neural networks field (Cohen & Welling, 2017).
6
Workshop track - ICLR 2018
Figure 4: In convolutional neural networks if the input image is translated by some amount (t1, t2),
What used to fall in the receptive field of neuron n',j is moved to the receptive field of n'+t1,j+t2.
Therefore, the activations transform in the very simple way f 0'+t1,j+t2 = fj In contrast, rota-
tions not only move the receptive fields around, but also permute the neurons in the receptive field
internally, therefore, in general, fj — = fj The right hand figure shows that if the CNN has a
horizontal filter (blue) and a vertical one (red) then their activations are exchanged by a 90 degree
rotation. In steerable CNNs, if (i,j) → (i0,j0), then f 0' j = R(fj) for some fixed linear function
of the rotation.
The situation in compositional networks is similar. The comp-net and message passing architectures
that we have examined so far, by virtue of the aggregation function being symmetric in its arguments,
are all quasi-invariant (with respect to permutations) in the following sense.
Definition 5. Let G be a compound object of n parts and G0 an equivalent object in which the atoms
have been permuted by some permutation σ. Let N be a comp-net for G based on an invariant
composition scheme, andN0 be the corresponding network for G0. We say that N is quasi-invariant
if for any ni ∈ N, letting n0j be the corresponding node in N0, fi = fj0 for any σ ∈ Sn
Quasi-invariance in comp-nets is equivalent to the assertion that the activation fi at any given node
must only depend on Pi = {ej1 , . . . , ejk} as a set, and not on the internal ordering of the atoms
ej1 , . . . , ejk making up the receptive field. At first sight this seems desirable, since it is exactly
what we expect from the overall representation φ(G). On closer examination, however, we realize
that this property is potentially problematic, since it means that ni has lost all information about
which vertex in its receptive field has contributed what to the aggregate information fi . In the CNN
analogy, we can say that we have lost information about the orientation of the receptive field. In
particular, if, further upstream, fi is combined with some other feature vector fj from a node with an
overlapping receptive field, the aggregation process has no way of taking into account which parts
of the information in fi and fj come from shared vertices and which parts do not (Figure 5).
The solution is to upgrade the Pi receptive fields to be ordered sets, and explicitly establish how fi
co-varies with the internal ordering of the receptive fields. To emphasize that henceforth the Pi sets
are ordered, we will use parentheses rather than braces to denote their content.
Definition 6. Let G, G0, N and N0 be as in Definition 5. Let ni be any node of N and nj the
corresponding node of N0. Assume that Pi = (ep1 , . . . , epm) while Pj0 = (eq1 , . . . , eqm), and
let π ∈ Sm be the permutation that aligns the orderings of the two receptive fields, i.e., for which
eqπ(a) = epa. We say that N is covariant to permutations if for any π, there is a corresponding
function Rπ such that fj0 = Rπ (fi).
4.1	First order covariant comp-nets
The form of covariance prescribed by Definition 6 is very general. To make it more specific, in
line with the classical literature on steerable representations, we make the assumption that the
{f 7→ Rπ (f)}π∈Sm maps are linear, and by abuse of notation, from now on simply treat them
as matrices (with Rπ (f) = Rπf). The linearity assumption automatically implies that {Rπ}π∈Sm
is a representation of Sm in the group theoretic sense of the word (for the definition of group repre-
sentations, see the Appendix)4.
Proposition 3. If for any π ∈ Sm, the f 7→ Rπ(f) map appearing in Definition 6 is linear, then the
corresponding {Rπ}π∈Sm matrices form a representation of Sm.
4This notion of representation must not be confused with the neural networks sense of representations of
objects, as in “f is a representation of P'"
7
Workshop track - ICLR 2018
Figure 5: Top left: At level ` = 1 n3 aggregates information from {n4 , n5 } and n2 aggregates
information {n5, n6}. At ` = 2, n1 collects this summary information from n3 and n2. Bottom left:
This graph is not isomorphic to the top one, but the activations ofn3 and n2 at` = 1 will be identical.
Therefore, at ` = 2, n1 will get the same inputs from its neighbors, irrespective of whether or not
n5 and n7 are the same node or not. Right: Aggregation at different levels. For keeping the figure
legible only the neighborhood around one node in higher levels is marked.
The representation theory of symmetric groups is a rich subject that goes beyond the scope of the
present paper (Sagan, 2001). However, there is one particular representation of Sm that is likely
familiar even to non-algebraists, the so-called defining representation, given by the Pπ ∈ Rn×n
permutation matrices
[P ]	=	1 ifπ(j)=i
[Pπ]i,j = 0 otherwise.
It is easy to verify that Pπ2π1 = Pπ2 Pπ1 for any π1,π2 ∈ Sm, so {Pπ}π∈Sm is indeed a representa-
tion of Sm . If the transformation rules of the fi activations in a given comp-net are dictated by this
representation, then each fi must necessarily be a |Pi| dimensional vector, and intuitively each com-
ponent of fi carries information related to one specific atom in the receptive field, or the interaction
of that specific atom with all the others. We call this case first order permutation covariance.
Definition 7. We say that ni is a first order covariant node in a comp-net if under the permutation
of its receptive field Pi by any π ∈ S|Pi|, its activation trasforms as fi 7→ Pπfi.
4.2	Second order covariant comp-nets
It is easy to verify that given any representation (Rg)g∈∈G of a group G, the matrices (Rg 0 Rg )g∈∈G
also furnish a representation of G. Thus, one step UP in the hierarchy from Pn-covariant comp-nets
are P∏ 0P∏-covariant comp-nets, where the f feature vectors are now |Pi|2 dimensional vectors
that transform under permutations of the internal ordering by π as fi 7→ (Pπ 0Pπ)fi.
If We reshape f into a matrix Fi ∈ RlPil×lPil, then the action
Fi 7→ Pπ Fi Pπ>
is equivalent to Pπ0Pπ acting on fi. In the following, we will prefer this more intuitive matrix view,
since it clearly expresses that feature vectors that transform this way express relationships between
the different constituents of the receptive field. Note, in particular, that if we define A" as the
restriction of the adjacency matrix to Pi (i.e., if Pi = (epι,..., epm) then [A^pi]ab = Apa,pb), then
A]pi transforms exactly as Fi does in the equation above.
Definition 8. We say that ni is a second order covariant node in a comp-net if under the permutation
of its receptive field Pi by any π ∈ S|Pi|, its activation transforms as Fi 7→ Pπ FiPπ>.
8
Workshop track - ICLR 2018
4.3	Third and higher order covariant comp-nets
Taking the pattern further lets us consider third, fourth, and general, k’th order nodes in our comp-
net, in which the activations are k’th order tensors, transforming under permutations as
Fi 7→ Fi0	where	[Fi0]j1,...,jk =XX...X[Pπ]j1,j10[Pπ]j2,j20 ...[Pπ]jk,jk0[Fi]j10,...,jk0,
j10 j20	jk0
In the more compact, so called Einstein notation5,
[Fi0]j1,...,jk = [Pπ]j1 1 [Pπ]j2 2 . . . [Pπ]jk k [Fi]j10 ,...,jk0 .	(1)
In general, we will call any quantity which transforms according to this equation a k’th order P-
tensor. Note that this notion of tensors is distinct from the common usage of the term in neural
networks, and more similar to how the word is used in Physics, because it not only implies that Fi
is a quanity representable by an m × m × . . . × m array of numbers, but also that Fi transforms in a
specific way.
Since scalars, vectors and matrices can be considered as 0th, 1st and 2nd order tensors, respectively,
the following definition covers Definitions 5, 7 and 8 as special cases (with quasi-invariance being
equivalent to zeroth order equivariance). To unify notation and terminology, regardless of the di-
mensionality, in the following we will always talk about feature tensors rather than feature vectors,
and denote the activations with Fi rather than fi , as we did in the first half of the paper.
Definition 9. We say that ni is a k’th order covariant node in a comp-net if the corresponding
activation Fi is a k 'th order P-tensor, i.e., it transforms under permutations of Pi according to(1),
or the activation is a Sequence of C separate P -tensors Fi(1),..., Fi(c) corresponding to C distinct
channels.
5 Tensor aggregation rules
The previous sections prescribed how activations must transform in comp-nets of different orders,
but did not explain how this can be assured, and what it entails for the Φ aggregation functions.
Fortunately, tensor arithmetic provides a compact framework for deriving the general form of these
operations. Recall the four basic operations that can be applied to tensors6:
1.	The tensor product of A ∈ Tk with B ∈ Tp yields a tensor C = A 0 B ∈ Tp+k where
i1 ,i2 ,...,ik+p	i1 ,i2 ,...,ik ik+1 ,ik+2 ,...,ik+p .
2.	The elementwise product of A ∈ Tk with B ∈ Tp along dimensions (a1, a2, . . . , ap ) yields a
tensor C = A(a1,...,ap)B ∈ Tk where
Ci1,i2,...,ik = Ai1,i2,...,ik Bia1 ,ia2 ,...,iap .
3.	The projection (summation) of A ∈ Tk along dimensions {a1, a2 , . . . , ap } yields a tensor
C = Ala1,…,ap ∈Tk-p With
4.
Ci1 ,i2,...,ik
. . .	Ai1,i2,...,ik,
where we assume that ia1 , . . . , iap have been removed from amongst the indices of C.
The contraction ofA ∈ Tk along the pair of dimensions {a, b} (assuming a < b) yields a k -
order tensor
Ci1 ,i2,...,ik
Ai1,.
..,ia-1 ,j,ia+i ,...,ib-1 ,j,ib+1
,...,k,
2
j
5The Einstein convention is that if, in a given tensor expression the same index appears twice, once “up-
stairs” and once “downstairs”, then it is summed over. For example, the matrix/vector product y = Ax would
be written yi = Aij xj
6Here and in the following Tk will denote the class of k’th order tensors (k dimensional tensors), regardless
of their transformation properties.
9
Workshop track - ICLR 2018
where again we assume that ia and ib have been removed from amongst the indices of C. Using
Einstein notation this can be written much more compactly as
,i2,...,ik = Ai1,i2,...,ik
where δia,ib is the diagonal tensor with δi,j = 1 if i = j and 0 otherwise. In a somewhat
unorthodox fashion, we also generalize contractions to (combinations of) larger sets of indices
{{a11, . . . , a1p1}, {a12, . . . , a2p2}, . . . , {aq1, . . . , aqpq}} as the (k - Pj pj) order tensor
1122	qq
C... = Ai1,i2,...,ik δa1,...,ap1 δa1,...,ap2 . . . δa1,...,apq
Note that this subsumes projections, since it allows Us to write AK,…q in the slightly unusual
looking form
AJaι,..,ap= Ai1,i2,...,ik δiaι δia2 . . . δiak .
The following proposition shows that, remarkably, all of the above operations (as well as taking
linear ConbinationS) preserve the way that P-tensors behave under permutations and thus they can
be freely “mixed and matched” within Φ.
Proposition 4. Assume that A and B are k 'th and P 'th order P-tensors, respectively. Then
1.	A 0 B is a k + P ,th order P-tensor.
2.	A (a1,...,ap)B is a k’th order P -tensor.
3.	A(。],…,a? is a k 一P,th order P-tensor.
4.	Ai1,i2,...,ik δa1,...,ap1 . . . δa1,...,aqpq is a k - Pj Pj ’th order P -tensor.
In addition, if A1, . . . , Au are k’th order P -tensors and α1, . . . , αu are scalars, then j αjAj is a
k’th order P -tensor.
The more challenging part of constructing the aggregation scheme for comp-nets is establishing how
to relate P -tensors at different nodes. The following two propositions answer this question.
Proposition 5. Assume that node na is a descendant of node nb in a comp-net N , Pa =
(ep1 , . . . , epm) and Pb = (eq1 , . . . , eqm0 ) are the corresponding ordered receptive fields (note that
this implies that, as sets, Pa ⊆ Pb), and χa→b ∈ Rm×m0 is an indicator matrix defined
b =	1 if qj = Pi
0 otherwise.
Assume that F is a k’th order P -tensor with respect to permutations of (ep1 , . . . , epm). Then,
dropping the a→b superscript for clarity,
Fi1,...,ik = χi1j1 χi2j2 . . . χikjk Fj1,...,jk
(2)
is a k’th order P -tensor with respect to permutations of (eq1 , . . . , eqm0 ).
Equation 2 tells us that when node nb aggregates P -tensors from its children, it first has to “promote”
them to being P -tensors with respect to the contents of its own receptive field by contracting along
each of their dimensions with the appropriate χa→b matrix. This is a critical element in comp-nets
to guarantee covariance.
Proposition 6. Let nc1 , . . . , ncs be the children of nt in a message passing type comp-net with
corresponding k’th order tensor activations Fc1 , . . . , Fcs . Let
[Fecu]i1,...,ik = [χcu→t]i1j1 [χcu→t]i2j2 ... [χcu→t]ikjk [Fcu]j1,...,jk
be the promotions of these activations to P -tensors of nt..Assume that Pt = (epi,..., epm). Now
let F be a k +1 ,th order object in which the j'th slice is Fpj if n?j is one of the children of nt, i.e.,
1 ,...,ik,j = [Fpj]i1,...,ik ,
and zero otherwise. Then F is a k + 1 ,th order P -tensor of nt.
Finally, as already mentioned, the restriction of the adjacency matrix to Pi is a second order P-
tensor, which gives an easy way of explicitly adding topological information to the activation.
Proposition 7. IfFi is a k’th order P -tensor at node ni, and A(Pi is the restriction of the adjacency
matrix to Pi as defined in Section 4.2, then F0A(Pi is a k+2’th order P -tensor.
10
Workshop track - ICLR 2018
5.1	The general aggregation function and its special cases
Combining all the above results, assuming that node nt has children nc1 , . . . , ncs , we arrive at the
following general algorithm for the aggregation rule Φt :
1.	Collect all the k’th order activations Fc1 , . . . , Fcs of the children.
2.	Promote each activation to Fec1 , . . . , Fecs (Proposition 5).
3.	Stack Fec1 , . . . , Fecs together into a k+ 1 order tensor T (Proposition 6).
4.	Optionally form the tensor product of T with AJPt to get a k+3 order tensor H (otherwise
just set H = T) (Proposition 7).
5.	Contract H along some number of combinations of dimensions to get s separate lower
order tensors Q1, . . . , Qs (Proposition 4).
6.	Mix Q1, . . . , Qs with a matrix W ∈ Rs0×s and apply a nonlinearity Υ to get the final
activation of the neuron, which consists of the s0 output tensors
s
F ⑴=Y X Wi,j Qj + bil	i =1, 2,...s0,
j=1
where the b scalars are bias terms, and Iisthe |Pt| × ... X |Pt| dimensional all ones
tensor.
A few remarks are in order about this general scheme:
1.	Since Fc1 , . . . , Fcs are stacked into a larger tensor and then possibly also multiplied by AJP ,
the general tendency would be for the tensor order to increase at every node, and the correspond-
ing storage requirements to increase exponentially. The purpose of the contractions in Step 5
is to counteract this tendency, and pull the order of the tensors back to some small number,
typically 1, 2 or 3.
2.	However, since contractions can be done in many different ways, the number of channels will
increase. When the number of input channels is small, this is reasonable, since otherwise the
number of learnable weights in the algorithm would be too small. However, if unchecked,
this can also become problematic. Fortunately, mixing the channels by W on Step 6 gives an
opportunity to stabilize the number of channels at some value s0.
3.	In the pseudocode above, for simplicity, the number of input channels is one and the number
of output channels is s0 . More realistically, the inputs would also have multiple channels (say,
s0) which would be propagated through the algorithm independently up to the mixing stage,
making W an s0 × S × so dimension tensor (not in the P-tensor sense!).
4.	The conventional part of the entire algorithm is Step 6, and the only learnable parameters are
the entries of the W matrix (tensor) and the bi bias terms. These parameters are shared by all
nodes in the network and learned in the usual way, by stochastic gradient descent.
5.	Our scheme could be elaborated further while maintaining permutation covariance by, for ex-
ample taking the tensor product of T with itself, or by introducing AJPt in a different way.
However, the way that Fec1 , . . . , Fecs and AJPt are combined by tensor products is already much
more general and expressive than conventional message passing networks.
6.	Our framework admits many design choices, including the choice of the order odf the activa-
tions, the choice of contractions, and c0 . However, the overall structure of Steps 1-5 is fully
dictated by the covariance constraint on the network.
7.	The final output of the network φ(G) = Fr must be permutation invariant. That means that
the root node nr must produce a tuple of zeroth order tensors (scalars) (Fr(1), . . . , Fr(c)). This
is similar to how many other graph representation algorithms compute φ(G) by summing the
activations at level L or creating histogram features.
We consider a few special cases to explain how tensor aggregation relates to more conventional
message passing rules.
11
Workshop track - ICLR 2018
5.1.1	Zeroth order tensor aggregation
Constraining both the input tensors Fc1 , . . . , Fcs and the outputs to be zeroth order tensors, i.e.,
scalars, and foregoing multiplication by AJPt greatly simplifies the form of Φ. In this case there is
no need for promotions, and T is just the vector (F' ,...,Ff). There is only one way to contract
a vector into a scalar, and that is to sum its elements. Therefore, in this case, the entire aggregation
algorithm reduces to the simple formula
c
Fi = Υ w X Fcu + b .
u=1
For a neural network this is too simplistic. However, it,s interesting to note that the Weisfeiler-
Lehmann isomorphism test essentially builds on just this formula, with a specific choice of Υ (Read
& Corneil, 1977). If we allow more channels in the inputs and the outputs, W becomes a matrix,
and we recover the simplest form of neural message passing algorithms (Duvenaud et al., 2015).
5.1.2	First order tensor aggregation
In first order tensor aggregation, assuming that |Pi | = m, Fc1 , . . . , Fcs are m dimensional column
vectors, and T is an m × m matrix consisting of Fec1 , . . . , Fecs stacked columnwise. There are
two ways of contracting (in our generalized sense) a matrix into a vector: by summing over its
rows, or summing over its columns. The second of these choices leads us back to summing over all
contributions from the children, while the first is more interesting because it corresponds to summing
Fec1 , . . . , Fecs as vectors individually. In summary, we get an aggregation function that transforms a
single input channel to two output channels of the form
Fi(1) =Υhw1,1(T>1)+w1,2(T1)+b11i,	Fi(2) =Υhw2,1(T>1)+w2,2(T1)+b21i,
where 1 denotes the m dimensional all ones vector. Thus, in this layer W ∈ R2×2 . Unless con-
strained by c0 , in each subsequent layer the number of channels doubles further and these channels
can all mix with each other, so W(2) ∈ R4×4, W(3) ∈ R8×8, and so on.
5.1.3	Second order tensor aggregation without the adjacency matrix
In second order tensor aggregation, T is a third order P -tensor, which can be contracted back to
second order in three different ways, by projecting it along each of its dimensions. Therefore the
outputs will be the three matrices
F⑶=Y (wi,ITL + wi,2TJ2 + wi,3TJ3 + bi1m×m)	i ∈{1, 2, 3},
and the weight matrix is W ∈ R3×3 .
5.1.4	Second order tensor aggregation with the adjacency matrix
The first nontrivial tensor contraction case occurs when Fec1 , . . . , Fecs are second order tensors, and
we multiply with AJPt, since in that case T is 5th order, and can be contracted down to second order
in a total of 50 different ways:
1.	The “1+1+1” case contracts T in the form Ti1,i2,i3,i4,i5δia1 δia2 δia3 , i.e., it projects T down
along 3 of its 5 dimensions. This alone can be done in 35 = 10 different ways7
2.	The “1+2” case contracts T in the form Ti1,i2,i3,i4,i5 δia1 δia2,ia3 , i.e., it projects T along one
dimension, and contracts it along two others. This can be done in 3 35 = 30 ways.
3.	The “3” case is a single 3-fold contraction Ti1,i2,i3,i4,i5δia1 ,ia2,ia3 , which again can be done in
(5) =10 different ways.
The tensor Ti1,i2,i3,i4,i5 will be symmetric with respect to two sets of indices, following the structure
of the promotion tensors and the adjacency matrix. Including these symmetries, the number of
contractions is 18 including: five "1+1+1”，ten “1+2”，and three “3”.
7For simplicity, We ignore the fact that symmetries, such as the symmetry of A(Pt, might reduce the number
of distinct projections somewhat.
12
Workshop track - ICLR 2018
Size |Pv| × |Pv| × |Pv|
'---------------y-------------
Tensor to be contracted
Figure 6: The activations of vertices in the receptive field Pv = {w1,w2,w3} of vertex V at level
`-th are stacked into a 3rd order tensor and undergo a tensor product operation with the restricted
adjacency matrix, and then contracted in different ways. In this figure, we only consider single
channel, each channel is represented by a 5th order tensor. In the general case of multi channels, the
resulting tensor would have 6th order, but we contract on each channel separately.
6 Experiments
We compared the second order variant (CCN 2D) of our CCNs framework (Section 4.2) to several
standard graph learning algorithms on three types of datasets that involve learning the properties of
molecules from their structure:
1.	The Harvard Clean Energy Project (Hachmann et al., 2011), consisting of 2.3 million organic
compounds that are candidates for use in solar cells. The regression target in this case is Power
Conversion Efficiency (PCE). Due to time constraints, instead of using the entire dataset, the
experiments were ran on a random subset of 50,000 molecules.
2.	QM9, which is a dataset of all 133k organic molecules with up to nine heavy atoms (C,O,N
and F) out of the GDB-17 universe of molecules. Each molecule has 13 target properties to
predict. The dataset does contain spatial information relating to the atomic configurations, but
we only used the chemical graph and atom node labels. For our experiments we normalized each
target variable to have mean 0 and standard deviation 1. We report both MAE and RMSE for all
normalized learning targets.
3.	Graph kernels datasets, specifically (a) MUTAG, which is a dataset of 188 mutagenic aromatic
and heteroaromatic compounds (Debnat et al., 1991); (b) PTC, which consists of 344 chemical
compounds that have been tested for positive or negative toxicity in lab rats (Toivonen et al.,
2003); (c) NCI1 and NCI109, which have 4110 and 4127 compounds respectively, each screened
for activity against small cell lung cancer and ovarian cancer lines (Wale et al., 2008).
In the case of HCEP, we compared CCN to lasso, ridge regression, random forests, gradient boosted
trees, optimal assignment Wesifeiler-Lehman graph kernel (Kriege et al., 2016) (WL), neural graph
fingerprints (Duvenaud et al., 2015), and the “patchy-SAN” convolutional type algorithm from
(Niepert et al., 2016) (referred to as PSCN). For the first four of these baseline methods, we cre-
ated simple feature vectors from each molecule: the number of bonds of each type (i.e. number of
13
Workshop track - ICLR 2018
H-H bonds, number of C-O bonds, etc) and the number of atoms of each type. Molecular graph
fingerprints uses atom labels of each vertex as base features. For ridge regression and lasso, we
cross validated over λ. For random forests and gradient boosted trees, we used 400 trees, and cross
validated over max depth, minimum samples for a leaf, minimum samples to split a node, and learn-
ing rate (for GBT). For neural graph fingerprints, we used 2 layers and a hidden layer size of 10.
In PSCN, we used a patch size of 10 with two convolutional layers and a dense layer on top as
described in their paper.
For the graph kernels datasets, we compare against graph kernel results as reported in (Kondor &
Pan, 2016) (which computed kernel matrices using the Weisfeiler-Lehman, Weisfeiler-edge, short-
est paths, graphlets and multiscale Laplacian graph kernels and used a C-SVM on top), Neural graph
fingerprints (with 2 levels and a hidden size of 10) and PSCN. For QM9, we compared against the
Weisfeiler-Lehman graph kernel (with C-SVM on top), neural graph fingerprints, and PSCN. The
settings for NGF and PSCN are as described for HCEP.
For our own method, second order CCN, we initialized the base features of each vertex with com-
puted histogram alignment features, inspired by (Kriege et al., 2016), of depth up to 10. Each vertex
receives a base label li = concatj1=0 1Hj(i) where Hj(i) ∈ Rd (with d being the total number of
distinct discrete node labels) is the vector of relative frequencies of each label for the set of vertices
at distance equal to j from vertex i. We use exactly 18 unique contractions defined in 5.1.4 that
result in additional channels. We used up to three levels and the intermediate number of channels in-
creases 18 time at each level. To avoid exponentially growing channels, we applied learnable weight
matrices to compress the channels into a fixed number of channels.
In each experiment we used 80% of the dataset for training, 10% for validation, and evaluated on
the remaining 10% test set. For the kernel datasets we performed the experiments on 10 separate
training/validation/test stratified splits and averaged the resulting classification accuracies. We used
Adam optimization method (Kingma & Ba, 2015). Our initial learning rate was set to 0.001 after ex-
perimenting on a held out set. The learning rate decayed linearly after each step towards a minimum
of10-6.
6.1	GraphFlow Deep Learning Framework
We developed our custom Deep Learning framework in C++/CUDA named GraphFlow that supports
symbolic/automatic differentiation, dynamic computation graphs, specialized tensor operations, and
computational acceleration with GPU. Our method, Covariant Compositional Networks, and other
graph neural networks such as Neural Graph Fingerprints (Duvenaud et al., 2015), PSCN (Niepert
et al., 2016) and Gated Graph Neural Networks (Li et al., 2016) are implemented based on the Graph-
Flow framework. Our source code can be found at https://github.com/HyTruongSon/
GraphFlow.
One challenge of the implementation of Covariant Compositional Networks is that the high-order
tensors (for example, in figure 6, we have a 5th order tensor after the tensor product operation)
cannot be stored explicitly in the memory. Our solution is to propose a virtual indexing system in
such a way that we never compute the whole sparse high-order tensor at once, but only compute its
elements when given the indices. Basically, we always work with a virtual tensor, and that allows
us to implement our tensor reduction/contraction operations efficiently with GPU.
6.2	Discussion
On the subsampled HCEP dataset, CCN outperforms all other methods by a very large margin. For
the graph kernels datasets, SVM with the Weisfeiler-Lehman kernels achieve the highest accuracy
on NCI1 and NCI109, while CCN wins on MUTAG and PTC. Perhaps this poor performance is
to be expected, since the datasets are small and neural network approaches usually require tens of
thousands of training examples at minimum to be effective. Indeed, neural graph fingerprints and
PSCN also perform poorly compared to the Weisfeiler-Lehman kernels.
In the QM9 experiments, CCN beats the three other algorithms in both mean absolute error and
root mean squared error. It should be noted that (Gilmer et al., 2017) obtained stronger results on
QM9, but we cannot properly compare our results with theirs because our experiments only use
14
Workshop track - ICLR 2018
Table 1: HCEP regression results
	Test MAE	Test RMSE
Lasso	0.867	1.437
Ridge regression	-0.854-	1.376
Random forest	-1.004-	1.799
Gradient boosted trees	-0704-	1.005
WL graph kernel	-0.805-	1.096
Neural graph fingerprints	-0.851-	∏77
PSCN (k =10)	-07T8-	-0.973
CCN 2D	—	0.340	0.449
Table 2: Kernel Datasets Classification results (accuracy +/- standard deviation)
	MUlAG	PTC	NCI1	NCI109
WL	84.50 ± 2.16zz	59.97 ± 1.6O=	84.76 ± 0.32=	85.12 ± 0.29~7
WL-edge	82.94 ± 2.33	60.18 ± 2.19	84.65 ± 0.25	85.32 ± 0.34
SP	85.50 ± 2.50	59.53 ± 1.71	73.61 ± 0.36	73.23 ± 0.26
Graphlet	82.44 ± 1.29	55.88 ± 0.31	62.40 ± 0.27	62.35 ± 0.28
P-RW	80.33 ± 1.35	59.85 ± 0.95	TIMED OUT	TIMEDOUT
MLG	87.94 ± 1.61	63.26 ± 1.48	81.75 ± 0.24	81.31 ± 0.22
PSCN k = 10 (NiePert et 市	88.95 ± 4.37	62.29 ± 5.68	76.34 ± 1.68	N/A
Neural graph fingerprints	89.00 ± 7.00	57.85 ± 3.36	62.21 ± 4.72	56.11 ± 4.31
CCN 2D	91.64 ± 7.2丁	70.62 ± 7.04一	76.27 ± 4.13-	75.54 ± 3.36-
the adjacency matrices and atom labels of each node, while theirs includes comprehensive chemical
features that better inform the target quantum properties.
7 Conclusions
We have presented a general framework called covariant compositional networks (CCNs) for con-
structing covariant graph neural networks, which encompasses other message passing approaches
as special cases, but takes a more general and principled approach to ensuring covariance with re-
spect to permutations. Experimental results on several benchmark datasets show that CCNs can
outperform other state-of-the-art algorithms.
Table 3: QM9 regression results (MAE)
	WLGK	NGF	PSCN (k =10)	CCN 2D
alpha	0.46	0.43	0.20 =	0.16
'^Cv	0.59	0.47	0.27	-0:23-
G	0.51	0.46	0.33	-0:29-
gap	0.72	0.67	0.60	-0:54-
H	0.52	0.47	0.34	-030-
HOMO	0.64	0.58	0.51	-0:39-
LUMO	0.70	0.65	0.59	-0:53-
mu	0.69	0.63	0.54	-0:48-
omega1	0.72	0.63	0.57	-0:45-
-R2	0.55	0.49	0.22	-0:19-
-U	0.52	0.47	0.34	-029-
-U0	0.52	0.47	0.34	-029-
ZPVE	0.57	0.51	0.43 —	0.39
15
Workshop track - ICLR 2018
Table 4: QM9 regression results (RMSE)
	WLGK	NGF	PSCN (k =10)	CCN 2D
alpha	0.68	0.65	0.31 =	0.26
'^Cv	0.78	0.65	034	-0:30-
G	0.67	0.62	0.43	-0:38-
gap	0.86	0.82	075	-0:69-
H	0.68	0.62	0.44	-0:40-
HOMO	0.91	0.81	070	-0:55-
LUMO	0.84	0.79	073	-0:68-
mu	0.92	0.87	075	-0:67-
omega1	0.84	0.77	073	-0:65-
-R2	0.81	0.71	031	-027-
-U	0.67	0.62	0.44	-0:40-
-UG	0.67	0.62	0.44	-039-
ZPVE	0.72	0.66	0.55 —	0.51
References
P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, and K. Kavukcuoglu. Interaction networks for
learning about objects, relations and physics. 29:4502-4510, 2016.
K. M. BorgWardt and H. P. Kriegel. ShorteSt-Path kernels on graphs. 5:74-81, 2005.
J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks
on graphs. 3, 2014.
J. Y. Cai, M. Furer, and N. Immerman. An optimal loWer bound on the number of variables for
graph identification. Combinatorica, 12:389-410, 12 1992.
T. Cohen and M. Welling. Group equivariant convolutional netWorks. 33:2990-2999, 2016.
T. Cohen and M. Welling. Steerable cnns. Proc. ICLR, 5, 2017.
A. K. Debnat, R. L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch. Structure-
activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation With
molecular orbital energies and hydrophobicity. J. Med. Chem., 34:786-97, 1991.
M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural netWorks on graphs With
fast localized spectral filtering. 29, 2016.
D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P.
Adams. Convolutional netWorks on graphs for learning molecular fingerprints. 28:2224-2232,
2015.
P. F. FelzenszWalb and D. P. Huttenlocher. Pictorial structures for object recognition. Int. J. Comput.
Vis., 61:55-71, 2005.
P. F. FelzenszWalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection With dis-
criminatively trained part-based models. IEEE Trans. Pattern Anal. Mach. Intell., 32:541-551,
2010.
A. Feragen, N. Kasenburg, J. Peterson, M. de Bruijne, and K. M. BorgWardt. Scalable kernels for
graphs With continuous attributes. 26, 2013.
M. Fischler and R. Elschlager. The representation and matching of pictorial structures. IEEE Trans.
Comput., C-22:67-92, 1973.
W. T. Freeman and E. H. Adelson. The design and use of steerable filters. IEEE Trans. Pattern Anal.
Mach. Intell., 13:891-906, 09 1991.
T. Gartner. Exponential and geometric kernels for graphs. Principles of modeling nonvectorial data,
2002.
16
Workshop track - ICLR 2018
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing
for quantum chemistry. In Doina Precup and Yee Whye Teh (eds.), Proc. ICML, volume 70
of Proceedings of Machine Learning Research, pp. 1263-1272, International Convention Cen-
tre, Sydney, Australia, 08 2017. PMLR. URL http://proceedings.mlr.press/v70/
gilmer17a.html.
J. Hachmann, R. Olivares-Amaya, S. Atahan-Evrenk, C. Amador-Bedolla, R. S. Sanchez-Carrera,
A. Gold-Parker, L. Vogt, A. M. Brockway, and A. Aspuru-Guzik. The harvard clean energy
project: Large-scale computational screening and design of organic photovoltaics on the world
community grid. J. Phys. Chem. Lett., 2:2241-2251, 2011.
M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data. arXiv
preprint arXiv:1506.05163, 06 2015.
S.	Kearnes, K. McCloskey, M. Brendl, V. Pande, and P. Riley. Molecular graph convolutions: mov-
ing beyond fingerprints. J. Comput. Aided Mol. Des., 30:595-608, 2016.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. ICLR, San Diego,
2015.
T.	N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. 5,
2017.
R. Kondor and K. M. Borgwardt. The skew spectrum of graphs. 25:496-503, 2008.
R. Kondor and H. Pan. The multiscale laplacian graph kernel. 29:2982-2990, 2016.
N. M. Kriege, P. Giscard, and R. Wilson. On valid optimal assignment kernels and applications to
graph classification. Adv. NIPS, 20:1623-1631, 2016.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional
neural networks. 25:1097-1105, 01 2012.
Y. LeCun, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proc. IEEE, pp. 2278-2324, 1998.
Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. Proc.
ICLR, 4, 2016.
R. Manduchi, P. Perona, and D. Shy. Efficient deformable filter banks. IEEE Trans. on Signal
Process., 46:1168-1173, 04 1998.
M. Neumann, R. Garnett, C. Baukhage, and K. Kersting. Propagation kernels: efficient graph kernels
from propagated information. 102, 02 2016.
M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. 33:
2014-2023, 2016.
Y. Ohta, T. Kanade, and T. Sakai. An analysis system for scenes containing objects with substruc-
tures. 4:752-754, 1978.
P. Perona. Deformable kernels for early vision. IEEE Trans. Pattern Anal. Mach. Intell., 17:488-
499, 05 1995.
R. C. Read and D. G. Corneil. The graph isomorphism disease. J. Graph Theory, 1:339-363, 1977.
B. E. Sagan. The Symmetric Group. Grad. Texts in Math. Springer, 2001.
F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network
model. IEEE Trans. on Neural Netw., 20:61-80, 2009.
K. T. Schutt, Kristof T., F. Arbabzadah, S. Chmiela, K. R. Muller, and A. Tkatchenko. QUantUm-
chemical insights from deep tensor neural networks. Nat. Commun., 8:13890, 01 2017.
17
Workshop track - ICLR 2018
N. Shervashidze, S. V. N. Vishwanathan, T. Petri, K. M., and K. M. Borgwardt. Efficient graphlet
kernels for large graph comparison. 12:488-495, 2009.
N. Shervashidze, P. Schweitzer, E. J. van Leeuwan, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-
lehman graph kernels. J. Mach. Learn. Res., 12:2539-2561, 2011.
E. P. Simoncelli, W. T. Freeman, E. H. Adelson, and D. J. Heeger. Shiftable multiscale transforms.
IEEE Trans. Inf. Theory., 38:587-607, 03 1992.
P. C. Teo and Y. Hel-Or. Lie generators for computing steerable functions. Pattern Recognit. Lett.,
16:7-17, 10 1998.
H. Toivonen, A. Srinivasan, R. D. King, S. Kramer, and C. Helma. Statistical evaluation of the
predictive toxicology challenge. Bioinformatics, 19:1183-1193, 2003.
Z. W. Tu, X. R. Chen, A. L. Yuille, and S. C. Zhu. Image parsing: Unifying segmentation, detection,
and recognition. Int. J. Comput. Vis., 63:113-140, 2005.
S. V. N. Vishwanathan, N. N. Schraudolf, R. Kondor, and K. M. Bogwardt. Graph kernels. J. Mach.
Learn. Res., 11:1201-1242, 2010.
N. Wale, I. A. Watson, and G. Karypis. Comparison of descriptor spaces for chemical compound
retrieval and classification. Knowl. Info. Sys., 14:347-375, 2008.
B. Weisfeiler and A. A. Lehman. A reduction of a graph to a canonical form and an algebra arising
during this reduction. Nauchno-Technicheskaya Informatsia, 9, 1968.
D. E. Worrall, S. Garbin, D. Turmukhambetov, and G. J. Brostow. Harmonic networks: Deep
translation and rotation equivariance. Proc. IEEE CVPR, 07 2017.
S. Zhu and D. Mumford. A stochastic grammar of images. Found. Trends Comput. Graphics Vis.,
2:259-362, 2006.
A Mathematical Background
Groups. A group is a set G endowed with an operation G × G → G (usually denoted multiplica-
tively) obeying the following axioms:
G1. for any u, v ∈ G, uv ∈ G (closure);
G2. for any u, v, w ∈ G, u(vw) = (uv)w (associativity);
G3. there is a unique e ∈ G, called the identity of G, such that eu = ue = u for any u ∈ G;
G4. for any u ∈ G, there is a corresponding element u-1 ∈ G called the inverse of u, such that
uu-1 = u-1u = e.
We do not require that the group operation be commutative, i.e., in general, uv 6= vu. Groups
can be finite or infinite, countable or uncountable, compact or non-compact. While most of the
results in this paper would generalize to any compact group, the keep the exposition as simple as
possible, throughout we assume that G is finite or countably infinite. As usual, |G| will denote the
size (cardinality) of G, sometimes also called the order of the group.
Representations. A (finite dimensional) representation of a group G over a field F is a matrix-
valued function R: G → Fdρ×dρ such that R(x) R(y) = R(xy) for any x, y ∈ G. We generally
assume that F = C, however in the special case when G is the symmetric group Sn we can restrict
ourselves to only considering real-valued representations, i.e., F = R.
B Proofs
Proof of Proposition 1. Let G and G0 be two compound objects, where G0 is equivalent to G up to a
permutation σ ∈ Sn of the atoms. For any node na of G we let n0a be the corresponding node of G0 ,
and let fa and fa0 be their activations.
We prove that fa = fa0 for every node in G by using induction on the distance of na from its farthest
descendant that is a leaf, which we call its height and denote h(a). For h(a) = 0, the statment is
18
Workshop track - ICLR 2018
clearly true, since fa = f[ = 'ξ(a). Now assume that it is true for all nodes with height UP to h*. For
any node n° with h(a) = h + 1, fa = Φ(fcι ,fc2,..., fck), where each of the children cι,...,ck
are of height at most h*, therefore
fa=Φ(fc1,fc2,...,fck)=Φ(fc01,fc02,...,fc0k)=fa0.
Thus, fa = fa0 for every node in G. The ProPosition follows by φ(G) = fr = fr0 = φ(G0).
Proof of Proposition 3. Let G, G0 , N and N 0 be as in Definition 5. As in Definition 6, for each
node (neuron) ni in N there is a node n0j in N 0 such that their recePtive fields are equivalent uP to
Permutation. That is, if |Pi| = m, then |Pj0 | = m, and there is a Permutation π ∈ Sm, such that if
Pi = (ep1, . . . , epm) and Pj0 = (eq1, . . . ,eqm), then eqπ(a) = epa. By covariance, then fj0 =Rπ(fi).
Now let G00 be a third equivalent object, and N 00 the corresPonding comP-net. N 00 must also
have a node, n0k0, that corresPonds to ni and n0j . In Particular, letting its recePtive field be
Pk00 = (er1 , . . . , erm), there is a Permutation σ ∈ Sm for which erσ(b) = eqb. Therefore, fk00 = Rσ (fj0).
At the same time, n0k0 is also in corresPondence with ni . In Particular, letting τ = σπ (which corre-
sPonds to first aPPlying the Permutation π, then aPPlying σ), erτ(a) = epa, and therefore fk00 = Rτ (fi).
Hence, the {Rπ } maPs must satisfy
Rσπ(fi) =Rσ(fj0) =Rσ(Rπ(fi)),
for any 力.More succinctly, Rσ∏ = Rσ ◦ Rn for any ∏,σ ∈ Sm. In the case that the {R∏} maps are
linear and rePresented by matrices, this reduces to Rσπ = RσRπ , which is equivalent to saying that
they form a group representation of Sm .
Proof of Proposition 4. Under the action of a permutation π ∈ Sm , A and B transform as
A7→A0	[A0]j1,...,jk	=	[Pπ]j1j10	[Pπ]j2j20	...	[Pπ]jkjk0	[A]j10,...,jk0,	(3)
B7→B0	[B0]j1,...,jp	=	[Pπ]j1j10	[Pπ]j2j20	...	[Pπ]jpjp0	[B]j10,...,jp0.	(4)
Case 1.	Let C = A 0 B. Under (3) and (4), C transforms into
[C0]i1,...,ik+p = [Pπ]i1i01 ...[Pπ]ii0kk [A]i01,...,i0k	[Pπ]ik+i10k+1 ... [Pπ]ik+ip0k+p [B]i0k+1,...,i0k+p
= [Pπ]i1 1 . . . [Pπ]ik+pk+p Ci01...,i0k+p ,
therefore, C is a k + p,th order P-tensor.
Case 2.	Let C = A (a1,...,ap)B. Under (3) and (4), C transforms as
[C 0]i1,...,ik = [Pπ]i1i1 . . . [Pπ]ikik [A]i01,...,i0k	[Pπ]ia1a1 . . . [Pπ]iapap [B]i0a1 ,...,i0ap	=
0	00	0
= [Pn ]iιi1 ... [Pn ]ikik ∙ [Pn KaI …[Pn ]iapap ∙ [C]i1 ...,ik .
i0
Note that each of the [Pn]ia aj factors in this expression repeats one of the earlier appearing
[Pn]i1i01 , . . . , [Pn]iki0k factors, but since Pn only has zero and one entries [Pn]2a,b = [Pn]a,b, so
these factors can be dropped. Thus, C is a k’th order P -tensor.
Case 3.	Let C = A^a1,...,ap and bi,..., bk-p be the indices (in increasing order) that are not
amongst {a1, . . . , ap}. Under (3), C becomes
[C0]ib1,...,ibk-p
X...X[Pn]i1i01 ... [Pn]iki0k[A]i01,...,i0k
ia1	iap
[Pn]ib1 b1 . . . [Pn]ibk-
-p ...	[A]i01,...,i0k
i0	i0
ia1	iap
p
Thus, C is a k - p’th order P -tensor.
19
Workshop track - ICLR 2018
Case 4.	Follows directly from 3.
Case 5.	Finally, if Aι,...,Au are k'th order P-tensors and C = Pj ajAj then
X00	00
αj [Pπ]i1 1 . . .	[Pπ]ik k	[Aj]i01,...,i0k	=	[Pπ]i1 1	. . .	[Pπ]ik k	αk	[Aj]i01,...,i0k,
jj
so C is a k’th order P -tensor.
Proof of Proposition 5. Under the action of a permutation π ∈ Sm0 on Pb, χ (dropping the a→b
superscipt) transforms to χ0, where χ0i,j = χπ-1(i),j. However, this can also be written as
χ0i,j = [Pπ χ]i,j =	[Pπ]i,i0 χi0,j.
i0
Therefore, Fi1 ,...,ik transforms to
Fei01,...,ik = χ0i1j1 χ0i2j2 . . . χ0ikjk Fj1,...,jk = [Pπ]i1i01 . . . [Pπ]iki0k χi01j1 χi02j2 . . . χi0kjk Fj1,...,jk ,
T-l ∙	7~»
so F is a P -tensor.
Proof of proposition 6. By Proposition 5, under the action of any permutation ∏, each of the Fpj
slices of F transforms as
00
[Fpj]i1,...,ik = [Pπ]i1 1 . . . [Pπ]ik k [Fpj]i1,...,ik .
At the same time, π also permutes the slices amongst each other according to
-^0	τ^0
Fi1,...,ik,j = [Fpπ-1(j)]i1,...,ik = F i1,...,ik,π-1(j) .
Therefore	_	_
FiI,...,ik ,j = [P∏ ]iιi1 …[P∏ ]ikik [P∏ ]j0 F il,...,ik,j ,
so F is a k + 1 'th order P-tensor.	■
Proof of Proposition 7. Under any permutation ∏ ∈ Sm of P AJPo transforms to AJPo, where
[A(po]∏(a),∏(b) = [AJpi]a,b. Therefore, AJPi is a second order P-tensor. By the first case of
Proposition 4, F 0 AJPaiS then a k + 2'th order P-tensor.	■
20