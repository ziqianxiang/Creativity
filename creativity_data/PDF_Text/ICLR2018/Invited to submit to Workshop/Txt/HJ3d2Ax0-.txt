Workshop track - ICLR 2018
B enefits of Depth for Long-Term Memory
of Recurrent Networks
Yoav Levine, Or Sharir & Amnon Shashua
The Hebrew University of Jerusalem
{yoavlevine,or.sharir,shashua}@cs.huji.ac.il
Ab stract
The key attribute that drives the unprecedented success of modern Recurrent Neu-
ral Networks (RNNs) on learning tasks which involve sequential data, is their
ever-improving ability to model intricate long-term temporal dependencies. How-
ever, a well established measure of RNNs’ long-term memory capacity is lacking,
and thus formal understanding of their ability to correlate data throughout time is
limited. Though depth efficiency in convolutional networks is well established by
now, it does not suffice in order to account for the success of deep RNNs on inputs
of varying lengths, and the need to address their ‘time-series expressive power’
arises. In this paper, we analyze the effect of depth on the ability of recurrent
networks to express correlations ranging over long time-scales. To meet the above
need, we introduce a measure of the information flow across time that can be sup-
ported by the network, referred to as the Start-End separation rank. Essentially,
this measure reflects the distance of the function realized by the recurrent network
from a function that models no interaction whatsoever between the beginning and
end of the input sequence. We prove that deep recurrent networks support Start-
End separation ranks which are exponentially higher than those supported by their
shallow counterparts. Moreover, we show that the ability of deep recurrent net-
works to correlate different parts of the input sequence increases exponentially
as the input sequence extends, while that of vanilla shallow recurrent networks
does not adapt to the sequence length at all. Thus, we establish that depth brings
forth an overwhelming advantage in the ability of recurrent networks to model
long-term dependencies, and provide an exemplar of quantifying this key attribute
which may be readily extended to other RNN architectures of interest, e.g. vari-
ants of LSTM networks. We obtain our results by considering a class of recurrent
networks referred to as Recurrent Arithmetic Circuits (RACs), which merge the
hidden state with the input via the Multiplicative Integration operation.
1	Introduction
Over the past few years, Recurrent Neural Networks (RNNs) have become the prominent machine
learning architecture for modeling sequential data, having been successfully employed for language
modeling (Sutskever et al., 2011; Graves, 2013), neural machine translation (Bahdanau et al., 2014),
speech recognition (Graves et al., 2013; Amodei et al., 2016), and more. The success of recurrent
networks in learning complex functional dependencies for sequences of varying lengths, readily
implies that long-term and elaborate correlations in the given inputs are somehow supported by
these networks. However, formal understanding of the influence of a recurrent network’s structure
on its expressiveness, and specifically on its ever-improving ability to integrate data throughout time
(e.g. translating long sentences, answering elaborate questions), is lacking.
An ongoing empirical effort to successfully apply recurrent networks to tasks of increasing com-
plexity and temporal extent, includes augmentations of the recurrent unit such as Long Short Term
Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) and their variants (e.g. Cho et al.
(2014)). A parallel avenue, which we focus on in this paper, includes the stacking of layers to form
deep recurrent networks (Schmidhuber, 1992). Deep recurrent networks, which exhibit empirical
superiority over shallow ones (see e.g. Graves et al. (2013)), implement hierarchical processing of
information at every time-step that accompanies their inherent time-advancing computation. Evi-
dence for a time-scale related effect arises from experiments (Hermans and Schrauwen, 2013) - deep
1
Workshop track - ICLR 2018
recurrent networks appear to model correlations which correspond to longer time-scales than shallow
ones. These findings, which imply that depth brings forth a considerable advantage in complexity
and in temporal capacity of recurrent networks, have no adequate theoretical explanation.
In this paper, we address the above presented issues. Based on the relative maturity of depth effi-
ciency results in neural networks, namely results that show that deep networks efficiently express
functions that would require shallow ones to have a super-polynomial size (e.g. Cohen et al. (2016);
Eldan and Shamir (2016)), itis natural to assume that depth has a similar effect on the expressiveness
of recurrent networks. Indeed, we show that depth efficiency holds for recurrent networks.
However, the distinguishing attribute of recurrent networks, is their inherent ability to cope with
varying input sequence length. Thus, once establishing the above depth efficiency in recurrent net-
works, a basic question arises, which relates to the apparent depth enhanced long-term memory in
recurrent networks: Do the functions which are efficiently expressed by deep recurrent networks cor-
respond to dependencies over longer time-scales? We answer this question, by showing that depth
provides an exponential boost to the ability of recurrent networks to model long-term dependencies.
In order to take-on the above question, we introduce in section 2 a recurrent network referred to
as a recurrent arithmetic circuit (RAC) that shares the architectural features of RNNs, and differs
from them in the type of non-linearity used in the calculation. This type of connection between
state-of-the-art machine learning algorithms and arithmetic circuits (also known as Sum-Product
Networks (Poon and Domingos, 2011)) has well-established precedence in the context of neural
networks. Delalleau and Bengio (2011) prove a depth efficiency result on such networks, and Co-
hen et al. (2016) theoretically analyze the class of Convolutional Arithmetic Circuits which differ
from common ConvNets in the exact same fashion in which RACs differ from more standard RNNs.
Conclusions drawn from such analyses were empirically shown to extend to common ConvNets
(e.g. Sharir and Shashua (2017); Levine et al. (2017)). Beyond their connection to theoretical mod-
els, the modification which defines RACs resembles that of Multiplicative RNNs (Sutskever et al.,
2011) and of Multiplicative Integration networks (Wu et al., 2016), which provide a substantial per-
formance boost over many of the existing RNN models. In order to obtain our results, we make
a connection between RACs and the Tensor Train (TT) decomposition (Oseledets, 2011), which
suggests that Multiplicative RNNs may be related to a generalized TT-decomposition, similar to the
way Cohen and Shashua (2016) connected ReLU ConvNets to generalized tensor decompositions.
We move on to introduce in section 3 the notion of Start-End separation rank as a measure of the
recurrent network’s ability to model elaborate long-term dependencies. In order to analyze the long-
term correlations of a function over a sequential input which extends T time-steps, we partition the
inputs to those which arrive at the first T/2 time-steps (“Start”) and the last T/2 time-steps (“End”),
and ask how far the function realized by the recurrent network is from being separable w.r.t. this
partition. Distance from separability is measured through the notion of separation rank (Beylkin
and Mohlenkamp, 2002), which can be viewed as a surrogate of the L2 distance from the closest
separable function. For a given function, high Start-End separation rank implies that the function
induces strong correlation between the beginning and end of the input sequence, and vice versa.
In section 4 we directly address the depth enhanced long-term memory question above, by examin-
ing depth L = 2 RACs and proving that functions realized by these deep networks enjoy Start-End
separation ranks that are exponentially higher than those of shallow networks, implying that indeed
these functions can model more elaborate input dependencies over longer periods of time. An ad-
ditional reinforcing result is that the Start-End separation rank of the deep recurrent network grows
exponentially with the sequence length, while that of the shallow recurrent network is independent of
the sequence length. Informally, this implies that vanilla shallow recurrent networks are inadequate
in modeling correlations of long input sequences, since in contrast to the case of deep recurrent net-
works, the modeled dependencies achievable by shallow ones do not adapt to the actual length of the
input. Finally, we present and motivate a quantitative conjecture by which the Start-End separation
rank of recurrent networks grows exponentially with the network depth. A proof of this conjecture,
which will provide an even deeper insight regarding the advantages of depth in recurrent networks,
is left as an open problem.
2	Recurrent Arithmetic Circuits
In this section, we introduce a class of recurrent networks referred to as Recurrent Arithmetic Cir-
cuits (RACs), which shares the architectural features of standard RNNs. As demonstrated below,
2
Workshop track - ICLR 2018
Figure 1: Shallow and deep recurrent networks, as described by eqs. 1 and 4, respectively.
the operation of RACs on sequential data is identical to the operation of RNNs, where a hidden state
mixes information from previous time-steps with new incoming data (see fig. 1). The two classes
differ only in the type of non-linearity used in the calculation, as described by eqs. 1-3. In the
following sections, we utilize the algebraic properties of RACs for proving results regarding their
ability to model long-term dependencies of their inputs.
We present below the basic framework of shallow recurrent networks (fig. 1(a)), which describes
both the common RNNs and the newly introduced RACs. A recurrent network is a network that
models a discrete-time dynamical system; we focus on an example of a sequence to sequence clas-
sification task into one of the categories {1, ..., C} ≡ [C]. Denoting the temporal dependence by
t, the sequential input to the network is {xt ∈ X}tT=1, and the output is a sequence of class scores
vectors {yt,L,Θ ∈ RC}tT=1, where L is the network depth, Θ denotes the parameters of the recurrent
network, and T represents the extent of the sequence in time-steps. We assume the input lies in some
input space X that may be discrete (e.g. text data) or continuous (e.g. audio data), and that some
initial mapping f : X → RM is preformed on the input, so that all input types are mapped to vectors
f(xt) ∈ RM. The function f (∙) may be viewed as an encoding, e.g. words to vectors or images to a
final dense layer via some trained ConvNet. The output at time t ∈ [T] of the shallow (depth L = 1)
recurrent network with R hidden channels, depicted in fig. 1(a), is given by:
ht = g (WHhtT,WIf(Xt))
yt,1,Θ = W Oht,
(1)
where ht ∈ RR is the hidden state of the network at time t (h0 is some initial hidden state), Θ
denotes the learned parameters WI ∈ RR×M , WH ∈ RR×R, WO ∈ RC ×R, which are the input,
hidden and output weights matrices respectively, and g is some non-linear operation. A bias term is
usually added to eq. 1, however, because it bears no effect on our analysis, we omit it for simplicity.
For common RNNs, the non-linearity is given by:
gRNN(a, b) = σ(a + b),	(2)
where σ(∙) is typically some point-wise non-linearity such as sigmoid, tanh etc. For the newly
introduced class of RACs, g is given by:
gRAC(a,b)=ab,	(3)
where the operation stands for element-wise multiplication between vectors, for which the re-
sultant vector upholds (a Θ b)i = a% ∙ bi. This form of merging the input and the hidden state by
multiplication rather than addition is referred to as Multiplicative Integration (Wu et al., 2016).
The extension to deep recurrent networks is natural, and we follow the common approach (see e.g.
Hermans and Schrauwen (2013)) where each layer acts as a recurrent network which receives the
hidden state of the previous layer as its input. The output at time t of the depth L recurrent network
3
Workshop track - ICLR 2018
with R hidden channels in each layer,1 * depicted in fig. 1(b), is constructed by the following:
ht,l = g (WH,lht-1,l,WI,lht,lτ)
ht,0 ≡ f(xt)	(4)
yt,L,Θ = WOht,L,
where ht,l ∈ RR is the state of the depth l hidden unit at time t (h0,l is some initial hidden state per
layer), and Θ denotes the learned parameters. Specifically, WI,l ∈ RR×R (l > 1), WH,l ∈ RR×R
are the input and hidden weights matrices at depth l, respectively. For l = 1, the weights matrix
which multiplies the inputs vector has the appropriate dimensions: WI,1 ∈ RR×M. The output
weights matrix is WO ∈ RC ×R as in the shallow case, representing a final calculation of the scores
for all classes 1 through C at every time-step. The non-linear operation g determines the type of the
deep recurrent network, where a common deep RNN is obtained by choosing g = gRNN (eq. 2), and
a deep RAC is obtained for g = gRAC (eq. 3).
We consider the newly presented class of RACs to be a good surrogate of common RNNs. Firstly,
there is an obvious structural resemblance between the two classes, as the recurrent aspect of the
calculation has the exact same form in both networks (fig. 1). In fact, recurrent networks that include
Multiplicative Integration similarly to RACs, have been shown to outperform many of the existing
RNN models (Sutskever et al., 2011; Wu et al., 2016). Secondly, as mentioned above, arithmetic
circuits have been successfully used as surrogates of convolutional networks. The fact that Cohen
and Shashua (2016) laid the foundation for extending the proof methodologies of convolutional
arithmetic circuits to common ConvNets with ReLU activations, suggests that such adaptations may
be made in the recurrent network analog, rendering the newly proposed class of recurrent networks
all the more interesting. In the following sections, we make use of the algebraic properties of RACs
in order to obtain clear-cut observations regarding the benefits of depth in recurrent networks.
3	Temporal Correlations Modeled by Recurrent Networks
In this section, we establish means for quantifying the ability of recurrent networks to model long-
term temporal dependencies in the sequential input data. We begin by introducing the Start-End
separation-rank of the function realized by a recurrent network as a measure of the amount of infor-
mation flow across time that can be supported by the network. We then tie the Start-End separation
rank to the algebraic concept of grid tensors (Hackbusch, 2012), which will allow us to employ tools
and results from tensorial analysis in order to show that depth provides an exponential boost to the
ability of recurrent networks to model elaborate long-term temporal dependencies.
3.1	The “Start-End” Separation Rank
We define below the concept of the Start-End separation rank for functions realized by recurrent
networks after T time-steps, i.e. real functions that take as input X = (x1, . . . , xT) ∈ XT . The
separation rank quantifies a function’s distance from separability with respect to two disjoint subsets
of its inputs. Specifically, let (S, E) be a partition of input indices, such that S = {1, . . . , T/2}
and E = {T/2 + 1, . . . , T} (we consider even values of T throughout the paper for convenience
of presentation). This implies that {xs }s∈S are the first T/2 (“Start”) inputs to the network, and
{xe}e∈E are the last T/2 (“End”) inputs to the network. For a function y : XT → R, the Start-End
separation rank is defined as follows:
sep(S,E) (y) ≡ min nK ∈ N ∪ {0} : ∃g1s. . .gKs : XT/2 → R, g1e. . .gKe : XT/2 → R s.t. (5)
y(x1, ..., XT) = XK=1 gs (x1,…，xT/2)ge (xT/2+1,…，XT)}.
In words, it is the minimal number of summands that together give y, where each summand is
separable w.r.t. (S, E), i.e. is equal to a product of two functions - one that intakes only inputs from
the first T /2 time-steps, and another that intakes only inputs from the last T /2 time-steps.
The separation rank w.r.t. a general partition of the inputs was introduced in Beylkin and
Mohlenkamp (2002) for high-dimensional numerical analysis, and was employed for various ap-
plications, e.g. chemistry (Harrison et al., 2003), particle engineering (Hackbusch, 2006), and ma-
chine learning (Beylkin et al., 2009). Cohen and Shashua (2017) connect the separation rank to the
1We assume for simplicity that the number of hidden channels is the same for all layers. See Levine et al.
(2017) for treatment of the channel numbers’ effect on the expressivity of convolutional networks .
4
Workshop track - ICLR 2018
L2 distance of the function from the set of separable functions, and use it to measure correlations
modeled by deep convolutional networks. Levine et al. (2017) tie the separation rank to the family
of quantum entanglement measures, which quantify correlations in many-body quantum systems.
In our context, if the Start-End separation rank of a function realized by a recurrent network is equal
to 1, then the function is separable, meaning it cannot model any interaction between the inputs
which arrive at the beginning of the sequence and the inputs that follow later, towards the end of the
sequence. Specifically, if sep(S,E) (y) = 1 then there exist gs : XT/2 → R and ge : XT/2 → R
such that y(x1, . . . , xT) = gs (x1, . . . , xT/2)ge(xT/2+1 , . . . , xT), and the function y cannot take
into account consistency between the values of {x1, . . . , xT/2} and those of {xT/2+1, . . . , xT}. In
a statistical setting, if y were a probability density function, this would imply that {x1, . . . , xT/2}
and {xT/2+1, . . . , xT} are statistically independent. The higher sep(S,E) (y) is, the farther y is from
this situation, i.e. the more it models dependency between the beginning and the end of the inputs
sequence. Stated differently, if the recurrent network’s architecture restricts the hypothesis space
to functions with low Start-End separation ranks, a more elaborate long-term temporal dependence,
which corresponds to a function with a higher Start-End separation rank, cannot be learned.
In section 4 we show that deep RACs support Start-End separations ranks which are exponentially
larger than those supported by shallow RACs, and are therefore much better fit to model long-term
temporal dependencies. To this end, we employ in the following sub-section the algebraic tool of
grid tensors that will allow us to evaluate the Start-End separation ranks of deep and shallow RACs.
3.2	B ounding the Start-End Separation Rank via Grid Tensors
We begin by laying out basic concepts in tensor theory required for the upcoming analysis. The core
concept of a tensor may be thought of as a multi-dimensional array. The order of a tensor is defined
to be the number of indexing entries in the array, referred to as modes. The dimension of a tensor in
a particular mode is defined as the number of values taken by the index in that mode. If A is a tensor
of order T and dimension Mi in each mode i ∈ [T], its entries are denoted Ad1...dT, where the
index in each mode takes values di ∈ [Mi]. A fundamental operator in tensor analysis is the tensor
product, which we denote by 0. Itis an operator that intakes two tensors A ∈ RM1× ×Mp and B ∈
RMP +1×…×MP+Q, and returns a tensor A0 B ∈ RM1×…×MP+Q defined by： (A0 B)dι...dp+q =
Adι...dp ∙BdP+1 …dP+q . An additional concept we will make use of is the matriCization of A w.r.t. the
partition (S, E), denoted JAKS,E, which is essentially the arrangement of the tensor elements as a
matrix whose rows correspond to S and columns to E (formally presented in appendix C).
We consider the function realized by a shallow RAC with R hidden channels, which computes the
score of class c ∈ [C] at time T . This function, which is given by a recursive definition in eqs. 1
and 3, can be alternatively written in the following closed form:
yT,1,θ (χ1,..∙, XT)= XdM..,τ=1 (AT,1,θ)dι,.,dτ YT=ι fdi (χi),	⑹
where the order T tensor AcT,1,Θ, which lies at the heart of the above expression, is referred to as the
shallow RAC weights tensor, since its entries are polynomials in the network weights Θ. Specifically,
denoting the rows of the input weights matrix, WI, by aI,α ∈ RM (or element-wise: aIj,α = WαI,j),
the rows of the hidden weights matrix, WH, by aH,β ∈ RR (or element-wise: ajH,β = WβH,j), and the
rows of the output weights matrix, WO, by aO,c ∈ RR, c ∈ [C] (or element-wise: ajO,c = WcO,j),
the shallow RAC weights tensor can be gradually constructed in the following fashion:
R
φ2,β = XR	aαH,β aI,α
|{z}	α=1 α
order 2 tensor
0 aI,α
R
φt,β = XR	aαH,βφt-1,α 0 aI,α
|{z}	α=1
order t tensor
• ∙ ∙
R
AcT,1,Θ = XR	aαO,cφT-1,α 0aI,α,	(7)
X{Z}	^^a=I
order T tensor
having set h0 = (WH) * 1, where f is the pseudoinverse operation. In the above equation, the tensor
products, which appear inside the sums, are directly related to the Multiplicative Integration property
5
Workshop track - ICLR 2018
of RACs (eq. 3). The sums originate in the multiplication of the hidden states vector by the hidden
weights matrix at every time-step (eq. 1). The construction of the shallow RAC weights tensor,
presented in eq. 7, is referred to as a Tensor Train (TT) decomposition of TT-rank R in the tensor
analysis community and is analogously described by a Matrix Product State (MPS) Tensor Network
(See Orus (2014)) in the quantum physics community. See appendix A for the Tensor Networks
construction of deep and shallow RACs, which provides graphical insight regarding the exponential
complexity brought forth by depth in recurrent networks.
We now present the concept of grid tensors, which are a form of function discretization. Essentially,
the function is evaluated for a set of points on an exponentially large grid in the input space and
the outcomes are stored in a tensor. Formally, fixing a set of template vectors x(1) , . . . , x(M) ∈ X,
the points on the grid are the set {(x(d1) , . . . , x(dT))}dM,...,d =1. Given a function y(x1, . . . , xT),
the set of its values on the grid arranged in the form of a tensor are called the grid tensor induced
by y, denoted A(y)d1,...,dT ≡ y(x(d1), . . . , x(dT)). The grid tensors of functions realized by recur-
rent networks, will allow us to calculate their separations ranks and establish definitive conclusions
regarding the benefits of depth these networks. Having presented the tensorial structure of the func-
tion realized by a shallow RAC, as given by eqs. 6 and 7 above, we are now in a position to tie its
Start-End separation rank to its grid tensor, as formulated in the following claim:
Claim 1. Let ycT,1,Θ be a function realized by a shallow RAC (fig. 1(a)) after T time-steps, and
let AcT,1,Θ be its shallow RAC weights tensor, constructed according to eq. 7. Assume that the
network’s initial mapping functions {fd}dM=1 are linearly independent, and that they, as well as the
functions gν, gν0 in the definition of Start-End separation rank (eq. 5), are measurable and square-
integrable.2 Then, there exist template vectors x(1) , . . . , x(M) ∈ X such that the following holds:
seP(s,E)(yT,1,θ) =rank J4yT,1,θ加s,E) =rank JAT,1,θK)S,e),	⑻
where A(ycT,1,Θ) is the grid tensor of ycT,1,Θ with respect to the above template vectors.
Proof. See appendix B.1.	□
The above claim establishes an equality between the Start-End separation rank and the rank of
the matrix obtained by the corresponding grid tensor matricization, denotedJA(ycT,1,Θ )KS,E, with
respect to a specific set of template vectors. Note that the limitation to specific template vectors
does not restrict our results, as grid tensors are merely a tool used to bound the separation rank.
The additional equality to the rank of the matrix obtained by matricizing the shallow RAC weights
tensor, will be of use to us when proving our main results below (theorem 1).
Due to the inherent use of data duplication in the computation preformed by a deep RAC (see
appendix A.3 for further details), it cannot be written in a closed tensorial form similar to that of
eq. 6. This in turn implies that the equality shown in claim 1 does not hold for functions realized
by deep RACs. The following claim introduces a fundamental relation between a function’s Start-
End separation rank and the rank of the matrix obtained by the corresponding matricization. This
relation, which holds for all functions, is formulated below for functions realized by deep RACs:
Claim 2. Let ycT,L,Θ be a function realized by a depth L RAC (fig. 1(b)) after T time-steps. Then,
for any template vectors x(1), . . . , x(M) ∈ X it holds that:
seP(s,E)(yT,L,θ) ≥rank JA(yT,La)KS,e),	⑼
where A(ycT,L,Θ) is the grid tensor ofycT,L,Θ with respect to the above template vectors.
Proof. See appendix B.2.	□
Claim 2 will allow us to provide a lower bound on the Start-End separation rank of functions real-
ized by deep RACs, which we show to be exponentially higher than the Start-End separation rank of
functions realized by shallow RACs (to be obtained via claim 1). Thus, in the next section, we em-
ploy the above presented tools to show that an exponential enhancement of the Start-End separation
rank is brought forth by depth in recurrent networks.
2 Square-integrability may seem as a limitation at first glance, as for example neurons fd(x) = σ(wd>x +
bd) with sigmoid or ReLU activation σ(∙), do not meet this condition. However, since in practice our inputs
are bounded (e.g. image pixels by holding intensity values, etc), we may view functions as having compact
support, which, as long as they are continuous (holds in all cases of interest), ensures square-integrability.
6
Workshop track - ICLR 2018
4 Depth Enhanced Long-Term Memory in Recurrent Networks
In this section, we present the main theoretical contributions of this paper. In section 4.1, we for-
mally present a result which exponentially separates between the memory capacity of a deep (L = 2)
recurrent network and a shallow (L = 1) one. Following the formal presentation of results in theo-
rem 1, we discuss some of their implications and then conclude by sketching a proof outline for the
theorem (full proof is relegated to appendix B.3). In section 4.2, we present a quantitative conjec-
ture regarding the enhanced memory capacity of deep recurrent networks of general depth L, which
relies on the inherent combinatorial properties of the recurrent network’s computation. We leave the
formal proof of this conjecture for future work.
4.1	Separating Between Shallow and Deep Recurrent Networks
Theorem 1 states, that the correlations modeled between the beginning and end of the input se-
quence to a recurrent network, as measured by the Start-End separation rank (see section 3.1), can
be exponentially more complex for deep networks than for shallow ones:
Theorem 1. Let ycT,L,Θ be the function computing the output after T time-steps of an RAC with L
layers, R hidden channels per layer, weights denoted by Θ, and initial hidden states h0,l , l ∈ [L]
(fig. 1 with g = gRAC). Assume that the network’s initial mapping functions {fd}dM=1 are linearly
independent. Let sep(S,E) ycT,L,Θ be the Start-End separation rank of ycT,L,Θ (eq. 5). Then, the
following holds almost everywhere, i.e. for all values of Θ × h0,l buta set of Lebesgue measure zero:
1.	sep(s,E) (yT,L,θ) = min {R,MT/2}, for L = 1 (shallow network).
2.	sep(s,E)(yT,L,θ) ≥ min (《minT/M,"} ) , MT/2} ,for L = 2 (deep network),
where (^mmTM,R})is the multiset coefficient, g^ven in the binomial form by (min{M,R}+T/2-1).
The above theorem readily implies that depth entails an enhanced ability of recurrent networks to
model long-term temporal dependencies in the sequential input. Specifically, theorem 1 indicates
depth efficiency - it ensures Us that upon randomizing the weights of a deep RAC with R hidden
channels per layer, with probability 1 the function realized by it after T time-steps may only be
realized by a shallow RAC with a number of hidden channels that is exponentially large.3 Stated
alternatively, this means that almost all functional dependencies which lie in the hypothesis space
of deep RACs with R hidden channels per layer, calculated after T time-steps, are inaccessible to
shallow RACs with less than an exponential number of hidden channels. Thus, a shallow recurrent
network would require exponentially more parameters than a deep recurrent network, if it is to
implement the same function.
The established role of the Start-End separation rank as a correlation measure between the beginning
and the end of the sequence (see section 3.1), implies that these functions, which are realized by
almost any deep network and can never be realized by a shallow network of a reasonable size,
represent more elaborate correlations over longer periods of time. The above notion is strengthened
by the fact that the Start-End separation rank of deep RACs increases with the sequence length T ,
while the Start-End separation rank of shallow RACs is independent of it. This indicates that shallow
recurrent networks are much more restricted in modeling long-term correlations than the deep ones,
which enjoy an exponentially increasing Start-End separation rank as time progresses. Below, we
present an outline of the proof for theorem 1 (see appendix B.3 for the full version):
Proof sketch of theorem 1.
1.	For a shallow network, claim 1 establishes that the Start-End separation rank of the func-
tion realized by a shallow (L = 1) RAC is equal to the rank of the matrix obtained by
matricizing the corresponding shallow RAC weights tensor (eq. 6) according to the Start-
End partition: sep(S,E) ycT,1,Θ = rank JAcT,1,ΘK)S,E . Thus, it suffices to prove that
rank JAcT,1,ΘK)S,E = R in order to satisfy bullet (1) of the theorem, as the rank is triv-
ially upper-bounded by the dimension of the matrix, MT/2 . To this end, we call upon the
TT-decomposition of AcT,1,Θ, given by eq. 7, which corresponds to the MPS Tensor Net-
work presented in appendix A. We rely on a recent result by Levine et al. (2017), who
3The combinatorial coefficient (广吗为⑸))=(min{M,R}+T/2T) is exponentially dependent on R ≡
min{M, R}: for T > 2 * (R — 1) this value is larger than4,4R	(for sufficiently large values of R).
7
Workshop track - ICLR 2018
state that the rank of the matrix obtained by matricizing any tensor according to a partition
(S, E), is equal to a min-cut separating S from E in the Tensor Network graph representing
this tensor. The required equality follows from the fact that the TT-decomposition in eq. 7
is of TT-rank R, which in turn implies that the min-cut in the appropriate Tensor Network
graph is equal to R.
2.	For a deep network, claim 2 assures us that the Start-End separation rank of the function
realized by a depth L = 2 RAC is lower bounded by the rank of the matrix obtained by
the corresponding grid tensor matricization: sep(S,E) ycT,L,Θ ≥ rank JA(ycT,L,Θ)KS,E .
Thus, proving that rank (JA(yT,L,θ)Ks,E) ≥《minT/M,“}》for all of the values of param-
eters Θ × h0,l but a set of Lebesgue measure zero, would satisfy the theorem, and again,
the rank is trivially upper-bounded by the dimension of the matrix, MT/2. We use a lemma
proved in Sharir et al. (2016), which states that since the entries of A(ycT,L,Θ) are polyno-
mials in the deep recurrent network’s weights, it suffices to find a single example for which
the rank of the matricized grid tensor is greater than the desired lower bound. Finding such
an example would indeed imply that for almost all of the values of the network parame-
ters, the desired inequality holds. We choose a weight assignment such that the resulting
matricized grid tensor resembles a matrix obtained by raising a rank-R ≡ min{M, R}
matrix to the Hadamard power of degree T /2. This operation, which raises each element
of the original rank-R matrix to the power of T/2, was shown to yield a matrix with a
rank upper-bounded by the multiset coefficient TR/2	(see e.g. Amini et al. (2012)). We
show that our assignment results in a matricized grid tensor with a rank which is not only
upper-bounded by this value, but actually achieves it.
□
4.2 Increase of Memory Capacity with Depth
Theorem 1 provides a lower bound of TR/2 on the Start-End separation rank of depth L = 2
recurrent networks, exponentially separating deep recurrent networks from shallow ones. By a triv-
ial assignment of weights in higher layers, the Start-End separation rank of even deeper recurrent
networks (L > 2) is also lower-bounded by this expression, which does not depend on L. In the
following, we conjecture that a tighter lower bound holds for networks of depth L > 2, the form of
which implies that the memory capacity of deep recurrent networks grows exponentially with the
network depth:
Conjecture 1. Under the same conditions as in theorem 1, for all values of Θ × h0,l but a set of
Lebesgue measure zero, it holds for any L that:
seP(s,E)(yT'L,θ) ≥ min I ((minTMjR}))，MT/2j .
We motivate conjecture 1 by investigating the combinatorial nature of the computation performed
by a deep RAC. By constructing Tensor Networks which correspond to deep RACs, we attain an
informative visualization of this combinatorial perspective. In appendix A, we provide full details
of this construction and present the formal motivation for the conjecture. Below, we qualitatively
outline this combinatorial approach.
A Tensor Network is essentially a graphical tool for representing algebraic operations which resem-
ble multiplications of vectors and matrices, between higher order tensors. Fig. 2 shows an example
of the Tensor Network representing the computation of a depth L = 3 RAC after T = 6 time-steps.
This well-defined computation graph hosts the values of the weight matrices at its nodes. The inputs
{x1, . . . , xT} are marked by their corresponding time-step {1, . . . , T}, and are integrated in a depth
dependent and time-advancing manner (see further discussion regarding this form in appendix A.3),
as portrayed in the example of fig. 2. We highlight in red the basic unit in the Tensor Network which
connects “Start” inputs {1, . . . , T/2} and “End” inputs {T/2 + 1, . . . , T}. In order to estimate a lower
bound on the Start-End separation rank of a depth L > 2 recurrent network, we employ a similar
strategy to that presented in the proof sketch of the L = 2 case (see section 4.1). Specifically, we
rely on the fact that it is sufficient to find a specific instance of the network parameters Θ × h0,l
for which JA(ycT,L,Θ)KS,E achieves a certain rank, in order for this rank to bound the Start-End
separation rank of the network from below.
8
Workshop track - ICLR 2018
In
6------------------∙I
ITm
# of reps.
(X
Γ
Figure 2: Tensor Network representing the computation of a depth L = 3 RAC after T = 6 time-
steps. See construction in appendix A.
Indeed, we find a specific assignment of the network weights, presented in appendix A.4, for which
the Tensor Network effectively takes the form of the basic unit connecting “Start” and “End”, raised
to the power of the number of its repetitions in the graph (bottom of fig. 2). This basic unit corre-
sponds to a simple computation represented by a grid tensor with Start-End matricization of rank
R. Raising such a matrix to the Hadamard power of any p ∈ Z, results in a matrix with a rank
upper bounded by Rp , and the challenge of proving the conjecture amounts to proving that the
upper bound is tight in this case. In appendix A.4, we prove that the number of repetitions of the
basic unit connecting “Start” and “End” in the deep RAC Tensor Network graph, is exactly equal to
LT-/21 for any depth L. For example, in the T = 6, L = 3 network illustrated in fig. 2, the number
of repetitions indeed corresponds to p = 32 = 6. It is noteworthy that for L = 1, 2 the bound in
conjecture 1 coincides with the bounds that were proved for these depths in theorem 1.
Conjecture 1 indicates that beyond the proved exponential advantage in memory capacity of deep
networks over shallow ones, a further exponential separation may be shown between recurrent net-
works of different depths. We leave the proof of this result, which can reinforce and refine the
understanding of advantages brought forth by depth in recurrent networks, as an open problem.
5 Discussion
The notion of depth efficiency, by which deep networks efficiently express functions that would
require shallow networks to have a super-polynomial size, is well established in the context of con-
volutional networks. However, recurrent networks differ from convolutional networks, as they are
suited by design to tackle inputs of varying lengths. Accordingly, depth efficiency alone does not
account for the remarkable performance of recurrent networks on long input sequences. In this pa-
per, we identified a fundamental need for a quantifier of ‘time-series expressivity’, quantifying the
memory capacity of recurrent networks. In order to meet this need, we proposed a measure of the
ability of recurrent networks to model long-term temporal dependencies, in the form of the Start-End
separation rank. The separation rank was used to quantify correlations in convolutional networks,
and has roots in the field of quantum physics. The proposed measure adjusts itself to the temporal
extent of the input series, and quantifies the ability of the recurrent network to correlate the incoming
sequential data as time progresses.
We analyzed the class of Recurrent Arithmetic Circuits, which are closely related to successful RNN
architectures, and proved that the Start-End separation rank of deep RACs increases exponentially
as the input sequence extends, while that of shallow RACs is independent of the input length. These
results, which demonstrate that depth brings forth an overwhelming advantage in the ability of recur-
rent networks to model long-term dependencies, were achieved by combining tools from the fields
of measure theory, tensorial analysis, combinatorics, graph theory and quantum physics.
Such analyses may be readily extended to other architectural features employed in modern recurrent
networks. Indeed, the same time-series expressivity question may now be applied to the different
variants of LSTM networks, and the proposed notion of Start-End separation rank may be employed
for quantifying their memory capacity. We have demonstrated that such a treatment can go beyond
unveiling the origins of the success of a certain architectural choice, and leads to new insights. The
above established observation that correlations achievable by vanilla shallow recurrent network do
not adapt at all to the sequence length, is an exemplar of this potential.
Moreover, practical recipes may emerge by such theoretical analyses. The experiments preformed
in Hermans and Schrauwen (2013), suggest that shallow layers of recurrent networks are related to
9
Workshop track - ICLR 2018
short time-scales, e.g. in speech: phonemes, syllables, words, while deeper layers appear to support
correlations of longer time-scales, e.g. full sentences, elaborate questions. These findings open the
door to further depth related investigations in recurrent networks, and specifically the role of each
layer in modeling temporal correlations may be better understood. Levine et al. (2017) establish
theoretical observations which translate into practical conclusions regarding the number of hidden
channels to be chosen for each layer in a deep convolutional network. The conjecture presented in
this paper, by which the Start-End separation rank of recurrent networks grows exponentially with
depth, can similarly entail practical recipes for enhancing their memory capacity. Such analyses
can be reinforced by experiments, and lead to a profound understanding of the contribution of deep
layers to the recurrent network’s memory. Indeed, we view this work as an important step towards
novel methods of matching the recurrent network architecture to the temporal correlations in a given
sequential data set.
References
Arash Amini, Amin Karbasi, and Farokh Marvasti. Low-rank matrix approximation using point-wise operators.
IEEE Transactions on Information Theory, 58(1):302-310, 2012.
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case,
Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech
recognition in english and mandarin. In International Conference on Machine Learning, pages 173-182,
2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to
align and translate. arXiv preprint arXiv:1409.0473, 2014.
Gregory Beylkin and Martin J Mohlenkamp. Numerical operator calculus in higher dimensions. Proceedings
of the National Academy of Sciences, 99(16):10246-10251, 2002.
Gregory Beylkin, Jochen Garcke, and Martin J Mohlenkamp. Multivariate regression and machine learning
with sums of separable functions. SIAM Journal on Scientific Computing, 31(3):1840-1857, 2009.
Kyunghyun Cho, Bart Van Merrienboer, Caglar GulCehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
maChine translation. arXiv preprint arXiv:1406.1078, 2014.
Nadav Cohen and Amnon Shashua. Convolutional reCtifier networks as generalized tensor deCompositions.
International Conference on Machine Learning (ICML), 2016.
Nadav Cohen and Amnon Shashua. InduCtive bias of deep Convolutional networks through pooling geometry.
In 5th International Conference on Learning Representations (ICLR), 2017.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis.
Conference On Learning Theory (COLT), 2016.
Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-produCt networks. In J. Shawe-Taylor, R. S.
Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing
Systems 24, pages 666-674. Curran AssoCiates, InC., 2011. URL http://papers.nips.cc/paper/
4350- shallow- vs- deep- sum- product- networks.pdf.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on
Learning Theory, pages 907-940, 2016.
Alex Graves. Generating sequenCes with reCurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. SpeeCh reCognition with deep reCurrent neural
networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pages
6645-6649. IEEE, 2013.
Wolfgang HaCkbusCh. On the effiCient evaluation of CoalesCenCe integrals in population balanCe models. Com-
puting, 78(2):145-159, 2006.
Wolfgang HaCkbusCh. Tensor spaces and numerical tensor calculus, volume 42. Springer SCienCe & Business
Media, 2012.
Godfrey Harold Hardy, John Edensor Littlewood, and George Polya. Inequalities. Cambridge university press,
1952.
10
Workshop track - ICLR 2018
Robert J Harrison, George I Fann, Takeshi Yanai, and Gregory Beylkin. Multiresolution quantum chemistry in
multiwavelet bases. In Computational Science-ICCS 2003, pages 103-110. Springer, 2003.
Michiel Hermans and Benjamin Schrauwen. Training and analysing deep recurrent neural networks. In Ad-
vances in Neural Information Processing Systems, pages 190-198, 2013.
SePP HoChreiter and Jurgen SChmidhuber. Long short-term memory. Neural ComPutation, 9(8):1735-1780,
1997.
Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits, 1998.
Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quantum entanglement:
Fundamental ConneCtions with impliCations to network design. arXiv preprint arXiv:1704.01552, 2017.
Roman OrUs. A practical introduction to tensor networks: Matrix product states and projected entangled pair
states. Annals of Physics, 349:117-158, 2014.
Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295-2317, 2011.
Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In Computer Vision
Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 689-690. IEEE, 2011.
Jurgen H Schmidhuber. Learning complex, extended sequences using the principle of history compression.
Neural Computation, 1992.
Or Sharir and Amnon Shashua. On the expressive power of overlapping architectures of deep learning. arXiv
preprint arXiv:1703.02065, 2017.
Or Sharir, Ronen Tamari, Nadav Cohen, and Amnon Shashua. Tractable generative convolutional arithmetic
circuits. 2016.
Edwin Stoudenmire and David J Schwab. Supervised learning with tensor networks. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing
Systems 29, pages 4799-4807. Curran Associates, Inc., 2016.
Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017-1024, 2011.
Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan R Salakhutdinov. On multiplicative
integration with recurrent neural networks. In Advances in Neural Information Processing Systems, pages
2856-2864, 2016.
11
Workshop track - ICLR 2018
Figure 3: A quick introduction to Tensor Networks (TNs). a) Tensors in the TN are represented by
nodes. The degree of the node corresponds to the order of the tensor represented by it. b) A matrix
multiplying a vector in TN notation. The contracted index k, which connects two nodes, is summed
upon, while the open index d is not. The number of open indices equals the order of the tensor
represented by the entire network. All of the indices receive values that range between 1 and their
bond dimension. The contraction is marked by the dashed line.
A Tensor Network Representation of Recurrent Arithmetic
CIRCUITS
In this section, we expand our algebraic view on recurrent networks and make use of a graphical approach to
tensor decompositions referred to as Tensor Networks (TNs). The tool of TNs is mainly used in the many-body
quantum physics literature for a graphical decomposition of tensors, and has been recently connected to the
deep learning field by Levine et al. (2017), who constructed a deep convolutional network in terms of a TN.
The use of TNs in machine learning has appeared in an empirical context, where Stoudenmire and Schwab
(2016) trained a Matrix Product State (MPS) TN to preform supervised learning tasks on the MNIST data
set (LeCun et al., 1998). The constructions presented in this section suggest a separation in expressiveness
between recurrent networks of different depths, as formulated by conjecture 1.
We begin in section A.1 by providing a brief introduction to TNs. Next, we present in section A.2 the TN
which corresponds to the calculation of a shallow RAC, and tie it to a common TN architecture referred to as
a Matrix Product State (MPS)(See overview in e.g. OnUS (2014)), and equivalently to the tensor train (TT)
decomposition (Oseledets, 2011). Subsequently, we present in section A.3 a TN construction of a deep RAC,
and emphaSize the characteriSticS of thiS conStruction that are the origin of the enhanced ability of deep RACS
to model elaborate temporal dependencieS. Finally, in Section A.4, we make uSe of the above TNS conStruction
in order to formally motivate conjecture 1, according to which the Start-End Separation rank of RACS growS
exponentially with depth.
A.1	Introduction to Tensor Networks
A TN iS a weighted graph, where each node correSpondS to a tenSor whoSe order iS equal to the degree of the
node in the graph. Accordingly, the edgeS emanating out of a node, alSo referred to aS itS legS, repreSent the
different modeS of the correSponding tenSor. The weight of each edge in the graph, alSo referred to aS itS bond
dimenSion, iS equal to the dimenSion of the appropriate tenSor mode. In accordance with the relation between
mode, dimenSion and index of a tenSor preSented in Section 3.2, each edge in a TN iS repreSented by an index
that runS between 1 and itS bond dimenSion. Fig. 3(a) ShowS three exampleS: (1) A vector, which iS a tenSor of
order 1, iS repreSented by a node with one leg. (2) A matrix, which iS a tenSor of order 2, iS repreSented by a
node with two legS. (3) Accordingly, a tenSor of order N iS repreSented in the TN aS a node with N legS.
We move on to preSent the connectivity propertieS of a TN. EdgeS which connect two nodeS in the TN repre-
Sent an operation between the two correSponding tenSorS. A index which repreSentS Such an edge iS called a
contracted index, and the operation of contracting that index iS in fact a Summation over all of the valueS it can
take. An index repreSenting an edge with one looSe end iS called an open index. The tenSor repreSented by the
entire TN, whoSe order iS equal to the number of open indiceS, can be calculated by Summing over all of the
contracted indiceS in the network. An example for a contraction of a Simple TN iS depicted in fig. 3(b). There,
a TN correSponding to the operation of multiplying a vector v ∈ Rr1 by a matrix M ∈ Rr2 ×r1 iS performed
by Summing over the only contracted index, k . AS there iS only one open index, d, the reSult of contracting
the network iS an order 1 tenSor (a vector): u ∈ Rr2 which upholdS u = Mv. Though we uSe below the
contraction of indiceS in more elaborate TNS, thiS operation can be eSSentially viewed aS a generalization of
matrix multiplication.
12
Workshop track - ICLR 2018
Figure 4: a) The Tensor Network representing the calculation performed by a shallow RAC. b) A
Tensor Network construction of the recursive relation given an eq. 1. c) A presentation of the shallow
RAC weights tensor in a standard MPS form.
A.2 Shallow RAC Tensor Network
The computation of the output at time T that is preformed by the shallow recurrent network given by eqs. 1
and 3, or alternatively by eqs. 6 and 7, can be written in terms of a TN. Fig. 4(a) shows this TN, which
given some initial hidden state h0, is essentially a temporal concatenation of a unit cell that preforms a similar
computation at every time-step, as depicted in fig. 4(b). For any time t < T , this unit cell is composed of
the input weights matrix, WI, contracted with the inputs vector, f (xt), and the hidden weights matrix, WH,
contracted with the hidden state vector of the previous time-step, ht-1. The final component in each unit cell
is the 3 legged triangle representing the order 3 tensor δ ∈ RR×R×R, referred to as the δ tensor, defined by:
1,
0,
i1 = i2 = i3
otherwise
(10)
with ij ∈ [R] ∀j ∈ [3], i.e. its entries are equal to 1 only on the super-diagonal and are zero otherwise. The use
of a triangular node in the TN is intended to remind the reader of the restriction given in eq. 10. The recursive
relation that is defined by the unit cell, is given by the TN in fig. 4(b):
RM
hkt =	X	X WklLIkt-1 hk-1ι Wdtdt fdt (Xt)δfct-idtkt =
kt-1 ,kt-1 ,dt=1 dt = 1
R
~ E	(W HhtT)kt-i (W 1 f (Xt))dt%-idtkt
kt-1 dt = 1
= (WHht-1)kt(WIf(Xt))kt,
(11)
where kt ∈ [R]. In the first equality, we simply follow the TN prescription and write a summation over all of
the contracted indices in the left hand side of fig. 4(b), in the second equality we use the definition of matrix
multiplication, and in the last equality we use the definition of the δ tensor. The component-wise equality of
eq. 11 readily implies ht = (WHht-1)	(WIf(Xt)), reproducing the recursive relation in eqs. 1 and 3, which
defines the operation of the shallow RAC. From the above treatment, it is evident that the restricted δ tensor is
in fact the component in the TN that yields the element-wise multiplication property. After T repetitions of the
unit cell calculation with the sequential input {Xt}tT=1, a final multiplication of the hidden state vector hT by
the output weights matrix WO yields the output vector yT,1,Θ .
The tensor network which represents the order T shallow RAC weights tensor AcT,1,Θ, which appears in eqs. 6
and 7, is given by the TN in the upper part of fig. 4(a). In fig. 4(c), we show that by a simple contraction of
indices, the TN representing the shallow RAC weights tensor AcT,1,Θ can be drawn in the form of a standard
MPS TN. This TN allows the representation of an order T tensor with a linear (in T) amount of parameters,
rather than the regular exponential amount (A has MT entries). The decomposition which corresponds to this
TN is known as the Tensor Train (TT) decomposition of rank R in the tensor analysis community, its explicit
form given in eq. 7.
13
Workshop track - ICLR 2018
The presentation of the shallow recurrent network in terms of a TN allows the employment of the min-cut anal-
ysis, which was introduced by Levine et al. (2017) in the context of convolutional networks, for quantification
of the information flow across time modeled by the shallow recurrent network. This was indeed preformed in
our proof of the shallow case of theorem 1. We now move on to present the computation preformed by a deep
recurrent network in the language of TNs.
A.3 Deep RAC Tensor Network
The construction of a TN which matches the calculation of a deep recurrent network is far less trivial than
that of the shallow case, due to the seemingly innocent property of reusing information which lies at the heart
of the calculation of deep recurrent networks. Specifically, all of the hidden states of the network are reused,
since the state of each layer at every time-step is duplicated and sent as an input to the calculation of the same
layer in the next time-step, and also as an input to the next layer up in the same time-step (see fig. 1(b)). The
required operation of duplicating a vector and sending it to be part of two different calculations, which is simply
achieved in any practical setting, is actually impossible to represent in the framework of TNs. We formulate
this notion in the following claim:
Claim 3. Let v ∈ RP , P ∈ N be a vector. v is represented by a node with one leg in the TN notation. The
operation of duplicating this node, i.e. forming two separate nodes of degree 1, each equal to v, cannot be
achieved by any TN.
Proof. We assume by contradiction that there exists a Tensor Network φ which operates on any vector v ∈ RP
and clones it to two separate nodes of degree 1, each equal to v, to form an overall TN representing V Z v.
Component wise, this implies that φ upholds ∀v ∈ RP : PiP=1 φijkvi = vjvk. By our assumption, φ
duplicates the standard basis elements of RP, denoted {^(α)}P=ι, meaning that ∀α ∈ [P]:
P
X φijk ^(α) = ^(α)^k").	(12)
i=1
By definition of the standard basis elements, the left hand side of eq. 12 takes the form φαjk while the right
hand side equals 1 only if j = k = α, and otherwise 0. Utilizing the δ-tensor notation presented in eq. 10,
in order to successfully clone the standard basis elements, eq. 12 implies that φ must uphold φαjk = δαjk .
However, for v = 1, i.e. ∀j ∈ [P] : vj = 1, a cloning operation does not take place when using this value
of φ, since PiP=1 φijkvi = PiP=1 δijk = δjk 6= 1 = vivj , in contradiction to φ duplicating any vector in
RP.	□
Claim 3 seems to pose a hurdle in our pursuit of a TN representing a deep recurrent network. Nonetheless, a
form of such a TN may be attained by a simple 'trick' 一 in order to model the duplication that is inherently
present in the deep recurrent network computation, we resort to duplicating the input data itself. By this
technique, for every duplication that takes place along the calculation, the input is inserted into the TN multiple
times, once for each sequence that leads to the duplication point. This principle, which allows us to circumvent
the restriction imposed by claim 3, yields the elaborate TN construction of deep RACs depicted in fig. 5.
It is important to note that these TNs, which grow exponentially in size as the depth L of the recurrent network
represented by them increases, are merely a theoretical tool for analysis and not a suggested implementation
scheme for deep recurrent networks. The actual deep recurrent network is constructed according to the simple
scheme given in fig. 1(b), which grows only linearly in size as the depth L increases, despite the corresponding
TN growing exponentially. In fact, this exponential ‘blow-up’ in the size of the TNs representing the deep
recurrent networks is closely related to their ability to model more intricate correlations over longer periods of
time in comparison with their shallower counterparts, which was established in section 4.
Fig. 5 shows TNs which correspond to depth L = 2, 3 RACs. Even though the TNs in fig. 5 seem rather
convoluted and complex, their architecture follows clear recursive rules. In fig. 5(a), a depth L = 2 recurrent
network is presented, spread out in time onto T = 4 time-steps. To understand the logic underlying the input
duplication process, which in turn entails duplication of entire segments of the TN, we focus on the calculation
of the hidden state vector h2,2 that is presented in fig. 5(b). When the first inputs vector, f(x1), is inserted
into the network, it is multiplied by WI,1 and the outcome is equal to h1,1.4 Next, h1,1 is used in two different
places, as an inputs vector to layer L = 2 at time t = 1, and as a hidden state vector in layer L = 1 for time
t = 2 calculation. Our input duplication technique inserts f(x1) into the network twice, so that the same exact
h1,1 is achieved twice in the TN, as marked by the red dotted line in fig. 5(b). This way, every copy of h1,1
4In this figure, the initial condition for each layer l ∈ L, hl,0, is chosen such that a vector of ones will be
present in the initial element-wise multiplication: (h0,l)T = IT(WH,l)*,where f denotes the pseudoinverse
operation.
14
Workshop track - ICLR 2018
Figure 5: a) The Tensor Network representing the calculation preformed by a depth L = 2 RAC
after 4 time-steps. b) A Tensor Network construction of the hidden state h2,2 (see eq. 13), which
involves duplication of the hidden state h1,1 that is achieved by duplicating the input x1. c) The
Tensor Network representing the calculation preformed by a depth L = 3 RAC after 3 time-steps.
Here too, as in any deep RAC, several duplications take place.
goes to the appropriate segment of the calculation, and indeed the TN in fig. 5(b) holds the correct value of
h2,2:
h2,2 = WH,2WI,2h1,1	WI,2((WH,1h1,1)	(WI,1f(x2))) .	(13)
The extension to deeper layers leads us to a fractal structure of the TNs, involving many self similarities, as
in the L = 3 example given in fig. 5(c). The duplication of intermediate hidden states, marked in red and
blue in this example, is the source of the apparent complexity of this L = 3 RAC TN. Generalizing the above
L = 1, 2, 3 examples, a TN representing an RAC of general depth L and of T time-steps, would involve in
its structure T duplications of TNs representing RACs of depth L - 1, each of which has a distinct length
in time-steps i, where i ∈ [T]. This fractal structure leads to an increasing with depth complexity of the TN
representing the depth L RAC computation, which we show in the next subsection to motivate the combinatorial
lower bound on the Start-End separation rank of deep RACs, given in conjecture 1.
A.4 A Formal Motivation for Conjecture 1
The above presented construction of TNs which correspond to deep RACs, allows us to further investigate
the effect of network depth on its ability to model long-term temporal dependencies. We present below a
formal motivation for the lower bound on the Start-End separation rank of deep recurrent networks, given in
conjecture 1. Though our analysis employs TNS visualizations, it is formal nonetheless - these graphs represent
the computation in a well-defined manner (see sections A.1-A.3).
Our conjecture relies on the fact that it is sufficient to find a specific instance of the network parameters Θ × h0,l
for which JA(ycT,L,Θ)KS,E achieves a certain rank, in order for this rank to be a lower bound on the Start-
End separation rank of the network. This follows from combining claim 2 and lemma 1. Claim 2 assures
us that the Start-End separation rank of the function realized by an RAC of any depth L, is lower bounded
by the rank of the matrix obtained by the corresponding grid tensor matricization: sep(S,E) ycT,L,Θ ≥
rank (JA(yT,L,θ )Ks,e ). Thus, one must show that rank (JA(yT,L,θ )Ks,e ) ≥ (( mjnT/lj))) for all of the
values of parameters Θ × h0,l but a set of Lebesgue measure zero, in order to establish the lower bound in
conjecture 1. Next, we rely on lemma 1, which states that since the entries ofA(ycT,L,Θ) are polynomials in the
deep recurrent network’s weights, it suffices to find a single example for which the rank of the matricized grid
15
Workshop track - ICLR 2018
345 1 1
# of reps.
Figure 6: Above: TN representing the computation of a depth L = 3 RAC after T = 6 time-steps,
when choosing WI,2 to be of rank-1. See full TN, for general values of the weight matrices, in fig. 2.
Below: Reduction of this TN to the factors affecting the Start-End matricization of the grid tensor
represented by the TN.
tensor is greater than the desired lower bound. Finding such an example would indeed imply that for almost all
of the values of the network parameters, the desired inequality holds.
In the following, we choose a weight assignment that effectively ‘separates’ between the first layer and higher
layers, in the sense that WI,2 is of rank-1. This is done in similar spirit to the assignment used in the proof
of theorem 1, in which WiIj,2 ≡ δi1 (see section B.3). Under this simplifying assignment, which suffices for
our purposes according to the above discussion, the entire computation performed in deeper layers contributes
only a constant factor to the matricized grid tensor. In this case, the example of the TN corresponding to an
RAC of depth L = 3 after T = 6 time-steps, which is shown in full in fig.2, takes the form shown in the upper
half of fig. 6. Next, in order to evaluate rank JA(ycT,L,Θ)KS,E , we note that graph segments which involve
only indices from the “Start” set, will not affect the rank of the matrix under mild conditions on WI,1, WH,1.5
Specifically, under the Start-End matricization these segments will amount to a different constant multiplying
each row of the matrix. For the example of the RAC of depth L = 3 after T = 6 time-steps, this amounts to the
effective TN given in the bottom left side of fig. 6. Finally, the dependence of this TN on the indices of time-
steps {T/2 + 2, . . . , T}, namely those outside of the basic unit involving indices of time-steps {1, . . . , T/2+ 1},
may only increase the resulting Start-End matricization rank.6 Thus, we are left with an effective TN resembling
the one shown in section 4.2, where the basic unit separating “Start” and “End” indices is raised to the power
of the number of its repetitions in the graph. In the following, we prove a claim according to which the number
of repetitions of this basic unit in the TN graph increases exponentially with the depth of the RAC:
Claim 4. Let φ(T, L, R) be the TN representing the computation performed after T time-steps by an RAC with
L layers and R hidden channels per layer. Then, the number of occurrences in layer L = 1 of the basic unit
connecting “Start” and “End” indices (bottom right in fig. 6), is exactly LT-/21 .
Proof. Let ycT,L,Θ be the function computing the output after T time-steps of an RAC with L layers, R hidden
channels per layer and weights denoted by Θ. In order to focus on repetitions in layer L = 1, we assign
WiIj,2 ≡ δi1 for which the following upholds7:
T tL	t3	R	t2	t2-1
A(yT'L,θ)dι,...,dτ = (Const.) Y Y …Y X Y Wrjdj Y WHrj+1
tL=1 tL-1 =1 t2=1 r1 ,...,rt2 =1	j=1	j=1
T	tL	t3	R	t2	t2-1
= (Const.) (Vd1...dT/2 )	Y	Y …Y X Y W⅞dj Y WHrIj+1
tL=T/2+1 tL-1=T/2+1	t2=T/2+1 r1,...,rt2 =1	j=1	j=1
where the constant term in the first line is the contribution of the deeper layers under this assignment,
and the tensor Vd1...dT/2, which becomes a vector under the Start-End matricization, reflects the contribu-
tion of the “Start” set indices. Observing the argument of the chain of products in the above expression,
PrR1,...,rt2=1 Qtj2=1WrIj,1djQtj2=-11WrHj,r1j+1,itis an order t2 tensor, exactly given by the TN representing
the computation of a depth L = 1 RAC after t2 time-steps. Specifically, for t2 = T/2 + 1, it is exactly equal to
the basic TN unit connecting “Start” and “End” indices, and for T/2 + 1 < t2 ≤ T it contains this basic unit.
This means that in order to obtain the number of repetition of this basic unit in φ, we must count the number of
5For example, this holds if WI,1 is fully ranked and does not have vanishing elements, and WH,1 = I.
6This is not true for any TN of this shape but holds due to the temporal invariance of the recurrent network’s
weights.
7See a similar and more detailed derivation in section B.3.
16
Workshop track - ICLR 2018
multiplications implemented by the chain of products in the above expression. Indeed this number is equal to:
T	tL
XX
tL=T/2+1 tL-1=T/2+1
t3
X t2
t2=T/2+1
LT-/21
□
Finally, the form of the lower bound presented in conjecture 1 is obtained by considering a rank R matrix, such
as the one obtained by the Start-End matricization of the TN basic unit discussed above, raised to the Hadamard
power of《 二匕).The rank of the resultant matrix, is upper bounded by ((《TR)))) . We leave it as an
open problem to prove conjecture 1, by proving that the upper bound is indeed tight in this case.
B Deferred proofs
B.1	Proof of Claim 1
We begin by proving the equality sep(S,E) ycT,1,Θ = rank JAcT,1,Θ KS,E . As shown in Cohen and Shashua
(2017), for any function f : Rs ×∙∙∙× Rs → R which follows the structure of eq. 6 with a general weights
tensor A, assuming that {fd}dM=1 are linearly independent, measurable, and square-integrable (as assumed in
claim 1), it holds that sep(S,E) (f) = rank (JAKS,E). Specifically, for f = ycT,1,Θ and A = AcT,1,Θ the above
equality holds.
It remains to prove that there exists template vectors for which rank JAcT,1,ΘKS,E	=
rank (JA(yT,1,θ)Ks,E).	For any given set of template vectors x(1),...,x(M)
∈ X, we define the
matrix F ∈ RM×M such that Fij = fj (x(i)), for which it holds that:
MT
A(ycT,1,Θ)k1,...,kT = X	AcT,1,Θ	Yfdi(x(ki))
d1,...,dT =1	d1,...,dT i=1
MT
= X	AcT,1,Θd d YFkidi.
d1,...,dT =1	d1,...,dT i=1
The right-hand side in the above equation can be regarded as a linear transformation of AcT,1,Θ specified by
the tensor operator F Z …Z F, which is more commonly denoted by (F Z …Z F)(AT,1,θ). Accord-
ing to lemma 5.6 in Hackbusch (2012), if F is non-singular then rank (J(F Z …Z F)(AT,1,θ)Ki,j)=
rank JAcT,1,Θ KI,J , for any partition (I, J). To conclude our proof, we simply note that Cohen and Shashua
(2016) showed that if {fd}dM=1 are linearly independent then there exists template vectors for which F is non-
singular.
B.2 Proof of Claim 2
If sep(S,E) ycT,L,Θ = ∞ then the inequality is trivially satisfied. Otherwise, assume that
seP(s,E) (yT,L©) = K ∈ N, and let {gis , gie }iK=1 be the functions of the respective decomposition to a
sum of separable functions, i.e. that the following holds:
K
yT,L,θ(x1,..., XT) = X gV(x1,..., XT/2) ∙ gν (XT/2+1,…,XT).
ν=1
Then, by definition of the grid tensor, for any template vectors X(1) , . . . , X(M) ∈ X the following equality
holds:
K
A(ycT,L,Θ)d1,...,dN = Xgνs(X(d1),. . . ,χ(dT∕2)) ∙ ge (X(IdT72+I)	χ(dT ) )
ν=1
K
≡ EVVI,…,dT∕2 Udτ∕2 + ι,...,dτ ,
ν=1
17
Workshop track - ICLR 2018
where V ν and Uν are the tensors holding the values of gνs and gνe , respectively, at the points defined by the
template vectors. Under the matricization according to the (S, E) partition, it holds that JV ν KS,E and JU ν KS,E
are column and row vectors, respectively, which we denote by vν and uνT . It follows that the matricization of
the grid tensor is given by:
K
JA(ycT,L,Θ)KS,E = XvνuνT,
ν=1
which means that rank (jA(yT,L,θ)Ks,E) ≤ K 二 SeP(S,e)(yT,L,θ).
18
Workshop track - ICLR 2018
B.3	Proof of Theorem 1
In this sub-section, we follow the proof strategy that is outlined in section 4, and prove theorem 1, which
shows an exponential advantage of deep recurrent networks over shallow ones in the ability to model long-term
dependencies, as measured by the Start-End separation rank (see section 3.1). In sections B.3.1 and B.3.2, we
prove the bounds on the Start-End separation rank of the shallow and deep RACs, respectively, while more
technical lemmas which are employed during the proof are relegated to section B.3.3.
B.3.	1 The Start-End Separation Rank of Shallow RACs
We consider the Tensor Network construction of the calculation carried out by a shallow RAC, given in fig. 4.
According to the presented construction, the shallow RAC weights tensor (eqs. 6 and 7) is represented by a
Matrix Product State (MPS) Tensor Network (Orus, 2014), With the following order-3 tensor building block:
Mkt-1dtkt = WkI t dt WkHt kt-1 , where dt ∈ [M] is the input index and kt-1, kt ∈ [R] are the internal indices
(see fig. 4(c)). In TN terms, this means that the bond dimension of this MPS is equal to R. We apply the result
of Levine et al. (2017), who state that the rank of the matrix obtained by matricizing any tensor according to a
partition (S, E) is equal to a min-cut separating S from E in the Tensor Network graph representing this tensor,
for all of the values of the TN parameters but a set of Lebesgue measure zero. In this MPS Tensor Network, the
minimal cut w.r.t. the partition (S, E) is equal to the bond dimension R, unless R > MT/2, in which case the
minimal cut contains the external legs instead. Thus, in the TN representing AcT,1,Θ, the minimal cut w.r.t. the
partition (S, E) is equal to min{R, MT/2}, implying rank (JAT,1,θK)s,E) = min{R, MT/2} for all values
of the parameters but a set of Lebesgue measure zero. The first half of the theorem follows from applying
claim 1, which assures us that the Start-End separation rank of the function realized by a shallow (L = 1) RAC
is equal to rank (JAT,1,θK)s,E).
B.3.2	Lower-bound on the Start-End Separation Rank of Deep RACs
For a deep network, claim 2 assures us that the Start-End separation rank of the function realized by a depth
L = 2 RAC is lower bounded by the rank of the matrix obtained by the corresponding grid tensor matricization,
for any choice of template vectors. Specifically:
sep(S,E) ycT,L,Θ ≥ rank JA(ycT,L,Θ)KS,E .
Thus, since it trivially holds that rank (JA(yT,L,θ)Ks,E) ≤ MT/2 (the rank is smaller than the dimension of
the matrix), proving that rank (JA(yT,L,θ)Ks,E) ≥ (^mmTRgM})) for all of the values of parameters Θ× h0,l
but a set of Lebesgue measure zero, would satisfy the theorem.
In the following, we provide an assignment of weight matrices and initial hidden states for which
rank (JA(yT,L,θ)Ks,E) = {mmT¾M}). In accordance with claim 5, this will suffice as such an assign-
ment implies this rank is achieved for all configurations of the recurrent network weights but a set of Lebesgue
measure zero.
We begin by choosing a specific set of template vectors x(1) , . . . , x(M) ∈ X. Let F ∈ RM×M be a matrix
with entries defined by Fij ≡ fj (x(i)). According to Cohen and Shashua (2016), since {fd}dM=1 are linearly
independent, then there is a choice of template vectors for which F is non-singular.
1 i=j
Next, we describe our assignment. In the expressions below we use the notation δij =	. Let
0 i 6= j
Z ∈ R \ {0} be an arbitrary non-zero real number, let Ω ∈ R+ be an arbitrary positive real number, and let
Z ∈ Rr×m beamatrix withentries Zij ≡ Jzω "i	i ≤ M.
ij 0 i > M
We set W1,1 ≡ Z ∙ (FT)-1 and set W1,2 such that its entries are Wij,2 ≡ δii, we set WH,1 ≡ WH,2 ≡ I, i.e.
to the identity matrix, and additionally we set the entries of WO to WiOj = δ1j. Finally, we choose the initial
hidden state values so they bear no effect on the calculation, namely h0,l = (WH,l) 11 = 1 for l = 1, 2.
19
Workshop track - ICLR 2018
Under the above assignment, the output for the corresponding class c after T time-steps is equal to:
ycT,L,Θ(x1,...,xT)= WOhT,2
(WiOj ≡ δ1j) ⇒ = (hT,2)1
(eq. 4) ⇒ = (W H,2 hT -1,2)	(WI,2hT,1)
(W H,2 ≡ I) ⇒ = (hT-1,2)	(WI,2hT,1)
T
(h0,2 = 1) ⇒ = Y WI,2ht,1
TR
(Wj2 ≡ δli) ⇒ = YX(ht,1)r
t=1 r=1
TR
(eq. 4) ⇒ = YX (WH,1ht-1,1)	(WI,1f(xt))
t=1 r=1	r
TR
(W H,1 ≡I) ⇒=YX (ht-1,1)	(WI,1f(xt))
t=1 r=1	r
TRt
(h0,1=1)⇒=YXYWI,1f(xj)	.
When evaluating the grid tensor for our chosen set of template vectors, i.e.	A(ycT,L,Θ)d1 ,...,dT
ycT,L,Θ (x(d1), . . . , x(dT)), we can substitute fj (x(i)) ≡ Fij, and thus
(WI,1f(x(d)))r = (WI,1FT)rd = (Z ∙ (FT)-1 FT)rd = Zrd.
Since We defined Z SuCh that for r ≥ min{R, M} Zrd = 0, and denoting R ≡ min{R, M} for brevity of
notation, the grid tensor takes the following form:
TR t	/ T/2 R t	∖	( T R t
A(Dd1,..,dτ = YXY Zrdj= (YXY Zrdj) I Y XY Zrdj
t=1 r=1 j=1	t=1 r=1 j=1	t=T/2+1 r=1 j=1
Where We split the product into tWo expressions, the left part that contains only the indices in the start
set S, i.e. d1 , . . . , dT/2, and the right part Which contains all external indices (in the start set S and the
end set E). Thus, under matricization W.r.t. the Start-End partition, the left part is mapped to a vector
a ≡ [qT=1 PR=I Qj=1 Zrdj	containing only non-zero entries per the definition of Z, and the right
part is mapped to a matrix B ≡ IQT=T/2+1 PR=I Qj=1 Zrdj	, Where each entry of u multiplies the
corresponding roW of B . This results in:
JA(yT,L,θ)dι,…,d"s,E = diag(a) ∙ B.
Since a contains only non-zero entries, diag(a) is of full rank, and so rank JA(ycT,L,Θ)d1 ,...,dT KS,E =
rank (B), leaving us to prove that rank (B) = TR/2	. For brevity of notation, We define N ≡ TR/2
To prove the above, it is sufficient to shoW that B can be Written as a sum of N rank-1 matrices, i.e. B =
PN=I u(i) Z V⑺,and that {u(i)}N=1 and {v(i) }iN=1 are tWo sets of linearly independent vectors. Indeed,
applying claim 6 on the entries of B, specified W.r.t. the roW (d1 , . . . , dT/2) and column (dT/2+1 , . . . , dT),
yields the folloWing form:
/	∖/	∖
R T/2	r T
B —	X	Y Y ZPr/2)	X	Y Y	ZPrT-j+1)
B(SE)=	ʌ	IlllZrdj	∙	ʌ	Illl	Zrdj
p(T∕2)∈states(R,T∕2)	r=1 j=1	(P(T/2-1),…,p(1) ) r=1 j = T∕2 + 1
∖	∈	∖∈trajectory (P(T/2))	)
where for all k, p(k) is JR-dimensional vector of non-negative integer numbers which sum to k, and we ex-
plicitly define states (R, T/2) and trajectory (P(T/2)) in claim 6, providing a softer more intuitive definition
20
Workshop track - ICLR 2018
hereinafter. states (R, T/2) can be viewed as the set of all possible states of a bucket containing T/2 balls of
R colors, where PT/2 for r ∈ [R] specifies the number of balls of the r'th color. trajectory (P(T/2)) can
be viewed as all possible trajectories from a given state to an empty bucket, i.e. (0, . . . , 0), where at each
step we remove a single ball from the bucket. We note that the number of all initial states of the bucket is
exactly ∣ states (R,T/2) ∣ = N ≡《T//). Moreover, since the expression in the left parentheses contains
solely indices from the start set S, i.e. d1 , . . . , dT/2, while the right contains solely indices from the end
set E, i.e. dT//+1 , . . . , dT, then each summand is in fact a rank-1 matrix. Specifically, it can be written as
up( T/2 Z vp( /2), where the entries of up( T/2 are represented by the expression in the left parentheses, and
those of vp(T//) by the expression in the right parentheses.
We prove that the set up(T//) ∈ RM T//	is linearly independent by arranging it as the
I	J P(T/2) ∈states(R,T//)
columns of the matrix U ∈ RMT// ×N, and showing that its rank equals to N. Specifically, we observe the
sub-matrix defined by the subset of the rows of U, such that we select the row d ≡ (d1 , . . . , dT//) if it holds
that ∀j, dj ≤ dj+1 . Note that there are exactly N such rows, similarly to the number of columns, which can be
intuitively understood since inside the imaginary bucket defining the columns there is no meaning of order in
the balls, and having imposed the restriction ∀j, dj ≤ dj +1 on the T/2 length tuple d, there is no longer a degree
of freedom to order the ‘colors’ in d, reducing the number of rows from MT// to N. Thus, in the resulting
sub-matrix, denoted by U ∈ RN×N, not only do the columns correspond to the vectors of states (R,T/2), but
also its rows, where the row specified by the tuple d, corresponds to the vector q(T/2) ∈ states (R,T/2), such
that for r ∈ [R] : q,T/2) ≡ |{j ∈ [T/2] |dj = r}| specifies the amount of repetitions of the number ('color’) r
in the given tuple.
Accordingly, for each element of U the following holds:
Uq(T//) ,P(T//)
R T/2
YY Zrpd(rTj//)
r=1 j=1
(Zij = 28％) ⇒
(definition of δij ) ⇒
(Grouping identical summands) ⇒
(q(T/2) ≡ |{j ∈ [T/2]|dj = r}|) ⇒
(qrT∕2)≡Ωr∕2qrτ/2' 0
∖prT*≡Ωr2pET* ) ⇒
ZPT=I PR=1 PrT/2)nrδrdj
zPT=1 Ωdj pdT/2)
zPR=1 Ωr∣{j∈[T∕2]∣dj =r}∣prT/2)
zPR=1 Ωrq*2)p*2)
ZDq(T/2),P(T/2)E.
Since the elements of U are polynomial in z, then according to lemma 1, it is sufficient to show that there
exists a single contributor to the determinant of U that has the highest degree of Z in order to ensure that
the matrix is fully ranked for all values of Z but a finite set. Observing the summands of the determinant,
i.e. ZPq(T/2)£states(R,T/2)(O( /2),σ(q )〉，where σ is a permutation on the rows of U, and noting that
states (R,T/2) is a set of non-negative numbers by definition, lemma 2 assures us the existence of a strictly
maximal contributor, satisfying the conditions of lemma 1.
We prove that the set vP(T/2) ∈ RMT/2	is linearly independent by arranging it as the
I	J p(T/2) ∈ states (RR, T/2)
columns of the matrix V ∈ RM T/2 ×N, and showing that its rank equals to N. As in the case of U, we select
the same sub-set of rows to form the sub-matrix V ∈ RN×N. We show that each of the diagonal elements of
V is a polynomial function whose degree is strictly larger than the degree of all other elements in its row. As
an immediate consequence, the product of the diagonal elements, i.e. QN=I Vii(Z), has degree strictly larger
than any other summand of the determinant det(V), and by employing lemma 1, V has full-rank for all values
21
Workshop track - ICLR 2018
of Z but a finite set. The degree of the polynomial function in each entry of V is given by:
deg (Vd,p(T∕2)
((τ∕2m1ax	(i))deg (YY YY	Zpj-j+1)
(p( /2-1) ,...,p(1))	r=1 j=T/2+1
∈trajectory (P(T/2))	'
max	deg (ZPT=T/2+1 Pr=I ω'PrT j+1)δrdj
(P(T/2-i),...,P(i))
∈tra jectory P(T/2)
T
max	X	Ωdj p(T-j+1)
(P(T/2-1) ,...,P(1)) j=T/2+1	j
∈trajectory(P(T/2))
The above can be formulated as the following combinatorial optimization problem. We are given an initial state
P(T/2) of the bucket of T/2 balls of R colors and a sequence of colors d = (dT/2+1,..., dτ). At time-step
j one ball is taken out of the bucket and yields a reward of ΩdjPdT-j+1), i.e. the number of remaining balls
of color dj times the weight Ωdj. Finally, deg(% p(T/2)) is the accumulated reward of the optimal strategy
of emptying the bucket. In lemma 3 We prove that there exists a value of Ω such that for every sequence of
colors d, i.e. a row of V, the maximal reward over all possible initial states is solely attained at the state q(T/2)
corresponding to d, i.e. q^T/2) = |{j ∈ {T/2 + 1,...,T}|dj = r}|. Hence, deg(V⅛) is indeed strictly larger
than the degree of all other elements in the i’th row.
Having proved that both U and V have rank N ≡	TR/2	for all values of Z but a finite set, we know there
exists a value of Z for which rank (B) = N, and the theorem follows.
B.3.3	Technical Lemmas and Claims
In this section we prove a series of useful technical lemmas, that we have employed in our proof for the case
of deep RACs, as described in section B.3.2. We begin by quoting a claim regarding the prevalence of the
maximal matrix rank for matrices whose entries are polynomial functions:
Claim 5. Let M, N, K ∈ N, 1 ≤ r ≤ min{M, N } and a polynomial mapping A : RK → RM ×N, i.e. for
every i ∈ [M] and j ∈ [N] it holds that Aij : RK → R is a polynomial function. If there exists a point
x ∈ RK s.t. rank(A(x)) ≥ r, then the set {x ∈ RK : rank(A(x)) < r} has zero measure (w.r.t. the Lebesgue
measure over RK).
Proof. See Sharir et al. (2016).	□
Claim 5 implies that it suffices to show a specific assignment of the recurrent network weights for which the
corresponding grid tensor matricization achieves a certain rank, in order to show this is a lower bound on its
rank for all configurations of the network weights but a set of Lebesgue measure zero. Essentially, this means
that it is enough to provide a specific assignment that achieves the required bound in theorem 1 in order to prove
the theorem. Next, we show that for a matrix with entries that are polynomials in x, if a single contributor to
the determinant has the highest degree of x, then the matrix is fully ranked for all values of x but a finite set:
Lemma 1. Let A ∈ RN×N be a matrix whose entries are polynomials in x ∈ R. In this case, its determinant
may be written as det(A) = σ∈S sgn(σ)pσ (x), where SN is the symmetric group on N elements and
pσ (x) are polynomials defined by pσ (x) ≡ QN=I Aiσ(i) (x), ∀σ ∈ Sn. Additionally, let there exist σ such that
deg(pσ (x)) > deg(pσ (x)) ∀σ = σ. Then, for all values of X but a finite set, A is fully ranked.
Proof. We show that in this case det(A), which is a polynomial in x by its definition, is not the zero poly-
nomial. Accordingly, det(A) = 0 for all values of X but a finite set. Denoting t ≡ deg(p亍(x)), since
t > deg(pσ (x)) ∀σ = σ, a monomial of the form C ∙ xt, c ∈ R \ {0} exists in pσ (x) and doesn,t exist in any
pσ (x), σ = σ. This implies that det(A) is not the zero polynomial, since its leading term has a non-vanishing
coefficient sgn(σ) ∙ c = 0, and the lemma follows from the basic identity: det(A) = 0 ^⇒ A is fully
ranked.	□
The above lemma assisted us in confirming that the assignment provided for the recurrent network weights
indeed achieves the required grid tensor matricization rank of TR/2 . The following lemma, establishes a
useful relation we refer to as the vector rearrangement inequality:
22
Workshop track - ICLR 2018
Lemma 2. Let {v(i) }N=ι	be a set of N different vectors in RR such that ∀i	∈	[N],	j ∈	[R]	:	Vji)	≥	0.	Then,
for all σ ∈ SN such that σ 6= IN , where SN is the symmetric group on N, it holds that:
v(σ(i)
N
)E < X v(i)2.
i=1
Proof. We rely on theorem 368 in Hardy et al. (1952), which implies that for a set of non-negative numbers
{a(1) , . . . , a(N ) } the following holds for all σ ∈ SN :
NN
X a(i)a(σ(i)) ≤X(a(i))2,
i=1	i=1
(14)
With equality obtained only for σ which upholds σ(i) = j ^⇒ a(i) = a(j). The above relation, referred to
as the rearrangement inequality, holds separately for each component j ∈ [R] of the given vectors:
NN
X vj(i)vj(σ(i)) ≤X(vj(i))2.
i=1	i=1
We now prove that for all σ ∈ SN such that σ =IN, ∃j ∈ [R] for which the above inequality is hard, i.e.:
NN
X Vji)Vjσ(i)) < X(v(i))2.	(15)
i=1	i=1
By contradiction, assume that ∃σ = IN for which ∀j ∈ [R]:
NN
X Vji)V严) = X(v(i))2.
i=1	i=1
From the conditions of achieving equality in the rearrangement inequality defined in eq. 14, it holds that ∀j ∈
[R] : Vjσ ⑶)=Vji), trivially entailing: v5(i)) = v(i).Thus, σ = IN would yield a contradiction to {v(i)}N=I
being a set of N different vectors in RR. Finally, the hard inequality of the lemma for σ 6= IN is implied from
eq. 15:
v(σ(i))
R
XVj(i)Vj(σ(i))
j=1
R
X
j=1
VjW心! <X
j=1
□
The vector rearrangement inequality in lemma 2, helped US ensure that our matrix of interest denoted U upholds
the conditions of lemma 1 and is thus fully ranked. Below, we show an identity that allowed us to make
combinatoric sense of a convoluted expression:
Claim 6. Let R and M be positive integers, let Z ∈ RR × M be a matrix, and let A be a tensor with T modes,
each of dimension M, defined by Ad1 ,...,dT ≡ QtT=T/2+1 PrR=1 Qtj=1 Zrdj, where d1 , . . . , dT ∈ [M]. Then,
the following identity holds:
A d = X	X	Y 信 ZprT/2) ʌ ( ITr	ZprfI) ʌ
Ad1,...,dT =	ʌ	IIl IIZrdj	Jl H	Zrdj	I，
p(T/2)	jp(T/2-1) ,...,p(1)) r=1 j=1	j=T/2+1
∈states(R,τ72) ∈trajectory (P(T/2))
where States(R, K)	≡ {p，*，	∈ (N ∪ {0})RI PR=1 pi = K}, and trajectory pjK)	≡
{(p(Kτ),..., pj1))∣∀k ∈ [K - 1], (pjk) ∈ states (R, k) ∧ ∀r ∈ 闿,0俨 ≤ prk+1))}. 8
Proof. We will prove the following more general identity by induction. For any k ∈ [T], define Ajdk),...,d ≡
QT=k PR=I Qj=ι Zrdj, then the following identity holds:
R /Li ,e ,	、\	/ T	,e	、\
Ajk)	= X	X	Y Y Z p(rT -k+1)	Y Z p(rT -j+1)
Ad1 ,...,dT =	Zrdj	Zrdj	.
P(T -k+1)	jP(T -k) ,...,P(1))	r=1 j=1	j=k
∈states(R,T — k+l) ∈trajectory(P(T-k + 1) )
8See section B.3.2 for a more intuitive definition of the sets states (R, K) and trajectory (P(T-k+1)
23
Workshop track - ICLR 2018
The above identity coincides with our claim for k = T/2 + 1 We begin with the base case of k = T , for
which the set states (R, 1) simply equals to the unit vectors of (N ∪ {0})R, i.e. for each SuCh P(I) there exists
r ∈ [R] such that pg1) = δrr ≡ ∖ r r I r. Thus, the following equalities hold:
I 0 r = r
R T	R R T	R T
X	YY Zrdjjj = XYY Zδrr = XY Zrdj= AdT) ...,近.
p(1)∈states(R,l) r=1 j=1	r=1 r=1 j=1	r=1 j = 1
By induction on k, we assume that the claim holds for A(k+1) and prove it on A(k) . First notice that we can
rewrite our claim for k < T as:
A(k)	= X	X	YR	Yk Z p(rT -k+j) !	YT Zp(rT-j+j)	(16)
Adj,...,dτ —匚	匚	U(UZrdj	Jl UZrdj	I ,	(16)
p(T -k+j)	(p(T-k),...,p(j))	r=1 j=1	j=k+1
∈states(R,T — k+l) ∈trajectory(P(T-k + j) )
where we simply moved the k’th term Zrpdr in the right product expression to the left product. We can also
can rewrite A(k) as a recursive formula:
CR k ∖	(R R k ∖
XY Z ,	A(k+1)	— XYY Z δ而	A(k + 1)
2^HZrdj	, Adj ,…,dτ = ʌ IlllZrdj	∙Adj,…,dτ
r=1j=1	)	∖亍=1r=1j=1	)
. Then, employing our induction assumption for A(k+1), results in:
CR R k	∖	R / k ,e ,、\	/ T ,e .,八、
XYY Zδrr I X	X	Y( Y zPrT )) I Y ZPr-j+j) I
Zrdj	Zrdj	Zrdj
亍=1 r=1 j=1	P p(T-k)	(P(T-k-j),…,p(j)) r=1 ∖j = 1	) ∖j = k + 1	)
∈states(R,T — k) ∈trajectory (P(T-kj )
=X X	X	Y /Y ZPrT-kj+δ"! Y Y zPrT-j+j)∖	(17)
=	Zrdj	l	Zrdj	I (17)
亍=1	P(T-k)	(P(T-k-j),…，p ⑴)r=1 ∖j=1	)∖j = k + 1	)
∈states(R,T — k) ∈trajectory (P(T-kj )
To prove that the right hand side of eq. 17 is equal to our alternative form of our claim given
by eq. 16, it is sufficient to show a bijective mapping from the terms in the sum of eq. 17, each
specified by a sequence (r, P(T-k),..., P(I)), where r ∈ [R], P(T-k) ∈ states (R,T — k), and
(p(T —k—1) , . . . , p(1)) ∈ trajectory p(T —k) , to the terms in the sum of eq. 16, each specified
by a similar sequence (P(T-k+1), P(T-k),..., P(I)), where P(T-k+1) ∈ states (R,T — k + 1) and
(P(T—k), . . . , P(1)) ∈ trajectory P(T —k+1) .
Let φ be a mapping such that(rr,P(T—k),...,P(1)) 7→φ (P(T—k+1),P(T—k),...,P(1)),wherep(rT—k+1) ≡
PrT-k) + δi φ is injective, because if φ(h, P(T-k,1),..., P(1,1)) = φ(r2, P(T-k,2),..., P(1,2)) then for all
j ∈ {1, . . . , T — k + 1} it holds that P(j,1) = P(j,2), and specifically for P(T —k+1,1) = P(T —k+1,2) it entails
that δf=jr = δf=2r, and thus r1 = r⅛. φ is surjective, because for any sequence (P(T-k+1), P(T-k),..., P(I)),
for which it holds that ∀j, P(j) ∈ (N ∪ {0})R, PrR=1 p(rj) = j, and ∀r,p(rj) ≤ p(rj+1), then it must also holds
thatPrT-k+1) — PrT-k) = δf=r for some r, since PR=I(PrT-k+1) — PrT-k)) = (T — k + 1) — (T — k) = 1
and every summand is a non-negative integer.
□
Finally, lemma 3 assists us in ensuring that our matrix of interest denoted Vr upholds the conditions of lemma 1
and is thus fully ranked:
Lemma 3. Let Ω ∈ R+ be a positive real number. For every P(T/2) ∈ states (R,T/2) (see definition in
claim 6) and every d = (dT/2+1 , . . . , dT) ∈ [Rr]T/2, where ∀j, dj ≤ dj+1, we define the following optimization
problem:
T
f (d, P(T/2))=( (T∕2max (j)) X	ΩdjPdT-j+1)
(P	,...,(PT/2)) j=T/2+1
∈trajectory P( / )
24
Workshop track - ICLR 2018
where trajectory (P(T/2)) is defined as in claim 6. Then, there exists Ω such thatfor every such d the maximal
value of f(d, p(T/2)) over all p(T/2) ∈ states (R, τ∕2) is strictly attained at P(T/2) defined by p，2) =
|{j ∈{T∕2 + 1,...,T}|dj = r}|.
Proof. We will prove the lemma by first considering a simple strategy for choosing the trajectory for the case of
f (d, p(T/2)), achieving a certain reward ρ*, and then showing that it is strictly larger than the rewards attained
for all of the possible trajectories of any other P(T/2) = p(T/2).
Our basic strategy is to always pick the ball of the lowest available color r. More specifically, if p1T/2) > 0,
then in the first PT/2 time-steps We remove balls of the color 1, in the process of which We accept a reward
of Ω1p1T/2) in the first time-step, Ω1(p1T/2) — 1) in the second time-step, and so on to a total reward of
Ω1 pp= 1 i. Then, we proceed to removing PT/2 balls of color 2, and so forth. This strategy will result in
an accumulated reward of:
r	PrT/2)
ρ ≡ X Ωr X i.
r=1	i=1
Next, we assume by contradiction that there exists P(T/2)= P(T/2) such that P ≡ f (d, P(T/2)) ≥ ρ*. We
show by induction that this implies ∀r,prT/2) ≥ PrT/2), which would result in a contradiction, since per our
assumption P(T/2) = P(T/2) this means that there is r such that PrT/2) > PrT/2), but since P(T/2), P(T/2) ∈
states (R, τ∕2) then the following contradiction arises T/2 = PR=I PrT/2) > PR=I PrT/2) = τ∕2. More
specifically, we show that our assumption entails that for all r starting with r = R and down to r = 1, it holds
that PrT/2) ≥ PrT/2).
Before we begin proving the induction, we choose a value for Ω that upholds Ω > (T/2)2 such that the
following condition holds: for any r ∈ [R], the corresponding weight for the color r, i.e. Ωr, is strictly greater
than Ωr-1(T∕2)2. Thus, adding the reward of even a single ball of color r is always preferable over any possible
amount of balls of color r0 < r .
We begin with the base case of r = R. If P(T/2) = 0 the claim is trivially satisfied. Otherwise, we assume by
contradiction that PRT/2) < PR/2). If PR/2) = 0, then the weight of the color R is not part of the total reward
ρ, and per our choice of Ω it must hold that ρ < ρ* since ρ* does include a term of Ωr by definition. Now,
we examine the last state of the trajectory P(1), where there is a single ball left in the bucket. Per our choice of
Ω, if PRP = 0, then once again ρ < ρ*, implying that PRP = 1. Following the same logic, for j ∈ [PRT/2)], it
holds that PR = j. Thus the total contribution of the R'th weight is at most:
ΩR
(
(6(T/2) _ TJ(T/2)
(PR	pR
∖
pRT/2) ʌ
)∙ PR/2) + X i .
(18)
This is because before spending all of the pR/2) balls of color R at the end, there are another (PR/2) — PRT/2))
time-steps at which we add to the reward a value of PRT/2). However, since eq. 18 is strictly less than the
R	方^(T/2)
corresponding contribution of ΩR in ρ*: ΩR ɪɪi i, then it follows that P < ρ*, in contradiction to our
assumption, which implies that to uphold the assumption the following must hold: PRT/2) ≥ PRT/2), proving the
induction base.
Assuming our induction hypothesis holds for all r0 > r, we show it also holds for r. Similar to our base case, if
PrT/2) = 0 then our claim is trivially satisfied, and likewise if PrT/2) = 0, hence it remains to show that the case
of PRT/2) < PRT/2) is not possible. First, according to our hypothesis, ∀r0 > r,PrT/2) ≥ PrT/2), and per our
_	0	.(T/2)
choice of Ω, the contributions to the reward of all of the weights for r0 > r, are atmost PZ==+ι Ωr Pi=I	i,
which is exactly equal to the corresponding contributions in ρ*. This means that per our choice of Ω it suffices
to show that the contributions originating in the color r are strictly less than the ones in ρ* to prove our
hypothesis. In this optimal setting, the state of the bucket at time-step j = T/2 — PR=「+1 PrT/2) must upholds
Prj) = PrT/2) for r0 > r, and zero otherwise. At this point, employing exactly the same logic as in our base
case, the total contribution to the reward of the weight for the r’th color is at most:
PrT/2) ∖
X i∣,
i=1
(19)
25
Workshop track - ICLR 2018
which is strictly less than the respective contribution in ρ*.
□
C Matricization Definition
Suppose A ∈ RM×…×M is a tensor of order T, and let (I, J) be a partition of [T], i.e. I and J are disjoint
subsets of [T] whose union gives [T]. The matricization of A w.r.t. the partition (I, J), denoted JAKI,J, is
the M|I|-by-M|J| matrix holding the entries of A such that Ad1...dT is placed in row index 1 + P|tI=|1 (dit -
1)M|I|-t and column index 1 + P|tJ=|1(djt - 1)M|J|-t.
26