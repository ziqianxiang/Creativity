Workshop track - ICLR 2018
Feature Incay for Representation Regulariza-
TION
Yuhui Yuan
Microsoft Research
yuyua@microsoft.com
Kuiyuan Yang
DeepMotion
kuiyuanyang@deepmotion.ai
Jianyuan Guo & Chao Zhang
Peking University
jyguo@pku.edu.cn
chzhang@cis.pku.edu.cn
Jingdong Wang
Microsoft Research
jingdw@microsoft.com
Ab stract
Softmax-based loss is widely used in deep learning for multi-class classification,
where each class is represented by a weight vector and each sample is represented
as a feature vector. Inspired by that weight decay is a common practice to regular-
ize the weight vectors, we investigate how to regularize the feature vectors since
representation is also tunable in deep learning.
One main observation is that elongating the feature norm of both correctly-
classified and mis-classified feature vectors improves learning: (1) increasing
the feature norm of correctly-classified examples enlarges the probability mar-
gin among different classes and ensures better generalization. (2) increasing the
feature norm of mis-classified examples can up-weight the contribution from hard
examples. Accordingly, we propose feature incay to regularize feature vectors
by encouraging larger feature norm. Extensive empirical results on MNIST, CI-
FAR10, CIFAR100 and LFW demonstrate the effectiveness of feature incay.
1 Introduction
Deep Neural Networks (DNNs) with softmax-based loss have achieved state-of-the-art performance
on numerous multi-class classification related tasks. In DNNs, both representations and classifiers
are learned within a unified network concurrently, where the final representation for a sample is the
feature vector f outputted from the penultimate layer, while the last layer outputs scores Zi = Wi ∙ f
for each category i, where wi is the weight vector for category i. Before defining the loss, the
scores for each category are normalized into probability via softmax function, i.e., Pi = eziz-.
jej
A well-trained DNN should output significant larger probability for the correct label than other
labels, which requires the score for the correct label is significantly larger than other labels. Since
Zi = Wi ∙ f = IlWikkfk cos(θ), where θ is the angle between Wi and f, the goal of significant
larger score for the correct label than other labels can be achieved by tuning kWik, kfk and θ. While
increasing the weight norm kWi k is constrained by weight decay for regularization, thus kf k and θ
become the two main factors for optimization. Although softmax loss can tune both of them, there
is still much room to improve either or both factors.
Recently, Liu et al. (2016), Wang et al. (2017) and Liu et al. (2017a) propose different approaches to
further optimize the factor of angular θ, and all of them have achieved obviously better performance.
To further emphasize the factor of angular, Ranjan et al. (2017) and Liu et al. (2017c) propose to use
normalized feature vectors for softmax loss where the factor of feature norm is totally ignored. In this
work, we make the effort to optimize the feature norm. Firstly, we analyze the connections between
the feature norm and the classification accuracy within softmax loss, they are highly correlated
as illustrated in Figure 1. Features with larger norm tend to be correctly classified with higher
probability.
1
Workshop track - ICLR 2018
18
Bboz s£3
(求)£eu AUejnUueas
3244
99-56
1130
100.00
411
features' Snorm
Figure 1: Test accuracy and number of samples vs Features’ L2-norm on CIFAR10 test set, and the model
is trained with softmax loss. The test accuracy increases monotonically with the feature norm. e.g., the test
accuracy reaches 100% for samples with feature norm exceeding 400.
6000
5000
4000 S
O.
3000 5
ω
2000
IOOO
CIFAR10
ιoαoα uαoα aoaoa 25βoa »000
Iwraa8
(b)
CIFAR1O
ιooaa UMO aooaa zs«o »000
ltwwton
(a)
(c)
O
Figure 2: Comparison of Softmax loss with Softmax loss + Feature incay(i.e., Reciprocal Norm Loss, we will
define it in Section 3) on test set of CIFAR10. (a) Average L2 -norm of feature vectors vs Iterations, (b) Softmax
loss vs Iterations, (c) Top-1 accuracy vs iterations. Figure (d), (e) and (f) illustrate different approaches using
binary classification as an example, where yellow points are samples of class1 and green points are samples
of class2. The black dashed line represents the decision boundary between the two classes. The two blue
dashed lines represent the hyperplanes that pass the points with minimal distances to the decision boundary.
The numbers 1-6 represent the order of L2-norm of the points within each class. W1 and W2 represent the
weight vectors. (d) Feature embedding of Softmax loss. (e) Feature embedding of Large-margin Softmax loss.
The purple arrows represent the additional angle constraints compared with Softmax loss. (f) Large-margin
Softmax loss + Feature incay. The red arrows correspond to the constraints from feature incay. The margin
between class1 and class2 increases from left to right.
2
Workshop track - ICLR 2018
Here we propose to optimize the feature norm by augmenting the softmax loss with feature incay.
In contrast to weight decay that shrinks weight vectors to be of small norm, feature incay tends
to stretch out the feature vectors. From the computational perspective, larger feature norm results
in larger score differences among categories, which can better separate the categories. From the
perspective of pattern detection, larger feature norm encourages model to learn and detect more
prominent patterns.
Figure 2(a), 2(b) and 2(c) show the results of comparison experiments by adding feature incay
to softmax loss, where feature incay achieves larger feature norm, smaller loss value and higher
accuracy on test set. The geometric interpretation of feature incay is illustrated in Figure 2(d),
2(e) and 2(f), we can achieve the larger inter-class separability by explicitly optimizing both the
kf k and θ compared with the other methods. Besides, the proposed feature incay (implemented as
Reciprocal Norm Loss) is designed to increase the feature norm adaptively according to the original
feature norm, which can also help reduce the intra-class variances as discussed in section 3.4.
Our main contributions are summarized as follows:
•	The benefits of increasing the feature norm for both correctly-classified examples and mis-
classified examples are analyzed theoretically.
•	We verify the effectiveness of feature incay empirically on four widely used classification
datasets(i.e., MNIST, CIFAR10, CIFAR100 and LFW) using various network architectures.
With feature incay, we achieve consistent performance improvements on all of them.
•	We also conduct experiments to study the weights’ distribution w/ feature incay and w/o
feature incay and investigate where the improvements come from.
2	Related work
Large-margin Softmax Loss. Liu et al. (2016) proposed to improve softmax loss by incorporating
an adjustable margin m multiplying the angle between a feature vector and the corresponding weight
vector. Compared with the softmax loss, it pays more attention to the angular decision margin
between classes as illustrated in Figure 2(e). Large-margin softmax loss appends stronger constraint
to the angular, while feature incay considers constraint to feature norm. As illustrated in Figure 2(f),
feature incay is orthogonal to large-margin softmax loss.
Center Loss. Wen et al. (2016) presented the center loss to learn centers for deep features of each
class and penalize the distances between the deep features and their corresponding class centers.
Combining softmax loss with center loss actually uses two sets of classifiers, where representation
is learned based on both the inner product to weight vector and the Euclidean distance to class center.
The added center loss helps minimize the intra-class distances also by influencing the feature norm,
namely, small feature norm will be increased and large feature norm will be decreased during the
process of pushing feature vectors to class centers. Different from center loss, feature incay also
increases the large feature norm instead of penalizing feature vectors with large norm as center loss.
Weight/Feature Normalization. Inspired by the fact that feature normalization before calculating
the sample distances usually achieves better performance for retrieval tasks, Ranjan et al. (2017)
proposed to use normalized feature vectors in softmax loss during training, thus the feature norm
has no effect on softmax loss and angle is the main factor to be optimized. Congenerous cosine
loss(Liu et al. (2017b)), NormFace(Wang et al. (2017)), and cosine normalization(Chunjie et al.
(2017)) take a step further to normalize the weight vectors which replace inner product with cosine
similarity within softmax loss, and only optimize the factor of angle. Recently, Wang et al. (2018b),
Wang et al. (2018a) and Deng et al. (2018) all proposed to add the angle margin after normalizing
both the weight vectors and feature vectors during nearly the same period. Although normalization
mechanism achieves much lower intra-class angular variability by emphasizing more on the angle
during training, they ignore that feature norm is another useful factor worth to optimize.
Feature Scale. COCO(Liu et al. (2017c)) and L2-softmax(Ranjan et al. (2017)) are the most rele-
vant to our work, and especially COCO is a concurrent work. Here we compare our method with
COCO theoretically, Relations: Both COCO and feature incay increase the feature norm, which is
the common reason for the performance improvement. Differences: COCO is optimizing the fea-
ture embedding on a hypersphere while feature incay optimizes feature embedding located between
two hyperspheres with different radiuses(Property 3 in section 3.4). COCO proposes a novel con-
3
Workshop track - ICLR 2018
generous cosine loss while feature incay uses the original softmax loss. Feature incay is simpler
than COCO and it can be easily plugged into almost all the related work that use softmax loss.
Empirically comparision is provided in section 4.2.
3	Our work
3.1 Revisiting softmax loss
Let X = {(xi, yi)}iN=1 be the training set contains N samples, where xi is the raw input to the DNN,
yi ∈ {1, 2,…，K} is the class label that supervises the output of the DNN. Denote f as the feature
vector for xi learned by the DNN, {wj}jK=1 represent weight vectors for the K categories. Then,
softmax loss is defined as,
L _	1 G l (	ewTi fi+byi	ʌ
LSOftmax = -N ʌ/ɪog IPK[gWTfi +bj)
(1)
Bias terms are ignored following the discussions in recent works Liu et al. (2016) and Wang et al.
(2017). Denote the angle between wj and fi as θwj,fi , the inner product between wj and fi can be
rewritten as
wjT fi = kwjkkfik cos(θwj,fi)
(2)
By combining the above two equations, we get
Lsoftmax
ekwyi kkfik cos(θwyi,fi)
(PK 1 ekwj kkfik cos(θWj,fi )
(3)
3.2 Feature norm matters
Here we will illustrate how feature norm influences the softmax loss from two aspects: (1) increasing
the feature norm of correctly-classified examples enlarges the probability margin among different
categories. (2) increasing the feature norm of mis-classified examples can up-weight the gradients
of weight vectors, especially the weight vectors of the correct category and mis-classified category.
The first property is similar to the proposition proved by Wang et al. (2017), which states that soft-
max loss always encourages features of the correctly-classified examples to have larger magnitudes.
Property 1 [Feature Norm Matters for Correctly-classified Examples] Suppose weight vectors and
directions of the feature vectors are fixed, increasing of the feature norm of correctly-classified
examples can enlarge the probability margin among different categories.
Proof. Assuming fi is correctly-classified into category yi , here we define the probability margin
between category yi and any other category j(j 6= yi) as:
ewyTi fi - ewjT fi
Myij (fi)= Pk-Pj= PKTew T f
1 - e(wjT -wyTi )fi
PKd e(wT-wTi )fi
(4)
Recall that when fi is correctly classified, we have wyTi fi > wjT fi for j 6= yi . Then, for any t > 0,
we have,
e(wjT -wyTi )((1+t)fi) < e(wjT -wyTi )fi
(5)
1 - e(wjT -wyTi )(1+t)fi > 1 - e(wjT -wyTi )fi
(6)
4
Workshop track - ICLR 2018
Figure 3: Distribution of features, norm over correctly-classified / mis-classified examples.
According to the above inequations, we have
Myi,j((1+t)fi)>Myi,j(fi)
(7)
In summary, the probability margin increases by increasing the feature norm and larger probability
margin ensures better generalization.
Property 2 [Feature Norm Matters for Mis-classified Examples] Assuming the feature vector fi is
mis-classified into category k(k 6= yi), increasing the feature norm of fi can up-weight the gradients
of the weight vector wk.
Proof. According to definition of softmax loss in Eq.(3), the gradient with respect to weight vector
Wj (j = 1,…，K)is:
d⅛詈=N X (Pji-Mi”
j	i=1
(8)
where Pji =
ewTf
PK=1 ewTf ,
and h(i) is an indicator function, h(i) = 1 if yi = j otherwise h(i) = 0.
Specifically, we analyze how the value of dLWtmax changes, which contains two factors: (1)(Pk -
h(i)) increases with larger kfik when fi is mis-classified as category k. By increasing kfik we will
always have larger Pki and h(i) = 0 when computing Wk . So the absolute value of (Pki - h(i)) is
increased. (2) kfi k is increased. So increasing the feature norm kfi k can up-weight the gradients for
Wk . Especially for hard examples fi with Pki close to 1 and Pyii close to 0, (Pyii - h(i)) is closer to
-1 by increasing kfik, thus increasing kfik can also increase the gradients for weight vector Wyi.
We investigate the distribution of feature norm over correctly-classified and mis-classified examples
on CIFAR10 in Figure3, and the gradients contributed by the mis-classified features is suppressed
due to most of the mis-classified examples are of small feature norm. Thus it is necessary to increase
the feature norm of mis-classified examples. By optimizing the feature norm of mis-classified vec-
tors, the errors can be fixed as illustrated in the fifth column of Table 6. (e.g., 336 examples from
the test set of CIFAR10 become correctly-classified by increasing their average feature norm from
169.3 to 195.9)
Besides, the proposed feature incay increases the originally small feature norm faster than the orig-
inally large feature norm. Thus the feature norm of the mis-classified examples will increase faster
than the feature norm of the correctly-classified examples, which can further up-weight the contri-
bution of the hard examples.
3.3	Reciprocal Norm Loss
The superiorities of increasing feature norm have been investigated in the previous subsection. Here
we explore several methods that increase the feature norm end-to-end by penalizing an additional
term, such as -IIfk2, -log(kf∣∣2) and k^. The comparison analysis is provided in supplemen-
tary. Specifically, We choose fp and propose the Reciprocal Norm Loss, where the definition of
5
Workshop track - ICLR 2018
W
-►
2	56
Figure 4: The original data distribution is on the left of the black dashed line and the data distribution updated
according to the Reciprocal Norm Loss is on the right. The numbers 1-6 represent that the points are of
increasing feature norm. The black point represents the original point. The lengths of the green bidirectional
arrows represent the maximal distance in the direction of the weight vectors within all the points of one class.
The purple bidirectional arrow means the minimal distance to origin, which is equal to the minimal feature
norm. The red arrows represent the gradients update along the directions of the weight vectors computed with
the Reciprocal Norm Loss, while the lengths represent the magnitude of the gradients.
Figure 5: Illustration in 2-dimensional space. α and β are the radiuses of the two circles, which represent the
lower bound and upper bound of the feature norm.
Reciprocal Norm Loss is,
1N
L = - N X log
i=1
^^^^{^^^^~
softmax loss
ewjT fi
T
yi
K	1N 1
)+ μ X kwk k2 + λN X f下
k=1	i=1 i
一 、------{----} 、-------{-------}
weight decay	feature incay
(9)
where is a small positive value to prevent dividing by value close to zero, λ is a hyper-parameter
used to control the influence of feature incay. The whole loss function consists of three parts:
softmax loss, weight decay and feature incay, of which the later two items prefer small weight norm
and large feature norm. Feature incay can be considered during the whole training procedure or after
some training iterations with softmax loss. Large feature norm brings large inter-class separability
under the constraint of small weight norm, otherwise large feature norm can be trivially achieved by
increasing the weight norm.
The simple reciprocal form of feature norm has an important property that moves feature vec-
tors with small norm faster along weight vectors than feature vectors with large norm and results
intra-class compactness, which is illustrated in Figure 4. Specifically, denote feature incay of fi as
F (fi) = ifj+e, the gradient is f = -(£售：)2. For any two feature vectors fp and fq satisfying
∣∣fqk > ∣∣fpk, we always have ∣∣ 箓k > k f k∙ Thus vectors with small norm increase fast along
their original directions while feature vectors with large norm increase slowly along their original
directions.
6
Workshop track - ICLR 2018
Table 1： ErrorRates (%)onMNIST/CIFAR1O∕CIFAR1OO.
Method	MNIST	CIFAR10	CIFAR100
CNN(Jarrett et al. (2009))	0.53	N/A	N/A
DropConnect(Wan et al. (2013))	0.57	9.41	N/A
FitNet(Romero et al. (2014))	0.51	N/A	35.04
NiN(Lin et al. (2013))	0.47	10.47	35.68
Maxout(Goodfellow et al. (2013))	0.45	11.68	38.57
DSN(Lee et al. (2015))	0.39	9.69	34.57
R-CNN(Liang & Hu (2015))	0.31	8.69	31.75
GenPool(Lee et al. (2016))	0.31	7.62	32.37
Hinge Loss(Liu et al. (2016))	0.47	9.91	33.10
Softmax(Liu et al. (2016))	0.40	9.05	32.74
L-Softmax(Liu et al. (2016))	0.31	7.58	29.53
Softmax	0.35	8.59	32.36
RN + Softmax	0.31	7.84	31.76
L-Softmax	0.25	7.56	29.95
RN + L-Softmax	0.29	7.22	29.18
3.4	Geometric Interpretation of Reciprocal Norm Loss
As stated in the previous subsection, Reciprocal Norm Loss can increase feature norm adaptively
to decrease the intra-class variance. Here we prove that there exists an upper bound for the ratio
between the feature norm’s upper bound and the feature norm’s lower bound.
Property 3 [Feature Norm Bound] Given (a) the angle between any feature vector fi and its cor-
responding weight vector wyi is zero, (b) the angles between any two neighbor weight vectors of
different classes are θ, we have (1) the minimal inter-class distance is 2α sin( 2), where a is lower
bound of feature norm, (2) to ensure the maximal intra-class distance is smaller than the minimal
inter-class distance, the upper bound offeature norm is 3α, especially when K < 2D(K is the num-
ber ofclasses and D is thefeatures, dimension), the upper bound in the range of [(1 + √2)ɑ, 3α].
Proof. Figure 5 shows the 2-dimensional case satisfying conditions (a) and (b), where black arrows
named with Wi represent weight vectors for each class. As we have assumed that all feature vectors
are lying on the directions of their corresponding Wi, blue circle and purple circle denote the lower
bound and upper bound of feature norm respectively. Thus the maximal intra-class distance is
d2 = β - α and the minimal inter-class distance is di = 2α sin(2). To ensure minimal inter-class
distance is larger than intra-class distance, i.e., di = 2α sin( 2) > d2 = β - α, which requires
β < 2α sin( 2) + α 6 3α. Thus 3a is the general upper bound for the feature norm.
Especially when K < 2D, according to the Lemma(refer to supplementary), we can ensure that
θ ≥ 90。. Besides, the angle between any two vectors is smaller than 180。. Then 2 ∈ [45。，900]
and sin(2) is a monotonously increasing function within the range [45。，90。]. Based on sin(2) ∈
[4,1], the upper bound of L2-norm of feature vectors is in the range [(1 + √2)ɑ, 3α].	□
According to the Property 3, the ratio of the upper bound β and the lower bound α is bounded( α ≤
3). We can estimate the upper bound of the feature norm based on the original features’ L2-norm.
In our experiments, we choose the average L2-norm as the lower bound to avoid the influence of
outliers. For example, if the average L2-norm on CIFAR10 is 200, We will choose 483 ≈ (1 +
√2) X 200 as the threshold to control the feature incay. The feature incay for features with feature
norm exceeding 483 will be set as 0.
4 Experiments
In this section, we verify the effectiveness of feature incay through empirical experiments.
4.1	Experimental Settings
We evaluate feature incay on four datasets, i.e., MNIST, CIFAR10, CIFAR100 and LFW. MNIST
consists of 60,000 training images and 10,000 test images from 10 handwritten digits, both CIFAR10
7
Workshop track - ICLR 2018
Table 2: Face verification accuracy (%) on LFW.
Method	Data	Network	mAcc
FaceNet(Schroff et al. (2015))	200M	N/A =	99.65
DeepID2(Sun et al. (2015))	300K	N/A	99.47
CenterFace(Wen et al. (2016))	700K	N/A	99.28
L-Softmax(Liu et al. (2016))	CASIA-WebFace	SphereNet-64	99.10
A-Softmax(Liu et al. (2017a))	CASIA-WebFace	SphereNet-20	99.26
A-Softmax(Liu et al. (2017a))	CASIA-WebFace	SphereNet-64	99.42
COCO(Liu et al. (2017c))	MS-1M	ResNet-101	99.78
COCO	CASIA-WebFacr	SPhereNet-2O=	98.90
RN + COCO	CASIA-WebFace	SPhereNet-20	99.02
L-Softmax	CASIA-WebFacr	SPhereNet-20	99.03
RN + L-Softmax	CASIA-WebFace	SPhereNet-20	99.18
A-Softmax	CASIA-WebFacr	SphereNet-64	99.42
RN + A-Softmax	CASIA-WebFaCe	SphereNet-64	99.47
Table 3: ACCuracy(%) Comparison of different λ.
Method	λ=1	λ = 0.1	λ = 0.01	λ = 0
RN + Softmax	91.68	92.16	91.96	91.41
RN + L-Softmax	92.40	92.78	92.65	92.44
and CIFAR100 contain 50,000 training images and 10,000 test images from 10 object categories
and 100 object categories respectively. LFW(Huang et al. (2007)) dataset contains 13,233 face
images from 5749 different identities, 6000 face pairs are used as test set following the standard
protocol. Images are subtracted by mean image Images and randomly flipped horizontally for data
augmentation. The specific network architectures are detailed in supplementary. We adopt Caffe
framework(Jia et al. (2014)) for training and testing. The weight μ for weight decay is 0.0005 in
all experiments. We choose different weight λ in different experiments, i.e., 1.0, 0.1 or 0.01. The
momentum is 0.9, and the learning rate starts from 0.1 and is divided by a factor of 10 three times
when the training error stops decreasing.
4.2	Comparison Experiments
Feature incay is added to Sofmax, L-Softmax and A-Softmax to compare with state-of-the-art ap-
proaches, which is represented with RN(Reciprocal Norm loss) plus the baseline method. e.g., RN +
Softmax means combining the feature incay with Softmax loss. The reproduced results by Softmax,
L-Softmax and A-Softmax following Liu et al. (2016; 2017a) are the same as or slightly better than
the referred numbers in general. Table 1 reports the error rates of compared approaches and our
method on MNIST, CIFAR10 and CIFAR100. It can be concluded that feature incay can consis-
tently improve over Softmax and L-Softmax on CIFAR10/CIFAR100. For example, RN decreases
the error rate of Softmax from 8.59% to 7.84% while L-Softmax achieves 7.56% on CIFAR10,
which demonstrates both of them are better than Softmax. By combining RN and L-Sofmax, we
achieve better result 7.22%, which means that they are reciprocal. However, RN + L-Softmax is
slightly worse than L-Softmax on MNIST, our hypothesis is that the performance on MNIST is
already saturate and difficult to improve further.
To further verify our method’s effectiveness on more challenging datasets, we test RN + COCO, RN
+ L-Softmax and RN + A-Softmax on LFW and achieve competitive performance with SphereNet-
20 or SphereNet-64. The results are illustrated in Table 2. The reproduced results with L-Softmax
and A-Softmax are comparable while the reproduced COCO result is not as good due to both the
training dataset and network structure are set different. With feature incay, RN + L-Softmax im-
proves the L-Softmax from 99.03% to 99.18%, RN + A-Softmax improves the A-Softmax from
99.42% to 99.47% and RN + COCO improves the COCO from 98.90% to 99.02%. Thus feature
incay can even promote both A-Softmax and COCO with normalized features by elongating the
features before normalization.
Table 4: Accuracy(%) Comparison by Scaling the Feature.
Scale IIl 2∣ 4∣6∣8 ∣> 10
SOftmax I 91.41 ∣ 90.90 ∣ 90.18 ∣ 90.91 ∣ 90.97 ∣ NAN
8
Workshop track - ICLR 2018
Softmax
RN + Softmax
Softmax
RN + Softmax
(a) Average L2-norm on test set.
(b) Accuracy on test set.
Figure 6: Histograms of Average L2-norm and Accuracy on CIFAR10, (a) the feature norm is increased over
all the classes. e.g., the feature norm increases from 230 to 282 for class airplane. (b) the test accuracy is
boosted over all the classes. e.g., the test accuracy increases from 95.4 to 96.0 for class ship.
Table 5: Accuracy(%) Comparison of different μ on CIFAR10. μ = 0.0005 is the best choice among
all of them, thus We choose this setting in all the other experiments.
Method	μ = 0.00001	μ = 0.00005	μ = 0.0005	μ = 0.005
Accuracy	89.21 二	89.35	=	91.41	91.16
Average Feature Norm	185.8	202.2	246.2	231.3
4.3	Effects of λ
Here we conduct experiments on CIFAR10 to investigate the influence of hyper-parameter λ. Results
are illustrated in Table 3. Both RN + Softmax and RN + L-Softmax achieve consistent improvement
for all different λ except on RN + L-Softmax when λ = 1, which is caused by that the loss item of
L-Softmax can be smaller than the loss item of feature incay. To balance the L-Softmax and feature
incay for training, λ should be set to a relatively small weight.
4.4	Simply Scale the Feature
To verify whether it is possible to improve the performance by simply rescaling the features before
computing softmax loss, we conduct extensive experiments on CIFAR10 and present the related
results in Table 4. Simply scaling the features fails to improve the performance, where too large
value can cause the network fails to converge. For example, when we scale the features more than
10 times, the softmax loss will explode during training(NAN represents the softmax loss is exploded
during training).
4.5	Effects of Weight Decay
Here we also investigate the influence of the weigth decay on the classification accuracy and feature
norm, where we conduct experiments considering only Softmax loss for fairness. The results are
reported in Table 5, where we find that it fails to improve neither accuracy nor feature norm by
Table 6: Average L2-norm and the corresponding number of examples on test set of CIFAR10, e.g., 253.2 /
9141 represents 9141 examples are correctly classified and their average L2-norm is 253.2. Error-fixed repre-
sents the examples mis-classified by Softmax but correctly-classified by RN + Softmax. Error-added represents
the examples correctly-classified by Softmax but mis-classified by RN + Softmax.
Method	Accuracy	correctly-classified	Mis-classified	Error-fixed	Error-added
Softmax	91.41	253.2/9141	167.5 / 859=	169.3 /336	161.6/261 二
RN + Softmax	92.16	308.3/9216	187.2/784	195.9 / 336	187.3/261
9
Workshop track - ICLR 2018
simply increasing the weight decay or decreasing the weight decay. Thus simply changing the
weight decay leads to either underfitting or overfitting.
4.6	Effects on Weights’ Distribution
To avoid overfitting, we consider weight decay in all experiments. However, one main concern is the
side effect of feature incay may increase the magnitude of the shallow layers. We plot the weights’
distributions of initial state, difference choices of weight decay within Softmax and RN + Softmax in
Figure 7(refer to supplementary for details of the network architectures settings). It is interesting to
find that the weights of the first convolution layer(Conv0) becomes even sparser with feature incay.
Overall, the influence of feature incay is limited due to the constraint from weight decay. So the
feature incay enlarges the feature norm without harming the magnitude of weights from previous
layers and will not lead to overfitting. Besides, we can also observe that different weight decay has
big impact on the final weight distribution, such as larger Weigth decay(e.g., μ = 0.005) results
in more weights are constrained to zero, which may lead to underfitting according to their final
classification performances. More details are illustrated in Figure 7.
4.7	Result Analysis
By analyzing the features’ L2-norm and classification accuracy on CIFAR10 for data of each class,
we investigate where the concrete improvements come from. Table 6 reports the related details and
we find 336 examples that are mis-classified by Softmax but correctly classified by RN + Softmax.
However, 261 mis-classified examples are further introduced by RN + Softmax, which limits the
final performance improvement.
We also plot the histograms of average L2-norm and accuracy for Softmax and Softmax + RN. The
details are illustrated in Figure 6. With feature incay, the feature norm is enlarged and the accuracy
is on all ten classes.
5	Conclusions
In this paper, we propose the feature incay implemented as Reciprocal Norm Loss to increase the
feature norm. Based on the theoretical analysis of the feature norm, the Reciprocal Norm Loss
enlarges the probability margin among different categories and up-weight the contribution from hard
examples by managing the feature norm of both the correctly-classified and mis-classified feature
vectors. Extensive experiments on MNIST, CIFAR10, CIFAR100 and LFW verify the effectiveness
of our method.
6	Supplementary
Table 7: The CNN architectures used for MNIST/CIFAR10/CIFAR100. The count of the Conv1.x, Conv2.x
and Conv3.x closely follows the settings in Liu et al. (2016). All the pooling layers are with window size 2 × 2
and stride of 2._________________________________________________________
Layer	MNIST	CIFAR10	CIFAR100
Conv0.x	[3 × 3, 64] × 1	[3 X 3, 64] X 1	[3 X 3,128] X 1
Conv1.x	[3 × 3, 64] × 3	[3 X 3, 64] X 4	[3 X 3,128] X 4
Conv2.x	[3 × 3, 64] × 3	[3 X 3,128] X 4	[3 X 3, 256] X 4
Conv3.x	[3 X 3, 64] X 3	[3 X 3, 256] X 4	[3 X 3, 512] X 4
Fully Connected	256	—	512	512
6.1	LEMMA
Here is the Lemma proposed by Ranjan et al. (2017) and is used in the proof of Property 3 and
Property 4.
Lemma When the number of classes K is smaller than twice the feature dimension D, we can
distribute the classes on a hypersphere of dimension D such that any two class weight vectors are
at least 90° apart.
10
Workshop track - ICLR 2018
ConvO
(a) Conv0.x
(b) Conv1.x	(c) Conv2.x
0.10-
ConV3
0 08
-Oa -βJ>2 -OΛ1 SM OΛ1 002 <M>3 SM
XIl-i3Rd
OOO-
-0.04 -0.03 -0.02 -0.01 OOO 0.01	0.02	0.03	0.04
weight
(d) Conv3.x	(e) FC
Figure 7: Histograms for Weights’ distribution of different layers from model trained on CIFAR10. Here we
consider five methods: (1) Weights’ distribution after Initialization. (2) Weights’ distribution after trained with
Softmax loss where the weight decay chooses μ = 0.005(ClassifiCation accuracy is 91.16%). (3) Weights'
distribution after trained with Softmax loss where the weight decay chooses μ = 0.00005(ClaSSifiCation accu-
racy is 89.35%). (4) Weights’ distribution after trained with Softmax loss where the weight decay chooses
μ = 0.0005(classification accuracy is 91.40%). (5) Weights, distribution after trained with RN + Soft-
max(classification accuracy is 92.16%). The magnitude of the weight parameters is only slightly influenced
by the feature incay. Besides, Softmax μ = 0.005 represents larger weigth decay while Softmax μ = 0.00005
represents smaller weight decay compared with the standard settings.(e.g., μ = 0.0005). The weights parame-
ters are very sparse within Softmax μ = 0.005 while very dense within Softmax μ = 0.00005, which induce
either underfitting or overfitting.
6.2 Feature norm within Softmax loss
Here we mainly want to present you that the feature norm of correctly classifed examples fails to be
increased within softmax loss, while feature incay can increase the feature norm consistently.
Property 4 [Feature Norm in Softmax Loss]For any feature vector fi, if Pyi → 1 and Pji → 0(∀j 6=
yi), then dLfmx → 0.
Proof. According to definition of softmax loss in Eq.(3), the gradient of feature vector fi is:
dL∂fmx = Nf + XX Pjwj)	(10)
i	j=1
where the	Pj	= —e	WfT	. When	Pj	→ 1 and Pj	→ 0(∀j	= yj, dLftmax	→ 0. That is
j	PkK=1 ewk fi	yi	j	∂fi
after a training sample is confidently classified correctly, it will have no contribution to its own
representation learning. For example, suppose ∣∣wj-k = 1, θwy.,f = 0 and θwj,f ≥ 90°(Vj = yi)
according the Lemma, then kwyikkfik cos(θwyi,fi) = kfik, kwjkkfik cos(θwj,fi) < 0,∀j 6= yi.
Putting together, we have Pyi ≥ £1肃+K-ι, for a modest number of categories say K = 10,
Pyi > 0.999 when ∣fi∣ = 10.
6.3	Functional format of feature incay
The choice of the functional format of feature incay is important. Here we mainly analyze three
different choices.
11
Workshop track - ICLR 2018
Table 8: Results of comparison experiments for three feature incay on CIFAR10, where LN repre-
Sents the linear form, LogN represents the log form and RN represents the reciprocal form.
Method	Softmax	Softmax + LN	Softmax + LogN	Softmax + RN
Accuracy	91.41	91.86 —	91.83	92.16 —
• Linear.	F (kfk2) = Tfk2	∂∂F = -2f
• Log.	F (kfk2) = -log(kf k2) 券=- f
• Reciprocal.	F (kf∣∣2) = fɪ
∂F _	2f
f = - f2f2
Here we mainly analyze the differences of the above three functions by investigating the relationship
between the gradients and the original feature norm. Assuming that we have two features f1 and f2
satisfying ∣∣f2k > ∣∣fιk. For the linear function case, we have ∣∣ f ∣∣>k f ∣∣. Then, the intra-
class variance will be increased as the distance between f1 and f2 increases with each gradient
update. Besides, the gradients of the linear function have the same magnitude with the feature
itself, such large gradients update can lead to explosion during the training. For the log function
case, We have f = fɪ = f^, where U is the unit vector with feature norm equals 1. Thus
Il ∂F ∣∣>∣ f ∣∣. The gradients within reciprocal function is also that ∣∣ f ∣∣>∣ f ∣∣ always
holds once ∣f2 ∣ > ∣f1 ∣. Although both log function and reciprocal function increase the features
with small feature norm faster than the features with larger feature norm, we find the performance
of reciprocal function is better. In summary, reciprocal function can increase the overall feature
norm and increase the intra-class similarity simultaneously, where the intra-class similarity along
the direction of the weight vectors can be decreased with the log function. We choose the reciprocal
function in all of our experiments.
Table 8 reports the classification accuracies adopting the three different considered feature incay.
The superiority of Softmax + RN over Softmax + LN and Softmax + LogN is well illustrated, where
Softmax + RN can achieve better intra-class similarity according to the above analysis.
6.4	CNN Architectures Settings
For LFW, we adopt 20-layer/64-layer SphereNet following the same settings in Liu et al. (2017a).
We modify the network settings for MNIST/CIFAR10/CIFAR100 based on the previous work(Liu
et al. (2016)) and list them in Table 7.
6.5	Experimental Analysis
Figure 8(a), 8(b) and 8(c) show the accuracy, average feature norm and softmax loss during training
by using λ = 1, 0.1, 0.01 respectively on CIFAR10. Feature incay achieves better or comparable
accuracy compared with softmax loss under a wide range of λ, and results in larger feature norm
on both training and test set. All methods achieve close to zero softmax loss on training set, while
feature incay ensures lower softmax loss on test set. Figure 8(d) shows the accuracy vs iteration
number on CIFAR100, which is similar to the results on CIFAR10.
References
Luo Chunjie, Yang Qiang, et al. Cosine normalization: Using cosine similarity instead of dot product
in neural networks. arXiv preprint arXiv:1702.05870, 2017.
Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep
face recognition. arXiv preprint arXiv:1801.07698, 2018.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. arXiv preprint arXiv:1302.4389, 2013.
Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild:
A database for studying face recognition in unconstrained environments. Technical Report 07-49,
University of Massachusetts, Amherst, October 2007.
12
Workshop track - ICLR 2018
0.95
ClFARIO
0.90
5
8
X32nuw
0∙80F ∕⅛f	1	|»—► Softmax	∣-
/	∖	∙——■	RN +	Softmax λ=ι
*	U	®—®	RN +	Softmax λ=0.ι
1	_*	RN +	Softmax λ = 0.0i
0.75 U--------ι--------1---------1---------1---------1---------
O 5000 IOOOO 15000	20000	25000	30000
Iteration
(a)
C∣FAR10
O υ O υ O
5 0 5 0 5
3 3 2 2 1
UUoN ΘJnωπεω><
25000
10000	15000	20000
iteration
RN + Softmax Test λ = 1
RN + SoftmaxTrain λ = 0.i
RN + Softmax Test λ=0.ι
RN + SoftmaxTrain λ = 0.01
RN + SoftmaxTestA=O.Oi
♦ SoftmaxTrain
•—∙ SoftmaxTest
<~* RN + SoftmaxTrain λ = ι
5000
(b)
30000
CIFAR10
2000Q
15000
Iteration
一 - - - - -O
5 0 5 0 5 0
7 7 6 6 5 5
X32n84
Cifarioo
*~~* L-Softmax
•~∙ Softmax
*~~* RN L-Softmax
•~~∙ RN Softmax
10000	15000	20000
Iteration
25000
30000

(c)	(d)
Figure 8: (a) Accuracy versus iterations with different choices of λ value on the test set of CIFAR10. The
RN Softmax achieves 92.04% when λ = 0.1 (b) The training/testing sets’ L2 norm vs iterations with different
choices of the λ value on CIFAR10. The RN Softmax with different λ all achieve larger L2-norm. (c) The
training/testing sets’ loss vs iterations with different choices of the λ value on CIFAR10. The RN Softmax
achieves notable smaller loss value 0.1432 than the Softmax with 0.1498. The training loss is very small for
all the methods, but RN + Softmax has significantly smaller testing loss.(It is best viewed by zooming the
figure.) (d) Accuracy vs iterations with Softmax/RN Softmax/L-Softmax/RN L-Softmax on CIFAR100. Both
RN Softmax and RN + L-Softmax achieve better performance compared with baseline, where the best method
is RN + L-Softmax.
Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What is the best multi-stage architecture
for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pp.
2146-2153. IEEE, 2009.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Ser-
gio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed-
ding. In Proceedings of the 22nd ACM international conference on Multimedia, pp. 675-678.
ACM, 2014.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-
supervised nets. In Artificial Intelligence and Statistics, pp. 562-570, 2015.
Chen-Yu Lee, Patrick W Gallagher, and Zhuowen Tu. Generalizing pooling functions in convolu-
tional neural networks: Mixed, gated, and tree. In International conference on artificial intelli-
gence and statistics, 2016.
13
Workshop track - ICLR 2018
Ming Liang and Xiaolin Hu. Recurrent convolutional neural network for object recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3367-
3375, 2015.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,
2013.
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for con-
volutional neural networks. In Proceedings of The 33rd International Conference on Machine
Learning, pp. 507-516, 2016.
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. arXiv preprint arXiv:1704.08063, 2017a.
Yu Liu, Hongyang Li, and Xiaogang Wang. Learning deep features via congenerous cosine loss for
person recognition. arXiv preprint: 1702.06890, 2017b.
Yu Liu, Hongyang Li, and Xiaogang Wang. Rethinking feature discrimination and polymerization
for large-scale recognition. 2017c.
Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-constrained softmax loss for discrimi-
native face verification. arXiv preprint arXiv:1703.09507, 2017.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 815-823, 2015.
Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deeply learned face representations are sparse, selective,
and robust. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2892-2900, 2015.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In Proceedings of the 30th International Conference on Machine
Learning (ICML-13), pp. 1058-1066, 2013.
Feng Wang, Xiang Xiang, Jian Cheng, and Alan L Yuille. Normface: l_2 hypersphere embedding
for face verification. arXiv preprint arXiv:1704.06369, 2017.
Feng Wang, Weiyang Liu, Haijun Liu, and Jian Cheng. Additive margin softmax for face verifica-
tion. arXiv preprint arXiv:1801.05599, 2018a.
Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jingchao Zhou,
and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. arXiv preprint
arXiv:1801.09414, 2018b.
Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach
for deep face recognition. In European Conference on Computer Vision, pp. 499-515. Springer,
2016.
14