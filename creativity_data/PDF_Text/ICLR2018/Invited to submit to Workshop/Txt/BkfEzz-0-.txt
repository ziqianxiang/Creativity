Under review as a conference paper at ICLR 2018
Neuron as an Agent
Anonymous authors
Paper under double-blind review
Ab stract
Existing multi-agent reinforcement learning (MARL) communication methods
have relied on a trusted third party (TTP) to distribute reward to agents, leaving
them inapplicable in peer-to-peer environments. This paper proposes reward dis-
tribution using Neuron as an Agent (NaaA) in MARL without a TTP with two key
ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory
is introduced because inter-agent reward distribution is insufficient for optimiza-
tion. Agents in NaaA maximize their profits (the difference between reward and
cost) and, as a theoretical result, the auction mechanism is shown to have agents
autonomously evaluate counterfactual returns as the values of other agents. NaaA
enables representation trades in peer-to-peer environments, ultimately regarding
unit in neural networks as agents. Finally, numerical experiments (a single-agent
environment from OpenAI Gym and a multi-agent environment from ViZDoom)
confirm that NaaA framework optimization leads to better performance in rein-
forcement learning.
1	Introduction
Now that intelligence with reinforcement learning has been shown to be superior to humans
(Tesauro, 1995; Mnih et al., 2015; Silver et al., 2016), reinforcement learning is expected to ex-
pand into industry applications such as stock trading, autonomous cars, smart grids, and IoT. In
future industrial applications, numerous companies will own agents used to improve their revenues.
Such a situation can be considered as each agent independently solving the problems of a partially
observed Markov decision process (POMDP).
These company agents are designed to independently maximize their own rewards; however, if such
agents could exchange information, the overall revenue of stakeholders would increase. As each
agent has limited visibility of the environment, information exchanges between agents would be
helpful in solving their independent tasks. Thus, this paper aimed to actualize a society where stake-
holders with conflicting interests willingly trade information.
We regard the situation as communication in multi-agent reinforcement learning (MARL). Multi-
agent communication is addressed by several methods such as R/DIAL (Foerster et al., 2016) and
CommNet (Sukhbaatar et al., 2016). CommNet is a state-of-the-art MARL method that considers
communication between agents and features learning among agents with backpropagation.
When considering conditions in MARL such that different stakeholders have created different agents
that communicate with each other, an reward distribution design (e.g., monetary payment) and a
framework without a trusted third party (TTP) are required. TTP (Wu & Soo, 1999; Sandholm &
Wang, 2002) is a neutral administrator that performs reward distributions for all participants, and is
supposed implicitly by most existing literature regarding MARL (Agogino & Tumer, 2006; Foerster
et al., 2016; Sukhbaatar et al., 2016). Although there is a requirement for TTP neutrality towards
all participants, several peer-to-peer trade configurations, such as inter-industry and -country trades,
cannot place TTP. If reward distribution is performed by an untrusted third party, the rewards for
partial participants may be undesirably altered.
To the best of our knowledge, no existing literature discusses reward distributions in the configura-
tion described above. Because CommNet assumes an environment that distributes a uniform reward
to all the agents, if the distributed reward is in limited supply (such as money), it causes the Tragedy
of the Commons (Lloyd, 1833), where the reward of contributing agents will be reduced due to the
participation of free riders. Although there are several MARL methods for distributing rewards ac-
1
Under review as a conference paper at ICLR 2018
Figure 1: A schematic illustration of reward distribution models in MARL. (a) Centralized reward
distribution model (Agogino & Tumer, 2006; Sukhbaatar et al., 2016; Foerster et al., 2016; 2017).
They should suppose TTP to distribute the optimal reward to the agents. (b) Inter-agent reward
distribution model (our model). Some agents receive reward from the environment directly, and
redistribute to other agents. The idea to determine the optimal reward without TTP is playing auction
game among the agents.
cording to agents’ contribution such as QUICR (Agogino & Tumer, 2006) and COMA (Sukhbaatar
et al., 2016), they suppose the existence of TTP and hence cannot be applied to the situation inves-
tigated here.
The proposed method, Neuron as an Agent (NaaA), extends CommNet to actualize reward distri-
butions in MARL without TTP based on two key ideas: (i) inter-agent reward distribution and (ii)
auction theory. Auction theory was introduced because inter-agent reward distributions were insuf-
ficient for optimization. Agents in NaaA maximize profit, the difference between their received re-
wards and the costs which they redistribute to other agents. If the framework is naively optimized, a
trivial solution is obtained where agents reduce their costs to zero to maximize profits. Then, NaaA
employs the auction theory in game design to prevent costs from dropping below their necessary
level. As a theoretical result, we show that agents autonomously evaluate the counterfactual return
as values of other agents. The counterfactual return is equal to the discounted cumulative sum of
counterfactual reward (Agogino & Tumer, 2006) distributed by QUICR and COMA.
NaaA enables representation trades in peer-to-peer environments and, ultimately, regards neural
network units as agents. As NaaA is capable of regarding units as agents without losing generality,
this setting was utilized in the current study. The concept of the proposed method is illustrated in
Figure 1.
An environment extending ViZDoom (Kempka et al., 2016), a POMDP environment, to MARL was
used for the experiment. Two agents, a cameraman sending information and a main player defeat-
ing enemies with a gun, were placed in the environment. Results confirmed that the cameraman
learned cooperative actions for sending information from dead angles (behind the main player) and
outperformed CommNet in score.
Interestingly, NaaA can apply to single- and multi-agent settings, since it learns optimal topology
between the units. Adaptive DropConnect (ADC), which combines DropConnect (Wan et al., 2013)
(randomly masking topology) with an adaptive algorithm (which has a higher probability of pruning
connections with lower counterfactual returns) was proposed as a further application for NaaA. Ex-
perimental classification and reinforcement learning task results showed ADC outperformed Drop-
Connect.
The remainder of this paper is organized as follows. In the next section, we show the problem
setting. Then, we show proposed method with two key ideas: inter-agent reward distribution and
auction theory in Section 3. After related works are introduced in Section 4, the experimental results
are shown in classification, single-agent RL and MARL in Section 5. Finally, a conclusion ends the
paper.
2
Under review as a conference paper at ICLR 2018
2	Problem Definition
Suppose there is an N -agent system in an environment. The goal of this paper was to maximize the
discounted cumulative reward the system obtained from the environment. This was calculated as:
T
G =	γtRtex,	(1)
t=0
where Rtex is a reward which the system obtains from the environment at t, and γ ∈ [0, 1] is the
discount rate and T is the terminal time.
Reward distribution is distributing Rt to all the agents under the following constraint.
N
XRit = Rtex,	(2)
i=1
where Rit is a reward which is distributed to i-th agent at time t. For instance, in robot soccer, the
environment give a reward 1 when an agent shoot a ball to the goal. Each agent should receive the
reward along to their contribution.
In most of MARL communication methods, the policy of reward distribution is determined by a
centralized agent. For example, QUICR (Agogino & Tumer, 2006) and COMA (Foerster et al.,
2017) distribute Rit according to counterfactal reward, difference of reward between an agent made
an action and not. The value of counterfactual reward is calculated by centralized agent, called
trusted third party (TTP).
In a peer-to-peer environment such as inter-industry and -country trade, they cannot place a TTP.
Hence, another framework required to actualize reward distribution without TTP.
3	Proposed Method
The proposed method, Neuron as an Agent (NaaA), extends CommNet (Sukhbaatar et al., 2016) to
actualize reward distributions in MARL without TTP based on two key ideas: (i) inter-agent reward
distribution and (ii) auction theory. As we show in Section 3.3, NaaA actualizes that we can regard
even a unit as an agent.
3.1	Inter-agent Reward Distribution
Some agents receive rewards from the environment directly Rietx, and they distribute these to other
agents as incentives for giving precise information. Rewards are limited, so if an agent distributes ρ
rewards, the reward of that agents is reduced by ρ to satisfy the constraint of Eq:(2). For this reason,
agents other than a specified agent of interest can be considered a secondary environment for the
agent giving rewards of -ρ instead of an observation x. This secondary environment was termed
the internal environment, whereas the original environment was called the external environment.
Similarly to CommNet (Sukhbaatar et al., 2016), the communication protocol between agents was
assumed to be a continuous quantity (such as a vector), the content of which could be trained by
backpropagation.
A communication network among the agents is represented as a directed graph G = (V, E) between
agents, where V = {v1, . . . , vN} is a set of the agents and E ⊂ V2 is a set of edges representing
the connections between two agents. If (vi , vj ) ∈ E, then connection vi → vj holds, indicating
that vj observes the representation of vi . Here, the representation of agent vi at time t was denoted
as xit ∈ R. Additionally, the set of agents that agent i connects to was designated to be Niout =
{j|(vi, vj) ∈ E} and the set of agents that agent i is connected from was Niin = {j|(vj, vi) ∈ E}.
Furthermore, Ni = Niin ∪ Niout . The following assumptions were added to the vi characteristics:
N1: (Selfishness) The utility each agent vi wants to maximize is its own return (cumulative
discounted reward): Git = PkT=0 γkRi,t+k.
N2: (Conservation) The summation of internal rewards over all V equals 0. Hence, the summa-
tion of rewards which V (receive both internal and external environment Rit) are equivalent
3
Under review as a conference paper at ICLR 2018
to the reward Rtex, which the entire multi-agent system receives from the external environ-
ment: Pi=1 Rit = Pi=1 Rietx = Rtex.
N3: (Trade) An agent vi will receive an internal reward ρjit from vj ∈ V in exchange for an
representation signal xi before transferring this signal to the agent. Simultaneously, ρjit
will be subtracted from the reward of vj .
N4: (NOOP) vi can select NOOP (no operation), for which the return is δ > 0, as an action. In
NOOP, the agent inputs and outputs nothing.
The social welfare function (total utility of the agents) Gall is equivalent to the objective function G.
That is,
N all G = Git i=1	N X i=1	T γtRit k=0	T =X k=0	N γt	Rit i=1	.	(3)
From N2, Gall = G holds.
3.1.1	Discounted Cumulative Profit Maximization
From N3, the reward Rit received by vi at t can be written as:
Rit = Rietx + X ρjit - X ρijt ,	(4)
j∈Niout	j∈Niin
which can be divided into positive and negative terms, where the former is defined as revenue, and
the latter as cost. These are respectively denoted as rit = Rietx + Pj∈Niout ρjit , cit = Pj∈Niin ρijt .
Here, Rit represents profit, difference between revenue and cost.
The agent vi maximizes its cumulative discounted profit, Git , represented as:
TT
Git = γkRi,t+k = γ k (ri,t+k - ci,t+k) = rt - ct + γGi,t+1.	(5)
Git could not be observed until the end of an episode (the final time). Because predictions based on
current values were needed to select optimal actions, Git was approximated with the value function
Viπi (sit) = Eπi [Git | sit] where sit is a observation for i-th agent at time t. Under these conditions,
the following equation holds:
Viπi (sit) = rit - cit + γViπi (si,t+1),	(6)
Thus, herein, we only consider maximization of revenue, the value function, and cost minimization.
The inequality Rit > 0 (i.e., rit > cit) indicates that the agent in question gave additional value to
the obtained data. The agent selected the NOOP action because Viπi (sit) ≤ 0 < δ if Rit ≤ 0 for all
t (because of N4).
3.1.2	Social Dilemma
If we directly optimize Eq:(6), a trivial solution is obtained in which the internal rewards converge
at 0, and all agents (excepting agents which directly receive reward from the external environment)
select NOOP as their action. This phenomenon occurs regardless of the network topology G, as no
nodes are incentivized to send payments ρijt to other agents. With this in mind, multi-agent systems
must select actions with no information, achieving the equivalent of taking random actions. For
that reason, the total external reward Rtex shrinks markedly. This phenomenon also known as social
dilemma in MARL, which is caused from a problem that each agent does not evaluate other agents’
value truthfully to maximize their own profit. We are trying to solve this problem with auction theory
in Section 3.2.
3.2	Auction Theory
To make the agents to evaluate other agents’ value truthfully, the proposed objective function bor-
rows its idea from the digital goods auction theory (Guruswami et al., 2005). In general, an auction
theory is a part of mechanism design intended to unveil the true price of goods. Digital goods auc-
tions are one mechanism developed from auction theory, specifically targeting goods that may be
copied without cost such as digital books and music.
4
Under review as a conference paper at ICLR 2018
3.2.1	Envy-free Auction
Although several variations of digital goods auctions exist, an envy-free auction (Guruswami et al.,
2005) was used here because it required only a simple assumption: equivalent goods have a single
simultaneous price. In NaaA, this can be represented by the following assumption:
N5: (Law of one price) Ifρj1,i,t,ρj2,i,t > 0, then ρj1,i,t = ρj2,i,t.
The assumption above indicates that ρjit takes either 0 or a positive value, depending on i at an
equal selected time t. Therefore, the positive side was named vi ’s price, and denoted as qit .
The envy-free auction process is shown in the left section of Figure 2, displaying the negotiation
process between one agent sending an representation (defined as a seller), and a group of agents
buying the representation (defined as buyers). First, a buyer places a bid with the agent at a bid-
ding price bjit (1). Next, the seller selects the optimal price ^it and allocates the representation (2).
Payment occurs if bj exceeds qjt. In this case, Pjit = H (bjit - qit) qit holds where H (∙) is a step
function. For this transaction, the definition gjit = H(bjit - qit) holds, and is named allocation.
After allocation, buyers perform payment: Pjit = gjit(^it (3). The seller sends the representation Xi
only to the allocated buyers (4). Buyers who do not receive the representation approximate xi with
Eπ [xi]. This negotiation is performed at each time step in reinforcement learning.
3.2.2	Theoretical Analysis
The sections below discuss the revenue, cost, and value functions based on Eq:(6).
Revenue: The revenue of an agent is given as
rit = X gjitqit + Rietx = qitdit + Rietx ,	(7)
j∈Niout
where dit = Pj∈Niout gjit is the demand, the number of agents for which the bidding price bjit is
greater than or equal to qit. Because Rex is independent of q^, the optimal price ^it maximizing r%t
is given as:
q it = argmax qdit (q).	(8)
q∈[0,∞)
The rit curve is shown on the right side of Figure 2.
Cost: Cost is defined as an internal reward that one agent pays to other agents. It is represented as:
cit =	gijtqjt = gitqt,	(9)
j∈Nin
where git = (gi1t, . . . , giNt)T and qt = (q1t, . . . , qNt)T. Although cit itself is minimized when
bijt = 0, this represents a tradeoff with the following value function.
Value Function: The representation xit depends on input from the agents in Niin affecting the
bidding price from agents in Niout . If bijt is minimized and bijt = 0, the representation purchase
fails and the reward obtained by the seller agent from the connected buyer agents is lower in the
future.
The effects of the value function were considered for both successful and unsuccessful vj purchasing
cases. The value function was approximated as a linear function git:
Viπi(si,t+1) ≈ oiTtgit + Vi0,t+1,	(10)
where oit is equivalent to the cumulative discount value of the counterfactual reward (Agogino &
Tumer, 2006), it was named counterfactual return. As Vi0t (a constant independent of git) is equal
to the value function when vi takes an action without observing x1, . . . , xN.
The optimization problem is, therefore, presented below using a state-action value function for i-th
agent Qi(∙, ∙):
max Qi (sit, a) = max qdit (q) - mbn Eq Jgit (b)T(q t - Y o it)] +const.,	(11)
5
Under review as a conference paper at ICLR 2018
price q
3. payment
Figure 2: Left: Trade process in an envy-free auction. Right: Agent price determination curve. Agent
revenue is a product of monotonically decreasing demands and prices. The price that maximizes
revenue is the optimal price.
where a = (b, q). Note that git = H(b 一 qt). The expectation EqJ∙] was taken because the asking
price qt was unknown for Vi, except when ^it and giit = 0.
Then, to identify the bidding price that bit maximizes returns, the following theorem holds.
ʌ
Theorem 3.1. (Truthfulness) the optimal bidding price for maximizing returns is bit = γoit.
This proof is shown in the Appendix A.
This implies agents should only consider their counterfactual returns! When γ = 0 it is equivalent
to a case without auction. Hence, the bidding value is raised if each agent considers their long-time
rewards. Consequently, when the NaaA mechanism is used agents behave as if performing valuation
for other agents, and declare values truthfully.
Under these conditions, the following corollary holds:
Corollary 3.1. The Nash equilibrium ofan envy-free auction is (bit, qit) is (γoit, argmax qdit(q)).
q
The remaining problem is how to predict ot . Q-learning was used to predict ot in this paper as the
same way as QUICR Agogino & Tumer (2006). As oit represented the difference between two Qs,
each Q was approximated. The state was parameterized using the vector st, which contained input
and weight. The -greedy policy with Q-learning typically supposed that discrete actions Thus the
allocation gijt was employed as an action rather than bit and qit .
Algorithm The overall algorithm is shown in Algorithm 1.
3.3 Neuron as an Agent
One benefit of NaaA is that it can be used not only for MARL, but also for network training. Typ-
ical neural network training algorithms such as RMSProp (Tieleman & Hinton, 2012) and Adam
(Kingma & Ba, 2014) are based on sequential algorithms such as the stochastic gradient descent
(SGD). Therefore, the problem they solve can be interpreted as a problem of updating a state (i.e.,
weight) to a goal (the minimization of the expected likelihood).
3.3.1	Adaptive DropConnect
Learning can be accelerated by applying NaaA to the optimizer. In this paper, the application of
NaaA to SGD was named Adaptive DropConnect (ADC), the finalization of which can be interpreted
as a combination of DropConnect (Wan et al., 2013) and Adaptive DropOut (Ba & Frey, 2013). In
the subsequent section, ADC is introduced as a potential NaaA application.
6
Under review as a conference paper at ICLR 2018
Algorithm 1 NaaA: inter-agent reward distribution with envy-free auction
1 2 3	: for t = 1 to T do : Compute a bidding price for every edge: for (Vj, Vi) ∈ E do bijt J γoijt Compute an asking price for every node: for Vi ∈ V do q沉 J argmax qd^ (q). q∈[0,∞)
4 5 6 7 8 9 10 11 12 13 14 15 16 17	:	for (Vi, Vj) ∈ E do Compute allocation: gjit J H(bji,t — ^it) Compute the price the agent should pay: Pjit J gj^qit : end for :	Make a payment: for Vi ∈ V do Rit J Pj∈Niout ρjit — Pj∈Niin ρijt, Make a shipment: for Vi ∈ V do x j = gijtxijt + (1 — gijt)xijt :	for Vi ∈ V do :	Observe external state sietx Sit J (SeX,文it, θi), where Xit = (Xi11,…,Xint)T and θi is Vi's parameter. Sample action(席 〜∏ix (sit) :	Receive external reward Rit J Rit + Rietx (aietx) :	Update oijt under the manner of Q-learning : end for : end for
Algorithm 2 Adaptive DropConnect
1:	for t = 1 to T do
2:	Compute a bidding price for every edge: for (vj, Vi) ∈ E do bj J ∣Wijt∣
3:	Compute an asking price for every node: for Vi ∈ V do ^it J argmax qdit(q).
q∈[0,∞)
4:	for (Vi, Vj) ∈ E do
5:	Compute allocation: gjit J H(bji,t — ^it)
6:	end for
7:	Sample a switching matrix Ut from a Bernoulli distribution: Ut 〜Bernoulli(ε)
8:	Sample the random mask Mt from a Bernoulli distribution: Mt 〜Bernoulli(1 /2)
9:	Generate the adaptive mask: Mt0 J Ut ◦ Mt + (1 — Ut) ◦ Gijt
10:	Compute ht for making a shipment: ht J (Mt0 ◦ Wt)xt + bt
11:	Update Wt and bt by backpropagation.
12:	end for
ADC uses NaaA for supervised optimization problems with multiple revisions. In such problems,
the first step is the presentation of an input state (such as an image) by the environment. Agents
are expected to update their parameters to maximize the rewards presented by a criterion calculator.
Criterion calculators gives batch-likelihoods to agents, representing rewards. Each agent, a classifier,
updates its weights to maximize the reward from the criterion calculator. These weights are recorded
as an internal state. A heuristic utilizing the absolute value of weight |wijt | (the technique used by
Adaptive DropOut) was applied as the counterfactual return oijt. The absolute value of weights was
used because it represented the updated amounts for which the magnitude of error of unit outputs
was proportional to |wijt|.
This algorithm is presented as Algorithm 2. Because the algorithm is quite simple, it can be easily
implemented and, thus, applied to most general deep learning problems such as image recognition,
sound recognition, and even deep reinforcement learning.
4	Related Works
Existing multi-agent reinforcement learning (MARL) communication methods have relied on a
trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer
environments. R/DIAL (Foerster et al., 2016) is a communication method for deep reinforcement
learning, which train the optimal communication among the agent with Q-learning. It focuses on that
paradigm of centralized planning. CommNet (Sukhbaatar et al., 2016), which exploits the character-
istics ofa unit that is agnostic to the topology of other units, employs backpropagation to train multi-
7
Under review as a conference paper at ICLR 2018
agent communication. Instead of reward R(at) of an agent i for actions at t at, QUICR-learning
(Agogino & Tumer, 2006) maximizes counterfactual reward R(at) - R(at - ait), the difference in
the case of the agent i takes an action ait (at) and not (at - ait). COMA (Foerster et al., 2017) also
maximizes CoUnterfactUal rewards in an actor-critic setting. CommNet, QUICR and COMA have a
centralized environment for distributing rewards through a TTP. In contrast, NaaA does not rely on
a TTP, and hence, each agent calcUlates its reward.
While inter-agent reward distribUtion has not been considered in the context of commUnication, trad-
ing agents have been considered in other contexts. Trading agent competition (TACs), competitions
for trading agent design, have been held in varioUs locations regarding topics sUch as smart grids
(Ketter et al., 2013), wholesale (KUate et al., 2013), and sUpply chains (Pardoe & Stone, 2009),
yielding innUmerable trading algorithms sUch as TesaUro’s bidding algorithm (TesaUro & Bredin,
2002) and TacTex’13 (Urieli & Stone, 2014). Since several competitions employed an aUction as
optimal price determination mechanism (Wellman et al., 2001; Stone et al., 2003), Using aUctions to
determine optimal prices is now a natUral approach. UnfortUnately, these existing methods cannot be
applied to the present sitUation. First, their agents did not commUnicate becaUse the typical pUrpose
of a TAC is to create market competition between agents in a zero-sUm game. Secondly, the traded
goods are not digital goods bUt instead goods in limited sUpply, sUch as power and tariffs. Hence,
this is the first paper to introdUce inter-agent reward distribUtion to MARL commUnications.
AUction theory is discUssed in terms of mechanism design (Myerson, 1983), also known as inverse
game theory. Second-price aUctions (Vickrey, 1961) are aUctions inclUding a single prodUct and
several bUyers. In this paper, a digital goods aUction (GUrUswami et al., 2005) was Used as an aUction
with an infinite sUpply. Several methods extend digital goods aUction to address collUsion, inclUding
the consensUs estimate (Goldberg & Hartline, 2003) and random sample aUction (Goldberg et al.,
2006), which can be Used to improve oUr method.
This paper is also related to DropConnect in terms of controlling connections between Units. Adap-
tive DropConnect (ADC), proposed in a later section of this paper as a fUrther application, extends
the DropConnect (Wan et al., 2013) regUlarization techniqUe. The finalized idea of ADC (which Uses
a skew probability correlated to the absolUte valUe of weights rather than dropping each connection
between Units by a constant probability) is closer to Adaptive DropOUt (Ba & Frey, 2013), althoUgh
their derivation differs. The adjective “adaptive” is added with respect to the method. NeUral network
optimizing Using RL was investigated by Andrychowicz et al. (2016); however, their methods Used
a recUrrent neUral network (RNN) and are therefore difficUlt to implement, whereas the proposed
method is RNN-free and forms as a layer. For these reasons, its implementation is simple and fast
and it also has a wide area of applicability.
5	Experiment
To confirm that NaaA works widely with machine learning tasks, we confirm oUr method of sUper-
vised learning tasks as well as reinforcement learning tasks. As sUpervised learning tasks, we Use
typical machine learning tasks sUch as image classification Using MNIST, CIFAR-10, and SVHN.
As reinforcement tasks, we confirm single- and mUlti-agent environment. The single-agent environ-
ment is from OpenAI Gym. We confirm the resUlt Using a simple reinforcement task: CartPole. In
mUlti-agent, we Use ViZDoom, a 3D environment for reinforcement learning.
5.1	Classification
5.1.1	Setup
For classification, three types of datasets were Used: MNIST, CIFAR-10, and STL-10. The given
task was to predict the label of each image, and each dataset had a class nUmber of 10. The first
dataset, MNIST, was a collection of black and white images of handwritten digits sized 28 × 28.
The training and test sets contained 60,000 and 10,000 example images, respectively. The CIFAR-
10 dataset images were colored and sized 32 × 32, and the assigned task was to predict what was
shown in each pictUre. This dataset contained 6,000 images per class (5,000 for training and 1,000
for testing). The STL-10 dataset was Used for image recognition, and had 1,300 images for each
class (500 training, 800 testing). Each image was sized 96 × 96; however, for the experiment, the
8
Under review as a conference paper at ICLR 2018
images were resized to 48 × 48 because the greater resolution of this dataset (relative to the above
datasets) required far more computing time and resources.
5.1.2	MODEL
Two models were compared in this experiment: DropConnect and Adaptive DropConnect (the model
proposed in this paper). The baseline model was composed of two convolutional layers and two
fully connected layers whose outputs are dropped out (we set the possibility as 0.5). The labels
of input data were predicted using log-softmaxed values from the last fully connected layer. In
the DropConnect and Adaptive DropConnect models, the first fully connected layer was replaced
by a DropConnected and Adaptive DropConnected layer, respectively. It should be noted that the
DropConnect model corresponded to the proposed method when ε = 1.0, meaning agents did not
perform their auctions but instead randomly masked their weights.
5.1.3	Results
The models were trained over ten epochs using the MNIST datasets, and were then evaluated using
the test data. The CIFAR-10 and STL-10 epoch numbers were 20 and 40, respectively. Experi-
ments were repeated 20 times for each condition, and the averages and standard deviations of error
rates were calculated. Results are shown in Table 1. As expected, the Adaptive DropConnect model
performed with a lower classification error rate than either the baseline or DropConnect models
regardless of the given experimental datasets.
Table 1: Experimental result for image classification tasks and single-agent RL
DropConnect (Wan et al., 2013)
Adaptive DropConnect
MNIST CIFAR-10 STL-10
1.72 ± 0.160 43.14 ± 1.335^^50.92 ± 1.322
1.36 ± 0.132 39.84 ± 1.035 42.17 ± 2.329
CartPole
285 ± 21.5
347 ± 29.4
5.2	Single-agent RL
Next, the single-agent reinforcement learning task was set as the CartPole task from OpenAI Gym
(Brockman et al., 2016) with visual inputs. In this setting, the agent was required to balance a pole
while moving a cart. The images contained a large amount of non-useful information, making pixel
pruning important. The result in Table 1 demonstrates that our method improves the standard RL.
5.3	Multi-agent RL
The proposed reward distribution method was confirmed to work as expected by a validation ex-
periment using the multi-agent setting in ViZDoom (Kempka et al., 2016), an emulator of Doom
containing a map editor where additional agents complement the main player. A main player in the
ViZDoom environment aims to seek the enemy in the map and then defeat the enemy.
5.3.1	Setup
A defend the center (DtC)-based scenario, provided by ViZDoom platform, was used for this ex-
periment. Two players, a main player and a cameraman, were placed in the DtC, where they started
in the center of a circular field and then attacked enemies that came from the surrounding wall. Al-
though the main player could attack the enemy with bullets, the cameraman had no way to attack,
only scouting for the enemy. The action space for the main player was the combination of { attack,
turn left, turn right }, giving a total number of actions 23 = 8. The cameraman had two possible
actions: { turn left, turn right }. Although the players could change direction, they could not move
on the field. Enemies died after receiving one attack (bullet) from the main player, and then player
received a score of +1 for each successful attack. The main player received 26 bullets by default at
the beginning of each episode. The main player died if they received attacks from the enemy to the
extent that their health dropped to 0, and received a score of -1 for each death. The cameraman did
not die if attacked by an enemy. Episodes terminated either when the maim player died or after 525
steps elapsed.
9
Under review as a conference paper at ICLR 2018
4.0
3.5
3.0
5 0 5
NNL
SP」BM①工
4 0 6
040403
a Qa
Figure 3: Left: Learning curve for ViZDoom multi-agent task. The proposed NaaA-based method
outperformed the other two methods (baseline and Comm DQNs). Right: Visualizing reward from
the main player to the cameramann shows us what is important information for the main player: (a)
The pistol. (b) The point at which the enemy appeared and approached.
Figure 4: NaaA leads agents to enter a cooperative relationship. First, the two agents face different
directions, and the cameraman sells their information to the main player (1). The main player (infor-
mation buyer) starts to turn right to find the enemy. The cameraman (information seller) starts to turn
left to seek new information by finding the blind area of the main player (2 and 3). After turning, the
main player attacks the first, having already identified enemy (4 and 5). Once the main player finds
the enemy, he attacks and obtains the reward (6 and 7). Both agents then return to watching the dead
area of the other until the next enemy appears (8).
5.3.2	Model
Three models, described below, were compared: the proposed method and two comparison targets.
Baseline: DQN without communication. The main player learned standard DQN with the perspec-
tive that the player is viewing. Because the cameraman did not learn, this player continued to move
randomly.
Comm: DQN with communication, inspired by Commnet. The main player learns DQN with two
perspectives: theirs and that of the cameraman. The communication vector is learned with a feed-
forward neural network.
NaaA: The proposed method. The main player learned DQN with two perspectives: theirs and that of
the cameraman. Transmissions of rewards and communications were performed using the proposed
method.
10
Under review as a conference paper at ICLR 2018
5.3.3	Results
Training was performed over the course of 10 million steps. Figure 3 Left demonstrates the pro-
posed NaaA model outperformed the other two methods. Improvement was achieved by Adaptive
DropConnect. It was confirmed that the cameraman observed the enemy through an episode, which
could be interpreted as the cameraman reporting enemy positions. In addition to seeing the enemy,
the cameraman observed the area behind the main player several times. This enabled the cameraman
to observe enemy attacks while taking a better relative position.
To further interpret this result, a heatmap visualization of revenue earned by the agent is presented
in Figure 3 Right. The background picture is a screen from Doom, recorded at the moment when
the CNN filter was most activated. Figure 4 shows an example of learnt sequence of actions by our
method.
6 Concluding Remarks and Future Work
This paper proposed a NaaA model to address communication in MARL without a TTP based on
two key ideas: inter-agent reward distribution and auction theory. Existing MARL communication
methods have assumed the existence of a TTP, and hence could not be applied in peer-to-peer envi-
ronments. The inter-agent reward distribution, making agents redistribute the rewards they received
from the internal/external environment, was reviewed first. When an envy-free auction was intro-
duced using auction theory, it was shown that agents would evaluate the counterfactual returns of
other agents. The experimental results demonstrated that NaaA outperformed a baseline method and
a CommNet-based method.
Furthermore, a Q-learning based algorithm, termed Adaptive DropConnect, was proposed to dy-
namically optimize neural network topology with counterfactual return evaluation as a further appli-
cation. To evaluate this application, experiments were performed based on a single-agent platform,
demonstrating that the proposed method produced improved experimental results relative to existing
methods.
Future research may also be directed toward considering the connection between NaaA and neu-
roscience or neuroevolution. Edeleman propounded the concept of neural Darwinism (Edelman,
1987), in which group selection occurs in the brain. Inter-agent rewards, which were assumed in
this paper, correspond to NTFs and could be used as a fitness function in genetic algorithms for
neuroevolution such as hyperparameter tuning.
As NaaA can be applied in peer-to-peer environments, the implementation of NaaA in blockchain
(Swan, 2015) is under consideration. This implementation would extend the areas where deep rein-
forcement learning could be applied. Bitcoin (Nakamoto, 2008) could be used for inter-agent reward
distribution, and the auction mechanism could be implemented by smart contracts (Buterin et al.,
2014). Using the NaaA reward design, it is hoped that the world may be united, allowing people to
share their own representations on a global scale.
References
A. K. Agogino and K. Tumer. QUICR-learning for multi-agent coordination. AAAI’06, 2006.
M. Andrychowicz, M. Denil, S. Gomez, M. W Hoffman, D. Pfau, T. Schaul, and N. de Freitas.
Learning to learn by gradient descent by gradient descent. In NIPS’16, 2016.
J. Ba and B. Frey. Adaptive dropout for training deep neural networks. In NIPS’13, 2013.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI gym, 2016.
V. Buterin et al. A next-generation smart contract and decentralized application platform. white
paper, 2014.
G. M Edelman. Neural Darwinism: The theory of neuronal group selection. Basic books, 1987.
11
Under review as a conference paper at ICLR 2018
J. Foerster, Y. Assael, N. de Freitas, and S. Whiteson. Learning to communicate with deep multi-
agent reinforcement learning. In NIPS’16, 2016.
J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy
gradients. arXiv:1705.08926, 2017.
A. V Goldberg and J. D Hartline. Competitiveness via consensus. In Proceedings of the fourteenth
annual ACM-SIAM symposium on Discrete algorithms, pp. 215-222. Society for Industrial and
Applied Mathematics, 2003.
A. V Goldberg, J. D Hartline, A. R Karlin, M. Saks, and A. Wright. Competitive auctions. Games
and Economic Behavior, 55(2):242-269, 2006.
V.	Guruswami, J. D Hartline, A. R Karlin, D. Kempe, C. Kenyon, and F. McSherry. On profit-
maximizing envy-free pricing. In ACM-SIAM symposium on Discrete algorithms, 2005.
M. KemPka, M. Wydmuch, G. Runc, J. Toczek, and Wojciech ja´kowski. Vizdoom: A doom-based
ai research platform for visual reinforcement learning. In Computational Intelligence and Games
(CIG), 2016 IEEE Conference on, pp. 1-8. IEEE, 2016.
W.	Ketter, J. Collins, and P. Reddy. Power tac: A competitive economic simulation of the smart grid.
Energy Economics, 39:262-270, 2013.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
R. Talla Kuate, M. He, M. Chli, and H. H Wang. An intelligent broker agent for energy trading: An
mdp approach. In IJCAI’13, 2013.
W. Forster Lloyd. Two lectures on the checks to population. 1833.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. Nature, 518(7540):529-533, 2015.
R.	B Myerson. Mechanism design by an informed principal. Econometrica: Journal of the Econo-
metric Society, pp. 1767-1797, 1983.
S.	Nakamoto. Bitcoin: A peer-to-peer electronic cash system, 2008.
D. Pardoe and P. Stone. An autonomous agent for supply chain management. Handbooks in Infor-
mation Systems Series: Business Computing, 3:141-172, 2009.
T.	Sandholm and X. Wang. (im) possibility of safe exchange mechanism design. In AAAI’02, 2002.
D.	Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. Nature, 529(7587):484-489, 2016.
P.	Stone, R. E Schapire, M. L Littman, J.anos A Csirik, and D. McAllester. Decision-theoretic bid-
ding based on learned density models in simultaneous, interacting auctions. Journal of Artificial
Intelligence Research, 19:209-242, 2003.
S. Sukhbaatar, R. Fergus, et al. Learning multiagent communication with backpropagation. In
NIPS’16, 2016.
M. Swan. Blockchain: Blueprint for a new economy. 2015.
G. Tesauro. Temporal difference learning and TD-gammon. Communications of the ACM, 38(3):
58-68, 1995.
G. Tesauro and J. L Bredin. Strategic sequential bidding in auctions using dynamic programming.
In Proceedings of the first international joint conference on Autonomous agents and multiagent
systems (AAMAS’02). ACM, 2002.
12
Under review as a conference paper at ICLR 2018
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networksfor machine learning, 4(2):26-31, 2012.
D. Urieli and P. Stone. Tactex’13: a champion adaptive power trading agent. In Proceedings of the
2014 international conference on Autonomous agents and multi-agent systems (AAMAS’14), pp.
1447-1448. International Foundation for Autonomous Agents and Multiagent Systems, 2014.
W. Vickrey. Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance,
16(1):8-37, 1961.
L.	Wan, M. Zeiler, S. Zhang, Y. L Cun, and R. Fergus. Regularization of neural networks using
dropconnect. In ICML’13, 2013.
M.	P. Wellman, P. R. Wurman, K. O’Malley, R. Bangera, D. Reeves, W. E Walsh, et al. Designing
the market game for a trading agent competition. IEEE Internet Computing, 5(2):43-51, 2001.
S. Wu and V. Soo. Game theoretic reasoning in multi-agent coordination by negotiation with
a trusted third party. In Proceedings of the third annual conference on Autonomous Agents
(AGENTS’99). ACM, 1999.
Appendix
A.1 Proof of Theorem 3.1
The optimization problem in Eq:11 is made of two terms except of the constant, and the only second
term is depends on b. Hence, we consider to optimize the second term. The optimal bidding prices
qt is given by the following equation.
bit = argminEq Jgit(b)T(qt - Yoit)] = argminEqJH(b - qt)T(qt - Yoit)]
bb
N
argmin Eqt	H (bj - qjt)(qjt - Yoijt)
b	j=1
argmin	Eqjt [H (bj - qjt)(qjt - Yoijt)] ,
b	j=1
(12)
From independence, the equation is solved if we solve the following problem.
ʌ
bijt = argmin Eqjt [H (b - qjt)(qjt - Yoijt)] , ∀j ∈ {1, . . . , N}	(13)
b
ʌ
Hence, bijt can be derived as the solution which satisfies the following equation.
∂	∂2
Eqjtqjt [H(b -	qjt)(qjt	- Yoijt)]	= 0,	2Eq Eqjt	[H(b	-	qjt)(qjt	-	Yoijt)]	> 0
db	b』jt	db	b =bijt
For simplicity, we let q = qjt and o = oij,t+1. Then, the following equation holds.
∂	∂∞
∂bEq [H(b - q)(q - Yo)] = ∂b J	H(b - q)(q - Yo)p(q)dq
∂b
=∂b J0 (q- Yo)p(q) dq
= (b - Yo)p(q = b),
∂2	∂
∂b2eq [H(b - q)(q- Yo)] = p(q =b) + (b - Yo)律①=b)
(14)
(15)
13
Under review as a conference paper at ICLR 2018
Then, condition ∂Eqjt [H(b - QjtMqjt - Yoijt)] b=加 =0 is satisfied only by bijt = Yoijtj We
ʌ
substitue bijt into Eq:15,
∂2
2Eq Eqjt [H(b - qjt Xqjt- γoijt)]	= p(q = bijt) +0 > 0	(16)
db	b』jt
ʌ ʌ
Therefore, bijt = Yoijt is a unique solution as the minimum point. From generality, bit = Yoit
holds.
14