Workshop track - ICLR 2018
Ensemble Robustness and Generalization
of Stochastic Deep Learning Algorithms
Tom Zahavy^, Alexander Sivak**, Bingyi Kang*, Jiashi Feng*, Huan Xut & Shie Manno声
I Department of EE, Technion
** Department of CS, Technion
f Department of ISE, National University of Singapore
* Department ofECE, National University of Singapore
{tomzahavy@campus,silex@cs,shie@ee}.technion.ac.il
{bingykang,jshfeng}@gmail.com
huan.xu@isye.gatech.edu
Ab stract
The question why deep learning algorithms generalize so well has attracted in-
creasing research interest. However, most of the well-established approaches,
such as hypothesis capacity, stability or sparseness, have not provided complete
explanations (Zhang et al., 2017; Kawaguchi et al., 2017). In this work, we focus
on the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypoth-
esis will not change much due to perturbations of its training examples, then it
will also generalize well. As most deep learning algorithms are stochastic (e.g.,
Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the ro-
bustness arguments of XU & Mannor, and introduce a new approach - ensemble
robustness - that concerns the robustness of a population of hypotheses. Through
the lens of ensemble robustness, we reveal that a stochastic learning algorithm can
generalize well as long as its sensitiveness to adversarial perturbations is bounded
in average over training examples. Moreover, an algorithm may be sensitive to
some adversarial examples (Goodfellow et al., 2015) but still generalize well. To
support our claims, we provide extensive simulations for different deep learning
algorithms and different network architectures exhibiting a strong correlation be-
tween ensemble robustness and the ability to generalize.
1	Introduction
Deep Neural Networks (DNNs) have been successfully applied in many artificial intelligence tasks,
providing state-of-the-art performance and a remarkably small generalization error. On the other
hand, DNNs often have far more trainable model parameters than the number of samples they are
trained on and were shown to have a large enough capacity to memorize the training data (Zhang
et al., 2017). The findings of Zhang et al. suggest that classical explanations for generalization
cannot be applied directly to DNNs and motivated researchers to look for new complexity mea-
sures and explanations for the generalization deep neural networks (Bartlett et al., 2017; Neyshabur
et al., 2017; Arpit et al., 2017; Kawaguchi et al., 2017). However, in this work, we focus on a dif-
ferent approach to study generalization of DNNs, i.e., the connection between the robustness of a
deep learning algorithm and its generalization performance. Xu & Mannor have shown that if an
algorithm is robust (i.e., its empirical loss does not change dramatically for perturbed samples), its
generalization performance can also be guaranteed. However, in the context of DNNs, practitioners
observe contradicting evidence between these two attributes. On the one hand, DNNs generalize
well, and on the other, they are fragile to adversarial perturbation on the inputs (Szegedy et al., 2014;
Goodfellow et al., 2015). Nevertheless, algorithms that try to improve the robustness of learning al-
gorithms have been shown to improve the generalization of deep neural networks. Two examples
are adversarial training, i.e., generating adversarial examples and training on them (Szegedy et al.,
2014; Goodfellow et al., 2015; Shaham et al., 2015), and Parseval regularization (Cisse et al., 2017),
i.e., minimizing the Lifshitz constant of the network to guarantee low robustness. While these meth-
1
Workshop track - ICLR 2018
ods minimize the robustness implicitly, their empirical success Indicates a connection between the
robustness of an algorithm and its ability to generalize.
To solve this contradiction, we revisit the robustness argument in (Xu & Mannor, 2012) and present
ensemble robustness, to characterize the generalization performance of deep learning algorithms.
Our proposed approach is not intended to give tight bounds for general deep learning algorithms,
but rather to pave the way for addressing the question: how can deep learning perform so well while
being fragile to adversarial examples? Answering this question is difficult, yet we present evidence
in both theory and simulation suggesting that ensemble robustness explains the generalization per-
formance of deep learning algorithms.
Ensemble robustness concerns the fact that a randomized algorithm (e.g., Stochastic Gradient De-
scent (SGD), Dropout (Srivastava et al., 2014), Bayes-by-backprop (Blundell et al., 2015), etc.) pro-
duces a distribution of hypotheses instead of a deterministic one. Therefore, ensemble robustness
takes into consideration robustness of the population of the hypotheses: even though some hypothe-
ses may be sensitive to perturbation on inputs, an algorithm can still generalize well as long as most
of the hypotheses sampled from the distribution are robust on average. Kawaguchi et al. (2017) took
a different approach and claimed that deep neural networks could generalize well despite nonrobust-
ness. However, our definition of ensemble robustness together with our empirical findings suggest
that deep learning methods are typically robust although being fragile to adversarial examples.
Through ensemble robustness, we prove that the following holds with a high probability: random-
ized learning algorithms can generalize well as long as its output hypothesis has bounded sensi-
tiveness to perturbation in average (see Theorem 1). Specified for deep learning algorithms, we
reveal that if hypotheses from different runs of a deep learning method perform consistently well
in terms of robustness, the performance of such deep learning method can be confidently expected.
Moreover, each hypothesis may be sensitive to some adversarial examples as long as it is robust on
average.
Although ensemble robustness may be difficult to compute analytically, we demonstrate an empir-
ical estimate of ensemble robustness and investigate the role of ensemble robustness via extensive
simulations. The results provide supporting evidence for our claim: ensemble robustness consis-
tently explains the generalization performance of deep neural networks. Furthermore, ensemble
robustness is measured solely on training data, potentially allowing one to use the testing examples
for training and selecting the best model based on its ensemble robustness.
2	Related Works
Xu et al.( 2012) proposed to consider model robustness for estimating generalization performance
for deterministic algorithms, such as for SVM (Xu et al., 2009b) and Lasso (Xu et al., 2009a). They
suggest using robust optimization to construct learning algorithms, i.e., minimizing the empirical
loss concerning the adversarial perturbed training examples.
Introducing stochasticity to deep learning algorithms has achieved great success in practice and
also receives theoretical investigation. Hardt et al. (2015) analyzed the stability property of SGD
methods, and Dropout (Srivastava et al., 2014) was introduced as a way to control over-fitting by
randomly omitting subsets of features at each iteration of a training procedure. Different explana-
tions for the empirical success of dropout have been proposed, including, avoiding over-fitting as
a regularization method (Baldi & Sadowski, 2013; Wager et al., 2013; Jain et al., 2015) and ex-
plaining dropout as a Bayesian approximation for a Gaussian process (Gal & Ghahramani, 2015).
Different from those works, this work will extend the results in (Xu & Mannor, 2012) to randomized
algorithms, to analyze them from an ensemble robustness perspective.
Robustness and ensemble robustness share some similarities with stability, a related yet different
property of learning algorithms that also guarantees generalization. An algorithm is stable if it
produces an output hypothesis that is not sensitive to the sampling of the empirical data set. In more
detail, if a training example is replaced with another example from the same distribution, the training
error will not change much; see (Bousquet & Elisseeff, 2002) for more details, and (Elisseeff et al.,
2005) for a discussion on randomized algorithms. We emphasize that robustness and stability are
different properties; robustness concerns global modifications of the training data while stability is
more local in that sense. Moreover, robustness concerns attributes of a single hypothesis, while
2
Workshop track - ICLR 2018
stability concerns two (one for the original data set and one for the modified one). Finally, a learning
algorithm may be both stable and robust, e.g., SVM (Xu et al., 2009b), or robust but not stable, e.g.,
Lasso Regression (Xu et al., 2009a).
Adversarial examples for deep neural networks were first introduced in (Szegedy et al., 2014), while
some recent works propose to utilize them as a regularization technique for training deep mod-
els (Goodfellow et al., 2015; Gu & Rigazio, 2014; Shaham et al., 2015). However, all of those
works attempt to find the “worst case” examples in a local neighborhood of the original training
data and are not focused on measuring the global robustness of an algorithm nor on studying the
connection between robustness and generalization.
3	Preliminaries
In this work, we investigate the generalization property of stochastic learning algorithms in deep
neural networks, by establishing their PAC bounds. In this section, we provide some preliminary
facts that are necessary for developing the approach of ensemble robustness. After introducing
the problem setup we are interested in, we in particular highlight the inherent randomness of deep
learning algorithms and give a formal description of randomized learning algorithms. Then, we
briefly review the relationship between robustness and generalization performance established in
(Xu & Mannor, 2012).
Problem setup We now introduce the learning setup for deep neural networks, which follows a
standard one for supervised learning. More concretely, we have Z and H as the sample set and the
hypothesis set respectively. The training sample set s = {s1, . . . , sn} consists of n i.i.d. samples
generated by an unknown distribution μ, and the target of learning is to obtain a neural network that
minimizes expected classification error over the i.i.d. samples from μ. Throughout the paper, We
consider the training set s with a fixed size of n.
We denote the learning algorithm as A, which is a mapping from Zn to H. We use A : s → hs to
denote the learned hypothesis given the training set s. We consider the loss function `(h, z) whose
value is nonnegative and upper bounded by M. Let L(∙) and 'emp(∙) denote the expected error and
the training error for a learned hypothesis hs, i.e.,
L(hs)，Ez〜μ'(hs, z), and 'emp(hs)，1 X '(hs, sj.	(1)
si∈s
We are going to characterize the generalization error ∣L(hs) - 'emp(hs) | of deep learning algorithms
in the following section.
Randomized algorithms Most of modern deep learning algorithms are in essence randomized
ones, which map a training set s to a distribution of hypotheses ∆(H) instead ofa single hypothesis.
For example, running a deep learning algorithm A with dropout for multiple times will produce dif-
ferent hypotheses which can be deemed as samples from the distribution ∆(H). Therefore, before
proceeding to analyze the performance of deep learning, we provide a formal definition of random-
ized learning algorithms here.
Definition 1 (Randomized Algorithms). A randomized learning algorithm A is a function from
Zn to a set of distributions of hypotheses ∆(H), which outputs a hypothesis hs 〜∆(H) with a
probability πs (h).
When learning with a randomized algorithm, the target is to minimize the expected empirical loss
for a specific output hypothesis hs, similar to the ones in (1). Here ` is the loss incurred by a specific
output hypothesis by one instantiation of the randomized algorithm A.
Examples of the internal randomness of a deep learning algorithm A include dropout rate (the pa-
rameter for a Bernoulli distribution for randomly masking certain neurons), random shuffle among
training samples in SGD, the initialization of weights for different layers, to name a few.
Robustness and generalization Xu & Mannor (2012) established the relation between algorith-
mic robustness and generalization for the first time. An algorithm is robust if the following holds: if
3
Workshop track - ICLR 2018
two samples are close to each other, their associated losses are also close. For being self-contained,
we here briefly review the algorithmic robustness and its induced generalization guarantee.
Definition 2 (Robustness, XU & Mannor (2012)). Algorithm A is (K, e(∙)) robust, for K ∈ N and
e(∙) : Zn → R ,if Z can be partitioned into K disjoint sets, denoted by {Ci}K=ι, such that the
following holds for all s ∈ Zn :
∀s ∈ s,∀z ∈ Z,∀i = 1, . . .,K :
if s, z ∈ Ci, then ∣'(As, S) - '(As, z)| ≤ e(n).
Based on the above robustness property of algorithms, Xu et al. (Xu & Mannor, 2012) prove that a
robust algorithm also generalizes well.
Motivated by their results, Shaham et al. (Shaham et al., 2015) proposed adversarial training algo-
rithm to minimize the empirical loss over synthesized adversarial examples. However, those results
cannot be applied for characterizing the performance of modern deep learning models well.
4	Ensemble Robustness
To explain the proper performance of deep learning, one needs to understand the internal randomness
of deep learning algorithms and the population performance of the multiple possible hypotheses. In-
tuitively, a single output hypothesis cannot be robust to adversarial perturbation on training samples
and the deterministic robustness argument in (Xu & Mannor, 2012) cannot be applied here. Fortu-
nately, deep learning algorithms output the hypothesis sampled from a distribution of hypotheses.
Therefore, even if some samples are not ”nice” for one specific hypothesis, they aren’t likely to fail
most of the hypothesis from the produced distribution. Thus, deep learning algorithms generalize
well. Such intuition motivates us to introduce the concept of ensemble robustness that is defined
over the distribution of output hypotheses of a deep learning algorithm.
Definition 3 (Ensemble Robustness). A randomized algorithm A is (K, e(n)) ensemble robust, for
K ∈ N and (n), ifZ can be partitioned into K disjoint sets, denoted by {Ci}iK=1, such that the
following holds for all s ∈ Zn :
∀s ∈ s, ∀i = 1, . . . , K :
if s ∈ Ci, then EA max ∣'(As, S) — '(As, z)| ≤ e(n).
z∈Ci
Here the expectation is taken w.r.t. the internal randomness of the algorithm A.
An algorithm with strong ensemble robustness can provide good generalization performance in ex-
pectation w.r.t. the generated hypothesis, as stated in the following theorem. We note that the proofs
for all the theorems that we present in this section can be found supplementary material. Also, the
supplementary material holds an additional proof for the special case of Dropout.
Theorem 1.	Let A be a randomized algorithm with (K, e(n)) ensemble robustness over the training
set s, with |s| = n. Let ∆(H) - A : S denote the output hypothesis distribution of A. Thenfor any
δ > 0, with probability at least 1 — δ with respect to the random draw of the S and h 〜∆(H), the
following holds:
∣L(h) — 'emp(h)l ≤《"叫)；2"2 .
Note that in the above theorem, we hide the dependency of the generalization bound on K in ensem-
ble robustness measure e(n). Generally, there is a trade-offbetween e(n) and K, the larger K is, the
smaller e(n) is due to the finer partition. This tradeoff is more evident in the bound of Theorem 2,
see also (Xu & Mannor, 2012). Studying the asymptotics of e(n) is hard for general algorithms and
deep networks, yet, it can be done for simpler learning algorithms. For example, for linear SVM,
e(n) is equivalent to the covering number (Xu et al., 2009b).
Due to space limitations, all the technical lemmas and details of the proofs throughout the paper are
deferred to supplementary material. Theorem 1 leads to the following corollary which gives a way
to minimize expected loss directly.
4
Workshop track - ICLR 2018
Corollary 1. Let A be a randomized algorithm with (K, e(n)) ensemble robustness. Let
Ci,..., CK be a partition of Z, and write zι 〜z2 if zι, z2 fall into the same Ck. If the train-
ing sample S is generated by i.i.d. draws from μ, then with probability at least 1 一 δ, the following
holds over h ∈ H
L(h) ≤
1n
一T max '(h, zi) + ∖ -
n z—z Zi〜Si	V
i=1
nM<≡(n) + 2M2
δn
Corollary 1 suggests that one can minimize the expected error of a deep learning algorithm effec-
tively through minimizing the empirical error over the training samples si perturbed in an adversarial
way. In fact, such an adversarial training strategy has been exploited in (Goodfellow et al., 2015;
Shaham et al., 2015).
Theorem 2.	Let A be a randomized algorithm with (K, e(n)) ensemble robustness over the training
set s, where |s| = n. Let ∆(H) denote the output hypothesis distribution of the algorithm A on the
training set s. Suppose following variance bound holds:
VarA max ∣'(As,Si) — '(As,z)∣ ≤ α
Z〜Si
Then for any δ > 0, with probability at least 1 一 δ with respect to the random draw of the s and
h 〜∆(H), we have
-d、A	一 、	1 一 ∕2Kln2 + 2ln(1∕δ)
IL(As) — `emp(As)I ≤ Hn) + √2δα+My	n
Theorem 2 implies that Ensemble robustness is a “weaker” requirement for the model compared
with Robustness proposed in (Xu & Mannor, 2012). To see this, consider the trade-off between the
expectation and variance of ensemble robustness on two extreme examples. When α = 0, we do
not allow any variance in the output of the algorithm A. Thus, A reduces to a deterministic one. To
achieve the above upper bound, it is required that the output hypothesis satisfies maxz∈a |'(h, Si)—
`(h, z)I ≤ (n). However, due to the intriguing property of deep neural networks (Szegedy et al.,
2014), the deterministic model robustness measure (n) (ref. Definition 2) is usually large. In
contrast, when the hypotheses variance α can be large enough, there are multiple possible output
hypotheses from the distribution ∆(H). We fix the partition of Z as C1, . . . , CK. Then,
Ea[ max ∣'(h, s) — '(h, z)∣]
z^s∈s∩Ci
P{h P{h = hj} max ∣'(hj, s) — '(hj,z)∣
≤ P{h = hj }
j∈∆(H)
max max |'(h, s) — '(h, z)∣
Z〜s∈s∩Ci h∈∆H
≤ max max |'(h, s) — '(h, z)∣.
一Z〜s∈s∩Ci h∈∆(H)
Therefore, allowing certain variance on produced hypotheses, a randomized algorithm can tolerate
the non-robustness of some hypotheses to certain samples. As long as the ensemble robustness
is small, the algorithm can still perform well. Indeed, in the following section we demonstrate
through simulations that generalization of deep learning models is more correlated with ensemble
robustness than robustness.
5	Simulations
This section is devoted to simulations for quantitatively and qualitatively demonstrating how ensem-
ble robustness of a deep learning method explains its performance. We first introduce our experiment
settings and implementation details.
5
Workshop track - ICLR 2018
5.1	Experiment Settings
Data sets We conduct simulations on two benchmarks. MNIST, a dataset of handwritten digit
images (28x28) with 50,000 training samples and 10,000 test samples (LeCun et al., 1998). NotM-
NIST1, a ”mnist like database” containing font glyphs for the letters A through J (10 classes). The
training set contains 367,440 samples and 18,724 testing examples. The images (for both data sets)
were scaled such that each pixel is in the range [0, 1]. We note that we did not use the cross-validation
data.
Network architecture and parameter setting Without explicit explanation, we use multi-layer
perceptrons throughout the simulations. All networks we examined are composed of three fully
connected layers, each of which is followed by a rectified linear unit on top. The output of the last
fully-connected layer is fed to a 10-way softmax. To avoid the bias brought by specific network
architecture on our observations, we sample at random the number of units in each layer (uniformly
over {400, 800, 1200} units) and the learning rate (uniformly over [0.005, 0.05] for SGD, and uni-
formly over [0.05, 0.5] for Bayes-by-backprop). Finally, we used a mini-batch of 128 training ex-
amples at a time.
Compared algorithms We evaluate and compare ensemble robustness as well as the generaliza-
tion performance for following 4 deep learning algorithms. (1) Explicit ensembles, i.e., using a
stochastic algorithm to train different members of the ensemble by running the algorithm multiple
times with different seeds. In practice, this was implemented using SGD as the stochastic algorithm,
trained to minimize the cross-entropy loss. (2) Implicit ensembles, i.e., learning a probability dis-
tribution on the weights of a neural network and sampling ensemble members from it. This was
implemented with the Bayes-by-backprop (Blundell et al., 2015) algorithm, a recent approach for
training Bayesian Neural Networks. It uses backpropagation to learn a probability distribution on the
weights of a neural network by minimizing the expected lower bound on the marginal likelihood (or
the variational free energy). Methods 3 and 4 correspond for adding adversarial training (Szegedy
et al., 2014; Goodfellow et al., 2015; Shaham et al., 2015) to the ensemble methods, where the
magnitude of perturbation is measured by its `2 norm and is sampled uniformly over {0.1, 0.3, 0.5}
to avoid sampling bias. From now on, a specific configuration will refer to a unique set of these
parameters (algorithm type, network width, learning rate and perturbation norm).
5.2	Empirical Ensemble Robustness and Generalization
We now present simulations that empirically validate Theorem 1, i.e., that the ensemble robustness
of a DNN (measured on the training set) is highly correlated with its generalization performance.
But empirically evaluating ensemble robustness of deep neural nets is hard for two reasons.
First, it is not clear how to define the partition of the samples into sets {Ci}iK=1. To deal with this
challenge, we use adversarial examples, and define K = n partitions such implicitly, such that
each partition contains a small `2 ball around each training example. We then approximate the loss
change in each partition using the adversarial example, i.e., approximating the maximal loss in the
partition using the adversarial example. While this approximation is loose, we will soon show that
empirically, it is correlated with generalization. We emphasize that under this partition, there is no
violation of the i.i.d assumption for general stochastic algorithms, but it is violated in the case of
adversarial training (since the adversarial examples used for training are not sampled i.i.d). Despite
the latter observation, we measured even stronger correlation for these algorithms.
Second, ensemble robustness involves taking an expectation over all the possible output hypothesis.
Hence itis computationally intractable to measure ensemble robustness for deep learning algorithms
exactly. In this simulation, we take the empirical average of robustness to adversarial perturbation
from 5 different hypotheses of the same learning algorithm as its ensemble robustness. In the case of
the SGD variants, for each configuration, we collect an ensemble of output hypotheses by repeating
the training procedures using the same configuration while using different random seeds. In the
case of the Bayes-by-backprop methods, the algorithm explicitly outputs a distribution over output
hypothesis, so we simply sample the networks from the learned weight distribution.
1http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html
6
Workshop track - ICLR 2018
Figure 1: Results for MNIST. Empirical ensemble robustness Qmp (x-axis) Vs generalization error
(y-axis). Results are given for four different deep learning algorithms.
In particular, we aim to empirically demonstrate that a deep learning algorithm with stronger en-
semble robustness presents better generalization performance (Theorem 1). Recall the definition of
ensemble robustness in Definition 3, another obstacle in calculating ensemble robustness is to find
the most adVersarial perturbation ∆s (or equiValently the most adVersarial example z = s + ∆s)
for a specific training sample s ∈ s within a partition set Ci . We therefore employ an approximate
search strategy for finding the adVersarial examples. More concretely, we optimize the following
first-order Taylor expansion of the loss function as a surrogate for finding the adVersarial example:
∆si ∈ arg max '(si) +〈▽'§“$), ∆si,	(2)
k∆sik≤r
with a pre-defined magnitude constraint r on the perturbation ∆si . In the simulations, we Vary the
magnitude r in order to calculate the empirical ensemble robustness at different perturbation leVels.
We then calculate the empirical ensemble robustness by aVeraging the difference between the loss
of the algorithm on the training samples and the adVersarial samples output by the method in (2):
1T
∈emp =于＞2 max	I'(Ast), Si)- '(Ast), Si + ∆si)∣,
T t=1 i∈{1,...,n}
(3)
with T = 10 denoting the size of the ensemble.
We emphasize that e(n) (Theorem 1) and the empirical approximation e(n)emp measure the non
robustness of an algorithm, i.e., an algorithm is more robust if e(n) is smaller.
5.3	Results
The generalization performance of different learning algorithms and different networks compared
with the empirical ensemble robustness on MNIST is giVen in Figure 1. Notice that the x-axis
corresponds to the empirical ensemble robustness (Equation 3), and the y-axis corresponds to the
7
Workshop track - ICLR 2018
test error. Examining Figure 1 we observe a high correlation between ensemble robustness and
generalization for all learning algorithms, i.e., algorithms that are more robust (have lower e(n))
generalize better on this data set.
Figure 2 in the appendix presents similar results on the notMNIST dataset, although we observe
lower (yet positive) correlation for the Bayes-by-backprop algorithm in this case. These observa-
tions support our claim on the relation between ensemble robustness and algorithm generalization
performance in Theorem 1.
We also compare ensemble robustness with robustness on MNIST in Table 1, where robustness is
measured similarly to ensemble robustness using Equation 3 but with T = 1 (while T = 10 for
ensemble robustness). Indeed, we observe that averaging over instances of the same algorithm,
exhibits a higher correlation between generalization and robustness, i.e., ensemble robustness is a
better estimation of the generalization performance than standard robustness.
Data set.	MNIST	
Metric	Robustness	Ensemble Robustness
SGD	0.978	0.995
SGD + adversarial training	0.854	0.873
BayeS-by-backprop	0.759	0.879
Bayes-by-backprop + adversarial training	0.834	0.903
Table 1: Empirical robustness vs. ensemble robustness.
6	Conclusions
In this paper, we investigated the generalization ability of stochastic deep learning algorithm based
on their ensemble robustness; i.e., the property that if a testing sample is similar to a training sam-
ple, then its loss is close to the training error. We established both theoretically and experimentally
evidence that ensemble robustness of an algorithm, measured on the training set, indicates its gen-
eralization performance well. Moreover, our theory and experiments suggest that DNNs may be
robust (and generalize) while being fragile to specific adversarial examples. Measuring ensemble
robustness of stochastic deep learning algorithms may be computationally prohibitive as one needs
to sample several output hypotheses of the algorithm. Thus, we demonstrated that by learning the
probability distribution of the weights of a neural network explicitly, e.g., via variational methods
such as Bayes-by-backprop, we can still observe a positive correlation between robustness and gen-
eralization while using fewer computations, making ensemble robustness feasible to measure.
As a direct consequence, one can potentially measure the generalization error ofan algorithm with-
out using testing examples. In future work, we plan to further investigate if ensemble robustness can
be used for model selection instead of cross-validation (and hence, increasing the training set size),
in particular in problems that have a small training set. A different direction is to study the resilience
of deep learning methods to adversarial attacks (Papernot et al., 2016). Strauss et al. (2017) recently
showed that ensemble methods are useful as a mean to defense against adversarial attacks. However,
they only considered implicit ensemble methods which are computationally prohibitive. As our sim-
ulations show that explicit ensembles are robust as well, we believe that they are likely to be a useful
defense strategy while reducing computational cost. Finally, Theorem 2 suggests that a randomized
algorithm can tolerate the non-robustness of some hypotheses to certain samples; this may help to
explain Proposition 1 in Kawaguchi et al. (2017): ”For any dataset, there exist arbitrarily unstable
non-robust algorithms such that has a small generalization gap”. We leave this intuition for future
work.
8
Workshop track - ICLR 2018
References
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look
at memorization in deep networks. arXiv preprint arXiv:1706.05394, 2017.
Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information
Processing Systems, 2013.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. Technical report, 2017.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
Olivier BoUsqUet and Andre Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2(Mar):499-526, 2002.
MoUstapha Cisse, Piotr Bojanowski, EdoUard Grave, Yann DaUphin, and Nicolas UsUnier. Parseval
networks: Improving robUstness to adversarial examples. In International Conference on Machine
Learning,pp. 854-863, 2017.
Andre Elisseeff, Theodoros EvgenioU, and Massimiliano Pontil. Stability of randomized learning
algorithms. Journal of Machine Learning Research, 6(Jan):55-79, 2005.
Yarin Gal and ZoUbin Ghahramani. DropoUt as a bayesian approximation: Insights and applications.
In Deep Learning Workshop, ICML, 2015.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Shixiang GU and LUca Rigazio. Towards deep neUral network architectUres robUst to adversarial
examples. arXiv preprint arXiv:1412.5068, 2014.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neUral network. In
NIPS Deep Learning and Representation Learning Workshop, 2014.
Prateek Jain, Vivek KUlkarni, Abhradeep ThakUrta, and Oliver Williams. To drop or not to
drop: RobUstness, consistency and differential privacy properties of dropoUt. arXiv preprint
arXiv:1503.02031, 2015.
Kenji KawagUchi, Leslie Pack Kaelbling, and YoshUa Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Yann LeCUn, Corinna Cortes, and Christopher JC BUrges. The mnist database of handwritten digits,
1998.
Colin McDiarmid. On the method of boUnded differences. Surveys in combinatorics, 141(1):148-
188, 1989.
Behnam NeyshabUr, Srinadh Bhojanapalli, and Nati Srebro. Exploring generalization in deep learn-
ing. In Advances in Neural Information Processing Systems, pp. 5943-5952, 2017.
Nicolas Papernot, Patrick McDaniel, Xi WU, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial pertUrbations against deep neUral networks. In Security and Privacy (SP),
2016 IEEE Symposium on, pp. 582-597. IEEE, 2016.
Uri Shaham, YUtaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of neUral nets throUgh robUst optimization. arXiv preprint arXiv:1511.05432, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya SUtskever, and RUslan SalakhUtdinov.
DropoUt: A simple way to prevent neUral networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
9
Workshop track - ICLR 2018
Thilo Strauss, Markus Hanselmann, Andrej Junginger, and Holger Ulmer. Ensemble meth-
ods as a defense to adversarial perturbations against deep neural networks. arXiv preprint
arXiv:1709.03423, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In
Advances in Neural Information Processing Systems, pp. 351-359, 2013.
Huan XU and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391423,
2012.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. In Advances in
Neural Information Processing Systems, 2009a.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector
machines. The Journal of Machine Learning Research, 10:1485-1510, 2009b.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. ICLR17, 2017.
10
Workshop track - ICLR 2018
Supplementary material
7	Additional simulations
Figure 2: Results for notMNIST. Empirical ensemble robustness ∈emp (x-axis) Vs generalization
error (y-axis). Results are given for four different deep learning algorithms.
8	Understanding Dropout via Ensemble Robustness
In this section, we illustrate how ensemble robustness can well characterize the performance of
Various training strategies of deep learning. In particular, we take the dropout as a concrete example.
Dropout is a widely used technique for optimizing deep neural network models. We demonstrate
that dropout is a random scheme to perturb the algorithm. During dropout, at each step, a random
fraction of the units are masked out in a round of parameter updating.
Assumption 1. We assume the randomness of the algorithm A is parametrized by r =
(r1, . . . , rL) ∈ R where rl, l = 1, . . . , L are random elements drawn independently.
For a deep neural network consisting of L layers, the random Variable rl is the dropout randomness
for the l-th layer. The next theorem establishes the generalization performance for the neural network
with dropout training.
Theorem 3 (Generalization of Dropout Training). Consider an L-layer neural network trained by
dropout. Let A be an algorithm with (K, e(n)) ensemble robustness. Let ∆(H) denote the output
11
Workshop track - ICLR 2018
hypothesis distribution of the randomized algorithm A on a training set s. Assume there exists a
β > 0 such that,
SUP SUP ∣'(As,r,z) - '(As,t,z)∣ ≤ β ≤ L-3/4,
r,t z∈Z
with r and t only differing in one element. Then for any δ > 0, with probability at least 1 - δ with
respect to the random draw ofthe S and h 〜 ∆(H),
L(hs,r) - 'emp(hs,r) ≤ 武n + ,2lθg(1∕δ)∕L + ʌ/" ln2；2ln(2/ll
Theorem 3 also establishes the relation between the depth of a neural network model and the gen-
eralization performance. It suggests that when using dropout training, controlling the variance β of
the empirical performance over different runs is important: when β converges at the rate of L-3/4,
increasing the layer number L will improve the performance of a deep neural network model. How-
ever, simply making L larger without controlling β does not help. Therefore, in practice, we usually
use voting from multiple models to reduce the variance and thus decrease the generalization error
(Hinton et al., 2014). Also, when dropout training is applied for more layers in a neural network
model, smaller variance of the model performance is preferred. This can be compensated by in-
creasing the size of training examples or ensemble of multiple models.
9 Technical Lemmas
Lemma 1. For a randomized learning algorithm A with (K, e(n)) uniform ensemble robustness,
and loss function ` such that 0 ≤ `(h, z) ≤ M, we have,
Ps ∣EA∣L(h) - 'emp(h)∣ ≤ l(n) + Mr2Kln212ln叵｝ ≥ 1 - δ,
where we use Ps to denote the probability w.r.t. the choice ofs, and |s| = n.
Proof. Given a random choice of training set s with cardinality of n, let Ni be the set of index of
points of s that fall into the Ci. Note that (|N1|, . . . , |NK |) is an i.i.d. multinomial random variable
with parameters n and (μ(Cι),..., μ(Cκ)). The following holds by the Breteganolle-Huber-Carol
inequality:
12
Workshop track -ICLR 2018
We have
EaIL(As) - 'emp(AS)I
K	n n
XEz〜“('(As, z)∣z ∈ Ci)μ(Ci) - - X'(As, Si)
(a)	ʌ	∣N-1	1 ʌ
≤ EA yX∕ Ez〜μ('(As, Z)Iz ∈ Ci)---^X '(As, Si)
--
i=1	i=1
+ EA
K
X Ez〜”('(As,z)∣z ∈
i=1
('(AS, z)|z ∈ Ci) "——
-
≤ XEA Ezi('(As,z)∣z ∈ Ci)凶一1 X '(As, Sj)
―^	n n乙』
i=1	j∈Ni
ʌ ...............................ʌ	..............∣N∣
+ EA	Ez〜μ('(As, z)∣z ∈ Ci)μ(Ci)- ɪ2 Ez〜μ('(AS, z)∖z ∈ Ci)----
i=1	i=1	n
1K	K
≤ n∑∑EA ( max ∣'(As, Sj) - '(As, ζ)∣ ) + max ∣'(As, Z)IE
n i=1 j∈N ∖zeCi	) z∈Z	i=1
(C)	A ∣Ni∣
≤ e(n) + Mf 3- μ(Ci)
n
i=1
(d) /、“ 卜2K ln2 + 2ln(1∕δ)
≤ Sn) + M∖ ----------1——
n
∣Ni∣	C
- -"(Ci)
(4)
(5)
Here the inequalities (a) and (b) are due to triangle inequality, (C) is from the definition of ensemble
robustness and the fact that the loss function is upper bounded by M, and (d) holds with a probability
greater than 1 - δ.	□
Lemma 2. For a randomized learning algorithm A with (K, e(n)) uniform ensemble robustness,
and loss function ' such that 0 ≤ '(h, ζ) ≤ M, we have,
ESIL(h) - 'emp(h)∣2 ≤ MW(n) +--------.
n
Proof. Let Ni be the set of index of points of S that fall into the Ci. Note that (∣N1∣,..., ∣Nk ∣) is
an i.i.d. multinomial random variable with parameters n and (μ(C1),... ,μ(Cκ). Then ESINk∣ =
13
Workshop track -ICLR 2018
n ∙ μ(Ck) for k = 1,..., K.
Es∖L(h) - 'emp(h)∖2
Es Ez∈Z'(h, z) —
1 n
-£'(h,Si)
i=1
2
K
n
1	] n
ES X Ez∈z'(h,z∖z ∈ Ck)μ(Ck) - - X '(h,si)
2
i=1
k=1
(XEz∈z'(h,z∖z ∈ Ck)μ(Ck)) + -2Es (X'(h,Si))
-2 (X Ez∈z'(h,z∖z ∈ Ck)μ(Ck)) Es (n X '(h, Si))
½Ez∈z'(h,z∖z ∈
Ck)μ(Ck η I X Ez∈z'(h,z∖z ∈ Ck )μ(Ck) - Es - X '(h,Si)
i=1
k = 1
+ -2Es (X '(h,Si
Ez∈z'(h,z∖z ∈ Ck)μ(Ck)) Es (1 X'(h,Si)
K	ι n
X Ez∈z'(h,z∖z ∈ Ck )μ(Ck) - Es - X '(h, Si)
k=1
i=1
sz
H
We then bound the term H as follows.
KN
∖2 EZ∈Z'(h, z∖z ∈ Ck)ES―--
n
k=1
2M2
+----
-
1
≤ -Es ]TEz∈z'(h,z∖z ∈ Ck)Nk - E '(h,sj)
k=1
1K
≤ 清 SE Nk
k=1
1K
=-TNkEs
n
k=1
≤ e(n).
j∈Ck
Sj ∈mz∈ak ∖'(h,z)- '(h，sj )∖
max	∖'(h, z) - '(h, Sj)∖
Sj ∈Ck,z∈Ck
≤ M
H
K
,
Then we have,
ES ∖L(h) - 'emp(h)∖2 ≤ ME(In) +-----.
n
□
To analyze the generalization performance of deep learning with dropout, following lemma is cen-
tral.
Lemma 3 (Bounded difference inequality (McDiarmid, 1989)). Let r = (r1,..., rL) ∈ R be L
independent random variables (rι can be vectors or scalars) with rι ∈ {0,1}mι. Assume that the
function f : RL → R satisfies:
sup I f(r(I))- f(e(I)) I ≤ Cl, ∀l = 1,...,L,
r(i) ,e(l) 1	1
14
Workshop track - ICLR 2018
whenever r(l) and er(l) differ only in the l-th element. Here, cl is a nonnegative function of l. Then,
for every > 0,
Pr{f(r1, . . . ,rL) -Erf(r1,. . . ,rL) ≥ }
≤ exp -22/ X cl2 .
10 Proof of Theorem 1
Proof of Theorem 1. Now we proceed to prove Theorem 1. Using Chebyshev’s inequality, Lemma
2 leads to the following inequality:
Prs {∣L(h) - 'emp(h)∣ ≥ e∣h} ≤ nMEs maxs∈s,z~s |丝 S)- '(h, z)| + 2M2 .
n2
By integrating with respect to h, we can derive the following bound on the generalization error:
Prs,A {∣L(h) - 'emp(h)∣ ≥ e} ≤ nMEA，s maxs∈s,z~s * S)- '(h, z)| + 2M2 .
This is equivalent to:
|L(h) - 'emp(h)∣ ≤ 4nM可 + 2M2
δn
holds with a probability greater than 1 - δ.	口
11 Proof of Theorem 2
Proof. To simplify the notations, We use X(h) to denote the random variable maxz~s |'(h, S)-
`(h, z)|. According to the definition of ensemble robustness, we have EAX(h) ≤ (n). Also, the
assumption gives var[X (h)] ≤ α. According to Chebyshev’s inequality, we have,
P {x(h) ≤ e(n) + √α= ≥ ≥ 1 - δ.
Now, we proceed to bound |L(h) - 'emp(h) | for any h 〜∆(H) output by As.
Following the proof of Lemma 2, we also divide the set Z into K disjoint set C1, . . . , CK and let
Ni be the set of index of points in ∫ that fall into Ci . Then we have,
∣L(h)- 'emp(h)∣
1	. ,	∖	2	∖ I	∕2K ln2 + 2ln(1∕δ)
≤ n E Σ max |'(h, Sj)- '(h,z)| + v--------------n---------
i=1 j∈Ni	i
，、 α	∕2Kln2 + 2ln(1∕δ)
≤ e(n)+ √δ + V ———2
holds with probability at least 1 - 2δ. Let δ be 2δ, we have,
α	2Kln2+2ln(1∕2δ)
|L(h) - 'emp(h)| ≤ e(n) + √= + ∖ -------------------
2δ	n
holds with probability at least 1 - δ. This gives the first inequality in the theorem. The second
inequality can be straightforwardly derived from the fact that var(X) = E[X2] - (E[X])2 ≤
ME[X] - (EX])2.	□
15
Workshop track - ICLR 2018
12 Proof of Theorem 3
Proof. Let R(s, r) = L(As,r) - 'emp (As,r) denote the random variable that We are going to bound.
For every r, t ∈ RL, and L ∈ N, we have
|R(s, r) - R(s, t)|
1 n
=Ez∈Z ['(As,r, Z)- '(As,t, Z)] --E ('(As,r, Zi)- '(As,t, Zi))
n i=1
1n
≤ Ez∈Z ∣'(As,r, Z) - '(As,t,z)∣ + - E ∣'(As,r, Zi)- '(As,t, Zi)| ∙
n i=1
According to the definition of β :
sup |R(s, r) - R(s, t)| ≤ 2β,
r,t
and applying Lemma 3 We obtain (note that s is independent of r)
Pr {R(s, r) - ErR(s, r) ≥ |s} ≤ exp
-C
2Lβ2
We also have
EsPr {R(s, r) - ErR(s, r) ≥ }
EsPr {R(s, r) - ErR(s, r) ≥ |s} ≤ exp
2⅛) ∙
Setting the r.h.s. equal to δ and Writing as a function of δ, We have that With probability at least
1 - δ W.r.t. the random sampling of s and r:
R(s, r) - ErR(s, r) ≤ β√2L log(1∕δ).
Then according to Lemma 1:
□ 〜	、	∕2Kln2 + 2ln(1∕δ)
ErR(s, r) ≤ <≡(n) + ∖ -----------------
n
holds With probability greater than 1-δ. Observe that the above tWo inequalities hold simultaneously
With probability at least 1 - 2δ. Combining those inequalities and setting δ = δ∕2 gives
R(s,r) ≤ βp2Llog(1∕δ) + e(n) + J2Kln2 + 2ln2/^.
n
□
16