Under review as a conference paper at ICLR 2018
No Spurious Local Minima in a Two Hidden
Unit ReLU Network
Anonymous authors
Paper under double-blind review
Ab stract
Deep learning models can be efficiently optimized via stochastic gradient descent,
but there is little theoretical evidence to support this. A key question in optimization
is to understand when the optimization landscape of a neural network is amenable
to gradient-based optimization. We focus on a simple neural network: RELU
network with one hidden layer consisting of two RELU units, and show that all
local minimizers are global. This combined with recent work of Lee et al. (2017);
Lee et al. (2016) show that gradient descent converges to the global minimizer.
1	Introduction
Deep learning has been used to achieve state-of-art performance on a wide variety of problems in
machine learning, artificial intelligence, computer vision, and natural language processing. In all
these applications, deep models often use hundreds of millions of parameters and are trained with
stochastic gradient descent (or other gradient-based methods such as Adagrad (Duchi et al., 2011),
Adam (Kingma and Ba, 2014)), a surprisingly simple method, and yet finds solutions with both low
train and test error.
Despite the empirical success, the mathematical justification for gradient-based methods is not
well-understood. Zhang et al. (2016a) empirically demonstrated that sufficiently over-parametrized
networks can be efficiently optimized to near global optimality with stochastic gradient. For a
two-layer network with leaky ReLU activation, Soudry and Carmon (2016) showed that gradient
descent on a modified loss function can obtain a global minimum of the modified loss function;
however, this does not imply reaching a global minimum of the original loss function. Under the same
setting, Xie et al. (2016) showed that critical points with large “diversity" are nearly globally optimal.
Choromanska et al. (2015) used several assumptions to simplify the loss function to a polynomial
with i.i.d. Gaussian coefficients. They then showed that every local minima of the simplified loss has
objective value comparable to the global minima. Kawaguchi (2016) used similar assumptions to
show that all local minimum are global minimum in a nonlinear network. However the assumptions
of Choromanska et al. (2015); Kawaguchi (2016) require independent activations, meaning that the
activations of the hidden units are independent of the input and/or mutually independent, which is
violated in practice.
Multiple works have been proposed to circumvent this assumption when dealing with the two-layer
ReLU network F (x; W) = PjK=1 σ(wjT x), where σ = max(0, x) is the ReLU activation function.
Under the realizable setting (i.e. the labels are generated from a network with “teaching” parameters
w*) and isotropic Gaussian input, Tian (2017) shows that when there is only a single ReLU node
gradient descent converges to the global optimum. For K = 2, he conjectured that there are no
spurious local minima, and provided a partial characterization of the critical point structure. With the
same assumptions, Brutzkus and Globerson (2017) proved, for a two-layer ReLU network with a
single non-overlapping convolutional filter, all local minimizers are global. Zhang et al. (2017a) show
that for two-layer networks with non-standard activation functions that gradient descent converges to
global minimizers.
In this paper, we focus on the case when K = 2 and prove that every local minimum is global. As
in previous works (Brutzkus and Globerson, 2017; Tian, 2017; Hardt and Ma, 2016), we focus on
the population loss. The ReLU function is positive homogeneous, so we can rewrite the function
as F(x; W) = v1σ(w1T x) + v2σ(w2Tx) where w1 and w2 are unit vectors; for simplicity, we will
1
Under review as a conference paper at ICLR 2018
assume that v1 = v2 = 1. Using these assumptions and an additional orthogonality assumption, we
prove that all local minima of the loss surface are global. Although the setting is a simplification of
practical neural networks, this is a meaningful step towards understanding the success of gradient-
based methods in deep learning and other non-convex optimization problems. For the non-orthogonal
case, we provide a partial characterization of the critical point structure.
The paper is organized as follows: Section 2 discusses related works, and Section 3 introduces the
notation and definitions. Section 4 shows our main result that all local minima are global and gives
a proof sketch and the formal proofs are in Section 5. Section 6 provides some extensions to the
non-orthogonal case. Section 7 presents the result of the experiments, and finally, Section 8 concludes
the paper.
2	Related Work
Single Hidden Node Networks: For a neural network with a single hidden unit and monotone
activation function σ, numerous authors (Mei et al., 2016; Hazan et al., 2015; Kakade et al., 2011;
Kalai and Sastry, 2009; Soltanolkotabi, 2017; Tian, 2017) have shown that gradient-based methods
converge to the true parameter w*. In the case of a single hidden unit, the loss function is weakly
quasi-convex, meaning that the gradient points in the direction of w*, which explains the success of
gradient-based methods. For K > 1 hidden units, the loss function is no longer quasi-convex, so this
analysis does not easily generalize. In fact, our analysis for K = 2 is considerably more involved,
and requires analyzing the gradient and hessian simultaneously.
Improper Learning: On the improper learning side, Shalev-Shwartz et al. (2011) pioneered a
kernel-based approach that can be used for learning a single halfspace or smoothed ReLU. This was
generalized to fully-connected deep neural networks in Zhang et al. (2016b) using the recursive kernel
method. Goel et al. (2016) designed a new smoothed ReLU function that is a better approximation to
the ReLU. Instead of learning a neural network, these methods learn a function in a RKHS, hence
improper learning. Zhang et al. (2017b) improved upon this by learning a neural network, instead of a
kernel machine, via a boosting approach, and with much lower sample complexity. The disadvantages
of improper learning are two-fold: 1) the sample complexity for these methods is exponentially larger
than the Rademacher complexity of the network, and 2) the practical success of deep learning is
intricately tied to using gradient-based training procedures, and the learnability of these networks
using improper learning does not explain the success of gradient-based methods. On a related line of
work, Janzamin et al. (2015) propose a method of moments estimator using tensor decomposition.
Over-Parametrization There have been several works on studying the effect of over-parametrization
on the training of neural networks (Poston et al., 1991; Haeffele and Vidal, 2015). These results
require the width of a hidden layer to be greater than the number of training samples, which is not
the case for commonly used networks. Finally, Zhang et al. (2016a) empirically demonstrated that
commonly used over-parametrized networks can be efficiently optimized to near global optimality
with stochastic gradient descent.
Non-Convex Optimization: Since the loss function of neural networks is non-convex, the theory of
training neural networks is closely related to the theory of non-convex optimization. Recently, there is
considerable progress on convergence guarantees of first-order and second-order methods, including
some applications in machine learning problems. Lee et al. (2016) and Lee et al. (2017) show gradient
descent and other first-order methods converge only to local minima, and not saddle points. Jin
et al. (2017) and Ge et al. (2015) show that variants of stochastic gradient method converge to local
minimizers in polynomial time. Ge et al. (2016) and Ge et al. (2017) show there is no spurious local
minima in matrix completion problem and non-convex low rank problems. For the phase retrieval
problem, Sun et al. (2016) show that there is no spurious local minimum.
3	Preliminaries
We study a simple two RELU hidden node network with output function
F (x; w) = σ(w1T x) + σ(w2T x).
2
Under review as a conference paper at ICLR 2018
For the duration of this paper, we will assume that x is standard normal in Rn and all expectations
are with respect to the standard normal. The population loss function is:
L(x, W) = 2E[(F(x, W) - F(x, W*))2].	(1)
Define
g(v1,v2) = E[σ(v1Tx)σ(v2Tx)],	(2)
so the loss can be rewritten as (ignoring additive constants, then multiplied by 4):
f (W) = X (g(Wi,wj) - 2g(wi,w*)).	⑶
i,j∈{1,2}
From Brutzkus and Globerson (2017) we get
g(u,v) = 2∏ Ilukkvk(Sinθu,v - (π - θu,v)cosθu,v).	(4)
and
M = τr kvk i∣-μ Sin θu,v + ^~(π - θu,v )v.	(5)
∂u	2π	kuk	2π
In this paper, we study the landscape of f over the manifold R = {kw1 k = kw2 k =1}. The
manifold gradient descent algorithm is:
Xk+1 = PR(Xk - α^Rf (xk)),
where PR is the orthogonal projector onto the manifold R, and Vr is the manifold gradient of f .
4	Main Result and Proof S ketch
In order to analyze the global convergence of manifold gradient descent, we need a characterization
of all critical points. We show that f(W) have no spurious local minimizer on the manifold R.
Theorem 4.1.	Assume ||w；k = ∣∣W2k = 1 and w；Tw2 = 0, then there is no spurious localminimizer
of the objective function (3) on the manifold R = {kw1 k = kw2 k =1}. Furthermore, every saddle
point or local maximizer has a direction of negative curvature.
The next theorem shows that manifold gradient descent with random initialization converges to the
global minimizer
Theorem 4.2.	With probability one, manifold gradient descent will converge to the global minimizers.
Proof. The objective function f is infinitely differentiable on manifold R. Using Corollary 6 of Lee
et al. (2017), manifold gradient descent will converge to a local minimizer with probability one. Since
the only local minima for function f are w1 = w1；, w2 = w2； and w1 = w2；, w2 = w1；, manifold
gradient descent converges to the true solutions.	□
Proof of Theorem 4.1. The proof of the main result is complicated, so let’s start with a simpler case,
in which both w1 and w2 are in Span{w1；, w2；}.
Proposition 4.3. Assume kw1； k = kw2； k = 1, w1；T w2； = 0 and w1, w2 ∈ Span{w1；, w2；}, then there
is no spurious local minimizer of the objective function (3) on the manifold R = {kw1k = kw2k = 1}.
Furthermore, every saddle point or local maximizer has a direction of negative curvature.
The complete proof is given in Appendix B and C, so here we just give a proof sketch.
To prove this, we need some observations. The first important observation is that we are always on
manifold {kw1 k = kw2k = 1}, and for each vector in the plane with fixed norm, there is only one
degree of freedom, which means we can express each vector with only one variable. Thus, we can
express the vectors in polar coordinates, where θ1 and θ2 are the angles for w1 and w2 .
3
Under review as a conference paper at ICLR 2018
The second observation is we only need to compute the gradient on the manifold and check whether
it's zero. Define m(wι) = Sin θι f----cos θι -f- and m(w2) = Sin θ2 TSdf------Cos θ2 f—. Then
∂w11	∂w12	∂w21	∂w22
for w1 and w2, the norm of the manifold gradients are |m(w1 )| and |m(w2)|. Thus, we only need to
check whether the value of function m is 0 and get rid of the absolute value sign.
Then we apply the polar coordinates onto the manifold gradients, and obtain:
m(w2) = 1(∏ - θw1,w2)sin(θ2 - Θ1) +cosθ2 - sinθ2
+ ∏ (θW2,w* sin θ2 - θw2,w* CoS θ2).
(6)
(7)
The last observation we need for this theorem is that we must divide this problem into several cases
because each angle in (300) is a piecewise linear function. If we discuss each case independently, the
resulting functions are linear in the angles. The details are in Appendix B. After the calculation of all
cases, we found the positions of all the critical points: WLOG assume θ1 ≤ θ2, then there are four
critical points in the 2D case: (θ1 ,θ2) = (0, 2), (4, 4), (4, 5∏) and (5∏, 5∏).
After finding all the critical points, we compute the manifold Hessian matrix for those points and
show that there is a direction of negative curvature. The details can be found in Appendix C.
The next step is to reduce it to a three dimensional problem. As stated in the two-dimensional case,
the gradient is in span{w1 ,w2,wɪ ,w2}, which is four-dimensional. However, using the following
lemma, we can reduce it to three dimensions and simplify the whole problem.
Lemma 4.4. If (w1 , w2) is a critical point, then there exists a set of standard orthogonal basis
(eι, e2, e3) such that eι = w；, e? = w2 and w1,w2 lies in span{eι, e2, e3}.
Even if we simplify the problem into three dimensional case, it still seems to be impossible to identify
all critical points explicitly. Our method to analyze the landscape of the loss surface is to find the
properties of critical points and then show all saddle points and local maximizers have a direction of
negative curvature.
The following two lemmas captures the main geometrical properties of the critical points in three
dimensional case. More detailed properties are given is Section 5.2
Lemma 4.5.
arCCos(-w11 )	arCCos(-w12 )	w23
arCCos(-w21 )	arCCos(-w22)	w13
(8)
The ratio in Lemma 4.5 captures an important property of all critical points. For simplicity, based on
D.5, We define ko = -k, θι = π - θw2,w; and θ2 = π - θw2,w*. Then
π - θwι,wi = k0θ1
π - θwι,w2 = k0θ2 .
(9)
(10)
Then from the properties of θ1 , θ2 and upper bound the value of k0 we get
Lemma 4.6. θ1 = θ2.
That lemma shows that w1 and w2 must be on a plane whose projection onto span{w1； , w2； } is the
bisector of w1； and w2；. Combining this with the computation of Hessian, we conclude that we have
found negative curvature for all possible critical points, which leads to the following proposition.
Proposition 4.7. Assume kw1； k = kw2； k = 1, w1；T w2； = 0 and ∃i ∈ [2], wi ∈/ span{w1；, w2；}, then
there isno spurious local minimizer of the objective function (3) on the manifold {kw1 k = kw2 k = 1}.
Furthermore, every saddle point or local maximizer has a direction of negative curvature.
Combining both Propositions 4.3 and 4.7, we have proved Theorem 4.1, which is the main result of
this paper.	□
4
Under review as a conference paper at ICLR 2018
5 Proofs
Here we provide some detailed proofs which are important for the understanding of the main theorem.
5.1	Why we only need 3 dimension
In general case, the following lemma shows we only need three dimension.
Lemma 5.1. If (w1, w2) is a critical point, then there exists a set of standard orthogonal basis
(eι, e2, e3) such that eι = w；, e? = wg and w1,w2 lies in span{eι, e2, e3}.
Proof. If (w1, w2) is a critical point, then
(I - WIWT) @； = 0.	(II)
where matrix (I - w1w1T) projects a vector onto the tangent space of w1. Since
(I - w1 w1T )w1 = w1 - w1 = 0,	(12)
we get
(I - WIWT) Jf	(13)
∂w1
=1(I	-	WiWT) ((π	-	θw1,w2)W2	-	(π -	θwι,wi )w；	-	(π	-	θwι,w* )wg) ,	(14)
which means that (π - θw1,w2 )w2 - (π - θwι,w* )w； - (π - θwι,w* )w； lies in the direction of wi.
If θw1,w2 = π, i.e., W1 = -W2, then of course the four vectors have rank at most 3, so we can find
the proper basis. If θw1,w2 < π, then we know that there exists a real number r such that
(∏ - θw1,w2 )W2 - (π - θwι,w)W； - (n - θwι,w )W； + r ∙ Wi =0∙	(15)
Since θw1,w2 < π , we know that the four vectors W1, W2, W1； and W2； are linear dependent. Thus, they
have rank at most 3 and We can find the proper basis.	□
5.2	Some properties of Critical Points
Next we will focus on the properties of critical points. Assume (Wi, W2) is one of the critical points,
from lemma D.1 we can find a set of standard orthogonal basis (ei, e2, e3) such that ei = Wi；,
e2 = W2； and Wi, W2 lies in span{ei, e2, e3}. Furthermore, assume Wi = Wiiei + Wi2e2 + Wi3e3
and W2 = W2iei + W22e2 + W23e3, i.e., Wi = (Wii, Wi2, Wi3) and W2 = (W2i,W22, W23). Since
we have already found out all the critical points when Wi3 = W23 = 0, in the following we assume
Wi23 + W223 6= 0.
First, we give the fundamental equation in our analysis.
Lemma 5.2.
arccos(-Wii)	arccos(-Wi2)	W23
arccos(-W2i)	arccos(-W22)	Wi3
(16)
Proof. Adapting from the proof of lemma D.4 and we know that
π-θwι ,w* 1	π-θwi ,w* 2
21	∏-θw1,w2 _	2	∏-θw1,w2	W23	κ
Wii	—	—	—k. Wi2	Wi3
(17)
Similarly, we have
π-θW2,w*	π-θw2,w*		
Wi1 - ∏-θw1,w2	二 W12 - ∏-θw1,w2	_ W13 _	1
W2i	W22	W23	=k
(18)
5
Under review as a conference paper at ICLR 2018
Taking the first component of (217) and (218) gives us
Thus,
Similarly, we get
π - θwι,w1
W21 = k ∙ W11 +----二^------
π - θw1 ,w2
1	Irn - θW2,W^
W21 = k ∙ W11 一 k-----h-----
π - θw1 ,w2
π - θWι,W*
π 一 θW2,W*
-k.
π - θwι,w2
π - θW2,W*
-k.
Since ∀i,j ∈ [2], π -。3名,3： = arccos(一。仪切)，We know that
arccos(-w11)	arccos(-w12)	w23
arccos(-w21)	arccos(-w22)	w13
(19)
(20)
(21)
(22)
(23)
□
Using this equation, we obtain several properties of critical points. The following two lemmas show
the basic properties of critical points in three dimensional case. Completed proofs are given in
Appendix B and C.
Lemma 5.3. θw1,w2 < π.
Lemma 5.4. w13 * w23 < 0.
These two lemmas restrict the position of critical points in some specific domains.
Then we construct a new function F in order to get more precise analysis. Define k0
θ1 = π 一 θW2,wf and θ2 = π 一 θw2,w* .
F (θ) =-------~-0θ——一.
k0 cos(k0θ) + cos(θ)
From the properties of that particular function and upper bound the value of k0 we get
Lemma 5.5. θ1 = θ2.
-k,
(24)
That lemma shows that wι and w2 must be on a plane whose projection onto span{w；,w2} is the
bisector of wɪ and wg. Although we cannot identify the critical points explicitly, we will show these
geometric properties already capture the direction of negative curvature.
6	Analysis of Critical Points for NON-ORTHOGONAL W *
In this section, we partially characterize the structure of the critical points when w；, Wg are non-
orthogonal, but form an acute angle. In other words, the angle between w； and w； is α ∈ (0, ∏).
Let us first consider the 2D cases, i.e., both w1 and w2 are in the span of w1； and w2；. similar to the
original problem, after the technique of changing variables(i.e., using polar coordinates and assume
θ1 and θ2 are the angles of w1 and w2 in polar coordinates), we divide the whole plane into 4 parts,
which are the angle in [0, α], [α, π], [π, π + α] and [π + α, 2π). We have the following lemma:
Lemma 6.1. Assume kw1； k = kw2； k = 1, w1；T w2； > 0 and w1, w2 ∈ span{w1；, w2；}. When w1 and
w2 are in the same part(one of four parts), the only critical points except the global minima are those
when both w1 and w2 are on the bisector of w1； and w2； .
Proof. The complete proof is given in appendix E, the techniques are nearly the same as things in the
original problem and a bit harder, so to be brief, we omit the proof details here.	□
For the three-dimensional cases cases of this new problem, it’s interesting that the first few lemmatas
are still true. specifically, Lemma D.1(restated as Lemma 4.4) to Lemma D.5(restated as Lemma 4.5)
are still correct. The proof is very similar to the proofs of those lemmas, except we need modification
to the coefficients of terms in the expressions of the manifold gradients.
6
Under review as a conference paper at ICLR 2018
120
'0Ooo,0
0 8 6 4 2
eE-eɔol Sno二nd/
23456789	10	11
Number of Nodes K
Figure 1: Spurious Local Minima for K ≥ 2 ReLU Network.
7	Experiments
We did experiments to verify the theoretical results. Since our results are restricted to the case of
K = 2 hidden units, it is also natural to investigate whether general two-layer ReLU networks also
have the property that all local minima are global minima. Unfortunately as we show via numerical
simulation, this is not the case. We consider the cases of K from 2 to 11 hidden units and we set the
dimension d = K . For each K, the true parameters are orthogonal to each other. For each K, we
run projected gradient descent with 300 different random initializations, and count the number of
local minimum (critical points where the manifold Hessian is positive definite) with non-zero training
error. If we reach a sub-optimal local minimum, we can conclude the loss surface exhibits spurious
local minima. The bar plot showing the number of times gradient descent converged to spurious local
minima is in Figure 1. From the plot, we see there is no spurious local minima from K = 2 to K = 6.
However for K ≥ 7, we observe a clear trend that there are more spurious local minima when there
are more hidden units.
8	Conclusion and Future Work
In this paper, we provided recovery guarantee of stochastic gradient descent with random initialization
for learning a two-layer neural network with two hidden nodes, unit-norm weights, ReLU activation
functions and Gaussian inputs. Experiments are also done to verify our results. For future work, here
we list some possible directions.
8.1	General Case of Networks
This paper focused on a ReLU network with only two hidden units, . And the teaching weights must
be orthogonal. Those are many conditions, in which we think there are some conditions that are not
quite essential, e.g., the orthogonal assumption. In experiments we have already seen that even if they
are not orthogonal, it still has some good properties such as the positions of critical points. Therefore,
in the future we can further relax or abandon some of the assumptions of this paper and preserve or
improve the result we have.
8.2	Bad Local Minima
The neural network we discussed in this paper is in some sense very simple and far from practice,
although it is already the most complex model when we want to analyze the whole loss surface. By
experiments we have found that when it comes to seven hidden nodes with orthogonal true parameters,
7
Under review as a conference paper at ICLR 2018
there will be some bad local minima, i.e., there are some local minima that are not global. We believe
that research in this paper can capture the characteristics of the whole loss surface and can help
analyze the loss surface when there are three or even more hidden units, which may give some bounds
on the performance of bad local minima and help us understand the specific non-convexity of loss
surfaces.
References
A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.
International Conference on Machine Learning (ICML), 2017.
A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer
networks. In AISTATS, 2015.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal ofMachine Learning Research, 12(Jul):2121-2159, 2011.
R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In Advances in
Neural Information Processing Systems, pages 2973-2981, 2016.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. arXiv:1503.02101, 2015.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial
time. arXiv preprint arXiv:1611.10258, 2016.
Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. arXiv preprint arXiv:1506.07540, 2015.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
optimization. In Advances in Neural Information Processing Systems, pages 1594-1602, 2015.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity:
Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473,
2015.
C.	Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efficiently.
arXiv preprint arXiv:1703.00887, 2017.
Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized
linear and single index models with isotonic regression. In Advances in Neural Information
Processing Systems, pages 927-935, 2011.
Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression.
In COLT, 2009.
K. Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information
Processing Systems, pages 586-594, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
J. D. Lee, I. Panageas, G. Piliouras, M. Simchowitz, M. I. Jordan, and B. Recht. First-order methods
almost always avoid saddle points. ArXiv e-prints, 2017.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges
to minimizers. University of California, Berkeley, 1050:16, 2016.
8
Under review as a conference paper at ICLR 2018
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses.
arXiv preprint arXiv:1607.06534, 2016.
Timothy Poston, C-N Lee, Y Choie, and Yonghoon Kwon. Local minima and back propagation. In
Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, volume 2, pages
173-176. IEEE, 1991.
Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with
the 0-1 loss. SIAM Journal on Computing, 40(6):1623-1646, 2011.
Mahdi Soltanolkotabi. Learning relus via gradient descent. arXiv preprint arXiv:1705.04591, 2017.
D.	Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. arXiv preprint arXiv:1602.06664,
2016.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. International Conference on Machine
Learning (ICML), 2017.
Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. arXiv
preprint arXiv:1611.03131, 2016.
C. Zhang, S.y Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. arXiv preprint arXiv:1611.03530, 2016a.
Q. Zhang, R. Panigrahy, S. Sachdeva, and A. Rahimi. Electron-proton dynamics in deep learning.
arXiv preprint arXiv:1702.00458, 2017a.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly
learnable in polynomial time. In International Conference on Machine Learning, pages 993-1001,
2016b.
Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of fully-
connected neural networks. In Artificial Intelligence and Statistics, pages 83-91, 2017b.
9
Under review as a conference paper at ICLR 2018
A Preliminaries
Consider a neural network with 2 hidden nodes and ReLU as the activation function:
F(x) = σ(wTX) + σ(WTX),	(25)
where σ(x) = max(0, x) is the ReLU function.
First we study the 2-D case, i.e., the input and all parameters are two dimensional. Assume that the
input follows standard normal distribution.
The loss function is population loss:
l(W) = Ex
σ(wT x) + σ(wT x)	σ(w^T x) + σ(w2T x)
-----------------------------------
2	2
Define
g(u, v) = Ex σ(uT x)σ(vT x) ,
then from Brutzkus and Globerson (2017) we get
g(u,v) = 2∏ Ilukkvk(Sinθu,v - (π - θu,v)cosθu,v).
Thus,
∂g 1 u	1
∂u = 2∏ kvk H θu,v +2∏(π	u,v)v.
Moreover, from (26) we get
I(W) = 1 X (g(wi,wj)- 2g(wi,wj) + g(w*,w*)).
i,j∈[2]
(26)
(27)
(28)
(29)
(30)
Assume ∣∣wjk = I∣w2 k and WiTWg = 0. WLOG, let eι = Wi and e2 = wg. Then We know that
∀i,j ∈ [2], g(w*,w*) is a constant number. Thus, define the objective function(which equals to
4l(W) up to an additive constant)
f(W) = g(W1, W1) + g(W2, W2) + 2g(W1, W2) - 2	g(Wi, Wji).	(31)
			i,j∈[2]		
Thus,	∂f ∂W1	1 =W1 +- ― π	IIw2∣I ∣w^ k Sin θWl,W2 + ∏ (π - θWl,W2	)W2	(32)
		-IkWik π	H~^Γ7 sin θwι,wf	(π - θwι,w* )w1 kW1k	1 1 π	1 1 1		(33)
		-IkWik π	∣w1 k sin θwι ,w2 - ∏ (π - θwι ,w2 )w2		(34)
		1 =W1 -- 一 π	∣∣w2∣∣ ∣w1∣ sin θw1,w2 + ∏ (π - θw1,w2	)W2	(35)
		1 W1 一 π kW1k	sin θwι w1 - ∏ (π - θwι,w* )w1		(36)
		1 W1 ———		 π kW1k	sin θwι ,w2 - ∏(π - θwι,w* )w2∙		(37)
Similarly, for W2, the gradient is					
	∂f ∂W2	1 =W2 -- 一 π	W2	1 IIw1II ∣w k sin θWl,W2 + ∏ (π - θWl,W2	)W1	(38)
		1 W2 ———		 π kW2k	sin θW2 w1 - ∏ (n - θW2,W* )w1		(39)
		1 W2 — π kW2k	sin θW2 ,w2 - ∏ (n - θw2,w* )w2∙		(40)
10
Under review as a conference paper at ICLR 2018
Assume that w1 = (w11, w12) and w2 = (w21, w22), then the gradient can be expressed in this form:
and
∂f ∂w11	1 kw2 k =w11 + ∏ 时 w11	sin θw1	1 ,W2 + ^ 一(n π	- θw1,w2 )w21	(41)
	-1 产『sin θwι,w; π kw1k	1 1	—ɪ(n π	-θwι,w1 )		(42)
	1 w11 	η ∏- sinθwι,w2 π kw1k	1 2				(43)
∂f ∂W12	1 kw2 k =w12 + π 西 w12	sin θw1	1 ,W2 + ∏ (π	- θw1,w2 )w22	(44)
	1 w12 	η	∏- sin θwι,w1 π kw1k	1 1				(45)
	1 w12 	η [7 sinθwι,w2 π kw1k	1 2	-1(∏ π	-θw1 ,w∖ ) .		(46)
Because of symmetry, for w2, the gradient is
df —	, 1 Ilwlk 百=w21+ π 西 w21	sin θW1,W2 + ∏ (n - θWι ,W2)W11	(47)
1 w21 -II	II Sin θW2,wj π Iw2I	2 1		(n - θW2,W* ) π1	(48)
1 w21 	η	∏- sin θw2,w2 π Iw2I	2 2		(49)
and
∂f 一，ɪ 1 kwιk,,”	h ∂w— = w22 + ∏ Iw k w22 sin θW1,W2 + ∏ (π - θWι ,W2 )w12 1 w22 -II	II sin θW2,W? π Iw2 I	2 1 1 w22	1 	∏	∏^ sin θW2 ,W2	(n - θW2 ,W* ) . π Iw2 I	2 2 π	2 2	(50) (51) (52)
B Critical Points in 2D Cases
B.1 2D Preliminaries
In 2D cases, we can translate W to polar coordinates and fix kw1 k = kw2 k = 1, so there are two
variables left: θ1 and θ2, i.e., w1 = (cos θ1, sin θ1) and w2 = (cos θ2, sinθ2).
For manifold gradient, we only need to consider its norm and check whether it’s zero. For w1 and w2,
the (directed) norm of manifold gradients(eXPressed by m) are m(wι) = Sin θι ∂f - Cos θι ∂f
and m(W2) = sin θ2∂fι - Cos θ2∂f2.
11
Under review as a conference paper at ICLR 2018
To make life easier, it’s better to simplify the m functions a bit using w1 = (cos θ1, sin θ1) and
w2 = (cos θ2, sinθ2):
(、.A ∂f	Adf
m(wι) = Sin θι--CoS θι~-
∂ w11	∂ w12
=Sin θι ( coS θι + ɪ coS θι Sin θwι ,仅？
+ 1(π - θw1,w2 )cos θ2
1	θw1 w*
-π CoS θ1Sin θw1,w* - 1 +
1 CoS θι Sin θwι,w2
一Cos θι ( Sin θι + — Sin θι Sin θwι ,仅？
+ 1(π - θw1,w2 )Sin θ2
-1Sin θ1 Sin θw1,w1 -1 + θwπw^
1Sin θ1 Sin θwι,w2
=1(π - θw1,w2)Sin(θι - θ2) +coSθι - Sinθι
+ ∏ (θwι,w* Sin θ1 - θwι,w* CoS θl).
(53)
(54)
(55)
(56)
(57)
(58)
(59)
—
—
Similarly,
m(w2) = 1(∏ - θw1,w2) Sin(θ2 - θι) +CoS θ2 - Sin θ2
+ ∏ (θW2,w* Sin θ2 - θw2,w* CoS θ2).
(60)
(61)
Then we can divide them into several cases and analyze them one by one to specify the positions and
properties of the critical points.
WLOG, assume θ1 ≤ θ2 .
B.2 0 ≤ θl ≤ θ2 ≤ 2
The norm of the manifold gradient w.r.t. w1 is
	m(wι) = ɪ(n - θ2 + θι) Sin(θι - θ2) + CoS θι - Sin θι	(62) π + ∏ (θι Sin θι - (2 - θι) CoS θι) .	(63)
Similarly, the norm of m(w2 ) is
	m(w2) = — (π - θ2 + θι) Sin(θ2 — θι) + CoS θ2 — Sin θ2	(64) + ∏ (θ2 Sin θ2 - (2 - θ2) CoS θ) .	(65)
Define	hι(θ) = CoSθ — Sinθ + ɪ (θSinθ — (∏ — θ) CoSθ) .	(66)
If m(w1 )	m(w2) = 0, then hι(θι) = 1(π - θ2 + θι) Sin(θ2 - θι)	(67) π
and	h1(θ2) = 1(∏ - θ2 + θ1)Sin(θ1 - θ2).	(68) π
12
Under review as a conference paper at ICLR 2018
Thus,
		h1(θ1) + h1(θ2)	= 0.					(69)
Note that when 0 ≤ θ ≤ -,								
h01(θ)	π 2 =-	-1 + θ Sin θ - π	-1 -	θ 	 CCq H / cos <		0.		(70)
		SLn - π	-					
Also note that								
π h1(θ) + h1(2	- θ) =	cos θ — Sin θ + — π	θ sin	θ-	U - 12	θ	cos θ	(71)
	+	cos ∏- - θ) - sin π— -		θ				(72)
		1π		∖	θ cos		- θ	
	+	-((--θ) sin (	- ——θ	-		-		(73)
		-2 cos θ — sin θ +——	2	)		2	JJ	
	=		θ sin	θ-	(--	θ	cos θ	(74)
		- sin θ — cos θ + —	∖		2	)	)	
	+		((--	θ	cos θ	-θ	sin θ	(75)
			2	)			)	
	=	- 0.						(76)
Thus, if m(wι) = m(w2) = 0, then θι + θ2 = ∏. From θι ≤ θ2 We know that θι ≤ ∏. Plug
θ2 = 2 - θι into (63) and we get
,	、	,	/ c、	2θι +π / c、
m(wι) = 0 ⇔ h1(θ1) =------------ cos(2θι).	(77)
π
LemmaB.1. If 0 ≤ θ ≤ ∏, then
,2	2θ + ∏	,.
hι(θ) ≤ -------- cos(2θ)	(78)
π
and the inequality becomes equality only then θ = 0 or θ = ∏.
Proof. When 0 ≤ θ ≤ π
2θ + ∏ 			 — hι(θ) - =Q+2θ) cos(2θ)+(1 - -) sin θ -Q+θ) CoS θ ≥ Q+-) cos(2θ)+(1 - -) sin θ - Q+-) CoS θ 1θ	3	1θ ≥ (2 + -) cos(2θ) + 4sinθ - (2 + -)cosθ =1 ɪ + —)(cos(2θ) — CoS θ) + - sin θ 2-	4 ≥ I (cos(2θ) — cos θ) + I sin θ 3 =W (cos(2θ) — (cosθ — sinθ)) 3 =-(cos2 θ — Sin2 θ — (cos θ — sin θ)) 3 =-(cos θ — sin θ) (cos θ + sin θ — 1) ≥ 0.	(79) (80) (81) (82) (83) (84) (85) (86) (87) (88)
Note that (84) is because cos(2θ) - cos θ is always non-positive when 0 ≤ θ ≤ ∏.
From (81), the inequality becomes an equality only when θ cos(2θ) = 0, which means that the only
possibilities are θ = 0 or θ = ∏. After plugging in those two possibilities in (78), we know that
h(θ) = 2θ+2 cos(2θ) holds when θ = 0 or θ = 4.	□
13
Under review as a conference paper at ICLR 2018
Using the above lemma, We conclude that m(wι) = 0 iff θι + θ2 = ∏ and θ = 0 or ∏, i.e.,
m(wι) = 0 iff (θι, θ2) = (0, 2) or (θ1,θ2) = (π, π).
In a word, there are two critical points in this case: (θ1,θ2) = (0, ∏) and (θ1,θ2) = (∏, ∏).
B.3	2 ≤ θι ≤ θ2 ≤ ∏
The norm of the manifold gradient w.r.t. w1 is
m(wι) = 1(∏ - θ2 + θ1)sin(θ1 - θ2) +		cos θ1	- sin θ1	(89)
+( (θι sin θι	-(θι - 2) cos θι	).		(90)
Similarly,				
m(w2) = }(∏ - θ2 + θl)sin(θ2 - θι) +		cos θ2	- sin θ2	(91)
+ 1 (θ2 sin θ2	-(θ2 - 2) cos θ2	).		(92)
Define				
h2 (θ) = cos θ — sin θ +——(θ sin θ — (θ		-2)cos θ),		(93)
Let θ0 = θ - 2, then				
h2(θ) = h2 (θ0 + 2)				(94)
=—sin θ0 — cos θ0 +( (	(θ0 + 2) sin (θ0 +	2)-	θ0cos (θ0+2))	(95)
=—sin θ0 — cos θ0 + ɪ (	(θ0 + § cos θ0 + θ0 sin θ0		)	(96)
=-sin θ0+1 ((θ0 - 2：	cos θ0 + θ0 sin θ0			(97)
=-sin θ0 + 1 (θ0 sin θ0 -	-(2 -θ0)cos θ0)			(98)
= h1 (θ0) - cos θ0				(99)
				(100)
Lemma B.2. When θ ∈ [∏ ,π],				
h2(θ) ≤ -2,				(101)
and the inequality becomes equality only then θ = ∏ or θ = π.				
Proof. Let θ0 = θ - ∏, then θ0 ∈ [0, ∏ ] and
h2(θ) = h1(θ0) - cos θ0	(102)
=-Sin θ0 + 1 (θ0 Sin θ0 - (π - θ0) cos θ0)	(103)
= (-—— ɪ ) cos θ0 + (-—— 1) sin θ0	(104)
≤ - ɪ cos θ0 — S sin θ0	(105)
=-ɪ (CoS θ0 + sin θ0)	(106)
1
≤ - 2.	(107)
Note that the inequality becomes equality only when θ0 cos θ0 = 0 and (^n - 2)sin θ0 = 0, i.e.,
θ = 2 or θ = π.	□
14
Under review as a conference paper at ICLR 2018
If m(w1) = m(w2) = 0, then h2(θ1) = 1(∏ - θ + θl)sin(θ2 - θι) π	(108)
and	
h2(θ2) = 1(∏ - θ2 + θ1)sin(θ1 - θ2).	(109)
Thus,	
h2(θ1) + h2(θ2) = 0.	(110)
However, we know that h2(θ1) < 0 and h2(θ1) < 0, which makes a contradiction.	
In a word, there is no critical point in this case.	
B.4	π ≤ θι ≤ θ2 ≤ 3π	
The norm of the manifold gradient w.r.t. w1 is	
m(wι) = — (∏ — θ2 + θι) sin(θι — θ2) + Cos θι — Sin θι	(111)
+ ɪ ((2π — θι) sin θι — (θι — ∏) Cos θι).	(112)
Similarly, the norm of m(w2 ) is	
m(w2) = — (∏ — θ2 + θι) sin(θ2 — θι) + cos θ2 — sin θ2	(113)
+ 1 ((2π — θ2) sin θ2 — (θ2 — 2) cos θ2).	(114)
Define	
h3(θ) = cos θ — sin θ + ɪ ((2π — θ) sin θ — (θ — ∏) cos θ).	(115)
Let θ = θ0 + π , then	
h3(θ) = h3(θ0 + π)	(116)
= cos(θ0 + π) — sin(θ0 + π)	(117)
+ 1 ((∏ — θ0) sin(θ0 + ∏) — (θ0 + ∏) cos(θ0 + π))	(118)
=—cosθ0 + sinθ0 + ɪ ((π — θ0)(-sinθ0) — (π + θ0 — ∏) (— cosθ0))	(119)
=—cos θ0 + sin θ0 + ɪ (一π sin θ0 + θ0 sin θ0 + π cos θ0 + (θ0 — ∏) cos θ0)	(120)
=—cos θ0 + sin θ0 — sin θ0 + cos θ0 + — (θ0 sin θ0 — (∏ — θ0) cos θ0)	(121)
=1 (θ0 sin θ0 — (π — θ0) cos θ0)	(122)
= h1 (θ0) — cos θ0 + sinθ0.	(123)
Moreover, ∀θ ∈ [∏, 3∏],	
h3 (θ) + h3( ɪ — θ) = hι(θ — π) — cos(θ — π) + sin(θ — π)	(124)
+ hι(52∏ — θ — ∏) — cos(52π — θ — π) + sin(52π — θ — π)	(125)
3π =hι(θ — π) + cos θ — sin θ + hι(-	θ) + sin θ — cos θ	(126)
=h1(θ - π) + h1(^2	θ)	(127)
= 0.	(128)
15
Under review as a conference paper at ICLR 2018
Also, when θ ∈ [∏, 3∏],
... π _ θ _ 1	θ - 3π — 1
h3(θ) =-cos θ +-2-Sin θ > 0,
ππ
(129)
So hɜ is an increasing function when θ ∈ [π, 3∏].
Thus, if m(wι) = m(w2) = 0, then θι + θ2 = 5∏. From θι ≤ θ2 we know that θι ≤ 5∏. Plug
θ2 = 5π - θι in (112) and we get
..	,.2θι - 3∏	,.
m(wι) = 0 ⇔ h3(θ1) = --------- cos(2θι).	(130)
π
From Lemma B.1,
h3(θ1) = h1 (θ1 - π) - cos(θ1 - π) + sin(θ1 - π)	(131)
2θ1 - 3π........................................
≤--------- cos(2(θι) — π) — cos(θι — π)+ sin(θι — π)	(132)
π
2θ1 - 3π..................................
=---------cos(2θι) — cos(θι — π) + sin(θι — π)	(133)
π
2θι — 3π	,八、
≤ ———L cos(2θ∕	(134)
π
Note that (132) becomes equality only when θι = ∏ or θι = 5∏, and (134) becomes equality only
when θι = 5∏. Therefore, in this case, m(wι) = 0 if and only if θι = 5∏.
In a word, the only critical point in this case is (θ1,θ2) = (5∏, 5∏).
B.5	3π ≤ θι ≤ θ2 ≤ 2π
Actually, this is symmetric to the B.3, so in this part I would like to specify this kind of symmetry.
We have already assumed that θ1 ≤ θ2 without loss of generality, and under this assumption, we can
find another symmetry: From w1 and w2, using line y = x as symmetry axis, we can get two new
vectors w10 and w20 . w10 is not necessarily the image of w1 because we need to preserve the assumption
that θ1 ≤ θ2 , but there exists one and only one mapping such that θ10 ≤ θ20 . In this kind of symmetry,
the angles, including θw1,w2 and θwa,w; where i,j ∈ [2], are the same, so the two symmetric cases
share the same gradients, thus the symmetric critical points.
We use (i,j) ,where i, j ∈ [4], to represent the case that θ1 is in the ith quadrant and θ2 is in the
jth one. Using this kind of symmetry, we conclude that (1, 2) is equivalent to (1, 4) and (2, 3) is
equivalent to (3, 4), so there are 4 cases left which are (1, 2), (1, 3), (2, 3) and (2, 4).
B.6	0 ≤ θι ≤ 2 ≤ θ2 ≤ π
Similar to previous cases,
	m(w1 ) =	1(π	— θ2 + θ1 ) sin(θ1 —	θ2) +	cos θ1	— sin θ1	(135)
	+	∏(θ	1 sin θι — (∏ — θι)	cos θ1			(136)
and	m(w2) =	1(π	— θ2 + θ1 ) sin(θ2 —	θ1 ) +	cos θ2	— sin θ2	(137)
	+	1 (θ	2 sin θ2 — (θ2 — ∏)	cos θ2	.		(138)
Using previous definitions, we conclude that
m(wι) = 1(∏ — θ2 + θ1)sin(θ1 — θ2) + h1(θ1)	(139)
π
16
Under review as a conference paper at ICLR 2018
and
m(w2) = 1(∏ - θ + θ1)sin(θ2 - θι) + h2(θ2).	(140)
π
If m(w1) = m(w2) = 0, then m(w1) + m(w2) = 0, i.e.,
h1(θ1) + h2(θ2) =0.	(141)
From (99) we know that
hi (θι) = h2 (θι + 2)+ cos θι.	(142)
Thus, using lemma B.2,
h1(θ1) + h2(θ2) = h2(θ1 + 2) + h2(θ2) + cosθi ≤ -2 - 1 + 1 = 0.	(143)
That means the only case that h1(θ1) + h2(θ2) = 0 is when the inequality (143) becomes equality,
which means that cos θi = 1 and h2(θ1 + ∏2) = h2(θ2) = - 11. Thus, We must have θi = 0, and
θ2 = 2 or θ1 = ∏. Plugging them back in (136) and (138), we can verify that the first one is a critical
point while the other is not. Since (θi, θ1) = (0, 1) has been counted in case 1, there are no new
critical points in this case.
B .7 0 ≤ θi ≤ 2, π ≤ θ2 ≤ 竽
Similar to previous cases,
m(wi) = 1(∏ - θw1,w2 )sin(θi - θ1) + cos	θ1 - sin θ1	(144)
+ 1 (θi sin θi - (2 - θi) cos θi) and		(145)
m(w1) = ∏(∏ - θw1,w2)sin(θ1 - θi) + cos	θ2 - sin θ2	(146)
+ 1 ((2π - θ1)sin θ1 - (θ1 - 2) cos θ1). Thus, using previous definitions		(147)
m(WI) = — (π - θw1,w2 ) sin(θi - θ2) + h1(θi)		(148)
and		
m(W2) = π(π -θw1,w2)sin(θ2 - θ1) + h3(θ2).		(149)
If m(w1) = m(w2) = 0, then m(w1) + m(w2) = 0, i.e., h1(θ1) + h3(θ2) = 0. For 0 ≤ θ ≤ 2, define		(150)
H(θ) = h1(θ) + h3(θ + —). Then we have the following lemma: Lemma B.3. When 0 ≤ θ ≤ ∏, H(θ) ≤ 0, and when ∏ ≤ θ ≤ points of H in [0, 1 ] are θ = 0, ∏ and π1.	2, H(θ) ≥ 0.	(151) Besides, all zero
Proof. From (123), h3(θ + —) = h1 (θ) - cos θ + sin θ. Thus, H(θ) = 2h1 (θ) - cos θ + sin θ		(152)
=cos θ — sin θ + — (θ sin θ 一 (——	θ) cos θ)	(153)
=" cos θ + ("π- 1) Sin θ ——		(154)
2θ = 一 (cos θ + Sin θ) — Sm θ. —		(155)
17
Under review as a conference paper at ICLR 2018
When 0 ≤ θ ≤ ∏4, since Sin θ is a concave function for θ, We know that
	sin ∏	2√2 sin θ ≥ --ɪθ =	θ. 4	π	(156)
Thus,	2θ H (θ) = 一 (Cos θ + Sm θ) — Sin θ π	(157)
	≤ 2√2 θ -Sin θ π	(158)
	≤ 0.	(159)
To make H(θ)	=0, We must have sin θ = 2-2 θ, so θ = 0 or θ =-.	
Besides, when	∏4 < θ ≤ 2, note that	
	π H(- - θ) + H(θ) = 2hι(θ) - CoS θ + sinθ	(160)
	+ 2hι(- - θ) - cos(- - θ) + sin(- - θ)	(161)
	=2 (hι(θ) + hι(2 - θ力	(162)
	—cos θ + sin θ — cos( — — θ) + sin( — — θ)	(163)
	= 0.	(164)
Thus, H(θ) = the proof.	-H(2 - θ) ≥ 0. And to make H(θ) = 0, the only possibility is θ =	=2, which ends □
Remember that if m(w1) = m(w2) = 0, then we have h3(θ2) = -h1 (θ1).
If h1(θ1) > 0, i.e., 0 ≤ θι < ∏, then from lemmaB.3, H(θι) ≤ 0, which means that
h3(θ1 + π) ≤ -h1(θ1).	(165)
Since h3 is a strictly increasing function, we know that if h3(θ2) = -h1(θ1), then θ2 ≥ θ1 + π, so
sin(θ1 - θ2) ≥ 0, and that means
m(wι) = ∏(∏ — θw1,w2) sin(θι — θ2) + h1(θ1) > 0 + 0 = 0.	(166)
Similarly, if h1(θ1) < 0, i.e., ∏4 < θι ≤ ∏, then from lemma B.3, H(θι) ≥ 0, which means that
h3(θ1 + π) ≥ -h1 (θ1).	(167)
Thus, if h3(θ2) = -h1(θ1), then θ2 ≤ θ1 + π, so sin(θ1 - θ2) ≤ 0, and that means
m(wι) = ∏(∏ — θw1,w2) sin(θι — θ2) + h1(θ1) < 0 + 0 = 0.	(168)
The last possibility is h1(θ1) = 0, i.e., θι = ∏. Plugging it into (150) and we know that h3(θ2) = 0,
so θ2 = 54π. And that is indeed a critical point.
In a word, the only critical point in this case is (θ1,θ2) = (∏,苧).
B.8	π2 ≤ θι ≤ π ≤ θ2 ≤ 3π
Like previous cases,
m(wι) = 1(π - θw1,w2)sin(θι - θ2) + h2(θ1)	(169)
π
18
Under review as a conference paper at ICLR 2018
and
m(w2) = 1(∏ - θw1,w2)sin(θ2 - θι) + h3(θ2).	(170)
If m(w1) = m(w2) = 0, then m(w1) + m(w2) = 0, i.e.,
h2(θ1) + h3(θ2) = 0.	(171)
Let θ0 = θ2 - π, then from (99) and (123), we know that
h3(θ2) =h3(θ0+π)	(172)
= h1 (θ0) - cos θ0 + sin θ0	(173)
π
=h2(θ + 2)+ sinθ .	(174)
Thus, from lemma B.2,
π
h2(θ1) + h3(θ2) = h2(θ1) + h2(θ2 - 2)+ sin(θ2 - ∏)	(175)
≤ - 2 - 2 + 1	(176)
= 0.	(177)
Therefore, in order to achieve h2(θ1) + h3(θ2) = 0, the only way is let (176) becomes equality,
which means that θ = 3∏ and θι = ∏ or π. Plugging them into (169) and (170) We conclude that
both of them are not critical points.
In a word, there is no critical point in this case.
B.9	π ≤ θι ≤ π, 3π ≤ θ2 < 2π
Similar to previous cases,
m(wι) = ∏(∏ - θw1,w2)sin(θι - θ2) + h2(θ1)	(178)
and
m(w2) = 1(π - θw1,w2)sin(θ2 - θι) + cosθ - Sinθ	(179)
+ 1 ((2π - θ2)sinθ2 - (5∏π - θ2) cosθ2) .	(180)
From 2 ≤ θι ≤ π and 3∏ ≤ θ2 ≤ 2π We know that θw1,w2 ≥ ∏, so
I -(π - θw1,w2 ) sin(θ1 - θ2) I ≤ — ∙ 1 = ʒ.	(181)
π	π2
When ∣∏(π - θw1,w2)sin(θι - Θ2)∣ = 2, we must have θw1,w2 = ∏, so it must be true that
(θ1,θ2) = (π, 3∏). However, when (θ1,θ2) = (π, 3∏), we have 1 (π - θw1,w2)sin(θι - θ2) = -2.
Thus,
—(π - θw1,w2 ) sin(θ1 - θ2) < —.	(182)
π2
Therefore, using lemma B.2,
m(WI) < 2 + (-1) = 0.	(183)
In a word, there is no critical point in this case.
B.10	Conclusion
In conclusion, based on the assumption that θ1 ≤ θ2 there are four critical points in the 2D case:
(θι, θ2) =(O, π), (4, 4), (4, 5π)and (5π, 5π).
19
Under review as a conference paper at ICLR 2018
C Hessian for 2D Cases
There are 4 critical points: (4, ∏), (4,等)，(等,5∏π), (0, ∏). Obviously, the point (0, ∏) is a global
minima. Next we want to compute the Hessian on other 3 points.
Assume the manifold is R = {(w1, w2) : kw1 k2 = kw2k2 = 1}, then the Hessian on the manifold is
ZTVRfz = ZTV2fz -(WTf) kzιk2 - (wTf) kz2k2	(184)
∂ w1	∂w2
_ τ d2f , τ d2f , T τ d2f	门只q∖
=z1 ∂wι∂wT z1 + z2 ∂W2∂WT z2 + 2z1 ∂W1 ∂wT z2	(185)
-(WT ∂f )kzιk2-(WT ∂W2) kz2 k2	(186)
where z = (z1 , z2 ) satisfies W1T z1 = 0, W2T z2 = 0.
Next, we compute each term in Hessian.
Since
∂f=w1+∏ ||w2||舟	-sin θw1,w2 + ∏ (n - θw1,w2 )w2	(187)
1 W1 Sin θwi,w1 π ||W1 ||	1 1		(π - θwι,w* )wl π1	(188)
1 W1 sin θwi,w2 π ||W1 ||	1 2		(π - θwι,wg )w2. π2	(189)
and
∂f	1	W2 丽=w2 + ∏ llwillM	sin θW1,W2 + ∏ (n - θW1,W2 )w1
1 W2		(n - θW2,wf )W1 π1
Il	Il sin θW2,w1 π llW2ll	2 1	
-1 舟 sin θw2,w2 -		(n - θw2,w* )w2. π2
(190)
(191)
(192)
20
Under review as a conference paper at ICLR 2018
Then we can get when w1 = w2 and wι = -w2,
∂2f	τ l ∣∣w2∣∣ /sinθwι,w2	sinθwι,w2	T
∂W1∂WT = I + — (TWlT I-IW^w1w1
cos θw1,w2	1	/	WIWT
kwi∣∣	J ( WTw2 )2 (IlWIlllIW2∣∣
V1	(kwιk"k )
Wi W2
IlWIll3 ∣∣w2∣∣
—
1
+	/	T ^=(
πJ1 _ ( WT w2 )2
π V 1	, kw1kkw2k J
w2 w2T
T
WT W2
llw1k kw2k IlWIk3 ∣∣W2∣∣
∙W2WT)
1 (Sinθwι,w* I sinθwι,w;
π( kwιk	∣∣W1∣∣3
—
—
—
—
cos θwι,w*
lwιk	rτ-7
WT w*
kwι∣∣∣wf
=(
)2
l)
WIWjT
-----Γ~~7--— — ------7；-----
l∣w IHlW ii ∣∣w 1131W 1I
w wWT W :
πy kwιk∣w:
=(
)2
l)
1	1T
W 1 W 1T
WT W1
------Γ~~7-----— — ----------7：-------
|w 1I IIWIIl IlW 113 IlWIIl
,w；WT )
1 (Sin θwι,w: I
π(	∣∣wι∣∣
sin θwι,w2
"I3
T
W1 W1
cos θwι,w2	1	/ WIWIT
-1W1 -^∕1	( WTW2 、2(∣W1H∣W却I
V1 - ( kwιk∣W:∣ )
wt W2
∣W1∣∣3∣∣W2∣∣
1
W T W ；
—
π /
1
—
WT w2	)2
kwιk∣w2∣)
1	1T
W21 W21T
IMTlHl
WT W2
TWIll3 ∣w2τ
WIWT)
1
—
Using the fact that wt zι = 0,
∂2f
∂W1 dwT '1
1 + Sin θw1,w2
π
sin θwι,w:
π
sin θwι,w:
π
Similarly,
∂2f
∂W2 dwT 2
sin θw2,w:
π
—
sin
2
21
Under review as a conference paper at ICLR 2018
Next,
∂2f = Sin θwιw
∂wι∂wT	π ∣∣w1k∣∣w2∣∣
—
∣∣W2∣∣ Cos θwι ,W2 _1_____(	1	T
πkw1k	q-(4)2 (kwιkkw2kSI
—
T
w1T w2	T
---------o wι w2
kwιkkw2k3
_	1	(	1	T
∏q1-(高⅛⅛)2 lkwιkkw2kw 叫
T
w1 w2
IlWIllIlW2k3
w2wT) +-----(π - θw1,w2 )I
π
and
ZT Adf T Z2 = —/	1	号ZTw2wTz2 + 1(π
dwidwT	π JI-(WT w2)2	π
In conclusion,
—
+
2
+ ∏ (π - θw1,w2 )zi z2
Z1Tw2w1TZ2
—
—
-∏(π -θw1,w1 )wTw1- ∏(π i,w2)wTw2)
-∏ (π - θW2,w1 )WTwl - ∏ (π - θW2,w曰)WTw2).
When w1 = w2 or w1 = -w2, we should consider the limit of the Hessian.
First, let’s compute the limit of some functions that we will use later. For simplicity, we just consider
the case when w1 → w2. The case w1 → -w2 will be the same.
Claim: lim	(z1 w2)= 0
w2→wi √1-(wTW2)2
Proof: WLOG, We assume wi = (1,0), w2 = (Cos θ, sin θ), θ → 0. Otherwise, We can do a rotation
which doesn,t affect the inner product. Since ZTwi = 0 , zi = (0,1). Then
lim
w2 →w1
sin2 θ
lim ,	—
θ→0 λ∕1 — cos2 θ
lim | sin θ∣
θ→0
0
Similarly, we have the following claims.
Claim: lim	(z2 WI)^= = 0
w2→w1 Ji-(WT W2)2
22
Under review as a conference paper at ICLR 2018
Claim: lim	ZT w2w%2 =0
w2→w1 √1-(wT W2)2
Using these claims, we can computet the Hessian when w1 = w2.
w2→Wιz1 ∂W1∂WT z1
w*
π
1 -
lim
w2→w1
w*
π
1 -
lim ZT Aaf T z2 = ZT z2
w2→w1	∂W1∂W2
Now, we can compute the hessian on critical points. For simplicity we just consider the case that
k = 1.
C.1	(4,4)
On the direction Z = (zι, z2) = (-^2, — ʌ22, ʌ22, — -^2),
So
zt VR/Z = zt v2∕z - (wT f) IEIl2 - (wT f)∣∣Z2∣∣2
∂W1	∂W2
T ∂2/	T ∂2/	T ∂2/	2√2 3√2
=ZT	777Z1 + ZT	Z2 + 2ZT	N2 — 4 +1
1 ∂wi∂wT	∂w2∂wt	1 ∂w↑∂wTT	π 2
3√2	2√2 门
=> 0
2 π
On the direction Z = (zi,z2) = (^2, — ʌ^2, 一 ^2, ^2)),
ZT VRfZ = ZT V2fZ -(WT ∂W ) kZ1k2 - (WT ∂W^ ) kZ2k2
∂2/	+ T	∂2f +2 T ∂2f	4 + 2√2 +	3√2
∂wι∂wT'1 ZII ∂w2∂wT'2	'1 ∂wι∂wTZiI	π 2
2√2	2√2	2√2	3√2
1 - -— + 1 ---——2 - 4+	+ -ɪ
π	π	π 2
π
< 0
So this point is a saddle point.
23
Under review as a conference paper at ICLR 2018
C.2 (哈苧)
On the direction Z = (Z1,Z2)=(空
—
√2 √2
F，ɪ，
—
WT f = 2-迎
∂wι	π
—
f = 2 -叵
∂W2	π
kz1k2 - (WT
—
IT2
∂2f
∂ 2f
∂W1∂WT
∂W2 ∂wT
Z2 十 2Z1T
∂2f
∂W1∂WT
2√2	√2
Z2 - 4十---十—-
π 2
2√2	√2
1十1十2 - 4十工十一
π 2
2√2	√2
*十宏> 0
On the direction Z = (Z1,Z2)=(4,-号,
—
∂2f
∂ 2f
∂wi∂wt
∂W2 ∂wt
Z2 十 2Z1T
∂2f
∂wi∂wt
2√2	√2
Z2 - 4十---十—-
π 2
1	2√2 ι ι	2√2
1 -----十 1 -----
2√2	√2
-2 - 4十工十三
π	2
π
π
2√2
———4 < 0
π
So this point is a saddle point.
On the direction Z = (z1,z2)=(苧
√2 √2
ɪ，ɪ，
C.3 (4,等)
—
—
—
WT f = 1 -迎
∂wι	π
3√2
丁
—
WT f =1-迎
∂W2	π
ZTVRfz = ZTV2 fz -(WT
—
kz1k2 - (WT
IT2
∂2f
∂ 2f
∂wι∂wT
∂W2 ∂wT
Z2 十 2ZT
∂2f
∂w∖∂WT
2√2	L
Z2 - 2十上十√2
π
T T C C	2√2	/-
1十1+2 — 2十----十λ∕2
π
C	2√2	Z-八
2 十------十 λ∕2 > 0
π
On the direction Z = (z1,z2)=(苧
—
√2
2，
—
24
Under review as a conference paper at ICLR 2018
ZT VRfz = ZT Jf Z1 + ZT Jf Z2 + 2zT Jf Z2
∂w1∂w1T	∂w2 ∂w2T	∂w1∂w2T
2√2
—2+上 +
π
√2
2√2	2√2	2√2	厂
1 ———+ 1 —————2 — 2+	+ √2
π
π
π
—
2√2
———2 ‹ 0
π
So this point is a saddle point.
C.4 Conclusion
In conclusion, we have four critical points: one is global maximal, the other three are saddle points.
D 3D Cases
D. 1 Why we only need 3 dimension
Lemma D.1. If (w1, w2 ) is a critical point, then there exists a set of standard orthogonal basis
(eι,匕2, e3) such that eι = w；, e? = w2 and w1,w2 lies in span{eι, e?, e3}.
Proof. If (w1, w2 ) is a critical point, then
(I - WIwT)∂w = O.	(193)
where matrix (I - w1w1T ) projects a vector onto the tangent space of w1. Since
(I - w1w1T)w1= w1- w1= 0,	(194)
we get
(I - WIwT)ʒ-ɪ	(195)
∂w1
=∏(I 一 wιwT) ((∏ 一 θw1,w2 )w2 一 (π 一 θwι,wi )w； 一 (π 一 θwι,w2 )w2) ,	(196)
which means that (π 一 θw1,w2 )w2 — (π 一 θwι,w* )w； — (π 一 θwι,w* )w2 lies in the direction of wι.
If θw1,w2 = π, i.e., wι = —w2, then of course the four vectors have rank at most 3, so we can find
the proper basis. If θw1,w2 < π, then we know that there exists a real number r such that
(π ― θw1,w2 )w2 ― (π ― θwι,w )wι 一 (∏ ― θwι,w )w2 + r ∙ wι =0.	(197)
Since θw1,w2 < π , we know that the four vectors w1, w2, w1； and w2； are linear dependent. Thus, they
have rank at most 3 and we can find the proper basis.	□
D.2 Some properties of Critical Points
Next we will focus on the properties of critical points. Assume (w1, w2) is one of the critical points,
from lemma D.1 we can find a set of standard orthogonal basis (e1, e2 , e3) such that e1 = w1；,
e2 = w2； and w1, w2 lies in span{e1, e2, e3}. Furthermore, assume w1= w11e1+ w12e2 + w13e3
and w2 = w21e1 + w22e2 + w23e3, i.e., w1= (w11, w12, w13) and w2 = (w21, w22, w23). Since
we have already found out all the critical points when w13 = w23 = 0, in the following we assume
w123 + w223 6= 0.
Lemma D.2. θw1 ,w2 < π.
Proof. If θw1,w2 = π, then w1 = ∙w2, so w2 is in the direction of wι. We have already known
from (196) that (π 一 θw1,w2)w2 — (π 一 θwι,w*)w； — (π 一 θwι,w*)w； lies in the direction of wι,
so further we know (π 一 θwι,w* )w； + (π 一 θwι,w* )w； lies in the direction of wι. However, (π 一
25
Under review as a conference paper at ICLR 2018
θwι,w* )wɪ — (π — θwι,w* )w曰 lies in span{eι, e2}, so wι ∈ span{eι, e2} and w2 ∈ span{eι, e2}.
Thus, w13 = w23 = 0 and that contradicts with the assumption.
In a word, θw1,w2 < ∏.	□
Lemma D.3. w13 * w23 = 0.
Proof. We have already known from (196) that (∏ 一 θw1,w2 )w2 — (∏ 一 θwι,w* )wɪ 一 (∏ 一 θwι,w^ )wg
lies in the direction of w1 . Writing it in each dimension and we know that there exists a real number
r0 such that
(π	一 θw1 ,w2	)w21 — (π — θwι,wf) = ro ∙ w11		(198)
(π	一 θw1,w2	)w22 — (π — θwι,w*) = ro ∙ W12		(199)
		(∏ — θw1,w2 )w23 = ro ∙ w13.		(200)
From lemma D.2 we know that θw1,w2		< π , so we can define		
		k =		(201)
		π — θw1	. w2	
Then the equations become	w21	π 一 θwι,w* 		:	=k ∙ w11	(202)
		π — θw1 ,w2 π 一 θwι,w*		
	w22		=k ∙ W12	(203)
		π — θw1 ,w2		
		w23	=k ∙ w13.	(204)
Similarly, we have	w11	π 一 θW2,W*	=k ∙ w21	(205)
		π — θw1 ,w2 π 一 θW2,W*		
	w12		=k ∙ w22	(206)
					二 π 一 θw1 ,w2		
		w13	=k0 ∙ w23.	(207)
Since w123 + w223 6= 0, at least one of those two variables cannot be 0. WLOG, we assume that
w13 6= 0. If w23 = 0, then from (207) we know that w13 6= 0, which contradicts the assumption.
Thus, w23 = 0, which means that w13 * w23 = 0.	□
Lemma D.4. w13 * w23 < 0.
Proof. Adapting from the proof of lemma D.3, we know that
and
w21 一	π 一 θwι,w*	=k ∙ w11	(208)
	- 			二 π 一 θw1 ,w2 π 一 θwι,w*		
w22 一		=k ∙ W12	(209)
	π 一 θw1 ,w2		
	w23	=k ∙ W13	(210)
w11 一	π 一 θW2,W*	二 k0 ∙ w21	(211)
	π 一 θw1 ,w2 π 一 θW2,w2		
w12 一		二 k0 ∙ w22	(212)
	π 一 θw1 ,w2		
	w13 =	二 k0 ∙ w23.	(213)
Furthermore, kk0 = w3 ∙ W13 = 1, so k0 = ɪ
26
Under review as a conference paper at ICLR 2018
From lemma D.2 we know that θw1,w2 < π, and from lemma D.3 we know that both w1 and w2 are
. π	π , 士、	. . π ,	∣- -∣ π	,∣	. . π ,	∣- -∣ π - θ Wi,W*∙	1
outside span{w"w2}, so ∀i,j ∈ [2],θwi,w* < π. Thus, ∀i,j ∈ [2], ∏--j > 0. Therefore, We
have
w21 > k ∙ w11
w11 > yW2i.
k
ThatmeanS k< 0, SO 器 > 0.
In a word, w13 * w23 < 0.
Lemma D.5.
arccos(-w11)	arccos(-w12)	w23
arccos(-w21)	arccos(-w22)	w13
(214)
(215)
□
(216)
Proof. Adapting from the proof of lemma D.4 and we know that				
-θw1,w* 0，，CT   	~*^	1	-θw1,w* 。，，CC   	~*^	2			
21	π-θw1,w2 =	22	π-θw1,w2 _	w23 _	k.	(217)
w11	w12	w13		
Similarly, we have				
-θw2 ,w1*	-θw2 ,w2*		1	
w11	π-θw1,w2 _	1w12	π 一 θ	w13 _		(218)
w21	- w1 ,w2 w22	w23	k.	
Taking the first component of (217) and (218) gives us				
w21 = k ∙	π - θw1,w1*			(219)
	u711 + ∏ - θw1,w2			
w21 = k ∙	π - θw2,w1* w11 - k _ A	 π - θw1 ,w2	.		(220)
Thus,				
π-	一 θW1,w* _ -k			(221)
π-	θw2,w1*			
Similarly, we get				
π-	一 θW1,w2 _ -k			(222)
π-	θw2,w2*			
Since ∀i, j ∈ [2], π - θwi,w* = arccos(-θwij), we know that				
arccos(-w11) arccos(-w21)	arccos(-w12) _ 	:		 _ ∙ arccos(-w22)	w23 — . w13		(223)
				□
For simplicity, based on D.5, we define k0	-k, θ1 _ π - θw2,	w1* and θ2 _		π - θw2,w2* . Then
π-	θw1 ,w1* _ k0θ1			(224)
π-	θw1 ,w2* _ k0 θ2 .			(225)
WLOG, assume k0 ≥ 1, otherwise we can switch w1 and w2.				
Thus,
w11 _	- cos(k0 θ1)	(226)
w12 _	- cos(k0θ2)	(227)
w21 _	- cos(θ1 )	(228)
w22 _	- cos(θ2).	(229)
27
Under review as a conference paper at ICLR 2018
Lemma D.6. θι 十 θ2 ≥ ∏.
Proof. Since θι = ∏ - 0仅2,仅；and θ2 = ∏ - θw2,w力 We know that θ1,θ2 ∈ [0, ∏]. Besides,
w121 + w122 = 1 - w123 ≤ 1	(230)
w221 + w222 = 1 - w223 ≤ 1.	(231)
Thus,
cos2 (k0 θ1) + cos2(k0θ2) ≤ 1	(232)
cos2 (θ1) + cos2 (θ2) ≤ 1.	(233)
If one of θι and θ2 is larger than ∏, say θι > ∏, then of course θι + θ2 ≥ ∏. If θι, θ2 ∈ [0, 2], then
sin2 (∏ — θι) = cos2(θ1) ≤ 1 — cos2(θ2) = sin2(θ2),	(234)
so ∏ - θι ≤ θ2, which means that θι + θ2 ≥ ∏.
In a word, θι + θ2 ≥ ∏.	□
Lemma D.7. 1 ≤ k0 ≤ 3.
Proof. First we prove that ko ≤ 4: From lemma D.6, we know that θι + θ2 ≥ ∏, so at least one of
θι and θ2 is no less than ∏, say θι ≥ ∏. If k0 > 4, then π -。仅八仅；=k0θι > π, which makes a
contradiction. Thus, k0 ≤ 4.
Furthermore, if 3 < ko ≤ 4, then θι, θ ∈ [0, ∏] because koΘ1,k0θ2 ∈ [0, π].
If θι, θ2 ∈ [0, 4), then θι + θ2 < ∏ which contradicts lemma D.6.
If θ1,θ2 ∈ [4, 3],then koθ1,koθ2 ∈ (3∏, ∏], which means that cos2(k0θ1)+cos2(k0θ2) > 2 + 2 = 1
and contradicts (232).
If θι ≤ 4 ≤ θ2 and k0θι < ∏, then θι < 2π- < ∏, so from lemma D.6, θ2 ≥ ∏ - θι > π3, which
contradicts koθ2 ≤ π.
If θι ≤ 4 ≤ θ2 and k0θι ≥ ∏, then koθι,k0θ2 ∈ [∏,π]. Since cos2(koθι) + cos2(k0θ2) ≤ 1, we
know that
sin2 (koθι — ∏) = cos2(koθι) ≤ 1 — cos2(k0θ2) = sin2(π — k0θ2),	(235)
so koθι - ∏ ≤ π - k0θ2, which means that koθι + koθ2 ≤ 3∏. Thus, θι + θ2 < ∏, which contradicts
lemma D.6.
In a word, 1 ≤ ko ≤ 3.
□
Lemma D.8. Define
F (θ) =------------0-------—
ko cos(koθ) + cos(θ)
(236)
then F(θι) = F(θ2)(θ1,θ2 ∈ [0,看]).
Proof. Since koθ1,koθ2 ∈ [0, π], we know that θ1,θ2 ∈ [0,看].
From (217), applying the change of variables on the first component and we get
- cos θ1 -
k0θ1
π-θwι ,w2
- cos(ko θ1 )
-ko.
(237)
Thus,
=	—koθι
'w2	ko cos(koθι) + cos(θι)
F(θ1).
(238)
28
Under review as a conference paper at ICLR 2018
Similarly, if we apply the change of variables onto the second component of (217), we will get
Thus,
π - θw1,w2 = k0 Cθs(k0θ2)+ COS(Θ2) = F IQ
π
F(θι) = F(Θ2)(Θ1,Θ2 ∈ ［0,厂］).
(239)
(240)
□
LemmaD.9. ∃θ° ∈ ［第,薨),st,
(< 0	0 ≤ θ<θo
F(θ) =	= ∞ θ = θ0
［> 0 θ0 < θ ≤ 看
(241)
Proof. Note that when θ ∈ ［0,看］,-k°θ is always non-positive. Define G(θ) = ko cos(k0θ) +
cOs(θ), then G(θ) is a strict decreasing function w.r.t. θ. Note that G(0) = k0 + 1 > 0 and
G (看)=cos (看)一 ko < 0, so there must be an θo ∈ (0,煮)such that G(θ0) = 0. Thus, when
0 ≤ θ<θo, G(θ) > 0, and when θo ≤ 看,G(θ) < 0.
Thus,
< 0	0 ≤ θ < θ0
F(θ) =	= ∞ θ = θ0
［> 0	θo<θ ≤ k
(242)
Then the only thing we need to prove is 而 ≤ θo < 患.Note that
G 莞 )=cos
(243)
(244)
Since the inequality (244) holds only when cos (急)=4 and √ =号,which means ko = 3
and k0 = 1, which makes a contradiction. Thus,
3π
G(T^) < 0.	(245)
4ko
Therefore,永 ≤ θo < -3π, which completes the proof.	□
LemmaD.10. F (θ) is eitherstri^tly decreasing or first decrease and then increase when θ ∈ (θo,看］.
Proof.
F0(θ)
—
ko (ko cos(koθ) + cos(θ)) - koθ -ko2 sin(koθ) - sin θ
-ko
(ko cos(koθ) + cos(θ))2
ko cos(koθ) + cos θ + ko2θ sin(koθ) + θ sin θ
(246)
(ko cos(koθ) + cos(θ))2
(247)
Define H(θ) = ko cos(k0θ)+cos θ+k0θsin(koθ)+θ Sin θ(θ ∈ (θo,看］),then H(θ) ∙F0(θ) < 0(i.e,,
when H(θ) is positive, F(θ) is decreasing, otherwise F(θ) is increasing), and we know that
H 0(θ)	=	-ko2 sin(koθ) - sin θ + ko3θ cos(ko θ) + ko2 sin(koθ) + θ cos θ + sin θ	(248)
=	ko3θ cos(koθ) + θ cos θ	(249)
=	θ(ko3 cos(koθ) + cos θ)	(250)
≤	θ(ko cos(koθ) + cos θ)	(251)
=θ ∙ G(θ)	(252)
< 0.	(253)
29
Under review as a conference paper at ICLR 2018
Note that (251) holds because θ > θo ≥ 就.
Thus, H(θ) is a strictly decreasing function when θ ∈ (θo, -π].
We can see that
H(θ0) = G(θ0) + k02θ0 sin(k0θ0) + θ0 sin θ0	(254)
= k02θ0sin(k0θ0) +θ0sinθ0 > 0.	(255)
Thus, if H(kπ) ≥ 0, then F(θ) is monotonically decreasing when θ ∈ (θo,看].Otherwise, F(θ) first
decrease and then increase when θ ∈ (θo,看].	□
Lemma D.11. ∀θ ∈ (箭,看],F(θ) < F (条).
Proof. From lemma D.10 we have already known that F(θ) is either strictly decreasing or first
decrease and then increase when θ ∈ (θo, kπ ], sothe maximum of the function value on an interval can
only be at the endpoints of that interval, which means that we only need to prove F(资)> F(看).
Note that
F(In) >F(-∏)
4k0	k0
3π
⇔ √~匚
~2^ ko — cos
3
(256)
(257)
(258)
(259)
(260)
(261)
1
π
k0 - cos 煮
Let h(x) = 4
Thus, h(x) is decreasing in [∏, 47∏] and increasing in [4∏ ,π]. However, we know that h(∏)
8 - 苧 < 0 and h(π) = -3 + 普 < 0, so h(x) is negative when X ∈ [∏ ,π].
Therefore,
ko > 0 > 3 cos π- - cos (3π-
0	4 k0	4k0
(262)
which means that F(条)> F(看).
Thus, ∀θ ∈ (4k0,看],F (θ) <f (4k0).
□
Lemma D.12. θ1 = θ2.
Proof. From the proof of lemma D.8 we get
F(θ1) = π- θw1,w2 = F(θ2).	(263)
Thus, F(θ1), F(θ2) ∈ [0, π].
Using IemmaD.9, θ1,θ2 > θo ≥ 第,so that koΘ1,k0θ2 ∈ (∏ ,π].
30
Under review as a conference paper at ICLR 2018
From (232), We know that k0(θ1 + θ2) ≤ 3∏, which means that at least one of θι and θ2 are less than
or equal to 孺,w.l.o.g. we assume θι ≤ 急.
Note that lemma D.11 tells us that F(条)> F(看)，so at the point θ =急,the function cannot
be increasing, which combining with lemma D.10 shows that F(θ) is strictly decreasing when
θ ∈ (θ0, 43π].
If θ2 > 43π, then we know that F(θι) ≥ F (孺)> F(θ2), which contradicts F(θι) = F(θ2).
Thus, θι, θ2(θ0, 4kπ]. Since F(θ) is monotonically decreasing when θ ∈ (θo, -3π], we can conclude
that θι = θ2.	0	0	□
D.3 Negative Curvature
First we compute the Hessian matrix:
If z = (tz1, z2), ||z1 || = ||z2|| = 1 and w1T z1 = w2T z2 = 0, then
ZT VRfz =	(264)
+
π
1
πq1 - (wT w2)2
(265)
2
2
=zT W2WT z2 + 2(∏ — θw1,w2 )zT z2
+ tI 一/	=
π 1 - W1T W2
(266)
- t2 (一 (π - θw1,w2 )WTw2-(π - θwι,w* )WTwI-(π - θwι,w2 )WTw2
π	π1π2
- (一 (π - θw1,w2 )WTW1-(π - θW2,W* ) WTwI-(π - θW2,W* )WTW2).
π	π1π2
Lemma D.13. For every critical point (W1, W2) outside span{W1；, W2；},
一(n - θw1,w2 )WTw2-(π - θwι,w* )WTwl-(π - θw
π	π1π
= -k0 (π - θw1 ,w2 )
∏(π - θw1,w2 )wT w1 - ∏(π - θw2,w1 )wT w； - ∏(π - θw2,w2 )wT 城
(267)
(268)
(269)
(270)
(271)
(272)
(273)
1
1
Proof. In lemma D.3, we have three equations, and we write them again for convenience:		
(π - θW1,W2)W21 - (π - θWι ,wɪ ) = r0	• W11	(274)
(π - θw1,w2 )W22 - (π - θwɪ,wg ) = r0	• W12	(275)
(π - θw1,w2 )W23 = ro	• W13.	(276)
Multiply 274 by W11, 275 by W12, 276 by W13, we get		
(π - θw1,w2)W21W11 - (π - θwɪ,w*)wu =	r0 • W121	(277)
(π - θw1,w2 )w22w12 - (π - θwɪ,w* )w12 =	r0 • W122	(278)
(∏ - θw1,w2 )W23W13 =	r0 • W123 .	(279)
31
Under review as a conference paper at ICLR 2018
Combine these three equations, we know that
∏(∏ - θw1,w2 )wTW2 = r0		-1(∏ π	-θwι,w* )WTW；	-1(∏ π	-θwι,w2 )WTW2	(280) (281)
	w23 一 (π - θw1,w2 ) 1	2 w13					(282)
	= -k0 (π - θw1 ,w2 ).					(283)
Similarly,	∏(∏ - θw1,w2 )wTWl	-1(∏ π	一θW2,W* )wTW	-1(∏ π	一θW2,w2 )wTW2	(284)
	w13 一(π - θw1,w2) 1	2 w23					(285)
	=-而(π - θWl,W2 ).					(286)
□
Lemma D.14. For every critical point (w1,w2) outside span{wj,w2}, there is negative curvature.
Proof. We select zι = (-√2, √2, 0) and z2 = (√, -√2,0), then
ZT VRfz =-----/	2-------/	2 + k0+ + T 2) (π - θw1,w2 ).	(287)
1 - w121	1 - w221	k0
From lemma D.7 we know that 1 ≤k0 ≤ 3.
If 1 ≤k0 ≤ 2, then
ZTVRfz ≤ -2 + 1 ∙ ∏ < 0.	(288)
If 2 < ko ≤ 3, from (232) and lemma D.12 We get 2cos2 (k0θ1) ≤ 1, so k0θ1 ≤ 3∏, which means
that
θι ≤ 3∏- < 3∏.	(289)
4k0	8
Thus,
∣wιι∣ = | cos θι | > cos —.	(290)
8
Besides, from (233) and lemma D.12 we know that 2 cos2 θι ≤ 1, so θι ≥ ∏, which means that
k0θ1 > 2 ∙ 4 = 2.	(291)
Using (289) and (291),
w11 = w12 = - cos(k0θ1) > 0	(292)
w21 = w22 = - cos θ1 < 0.	(293)
From lemma D.4, we conclude that
hw1,w2i = W11 ∙ W21 + W12 ∙ W22 + W13 ∙ W23 < 0,	(294)
which means that
θw1,w2 > 2.	(295)
Thus,
ZTVRfz ≤ —/	i 二-1 + f3 +1- 2) (π - ∏)	(296)
1- - cos2 3π	∖	3 八 2/
=-√2 - 1 + 2π	(297)
< 0.	(298)
In a word, for every critical point (wι, w2) outside span{w*,w2}, there is negative curvature.	□
32
Under review as a conference paper at ICLR 2018
E 2D Cases with Assumption Relaxation
Since this section is pretty similar to B, I will try my best to make it brief and point out the most
important things in the proof.
E.1 Preliminaries
After the changing of variables(i.e., polar coordinates), we know that w1 = (cos θ1, sin θ1) and w2 =
(Cos θ2, Sin θ2). And the manifold gradient(eXPressed by m) are m(wι) = Sin θι ∂f — Cos θι ∂f
and τm (W2) sin θ2 ^3~-— - cos θ2 ^3~-—.
∂w21	∂w22
APPlying the changing of variables and multiPly it by π, we get
m(wι) = (π — θwι,w )sin(θι — θ2) + (π 一 θ3 ,w^) sin(α 一 θι) — (π — θwι,w* )sin θι. (299)
And
m(w2) = (∏ — θw1,w2 )sin(θ2 — θι) + (∏ — θw2 ,w2) sin(α — θ2) — (π — θw2,w* )sin θ2. (300)
Define(where w = (Cos θ, sin θ))
h(θ) =	(∏ — θw,w*) sin(α — θ) — (π — θw,wi) sin θ.	(301)
Then when θ is in the first Part to the fourth Part, the function h will change to four different functions:		
h1(θ)	= (π — α + θ) sin(α — θ) — (π — θ) sin θ	(302)
h2(θ)	= (π — θ + α) sin(α — θ) — (π — θ) sin θ	(303)
h3(θ)	= (π — θ + α) sin(α — θ) — (θ — π) sin θ	(304)
h4(θ)	= (θ — α — π) sin(α — θ) — (π — θ) sin θ.	(305)
WLOG, we assume θ1 ≤ θ2 .
E.2 0 ≤ θ1 ≤ θ2 ≤ α
First, it’s easy to verify that ∀θ ∈ [0, θ], h1 (θ) + h1(α — θ) = 0.
Besides,
h01 (θ) = sin θ + sin(α — θ) — (π — θ) Cos θ — (π — α + θ) Cos(α — θ)
=2 sin α cos(θ — α) — (∏ — θ) Cos θ — (∏ — α + θ) cos(α 一 θ)
≤ 2 sin α — ∏(Cos θ + cos(α — θ))
=2 sin α	—	π cos a cos(θ —	α)
2	2	2，
≤ 2 sin α	—	π cos α
-	2	2
(306)
(307)
(308)
(309)
< 0.	(310)
When m(w1) = m(w2) = 0, we know that h1(θ1) +h1(θ2) = 0, and because of those two ProPerties
above, We know that θι + θ2 = α. Thus, θι ∈ [0, 2]. And We have the following lemma
Lemma E.1. m(w1) ≤ 0.
Proof.
m(w1) = sin(α — 2θ1)(π — α + 2θ1) — (π — α + θ1) sin(α — θ1) + (π — θ1) sin θ1	(311)
≥ sin(α — 2θ1)(π — α + θ1) — (π — α + θ1) sin(α — θ1) + (π — θ1) sin θ1	(312)
α
≥ sin(α — 2θι)(π — α + θι) — (π — α + θι) sin(α — θι) + (π — —) sin θι (313)
α
=(π — α + θι)(sin(α — 2θι) — sin(α — θι)) + (π — —) sin θι	(314)
α
≥ (π — —)(sin(α — 2θι) — sin(α — θι) + sinθι)	(315)
α
=(π — —)(sin(α — 2θι) — sinθι — sinθι cos(α — 2θι) — cosθι sin(α — 2θι)) (316)
≥ 0.	(317)
33
Under review as a conference paper at ICLR 2018
Thus, the only possible critical points are m(wι) = 0, which are 0 and 2. After verification,
we conclude that there are only two critical points in this case: (θ1, θ2) = (0, α) or (θ1, θ2) =
(2,2).	□
E.3 α ≤ θ1 ≤ θ2 ≤ π
When m(w1) = m(w2) = 0, we know that h1(θ1) + h1(θ2) = 0. However, when θ ∈ [α, π], we
know that
h2(θ) = (π - θ + α) sin(α - θ) - (π - θ) sin θ	≤ 0.	(318)
The inequality cannot become equal because the possible values of θs such that each term equals
zero has no intersection. Thus, h2(θ) is always negative, which means that in this case there are no
critical points.
E.4 π ≤ θ1 ≤ θ2 ≤ π + α
It’s easy to verify that ∀θ ∈ [π, π + α], h3(θ) + h3(2π + α - θ) = 0. Furthermore,
h03(θ) = - sin(α - θ) - cos(α - θ)(π + α - θ) - sin θ - (θ - π) cos θ	(319)
=-2 Sin α cos(θ — α) — (θ — ∏) cos θ — (∏ + α — θ) cos(α — θ)	(320)
> 0.	(321)
Thus, from m(w1) = m(w2) = 0, we know that h1(θ1) + h1(θ2) = 0 we get θ1 + θ2 = 2π + α,
which means that θι ∈ [∏,∏ + 2], so we can prove the following lemma:
Lemma E.2. m(w1) ≤ 0.
Proof. Let θ0 = θ1 — π, then
m(wι) = (π — θ2	+ θι)sin(θι — θ2) + h3(θ1)	(322)
= (π + θ0	— α + θ0) sin(2θ0 — α) + h1 (θ0) +	π sin θ0 —	π sin(α	— θ0)	(323)
≤ (π + 2θ0 — α) sin(2θ0 — α) + sin(α — 2θ0)(π + 2θ0 — α) + π(sin θ0 — sin(α — θ0))
(324)
≤ π(sin θ0 — cos θ0)	(325)
≤ 0.	(326)
The first inequality is from lemma E.1.	□
Thus, the only possible critical points are m(wι) = 0, which are π and π + 2. After verification,
we conclude that there are only two critical points in this case: (θ1, θ2) = (π, π + α) or (θ1, θ2) =
(π + 2, π + 2).
34