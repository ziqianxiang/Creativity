Workshop track - ICLR 2018
Gradients explode
- Deep Networks are shallow
- ResNet explained
George Philipp*
Carnegie Mellon University
george.philipp@email.de
Dawn Song
University of California, Berkeley
dawnsong@gmail.com
Jaime G. Carbonell
Carnegie Mellon University
jgc@cs.cmu.edu
Ab stract
Whereas it is believed that techniques such as Adam, batch normalization and,
more recently, SeLU nonlinearities “solve” the exploding gradient problem, we
show that this is not the case in general and that in a range of popular MLP archi-
tectures, exploding gradients exist and that they limit the depth to which networks
can be effectively trained, both in theory and in practice. We explain why explod-
ing gradients occur and highlight the collapsing domain problem, which can arise
in architectures that avoid exploding gradients.
ResNets have significantly lower gradients and thus can circumvent the exploding
gradient problem, enabling the effective training of much deeper networks, which
we show is a consequence of a surprising mathematical property. By noticing
that any neural network is a residual network, we devise the residual trick, which
reveals that introducing skip connections simplifies the network mathematically,
and that this simplicity may be the major cause for their success.
1	Introduction
Arguably, the primary reason for the recent success of neural networks is their “depth”, i.e. their
ability to compose and jointly train nonlinear functions so that they co-adapt. A large body of work
has detailed the benefits of depth (e.g. Montafur et al. (2014); Delalleau & Bengio (2011); Martens
et al. (2013); Bianchini & Scarselli (2014); Shamir & Eldan (2015); Telgarsky (2015); Mhaskar &
Shamir (2016)).
The exploding gradient problem has been a major challenge for training very deep feedforward
neural networks at least since the advent of gradient-based parameter learning (Hochreiter, 1991). In
a nutshell, it describes the phenomenon that as the gradient is backpropagated through the network,
it may grow exponentially from layer to layer. This can, for example, make the application of vanilla
SGD impossible for networks beyond a certain depth. Either the step size is too large for updates
to lower layers to be useful or it is too small for updates to higher layers to be useful. While this
intuitive notion is widely understood, there are important gaps in the foundational understanding of
this phenomenon. In this paper, we take a significant step towards closing those gaps.
To begin with, there is no well-accepted metric for determining the presence of pathological explod-
ing gradients. Should we care about the length of the gradient vector? Should we care about the size
of individual components of the gradient vector? Should we care about the eigenvalues of the Ja-
cobians of individual layers? Depending on the metric used, different strategies arise for combating
exploding gradients. For example, manipulating the width of layers a suggested by e.g. Anonymous
(2018d); Han et al. (2017) can greatly impact the size of gradient vector components but leaves the
length of the gradient vector relatively unchanged.
*work done while at University of California, Berkeley
1
Workshop track - ICLR 2018
The underlying problem is that it is unknown whether exploding gradients according to any of these
metrics necessarily lead to training difficulties. There is a large body of evidence that gradient explo-
sion defined by some metric when paired with some optimization algorithm on some architectures
and datasets is associated with poor results (e.g. Schoenholz et al. (2017); Yang & Schoenholz
(2017)). But, can we make general statements about entire classes of algorithms and architectures?
Algorithms such as RMSprop (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2015) or vSGD
(Schaul et al., 2013) are light modifications of SGD that rescale different parts of the gradient vector
and are known to be able to lead to improved training outcomes. This raises an another important
unanswered question. Are exploding gradients merely a numerical quirk to be overcome by sim-
ply rescaling different parts of the gradient vector or are they reflective of an inherently difficult
optimization problem that cannot be easily tackled by simple modifications to a stock algorithm?
It has become a common notion that techniques such as introducing normalization layers (e.g. Ioffe
& Szegedy (2015), Ba et al. (2016), Chunjie et al. (2017), Salimans & Kingma (2016)) or careful
initial scaling of weights (e.g. He et al. (2015), Glorot & Bengio (2015), Saxe et al. (2014), Mishking
& Matas (2016)) largely eliminate exploding gradients by stabilizing forward activations. This
notion was espoused in landmark papers. The paper that introduced batch normalization (Ioffe
& Szegedy, 2015) states:
In traditional deep networks, too-high learning rate may result in the gradients that explode or
vanish, as well as getting stuck in poor local minima. Batch Normalization helps address these
issues.
The paper that introduced ResNet (He et al., 2016b) states:
Is learning better networks as easy as stacking more layers? An obstacle to answering this question
was the notorious problem of vanishing/exploding gradients, which hamper convergence from the
beginning. This problem, however, has been largely addressed by normalized initialization and
intermediate normalization layers, ...
We argue that these claims are overly optimistic. While scaling weights or normalizing forward ac-
tivations can reduce gradient growth defined according to certain metrics in certain situations, these
techniques are not effective in general and can cause other problems even when they are effective.
We intend to add nuance to these ideas which have been widely adopted by the community (e.g.
Chunjie et al. (2017); Balduzzi et al. (2017)). In particular, we intend to correct the misconception
that stabilizing forward activations is sufficient for avoiding exploding gradients (e.g. Klambauer
et al. (2017)).
ResNet (He et al., 2016b) and other neural network architectures utilizing skip connections (e.g.
Huang et al. (2017), Szegedy et al. (2016)) have been highly successful recently. While the per-
formance of networks without skip connections starts to degrade when depth is increased beyond a
certain point, the performance of ResNet continues to improve until a much greater depth is reached.
While favorable changes to properties of the gradient brought about by the introduction of skip
connections have been demonstrated for specific architectures (e.g. Yang & Schoenholz (2017);
Balduzzi et al. (2017)), a general explanation for the power of skip connections has not been given.
Our contributions are as follows:
1.	We introduce the ‘gradient scale coefficient’ (GSC), a novel measurement for assessing the
presence of pathological exploding gradients (section 2). It is robust to confounders such
as network scaling (section 2) and layer width (section 3) and can be used directly to show
that training is difficult (section 4). Therefore, we propose the unification of research on
the exploding gradient problem under this metric.
2.	We demonstrate that exploding gradients are in fact present in a variety of popular MLP
architectures, including architectures utilizing techniques that supposedly combat explod-
ing gradients. We show that introducing normalization layers may even exacerbate the
exploding gradient problem (section 3).
3.	We show that exploding gradients as defined by the GSC are not a numerical quirk to be
overcome by rescaling different parts of the gradient vector, but are indicative of an in-
herently complex optimization problem and that they limit the depth to which MLP archi-
2
Workshop track - ICLR 2018
tectures can be effectively trained, rendering very deep MLPs effectively much shallower
(section 4). To our knowledge, this is the first time such a link has been established.
4.	For the first time, we show why exploding gradients are likely to occur in deep networks
even when the forward activations do not explode (section 5). We argue that this is a
fundamental reason for the difficulty of constructing very deep trainable networks.
5.	For the first time, we define the ‘collapsing domain problem’ for training very deep feed-
forward networks. We show how this problem can arise precisely in architectures that
avoid exploding gradients via careful initial scaling of weights and that it can be at least as
damaging to the training process (section 6).
6.	For the first time, we show that the introduction of skip connections has a strong gradient-
reducing effect on deep network architectures in general. We detail the surprising mathe-
matical relationship that makes this possible (section 7).
7.	We introduce the ‘residual trick’ (section 4), which reveals that ResNets are a mathemati-
cally simpler version of networks without skip connections and thus approximately achieve
what we term the ‘orthogonal initial state’. This provides, we argue, the major reason for
their superior performance at great depths as well as an important criterion for neural net-
work design in general (section 7).
In section 8, we conclude and derive practical recommendations for designing and training deep
networks as well as key implications of our work for deep learning research.
In the appendix in section B, we provide further high-level discussion. In section B.1, we dis-
cuss related work including the relationship of exploding gradients with other measures of network
trainability, such as eigenspectrum analysis (Saxe et al., 2014), shattering gradients (Balduzzi et al.,
2017), trajectory lengths (Raghu et al., 2017), covariate shift (e.g. (Ioffe & Szegedy, 2015)) and Hes-
sian conditioning (e.g. (Luo, 2017)). Recently, the behavior of neural networks at great depth was
analyzed using mean field theory (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz,
2017; Anonymous, 2018d) and dynamical systems theory (Haber et al., 2017; Haber & Ruthotto,
2017; Chang et al., 2017; Anonymous, 2018a). We discuss these lines of work in relation to this
paper in sections B.1.1 and B.1.2 respectively. We discuss the implications of our work for the van-
ishing gradient problem in section B.2. We compare the exploding gradient problem as it occurs
in feedforward networks to the exploding and vanishing gradient problems in RNNs (e.g. Pascanu
et al. (2013)) in section B.3. In section B.4, we highlight open research questions and potential
future work.
2	Exploding gradients defined - the gradient scale coefficient
2.1	Notation and terminology
For the purpose of this paper, we define a neural network f as a succession of layers fl , 0 ≤ l ≤ L,
where each layer is a vector-to-vector transformation. We assume a prediction framework, where
the ‘prediction layer’ f1 is considered to output the prediction of the network and the goal is to
minimize the value of the error layer f0 over the network’s prediction and the true label y, summed
over some dataset D.
arg min E, where E =击 X fo(y, f1(θ1,f2(θ2, f3(..fL(θL,x)..))))	⑴
(x,y)∈D
Note that in contrast to standard notation, we denote by fL the lowest layer and by f0 the high-
est layer of the network as we are primarily interested in the direction of gradient flow. Let the
dimensionality / width of layer l be dl with d0 = 1 and the dimensionality of the data input x be d.
Each layer except f0 is associated with a parameter sub-vector θl that collectively make up the
parameter vector θ = (θ1, .., θL). This vector represents the trainable elements of the network.
Depending on the type of the layer, the sub-vector might be empty. For example, a layer composed
of tanh nonlinearities has no trainable elements, so its parameter sub-vector is empty. We call these
3
Workshop track - ICLR 2018
layers ‘unparametrized’. In contrast, a fully-connected linear layer has trainable weights, which are
encompassed in the parameter sub-vector. We call these layers ‘parametrized’.
We say a network that has layers f0 through fL has ‘nominal depth’ L. In contrast, we say the
‘compositional depth’ is equal to the number of parametrized layers in the network, which is the
quantity that is commonly referred to as “depth”. For example, a network composed of three linear
layers, two tanh layers and a softmax layer has nominal depth 6, but compositional depth 3.
Let the 'quadratic expectation' Q of a random variable X be defined as Q[X] = EX2] 1, i.e.
the generalization of the quadratic mean to random variables. Similarly, let the ‘inverse quadratic
expectation, QT of a random variable X be defined as Q[X] = E[X-2]-2. Further terminology,
notation and conventions used only in the appendix are given in section C.
2.2	Colloquial notion of exploding gradients
Colloquially, the exploding gradient problem is understood approximately as follows:
When the error is backpropagated through a neural network, it may increase exponentially from
layer to layer. In those cases, the gradient with respect to the parameters in lower layers may be
exponentially greater than the gradient with respect to parameters in higher layers. This makes the
network hard to train if it is sufficiently deep.
Let Jkl(θ, x, y) be the Jacobian of the l’th layer fl with respect to the k’th layer fk evaluated with
parameter θ at (x, y), where 0 ≤ l ≤ k ≤ L. Similarly, let Tkl (θ, x, y) be the Jacobian of the
l’th layer fl with respect to the parameter sub-vector of the k’th layer θk. Then we might take this
colloquial notion to mean that if ||Jkl || and / or ||Tkl || grow exponentially in k - l, according to some
to-be-determined norm ||.||, the network is hard to train ifit is sufficiently deep.
However, this notion is insufficient because we can construct networks that can be trained success-
fully yet have Jacobians that grow exponentially at arbitrary rates. In a nutshell, all we have to
do to construct such a network is to take an arbitrary network of desired depth that can be trained
successfully and scale each layer function fl and each parameter sub-vector θl by R-l for some
constant R > 1. During training, all we have to do to correct for this change is to scale the gradient
sub-vector corresponding to each layer by R-2l .
Proposition 1. Consider any r > 1 and any neural network f which can be trained to some error
level in a certain number of steps by some gradient-based algorithm. There exists a network f0 that
can also be trained to the same error level as f and to make the same predictions as f in the same
number of steps by the same algorithm, and has exponentially growing Jacobians with rate r. (See
section E.1 for details.)
Therefore, we need a definition of ‘exploding gradients’ different from ‘exponentially growing Ja-
cobians’ if we hope to derive from it that training is intrinsically hard and not just a numerical issue
to be overcome by gradient rescaling.
Note that all propositions and theorems are stated informally in the main body of the paper, for the
purpose of readability and brevity. In the appendix in sections E and F respectively, they are re-stated
in rigorous terms, proofs are provided and the practicality of conditions is discussed.
2.3	The gradient scale coefficient
In this section, we outline our definition of ‘exploding gradients’ which can be used to show hardness
of training. It does not suffer from the confounding effect outlined in the previous section.
Definition 1. Let the ‘quadratic mean norm’ or ‘qm norm’ ofan m × n matrix A be the quadratic
mean of its singular values where the sum of squares is divided by its right dimension n. If s1, s2,
.., smin(m,n) are the singular values ofA, we write:
IlAllqm =『+ Ss + ∙n+ Smin(m,n)
4
Workshop track - ICLR 2018
An equivalent definition would be ||A||qm = Qu||Au||2, where u is a uniformly random unit length
vector. In plain language, it measures the expected impact the matrix has on the length of a vector
with uniformly random orientation. The qm norm is closely related to the L2 norm via √n∣∣A∣∣qm =
||A||2. We use ||.||2 to denote the L2 norm of both vectors and matrices.
Definition 2. Let the ‘gradient scale coefficient (GSC)’ for 0 ≤ l ≤ k ≤ L be as follows:
GSC(k, l, f, θ, x, y)
IlJk I∣2mllfk∣l2
-≡-
Definition 3. We say that the network f(θ) has ‘exploding gradients with rate r and intercept
c’ at some point (x, y) if for all k and l we have GSC(k, l, f, θ, x, y) ≥ crk-l, and in particular
GSC(l,0,f,θ,x,y) ≥ crl.
Of course, under this definition, any network of finite depth has exploding gradients for sufficiently
small c and r. There is no objective threshold for c and r beyond which exploding gradients become
pathological. Informally, we will say that a network has ‘exploding gradients’ if the GSC can be
well-approximated by an exponential function.
The GSC combines the norm of the Jacobian with the ratio of the lengths of the forward activation
vectors. In plain language, it measures the size of the gradient flowing backward relative to the size
of the activations flowing forward. Equivalently, it measures the relative sensitivity of layer l with
respect to small random changes in layer k.
Proposition 2. GSC(k, l) measures the quadratic expectation of the relative size of the change in
the value of fl in response to a small random change in fk. (See section E.2 for details.)
What about the sensitivity of layers with respect to the parameter? For fully-connected linear layers,
we obtain a similar relationship.
Proposition 3. When fk is a fully-connected linear layer without trainable bias parameters and θk
contains the entries of the weight matrix, GSC(k,l) || θk || 2 || fk+dh- measures the quadratic expecta-
||fk ||2 dk+1
tion of the relative size of the change in the value offl in response to a small random change in θk.
Further, if the weight matrix is randomly initialized,
details.)
llθkll2llfk+lU2
|| fk || 2 √dk+1
1. (See section E.3 for
For reasons of space and mathematical simplicity, we focus our analysis for now on multi-layer
perceptrons (MLPs) which are comprised only of fully-connected linear layers with no trainable bias
parameters, and unparametrized layers. Therefore we also do not use trainable bias and variance
parameters in the normalization layers. Note that using very deep MLPs with some architectural
limitations as a testbed to advance the study of exploding gradients and related problems is a well-
established practice (e.g. Balduzzi et al. (2017); Yang & Schoenholz (2017); Raghu et al. (2017)).
As Schoenholz et al. (2017), we focus on training error rather than test error in our analysis as we
do not consider the issue of generalization. While exploding gradients have important implications
for generalization, this goes beyond the scope of this paper.
In section 2.2, we showed that we can construct trainable networks with exponentially growing
Jacobians by simple multiplicative rescaling of layers, parameters and gradients. Crucially, the GSC
is invariant to this rescaling as it affects both the forward activations and the Jacobian equally, so the
effects cancel out.
Proposition 4. GSC(k, l) is invariant under multiplicative rescalings of the network that do not
change the predictions or error values of the network. (See section E.4 for details.) 3 * *
3	Gradients explode - despite bounded activations
In this section, we show that exploding gradients exist in a range of popular MLP architectures.
Consider the decomposability of the GSC.
5
Workshop track - ICLR 2018
(A)10000
000
001
01
1
tneicffieoC elacS tneidarG
18642 0
................................
0000
)
BnoitaiveD dradnatS noitavitcA-erP
40
30
20
0
10	20	30	40	50	0
Nonlinearity layer
ytisreviD ngiS noitavitcA-er
(C) 0.5
0.4
0.3
0.2
0.1
10	20	30	40	50
Nonlinearity layer
Compositional depth
O
Figure 1: Key metrics for architectures in their randomly initialized state evaluated on Gaussian
noise. The x axis in the left graph shows depth in terms of the number of linear layers counted from
the input. Note: The curve for ReLU-layer is shadowed by tanh in the figure A and by ReLU in
figure C.
Proposition 5. Assuming the approximate decomposability of the norm of the product of Jaco-
bians, i.e. ||Jll+1Jll++21..Jkk-1||qm ≈ ||Jll+1||qm||Jll++21||qm..||Jkk-1||qm, we have GSC(k, l) ≈
GSC(k, k - 1)GSC(k - 1,k - 2)..GSC(l + 1, l). (See section E.5 for the proof.)
This suggests that as long as the GSC of individual layers is approximately r > 1, we may have an
exponential growth of GSC(k, l) in k - l. In figure 1A, we show GSC(l, 0) for seven MLP archi-
tectures. A linear layer is followed by (i) a ReLU nonlinearity (‘ReLU’), (ii) layer normalization
(Ba et al., 2016) followed by a ReLU nonlinearity (‘layer-ReLU’), (iii) batch normalization plus
ReLU (‘batch-ReLU’), (iv) tanh, (v) layer norm plus tanh (‘layer-tanh’), (vi) batch norm plus tanh
(‘batch-tanh’), (vii) SeLU Klambauer et al. (2017). All networks have compositional depth 50 (i.e.
50 linear layers) and each layer has 100 neurons. Both data input and labels are Gaussian noise and
the error layer computes the dot product between the label and the prediction. The entries of weight
matrices are dawn from independent Gaussian distributions with mean zero. Weight matrix entries
for ReLU architectures are initialized with variance 斋 as suggested by He et al. (2015), weight
matrix entries for tanh architectures with variance 忐 as suggested by Saxe et al. (2014) and Glorot
& Bengio (2015), and weight matrix entries for SeLU architectures with variance 忐 as suggested
by Klambauer et al. (2017). For further experimental details, see section I.
We find that in four architectures (batch-ReLU, layer-tanh, batch-tanh and SeLU), GSC(l, 0) grows
almost perfectly linearly in log-space. This corresponds to gradient explosion. We call those archi-
tectures ‘exploding architectures’. Among these architectures, a range of techniques that supposedly
reduce or eliminate exploding gradients are used: careful initial scaling of weights, normalization
layers, SeLU nonlinearities. Adding normalization layers may even bring about or exacerbate ex-
ploding gradients. The exploding architectures have all been designed to have stable forward acti-
vations and they exhibit gradient explosion under any reasonable metric.
In light of proposition 4, it is not surprising that these techniques are not effective in general at
combating exploding gradients as defined by the GSC, as this metric is invariant under multiplicative
rescaling. Normalization layers are used to scale the activations. Carefully choosing the initial scale
of weights corresponds to a multiplicative scaling of weights. SeLU nonlinearities, again, act to
scale down large activations and scale up small activations. While these techniques may of course
impact the GSC by changing the fundamental mathematical properties of the network (as can be
seen, for example, when comparing ReLU and batch-ReLU), they do not reduce it simply by virtue
of controlling the size of forward activations.
In contrast, the other three architectures (ReLU, layer-ReLU and tanh) do not exhibit exploding
gradients. However, this apparent advantage comes at a cost, as we further explain in section 5.
All curves in figure 1A exhibit small jitters. This is because we plotted the value of the GSC at
every linear layer, every normalization layer and every nonlinearity layer in the graph and then
connected the points corresponding to these values. Layers were placed equispaced on the x axis
in the order they occurred in the network. Not every type of layer affects the GSC equally. In
6
Workshop track - ICLR 2018
fact, we find that as gradients pass through linear layers, they tend to shrink relative to forward
activations. In the exploding architectures, this is more than counterbalanced by the relative increase
the gradient experience as it passes through e.g. normalization layers. Despite these layer-dependent
differences, it is worth noting that each individual layer used in the architectures studied has only a
small impact on the GSC. This would not be true for either the forward activations or gradients taken
by themselves. For example, passing through a ReLU layer reduces the length of both activation
and gradient vector by ≈ √2. This relative invariance to individual layers suggests that the GSC
measures not just a superficial quantity, but a deep property of the network. This hypothesis is
confirmed in the following sections.
Finally, we note that the GSC is also robust to changes in width and depth. Changing the depth has
no impact on the rate of explosion of the four exploding architectures as the layer-wise GSC, i.e.
GSC(l + 1, l), is itself independent of depth. In figure 1A, we also show the results for the SeLU
architecture where each layer contains 200 neurons instead of 100 (‘SeLU wide’). We found that the
rate of gradient explosion decreases slightly when width increases. We also studied networks with
exploding architectures where the width oscillated from layer to layer. GSC(k, 0) still increased
approximately exponentially and at a similar rate to corresponding networks with constant width.
A summary of results can be found in table 1.
4	Exploding gradients limit depth - the residual trick
4.1	Background: Effective depth
In this section, we introduce the concept of ‘effective depth’ as defined for the ResNet architecture
by Veit et al. (2016). We denote a residual network by writing each layer fl (except f0) as the sum
of a fixed initial function il and a residual function rl . We define the optimization problem for a
residual net analogously to equation 1.
arg min E, where E =焉 X f0(y,(i1+ r1(θ1)) ◦ (i2 + r2(θ2)) ◦ .. ◦ (iL + 北仇))。x)⑵
(x,y)∈D
Let’s assume for the sake of simplicity that the dimension of each layer is identical. In that case, the
initial function for ResNet is generally chosen to be the identity function. Then, writing the identity
matrix as I , we have
dfo
dx
f (I + f )(I + f )..(1 + f )(I + T)
Multiplying out, this becomes the sum of 2L terms. Almost all of those terms are the product of
approximately LL identity matrices and LL residual Jacobians. However, if the operator norm of the
residual Jacobians is less than p for some p < 1, the norm of terms decreases exponentially in
the number of residual Jacobians they contain. Let the terms in df0 containing λ or more residual
Jacobians be called 'λ-residual' and let resλ be the sum of all λ-residual terms. Then:
∣∣resλ∣∣2 ≤
Again, if p < 1, the right hand side decreases exponentially in λ for sufficiently large λ, for example
when λ > LL, so the combined size of λ-residual terms is exponentially small. Therefore, Veit et al.
(2016) argue, the full set of network layers does not jointly co-adapt during training because the
information necessary for such co-adaption is contained in terms that contain many or all residual
Jacobians. Only sets of layers of size at most λ where resλ is not negligably small co-adapt. The
largest such λ is called the ‘effective depth’ of the network. Veit et al. (2016) argue that if the
effective depth is less than the compositional depth of a residual network, the network is not really as
deep as it appears, but rather behaves as an ensemble of relatively shallow networks. This argument
7
Workshop track - ICLR 2018
is bolstered by the success of the stochastic depth (Huang et al., 2016) training technique, where
random sets of residual functions are deleted for each mini-batch update.
Veit et al. (2016) introduced the concept of effective depth somewhat informally. We give our formal
definition in section D. There, we also provide a more detailed discussion of the concept and point
out limitations.
4.2	The residual trick
Now we make a crucial observation. Any neural network can be expressed as a residual network as
defined in 2. We can simply choose arbitrary initial functions il and define rl (θl) := fl (θl) - il .
Specifically, ifwe train a network f from some fixed initial parameter θ(0), we can set il := fl(θl(0))
and thus rl (θl) := fl (θl) - fl (θl(0) ). Then training begins with all residual functions being zero
functions. Therefore, all analysis devised for ResNet that relies on the small size of the residual
Jacobians can then be brought to bear on arbitrary networks. We term this the ‘residual trick’.
Indeed, the analysis by Veit et al. (2016) does not rely on the network having skip connections in
the computational sense, but only on the mathematical framework of equation 2. Therefore, as long
as the operator norms of f (：；) - df* ) are small, f is effectively shallow.
Terminology From now on, we will make a distinction between the terms ’ResNet’ and ‘residual
network’. The former will be used to refer to networks that have an architecture as in He et al.
(2016b) that uses skip connections. The latter will be used to refer to arbitrary networks expressed
in the framework of equation 2. Networks without skip connections will be referred to as ‘vanilla
networks’.
4.3	Theoretical analysis
In this section, we will show that an exploding gradient as defined by the GSC causes the effective
training time of deep MLPs to be exponential in depth and thus limits the effective depth that can be
achieved.
The proof is based on the insight that the relative size of a gradient-based update ∆θl on θl is
bounded by the inverse of the GSC if that update is to be useful. The basic assumption underly-
ing gradient-based optimization is that the function optimized is locally approximated by a linear
function as indicated by the gradient. Any update made based on a local gradient computation must
be small enough so that the updated value lies in the region around the original value where the
linear approximation is sufficiently accurate. Let’s assume we apply a random update to θl with
relative magnitude gs/” b, i.e. ll∣谓12 = GSC(O ?). Then under the local linear approximation,
according to proposition 3, this would change the output f0 approximately by a value with quadratic
expectation f0 . Hence, with significant probability, the error would become negative. This is not
reflective of the true behavior of the function f0 in response to changes in θl of this magnitude. Since
gradient-based updates impact the function value even more than random updates, useful gradient-
based updates are even more likely to be bounded in relative magnitude by GSC(O ?).
In a nutshell, if GSCI(O ?) decreases exponentially in l, so must the relative size of updates. So for a
residual function to reach a certain size relative to the corresponding initial function, an exponential
number of updates is required. But to reach a certain effective depth, a certain magnitude of λ-
residual terms is required and thus a certain magnitude of residual functions relative to corresponding
initial functions is required, and thus exponentially many updates.
Theorem 1. Under certain conditions, if an MLP has exploding gradients with explosion rate r and
intercept c on some dataset, then there exists a constant c0 such that training this neural network
with a gradient-based algorithm to have effective depth λ takes at least Ccrλ updates. (See section
F.1 for details.)
Importantly, the lower bound on the number of updates required to reach a certain effective depth
stated by theorem 1 is independent of the nominal depth of the network. While the constant c0
depends on some constants that arise in the conditions of the theorem, as long as those constants do
not change when depth is increased, neither does the lower bound.
8
UolaUBIdxoJOJ Ix-∙sEUIo°SolXVH lɔ Uo po∙sEbsoJnso=ΨHB JOJ so∙csUI A°M ZoJn∙sp工
UoI⅛np<υJ qκφp IEUOla®OduIOO
g 寸。「 Sg Om gN ON gɪsg 0
0
J<υx⅛I JBeUlHq
o
c» co	Ol O
OOOO
lpodM
Oog CW c≡ Ooz OOI 0
-----._ 0
O

§

O
O
ɔ -g
Ξ
d
O M
o
与
I
O
ɔ,ɪ
O
巴 ɪouə uoι^υoμτssυ∣o Smurejjj
O
O
I 。 IGlCo 寸 g
φ	φ	I	I	I	I	I
119	9	9 9a
z,、	I-I I-I I-I I-I I-I
V PUnoq q其丛 əzɪs ə^pdn əʌɪ^ɪəʌ^
8107 XUɔɪ — 5PEbdoqs*JOM
6
əMOqS əM cz 号UIMAqlsəs də1s pəɪŋɔs əM Cɪŋpokəd .səieuipsə pəipooUlS
əsOql qM Sqɔodə Oo^ JOJ mOMləu qoE5 pəuu PuB sə-euipsə əs də-s Isoq əsətp PoqlOOUIS uəip əʌv
SPloq IuləjoəEJωpun uldumssE MUpunoJ əlp
əjojəji .əiej jəmw E qM səpdXə luPEJDql 匕 WOUnOUOJd əjouilɔəjjə slql PUEəlBPdn
JgIEUISəməj əifnbəj sjəAjəmieip puəj- JBgO EJ弓SlInsgASu səaJAEl qoEJoJ
əs də-s Isoq əipuμ!πωp JOJ UnPyOIE ⅛0 qnoql UOA∙SJωnoJoqumu UEUlS E JoJldəɔXə
əmeajəsusPuB SOq PUnOq slql PUy 段Epdn InJəsn E JO əs əməj OqlJOJ PUnOq jəddn
UB(7)3Dql wnIB əM C寸 ulɔəs Ul əu= POqSEP E SE (7)3D SE =gM SE əs də-s Isoq əip Oq
pə-ɔəɪəs SEMqM Aq jəAEl JBu= qoE5 UOPəɔnpu1s gpdn dməj əip MoqS əM CVZ 号Ul
寸1 uoɔəs
£ uAWUowlsɪʧ E PuB uoɔəɪəs əs də-s JOJUnPoIE UnJ əm κqsIqəM əsnpɔ UEɔ
sə-BPdn jəmw əsnEɔəg CSS9 jo Io JO səs ə-ŋpdn əməj oonp£ql səs də-s pəjəpusUo əM
ql ə-oN .pIS SEModəql jə-jE Jojjə UowssepulsωMətp pəAəɔEql əs də1s
əm sjəAEl W-O1zωω,y əim səs də-s SnOpBA qModə IJOJJEIqlUOPəuu əM
SAIBəuHqəeə JOJ euəm SqɔodəJOJ ISOuIInqC8 Mωq SEM Jojjə uossou
əUn əs də-s UnoJn UEUlS EqM 3μoMu qoE5 JO sjəAEllsωqqqluWəjd Aq pəw-s
段 女jbəu= Wpwpe qoE5 JOJ αSJOJ Ds də-s Isoq 0uηx0JddE ətp pəeuuəiəp əM ClSJ
•I uoɔəs £ PUnOJ Oq UEɔ SlRləp wuUIydxI4 S jəAJojjə PuB ulPOJd
qndOqlJOJ -dəɔXə jəAElqəeə £ SUaməu OOIPUB (SjəAjbəuhə.DJo ipdəp -EUosodu18
E OAEq PgpnlsJ-əu =VxvHIO co (∩DS PUEqUlqoq CqUBJ—JEI c∩Dxlqoq) səjnl
—ɔəlɔɪBpdx5 JnOJ JnO uəMdəp əaəəjjə pəuɪ= JO AJOOqlJnOEPXMAMEjd OH
SlNHnraHdx寸•寸
UnPyOIEql qM 9qEqoEəjun Wqldəp
əaəəjjə jəieəjql uəlpdəp əaəəjjə jəieəjE Eolpənbəj Oq Pw əjouiql sωp
I UlOJOOqllnqəlBPdn JO Joqumu URlJəɔ E jə-jE əəuəmjəaus IP© əMqlsduηs əM əjəH
.dap 21o2
OSePaPUHoq2224SDPdn jo ∖3qumu 2⅛ 7 ula'oaw jo OMml0s 2Iq ∙I AJjoɔ
Workshop track - ICLR 2018
training classification error of each architecture. There is a trend that architectures with less gradient
explosion attain a lower final error. Note that, of course, all these error values are still much higher
than the state of the art on CIFAR10. This is not a drawback however, as the goal of this section is to
study and understand pathological architectures rather than find optimal ones. Those architectures,
by definition, attain high errors.
In figure 2C, we show the GSC across the entire network, i.e. GSC(L, 0), as training progresses.
During the initial pre-training phase, this value drops significantly but later regains or even exceeds
its original value. In figure 2A, the dashed line indicates the inverse of GSC(l, 0) for each l after
pre-training. We find that the GSC actually falls below 1 as the gradient passes through the pre-
trained layers, but then resumes explosion once it reached the layers that were not pre-trained. We
find this behavior surprising and unexpected. We conclude that nonstandard training procedures can
have a significant impact on the GSC but that there is no evidence that when all layers are trained
jointly, which is the norm, the GSC either significantly increases or decreases during training.
We then went on to measure the effective depth of each network. We devised a conservative, compu-
tationally tractable estimate of the cumulative size of updates that stem from λ-residual terms. See
section D.2 for details. The effective depth depicted in figure 2D is the largest value of λ such that
this estimate has a length exceeding 10-6. As expected, none of the architectures reach an effective
depth equal to their compositional depth, and there is a trend that architectures that use relatively
smaller updates achieve a lower effective depth. It is worth noting that the effective depth increases
most sharply at the beginning of training. Once all step sizes have been multiplied by 1 several
times, effective depth no longer changes significantly while the error, on the other hand, is still go-
ing down. This suggests that, somewhat surprisingly, high-order co-adaption of layers takes place
towards the beginning of training and that as the step size is reduced, layers are fine-tuned relatively
independently of each other.
SeLU and especially tanh-batch reach an effective depth close to their compositional depth according
to our estimate. In figure 2E, we show the operator norm of the residual weight matrices after
training. All architectures except SeLU, which has a GSC(L, 0) close to 1 after pre-training, show
a clear downward trend in the direction away from the error layer. If this trend were to continue for
networks that have a much greater compositional depth, then those networks would not achieve an
effective depth significantly greater than our 51-linear layer networks.
Veit et al. (2016) argue that a limited effective depth indicates a lack of high-order co-adaptation. We
wanted to verify that our networks, especially layer-tanh and batch-ReLU, indeed lack these high-
order co-adaptations by using a strategy independent of the concept of effective depth to measure
this effect. We used Taylor expansions to do this. Specifically, we replaced the bottom k layers of the
fully-trained networks by their first-order Taylor expansion around the initial functions. See section
G for how this is done. This reduces the compositional depth of the network by k - 2. In figure 2F,
we show the training classification error in response to compositional depth reduction. We find that
the compositional depth of layer-tanh and batch-ReLU can be reduced enormously without suffering
a significant increase in error. In fact, the resulting layer-tanh network of compositional depth 15
greatly outperforms the original batch-tanh and batch-ReLU networks. This confirms that these
networks lack high-order co-adaptations. Note that cutting the depth by using the Taylor expansion
not only eliminates high-order co-adaptions among layers, but also co-adaptions of groups of 3 or
more layers among the bottom k layers. Hence, we expect the increase in error induced by removing
only high-order co-adaptions to be even lower than what is shown in figure 2F. Unfortunately, this
cannot be tractably computed.
Finally, we trained each of the exploding architectures by using only a single step size for each layer
that was determined by grid search, instead of custom layer-wise step sizes. As expected, the final
error was higher. The results are found in table 2.
Summary For the first time, we established a direct link between exploding gradients and severe
training difficulties that cannot be overcome by gradient rescaling. These difficulties arise in MLPs
composed of the most popular layer types, even those that utilize techniques that stabilize forward
activations which are believed to combat exploding gradients. The gradient scale coefficient not only
underpins this analysis, but is largely invariant to the confounders of network scaling (proposition
4), layer width and individual layers (section 3). Therefore we argue the GSC is the best metric for
the study of exploding gradients in general.
10
Workshop track - ICLR 2018
4.5 A note on batch normalization
We used minibatches of size 1000 to train all architectures except batch-ReLU, for which we con-
ducted full-batch training. When minibatches were used on batch-ReLU, the training classification
error stayed above 89% throughout training. (Random guessing achieves a 90% error.) In essence,
no learning took place. This is because of the pathological interplay between exploding gradients
and the noise inherent in batch normalization. Under batch normalization, the activations at a neuron
are normalized by their mean and standard deviation. These values are estimated using the current
batch. Hence, if a minibatch has size b, we expect the noise induced by this process to have relative
size ≈ -√ζ. But We know that according to proposition 2, under the local linear approximation,
this noise leads to a change in the error layer of relative size ≈ G√C. Hence, if the GSC between
the error layer and the first batch normalization layer is larger than √b, learning should be seri-
ously impaired. For the batch-ReLU architecture, this condition was satisfied and consequently, the
architecture was untrainable using minibatches. Ironically, the gradient explosion that renders the
noise pathological was introduced in the first place by adding batch normalization layers. Note that
techniques exist to reduce the dependence of batch normalization on the current minibatch, such as
using running averages (Ioffe, 2017). Other prominent techniques that induce noise and thus can
cause problems in conjunction with large gradients are dropout Srivastava et al. (2014), stochastic
nonlinearities (e.g. Gulcehre et al. (2016)) and network quantization (e.g. (Anonymous, 2018c)).
5	Why gradients explode - determinants vs qm norm
Why do exploding gradients occur? As mentioned in section 3, gradients explode with rate r > 1
as long as we have (A) GSC(k, l) ≈ GSC(l + 1,1)GSC(l + 2,l + 1)..GSC(k, k - 1) and (B)
GSC(l + 1, 1) ≈ r for all k and l. It turns out that we can show both of these hold in expectation
under fairly realistic conditions if we view the network parameter as a random variable.
Theorem 2. Under certain conditions, for any neural network f with random parameter θ com-
posed of layer functions fl that are surjective endomorphisms on the hypersphere, where the abso-
lute singular values of the Jacobian of each layer are IID and differ by at least with probability δ,
gradients explode in expectation with rate r(δ, ). (See section F.2 for details.)
Proof Summary. Consider a surjective endomorphism fl on the hypersphere and a random input x
distributed uniformly on that hypersphere. Surjectivity implies that the absolute determinant of the
Jacobian, in expectation over the input, is at least 1. The absolute determinant is the product of the
absolute singular values. If those absolute singular values are IID and their expected product is at
least 1, the expectation of each absolute singular value is also at least 1. So if these singular values
are sufficently different from each other with sufficient probability, the expected quadratic mean of
the singular values is at least r > 1. Since both input and output of fl have length 1, the expected
GSC will also be at least r.
□
While the conditions of theorem 2 cannot be fulfilled exactly, it nevertheless reveals an important in-
sight. Exploding gradients tend to arise in practice even when forward activations are stable because
in order to preserve the domain of the forward activations from layer to layer, Jacobians of individual
layers need to have unit absolute determinants in expectation, and this tends to cause their qm norm
values to be greater than 1, and then these values tend to compound exponentially.1 The theorem is
stated for layers that are surjective endomorphisms on the hypersphere. Let ‘length-only layer nor-
malization’ (LOlayer) be a function that divides its input vector by its length. Then a sequence of a
tanh / SeLU layer, a linear layer and a LOlayer layer, all of the same width, when viewed as a single
macro-layer, is a surjective endomorphism on the hypersphere, as shown in proposition 6. Conse-
quently, both LOlayer-tanh and LOlayer-SeLU exhibit exploding gradients (table 1). Layer-tanh and
SeLU explode at very similar rates to LOlayer-tanh and LOlayer-SeLU respectively.
1It is possible to define probability distributions over matrices where the expected determinant is 1 and the
expected qm norm is less than 1, though these examples are contrived and tend not to occur in practice.
11
Workshop track - ICLR 2018
Proposition 6. Any endomorphism on the hypersphere composed of (i) a strictly monotonic, contin-
uous nonlinearity σ that has σ(0) = 0, (ii) multiplication with a full-rank matrix and (iii) length-only
layer normalization is bijective. (See section E.6 for the proof.)
Theorem 2 presents two clear avenues for avoiding exploding gradients: (i) use non-surjective layer
functions, (ii) ensure that Jacobians get progressively closer to multiples of orthogonal matrices
as we go deeper. It turns out that these are exactly the strategies employed by ReLU and ResNet
respectively to avoid exploding gradients, and we will discuss these in the next two sections.
6	The collapsing domain problem - how not to reduce gradients
In the previous section, we showed how surjective endomorphisms can exhibit exploding gradients.
This suggests that we can avoid exploding gradients by non-surjectivity, i.e. if we reduce the do-
main of the forward activations from layer to layer. Informally, this can be understood as follows.
Consider some layer function fl. Ifwe shrink its co-domain by a factor c, we reduce the eigenvalues
of the Jacobian and hence its qm norm by c. If we also ensure that the length of the output stays the
same, the GSC is also reduced by c. Similarly, inflating the co-domain would cause the qm norm
to increase.
This suggests that in neural network design, we can actively trade off exploding gradients and shrink-
age of the domain and that eliminating one effect may exacerbate the other. This is precisely what
we find in practice. Returning to figure 1, we now turn our attention to the middle graph (1B). Here,
we plot the standard deviation of the activation values at each neuron across datapoints in the lay-
ers before each nonlinearity layer (‘pre-activations’), averaged over all neurons in the same layer.
The four exploding architectures exhibit a near constant standard deviation, whereas the other three
architectures (ReLU, layer-ReLU and tanh) exhibit a rapidly collapsing standard deviation, which
shows that the activations corresponding to different datapoints become more and more similar with
depth. We term a layer-to-layer shrinkage of the domain the ‘collapsing domain problem’. But why
is this effect a problem? Two reasons.
Collapsing depth causes pseudo-linearity If the pre-activations that are fed into a nonlinearity
are highly similar, the nonlinearity can be well-approximated by a linear function. In the tanh archi-
tecture we studied, for example, activation values become smaller and smaller as they are propagated
forward. If the pre-activations of a tanh nonlinearity have small magnitude, the tanh nonlinearity can
be approximated by the identity function. But if a tanh layer is approximately equal to an identity
layer, the entire network becomes equivalent to a linear network. We say the network becomes
‘pseudo-linear’. Of course, linear networks of any depth have the representational capacity of a
linear network of depth 1 and are unable to model nonlinear functions. Hence, a tanh network that
is pseudo-linear beyond compositional depth k approximately has the representational capacity of
a compositional depth k + 1 tanh network. Based on the decrease in pre-activation standard devi-
ation exhibited by the tanh architecture in figure 1B, a reasonable estimate is that the network of
compositional depth 50 has the representational capacity of a network of compositional depth 10.
Similarly, for a ReLU nonlinearity, if either all or most pre-activations are positive or all or most
pre-activations are negative, the nonlinearity can be approximated by a linear function. If all or
most pre-activations are positive, ReLU can be approximated by the identity function. If all or most
pre-activations are negative, ReLU can be approximated by the zero function. In figure 1C, we plot
the proportion of pre-activations for each neuron in a nonlinearity layer that are positive or negative,
whichever is smaller, averaged over each layer. We call this metric ‘sign diversity’. For both ReLU
and layer-ReLU, sign diversity decreases rapidly to cause pseudo-linearity from at least, say, the
20th linear layer onwards. None of the four exploding architectures suffers a significant loss in sign
diversity.
Collapsing depth is exploding gradient in disguise In theorem 1, we used the fact that the output
of the error layer of the network was positive to bound the size of a useful gradient-based update.
In other words, we used the fact that the domain of the error layer was bounded. However, the
collapsing domain problem causes not just a reduction of the size of the domain of the error layer,
but of all intermediate layers. Hence, we expect the largest useful update to shrink in proportion
with the reduction of the size of the domain. Therefore, we suspect that a collapsing domain will
12
Workshop track - ICLR 2018
ultimately have the same effect on the largest useful update size of each layer as exploding gradients,
that is to reduce them and thus cause a low effective depth.
In table 2, we show the final error values achieved by training ReLU, layer-ReLU and tanh on
CIFAR10. The errors are substantially higher than those achieved by the exploding architectures,
except for batch-ReLU. Also, training with layer-wise step sizes did not help compared to training
with a single step size. In figure 4A, we show the estimated best relative update sizes for each layer.
This time, there is no downward trend towards lower layers, which is likely why training with a
single step size is “sufficient”. Interestingly, We note that the difference between the GSIC bound
and the empirically optimal relative update sizes is now much larger than it is for the exploding
architectures (see figure 2A). This suggests that indeed, collapsing domains may similarly reduce
the optimal relative update size, just as exploding gradients. In figure 4D, we find that again, the
effective depth reached is significantly lower than the compositional depth of the network and is
comparable to that of architectures with exploding gradients.
In figure 4G and H, we plot the pre-activation standard deviation and sign diversity at the highest
nonlinearity layer throughout training. Interestingly, pseudo-linearity declines significantly early in
training. The networks become less linear through training.
Summary In neural network design, there is an inherent tension between avoiding exploding gra-
dients and preserving the domain of forward activations. Avoiding one effect can bring about or ex-
acerbate the other. Both effects are capable of severely hampering training. This tension is brought
about by the discrepancy of determinant and qm norm of layer-wise Jacobians and is a foundational
reason for the difficulty in constructing very deep trainable network.
In this paper, we do not give a rigorous definition of the collapsing domain problem, because it
is hard to assess and measure. A number of metrics exist which all apply is somewhat different
situations. We already discussed two metrics: pre-activation standard deviation and pre-activation
sign diversity. In mean field theory, activation correlation plays a prominent role (section B.1.1).
In fact, mean field theory is a formidable tool for statically estimating the properties of specific
architectures, such as explosion rate and pre-activation standard deviation. See e.g. Schoenholz
et al. (2017) for an anlysis of tanh and Arpit et al. (2016) for an analysis of batch-ReLU. We discuss
collapsing domains further in the future work section B.4.
7	ResNet reduces the gradient - and ResNet explained
ResNet and related architectures that utilize skip connections have been very successful recently.
One reason for this is that they can be successfully trained to much greater depths than vanilla
networks. In this section, we show how skip connections are able to greatly reduce the GSC and
thus largely circumvent the exploding gradient problem.
Let sb, 1 ≤ b ≤ B be the function computed by the b’th skip connection. Let ρb be the function
computed by the b’th residual block. b = B corresponds to the lowest block and b = 1 corresponds
to the highest block. Let fb = sb + ρb .
Definition 4. We say a function fb is ‘k-diluted’ with respect to a random vector v if there exists a
matrix Sb and a function Pb such that fb(v) = Sbv + ρb(v) and『［以北卮 =k.
k-dilution expresses the idea that the kinds of functions that fb represents are of a certain form if sb
is restricted to matrix multiplication. The larger the value of k, the more ρb is “diluted” by a linear
function, bringing fb itself closer and closer to a linear function. Note that the identity function can
be viewed as matrix multiplication with the identity matrix.
Theorem 3. Under certain conditions, if a function ρb would cause the GSC to grow with expected
rate r, k-diluting Pb with an uncorrelated linear transformation reduces this rate to 1 + k-吉 +
O((r - 1)2). (See section F.3 for details.)
This reveals the reason why ResNet circumvents the exploding gradient problem. Diluting the block
function by a factor k does not just reduce the growth of the GSC by factor k, but by k2+1. Therefore
what appears to be a relatively mild reduction in representational capacity achieves, surprisingly, a
13
Workshop track - ICLR 2018
(A)10
O
10	20	30	40	50
ytisreviD ngiS noitavitcA-er
(B)0.5
0.4
0.3
0.2
0.1
10	20	30	40	50
)detcerroc-noitulid( CS
(C)100000
10000
1000
100
10
1
0	10	20	30	40	50
Compositional depth	Nonlinearity layer	Compositional depth
Figure 3: Key metrics for ResNet architectures at various depths. Left and right graph only show
values between skeip connections. The x axis shows depth in terms of the number of linear layers
counted from the input. Note: The curve for layer-tanh and batch-tanh is shadowed by SeLU in the
right graph.
relatively large amount of gradient reduction, and therefore ResNet can be trained successfully to
“unreasonably” great depths for general architectures.
To validate our theory, we repeated the experiments in figure 1 with 5 ResNet architectures: layer-
ReLU, batch-ReLU, layer-tanh, batch-tanh and layer-SeLU. Each residual block is bypassed by an
identity skip connection and composed of 2 sub-blocks of 3 layers each: first a normalization layer,
then a nonlinearity layer, and then a fully-connected layer, similar to He et al. (2016a); Zaguroyko &
Komodakis (2016). For further architectural details, see section I.1. Comparing figure 3A to figure
1A, we find the gradient growth is indeed much lower for ResNet compared to vanilla networks, with
much of it taking place in the lower layers. In figure 3B we find that the rate of domain collapse for
layer-ReLU, as measured by pre-activation sign diversity, is also significantly slowed.
We then went on to check whether the gradient reduction experienced is in line with theorem 3.
We measured the dilution level kb and growth rate rb at each residual block b and then replaced
the growth rate with 1 + (kb2 + 1)(rb - 1). The result of this post-processing is found in figure
3C. Indeed, the GSC of the exploding architectures now again grows almost linearly in log space,
with the exception of batch-ReLU in the lowest few layers. The explosion rates closely track those
in figure 1A, though they are slightly higher. This confirms that the estimate of the magnitude of
gradient reduction from theorem 3 is quite accurate.
We then repeated the CIFAR10 experiments depicted in figure 2 with our 5 ResNet architectures.
The results are shown in figure 5. As expected, in general, ResNet enables higher relative update
sizes, achieves lower error, a higher effective depth and is less “robust” to taylor approximation than
vanilla networks. The only exception to this trend is the layer-SeLU ResNet when compared to
the SeLU vanilla network, which already has a relatively slowly exploding gradient to begin with.
Note that the severe reduction of the GSC persists throughout training (figure 5C). Also see table
2 to compare final error values. Note that in order to make the effective depth results in figure
5D comparable to those in figure 2D, we applied the residual trick to ResNet. We let the initial
function i encompass not just the skip function s, but also the initial block function ρ. Hence, we
set ib = sb + ρb (θ(0)) and rb = ρb(θ) - ρb(θ(0)). Note that our effective depth values for ResNet
are much higher than those of Veit et al. (2016). This is because we use a much more conservative
estimate of this intractable quantity for both ResNet and vanilla networks.
Gradient reduction is achieved not just by identity skip connections but, as theorem 3 suggests, also
by skip connections that multiply the incoming value with a Gaussian random matrix. Results for
those skip connections can be found in table 1.
Veit et al. (2016) argues that deep ResNets behave like an ensemble of relatively shallow networks.
We argue that comparable vanilla networks often behave like ensembles of even shallower networks.
Jastrzebski et al. (2018) argues that deep ResNets are robust to lesioning. Additionally, we argue
that comparable vanilla networks are often even more robust to depth reduction when considering
the first order Taylor expansion.
14
Workshop track - ICLR 2018
7.1	The limits of dilution
k-dilution has its limits. Any k-diluted function with large k is close to a linear function. Hence, we
can view k-dilution as another form of pseudo-linearity that can damage representational capacity.
It also turns out that under similar conditions to those used in theorem 3, dilution only disappears
slowly as diluted functions are composed. If the diluting linear functions sb are the identity func-
tions, this corresponds to feature refinement as postulated by Jastrzebski et al. (2018).
Proposition 7. Under certain conditions, the composition of B random functions that are kb-diluted
in expectation respectively is ((Qι(1 + j1)) 一 1) 2 -diluted in expectation. (See section E.7 for
details.)
More simply, assume all the kb are equal to some k. Ignoring higher-order terms, the composition is
√Bk-diluted. The flipside of an O(k2) reduction in gradient via dilution is thus the requirement of
O(k2) layers to eliminate that dilution. This indicates that the overall amount of gradient reduction
achievable through dilution without incurring catastrophic pseudo-linearity is limited.
7.2	Choosing dilution levels
The power of our theory lies in exposing the GSC-reducing effect of skip connections for general
neural network architectures. As far as we know, all comparable previous works (e.g. Yang &
Schoenholz (2017); Balduzzi et al. (2017)) demonstrated similar effects only for specific architec-
tures. Our argument is not that certain ResNet’s achieve a certain level of GSC reduction, but that
ResNet users have the power to choose the level of GSC reduction by controlling the amount of
dilution. For example, while the level of dilution increases as we go deeper in the style of ResNet
architecture we used for experiments in this section, this need not be so.
The skip function s and block function ρ can be scaled with constants to achieve arbitrary, desired
levels of dilution (Szegedy et al., 2016). Alternatively, instead of putting all normalization layers
in the residual blocks, we can insert them between blocks / skip connections. This would keep the
dilution level constant and hence cause gradients to explode again, though at a lower rate compared
to vanilla networks.
7.3	The orthogonal initial state
Applying the residual trick to ResNet reveals several insights. The difference between ResNet and
vanilla networks in terms of skip connections is superficial, because both ResNet and vanilla net-
works are residual networks in the framework of equation 2. Also, both ResNet and vanilla networks
have nonlinear initial functions, because ρb(θb(0)) is initially nonzero and nonlinear. However, there
is one key difference. The initial functions of ResNet are closer to a linear transformation and indeed
closer to an orthogonal transformation because they are composed of a nonlinear function ρb(θb(0))
that is significantly diluted by an orthogonal transformation sb . Therefore, ResNet, while being
conceptually more complex, is mathematically simpler.
We have shown how ResNets achieve a reduced gradient via k-dilution. And just as with effective
depth, the residual trick allows us to generalize this notion to arbitrary networks.
Definition 5. We say a residual network f(θ) has an ‘orthogonal initial state’ if each initial func-
tion il is multiplication with an orthogonal matrix or a slice / multiple thereof and rl(θl) is the zero
function.
Any network that is trained from an (approximate) orthogonal initial state can benefit from reduced
gradients via dilution to the extent to which initial and residual function are uncorrelated. (See sec-
tion F.3 for more information.). ResNet is a style of architecture that achieves this, but it is far from
being the only one. Balduzzi et al. (2017) introduced the ‘looks-linear initialization’ (LLI) for ReLU
networks, which achieves not only an approximate orthogonal initial state, but outperformed ResNet
in their experiments. We detail this initialization scheme in section H. In table 2, we show that a
simple ReLU network with LLI can achieve an ever lower training error than ResNet on CIFAR10.
In figure 6C, we find that indeed LLI reduces the gradient growth of batch-ReLU drastically not just
15
Workshop track - ICLR 2018
in the initialized state, but throughout training even as the residual functions grow beyond the size
achieved under Gaussian initialization (compare figure 6E to 2E and 4E). DiracNet (Zagoruyko &
Komodakis, 2017) achieves an approximate orthogonal initial state in a very similar way to LLI. An
even simpler but much less powerful strategy is to initialize weight matrices as orthogonal matrices
instead of Gaussian matrices. This reduces the gradient growth in the initialized state somewhat
(table 1).
Using the ensemble view of very deep networks reveals another significant disadvantage of non-
orthogonal initial functions. The output computed by an ensemble member must pass through the
initial functions of the layers not contained in that ensemble member to reach the prediction layer.
Therefore, having non-orthogonal initial functions is akin to taking a shallow network and adding
additional, untrainable non-orthogonal layers to it. This has obvious downsides such as a collapsing
domain and / or exploding gradient, and an increasingly unfavorable eigenspectrum of the Jacobian
(Saxe et al., 2014). One would ordinarily not make the choice to insert such untrainable layers.
While there has been some success with convolutional networks where lower layers are not trained
(e.g. Saxe et al. (2011); He et al. (2016c)), it is not clear whether such networks are capable of out-
performing other networks where such layers are trained. While skip connections do not resolve the
tension between exploding gradients and collapsing domains, they reduce the pathology by avoiding
unnecessary non-orthogonality contained in the initial function.
The big question is now: What is the purpose of not training a network from an orthogonal initial
state? We are not aware of such a purpose. Since networks with orthogonal initial functions are
mathematically simpler than other networks, we argue they should be the default choice. Using
non-orthogonality in the initial function, we believe, is what requires explicit justification.
Balduzzi et al. (2017) asks in the title: If ResNet is the answer, then what is the question? We argue
that a better question would be: Is there a question to which vanilla networks are the answer?
8 Conclusion
Summary In this paper, we demonstrate that contrary to popular belief, many MLP architectures
composed of popular layer types exhibit exploding gradients, and those that do not exhibit collaps-
ing domains (section 3). This tradeoff is caused by the discrepancy between absolute determinants
and qm norms of layer-wise Jacobians (section 5). Both sides of this tradeoff cause pathologies.
Exploding gradients, when defined by the GSC (section 2) cause low effective depth (section 4).
Collapsing domains cause pseudo-linearity and can also cause low effective depth (section 6). How-
ever, both pathologies are caused to a surprisingly large degree by untrainable, and thus potentially
unnecessary non-orthogonality contained in the initial functions. Making the initial functions more
orthogonal via e.g. skip connections leads to improved outcomes (section 7).
Practical Recommendations
•	Train from an orthogonal initial state, i.e. initialize the network such that it is a series
of orthogonal linear transformations. This can greatly reduce the growth of the GSC and
domain collapse not just in the initial state, but also as training progresses. It can prevent
the forward activations from having to pass through unnecessary non-orthogonal transfor-
mations. Even if a perfectly orthogonal initial state is not achievable, an architecture that
approximates this such as ResNet can still confer significant benefit.
•	When not training from an orthogonal initial state, avoid low effective depth. A low effec-
tive depth signifies that the network is composed of an ensemble of networks significantly
shallower than the full network. If the initial functions are not orthogonal, the values com-
puted by these ensemble members have to pass through what may be unnecessary and
harmful non-orthogonal transformations. Low effective depth may be caused by, for exam-
ple, exploding gradients or a collapsing domain.
•	Avoid pseudo-linearity. For the representational capacity of a network to grow with depth,
linear layers must be separated by nonlinearities. If those nonlinearities can be approx-
imated by linear functions, they are ineffective. Pseudo-linearity can be caused by, for
example, a collapsing domain.
16
Workshop track - ICLR 2018
•	Keep in mind that skip connections help in general, but other techniques do not Di-
luting a nonlinear function with an uncorrelated linear function can greatly help with the
pathologies described above. Techniques such as normalization layers, careful initialization
of weights or SeLU nonlinearities can prevent the explosion or vanishing of forward acti-
vations. Adam, RMSprop or vSGD can improve performance even if forward activations
explode or vanish. While those are important functionalities, these techniques in general
neither help address gradient explosion relative to forward activations as indicated by the
GSC nor collapsing domains.
•	As the GSC grows, adjust the step size. If it turns out that some amount of growth of the
GSC is unavoidable or desirable, weights in lower layers could benefit from experiencing
a lower relative change during each update. Optimization algorithms such as RMSprop or
Adam may partially address this.
•	Control dilution level to control network properties. Skip connections, normalization
layers and scaling constants can be placed in a network to trade off gradient growth and
representational capacity. Theorem 3 can be used for a static estimate of the amount of
gradient reduction achieved. Similarly, proposition 7 can be used for a static estimate of
the overall dilution of the network.
•	Great compositional depth may not be optimal. Networks with more than 1000 layers
have recently been trained (He et al., 2016b). Haber & Ruthotto (2017) gave a formalism
for training arbitrarily deep networks. However, ever larger amounts of dilution are re-
quired to prevent gradient explosion (Szegedy et al., 2016). This may ultimately lead to an
effective depth much lower than the compositional depth and individual layers that have a
very small impact on learning outcomes, because functions they represent are very close to
linear functions. If there is a fixed parameter budget, it may be better spent on width than
extreme depth (Zaguroyko & Komodakis, 2016).
Implications for deep learning research
•	Exploding gradients matter. They are not just a numerical quirk to be overcome by
rescaling but are indicative of an inherently difficult optimization problem that cannot be
solved by a simple modification to a stock algorithm.
•	Use GSC as a benchmark for gradient explosion. For the first time, we established a
rigorous link between a metric for exploding gradients and hardness of training. The GSC
is also robust to network rescaling, layer width and individual layers.
•	Any neural network is a residual network. The residual trick allows the application of
ResNet-specific tools such as the popular theory of effective depth to arbitrary networks.
•	Step size matters when studying the behavior of networks. We found that using differ-
ent step sizes for different layers had a profound impact on the training success of various
architectures. Many studies that investigate fundamental properties of deep networks ei-
ther do not consider layerwise step sizes (e.g. Schoenholz et al. (2017)) or do not even
consider different global step sizes (e.g. Keskar et al. (2017)). This can lead to inaccurate
conclusions.
We provide continued discussion in section B.
17
Workshop track - ICLR 2018
Nonlinearity	Normalization	Matrix type	Skip type	Width	GSC(L, 0)	St. Dev.	Sign Div.
ReLU	none	Gaussian	none	100	1.52	0.22	0.030
ReLU	layer	Gaussian	none	100	1.16	0.096	0.029
ReLU	LOlayer	Gaussian	none	100	1.30	0.10	0.030
ReLU	batch	Gaussian	none	100	5728	1.00	0.41
tanh	none	Gaussian	none	100	1.26	0.096	0.50
tanh	layer	Gaussian	none	100	72.2	1.00	0.50
tanh	LOlayer	Gaussian	none	100	71.9	1.00	0.50
tanh	batch	Gaussian	none	100	93.6	1.00	0.50
SeLU	none	Gaussian	none	100	6.36	0.97	0.42
SeLU	LOlayer	Gaussian	none	100	7.00	0.98	0.42
ReLU	batch	Gaussian	none	200	5556	1.00	0.42
ReLU	batch	Gaussian	none	100/200	5527	1.00	0.41
SeLU	none	Gaussian	none	200	5.86	0.99	0.45
SeLU	none	Gaussian	none	100/200	6.09	0.98	0.43
ReLU	none	orthogonal	none	100	1.29	0.20	0.03
ReLU	layer	orthogonal	none	100	1.00	0.10	0.03
ReLU	batch	orthogonal	none	100	5014	1.00	0.42
tanh	none	orthogonal	none	100	1.18	0.10	0.50
tanh	layer	orthogonal	none	100	56.3	1.00	0.50
tanh	batch	orthogonal	none	100	54.6	1.00	0.50
SeLU	none	orthogonal	none	100	5.47	1.00	0.49
ReLU	none	looks-linear	none	100	1.00	1.00	0.50
ReLU	layer	looks-linear	none	100	1.00	1.00	0.50
ReLU	batch	looks-linear	none	100	1.00	1.00	0.50
ReLU	layer	Gaussian	identity	100	1.08	0.56	0.19
ReLU	batch	Gaussian	identity	100	4.00	1.00	0.48
tanh	layer	Gaussian	identity	100	1.63	1.00	0.50
tanh	batch	Gaussian	identity	100	1.57	1.00	0.50
SeLU	layer	Gaussian	identity	100	1.31	0.99	0.48
ReLU	layer	Gaussian	Gaussian	100	1.17	0.56	0.18
ReLU	batch	Gaussian	Gaussian	100	4.50	1.00	0.48
tanh	layer	Gaussian	Gaussian	100	1.97	1.00	0.50
tanh	batch	Gaussian	Gaussian	100	1.71	1.00	0.50
SeLU	layer	Gaussian	Gaussian	100	1.53	9.97	0.48
Table 1: Key metrics for architectures in their randomly initialized state evaluated on Gaussian
noise. In the ‘Normalization’ column, ‘layer’ refers to layer normalization, ‘batch’ refers to batch
normalization, ‘LOlayer’ refers to length-only layer normalization and ‘none’ refers to an absence
of a normalization layer. In the ‘Matrix type’ column, ‘Gaussian’ refers to matrices where each
entry is drawn from an independent Gaussian distribution with mean zero and a standard deviation
that is constant across all entries. ‘orthogonal’ refers to a uniformly random orthogonal matrix and
‘looks-linear’ refers to the initialization scheme proposed by Balduzzi et al. (2017) and expounded
in section H. In the ‘Skip type’ column, ‘identity’ refers to identity skip connections and ‘Gaussian’
refers to skip connections that multiply the incoming value with a matrix where each entry is drawn
from an independent Gaussian distribution with mean zero and a standard deviation that is constant
across all entries. ‘none’ refers to an absence of skip connections. In the ‘Width’ column, ‘100/200’
refers to linear layers having widths alternating between 100 and 200. ‘St. Dev.’ refers to pre-
activation standard deviation. ‘Sign Div.’ refers to pre-activation sign diversity. For details and
definitions, see section I. Red values indicate gradient explosion or pseudo-linearity.
18
Workshop track - ICLR 2018
Nonlinearity	Normalization	Matrix type	Skip type	Error (custom step size)	Error (single step size)
ReLU	none	Gaussian	none	31.48%	19.24%
ReLU	layer	Gaussian	none	42.48%	21.23%
ReLU	batch	Gaussian	none	34.83%	76.65%
tanh	none	Gaussian	none	23.42%	16.22%
tanh	layer	Gaussian	none	1.92%	17.5%
tanh	batch	Gaussian	none	12.31%	23.8%
SeLU	none	Gaussian	none	0.24%	1.78%
ReLU	none	looks-linear	none	0.002%	0.008%
ReLU	layer	looks-linear	none	0.77%	1.2%
ReLU	batch	looks-linear	none	0.38%	0.19%
tanh	layer	Gaussian	id	0.35%	0.27%
tanh	batch	Gaussian	id	0.13%	0.24%
ReLU	layer	Gaussian	id	2.09%	1.49%
ReLU	batch	Gaussian	id	0.06%	0.096%
SeLU	layer	Gaussian	id	1.55%	1.55%
Table 2: Training classificaion error for architectures trained on CIFAR10. In the ‘Normalization’
column, ‘layer’ refers to layer normalization, ‘batch’ refers to batch normalization and ‘none’ refers
to an absence of a normalization layer. In the ‘Matrix type’ column, ‘Gaussian’ refers to matrices
where each entry is drawn from an independent Gaussian distribution with mean zero and a standard
deviation that is constant across all entries. ‘looks-linear’ refers to the looks-linear initialization
scheme proposed by Balduzzi et al. (2017) and expounded in section H. In the ‘Skip type’ column,
‘identity’ refers to identity skip connections and ‘none’ refers to an absence of skip connections. In
the two rightmost columns, we show the training classification error achieved when using a single
step size and when using a custom step size for each layer. Whichever error value is lower is shown
in bold. For further methodological details, see section I. For a detailed breakdown of these results,
see figures 2, 4, 5 and 6.
19
Workshop track - ICLR 2018
References
Anonymous. Multi-level residual networks from dynamical systems view. In ICLR (under review),
2018a. URL https://openreview.net/forum?id=SyJS-OgR-.
Anonymous. Orthogonal recurrent neural networks with scaled cayley transform. In ICLR (under
review), 2018b. URL https://openreview.net/forum?id=HyEi7bWR-.
Anonymous. Training and inference with integers in deep neural networks. In ICLR (under review),
2018c. URL https://openreview.net/forum?id=HJGXzmspb.
Anonymous. Deep mean field theory: Variance and width variation by layer as methods to control
gradient explosion. In ICLR (under review), 2018d. URL https://openreview.net/
forum?id=rJGY8GbR-.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
ICML, 2016. URL https://arxiv.org/abs/1511.06464.
Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, and Venu Govindaraju. Normalization propagation:
A parametric technique for removing internal covariate shift in deep networks. In ICML, 2016.
URL https://arxiv.org/abs/1603.01431.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. 2016. URL
https://arxiv.org/abs/1607.06450.
David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.
The shattered gradients problem: If resnets are the answer, then what is the question? In ICML,
2017. URL https://arxiv.org/abs/1702.08591.
Yoshua Bengio, P Simard, and Frasconi P. Learning long-term dependencies with gradient descent
is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994. URL http://ai.
dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf.
Monica Bianchini and Franco Scarselli. On the complexity of shallow and deep neural network
classifiers. In ESANN, 2014. URL https://www.elen.ucl.ac.be/Proceedings/
esann/esannpdf/es2014- 44.pdf.
Lili Chang, Bo nad Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham.
Reversible architectures for arbitrarily deep residual neural networks. arXiv preprint
arXiv:1709.03698, 2017. URL https://arxiv.org/abs/1709.03698.
KyUnghyUn Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259,
2014. URL https://arxiv.org/abs/1409.1259.
Luo Chunjie, Zhan Jianfeng, Wang Lei, and Yang Qiang. Cosine normalization: Using cosine
similarity instead of dot product in neural networks. arXiv preprint arXiv:1702.05870, 2017.
URL https://arxiv.org/abs/1702.05870.
Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product net-
works.	In NIPS,	2011.	URL http://papers.nips.cc/paper/
4350- shallow- vs- deep- sum- product- networks.pdf.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In ICCV, 2015. URL http://proceedings.mlr.press/v9/glorot10a/
glorot10a.pdf.
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions.
In ICML, 2016. URL https://arxiv.org/abs/1603.00391.
Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. arXiv preprint
arXiv:1705.03341, 2017. URL https://arxiv.org/abs/1705.03341.
20
Workshop track - ICLR 2018
Eldad Haber, Lars Ruthotto, and Elliot Holtham. Learning across scales - multiscale methods
for convolution neural networks. arXiv preprint arXiv:1703.02009, 2017. URL https://
xtract.ai/wp-content/uploads/2017/05/Learning-Across-Scales.pdf.
Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. In CVPR, 2017.
URL https://arxiv.org/abs/1610.02915.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In ICCV, 2015. URL https://arxiv.
org/abs/1502.01852.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual net-
works. arXiv preprint arXiv:1603.05027, 2016a. URL https://arxiv.org/abs/1603.
05027.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016b. URL https://arxiv.org/abs/1512.03385.
Kun He, Yan Wang, and John Hopcroft. A powerful generative model using random weights for
the deep image representation. In NIPS, 2016c. URL https://arxiv.org/abs/1606.
04801.
Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Masters thesis, Technische
Universitat Munchen, 04 1991.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780,1997.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
stochastic dePth. In ECCV, 2016. URL https://arxiv.org/abs/1603.09382.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, 2017. URL https://arxiv.org/abs/1608.06993.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dePendence in batch-normalized
models. In NIPS, 2017. URL https://arxiv.org/abs/1702.03275.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deeP network training by
reducing internal covariate shift. In ICML, 2015. URL https://arxiv.org/abs/1502.
03167.
Stanislaw Jastrzebski, Devansh ArPit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Ben-
gio. Residual connections encourage iterative inference. In CVPR (under review), 2018. URL
https://arxiv.org/abs/1710.04773.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyankiy, and Ping Tak Peter
Tang. On large-batch training for deeP learning - generalization gaP for sharP minima. In ICLR,
2017. URL https://arxiv.org/abs/1609.04836.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic oPtimization. In ICLR,
2015. URL https://arxiv.org/abs/1412.6980.
GUnther Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In NIPS, 2017. URL https://arxiv.org/abs/1706.02515.
Ping Luo. Learning deep architectures via generalized whitened neural networks. In ICML, 2017.
URL http://proceedings.mlr.press/v70/luo17a/luo17a.pdf.
James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational ef-
ficiency of restricted boltzmann machines. In NIPS, 2013. URL http://www.cs.toronto.
edu/〜jmartens/docs/RBM_%20Representational_%20Efficiency.pdf.
Hrushikesh N. Mhaskar and Ohad Shamir. Deep vs. shallow networks: An approximation theory
perspective. Analysis and Applications, 14:829-848, 2016. URL https://arxiv.org/
abs/1608.03287.
21
Workshop track - ICLR 2018
Dmytro Mishking and Jiri Matas. All you need is a good init. In ICLR, 2016. URL https:
//arxiv.org/abs/1511.06422.
Guido F. Montafur, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In NIPS, 2014. URL https://arxiv.org/abs/1402.
1869.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In ICML, 2013. URL https://arxiv.org/abs/1211.5063.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Expo-
nential expressivity in deep neural networks through transient chaos. In NIPS, 2016. URL
https://arxiv.org/abs/1606.05340v1.
Maithra Raghu, Ben Poole, Surya Ganguli, Jon Kleinberg, and Jascha Sohl-Dickstein. On the ex-
pressive power of deep neural networks. In ICML, 2017. URL https://arxiv.org/abs/
1606.05336.
Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparametrization to ac-
celerate training of deep neural networks. In NIPS, 2016. URL https://arxiv.org/abs/
1602.07868.
Andrew M Saxe, Pang Wei Koh, Chenghao Zhen, Maneesh Bhand, Bipin Suresh, and Andrew Y.
Ng. On random weights and unsupervised feature learning. In ICML, 2011. URL http://
www.icml-2011.org/papers/551_icmlpaper.pdf.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2014. URL
https://arxiv.org/abs/1312.6120.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In ICML, 2013. URL
https://arxiv.org/abs/1206.1106.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. In ICLR, 2017. URL https://arxiv.org/abs/1611.01232.
Ohad Shamir and Ronen Eldan. The power of depth for feedforward neural networks. arXiv preprint
arXiv:1512.03965, 2015. URL https://arxiv.org/abs/1512.03965.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15:1929-1958, 2014.
URL https://www.cs.toronto.edu/~hinton∕absps∕JMLRdropout.pdf.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex A. Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In ICLR Workshop, 2016. URL
https://arxiv.org/abs/1602.07261.
Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint
arXiv:1509.08101, 2015. URL https://arxiv.org/abs/1509.08101.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5 - rmsprop, coursera: Neural networks for ma-
chine learning. 2012.
Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles of rel-
atively shallow networks. In NIPS, 2016. URL https://arxiv.org/abs/1605.06431.
Greg Yang and Samuel S. Schoenholz. Mean field residual networks: On the
edge of chaos. In NIPS, 2017. URL https://papers.nips.cc/paper/
6879- mean- field- residual- networks- on- the- edge- of- chaos.
Sergey Zagoruyko and Nikos Komodakis. Diracnets: Training very deep neural networks without
skip-connections. arXiv preprint arXiv:1706.00388, 2017. URL https://arxiv.org/abs/
1706.00388.
Sergey Zaguroyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016. URL https:
//arxiv.org/abs/1605.07146.
22
∙sossoJ°βOJd
0β∙su-i3SE jəAEl-pbəuHUOU ls-∙-q əip JO (UO≡uyωp JOJ∙I u-lɔəs əəs)-"sjəaip u∙-S u∙2aMɔE
—əjd PuB u∙l∙Aωp piBPUBlS u∙a∙sE2d SMoqS MOJ UIssq əm Z 号-£ SqdEJ21U9EA-bω
əjb SMOJ OAM doləm ∙(zIqS) səs dIS əʌwəAEl qMuUBqIjojjə jəmpəAəɔE Wql
SE də-s əes E qMuUlay pəuwo S=nsəj MoqS SqdEJuUIx ∙I uSpqosωp
UnPyoM əPJ80E jəAqoE5 £ əsEpdn əməjuφdo pə-euipsə ətp SMoqS qdEJ
IJ9 dOqI OlxVHI。UOPəuu EEUIOPudE=oo qM səjmɔəlɔɪB JoJ sɔpjəuɪ Aəɔj 寻 əjn工
ɪo
寸
i
g
co
S
g
N
W
g
UOIaonPaIlμdφp IEUON®OduIOO
S
O

o
S
∞	CO	07
OOOO
GlIoI
9	（0 '7）QSD
巴 joij© uoτ^oμτssB∣o Surarejjj
SIn∩sωx q<INωNIXωdxωU<ZOPSQ< <
8io<nUUoT Jpet3dOqS 电Om
Workshop track - ICLR 2018
210
eee
111
(C )0,L(CSG
186420
..............................
0000
(B rorre noitacfiissalc gniniarT
200	300	400
Epoch
100
^^1
A dnuob htiw ezis etadpu evitaleR
10	20	30	40
Linear layer
0
50
0
40
00
30
0
20
00
1
0
50
50
18642
..............................
0000
(F) rorre noitacfiissalc gniniarT
0 5 10 15 20 25 30 35 40 45
Compositional depth reduction
01
1e e-
1
mron rotarepO
0	10	20	30	40	50
20
htped evitceffE
0	100	200	300	400	500
Linear layer
(H ytisreviD ngiS noitavitcA-erP
Epoch
0	100	200	300	400	500
Epoch
Figure 5: Key metrics for ResNet architectures trained on CIFAR10. The top left graph shows the
estimated optimal relative update size in each layer according to the algorithm described in section
I.3. Remaining graphs show results obtained from training with either those step sizes or a single
step size, whichever achieved a lower error (see table 2). The top two rows are equivalent to graphs
in figure 2. The bottom row shows pre-activation standard deviation and pre-activation sign diversity
(see section I.2 for definition) of the highest nonlinearity layer as training progresses.
24
Workshop track - ICLR 2018
2
e
1
50
htped evitceffE
30
1 01234
ee - - - -
11 e e e e
) 1111
A dnuob htiw ezis etadpu evitaleR
10	20	30	40
Linear layer
2 54321 0
.............................................
e- 0 0 0 0 0
1)
H ytisreviD ngiS noitavitcA-erP
rorre noitacfiissalc gniniarT
e0
1
)0,L(CSG
50
10	20	30	40	50
Linear layer
100	200	300	400	500
Epoch
200	300	400	500
Epoch
100
0
0 5 10 15 20 25 30 35 40 45
Compositional depth reduction
Figure 6: Key metrics for ReLU-based architectures with looks-linear initialization trained on
CIFAR10. The top left graph shows the estimated optimal relative update size in each layer ac-
cording to the algorithm described in section I.3. Remaining graphs show results obtained from
training with either those step sizes or a single step size, whichever achieved a lower error (see table
2). The top two rows are equivalent to graphs in figure 2. The bottom row shows pre-activation
standard deviation and pre-activation sign diversity (see section I.2 for definition) of the highest
nonlinearity layer as training progresses.
25
Workshop track - ICLR 2018
B Discussion
B.1	Related work
So far, we have discussed exploding gradients and collapsing domains. In this section, we review
related metrics and concepts from literature.
We build on the work of Balduzzi et al. (2017), who introduced the concept of gradient shattering.
This states that in deep networks, gradients with respect to nearby points become more and more
uncorrelated with depth. This is very similar to saying that the gradient is only informative in
a smaller and smaller region around the point at which it is taken. This is precisely what happens
when gradients explode and also, as we argue in section 6, when the domain collapses. Therefore, the
exploding gradient problem and collapsing domain problem can be viewed as a further specification
of the shattering gradient problem rather than as a counter-theory or independent phenomenon.
We extend the work of Balduzzi et al. (2017) in several important ways. First, they claim that the
exploding gradient problem “has been largely overcome”. We show that this is not true, especially
in the context of very deep batch-ReLU MLPs, which are central to their paper. Second, by using ef-
fective depth we make a rigorous argument as to why exploding gradients cause hardness of training.
While Balduzzi et al. (2017) point out that shattering gradients interfere with theoretical guarantees
that exist for various optimization algorithms, they do not provide a definitive argument as to why
shattering gradients are in fact a problem. Third, our analysis extends beyond ReLU networks.
We also build on the work of Raghu et al. (2017). They showed that both trajectories and small
perturbations, when propagated forward, can increase exponentially in size. However, they do not
distinguish too important cases: (i) an explosion that is simply due to an increase in the scale of
forward activations and (ii) an explosion that is due to an increase in the gradient relative to forward
activations. We are careful to make this distinction and focus only on case (ii). Since this is arguably
the more interesting case, we believe the insights generated in our paper are more robust.
Saxe et al. (2014) investigated another important pathology of very deep networks: the divergence
of singular values in multi-layer Jacobians. As layer-wise Jacobians are multiplied, the variances of
their singular values compound. This leads to the direction of the gradient being determined by the
dominant eigenvectors of the multi-layer Jacobian rather than the label, which slows down training
considerably.
In their seminal paper, Ioffe & Szegedy (2015) motivated batch normalization with the argument
that changes to the distribution of intermediate representations, which they term ‘covariate shift’,
are pathological and need to be combated. This argument was then picked up by e.g. Salimans &
Kingma (2016) and Chunjie et al. (2017) to motivate similar normalization schemes. We are not
aware of any rigorous definition of the ‘covariate shift’ concept nor do we understand why it is
undesirable. After all, isn’t the very point of training deep networks to have each layer change the
function it computes, to which other layers co-adapt, to which then other layers co-adapt and so on?
Having each layer fine-tune its weights in response to shifts in other layers seems to be the very
mechanism by which deep networks achieve high accuracy.
A classical notion of trainability in optimization theory is the conditioning of the Hessian. This
can also deteriorate with depth. Recently, Luo (2017) introduced an architecture that combats this
pathology in an effective and computationally tractable way via iterative numerical methods and
matrix decomposition. Matrix decomposition has also been used by e.g. Arjovsky et al. (2016);
Anonymous (2018b) to maintain orthogonality of recurrent weight matrices. Maybe such techniques
could also be used to reduce the divergence of singular values of the layer-wise Jacobian during
training.
B.1.1	Mean field theory - exploding gradients / collapsing domain vs order /
CHAOS
Our work bears similarity to a recent line of research studying deep networks using mean field theory
(Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Anonymous, 2018d). Those
papers use infinitely wide networks to statically analyze the expected behavior of forward activations
and gradients in the initialized state. They identify two distinct regimes, order and chaos, based on
whether an infinitesimal perturbation shrinks or grows in expectation respectively as it is propagated
26
Workshop track - ICLR 2018
forward. This corresponds to the expected qm norm of the layer-wise Jacobian being smaller or
larger than 1 respectively. They show that in the chaotic regime, gradient vector length explodes
whereas in the ordered regime, gradient vector length vanishes. Further, they show that for tanh
MLPs the correlation between forward activation vectors corresponding to two different data inputs
converges to 1 (‘unit limit correlation’) in the ordered regime as activations are propagated forward
and to some value less than 1 in the chaotic regime. Specifically, in a tanh MLP without biases, in
the chaotic regime, the correlation converges to 0 (‘zero limit correlation’). They show how to use
mean field theory as a powerful tool for the static analysis of individual network architectures.
As in mean field theory, some of our analysis relies on the expected behavior of networks in their
randomly initialized state (theorem 2). Further, it is clear that the order / chaos dichotomy bears
similarity to the exploding gradient problem / collapsing domain problem dichotomy as presented
in this paper. However, there are also important differences.
One difference is that we argue that the GSC is a better metric for determining the presence of
pathological exploding or vanishing gradients than gradient vector length and thus more meaningful
than order / chaos. Using the GSC, we obtain very different regions of explosion, vanishing and
stability for popular architectures compared to gradient vector length. For a tanh MLP with no
biases, using gradient vector length, vanishing is achieved for σw < 1, stability for σw = 1 and
explosion for σw > 1. (σw denotes the standard deviation of weight matrix entries times the square
root of the right dimsion of the weight matrix, as defined in Poole et al. (2016).) For a tanh MLP with
no biases, using the GSC, vanishing is impossible, stability is achieved for σw ≤ 1 and explosion
for σw > 1. For a ReLU MLP with no biases, using gradient vector length, vanishing is achieved
for σw < √2, stability for σw = √2 and explosion for σw > √2. For a ReLU MLP with no biases,
using the GSC, stability is inevitable.
The advantage of considering GSC can be seen in the case of the ReLU network. For tanh, Schoen-
holz et al. (2017) showed that order corresponds to an exponential convergence towards a unit limit
correlation and chaos corresponds to an exponential convergence towards a zero limit correlation.
For a ReLU MLP with no biases and。加 > √2, infinitesimal noise grows (chaos), yet the correlation
converges sub-exponentially to zero, which is a behavior we would expect from the edge of chaos.
As we saw above, using the GSC to define order / chaos that is precisely where we are: the edge of
chaos.
A second difference is that the concepts of unit limit correlation and the collapsing domain problem
are not the same. In fact, the former can be seen as a special case of the latter. In a tanh MLP with no
bias and σw slightly larger than 1, correlation converges to 0 and eventually, gradients explode. Yet
the domain can still collapse dramatically in the short term as shown in figure 1B to cause pseudo-
linearity. In a tanh MLP with no bias and σw very large, again, correlation converges to 0 and
gradients explode. However, the tanh layer maps all points close to the corners of the hypercube,
which corresponds to domain collapse.
We do not use the assumption of infinite width in our analysis. The only possible exception is that
the SSD assumption in proposition 7 can be viewed as implying infinite width.
While Schoenholz et al. (2017) conjectures that the edge of chaos is necessary for training very
deep networks, our paper provides somewhat contrary evidence. Our two best performing vanilla
architectures, SeLU and layer-tanh, are both inside the chaotic regime whereas ReLU, layer-ReLU
and tanh, which are all on the edge of chaos, exhibit a higher training classification error. Clearly,
chaotic architectures avoid pseudo-linearity. The difference between our experiments and those in
Schoenholz et al. (2017) is that we allowed the step size to vary between layers. This had a large
impact, as can be seen in table 2. We believe that our results underscore the importance of choosing
appropriate step sizes when comparing the behavior of different neural architectures or training
algorithms in general.
In section 4, we present a rigorous argument for the harmful nature of exploding gradients, and
thus of chaos as defined by the GSC, at high depth. No comparable argument exists in mean field
literature.
While Schoenholz et al. (2017) obtained low accuracies for networks exhibiting unit limit correla-
tion, it is not clear a priori that this effect is harmful for accuracy. After all, correlation information
is a rather small part of the information present in the data, so the remaining information might be
27
Workshop track - ICLR 2018
sufficient for learning. As a simple example, consider k-means. Performing k-meafns on an arbi-
trary dataset yields the same result as first adding a large constant to the data and then performing
k-means, even though the addition can easily destroy correlation information. In contrast, in section
6 , we show how collapsing domains can directly harm expressivity and trainability.
Yang & Schoenholz (2017) shows that pathologies such as gradient explosion that arise in vanilla
networks are reduced in specific ResNet architectures. We extend this finding to general ResNet
architectures.
Anonymous (2018d) proposes to combat gradient growth by downscaling the weights in the residual
block of a ResNet. This corresponds to increased dilution, which indeed reduces gradient growth
as shown in section 7. However, we also show in proposition 7 that the reduction achievable in
this way may without suffering catastrophic pseudo-linearity be limited. Anonymous (2018d) also
proposes to combat the exploding gradient problem by changing the width of intermediate layers.
Our analysis in section 4.4 suggests that this is not effective in reducing the growth of the GSC.
Anonymous (2018d) concludes that changing the width combats the exploding gradient problem
because they implicitly assume that the pathology of exploding gradients is determined by the scale
of individual components of the gradient vector rather than the length of the entire vector or the GSC.
They do not justify this assumption. We propose the GSC as a standard for assessing pathological
exploding gradients to avoid such ambiguity.
B.1.2	ResNet from a dynamical systems view
Recently, Haber et al. (2017); Haber & Ruthotto (2017); Chang et al. (2017); Anonymous (2018a)
proposed ResNet architectures inspired by dynamical systems and numerical methods for ordinary
differential equations. The central claim is that these architectures are stable at arbitrary depth,
i.e. both forward activations and gradients (and hence GSC) are bounded as depth goes to infinity.
They propose four practical strategies for building and training ResNets: (a) ensuring that residual
and skip functions compute vectors orthogonal to each other by using e.g. skew-symmetric weight
matrices (b) ensuring that the Jacobian of the skip function has eigenvalues with negative real part
by using e.g. weight matrices factorized as -CTC (c) scaling each residual function by 1/B where
B is the number of residual blocks in the network and (d) regularizing weights in successive blocks
to be similar via a fusion penalty.
Architecture	GSC(L, 0) (base 10log)	GSC(L, 0) dilution-corrected (base 10 log)
batch-ReLU (i)	0337	4.23
batch-ReLU (ii)	0.329	4.06
batch-ReLU (iii)	6.164	68.37
batch-ReLU (iv)	0.313	7.22
layer-tanh (i)	0.136	2.17
layer-tanh (ii)	0.114	1.91
layer-tanh (iii)	3.325	5.46
layer-tanh (iv)	0.143	2.31
Table 3: Key metrics for architectures derived from dynamical systems theory.
We evaluated those strategies empirically. In table 3, we show the value of the GSC across the
network for 8 different architectures in their initialized state applied to Gaussian noise. (See section
B.1.2 for details.) All architectures use residual blocks containing a single normalization layer, a
single nonlinearity layer and a single linear layer. We initialize the linear layer in four different ways:
(i) Gaussian initialization, (ii) skew-symmetric initialization, (iii) initialization as -CT C where C
is Gaussian initialized and (iv) Gaussian initialization where weight matrices in successive blocks
have correlation 0.5. Initializations (ii), (iii) and (iv) mimic strategies (a), (b) and (d) respectively.
To enable the comparison of the four initialization styles, we normalize each weight matrix to have
a unit qm norm. We study all four initializations for both batch-ReLU and layer-tanh.
Initialization (ii) improves slightly over initialization (i). This is expected given theorem 3. One of
the key assumptions is that skip and residual function be orthogonal in expectation. While initial-
ization (i) achieves this, under (ii), the two functions are orthogonal not just in expectation, but with
probability 1.
28
Workshop track - ICLR 2018
Initialization (iii) has gradients that grow much faster than initialization (i). On the one hand, this
is surprising as Haber & Ruthotto (2017) states that eigenvalues with negative real parts in the
residual Jacobian supposedly slow gradient growth. On the other hand, it is not surprising because
introducing correlation between the residual and skip path breaks the conditions of theorem 3.
Initialization (iv) performs comparably to initialization (i) in reducing gradient growth, but requires
a larger amount of dilution to achieve this result. Again, introducing correlation between successive
blocks and thus between skip and residual function breaks the conditions of theorem 3 and weakens
the power of dilution.
While we did not investigate the exact architectures proposed in Haber & Ruthotto (2017); Chang
et al. (2017), our results show that more theoretical and empirical evaluation is necessary to deter-
mine whether architectures based on (a), (b) and (d) are indeed capable of increasing stability. Of
course, those architectures might still confer benefits in terms of e.g. inductive bias or regularization.
Finally, strategy (c), the scaling of either residual and/or skip function with constants is a technique
already widely used in regular ResNets. In fact, our study suggests that in order to bound the GSC
at arbitrary depth in a regular ResNet, it is sufficient to downscale each residual function by only
√= instead of B as papers in this line of work suggest.
B.2	Vanishing gradients in feedforward networks
We have not experienced vanishing gradients as defined by the GSC in our experiments. Our analysis
suggests that strong domain collapse is necessary to not only overcome the gradient growth implied
by theorem 2, but reverse it. We conjecture that such domain collapse could actually occur in e.g.
ReLU and tanh architecture if a non-zero additive bias was introduced, though this goes beyond the
scope of this paper.
B.3	Exploding and vanishing gradients in RNNs
Exploding gradients and their counterpart, vanishing gradients, have been studied more extensively
in the context of on RNNs (e.g. Pascanu et al. (2013); Bengio et al. (1994)). It is important to
note that the problem as it arises in RNNs is similar but also different from the exploding gradient
problem in feedforward networks. The goal in RNNs is often to absorb information early on and
store that information through many time steps and sometimes indefinitely. In the classical RNN
architecture, signals acquired early would be subjected to a non-orthogonal transformation at every
time step which leads to all the negative consequences described in this paper. LSTMs (Hochreiter
& Schmidhuber, 1997) and GRUs (Cho et al., 2014), which are the most popular solutions to ex-
ploding / vanishing gradients in RNNs, are capable of simply leaving each neuron that is considered
part of the latent state completely unmodified from time step to time step unless new information is
received that is pertinent to that specific neuron. This solution does not apply in feedforward net-
works, because it is the very goal of each layer to modify the signal productively. Hence, managing
exploding gradients in feedforward networks is arguably more difficult.
Nevertheless, there is similarity between LSTM and the orthogonal initial state because both elim-
inate non-orthogonality “as much as possible”. LSTM can eliminate non-orthogonality completely
from time step to time step whereas in the orthogonal initial state, non-orthogonality is eliminated
only from the initial function. Again, viewing feedforward networks as ensembles of shallower net-
works, orthogonal initial functions ensure that information extracted from each ensemble member
does not have to pass through non-orthogonal transformations needlessly. This is precisely what
LSTM attempts to achieve.
B.4	Open research questions and future work
Biases, convolutional and recurrent layers In this paper, we focus our analysis on MLPs without
trainable bias and variance parameters. Theorem 1, in its formulation, applies only to such MLPs.
Theorems 2, 3 and proposition 7 use assumptions that are potentially harder to achieve in non-MLP
architectures. Our experimental evaluation is limited to MLPs.
We think that results very similar to those presented in this paper are acheivable for other types of
neural networks, such as those containing trainable biases, convolutional layers or recurrent layers.
29
Workshop track - ICLR 2018
The fundamental behavior of those architectures should be the same, though additional nuance and
heavier mathematical bookkeeping might come into play.
Analysis of deep gradients has so far focused on MLPs (e.g. Balduzzi et al. (2017); Schoenholz et al.
(2017); Yang & Schoenholz (2017); Saxe et al. (2014)), so a principled extension of these results to
other network types would break new and important ground.
Understanding collapsing domains It is difficult to assess or measure the degree to which the
domain collapses in a given network. There is no single correct metric to measure this effect and
depending on the metric chosen, the set of networks exhibiting collapsing domains may look very
different. So far, we discussed pre-activation standard deviation (section 6), pre-activation sign
diversity (section 6) and activation correlation (section B.1.1). The volume of the domain and the
entropy of the distribution of activations may also be of interest.
Not all domains collapse in the same way. In the tanh architecture we studied in this paper, the
domain collapses onto the origin. In linear MLPs, the domain collapses onto the line through the
dominant eigenvector of the product of weight matrices, but never collapses onto a single point. In
ReLU, the domain collapses onto a ray from the origin. In tanh with very large weights, activation
vectors are mapped approximately onto the corners of the hypercube by the tanh layer.
What gradient scale is best? GSC(1, L) indicates the relative responsiveness of the prediction
layer with respect to changes in the input layer. Of course, the goal in deep learning, at least within a
prediction framework, is to model some ground truth function t that maps data inputs to true labels.
That function has itself a GSC at each input location x that measures the relative responsiveness of
t(x) to changes in x. If the network is to perfectly represent the ground truth function, the GSCs
would also have to match up. If, on the other hand, the GSC of the network differs significantly from
that of t, the network is not fitting t well. This suggests that in fact, the “best” value of the GSC is
one that matches that of the ground truth. If the GSC of the network is too low, we may experience
underfitting. If the GSC of the network is too high, we may experience overfitting.
How to achieve the “right” gradient? To model the ground truth function, we may not just want
to consider the overall magnitude of the GSC across the dataset, but to enable the network to have
gradients of different magnitudes from one data input to the next; or to learn highly structured gra-
dients. For example, given an image of a dog standing in a meadow, we might desire a high gradient
with respect to pixels signifying e.g. facial features of the dog but a low gradient with respect to
pixels that make up the meadow, and a uniformly low gradient given an image of a meadow. Such
gradients would be very valuable not just in modelling real world functions more accurately and im-
proving generalization, but in making the output of neural networks more explainable and avoiding
susceptibility to attacks with adversarial inputs.
Understanding representational capacity and pseudo-linearity In section 7, we explained how
dilution reduces gradient growth but may harm representation capacity. While removing untrainable
non-orthogonality from the initial functions may not be harmful, to achieve large levels of dilution
and thus large amounts of GSC reduction, many ResNet architectures also suppress the size of the
residual function relative to the initial function. This may happen naturally when the size of the skip
path grows via repeated addition as we go deeper, or by deliberately scaling down residual blocks
(Szegedy et al., 2016). Clearly, ifa neural network can only represent functions that are very close to
linear functions, it may not be able to model the ground truth function well. However, there exist no
mechanisms to determine how much dilution is harmful for a given ground truth function or dataset.
Dilution is not only present in ResNet and special constructs such as looks-linear initialized ReLU
networks, but even in vanilla, Gaussian initialized MLPs. For example, a SeLU nonlinearity can be
more easily approximated by a linear function in terms of mean square error over a unit Gaussian
input than a ReLU nonlinearity. We suspect that this is related to gradients in a SeLU MLP exploding
more slowly than in a batch-ReLU MLP. Assessing the total amount of “linearity” present in a
network is an open question. Therefore, we also cannot make blanket statements such as “SeLU is
superior to batch-ReLU because gradients explode more slowly”, because a batch-ReLU MLP with
fewer layers might in some sense have as much representational power as a SeLU MLP with more
layers.
30
Workshop track - ICLR 2018
Finally, the impact of dilution on the representational power conferred by depth is an open question.
How far does the orthogonal initial state take us? An orthogonal initial state reduces gradients
via dilution, which allows for relatively larger updates, which enables increased growth of residual
functions, which allows for greater effective depth. However, as residual functions grow, dilution
decreases, so the gradient increases, so updates must shrink, so the growth of residual functions
slows, so the growth of effective depth slows.
In other words, for the network to become deeper, it needs to be shallow.
Therefore, while training from an orthogonal initial state can increase effective depth, we expect
this effect to be limited. Additional techniques could be required to learn functions which require a
compositional representation beyond this limit.
C Further terminology, notation and conventions
•	x and y are generally used to refer to the components of a datapoint. Then, we have
(x,y) ∈ D.
•	X refers to a vector of dimension d, i.e. the same dimension as the x component of dat-
apoints. Similarly, Y refers to an element of the domain of possible labels. We call X a
‘data input’ and Y a ‘label input’.
•	Fl refers to a vector of dimension dl, i.e. the same dimension as fl.
•	We write fl (θ, x) as a short form of fl(θl, fl+1 (..(fL(θL, x))..)). Sometimes, we omit x
and / or θ. In that case, x and / or θ remains implicit. fl(θ, X) is an analogous short form.
•	We write fl(θ, fk) as a short form of fl(θl, fl+1(..fk-1(θk-1, fk)..)). Sometimes, we omit
fk and / or θ. In that case, fk and / or θ remain implicit. fl (θ, Fk) is an analogous short
form.
•	We use fL+1 , iL+1 and FL+1 interchangeably with x or X .
•	We say a random vector is ‘radially symmetric’ if its length is independent of its orientation
and its orientation is uniformly distributed.
•	We say a random matrix is ‘Gaussian initialized’ if its entries are independent Gaussian
random variables with mean zero and the standard deviation of all entries is the same.
•	We say an m * n random matrix is 'orthogonally initialized' if it is a fixed multiple of an
m * n submatrix of a max(m, n) * max(m, n) uniformly random orthogonal matrix.
•	We use parentheses () to denote vector and matrix elements, i.e. A(3, 4) is the fourth
element in the third row of the matrix A.
•	Throughout sections E and F, we assume implicitly that the GSC is defined and thus that
neural networks are differentiable. All results can be trivially extended to cover networks
that are almost surely differentiable and directionally differentiable everywhere, which in-
cludes SeLU and ReLU networks.
•	All theoretical results apply to arbitrary networks, not just MLPs, unless otherwise stated.
However, some assumptions arising in proposition 7 and theorems 2 and 3 may be less
easy to achieve in general architectures. We focus our discussion of these assumptions
exclusively on the MLPs within the scope of this paper as outlined at the end of section 2.
D	Effective depth: details
D. 1 Formal definition
Let a ‘gradient-based algorithm’ for training a mutable parameter vector θ from an initial value θ(0)
for a network f be defined as a black box that is able to query the gradient df(θdχ,γ) at arbitrary query
points (X, Y ) but only at the current value of the mutable parameter vector θ. It is able to generate
updates ∆θ which are added to the mutable parameter vector θ. Let the sequence of updates be
31
Workshop track - ICLR 2018
denoted as ∆θ(1), ∆θ(2), ... We define the successive states ofθ recursively as θ(t) = θ(t-1) + ∆θ(t).
For simplicity, assume the algorithm is deterministic.
In a residual network defined according to equation 2, we can write the gradient with respect to
d net*CEdfdY	df(θ TrCCfCY	CC df(θ,x,Y) 一 df0 df1 dfl-1 dfl 一	df0 d di1	I	dr1、d dil-1	I	drl-1 d dfl
a parameter	SUb-Vector	as —dθι— = f df2 ..-dJΓ西 =	f (f	+	f)..(-dJΓ	+	~fΓΓ)西.
Multiplying this out, We obtain 2l-1 terms. We call a term 'λ-residual' if it contains λ or more
Jacobians of residual functions, as opposed to Jacobians of initial functions. Let reslλ(f, θ, X, Y)
be the sum of all λ-residual terms in df(θ;XY).
dθl
NoW consider tWo scenarios. In scenario (1), When the algorithm queries the gradient, it re-
ceives { df(d,χY), df (dX,Y),.., df (dθX,Y)} i.e. the “regular” gradient. In scenario (2), it receives
{ d"θ,X,Y) -resλ(f ° χ γ) d"θ,X,Y) -resλ(f θ χ γ) d"θ,X,Y) - 丁©§人 ( f ° X Y)} ie a
{ dθ1	resι (f,θ,χ,Y ), dθ2	res2(f,θ,χ,Y ),.., dθL	r esL(f ,θ,χ,Y )},i.e.a
version of the gradient Where all λ-residual terms are removed. Let the parameter vector attain states
θ(1), θ(2),.. in scenario (1) and θ(1,λ), θ(2,λ),.. in scenario (2). Then we say the ζλ-contribution, at
time t is °t) - °(tλ). Finally, We say the ‘effective depth at time t With threshold h’ is the largest λ
such that there exists an l with ||°l(t) - °l(t,λ) ||2 ≥ h.
There is no objectively correct value for the threshold h. In practice, we find that the λ-contribution
decreases quickly when λ is increased beyond a certain point. Hence, the exact value of h is not
important when comparing different networks by effective depth.
The impact that the shift °l(t) - °l(t,λ) has on the output of the network is influenced by the scale of
°lt) as well as GSC(l, 0). If those values vary enormously between layers, it may be advisable to
set different thresholds for different layers.
D.2 Computational estimate
Unfortunately, computing the effective depth measure is intractable as it would require computing
exponentially many gradient terms. In this section, we explain how we estimate effective depth in
our experiments.
In this paper, we train networks only by stochastic gradient descent with either a single step size for
all layers or a custom step size for each layer. Our algorithm for computing effective depth assumes
this training algorithm.
Vanilla networks Assume that the network is expressed as a residual network as in equation
2. Let B be the batch size, let cl(t) be the step size used at layer l for the t’th update and let
((Xt1), Yt1)), (X(t2), Y(t,2)), .., (X (t,B), Y (t,B))) be the batch of query points used to compute
the t’th update. Then SGD computes
∆°lt)	=	cl(t) X
df0(°t-1), X(tb), Y (tb))
dθ(t T)
b
°lt-1) + ∆°lt)
32
Workshop track - ICLR 2018
For any update t and query point b, we estimate its λ-contribution at layer l as follows.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
arr := [1];
for k = 0 to l - 1 do
size = size(arr);
arr.push-back(arr[size 一 1] * ||rk ||op);
for i = size - 1 to 1 do
|| ddf-1∣2
arr[i] = arr[i] * &1——+ arr[i 一 1] * ||rk||op；
|| 酝 ||2
end
r∩1	r∩1	|| ddf+i||2
arr[0]=arr[0]*、；
end
out = 0;
for i = λ to size(arr) 一 1 do
I out = out + arr[λ];
end
return out * cl(t) * ||fl+1||2;
For unparametrized layers, ||rk ||op is set to zero. For linear layers, it is the operator norm of the
residual weight matrix. The final estimate of the length of the λ-contribution at layer l for the entire
training period is then simply the sum of the lengths of the estimated λ-contributions over all time
points and query points.
The core assumption here is that applying the Jacobian of the initial function of a given layer will
increase the lengths of all terms approximately equally, no matter how many residual Jacobians they
contain. In other words, we assume that in λ-residual terms, the large singular values of layer-wise
Jacobians do not compound disproportionately compared to other terms. This is similar to the core
assumption in theorem 1 in section F.1.
We conservatively bound the impact of the Jacobian of the initial function with the impact of the
TK Ffk f 1	∙	|| df+1 ||2
Jacobian of the entire layer, i.e.	f+1 —.
|1 f |12
We use ||rk||op as a conservative estimate on how a residual Jacobian will increase the length of a
term.
We use the sum of the lengths of all λ-residual terms in a batch as a conservative bound on the length
of the λ-contribution of the batch. In essence, we assume that all λ-residual terms have the same
orientation.
Finally, we use the sum of the lengths of the λ-contributions within each update as an estimate of the
length of the total λ-contribution of the entire training period. On the one hand, this is conservative
as we implicitly assume that the λ-contributions of each batch have the same orientation. On the
other hand, we ignore indirect effects that λ-contributions in early batches have on the trajectory of
the parameter value and hence on λ-contributions of later batches. Since we are ultimately interested
in effective depth, we can ignore these second-order effects as they are negligible when the total λ-
contribution is close to a small threshold h.
Overall, we expect that our estimate of the effective depth (e.g. figure 2D) is larger than its actual
value. This is bolstered by the robustness of some of our trained networks to Taylor expansion (see
figure 2F).
ResNet For ResNet architectures, we need to tweak our estimate of effective depth to take into
account skip connections. Below, we detail how the variable arr is modified as it crosses a skip
connection / residual block. We write fn(fm) = sn(fm) + ρn(fm), where fn is the layer at which
the skip connection terminates, fm is the layer at which the skip connection begins, sn is the function
computed by the skip connection and ρn(fm) = ρn(fn+1(..fm-1(fm)..)) is the function computed
33
Workshop track - ICLR 2018
by the residual block. We write fk = ik + rk for n + 1 ≤ k ≤ m - 1 and ρn = in + rn , i.e. we
break down each layer in the residual block into an initial function and a residual function.
1
2
3
4
5
6
7
8
9
10
11
12
arrcopy = arr ;
for k = n to m - 1 do
size = size(arr);
arr.push-back(arr[size 一 1] * ||rk ||op);
for i = size - 1 to 1 do
|| f -^n-1∣2
arr[i] = arr[i] *	PndfOk：1---+。"忖 一 1] * ||rk||op；
|1 f |12
end
arr [0] = arr[0] *
end
|| dfO- dPn Ho
U dPn dfk+1 H2
噬l∣2
for i = 0 to size(arrcopy) 一 1 do
…	…	… Il f IId-II f dρn 小
arr [i] = arr [i] + arrcopy [i] * ^m-----dfcdρn dfm一
ii dfOII2
end
In line 11, the combined effect of the skip connection and the initial functions of the residual block
.	一	.一 I . II f II2	1	-	1
is approximated by the effect of the entire block, i.e. dfm . In the same line, We must sub-
ii fII2
tract the impact of the initial functions accumulated While passing through the residual block, i.e.
-II df0 ddpn II2	, .	一	……	.,一 …	1	” 一 ，一
-dPfodfm-. The impact of the residual functions in the block is, correctly, unaffected by the SkiP
ii fII2
connection and bounded by the operator norm, as before.
D.3 Discussion
The effective depth measure has several limitations.
One can train a linear MLP to have effective depth much larger than 1, but the result Will still be
equivalent to a depth 1 netWork.
Consider the folloWing training algorithm: first randomly re-sample the Weights, then apply gradi-
ent descent. Clearly, this algorithm is equivalent to just running gradient descent in any meaningful
sense. The re-sampling step nonetheless bloWs up the residual functions so as to significantly in-
crease effective depth.
The effective depth measure is very susceptible to the initial step size. In our experiments, We found
that starting off With unnecessarily large step sizes, even if those step sizes Were later reduced, lead
to Worse outcomes. HoWever, because of the inflating impact on the residual function, the effective
depth Would be much higher nonetheless.
Effective depth may change depending on hoW layers are defined. In a ReLU MLP, for example,
instead of considering a linear transformation and the folloWing ReLU operation as different layers,
We may define them to be part of the same layer. While the function computed by the netWork
and the course of gradient-based training do not depend on such redefinition, effective depth can be
susceptible to such changes.
E Propositions and proofs
E.1 Proposition 1
Proposition 1. Given:
•	a neural network f of nominal depth L
•	an initial parameter value θ(0)
34
Workshop track - ICLR 2018
•	a mutable parameter value θ that can take values in some closed, bounded domain Θ
•	a dataset D of datapoints (x, y)
•	a closed, bounded domain D of possible query points (X, Y )
•	a function ||.|| from matrices to the reals that has c||.|| = ||c.|| and ||.|| ≥ 0
•	some deterministic algorithm that is able to query gradients of f at the current parameter
value and at query points in D and that is able to apply updates ∆θ to the parameter value
•	constant r0
Assume:
•	Running the algorithm on f with θ initialized to θ(0) for a certain number of updates T
causes θ to attain a value θ at which f attains some error value Efinal on D.
•	At every triplet (θ, X, Y ) ∈ Θ × D, we have ||Jkl || 6= 0 and ||Tkl|| 6= 0 for all 0 ≤ l ≤ k ≤
L.
Then we can specify some other neural network f0 and some other initial parameter value θ0(0) such
that the following claims hold:
1.	f0 has nominal depth L and the same compositional depth as f.
2.	The algorithm can be used to compute T updates by querying gradients of f0 at the current
parameter value and at query points in D which cause θ to attain a value θ0 where f
attains error Efinal on D	and makes the same predictions as f(θ) on D.
3.	At every triplet (θ, X, Y	) ∈ Θ × D, we have ||T0lk|| ≥ r0k-l for all 0	≤	l	≤	k	≤	L and
||J0lk|| ≥ r0k-l for all 0	≤ l ≤ k ≤ L except (k, l) = (1, 0).
Proof. Since Θ and D are closed	and bounded, so is Θ × D. Therefore for all 0	≤	l	≤ k ≤	L, both
||Jkl || and ||Tkl || attain their infimum on that domain ifit exists. ||.|| is non-negative, so the infimum
exists. ||.|| is non-zero on the domain, so the infimum, and therefore the minimum, is positive. Since
f has finite depth, there is an r such that for all tuplets (θ, X, Y, k, l), we have ||Jkl|| ≥ rk-l and
IITklII ≥ rk-l. Let R =..
Now, we define f0 via its layer functions.
f00 = f0
f10(θ1,F2)	= f1(Rθ1, R2F2)
fl0(θl,Fl+1)	= R-lfl(Rlθl, Rl+1Fl+1) for 2 ≤ l ≤L-1
fL0 (θL,X)	= R-LfL(RLθL, X)
f and f0 clearly have the same nominal and compositional depth, so claim (1) holds. Given any
vector v with L sub-vectors, define the transformation R(v) as R(v)l = Rlvl . Finally, we set
θ0(0) := R-1(θ(0)).
We use the algorithm to train f0 as follows. Whenever the algorithm queries some gradient value
务,We instead submit to it the value R-1(务). Whenever the algorithm wants to apply an update
∆θ to the parameter, we instead apply R-1(∆θ). Let S0(t), 0 ≤ t ≤ T be the state of the system
after applying t updates to θ under this training procedure. Let S(t), 0 ≤ t ≤ T be the state of the
system after applying t updates to θ when the algorithm is run on f . Then the following invariances
hold.
35
Workshop track - ICLR 2018
A θ0(t) = R-1(θ(t)), where θ0(t) is the value of θ under S0(t) and θ(t) is the value of θ under
S(t).
B f makes the same predictions and attains the same error on D under S(t) as f0 under S0(t).
C Any state the algorithm maintains is equal under both S(t) and S0(t) .
We will show these by induction. At time t = 0, we have θ0(0) = R-1 (θ(0)) as chosen, so (A)
holds. It is easy to check that (B) follows from (A). Since the algorithm has thus far not received
any inputs, (C) also holds.
Now for the induction step. Assuming that θ0(t) = R-1 (θ(t)), it is easy to check that
df (θ∕)) = R( df(θθ))). Therefore, whenever the algorithm queries a gradient of f, it will re-
ceive R-1(df dθ"") = R-1(R(df dθ")) = df(d;). Therefore, the algorithm receives the same
inputs under both S(t) and S0(t). Since the internal state of the algorithm is also the same, and the
algorithm is deterministic, the update returned by the algorithm is also the same and so is the internal
state after the update is returned, which completes the induction step for (C). Because the algorithm
returns the same update in both cases, after the prescribed post-processing of the update under f0,
we have ∆θ0(t) = R-1(∆θ(t)). Therefore θ0(t+1) = θ0(t) + ∆θ0(t) = R-1(θ(t)) + R-1(∆θ(t)) =
R-1(θ(t) + ∆θ(t)) = R-1(θ(t+1)). This completes the induction step for (A) and again, (B) follows
easily from (A).
(B) implies directly that claim (2) holds. Finally, for any tuplet (θ, X, Y, k, l), we have ||T0lk|| =
||d≡0P|| = IlRkTdfθpIl ≥ RkTrk-l = r0k-l andunless (k,l) = (1,0) we have ||J0k|| =
||dfl(θ0)) Il = ∣∣Rk-1 dfl(θ()) Il ≥ RkTrk-l = r0k-l. Therefore, claim (3) also holds, which
fk	fk
completes the proof.
□
Notes:
•	The condition that the Jacobians of f always have non-zero norms may be unrealistic. For
practical purposes, it should be enough to have Jacobians that mostly have non-zero norms.
This leads to a network f0 that has exploding Jacobians wherever f has Jacobians of size
above some threshold, where that threshold can be arbitrarily chosen.
•	Claim (3) of the proposition does not include the case (k, l) = (1, 0) and it does not include
Jacobians with respect to the input X . These Jacobians have to be the same between f
and f0 if we require f0 to have the same error and predictions as f . However, if we are
ok with multiplicatively scaled errors and predictions, claim (3) can be extended to cover
those two cases. Scaled training errors and predictions are generally not a problem in e.g.
classification.
•	Note that not only does the algorithm achieve the same predictions in the same number of
updates for both f and f0, but the computation conducted by the algorithm is also identical,
so f0 is as “easy to train” as f no matter how we choose to quantify this as long as we know
to apply the scaling transformation.
•	There are no constraints on the explosion rate r0. If we can successfully train a network
with some explosion rate, we can successfully train an equivalent network with an arbitrary
explosion rate.
•	f0 is very similar to f, so this proposition can be used to construct trainable networks with
exploding Jacobians of any shape and depth as long as there exists some trainable network
of that shape and depth.
•	The proposition can be easily be extended to non-deterministic algorithms by using distri-
butions and expectations.
•	The proposition can be easily extended to use directional derivatives instead of total deriva-
tives to cover e.g. ReLU and SeLU nonlinearities.
36
Workshop track -ICLR 2018
E.2 Proposition 2
Proposition 2. Let U be the uniform distribution over the hypersphere. Then GSC(k, l) measures
the quadratic expectation Q of the relative size of the change in the value of 力 in response to a
change in fk that is a small multiple of a random variable drawnfrom U.
lfl(fk + eu) —fl(fk)U2
Equivalently, GSC(k,l) = lime→0 Qu〜U —ifk+f—fι∣2-
wih
Proof. We use LΣRt to denote the singular value decomposition and Si to denote singular values.
(IimI Qu
flfk +eu) —∕l(∕k)U2
______UfMfk)U2_____
Ifk +eu-fk 112
IfTK
JimI Qu
Ilfl (fk + Cu) — fi(fk )∣∣2∣Ifkll2
ellfl(fk )||2
f⅛
Qullfl(fk + EU)- fl (fk)lll2
唯 fK Qullfι(fk)+Jk U+Oe-力(fk)"2
QuIIJkU+夕 ll2
唯≡k⅛
Il fk Il 2
I 力(fk) I
l l fk 11 2
I 力(fk) I
Il fk Il 2
I 力(fk) I
7 QuIIJk U112
-Qu I I L∑RUI I 2
2
-Qu∣∣∑u∣∣2
2
Il fk Il 2
I 力(fk) I
min(dk,dl)
s2u(iy2
Il fk Il 2
I 力(fk) I
min(dk ,dl)
Eu	X siu(i)2
i=1
l l fk 11 2
I 力(fk) I
min(dk,dl)
X	s2Euu(i)2
i=1
Il fk Il 2
I 力(fk) I
min(dk,dl)
t i X SS
i=1
Il fk Il 2
I 力(fk) I
l l fk 11 2
I 力(fk) I
—ll ς l l qm
2
-I I Jk Ilqm
2
2
2
2
2
GSC(k, l)
□
37
Workshop track -ICLR 2018
E.3 Proposition 3
Proposition 3. Let U be the uniform distribution over the hypersphere. Assume fk is a fully-
connected linear layer without trainable bias parameters and θk contains the entries of the weight
llθkll2llfk +1口2
matrix. Then GSC(k, l)
measures the quadratic expectation Q of the relative size of
the change in the value of 力 in response to a change in θk that is a small multiple of a random
variable drawn from U.
Equivalently, GSC(k, l)
llθkll2llfk +1U2
lime→0 Qu 〜U
lflfk（θk +eu，fk+1）） 一力（九（九，九十1）升2
llfι∣∣2 一
∣∣θk + eu-θk U2
T^rr^
IlfkIl 2 √dk 十ι
Further, if θk is random and
•	all entries of θk have the same quadratic expectation
•	all products oftwo different entries of θk have an expectation of0
•	the orientation of θk is independent ofits length
we have Q-1
llθkll2llfk +1口2
llfk∣∣2 √dk十1
1.
Proof. Throughout this derivation, We will use θk to refer to both the parameter sub-vector and the
weight matrix. Similarly, we will use eu to refer to both a perturbation of the parameter sub-vector
and of the weight matrix. We use LΣRt to denote the singular value decomposition and Si to denote
singular values.
llfι(fk(θk+eu,fk+ι))—力(fk(θk,fk+ι 力历
J吧Qu
llflll2
ll θk +eu —θk U 2
Tm^
lim 一
e→0 €
lim
e→0 €
lim
e→0 €
lim
e→0 €
θk ∖ ∖ 2
fdK
θk ∖∖ 2
∖ 力 ∖ ∖ 2
θk ∖ ∖ 2
fdK
θk ∖ ∖ 2
fdK
Qu ∖∖fl (fk(θk + €u, fk+1)) — fl(fk (θk, fk+1))∖∖2
Qullfl ((θk + €u)fk+1) — fl(θk fk + 1)∖∣2
Qullfl (θk fk+1 + €ufk+1) — fl(θkfk+1)∖∣2
QullfI (θk fk+1) + JkeUfk+1 +。(€2) — fKθkfk+1)∖∣2
lim
e→0
∖∖θk∖
耕 Qu ∖ ∖ Jkufk+1 + 但 ∖ ∖ 2
fl 2	€
||fl ||
∖∖θk∖
∖ ∖ fl ∖ ∖
∖∖θk ∖
∖ ∖ fl ∖ ∖
7 QuJk ufk+"∖2
-Qu∖∖LΣRr ufk+1∖∖2
2
-Qu∖∖∑ufk+1 ∖∖2
2
∖∖θk∖
∖ ∖ fl ∖ ∖
|2
2
min(dk,dι) dk+ι
Qu∖	E (E
SiU(Kj )fk + 1j))* 2
i=1	j=1
∖∖θk∖
2
W2∖
min(dfc ,dι) dk+1
Eu	X (X Siu(i,j)fk+1(j))2
i=1	j=1
38
Workshop track -ICLR 2018
∖θk∖∖2
≡7∖
min(dfc ,dι) dk+1 dk+1
Eu	X XXS2u(i, j)fk+ι(j )u(i,m)fk+ι(m)
i=1	j = 1 m=1
∖%∖∖2
≡7∖
min(dk,dι) dk+1 dk+1
X XXs2fk+ι(j)fk+ι(m)Euu(i,j)u(i, m)
i=1	j=1 m=1
∖θk∖∖2
≡7∖
∖θk∖∖2
W7∖
min(dk,dι) d⅛+ι d⅛+ι
X XXs2fk+1(j)fk+1(m)δjmEuu(i, j )2
i=1	j=1 m=1
min(dk,dι) dfc+ι
X X s2fk+ι(j)2Euu(i,j)2
i=1	j=1
θk∣∣2
1
∖∖fι∖∖2 ∖ dkdk+1
min(dk,dι) dk+1
X X s2fk+ι(j)2
i=1	j=1
l∣θk ∖∖2
1
fl∖∖2pdk + l ∖ dk
min(dk ,dι)	dk+1
X S X fk+1 (j)2
i=1	j=1
∖∖θk ∖∖2
fι∖∖2，dk+1
∖∖θk ∖∖2
∖∖∑∖∖qm∖∖fk+1∖∖2
∖∖Jk∖∖qm∖∖fk+1∖∖2
GSC(k,l)
∖∖θk∖∖2∖∖fk+1∖∖2
WWdkZ
Further, assume that θk is random and fulfills the conditions stated. Under those conditions, θk
is the product of a random scalar length variable ' and an independent random vector orientation
variable θ'k of unit length. Then for all 1 ≤ i ≤ dk and 1 ≤ j ≤ dfc+1, We have Eθfc(i,j)2 =
E'2θk(i, j)2 = e`22Eθoθk(i, j)2 and so Qθrk(i, j) = Q,Q；j). Since all entries of θk have the same
quadratic expectation, all entries in θ'k have the same quadratic expectation. Further, 1 = ∖∖θk∖∖2 =
Q∖∖θk ∖∖2 = JE Pij θk (i,j)2 =
expectation of each entry of θ'k is
Vzdkdk+ιEθk(1,1)2 = √dkdk+1Qθk (1,1), so the quadratic
1
dkdk+1
.Further, for all 1 ≤ i1,i2 ≤ dk and 1 ≤ j1,j2 ≤
dk+1 with (iι, jι) = (i2,j2), we have 0 = Eθk(iι, jι)θk(i2,j2) = Eθk(i1,j1)'θk(i2,j2)'
e`22Eqθk (i1, j1)θk (i2, j2), so the expectation of the product of two different entries of θk is 0.
Then, we have:
Q- 1 Wθk ∖∖2∖∖fk+1∖∖2
θk	∖∖fk∖∖2√¾;Γ
∖∖fk+1∖∖2 Q-1 Wθk ∖∖2
-√dkZQ	WK
∖∖fk+1∖∖2 q-1 ∕hmΓ
√dk+1Q V ∖∖fk∖∖2
W^ Q-IJ
ddk+1	V
∖∖θk ∖∖2
∖∖θk fk+1∖∖2
--------------1
∖∖θk fk+1∖∖2
q L
ddk+1 V
W
39
Workshop track - ICLR 2018
lfk+ι∣
√dk+
|fk + 1|
Vdk+
lfk+ι∣
√dk+
|fk+1 |
|2
|2
|2
|2
dk+1
|fk + 1|
Vdk+
|fk + 1|
Vdk+
lfk+ι∣
√dk+
|fk+1 |
|2
|2
|2
2
dk+1
|fk+1|
2
dk+1
|fk+1|
2
dk+1
1
-1
IE ll'θk fk+1∣∣2
V	ll'θk 112
SW ∣∣θk fk+ι∣∣2
VE Fr
I------------1
√E∣∣θk fk+1112
---------------------------1
E X(X θk0 (i, j)fk+1(j))2
ij
SsE X θk(i,j)fk+ι(j)θk(i,m)fk+ι(m)
i,j,m
s
s
s
s
E X θk (i,j )2fk+ι(j )2
i,j
------------------------1
Xfk+1(j)2XEθk0(i,j)2
j
j
i
i
f¾⅛ T
ji
|fk + 1||2dk；^一
dkdk+1
□
The conditions stated in the proposition for the random parameter sub-vector θk are fulfilled, for
example, if the corresponding weight matrix is either Gaussian initialized or orthogonally initial-
ized. Therefore, the most popular initialization strategies for weight matrices are covered by this
proposition.
E.4 Proposition 4
Proposition 4. Given:
•	some network f of nominal depth L
•	some parameter value θ = (θ1, .., θL)
•	constants c2, .., cL and γ1, .., γL
•	a networkf0 of nominal depth L defined via its layer functions as follows.
f00 = f0
f10(θ1,F2)	= f10(γ1θ1,c2F2)
fl(θl, Fl + 1) = clfl(Ylθl, ---Fl+1) for 2 ≤ l ≤ L - 1
cl+1
fL0 (θL,X)	= cLfL(γLθL,X)
40
Workshop track - ICLR 2018
•	a parameter value θ0 = (θ1 ,..,θL) defined via θ0 = γ11∙仇
Then for all tuples (θ,θ0,X,Y), GSC(k,l,f,θ,X,Y) = GSC(k,l,f0,θ0,X,Y).
Proof. Let c0 = c1 = 1. Then we have ||fl0||2 = cl||fl||2 for 0 ≤ l ≤ L and we have
IlJk1Ilqm = Il Cl Jkl∖∖qm = ⅛ IJkl Ilqm 。≤ l ≤ k ≤ L SO GSCW Jrfms "2 =
Ck Jk IlqmCk fk l∣2
cl∖∖fl∖∖2
GSC(k, l) fOr 0 ≤ l ≤ k ≤ L as required.
□
Here, we cOnsider general multiplicative rescalings prOvided they dO nOt change the predictiOns and
errOr values Of the netwOrk. TO ensure this, each layer functiOn must cOmpensate fOr the factOr
intrOduced by the previOus layer as well as fOr the rescaling Of the parameter. NOt all netwOrk
transfOrmatiOns that are used in practice tO cOntrOl the scale Of fOrward activatiOns fall under this
prOpOsitiOn. Changing the scale Of weights in a tanh Or SeLU netwOrk Or adding nOrmalizatiOn
layers is nOt cOvered. These changes can have a drastic impact On the high-level prOperties Of the
netwOrk, as shOwn thrOughOut the paper. On the Other hand, changing the scale Of weights in a
ReLU netwOrk is cOvered by the prOpOsitiOn, as lOng as the errOr layer f0 alsO cOmpensates fOr this
rescaling. AlsO, changing the scale Of weights in any architecture where linear layers are fOllOwed
by a nOrmalizatiOn layer is cOvered by the prOpOsitiOn.
E.5 Proposition 5
Proposition 5. Assuming the approximate decomposability of the norm of the product of Jaco-
bians, i.e. IIJ11+1J11++21..Jkk-1IIqm ≈ IIJ11+1IIqmIIJ11++21IIqm..IIJkk-1IIqm, we have GSC(k, l) ≈
GSC(k, k - 1)GSC(k - 1, k - 2)..GSC(l + 1, l).
Proof.
GSC(k, l)
IIJ1 Mqm fkg
-\\f1\\2
IIJ11+1J11++21.
.Jkk-1IIqmIIfkII2
IIf1II2

IIJ+lIIqmIIJ+21 IIqm-IIJUIIqmIIfk II2
IfI
UJl+lUqmHJl++1 IIqm..IIJk ,"gmfk "2 ”力+ lg f1+2 “2 fk-lg
IIf1 W2	fl + lg W 力+2 W2 Wfk-1 W2
IIJ1+1IIqmII/1 + 1II2 WJl+21 IIqmIIf1+2II2 WJk 」〔qmfk “2
IfI	If+⅛-..-If-⅛-
GSC(k, k - 1)GSC(k - 1,k-2)..GSC(l+1,l)
□
E.6 Proposition 6
Proposition 6. Any endomorphism on the hypersphere composed of (i) a strictly monotonic, contin-
uous nonlinearity σ that has σ(0) = 0, (ii) multiplication with a full-rank matrix and (iii) length-only
layer normalization is bijective.
Proof. We will prOve this by shOwing that the inverse image Of any pOint under such an endOmOr-
phism is a single pOint. Take any pOint On the hypersphere. The inverse image under length-Only
layer nOrmalizatiOn is a ray frOm the Origin nOt including the Origin. The inverse image Of this ray
under multiplicatiOn with a full-rank matrix is alsO a ray frOm the Origin nOt including the Origin.
41
Workshop track - ICLR 2018
What remains to be shown is that the inverse image of this ray under the nonlinearity layer, when
intersected with the hypersphere, yields a single point. We will show this via a series of claims. Let
the dimension of the hypersphere be d and its radius be r.
Claim 1: If a point on this ray has an inverse image, that inverse image is a single point.
Let this point on the ray be x. Assume its inverse image contains two points y and z. Then for
1 ≤ i ≤ d, σ(y(i)) = x(i) and σ(z(i)) = x(i) and so σ(y(i)) = σ(z(i)). But y 6= z, so there exists
an i such that y(i) 6= z(i). So there exist two different values y(i) and z(i) at which σ returns the
same result. But σ is strictly monotonic. Contradiction.
Claim 2: If two points x1 and x2 on the ray have inverse images y1 and y2 and x1 is closer to the
origin than x2, then for 1 ≤ i ≤ d, we have |y1(i)| ≤ |y2(i)|.
For 1 ≤ i ≤ d, σ attains x2 (i) at y2 (i) and 0 at 0. Since σ is strictly monotonic, continuous and
0 ≤ |x1(i)| ≤ |x2(i)| and x1(i) and x2(i) have the same sign, σ attains x1(i) at a point between 0
and y2(i). Hence, |y1(i)| ≤ |y2(i)| as required.
Claim 3: The function f that assigns to each point on the ray that has an inverse image the length of
that inverse image is strictly increasing in the direction away from the origin.
Take any two points on the ray x1 and x2 that have inverse images y1 and y2 where x1 is closer
to the origin. By the previous	claim,	for 1 ≤ i	≤ d, we have	|y1(i)| ≤	|y2(i)|	and therefore
||y1 ||2 ≤ ||y2||2 and therefore	f(x1)	≤ f(x2).	Assume f(x1)	= f(x2).	Then	we must have
|y1(i)| = |y2(i)| for 1 ≤ i ≤ d. Since σ is strictly monotonic and σ(0) = 0, σ either preserves the
sign of all inputs or reverses the sign of all inputs. Since x1 (i) and x2(i) have the same sign, so do
y1(i) and y2(i). So y1(i) = y2(i), so y1 = y2, so the forward images of y1 and y2 are the same, so
x1 = x2. Contradiction. So f(x1) 6= f(x2), so f(x1) < f(x2).
Claim 4: The function f that assigns to each point on the ray that has an inverse image the length of
that inverse image is continuous.
Since f is only defined on a 1-dimensional space, it is enough to show left-continuity and right-
continuity.
Part 1:	left-continuity. Take a sequence of points on the ray with inverse images that approach some
point xlim on the ray from the left and assume that xlim also has an inverse image ylim. Then we need
to show that the length of ylim is the limit of the lengths of the inverse images of the sequence. It is
enough to show this for the monotonic re-ordering of the sequence. Let that monotonic re-ordering
be xn. Then we have xn → xlim and ||xn || increases. By claim 2, for 1 ≤ i ≤ d, |yn (i)| is an
increasing sequence. This means that |yn (i)| either converges or it is an unbounded sequence. If
the latter is true, then it will exceed |ylim(i)|. But since xlim is at least as large as all xn, again by
claim 2 we must have |ylim(i)| ≥ |yn(i)|. Contradiction. So |yn(i)| converges. Since σ is strictly
monotonic and σ(0), σ either preserves the sign of all values or it reverses the sign of all values.
Since for each 1 ≤ i ≤ d, the xn(i) all have the same sign because the xn are on a ray, the yn (i) all
have the same sign and so since |yn(i)| converges, yn(i) converges. Let its limit be yl0im(i). Because
σ is continuous, its value at yl0im(i) is the limit of σ(yn(i)). But that is xlim(i). So if yl0im is the vector
made up of the yl0im(i), itis the inverse image ofxlim. i.e. yl0im = ylim. Since ||.||2 is also a continuous
function, ||yn||2 → ||yl0im||2 and so ||yn||2 → ||ylim||2 and so f(xn) → f(x) as required.
Part 2:	right-continuity. This case is analogous. We have a decreasing sequence xn and so decreasing
sequences |yn (i)| and so convergent sequences yn (i) with a limit yl0im that is equal to ylim and so
f (xn) → f (x) as required.
Claim 5: The co-domain of the function f that assigns to each point on the ray that has an inverse
image the length of that inverse image is the positive reals.
We argue by contradiction. Assume the co-domain of f is not the positive reals. Then the set S of
positive reals not attained by f is non-empty. Let s be the infimum of S .
Case 1:	s = 0. Since σ is strictly monotonic with σ(0) = 0, there exists an > 0 such that σ attains
all values in the interval [-, ]. So all points on the ray for which all components have absolute
value less than have an inverse image, so there exists an 0 > 0 such that all points on the ray with
length in the interval (0, 0] have an inverse image, so f is defined there. Further, if we extend the
domain of f to include the origin, we have f (0) = 0. But by the same argument used for claim 4, f
42
Workshop track - ICLR 2018
is then right-continuous at 0. Since f is also continuous and defined on (0, 0], it attains all values in
the interval [0, f (0)]. Specifically, if f is restricted to its original domain (the ray), it still attains all
values in (0, f(0)]. So the infimum of S is at least f(0). Contradiction.
Case 2:	s > 0 and s ∈ S. Then there exists a sequence xn of points on the ray such that f(xn) → s
and f (xn) is strictly increasing. By claim 3, ||xn||2 is strictly increasing. Let the inverse images of
the xn be yn. By claim 2, |yn(i)| is increasing for 1 ≤ i ≤ d. Since |yn(i)| ≤ ||yn||2 < s, |yn(i)| is
bounded from above, so it converges. As σ is strictly monotonic and σ(0) = 0, σ either preserves
the sign or reverses it for all inputs. So since the xn (i) all have the same sign, so do the yn (i). So
yn (i) converges. Let this limit be ylim(i). Since for 1 ≤ i ≤ d, yn (i) → ylim(i), we have yn → ylim
where ylim is the vector composed of the ylim(i). Since σ is continuous, the forward image of ylim is
the limit of forward images of the yn. Since the forward images of the yn lie on the ray, so does their
limit. Hence, the forward image of ylim lies on the ray. Call it xlim. Since length is also continuous,
s = limn→inf ||yn||2 = ||ylim||2. So f (xlim) = s so s 6∈ S. Contradiction.
Case 3:	s > 0 and s 6∈ S. Then there is a point x on the ray for which f (x) = s. Let its inverse
image be y. Let I be the set of indeces i with 1 ≤ i ≤ d and x(i) 6= 0. Since σ has σ(0) = 0 and
it is strictly monotonic, y(i) 6= 0. For i ∈ I, let σmax(i) = σ(2y(i)). Since σ has σ(0) = 0 and it
is strictly monotonic, We have ∣σ(2y(i))∣ > ∣σ(y(i))∣ > 0 and so ∣σmaχ(i)∣ > |x(i)| > 0 and also
σmaχ(i) and x(i) have the same sign. Let C = mi□i∈∕ σχχ(i). Take some vector y0 that can vary.
Since σ is continuous, as y0(i) varies betWeen y(i) and 2y(i), it attains all values betWeen x(i) and
σmax(i). So for all 1 ≤ c ≤ C, We can set y0(i) to some value yc(i) such that σ(yc(i)) = cx(i). So
the vector yc that has the aforementioned yc(i) components for i ∈ I and has zero components for
i 6∈ I is the inverse image of cx. So f is defined on cx for 1 ≤ c ≤ C. Let s0 := f(Cx). By claim
3, f is strictly increasing so s0 > s. By claim 4, f is continuous. So betWeen x and Cx, f takes
all values betWeen and including s and s0 . Also, by the original definition of s, f attains all positive
real values less than s. So f attains all positive real values less than s0. But s Was defined to be the
infimum of positive real values that f does not attain. Contradiction.
Claim 6: The inverse image of the ray intersects the hypersphere in a single point.
By claim 5, there is a point on the ray that has an inverse image of length r. By claim 3, there is
exactly one such point. Therefore, the inverse image of the ray contains exactly one point of length
r, so it intersects the hypersphere in exactly one point, as required.
□
The proposition also applies if each neuron in the nonlinearity layer uses a different nonlinearity σi
as long as it fulfills the stated conditions.
E.7 Proposition 7
We say a random function fb is ‘k-diluted in expectation’ With respect to random vector v if there
exists a random matrix Sb and a random function Pb such that fb(v) = Sbv + ρb(v) and 第；：(：|卮 =
k.
We say a random function ρ(v) is ‘scale-symmetric decomposable’ (SSD) if it can be Written as
uρ'ρ( 1⅛^ )llvl∣2, where 'ρ is a random scalar function and UP is uniformly distributed on the hy-
persphere and independent of both 'ρ and v.
We say a random matrix S is ‘scale-symmetric decomposable’ (SSD) if Sv, when viewed as a
function of the vector v is SSD.
Proposition 7. Let u be a uniformly distributed unit length vector. Given random functions fb,
1 ≤ b ≤ B, which are kb-diluted in expectation with respect to u, a matrix Sb that is either SSD or
a multiple of the identity and an SSD random function ρb := fb - Sb where all the Sb and ρb are
independent, fι ◦ f2 ◦ .. ◦ /b is ((Qb(1 + 专))一 1)2 -diluted in expectation with respect to U.
Proof. Let U be the uniform distribution over unit length vectors and U 〜 U. We will procede by
induction over B, where the induction hypothesis includes the following claims.
43
Workshop track -ICLR 2018
_1
1.	fι o /2 ◦ .. ◦ fβ is ((Qb(1 + 表))-1) 2 -diluted in expectation.
2.	Q∣∣fl0f20..0fB(u)∣∣2 = P(Q∣∣S1S2∙∙SBu∣∣2)2 + (Q||/1 o /2 o ∙∙ o /b(U)- S1S2∙∙SBu∣∣2)2
3.	S1 S2∙∙SbM fl 0 f2 0 ∙∙ 0 /b(U) and fι 0 f2 0 ∙∙ 0 /b(U) - S1S2∙∙SBU are all radially
symmetric
Let,s start by looking at the case B = 1. Claim (1) follows directly from the conditions of the
proposition. We have:
Q∣∣f1(u)∣∣2
Q∣∣S1u + ρ1(u)∣∣2
QP (Siu + ρι(u))∙(Sιu + ρι(u))
，E(Siu + ρι(u))∙(Sιu + ρι(u))
ʌ/E(SIU)∙(Sιu) + 2E(Sιu)∙pι(u) + Epι(u)∙pι(u)
Je∙∙∙ + 2E(S1u)∙(uρ1 ,ρι (血)∣∣u∣∣2) + E∙∙∙
Je∙∙∙ + 2Eu,Si,'.i (S1u)∙(4ρ1 (血)∣∣u∣∣2E%ιup1)+ E∙∙∙
pE(SIU)∙(Sιu) + 0 + Ep1(u)∙p1(u)
√(Q∣∣S1u∣∣2)2 + (Q∣∣ρ1(u)∣∣2)2
√(Q∣∣Siu∣∣2)2 + (Q∣∣fι(u) - S1u∣∣2)2
This is claim (2). For any u, ρι(u) is radially symmetric because pi is SSD. If Si is SSD, Siu is
also radially symmetric for arbitrary u. If Si is a multiple of the identity, Siu is radially symmetric
because U is radially symmetric. In either case, Siu is radially symmetric. Because the orientation
of pi (u) is governed only by Upι which is independent ofboth U and Si, the orientations of Siu and
pi(u) are independent. But the sum of two radially symmetric random variables with independent
orientations is itself radially symmetric, so fi(u) is also radially symmetric. This yields claim (3).
Now for the induction step. Set B to some value and also define ko ：= ((QB=2(1 + 专))-1) 2,
So := S2S3∙∙SB, fo ：= f2 0 ∙∙ 0 /b and po ：= fo - So. Then the induction hypothesis
yields that fo is ko-diluted in expectation, that Q∣∣fo(u)∣∣2 = yzQ∣∣SoU∣∣2 + Q∣∣po(u)∣∣2 and
that fo(u), po(u) and S°u are radially symmetric. This implies that f。尚, ∣∕(l¾∣2 and
USloiu∣2 are uniformly distributed unit length vectors. Define c0 := Q∣∣S°u∣∣2. Then 晨-dilution
in expectation implies Q∣∣po(u)∣∣2 = * and hence Q∣∣fo(u)∣∣2 = JI + co. Similarly, let
ci := Q∣∣Siu∣∣2 and then Q∣∣pi(u)∣∣2 = 置 and analogously to the B = 1 case We have
Q∣∣fi(u)∣∣2 = √(Q∣∣Siu∣∣2)2 + (Q∣∣fi(u) - Siu∣∣2)2 = M + (■&)2 = q1 + k⅛ci. Wehave
Q∣∣pi(fo (u))∣∣2
√Epi(fo(u))∙pi(fo(u))
E(UPI 'p1 (i∣Λo(Uk )∣∣fo(U)∣∣2)∙(UPI 'pι (fu⅛ )∣∣fo(U)∣∣2)
S
Ef o(u州 2, || f0θ(U) 11 2 ,P1
∣∣fo(U)∣∣2(UPI 'pι (i∣ffo⅛⅛ ))∙(upι 'pι (ifUfe))
44
Workshop track -ICLR 2018
{EI1f。(U)|12||f。(U)112E TTfUk ,P1(UPI&1 (J(U))||2)).(UPI °ρl (Hf
1 (I + 硬)c2E f。(U) p ρ1(T∖7^7~∖∖∖^)∙ρ1(^∖∖7^7~y∖∖~)
V	k2	∣∣∕o(u)∣∣2,p1	|| 九(u)∣∣2	Il 九(u)∣∣2
J(i + 3 )ci(QHρι( JU^)||2)2
k	k2	||f。(u)||2
If	Si	is SSD, analogously, we have	Q||SiS°u||2	=	coc1,	Q||Sip0(u)||2 =	ɪ0-c1	and
Q||Si/o(u)||2 = J1 + 6CoCi. If Si is a multiple of the identity, by the definition of ci,
it is ci times the identity. Therefore Q||SiSou||2 = Q||ciSou||2 = c0ci, Q||Sipo(u)||2 =
Q||cipo(u)||2 = kθθci and Q||Sfo(u)||2 = Q||cfo(u)||2 = , 1 + 表c°ci also hold. Then we
have
Qf i(fo(U))- SiSou||2
Q||Si(fo(u))+ ρi(fo(u)) - SiSou||2
Q||Si(SoU + ρo(u)) + ρi(fo(u)) - SiSou||2
QIISiPo(u) + ρifo(U))||2
√E(SiPo(u)).(SiPo(u)) + E(SiPo(u)).ρi(∕o(u)) + Eρi(fo(u)).ρi(fo(u))
JE…+ 2E(Siρo(u)).(upιCpi (	fo(U)	)||fo(u)||2)+ E...
||fo(U)||2
jE... + 2E∕o(u),po(u),Sι,'ρi (Siρo(u)).(Cpι ( fo(U)||/||fo(u)||2Eu.i UpJ + E...
PE(SiPo(u)).(SiPo(u)) + 0 + Eρi(fo(u)).ρi(fo(u))
coci
j-1 + (I +
And analogously we have
Q||fi (fo(U))||2
Q||Si(fo(U))+ Pi(fo(U))||2
P(Q||Sifo(U)||2)2 + (Q||pi (fo(U))||2)2
45
Workshop track - ICLR 2018
S	Q∣∣SiSoU∣∣2
So QfIfO(U))-SiSoU∣∣2
CoCl
coci ^-i+(ι+k12 )(ι+k⅛)
(-1 + (I + k12 )(1 + k12 ))- 2 . Substituting
back k°, SQ and f°, We obtain claim (1). Claim (2), when substituting in k°, So and f becomes
Q∣∣fι(f°(υ))∣∣2 = P(Q∣∣SiSqU∣∣2)2 + (Q∣∣fι(fο(u)) - SιS°u∣∣2)2. Substituting the identities
we obtained results in c°cι J(1 +	)(1 + 吉) = j(c°cι)2 + 卜心^-1 + (1 + 6)(1 + 吉)),
which is true, so we have claim (2).
Consider S1 S2..SbU = S1SQυ. We know Sou is radially symmetric by the induction hypothesis,
so if Si is a multiple of the identity, so is SιS°u. If Si is SSD, then SιS°u is radially symmetric for
any value of Sou. In either case, S1Sou is radially symmetric.
Consider fif2..fBu = fifou. We know fou is radially symmetric by the induction hypothesis. We
also have fifou = Sifou + ρi(fou). We just showed Sifou is radially symmetric. Because ρi
is SSD, ρi(fou) is radially symmetric with an orientation independent of that of Sifou because it
is governed only by uρ1 . The sum of two radially symmetric random variables with independent
orientation is itself radially symmetric, so fifou is radially symmetric.
Finally, consider fif2..fBu - SiS2..SBu = Siρo(u) + ρi(fo(u)). We know ρo(u) is radially
symmetric by the induction hypothesis so as before, Siρo (u) is radially symmetric. And again,
ρi(fo(u)) is radially symmetric with independent orientation, so the sum fif2..fBu - SiS2..SBu
is radially symmetric.
So we also have claim (3). This completes the proof.
□
A Gaussian initialized matrix and an orthogonally initialized matrix are both SSD. Therefore the
condition that the Sb are either the identity or SSD is fulfilled for all popular skip connection types.
Unfortunately, many ResNets do not quite fulfill the SSD condition on ρb, but they come close. If
the last operation of each ρb is multiplication with an SSD matrix, a popular choice, the orientation
of ρb is indeed governed by an independent, uniform unit length vector uρ as required. However,
many choices of ρb do not preserve the length of the incoming vector ||v||2 as in an SSD function.
However, because the inputs to each ρb are random and high-dimensional, we do not expect their
lengths to vary much, especially if the original inputs to the network have themselves been normal-
ized. So the loss of the information of the length of the incoming vector should not cause the overall
behavior to change significantly.
A block function ρb that is SSD, for example, is any such function that is composed of only linear
and ReLU layers, where the final layer is Gaussian or orthogonally initialized.
Finally, note that this proposition applies only in expectation over randomly initialized matrices.
As long as those matrices are high-dimensional, we expect it to apply approximately to specific
realizations of those matrices as well.
F Theorems and proofs
F.1 Theorem 1 - exploding gradients limit depth
See section D for the formal definition of effective depth and related concepts.
Consider some MLP f with nominal depth L and layers fl , 1 ≤ l ≤ L. Let its compositional depth
be N and its linear layers be fln, 1 ≤ n ≤ N where li < l2 < .. < lN. Let each linear layer be
the sum of an unparametrized initial function iln and a parametrized residual function rln (θln ). iln
represents multiplication with the initial weight matrix and is used interchangeably to denote that
initial weight matrix. rln (θln) represents multiplication with the residual weight matrix and is used
46
Workshop track - ICLR 2018
interchangeably to denote that residual weight matrix. The parameter sub-vector θln contains the
entries of the residual weight matrix.
Let an N -trace φN be a subset of {1, .., N}. Let ΦN be the set of all possible N -traces and let ΦλN
be the set of all N -traces of size λ or more. We define the ‘gradient term’ G(φN, f, θ, X, Y ) :=
J0J1..JlN+1-1 where Jk = Jkk+1 if layer k is not a linear layer, Jk = rln (θln) if layer k corresponds
to linear layer ln and n ∈ φN, and Jk = iln if layer k corresponds to linear layer ln and n 6∈ φN.
LetresΛN(f,θ,X,Y) := PλN=-Λ1 PφN-1∈ΦλN-1 G(φN-1, f, θ, X, Y).
Theorem 1. Consider an MLP f as defined above with all parameter sub-vectors initialized to
θl(0) = 0. Taken some set of possible query points D. Let each of the parameter sub-vectors be
updated with a sequence of updates ∆θl(t) such that θl(t) = θl(t-1) + ∆θl(t) with 1 ≤ t ≤ T. Let alg
be a fixed function and Λ an integer. Further assume:
ι ∆θ(n)=aig( df(yt,γ B)for some (X (t) , Y (t) ) ∈ D
ln
I 1	1	1 1	∣∣∆θ(t)∣∣2	1 z,	7
2.	there exist constants r and C such that n∣? ≤ Crn for all n and t
3.	there exists a constant c0 ≥ 1 such that
∖∖nln ( df(θ(I)Xt) ,Y ⑴)ʌ	njn ( df(θ(τ1,X(t,Y ⑴)	Λ/ f θ(t-1) χ (t) γ (t) Λ∣∣
||algl	dθ(t-i)	- - ajg∖	dθ(t-i)	- resN(f,θ	,X ,Y )J ||2
_____________IN	IN_______________________________________
||alg(	X (» )∣∣2
g	dθ(t-i)	2
lN
≤ C NX	X Y	llrln ^?)“2
.λ=Λ φN-1∈ΦN-1 n∈φN-ι	lliln ||2
for all N and t.
4.	T ≤ 32卷(ln r)3r4 for some h ≤ 1
Then for all N we have
≤
≤
1T
llilN ||2	t=1
Proof. FOrall T0 ≤ T,wehave 眄，在=|| PL ∆θ(*∣2 ≤ PT; 1 ∣∣∆θ(n)∣∣2 ≤ 琮限方 ≤
CTn∣∣iin∣∣2. Then we have
1
llilN ||2
T
X ||ajg
t=1
resΛ
1
llilN ||2
dθ
(t-1)
lN
1
||ilN ||2
∑NaJg( D )∣∣2 c0 NX1	X Y
t=1	dθlN	λ=Λ φN-i∈ΦλN-i n∈φN
值?(九)1|2
l∣iinl∣2
T	∆θ(t)	N-1
C0Xɪɪ X X Y
t=1	N λ=Λ φN -i ∈ΦλN -i n∈φN-i
l∣rin (θ(n) )l∣2
lliln ll2
47
Workshop track - ICLR 2018
≤	c0TX X Y	∣∣θ(n)∣∣2
-crN λ=Λ φN-1∈ΦN-1 n∈φN-ι ∣∣ilnl∣2
≤
Tc0
CrN
T
crn
N
XX Y
λ=Λ φN-1 ∈ΦλN-1 n∈φN-1
≤
∏ rn
n∈φ∞
Let K(λ, n) be the number of ways to choose λ distinct positive integers such that their sum is n.
Clearly, K(λ, n) = 0 for n < 入时^. For n ≥ 入时^, the largest number that can be chosen is
n - λ(λ2+1) + λ and so K(λ,n) ≤ (n - λ(¾≡ + λ)λ = (n - λ(λ2-1))λ. So we have
0∞
CrN X(7)λ X Y
λ=Λ
φ∞ ∈Φλ∞ n∈φ∞
rn
1
K(λ, n)
λ=Λ
rn
n
K(λ, n)
λ=Λ
n= λ(λ-1) +1
rn
≤
<
≤
≤
<
<
<
λ=Λ
n= λ(λ-1) +1
生 X (T)λr
λ=Λ
ITC X (T)λr
λ=Λ
(n - λ(λ-1))λ
rn
λ(λ-1)
λλ(ln r)-λ
∞
c0 X (T)λ+1r- - λλ(ln r)-λ
λ=Λ
co X (船(In r)3r 4
λ=Λ
)λ+1 r-λ(λ-u λλ(ln r)-λ
∞
X h关(-)λ+1(-)λ+1 (lnr)2λ+3r-4λ λλ
λ=Λ	2	16
∞
X h斗2产
λ=Λ
h
c


□
alg represents the gradient-based algorithm and the quantity that is ultimately bounded by h is the
first-order approximation of the relative λ-contribution at layer lN until time T. To obtain that the
network has effective depth Λ, all we need is to set h to a small value. In that case, the first-order
approximation is sufficient.
Now, we analyze the four conditions in turn.
48
Workshop track - ICLR 2018
Condition (1) states that the algorithm computes the update. For convenience, we write the algorithm
as a deterministic function of the gradient of the layer for which the update is computed. The proof
can be trivially extended to algorithms that use the gradients of other layers, past gradients and
as well randomness if we add the same dependencies to condition (3). Also for convenience, we
assume a batch size of 1. We can apply the result to larger batch sizes, for example, by having alg
use past gradients and setting the majority of updates to zero.
Condition (2) reflects the argument from section 4.3 that the area around the current parameter
value in which the gradient is reflective of the function is bounded by a hypersphere of relative
radius gs。% 0), and the assumption that gradients explode, i.e. GSC(ln, 0) ≥ CrIn. Note that for
convenience, We divide the size of the update ∣∣∆θ(t) ∣∣2 by the weight matrix in the initialized state
∣∣i1n∣∣2 instead of ∣∣θ(t-1)∣∣2. This is realistic given the general observation that the largest useful
update size decreases in practice when training a deep network. Therefore, we can bound all updates
by the largest useful update size in the initialized state.
The strongest condition is (3). It can be understood as making two distinct assertions.
Firstly, ignoring the alg() function, it bounds the length of the sum of the Λ-residual terms. In
essence, it requires that on average, the size of these terms is “what one would expect” given the
L2 norm of the initial and residual weight matrices up to some constant C0 . In other words, we
assume that in λ-residual terms, the large singular values of layer-wise Jacobians do not compound
disproportionately compared to the full gradient. The bound is however also very conservative in
the sense that it implicitly assumes that all λ-residual terms have the same orientation.
Secondly, it asserts that alg() is “relatively Lipschitz” over the gradient. This is fulfilled e.g. for
SGD and SGD with custom layer-wise step sizes as used in our experiments. It is fulfilled by
SGD with momentum as long as size of the momentum term is bounded below. In theory, it is
not fulfilled by RMSprop or Adam as gradients on individual weight matrix entries can be “scaled
up” arbitrarily via the denominator. In practice, the regularization term used in the denominator
prevents this, although this is rarely necessary.
Finally, condition (4) states that the training time is limited. Importantly, the bound on T is expo-
nential in Λ and independent of both L and N . Note that we did not attempt to make the bound tight.
As it stands, unfortunately, the bound too loose to have much practical value. It would indicate that
networks can be trained to far greater depth than is possible in practice. The limitation of effective
depth in practice is studied in section 4.4.
F.2 Theorem 2 - why gradients explode
Theorem 2. Consider a neural network f with random parameter θ composed of layer functions
fl that are surjective endomorphisms on the d-dimensional hypersphere. Let Jl := Jll+1. Let S be
the hypersphere and let S be the uniform distribution on it. Let Fl, 1 ≤ l ≤ L, be random vectors
independent of θ where Fl 〜S. Assume:
1.	The θl are independent of each other.
2.	Each Jacobian Jl(θl, Fl+1) has d - 1 nonzero singular values which, as Fl+1 and θl vary,
are independent and drawn from some distribution Pl.
3.	There exist some e > 0 and δ > 0 such that Ppl 〜Pl (||pl|- Eql 〜Pl (| ql |) | > e) > δ for all l.
4.	fl(θl, Fl+1) is a uniformly distributed vector on the hypersphere.
5.	For any unit length vector u, Jl(θl, Fl+1)u can be written as `l(θl, Fl+1, u)ul. Here, `l is
random scalar independent of both ul and fl(θl, Fl+1). ul, conditioned on fl(θl, Fl+1), is
uniformly distributed in the space of unit length vectors orthogonal to fl(θl, Fl+1).
Let X 〜S be a random vector independent of θ. Then setting r(δ, C) := √1+^δe2 we have
Qθ,xGSC(k,l,f, θ,X) ≥ qd-1r(δ,c)k-l.
49
Workshop track - ICLR 2018
Proof. The hypersphere is a d - 1-dimensional subspace in Rd. Hence, any endomorphism fl on
that subspace will have Jacobians with at least one zero singular value with right eigenvector equal
to the normal of the subspace at the input and left eigenvector equal to the normal of the subspace
at the output. Since the subspace is the unit hypersphere, the normal at the input is the input and the
normal at the output is the output. By assumption, the Jacobian has no other zero singular values.
Let the singular values of the Jacobian be s1, .., sd-1, sd. WLOG we set sd = 0.
Throughout this proof, we use det(Jl) to denote the product of singular values of the Jacobian
excluding the zero singular value, i.e. det(Jl) = Qid=-11 si. We use ||Jl||qm to denote the quadratic
mean of the singular values excluding the zero singular value, i.e. ||Ji ||qm = p Pd=-IISi
As fl is surjective, for fixed θl, we have by integration by substitution S 1	≤
RS | det(Jι(θ1,F1+1))∣dF1+1.	BUt we also have R | detjl(7,Fl+1))ldFl+1	=
S
Efi+1 | det(Jι(θι,Fι+ι))∣ and so Ef5 | det(Jι(θι, Fι+ι))∣	≥	1 and so
EFl+1,θl | det(Jl(θl, Fl+1))| ≥ 1 and so EFl+1,θl | Qid=-11 si| = EFl+1,θl Qid=-11 |si| ≥ 1.
But the nonzero singular values are assumed to be independent by condition (2). So
Efi+i,Θi Qd-1 ∣Si∣ = (Efi+i,Θi∣Si∣)d-1 for 1 ≤ i ≤ d - L So (Efi+i,Θi∣Si∣)d-1 ≥ 1 and
so EFl+1,θl |si| ≥ 1 for 1 ≤ i ≤ d - 1.
Similarly, we have
QFl + ι,θl llJl(θl,Fl+1)llqm
=QFl+ι,θι jP⅛2
=SEFl+ι,θl P=T2
=√EFl + ι,θl S2
=	(EFl+1,θl |s1|)2+ VarFl+1,θl (|s1|)
≥ Pl + δe2
The last identity uses condition (3). Let θkl := (θl, θl+1, .., θk-1).
Now, we will prove the following claim by induction:
(A)	fl(θLl , X) is a uniformly distributed vector on the hypersphere independent of θl1.
Since X 〜S, by condition (4), We have ∕l(Θl, X)〜S. Also, since X is independent of θL and
θL is independent of θL1 , fL(θL, X) is independent of θL1 , as required.
Now for the induction step. Assume (A) is true for some l. Then by the induction hypothesis,
fl(θLl ,X) 〜S and fι(θL,X) is independent of θ11 and thus independent of θ11-ι. Then by con-
dition (4),力-ι(θl-1,X) = fι-ι(θι-ι,fι(θlL,X))〜S. Also, since both θι-ι and f®, X) are
independent of θl1-1, so is fl-1(θLl-1, X). This completes the induction step.
Analogously, we have claim (A2): fl (θkl , Fk) is a uniformly distributed vector on the hypersphere
independent of θl1 .
Now we will prove the following claim by induction on k - l.
(B)	For any unit length vector u, Jkl(θkl , Fk)u can be written as `lk(θkl , Fk, u)ul. Here, `lk is a random
scalar independent of both ul and fl(θkl , Fk). ul, conditioned on fl(θkl , Fk), is uniformly distributed
in the space of unit length vectors orthogonal to fl (θkl , Fk).
The case k = l + 1 is equivalent to condition (5). For the induction step, consider
50
Workshop track - ICLR 2018
Jkl(θkl ,Fk)u
Jll+1(θl,fl+1(θkl+1,Fk))Jkl+1(θkl+1,Fk)u
Jι+ι(θι,fι+ι(θk+1,Fk))'k+1(θk+1, Fk,u)uι+ι
'k+1 (θk+1, Fk,u) J+1(θ1, fι+ι(θk+1, Fk))uι+ι
'k+1 (θk+1,Fk,u)'l+ι(θι, fι+ι,uι+ι)uι
Here, we use claim (A2) and the induction hypothesis twice. Let `lk (θkl , Fk, u) :=
'k+1(θk+1,Fk,u)'l+ι(θι,fι+ι,uι+ι). Then Jl(θk,Fk)u has the required decomposition. From
when We used the induction hypothesis the first time, We obtained that'+ is independent of f+
and ul+1, and therefore of fl(θl, fl+1) and Jll+1(θl, fl+1), and therefore of ul. From when we used
the induction hypothesis the second time, we obtain that'+ is independent of Ul and f. Therefore,
both 'l+1 and 'l+1 are independent of both ul and fl, and then so is `lk. From using claim (B) the
second time we also obtained that conditioned on fl, ul is a uniformly distributed among unit vectors
orthogonal to fl. Therefore, the decomposition Jkl(θkl , Fk)u = `lkul fulfills the required conditions,
so the induction step is complete.
Now we will prove the main claim by induction on k - l.
We will begin with the case k = l + 1. We have Qθ,xGSC(l + 1,l,f,θ,X)=
Qθ,x jd-ɪlljl+1∣∣fmmlfl+lll2. (Note that the qm norm was redefined.) Because the domain
of all layer functions is the hypersphere, we have ||fl+1||2 = ||fl ||2 and so Qθ,XGSC(l +
1,l,f,θ,X) = qd--1 Qθ,χ ∣∣Jl+ι∣∣qm = qd- Qθ,χ ∣∣J+ι(θl ,fl+ι(θl+1,X ))||qm. But by claim
(A), fl+ι(θLl+1,X)〜S and 力+ι(θl+1,X) is independent of θl. So Qθ,x∣∣Jll+1∣∣qm ≥ √1 + δe2,
so Qθ,χ GSC(I + 1,l,f, θ,X) ≥ qd--1 √1 + Ji2.
Now for the induction step. Let U be the uniform distribution over the unit hypersphere. As before,
we have Qθ,x GSC(k,l, f,θ,X) = qd--1Qθ,x ∣∣Jl∣∣qm and so
Qθ,x GSC(k,l,f,θ,X)
=ʌ/d--ɪQθ,χ I∣J+1 Jk+1IIqm
=-d-ɪ-Qθ,X,u〜U ||Jll+1 Jk + 1u||2
=ʌ/d--ɪQθ,X,u〜UI∣J+ι(θl, fl+ι(θl+ ,X))Jl+1(θ∣+1,fk(θL,X))u∣∣2
fk (θL, X) is 〜S and independent of θll+1 by claim (A). So by claim (B), we have that Jl+1u can
be written as 'k+1(θl+1, fk,u)uι+ι with the properties stated in claim (B). Using those properties,
we obtain
Qθ,XGSC(k, l, f, θ, X)
=Qθ,χ,u 〜uIlJ+ι(θι,fι+ι(θL+1,X ))Jl+1(θl+1,fk (θL ,X ))u∣∣2
=(Q'l+1 'k+1)(Qθι,fι + ι,uι + ιI∣Jll+l(θl,力+ l)ui + 1∣∣2)
k
Let,s look at the first term Q'i+ι 'k+1. We have
Q'k+ι 'k+1
51
Workshop track - ICLR 2018
=Qθ,χ,u∣∣Jk+1u∣∣2
=Qθ,χ GSC(k,l +1,f,θ,X)
Id - 1 I-----k-l-1
≥ 1 P^
The last line comes from the induction hypothesis.
Now, let’s look at the second term Qθι,fι+ι,uι+ι ∣∣J1+1(Θ1,力+1)u1+1∣∣2. u1+1 if uniform among
unit length vectors orthogonal to fl+1. But this leads to ul+1 being orthogonal to the normal of
the hypersphere at fl+1 and thus orthogonal to the right null space of Jll+1. Since ul+1 is also
independent Of θι, We have Qθι,fι + ι,uι + ι 11J11+I(θl, fl +I)Ul+ 1||2 = Qθι,fι + ι ||J+1 (θl, fl+1) ||qm. By
claim (A), fl+ι is 〜S and independent of θl, so Qθι,fι+ι ∣∣Jll+1 (θl, fl+ι)∣∣qm ≥ √1 + δe2.
Putting those results together, we obtain
Qθ,XGSC(k, l, f, θ, X)
=(Q'l+1 'k+1)(Qθι,fι + ι,uι + ιllJ+l(θl,力+ l)ul+1∣∣2)
k
Id — 1 I------k—l—1 I------
≥	J ----∖∕l + δe2	-∖∕l + δe2
Id — 1 /-----k—l
=vɪ p1+7≡2
This is the desired claim.
□
Let’s look at the conditions.
Condition (1) is standard for randomly initialized weight matrices.
Conditions (4) and (5) are both fulfilled if the last two operations of each layer function are multipli-
cation with a weight matrix and length-only layer normalization and that weight matrix is Gaussian
or orthogonally initialized.
If the weight matrix is orthogonally initialized, this is easy to see, because the linear transforma-
tion and the normalization operation commute. If we exchange those two operations then the last
operation applied in both fl(θl, Fl+1) and Jl(θl, Fl+1)u is the orthogonal transformation, which de-
couples the orientations of both terms from the length ofJl(θl, Fl+1)u as well as decoupling the ori-
entations of the terms from each other up to preserving their angle. Finally, note that Jl(θl, Fl+1)u
always lies in the left-null space of Jl(θl, Fl+1). But that space is orthogonal to fl(θl, Fl+1), and
hence the two terms are orthogonal as required.
If the weight matrix is Gaussian initialized, note that the product of a Gaussian initialized matrix
and an orthogonally initialized matrix is Gaussian initialized. Hence, we can insert an additional
orthogonally initialized matrix and then proceed with the previous argument to show that conditions
(4) and (5) are fulfilled.
After applying a linear transformation with one of the two initializations, conditions (4) and (5) hold
except for the length of fl is not 1. Hence, even if length-only layer normalization is not used as part
of the endomorphism, we expect (4) and (5) to hold approximately in practice.
As far as we can tell, conditions (2) and (3) are not fulfilled in practice. They are both used to derive
from unit determinants a greater than unit qm norm. As long this implications holds for practical
layer functions, (2) and (3) are not necessary.
F.3 Theorem 3 - skip connections reduce the gradient
Theorem 3. Let g and u be random vectors. Consider a function f that is k-diluted with respect to
u, a matrix S and a function ρ. Let R(v) be the Jacobian of ρ at input v. Let r :
QllgR(u)ll2QIIuII2
QuP(U)ll2QUgll2 '
52
Workshop track -ICLR 2018
QoQ/igS tQt∕7≠ IIT(Q11ʌ (Cr∙?八、—八 ∕7m∕/ ≠∕j∕7≠ IIT(CZτ?(1?八、QQQl∖	—八	QQQ1 QGQ/igS	QiQt	QUgSU2QUuU2 — ι
Assufme Ihal E(su).(p(u))— U ana Ihal e(g∕R(u))♦ (gs)	—	U∙	Also assumιe	Ihat	Q||SU11 Q||g|| — L
Then QIIgR(U)+gS||2QIIU口2 — ι + r-1 + o((r _ 1)2)
Then Q||p(u)+Su||2Q||g||2 —1十 k2 + 1 + OKr T))'
Proof. We have
—	Q∣∣gR(u) + gS∣∣2Q∣∣u∣∣2 QIlP(U)十 Su∣∣2Q∣∣g∣∣2 Q√∣∣ffR(u) + gS ∣∣2q∣∣u∣∣2 QpIIP(U) + su∣∣2QIIg∣∣2 √EIIgR(U) + gS∣∣2QIIu∣∣2 √EIIP(U) + Su∣∣2 Q∣∣g∣∣2 √E(gR(u) + gS).(gR(u) + gS)Q∣∣u∣∣2 √E(ρ(u) + Su).(ρ(u) + Su)Q ∣∣g∣∣2 √E[(gR(u)).(gR(U))+ 2(gS).(gR(U))+(gS).(gS)]QIIu∣∣2 √E[ρ(u).ρ(u) + 2(Su).ρ(u) + (Su).(Su)]Q ∣∣g∣∣2 √E[(gR(U)).(gR(U))+(gS).(gS)]QIIU∣∣2 √E[ρ(u).ρ(u) + (Su).(Su)]Q ∣∣g∣∣2 √(QI∣gR(U)∣∣2)2 + (Q∣∣gS ∣∣2)2Q∣∣U∣∣2 √(Q∣∣ρ(U)∣∣2)2 + (Q∣∣SU∣∣2)2Q∣∣g∣∣J “Q 口 % |U 2 Q u g u 2 )2 + (Q∣∣gs∣∣2)2qi∣u∣∣2 √(Q∣∣p(u)∣∣2)2 + (Q∣∣SU∣∣2)2Q∣∣g∣Γ~ √( rQ^QjS5^ )2 + (QIIgS I∣2)2QI∣U∣∣2 √( Q-⅞uh )2 + (QI∣su∣∣2)2Q∣∣g∣∣2 ，(rQ % | 啬 | 2g u 2 )2 + (QI∣g∣∣2)2QI∣u∣∣2 √(⅛⅜+≡⅛7QI∣g∣∣2 √i⅛ √k⅛+1 √k2 + r2 √k2 + 1 √k2 + (1 + (r - 1))2 √k2 + 1 √k2 + 1 + 2(r - 1) +(r - 1)2 √k2 + 1 J1+k⅛(r-1)+k⅛≠-1)2 1 + IrSI + °((r- 1)2) k2 +1
□
53
Workshop track - ICLR 2018
u represents the incoming activation vector of some residual block and g the incoming gradient.
represents a type of expectation over the ratio
GSC(b+1,0)
GSC(b,0)
||Jb0+1 ||
qm || fb+1 || 2
llfθ∣∣2	- - ||gb+1 ||2 f b+1 ||2
l∣J0llqmllfb∣∣2	=	l∣gb∣∣2fb∣∣2
Ilf0ll2
||gbR(fb+I) ||2 || fb+1 ||2 — ||gR(U) ll2lluU2 1 cynMfirin fhɑ ctiτ⅛ CrYnnQCf；rYn Tharafcra v mn hɑ \7；01170八 CIa
llgb∣∣21∣ρ(fb+1)∣∣2	= ∣∣ρ(u)∣∣2∣∣g∣∣2，ignoring the skip connection. Therefore r can be viewed as
the growth of the GSC. Similarly, QQgRuu+SS∣蜀gu∣lJ2 represents the growth of the GSC after the
skip connection has been added.
r
The key assumptions are E(Su).(ρ(u)) = 0 and E(gR(u)).(gS) = 0. In plain language, we
assume that the function computed by the skip connection is uncorrelated to the function computed
by the residual block and that the same is true for the gradient flowing through them. For the
forward direction, this is true if either the skip connection is Gaussian / orthogonally initialized
or the last layer of the residual block is linear and Gaussian / orthogonally initialized and if the
randomness of the initialization is absorbed into the expectation. Unfortunately, for the backward
direction, such a statement cannot be made because the gradient has a complex dependence both
on S and R. However, the assumption that this dependence between the forward and backward
direction is immaterial has proven realistic in mean field theory based studies (see section B.1.1).
Under this assumption, as in the forward direction, we require that either the skip connection is
Gaussian / orthogonally initialized or the first layer of the residual block is linear and Gaussian /
orthogonally initialized. Note that even if both assumptions are only fulfilled approximately, this is
not catastrophic to the theorem.
The other assumption is Q∣∣SU∣∣∣∣2Q∣∣l il∣u∣∣2 = 1. This is true if S is an orthogonal matrix and so specif-
ically if S is the identity matrix. If S is Gaussian / orthogonally initialized, this is true if the ran-
domness of the initialization is absorbed into the Q terms. Again, if this assumption only holds
approximately, it is not catastrophic to the theorem.
An implicit assumption made is that the distribution of the incoming gradient g is unaffected by the
addition of the skip connection, which is of course not quite true in practice. The addition of the
skip connection also has an indirect effect on the distribution and scale of the gradient as it flows
further towards the input layer.
The experiments in figure 3 bear out the theory discussed here.
G Taylor approximation of a neural network
We define the first-order Taylor approximation Tl of the bottom layers up to layer l recursively.
Write il(x) as the short form ofil(il+1(..iL(x)..)). Then
TL(θ, X)
Tl(θ, X)
fL(θL,X)
L
il (Tl+1 (θ, X)) + rl(θl, il+1 (X)) + X
k=l+1
drl(θl, il+1(X))
----7	----rk(θk,ik+ι(X)) for l<L
dX )
The maximum number of parametrized residual functions composed in Tl is 2. Otherwise, only
addition and composition with fixed functions is used. Hence, the compositional depth of Tl is
min(L - l, 2). Hence, the network fTaylor(l) := f0(y, f1(..fl-1(Tl(X))..)) has compositional depth
max(l + 1, L).
For ResNet architectures, as in section D.2, we divide each layer in the residual block into its initial
and residual function. Then the definition of the Taylor expansion remains as above, except a term
sl (Tm( X)) is added at each layer l where a skip connection, represented by skip function sl,
terminates. Tm is the Taylor expansion at the layer where the skip connection begins.
H Looks-linear initialization
The looks-linear initialization (‘LLI’) of ReLU MLPs achieves an approximate orthogonal initial
state. Consider a ReLU MLP with some number of linear layers and a ReLU layer between each
54
Workshop track - ICLR 2018
pair of linear layers. LLI initializes the weight matrix of the lowest linear layer differently from the
weight matrix of the highest linear layer and differently from the weight matrices of the intermediate
linear layers. Let a weight matrix W have dimension m*n, where n is the dimension of the incoming
vector and m is the dimension of the linear layer itself. Also, we require that the dimension of all
ReLU layers, and thus the dimension of all linear layers except the highest linear layer, is even. Then
the weight matrices are initialized as follows.
•	Lowest linear layer: Draw a uniformly random orthogonal matrix W0 of dimension
max(3,n) * max(3,n). Then, for all 1 ≤ i ≤ mm and 1 ≤ j ≤ n, set W(2i,j)=
max(P2n, 1)W0(i,j) and W(2i + 1,j) = - max(P1, 1)W0(i,j).
•	Highest linear layer: Draw a uniformly random orthogonal matrix W0 of dimension
max(m, 2) * max(m, 2). Then, for all 1 ≤ i ≤ m and 1 ≤ j ≤ 2, set W(i, 2j)=
max(y2nm, 1)W0(i,j) and W(i, 2j + 1)
2m
n
•	Intermediate linear layers: Draw a uniformly random orthogonal matrix W0 of dimen-
sion max(mm, n) * max(mm, n). Then, for all 1 ≤ i ≤ 节 and 1 ≤ j ≤ n,
set W (2i, 2j) = max(pn, 1)W 0(i,j), W (2i + 1, 2j) = - max(Pf, 1)W 0(i,j),
W (2i, 2j + 1) = - max(pn, 1)W 0(i,j) and W (2i + 1, 2j + 1)=max(P⅝, 1)W 0(i,j).
Under LLI, pairs of neighboring ReLU neurons are grouped together to effectively compute the
identity function. The incoming signal is split between ReLU neurons of even and odd indeces.
Each of the two groups preserves half the signal, which are then “stitched together” in the next
linear layer only to be re-divided in a different way to pass through the next ReLU layer.
LLI networks can be said to be approximately orthogonal. Let Xlr be the representation computed
by the r'th ReLU layer. Then let Xlr (i) = Xir (2i) 一 Xir (2i + 1) for 1 ≤ i ≤ d2r. Since Xιr(2i)
or Xlr (2i + 1) is 0, this transformation is bijective. Finally, the transformation from χlr to χlr-1 is
an orthogonal transformation.
I Experimental details
I.1	Architectures used
Vanilla networks without skip connections All networks are MLPs composed of only fully-
connected linear layers and unparametrized layers. The following types of layers are used.
•	linear layer: fi (θi, fi+1) = Wifi+1 where the entries of the weight matrix Wi are the
entries of the parameter sub-vector θi. Trainable bias parameters are not used.
•	ReLU layer: fi(fi+1) = σReLU.(fi+1), where the scalar function σReLU is applied element-
wise as indicated by .() We have σReLU(a) = a if a ≥ 0 and σReLU(a) = 0 if a < 0.
•	tanh layer: fi(fi+1) = σtanh.(fi+1), where σtanh(a) = tanh(a).
•	SeLU layer: fi(fi+1) = σSeLU.(fi+1). We have σSeLU(a) = cposa ifa ≥ 0 andσSeLU(a) =
1.0507 and cneg = 1.0507 * 1.6733 as suggested by
cneg(ea - 1) if a < 0. We set cpos =
Klambauer et al. (2017).
• batch normalization layer: fi(fi+1)
力+；-*, where μ is the component-wise mean of
fi+1 over the current batch and σ is the componentwise standard deviation of fi+1 over the
current batch.
•	layer normalization layer: f (力+ι) = f+σ-μ, where μ is mean of the entries of 力+ι and
σ is the standard deviation of the entries of fi+1.
•	length-only layer normalization layer: fι(fι+ι) = flm, where qm is the quadratic mean
of the entries of fi+1.
•	dot product error layer: f0(f1,y) = f1.y
•	SOftmaxIayer: fifi+i)(i) = Pfe++l)j)
55
Workshop track - ICLR 2018
•	kl error layer: f0(f1, y) = ln f1 (y) where y is an integer class label and f1 has one entry
per class.
Note that normalization layers (batch normalization, layer normalization or length-only layer nor-
malization) do not use trainable bias and variance parameters.
A network of compositional depth N contains N linear layers and N - 1 nonlinearity layers (ReLU,
tanh or SeLU) inserted between those linear layers. If the network uses normalization layers, one
normalization layer is inserted after each linear layer. For Gaussian noise experiments, the error
layer is the dot product error layer. For CIFAR10 experiments, a softmax layer is inserted above the
last linear or normalization layer and the error layer is a kl error layer.
For Gaussian noise experiments, data inputs as well as predictions and labels have dimension 100.
We used a compositional depth of 50. We generally used a uniform width of 100 throughout the
network. However, we also ran experiments where the width of all layers from the first linear layer
to the layer before the last linear layer had width 200. We also ran experiments where linear layers
alternated in width between 200 and 100. For CIFAR10 experiments, data inputs have dimension
3072 and predictions have dimension 10. We use a compositional depth of 51. The first linear layer
transforms the width to 100 and the last linear layer transformed the width to 10.
The following initialization schemes for the weight matrices are used.
•	Gaussian: Each entry of the weight matrix is drawn as an independent Gaussian with mean
0. The variance of this Gaussian is one over the dimension of the incoming vector except
when the weight matrix follows a ReLU layer. In that case, the variance of the Gaussian is
two over the dimension of the incoming vector.
•	orthogonal: The weight matrix is a uniformly random orthogonal matrix. Note that this
initialization scheme is only used for square matrices.
•	looks-linear: See section H.
ResNet In all cases, the first layer is a linear layer. After that, there are 25 skip connections. Each
skip connection bypasses a block of 6 layers: a normalization layer, a nonlinearity layer, a linear
layer, another normalization layer, another nonlinearity layer, and another linear layer. Above the
last skip connection, a final normalization layer is inserted, followed by softmax (CIFAR10 only)
and then the error layer. For Gaussian noise experiments, we use a constant width of 100. For
CIFAR10, the first linear layer transforms the width from 3072 to 100, and the last skip connection
as well as the last linear linear in the last residual block transform the width from 100 to 10.
Skip connections are identity skip connections, except the last skip connection in CIFAR10 experi-
ments that is responsible for reducing the width. There, the skip connection multiplies its incoming
value by a fixed 10 * 100 submatrix of a 100 * 100 orthogonal matrix. For Gaussian noise experi-
ments, we also conducted some experiments where skip connections used random matrices where
each entry is drawn from an independent Gaussian with mean 0 and the variance being one over the
dimension of the incoming vector.
I.2	Protocol for Gaussian noise experiments
For Gaussian noise experiments, both inputs and labels are 100-dimensional vectors were each entry
is drawn from an independent Gaussian with mean 0 and variance 忐. We normalized the input
vectors to have length 10. We drew 100 independent datasets of size 10.000.
For each dataset and each architecture we studied (see table 1 for the full list), we computed both the
forward activations and the gradient for each datapoint. For architectures with batch normalization,
all 10.000 datapoints were considered part of a single batch. Note that no training was conducted.
We then computed the following metrics:
Expected GSC: At each layer l, we computed
the “regular gradient” of the network.
QD JOuqmQD fl||2
QD f0||2
. Note that Jl0 is simply
•	Pre-activation standard deviation: For each nonlinearity layer l, we computed the stan-
dard deviation of the activations of each neuron in fl+1 over the 10.000 datapoints, i.e.
56
Workshop track - ICLR 2018
(EDfl+1(i)2) - (EDfl+1(i))2 for all 1 ≤ i ≤ dl+1. We then computed the quadratic mean
of those standard deviations as a summary statistic for the layer.
•	Pre-activation sign diversity: For each nonlinearity layer l, at each neuron in fl+1, we
computed min(pos, 1 - pos), where pos is the fraction of activations that were positive
across the 10.000 datapoints. We then computed the mean of those values across the layer
as a summary statistic.
Finally, we obtained a summary statistic for each layer and architecture by averaging the results over
the 100 datasets. Results are shown in table 1, figure 1 and figure 3.
I.3	Protocol for CIFAR 1 0 experiments
For CIFAR10 experiments, we preprocessed each feature to have zero mean and unit variance. We
used the training set of 50.000 datapoints and disregarded the test set. We used batches of size 1.000
except for the vanilla batch-ReLU architecture with Gaussian initialization, for which we used a
batch size of 50.000. (See section 4.5 for the explanation.)
We trained each architecture we studied (see table 2 for the full list) with SGD in two ways. First,
with a single step size for all layers. Second, with a custom step size for each layer.
Single step size We perform a grid search over the following starting step sizes:
{1e5, 3e4, 1e4, 3e3, .., 1e - 4, 3e - 5, 1e - 5}. For each of those 21 starting step sizes, we train
the network until the end-of-epoch training classification error has not decreased for 5 consecutive
epochs. Once that point is reached, the step size is divided by 3 and training continues. Once the
end-of-epoch training classification error has again not decreased for 5 epochs, the step size is di-
vided by 3 again. This process is repeated until training terminates. Termination occurs either after
500 epochs or after the step size is divided 11 times, whichever comes first. The starting step size
that obtains the lowest final training classification error is selected as the representative step size for
which results are presented in the paper.
Custom step sizes In this scenario, we use a different starting step size for each layer. After those
step sizes are computed, smoothed and scaled as described in section I.4, we train the pre-trained
network with those step sizes. As before, periodically, we divide all step sizes jointly by 3. As
before, training is terminated after 11 divisions or when 500 epochs are reached, whichever comes
first.
We compute the following metrics:
•	Largest relative update size for each layer induced by the estimated optimal step size during
the epoch where that optimal step size was estimated. See section I.4 for details.
•	Effective depth throughout training: see section D.2 for details. λ-contributions are accu-
mulated from batch to batch.
•	Training classification error at the end of each epoch.
•	Training classification error when compositional depth is reduced via Taylor expansion
after training: see section G for details.
•	GSC, pre-activation standard deviation and pre-activation sign diversity: for details, see the
end of section I.2. Note that the expectations over the dataset were computed by maintain-
ing exponential running averages across batches.
•	Operator norms of residual weight matrices after training.
See table 2 and figures 2, 4, 5 and 6 for results.
I.4	Selecting custom step sizes
We estimated the optimal step size for each linear layer under SGD for our CIFAR10 experiments.
This turned out to be more difficult than expected. In the following, we describe the algorithm we
used. It has five stages.
57
Workshop track - ICLR 2018
Pre-training We started by pre-training the network. We selected a set of linear layers in the
network that we suspected would require similar step sizes. In exploding architectures (vanilla
batch-ReLU with Gaussian initialization, vanilla layer-tanh, vanilla batch-tanh, SeLU), we chose
the second highest linear layer through the sixth highest linear layer for pretraining, i.e. 5 linear
layers in total. We expected these layers to require a similar step size because they are close to
the output and the weight matrices have the same dimensionality. For vanilla ReLU, vanilla layer-
ReLU, vanilla tanh and looks-linear initialization, we chose the second lowest linear layer through
the second highest linear layer (i.e. 49 linear layers in total) because the weight matrices have the
same dimensionality. Finally, for ResNet, we chose the second lowest through the third highest
linear layer (i.e. 48 linear layers in total), because the blocks those layers are in have the same
dimensionality.
We then trained those layers with a step size that did not cause a single relative update size of more
than 0.01 (exploding architectures) or 0.001 (other architectures) for any of the pre-trained layers or
any batch. We chose small step sizes for pre-training to ensure that pre-training would not impact
effective depth. We pre-trained until the training classification error reached 85%, but at least for one
epoch and at most for 10 epochs. The exact pre-training step size was chosen via grid search over
a grid with multiplicative spacing of 3. The step size chosen was based on which step size reached
the 85% threshold the fastest. Ties were broken by which step size achieved the lowest error.
Selection In the selection phase, we train each linear layer one after the other for one epoch while
freezing the other layers. After each layer is trained, the change to the parameter caused by that
epoch of training is undone before the next layer is trained. For each layer, we chose a step size
via grid search over a grid with multiplicative spacing 1.5. The step size that achieved the lowest
training classification error after the epoch was selected. Only step sizes that did not cause relative
update sizes of 0.1 or higher were considered, to prevent weight instability.
Now we can explain the need for pre-training. Without pre-training, the selection phase yields very
noisy and seemingly random outcomes for many architectures. This is because it was often best to
use a large step size to jump from one random point in parameter space to the next, hoping to hit a
configuration at the end of the epoch where the error was, say, 88%. Since we used a tight spacing
of step sizes, for most layers, there was at least one excessively large step size that achieved this
spurious “success”. Since we only trained a single layer out of 51 for a single epoch, the error of the
“correct” step size after pre-training often did not reach, say, 88%. When we trained the network for
500 epochs with those noisy estimates, we obtained very high end-of-training errors.
Pre-training ensures that training with an excessively high step size causes the error to exceed 85%
again. Therefore, those step sizes are punished and step sizes that ultimately lead to a much better
end-of-training error are selected.
Clipping Even though pre-training was used, for some architectures, it was still beneficial to add
the following restriction: as we consider larger and larger step sizes during grid search, as soon as
we find a step size for which the error is at least 0.1% higher than for the current best step size, the
search is terminated. Clipping is capable of further eliminating outliers and was used if and only it
improved the end-of-training error. It was used for vanilla tanh, ResNet layer-tanh and looks-linear
layer-ReLU.
For each linear layer, the largest relative update size induced by the step size obtained for that layer
after the clipping phase (or after the selection phase if clipping was not used) during the epoch of
training conducted in the selection phase is shown in the in figures 2A, 4A, 5A and 6A.
Smoothing In this stage, we built a mini-regression dataset of (X, Y ) points as follows. For each
X from 1 to 51, we include the point (X, Y ) where Y is the largest relative update size the step size
selected for linear layer X after clipping induced during the epoch of training in the selection phase.
We then fit a line via least-squares regression on that dataset in log scale. For each X, we thus obtain
a smoothed value Y0. The ratio 专 was multiplied to the step size obtained for each layer at the end
of the clipping phase.
We added this phase because we found that the end-of-training error could still be significantly
improved by reducing noise among the layer-wise step sizes in this way.
58
Workshop track - ICLR 2018
Scaling Finally, we jointly scale all layer-wise step sizes with a single constant. That value is
chosen as in the selection phase by trying a small constant, training for one epoch, rewinding that
epoch, multiplying that constant by 1.5, rinse, repeat. Again, that process was terminated once any
layer experiences an update of relative size at least 0.1. This stage is necessary because the size of
the update on the entire parameter vector when all layers are trained jointly is ≈ 5lti times larger
than when only single layers are trained as in the selection phase. Hence, a scaling constant less than
1 is usually needed to compensate. Again, some architectures benefited from using clipping, where
we terminated the scaling constant search as soon as one exhibited an error more than 0.1% above
the current best scaling constant. Vanilla tanh, vanilla layer-tanh, ResNet layer-tanh and looks-linear
layer-ReLU used this clipping.
Formally, for each architecture, we trained three networks to completion. One using no clipping,
one using only clipping during the scaling phase, and using the clipping phase as well as clipping
during the scaling phase. Whichever of these three networks had the lowest end-of-training error
was selected for presentation in the paper. To compare, for single step size training, we compared
21 end-of-training error values.
59