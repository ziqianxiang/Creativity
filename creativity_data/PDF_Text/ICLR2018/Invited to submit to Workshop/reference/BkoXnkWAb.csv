title,year,conference
 Natural gradient works efficiently in learning,1998, Neural Computation
 Layer normalization,2016, CoRR
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE Trans
 Hierarchical multiscale recurrent neural networks,2016, CoRR
 Recurrent batch normalization,2016, CoRR
 Finding structure in time,1990, Cognitive science
 Generating sequences with recurrent neural networks,2013, arXiv preprint arXiv:1308
 Deep residual learning for image recognition,2015, CoRR
 Long short-term memory,1997, Neural COmPUtatiOn
 Learning simpler language models with the deltarecurrent neural network framework,2017, CoRR
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In Proceedings of the 32nd International Conference on Machine Learning
 Adam: A method for stochastic optimization,2014, CoRR
 MultiPlicative LSTM for sequence modelling,2016, CoRR
 Learning multiPle layers of features from tiny images,2009, Masterâ€™s thesis
 Regularizing rnns by stabilizing activations,2015, CoRR
 Zoneout: Regularizing rnns byrandomly Preserving hidden activations,2016, CoRR
 Batch normalized recurrentneural networks,2016, In 2016 IEEE International Conference on Acoustics
 A simple way to initialize recurrent networks of rectifiedlinear units,2015, CoRR
 Eigenvalues of covariance matrices: Application to neural-networklearning,1991, Physical Review Letters
 Efficient backprop,1998, In Neuralnetworks: Tricks of the trade
 Rectifier nonlinearities improve neural network acousticmodels,2013, In Proc
 Building a large annotated corpus ofenglish: The penn treebank,1993, Computational linguistics
 All you need is a good init,2015, CoRR
 Rnndrop: A novel dropout for rnns in asr,2015, InAutomatic Speech Recognition and Understanding (ASRU)
 Path-normalized optimization ofrecurrent neural networks with relu activations,2016, In D
 How to construct deep recurrentneural networks,2013, CoRR
 Dropout improves recurrent neural networks forhandwriting recognition,2013, CoRR
 Weight normalization: A simple reparameterization to accelerate trainingof deep neural networks,2016, In D
 Recurrent dropout without memory loss,2016, CoRR
 Understanding and improving convolutionalneural networks via concatenated rectified linear units,2016, CoRR
 On multiplicativeintegration with recurrent neural networks,2016, CoRR
 Empirical evaluation of rectified activations in convolutionalnetwork,2015, CoRR
 Wide residual networks,2016, CoRR
 Recurrent neural network regularization,2014, CoRR
 Oriented response networks,2017, CoRR
