title,year,conference
 Understanding intermediate layers using linear classifier probes,2016, In ICLRWorkshop
 Learning polynomials with neural networks,2014, InICML
 Exponentially many local minima for single neurons,1996, InNIPS
 Neural networks and principle component analysis: Learning from exampleswithout local minima,1988, Neural Networks
 Training a 3-node neural network is np-complete,1989, In NIPS
 The loss surfaces of multilayernetworks,2015, In AISTATS
 Open problem: The landscape of the loss surfaces ofmultilayer networks,2015, COLT
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 Under-standing synthetic gradients and decoupled neural interfaces,2017, In ICML
 Identifying and attackingthe saddle point problem in high-dimensional non-convex optimization,2014, In NIPS
 Topology and geometry of half-rectified network optimization,2017, InICLR
 Globally optimal training of generalized polynomial neuralnetworks with nonlinear spectral methods,2016, In NIPS
 Qualitatively characterizing neural network optimiza-tion problems,2015, In ICLR
 Identity matters in deep learning,2017, In ICLR
 Deep residual learning for image recognition,2016, In CVPR
 Multilayer feedforward networks are universal approxi-mators,1989, Neural Networks
 Beating the perils of non-convexity: Guaranteedtraining of neural networks using tensor methods,2016, arXiv:1506
 Deep learning without poor local minima,2016, In NIPS
 Adam: A method for stochastic optimization,2015, In ICLR
 A Primer of Real Analytic Functions,2002, Birkhauser
 Imagenet classification with deep convolutionalneural networks,2012, In NIPS
 Hand-written digit recognition with a back-propagation network,1990, In NIPS
 On the computational efficiency of training neuralnetworks,2014, In NIPS
 Understanding deep image representations by inverting them,2015, InCVPR
 Deep vs,2016, shallow networks : An approximation theory perspective
 On the number of linear regions of deep neuralnetworks,2014, In NIPS
 The loss surface of deep and wide neural networks,2017, In ICML
 On the number of response regions of deep feedforwardnetworks with piecewise linear activations,2014, In ICLR
 Numerical Recipes 3rd edition: The art of scientific computing,2007, Cambridge UniversityPress
 On the quality of the initial basin in overspecified networks,2016, In ICML
 Depth-width tradeoffs in approximating natural functions with neuralnetworks,2017, In ICML
 Provable methods for training neural networks with sparse connec-tivity,2015, In ICLR Workshop
 Failures of gradient-based deep learning,2017, In ICML
 Training a single sigmoidal neuron is hard,2002, Neural Computation
 Very deep convolutional networks for large-scale image recogni-tion,2015, In ICLR
 Going deeper with convolutions,2015, In CVPR
 Benefits of depth in neural networks,2016, In COLT
 An analytical formula of population gradient for two-layered relu network and its applica-tions in convergence and critical point analysis,2017, In ICML
 Understanding neural networks throughdeep visualization,2015, In ICML
 Visualizing and understanding convolutional networks,2014, In ECCV
 Understanding deep learning requiresre-thinking generalization,2017, In ICLR
 Recovery guarantees for one-hidden-layerneural networks,2017, In ICML
3To prove Lemma 4,2017,3
