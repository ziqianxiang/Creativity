title,year,conference
 Random matrices and complexity of SPinglasses,2013, Communications on Pure and Applied Mathematics
 Phase transition of the largest eigenvalue fornonnull complex sample covariance matrices,2005, The Annals of Probability
 Energy landscapes for machine learning,2017, Physical ChemistryChemical Physics
 On the principal components ofsample covariance matrices,2016, Probability Theory and Related Fields
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTATâ€™2010
 Parallelization of a neural network learning algorithm on a hypercube,1989, Hypercube anddistributed computers
 Entropy-sgd: Biasing gradient descentinto wide valleys,2016, arXiv preprint arXiv:1611
 Sharp minima can generalize fordeep nets,2017, arXiv preprint arXiv:1703
 Topology and geometry of deep rectified network optimizationlandscapes,2016, arXiv preprint arXiv:1611
 Flat minima,1997, Neural Computation
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Efficient backprop,1998, Lecture notes in computerscience
 Gradient descent convergesto minimizers,2016, University of California
 Gradient descent only converges to minimizers: Non-isolated critical points and invariant regions,2016, arXiv preprint arXiv:1605
 Fast exact multiplication by the hessian,1994, Neural computation
 Explorations on high dimensionallandscapes,2014, ICLR 2015 Workshop Contribution
 Singularity of the hessian in deep learning,2016, arXivpreprint arXiv:1611
 No more pesky learning rates,2013, ICML (3)
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
