title,year,conference
 A closer lookat memorization in deep networks,2017, arXiv preprint arXiv:1706
 Understanding dropout,2013, In Advances in Neural InformationProcessing Systems
 Spectrally-normalized margin bounds for neuralnetworks,2017, Technical report
 Weight uncertainty inneural networks,2015, arXiv preprint arXiv:1505
 Parsevalnetworks: Improving robUstness to adversarial examples,2017, In International Conference on MachineLearning
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations
 Towards deep neUral network architectUres robUst to adversarialexamples,2014, arXiv preprint arXiv:1412
 Distilling the knowledge in a neUral network,2014, InNIPS Deep Learning and Representation Learning Workshop
 On the method of boUnded differences,1989, Surveys in combinatorics
 Exploring generalization in deep learn-ing,2017, In Advances in Neural Information Processing Systems
 Distillation as adefense to adversarial pertUrbations against deep neUral networks,2016, In Security and Privacy (SP)
 Understanding adversarial training: Increasinglocal stability of neUral nets throUgh robUst optimization,2015, arXiv preprint arXiv:1511
 Ensemble meth-ods as a defense to adversarial perturbations against deep neural networks,2017, arXiv preprintarXiv:1709
 Intriguing properties of neural networks,2014, In International Conference onLearning Representations
 Dropout training as adaptive regularization,2013, InAdvances in Neural Information Processing Systems
 Robustness and generalization,2012, Machine learning
 Robust regression and lasso,2009, In Advances inNeural Information Processing Systems
 Understandingdeep learning requires rethinking generalization,2017, ICLR17
 Consider an L-layer neural network trained bydropout,2014, Let A be an algorithm with (K
 Let Ni be the set of index of points of S that fall into the Ci,2018, Note that (∣N1∣
