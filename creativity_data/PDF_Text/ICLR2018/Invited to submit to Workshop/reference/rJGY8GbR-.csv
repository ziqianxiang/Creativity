title,year,conference
 A Learning Algorithm forBoltzmann Machines*,1551, Cognitive Science
 Natural Gradient Works Efficiently in Learning,1530, Neural Computation
 Information geometry and its applications,2016, Number volume 194 in Appliedmathematical sciences
 At the Edge of Chaos:Real-time Computations and Self-Organized Criticality in Recurrent Neural Networks,2005, InL
 Kernel methods for deep learning,2009, In Advances in neuralinformation processing systems
 Open Problem: The landscape of the losssurfaces of multilayer networks,2015, In COLT
 Natural NeuralNetworks,2015, In C
 Understanding the difficulty of training deep feedforwardneural networks,2010, In PMLR
 A Kronecker-factored approximate Fisher matrix for convolutionlayers,2016, In PMLR
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Optimizing Neural Networks with Kronecker-factored Approxi-mate Curvature,1503, arXiv:1503
 Geometry of Neural Network Loss Surfaces via RandomMatrix Theory,2017, In Doina Precup and Yee Whye Teh
 Nonlinear random matrix theory for deep learn-ing,2017, In I
 Resurrecting the sigmoidin deep learning through dynamical isometry: theory and practice,2017, In I
 Ex-ponential expressivity in deep neural networks through transient chaos,2016, In Advances In NeuralInformation Processing Systems
 Deep InformationPropagation,2017, 2017
 Scalable trust-region methodfor deep reinforcement learning using Kronecker-factored approximation,1708, arXiv:1708
 Meanfield Residual Network: On the Edge of Chaos,2017, InAdvances in neural information processing systems
