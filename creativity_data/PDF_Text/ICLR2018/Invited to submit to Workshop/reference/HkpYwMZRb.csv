title,year,conference
 Multi-level residual networks from dynamical systems view,2018, In ICLR (under review)
 Orthogonal recurrent neural networks with scaled cayley transform,2018, In ICLR (underreview)
 Training and inference with integers in deep neural networks,2018, In ICLR (under review)
 Deep mean field theory: Variance and width variation by layer as methods to controlgradient explosion,2018, In ICLR (under review)
 Unitary evolution recurrent neural networks,2016, InICML
 Normalization propagation:A parametric technique for removing internal covariate shift in deep networks,2016, In ICML
 Layer normalization,2016, 2016
 On the complexity of shallow and deep neural networkclassifiers,2014, In ESANN
 On the propertiesof neural machine translation: Encoder-decoder approaches,2014, arXiv preprint arXiv:1409
 Cosine normalization: Using cosinesimilarity instead of dot product in neural networks,2017, arXiv preprint arXiv:1702
 Understanding the difficulty of training deep feedforward neuralnetworks,2015, In ICCV
 Stable architectures for deep neural networks,2017, arXiv preprintarXiv:1705
 Learning across scales - multiscale methodsfor convolution neural networks,2017, arXiv preprint arXiv:1703
 Deep pyramidal residual networks,2017, In CVPR
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In ICCV
 Identity mappings in deep residual net-works,2016, arXiv preprint arXiv:1603
 Deep residual learning for image recog-nition,2016, In CVPR
 Untersuchungen zu dynamischen neuronalen netzen,1991, Masters thesis
 Long short-term memory,1997, Neural Computation
 Deep networks withstochastic dePth,2016, In ECCV
 Densely connectedconvolutional networks,2017, In CVPR
 Batch renormalization: Towards reducing minibatch dePendence in batch-normalizedmodels,2017, In NIPS
 Batch normalization: Accelerating deeP network training byreducing internal covariate shift,2015, In ICML
 On large-batch training for deeP learning - generalization gaP for sharP minima,2017, In ICLR
 Adam: A method for stochastic oPtimization,2015, In ICLR
 Self-normalizingneural networks,2017, In NIPS
 Learning deep architectures via generalized whitened neural networks,2017, In ICML
 On the representational ef-ficiency of restricted boltzmann machines,2013, In NIPS
 Deep vs,2016, shallow networks: An approximation theoryperspective
 All you need is a good init,2016, In ICLR
 On the number of linearregions of deep neural networks,2014, In NIPS
 On the difficulty of training recurrent neuralnetworks,2013, In ICML
 Expo-nential expressivity in deep neural networks through transient chaos,2016, In NIPS
 Weight normalization: A simple reparametrization to ac-celerate training of deep neural networks,2016, In NIPS
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural networks,2014, arXiv preprint arXiv:1312
 No more pesky learning rates,2013, In ICML
 Deep informationpropagation,2017, In ICLR
 Representation benefits of deep feedforward networks,2015, arXiv preprintarXiv:1509
 Residual networks behave like ensembles of rel-atively shallow networks,2016, In NIPS
 Mean field residual networks: On theedge of chaos,2017, In NIPS
 Diracnets: Training very deep neural networks withoutskip-connections,2017, arXiv preprint arXiv:1706
 Wide residual networks,2016, In BMVC
 Let U be the uniform distribution over the unit hypersphere,2018, As before
