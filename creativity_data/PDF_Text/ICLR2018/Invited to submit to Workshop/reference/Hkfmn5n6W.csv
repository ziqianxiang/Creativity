title,year,conference
 Identifiability of parameters in latent structuremodels with many observed variables,0090, Annals ofStatistics
 Learning Polynomials with Neural Networks,2014, In ICML
 Linear Learning: Landscapes and Algorithms,1989, Advances in Neural Information Processing Systems1
 The Loss Surfaces ofMultilayer Networks,2015, AISTATS15
 Geometrical and statistical properties of systems of linear inequalities with applications in patternrecognition,1965, Electronic Computers
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 Ensemble Robustness of Deep LearningAlgorithms,2016, ArXiv
 Topology and Geometry of Deep Rectified Network Optimization Landsca-pes,2016, ArXiv: 1611
 Qualitatively characterizing neural network optimizationproblems,2015, In ICLR
 On the problem of local minima in backpropagation,0162, IEEE Transactions onPattern Analysis and Machine Intelligence
 Identity Matters in Deep Learning,2017, ICLR
 Deep Residual Learning for Image Recognition,2016, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,2015, In Proceedings of the IEEE International Conference onComputer Vision
 Approximation capabilities of multilayer feedforward networks,1991, Neural networks
 Beating the Perils of Non-Convexity: Guaranteed Training ofNeural Networks using Tensor Methods,2015, ArXiv:1506
 Deep Learning without Poor Local Minima,2016, In NIPS
 Adam: A Method for Stochastic Optimization,2014, arXiv preprintarXiv:1412
 Deep learning,2015, Nature
 Gradient Descent Converges toMinimizers,2016, Conference on Learning Theory
 Convergence Analysis of Two-layer Neural Networks with ReLU Activation,2017, arXiv
 Depth Creates No Bad Local Minima,2017, ArXiv
 Rectifier nonlinearities improve neural network acousticmodels,2013, In ICML Workshop on Deep Learning for Audio
 The loss surface of deep and wide neural networks,2017, Arxiv
 Nonconvergence to unstable points in urn models and stochastic approximations,1990, The Annals ofProbability
 Geometry of Neural Network Loss Surfaces via Random MatrixTheory,2017, Proceedings of the 34th International Conference on Machine Learning
 Exponential expressivityin deep neural networks through transient chaos,2016, In NIPS
 Non-asymptotic Theory of Random Matrices: Extreme SingularValues,2010, Proceedings of the International Congress of Mathematicians
 On the Quality of the Initial Basin in Overspecified Neural Networks,2016, In ICML
 Exact solutions to the nonlinear dynamics of learning in deep linearneural networks,2014, ICLR
 Distribution Specific Hardness of Learning Neural Networks,2016, arXiv preprint arXiv:1609
 Designing and Training Feedforward Neural Networks: A Smooth Optimisation Perspective,2016, ArXiv
 Training a single sigmoidal neuron is hard,2002, Neural computation
 Theoretical insights into the optimization landscapeof over-parameterized shallow neural networks,2017, arXiv
 No bad local minima: Data independent training error guarantees for multilayerneural networks,2016, In arXiv:1605
 Local minima in training of deepnetworks,2016, arXiv:1611
 Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks withReLU nonlinearity,2017, Submitted to ICLR
 Diversity Leads to Generalization in Neural Networks,2016, pp
 Can Backpropagation Error Surface Not Have Local Minima,1941, IEEE Transactions on NeuralNetworks
 Understanding deep learningrequires rethinking generalization,2017, In ICLR
 Recovery Guaranteesfor One-hidden-layer Neural Networks,2017, ICML
 The Landscape of Deep Learning Algorithms,2017, may 2017
