Figure 1: Training losses of (a) straightforward and (b) large margin formulationFigure 2: Histogram of log p(xi) - logp(xi,j), the margin between the log-likelihoods of ground-truth sentences and (erroneous) beam candidates. The more positive, the more the discrimination.
Figure 2: Histogram of log p(xi) - logp(xi,j), the margin between the log-likelihoods of ground-truth sentences and (erroneous) beam candidates. The more positive, the more the discrimination.
Figure 3: Architecture of baseline-LM4.1	Experimental ProtocolThe experimental setup is as follows. First we train an ASR/SMT model on the training set. Then weextract B beam candidates for every training sample. This beam set, together with the correspond-ing ground-truth text, are used as the training data for LMLM and rank-LMLM. We then re-scorethe beams by linearly combining ASR/SMT and language model scores. The combination weight5Under review as a conference paper at ICLR 2018is found by optimizing the WER/BLEU on the dev set. Finally, WER/BLEU on the test set arereported.
Figure 4: Compare the training of LMLM wither with or without warm startingFigure 4a compares the learning curves for both cases. With warm starting, training and dev lossesstart to decay from smaller values, approaching zero at convergence. The stronger generalizationability is further illustrated in Figure 4b. Figure 4b shows the histogram of the margins betweenthe scores for the ground-truths and beam candidates in the dev set. The more positive the marginis, the more the separation between positive and negative examples. In the warm-started case, thedistribution is shifted rightwards, indicating more discrimination.
