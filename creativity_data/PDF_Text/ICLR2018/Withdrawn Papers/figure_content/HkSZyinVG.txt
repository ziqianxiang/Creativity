Figure 1: Soft histogram decomposed into rectangular and linear basis functions.
Figure 2: Weighted sum examples, ReLU activation (left) and LReLU activation (right).
Figure 3: Bias levels for convolutional layers and shifts for activation function. Biases/shifts caneither be shared (large squares) or individual for each neuron (small squares).
Figure 4:	Initialization (red) and 20, 50 and 80 percentiles for tuned activation functions in last layer;ReLU (left), LReLU (middle) and ELU (right).
Figure 5:	Training (dashed) and test (solid) errors on Cifar-100 with network Lenet (left). Test errors(final part) for ELU, SvELU and ShELU (middle), and ELU, PELU and PShELU (right).
Figure 6:	Training (dashed) and test (solid) errors on Cifar-100 with network Clevert-11 (left). Testerrors (final part) for ELU, SvELU and ShELU (middle), and ELU, PELU and PShELU (right).
Figure 7:	Learned shifts for ShELU activation function, relative frequency (left), kurtosis (middle)and spatial variation (right).
