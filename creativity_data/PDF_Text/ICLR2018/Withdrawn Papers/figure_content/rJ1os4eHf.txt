Figure 1: Steps of the adaptive sparsity algorithm: (a) sample active weights, (b) compute deacti-vating probability and identify pruning targets, (c) apply deactivation, (d) sample inactive weightsand compute their gradients, (e) calculate activation probability and identify initialization targets, (f)initialize new weights.
Figure 2: Results for best choices of α and the adaptive threshold k . Controlling for number ofparameters, adaptive weight sparsity yielded the best results across all models. Note that the LSTMmodel employed early stopping. Best viewed in color.
Figure 3: A representative example of training anetwork from scratch using the connection patternlearned by adaptive sparsity, with log α = -0.5, k =0.25. On all models, this experiment yielded less im-provement over random sparsity than full adaptivity,and sometimes none at all. Best viewed in color.
Figure 4: Number of low-strength weights(relative to mean) over time on a represen-tative layer of the multi-layer perceptronmodel. For the non-baselines log α = -2.
Figure 5: Evolution of a variety of graph metrics during the training of an LSTM model with weightadaptivity. Best viewed in color.
