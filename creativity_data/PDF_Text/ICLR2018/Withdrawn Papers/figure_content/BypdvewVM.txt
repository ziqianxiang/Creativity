Figure 1: In BP and DTP, the final layer target is used to compute a loss, and the gradients from thisloss are shuttled backwards (through all layers, in BP, or just one layer, in DTP) in error propagationsteps that do not influence actual neural activity. SDTP never transports gradients using errorpropagation steps, unlike DTP and BP.
Figure 2: Classification error on CIFAR dataset. Train and test errors are drawn using solid and(b) LC network, no augmentation.
Figure 3: Top-1 test error on ImageNet.
Figure 4: Classification error on MNIST dataset. Train and test errors are drawn using solid anddashed lines correspondingly.
Figure 5: Train and test errors (%) for a FC network trained with noise-preserving inverse loss onaugmented CIFAR dataset.
