Figure 1: Mutual information between two bivariate Gaussians with component-wise correlation ofcorr(Xa,Xb) = ρ ∈ [-0.99, -0.9, -0.7, -0.5, -0.3, -0.1, 0., 0.1, 0.3, 0.5, 0.7, 0.9, 0.99].
Figure 2: Estimates of the mutual information as a function of the number of iterations.
Figure 3: The generator of the GAN model without mutual information maximization suffers frommode collapse (has poor coverage of the target dataset). In addition to the the GAN objective,MINEGAN maximizes the mutual information I (G(Z); Z). The MINEGAN generator learns adistribution with a high amount of structured noise. In addition, MINEGAN converges faster, showsbetter coverage of the ground truth distribution, as well as less mode dropping.
Figure 4: Kernel density estimate (KDE) plots for MINEGAN samples and GAN samples on 25Gaussians dataset. It is evident from the plot that again MINE does a decent job of capturing all themodes of the distribution while the standard GAN drops quite a few.
Figure 5: Reconstructions, samples, and embeddings from adversarially learned inference (ALI) andvariations intended to increase the mutual information. Shown left to right are the baseline (ALI),ALICE with the L2 loss to minimize the reconstruction error, ALI with an additional adversarialloss, and MINE. Top to bottom are the reconstructions, samples from the prior, and the embeddings.
