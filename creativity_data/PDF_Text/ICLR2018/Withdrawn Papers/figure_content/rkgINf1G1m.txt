Figure 1: (a) Features extracted by CNN with softmax loss are separated well into two classes by aline (hyperplanes in high-dimensional spaces), but lots of orange samples are included in a neighbor-hood (the blue circle) of a blue sample. (b) The performance of agglomerative clustering Beeferman& Berger (2000) on these features is poor.
Figure 2: Visualization of MNIST. (a) Features of 1000 MNIST samples obtained with softmaxloss. Different colors represent different classes. (b) Normalize 2-D features to a circle. Bestviewed in color. (c) Schematic illustration of our method. We enforce a restriction on features in theapproximately orthogonal direction from that of softmax to reshape the feature distribution.
Figure 3: Feature distribution of MNIST. 10 different colors represent 10 different classes of digits0-9. (a) and (b) Combination of isotropic loss and softmax loss with relatively small α supervise thenetwork to extract better features. (c) Isotropic loss dominates the training and softmax loss losesits role due to a large α.
Figure 4: Illustrative comparison of isomax loss and center loss. When we retrain networks withisomax loss, the overall distribution of test samples is robust and stable. For center loss, differenttraining batches form different centers, and finally result in different global distributions.
Figure 5: Accuracy curves of CASIA-WebFace testing set. The boost at 32K step is because of thedecay of learning rate. (a): Softmax classifier accuracy relative to training steps. (b): k-NN (k=5)classification accuracy.
Figure 6: Values of center loss relative to training steps on different datasets. Two of four datasetsare shown here as examples.
Figure 7: LFW verification accuracy during training. (a): Triplet loss converges very slowly. (b):Zooming in the final stage of training shows that our proposed loss achieves better performance.
