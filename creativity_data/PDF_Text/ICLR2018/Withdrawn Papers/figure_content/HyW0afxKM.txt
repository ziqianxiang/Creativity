Figure 1: SGIM-PB architectureThe SGIM-PB algorithm (see Algo. 2, Fig. 1) learns by episodes, where an outcome ωg ∈ Ω totarget and an exploration strategy σ have been selected. It is an extension of SGIM-ACTS (Nguyen& Oudeyer, 2012), which can perform complex motor policies and size 2 procedures (sequences of2 subtasks only). It uses the same interest model and memory based inverse model.
Figure 2: Experimental setup: a robotic arm, can interact with the different objects in its environment(a pen and two joysticks). Both joysticks enable to control a video-game character (represented intop-right corner). A grey floor limits its motions and can be drawn upon using the pen (a possibledrawing is represented).
Figure 3: Evaluation of all algorithms (final standard deviation shown in caption)10110010-1① uusMP5,000 10,000 15,000 20,000 25,000Iterationω25,000 10,000 15,000 20,000 25,000IterationI--SAGG-RIAC—IM-PB一SGIM-PBI—SGlM-AeTS5,00010,000 15,000 20,000 25,000Iterationω35,000 10,000 15,000 20,000 25,000Iteration5,000 10,000 15,000 20,000 25,000Iteration
Figure 4: Evaluation of all algorithms Per outcome space (for Ω0, all evaluations are superposed)If we look at the evaluation on each individual outcome space (Fig. 4), we can see that the learnerswith demonstrations (SGIM-PB and SGIM-ACTS) outperforms the other algorithms, except for theoutcome space Ω5 where IM-PB is better, due to the fact that IM-PB practiced much more on thisoutcome space (500 iterations where the goal was in Ω5 against 160 for SGIM-PB) on this outcomespace. SGIM-PB and SGIM-ACTS are much better than the other algorithms on the two joysticksoutcome spaces (Ω3 and Ω4). This is not surprising given the fact that those outcome spaces requireprecise policies. Indeed, if the end-effector gets out of the area where it can control the joystick, thelatter is released, thus potentially ruining the attempt. So on these outcome spaces working directlyon carefully crafted policies can alleviate this problem, while using procedures might be tricky, asthe outcomes used don’t take into account the motion trajectory but merely its final state. SGIM-PB was provided with such policies by the policy teachers. Also if we compare the results of theautonomous learner without procedures (SAGG-RIAC) with the one with procedures (IM-PB), wecan see that it learn less on any outcome space but Ω0 (which was the only outcome space reachableusing only single primitive policies and that could not benefit from using the task hierarchy to belearned) and especially for Ω1, Ω2 and Ω5 which were the most hierarchical in this setup. Moregenerally, it seems than on this highly hierarchical Ω5, the learners with procedures were better. Sothe procedures helped when learning any potentially hierarchical task in this experiment.
Figure 5: Choices of teachers and target outcomes of the SGIM-PB learnerWe wanted to see if our SGIM-PB learner adapts the complexity of its policies to the working task.
Figure 6: Number of policies selected per policy size for three increasingly more complex outcomespaces by the SGIM-PB learnerAs we can see on those three interrelated outcome subspaces (Fig. 6), the learner is capable to adaptthe complexity of its policies to the outcome at hand. It chooses longer policies for the Ω1 subspace(policies of size 2 and 3 while using mostly policies of size 1 and 2 for Ω0) and even longer forthe Ω2 subspace (using far more policies of size 3 than for the others). It shows that our learneris capable to correctly limit the complexity of its policies instead of being stuck into always tryinglonger and longer policies.
