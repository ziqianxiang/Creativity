Figure 1: InterBoost training procedure. n is the number of iteration. W1d and W2d are the weightsof data point {xd, yd}, d∈ {1, 2, ..., D} for two base networks. θ1 and θ2 are the parameters of twobase neural networks. W1(dn) + W2(dn) = 1 and 0 < W1(dn), W2(dn) < 1. P(yd|xd, θi(n)), i ∈ {1, 2} isthe probability that the ith base network can classify xd correctly after nth iteration.
Figure 2: Function w1 = p2/(p1 + p2) (left) and function w1 = ln(p1)/(ln(p1) + ln(p2)) (right),where 0 < p1 , p2 < 1.
Figure 3: Generated training datasets during the InterBoost training process. The datasets on the leftside are for the first base network, and the datasets on the right side are for the second base network.
Figure 4: Comparison of accuracies obtained by FC, Bagging, Adaboost, SnapShot and our methodvia box plot on UIUC and LM datasets. The central mark is the median, and the edges of the box arethe 25th and 75th percentiles. The outliers are marked individually. In two boxplots, each methodruns 60 rounds. Epoch number of FC and base network in ensemble methods is 800 epochs, inwhich our method has 8 iterations, and each base network in each iteration is trained 100 epochs.
