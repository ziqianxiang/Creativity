Figure 1: (a) The real visual world yields long-tailed datasets. Classes in the head are common (e.g.,cats) while classes in the tail are rare (e.g., white reindeers). (b) The proposed approach builds agenerative (Bayesian) classifier over a learned embedding to compute class-posterior probabilities.
Figure 2: We compare the effect of gradient-based updates for a traditional softmax classifier versusour Bayesian embedding model. Recall that our approach learns an embedding for which Bayesianclassifiers produce accurate class posteriors. During softmax training, an “easy” example of a classwill tend to not generate a strong gradient update, and so is not useful for learning (left). This mightbe considered paradoxical: when children learn a new concept (for say, a never-before-seen animal),an easy or “protypical” example might be most informative for learning. On the other hand, inour framework, an easy example of a class will change its centroid, generating a strong signal forupdating our learned representation (right).
Figure 4: Left column: Relative classification accuracy gain of the competing and proposed meth-ods with respect to a softmax classifier using long-tailed datasets. Overall, the proposed methodtends to achieve a comparable accuracy to that of a softmax classifier while delivering an increasefor tail classes. Right column: The performance of a softmax classifier. The performance forclasses in the head is higher than that of the classes in the tail.
Figure 5: Accuracy increase achieved by using the proposed regularizer on CIFAR 100. Overall, theproposed approach with regularizer tends to increase the accuracy across all classes compared to theproposed approach without one.
