Figure 1: Framework of the Sequence Labeling models. Solid lines depicted the basic model (B-LSTM-CRF) and dashed lines visualized several enhancements.
Figure 3: Performance of LSTM-CRF and LSTM-CNN-CRF with default parametersFigure 2: Character-level representation modules. The left one is LSTM (LamPle et al., 2016) andthe right one is CNN (Ma & Hovy, 2016)•	WSJ-PTB, i.e., the Wall Street Journal portion of Penn Treebank POS-tagging dataset (Marcuset al., 1993), categorizes each word into one of the 45 POS tags. The dataset has 25 sections andwe use sections 0-18 as training data, sections 19-21 as development data, and sections 22-24 astest data (Ma & Hovy, 2016; Manning, 2011).
Figure 2: Character-level representation modules. The left one is LSTM (LamPle et al., 2016) andthe right one is CNN (Ma & Hovy, 2016)•	WSJ-PTB, i.e., the Wall Street Journal portion of Penn Treebank POS-tagging dataset (Marcuset al., 1993), categorizes each word into one of the 45 POS tags. The dataset has 25 sections andwe use sections 0-18 as training data, sections 19-21 as development data, and sections 22-24 astest data (Ma & Hovy, 2016; Manning, 2011).
Figure 4: Performance of B-L-LSTM-CRF and B-C-LSTM-CRF. The adopted CRF is indicated by X-axis.
Figure 5: Performance OfHC-L-LSTM-CRF and H-L-LSTM-CRF. The hidden size of character-level LSTMis referred as “LSTM (#)”. The layer numbers of word-level and character-level LSTM are referred as “Word(#)Char(#)”. (a) compares HC-L-LSTM-CRF with H-L-LSTM-CRF and (b) compares different settings of HC-L-LSTM-CRF.
Figure 6: Word Representation Modulesdictionary of pre-trained embeddings. Leveraging such embedding, our NER model can correctlyannotate it as an ORG (organization) entity instead of PER (person) entity. Consequently, the choiceof pre-trained embeddings has a big impact on the resulting performance (Ma & Hovy, 2016).
Figure 7: Performance of Word-wise Dropout on CoNLL03 with different Dropout RatiosModels	Performance on Different Datasets				CoNLL03 (F1 score)		WSJ-PTB (Accuracy)		mean ± std	max	mean ± std	maxWC-LSTM-CRF	91.82±0.23	92.19	97.56±0.03	97.64HC-L-LSTM-CRF	91.68±0.13	91.91	97.50±0.02	97.52B-L-LSTM-CRF	91.47±0.12	91.68	97.52±0.03	97.55B-C-CNN-CRF	90.82±0.26	91.25	96.96±0.23	97.11Table 6: Performance Comparison between H-C-LSTM-CRF (referred as “H-C”) and B-C-LSTM-CRF (referred as “B-C”). Highlight refers to the winning setting between “H-C” and “B-C”. Thefilter number of CNN is marked as CNN (#).
