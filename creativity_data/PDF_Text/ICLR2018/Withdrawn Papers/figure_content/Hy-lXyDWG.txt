Figure 1: Incremental learning model: the network needs to grow its capacity with arrival of data ofnew classes.
Figure 2: (a) Network architecture for proposed training methodology. For simplicity, the input by-pass connections of ResNet (He et al., 2016a;b) is not shown here. (b) Combined network accuracyfor 100 classes varied with different amount of network sharing. ‘%’ Sharing in figure 2b is theportion of trainable parameters which are frozen and shared between the base and the new network.
Figure 3: Overview of the DCNN incremental training methodology with partial network sharing.
Figure 4: Comparison of (a) energy/accuracy trade-off and (b) training time requirements, betweenincremental training with and without sharing convolutional layers, is shown for different sharingconfigurations.
Figure 5: Comparison of (a) storage and (b) memory access requirements, between incrementaltraining with and without sharing convolutional layers, is shown for different sharing configurations.
