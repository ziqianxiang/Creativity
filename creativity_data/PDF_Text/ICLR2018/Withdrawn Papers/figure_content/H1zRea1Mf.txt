Figure 1: Overview of the pix2code model architecture. During training, the GUI image is encodedby a CNN-based vision model; the context (i.e. a sequence of one-hot encoded tokens correspondingto DSL code) is encoded by a language model consisting of a stack of LSTM layers. The tworesulting feature vectors are then concatenated and fed into a second stack of LSTM layers actingas a decoder. Finally, a softmax layer is used to sample one token at a time; the output size of thesoftmax layer corresponding to the DSL vocabulary size. Given an image and a sequence of tokens,the model (i.e. contained in the gray box) is differentiable and can thus be optimized end-to-endthrough gradient descent to predict the next token in the sequence. During sampling, the inputcontext is updated for each prediction to contain the last predicted token. The resulting sequence ofDSL tokens is compiled to the desired target language using traditional compiler design techniques.
Figure 2: An example of a native iOS GUI written in our markup-like DSL.
Figure 3: Training loss on different datasets and ROC curves calculated during sampling with themodel trained for 10 epochs.
Figure 4: Experiment samples for the iOS GUI dataset.
Figure 5: Experiment samples from the Android GUI dataset.
Figure 6: Experiment samples from the web-based GUI dataset.
