Figure 1: Base model vs. whitebox & blackbox meta-models.
Figure 2: Neural structure of the base model.
Figure 3: Figures (1a)-(1d) show the performance metrics for the various model in the “Clean/Clean”condition, i.e., when both the base model as well as the meta-model were trained using uncorrupteddata. The AUC (area under curve) values were calculated for each model and are shown in thecorresponding legend of the ROC plots. Similar plots for the 30%-degraded version of the basemodel but clean meta-models are shown in Figures (2a)-(2d). Finally, performance curves for the“Noisy/Noisy” condition, i.e., one where both the base and the meta-model are degraded by 30%-noise are shown in Figures (3a)-(3d).
Figure 4: Accuracies for the meta-modeltraining data at varying probe levelsThe accuracy plots do not provide insights into how the whitebox models achieve their higher per-formances and how this changes going from the clean data scenario to the scenarios with added labelnoise. To gain additional insight we performed a feature informativeness analysis based on a methoddescribed in (Friedman (2001)). Considering the clean data scenario first, the top features based ontheir relative importance estimation for GBM models include the probe outputs at the deepest layers(17, 16, 15, 14) corresponding to the predicted class with the highest final base model score andthe class with the second highest base model score. This aligns with the intuition that having highscores for the predicted class and large gaps between the scores for the top two classes might beindicative of the base model being correct. This changes when we consider the two scenarios withlabel noise. The most important features for the GBM whitebox model in these cases are the probescores for the predicted class at intermediate layers (11, 12, 13) followed by the second last layer(15). The ability of the meta-model to utilize the information from the intermediate probes leads toits significant improvement in performance in the two noisy scenarios.
Figure 5: In-domain and out-of-domain performances using whitebox logistic regression meta-model (left) and base model scores (right)The x-axes in these plots represents the corresponding threshold values for the respective models forfiltering the base model predictions (i.e., samples with confidence scores lower than the thresholdvalue would be filtered). First, consider the whitebox meta-model case in Figure 5 (left). Let’s say,7Under review as a conference paper at ICLR 2018in an application setting, we pick a threshold (≈0.5) that achieves an in-domain recall of 0.8. At thisthreshold, the logistic regression whitebox meta-model achieves an in-domain precision greater than0.9. If we encounter a domain shift as represented by the out-of-domain task the precision degradesto ≈0.65. Consider the same situation if we were using the base model score as in Figure 5(right).
Figure 6: Confusion-quadrant examples for the whitebox logistic regression (left) and the basemodel score (right).
