Figure 1: (a) The explanation module is a dimensionality reduction mechanism so that the original prediction ycan be reproduced from this low-dimensional space. An explanation module can be attached to any layer in theprediction deep network (DNN). The output of the DNN can be faithfully recovered from this low-dimensionalexplanation space, which represents high-level features that are interpretable to humans. (b) Two non-localizedand highly correlated heat map explanations in the first row; two localized and largely orthogonal heat mapexplanations in the second row.
Figure 2: Illustration of the SRAE used for the explanation module. Both the prediction and a sparsereconstruction are generated from the explanation space; (b) The log penalty function log(1 + q ∙ r2) whenq = 1; (c) The log penalty function log(1 + q ∙ r2) when q = 10.
Figure 3: (a) an example generated by our SRAE. The first line shows the original image, the part labelsof the image in the ground truth, and the Voronoi diagram of the image; the second and third lines show thevisualization results for the 9 neurons in the x-layer sorted by the weights (viEi, i = 1, 2, . . . , 9) for the finalprediction; (b) Examples of the interactive visualization system.
Figure 4: (a) The most important x-feature for several categories. The weight above the feature is viEi , theproduct of the weight of the x-feature in the approximation of y timed by the activation of the x-feature; (b)ExcitationBP on the predictions and on the x-features.
Figure 5: (a) Pixel-level probability Sin,,mj ; (b) Voronoi-based probability Spn,m for the example image inFigure 3(a).
Figure 6: The x-features for male and female downy woodpeckers.
Figure 7: The most important x-feature for several categories. The weight above the feature is viEi ,the product of the weight of the x-feature in the approximation of y timed by the activation of thex-feature.
Figure 8: (a) Good examples learned by SRAE, the number of the x-feature is 3, where the 3 neuronsare orthogonal to each other; (b) Degenerated examples learned by NN, the number of the x-featureis 3, where the first two neurons are very similar, and there is only one positive neuron; (c) Anotherdegenerated example learned by NN, the number of the x-feature is 9. Most of the neurons are verysimilar, and there are only two positive neurons.
Figure 9: ExcitationBP on the predictions and on the x-features.
Figure 10: The first row shows the heatmaps generated by ExcitationBP using the original images;the second row shows the heatmaps generated by ExcitationBP using the images with a constantvector shift similar to (Kindermans et al., 2017). The results show that ExcitationBP doesn’t sufferfrom the issue that most of the existing saliency methods are sensitive to the transformation of theinput.
