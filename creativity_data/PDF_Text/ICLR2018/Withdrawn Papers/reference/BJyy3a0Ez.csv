title,year,conference
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 Deep Learning,2016, MIT Press
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, CoRR
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Asynchronous parallel stochastic gradient fornonconvex optimization,2015, In Advances in Neural Information Processing Systems
 Network in network,2013, CoRR
 A scaled conjugate gradient algorithm for fast supervised learning,1993, Neuralnetworks
 The general inefficiency of batch training for gradientdescent learning,2003, Neural Networks
 Asynchronous stochastic gradientdescent for dnn training,2013, In Acoustics
