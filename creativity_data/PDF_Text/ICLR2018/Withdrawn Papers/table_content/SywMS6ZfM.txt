Table 1: Comparison with previous unsupervised embeddings. All values are percentages. AP@all(%) for 10 datasets and Spearman ρ (%) for HyperLex. Word2Vec+C scores the word pairs usingthe cosine similarity on skip-grams (SGNS). GE+C and GE+KL computes cosine similarity andnegative KL divergence on Gaussian embedding, respectively.
Table 2: Micro average AP@all (%) of 10 datasets using different scoring functions. The featurespace is SBOW using word frequency.
Table 3: Comparison with previous methods based on sparse bag of word (SBOW). All valuesare percentages. The results of invCL (Lenci & Benotto, 2012), APSyn (Santus et al., 2016), andCDE (Clarke, 2009) are selected because they have the best AP@100 in the first 4 datasets (San-tus et al., 2017). Cosine similarity (Levy et al., 2015b), balAPinc (Kotlerman et al., 2010) in 3datasets (Turney & Mohammad, 2015), SLQS (Santus et al., 2014) in HyperNet dataset (Shwartzet al., 2016), and Freq ratio (FR) (VUlic et al., 2016) are compared.
Table 4: AP@all (%) of 10 datasets. The box at lower right corner compares the micro average APacross all 10 datasets. Numbers in different rows come from different feature or embedding spaces.
Table 5: Spearman ρ (%) in HyperLex.
Table 6: The average number of non-zero dimen-sions across all testing words in 10 datasets.
Table 7: Spectral clustering on the non-zero DIVE dimensions of the query word. When the numberof clusters is set to be 2, we present the top 4 dimensions in each cluster, which have the highestvalues on the query word embedding. Otherwise, the top 2 dimensions are presented. CID refers tocluster ID. The top 5 words with the highest values of each dimension are presented.
Table 8: Comparison with semi-supervised embeddings (with limited training data). All valuesare percentages. The number in parentheses beside each approach indicates the number of anno-tated hypernymy word pairs used to train the model. Semi-supervised embeddings include Hyper-Score (Nguyen et al., 2017) and H-feature (Roller & Erk, 2016). When we compare F1 with theresults from other papers, we use 20 fold cross validation to determine prediction thresholds, asdone by Roller & Erk (2016). Note that HyperScore ignores POS in the testing data, so we followthe setup when comparing with it.
Table 9: We show the top 30 words with the highest embedding magnitude after dot product withthe query embedding q (i.e. showing w such that |wT q|1 is one of the top 30 highest values). Therows with the empty query word sort words based on |w|1.
Table 10: Accuracy (%) of hypernym directionality prediction across 10 datasets.
Table 11: Dataset sizes. N denotes the number of word pairs in the dataset, and OOV shows howmany word pairs are not processed by all the methods in Table 4 and Table 5.
