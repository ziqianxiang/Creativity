Table 1: OMDb fix total feature number word/word pair performance evaluationmAP	F = 10.5K		F = 11K		F = 11.5K		F = 12K		F = 12.5K		F = 15K		word	word pair	word	word pair	word	word pair	word	word pair	word	word pair	word	word pairmAP 1	0.14772	0.14603	0.13281	0.14673	0.13817	-0.14789	0.13860	0.14754	0.14019	0.14870	0.13686	0.14708mAP 3	0.09381	0.09465	0.08606	0.09327	0.08933	0.09507	0.08703	0.09517	0.09054	0.09657	0.09009	0.09537mAP 5	0.07453	0.07457	0.06835	0.07380	0.07089	0.07508	0.06925	0.07485	0.07117	0.07635	0.07175	0.07511mAP 10	0.05273	0.05387	0.04862	0.05340	0.04976	0.05389	0.04900	0.05322	0.05019	0.05501	0.05083	0.05388The two models are further applied on the Reuters dataset, and the results are shown in Figure 4 andTable 2. Again the word/word pair combined model outperforms the word-only model almost all thetime. For the mAP 1, the most significant improvement happens when Feature Number = 12500.
Table 2: Reuters fix total feature number word/word pair performance evaluationmAP	F = 10.5K		F=11K		F = 11.5K		F = 12K		F = 12.5K		F = 15K		word	word pair	word	word pair	word	word pair	word	word pair	word	word pair	word	word pairmAP 1	0.94195	0.95127	0.94277	0.95023	0.94407	-0.95179	0.94244	0.94997	0.94277	0.95270	0.94163	0.94984mAP 3	0.92399	0.93113	0.92448	0.93117	0.92604	0.93276	0.92403	0.93144	0.92249	0.93251	0.92326	0.93353mAP 5	0.91367	0.92123	0.91366	0.91939	0.91589	0.92221	0.91367	0.92051	0.91310	0.92063	0.91284	0.92219mAP 10	0.89813	0.90425	0.89849	0.90296	0.90050	0.90534	0.89832	0.90556	0.89770	0.90365	0.89698	0.90499The results for 20NewsGroup dataset are shown in Figure 5 and Table 3. Similar to previous twodatasets, all the results from word/word pair combined model are better than the word-only model.
Table 3: 20NewsGroup fix total feature number word/word pair performance evaluationmAPF = 10.5Kword word pairF=11K	F = 11.5K	F=12K	F = 12.5K	F=15Kword word pair word word pair word word pair word word pair word word pairmAP 1mAP 3mAP 5mAP 100.73736	0.771290.65227	0.689050.60861	0.646200.55103	0.589920.73375	0.760930.64848	0.680420.60548	0.637830.54812	0.580570.68720	0.758650.60356	0.67546
Table 4: Different K Values for OMDbmAP	Baseline	K=100	K = 300	K = 500	K = 800	K = 1000mAP 1	0.14134	0.14474	0.14318	0.14312	0.14212	0.14275mAP 3	0.09212	0.09410	0.09222	0.09324	0.09219	0.09223mAP 5	0.07312	0.07419	0.07313	0.07374	0.07293	0.07310mAP 10	0.05113	0.05341	0.05254	0.05320	0.05243	0.05274mAP score for Reuters dataset in original model is already very high almost all of them higher than0.9, it is hard to get the improvement as large as OMDb dataset. For the mAP 1, the most significantimprovement happens when K = 500, which is 0.31%. For the mAP 5, the mAP5 and the mAP 10,the most significant improvement shown in K = 800, about 0.50%, 0.38% and 0.42%.
Table 5: Different K Values for ReutersmAP	Baseline	K=100	K = 300	K = 500	K = 800	K = 1000mAP 1	0.94692	0.94565	0.94860	0.94988	0.94955	0.94898mAP 3	0.92719	0.92149	0.92715	0.92987	0.93146	0.93179mAP 5	0.91740	0.90859	0.91577	0.91881	0.92064	0.92088mAP 10	0.90078	0.88975	0.89869	0.90179	0.90403	0.90460provements about 2.82%, 2.90%, 3.2% and 3.33% respectively, and they all happened when K =1000.
Table 6: Different K Values for 20NewsGroupmAP	Baseline	K=100	K = 300	K = 500	K = 800	K = 1000mAP 1	0.73582	0.68122	0.71698	0.72272	0.75207	0.75659mAP 3	0.65447	0.58838	0.62933	0.63745	0.66834	0.67343mAP 5	0.61153	0.54296	0.58551	0.59431	0.62492	0.63118mAP 10	0.55651	0.48424	0.52789	0.53714	0.56724	0.575054.3.3 Word Pair Generation PerformanceIn this experiment, we compare different word pair generation algorithms with the baseline. Similarto previous experiments, the baseline is the word-only RBM model whose input consists of the10000 most frequent words. The ”semantic” word pair generation is the method we proposed in thispaper. By applying the idea from the skip-gram Mikolov et al. (2013b) algorithm, we generate theword pairs from each word’s adjacent neighbor, and we call it ”N-gram” word pair generation. Andthe window size we used in here is N = 2. For the Non-K word pair generation, we use the samealgorithm as the semantic except that no K-means clustering is applied on the generated word pairs.
Table 7: Different Algorithms for Word Pair Generation for OMDbmAP	Baseline	Semantic	N-gram	Non-KmAP 1	0.14134	0.14870	0.13202	0.14302mAP 3	0.09212	0.09657	0.08801	0.09406mAP 5	0.07312	0.07635	0.07111	0.07575mAP 10	0.05113	0.05501	0.05132	0.05585The first thing we observe from From the Table 7 is that both ”semantic” word pair generation and”Non-K” word pair generation give us better mAP score than the baseline; however, the mAP score10Under review as a conference paper at ICLR 201820NewsGroUp - mAP 30.9000.8000.7000 0.6000 0.500d 0.400W 0.3000.2000.100
