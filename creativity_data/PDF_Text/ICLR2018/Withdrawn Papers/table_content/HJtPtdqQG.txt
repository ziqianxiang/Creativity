Table 1: Speed up in training (in terms of no. of iterations) over manually tuned to reach x% of thebest reported top-1 validation accuracysubsequent epochs, we employ standard Bayesian optimization (BO) to obtain new proposals basedon the Gaussian process framework Shahriari et al. (2016). However, we do not always accept theoutcomes of BO. We design a simple probabilistic wait and watch test to decide whether to acceptthe BO outcome or to stick to the previously chosen learning rate, based on the improvement of thevalidation accuracy over the past few epochs. This rejection test is very crucial for obtaining goodperformance. Our experiments show that if we naively switch the learning rate to the BO outputat the end of each epoch, we have training instability and bad generalization performance. Thisrejection framework is philosophically similar to the hyperband framework Li et al. (2016) wheremore time is spent exploring the more promising choices. Here we require a more sophisticatedframework that utilizes the temporal history to assess whether the current choice of learning rate ispromising or if one should switch to a new learning rate, as proposed by BO.
