Table 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single modelbaselines across six benchmark NLP tasks. The performance metric varies across tasks - accuracyfor SNLI and SST-5; F1 for SQuAD, SRL and NER; average F1 for Coref. Due to the small testsizes for NER and SST-5, we report the mean and standard deviation across five runs with differentrandom seeds. The “increase” column lists both the absolute and relative improvements over ourbaseline.
Table 2:	Development set			performance for		Table 3:	Development set perfor-		SQuAD, SNLI and layers of the biLM regularization strength			SRL comparing (with different λ) to just the		using all choices of top layer.	mance for SQuAD, SNLI and SRL when including ELMo at different locations in the supervised model.			Task	Baseline		Last Only	All layers		Task	Input Only	Input & Output	Output Only				λ=1	λ=0.001	SQuAD SNLI SRL	84.2 88.9 84.7	848~ 89.5 84.3	83.7 88.7 80.9SQuAD SNLI SRL	80.8 88.1 81.6	-825- 89.1 84.1		-836" 89.3 84.6	84.8 89.5 84.8				lowing recent state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the baseline model isa biLSTM-CRF based sequence tagger. It forms a token representation by concatenating pre-trainedword embeddings with a character-based CNN representation, passes it through two layers of biL-STMs, and then computes the sentence conditional random field (CRF) loss (Lafferty et al., 2001)during training and decodes with the Viterbi algorithm during testing, similar to Collobert et al.
Table 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM.
Table 6: Test set POS tagging accuracies forPTB. For CoVe and the biLM, we report scoresfor both the first and second layer biLSTMs.
Table 5: All-words fine grained WSD F1.
Table 7: Development set perplexity before and after fine tuning for one epoch on the training setfor various datasets (lower is better). Reported values are the average of the forward and backwardperplexities. Dataset	Before tuning	After tuningSNLI	=	72.1	16.8CoNLL 2012 (coref/SRL)	^9Σ3-	-CoNLL 2003 (NER) 一	103.2	-46.3-u c An Context SQuAD Questions	ɪ1- 158.2	-43.5- 52.0-SST	131.5	^6-applied context-to-context, and another linear layer with ReLU activations. Finally, the results arefed through linear layers to predict the start and end token of the answer.
Table 8: SNLI test set accuracy.4Single model results occupy the portion, with ensemble results atthe bottom.
Table 9: Single model test set results for SQuAD, showing both Exact Match (EM) and F1. Refer-ences provided where available.
Table 10: SRL CoNLL 2012 test set F1.
Table 11: Coreference resolution average F1 on the test set from the CoNLL 2012 shared task.
Table 12: Test setFι for CoNLL 2003 NER task. Models with * included gazetteers and those with♦ used both the train and development splits for training.
Table 13: Test set accuracy for SST-5.
