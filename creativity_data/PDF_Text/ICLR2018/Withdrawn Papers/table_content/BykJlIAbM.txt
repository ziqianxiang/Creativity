Table 1: Our proposed framework can generalize MLE as point-to-point case, while RAML aspoint-to-cluster case.
Table 2: Four systems with different cluster models and different training methods for NMTs.
Table 3: Experimental results on IWSLT-2014 German-English Machine Translation TaskLDC Chinese-English We can see from Table 4 that System-D achieves the best performance inthe CH2EN translation direction, while System-D achieves the best performance on most test datasets in the EN2CH translation direction. The average improvement over the baseline systems forboth EN2CH and CH2EN direction are around 1.0 BLEU.
Table 4: Experimental results on NIST Chinese-English Machine Translation TaskWMT2014 German-English We can see from Table 5 that system-C achieves the strongest resulton both WMT14 EN-DE and DE-EN tasks, which outperforms the baseline system by over 1.1BLEU points. It’s worth noting that our one-layer RNN model even outperforms the deep multi-layer RNN model of Zhou et al. (2016) and Luong et al. (2015), which contain a stack of 4-7 LSTMlayers. By using cluster-to-cluster framework, our one-layer RNN model can fully exploit the datasetand learn to generalize better.
Table 5: Experimental results on WMT-2014 German-English Machine Translation TaskRAML’s random replacement strategy may introduce noisy and wrong bilingual pairs to hurt thetranslation performance (like in LDC Chinese-English translation task). Our adaptive cluster takesinto account more semantic contexts to enclose more rational paraphrases, and the bilateral augmen-tation also empowers the model more chance to access various inputs.
Table 6: Samples drawn from cluster distribution in LDC Chinese-English Translation taskalles hangt zusammen .
