Table 1: Data separationIn the following experiments, the whole subset A1 is used to train the base deep LSTM model,called Base. Then, for any time series bi in subset B, we use bi1 ∈ B1 for training, and bi2 ∈ B2 fortesting. The transfer learning is implemented as: given n as the transferred layer from Base, wefreeze the first n layers of the Base model, and use bi1 to train the other 6 - n layers. If n = 6, thenwe only use the specific data bi1 to train the last fully connected layer. We call the transfer learningmodel as AnB for a given n. We can also initialize the deep LSTM model with randomly chosenhyperparameters, and only use bi1 to train the deep LSTM model. We call the model Single. Allthe three models are tested using bi2 ∈ B2. The training data and test data setup is shown in Table 2.
Table 2: Training and test data for i-th customer for different deep LSTM models4 Results and DiscussionIn this section, we systematically analyze the feature transferability and their stability among diversetime series. We also provide a geometric understanding to explain the observed transfer learning fordeep LSTM model. Then, we show that by using transfer learning, we can use deep LSTM modelfor accurate time-series forecasting with limited history at a very small computational cost.
