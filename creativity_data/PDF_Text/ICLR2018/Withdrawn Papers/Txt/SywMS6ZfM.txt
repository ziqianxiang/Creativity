Under review as a conference paper at ICLR 2018
Distributional Inclusion Vector Embedding
for Unsupervised Hypernymy Detection
Anonymous authors
Paper under double-blind review
Ab stract
Modeling hypernymy, such as poodle is-a dog, is an important generalization aid
to many NLP tasks, such as entailment, relation extraction, and question answer-
ing. Supervised learning from labeled hypernym sources, such as WordNet, limit
the coverage of these models, which can be addressed by learning hypernyms from
unlabeled text. Existing unsupervised methods either do not scale to large vocab-
ularies or yield unacceptably poor accuracy. This paper introduces distributional
inclusion vector embedding (DIVE), a simple-to-implement unsupervised method
of hypernym discovery via per-word non-negative vector embeddings which pre-
serve the inclusion property of word contexts. In experimental evaluations more
comprehensive than any previous literature of which we are aware—evaluating on
11 datasets using multiple existing as well as newly proposed scoring functions—
we find that our method provides up to double the precision of previous unsuper-
vised methods, and the highest average performance, using a much more compact
word representation, and yielding many new state-of-the-art results. In addition,
the meaning of each dimension in DIVE is interpretable, which leads to a novel ap-
proach on word sense disambiguation as another promising application of DIVE.
1	Introduction
Numerous applications benefit from compactly representing context distributions, which assign
meaning to objects under the rubric of distributional semantics. In natural language processing,
distributional semantics has long been used to assign meanings to words (that is, to lexemes in the
dictionary, not individual instances of word tokens). The meaning of a word in the distributional
sense is often taken to be the set of textual contexts (nearby tokens) in which that word appears,
represented as a large sparse bag of words (SBOW). Without any supervision, word2vec (Mikolov
et al., 2013), among other approaches based on matrix factorization (Levy et al., 2015a), successfully
compress the SBOW into a much lower dimensional embedding space, increasing the scalability and
applicability of the embeddings while preserving (or even improving) the correlation of geometric
embedding similarities with human word similarity judgments.
While embedding models have achieved impressive results, context distributions capture more se-
mantic features than just word similarity. The distributional inclusion hypothesis (DIH) (Weeds &
Weir, 2003; Geffet & Dagan, 2005; Cimiano et al., 2005) posits that the context set of a word tends to
be a subset of the contexts of its hypernyms. For a concrete example, most adjectives that can be ap-
plied to poodle can also be applied to dog, because dog is a hypernym of poodle. For instance, both
can be obedient. However, the converse is not necessarily true — a dog can be straight-haired but a
poodle cannot. Therefore, dog tends to have a broader context set than poodle. Many asymmetric
scoring functions comparing SBOW based on DIH have been developed for automatic hypernymy
detection (Weeds & Weir, 2003; Geffet & Dagan, 2005; Santus et al., 2017).
Hypernymy detection plays a key role in many challenging NLP tasks, such as textual entail-
ment (Sammons et al., 2011), coreference (Ponzetto & Strube, 2006), relation extraction (Demeester
et al., 2016) and question answering (Huang et al., 2008). Leveraging the variety of contexts and
inclusion properties in context distributions can greatly increase the ability to discover taxonomic
structure among words (Santus et al., 2017). The inability to preserve these features limits the se-
mantic representation power and downstream applicability of some popular existing unsupervised
learning approaches such as word2vec.
1
Under review as a conference paper at ICLR 2018
Several recently proposed methods aim to encode hypernym relations between words in dense
embeddings, such as Gaussian embedding (Vilnis & McCallum, 2015; Athiwaratkun & Wilson,
2017), order embedding (Vendrov et al., 2016), H-feature detector (Roller & Erk, 2016), Hyper-
Score (Nguyen et al., 2017), dual tensor (Glavas & Ponzetto, 2017), Poincare embedding (Nickel
& Kiela, 2017), and LEAR (VUlic & Mrksic, 2017). However, the methods focus on supervised
or semi-supervised setting (Vendrov et al., 2016; Roller & Erk, 2016; Nguyen et al., 2017; Glavas
& Ponzetto, 2017; Vulic & Mrksic, 2017), do not learn from raw text (Nickel & Kiela, 2017) or
lack comprehensive experiments on the hypernym detection task (Vilnis & McCallum, 2015; Athi-
waratkun & Wilson, 2017).
Recent studies (Levy et al., 2015b; Santus et al., 2017) have underscored the difficulty of general-
izing supervised hypernymy annotations to unseen pairs — classifiers often effectively memorize
prototypical hypernyms (‘general’ words) and ignore relations between words. These findings mo-
tivate us to develop more accurate and scalable unsupervised embeddings to detect hypernymy and
propose several scoring functions to analyze the embeddings from different perspectives.
1.1	Contributions
•	A novel unsupervised low-dimensional embedding method to model inclusion relations
among word contexts via performing non-negative matrix factorization (NMF) on a
weighted PMI matrix, which can be efficiently optimized using modified skip-grams.
•	Several new asymmetric comparison functions to measure inclusion and generality proper-
ties and to evaluate different aspects of unsupervised embeddings.
•	Extensive experiments on 11 datasets demonstrate the learned embeddings and comparison
functions achieve state-of-the-art performances on unsupervised hypernym detection while
requiring much less memory and compute than approaches based on the full SBOW.
•	A qualitative experiment illustrates DIVE can be used to solve word sense disambiguation,
especially when efficiently modeling word senses at multiple granularities is desirable.
2	Method
The distributional inclusion hypothesis (DIH) suggests that the context set of a hypernym tends
to contain the context set of its hyponyms. That is, when representing a word as the counts of
contextual co-occurrences, the count in every dimension of hypernym y tends to be larger than or
equal to the corresponding count of its hyponym x:
X W y ^⇒ ∀c ∈ V, #(x, C) ≤ #(y, c),	(1)
where x y means y is a hypernym of x, V is the set of vocabulary, and #(x, c) indicates the
number of times that word x and its context word c co-occur in a small window with size |W | in
corpus D .
Our goal is to produce lower-dimensional embeddings that preserve the inclusion property that the
embedding of hypernym y is larger than or equal to the embedding of its hyponym x in every
dimension. Formally, the desirable property can be written as
X W y ^⇒ x[i] ≤ y[i] ,∀i ∈{1,…,do},	(2)
where d0 is number of dimensions in the embedding space. We add additional non-negativity con-
straints, i.e. X[i] ≥ 0, y[i] ≥ 0, ∀i, in order to increase the interpretability of the embeddings (the
reason will be explained later in this section).
This is a challenging task. In reality, there are a lot of noise and systematic biases which cause the
violation of DIH in Equation (1) (i.e. #(X, c) > #(y, c) for some neighboring word c), but the
general trend can be discovered by processing several thousands of neighboring words in SBOW
together. After the compression, the same trend has to be estimated in a much smaller embedding
space which discards most of the information in SBOW, so it is not surprising to see most of the
unsupervised hypernymy detection studies use SBOW (Santus et al., 2017) and the existing unsu-
pervised embeddings like Gaussian embedding have degraded accuracy (Vulic et al., 2016).
2
Under review as a conference paper at ICLR 2018
2.1	Inclusion Preserving Matrix Factorization
Popular methods of unsupervised word embedding are usually based on matrix factorization (Levy
et al., 2015a). The approaches first compute a co-occurrence statistic between the wth word and the
cth context word as the (w, c)th element of the matrix M [w, c]. Next, the matrix M is factorized
such that M[w, c] ≈ wTc, where w is the low dimension embedding of wth word and c is the cth
context embedding.
The statistic in M [w, c] is usually related to pointwise mutual information: PMI(w, c) =
log( PP(WPl)), where P(w,c) = #(W|c), |D| = PP #(w,c) is number of co-occurrence
w∈V c∈V
word pairs in the corpus, P(W) = #D(W, #(W) = P #(w, C) is the frequency of the word W times
c∈V
the window size |W|, and similarly for P (c). For example, M[W, c] could be set as positive PMI
(PPMI), max(P M I (W, c), 0), or shifted PMI, PMI (W, c) - log(k), like skip-grams with negative
sampling (SGNS) (Levy et al., 2015a). Intuitively, since M[W, c] ≈ wTc, larger embedding values
of w at every dimension seems to imply larger wTc, larger M [W, c], larger PMI(W, c), and thus
larger co-occurrence count #(W, c). However, the derivation has two flaws: (1) c could be negative
and (2) lower #(W, c) could still lead to larger PMI(W, c) as long as the #(W) is small enough.
To preserve DIH, we propose a novel word embedding method, distributional inclusion vector em-
bedding (DIVE), which fixes the two flaws by performing non-negative factorization (NMF) on the
matrix M, where
P( [ 1 / P(W,c)	#(w)\ 1 /#(w, c)|V K	c∖
M[w, c] = log(P(W) ∙ P(C) ∙ ⅛ΓZ) = log( #(c)k ),	⑶
where k is a constant which shifts PMI value like SGNS, Z =畜 is the average word frequency,
and |V | is the vocabulary size.
The design encourages the inclusion property in DIVE (i.e. Equation (2)) to be satisfied because
the property implies that Equation (1) (DIH) holds if the matrix is reconstructed perfectly. The
derivation is simple: Since context vector c is non-negative, if the embedding of hypernym y is
greater than or equal to the embedding of its hyponym x in every dimension, xTc ≤ yTc. Then,
M[x, C] ≤ M[y, C] tends to be true because wTc ≈ M[W, C]. This leads to #(x, C) ≤ #(y, C)
because M[w, c] = log( ##(：)}|) and only #(w, C) change with w.
2.2	Optimization
Due to its appealing scalability properties during training time (Levy et al., 2015a), we optimize
our embedding based on the skip-gram with negative sampling (SGNS) (Mikolov et al., 2013). The
objective function of SGNS is
lSGNS = ΣΣ#(W, C) log σ(wTc) +	k0	#(W, C) E [logσ(-wTcN)],	(4)
w∈Vc∈V	w∈V c∈V
where w ∈ R, c ∈ R, cN ∈ R, k0 is a constant hyper-parameter indicating the ratio between positive
and negative samples.
Levy & Goldberg (2014) prove SGNS is equivalent to factorizing a shifted PMI matrix M0, where
M0[w, c] = log( PP(WPc)C) ∙ ko). By setting k0 = ##(Z) and applying non-negativity constraints to
the embeddings, DIVE can be optimized using the similar objective function:
Z
IDIVE =EE#(W,c)log σ(wTc) + k V ɪʒ V#(w，c)旧。睡 σ(-wT Cn)],
W∈VC∈V	W∈V #(W) C∈V	cn~pd
(5)
where w ≥ 0, c ≥ 0, cN ≥ 0,σ is the logistic sigmoid function, and k is a constant hyper-parameter.
PD is the distribution of negative samples, which we set to be the corpus word frequency distribution
in this paper. Equation (5) is optimized by ADAM (Kingma & Ba, 2015), a variant of stochastic
gradient descent (SGD). The non-negativity constraint is implemented by projection (i.e., clipping
any embedding which crosses the zero boundary after an update).
3
Under review as a conference paper at ICLR 2018
…when a electron be remove from
a core level of a atom …
…the innovation of the common core , a
educational strategy …
…both basic CpUs and standard product
built around a CPU core …
I (dimension id)
id
Top 1-5 words
Top 101-105 words
1
2
methane, crystalline, complex, surround, proton
functional, requirement, processing, compatible, api
element, gas, atom, rock, carbon
system, architecture, develop, base, language
also, well, several, early, see
part, almost, see, addition, except
star, orbit, sun, orbital, planet
several, main, province, include, consist
science, philosophy, theory, philosopher, term
10
version, game, release, original, file
electron, current, electric, circuit, voltage
tank, cylinder, wheel, engine, steel
11
12
13
14
15
school, university, student, education, college
network, user, server, datum, protocol
high, low, temperature, energy, speed
acid, carbon, product, use, zinc
access, need, require, allow, program
fall, eventually, main, rise, mostly
incorporate, stage, instead, opening, add
bright, position, centauri, interaction, universe
designate, exist, swiss, branch, thai
ethical, advocate, topic, basic, universe
cassette, virtual, code, project, kb
anode, wire, ac, perform, resistor
aluminum, automatic, pilot, prevent, remove
doctorate, doctoral, middle, arts, compulsory
technology, rout, agent, microsoft, command
atmospheric, fast, blood, m, population
ph, monoxide, phosphorus, bond, manufacture
size, ability, format, run, typically
3
4
5
6
7
8
9
Figure 1: The top 15 dimensions in the distributional inclusion vector embedding (DIVE) of the
word core trained by the co-occurrence statistics of context words. The index of dimensions is
sorted by the embedding values. The words in each row of the table are sorted by its embedding
value in the dimension.
The optimization process provides an alternative angle to explain how DIVE preserves DIH. The
gradients for the word embedding w is
dlDWVE = χ #(w,C)(I -σ(WTC))C - k χ #|VN)σ(WTCN)cn.	(6)
c∈V	cN∈V
Assume hyponym x and hypernym y satisfy DIH in Equation (1) and the embeddings x and y are
the same at some point during the gradient ascent. In the case, the gradients coming from negative
sampling (the second term) decrease the same amount of embedding values for both x and y because
k is a constant hyper-parameter. However, the embedding of hypernym y would get higher or
equal positive gradients from the first term than x in every dimension because #(x, c) ≤ #(y, c).
This means Equation (1) tends to imply Equation (2). Combining the analysis from the matrix
factorization viewpoint, DIH in Equation (1) is approximately equivalent to the inclusion property
in DIVE (i.e. Equation (2)).
2.3	PMI Filtering
For a frequent target word, there must be many neighboring words that incidentally appear near the
target word without being semantically meaningful, especially when a large context window size
is used. The unrelated context words cause noise in both the word vector and the context vector of
DIVE. We address this issue by filtering out context words c for each target word w when the PMI of
the co-occurring words is too small (i.e., log( PP(WP(C)) < log(kf)). That is, We set #(w, c) = 0 in
the objective function. This preprocessing step is similar with computing PPMI in SBOW (Bullinaria
& Levy, 2007), where low PMI co-occurrences are removed from the count-based representation.
4
Under review as a conference paper at ICLR 2018
2.4	Interpretability
After applying the non-negativity constraint, we observe that each dimension roughly corresponds
to a topic, as previous findings suggest (Pauca et al., 2004; Murphy et al., 2012). This gives rise to a
natural and intuitive interpretation of our word embeddings: the word embeddings can be seen as un-
normalized probability distributions over topics. By removing the normalization of the target word
frequency in the shifted PMI matrix, specific words have values in few dimensions (topics), while
general words appear in more topics and correspondingly have high values in more dimensions, so
the concreteness level of two words can be easily compared using the magnitude of their embed-
dings. In other words, general words have more diverse context distributions, so we need more
dimensions to store the information in order to compress SBOW well (Nalisnick & Ravi, 2015).
In Figure 1, we present three mentions of the word core and its surrounding contexts. These various
context words increase the embedding values in different dimensions. Each dimension of the learned
embeddings roughly corresponds to a topic, and the more general or representative words for each
topic tend to have the higher value in the corresponding dimension (e.g. words in the second column
of the table). The embedding is able to capture the common contexts where the word core appears.
For example, the context of the first mention is related to the atom topic (dimension id 1) and the
electron topic (id 9), while the second and third mention occur in the computer architecture topic (id
2) and education topic (id 11), respectively.
3	Experiment S etup
We describe four experiments in Section 4-7. The first 3 experiments compare DIVE with other
unsupervised embeddings and SBOW using different hypernymy scoring functions. In these ex-
periments, unsupervised approaches refer to the methods that only train on plaintext corpus without
using any hypernymy or lexicon annotation. The last experiment presents qualitative results on word
sense disambiguation.
3.1	Datasets and Testing Setup
The SBOW and embeddings are tested on 11 datasets. The first 4 datasets come from the recent
review of Santus et al. (2017): BLESS (Baroni & Lenci, 2011), EVALution (Santus et al., 2015),
Lenci/Benotto (Benotto, 2015), and Weeds (Weeds et al., 2014). The next 4 datasets are down-
loaded from the code repository of the H-feature detector (Roller & Erk, 2016): Medical (i.e., Levy
2014) (Levy et al., 2014), LEDS (also referred to as ENTAILMENT or Baroni 2012) (Baroni et al.,
2012), TM14 (i.e., Turney 2014) (Turney & Mohammad, 2015), and Kotlerman 2010 (Kotlerman
et al., 2010). In addition, the performance on the test set of HyperNet (Shwartz et al., 2016) (using
the random train/test split), the test set of WordNet (Vendrov et al., 2016), and all pairs in Hyper-
Lex (VUlic et al., 2016) are also evaluated.
The F1 and accuracy measurements are sometimes very similar even though the quality of predic-
tion varies, so average precision AP@all is adopted as the main evaluation metric. The HyperLex
dataset has a continuous score on each candidate word pair, so we adopt Spearman rank coefficient
P as suggested by the review study of Vulic et al. (2016). Any OOV (out-of-VocabUlary) word en-
countered in the testing data is pushed to the bottom of the prediction list (effectively assuming the
word pair does not have a hypernym relation).
3.2	Training Setup
We use WaCkypedia corpus (Baroni et al., 2009), a 2009 Wikipedia dump, to compute SBOW and
train the embedding. For the datasets without Part of Speech (POS) information (i.e. Medical,
LEDS, TM14, Kotlerman 2010, and HyperNet), the training data of SBOW and embeddings are raw
text. For other datasets, we concatenate each token with the Part of Speech (POS) of the token before
training the models except the case when we need to match the training setup of another paper.
All words are lower cased. Stop words and rare words (occurs less than 10 times) are removed
during our preprocessing step. The number of embedding dimensions in DIVE d0 is set to be 100.
Other hyper-parameters used in the experiments are listed in the supplementary materials. The
5
Under review as a conference paper at ICLR 2018
Table 1: Comparison with previous unsupervised embeddings. All values are percentages. AP@all
(%) for 10 datasets and Spearman ρ (%) for HyperLex. Word2Vec+C scores the word pairs using
the cosine similarity on skip-grams (SGNS). GE+C and GE+KL computes cosine similarity and
negative KL divergence on Gaussian embedding, respectively.
Dataset	BLESS	EVALution	LenciBenotto	Weeds	Medical	LEDS
Random	~5.3-	2616	412	-514-	8.5	50.5
Word2Vec + C	9.2	25.4	40.8	51.6	11.2	71.8
GE + C	10.5	26.7	43.3	52.0	14.9	69.7
GE + KL	7.6	29.6	45.1	51.3	15.7	64.6
DIVE + C∙∆S	16.3	33.0	50.4	65.5	25.3	83.5
Dataset	TM14	KOtlerman 2010	HyperNet	WordNet	HyperLex	
Random	52.0-	308	2415	-55.2-	0	
Word2Vec + C	52.1	39.5	20.7	63.0	16.3	
GE + C	53.9	36.0	21.6	58.2	16.4	
GE + KL	52.0	39.4	23.7	54.4	9.6	
DIVE + C∙∆S	57.2	36.6	41.9	60.9	32.8	
hyper-parameters of DIVE were decided based on the performance of HyperNet training set. To
train embeddings more efficiently, we chunk the corpus into subsets/lines of 100 tokens instead of
using sentence segmentation. Preliminary experiments show that this implementation simplification
does not hurt the performance.
In the following experiments, we train both SBOW and DIVE on only the first 512,000 lines (51.2
million tokens) because we find this way of training setting provides better performances (for both
SBOW and DIVE) than training on the whole WaCkypedia or training on randomly sampled 512,000
lines. We suspect this is due to the corpus being sorted by the Wikipedia page titles, which makes
some categorical words such as animal and mammal occur 3-4 times more frequently in the first 51.2
million tokens than the rest. The performances of training SBOW PPMI on the whole WaCkypedia
is also provided for reference in Table 4 and Table 5.
4	Experiment 1: Comparison with Unsupervised Embeddings
If a pair of words has the hypernym relation, the words tend to be similar and the hypernym should
be more general than the hyponym. As in HyperScore (Nguyen et al., 2017), we score the hypernym
candidates by multiplying two factors corresponding to these properties. The C∙∆S (i.e. the cosine
similarity multiply the difference of summation) scoring function is defined as
T
ww
C ∙ δS(Wq → Wp) = η-nɪn-∏- YkWpkI - kwq l∣1),	⑺
||wq ||2 ∙ llwpll2
where Wp is the embedding of hypernym and Wq is the embedding of hyponym.
As far as we know, Gaussian embedding (GE) is the only unsupervised embedding method which
can capture the asymmetric relations between a hypernym and its hyponyms. Using the same train-
ing and testing setup, we use the code implemented by Athiwaratkun & Wilson (2017)1 to train
Gaussian embedding on the first 51.2 million tokens and test the embeddings on 11 datasets. Its
hyper-parameters are determined using the same way as DIVE (i.e. maximizing the AP on Hyper-
Net training set). We compare DIVE with GE2 in Table 1, and the performances of random scores
and only measuring word similarity using skip-grams are also presented for reference. As we can
see, DIVE is usually significantly better than other baselines.
5	Experiment 2: Hypernymy Scoring Functions Analysis
In Experiment 1, we show that there exists a scoring function (C∙∆S) which detects hypernymy
accurately using the embedding space of DIVE. Nevertheless, different scoring functions measure
1https://github.com/benathi/word2gm
2Notice that higher AP is reported in the previous literature: 80 (Vilnis & McCallum, 2015) in LEDS,
74.2 (Athiwaratkun & Wilson, 2017) in LEDS, and 20.6 (VuliC et al., 2016) in HyperLex. The difference might
be caused by different training and testing setup (e.g. their hyper-parameters might be determined by (parts of)
testing dataset).
6
Under review as a conference paper at ICLR 2018
different signals in SBOW or embeddings. Since there are so many scoring functions and datasets
available in the domain, we first introduce and test the performances of various scoring functions
so as to select the representative ones for a more comprehensive evaluation of DIVE on the hyper-
nymy detection tasks. We denote the embedding/context vector of the hypernym candidate and the
hyponym candidate as wp and wq , respectively. The SBOW model which represents a word by the
frequency of its neighboring words is denoted as SBOW Freq, while the SBOW which uses PPMI
of its neighboring words as the features (Bullinaria & Levy, 2007) is denoted as SBOW PPMI.
5.1	Unsupervised Scoring Functions
5.1.1	Similarity
A hypernym tends to be similar to its hyponym, so we measure the cosine similarity between word
vectors of the SBOW features (Levy et al., 2015b) or DIVE. We refer to the symmetric scoring
function as Cosine or C for short in the following tables. We also train the original skip-grams with
100 dimensions and measure the cosine similarity between the resulting word2vec embeddings. This
scoring function is referred to as Word2vec or W.
5.1.2	Generality
The distributional informativeness hypothesis (Santus et al., 2014) observes that in many corpora,
semantically ‘general’ words tend to appear more frequently and in more varied contexts. Thus,
Santus et al. (2014) advocate using entropy of context distributions to capture the diversity of con-
text. We adopt the two variations of the approach proposed by Santus et al. (2017): SLQS Row and
SLQS Sub functions. We also refer to SLQS Row as ∆E because it measures the entropy difference
of context distributions. For SLQS Sub, the number of top context words is fixed as 100.
Although effective at measuring diversity, the entropy totally ignores the frequency signal from the
corpus. To leverage the information, we measure the generality of a word by its L1 norm (|wp|1) and
L2 norm (||wp||2). Recall that Equation (2) indicates that the embedding of the hypernym y should
have a larger value at every dimension than the embedding of the hyponym x. When the inclusion
property holds, |y|1 = Pi y[i] ≥ Pi x[i] = |x|1 and similarly ||y||2 ≥ ||x||2. Thus, we propose
two scoring functions, difference of vector summation (|wp|1 - |wq|1) and the difference of vector
2-norm (||wp||2 - ||wq||2). Notice that when applying the difference of vector summations (denoted
as ∆S) to SBOW Freq, it is equivalent to computing the word frequency difference between the
hypernym candidate pair.
5.1.3	Combination
The combination of 2 similarity functions (Cosine and Word2vec) and the 3 generality functions
(difference of entropy, summation, and 2-norm of vectors) leads to six different scoring functions as
shown in Table 2, and C∙∆S is the same scoring function We used in Experiment 1. It should be noted
that if we use skip-grams with negative sampling (word2vec) as the similarity measurement (i.e.,
W ∙ ∆ {E,S,Q}), the scores are determined by two embedding/feature spaces together (Word2vec
and DIVE/SBOW).
5.1.4	Inclusion
Several scoring functions are proposed to measure inclusion properties of SBOW based on DIH.
Weeds Precision (Weeds & Weir, 2003) and CDE (Clarke, 2009) both measure the magnitude of the
intersection between feature vectors (|wp ∩ wq |). For example, wp ∩ wq is defined by the element-
wise minimum in CDE. Then, both scoring functions divide the intersection by the magnitude of the
potential hyponym vector (|wq|). invCL (Lenci & Benotto, 2012) (A variant of CDE) is also tested.
We choose these 3 functions because they have been shown to detect hypernymy well in a recent
study (Santus et al., 2017). However, it is hard to confirm that their good performances come from
the inclusion property between context distributions — it is also possible that the context vectors of
more general words have higher chance to overlap with all other words due to their high frequency.
For instance, considering a one dimension feature which stores only the frequency of words, the
naive embedding could still have reasonable performance on the CDE function, but the embedding
7
Under review as a conference paper at ICLR 2018
Table 2: Micro average AP@all (%) of 10 datasets using different scoring functions. The feature
space is SBOW using word frequency.
Word2vec (W) 24.8	Cosine (C) 26.7	SLQS Sub 27.4	SLQS Row (∆E) 27.6	Summation (∆S) 31.5	Two norm (∆Q) 31.2
W∙∆E	-C∙∆E-	-W∙∆S-	C∙∆S	W∙∆Q	C∆Q
28.8	29.5	31.6	31.2	31.4	31.1
Weeds 19.0	CDE 31.1	invCL 30.7	Asymmetric L1 (ALi) 28.2		
in fact only memorizes the general words without modeling relations between words (Levy et al.,
2015b) and loses lots of inclusion signals in the word co-occurrence statistics.
In order to measure the inclusion property without the interference of the word frequency signal
from the SBOW or embeddings, we propose a new measurement called asymmetric L1 distance.
We first get context distributions dp and dq by normalizing wp and wq , respectively. Ideally, the
context distribution of the hypernym dp will include dq . This suggests the hypernym distribution dp
is larger than context distribution of the hyponym with a proper scaling factor adq (i.e., max(adq -
dp , 0) should be small). Furthermore, both distributions should be similar, so adq should not be too
different from dp (i.e., max(dp - adq, 0) should also be small). Therefore, we define asymmetric
L1 distance as
ALi = min ɪ2 wo ∙ max(adq [c] — dp[c], 0) + max(dp [c] — adq [c], 0),
c
(8)
where w0 is a constant which emphasizes the inclusion penalty. If w0 = 1 and a = 1, AL1
is equivalent to L1 distance. The lower AL1 distance implies a higher chance of observing the
hypernym relation. We tried w0 = 5 and w0 = 20. w0 = 20 produces a worse micro-average
AP@all on SBOW Freq, SBOW PPMI and DIVE, so we fix w0 to be 5 in all experiments. An
efficient way to solve the optimization in AL1 is presented in the supplementary materials.
5.2	Results and Discussions
We show the micro average AP@all on 10 datasets using different hypernymy scoring functions in
Table 2. We can see the combination functions such as C∙∆S and W∙∆S perform the best overall.
Among the unnormalized inclusion based scoring functions, CDE works the best. AL1 performs
well compared with other functions which remove the frequency signal such as Word2vec, Cosine,
and SLQS Row. The summation is the most robust generality measurement. In the table, the scoring
functions are applied to SBOW Freq, but the performances of hypernymy scoring functions on the
other feature spaces (e.g. DIVE) have a similar trend.
6	Experiment 3: Comparison with SBOW
6.1	Comparison with previously reported results
In Table 3, DIVE with two of the best scoring functions (C∙∆S and W∙∆S) is compared with the
previous unsupervised state-of-the-art approaches based on SBOW on different datasets.
There are several reasons which might cause the large performance gaps in some datasets. In addi-
tion to the effectiveness of DIVE, some improvements come from our proposed scoring functions.
The fact that every paper uses a different training corpus also affects the performances. Furthermore,
Santus et al. (2017) select the scoring functions and feature space for the first 4 datasets based on
AP@100, which we believe is too sensitive to the hyper-parameter settings of different methods. To
isolate the impact of each factor, we perform a more comprehensive comparison next.
6.2	Performance analysis
In this experiment, we examine whether DIVE successfully preserves the signals for hypernymy
detection tasks, which are measured by the same scoring functions designed for SBOW. Summation
difference (∆S) and CDE perform the best among generality and inclusion functions in Table 2,
8
Under review as a conference paper at ICLR 2018
Table 3: Comparison with previous methods based on sparse bag of word (SBOW). All values
are percentages. The results of invCL (Lenci & Benotto, 2012), APSyn (Santus et al., 2016), and
CDE (Clarke, 2009) are selected because they have the best AP@100 in the first 4 datasets (San-
tus et al., 2017). Cosine similarity (Levy et al., 2015b), balAPinc (Kotlerman et al., 2010) in 3
datasets (Turney & Mohammad, 2015), SLQS (Santus et al., 2014) in HyperNet dataset (Shwartz
et al., 2016), and Freq ratio (FR) (VUlic et al., 2016) are compared.
Dataset	BLESS	EVALution	LenciBenotto	Weeds	Medical
Metric	AP@all				Fl
Baselines	invCL		APSyn	CDE	Cosine
	5.1	35.3	38.2	-44.1	23∏
DIVE + C∙∆S	-163-	33.0	5014	-655	25.3
DIVE + W∙∆S-	18.6-	3213	51.5	68.6	25.7	-
Dataset	LEDS	TM14	Kotlerman 2010	HyperNet	HyperLex
Metric	AP@a		l	F1	Spearman P
Baselines	balAPinc			-SLQS-	Freq ratio
	73	56	37	-22.8	27.9
DIVE + C∙∆S	83.5-	57.2	3616	-41.9-	32.8
DIVE + W∙∆S-	86.4	57.3	37.4	38.6	33.3
respectively. AL1 could be used to examine the inclusion properties after removing the frequency
signal. Therefore, We will present the results using these 3 scoring functions, along withW∙∆S and
C∙∆S.
6.2	. 1 Baselines
In addition to classic representations such as SBOW Freq and SBOW PPMI, we compare distribu-
tional inclusion vector embedding (DIVE) with additional 4 baselines in Table 4.
SBOW PPMI with additional frequency weighting (PPMI w/ FW). Specifically, w[c]
max(log(
P (w,c)
P(W)*p (C)* #Zw)
), 0). This forms the matrix reconstructed by DIVE when k
1.
•	DIVE without the PMI filter (DIVE w/o PMI)
•	NMF on shifted PMI: Non-negative matrix factorization (NMF) on the shifted PMI without
frequency weighting for DIVE (DIVE w/o FW). This is the same as applying the non-
negative constraint on the skip-gram model.
•	K-means (Freq NMF): The method first uses Mini-batch k-means (Sculley, 2010) to cluster
words in skip-gram embedding space into 100 topics, and hashes each frequency count in
SBOW into the corresponding topic. If running k-means on skip-grams is viewed as an
approximation of clustering the SBOW context vectors, the method can be viewed as a
kind of NMF (Ding et al., 2005).
Let the N × N context matrix be denoted as Mc, where the (i, j)th element stores the
count of word j appearing beside word i. K-means hashing creates a N × 100 matrix G
with orthonormal rows (GT G = I), where the (i, k)th element is 0 if the word i does not
belong to cluster k . The orthonormal G is also an approximated solution of a type of NMF
(Mc ≈ FGT) (Ding et al., 2005). Hashing context vectors into topic vectors can be written
as McG ≈ FGTG = F.
In the experiment, we also tried to apply a constant log(k) shifting to SBOW PPMI (i.e.
max(P M I - log(k), 0)). We found that the performance degrades as k increases. Similarly, ap-
plying PMI filter to SBOW PPMI (set context feature to be 0 if the value is lower than log(kf))
usually makes the performances worse, especially when kf is large. Applying PMI filter to SBOW
Freq only makes its performances closer to (but still much worse than) SBOW PPMI, so we omit
this baseline as well.
6.2.2 Results and Discussions
In Table 4, we first confirm the finding of the previous review study of Santus et al. (2017): there is
no single hypernymy scoring function which always outperforms others. One of the main reasons
is that different datasets collect negative samples differently. This is also why we evaluate our
9
Under review as a conference paper at ICLR 2018
Table 4: AP@all (%) of 10 datasets. The box at lower right corner compares the micro average AP
across all 10 datasets. Numbers in different rows come from different feature or embedding spaces.
Numbers in different columns come from different datasets and unsupervised scoring functions. We
also present the micro average AP across the first 4 datasets (BLESS, EVALution, Lenci/Benotto
and Weeds). All wiki means SBOW using PPMI features trained on the whole WaCkypedia. FW
refers to frequency weighting on the shifted PMI matrix.
AP@all (%)	BLESS					EVALution					Lenci/Benotto				
	CDE	AL1	∆S	W-AS	C-AS	CDE	ALi	^AS	W-AS	C-AS	CDE	ALi	AS	W-AS	C-AS
Freq	6.3	7.3	5.6	11.0	5.9	ɪr	32.6	36.2	33.0	36.3	51.8	47.6	51.0	51.8	51.1
SBO	PPMI	13.6	5.1	5.6	17.2	15.3	30.4	27.7	34.1	31.9	34.3	47.2	39.7	50.8	51.1	52.0
SBOW PPMI w/ FW	6.2	5.0	5.5	12.4	5.8	36.0	27.5	36.3	32.9	36.4	52.0	43.1	50.9	51.9	50.7
All wiki	12.1	5.2	6.9	12.5	13.4	28.5	27.1	30.3	29.9	31.0	47.1	39.9	48.5	48.7	51.1
Full	9.3	7.6	6.0	18.6	16.3	30.0	27.5	34.9	32.3	33.0	46.7	43.2	51.3	51.5	50.4
DIVE	w/o PMI	7.8	6.9	5.6	16.7	7.1	32.8	32.2	35.7	32.5	35.4	47.6	44.9	50.9	51.6	49.7
w/o FW	9.0	6.2	7.3	6.2	7.3	24.3	25.0	22.9	23.5	23.9	38.8	38.1	38.2	38.2	38.4
Kmean (Freq NMF)	6.5	7.3	5.6	10.9	5.8	33.7	27.2	36.2	33.0	36.2	49.6	42.5	51.0	51.8	51.2
AP@all(%)	Weeds					Micro Average (4 datasets)					Medical				
	CDE	AL1	∆S	W-AS	C-AS	CDE	ALi	^AS	W-AS	C-AS	CDE	ALi	AS	W-AS	C-AS
Freq	69.5	58.0	68.8	68.2	68.4	^3T	21.8	22.9	25.0	23.0	19.4	19.2	14.1	18.4	15.3
SBOW	PPMI	61.0	50.3	70.3	69.2	69.3	24.7	17.9	22.3	28.1	27.8	23.4	8.7	13.2	20.1	24.4
SBOW PPMI w/ FW	67.6	52.2	69.4	68.7	67.7	23.2	18.2	22.9	25.8	22.9	22.8	10.6	13.7	18.6	17.0
All wiki	61.3	48.6	70.0	68.5	70.4	23.4	17.7	21.7	24.6	25.8	22.3	8.9	12.2	17.6	21.1
Full	59.2	55.0	69.7	68.6	65.5	22.1	19.8	22.8	28.9	27.6	11.7	9.3	13.7	21.4	19.2
DIVE	w/o PMI	60.4	56.4	69.3	68.6	64.8	22.2	21.0	22.7	28.0	23.1	10.7	8.4	13.3	19.8	16.2
w/o FW	49.2	47.3	45.1	45.1	44.9	18.9	17.3	17.2	16.8	17.5	10.9	9.8	7.4	7.6	7.7
Kmean (Freq NMF)	69.4	51.1	68.8	68.2	68.9	22.5	19.3	22.9	24.9	23.0	12.6	10.9	14.0	18.1	14.6
AP@all(%)	LEDS					TMΓ4					Kotlerman 2010				
	CDE	AL1	∆S	W-AS	C-AS	CDE	ALi	^AS	W-AS	C-AS	CDE	ALi	AS	W-AS	C-AS
SBOW Freq	82.7	70.4	70.7	83.3	73.3	"35.6"	53.2	54.9	55.7	55.0	35.9	40.5	34.5	37.0	35.4
SBOW PPMI	84.4	50.2	72.2	86.5	84.5	56.2	52.3	54.4	57.0	57.6	39.1	30.9	33.0	37.0	36.3
All wiki	83.1	49.7	67.9	82.9	81.4	54.7	50.5	52.6	55.1	54.9	38.5	31.2	32.2	35.4	35.3
	DIVE		83.3	74.7	72.7	86.4	83.5	55.3	52.6	55.2	57.3	57.2	35.3	31.6	33.6	37.4	36.6
AP@all (%)	HyperNet					WOrdNet					Micro Average (10 datasets)				
	CDE	AL1	∆S	W-AS	C-AS	CDE	ALi	^AS	W-AS	C-AS	CDE	ALi	AS	W-AS	C-AS
SBOW Freq	37.5	28.3	46.9	35.9	43.4	ɪr	55.2	55.5	56.2	55.6	31.1	28.2	31.5	31.6	31.2
SBOW PPMI	23.8	24.0	47.0	32.5	33.1	57.7	53.9	55.6	56.8	57.2	30.1	23.0	31.1	32.9	33.5
All wiki	23.0	24.5	40.5	30.5	29.7	57.4	53.1	56.0	56.4	57.3	29.0	23.1	29.2	30.2	31.1
	DIVE		25.3	24.2	49.3	33.6	32.0	60.2	58.9	58.4	61.1	60.9	27.6	25.3	32.1	34.1	32.7
method on many datasets to make sure our conclusions hold in general. For example, if negative
samples come from random word pairs (e.g. WordNet dataset), a symmetric similarity measure is
already a pretty good scoring function. On the other hand, negative samples come from related or
similar words in HyperNet, EVALution, Lenci/Benotto, and Weeds, so only computing generality
difference leads to the best (or close to the best) performance. The negative samples in many datasets
are composed of both random samples and similar words (such as BLESS), so the combination of
similarity and generality difference yields the most stable results.
DIVE performs similar or better on all the scoring functions compared with SBOW consistently
across all datasets in Table 4, while using many fewer dimensions (see Table 6). Its results on
combination scoring functions outperform SBOW Freq. Meanwhile, its results on AL1 outperform
SBOW PPML The fact that combination scoring functions (i.e., W∙∆S or C∙∆S) usually outperform
generality functions suggests that only memorizing general words is not sufficient. The best average
performance on 4 and 10 datasets are both produced by W∙∆S on DIVE.
SBOW PPMI improves the combination functions from SBOW Freq but sacrifices AP on the inclu-
sion functions. It generally hurts performance to change the frequency sampling of PPMI (PPMI w/
FW) or compute SBOW PPMI on the whole WaCkypedia (all wiki) instead of the first 51.2 million
tokens. The similar trend can also be seen in Table 5. Note that AL1 completely fails in HyperLex
dataset using SBOW PPMI, which suggests that PPMI might not necessarily preserve the distribu-
tional inclusion property, even though it can have good performance on combination functions.
Removing the PMI filter from DIVE slightly drops the overall precision while removing frequency
weights on shifted PMI (w/o FW) leads to poor performances. K-means (Freq NMF) produces
similar AP compared with SBOW Freq, but has worse AL1 scores. Its best AP scores on differ-
ent datasets are also significantly worse than the best AP of DIVE. This means that only making
word2vec (skip-grams with negative sampling) non-negative or naively accumulating topic distribu-
tion in contexts cannot lead to satisfactory embeddings.
10
Under review as a conference paper at ICLR 2018
Table 5: Spearman ρ (%) in HyperLex.
Spearman ρ (%)	HyperLeX				
	CDE	ALi	∆S	W∙∆S	C∙∆S
SBOW Freq	31.7	"T96	27.6	29.6	27.3
SBOW PPMI	28.1	-2.3	31.8	34.3	34.5
All wiki	25.3	-2.2	28.0	30.5	31.0
DIVE	28.9	18.7	31.2	33.3	32.8
Table 6: The average number of non-zero dimen-
sions across all testing words in 10 datasets.
SBOW Freq	SBOW PPMI	DIVE
5799 一	3808	—	20
Table 7: Spectral clustering on the non-zero DIVE dimensions of the query word. When the number
of clusters is set to be 2, we present the top 4 dimensions in each cluster, which have the highest
values on the query word embedding. Otherwise, the top 2 dimensions are presented. CID refers to
cluster ID. The top 5 words with the highest values of each dimension are presented.
Query	CID	Top 5 words in the top dimensions	
rock	1	element, gas, atom, rock, carbon find, specie, species, animal, bird	sea, lake, river, area, water point, side, line, front, circle
	2	band, song, album, music, rock early, work, century, late, begin	write, john, guitar, band, author include, several, show, television, film
bank	1	county, area, city, town, west building, build, house, palace, site	several, main, province, include, consist sea, lake, river, area, water
	2	money, tax, price, pay, income united, states, country, world, europe	company, corporation, system, agency, service state, palestinian, israel, right, palestine
apple	1	food, fruit, vegetable, meat, potato war, german, ii, germany, world	goddess, zeus, god, hero, sauron write, john, guitar, band, author
	2	version, game, release, original, file system, architecture, develop, base, language	car, company, sell, manufacturer, model include, several, show, television, film
star	1	film, role, production, play, stage wear, blue, color, instrument, red	character, series, game, novel, fantasy write, john, guitar, band, author
	2	element, gas, atom, rock, carbon give, term, vector, mass, momentum	star, orbit, sun, orbital, planet light, image, lens, telescope, camera
tank	1	tank, cylinder, wheel, engine, steel acid, carbon, product, use, zinc	industry, export, industrial, economy, company network, user, server, datum, protocol
	2	army, force, infantry, military, battle however, attempt, result, despite, fail	aircraft, navy, missile, ship, flight war, german, ii, germany, world
	-1-	win, world, cup, play, championship	two, one, three, four, another
race	-2-	railway, line, train, road, rail	car, company, sell, manufacturer, model
	-	population, language, ethnic, native, people-	female, age, woman, male, household
	1	system, architecture, develop, base, language	access, need, require, allow, program
run	-2-	railway, line, train, road, rail	also, well, several, early, see
	-	game, team, season, win, league	game, player, run, deal, baseball
	-1-	bc, source, greek, ancient, date	book, publish, write, work, edition
tablet	-2-	use, system, design, term, method	version, game, release, original, file
	3	system, blood, vessel, artery, intestine	patient, symptom, treatment, disorder, may
7	Experiment 4: Word Sense Disambiguation
In addition to hypernymy detection, Athiwaratkun & Wilson (2017) show that the mixture of Gaus-
sian distributions can also be used to discover multiple senses of each word. In our qualitative
experiment, we show that DIVE can achieve the similar goal without fixing the number of senses
before training the embedding.
Recall that each dimension roughly corresponds to one topic. Given a query word, the higher embed-
ding value on a dimension implies higher likelihood to observe the word in the context of the topic.
The embedding of a polysemy would have high values on different groups of topics/dimensions.
This allows us to discover the senses by clustering the topics/dimensions of the polysemy. We use
the embedding values as the feature each dimension, compute the pairwise similarity between di-
mensions, and apply spectral clustering (Stella & Shi, 2003) to group topics as shown in the Table 7.
See more implementation details in the supplementary materials.
11
Under review as a conference paper at ICLR 2018
In the word sense disambiguation tasks, it is usually challenging to determine how many
senses/clusters each word should have. Many existing approaches fix the number of senses be-
fore training the embedding (Tian et al., 2014; Athiwaratkun & Wilson, 2017). Neelakantan et al.
(2014) make the number of clusters approximately proportional to the diversity of the context, but
the assumption does not always hold. Furthermore, the training process cannot capture different
granularity of senses. For instance, race in the car context could share the same sense with the race
in the game topic because they all mean contest, but the race in the car context actually refers to the
specific contest of speed. Therefore, they can also be viewed as separate senses (like the results in
Table 7). This means the correct number of clusters is not unique, and the methods, which fixes the
cluster numbers, need to re-train the embedding many times to capture such granularity.
In our approach, clustering dimensions is done after the training process of DIVE is completed, so it
is fairly efficient to change the cluster numbers and hierarchical clustering is also an option. Similar
to our method, Pelevina et al. (2016) also discover word senses by graph-based clustering. The main
difference is that they cluster the top n words which are most related to the query word instead of
topics. However, choosing the hyper-parameter n is difficult. Large n would make graph clustering
algorithm inefficient, while small n would make less frequent senses difficult to discover.
8	Related work
Most previous unsupervised approaches focus on designing better hypernymy scoring functions for
sparse bag of word (SBOW) features. They are well summarized in the recent study (Santus et al.,
2017). Santus et al. (2017) also evaluate the influence of different contexts, such as changing the
window size of contexts or incorporating dependency parsing information, but neglect scalability
issues inherent to SBOW methods.
A notable exception is the Gaussian embedding model (Vilnis & McCallum, 2015). The context dis-
tribution of each word is encoded as a multivariate Gaussian distribution, where the embeddings of
hypernyms tend to have higher variance and overlap with the embedding of their hyponyms. How-
ever, since a Gaussian distribution is normalized, it is difficult to retain frequency information during
the embedding process, and experiments on HyPerLex (VUlic et al., 2016) demonstrate that a simple
baseline only relying on word frequency can achieve good results. Follow-up work models contexts
by a mixtUre of GaUssians (AthiwaratkUn & Wilson, 2017) relaxing the Unimodality assUmption bUt
achieves little improvement on hypernym detection tasks.
Kiela et al. (2015) show that images retrieved by a search engine can be a UsefUl soUrce of infor-
mation to determine the generality of lexicons, bUt the resoUrces might not be available for some
corpora sUch as scientific literatUre.
Order embedding (Vendrov et al., 2016) is a sUpervised approach to encode many annotated hyper-
nym pairs (e.g. all of the whole WordNet (Miller, 1995)) into a compact embedding space, where the
embedding of a hypernym shoUld be smaller than the embedding of its hyponym in every dimension.
OUr method learns embedding from raw text, where a hypernym embedding shoUld be larger than
the embedding of its hyponym in every dimension. ThUs, DIVE can be viewed as an UnsUpervised
and reversed form of order embedding.
Other semi-sUpervised hypernym detection methods aim to generalize from sets of annotated word
pairs Using raw text corpora. The goal of HyperScore (NgUyen et al., 2017) is similar to oUr model:
the embedding of a hypernym shoUld be similar to its hyponym bUt with higher magnitUde. How-
ever, their training process relies heavily on annotated hypernym pairs, and the performance drops
significantly when redUcing the amoUnt of sUpervision. In addition to context distribUtions, previ-
oUs work also leverages training data to discover UsefUl text pattern indicating is-a relation (Shwartz
et al., 2016; Roller & Erk, 2016), bUt it remains challenging to increase recall of hypernym detection
becaUse commonsense facts like cat is-a animal might not appear in the corpUs.
Non-negative matrix factorization (NMF) has a long history in NLP, for example in the constrUction
of topic models (PaUca et al., 2004). Non-negative sparse embedding (NNSE) (MUrphy et al., 2012)
and FarUqUi et al. (2015) indicate that non-negativity can make embeddings more interpretable and
improve word similarity evalUations. The sparse NMF is also shown to be effective in cross-lingUal
lexical entailment tasks bUt does not necessarily improve monolingUal hypernymy detection (Vyas &
12
Under review as a conference paper at ICLR 2018
Carpuat, 2016). In our study, a new type of NMF is proposed, and the comprehensive experimental
analysis demonstrates its state-of-the-art performances on unsupervised hypernymy detection.
9	Conclusions
Compressing unsupervised SBOW models into a compact representation is challenging while pre-
serving the inclusion, generality, and similarity signals which are important for hypernym detec-
tion. Our experiments suggest that simple baselines such as accumulating K-mean clusters and
non-negative skip-grams do not lead to satisfactory performances in this task.
To achieve this goal, we proposed an interpretable and scalable embedding method called distribu-
tional inclusion vector embedding (DIVE) by performing non-negative matrix factorization (NMF)
on a weighted PMI matrix. We demonstrate that scoring functions which measure inclusion and gen-
erality properties in SBOW can also be applied to DIVE to detect hypernymy, and DIVE performs
the best on average, slightly better than SBOW while using many fewer dimensions.
Our experiments also indicate that unsupervised scoring functions, which combine similarity and
generality measurements, work the best in general, but no one scoring function dominates across all
datasets. A combination of unsupervised DIVE with the proposed scoring functions produces new
state-of-the-art performances on many datasets under the unsupervised setup.
Finally, a qualitative experiment shows that clusters of the topics discovered by DIVE often corre-
spond to the word senses, which allow us to do word sense disambiguation without the need to know
the number of senses before training the embeddings.
References
Ben Athiwaratkun and Andrew Gordon Wilson. Multimodal word distributions. In ACL, 2017.
Marco Baroni and Alessandro Lenci. How we BLESSed distributional semantic evaluation. In
Workshop on GEometrical Models of Natural Language Semantics (GEMS), 2011.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. The WaCky wide web: a
collection of very large linguistically processed web-crawled corpora. Language resources and
evaluation, 43(3):209-226, 2009.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. Entailment above the
word level in distributional semantics. In EACL, 2012.
Giulia Benotto. Distributional models for semantic relations: A study on hyponymy and antonymy.
PhD Thesis, University of Pisa, 2015.
John A Bullinaria and Joseph P Levy. Extracting semantic representations from word co-occurrence
statistics: A computational study. Behavior research methods, 39(3):510-526, 2007.
Philipp Cimiano, Andreas Hotho, and Steffen Staab. Learning concept hierarchies from text corpora
using formal concept analysis. J. Artif. Intell. Res.(JAIR), 24(1):305-339, 2005.
Daoud Clarke. Context-theoretic semantics for natural language: an overview. In workshop on
geometrical models of natural language semantics, pp. 112-119, 2009.
Thomas Demeester, Tim RocktascheL and Sebastian Riedel. Lifted rule injection for relation em-
beddings. In EMNLP, 2016.
Chris Ding, Xiaofeng He, and Horst D Simon. On the equivalence of nonnegative matrix factoriza-
tion and spectral clustering. In ICDM, 2005.
Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah Smith. Sparse overcomplete
word vector representations. In ACL, 2015.
Maayan Geffet and Ido Dagan. The distributional inclusion hypotheses and lexical entailment. In
ACL, 2005.
13
Under review as a conference paper at ICLR 2018
Goran Glavas and Simone Paolo Ponzetto. Dual tensor model for detecting asymmetric lexico-
semantic relations. In EMNLP, 2017.
Zhiheng Huang, Marcus Thint, and Zengchang Qin. Question classification using head words and
their hypernyms. In EMNLP, 2008.
Douwe Kiela, Laura Rimell, Ivan Vulic, and Stephen Clark. Exploiting image generality for lexical
entailment detection. In ACL, 2015.
Diederik Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. In ICLR, 2015.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. Directional distribu-
tional similarity for lexical inference. Natural Language Engineering, 16(4):359-389, 2010.
Alessandro Lenci and Giulia Benotto. Identifying hypernyms in distributional semantic spaces. In
SemEval, 2012.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In NIPS,
2014.
Omer Levy, Ido Dagan, and Jacob Goldberger. Focused entailment graphs for open IE propositions.
In CoNLL, 2014.
Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned
from word embeddings. Transactions of the Association for Computational Linguistics, 3:211-
225, 2015a.
Omer Levy, Steffen Remus, Chris Biemann, and Ido Dagan. Do supervised distributional methods
really learn lexical inference relations? In NAACL-HTL, 2015b.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representa-
tions of words and phrases and their compositionality. In NIPS, 2013.
George A. Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):
39-41, 1995.
Brian Murphy, Partha Talukdar, and Tom Mitchell. Learning effective and interpretable semantic
models using non-negative sparse embedding. COLING, pp. 1933-1950, 2012.
Eric Nalisnick and Sachin Ravi. Infinite dimensional word embeddings. arXiv preprint
arXiv:1511.05392, 2015.
Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. Efficient non-
parametric estimation of multiple embeddings per word in vector space. In EMNLP, 2014.
Kim Anh Nguyen, Maximilian KoPer, Sabine Schulte im Walde, and NgoC Thang Vu. Hierarchical
embeddings for hypernymy detection and directionality. In EMNLP, 2017.
Maximilian Nickel and DoUWe Kiela. PoinCare embeddings for learning hierarchical representa-
tions. In NIPS, 2017.
V. Paul Pauca, Farial Shahnaz, Michael W Berry, and Robert J. Plemmons. Text mining using
non-negative matrix factorizations. In ICDM, 2004.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12(Oct):2825-2830, 2011.
Maria Pelevina, Nikolay Arefyev, Chris Biemann, and Alexander Panchenko. Making sense of Word
embeddings. In Workshop on Representation Learning for NLP, 2016.
Simone Paolo Ponzetto and Michael Strube. Exploiting semantic role labeling, Wordnet and
Wikipedia for coreference resolution. In ACL, 2006.
14
Under review as a conference paper at ICLR 2018
Stephen Roller and Katrin Erk. Relations such as hypernymy: Identifying and exploiting hearst
patterns in distributional vectors for lexical entailment. In EMNLP, 2016.
Mark Sammons, V Vydiswaran, and Dan Roth. Recognizing textual entailment. Multilingual Nat-
ural Language Applications: From Theory to Practice. Prentice Hall, Jun, 2011.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte Im Walde. Chasing hypernyms in
vector spaces with entropy. In EACL, 2014.
Enrico Santus, Frances Yung, Alessandro Lenci, and Chu-Ren Huang. EVALution 1.0: an evolving
semantic dataset for training and evaluation of distributional semantic models. In Workshop on
Linked Data in Linguistics (LDL), 2015.
Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, and Chu-Ren Huang. Unsupervised
measure of word similarity: how to outperform co-occurrence and vector cosine in vsms. arXiv
preprint arXiv:1603.09054, 2016.
Enrico Santus, Vered Shwartz, and Dominik Schlechtweg. Hypernyms under siege: Linguistically-
motivated artillery for hypernymy detection. In EACL, 2017.
David Sculley. Web-scale k-means clustering. In WWW, 2010.
Vered Shwartz, Yoav Goldberg, and Ido Dagan. Improving hypernymy detection with an integrated
path-based and distributional method. In ACL, 2016.
X Yu Stella and Jianbo Shi. Multiclass spectral clustering. In ICCV, 2003.
Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. A proba-
bilistic model for learning multi-prototype word embeddings. In COLING, 2014.
Peter D Turney and Saif M Mohammad. Experiments with three approaches to recognizing lexical
entailment. Natural Language Engineering, 21(3):437-476, 2015.
Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In ICLR, 2016.
Luke Vilnis and Andrew McCallum. Word representations via gaussian embedding. In ICLR, 2015.
Ivan Vulic and Nikola Mrksic. Specialising word vectors for lexical entailment. arXiv preprint
arXiv:1710.06371, 2017.
Ivan Vulic, Daniela Gerz, DoUWe Kiela, Felix Hill, and Anna Korhonen. HyPerlex: A large-scale
evaluation of graded lexical entailment. arXiv preprint arXiv:1608.02117, 2016.
Yogarshi Vyas and Marine Carpuat. Sparse bilingual word representations for cross-lingual lexical
entailment. In HLT-NAACL, 2016.
Julie Weeds and David Weir. A general framework for distributional similarity. In EMNLP, 2003.
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir, and Bill Keller. Learning to distinguish
hypernyms and co-hyponyms. In COLING, 2014.
15
Under review as a conference paper at ICLR 2018
Table 8: Comparison with semi-supervised embeddings (with limited training data). All values
are percentages. The number in parentheses beside each approach indicates the number of anno-
tated hypernymy word pairs used to train the model. Semi-supervised embeddings include Hyper-
Score (Nguyen et al., 2017) and H-feature (Roller & Erk, 2016). When we compare F1 with the
results from other papers, we use 20 fold cross validation to determine prediction thresholds, as
done by Roller & Erk (2016). Note that HyperScore ignores POS in the testing data, so we follow
the setup when comparing with it.
Dataset	HyperLex	EVALUtion ∣ LenCiBenotto ∣ Weed厂			Medical
Metric	SPearman P	AP@all			F1
Baselines (#Training Hypernymy)	HyperScore (1337)				H-feature (897)
	30	39	44.8	58.5	26
DIVE + C∙∆S (0)	34.5	33.8	52.9	―	70.0	25.3 ―
Table 9: We show the top 30 words with the highest embedding magnitude after dot product with
the query embedding q (i.e. showing w such that |wT q|1 is one of the top 30 highest values). The
rows with the empty query word sort words based on |w|1.
Query	ToP 30 general words
	use	name	system	include	base	city large	state	group	power	death	form american	life	may	small	find	body design	work	produce	control	great	write study	lead	type	people	high	create
species	specie	species	animal	find	plant	may human	bird	genus	family	organism	suggest gene	tree	name	genetic	study	occur fish	disease	live	food	cell	mammal evidence	breed	protein	wild	similar	fossil
system	system	use	design	provide	operate	model standard	type	computer	application	develop	method allow	function	datum	device	control	information process	code	via	base	program	software network	file	development	service	transport	law
A Supplementary materials
A. 1 Comparison with Semi-supervised Embeddings
In addition to the unsupervised approach, we also compare DIVE with semi-supervised approaches.
When there are sufficient training data, there is no doubt that the semi-supervised embedding ap-
proaches such as HyperNet (Shwartz et al., 2016), H-feature detector (Roller & Erk, 2016), and
HyperScore (Nguyen et al., 2017) can achieve better performance than all unsupervised methods.
However, in many domains such as scientific literature, there are often not many annotated hyper-
nymy pairs (e.g. Medical dataset (Levy et al., 2014)).
Since we are comparing an unsupervised method with semi-supervised methods, it is hard to fairly
control the experimental setups and tune the hyper-parameters. In Table 8, we only show several
performances which are copied from the original paper when training data are limited3 . As we can
see, the performance from DIVE is roughly comparable to the previous semi-supervised approaches
trained on small amount of hypernym pairs. This demonstrates the robustness of our approach and
the difficulty of generalizing hypernymy annotations with semi-supervised approaches.
A.2 Generality estimation and hypernym directionality detection
In Table 9, we show the most general words in DIVE under different queries as constraints. We also
present the accuracy of judging which word is a hypernym (more general) given word pairs with
3We neglect the performances from models trained on more than 10,000 hypernym pairs, models trained
on the same datasets with more than 1000 hypernym pairs using cross-validation, and models using image
classifier.
16
Under review as a conference paper at ICLR 2018
Table 10: Accuracy (%) of hypernym directionality prediction across 10 datasets.
Micro Average (10 datasets)			
SBOW Freq + SLQS Sub 	64.4	SBOW Freq + 4S 	66.8	SBOW PPMI + 4S 	66.8	DIVE + 4S 67.0
Table 11: Dataset sizes. N denotes the number of word pairs in the dataset, and OOV shows how
many word pairs are not processed by all the methods in Table 4 and Table 5.
BLESS N	OOV 26554	1507	EVALution N	OOV 13675	2475	LenCi/Benotto N OOV 5010	1464	Weeds N	OOV 2928	643	Avg (4 datasets) N	OOV 48167	6089
Medical N	OOV 12602	3711	LEDS N	OOV 2770	28	TM14 N OOV 2188	178	Kotlerman 2010 N	OOV 2940	89	HyperNet N	OOV 17670	9424
WordNet N	OOV 8000	3596	Avg (10 datasets) N	OOV 94337	24110	HyperLex N OOV 2616	59		
hypernym relations in Table 10. The direction is classified correctly if the generality score is greater
than 0 (hypernym is indeed predicted as the more general word). For instance, summation difference
(∆S) classifies correctly if |wp|1 - |wq|1 > 0 (|wp|1 > |wq|1).
From the table, we can see that the simple summation difference performs better than SQLS Sub,
and DIVE predicts directionality as well as SBOW. Notice that whenever we encounter OOV, the
directionality is predicted randomly. If OOV is excluded, the accuracy of predicting directionality
using unsupervised methods can reach around 0.7-0.75.
A.3 Experimental details for hypernymy detection
In HyperNet and WordNet, some hypernym relations are determined between phrases instead of
words. Phrase embeddings are composed by averaging word embeddings or SBOW features. For
WordNet, we assume the Part of Speech (POS) tags of the words are the same as the phrase. All
part-of-speech (POS) tags in the experiments come from NLTK.
The window size |W | of SBOW, DIVE, and GE are set as 20 (left 10 words and right 10 words).
For DIVE, the number of epochs is 15, the learning rate is 0.001, the batch size is 128, the threshold
in PMI filter kf is set to be 30, and the ratio between negative and positive samples (k) is 1.5. The
hyper-parameters of DIVE were decided based on the performance of HyperNet training set. The
window size of skip-grams (word2vec) is 10. The number of negative samples (k0) in skip-gram is
set as 5. When composing skip gram into phrase embedding, average embedding is used.
For Gaussian embedding (GE), the number of mixture is 1, the number of dimension is 100, the
learning rate is 0.01, the lowest variance is 0.1, the highest variance is 100, the highest Gaus-
sian mean is 10, and other hyper-parameters are the default value in https://github.com/
benathi/word2gm. The hyper-parameters ofGE were also decided based on the performance of
HyperNet training set. When determining the score between two phrases, we use the average score
of every pair of tokens in two phrases.
The number of testing pairs N and the number of OOV word pairs is presented in Table 11.
A.4 Clustering dimensions for word sense disambiguation
We use all the default hyper-parameters of the spectral clustering library in Scikit-learn 0.18.2 (Pe-
dregosa et al., 2011) except the number of clusters is set manually. A simple way to prepare the
feature of each dimension f (ci) is to use the embedding values in that dimension w[ci] of all the
words in our vocabulary w ∈ V . That is,
f(ci)= [w[ci]]w∈V.	(9)
However, clustering on the global features might group topics together based on the co-occurrence of
words which are unrelated to the query words and we want to make the similarity dependent on the
17
Under review as a conference paper at ICLR 2018
query word. For example, a country topic should be clustered together with a city topic if the query
word is place, but it makes more sense to group the country topic with the money topic together if the
query word is bank like we did in the word sense disambiguation experiment (Table 7). This means
we want to focus on the geographical meaning of country when the query is related to geography,
while focus on the economic meaning of country when the query is about economics.
To create query dependent similarity measurement, we only consider the embedding of words which
are related to the query word when preparing the features of dimensions. Specifically, given a query
word q, the feature vector of the ith dimension f(ci, q) is defined as:
d0
f (ci, q) = j®1 [w[ci] ∙ wq [cj]]w∈Cj (n),
(10)
where wq[cj] is the value of jth dimension of query word embedding, Cj(n) is the set of embeddings
of top n words in the jth dimension, and the operator ㊉ means concatenation. This means instead
of considering all the words in the vocabulary, we only take the top n words of every dimension j (n
is fixed as 100 in the experiment), weight the feature based on how likely to observe query word in
dimension j (wq [cj]), and concatenate all features together. That is, when measuring the similarity
of dimensions, we only consider the aspects related to query word (e.g. mostly considering words
related to facility and money when the query word is bank).
After the features of all dimensions are collected, we normalize the feature of each dimension to
have the norm 1, compute the pairwise similarity and run the spectral clustering to get the clustering
results.
A.5 EFFICIENT WAY TO COMPUTE ASYMMETRIC L1 (AL1)
Recall that Equation (8) defines AL1 as follows:
AL1 = L = min	w0 max(adq [c] - dp[c], 0) + max(dp[c] - adq [c], 0),
c
where dp [c] is one of dimension in the feature vector of hypernym dp, adq is the feature vector of
hyponym after proper scaling. In Figure 2, an simple example is visualized to illustrate the intuition
behind the distance function.
By adding slack variables ζ and ξ, the problem could be converted into a linear programming prob-
lem:
L
min w0
a,ζ,ξ
ζc+	ξc
cc
ζc ≥ adq [c] - dp [c], ζc ≥ 0
ξc ≥ dp [c] - adq [c], ξc ≥ 0
a ≥ 0,
so it can be simply solved by a general linear programming library.
Nevertheless, the structure in the problem actually allows us to solve this optimization by a simple
sorting. In this section, we are going to derive the efficient optimization algorithm.
By introducing Lagrangian multiplier for the constraints, we can rewrite the problem as
L = min max w0	ζc+	ξc-	αc(ζc-adq[c]+dp[c])
a,ζ,ξ α,β,γ,δ
ccc
-Xβc(ξc-dp[c]+adq[c])-Xγcζc-Xδcξc
c	cc
ζc ≥ 0, ξc ≥ 0, αc ≥ 0, βc ≥ 0, γc ≥ 0, δc ≥ 0, a ≥ 0
18
Under review as a conference paper at ICLR 2018
Figure 2: An example of AL1 distance. If the word pair indeed has the hypernym relation, the
context distribution of hyponym (dq) tends to be included in the context distribution of hypernym
(dp) after proper scaling according to DIH. Thus, the context words only appear beside the hyponym
candidate (adq[c] - dp[c]) causes higher penalty.
First, we eliminate the slack variables by taking derivatives with respect to them:
∂L -	: ∂Zc	= 0 = 1 - βc - δc δc = 1 - βc, βc ≤ 1
∂L --: ∂ξc	= 0 = 1 - γc - αc γc = w0 - αc ,	αc ≤ w0 .
By substituting in these values for γc and δc, we get rid of the slack variables and have a new
Lagrangian:
L = min max -	αc(-adq[c] + dp[c]) -	βc(-dp[c] + adq[c])
a α,β
cc
0 ≤ αc ≤ w0, 0 ≤ βc ≤ 1, a ≥ 0
We can introduce a new dual variable λc = αc - βc + 1 and rewrite this as:
L = min max	(λc - 1)(adq[c] - dp[c])
aλ
c
0 ≤ λc ≤ w0 + 1, a ≥ 0
Let’s remove the constraint on a and replace with a dual variable η :
L = min max	(λc - 1)(adq [c] - dp[c]) - ηa
aλ
c
0 ≤ λc ≤ w0 + 1,η ≥ 0
19
Under review as a conference paper at ICLR 2018
Now let’s differentiate with respect to a to get rid of the primal objective and add a new constraint:
∂L
∂a
0 =	λcdq[c] -	dq [c] - η
λcdq [c] =	dq [c] + η
L = max	dp[c] -	λcdp[c]
X λcdq [c] = Xdq[c] +η
0 ≤ λc ≤ w0 + 1,η ≥ 0
Now we have some constant terms that are just the sums of dp and dq , which will be 1 if they are
distributions.
L = max 1 -	λcdp[c]
c
X λcdq [c] = 1 + η
c
0 ≤ λc ≤ w0 + 1, η ≥ 0
Now We introduce a new set of variables μc = λcdq [c] and We can rewrite the objective as:
L = max 1 一 ^' μc ?p[c]
μ V	dq [c]
£〃c = 1 + η
c
0 ≤ μc ≤ (wo + 1)dq[c],η ≥ 0
Note that for terms where dq[c] = 0 we can just set dq[c] = for some very small epsilon, and in
practice, our algorithm will not encounter these because it sorts.
So μ we can think of as some fixed budget that we have to spend UP until it adds up to 1, but it has
a limit of how much we can spend for each coordinate, given by (w0 + 1)dq [c]. Since we’re trying
to minimize the term involving μ, we want to allocate as much budget as possible to the smallest
terms in the summand, and then 0 to the rest once we’ve spent the budget. This also shows us that
our optimal value for the dual variable η is just 0 since we want to minimize the amount of budget
we have to allocate.
To make presentation easier, lets assume we sort the vectors in order of increasing ddp[∣], so that
ddp[1] is the smallest element, etc. We can now give the following algorithm to find the optimal μ.
init S = 0, c = 1, μ = 0
while S ≤ 1 :
μc = min(1 — S, (wo + 1)dq[c])
S = S + μc
c=c+1
At the end we canjust plug in this optimal μ to the objective to get the value of our scoring function.
20