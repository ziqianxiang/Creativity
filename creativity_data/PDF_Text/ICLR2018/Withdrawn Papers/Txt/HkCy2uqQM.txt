Under review as a conference paper at ICLR 2018
Complex- and Real-Valued Neural Network Architec-
tures
Anonymous authors
Paper under double-blind review
Abstract
Complex-value neural networks are not a new concept, however, the use of real-
values has often been favoured over complex-values due to difficulties in train-
ing and accuracy of results. Existing literature ignores the number of parameters
used. We compared complex- and real-valued neural networks using five activa-
tion functions. We found that when real and complex neural networks are com-
pared using simple classification tasks, complex neural networks perform equal
to or slightly worse than real-value neural networks. However, when specialised
architecture is used, complex-valued neural networks outperform real-valued neu-
ral networks. Therefore, complexvalued neural networks should be used when the
input data is also complex or it can be meaningfully to the complex plane, or when
the network architecture uses the structure defined by using complex numbers.
1	Introduction
In recent years complex numbers in neural networks are increasingly frequently used. Complex-
Valued neural networks have been sucessfully applied to a variety of tasks specifically in signal
processing where the input data has a natural interpretation in the complex domain.
In most publications complex-valued neural networks are compared to real-valued architectures.
We need to ensure that these architectures are comparable in their ability to approximate functions.
A common metric for their capacity are the number of real-valued parameters. The number of
parameters of complex-valued neural networks are rarely studied aspects. While complex numbers
increase the computational complexity, their introduction also assumes a certain structure between
weights and input. Hence, it is not sufficient to increase the number of parameters.
Even more important than in real-valued networks is the choice of activation function for each
layer. We test 5 functions: identity or no activation function, rectifier linear unit, hyperbolic tangent,
magnitude, squared magnitude.
This paper explores the performance of complex-valued multi-layer perceptrons (MLP) with varying
depth and width in consideration of the number of parameters and choice of activation function on
benchmark classification tasks.
In section 2 we will give an overview of the past and current developments in the applications
of complex-valued neural networks. We shortly present the multi-layer perceptron architecture in
section 3 using complex numbers and review the building blocks of complex-valued network.
In section 4 we consider the multi-layer perceptron with respect to the number of real-valued pa-
rameters in both the complex and real case. We construct complex MLPs with the same number of
units in each layer. We propose two methods to define comparable networks: A fixed number of
real-valued neurons per layer or a fixed budget of real-valued parameters.
In the same section we also consider the structure that is assumed by introducing complex numbers
into a neural network.
We present the activation function to be used in our experiments in section 5. In section 6 we present
our experiments and their settings. Section 7 discuss the results of different multi-layer perceptrons
on MNIST digit classification, CIFAR-10 image classification, CIFAR-100 image classification,
Reuters topic classification and bAbI question answering. We identify a general direction of why
and how to use complex-valued neural networks.
1
Under review as a conference paper at ICLR 2018
2	Related Literature
The idea of artificial neural networks with complex-valued input, complex-valued weights and
complex-valued output was proposed in the 1970s (Aizenberg, 2016). A complex-valued back-
propogation algorithm to train complex multi-layer networks was proposed in the 1990s by several
authors (Benvenuto & Piazza, 1992; Nitta, 1993; Georgiou & Koutsougeras, 1992). In the 2000s
complex neural networks, like real-valued neural networks, have been successfully applied to a va-
riety of tasks. These tasks included the processing and analysis of complex-valued data or data with
an intuitive mapping to complex numbers. Particularly, signals in their wave form were used as
input data to complex-valued neural networks (Hirose, 2009).
Another natural application of complex numbers are complex convolutions (Bruna et al., 2015),
since they have an application in both image and signal processing. While real convolutions are
widely used in deep neural networks for image processing, complex convolution can replace real-
valued convolutions (Trabelsi et al., 2017; Guberman, 2016; Popa, 2017; Haensch & Hellwich,
2010).
The properties of complex numbers and matrices introduce constraints into deep learning models.
Introduced by Arjovsky et al. (2015) and developed further by Wisdom et al. (2016) recurrent net-
works, which constrain their weights to be unitary, reduce the impact of the vanishing or exploding
gradient problem.
More recently complex numbers have been (re)discovered by a wider audience and used in ap-
proaches to other tasks like embedding learning (Trouillon et al., 2016; Sarroff et al., 2015), knowl-
edge base completion (Trouillon et al., 2017) or memory networks (Kobayashi, 2017).
Despite their success in signal processing tasks, complex-valued neural networks have been less
popular than their real-valued counter-parts. This may be due to training and reports of varying
results in related tasks. The training process and architecture design are less intuitive, which stems
from difficulties in differentiability of activation functions in the complex plane (Zimmermann et al.,
2011; Hirose, 2004; Nitta, 2014).
An aspect that has received little attention is an appropriate comparison of real- and complex-valued
neural networks. Many publications ignore the number of parameters all together (?), consider only
the number of parameters of the entire model (?) or do not distinguish in complex- or real-valued
parameters (?). While the latter is most confusing for the reader, all three problems lead to an
inappropriate comparison of the overall performance.
There exists a significant body of work on exploring deep learning architectures for real-valued
neural networks. Deep complex-valued neural networks are still to be explored. Previous work has
also shown the significance of the activation, not only for the training and gradient computation, but
also for the accuracy. Therefore, the width, depth and the choice of activation function need to be
considered together.
We aim to fill this gap by systematically exploring the performance of multi-layered architectures
on simple classification taks.
3	Complex-Valued Neural Network Architectures
Many fundamental building blocks of neural networks can be used in the complex domain by re-
placing real-valued parameters with complex parameters. However, there are some differences in
training complex-valued neural networks. We introduce the building blocks and consider differences
in structure and training. While the convolution on the complex plane using complex-valued filters
is natural, it has been investigated in related literature (see section 2). In this work we focus on layers
consisting of complex-valued neurons as building blocks and their use in multi-layer architecture.
We define a complex-valued neuron analogous to its real-valued counter-part. In consequence we
can use projection onto a complex weight matrix to realise complex-numbered embeddings.
The complex valued neuron can be defined as:
o = φ(X ∙ w + b)
(1)
2
Under review as a conference paper at ICLR 2018
with the (real or complex) input X ∈ Cn, complex weight W ∈ Cn and complex bias b ∈ C. Arranging
m neurons into a layer:
o = φ(XW +b)	(2)
with the input X ∈ Cn , W ∈ Cn×m, b ∈ Cm . Similarly, we can define the projection onto a complex
matrix if the input X is a projector (e.g. one-hot vector).
The activation function φ in all of the above definitions can be a real function φ : C → ’ or complex
function φ : C → C, but the function always acts on a complex variable. We will consider the choice
of the non-linear activation function φ in more detail in section 5.
The loss function J should be a real function J : C → ’ or J : ’ → ’. Since there is no total
ordering on the field of complex numbers, because the i2 = -1, a complex-valued function may lead
to added difficulties in training. To be able to interpret the output of the last layer as probability one,
can use an additional activation function. Thus the activation of the output layer is sigmoid(φ(z))
resp. so f tmaX(φ(z)) with φ : C → ’ and is used as a real number in a classical loss function (e.g.
cross entropy).
Both activation and loss functions are not always complex-differentiable. Hence, the training process
in the complex domain differs. Similar to a real function, a complex function f : C → C at a point
Z0 ∈ Ω ⊂ C of an open subset Ω is complex-differentiable if there exists a limit such that
0	f (z) - f (z0)
f (ZO) = limz|toz0	(3)
z - z0
A complex-valued function of one or more complex variables that is entire and complex differen-
tiable is called holomorphic. While in the real-valued case the existence of a limit is sufficient for
differentiability, the complex definition in equation 3 implies a stronger property. We map C to ’2
to illustrate this point. A complex function f(X + iy) = u(X, y) + iv(X, y) with real-differentiable
functions u(X, y) and v(X, y) is complex-differentiable if they satisfy the Cauchy-Riemann equations:
old万6ld一19∂
==
old一(9∂加一方
(4)
We simply separate a complex number Z ∈ C into two real numbers Z = X + iy. For f to be
holomorphic, the limit not only needs to exist for the two functions u(X, y) and v(X, y), but they
must also satisfy the Cauchy-Riemann equations. That also means that a function can be non-
holomorphic (not complex-differentiable) in Z, but still be analytic in its parts X, y. That is exactly if
the two functions are real-differentiable, but do not satisfy the Cauchy-Riemann equations.
To be able to apply the chain rule, the basic principle of backpropagation, to non-holomorphic
functions, we exploit the fact that many non-holymorphic functions, are still differentiable in their
real and imaginary parts. We consider the complex function f to be a function of Z and its complex
conjugate Z. Effectively, We choose a different basis for our partial derivatives.
∂	1∂	∂
∂Z = 2(∂X -∂y),
dz = 1( ∂X + i 齐
(5)
These derivatives are a consequence of the Wirtinger calculus (or CR-calculus). With the new ba-
sis we are able allow the application of the chain rule to non-holomorphic functions for multiple
complex variables Zi :
六(f ◦ g)=X 手◦
∂Zi	∂Zj
j=1
+
◦
,∂l(f ◦ g)
◦
+
◦
(6)
3
Under review as a conference paper at ICLR 2018
4 Real and Complex Parameters
In this section we discuss complex- and real-valued parameters in consideration of the number of
parameters per layer and the structure assumed by complex numbers.
Any complex number Z = X + iy = r * elW can be represented by two real numbers: the real part
Re(Z) = x and the imaginary part Im(Z) = y or as length or magnitude |z| =，X2 + y2 = r and a phase
φ. Effectively the number of real parameters of each layer is doubled: PC = 2P’.
The number of (real-valued) parameters is a metric of capacity or the ability to approximate func-
tions. Too many and a neural network tends to overfit the data, too few and the neural network tends
to underfit.
Fora comparison of architectures the real-valued parameters per layer should be equal (or at least as
close as possible) in the real architecture and its complex counter part. This ensures that models have
the same capacity and a comparison shows the performance difference due to the added structure,
not due to varying capacity.
Consider the number of parameters in a fully-connected layer in the real case and in the complex
case. Let n be the input dimension and m the number of neurons.
P’ = (n × m) + m, PC = 2(n × m) + 2m	(7)
For a multi-layer perceptron with k the number of hidden layers, and output dimension c the number
of real-valued parameters without bias is given by:
P’ = n × m +	k(m	× m) + m × c, PC	=	2(n	× m) +	2k(m	× m) + 2(m ×	c),	(8)
At first glance designing comparable multi-layer neural network architectures, i.e. they have the
same number of real-valued parameters in each layer, is trivial. Halving the number of neurons
in every layer will not achieve a comparison, because the number of neurons define the output
dimensions of the layer and the following layer’s input dimension. We adressed this problem by
choosing MLP architectures with an even number of hidden layers k and choose the number of
neurons per layer to be alternating between m and mm. Thus we receive the same number of real
parameters in each layer compared to a real-valued network. As an example, let us consider the
dimensions of outputs and weights in k = 4. For the real-valued case:
Input layer
Hidden layer
(1 × n) (n × m1) → (1 × m1) (m1 × m2)
Hidden layer	Hidden layer
z }| {	z }| {
→ (1 × m2) (m2 × m3) → (1 × m3) (m3 × m4)
Hidden layer	Output layer
z }| {	z }| {
→ (1 × m4) (m4 × m5) → (1 × m5) (m5 × c)
(9)
Modelout Put
z }| {
→ (1 × c)
where ml is the number of (complex or real) neurons of the l-th layer. The equivalent with m
complex-valued neurons would be:
(1 × n)(n × m21) → (1 × m21)(m × m2)
m3	m3 m3
→ (I × m2)(m2 × ɪ) → (I × ^2^)(^2^ × m4)
→ (1 X m4)(m4 X m5) → (1 X m5)(m X C)
→ (1 × c)
(10)
Another approach to the design of comparable architectures is to work with a parameter budget.
Given a fixed budget of real parameters P’ we can define real or complex multi-layer perceptron
4
Under review as a conference paper at ICLR 2018
with an even number k of hidden layers such that the network’s parameters are within that budget.
All k + 2 layers have the same number of real-valued neurons m’ or complex-valued neurons mC
m C
{P R
n+c ,
n+C,
-^2T +
ifk=0
otherwise
'P R
2(n+c),
n+c .
-IF +
ifk=0
业签）2+患
otherwise
(11)
(12)
J卷)2 +年,
Despite the straight forward use and representation in neural networks, complex numbers define an
additional structure compared to real-valued networks. This interaction of the two parts becomes
particularly apparant if we consider operations on complex numbers to be composed of the real and
imaginary part or magnitude and phase:
z1 z2 = (a + ib)(c + id) = (ac - bd) + i(ad + bc),
z1 + z2 = (a + ib) + (c + id) = (a + c) + i(b + d)
(13)
with complex numbers z1 = a + ib, z2 = c + id. In an equivalent representation with Euler’s constant
eW = Cos(φ) + isin(φ) the real parts do not interact.
Z1 z 2 = (r 1 e W1)( r2 el W2) = (r 1 r2 el wi+w2),
z1 + z2 = (r1 eiW1 ) + (r2 eiW2)
= r1cos(W1) + r2cos(W2) + i(r2 sin(W2) + r1sin(W1))
(14)
Complex parameters increase the computational complexity of a neural network, since more opera-
tions are required. Instead ofa single real-valued multiplication operation, up to four real multplica-
tion and two real additions are required. Depending on the implementation and the representation,
this can be significantly reduced.
Nevertheless, it is not sufficient to double the numbers of real parameters per layer to achieve the
same effect as in complex-valued neural networks. This is also illustrated expressing a complex
number z = a + ib ∈ C as 2 × 2 matrices M in the ring of M2(R):
so
Ma,b =	x y	-y x	with Mi =		0 1	-1 0	, M1 =		10 01	(15)
		a	-b c	-d		ac	- bd	bc + dc		(16)
Ma,b Mc,d =		b	ad	c	=	bc + dc		ac - bd		
An augmented representation ofan input x allows the represention of complex matrix multiplication
with an weight matrix W as larger real matrix multiplication:
Wx=
Re(W)
Im(W)
-Im(W) Re(x) Re(W)Re(x) - Im(W)Im(x)
Re(W) Im(x) = I m(W)Re( x) + I m( x)Re(W)
(17)
This added structure, however, also means that architecture design needs to be reconsidered. A deep
learning architecture which performs well with real-valued parameters, may not work for complex-
valued parameters and vice versa. In later sections we experimentally investigate what consequences
this particular structure has for the overall performance of a model.
5	Activation Functions
In any neural network, real or complex, an important decision is the choice of non-linearity. With the
same number of parameters in each layer, we are able to study the effects activation functions have
5
Under review as a conference paper at ICLR 2018
on the overall performance. The Liouville Theorem states that any bounded holomorphic function
f : CtoC (that is differentiable on the entire complex plane) must be constant.
Hence, we need to choose unbounded and/or non-holomorphic activation functions. We chose the
identity function to investigate the performance of complex models assuming a function which is
linearly separable in the complex parameters by not introducing a non-linearity into the model. The
hyperbolic tangents is a well-studied function and defined for both complex and real numbers. The
rectifier linear unit is a well-studied function and illustrates the separate application of an activation
function. The magnitude and squared magnitude functions are chosen to map complex numbers to
real numbers.
•	Identity (or no activation function):
•	Hyperbolic tangent:
φ(z) = tanh(z)
φ(z) = z
Sinh (Z)	ez - e-Z	e2Z - 1
cosh (z)	ez + e-Z	e2 Z + 1
(18)
(19)
•	Rectifier linear unit (relU):
φ( Z) = relU (Z) = relU (Re(Z)) + relU (Im (Z)) j = max (0, Re (Z)) + max (0, Im (Z)) j	(20)
• Magnitude squared:
φ(Z) = |Z|2 = x2 + y2
(21)
•	Magnitude (or complex absolute):
φ(Z)
(22)
In the last layer we apply an activation function φ : C → ’ before using the softmax or sigmoid to
use in a receive a real loss. Note that the squared magnitude allows a more efficient implementation
than the magnitude. Thus we change the activation function of the last layer to:
sigmoid(|Z|2) =
1
1 + e-X 2-俨
(23)
For an output vector Z = [Z0,Z1, . . . ,Zc]
2	ex2+y2
softmaχ(|Zj| ) = PC ] eχ2+ 俨
(24)
Applying the two functions in the opposite order as in |sigmoid(Z)|2 and |so f tmax(Z)|2 does not return
probabilities from the last layer of a network and would take away the direct interpretability of the
models output.
6	Experiments
To compare real and complex-valued neural networks and their architecture we chose various clas-
sification tasks and defined experiments. The settings are as follows:
•	Experiment 1: We tested multi-layer perceptrons (MLP) with with k = 0, 2, 4, 8 hidden
layers, fixed width of units in each layer in real-valued architectures and alternating 64, 32
units in complex-valued architectures (see section 4), no fixed budget, applied to Reuters
topic classification , MNIST digit classification, CIFAR-10 Image classification, CIFAR-
100 image classification. Reuters topic classification and MNIST digit classification use
64 units per layer, CIFAR-10 and CIFAR-100 use 128 units per layer. All tested activation
functions are introduced in 5.
6
Under review as a conference paper at ICLR 2018
•	Experiment 2: We tested multi-layer perceptrons (MLP) with fixed budget of 500,000 real-
valued parameters, no fixed width, applied to MNIST digit classification, CIFAR-10 Image
classification, CIFAR-100 image classification and Reuters topic classification. All tested
activation functions introduced are in section 5. Used sigmoid(|z|2) function for the gates.
•	Experiment 3: We tested the Memory Network architecture introduced by Weston et al.
(2014) as complex-valued network in two versions - one below and one above parameter
budget of the real-valued network. We used the bAbI question answering tasks with one,
two and three supporting facts. Activation functions in each layer were defined by the
original publication. The network used a recurrent layer, which defined by replacing the
real-valued weight matrices with complex weight matrices.
For all of our experiments we used the weight initialisation discussed by (Trabelsi et al., 2017).
However, to reduce the impact of the initialisation we ran each model at least 10 times. The larger
memory networks were initialised 30 times. All models were trained over 100 epochs with an
Adam optimisation. We used categorical or binary cross entropy as a loss function for all of our
experiments and models. We used sigmoid(|z|2) or so f tmax(|z|2) as activation function for the last
layer of the complex models.
7	Results and Discussion
Tables 1, 2, 3, 4 show the results for experiment 1. Generally, the performance of complex and
real neural network in this setting is similar, altough the complex valued neural network tends to
perform slightly worse. We found that the best choice for the activation function for the complex
neural network is relu applied separatly to the imaginary and real parts. Suprisingly the hyperbolic
tangents tanh and squared magnitude |z|2 perform significantly worse
Tables 5, 6, 7, 8 show the results for experiment 2. Similar to experiment 1 the results show that the
best choice for the activation function for the complex neural network is relu applied separatly to the
imaginary and real parts. In both experiments with the depth of the architecture the performance of
the complex neural network decreases significantly. These experiments illustrate that an increased
width per layers outperforms an increased depth in classification tasks. This is true for the real and
the complex case.
Table 9 shows the results for experiment 3. For a single supporting fact in the bABi data set real-
valued neural network. In the first bABi task the real-valued version outperforms the two complex
version of the memory network. In the more diffcult tasks with two or three supporting facts both, the
small and large version, of the complex-valued neural network outperform the real-valued version -
despite the reduce number of parameters.
We made the observation that the assumed structure with introducing complex numbers into neural
networks has a regularising effect on the training procedure if used in combination with real-valued
input. We also found that complex-valued neural networks are more sensitive towards their initiali-
sation than real-valued neural networks.
Overall the complex-valued neural networks do not perform as well as expected. This may be due
to the nature of the chosen tasks or the simple architecture of a multi-layer perceptron. Complex
neural networks should be used if the data is naturally in the complex domain or can be mapped
to complex numbers. The architecture should be selected to respect the expected structure complex
numbers introduce to the network.
In conclusion the architecture needs to reflect the interaction of the real and imaginary part. If the
structure is ignored, the model will not perform as well as the corresponding real-valued network.
Acknowledgments
Acknowledgements are hidden due to double blind review.
7
Under review as a conference paper at ICLR 2018
References
Igor Aizenberg. Complex-Valued Neural Networks with Multi-Valued Neurons. Springer Publishing
Company, Incorporated, 2016. ISBN 3662506319, 9783662506318.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.
CoRR, abs/1511.06464, 2015.
N. Benvenuto and F. Piazza. On the complex backpropagation algorithm. IEEE Transactions on
SignalProcessing,40(4):967-969, Apr 1992. ISSN 1053-587X. doi: 10.1109/78.127967.
Joan Bruna, Soumith Chintala, Yann LeCun, Serkan Piantino, Arthur Szlam, and Mark Tygert. A
theoretical argument for complex-valued convolutional networks. CoRR, abs/1503.03438, 2015.
G. M. Georgiou and C. Koutsougeras. Complex domain backpropagation. IEEE Transactions on
Circuits and Systems II: Analog and Digital Signal Processing, 39(5):330-334, May 1992. ISSN
1057-7130. doi: 10.1109/82.142037.
Nitzan Guberman. On complex valued convolutional neural networks. CoRR, abs/1602.09046,
2016.
R. Haensch and O. Hellwich. Complex-valued convolutional neural networks for object detection
in polsar data. In 8th European Conference on Synthetic Aperture Radar, pp. 1-4, June 2010.
A. Hirose. Complex-valued neural networks: The merits and their origins. In 2009 International
Joint Conference on Neural Networks, pp. 1237-1244, June 2009. doi: 10.1109/IJCNN.2009.
5178754.
Akira Hirose. Complex-Valued Neural Networks: Theories and Applications (Series on Innovative
Intelligence, 5). World Scientific Press, 2004. ISBN 9812384642.
Masaki Kobayashi. Dual-numbered hopfield neural networks. IEEJ Transactions on Electrical and
Electronic Engineering, 2017. ISSN 1931-4981.
T. Nitta. A back-propagation algorithm for complex numbered neural networks. In Proceedings of
1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan), volume 2, pp.
1649-1652 vol.2, Oct 1993.
Tohru Nitta. Learning dynamics of the complex-valued neural network in the neighborhood of
singular points. Journal of Computer and Communications, 2(1):27-32, 2014. doi: 10.4236/jcc.
2014.21005.
C. A. Popa. Complex-valued convolutional neural networks for real-valued image classification. In
2017 International Joint Conference on Neural Networks (IJCNN), pp. 816-822, May 2017. doi:
10.1109/IJCNN.2017.7965936.
Andy M. Sarroff, Victor Shepardson, and Michael A. Casey. Learning representations using
complex-valued nets. CoRR, abs/1511.06351, 2015.
Chiheb Trabelsi, Sandeep Subramanian, Negar Rostamzadeh, Soroush Mehri, Dmitriy Serdyuk,
Joao FeliPe Santos, Yoshua Bengio, and Christopher PaL Deep complex networks. 2017.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Enc GauSSier, and Guillaume Bouchard. Com-
plex embeddings for simple link prediction. In International Conference on Machine Learning
(ICML), volume 48, pp. 2071-2080, 2016.
Theo Trouillon, Christopher R. Dance, Johannes Welbl, Sebastian Riedel, Enc GaUSSier, and
Guillaume Bouchard. Knowledge graph completion via complex tensor factorization. CoRR,
abs/1702.06879, 2017.
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. CoRR, abs/1410.3916, 2014.
8
Under review as a conference paper at ICLR 2018
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity
unitary recurrent neural networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 29,pp. 4880-4888. Curran
Associates, Inc., 2016.
Hans-Georg Zimmermann, Alexey Minin, and Victoria Kusherbaeva. Comparison of the complex
valued and real valued neural networks trained with gradient descent and random search algo-
rithms. In ESANN, 2011.
9
Under review as a conference paper at ICLR 2018
Table 1: Test accuaracy of multi-layer perceptron conisting of k + 2 dense layers each with 64 units
on MNIST digit classification task. Selected from best of 10 runs with each run 100 epochs to
converge.
Hidden layers k	Real parameters p’	Activation function 中	MNIST	
			’	C
k = 0	50,816	identity	0.9282	0.9509
		tanh	0.9761	0.9551
		relU	0.9780	0.9710
		IN2	0.9789	0.9609
		IW	0.9770	0.9746
k = 2	59,008	identity	0.9274	0.9482
		tanh	0.9795	0.8923
		relU	0.9804	0.9742
		IN2	0.9713	0.6573
		IW	0.9804	0.9755
k = 4	67,200	identity	0.9509	0.9468
		tanh	0.9802	0.2112
		relU	0.9816	0.9768
		IN2	0.8600	0.2572
		IW	0.9789	0.9738
k = 8	83,584	identity	0.9242	0.1771
		tanh	0.9796	0.1596
		relU	0.9798	0.9760
		IN2	0.0980	0.098
		IW	0.9794	0.1032
Appendix
10
Under review as a conference paper at ICLR 2018
Table 2: Test accuaracy of multi-layer perceptron conisting of k + 2 dense layers each with 64 units
on Reuters topic classification task. Selected from best of 10 runs with each run 100 epochs to
converge.
Hidden layers k	Real parameters p’	Activation function 中	Reuters	
			’	C
k = 0	642,944	identity	0.8116	0.7939
		tanh	0.8117	0.7912
		relU	0.8081	0.7934
		IN2	0.8050	0.7885
		IW	0.8068	0.7992
k = 2	651,136	identity	0.8005	0.7836
		tanh	0.7978	0.7320
		relU	0.7921	0.7854
		IN2	0.7725	0.6874
		IW	0.7996	0.7823
k = 4	659,328	identity	0.7925	0.7787
		tanh	0.7814	0.4199
		relU	0.7734	0.7671
		IN2	0.5895	0.0650
		IW	0.7863	0.7694
k = 8	675,712	identity	0.7929	0.7796
		tanh	0.7542	0.1861
		relU	0.7555	0.7676
		IN2	0.0053	0.0053
		IW	0.7671	0.7524
11
Under review as a conference paper at ICLR 2018
Table 3: Test accuaracy of multi-layer perceptron conisting ofk+2 dense layers each with 128 units
on CIFAR-10 image classification task. Selected from best of 10 runs with each run 100 epochs to
converge.
Hidden layers k	Real parameters p’	Activation function 中	CIFAR-10	
			’	C
k = 0	394,496	identity	0.4044	0.1063
		tanh	0.4885	0.1431
		relU	0.4902	0.4408
		IN2	0.5206	0.1000
		IW	0.5256	0.1720
k = 2	427,264	identity	0.4039	0.1000
		tanh	0.5049	0.1672
		relU	0.5188	0.496
		IN2	0.1451	0.1361
		IW	0.5294	0.1000
k = 4	460,032	identity	0.4049	0.1000
		tanh	0.4983	0.1549
		relU	0.8445	0.6810
		IN2	0.1000	0.1000
		IW	0.5273	0.1000
k = 8	525,568	identity	0.4005	0.1027
		tanh	0.4943	0.1365
		relU	0.5072	0.4939
		IN2	0.1000	0.1000
		IW	0.5276	0.1000
12
Under review as a conference paper at ICLR 2018
Table 4: Test accuaracy of multi-layer perceptron conisting of k + 2 dense layers each with 128 real
and 64 complex units on CIFAR-100 image classification task. Selected from best of 10 runs with
each run 100 epochs to converge.
Hidden layers k	Real parameters p’	Activation function 中	CIFAR-100	
			’	C
k = 0	406,016	identity	0.1758	0.0182
		tanh	0.2174	0.0142
		relU	0.1973	0.1793
		IN2	0.2314	0.0158
		IW	0.2423	0.0235
k = 2	438,784	identity	0.1720	0.0100
		tanh	0.2314	0.0146
		relU	0.2400	0.2123
		IN2	0.0143	0.0123
		IW	0.2411	0.0100
k = 4	471,552	identity	0.1685	0.0100
		tanh	0.2178	0.0157
		relU	0.2283	0.2059
		IN2	0.0109	0.0100
		IW	0.2313	0.0100
k = 8	537,088	identity	0.1677	0.0100
		tanh	0.2000	0.0130
		relU	0.2111	0.1956
		IN2	0.0100	0.0100
		IW	0.2223	0.0100
13
Under review as a conference paper at ICLR 2018
Table 5: Test accuaracy of multi-layer perceptron conisting of k + 2 dense layers with an overall
budget of 500, 000 real-valued parameters on MNIST digit classification. Selected from best of 10
runs with each run 100 epochs to converge.
Hidden layers k	Units		Activation function 中	MNIST	
	m ’	m C		’	C
k = 0	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			IW2	0.0000	0.0000
			IW	0.0000	0.0000
k = 2	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			I #	0.0000	0.0000
			IW	0.0000	0.0000
k = 4	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			IW2	0.0000	0.0000
			IW	0.0000	0.0000
k = 8	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			I #	0.0000	0.0000
			IW	0.0000	0.0000
14
Under review as a conference paper at ICLR 2018
Table 6: Test accuaracy of multi-layer perceptron conisting of k + 2 dense layers with an overall
budget of 500, 000 real-valued parameters on Reuters topic classification. Selected from best of 10
runs with each run 100 epochs to converge.
Hidden layers k	Units		Activation function 中	Reuters	
	m ’	m C		’	C
k = 0	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			IW2	0.0000	0.0000
			IW	0.0000	0.0000
k = 2	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			I #	0.0000	0.0000
			IW	0.0000	0.0000
k = 4	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			IW2	0.0000	0.0000
			IW	0.0000	0.0000
k = 8	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			I #	0.0000	0.0000
			IW	0.0000	0.0000
15
Under review as a conference paper at ICLR 2018
Table 7: Test accuaracy of multi-layer perceptron conisting of k + 2 dense layers with an overall
budget of 500, 000 real-valued parameters on CIFAR-10 image classification. Selected from best of
10 runs with each run 100 epochs to converge.
Hidden layers k	Units		Activation function 中	CIFAR-10	
	m ’	m C		’	C
k = 0	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			IW2	0.0000	0.0000
			IW	0.0000	0.0000
k = 2	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			I #	0.0000	0.0000
			IW	0.0000	0.0000
k = 4	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			IW2	0.0000	0.0000
			IW	0.0000	0.0000
k = 8	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			I #	0.0000	0.0000
			IW	0.0000	0.0000
16
Under review as a conference paper at ICLR 2018
Table 8: Test accuaracy of multi-layer perceptron conisting of k + 2 dense layers with an overall
budget of 500, 000 real-valued parameters on CIFAR-100 image classification. Selected from best
of 10 runs with each run 100 epochs to converge.
Hidden layers k	Units		Activation function 中	CIFAR-100	
	m ’	m C		’	C
k = 0	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			I #	0.0000	0.0000
			IW	0.0000	0.0000
k = 2	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			IW2	0.0000	0.0000
			IW	0.0000	0.0000
k = 4	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			I #	0.0000	0.0000
			IW	0.0000	0.0000
k = 8	500000	250000	identity	0.0000	0.0000
			tanh	0.0000	0.0000
			relU	0.0000	0.0000
			IW2	0.0000	0.0000
			IW	0.0000	0.0000
Table 9: Test accuaracy of Memory Networks (Weston et al., 2014) in complex and real version on
the first three bAbI tasks. Selected from best of 30 runs with each run 100 epochs to converge.
	Parameters P ’	Supporting facts		
		One	TWo	Three
Memory Network	24,750	0.9920	0.4030	0.3830
Memory Network (small)	-18,716	0.9570	0.4250	0.3820
Memory Network (large)	24,456	0.9790	0.4420	0.3920
17