Under review as a conference paper at ICLR 2018
Deep Hyperspherical Defense against
Adversarial Perturbations
Anonymous authors
Paper under double-blind review
Ab stract
Recent studies show that deep neural networks are extremely vulnerable to adver-
sarial examples which are semantically indistinguishable from natural data and
yet incorrectly classified. These adversarial examples are generated from the nat-
ural data by adding a small amount of adversarial perturbation. This paper tackles
the adversarial attack problem with hyperspherical defense - a defense strategy
that learns neural network over hyperspheres. The hyperspherical defense frame-
work is well motivated by: (i) Learning on hyperspheres gives us bounded output,
which may make the geometry of neural networks more smooth; (ii) Learning on
hyperspheres could naturally eliminate some adversarial perturbations and reduce
the effect of adversarial perturbations; (iii) Representing data on hyperspheres se-
lectively drops some information of the inputs, but these information are shown
to be not crucial to visual recognition (based on the fact that hyperspherical neu-
ral network performs comparable to or even better than standard neural networks
in visual recognition). Furthermore, we introduce the hyperspherical compact-
ness and propose a robust geodesic inference. We also provide theoretical insights
about why our hyperspherical defense improves robustness. Last, we validate the
superiority of hyperspherical defense with extensive experiments on both white-
box and black-box adversarial attacks on multiple datasets.
1 Introduction
Recent years have witnessed great breakthroughs in deep learning in a variety of applications ranging
from face recognition (Taigman et al., 2014), object detection/recognition (Girshick et al., 2014; He
et al., 2016) and speech recognition (Amodei et al., 2016) to machine translation (Sutskever et al.,
2014) and gaming (Silver et al., 2016). However, recent studies show that most of these machine
learning models (especially deep models) are very vulnerable to adversarial attacks. Such attacks
can seriously undermine the security of a system supported by the deep learning models, causing
the reliability problem in autonomous driving, biometric authentication, etc. Specifically, attackers
transform a normal sample to a malicious one by simply adding a small amount of perturbations.
These malicious samples (i.e. adversarial examples) are semantically indistinguishable from the
normal ones, but usually will be incorrectly classified by deep learning models.
Due to the significance in practice, both adversarial attack and defense in deep learning have received
particular attention. While we have seen many work on generating successful attacks (Szegedy et al.,
2013; Goodfellow et al., 2014; Nguyen et al., 2015), finding an effective defense strategy remains an
open challenge. In this paper, we present a simple yet effective hyperspherical defense framework
to improve the robustness against adversarial attacks. The basic idea is to constrain the network
learning onto a hypersphere. Specifically for CNNs, we basically replace the inner product between
the kernel and the local patch with hyperspherical correlation. The hyperspherical correlation is
defined as a function of the angle between two input vectors (feel free to think about cos(θ) as a
simple example). The hyperspherical correlation is supposed to be upper and lower bounded in
general, making the output of each layer also bounded. Because hyperspheres are the simplest
Riemannian manifold, we are essentially transforming the learning space from Euclidean space to a
manifold space. Our direct intuition comes from the observation from Fig. 1. As one could see from
Fig. 1, the geodesic (angular) distance on the unit sphere corresponds to the semantic difference,
while the distance on the radius direction corresponds to the intra-class variation (their labels are
the same). Because the adversarial examples usually will not change the semantic meaning of an
1
Under review as a conference paper at ICLR 2018
image, it is very likely that the adversarial perturbation has large components on the radius direction
(verified by Fig. 2(a)). Naturally, it inspires us to learn the neural network on the hypersphere such
that many adversarial perturbations could alleviated or eliminated. In the following sections, we will
give more empirical and theoretical justifications for this observations.
Our motivations to learn a neural network on hyper-
spheres against adversarial attacks mainly lie in three as-
pects. First, learning on unit hyperspheres can increase
the smoothness of the network (more smooth than the
standard neural networks). As one may see, many of
the recent works (Cisse et al., 2017; Miyato et al., 2015)
highly favor the smoothness in the network, because it
can make the effect of adversarial perturbations small and
also empirically perform well. They usually achieve the
smoothness by modifying learning objectives or adding
additional constraints, while ours achieves it in a more
explicit way by directly constraining the network to learn
on a unit hypersphere. Second, projection from Euclidean
space onto hyperspheres naturally alleviates the effect of
some adversarial perturbations. For example, if the ad-
150
100
50
0
-50
-100
-150
Intra-Class Variation
o
1
2
3
4
6
7
8
9
Semantic/Label Difference
-150	-100	-50	0	50	100	150
The first dimension
Figure 1: 2D CNN Features on MNIST.
versarial perturbations point to the radius direction of a hyphersphere, then such perturbations will
not affect the final prediction. Third, we observe that hyperspherical neural networks achieve com-
parable or better performance in visual recognition with much smaller parameter space (Liu et al.,
2017), indicating that it preserves the semantic information (information related to the label) of an
image. As we know, the adversarial perturbations will not affect the semantic meaning of an image,
so it is very likely that learning on hyperspheres can drop most of the components of the adversarial
perturbations.
Our major contributions can be summarized as follows:
•	We introduce a simple yet effective robust strategy to train a neural network with strong robust-
ness against adversarial perturbations. Specifically, we present a defensive hyperspherical neural
networks (D-SphereNet) that are learned on hyperspheres, and propose two new hyperspherical
correlations: quadratic one and input-truncated one.
•	Besides adapting hyperspherial learning to train a robust neural network, we further propose sev-
eral learning objectives towards hyperspherical compactness and robust inference methods.
•	We also give partial theoretical insights for the advantages of utilizing hyperspherical defense to
improve robustness against adversarial attacks.
•	Most importantly, our D-SphereNet yields strong and consistent empirical performance in resist-
ing both black-box and white-box adversarial attacks.
2	Related work
There is an increasingly growing body of work on generating adversarial examples (Szegedy et al.,
2013; Goodfellow et al., 2014; Nguyen et al., 2015). Szegedy et al. (2013) observes that a deep
convolutional neural networks (CNNs) can easily fail with a small amount of artificial perturbations.
They use box-constrained Limited-memory BFGS algorithm to obtain adversarial examples that
are transferrable across different neural network architectures. Goodfellow et al. (2014) make an
attempt to explain that it is the linear nature of CNNs that causes such vulnerabilities. They also
propose the fast gradient sign method (FGSM) to efficiently generate adversarial examples. Some
recent work (Papernot et al., 2016b; Fawzi et al., 2015; Nguyen et al., 2015; Moosavi-Dezfooli et al.,
2016b;a; Madry et al., 2017) also explore adversarial attacks in depth and try to find other ways to
generate adversarial examples accurately and efficiently. These adversarial examples are generally
transferable across different network architectures, which also makes the black-box attacks possible.
Current most prevailing defense strategies can be viewed as modifying objective functions, such as
adversarial training (Szegedy et al., 2013; Goodfellow et al., 2014), smooth regularization (Miyato
et al., 2015; Zheng et al., 2016), defensive distillation (Papernot et al., 2016c), Parseval training
2
Under review as a conference paper at ICLR 2018
(Cisse et al., 2017) etc. One of the most effective defense by far is the adversarial training (Szegedy
et al., 2013; Goodfellow et al., 2014). It incorporates adversarial examples into the training stage
to improve the robustness, so it is time-consuming and may only be effective to the adversarial
examples that have appeared in the training stage. Lu et al. (2017) construct a detection network to
detect and reject the adversarial examples, but it does not improve the robustness of deep models.
Different from the current defense strategies, hyperspherical defense effectively combines robust
learning architecture, compactness learning objective and improved inference method into a unified
framework and significantly improves the robustness of deep models.
Our hyperspherical defense is largely inspired by hyperspherical learning which is first proposed in
(Liu et al., 2017) mostly as a way to improve the convergence of learning neural networks. Differ-
ent from (Liu et al., 2017), our hyperspherical defense framework generalizes the the hyperspherical
convolutional operator to a more general concept - hyperspherical correlation (SphereCorr). Besides
generalizing the exisitng hyperspherical convolutions to SphereCorr, we also propose the quadratic
SphereCorr to introduce more non-linearity and the input-truncated SphereCorr for stronger ro-
bustness. Furthermore, we utilize a hyperspherical compactness objective function and a robust
inference method, which can effectively resist and reject adversarial examples. We also general-
ize the hyperspherical setup to a mixed setup of hypersphere and simplices. This not only makes
learning more flexible, but also make it difficult when one hacks the structure of the neural network
(black-box attack).
3	Some Observations about Adversarial Perturbations
Before delving into the details, we first motivate the readers with a few empirical observations about
adversarial perturbations. Our initial intuitions and motivations are built upon them.
150
100
S
<υ
50
0
C -
S
-50
q
F
-100
-150
-150	-100	-50	0	50	100	150
The first dimension
(a) Trajectory of an adversarial example
150
100
C-
50
P
0
-50
F
-100
-150
-150	-100	-50	0	50	100	150
The first dimension
(b) Successful adversarial examples in CNN
(c) Successful adversarial examples in D-SphereNet
Figure 2: 2D CNN Features on MNIST. These 2D features are directly learned from the neural network by
setting the feature dimension as 2. Feature dimension refers to the output dimension of the fully connected
layer before the 10-class classifiers. These 2D features are true distributions from the neural network, rather
than generated some visualization methods. Fig. 1 is also plotted using the same method.
Adversarial examples tend to approach the origin first and then shift to another class. In
Fig. 2(a), we draw the trajectory (purple line) of an adversarial example. The trajectory is computed
by varying λ from 0 to 1 in X + λ ∙ FGSM where X is the normal sample and FGSM denotes the
adversarial perturbations generated by the FGSM. Specifically, we use a normal image from digit
3 and gradually increase the magnitude of the adversarial perturbation. We observe from Fig. 2(a)
that the adversarial example will approach to the origin first and then start shifting to another class.
The trajectory indicates that it is easier to maliciously change the predicted label of a network from
a less semantic space1. It motivates us to learn representation on a hypersphere, which can cause the
adversarial perturbation much more effort in order to fool the network (the adversarial perturbations
can no longer approach to the origin to affect the output label distribution).
Adversarial examples is more seperable from the normal data in D-SphereNet. In Fig. 2(b)
and Fig. 2(c), we plot the adversarial examples of digit 0 that have successfully fooled the network.
Fig. 2(b) shows that the adversarial examples of standard CNNs overlap with a large number of
natural data. In contrast, we can see in Fig. 2(c) that the adversarial examples of D-SphereNets
1We view the feature space near the origin as a less semantic space, because in origin it only requires small
perturbations to change from one class to another.
3
Under review as a conference paper at ICLR 2018
are clearly separable from all the natural data, which also means the adversarial examples will not
appear in where the natural data are dense. It clearly shows the advantages of D-SphereNets.
Hyperspherical defense can better group normal data from different classes. We could also
observe from Fig. 2(b) and Fig. 2(c) that the D-SphereNets could better group the natural data of
the same class together. The embedding space2 for natural data in D-SphereNets are much more
compact than standard CNNs, which motivates us to propose the robust inference.
4 Hyperspherical Defense
4.1	Defensive Hyperspherical Neural Networks
The proposed D-SphereNet can be viewed as a neural network
that learns its parameters on hyperspheres. To achieve this, we
measure the correlation between the input and weights on hyper-
spheres instead of the original inner product. The hyperspherical
correlation (SphereCorr) is defined as
hw, xiM = g(w, x)
(1)
which can be viewed as a projected inner product defined on hy-
persphere M. Similar to (Liu et al., 2017), g(w, x) have differ-
ent instantiations.
Angle between input vector and weight vector
Cosine SphereCorr. The simplest form is the cosine hyper-
spherical correlation: g(w, x) = cos(θw,x) where θw,x denotes Figure 3: Different SphereCorr.
the angle between the input vector x and the weight vector w.
The cosine SphereCorr is closely related to the geodesic distance on the unit hypersphere. Fol-
lowing (Liu et al., 2017), we could also derive linear SphereCorr g(w, x) = aθw,x + b, sigmoid
SphereCorr, etc. Generally, we make the output of SphereConv to be bounded from below and above
and decrease monotonically with the angle θw,x .
Quadratic SphereCorr. Inspired by the observation that the RBF networks have nearly no ad-
versarial examples and the claim that adversarial examples are partially caused by the neural net-
work being too “linear” (Goodfellow et al., 2014), we increase the non-linearity by incorporating a
quadratic effect to form a composite function:
g(w, x) = 1 - a(1 - cos(θw,x))2
(2)
where a > 0 is a parameter that controls the curvature of the quadratic SphereCorr. In this paper,
we mainly consider the case where a = 1 to make the output the same as the other SphereCorr.
The quadratic SphereCorr can bring the quadratic non-linearity to the neural network and poten-
tially make the network more non-linear. From Fig. 3, we could have an intuitive sense for these
SphereCorr functions. Linear SphereCorr and Cosine SphereCorr are two extreme cases, while the
quadratic SphereCorr is somewhere in the middle. Because we usually use rectified linear units
(ReLUs) in the network, We only show the valid range [0, 2] instead of [0, ∏] in Fig. 3.
Input-Truncated SphereCorr. If the norm of the input kxk is larger than 1, the input-truncated
SphereCorr will behave exactly like cosine SphereCorr. Differently, if kxk is smaller than 1, the
input-truncated SphereCorr will preserve kxk instead of normalizing it to 1. The specific form is
g(w, x) = min (1, kxk) cos (θw,x)	(3)
This SphereConv is still bounded in [-1, 1] and is especially robust in the sense that it could better
reduce the perturbation inside the hypersphere. For example, given kxk < 1, then its direction will
largely affect the output of the other SphereCorrs. However, because we have kxk in g(w, x), the
multiplicative factor kxk < 1 can reduce the effect of perturbations within the hypersphere. Thus the
input-truncated SphereCorr can adaptively reduce adversarial perturbations and achieve robustness.
Optimization. Because we replace the original inner product similarity with the SphereCorr, it will
cause some minor differences in optimization. We still use the back-prop to learn the parameters of
the network. Because θw,x is an intermediate variable, we need to use the chain rule to compute the
2The embedding space is the feature space before entering the classifier (i.e., last fully connected layer).
4
Under review as a conference paper at ICLR 2018
gradient with respect to W and x. We have dg∂W,X)= 驾：,X) ∙ dθWx and
∂g(w,x) _ ∂g(w,x) . ∂θw,χ
TT7i---- ∙ -F；-
∂θw,x ∂x
∂x
T
where θw,χ = cos( 1岗心口). With these chain rule computation, it is trivial to perform back-prop.
Learning objective. Because we are representing features on Hyperspheres and all the similarities
are also measured on hyperspheres, we also need to adapt the original softmax loss3 to our hyper-
spherical learning. Considering the binary classification, we have that the original decision boundary
is kW1 kkxk cos(θW1,x) = kW2kkxk cos(θW2,x) where Wi is the classifier for the i-th class and
x is the feature outputted from the fully connected layer before classifiers. To make the decision
merely depend on geodesic distance on unit hyperspheres, we only need to normalize the W1 and
W2 to the same length (we use length 1 here). Therefore, we usually use the weight-normalized
softmax (W-Softmax) loss in D-SphereNet instead of original softmax loss.
Encouraging Orthogonality. While we are computing all the similarities based on the unit hy-
persphere, we will no longer use the weight decay in our network. To encourage the direction of
weights to be uniformly spaced out, we impose add an orthogonality-inducing regularizer. Specif-
ically, given a set of kernels W where the i-th column vector Wi is the weights of the i-th kernel,
the orthogonality-inducing regularizer is written as kW>W - Ik2F, where I is identity matrix.4
Convolutional layers and fully connected layers in D-SphereNets. Using SphereCorr, we could
easily transform the standard convolutional layers and fully connected layers into the ones in D-
SphereNet by replacing the inner product with the SphereCorr.
4.2	Hyperspherical Compactness
From Fig. 2(b) and Fig. 2(c), we notice that if a testing sample is not close to any training data clouds,
then it is very likely to be an adversarial example. In order to better distinguish the adversarial
examples and the normal data, we need to make the features in the embedding space more compact
and well grouped. To this end, we propose some learning objectives for hyperspherical compactness
in this section. We start by looking into the W-Softmax loss:
L = NN X Li = NN X - log
ekxi kg(Wyi ,xi)
P , ekxikg(Wj,Xi)
(4)
i
i
where g(∙) is the SphereCorr, yi is the label of the sample Xi, Wj is the classifier corresponding to
the j-th class (i.e., the weights of the last fully connected layer). There are two ways to minimize
the W-Softmax loss to a low value. One is to increase g(Wyi, xi), while the other way is to increase
kxi k	given that	g(Wyi ,	xi)	is already larger than	g(Wj ,	xj ), j	6=	yi.	As a result, we can not
guarantee that the learned features have strong hyperspherical compactness. In order to guarantee
these features to be well grouped on hyperspheres, the simplest way is to constrain the norm of the
column weights to a small value. However, if such value is too small, the optimization is too difficult
and the loss will not decrease at all. To address this, we design two new objective functions:
Weight multiplier. Following the idea of constraining the norm of weights, we use α to denote
maxj kWj k. Then we can instead optimize the following constrained objective function:
1	I eα∙g(Wyi ,dSN(Xi)) ʌ
argmin — — — log I ------------/…	~ττ I s.t. 0 < α ≤ m
W,DSN(Xi) N Li	g IPj eα∙g(Wj，SN(Xi)) )	≤
(5)
where m is a preset parameter and DSN(xi) is the weights of the D-SphereNet with the input sample
xi . Because the inequality constraint is difficult to handle in neural network, we approximate it with
a unconstrained optimization which the neural network is easy to deal with:
1	e elɑl∙g(Wyi ,dSN(Xi)) ʌ
W即n,) N ? - log (Pj e∣α∣∙g(Wj,SN(Xi)) ) + η ∙ 1a1
(6)
where η is a hyperparameter that controls how small we want the norm of column weights to be.
3The softmax loss is defined as the combination of the last fully connected layer (i.e., classifiers), softmax
function and the cross-entropy loss.
4We do not directly apply an orthogonal transformation to W, because we find such a regularizer is more
computationally efficient in practice. We also adopt the stochastic manifold gradient optimization, since the
orthogonality constraint is essentially the Stefiel Manifold.
5
Under review as a conference paper at ICLR 2018
ekxi kg(Wyi ,xi)
P , ekxikg(Wj,xi)
l =- N X τ⅛λlog
i
Loss annealing. Alternatively, we could introduce a sphere-normalized softmax loss defined as:
1	1	eg(Wyi,xi)
=N 2 L = N } g IPj eg(Wj,χi) J	()
which simultaneously normalize both the column weighs Wj and the input feature xi . However,
this loss function defines a very difficult task that can not even converge. So we use an annealing
method to optimize such loss function. Eventually, we optimize a loss function as follows:
eg(Wyi ,xi)	λ
Pj eg(Wj,Xi) J + ι + λ log
where λ starts with a relatively large value and is usually a monotonically decreasing function (in
terms of iteration number) with a non-negative minimum. The intuition hehind is to learn the net-
work from a easy task (loss) first and gradually shift to a difficult task (loss).
4.3	Robust Geodesic Inference
To take full advantage of the hyperspherical compactness, we further design a robust geodesic in-
ference method that could help to reject adversarial examples and achieve robust recognition. The
geodesic inference is a simple two-step approach. We will first calculate the mean θ*j and the Stan-
dard deviation θσj of angles between all the j-th class training samples and classifier Wj (i.e., the
j-th column weights of the last fully connected layer).
Step 1. Given a testing sample, we use the softmax probability to perform standard neural network
inference and output the prediction label ybte. We can also obtain the features xte before entering the
last fully connected layer.
Step 2. We compute the angle θ between xte and the classifier Wyte. If the angle θ is larger than the
angular thresholdp(θ*j + θσj) where P is a hyperparameter controlling this threshold, We reject this
testing sample and regard it as adversarial attack. If θ is smaller than the threshold, we accept it and
the final prediction is its output label in Step 1. Note that, we usually set p to 1, but it could also be
set using cross validation.
4.4	Generalized Hyperspherical Defense
For now, we are still working on the `2 hypersphere which is one of the simplest manifold. Could we
actually generalize to different manifolds? In fact, we could easily generalize our hyperspherical de-
fense framework to generalized hyperspheres (e.g., simplex, hyperbola, affine). This not only make
the architecture more flexible, but also make it more difficult if one wants to hack the architecture of
the neural networks.
5 Theoretical Insights
Before we proceed, we denote some notations for simplicity.
Notations: Given a vector x = (x1, ..., xp)> ∈ Rp, sign(x) := (sign(x1), ..., sign(xp))>, we
define vector norms: kxk1 = Pj |xj |, kxk22 = Pj (xj)2. Define Sp-1 = {x ∈ Rp : kxk2 = 1}.
We provide theoretical insights of the hyperspherical defense in both adversarial training and attack.
Here is the high level intuition: The black-box adversarial attack assumes that the inputs of each
layer lie in the Euclidean space, and perturbs them in the Euclidean space. However, the inputs of
each layer in the D-Spherenet lie on a spherical manifold, and the adversarial perturbation can be
naturally reduced by “projecting the input onto the sphere” in each layer.
Due to the complex structures of the D-Spherenet, we illustrate the effectiveness of the projection
using an example based on linear model, which can be viewed as a single neuron in the network.
Similar approaches have been used in justifying other deep learning techniques such as dropout
(Srivastava et al., 2014; Wager et al., 2013). Before we proceed, we first introduce the following
linear model.
Assumption 1. We consider a linear model with random design,
y = x>w* + e,
6
Under review as a conference paper at ICLR 2018
where feature vector X ∈ Rp is uniformly distributed over Sp-1, i.e., X 〜 Unif(Sp-1), E 〜
N(0, σ2) is the observation noise and independent on X, and w* is a dense regression coefficient
vector.
Here we assume that w* is dense, because the D-SphereNet is not learning a sparse neural net-
work. Moreover, σ is also assumed to be small such that the signal-noise-ratio is sufficient large for
learning.
Given the linear model defined in Assumption 1, we further explicitly characterize the FGSM attack
in the following proposition.
Proposition 1. Suppose Assumption 1 holds, we train the model using the square loss function
'(χ, W) = 1(y - χ>w)2.
Given a sample (X, y), FSGM essentially generates an adversarial sample (Xe, y) by
X := x — δsign(Vχ'(x, W)) = X  δ ∙ Sign(E) ∙ Sign(W*).	(9)
Here δ is assumed to be very small, since the FGSM needs to generate semantically indistinguishable
adversarial samples. As can be seen from Proposition 1, the FGSM perturbation is proportional to
Sign(W*) up to sign change (determined by E associated with X).
5.1	Adversarial Training
The next theorem shows that training the model using adversarial sample is essentially adding bias
to estimation.
Theorem 2.	Suppose Assumption 1 holds, we train the model using the adversarial training sample
generated by FGSM. We solve:
Wb1 := argmin Ex, (y - Xe>W)2
w
Given sufficiently small δ and σ, we have the following approximation:
wbι ≈ argmin 1 ∣∣w — w*∣∣2 + ( δ ∣∣w∣∣ι + ^2-σ) .	(10)
The proof of Theorem 2 is provided in the Appendix B.1. Theorem 2 implies that training model
using adversarial samples is essentially solving an 'ι-penalized least square problem. However,
since the parameter of our interest W* is a dense vector (as in Assumption 1), adding such an `1
norm penalty induces estimation bias. Thus, it does not help, but harms the training. Note that our
theoretical analysis only considers a linear model, which can be viewed as a single neuron. For a
deep neural network, there are multiple layers with many neurons. Thus, the estimation bias at each
neuron is passed to the next layer. When these biases are eventually accumulated at the last layer,
the neural network can be significantly fooled.
The next theorem shows that by projecting Xe to the sphere, we can obtain a estimate.
Theorem 3.	Suppose Assumption 1 holds, we train the model using the adversarial samples pro-
jected to spheres. We solve:
ʌ	.雨 1( ι 〜> V-
W2 := argminEχ,e	y - TP^-X W .
w ,	kXek2
Given sufficiently small δ and σ, then we have the following approximation:
W2 ≈	argmin1 kw	- w*k2 +	p—2 (δ IHlI +	J!σ)	+ δ2( -1— IHl2	+ -7-2-2 Ilwk2 ).	(II)
w p	p	π	p + 2	p(p + 2)
The proof of Theorem 3 is provided in the Appendix B.2. Theorem 3 indicates that training the
model with the adversarial samples projected onto the spheres improves the estimation. Specifically,
we refine (11) in the following corollary.
7
Under review as a conference paper at ICLR 2018
Corollary 4. Suppose Assumption 1 holds, given wb1 in (10) and wb2 in (11), then
kWι - w*k2 ≥ ∣∣W2 - w*∣∣2 .	(12)
Proof. Note for any w ∈ Rp , we have
p-2 (δ kwki + r2σ σ)2+δ2( p+2 kwk2 + p⅛) kwk2)
≤ p-2 但 kwki+r∏ °)2+δp- kwki ≤ p-1 但 kwki+r∏Ο)2.
(13)
The second inequality holds for kwk22 ≤ kwk12 . We view (10) and (11) as two Lagrangian formula.
Notice there exists an one-to-one map between the Lagrangian formula and the constrained forms
of the optimization problems. We have
Wι = argmin ɪ ∣∣w — w*k2 s.t. W ∈ Ci. and W2 = argmin 工 ∣∣w — w*k2 s.t. W ∈ C2.	(14)
wp	wp
Essentially, (13) reveals that C1 ⊂ C2 . Then wb1 is just one feasible solution in C2. Therefore, by
□
the optimal property of wb2 , we have the desired result.
Corollary 4 shows that by projecting the adversarial samples to the spheres, we alleviate the regu-
larization, and therefore reduce the estimation bias. Note that our theoretical analysis only consider
a linear model, which can be viewed as a single neuron. For a deep neural network, there are mul-
tiple layers with many neurons, and we apply such a projection step at every neuron. Therefore,
the overall bias (attack) reduction at these neurons is significant. This justifies the superiority of the
D-SphereNet in adversarial training.
5.2 Robustness Against Adversarial Attack
We then provide theoretical insights of the hyperspherical defense when predicting adversarial test-
ing samples.
Theorem 5. Suppose Assumption 1 holds. Given a new input b 〜Unf Sp-1) with an associated
noise and the adversarial example generated by (9), we have
Eb ∣∣ι/ kek2 ∙ e>w* — b>w*Il ≤ p——-EbIe>w* — b>w*∣∣ .	(15)
The proof of Theorem 5 is provided in the Appendix B.3. Theorem 5 suggests by projecting the
adversarial testing sample to the sphere, we can also improve prediction. Note that our theoretical
analysis only consider a linear model, which can be viewed as a single neuron. For a deep neural
network, there are multiple layers with many neurons. Thus, the improvement at each neuron is
passed to the next layer. When these improvements are eventually accumulated at the last layer,
we obtain significant better prediction. This reveals why the D-SphereNet is more robust against
adversarial attack than other deep networks.
6	Discussions
Why hyperspherical defense is useful in practice. While facing white-box attacks, hyperspherical
defense is very likely to reduce or even eliminate the effect of adversarial examples. While facing
black-box attacks, hyperspherical defense has its unique advantages. If the attackers assume we are
using standard CNNs, they can not get good attack performance since we are using D-SphereNets.
If the attackers know we are using D-SphereNets, they will even get worse results if they do not have
access to our exact network parameters (see black-box attack experiments). In practice, one could
also randomize over different manifold setup to prevent attackers to explore the manifold structure.
Comparisons and connections. Our hyperspherical defense is designed to improve robustness
against adversarial perturbations. Compared to (Liu et al., 2017), the hyperspherical defense frame-
work has different robust SphereCorrs, new training objectives towards hyperspherical compactness,
robust geodesic inference, and aims for totally different task. The only thing in common is that both
(Liu et al., 2017) and hyperspherical defense use the hypersphere concepts. Similar to (Miyato
et al., 2015; Zheng et al., 2016; Cisse et al., 2017), our framework also improves smoothness in
neural networks, but we achieve this in a completely different way. We effectively combine novel
architectures, compactness training, robust inference into a unified framework.
8
Under review as a conference paper at ICLR 2018
Table 1: White-box attack on MNIST dataset.
Attack	BaSeline CNN			D-SphereNet			D-SPhereNet + WM		
	Natural Train	FGSM Train	PGD Train	Natural Train	FGSM Train	PGD Train	Natural Train	FGSM Train	PGD Train
Natural Data	99.36	99.43	99.29	99.50	99.51	99.40	99.37	99.37	99.44
FGSM	29.42	98.76	98.04	84.70	98.81	98.36	82.98	82.81	98.23
PGD 一	00.51	01.45	94.18	02.31	10.16	95.96	12.19	35.41	94.58
Attack	D-SPhereNet + WM + Quad SphereCorr			D-SPhereNet + LoSS Annealing			D-SphereNet + WM + Input-truncated Corr		
	Natural Train	FGSM Train	PGD Train	Natural Train	FGSM Train	PGD Train	Natural Train	FGSM Train	PGD Train
Natural Data	99.48	99.34	99.36	99.22	99.47	99.49	99.63	99.59	99.44
FGSM	89.62	98.51	95.82	88.39	98.22	98.18	76.52	99.06	98.09
PGD 一	14.84	26.34	82.81	17.12	34.67	94.66	03.48	19.10	95.02
7	Experiments
7.1	Experimental Settings
General. We put the detailed network architecture in the appendix. WM denotes the weight multi-
plier in all the tables. Our CNN models are trained with ADAM with initial learning rate as 0.001.
In MNIST, we divide the learning rate by 10 every 10K iterations and stop at 30K iterations. In
CIFAR-10, we divide the learning rate by 10 at 34K and 54K iterations and stop at 64K iterations.
For all experiments, we use the CleverHan library (Papernot et al., 2016a).
White-box Attack. For MNIST, we train both baseline CNN and D-SphereNet model with natural,
FGSM and PGD training. Specifically, for FGSM training, we set = 0.3. For PGD training we
set = 0.3, step size as 0.75 and number of iterations as 10. We evaluate the D-SphereNets with
different SphereCorrs and different learning objectives. For CIFAR-10, we train both vanilla and
D-SphereNet models, including variants models, with natural and FGSM training. Specifically, for
FGSM training, we set = 8/255 and for PGD training we set = 8/255, step size as 2/255 and
number of iterations as 7. We evaluate the D-SphereNets with different SphereCorrs and different
learning obejctives.
Black-box Attack. For substitute models in blackbox attacks in both MNIST and CIFAR-10, we
independently train both vanilla and D-SphereNet models with natural and FGSM training. The
training data and parameters are kept the same to show the worst possible case.
7.2	White-box Adversarial Attacks
MNIST. For each model with certain training method, we attack it with both FGSM and PGD. For
FGSM attack, we use = 0.3 as in the FGSM training. For PGD attack, we set step size as 0.01 and
the number of iterations as 100. The results are shown in table 1.
With white-box attacks, vanilla CNN is prone to attacks on which the network is not trained on.
On the contrary, variants of D-Spherenet are able to resist some portion of adversrial examples even
though they are not explicitly trained on.
Among those trained with FGSM adversarial training, D-SphereNets with weight multiplier and
loss annealing have the best performance against PGD attacks. Since loss annealing does not show
significant advantage over weight multiplier, we will only experiment with weight multiplier in the
following experiments.
CIFAR-10. For each model trained with each training method, we attack it with both FGSM and
PGD. For FGSM attack, we use = 0.3 (where the values of images range from 0 to 255) as in the
FGSM training. For PGD attack, we use step size=0.01 and number of iterations=100. The result is
shown in Table 2.
The results are mostly consistent with the observations in MNIST experiments, where D-Spherenets
are able to classifier more adversarial examples than vanilla CNN.
9
Under review as a conference paper at ICLR 2018
Table 2: White-box attack on CIFAR-10 dataset.
Attack	BaSeline CNN		D-SphereNet		D-SphereNet + Weight Multiplier	
	Natural Training	FGSM Training	Natural Training	FGSM Training	Natural Training	FGSM Training
Natural Data	0.8535	0.837-	0.8858	0.8741	0.8797	0.884
FGSM	0.1882	0.7896	0.4364	0.8598	0.3834	0.869
PGD 一	0.0867	0.0796	0.0989	0.3507	0.2132	0.4443
Attack	D-SPhereNet + WM + Quad SphereCorr		D-SphereNet + WM + LoSS Annealing		D-SphereNet + WM + Input-truncated SphereCorr	
	Natural Training	FGSM Training	Natural Training	FGSM Training	Natural Training	FGSM Training
Natural Data	0.8851	0.8826	0.8964	0.8993	0.9214	0.9208
FGSM	0.4672	0.8673	0.4356	0.8939	0.3786	0.8951
PGD 一	0.2178	0.4669	0.1264	0.3811	0.0912	0.1728
Table 3: Blackbox attacks on CIFAR-10 dataset.
Source Model	Attack	Target Model					
		Vanilla		D-SphereNet		D-SphereNet + WM	
		Natural Training	FGSM Training	Natural Training	FGSM Training	Natural Training	FGSM Training
Baseline CNN	FGSM	0.5090	0.7757	0.5671	0.7629	0.5405	0.7468
	PGD	0.3622	0.7855	-0.431-	0.7779	0.3897	0.7599
D-SphereNet	FGSM	0.6877	0.7677	0.6725	0.8128	0.6505	0.8102
	PGD	0.7168	-0.775-	0.6946	0.836-	0.6612	0.8415
D-SphereNet + WM	FGSM	0.674	-0765-	0.6717	0.8102	0.6475	0.806-
	PGD	0.7438	0.7798	0.7464	0.8457	0.7168	0.8458
D-SphereNet with both weight multiplier and trauncated projections is more capable of fitting both
normal examples and FGSM generated examples than any other networks. We also notice that this
variant network converges within only 25k iterations. However, this network, if trained with FGSM
training, is not as robust as D-Spherenet with only weight multiplier.
7.3	Black-box Adversarial Attacks
We evaluate the robustness of D-SphereNets against black-box attacks on CIFAR-10. We use inde-
pendently trained substitute networks to attack the D-Spherenets. In some experiments, the source
model (substitute) and the target model have the same architecture. All the networks are trained on
the same set of training data in order to examine the worst case performance of D-SphereNets. The
results are shown in table 3.
Presumably, one can easily attack the target network with the knowledge of the architecture. This
is not the case for D-SphereNets, however, as D-SphereNets are more robust against attacks on the
D-SphereNets. We believe the source of robustness comes from the complex landscape of the loss
function.
We also notice that, D-SphereNets have similar values of accuracy, compared to vanilla CNN, on
adversarial examples generated by vanilla substitute CNN. This fact indicates that D-SphereNets
cannot be specifically targeted while maintaining reasonable accuracy on adversarial examples gen-
erated with simple methods, with WM denoting weight multiplier.
References
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl
Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-
to-end speech recognition in english and mandarin. In ICML, 2016.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In International Conference on Machine
10
Under review as a conference paper at ICLR 2018
Learning,pp. 854-863, 2017.
Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Fundamental limits on adversarial robustness.
In Proc. ICML, Workshop on Deep Learning, 2015.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu-
rate object detection and semantic segmentation. In CVPR, 2014.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep
hyperspherical learning. In NIPS, 2017.
Jiajun Lu, Theerasit Issaranon, and David Forsyth. Safetynet: Detecting and rejecting adversarial
examples robustly. arXiv preprint arXiv:1704.00103, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional
smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677, 2015.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. arXiv preprint arXiv:1610.08401, 2016a.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2574-2582, 2016b.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confi-
dence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 427-436, 2015.
Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman, and Patrick McDaniel. clev-
erhans v1. 0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768, 2016a.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (Eu-
roS&P), 2016 IEEE European Symposium on, pp. 372-387. IEEE, 2016b.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP),
2016 IEEE Symposium on, pp. 582-597. IEEE, 2016c.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning
research, 15(1):1929-1958, 2014.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In NIPS, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to
human-level performance in face verification. In CVPR, 2014.
11
Under review as a conference paper at ICLR 2018
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In
Advances in neural information processing Systems, pp. 351-359, 2013.
Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep
neural networks via stability training. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 4480-4488, 2016.
12
Under review as a conference paper at ICLR 2018
1e6 Unit Data with 0.05 perturbation
90°HUoqo①6Ja-JO=B Jo」」山
0000^ _■
Pr(Error after ProjeCtion<0.05)=0.983944
00000- 0000^
AOUSnbE
0.00	0.01	0.02	0.03	0.04	0.05
Error after Projection
Figure 4: Histogram of Euclidean Error after Projecting the Adversarial Input
A Illustration of Why Projection Works
Here, we draw a simple two-dimension example in Figure 4 to show that with high probability,
projection does reduce the Euclidean Error caused by adversarial attack. The generating process is
as follows: Fist, we uniformly sample one million points over S1. Without loss of generality, we
choose (0, 0.05) as the adversarial perturbation. After adding this to the whole sample, we project
these adversarial examples to S1. Then we calculate the Euclidean distance between the original
point and its projection point as its Euclidean Error. Next, we draw the histogram of the Euclidean
Error. As we can see, projection seldom increases the Euclidean Error. For one million points in two
dimensions, the probability of projection preserving the Error equals 0.98. This implies the function
of projection.
B Proof of the Theorem Justification
B.1 Proof of Theorem 2
Proof. By assumption 1, we know that x is uniformly sampled from Sp-1, the sphere of the unit
ball in Rp. Let Xi and Wi denote the i-th entry of X and w*, respectively. Then We have
E [(y - e>w)[ = E [(x>w* — (X — δ ∙ sign(εw* )) > W + 上)2]
=E h(x> w* — x>w)[ + 2E h(x> (w* — w))(δ sign(εw*)> w +。] + E [(δ sign(εw* )> w +。2]
pp	2
= E X	xi (wi*	-	wi)	X xj(wj*	-	wj )	+	δ2	sign(w* )> w	+ σ2 +	2Eδ sign(w* )> w
E [ XX χ2 (w* — Wi)2] + 2\ —δσ sign(w* )> w + δ2 (Sign(w* )> w) 2 + σ2 , (by symmetric of Xi)
π
(16)
ɪ ∣∣w - w* ∣∣2 + (δ Sign(W*)>w + S^σ)	+ ∏——-σ2.
The last equality holds because we can obtain Ex2 = P by EPp=I x2 = 1 and the isotropic of
xi’s. Also, since both δ and σ are sufficiently small, the first term of Right Hand Side (RHS) in (16)
dominates the whole loss. This leads the optimal solution, i.e. wb1, is close to wi, which implies
that
Sign(W*)>Wι ≈ ∣∣Wιkι∙
13
Under review as a conference paper at ICLR 2018
Therefore, we have the following approximate solution for Wι,
Wi ≈ argmin ɪ IlW - w* ∣∣2 + ( δ IlWllI +
W P	∖
□
B.2 PROOF OF THEOREM 3
Proof. For notation simplicity, let a = Sign(E), b = Sign(W*). Note α2 = 1, bτb = p. We obtain
1 e ________________1___________X
kxk2	y(1 - 2δ ∙ ab>x + δ2bτb)
[1 — ɪ(-2δɑb>x + pδ2)] (x — δɑb) + O(δ2)
X + δ (αb>x ∙ X — ab} + O(δ2).
(17)
The second equality holds because of Taylor Expansion. We plug (17) into the objective function as
follows: (if not clear specified, we denote Eχ,e by E for notational simplicity)
E[(y - ∣⅛x>w)2]
一(X + δ (αb> x ∙ x —
ab) ) w + e)2]
:E ∣^( x>(w* — w) + e — δab>x ∙ XTW + δab>w )2
L `-----V------' '---------------------------/
A	B
(18)
:E[A2 +2A ∙ B + B2].
Let Xi, bi, Wi denote i-th component of x, b, W respectively. Now, let,s calculate the terms of
RHS in (18) one by one:
E[A2] = E[xτ(w* — w)] = LllW — w*∣∣2, (from (16));
E[A ∙ B] = Ex [(xτ(w* — W)Ee(E — δαbτx ∙ xτw + δabτw))]
=0, (notice Eea = EeE = 0, X and E are independent);
e[b2] = E[(e — δαbτx ∙ xτw + δabτw)2]
=e[ (e+ = δabτw)2 —2 (δ2wτb 十 Ea)bτxxτw + δ2(bτxxτw)2 ];
}	1
|
} X-------V------Z
E
{z
C
{z
D
Like before, we calculate the each part of RHS in (21) as follows:
E[C] = Ee [e2 + 2δEabτw + δ2(bτw)2] = σ2 + 2^^-δσbγw + δ2(bτw)2;
e[d] = Ee [δ2wτb + Eaδ] Ex [bτxxτw] = (δ2wτb + ^^δσjbτw;
P
e[e] = δ2Ex[ E	biXiXj Wj bk XkXlWι]
i,j,k,l = l
PP	P
=δ2Ex [ E	X2X2W2 + 2 E	biX2WibkXkWk + E X4w2].
i,j = 1,i=j	i,k = 1,i=k	i=l
(19)
(20)
(21)
(22)
(23)
(24)
To simplify (24), let,s consider a random vector Z. %, the i-th entry of Z, is defined as follows:
Zi := —/ Zz	, where Zi 〜 N(0,1).
√P=^
14
Under review as a conference paper at ICLR 2018
Note that actually Z follows a uniform distribution over SPT and Z2 〜 B( 1，p-1). By this, We
calculate E xi4 in the following way:
/ I p-1	I ∖	3
E[x4] = E(Z2)2 = (Var(Z2) + (EZ2)2)= (一+ ( f⅛ )j = (P⅛.
By E(Pp=I x2)2 = 1 and symmetric of xi, We get Eχ2χ2 =(p+12)p, ∀i = j. Plug these into (24).
E[E]= δ2[(P+2)p	X	Ww + 2(P⅛p	X 帅jWjWi+
p	p i,j=1,i6=j	p	p i,j=1,i6=j
=δ2 ∖--- ∣∣wkw +—————-∣∣wkw ].
Lp + 2 k k2 + p(p + 2)k "lj
Combine (19), (21), (22), (23), (25) together, We obtain
3p
Ep X wi]
(25)
(18)	= ɪ kw — w*k2 + σ2 + 2\ —δσb>w + δ2(b>w)2
p2	π
- 2((δ2w>b+r∏δ δσ) P b>w+δ2[ pɪ kwk2
2
+pp+ιy kwkJ
Note that both δ, σ are small enough, the dominant term is P ∣∣w* 一 w∣w. This implies
Sign(W*)>ww ≈ kwwk1.
By this approach, We further approximate wb2 as folloWs:
ww ≈ argmin1 ∣w 一 w*k2 + p_2(δ ∣w∣i + J2σ)2 + δ2(-ɪ ∣∣w∣2 + (
w p	2 p	1 π	p + 2	2	p(p + 2)
kwk21 ).
□
B.3 Proof of Theorem 5
Proof. First, let’s calculate the Left Hand Side (LHS) in (15).
2 δ 2
Ex ∣∣x>w* - b>w* Il = Ex b>w*------------Sign(C) sign(w*)>w* - b>w*
xl'	"2 X	√P	2
∣ δ	∣2	δ2
=Ex --尸 sign(w )>w = — ∣w kl,
√p	2	P
Where C does not affect the value at all. For the projected input, by (17), (25) and (23), We have
2
Exb
1 ~> *	^> *
∣∣~∣∣ X W - X W
kxek2
2
=-Eb ∣∣sign(w*)>b ∙ b>w* — sign(w*)>w* ||
=δ2 [ p⅛ kw*k2+ P(P⅛ kw*k1- P kw*k2 + kw*k2 ]
≤ ∖ ∣∣w*k2 - p2 ∣∣w*kι = p-1 Eb ∣∣e>w*- b> w*∣∣2.
Where C does not affect the value, too. And We prove the desired result.
□
C Experimental Settings
15
Under review as a conference paper at ICLR 2018
Layer	CNN for MNIST	CNNfOr CIFAR10
Conv1.x	[3×3,32]×2	[3×3, 64]×3
Pool1		2×2Max, Stride 2		
Conv2.x	[3×3, 64]×2	[3×3,128]×3
Pool2		2×2Max, Stride 2		
Conv3.x	[3×3,128]×2	[3×3, 256]×3
Pool3		2×2Max, Stride 2		
Fully Connected	256		512	
Table 4: Our CNN architectures for different benchmark datasets. Conv1.x, Conv2.x and Conv3.x denote con-
volution units that may contain multiple convolution layers. E.g., [3×3, 64]×3 denotes 3 cascaded convolution
layers with 64 filters of size 3×3.
16