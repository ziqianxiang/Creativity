Under review as a conference paper at ICLR 2018
Empirical Investigation on Model capacity
and Generalization of neural networks for
Text
Anonymous authors
Paper under double-blind review
Ab stract
Recently, deep neural network models have shown promising opportunities for
many natural language processing (NLP) tasks. In practice, the number of param-
eters of deep neural models is often significantly larger than the size of the training
set, and its generalization behavior cannot be explained by the classic generaliza-
tion theory. In this paper, with extensive experiments, we empirically investigate
the model capacity and generalization of neural models for text. The experiments
show that deep neural models can find patterns better than brute-force memoriza-
tion. Therefore, a large-capacity model with early-stopping stochastic gradient
descent (SGD) as implicit regularizer seems to be the best choice, as it has better
generalization ability and higher convergence speed.
1	Introduction
Although neural network models are non-convex and overly-expressive, they have still achieved
great success in many fields, such as in computer vision, natural language processing, etc.
(Krizhevsky et al. (2012); He et al. (2016); Bahdanau et al. (2015); Seo et al. (2017)). Currently,
most neural models are trained by SGD optimization. Although SGD is simple, it is able to find a
“good” local optimal by minimizing the training error. More surprisingly, the learned models also
exhibit good generalization behavior under the over-parameterized setting (Neyshabur et al. (2017);
Zhang et al. (2017)).
Therefore, there is rising interest in studying the capacity and generalization of neural models from
both theoretic (Neyshabur et al. (2017)) and practical (Zhang et al. (2017)) views. However, the
theoretic upper bounds of generalization error cannot give satisfactory interpretations of the practical
observations of the neural models.
There are some empirical investigations on the generalization of neural models, Zhang et al. (2017)
showed the capability of neural models by memorizing the samples with random labels. They also
empirically observed that explicit regularization, such as weight decay, seemed to be unnecessary to
obtain small test errors, which is different from the generalization of linear models.
Although these empirical investigations are inspiring, they are limited to image data. There is natu-
rally a question on how they perform on text, particularly on natural language. What are the effective
capability and generalization of the popular neural architectures on text? What is the difference be-
tween these architectures? Which architectures should be chosen?
In this paper, we investigate the capacity and generalization of widely used neural architectures
on text, including continuous bag-of-words (CBOW), long short-term memory network (LSTM)
(Hochreiter & Schmidhuber (1997)) and convolutional neural network (CNN). We first evaluate
the effective capacity of the three different architectures, and measure their generalization ability
with several regularization methods, including weight decay, dropout (Srivastava et al. (2014)) and
early stopping. Specifically, we study the model capacity and generalization from the following two
views.
Effective Capacity Following Zhang et al. (2017), we evaluate the effective capacity of different
models on samples with random labels, without considering their generalization error. We found
1
Under review as a conference paper at ICLR 2018
all the models are able to fit the randomly-labeled data with zero error, provided there are enough
parameters. CNN has the largest effective capacity with the same number of parameters, and the
attention mechanism can also effectively increase the capacity of LSTM. The capacity also depends
on the length and linguistic regularities of the text.
Generalization Although generalization theory states that the generalization error is low when
the model complexity is small, low model complexity is not a necessary condition of low general-
ization error (Kawaguchi et al. (2017)). Following the experience on linear models, weight decay
is often used as regularization in deep neural models. Besides weight decay, there are some other
regularization methods, such as dropout, early stopping, and batch normalization (Ioffe & Szegedy
(2015)).
We evaluate the generalization of different neural architectures with different regularization methods
on the corrupted data. Although the large-capacity model prefers to overfit the training data, stopping
SGD early is an effective regularization strategy, which can effectively prevent it from overfitting.
Similar to Zhang et al. (2017), we also find that weight decay seems to be non-essential, compared
to early-stopping.
Optimization We also investigate three commonly used optimizers: vanilla SGD, Adagrad (Duchi
et al. (2011)) and Adam (Kingma & Ba (2015)). The experiments show that Adam works well on
text and outperforms other optimizers. With the Adam optimizer, we could achieve the best results
without the need of adjusting the learning rate.
2	Related Work
For the capacity, Bartlett et al. (1998) showed if the number of layers of neural networks is fixed, the
Vapnik Chervonenkis (VC) dimension grows as W log W, where W is the number of parameters
in the network. Following this, Bartlett et al. (2017) showed new upper and lower bounds on the
VC dimension of deep neural networks with the rectified linear unit (ReLU) activation function.
The VC dimension is O(WL log(W)), where W is the number of weights and L is the number of
layers. For the generalization ability of neural network, Neyshabur et al. (2017) studied how norm-
based control, sharpness and robustness can ensure generalization and connected the sharpness to
PAC-Bayes theory. In spite of the very large capacity and instability, non-robustness and sharp
minima, Kawaguchi et al. (2017) gave us an explanation for the strong generalization ability of the
neural network. Zhang et al. (2017) found that deep neural networks can easily fit random labels
and showed that the traditional solutions can not be applied to the explanation of the generalization
ability of neural networks.
Pascanu et al. (2014) provided a framework for comparing deep and shallow models in the family
of piecewise linear functions. Livni et al. (2014) revisited the computational complexity of training
neural networks and offered practical methods for training specific neural networks. Poggio et al.
(2017) showed deep networks perform better than shallow networks and the weight sharing is not
the main reason for the better performance. The parameters of the neural network are in the order
of one million, and in the case of over parameters (i.e., the number of parameters is very large), the
optimization method has a great influence on finding the local optimum. Neyshabur et al. (2015)
introduced Path-SGD method and showed better generalization ability.
However, for the measurement of capacity, there is no empirical results. When the sample size
reaches a certain level, the model with a certain limit of the parameters may not be able to fit the
random labels. This prompts us to explore the empirical relationship between model capacity and
generalization ability.
3	Dataset and Model Architecture
3.1	Overview of deep learning methods
Learning theory of classification. Given training data (X, Y ) = {(xi, yi), i = 1, 2, ..., n}, (xi ∈
Rd, yi ∈ {1, 2, ..., K}), xi is independent identically distributed. We want to learn a classifier f that
can predict the output given the input so that y ≈ f (x). We parameterize the set of learned functions
2
Under review as a conference paper at ICLR 2018
f(x, θ : θ ∈ Θ) and the loss function is L(y, f(x, θ)). The process of learning is to pick an optimal
one from these function sets. The evaluation criteria of optimality is expected risk minimization,
where expected risk R(θ) = L(y, f(x, θ))dF (x, y), and F (x, y) is the true distribution function
ofX, Y . Since we do not know F(x, y), the usual practice is to use the arithmetic average to replace
the mathematical expectation, namely, empirical risk minimization, with empirical risk Remp(θ) =
n pn=1 L(yi, f (xi, θ)). Consistency means that the optimal value of Remp(θ) converges to the
optimal value of R(θ) when the size of samples tends to infinity. If empirical risk minimization can
provide a function sequence f(x, θ) such that Remp(θ) and R(w) converge to the smallest possible
risk value R(θ0), the learning process of this empirical risk minimization learning is consistent.
The necessary and sufficient condition for the consistency of empirical risk minimization is that the
empirical risk converges to the expected risk on the set of functions:
lim P [sup ∣R(θ) — Remp (θ) | > e] = 0, ∀e > 0
n→∞ θ
where P[∙] is the probability. There is no learning method to determine whether the set of functions
can satisfy the condition of consistency. Thus, some norms are defined to measure the performance
of the set of functions and the most important one is VC dimension. VC dimension reflects the
learning ability of the set of functions. The greater the VC dimension, the more complex the learning
machine. At present, the VC dimension of some special function sets can be accurately understood,
but for some complex learning machines (such as neural networks), the VC dimension is not only
related to the choice of the set of functions, but also the optimization algorithm and it is hard to
determine.
Role of regularization. Generalized regularization is a strategy that is added to the training process
to improve the generalization of the model without necessarily reducing the training error. The most
common way is parameter norm penalty, namely:
L(y,f (X, θD = L(y,f (X, θD + Yg(。》
For the choice of g, the usual way is the L2 norm (also called weight decay or ridge regression),
namely:
L(y, f (χ, θ)) = L(y, f (χ, θ)) + γθτ θ.
Another commonly used regularization is dropout. In dropout, during the training process some
inputs or hidden states are randomly chosen and involved in the calculation. Parameter norm penalty
and dropout are explicit regularizations. For large models, large training sets are advantageous, so
data augmentation is also an explicit regularization and it can improve the generalization ability.
For implicit regularization, early stop is commonly used. When training large models, usually the
validation error will increase when the training error is reduced. Early stop is a strategy which may
be used to deal with this problem. In the process of training, we save the parameter value of the
minimum validation error in the training history and when the validation error is larger than the best
saved value for many times, we stop the training and return to the saved value.
Different kinds of neural networks Feed-forward neural network is the most common network
topology, in which the neurons receive the inputs of the previous layer and output to the next layer.
In practice, it is often used as the fully connected layer. Because feed-forward neural network
has so many parameters and it can not be used for high-dimensional space, there is the parameter
sharing mechanism and the most common is CNN and RNN. CNN differs from the ordinary neural
network in that CNN contains a feature extractor composed of convolution and sub-sampling layers.
In the convolution layer of CNN, a neuron is connected only to a portion of the adjacent neurons.
Sub-sampling is also called pooling and usually in the form of mean pooling and max pooling.
Sub-sampling can be seen as a special convolution process. Convolution and sub-sampling greatly
simplify the model complexity and reduce the number of parameters of the model. The purpose of
RNN is to process the sequential data and the network will remember the previous information and
apply it to the calculation of the current output. Theoretically, RNN can process the data of any
length. In the past few years, researchers have proposed complex RNN to improve the shortcomings
of the vanilla RNN model. The most common RNN is LSTM, which is essentially the same as the
vanilla RNN structure, except that different functions are used to calculate the hidden state.
3
Under review as a conference paper at ICLR 2018
3.2	Dataset
We performed an experiment on the task of text classification (Kim (2014); Yogatama et al. (2017))
using the data set from the Yelp Dataset Challenge (2015)1 which is sentiment analysis with five
categories. The total number of samples selected is 650000, split into training set (645000), devel-
opment set (5000) and test set (50000). Figure 1 shows the statistics of the data set.
Sa-dEes J0⅛5qEnN
Figure 1: Statistics of data set. Most of the sentences are of length 0 - 600.
3.3	Model Architecture
The architecture of the model consists of three basic layers:
Embedding layer. Given a context c consisting of t tokens {c1 , c2, ..., ct}, we use a pre-trained
word embedding to map each word to a word vector space to form the word embedding space
{p1,p2, ..., pt}. The pre-trained word embedding we use here is Glove.6B.300d 2 (Pennington et al.
(2014)). Then, the embedding space is passed to the processing layer.
Processing layer.
For LSTM-based classification, we use word embedding space as the input of LSTM, and gen-
erate hidden states {h1, h2, ..., ht}. Finally, we average the hidden state and get the vector
m = hι+h2 +…+ht
For CNN-based classification, the input layer gets a number of feature maps by convolution oper-
ation. The size of the convolution window is t × d, where t is the number of words and d is the
dimension of the word vector. Through such a large convolution window, we can get a number of
feature maps. The filter size we use here is 3, 4 and 5. The number of filters is 68. After 3 layers
of convolution operation, we use a method called max-over-time pooling in the next pooling layer.
This method simply retrieves the maximum value from the previous feature maps, thus solving the
problem of variable input length. The output of the final pool layer is the maximum value of each
feature map, that is, a one-dimensional vector m.
For CBOW-based classification, we average the embedding space and get the vector m =
p1+p2 +…+pt
t.
Output layer. The output of the one-dimensional vector m is connected to a softmax layer by
means of a fully connected layer, and the softmax layer reflects the probability distribution on the
final classes.
1https://www.yelp.com/dataset/challenge
2https://nlp.stanford.edu/projects/glove/
4
Under review as a conference paper at ICLR 2018
4 Experiments
4.1	Capacity of neural networks
Relation between capacity and parameters. The capacity of neural networks is approximately
equal to the maximum number of samples that can be remembered (i.e. training error is very small)
under the condition of random labels (Neyshabur et al. (2017)). For the neural network with the
order of 10k parameters, we want to know the capacity of the neural network. We believe that
one of the factors that affects the capacity of neural networks is the number of parameters. We
want to explore the exact relationship between capacity and the number of parameters. We use text
classification for this exploration. For the determination of capacity, we will use a random label for
each sample and treat the number of samples remembered by the neural network as the capacity
of the model. We choose D ∈ {50k, 100k, 150k, 200k, 250k, 300k, 350k, 400k, 450k, 500k, 550k}
samples from the training set, and then measure the accuracy of each training set under different
number of parameters after the model converges. Then, we interpolate between two adjacent points
to calculate the number of samples with the four levels of accuracy 90%, 80%, 70% and 60%.
The results of the measurement are shown in Figure 2. From the experiments we can see that the
capacity and parameters of neural networks show logarithmic linear relationship. This means that
the marginal effect of the capacity on the number of parameters will be reduced as the number of
parameters increases.
Impact of different architectures. Another factor which affects the model capacity is the architec-
ture of the model. We compare the capacity of CBOW and LSTM with the same order of number of
parameters. The capacity of LSTM is significantly larger than that of CBOW. From Table 1 we can
see that when the sample size changes from 50k to 350k, the accuracy of LSTM declines slowly,
and that of CBOW drops dramatically. Although when the sample size is 50k and 100k, CBOW
can almost completely remember the sample size, from Table 2 we can see that the number of train-
ing steps which CBOW requires reaches the order of 20M, and the convergence is extremely slow.
From the experimental results, in the task of text classification, CNN and LSTM with attention can
increase the capacity of the model, and they also converge readily in the training process.
Figure 2: Impact of number of parameters. As Figure 3: Impact of dropout. As the keep rate in-
the number of parameters increases, the marginal creases, the capacity of the model gradually in-
effect of the capacity of models decreases. creases.
Model	Number of parameters	50k	100k	150k	200k	250k	300k	350k
LSTM64d	97,925	100%	71%	56%	48%	42%	39%	37%
CBOW	91,805	100%	100%	23%	22%	22%	21%	21%
LSTM256d	637,445	100%	100%	100%	100%	100%	86%	73%
CNN	641,345	100%	100%	100%	98%	98%	90%	91%
LSTM with attention	636,340	100%	100%	100%	100%	99%	92%	89%
Table 1: Capacity of different architectures. Overall, the capacity of LSTM changes gradually, but
CBOW does not. CNN and LSTM with attention mechanism can effectively increase the capacity
of models.
Impact of regularization. In statistical learning theory, VC dimension is a measurement of the
complexity of the model. As can be seen from our experiments (Figure 3, Table 3), the effect of
dropout on model capacity is far weaker than the effect of weight decay. Weight decay greatly limits
5
Under review as a conference paper at ICLR 2018
Model	Number of parameters	Number of samples	Convergence steps
LSTM64d	97,925	100k	584,000
CBOW	91,805	50k	23,400,000
		100k	46,800,000
Table 2: Steps of convergence. LSTM converges more readily and faster than CBOW.
the complexity of the model. From the experiments, we can see that when the weight decay is over a
certain range, the model will completely forget the training set, even on a small data set. The effect
of weight decay on model capacity is a hard effect, that is, when the weight decay rate is less than
some specific value, the model will completely remember the training set, and when it is greater
than this value, the training process of the model will be completely disrupted. On the other hand,
the effect of dropout on model capacity is a soft effect. As the keep rate of dropout rises, the number
of samples that the model can remember will gradually increase.
Model	Number of parameters	Number of samples	Weight decay rate	Training accuracy
LSTM53	1,930,245	50k	≤ 10-4	≥ 99%
			≥ 10-3	≈ 20%(fail to converge)
Table 3: Impact of weight decay. The effect of weight decay on model capacity is very large.
Impact of random context and short context. We shuffle the context of the data set and give
each sample a random label to see if the neural network of different architectures can remember the
samples. From the experiment shown in Table 4, we observe that LSTM, CNN and LSTM with
attention are all able to remember the samples. Except for LSTM, the number of samples which
are remembered by CNN and LSTM with attention is almost the same as when the context is not
shuffled. In addition to being able to capture the distribution of the language model, LSTM can
capture the distribution of sequences that do not conform to the language model. We also reduce the
length of the text from 400 to 100, observing that LSTM can remember more samples.
Mode of context	Model	Number of parameters	50k	100k	150k	200k	250k	300k	350k
Random context	LSTM256d	637,445	100%	100%	100%	99%	94%	77%	72%
	CNN	641,345	100%	100%	100%	99%	96%	91%	94%
	LSTM With attention	636,340	100%	100%	100%	100%	99%	95%	90%
Short context	LSTM256d	637,445	100%	100%	100%	100%	100%	96%	87%
Table 4: Impact of random and short contexts. LSTM, CNN and LSTM with attention do not fail to
fit the random labels. For short context, LSTM performs better.
4.2	Generalization of neural networks
Dataset and training details. The dataset is split into a training set (645000), a development set
(5000) and a test set (50000). The labels of the development and test sets are correct. The labels of
the training set are randomized from 0 to 1.0. 0.5 means that the labels of 50% of the training set are
randomized and in general, approximately 60% (50% + 50% × 0.2) of the labels which are correct
in the training set. For training, we use a mini-batch size of 512 and the largest length of text is set
to 400. We use the Adam optimizer to train the models with the initial learning rate 0.001 and we
decay it with the rate of 0.9 every 2000 steps. The validation step is set to 100 steps and early stop
is set to 10 validation steps.
Generalization ability. Figure 4a shows that even if the training set is mixed with the data of
wrong labels, the model which is trained with the training set also has a strong generalization ability.
From the experiment, we can see that there is a jump in the test accuracy when the label corruption
changes from 1.0 from 0.9. More specifically, when all the training set is contaminated (in fact
approximately 20% (100% × 0.2) of the labels of the training set are correct) and each category
has almost the same number of samples, we can see that the model trained with these data just
guesses randomly. However, when 90% of the training set is contaminated (in fact about 28%
(10% + 90% × 0.2) of the labels of the training set are correct), we can see that the generalization
ability of the model trained with the data is greatly enhanced. The reason may be that SGD has the
6
Under review as a conference paper at ICLR 2018
(a) Test Accuracy
Figure 4: Generalization of corrupted training set. Although the training set only has a small number
of samples with correct labels, the model can generalize very well.
(b) Mode Selection
ability to perform implicit regularization, which prefers to select the generalizable models over their
counterparts. Experiments (Figure 4a) also show that SGD training finds the generalizable models
before memorizing the random noise.
Linked to the mode selection of distribution. There are many patterns of distribution in the data
set, and the training process of the neural network is to find one or more distribution patterns. Given
supervised information, supervised learning is used to guide the neural network to find the pattern
of distribution corresponding to it and to fit the pattern of this distribution or the mixed pattern of
multiple distributions in the course of training. From the experiment, we can see that when the
label corruption is 1.0, the trained model is in a random guessing state. The distribution of the
training set is in a level of equilibrium, so the neural network cannot choose the correct pattern of
distribution corresponding to the supervised information, and cannot achieve generalization ability.
When the label corruption is 0.9, although the number of correct labels is only increased slightly, the
equilibrium of supervised information is broken, and with the change of the degree of inclination,
the neural network is guided to choose the correct distribution mode. Once the correct distribution
mode is chosen, optimization becomes easier, so there will be a jump when the label corruption
changes from 1.0 to 0.9. Figure 4b shows the process, and the small supervised information will
make the neural network select the distribution pattern that conforms to the supervised information.
Assuming there are two distribution modes in the training data, the Model 1 is the distribution mode
of the supervised information, and the supervised information will cause the neural network to find
its distribution.
4.3	Optimization of neural networks
The function represented by neural network is a high dimensional non-convex function space. There-
fore, the choice of optimization methods greatly impacts the performance of the network. Here, we
compare several commonly used optimization methods, namely, vanilla gradient descent, Adagrad
and Adam. Table 5 shows the results. Adam’s convergence is very fast and it can also converge for
the samples of random labels. However, vanilla gradient descent and Adagrad fail to converge for
the samples of random labels, even for the small dataset.
Model	Number of parameters	Number of samples	Optimizer	Training accuracy
LSTM512d	1,930,245	50k(True label)	Adam	100%
			Adagrad	62%
			GradientDeSCent	56%
		50k(Random label)	Adam	100%
			Adagrad	≈ 20%(fail to converge)
			GradientDeSCent	≈ 20%(fail to converge)
Table 5: Impact of optimizer. Adagrad and vanilla gradient descent fail to converge for samples of
random labels.
7
Under review as a conference paper at ICLR 2018
4.4	Model Selection
We want to provide guidelines on the choice of a model from the perspective of the capacity of neural
network. The main problem is whether we choose a large-capacity model or a small-capacity model.
Figure 5a shows the convergence trend of the training loss vs. the training steps under different
capacity of the same architecture. From Figure 5a we can see that the convergence of large-capacity
models for random samples is fast. Figure 5b and Figure 5c show the loss trend for the training
set and validation set under the training set of true labels. For the experiments shown in Figure 5,
we use the dataset of 150k samples. Although the lowest loss that each model achieves is almost
the same, the large-capacity model is easier to fit and in the condition of implicit regularization of
early stop, the large-capacity model can stop earlier. Table 6 shows the generalization ability of
models of different capacities on the test set. We can get the intuition that for the large dataset, the
model of large capacity can generalize better than the model of small capacity. However, for the
small dataset, the model of large capacity and small capacity have almost the same generalization
ability if the regularization is chosen properly. Overall, weight decay is not an effective way for
generalization. Early stop performs much better than weight decay.
(a) Training Loss (random label)
(b) Training Loss (true label)
(c) Validation Loss
Figure 5: Curves of loss. (a) shows that LSTM with 512-dimension and 256-dimension hidden size
can fit the samples of random labels completely. (b) and (c) show the training and validation losses
of the training process with random labels and early stop.
Amount of data	Weight decay(10-4)	LSTM32d	LSTM64d	LSTM128d	LSTM256d	LSTM512d
		43,845	97,925	236,805	637,445	1,930,245
Large data(645k)	Yes	60.56%	62.52%	63.69%	63.53%	63.62%
	No	61.08%	63.18%	63.59%	64.10%	64.23%
Small data(150k)	Yes	58.60%	59.15%	59.50%	59.84%	59.57%
	No	58.96%	59.04%	59.65%	59.75%	59.21%
Table 6: Test accuracy. Large data can increase the generalization ability over small data. A model
with more parameters can generalize better than one with less parameters with early stop.
5 Conclusion
We performed an empirical investigation of the capacity and generalization ability of neural net-
works on text. We give the definition of model capacity and the exact relationship between the
capacity of neural networks commonly used for text and the number of parameters from an empiri-
cal point of view. We compare the capacity of different architectures and find that CNN and attention
mechanism will effectively increase the capacity of the model, and the structured model converges
more readily and is more robust in the process of training. In the experiment of shuffled context
and short text, we conclude that although neural networks can fit any distribution, it is easier to fit
simple distribution than complex distribution. For the generalization ability of the model and the
model selection, we conclude that in the case of over-parameterized neural networks, although the
model of large capacity is more likely to fit, if we use appropriate regularization mechanism, such
as early stop, the model of large capacity will have a greater advantage than that of small capacity.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations, 2015.
8
Under review as a conference paper at ICLR 2018
Peter L. Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise
polynomial networks. Neural COmPutatiOn, 10(8):2159-2173, 1998.
Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension
bounds for piecewise linear neural networks. arXiv preprint arXiv:1703.02930, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. JOurnal Of Machine Learning Research, 12(Jul):2121-2159, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2016.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deeP network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning, 2015.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deeP learning.
arXiv PrePrint arXiv:1710.05468, 2017.
Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of the Con-
ference on EmPirical Methods in Natural Language Processing, 2014.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In International
Conference on Learning RePresentations, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deeP convo-
lutional neural networks. In Advanced in Neural Information Processing Systems, 2012.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the comPutational efficiency of training
neural networks. In Advanced in Neural Information Processing Systems, 2014.
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-sgd: Path-normalized oPti-
mization in deeP neural networks. In Advanced in Neural Information Processing Systems, 2015.
Behnam Neyshabur, Srinadh BhojanaPalli, David McAllester, and Nathan Srebro. ExPloring gener-
alization in deeP learning. arXiv PrePrint arXiv:1706.08947, 2017.
Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of resPonse regions of deeP
feed forward networks with Piece-wise linear activations. In International Conference on Learning
RePresentations, 2014.
Jeffrey Pennington, Richard Socher, and ChristoPher D Manning. Glove: Global vectors for word
rePresentation. In EmPirical Methods on Natural Language Processing, 2014.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why
and when can deeP - but not shallow - networks avoid the curse of dimensionality: a review.
International Journal of Automation and ComPuting, 2017.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hananneh Hajishirzi. Bidirectional attention
flow for machine comPrehension. In International Conference on Learning RePresentations, 2017.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
DroPout: A simPle way to Prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(Jun:1929-1958, 2014.
Dani Yogatama, Chris Dyer, Wang Ling, and Phil Blunsom. Generative and discriminative text
classification with recurrent neural networks. arXiv PrePrint arXiv:1703.01898, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deeP learning requires rethinking generalization. In International Conference on Learning RePre-
sentations, 2017.
9