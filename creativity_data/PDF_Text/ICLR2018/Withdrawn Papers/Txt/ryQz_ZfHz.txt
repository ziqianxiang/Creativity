Under review as a conference paper at ICLR 2018
Neural Variational Sparse Topic Model
Anonymous authors
Paper under double-blind review
Ab stract
Effectively inferring discriminative and coherent latent topics of short texts is a
critical task for many real world applications. Nevertheless, the task has been
proven to be a great challenge for traditional topic models due to the data spar-
sity problem induced by the characteristics of short texts. Moreover, the com-
plex inference algorithm also become a bottleneck for these traditional models to
rapidly explore variations. In this paper, we propose a novel model called Neu-
ral Variational Sparse Topic Model (NVSTM) based on a sparsity-enhanced topic
model named Sparse Topical Coding (STC). In the model, the auxiliary word
embeddings are utilized to improve the generation of representations. The Varia-
tional Autoencoder (VAE) approach is applied to inference the model efficiently,
which makes the model easy to explore extensions for its black-box inference pro-
cess. Experimental results on Web Snippets, 20Newsgroups, BBC and Biomedical
datasets show the effectiveness and efficiency of the model.
1	Introduction
With the great popularity of social networks and Q&A networks, short texts have been the prevalent
information format on the Internet. Uncovering latent topics from huge volume of short texts is
fundamental to many real world applications such as emergencies detection (Sakaki et al., 2010),
user interest modeling (Sasaki et al., 2014), and automatic query-reply (Peng et al., 2016). However,
short texts are characteristic of short document length, a very large vocabulary, a broad range of
topics, and snarled noise, leading to much sparse word co-occurrence information. Thus, the task
has been proven to be a great challenge to traditional topic models. Moreover, the complex inference
algorithm also become a bottleneck for these traditional models to rapidly explore variations.
To address the aforementioned issue, there are many previous works introducing new techniques
such as word embeddings and neural variational inference to topic models. Word embeddings are
the low-dimensional real-valued vectors for words. It have proven to be effective at capturing syn-
tactic and semantic information of words. Recently, many works have tried to incorporate word
embeddings into topic models to enrich topic modeling (Das et al., 2015; Hu & Tsujii, 2016; Xun
et al., 2017). Yet these models general rely on computationally expensive inference procedures
like Markov Chain Monte Carlo, which makes them hard to rapidly explore extensions. Even mi-
nor changes to model assumptions requires a re-deduction of the inference algorithms, which is
mathematic challenging and time consuming. With the advent of deep neural networks, the neural
variational inference has emerged as a powerful approach to unsupervised learning of complicated
distributions (Kingma & Welling, 2013; Rezende et al., 2014; Mnih & Gregor, 2014). It approxi-
mates the posterior of a generative model with a variational distribution parameterized by a neural
network, which allows back-propagation based function approximations in generative models. The
variational autoencoder (VAE) (Kingma & Welling, 2013), one of the most popular deep generative
models, has shown great promise in modeling complicated data. Motivated by the promising poten-
tial of VAE in building generative models with black-box inference process, there are many works
devoting to inference topic models with VAE (Srivastava & Sutton, 2017; Miao et al., 2017; Card
et al., 2017). However, these methods yield the same poor performance in short texts as LDA.
Based on the analysis above, we propose a Neural Variational Sparse Topic Model (NVSTM) based
on a sparsity-enhanced topic model STC for short texts. The model is parameterized with neural
networks and trained with VAE. It still follows the probabilistic characteristics of STC. Thus, the
model inherit s the advantages of both sparse topic models and deep neural networks. Additionally,
we exploit the auxiliary word embeddings to improve the generation of short text representations.
1
Under review as a conference paper at ICLR 2018
To summarize, the main contributions of this paper are as follows:
1.	We propose a novel Neural Variational Sparse Topic Model (NVSTM) to learn sparse rep-
resentations of short texts. The VAE is utilized to inference the model effectively.
2.	The general word semantic information is introduced to improve the sparse representations
of short texts via word embeddings.
3.	We conduct experiments on four datasets. Experimental results demonstrate our model’s
superiority in topic coherence and text classification accuracy.
The rest of this paper is organized as follows. First, we reviews related work. Then, we present
the details of the proposed NVSTM, followed by the experimental results. Finally, we draw our
conclusions.
2	Related Work
Topic models. Traditional topic models and their extensions (Archambeau et al., 2015; Blei et al.,
2003; Mcauliffe & Blei, 2008) have been widely applied to many tasks such as information retrieval,
document classification and so on. These models work well on long texts which have abundant word
co-occurrence information for learning, but get stuck in short texts. There have been many efforts to
address the data sparsity problem of short texts. To achieve sparse representations in the document-
topic and topic-term distributions, Williamson et al. (2010) introduced a Spike and Slab prior to
model the sparsity in finite and infinite latent topic structures of text. Similarly, Lin et al. (2014)
proposed a dual-sparse topic model that addresses the sparsity in both the topic mixtures and the
word usage. These models are inspired by the effect of the variation of the Dirichlet prior on the
probabilistic topic models. There are also some non-probabilistic sparse topic models aiming at
extracting focused topics and words by imposing various sparsity constraints. Heiler & Schnorr
(2006) formalized topic modeling as a problem of minimizing loss function regularized by lasso.
Subsequently, Zhu & Xing presented sparse topical coding (STC) by utilizing the Laplacian prior
to directly control the sparsity of inferred representations. However, over complicated inference
procedure of these sparse topic models has limited their applications and extensions.
Topic Models with Word Embeddings. Since word embeddings can capture the semantic mean-
ings of words via low-dimensional real-valued vectors, there have been a large number of works
on topic models that incorporate word embeddings to improve topic modeling. (Das et al., 2015)
proposed a new technique for topic modeling by treating the document as a collection of word em-
beddings and topics itself as multivariate Gaussian distributions in the embedding space. However,
the assumption that topics are unimodal in the embedding space is not appropriate, since topically
related words can occur distantly from each other in the embedding space. Therefore, (Hu & Tsu-
jii, 2016) proposed latent concept topic model (LCTM), which modeled a topic as a distribution of
concepts, where each concept defined another distribution of word vectors. (Nguyen et al., 2015)
proposed Latent Feature Topic Modeling (LFTM), which extended LDA to incorporate word em-
beddings as latent features. Lately, (Xun et al., 2017) proposed a novel correlated topic model using
word embeddings, which is enable to exploit the additional word-level correlation information in
word embeddings and directly model topic correlation in the continuous word embedding space.
However, these models also have trouble to rapidly explore extensions.
Neural Variational Inference for topic models. Neural variational inference is capable of ap-
proximating the posterior of a generative model with a variational distribution parameterized by a
neural network (Kingma & Welling, 2013; Rezende et al., 2014; Mnih & Gregor, 2014). The vari-
ational autoencoder (VAE), as one of the most popular neural variational inference approach, has
shown great promise in building generative models with black-box inference process (Kingma &
Welling, 2013). To break the bottleneck of over complicated inference procedure in topic models,
there are many efforts devoting to inference topic models with VAE. Srivastava & Sutton (2017)
presents auto-encoding variational Bayes (AEVB) based inference method for latent Dirichlet al-
location (LDA), tackling the problems caused by the Dirichlet prior and component collapsing in
AEVB. Miao et al. (2017) presents alternative neural approaches in topic modeling by providing
parameterized distributions over topics. It allows training the topic model via back-propagation un-
der the framework of neural variational inference. Card et al. (2017) combines certain motivating
ideas behind variations on topic models with modern techniques for variational inference to produce
2
Under review as a conference paper at ICLR 2018
a flexible framework for topic modeling that allows for rapid exploration of different models. Nev-
ertheless, aforementioned works are based on traditional LDA, thus bypass the sparsity problem of
short texts.
Drawing inspiration from the above analysis, we propose a novel neural variational sparse topic
model NVSTM based on VAE for short texts, which combines the merits of neural networks and
sparsity-enhanced topic models.
3	Neural Variational Sparse Topic Model
In this section, we start from describing Sparse Topical Coding (STC). Based on it, we further
propose Neural Variational Sparse Topic Model (NVSTM). Later, we focus on the discussion of the
inference process for NVSTM. Firstly, we define that D = {1, ..., M} is a document set with size
M, T = {1, ..., K} is a topic collection with K topics, V = {1, .., N} is a vocabulary with N words,
and wd = {wd,1, .., wd,|I|} is a vector of terms representing a document d, where I is the index of
words in document d, and wd,n (n ∈ I) is the frequency of word n in document d. Moreover, we
denote β ∈ RN ×K as a topic dictionary for the whole document set with k bases, θd ∈ RK is the
document code of document d and sd,n ∈ RK is the word code of word n in document d. To yield
interpretable patterns, (θ, s, β) are constrained to be to be non-negative.
3.1	Sparse Topical Coding
In standard STC, each document and each word is represented as a low-dimensional code in topic
space. Based on the topic dictionary β with K topic bases sampled from a uniform distribution, the
generative process is described as follows:
1.	Sample the document code of θd of document d from a prior p(θd) ~ Laplace(0, λ-1).
2.	For each observed word n in document d:
(a)	Sample the word code sd,n from a conditional distribution p(sd,n∣θd) ~ super -
g aussian(θd, γ-1, ρ-1).
(b)	Sample the observed word count wd,n from a distribution p(wd,n∣sd,n. * βn) ~
PoiSSon(Sd,n. * βn)
STC reconstructs each observed word count from a linear combination ofa set of topic bases, where
the word code is utilized as the coefficient vector. To achieve sparse word codes, STC defines
p(sd,n∣θd) as a product of two component distributions
p(sd,n∣θd)〜p(sd,n∣θd, Y)p(sd,n∣ρ)
where p(sd,n∖θd,γ) is an isotropic Gaussian distribution N(θ,γ-1), and p(sd,n∣ρ) is a Laplace
distribution Laplace(0, ρ-1). The composite distribution is super-Gaussian:
p(sd,n∖θd) H exp(-γ∖∖sd,n 一 θd∖∖2 一 ρ∖s∖)
With the Laplace term, the composite distribution tends to yield sparse word codes. For the same
purpose, the prior distribution p(θd) of sparse document codes is a Laplace prior Laplace(0, λ-1).
Additionally, According to the above generative process, we have the joint distribution:
p(θ, s, w∖β) =	p(θd)	p(sd,n∖θd)p(wd,n∖sd,n,βn)	(1)
To simplify the calculation, the document code can be collapsed and later obtained via an aggrega-
tion of the individual word codes of all its terms. Although STC has closed form coordinate descent
equations for parameters (θ, s, β), it is inflexible for its complex inference process.
3.2	Generative Process for NVSTM
To address the aforementioned issue, we introduce black box inference methods into STC. We
present NVSTM based on VAE and introduces word embeddings. As in (Bai et al., 2013), we re-
move the document code and generate it via a simple aggregation of all sampled word codes among
3
Under review as a conference paper at ICLR 2018
all topics:
D Nd	D Nd K
θd=	sd,nk βkn/	sd,nk βkn
d=1 n=1	d=1 n=1 k=1
. Analogous to the generative process in STC, our model follows the generative story below for each
document d:
1.	For each word n in document d:
(a)	Sample a latent variable word code sd,n ~ U(-0.5,0.5).
(b)	Sample the observed word count Wn fromp(wd,n∖sd,n, βn) ~ Poisson(sd,n∙ * βn)
Word embeddings
<-GDOOOOOC¾
Cδooooo(3
Cooooood)
Figure 1: The graphical model of NVSTM.
The graphical representation of NVSTM is depicted in Figure 1. Different from STC, we replace
the super-Gaussian with a uniform distribution. In the inference process, we adopt the variational
posterior Laplace(sd,nk; 0, σd,nk(wd,n)) to approximate the intractable posterior p(sd,nk ∖wd,n) for
the sparse word codes, where σd,nk is the scale parameter of Laplace distribution. Therefore, in the
above generative process, each word code vector is generated from the uniform prior distribution.
The observed word count is sampled from Poisson distribution. Different from traditional STC, we
replace the uniform distribution of the topic dictionary with a topic dictionary neural network. In the
topic dictionary neural network, we introduce the word semantic information via word embeddings
to enrich the feature space for short texts. The topic dictionary neural network is comprised of
following:
Word embedding layer (E ∈ RN ×300): Supposing the word number of the vocabulary is N , this
layer devotes to transform each word to a distributed embedding representation. Here, we adopt the
pre-trained embeddings by GloVe based on a large Wikipedia dataset 1. Given a word embedding
matrix E, we map each word to a 300-dimensional embedding vector, which can capture subtle
semantic relationships between words.
Topic dictionary layers (β ∈ RN×K): This layers aim at converting E to a topic dictionary similar
to the one in STC.
β = f (E)	(2)
where f is a multilayer perceptron. To conform to the framework of STC, we make a simplex
projection among the output of topic dictionary neural network. We normalize each column of the
dictionary via the simplex projection as follow:
β.k = project(β.k), ∀k	(3)
The simplex projection is the same as the sparsemax activation function in (Martins & Astudillo,
2016), which declares how the Jacobian of the projection can be efficiently computed, providing
the theoretical base of its employment in a neural network trained with backpropagation. After the
simplex projection, each column of the topic dictionary is promised to be sparse, non-negative and
united.
Based the above generative process, the traditional variational inference for the model is to minimize
the follow optimization problem, which is a lower bound to the marginal log likelihood:
L(γ∖β) = DKL[q(s∖γ)∖∖p(s∖w, β)] - logp(w∖β)	(4)
where q (s∖γ ) is approximate variational posterior, and γ is the variational parameter.
1http://nlp.stanford.edu/projects/glove/
4
Under review as a conference paper at ICLR 2018
3.3	Variational Autoencoder for NVSTM
In this paper, we employ the VAE to carry out neural variational inference for our model. Varia-
tional Autoencoder (VAE) is one of the most popular deep generative network. It is a black-box
variational method which bridges the conceptual and language gap of neural networks and prob-
ability generative models. From neural network perspective, a variational autoencoder consists of
an encoder network, a decoder network, and a loss function. In our model, the encoder network is
to parametrize the approximate posterior q&(s|w), which takes input as the observed word count to
output the latent variable s with the variational parameters θ:
logσd,nk = fe(wd,n)	(5)
where fe(wd,n) is a multilayer perceptron acting on the word counts wd,n in document d, and
logσd,nk is the scale parameter of the approximate posterior, from which the word codes sd,nk
are sampled. The decoder network outputs the observed data w with given s and the generative
parameters φ, which is denoted as pφ(w∣s,β). According to STC, We define the decoder network
Pφd,n(wd,n∖sd,n) = Poisson(wd,n；(sd,n. * βn)) as apoisson MLP:
logwd,n = fd(sd,n. * Bn )	* 1 2 3 4 5 (6)
where fd is a multilayer perceptron. Based on VAE, we rewrite the ELBO as:
L(θ,φ∖β) = -DKL[qθ (Slw)Ilp(S)]+ 鸟03W)(IogPφ(Wls,β))	⑺
The first term is a regularizer that constraints the Kullback-Leibler divergence between the encoder’s
distribution distribution and the prior of the latent variables. The second term is the reconstruction
loss, which encourages the decoder to reconstruct the data in minimum cost.
3.4	The Reparameterization Trick and Optimizing
We devote to differentiate and optimize the lower bound above with stochastic gradient decent
(SGD). However, the gradient of the lower bound is tricky since the error is unable to back prop-
agate through a random drawn variable s, which is a non-continuous and has no gradient. Similar
to the standard VAE, we make a differentiable transformation, called reparameterization trick.We
approximate s with an auxiliary noise variable ε ~ U(-0.5,0.5):
sd,nk 〜Laptace(0,σd,nk) → sd,nk = -σ.,nksign(ε)ln(1 — 2∣ε∣),ε 〜U(-0.5,0.5)	(8)
Through reparametrization, we can take s as a function with the parameter b deriving from the
encoder network. It allows the reconstruction error to flow through the whole network. Figure
2 presents the complete VAE inference process for NVSTM. Moreover, in order to achieve inter-
pretable word codes as in STC, we constrain s to be non-negative, activation function on the output
s of encoder. After apply the reparameterization trick to the variational lower bound, we can yield
DN	DNK	K
L(Θ) =	(1 + log2σd,
n) + Eε 〜U (-0.5,0.5)	(	sd,nk βnk - wd,nln(	sd,nk βnk ))
d=1 n=1	d=1 n=1 k=1	k=1	(9)
where Θ represents the set of all the model. As explained above, the decoding term logp(wd,n lsd,nk,
βnk) is the Poisson distribution, and β is generated by a topic dictionary neural network. After the
differentiable transformation, the variation objective function can be computed in closed form and
efficiently solved with SGD. The detailed algorithm is shown in Algorithm 1.
Algorithm 1 Training Algorithm for NVSTM
Input: initialize θ, φ, W
1: repeat
2: WM . Random mini-batch of M word counts from full datasets
3: ε J Random samples from noise distribution p(ε)
4:	g J 5θ,φ,wL(θ, Φ; wM, ε)
5: θ, φ, W J Update parameters using SGD
6: until convergence
5
Under review as a conference paper at ICLR 2018
qθ(s | w)	P人w | S, β)
Word embeddings
Figure 2: The VAE inference for NVSTM, the VAE is implemented as a feedforward neural network.
4	Experiments
4.1	Data and Setting
To evaluate the performance of our model, we present a series of experiments below. The objectives
of the experiments include: (1) the qualitative evaluations: classification accuracy of documents and
sparse ratio of latent representations; (2) the qualitative inspection: the quality of extracted topics
and document representations. Our evaluation is based on the four datasets:
•	20Newsgroups: The classic 20 newsgroups dataset, which is comprised of 18775 news-
group articles with 20 categories, and contains 60698 unique words 2.
•	Web Snippet: The web snippet dataset, which includes 12340 Web search snippets in 8
categories. We remove the words with fewer than 3 characters or whose document fre-
quency less than 3 in the dataset. After the preprocessing, it contains 5581 unique words.3.
•	BBC: It consists of 2225 BBC news articles from 2004-2005 with 5 classes. We only use
the title and headline of each article. We remove the stop words and the words whose
document frequency less than 3 in the dataset 4.
•	Biomedical: It consists of 20000 paper titles from 20 different MeSH in BioASQ’s official
website. We convert letters into lower case and remove the words whose document fre-
quency less than 3 in the dataset. After preprocessing, there are 19989 documents with 20
classes 5.
Statistics on the four datasets after preprocessing is reported in Table 1.
We compare our model with five topic models:
•	LDA (Blei et al., 2003). A classical probabilistic topic model. We use the open source LDA
implemented by collapsed Gibbs sampling 6. We use the default settings with iteration
2http://www.qwone.com/ jason/20Newsgroups/
3http://jwebpro.sourceforge.net/data-web-snippets.tar.gz
4http://mlg.ucd.ie/datasets/bbc.html
5http://participants-area.bioasq.org/
6https://pypi.python.org/pypi/lda
6
Under review as a conference paper at ICLR 2018
Table 1: Statistics on the two datasets. Label: the number of ground truth labels or categories; Docs:
the total number of documents; Words: average number of words per document.
Dataset	Label	Docs	Words	Vocabulary
20Newsgroups	-20Γ~	18775	-13^	-60698
Web Snippet	-8-	12265	10.72	5581
BBC	-5-	2225	11.97	2453
Biomedical	20	19989	7.95	6887 一
number n = 2000, the Dirichlet parameter for distribution over topics α = 0.1 and the
Dirichlet parameter for distribution over words η = 0.01.
•	STC (Zhu & Xing). A sparsity-enhanced topic model which has been proven to perform
better than many existing models. We adopt the implementation of STC released by its
authors 7. We set the regularization constants as λ = 0.2, ρ = 0.001 and the maximum
number of iterations of hierarchical sparse coding, dictionary learning as 100.
•	NTM (Cao et al., 2015). A recently proposed neural network based topic model, which
has been reported to outperform the Replicated Softmax model 8. In NTM, the learn-
ing rate is 0.01 and the regularization factor is 0.001. During the pre-training proce-
dure for all weight matrices, they are initialized with a uniform distribution in interval [-
4*sqrt(6./(n_visible+n_hidden)),4*sqrt(6./(n_visible+n_hidden))], where n_visible=784 and
n_hidden=500.
•	DocNADE (Larochelle & Lauly, 2012). An unsupervised neural network topic model of
documents and have shown that it is a competitive model both as a generative model and
as a document representation learning algorithm 9. In DocNADE, we choose the sigmoid
activate function, the hidden size is 50, the learning rate is 0.01 , the bath size is 64 and the
max training number is 1000.
•	GaussianLDA (Das et al., 2015). A new technique for topic modeling by treating the
document as a collection of word embeddings and topics itself as multivariate Gaussian
distributions in the embedding space 10. We use default values for the parameters.
Our model is implemented in Python via TensorFlow. For four datasets, we utilize the pre-trained
300-dimensional word embeddings from Wikipedia by GloVe, which is fixed during training. For
each out-of-vocabulary word, we sample a random vector from a normal distribution in interval
[0, 1]. We adopted ADAM optimizer for weight updating with an initial learning rate of 4e - 4 for
four dataset. All weight matrices are initialized with a uniform distribution in interval [0, 1e - 5]. In
practice, we found that our model is stable with the size of hidden layer, and set it to 500.
4.2	Classification Accuracy
To evaluate the effectiveness of the representation of documents learned by NVSTM, we perform
text classification tasks on web snippet, 20NG, BBC and Biomedical using the document codes
learned by topic models as the feature representation in a multi-class SVM. For each method, after
obtaining the document representations of the training and test sets, we trained an classifier on the
training set using the scikit-learn library. We then evaluated its predictive performance on the test
set. On web snippet, we utilize 80% documents for training and 20% for testing. On the 20NG
dataset, we keep 60% documents for training and 40% for testing, which is the same configuration
as in (Lin et al., 2014). For BBC and Biomedical dataset, we also keep 60% documents for training
and 40% for testing. Table 2 and Table 3 report the classification accuracy under different meth-
ods with different settings on the number of topics among the four datasets. It clearly denotes that
1) In the four datasets, the NVSTM yields the highest accuracy. 2) In general, the neural network
based NVSTM, NTM ,DocNADE and GLDA generate better document representations than STC
7http://bigml.cs.tsinghua.edu.cn/ jun/stc.shtml/
8https://github.com/elbamos/NeuralTopicModels
9https://github.com/huashiyiqike/TMBP/tree/master/DocNADE
10https://github.com/rajarshd/Gaussian_LDA
7
Under review as a conference paper at ICLR 2018
and LDA, demonstrating the representative advantage of neural networks in distributed word repre-
sentations. 3) Sparse models NVSTM are superior to non-sparse models (DocNADE, NTM, GLDA
and LDA) separately. It indicates that sparse topic models are more capable to extract topics from
short documents.
Table 2: Classification accuracy of different models on Web snippet and 20NG, with different num-
ber of topic K settings.
Dataset	Snippet						20NG					
k	50	75	100	125	150	50	100	150	200	250
LDA	0.682	0.615	0.592	0.583	0.573	0.545	0.615	0.607	0.613	0.623
STC	0.678	0.686	0.699	0.724	0.701	0.602	0.631	0.647	0.652	0.654
-NTM-	0.660	0.667	0.723	0.732	0.747	0.623	0.627	0.641	0.632	0.667
DocNADE	0.656	0.656	0.645	0.646	0.647	0.682	0.670	0.646	0.583	0.573
-GLDA-	0.669	0.689	0.675	0.670	0.623	0.367	0.438	0.465	0.496	0.526
NVSTM	0.742	0.808	0.799	0.805	0.818	0.654	0.671	0.672	0.683	0.691
4.3	Characteristics of Code Representation
In this part, we quantitatively investigate the word codes and documents codes learned by our model.
Word code: We compute the average word code as Sn =齐 Pd∈0ιτ sd,n over all documents that
word n appears in. Table 4 shows the average word codes of some representative words learned
by NVSTM and LDA in 8 categories of web snippet. For each category, we also present the topics
learned by NVSTM in Table 5. We list top-9 words according to their probabilities under each
topic. In Table 4, the results illustrate that the codes discovered by NVSTM are apparently much
sparser than those discovered by LDA. It tends to focus on narrow spectrum of topics and obtains
discriminative and sparse representations of word. In contrast, LDA generates word codes with many
non-zeros due to the data sparsity, leading to a confused topic distribution. Besides, in NVSTM, it
is clear that each non-zero element in the word codes represents the topical meaning of words in
corresponding position. The weights of these elements express their relationship with the topics.
Noticed that there are words (e.g. candidates) have only a small range of topical meanings, indicating
a narrow usage of those terms. While other words (e.g. hockey and marketing) tend to have a broad
spectrum of topical meanings, denoting a general usage of those terms.
Document code:	Here, each document code is calculated as θd =
D Nd	D Nd K
sd,nk βkn/	sd,nk βkn (Bai et al., 2013). To demonstrate the quality of the
d=1 n=1	d=1 n=1 k=1
learned representations by our model, we produce a t-SNE projection with for the document codes
of the four datasets learned by our model in Figure 3. For Web Snippet, we sample 10% of the
whole document codes. For 20newsgroups and Biomedical, we sample 30% of the whole document
codes. As for BBC, we present the whole document codes. It is obvious to see that all documents
are clustered into distinct categories, which is equal to the ground truth number of categories in the
four datasets. It proves the semantic effectiveness of the documents codes learned by our model.
Table 3: Classification accuracy of different models on BBC and Biomedical, with different number
of topic K settings.
Dataset		BBC						Biomedical				
k	20	30	40	50	60	50	100	150	200	250
LDA	0.784	0.774	0.796	0.762	0.758	0.536	0.534	0.547	0.534	0.541
STC	0.602	0.593	0.599	0.634	0.604	0.351	0.405	0.439	0.464	0.494
-NTM-	0.639	0.727	0.710	0.699	0.654	0.533	0.545	0.573	0.627	0.657
DocNADE	0.793	0.839	0.832	0.834	0.819	0.597	0.588	0.588	0.583	0.582
-GLDA-	0.609	0.566	0.573	0.564	0.567	0.482	0.515	0.497	0.483	0.513
NVSTM	0.783	0.835	0.833	0.836	0.813	0.567	0.623	0.645	0.671	0.664
8
Under review as a conference paper at ICLR 2018
Table 4: The word codes of representative words for different categories discovered by NVSTM and
LDA.
Category
Business
Health
Politics
-society
Engineering
Sports
NVSTM
LDA
K
0.025
0 .0 2 0
0 .0 15
0 .0 10
0 .0 0 5
0.000
0	4 0	8 0	12 0	16 0	2 00
K
0.030
I ,.	.,1	∣^Her⅛l] I	
0 .0 2 4
0 .0 18
0 .0 12
0 .0 0 6
0.000
0	4 0	8 0	12 0	16 0	2 00
________	K
0.030
0 .0 2 4
0 .0 18
0 .0 12
0 .0 0 6
0.000
0	4 0	8 0	12 0	16 0	2 00
K
①Poo p」OM
①Poo p」OM
K
0	.0	0	0	4	0
0	.0	0	0	3	2
0	.0	0	0	2	4
0	.0	0	0	16
0	.0	0	0	0	8
0	.0	0	0	0	0
0 .0 0 10
0	.0	0	0	8
0	.0	0	0	6
0	.0	0	0	4
0	.0	0	0	2
0	.0	0	0	0
[^^[hospitaΓ]	
0	40	80	120	160	200
K
00
20
60
1
20
1
80
40
0
①poɔ Pos
①Poo ①Poo p」OM
K
K
9
Under review as a conference paper at ICLR 2018
Table 5: The topics discovered by NVSTM.
Category	Topic
Business	T6: marketing parascope development business sustainable partnerships movieactors developing partnership T63: finance loans equity loan mortgage financing banking investment mortgages T67: investing ratneshwar investments investment investors invest equity niddk income T133: products source product quality premium csail content manufacture socialsciences T144: development sciserv ecommerce developing innovation developers business marketing projects T176: trade trading markets commodities commodity stocks market parascope currencies
Computers	T38: processor microprocessor processors llnl signonsandiego cpu microprocessors intel cores T108: memory laptop computer computers processor nutritionsource laptops intel disk T112: firefox mozilla netscape macintosh linux windows adobe verizon zdnet T118: systems system control security controls remote automatic monitoring automation T121: msn yahoo firefox aol gmail java algorithm algorithms signonsandiego T159: quantum computing space nasa cpu computational computers astrophysics physics
culture -arts- entertainment	T3: ocos parascope space socialsciences living world academyawards planet intradoc T5: film films indie filmmaker filmmakers movie comedy screening filmmaking T10: sound audio voice acoustic recordings recording listening bass song T16: photography poetry poems prose poet writing getthejob poem photographer T58: sculptor painter artist sculpture sculptures paintings artists artwork surrealist T177: art sculpture socialsciences sculptures painter paintings sculptor painting pcguide
education - science	T41: mathematics physics maths professors students undergraduate science teachers ncidod T59: undergraduate degree student undergraduates faculty students acts particles mathematics T82: teaching school mathforum english teacher mathematics education college schools T102: lecture book lectures papers essay journal seminar conference books T109: topics mathforum essays lectures articles journals emedicinehealth literature syllabus T147: science scientific research journals published theories sciences publications articles
Engineering	T7: machine machines software printer llnl pcguide converter freeware kurose T69: engine engines fuel diesel cylinder gearbox piston motor petrol T90: factory inc steel searchsmb chrome ford socialsciences wieeless ltd T100: device cancertopics devices modem cable died wireless semiconductor connection T114: gasoline diesel fuel petrol engines engine emissions gas combustion T141: salary cancertopics engineer nutritionsource engineering engineers jobs job talent
Health	T55: therapists medical physicians therapy nurses pediatric therapist clinics doctors T56: calories nutritional vitamins calorie diet carbohydrates fats nutrition foods T86: hospital webobjects home nutritionsource clinic homes nursing emedicinehealth center T124: disease cancer lung flu cancers infection influenza arthritis infections T142: vitamins foods herbal diet alcohol supplements vitamin oils nutritional T156: drugs drug medications medication tobacco smoking alcohol prescription cancer
Politics -society	T27: culture democracy capitalism politics socialism society socialist ideology democratic T103: secretary party fhlb highfat parliamentary managementhelp parties elections election T128: candidates candidate election elections presidential ballot electoral vote voting T148: scandal election president senator minister senate presidency elections chairman T155: participatory debates debate democracy activism democratic pluralism grassroots encourage T168: party parties election elections electoral parliamentary parliament presidential political
Engineering	T7: machine machines software printer llnl pcguide converter freeware kurose T69: engine engines fuel diesel cylinder gearbox piston motor petrol T90: factory inc steel searchsmb chrome ford socialsciences wieeless ltd T100: device cancertopics devices modem cable died wireless semiconductor connection T114: gasoline diesel fuel petrol engines engine emissions gas combustion T141: salary cancertopics engineer nutritionsource engineering engineers jobs job talent
Sports	T72: player socialsciences players essortment game games straight button bottom T79: hockey basketball football soccer volleyball sports baseball tennis sport T95: resort village resorts town mountain peaceful hotel valley coastal T119: tennis volleyball emnlp swimming tournaments cricket hockey softball skiing T125: poker tournaments games tournament game betting gaming competitions football T127: football league soccer rugby championship basketball championships playoffs leagues
10
Under review as a conference paper at ICLR 2018
(b) 20NeWsgroUPs
(a) Web snippet
-60	-40	-20	0	20	40	60	80
(c) Biomedical
FigUre 3: t-SNE projection of the estimated docUment codes from Web snippet, 20NeWsgroUps,
BBC and Biomedical. The vectors are learned by the NVSTM model With 200 topics, each color
represents one category from the different categories of the dataset.
5	Conclusion
We propose a neUral sparsity-enhanced topic model NVSTM, Which is the first effort in introdUcing
effective VAE inference algorithm to STC as far as We knoW. We take advantage of VAE to simplify
the inference process, Which reqUire no model-specific algorithm derivations. With the employing of
Word embeddings and neUral netWork frameWork, NVSTM is able to generate clearer and semantic-
enriched representations for short texts. The evalUation resUlts demonstrate the effectiveness and
efficiency of oUr model. FUtUre Work can inclUde extending oUr model With other deep generative
models, sUch as generative adversarial netWork (GAN).
References
Cedric ArchambeaU, Balaji Lakshminarayanan, and GUillaUme BoUchard. Latent ibp compoUnd
dirichlet allocation. IEEE transactions on pattern analysis and machine intelligence, 37(2):321—
333, 2015.
LU Bai, Jiafeng GUo, Yanyan Lan, and XUeqi Cheng. GroUp sparse topical coding: from code to
topic. In Proceedings of the sixth ACM international conference on Web search and data mining,
pp. 315-324. ACM, 2013.
David M Blei, AndreW Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine
Learning research, 3(Jan):993-1022, 2003.
Ziqiang Cao, SUjian Li, Yang LiU, Wenjie Li, and Heng Ji. A novel neUral topic model and its
sUpervised extension. In AAAI, pp. 2210-2216, 2015.
Dallas Card, Chenhao Tan, and Noah A Smith. A neUral frameWork for generalized topic models.
arXiv preprint arXiv:1705.09296, 2017.
11
Under review as a conference paper at ICLR 2018
Rajarshi Das, Manzil Zaheer, and Chris Dyer. Gaussian lda for topic models with word embeddings.
In ACL (1),pp. 795-804, 2015.
Matthias Heiler and Christoph Schnorr. Learning sparse representations by non-negative matrix
factorization and sequential cone programming. Journal of Machine Learning Research, 7(Jul):
1385-1407, 2006.
Weihua Hu and Junichi Tsujii. A latent concept topic model for robust topic inference using word
embeddings. In The 54th Annual Meeting of the Association for Computational Linguistics, pp.
380, 2016.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Hugo Larochelle and Stanislas Lauly. A neural autoregressive topic model. In Advances in Neural
Information Processing Systems, pp. 2708-2716, 2012.
Tianyi Lin, Wentao Tian, Qiaozhu Mei, and Hong Cheng. The dual-sparse topic model: mining
focused topics and focused terms in short text. In Proceedings of the 23rd international conference
on World wide web, pp. 539-550. ACM, 2014.
Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention
and multi-label classification. In International Conference on Machine Learning, pp. 1614-1623,
2016.
Jon D Mcauliffe and David M Blei. Supervised topic models. In Advances in neural information
processing systems, pp. 121-128, 2008.
Yishu Miao, Edward Grefenstette, and Phil Blunsom. Discovering discrete latent topics with neural
variational inference. arXiv preprint arXiv:1706.00359, 2017.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv
preprint arXiv:1402.0030, 2014.
Dat Quoc Nguyen, Richard Billingsley, Lan Du, and Mark Johnson. Improving topic models with la-
tent feature word representations. Transactions of the Association for Computational Linguistics,
3:299-313, 2015.
Min Peng, Binlong Gao, Jiahui Zhu, Jiajia Huang, Mengting Yuan, and Fei Li. High quality infor-
mation extraction and query-oriented summarization for automatic query-reply in social network.
Expert Systems with Applications, 44:92-101, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings of the 19th international conference on World
wide web, pp. 851-860. ACM, 2010.
Kentaro Sasaki, Tomohiro Yoshikawa, and Takeshi Furuhashi. Online topic model for twitter con-
sidering dynamics of user interests and topic trends. In EMNLP, pp. 1977-1985, 2014.
Akash Srivastava and Charles Sutton. Neural variational inference for topic models. In Proceedings
of the International Conference on Learning Representations (ICLR), 2017.
Sinead Williamson, Chong Wang, Katherine A Heller, and David M Blei. The ibp compound dirich-
let process and its application to focused topic modeling. In Proceedings of the 27th international
conference on machine learning (ICML-10), pp. 1151-1158, 2010.
Guangxu Xun, Yaliang Li, Wayne Xin Zhao, Jing Gao, and Aidong Zhang. A correlated topic model
using word embeddings. In Proceedings of the 26th International Joint Conference on Artificial
Intelligence.[doK 10.24963/ijcai. 2017/588], 2017.
Jun Zhu and Eric P Xing. Sparse topical coding. In Proceedings of the 27th International Conference
on Uncertainty in Artificial Intelligence, volume 831, pp. 838.
12