Under review as a conference paper at ICLR 2018
The Multilinear Structure of ReLU Networks
Anonymous authors
Paper under double-blind review
Ab stract
We study the loss surface of neural networks that involve only rectified linear unit
(ReLU) nonlinearities from a theoretical point-of-view. Any such network defines
a piecewise multilinear form in parameter space. As a consequence, optima of
such networks generically occur in non-differentiable regions of parameter space
and so any understanding of such networks must carefully take into account their
non-smooth nature. We then proceed to leverage this multilinear structure in an
analysis of a neural network with one hidden-layer. Under the assumption of
linearly separable data, the piecewise bilinear structure of the loss allows us to
provide an explicit description of all critical points.
1	Introduction
Empirical practice tends to show that modern neural networks have relatively benign loss surfaces, in
the sense that training a deep network proves less challenging than the non-convex and non-smooth
nature of the optimization would naively suggest. Many theoretical efforts have attempted to explain
this phenomenon and, more broadly, the successful optimization of deep networks in general (Gori
& Tesi (1992); Choromanska et al. (2015); Kawaguchi (2016)). The properties of the loss surface
of neural networks remain poorly understood despite these many efforts. Developing of a coherent
mathematical understanding of them is therefore one of the major open problems in deep learning.
We focus on investigating the loss surfaces that arise from feed-forward neural networks where
ReLUs σ(x) := max(x, 0) = (x)+ account for all nonlinearities present in the network. We allow
the transformations defining the hidden-layers of the network to take the form of fully connected
affine transformations, convolutional transformations or some other combination of structured affine
maps. For the network criterion we elect to use the hinge loss objective
R
'(y,r) = Eσ(1 + yq- yr)
q=1
for classification. We use this choice for two reasons. First, the hinge loss is the natural choice
if we wish to maintain ReLU nonlinearities throughout the network. As the only nonlinearities
from input to loss are ReLUs, each input simply flows through a succession of affine and piecewise
linear transformations. This rather homogeneous structure allows us to derive results concerning
loss surface of such networks. Second, this choice also allows us to avoid certain pathologies that
arise with other objectives; global minimizers generally do not exist, for instance, when using a
logistic loss instead of the hinge loss.
To see the type of structure that emerges in these networks, let Ω denote the space of network
parameters and let L(ω ) denote the loss. Each nonlinearity involved in the network, including the
hinge loss, is either active (σ(x) > 0) or inactive (σ(x) = 0) at any point ω in parameter space.
This dichotomy leads to a partition of the parameter space
Ω = Ωι ∪ Ω2 ∪ ... ∪ Ωm
(1)
into cells, where each cell Ωu corresponds to a given activation pattern of the nonlinearities. Cross-
ing the boundary of a cell Ωu corresponds to a ReLU switching from active to inactive, or vice-versa.
The loss L(ω) is therefore smooth in the interior of cells and (potentially) non-differentiable on cell
boundaries. In this way the decomposition (1) provides a description of the smooth and non-smooth
regions of parameter space.
1
Under review as a conference paper at ICLR 2018
(a) Loss Surface L(ω)
(b) Parameter Space Ω
(c) Loss Surface L(ω)
Figure 1: The loss surface corresponding to a piecewise multilinear form. In (a): Local minima
are located in the interior of the flat cells Ω(3) and Ω(5) (type I), on the boundary between cells
Ω ⑴ and Ω ⑵(type II) and the boundary between cells Ω ⑷ and Ω ⑸(type II). In (b): Parameter
space Ω = R2 decomposes into a partition of five cells. The loss L on each cell is a SUm of
multilinear forms. In (c): A rotation of (a) shows the saddle-like surface of the nontrivial forms on
cells Ω⑴，Ω⑵ and Ω(4).
We begin by using this decomposition to show that, when restricted to a fixed cell Ωu, the loss L
is a sum of multilinear forms. Thus the loss L(ω) is a piecewise multilinear form1, and different
multilinear forms characterize the loss on different cells. To see the significance of this structure,
recall that a multilinear form is a function φ : Rd1 × . . . × Rdn → R which is linear with respect to
each of its inputs when the other inputs are fixed. That is, each of the n linear relations
φ(v1,...,αvk +βwk,...,vn) = αφ(v1,...,vk,...,vn) +βφ(v1,...,wk,...,vn)
hold. The functions φ(x, y) = xy or φ(x, y, z) = xyz provide canonical examples of multilinear
forms, and both of these functions clearly have a saddle like structure. In fact any nontrivial multi-
linear form has such a saddle-type structure, for the Hessian matrix of a nontrivial multilinear form
always has at least one strictly positive and one strictly negative eigenvalue (see appendix for a proof
of this statement). Consequently, the graph of a nontrivial multilinear form always has at least one
direction of positive curvature and at least one direction of negative curvature. We therefore have
the following picture for the loss surface L(ω) of a piecewise multilinear form: inside each cell Ωu
the loss is either flat, linear or has a saddle-like structure; see figure 1 for a visual example. It is
therefore impossible for a local minima to occur in the interior of a cell on which the loss has a
linear or saddle-like structure, and so neural networks with ReLU nonlinearities have only two types
of local minima —
•	Type I (Smooth): Those local minima that occur in the interior ofa cell with constant loss.
•	Type II (Non-smooth): Those local minima that occur on a cell boundary.
We state this result precisely in theorem 1, but figure 1 already shows the presence of these two types
of local minima.
This observation has several consequences. A (continuous time) gradient decent algorithm can
never reach a type I local minimum. As soon as the algorithm enters a flat cell it must stop since the
gradient vanishes at such points. The descent therefore terminates on the boundary of the cell, and
so only non-smooth local minima arise when using a local, gradient-based algorithm. Moreover,
this structure has potential implications for other various optimization algorithms. An off-the-shelf
Newton method, for example, is inappropriate for such networks since the Hessian of L is never
positive definite and typically is indefinite. In other words, any study of algorithms for such a loss
must take into account both the nonsmooth structure and the indefinite structure of the loss surface.
Local minimizers simply cannot be studied using second-order (i.e. Hessian) information.
We then proceed to leverage this structure in an analysis of a neural network with one hidden-layer.
Under the assumption of linearly separable data, the piecewise bilinear structure of the loss allows us
to provide an explicit description of critical points. The reasons for this analysis are two-fold. First,
1 By “piecewise multilinear form” we really mean a sum of piecewise multilinear forms.
2
Under review as a conference paper at ICLR 2018
(a) L(ω) = 0
(b) L(ω) = 3/12
(c) L(ω) = 12/12
Figure 2: Three different local minima of the loss L(ω) for a network with two hidden neurons.
Points belonging to class +1 (resp. -1) are denoted by stars (resp. squares). Data points for which
the loss is zero (solved points) are colored in green, while data points with non-zero loss (unsolved
points) are in red.
it allows us to understand how the addition of depth (i.e. a hidden-layer) affects the loss surface
of a simple classification task. With separable data and a purely linear classifier, the corresponding
convex optimization problem has only global minimizers with zero loss. Adding a hidden-layer
affects this structure and complicates the loss surface, in the sense that non-optimal local minima
now occur. Our analysis characterizes this precisely. Secondly, this simple problem serves as a
model for the top of a deep network. As we generally expect linearly separable features given
enough depth, any non-optimal critical points in a network with one hidden-layer and separable data
might manifest in a deep network as well.
To describe the results of this analysis we recall that the hinge loss for binary classification takes the
form
'(y,t) = σ(1- ty),	Q)
where y denotes the scalar output of the network and t ∈ {+1, -1} the classification target. For
simplicity of exposition, consider the resulting one hidden-layer network without an output bias, i.e.
K
y = X Vk σ (hWk, Xi + bk)	(3)
k=1
with〈•, ∙ denoting the Euclidian inner product. The network has K hidden neurons, and each
hidden neuron has an associated hyperplane (wq ∙) + bk as well as a scalar weight Vk used to form
the output. Figure 2 shows three different local minima of such a network with two hidden neurons.
The first panel, figure 2(a), shows a global minimum where all the data points have zero loss. Figure
2(b) shows a local minimum. All unsolved data points, namely those that contribute a non-zero
value to the loss, lie on the “blind side” of the two hyperplanes. For each of these data points the
corresponding network output y vanishes and so the loss is '(y,t) = 1 for these unsolved points.
Small perturbations of the hyperplanes or of the values of the Vk do not change the fact that these
data points lie on the blind side of the two hyperplanes. Their loss will not decrease under small
perturbations, and so the configuration is, in fact, a local minimum. The same reasoning shows that
the configuration in figure 2(c) is also a local minimum. All data points have loss equal to one,
and so this local minimum is also a global maximum. Finally, these configurations still define local
minimizers if we allow small perturbations to the data points; If the data points represent features of
a deep network then such configurations will define local minimizers of a deep network as well.
Despite the presence of sub-optimal local minimizers, the local minima depicted in figure 2 are
somehow trivial cases. They simply come from the fact that, due to inactive ReLUs, some data points
are completely ignored by the network, and this fact cannot be changed by small perturbations. We
show that for binary classification tasks with linearly separable data these are, in fact, the only
possible local minima that occur. More precisely, let us say that a hyperplane hWk, •i +bk is active if
the corresponding Vk is non-zero. Then at any local minima, a data point with nonzero loss must
lie in the blind side of all active hyperplanes. This result remains true if a bias is added in (3) to the
output neuron. For multi-class tasks the answer is more delicate (c.f. section 4), but if we apply an
appropriate modifications then the multilinear structure allows us to conclude the analogous result
for multi-class partitioning problems as well. In fact, this analysis shows how to reduce the study of
any multilinear deep network to that of a binary classification problem.
3
Under review as a conference paper at ICLR 2018
Previous work also address the loss surface of ReLU neural networks, c.f. Safran & Shamir (2016)
and Choromanska et al. (2015). The first reference uses ReLU nonlinearities to partition the parame-
ter space into basins that, while similar in spirit, are different from our notion of cells. They estimate
the probability of initializing the network in a basin containing a good local minimum under various
assumptions on the distribution of data points. However, as noted by the authors, there is no reason
to believe that a descent algorithm initialized inside such a basin will actually converge to the local
minimum within it. The second reference investigates “randomized” ReLU networks. It provides a
description of the quality and asymptotic distribution of the local minima under the assuption that
the ReLU activations are independent Bernoulli variables. Similar ideas were pursued in Dauphin
et al. (2014) and Kawaguchi (2016). In a different vein, the loss surface of fully connected neural
networks with smooth nonlinearities (i.e, sigmoid or tanh) and `2 loss have also received attention.
The dominant strand of this line of work focuses on a search for situations wherein local minima
and global minima coincide. For example, if the weight matrices and features at a given layer of the
network satisfy certain structural assumptions (e.g. full rank conditions and linear independence)
then such a “local equals global” result holds, c.f. Gori & Tesi (1992); Yu & Chen (1995); Frasconi
et al. (1997); Nguyen & Hein (2017). Deep linear networks, i.e. a deep network with no nonlineari-
ties, represent the extreme case of this line of work. It is shown in Baldi & Hornik (1989); Baldi &
Lu (2012); Kawaguchi (2016) that these networks do not have sub-optimal local minimizers.
2	Piecewise Multilinear S tructure
We begin by describing the precise manner in which ReLU networks give rise to piecewise multilin-
ear forms. This will entail both a precise formulation of the decomposition of parameter space into
cells as well as an explicit description of the multilinear structure of the loss on each cell. We shall
employ the following notation when accomplishing these two tasks. Bold-face Roman and Greek
letters such as x, y, λ, ε denote vectors in standard Euclidean space, with hx, yi := xTy the usual
inner-product and X 0 y := XyT the standard outer-product of vectors. Their light-face Roman
counterparts with sub-scripts xi , yi , λj , εk denote individual entries. Capital Roman letters such as
U, V, W will always refer to matrices while the corresponding lower-case letters uij , vij , wij will
denote the corresponding matrix entries. We reserve Id for the identity matrix, 0 = (0, . . . , 0)t for
the zero vector and 1 = (1, . . . , 1)t for the constant vector. We reserve parenthetical super-scripts
such as x(i) or W(') for enumerating a collection of vectors or matrices, respectively. We view a
collection of N labelled data points as a set of ordered pairs (X(i), y(i)) with the X(i) representing
generic points belonging to Rd and y(i) ∈ RR representing one-hot vectors coding for the class of
the ith data point. All the proofs are presented in the appendix.
Our analysis considers the following multi-class model with hinge loss. Fix a target y and let y
denote the prediction of the network. Then the expression
R
'(y, y) = -1 + X σ(l + y - hy, y))= -1 +( 1 , σ( (Id - 1 0 y)y + 1 )〉	(4)
q=1
furnishes the multi-class hinge loss. We consider a neural network with L hidden layers,
X(Y) = σ(W ⑶X(i，'T) + b⑶)	for ' =1,...,L
y⑴=v χ(i,L) + C
z(i) = σ ((Id - 1 0 y(i)) y(i) + 1 ) ,	(5)
where x(i,') denotes the feature vector of the ith data point at the 'th layer (with the convention that
x(i,O) = X(Z)), y(i) denotes the output of the network for the ith datum and z(i) ∈ RR describes
the loss of data point x(i) associated with each of the R classes. The matrices W(') and vector
b(`) define the affine transformation at layer ` of the network, and V and c denote the weights
and bias of the output layer. We allow for fully-connected as well as structured models, such as
convolutional networks, by imposing the assumption that each W(') is a matrix-valued function that
depends linearly on some set of parameters ω(') ——
W (')(αω(') + βω ⑶)=αW (')(ω(')) + βW (')(ω ⑶)；
4
Under review as a conference paper at ICLR 2018
thus the collection ω = (ω(1),..., ω(L),V, b(1),..., b(L), C) ∈ Ω represent the parameters of the
network and Ω denotes parameter space, i.e. a vector space. We let d` denote the dimension of the
features at layer ` of the network, with the convention that d0 = d (dimension of the input data) and
dL+1 = R (number of classes). We use D = d1 + . . . + dL+1 for the total number of pointwise
nonlinearities and Np := dim(Ω) for the total number of parameters. We then finally arrive at the
expression
1N
L(ω) = N ∑h1, Z(i)i- 1
N i=1
for the total loss over all data points and all classes.
2.1	PARTITIONING Ω INTO CELLS
The lack of differentiability of the nonlinearity σ(x) induces a subsequent lack of differentiability of
L(ω), but we may still characterize differentiable regions of the loss precisely. This characterization
will prove essential for our analysis. Given a data point x(i) let us define the collection of functions
λ(i,')(ω) := σ0(W⑶x(i，'T) + b')	for ' = 1,...,L
ε(i)(ω) := σ' ((Id - 1 ㊈ y⑶)y⑴ + 1 ) ,	(6)
where we make the arbitrary re-definition σ0(0) := 1/2 to handle those points where σ0(x) does not
exist. Thus λ(i,') : Ω → {0,1/2,1}d' and ε(i) : Ω → {0,1/2, 1}r, and by collecting all of these
functions into a single signature function
S(ω) = (λ(1,1)(ω),..., λ(1,L)(ω), ε(1)(ω);....; λ(N,1)(ω),..., λ(N,L)(ω), ε(N)(ω))
We obtain a function S : Ω → {0, 1/2, 1}nd since there are D total nonlinearities and N total
data points. The signature function S describes how each ReLU in the network activates. These
activations take one of three possible states, the fully active state (encoded by a one), the fully
inactive state (encoded by a zero), or an in-between state (encoded by a 1/2). If none of the ND
entries of S (ω) equal 1/2 then all of the ReLUs are differentiable near ω , and so the loss L is
smooth near such points. With this in mind, for a given U ∈ {0, 1}nd we define the cell Ωu as the
(possibly empty) set
Ωu := S-1(u) := {ω ∈ Ω : S(ω) = u}
of parameter space. By choice L is smooth on each non-empty cell Ωu, and so the cells Ωu provide
us with a partition of the parameter space
Ω= (U	Ωu∣ [ N.
u∈{0,1}ND
into smooth and potentially non-smooth regions. The set N contains those ω for which at least one
of the ND entries ofS(ω) takes the value 1/2, which implies that at least one of the nonlinearities
is non-differentiable at such a point. Thus N consists of points at which the loss is potentially non-
differentiable. The following lemma collects the various properties of the cells Ωu and of N that we
will need in the rest of the paper.
Lemma 1. Each cell Ωu for U ∈ {0,1}ND is an open set. If U = U then Ωu and Ωu are disjoint.
The set N is closed and has Lebesgue measure 0.
2.2	Piecewise Multilinear Structure
Let us briefly assume for the sake of exposition that instead of (5) we have a simplified model
without any bias parameters
L(ω ⑴,...,ω(L) ,V ):= -1 + X 1Tσ(T⑴ Vσ(W(L)…σ(W ⑵ σ(W ⑴ X⑺)))+ 1)
i
5
Under review as a conference paper at ICLR 2018
where T(i) =Id - 1 0 y(i). By definition, inside a cell Ωu each nonlinearity σ(x) acts as a linear
function, i.e. matrix multiplication by a diagonal matrix, or mask, containing zeroes or ones in its
diagonal. More precisely, restricted to the cell Ωu the loss takes the form
-1 + 1 E(，)1 I ɪ	ITE(i,U)τ(i)vΛ(i,L,u)W(L)... Λ(i,2,u)W(2)Λ(i,1,u)W(I)x(i)
N乙
i
E(i,u) := diag(ε(i,u)), A(i，'，U) := diag(λ(i,',u));
the "u" super-scripts in ε(i,u), λ(i,',u) indicate the dependence of the masks on the cell. UP to the
constant factor -1 + h1, E(i,U)1i/N that simply counts the average number of errors on the cell,
it is then clear that L∣ωu is a multilinear form of its arguments. As a consequence, the loss is a
piecewise multilinear form up to constants and it is therefore smooth in the interior of each cell. As
non-zero multilinear forms do not have local minima, it is clear that a local minimum of the loss
can only occur (i) in the interior of cells where the loss is constant, or (ii) on the boundary of one or
more cells. Going back to our case of interest (5), the presense of bias parameters complicates the
picture slightly — the loss on a cell is now a sum of multilinear form rather than a single multilinear
form. However, the overall conclusions regarding local minima remain unchanged. The following
theorem describes the precise result.
Theorem 1 (Structure of the loss).
(i)	For each cell Ωu there exist multilinear forms φ0, φU,..., φL, a linear function ΦL+ι and a
constant φUL+2 such that
L∣Ωu (ω(1),. .. ,ω(L), V, b(1),..., b(L), c) = φu(ω(1),ω ⑵,ω ⑶,ω ⑷...，ω(L), V)
+φ1U(b(1), ω(2), ω(3), ω(4) . . . , ω(L), V)
+φ2U(b(2), ω(3), ω(4) .. .,ω(L),V)
+φ3U(b(3), ω(4) .. .,ω(L),V)
+φUL-1(b(L-1), ω(L), V)
+φUL(b(L),V)
+φUL+1(c)
+φUL+2.
The constant φUL+2 counts the average number of errors on the cell.
(ii)	The loss L is smooth on each cell Ωu. Moreover, if ω ∈ Ω \ N and the Hessian matrix
H L(ω) does not vanish then it must have at least one strictly positive and one strictly negative
eigenvalue.
(iii)	Local minima and maxima of L occur only on cell boundaries (i.e. on N) or on those cells Ωu
where the loss is constant. In the latter case, L∣ωu (ω) = φLL+2 for all ω ∈ Ωu.
3 critical point analysis
Recall the hinge loss
'(y,y) := σ (1 - yy)
for binary classification, where y ∈ {+1, -1} denotes the target. For a given set of parameters
ω = (W, v, b, c) the expression
L(W, v, b, C) = N X σ h 1 - y⑴{vτσ(WX⑴ + b) + Coi
(7)
then defines the loss associated to a fully connected network with one hidden layer. Let {wk}1≤k≤K
denote the rows of the linear transformation W defining the hidden layer. A straightforward com-
putation shows that we may specify the multilinear forms in theorem 1 more precisely.
6
Under review as a conference paper at ICLR 2018
Lemma 2 (Decomposition with L = 1). Let
L|Ωu (W, v, b,c) = ΦU(W, v)+ ΦU(b, v) + ΦU(c) + ΦU
denote the loss on a cell Ωu. For 1 ≤ k ≤ K define
N ( X	ε(i,U)λki,U)X(J	X
i:y(i)=1
i:y(i)
ε(i,u)λ(ki,u)x(i)
-1
ɑku) ：= N (X f(i，U)λki,U)- X
i:y(i) =1
i:y(i)
ε
-1
γ(u) := ɪ I X ε(i,u) - X	ε(i,u)
i:y(i) =1	i:y(i)=-1
Then φ3u = δu and φ2u(c) = -γ(u)c, while the relations
δu := ɪ X ε(i,u)
N乙
i
φ0U(W, v) = - Xvkha(kU),wki, and φ1U(b, v) = - Xvkα(kU)bk,
kk
(8)
(9)
(10)
(11)
(12)
furnish the multilinear forms defining the loss on Ωu.
With this description in hand, we may now explore the consequences of this decomposition under
the assumption of linearly separable data. Since the data are linearly separable there exists a unit
vector q ∈ Rd, a bias β ∈ R and a margin μ > 0 SUCh that the family of inequalities
hq, x(i)i + β ≥ μ	if	y(i) =+1	(13)
hq, x(i)i + β ≤-μ	if y(i) = -1	(14)
hold. By combining (9)-(10) with (13)-(14) We easily obtain the following estimate
haku), qi + ɑku)β ≥ μ (N Xε(i,u)λki,u)) ,	(15)
which we may then use to find a decent direction for the loss whenever the right-hand-side of (15)
does not vanish. The idea is simple, i.e. that adding a multiple of ±q to wk and a multiple of ±β to
bk will usually lead to a decrease of the loss. To see this let ek = (0, . . . , 1, . . . , 0)T denote the kth
standard basis vector, Ωu the closure of the cell Ωu and sign(χ) the signum function that vanishes at
zero. The following lemma then makes this idea precise.
Lemma 3. Let ω = (W, v, b,c) ∈ Ω denote any point. Define
W = Sign(Vk) ek 0 q and b = β Sign(Vk) ek.
For t ∈ R let ω(t) := (W + tW, v, b + tb, c) denote a perturbation ofω. Then
(i)	There exists to > 0 and U ∈ {0,1}ND Such that ω(t) ∈ Ωu forall t ∈ [0,to).
(ii)	L(ω) ≥ L(ω(t)) + t|v® | N Pi ε(i,u)λki,u) forall t ∈ [0,to).
We may now state and prove the theorem underyling figure 2 in full generality. If we let `(i) (ω)
denote the contribution of the ith data point x(i) to the total loss, so that L(ω) = NN Pi '(i)(ω),
then we may conclude
Theorem 2.	Let ω = (W, v, b, c) be a local minimum of the loss and assume the data {x(i)}iN=1
are linearly separable. Then
`(i) (ω) > 0	=⇒	Vk σ(hwk, x(i)i + bk) = 0 for all k ∈ {1, . . . , K}.
7
Under review as a conference paper at ICLR 2018
Essentially, this theorem says that local minima obey the property sketched in figure 2. If a data
point x(i) has non-zero loss one of vk or σ(hwk, x(i)i + bk) must vanish for all hidden neurons. We
therefore have a dichotomy. Either x(i) lies in the blind side of the hyperplane hwk, x(i)i + bk or
else vk = 0. In the latter case the kth feature is not used when forming network predictions and so
the corresponding hyperplane is inactive. Succinctly, theorem 2 states that if a data point x(i) is
unsolved it must lie on the blind side of every active hyperplane. Moreover, this result applies
to both critical points as well as to local minimizers. While the proof of theorem 2 only yields the
result for minimizers, it has the benefits of both transparency and directness - We invite the reader
to read the proof of Lemma 3(ii) and theorem 2 in the appendix, which are particularly simple.
Extending the result of theorem 2 to include critical points is less direct, and it requires an invo-
cation of machinery from non-smooth analysis. To begin, We recall that for a Lipschitz but non-
differentiable function f (ω) the Clarke Subdiferential ∂0f (ω) of f at a point ω ∈ Ω provides a
generalization of both the gradient Vf (ω) and the usual subdifferential of a convex function. For
a Lipschitz function f We may employ the folloWing definition (c.f. page 133 of BorWein & LeWis
(2010)).
Definition 1 (Clarke Subdifferential). Suppose that a function f : Ω → R is locally Lipschitz
around ω ∈ Ω, and differentiable on Ω \ M where M is a set of Lebesgue measure zero. Then the
convex hull
∂of (ω) := c.h. lim Vf(ωk) : ωk → ω, ωk ∈/ M
is the Clarke subdifferential off at ω.
With this definition in hand, We can noW state the folloWing stronger version of theorem 2:
Theorem 3.	Let ω = (W, v, b, c) and assume that 0 ∈ ∂oL(ω). Assume also that the data
{x(i)}iN=1
are linearly separable. Then
`(i) (ω) > 0	=⇒	vk σ(hwk, x(i)i + bk) = 0 for all k ∈ {1, . . . , K}.
4	Exact Penalties and Multi- Clas s Structure
Unfortunately, the result of theorem 2 and 3 does not extend naively to the multi-class case. To the
contrary, counter-examples shoW that there exist non-trivial critical points for linearly separable data
Whenever the number of classes exceeds tWo. In other Words, in the presence of three or more classes
a critical point may contain active yet unsolved data points. This begs the question of Whether some
variant of theorem 3 holds in the multi-class context. A first attempt might simply modify the loss
itself. We might hope that substituting the multi-class hinge loss (4) With its one-versus-all variant
RN
欢y, y):= X σ(l+ 犷(-1芦))	L(ω) = N X “y⑴,y⑴)	(16)
r=1	i=1
Would restore the tWo-class structure of critical points. The idea here is to use the binary hinge loss
' in the hope of decoupling a multi-class problem into R two-class problems. Yet similar counter-
examples dash this hope as Well, as non-trivial critical points persist for netWork With modified loss
(16) and one hidden layer. The inherent difficulty comes from the fact that all of the parameters ω in
the network still couple through the joint nonlinear minimization of (16), and so simply modifying
the hinge loss does not restore the two-class structure.
We may, however, introduce a sufficient amount of decoupling if we modify both the loss as well as
the algorithm used in its optimization. Let us begin this process by recalling that
x(i,L) (ω⑴，...，ω(L), b⑴，...，b(L)) and y⑴=Vx(i,L) + C
denote the features and predictions of the network with L hidden layers, respectively. The sub-
collection of parameters
ω := (ω(1),..∙, ω(L), b(1), ..., b(L))
therefore determine a common set of features x(i,L) while the parameters V, c determine R one-
versus-all classifiers utilizing these features. We may write the loss for the rth class as
N
L(r)(ω, Vr, Cr) = N X σ(1 + 泼)(-1浮)
i=1
8
Under review as a conference paper at ICLR 2018
and then sum L(ω) := (L(1) +---+ L(R)) (ω) to recover the total objective. Thus each classifier in
C shares a common set of features, and the joint minimization over features and classifiers couples
the R binary problems together.
We may then seek to minimize C by applying a soft-penalty approach. We introduce the R replicates
ω(r) = (ω(1,r),..., ω(L,r), b(1，r),..., b(L,r))	1 ≤ r ≤ R
of the hidden-layer parameters ω and include a soft '2-penalty
LR
R(ω⑴，…，ω(R)) := ɪ XX kω(',r) -ω⑹k2 + kb(',r) - b⑶k2
-1'=1r=1
to enforce that the replicated parameters ω(',r), b(',r) remain close to their corresponding means
(ω('), b(')) across classes. We then proceed by minimizing the penalized loss
R
E(ω(1),..., ω(R)) := X C(r) (ω(r)) + YR(ω⑴，...，ω(R))	(17)
r=1
for γ > 0 some parameter controlling the strength of the penalty. Remarkably, performing this
process yields
Theorem 4 (Exact Penalty and Recovery of Two-Class Structure). If γ > 0 then the following hold
for (17) —
(i)	The penalty is exact, that is, at any critical point ω(1), . . . , ω(R) of E the equalities
ω(',1)
b(',1)
1R
ω(',R) = ω⑶:=4- X ω(',r)
R
r=1
1R
b(',R) = b(') := 1 X b(',r)
r=1
hold for all 1 ≤ ` ≤ L.
(ii)	At any critical point of E the two-class critical point relations
0 ∈ ∂oC(r)(ω, Vr ,Cr )
hold for all 1 ≤ r ≤ R.
In other words, applying a soft-penalty approach to minimizing the coupled problem L actually
yields an exact penalty method. By (i), at critical points we obtain a common set of features x(i,L)
for each of the R binary classification problems. Moreover, by (ii) these features simultaneously
yield critical points
0 ∈ ∂oC(r)(ω, Vr, Cr)	(18)
for all of these binary classification problems. If (18) holds then clearly the weaker critical point
relation 0 ∈ ∂0C(ω) for the full loss holds as well, and so the penalty approach certainly yields
critical points of the original loss. More importantly, the fact that (18) may fail for critical points of
C is responsible for the presence of non-trivial critical points in the context of a network with one
hidden layer. We may therefore interpret (ii) as saying that the penalty avoids pathological critical
points where 0 ∈ ∂0C(ω) but (18) does not. To be clear, We may say
Corollary 1. Assume the hypotheses of theorem 3, and that the {x(i)} are linearly separable. Let
ω denote any critical point of E and `(i,r) (ω) the loss associated to the ith data point and the rth
class. Then
`(i,r) (ω) > 0	=⇒	(Vr)k σ(hwk, x(i)i + bk) = 0 for all k ∈ {1, . . . , K}.
The corollary follows immediately from (18) and the argument for the two class case. In principle,
the penalty approach also provides a path forward for studying multi-class problems. Regardless of
the number L of hidden layers, an understanding of the family of critical points (18) reduces to a
study of critical points of binary classification problems.
9
Under review as a conference paper at ICLR 2018
References
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, l989.
Pierre Baldi and Zhiqin Lu. Complex-valued autoencoders. Neural Networks, 33:136-147, 2012.
Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and
examples. Second Edition. Springer Science & Business Media, 2010.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192-204, 2015.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
P Frasconi, M Gori, and A Tesi. Successes and failures of backpropagation: A theoretical. Progress
in Neural Networks: Architecture, 5:205, 1997.
Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 14(1):76-86, 1992.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Interna-
tional Conference on Machine Learning, pp. 2603-2612, 2017.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks.
In International Conference on Machine Learning, pp. 774-782, 2016.
Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning.
IEEE Transactions on Neural Networks, 6(5):1300-1303, 1995.
10
Under review as a conference paper at ICLR 2018
Appendix: Proofs of Lemmas and Theorems
Multilinear forms have a saddle like structure
Lemma. The Hessian matrix of a non-trivial multilinear form has at least one strictly positive and
one strictly negative eigenvalue.
Proof. A multilinear form φ : Rd1 × . . . × Rdn → R can always be written as
d1	dn
φ(v1,...,vn) =	...	Aj1,...,jn v1,j1 . . . vn,jn	(19)
j1=1	jn=1
for some tensor {Aj1,...,jn : 1 ≤ jk ≤ dk}. Here vk,j denotes the jth component of the vector vk.
∂2 φ
From (19) it is clear that ^∏φ = 0 and therefore the trace of the Hessian matrix of φ is equal to zero.
vk,j
This implies that the sum of the eigenvalues of the Hessian is equal to zero. So if the Hessian is not
the zero matrix, then it has at least one strictly positive and one strictly negative eigenvalue. □
Proof of Lemma 1
The features x(i,') at each hidden layer depend in a Lipschitz fashion on parameters. Thus each
Ωu defines an open set in parameter space. Moreover, if U = U then Ωu and Ωu are disjoint by
definition. If ω ∈ Ωu for all U ∈ {0,1}ND then at least one of the equalities
bj') = -hwj'), x(i,'-1)i or Cs = -(1 + (vs - Vr, x(i,L)i),	(20)
must hold. The set of parameters N ⊂ Ω where an equality of the form (20) holds corresponds to a
Lipschitz graph in Ω of the bias parameter for that equality. We may therefore conclude that
N ：= Ω \ I U Ωu I
u∈{0,1}ND
defines a set contained in a finite union of Lipschitz graphs. Thus N is (Np - 1)-rectifiable, and
in particular, has Lebesgue measure zero. That N is closed follows from the fact that it is the
complement of an open set.
Proof of Theorem 1
Part (i) follows by carefully expanding
L = -1 + N X 1tσ ( T⑴(Vσ(WLσ(…W?(WιX⑺ + bi) + b2 ...) + 瓦)+ c) + 1)
i
where T(i) := Id-10 y(i). Part (ii) comes from the fact that the trace of the Hessian of a multilinear
form is equal to zero.
To prove part (iii), note part (ii) implies that for any ω ∈ Ωu there exists a small neighborhood
Bε(ω) := {ω ∈ Ω : ∣∣ω — ω∣∣ < ε} ⊂ Ωu
on which L∣ωu (ω) is constant. Thus
L∣Ωu (ω + δω) = L∣Ωu (ω)
must hold for all ω and all δ small enough. Now use part (i) and multilinearity to expand the
left-hand-side into powers of δ:
L
L∣Ωu (ω + δω) = L∣Ωu (ω) + X δk fk(ω) + δL+1(φU + φU) (ω).	(21)
k=1
11
Under review as a conference paper at ICLR 2018
That φ0u + φ1u (ω) is, in fact, the highest-order term is a consequence of the multilinear decompo-
sition from part (i). Since (21) must hold for all δ small enough, all like powers must vanish
fk (ω) = 0 and (φU + ΦU)(ω) = 0.
Now take any ω with b(1) = 0 to conclude
φ0u(ω(1) , ω(2) , ω(3) , ω(4) . . . , ω(L) , V ) = 0
for all ω('),V. BUt then (φU + φU)(ω) = 0 for all ω implies
φ1u(b(1), ω(2), ω(3), ω(4) . . . , ω(L), V)
for all ω('), V, b(1) as well. Thus φ0 + φU is the zero function, and so φU is the highest-order
mUltilinear form in the decomposition from part (i). This implies that
fL(ω) = φ2u(b2,ω(3),...,ω(L),V),
but fL must vanish by (21). Thus φ2u is the zero function as well. Continuing in this way shows that
each φU is the zero function for 0 ≤ ' ≤ L + 1, and so in fact
LlΩu (ω) = φL+2
as claimed.
Proof of Lemma 2
Restricted to a cell Ωu, the loss can be written
L∣Ωu(W, V, b,c) = Nn X σ h -y⑺{vτσ(Wx(i) + b) + Co + l]
i
=ɪ X ε(i,u) h -y(i) {vτΛ(i,u)(WX⑺ + b) + co + l]
i
Expanding parenthesis after parenthesis the above formula leads to:
L∣Ωu(W, v, b,c) = ɪ X ε(i,u) [ -y(i) {vτA(i，U)WX⑺ + vτA(i，U)b + co + l]
i
=ɪ X ε(i,u) h -y(i)vTΛ(i,u)WX⑴一y⑴VTA(i，U)b - y⑴C + l]
i
=ɪ X -ε(i,U)y⑴VTΛ(i,u)WX⑴-ε(i,u)y(i)vTA(i，U)b - ε(i,u)y(i)c + ε(i,u)
i
= φ(0U)(W, v) + φ(1U)(b, v) + φ(2U)(c) + φ(3U)
Letting wk be the kth row of the matrix W, and noting that vT Λ(i,U)W = Pk vkλ(ki,U)wkT we find
that
hwk, x(i)i
wkT	x(i)
ik
- XVk *N Xf(i，U)y(i或，U)x(i), Wk +
-Xvkha(kU),wki
k
12
Under review as a conference paper at ICLR 2018
where the vector a(ku) is defined by
aku) = N X y……)
i
X ε(i,u)λ(ki,u)x(i) - X ε(i,u)λ(ki,u)x(i)
i:y(i) =1	i:y(i) =-1
Similarly we find that
φ1u)(b, V)= - ɪ X ε(i,U)y⑴VTA(i，U)b
i
=- N X ε”(i Xkkλk"k)
ik
=-X V G X WNNlbk
= -	vk αkU bk
k
where
αku) = ɪ ( X ε(i5"- X ε(i,U)λ"[
kN	k	k
i:y(i) =1	i:y(i) =-1
and finally we have
φ1U)(C) = -Y(U)C	where	Y(U) = g ( X ε(i'u) - X ε(i'u)
i:y(i) =1	i:y(i) =-1
Proof of Lemma 3
Proof of (i) ⇒ (ii). We start by using (i) to prove (ii). First note that (8) holds for ω ∈ Ωu due to
the continuity of the loss. By part (i), ω(t) remains in some fixed Ωu for all t small enough. Thus
(9-11) apply. The bilinearity of φ0U and φ1U then yield
L(ω(t)) - L(ω) = tφR(W, v) + tO?(b, v) = -t|vk| (ha" qi+。尸β),
which combined with (15) proves (ii).
Proof of (i). We now prove part (i). While straightforward, the proof is a little longer. Let us denote
by ω(t) = (W + tW, v, b + tb, C) = (W (t), v, b(t), C) the perturbation considered in the lemma.
Without loss of generality, let us choose k = 1. Then the first row of W(t) and the first entry of b(t)
are given by
w1(t) = w1 +tsign(v1)q,	b1(t) = b1 + tsign(v1)β
whereas the other rows and entries remains unchanged,
wk (t) = wk,	and	bk(t) = bk for k ≥ 2.
Define the activations,
Y1(i)(t) = hw1(t),x(i)i + b1(t) = hw1,x(i)i + b1 +tsign(v1) hq,x(i)i +β
Yk(i) = hwk,x(i)i + bk	for k ≥ 2.
13
Under review as a conference paper at ICLR 2018
so that the functions involved in the signature S(ω(t)) can be written as:
λ(1i) (ω(t)) = σ0 (γ1(i) (t))
λ(ki)(ω(t)) = σ0 (γk(i))	fork ≥ 2
(22)
(23)
ε(i) (ω(t)) = σ0
1 - y(i) c+v1σ(γ1(i)(t)) + Xvkσ(γk(i))
Recall that that signature function, for the network considered here, is simply given by the collection
of all the functions λ(i) = (λ(1i), . . . , λ(ki))T and ε(i):
S(ω(t)) = λ(1)(ω(t)), ε(1)(ω(t));...; λ(N)(ω(t)), ε(N)(ω(t))
For the network considered here, since there are K hidden neurons, one output neuron, and N data
points, We have that S : Ω → {0,1/2,1}N(K+1). We now make the following claim, that will be
proven at the end of this section:
Claim. There exists t0 > 0 such that the function t 7→ S (ω (t)) is constant on (0, t0).
Note that the above claim implies that for t ∈ (0, to), ω(t) either remains in a fixed cell Ωu (if none
of the entries of S(ω(t)) are equal to 1/2) or on the boundary of a fixed cell Ωu (if some of the
entries of S(ω(t)) are equal to 1/2). In both cases we have that ω(t) ∈ Ωu for all t ∈ (0, to). Since
ω(t) is continuous and since Ωu is closed, we then clearly have that ω(t) ∈ Ωu for all t ∈ [0, to),
which conclude the proof of Lemma 3(i). We now prove the claim:
Proof of the claim. Let us fix i ∈ {1, . . . , N}. Note that the function t 7→ γ1(i) (t) is monotone and
continuous (it is simply an affine function of t). As a consequence, the quantity appearing inside
σ0[∙] in equation (23), that is,
g(t) = 1 - y(i) c+ v1σ(γ1(i)(t)) + X vkσ(γk(i))
k=2
is also continuous and monotone. Without loss of generality, let assume that g is non-decreasing.
Continuity and monotonicity then implies that there are 3 intervals (-∞, a), [a, b] and (b, ∞) on
which g is strictly negative, equal to zero, then strictly positive (with the understanding that a ≤ b,
and both a and b can take infinite values, in which case we would have less than three intervals).
As a consequence ε(i)(ω(t)) is equal to 0, then 1/2, then 1 on each of these three intervals. Note
that one can always choose τ(i) > 0 small enough so that (0, τ(i)) is fully contained in one of these
three intervals, and this implies that ε(i)(ω(t)) is constant on the interval (0, τ(i)). A similar line of
reasoning shows that there exists τ(i) > 0 such that λf)(ω(t)) is constant on (0, τ(i)). The interval
(0, to ) is then obtained by taking the intersection of all these intervals:
N
(0,to)= ∩(0,τ(i)) ∩ (0,T(i))
□
Proof of Theorem 2
The proof is by contradiction. Suppose '(i)(ω) > 0 andfor some k both Vk=I and σ(hwk, x(i)i +
bk) 6= 0 hold. Consider the perturbation ω(t) of lemma 3. Then there exists u ∈ {0, 1}ND and
to > 0 such that ω(t) ∈ Ωu for t ∈ [0,to). By continuity of ω(t) there exists ω = (W, v, b, C) ∈
Ωu such that '⑻(ω) > 0 and σ(hWk, x(i)〉+ bk) = 0. Thus ε(i,u) = 1 and λki,u) = 1 in Ωu.
As |vk | > 0 lemma 3(ii) implies that the perturbation leads to a strict decrease of the loss, which
contradicts the assumption that ω is a local minimizer.
14
Under review as a conference paper at ICLR 2018
Proof of Theorem 3
Definition 1 and theorem 1 allow us to compute the Clarke subdifferential of L at ω relatively easily.
First recall that the open cells Ωu fill parameter space UP to a set N of measure zero. If ω ∈ N then
ω must lie on the boundary ∂Ωu of some cell. Define the incidence set
I(ω) := {u ∈ {0,1}ND : ω ∈ ∂Ωu}
of such a point ω ∈ N as the collection of all such possible cells. Thus I(ω ) is both non-empty
and finite. If ωk → ω and ωk ∈/ N we may, by passing to a subsequence if necessary, assume
that ωk ∈ Ωu for some U ∈ I(ω) and all k sufficiently large. But then VL(ωk) = vL∣ωu (ωQ,
and since VL∣ωu is a continuous function (i.e. a sum of multilinear gradients), it can be extended
by continuity to the point ω ∈ ∂Ωu and the limit VL(ωk) → VL∣ωu (ω) follows. We therefore
conclude from definition 1 that the Clark subdifferential at ω ∈ N is given by the convex hull
∂oL(ω) = J X θ(U)VL∣Ωu(ω) : θ(U) ≥ 0, X θ(U) = 1 '	(24)
[u∈I(ω)	u
where it has to be understood that VL∣ωu (ω) denotes the extension by continuity of VL∣ωu to the
point ω. If ω ∈ Ωu for some U We let I(ω) = {u} denote its incidence set. As the gradient VL
depends continuously on ω in cells we therefore have VL(ωk) → VL(ω) and so (24) also gives
the Clark subdifferential in this case with θ(U) = 1.
Suppose now that 0 ∈ ∂oL(ω), then we must have that
0 = E θ(u)VL∣Ωu (ω)
U∈I(ω)
(25)
for some collection of positive coefficients θ(U) due to the characterization (24) of the subdifferential.
Using the explicit formula from lemma 2 we can compute the gradients VL∣ωu (ω). In particular,
from equations (12) we find that
dLhu (ω) = -vkakU)	and	"LFu (ω) = -Vkαku)
∂wk	k k	∂bk	k k
Equation (25) then obviously implies:
X θ(U)vka(kU) = 0 and X θ(U)vkα(kU) = 0
U∈I(ω)	U∈I(ω)
for all k . The precise formula for a(kU) and bk provided in lemma 2 then give the equalities
0 = Vk (X X	θ(U)ε(i,U)λki,U)X⑴-X X	θ(u)ε(i,u)λki,u)x(i)
U i:y(i) =1	U i:y(i)=-1
0 = Vk	X X θ(U)ε(i,U)λ(ki,U) - X X	θ(U)ε(i,U)λ(ki,U)
U i:y(i) =1
U i:y(i)=-1
If Vk 6= 0 then we may interchange summations to find
X %(ki)x(i) =	X	%(ki)x(i)
i:y(i) =1	i:y(i)=-1
X %ki) =	X	%ki)	where %ki) := X。(%(，，心旌U)
i:y(i) =1	i:y(i)=-1	U
(26)
(27)
We now claim that equalities (26)-(27) cannot happen unless all the %ki) vanish. To see this, note
that if the %(ki) do not vanish, we can set Q = Pi:y(i)=1 %(ki) = Pi:y(i)=-1%(ki),and dividing both
side of equality (26) by Q to obtain
Σ
i:y(i) =1
(i)
%k
Q
(i)
X ⑴=X %Q-
i:y(i)=-1
(i)
X
15
Under review as a conference paper at ICLR 2018
The above equation shows that a convex combination of data points of class +1 is equal to a con-
vex combination of data points of class -1, which is not possible since the data points are linearly
separable.
Assume now that for some data point x(i) We have '(i) (ω) > 0. Then by continuity of the loss We
also have '(i) > 0 on each neighboring cell Ωu, U ∈ I(ω). Using the definition (6) of ε(i,u) we then
see that ε(i,u) = 1 for all u ∈ I(ω). Ifvk 6= 0 for some k, then the corresponding %(ki) must be equal
to zero, which necessarily implies that λ(ki,u) = 0 some u since the ε(i,u) are all equal to one and at
least one of the θ(u) is nonzero. This in turn implies σ(hwk, x(i)i + bk) = 0 due to the definition of
the λ(ki) .
Proof of Theorem 4
The notion of a cell Ωu for the model (17) consists of sets (Cartesian products) of the form
ωu = ωu(i) X ωu(2) ×∙∙∙× ωU(R),
where each u(r) ∈ {0, 1}ND denotes a signature collection for the individual two-class losses L(r)
and thus
U = (U⑴,...，U(R)) ∈{0,1}NDR
defines a signature collection for the full model. That sets of this form cover the product space Ω ×
…× Ω (R-copies) up to a set of measure zero follows easily from the fact that if (ω(1),..., ω(R)) ∈
Ωu for all U then at least one of the u(r) (say U(I) WLOG) lies in the set
N := ω \ (	[	ωU(I))
u(1) ∈{0,1}N D
which has measure zero in Ω. Thus U must lie in the set
N× Ω ×…×Ω
which has measure zero in the product space Ω ×∙∙∙× Ω, and so the union of the R measure zero
sets of the form
Ω ×…×N ×…×Ω
contains all parameters ω(1), . . . , ω(R) that do not lie in a cell.
Now let ω(1), . . . , ω(R) denote any critical point. For each (`, r) we have
Vω(',r)R = Y3%r) — ωWr)) ωBr) := -ɪ X ω(',s)
R - 1 s6=r
Vb(',r) R = Y(b('，r) — b('，r))	b(',r) :=	X b('，S)
R - 1 s6=r
by straightforward calculation. By definition of a critical point, for each cell Ωu adjacent to the
critical point there exist corresponding constants θ(U) ≥ 0 with PU θ(U) = 1 so that the equalities
0 = X θ(U)VvrChu
U
0 = X θ(U) (Vω(',r)C∣Ωu + Vω(',r) R) = γ(ω(',r) - ω('，r)) + X θ(U)Vω(',r) C|Qu
UU
0 = X θ(U) (Vb(',r)ClΩu + Vb(',r) R) = γ(b(',r) - b ('，r)) + X e(U)Vb(',r”hu	(28)
UU
hold for all 1 ≤ ` ≤ L and 1 ≤ r ≤ R, where the final equalities in the second and third line follow
from the fact that R is smooth and so its gradients do not depend upon the cell. Now on any cell we
may decompose each L(r) into a sum of multilinear forms
L-1
C(r)∣Ωu= 00U，r)3(1，r),...,S(L，r), Vr) + X ¢""5/+5,..3" Vr)
'=1
+。尸 Cb(L，r), Vr) + φL+rι) (Cr) + φL+r)
16
Under review as a conference paper at ICLR 2018
by theorem 1. For any multilinear form φ(v1, . . . , vn) we have
φ(vi,..., Vn) = hvk, Vvk φ(vι,..., Vn)i
for all 1 ≤ k ≤ n by Euler’s theorem for homogeneous functions. Taking the inner-product of (28)
with vr, ω(L,r) and b(L,r) then shows
0 = X θ(u)(φ0u,r) + …+。尸)
u
0 = Xθ3(φ0u,r) + …+ φL-?) + γ(kω(L,r)k2 -hω(L,r),ω(L,r)i)
u
0 = X θ(U) (φLt,r)) + Y (kb(L,r)k2 - hb(L，r), b (Lr)))	(29)
u
which upon adding the second and third equalities yields
kω(L,r)k2 + Ilb(Llr)II2 = hω(L,r),ω(Llr)i + hb(L,r), b(Lb)i
for all 1 ≤ r ≤ R. By the definitions of ω(L,r) and b(L,r) (c.f. lemma 4), this can happen if and
only if
ω(LJ)=…=ω(L,R)	and	b(L,1)=…=b(L,R).
Using this in the second and third equations in (29) then shows that
0 = X θ(U) (φ0u,r) + ∙∙∙ +。⑶)=X θ(U)心"	(30)
uu
for all 1 ≤ r ≤ R as well. Now take the inner-product of (28) with ω(L-1,r) and b(L-1,r) to find
0 = Xθ(U)(φ0" + …+ φLT2)) + γ(kω(LT，r)k2 -hω(LTRω(LT，r)))
U
0 = X θ(U) (φL-7) + γ(kb(LT，r)k2 -hb(LT，r), b(LT，r)i)
U
Adding these equations and using (30) then reveals
ω(LTJ) = ... = ω(LT，R)	and	b(LTJ) = ∙∙∙ = RLfR)
must hold as well, and so also
0=X θ(U)(o0U，r) + ∙∙∙+ΦL-r2)) = X θ(U)(。⑶)
UU
must hold. Continuing from ` = L - 2 to ` = 1 by induction reveals
s('，1)= ... = ω%R) and	b('，1)= ∙∙∙ = b('，R).
for all 1 ≤ ` ≤ L and so the penalty is exact as claimed. Part (ii) then follows from part (i) since the
equalities
ω ('，r) = ω (') = s('，r)	b ('，r) = b(') = b('，r)
for all (`, r) at any critical point. Thus (28) yields
0 = X θ(U)Vvr L(r)∣Ωu(r)
U
0 = X θ(U)Vω(',r) L(r)∣Ωu(r)
U
0 = X θ(U)Vb(',r) L(r)∣Ωu(r)	(31)
U
for all 1 ≤ ` ≤ L, 1 ≤ r ≤ R. Now consider (31) for r = 1. Any cells appearing in the sum
(31) satisfy either (ω, vι,cι) ∈ Ωu(i) or (ω, vι,cι) ∈ ∂Ωu(i). If (ω, vι,cι) ∈ Ωu(i) for some
U(I) then (31) must consist only of gradients on the single cell 0凉1)and so (ω, vι, ci) ∈ Ωu(i)
is a critical point of L(I) in the classical sense. If (ω, vι,cι) ∈ ∂Ωu(i) for some U(I) in the sum
then (ω, vι,cι) ∈ ∂Ωu(i) for all cells U the sum. Thus (31) consists of a positive combination of
gradients of L(I) on cells adjacent to (ω, vι,cι), and so (ω, vι,cι) defines a critical point of L(I) in
the extended Clarke sense. Applying this reasoning for r = 2, . . . , R then yields part (ii) and proves
the theorem.
17
Under review as a conference paper at ICLR 2018
Lemma 4. For any R vectors x(1) , . . . , x(R) ∈ Rd, if
kx(r)k2 = R1^ XhX(S), Xm)	for all r ∈{1,...,R}
s6=r
then xι = •…=xr.
Proof. By relabelling if necessary, assume x(1) has largest norm. Thus kx(1) k ≥ kx(r) k for all
1≤ r ≤ R. If kx(1) k = 0 then there is nothing to prove. Otherwise apply Cauchy-Schwarz and the
hypothesis of the lemma to find
kχ ⑴ k2 ≤ R-IX kχ(s)kkkχ ⑴ k
R - 1 s6=1
kχ(1)k≤ R-IX kχ(s)k.
R - 1 s6=1
The latter inequality implies IlX(Dk = •… =IlX(R)Il since χ(1) has largest norm. Thus
IlX ⑴ k2 = R-i X cos θrkχ ⑴ k2
s6=1
1= R-1 Ecos θr
s6=1
by the hypothesis of the lemma. The latter equality implies cos θr = 1 for all r, and so the lemma is
proved.
□
18