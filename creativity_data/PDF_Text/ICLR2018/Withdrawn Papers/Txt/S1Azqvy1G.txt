Under review as a conference paper at ICLR 2018
Pseudo sequence based deep neural network compression
Anonymous authors
Paper under double-blind review
Abstract
Along with the performance increase of the neural network, both the num-
ber of layers and the number of parameters in each layer are becoming larger
and larger. Therefore, there are more and more works trying to compress
the neural network efficiently while keeping the performance. However, all
of them have not taken the similarity among the kernels into considera-
tion. In this paper, we try to organize the kernels in different channels into
a frame and encode the frames using block-based video coding methods.
First, we try to reshape the weights in different channels into a pseudo se-
quence. Second, after obtaining all the frames in the videos, we will convert
the weight into the PCA domain to obtain a more compact representation.
Then both the intra prediction and the inter prediction will be performed
in the PCA domain to achieve better performance. Finally, the uniform
quantization and entropy coding will be used to encode the residue blocks.
The experimental results show that we can achieve 58 times compression
for the classical VGG-16 model. Not only with very high compression ra-
tio, the proposed method can also provide the benefits of getting a better
balance between the bits per weight and the error in smaller granularity by
adjusting the quantization parameters.
1 Introduction
Deep neural networks have found countless applications in both computer vision and im-
age processing tasks such as image classification (Krizhevsky et al. (2012); Szegedy et al.
(2015)), object detection (He et al. (2016)), image super resolution (Lim et al. (2017); Li
et al. (2017)), and image compression artifacts removal (Dong et al. (2015); Dai et al.
(2017)). However, along with the wide use and performance increase of the neural network,
both the number of layers and the number of parameters in each layer are becoming larger
and larger. It will be difficult for us to apply the deep neural networks to the light-weight
platforms.
Therefore, there are more and more works trying to compress the neural network efficiently
while keeping the performance. For example, Han et al. Han et al. (2015) proposed to prune
the small-weight connections with no loss of accuracy. Then quantization and Huffman
coding are used to reduce the number of bits required to represent each weight. This
work is quite simple yet very effective. Besides, Zhang et al. Zhang et al. (2016) and Yu
et al. Yu et al. (2017) introduced the low rank and sparse decomposition to effectively
reduce the number of parameters and accelerate the neural network. Also, Matthieu et
al. Courbariaux et al. (2015) and Yang et al. Yang et al. (2017) proposed to apply the
so-called binary neural network to store the weight in their binarized format to achieve less
storage and acceleration. However, all these works have not tried to take the mature video
and image coding algorithms into account. The image and video coding standards have
demonstrated its effectiveness in the compression society.
Therefore, in this paper, we try to organize all the weights into a pseudo sequence to
make use of the technologies in modern video and image coding standards. Fig. 1 gives
the basic coding process of the proposed algorithm. We will first reshape the weights of
different layers into a pseudo sequence. After obtaining all the frames in the videos, we
will convert the weight into the principal component analysis (PCA) domain to obtain a
more compact representation. Then both the intra prediction and the inter prediction will
1
Under review as a conference paper at ICLR 2018
Reshape the weights
into pseudo videos
Original
weights
φ
Transform and
prediction
Pseudo
video
Residue
Quantization and
Entropy Coding
blocks
φ
Compressed
weights
Figure 1: The basic framework of proposed pseudo sequence based deep neural network
compression
be performed in the PCA domain to achieve better performance. Finally, the uniform
quantization and entropy coding will be used to encode the residue blocks. As far as we can
see, the main insight of this paper is that it is the first work to involve the video coding based
technologies such as motion estimation and motion compensation into the neural network
compression framework. Also, quite high compression efficiency is achieved through the
proposed algorithms.
This paper is organized as follows. In Section 2, we will give a detailed introduction of the
three parts of the proposed algorithms. The detailed experimental results and discussions
on the compression efficiency will be shown in Section 3. Section 5 concludes the whole
paper and suggests some future works.
2 Proposed algorithms
2.1	Reshape the weights into a pseudo sequence
Under this step, we try to reshape the weights into video frames to beneficial the following
coding process as shown in Fig. 2. According to the dimensions of the weights of different
layers, we divide the weights of different layers into two kinds. The first kind can be naturally
divided into various frames with different block sizes such as the FC6 layer and convolution
layers of the VGG-16 network Simonyan & Zisserman (2014) as shown in Fig. 2 (a). The
kernel size will be directly converted into the size of the block, the channel size can be
directly converted into the number of blocks in one frame, and the number of outputs will
be converted to the number of frames. Take the FC6 layer of VGG-16 as an example, the
kernel size is 7, the number of channels is 512, and the number of outputs is 4096. Therefore,
there are 7 × 7 × 512 × 4096 weights in the FC6 layer. It can be naturally considered as a
pseudo sequence with 4096 frames, each frame with 512 blocks, and the size of each block
is 7 × 7. Note that the number of blocks, the block size, and the number of frames can be
adjusted to achieve even better performance.
The second kind cannot be divided into various frames directly since the kernel size is 1 such
as the FC7 layer and FC8 layer of the VGG-16 network. Take the FC7 layer of VGG-16 as
an example, the kernel size is 1, the number of channels is 4096, and the number of outputs
is 4096. Since we cannot set the block size as 1 × 1, we need to divide the number of channels
into the block size and the number of blocks as shown in Fig. 2 (b). For example, the block
size can be 8 × 8 and the number of blocks can be 64. Under such a case, it will be more
natural to adjust the number of blocks, the block size, and the number of frames to achieve
even better performance.
Using the above mentioned methods, all the layers can be organized into a number of frames
with some blocks. It should also be mentioned that since the weights in both the convolution
layers and full connection layers are very small, we multiply all the weights by 4096 × 16 to
make it beneficial to the following prediction and quantization processes.
2
Under review as a conference paper at ICLR 2018
φ
(a)
(b)
0.5
0.4
Figure 2: The illustration of reshaping the weights into video frames
%iPaUW
0.3
123456789
NO. of dimension preserved
(a) Conv1_1
9°s8°s7°7
%∕Apauw
0.65
0.6
0.55
05 '
1	2	3	4	5	6	7
NO. of dimension preserved
(b) Conv4_1
5	10	15	20	25	30	35	40	45	50
NO. of dimension preserved
(c) FC6
Figure 3: The energy compaction of different layers after PCA
0.2
0.1
0
0	10	20	30	40	50	60	70
NO. of dimension preserved
(d) FC7
2.2	Intra and inter prediction
Before performing the intra and inter prediction, for a specified layer, we first derive a PCA
from all the blocks and apply PCA to all the blocks to obtain a more compact representation
for each block. All the blocks are converted into vectors to calculate the PCA. For example,
for a 7 × 7 block, the vector size is 1 × 49 and will be derived a PCA with 49 × 49 dimensions.
The PCA is calculated by all the blocks in the current layer and will be transmitted to the
decoder for reconstruction. In our scheme, the PCA is transmitted using 32 bits float
numbers to guarantee the precision of the PCA. Fig. 3 shows energy compaction extent of
some layers of the VGG-16 after the PCA. From Fig. 3, we can obviously see the extents
of the energy compaction of various layers are totally different. The extent of the energy
compaction will finally determine the compression ratio of a specified layer. The more
compact layers will be easier to compress while the less compact layers will be harder to
compress. Then in the following, we will do the prediction in the PCA domain.
For the first frame, since there are no previous coded frames, we can only use the intra
prediction mode. According to our observations, the spatial correlations among various
blocks inside one frame are quite low. No matter using the blocks in the current frame or
the neighboring pixels, there will always be no obvious gains through the intra prediction.
So we choose to directly encode the coefficients after PCA under the intra prediction mode.
3
Under review as a conference paper at ICLR 2018
Current block
Figure 4: The illustration of the block based motion estimation
For the other frames, besides the intra prediction mode, an inter prediction mode is also
performed by searching all the previously coded blocks in the previous frames as shown in
Fig. 4. Such a process is very similar to the block-based motion estimation in the video
coding standard. However, since the prediction is performed in the PCA domain, we will
obtain the best matching block through the motion estimation process block by block instead
of the pixel. As can be obviously seen from Fig. 4, an index is needed to be transmitted to
the decoder to indicate which block will be used to as best match. For example, if the total
number of blocks in the previous frame is 16, 4 bits will be used to signal which block will
be used. It should be emphasized that the larger the block size, the less number of bits will
be spent on the number of bits to signal the block index while the prediction will be less
accurate. The best performance can be obtained by achieving the optimal trade-off between
the residue bits and block index bits. In the final, the difference of the current block and
its best match will be coded under the inter prediction mode.
As mentioned above, in all the frames except the first frame, we can choose to use intra
prediction mode or inter prediction mode. In essence, we need to use the rate distortion op-
timization by considering both the distortion and the block index bits to determine whether
we should choose the intra prediction mode or inter prediction mode. However, in our case,
we choose to only use the energy of the current blocks before inter prediction and after inter
prediction as the criterion. If the energy of one block becomes less after inter prediction,
we will choose inter prediction mode and vice versa. The energy is derived by calculating
the sum of the square of all the coefficients.
2.3 Quantization and entropy coding
After obtaining the intra and inter residue blocks in the PCA domain, the uniform quan-
tization will be used to quantize all the coefficients. Since we have already enlarged the
coefficients in advance, here we can use relatively larger quantization parameters such as
256, 512, and 1024 while keeping the performance.
For seeking more efficient compression performance, we take advantage of PAQ (Knoll &
d. Freitas (2012)), a compression scheme with machine learning perspectives for further
compression of the residue blocks after quantization. Since the compression efficiency of the
PAQ will be higher for the larger file sizes, we organize the residue blocks of all the frames
for a specified layer into one file to compress the residue blocks more efficiently.
3	Experimental results
We try to use the proposed algorithms to compress two networks efficiently both on imageNet
data-sets (Deng et al. (2009)): AlexNet (Krizhevsky et al. (2012)) and VGG-16. To save
the verification time, we randomly choose 5000 samples from imageNet data-sets to verify
the performance of the proposed algorithm. The overall network parameters and accuracy
before and after compression can be seen from Table 1. In Table 1, the conv QP and FC
QP mean the quantization parameter used for the convolution layer and full connection
layer, respectively. The conv QP and FC QP equal to 1 means that the network has not
4
Under review as a conference paper at ICLR 2018
Table 1: The overall network parameters and accuracy before and after compression
	Conv QP	FC QP	Top1-Error	Top5-Error	BPW	Compression ratio
AlexNet	Γ^	1	0.4336	0.2062	32^	一
	256	256	0.4402	0.2044	1.48	21.7
	256	512	0.4394	0.2116	0.95	33.5
	512	512	0.4426	0.2094	0.93	34.3
	512	1024	0.4492	0.2196	0.51	62.0
VGG-16	Γ^	1	0.3442	0.1342	32^	一
	256	256	0.3500	0.1320	0.89	36.2
	256	512	0.3518	0.1356	0.55	58.3
	512	512	0.3630	0.1452	0.49	64.9
	512	1024	0.3788	0.1566	0.31	101.7
Table 2: The compression ratio per layer of the AlexNet
	The proposed algorithm			The state-of-the-art algorithm	
Layer	QP	BPW	Compression ratio	BPW	Compression ratio
Conv1	"^56"	2.24	nɪ	6.57^^	4.9
Conv2	256	2.41	13.3	3.02	10.6
Conv3	256	2.80	11.4	2.70	11.9
Conv4	256	2.94	10.9	2.92	11.0
Conv5	256	2.93	10.9	3.02	10.6
FC6	""512-	0.77	413-	0.76^^	41.8
FC7	512	0.98	32.6	0.79	40.7
FC8	512	1.44	22.2	1.87	17.1
Avg	—	0.95	33.5	0.92	34.7
been compressed. The BPW in the table means the bits per weight, which is 32 for the
uncompressed network. We tried different combinations of the quantization parameters for
the convolution layers and the full connection layers to obtain a better trade-off to the
overall performance.
From Table 1, we can see that the proposed algorithms can achieve quite significant com-
pression ratios within very small Top-1 error and Top-5 error increase compared with the
original network without compression. For the AlexNet, when both the convolution lay-
ers and the full connection layers are using the quantization parameters less or equal to
512, both the Top-1 error and the Top-5 error only increase a little. The VGG-16 network
is more sensitive to the quantization parameters compared with the AlexNet. When the
quantization parameter of the convolution layers reaches 512, there will be some obvious
performance losses. From the table, we can also observe one of the advantages of the pro-
posed algorithm is that we can have more precise granularity to adjust the balance between
the errors and bits per weight by adjusting the quantization parameters.
3.1	Comparison of the proposed method with the state-of-the-art method
Besides the results of showing the overall compression efficiency with different quantization
parameters. We also derive one representative quantization parameter (Conv QP 256 and
FC QP 512) to show the compression efficiency of each layer and also to compare with
the state-of-the-art network compression algorithm (Han et al. (2015)). The experimental
results of the compression ratio per layer of the AlexNet and the VGG-16 network are shown
in Table 2 and Table 3, respectively. The proposed algorithm achieves very similar Top-1
and Top-5 errors compared with the state-of-the-art algorithm. For the compression ratio,
the proposed algorithm achieves similar or slightly worse compression ratio for the AlexNet.
While for the VGG-16, the proposed algorithm achieves better compression ratio.
Although the overall compression ratios of both the AlexNet and the VGG-16 are similar,
the compression ratios of each specified layer for each layer are totally different. For both the
5
Under review as a conference paper at ICLR 2018
Table 3: The compression ratio per layer of the VGG-16					
	The proposed algorithm			The state-of-the-art algorithm	
Layer	QP	BPW	Compression ratio	BPW	Compression ratio
Conv1_1	^256	3.75	85^	9.59^^	3.3
Conv1_2	256	2.99	10.7	2.24	14.3
Conv2_1	256	2.95	10.9	2.85	11.2
Conv2_2	256	2.80	11.4	2.98	10.7
Conv3_1	256	2.54	12.6	3.57	9.0
Conv3_2	256	2.34	13.7	1.81	17.6
Conv3_3	256	2.38	13.5	2.87	11.2
Conv4_1	256	2.19	14.6	2.33	13.7
Conv4_2	256	2.03	15.8	1.90	16.9
Conv4_3	256	2.10	15.2	2.39	13.4
Conv5_1	256	2.18	14.7	2.56	12.5
Conv5_2	256	2.05	15.6	2.09	15.3
Conv5_3	256	1.97	16.3	2.56	12.5
FC6	-512	0.27	∏96^	0.35^^	90.9
FC7	512	1.73	44.0	0.40	80.0
FC8	512	1.23	26.1	1.68	19.1
Avg	一	0.55	58.3	0.66	48.8
AlexNet and VGG-16, we can observe a quite different compression ratio for the first layer
of the whole network. The proposed algorithm compresses the first convolution layer with
much higher compression ratio compared with the state-of-the-art algorithm. According
to our common sense, compressing the first layer of the whole network with a quite large
quantization parameter may lead to bad performance for the whole network. This can
partially reveal that different layers are with different importance for the whole network
and thus should be applied to a smaller quantization parameter. Right now, we are just
using different quantization parameters for the convolution layers and full connection layers.
We can anticipate that the proposed algorithm will bring better compression ratio without
influencing the errors if we can adjust the quantization parameters for each specified layer.
Also, for the FC7 layer of the VGG-16, we can see that the proposed algorithm achieves
quite less compression ratio compared with that of the FC6 layer and also the FC7 layer
of the previous algorithm. This phenomenon can be explained by Fig. 3. Through the
comparison between FC6 and FC7, we can observe that the energy is much more compact
for FC6 compared with FC7. The more compact the energy, the easier the high compression
ratio will be achieved through the quantization process. You may also find out that the
compression ratio of the convolution layers is even incomparable with the FC7. The reason
is that the convolution layers is coded with smaller quantization parameters and with higher
peak signal to noise ratio (PSNR).
4	Discussions
To better analyze the compression ratios of different layers, we also present the PSNR of
each layer of the VGG-16 when coded using different quantization parameters as shown in
Table 4. From Table 4, we can first obviously see that the PSNR of the full connection layer
is lower than that of the convolution layer. Also, for the convolution layers, we can see an
obvious trend of PSNR decrease from high to low. This is partially due to the reason that
the values of the previous layer are generally larger than the following layers. Especially
the difference between the values of the convolution layers and full connection layers is
quite large. Therefore, there will be obvious PSNR differences for the convolution layers
and full connection layers. It should be also noted that the convolution layers coded with
higher PSNR are actually achieving a suitable bit allocation for the whole scheme since the
precision of the convolution layers can propagate to more layers and have a large influence
on the final classification accuracy.
6
Under review as a conference paper at ICLR 2018
Table 4: The quantization parameter and PSNR relationship of different layers
Layer	QP	PSNR(dB)	QP	PSNR(dB)	QP	PSNR(dB)
Conv1_1	256	46.24	-5T2^	40.20	1024	34.51
Conv1_2	256	40.55	512	34.84	1024	29.36
Conv2_1	256	36.80	512	30.94	1024	25.32
Conv2_2	256	37.21	512	31.42	1024	25.97
Conv3_1	256	34.20	512	28.43	1024	23.13
Conv3_2	256	32.90	512	27.31	1024	22.21
Conv3_3	256	33.64	512	28.01	1024	22.93
Conv4_1	256	30.63	512	25.11	1024	20.24
Conv4_2	256	29.37	512	24.04	1024	19.39
Conv4_3	256	30.05	512	24.61	1024	19.91
Conv5_1	256	30.45	512	24.95	1024	20.28
Conv5_2	256	30.24	512	24.95	1024	20.28
Conv5_3	256	30.30	512	25.08	1024	20.73
FC6^	256	18.37	""512"	15.09	1024	13.42
FC7	256	20.00	512	14.01	1024	9.27
FC8	256	25.43	512	19.41	1024	13.42
Table 5: The performance when compressing FC6 of VGG-16 individually
	FC6 QP	Top1-Error	Top5-Error	BPW	Compression ratio
VGG-16	T	0.3442	0.1342	3∑^	一
	256	0.3470	0.1338	0.61	52.4
	512	0.3452	0.1348	0.27	119.6
	1024	0.3488	0.1348	0.13	250.8
	2048	0.3814	0.1514	0.06	581.8
	4096	0.9044	0.7974	0.03	1167.9
As we have explained above, all the layers are not independent, the output of the current
layer will be the input of the next layer. Therefore, compressing one layer is totally different
from compressing all the layers. In Table 5, we also show the case when we only compress
the FC6 layer of the VGG-16. We can obviously see that if we do not compress the other
layers, we can compress the FC6 with 250 times or even 500 times without influencing the
overall classification accuracy. It obviously demonstrates that the accuracy of one layer will
have influence on the compression of the other layers. This can also be extended to the
combination of some layers. If we keep some of the layers uncompressed, we can compress
the other layers with higher compression ratio while maintaining the overall performance.
For example, if we compress the layers with more weights while keeping the layers with
fewer weights unchanged, it may lead to better compression efficiency. This is actually an
extreme case of the bit allocation, in which some of the layers are losslessly coded.
5	Conclusion
In this paper, to improve the compression efficiency of the deep neural networks, we try to
organize the kernels in different channels into a frame and encode the frames using block-
based video coding methods. First, we try to reshape the weights in different channels into
a pseudo sequence. Second, after obtaining all the frames in the videos, we will convert the
weight into the PCA domain and perform intra and inter prediction. Finally, the uniform
quantization and entropy coding will be used to encode the residue blocks. After the above
steps, we can compress the classical VGG-16 model by 58 times, which is better than the
state-of-the-art method. In the future, we will try to further improve the compression
performance by using more flexible encoding structure, involving more previously coded
blocks for a better prediction, and assigning optimal quantization parameter to each layer.
7
Under review as a conference paper at ICLR 2018
References
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training
deep neural networks with binary weights during propagations. In Advances in Neural
Information Processing Systems, pp. 3123-3131, 2015.
Yuanying Dai, Dong Liu, and Feng Wu. A convolutional neural network approach for post-
processing in hevc intra coding. In Multimedia Modeling, 2017.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale
Hierarchical Image Database. In CVPR09, 2009.
Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. Compression artifacts
reduction by a deep convolutional network. In The IEEE International Conference on
Computer Vision (ICCV), December 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neu-
ral networks with pruning, trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for
image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
B.	Knoll and N. d. Freitas. A machine learning perspective on predictive coding with paq8. In
2012 Data Compression Conference, pp. 377-386, April 2012. doi: 10.1109/DCC.2012.44.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with
deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou,
and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems
25, pp. 1097-1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.
Yue Li, Dong Liu, Houqiang Li, Li Li, Feng Wu, Hong Zhang, and Haitao Yang. Convolu-
tional neural network-based block up-sampling for intra frame coding. IEEE Transactions
on Circuits and Systems for Video Technology, 2017.
Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced
deep residual networks for single image super-resolution. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) Workshops, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556, 2014.
C.	Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Van-
houcke, and A. Rabinovich. Going deeper with convolutions. In 2015 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR), pp. 1-9, June 2015. doi:
10.1109/CVPR.2015.7298594.
Haojin Yang, Martin Fritzsche, Christian Bartz, and Christoph Meinel. Bmxnet: An
open-source binary neural network implementation based on mxnet. arXiv preprint
arXiv:1705.09864, 2017.
Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by
low rank and sparse decomposition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 7370-7379, 2017.
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convo-
lutional networks for classification and detection. IEEE transactions on pattern analysis
and machine intelligence, 38(10):1943-1955, 2016.
8