Under review as a conference paper at ICLR 2018
Continuous Propagation:
Layer-Parallel Training
Anonymous authors
Paper under double-blind review
Ab stract
Continuous propagation is a parallel technique for training deep neural networks
with batch size one at full utilization of a multiprocessor system. It enables spa-
tially distributed computations on emerging deep learning hardware accelerators
that do not impose programming limitations of contemporary GPUs. The algo-
rithm achieves model parallelism along the depth of a deep network. The method
is based on the continuous representation of the optimization process and enables
sustained gradient generation during all phases of computation. We demonstrate
that in addition to its increased concurrency, continuous propagation improves the
convergence rate of state of the art methods while matching their accuracy.
1 Introduction
Stochastic gradient descent (SGD) with back-propagation has become a ubiquitous algorithm for
training deep neural networks. Learning via gradient descent is inherently sequential because each
update in parameter space requires a gradient measurement at the new point in parameter space.
(1)
Θt-1
One technique for parallelization is gradient averaging over a mini-batch but it does nothing to speed
up the rate of sequential parameter updates. As mini-batch size increases, gradient noise is reduced.
Beyond a point this causes poor generalization (Sec. 2 and Sec. 3.1).
While deep neural networks have many layers, it is not possible to process them in parallel because
information passes sequentially through the layers of a network.
To overcome these limitations we propose continuous propagation—an approach that allows param-
eters in all layers to update simultaneously while samples propagate through network layers.
Information flows and concurrency in training algorithms can be visualized with pipeline diagrams.
Figure 1(a) shows that gradient-descent algorithms require a full forward and backward pass through
the network to compute a gradient estimate. Use of differential equations replaces sequential depen-
dency with a continuous model that has sustained gradient generation, where all layers compute
gradients at all times rather than having modal forward and backward phases (Fig. 1(c), Sec. 4).
The main advantage of this approach is that it enables layer parallelism by allowing each layer to
learn concurrently with others without explicit synchronization. As a result, parallelization along
the depth of a network allows more computing resources to be applied to training. This computa-
tional framework differs from a GPU-optimized implementation, where computations, even though
performed in parallel, happen sequentially in terms of layer utilization.
While the minibatch approach waits for a group of gradient measurements to be completed, in
continuous propagation the gradient estimate is used as soon as it becomes available leading to
statistically superior behavior. This incremental contribution of each observation is matched by
a continuous representation of the problem that allows us to formalize this approach and enables
convergence analysis.
The theoretical foundation of this technique relies on a continuous differential representation of the
learning process (Sec. 3.2). Itis based on the observation that the time iteration equation of gradient-
1
Under review as a conference paper at ICLR 2018
(-eΛJ①：IU一 ①aepdn) ∩
(d)
l-ay-e-r -→
X1 ..	'∖ h0
(c)
xι -,
X0 ∖ h0(Θo)
∖hD
h0〉
4
×.h0 Zi
00,.'
(a) SGD requires a full forward and backward pass
before updating parameters. (b) MBGD processes
many inputs with the same weights and has coor-
dinated quiet times to synchronize parameter up-
dates. (c) CPGD maintains a network in flux. Hid-
den representations and deltas enter every layer at
every time step, and parameters update at every time
step. This is a coordinated synchronous operation.
(d) Reverse checkpoint reduces memory (seen as re-
duced area covered by vertical lines passing saved
hidden representations forward in time) and reduces
time disparity between calculated hidden represen-
tations and their corresponding deltas.
Figure 1: Pipeline diagrams for forward and backward propagation.
descent learning algorithm (1) can be viewed as a numerical integration-approximation method of
the differential system
.
Θ
∂ L
∂Θ .
(2)
Theoretical results include a formal convergence proof of the method (sec. 4.1, Appendix).
Experiments with SVHN, CIFAR-10, and CIFAR-100 image classification sets show that in addition
to increased concurrency compared to state of the art algorithms, continuous propagation offers
comparable accuracy and improved convergence rate expressed in epochs of training (Sec. 5).
2	Related Work
Optimization in the presence of stochastic variables was introduced in the preeminent work by Rob-
bins & Monro (1951). Robbins’ stochastic approximation finds roots of the expected value of an
unknown stochastic function. Robbins identified efficiency as the convergence rate of the sequence
of stochastic observations.
Stochastic gradient descent (SGD) extends the original gradient-descent (GD) method with a process
of stochastic observations of a Lyapunov (cost) function and its gradient. A principal advantage of
SGD is that the effort used to obtain a gradient is fixed and independent of the size of the input
domain. This independence also allows extension to infinite training sets. In exchange, notions of
convergence also have to be made probabilistic (Bottou, 1998).
Mini-batch gradient descent (MBGD) was first introduced as a hybrid approach between SGD and
GD M0ller (1993) in order to enjoy both the speed advantages of SGD and the convergence guar-
antees of GD. More recently, mini-batches are important in amortizing the cost of context switching
in GPU-accelerated computation. When the state associated with model parameters (weights, gra-
2
Under review as a conference paper at ICLR 2018
dients, momentum, etc.) are too large to fit in cache memory, time efficiency is gained by reusing
the parameters that can fit in cache across a mini-batch of activations.
Asynchronous SGD (ASGD) is a parallel multi-GPU algorithm for mini-batch learning (Zhang et al.,
2013; Dean et al., 2012). Its primary gain is in operational efficiency in dealing with a cluster
of machines. ASGD eliminates cluster synchronization at the end of every gradient computation,
which accommodates machine faults, but also causes some parameter differences between worker
nodes. Synthetic Gradients (Czarnecki et al., 2017) is another approach to train a deep network
asynchronously by modeling the error gradients at each layer independently.
Thematic in both MBGD and ASGD is the recovery of otherwise idle computational resources
to perform additional gradient measurements without a parameter update. While the problem of
recruiting many machines to assist in gradient computation is solved by relying on increasingly
large batches in the SGD algorithm, it is known that mini-batch training is an inefficient use of this
computing power Wilson & Martinez (2003), Keskar et al. (2016).
Recent research has explored fundamental changes to GD. Difference target propagation (Lee et al.,
2015) eliminates back-propagation entirely by learning bidirectional inverse mappings between ad-
jacent layers. Feedback alignment (Lillicrap et al., 2014), in contrast, uses fixed random matricies
for the backprop phase. It depends on feedback to maintain parameters as approximate pseudo-
inverses of the fixed random matricies.
Current research is not in computing more-accurate gradients but in being able to scale to larger
models. For example, Shazeer et al. (2017) set a goal of training a network of one trillion parameters.
Such large models make it even more important to develop efficient parallelization methods.
3	Theory
3.1	Advantages of Model Parallelism
Parallelizing a computation involves spreading its evaluation over separate computational units op-
erating simultaneously. In consideration of deep neural networks, there are two principal ways this
can be imagined. In a model-parallel regime, separate workers simultaneously evaluate the same
network input using distinct model parameters. Conversely, in a data-parallel regime, separate
workers simultaneously evaluate distinct network inputs using the same formal model parameters.
Current scaling efforts use fine-grain model parallelism in block matrix-vector multiplication and
coarse-grain data parallelism across layers and among workers in a cluster.
Parameter values used in parallel cannot have sequential dependencies. Therefore, coordination
is necessary among workers responsible for the same parameters. Strict synchronization ensures
identical values of corresponding parameters; loose synchronization allows some discrepancy.
While there are distinct ways to implement data parallelism, they all share the attribute that multiple
gradients are evaluated at points independent of each other’s outcomes. That is, the point of eval-
uation of the gradient is not able to benefit from learning based on the gradients that are evaluated
in parallel. Since mini-batches capture this attribute well, we use increasing mini-batch sizes as a
proxy for all forms of increasing data parallelism.1
In addition to discovering sharp minima in parameter space that lead to bad generalization, increas-
ing mini-batch size is ineffective for scaling for three other reasons.
First, given that all gradient measurements provide independent estimates of the true gradient,
∂L	∂L
^ξττ = E +N(0, σ) ;	(3)
dθ X〜X	χ~χ LdθJ
having n samples increases the accuracy of our gradient estimate to
1	G ∂L	Γ∂Ll i/ L、	/八
-ΣS 所S	= E "əʒ +N(0, σ/√n) .	(4)
n 匕 dθ Xi〜X	X~X Ldθ
1 Specially designed schemes may be able to exploit additional information from data parallelism, such as
gradient variance or cost curvature. We specifically preclude second-order methods from consideration.
3
Under review as a conference paper at ICLR 2018
Figure 2: Feed-forward neural network
Table 1: Equations for deep network nodes
Node Type		Function	
A	Forward	h(l)	=Θ⑷ h(IT)
A|	Backward	δ(l)	=Θ(I)I δ(1+1)
Θ	Update	Θ(l)	= Θ0 - R0t h(l-1) × δ(l)dt
Φ	Activation	h(l)	= φ(h(l-1))
Φ0	Tangent	δ(l)	= φ0(h(l-1))	δ(l+1)
X	Input*	x=	Xbt/aC
Y	Label	y(x)	
L	Loss	L(h(D),y)		
* {Xk 〜 X} is a sequence of random observa-
tions of X. x(t) is piecewise constant.
For the same computational effort We could have taken n steps, each with a step size equal to
(ɑ∕√n) ∂L. Since the batch step size stayed within a neighborhood of Θ well approximated by
its first-order Taylor expansion, each of the n steps of the SGD algorithm stays within this same
neighborhood. However, we have now proceeded through a total distance of (αn∕√n)∂∂L, so we
are more efficient by approximately √n (Goodfellow et al., 2016, section 8.1.3).
Second, our objective function L is nonlinear. Given this, the accuracy of the first-order gradient
provides limited utility, owing to the high curvature of the loss surface. Beyond this, the ability
to use faster learning rate α by employing larger batch size is fruitless because it is bound to be
inaccurate, and an update in parameter space is necessary to assess a gradient at a new location.
Finally, much of the computational efficiency of SGD comes from the noise realized by replacing
Ex〜X with a sample estimate. Computing a larger sample estimate undoes this efficiency. The
sampling noise causes model regularization. Beyond a certain point, the gain in accuracy exceeds
the desired regularization, and synthetic noise must be added back to the computation to achieve the
desired results.
3.2	Formulation as Differential Equations
Deep neural networks are high-dimensional parameterized functions f(x; Θ), which are often ex-
pressed as directed acyclic graphs. The back-propagation algorithm, however, is best expressed by
a cyclic graph (Fig. 2).
The cycle in the graph is a feedback iteration: the gradients produced by the first full network
evaluation change the weights used in the next iteration. This is because the iterative algorithms are
discrete approximations of a continuous differential system:
Θ=-E
x〜X
∂ L(f (x; Θ), y(x))
∂Θ
+ G(σ(t)),
(5)
.
where G is an unbiased continuous-noise process, with time-varying statistics σ. G provides regu-
larization to allow the continuous system to model phenomena observed in discrete-time learning
systems. In the discrete case, regularization is provided by the sampling procedure (SGD), by the
learning rate, and by other explicit mechanisms. We choose time-dependent G because using a
learning-rate schedule has practical importance in training deep models. Specifically, more regular-
ization erases local high-frequency contours in parameter space. As the correct region is approached,
regularization is reduced, leading to a better final solution.
4
Under review as a conference paper at ICLR 2018
Algorithm 1 Continuous Propagation
Input: X, Θ0, L, y
Output: Θ∞
{Array access is modulo array size}
h[D][D], hidden activation storage
δ[D][2], delta storage
Θ[D], parameters
Θ 一 Θo
for all t ∈ N do
h[0][t] . x~X
δ[D -1][t] -L(h[D∏t],y(h[0∏t- D]))
for all layers k in parallel do
h[k][t] J φ(Θ[k]h[k — 1][t — 1])
δ[k][t] J φ0(h[k]∖t+k])Θτ[k]δ[k+1][t-1])
Θ[k] J Θ[k] — α(h[k][t + k — D] × δ[k][t])
end for
end for
Θ∞ J Θ
	hl(t)	>		—hl+1(Θ)—>
	Θ(t)	
— δl(Θ; L)一		一 δl+1(Θ; L)一
Figure 3: Each processor implements layer dy-
namics. Param values and states (e.g., momen-
tum) reside locally in the node. Activations and
deltas stream through, modified by local params.
Algorithm 2 Local Learning Rules for MBGD
V = GTh(IT)) × δ(I)
G	(Gt-1 + V, t — 2D + l < N mod U
t 0,	otherwise
Θf = ∫Θt-ι — αGt, t — 2D + L = N mod U
t	Θt-1,	otherwise
4	Continuous Propagation
Continuous propagation is derived by considering an arbitrary feed-forward neural network (Fig 2).
Expressing all nodes as functions of time (Table 1), and applying function composition,2 gives
h(k) = (Y φ⑴◦ Θ⑺)。x,
δ(k) = (Y φ0(h⑴)Θ⑴)L(h(D),y),
Θ(k) = -(h(k) × δ(k)).
(6)
(7)
(8)
This factorization shows individual network layers as systems with their own local dynamics (Eq. 8;
Fig. 3). Internal state Θ evolves according to stimuli h and δ to which it is subjected. The framework
is general, and modifications yield analogs of other learning rules. For example, gradient descent
with momentum corresponds to3
V(mko)m(t) ≡ h(k) ×δ(k),	(9)
∞
νm(ko)m(t) ≡	e-λτ V(mko)m(t — τ)dτ ,	(10)
0
θ mm(t) ≡ —λν⅛m(t).	(”)
The system we are characterizing has two natural dimensions: network depth and time evolution of
parameters. These are depicted in the pipeline drawings of Figure 1. Any implementation that seeks
acceleration by mapping network layers to computational units separated in space must also accept
k
2 Y。fi ≡ fk ◦ fk-1。…。fo.
i=0
3 Vmm is an IIR filter operating on Vmom.
5
Under review as a conference paper at ICLR 2018
latency communicating between these layers. Therefore, we introduce a time delay4 between layers:
h(k) = (Y d1 ◦ φ(i) ◦ Θ(i)) ◦ X ;
δ(k) = (Y φ0(dDi(h(i)))Θ⑴|) L(h(D),y);
Θ(k) = - (dD-k(h(k))) × δ(k) .
(12)
(13)
(14)
The continuous-propagation algorithm (Alg. 1; Fig. 1(c)) is the synchronous implementation of
these time-delay equations.
Notice that an input vector and its hidden representations experience model parameters at different
time steps as it travels forward in the network. This difference cannot be detected by the vector itself
on the way forward. It is as if we choose a fixed set of parameters from successive time steps and
applied learning to the aggregate parameter state that results.
Naturally, we have the choice when the delta values return through the network on the backward
pass either to use the immediate model parameters as they now stand or else to retrieve the historical
version of model parameters5 anchored to when the layer was used for the corresponding forward-
pass activity. Deltas computed from the immediate network parameters use updated information
corresponding to the current parameter slope. We might expect that they improve gradient descent:
∂L _ ∂h(k)
∂ Θ(k) = ∂Θ(k)
∂ L
∂h(k)
h(k-1) × δ(k)
(15)
Our choice of δ(k) in this vector product can point either in the current direction downhill or in a
historical direction. We are better with the current direction so long as h(k-1) is uncorrelated in
expectation. In practice we do not see a large difference in these approaches (Sec. 5.2).
Total memory required in this algorithm is O(D2), the same as in SGD. Traditional techniques like
reverse checkpoint (DaUvergne & Hascoet, 2006) can be adapted to this regime (Fig. 1(d)). We
have the interesting choice when using reverse checkpoint to admit immediate network weights for
recompUted activations. When we do this, the recompUted activations differ from those originally
compUted becaUse of parameter evolUtion. In addition to reverse checkpoint’s normal Use in redUcing
memory footprint, this also redUces the time disparity in parameters Used for compUting forward-
propagating activations and backward-propagating deltas in the aligning wave fronts.
When continUoUs propagation is implemented as a series of local learning rUles for each layer, it is
capable of expressing a variety of traditional algorithms. For example, rUles for MBGD are shown in
Algorithm 2 (Fig. 1(b)). Observe that while the compUtation exactly matches the MBGD algorithm,
the expression of synchronization and deferred parameter Updates appears somewhat arbitrary.
4.1 Convergence Analysis
ContinUoUs representation of the learning process allows Us to relate the time delay Used in parameter
Updates (12-14) to the discrete form of SGD (1). A key observation is that the simUlated continUoUs
model time ∆t that elapses dUring a layer compUtation is proportional to the learning rate α. As the
learning rate schedUle redUces α, the model becomes asymptotically closer to traditional SGD.
Therefore, we can adapt traditional convergence proofs for online descent to continUoUs propagation.
We extend the approach of Lian et al. (2015) to the case where each layer’s parameters get different
delays of Updates. The asymptotic eqUivalence allows Us to boUnd layer delays and demonstrate that
the expected norms of gradients for each layer converge.
The Appendix presents the formal convergence proof for the continUoUs-propagation method Un-
der the weak assUmptions that the gradient of the objective fUnction is Lipschitzian and that the
stochastic gradient is Unbiased with boUnded variance.
4 dk f (t) ≡ f (t - k∆t), where ∆t is layer latency in Units of model time.
5 To use anchored parameters, replace Equation 13 with δ(k) = (QD=k φ0(dD-k (h(i)))dD-k (Θ(i) l)) L(h(D),y).
6
Under review as a conference paper at ICLR 2018
Table 2: Hyper Parameters tested with Continuous Propagation
Momentum	μ1∕2	half-life in epochs	[0, 0.10]
Normalization	-	-	None, Normalization PropagationArpit et al.
Learning Rate	α	-	(2016) [0.001, 0.05]
Learning Rate	α1∕2	half-life in epochs	CIFAR: [12.5, 50] SVHN: 1
Decay Parameter	θ1∕2	half-life in epochs	[0, 1]
Averaging Data Set	-	-	SVHN, CIFAR-10, CIFAR-100
Delta rule	-	-	Anchored, Immediate
Bias	-	-	With bias, No bias allowed
Initialization	-	-	Glorot, Glorot + Orthonormal
5 Experimental Results
We studied continuous propagation on deep convolutional networks with the network-in-network
architecture Lin et al. (2013). We observed successful training in a variety of settings shown in table
2. In our experiments we initiate the learning schedule hyperparameters based on the values derived
for MBGD. We decrease the α proportionally to the square root of the batch size VMB and increase
the momentum μ to adjust for the √MB factor in the half-life decay.
5.1 Compatible with State of the Art Methods
We show CP is compatible with state of the art normalization techniques. We incorporate Normal-
ization Propagation Arpit et al. (2016) which is an adaptation of the popular Batch Normalization
Ioffe & Szegedy (2015) technique that works at batch size 1.
We compare validation accuracy on SVHN, CIFAR-10 and CIFAR-100 using continuous propaga-
tion with normalization propagation with the results obtained by Arpit et al. (2016) using the same
network architecture as the comparison study.
Figure 4 shows validation accuracy in these experiments. In all cases continuous propagation is able
to match the validation accuracy in many fewer training epochs.
(a) CIFAR-100
0.70
0.65
CP: β=0.014, guz=0.02,
βιβ=20,0ιa=O.O2
MB-5O: α=O.O5O
O 20	40	60	80 IOO
Epoch
0∙75^	——MB-1∞: 0=0.080, αiβ=5
——CP: a=0.014, ««2=1.03
0.70 J------1—1-------------l-.------1--------
O 5	10	15	20	25
Epoch
(b) CIFAR-10
(c) SVHN
Figure 4:	Validation-sample accuracy during training
5.2 Anchored-Delta versus Immediate-Delta Rules
We train the convolution network under two conditions, following the discussion from Section 4: the
anchored-delta rule, which uses the original parameters; and the immediate-delta rule, which uses
the evolved parameters. The goal of this substudy is to compare the learning performance of the two
methods. The immediate rule allows us to avoid using extra memory to store the model parameter
values for multiple input vectors.
To facilitate comparison, we disable stability-inducing features from the network. For instance, we
have no bias addition or normalization of the hidden features. We performed a search over the
learning hyper parameter a with momentum fixed at μ ι = 0.0008.
7
Under review as a conference paper at ICLR 2018
Learning rate = 0.6x∕100
(a) Immediate- and anchored-rules achieve similar
accuracy. Plot shows accuracy at epoch 20 for a
sweep of learning rates.
Epoch
(b) Immediate is better than anchored for first
epoch. Plot shows average performance over 100
random initializations.
Figure 5:	Comparison of immediate- and anchored- delta rules.
We observe a negligible difference in accuracy between these methods after 20 epochs of training
(Fig. 5(a)). We noticed a trend that early in training the immediate rule seemed to out-perform the
anchor rule but became slightly worse during the second epoch. Because this difference was small,
we decided to run 200 randomized trials to confirm the existence of this effect (Fig. 5(b)).
6 Discussion
The computing model we have posited is generic and capable of implementing traditional MBGD-
style algorithms exactly and efficiently. As in other batch schemes, our framework enjoys a property
that for large batch sizes computational utilization asymptotically approaches 100%.
We realized that instead of idly waiting for batch-boundary synchronization, we could also im-
mediately start processing the subsequent batch. This results in a strategy that is similar to ASGD.
Namely, the parameter inconsistency within the pipeline is limited to no more than one batch bound-
ary. This is also true in ASGD if the worker nodes are balanced on vector throughput. In that case,
workers should never be more than one time step removed from their peers.
In considering the optimal batch size to use, we were led down a path of differential equations and
rules for the learning dynamics of each layer. Research in feedback alignment shows the importance
of feedback dynamics in learning. To implement local dynamics it is natural for weights to reside
in physical proximity to their use. This is true in the case of biological neurons (weight encoded in
synaptic strength), as it is in our model-parallel algorithm.
When considering what an optimal strategy may look like, we realize that we always have the ability
to specify that a layer remain idle at a time step in order to create a global synchronization boundary.
Likewise, we also have the ability to allow a weight to remain fixed instead of adopting a new
value. Why would either of these strategies be ideal? As we allow data to stream through the neural
network, each input from the environment and each measurement of the cost function contains some
amount of useful information not yet extracted from the environment. The purpose of any learning
equation is to allow the network to respond to this information. In this light both the strategy of “idly
waiting” and the strategy of “keeping fixed” are rejections of the utility of this information. See the
step-function specification of the MBGD algorithm (Alg. 2). These are indications of suboptimality.
We demonstrated that choosing a lower learning rate dominates using larger batch sizes. Continuous
propagation allows statistically efficient learning while maintaining all the cores of a multiprocessor
system busy. It permits explicit regularization terms such as L1 penalty on parameters or activations.
In gradient descent, explicit regularization works by biasing the gradient in favor of regularization.
Regularization’s contribution to gradient is available immediately and does not require traversal
through the entire network. Therefore in CP parameter update based on regularization penalty is
applied before the corresponding loss gradient is applied.
We note that the hidden activity and delta arcs in our pipeline diagrams (Fig. 1) can be considered as
individual vectors or batch matrices. Interpreting them as batch matrices allows investigating these
techniques directly on contemporary GPU hardware. This is also a demonstration that continuous
propagation can be combined with data parallelism in case network depth is exhausted.
8
Under review as a conference paper at ICLR 2018
References
Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, and Venu Govindaraju. Normalization Propaga-
tion: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks. ArXiv
e-prints, March 2016.
Leon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,
17(9):142, 1998.
Wojciech Marian Czarnecki, Grzegorz Swirszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals,
and Koray Kavukcuoglu. Understanding synthetic gradients and decoupled neural interfaces.
CoRR, abs/1703.00522, 2017. URL http://arxiv.org/abs/1703.00522.
Benjamin Dauvergne and Laurent Hascoet. The data-flow equations of checkpointing in reverse
automatic differentiation. In International Conference on Computational Science, pp. 566-573.
Springer, 2006.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in
neural information processing systems, pp. 1223-1231, 2012.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/
abs/1502.03167.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp.
498-515. Springer, 2015.
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for
nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 2737-2745,
2015.
Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random feedback
weights support learning in deep neural networks. arXiv preprint arXiv:1411.0247, 2014.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. CoRR, abs/1312.4400, 2013. URL
http://arxiv.org/abs/1312.4400.
Martin Fodslette M0ller. A scaled conjugate gradient algorithm for fast supervised learning. Neural
networks, 6(4):525-533, 1993.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538, 2017.
D Randall Wilson and Tony R Martinez. The general inefficiency of batch training for gradient
descent learning. Neural Networks, 16(10):1429-1451, 2003.
Shanshan Zhang, Ce Zhang, Zhao You, Rong Zheng, and Bo Xu. Asynchronous stochastic gradient
descent for dnn training. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pp. 6660-6663. IEEE, 2013.
9
Under review as a conference paper at ICLR 2018
Appendix: Convergence Proof
The variable xk,m,d denotes the random variable for data specifically for the kth training iteration,
for the mth sample with the mini-batch used for the dth layer. The variable Θd with the superscript
d denotes parameters from the dth layer only. The variable τk,m,d is used to denote that the gradient
update for the dth-layer parameter at the kth iteration uses a gradient computed from the parameter
Θd at the (k - τk,m,d)th iteration using the mth sample.
Assumption 1. The stochastic gradient G(Θ; x) is unbiased: Vf (Θ) = Eχ[G(Θ; x)].
Assumption 2. The variance ofthe stochastic gradient is bounded: Eχ[∣∣G(Θd; x)-Vf(Θd)k2] ≤
σd2 ∀Θ.
Assumption 3. The gradient function Vf (.) is Lipschitzian: kVf (Θd) - Vf (Γd)k2 ≤ LdkΘd -
Γdk2 ∀Θ, Γ.
Assumption 4. All the random variables in {xk,m,d}k∈{0,1,...,K},m∈{1,...,M},d∈{1,...,D} are inde-
pendent.
Assumption 5. All decay variables τk,m,d have an upper bound of some fixed integer T.
Proposition. If the assumptions hold and the step-length sequence {γk}k∈{1,...,K} satisfies
T
LMγk+2L2M2TγkXγk+t ≤ 1 ∀k,	(16)
t=1
then we have the following convergence for Algorithm 1,
1
-K
P γk
k=1
K
XγkE[kVf(Θk)k2] ≤
k=1
2(f(Θι)- f (Θ*))
K
M P γk
k=1
D	K	k-1
P σd2 P γk2MLd + 2L2dM2γk P γj2
d=1	k=1	j=k-T
K
M P γk
k=1
Proof. From the Lipschitzian assumption on f (.), we have
f(Θk+ι) - f(Θk) ≤ hVf(Θk), Θk+1 - Θki + 2kΘk+ι - Θkk2.	(17)
Taking expectation with respect to xk,m,l on both sides (notice that the left-hand side is independent
of xk,m,l ), we get
D	M	2L	M
f(θk+l) - f(θk) ≤ XhVf (θd), X G(θd-τkm,d , Xk,m,l)i + γ2- k X G(θd-Tkm,d , Xk,m,l)k2 .
d=1	m=1	m=1
(18)
Given that the assumptions hold, and following the proof of theorem 1 of Lian et al. (2015), we get
f(Θk+1) - f(Θ1)
≤ X -MM X YkE[kVf(θd)k2] + Xσ I γkM2-d + -dM2Yk X γ2 I .	(19)
d=1	k=1	k=1	j=k-T
K
Rearranging the terms and dividing by P Yk on both sides, we get
k=1
D K	k-1
K	2(f(Θ1) -	f(Θk+1)) + Pσd2	P	Yk2M-d+2-2dM	2Yk	P	Yj2
1	d=1	k=1	j=k-T
~K~ EYkE[kVf(Θk)k2] ≤ ----------------------------------W-----------------------------'-
P Yk k=1	M P Yk
k=1	k=1
(20)
Since f (Θ*) ≤ f (Θk+ι), we achieve our claim.	□
10