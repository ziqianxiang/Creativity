Under review as a conference paper at ICLR 2017
Epitomic Variational Autoencoder
Serena Yeung *
Stanford University
{serena}@cs.stanford.edu
Li Fei-Fei
Stanford University
{feifeili}@cs.stanford.edu
Anitha Kannan & Yann Dauphin
Facebook AI Research
{akannan,ynd}@fb.com
Ab stract
In this paper, we propose epitomic variational autoencoder (eVAE), a probabilis-
tic generative model of high dimensional data. eVAE is composed of a number
of sparse variational autoencoders called ‘epitome’ such that each epitome par-
tially shares its encoder-decoder architecture with other epitomes in the composi-
tion. We show that the proposed model greatly overcomes the common problem
in variational autoencoders (VAE) of model over-pruning. We substantiate that
eVAE is efficient in using its model capacity and generalizes better than VAE, by
presenting qualitative and quantitative results on MNIST and TFD datasets.
1	Introduction
Unsupervised learning holds the promise of learning the inherent structure in data so as to enable
many future tasks including generation, prediction and visualization. Generative modeling is an
approach to unsupervised learning wherein an explicit stochastic generative model of data is defined,
such that independent draws from this model are likely to produce the original data distribution,
while the learned latent structure itself is useful in prediction, classification and visualization tasks.
The recently proposed variational autoencoder (VAE) (Kingma & Welling, 2014) is an example of
one such generative model. VAE pairs a top down generative model with a bottom up recognition
network for amortized probabilistic inference. Both networks are jointly trained to maximize a
variational lower bound on the data likelihood. A number of recent works use VAE as a modeling
framework, including iterative conditional generation of images (Gregor et al., 2015) and conditional
future frame prediction (Xue et al., 2016).
A commonly known problem with the VAE lower bound is that it is known to self-prune or un-
der utilize the model’s capacity (Mackay, 2001). This can lead to poor generalization. A common
approach to alleviate this problem is to resort to optimization schedules and regularization tech-
niques (Bowman et al., 2015; Kaae Sonderby et al., 2016) that trade-off two competing terms, latent
cost and data reconstruction, in the bound. Fig. 1 provides a quick insight into this problem of
over-pruning and how commonly used regularization techniques may not be sufficient. Detailed
discussion is provided in § 2.1.
In this paper, we take a model-based approach to directly address this problem. We present an exten-
sion of variational autoencoders called epitomic variational autoencoder (Epitomic VAE, or eVAE,
for short) that automatically learns to utilize its model capacity more effectively, leading to better
generalization. Consider the task of learning a D-dimensional representation for the examples in
a given dataset. The motivation for our model stems from the hypothesis that a single example in
the dataset can be sufficiently embedded in a smaller K-dimensional (K	D) subspace of D.
However, different data points may need different subspaces, hence the need for D. Sparse coding
methods also exploit a similar hypothesis. Epitomic VAE exploits sparsity using an additional cat-
egorical latent variable in the encoder-decoder architecture of the VAE. Each value of the variable
activates only a contiguous subset of latent stochastic variables to generate an observation. This
*Work done during an internship at Facebook AI Research.
1
Under review as a conference paper at ICLR 2017
enables learning multiple shared subspaces such that each subspace specializes, and also increases
the use of model capacity (Fig. 4), enabling better representation. The choice of the name Epit-
omic VAE comes from the fact that multiple miniature models with shared parameters are trained
simultaneously.
The rest of the paper is organized as follows. We first describe variational autoencoders and math-
ematically show the model pruning effect in § 2. We then present our epitomic VAE model in § 3
that overcomes these shortcomings. Experiments showing qualitative and quantitative results are
presented in § 4. We finally provide more general context of our work in the related work in § 5, and
conclude with discussions.
2	Variational Autoencoders
The generative model (decoder) of a VAE consists of first generating a D-dimensional stochastic
variable z drawn from a standard multivariate Gaussian
p(z) = N (z; 0; I)
(1)
and then generating the N-dimensional observation x from a parametric family of distributions such
as a Gaussian
Pθ(XIZ)= N(x; fi(z)；exp(f2(z)))	(2)
where f1 and f2 define non-linear deterministic transformations of z modeled using a neural net-
work. The parameters θ of the model are the weights and biases of the neural network that encodes
the functions f1 and f2.
Given a dataset X of T i.i.d samples, the model is learned such that it maximizes the likelihood of
the parameters to have generated the data, P(X ∣θ). This maximization requires marginalizing the
unobserved z. However, computing p(z|x) is intractable due to dependencies induced between the
zi when conditioned on x.
Variational autoencoders, as the name suggests, use variational inference to approximate the exact
posterior with a surrogate parameterized distribution. However, instead of having separate parame-
ters for the posterior distribution of each observation, VAE amortizes the cost by learning a neural
network with parameters φ that outputs the posterior distribution of the form qφ(z∣x) = Qd q(zi∣x).
This results in the lower bound given by
log pθ (X)
X log	pθ(x(t) , z)
≥
T
X Eqφ(z∣χ(t))[logP(X(t)|Z)] - KL(qφ(Z|x(t)) k P(Z))
t=1
(3)
(4)
VAE is trained with standard backpropagation using minibatch gradient descent to minimize the
negative of the lowerbound
T	TD
Cvae = - X Eqφ(z∣χ(t))[lθg P(X ⑴ |z)] + XX KL ^φ(zi∣x(t) ) k P(Zi))	(5)
2.1	Automatic model over-pruning in VAE
Cvae introduces a trade-off between data reconstruction (first term) and satisfying the independence
assumption of P(z) (second term, KL).
Of particular interest is the KL term. Since the KL term is the sum of independent contributions from
each dimension d of D, it provides unduly freedom for the model in how it minimizes this term. In
particular, the model needs to only ensure that the overall KL term is minimized, on average, and
not per component wise. The easiest way for the model to do this is to have a large number of
components that satisfies the KL term effectively, by turning off the units so that the posterior for
those units becomes the same as the prior1. This effect is quite pronounced in the early iterations of
1Since log variance is modeled using the neural network, turning it off will lead to a variance of 1.
2
Under review as a conference paper at ICLR 2017
£。/看S.第t⅜及炉号
抄S'F。可-?r
5宏2代今?々?
m4'子鹿 2Λ∙必⅛3 g
邑∙>∙c∕Sa%&‹«$ f
/-¥◎,寸&¾£。夕 S
。& * Y WV.-4C7 ？
⅜3*r?。y 鼠，y 56
4 ~y⅛∕>ɑ∙ M3->E /
9-4;刍Jλ2i->K
？ U + q ? " ?:c'λ
V-A I e∙J<^⅛rf ⅛ ?/
y>⅛∙⅞∙ G2“≡邑 3 8
V?与金 9，-34r7∙r.J
Tirg JS / 07-1 3 +
*3 N"√彳户 W-P£ Cb >
~< 35r5∙71>ZJ £ -U
彳f?J 9 7 k
≠,L7 ∙ Qr 号 2 q<?-Zz
X)?7∙g -ZΛ5S'欠 F
S<14⅛J∙OZ4 令
£4 3 3 8 C 72α G
N 9£ -4 G 工 q;OT
./ &x;4: 3 5，r
a KaISJYJ/F
674rr7 039sc'
/亍 2<τ<37)4ot,咛
∑qrsα⅛∕osq7
a3Z2S 7 3 £
40£夕。κ〜？，々
O
4 2 O 8 6 4 2 O
L L L α α α α α
xAov
40
30
20
Figure 1: Sorted activity level of latent units and corresponding generations on MNIST, for a 50-d VAE with a
hidden layer of 500 units. Shown for varying values of the KL weight λ. When λ = 1, only 30 units are active.
As λ is decreased, more units are active; however generation does not improve since the model uses the capacity
to model increasingly well only regions of the posterior manifold near training samples (see reconstructions in
Fig. 8).
Dead units
Active units
All units
Figure 2:	Only active units contribute to generation, whereas units that have “died” have no effect. Shown for
a 50-d VAE with λ = 1.
training: the model for logp(x|z) is quite impoverished and hence the easiest way to improve the
bound is by turning off the KL terms. However, once the units have become inactive, it is almost
impossible for them to resurrect, and hence the full capacity of the model is not utilized.
A quantity that is useful in understanding this effect, is the activity level of a unit. Following Burda
et al. (2015), We define a unit to be used, or “active”, if Au = Covx(Eu〜q®x)[u]) > 0.02.
A commonly used approach to overcome this problem is to use a trade-off between the two terms
using parameter λ so that the cost is
D
C = -Eqφ(z∣x)[logp(x∣z)] + λXKLQqφ(zi∣x) k P(Zi))	⑹
i=1
Fig. 1 shoWs the effect of λ on unit activity and generation, With λ = 1 being the correct objective
to optimize. While tuning doWn λ increases the number of active units, samples generated from
the model are still poor. Fig. 2 shoWs generation using all units, active units only, and dead units
only, for λ = 1. The model spends its capacity in ensuring that reconstruction of the training set is
optimized (reconstruction visualizations are shoWn in § 8.1), at the cost of generalization. This has
led to more sophisticated schemes such as using an annealed optimization schedule for λ (BoWman
et al., 2015; Kaae Sonderby et al., 2016) or enforcing minimum KL contribution from subsets of the
latent units (Kingma et al., 2016).
In this paper, We present a model based approach called “epitomic variational autoencoder” to ad-
dress the problem of over pruning.
3 Model
We propose epitomic variational autoencoder (eVAE) to overcome the shortcomings of VAE by
enabling more efficient use of model capacity to gain better generalization. We base this on the
observation that While We may need a D-dimensional representation to accurately represent every
example in a dataset, each individual example can be represented With a smaller K-dimensional
subspace. As an example, consider MNIST With its variability in terms of digits, strokes and thick-
3
Under review as a conference paper at ICLR 2017
y=ι
Ra2 2 2 2 2 2 2 2
8222222223
888C，，，
888ggC,
88gggggrrr
ggggggggrr
BggggggF，，
ggggggggg，
*ggggggggg
22g88ggggg
23 00 00 00 00 00 00 009
3800800000Ogg，
2 2 2 2 2
2 2 2 2 2
2
-i2 ? 2 E
∙~ 2 2 2
2 2 2 Oo Oo
2 2 g g g
2 g g Oo 9
OQgggg
OOgggg
Oogggg
Oo Oo Oo Oo 9
Γ,66666666
γ5,66s,6666
Γs∙s66s666g
ΓSS5SSS664
.....
片S5555S5SS6
S55SSSSS44
55555SS664
r55ss66ss4
Figure 3:	Left: Illustration of an epitomic VAE with dimension D=8, epitome size K=2 and stride S=2. In this
depiction, the second epitome is active. Right: Learned manifolds on MNIST for 4 different epitomes in a 20-d
eVAE with size K = 2 and stride s = 1. We observe that each epitome specializes on a coherent subset of
examples.
ness of ink, to name a few. While the overall D is large, it is likely that only a few K dimensions of
D are needed to capture the variability in strokes of some digits (see Fig. 3).
Epitomic VAE can be viewed as a variational autoencoder with latent stochastic dimension D that
is composed of a number of sparse variational autoencoders called epitomes, such that each epitome
partially shares its encoder-decoder architecture with other epitomes in the composition. In this
paper, we assume simple structured sparsity for each epitome: in particular, only K contiguous
dimensions of D are active2.
The generative process can be described as follows: A D-dimensional stochastic variable z is drawn
from a standard multivariate Gaussian p(z) = N (z; 0; I). In tandem, an epitome is implicitly
chosen through an epitome selector variable y, which has a uniform prior over possible epitomes.
The N -dimensional observation x is then drawn from a Gaussian distribution:
Pθ(x|y, Z) = N(x; fι(my © z), exp(f2(my Θ Z)))	(7)
my enforces the epitome constraint: it is also a D-dimensional vector that is zero everywhere except
in the active dimensions of the epitome. © is element-wise multiplication between the two operands.
Thus, my masks the dimensions of z other than those dictated by the choice of y. Fig. 3 illustrates
this for an 8-d z with epitome size K = 2, so that there are four possible epitomes (the model also
allows for overlapping epitomes, but this is not shown for illustration purposes). Epitome structure
is defined using size K and stride s, where s = 1 corresponds to full overlap in D dimensions3. Our
model generalizes the VAE and collapses to a VAE when D = K = s.
f1() and f2() define non-linear deterministic transformations of modeled using neural networks.
Note that the model does not snip off the K dimensions corresponding to an epitome, but instead
deactivates the D-K dimensions that are not part of the chosen epitome. While the same deterministic
functions f1 and f2 are used for any choice of epitome, the functions can still specialize due to the
2The model also allows for incorporating other forms of structured sparsity.
3The strided epitome structure allows for learning O(D) specialized subspaces, that when sampled during
generation can each produce good samples. In contrast, if only a simple sparsity prior is introduced over
arbitrary subsets (e.g. with Bernoulli latent units to specify if a unit is active for a particular example), it can
lead to poor generation results, which we confirmed empirically but do not report. The reason for this is as
follows: due to an exponential number of potential combinations of latent units, sampling a subset from the
prior during generation cannot be straightforwardly guaranteed to be a good configuration for a subconcept in
the data, and often leads to uninterpretable samples.
4
Under review as a conference paper at ICLR 2017
sparsity of their inputs. Neighboring epitomes will have more overlap than non-overlapping ones,
which manifests itself in the representation space; an intrinsic ordering in the variability is learned.
3.1 Overcoming over-pruning
Following Kingma & Welling (2014), we use a recognition network q(z, y|x) for approximate pos-
terior inference, with the functional form
q(z, y|x) = q(y|x)q(z|y, x)	(8)
= q(y∣x)N(z; my Θ μ, exp(my Θ φ))	(9)
where μ = hi (x) and φ = h2 (x) are neural networks that map X to D dimensional space.
We use a similar masking operation to deactivate units, as decided by the epitome y . Unlike the
generative model (eq. 7), the masking operation defined by y operates directly on outputs of the
recognition network that characterizes the parameters of q(z|y, x).
As in VAE, we can derive the lower bound on the log probability of a dataset, and hence the cost
function (negative bound) is
Cevae
T
- X Eq(z,y|x(t))[log p(x(t)|y, z)]
t=1
T
-XKL q°(y|x(t)) Il pθ⑼
t=1
T
—XXqφ(y∣χ(t))KL qφ(z∖y,χ(t)) k p(Z)
t=1 y
(10)
The epitomic VAE departs from the VAE in how the contribution from the KL term is constrained. Let us
consider the third term in eq. 10, and substituting in eq. 9:
T
XXqφ(y∣χ(t))KL qφ(z∖y,χ(t)) kp(Z)	(ii)
t=1 y
T
=XX qφ(y∣x(t))KL [n (z; my Θ μ(t), exp(my Θ φ(t))) k N (z; 0, I)	(12)
t=1 y
TD
=XXq@(y|x(t))X 1[md,y = 1]KL N(Zd；μdt,eχp(Odt))) k N(0,1)	(13)
t=1 y	d=1
where 1[?] is an indicator variable that evaluates to 1 if only if its operand ? is true.
For a training example χ(t) and for a fixed y (and hence the corresponding epitome), the number of KL terms
that will contribute to the bound is exactly K. The dimensions of z that are not part of the corresponding
epitome will have zero KL because their posterior parameters are masked to have unit Gaussian, the same as
the prior. By design, this ensures that only the K dimensions that explain χ(t) contribute to Cevae.
This is quite in contrast to how VAE optimizes Cvae (§. 2.1). For Cvae to have a small contribution from the
KL term of a particular zd, it has to infer that unit to have zero mean and unit variance for many examples
in the training set. In practice, this results in VAE completely deactivating units, and leading to many dead
units. EpitomicVAE chooses the epitome based on χ(t) and ensures that the dimensions that are not useful
in explaining χ(t) are ignored in Cevae . This means that the unit is still active, but by design, only a fraction
of examples in the training set contributes a possible non-zero value to zd’s KL term in Cevae. This added
flexibility gives the model the freedom to use more total units without deactivating them, while optimizing the
bound. With these characteristics, during training, the data points will naturally group themselves to different
epitomes, leading to a more balanced use of Z.
In Fig. 4 we compare the activity levels of VAE, dropout VAE and our model. We see that compared with VAE,
our model is able to better use the model capacity. In the same figure, we also compare with adding dropout to
the latent variable Z of the VAE (Dropout VAE). While this increases the number of active units, it generalizes
poorly as it uses the dropout layers to merely replicate representation, in contrast to eVAE. See Fig. 5 along
with the explanation in § 4.1 where we compare generation results for all three models.
5
Under review as a conference paper at ICLR 2017
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
VAE
△ Dropout VAE
・ eVAE
0	10	20	30	40	50
Unit
Figure 4: Adding dropout to a VAE (here, dropout rate 0.5 is shown) can prevent the model from pruning
units, shown for MNIST. However, in contrast to eVAE, it uses the additional units to encode redundancy, not
additional information, and therefore does not address the problem. Generation results are shown in Fig. 5.
3.2 Training
The generative model and the recognition network are trained simultaneously, by minimizing Cevae in eq. 10.
For the stochastic continuous variable z, we use the reparameterization trick as in VAE. The trick involves
reparametrizing the recognition distribution in terms of auxiliary variables with fixed distributions. This allows
efficient sampling from the posterior distribution as they are deterministic functions of the inputs and auxiliary
variables.
For the discrete variable y, we cannot use the reparameterization trick. We therefore approximate q(y|x)
by a point estimate y* so that q(y∣x) = δ(y = y*), where δ evaluates to 1 only if y = y* and the best
y* = arg min Cevae. We also explored modeling q(y∣x) = Cat(h(x)) as a discrete distribution with h
being a neural network. In this case, the backward pass requires either using REINFORCE or passing through
gradients for the categorical sampler. In our experiments, we found that these approaches did not work well,
especially when the number of possible values of y becomes large. We leave this as future work to explore.
The recognition network first computes μ and φ. It is then combined with the optimal y* for each example, to
arrive at the final posterior. The model is trained using a simple algorithm outlined in Algo. 1. Backpropaga-
tion with minibatch updates is used, with each minibatch constructed to be balanced with respect to epitome
assignment.
Algorithm 1 Learning Epitomic VAE
1:	θ, φ —Initialize parameters
2:	for until convergence of parameters (θ, φ) do
3:	Assign each X to its best y* = arg min Cevae
4:	Randomize and then partition data into minibatches with each minibatch having proportion-
ate number of examples ∀ y
5:	for k ∈ numbatches do
6:	Update model parameters using kth minibatch consisting of X, y pairs
7:	end for
8:	end for
4	Experiments
We present experimental results on two datasets, MNIST (LeCun et al., 1998) and Toronto Faces Database
(TFD) (Susskind et al., 2010). We show generation results that illustrate eVAE’s ability to better utilize model
capacity for modeling data variability, and then evaluate the effect of epitome choice and model complexity. Fi-
nally we present quantitative comparison with other models and qualitative samples from eVAE. We emphasize
that in all experiments, we keep the weight of the KL term λ = 1 to evaluate performance under optimizing the
true derived lower bound, without introducing an additional hyperparameter to tune.
6
Under review as a conference paper at ICLR 2017
We use standard splits for both MNIST and TFD. In our experiments, the encoder and decoder are fully-
connected networks, and we show results for different depths and number of units of per layer. ReLU non-
linearities are used, and models are trained using the Adam update rule (Kingma & Ba, 2014) for 200 epochs
(MNIST) and 250 epochs (TFD), with base learning rate 0.001.
Z Sτf9.g*∕τ7r>ict□
OEU *⅛/- +&£ Ti r⅛ Λv
∙4Js3 227oy3
*Q ,C J 8 g2.y2 Nj--
”:•:口，；
0，；>?亍。。//
z"α4 n.6 ZA7 /
£1?62"z7rd T
m J,77⅛l 1(3 夕 …
S 77 4-1 >"∕<<J3J
5 5 今 , 3 3 ,¥ 6 3，
464 6-3⅛∙M^s,7
θ 4 2 2 θ X- 5*zgo
7 9CK 9 8d + 7s--
ɔ-d: ::
W 夕----？ * 8772 ʃ
7777///2 夕*7x
95g6G ,34.
7，、47/5，Jr 幺
7夕4，JgA8 "Iq
75747 8*06 .3
5 3 3 夕 4 % O 6 2 4
夕”qJ√“夕5F2
d . !，: &
。711/工。74q
JI5fl^s√^3
7 S 96 a T / J r 8
In R 96，，3 q∙)J
Cj√d3o2 夕 q/
J2 3“7Bτlħqg
C⅛a6azjqgd>q
∙e**763Γmay
¥FrI 3∂FG*F7
3gβG5 Cx / 2 S f
Sfcg35≡7? IJg
才 CrsqgIT'q 09 Q
1 9424gg3，N3
u∖93a3t>jpqcr
6⅜π63Jserτ,3fφ
36g6,73 qag
S¼4*93∙cβ833<∕
彳Q3Ga6qo0a夕
gaa3gcrgΓα9
0a∂963r<κ4 33
2g∙yH83g6v
aovd*ne399∂
J8fgr‰7ye8,
“q693FB93g
GqgaaW 旷2 9g
QeG 95QS87分
❷g0DayQa,2
2“ Gα0 a"0J8
。7个49—鼻g夕G
G3yGaga 夕9夕
。2 2<%333G OG
65□gs3BGgu
"3⅝Gg3g⅜5 3
30cx6r<ΓI9<⅜g
8470385086
80。23g3"5g
¥70»7/55J6F
6Q3g□■9 355,
5ggA∕5M451
3708夕IGg.5
58αs∕g61π34
Atf Zto□O q31q6 夕
<⅛SΓ*48 专 31 夕O
5，Γ*3g5xlgQ
OO^ggSoJɪ.ua
JW3RX∕α∙⅛x⅛lf
⅛I4 Λv√-63∕b f- 1
QfGgagSOI3
Jo6rl2Gsqqcr
6 夕 2 4 3aoa2,
18∕4>4s2Ffx
0 8**J4 7sCΓ<cr
Zto 8 I f Oi 09 5 6 S 3
θ 5 ʃ 4 ʃ 3 ʃ J S O
2-d
6t>32A933s 9
go 夕r7gggg9
gzya3J3∙0g
agas,30a⅞夕 3
α夕993 夕 97
9J7¾⅛3 77AS
里/ Wɔʤ
Figure 5: Generations from VAE, Dropout VAE, and eVAE models for different dimensions of latent variable
z. Across each row are 2-d, 5-d, 10-d, and 20-d models. VAE generation quality (1st row) degrades as latent
dimension increases, and it is unable to effectively use added capacity to model greater variability. Adding
dropout to the VAE (2nd row) fails to solve the problem since additional units are used to encode redundancy,
not additional information. eVAE (3rd row) overcomes the problem by modeling multiple shared subspaces,
here 2-d (overlapping) epitomes are maintained as the latent dimension is increased. Learned epitome manifolds
from the 20-d model are shown in Fig. 3. Boxed digits highlight the difference in variability that the VAE vs.
eVAE model is able to achieve.
4.1	Overcoming over-pruning.
We first qualitatively illustrate the ability of eVAE to overcome over-pruning and utilize latent capacity to model
greater variability in data. Fig. 5 compares generation results for VAE, Dropout VAE, and eVAE for different
dimensions D of latent variable z. With D = 2, VAE generates realistic digits but suffers from lack of diversity.
When D is increased to 5, the generation exhibits some greater variability but also begins to degrade in quality.
As D is further increased to 10 and 20, the degradation continues. As explained in Sec. 2.1, this is due to
VAE’s propensity to use only a portion of its latent units for modeling the training data and the rest to minimize
the KL term. The under-utilization of model capacity means that VAE learns to model well only regions of the
posterior manifold near training samples, instead of generalizing to model the space of possible generations.
The effect of this is good reconstruction (examples are shown in Fig. 9) but poor generation samples.
Adding dropout to the latent variable z of the VAE (row 2 of Fig. 5) encourages increased usage of model
capacity, as shown in Fig. 4 and the discussion in Sec. 2. However, due to the stochastic nature of dropout,
the model is forced to use the additional capacity to encode redundancy in the representation. It therefore
does not achieve the desired effect of encoding additional data variability, and furthermore leads to blurred
samples due to the redundant encoding. Epitomic VAE addresses the crux of the problem by learning multiple
specialized subspaces. Since the effective dimension of any example is still small, eVAE is able to model each
subspace well, while encoding variability through multiple possibly shared subspaces. This enables the model
to overcome over-pruning from which VAE suffered. Fig. 5 shows that as the dimension D of z is increased
7
Under review as a conference paper at ICLR 2017
while maintaining epitomes of size K = 2, eVAE is able to model greater variability in the data. Highlighted
digits in the 20-d eVAE show multiple styles such as crossed versus un-crossed 7, and pointed, round, thick, and
thin 4s. Additional visualization of the variability in the learned 2-d manifolds are shown in Fig. 3. In contrast,
the 2-d VAE generates similar-looking digits, and is unable to increase variability and maintain sample quality
as the latent dimension is increased.
4.2	Choice of epitome size
We next investigate how the choice of epitome size, K, affects generation performance. We evaluate the
generative models quantitatively through their samples by measuring the log-density with a Parzen window
estimator Rifai et al. (2012). Fig. 6 shows the Parzen log-density for different choices of epitome size on
MNIST, with encoder and decoder consisting of a single deterministic layer of 500 units. Epitomes are non-
overlapping, and the results are grouped by total dimension D of the latent variable z. For comparison, we also
show the log-density for VAE models with the same dimension D, and for mixture VAE (mVAE), an ablative
version of eVAE where parameters are not shared. mVAE can also be seen as a mixture of independent VAEs
trained in the same manner as eVAE. The number of deterministic units in each mVAE component is computed
so that the total number of parameters is comparable to eVAE.
As we increase D, the performance of VAE drops significantly, due to over-pruning. In fact, the number of
active units for VAE are 8, 22 and 24 respectively, for D values of 8, 24 and 48. In contrast, eVAE performance
increases as we increase D, with an epitome size K that is significantly smaller than D. Table 1 provides more
comparisons. This confirms the advantage of using eVAE to avoid overpruning and effectively capture data
distribution.
eVAE also performs comparably or better than mVAE at all epitome sizes. Intuitively, the advantage of pa-
rameter sharing in eVAE is that each epitome can also benefit from general features learned across the training
set.
Epitome size
□ □VAE□□mVAE□□eVAE
Figure 6:	Epitome size vs. Parzen log-density (nats) on MNIST, grouped by different dimensions D of latent
variable z. VAE performance for equivalent D is shown for comparison, as well as mVAE (ablative version of
eVAE without parameter sharing). For each D, the optimal epitome size is significantly smaller than D.
4.3	Increasing complexity of encoder and decoder
Here, we would like to understand the role of encoder and decoder architectures on over pruning, and the
generative performance. We control model complexity through number of layers L of deterministic hidden
units, and number of hidden units H in each deterministic layer.
Table 1 shows the Parzen log-densities of VAE, mVAE and eVAE models trained on MNIST and TFD with
different latent dimension D. For mVAE and eVAE models on MNIST, the maximum over epitomes of size
K = 3 and K = 4 is used, and on TFD epitomes of size K = 5 are used. All epitomes are non-overlapping.
We observe that for VAE, increasing the number of hidden units H (e.g. from 500 to 1000) for a fixed network
depth L has a negligible effect on the number of active units and performance. On the other hand, as the depth
of the encoder and decoder L is increased, the number of active units in VAE decreases though performance is
still able to improve. This illustrates that increase in the complexity of the interactions through use of multiple
8
Under review as a conference paper at ICLR 2017
layers counteract the perils of the over-pruning. However, this comes with the cost of substantial increase in
the number of model parameters to be learned.
In contrast, for any given model configuration, eVAE is able to avoid the over-pruning effect in the number
of active units and outperform VAE. While both VAE and eVAE approach what appears to be a ceiling in
generative performance with large models for MNIST, the difference between VAE and eVAE is significant for
all TFD models.
Table 1 also shows results for mVAE, the ablative version of eVAE where parameters are not shared. The
number of deterministic units per layer in each mVAE component is computed so that the total number of
parameters is comparable to eVAE. While mVAE and eVAE perform comparably on MNIST especially with
larger models (reaching a limit in performance that VAE also nears), eVAE demonstrates an advantage on
smaller models and when the data is more complex (TFD). These settings are in line with the intuition that
parameter sharing is helpful in more challenging settings when each epitome can also benefit from general
features learned across the training set.
		L = 1	H = 500 L=2	L=3	L = 1	H = 1000 L=2	L=3
MNIST						
VAE	283(8)	292(8)	325(8)	283(8)	290(8)	322(6)
D = 8 mVAE	300(8)	328(8)	337(8)	309(8)	333(8)	335(8)
eVAE	300（8）	330(8)	337(8)	312(8)	331(8)	334(8)
VAE	213(22)	273(11)	305(8)	219(24)	270(12)	311(7)
D = 24 mVAE	309(24)	330(24)	336(24)	313(24)	333(24)	338(24)
eVAE	311(24)	331(24)	336(24)	317(24)	332(24)	336(24)
VAE	213(24)	267(13)	308(8)	224(24)	273(12)	309(8)
D = 48 mVAE	314(48)	334(48)	336(48)	315(48)	333(48)	337(48)
eVAE	319(48)	334(48)	337(48)	321(48)	334(48)	332(48)
TFD						
D = 15 VAE	-	2173(15)	2180(15)	-	2149(15)	2116(15)
D = 15 mVAE	-	2276(15)	2314(15)	-	2298(15)	2343(15)
eVAE	-	2298(15)	2353(15)	-	2278(15)	2367(15)
D = 25 VAE	-	2067(25)	2085(25)	-	2037(25)	2101(25)
D = 25 mVAE	-	2287(25)	2306(25)	-	2332(25)	2351(25)
eVAE	-	2309(25)	2371(25)	-	2297(25)	2371(25)
D = 50 VAE	-	1920(50)	2062(29)	-	1886(50)	2066(30)
D = 50 mVAE	-	2253(50)	2327(50)	-	2280(50)	2358(50)
eVAE	-	2314(50)	2359(50)	-	2302(50)	2365(50)
Table 1: Parzen log-densities in nats of VAE, mVAE and eVAE for increasing model parameters, trained on
MNIST and TFD with different dimensions D of latent variable z. For mVAE and eVAE models on MNIST,
the maximum over epitomes of size K = 3 and K = 4 is used, and on TFD epitomes of size K = 5 are used.
All epitomes are non-overlapping. Across each row shows performance as the number of encoder and decoder
layers L increases for a fixed number of hidden units H in each layer, and as H increases. Number of active
units are indicated in parentheses.
4.4	Comparison with other models
In Table 2 we compare the generative performance of eVAE with other models, using Parzen log-density.
VAE- , mVAE- , and eVAE- refer to models trained using the same architecture as Adversarial Autoencoders,
for comparison. Encoders and decoders have L = 2 layers of H = 1000 deterministic units. D = 8
for MNIST, and D = 15 for TFD. VAE, mVAE, and eVAE refer to the best performing models over all
architectures from Table 1. For MNIST, the VAE model is (L, H, D) = (3, 500, 8), mVAE is (3, 1000, 24),
and eVAE is (3, 500, 48). For TFD, the VAE model is (3, 500, 15), mVAE is (3, 1000, 50), and eVAE is
(3, 500, 25).
We observe that eVAE significantly improves over VAE and is competitive with several state-of-the-art models,
notably Adversarial Autoencoders. Samples from eVAE on MNIST and TFD are shown in Fig. 7.
9
Under review as a conference paper at ICLR 2017
Method	MNIST(10K)	TFD(10K)
DBN	138 ± 2	1909 ± 66
Deep CAE	121±1	2110 ± 50
Deep GSN	214±1	1890 ± 29
GAN	225 ± 2	2057 ± 26
GMMN + AE	282 ± 2	2204 ± 20
Adversarial AE	340 ± 2	2252 ± 16
VAE-	290 ± 2	2149 ± 23
mVAE-	333 ± 2	2298 ± 23
eVAE-	331 ± 2	2278 ± 26
VAE	325 ± 2	2180 ± 20
mVAE	338 ± 2	2358 ± 20
eVAE	337 ± 2	2371 ± 20
Table 2: Parzen log-densities in nats on MNIST and TFD. VAE-, mVAE-, and eVAE- refer to models trained
using the same architecture as Adversarial Autoencoders, for comparison. VAE, mVAE, and eVAE refer to the
best performing models over all architectures from Table 1.
5/6 / & 7∕∕%σ~
6330 ¥
Zgo4q 67173
86G34/39QO
£4，〃69 j-ʃ〃。
G7q7。Sl57,
7「夕GG Odr%d)o
g0 52 SFAO 767
、□q73l8GH2
Figure 7:	eVAE samples for MNIST (left) and TFD (right).
5	Related Work
A number of applications use variational autoencoders as a building block. In Gregor et al. (2015), a generative
model for images is proposed in which the generator of the VAE is an attention-based recurrent model that is
conditioned on the canvas drawn so far. Eslami et al. (2016) proposes a VAE-based recurrent generative model
that describes images as formed by sequentially choosing an object to draw and adding it to a canvas that is
updated over time. In Kulkarni et al. (2015), VAEs are used for rendering 3D objects. Conditional variants of
VAE are also used for attribute specific image generation (Yan et al., 2015) and future frame synthesis (Xue
et al., 2016). All these applications suffer from the problem of model over-pruning and hence have adopted
strategies that takes away the clean mathematical formulation of VAE. We have discussed these in § 2.1.
A complementary approach to the problem of model pruning in VAE was proposed in Burda et al. (2015); the
idea is to improve the variational bound by using multiple weighted posterior samples. Epitomic VAE provides
improved latent capacity even when only single sample is drawn from the posterior.
10
Under review as a conference paper at ICLR 2017
Methods to increase the flexibility of posterior inference are proposed in (Salimans et al., 2015; Rezende &
Mohamed, 2016; Kingma et al., 2016). In Rezende & Mohamed (2016), posterior approximation is constructed
by transforming a simple initial density into a complex one with a sequence of invertible transformations.
In a similar vein, Kingma et al. (2016) augments the flexibility of the posterior through autoregression over
projections of stochastic latent variables. However, the problem of over pruning still persists: for instance,
Kingma et al. (2016) enforces a minimum information constraint to ensure that all units are used.
Related is the research in unsupervised sparse overcomplete representations, especially with group sparsity
constraints c.f. (Gregor et al., 2011; Jenatton et al., 2011). In the epitomic VAE, we have similar motivations
that enable learning better generative models of data.
6	Conclusion
This paper introduces Epitomic VAE, an extension of variational autoencoders, to address the problem of model
over-pruning, which has limited the generation capability of VAEs in high-dimensional spaces. Based on the
intuition that subconcepts can be modeled with fewer dimensions than the full latent space, epitomic VAE
models the latent space as multiple shared subspaces that have learned specializations. We show how this model
addresses the model over-pruning problem in a principled manner, and present qualitative and quantitative
analysis of how eVAE enables increased utilization of the model capacity to model greater data variability.
We believe that modeling the latent space as multiple structured subspaces is a promising direction of work,
and allows for increased effective capacity that has potential to be combined with methods for increasing the
flexibility of posterior inference.
7	Acknowledgments
We thank the reviewers for constructive comments. Thanks to helpful discussions with Marc’Aurelio Ranzato,
Joost van Amersfoort and Ross Girshick. We also borrowed the term ‘epitome’ from an earlier work of Jojic
et al. (2003).
References
S.	R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R Jozefowicz, and Bengio. S. Generating sentences from a
continuous space. arXiv preprint arXiv:1511.06349, 2015.
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. ICLR, 2015.
S.	M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, Koray Kavukcuoglu, and Geoffrey E. Hinton.
Attend, infer, repeat: Fast scene understanding with generative models. CoRR, abs/1603.08575, 2016.
Karol Gregor, Arthur Szlam, and Yann LeCun. Structured sparse coding via lateral inhibition. In Proceedings
of the 24th International Conference on Neural Information Processing Systems, 2011.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A recurrent
neural network for image generation. arXiv preprint arXiv:1502.046239, 2015.
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for hierarchical sparse coding. Journal of
Machine Learning Research, 12, 2011.
Nebojsa Jojic, Brendan J. Frey, and Anitha Kannan. Epitomic analysis of appearance and shape. In Proceedings
of International Conference on Computer Vision, 2003.
C. Kaae Sonderby, T. Raiko, L. Maale, S. Kaae Snderby, and O. Winther. How to train deep variational
autoencoders and probabilistic ladder networks. arXiv preprint arXiv:1602.02282, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse autoregres-
sive flow. arXiv preprint arXiv:1606.04934, 2016.
D.P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.
T.D. Kulkarni, W. Whitney, P. Kohli, and J.B Tenenbaum. Deep convolutional inverse graphics network. NIPS,
2015.
Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits, 1998.
11
Under review as a conference paper at ICLR 2017
D.J.C. Mackay. Local minima, symmetry-breaking, and model pruning in variational free energy minimization.
2001.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv preprint
arXiv:1505.05770, 2016.
Salah Rifai, Yoshua Bengio, Yann Dauphin, and Pascal Vincent. A generative process for sampling contractive
auto-encoders. arXiv preprint arXiv:1206.6434, 2012.
T.	Salimans, D.P. Kingma, and M. Welling. Markov chain monte carlo and variational inference: Bridging the
gap. ICML, 2015.
Josh M Susskind, Adam K Anderson, and Geoffrey E Hinton. The toronto face database. Department of
Computer Science, University of Toronto, Toronto, ON, Canada, Tech. Rep, 3, 2010.
Tianfan Xue, Jiajun Wu, Katherine L. Bouman, and William T. Freeman. Visual dynamics: Probabilistic future
frame synthesis via cross convolutional networks. arXiv preprint arXiv:1607.02586, 2016.
Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image generation
from visual attributes. CoRR, abs/1512.00570, 2015.
12
Under review as a conference paper at ICLR 2017
8	Appendix
8.1	EFFECT OF KL WEIGHT λ ON RECONSTRUCTION
We visualize VAE reconstructions as the KL term weight λ is tuned down to keep latent units active. The top
half of each figure are the original digits, and the bottom half are the corresponding reconstructions. While
reconstruction performance is good, generation is poor (Fig. 1). This illustrates that VAE learns to model well
only regions of the posterior manifold near training samples, instead of generalizing to model well the full
posterior manifold.
/7义 G*b 7 少A∙qb
夕/QC3名勿/ QB幺
∖9JF3∖9 3 F3
⅜ q<s!g' q2fb>
2
3HIO73UT1O7O.
7g皆〃3718。6 =
7 q 犷，K 7 q ⅛j 7 g
Nw;4aNS——。
rsfo#r36Oy
∕½⅛3S5S3 g3≤β5
夕737/1732/
学7专gJ夕766a
*+C3*3 143g21
〃 qJf&z^，3 y45
i>gF⅛τoq∙λ70.<
g号Ns g"a =
3qe>oo3qo0o Λ
* qq7γo⅜ QH77
后 Λσq5ζ>5 Bf 5。
273夕步274夕肾
Z 76/32 76/ 3
夕^3三^,已3三&
7 234λ9z34λ
G5彳√以 0 32√u O
3∕O0⅛3∕0OG L
⅛>"E"=
7 3 t S / 7s I S /
Ol352cl 36λ
J 71t>5x/ 7 5SX
Figure 8:	Reconstructions for a 50-d VAE with KL weight λ = 1, 0.5, and 0.2. The top half of each figure
are the original digits, and the bottom half are the corresponding reconstructions.
13
Under review as a conference paper at ICLR 2017
8.2 Effect of increasing latent dimension on reconstruction
In § 4.1, Fig. 5 shows the effect of increasing latent dimension on generation for VAE, Dropout VAE, and eVAE
models. Here we show the effect of the same factor on reconstruction quality for the models. The top half of
each figure are the original digits, and the bottom half are the corresponding reconstructions. As the dimension
of the latent variable z increases from 2-d to 20-d, reconstruction becomes very sharp (the best model), but
generation degrades (Fig. 5). Dropout VAE has poorer reconstruction but still blurred generation, while eVAE
is able to achieve both good reconstruction and generation.
7,533 78 5 13
3夕66339663
弓/73/7/78/
03，。0。3，。0
d;p， 、4
b5b 已 656C6
V1yg-25z∖Jg2
W2tτq42tτq
M534i3 15 5。3
s∕√^□" Zb/√uπ?
Λ7 y∙n 夕 7 flr ? I ^/ 7*
770food 7
名 S7∙7v>gs 77S
^qm!1yq3l-≡^
τdj
匕/,7∕cm
7tf^G*7*77J 8 7 4/
∕⅛>⅛tG 4/5/G9
6 0 73 0 73*7
Γ<s08∕r< Sooo/
3々2 又 43-222 q
2vΛ-1r7 2 W∙Λ 77
WG ð 夕6rf SD 6 , Q
Z-。∙74G 2 0fc76 G
d * J , 7/ 7
7aq7
2ΛΓ6Q∕⅛~0∙qg∕β~
>∙5 024 y 60，”
J <Γ5i2N q£l 夕
Q / !。?'G /Iuo/
。，广3 7。/OoSy
4B/6 3。3，6,
/ ð 3 Pv 5 / O 夕 83
2 HA 0/2 0T4 0/
d 3 q ? 丫山
/ t 3
451mgo Γ-l OrG
T 夕∙s A 6 T 9∙ 3 Δ 6
/ ./ 9∙zo CJ/ /766
XSG6σΛ Oo 3G 4 Oo
HVA
%; NlO337233
44rlqH45z%
56 7f-3^s‰s Γ∙*9q
*r60smΓG0fr
b*□∖slHuc04l¼
OoVqʃo CeqM
r6c√Q5^65yG
Uq7。^Mq7oa
dɑʃ*74 夕 5 u∙ T
8¾0∕3^8,∕3ci
Gq %、夕u□33 夕
xqAqoNq&qe
sλ^o∕λλ3θs∕λ
3 /QiGrt 4 *3 LG
0 U '04bG=e4G
。。/024。，58
⅜≠2 ΠJ∙5 3 a X 9 83
H6LS8M41SS
区袅？4 73a94o⅛
oo6t?OO0 1 "
=3 = CnfSCnCnaB 夕
* NO7Mg2095
/^27∕∕gz9,
∕1÷ ftτrv∕⅛ / 9 Ql G Zto
J∙73rb 夕夕 5yG>
4ggs3,8B28
21Ho2a9qoy
7 9 ∣>专 q7g⅜g∙
g∙√lJ86qg 9
29 6-2 724 93夕
3歹。。7399A夕
5532 ^∙3 3 3 37
5NI∕S333 9g
⅛.*3QriSg7<13
q4∕q2g3ʃ 9-B
25 7 3 7 a 373 9
G 7 7。e37V 3 *3
^QooZOJ393G
£VIQ *>0-33279
2fkl∕7∙77r夕
Mo Wɔʤ
r0√∕350√4 3
CΛS3 QeA4ωq
√ W 夕白ʃUMv6r
(^>ZΓβlooo∕6
—?，<β ⅞u I 7 S S >3
7HJ bozuɪoo
/3。3。/3030
rf 另 gi⅛,gl8gf
7 彳 /676 夕/5
5∙⅛8ym5 TT Oo Ofl 3
ΛOQ∙7q 2 0。73
SqʃQl/ 9/
n7%qo ŋ 7 y 30
ð/ ʒ It 口 if 9« OD H ə Ii
^≠-∙∖∂o∕J4∕8/
。54 3，0 B 4 8 9
&5830G5B30
25 I <9*25-J 8
7由./3另？6/S ,
10 ʃ(^ɔlo ʃl^
f 办 Y4 Gf6g6 O
%吩0goIq
/F9.yΓ6∕F /？ 1
/6356 / 6S 36
kj^83*G653F
O3g¥b Cfog4
l%Hd∕l4 M /
7∕flG-37∕ / 013
c>355v⅛。夕 38J
¥。74 斤夕。7uq
QjΛqord，aVq
7H2 夕a
o/y4 / O / y/to /
2H6 ?，2Q-6 7,
MηwZ点 trnoFo
22Sg/393q/
ʃ 匕£5R，6夕3。
/^rw7c∙F7^∙τ∙
NgfWi2,R Oa tj - 2
ωVΛ 。
Figure 9:	Reconstructions from VAE, Dropout VAE, and eVAE models for different dimensions of latent
variable z. Across each row are 2-d, 5-d, 10-d, and 20-d models. The top half of each figure are the original
digits, and the bottom half are the corresponding reconstructions. The eVAE models multiple shared subspaces
by maintaining 2-d (overlapping) epitomes as the latent dimension is increased. eVAE is the only model that
achieves both good reconstruction and generation.
14
Under review as a conference paper at ICLR 2017
8.3 Evaluation metric for generation
There have been multiple approaches for evaluation of variational autoencoders, in particular log-likelihood
lower bound and log-density (using the Parzen window estimator, Rifai et al. (2012)). Here we show that for
the generation task, log-density is a more appropriate measure than log-likelihood lower bound. Models are
trained on binarized MNIST, to be consistent with literature reporting likelihood bounds. The encoder and
decoder for all models consist of a single deterministic layer with 500 units.
Table 3 shows the log-likelihood bound and log-density for VAE and eVAE models as the dimension D of latent
variable z is increased. For VAE, as D increases, the likelihood bound improves, but the log-density decreases.
Referring to the corresponding generation samples in Fig. 11, we see that sample quality in fact decreases,
counter to the likelihood bound but consistent with log-density. The reported VAE bounds and sample quality
also matches Figs. 2 and 5 in Kingma & Welling (2014). On the other hand, eVAE log-density first decreases
and then improves with larger D. We see that this is also consistent with Fig. 11, where eVAE samples for
D = 8 are the most interpretable overall, and D = 48 improves over D = 24 but still has some degenerate
or washed out digits. (Note that these models are consistent with Kingma & Welling (2014) but are not the
best-performing models reported in our experiments.) Since our work is motivated by the generation task, we
therefore use log-density as the evaluation metric in our experiments.
Intuitively, the reason why VAE improves the likelihood bound but generation quality still decreases can be seen
in the breakdown of the bound into the reconstruction and KL terms (Table 3 and Fig. 10). The improvement
of the bound is due to large improvement in reconstruction, but the KL becomes significantly worse. This
has a negative effect on generation, since the KL term is closely related to generation. On the other hand,
eVAE reconstruction improves to a lesser extent, but the KL is also not as strongly affected, so generation
ability remains stronger overall. As a result of this, simply tuning the KL weight λ in the training objective is
insufficient to improve VAE generation, as shown in Fig. 1 in the main paper.
	Rec. term			KLD term	Likelihood bound	Log-density
	D=	二 8	-89.4	-16.6	-106.0	278
VAE	D=	二 24	-61.1	-29.3	-90.4	152
	D=	二 48	-59.1	-30.3	-89.4	151
	D=	二 8	-110.1	-9.6	-119.7	298
eVAE	D=	二 24	-84.2	-15.7	-99.9	274
	D=	二 48	-82.8	-14.2	-97.0	284
Table 3: Likelihood bound and log-density for VAE and eVAE as dimension D of latent variable z is increased.
The encoder and decoder for all models consist of a single deterministic layer with 500 units. eVAE models
have epitomes of size K = 4 for D = 8, and K = 8 for D = 24 and D = 48. The breakdown of the
likelihood bound into reconstruction term and KLD term is also shown.
Figure 10: Likelihood bound for VAE and eVAE as D increases (shown as NLL). VAE improvement of the
bound is due to significant reduction of reconstruction error, but at high cost of KL, which is closely related
to generation. eVAE improves reconstruction more moderately, but also maintains lower KL, and has stronger
generation overall.
15
Under review as a conference paper at ICLR 2017
夕rκzd*agd∙ 〃
s∕÷⅛y26Jq∙s
*CIt⅛*0wc∕^3g1
932，2s/*/•// 3
≡τς⅛√; 5，3a2 2
jr⅞3 9，3 S e 24
2z</ Cs，QgG L：W
石4 3，G 2 6夕52
9 t .4 4京 GvB√q
a。7 C , q ∖Λ⅛6 Q
僮弓 05Z 4 ? q7G
√yl> g G 彳弓 q w。
Nd* 匕 397<74 丁
2 /r 6 4 产 i 弓 3 g ?
J7O9.2 " / 4 WR
l>3 3 q∕G16「J
? Cv3，SΓ⅜1q,3
y7z-r & 夕 FJ 3 J
* 夕—ɔra F ∖nτa
∙c4;1- q Z 3 N Fyy
2 Q 3 24γ g " Cr 9
，4>ʃ ∕≤7∕G2fs-
。日£ 夕。3?5-0,
t7…>Λ> ? "，5 ¥
5 7 967 30，，
r'* 1 </ O ¼ Tt / Ow / ?
，/ 7。，。6675
Y-:;?0z 364 7 C⅛
S 1 / 9 50 αΛss
?* 3 6 /* 5 5 ⅛ ∙‰9
VAE (D = 48)
4 T Or?Od∕±4
15G 夕夕；2003:
夕S分 g G /
S33g∕3 / ʌr S
ɪ Λ* Λ/ H Q 2 , j 3 G 7
/;g 7夕 bB43Γ∙
93 5 3 OoeXG 5 3LQ
Γ*ffoJ夕 πi 7 /
0-57，c>,AJd /
y∕∕3* 7‹7∕4 O
⅛t⅛√，彳733。"
夕夕♦子 >Γ6 7λ-j3
7r9∖zqlσ 夕9 4
£ 6 2 6 3 V彳。半夕
eVAE (D
3 7 S 7 7 /f ə C-7X
£。夕“，5dfqz
⅞ 7s ZGu62zλ.
9∕√c7822i"
3dQqcΓ3 夕F3。
04×0 9q261v6
5"qp∙l∕B9b■夕
T “‰夕 4607
J c,qr‰4Q3o q
z⅛ 73fτ‰57v7√l
eVAE (D = 8)
Figure 11: Generation samples for VAE and eVAE as dimension D of latent variable z is increased. VAE
sample quality decreases, which is consistent with log-density but not likelihood bound.
16