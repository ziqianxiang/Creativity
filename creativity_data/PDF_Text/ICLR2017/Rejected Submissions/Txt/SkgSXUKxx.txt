Under review as a conference paper at ICLR 2017
An Analysis of Feature Regularization for
Low-shot Learning
Zhuoyuan Chen, Xiao Liu & Wei Xu
Baidu Research
Sunnyvale, CA 94089, USA
{chenzhuoyuan,liuxiao,wei.xu}@baidu.com
Han Zhao
Department of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
han.zhao@cs.cmu.edu
Ab stract
Low-shot visual learning, the ability to recognize novel object categories from
very few, or even one example, is a hallmark of human visual intelligence. Though
successful on many tasks, deep learning approaches tends to be notoriously data-
hungry. Recently, feature penalty regularization has been proved effective on cap-
turing new concepts. In this work, we provide both empirical evidence and the-
oretical analysis on how and why these methods work. We also propose a better
design of cost function with improved performance. Close scrutiny reveals the
centering effect of feature representation, as well as the intrinsic connection with
batch normalization. Extensive experiments on synthetic datasets, the one-shot
learning benchmark “Omniglot”, and large-scale ImageNet validate our analysis.
1	Introduction
The current success of deep learning hinges on the ability to apply gradient-based optimization to
high-capacity models. It has achieved impressive results on many large-scale supervised tasks such
as image classification Krizhevsky et al. (2012); He et al. (2016) and speech recognition Yu & Deng
(2012). Notably, these models are extensively hungry for data.
In contrast, human beings have strong ability to learn novel concepts efficiently from very few or
even one example. As pointed out in Lake et al. (2016), human learning is distinguished by its
richness and efficiency. To test whether machines can approach this goal, Lake et al. propose the
invaluable “Omniglot” hand-written character classification benchmark Lake et al. (2011), where
each training class has very few examples and the ability to fast-learn is evaluated on never-seen
classes with only one example.
There has been previous work on attaining rapid learning from sparse data, denoted as meta-learning
or learning-to-learn Thrun. (1998); Baxter. (1998). Although used in numerous senses, the term
generally refers to exploiting meta-knowledge within a single learning system across tasks or algo-
rithms. In theory, a meta-learning is able to identify the right “inductive bias shifts” from previous
experiences given enough data and many tasks Baxter. (1998). However, even if a well-designed
convolutional neural network is a good “inductive bias shift” for a visual recognition task, it is still
elusive to find the optimal parameter from a small training set without any prior knowledge.
To alleviate this issue, low-shot learning methods have been proposed to transfer knowledge from
various priors to avoid over-fitting, such as Bart & Ullman (2005). Recently, Hariharan & Girshick.
(2016) propose a novel prior of gradient penalty, which works pretty well experimentally. Although
an intuitive explanation is provided in Hariharan & Girshick. (2016) that a good solution of the
network parameters should be stable with small gradients, it is mysterious why adding such a regu-
larization magically improves the low-shot task by a large margin. Mathematical derivation shows
that gradient penalty is closely related to regularizing the feature representation.
In this paper, we give more analysis, both empirically and theoretically, on why adding a gradient
regularization, or feature penalty performs so well. Moreover, we carefully carry out two case
studies: (1) the simplest non-linear-separable XOR classification, and (2) a two-layer linear network
for regression. The study does give insight on how the penalty centers feature representations and
1
Under review as a conference paper at ICLR 2017
make the learning task easier. Furthermore, we also theoretically show that adding another final-
layer weight penalty is necessary to achieve better performance. To be added, close scrutiny reveals
its inherent connection with batch normalization Ioffe & Szegedy (2015). From a Bayesian point
of view, feature penalty essentially introduces a Gaussian prior which softly normalizes the feature
representation and eases the following learning task.
1.1	Related Work
There is a huge body of literature on one-shot learning and it is beyond the scope of this paper to
review the entire literature. We only discuss papers that introduce prior knowledge to adjust the
neural network learning process, as that is the main focus of this work.
Prior knowledge, or “inductive bias” Thrun. (1998), plays an important role in one-shot or low-shot
learning. To reduce over-fitting problems, we need to regularize the learning process. Common
techniques include weight regularization Bishop (1995). Recently in Hariharan & Girshick. (2016),
a gradient penalty is introduced, which works well experimentally in low-shot scenarios.
Various forms of feature regularization have been proposed to improve generalization: Dropout Sri-
vastava et al. (2014) is effective to reduce over-fitting, but has been eschewed by recent architectures
such as batch-normalization Ioffe & Szegedy (2015) and ResNets He et al. (2016). Other forms have
also been proposed to improve transfer learning performance, such as minimizing the correlation of
features Cogswell et al. (2016) and the multiverse loss Littwin & Wolf (2015) .
Our work is also closely related to metric learning and nearest neighbor methods, in which repre-
sentations from previous experience are applied in cross-domain settings Fink (2005); Koch et al.
(2015); Goldberger et al. (2005); Chopra et al. (2005). The insight lies in that a well-trained rep-
resentational model have strong ability to generalize well on new tasks. In a recent work Santoro
et al. (2016), DeepMind proposed a Memory Augmented Neural Network (MANN) to leverage the
Neural-Turing-Machine for one-shot tasks. However, in Vinyals et al. (2016), it is found that a good
initialization such as VGG-Net largely improves the one-shot performance. In our opinion, a good
feature representations still play a central role in low-shot tasks.
1.2	Contributions
The main contributions of our comprehensive analysis are three-fold:
1.	We carefully carry out two case studies on the influence of feature regularization on shallow
neural networks. We observe how the regularization centers features and eases the learning
problem. Moreover, we propose a better design to avoid degenerate solutions.
2.	From Bayesian point of view, close scrutiny reveals internal connections between feature
regularization and batch normalization.
3.	Extensive experiments on synthetic, the “Omniglot” one-shot and the large-scale ImageNet
datasets validate our analysis.
2	An Analysis of Feature Regularization
We briefly introduce the notations in our work: we denote uppercase A, bold a and lowercase a for
matrices, vectors and scalars respectively. For a vector ai, we denotes ai,j as its j-th element. ||.||F
stands for the Frobenius norm of a matrix. Given N examples {(xi, yi)|i = 1, ..., N}, we define
E{.} as an expectation taken with respect to the empirical distribution generated by the training set.
Following Hariharan & Girshick. (2016), we aim to learn a neural network model to extract the
feature representations φ(xi) and make predictions r^i = Wφ(xi) with W = [wι,…，w∣c∣]. This
setting includes both classification and regression problems, with |C | as the number of classes or
target dimension, respectively. The problem can be generally formulated as:
W *,φ* = arg min E{l(W, φ(xi),yi)}	(1)
W,φ
where l(.) can be any reasonable cost function. In this paper, we focus on cross-entropy and L2 loss
due to their convexity and universality.
2
Under review as a conference paper at ICLR 2017
In Hariharan & Girshick. (2016), it is suggested that adding a squared gradient magnitude loss
(SGM) on every sample can regularize the learning process.
W*,Φ* = argminE{l(W, Φ(xi), yi) + λ∣∣Vwl(W, φ(xi, yi)川2}	(2)
W,φ
The insight is that for a good solution, the parameter gradient should be small at convergence. How-
ever, we know that the convergence of a neural network optimization is a dynamic equilibrium. In
other words, at a stationary point, we should have E{VW l(W, φ(x))} → 0. Intuitively when close
to convergence, about half of the data-cases recommend to update a parameter to move positive,
while the other half recommend to move negative. It is not very clear why small gradients on every
sample E{∣∣Vwl(W, Φ(x))∣∣2} produces good generalization experimentally.
Mathematical derivation shows that the optimization problem with gradient penalty is equivalent
with adding a weighted L2 regularizer φ(xi):
argminE{l(W, φ(xi),yi) + λαi∣∣φ(xi)∣∣2}	(3)
W,φ
where the example-dependent αi measures the deviation between the prediction yi and the target
yi. In a regression problem, We have ɑi = r2 = ||yi 一 yi||2, with the residual r = yi 一 yi;
in a classification problem, we have αi = Pk(pik 一 I(yi = k))2. Intuitively, the misclassified
high-norm examples might be outliers, and in a low-shot learning scenario, such outliers can pull
the learned weight vectors far away from the right solution. In Hariharan & Girshick. (2016), the
authors compare dropping αi and directly penalizing ∣∣φ(χi)∣∣2, which performs almost equally well.
In our work, we argue that a better and more reasonable design should be:
图带？％”。(XiM+λ1αiMxi )H2} CM ||F
(4)
where we add another weight regularizer ||W ||2F , which is necessary to avoid degenerate solutions.
We will give further explanation in our second case study. In following analysis, we denote the cost
in Eqn(4) with example-dependent αi as weighted L2 feature penalty, and the example-independent
(setting αi ≡ 1) as uniform L2 feature penalty.
We carry out two case studies: (1) an XOR classification and (2) a regression problem, both em-
pirically and theoretically to analyze how the uniform and weighted L2 feature penalty regularize
the neural network. In our paper, we will focus on the uniform feature regularization and will also
cover the weighted scenario as well.
2.1	Case Study 1: an Empirical Analysis of XOR Classification
First, we study the simplest Iinear-non-separable problem- exclusive-or (XOR). Suppose that we
have four two-dimensional input points x = [x1, x2]T ∈ R2: {x1 = [1, 1]T, x2 = [0, 0]T} ∈ C+
belongs to the positive class, while {x3 = [1, 0]T, x4 = [0, 1]T} ∈ C- the negative.
As shown in Figure 1, we use a three-layer neural network to address the problem (left figure):
h1 = x + b is a translation with b = [b1, b2]T ∈ R2 as the offset; h2 = h1,1 * h1,2 is a non-linear
layer, multiplying the first and second dimension ofh1 and producing a scalar; y is a linear classifier
on h2 parametrized by w . The original classification problem can be formulated as:
4
arg min〉log(1 +exp(-yi * W * h2))
w1 ,b,w
i=1
Suppose that we start from an initialization b = [0, 0]T, all three samples {x2, x3, x4} from different
classes will produce the same representation h2 = 0, which is not separable at all. It takes efforts to
tune the learning rate to back-propagate w, b updates from the target y.
However, if we introduce the uniform L2 feature regularization as:
4λ
arg min Elog(I + exp(-yiwh2)) + ʒɪ|陶||2
w1 ,b,w	2
i=1
3
Under review as a conference paper at ICLR 2017
Then, we have:
Figure 1: Case 1: an empirical study of the XOR classification task. The left figure: the network
structure we use: h1 is a linear transformation, h2 is a non-linear transform of h1 and y is the
prediction; The right column: The linear transformation maps x to h1. As shown in the red arrow,
an L2 norm penalty on h2 centers the feature of h1 and make the points from different sets separable.
’X’s refer to positive examples, and ’O’s are negative ones.
Figure 2: Case 2: a comprehensive study of a two-layer linear neural network for regression task.
We minimize the L2 distance between the prediction y = W2 (W1X + b1) + b2 and y. The latent
representation h = W1x + b1 is a linear mapping.
λι ∂||h21|2 =λ (E{h2(x2 + b2)}
^2	∂ b = 1 * * VE{hι (xι + bl)}
(5)
the gradient descent pulls Eqn(5) towards zero, i.e., pulling b towards b1 = -E{x1} = -0.5 and
b2 = -E{x2} = -0.5.
As shown on the right of Figure 1, the gradient of feature regularization pulls h1 along the direction
of red arrows. Then, we have h2 > 0 for positive examples and h2 < 0 for negative ones, which
means h2 is linearly-separable. In summary, we can observe that:
Empirically, the feature regularization centers the representation h2 = φ(X) and makes the
following classification more learnable.
For the weighted case, the offset b have similar effects. It can be derived that when converged the
feature representation will satisfy E{h1} = 0 and E{h2}=0.
2.2	Case Study 2: a Comprehensive Analysis on a Regression Problem
Next, we analyze a two-layer linear neural network as shown in Figure 2. Denoting the input as
X = [X1, X2, ...] and the target as Y = [y1, y2, ...]. The regression loss can be formulated as:
E{||y-W2W1x||2}
where the latent feature is h = φ(X) = W1X. The optimization of {W1, W2} in this multi-layer
linear neural network is not trivial, since it satisfies following properties:
1.
2.
3.
The regression loss is non-convex and non-concave. It is convex on W1 (or W2) when the
other parameter W2 (or W1 ) is fixed, but not convex on both simultaneously;
Every local minimum is a global minimum;
Every critical point that is not a global minimum is a saddle point;
4
Under review as a conference paper at ICLR 2017
4. If W2 * W1 is full-rank, the Hessian at any saddle point has at least one negative eigenvalue.
We refer interested readers to Baldi & Hornik (1989); Kawaguchi (2016) for detailed analysis.
In case of the uniform L2 feature penalty, the problem becomes:
E(Wι, W2) = E{ 1 ||y - W2Wιχ∣∣2 + λ11 IWιχ∣l2} + y I∣W2I∣F
At the global minimum {Wj, WS}, we should have:
∂E
∂w |w： = W2 ςXy - W2 w2wiςXx + λ1W1∑xx = 0
∂E
Wm W = ςxjWI - W2WIςxxwi + λ2W2 = 0
∂ W2	2
(6)
(7)
(8)
where we define the variance and covariance matrix as ΣXX = E{xxT}, ΣXy = E{yxT}. Carrying
out Eqn(7) * W1T - W2T * Eqn(8) = 0 reveals a very interesting conclusion:
λ1E{llW1xll2} = λ2llW2IIF
(9)
This reads as the expected L2 feature penalty should be equal to final-layer weight regularizer
when converged. Or equivalently, when close to convergence, the L2 feature penalty reduces over-
fitting by implicitly penalizing the corresponding weight matrix W . A more generalized form is:
Lemma 1 For a cost function of form in Eqn (4) with uniform L2 feature regularization:
遇带？时(W,φ(Xi),yi)+IM(Xi))112} CM ||F
we have:
λ1E{∣∣φ(x)∣∣2} = λ2∣∣W ||F
(10)
The φ(.) can take a quite general form of a convolutional neural netWork With many common non-
linear operations such as the ReLU, max-pooling and so on. One can folloW the derivation of Eqn(9)
to easily derive Lemma 1.
Lemma 1 also reveals the importance of adding the Weight penalty ||W ||2F in Eqn(4). If We only
include the the feature penalty and drop the Weight penalty (λ2 = 0 in our case), then a scaling
as φ(.) = γφ(.) and W = YW with γ < 1 Will alWays decrease the energy and the solution Will
become very ill-conditioned with γ → 0.
2.2.1	L2 FEATURE REGULARIZATION MAKES OPTIMIZATION EASIER
Moreover, we analyze numerically how the L2 feature penalty influences the optimization process
in our regression problem.
We study a special case {x ∈ Rd, y ∈ R} with W1 ∈ R1×m, W2 ∈ R and include offsets b1 ∈ R and
b2 ∈ R to make the problem more general. Then, the latent representation becomes h = W1 x + b1
and the prediction is y = W2h + b2. The cost function of Eqn(4) becomes:
E(Wι,bι,W2,b2) = JE{(W2h + b2 - y)2 +	r2h2} + λ22W22
We define the prediction residual r and r∣ as y 一 y for better readability, and substitute αi
(11)
(4)2
Numerically, We apply a two-step process: in the first step, We calculate the sample-dependent ri in
the feed-forward process to obtain our L2 feature regularization weights αi for each (xi, yi); in the
second step, we treat r↑ as a constant in the optimize. The gradient and Hessian matrix of Eqn(11)
can be derived as:
∂E
∂W2
∂E
∂W
E{rhT} +λ2W2
W2E{rxτ } + XiE{r；hxT }
∂E
福
∂E
∂b1
E{r}
W2E{r} + λιE{r2h}
∂E = Wri + λ"i )2 hi
5
Under review as a conference paper at ICLR 2017
and
∂WE = E{hhT}+ λ2	d∂EE = 1	⅛Eρ = W2 + λ1(ri )2
∂ 2E	∂ 2E
dW2 = (W22 + λιE{r2})E{xxT}	=W22 + λιE{r2}
∂ W1	∂b1
Suppose that we apply a second-order optimization algorithm for the network parameter θ ∈
{w1,w2,b1,b2}, the updates should be M θ = -η * (∂2E∕∂θ2)-1 (∂E∕∂θ) with η > 0 as the
step-size. If we unluckily start from a bad initialization point W2 → 0, the updates of hi, W1 and b1
are of the form:
M hi = -η(W22 + λι(ri)2})T(W2ri + λι(ri)2hi)
M wι = -η[(W22 + λιE{r2})E{xxτ}]-1
(W2E{r} + λ1E{r2h})E{x}
M b1 = -η(W22 + λ1E{r2})T(W2E{r} + λ1E{r2h})
The updates will become very ill-conditioned without the regularization term (λ1 = 0), since spec-
trum of the Hessian matrix is two-orders of infinitesimal O(W22) and the gradient is of one-order
O(W2). In comparison, with a reasonable choice of λ1 > 0, the computation can be largely stabi-
lized when E{r；} = 0.
When the algorithm finally converges to a local minimum {Wj, b；,W$,好}, the expectation of
parameter and latent feature should have gradient close to 0:
∂E
∂W^ = E{rhT} + λ2W2 → 0
∂E
福=E{r}→0
Substituting this in the analysis of b1, we have:
∂E
=W2E{r} + λιE{αh} =⇒ E{ah} → 0
∂ b1
(12)
In other words, the feature penalty centralizes the final hidden layer representation h(x) = φ(x).
Especially, in the uniform L2-feature penalty case, we simply drop α in Eqn (12) and have E{h} =
E{φ(x)} → 0.
In summary, the feature penalty improves the numerical stability of the optimization process in the
regression problem. The conclusion also holds for the classification.
Similar results for φ(x) can be extended to deeper multilayer perceptrons with convex differentiable
non-linear activation functions such as ReLU and max-pooling. In an m-layer model parametrized
by {W1, W2, ..., Wm}, the Hessian matrix of hidden layers becomes strongly convex by back-
propagating from the regularizer ∣∣σm-1(Wm-1 * σm-2(W2 * (…* σι(Wιx))))∣∣2.
2.3	UNIFORM L2 FEATURE PENALTY IS A SOFT BATCH NORMALIZATION
In our two case studies, we can observe that L2 feature penalty centers the representation φ(x),
which reminds us of the batch normalization Ioffe & Szegedy (2015) with similar whitening effects.
We reveal here that the two methods are indeed closely related in spirit.
From the Bayesian point view, we analyze a binary classification problem: the probability of pre-
diction y given X observes a Bernoulli distribution p(y∣x, φ, W) = Ber(y∣sigm(Wφ(x)), where
φ(x) ∈ Rd is a neural network parametrized by φ and W ∈ R1×d. Assuming a factorized Gaussian
prior on φ(x)〜 N(0, Diag(σ2)) and W 〜 N(0, Diag(σ2)), We have the posterior as:
p(Φ,W ∣{(xi, yi)}) = p({(xi, yi)}∣Φ,W )p(φ)p(W)
π	1	i 1 Ii exp(-≡⅛>2 )exp(-|WF)	(13)
X (________)______)y (	)i-y _______2σ____________2σ2
i 1 + e-Wφ(xi)	1 + ewφ(xi)	(√2∏σ1)d	(√2∏σ2)d
Applying the maximum-a-posteriori (MAP) principle to Eqn(13) leads to the following objective:
E{l(W, φ(xi),yi) + 2⅛ ∣∣Φ(xi)∣∣2} + 2⅛ ||W ||F + C	(14)
2σ1	2σ2
6
Under review as a conference paper at ICLR 2017
with C = —dln(√2πσι) 一 dln(√2∏σ2) is a constant. This is exactly the uniform L2 version of
Equation (4) with λι =表,λ2 =* and ɑi = 1. The i.i.d. Gaussian prior on W and φ(x) has
the effect of whitening the final representation.
The difference between uniform L2 feature penalty and batch normalization is that the former
whitens φ(x) implicitly with an i.i.d. Gaussian prior during training, while the latter explicitly
normalizes the output by keeping moving average and variance. We could say that the uniform L2
penalty is a soft batch normalization.
As analyzed in Wiesler & Ney (2011) this kind of whitening improved the numerical behavior and
makes the optimization converge faster. In summary, the L2 feature regularization on φ(x) indeed
tends to reduce internal covariate shift.
2.4	Feature Regularization May Improve Generalization
As pointed out in Hariharan & Girshick. (2016), the gradient penalty or feature regularization im-
proves the performance of low-shot learning experimentally. Here, we give some preliminary analy-
sis on how and why our modified model in Eqn 4 may improve generalization performance in neural
network learning. Denoting the risk functional (testing error) R(W) as:
11
R(W) = 2E{∣∣y — wφ(χ)ll2} = 2/	)	(	— wφ(χ)ll2dP(χ,y)
and empirical risk function (training error) Remp(W) on {(xi, yi)|i = 1, ..., N} as:
1N
Remp(W ) = 2N J^∖∣yi — Wφ(xi)∣∣2
As we know, the training and testing discrepancy depends on the model complexity Vapnik (1998):
R(W) — Remp(W) ≤ V(W) + o*(h-Nnη)	(15)
where O*(.) denotes order of magnitude UP to logarithmic factor. The upper bound 15 holds true
with probability 1 — η for a chosen 0 < η < 1. In the equation, h is a non-negative integer named
the VC-dimension. The right side of Eqn 15 contains two term: the first one ν(W) is the error on
training set while the second one models complexity of the learning system. In a multilayer neural
network with ρ parameters and non-linear activations, the system has a VC dimension Sontag (1998)
of O(ρ log ρ).
In our model in Eqn 4, the final cost function includes both feature and weight penalty as:
arg min E{l(W, φ(χi), yi) +λ1αi∖∖φ(χi)∖∖2} + λ2∖∖W∖∖2F
W,φ
Empirically, the term λ2 ∖∖W ∖∖2F enforces large margin which limits the selection space of W. The
term λ1 ∖∖φ(xi)∖∖2 not only improves numerical stability by feature whitening as discussed in the
above sections, but also limits the selection of hidden layer parameter. These regularization terms
thus reduces the VC-dimension of our learning. According to Eqn 15, the reduction ofVC dimension
of our model further reduces the theoretical discrepancy of training and testing performance.
3 Experimental Results
We evaluate our algorithm on three datasets: synthetic XOR datasets, the Omniglot low-shot bench-
mark and the large-scale ImageNet dataset. We use our own implementation of SGM approach
Hariharan & Girshick. (2016) for a fair comparison.
3.1	Synthetic Datasets
We first evaluate our model on the XOR dataset. Without loss of generality, we assume the data
points χ = [x1, x2]T are uniformly sampled from a rectangle x1 ∈ [—1, 1], x2 ∈ [—1, 1], and
y(x) = I(χι * X2 > 0). The structure We use is a two-layer non-linear neural network with one
7
Under review as a conference paper at ICLR 2017
Figure 3: Evaluation on the XOR classification task. The red, blue and green lines stand for the
accuracy of the Neural Network without any regularization, only the L2 feature penalty, and our
model with both weight and feature regularizer, respectively.
Model	One-shot Evaluation
Random Guess	5%
Pixel-KNN	26.7%
MANN Santoro et al. (2016)	36.4%
CNN Metric Learning	72.9%
CNN (OUr implementation)	85.0%
Low-shot Hariharan & Girshick. (2016)	89.5%
Matching Network Vinyals et al. (2016)	93.8%
Ours	91.5%	
Table 1: Experimental results of our algorithm on the Omniglot benchmark Lake et al. (2011).
latent layer h = σ(W1x + b) where σ(.) is the rectified linear unit. ADAM Kingma & Ba (2014) is
leveraged to numerically optimize the cross entropy loss.
During training, we make use of only 4 points {[1, 1], [-1, 1], [1, -1], [-1, -1]}, while randomly
sampled points from the whole rectangle as the test set. It is a very low-shot task. As shown in
Figure 3, our model with both feature and weight regularization outperforms the gradient penalty
Hariharan & Girshick. (2016) and no regularization. We use uniform feature penalty as in Eqn(4)
and set λ1 = λ2 = 0.1 in our experiments.
3.2	Low-shot learning on Omniglot
Our second experiment is carried out on the Omniglot one-shot benchmark Lake et al. (2011). Om-
niglot training set contains 964 characters from different alphabets with only 20 examples per each
character. The one-shot evaluation is a pairwise matching task on completely unseen alphabets.
Following Vinyals et al. (2016), we use a simple yet powerful CNN as feature representation model,
consisting of a stack of modules, each of which is a 3 × 3 convolution with 128 filters followed by
batch normalizationIoffe & Szegedy (2015), a ReLU and 2 × 2 max-pooling. We resized all images
to 28 × 28 so that the resulting feature shape satisfies φ(x) ∈ R128. A fully connected layer followed
by a softmax non-linearity is used to define the Baseline Classifier.
We set λ1=1e-4 in SGM Hariharan & Girshick. (2016) and λ1=λ2=1e-4 in our model. A nearest
neighbor approach with L2 distance of feature φ(x) is applied for one-shot evaluation. As shown in
Table 1, we can see that our model with both feature and weight penalty is able to achieve satisfac-
tory performance of one-shot 91.5% accuracy, highly competitive with the state-of-the-art Matching
NetworkVinyals et al. (2016) with CNN warm-start and RNN hyper-parameter tuning.
8
Under review as a conference paper at ICLR 2017
Model
CNN baseline
One-shot Evaluation
40.1%
Low shot Hariharan & Girshick. (2016)
Ours
46.0%
46.6%
Table 2: Experimental results of our algorithm on the ImageNet benchmark Russakovsky et al.
(2015) with the 20-way one-shot setting.
Figure 4: Classification accuracy comparison of our feature penalty (termed “PN”) with batch-
normalization (termed “BN”) Ioffe & Szegedy (2015) on MNIST, CIFAR-10, Omniglot and Im-
ageNet benchmarks. We compare baseline methods with neither batch-normalization nor feature
penalty, with each module added, and with modules included.
3.3	Large-scale Low-shot Learning on ImageNet
Our last experiment is on the ImageNet benchmark Russakovsky et al. (2015). It contains a wide
array of classes with significant intra-class variation. We divide the 1000 categories randomly into
400 base for training and evaluate our feature representation on the 600 novel categories.
We use a 50-layer residual network He et al. (2016) as our baseline. Evaluation is measured by
top-1 accuracy on the 600 test-set in a 20-way setup, i.e., we randomly choose 1 sample from 20 test
classes, and applies a nearest neighbor matching. As shown in Table 2, we can see that our model
learns meaningful representations for unseen novel categories even with large intra-class variance.
3.4	Comparison with Batch-Normalization
As discussed in Section 2.3, feature penalty has similar effects with batch normalization Ioffe &
Szegedy (2015). It is of interest to compare the influence of two modules influence training perfor-
mance of neural networks. We study the performance of the classification with and without each
modules on four classic image classification benchmarks. For CIFAR-10 and ImageNet, we applied
the Residual Net architecture He et al. (2016), while stacked convolution layers with ReLU and
max-pooling is applied for MNIST and Omniglot. For ImageNet benchmark evaluation, we test the
top-1 accuracy on the validation set with 50,000 images.
Since in our model the feature penalty regularizer is applied only on the last hidden layer, we still
keep the batch-normalization modules in previous layers in our “FP” model. As shown in Figure
4, we observe that baseline models with neither “BN” nor “FP” takes much longer to converge
and achieve inferior performance; our “FP” regularizer achieves almost the same performance on
MNIST (both 99%), CIFAR-10 (both 93%) and Omniglot 1-shot (88% BN v.s. 89% FP); on Ima-
geNet, “BN” performs better than our “FP” (75% BN v.s. 74% FP). With both batch-normalization
9
Under review as a conference paper at ICLR 2017
and feature penalty modules added, we achieve the best classification performance on all four bench-
marks.
4	Conclusion
In this work, we conduct an analysis, both empirically and theoretically, on how feature regulariza-
tion influences and improves low-shot learning performance. By exploiting an XOR classification
and two-layer linear regression, we find that the regularization of feature centers the representation,
which in turn makes the learning problem easier with better numerical behavior. From the Bayesian
point of view, the feature regularization is closely related to batch normalization. Evaluation on
synthetic, “Omniglot” one-shot and large-scale ImageNet benchmark validates our analysis.
References
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples
without local minima. Neural networks, 2, 1989.
Evgeniy Bart and Shimon Ullman. Cross-generalization: Learning novel classes from a single example by
feature replacement. CVPR, 2005.
Jonathan Baxter. Theoretical models of learning to learn. Learning to learn. Springer US, pp. 71-94, 1998.
Christopher Bishop. Neural networks for pattern recogsontag, eduardo d. ”vc dimension of neural networks.”
nato asi series f computer and systems sciences 168 (1998): 69-96.nition. Oxford university press, 1995.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with application
to face verification. CVPR, 2005.
Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfitting in deep
networks by decorrelating representations. ICLR, 2016.
Michael Fink. Object classification from a single example utilizing class relevance metrics. NIPS, 2005.
Jacob Goldberger, Sam Roweis, Geoff Hinton, and Ruslan Salakhutdinov. Neighborhood components analysis.
NIPS, 2005.
Bharath Hariharan and Ross Girshick. Low-shot visual object recognition. arxiv, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
CVPR, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arxiv, 2015.
Kenji Kawaguchi. Deep learning without poor local minima. NIPS, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arxiv, 2014.
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recog-
nition. ICML Deep Learning workshop, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In NIPS, pp. 1097-1105, 2012.
Brenden Lake, Tomer Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building machines that learn
and think like people. In arxiv, 2016.
Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning of simple
visual concepts. In CogSci, 2011.
Etai Littwin and Lior Wolf. The multiverse loss for robust transfer learning. arxiv, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual
recognition challenge. IJCV, 115(3):211-252, 2015.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. One-shot learning
with memory-augmented neural networks. arxiv, 2016.
10
Under review as a conference paper at ICLR 2017
Eduardo Sontag. Vc dimension of neural networks. NATO ASI Series F Computer and Systems Sciences, pp.
69-96,1998.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overfitting. JMLR, 15(1):1929-1958, 2014.
Sebastian Thrun. Lifelong learning algorithms. Learning to learn, pp. 181-209, 1998.
Vladimir Vapnik. The nature of statistical learning theory. Data mining and knowledge discovery, Springer,
1998.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching net-
works for one shot learning. arxiv, 2016.
Simon Wiesler and Hermann Ney. A convergence analysis of log-linear training. NIPS, 2011.
Dong Yu and Li Deng. Automatic speech recognition. Springer, 2012.
11