Under review as a conference paper at ICLR 2017
Non-linear Dimensionality Regularizer for
Solving Inverse Problems
Ravi Garg
University of Adelaide
ravi.garg@adelaide.edu.au
Anders Eriksson
Queensland University of Technology
anders.eriksson@qut.edu.au
Ian Reid
University of Adelaide
ian.reid@adelaide.edu.au
Ab stract
Consider an ill-posed inverse problem of estimating causal factors from observa-
tions, one of which is known to lie near some (unknown) low-dimensional, non-
linear manifold expressed by a predefined Mercer-kernel. Solving this problem re-
quires simultaneous estimation of these factors and learning the low-dimensional
representation for them. In this work, we introduce a novel non-linear dimension-
ality regularization technique for solving such problems without pre-training.
We re-formulate Kernel-PCA as an energy minimization problem in which low
dimensionality constraints are introduced as regularization terms in the energy.
To the best of our knowledge, ours is the first attempt to create a dimensionality
regularizer in the KPCA framework. Our approach relies on robustly penalizing
the rank of the recovered factors directly in the implicit feature space to create
their low-dimensional approximations in closed form.
Our approach performs robust KPCA in the presence of missing data and noise.
We demonstrate state-of-the-art results on predicting missing entries in the stan-
dard oil flow dataset. Additionally, we evaluate our method on the challenging
problem of Non-Rigid Structure from Motion and our approach delivers promis-
ing results on CMU mocap dataset despite the presence of significant occlusions
and noise.
1	Introduction
Dimensionality reduction techniques are widely used in data modeling, visualization and unsuper-
vised learning. Principal component analysis (PCAJoniffe (2002)), Kernel PCA (KPCAScholkopf
et al. (1998)) and Latent Variable Models (LVMsLawrence (2005)) are some of the well known
techniques used to create low dimensional representations of the given data while preserving its
significant information.
One key deployment of low-dimensional modeling occurs in solving ill-posed inference problems.
Assuming the valid solutions to the problem lie near a low-dimensional manifold (i.e. can be
parametrized with a reduced set of variables) allows for a tractable inference for otherwise under-
constrained problems. After the seminal work of Candes & Recht (2009); Recht et al. (2010) on
guaranteed rank minimization of the matrix via trace norm heuristics Fazel (2002), many ill-posed
computer vision problems have been tackled by using the trace norm — a convex surrogate of the
rank function — as a regularization term in an energy minimization frameWorkCandeS & Recht
(2009); Zhou et al. (2014). The flexible and easy integration of low-rank priors is one of key factors
for versatility and success of many algorithms. For example, pre-trained active appearance models
Cootes et al. (2001) or 3D morphable models Blanz & Vetter (1999) are converted to robust feature
tracking Poling et al. (2014), dense registration Garg et al. (2013b) and vivid reconstructions of natu-
ral videos Garg et al. (2013a) with no a priori knowledge of the scene. Various bilinear factorization
problems like background modeling, structure from motion or photometric stereo are also addressed
with a variational formulation of the trace norm regularization Cabral et al. (2013).
1
Under review as a conference paper at ICLR 2017
On the other hand, although many non-linear dimensionality reduction techniques — in particular
KPCA — have been shown to outperform their linear counterparts for many data modeling tasks,
they are seldom used to solve inverse problems without using a training phase. A general (discrim-
inative) framework for using non-linear dimensionality reduction is: (i) learn a low-dimensional
representation for the data using training examples via the kernel trick (ii) project the test exam-
ples on the learned manifold and finally (iii) find a data point (pre-image) corresponding to each
projection in the input space.
This setup has two major disadvantages. Firstly, many problems of interest come with corrupted
observations — noise, missing data and outliers — which violate the low-dimensional modeling
assumption.Secondly, computing the pre-image of any point in the low dimensional feature subspace
is non-trivial: the pre-image for many points in the low dimensional space might not even exist
because the non linear feature mapping function used for mapping the data from input space to the
feature space is non-surjective.
Previously, extensions to KPCA like Robust KPCA (RKPCANguyen & De la Torre (2009)) and
probabilistic KPCA (PKPCASanguinetti & Lawrence (2006)) with missing data have been proposed
to address the first concern, while various additional regularizers have been used to estimate the
pre-image robustly Bakir et al. (2004); Mika et al. (1998); Kwok & Tsang (2004); Abrahamsen &
Hansen (2009).
Generative models like LVMs Lawrence (2005) are often used for inference by searching the low-
dimensional latent space for a location which maximizes the likelihood of the observations. Prob-
lems like segmentation, tracking and semantic 3D reconstruction Prisacariu & Reid (2011); Dame
et al. (2013) greatly benefit from using LVM. However, the latent space is learned a priori with clean
training data in all these approaches.
Almost all non-linear dimensionality reduction techniques are non-trivial to generalize for solving
ill-posed problems (See section 4.2) without a pre-training stage. Badly under-constrained problems
require the low-dimensional constraints even for finding an initial solution, eliminating applicability
of the standard “projection + pre-image estimation” paradigm. This hinders the utility of non-
linear dimensionality reduction and a suitable regularization technique to penalize the non-linear
dimensionality is desirable.
Sum and Substance: A closer look at most
non-linear dimensionality reduction techniques
reveals that they rely upon a non-linear map-
ping function which maps the data from in-
put space to a (usually) higher dimensional fea-
ture space. In this feature space the data is as-
sumed to lie on a low-dimensional hyperplane
— thus, linear low-rank prior is apt in the fea-
ture space. Armed with this simple observa-
tion, our aim is to focus on incorporating the
advances made in linear dimensionality reduc-
tion techniques to their non-linear counterparts,
while addressing the problems described above.
Figure 1 explains this central idea and proposed
dimensionality regularizer in a nutshell with
Non Rigid Structure from Motion (NRSfM) as
the example application.
Our Contribution: In this work we propose a
unified for simultaneous robust KPCA and pre-
image estimation while solving an ill-posed in-
ference problem without a pre-training stage.
In particular we propose a novel robust en-
ergy minimization algorithm which handles the
implicitness of the feature space to directly
penalize its rank by iteratively: (i) creating
robust low-dimensional representation for the
©Dimensionality Regularizer
Penalizing rank in the feature space
Dimensionality OfSZ M(S)I∣*
Figure 1: Non-linear dimensionality regularizer for
NRSfM. The top part of the figure explains the ill-posed
inverse problem of recovering the causal factors (1);
projection matrices Ri and 3D structures Si , from 2D
image observations (2) Wi ’s, by minimizing the image
reprojection error f(W, R, S) = Pi kWi - RiSik2.
Assuming that the recovered 3D structures (Si ’s) lies
near an unknown non-linear manifold (represented by
the blue curve) in the input space, we propose to regu-
larize the dimensionality of this manifold (3) — span of
the non-linearly transformed shape vectors φ(Si)'s —
by minimizing ∣∣φ(S) k *. The non-linear transformation
φ is defined implicitly with a Mercer kernel and maps
the non-linear manifold to a linear low rank subspace
(shown in blue line) of RKHS.
2
Under review as a conference paper at ICLR 2017
data given the kernel matrix in closed form and (ii) reconstructing the noise-free version of the data
(pre-image of the features space projections) using the estimated low-dimensional representations
in a unified framework.
The proposed algorithm: (i) provides a novel closed form solution to robust KPCA; (ii) yields state-
of-the-art results on missing data prediction for the well-known oil flow dataset; (iii) outperforms
state-of-the-art linear dimensionality (rank) regularizers to solve NRSfM; and (iv) can be trivially
generalized to incorporate other cost functions in an energy minimization framework to solve various
ill-posed inference problems.
2	Problem formulation
This paper focuses on solving a generic inverse problem of recovering causal factor S =
[si, s2, •…SN] ∈ X × N from N observations W = [wι, w2, •… WN] ∈ Y × N such that
f (W, S) = 0. Here function f (observation,variable), is a generic loss function which aligns the
observations W with the variable S (possibly via other causal factors. e.g. R or Z in Section 4.1
and 4.2).
If, f (W, S) = 0 is ill-conditioned (for example when Y X), we want to recover matrix S under
the assumption that the columns of it lie near a low-dimensional non-linear manifold. This can be
done by solving a constrained optimization problem of the following form:
min	rank(Φ(S))
s.t. f(W,S) ≤
(1)
where Φ(S) = [φ(sι), φ(s2), …，Φ(sn)] ∈ H × N is the non-linear mapping of matrix S from
the input space X to the feature space H (also commonly referred as Reproducing Kernel Hilbert
Space), via a non-linear mapping function φ : X → H associated with a Mercer kernel K such that
K(S)i,j = φ(si)Tφ(sj).
In this paper we present a novel energy minimization framework to solve problems of the general
form (1).
As our first contribution, we relax the problem (1) by using the trace norm of Φ(S) — the convex
surrogate of rank function ——as a penalization function. The trace norm ∣∣Mk* =: Pi λi(M) of
a matrix M is the sum of its eigenvalues λi (M) and was proposed as a tight convex relaxation1 of
the rank(M) and is used in many vision problems as a rank regularizer Fazel (2002). Although
the rank minimization via trace norm relaxation does not lead to a convex problem in presence of
a non-linear kernel function, we show in 3.2 that it leads to a closed-form solution to denoising a
kernel matrix via penalizing the rank of recovered data (S) directly in the feature space.
With these changes we can rewrite (1) as:
min f(W,S) + Tkφ(S)k*
(2)
where τ is a regularization strength.1 2
It is important to notice that although the rank of the kernel matrix K(S) is equal to the rank of
Φ(S), ∣K(S)k* is merely ∣∣Φ(S)∣F. Thus, directly penalizing the sum of the singular values of
K(S) will not encourage low-rank in the feature space.3
Although we have relaxed the non-convex rank function, (2) is in general difficult to minimize
due to the implicitness of the feature space. Most widely used kernel functions like RBF do not
have a explicit definition of the function φ. Moreover, the feature space for many kernels is high-
(possibly infinite-) dimensional, leading to intractability. These issues are identified as the main
1More precisely, kM∣∣ψ was shown to be the tight convex envelope of rank(M)∕∣∣M∣∣s, where ∣∣M∣∣s
represent spectral norm of M.
21∕τ can also be viewed as Lagrange multiplier to the constraints in (1).
3Although it is clear that relaxing the rank of kernel matrix to ∣∣K(S) ∣* is suboptimal, works like Huang
et al. (2012); Cabral et al. (2013) with a variational definition of nuclear norm, allude to the possibility of
kernelization. Further investigation is required to compare this counterpart to our tighter relaxation.
3
Under review as a conference paper at ICLR 2017
barriers to robust KPCA and pre-image estimation Nguyen & De la Torre (2009). Thus, we have to
reformulate (2) by applying kernel trick where the cost function (2) can be expressed in terms of the
kernel function alone.
The key insight here is that under the assumption that kernel matrix K(S) is positive semidefinite,
we can factorize it as: K(S) = CTC. Although, this factorization is non-unique, it is trivial to show
the following:
√λi(K (S)) = λi(C) = λi(Φ(S))
Thus: IIC k* = ∣∣Φ(S)k* ∀ C : C T C = K (S)	(3)
where λi(.) is the function mapping the input matrix to its ith largest eigenvalue.
The row space of matrix C in (3) can be seen to span the eigenvectors associated with the kernel
matrix K(S) — hence the principal components of the non-linear manifold we want to estimate.
Using (3), problem (2) can finally be written as:
min f(W,S) +TkCk*
S,C
s.t. K(S) = CTC	(4)
The above minimization can be solved with a soft relaxation of the manifold constraint by assuming
that the columns of S lie near the non-linear manifold.
min f (W, S) + PkK(S) - CTCkF + τkCk*	(5)
As ρ → ∞, the optimum of (5) approaches the optimum of (4) . A local optimum of (4) can be
achieved using the penalty method of Nocedal & Wright (2006) by optimizing (5) while iteratively
increasing ρ as explained in Section 3.
Before moving on, we would like to discuss some alternative interpretations of (5) and its rela-
tionship to previous work - in particular LVMs. Intuitively，We can also interpret (5) from the
probabilistic viewpoint as commonly used in latent variable model based approaches to define ker-
nel function Lawrence (2005). For example a RBF kernel with additive Gaussian noise and inverse
width Y can be defined as: K(S)i,j = e-γksi-sj k + g where E 〜 N(0, σ). In other words, with
a finite ρ, our model allows the data points to lie near a non-linear low-rank manifold instead of
on it. Its worth noting here that like LVMs, our energy formulation also attempts to maximize the
likelihood of regenerating the training data W, (by choosing f(W, S) to be a simple least squares
cost) while doing dimensionality reduction.
Note that in closely related work Geiger et al. (2009), continuous rank penalization (with a loga-
rithmic prior) has also been used for robust probabilistic non-linear dimensionality reduction and
model selection in LVM framework. However, unlike Geiger et al. (2009); Lawrence (2005) where
the non-linearities are modeled in latent space (of predefined dimensionality), our approach directly
penalizes the non-linear dimensionality of data in a KPCA framework and is applicable to solve
inverse problems without pre-training.
3 Optimization
We approach the optimization of (5) by solving the following two sub-problems in alternation:
min f (W, S) + 2∣K(S)- CTCkF	(6)
min τ∣∣Ck* + 2kK(S)- CTCkF	⑺
Algorithm 1 outlines the approach and we give a detailed description and interpretations of both
sub-problems (7) and (6) in next two sections of the paper.
3.1	Pre-image estimation to solve inverse problem.
Subproblem (6) can be seen as a generalized pre-image estimation problem: we seek the factor si,
which is the pre-image of the projection of φ(si) onto the principle subspace of the RKHS stored in
4
Under review as a conference paper at ICLR 2017
Algorithm 1: Inference with Proposed Regularizer.
Input: Initial estimate S0 of S.
Output: Low-dimensional S and kernel representation C.
Parameters: Initial ρ0 and maximum ρmax penalty, with scale ρs .
- S = S0,ρ = ρ0 ;
while ρ ≤ ρmax do
while not converged do
-	Fix S and estimate C via closed-form solution of (7) using Algorithm 2;
-	Fix C and minimize (6) to update S using LM algorithm;
-	ρ = ρρs ;
CT C, which best explains the observation wi. Here (6) is generally a non-convex problem, unless
the Mercer-kernel is linear, and must therefore be solved using non-linear optimization techniques.
In this work, we use the Levenberg-Marquardt algorithm for optimizing (6).
Notice that (6) only computes the pre-image for the feature space projections of the data points with
which the non-linear manifold (matrix C) is learned. An extension to our formulation is desirable
if one wants to use the learned non-linear manifold for denoising test data in a classic pre-image
estimation framework. Although a valuable direction to pursue, it is out of scope of the present
paper.
3.2 Robust dimensionality reduction
One can interpret sub-problem (7) as a robust
form of KPCA where the kernel matrix has
been corrupted with Gaussian noise and we
want to generate its low-rank approximation.
Although (7) is non-convex we can solve it in
closed-form via singular value decomposition.
This closed-form solution is outlined in Algo-
rithm 2 and is based on the following theorem:
Theorem 1. With Sn 3 A 0 let A = UΣUT
denote its singular value decomposition. Then
min P∣∣A -* LTL∣∣F +TIILii*	(8)
L2
n
=X (ρ(σi - γ*2)2 + τYi*).⑼
i=1
A minimizer L* of (8) is given by
L* = Γ*UT	(10)
with Γ* ∈ D+, Yi ∈ {α ∈ R+ ∣ 小小=
0}	{0}, where pa,b denotes the depressed cubic
pa,b(x) = x3 - ax + b. D+n is the set of n-by-n diagonal matrices with non-negative entries.
Algorithm 2: Robust Dimensionality Reduction.
Input: Current estimate of S .
Output: Low-dimensional representation C .
Parameters: Current ρ and regularization strength τ .
- [U Λ UT] = Singular Value Decomposition of K(S);
// Λ is a diagonal matrix, storing N
singular values λi of K(S).
for i = 1 to N do
- Find three solutions (lr : r ∈ {1, 2, 3}) of:
l3 - lλi + — =0 ;
+ 2ρ
- set l4 = 0;
- lr = max(lr , 0) ∀r ∈ {1, 2, 3, 4} ;
-r = argmin{2∣∣λi - l2∣∣2 + Tlr };
r2
— λi = Ir ；
— C = Λ UT ；
// Λ is diagonal matrix storing λi.
Theorem 1 shows that each eigenvalue of the minimizer Ci of (7) can be obtained by solving a
depressed cubic whose coefficients are determined by the corresponding eigenvalue of the kernel
matrix and the regularization strength τ. The roots of each cubic, together with zero, comprise a
set of candidates for the corresponding eigenvalue of Ci . The best one from this set is obtained by
choosing the value which minimizes (9) (see Algorithm 2).
As elaborated in Section 2, problem (7) can be seen as regularizing sum of square root (L1/2 norm)
of the eigenvalues of the matrix K(S). In a closely related work Zongben et al. (2012), authors
advocate L1/2 norm as a better approximation for the cardinality of a vector then the more commonly
used L1 norm. A closed form solution for L1/2 regularization similar to our work was outlined in
Zongben et al. (2012) and was shown to outperform the L1 vector norm regularization for sparse
coding. To that end, our Theorem 1 and the proposed closed form solution (Algo 2) for (7) can
5
Under review as a conference paper at ICLR 2017
Table 1: Performance comparison on missing data completion on Oil Flow Dataset: Row 1 shows the amount
of missing data and subsequent rows show the mean and standard deviation of the error in recovered data
matrix over 50 runs on 100 samples of oil flow dataset by: (1) The mean method (also the initialization of
other methods) where the missing entries are replaced by the mean of the known values of the corresponding
attributes, (2) 1-nearest neighbor method in which missing entries are filled by the values of the nearest point,
(3) PPCA Tipping & Bishop (1999), (4) PKPCA of Sanguinetti & Lawrence (2006), (5)RKPCA Nguyen & De
la Torre (2009) and our method.
P(del)	0.05	0.10	0.25	0.50
mean	-13 ± 4	28 ± 4	70 ± 9	139 ± 7
-1-NN	-5±3-	14 ± 5	90 ± 20	NA
PPCA	3.7 ± .6	9 ± 2	50± 10	140 ± 30
PKPCA	-5±I-	12 ± 3	32 ± 6	100 ± 20
RKPCA	3.2 ± 1.9	8 ± 4	27 ± 8	83 ± 15
-Ours-	2.3±2	6±3	22±7	70±11
be seen as generalization of Zongben et al. (2012) to include the L1/2 matrix norms for which a
simplified proof is included in the Appendix A. It is important to note however, that the motivation
and implication of using L1/2 regularization in the context of non-linear dimensionality reduction
are significantly different to that of Zongben et al. (2012) and related work Du et al. (2013); Zhao
et al. (2014) which are designed for linear modeling of the causal factors. The core insight of using
L1 regularization in the feature space via the parametrization given in 3 facilitates a natural way for
non-linear modeling of causal factors with low dimensionality while solving an inverse problem by
making feature space tractable.
4	Experiments
In this section we demonstrate the utility of the proposed algorithm. The aims of our experiments are
twofold: (i) to compare our dimensionality reduction technique favorably with KPCA and its robust
variants; and (ii) to demonstrate that the proposed non-linear dimensionality regularizer consistently
outperforms its linear counterpart (a.k.a. nuclear norm) in solving inverse problems.
4.1	Matrix completion
The nuclear norm has been introduced as a low rank prior originally for solving the matrix comple-
tion problem. Thus, it is natural to evaluate its non-linear extensions on the same task. Assuming
W ∈ Rm×n to be the input matrix and Z a binary matrix specifying the availability of the observa-
tions in W , Algorithm 1 can be used for recovering a complete matrix S with the following choice
of f (W, Z, S):
f(W,Z,S) = kZ ◦ (W - S)kF	(11)
where ◦ represents Hadamard product.
To demonstrate the robustness of our algorithm for matrix completion problem, we choose 100
training samples from the oil flow dataset described in section 3.2 and randomly remove the elements
from the data with varying range of probabilities to test the performance of the proposed algorithm
against various baselines. Following the experimental setup as specified in Sanguinetti & Lawrence
(2006), we repeat the experiments with 50 different samples of Z . We report the mean and standard
deviation of the root mean square reconstruction error for our method with the choice of τ = 0.1,
alongside five different methods in Table 1. Our method significantly improves the performance of
missing data completion compared to other robust extensions of KPCA Tipping & Bishop (1999);
Sanguinetti & Lawrence (2006); Nguyen & De la Torre (2009), for every probability of missing
data.
Although we restrict our experiments to least-squares cost functions, it is vital to restate here that
our framework could trivially incorporate robust functions like the L1 norm instead of the Frobenius
norm — as a robust data term f(W, Z, S) — to generalize algorithms like Robust PCA Wright et al.
(2009) to their non-linear counterparts.
4.2	Kernel non-rigid structure from motion
6
Under review as a conference paper at ICLR 2017
Non-rigid structure from motion under orthography
is an ill-posed problem where the goal is to esti-
mate the camera locations and 3D structure of a de-
formable objects from a collection of 2D images
which are labeled with landmark correspondences
Bregler et al. (2000). Assuming si (xj) ∈ R3 to
be the 3D location of point xj on the deformable
object in the ith image, its orthographic projection
wi(xj) ∈ R2 can be written as wi(x) = Risi(xj),
where Ri ∈ R2×3 is a orthographic projection ma-
trix Bregler et al. (2000). Notice that as the object
deforms, even with given camera poses, reconstruct-
ing the sequence by least-squares reprojection error
minimization is an ill-posed problem. In their semi-
nal work, Bregler et al. (2000) proposed to solve this
problem with an additional assumption that the re-
constructed shapes lie on a low-dimensional linear
subspace and can be parameterized as linear combi-
nations of a relatively low number of basis shapes.
NRSfM was then cast as the low-rank factorization
problem of estimating these basis shapes and corre-
sponding coefficients.
Recent work, like Dai et al. (2014); Garg et al.
(2013a) have shown that the trace norm regularizer
can be used as a convex envelope of the low-rank
prior to robustly address ill-posed nature of the prob-
lem. A good solution to NRSfM can be achieved by
optimizing:
Figure 2: Non-linear dimensionality regular-
isation improves NRSfM performance com-
pared to its linear counterpart. Figure shows
the ground truth 3D structures in red wire-frame
overlaid with the structures estimated using: (a)
proposed non-linear dimensionality regularizer
shown in blue dots and (b) corresponding lin-
ear dimensionality regularizer (TNH) shown in
black crosses, for sample frames of CMU mo-
cap sequence. Red circles represent the 3D points
for which the projections were known whereas
squares annotated missing 2D observations. See
text and Table 2 for details.
FN
min T ∣∣S k* + ΣΣZi (xj )∣wi (xj ) - Risi (xj )∣F
S,R
,	i=1 j=1
(12)
where S is the shape matrix whose columns are 3N dimensional vectors storing the 3D coordinates
Si(xj) of the shapes and Zi(xj) is a binary variable indicating if projection of point xj is available
in the image i.
Assuming the projection matrices to be fixed, this problem is convex and can be exactly solved
with standard convex optimization methods. Additionally, if the 2D projections wi (xj ) are noise
free, optimizing (12) with very small τ corresponds to selecting the the solution — out of the many
solutions — with (almost) zero projection error, which has minimum trace norm Dai et al. (2014).
Thus henceforth, optimization of (12) is referred as the trace norm heuristics (TNH). We solve this
problem with a first order primal-dual variant of the algorithm given in Garg et al. (2013a), which
can handle missing data. The algorithm is detailed and compared favorably with the state of the art
NRSfM approaches (based on linear dimensionality regularization) Appendix C.
A simple kernel extension of the above optimization problem is:
min
S,R
FN
T kφ(S)k* + XXZi (xj )∣wi (xj ) - Risi (xj )∣F
i=1 j=1
'------------------------{z-------------}
f(W,Z,R,S)
(13)
where Φ(S) is the non-linear mapping of S to the feature space using an RBF kernel.
With fixed projection matrices R, (13) is of the general form (2), for which the local optima can be
found using Algorithm 1.
7
Under review as a conference paper at ICLR 2017
Table 2: 3D reconstruction errors for linear and non-linear dimensionality regularization with ground truth
camera poses. Column 1 and 4 gives gives error for TNH while column (2-3) and (5-6) gives the corresponding
error for proposed method with different width of RBF kernel. Row 5 reports the mean error over 4 sequences.
Dataset	No Missing Data			50% Missing Data		
	Linear	Non-Linear		Linear	Non-Linear	
		dmax	dmed		dmax	dmed
Drink	0.0227-	0.0114-	0.0083-	0.0313-	0.0248-	0.0229-
Pickup	0.0487-	0.0312-	0.0279-	0.0936-	0.0709-	0.0658-
Yoga	0.0344-	0.0257-	0.0276-	0.0828-	0.0611-	0.0612-
Stretch 一	0.0418-	0.0286-	0.0271-	0.0911-	0.0694-	0.0705
Mean	0.0369	0.0242	0.0227	0.0747	0.0565	0.0551
4.2.1	Results on the CMU dataset
We use a sub-sampled version of CMU mocap dataset by selecting every 10th frame of the smoothly
deforming human body consisting 41 mocap points used in Dai et al. (2014).4
In our experiments we use ground truth camera projection matrices to compare our algorithm against
TNH. The advantage of this setup is that with ground-truth rotation and no noise, we can avoid the
model selection (finding optimal regularization strength τ) by setting it low enough. We run the
TNH with τ = 10-7 and use this reconstruction as initialization for Algorithm 1. For the proposed
method, we set τ = 10-4 and use following RBF kernel width selection approach:
•	Maximum distance criterion (dmax): we set the maximum distance in the feature space to
be 3σ. Thus, the kernel matrix entry corresponding to the shape pairs obtained by TNH
with maximum Euclidean distance becomes e-9/2 .
•	Median distance criterion (dmed): the kernel matrix entry corresponding to the median
euclidean distance is set to 0.5.
Following the standard protocol in Dai et al. (2014); Akhter et al. (2009), we quantify the recon-
StrUction results with normalized mean 3D errors e3D = σF1N Pi Pj ej, where ej is the euclidean
distance of a reconstructed point j in frame i from the ground truth, σ is the mean of standard devi-
ation for 3 coordinates for the ground truth 3D structures, and F, N are number of input images and
number of points reconstructed.
Table 2 shows the results of the TNH and non-linear dimensionality regularization based methods
using the experimental setup explained above, both without missing data and after randomly remov-
ing 50% of the image measurements. Our method consistently beats the TNH baseline and improves
the mean reconstruction error by 〜40% with full data and by 〜25% when used with 50% miss-
ing data. Figure 2 shows qualitative comparison of the obtained 3D reconstruction using TNH and
proposed non-lienar dimensionality regularization technique for some sample frames from various
sequences. We refer readers to Appendix B for results with simultaneous reconstruction pose opti-
mization.
5	Conclusion
In this paper we have introduced a novel non-linear dimensionality regularizer which can be incor-
porated into an energy minimization framework, while solving an inverse problem. The proposed
algorithm for penalizing the rank of the data in the feature space has been shown to be robust to noise
and missing observations. We have picked NRSfM as an application to substantiate our arguments
and have shown that despite missing data and model noise (such as erroneous camera poses) our
algorithm significantly outperforms state-of-the-art linear counterparts.
Although our algorithm currently uses slow solvers such as the penalty method and is not directly
scalable to very large problems like dense non-rigid reconstruction, we are actively considering
alternatives to overcome these limitations. An extension to estimate pre-images with a problem-
4 Since our main goal is to validate the usefulness of the proposed non-linear dimensionality regularizer, we
opt for a reduced size dataset for more rapid and flexible evaluation.
8
Under review as a conference paper at ICLR 2017
specific loss function is possible, and this will be useful for online inference with pre-learned low-
dimensional manifolds.
Given the success of non-linear dimensionality reduction in modeling real data and overwhelming
use of the linear dimensionality regularizers in solving real world problems, we expect that pro-
posed non-linear dimensionality regularizer will be applicable to a wide variety of unsupervised
inference problems: recommender systems; 3D reconstruction; denoising; shape prior based object
segmentation; and tracking are all possible applications.
References
Trine Julie Abrahamsen and Lars Kai Hansen. Input space regularization stabilizes pre-images for
kernel pca de-noising. In EEE International Workshop on Machine Learning for Signal Process-
ing,pp.1-6, 2009.
Ijaz Akhter, Yaser Sheikh, Sohaib Khan, and Takeo Kanade. Nonrigid structure from motion in
trajectory space. In Advances in neural information processing systems, pp. 41T8, 2009.
Gokhan H Bakir, Jason Weston, and Bernhard Scholkopf. Learning to find pre-images. Advances in
neural information processing SyStemS, 16(7):449456, 2004.
Christopher M Bishop and Gwilym D James. Analysis of multiphase flows using dual-energy
gamma densitometry and neural networks. Nuclear Instruments and Methods in Physics Research
Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 327(2):580-593,
1993.
Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In 26th annual
conference on Computer graphics and interactive techniques, pp. 187-194, 1999.
Christoph Bregler, Aaron Hertzmann, and Henning Biermann. Recovering non-rigid 3d shape from
image streams. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 690-696,
2000.
R. Cabral, F. De la Torre, J. P. Costeira, and A. Bernardino. Unifying nuclear norm and bilinear
factorization approaches for low-rank matrix decomposition. In International Conference on
Computer Vision (ICCV), 2013.
Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717-772, 2009.
Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with
applications to imaging. Journal of Mathematical Imaging and Vision, 40(1):120-145, 2011.
Timothy F Cootes, Gareth J Edwards, and Christopher J Taylor. Active appearance models. IEEE
Transactions on pattern analysis and machine intelligence, 23(6):681-685, 2001.
Yuchao Dai, Hongdong Li, and Mingyi He. A simple prior-free method for non-rigid structure-
from-motion factorization. International Journal of Computer Vision, 107(2):101-122, 2014.
Amaury Dame, Victor Adrian Prisacariu, Carl Yuheng Ren, and Ian Reid. Dense reconstruction
using 3d object shape priors. In Computer Vision and Pattern Recognition, pp. 1288-1295. IEEE,
2013.
Rong Du, Cailian Chen, Zhiyi Zhou, and Xinping Guan. L 1/2-based iterative matrix completion for
data transmission in lossy environment. In Computer Communications Workshops (INFOCOM
WKSHPS), 2013 IEEE Conference on, pp. 65-66. IEEE, 2013.
Maryam Fazel. Matrix rank minimization with applications. PhD thesis, Stanford University, 2002.
Ravi Garg, Anastasios Roussos, and Lourdes Agapito. Dense variational reconstruction of non-rigid
surfaces from monocular video. In Computer Vision and Pattern Recognition, pp. 1272-1279,
2013a.
9
Under review as a conference paper at ICLR 2017
Ravi Garg, Anastasios Roussos, and Lourdes Agapito. A variational approach to video registration
with subspace constraints. International journal ofcomputer vision, 104(3):286-314, 2013b.
Andreas Geiger, Raquel Urtasun, and Trevor Darrell. Rank priors for continuous non-linear dimen-
sionality reduction. In Computer Vision and Pattern Recognition, pp. 880-887. IEEE, 2009.
Paulo FU Gotardo and Aleix M Martinez. Kernel non-rigid structure from motion. In IEEE Inter-
national Conference on Computer Vision, pp. 802-809, 2011a.
Paulo FU Gotardo and Aleix M Martinez. Non-rigid structure from motion with complementary
rank-3 spaces. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on,
pp. 3065-3072. IEEE, 2011b.
Dong Huang, Ricardo Silveira Cabral, and Fernando De la Torre. Robust regression. In European
Conference on Computer Vision (ECCV), 2012.
Ian Jolliffe. Principal component analysis. Wiley Online Library, 2002.
JT-Y Kwok and Ivor W Tsang. The pre-image problem in kernel methods. IEEE Transactions on
Neural Networks,, 15(6):1517-1525, 2004.
Neil D Lawrence. Probabilistic non-linear principal component analysis with gaussian process latent
variable models. The Journal of Machine Learning Research, 6:1783-1816, 2005.
Sebastian Mika, Bernhard Scholkopf, Alex J Smola, KIaUS-Robert Muller, Matthias Scholz, and
Gunnar Ratsch. Kernel pca and de-noising in feature spaces. In NIPS, volume 4, pp. 7, 1998.
Minh Hoai Nguyen and Fernando De la Torre. Robust kernel principal component analysis. In
Advances in Neural Information Processing Systems. 2009.
Jorge Nocedal and Stephen J. Wright. Numerical optimization. Springer, New York, 2006.
Bryan Poling, Gilad Lerman, and Arthur Szlam. Better feature tracking through subspace con-
straints. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp.
3454-3461. IEEE, 2014.
Victor Adrian Prisacariu and Ian Reid. Nonlinear shape manifolds as shape priors in level set seg-
mentation and tracking. In Computer Vision and Pattern Recognition, pp. 2185-2192. IEEE,
2011.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.
Ralph Tyrrell Rockafellar. Conjugate duality and optimization, volume 14. SIAM, 1974.
Guido Sanguinetti and Neil D Lawrence. Missing data in kernel pca. In Machine Learning: ECML
2006, pp. 751-758. Springer, 2006.
Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Nonlinear component analysis as
a kernel eigenvalue problem. Neural computation, 10(5):1299-1319, 1998.
Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611-622, 1999.
John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. Robust principal component
analysis: Exact recovery of corrupted low-rank matrices via convex optimization. In Advances in
Neural Information Processing Systems, pp. 2080-2088. 2009.
Qian Zhao, DeYu Meng, and ZongBen Xu. Robust sparse principal component analysis. Science
China Information Sciences, 57(9):1-14, 2014.
Xiaowei Zhou, Can Yang, Hongyu Zhao, and Weichuan Yu. Low-rank modeling and its applications
in image analysis. ACM Computing Surveys (CSUR), 47(2):36, 2014.
Xu Zongben, Chang Xiangyu, Xu Fengmin, and Zhang Hai. L1/2 regularization: a thresholding
representation theory and a fast solver. IEEE Transactions on neural networks and learning
systems, 23(7):1013-1027, 2012.
10
Under review as a conference paper at ICLR 2017
A Proof of Theorem 3.1
Proof. We will prove theorem 1 by first establishing a lower bound for (8) and subsequently showing that this
lower bound is obtained at L* given by (10). The rotational invariance of the entering norms allows Us to write
(8) as:
mnn	2∣∣Σ - WΓ2WT||F + τ∣∣Γ∣∣*.	(14)
Γ∈D ,	2
WT W=I
Expanding (14) we obtain
n
P mintr (∑2) - 2tr (∑WΓ2WT) + tr (Γ4) + 2τ X Yi	(15)
2Γ,W	ρ i=1
n	nn
=P min X (σ2 + Y4 + -pYi) - 2 XX w2j Y2σi	(16)
, i=1	i=1 j=1
≥ P min X (σ2 - 2γ2σi + γ4 + ^Yi)	(17)
=P X min fσ2 - 2γ2σi + γ4 + 2τγi)	(18)
2	i=1 γi≥0	P
The inequality in (17) follows directly by applying Holder,s inequality to (16) and using the property that the
column vectors wi are unitary.
Next, with L = ΓUT in (8) we have
2	l∣A - LT L∣∣F+t∣∣L∣∣* = P ∣∣∑-Γ2∣∣F + τ∣∣Γ∣∣*
n
=X(P 3-γiΓ + TYi).	(19)
i=1
Finally, since the subproblems in (18) are separable in Yi , its minimizer must be KKT-points of the individual
subproblems. As the constraints are simple non-negativity constraints, these KKT points are either (positive)
stationary points of the objective functions or 0. It is simple to verify that the stationary points are given by the
roots of the cubic function pσi^∕2p. Hence it follows that there exists a Yi such that
P ^σ2 - 2Y2σi + Y4 + 2^Yi) ≥ 彳⑷-Yi2)2 + T喧,	(20)
∀Yi ≥ 0, which completes the proof.	□
A. 1 Validating the closed form solution
Given the relaxations proposed in Section 2, our assertion that the novel trace regularization based
non-linear dimensionality reduction is robust need to be substantiated. To that end, we evaluate our
closed-form solution of Algorithm 2 on the standard oil flow dataset introduced in Bishop & James
(1993).
This dataset comprises 1000 training and 1000 testing data samples, each of which is of 12 dimen-
sions and categorized into one of three different classes. We add zero mean Gaussian noise with
variance σ to the training data5 and recover the low-dimensional manifold for this noisy training
data Sσ with KPCA and contrast this with the results from Algorithm 2. An inverse width of the
Gaussian kernel γ = 0.075 is used for all the experiments on the oil flow dataset.
It is important to note that in this experiment, we only estimate the principal components (and their
variances) that explain the estimated non-linear manifold, i.e. matrix C by Algorithm 2, without
reconstructing the denoised version of the corrupted data samples.
Both KPCA and our solution require model selection (choice of rank and τ respectively) which
is beyond the scope of this paper. Here we resort to evaluate the performance of both methods
under different parameters settings. To quantify the accuracy of the recovered manifold (C) we use
following criteria:
5Note that our formulation assumes Gaussian noise in K(S) where as for this evaluation we add noise to S
directly.
11
Under review as a conference paper at ICLR 2017
Table 3: Robust dimensionality reduction accuracy by KPCA versus our closed-form solution on the full oil
flow dataset. Columns from left to right represent: (1) standard deviation of the noise in training samples (2-3)
Error in the estimated low-dimensional kernel matrix by (2) KPCA and (3) our closed-form solution, (4-5)
Nearest neighbor classification error of test data using (4) KPCA and (5) our closed-form solution respectively.
	Manifold Error		Classification Error	
STD	KPCA	Our CFS	KPCA	Our CFS
.2	0.1099	0.1068	9.60%	9.60%
.3	0.2298	0.2184	19.90%	15.70%
.4	0.3522	0.3339	40.10%	22.20%
Figure 3: Performance comparison between KPCA and our Robust closed-form solution with dimensionality
regularization on oil flow dataset with additive Gaussian noise of standard deviation σ. Plots show the normal-
ized kernel matrix errors with different rank of the model. Kernel PCA results are shown in dotted line with
diamond while ours are with solid line with a star. Bar-plot show the worst and the best errors obtained by our
method for a single rank of recovered kernel matrix.
•	Manifold Error : A good manifold should preserve maximum variance of the data — i.e.
it should be able to generate a denoised version K(Sest) = CT C of the noisy kernel
matrix K(Sσ). We define the manifold estimation error as kK(Sest) - K (SGT)k2F, where
K(SGT ) is the kernel matrix derived using noise free data. Figure 3 shows the manifold
estimation error for KPCA and our method for different rank and parameter τ respectively.6
•	Classification error: The accuracy of a non-linear manifold is often also tested by the near-
est neighbor classification accuracy. We select the estimated manifold which gives mini-
mum Manifold Error for both the methods and report 1NN classification error (percentage
of misclassified example) of the 1000 test points by projecting them onto estimated mani-
folds.
B Kernel NRSfM with Camera Pose Estimation
Extended from section 4.2
Table 4 shows the reconstruction performance on a more realistic experimental setup, with the mod-
ification that the camera projection matrices are initialized with rigid factorization and were refined
with the shapes by optimizing (2). To solve NRSfM problem with unknown projection matrices,
we parameterize each Ri with quaternions and alternate between refining the 3D shapes S and pro-
jection matrices R using LM. The regularization strength τ was selected for the TNH method by
golden section search and parabolic interpolation for every test case independently. This ensures the
best possible performance for the baseline. For our proposed approach τ was kept to 10-4 for all
sequences for both missing data and full data NRSfM. This experimental protocol somewhat disad-
vantages the non-linear method, since its performance can be further improved by a judicious choice
of the regularization strength.
6Errors from non-noisy kernel matrix can be replaced by cross validating the entries of the kernel matrix for
model selection for more realistic experiment.
12
Under review as a conference paper at ICLR 2017
Table 4: 3D reconstruction errors for linear and non-linear dimensionality regularization with noisy camera
pose initialization from rigid factorization and refined in alternation with shape. The format is same as Table 2.
Dataset	No Missing Data			50% Missing Data		
	Linear	Non-Linear		Linear	Non-Linear	
	T = T *	T = dmax	10-4 dmed	τ = τ *	τ= dmax	10-4 dmed
Drink	0.0947	0.0926	0.0906	0.0957	0.0942	0.0937
Pickup	0.1282	0.1071	0.1059	0.1598	0.1354	0.1339
Yoga	0.2912	0.2683	0.2639	0.2821	0.2455	0.2457
Stretch	0.1094	0.1043	0.1031	0.1398	0.1459	0.1484
Mean	0.1559	0.1430	0.1409	0.1694	0.1552	0.1554
However our purpose is primarily to show that the non-linear method adds value even without time-
consuming per-sequence tuning. To that end, note that despite large errors in the camera pose esti-
mations by TNH and 50% missing measurements, the proposed method shows significant (〜10%)
improvements in terms of reconstruction errors, proving our broader claims that non-linear repre-
sentations are better suited for modeling real data, and that our robust dimensionality regularizer can
improve inference for ill-posed problems.
As suggested by Dai et al. (2014), robust camera pose initialization is beneficial for the structure es-
timation. We have used rigid factorization for initializing camera poses here but this can be trivially
changed. We hope that further improvements can be made by choosing better kernel functions, with
cross validation based model selection (value of τ) and with a more appropriate tuning of kernel
width. Selecting a suitable kernel and its parameters is crucial for success of kernelized algorithms.
It becomes more challenging when no training data is available. We hope to explore other kernel
functions and parameter selection criteria in our future work.
We would also like to contrast our work with Gotardo & Martinez (2011a), which is the only work
we are aware of where non-linear dimensionality reduction is attempted for NRSfM. While esti-
mating the shapes lying on a two dimensional non-linear manifold, Gotardo & Martinez (2011a)
additionally assumes smooth 3D trajectories (parametrized with a low frequency DCT basis) and a
pre-defined hard linear rank constraint on 3D shapes. The method relies on sparse approximation of
the kernel matrix as a proxy for dimensionality reduction. The reported results were hard to replicate
under our experimental setup for a fair comparison due to non-smooth deformations. However, in
contrast to Gotardo & Martinez (2011a), our algorithm is applicable in a more general setup, can
be modified to incorporate smoothness priors and robust data terms but more importantly, is flexible
to integrate with a wide range of energy minimization formulations leading to a larger applicability
beyond NRSfM.
C TNH algorithm for NRSfM
In section 4.2, we have compared the proposed non-linear dimensionality reduction prior against a
variant of Garg et al. (2013a) which handles missing data by optimizing:
min
S,R
FN
T IlSlI* + XX
Zi (xj )Iwi (xj ) - Risi (xj )I
i=1 j=1
(21)
This problem is convex in S given noise free projection matrix Ri ’s but non-differentiable. To
optimize (21), we first rewrite it in its primal-dual form by dualizing the trace norm 7 :
max min
Q S,R
FN
τ < S,Q > + XX
Zi (xj )Iwi (xj ) - Risi (xj )I
i=1 j=1
s.t.IQIs ≤ 1
(22)
where Q ∈ RX×N stores the dual variables to S and I.Is represent spectral norm (highest eigen-
value) of a matrix.
7For more details on primal dual formulation and dual norm of the trace norm see Rockafellar (1974); Recht
et al. (2010); Chambolle & Pock (2011).
13
Under review as a conference paper at ICLR 2017
Algorithm 3: Trace norm Heuristics.
Input: Initial estimates S0, R0 of S and R.
Output: Low-dimensional S and camera poses R.
Parameters: Regularization strength τ , measurements W and binary mask Z .
-	S=S0, R=R0;
// set iteration count η step size σ and duals Q
-	η = 0;
-	σ = 1∕τ ;
-	Q=0;
while not converged do
// projection matrix estimation
- Fix S, Q and refine Ri for every image i with LM;
// steepest descend update for Sij for each point xj and each frame i
for i = 1 to F do
for j = 1 to N do
[-Sη+1 = (I2×2 + σ(ZijRTRi))T(Sj- στQj + σRT(Zij ◦ Wij));
// accelerated steepest ascend update for Q
-	Q = Qn + τσ(2Sn+1 - Sn);
-	U DV T = singular value decomposition of Qη;
-	D = min(D, 1);
-	Qn+1 = UD Vt ;
// Go to next iteration
_ - η = η +1
Table 5: 3D reconstruction errors for different NRSfM approaches and our TNH Algorithm given ground truth
camera projection matrices. Results for all the methods (except TNH) are taken from Dai et al. (2014).
Dataset	PTAAkhter et al. (2009)	CSF2Gotardo & Martinez (2011b)	BMMDai etal. (2014)	TNH
Drink	0.0229	00215	0.0238	0.0237
Pick-up	0.0992	0.0814	0.0497	0.0482
Yoga	0.0580	0.0371	0.0334	0.0333
Stretch	0.0822	0.0442	0.0456	0.0431
We choose quaternions to perametrize the 2 × 3 camera matrices Ri to satisfy orthonormality con-
straints as done in Garg et al. (2013a) and optimize the saddle point problem (22) using alternation.
In particular, for a single iteration: (i) we optimize the camera poses Ri ’s using LM, (ii) take a
steepest descend step for updating S and (ii) a steepest ascend step for updating Q which is fol-
lowed by projecting its spectral norm to unit ball. Given ground truth camera matrices ( without
step (i)), alternation (ii-iii) can be shown to reach global minima of (22). Algorithm 3 outlines TNH
algorithm.
As the main manuscript uses NRSfM only as a practical application of our non-linear dimension-
ality reduction prior, we have restricted our NRSfM experiments to only compare the proposed
method against its linear counterpart. For the timely evaluation, the reported experiments we con-
ducted on sub-sampled CMU mocap dataset. Here, we supplement the arguments presented in the
main manuscript by favorably comparing the linear dimensionality reduction based NRSfM algo-
rithm(TNH) to other NRSfM methods on full length CMU mocap sequences.
14