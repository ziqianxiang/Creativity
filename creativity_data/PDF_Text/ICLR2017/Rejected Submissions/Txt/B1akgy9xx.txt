Under review as a conference paper at ICLR 2017
Making Stochastic Neural Networks
from Deterministic Ones
Kimin Lee, Jaehyung Kim, Song Chong, Jinwoo Shin
School of Electrical Engineering
Korea Advanced Institute of Science Technology, Republic of Korea
{kiminlee, jaehyungkim, jinwoos}@kaist.ac.kr, songchong@kaist.edu
Ab stract
It has been believed that stochastic feedforward neural networks (SFNN) have
several advantages beyond deterministic deep neural networks (DNN): they have
more expressive power allowing multi-modal mappings and regularize better due
to their stochastic nature. However, training SFNN is notoriously harder. In this
paper, we aim at developing efficient training methods for large-scale SFNN, in
particular using known architectures and pre-trained parameters of DNN. To this
end, we propose a new intermediate stochastic model, called Simplified-SFNN,
which can be built upon any baseline DNN and approximates certain SFNN by
simplifying its upper latent units above stochastic ones. The main novelty of our
approach is in establishing the connection between three models, i.e., DNN →
Simplified-SFNN → SFNN, which naturally leads to an efficient training pro-
cedure of the stochastic models utilizing pre-trained parameters of DNN. Us-
ing several popular DNNs, we show how they can be effectively transferred to
the corresponding stochastic models for both multi-modal and classification tasks
on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our
stochastic model built from the wide residual network has 28 layers and 36 million
parameters, where the former consistently outperforms the latter for the classifica-
tion tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.
1	Introduction
Recently, deterministic deep neural networks (DNN) have demonstrated state-of-the-art perfor-
mance on many supervised tasks, e.g., speech recognition (Hinton et al., 2012a) and object recog-
nition (Krizhevsky et al., 2012). One of the main components underlying these successes is on the
efficient training methods for deeper and wider DNNs, which include backpropagation (Rumelhart
et al., 1988), stochastic gradient descent (Robbins & Monro, 1951), dropout/dropconnect (Hinton
et al., 2012b; Wan et al., 2013), batch/weight normalization (Ioffe & Szegedy, 2015; Salimans &
Kingma, 2016), and various activation functions (Nair & Hinton, 2010; Gulcehre et al., 2016). On
the other hand, stochastic feedforward neural networks (SFNN) (Neal, 1990) having random latent
units are often necessary in order to model complex stochastic natures in many real-world tasks, e.g.,
structured prediction (Tang & Salakhutdinov, 2013), image generation (Goodfellow et al., 2014) and
memory networks (Zaremba & Sutskever, 2015). Furthermore, it has been believed that SFNN has
several advantages beyond DNN (Raiko et al., 2014): it has more expressive power for multi-modal
learning and regularizes better for large-scale learning.
Training large-scale SFNN is notoriously hard since backpropagation is not directly applicable. Cer-
tain stochastic neural networks using continuous random units are known to be trainable efficiently
using backpropagation under the variational techniques and the reparameterization tricks (Kingma
& Welling, 2013). On the other hand, training SFNN having discrete, i.e., binary or multi-modal,
random units is more difficult since intractable probabilistic inference is involved requiring too many
random samples. There have been several efforts developing efficient training methods for SFNN
having binary random latent units (Neal, 1990; Saul et al., 1996; Tang & Salakhutdinov, 2013; Ben-
gio et al., 2013; Raiko et al., 2014; Gu et al., 2015) (see Section 2.1 for more details). However,
training SFNN is still significantly slower than doing DNN of the same architecture, e.g., most prior
1
Under review as a conference paper at ICLR 2017
works on this line have considered a small number (at most 5 or so) of layers in SFNN. We aim for
the same goal, but our direction is orthogonal to them.
Instead of training SFNN directly, we study whether pre-trained parameters of DNN (or easier mod-
els) can be transferred to it, possibly with further fine-tuning of light cost. This approach can be
attractive since one can utilize recent advances in DNN on its design and training. For example,
one can design the network structure of SFNN following known specialized ones of DNN and use
their pre-trained parameters. To this end, we first try transferring pre-trained parameters of DNN
using sigmoid activation functions to those of the corresponding SFNN directly. In our experiments,
the heuristic reasonably works well. For multi-modal learning, SFNN under such a simple trans-
formation outperforms DNN. Even for the MNIST classification, the former performs similarly as
the latter (see Section 2 for more details). However, it is questionable whether a similar strategy
works in general, particularly for other unbounded activation functions like ReLU (Nair & Hinton,
2010) since SFNN has binary, i.e., bounded, random latent units. Moreover, it lost the regularization
benefit of SFNN: itis rather believed that transferring parameters of stochastic models to DNN helps
its regularization, but the opposite direction is unlikely possible.
To address the issues, we propose a special form of stochastic neural networks, named Simplified-
SFNN, which intermediates between SFNN and DNN, having the following properties. First,
Simplified-SFNN can be built upon any baseline DNN, possibly having unbounded activation func-
tions. The most significant part of our approach lies in providing rigorous network knowledge trans-
ferring (Chen et al., 2015) between Simplified-SFNN and DNN. In particular, we prove that param-
eters of DNN can be transformed to those of the corresponding Simplified-SFNN while preserving
the performance, i.e., both represent the same mapping and features. Second, Simplified-SFNN ap-
proximates certain SFNN, better than DNN, by simplifying its upper latent units above stochastic
ones using two different non-linear activation functions. Simplified-SFNN is much easier to train
than SFNN while utilizing its stochastic nature for regularization.
The above connection DNN → Simplified-SFNN → SFNN naturally suggests the following training
procedure for both SFNN and Simplified-SFNN: train a baseline DNN first and then fine-tune its
corresponding Simplified-SFNN initialized by the transformed DNN parameters. The pre-training
stage accelerates the training task since DNN is faster to train than Simplified-SFNN. In addition,
one can also utilize known DNN training techniques such as dropout and batch normalization for
fine-tuning Simplified-SFNN. In our experiments, we train SFNN and Simplified-SFNN under the
proposed strategy. They consistently outperform the corresponding DNN for both multi-modal and
classification tasks, where the former and the latter are for measuring the model expressive power
and the regularization effect, respectively. To the best of our knowledge, we are the first to confirm
that SFNN indeed regularizes better than DNN. We also construct the stochastic models following
the same network structure of popular DNNs including Lenet-5 (LeCun et al., 1998), NIN (Lin
et al., 2014) and WRN (Zagoruyko & Komodakis, 2016). In particular, WRN (wide residual net-
work) of 28 layers and 36 million parameters has shown the state-of-art performances on CIFAR-10
and CIFAR-100 classification datasets, and our stochastic models built upon WRN outperform the
deterministic WRN on the datasets.
Organization. In Section 2, we focus on DNNs having sigmoid and ReLU activation functions and
study simple transformations of their parameters to those of SFNN. In Section 3, we consider DNNs
having general activation functions and describe more advanced transformations via introducing a
new model, named Simplified-SFNN.
2	Simple Transformation from DNN to SFNN
2.1	Preliminaries for SFNN
Stochastic feedforward neural network (SFNN) is a hybrid model, which has both stochastic binary
and deterministic hidden units. We first introduce SFNN with one stochastic hidden layer (and
without deterministic hidden layers) for simplicity. Throughout this paper, we commonly denote
the bias for unit i and the weight matrix of the '-th hidden layer by b' and W', respectively. Then,
the stochastic hidden layer in SFNN is defined as a binary random vector with N 1 units, i.e., h1 ∈
2
Under review as a conference paper at ICLR 2017
{0, 1}N1, drawn under the following distribution:
N1
P (h1 I x) = Y P (h1 I x), where P (h1 = 1 | x) = σ (W1x + b1).	⑴
i=1
In the above, x is the input vector and σ (x) = 1/ (1 + e-x) is the sigmoid function. Our conditional
distribution of the output y is defined as follows:
P (y I x) = EP(hi∣x) [P (y I h1)] = EP(hi|x) [N (y I W2h1 + b2, σ2)],
where N(∙) denotes the normal distribution with mean W2h1 + b2 and (fixed) variance σ%. There-
fore, P (y I x) can express a very complex, multi-modal distribution since it is a mixture of expo-
nentially many normal distributions. The multi-layer extension is straightforward via a combination
of stochastic and deterministic hidden layers, e.g., see Tang & Salakhutdinov (2013), Raiko et al.
(2014). Furthermore, one can use any other output distributions as like DNN, e.g., softmax for
classification tasks.
There are two computational issues for training SFNN: computing expectations with respect to
stochastic units in forward pass and computing gradients in backward pass. One can notice that both
are computationally intractable since they require summations over exponentially many configura-
tions of all stochastic units. First, in order to handle the issue in forward pass, one can use the follow-
M
ing Monte Carlo approximation for estimating the expectation: P (y I x) W M P P(y I h(m)),
m=1
where h(m)〜P (h1 I x) and M is the number of samples. This random estimator is unbiased and
has relatively low variance (Tang & Salakhutdinov, 2013) since its accuracy does not depend on the
dimensionality of h1 and one can draw samples from the exact distribution. Next, in order to handle
the issue in backward pass, Neal (1990) proposed a Gibbs sampling, but it is known that it often
mixes poorly. Saul et al. (1996) proposed a variational learning based on the mean-field approxi-
mation, but it has additional parameters making the variational lower bound looser. More recently,
several other techniques have been proposed including unbiased estimators of the variational bound
using importance sampling (Tang & Salakhutdinov, 2013; Raiko et al., 2014) and biased/unbiased
estimators of the gradient for approximating backpropagation (Bengio et al., 2013; Raiko et al.,
2014; Gu et al., 2015).
2.2	Simple transformation from sigmoid-DNN and ReLU-DNN to SFNN
Despite the recent advances, training SFNN is still very slow compared to DNN due to the sampling
procedures: in particular, it is notoriously hard to train SFNN when the network structure is deeper
and wider. In order to handle these issues, we consider the following approximation:
P (y I x) = Ep(hi |x) [N (y I W2h1 + b2, σ2)]
W N (y I EP(hi∣x) [W2h1] + b2, σ2) = N (y I W2σ (W1x + b1) + b2, σy2).⑵
Note that the above approximation corresponds to replacing stochastic units by deterministic ones
such that their hidden activation values are same as marginal distributions of stochastic units, i.e.,
SFNN can be approximated by DNN using sigmoid activation functions, say sigmoid-DNN. When
there exist more latent layers above the stochastic one, one has to apply similar approximations to
all of them, i.e., exchanging the orders of expectations and non-linear functions, for making DNN
and SFNN are equivalent. Therefore, instead of training SFNN directly, one can try transferring pre-
trained parameters of sigmoid-DNN to those of the corresponding SFNN directly: train sigmoid-
DNN instead of SFNN, and replace deterministic units by stochastic ones for the inference purpose.
Although such a strategy looks somewhat ‘rude’, it was often observed in the literature that it rea-
sonably works well for SFNN (Raiko et al., 2014) and we also evaluate it as reported in Table 1. We
also note that similar approximations appear in the context of dropout: it trains a stochastic model
averaging exponentially many DNNs sharing parameters, but also approximates a single DNN well.
Now we investigate a similar transformation in the case when DNN uses the unbounded ReLU
activation function, say ReLU-DNN. Many recent deep networks are of ReLU-DNN type due to
the gradient vanishing problem, and their pre-trained parameters are often available. Although it
is straightforward to build SFNN from sigmoid-DNN, it is less clear in this case since ReLU is
3
Under review as a conference paper at ICLR 2017
(a)
(b)
Figure 1: The generated samples from (a) sigmoid-DNN and (b) SFNN which uses same parameters
trained by sigmoid-DNN. One can note that SFNN can model the multiple modes in outupt space y
around x = 0.4.
Inference Model	Network Structure	Training NLL	MNIST Classification Training Error (%)	Test Error (%)	Multi-modal Learning Test NLL
sigmoid-DNN	2 hidden layers	0	0	1.54	5.290
SFNN	2 hidden layers	0	0	1.56	1.564
sigmoid-DNN	3 hidden layers	0.002	0.03	1.84	4.880
SFNN	3 hidden layers	0.022	0.04	1.81	0.575
sigmoid-DNN	4 hidden layers	0	0.01	1.74	4.850
SFNN	4 hidden layers	0.003	0.03	1.73	0.392
ReLU-DNN	2 hidden layers	0.005	0.04	1.49	7.492
SFNN	2 hidden layers	0.819	4.50	5.73	2.678
ReLU-DNN	3 hidden layers	0	0	1.43	7.526
SFNN	3 hidden layers	1.174	16.14	17.83	4.468
ReLU-DNN	4 hidden layers	0	0	1.49	7.572
SFNN	4 hidden layers	1.213	13.13	14.64	1.470
Table 1: The performance of simple parameter transformations from DNN to SFNN on the MNIST
and synthetic datasets, where each layer of neural networks contains 800 and 50 hidden units for
two datasets, respectively. For all experiments, the only first hidden layer of DNN is replaced by
stochastic one. We report negative log-likelihood (NLL) and classification error rates.
unbounded. To handle this issue, we redefine the stochastic latent units of SFNN:
P(h1∣ x) = Y P	(h1	| x), where	P(M	= 1 |	x)	= min ,f	(W1x + b1)	,l1.⑶
i=1
In the above, f(x) = max{x, 0} is the ReLU activation function and α is some hyper-parameter. A
simple transformation can be defined similarly as the case of sigmoid-DNN via replacing determin-
istic units by stochastic ones. However, to preserve the parameter information of ReLU-DNN, one
has to choose α such that af (WtX + bɪ) ≤ 1 and rescale upper parameters W2 as follows:
α-1 - max f	(W 11x + 硝］(W1,	b1)	— (W1,	b1)	,	(W2,	b2)	— ( W2/α,	b2)	. (4)
Then, applying similar approximations as in (2), i.e., exchanging the orders of expectations and
non-linear functions, one can observe that ReLU-DNN and SFNN are equivalent.
We evaluate the performance of the simple transformations from DNN to SFNN on the MNIST
dataset (LeCun et al., 1998) and the synthetic dataset (Bishop, 1994), where the former and the latter
are popular datasets used for a classification task and a multi-modal (i.e., one-to-many mappings)
prediction learning, respectively. In all experiments reported in this paper, we commonly use the
softmax and Gaussian with standard deviation of σy = 0.05 are used for the output probability
on classification and regression tasks, respectively. The only first hidden layer of DNN is replaced
by stochastic one, and we use 500 samples for estimating the expectations in the SFNN inference.
As reported in Table 1, we observe that the simple transformation often works well for both tasks:
the SFNN and sigmoid-DNN inferences (using same parameters trained by sigmoid-DNN) perform
similarly for the classification task and the former significantly outperforms for the latter for the
4
Under review as a conference paper at ICLR 2017
multi-modal task (also see Figure 1). It might suggest some possibilities that the expensive SFNN
training might not be not necessary, depending on the targeted learning quality. However, in case of
ReLU, SFNN performs much worse than ReLU-DNN for the MNIST classification task under the
parameter transformation.
3	Transformation from DNN to SFNN via Simplified-SFNN
In this section, we propose an advanced method to utilize the pre-trained parameters of DNN for
training SFNN. As shown in the previous section, simple parameter transformations from DNN to
SFNN are not clear to work in general, in particular for activation functions other than sigmoid.
Moreover, training DNN does not utilize the stochastic regularizing effect, which is an important
benefit of SFNN. To address the issues, we design an intermediate model, called Simplified-SFNN.
The proposed model is a special form of stochastic neural networks, which approximates certain
SFNN by simplifying its upper latent units above stochastic ones. Then, we establish more rigorous
connections between three models: DNN → Simplified-SFNN → SFNN, which leads to an effi-
cient training procedure of the stochastic models utilizing pre-trained parameters of DNN. In our
experiments, we evaluate the strategy for various tasks and popular DNN architectures.
3.1	Simplified-SFNN of two hidden layers and non-negative activation
FUNCTIONS
For clarity of presentation, we first introduce Simplified-SFNN with two hidden layers and non-
negative activation functions, where its extensions to multiple layers and general activation functions
are presented in Appendix B. We also remark that we primarily describe fully-connected Simplified-
SFNNs, but their convolutional versions can also be naturally defined. In Simplified-SFNN of two
hidden layers, we assume that the first and second hidden layers consist of stochastic binary hidden
units and deterministic ones, respectively. As like (3), the first layer is defined as a binary random
vector with N1 units, i.e., h1 ∈ {0, 1}N1, drawn under the following distribution:
P(h1∣ x) = Y P	(h1	| x), where	P (h1	= 1 |	x)	= min 卜 1 f	(W1x +	b1)	,l1.⑸
i=1
where x is the input vector, α1 > 0 is a hyper-parameter for the first layer, and f : R → R+ is some
non-negative non-linear activation function with |f0(x)| ≤ 1 for all x ∈ R, e.g., ReLU and sigmoid
activation functions. Now the second layer is defined as the following deterministic vector with N2
units, i.e., h2(x) ∈ RN2:
h2 (x)= f (α2 (EP(hi∣χ) [s (W2h1 + b2)] - s (0))) : ∀j ∈ N2] ,	(6)
where α2 > 0 is a hyper-parameter for the second layer and s : R → R is a differentiable function
with |s00(x)| ≤ 1 for all x ∈ R, e.g., sigmoid and tanh functions. In our experiments, we use the
sigmoid function for s(x). Here, one can note that the proposed model also has the same computa-
tional issues with SFNN in forward and backward passes due to the complex expectation. One can
train Simplified-SFNN similarly as SFNN: we use Monte Carlo approximation for estimating the
expectation and the (biased) estimator of the gradient for approximating backpropagation inspired
by Raiko et al. (2014) (more detailed explanation is presented in Appendix A).
We are interested in transferring parameters of DNN to Simplified-SFNN to utilize the training
benefits of DNN since the former is much faster to train than the latter. To this end, we consider the
following DNN of which `-th hidden layer is deterministic and defined as follows:
h' (x) = hb' (x)= f (W'h'-1 (x) + 叫：i ∈ N' i ,	⑺
where hb0(x) = x. As stated in the following theorem, we establish a rigorous way how to initialize
parameters of Simplified-SFNN in order to transfer the knowledge stored in DNN.
Theorem 1 Assume that both DNN and Simplified-SFNN with two hidden layers have same network
structure with non-negative activation function f. Given parameters {W', b' : ' = 1,2} of DNN
and input dataset D, choose those of Simplified-SFNN as follows:
(αι, W1, b1) — (ɪ, W1, b1) , (α2, W2, b2) — (YY1, ɪW2, ɪb2) ,	(8)
γ1	s0 (0) γ2	γ1γ2
5
Under review as a conference paper at ICLR 2017
I	I： Stochastic layer
I	J Deterministic layer
Input --►串 3→ O> -------► Output
Input --►窜 O Output
(b)
(c)
一+： Stochasticity
(a)
Figure 2: (a) Simplified-SFNN (top) and SFNN (bottom). (b) For first 200 epochs, we train a
baseline ReLU-DNN. Then, we train simplified-SFNN initialized by the DNN parameters under
transformation (8) with γ2 = 50. We observe that training ReLU-DNN* directly does not reduce
the test error even when network knowledge transferring still holds between the baseline ReLU-
DNN and the corresponding ReLU-DNN*. (c) As the value ofγ2 increases, knowledge transferring
loss measured as 由N1' P P ∣h' (x) - h' (x) ∣ is decreasing.
where γ1 = max f Wci1x + bbi1 and γ2 > 0 is any positive constant. Then, it follows that
∣h2(X)- h2(X)∣≤ γ1 (PijiW0j0+j1)，∀j,X ∈ D.
The proof of the above theorem is presented in Appendix D.1. Our proof is built upon the
first-order Taylor expansion of non-linear function s(x). Theorem 1 implies that one can make
Simplified-SFNN represent the function values of DNN with bounded errors using a linear trans-
formation. Furthermore, the errors can be made arbitrarily small by choosing large γ2 , i.e.,
lim ∣∣hj2 (X) - bhj2 (X)∣∣ = 0, ∀j, X ∈ D. Figure 2(c) shows that knowledge transferring loss de-
creases as γ2 increases on MNIST classification. Based on this, we choose γ2 = 50 commonly for
all experiments.
3.2	Why Simplified-SFNN
Given a Simplified-SFNN model, the corresponding SFNN can be naturally defined by taking out the
expectation in (6). As illustrated in Figure 2(a), the main difference between SFNN and Simplified-
SFNN is that the randomness of the stochastic layer propagates only to its upper layer in the latter,
i.e., the randomness ofh1 is averaged out at its upper units h2 and does not propagate to h3 or output
y. Hence, Simplified-SFNN is no longer a Bayesian network. This makes training Simplified-SFNN
much easier than SFNN since random samples are not required at some layers1 and consequently
the quality of gradient estimations can also be improved, in particular for unbounded activation
functions. Furthermore, one can use the same approximation procedure (2) to see that Simplified-
SFNN approximates SFNN. However, since Simplified-SFNN still maintains binary random units,
it uses approximation steps later, in comparison with DNN. In summary, Simplified-SFNN is an
intermediate model between DNN and SFNN, i.e., DNN → Simplified-SFNN → SFNN.
The above connection naturally suggests the following training procedure for both SFNN and
Simplified-SFNN: train a baseline DNN first and then fine-tune its corresponding Simplified-SFNN
initialized by the transformed DNN parameters. Finally, the fine-tuned parameters can be used for
SFNN as well. We evaluate the strategy for the MNIST classification, which is reported in Table 2
(see Appendix C for more detailed experiment setups). We found that SFNN under the two-stage
training always performs better than SFNN under a simple transformation (4) from ReLU-DNN.
1 For example, if one replaces the first feature maps in the fifth residual unit of Pre-ResNet having 164
layers (He et al., 2016) by stochastic ones, then the corresponding DNN, Simplified-SFNN and SFNN took 1
mins 35 secs, 2 mins 52 secs and 16 mins 26 secs per each training epoch, respectively, on our machine with
one Intel CPU (Core i7-5820K 6-Core@3.3GHz) and one NVIDIA GPU (GTX Titan X, 3072 CUDA cores).
Here, we trained both stochastic models using the biased estimator (Raiko et al., 2014) with 10 random samples
on CIFAR-10 dataset.
6
Under review as a conference paper at ICLR 2017
Inference Model	Training Model	Network Structure	without BN & DO	with BN	with DO
sigmoid-DNN	sigmoid-DNN	2 hidden layers	1.54	1.57	1.25
SFNN	sigmoid-DNN	2 hidden layers	1.56	2.23	1.27
Simplified-SFNN	fine-tuned by Simplified-SFNN	2 hidden layers	1.51	1.5	1.11
sigmoid-DNN*	fine-tuned by Simplified-SFNN	2 hidden layers	1.48 (0.06)	1.48 (0.09)	1.14 (0.11)
SFNN	fine-tuned by Simplified-SFNN	2 hidden layers	1.51	1.57	1.11
ReLU-DNN	ReLU-DNN	2 hidden layers	1.49	1.25	1.12
SFNN	ReLU-DNN	2 hidden layers	5.73	3.47	1.74
Simplified-SFNN	fine-tuned by Simplified-SFNN	2 hidden layers	1.41	1.17	1.06
ReLU-DNN*	fine-tuned by Simplified-SFNN	2 hidden layers	1.32 (0.17)	1.16 (0.09)	1.05 (0.07)
SFNN	fine-tuned by Simplified-SFNN	2 hidden layers	2.63	1.34	1.51
ReLU-DNN	ReLU-DNN	3 hidden layers	1.43	1.34	1.24
SFNN	ReLU-DNN	3 hidden layers	17.83	4.15	1.49
Simplified-SFNN	fine-tuned by Simplified-SFNN	3 hidden layers	1.28	1.25	1.04
ReLU-DNN*	fine-tuned by Simplified-SFNN	3 hidden layers	1.27 (0.16)	1.24 (0.1)	1.03 (0.21)
SFNN	fine-tuned by Simplified-SFNN	3 hidden layers	1.56	1.82	1.16
ReLU-DNN	ReLU-DNN	4 hidden layers	1.49	1.34	1.30
SFNN	ReLU-DNN	4 hidden layers	14.64	3.85	2.17
Simplified-SFNN	fine-tuned by Simplified-SFNN	4 hidden layers	1.32	1.32	1.25
ReLU-DNN*	fine-tuned by Simplified-SFNN	4 hidden layers	1.29 (0.2)	1.29 (0.05)	1.25 (0.05)
SFNN	fine-tuned by Simplified-SFNN	4 hidden layers	3.44	1.89	1.56
Table 2: Classification test error rates [%] on MNIST, where each layer of neural networks contains
800 hidden units. All Simplified-SFNNs are constructed by replacing the first hidden layer of a base-
line DNN with stochastic hidden layer. We also consider training DNN and fine-tuning Simplified-
SFNN using batch normalization (BN) and dropout (DO). The performance improvements beyond
baseline DNN (due to fine-tuning DNN parameters under Simplified-SFNN) are calculated in the
bracket.
More interestingly, Simplified-SFNN consistently outperforms its baseline DNN due to the stochas-
tic regularizing effect, even when we train both models using dropout (Hinton et al., 2012b) and
batch normalization (Ioffe & Szegedy, 2015). In order to confirm the regularization effects, one can
again approximate a trained SimPlified-SFNN by a new deterministic DNN which We call DNN*
and is different from its baseline DNN under the following approximation at upper latent units above
binary random units:
Ep(h'∣x) [s (Wj+1h')] w S (EP(h'∣x) [W'+1h']) = S XW Wj+1P (h' = 1 | x)) .	(9)
We found that DNN* using fined-tuned parameters of Simplified-SFNN also outperforms the base-
line DNN as shown in Table 2 and Figure 2(b).
3.3 Experimental results on multi-modal Learning and convolutional
NETWORKS
We present several experimental results for both multi-modal and classification tasks on MNIST
(LeCun et al., 1998), Toronto Face Database (TFD) (Susskind et al., 2010), CIFAR-10, CIFAR-100
(Krizhevsky & Hinton, 2009) and SVHN (Netzer et al., 2011). Here, we present some key results
due to the space constraints and more detailed explanations for our experiment setups are presented
in Appendix C.
We first verify that it is possible to learn one-to-many mapping via Simplified-SFNN on the TFD
and MNIST datasets, where the former and the latter are used to predict multiple facial expressions
from the mean of face images per individual and the lower half of the MNIST digit given the upper
half, respectively. We remark that both tasks are commonly performed in recent other works to
test the multi-modal learning using SFNN (Raiko et al., 2014; Gu et al., 2015). In all experiments,
we first train a baseline DNN, and the trained parameters of DNN are used for further fine-tuning
those of Simplified-SFNN. As shown in Table 3 and Figure 3, stochastic models outperform their
baseline DNN, and generate different digits for the case of ambiguous inputs (while DNN does
not). We also evaluate the regularization effect of Simplified-SFNN for the classification tasks on
CIFAR-10, CIFAR-100 and SVHN. Table 4 reports the classification error rates using convolutional
neural networks such as Lenet-5 (LeCun et al., 1998), NIN (Lin et al., 2014) and WRN (Zagoruyko
& Komodakis, 2016). Due to the regularization effects, Simplified-SFNNs consistently outperform
7
Under review as a conference paper at ICLR 2017
Inference Model	Training Model	MNIST-half		TFD	
		2 hidden layers	3 hidden layers	2 hidden layers	3 hidden layers
sigmoid-DNN	sigmoid-DNN	1.409	1.720	-0.064	0.005
SFNN	sigmoid-DNN	0.644	1.076	-0.461	-0.401
Simplified-SFNN	fine-tuned by Simplified-SFNN	1.474	1.757	-0.071	-0.028
SFNN	fine-tuned by Simplified-SFNN	0.619	0.991	-0.509	-0.423
ReLU-DNN	ReLU-DNN	1.747	1.741	1.271	1.232
SFNN	ReLU-DNN	-1.019	-1.021	0.823	1.121
Simplified-SFNN	fine-tuned by Simplified-SFNN	2.122	2.226	0.175	0.343
SFNN	fine-tuned by Simplified-SFNN	-1.290	-1.061	-0.380	-0.193
Table 3: Test negative log-likelihood (NLL) on MNIST-half and TFD datasets, where each layer of
neural networks contains 200 hidden units. All Simplified-SFNNs are constructed by replacing the
first hidden layer of a baseline DNN with stochastic hidden layer.
The original digits and the corresponding inputs (first). The generated samples from Sigmoid-DNN
(second), SFNN under the simple transformation (third), and SFNN fine-tuned by Simplified-SFNN
(forth). We observed that SFNN fine-tuned by Simplified-SFNN can generate more various samples
from same inputs, e.g., 3 and 8, better than SFNN under the simple transformation.
Inference model	Training Model	CIFAR-10	CIFAR-100	SVHN
Lenet-5	Lenet-5	37.67	77.26	11.18
Lenet-5*	Simplified-SFNN	33.58	73.02	9.88
NIN	NIN	9.51	32.66	3.21
NIN*	Simplified-SFNN	9.33	30.81	3.01
WRN	WRN	4.22 (4.39)f	20.30 (20.04)f	3.25f
WRN*	Simplified-SFNN (one stochastic layer)	4.21f	19.98f	3.09f
WRN*	Simplified-SFNN (two stochastic layers)	4.14f	19.72f	3.06f
Table 4: Test error rates [%] on CIFAR-10, CIFAR-100 and
SVHN. The error rates for WRN are from our experiments,
where original ones reported in (Zagoruyko & Komodakis,
2016) are in the brackets. Results with f are obtained using
the horizontal flipping and random cropping augmentation.
Epoch
Figure 4: Test errors of WRN* per each
training epoch on CIFAR-100.
their baseline DNNs. For example, WRN* outperforms WRN by 0.08% on CIFAR-10 and 0.58% on
CIFAR-100. We expect that if one introduces more stochastic layers, the error would be decreased
more (see Figure 4), but it increases the fine-tuning time-complexity of Simplified-SFNN.
4	Conclusion
In order to develop an efficient training method for large-scale SFNN, this paper proposes a new
intermediate stochastic model, called Simplified-SFNN. We establish the connection between three
models, i.e., DNN → Simplified-SFNN → SFNN, which naturally leads to an efficient training
procedure of the stochastic models utilizing pre-trained parameters of DNN. This connection natu-
rally leads an efficient training procedure of the stochastic models utilizing pre-trained parameters
and architectures of DNN. We believe that our work brings a new important direction for training
stochastic neural networks, which should be of broader interest in many related applications.
8
Under review as a conference paper at ICLR 2017
References
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through StoChas-
tic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Christopher M Bishop. Mixture density networks, 1994.
Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer.
arXiv preprint arXiv:1511.05641, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Process-
ing Systems (NIPS), 2014.
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation for stochas-
tic neural networks. arXiv preprint arXiv:1511.05176, 2015.
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions. arXiv
preprint arXiv:1603.00391, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. arXiv
preprint arXiv:1603.05027, 2016.
Geoffrey E Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Se-
nior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling
in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 2012a.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving
neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. International Conference on Machine Learning (ICML), 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Alex Krizhevsky and Geoffrey E Hinton. Learning multiple layers of features from tiny images. Master’s
thesis, Department of Computer Science, University of Toronto, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems (NIPS), 2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 1998.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. International Conference on Learning Repre-
sentations (ICLR), 2014.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Interna-
tional Conference on Machine Learning (ICML), 2010.
Radford M Neal. Learning stochastic feedforward networks. Department of Computer Science, University of
Toronto, 1990.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. NIPS Workshop on Deep Learning and Unsupervised
Feature Learning, 2011.
Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning binary stochastic
feedforward neural networks. arXiv preprint arXiv:1406.2989, 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics,
1951.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error
propagation. Technical report, MIT Press, 1988.
9
Under review as a conference paper at ICLR 2017
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate train-
ing of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.
Lawrence K Saul, Tommi Jaakkola, and Michael I Jordan. Mean field theory for sigmoid belief networks.
Journal of artificial intelligence research, 1996.
Josh M Susskind, Adam K Anderson, and Geoffrey E Hinton. The toronto face database. Department of
Computer Science, University of Toronto, Toronto, ON, Canada, Tech. Rep, 2010.
Yichuan Tang and Ruslan R Salakhutdinov. Learning stochastic feedforward neural networks. In Advances in
Neural Information Processing Systems (NIPS), 2013.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using
dropconnect. In International Conference on Machine Learning (ICML), 2013.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines. arXiv preprint
arXiv:1505.00521, 2015.
A	Training Simplified-SFNN
The parameters of Simplified-SFNN can be learned using a variant of the backpropagation algorithm
(Rumelhart et al., 1988) in a similar manner to DNN. However, in contrast to DNN, there are two
computational issues for simplified-SFNN: computing expectations with respect to stochastic units
in forward pass and computing gradients in back pass. One can notice that both are intractable since
they require summations over all possible configurations of all stochastic units. First, in order to
handle the issue in forward pass, we use the following Monte Carlo approximation for estimating
the expectation:
1M
EP (h1 |x) [s (W2h1 + b2)] W MM X S (W2h(m) + b2),	h(m) 〜P (h11 x),
m=1
where M is the number of samples. This random estimator is unbiased and has relatively low
variance (Tang & Salakhutdinov, 2013) since its accuracy does not depend on the dimensionality of
h1 and one can draw samples from the exact distribution. Next, in order to handle the issue in back
pass, we use the following approximation inspired by (Raiko et al., 2014):
∂W2 EP(h1∣x) [s (w2h1 + b2)] W MM X ∂W S (W2h(m) + b2),
j	mj
∂W EP (hi∣χ) [s (w2h1 + b2)] w Wj X s0 (W2h(m) + b2) ∂W P(h = 11 χ),
i	mi
where h(m)〜P (h1 | x) and M is the number of samples. In our experiments, We commonly
choose M = 20.
B	Extensions of S implified - S FNN
In this section, we describe how the network knowledge transferring between Simplified-SFNN and
DNN, i.e., Theorem 1, generalizes to multiple layers and general activation functions.
B.1 Extension to multiple layers
A deeper Simplified-SFNN with L hidden layers can be defined similarly as the case of L = 2. We
also establish network knowledge transferring between Simplified-SFNN and DNN with L hidden
layers as stated in the following theorem. Here, we assume that stochastic layers are not consecutive
for simpler presentation, but the theorem is generalizable for consecutive stochastic layers.
10
Under review as a conference paper at ICLR 2017
Theorem 2 Assume that both DNN and Simplified-SFNN with L hidden layers have same network
structure with non-negative activation function f. Given parameters {W', b' : ' = 1,...,L} of
DNN and input dataset D, choose the same ones for Simplified-SFNN initially and modify them for
each `-th stochastic layer and its upper layer as follows:
ɑ` J 1-, (α'+ι, W'+1, b'+1) J
Y'
Y'Y'+ι W '+1	b'+1
s0 (0) , Y'+1 , Y'Y'+1
where γ' = max ∣f (W 'h'-1(x) + b') ∣ and γ'+ι is any positive Constant Then, itfollows that
γ'+i¾∞	IhL (X)- bL(X) ∣=0,
∀ stochastic hidden layer `
∀j, X ∈ D.
The above theorem again implies that it is possible to transfer knowledge from DNN to Simplified-
SFNN by choosing large γl+1 . The proof of Theorem 2 is similar to that of Theorem 1 and given in
Appendix D.2.
B.2 Extension to general activation functions
In this section, we describe an extended version of Simplified-SFNN which can utilize any activation
function. To this end, we modify the definitions of stochastic layers and their upper layers by
introducing certain additional terms. If the `-th hidden layer is stochastic, then we slightly modify
the original definition (5) as follows:
P (h' | x) = Y P (h' | x) With P (h' = 1 | x) = min } `f(W1 X + b1 + 2) , l},
Where f : R → R is a non-linear (possibly, negative) activation function With |f0(x)| ≤ 1 for all
x ∈ R. In addition, We re-define its upper layer as folloWs:
h'+1 (x)
f 卜'+1 (EP(h'∣x) [s (Wj+1h' + b'+1)] — S(O) - s 20X X Wj+1)) : ∀j
Where h0(x) = x and s : R → R is a differentiable function With |s00 (x)| ≤ 1 for all x ∈ R.
Under this general Simplified-SFNN model, We also shoW that transferring netWork knoWledge from
DNN to Simplified-SFNN is possible as stated in the folloWing theorem. Here, We again assume
that stochastic layers are not consecutive for simpler presentation.
Theorem 3 Assume that both DNN and Simplified-SFNN with L hidden layers have same network
structure with non-linear activation function f. Given parameters {W', b' : ' = 1,...,L} ofDNN
and input dataset D, choose the same ones for Simplified-SFNN initially and modify them for each
`-th stochastic layer and its upper layer as follows:
α'
(α'+ι, W'+1, b'+1) J 2Y'Y0+1
S (0)
Wc '+1	bb'+1
γ'+1	2γ' γ'+1
where γ' = max
i,x∈D
∣∣fWci'h'-1(x)+bbi'∣∣,
and γ'+1 is any positive constant. Then, it follows that
1
J 2Y',
Y'+1→∞	IhL (X)- bL (X) I=0,
∀ stochastic hidden layer '
j, x ∈ D.
We omit the proof of the above theorem since it is someWhat direct adaptation of that of Theorem 2.
C Experimental Setups
In this section, We describe detailed explanation about all the experiments described in Section 3.
In all experiments, the softmax and Gaussian With the standard deviation of 0.05 are used as the
output probability for the classification task and the multi-modal prediction, respectively. The loss
Was minimized using ADAM learning rule (Kingma & Ba, 2014) With a mini-batch size of 128. We
used an exponentially decaying learning rate.
11
Under review as a conference paper at ICLR 2017
C.1 Classification on MNIST
The MNIST dataset consists of 28 × 28 pixel greyscale images, each containing a digit 0 to 9 with
60,000 training and 10,000 test images. For this experiment, we do not use any data augmentation
or pre-processing. Hyper-parameters are tuned on the validation set consisting of the last 10,000
training images. All Simplified-SFNNs are constructed by replacing the first hidden layer of a
baseline DNN with stochastic hidden layer. As described in Section 3.2, we train Simplified-SFNNs
under the two-stage procedure: first train a baseline DNN for first 200 epochs, and the trained
parameters of DNN are used for initializing those of Simplified-SFNN. For 50 epochs, we train
simplified-SFNN. We choose the hyper-parameter γ2 = 50 in the parameter transformation. All
Simplified-SFNNs are trained with M = 20 samples at each epoch, and in the test, we use 500
samples.
C.2 Multi-modal regression on TFD and MNIST
The Toronto Face Database (TFD) (Susskind et al., 2010) dataset consists of48 × 48 pixel greyscale
images, each containing a face image of 900 individuals with 7 different expressions. Similar to
(Raiko et al., 2014), we use 124 individuals with at least 10 facial expressions as data. We randomly
choose 100 individuals with 1403 images for training and the remaining 24 individuals with 326
images for the test. We take the mean of face images per individual as the input and set the output
as the different expressions of the same individual. The MNIST dataset consists of 28 × 28 pixel
greyscale images, each containing a digit 0 to 9 with 60,000 training and 10,000 test images. For
this experiments, each pixel of every digit images is binarized using its grey-scale value. We take the
upper half of the MNIST digit as the input and set the output as the lower half of it. All Simplified-
SFNNs are constructed by replacing the first hidden layer of a baseline DNN with stochastic hidden
layer. We train Simplified-SFNNs with M = 20 samples at each epoch, and in the test, we use 500
samples. We use 200 hidden units for each layer of neural networks in two experiments. Learning
rate is chosen from {0.005 , 0.002, 0.001, ... , 0.0001} , and the best result is reported for both tasks.
C.3 CLASSIFICATION ON CIFAR-10, CIFAR- 1 00 AND SVHN
The CIFAR-10 and CIFAR-100 datasets consist of 50,000 training and 10,000 test images. The
SVHN dataset consists of 73,257 training and 26,032 test images.2 We pre-process the data using
global contrast normalization and ZCA whitening. For these datasets, we design a convolutional
version of Simplified-SFNN. In a similar manner to the case of fully-connected networks, one can
define a stochastic convolution layer, which considers the input feature map as a binary random ma-
trix and generates the output feature map as defined in (6). All Simplified-SFNNs are constructed by
replacing a hidden feature map of a baseline models, i.e., Lenet-5, NIN and WRN, with stochastic
one as shown in Figure 5(d). We use WRN with 16 and 28 layers for SVHN and CIFAR datasets, re-
spectively, since they showed state-of-the-art performance as reported by Zagoruyko & Komodakis
(2016). In case of WRN, we introduce up to two stochastic convolution layers.For 100 epochs, we
first train baseline models, i.e., Lenet-5, NIN and WRN, and trained parameters are used for ini-
tializing those of Simplified-SFNNs. All Simplified-SFNNs are trained with M = 5 samples and
the test error is only measured by the approximation (9). The test errors of baseline models are
measured after training them for 200 epochs similar to Zagoruyko & Komodakis (2016).
D	Proofs of Theorems
D.1 Proof of Theorem 1
First consider the first hidden layer, i.e., stochastic layer. Let γ1 = max f Wci1x + bbi1 be
the maximum value of hidden units in DNN. If We initialize the parameters (αι, W1, b1) J
(Y^, W1, b1) ,then the marginal distribution of each hidden unit i becomes
=1 | x, W1, b1) = min{αιf (W 1X + 星),l} = γ1 f (W 1X + 星),∀i, X ∈ D. (10)
2We do not use the extra SVHN dataset for training.
12
Under review as a conference paper at ICLR 2017
6
6
Input
feature maps (f. maps) Stochastic (Stoc.) f. maps
16
f. maps
[Convolution (Conv.)]	[Max pool]
[Stochastic (Stoc.) Conv.]	[Max pool]
[Fully-connected] [Fully-connected] [Fully-connected]
Oo
O
(a)
Input
192	160	96	96
f. maps	f. maps f. maps	f. maps
192	192	192
f. maps f. maps f. maps
[Conv.]	[Conv.] [Conv.] [Max pool] [Conv.]
[Conv.] [Conv.] [Avg pool]	[Conv.]
192	192	192	10 C
Output
ι. maps	ι. maps Stoc. f. maps ɪ. maps
[Conv.] [Stoc. Conv.] [Avgpool]
(b)
(c)
Figure 5: The overall structures of (a) Lenet-5, (b) NIN, (c) WRN with 16 layers, and (d) WRN with
28 layers. The red feature maps correspond to the stochastic ones. In case of WRN, we introduce
one (v0 = 3) and two (v0 = 2) stochastic feature maps.
6
Next consider the second hidden layer. From Taylor,s theorem, there exists a value Z between 0
s	s ( . S (	s07z)χ2
and x such that S(X) = s(0) + s0(0)x + R(x), where R(x) = -M-. Since we consider a binary
random vector, i.e., h1 ∈ {0, 1}n 1, one can write
EP(hi∣x) [s (1¾ (h1))] = E (S(O)十 S (0) Bj (h1) + R (为(h1))) p(h1 1 x)
h1
S (0) + s0 (0)
(X w2j P (hi =11x)+b2
+ Ep(hi∣x) [R(Bj(h1))],
where βj (h1) := W2h1 + j is the incoming signal. From (6) and (10), for every hidden unit j, it
follows that
h2 (x； W2, b2) = f ɑ2
SO(O) ( γ- X WjhI(X) + b2 ) + EP(h1∣x) [r (βj (h1))]
))
13
Under review as a conference paper at ICLR 2017
Since we assume that |f 0(χ) | ≤ 1, the following inequality holds:
h2(x; W2, b2) - f (α2s/(0) (W X WjbI(X) + b2)) ≤ ∣α2Ep(hi∣χ) [R(βj(h1))] ∣
≤ OaEP(h1∣χ) [(w2h1 + b2)2],
where we use ∣s00(z)∣ < 1 for the last inequality. Therefore, it follows that
∣ h(X； w2, b2) - %
(X； W 2, b2)∣≤ Y1 (PstM2
∀j,
SinCeWeSet (02, W2, b2)一
Y2Y1
. This completes the proof of Theorem 1.
D.2 PROOF OF THEOREM 2
For the proof of Theorem 2, we first state the two key lemmas on error propagation in Simplified-
SFNN.
Lemma 4 Assume that there exists some positive constant B such that
∣ h'-1 (x) - b'τ (x) ∣ ≤ B,	∀i, X ∈ D,
and the '-th hidden layer OfNCSFNN is standard deterministic layer as defined in (7). Given pa-
rameters {W', b'} ofDNN, choose same ones for NCSFNN. Then, thefollowing inequality holds:
BN`tWmax,	∀j, X ∈ D.
where Wmax = max
max ij
Proof. See Appendix D.3.
□
Lemma 5 Assume that there exists some positive COnStant B such that
∣ h'-1 (x) - b'T (x) ∣ ≤ B, ∀i, x ∈ D,
and the '-th hidden layer of simplified-SFNN is stochastic layer. Given parameters
{W', W '+1, b', b'+1} ofDNN, choose those of Simplified-SFNN as follows:
α' 一 :(J, w'+1, b'+1)j
Y'Y'+1 W '+1	b'+1
s0 (0) , Y'+1 , Y'Y'+1
where γ = max ∣ f (W'H'-1(x) + bj) ∣ and γ`+ι is any positive constant. Then, itfollows that
∣ hk+ι(X)- hk+ι(X)∣ ≤ bn '-in `wm aχcm+χ+
Y (n 'wm+ι+⅛axγ-1)2
2s0(0)γ'+ι
,∀k, x ∈ D,
where bLx = max ∣ b' ∣ and Wmax
max
i
Proof. See Appendix D.4.
□
Assume that '-th layer is first stochastic hidden layer in Simplified-SFNN. Then, from Theorem 1,
we have
∣ h'+1 (X)- b'+1 (x) ∣ ≤
γ' (n 'wm+ι+⅞⅛3-1)2
2s0(0)γ'+ι
∀j, x ∈ D.
(11)
14
Under review as a conference paper at ICLR 2017
According to Lemma 4 and 5, the final error generated by the right hand side of (11) is bounded by
Tn` (N'W僦 + ⅞+⅛1)
2s0 (0) γ'+ι
(12)
L
where t` = Q
'0 = l+2
TccIm'ax) . One can note that every error generated by each stochastic layer
is bounded by (12). Therefore, it follows that
IhL (X)- bL (X) I ≤ X
©stochastic hidden layer
∀j, x ∈ D.
∀j, x ∈ D.
From above inequality, we can conclude that
U→∞	卜L (X)- bL (X)I=0,
V stochastic hidden layer '
This completes the proof of Theorem 2.
D.3 PROOF OF LEMMA 4
From assumption, there exists some constant G such that |g| < B and
h'-1 (x) = b'-1 (x) + G, ∀i, x.
By definition of standard deterministic layer, it follows that
hj (x) = f (Xch'-1 (x) + 珞T)= f (X碎虏T (x) + X W口 + b).
Since we assume that ∣f 0(χ) ∣ ≤ 1, one can conclude that
h' (x) - f (X W超T (x) + bj) J ≤ X W□ ≤ B X 碎 ≤ BN'-1Wmaχ.
This completes the proof of Lemma 4.
D.4 Proof of Lemma 5
From assumption, there exists some constant ef-1 such that ∣ e'-1 ∣ < B and
h'-1 (x) = b'-1 (x) + e'-1, ∀i, x.	(13)
Let γ' = max ∣ f (W` h`-1(x) + bj) ∣ be the maximum value of hidden units. If we initialize the
parameters (ɑ`, W', b') J &, W', b'^, then the marginal distribution becomes
P (h' = 1 ∣ x, W', b') = min }`f (W'h'-1 (x) + b') , 1} = if (W'h'-1 (x) + b') , ∀j, x.
From (13), it follows that
P(h' = 1 ∣ x,W',b') = Jf (W'h'-1 (x) + XWe'-1 + b') ,	∀j,x.
Similar to Lemma 4, there exists some constant Ej such that ∣ e' ∣ < BN'-1 Wmax and
P(h' = 1 ∣ x, W', b') = ± (b' (x) + V) , ∀j, x.	(14)
15
Under review as a conference paper at ICLR 2017
Next, consider the upper hidden layer of stochastic layer. From Taylor,s theorem, there exists a
s	s , s ( . S ∖	s00 (z)χ2
value Z between 0 and t such that S(X) = s(0) + s0(0)x + R(x), where R(X) = —(^—. Since we
consider a binary random vector, i.e., h' ∈ {0, 1}n', one can write
Ep(h'∣x)[s(βk(h'))] = E (s(0) + s0(0)βk(h') + R (嫌(h'))) P(h' | x)
h'
=s(0) + S0(0) (X W+1P(hj = 1 | x) + Q) + X R(βk(h'))P(h' | x),
where βk (h`) = Wk+1h' + 埠1 is the incoming signal. From (14) and above equation, for every
hidden unit k, we have
hk+1(x; W'+1, b'+1)
=f 卜+1 (s0(0) (Y (X WK 居(x) + X Wj+1 e) + bk+) +EP (h`∣χ) [R(βfc (h'))jj .
Since we assume that |f 0(χ) | < 1, the following inequality holds:
hk+1(x; W'+1, b'+1) - f (α'+1S0(0) (Y- X Wj+1b' (x) + b'+)j
≤ α'+1 S0(0) X Wj+1e' + αe+1Ep(h'∣χ) [R(βfc(h'))]
Y'	j
≤ α+^ X Wj+1ej +∣ 号 EP (h'∣x) [(Wk+1h' + QIH ∣,	(15)
Y j
where we use |s00(z)| < 1 for the last inequality. Therefore, it follows that
∣ hk+1 (x) - bk+1 (x) ∣≤ bn '-1n ` Cm aχWm+χ+
2s0(0)γ'+1
since we set (αe+1, W'+1, b'+1) 一 (γ%γ', cζ；1,	). This completes the proof of
Lemma 5.
16