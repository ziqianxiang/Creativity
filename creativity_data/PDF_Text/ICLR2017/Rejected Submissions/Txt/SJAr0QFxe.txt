Under review as a conference paper at ICLR 2017
Demystifying ResNet
Sihan Li
Department of Electronic Engineering
Tsinghua University
Beijing 100084, China
lisihan13@mails.tsinghua.edu.cn
Jiantao Jiao, Yanjun Han, Tsachy Weissman
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
{jiantao,yjhan,tsachy}@stanford.edu
Ab stract
We provide a theoretical explanation for the great performance of ResNet via the
study of deep linear networks and some nonlinear variants. We show that with
or without nonlinearities, by adding shortcuts that have depth two, the condi-
tion number of the Hessian of the loss function at the zero initial point is depth-
invariant, which makes training very deep models no more difficult than shallow
ones. Shortcuts of higher depth result in an extremely flat (high-order) stationary
point initially, from which the optimization algorithm is hard to escape. The 1-
shortcut, however, is essentially equivalent to no shortcuts. Extensive experiments
are provided accompanying our theoretical results. We show that initializing the
network to small weights with 2-shortcuts achieves significantly better results than
random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of
deeper depth, from various perspectives ranging from final loss, learning dynam-
ics and stability, to the behavior of the Hessian along the learning process.
1 Introduction
Residual network (ResNet) was first proposed in He et al. (2015a) and extended in He et al. (2016).
It followed a principled approach to add shortcut connections every two layers to a VGG-style net-
work (Simonyan & Zisserman, 2014). The new network becomes easier to train, and achieves both
lower training and test errors. Using the new structure, He et al. (2015a) managed to train a network
with 1001 layers, which was virtually impossible before. Unlike Highway Network (Srivastava
et al., 2015a;b) which not only has shortcut paths but also borrows the idea of gates from LSTM
(Sainath et al., 2013), ResNet does not have gates. Later He et al. (2016) found that by keeping a
clean shortcut path, residual networks will perform even better.
Many attempts have been made to improve ResNet to a further extent. “ResNet in ResNet” (Targ
et al., 2016) adds more convolution layers and data paths to each layer, making it capable of repre-
senting several types of residual units. “ResNets of ResNets” (Zhang et al., 2016) construct multi-
level shortcut connections, which means there exist shortcuts that skip multiple residual units. Wide
Residual Networks (Zagoruyko & Komodakis, 2016) makes the residual network shorter but wider,
and achieves state of the art results on several datasets while using a shallower network. Moreover,
some existing models are also reported to be improved by shortcut connections, including Inception-
v4 (Szegedy et al., 2016), in which shortcut connections make the deep network easier to train.
Why are residual networks so easy to train? He et al. (2015a) suggests that layers in residual net-
works are learning residual mappings, making them easier to represent identity mappings, which
prevents the networks from degradation when the depths of the networks increase. However, Veit
et al. (2016) claims that ResNets are actually ensembles of shallow networks, which means they do
not solve the problem of training deep networks completely.
We propose a theoretical explanation for the great performance of ResNet. We concur with He et al.
(2015a) that the key contribution of ResNet should be some special structure of the loss function
that makes training very deep models no more difficult than shallow ones. Analysis, however, seems
non-trivial. Quoting He et al. (2015a):
“But if F has only a single layer, Eqn.(1) is similar to a linear layer: y = W1x + x, for which
we have not observed advantages. ”
1
Under review as a conference paper at ICLR 2017
“Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy from increased depth (as
shown on CIFAR-10), but are not as economical as the bottleneck ResNets. So the usage of bottle-
neck designs is mainly due to practical considerations. We further note that the degradation problem
of plain nets is also witnessed for the bottleneck designs. ”
Their empirical observations are inspiring. First, the 1-shortcuts mentioned in the first paragraph
do not work. Second, noting that the non-bottleneck ResNets have 2-shortcuts, but the bottleneck
ResNets use 3-shortcuts, one sees that shortcuts with depth three also do not work. Hence, a rea-
sonable theoretical explanation must be able to distinguish the 2-shortcut from shortcuts of other
depths, and clearly demonstrate why the 2-shortcuts are special and are able to ease the optimization
process so significantly for deep models, while shortcuts of other depths may not do the job.
Aiming at explaining the performance of 2-shortcuts, we need to eliminate other variables that may FIX
contribute to the success of ResNet. Indeed, one may argue that the deep structure of ResNet may
give it better representation power (lower approximation error), which contributes to lower training
errors. To eliminate this effect, we focus on deep linear networks, where deeper models do not have
better approximation properties. The special role of 2-shortcuts naturally arises in the study.
2 Main results
Our work reveals that non-degenerate depth-invariant initial condition numbers, a unique property
of residual networks with 2-shortcuts, contributed to the success of ResNet. In fact, in a linear
network that will be defined rigorously later, the condition number of Hessian of the Frobenius loss
function at the zero initial point is
Cond(H) = JCond((Σxx - Σyx)t(Σxx - Σyx)),	⑴
which is independent of the number of layers. Here ΣXX and ΣYX denote the input-input and
the output-input correlation matrices, defined in Section 3.3. The condition number of a possibly
non-PSD matrix is defined as:
Definition 1. The condition number of a matrix A is defined as
cond(A) = σmax(A,	(2)
σmin(A)
where σmax(A) and σmin (A) are the maximum and minimum of singular values ofA. In particular,
ifA is normal, i.e. ATA = AAT, the definition can be simplified to 1
ZVZn	lλ(A)ImaX
Cond(A)= mu，
(3)
where ∣λ(A) |max and ∣λ(A)岛由 are the maximum and minimum of the absolute values of eigenval-
ues of A.
Moreover, the zero initial point for ResNet with 2-shortcuts is in fact a so-called strict saddle
point (Ge et al., 2015), which are proved to be easy to escape from.
Why shortcuts of other depths do not work? We show that the Hessian at the zero initial point for the
1-shortcut ResNet has condition number growing unboundedly for deep nets. As is well known in
convex optimization theory, large condition numbers can have enormous adversarial impact on the
convergence of first order methods (Nemirovski, 2005). Hence, it is quite clear that starting training
at a point with a huge condition number would make the algorithm very difficult to escape from the
initial point, making 1-shortcut ResNet no better than conventional approaches.
1The equivalence of Equation (2) and Equation (3) can be proved easily using the eigenvalue decomposition
ofA. Note that as Hessians are symmetric (if all the second derivatives are continuous), we will use Equation (3)
for their condition numbers. As the ∣λ∣m⅛ ofHessian is usually very unstable, we calculated Mmax) to represent
condition numbers instead, where ∣λ∣(o.i) is the 10th percentile of the absolute values of eigenvalues. Note that
the Hessian at zero initial point for 2-shortcut ResNet also has a nice structure of spectrum: see Theorem 1 for
details.
2
Under review as a conference paper at ICLR 2017
For shortcuts with depth deeper than two, the Hessian at the zero initial point is a zero matrix, making
it a higher-order stationary point. Intuitively, the higher order the stationary point is, the harder it
is to escape from it. Indeed, it is supported both in theory (Anandkumar & Ge, 2016) and by our
experiments.
One may still ask: why are we interested in the Hessian at the zero initial point? It is because in order
for the outputs of deep neural networks not explode, the singular values of the mapping of each layer
are not supposed to be deviating too much from one. Indeed, it is because it is extremely challenging
to keep Qin=um1 of layers λi from exploding or vanishing without keeping all of the λi having unit norm.
However, by design ResNet with shortcuts already have an identity mapping every few layers, which
forces the mappings inside the shortcuts to have small operator norms. Hence, analyzing the network
at zero initial point gives a decent characterization of the searching environment of the optimization
algorithm.
Our results also shows that the form of Hessian is more important than the existance of nonlinearities FIX
when training the networks. The behaviors of the networks we studied are consistent across both
linear and nonlinear structures, where networks with clearer Hessians are much easier to achieve
lower training errors.
On the other hand, our experiments reveal that orthogonal initialization (Saxe et al., 2013) is sub-
optimal. Although better than Xavier initialization (Glorot & Bengio, 2010), the initial condition
numbers of the networks still explode as the networks become deeper, which means the networks
are still initialized on “bad” submanifolds that are hard to optimize using gradient descent.
3	Model
3.1	Deep linear networks
Deep linear networks are feed-forward neural networks that only contain linear units, which means
their input-output mappings are simply linear transformations. Apparently, increasing their depths
will not affect the representational power of the networks. However, linear networks with depth
deeper than one show nonlinear dynamics of training (Saxe et al., 2013). As a result, analyzing
the training of deep linear networks gives us a better understanding of the training of non-linear
networks.
Much theoretical work has been done on deep linear networks. Kawaguchi (2016) extended the
work of Choromanska et al. (2015a;b) and proved that with few assumptions, every local minimum
point in deep linear networks is a global minimum point. This means that the difficulties in the
training of deep linear networks mostly come from saddle points on the loss surfaces, which are also
the main causes of slow learning in nonlinear networks (Pascanu et al., 2014).
Saxe et al. (2013) studied the dynamics of training using gradient descent. They found that for a
special class of initial conditions, which could be obtained from greedy layerwise pre-training, the
training time for a deep linear network with an infinity depth can still be finite. Furthermore, they
found that by setting the initial weights to random orthogonal matrices (produced by performing QR
or SVD decompositions on random Gaussian matrices), the network will still have a depth indepen-
dent learning time. They argue that it is caused by the eigenvalue and singular value spectra of the
end-to-end linear transformation. When using orthogonal initialization, the overall transformation
is an orthogonal matrix, which has all the singular values equal to 1. In the meantime, when using
scaled Gaussian initialization, most of the singular values are close to zero, making the network
unsuitable for backpropagating errors. However, this explanation is not sufficient to prove that the
training difficulty of orthogonal initialized networks is depth-invariant. It only gives us an intuition
on why orthogonal initialization performs better than scaled Gaussian initialization.
Thus, we use deep linear networks to study the effect of shortcut connections. After adding the
shortcuts, the overall model is still linear and the global minimum does not change.
3.2	Network structure
We first generalize a linear network by adding shortcuts to it to make it a linear residual network.
We organize the network into R residual units. The r-th residual unit consists of Lr layers whose
3
Under review as a conference paper at ICLR 2017
weights are W r,1, . . . , Wr,Lr-1, denoted as the transformation path, as well as a shortcut Sr con-
necting from the first layer to the last one, denoted as the shortcut path. The input-output mapping
can be written as
R Lr-1
y=Y(Y Wr,l + Sr)x = Wx,	(4)
where x ∈ Rdx , y ∈ Rdy, W ∈ Rdy ×dx. Here if b ≥ a, Qib=a Wi denotes
WbW(b-1) •…W(a+1)Wa, otherwise it denotes an identity mapping. The matrix W represents the
combination of all the linear transformations in the network. Note that by setting all the shortcuts to
zeros, the network will go back to a (Pr(Lr - 1) + 1)-layer plain linear network.
Instead of analyzing the general form, we concentrate on a special kind of linear residual networks,
where all the residual units are the same.
Definition 2. A linear residual network is called an n-shortcut linear network if
1.	its layers have the same dimension (so that dx = dy);
2.	its shortcuts are identity matrices;
3.	its shortcuts have the same depth n.
The input-output mapping for such a network becomes
y=YR (Yn Wr,l+Idx)x=Wx,	(5)
r=1 l=1
where Wr,l ∈ Rdx ×dx.
Then we add some activation functions to the networks. We concentrate on the case where activation
functions are on the transformation paths, which is also the case in the latest ResNet (He et al., 2016).
Definition 3. An n-shortcut linear network becomes an n-shortcut network if element-wise activa-
tion functions σpre(x), σmid(x), σpost(x) are added at the transformation paths, where on a transfor-
mation path, σpre(x) is added before the first weight matrix, σmid(x) is added between two weight
matrixes and σpost (x) is added after the last weight matrix.
Figure 1: An example of different position for nonlinearities in a residual unit of a 2-shortcut net-
work.
Note that n-shortcut linear networks are special cases of n-shortcut networks, where all the activa-
tion functions are identity mappings.
3.3	Optimization
We denote the collection of all the variable weight parameters in an n-shortcut linear network as
w. Consider m training samples {xμ, yμ}, μ = 1,...,m. Using FrobeniUS loss, for an n-shortcut
linear network, we define the loss function as follows:
1m	1
L(w)=厂 Eky" - Wχμk2 =厂 kY - WXkF,	⑹
2m	2m
μ=1
4
Under review as a conference paper at ICLR 2017
where χμ, yμ are the μ-th columns of X, Y, and |卜|旧 denotes the FrobeniUs norm. Using gradient
descent with learning rate α, we have the weights updating rules as
∆W r,l =α(WarfterWarf,tler)T(ΣYX -WΣXX)(Wbre,lforeWbrefore)T,	(7)
where ΣXX and ΣYX denote the input-input and the output-input correlation matrices, defined as
m
Σxx = — X xμ(xμ)T	(8)
μ=1
m
∑yx = — X yμ(χμ)τ.	(9)
m
μ=1
Here Wbrefore , Warfter denote the linear mappings before and after the r-th residual unit,
Wbre,lfore, Warf,tler denote the linear mappings before and after Wr,l within the transformation path
of the r-th residual unit. In other words, the overall transformation can be represented as
y=Warfter(Warf,tlerWr,lWbre,lfore+Idx)Wbreforex.	(10)
4	Theoretical study
4.1	Initial point properties
Before we analyze the initial point properties of n-shortcut networks, we have to choose the way to
initialize them. ResNet uses MSRA initialization (He et al., 2015b). It is a kind of scaled Gaussian
initialization that tries to keep the variances of signals along a transformation path, which is also the
idea behind Xavier initialization (Glorot & Bengio, 2010). However, because of the shortcut paths,
the output variance of the entire network will actually explode as the network becomes deeper.
Batch normalization units partly solved this problem in ResNet, but still they cannot prevent the
large output variance in a deep network.
A simple idea is to zero initialize all the weights, so that the output variances of residual units
stay the same along the network. It is worth noting that as found in He et al. (2015a), the deeper
ResNet has smaller magnitudes of layer responses. This phenomenon has been confirmed in our
experiments. As illustrated in Figure 2 and Figure 3, the deeper a residual network is, the small
its average Frobenius norm of weight matrixes is, both during the traning process and when the
training ends. Also, Hardt & Ma (2016) proves that if all the weight matrixes have small norms, a
linear residual network will have no critical points other than the global optimum.
All these evidences indicate that zero is spacial in a residual network: as the network becomes
deeper, the training tends to end up around it. Thus, we are looking into the Hessian at zero. As the
zero is a saddle point, in our experiments we use zero initialization with small random perturbations
to escape from it. We first Xavier initialize the weight matrixes, and then multiply a small constant
(0,01) to them.
We begin with the definition of k-th order stationary point.
Definition 4. Suppose function f(x) admits k-th order Taylor expansion at point x0. We say that
the point x0 is a k-th order stationary point of f(x) if the corresponding k-th order Taylor expansion
of f(x) at x = x0 is a constant: f(x) = f(x0) + o(kx - x0 k2k).
Now we state our main theorem, whose proof can be found in Appendix A.
Theorem 1. Assume that σmid(0) = σpost(0) = 0 and all of σp(kre) (0), σm(ki)d(0), σp(ko)st(0), 1 ≤ k ≤
max(n - 1, 2) exist. For the loss function of an n-shortcut network, at point zero,
1. if n ≥ 2, it is an (n - 1)th-order stationary point. In particular, if n ≥ 3, the Hessian is a
zero matrix;
NEW
NEW
NEW
5
Under review as a conference paper at ICLR 2017
25
20
15
10
5
0
-3.0	-2.5	-2.0	-1.5	-1.0	-0.5	0.0	0.5
Ig(Ioss)
NEW
Figure 2:	The average Frobenius norms of ResNets of different depths during the training process.
The pre-ResNet implementation in https://github.com/facebook/fb.resnet.torch
is used. The learning rate is initialized to 0.1, decreased to 0.01 at the 81st epoch (marked with
circles) and decreased to 0.001 at the 122nd epoch (marked with triangles). Each model is trained
for 200 epochs.
3.5
3
2.5
2
N
Φ
CD
2
1.5
‹
1
0.5
depth = 2
-2.5	-2	-1.5	-1
log10(loss - lossopt)
mroN egarev
0 ---------------L_
-3.5	-3
NEW
Figure 3:	The average Frobenius norms of 2-shortcut networks of different depths during the training
process when zero initialized. Left: Without nonlinearities. Right: With ReLUs at mid positions.
6
Under review as a conference paper at ICLR 2017
2.	if n = 2, the Hessian can be written as
-O AT	-
A O
H =	0 AT ,	(11)
A O ,
whose condition number is
Cond(H) = JCond((∑xσPre(X) - ∑Yσpre(X))T (Σxσpre(X) - ∑Yσpre(X))),	(设)
where A only depends on the training set and the activation functions. Except for degener-
ate cases, it is a strict saddle point (Ge et al., 2015).
3.	if n = 1, the Hessian can be written as
B AT AT
A B AT
H = AAB
AT
AT
(13)
A A … A
AT
B
where A, B only depend on the training set and the activation functions.
Theorem 1 shows that the condition numbers of 2-shortcut networks are depth-invariant with a nice
structure of eigenvalues. Indeed, the eigenvalues of the Hessian H at the zero initial point are
multiple copies of ± ∙∖∕eigs(ATA), and the number of copies is equal to the number of shortcut
connections.
The Hessian at zero initial point for the 1-shortcut linear network follows block Toeplitz structure,
which has been well studied in the literature. In particular, its condition number tends to explode as
the number of layers increase (Gray, 2006).
The assumptions hold for most activation functions including tanh, symmetric sigmoid and ReLU
(Nair & Hinton, 2010). Note that although ReLU does not have derivatives at zero, one may do a
local polynomial approximation to yield σ(k), 1 ≤ k ≤ max(n - 1, 2).
To get intuitive explanations of the theorems, imagine changing parameters inan n-shortcut network.
One has to change at least n parameters to make any difference in the loss. So zero is an (n - 1)th-
order stationary point. Notice that the higher the order of a stationary point, the more difficult for a
first order method to escape from it.
On the other hand, if n = 2, one will have to change two parameters in the same residual unit but
different weight matrices to affect the loss, leading to a clear block diagonal Hessian.
4.2	Learning dynamics
To understand Equation (7) better, we can take n-shortcut linear networks to two extremes. First,
when n = 1, let V r,1 = Wr,1 + Idx , r = 1, . . . , R - 1. As Idx is a constant, we have
R-1	R-1	r-1
∆V r,1 =α( Y Vr0,1)T(ΣYX - (Y Vr0,1)ΣXX)(Y V r0,1)T,	(14)
r0=r+1	r0=1	r0=1
which can be seen as a linear network with identity initialization, a special case of orthogonal ini-
tialization, if the original 1-shortcut network is zero initialized.
On the other side, if the number of shortcut connections R = 1, the shortcut will only change the
distribution of the output training set from Y to Y - X . These two extremes are illustrated in
Figure 4
7
Under review as a conference paper at ICLR 2017
Figure 4: Equivalents of two extremes of n-shortcut linear networks. 1-shortcut linear networks are
equivalent to linear networks with identity initialization, while skip-all shortcuts will only change
the effective dataset outputs.
4.3	Learning results
The optimal weights ofan n-shortcut linear network can be easily computed via least squares, which
leads to
W = YXT(XXT)-1 = ΣYX(ΣXX)-1,	(15)
and the minimum of the loss function is
Lmin = 1- 1丫 — ∑YX (∑xx )-1XkF,	(16)
2m
where ∣∣∙∣∣f denotes the FrobeniUs norm and (Σxx)-1 denotes any kind of generalized inverse of
ΣXX . So given a training set, we can pre-compute its Lmin and use it to evaluate any n-shortcut
linear network.
5	Experiments
We compare networks with Xavier initialization (Glorot & Bengio, 2010), networks with orthogo-
nal initialization (Saxe et al., 2013) and 2-shortcUt networks with zero initialization. The training
dynamics of 1-shortcUt networks are similar to that of linear networks with orthogonal initialization
in oUr experiments. SetUp details can be foUnd in Appendix B.
5.1	Initial point
We first compUte the initial condition nUmbers for different kinds of linear networks with different
depths.
As can be seen in FigUre 5, 2-shortcut linear networks have constant condition numbers as
expected. On the other hand, when Using Xavier or orthogonal initialization in linear networks, the
initial condition nUmbers will go to infinity as the depths become infinity, making the networks hard
to train. This also explains why orthogonal initialization is helpfUl for a linear network, as its initial
condition nUmber grows slower than the Xavier initialization.
5.2	Learning dynamics
Having a good beginning does not gUarantee an easy trip on the loss sUrface. In order to depict
the loss sUrfaces encoUntered from different initial points, we plot the maxima and 10th percentiles
(instead of minima, as they are very Unstable) of the absolUte valUes of Hessians eigenvalUes at
different losses.
As shown in FigUre 6 and FigUre 7, the condition nUmbers of 2-shortcUt networks at different losses
are always smaller, especially when the loss is large. Also, notice that the condition nUmbers roUghly
evolved to the same valUe for both orthogonal and 2-shortcUt linear networks. This may be explained
by the fact that the minimizers, as well as any point near them, have similar condition nUmbers.
8
Under review as a conference paper at ICLR 2017
QBqEnU u94-PUou)-
o
o.o
0.5	1.0	1.5	2.0	0.0
∣g(depth)
0.5	1.0	1.5	2.0
lg(depth)
6 4 2
(-6φ-)6-
Figure 5: Initial condition numbers of Hessians for different linear networks as the depths of the
networks increase. Means and standard deviations are estimated based on 10 runs.
0
—3	—2	—1
lg(loss - opt loss)
Figure 6:	Maxima and 10th percentiles of absolute values of eigenvalues at different losses when the
depth is 16. For each run, eigenvalues at different losses are calculated using linear interpolation.
10
(一 6ω)q
-5
-10
-15 l-
-1.4
Maximum
Orthogonal
一	,∙<w¾-√-
2-Shortcut ∙
-1.3
-1.1
10th percentile
-1.2
lg(loss)
-1
5
0
NEW
Figure 7:	Maxima and 10th percentiles of absolute values of eigenvalues at different losses when the
depth is 16. Eigenvalues at different losses are calculated using linear interpolation.
9
Under review as a conference paper at ICLR 2017
Another observation is the changes of negative eigenvalues ratios. Index (ratio of negative eigen-
values) is an important characteristic of a critical point. Usually for the critical points of a neural
network, the larger the loss the larger the index (Dauphin et al., 2014). In our experiments, the in-
dex of a 2-shortcut network is always smaller, and drops dramatically at the beginning, as shown in
Figure 8, left. This might make the networks tend to stop at low critical points.
4 2 0 8 6
5-5 4 4
sA4e6u ⅞ 0-⅛<-
30
O
QUω一 pe」9-
Figure 8:	Left: ratio of negative eigenvalues at different losses when the depth is 16. For each run,
indexes at different losses are calculated using linear interpolation. Right: the dynamics of gradient
and index of a 2-shortcut linear network in a single run. The gradient reaches its maximum while
the index drops dramatically, indicating moving toward negative curvature directions.
This is because the initial point is near a saddle point, thus it tends to go towards negative curvature
directions, eliminating some negative eigenvalues at the beginning. This phenomenon matches the
observation that the gradient reaches its maximum when the index drops dramatically, as shown in
Figure 8, right.
5.3 Learning results
We run different networks for 1000 epochs using different learning rates at log scale, and compare
the average final losses of the optimal learning rates.
-1 J
0.5
1.5
2-Shortcut, ♦
0.5
Xavier
0
-0.5
3-Shortcut
I-Shortcut
Orthogonal ∖
2
1	1.5
lg(depth)
NEW
Figure 9:	Left: Optimal Final losses of different linear networks. Right: Corresponding optimal
learning rates. When the depth is 96, the final losses of Xavier with different learning rates are
basically the same, so the optimal learning rate is omitted as it is very unstable.
Figure 9 shows the results for linear networks. Just like their depth-invariant initial condition num-
bers, the final losses of 2-shortcut linear networks stay close to optimal as the networks become
deeper. Higher learning rates can also be applied, resulting in fast learning in deep networks.
Then we add ReLUs to the mid positions of the networks. To make a fair comparison, the numbers
of ReLU units in different networks are the same when the depths are the same, so 1-shortcut and
3-shortcut networks are omitted. The result is shown in Figure 10.
10
Under review as a conference paper at ICLR 2017
5
0
(SSo = eu!E6_
-2∙8.5
1.0	1.5	2
lg(depth)
一 - - 5
3 Q-Oo
LLe5
6」6u 三」二 eEQdo)O
2-Shortcut-ReLU :
Xavier-ReLU
Orthogonal-ReLU
1.0	1.5	2.0
lq(depth)
Figure 10:	Left: Optimal Final losses of different networks with ReLUs in mid positions. Right:
Corresponding optimal learning rates. Note that as it is hard to compute the minimum losses with
ReLUs, we plot the log10 (final loss) instead of log10 (final loss - optimal loss). When the depth is
64, the final losses of Xavier-ReLU and orthogonal-ReLU with different learning rates are basically
the same, so the optimal learning rates are omitted as they are very unstable.
Note that because of the nonlinearities, the optimal losses vary for different networks with different
depths. It is usually thought that deeper networks can represent more complex models, leading
to smaller optimal losses. However, our experiments show that linear networks with Xavier or
orthogonal initialization have difficulties finding these optimal points, while 2-shortcut networks
find these optimal points easily as they did without nonlinear units.
6 Future Directions
Further studies should concentrate on the behavior of shortcut connections on convolution networks,
as well as the influences of batch normalization units (Ioffe & Szegedy, 2015) in ResNet. Mean-
while, it would be very interesting to extend the insights obtained in this paper to recurrent neural
networks such as LSTM (Sainath et al., 2013).
References
Anima Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points in
non-convex optimization. arXiv preprint arXiv:1602.05908, 2016.
Chris Bishop. Exact calculation of the hessian matrix for the multilayer perceptron. Neural Compu-
tation, 4(4):494-501,1992.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In AISTATS, 2015a.
Anna Choromanska, Yann LeCun, and Gerard Ben Arous. Open problem: The landscape of the
loss surfaces of multilayer networks. In Proceedings of The 28th Conference on Learning Theory,
COLT 2015, Paris, France, July 3, volume 6, pp. 1756-1760, 2015b.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pp. 797-842, 2015.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Aistats, volume 9, pp. 249-256, 2010.
Robert M Gray. Toeplitz and circulant matrices: A review. now publishers inc, 2006.
11
Under review as a conference paper at ICLR 2017
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. arXiv preprint arXiv:1512.03385, 2015a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE International
Conference on Computer Vision, pp.1026-1034, 2015b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. arXiv preprint arXiv:1603.05027, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Kenji Kawaguchi. Deep learning without poor local minima. arXiv preprint arXiv:1605.07110,
2016.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807-814,
2010.
Arkadi Nemirovski. Efficient methods in convex programming. 2005.
Razvan Pascanu, Yann N Dauphin, Surya Ganguli, and Yoshua Bengio. On the saddle point problem
for non-convex optimization. arXiv preprint arXiv:1405.4604, 2014.
T.N. Sainath, O. Vinyals, A. Senior, and H. Sak. Convolutional, Long short-term memory, fully
connected deep neural networks. Journal of Chemical Information and Modeling, 53(9):1689-
1699, 2013. ISSN 1098-6596. doi: 10.1017/CBO9781107415324.004.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
RUPesh K Srivastava, KlaUs Greff, and Jurgen Schmidhuber. Training very deep networks. In
Advances in neural information processing systems, pp. 2377-2385, 2015a.
Rupesh Kumar Srivastava, Klaus Greff, and JUrgen Schmidhuber. Highway networks. arXivpreprint
arXiv:1505.00387, 2015b.
Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. Inception-v4, Inception-ResNet and the
Impact of Residual Connections on Learning. feb 2016. URL http://arxiv.org/abs/
1602.07261.
Sasha Targ, Diogo Almeida, and Kevin Lyman. Resnet in resnet: Generalizing residual architectures.
arXiv preprint arXiv:1603.08029, 2016.
Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks are exponential ensembles
of relatively shallow networks. arXiv preprint arXiv:1605.06431, 2016.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Ke Zhang, Miao Sun, Tony X Han, Xingfang Yuan, Liru Guo, and Tao Liu. Residual networks of
residual networks: Multilevel residual networks. arXiv preprint arXiv:1608.02908, 2016.
12
Under review as a conference paper at ICLR 2017
A Proofs of theorems
Definition 5. The elements in Hessian of an n-shortcut network is defined as
∂2L
∂w1 ∂w2
(17)
where L is the loss function, and the indices ind(∙) is ordered lexicographically following the four
indices (r, l,j, i) of the weight variable wir,,jl. In other words, the priority decreases along the index
of shortcuts, index of weight matrix inside shortcuts, index of column, and index of row.
Note that the collection of all the weight variables in the n-shortcut network is denoted as w. We
study the behavior of the loss function in the vicinity of w = 0.
Lemma 1. Assume that wι = WrIjI ,•…,wn = WrNjN are N parameters of an n-shortcut
newrk I ∂W⅛NL=0
is nonzero, there exists r and kι, ∙∙∙ ,kn such that rkm
r and lkm = m
for m = 1, ∙∙∙ ,n.
Proof. Assume there does not exist such r and kι, ∙∙∙ ,kn, then for all the shortcut units r =
1,…，R, there exists a weight matrix l such that none of wι, ∙ ∙ ∙ ,wn is in Wr,1, so all the trans-
formation paths are zero, which means W = Idx. Then ∂w∂∙2Lwn |	= 0, leading to a contradic-
tion.	□
Lemma 2. Assume that W1 = Wir1,,j11 , W2 = Wir2,,j12, r1 ≤ r2. Let L0(W1, W2) denotes the loss
function with all the parameters except wι and w2 set to 0, wj = WYjl ,w2 = w1+~j2(rι=r2),12.
Then
∂2L0(w1,w2)
∂w1∂w2
I	— ∂2L0(wl ,w2 ) I
l(w1,w2) = 0 — —∂w0∂w2 一 |(w1 ,w2 ) = 0.
Proof. As all the residual units expect unit r1 and r2 are identity transformations, reordering
residual units while preserving the order of units r1 and r2 will not affect the overall trans-
formation, i.e. L0(wι, w2 ) | wι =a,w2 = b = L0(W1，W2)|w1 =a,w2=b. So ∂wj∂w2~| (wj ,w2 ) = 0 =
d2L0(w1,w2) | / /	/、c	∏
∂w0∂w2	|(w1 ,w2 )=0∙	LJ
Proof of Theorem 1. Now we can prove Theorem 1 with the help of the previously established lem-
mas.
1. Using Lemma 1, for an n-shortcut network, at zero, all the k-th order partial derivatives of
the loss function are zero, where k ranges from 1 to n - 1. Hence, the initial point zero is
a (n - 1)th-order stationary point of the loss function.
2. Consider the Hessian in n = 2 case. Using Lemma 1 and Lemma 2, the form of Hessian
can be directly written as Equation (11), as illustrated in Figure 11.
So we have
eigs(H) = eigs( A A)) = ±Jeigs(ATA).
(18)
Thus Cond(H) = ,cond(ATA), which is depth-invariant. Note that the dimension of A
is d2x × d2x .
To get the expression of A, consider two parameters that are in the same residual unit but
different weight matrices, i.e. W1 = Wir,2,j , W2 = Wir,1,j .
13
Under review as a conference paper at ICLR 2017
LIc∩TOnpω投CNIc∩ronpωφQ≤
Figure 11: The Hessian in n = 2 case. It follows from Lemma 1 that only off-diagonal subblocks in
each diagonal block, i.e., the blocks marked in orange (slash) and blue (chessboard), are non-zero.
From Lemma 2, we conclude the translation invariance and that all blocks marked in orange (slash)
(resp. blue (chessboard)) are the same. Given that the Hessian is symmetric, the blocks marked in
blue and orange are transposes of each other, and thus it can be directly written as Equation (11).
If j1 = i2 , we have
∂2L
CjI-I)dx+iι,Cj2-I)dx+i2 = ∂w1∂w2 lw=0
=d2 Pm=I 21m (yμ -琮-σpost(W1σmid(W2σpre(Xr))))2 I
∂w1∂w2	w=0
σ0nid ⑼σpost(0) XX	μ μ μ μ、
=-----------------2^σpre(XU(XiI - yι ).
m	μ=1
(19)
Else, we have ACj1-1)dx+i1,Cj2-1)dx+i2 = 0.
Noting that ACj1-1)dx+i1,Cj2-1)dx+i2 in fact only depends on the two indices i1, j2
(with a small difference depending on whether j1 = i2), we make a dx ×
dx matrix with rows indexed by i1 and columns indexed by j2, and the entry
at (i1,j2) equal to ACj1-1)dx+i1,Cj2-1)dx+i2. Apparently, this matrix is equal to
σm0 id(0)σp0 ost(0)(ΣXσpreCX) - ΣY σpre CX)) when j1 = i2, and equal to the zero matrix
when j1 6= i2 .
To simplify the expression of A, we rearrange the columns of A by a permutation matrix,
i.e.
A0 = AP,	(20)
where Pij = 1 if and only if i = ((j - 1) mod dχ)dχ + d j]. Basically it permutes the
i-th column of A to the j -th column.
Then we have
-∑Xσpre(X) - ∑Yσpre(X)	-
A = σ0n id(0)σP ost(0)	...	PT
ΣXσpreCX) - ΣYσpre CX)
(21)
14
Under review as a conference paper at ICLR 2017
So the eigenvalues of H becomes
eigs(H ) = ±σ0n id(0)σp ost(0),eigs((∑xσPre(X ) - ∑Yσpre(X))T(Σxσpre(X) - ∑Yσpre(X))),
(22)
which leads to Equation (12).
3. Now consider the Hessian in the n = 1 case. Using Lemma 2, the form of Hessian can be
directly written as Equation (13).
To get the expressions of A and B in σpre(x) = σpost (x) = x case, consider two parame-
ters that are in the same residual units, i.e. w1 = wir,1,j , w2 = wir,1,j .
We have
B	=	d2L	I
CjLI)dx+ii,(j2-I)dχ+i2	∂w1∂w2 1w=0
=(m pm=ι 吟W2
=0
Rearrange the order of variables using P , we have
B = p[ςxx ..	Ipt.
.
ΣXX
i1 = i2
i1 6= i2
(23)
(24)
(25)
Then consider two parameters that are in different residual units, i.e. w1 = wir1,,j1 , w2
r2 ,1
wi22,,j2, r1 >r2.
We have
A(j1 -1)dx +i1,(j2 -1)dx +i2
∂2L
∂w1 ∂w2 Iw=0
(26)
PPP
1- ml- ml-mO
ΓIJ∖I
=
μ
i1
lM
μ μ
1 xj1 xj2
yiμ1)xjμ2+xjμ1xjμ2
yiμ1)xjμ2
j1 = i2 , i1 = i2
j1 = i2 , i1 6= i2
j1 6= i2 , i1 = i2
j1 6= i2 , i1 6= i2
(27)
—
—
In the same way, we can rewrite A as
ΣXX - ΣYX
A
PT + B.
(28)
ΣXX - ΣYX
□
B	Experiment setup
We took the experiments on whitened versions of MNIST. 10 greatest principal components are kept
for the dataset inputs. The dataset outputs are represented using one-hot encoding. The network
was trained using gradient descent. For every epoch, the Hessians of the networks were calculated
using the method proposed in (Bishop, 1992). As the ∣λ∣mi∩ of Hessian is usually very unstable, We
calculated Mmax) to represent condition number instead, where ∣λ∣(o.i) is the 10th percentile of the
absolute values of eigenvalues.
As pre, mid or post positions are not defined in linear networks without shortcuts, when comparing
Xavier or orthogonal initialized linear networks to 2-shortcut networks, we added ReLUs at the same
positions in linear networks as in 2-shortcuts networks.
15