Under review as a conference paper at ICLR 2017
Dynamic Partition Models
Marc Goessling	Yali Amit
Department of Statistics	Departments of Statistics and Computer Science
University of Chicago	University of Chicago
Chicago, IL 60637, USA	Chicago, IL 60637, USA
goessling@galton.uchicago.edu amit@galton.uchicago.edu
Ab stract
We present a new approach for learning compact and intuitive distributed rep-
resentations with binary encoding. Rather than summing up expert votes as in
products of experts, we employ for each variable the opinion of the most reliable
expert. Data points are hence explained through a partitioning of the variables
into expert supports. The partitions are dynamically adapted based on which ex-
perts are active. During the learning phase we adopt a smoothed version of this
model that uses separate mixtures for each data dimension. In our experiments we
achieve accurate reconstructions of high-dimensional data points with at most a
dozen experts.
1 Introduction
We consider the task of learning a compact binary representation (e.g. Goessling & Amit, 2015).
That means we are seeking a parsimonious set of experts, which can explain a given collection of
multivariate data points. In contrast to most existing approaches the emphasis here is on finding
experts that are individually meaningful and that have disjoint responsibilities. Ideally, each expert
explains only one factor of variation in the data and for each factor of variation there is exactly one
expert that focuses on it.
Formally, the experts Pk, k = 1, . . . , K, are probability distributions that depend on binary latent
variables h(k). The latent state h specifies which experts are active and has to be inferred for each
D-dimensional data point x. The active experts then define a probability distribution P. The goal
of representation learning is to train experts such that the conditional likelihood P(x | h) of the data
given the latent activations is maximized.
We start by describing a simple model family, which forms the basis of our work. A partition model
(Hartigan, 1990) makes use of a manually specified partitioning of the D variables into subsets
L
{1 ,...,D} = [ S'.
' =1
For each subset of variables x (S') = (x (d)) d∈S' there exists a separate model P`. It is then typically
assumed that variables in different subsets are conditionally independent, i.e.,
L
P(XI h) = Y P`(X(S') I h(')).	(1)
'=1
The model is completed by specifying a prior distribution P(h) for the latent state h. One advantage
of partition models is that estimating P' from observations is straightforward, while learning expert
models in general requires computationally involved procedures (Bengio et al., 2013). However, in
order to be able to define a satisfactory partitioning of the variables some prior knowledge about
the dependence structure is needed. For image data a common choice is to use a regular grid that
divides the image into patches (e.g. Pal et al., 2002). In general, a good partitioning is characterized
by providing weakly dependent subsets of variables so that the conditional independence assumption
(1) is reasonable and the distribution of the latent variables is easy to model. Unfortunately, often
there simply is no single fixed partitioning that works well for the whole dataset because the set
1
Under review as a conference paper at ICLR 2017
of variables, which are affected by different factors of variation, might overlap. This restricts the
scenarios in which partition models are useful.
In this paper we extend partition models to allow for dynamically adapting partitionings. In Section
2 we introduce the model and present an appropriate learning procedure. Related work is discussed
in Section 3. Special emphasis is given to the comparison with products of experts (Hinton, 2002).
Experiments on binary and real-valued data are performed in Section 4. While it is important to
explain high-dimensional data points through multiple experts, our work shows that it is possible
to assign the responsibility for individual variables to a single expert (rather than having all active
experts speak for every variable).
2 Dynamic partition models
Our main proposal is to define for each expert Pk its level of expertise ek ∈ R+D for all variables.
We can then dynamically partition the variables based on the active experts. Specifically, for each
variable we employ the most reliable (active) expert
D
P(x|h) =	Pk?(d)(x(d)),
d=1
k? (d) = argmax ek (d).
k:h(k)=1
(2)
That means, each variable x(d) is explained by only a single expert k? (d). The partitioning into
expert supports Sk (h) = {d ∈ {1, . . . , D} : k? (d) = k} is determined dynamically based on the
latent configuration h. We hence call our model a dynamic partition model.
2.1	Inference
In the inference step we try to find for each data point xn the subset of experts {k : hn(k) = 1} that
maximizes P(xn | hn). To do this, we suggest to sequentially activate the expert that most improves
the likelihood, until the likelihood cannot be improved anymore. This approach is called likelihood
matching pursuit (Goessling & Amit, 2015). The greedy search works well for our model because
we are working with a small set of experts and each expert focuses on a rather different structure
in the data. Consequently, the posterior distribution on the latent variables given xn is often highly
peaked at a state hn (note that for high-dimensional data the effect of the prior P(h) is typically
negligible).
2.2	Learning
In contrast to traditional approaches, which combine multiple experts for individual variables, train-
ing the experts in a dynamic partition model is trivial. Indeed, the maximum-likelihood estimates
are simply the empirical averages over all observations for which the expert was responsible. For
example, the expert means can be estimated from training data xn, n = 1, . . . , N, as
N
Σ 1 {谓(d)= k}Xn (d)
μk(d) = n=1N-----------------	⑶
P i{kn(d)=k}
n=1
Here, kn? (d) denotes the expert with the highest level of expertise ek(d) among all experts k with
hn(k) = 1.
2.2.1	Expertise-weighted composition
In order to compute the estimator in (3) the levels of expertise ek have to be known. Since in this
paper we are trying to train the experts as well as the associated levels of expertise we consider a
smoothing of the maximum-expertise composition (2) to motivate our learning procedure. Rather
than using the expert with the highest level of expertise, we form a mixture of the active experts,
where the mixture weight is proportional to the level of expertise. Thus, the smoothed composition
2
Under review as a conference paper at ICLR 2017
rule is
〜	DK	e V-__ek(d)-E if h (k) = 1
Pe(x | h) =	rk(d)Pk(x(d)),	rk(d)	PP k0:h(k0 ) = 1 ek0(d)	= .	(4)
d=1 k=1	0	if h(k) = 0
In contrast to classical mixture models (e.g. McLachlan & Peel, 2004) we use different mixture
weights for each dimension d ∈ {1, . . . , D}. The mixture weight rk(d) is the degree of responsibil-
ity of k-th expert for the d-th dimension and depends on the latent state h. An expert with a medium
level of expertise assumes full responsibility ifno other reliable expert is present and takes on a low
degree of responsibility if experts with a higher level of expertise are present.
According to the total variance formula
V[Pe] = Erk[V[Pk]] +Vrk[E[Pk]]
the variance of a mixture is always larger than the smallest variance of its components. In other
words, the precision of the smoothed model is maximized when all the mixture weight (individually
for each dimension) is concentrated on the most precise expert. We can thus learn a dynamic parti-
tion model in an EM manner (Dempster et al., 1977) by interleaving inference steps with updates of
the experts and levels of expertise in the smoothed model.
2.2.2	Expert update
The sequential inference procedure (from Section 2.1) provides for each data point xn the latent rep-
resentation hn . We denote the corresponding expert responsibilities (using the current estimates for
the level of expertise) by rnk . The smooth analog to the hard update equation (3) is a responsibility-
weighted average of the training samples
N
E Tnk(d) Xn (d) + βμ 0
μk(d) = n=1-N--------------------	(5)
rnk(d) +
n=1
For stability We added a term that shrinks the updated templates towards some target μo if the total
responsibility of the expert is small. In our experiments we set μo to the average of all training
examples. The update rule implies that the experts have local supports, in the sense that they are
uninformative about variables for which they are not responsible.
For binary data the mean templates μk are all we need. Continuous data x ∈ RD is modeled
through Gaussians and hence we also have to specify the variance vk of the experts. We again use a
responsibility-weighted average
N
P rnk(d)(Xn(d) — μk(d))2 + evo
Vk(d) = n=1--------N----------------------,	(6)
rnk(d) + e
n=1
where v0 is the empirical variance of all training samples.
2.2.3	Expertise update
We now turn to the updates of the levels of expertise. The log-likelihood of the smoothed model
(4) as a function of ek is rather complex. Using gradient descent is thus problematic because the
derivatives with respect to ek can have very different scales, which makes it difficult to choose an
appropriate learning rate and hence the convergence could be slow. However, exact optimization is
not necessary because in the end only the order of the levels of expertise matters. Consequently, we
propose to adjust ek(d) only based on the sign of the gradient. We simply multiply or divide the
current value by a constant C. If the gradient is very close to 0 we leave ek(d) unchanged. For all
our experiments we used C = 2. Larger values can speed up the convergence but sometimes lead to
a worse solution. Using an exponential decay is common practice when learning levels of expertise
(e.g. Herbster & Warmuth, 1998).
In the learning procedure we perform the expertise update first. We then recompute the responsibil-
ities using these new levels of expertise and update the experts. Our algorithm typically converges
after about 10 iterations.
3
Under review as a conference paper at ICLR 2017
3	Related work
Herbster & Warmuth (1998) proposed an algorithm for tracking the best expert in a sequential pre-
diction task. In their work it is assumed that a linear ordering of the variables is known such that
the expert with the highest level of expertise is constant on certain segments. In contrast to that,
our approach can be applied to an arbitrary permutation of the variables. Moreover, they consider
a single sequence of variables with a fixed partitioning into experts supports. In our setup the par-
titioning changes dynamically depending on the observed sample. However, the greatest difference
to our work is that Herbster & Warmuth (1998) do not learn the individual experts but only focus on
training the levels of expertise.
Lucke & Sahani (2008) studied a composition rule that also partitions the variables into expert
supports. In their model the composed template is simply the maximum of the experts templates
μk. This rule is only useful in special cases. A generalization, in which the composition depends
on the maximum and the minimum of the expert templates μk(d), was considered by Goessling &
Amit (2015). While the motivation for that rule was similar, the maximum-expertise rule in this
paper is more principled and can be applied to continuous data.
In the work by Amit & TrOUve (2007) a simple average (i.e., an equal mixture) of the individual
templates was used. With such a composition rule, all experts are equally responsible for each of the
variables and hence specialization on local structures is not possible. To circumvent this problem,
in their work ek(d) was manually set to 1 for some subset of the dimensions (depending on a latent
shift variable) and to 0 elsewhere.
A popular model family with latent binary representation are products of experts (Hinton, 2002). In
such a model the individual distributions Pk are multiplied together and renormalized. Computation
of the normalizing constant is in general intractable though. A special case, in which an explicit
normalization is possible, are restricted Boltzmann machines (Hinton, 2002). In these models the
experts are product Bernoulli distributions with templates μk ∈ [0, 1] D. The composed distribution
is then also a product Bernoulli distribution with composed template
μ RBMm ) = σ (X k ： h( k )=ι wk (d))，
where the weights wk(d) = log(μk(d)/(1 一 μk(d)) ∈ R are the log-odds of the experts and
σ(t) = (1 + exp(-t))-1 is the logistic function. This sum-of-log-odds composition rule arises
naturally from generalized linear models for binary data because the log-odds are the canonical
parameter of the Bernoulli family. In a product of experts, the variance of the composition is usually
smaller than the smallest variance of the experts. As a consequence, products of experts tend to
employ many experts for each dimension (for more details on this issue see Goessling & Amit
(2015)). Even with an L1-penalty on the votes wk(d) the responsibility for individual variables
x(d) is typically still shared among many experts. The reason for this is that under the constraint
Pk wk(d) = w(d) the quantity Pk |wk(d)| is minimized whenever wk(d) has the same sign for
all k. The usual inference procedure for products of experts independently activates experts based on
their inner product with the data point. In particular, not just the most probable expert configuration
is determined but the whole posterior distribution on latent states given the data is explored through
Monte Carlo methods. For learning in products of experts, simple update rules like (5) and (6) cannot
be used because for each expert the effects of all other experts have to be factored out. Dynamic
partition models essentially decompose the expert votes wk into expert opinions μk and levels of
expertise ek . Apart from the computational advantages for learning, this introduces an additional
degree of flexibility because the expert supports are adjusted depending on which other experts are
present (cf. Figure 5). Moreover, the decomposition into opinions and levels of expertise avoids
ambiguities. For example, a vote wk (d) ≈ 0 could mean that μk(d) ≈ 1 /2 or that ek(d) ≈ 0.
Another common model for representation learning are autoencoders (Vincent et al., 2008), which
can be considered as mean-field approximations of restricted Boltzmann machines that use latent
variables h(k) with values in [0, 1]. To obtain a sparse representation a penalty on the number of
active experts can be added (Ng, 2011). Such approaches are also known as sparse dictionaries
(e.g., Elad, 2010) and are based on opinion pools of the form Pk h(k)wk(d). The strength of
the sparsity penalty is an additional tuning parameter which has to be tuned. In dynamic partition
models sparse activations are inherent. In the next section, we experimentally compare products of
experts, autoencoders and sparse dictionaries to our proposed model.
4
Under review as a conference paper at ICLR 2017
corresponds to μk (d) = 0/1) of the 10 experts (rows) for the 10 dimensions (columns). 1st panel:
Random initialization. 2nd-4th panel: Our learning procedure after 3/5/15 iterations.
panel), a sparse dictionary (2nd panel) and a restricted Boltzmann machine (3rd panel).
4	Experiments
4.1	Synthetic data
We consider a synthetic example and try to learn the underlying factors of variation. The dataset
consists of the 32-element subset {(0, 1), (1, 0)}5 ⊂ {0, 1}10. Note that there are 5 factors of
variation corresponding to the state of the pairs (x(2'— 1), x(2')) for ' = 1,..., 5 with the two
factor levels (0, 1) and (1, 0). Indeed, the distribution can be easily expressed through a partition
model with partitioning
{1,2}∪{3,4}∪{5,6}∪{7,8}∪{9,10}
and corresponding models
Pg(x(2'-1), x(2')) = 2 ∙ l{x(2'-1)=0, x(2')=1} + 1 ∙ l{x(2'-1)=1, x(2')=0}.
We show that our dynamic partition model is able to learn these factors of variation without requiring
a manual specification of the partitioning. Here, the total number of experts we need to accurately
reconstruct all data points happens to be equal to the number of dimensions. However, in other cases
the number of required experts could be smaller or larger than D . We ran our learning algorithm
for 15 iterations starting from a random initialization of the experts. The resulting templates after
3, 5 and 15 iterations are shown in Figure 1. We see that each of the final experts specializes in
exactly two dimensions d and d+ 1. Its opinion for these variables are close to 0 and 1, respectively,
while the opinions for the remaining variables are about 1/2. Every data point can now be (almost)
perfectly reconstructed by using exactly 5 of these experts.
For comparison we trained various other models with 10 experts, which use a sum-of-log-odds
composition. We first tried an autoencoder (Vincent et al., 2008), which in principle could adopt
the identity map because it uses (in contrast to our model) a bias term for the observable and latent
variables. However, the gradient descent learning algorithm with tuned step size yielded a different
representation (Figure 2, 1st panel). While the reconstruction errors are rather low, they are clearly
nonzero and the factors of variations have not been disentangled. Next, we considered a dictionary
with a sparse representation (e.g., Elad, 2010). The sparsity penalty was adjusted so that the average
number of active dictionary elements was around 5. The learning algorithm again yielded highly
dependent experts (Figure 2, 2nd panel). Finally, we trained a restricted Boltzmann machine through
batch persistent contrastive divergence (Tieleman, 2008) using a tuned learning rate. Note that a
5
Under review as a conference paper at ICLR 2017
4 76y，丫夕 9Oa
夕 23 F3oa4fπf∙≠
5 2 勺。2zx3 q £
sraGl5多 qg9
/⅛gqo*6 78J
,拜：3 g 3 G÷7 7。
q 二，「4T 6r3 3 a
/rQΓ¾∕e√3∕7
Γ S O 6 7 ÷ ʒ O /
Figure 3:	Trained experts for MNIST digits. Left: Expert probabilities (white/black corresponds to
μk (d) = 0/1). Right: Levels of expertise (blue/red corresponds to small/large values).
G066bb^
9夕y夕YVVYYYY
G4>66GG66&6
7 T- T-1-1 T- 7
Oo夕。。
222EZiNN22z
，Fb KS33333533
3666δ666666
ΓV》Γl-«•》u》r)r“r》r》fʃ
q qqq q M rr q q τr q
5ΓH0555SaU<J55
9999夕夕9q77o
in > I
?Q Q NΛɑN N N 2 Z
/f /f »/ tt TJ tj 1/
q中9999?IqOG
，辛？77 H777?
2 2 2 V⅜ V⅜ £ 5 3 V4 W 5
Figure 4:	Reconstruction of MNIST test examples using likelihood matching pursuit. Each column
visualizes the composed Bernoulli templates during the sequential inference procedure (top down)
for one sample. The bottom row are the original data points.
restricted Boltzmann machine in principle only requires 5 experts to model the data appropriately
because it uses bias terms. However, we again learned 10 experts (Figure 2, 3rd panel). While
the results look better than for the previous two models they are still far from optimal. In earlier
work Goessling & Amit (2015) we performed a quantitative comparison for a similar dataset, which
showed that the reconstruction performance of models with sum-of-log-odds composition is indeed
suboptimal.
4.2	MNIST digits
We now consider the MNIST digits dataset (LeCun et al., 1998), which consists of 60,000 training
samples and 10,000 test samples of dimension 28 × 28 = 784. We ran our learning algorithm for 10
6
Under review as a conference paper at ICLR 2017
/夕 9 era
夕“3£3
Z 73Λ ʒ
之,q * 5
λx y O y 5
夕 9 3y5
z6 OS 5
Z7夕A ʒ
/ 7 ¾ Wʒ
Figure 5: Dynamic supports for 5 MNIST experts. Left column: Expert probabilities. Remaining
columns: Composed Bernoulli templates for 10 latent configurations. The cast opinion of the expert
is shown in shades of red (White/red corresponds to μk (d) = 0/1).
Figure 6: Trained experts for Weizmann horses. Left: Expert probabilities (white/black corresponds
to μk (d) = 0/1). Right: Levels of expertise (blue/red corresponds to small/large values).
iterations and trained 100 experts (Figure 3). We see that some experts specialize on local structures
while others focus on more global ones. In Figure 4 we visualize the inference procedure for some
test samples using these 100 learned experts. On average 12 experts were activated for each data
point. For easier visualization we show at most 10 iterations of the likelihood matching pursuit
algorithm. The reconstructions are overall accurate and peculiarities of the samples are smoothed
out. In Figure 5 we illustrate how the expert supports change based on the latent representation.
Depending on which other experts are present the supports can vary quite a bit.
4.3	Weizmann horses
The following experiment shows that our model is able to cope with very high-dimensional data.
The Weizmann horse dataset (Borenstein & Ullman, 2008) consists of 328 binary images of size
200 × 240. We used the first 300 images to train 20 experts (Figure 6) and used the remaining 28
images for testing. Some of the experts are responsible for the background and the central region
of the horse while other experts focus on local structures like head posture, legs and tail. In Figure
7 we illustrate the partitioning of the test examples into expert opinions. For simplicity we used
exactly 4 experts to reconstruct each sample. Not all characteristics of the samples are perfectly
reconstructed but the general pose is correctly recovered. The same dataset was used to evaluate
the shape Boltzmann machine (Eslami et al., 2014), where 2,000 experts were learned. For those
experiments the images were downsampled to 32 × 32 pixels. This is a factor 50 smaller than the
full resolution of 48,000 dimensions that we use.
7
Under review as a conference paper at ICLR 2017
XM⅝然 M3>
mt
K⅝∙J≡x¼⅞rH
Figure 7: Decomposition of the test examples from the Weizmann horse dataset. 1st column:
Original data points. 2nd column: Reconstructions (shown are the composed Bernoulli templates).
3rd-6th column: Partitioning into experts opinions (White/black corresponds to μk (d) = 0/1, gray
indicates regions for which the expert is not responsible).
Figure 8: Reconstructions of the test examples from the Caltech motorcycle dataset. Odd rows:
Original data. Even rows: Reconstructions (shoWn are the composed Gaussian means).
4.4	Caltech motorcycles
We also experimented With real-valued data using the Caltech-101 motorcycle dataset (Fei-Fei et al.,
2007), Which consists of 798 images of size 100 × 180. The first 750 images Were used for training
and the remaining 48 images for testing. We trained 50 experts by running our learning procedure
for 10 iterations. In Figure 8 We visualize the reconstructed test examples. The reconstructions
are a bit blurry since We use a fairly sparse binary representation. Indeed, for each data point on
average only 7 experts Were employed. Note that the shapes of the motorcycles are reconstructed
quite accurately.
5	Discussion
In order to improve the reconstructions for continuous image data We could use real-valued latent
variables in addition to binary ones (as in Hinton et al. (1998)). This Would alloW us to model inten-
sities and contrasts more accurately. The inference procedure Would have to be adapted accordingly
such that continuous activations can be returned.
Our Work focused on product distributions. In order to apply the proposed approach to models With
dependence structure one can make use of an autoregressive decomposition (e.g., Goessling & Amit,
2016). If the joint distribution is Written as a product of conditional distributions then We can employ
the same composition rule as before. Indeed, We can model composed the conditionals as
P(x(d) | x(1:d-1), h) = Pk?(d) (x(d) | x(1:d-1)),
Where Pk are autoregressive expert models and k? (d) is the active expert With the highest level of
expertise for dimension d.
8
Under review as a conference paper at ICLR 2017
References
Yali Amit and Alain Trouve. Pop: Patchwork of parts models for object recognition. International
Journal of Computer Vision,75(2):267-282, 2007.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Eran Borenstein and Shimon Ullman. Combined top-down/bottom-up segmentation. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 30(12):2109-2125, 2008.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical Society. Series B (methodological), pp.
1-38, 1977.
Michael Elad. Sparse and redundant representations. Springer, 2010.
SM Ali Eslami, Nicolas Heess, Christopher KI Williams, and John Winn. The shape boltzmann
machine: a strong model of object shape. International Journal of Computer Vision, 107(2):
155-176, 2014.
Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training
examples: An incremental bayesian approach tested on 101 object categories. Computer Vision
and Image Understanding, 106(1):59-70, 2007.
Marc Goessling and Yali Amit. Compact compositional models. In International Conference on
Learning Representations (Workshop), 2015. URL http://arxiv.org/abs/1412.3708.
Marc Goessling and Yali Amit. Mixtures of sparse autoregressive networks. In International Con-
ference on Learning Representations (Workshop), 2016. URL http://arxiv.org/abs/
1511.04776.
John A Hartigan. Partition models. Communications in statistics-Theory and methods, 19(8):2745-
2756, 1990.
Mark Herbster and Manfred K Warmuth. Tracking the best expert. Machine Learning, 32(2):151-
178, 1998.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
Geoffrey E Hinton, Brian Sallans, and Zoubin Ghahramani. A hierarchical community of experts.
In Learning in graphical models, pp. 479-494. 1998.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jorg LUcke and Maneesh Sahani. Maximal causes for non-linear component extraction. The Journal
of Machine Learning Research, 9:1227-1267, 2008.
Geoffrey McLachlan and David Peel. Finite mixture models. John Wiley & Sons, 2004.
Andrew Ng. Sparse autoencoder. CS294A Lecture Notes, 72:1-19, 2011.
Chris Pal, Brendan J Frey, and Nebojsa Jojic. Learning montages of transformed latent images as
representations of objects that change in appearance. In Computer Vision-ECCV, pp. 715-731.
2002.
Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood
gradient. In International Conference on Machine learning, pp. 1064-1071, 2008.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In International Conference on Machine
Learning, pp. 1096-1103, 2008.
9
Under review as a conference paper at ICLR 2017
6	Derivatives
We provide here the derivatives of the log-likelihood in the expertise-weighted compositional model
(4) with respect to the expert parameters.
6.1	Bernoulli model
The Bernoulli log-likelihood is
f (μ) = N log μ + (1 — N )iog(i — μ),
where the composition rule for the probability is
μ =>: rk μk,
k
ek
k0 ek0
6.1.1	Derivatives with respect to the composed probability
The first and second derivative of the log-likelihood with respect to the composed probability are
df	N	1 — N N — μ
dμ	μ	1 — μ	μ (1 — μ)，
d2 f	N 1 — N (N — μ )2
dμ2	μ2	(1 — μ)2	μ2(1 — μ)2 ,
6.1.2	Derivatives with respect to the expert probabilities
The first and second derivative of the composed probability with respect to the expert probabilities
are
dμ	d 2 μ
=—=rk,	~r= = 0 •
dμk	dμk
Consequently, the derivatives of the log-likelihood with respect to the expert probabilities are
df	df	dμ N — μ
dμk	dμ dμk	μ(1 — μ))
d2f	d2 f	( dμ )2	+ df	d2μ	2 (N — μ)2
dμk	dμ2	∖dμk ) dμ	dμk	k μ2(1 — μ)2
We See that d2 f∕dμk < 0 for μ ∈ (0,1), i.e., the log-likelihood is a strictly concave function of μk.
6.1.3	Derivative with respect to the levels of expertise
The derivative of the composed probability with respect to the levels of expertise is
dμ _ μkE — Eekμko _ μk 一 μ
阳= E2	= E ,
where E = Pk0 ek0. The derivative of the log-likelihood with respect to the levels of expertise can
be computed as
df	df	dμ
---=----------.
dek dμ dek
6.2 Gaussian model
The Gaussian log-likelihood is
f(μ, V) = 一(N 一 μ) 一 1 log(V) 一 1 log(2π),
2v	2	2
where the composition rules for the mean and variance are
μ =>： rk μk, V = X ： rk (Vk + μk ) 一 μ, rk =	•
k	k	k	k0 ek0
10
Under review as a conference paper at ICLR 2017
6.2. 1 Derivative with respect to the composed mean and variance
The derivative of the log-likelihood with respect to the composed mean and variance are
df x — μ df (x — μ )2	1	(x — μ )2 — V
dμ v , dv	2 V 2	2 V	2 V 2	.
6.2.2 Derivative with respect to the levels of expertise
The derivative of the composed mean and variance with respect to the levels of expertise are
dμ _ μkE - fekμko _ μk 一 μ
而=	E2	= E
dV qkE — ek0 qk0
---=----------------
dek	E2
o dμ	qk — q o μk — μ	Vk — V + (μk — μ)2
-2 F = ɪ - 2 μ^- =-----E----
where E = P 卜,已卜，and qk = Vk + μ k, q = V + μ2. The derivative of the log-likelihood with respect
to the levels of expertise can be computed as
df
dek
df dμ df
dμ dek dV
dV
dek
7 Numerical optimization
For binary data, the log-likelihood of the smoothed model is a concave function of μk(d), see
Section 6.1.2. We could therefore in principal perform an optimization for the experts opinions
using Newton’s method. There are a few complications though. One problem is that the second
derivative is proportional to the squared responsibility and hence close to 0 if the level of expertise
is small. Consequently, template updates in regions with low expertise would be unstable. To deal
with that we could add a penalty on the squared log-odds for example. Another problem is that the
Newton steps may lead to probability estimates outside of [0, 1]. This can be dealt with by pulling
the estimates back into the unit interval. Note that working on the log-odds scale is not possible
because the log-likelihood of our model is not concave in the expert log-odds. Because of these
complications we use the simple, fast and robust heuristic (5) instead of Netwon’s method.
11