Under review as a conference paper at ICLR 2017
An empirical analysis of deep network loss
SURFACES
Daniel JiWoong Im1,2；Michael Tao3, & Kristin Branson2
1Janelia Research Campus, HHMI, 2AIFounded Inc. 3University of Toronto,
{imd, bransonk}@janelia.hhmi.org
{mtao}@dgp.toronto.edu
Ab stract
The training of deep neural networks is a high-dimension optimization problem
with respect to the loss function of a model. Unfortunately, these functions are of
high dimension and non-convex and hence difficult to characterize. In this paper,
we empirically investigate the geometry of the loss functions for state-of-the-art
networks with multiple stochastic optimization methods. We do this through sev-
eral experiments that are visualized on polygons to understand how and when
these stochastic optimization methods find local minima.
1	Introduction
Deep neural networks are trained by optimizing an extremely high-dimensional loss function with
respect to the weights of the network’s linear layers. The objective function minimized is some
measure of the error of the network’s predictions based on these weights compared to training data.
This loss function is non-convex and has many local minima. These loss functions are usually
minimized using first-order gradient descent (Robbins & Monro, 1951; Polyak, 1964) algorithms
such as stochastic gradient descent (SGD) (Bottou, 1991). The success of deep learning critically
depends on how well we can minimize this loss function, both in terms of the quality of the local
minima found and the time to find them. Understanding the geometry of this loss function and how
well optimization algorithms can find good local minima is thus of vital importance.
Several works have theoretically analyzed and characterized the geometry of deep network loss
functions. However, to make these analyses tractible, they have relied on simplifications of the
network structures, including that the networks are linear (Saxe et al., 2014), or assuming the path
and variable independence of the neural networks (Choromanska et al., 2015). Orthogonally, the
performance of various gradient descent algorithms has been theoretically characterized (Nesterov,
1983). Again, these analyses make simplifying assumptions, in particular that the loss function is
strictly convex, i.e. there is only a single local minimum.
In this work, we empirically investigated the geometry of the real loss functions for state-of-the-art
networks and data sets. In addition, we investigated how popular optimization algorithms interact
with these real loss surfaces. To do this, we plotted low-dimensional projections of the loss function
in subspaces chosen to investigate properties of the local minima selected by different algorithms.
We chose these subspaces to address the following questions:
•	What types of changes to the optimization procedure result in different local minima?
•	Do different optimization algorithms find qualitatively different types of local minima?
2	Related Work
2.1	Loss Surfaces
There have been several attempts to understand the loss surfaces of deep neural networks. Some
have studied the critical points of the deep linear neural networks (Baldi, 1989; Baldi & Hornik,
*Work done during an internship at Janelia Research Campus
1
Under review as a conference paper at ICLR 2017
1989; Baldi & Lu, 2012). Others further investigated the learning dynamics of the deep linear
neural networks (Saxe et al., 2014). More recently, several others have attempted to study the loss
surfaces of deep non-linear neural networks (Choromanska et al., 2015; Kawaguchi, 2016; Soudry
& Carmon, 2016).
One approach is to analogize the states of neurons as the magnetics dipoles used in spherical spin-
glass Ising models from statistical physics (Parisi, 2016; Fyodorov & Williams, 2007; Bray & Dean,
2007). Choromanska et al. (2015) attempted to understand the loss function of neural networks
through studying the random Gaussian error functions of Ising models. Recent results (Kawaguchi,
2016; Soudry & Carmon, 2016) have provided cursory evidence in agreement with the theory pro-
vided by Choromanska et al. (2015) in that they found that that there are no “poor” local minima in
neural networks still with strong assumptions.
There is some potential disconnect between these theoretical results and what is found in practice
due to several strong assumptions such as the activation of the hidden units and output being inde-
pendent of the previous hidden units and input data. The work of Dauphin et al. (2014) empirically
investigated properties of the critical points of neural network loss functions and demonstrated that
their critical points behave similarly to the critical points of random Gaussian error functions in high
dimensional space. We will expose further evidence along this trajectory.
2.2	Optimization
In practice, the local minima of deep network loss functions are for the most part decent. This
implies that we probably do not need to take many precautions to avoid bad local minima in practice.
If all local minima are decent, then the task of finding a decent local minimum quickly is reduced to
the task of finding any local minimum quickly. From an optimization perspective this implies that
solely focusing on designing fast methods are of key importance for training deep networks.
In the literature the common method for measuring performance of optimization methods is to
analyze them on nice convex quadratic functions (Polyak, 1964; Broyden, 1970; Nesterov, 1983;
Martens, 2010; Erdogdu & Montanari, 2015) even though the problems are applied to non-convex
problems. For non-convex problems, however, if two methods converge to different local minima
their performance will be dictated on how those methods solve those two convex subproblems. It
is challenging to show that one method will beat another without knowledge of the sort of convex
subproblems, which is generally not known apriori. What we will explore is whether indeed are
some characteristics that can found experimentally. If so, perhaps one could validate where these
analytical results are valid or even improve methods for training neural networks.
2.2.1	Learning Phases
Figure 1: An example of learning curve of neural network
One of the interesting empirical observation is that we often observe is that the incremental improve-
ment of optimization methods decreases rapidly even in non-convex problems. This behavior has
been discussed as a “transient” phase followed by a “minimization” phase (Sutskever et al., 2013)
2
Under review as a conference paper at ICLR 2017
where the former finds the neighborhood of a decent local minima and the latter finds the local
minima within that neighborhood. The existence of these phases implies that if certain methods are
better at different phases one could create novel methods that schedule when to apply each method.
3	Experimental Setup and Tools
3.1	Network architectures and data sets
We conducted experiments on three state-of-the-art neural network architectures. Network-in-
Network (NIN) (Lin et al., 2014) and the VGG(Simonyan & Zisserman, 2015) network are feed-
forward convolutional networks developed for image classification, and have excellent performance
on the Imagenet (Russakovsky et al., 2014) and CIFAR10 (Krizhevsky, 2009) data sets. The long
short-term memory network (LSTM) (Hochreiter & Schmidhuber, 1997) is a recurrent neural net-
work that has been successful in tasks that take variable-length sequences as input and/or produce
variable-length sequences as output, such as speech recognition and image caption generation. These
are large networks currently used in many machine vision and learning tasks, and the loss functions
minimized by each are highly non-convex.
All results using the feed-forward convolutional networks (NIN and VGG) are on the CIFAR10
image classification data set, while the LSTM was tested on the Penn Treebank next-word prediction
data set.
3.2	Optimization methods
We analyzed the performance of five popular gradient-descent optimization methods for these learn-
ing frameworks: Stochastic gradient descent (SGD) (Robbins & Monro, 1951), stochastic gradient
descent with momentum (SGDM), RMSprop (Tieleman & Hinton, 2012), Adadelta (Zeiler et al.,
2011), and ADAM (Kingma & Ba, 2014). These are all first-order gradient descent algorithms that
estimate the gradients based on randomly-grouped minibatches of training examples. One of the
major differences between these algorithms is how they select the weight-update step-size at each
iteration, with SGD and SGDM using fixed schedules, and RMSprop, Adadelta, and ADAM using
adaptive, per-parameter step-sizes. Details are provided in Section A.2.
In addition to these five existing optimization methods, we compare to a new gradient descent
method we developed based on the family of Runge Kutta integrators. In our experiments, we
tested a second-order Runge-Kutta integrator in combination with SGD (RK2) and in combination
with ADAM (ADAM&RK2). Details are provided in Section A.3).
3.3	Analysis methods
Several of our empirical analyses are based on the technique of Goodfellow et al. (Goodfellow et al.,
2015). They visualize the loss function by projecting it down to one carefully chosen dimension.
They plot the value of the loss function along a set of samples along this dimension. The projec-
tion space is chosen based on important weight configurations, thus they plot the value of the loss
function at linear interpolations between two weight configurations. They perform two such analy-
ses: one in which they interpolate between the initialization weights and the final learned weights,
and one in which they interpolate between two sets of final weights, each learned from different
initializations.
In this work, we use a similar visualization technique, but choose different low-dimensional sub-
spaces for the projection of the loss function. These subspaces are based on the initial weights as
well as the final weights learned using the different optimization algorithms and combinations of
them, and are chosen to answer a variety of questions about the loss function and how the different
optimization algorithms interact with this loss function. In contrast, Goodfellow et al. only looked
at SGDM. In addition, we explore the use of two-dimensional projections of the loss function, al-
lowing us to better visualize the space between local minima. We do this via barycentric and bilinar
interpolation for triplets and quartets of points respectively (details in Section A.1).
We refer to the critical points found using these variants of SGD, for which the gradient is approxi-
mately 0, as local minima. Our evidence that these are local minima as opposed to saddle points is
3
Under review as a conference paper at ICLR 2017
(a) NIN	(b) VGG
(a) NIN	(b) VGG
Figure 2: Visualization of the loss surface at
weights interpolated between two initial configu-
rations and the final weight vectors learned using
SGD from these initializations.
Figure 3: Visualization of the loss surface at
weights interpolated between the weights learned
by four different algorithms from the same ini-
tialization.
similar to that presented in Goodfellow et al. (Goodfellow et al., 2015). If we interpolate beyond the
critical point, in this one-dimensional projection, the loss increases (Fig. 10).
3.4 Technical details
We used the VGG and NIN implementations from https://github.com/szagoruyko/cifar.torch.git.
The batch size was set to 128 and the number of epochs was set to 200. The learning rate was chosen
from the discrete range between [0.2, 0.1, 0.05, 0.01] for SGD and [0.002, 0.001, 0.0005, 0.0001] for
adaptive learning methods. We doubled the learning rates when we ran our augmented versions with
Runge-Kutta because they required two stochastic gradient computations per epoch. We used batch-
normalization and dropout to regularize our networks. All experiments were run on a 6-core Intel(R)
Xeon(R) CPU @ 2.40GHz with a TITAN X.
4 Experimental results
4.1	Different optimization methods find different local minima
We trained the neural networks described above using each optimization method starting from the
same initial weights and with the same minibatching. We computed the value of the loss function
for weight vectors interpolated between the initial weights, the final weights for one algorithm, and
the final weights for a second algorithm for several pairings of algorithms. The results are shown in
the lower triangle of Table 1.
For every pair of optimization algorithms, we observe that the training loss between the final weights
for different algorithms shows a sharp increase along the interpolated path. This suggests that each
optimization algorithm found a different critical point, despite starting at the same initialization. We
investigated the space between other triplets and quadruples of weight vectors (Figure 2 and 3), and
even in these projections of the loss function, we still see that the local minima returned by different
algorithms are separated by high loss weight parameters.
Deep networks are overparameterized. For example, if we switch all corresponding weights for a
pair of nodes in our network, we will obtain effectively the same network, with both the original
and permuted networks outputting the same prediction for a given input. To ensure that the weight
vectors returned by the different algorithms were functionally different, we compared the outpts of
the networks on each example in a validation data set:
1
dist(θ1,θ2) =
∖ Ntest
Ntest
kF(xi,θ1)-F(xi,θ2)k* 2,
i=1
where θ1 and θ2 are the weights learned by two different optimization algorithms, xi is the input for
a validation example, and F(x, θ) is the output of the network for weights θ on input x.
4
Under review as a conference paper at ICLR 2017
SGD	RMSprop	Adadelta	Adam	RK2	Adam&RK2
SGD
ecnatsiD
RMSprop a SGD
ecnatsiD
0O	,5	1
Adam	Q	SGD
ecnatsiD
0Ogd .5 rk2
① uu--0
RMSprop
'(l-α)RMSProP)
RMSprop Adam
Adadelta
Adam
RK2
Adam&RK2
RMSProP
①UU
00	.5^-
Adam Adam&
①uun3IM0
0O	.5
RK2	Adam&
a
Table 1: Visualization of the loss surface near and between local minima found by different opti-
mization methods. Each box corresponds to a pair of optimization methods. In the lower triangle,
we plot the projection of the loss surface at weight vectors between the initial weight and the learned
weights found by the two optimization methods. Color as well as height of the surface indicate the
loss function value. In the upper triangle, we plot the functional difference between the network
corresponding to the learned weights for the first algorithm and networks corresponding to weights
linearly interpolated between the first and second algorithm’s learned weights. (Best viewed in
zoom)
5
Under review as a conference paper at ICLR 2017
99.0
95.5
98.5
98.0
> 97,5
< 97.0
96.5
96.0
SGD
(a) Train
RK2 ADAM ADAM&RK2 Rmsprop Adadelta
90.2
88.4
90.0
89.8
89.6
S, 89.4
W 89.2
89.0
88.8
88.6
SGD
(b) Test
RK2 ADAM ADAMSRIQRmsprop AdadeIta
Figure 4:	(a) Training accuracy and (b) test accuracy for each of the optimization methods. Colors
correspond to different initializations.
We found that, for all pairs of algorithms, the average distance between the outputs of the networks
(Equation 4.1) was approximately 0.16, corresponding to a label disagreement of about 8% (upper
triangle of Table 1). Given the generalization error of these networks (approximately 11%, Figure 4),
the maximum disagreement we could see was 22%. Thus, these networks disagreed on a large
fraction of these test examples - over 1 rd. Thus, the local minima found by different algorithms
correspond to effectively different networks, not trivial reparameterizations of the same one.
(a) NIN - Initial to Final.
——sgd-RK2 ——Re-⅞da∏>s<ιικ2 I
—S9d-adam	adam-adaπ>S∣IU≡
—s9<MgmS<RK2 — Re-adam ∣
I ——lnlt-sgd	I 但 daπ>S∣IIK21
I — mzgm — Mlt-Ma∣
WPhS
I ——sgd-RK2	——RK2-adaπ>s<RlC2 I
—sg<tw⅛m	w⅛m-adaπ⅛RK2
I - sadxtom&Ria — RKZadam∣
(b) NIN - Final to Final.	(c) VGG - Initial to Final. (d) VGG - Final to Final.
Figure 5:	Loss function value near local minima found by multiple restarts of each algorithm.
4.2 Different optimization algorithms find different types of local minima
Next, we investigated whether the local minima found by the different optimization algorithms had
distinguishing properties. To do this, we trained the networks with each optimization algorithm
using different initial parameters. We then compared differences between runs of the same algorithm
but different initializations to differences between different algorithms.
As shown in Figure 4(a), in terms of training accuracy, we do see some stereotypy for the optima
found by different algorithms, with SGD finding local minima with the lowest training accuracy and
ADAM, Rmsprop, and Adadelta finding local minima with the highest training accuracy. However,
this could be attributed to SGD’s asymtotically slow convergence near local minima due to the
gradient diminishing near extrema. Despite this limitation, Figure 4(b) shows that the generalization
accuracy of these different local minima on validation data was not significantly different between
algorithms. We also did not see a relationship between the weight initialization and the validation
accuracy. Thus, while these algorithms fall into different local minima, they are not different in
terms of their final quality.
We visualized the loss surface around each of the local minima for the multiple runs. To do this,
we plotted the value of the loss function between the initial and final weights for each algorithm
(Figure 5(a,c)) for each run of the algorithm from a different initialization. In addition, we plotted
6
Under review as a conference paper at ICLR 2017
—adadelta	—	rmsprop
——ADAM	——Sgd
ADAM&RK2	—	RK2
Figure 6:	Observing the absolute size of basin for different local minimas found by different opti-
mization methods.
the value of the loss function between the final weights for selected pairs of algorithms for each
run (Figure 5(b,d)). We see that the surfaces look strikingly similar for different runs of the same
algorithm, but characteristically different for different algorithms. Thus, we found evidence that the
different algorithms land in qualitatively different types of local minima.
In particular, we see in Figure 5(a,c) that the size of the basins around the local minima found by
ADAM and ADAM&RK2 are larger than those found by SGD and RK2, i.e. that the training loss is
small for a wider range of α values. This is a relative measure, and the magnitude of the change in
the weight vector is ∆αkθ1 - θ0 k for a change of size ∆α, where θ0 is the initial weight vector θ1
is the result found by a given optimization algorithm. In Figure 6, we repeat this analysis, instead
showing the loss as a function of the absolute distance in parameter space:
θ(λ)
θ1 + λ
。0 -。1
kθ0 - θ1k
(1)
We again see that the size of the basin around the local minima varies by optimization algorithm.
Note that we evaluate the loss for weight vectors beyond the initial configuration, which had a loss
of 2.4.
4.3 Analyzing learning after “transient period”
Recall that, during optimization, it has been observed that there is a short “transient” phase when the
loss decreases rapidly and a “minimization” phase in which the loss decreases slowly (Section 2.2.1
and Figure 1). In this set of experiments, we investigated the effects of switching from one type
of optimization method to another at various points during training, in particular at late stages of
training when it is thought that a local minimum has been chosen and is only being localized. We
switched from one optimization method to another 25%, 50%, and 75% of the way through training.
The results are plotted in Figure 7d. We emphasize that we are not switching methods to improve
performance, but rather to investigate the shape of the loss function in regions explored during the
“minimization” phase of optimization.
We found that, regardless of how late we switch optimization algorithms, as shown in the right-
most column of Figure 7, the local minima found were all different. This directly disagrees with
the notion that the local minimum has effectively been chosen before the “minimization” phase, but
instead that which local minimum is found is still in flux this late in optimization. It appears that
this switch from one local minimum to another happens almost immediately after the optimization
method switches, with the training accuracy jumping to the characteristic accuracy for the given
method within a few epochs (Figure 7, left column). Interestingly, we also see the distance between
the initial and current weight vectors changes drastically after switching from one optimization
7
Under review as a conference paper at ICLR 2017
OQOOOO
0 9 8 7 6 5
eaHule∙ls~ue∙l3E*»U-UMMIaCrsa
—A200<->A50-S150	—	A50-S150->Al OD-SlOO
—A200<->A100-S100	—	A50-S150->A150-S50
—A200<->A150-S50	—	AlOO-Sl 00->A150-S50

(a)	The learning rate is set to 0.001 for ADAM , and then switched to SGD with learning rate 0.01.
1«,-------,--------,--------,-------- 45I----------'--------'--------'-------
50	100	150
Epoch
I—— S200 一 SIS血叫
I - S50-⅛150	— S150∙A50 ∣
« K M M W W c
sjəiəUleWs-β≡c- UMMq .a
50	100	150
Epoch
S200	——S100⅛100
S504kl50 — S1504k50
2.5
2.0
1.5
1.0
0.5
0.2	0.4	0.6	0.8	1.0
alpha
—S200<->S50A150	—	S50√K150->S100√K100
—S200<->S100√K100	—	S50√K150->S150√V50
一	S200<->S150√⅛50	—	SlOOAl 00->S15(‰A50
(b)	The learning rate is set to 0.1 for SGD, and then switched to ADAM with learning rate 0.0001.
l∞r
90
80
150
50 1
40
30
2V
fapωeed-UE3 E≈∙∙≡CVV2K- .-°α

(c)	The learning rate is set to 0.001 for ADAM , and then switched to Adadelta (learning rate is not required).
IOO1	,-------------L _ _ _ , _   __	900.------,--------,-------,-------
s∙mauleg ~UR3 S-β≡c~ CVVBVn- .Sa
alpha	,
——A□E2OO<->A□E5O√U 50	——A□E5O-A15O->A□E1OO^A1OO
—A□E200<->A□E100-A100	——A□E5O-A15O->A□E15O^A5O
—A□E2OO<->A□E15O-A5O	— A□E100^A100->ADE15<M50
(d)	The learning rate is not required for Adadelta, and then switched to ADAM with learning rate 0.0001.
Figure 7: Switching methods from one method to another method at epoch 50 and 100 and 150.
Accuracy curve (Left two columns). Distance between initial weights to weights at each epoch
(Middle). The interpolation between different convergence parameters (Right). For instance, S100-
A100 as trained with SGD in the first 100 epoch and switched to ADAM for the rest of the epoch.
(Best viewed in zoom)
8
Under review as a conference paper at ICLR 2017
Table 2: Visualization of the Loss Surface with and without batch-normalization.
I ——~init-adam+ra Iston	= _init-rmsprop
I — init-adam	——init-ra Iston
(b) VGG - Initial to Final.
(c) LSTM - Initial to Final.
(a) NIN - Initial to Final.
——rmsprop-ra Iston	——ra lston-ada m+ra Iston
——	rmsprop-adam	——	adam-adam+ralston
——rmsprop-adam+ra Iston	——ralston-adam
I ——adam-adamδcRK2 adam&RK2-nmsprop
I - adam-rmsprop
——sgd-sgdm	——rmsprop-adam
——sgd-adagrad	——adagrad-adam
——sgd-adamrmsprop-adagrad
(d)	NIN - Final to Final.
(e)	VGG - Final to Final.
(f)	LSTM - Final to Final.
Figure 8: (Without batch normalization) Loss function with parameter interporlated between Initial
to Final. (Bottom) Loss function with parameters interporlated between Final to Final. Each column
of the plots are from different initializations.
method to another, and that this distance is characteristic per algorithm (Figure 7, middle column).
While distance increases with training epoch for any single optimization method, it actually starts to
decrease when switching from ADAM to SGD.
4.4 Effects of batch-normalization
To understand how batch normalization affects the types of local minima found, we performed a set
of experiments comparing loss surfaces near local minima found with and without batch normal-
9
Under review as a conference paper at ICLR 2017
ization for each of the optimization methods. We visualized the surface near these local minima
by interpolating between the initial weights and the final weights as well as between pairs of final
weights found with different algorithms.
We observed clear qualitative differences between optimization with (Figure 5) and without (Fig-
ure 8) batch normalization. We see that, without batch normalization, the quality of local minimum
found by a given algorithm is much more dependent on the initialization. In addition, the sur-
faces between different local minima are more complex in appearance: with batch normalization we
see sharp unimodal jumps in performance but without batch normalization we obtain wide bumpy
shapes that aren’t necessarily unimodal.
The neural networks are typically initialized with very small parameter values (Glorot & Bengio,
2010; He et al., 2015). Instead, we trained NIN with exotic intializations such as initial parameters
drawn from N (-10.0, 0.01) or N (-1.0, 1.0) and observe the loss surface behaviours. The details
of results are discussed in Appendix A.5.
5 Conclusions
In this work, we performed a series of empirical analyses to understand the geometry of the loss
functions corresponding to deep neural networks, and how different optimization methods minimize
this loss to answer the two questions posed in the introduction.
What types of changes to the optimization procedure result in different local minima?
We found that every type of change to the optimization procedure we tested resulted in a different
local minimum. Different local minima were found using the different optimization algorithms from
the same initialization (Section 4.1). Even switching the optimization algorithm to another very late
in optimization - during the slow “mimimization” portion of learning - resulted in a different local
minimum (Section 4.3). The quality of the local minima found, in terms of training and generaliza-
tion error, is similar. These different local minima were not equivalent, and made mistakes on differ-
ent test examples (Section 4.1). Thus, they were not trivially different local minima, as would occur
if nodes in internal layers of the network were permuted. We observed that the quality of these local
minima was only consistently good when we used batch normalization for regularization. Without
batch normalization, the quality of the critical points found depended on the initialization, and some
solutions found were not as good as others. Our observations are in contrast to the conclusions of
Goodfellow et al., i.e. that local minima are not a problem in deep learning because, in the region
of the loss function explored by SGD algorithms, the loss function is well-behaved (Goodfellow
et al., 2015). Instead, our observations are more consistent with the explanation that the local min-
ima found by popular SGD optimization methods are almost all good (Choromanska et al., 2015;
Kawaguchi, 2016; Soudry & Carmon, 2016).
Do different optimization algorithms find qualitatively different types of local minima?
Interestingly, we found that, while the local minima found by the same optimization algorithm from
different initializations were different, the shape of the loss function around these local minima was
strikingly similar, and was a characteristic of the optimization algorithm. In particular, we found
that the size of the basin around ADAM-based optimization was larger than that around vanilla
SGD (Section 4.2). A large basin is related to a large margin, as small changes in the weight vector
will not affect the training error, and perhaps could have some implications for generalization error.
In our experiments, however, we did not observe better generalization error for ADAM than SGD.
Questions for potential future research are why the shapes of the loss functions around different local
minima found by the same algorithm are so similar, and what the practical implications of this are.
10
Under review as a conference paper at ICLR 2017
References
Pierre. Baldi. Linear learning: Landscapes and algorithms. In In Advances in neural information
processing systems., pp. 65-72, 1989.
Pierre Baldi and K. Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural Networks, 2:53-58, 1989.
Pierre. Baldi and Zhiqin. Lu. Complex-valued autoencoders. Neural Networks, 33:136-147, 2012.
Leon Bottou. Stochastic gradient learning in neural networks. In Proceedings of Nuero-Nimes,
1991.
Alan J. Bray and David S. Dean. The statistics of critical points of gaussian fields on large-
dimensional spaces. In Physics Review Letter, 2007.
C. G Broyden. The convergence of a class of double-rank minimization algorithms 1. general con-
siderations. Journal of Applied Mathematics, 6:76-90, 1970.
John C. Butcher. Coefficients for the study of runge-kutta integration processes. Society for Indus-
trial and Applied Mathematics, 3:185-201, 1963.
Anna Choromanska, Mikael Henaf, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. arXiv preprint arXiv:1406.2572, 2015.
Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. arXiv preprint arXiv:1406.2572, 2014.
Murat A. Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton methods. In
Proceedings of the Neural Information Processing Systems (NIPS), 2015.
Yan V. Fyodorov and Ian Williams. Replica symmetry breaking condition exposed by random matrix
calculation of landscape complexity. Journal of Statistical Physics,, 129:1081-1161, 2007.
Xavier Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural
networks. In International conference on artificial intelligence and statistics, 2010.
Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe. Qualitatively characterizing neural network
optimization problems. In Proceedings of the International Conference on Learning Representa-
tions (ICLR), 2015.
Ernst Hairer, Nrsett Syvert P., and Gerhard Wanner. Solving Ordinary Differential Equations I
Nonstiff. Springer, 1987.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In arXiv preprint arXiv:1502.01852, 2015.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9:1735-1780,
1997.
Kenji Kawaguchi. Deep learning without poor local minima. arXiv preprint arXiv:1605.07110,
2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations (ICLR), 2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. In MSc thesis, Univesity of
Toronto, 2009.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In Proceedings of the International
Conference on Learning Representations (ICLR), 2014.
James Martens. Deep learning via hessian-free optimization. In Proceedings of the International
Conference of Machine Learning (ICML), 2010.
11
Under review as a conference paper at ICLR 2017
Yurii Nesterov. A method of solving a convex programming problem with convergence rate
o(1∕sqr(k)). Soviet Mathematics Doklady, 27:372-376, 1983.
Giorgio Parisi. Probabilistic line searches for stochastic optimization. arXiv preprint
arXiv:0706.0094, 2016.
B.T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathematical
Statistics, 22(3):400-407, 1951.
O. Russakovsky, H. Deng, J. adn Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,
A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition
challenge. In arXiv preprint arXiv:1409.0575, 2014.
Andrew M Saxe, James L McClelland, and Surya. Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. In In International Conference on Learning
Representations., 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Proceedings of the International Conference on Learning Representations (ICLR),
2015.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Ilya Sutskever, James Martens, George Dahl, and Geoffery Hinton. On the importance of momentum
and initialization in deep learning. In Proceedings of the International Conference of Machine
Learning (ICML), 2013.
Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of
deep networks. In International conference on artificial intelligence and statistics, 2016.
Tijmen Tieleman and Geoffery Hinton. Rmsprop gradient optimization. In Neural Networks for
Machine Learning slide: http://www.cs.toronto.edu/tijmen/csc321/slides/lecture-slides_lec6.pdf,
2012.
Matthew D. Zeiler, Graham W. Taylor, and Rob Fergus. Adaptive deconvolutional networks for mid
and high level feature learning. In International Conference on Computer Visio, 2011.
12
Under review as a conference paper at ICLR 2017
A Supplemantary Materials
A. 1 3D Visualization
Goodfellow et al. (2015) introduced the idea of visualizing 1D subspace of the loss surface between
the parameters. Here, we propose to visualize loss surface in 3D space through interpolating over
three and four vertices.
Linear Interpolation Given two parameters θ1 and θ2,
θi = αθ1 + (1 - α)θ2,	∀α ∈ [0, 1].	(2)
Bilinear Interpolation Given four parameters θ0,	θ1, θ2, and θ3,
φi =	αθ1 +	(1	- α)θ2	(3)
ψi =	αθ3 +	(1	- α)θ4	(4)
θj=	βφi +	(1	- β)ψi	(5)
for all α ∈ [0, 1] and β ∈ [0, 1].
Barycentric Interpolation Given four parameters θ0, θ1, and θ2, let d1 = θ1 - θ0 and d2 =
θ2 - θ0. Then, the formulation of the interpolation is
φi =	αd1 + θ0	(6)
ψi =	αd2 + θo	(7)
θj =	βφi + (1 - β)中i	⑻
for all α ∈ [0, 1] and β ∈ [0, 1].
A.2 Optimization Methods
A.2.1 Stochastic Gradient Descent
In many deep learning applications both the number of parameters and quantity of input data points
can be quite large. This makes the full evaluation of U(θ) be prohibitively expensive. A standard
technique for aleviating computational loadis to apply an stochastic approximation to the gradi-
ent Robbins & Monro (1951). More precisely, one approximates U by a subset of n data points,
denoted by {σj }jN=1 at each timestep:
1n	1N
Un(θ) = - ∑'(θ, Xσj) ' NN E'(θ, Xi) = U(θ)	(9)
Of course this approximation also carries over to the gradient, which is of vital importance to opti-
mization techniques:
1n
VU n(θ) = n fv`(θ, Xσj) 'VU (θ)	(10)
j=1
This method is what is commonly called Stochastic Gradient Descent or SGD. So long as the data is
distributed nicely the approximation error of Un should be sufficiently small such that not only will
SGD still behave like normal GD , but it’s wall clock time for to converge should be significantly
lower as well.
Usually one uses the stochastic gradient rather than the true gradient, but the inherent noisiness must
be kept in mind. In what follows we will always mean the stochastic gradient.
A.2.2 Momentum
In order to aleviate both noise in the input data as well as noise from stochasticity used in computing
quantities one often maintains history of previous evaluations. In order to only require one extra
variable one usually stores variables of the form
E[F]t = αFt + βE[F]t-1.	(11)
13
Under review as a conference paper at ICLR 2017
where Ft is some value changing over time and E[F]t is the averaged quantity.
An easy scheme to apply this method to is to compute a rolling weighted average of gradients such
as
E[g]t = (1 - α)gt + αE[g]t-1
but there will be other uses in the future.
A.2.3 Pertinent Methods
With the aforementioned tools there are a variety of methods that can be constructed. We choose to
view these algorithms as implementations of Explicit Euler on a variety of different vector fields to
remove the ambiguity between η and gt . We therefore can define a method by the vector field Xt
that explicit Euler is applied to with a single η that is never changed.
SGD with Momentum (SGDM) By simply applying momentum to gt one obtains this stabilized
stochastic version of gradient descent:
Xt = -E[g]t.	(12)
This is the most fundamental method that is used in practice and the basis for everything that follows.
Adagrad Adagrad rescales Xt by summing up the sqaures of all previous gradients in a coefficient-
wise fashion:
Xt = --/	gt	.	(13)
Pit=1 gi2 +
Here is simply set to some small positive value to prevent division-by-zero. In the future we will
neglect this term in denominators because it is always necessary.
The concept is to accentuate variations in gt, but because the denominator is monotonically nonde-
creasing over time this method is doomed to retard its own progress over time. The denominator can
also be seen as a form of momentum where α and β are both set to 1.
Rmsprop A simple generalization of ADAGrad is to simply allow for α and β to be changed from
1. In particular one usually chooses a β less than 1, and presumably α = 1 - β. Thus one arrives at
a method where the effects of the distance history are diminished:
Xt
gt
PE[g2]t
(14)
Adadelta Adadelta adds another term to RMSprop in order to guarantee that the magnitude of X
is balanced with gt Zeiler et al. (2011). More precisely it maintains
Xt	gt
--, =——	,
PEXi	pE[g2i
(15)
which results in the following vector field:
Xt
√e[x2I
√E[g2i
gt.
(16)
—
and η is set to 1.
ADAM By applying momentum to both gt and gt2 one arrives at what is called ADAM. This is
often considered a combination of SGDM + RMSprop,
Xt =Ct PEfe
(17)
∖∕l-βt
Ct = Y∖-βt is the initialization bias correction term With β1,β2 ∈ [0,1) being the β parameters
used in momentum for g and g2 respectively. Initialization bias is caused by the history of the
momentum variable being initially set to zero.
14
Under review as a conference paper at ICLR 2017
A.3 Runge Kutta
Runge-Kutta methods Butcher (1963) are a broad class of numerical integrators categorized by their
truncation error. Because the ordinary differential equations Runge-Kutta methods solve generalize
gradient descent, our augmentation is quite straightforward. Although our method applies to all
explicit Runge-Kutta methods we will only describe second order methods for simplicity.
The general form of second-order explicit Runge-Kutta on
a time-independent vector field is
θt+1	θt + (a1k1 + a2k2)h	(18)
k1	X(θt)	(19)
k2	X(θt + q1hk1)	(20)
where a1 , a2, and q1 are parameters that define a given
Runge-Kutta method. Table 3 refers to the parameters
used for the different Runge-Kutta variants we use in our
experiments.
Table 3: The coefficients of vari-
ous second order Runge-Kutta methods
Hairer et al. (1987)
Method Name	aι	a2	qι
Midpoint	~δ~	~~Γ	~r~ 2
Heun	1 2	-r~ 2	1
Ralston	1 3	2 3	-3- 4
A.3.1 Augmenting Optimization with Runge
Kutta
For a given timestep, explicit integrators can be seen as a morphism over vector fields X → Xh.
For a gradient gt = Vθ U We can solve a modified RK2 gradient gt in the following fashion:
θt+1 rearranged with respect to gt	=θt + gth = AdveCtrgki2 (θ, h)	(21)
gt 二	Adveagk2 (θ,h) - θt h θt + (a1k1 + a2k2)h - θt	,ɔɔʌ 二	h	( ) (a1k1 + a2k2).	(24)
If we simply substitute the gradient gt with gt one obtains an RK2-augmented optimization tech-
nique.
A.4 Experiments with Runge-Kutta integrator
The results in Figure 9 illustrates that, with the exception of the Midpoint method, stochastic Runge-
Kutta methods outperform SGD. “SGD x2” is the stochastic gradient descent with twice of the learn-
ing rate of “SGD”. From the figure, we observe that the Runge-Kutta methods perform even better
with half the number of gradient computed by SGD. The reason is because SGD has the accumulated
truncated error of O(h) while second-order Runge-Kutta methods have the accumulated truncated
error of O(h2).
Unfortunately, ADAM outperforms ADAM+RK2 methods. We speculate that this is because the
way how ADAM’s renormalization of input gradients in conjunction with momentum eliminates the
value added by using our RK-based descent directions.
A.5 Effects of Batch-Normalization and Extreme Initializations
The neural networks are typically initialized with very small parameter values (Glorot & Bengio,
2010; He et al., 2015). Instead, we trained NIN with exotic intializations such as initial parameters
drawn from N (-10.0, 0.01) orN(-1.0, 1.0) and observe the loss surface behaviours. The results
are shown in Figure 11. We can see that NIN without BN does not train at all with any of these ini-
tializations. Swirszcz et al. (2016) mentioned that bad performance of neural networks trained with
these initializations are due to finding a bad local minima. However, we see that loss surface region
around these initializations are plateau 1 rather than a bad local minima as shown in Figure 11b. On
1We used same initializations as (Swirszcz et al., 2016) but we trained different neural networks with SGD
on a different dataset. We used NIN and CIFAR10 and Swirszcz et al. (2016) used smaller neural network and
MNIST.
15
Under review as a conference paper at ICLR 2017
Figure 9: Training accuracy curve
Figure 10: Interpolation between initial points to final points upto α = 2 in Equation 2.
the other hand, NIN with BN does train slowly over time but finds a local minima. This implies that
BN redeems the ill-posed loss surface (plateau region). Nevertheless, the local minima it found was
not good as when the parameters were initialized with small values. However, it is not totally clear
whether this is due to difficulty of training or due to falling in a bad local minima.
16
Under review as a conference paper at ICLR 2017
—— N(-10f 0.01) w. BN	—— N(-10f 0.01) wo. BN I
一 N(-l,l) w. BN	— N(-l,l) wo. BN
(a)	NIN - Learning curve
(b)	NIN without batch normaliza-
tion	(c) NIN with batch normalization
Figure 11:	NIN trained from different initializaitons.
A.6 Switching optimization methods
>U2⊃UU<
50	100	150	200
Epoch
——A200	——S150-A50
一	S200	一	A50-S150
——S50-A150	—	A100-S100
——S100-A100	--	A150-S50
908070605°
>U2⊃UU<
100
Epoch
150
——A200	——S150-A50
一	S200	一	A50-S150
——S50-A150	—	A100-S100
——S100-A100	--	A150-S50
100
Epoch
150	200
——A200	—	S150-A50
一	S200	一	A50-S150
一	S50-A150	—	A100-S100
——S100-A100	--	A150-S50

(a)
Figure 12:	NIN - Learning curve when switching methods from SGD to ADAM and visa versa at
epoch 50 and 100. Learning rate switched from SGD (ADAM) to ADAM (SGD) at (left) 0.001 (0.1)
to 0.1 (0.001), (middle) 0.001 (0.1) to 0.05, (0.001), and (right) 0.001 (0.1) to 0.01 (0.001).
17
Under review as a conference paper at ICLR 2017
9s8!m
60	80 IM 120	140	160	180	200
Epoch
—A200	— S1004M00
一 S200	A50-S150
—S50-A150	— A100-S200
fsaωeed-UaJJnS E∙=sUMMPq-°α
50	100	150
Epoch
——A200	——S100-A100
一	S200	—	A50-S150
—S50√U50	—	A100-S200
0.2	0.4	0.6	0.6	1.0
alpha
——A200->Al OO-Sl OO	——A200->A50-S150
—S200->S100A100	— S50Λ150->S100Λ100
—S200->S50^A150	一 A50-S150->A100-S100
(a) The learning rates is set to 0.001 and 0.05 for ADAM and SGD in the beginning, and then switched it to
0.05, 0.001 for SGD and ADAM.
1∞l——'——1——1——1——1——1——1——'——	1200,----,------,------,------
—A200->A100-S100
—S200->S10D√K100
—S200->S5(‰A150
A200->A50-S150
S50-Al 5O-> S100√K100
A50-Sl 50->A100-S100
—A200	—	S100-A100
一	S200	—	A50-S150
—S50√U50	—	A100-S200
—A200	— S1004M00
一 S200	A50-S150
—S50-A150	— A100-S200

(b)	The learning rates is set to 0.001 and 0.05 for ADAM and SGD in the beginning, and then switched it to
0.05, 0.0005 for SGD and ADAM .
1∞l——'——'——'——'——'——'——'——'——	1200,----,------,------,------
—A200	—	S100-A100
一	S200	—	A50-S150
—S50√U50	—	A100-S200
—A200	— S1004M00
—S200	A50-S150
—S50-A150	— A100-S200
——A200->Al OO-Sl OO	——A200->A50-S150
—S200->S100A100	— S50Λ150->S100Λ100
—S200->S50^A150	一 A50-S150->A100-S100
(c)	The learning rates is set to 0.001 and 0.05 for ADAM and SGD in the beginning, and then switched it to
0.01, 0.0001 for SGD and ADAM.
Figure 13: VGG - Switching methods from SGD to ADAM and ADAM to SGD at epoch 50 and
100. Zoomed in version (Left). Distance between initial weights to weights at each epoch (Middle).
The interpolation between different convergence parameters (Right). Each figure shows the results
of switching methods at different learning rate. We label the switch of methods in terms of ratio.
For instance, S100-A100 as trained with SGD in the first 100 epoch and swithced to ADAM for the
rest of the epoch.
18
Under review as a conference paper at ICLR 2017
—A200	— S1004M00
一 S200	A50-S150
—S50-A150	— A100-S200
—A200	—	S1004M00
一	S200	—	A50-S150
—S50-A150	—	A100-S200
一	A200->A100-S100	—	A200->A50-S150
一	S200->S10D√K100	—	S50-Al 50-> S100√K100
—S200->S5(‰A150	—	A5O-S150-> A100-S100
(a) Learning rate is not required for Adadelta. Learning rate is set to 0.05 for SGD in the beginning, and then
switched it to 0.1.
9s8!m∙∙,s
—A200	—	S1004M00
一	S200	—	A50-S150
—S50-A150	—	A100-S200
—A200	— S1004M00
一 S200	A50-S150
—S50-A150	— A100-S200
一	A200->A100-S100	-	A200->A50-S150
一	S200->S10D√K100	—	S50-Al 50-> S100√K100
—S200->S5(‰A150	—	A5O-S150-> A100-S100
(b) Learning rate is set to 0.05 for SGD.
84)
—A200	—	S1004M00
一	S200	—	A50-S150
—S50-A150	—	A100-S200
—A200	— S1004M00
—S200	A50-S150
—S50-A150	— A100-S200
0.2	0.4	0.6	0.8	1.0
alpha
一	A200->A100-S100	-	A200->A50-S150
一	S200->S10D√K100	—	S5O-A150-> S100√K100
—S200->S5(‰A150	—	A5O-S150-> A100-S100
(c) Learning rate is set to 0.05 for SGD in the beginning, and then switched it to 0.01.
Figure 14: VGG - Switching methods from SGD to Adadelta and Adadelta to SGD at epoch 50 and
100. Zoomed in version (Left). Distance between initial weights to weights at each epoch (Middle).
The interpolation between different convergence parameters (Right). Each figure shows the results
of switching methods at different learning rate. We label the switch of methods in terms of ratio.
For instance, S50-A50 as trained with SGD in the first 100 epoch and swithced to Adadelta for the
rest of the epoch.
19
Under review as a conference paper at ICLR 2017

40	60	80	1∞	120	140	160	180	200
Epoch
ADE100-A100
A50-ADE150
A1004U3E100
A200
ADE200
ADE50-A150
f*aωeed-UR3 E -∙-≡UMM4aq .δα
0.2	0.4	0.6	0.8	1.0
alpha
一	A200->A100-S100	—	A200->A50-S150
一	S200->S10D√K100	—	S5O-A150-> S100√K100
—S200->S5(‰A150	—	A5O-S150-> A100-S100

(a) Learning rate is not required for Adadelta. Learning rate is set to 0.05 for SGD in the beginning, and then
switched it to 0.1.
Figure 15: VGG - Switching methods from ADAM to Adadelta and Adadelta to ADAM at epoch
50 and 100. Zoomed in version (Left). Distance between initial weights to weights at each epoch
(Middle). The interpolation between different convergence parameters (Right). Each figure shows
the results of switching methods at different learning rate. We label the switch of methods in terms
of ratio. For instance, S50-A50 as trained with SGD in the first 100 epoch and swithced to Adadelta
for the rest of the epoch.
20