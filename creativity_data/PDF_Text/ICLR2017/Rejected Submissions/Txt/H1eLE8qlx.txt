Under review as a conference paper at ICLR 2017
Options Discovery with Budgeted Reinforce-
ment Learning
Aurelia Leon
Sorbonne Universites, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France
aurelia.leon@lip6.fr
Ludovic Denoyer
Sorbonne Universites, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France
ludovic.denoyer@lip6.fr
Ab stract
We consider the problem of learning hierarchical policies for Reinforcement
Learning able to discover options, an option corresponding to a sub-policy over
a set of primitive actions. Different models have been proposed during the last
decade that usually rely on a predefined set of options. We specifically address the
problem of automatically discovering options in decision processes. We describe
a new learning model called Budgeted Option Neural Network (BONN) 1 able to
discover options based on a budgeted learning objective. The BONN model is
evaluated on different classical RL problems, demonstrating both quantitative and
qualitative interesting results.
1	Introduction
Reinforcement Learning (RL) is one of the key problem in machine learning , and the interest of
the research community has been recently renewed with the apparition of models mixing classical
reinforcement learning techniques and deep neural networks. These new methods include for ex-
ample the DQN algorithm (Mnih et al., 2015) and its variants (Van Hasselt et al., 2015), the use
of recurrent architectures with policy gradient models (Wierstra et al., 2010), or even approaches
like Guided Policy Search (Levine & Koltun, 2013) or actor-critic algorithms (Konda & Tsitsiklis,
1999).
Research in cognitive science based on the study of human or animal behavior have long emphasized
that the internal policy of such agents can be seen as a hierarchical process where solving a task is
obtained by sequentially solving sub-tasks, each sub-task being treated by choosing a sequence of
primitive actions (Botvinick et al., 2009). In the computer science domain, these researches have
been echoed during the last decade with the apparition of the hierarchical reinforcement learning
paradigm (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) and its generalization to
options (Sutton et al., 1999). The underlying idea is to define a policy at two different levels: a level
which goal is to choose between options, and a level which will select the actions to apply to the
environment based on the current option. Informally, in a maze an option can correspond to an order
like go to the door, while the actions are primitive moves (up, down, left, right). In the literature, the
catalog of available options is usually specified manually which is not satisfactory.
We propose a new architecture called BONN (Budgeted Options Neural Network) able to simul-
taneously discover options and to learn how and when to use them. It is based on the idea that a
good policy is a trade-off between policy efficiency and cognitive effort: a system will learn relevant
options if these options allow it to reduce the cognitive effort for solving the task, without decreasing
the quality of the solution. This idea is implemented here through a budgeted learning problem that
encourages the BONN model to learn to acquire as few information as possible.
1code available here: https://github.com/aureliale/BONN-model
1
Under review as a conference paper at ICLR 2017
The contributions of the paper are: (i) We propose the BONN model able to discover options based
on a Budgeted Reinforcement Learning problem where information acquisition has a cost, each
option being a continuous vector in an learned latent option space. (ii) We propose a discrete variant
of BONN (D-BONN) where a discrete set of options is learned, each option corresponding to a
particular embedding in the latent option space. (iii) The model is tested on different RL tasks and
exhibits interesting properties and a strong ability to capture relevant options.
The paper is organized as follows: we present the background in RL and on recurrent policy gra-
dients methods in Section 2. The BONN model is presented in Sections 3.1 while the budgeted
learning problem is described in Section 3.2. The variant of BONN able to extract a discrete set
of options is given in Section 3.3. At last, experiments are proposed in Section 4 while the related
works are presented in Section 5.
2	Background
2.1	(PO-) Markov Decision Processes and Reinforcement Learning
Let us denote a MDP as a set of states S, a discrete set of possible actions A, a transition distribution
P (st+1 |st, at) and a reward function r(s, a) ∈ R+. We consider that each state st is associated with
an observation xt ∈ Rn, and that xt is a partial view of st (i.e POMDP), n being the size of the
observation space. Moreover, we denote PI the probability distribution over the possible initial
states of the MDP.
Given a current trajectory x1, a1, x2, a2,  , xt, a policy is defined by a probability distribution
such that π(x1 , a1 , x2, a2,  , xt, a) = P (a|x1, a1, x2, a2,  , xt) which is the probability of each
possible action a at time t, knowing the history of the agent.
2.2	Learning with Recurrent Policy Gradient
T-1
Let us denote γ ∈]0, 1] the discount factor, and Rt = P γk-tr(sk , ak) the discounted sum of
k=t
rewards (or discount return) at time t 2, corresponding to the trajectory (s0, a0, s1 , a1 ,  , sT) with
T the size of the trajectories sampled by the policy 3.
We can define the reinforcement learning problem as the optimization problem such that the optimal
policy ∏* is computed by maximizing the expected discounted return J(∏):
J(π) = Es0 ≈PI,a0, ,aT -1 ≈π [R0]	(1)
where s0 is sampled following PI and the actions are sampled based on π.
Different learning algorithms aim at maximizing J(π). In the case of policy gradient techniques,
if we consider that, for sake of simplicity, π also denotes the set of parameters of the policy, the
gradient of the objective can be approximated with:
1 M T-1
v∏ J(π) ≈ ME E Rn log∏(at∣X0,ao,…，xt)(Rt - bt)	(2)
m=1 t=0
where M is the number of sampled trajectories used for approximating the gradient using Monte
Carlo sampling techniques, bt is a variance reduction term at time t estimated during learning, and
we consider that future actions do not depend on past rewards (see Wierstra et al. (2010) for details
on recurrent policy gradients).
T-1
2Note that R0 = P γkr(sk , ak) correspond to the classical discount return
k=0
3We describe finite-horizon problems where T is the size of the horizon and γ ≤ 1, but the approach can
also be applied to infinite horizon problems with discount factor γ < 1
2
Under review as a conference paper at ICLR 2017
Figure 1: The BONN Architecture. Arrows correspond to dependencies, dashed arrows correspond
to sampled values. Note that when σt = 1 the model observes yt and compute a new option (in this
example we have σ3 = 1 and σ6 = 1 ), and that when σt = 0 (everywhere else in this example) the
model doesn’t use yt and keeps the same option.
3	Budgeted Option Neural Network
3.1	The BONN Architecture
We consider here a particular case of POMDP where the agent always observe xt , but can also
ask for the supplementary observation yt that will help him to decide which action to choose. This
situation corresponds to many practical cases: for example a robot that acquires information through
its camera (xt ) but can sometimes decide to make a complete scan of the room (yt ); a user driving a
car (using xt) but who decides to consult its map or GPS (yt); a virtual agent taking decisions in a
virtual world (based on xt) but that can ask instructions from a human (yt), etc. Note that xt or yt
can be an empty observation.
We now describe the budgeted option neural network. This model is composed of three components.
The underlying idea is that the first component will use the additional observations yt to compute
which option to use, while the second component will use the basic observations xt and the lastly
chosen option to sample primitive actions; the third component being used to decide when to switch
between options. A new option will thus be computed each time yt is acquired. Let us now describe
how each component works: (i) The first one (or option model) aims at choosing which option to
apply depending on the observations yt collected over the states of the process. In our model, an
option is represented by a vector denoted ot ∈ RO, O being the size of the options representation
space. (ii) Choosing a new option ot will then initialize the second component (or actor model).
During the next time steps, the actor model will sequentially choose actions based on observations xt
and update its state until a new option is generated. (iii) The acquisition model denoted σt ∈ {0; 1}
will decide if the model has to acquire yt or not.
To better understand this two-levels architecture, we provide the inference pseudo-code in Algorithm
2 and the architecture of BONN in Figure 1. We now describe the resulting components (the details
are given in the Appendix):
Option model: The option model will be denoted f such that f (yt) = ot generates an option ot
as a latent vector in a latent space RO, O being the dimension of this space. Note that the option
model is a deterministic model where the option is computed based on the current observation using
a neural network (see Appendix). Recurrent versions of the option model will be studied in a future
work.
Actor Model: The state of the actor model is represented by a vector zt ∈ RZ, Z being the size
of the latent space of the actor model. At each time step, the distribution over the possible set
of actions is computed by the function d such that P(at|zt) ≈ d(zt, at). Note that d is typically
based on a soft-max function mapping action scores to action probabilities (see Appendix). If a new
option ot is computed by the option model, then the state zt is re-initialized with zt = p(ot, xt).
p is a reset function which aims at choosing the ”initial” state of the actor for each new option. If
3
Under review as a conference paper at ICLR 2017
Algorithm 2 The pseudo code of the inference algorithm for the BONN model.
1	: procedure INFERENCE(s1)	. s1 is the initial state
2	:	initialize z0 with the empty option= (0, 0, ..., 0) ∈ RO
3	:	for t = 1 to T do
4	acquisition model: Draw σt 〜h(zt-ι, at-ι,xt)
5	:	if σt == 1 then
6	:	option level: Acquire yt and compute a new option ot = f(yt)
7	:	actor level: Initialize the actor zt = r(ot, xt)
8	:	else
9	:	actor level: Update the actor state zt = h(zt-1, at-1, xt)
10	:	end if
11	:	actor level: Choose the action at w.r.t zt
12	:	Execute the chosen action
13	:	end for
14	: end procedure
a new option is not generated, the actor state is updated with a classical recurrent mechanism i.e
zt+1 = g(zt, at, xt+1)
Acquisition Model: The acquisition model aims at deciding if a new option has to be generated.
It is a stochastic process such that σt = 1 (new option) or σt = 0 (keep the same option) and is
computed over the state of the actor and the new observation xt: P (σt+1 = 1) = h(zt, at, xt+1). In
our case, this probability is based on a Bernoulli distribution over a sigmoid-based h function (see
Appendix).
3.2	Budgeted Learning for Options Discovery
The way options emerge in a hierarchical reinforcement learning system has been the topic of many
different works in both reinforcement learning and cognitive science. Most of these techniques as-
sociate the problem of option discovery with the problem of sub-goals discovery where different
strategies are used to discover the sub-goals - see Botvinick et al. (2009) for a review on links be-
tween cognitive research and hierarchical reinforcement learning. The BONN model is based on a
different approach, where we consider that the discovery of options will result in learning a good
trade-off between policy efficiency and the cognitive effort generated by such a policy. The underly-
ing idea is that a system will learn relevant options if these options allow to reduce the cognitive effort
that is generated when solving the task, without decreasing the quality of the solution. Note that the
reduction of the cognitive effort has already been studied in cognitive science (Kool & Botvinick,
2014), and very recently in the RL context (Bacon & Precup, 2015b) but defined differently.
Here, the cognitive effort is associated with the acquisition of the additional information yt, this
additional information (and its computation) being considered costly but crucial for discovering a
good policy: an agent only using the observations xt would be unable to solve the task, but using yt
at each time step would be ”too expensive”. The design choice of xt and yt is fundamental in the
architecture, and is a distinct solution for bringing expert knowledge rather than explicitly defining
sub-tasks.
T-1
Let us denote C = P σt the acquisition cost for a particular episode: by reducing C, we will
t=0
encourage the agent to learn relevant options that will be used during many time steps, the model
extracting relevant sub-policies. We propose to integrate the acquisition cost C (or cognitive effort)
in the learning objective, relying on the budgeted learning paradigm already explored in different
RL-based applications (Contardo et al., 2016; Dulac-Arnold et al., 2012). We define an augmented
reward r* that includes the generated cost:
r*(st,at,σt) = r(st,at) - λσt	(3)
where λ controls the trade-off between the task efficiency and the cognitive charge. The resulting
discounted return denoted R0 will be used as the objective to maximize instead of the classical
4
Under review as a conference paper at ICLR 2017
discounted return R0, resulting in the following policy gradient update rule:
T-1
∏ J ∏ - Y X (v∏ log P(at∣zt) + V∏ log P(σt∣zt-i,at-i,xt)) (R↑ — b；)	(4)
t=0
where γ is the learning rate. Note that this rule now updates both the probabilities of the chosen
actions at , but also the probabilities of the σt that can be seen as internal actions and that decide if
a new option has to be computed or not, bt； being the new resulting variance reduction term.
3.3	Discovering a discrete set of options
In the previous sections, we considered that the option ot generated by the option model is a vector
in a latent space RO . This is slightly different than the classical option definition which usually
considers that an agent has a given ”catalog” of possible sub-routines i.e the set of options is a finite
discrete set. We propose here a variant of the model where the model learns a finite discrete set of
options.
Let us denote K the (manually-fixed) number of options one wants to discover. Each option will
be associated with a (learned) embedding denoted ok . The option model will store the different
possible options and choose which one to use each time an option is required. In that case, the
option model will be considered as a stochastic model able to sample one option index denoted it
in {1, 2, ..., K} by using a multinomial distribution on top of a softmax computation. In that case,
as the option model computes some stochastic choices, the policy gradient update rule will integrate
these additional internal actions with:
T-1
∏ J ∏ — Y X (V log P (at∣zt) + V log P (σt∣zt-i,at-i,xt) + V log P (it∣yt)) (R； — bt)	(5)
t=0
By considering that P(it|yt) is computed based on a softmax over a scoring function P(it|yt) ≈
`(oit , yt ) where ` is a differentiable function, the learning will update both the ` function and the
options embedding ok .
4	Experiments
The complete details of the architecture used for the experiments are provided in the Appendix. We
have tested this architecture on 3 different types of environments and compared it to a Recurrent
Policy Gradient algorithm using a GRU-based neural network (R-PG):
CartPole: This is the classical cart-pole environment as implemented in the OpenAI Gym4 platform
where observations are (position, angle, speed, angularspeed), and actions are right or lef t. The
reward is +1 for every time step without failure. For BONN, the observation is only used by the
option model i.e yt = (position, angle, speed, angularspeed), the actor model receiving an empty
observation Xt = 0 at each time step.
Lunar Lander: This environment corresponds to the Lunar Lander environment proposed in
OpenAI Gym where observations describe the position, velocity, angle of the agent and if he is
in contact with the ground or not, and actions are do nothing, fire left engine, fire main engine, fire
right engine. The reward is +100 if landing, +10 for each leg on the ground, -100 if crashing and
-0.3 each time the main engine is fired. As for the cart-pole, the observation is only acquired by the
option model, the actor model receiving an empty observation xt = 0 at each time step.
Multi-room Maze: The Multi-room Maze corresponds to a maze composed of k × k rooms (k = 2
or k = 3), with doors between them (see Figure 3). The agent always starts at the upper-left corner,
while the goal position is chosen randomly at each episode: it can be in any room, in any position and
its position changes at each episode. The reward function is -1 when moving and +20 when reaching
the goal, while 4 different actions are possible: (up, down, lef t, right). We consider two variants:
M AZE1 where xt = 0 is the empty observation and the agent must learn when to acquired the
more informative observation yt which contains the observed doors, the agent position and the goal
4https://gym.openai.com/
5
Under review as a conference paper at ICLR 2017
(a)
(b)
Figure 2: Cost/reward curves for cart-pole (a) and M AZE2 (b) with different levels of stochasticity.
The X-axis corresponds to the ratio of options used in each episode (100% means that the agent
observes yt and computes a new option at each time step). The Y-axis corresponds to the reward R
obtained on the task. The dashed lines are the R-PG performance. Note that the R-PG performance
is obtained without options (i.e using all the information available at each time step)
Figure 3: (a) (b) Examples of trajectories generated by the agent. Each point corresponds to a
position where the agent decides to acquire yt and generates a new option. (c) Trajectories generated
with the D-BONN model where K = 9.
position if the goal is in the same room than the agent (i.e the agent only observes the current room).
In the MAZE2 world, xt is the agent position in the room, while yt corresponds to the description
of the room and the goal position if the goal is in the same room (i.e contrary to MAZE1, the
agent always observes his position). The R-PG baseline has access to all these information (doors,
position, goal) at each time step. Note that this environment is much more difficult than other 4-
rooms problems (introduced by Sutton et al. (1999)) in others RL works, where there is only one or
two goal position(s), and that, in a more realistic way, the agent only observes the room he is in.
For all the environments, we consider different levels of stochasticity such that the movement of
the agent can fail with probability , in which case a random transition is applied.
the action chosen by the agent is applied with probability 1 - while a random action is chosen with
probability . The higher epsilon is, the more the environment is stochastic.
4.1	Quantitative Analysis
We illustrate the quality of BONN with cost/reward curves (see Figure 2) where the X-axis corre-
sponds to the number of times an option is computed (normalized w.r.t the size of the episode) while
the Y-axis corresponds to the overall reward R = P r(st , at) obtained on each task, for different
levels of stochasticity . Note that cost/reward curves are generated by computing the Pareto front
over all the learned models at different cost levels λ. These curves have been obtained by first learn-
6
Under review as a conference paper at ICLR 2017
Figure 4: (a) Cost/reward curve for MAZE1 and MAZE2 with a stochasticity level of = 0.25.
(b) The options latent vectors visualized through the t-SNE algorithm. Similar colors mean the goal
is in similar areas in the room, except for the red points that corresponds to the options used to reach
one of the four possible doors, or when the goal is just near a door.
(b)
ing our model with a zero cost λ = 0 and then by progressively increasing this cost, forcing the
model to acquire yt less frequently and to discover options5 .
First, one can see that even at low cost values (with only a few options computed), the BONN model
is able to keep the same performance than the R-PG model, even if R-PG uses all the information
contained in both xt and yt at each time step. Some specific cost/reward values are given in Table
1 for different environments and different values of λ, confirming that BONN is able to keep a high
performance level while discovering relevant options. Note that if the cost of computing a new
option is too expensive, the BONN model is not able to find a good policy since it is not allowed to
switch between options.
We can also see that the obtained reward decreases when the environments are more stochastic,
which seems intuitive since stochasticity makes the tasks harder. Figure 4a compares the results
obtained on the MAZE1 environment and the MAZE2 environment when λ = 0.25. We note that
the drop of performance in MAZE2 happens at a lower cost than the one in MAZE1. Indeed, in
M AZE2 , the agent has access to its position at each time step and is more able to ”compensate”
the stochasticiy of the environment than in the MAZE1 case, where the position is only available
through yt .
Figures 3b and 3a illustrates trajectories generated by the agent in the MAZE2 environment, and
the positions where the options are generated. We can see that the agent learns to observe yt only
once in each room and that the agent uses the resulting option until it reaches another room (thus
the agent deducts from yt if he must move to another room, or reach the goal if it is in the current
room). Note that the agent cannot find the shortest path to the goal’s room because, having no infor-
mation about the position of the goal in another room, it learns to explore the maze in a ”particular”
order until reaching the goal’s room. We have visualized the options latent vectors using the t-SNE
algorithm (Figure 4b). Similar colors (for example all green points) mean that the options computed
correspond to observations where the goals are in similar areas. We can for example see that all
green options are close, and it shows that the latent option space has captured a particular structure.
Analyzing this latent structure will be the topic of a future research.
The D-BONN model has been experimented on the MAZE1 2 × 2, and an example of generated
trajectories is given in Figure 3c. Each color corresponds to one of the learned discrete options. One
can see that the model is still able to learn a good policy, but the constraint over the fixed number of
discrete options clearly decreases the quality of the obtained policy. It seems thus more interesting to
use continuous options instead of discrete ones, the continuous options being regrouped in smooth
clusters as illustrated in Figure 4b.
5Learning separate models for many λ values is time-consuming and does not significantly improve the
obtained results
7
Under review as a conference paper at ICLR 2017
			=	_0		=	0.25
			R	%obs	R	%obs
Cartpole	R-PG		-200^^	1-	196.02	1
	BONN λ =	0.5	199.76	0.06	181.65	0.26
	BONN λ 二	二 1	190.346	0.05	172.23	0.20
MAZE2 3x3	R-PG BONN λ 二	二 3	-5.86 -5.67	1- 0.19	-15.82 -27.11	1 0.16
Lunar Lander	R-PG		227.35	1-	109.31	1
	BONN λ =	0.5	221.24	0.16	91.68	0.07
	BONN λ 二	二 5	210.51	0.06	90.41	0.04
Table 1: Cost/reward values for the different environments, at different cost levels λ and different
stochasticiy levels
5	Related Work
Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell,
1998) has been the surge of many different works during the last decade because it is considered
as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks.
Many different models have been proposed where subtasks are a priori known like Dietterich (1998)
which proposes the MAXQ method. The concept of option has been introduced by Sutton et al.
(1999). In this architecture, each option consists of an initiation set, its own policy (over primitive
actions or other options), and a termination function which defines the probability of ending the
option given a certain state. Other works defines hierarchical policies based on different levels
of observations: the Abstract Hidden Markov Model (Bui et al., 2002) is based on discrete options
defined on each space region, while in Heess et al. (2016) the architecture uses a low-level controller
that as only access to the proprioceptive information and a high-level controller has access to all
observations.
The concept of options is at the core of many recent articles, for example in Kulkarni et al. (2016),
the Deep Q-Learning framework is extended to integrate hierarchical value functions using intrinsic
motivation to learn the option policies. But in these different models, the options have to be manually
chosen a priori and are not discovered during the learning process. Still in the option framework,
Daniel et al. (2016) learns options (both policies and termination probabilities) without supervision
using the Expectation Maximization algorithm. More recently, Bacon & Precup (2015a) does the
same with the option-critic architecture, close to an actor-critic algorithm where options are discrete.
They used a similar experiment to ours with four rooms but only one (fixed) goal ; learning both
option policies and termination functions, the model converges to an option in the first room followed
by a second one in rooms 2 and 3. The closest work to our seems to be Bacon & Precup (2015b) but
the model is also based on a discrete set of options in the POMDP framework. Note that this article
also introduces the cognitive effort concept. Some models are focused on the problem of learning
macro-actions (Hauskrecht et al., 1998; Mnih et al., 2016). In that case a given state is mapped to
a sequence of actions (i.e macro-actions), similar than when having xt empty in the BONN-model.
But macro-actions are more restricted than options since the sequence of actions is fixed.
The main difference between these works and the BONN architecture is that, in the case of BONN,
options are latent vectors and the model is able to learn a manifold of possible options - even if a
discrete version has also been proposed with less convincing performances.
Outside reinforcement learning, our work is also in relation with the Hierarchical Multiscale Recur-
rent Neural Networks (Chung et al., 2016) that discover hierarchical structure in sequences.
6	Conclusion and Perspectives
We have proposed a new model for learning options in POMDP where the agent can choose to ac-
quire a more informative observation at each time step. The model is learned in a budgeted learning
setting where the acquisition of an additional information, and thus the use of a new option, has a
cost. The learned policy is a trade-off between the efficiency and the cognitive effort of the agent.
In our setting, the options are handled through learned latent representations, and we have also pro-
posed a discrete version of BONN where the number of options is kept constant. Experimental
8
Under review as a conference paper at ICLR 2017
results show that the model is able to extract relevant options in complex environments. This work
opens different research directions. One is to study if BONN can be applied in multi-task reinforce-
ment learning problems (the environment MAZE, since the goal position is randomly chosen at
each episode, can be seen as a particular simple multitask RL problem). Another question would be
to study problems where many different observations can be acquired by the agent at different costs
- e.g many different sensors on a robot.
Acknowlegments
This work has been supported within the Labex SMART supported by French state funds managed
by the ANR within the Investissements d’Avenir programme under reference ANR-11-LABX-65.
References
Pierre-Luc Bacon and Doina Precup. The option-critic architecture. In NIPS Deep Reinforcement
Learning Workshop, 2015a.
Pierre-Luc Bacon and Doina Precup. Learning with options: Just deliberate and relax. 2015b.
Matthew Botvinick, Yael Niv, and Andrew C. Barto. Hierarchically organized behavior and its
neural foundations: A reinforcement-learning perspective. cognition, 113.3, 2009.
Hung Hai Bui, Svetha Venkatesh, and Geoff West. Policy recognition in the abstract hidden markov
model. Journal OfArtificial Intelligence Research, 17:451-499, 2002.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural net-
works. arXiv preprint arXiv:1609.01704, 2016.
Gabriella Contardo, LUdovic Denoyer, and Thierry Artieres. Recurrent neural networks for adaptive
feature acquisition. In International Conference on Neural Information Processing, pp. 591-599.
Springer International Publishing, 2016.
Christian Daniel, Herke van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for
determining options in reinforcement learning. 2016.
Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural infor-
mation processing systems, pp. 271-271. Morgan Kaufmann Publishers, 1993.
Thomas G Dietterich. The maxq method for hierarchical reinforcement learning. In ICML, pp.
118-126. Citeseer, 1998.
Gabriel Dulac-Arnold, Ludovic Denoyer, Philippe Preux, and Patrick Gallinari. Sequential ap-
proaches for learning datum-wise sparse representations. Machine Learning, 89(1-2):87-122,
2012.
Milos Hauskrecht, Nicolas Meuleau, Leslie Pack Kaelbling, Thomas Dean, and Craig Boutilier.
Hierarchical solution of markov decision processes using macro-actions. In Proceedings of the
Fourteenth conference on Uncertainty in artificial intelligence, pp. 220-229. Morgan Kaufmann
Publishers Inc., 1998.
Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, and David Silver.
Learning and transfer of modulated locomotor controllers. arXiv preprint arXiv:1610.05182,
2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December 4,
1999], pp. 1008-1014, 1999.
9
Under review as a conference paper at ICLR 2017
Wouter Kool and Matthew Botvinick. A labor/leisure tradeoff in cognitive control. Journal of
Experimental Psychology: General, 143(1):131, 2014.
Tejas D Kulkarni, Karthik R Narasimhan, Ardavan Saeedi, and Joshua B Tenenbaum. Hierarchical
deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. arXiv
preprint arXiv:1604.06057, 2016.
Sergey Levine and Vladlen Koltun. Guided policy search. In ICML (3), pp. 1-9, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, John Agapiou, Simon Osindero, Alex Graves, Oriol Vinyals, Koray Kavukcuoglu,
et al. Strategic attentive writer for learning macro-actions. arXiv preprint arXiv:1606.04695,
2016.
Ronald Parr and Stuart Russell. Reinforcement learning with hierarchies of machines. Advances in
neural information processing systems, pp. 1043-1049, 1998.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1):181-211, 1999.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. CoRR, abs/1509.06461, 2015.
Daan Wierstra, Alexander Forster, Jan Peters, and Jurgen Schmidhuber. Recurrent policy gradients.
Logic Journal of the IGPL, 18(5):620-634, 2010.
10
Under review as a conference paper at ICLR 2017
Architecture Details
For all experiments, we used the ADAM optimizer (Kingma & Ba, 2014) with gradient clipping6
Option model: The option ot ∈ RO is generated by: ot = f(yt) = relu(Woyt) where Wo is a
matrix of parameters.
Actor model: The state of the actor is zt ∈ RZ computed by:
•	If a new option ot is generated:
zt = r(ot, xt) = tanh(Wzo concat(ot, xt))	(6)
where Wzo is a matrix of parameters.
•	else, the new state zt+1 is compute by a gated recurrent unit (GRU):
r = sigmoid(Wrxt+1 + Yaat + Urzt)	(7)
u = sigmoid(Wuxt+1 + Yuat + Uuzt)	(8)
C = tanh(WcXt+1 + Ycat + Uc(r ∙ Zt))	(9)
zt+1 = uzt + (1 - u)c	(10)
where ∙ is an element-wise multiplication and W., K and U. are matrices of parameters.
•	in both cases, the distribution over the set of actions is compute by a softmax function on
d(zt) = Wdzt where Wd is a matrix of parameters.
Acquisition model: At time t + 1, the probability to compute a new option is h(zt, at, xt+1) =
sigmoid(Uhzt + Yhat + Whxt+1) where Uh,Yh and Wh are a matrices of parameters.
MAZE environment details: The position of the agent is given by a vector of length 5 × 5 with
zeros everywhere and a one corresponding to its position in the room. The doors are encoded with
a vector of length 4 (0 if no door, 1 if a door), and the goal (if present in the same room than the
agent) is also encoded with a vector of length 5 × 5 (with only 0 if the goal is in another room).
Experiments: The only differences between experiments are the dimensions O and Z, and some
hyper-parameters. The details of the tested values are given in Table 2.
cartpole lunar lander maze1 maze2
dim of option space O
dim of xt representation
size of actor state Z
000
111
20020
10010
55
Table 2: Values of parameters for the BONN architecture. Note that the values for MAZE1 and
M AZ E2 are both for the 2 × 2 maze and the 3 × 3 maze.
6An open-source version of the model is available at https://github.com/aureliale/BONN-model.
11