Under review as a conference paper at ICLR 2017
Riemannian Optimization for
Skip-Gram Negative Sampling
Alexander Fonarev123, Alexey Grinchuk12, Gleb Gusev2, Pavel Serdyukov2, Ivan Oseledets14
1 Skolkovo Institute of Science and Technology, Moscow, Russia
2Yandex LLC, Moscow, Russia
3SBDA Group, Dublin, Ireland
4Institute of Numerical Mathematics, Russian Academy of Sciences, Moscow, Russia
newo@newo.su, oleksii.hrinchuk@skolkovotech.ru, gleb57@yandex-team.ru,
pavser@yandex-team.ru, ioseledets@skoltech.ru
Ab stract
Skip-Gram Negative Sampling (SGNS) word embedding model, well known by
its implementation in “word2vec” software, is usually optimized by stochastic gra-
dient descent. It can be shown that optimizing for SGNS objective can be viewed
as an optimization problem of searching for a good matrix with the low-rank con-
straint. The most standard way to solve this type of problems is to apply Rieman-
nian optimization framework to optimize the SGNS objective over the manifold of
required low-rank matrices. In this paper, we propose an algorithm that optimizes
SGNS objective using Riemannian optimization and demonstrates its superiority
over popular competitors, such as the original method to train SGNS and SVD
over SPPMI matrix.
1	Introduction
In this paper, we consider the problem of embedding words into a low-dimensional space in order
to measure the semantic similarity between them. As an example, how to find whether the word
“table” is semantically more similar to the word “stool” than to the word “sky”? That is achieved
by constructing a low-dimensional vector representation for each word and measuring similarity
between the words as the similarity between the corresponding vectors.
One of the most popular word embedding models by Mikolov et al. (2013) is a discriminative neural
network that optimizes Skip-Gram Negative Sampling (SGNS) objective (see Equation 3). It aims at
predicting whether two words can be found close to each other within a text. As shown in Section 2,
the process of word embeddings training using SGNS can be divided into two general steps with
clear objectives:
Step 1. Search for a low-rank matrix X that provides a good SGNS objective value;
Step 2. Search for a good low-rank representation X = WC> in terms of linguistic metrics,
where W is a matrix of word embeddings and C is a matrix of so-called context embed-
dings.
Unfortunately, most previous approaches mixed these two steps into a single one, what entails a
not completely correct formulation of the optimization problem. For example, popular approaches
to train embeddings (including the original “word2vec” implementation) do not take into account
that the objective from Step 1 depends only on the product X = WC> : instead of straightforward
computing of the derivative w.r.t. X, these methods are explicitly based on the derivatives w.r.t.
W and C, what complicates the optimization procedure. Moreover, such approaches do not take
into account that parametrization W C> of matrix X is non-unique and Step 2 is required. Indeed,
for any invertible matrix S, we have X = W1C1> = W1SS-1C1> = W2C2>, therefore, solutions
W1 C1 and W2C2 are equally good in terms of the SGNS objective but entail different cosine sim-
ilarities between embeddings and, as a result, different performance in terms of linguistic metrics
(see Section 4.2 for details).
1
Under review as a conference paper at ICLR 2017
A successful attempt to follow the above described steps, which outperforms the original SGNS op-
timization approach in terms of various linguistic tasks, was proposed by Levy & Goldberg (2014).
In order to obtain a low-rank matrix X on Step 1, the method reduces the dimensionality of Shifted
Positive Pointwise Mutual Information (SPPMI) matrix via Singular Value Decomposition (SVD).
On Step 2, it computes embeddings W and C via a simple formula that depends on the factors ob-
tained by SVD. However, this method has one important limitation: SVD provides a solution to a
surrogate optimization problem, which has no direct relation to the SGNS objective. In fact, SVD
minimizes the Mean Squared Error (MSE) between X and SPPMI matrix, what does not lead to
minimization of SGNS objective in general (see Section 6.1 and Section 4.2 in Levy & Goldberg
(2014) for details).
These issues bring us to the main idea of our paper: while keeping the low-rank matrix search setup
on Step 1, optimize the original SGNS objective directly. This leads to an optimization problem
over matrix X with the low-rank constraint, which is often (Mishra et al. (2014)) solved by applying
Riemannian optimization framework (Udriste (1994)). In our paper, we use the projector-splitting
algorithm (Lubich & Oseledets (2014)), which is easy to implement and has low computational
complexity. Of course, Step 2 may be improved as well, but we regard this as a direction of future
work.
As a result, our approach achieves the significant improvement in terms of SGNS optimization on
Step 1 and, moreover, the improvement on Step 1 entails the improvement on Step 2 in terms of
linguistic metrics. That is why, the proposed two-step decomposition of the problem makes sense,
what, most importantly, opens the way to applying even more advanced approaches based on it (e.g.,
more advanced Riemannian optimization techniques for Step 1 or a more sophisticated treatment of
Step 2).
To summarize, the main contributions of our paper are:
•	We reformulated the problem of SGNS word embedding learning as a two-step procedure
with clear objectives;
•	For Step 1, we developed an algorithm based on Riemannian optimization framework that
optimizes SGNS objective over low-rank matrix X directly;
•	Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and
the semantic similarity linguistic metric (Levy & Goldberg (2014); Mikolov et al. (2013);
Schnabel et al. (2015)).
2 Problem Setting
2.1 Skip-Gram Negative Sampling
In this paper, we consider the Skip-Gram Negative Sampling (SGNS) word embedding model
(Mikolov et al. (2013)), which is a probabilistic discriminative model. Assume we have a text cor-
pus given as a sequence of words w1 , . . . , wn, where n may be larger than 102 * * * * * * * * * 12 and wi ∈ VW
belongs to a vocabulary of words VW . A context c ∈ VC of the word wi is a word from set
{wi-L, ..., wi-1, wi+1, ..., wi+L} for some fixed window size L. Let w, c ∈ Rd be the word embed-
dings of word w and context c, respectively. Assume they are specified by the following mappings:
W : VW →Rd, C : VC →Rd.
The ultimate goal of SGNS word embedding training is to fit good mappings W and C .
In the SGNS model, the probability that pair (w, c) is observed in the corpus is modeled as a follow-
ing function:
1
P ((w, c) ∈ D|w, c) = σ(hw,ci)
1 + exp(-hw, ci) ,
(1)
where D is the multiset of all word-context pairs (w, c) observed in the corpus and hx, yi is the
scalar product of vectors x and y. Number d is a hyperparameter that adjusts the flexibility of the
model. It usually takes values from tens to hundreds.
In order to collect a training set, we take all pairs (w, c) from D as positive examples and k randomly
generated pairs (w, c) as negative ones. Let #(w, c) be the number of times the pair (w, c) appears
2
Under review as a conference paper at ICLR 2017
in D . Thereby the number of times the word w and the context c appear in D can be computed as
#(w) = Pc∈Vc #(w, c) and #(c) = Pw∈Vw #(w, c) accordingly. Then negative examples are
generated from the distribution defined by #(C) counters: PD(C) = #^). In this way, We have a
model maximizing the following logarithmic likelihood objective for each word pair (w, c):
#(w,c)(logσ(hW, CC) + k ∙ Ec'〜PD logσ(— W, C∖))∙
(2)
In order to maximize the objective over all observations for each pair (w, C), we arrive at the follow-
ing SGNS optimization problem over all possible mappings W and C:
l=	#(w, c)(logσ(hw, ci) + k ∙ E。,〜PD logσ(-hW, c0)) →
w∈VW c∈VC
max .
W,	(3)
Usually, this optimization is done via the stochastic gradient descent procedure that is performed
during passing through the corpus (Mikolov et al. (2013); Rong (2014)).
2.2	Optimization over Low-Rank Matrices
Relying on the prospect proposed by Levy & Goldberg (2014), let us show that the optimization
problem given by (3) can be considered as a problem of searching for a matrix that maximizes a
certain objective function and has the rank-d constraint (Step 1 in the scheme described in Section 1).
2.2.1	SGNS Loss Function
As shown by Levy & Goldberg (2014), the logarithmic likelihood (3) can be represented as the sum
of lw,c(W, c) over all pairs (w, C), where lw,c(W, c) has the following form:
lw,c(w, c) =#(w, c) logσ(hw, ci) + k#(w)#(C) logσ(-hW, ci).
(4)
A crucial observation is that this loss function depends only on the scalar product hw, ci but not on
embeddings w and c separately:
lw,c(w, c) = fw,c(xw,c),
fw,c (xw,c)
aw,c log σ (xw,c) + bw,c log σ(-xw,c),
where xw,c is the scalar product hW, Ci and aw,c = #(w, C), bw,c = k #(WD#(C)are constants.
2.2.2	Matrix Notation
Denote |VW | as n and |VC | as m. Let W ∈ Rn×d and C ∈ Rm×d be matrices, where each row
W ∈ Rd of matrix W is the word embedding of the corresponding word w and each row c ∈ Rd of
matrix C is the context embedding of the corresponding context C. Then the elements of the product
of these matrices
X=WC>
are the scalar products xw,c of all pairs (w, C):
X= (xw,c),	w∈VW,C∈VC.
Note that this matrix has rank d, because X equals to the product of two matrices with sizes (n × d)
and (d × m). Now we can write SGNS objective given by (3) as a function of X:
F(X) = XX fw,c(Xw,c),	F : Rn×m → R.	(5)
w∈VW c∈VC
This arrives us at the following proposition:
Proposition 1 SGNS optimization problem given by (3) can be rewritten in the following con-
strained form:
maximize F(X),
X∈Rn×m	(6)
subject to X ∈ Md ,
where Md is the manifold (Udriste (1994)) of all matrices in Rn×m with rank d:
Md = {X ∈ Rn×m : rank(X) = d}.
3
Under review as a conference paper at ICLR 2017
The key idea of this paper is to solve the optimization problem given by (6) via the framework of
Riemannian optimization, which we introduce in Section 3.
Important to note that this prospect does not suppose the optimization over parameters W and C
directly. This entails the optimization in the space with ((n + m — d) ∙ d) degrees of freedom
(Mukherjee et al. (2015)) instead of ((n + m) ∙ d), what simplifies the optimization process (see
Section 5 for the experimental results).
2.3	Computing Embeddings from a Low-Rank Solution
Once X is found, we need to recover W and C such that X = WC> (Step 2 in the scheme described
in Section 1). This problem does not have a unique solution, since if (W, C) satisfy this equation,
then WS-1 and CS> satisfy it as well for any non-singular matrix S. Moreover, different solutions
may achieve different values of the linguistic metrics (see Section 4.2 for details). While our paper
focuses on Step 1, we use, for Step 2, a heuristic approach that was proposed by Levy et al. (2015)
and it shows good results in practice. We compute SVD of X in the form X = UΣV > , where U
and V have orthonormal columns, and Σ is the diagonal matrix, and use
W = U √Σ, C = V √Σ
as matrices of embeddings.
A simple justification of this solution is the following: we need to map words into vectors in a way
that similar words would have similar embeddings in terms of cosine similarities:
cos(w1, w2)
hW1, W2)
Il wι IHIW2 Il'
It is reasonable to assume that two words are similar, if they share contexts. Therefore, we can
estimate the similarity of two words w 1, W2 as S(w 1 ,w2) = £出心 XW1 ,c ∙ XW2,c, What is the
element of the matrix XX> with indices (w1, w2). Note that XX> = UΣV>VΣU> = UΣ2U>.
If we choose W = UΣ, we exactly obtain hW1, W2i = s(w1, w2), since WW> = XX> in this
case. That is, the cosine similarity of the embeddings wι, W2 coincides with the intuitive similarity
S(w 1 ,w2). However, scaling by √Σ instead of Σ was shown by Levy et al. (2015) to be a better
solution in experiments.
3 Proposed Method
3.1	Riemannian Optimization
3.1.1	General Scheme
The main idea of Riemannian optimization (Udriste (1994)) is to consider (6) as a constrained op-
timization problem. Assume we have an approximated solution Xi on a current step of the opti-
mization process, where i is the step number. In order to improve Xi , the next step of the stan-
dard gradient ascent outputs Xi + ^F(Xi), where NF(Xi) is the gradient of objective F at the
point Xi. Note that the gradient NF(Xi) can be naturally considered as a matrix in Rn×m. Point
Xi + NF(Xi) leaves the manifold Md, because its rank is generally greater than d. That is why
Riemannian optimization methods map point Xi + NF(Xi) back to manifold Md. The standard
Riemannian gradient method first projects the gradient step onto the tangent space at the current
point Xi and then retracts it back to the manifold:
Xi+1 = R (PTM (Xi + NF(Xi))),
where R is the retraction operator, and PTM is the projection onto the tangent space.
3.1.2	Projector-Splitting Algorithm
In our paper, we use a much simpler version of such approach that retracts point Xi + NF(Xi)
directly to the manifold, as illustrated on Figure 1: Xi+1 = R(Xi + NF (Xi)).
4
Under review as a conference paper at ICLR 2017
Figure 1: Geometric interpretation of one step of projector-splitting optimization procedure: the
gradient step an the retraction of the high-rank matrix Xi + VF(Xi) to the manifold of low-rank
matrices Md.
Intuitively, retractor R finds a rank-d matrix on the manifold Md that is similar to high-rank ma-
trix Xi + VF(Xi) in terms of Frobenius norm. How can we do it? The most straightforward way to
reduce the rank ofXi + VF(Xi) is to perform the SVD, which keeps d largest singular values of it:
1: Ui +1 ,Si+1 ,Vi+ι - SVD(Xi + VF(Xi)),
2: Xi +ι J Ui +ιSi +ιVi+ι∙
(7)
However, it is computationally expensive. Instead of this approach, we use the projector-splitting
method (Lubich & Oseledets (2014)), which is a second-order retraction onto the manifold (for
details, see the review by Absil & Oseledets (2015)). Its practical implementation is also quite
intuitive: instead of computing the full SVD of Xi + VF(Xi) according to the gradient projection
method, we use just one step of the block power numerical method (Bentbib & Kanber (2015))
which computes the SVD, what reduces the computational complexity.
Let us keep the current point in the following factorized form:
Xi = Ui Si Vi> ,	(8)
where matrices Ui ∈ Rn×d and Vi ∈ Rm×d have d orthonormal columns and Si ∈ Rd×d . Then we
need to perform two QR-decompositions to retract point Xi + VF(Xi) back to the manifold:
1:	Ui+1,Si+1 J QR ((Xi + VF (Xi))Vi),
2:	Vi+1, Si>+1 J QR ((Xi + VF (Xi))>Ui+1,
3:	Xi+1 J Ui+1Si+1Vi>+1.
In this way, we always keep the solution Xi+1 = Ui+1Si+1Vi>+1 on the manifold Md and in the
form (8).
What is important, we only need to compute VF(Xi), so the gradients with respect to U, S and V
are never computed explicitly, thus avoiding the subtle case where S is close to singular (so-called
singular (critical) point on the manifold). Indeed, the gradient with respect to U (while keeping the
orthogonality constraints) can be written (Koch & Lubich (2007)) as:
∂F _ ∂FvS-ι
∂U = ∂xv ,
which means that the gradient will be large if S is close to singular. The projector-splitting scheme
is free from this problem.
3.2 Algorithm
In case of SGNS objective given by (5), an element of gradient VF has the form:
(VF (X)) w,c = f( 'w,c) = #( w, C) ∙σ (-xw,c) - k #( WwD#3 ∙ σ (Xw,c).
∂ xw,c	|D|
To make the method more flexible in terms of convergence properties, we additionally use λ ∈
R, which is a step size parameter. In this case, retractor R returns Xi + λVF (Xi ) instead of
Xi + VF(Xi) onto the manifold.
The whole optimization procedure is summarized in Algorithm 1.
5
Under review as a conference paper at ICLR 2017
Algorithm 1 Riemannian Optimization for SGNS
Require: Dimentionality d, initialization W0 and C0, step size λ, gradient function NF : Rn×m →
Rn×m, number of iterations K
Ensure: Factor W ∈ Rn×d
1:	X0 J W0 C>	# get an initial point at the manifold
2:	U0, S0, Vf> J SVD(X0)	# compute the first point satisfying the low-rank constraint
3:	i J 0
4:	while i < K do
5:	Ui+1, Si+1 J QR ((Xi + λNF(Xi))Vi)	# perform one step of the block power method
with two QR-decompositions
6:	Vi+1,Si>+1 JQR (Xi+λNF(Xi))>Ui+1
7:	Xi+1 J Ui+1Si+1Vi>+1	# update the point at the manifold
8:	i J i + 1
9:	end while
10:	U, Σ,V> J SVD(XK)
11:	W J U √Σ	# compute word embeddings
12:	return W
4	Experimental Setup
4.1	Training Models
We compare our method (“RO-SGNS” in the tables) performance to two baselines: SGNS embed-
dings optimized via Stochastic Gradient Descent, implemented in the original “word2vec”, (“SGD-
SGNS” in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix
(“SVD-SPPMI” in the tables) by Levy & Goldberg (2014). We have also experimented with the
blockwise alternating optimization over factors W and C, but the results are almost the same to SGD
results, that is why we do not to include them into the paper. The source code of our experiments is
available online1.
The models were trained on English Wikipedia “enwik9” corpus2, which was previously used in
most papers on this topic. Like in previous studies, we counted only the words which occur more
than 200 times in the training corpus (Levy & Goldberg (2014); Mikolov et al. (2013)). As a result,
we obtained a vocabulary of 24292 unique tokens (set of words VW and set of contexts VC are
equal). The size of the context window was set to 5 for all experiments, as it was done by Levy &
Goldberg (2014); Mikolov et al. (2013). We conduct two series of experiments: for dimensionality
d= 100andd= 200.
Optimization step size is chosen to be small enough to avoid huge gradient values. However, thor-
ough choice of λ does not result in a significant difference in performance (this parameter was tuned
on the training data only, the exact values used in experiments are reported below).
4.2	Evaluation
We evaluate word embeddings via the word similarity task. We use the following popular datasets
for this purpose: “wordsim-353” (Finkelstein et al. (2001); 3 datasets), “simlex-999” (Hill et al.
(2016)) and “men” (Bruni et al. (2014)). Original “wordsim-353” dataset is a mixture of the word
pairs for both word similarity and word relatedness tasks. This dataset was split (Agirre et al. (2009))
into two intersecting parts: “wordsim-sim” (“ws-sim” in the tables) and “wordsim-rel” (“ws-rel” in
the tables) to separate the words from different tasks. In our experiments, we use both of them on
a par with the full version of “wordsim-353” (“ws-full” in the tables). Each dataset contains word
pairs together with assessor-assigned similarity scores for each pair. As a quality measure, we use
Spearman’s correlation between these human ratings and cosine similarities for each pair. We call
this quality metric linguistic in our paper.
1https://github.com/newozz/riemannian_sgns
2Enwik9 corpus can be found here: http://mattmahoney.net/dc/textdata
6
Under review as a conference paper at ICLR 2017
	d = 100	d = 200
SGD-SGNS	-1.68 - 109	—1.67 ∙ 109
SVD-SPPMI	-1.65 ∙ 109	—1.65 ∙ 109
RO-SGNS	—1.44 ∙ 109	—1.43 • 109
Table 1: Comparison of SGNS values obtained by the models. The larger is better.
Dim. d	Algorithm	ws-sim	ws-rel	ws-full	simlex	men
	SGD-SGNS	0.719	0.570	0.662	0.288	0.645
d = 100	SVD-SPPMI	0.722	0.585	0.669	0.317	0.686
	RO-SGNS	0.729	0.597	0.677	0.322	0.683
	SGD-SGNS	0.733	0.584	0.677	0.317	0.664
d = 200	SVD-SPPMI	0.747	0.625	0.694	0.347	0.710
	RO-SGNS	0.757	0.647	0.709	0.353	0.701
Table 2: Comparison of the methods in terms of the semantic similarity task. Each entry represents
the Spearman’s correlation between predicted similarities and the manually assessed ones.
5	Results of Experiments
First of all, we compare the value of SGNS objective obtained by the methods. The comparison is
demonstrated in Table 1.
We see that SGD-SGNS and SVD-SPPMI methods provide quite similar results, however, the pro-
posed method obtains significantly better SGNS values, what proves the feasibility of using Rie-
mannian optimization framework in SGNS optimization problem. It is interesting to note that SVD-
SPPMI method, which does not optimize SGNS objective directly, obtains better results than SGD-
SGNS method, which aims at optimizing SGNS. This fact additionally confirms the idea described
in Section 2.2.2 that the independent optimization over parameters W and C may decrease the per-
formance.
However, the target performance measure of embedding models is the correlation between semantic
similarity and human assessment (Section 4.2). Table 2 presents the comparison of the methods in
terms of it. We see that our method outperforms the competitors on all datasets except for “men”
dataset where it obtains slightly worse results. Moreover, it is important that the higher dimension
entails higher performance gain of our method in comparison to the competitors.
In order to understand how exactly our model improves or degrades the performance in comparison
to the baseline, we found several words, whose neighbors in terms of cosine distance change signif-
icantly. Table 3 demonstrates neighbors of words “five”, “he” and “main” in terms of our model and
its nearest competitor according to the similarity task — SVD-SPPMI. These words were chosen
as representative examples whose neighborhoods in terms of SVD-SPPMI and RO-SGNS models
are strikingly different. A neighbour of a source word is bold if we suppose that it has a similar
semantic meaning to the source word. First of all, we notice that our model produces much better
neighbors of the words describing digits or numbers (see word “five” as an example). The similar
situation happens for many other words, e.g. in case of word “main” — the nearest neighbors con-
tain 4 similar words in case of our model instead of 2 in case of SVD-SPPMI. The neighbourhood
of word “he” contains less semantically similar words in case of our model. However, it filters out
completely irrelevant words, such as “promptly” and “dumbledore”.
Talking about the optimal number K of iterations in the optimization procedure and step size λ,
we found that they depend on the particular value of dimensionality d. For d = 100, we have
K = 25,λ ≈ 5 ∙ 10—5, and for d = 200, We have K = 13,λ = 10-4. Moreover, it is interesting
that the best results were obtained when SVD-SPPMI embeddings were used as an initialization of
Riemannian optimization process.
6	Related Work
6.1	Word Embeddings
Skip-Gram Negative Sampling Was introduced by Mikolov et al. (2013). The “negative sampling”
approach Was thoroughly described by Goldberg & Levy (2014), and the learning method is ex-
7
Under review as a conference paper at ICLR 2017
five				he				main			
SVD-SPPMI		RO-SGNS		SVD-SPPMI		RO-SGNS		SVD-SPPMI		RO-SGNS	
Neighbors	Dist.	Neighbors	Dist.	Neighbors	Dist.	Neighbors	Dist.	Neighbors	Dist.	Neighbors	Dist.
lb	0.748	four	0.999	She	0.918	when	0.904	major	0.631	major	0.689
kg	0.731	three	0.999	was	0.797	had	0.903	busiest	0.621	important	0.661
mm	0.670	six	0.997	promptly	0.742	was	0.901	principal	0.607	line	0.631
mk	0.651	seven	0.997	having	0.731	who	0.892	nearest	0.607	external	0.624
lbf	0.650	eight	0.996	dumbledore	0.731	she	0.884	connecting	0.591	principal	0.618
per	0.644	and	0.985	him	0.730	by	0.880	linking	0.588	primary	0.612
Table 3: Examples of the semantic neighbors obtained for words “five”, “he” and “main” by our
method and SVD-SPPMI.
plained by Rong (2014). There are several open-source implementations of SGNS neural network,
which is widely known as “word2vec” 34.
As shown in Section 2.2, Skip-Gram Negative Sampling optimization can be reformulated as a
problem of searching for a low-rank matrix. In order to be able to use out-of-the-box SVD for this
task, Levy & Goldberg (2014) used the surrogate version of SGNS as the objective function. There
are two general assumptions made in their algorithm that distinguish it from the SGNS optimization:
1.	SVD optimizes Mean Squared Error (MSE) objective instead of SGNS loss function.
2.	In order to avoid infinite elements in SPMI matrix, it is transformed in ad-hoc manner
(SPPMI matrix) before applying SVD.
This makes the objective not interpretable in terms of the original task (3). As mentioned by Levy &
Goldberg (2014), SGNS objective weighs different (w, c) pairs differently, unlike the SVD, which
works with the same weight for all pairs, what may entail the performance fall. The comprehen-
sive explanation of the relation between SGNS, SPPMI, SVD-over-SPPMI methods is provided by
Keerthi et al. (2015). Lai et al. (2015); Levy et al. (2015) give a good overview of highly practical
methods to improve these word embedding models.
6.2	Riemannian Optimization
An introduction to optimization over Riemannian manifolds can be found in the paper of Udriste
(1994). The overview of retractions of high rank matrices to low-rank manifolds is provided by Ab-
sil & Oseledets (2015). The projector-splitting algorithm was introduced by Lubich & Oseledets
(2014), and also was mentioned by Absil & Oseledets (2015) as “Lie-Trotter retraction”.
Riemannian optimization is succesfully applied to various data science problems: for example, ma-
trix completion (Vandereycken (2013)), large-scale recommender systems (Tan et al. (2014)), and
tensor completion (Kressner et al. (2014)).
7	Conclusions and Future Work
In our paper, we proposed the general two-step scheme of training SGNS word embedding model
and introduced the algorithm that performs the search of a solution in the low-rank form via Rie-
mannian optimization framework. We also demonstrated the superiority of the proposed method, by
providing the experimental comparison to the existing state-of-the-art approaches.
It seems to be an interesting direction of future work to apply more advanced optimization tech-
niques to Step 1 of the scheme proposed in Section 1 and to explore the Step 2 — obtaining embed-
dings with a given low-rank matrix.
3Original Google word2vec: https://code.google.com/archive/p/word2vec/
4Gensim word2vec: https://radimrehurek.com/gensim/models/word2vec.html
8
Under review as a conference paper at ICLR 2017
References
P-A Absil and Ivan V Oseledets. Low-rank retractions: a survey and new results. Computational
Optimization andApplications, 62(1):5-29, 2015.
Eneko Agirre, EnriqUe Alfonseca, Keith Hall, Jana Kravalova, Marius PaSca, and Aitor Soroa. A
study on similarity and relatedness using distributional and wordnet-based approaches. In NAACL,
pp. 19-27, 2009.
AH Bentbib and A Kanber. Block power method for svd decomposition. Analele Stiintifice Ale
Unversitatii Ovidius Constanta-Seria Matematica, 23(2):45-58, 2015.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni. Multimodal distributional semantics. J. Artif.
Intell. Res.(JAIR), 49(1-47), 2014.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and
Eytan Ruppin. Placing search in context: The concept revisited. In WWW, pp. 406-414, 2001.
Yoav Goldberg and Omer Levy. word2vec explained: deriving mikolov et al.’s negative-sampling
word-embedding method. arXiv preprint arXiv:1402.3722, 2014.
Felix Hill, Roi Reichart, and Anna Korhonen. Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguistics, 2016.
S Sathiya Keerthi, Tobias Schnabel, and Rajiv Khanna. Towards a better understanding of predict
and count models. arXiv preprint arXiv:1511.02024, 2015.
Othmar Koch and Christian Lubich. Dynamical low-rank approximation. SIAM J. Matrix Anal.
Appl., 29(2):434-454, 2007.
Daniel Kressner, Michael Steinlechner, and Bart Vandereycken. Low-rank tensor completion by
riemannian optimization. BIT Numerical Mathematics, 54(2):447-468, 2014.
Siwei Lai, Kang Liu, Shi He, and Jun Zhao. How to generate a good word embedding? arXiv
preprint arXiv:1507.05523, 2015.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In NIPS,
pp. 2177-2185, 2014.
Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned
from word embeddings. ACL, 3:211-225, 2015.
Christian Lubich and Ivan V Oseledets. A projector-splitting integrator for dynamical low-rank
approximation. BIT Numerical Mathematics, 54(1):171-188, 2014.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representa-
tions of words and phrases and their compositionality. In NIPS, pp. 3111-3119, 2013.
Bamdev Mishra, Gilles Meyer, Silvere Bonnabel, and Rodolphe Sepulchre. Fixed-rank matrix fac-
torizations and riemannian low-rank optimization. Computational Statistics, 29(3-4):591-621,
2014.
A Mukherjee, K Chen, N Wang, and J Zhu. On the degrees of freedom of reduced-rank estimators
in multivariate regression. Biometrika, 102(2):457-477, 2015.
Xin Rong. word2vec parameter learning explained. arXiv preprint arXiv:1411.2738, 2014.
Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. Evaluation methods for
unsupervised word embeddings. In EMNLP, 2015.
Mingkui Tan, Ivor W Tsang, Li Wang, Bart Vandereycken, and Sinno Jialin Pan. Riemannian pursuit
for big matrix recovery. In ICML, volume 32, pp. 1539-1547, 2014.
Constantin Udriste. Convex functions and optimization methods on Riemannian manifolds, volume
297. Springer Science & Business Media, 1994.
Bart Vandereycken. Low-rank matrix completion by riemannian optimization. SIAM Journal on
Optimization, 23(2):1214-1236, 2013.
9