Figure 1: A comparison of a unit hypersphere initialization with a forward correction, the Xavierinitialization, and the He initialization. Each plot shows the probability density function of a neu-ronâ€™s inputs and outputs across layers. In particular, the range of values vary widely between eachinitialization, with exponential blowups and decay for He and Xavier initializations, respectively.
Figure 3: Our weight initialization enables consis-tent, healthy gradients and a worse initializationmay require dozens of epochs to begin optimizingthe first layer due to vanishing gradients.
Figure 2: MNIST Classification Results. The first row shows the log loss curves for the tanhunit, the second row is for the ReLU, and the third the ELU. The leftmost column shows loss curveswhen there is no dropout, the middle when the dropout rate is 0.5, and rightmost is when the dropoutpreservation probability rate is 0.3. Each curve is selected by tuning over learning rates.
Figure 4: CIFAR-10 VGG Net Results. The left convergence curves show how the network trainedwith Nesterov momentum, and the right shows the training under the Adam optimizer. Training setlog losses are the darker curves, and the fainter curves are the test set log loss curves.
Figure 5: A comparison of a unit hypersphere initialization with a backward correction, the Xavierinitialization, and the He initialization. Values set to zero by dropout are removed from the normal-ized histograms. The first row has a dropout keep probability of 1.0, the second 0.6.
