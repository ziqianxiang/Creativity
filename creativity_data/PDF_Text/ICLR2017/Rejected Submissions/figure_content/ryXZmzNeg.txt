Figure 1: P (X) is the data generating distribution. We may access some samples from P (X) bydrawing samples from the training data. Qφ (Z|X) is the conditional distribution, modeled by anencoder, which maps samples from Ra to samples in Rb. An ideal encoder maps samples fromP (X) to a known, prior distribution P (Z): in reality the encoder maps samples from P (X) to anunknown distribution P(Z). P0(X|Z) is a conditional distribution, modeled by a decoder, whichmaps samples from Rb to Ra. During training the decoder learns to map samples drawn from P(Z)to P(X) rather than samples drawn from P(Z) because the decoder only sees samples from P(Z).
Figure 2: Prior work: Spherically interpolating (White, 2016) between two faces using a VAE (a,c). In (a), the attempt to gradually generate sunglasses results in visual artifacts around the eyes.
Figure 3: Reconstructions of faces from a DVAE trained with additive Gaussian noise: Q(X|X) =N(X, 0.25I). The model successfully recovers much of the detail from the noise-corrupted images.
Figure 4: Samples from a VAE (a-d), DVAE (e-h), AAE (i-l) and DAAE (m-p) trained on the CelebAdataset. (a), (e), (i) and (m) show initial samples conditioned on Z 〜 P(Z), which mainly result inrecognisable faces emerging from noisy backgrounds. After 1 step of MCMC sampling, the moreunrealistic generations change noticeably, and continue to do so with further steps. On the otherhand, realistic generations, i.e. samples from a region with high probability, do not change as much.
Figure 5: Interpolating between two faces using (a-d) a DVAE. The top rows (a, c) for each face isthe original interpolation, whilst the second rows (b, d) are the result of 5 steps of MCMC samplingapplied to the latent samples that were used to generate the original interpolation. The only qualita-tive difference when compared to VAEs (see Figure 4) is a desaturation of the generated images.
Figure 6: Interpolating between two faces using (a-d) an AAE and (e-h) a DAAE. The top rows (a,c, e, g) for each face is the original interpolation, whilst the second rows (b, d, f, h) are the resultof 5 steps of MCMC sampling applied to the latent samples that were used to generate the originalinterpolation. Although the AAE performs poorly (b, d), the regularisation effect of denoising canbe clearly seen with the DAAE after applying MCMC sampling (f, h).
Figure 7: Samples from a VAE (a-d), DVAE (e-h), AAE (i-l) and DAAE (m-p) trained on the SVHNdataset. The samples from the models imitate the blurriness present in the dataset. Although veryfew numbers are visible in the initial sample, the VAE and DVAE produce recognisable numbersfrom most of the initial samples after a few steps of MCMC sampling. Although the AAE andDAAE fail to produce recognisable numbers, the final samples are still a clear improvement over theinitial samples.
Figure 8: Interpolating between Google Street View house numbers using (a-d) a VAE and (e-h) aDVAE. The top rows (a, c, e, g) for each house number are the original interpolations, whilst thesecond rows (b, d, f, h) are the result of 5 steps of MCMC sampling. If the original interpolationproduces symbols that do not resemble numbers, as observed in (a) and (e), the models will attemptto move the samples towards more realistic numbers (b, f). Interpolation between 1- and 2-digitnumbers in an image (c, g) results in a meaningless blur in the middle of the interpolation. After afew steps of MCMC sampling the models instead produce more recognisable 1- or 2-digit numbers(d, h). We note that when the contrast is poor, denoising models in particular can struggle to recovermeaningful images (h).
