Figure 1: Wild variational inference allows us to train general stochastic neural inference networks to learn todraw (approximate) samples from the target distributions, without restriction on the computational tractabilityof the density function of the neural inference networks.
Figure 2: Results on a 1D Gaussian mixture when training the step sizes of SGLD with T = 20iterations. The target distribution p(x) is shown by the red dashed line. (a) The distribution ofthe initialization z0 of SGLD (the green line), visualized by kernel density estimator. (b)-(d) Thedistribution of the final output zT (green line) given by different types of step sizes, visualized bykernel density estimator.
Figure 3: The testing accuracy (a) and testing likelihood (b) when training Langevin inference net-work with T ∈ {10, 50, 100} layers, respectively. The results reported here are the performance ofthe final result zT outputted by the last layer of the network. We find that both amortized SVGDand KSD minimization (with U-statistics) outperform all the hand-designed learning rates. Resultsaveraged on 100 random trails.
Figure 4:	The testing accuracy (a) and testing likelihood (b) of the outputs of the intermediate layerswhen training the Langevin network with T = 100 layers. Note that both amortized SVGD andKSD minimization target to optimize the performance of the last layer, but need to optimize theprogress of the intermediate steps in order to achieve the best final results.
Figure 5:	Comparation between our methods and NUTS on 50 dimension Gaussian Mixture. (a)-(c)show the mean square errors when using different number particles to estimate expectation E(h(x))for h(x) = x, x2, and cos(x + b); for cos(ωx + b), we random draw ω ~ N(0,1) and b ~Uniform([0, 2π]) and report the average MSE over 10 random draws of and b.
Figure 6:	Comparation between our methods and NUTS For different dimension Gaussian Mixture.
