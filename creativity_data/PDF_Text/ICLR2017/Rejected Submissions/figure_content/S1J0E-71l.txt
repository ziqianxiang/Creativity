Figure 1: Illustration of St signal on a typical batch of 16 sequences of length 100 from enwik8dataset. y-axis is negative log probability in bits. Intuitively surprise signal is low when a textfragment is highly predictable (i.e. in the < timestamp > part - sequence no 10, the tag itselfis highly predictable, whereas the exact date cannot be predicted and should not be the focus ofattention). The main idea presented in this paper is that feedback signal st should be able to help indistinguishing predictable and inherently unpredictable parts during the inference phase.
Figure 2: Simple RNN; h - internal (hidden) states; x are inputs, y are optional outputs to be emitted3Under review as a conference paper at ICLR 20172.3	Feedback augmented recurrent networkserror feedbackfeedforward inputFigure 3: Surprisal-Feedback RNN; st represents surprisal (in information theory sense) - the dis-crepancy between prediction at time step t- 1 and the actual observation at time step t; it constitutesadditional input signal to be considered when making a prediction for the next time step.
Figure 3: Surprisal-Feedback RNN; st represents surprisal (in information theory sense) - the dis-crepancy between prediction at time step t- 1 and the actual observation at time step t; it constitutesadditional input signal to be considered when making a prediction for the next time step.
Figure 4: Training progress on enwik8 corpus, bits/characteruφ4ju∙,u∙,q"∕"4τnutbuh1.61.71.75	1.71.65	1.6	1.55	1.5	1.45	1.4Test Bits/Character3 ExperimentsWe ran experiments on the enwik8 dataset. It constitutes first 108 bytes of English Wikipedia dump(with all extra symbols present in XML), also known as Hutter Prize challenge dataset1 2 3. First 90%of each corpus was used for training, the next 5% for validation and the last 5% for reporting testaccuracy. In each iteration sequences of length 10000 were randomly selected. The learning algo-rithm used was Adagrad1 with a learning rate of 0.001. Weights were initialized using so-calledXavier initialization Glorot & Bengio (2010). Sequence length for BPTT was 100 and batch size128, states were carried over for the entire sequence of 10000 emulating full BPTT. Forget bias wasset initially to 1. Other parameters set to zero. The algorithm was written in C++ and CUDA 8 andran on GTX Titan GPU for up to 10 days. Table 1 presents results comparing existing state-of-the-art approaches to the introduced Feedback LSTM algorithm which outperforms all other methodsdespite not having any regularizer.
