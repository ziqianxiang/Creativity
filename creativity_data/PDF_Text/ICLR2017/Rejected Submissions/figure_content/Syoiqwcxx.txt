Figure 1: Local minima for ReLU-based regression. Both lines represent local optima, where theblue one is better than the red one.
Figure 2: On the left: exemPlary dataset constructed in ProPosition 4, color denotes label. TWomiddle ones: how big the mean of the normal distribution N(μ, σ2) has to be in order to have atleast 99% Probability of the effect (very bad local minima) described in the ProPosition 4, as afunction of number of hidden units in 2 - h - ... - h - 1 classification network. By LeCun’98initialization we mean taking weights from N(μ, 1) and by Xavier'10 from N(μ,和 ：九 土). Inboth cases the original papers used μ = 0. Rightmost one: Proposition 5, probability of learningfailing with increasing number of layers when the initialization is fully correct.
Figure 3: Plots of final training accuracy on MNIST dataset after 1,000,000 updates. Each point isa single neural net (blue triangles - 5 layer models with same number of hidden units in each layer,red triangles - 2 layer models with same number of hidden units in each layer). The title of eachcolumn shows the distribution used to initialize weights (w) and biases (b). Top row shows resultson MNIST, bottom row shows results when the labels of MNIST had been randomly permuted. Thenumber of hidden units per layer is indicated on x-axis.
Figure 4: Plots of the final train accuracy on scaled MNIST dataset after 1,200,000 updates of asingle hidden layer neural net. The title of each column shows the scaling factor applied to the data.
Figure 5: Plots of training MSE error on the Zig-Zag regression task after 2,000,000 updates. Seecaption of Figure 4 for more details. The left panel depicts the Zig-Zag regression task with threefound solutions for τ = 0.01. The actual datapoints are shown by the diamond shaped dots.
Figure 6: Examples of different outcomes of learning on the Jellyfish dataset.
