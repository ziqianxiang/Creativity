Figure 1: Topl ns. network. Single-crop top-1 vali-dation accuracies for top scoring single-model archi-tectures. We introduce with this chart our choice ofcolour scheme, which will be used throughout thispublication to distinguish effectively different archi-tectures and their correspondent authors. Notice thatnetworks of the same group share the same hue, forexample ResNet are all variations of pink.
Figure 2: Topl ns. operations, size (X parameters.
Figure 3: Inference time vs. batch size. Thischart show inference time across different batch sizeswith a logarithmic ordinate and logarithmic abscissa.
Figure 4: Power vs. batch size. Net power consump-tion (due only to the forward processing of severalDNNs) for different batch sizes. The idle power ofthe TX1 board, with no HDMI screen connected, was1.30 W on average. The max frequency componentof power supply current was 1.4 kHz, correspondingto a Nyquist sampling frequency of 2.8 kHz.
Figure 5: Memory vs. batch size. Maximum sys-tem memory utilisation for batches of different sizes.
Figure 6: Memory vs. parameters count. De-tailed view on static parameters allocation and cor-responding memory utilisation. Minimum memoryof 200 MB, linear afterwards with slope 1.30.
Figure 7: Operations ns. inference time, size (X parameters. Relationship between operations and inferencetime, for batches of size 1 and 16 (biggest size for which all architectures can still run). Not surprisingly, wenotice a linear trend, and therefore operations count represent a good estimation of inference time. Furthermore,we can notice an increase in the slope of the trend for larger batches, which correspond to shorter inferencetime due to batch processing optimisation.
Figure 8: Operations ns. power consumption, size (X parameters. Independency of power and operations isshown by a lack of directionality of the distributions shown in these scatter charts. Full resources utilisationand lower inference time for AlexNet architecture is reached with larger batches.
Figure 9: Accuracy ns. inferences per second, size X operations. Non trivial linear upper bound is shownin these scatter plots, illustrating the relationship between prediction accuracy and throughput of all examinedarchitectures. These are the first charts in which the area of the blobs is proportional to the amount of operations,instead of the parameters count. We can notice that larger blobs are concentrated on the left side of the charts,in correspondence of low throughput, i.e. longer inference times. Most of the architectures lay on the linearinterface between the grey and white areas. If a network falls in the shaded area, it means it achieves exceptionalaccuracy or inference speed. The white area indicates a suboptimal region. E.g. both AlexNet architecturesimprove processing speed as larger batches are adopted, gaining 80 Hz.
Figure 10: Accuracy per parameter vs. network. Information density (accuracy per parameters) is an effi-ciency metric that highlight that capacity of a specific architecture to better utilise its parametric space. Modelslike VGG and AlexNet are clearly oversized, and do not take fully advantage of their potential learning abil-ity. On the far right, ResNet-18, BN-NIN, GoogLeNet and ENet (marked by grey arrows) do a better job at“squeezing” all their neurons to learn the given task, and are the winners of this section.
