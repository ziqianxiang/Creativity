Figure 1: Example of grid-world and in-structions. The agent is tasked to exe-cute longer sequences of instructions aftertrained on short sequences of instructions; inaddition previously unseen instructions canbe given during evaluation (blue text). Theagent can get more rewards if it deals withrandomly appearing enemies (red outlinedbox) regardless of current instructions.
Figure 2: Overview of our architecture3Under review as a conference paper at ICLR 2017InputOutputRecurrentatStΓ∕,-1St-LbtSubtasktermination?ObservationRetrievedinstructionSUbtaSkarguments(a) Subtask controller	(b) Meta controllerFigure 3: Proposed neural network architectures. See text for details.
Figure 3: Proposed neural network architectures. See text for details.
Figure 4: Performance per number of instructions. From left to right, the plots show reward, success rate, thenumber of steps, and the average number of instructions completed respectively. Solid and dashed curves showthe performances on seen instructions and unseen instructions respectively.
Figure 5: Analysis of the learned policy. ‘Update’ shows our agent’s internal update decision. ‘Shift’ showsour agent’s memory-shift decision which is either -1, 0, or +1 from top to bottom. The bottom text shows theinstruction indicated by the memory pointer, while the top text shows the subtask chosen by the meta controller.
Figure 6: Learned policy in 3D environment. The agent observes ‘First-person-view’ images, while ‘Top-down-view’ is not available to the agent. The right text shows the list of instructions. (A) The agent cannot see thetarget block (blue) at this point due to the partially observable nature of the environment and the randomnessof the topology. The agent learned to explore the map to find the target block. (B) Although the currentinstruction is ‘Transform purple’, the agent decides to transform the green block because transforming a greenblock gives a large positive reward (stochastic event). (C) After dealing with the stochastic event, the agentresumes executing the instruction (Traansform purple). (D) The agent finishes the whole list of instructions.
Figure 7:	Value function visualization given unseen subtasks. (b-d) visualizes learned values for each positionof the agent in a grid world (a). The agent estimates high values around the target object in the world.
Figure 8:	Example of grid-world with object specification. The arrows represent the outcome of object trans-formation. Objects without arrows disappear when transformed. The agent is not allowed to go through blocksand gets a penalty for going through water.
Figure 9:	Proposed neural network architectures.
