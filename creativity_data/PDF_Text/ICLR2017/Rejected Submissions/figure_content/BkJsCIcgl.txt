Figure 1: a) The k-step predictron architecture. The first three columns illustrate 0, 1 and 2-steppathways through the predictron. The 0-step preturn reduces to standard model-free value functionapproximation; other preturns “imagine” additional steps with an internal model. Each pathwayoutputs a k-step preturn gk that accumulates discounted rewards along with a final value estimate. Inpractice all k-step preturns are computed in a single forward pass. b) The λ-predictron architecture.
Figure 2: Left: Two sample mazes from the random-maze domain. Light blue cells are empty,darker blue cells contain a wall. One maze is connected from top-left to bottom-right (indicated inblack), the other is not. Right: An example trajectory in the pool domain (before downsampling).
Figure 3: Exploring predictron variants. Aggregated prediction errors over all predictions (20for mazes, 280 for pool) for the eight predictron variants corresponding to the cube on the left (asdescribed in the main text), for both random mazes (top) and pool (bottom). Each line is the medianof RMSE over five seeds; shaded regions encompass all seeds. The full (r, γ, λ)-prediction (red)consistently performed best.
Figure 4: Comparing predictron to baselines. Aggregated prediction errors on random mazes(top) and pool (bottom) over all predictions for the eight architectures corresponding to the cube onthe left. Each line is the median of RMSE over five seeds; shaded regions encompass all seeds. Thefull (r, γ, λ)-predictron (red), consistently outperformed conventional deep network architectures(black), with and without skips and with and without weight sharing.
Figure 5: Semi-supervised learning. Prediction errors of the (r, γ, λ)-predictrons (shared core, noskips) using 0, 1, or 9 consistency updates for every update with labelled data, plotted as function ofthe number of labels consumed. Learning performance improves with more consistency updates.
Figure 6: Thinking depth. Distributions of thinking depth on pool for different types of predictionsand for different real-world discounts.
Figure 7: The predictron core used in our experiments.
Figure 8: Comparing depths. Comparing the (r, γ, λ)-predictron (red) against more conventionaldeep networks (black) for various depths (2, 4, 8, or 16 model steps, corresponding to 10, 16, 28, or52 total layers of depth). Lighter colours correspond to shallower networks. Dashed lines correspondto networks with skip connections.
Figure 9: Comparing depths. Comparing the (r, γ, λ)-predictron (red) against more conventionaldeep networks (blue) for different numbers hidden nodes in the fully connected layers, and thereforedifferent total numbers of parameters. The deep networks with 32, 128, and 512 nodes respectivelyhave 381,416, 1,275,752, and 4,853,096 parameters in total. The predictrons with 32 and 128 nodesrespectively have 1,275,752, and 4,853,096 parameters in total. Note that the number of parametersfor the 32 and 128 node predictrons are exactly equal to the number of parameters for the 128 and512 node deep networks.
Figure 10: Pool input frame. An ex-ample of a 28x28 RGB input frame inthe pool domain.
