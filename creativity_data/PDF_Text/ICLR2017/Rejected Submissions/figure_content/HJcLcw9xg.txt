Figure 1: Mapping from activity x at level l to level l + 1 and the associated preimage sets P (x).
Figure 2: Preimages at various levels of a rectifier network with input (x1, x2) and output activity(x(13), x(23)) All elements in the grey shaded area eventually get mapped to output activity (0, 0) andare irreversibly mixed.
Figure 3: Hyperplanes Π1, Π2Π3 of nullspaces for transformation kernels and the associated unitvectors e1, e2, e3 from pairwise intersections (Π2Π3) (Π1, Π3) and (Π1, Π2) respectively. Thepreimages of various points in the output are indicated as arrows or the shaded area3 Convolutional Networks have “well-behaved” preimagesConvolutional networks where the mappings W consists of convolutional matrices:(WT	0	...			0 \	0	T W	0	...		0	0	0	T WT	0	...	0	(9)0	0	0	T W	0.	.. 0	0				. . .	)	are the standard realisations of multilayer networks. Using a heuristic argument, it can be seen thatthe preimages of these networks are in general “well-behaved” in the sense that under very generalassumptions that are typically valid when training these networks the preimages generated for aspecific activity associated with an image will be semantically equivalent to the given image. Forany layer, the accumulated kernel w(l) mapping from the input space will be the concatenation withthe specific kernel w0(1) of that layer and all specific kernels from the previous layers. I.e. they aregenerated by repeated convolutions:W(I)= w0 ⑴ * w0 ⑵ * ... * w0(l-1) * w0(l)	(10)of the specific kernels w0(i) from the previous layers. In general the kernels associated with lowerlevels of the network will be associated with features such as edges in various orientation. They
Figure 4: The “general image manifold” and manifolds of individual object classes due to externalfactor and intra class variation assuming a high degree of covariance between classes due to externalfactorsFigure 5: Hyperplanes associated with kernels at various levels in a rectifier network that will avoidmixing of preimages from different classes.
Figure 5: Hyperplanes associated with kernels at various levels in a rectifier network that will avoidmixing of preimages from different classes.
Figure 6: Illustration how different classes A and B on the same manifold can be split to differentmanifolds and how the same class B on different manifolds can be merged to the same manifoldIn summary we have discussed how the properties of a network regarding it’s ability to model inputmanifolds and achieve efficient classification can be described with the concept of the preimage ofthe activities of the rectifier network at a certain level. For a specific class the set of preimagesexpressed in input image space resulting from the totality of activities at the output of the networkconstitutes the network’s model of the manifold of that class. The efficient training of a networkcan be seen as obtaining the correct model of every class to be discriminated. The preimage con-cept allows us to describe how the network is constrained by properties of these manifolds and therequirement of obtaining efficient classification at the end.
