Figure 1: The hex dump represented on the left has more information contents than the imageon the right. Only one of them can be processed by the human brain in time to save their lives.
Figure 2: Probes being added to every layer of a model. These additional probes are not supposedto change the training of the model, so we add a little diode symbol through the arrows to indicatethat the gradients will not backpropagate through those connections.
Figure 3: Toy experiment described in section 3.3,with linearly separable data (two labels), an un-trained MLP with 32 layers, and probes at ev-ery layer. We report the prediction error for ev-ery probe, where 0.50 would be the performenceof a coin flip and 0.00 would be ideal. Note thatthe layer 0 here corresponds to the raw data, andthe probes are indeed able to classify it perfectly.
Figure 4: This graphical model represents the neural network that we are going to use for MNIST.
Figure 5: We represent here the test prediction error for each probe, at the beginning and at theend of training. This measurement was obtained through early stopping based on a validation set of104 elements. The probes are prevented from overfitting the training data. We can see that, at thebeginning of training (on the left), the randomly-initialized layers were still providing useful trans-formations. The test prediction error goes from 8% to 2% simply using those random features. Thebiggest impact comes from the first ReLU. At the end of training (on the right), the test predictionerror is improving at every layer (with the exception of a minor kink on fc1_preact).
Figure 6: Examples of deep neural network with one probe at every layer (drawn above the graph).
Figure 7: A pathologically deep model with 128 layers gets an auxiliary loss added at every 16layers (refer to simplified sketch in Figure 6a if needed). This loss is added to the usual modelloss at the last layer. We fit a probe at every layer to see how well each layer would perform if itsvalues were used as a linear classifier. We plot the train prediction error associated to all the probes,at three different steps. Before adding those auxiliary losses, the model could not successfully betrained through usual gradient descent methods, but with the addition of those intermediate losses,the model is “guided” to achieve certain partial objectives. This leads to a successful training ofthe complete model. The final prediction error is not impressive, but the model was not designed toachieve state-of-the-art performance.
Figure 8: A pathologically deep model with 128 layers gets a skip connection from layer 0 to layer64 (refer to sketch in Figure 6b if needed). We fit a probe at every layer to see how well eachlayer would perform if its values were used as a linear classifier. We plot the train prediction errorassociated to all the probes, at three different steps. We can see how the model completely ignoreslayers 1-63, even when we train it for a long time. The use of probes allows us to diagnose thatproblem through visual inspection.
Figure 9: Inserting a probe at multiple moments during training the Inception v3 model on theImageNet dataset. We represent here the prediction error evaluated at a random subset of 1000features. As expected, at first all the probes have a 100% prediction error, but as training progresseswe see that the model is getting better. Note that there are 1000 classes, so a prediction error of 50%is much better than a random guess. The auxiliary head, shown under the model, was observed tohave a prediction error that was slightly better than the main head. This is not necessarily a conditionthat will hold at the end of training, but merely an observation.
