Figure 1: High level schematic of a Transition-Based Recurrent Unit (TBRU), and common networkarchitectures that can be implemented with multiple TBRUs. The discrete state is used to computerecurrences and fixed input embeddings, which are then fed through a network cell. The networkpredicts an action which is used to update the discrete state (dashed output) and provides activationsthat can be consumed through recurrences (solid output). Note that we present a slightly simplifiedversion of Stack-LSTM (Dyer et al., 2015) for clarity.
Figure 2: Using TBRUs to share fine-grained, structured representations. ToP left: A high level VieWof multi-task learning with DRAGNN in the style of multi-task seq2seq (Luong et al., 2016). Bottomleft: Extending the “stack-propagation” Zhang & Weiss (2016) idea to included dependency parsetrees as intermediate representations. Right: Unrolled TBRUs for each setup for a input fragment“Uniformed man laughed”, utilizing the transition systems described in Section 4.
Figure 3: Left: TBRU schematic. Right: Dependency parsing example. For the given gold depen-dency parse tree and a arc-standard transition state with two sub-trees on the stack is shown. Fromthis state, two possible actions are also shown (Shift and Right arc). To reproduce the tree, the Shiftaction should be taken.
Figure 4: Detailed schematic for the compositional dependency parser used in our experiments.
