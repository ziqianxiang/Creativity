Figure 1: Illustrations of hashing approaches for neural networks compression. (a) and (b) illustrateHashedNets (Chen et al., 2015a) and our HFH on one layer’s weight matriX. (C) shows HFH onAleXNet. (Best viewed in Color)•	Eq. (3) of HashedNets assigns an equality between one virtual element and one hashedvalue, i.e., an impliCit identity mapping. In Contrast, this paper uses a multivariate funCtiong(∙) to map from multiple hashed values {ξu(i,j)whu(i,j∙)}U=1 to Vij. Specifically,Vij = g ξ1(i,j)wh1(i,j), . . . , ξU(i,j)whU(i,j) ; α .	(4)Therein, a is referred to as the parameters in g(∙). Note that the input ξu(i,j)whu(ij)isorder sensitive from U = 1 to U. For optimization convenience, We choose g(∙) to be amulti-layer feed forward neural network, and other multivariate functions can be consideredas alternatives.
Figure 2: Testing errors by varying compression ratio with a fixed virtual network size.
Figure 5: Performances for pairwise semanticranking. Testing correct-to-wrong pairwiseFigure 4: Performances by compressed con-volutional NNs on CIFAR-10 data.
Figure 4: Performances by compressed con-volutional NNs on CIFAR-10 data.
