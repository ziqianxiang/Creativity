Figure 1: (a) Standard feed-forward neural network model, (b) feed-forward neural network modelwith reconstruction paths, and (c) feed-forward neural network model with reconstruction andstochastic perturbation paths.
Figure 2: Previous works for supervised learning; (a) traditional feed-forward model, and (b) jointlearning model with both supervised and unsupervised losses.
Figure 3: Ladder network; a representative model for semi-supervised learning (Rasmus et al.,2015).
Figure 4: Target network architecture; 3 convolution and 2 fully-connected layers were used forMNIST, 5 fully-connected layers were used for permutation-invariant MNIST, and 4 convolutionand 3 fully-connected layers were used for CIFAR-10.
Figure 5: Examples reconstructed from the perturbed latent vectors via (a) random perturbation,and (b) semantic perturbation (top row shows the original training examples). More examples aresummarized in Appendix (A4.1).
Figure 6: Training examples (circles or crosses with colors described below) over the examplesnot used for training (depicted as background with different colors); (a) training examples (blackcircles), (b) training examples (yellow circles) with 3× random-perturbed samples (blue crosses),and (c) training examples (yellow circles) with 3× semantic-perturbed samples (blue crosses). Bestviewed in color.
Figure 8: From top to bottom: 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. From left to right: training exam-ples (circle), training examples (circle) + random-perturbed samples (cross), and training examples(circle) + semantic-perturbed samples (cross). Best viewed in color.
