Figure 1: A fixed-point representation. Hard- Figure 2: A floating-point representation. Hard-ware parameters include the total number of bits ware parameters include the number of mantissaand the position of the radix point.	and exponent bits, and the bias.
Figure 3: Floating point multiply-accumulate (MAC) unit with various levels of detail: (a) the highlevel mathematical operation, (b) the modules that form a floating point MAC, and (c) the signalpropagation of the unit.
Figure 4: Delay and area implications of man-tissa width, normalized to a 32-bit Single Preci-sion MAC with 23 mantissa bits.
Figure 5: Speedup calculation with a fixed areabudget. The speedup exploits the improvedfunction delay and parallelism.
Figure 6: The inference accuracy versus speedup design space for each of the neural networks,showing substantial computational performance improvements for minimal accuracy degradationwhen customized precision floating-point formats are used.
Figure 7:	The speedup and energy savings as the two parameters are adjusted for the custom floatingpoint and fixed-point representations. The marked area denotes configurations where the total lossin AlexNet accuracy is less than 1%.
Figure 9: The linear fit fromthe correlation between nor-malized accuracy and lastlayer activations of the ex-act and customized preci-sion DNNs.
Figure 8:	The accumulation of weighted neuron inputs for a spe-cific neuron with various customized precision DNNs as well asthe IEEE 754 single precision floating point configuration for refer-ence. FL and FI are used to abbreviate floating point and fixed-point,respectively. The format parameters are as follows: M=mantissa,E=exponent, L=bits left of radix point, R=bits right of radix point.
Figure 10: The speedup achieved by selecting the customized precision using an exhaustive search(i.e. the ideal design) and prediction using the accuracy model with accuracy evaluated for somenumber of configurations (model + X samples). The floating-point (FL) and fixed-point (FI) resultsare shown in the top and bottom rows, respectively. The model with two evaluated designs producesthe same configurations, but requires <0.6% of the search time.
Figure 11: The speedup resulting from searching for the fastest setting with less than 1% inferenceaccuracy degradation. All selected customized precision DNNs meet this accuracy constraint.
