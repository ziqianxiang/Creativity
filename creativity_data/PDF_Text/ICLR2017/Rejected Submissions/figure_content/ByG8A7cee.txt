Figure 1: Reference-aware language models.
Figure 2:	Hierarchical RNN Seq2Seq modelWe build a model based on the hierarchical RNN model described in (Serban et al., 2016), as indialogues, the generation of the response is not only dependent on the previous sentence, but on allsentences leading to the response. We assume that a dialogue is alternated between a machine and auser. An illustration of the model is shown in Figure 2.
Figure 3: Table based decoder.
Figure 4: Recipe pointerLet the ingredients of a recipe be X = {xi}T=ι and each ingredient contains L tokens Xi ={xij}L=ι∙ The corresponding recipe is y = {yv}K=ι. We first use a LSTM to encode each in-gredient:hi,j= LSTMe (卬石Xij, hi,j-1) ViThen, we sum the resulting state of each ingredient to obtain the starting LSTM state of the decoder.
Figure 5: Coreference based language model, example taken from Wiseman et al. (2016).
Figure 6: Recipe heat map example 1. The ingredient tokens appear on the left while the recipetokens appear on the top. The first row is the p(zv |sv).
Figure 7: Recipe heat map example 2.
