Figure 1: The GELU (μ = 0,σ = 1), ReLU, and ELU(α = 1).
Figure 2: MNIST Classification Results. Left are the loss curves without dropout, and right arecurves with a dropout rate of 0.5. Each curve is the the median of five runs. Training set log lossesare the darker, lower curves, and the fainter, upper curves are the validation set log loss curves.
Figure 3: MNIST Robustness Results. Using different nonlinearities, we record the test set accuracydecline and log loss increase as inputs are noised. The MNIST classifier trained without dropoutreceived inputs with uniform noise Unif[-a, a] added to each example at different levels a, wherea = 3 is the greatest noise strength. Here GELUs display robustness matching or exceeding ELUsand ReLUs.
Figure 4: MNIST Autoencoding Results. Each curve is the median of three runs. Left are losscurves for a learning rate of 10-3, and the right figure is for a 10-4 learning rate. Light, thin curvescorrespond to test set log losses.
Figure 5: TIMIT Frame Classification. Learning curves show training set convergence, and thelighter curves show the validation set convergence.
Figure 6: CIFAR-10 Results. Each curve is the median of three runs. Learning curves show trainingset error rates, and the lighter curves show the test set error rates.
Figure 7: CIFAR-100 Wide Residual Network Results. Learning curves show training set conver-gence with dropout on, and the lighter curves show the test set convergence with dropout off.
Figure 8: Although a logistic sigmoid function approximates a Gaussian CDF, the difference is stillconspicuous and is not a suitable approximation.
Figure 9: MNIST Autoencoding Results for a learning rate of 10-5. Each curve is a median of threeruns. Light, thin curves correspond to test set log losses. Note that reconstruction errors are higherthan models trained with 10-3 or 10-4 learning rates.
