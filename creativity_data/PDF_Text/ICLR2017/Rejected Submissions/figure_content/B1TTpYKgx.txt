Figure 1: The exponential growth of trajectory length with depth, in a random deep network withhard-tanh nonlinearities. A circular trajectory is chosen between two random vectors. The image ofthat trajectory is taken at each layer of the network, and its length measured. (a,b) The trajectorylength vs. layer, in terms of the network width k and weight variance σw2 , both of which determineits growth rate. (c,d) The average ratio of a trajectory’s length in layer d + 1 relative to its length inlayer d. The solid line shows simulated data, while the dashed lines show upper and lower bounds(Theorem 1). Growth rate is a function of layer width k, and weight variance σw2 .
Figure 2: The number of transitions is linear in trajectory length. Here we compare the empiricalnumber of sign changes to the length of the trajectory, for images of the same trajectory at differentlayers of a hard-tanh network. We repeat this comparison for a variety of network architectures,with different network width k and weight variance σw2 .
Figure 3: Deep networks with piecewise linear activations subdivide input space into convex poly-topes. Here we plot the boundaries in input space separating unit activation and inactivation for allunits in a three layer ReLU network, with four units in each layer. The left pane shows activationboundaries (corresponding to a hyperplane arrangement) in gray for the first layer only, partitioningthe plane into regions. The center pane shows activation boundaries for the first two layers. Insideevery first layer region, the second layer activation boundaries form a different hyperplane arrange-ment. The right pane shows activation boundaries for the first three layers, with different hyperplanearrangements inside all first and second layer regions. This final set of convex regions correspond todifferent activation patterns of the network - i.e. different linear functions.
Figure 4: The number of functions achievable in a deep hard-tanh network by sweeping a singlelayer’s weights along a one dimensional trajectory is exponential in the remaining depth, but in-creases only slowly with network width. Here we plot the number of classification dichotomies overs = 15 input vectors achieved by sweeping the first layer weights in a hard-tanh network along aone-dimensional great circle trajectory. We show this (a) as a function of remaining depth for severalwidths, and (b) as a function of width for several remaining depths. All networks were generatedwith weight variance σw2 = 8, and bias variance σb2 = 0.
Figure 5: Expressive power depends only on remaining network depth. Here we plot the number ofdichotomies achieved by sweeping the weights in different network layers through a 1-dimensionalgreat circle trajectory, as a function of the remaining network depth. The number of achievabledichotomies does not depend on the total network depth, only on the number of layers above thelayer swept. All networks had width k = 128, weight variance σw2 = 8, number of datapointss = 15, and hard-tanh nonlinearities. The blue dashed line indicates all 2s possible dichotomies forthis random dataset.
Figure 6: Demonstration of expressive power of remaining depth on MNIST. Here we plot trainand test accuracy achieved by training exactly one layer of a fully connected neural net on MNIST.
Figure 7: We repeat a similar experiment in Figure 6 with a fully connected network on CIFAR-10,and mostly observe that training lower layers again leads to better performance. The networks hadwidth k = 200, weight variance σw2 = 1, and hard-tanh nonlinearities. We again only train from thesecond hidden layer on so that the number of parameters remains fixed.
Figure 8:	Training acts to stabilize the input-output map by decreasing trajectory length for σwlarge. The left pane plots the growth of trajectory length as a circular interpolation between twoMNIST datapoints is propagated through the network, at different train steps. Red indicates thestart of training, with purple the end of training. Interestingly, and supporting the observation onremaining depth, the first layer appears to increase trajectory length, in contrast with all later layers,suggesting it is being primarily used to fit the data. The right pane shows an identical plot but for aninterpolation between random points, which also display decreasing trajectory length, but at a slowerrate. Note the output layer is not plotted, due to artificial scaling of length through normalization.
Figure 9:	Training increases expressivity of input-output map for σw small. The left pane plots thegrowth of trajectory length as a circular interpolation between two MNIST datapoints is propagatedthrough the network, at different train steps. Red indicates the start of training, with purple theend of training. We see that the training process increases trajectory length, likely to increase theexpressivity of the input-output map to enable greater accuracy. The right pane shows an identicalplot but for an interpolation between random points, which also displays increasing trajectory length,but at a slower rate. Note the output layer is not plotted, due to artificial scaling of length throughnormalization. The network is initialized with σw2 = 3.
Figure 10: Here we plot the number of unique dichotomies that have been observed as a function ofthe number of transitions the network has undergone. Each datapoint corresponds to the number oftransitions and dichotomies for a hard-tanh network of a different depth, with the weights in the firstlayer undergoing interpolation along a great circle trajectory W(0) (t). We compare these plots to arandom walk simulation, where at each transition a single class label is flipped uniformly at random.
Figure 12: We repeat the experiment in Figure 6 for a convolutional network trained on CIFAR-10. The network has eight convolutional hidden layers, with three by three filters and 64 filters ineach layer, all with ReLU activations. The final layer is a fully connected softmax, and is trained inaddition to the single convolutional layer being trained. The results again support greater expressivepower with remaining depth. Note the final three convolutional layers failed to effectively train, andperformed at chance level.
