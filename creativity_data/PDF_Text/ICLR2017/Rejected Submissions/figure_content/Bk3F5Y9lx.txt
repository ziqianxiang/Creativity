Figure 1: Sorted activity level of latent units and corresponding generations on MNIST, for a 50-d VAE with ahidden layer of 500 units. Shown for varying values of the KL weight λ. When λ = 1, only 30 units are active.
Figure 2:	Only active units contribute to generation, whereas units that have “died” have no effect. Shown fora 50-d VAE with λ = 1.
Figure 3:	Left: Illustration of an epitomic VAE with dimension D=8, epitome size K=2 and stride S=2. In thisdepiction, the second epitome is active. Right: Learned manifolds on MNIST for 4 different epitomes in a 20-deVAE with size K = 2 and stride s = 1. We observe that each epitome specializes on a coherent subset ofexamples.
Figure 4: Adding dropout to a VAE (here, dropout rate 0.5 is shown) can prevent the model from pruningunits, shown for MNIST. However, in contrast to eVAE, it uses the additional units to encode redundancy, notadditional information, and therefore does not address the problem. Generation results are shown in Fig. 5.
Figure 5: Generations from VAE, Dropout VAE, and eVAE models for different dimensions of latent variablez. Across each row are 2-d, 5-d, 10-d, and 20-d models. VAE generation quality (1st row) degrades as latentdimension increases, and it is unable to effectively use added capacity to model greater variability. Addingdropout to the VAE (2nd row) fails to solve the problem since additional units are used to encode redundancy,not additional information. eVAE (3rd row) overcomes the problem by modeling multiple shared subspaces,here 2-d (overlapping) epitomes are maintained as the latent dimension is increased. Learned epitome manifoldsfrom the 20-d model are shown in Fig. 3. Boxed digits highlight the difference in variability that the VAE vs.
Figure 6:	Epitome size vs. Parzen log-density (nats) on MNIST, grouped by different dimensions D of latentvariable z. VAE performance for equivalent D is shown for comparison, as well as mVAE (ablative version ofeVAE without parameter sharing). For each D, the optimal epitome size is significantly smaller than D.
Figure 7:	eVAE samples for MNIST (left) and TFD (right).
Figure 8:	Reconstructions for a 50-d VAE with KL weight λ = 1, 0.5, and 0.2. The top half of each figureare the original digits, and the bottom half are the corresponding reconstructions.
Figure 9:	Reconstructions from VAE, Dropout VAE, and eVAE models for different dimensions of latentvariable z. Across each row are 2-d, 5-d, 10-d, and 20-d models. The top half of each figure are the originaldigits, and the bottom half are the corresponding reconstructions. The eVAE models multiple shared subspacesby maintaining 2-d (overlapping) epitomes as the latent dimension is increased. eVAE is the only model thatachieves both good reconstruction and generation.
Figure 10: Likelihood bound for VAE and eVAE as D increases (shown as NLL). VAE improvement of thebound is due to significant reduction of reconstruction error, but at high cost of KL, which is closely relatedto generation. eVAE improves reconstruction more moderately, but also maintains lower KL, and has strongergeneration overall.
Figure 11: Generation samples for VAE and eVAE as dimension D of latent variable z is increased. VAEsample quality decreases, which is consistent with log-density but not likelihood bound.
