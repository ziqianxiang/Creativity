Figure 1: Basic LSTM cell2016a) (Albericio et al., 2016) have proposed various approaches to eliminate ineffectual MAC op-erations with zero operands. Although these sparsity-centric optimization approaches have achievedpromising results on CNNs, much less attention has been paid to LSTM-based RNNs, because thereis a common belief that the major source of sparsity is the ReLU function, which is widely usedin the convolutional layers but not in LSTM-based RNNs. To accelerate LSTM-based RNNs andimprove the energy efficiency, we investigate opportunities to exploit sparsity in the LSTM-basedRNN training process. As an initial step, in this paper we focus on the basic LSTM cell withoutpeephole or other advanced features, as shown in Figure 1.
Figure 2: Values of gates and new cell activations of LSTM. For the three sigmoid-based gates, therange of x-axis is from 0 to 1. For the tanh-based new cell activation, the range is from -1 to 1.
Figure 3: Impact of Training with Sparsified SGDFigure 3 shows the evaluation result. We measure the change ofdW by the normalized inner productof sparsified dW and the original dW without sparisifying (the baseline shown in Figure 3). If wedenote the original weight gradient as dW0, the correlation between sparsified dW and dW0 can bemeasured by normalized inner productcorrelationdW ∙ dWo||dW l∣∙∣∣dWoll6Under review as a conference paper at ICLR 2017If the correlation is 1, it means dW is exactly the same to dW0 . If the correlation is 0, it meansdW is orthogonal to dW0 . The higher the correlation is, the less impact the sparsification has onthis single step backward propagation. From Figure 3 we can see that even without our thresholdingtechnique, the dnet still exhibits approximately 10% sparsity. These zero values are resulted fromthe limited dynamic range of floating point numbers, in which extremely small values are roundedto zero. By applying the thresholds to dnet, we can induce more sparsity shown by the bars. Evenwith a low threshold (10-8), the sparsity in dnet is increased to about 45%. With a relatively highthreshold (10-6), the sparsity can be increased to around 80%. Although the sparsity is high, thecorrelation between the sparsified dW and dW0 is close to 1 even with the high threshold. Therefore,we can hypothesize that we can safely induce a considerable amount of sparsity with an appropriate
Figure 4: Sparsity in dnet with different thresholdsFigure 4 shows the sparsity of the linear gate gradients (dnet) of each layer during the whole trainingprocess. In the baseline configuration, the training method is standard SGD without sparsifying (zero7Under review as a conference paper at ICLR 2017threshold). The baseline configuration exhibits about 20% sparsity in dnet. By applying only a lowthreshold (10-7), the sparsity is increased to around 70%. And we can consistently increase thesparsity further by raising the threshold. However, we have to monitor the impact of the thresholdon the overall training performance to check if the threshold is too large to use.
Figure 5: Validation Loss with different thresholdsFigure 5 shows the validation loss of each iteration. We observe that up to the medium threshold(10-6), the validation loss of the model trained with sparsified SGD keeps close to the baseline.
