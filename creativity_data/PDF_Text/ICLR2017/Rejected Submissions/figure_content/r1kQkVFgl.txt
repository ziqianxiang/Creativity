Figure 1: Sparse pointer network for code suggestion on a Python code snippet, showing the next-word distributions of the language model and identifier attention and their weighted combinationthrough λhtλ = xtand we use a controller to calculate a distribution λt ∈ R2 over the language model and pointernetwork for the final weighted next-word distribution y； via∈ R3k	(13)λt = softmax(Wλhtλ + bλ)	∈ R2	(14)y； = [yt it] λt	∈ R1V1	(15)Here, xt is the representation of the input token, and Wλ ∈ R2×3k and bλ ∈ R2 a trainableweight matrix and bias respectively. This controller is conditioned on the input, output and contextrepresentations. This means for deciding whether to refer to an identifier or generate from the globalvocabulary, the controller has access to information from the encoded next-word distribution ht ofthe standard neural language model, as well as the attention-weighted identifier representations ctfrom the current history.
Figure 2: Example of the Python code normalization. Original file on the left and normalized versionon the right.
Figure 3: Code suggestion example involving a reference to a variable (a-d), a long-range dependency(e-h), and the attention weights of the Sparse Pointer Network (i).
Figure 4: Full example of code suggestion with a Sparse Pointer Network. Boldface tokens on the leftshow the first declaration of an identifier. The middle part visualizes the memory of representations ofthese identifiers. The right part visualizes the output λ of the controller, which is used for interpolatingbetween the language model (LM) and the attention of the pointer network (Att).
