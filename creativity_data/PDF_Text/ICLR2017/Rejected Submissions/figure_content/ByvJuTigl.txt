Figure 1: End-to-end learnable histogramfilters. Models are learned; algorithmicstructure is giVen.
Figure 2: Information sources: prior and datamuch and which information should be provided as a prior or emerge from data, we should considerthe entire spectrum rather than limit ourselves to these two end points.
Figure 3: End-to-end learnable histogramfilter. Motion model (purple) and measure-ment model (green) are learned; the algo-rithmic structure is given (* : convolution,: element-wise multiplication).
Figure 5: Hallway task, learning curves for different metrics: (a) mean squared error of estimat-ing the continuous state—lower is better, (b) accuracy of estimation the discrete state—higher isbetter, (c) accuracy of predicting the next 32 observations—higher is better. The legend specifiesboth the architecture and the learning objective. Lines show means, shaded surfaces show standarderrors. The dashed line highlights unsupervised learning (no state labels). LSTMs trained for stateestimation cannot predict observations and therefore are not included in (c).
Figure 6: Hallway navigation task: (a-b) learned models for one environment (D=door state) and(c) belief evolution for a single test run in this environment. All methods used 4000 training samples.
Figure 7: Drone localization task: belief evolution during single test run for different methods. Blackdots/lines show the true position/trajectory of the drone. All methods used 4000 training samples.
