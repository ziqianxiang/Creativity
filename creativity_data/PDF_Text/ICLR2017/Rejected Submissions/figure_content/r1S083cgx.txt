Figure 1: A sequence of virtual targets and the corresponding ΣΛ trajectory. (a), the virtual targets and thecorresponding stroke aiming directions. (b), the virtual targets and the corresponding circular arcs. (c), a possibletrajectory generated over the given sequence of virtual targets. While the generated trajectory might appearsimilar to a polynomial curve such as a B-Spline, it also describes a smooth and physiologically plausiblevelocity profile (d).
Figure 2: ΣΛ parameter reconstruction. (a) The original and reconstructed trajectories. (b) The reconstructedvirtual targets. Note that the virtual targets define a shape that is perceptually similar to the input. (c) Alignedand scaled speed profiles of the original (gray) and reconstructed (black) trajectories. Although the dynamicinformation in the input is ignored (due to uniform sampling), the two speed profiles show similarities in numberand relative-height of peaks.
Figure 3: Data augmentation step.
Figure 4:	Predicting virtual targets. (a) Virtual targets from the test set (not seen during V2V training) usedto prime the V2V models. (b) Sequences generated with the V2V model. (c) Sequences generated with theaugmented V2V model. Note that the non-augmented V2V model produces more undesired ‘errors’. This ismore visibly noticable when rendered with dynamic parameters (Fig. 6).
Figure 5:	Dynamic parameter prediction. (a) Virtual targets from samples in the test set (not seen during training).
Figure 6:	Trajectories reconstructed with dynamic parameters predicted for generated virtual targets from Fig. 4using (b) non-augmented V2V, (c) augmented V2V.
Figure 7: Dynamic parameters generated over user specified virtual targets for the word ‘Res’, using the A2Dmodel trained on the IAM database.
Figure 8: Training with small (n = 4) datasets. (a) Training set with 4 samples. (b) Output of the networkswhen using 50× data augmentation. (c) Output of the networks with 700× data augmentation.
Figure 9:	Training with single training samples. For each row: (a) Training sample (augmented ×7000). (b)Output of combined V2V/A2D models primed on the training sample. (c) Output without priming.
Figure 10:	Style transfer mixing training sets. (a) The priming sequence from the V2V dataset (IAM). (b) A2Dis trained on a different, single user specified sample. (c) The virtual targets from (a) rendered with the dynamicparameters predicted form the A2D model from (b).
Figure 11: Style transfer using priming. The leftmost column shows the entire training set consisting of 5 userdrawn samples. The top row (slightly greyed out) shows the virtual targets for two of the training examples.
Figure 12: Lognormals with varying "skeweness" parameter α and corresponding values for μ, σ. As α → 0,the lognormal approaches a Gaussian.
Figure 13: Input key-point estimation. Left, the (smoothed) turning angle surprisal signal and the key-pointsestimated with peak detection. Right, the corresponding key-points along the input trajectory.
Figure 14: Fitting circles (dotted red) and circular arcs (red) to the input.
Figure 15: Sharpness estimation. Left, the GMM components estimated from the turning angle surprisal signal.
Figure 16: Final trajectory reconstruction step. Left, iterative adjustment of virtual target positions. Right, thefinal trajectory generated with the reconstructed dynamic parameters.
