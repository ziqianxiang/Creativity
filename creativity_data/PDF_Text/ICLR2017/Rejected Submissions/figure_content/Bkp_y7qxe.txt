Figure 1: Illustration of the architecture with four Siamese networks4.3	Model architectureThe architecture of the network uses four stacked convolution layers with 3*3 filters (inspired by(Simonyan & Zisserman, 2014)) with for each convolution layer, batch normalization, ReLu activa-tion and 2*2 maxpooling (in that order). The convolution layers have 32-64-128-256 filters/Layerrespectively. On the top of the network, there are two stacked fully connected layer (500 neuronsand then one neurons for one dimension output). Between the last convolution layer with 3*3 filtersand the fully connected layers, We insert a convolution layer with one 1*1 filters. This architecturehelps the network to choose which feature map it wants to use to build its representation. This leadsto a reduction of parameters in the fully connected layer by a factor of 256.
Figure 2: Illustration of the Baxter simulation used to generate dataDuring the training process, the sets of images compatible with a certain prior are randomly sampledfor training inside 26 of the 27 series. The last series is used for validation.
Figure 3: Sum of the cost functions at each epochFigure 4: cost functions at each epoch5.3	ResultThe resulting state representation of the head position for the validation data, after training with thedeep model presented above and the results after training with 1 fully connected layer model similarto the one used in (Jonschkowski & Brock, 2015) are on figure 5. The correlation computed betweenstate representation learned for both models and ground truth are in table 1. Those results show thatboth models are able to learn a good state representation of the Baxter head position in the casewhere no noise is present. However, table 1 shows that the deep neural network is much more robustto noise and luminosity perturbation than the one-layer-network. The evolution at each epoch of thecorrelation during bootstrapping and training are in figure 6.
Figure 4: cost functions at each epoch5.3	ResultThe resulting state representation of the head position for the validation data, after training with thedeep model presented above and the results after training with 1 fully connected layer model similarto the one used in (Jonschkowski & Brock, 2015) are on figure 5. The correlation computed betweenstate representation learned for both models and ground truth are in table 1. Those results show thatboth models are able to learn a good state representation of the Baxter head position in the casewhere no noise is present. However, table 1 shows that the deep neural network is much more robustto noise and luminosity perturbation than the one-layer-network. The evolution at each epoch of thecorrelation during bootstrapping and training are in figure 6.
Figure 5: Comparison of the ground truth of the head position for each image of the validation set(Ground truth), the estimation of the state base on the original images (State) and the state based onthe images with noise and random luminosity perturbations (State with DA). The left hand figureshows the result after training a deep neural network the right hand figure show the result aftertraining a one layer fully connected neural network	one Layer Network	Deep Networkwithout Data Augmentation	97.0%	977%with Data Augmentation	61.7 %	96.4%Table 1: Influence of the neural network deepness on Correlation between learned representationand ground-truth on the validation set.
Figure 6: Those figures show the evolution of the correlation between ground-truth and learned staterepresentation on the validation set. The left hand figure shows the correlation at each Epoch of thebootstrapping training. The right hand figure shows the correlation at each Epoch of the trainingwith data augmentation. The final results in table 1 and 2 are the absolute values of the correlationBeside the performance gain, our deep model makes it possible to learn relevant visual features thatcould be interesting for other tasks in a transfer learning scenario. For example, the left image offigure 7 shows that a button of the environment has been learned to be a good feature for the currenttask, but could obviously be used in other scenarios.
Figure 7: The figures show a representation of the activation produced by the neural network onits last convolution layer (10 pixels*10pixels). For each figure, on the left is presented a featuremap for a given image , on the right the original image (200 pixels*200pixels) and in the middlethe superposition of both images to show which part of the image produces activation. The lefthand side image shows activation produce by the image after training without data-augmentation.
