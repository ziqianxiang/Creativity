Figure 1: The Neural GPU.
Figure 2: Generalization of training error of Neural GPU is highly dependent on initialization seed.
Figure 3: Training (top) and test error (bottom) on the decimal multiplication task. (First) modelsof different sizes trained without curriculum. Only very large models can successfully optimizethe training error, and none are able to generalize. (Second) variable-sized models trained withcurriculum over different bases. For instance, we find that a medium-sized model generalizes tomuch longer test cases on decimal multiplication if it is first trained on binary multiplication andthen on quaternary multiplication, before being trained on decimal multiplication. (Third) the bestresults are achieved when we apply curriculum on models with large numbers of filters. We findthat training a model first on binary numbers, then 4-ary, then decimal works significantly betterthan training it on binary numbers, then 5-ary, then decimal. The difference is comparable to thedifference between 128 and 256 filters.
Figure 4: Influence of curriculum on 3-numbers multiplication task.
Figure 5: Task of learning binary arithmetic on multiple numbers simultaneously using the operators+, -, ร, รท.
Figure 6: Left: while trained models for addition work on random inputs, they do not reliably workon inputs containing long carry sequences. Right: occasional training runs yield models that carryover somewhat longer sequences.
Figure 7: This figure shows the importance of input argument alignment on performance on theaddition and the multiplication tasks. We find that alignment is beneficial to addition, but detrimentalto multiplication. Note that these experiments were conducted with a small neural GPU that had only24 filters, so its generalization error is not as strong as that of the larger models. However, the overallpatterns tend to carry over from small models to large ones.
