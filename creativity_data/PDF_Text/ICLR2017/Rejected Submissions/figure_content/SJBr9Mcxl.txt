Figure 1: Normalized activations of a subset of neurons for the first 400 ranked images through allconvolutional layers. For each layer we plot the normalized activation for the neurons with highestand smallest AUC (Area Under Curve), and some other examples in between these extremes. Forall neurons the highest normalized activations is 1, and the percentage of AUC is computed withrespect to the neuron AUC achieving the biggest area in the entire network.
Figure 2: Neuron Feature (NF) visualizations (top) for 5 neuronsof the different convolutional layersof VGG-M with their corresponding 100 cropped images (bottom). We scale all layers to the samesize due to space constraints.
Figure 3: Examples of NFs for each convolutional layer of the network VGG-M (see section 4.1.
Figure 5: Conv1 NFs sortedby their color selectivity in-dex.
Figure 4: Number of neurons and degree of activationas a response to their own NF. Activations values arenormalized to a specific range within each layer.
Figure 6:	Neurons with different color selectivity indexes. Images in 4 rows (1st and 3rd row areNFs, 2nd and 4th rows are sets of cropped images that maximally activates the neuron).
Figure 7:	Distribution of color selective neurons on a hue color space through layers. Maximumactivation images for 4 top color selective neurons for each layer. Dashed rings connect NFs ofcolor selective neurons through layers, from inner ring (conv1) to outer ring (conv5).
Figure 8:	Number of neurons and degree ofcolor selectivity through layers. Grayish barsare for low index values and reddish for highindex values.
Figure 9:	Number of neurons and degree ofclass selectivity through layers. Grayish barsare for low index values and bluish for highindex values.
Figure 10:	Neurons with different class selectivity indexes. For each neuron two images (top: NF,bottom: cropped images) and two tag clouds (top: leave classes, bottom: all classes in the ontology).
Figure 11:	Examples of neurons with high color and class selectivity indexes.
