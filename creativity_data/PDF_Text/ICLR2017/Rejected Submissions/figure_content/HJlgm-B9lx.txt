Figure 1: Two-Scan Approach Attention Model (TS-ATT)Second Scan: During the second scan of the text, we use another Bi-LSTM networks to learn localcontext for each word. Since already with the rough global context in mind, when looking at oneword, we actually see its context meaning rather than the word itself. Thus, we use a word’s localcontext vector instead of its word embedding here. Inspired by the work of Lai et al. (2015), for(l)	(r)word wi, we concatenate its left context ci , right context ci and embedding ei to construct itsfinal local context represetation ci , which is:ci(l)	=	f W (lc)ci(+l)1 +W(le)eici(r)	=	f W(rc)ci(-r)1+W(re)eiCi =	c(l ㊉ ei ㊉ c(r)(9)(10)(11)Attention: As we may expect, during the second scan, we don’t have to encode every word intothe text representation. Just paying attention to the parts which are important for our problem inhand would be enough. Thus, we use the global context vector hscan1 as the attention of all the localcontexts. Attention weights are acquired by one-layer feed-forward network, which is:yatt = f (Watt(hscan1 ㊉ Ci) + batt)(12)
Figure 2: Single-Scan Approach Attention Model (TS-ATT)4	ExperimentsWe evaluated our models on some benchmark datasets for sentiment classification. These datasetsincludes:•	Amazon1 by Blitzer et al. (2007). Amazon dataset contains product reviews from Ama-zon.com from many product types. This dataset is origially used for domain adaption, here,we only use it for open-domain classification, regardless of the domain difference.
Figure 4: A negtive exampleFigure 3: A positive exampleFrom Figure (3) and Figure (4), we can see that the model actually concentrations on importantlocal contexts rather than single embeddings. This further verifies our initial idea that, using aglobal context to incorporate local contexts is a suitable way for sentiment or more general textclassification task.
Figure 3: A positive exampleFrom Figure (3) and Figure (4), we can see that the model actually concentrations on importantlocal contexts rather than single embeddings. This further verifies our initial idea that, using aglobal context to incorporate local contexts is a suitable way for sentiment or more general textclassification task.
