Figure 1: With typical deep reinforcement learning techniques, an agent may forget about catastrophicfailure modes as they become unlikely under an updated policy. With intrinsic fear, we learn torecognize danger zones (red circles) around catastrophes and shape reward functions away from thesezones.
Figure 2: In experiments, we consider two toy environments (a,b) and the Atari game Seaquest (c)(c) Seaquestreplay successively plunges to its death. To be clear, the DQN does learn a near-optimal thresholdingpolicy quickly. But over the course of continued training, the agent oscillates between a reasonablethresholding policy and one which always moves right, regardless of the state. The pace of thisoscillation evens out and all networks (over multiple runs) quickly reach a constant catastrophe perturn rate (Figure 3a) that does not attenuate with continued training. How could we ever trust asystem that canâ€™t solve Adventure Seeker to make consequential real-world decisions?Cart-Pole (Figure 2b) is a classic RL environment in which an agent tries to balance a pole atop acart. Qualitatively, the game exhibits four distinct catastrophe modes. The pole could fall down to theright or fall down to the left. Additionally, the cart could run off the right boundary of the screen orrun off the left.
Figure 3: Total catastrophes and total reward for DQNs, an ESARSA variant, and Intrinsic FearDQNs on Adventure Seeker, Cart-Pole and the Atari game Seaquest. All results on AS and Cart-Pole averaged over 5 runs, Atari results averaged over 3 runs. On Adventure Seeker, all IntrinsicFear models achieve immortality within 14 runs, giving unbounded total reward and 0 catastrophesthereafter. On Seaquest, for fear factor setting of 10, the IF model achieves a near-identical catastropherate but significantly higher total reward.
