Figure 1: Similarity encoder (SimEc) architecture.
Figure 2: MNIST digits visualized in two dimensions by linear kPCA and a linear SimEc.
Figure 3: MNIST digits visualized in two dimensions by SimEcs with an increasing number ofnon-linear hidden layers and the objective to retain similarities based on class membership.
Figure 4: 20 newsgroups texts visualized in two dimensions by a non-linear SimEc with one hiddenlayer and the objective to preserve the similarities based on class membership in the embedding.
Figure 5: Continuous BOW word2vec model trained using negative sampling (Mikolov et al., 2013a;b;Goldberg & Levy, 2014).
Figure 6: Context encoder (ConEc) architecture. The input consists of a context vector, but instead ofcomparing the output to a full similarity vector, only the target word and k noise words are considered.
Figure 7: Results of the CoNLL 2003 NER task based on three random initializations of the word2vecmodel. The overall results are shown on the left, where the mean performance using word2vecembeddings is considered as our baseline indicated by the dashed lines, all other embeddings arecomputed with context encoders using various combinations of the wordsâ€™ global and local contextvectors. On the right, the increased performance (mean and std) on the test fold achieved by usingConEc is highlighted: Enhancing the word2vec embeddings with global context information yields aperformance gain of 2.5 percentage points (A). By additionally using local context vectors to createOOV word embeddings (wl = 0) we gain another 1.7 points (B). When using a combination ofglobal and local context vectors (wl = 0.4) to distinguish between the different meanings of words,the F1-score increases by another 5.1 points (C), yielding a F1-score of 39.92%, which marks asignificant improvement compared to the 30.59% reached with word2vec features.
Figure 8: MNIST digits visualized in two dimensions by isomap and a non-linear SimEc.
Figure 9: 20 newsgroups dataset embedded with linear kernel PCA and a corresponding linear SimEc.
