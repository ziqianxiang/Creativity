Figure 1: The network architectures developed for （a） our experiments with images and （b） ourexperiments with permutation-invariant data. We describe how computation proceeds through thesearchitectures at each step of a trial in Section 3.4. The current trial history h:t is input to （a） throughthe bottom-up network. The values fX（h：t）, fy （h：t）, Ve（h：t）, and ∏θ（qt|h：t） are collected from theindicated locations. We use fθx,y to denote a model’s predictions about the complete observable datax and the unobservable data y. Computation in architecture （b） proceeds from bottom to top, startingwith the history input h:t and then passing through several fully-connected layers linked by shortcutconnections. The values fχ（hQ, fy （h：t）, Vθ（h：t）, and ∏e（qt|h：t） are computed as linear functionsof the output of the shared network.
Figure 2: Model behavior on cluttered MNIST task. Zoom in for best viewing. (a) and (c) showthe first 15 peeks made by a model in a success and failure case, respectively. (b) and (c) show thecorresponding final 5 peeks for each of these trials. The model has learned a fairly efficient searchingbehavior, and is generally able to locate the digit and then concentrate its resources on that location.
Figure 4: Conditional classification for CelebA (for a description of the rows, see Figure 3). In theleft image, the model correctly guesses that the attribute “bald” is true. In right figure, the modelmakes a mistake about the attribute “narrow eyes”, even if it correctly identifies the most relevantpart to discriminate the attribute.
Figure 5: (a) Cumulative distribution of completed Hangman games with respect to the number ofwrong guesses. (b) Example execution of the hangman game guessing that weight gain with theassociated sequence of guesses and rewards.
