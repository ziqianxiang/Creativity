Figure 1: The generated samples from (a) sigmoid-DNN and (b) SFNN which uses same parameterstrained by sigmoid-DNN. One can note that SFNN can model the multiple modes in outupt space yaround x = 0.4.
Figure 2: (a) Simplified-SFNN (top) and SFNN (bottom). (b) For first 200 epochs, we train abaseline ReLU-DNN. Then, we train simplified-SFNN initialized by the DNN parameters undertransformation (8) with γ2 = 50. We observe that training ReLU-DNN* directly does not reducethe test error even when network knowledge transferring still holds between the baseline ReLU-DNN and the corresponding ReLU-DNN*. (c) As the value ofγ2 increases, knowledge transferringloss measured as 由N1' P P ∣h' (x) - h' (x) ∣ is decreasing.
Figure 4: Test errors of WRN* per eachtraining epoch on CIFAR-100.
Figure 5: The overall structures of (a) Lenet-5, (b) NIN, (c) WRN with 16 layers, and (d) WRN with28 layers. The red feature maps correspond to the stochastic ones. In case of WRN, we introduceone (v0 = 3) and two (v0 = 2) stochastic feature maps.
