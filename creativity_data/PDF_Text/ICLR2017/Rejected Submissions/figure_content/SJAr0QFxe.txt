Figure 1: An example of different position for nonlinearities in a residual unit of a 2-shortcut net-work.
Figure 2:	The average Frobenius norms of ResNets of different depths during the training process.
Figure 3:	The average Frobenius norms of 2-shortcut networks of different depths during the trainingprocess when zero initialized. Left: Without nonlinearities. Right: With ReLUs at mid positions.
Figure 4: Equivalents of two extremes of n-shortcut linear networks. 1-shortcut linear networks areequivalent to linear networks with identity initialization, while skip-all shortcuts will only changethe effective dataset outputs.
Figure 5: Initial condition numbers of Hessians for different linear networks as the depths of thenetworks increase. Means and standard deviations are estimated based on 10 runs.
Figure 6:	Maxima and 10th percentiles of absolute values of eigenvalues at different losses when thedepth is 16. For each run, eigenvalues at different losses are calculated using linear interpolation.
Figure 7:	Maxima and 10th percentiles of absolute values of eigenvalues at different losses when thedepth is 16. Eigenvalues at different losses are calculated using linear interpolation.
Figure 8:	Left: ratio of negative eigenvalues at different losses when the depth is 16. For each run,indexes at different losses are calculated using linear interpolation. Right: the dynamics of gradientand index of a 2-shortcut linear network in a single run. The gradient reaches its maximum whilethe index drops dramatically, indicating moving toward negative curvature directions.
Figure 9:	Left: Optimal Final losses of different linear networks. Right: Corresponding optimallearning rates. When the depth is 96, the final losses of Xavier with different learning rates arebasically the same, so the optimal learning rate is omitted as it is very unstable.
Figure 10:	Left: Optimal Final losses of different networks with ReLUs in mid positions. Right:Corresponding optimal learning rates. Note that as it is hard to compute the minimum losses withReLUs, we plot the log10 (final loss) instead of log10 (final loss - optimal loss). When the depth is64, the final losses of Xavier-ReLU and orthogonal-ReLU with different learning rates are basicallythe same, so the optimal learning rates are omitted as they are very unstable.
Figure 11: The Hessian in n = 2 case. It follows from Lemma 1 that only off-diagonal subblocks ineach diagonal block, i.e., the blocks marked in orange (slash) and blue (chessboard), are non-zero.
