Figure 1: Structure of a basic generative matching network, see equation (5) in section 3.1 forthe description of functions f, g and Ïˆ. Subscripts L and R denote conditional likelihood andrecognition model correspondingly.
Figure 2: Lower bound estimates (left) and entropy of prior (right) for various numbers of attentionsteps and numbers of conditioning examples. Numbers are reported for the training part of Omniglot.
Figure 3: Conditionally generated samples. First column contains conditioning data in the order itis revealed to the model. Row number t (counting from zero) consists of samples conditioned onfirst t input examples.
Figure 4: Conditionally generated samples on MNIST. Models were trained on the train part ofOmniglot. Format of the figure is similar to fig. 3.
