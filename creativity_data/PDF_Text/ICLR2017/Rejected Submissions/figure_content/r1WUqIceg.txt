Figure 1: Training loss for convolutional neural networks on CIFAR10 and CIFAR100. The verticalaxis is shown in log scale. In both cases, our proposed method achieves the best performance.
Figure 2: Behavior of the tuning coefficient dt during the experiment shown in Figure 1a. There is anoverall trend of acceleration followed by decay, but also more fine-grained behavior as indicated bythe bottom-right plot.
Figure 3: Minibatch losses for Eve during the CNN experiment on CIFAR10 (Figure 1a). Thevariance in the losses increases throughout the training.
Figure 4: Loss curves and tuning coefficient dt for batch gradient descent training of a logisticregression model. For Eve, dt continuously decreases and converges to the lower threshold 0.1.
Figure 5: The left plot shows Adam with different learning rates plotted with Eve (learning rate 10-2).
Figure 6: Loss curves for experiments with recurrent neural networks. Eve consistently achievesbetter performance than other methods.
