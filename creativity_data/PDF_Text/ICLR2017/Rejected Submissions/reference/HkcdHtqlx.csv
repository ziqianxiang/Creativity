title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Embracing data abundance: Booktest datasetfor reading comprehension,2016, arXiv preprint arXiv:1610
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Broad context language modeling asreading comprehension,2016, arXiv preprint arXiv:1610
 Attention-over-attention neural networks for reading comprehension,2016, arXiv preprint arXiv:1607
 Teaching machines to read and comprehend,2015, In Advances inNeural Information Processing Systems
 The goldilocks principle: Readingchildrenâ€™s books with explicit memory representations,2015, arXiv preprint arXiv:1511
 Long short-term memory,1997, Neural computation
 Text understanding with theattention sum reader network,2016, arXiv preprint arXiv:1603
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 A multiplicative model for learningdistributed text-based attribute representations,2014, In Advances in Neural Information ProcessingSystems
 Ask me anything: Dynamic memory networks fornatural language processing,2015, arXiv preprint arXiv:1506
 Vector-based models of semantic composition,2008, In ACL
 Recurrent models of visual attention,2014, InAdvances in Neural Information Processing Systems
 Neural semantic encoders,2016, arXiv preprint arXiv:1607
 Reasoning with memory augmented neural networks forlanguage comprehension,2016, arXiv preprint arXiv:1610
 Who did what: Alarge-scale person-centered cloze dataset,2016, EMNLP
 On the difficulty of training recurrent neuralnetworks,2012, arXiv preprint arXiv:1211
 Glove: Global vectors for wordrepresentation,2014, In Empirical Methods in Natural Language Processing (EMNLP)
 Reasonet: Learning to stop readingin machine comprehension,2016, arXiv preprint arXiv:1609
 Iterative alternating neural attention formachine reading,2016, arXiv preprint arXiv:1606
 End-to-end memory networks,2015, In Advancesin Neural Information Processing Systems
 Natural language comprehensionwith the epireader,2016, arXiv preprint arXiv:1606
 Memory networks,2014, arXiv preprintarXiv:1410
 On multi-plicative integration with recurrent neural networks,2016, arXiv preprint arXiv:1606
 Learning multi-relationalsemantics using neural-embedding models,2014, arXiv preprint arXiv:1411
 Multi-task cross-lingual sequence taggingfrom scratch,2016, arXiv preprint arXiv:1603
