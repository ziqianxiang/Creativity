title,year,conference
 Project adam: Building an efficient and scalable deeplearning training system,2014, In Proceedings of the 11th USENIX Symposium on Operating Systems Design andImplementation
 Distributed deep learning using synchronous stochastic gradientdescent,2016, arXiv preprint arXiv:1602
 Taming the wild: A unified analysis ofhogwild-style algorithms,2015, In Advances in Neural Information Processing Systems
 Large scale distributed deep networks,2012, In Advances in Neural InformationProcessing Systems
 The tail at scale,2013, Communications of the ACM
 Deep neural networks for acoustic modeling in speech recognition,2012, IEEE SignalProcessing Magazine
 Batch normalization: Accelerating deep network training by reducing internal covariateshift,2015, In Proceedings of the 32nd International Conference on Machine Learning
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXiv preprintarXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Asaga: Asynchronous parallel saga,2016, arXivpreprint arXiv:1606
 Scaling distributed machine learning with the parameter server,2014, In 11thUSENIX Symposium on Operating Systems Design and Implementation (OSDI 14)
 Perturbed iterate analysis for asynchronous stochastic optimization,2015, arXiv preprintarXiv:1507
 Hogwild: A lock-free approach to paral-lelizing stochastic gradient descent,2011, In Advances in Neural Information Processing Systems
 On variance reduction instochastic gradient descent and its asynchronous variants,2015, In Advances in Neural Information ProcessingSystems
 Rethinking the inception architecture for computervision,2016, In ArXiv 1512
 Ako: Decentraliseddeep learning with partial gradient exchange,2016, In Proceedings of the Seventh ACM Symposium on CloudComputing
 Petuum: A new platform for distributed machine learning on big data,2015, IEEETransactions on Big Data
 Deep learning with elastic averaging sgd,2015, In Advancesin Neural Information Processing Systems
 Splash: User-friendly programming interface for parallelizing stochasticalgorithms,2015, arXiv preprint arXiv:1506
 Parallelized stochastic gradient descent,2010, InAdvances in neural information processing systems
