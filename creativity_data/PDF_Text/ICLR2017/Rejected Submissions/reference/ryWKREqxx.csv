title,year,conference
 Broad context language modeling asreading comprehension,2016, Arxiv
 Attention-over-attention neural networks for reading comprehension,2016, Arxiv
 Gated-attentionreaders for text comprehension,2016, Arxiv
 Teaching machines to read and comprehend,2015, In Proceedings of theAdvances in Neural Information Processing Systems (NIPS)
 The goldilocks principle: Readingchildrens books with explicit memory representations,2016, In Proceedings of the 4th InternationalConference on Learning Representations
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the Conference on Empirical Methods on Natural LanguageProcessing 
 Text understanding with theattention sum reader network,2016, In Proceedings of the 54th Annual Meeting of the Association forComputational Linguistics
 Adam: A method for stochastic optimization,2015, In Proceedingsof the 3rd International Conference on Learning Representations
 Reasoning with memory augmented neural networks forlanguage comprehension,2016, Arxiv
 Who did what: Alarge-scale person-centered cloze dataset,2016, In Proceedings of the EMNLP
 On the difficulty of training recurrent neuralnetworks,2013, In Proceedings ofICML 
 Mctest: A challenge dataset forthe open-domain machine comprehension of text,2013, In Proceedings of the Conference on EmpiricalMethods on Natural Language Processing
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural networks,2013, Arxiv
 Reasonet: Learning to stop readingin machine comprehension,2016, Arxiv
 Iterative alternating neural attentionfor machine reading,2016, Arxiv
 Natural language comprehensionwith the epireader,2016, Arxiv
