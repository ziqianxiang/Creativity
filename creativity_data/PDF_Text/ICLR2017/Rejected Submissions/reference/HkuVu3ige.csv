title,year,conference
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Aistats
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Orthogonal rnns and long-memory tasks,2016, arXivpreprint arXiv:1602
 Long short-term memory,1997, Neural computation
 Regularizing rnns by stabilizing activations,2015, arXiv preprintarXiv:1511
 A simple way to initialize recurrent networksof rectified linear units,2015, arXiv preprint arXiv:1504
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational linguistics
 A note on riemannian optimization methods on the stiefel and the grassmannmanifolds,2005, dim
 On the difficulty of training recurrent neuralnetworks,2013, ICML (3)
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Notes on optimization on stiefel manifolds,2011, Technical report
 Lecture 6,2012,5â€”RmsProp: Divide the gradient by a running average of itsrecent magnitude
 Full-capacityunitary recurrent neural networks,2016, To appear in NIPS
