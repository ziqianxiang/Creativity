title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, ICLR
 Embracing data abundance: Booktest dataset forreading comPrehension,2016, arXiv preprint arXiv:1610
 Theano: a CPU and GPU math exPression comPiler,2010, In In Proc
 Understanding the difficulty of training deeP feedforward neuralnetworks,2010, In Aistats
 Teaching machines to read and comprehend,2015, In Advances in NeuralInformation Processing Systems
 The goldilocks principle: Readingchildrenâ€™s books with explicit memory representations,2016, ICLR
 Text understanding with theattention sum reader network,2016, arXiv preprint arXiv:1603
 Adam: A method for stochastic optimization,2015, ICLR
 On the difficulty of training recurrent neuralnetworks,2013, ICML (3)
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Mctest: A challenge dataset for theopen-domain machine comprehension of text,2013, In EMNLP
 Exact solutions to the nonlinear dynamicsof learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Iterative alternating neural attention formachine reading,2016, arXiv preprint arXiv:1606
 A parallel-hierarchical model for machine comprehension on sparse data,2016, In Proceedings of the 54th AnnualMeeting of the Association for Computational Linguistics
 Natural language comprehensionwith the epireader,2016, In EMNLP
 Learning natural language inference with lstm,2016, NAACL
