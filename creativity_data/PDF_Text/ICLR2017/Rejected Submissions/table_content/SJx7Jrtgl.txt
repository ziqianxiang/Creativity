Table 1: Unsupervised classification accuracy for MNIST with different numbers of clusters (K)(reported as percentage of correct labels)Method	K	Best Run	Average RunCatGAN (Springenberg, 2015)	20	90.30	-AAE (Makhzani et al., 2015)	16	-	90.45 ± 2.05AAE (Makhzani et al., 2015)	30	-	95.90 ± 1.13DEC (Xie et al., 2015)	10	84.30	-GMVAE (M = 1)	10	87.31	77.78 ± 5.75GMVAE (M = 10)	10	88.54	82.31 ± 3.75GMVAE (M = 1)	16	89.01	85.09 ± 1.99GMVAE (M = 10)	16	96.92	87.82 ± 5.33GMVAE (M = 1)	30	95.84	92.77 ± 1.60GMVAE (m = 10)	30	93.22	89.27 ± 2.50Figure 4: Clustering Accuracy with different numbers of clusters (K) and Monte Carlo samples(M): After only few epochs, the GMVAE converges to a solution. Increasing the number of clustersimproves the quality of the solution considerably.
Table A.1: Neural network architecture models ofqφ(x, w): The hidden layers are shared betweenq(x) and q(w), except the output layer where the neural network is split into 4 output streams, 2with dimension Nx and the other 2 with dimension Nw . We exponentiate the variance componentsto keep their value positive. An asterisk (*) indicates the use of batch normalization and a ReLUnonlinearity. For convolutional layers, the numbers in parentheses indicate stride-padding.
Table A.2: Neural network architecture models of pβ (x∣w,z): The output layers are split into 2Kstreams of output, where K streams return mean values and the other K streams output variances ofall the clusters.
Table A.3: Neural network architecture models of pθ (y|x): The network outputs are Gaussianparameters for the synthetic dataset and Bernoulli parameters for MNIST and SVHN, where we usethe logistic function to keep value of Bernoulli parameters between 0 and 1. An asterisk (*) indicatesthe use of batch normalization and a ReLU nonlinearity. For convolutional layers, the numbers inparentheses indicate stride-padding.
