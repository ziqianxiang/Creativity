Table 1: Average and best test accuracy on MNIST over 40 runs. Higher values are better.
Table 2: The effects of adding gradient noise to End-to-EndMemory NetWorks. LoWer values are better.
Table 3: The effects of adding random noise to the gradient on Neural Programmer. Higher valuesare better. Adding random noise to the gradient always helps the model. When the models areapplied to these more complicated tasks than the single column experiment, using dropout and noisetogether seems to be beneficial in one case while using only one of them achieves the best result inthe other case.
Table 4: Percentage of successful runs on the k-th element task. All tests were performed with thesame set of 100 random initializations (seeds). Higher values are better.
Table 5: Number of successful runs on 7290 random trials. Higher values are better. The models aretrained on length 20 and tested on length 200.
