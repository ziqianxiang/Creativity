Table 1: Best ranked words w.r.t. entropy (left) and norm (right) on the Amazon full review dataset.
Table 2: Performance on very small models. We use a quantization with k = 1, hashing and anextreme pruning. The last row shows the average drop of performance for different size.
Table 3: FlickrTag: Influence of quantizing the output matrix on performance. We use PQ forquantization with an optional normalization. We also retrain the output matrix after quantizing theinput one. The ”norm” refers to the separate encoding of the magnitude and angle, while ”retrain”refers to the re-training bottom-up PQ method described in Section 3.2.
Table 4: FlickrTag: Comparison of entropy pruning, norm pruning and max-cover pruning methods.
Table 5: Comparison between standard quantization methods. The original model has a dimension-ality of 8 and 2M buckets. Note that all of the methods are without dictionary.
Table 6: Comparison with different quantization and level of pruning. “co” is the cut-off parameterof the pruning.
Table 7: Comparison between CNNs and fastText with and without quantization. The numbersfor Zhang et al. (2015) are reported from Xiao & Cho (2016). Note that for the CNNs, we reportthe size of the model under the assumption that they use float32 storage. For fastText(+PQ) wereport the memory used in RAM at test time.
Table 8: Comparison with and without Bloom filters. For NPQ, we set d = 8 and k = 2.
Table 9: FlickrTag: Comparison for a large dataset of (i) different quantization methods and param-eters, (ii) with or without re-training.
