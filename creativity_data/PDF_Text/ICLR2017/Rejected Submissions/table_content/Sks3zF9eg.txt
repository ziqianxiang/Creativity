Table 1: MNIST results for DNNs. For each row, we train a network using either tanh or sin andreport the results on the test data. We then replace the activation in the trained models with the onefollowed by →, and directly recompute the accuracy on the test set without retraining the networks.
Table 2: MNIST results for RNN and LSTM.
Table 3: Reuters results for DNNs. On the training data all the original architectures — i.e. withoutchanging the activation function after training — reach an accuracy > 90%Network	tanh	tanh → sin	sin	sin → tr. sinDNN 2-L init 0.01	75.9	71.6	76.1	76.3DNN 2-L init 0.1	77.0	76.0	77.3	77.9DNN 2-L init 1	61.6	3.2	16.4	8.54.2	Learning algorithmic tasksWe test the networks using sine as activation on a couple of algorithmic tasks, such as sum ordifference of D-digits numbers in base 10. In both tasks the data is presented as a sequence ofone-hot encoded vectors, where the size of the vector at each timestep is 12: the first 10 entriescorrespond to the numbers from 0 to 9, the last two entries correspond to the operator symbol—‘+’2The network with sin, 1-L and σ = 1 reaches an accuracy of 40% on the training data after 20 epochs and83% after 1000 epochs. With two hidden layers it has random guessing accuracy on the training data after 20epochs, and after 100 epochs 100% accuracy on training data and random guessing accuracy on test data.
