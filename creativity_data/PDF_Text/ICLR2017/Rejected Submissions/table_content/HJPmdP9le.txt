Table 1: Different Read-Again Model. Ours denotes Read-Again models. C denotes copy mechanism.
Table 2: Rouge-N limited-length recall on DUC2004. Size denotes the size of decoder vocabulary in amodel.
Table 3: ROUGE Evaluation for Models with Different Decoder Size and 110k Encoder Size. Oursdenotes Read-Again. C denotes copy mechanism.
Table 4: ROUGE Evaluation for Models with Different Encoder Size and 15k Decoder Size. Oursdenotes Read-Again. C denotes copy mechanism.
Table 5:	Decoding Time (s) per Sentence of Models with Different Decoder SizeTable 6 provides some examples of visualization of the copy mechanism. Note that we are able tocopy key words from source sentences to improve the summary. From these examples we can seethat our model is able to copy different types of rare words, such as special entities’ names in case 1and 2, rare nouns in case 3 and 4, adjectives in case 5 and 6, and even rare verbs in the last example.
Table 6:	Visualization of Copy Mechanism5 ConclusionIn this paper we have proposed two simple mechanisms to alleviate the problems of current encoder-decoder models. Our first contribution is a ‘Read-Again' model which does not form a representa-tion of the input word until the whole sentence is read. Our second contribution is a copy mechanismthat can handle out-of-vocabulary words in a principled manner allowing us to reduce the decodervocabulary size and significantly speed up inference. We have demonstrated the effectiveness of ourapproach in the context of summarization and shown state-of-the-art performance. In the future, weplan to tackle summarization problems with large input text. We also plan to exploit our findings inother tasks such as machine translation.
