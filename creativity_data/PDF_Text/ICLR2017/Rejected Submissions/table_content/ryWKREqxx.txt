Table 1: Statistics to support (16) and (17). These statistics are computed for the Stanford Reader.
Table 2: Accuracy on WDW dataset. All these results are based on single model. Results for neuralreaders other than NSE are based on replications of those systems. All models were trained on therelaxed training set which uniformly yields better performance than the restricted training set. Thefirst group of models are explicit reference models and the second group are aggregation models. +indicates anonymization with better reference identifier.
Table 3: Accuracy on CNN, DailyMail, CBTest NE and CBTest CN. All results are based on a singlemodel. Results other than those involving pointer or linguistic feature annotations are taken fromthe original publications. Readers in the first group are explicit reference readers. Readers in thesecond group are aggregation readers. The final reader defies this classification.
Table 4: Training Details on Different DatasetsDataset	Embedding	Hidden State	Time Per Epoch	Trained Epochs	K-CNN	128	256	18 hours	5	丁DailyMail	128	256	2 days	5	丁WDW Relaxed	200	384	2.5 hours	8	~ΓCBTNE	384	384	1 hour	8	~ΓCBT CN 一	384	256	—	1 hour	7	—	~Γ8.2	Heat map of Stanford Reader for different answer candidatesWe randomly choose one article from CNN dataset and show softmax(eo(a)ht) for t ∈ [0, |p|] foreach answer candidate a in figure.2, figure.3, figure.4, figure.5 and figure.6. Red color indicates2http://nlp.stanford.edu/data/glove.6B.zip3http://nlp.stanford.edu/software/CRF-NER.shtml12Under review as a conference paper at ICLR 2017larger probability and orange indicates smaller probability and the remaining indicates very lowprobability that can be ignored. From those figures, we can see that our assumption that eo(a) isused to pick up its occurrence is reasonable.
