Table 1: Comparison of performance on three datasets. The performances are measure with themetric Recall@M. We report the results where M are set to 50, 100, and 200.
Table 2: Comparison for out-matrix predictions on CiteULike	ReCaU@50	Recall@100	Recall@200-CDL	-32.18%-	-43.90%-	56.36% 一DUaINet	47.51%	56.59% 一	66.36%Note that on CiteULike, there are two ways to split the data. One is the scheme in (Wang et al.,2015), and the other is the scheme in (Wang & Blei, 2011), which is the one presented in the previoussection. Note that in the former scheme, a fixed number of ratings from each user are selected fortraining. This may result in some testing items being missed in the training set. To provide a completecomparison with prior work, we use both schemes in our experiments, which are respectively denotedas CiteULike1 and CiteULike2.
Table 3: Comparison of different network architecture designs on CiteULike	Recan@10	Recan@50	Recall@100basic	-15.86%-	-38.86%-	-51.03%-multi-level	-16.89%-	-39.92%-	-51.26%-multi-level branching	17.43%	40.31%	51.78% 一gains are observed when the branching design is introduced. This shows that the branches contributea lot to the overall performance.
