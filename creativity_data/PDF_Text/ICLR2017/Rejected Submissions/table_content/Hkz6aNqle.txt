Table 1: Details of the UCI data sets (T: training samples; A: attributes; C: classes).
Table 2: Classification accuracy and standard deviation obtained by DeepECOC and the comparedapproaches on 16 UCI data sets. Here, DeePECOC(1)〜DeePECOC(3) are 3 variant of DeepECOCwith the ECOCONE coding design initialized by one-versus-one, one-versus-all and DECOC re-SPectively. The best results are highlighted in boldface.
Table 3: Details of the learning rate η and epoch on the UCI data sets.					Problem	η	Epoch	Problem	η	EpochDermatology	0.1	^^2000	Yeast	0.01	4000Iris	0.1	400	Satimage	0.01	4000Ecoli	0.1	2000	Letter	0.01	8000Wine	0.1	2000	Pendigits	0.01	2000Glass	0.01	4000	Segmentation	0.01	8000Thyroid	0.1	800	Optdigits	0.01	2000Vowel	0.1	4000	Shuttle	0.1	2000Balance	0.1	4000	Vehicle	0.1	40004.2 Classification on the USPS data setThe USPS handwritten digits data set includes 7291 training samples and 2007 test samples from 10classes. The size of the images is 16 × 16 = 256. Our experiments on this data set were divided into2 parts. Firstly, we compared DeepECOC with two traditional feature learning models (principalcomponents analysis (PCA) (Jolliffe, 2002) and marginal Fisher analysis (MFA) (Yan et al., 2007)),autoencoder (AE), denoising autoencoder (DAE), LeNet (LeCun et al., 1998), PCANet (Chan et al.,2015) and single-layer ECOC approaches. Here, PCA is an unsupervised method, MFA is a super-vised method. For MFA, the number of nearest neighbors for constructing the intrinsic graph wasset to 5, while that for constructing the penalty graph was set to 15. For DeepECOC, we also used3 coding design methods in this experiment. We used batch gradient descent for the fine-tuning
Table 4: Classification accuracy obtained on the LBP-CIFAR10 data set. The best result for eachscenario is highlighted in bold face.
