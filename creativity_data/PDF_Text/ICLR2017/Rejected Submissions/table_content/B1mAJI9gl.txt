Table 1: Classification accuracy of CNNs with random and learnable filters on CIFAR-10. A typical layerconsists of four operators: convolution, ReLU, batch normalization and max pooling. Networks with optimalfilter size and numbers of output channels are used (see Section C.1 in the supplementary materials for thearchitecture details). The random filters, assumed in our theoretical analysis, perform reasonably well, not faroff the learned filters.
Table 2: Layer-wise sparsity of VGGNet on ILSVRC-2012 validation set. “c” stands for convolutional layerswhile “p” represents pooling layers. CNN with random filters in Section 4.4 can be simulated with the samesparsity.
Table 3: Layer-wise relative reconstruction errors by different methods in activation space and image spacebetween reconstructed and original activations. For layer i, we take its activation after pooling from that layerand reconstruct it with different methods (using learned filters from the layer above or scaled Gaussian randomfilters) and feed the reconstructed activation to a pretrained corresponding decoding network.13layer	(1,1)	(1,2)	(2,1)	(2,2)	(3,1)	(3,2)	(3,3)coherence of learned filters	0.9427	0.7340	0.6435	0.7465	0.5838	0.4844	0.5194coherence of random filters	0.6701	0.1218	0.1546	0.1053	0.1099	0.0895	0.0802layer	(4,1)	(4,2)	(4,3)	(5,1)	(5,2)	(5,3)	coherence of learned filters	0.4596	0.4574	0.4043	0.4099	0.4099	0.4046	coherence of random filters	0.0920	0.0619	0.0617	0.0696	0.0674	0.0674	Table 4: Comparison of coherence between learned filters in each convolutional layer of VGGNet and Gaussianrandom filters with corresponding sizes.
Table 4: Comparison of coherence between learned filters in each convolutional layer of VGGNet and Gaussianrandom filters with corresponding sizes.
Table 5: Best-performing architecture and classification accuracy of random CNNs on CIFAR-10. “([n])[k]c”denotes a convolution layer with a stride 1, a kernel size [k] and [n] output channels, “[k]pmax” denotes a maxpooling layer with a kernel size [k] and a stride [k], and “[k]pave” denotes a average pooling layer. A typical layerconsists of four operations, namely convolution, ReLU, batch normalization, and max pooling.
Table 6: Comparison of coherence between learned filters in each layer of AlexNet and Gaussian random filterswith corresponding sizes.
Table 7: Layer-wise sparsity of AlexNet on ILSVRC-2012 validation set.
