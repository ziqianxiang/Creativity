Table 1: Padding for word decomposition in character 5-grams: ◦ is a character token indicatingthe beginning of the word, while • indicates the end of the word. The left part of the table showsour original padding scheme, which makes very different words share character 5-grams, especiallywith short, frequent words. The right part of the table shows our alleviated padding scheme.
Table 2: Best perplexity reached on the development set, on a 250K output vocabulary, after 15epochs of 2,5M n-gramsTable 2 shows that a character-level word representation helps to decrease the perplexity, even ifa larger context closes the gap. To compute the perplexity of CWE-CWE models, we use the6For short words, we add the numbers of tokens necessary for the word to have at least nC = 5 characters,as shown in table 16Under review as a conference paper at ICLR 2017Epochs	EpochsFigure 4: Model perplexity measured on the development set during training. The context size is3 words. Figure 3 shows models based on character-level word representations, with and withoutcomplete padding. Models are trained on the same data than Figure 2 but on smaller epochs (250Kn-grams).
Table 3: Best BLEU score obtained after n-best reranking of the hypothesis given by the translationand translation + k-best reinflection systems. n is the context size (in number of words)The reranking results are shown in table 3. The first line corresponds to experiments with a di-rect translation from English to Czech, where n-best lists generated by the MT system are simplyrescored by our models. The best result is given by the longest-context CWE model, which producesa +0.7 BLEU score improvement. CWE models gives on average +0.1 BLEU point compared toWE models, while CWE-CWE are -0.2 BLEU point under. Doubling the context size consistentlyimproves results of +0.2 BLEU point.
Table 4: Ratio of unknown words in system outputs measured on the test set.
