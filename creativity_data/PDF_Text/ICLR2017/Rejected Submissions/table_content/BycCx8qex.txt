Table 1: Dynamic links enable much more accurate, efficient linear-time parsing models on theTreebank Union dev set. We vary the recurrences r to explore utilizing explicit structure in theparsing TBRU. Utilizing the explicit Input(si) pointer is more effective and more efficient than aquadratic attention mechanism. Incorporating the explicit stack structure via recurrent links furtherimproves performance.
Table 2: Single- vs. multi-task learning with DRAGNN on extractive summarization. “A” is full-sentence accuracy of the extraction model, “F1” is per-token F1 score, and “LAS” is labeled parsingaccuracy on the Treebank Union News dev set. Both multi-task models that utilize the parsing dataoutperform the single-task approach, but the model that uses parses as an intermediate representationin the vein of Zhang & Weiss (2016) (Figure 2) makes better use of the data. Note that the locallynormalized model from Andor et al. (2016) obtains 30.50% accuracy and 78.72% F1 on the test setwhen trained on 100 × more data.
Table 3: Deep stacked parsing compared to state-of-the-art on PTB. ? indicates that additional re-sources beyond the Penn Treebank are used. Our model is roughly comparable to an ensemble ofmultiple Stack-LSTM models, and the most accurate without any additional resources.
