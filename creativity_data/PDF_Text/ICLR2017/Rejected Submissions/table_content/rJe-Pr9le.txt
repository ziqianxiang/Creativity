Table 1: f function of the Predictionnetwork. We apply the non-linearity be-fore the linear layer, this way we avoidalways adding positive values. TheReLU is not applied to the control in-puts.
Table 2: Valuation network. We applyLayer Normalization to bound theincoming values to the network.
Table 3: After one iteration Preditive Reinforcement Learning (PRL) has only observed random playbut it can play much better. This means that it is able to generalize well to many situations it has notobserved during training.
Table 4: The best iteration of PRL is able to surpass human performance in all three tasks. Still,state of the art model-free approaches work better.
