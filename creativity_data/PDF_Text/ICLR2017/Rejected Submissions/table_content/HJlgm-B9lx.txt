Table 1: Statistical Information of Datasetsdataset	# classes	# docs	Avg doc length	# words	# Train/(Val/)/TestAmazon	2	8,000	^^25	42,266	6,400 / 1,600IMDB	2	50,000	228	88,584	25,000 / 25,000Yelp2013	5	78,966	161	73,835	62,522 / 7,773 / 8,6711https://www.cs.jhu.edu/~mdredze∕datasets∕sentiment∕2http://ai.stanford.edu/~amaas∕data∕sentiment∕3http://ir.hit.edu.cn/~dytang/6Under review as a conference paper at ICLR 2017For IMDB, the original dataset already has a train/test split, we stick to this behavior in our exper-iments. For Amazon, we split 20% of the data as the test set. For Yelp datasets, they also have atrain/validation/test split.
Table 2: Classification Accuracy (in percentage)Model	Amazon	IMDB	Yelp2013SVM+BOW	80.31	84.88	-NB+BOW	80.75	82.98	-CNN (Kim, 2014)	83.94	85.68	55.67LSTM (Graves, 2012)	79.50	82.00	56.74Bi-LSTM (Graves, 2012)	80.13	82.54	56.35RCNN (Lai et al., 2015)	81.38	82.75	57.21NAM (Shen & Lee, 2016)	79.23	86.02	57.05TS-ATT	82.19	86.25	58.66SS-ATT	83.25	86.74	58.38As shown in Table (2), our methods (TS-ATT or SS-ATT) achieve the best accuracy on two of threedatasets (IMDN and Yelp2013). For Amazon dataset, the result of our method is almost as good asthe best model (CNN). These results demonstrate the effectiveness of our proposed models.
Table 3: Words with the largest average attentionSentiment	Words with largest average attention in IMDB datasetpositive	recommended, surprised, underrated, excellent, fascinating favorites, incredible, terrific, beautifully, funniest, amazingnegative	avoid, blah, disappointment, waste, awful, disappointing, recommended dreadful, worst, lousy, miserably, horrible, boredom, pretentiousboth polarities. In this case, a model has to rely its local context (such as not recommended orrecommended) to make a right decision. This will be illustrated in following case study.
