Table 1: Performance of subtask controller. ‘Analogy’ indicates analogy-making regularization. ‘Accuracy’represents termination prediction accuracy. We assume a termination prediction is correct only if predictionsare correct throughout the whole episode.
Table 2: Performance of meta controller. Each column corresponds to different evaluation sets of instructions,while each row corresponds to different configurations of our architecture and the flat controller. Test #3and Test #4 do not include ‘Transform/Pick up all X’ instructions. ‘TA’ indicates the meta controller withtemporal abstraction. Each entry in the table represents reward with success rate in parentheses averaged over10-best runs among 20 independent runs. ‘Shortest Path’ is a hand-designed policy which executes instructionsoptimally based on the shortest path but ignores enemies. ‘Near-Optimal’ is a near-optimal policy that executesinstructions based the shortest path and transforms enemies when they are close to the agent. ‘Forgiving’rows show the result from the forgiving environment used for curriculum learning where target objects areregenerated whenever they do not exist in the world.
Table 3: Performance on 3D environment.
Table 4: Injecting prior knowledge through analogy-making. ‘Unseen’ column shows performances on unseen‘Interact with X’ subtasks. ‘Reward’, ‘Success’, and ‘Accuracy’ represent reward, success rate, and terminationprediction accuracy, respectively.
Table 5: Comparison of the hard-architecture and the soft-architecture.
Table 6: Set of subtasks. ‘Train (Seen)’ shows subtasks used to train the subtask controller. The other uncheckedsubtasks are used as the unseen set of subtasks for evaluation.
Table 7: Set of instructions. ‘Train’ and ‘Validation’ columns show the set of instructions used for training andvalidation. The unseen set of instructions are defined as the unchecked instructions in ‘Train’ column.
Table 8: Task split.
