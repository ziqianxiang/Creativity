Table 1: Baseline Performances of Deep Learning	Baseline		Train Error (%)	Test Error (%)MNIST	0.04	0.55SVHN	0.13	3.81CIFAR-10	0.01	19.40CIFAR-100	0.17	50.903.1	Experiment 1Table 2 shows the performance of conventional and distributed transfer learnings for the first sce-nario. The first values before dash correspond to the training errors (left) and the second ones presentthe testing errors (right).
Table 2:	Performance of Conventional and Distributed Transfer Learning for Experiment 1TargetConventional MNIST SVHNMNIST	-	0.01 — 29.57SVHN 0.35 — 1.04	-TargetDistribUted MNIST SVHNMNIST	-	0.24 — 5.18SVHN 0.16 — 0.46	-TargetConventional CIFAR-10	CIFAR-100CIFAR-10	-	0.53 — 68.44CIFAR-100	0.11 — 24.08	-TargetDistribUted	CIFAR-10	CIFAR-100CIFAR-10	-	0.29 — 54.32CIFAR-100 0.05 — 18.24	-lanigirOn the other hand, transferring of SVHN from MNIST does not overfit when oUr distribUted transferlearning is employed. In both settings of original-target domains, oUr distribUted strategy oUtper-
Table 3:	Performance of Conventional and Distributed Transfer Learning for Experiment 2TargetConventional MNIST CIFAR-10MNIST	-	0.43 — 28.92CIFAR-10 0.44 — 2.37	-TargetDistribUted MNIST CIFAR-10MNIST	-	0.25 — 20.85CIFAR-10 0.23 — 0.95	-TargetConventional SVHN CIFAR-100SVHN	-	0.71 — 89.31CIFAR-100 0.01 — 12.18	-TargetDistribUted SVHN CIFAR-100SVHN	-	0.46 — 61.10CIFAR-100 0.28 — 7.25	-lanigirO lanigirtargeting of MNIST on CIFAR-10 network gives close performance to the deep learning oUtcomes.
