Table 1: MNIST Comparison: minimum loss, loss at 100th epoch, max accuracyNet	Vanilla	SoftTarget	SoftTarget+Dropout (0.2)	SoftTarget+Dropout (0.5)	Dropout (0.2)	Dropout (0.5)4 — 256	0.076|0.208|0.981	0.063|0.095|0.982	0.068|0.102|0.989	0.114|0.143|0.974	0.081|0.150|0.983	0.137|0.198|0.9785 . 512	0.077|0.206|0.984	0.056|0.069|0.986	0.060|0.113|0.985	0.101|0.117|0.978	0.087|0.164|0.984	0.088|0.170|0.9766 - 256	0.199|0.334|0.979	0.063|0.092|0.990	0.075|0.101|0.982	0.148|0.150|0.985	0.101|0.202|0.981	0.086|0.252|0.9706 — 512	0.079|0.241|0.981	0.056|0.068|0.990	0.064|0.131|0.985	0.131|0.159|0.977	0.089|0.190|0.981	0.152|0.339|0.9787 — 256	0.092|0.246|0.981	0.065|0.079|0.985	0.083|0.100|0.983	0.207|0.222|0.978	0.108|0.215|0.977	0.216|0.232|0.9687 — 512	0.090|0.244|0.982	0.056|0.077|0.985	0.071|0.107|0.985	0.172|0.211|0.978	0.099|0.236|0.983	0.175|0.383|0.9743 — 256	0.074|0.197|0.981	0.064|0.105|0.985	0.068|0.092|0.990	0.109|0.145|0.975	0.079|0.121|0.985	0.118|0.155|0.9803 — 1024	0.065|0.138|0.982	0.055|0.088|0.983	0.054|0.084|0.990	0.072|0.112|0.982	0.065|0.138|0.985	0.088|0.137|0.9833 — 2048	0.065|0.139|0.982	0.053|0.104|0.983	0.052|0.072|0.990	0.060|0.096|0.990	0.071|0.141|0.978	0.088|0.104|0.987In all our experiments, the best performing regularization for all of the architectures described aboveincluded SoftTarget regularization. Two representative results are plotted in Figure 1 for a shallow(three layer) and deep (seven layer) neural network. We saw that for deep neural networks (greaterthan three layers) SoftTarget regularization outperformed all the other regularization schemes. Forshallow (three layer) neural networks SoftTarget+Dropout was the optimal scheme.
Table 2: CIFAR-10 ComparisonAmount of Dropout	BN	SoftTarget	Just Dropout	SoftTarget+BN0	0.731|1.876|0.821	0.511∣0.592∣0.838	0.595|1.120|0.831	0.502|0.540|0.8400.2	0.517|0.855|0.866	0.450∣0.501∣0.854	0.518|0.706|0.848	0.408|0.410|0.8720.4	0.452|0.596|0.865	0.434∣0.478∣0.865	0.463|0.543|0.851	0.403|0.432|0.8710.6	0.487|0.560|0.855	0.47 ∣0.488∣0.845	0.480|0.550|0.835	0.489|0.526|0.8700.8	0.677|0.741|0.772	0.672∣0.695∣0.774	0.620|0.714|0.737	0.721|0.777|0.756(c) Dropout=0.6(d) Dropout=0.8Input → Convolution (64,3,3) → BN → ReLU → Convolution (64,3,3) → BN → ReLU → Max-Pooling ((3,3), (2,2)) → Dropout (p) → Convolution (128,3,3) → BN → ReLU → Convolution(128,3,3) → BN → ReLU → MaxPooling ((3,3), (2,2)) → Dropout (p) → Convolution (256,3,3) →BN → ReLU → Convolution (256,1,1) →BN→ReLU→ Convolution (256,1,1) →BN→ReLU→ Dropout (p) → AveragePooling ((6,6)) → Flatten () → Dense (256) → BN → ReLU → Dense(256) → BN → ReLU → Dense (256) → SoftMax.
Table 3: Residual Networks on SVHN	No Regularization	BN	SoftTarget	SoftTarget+BNTest Loss	0.254—0.347	0.298—0.404	0.244—0.244	0.237—0.249Test Accuracy	0.929—0.923	0.921—0.915	0.931—0.931	0.932—0.929(a) No SoftTarget Regularization(b) SoftTarget RegularizationFigure 3:	SoftTarget regularization applied to SVHN dataset.
