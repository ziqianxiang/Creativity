Table 1: Comparison of some properties of existing datasets vs MS MARCO. MS MARCO is theonly large dataet with open ended answers from real user queriesDatasets have played a significant role in making forward progress in difficult domains. The Ima-geNet dataset Deng et al. (2009) is one of the best known for enabling advances in image classifi-cation and detection and inspired new classes of deep learning algorithms Krizhevsky et al. (2012)Girshick et al. (2014) He et al. (2015). Reading comprehension and open domain question answer-ing is one of those domains existing systems still struggle to solve Weston et al. (2015). Here wesummarize a couple of the previous approaches towards datasets for reading comprehension andopen domain question answering.
Table 2: MS MARCO Dataset Compositioncally extracted context passages from those documents (2). Then, human judges selected relevantpassages that helped them write natural language answers to each query in a concise way (3). Fol-lowing detailed guidelines, judges used a Web-based user interface (UI) to complete this task (3 and4). A simplified example of such a UI is shown in figure 2.
Table 3: Percentage of queries containing question keywordsThe following table shows the distribution of queries across different answer types as describedearlier in this section.
Table 4: Distribution of queries based on answer-type classifier4	Experimental ResultsIn this section, we present our results over a range of experiments designed to showcase characteris-tics of MS MARCO dataset. As we discussed in section 3, human judgments are being accumulatedin order to grow the dataset to the expected scale. Along the time line various snapshots of thedataset were taken and used in thoughtfully designed experiments for validation and insights. Withdataset developing, the finalized experiment results may differ on the complete dataset, however, weexpect observations and conclusions to be reasonably representative.
Table 5: ROUGE-L of Different QA Models Tested against a Subset of MS MARCO	BLEU	pa-BLEUBest Passage	0.359	0.453Memory Network	0.340	0.341Table 6: BLEU and pa-BLEU on a Multi-Answer Subset of MS MARCOTable 5 shows the result quality from these models using ROUGE-L metric. While passages pro-vided in MS MARCO generally contains useful information for given queries, the answer generation6Under review as a conference paper at ICLR 2017nature of the problem makes it relatively challenging for simple generative models to achieve greatresults. Model advancement from Seq2Seq to Memory Networks are captured by MS MARCO onROUGE-L.
Table 6: BLEU and pa-BLEU on a Multi-Answer Subset of MS MARCOTable 5 shows the result quality from these models using ROUGE-L metric. While passages pro-vided in MS MARCO generally contains useful information for given queries, the answer generation6Under review as a conference paper at ICLR 2017nature of the problem makes it relatively challenging for simple generative models to achieve greatresults. Model advancement from Seq2Seq to Memory Networks are captured by MS MARCO onROUGE-L.
Table 7: Accuracy of MRC Models on Numeric Segment of MS MARCOFigure 1: Precision-Recall of Machine Reading Comprehension Models on MS MARCO Subset ofNumeric CategoryWe show model accuracy numbers on both datasets in table 7, and precision-recall curves on MSMARCO subset in figure 1.
