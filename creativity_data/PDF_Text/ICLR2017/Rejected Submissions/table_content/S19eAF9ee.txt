Table 1: Comparison between models. Runtime is the time spent per each mini-batch in seconds.
Table 2: Comparison of models in terms of perplexity. Zaremba et al. (2014) code6 is ran as bench-mark algorithm. The original Zaremba et al. (2014) code used as input representation for xt the200-dim embedding representation of words, computed here by the gensim library5 . As our modelruns on the 10,000-dim one-hot representation of words, we also ran Zaremba et al. (2014) code onthis representation. We re-implemented Zaremba et al. (2014) code with the same architecture andhyperparameters. We remind that GCRN-M1 refers to GCRN Model 1 defined in (8).
