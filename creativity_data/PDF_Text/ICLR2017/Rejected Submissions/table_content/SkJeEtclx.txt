Table 1: Ablation of our model with and without the HAM component on the test set of 670 videosMethod	METEOR	BLEU@1	BLEU@2	BLEU@3	BLEU@4	CIDErAtt + No TEM	31.20	77.90	65.10	55.3	44.90	63.90No HAM + TEM	30.5	78.10	65.20	55.10	44.60	60.50HAM + No TEM	31.0	78.70	66.90	57.40	47.0	62.10HAM + TEM	31.70	79.0	66.20	56.0	45.6	62.20Performance ComparisonNext, to extensively evaluate our model, we compare our model with state-of-the-art models andbaselines for the video caption generation task on the MSVD dataset. In this experiment, we use 8frames per video as the inputs to the TEM module. Table 25 shows the results for this experiment. Asthe results show, our model gets state-of-the-art scores either in BLEU-4 or METEOR, compared toother methods. This is particularly noteworthy because we do not use external features for the video,such as Optical Flow (Brox et al., 2004) (denoted as Flow in table), 3-Dimensional ConvolutionalNetwork features (Tran et al., 2015) (denoted as C3D), or fine-tuned CNN features (denoted as FT)on the action recognition task with dataset such as UCF-101. The only exception happens whenwe compare our model with (Yu et al., 2016), who uses C3D features. In this method, adding C3Dfeatures leads to a huge improvement in their results (compare row 4 with 11 in Table 2). On theother hand, our method without using any external features can achieve better results in comparisonwith all other methods. This is important because our proposed architecture can alone not only learna representation for video that can model the temporal structure of a video sequence, but also a
Table 2: Video captioning evaluation on the test set of 670 videos in MSVD.
Table 3: Video captioning evaluation on the test set of 1863 videos in Charades.
