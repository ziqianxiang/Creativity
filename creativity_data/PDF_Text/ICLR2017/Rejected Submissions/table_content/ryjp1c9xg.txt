Table 1: This table compares the number of computational steps performed by various architecturesas a function of their input size. In our notation, O 1 means that the number of steps (or the size ofthe input) are constant. In contrast, O f(n) means that the input can be of variable length n, andthat the number of steps grows as f(n) in the size of the input.
Table 2: The table summarizes the setting where the Neural GPU succeeds to learn decimal multi-plication. It is essential to use a large model and to use a high-quality curriculum. The curriculumcolumn indicates the base in which training took place; i.e. 2 → 4 → 10 means that we have startedtraining in the binary base, then we moved to quaternary base, and finally to the decimal.
Table 3: Neural GPU generalizes well to random 100-digit examples. However, it might fail tomultiply single digit examples with large number of prepended zeros. Models trained on differentseeds suffer on different examples with prepended zeros. Therefore, problem is removable by modelaveraging. However, they all consistently fail on examples that require carrying for many steps.
