Figure 1: Designing CNN Architectures with Q-learning: The agent begins by sampling a Con-volutional Neural Network (CNN) topology conditioned on a predefined behavior distribution andthe agent’s prior experience (left block). That CNN topology is then trained on a specific task; thetopology description and performance, e.g. validation accuracy, are then stored in the agent’s mem-ory (middle block). Finally, the agent uses its memories to learn about the space of CNN topologiesthrough Q-learning (right block).
Figure 2: Markov Decision Process for CNN Architecture Generation: Figure 2(a) shows thefull state and action space. In this illustration, actions are shown to be deterministic for clarity, butthey are stochastic in experiments. C(n, f, l) denotes a convolutional layer with n filters, receptivefield size f, and stride l. P(f, l) denotes a pooling layer with receptive field size f and stride l. Gdenotes a termination state (Softmax/Global Average Pooling). Figure 2(b) shows a path the agentmay choose, highlighted in green, and the corresponding CNN topology.
Figure 3: Q-Learning Performance. In the plots, the blue line shows a rolling mean of modelaccuracy versus iteration, where in each iteration of the algorithm the agent is sampling a model.
Figure A1: Representation size binning: In this figure, we show three example state transitions.
Figure A2: MNIST Q-Learning Performance. The blue line shows a rolling mean of modelaccuracy versus iteration, where in each iteration of the algorithm the agent is sampling a model.
Figure A3: Figure A3a shows the mean model accuracy and standard deviation at each over 10independent runs of the Q-learning procedure on 10% of the SVHN dataset. Figure A3b shows themean model accuracy at each for each independent experiment. Despite some variance due to arandomized exploration strategy, each independent run successfully improves architecture perfor-mance.
Figure A4: Accuracy Distribution versus : Figures A4a, A4c, and A4e show the accuracy dis-tribution for each for the SVHN, CIFAR-10, and MNIST experiments, respectively. Figures A4b,A4d, and A4f show the accuracy distributions for the initial = 1 and the final = 0.1. One cansee that the accuracy distribution becomes much more peaked in the high accuracy ranges at smallfor each experiment.
Figure A5: Average Q-Value versus Layer Depth for different layer types are shown in the leftcolumn. Average Q-Value versus Layer Depth for different receptive field sizes of the convolutionlayer are shown in the right column.
