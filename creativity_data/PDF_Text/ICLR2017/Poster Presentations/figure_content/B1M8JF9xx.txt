Figure 1: (a) samples from a GAN with 10 latent dimensions, (b) and (c) samples from a GAN with 50 latentdimensions at different epochs of training. While it is difficult to visually discern differences between these threemodels, their log-likelihood (LLD) values span almost 300 nats.
Figure 2: (a) Log-likelihood of GAN-50, under different choices of variance parameter. (b) Log-likelihood ofGMMN-10 on 100 simulated examples evaluated by AIS and KDE vs. the corresponding running time. We showthe BDMC gap converges to almost zero as we increase the running time. (c) Log-likelihood of IWAE on 10,000test examples evaluated by AIS and IWAE bound vs. running time. (a), (b) are results on continuous MNIST,and (c) is on binarized MNIST. Note that AIS/AIS+encoder dominates the other estimate in both estimationracy and r (Nats)	unning tim AIS	e. AIS+encoder	IWAE bound	# dist AIS	# dist AIS+encoder	# samplesIWAE	-85.679	-85.754	-86.902-	1000	100	10000	-85.619	-85.621	-86.464	10000	1000	100000Table 1: AIS vs. IWAE bound on 10,000 test examples of binarized MNIST. “# dist” denotes the numberof intermediate distributions used for evalution. We find AIS estimate is consistently 1 nat higher than IWAEbound; AIS+encoder can achieve about the same estimate as AIS, but with 1 order of magnitude less number ofintermediate distributions.
Figure 3: Training curves for (a) GAN-50, (b) VAE-50, and (c) GMMN-10, as measured by AIS, KDE, and (ifapplicable) the IWAE lower bound. All estimates shown here are lower bounds. In (c), the gap between trainingand validation log-likelihoods is not fairly small (see Table 2).
Figure 4: (a) and (b) show visualization of posterior samples of 10 training/validation examples. (c) shows											visualization of posterior samples of 10 training examples of digit “2".						Each column of 10 digits			comes from		true data and the six models. The order of visualization is: True data, GAN-10, VAE-10, GMMN-10, GAN-50,											VAE-50, GMMN-50.											reconstruct some of the examples on both the training and validation sets, suggesting that they failed											to learn some modes of the distribution.											AcknowledgmentsWe like to thank Yujia Li for providing his original GMMN model and codebase, and thank JimmyBa for advice on training GANs. Ruslan Salakhutdinov is supported in part by Disney and ONR grantN000141310721. We also thank the developers of Lasagne (Battenberg et al., 2014) and Theano(Al-Rfou et al., 2016).
Figure 5: Posterior samples of digit “2" for GAN-10.
Figure 6: Posterior samples of digit “2" for GAN-50.
Figure 7: Posterior samples of digit “2" for VAE-10.
Figure 8: 100222ZAr/d/Λdigit "2"&乙 O 」工ZZa 2 a之	a	a	a	2Z	=	K	2	&又	2	1	a	2月久 N N 2)	2	二二&2	2	、Z 标
