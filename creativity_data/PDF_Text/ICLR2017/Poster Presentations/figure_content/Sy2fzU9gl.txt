Figure 1: Manipulating latent variables on celebA: Qualitative results comparing disentanglingperformance of β-VAE (β = 250), VAE (Kingma & Welling, 2014) (β = 1) and InfoGAN (Chenet al., 2016). In all figures of latent code traversal each block corresponds to the traversal of a singlelatent variable while keeping others fixed to either their inferred (β-VAE, VAE and DC-IGN whereapplicable) or sampled (InfoGAN) values. Each row represents a different seed image used to inferthe latent values in the VAE-based models, or a random sample of the noise variables in InfoGAN.
Figure 2: Manipulating latent variables on 3D chairs: Qualitative results comparing disentanglingperformance of β-VAE (β = 5), VAE (Kingma & Welling, 2014) (β = 1), InfoGAN (Chen et al.,2016) and DC-IGN (Kulkarni et al., 2015). InfoGAN traversal is over the [-1, 1] range. VAE alwayslearns an entangled representation (e.g. chair width is entangled with azimuth and leg style (b)).
Figure 3: Manipulating latent variables on 3D faces: Qualitative results comparing disentanglingperformance of β-VAE (β = 20), VAE (Kingma & Welling, 2014) (β = 1), InfoGAN (Chen et al.,2016) and DC-IGN (Kulkarni et al., 2015). InfoGAN traversal is over the [-1, 1] range. All modelslearnt to disentangle lighting (b) and elevation (c). DC-IGN and VAE struggled to continuouslyinterpolate between different azimuth angles (a), unlike β-VAE, which additionally learnt to encode awider range of azimuth angles than other models. InfoGAN and DC-IGN images adapted from Chenet al. (2016) and Kulkarni et al. (2015), respectively. Reprinted with permission.
Figure 4: Latent factors learnt by β-VAE on celebA: traversal of individual latents demonstratesthat β-VAE discovered in an unsupervised manner factors that encode skin colour, transition from anelderly male to younger female, and image saturation.
Figure 5: Schematic of the proposed disen-tanglement metric: over a batch of L samples,each pair of images has a fixed value for onetarget generative factor y (here y = scale)and differs on all others. A linear classifieris then trained to identify the target factor us-ing the average pairwise difference zdbiff in thelatent space over L samples.
Figure 6: Disentanglement metric classification accuracy for 2D shapes dataset. Left: Accuracy fordifferent models and training regimes Right: Positive correlation is present between the size of z andthe optimal normalised values of β for disentangled factor learning for a fixed β-VAE architecture. βvalues are normalised by latent z size m and input x size n. Note that β values are not uniformlysampled. Orange approximately corresponds to unnormalised β = 1. Good reconstructions are asso-ciated with entangled representations (lower disentanglement scores). Disentangled representations(high disentanglement scores) often result in blurry reconstructions.
Figure 8: Negative correlation between data transform continuity and the degree of disentanglingachieved by β-VAE. Abscissa is the average normalized Hamming distance between each of thetwo consecutive transforms of each object. Ordinate is disentanglement metric score. Disentanglingperformance is robust to Bernoulli noise added to the data at test time, as shown by slowly degradingclassification accuracy up to 10% noise level, considering that the 2D objects occupy on averagebetween 2-7% of the image depending on scale. Fluctuations in classification accuracy for similarHamming distances are due the different nature of subsampled generative factors (i.e. symmetries arepresent in rotation but are lacking in position).
Figure 9: Samples from β-VAE trained on the dataset of 2D shapes that learnt either a disentangled(left, β = 4) or an entangled (right, β = 1) representation of the data generative factors. It can beseen that sampling from an entangled representation results in some unrealistic looking samples. Adisentangled representation that inverts the original data generation process does not suffer from sucherrors.
Figure 10: Latent traversal plots from β-VAE that learnt disentangled representations on the 3Dchairs dataset.
Figure 11: Latent traversal plots from β-VAE that learnt disentangled representations on the 3Dchairs dataset.
Figure 12: Latent traversal plots from β-VAE that learnt disentangled representations ondataset.
Figure 13: Latent traversal plots from β-VAE that learnt disentangled representations on the CelebAdataset.
Figure 14: Latent traversal plots from β-VAE that learnt disentangled representations on the CelebAdataset.
