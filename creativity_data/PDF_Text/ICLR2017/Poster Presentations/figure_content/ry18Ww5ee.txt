Figure 1: (a) The heatmap shows the validation error over a two dimensional search space, withred corresponding to areas with lower validation error, and putative configurations selected in asequential manner as indicated by the numbers. (b) The plot shows the validation error as a functionof the resources allocated to each configuration (i.e., each line in the plot). Configuration evaluationmethods allocate more resources to promising configurations. (c) The validation loss as a function oftotal resources allocated for two configurations. The shaded areas bound the maximum distance fromthe terminal validation loss and monotonically decreases with the resource.
Figure 2: Performance of individ-ual brackets s and HYPERBAND.
Figure 3: Average test error across 10 trials is shown in all plots. Label "SMAC_early" corresponds toSMAC with the early stopping criterion proposed in Domhan et al. (2015) and label “bracket s = 4”corresponds to repeating the most exploratory bracket of Hyperband.
Figure 4: Average test error of the best kernelregularized least square classification modelfound by each searcher on CIFAR-10. Thecolor coded dashed lines indicate when the lasttrial of a given searcher finished.
Figure 5: Average test error of the best ran-dom features model found by each searcheron CIFAR-10. The test error for Hyperbandand bracket s = 4 are calculated in every eval-uation instead of at the end of a bracket.
Figure 6: Average test error across 10 trials is shown in all plots. Error bars indicate the maximumand minimum ranges of the test error corresponding to the model with the best validation errorwith SMAC to speed up hyperparameter optimization. Their method stops training a configurationif the probability of the configuration beating the current best is below a specified threshold. Thisprobability is estimated by extrapolating learning curves fit to the intermediate validation error lossesof a configuration. If a configuration is terminated early, the predicted terminal value from theestimated learning curves is used as the validation error passed to the hyperparameter optimizationalgorithm. Hence, if the learning curve fit is poor, it could impact the performance of the configura-tion selection algorithm. While this approach is heuristic in nature, it does demonstrate promisingempirical performance so we included SMAC with early termination as a competitor. We used theconservative termination criterion with default parameters and recorded the validation loss every400 iterations and evaluated the termination criterion 3 times within the training period (every 8kiterations for CIFAR-10 and MRBI and every 16k iterations for SVHN).7 Comparing performance bythe total multiple of R used is conservative because it does not account for the time spent fitting thelearning curve in order to check the termination criterion.
Figure 7:	Average test error of the best kernel regularized least square classification model foundby each searcher on CIFAR-10. The color coded dashed lines indicate when the last trial of a givensearcher finished. Error bars correspond to observed minimum and maximum test error across 10trials.
Figure 8:	Average test error of the best random features model found by each searcher on CIFAR-10.
