Figure 1: Example learning curves of random hyperparameter configurations of 4 different iterativemachine learning methods: convolutional neural network (CNN), fully connected neural network(FCNet), logistic regression (LR), and variational auto-encoder (VAE). Although different configura-tions lead to different learning curves, they usually share some characteristics for a certain algorithmand dataset (but vary across these).
Figure 2: Example functions generated with our k = 5 basis functions (formulas for which are givenin Appendix B). For each function, we drew 50 different parameters θi uniformly at random in theoutput domain of the hidden layer(s) of our model. This illustrates the type of functions used tomodel the learning curves.
Figure 3: On the left, we plot the noise estimate of 40 different configurations on the FCNetbenchmark sorted by their asymptotic mean performance at t = 1. The color indicates the time step t,darker meaning a larger value. On the right, we show the learning curves of 5 different configurationseach evaluated 10 times (indicated with the same color) with a different seed.
Figure 4: Our neural network architecture tomodel learning curves. A common hidden layeris used to simultaneously model μ∞, the param-eters Θ of the basis functions, their respective一^一、weights w, and the noise σ2 .
Figure 5: Qualitative comparison of the different models. The left panel shows the mean predictionsof different methods on the CNN benchmark. All models observed the validation error of the first 12epochs of the true learning curve (black). On the right, the posterior distributions over the value at 40epochs is plotted.
Figure 6: Assessment of the predictive quality based on partially observed learning curves on all 4benchmarks. The panels on the left show the mean squared error of the predicted asymptotic value(y-axis) after observing all learning curves up to a given fraction of maximum number of epochs(x-axis). The panels on the right show the average log-likelihood based on the predictive mean andvariance of the asymptotic value. Note that LastSeenValue does not provide a predictive variance.
Figure 7: On the horizontal axis, we plot the true value and on the vertical axis the predicted values.
Figure 8: Comparison of Hyperband, Hyperband with our model, and standard Bayesian optimizationon the CNN benchmark. Hyperband finds a good configuration faster than standard Bayesianoptimization, but it only approaches the global optimum quickly when extended with our model.
Figure 9: Assessment of the predictive quality based on partially observed learning curves on all 4benchmarks. The panels on the left show the mean squared error of the predicted asymptotic value(y-axis) after observing all learning curves up to a given fraction of maximum number of epochs(x-axis). The panels on the right show the average log-likelihood based on the predictive mean andvariance of the asymptotic value.
Figure 10: Distributions over runtimes for different random configurations for the CNN, FCNet, LRand VAE benchmark described in Section 3.One can see that all distributions are long tailed and thatespecially on the FCNet benchmark some configuration need order of magnitudes longer than others.
Figure 11: Empirical cumulative distributions for the CNN, FCNet, LR and VAE benchmark.
Figure 12: Different optimizers on a random forests based suggorate of all collected learning curveson different benchmarks. Note how sampling from the model improves Hyperband’s performance byconverging to the optimum faster.
