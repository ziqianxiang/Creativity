Figure 1: (GMAN) The generator trains using feedback aggregated over multiple discriminators. IfF := max, G trains against the best discriminator. If F := mean, G trains against an ensemble.
Figure 2: Consider a dataset consisting of the nine 1-dimensional samples in black. Their corre-sponding probability mass function is given in light gray. After training GMAN, three discrimina-tors converge to distinct local optima which implicitly define distributions over the data (red, blue,yellow). Each discriminator may specialize in discriminating a region of the data space (placingmore diffuse mass in other regions). Averaging over the three discriminators results in the distribu-tion in black, which we expect has higher likelihood under reasonable assumptions on the structureof the true distribution.
Figure 3: Generator objective, F, averagedover 5 training runs on MNIST. Increas-ing the number of discriminators acceleratesconvergence of F to steady state (solid line)and reduces its variance, σ2 (filled shadow±1σ). Figure 4 provides alternative evidenceof GMAN*'s accelerated convergence.
Figure 4: Stdev, σ, of the generator objec-tive over a sliding window of 500 iterations.
Figure 5: Comparison of image quality across epochs for N = {1, 2, 5} using GMAN-0 on MNIST.
Figure 6: GMAN* regulates difficulty of thegame by adjusting λ. Initially, G reduces λ toease learning and then gradually increases λfor a more challenging learning environment.
Figure 7: PairwiseGMAMSfdev(GMAM)for GMAN-λ andGMAN* (λ*) over 5 runs on MNIST.
Figure 8: Image quality improvement across number of generators at same number of iterations forGMAN-0 on CelebA.
Figure 10: Generator objective, F , averagedover 5 training runs on CelebA. IncreasingN (# of D) accelerates convergence of F tosteady state (solid line) and reduces its vari-ance, σ2 (filled shadow ±1σ). Figure 11 pro-vides alternative evidence of GMAN-0’s ac-celerated convergence.
Figure 11: Stdev, σ, of the generator objec-tive over a sliding window of 500 iterations.
Figure 12: Generator objective, F , averagedover 5 training runs on CIFAR-10. Increas-ing N (# of D) accelerates convergence ofF to steady state (solid line) and reduces itsvariance, σ2 (filled shadow ±1σ). Figure 13provides alternative evidence of GMAN-0’saccelerated convergence.
Figure 13: Stdev, σ, of the generator objec-tive over a sliding window of 500 iterations.
Figure 14: Sample of pictures generated on CelebA cropped dataset.
Figure 15: Sample of pictures generated by GMAN-0 on CIFAR dataset.
Figure 16: Sample of pictures generated across 4 independent runs on MNIST With F-boost (similarresults with P-boost).
