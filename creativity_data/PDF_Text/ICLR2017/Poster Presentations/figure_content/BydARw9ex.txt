Figure 1: All neural network architectures can store approximately five bits per parameter about atask, with only small variations across architectures. (a) Stored bits as a function of network size.
Figure 2: Additional RNN capacity analysis. (a) The effect of the ReLU nonlinearity on capacity.
Figure 3: All RNN architectures achieved near identical performance given the same number ofparameters, on a language modeling and random function fitting task. (a-c) text8 Wikipedia numberof parameters vs bits per character for all RNN architectures. From left to right: 1 layer, 2 layer, 4layer models. (d) text8 number of hidden units vs bits per character for 1 layer architectures. Wenote that this is almost always a misleading way to compare architectures as the more heavily gatedarchitectures appear to do better when compared per-unit. (e-g) Same as (a-c), except showing squareerror for different model sizes trained on RCFs.
Figure 4: Some RNN architectures are far easier to train than others. Results of HP searches onextremely difficult tasks. (a) Median evaluation error as a function of HP optimization iterationfor 1 layer architectures on the parentheses task. Dots indicate evaluation loss achieved on that HPiteration. (b-d) Same as (a), but for 2, 4 and 8 layer architectures. (e-h) Minimum evaluation error asa function of HP optimization iteration for parentheses task. Same depth order as (a-d). (i-p) Same as(a-h), except for the arithmetic task. We note that the best loss for the vanilla RNN is still decreasingafter 2400+ HP evaluations.
Figure 5: For randomly generated hyperparameters, GRU and +RNN are the most easily trainablearchitectures. Evaluation losses from 1000 iterations of randomly chosen HP sets for 1 and 8 layer,200k parameter models on the parentheses task. Statistics from a Welch,s t-test for equality of meanson all pairs of architectures are presented in Table App.2. (a) Box and whisker plot of evaluationlosses for the 1 layer model. (b) Same as (a) but for 8 layers.
