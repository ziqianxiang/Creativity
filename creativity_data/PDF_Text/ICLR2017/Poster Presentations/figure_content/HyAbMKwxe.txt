Figure 1: Training classification errors for covertype (top left), alpha (top right), MNist (bottomleft) and IJCNN (bottom right). We can immediately see that all values of T > 1 yield significantlower errors than the standard log-loss (the confidence intervals represent ± 3 standard deviations).
Figure 2: Training (top) and validation (bottom) negative log-likelihood (left) and classificationerror (right) for the covertype dataset. We only display the result for the value of λ yielding thelowest validation error. As soon as the importance weights are recomputed, the NLL increases andthe classification error decreases (the confidence intervals represent ± 3 standard deviations).
Figure 3: Training (top) and validation (bottom) negative log-likelihood (left) and classificationerror (right) for the alpha dataset. We only display the result for the value of λ yielding the lowestvalidation error. As soon as the importance weights are recomputed, the NLL increases. Overfittingoccurs very quickly and the best validation error is the same for all values of T (the confidenceintervals represent ± 3 standard deviations).
Figure 4: Training (top) and validation (bottom) negative log-likelihood (left) and classificationerror (right) for the MNist dataset. We only display the result for the value of λ yielding the lowestvalidation error. As soon as the importance weights are recomputed, the NLL increases. Overfittingoccurs quickly but higher values of T still lead to lower validation error. The best training error was2.52% with T = 10.
Figure 5: Training (top) and validation (bottom) negative log-likelihood (left) and classificationerror (right) for the IJCNN dataset. We only display the result for the value of λ yielding the lowestvalidation error. As soon as the importance weights are recomputed, the NLL increases. Since thenumber of training samples is large compared to the dimension of the input, the standard logisticregression is underfitting and higher values of T lead to better validation errors.
Figure 6: Training data (left) and test ROC curve (right) for the binary classification problemfrom Bach et al. (2006). The black dots are obtained when minimizing the log-loss for variousvalues of the cost asymmetry. The red stars correspond to the ROC curve obtained when directlyoptimizing the probabilities. While the former is not concave, a problem already mentioned by Bachet al. (2006), the latter is.
Figure 7: Test false positive rate as a function ofthe desired false positive rate cFP . The dottedline representing the optimal behaviour, we cansee that the constraint is close to being satisfied.
