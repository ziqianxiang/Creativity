Figure 1: Zoneout as a special case of dropout; Bt is the unit h's hidden activation for the next timestep (if not zoned out). Zoneout can be seen as applying dropout on the hidden state delta, htt - ht-ι.
Figure 2: (a) Zoneout, vs (b) the recurrent dropout strategy of (Semeniuta et al., 2016) in an LSTM.
Figure 3: Validation BPC (bits per character) on Character-level Penn Treebank, for differentprobabilities of zoneout on cells zc and hidden states zh (left), and comparison of an unregularizedLSTM, zoneout zc = 0.5, zh = 0.05, stochastic depth zoneout z = 0.05, recurrent dropout p = 0.25,norm stabilizer β = 50, and weight noise σ = 0.075 (right).
Figure 4: Training and validation bits-per-character (BPC) comparing LSTM regularization methodson character-level Penn Treebank (left) and Text8. (right)」3d s-8,,… Unregularized LSTM (training)— Unregularized LSTM (validation)Recurrent dropout (training)Recurrent dropout (validation)Zoneout (training)Zoneout (validation)5	10	15	20	25	30EpochsUnregulanzed LSTM (training)Unregularized LSTM (validation)Recurrent dropout (training)Recurrent dropout (validation)Zoneout (training)Zoneout (validation)4.2	Text8Enwik8 is a corpus made from the first 109 bytes of Wikipedia dumped on Mar. 3, 2006. Text8 is a"clean text" version of this corpus; with html tags removed, numbers spelled out, symbols converted
Figure 5: Training and validation error rates for an unregularized LSTM, recurrent dropout, andzoneout on the task of permuted sequential MNIST digit classification.
Figure 6: Normalized Ek ∂dL ∣∣ of loss L with respect to cell activations Ct at each timestep t forzoneout (zc = 0.5), dropout (zc = 0.5), and an unregularized LSTM on one epoch of pMNIST5	ConclusionWe have introduced zoneout, a novel and simple regularizer for RNNs, which stochastically preserveshidden units’ activations. Zoneout improves performance across tasks, outperforming many alterna-tive regularizers to achieve results competitive with state of the art on the Penn Treebank and Text8datasets, and state of the art results on pMNIST. While searching over zoneout probabilites allowsus to tune zoneout to each task, low zoneout probabilities (0.05 - 0.2) on states reliably improveperformance of existing models.
Figure 7: Training and validation curves for an LSTM with static identity connections comparedto zoneout (both Zc = 0.5 and Zh = 0.05) and compared to a vanilla LSTM, showing that staticidentity connections fail to capture the benefits of zoneout.
