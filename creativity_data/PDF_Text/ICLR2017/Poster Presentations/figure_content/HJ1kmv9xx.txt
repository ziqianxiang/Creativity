Figure 1: Generation results of our model on CUB-200 (Welinder et al., 2010). It generates imagesin two timesteps. At the first timestep, it generates background images, while generates foregroundimages, masks and transformations at the second timestep. Then, they are composed to obtain thefinal images. From top left to bottom right (row major), the blocks are real images, generatedbackground images, foreground images, foreground masks, carved foreground images, carved andtransformed foreground images, final composite images, and their nearest neighbor real images inthe training set. Note that the model is trained in a completely unsupervised manner.
Figure 2: LR-GAN architecture unfolded to three timesteps. It mainly consists of one backgroundgenerator, one foreground generator, temporal connections and one discriminator. The meaning ofeach component is explained in the legend.
Figure 3: Generated images on CIFAR-10 based on our model.
Figure 4: Generated images on CUB-200 based on our model.
Figure 5: Generation results of our model on MNIST-ONE. From left to right, the image blocks arereal images, generated background images, generated foreground images, generated masks and finalcomposite images, respectively.
Figure 6: Generation results of our model on MNIST-TWO. From top left to bottom right (rowmajor), the image blocks are real images, generated background images, foreground images andmasks at the second timestep, composite images at the second time step, generated foregroundimages and masks at the third timestep and the final composite images, respectively.
Figure 7: Matched pairs of generated images based on DCGAN and LR-GAN, respectivedly. Theodd columns are generated by DCGAN, and the even columns are generated by LR-GAN. Theseare paired according to the perfect matching based on Hungarian algorithm.
Figure 8: Qualitative comparison on CIFAR-10. Top three rows are images generated by DCGAN;Bottom three rows are by LR-GAN. From left to right, the blocks display generated images withincreasing quality level as determined by human studies.
Figure 10: Category specific generation results of our model on CIFAR-10 categories of horse, frog,and cat (top to bottom). The blocks from left to right are: generated background images, foregroundimages, foreground masks, foreground images carved out by masks, carved foregrounds after spatialtransformation and final composite images.
Figure 11: Generation results from an ablated LR-GAN model without affine transformations. Fromtop to bottom, the block rows correspond to different datasets: MNIST-ONE, CUB-200, CIFAR-10.
Figure 12: Generation results from an ablated LR-GAN model without mask generator. The blockrows correspond to different datasets (from top to bottom: MNIST-ONE, CUB-200, CIFAR-10).
Figure 13: Statistics of annotations in human studies on MNIST-ONE. Left: distribution of qualitylevel; Right: distribution of recognized digit categories.
Figure 14: Qualitative comparison on MNIST-ONE. Top three rows are samples generated by DC-GAN. Bottom three rows are samples generated by LR-GAN. The quality level increases from leftto right as determined via human studies.
Figure 15: Generation results of our model on CUB-200 when setting minimal allowed scale to1.1. From left to right, the blocks show the generated background images, foreground images,foreground masks, foreground images carved out by masks, carved foreground images after spatialtransformation. The sixth and seventh blocks are final composite images and the nearest neighborreal images.
Figure 17: Walking in the latent foreground space by fixing backgrounds in our model on CIFAR-10. From left to right, the blocks are: generated background images, foreground images, foregroundmasks, foreground images carved out by masks, carved out foreground images after spatial transfor-mation, and final composite images. Each row has the same background, but different foregrounds.
Figure 18: Statistics of annotations in human studies on CIFAR-10. Left to right: word cloud forreal images, images generated by DCGAN, images generated by LR-GAN.
Figure 19: Generation results of our model on LFW. From left to right, the blocks are: generatedbackground images, foreground images, foreground masks, carved out foreground images after spa-tial transformation, and final composite images.
Figure 20: Histograms of transformation parameters learnt in our model for different datasets. Fromleft to right, the datasets are: MNIST-ONE, CUB-200, CIFAR-10 and LFW. From top to bottom,they are scaling sx , sy, translation tx , ty, and rotation rx , ry in x and y coordinate, respectively.
Figure 21: Conditional generation results of our model on CIFAR-10. From left to right, the blocksare: real images, generated background images, foreground images, foreground masks, foregroundimages carved out by masks, carved foreground images after spatial transformation, and final com-posite (reconstructed) images.
Figure 22: Conditional generation results of our model on LFW, displayed with the same layout toFig. 21.
