Figure 1: (a) Mean margin of victory of each algorithm for optimizing the logistic regression loss.
Figure 2: (a) Mean margin of victory of each algorithm for optimizing the robust linear regressionloss. Higher margin of victory indicates better performance. (b-c) Objective values achieved byeach algorithm on two objective functions from the test set. Lower objective values indicate betterperformance. Best viewed in colour.
Figure 3: (a) Mean margin of victory of each algorithm for training neural net classifiers. Highermargin of victory indicates better performance. (b-c) Objective values achieved by each algorithmon two objective functions from the test set. Lower objective values indicate better performance.
Figure 4: Objective values and trajectories produced by different algorithms on unseen randomtwo-dimensional logistic regression problems. Each pair of plots corresponds to a different logisticregression problem. Objective values are shown on the vertical axis in the left plot and as contourlevels in the right plot, where darker shading represents higher objective values. In the right plot,the axes represent the values of the iterates in each dimension and are of the same scale. Eacharrow represents one iteration of an algorithm, whose tail and tip correspond to the preceding andsubsequent iterates respectively. Best viewed in colour.
Figure 5: Performance of the learned neural net optimizer on classification problems on data drawnfrom mixtures of different numbers (5, 6, 8 and 12) of random Gaussians. The left column shows themean margin of victory of each algorithm, and the middle and right columns show objective valuesachieved by each algorithm on two neural net classification problems from the test set. Best viewedin colour.
