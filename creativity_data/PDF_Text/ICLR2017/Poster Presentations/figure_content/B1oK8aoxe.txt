Figure 1: Different architectures for the integration of the latent variables in a FNNThe simplest joint embedding, as shown in Figure 1(a), is to concatenate the observations and thelatent variables directly.3 However, this limits the expressiveness power of integration between theobservation and latent variable. Richer forms of integrations, such as multiplicative integrations andbilinear pooling, have been shown to have greater representation power and improve the optimizationlandscape, achieving better results when complex interactions are needed (Fukui et al., 2016; Wuet al., 2016). Inspired by this work, we study using a simple bilinear integration, by forming theouter product between the observation and the latent variable (Fig. 1(b)). Note that the concatenation3In scenarios where the observation is very high-dimensional such as images, we can form a low-dimensional embedding of the observation alone, say using a neural network, before jointly embedding itwith the latent variable.
Figure 2: Hierarchical SNN archi-tecture to solve downstream taskswe show in our experiments that frozen low-level policies are already sufficient to achieve goodperformance in the studied downstream tasks, so these directions are left as future research.
Figure 3: Illustration of the sparse reward tasks7	ResultsWe evaluate every step of the skill learning process, showing the relevance of the different piecesof our architecture and how they impact the exploration achieved when using them in a hierarchicalfashion. Then we report the results5 on the sparse environments described above. We seek to answerthe following questions:•	Can the multimodality of SNNs and the MI bonus consistently yield a large span of skills?•	Can the pre-training experience improve the exploration in downstream environments?•	Does the enhanced exploration help to efficiently solve sparse complex tasks?7.1	Skill learning in pretrainTo evaluate the diversity of the learned skills we use “visitation plots”, showing the (x, y) positionof the robot’s Center of Mass (CoM) during 100 rollouts of 500 time-steps each. At the beginningof every rollout, we reset the robot to the origin, always in the same orientation (as done duringtraining). In Fig. 4(a) we show the visitation plot of six different feed-forward policies, each trainedfrom scratch in our pre-training environment. For better graphical interpretation and comparisonwith the next plots of the SNN policies, Fig. 4(b) superposes a batch of 50 rollout for each of the6 policies, each with a different color. Given the morphology of the swimmer, it has a naturalpreference for forward and backward motion. Therefore, when no extra incentive is added, thevisitation concentrates heavily on the direction it is always initialized with. Note nevertheless thatthe proxy reward is general enough so that each independently trained policy yields a different way
Figure 4: Span of skills learn by different methods and architecturesfeed-forward policy from Fig. 4(a), therefore this method of learning skills effectively reduces thesample complexity by a factor equal to the numbers of skills being learned. With an adequate archi-tecture and MI bonus, we show that the span of skills generated is also richer. In the two rows ofFigs. 4(c)-4(d) we present the visitation plots of SNNs policies obtained for different design choices.
Figure 5: Visitation plots for different randomly initialized architectures (one rollout of 1M steps).
Figure 6: Faster learning of the hierarchical architectures in the sparse downstream MDPs8 Discussion and future workWe propose a framework for first learning a diverse set of skills using Stochastic Neural Networkstrained with minimum supervision, and then utilizing these skills in a hierarchical architecture tosolve challenging tasks with sparse rewards. Our framework successfully combines two parts, firstlyan unsupervised procedure to learn a large span of skills using proxy rewards and secondly a hierar-chical structure that encapsulates the latter span of skills and allows to re-use them in future tasks.
Figure 7: Results for Gather environment in the benchmark settingsC SnakeIn this section we study how our method scales to a more complex robot. Werepeat most experiments from Section 7 with the ”Snake” agent, a 5-link robotdepicted in Fig. 8. Compared to Swimmer, the action dimension doubles andthe state-space increases by 50%. We also perform a further analysis on the rel-evance of the switch time T and end up by reporting the performance variationamong different pretrained SNNs on the hierarchical tasks.
Figure 8: SnakeC.1 S kill learning in pretrainThe Snake robot can learn a large span of skills as consistently as Swimmer. We report in Figs. 9(a)-9(e) the visitation plots obtained for five different random seeds in our SNN pretraining algorithmfrom Secs. 5.1-5.3, with Mutual Information bonus of 0.05. The impact of the different spans ofskills on the later hierarchical training is discussed in Sec. C.4.
Figure 9: Span of skills learned using different random seeds to pretraining a SNN with MI = 0.05C.2 Hierarchical use of skills in Maze and GatherHere we evaluate the performance of our hierarchical architecture to solve with Snake the sparsetasks Maze 0 and Gather from Sec. 6. Given the larger size of the robot, we have also increasedthe size of the Maze and the Gather task, from 2 to 7 and from 6 to 10 respectively. The maximumpath length is also increased in the same proportion, but not the batch size nor the switch time T(see Sec. C.3 for an analysis on T). Despite the tasks being harder, our approach still achieves goodresults, and Figs. 10(a)-10(b), clearly shows how it outperforms the baseline of having the intrinsicmotivation of the Center of Mass in the hierarchical task.
Figure 10: Faster learning of our hierarchical architecture in sparse downstream MDPs with SnakeC.3 ANALYSIS OF THE SWITCH TIME TThe switch time T does not critically affect the performance for these static tasks, where the robotis not required to react fast and precisely to changes in the environment. The performance of verydifferent T is reported for the Gather task of sizes 10 and 15 in Figs. 11(a)-11(b). A smaller en-vironment implies having red/green balls closer to each other, and hence more precise navigationis expected to achieve higher scores. In our framework, lower switch time T allows faster changesbetween skills, therefore explaining the slightly better performance of T = 10 or 50 over 100 for theGather task of size 10 (Fig. 11(a)). On the other hand, larger environments mean more sparcity asthe same number of balls is uniformly initialized in a wider space. In such case, longer commitmentto every skill is expected to improve the exploration range. This explains the slower learning ofT = 10 in the Gather task of size 15 (Fig. 11(b)). It is still surprising the mild changes produced bysuch large variations in the switch time T for the Gather task.
Figure 11: Mild effect of switch time T on different sizes of Gather and Maze 0C.4 Impact of the pre-trained SNN on the hierarchyIn the previous section, the low variance of the learning curves for Gather indicates that all pre-trained SNN preform equally good in this task. For Maze, the performance depends strongly onthe pretrained SNN. For example we observe that the one obtained with the random seed 50 has avisitation with a weak backward span (Fig. 9(e)), which happens to be critical to solve the Maze 0(as can be seen in the videos2). This explains the lack of learning in Fig. 12(a) for this particularpretrained SNN. Note that the large variability between different random seeds is even worse in thebaseline of having the CoM reward in the Maze, as seen in Fig. 12(b). 3 out of 5 seeds never reachthe goal and the ones that does have an unstable learning due to the long time-horizon.
Figure 12: Variation in performance among different pretrained SNNs and different TRPO seedsD	AntIn this section we report the progress and failure modes of our approach withmore challenging and unstable robots, like Ant (described in Duan et al. (2016)).
Figure 13: Ant16Published as a conference paper at ICLR 2017D.1 Sill learning in pretrainOur SNN pretraining step is still able to yield large span of skills covering most directions, as longas the MI bonus is well tuned, as observed in Fig. 14. Videos comparing all the different gaits canbe observed in the attached videos6.
Figure 14: Pretrain visitation for Ant with different MI bonusD.2 Failure modes for unstable robotsWhen switching to a new skill, the robot finds itself in a region of the state-space that mightbe unknown to the new skill. This increases the instability of the robot, and for example inthe case of Ant this might leads to falling over and terminating the rollout. We see this inFig. 15, where the 5 depicted rollouts terminate because of the ant falling over. For more de-tails on failure cases with Ant and the possible solutions, please refer to the technical report:http://bit.ly/snn4hrl-antReportRepetitionslatent: 5latent: 4Figure 15: Failure mode of Ant: here 5 rollouts terminate in less than 6 skill switches.
Figure 15: Failure mode of Ant: here 5 rollouts terminate in less than 6 skill switches.
