Figure 1: Pruning a filter results in removal of its corresponding feature map and related kernels inthe next layer.
Figure 2: (a) Sorting filters by absolute weights sum for each layer of VGG-16 on CIFAR-10. Thex-axis is the filter index divided by the total number of filters. The y-axis is the filter weight sumdivided by the max sum value among filters in that layer. (b) Pruning filters with the lowest absoluteweights sum and their corresponding test accuracies on CIFAR-10. (c) Prune and retrain for eachsingle layer of VGG-16 on CIFAR-10. Some layers are sensitive and it can be harder to recoveraccuracy after pruning them.
Figure 3: Pruning filters across consecutive layers. The independent pruning strategy calculatesthe filter sum (columns marked in green) without considering feature maps removed in previouslayer (shown in blue), so the kernel weights marked in yellow are still included. The greedy pruningstrategy does not count kernels for the already pruned feature maps. Both approaches result in a(ni+1 - 1) Ã— (ni+2 - 1) kernel matrix.
Figure 4: Pruning residual blocks with the projection shortcut. The filters to be pruned for the secondlayer of the residual block (marked as green) are determined by the pruning result of the shortcutprojection. The first layer of the residual block can be pruned without restrictions.
Figure 5: Visualization of filters in the first convolutional layer of VGG-16 trained on CIFAR-10.
Figure 6: Sensitivity to pruning for the first layer of each residual block of ResNet-56/110.
Figure 7: Sensitivity to pruning for the residual blocks of ResNet-34.
Figure 8: Comparison of three pruning methods for VGG-16 on CIFAR-10: pruning the smallestfilters, pruning random filters and pruning the largest filters. In random filter pruning, the order offilters to be pruned is randomly permuted.
Figure 9: Comparison of activation-based feature map pruning for VGG-16 on CIFAR-10.
Figure 10: Comparison of '1 -norm and '2-norm based filter pruning for VGG-16 on CIFAR-10.
