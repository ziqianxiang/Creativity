Figure 1: Proposed recurrent mixture density network for saliency prediction. The input clip ofK frames is fed into a 3D convolutional network (in blue), whose output becomes the input of along short-term memory (LSTM) network (in green). Finally, a linear layer projects the LSTMrepresentation to the parameters of a Gaussian mixture model, which describes the saliency map.
Figure 2: Model for action recognition. The original clip of K frames is fed into a 3D convolutionalnetwork. The same clip is then weighted by the predicted saliency map estimated by our RMDN andthen fed into the 3D convolutional network. The final clip-level representation is then concatenated.
