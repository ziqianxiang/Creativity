Figure 1: A Mixture of Experts (MoE) layer embedded within a recurrent language model. In thiscase, the sparse gating function selects two experts to perform computations. Their outputs aremodulated by the outputs of the gating network.
Figure 2: Model comparison on I-Billion-Word Language-Modeling Benchmark. On the left, Weplot test perplexity as a function of model capacity for models with similar computational budgetsof approximately 8-million-ops-per-timestep. On the right, We plot test perplexity as a function ofcomputational budget. The top line represents the LSTM models from (JozefoWicz et al., 2016).
Figure 3: Language modeling on a 100 billion word corpus. Models have similar computationalbudgets (8 million ops/timestep).
Figure 4: Perplexity on WMT'14 En→ Fr (left) and Google Production En→ Fr (right) datasets asa function of number of words processed. The large differences between models at the beginningof training are due to different batch sizes. All models incur the same computational budget (85Mops/timestep) except the one with no experts.
