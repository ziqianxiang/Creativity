Figure 1: Left: Schematic depiction of multi-layer Graph Convolutional Network (GCN) for semi-supervised learning with C input channels and F feature maps in the output layer. The graph struc-ture (edges shown as black lines) is shared over layers, labels are denoted by Yi . Right: t-SNE(Maaten & Hinton, 2008) visualization of hidden layer activations of a two-layer GCN trained onthe Cora dataset (Sen et al., 2008) using 5% of labels. Colors denote document class.
Figure 2: Wall-clock time per epoch for randomgraphs. (*) indicates out-of-memory error.
Figure 3: Left: Zacharyâ€™s karate club network (Zachary, 1977), colors denote communities obtainedvia modularity-based clustering (Brandes et al., 2008). Right: Embeddings obtained from an un-trained 3-layer GCN model (Eq. 13) with random weights applied to the karate club network. Bestviewed on a computer screen.
Figure 4: Evolution of karate club network node embeddings obtained from a GCN model after anumber of semi-supervised training iterations. Colors denote class. Nodes of which labels wereprovided during training (one per class) are highlighted (grey outline). Grey links between nodesdenote graph edges. Best viewed on a computer screen.
Figure 5: Influence of model depth (number of layers) on classification performance. Markersdenote mean classification accuracy (training vs. testing) for 5-fold cross-validation. Shaded areasdenote standard error. We show results both for a standard GCN model (dashed lines) and a modelwith added residual connections (He et al., 2016) between hidden layers (solid lines).
