Figure 1: From left to right, the three domains we consider in this paper: pointmass, reacher, andpendulum. Top-row is the third-person view of a teacher demonstration. Bottom row is the agent’sview in their version of the environment. For the point and reacher environments, the camera anglesdiffer by approximately 40 degrees. For the pendulum environment, the color of the pole differs.
Figure 2: Architecture diagram for third-person imitation learning. Images at time t and t + 4 aresent through a feature extractor to obtain F (ot) and F (ot+4). Subsequently, these feature vectorsare reused in two places. First, they are concatenated and used to predict whether the samples aredrawn from expert or non-expert trajectories. Second, F (ot) is utilized to predict a domain label(expert vs. novice domain). During backpropogation, the sign on the domain loss LD is flippedto destroy information that was useful for distinguishing the two domains. This ensures that thefeature extractor F is domain agnostic. Finally, the classes probabilities that were computed usingthis domain-agnostic feature vector are utilized as a cost signal in TRPO; which is subsequentlyutilized to train the novice policy to take expert-like actions and collect further rollouts.
Figure 3: Reward vs training iteration for reacher, inverted pendulum, and point environments. Thelearning curves are averaged over 5 trials with error bars represent one standard deviation in thereward distribution at the given point.
Figure 4: Domain accuracy vs. training iteration for reacher, inverted pendulum, and point environ-ments.
Figure 5: Reward vs iteration for reacher, inverted pendulum, and point environments with no do-main confusion and no velocity (red), domain confusion (orange), velocity (brown), and both do-main confusion and velocity (blue).
Figure 6: Reward of final trained policy vs domain confusion weight λ for reacher, inverted pendu-lum, and point environments.
Figure 7: Reward of final trained policy vs number of look-ahead frames for reacher, inverted pen-dulum, and point environments.
Figure 8: Point and reacher final reward after 20 epochs of third-person imitation learning vs thecamera angle difference between the first and third-person perspective. We see that the point followsa fairly linear slope in regards to camera angle differences, whereas the reacher environment is morestochastic against these changes.
Figure 9: Learning curves for third-person imitation vs. three baselines: 1)RL with true reward, 2)first-person imitation, 3) attempting to use first-person features on the third-person agent.
Figure 10: Inverted Pendulum performance under a policy trained on RL, first-person imitationlearning, third-person imitation, and a first-person policy applied to a third-person agent.
Figure 11: Reacher performance under a policy trained on RL, first-person imitation learning, third-person imitation, and a first-person policy applied to a third-person agent.
Figure 12: Point performance under a policy trained on RL, first-person imitation learning, third-person imitation, and a first-person policy applied to a third-person agent.
