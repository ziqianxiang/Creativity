Figure 1: Eigenspectrum of the Hessian at a local minimum of a CNN on MNIST (two independent runs).
Figure 2: Local entropy concentrates onwide valleys in the energy landscape.
Figure 3: Universality of the Hessian: for a wide variety of network architectures, sizes and datasets, optimaobtained by SGD are mostly flat (large peak near zero), they always have a few directions with large positivecurvature (long positive tails). A very small fraction of directions have negative curvature, and the magnitudeof this curvature is extremely small (short negative tails).
Figure 4: Comparison of Entropy-SGD vs. Adam on MNIST10Published as a conference paper at ICLR 20175.3	CIFAR-10We train on CIFAR-10 without data augmentation after performing global contrast normalizationand ZCA whitening (Goodfellow et al., 2013). We consider the All-CNN-C network of Sprin-genberg et al. (2014) with the only difference that a batch normalization layer is added after eachconvolutional layer; all other architecture and hyper-parameters are kept the same. We train for 200epochs with SGD and Nesterov,s momentum during which the initial learning rate of 0.1 decreasesby a factor of 5 after every 60 epochs. We obtain an average error of 7.71 ± 0.19% in 200 epochsvs. 9.08% error in 350 epochs that the authors in Springenberg et al. (2014) report and this is thusa very competitive baseline for this network. Let us note that the best result in the literature onnon-augmented CIFAR-10 is the ELU-network by Clevert et al. (2015) with 6.55% test error.
Figure 5: Comparison of Entropy-SGD vs. SGD on CIFAR-105.4	Recurrent neural networks5.4.1	Penn Tree BankWe train an LSTM network on the Penn Tree Bank (PTB) dataset for word-level text prediction.
Figure 6: comparison of Entropy-SGD vs. SGD / Adam on RNNsModel	Entropy-SGD		SGD / Adam		Error (%) / Perplexity	Epochs	Error (%) / Perplexity	Epochsmnistfc	1.37 ± 0.03	120	1.39 ± 0.03	66LeNet	0.5 ± 0.01	80	0.51 ± 0.01	100All-CNN-BN	7.81 ± 0.09	160	7.71 ± 0.19	180PTB-LSTM	77.656 ± 0.171	25	78.6 ± 0.26	55char-LSTM	1.217 ± 0.005	25	1.226 ± 0.01	40Table 1: Experimental results: Entropy-SGD vs. SGD / Adam12Published as a conference paper at ICLR 2017Tuning the momentum in Entropy-SGD was crucial to getting good results on RNNs. While the SGDbaseline on PTB-LSTM does not use momentum (and in fact, does not train well with momentum)we used a momentum of 0.5 for Entropy-SGD. On the other hand, the baseline for char-LSTM wastrained with Adam with β1 = 0.9 (β1 in Adam controls the moving average of the gradient) whilewe set β1 = 0.5 for Entropy-Adam. In contrast to this observation about RNNs, all our experimentson CNNs used a momentum of 0.9. In order to investigate this difficulty, we monitored the anglebetween the local entropy gradient and the vanilla SGD gradient during training. This angle changesmuch more rapidly for RNNs than for CNNs which suggests a more rugged energy landscape forthe former; local entropy gradient is highly uncorrelated with the SGD gradient in this case.
