Figure 1: (a) A2T architecture. The doted arrows represent the path of back propagation. (b) Actor-Critic using A2T.
Figure 2: Different worlds for policy transfer experimentsLQB (θb) = Es,a,r,s0 [(yQT - Qb(s,a; θb))2]	(12)VθaLQT = E[(yQT - Qτ(s, a))VθaQτ(s, a)]	(13)VθbLQB = E[(yQT - QB(s,a))VθbQR(s,a)]	(14)θa and θb are updated with the above gradients using RMSProp. Note that the Q-learning updates forboth the attention network (Eq.(11)) and the base network (Eq.(12)) use the target value generatedby QT . We use target networks for both QB and QT to stabilize the updates and reduce the non-stationarity as in DQN training. The parameters of the target networks are periodically updated tothat of the online networks.
Figure 3: Results of the selective policy transfer experimentsThis is illustrated for the Policy Transfer setting using the chain world shown in (Fig. 2a). Considerthat the target task LT is to start in A or B with uniform probability and reach C in the least numberof steps. Now, consider that two learned source tasks, viz., L1 and L2, are available. L1 is thesource task where the agent has learned to reach the left end (A) starting from the right end (B). Incontrast, L2 is the source task where the agent has learned to reach the right end (B) starting fromthe left end (A). Intuitively, it is clear that the target task should benefit from the policies learnt fortasks L1 and L2. We learn to solve the task LT using REINFORCE given the policies learned forL1 and L2. Figure 3a (i) shows the weights given by the attention network to the two source taskpolicies for different parts of the state space at the end of learning. We observe that the attentionnetwork has learned to ignore L1, and L2 for the left, and right half of the state space of the targettask, respectively. Next, we add base network and evaluate the full architecture on this task. Figure3a (ii) shows the weights given by the attention network to the different source policies for differentparts of the state space at the end of learning. We observe that the attention network has learned toignore L1, and L2 for the left, and right half of the state space of the target task, respectively. As thebase network replicates πT over time, it has a high weight throughout the state space of the targettask.
Figure 4: Visualisation of the attention weights in the Selective Transfer with Attention Networkexperiment: Green and Blue bars signify the attention probabilities for Expert-1 (L1) and Expert-2 (L2) respectively. We see that in the first two snapshots, the ball is in the lower quadrant andas expected, the attention is high on Expert-1, while in the third and fourth snapshots, as the ballbounces back into the upper quadrant, the attention increases on Expert-2.
Figure 5: Selective Value Transfer.
Figure 7: Avoiding negative transfer and transferring value from a favorable task(higher the better).
Figure 6: Avoiding negative transfer and trans-ferring policy from a favorable task(lower thebetter).
Figure 8: Evolution of attention weights withone positive and one negative expert.
Figure 9: Partial Positive Expert Experimentdividual average rewards is upper bounded by theperformance of the best available positive expert, which happens to be an imperfect expert on the tar-get task. Rather, the base network has to acquire new skills not present in the source task networks.
Figure 10: The figures above explain the blurring mechanism for selective transfer experiments onPong. The background of the screen is made black. Let X (84 × 84) denote an array containingthe pixels of the screen. The paddle controlled by the agent is the one on the right. We focus onthe two quadrants X1 = X[: 42, 42 :] and X2 = X[42 :, 42 :] of the Pong screen relevant to theagent controlled paddle. To simulate an expert that is weak at returning balls in the upper quadrant,the portion of X1 till the horizontal location of agent-paddle, ie X1[:, : 31] is blacked out, whilesimilarly, for simulating weakness in the bottom quadrant, we blur the portion of X 2 till the agent-paddle’s horizontal location, ie X2[:, : 31] = 0. Figures 10a and 10b illustrate the scenarios ofblurring the upper quadrant before and after blurring; and similarly do 10c and 10d for blurring thelower quadrant. Effectively, blurring this way with a black screen is equivalent to hiding the ball(white pixel) in the appropriate quadrant where weakness is to be simulated. Hence, Figures 10band 10d are the mechanisms used while training a DQN on Pong to hide the ball at the respectivequadrants, so to create the partially useful experts which are analogous to forehand-backhand expertsin Tennis. X [: a, : b] indicates the subarray of X with all rows upto row index a and all columnsupto column index b.
Figure 11: The figures above explain the blurring mechanism used for selective transfer experimentson Breakout. The background of the screen is already black. Let X (84 × 84) denote an arraycontaining the pixels of the screen. We focus on the two quadrants X1 = X[31 : 81, 4 : 42] andX2 = X [31 : 81, 42 : 80]. We perform blurring in each case by ensuring X1 = 0 and X2 = 0 forall pixels within them for training L1 and L2 respectively. Effectively, this is equivalent to hidingthe ball in the appropriate quadrants. Blurring X 1 simulates weakness in the lower left quadrant,while blurring X2 simulates weakness in the lower right quadrant. We don’t blur all the way downupto the last row to ensure the paddle controlled by the agent is visible on the screen. We also don’tblack the rectangular border with a width of 4 pixels surrounding the screen. Figures 11a and 11billustrate the scenarios of blurring the lower left quadrant before and after blurring; and similarly do11c and 11d for blurring the lower right quadrant.
Figure 12: Visualisation of the attention weights in the Selective Transfer with Attention for Break-out: Green and Blue bars signify the attention probabilities for Expert-1 (L1) and Expert-2 (L2)respectively on a scale of [0, 1]. We see that in the first two snapshots, the ball is in the lower rightquadrant and as expected, the attention is high on Expert-1, while in the third and fourth snapshots,the ball is in the lower right quadrant and hence the attention is high on Expert-2.
Figure 13: This experiment is a case study on a target task where the performance is limited by dataavailability. So far, we focused on experiments where the target task is to solve Pong (normal orblack background) for Value Transfer, and Puddle Worlds for Policy Transfer. In both these cases, arandomly initialized value (or policy) network learning without the aid of any expert network is ableto solve the target task within a reasonable number of epochs (or iterations). We want to illustrate acase where solving the target task in reasonable time is hard and the presence of a favorable sourcetask significantly impacts the speed of learning. To do so, we consider a variant of Pong as our targettask. In this variant, only a small probability ρ of transition tuples (s, a, r, s0) with non-zero reward rare added to the Replay Memory (and used for learning through random batch sampling). This way,the performance on the target task is limited by the availability of rewarding (positive or negative)transitions in the replay memory. This synthetically makes the target task of Pong a sparse rewardproblem because the replay memory is largely filled with transition tuples that have zero reward. Wedo not use any prioritized sampling so as to make sure the sparsity has a negative effect on learningto solve the target task. We use a version of Pong with black background (as used in Section 4.1for the Blurring experiments) for faster experimentation. ρ = 0.1 was used for the plots illustratedabove. Figure 13a clearly shows the difference between a normal Pong task without any syntheticsparsity and the new variant we introduce. The learning is much slower and is clearly limited by dataavailability even after 20 epochs (20 million frames) due to reward sparsity. Figure 13b describesa comparison between the A2T setting with one positive expert which expertly solves the targettask and one negative expert, learning from scratch, and direct fine-tuning on a negative expert. We
