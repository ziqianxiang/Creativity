Figure 1: Performance of hopper policies when testing on target domains with different torso masses.
Figure 2: On the left, is an illustration of the simulated 2D hopper task studied in this paper. Onright, we depict the performance of policies for various model instances of the hopper task. Theperformance is depicted as a heat map for various model configurations, parameters of which aregiven in the x and y axis. The adversarially trained policy, EPOpt( = 0.1), is observed to generalizeto a wider range of models and is more robust.
Figure 3: Comparison between policies trainedon a fixed maximum-likelihood model with mass(6), and an ensemble where all models have thesame mass (6) and other parameters varying asdescribed in Table 1.
Figure 4: (a) Visualizes the source distribution during model adaptation on the hopper task, wheremass and friction coefficient are varied in the source domain. The red cross indicates the unknownparameters of the target domain. The contours in the plot indicate the distribution over models(we assume a Gaussian distribution). Lighter colors and more concentrated contour lines indicateregions of higher density. Each iteration corresponds to one round (episode) of interaction with thetarget domain. The high-density regions gradually move toward the true model, while maintainingprobability mass over a range of parameters which can explain the behavior of target domain.
Figure 5: Illustrations of the 2D simulated robot models used in the experiments. The hopper (a) andhalf-cheetah (b) tasks present the challenges of under-actuation and contact discontinuities. Thesechallenges when coupled with parameter uncertainties lead to dramatic degradation in the quality ofpolicies when robustness is not explicitly considered.
Figure 6: 10th percentile of return distribution for the hopper task. EPOpt( = 0.1) clearly outper-forms the other approaches. The 10th of return distribution for EPOpt( = 0.1) also nearly overlapswith the expected return, indicating that the policies trained using EPOpt( = 0.1) are highly robustand reliable.
Figure 7: Performance of policies for various model instances for the half-cheetah domain, similar toFigure 2. Again, it is observed that the adversarial trained policy is robust and generalizes well to allmodels in the source distribution.
Figure 8: (a) depicts the learning curve for EPOpt( = 1) with and without baselines. The learningcurves indicate that use of a baseline provides a better ascent direction, thereby enabling fasterlearning. Figure 8(b) depicts the learning curve when using the average return and CVaR objectives.
Figure 9: Learning curves for EPOpt( = 1) when using the TRPO and REINFORCE methods forthe BatchPolOpt step.
