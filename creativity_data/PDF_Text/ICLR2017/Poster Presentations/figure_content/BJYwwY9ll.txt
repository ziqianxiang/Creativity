Figure 1: Left: Illustration of SGD optimization With a typical learning rate schedule. The model convergesto a minimum at the end of training. Right: Illustration of Snapshot Ensembling. The model undergoes severallearning rate annealing cycles, converging to and escaping from multiple local minima. We take a snapshot ateach minimum for test-time ensembling.
Figure 3: DenseNet-100 Snapshot Ensemble performance on CIFAR-10 and CIFAR-100 With restart learningrate αo = 0.1 (left two) and αo = 0.2 (right two). Each ensemble is trained with M = 6 annealing cycles (50epochs per each).
Figure 4: Snapshot Ensembles under different training budgets on (Left) CIFAR-10 and (Middle) CIFAR-100.
Figure 5: Interpolations in parameter space between the final model (sixth snapshot) and all intermediatesnapshots. λ = 0 represents an intermediate snapshot model, while λ = 1 represents the final model. Left: ASnapshot Ensemble, with cosine annealing cycles (α0 = 0.2 every B/M = 50 epochs). Right: A NoCycleSnapshot Ensemble, (two learning rate drops, snapshots every 50 epochs).
Figure 6: Pairwise correlation of softmax outputs be-tween any two snapshots for DenseNet-100. Left:A Snapshot Ensemble, with cosine annealing cycles(restart with α0 = 0.2 every 50 epochs). Right: ANoCycle Snapshot Ensemble, (two learning rate drops,snapshots every 50 epochs).
Figure 7: Single model and Snapshot Ensemble performance over time (part 1).
Figure 8: Single model and Snapshot Ensemble performance over time (part 2).
Figure 9: Single model and Snapshot Ensemble performance over time (part 3).
