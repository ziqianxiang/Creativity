Figure 1: Results of VIB model on MNIST. (a) Error rate vs β for K = 256 on train and test set.
Figure 2: Visualizing embeddings of 1000 test images in two dimensions. We plot the 95% confi-dence interval of the Gaussian embedding p(z|x) = N(μ, Σ) as an ellipse. The images are coloredaccording to their true class label. The background greyscale image denotes the entropy of the vari-ational classifier evaluated at each two dimensional location. As β becomes larger, we forget moreabout the input and the embeddings start to overlap to such a degree that the classes become indis-tinguishable. We also report the test error using a single sample, err1, and using 12 Monte Carlosamples, errmc. For “good” values of β, a single sample suffices.
Figure 3:	The adversary is trying to force each 0 to be classified as a 1. Successful attacks have a redbackground. Unsuccessful attacks have a green background. In the case that the label is changedto an incorrect label different from the target label (i.e., the classifier outputs something other than0 or 1), the background is purple. The first column is the original image. The second column isadversarial examples targeting our deterministic baseline model. The third column is adversarialexamples targeting our dropout model. The remaining columns are adversarial examples targetingour VIB models for different β .
Figure 4:	(a) Relative magnitude of the adversarial perturbation, measured using L0, L2 , and L∞norms, for the images in Figure 3 as a function of β. (We normalize all values by the correspondingnorm of the perturbation against the base model.) As β increases, L0 decreases, but both L2 and L∞increase, indicating that the adversary is being forced to put larger modifications into fewer pixelswhile searching for an adversarial perturbation. (b) Same as (a), but with the dropout model as thebaseline. Dropout is more robust to the adversarial perturbations than the base deterministic model,but still performs much worse than the VIB model as β increases.
Figure 5:	Classification accuracy of VIB classifiers, divided by accuracy of baseline classifiers, onFGS-generated adversarial examples as a function of β. Higher is better, and the baseline is alwaysat 1.0. For the FGS adversarial examples, when β = 0 (not shown), the VIB model’s performance isalmost identical to when β = 10-8. (a) FGS accuracy normalized by the base deterministic modelperformance. The base deterministic model’s accuracy on the adversarial examples ranges fromabout 1% when = 0.5 to about 5% when = 0.35. (b) Same as (a), but with the dropout modelas the baseline. The dropout model is more robust than the base model, but less robust than VIB,particularly for stronger adversaries (i.e., larger values of ). The dropout model’s accuracy on theadversarial examples ranges from about 5% when = 0.5 to about 16% when = 0.35. As inthe other results, relative performance is more dramatic as β increases, which seems to indicate thatthe VIB models are learning to ignore more of the perturbations caused by the FGS method, eventhough they were not trained on any adversarial examples.
Figure 6: Classification accuracy (from 0 to 1) on L2 adversarial examples (of all classes) as afunction of β. The blue line is for targeted attacks, and the green line is for untargeted attacks(which are easier to resist). In this case, β = 10-11 has performance indistinguishable from β = 0.
Figure 7: The results of our ImageNet targeted L2 optimization attack. In all cases we target anew label of 222 (“soccer ball”). Figure (a) shows the 30 images from the first 40 images in theImageNet validation set that the VIB network classifies correctly. The class label is shown in greenon each image. The predicted label and targeted label are shown in red. Figure (b) shows adversarialexamples of the same images generated by attacking our VIB network with β = 0.01. While allof the attacks change the classification of the image, in 13 out of 30 examples the attack fails tohit the intended target class (“soccer ball”). Pink crosses denote cases where the attack failed toforce the model to misclassify the image as a soccer ball. Figure (c) shows the same result butfor our deterministic baseline operating on the whitened precomputed features. The attack alwayssuccceeds. Figure (d) is the same but for the original full Inception ResNet V2 network withoutmodification. The attack always succceeds. There are slight variations in the set of adversarialexamples shown for each network because we limited the adversarial search to correctly classifiedimages. In the case of the deterministic baseline and original Inception ResNet V2 network, theperturbations are hardly noticable in the perturbed images, but in many instances, the perturbationsfor the VIB network can be percieved.
Figure 8: Shown are the absolute differences between the original and final perturbed images forall three networks. The left block shows the perturbations created while targeting the VIB network.
