Figure 1: Architecture of the nonparametric networks used in the experiments. Activations flowrightward, gradients flow leftward. In color, we show how each element corresponds to our defini-tion of a neural network in section 2. CapNorm does not fully fit our definition of nonlinearity as itrequires information from multiple datapoints to compute its value. Hence, theorem 1 and propo-sition 1 do not technically apply. However, CapNorm is a benign operation that does not lead toproblems in practice.
Figure 2: Test classification error of trained networks. Nonparametric networks are shown in black,parametric networks in red and blue. Error bars indicate the range over 10 random reruns of the samesetting. For parametric networks, the square represents the median test error over those 10 runs. Fornonparametric networks, the square represents the test error and size of a single representative runthat was close to the median in both size and error. In brackets below or above each plotted point,we show the number of units in the two hidden layers.
Figure 3: Detailed statistics of a nonparametric training run. See main text for details.
Figure 4: Length of time individual units in the second hidden layer were present during training.
