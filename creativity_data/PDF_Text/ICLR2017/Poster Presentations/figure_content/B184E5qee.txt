Figure 1: The neural cache storesthe previous hidden states in memorycells. They are then used as keys to re-trieve their corresponding word, thatis the next word. There is no transfor-mation applied to the storage duringwriting and reading.
Figure 2: Perplexity on the validation set of Penn Tree Bank for linear interpolation (left) andglobal normalization (right), for various values of hyperparameters θ, λ and α. We use a cachemodel of size 500. The base model has a validation perplexity of 86.9. The best linear interpolationhas a perplexity of 74.6, while the best global normalization has a perplexity of 74.9.
Figure 3: Perplexity on the validation set of Wikitext2 for linear interpolation (left) and globalnormalization (right), for various values of hyperparameters θ, λ and α. We use a cache model ofsize 2000. The base model has a validation perplexity of 104.2. The best linear interpolation has aperplexity of 72.1, while the best global normalization has a perplexity of 73.5.
Figure 4: Test perplexity as a function of the number of words in the cache, for our method and aunigram cache baseline. We observe that our approach can uses larger caches than the baseline.
Figure 5: Perplexity on the development and control sets of lambada, as a function of the interpo-lation parameters λ.
