Figure 1: Beta-Bernoulli program (left) alongside its computational graph (right). Fetching x fromthe graph generates a binary Vector of 50 elements.
Figure 2:	Variational auto-encoder for a data set of 28×28 pixel images: (left) graphical model, withdotted lines for the inference model; (right) probabilistic program, with 2-layer neural networks.
Figure 3:	Bayesian RNN: (left) graphical model; (right) probabilistic program. The program hasan unspecified number of time steps; it uses a symbolic for loop (tf.scan).
Figure 4: Computational graph for a probabilistic program with stochastic control flow.
Figure 5: Hierarchical model: (left) graphical model; (right) probabilistic program. It is a mixtureof Gaussians over D-dimensional data {xn} ∈ RN×D. There are K latent cluster means β ∈RK×D.
Figure 6: (left) Variational inference. (right) Monte Carlo.
Figure 7:	Generative adversarial networks: (left) graphical model; (right) probabilistic program.
Figure 8:	Data subsampling with a hierarchical model. We define a subgraph of the full model,forming a plate of size M rather than N. We then scale all local random variables by N/M.
Figure 9: Edward program for Bayesian logistic regression with Hamiltonian Monte Carlo (HMC).
Figure 10:	Bayesian neural network for classification.
Figure 11:	Latent Dirichlet allocation (Blei et al., 2003).
Figure 12:	Gaussian matrix factorization.
Figure 13:	Dirichlet process mixture model.
Figure 14: Complete script for a VAE (Kingma & Welling, 2014) with batch training. It generatesMNIST digits after every 1000 updates.
Figure 15: Exponential family embedding for binary data (Rudolph et al., 2016). Here, MAP is usedto maximize the total sum of conditional log-likelihoods and log-priors.
