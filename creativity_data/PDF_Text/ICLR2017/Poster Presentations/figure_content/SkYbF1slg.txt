Figure 1: A neural network interpretaton for random variables X, Y, Y,Y, R.
Figure 2: Comparison of filters obtained from 105 natural image patches of size 12×12 by ourmethods (Alg.1 and Alg.2) and other methods. The number of output filters was K1 = 144. (a):Alg.1. (b): Alg.2. (c): IICA. (d): FICA. (e): SRBM (p = 0.01). (f): SRBM (p = 0.03).
Figure 3: Comparison of quantization effects and convergence rate by coefficient entropy (seeA.122) and conditional entropy (see A.125) corresponding to training results (filters) shown in Fig-ure 2. The coefficient entropy (panel a) and conditional entropy (panel b and c) are shown as afunction of training time on a logarithmic scale. All experiments run on the same machine usingMatlab. Here we sampled once every 10 epoches out of a total of 300 epoches. We set epoch numbert0 = 50 for Alg.1 and Alg.2 and the start time to 1 second.
Figure 4:	Comparison of basis vectors obtained by our method and other methods. Panel (a)-(e)correspond to panel (a)-(e) in Figure 2, where the basis vectors are given by (A.130). The basisvectors in panel (f) are learned by MBDL and given by (A.127).
Figure 5:	Filters and bases obtained from Olshausen’s image dataset and MNIST dataset by Al-gorithm 2. (a) and (b): 400 typical filters and the corresponding bases obtained from Olshausen’simage dataset, where K0 = 82 and K1 = 1024. (c) and (d): 400 typical filters and the correspondingbases obtained from the MNIST dataset, where K0 = 183 and K1 = 1024.
Figure 6: CFE as a function of training time for Alg.2, with v1 = 0.2, 0.4, 0.6 or 0.8. In allexperiments parameters were set to tmax = 100, t0 = 50 and τ = 0.8. (a): corresponding toFigure 5(a) or Figure 5(b). (b): corresponding to Figure 5(c) or Figure 5(d).
Figure 7: Image denoising. (a): the right half of the original image is distorted by Gaussian noiseand the norm of the difference between the distorted image and the original image is 23.48. (b):image denoising by our method (Algorithm 1), with 14 bases used. (c) and (d): image denoisingusing dictionary learning, with 100 bases used.
