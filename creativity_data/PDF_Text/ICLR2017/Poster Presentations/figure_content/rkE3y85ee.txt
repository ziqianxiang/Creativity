Figure 1: The Gumbel-Softmaχ distribution interpolates between discrete one-hot-encoded categor-ical distributions and continuous categorical densities. (a) For low temperatures (τ = 0.1, τ = 0.5),the eχpected value of a Gumbel-Softmaχ random variable approaches the eχpected value of a cate-gorical random variable with the same logits. As the temperature increases (τ = 1.0, τ = 10.0), theeχpected value converges to a uniform distribution over the categories. (b) Samples from Gumbel-Softmaχ distributions are identical to samples from a categorical distribution as τ → 0. At highertemperatures, Gumbel-Softmaχ samples are no longer one-hot, and become uniform as τ → ∞.
Figure 2: Gradient estimation in stochastic computation graphs. (1) Vθf (x) can be computed viabackpropagation if x(θ) is deterministic and differentiable. (2) The presence of stochastic nodez precludes backpropagation as the sampler function does not have a well-defined gradient. (3)The score function estimator and its variants (NVIL, DARN, MuProp, VIMCO) obtain an unbiasedestimate of Vθf(x) by backpropagating along a surrogate loss f logpθ(z), where f = f(x) - b andb is a baseline for variance reduction. (4) The Straight-Through estimator, developed primarily forBernoulli variables, approximates Vθz ≈ 1. (5) Gumbel-Softmax is a path derivative estimator fora continuous distribution y that approximates z . Reparameterization allows gradients to flow fromf(y) to θ. y can be annealed to one-hot categorical variables over the course of training.
Figure 4: Test loss (negative variational lower bound) on binarized MNIST VAE with (a) Bernoullilatent variables (784 - 200 - 784) and (b) categorical latent variables (784 - (20 × 10) - 200).
Figure 5: GUmbel-Softmax allows Us to backpropagate through samples from the posterior qφ(y |x),providing a scalable method for semi-supervised learning for tasks with a large number ofclasses. (a) Comparison of training speed (steps/sec) between Gumbel-Softmax and marginaliza-tion (Kingma et al., 2014) on a semi-supervised VAE. Evaluations were performed on a GTX TitanXR GPU. (b) Visualization of MNIST analogies generated by varying style variable z across eachrow and class variable y across each column.
Figure 6: Semi-supervised generative model proposed by Kingma et al. (2014). (a) Generativemodel pθ(x|y, Z) synthesizes images from latent Gaussian “style" variable Z and categorical classvariable y. (b) Inference model qφ(y, z|x) samples latent state y, z given x. Gaussian z can bedifferentiated with respect to its parameters because it is reparameterizable. In previous work, wheny is not observed, training the VAE objective requires marginalizing over all values of y. (c) Gumbel-Softmax reparameterizes y so that backpropagation is also possible through y without encounteringstochastic nodes.
Figure 7: Network architecture for (a) classification qφ(y∣χ) (b) inference qφ(z∣χ, y), and (C) gen-erative pθ(x|y, Z) models. The output of these networks parameterize Categorical, Gaussian, andBernoulli distributions which we sample from.
