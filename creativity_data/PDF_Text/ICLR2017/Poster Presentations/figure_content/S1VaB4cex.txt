Figure 1: Fractal architecture. Left: A simple expansion rule generates a fractal architecture withC intertwined columns. The base case, f1pzq, has a single layer of the chosen type (e.g. convolutional)between input and output. Join layers compute element-wise mean. Right: Deep convolutionalnetworks periodically reduce spatial resolution via pooling. A fractal version uses fC as a buildingblock between pooling layers. Stacking B such blocks yields a network whose total depth, measuredin terms of convolution layers, is B ∙ 2Cτ. This example has depth 40 (B = 5, C = 4).
Figure 2: Drop-path. A fractal network block functions with some connections between layersdisabled, provided some path from input to output is still available. Drop-path guarantees at least onesuch path, while sampling a subnetwork with many other paths disabled. During training, presentinga different active subnetwork to each mini-batch prevents co-adaptation of parallel paths. A globalsampling strategy returns a single column as a subnetwork. Alternating it with local samplingencourages the development of individual columns as performant stand-alone subnetworks.
Figure 3: Implicit deep supervision. Left: Evolution of loss for plain networks of depth 5, 10, 20and 40 trained on CIFAR-100. Training becomes increasingly difficult for deeper networks. At 40layers, we are unable to train the network satisfactorily. Right: We train a 4 column fractal networkwith mixed drop-path, monitoring its loss as well as the losses of its four subnetworks correspondingto individual columns of the same depth as the plain networks. As the 20-layer subnetwork starts tostabilize, drop-path puts pressure on the 40-layer column to adapt, with the rest of the network as itsteacher. This explains the elbow-shaped learning curve for Col #4 that occurs around 25 epochs.
