Figure 1: An n-layer neural network structure for finding the binary expansion of a number in [0, 1].
Figure 2: The implementation of polynomial functionThis indicates, to achieve ε-approximation error, one should choose n = [log P] + 1. Besides,since we used O(n + p) layers with O(n) binary step units and O(pn) rectifier linear units intotal, this multilayer neural network thus has O (P + log ∣) layers, O (log P) binary step units andO (P log P) rectifier linear units.	□In Theorem 2, we have shown an upper bound on the size of multilayer neural network for approxi-mating polynomials. We can easily observe that the number of neurons in network grows as PlogPwith respect to P, the degree of the polynomial. We note that both Andoni et al. (2014) and Barron(1993) showed the sizes of the networks grow exponentially with respect toP if only 3-layer neuralnetworks are allowed to be used in approximating polynomials.
