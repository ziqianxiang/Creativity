Figure 1: A sequence of optimization problems of increasing complexity, where the first ones areeasy to solve but only the last one corresponds to the actual problem of interest. It is possible totackle the problems in order, starting each time at the solution of the previous one and tracking thelocal minima along the way.
Figure 2: Top: Stochastic depth. Bottom: mollifying network. The dashed line represents theoptional residual connection. In the top path, the input is processed with a convolutional blockfollowed by a noisy activation function, while in the bottom path the original activation of the layerl - 1 is propagated untouched. For each unit, one of the two paths in picked according to a binarystochastic decision π.
Figure 3: The figures show how to evolve the model to make it closer to a linear network. Arrowsdenote the direction of the noise pushing the activation function towards the linear function. a) Thequasi-convex envelope established by a ∣sigmoid(∙)∣ around ∣0.25x∣. b) A depiction of how the noisepushes the sigmoid to become a linear function.
Figure 4: We compare different annealing schedules With respect to the time in y-axis(iterations).
Figure 5: We compare the training performance of different types of annealing methods used withmollification procedure to anneal the parameter of the mollification p. Decaying the p exponentiallyachieves both better training and validation performance.
Figure 6: We show the learning curves of the model where we don’t inject the noise during thetraining and instead use the deterministic approximation for the mollification during the training aswell. The differerence in terms of speed of learning is very small.
Figure 7: We investigate the effect of using batch norm and residual connection for mollifcation andcompare against to the network with residual connections and batch-norm. The effect of batch normon this task for mollification seems to be very small and training convergence performance of the allthe approaches are very close to each other.
Figure 8: The learning curves of a 6-layers MLP Table 1: CIFAR10 deep convolutionalwith sigmoid activation function on 40 bit parity neural network.
Figure 9: The training curve of a bidirectional-RNN that predicts the embedding correspondingto a sequence of characters.
Figure 10: Training and validation losses over 500 epochs of a mollified convolutional networkcomposed by 110-layers. We compare against ResNet and Stochastic depth.
