Figure 1: Overview of the trained ternary quantization procedure.
Figure 2: Ternary weights value (above) and distribution (below) with iterations for different layersof ResNet-20 on CIFAR-10.
Figure 3: ReSNet-20 on CIFAR-10 with different weight precision.
Figure 4: Training and validation accuracy of AlexNet on ImageNetWe draw the process of training in Figure 4, the baseline results of AlexNet are marked with dashedlines. Our ternary model effectively reduces the gap between training and validation performance,which appears to be quite great for DoReFa-Net and TWN. This indicates that adopting trainable Wlpand Wn helps prevent models from ovefitting to the training set.
Figure 5:	Accuracy v.s. Sparsity on ResNet-206.1	.2 Sparsity and efficiency of ALEXNETWe further analyze parameters from our AlexNet model. We calculate layer-wise density (complementof sparsity) as shown in Table 4. Despite We use different Wp and Wln for each layer, ternary weightscan be pre-computed when fetched from memory, thus multiplications during convolution and innerproduct process are still saved. Compared to Deep Compression, we accelerate inference speed usingternary values and more importantly, we reduce energy consumption of inference by saving memoryreferences and multiplications, while achieving higher accuracy.
Figure 6:	Visualization of kernels from Ternary AleXNet trained from Imagenet.
