Figure 1: (a) An input image and a corresponding spatial attention map of a convolutional networkthat shows where the network focuses in order to classify the given image. Surely, this type ofmap must contain valuable information about the network. The question that we pose in this paperis the following: can we use knowledge of this type to improve the training of CNN models ?(b) Schematic representation of attention transfer: a student CNN is trained so as, not only to makegood predictions, but to also have similar spatial attention maps to those of an already trained teacherCNN.
Figure 2: Sum of absolute values attention maps Fsum over different levels of a network trained forface recognition. Mid-level attention maps have higher activation level around eyes, nose and lips,high-level activations correspond to the whole face.
Figure 3: Attention map-ping over feature dimen-sion.
Figure 4: Activation attention maps for various ImageNet networks: Network-In-Network (62%top-1 val accuracy), ResNet-34 (73% top-1 val accuracy), ResNet-101 (77.3% top-1 val accuracy).
Figure 5: Schematics of teacher-student attention transfer for the case when both networks areresidual, and the teacher is deeper.
Figure 6: Top activation attention maps for different Scenes networks: original pretrained ResNet-18(ResNet-18-ImageNet), ResNet-18 trained on Scenes (ResNet-18-scenes), ResNet-18 trained withattention transfer (ResNet-18-scenes-AT) with ResNet-34 as a teacher, ResNet-34 trained on Scenes(ResNet-34-scenes). Predicted classes for each task are shown on top. Attention maps look moresimilar after transfer (images taken from test set).
