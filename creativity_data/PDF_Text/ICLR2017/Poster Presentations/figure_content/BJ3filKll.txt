Figure 1: We illustrate the embedding of a manifold by a deep network using the famous Swiss Roll example(left). Dots represent color coded input data, with color indicating one of the intrinsic coordinates of each inputpoint. In the center, the data is divided into three parts using hidden units represented by the yellow and cyanplanes. Each part is then approximated by a monotonic chain of linear segments. Additional hidden units, alsodepicted as planes, control the orientation of the next segments in the chain. A second layer of the network thenflattens each chain into a 2D Euclidean plane, and assembles these into a common 2D representation (right).
Figure 2:	Left: A continuous chain of linear segments (above) that can beflattened to lie in a single low-dimensional linear subspace (bottom). Right:A monotonic chain. Sk denotes the k,th segment in the chain. Hk is a hyper-plane bounding the half-space that separates Sι,...,Sk from Sk+ι,..., SK.
Figure 3:	This plot shows the error in flattening the Swiss Roll. Relativeerror is constant in every segment, starting from zero for each monotonic chainand increasing with each segment. The absolute error (for display purposesthis error is normalized by the maximal distance from the Swiss Roll to itslinear approximation) behaves similarly, but vanishes at the end points of eachsegment where the Swiss Roll and its linear approximation coincide.
Figure 4:	This graph shows error in the embedding produced by a trainednetwork. Each curve represents a manifold of different dimension, witha different number of segments. Each curve shows how error in the em-bedding on validation points changes as the number of hidden units in-creases. Stars indicate the validation error at the point of each curve inwhich h = m + k - 1. As our theory predicts, the error has reached anasymptote close to zero at these points.
Figure 5:	A network trained with m = 2,k = 7,h = 6. Left: Colored dotsrepresent points on the manifold. Their ground truth coordinates are encodedby the size and hue of the dots. Colored rectangles represent the hyperplanesassociated with the six hidden units. Right: We show the labels generated foreach point, in 2D. Points are colored to indicate their segment. The embedding isnear perfect.
Figure 6: We train a regression network to learn the azimuth and elevation of face images. Left: the faceimages projected to 3D and the hyperplanes learned in the first network layer. Dot size encodes elevation andhue encodes azimuth. Right: We show the images embedded in a 2D space by the trained network.
Figure 7: In black, we show a 1D monotonic chain with three segments. In red, we show three hidden unitsthat flatten this chain into a line. Note that each hidden unit corresponds to a hyperplane (in this case, a line)that separates the segments into two connected components. The third hyperplane must be almost parallel tothe third segment. This leads to large errors for noisy points near S3.
Figure 8: We train a network on a classification problem in which the points lie on a low-dimensional manifold.
