Figure 1: Network architecture for the MNIST data set.
Figure 2: Results on MNIST of Adagrad, Adadelta and Adam followed by LW-SVM. Weverify that switching to LW-SVM leads to better solutions than running SGD longer (shadedcontinued plots).
Figure 3: Network architecture for the CIFAR data sets.
Figure 4: Results on CIFAR-10 of Adagrad, Adadelta and Adam followed by LW-SVM. Thesuccessive drops of the training objective function with LW-SVM correspond to the passesover the layers.
Figure 5: Results on CIFAR-100 of Adagrad, Adadelta and Adam followed by LW-SVM.
Figure 6:	Detailed network architecture for the MNIST data set.
Figure 7:	Difference of Convex Networks for the optimization of Conv1 in the MNISTarchitecture. The two leftmost columns represent the DC networks. For each layer, theright column indicates the non-decomposed corresponding operation. Note that we representthe DC decomposition of the SVM layer as unique blocks to keep the graph simple. Giventhe decomposition method for linear and non-linear layers, one can write down the explicitoperations without special difficulty.
Figure 8: Behavior of different algorithms for Î» = 0.01. The x-axis has been rescaled tocompare the evolution of all algorithms (real training times vary between half an hour to afew hours for the different runs).
