Figure 1: Samples with very high discrim-ination values (D=1.0) in DCGAN modeltrained on CelebA dataset.
Figure 2: Illustration of missing modes problem.
Figure 3: The distributions of MODE scores for GAN and regularized GAN.
Figure 4: (Left 1-5) Different hyperparameters for MNIST generation. The values of the λ1 andλ2 in our Regularized GAN are listed below the corresponding samples. (Right 6-7) Best samplesthrough grid search for GAN and Regularized GAN.
Figure 5: Test set images that are on missing mode. Left: Both MDGAN and DCGAN missing.
Figure 6: Samples generated from different generative models. For each compared model, wedirectly take ten decent samples reported in their corresponding papers and code repositories. Notehow MDGAN samples are both globally more coherent and locally have sharp textures.
Figure 7: Sideface samples generated by Regularized-GAN and MDGAN.
Figure 8: The detailed training procedure ofan MDGAN example.
Figure 9: Comparison results on a toy 2D mixture of Gaussians dataset. The columns on the leftshows heatmaps of the generator distributions as the number of training epochs increases, whereasthe rightmost column presents the target, the original data distribution. The top row shows standardGAN result. The generator has a hard time oscillating among the modes of the data distribution, andis only able to “recover” a single data mode at once. In contrast, the bottom row shows results of ourregularized GAN. Its generator quickly captures the underlying multiple modes and fits the targetdistribution.
Figure 10: Samples generated by our models and VAEGAN. The third line are samples generated byour self-trained VAEGAN model, with default settings. The last line are generated samples reportedin the original VAEGAN paper. We depict both of them here for a fair comparison.
