Figure 1: Graphical models of the smoothed approximating posterior (a) and prior (b), and thenetwork realizing the autoencoding term of the ELBO from Equation 2 (c). Continuous latent vari-ables ζi are smoothed analogs of discrete latent variables zi , and insulate z from the observed vari-ables x in the prior (b). This facilitates the marginalization of the discrete z in the autoencoding termof the ELBO, resulting in a network (c) in which all operations are deterministic and differentiablegiven independent stochastic input P 〜U[0,1].
Figure 2: Inverse CDF of the spike-and-exponential smoothing transformation forρ ∈ {0.2, 0.5, 0.8}; β = 1 (dotted), β = 3 (solid), and β = 5 (dashed) (a). Rectified linearunit with dropout rate 0.5 (b). Shift (red) and scale (green) noise from batch normalization; withmagnitude 0.3 (dashed), -0.3 (dotted), or 0 (solid blue); before a rectified linear unit (c). In allcases, the abcissa is the input and the ordinate is the output of the effective transfer function. Thenovel stochastic nonlinearity F-Z1∣x @)(P) from Figure 1c, of which (a) is an example, is qualitativelysimilar to the familiar stochastic nonlinearities induced by dropout (b) or batch normalization (c).
Figure 3: Graphical model of the hierarchical approximating posterior (a) and the network realizingthe autoencoding term of the ELBO (b) from Equation 2. Discrete latent variables zj only dependon the previous zi<j through their smoothed analogs ζi<j . The autoregressive hierarchy allows theapproximating posterior to capture correlations and multiple modes. Again, all operations in (b) aredeterministic and differentiable given the stochastic input ρ.
Figure 4: Graphical models of the approximating posterior (a) and prior (b) with a hierarchy ofcontinuous latent variables. The shaded regions in parts (a) and (b) expand to Figures 3a and 1brespectively. The continuous latent variables z build continuous manifolds, capturing properties likeposition and pose, conditioned on the discrete latent variables z, which can represent the discretetypes of objects in the image.
Figure 5: Evolution of samples from a discrete VAE trained on dynamically binarized MNIST, usingpersistent RBM Markov chains. We perform 100 iterations of block-Gibbs sampling on the RBMbetween successive rows. Each horizontal group of 5 uses a single, shared sample from the RBM,but independent continuous latent variables, and shows the variation induced by the continuouslayers as opposed to the RBM. The long vertical sequences in which the digit ID remains constantdemonstrate that the RBM has well-separated modes, each of which corresponds to a single (oroccasionally two) digit IDs, despite being trained in a wholly unsupervised manner.
Figure 6: Log likelihood versus the number of iterations of block Gibbs sampling per minibatch (a),the number of units in the RBM (b), and the number of layers in the approximating posterior overthe RBM (c). Better sampling (a) and hierarchical approximating posteriors (c) support better per-formance, but the network is robust to the size of the RBM (b).
Figure 7: Inverse CDF of the mixture of ramps transformation for P ∈ {0.2, 0.5, 0.8}In Equation 20, Fq-(ζ1Ix,φ)(P) is quasi-sigmoidal as a function of q(z = 1|x, φ). If P 0.5, F-1is concave-up; if P 0.5, F-1 is concave-down; if P ≈ 0.5, F-1 is sigmoid. In no case is F-1extremely flat, so it does not kill gradients. In contrast, the sigmoid probability of z inevitablyflattens.
Figure 8: Inverse CDF of the spike-and-slab transformation for P ∈ {0.2, 0.5, 0.8}(D.3 Engineering effective smoothing transformationsIf the smoothing transformation is not chosen appropriately, the contribution of low-probabilityregions to the expected gradient of the inverse CDF may be large. Using a variant of the inversefunction theorem, we find:∂Θf(FT(P)) = >L-i(ρ) + dFL-1(P) ∙沙T(P) = 0∂	∂FP(Z) ∙ ∂θf (P) = - ðθ z,	(21)where Z = FT(P). Consider the case where r(Zi∣Zi = 0) and r(Zi∣Zi = 1) are unimodal, but havelittle overlap. For instance, both distributions might be Gaussian, with means that are many standarddeviations apart. For values of ζi between the two modes, F(ζi) ≈ q(Zi = 0|x, φ), assumingwithout loss of generality that the mode corresponding to Zi = 0 occurs at a smaller value of ζi thanthat corresponding to Zi = 1. As a result, ∂∂q ≈ 1 between the two modes, and dFq- ≈ r(Zi)evenif r(ζi) ≈ 0. In this case, the stochastic estimates of the gradient in equation 8, which depend upondFq , have large variance.
Figure 9: Log likelihood on statically binarized MNIST versus the number of hidden units per neuralnetwork layer, in the prior (a) and approximating posterior (b). The number of deterministic hiddenlayers in the networks parameterizing the prior/approximating posterior is 1 (blue), 2 (red), 3 (green)in (a/b), respectively. The number of deterministic hidden layers in the final network parameterizingp(x|z) is 0 (solid) or 1 (dashed). All models use only 10 layers of continuous latent variables, withno parameter sharing.
Figure 10:	Distribution of estimates of the log-partition function, using Bennett’s acceptance ratiomethod with parallel tempering, for a single model trained on dynamically binarized MNIST (a),statically binarized MNIST (b), Omniglot (c), and Caltech-101 Silhouettes (d)I Comparison modelsIn Table 1, we compare the performance of the discrete variational autoencoder to a selection ofrecent, competitive models. For dynamically binarized MNIST, we compare to deep belief networks(DBN; Hinton et al., 2006), reporting the results of Murray & Salakhutdinov (2009); importance-weighted autoencoders (IWAE; Burda et al., 2016); and ladder variational autoencoders (LadderVAE; S0nderby et al., 2016).
Figure 11:	Evolution of samples from a discrete VAE trained on statically binarized MNIST, usingpersistent RBM Markov chains. We perform 100 iterations of block-Gibbs sampling on the RBMbetween successive rows. Each horizontal group of 5 uses a single, shared sample from the RBM, butindependent continuous latent variables, and shows the variation induced by the continuous layersas opposed to the RBM. Vertical sequences in which the digit ID remains constant demonstrate thatthe RBM has distinct modes, each of which corresponds to a single digit ID, despite being trainedin a wholly unsupervised manner.
Figure 12: Evolution of samples from a discrete VAE trained on Omniglot, using persistent RBMMarkov chains. We perform 100 iterations of block-Gibbs sampling on the RBM between successiverows. Each horizontal group of 5 uses a single, shared sample from the RBM, but independentcontinuous latent variables, and shows the variation induced by the continuous layers as opposed tothe RBM.
Figure 13: Evolution of samples from a discrete VAE trained on Caltech-101 Silhouettes, usingpersistent RBM Markov chains. We perform 100 iterations of block-Gibbs sampling on the RBMbetween successive rows. Each horizontal group of 5 uses a single, shared sample from the RBM,but independent continuous latent variables, and shows the variation induced by the continuouslayers as opposed to the RBM. Vertical sequences in which the silhouette shape remains similardemonstrate that the RBM has distinct modes, each of which corresponds to a single silhouette type,despite being trained in a wholly unsupervised manner.
