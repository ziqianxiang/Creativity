Table 2: Costs on test executions of the dynamics-based servoing policies for different feature dynamics andweighting of the features. The reported numbers are the mean and standard error across 100 test trajectories, ofup to 100 time steps each. We test on executions with the training cars and the novel cars; for consistency, thenovel cars follow the same route as the training cars. We compare the performance of policies with unweightedfeatures or weights learned by other methods. For the case of unweighted feature dynamics, we use the crossentropy method (CEM) to learn the relative weights λ of the control and the single feature weight w. Forthe other cases, we learn the weights with CEM, Trust Region Policy Optimization (TRPO) for either 2 or 50iterations, and our proposed FQI algorithm. CEM searches over the full space of policy parameters w andλ, but it was only ran for pixel features since it does not scale for high-dimensional problems. We reportthe number of training trajectories in parenthesis. For TRPO, we use a fixed number of training samples periteration, whereas for CEM and FQI, we use a fixed number of training trajectories per iteration. We use abatch size of 4000 samples for TRPO, which means that at least 40 trajectories were used per iteration, sincetrajectories can terminate early, i.e. in less than 100 time steps.
Table 3: Sample observations from test executions in our experiments, and the costs for each trajectory, fordifferent feature dynamics. We use the weights learned by our FQI algorithm. This table follows the sameformat as Table 1. Some of the trajectories were shorter than 100 steps because of the termination condition(e.g. the car is no longer in the image). The first observation of each trajectory is used as the target observation.
Table 4: Costs on test executions of servoing policies that were trained end-to-end with TRPO. These policiestake in different observation modalities: ground truth car position or image-based observations. This tablefollows the same format as Table 2. The mean of the first policy is parametrized as a 3-layer MLP, with tanhnon-linearities except for the output layer; the first 2 fully connected layers use 32 hidden units each. For theother policies, each of their means is parametrized as a 5-layer CNN, consisting of 2 convolutional and 3 fully-connected layers, with ReLU non-linearities except for the output layer; the convolutional layers use 16 filters(4 × 4, stride 2) each and the first 2 fully-connected layers use 32 hidden units each. All the policies are trainedwith TRPO, a batch size of 4000 samples, 500 iterations, and a step size of 0.01. The car position observationsare not affected by the appearance of the cars, so the test performance for that modality is the same regardlessof which set of cars are used.
Table 5: Costs on test executions when using classical image-based visual servoing (IBVS) with respect tofeature points derived from bounding boxes and keypoints derived from hand-engineered features. Since thereis no learning involved in this method, we only test with one set of cars: the cars that were used for training inthe other methods. This table follows the same format as Table 2. This method has one hyperparameter, whichis the gain for the control law. For each feature type, we select the best hyperparameter (shown in parenthesis)by validating the policy on 10 validation trajectories for gains between 0.05 and 2, in increments of 0.05. Theservoing policies based on bounding box features achieve low cost, and even lower ones if ground truth cardynamics is used. However, servoing with respect to hand-crafted feature points is significantly worse than theother methods.
Table 6: Costs on test executions when using classical position-based visual servoing (PBVS). Since there isno learning involved in this method, we only test with one set of cars: the cars that were used for training in theother methods. This table follows the same format as Table 2. This method has one hyperparameter, which isthe gain for the control law. For each condition, we select the best hyperparameter (shown in parenthesis) byvalidating the policy on 10 validation trajectories for gains between 0.05 and 2, in increments of 0.05. Theseservoing policies, which use ground truth car poses, outperforms all the other policies based on images. Inaddition, the performance is more than two orders of magnitude better if ground truth car dynamics is used.
