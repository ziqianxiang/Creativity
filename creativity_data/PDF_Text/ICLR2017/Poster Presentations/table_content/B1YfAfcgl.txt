Table 1: Experimental results: Entropy-SGD vs. SGD / Adam12Published as a conference paper at ICLR 2017Tuning the momentum in Entropy-SGD was crucial to getting good results on RNNs. While the SGDbaseline on PTB-LSTM does not use momentum (and in fact, does not train well with momentum)we used a momentum of 0.5 for Entropy-SGD. On the other hand, the baseline for char-LSTM wastrained with Adam with β1 = 0.9 (β1 in Adam controls the moving average of the gradient) whilewe set β1 = 0.5 for Entropy-Adam. In contrast to this observation about RNNs, all our experimentson CNNs used a momentum of 0.9. In order to investigate this difficulty, we monitored the anglebetween the local entropy gradient and the vanilla SGD gradient during training. This angle changesmuch more rapidly for RNNs than for CNNs which suggests a more rugged energy landscape forthe former; local entropy gradient is highly uncorrelated with the SGD gradient in this case.
