Table 1: Results on the IFTTT task. Left: non-English and unintelligible examples removed (2,262recipes). Right: examples for which at least 3+ humans agree with gold (758 recipes).
Table 2: Translations at different resolutions (size Constraints im-posed during deCoding) for two example sentenCes.
Table 3: Hyperparameter choice for DRNNs in the synthetic and IFTTT tasksTask	Encoder	Dim	Batch	Learning Rate	Regularization ρsynthetic	LSTM	50	20	0.05	1×10-5IFTTT	GRU	150	35	0.06	1×10-4IFTTT	LSTM	150	35	0.05	5×10-4Table 4: Models used in the machine translation task.
Table 4: Models used in the machine translation task.
Table 5: Synthetic tree dataset statistics. Tree size is measured in number of nodes, depth is thelargest path from the root node to a leaf and width is the maximum number of children for any nodein the tree. The values reported correspond to means with one standard deviation in parentheses.
Table 6: IFTTT dataset statistics. The middle columns show percentage of trees that containnonempty argumentS and parameterS in trigger (IF) and aCtion (THEN) branCheS. The laSt ColumnShowS average (with Standard deviation) tree Size and depth.
Table 7: Example structural perturbations for likelihood robustness experiments.
