Table 1: Experiments on Penn Treebank without dropout.
Table 2: Experiments on Text8 without dropoutModel	Size	Training	Perp. on development setVanilla RNN	500 hidden units	Mikolov etal.(2014)-	184SCRN	500 hidden units	Mikolov et al. (2014)	161LSTM	500 hidden units	Mikolov et al. (2014)	156MemN2N	500 hidden units	Sukhbaatar et al. (2015)	147LSTM (2 IayerS)	46.4M parameters	Trained by US	139.9CFN (2 IayerS)	46.4M parameters	Trained by US		142.0	For concreteness, the exact implementation for the two-layer architecture of our model isht(0) = W(0)xth t0) = Drop(ht0),p)h(1) = θ(1) Θ tanh(h(-)ι) + η(1) Θ tanh(W⑴h(O))向1) = Drop(hf),p)h(2) = θ(2) Θ tanh(h(-)1) + η(2 Θ tanh(W⑵h(1))h t2) = Drop(ht2),p)yt = LogSoftmax(W ⑶必2) + b)where Drop(z, p) denotes the dropout operator with a probability p of setting components in z tozero. We compute the gates according toθ(') ：= σ (uθ')h(-1 + v(')h('-1) + bθ) and η(') := σ (u^)h(')1 + 吗')h('-1) + b)where	h(-1 = Drop(h(-1,q) and h ('-1) = Drop(ht'-1) ,q),
Table 3: Experiments on Penn Treebank with dropout.
