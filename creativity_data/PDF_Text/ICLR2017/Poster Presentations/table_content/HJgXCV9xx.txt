Table 1: Test accuracy of various models per iteration in the dataset batch size case (using batch sizeequal to the size of the full training set) for bAbI, Task 6. Results > 0.95 are in bold.
Table 2: Incorporating Feedback From Humans via Mechanical Turk. Textual feedback isprovided for 10,000 model predictions (from a model trained with 1k labeled training examples),and additional sparse binary rewards (fraction r of examples have rewards). Forward Prediction andReward-based Imitation are both useful, with their combination performing best.
Table 3: Test accuracy of various models in the dataset batch size case (using batch size equal to thesize of the full training set) for bAbI, task 3. Results > 0.95 are in bold.
Table 4: Incorporating Feedback From Humans via Mechanical Turk: comparing real humanfeedback to synthetic feedback. Textual feedback is provided for 10,000 model predictions (froma model trained with 1k labeled training examples), and additional sparse binary rewards (fraction rof examples have rewards). We compare real feedback (rows 2 and 3) to synthetic feedback whenusing FP or RBI+FP (rows 4 and 5).
Table 5: Fully Supervised (Imitation Learning) Results on Human Questions	r = 0	r = 0.1	r = 0.5	r = 1=0	0.499	0.502	0.501	0.502=0.1	0.494	0.496	0.501	0.502=0.25	0.493	0.495	0.496	0.499= 0.5	0.501	0.499	0.501	0.504=1	0.497	0.497	0.498	0.497Table 6: Second Iteration of Feedback Using synthetic textual feedback of synthetic Task2+3 withthe RBI+FP method, an additional iteration of data collection of 10k examples, varying sparse binaryreward fraction r and exploration . The performance of the first iteration model was 0.478.
Table 6: Second Iteration of Feedback Using synthetic textual feedback of synthetic Task2+3 withthe RBI+FP method, an additional iteration of data collection of 10k examples, varying sparse binaryreward fraction r and exploration . The performance of the first iteration model was 0.478.
