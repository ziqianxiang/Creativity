Table 1: Error rates of full-precision and ternary ResNets on Cifar-105.1	ImageNetWe further train and evaluate our model on ILSVRC12(Russakovsky et al. (2015)). ILSVRC12 is a1000-category dataset with over 1.2 million images in training set and 50 thousand images in validationset. Images from ILSVRC12 also have various resolutions. We used a variant of AlexNet(Krizhevskyet al. (2012)) structure by removing dropout layers and add batch normalization(Ioffe & Szegedy,2015) for all models in our experiments. The same variant is also used in experiments described inthe paper of DoReFa-Net.
Table 2: Top1 and Top5 error rate of AlexNet on ImageNet1https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val6Published as a conference paper at ICLR 2017EpochsFigure 4: Training and validation accuracy of AlexNet on ImageNetWe draw the process of training in Figure 4, the baseline results of AlexNet are marked with dashedlines. Our ternary model effectively reduces the gap between training and validation performance,which appears to be quite great for DoReFa-Net and TWN. This indicates that adopting trainable Wlpand Wn helps prevent models from ovefitting to the training set.
Table 3: Topl and Top5 error rate of ResNet-18 on ImageNet6 DISCUSSIONIn this section we analyze performance of our model with regard to weight compression and inferencespeeding up. These two goals are achieved through reducing bit precision and introducing sparsity.
Table 4: Alexnet layer-wise sparsity8Published as a conference paper at ICLR 2017HUHDEHnilHBDDVEHaEHHHM3HII -∣9MEπ3-bbsbi- D .■ ■U -Figure 6:	Visualization of kernels from Ternary AleXNet trained from Imagenet.
