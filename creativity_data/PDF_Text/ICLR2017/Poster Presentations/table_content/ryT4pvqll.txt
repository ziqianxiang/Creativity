Table 1: Each cell shows the percentage of 60 trials with different hyper-parameters (η, c) andrandom restarts that successfully solve an algorithmic task. UREX is more robust to hyper-parameterchanges than MENT. We evaluate MENT with a few temperatures and UREX with τ = 0.1.
Table 2: Results on several algorithmic tasks comparing Q-learning and policy gradient based onMENT and UREX. We find the best hyper-parameters for each method, and run each algorithm 5times with random restarts. Number of successful attempts (out of 5) that achieve a reward thresholdis reported. Expected reward computed over the last few iterations of training is also reported.
Table 3: Copy - number of successful attempts out of 5.
Table 4: DuplicatedInput - number of successful attempts out of 5.							MENT (T = 0.01)			UREX (T = 0.1)			η= 0.1	η = 0.01	η = 0.001	η = 0.1	η = 0.01	η = 0.001C =1	3	5	3	5	5	5C =10	2	5	3	5	5	5C = 40	4	5	3	5	5	5C = 100	2	5	4	5	5	5Table 5: RepeatCopy - number of successful attempts out of 5.
Table 5: RepeatCopy - number of successful attempts out of 5.
Table 7: ReversedAddition - number of successful attempts out of 5.
Table 8: BinarySearch - number of successful attempts out of 5.
Table 9: Generalization results. Each cell includes the number of runs out of 60 different hyper-parameters and random initializations that achieve 100% accuracy on input of length up to the spec-ified length. The bottom row is the maximal length (≤ 2000) up to which at least one model achieves100% accuracy.
Table 10: Example trace on the BinarySearch task where n = 512 and the number to find is atposition 100. At time t the agent observes St from the environment and samples an action at. Wealso include the inferred range of indices to which the agent has narrowed down the position of x.
