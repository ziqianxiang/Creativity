Table 1: Five Topics from the TopicRNN Model with 100 Neurons and 50 Topics on the PTB Data.
Table 2: TopicRNN and its counterparts exhibit lower perplexity scores across different networksizes than reported in Mikolov and Zweig (2012). Table 2a shows per-word perplexity scores for 10neurons. Table 2b and Table 2c correspond to per-word perplexity scores for 100 and 300 neuronsrespectively. These results prove TopicRNN has more generalization capabilities: for example weonly need a TopicGRU with 100 neurons to achieve a better perplexity than stacking 2 LSTMs with200 neurons each: 112.4 vs 115.9)(a)			(b)		10 Neurons	Valid	Test	100 Neurons	Valid	TestRNN (no features)	239.2	225.0	RNN (no features)	150.1	142.1RNN (LDA features)	197.3	187.4	RNN (LDA features)	132.3	126.4TopicRNN	184.5	172.2	TopicRNN	128.5	122.3TopicLSTM	188.0	175.0	TopicLSTM	126.0	118.1TopicGRU	178.3	166.7	TopicGRU	118.3	112.4(C)					300 Neurons	Valid	TestRNN (no features)	-	124.7RNN (LDA features)	-	113.7TopicRNN	118.3	112.2TopicLSTM	104.1	99.5TopicGRU	99.6	97.3
Table 3: Generated text using the TopicRNN model on the PTB (top) and IMDB (bottom).
Table 4: Classification error rate on IMDB 100k dataset. TopicRNN provides the state of the arterror rate on this dataset.
Table 5: Dimensions of the parameters of the model.
Table 6: Some Topics from the TopicRNN Model on the IMDB Data.
