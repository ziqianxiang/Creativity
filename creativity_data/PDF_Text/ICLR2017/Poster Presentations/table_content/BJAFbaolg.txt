Table 3: Inception score (with standard error) of 50 000 samples generated by models trained onCIFAR-10. We use the models in Salimans et al. (2016) as baseline. ’SP’ corresponds to the bestmodel described by Salimans et al. (2016) trained in a semi-supervised fashion. ’-L’ correspondsto the same model after removing the label in the training process (unsupervised way), ’-MBF’corresponds to a supervised training without minibatch features.
Table 1: Parzen-window-based estimator oflower bound on average test log-likelihoodon MNIST (in nats).
Table 2: Log-likelihood (in nats) estimated by AIS on MNIST test and training sets as reported inWu et al. (2016) and the log likelihood estimates of our model obtained by infusion training (lastthree lines). Our initial model uses a Gaussian output with diagonal covariance, and we appliedboth our lower bound and importance sampling (IS) log-likelihood estimates to it. Since Wu et al.
Table 4: Infusion rate impact on the lower bound log-likelihood (test set) and the samples generatedby a network trained with different number of steps. Each sub-table corresponds to a fixed numberof steps. Each row corresponds to a different infusion rate, where we show its lower bound and alsoits corresponding generated samples from the trained model. Note that for images, we show themean of the Gaussian distributions instead of the true samples. As the number of steps increases, theoptimal infusion rate decreases. Higher number of steps contributes to better qualitative samples, asthe best samples can be seen with 15 steps using α = 0.01.
