Table 1: AIS vs. IWAE bound on 10,000 test examples of binarized MNIST. “# dist” denotes the numberof intermediate distributions used for evalution. We find AIS estimate is consistently 1 nat higher than IWAEbound; AIS+encoder can achieve about the same estimate as AIS, but with 1 order of magnitude less number ofintermediate distributions.
Table 2: Model comparisons on 1000 test and training examples of continuous MNIST. Confidence intervalsreflect the variability from the choice of training or test examples (which appears to be the dominant source oferror for the AIS values). AIS, KDE, and IWAE are all stochastic lower bounds on the log-likelihood.
Table 3: OPtimal variance vs. Fixed varianceIn this section, we consider whether the difference in log-likelihood between models could be anartifact of the Gaussian noise model (which we know to be a Poor fit). In PrinciPle, the Gaussian noiseassumPtion could be unfair to the GANs and GMMNs, because the VAE training uses the correct12Published as a conference paper at ICLR 2017observation model, while the GAN and GMMN objectives do not have any particular observationmodel built in.
