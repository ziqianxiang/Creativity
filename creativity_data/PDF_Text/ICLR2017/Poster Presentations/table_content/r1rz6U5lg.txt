Table 1: Size of the differentmodels compared.
Table 2: Final average relative score on the Hacker’s Delightbenchmark. While all models improve with regards to the ini-tial proposal distribution based on uniform sampling, the modelconditioning on program features reach better performances.
Table 3: Throughput of the pro-posal distribution estimated by timingMCMC for 10000 iterationsModel	Training	TestUniform	76.63%	78.15 %Bias	61.81%	63.56%MLP	60.13%	62.27%Table 4: Final average relative score. The MLP con-ditioning on the features of the program perform bet-ter than the simple bias. Even the unconditioned biasperforms significantly better than the Uniform proposaldistribution.
Table 4: Final average relative score. The MLP con-ditioning on the features of the program perform bet-ter than the simple bias. Even the unconditioned biasperforms significantly better than the Uniform proposaldistribution.
Table 5: Architecture of the BiasEmbedding	Linear (3874 → 100) + ReLU Linear (100 → 300) + ReLU Linear (300 → 300) + ReLU	Outputs	Linear (300 → 9) SoftMax	Linear (300 → 2903) SoftMaxTable 6: Architecture of the Multi Layer PerceptronA.2 Training parametersAll of our models are trained using the Adam (Kingma & Ba, 2015) optimizer, with its defaulthyper-parameters β1 = 0.9, β2 = 0.999, = 10-8. We use minibatches of size 32.
Table 6: Architecture of the Multi Layer PerceptronA.2 Training parametersAll of our models are trained using the Adam (Kingma & Ba, 2015) optimizer, with its defaulthyper-parameters β1 = 0.9, β2 = 0.999, = 10-8. We use minibatches of size 32.
Table 7: Values of the Learning rate used.
