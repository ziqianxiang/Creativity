Table 1: Experimental State Space. For each layer type, we list the relevant parameters and thevalues each parameter is allowed to take.
Table 2: Schedule. The learning agent trains the specified number of unique models at each .
Table 3: Error Rate Comparison with CNNs that only use convolution, pooling, and fully con-nected layers. We report results for CIFAR-10 and CIFAR-100 with moderate data augmentationand results for MNIST and SVHN without any data augmentation.
Table 4: Error Rate Comparison with state-of-the-art methods with complex layer types. We re-port results for CIFAR-10 and CIFAR-100 with moderate data augmentation and results for MNISTand SVHN without any data augmentation.
Table 5: Prediction Error for the top MetaQNN (CIFAR-10) model trained for other tasks. Fine-tuning refers to initializing training with the weights found for the optimal CIFAR-10 model.
Table A1: Top 5 model architectures: CIFAR-10.
Table A2: Top 5 model architectures: SVHN. Note that we do not report the best accuracy on testset from the above models in Tables 3 and 4 from the main text. This is because the model thatachieved 2.28% on the test set performed the best on the validation set.
Table A3: Top 10 model architectures: MNIST. We report the top 10 models for MNIST becausewe included all 10 in our final ensemble. Note that we do not report the best accuracy on test setfrom the above models in Tables 3 and 4 from the main text. This is because the model that achieved0.44% on the test set performed the best on the validation set.
