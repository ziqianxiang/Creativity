Table 1: AUC Comparison for AHRF Mortality Prediction task with and without Domain AdaptationSource-Target	LR	Adaboost	DNN	DANN	VFAE	R-DANN	VRADA3- 2	0.555	0.562	0.569	0.572	0.615	0.603	0.6544- 2	0.624	0.645	0.569	0.589	0.635	0.584	0.6565- 2	0.527	0.554	0.551	0.540	0.588	0.611	0.6162- 3	0.627	0.621	0.550	0.563	0.585	0.708	0.7244- 3	0.681	0.636	0.542	0.527	0.722	0.821	0.7705- 3	0.655	0.706	0.503	0.518	0.608	0.769	0.7822- 4	0.585	0.591	0.530	0.560	0.582	0.716	0.7773- 4	0.652	0.629	0.531	0.527	0.697	0.769	0.7645- 4	0.689	0.699	0.538	0.532	0.614	0.728	0.7382- 5	0.565	0.543	0.549	0.526	0.555	0.659	0.7193- 5	0.576	0.587	0.510	0.526	0.533	0.630	0.7214- 5	0.682	0.587	0.575	0.548	0.712	0.747	0.7755- 1	0.502	0.573	0.557	0.563	0.618	0.563	0.6394- 1	0.565	0.533	0.572	0.542	0.668	0.577	0.6363- 1	0.500	0.500	0.542	0.535	0.570	0.591	0.6312- 1	0.520	0.500	0.534	0.559	0.578	0.630	0.637In the above table, we test classification without adaptation using Logistic Regression (LR), Adaboost withdecision tree classifiers and Feed forward Deep Neural Networks (DNN); and with adaptation using Deep
Table 2: AUC Comparison for ICD9 Diagnosis Code Prediction taskModel23	24	25	32	34	35DANNR-DANNVRADAentire targettarget testentire targettarget testentire targettarget test0.513 0.508 0.5090.509 0.513 0.5310.608 0.581 0.5620.605 0.579 0.5700.620 0.564 0.5570.609 0.563 0.5600.511 0.508 0.5140.527 0.515 0.531
Table 3: AUC Comparison for AHRF Mortality Prediction task for different types of VRADA trainingTraining	23	24	25	32	34	35	42	43	45	52	53	54	0.704	0.777	0.682	0.540	0.764	0.721	0.603	0.727	0.710	0.616	0.782	0.738^H	0.724	0.656	0.719	0.627	0.748	0.683	0.656	0.770	0.755	0.595	0.736	0.732In	0.721	0.688	0.656	0.654	0.757	0.691	0.609	0.766	0.775	0.602	0.709	0.714Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and naturallanguage with recursive neural networks. In Proceedings of the 28th international conference onmachine learning (ICML-11), pp.129-136, 2011.
Table 4:	AUC Comparison for AHRF Mortality Prediction task with adversarial training done at every time-stepModelR-DANNVRADA23	24	25	32	34	35	42	43	45.651 .599 .598 .557 .679 .534 .563 .768 .588.681 .691 .643 .594 .733 .641 .733 .794 .67552	53	54.528 .696 .669.583 .755 .7266.2.2 Effect of Reconstruction LossTable 5 shows the effect of reconstruction loss for our VRADA model. We observe that reconstructingthe original data (i.e. using the decoder for reconstructing the data) helps in the overall performanceimprovement of our VRADA model.
Table 5:	AUC Comparison of VRADA model for AHRF Mortality Prediction task with and without reconstruc-tion lossModelWithout reconstructionWith reconstruction23	24	25	32	34	35	42	43	450.703 0.623 0.570 0.647 0.622 0.564 0.577 0.608 0.5520.724 0.777 0.719 0.654 0.764 0.721 0.656 0.770 0.77552	53	540.599 0.640 0.6760.616 0.782 0.7386.2.3 Impact of Adversarial trainingIn figures 5 and 6 we show the cell state activations for the VRADA and R-DANN without domainadaptation (i.e. no adversarial training). From these figures, we see that the dependencies betweensource and target domains are not transferred correctly since we do not perform adversarial training.
