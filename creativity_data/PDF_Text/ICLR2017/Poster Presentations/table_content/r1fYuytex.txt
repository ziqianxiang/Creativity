Table 1: MiSdaSSification rate for Diferent NetWork SiZeS on MNISTCaSe	Method	NetWork Configuration	MiSclaSSification Rate (%)	Number of ParameterS1	Fully-Connected	784-512-512-10	1.18	669706	SparSely-Connected 50%	784-512-512-10	1.19	335370	Fully-Connected	784-256-256-10	1.35	2693222	SparSely-Connected 60%	784-512-512-10	1.20	268503	SparSely-Connected 70%	784-512-512-10	1.31	2016363	Fully-Connected	784-145-145-10	1.41	136455	SparSely-Connected 80%	784-512-512-10	1.28	1347684	Fully-Connected	784-77-77-10	1.75	67231	SparSely-Connected 90%	784-512-512-10	1.75	679015	Fully-Connected	784-12-12-10	4.68	9706	SparSely-Connected 90%	784-100-100-10	3.16	89614.1	Experimental Results on MNISTThe MNIST dataSet containS 60000 gray-Scale 28 × 28 imageS (50000 for training and 10000 forteSting), falling into 10 claSSeS. A deep fully-connected neural netWork iS uSed for evaluation andthe hinge loSS iS conSidered aS the coSt function. The training Set iS divided into tWo Separate partS.
Table 2: Misclassfication rate for a 784-1024-1024-1024-10 neural network on MNISTMisclassification Rate (%)Method	Without Data Augmentation	With Data Augmentation	# of ParametersSingle-Precision Floating-Point (SPFP)	1.33	0.67	2913290Sparsely-Connected 50% + SPFP	1.17	0.64	1458186Sparsely-Connected 90% + SPFP	1.33	0.66	294103BinaryConnecta (Courbariaux et al. (2015))	1.23	0.76	2913290TernaryConnectb (Lin et al. (2015))	1.15	0.74	2913290Sparsely-Connected 50% + BinaryConnecta	099	0.75	1458186Sparsely-Connected 60% + BinaryConnecta	1.03	0.81	1167165Sparsely-Connected 70% + BinaryConnecta	1.16	0.85	876144Sparsely-Connected 80% + BinaryConnecta	1.32	1.06	585124Sparsely-Connected 90% + BinaryConnecta	1.33	1.36	294103Sparsely-Connected 50% + TernaryConnectb	0.95	0.63	1458186Sparsely-Connected 60% + TernaryConnectb	1.05	0.64	1167165Sparsely-Connected 70% + TernaryConnectb	1.01	0.73	876144Sparsely-Connected 80% + TernaryConnectb	1.11	0.85	585124Sparsely-Connected 90% + TernaryConnectb	1.41	1.05	294103a Binarizing algorithm was only used in the learning phase and single-precision floating-point weights were used during thetest run.
Table 3: MiSclaSSification rate for a Convolutional NetWork on CIFAR10Misclassification Rate (%)Method	Without Data Augmentation	With Data Augmentation	# of ParameterSSingle-PreciSion Floating-Point (SPFP)	1245	9.77	14025866SparSely-Connected 90% + SPFP	12.05	9.30	5523184BinaryConnect a (Courbariaux et al. (2015))	9.91	8.01	14025866TernaryConnect b (Lin et al. (2015))	9.32	7.83	14025866SparSely-Connected 50% + BinaryConnect a	895	7.27	9302154SparSely-Connected 90% + BinaryConnect a	8.05	6.92	5523184SparSely-Connected 50% + TernaryConnect b	845	7.13	9302154SparSely-Connected 90% + TernaryConnect b	7.88	6.99	5523184a Binarizing algorithm WaS only uSed in the learning phaSe and Single-preciSion floating-point WeightS Were uSed during theteSt run.
Table 4: MiSclaSSification rate for a ConVolUtional NetWork on SVHNMethod	MiSclaSSification Rate (%)	Number of ParameterSSingle-PreciSion Floating-Point	4.734615	14025866BinaryConnect a (Courbariaux et al. (2015))	2.134615	14025866TernaryConnectb (Lin et al. (2015))	2.9	14025866SparSely-Connected 90% + BinaryConnect a	2.003846	5523184SparSely-Connected 90% + TernaryConnect b	1.957692	5523184a Binarizing algorithm WaS only uSed in the learning phaSe and Single-preciSion floating-point WeightS Were uSed during the teSt run.
Table 5: Misclassification rate comparison. Sparsity degree for the proposed network is 50% inMNIST, and 90% in SVHN and CIFAR10.__________________________________________________	MNIST	Datasets			SVHN	CIFAR10Method	Binarized/Ternarized Weights During Test Run		BNN (Torch7) (Courbariaux & Bengio (2016))	1.40%	2.53%	10.15%BNN (Theano) (Courbariaux & Bengio (2016))	0.96%	2.80%	11.40%(Baldassi et al. (2015))	1.35%	-	一BinaryConnect (Courbariaux et al. (2015))	1.29%	2.30%	9.90%EBP (Cheng et al. (2015))	2.2%	-	一Bitwise DNNs (Kim & Smaragdis (2016))	1.33%	—	一(Hwang & Sung (2014))	1.45%	-	一Sparsely-Connected + BinaryConnect	1.08%	2.053846%	8.66%Sparsely-Connected + TernaryConnect	0.98%	1.992308%	8.24%Method	Single-Precision Floating-Point Weights During Test Run		TernaryConnect (Lin et al. (2015))	1.15%	2.42%	12.01%Maxout Networks (Goodfellow et al. (2013))	0.94%	2.47%	11.68%Network in Network (Lin et al. (2013))	一	2.35%	10.41%Gated pooling (Lee et al. (2015))	一	1.69%	7.62%Sparsely-Connected + BinaryConnect	0.99%	2.003846%	8.05%
Table 6: ASIC Implementation Results for a Single Neuron of Sparsely-Connected Network @ 400MHz in TSMC 65 nm CMOS Technology._______________________________________________________	Sparsity Degree					p=0	P = 0.5	p=0.75	p = 0.875	p = 0.9375	Fully-Connected (FC)	Sparsely-Connected	Sparsely-Connected	Sparsely-Connected	Sparsely-ConnectedMemory Size [bits]	1024	512	256	128	64Area [μm2] (improvement w.r.t. FC)	26265	13859 (47% D	7316 (72% D	4221 (84% J)	2662 (90% J)Power[μW]	278	155	86	60	43Energy [pJ] (improvement w.r.t. FC)	712	397 (44% D	220(69% J)	154 (78% J)	110 (84% J)Latency [μs]	2.56	2.56	2.56	2.56	2.56and the compressed matrix Wc stored in on-chip memories is「W21 W22]W _ W31 W52Wc = W41 W62	.
