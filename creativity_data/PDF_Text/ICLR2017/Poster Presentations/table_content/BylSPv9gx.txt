Table 1: Hyper-Parameters used for determining threshold ()HYPER-PARAM	DESCRIPTION	HEURISTIC VALUESstart _itr	Iteration to start pruning	Start of second epochrampdtr	Iteration to increase the rate of pruning	Start of 25% of total epochsenddtr	Iteration to stop pruning more pa- rameters	Start of 50% of total epochsStart _SloPe (θ)	Initial slope to prune the Weights	See equation 1ramp slope (φ)	Ramp slope to change the rate of pruning	1.5θ to 2θfreq	Number of iterations after Which is updated	100for recurrent networks (Lu et al., 2016). Another approach for pruning feed forward neural networksfor speech recognition is using simple threshold to prune all weights (Yu et al., 2012) at a particularepoch. However, we find that gradual pruning produces better results than hard pruning.
Table 2: Deep Speech 2 architecture with 1760 hidden unitsLAYER ID	TYPE	# PARAMSlayer 0	2D Convolution	19616layer 1	2D Convolution	239168layer 2	Bidirectional Recurrent Linear	8507840layer 3	Bidirectional Recurrent Linear	9296320layer 4	Bidirectional Recurrent Linear	9296320layer 5	Bidirectional Recurrent Linear	9296320layer 6	Bidirectional Recurrent Linear	9296320layer 7	Bidirectional Recurrent Linear	9296320layer 8	Bidirectional Recurrent Linear	9296320layer 9	FullyConnected	3101120layer 10	CTCCost	95054number of epochs as the gradual pruning experiments. These hard threshold results are comparedwith the RNN Sparse 1760 model in Table 3. For approximately same number of parameters, gradualpruning is 7% to 9% better than hard pruning.
Table 3: GRU & bidirectional RNN model resultsMODEL	# UNITS	CER	# PARAMS	RELATIVE PERFRNN Dense Baseline	1760	10.67	67 million	0.0%RNN Dense Small	704	14.50	11.6 million	-35.89%RNN Dense Medium	2560	9.43	141 million	11.85%RNN Sparse 1760	1760	12.88	8.3 million	-20.71%RNN Sparse Medium	2560	10.59	11.1 million	0.75%RNN Sparse Big	3072	10.25	16.7 million	3.95%GRU Dense	2560	9.55	115 million	0.0%GRU Sparse	2560	10.87	13 million	-13.82%GRU Sparse Medium	3568	9.76	17.8 million	-2.20%Table 4: RNN dense baseline model with hard pruning# UNITS	PRUNED EPOCH	CER	# PARAMS	RELATIVE PERF1760	5	13.82	8 million	-29.52%1760	7	13.27	11 million	-24.37%1760	10	13.41	8.4 million	-25.68%1760	12	13.63	8 million	-27.74%1760	15	26.33	9.2 million	-146.77%4.2	Gated Recurrent UnitsWe also experimented with GRU models shown in Table 5, that have 2560 hidden units in the GRU
Table 4: RNN dense baseline model with hard pruning# UNITS	PRUNED EPOCH	CER	# PARAMS	RELATIVE PERF1760	5	13.82	8 million	-29.52%1760	7	13.27	11 million	-24.37%1760	10	13.41	8.4 million	-25.68%1760	12	13.63	8 million	-27.74%1760	15	26.33	9.2 million	-146.77%4.2	Gated Recurrent UnitsWe also experimented with GRU models shown in Table 5, that have 2560 hidden units in the GRUlayer and a total of 115 million parameters. For these experiments, we prune all layers except theconvolution layers since they have relatively fewer parameters.
Table 5: Gated recurrent units modelLAYER ID	TYPE	# PARAMSlayer 0	2D Convolution	19616layer 1	2D Convolution	239168layer 2	Gated Recurrent Linear	29752320layer 3	Gated Recurrent Linear	39336960layer 4	Gated Recurrent Linear	39336960layer 5	Row Convolution	107520layer 6	FullyConnected	6558720layer 7	CTCCost	74269Figure 1b compares the training and dev curves of a sparse GRU model a dense GRU model. Thesparse GRU model has a 13.8% drop in the accuracy relative to the dense model. As shown inTable 3, the sparse model has an overall sparsity of 88.6% with 13 million parameters. Similarto the RNN models, we train a sparse GRU model with 3568 hidden units. The dataset and thehyperparameters are not changed from the previous GRU experiments. This model has an overallsparsity of 91.82% with 17.8 million parameters. As shown in Table 3, the model with 3568 hiddenunits is only 2.2% worse than the baseline dense GRU model. We expect to match the performanceof the GRU dense network by slightly lowering the sparsity of this network or by increasing thehidden units for the layers.
Table 6: GEMM times for recurrent layers with different sparsityLAYER SIZE	SPARSITY	LAYER TYPE	TIME (μsec)	SPEEDUP1760	0%	RNN	56	11760	95%	RNN	20	2.82560	95%	RNN	29	1.933072	95%	RNN	48	1.162560	0%	GRU	313	12560	95%	GRU	46	6.803568	95%	GRU	89	3.5(a)Figure 2: Pruning characteristics. Figure 2a plots sparsity of recurrent layers in the network withthe same hyper-parameters used for pruning . Figure 2b plots the pruning schedule of a single layerduring a training run.
