Table 1: A paragraph from Wikipedia and three associated questions together with their answers,taken from the SQuAD dataset. The tokens in bold in the paragraph are our predicted answers whilethe texts next to the questions are the ground truth answers.
Table 2: Experiment Results on SQuAD and MSMARCO datasets. Here “LSTM with Ans-Ptr”removes the attention mechanism in match-LSTM (mLSTM) by using the final state of the LSTMfor the question to replace the weighted sum of all the states. Our best boundary model is the furthertuned model and its ablation study is shown in Table 4. “en” refers to ensemble method.
Table 3: Statistical analysis on thedevelopment datasets. #w: num-ber of words on average; P: passage;Q: question; A: answer; raw: rawdata from the development dataset;seq/bou: the answers generated bythe sequence/boundary models withmatch-LSTM.
Table 4: Ablation study for our best boundary model onthe development datasets. Our best model is a further tunedboundary model by considering “bi-Ans-Ptr” which adds bi-directional answer pointer, “deep” which adds another two-layer bi-directional LSTMs between the match-LSTM andthe Answer Pointer layers, and “elem” which adds element-wise comparison ,(hp -Hqa∣) and (hp ΘHqɑ∣), into Eqn 3.
Table 5: Prediction samples for sequence and boundary models. The first case is sampled fromSQuAD dataset and the second is sampled from MSMARCO dataset.
Table 6: Different types of reasoning samples in SQuAD dataset. “match-LSTM” refers to the“match-LSTM with Ans-Ptr” and “LSTM” refers to the “LSTM with Ans-Ptr” which is the ablationof attention mechanism in match-LSTM.
