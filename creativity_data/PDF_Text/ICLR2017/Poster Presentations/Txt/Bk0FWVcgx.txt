Published as a conference paper at ICLR 2017
Topology and Geometry of Half-Rectified
Network Optimization
C. Daniel Freeman
Department of Physics
University of California at Berkeley
Berkeley, CA 94720, USA
daniel.freeman@berkeley.edu
Joan Bruna *
Courant Institute of Mathematical Sciences
New York University
New York, NY 10011, USA
bruna@cims.nyu.edu
Ab stract
The loss surface of deep neural networks has recently attracted interest in the
optimization and machine learning communities as a prime example of high-
dimensional non-convex problem. Some insights were recently gained using spin
glass models and mean-field approximations, but at the expense of strongly sim-
plifying the nonlinear nature of the model.
In this work, we do not make any such assumption and study conditions on the data
distribution and model architecture that prevent the existence of bad local minima.
Our theoretical work quantifies and formalizes two important folklore facts: (i) the
landscape of deep linear networks has a radically different topology from that of
deep half-rectified ones, and (ii) that the energy landscape in the non-linear case
is fundamentally controlled by the interplay between the smoothness of the data
distribution and model over-parametrization. Our main theoretical contribution
is to prove that half-rectified single layer networks are asymptotically connected,
and we provide explicit bounds that reveal the aforementioned interplay.
The conditioning of gradient descent is the next challenge we address. We study
this question through the geometry of the level sets, and we introduce an algo-
rithm to efficiently estimate the regularity of such sets on large-scale networks.
Our empirical results show that these level sets remain connected throughout all
the learning phase, suggesting a near convex behavior, but they become exponen-
tially more curvy as the energy level decays, in accordance to what is observed in
practice with very low curvature attractors.
1	Introduction
Optimization is a critical component in deep learning, governing its success in different areas of
computer vision, speech processing and natural language processing. The prevalent optimization
strategy is Stochastic Gradient Descent, invented by Robbins and Munro in the 50s. The empirical
performance of SGD on these models is better than one could expect in generic, arbitrary non-convex
loss surfaces, often aided by modifications yielding significant speedups Duchi et al. (2011); Hinton
et al. (2012); Ioffe & Szegedy (2015); Kingma & Ba (2014). This raises a number of theoretical
questions as to why neural network optimization does not suffer in practice from poor local minima.
The loss surface of deep neural networks has recently attracted interest in the optimization and ma-
chine learning communities as a paradigmatic example of a hard, high-dimensional, non-convex
problem. Recent work has explored models from statistical physics such as spin glasses Choroman-
ska et al. (2015), in order to understand the macroscopic properties of the system, but at the expense
of strongly simplifying the nonlinear nature of the model. Other authors have advocated that the real
danger in high-dimensional setups are saddle points rather than poor local minima Dauphin et al.
(2014), although recent results rigorously establish that gradient descent does not get stuck on saddle
points Lee et al. (2016) but merely slowed down. Other notable recent contributions are Kawaguchi
(2016), which further develops the spin-glass connection from Choromanska et al. (2015) and re-
solves the linear case by showing that no poor local minima exist; Sagun et al. (2014) which also
* Currently on leave from UC Berkeley.
1
Published as a conference paper at ICLR 2017
discusses the impact of stochastic vs plain gradient, Soudry & Carmon (2016), that studies Empir-
ical Risk Minimization for piecewise multilayer neural networks under overparametrization (which
needs to grow with the amount of available data), and Goodfellow et al. (2014), which provided in-
sightful intuitions on the loss surface of large deep learning models and partly motivated our work.
Additionally, the work Safran & Shamir (2015) studies some topological properties of homogeneous
nonlinear networks and shows how overparametrization acts upon these properties, and the pioneer-
ing Shamir (2016) studied the distribution-specific hardness of optimizing non-convex objectives.
Lastly, several papers submitted concurrently and independently of this one deserve note, particu-
larly Swirszcz et al. (2016) which analyzes the explicit criteria under which sigmoid-based neural
networks become trapped by poor local minima, as well as Tian (2017), which offers a complemen-
tary study of two layer ReLU based networks, and their learning dynamics.
In this work, we do not make any linearity assumption and study conditions on the data distribution
and model architecture that prevent the existence of bad local minima. The loss surface F (θ) of
a given model can be expressed in terms of its level sets Ωλ, which contain for each energy level
λ all parameters θ yielding a loss smaller or equal than λ. A first question we address concerns
the topology of these level sets, i.e. under which conditions they are connected. Connected level
sets imply that one can always find a descent direction at each energy level, and therefore that no
poor local minima can exist. In absence of nonlinearities, deep (linear) networks have connected
level sets Kawaguchi (2016). We first generalize this result to include ridge regression (in the two
layer case) and provide an alternative, more direct proof of the general case. We then move to the
half-rectified case and show that the topology is intrinsically different and clearly dependent on the
interplay between data distribution and model architecture. Our main theoretical contribution is to
prove that half-rectified single layer networks are asymptotically connected, and we provide explicit
bounds that reveal the aforementioned interplay.
Beyond the question of whether the loss contains poor local minima or not, the immediate follow-up
question that determines the convergence of algorithms in practice is the local conditioning of the
loss surface. It is thus related not to the topology but to the shape or geometry of the level sets.
As the energy level decays, one expects the level sets to exhibit more complex irregular structures,
which correspond to regions where F (θ) has small curvature. In order to verify this intuition, we
introduce an efficient algorithm to estimate the geometric regularity of these level sets by approx-
imating geodesics of each level set starting at two random boundary points. Our algorithm uses
dynamic programming and can be efficiently deployed to study mid-scale CNN architectures on
MNIST, CIFAR-10 and RNN models on Penn Treebank next word prediction. Our empirical results
show that these models have a nearly convex behavior up until their lowest test errors, with a single
connected component that becomes more elongated as the energy decays. The rest of the paper is
structured as follows. Section 2 presents our theoretical results on the topological connectedness
of multilayer networks. Section 3 presents our path discovery algorithm and Section 4 covers the
numerical experiments.
2	Topology of Level Sets
Let P be a probability measure on a product space X × Y , where we assume X and Y are Euclidean
vector spaces for simplicity. Let {(xi, yi)}i be an iid sample of size L drawn from P defining the
training set. We consider the classic empirical risk minimization of the form
1L
Fe(θ) = L E kΦ(Xi;θ) - yik2 + κR(θ) ,	(1)
l=1
where Φ(x; θ) encapsulates the feature representation that uses parameters θ ∈ RS and R(θ) is a
regularization term. In a deep neural network, θ contains the weights and biases used in all layers.
For convenience, in our analysis we will also use the oracle risk minimization:
Fo(θ) = E(X,Y)〜pkΦ(X；θ) — Yk2 + κR(θ) .	(2)
Our setup considers the case where R consists on either `1 or `2 norms, as we shall describe below.
They correspond to well-known sparse and ridge regularization respectively.
2
Published as a conference paper at ICLR 2017
2.1	Poor local minima characterization from topological connectedness
We define the level set of F (θ) as
Ωf(λ) = {θ ∈ RS ; F(θ) ≤ λ} .	(3)
The first question we study is the structure of critical points of Fe(θ) and Fo (θ) when Φ is a mul-
tilayer neural network. For simplicity, we consider first a strict notion of local minima: θ ∈ RS is
a strict local minima of F if there is > 0 with F (θ0) > F(θ) for all θ0 ∈ B(θ, ) and θ0 6= θ.
In particular, we are interested to know whether Fe has local minima which are not global minima.
This question is answered by knowing whether ΩF (λ) is connected at each energy level λ:
Proposition 2.1. If Ωf (λ) is connectedfor all λ then ^very local minima of F (θ) is a global minima.
Strict local minima implies that VF(θ) = 0 and HF(θ)占 0, but avoids degenerate cases where
F is constant along a manifold intersecting θ. In that scenario, if Uθ denotes that manifold, our
reasoning immediately implies that if Ωf(λ) are connected, then for all e > 0 there exists θ0 with
dist(θ0, Uθ) ≤ and F (θ0) < F (θ). In other words, some element at the boundary ofUθ must be a
saddle point. A stronger property that eliminates the risk of gradient descent getting stuck at Uθ is
that all elements at the boundary of Uθ are saddle points. This can be guaranteed if one can show
that there exists a path connecting any θ to the lowest energy level such that F is strictly decreasing
along it.
Such degenerate cases arise in deep linear networks in absence of regularization. If θ =
(W1, . . . , WK) denotes any parameter value, with N1, . . . NK denoting the hidden layer sizes, and
Fk ∈ GL+N (R) are arbitrary elements of the general linear group of invertible Nk × Nk matrices
with positive determinant, then
Uθ ={W1F1-1,F1W2F2-1,...,FKWK; Fk ∈ GL+Nk(R)} .
In particular, Uθ has a Lie Group structure. In the half-rectified nonlinear case, the general linear
group is replaced by the Lie group of homogeneous invertible matrices Fk = diag(α1, . . . , αNk )
with αj > 0.
This proposition shows that a sufficient condition to prevent the existence of poor local minima is
having connected level sets, but this condition is not necessary: one can have isolated local minima
lying at the same energy level. This can be the case in systems that are defined up to a discrete
symmetry group, such as multilayer neural networks. However, as we shall see next, this case puts
the system in a brittle position, since one needs to be able to account for all the local minima (and
there can be exponentially many of them as the parameter dimensionality increases) and verify that
their energy is indeed equal.
2.2	The Linear Case
We first consider the particularly simple case where F is a multilayer network defined by
Φ(x; θ) = WK. ..W1x , θ = (W1,.. .,WK) .	(4)
and the ridge regression R(θ) = kθk2. This model defines a non-convex (and non-concave) loss
Fe(θ). When κ = 0, it has been shown in Saxe et al. (2013) and Kawaguchi (2016) that in this case,
every local minima is a global minima. We provide here an alternative proof of that result that uses
a somewhat simpler argument and allows for κ > 0 in the case K = 2.
Proposition 2.2. Let W1, W2, . . . , WK be weight matrices of sizes nk × nk+1, k < K, and let
Fe (θ), Fo(θ) denote the risk minimizations using Φ as in (4). Assume that nj ≥ min(n1, nK) for
j = 2 ... K 一 1. Then Ωpe(λ) (and Ωfo) is connected for all λ and all K when K = 0, and for
κ > 0 when K = 2; and therefore there are no poor local minima in these cases. Moreover, any θ
can be connected to the lowest energy level with a strictly decreasing path.
Let us highlight that this result is slightly complementary than that of Kawaguchi (2016), Theorem
2.3. Whereas we require nj ≥ min(n1, nK) for j = 2 . . . K 一 1 and our analysis does not inform
about the order of the saddle points, we do not need full rank assumptions on ΣX nor the weights
Wk.
3
Published as a conference paper at ICLR 2017
This result does also highlight a certain mismatch between the picture of having no poor local min-
ima and generalization error. Incorporating regularization drastically changes the topology, and the
fact that we are able to show connectedness only in the two-layer case with ridge regression is pro-
found; we conjecture that extending it to deeper models requires a different regularization, perhaps
using more general atomic norms Bach (2013). But we now move our interest to the nonlinear case,
which is more relevant to our purposes.
2.3	Half-Rectified Nonlinear Case
We now study the setting given by
Φ(x; θ) = WKρWK-1ρ . . . ρW1x , θ = (W1, . . . , WK) ,	(5)
where ρ(z) = max(0, z). The biases can be implemented by replacing the input vector x with
X = (x, 1) and by rebranding each parameter matrix as
Wi=( jWH¼),
where bi contains the biases for each layer. For simplicity, we continue to use Wi and x in the
following.
2.3.1	Nonlinear models are generally disconnected
One may wonder whether the same phenomena of global connectedness also holds in the half-
rectified case. A simple motivating counterexample shows that this is not the case in general. Con-
sider a simple setup with X ∈ R2 drawn from a mixture of two Gaussians N-1 and N1, and let
Y =(X - μz) ∙ Z , where Z is the (hidden) mixture component taking {1, -1} values. Let
Y = Φ(X; {Wι, W2}) be a single-hidden layer ReLU network, with two hidden units. Let θA be
a configuration that bisects the two mixture components, and let θB the same configuration, but
swapping the bisectrices. One can verify that they can both achieve arbitrarily small risk by letting
the covariance of the mixture components go to 0. However, any path that connects θA to θB must
necessarily pass through a point in which W1 has rank 1, which leads to an estimator with risk at
least 1/2.
In fact, it is easy to see that this counter-example can be extended to any generic half-rectified ar-
chitecture, if one is allowed to adversarially design a data distribution. For any given Φ(X; θ) with
arbitrary architecture and current parameters θ = (Wi), let Pθ = {A1, . . . , AS} be the underly-
ing tessellation of the input space given by our current choice of parameters; that is, Φ(X; θ) is
piece-wise linear and Pθ contains those pieces. Now let X be any arbitrary distribution with density
p(x) > 0 for all x ∈ Rn, for example a Gaussian, and let Y | X =d Φ(X; θ) . Since Φ is invariant
under a subgroup of permutations θσ of its hidden layers, it is easy to see that one can find two pa-
rameter values θA = θ and θB = θσ such that Fo(θA) = Fo(θB) = 0, but any continuous path γ(t)
from θA to θB will have a different tessellation and therefore won’t satisfy Fo(γ(t)) = 0. Moreover,
one can build on this counter-example to show that not only the level sets are disconnected, but also
that there exist poor local minima. Let θ0 be a different set of parameters, and Y0 | X =d Φ(X; θ0)
be a different target distribution. Now consider the data distribution given by the mixture
X | P(X) , Z 〜BemoUUi(π) , Y | X,z = zΦ(X; θ) + (1 - z)Φ(X; θ0).
By adjusting the mixture component π we can clearly change the risk at θ and θ0 and make them
different, but we conjecture that this preserves the status of local minima of θ and θ0 . Appendix E
constructs a counter-example numerically.
This illustrates an intrinsic difficulty in the optimization landscape if one is after universal guarantees
that do not depend upon the data distribution. This difficulty is non-existent in the linear case and
not easy to exploit in mean-field approaches such as Choromanska et al. (2015), and shows that
in general we should not expect to obtain connected level sets. However, connectedness can be
recovered if one is willing to accept a small increase of energy and make some assumptions on the
complexity of the regression task. Our main result shows that the amount by which the energy is
allowed to increase is upper bounded by a quantity that trades-off model overparametrization and
smoothness in the data distribution.
4
Published as a conference paper at ICLR 2017
For that purpose, we start with a characterization of the oracle loss, and for simplicity let us assume
Y ∈ R and let us first consider the case with a single hidden layer and `1 regularization: R(θ) =
kθk1.
2.3.2 Preliminaries
Before proving our main result, we need to introduce preliminary notation and results. We first
describe the case with a single hidden layer of size m.
We define
e(m) =	min	E{∣Φ(X; θ) - Y|2} + κkW2k1 .
W1∈Rm×n,kW1(i)k2≤1,W2∈Rm
(6)
to be the oracle risk using m hidden units with norm ≤ 1 and using sparse regression. It is a well
known result by Hornik and Cybenko that a single hidden layer is a universal approximator under
very mild assumptions, i.e. limm→∞ e(m) = 0. This result merely states that our statistical setup is
consistent, and it should not be surprising to the reader familiar with classic approximation theory.
A more interesting question is the rate at which e(m) decays, which depends on the smoothness of
the joint density (X, Y) 〜P relative to the nonlinear activation family We have chosen.
For convenience, we redefine W = W1 and β = W2 and Z(W) = max(0, WX). We also write
z(w) = max(0, hw, X))where (X, Y)〜 P and W ∈ RN is any deterministic vector. Let Σχ =
EPXXT ∈ RN×N be the covariance operator of the random input X. We assume kΣX k < ∞.
A fundamental property that will be essential to our analysis is that, despite the fact that Z is
nonlinear, the quantity [w1, w2]Z := EP {z(w1)z(w2)} is locally equivalent to the linear metric
hw1, w2iX = EP{w1TXXTw2} = hw1, ΣXw2i, and that the linearization error decreases with the
angle between w1 and w2. Without loss of generality, we assume here that kw1k = kw2 k = 1, and
we write kwk2Z = E{|z(w)|2}.
Proposition 2.3. Let α = cos-1(hw1, w2i) be the angle between unitary vectors w1 and w2 and let
Wm = kW1+W2k be their unitary bisector. Then
1+20sα IlWmIlZ - 2k∑xk (I-Iosa + sin2 α) ≤ [w1,w2]z ≤ 匕产 IlWmIlZ .⑺
The term kΣX k is overly pessimistic: we can replace it by the energy of X projected into the
subspace spanned by W1 and W2 (which is bounded by 2IΣX I). When α is small, a Taylor expansion
of the trigonometric terms reveals that
2	2	2	α2
3PXΓ hwi2i = Ecos α = E(I- E+O(a))
≤ (I - a2/4) IIWmllZ -∣∣ςX k(a2/4 + α2) + O(a4)
≤	[W1,W2]Z + O(α4) ,
and similarly
[W1, W2]Z ≤ hW1, W2iIWmI2Z ≤ IΣXIhW1,W2i .
The local behavior of parameters W1, W2 on our regression problem is thus equivalent to that of hav-
ing a linear layer, provided W1 and W2 are sufficiently close to each other. This result can be seen as
a spoiler of what is coming: increasing the hidden layer dimensionality m will increase the chances
to encounter pairs of vectors W1 , W2 with small angle; and with it some hope of approximating the
previous linear behavior thanks to the small linearization error.
In order to control the connectedness, we need a last definition. Given a hidden layer of size m with
current parameters W ∈ Rn×m , we define a “robust compressibility” factor as
δw(l,a; m) =	min	E{∣Y 一 YZ(W)∣2 + κ∣∣γ∣∣ι} , (l ≤ m) .	(8)
kγko≤l,suPi ∣∠(Wi,w∕≤α
This quantity thus measures how easily one can compress the current hidden layer representation,
by keeping only a subset of l its units, but allowing these units to move by a small amount controlled
byα. It is a form of n-width similar to Kolmogorov width Donoho (2006) and is also related to
robust sparse coding from Tang et al. (2013); Ekanadham et al. (2011).
5
Published as a conference paper at ICLR 2017
2.3.3 Main result
Our main result considers now a non-asymptotic scenario given by some fixed size m of the hid-
den layer. Given two parameter values θA = (W1A, W2A) ∈ W and θB = (W1B , W2B) with
Fo(θ{A,B}) ≤ λ, we show that there exists a continuous path γ : [0, 1] → W connecting θA
and θB such that its oracle risk is uniformly bounded by max(λ, ), where decreases with model
overparametrization.
Theorem 2.4. For any θA, θB ∈ W andλ ∈ R satisfying Fo(θ{A,B}) ≤ λ, there exists a continuous
path γ : [0, 1] → W such that γ(0) = θA, γ(1) = θB and
Fo(γ(t)) ≤ max(λ, ) , with	(9)
= inf max e(l),δW A (m, 0; m), δWA (m - l, α; m),
δWB (m, 0; m), δWB (m - l, α; m) + C1α + O(α2) ,
where C1 is an absolute constant depending only on κ and P.
(10)
(11)
Some remarks are in order. First, our regularization term is currently a mix between `2 norm con-
straints on the first layer and `1 norm constraints on the second layer. We believe this is an artifact of
our proof technique, and we conjecture that more general regularizations yield similar results. Next,
this result uses the data distribution through the oracle bound e(m) and the covariance term. The
extension to empirical risk is accomplished by replacing the probability measure P by the empirical
measure P = L P1 δ ((χ,y) - (χι, yι)). However, our asymptotic analysis has to be carefully re-
examined to take into account and avoid the trivial regime when M outgrows L. A consequence of
Theorem 2.4 is that as m increases, the model becomes asymptotically connected, as proven in the
following corollary.
Corollary 2.5. As m increases, the energy gap E satisfies E = O(m- 1) and therefore the level sets
become connected at all energy levels.
This is consistent with the overparametrization results from Safran & Shamir (2015); Shamir (2016)
and the general common knowledge amongst deep learning practitioners. Our next sections ex-
plore this question, and refine it by considering not only topological properties but also some rough
geometrical measure of the level sets.
3	Geometry of Level Sets
3.1	The Greedy Algorithm
The intuition behind our main result is that, for smooth enough loss functions and for sufficient
overparameterization, it should be “easy” to connect two equally powerful models—i.e., two models
with FoθA,B ≤ λ. A sensible measure of this ease-of-connectedness is the normalized length
of the geodesic connecting one model to the other: ∣ya,b(t)∣∕∣θA - Θb|. This length represents
approximately how far of an excursion one must make in the space of models relative to the euclidean
distance between a pair of models. Thus, convex models have a geodesic length of 1, because
the geodesic is simply linear interpolation between models, while more non-convex models have
geodesic lengths strictly larger than 1.
Because calculating the exact geodesic is difficult, we approximate the geodesic paths via a dynamic
programming approach we call Dynamic String Sampling. We comment on alternative algorithms
in Appendix A.
For a pair of models with network parameters θi, θj, each with Fe(θ) below a threshold L0, we aim
to efficienly generate paths in the space of weights where the empirical loss along the path remains
below Lo. These paths are continuous curves belonging to Ωf(λ)-that is, the level sets of the loss
function of interest.
6
Published as a conference paper at ICLR 2017
Algorithm 1 Greedy Dynamic String Sampling
1： Lo — Threshold below which path will be found
2： Φι — randomly initialize θι, train Φ(xi θι) to Lo
3： Φ2 — randomly initialize θ2, train Φ(xi θ2) to Lo
4:	BeadList .(Φι, Φ2)
5:	Depth . 0
6： procedure findconnection(Φ1 , Φ2)
7:	t* . t such that dγ(θ1 ,θ2 ㈤ I = 0 OR t = 0.5
dt	.
t
8:	Φ3 . train Φ(xi; t* θ1 + (1 - t* )θ2 ) to Lo
9:	beadlist . insert(Φ3, after Φ1 , beadlist)
10:	MaxError1 . maxt(Fe (tθ3 + (1 - t)θ1 ))
11:	MaxError2 . maxt(Fe (tθ2 + (1 - t)θ3 ))
12:	if MaxError1 > Lo then return findconnection(Φ1 ,	Φ3 )
13:	if MaxError2 > Lo then return findconnection(Φ3 ,	Φ2 )
14:	depth . depth+1
The algorithm recursively builds a string of models in the space of weights which continuously
connect θi to θj . Models are added and trained until the pairwise linearly interpolated loss, i.e.
maxtFe(tθi + (1 - t)θj) for t ∈ (0, 1), is below the threshold, L0, for every pair of neighboring
models on the string. We provide a cartoon of the algorithm in Appendix C.
3.2 Failure Conditions and Practicalities
While the algorithm presented will faithfully certify two models are connected if the algorithm
converges, it is worth emphasizing that the algorithm does not guarantee that two models are dis-
connected if the algorithm fails to converge. In general, the problem of determining if two models
are connected can be made arbitrarily difficult by choice of a particularly pathological geometry for
the loss function, so we are constrained to heuristic arguments for determining when to stop run-
ning the algorithm. Thankfully, in practice, loss function geometries for problems of interest are not
intractably difficult to explore. We comment more on diagnosing disconnections more carefully in
Appendix E.
Further, if the MaxError exceeds L0 for every new recursive branch as the algorithm progresses,
the worst case runtime scales as O(exp(Depth)). Empirically, we find that the number of new
models added at each depth does grow, but eventually saturates, and falls for a wide variety of
models and architectures, so that the typical runtime is closer to O (poly(Depth))—at least up
until a critical value of L0 .
To aid convergence, either of the choices in line 7 of the algorithm works in practice-choosing t* at
a local maximum can provide a modest increase in algorithm runtime, but can be unstable if the the
calculated interpolated loss is particularly flat or noisy. t* = .5 is more stable, but slower. Finally,
we find that training Φ3 to αL0 for α < 1 in line 8 of the algorithm tends to aid convergence without
noticeably impacting our numerics. We provide further implementation details in 4.
4 Numerical Experiments
For our numerical experiments, we calculated normalized geodesic lengths for a variety of regression
and classification tasks. In practice, this involved training a pair of randomly initialized models to
the desired test loss value/accuracy/perplexity, and then attempting to connect that pair of models via
the Dynamic String Sampling algorithm. We also tabulated the average number of “beads”, or the
number intermediate models needed by the algorithm to connect two initial models. For all of the
below experiments, the reported losses and accuracies are on a restricted test set. For more complete
architecture and implementation details, see our GitHub page.
The results are broadly organized by increasing model complexity and task difficulty, from easiest
to hardest. Throughout, and remarkably, we were able to easily connect models for every dataset
and architecture investigated except the one explicitly constructed counterexample discussed in Ap-
pendix E.1. Qualitatively, all of the models exhibit a transition from a highly convex regime at high
loss to a non-convex regime at low loss, as demonstrated by the growth of the normalized length as
well as the monotonic increase in the number of required “beads” to form a low-loss connection.
7
Published as a conference paper at ICLR 2017
4.1 Polynomial Regression
We studied a 1-4-4-1 fully connected multilayer perceptron style architecture with sigmoid nonlin-
earities and RMSProp/ADAM optimization. For ease-of-analysis, we restricted the training and test
data to be strictly contained in the interval x ∈ [0, 1] and f(x) ∈ [0, 1]. The number of required
beads, and thus the runtime of the algorithm, grew approximately as a power-law, as demonstrated
in Table 1 Fig. 1. We also provide a visualization of a representative connecting path between two
models of equivalent power in Appendix D.
(2a)
(1a)
(1b)
(2b)
(3a)
% error onteet set
(3b)
(4a)
50	60
% eπσr OlIteetBet
(4b)
(5a)
K» 500	60
Pwpfedty g tert art
(5b)
Figure 1: (Column a) Average normalized geodesic length and (Column b) average number of beads
versus loss. (1) A quadratic regression task. (2) A cubic regression task. (3) A convnet for MNIST.
(4) A convnet inspired by Krizhevsky for CIFAR10. (5) A RNN inspired by Zaremba for PTB next
word prediction.
The cubic regression task exhibits an interesting feature around L0 = .15 in Table 1 Fig. 2, where
the normalized length spikes, but the number of required beads remains low. Up until this point, the
8
Published as a conference paper at ICLR 2017
cubic model is strongly convex, so this first spike seems to indicate the onset of non-convex behavior
and a concomitant radical change in the geometry of the loss surface for lower loss.
4.2	Convolutional Neural Networks
To test the algorithm on larger architectures, we ran it on the MNIST hand written digit recognition
task as well as the CIFAR10 image recognition task, indicated in Table 1, Figs. 3 and 4. Again,
the data exhibits strong qualitative similarity with the previous models: normalized length remains
low until a threshold loss value, after which it grows approximately as a power law. Interestingly,
the MNIST dataset exhibits very low normalized length, even for models nearly at the state of the
art in classification power, in agreement with the folk-understanding that MNIST is highly convex
and/or “easy”. The CIFAR10 dataset, however, exhibits large non-convexity, even at the modest test
accuracy of 80%.
4.3	Recurrent Neural Networks
To gauge the generalizability of our algorithm, we also applied it to an LSTM architecture for solving
the next word prediction task on the PTB dataset, depicted in Table 1 Fig. 5. Noteably, even for a
radically different architecture, loss function, and data set, the normalized lengths produced by the
DSS algorithm recapitulate the same qualitative features seen in the above datasets—i.e., models
can be easily connected at high perplexity, and the normalized length grows at lower and lower
perplexity after a threshold value, indicating an onset of increased non-convexity of the loss surface.
5 Discussion
We have addressed the problem of characterizing the loss surface of neural networks from the per-
SPective of gradient descent algorithms. We explored two angles - topological and geometrical
aspects - that build on top of each other.
On the one hand, we have presented new theoretical results that quantify the amount of uphill climb-
ing that is required in order to progress to lower energy configurations in single hidden-layer ReLU
networks, and proved that this amount converges to zero with overparametrization under mild con-
ditions. On the other hand, we have introduced a dynamic programming algorithm that efficiently
approximates geodesics within each level set, providing a tool that not only verifies the connected-
ness of level sets, but also estimates the geometric regularity of these sets. Thanks to this informa-
tion, we can quantify how ‘non-convex’ an optimization problem is, and verify that the optimization
of quintessential deep learning tasks - CIFAR-10 and MNIST classification using CNNs, and next
word prediction using LSTMs - behaves in a nearly convex fashion up until they reach high accuracy
levels.
That said, there are some limitations to our framework. In particular, we do not address saddle-point
issues that can greatly affect the actual convergence of gradient descent methods. There are also a
number of open questions; amongst those, in the near future we shall concentrate on:
•	Extending Theorem 2.4 to the multilayer case. We believe this is within reach, since the
main analytic tool we use is that small changes in the parameters result in small changes in
the covariance structure of the features. That remains the case in the multilayer case.
•	Empirical versus Oracle Risk. A big limitation of our theory is that right now it does not
inform us on the differences between optimizing the empirical risk versus the oracle risk.
Understanding the impact of generalization error and stochastic gradient in the ability to do
small uphill climbs is an open line of research.
•	Influence of symmetry groups. Under appropriate conditions, the presence of discrete sym-
metry groups does not prevent the loss from being connected, but at the expense of increas-
ing the capacity. An important open question is whether one can improve the asymptotic
properties by relaxing connectedness to being connected up to discrete symmetry.
•	Improving numerics with Hyperplane method. Our current numerical experiments employ a
greedy (albeit faster) algorithm to discover connected components and estimate geodesics.
We plan to perform experiments using the less greedy algorithm described in Appendix A.
9
Published as a conference paper at ICLR 2017
Acknowledgments
We would like to thank Mark Tygert for pointing out the reference to the -nets and Kolmogorov
capacity, and Martin Arjovsky for spotting several bugs in early version of the results. We would
also like to thank Maithra Raghu and Jascha Sohl-Dickstein for enlightening discussions, as well as
Yasaman Bahri for helpful feedback on an early version of the manuscript. CDF was supported by
the NSF Graduate Research Fellowship under Grant DGE-1106400.
References
Francis Bach. Convex relaxations of structured matrix factorizations. arXiv preprint
arXiv:1309.3117, 2013.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Proc. AISTATS, 2015.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in Neural Information Processing Systems, pp. 2933-2941, 2014.
David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289—
1306, 2006.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Chaitanya Ekanadham, Daniel Tranchina, and Eero P Simoncelli. Recovery of sparse translation-
invariant signals with continuous basis pursuit. IEEE transactions on signal processing, 59(10):
4735-4744, 2011.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network
optimization problems. arXiv preprint arXiv:1412.6544, 2014.
Geoffrey Hinton, N Srivastava, and Kevin Swersky. Lecture 6a overview of mini-batch gradient
descent. Coursera Class, 2012.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Kenji Kawaguchi. Deep learning without poor local minima. arXiv preprint arXiv:1605.07110,
2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges
to minimizers. University of California, Berkeley, 1050:16, 2016.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks.
arXiv preprint arXiv:1511.04210, 2015.
Levent Sagun, V Ugur Guney, Gerard Ben Arous, and Yann LeCun. Explorations on high dimen-
sional landscapes. arXiv preprint arXiv:1412.6615, 2014.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Ohad Shamir. Distribution-specific hardness of learning neural networks. arXiv:1609.01037, 2016.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of
neural networks. arXiv preprint arXiv:1611.06310, 2016.
10
Published as a conference paper at ICLR 2017
Gongguo Tang, Badri Narayan Bhaskar, Parikshit Shah, and Benjamin Recht. Compressed sensing
off the grid. IEEE Transactions on Information Theory, 59(11):7465-7490, 2013.
Yuandong Tian. Symmetry-breaking convergence analysis of certain two-layered neural networks
with relu nonlinearity. ICLR Workshop 2017, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
A Constrained Dynamic String S ampling
While the algorithm presented in Sec. 3.1 is fast for sufficiently smooth families of loss surfaces
with few saddle points, here we present a slightly modified version which, while slower, provides
more control over the convergence of the string. We did not use the algorithm presented in this
section for our numerical studies.
Instead of training intermediate models via full SGD to a desired accuracy as in step 8 of the al-
gorithm, intermediate models are be subject to a constraint that ensures they are “close” to the
neighboring models on the string. Specifically, intermediate models are constrained to the unique
hyperplane in weightspace equidistant from its two neighbors. This can be further modified by ad-
ditional regularization terms to control the “springy-ness” of the string. These heuristics could be
chosen to try to more faithfully sample the geodesic between two models.
In practice, for a given model on the string, θi, these two regularizations augment the standard loss
by: F(θ) = F (θ)+Z (kθi-ι- θik+kθi+ι- θik)+Kk k(θi-ι-θi+ι)∕2k ∙ k(θi-(θi-ι-θi+ι)∕2)k k.The
ζ regularization term controls the “springy-ness” of the weightstring, and the κ regularization term
controls how far off the hyperplane a new model can deviate.
Because adapting DSS to use this constraint is straightforward, here we will describe an alternative
“breadth-first” approach wherein models are trained in parallel until convergence. This alternative
approach has the advantage that it will indicate a disconnection between two models “sooner” in
training. The precise geometry of the loss surface will dictate which approach to use in practice.
Given two random models σ% and σ7- where 氏 一 σ/ < L0, We aim to follow the evolution of
the family of models connecting σi to σj . Intuitively, almost every continuous path in the space of
random models connecting σi to σj has, on average, the same (high) loss. For simplicity, we choose
to initialize the string to the linear segment interpolating between these two models. If this entire
segment is evolved via gradient descent, the segment will either evolve into a string which is entirely
contained in a basin of the loss surface, or some number of points will become fixed at a higher loss.
These fixed points are difficult to detect directly, but will be indirectly detected by the persistence of
a large interpolated loss between two adjacent models on the string.
The algorithm proceeds as follows:
(0.) Initialize model string to have two models, σi and σj .
1.	Begin training all models to the desired loss, keeping the instantaneous loss, L0(t), of all models
being trained approximately constant.
2.	If the pairwise interpolated loss between σn and σn+1 exceeds L0(t), insert a new model at the
maximum of the interpolated loss (or halfway) between these two models.
3.	Repeat steps (1) and (2) until all models (and interpolated errors) are below a threshold loss
L0(tfinal) := L0, or until a chosen failure condition (see 3.2).
B Proofs
B.1	Proof of Proposition 2.1
Suppose that θ1 is a local minima and θ2 is a global minima, but F (θ1) > F (θ2). If λ = F (θ1),
then clearly θι and θ2 both belong to Ωf(λ). Suppose now that Ωf(λ) is connected. Then we
11
Published as a conference paper at ICLR 2017
could find a smooth (i.e. continuous and differentiable) path γ(t) with γ(0) = θ1, γ(1) = θ2 and
F(γ(t)) ≤ λ = F(θι). But this contradicts the strict local minima status of θι, and therefore Ωf(λ)
cannot be connected .
B.2	Proof of Proposition 2.2
Let us first consider the case with κ = 0. We proceed by induction over the number of layers K.
For K = 1, the loss F(θ) is convex. Let θA, θB be two arbitrary points in a level set Ωλ. Thus
F(θA) ≤ λ and F(θB) ≤ λ. By definition of convexity, a linear path is sufficient in that case to
connect θA and θB:
F((1 - t)θA + tθB) ≤ (1 -t)F(θA)+tF(θB) ≤ λ.
Suppose the result is true for K - 1. Let θA = (W1A, . . . , WKA) and θB = (W1B , . . . , WKB) with
F(θA) ≤ λ, F(θB) ≤ λ. Since n7- ≥ min(nι,nκ) for j = 2 ...K — 1, we can find k* =
r 1 T7 1 T	1.1	、♦/	∖	-ɪ-,	1 TTT-	TTT-	1	. TTT-	TTT- Γ∙
{1, K — 1} such that nk* ≥ mm(nk*-ι,nk*+ι). For each Wi,..., WK, we denote Wj = Wj for
j = k*,k* — 1 and Wk* = Wk*-ιWk*. By induction hypothesis, the loss expressed in terms of
θ = (W1, . . . , WK-1) is connected between θA and θB. Let Wk* (t) the corresponding linear path
projected in the layer k*. We need to produce a path in the variables Wk*-i (t), Wk* (t) such that:
i Wk*-1(0) = WkA*-1, Wk*-1(1) =WkB*-1,
ii	Wk*(0) =WkA*,Wk*(1) =WkB*,
iii	Wk* (t)Wk*-ι(t) = Wk*-ι(t) for t ∈ (0,1).
We construct it as follows. Let
Wk*(t) = tWkB* +(1 — t)WkA* +t(1 —t)V ,
Wk*-i(t) = Wk*(t)tWk*-i(t),
where Wk*(t)t = (Wk*(t)T Wk* (t))-1Wk* (t)T denotes the pseudoinverse and V is a nk*-ι X nk*
matrix drawn from a iid distribution. Conditions (i) and (ii) are immediate from the definition, and
condition (iii) results from the fact that
Wk*(t)Wk*(t)t = INk* ,
since Wk*(t) has full rank for all t ∈ (0, 1).
Finally, let us prove that the result is also true when K = 2 and κ > 0. We construct the path
using the variational properties of atomic norms Bach (2013). When we pick the ridge regression
regularization, the corresponding atomic norm is the nuclear norm:
kχk* = UminX 2(ku『+kv k2).
The path is constructed by exploiting the convexity of the variational norm kXk*. Let θA =
(WA,RA) and θB = (WB,WB), and we define W = W1W2. Since W{A，B}=
W1{A,B}W2{A,B}, it results that
kW{A,B}∣∣* ≤ 2(kW{A,B}k2 + kW产B}k2).	(12)
From (12) it results that the loss Fo(W1, W2) can be minored by another loss expressed in terms of
W of the form
E{∣Y — WX|2} + 2κ∣∣Wk* ,
which is convex with respect to W. Thus a linear path in W from WA to WB is guaranteed to be
below Fo(θ{A,B} ). Let us define
∀t , W1(t),W2(t) = arg min (kU k2 + kVk2) .
UV t=W(t)
One can verify that we can first consider a path (β1A(s), β2A(s)) from (W1A, W2A) to (W1(0), W2(0)
such that
∀ S β1(s)β2(s) = WA and kβι(s)k2 + kβ2(s)∣∣2 decreases,
12
Published as a conference paper at ICLR 2017
and similarly for (W1B, W2B) to (W1(1), W2(1). The path (β{A1,2}(s), W{1,2}(t), β{B1,2}(s)) satisfies
(i-iii) by definition. We also verify that
kWι(t)k2 + kW2(t)k2 = 2∣W(t)k*
... ~ . ..	.. ~ ...
≤ 2(1- t)kW(0)k* + 2tkW(1)k*
≤	(1 - t)(kW k12(0) + kW k22(0)) + t(kW k12(1) + kW k22(1)) .
Finally, We verify that the paths We have just created, when applied to θA arbitrary and θB = θ* a
global minimum, are strictly decreasing, again by induction. For K = 1, this is again an immediate
consequence of convexity. For K > 1, our inductive construction guarantees that for any 0 < t < 1,
the path θ(t) = (Wk(t))k≤K satisfies Fo(θ(t)) < Fo(θA). This concludes the proof .
B.3 Proof of Proposition 2.3
Let
A(w1, w2) = {x ∈ Rn; hx, w1i ≥ 0 , hx, w2i ≥ 0} .
By definition, We have
hw1,w2iZ
E{max(0, hX, w1i) max(0, hX, w2i)}
/
A(w1,w2)
hx, w1 ihx, w2 idP (x) ,
/
Q(A(w1,w2))
hQ(χ),w1ihQ(χ),w2i(dP(Q(χ))),
(13)
(14)
(15)
where Q is the orthogonal projection onto the space spanned by wι and w2 and dP(χ) = dP(χι, χ2)
is the marginal density on that subspace. Since this projection does not interfere With the rest of the
proof, we abuse notation by dropping the Q and still referring to dP (x) as the probability density.
Now, let r = ɪ∣∣wι + w2 ∣∣ = 1+c0Sg) and d = w2-w1. By construction we have
w1 = rwm - d , w2 = rwm + d ,
and thus
hx, w1ihx, w2i = r2|hx, wmi|2 - |hx, di|2 .	(16)
By denoting C(wm) = {x ∈ Rn; hx, wmi ≥ 0}, observe that A(w1, w2) ⊆ C(wm). Let us denote
by B = C(wm) \ A(w1, w2) the disjoint complement. It results that
hw1, w2iZ =	hx, w1ihx, w2idP (x)
=	[r2|hx, wmi|2 - |hx,di|2]dP (x)-
C(wm)
r2	|hx, wmi|2dP (x) + |hx, di|2dP (x)
BB
r2∣wm∣2Z - r2	|hx,wmi|2
B
X--------------
*{z
E1
dP (x) -	|hx, di|2 dP (x)
A(w1,w2)
-------} 、---------------------------}
E2
(17)
We conclude by bounding each error term E1 and E2 separately:
0 ≤ E1 ≤ r2
| Sinm) 12 [
B
∣∣x∣2dP(x) ≤ r2∣ sin(α)∣22∣∑χ∣ ,
(18)
since every point in B by definition has angle greater than π∕2 — α from wm. Also,
0 ≤ E2 ≤
∣d∣2
A(w1,w2)
∣x∣2dP (x) ≤
T⅛∑X k
(19)
by direct application of Cauchy-Schwartz. The proof is completed by plugging the bounds from
(18) and (19) into (17) .
13
Published as a conference paper at ICLR 2017
B.4	Proof of Theorem 2.4
Consider a generic α and l ≤ m. A path from θA to θB will be constructed by concatenating the
following paths:
1.	from θA to θlA , the best linear predictor using the same first layer as θA ,
2.	from θlA to θsA, the best (m - l)-term approximation using perturbed atoms from θA,
3.	from θsA to θ* the oracle l term approximation,
4.	from θ* to 9§b , the best (m - l)-term approximation using perturbed atoms from θB,
5.	from θsB to θlB, the best linear predictor using the same first layer as θB,
6.	from θlB to θB .
The proof will study the increase in the loss along each subpath and aggregate the resulting increase
into a common bound.
Subpaths (1) and (6) only involve changing the parameters of the second layer while leaving the first-
layer weights fixed, which define a convex loss. Therefore a linear path is sufficient to guarantee
that the loss along that path will be upper bounded by λ on the first end and δWA (m, 0, m) on the
other end.
Concerning subpaths (3) and (4), we notice that they can also be constructed using only parameters
of the second layer, by observing that one can fit into a single n × m parameter matrix both the
(m - l)-term approximation and the oracle l-term approximation. Indeed, let us describe subpath
(3) in detail ( subpath (4) is constructed analogously by replacing the role of θsA with θsB). Let WA
the first-layer parameter matrix associated with the m - l-sparse solution θsA, and let γA denote
its second layer coefficients, which is a m-dimensional vector with at most m - l non-zero coeffi-
Cients. Let W* be the first-layer matrix of the l-term oracle approximation, and γ* the corresponding
second-layer coefficients. Since there are only m - l columns of WA that are used, corresponding
to the support of ya, We can consider a path G that replaces the remaining l columns with those
from W* while keeping the second-layer vector γA fixed. Since the modified columns correspond
to zeros in γA, such paths have constant loss. Call WG the resulting first-layer matrix, containing
both the active m -1 active columns of WA and the l columns of W* in the positions determined by
the zeros of γA. Now we can consider the linear subpath that interpolates between γA and γ* while
keeping the first layer fixed at WG . Since again this is a linear subpath that only moves second-layer
coefficients, it is non-increasing thanks to the convexity of the loss while fixing the first layer. We
easily verify that at the end of this linear subpath we are using the oracle l-term approximation,
which has loss e(l), and therefore subpath (3) incurs in a loss that is bounded by its extremal values
δWA (m - l, α, m) and e(l).
Finally, we need to show how to construct the subpaths (2) and (5), which are the most delicate
step since they cannot be bounded using convexity arguments as above. Let WA be the resulting
perturbed first-layer parameter matrix with m-l sparse coefficients γA. Let us consider an auxiliary
regression of the form
W = [WA; WA] ∈ Rn×2m .
and regression parameters
β1 = [β1M,β2 = [0; γA].
Clearly
E{∣Y - β1W∣2} + κkβ1k1 = E{∣Y - βιWA|2} + κkβ1k1
and similarly for β2. By convexity, the augmented linear path η(t) = (1 - t)β 1 + tβ2 thus satisfies
∀ t,L(t) = E{∣Y - η(t)W|2} + κkη(t)kι ≤ max(L(0),L⑴).
Let us now approximate this augmented linear path with a path in terms of first and second layer
weights. We consider
ηι(t) = (1 - t)WA + tWA , and η2(t) = (1 - t)βι + tγA .
14
Published as a conference paper at ICLR 2017
We have that
Fo({ηι(t), η2(t)}) = e{∣y - η2(t)z (η1(t))l2} + κkη2 (t)kι	(20)
≤ E{∣Y - η2(t)Z(η1(t))l2} + κ((1- t)kβ1k1 + tkYAki)
=L(t) + E{∣Y - η2(t)z (η1(t))∣2}
-E{∣Y -(1 - t)βιZ(WA)- tYAZ(Wa)∣2} .	(21)
Finally, we verify that
∣E{∣Y - η2(t)Z(η1(t))∣2} - E{∣Y -(1- t)βιZ(WA) - tγAZ(Wa)∣2}∣ ≤	(22)
≤ 4amax(E∣Y|2, ,E∣Y2∣)k∑χ∣∣(κ-1/2 + α,E∣Y2∣κ-1) + o(a2).
Indeed, from Proposition 2.3, and using the fact that
∀ i ≤ M, t ∈ [O, 1] , ∣∠((1 - t)wA + tWA； WA) I ≤ α"∠((1- t)wA + tWA； WA) ∣≤ α
we can write
(1 - t)βι,iz(wA) + tγA,iz(WA) = η2(t)iz(η1(t)i) + n ,
with E{∣n∕2} ≤ 4∣η2(t)i∣2k∑χ∣∣α2 + O(α4) and E∣n∕ ≤ 2∣η2(t)∕αP∣∣Σχk using concavity of
the moments. Thus
∣E{∣Y -η2(t)Z(ηι(t))∣2}- E{∣Y - (1 -t)βιZ(WA)- tγAZ(WA)∣2}∣
≤
≤
≤
≤
- η2(t)Z(η1(t)))ni +E | Xni|2
4 (αPEF2∣k∑xkkη2k + α2(kη2k1)2k∑χk)
4αmax(1, √E∣Y2∣)∣∑χk(kη2k1 + α∣η2∣2) + o(ɑ2)
4αmax(PE∣Y2∣, E|Y2∣)∣∑χ∣(κ-1 + α,E∣Y21κ-2) + o(α2),
which proves (22).
We have just constructed a path from θA to θB, in which all subpaths except (2) and (5) have energy
maximized at the extrema due to convexity, given respectively by λ, δW1 (m, 0, m), δW1 (m -
l, α, m), e(l), δW1 (m - l, α, m), and δW1 (m, 0, m). For the two subpaths (2) and (5), (22) shows
BB
that it is sufficient to add the corresponding upper bound to the linear subpath, which is of the form
Cα + o(α2) where C is an explicit constant independent of θ. Since l and α are arbitrary, we are
free to pick the infimum, which concludes the proof.
B.5	Proof of Corollary 2.5
Let us consider a generic first layer weight matrix W ∈ Rn×m. Without loss of generality, we can
assume that kWkk = 1 for all k, since increasing the norm of kWkk within the unit ball has no penalty
in the loss, and we can compensate this scaling in the second layer thanks to the homogeneity of
the half-rectification. Since this results in an attenuation of these second layer weights, they too are
guaranteed not to increase the loss.
From Vershynin (2010) [Lemma 5.2] we verify that the covering number N(Sn-1, ) of the Eu-
clidean unit sphere Sn-1 satisfies
N (S n-1,e) ≤ (1 + ：)”,
which means that we can cover the unit sphere with an -net of size N(Sn-1, ).
Let 0 < η < n-1(1 + n-1)-1, and let Us pick, for each m, Em = mη-n1. Let Us consider its
corresponding ：-net of size
Um = N (S n-1,Em) ' (1 + 2) ' m1-η .
Em
15
Published as a conference paper at ICLR 2017
Since we have m vectors in the unit sphere, it results from the pigeonhole principle that at least one
element of the net will be associated with at least vm = mu-m1 ' mη vectors; in other words, we
are guaranteed to find amongst our weight vector W a collection Qm of vm ' mη vectors that are
all at an angle at most 2m apart. Let us now apply Theorem 2.4 by picking n = vm and α = m .
We need to see that the terms involved in the bound all converge to 0 as m → ∞.
The contribution of the oracle error e(vm) - e(m) goes to zero as m → ∞ by the fact that
limm→∞ e(m) exists (it is a decreasing, positive sequence) and that vm → ∞.
Let us now verify that δ(m - vm , m , m) also converges to zero. We are going to prune the first
layer by removing one by one the vectors in Qm . Removing one of these vectors at a time incurs in
an error of the order of m. Indeed, let wk be one of such vectors and let β0 be the solution of
minE(β0) = min	E{∣Y - βf Z(W—k) - βkz(wk)|2} + κ(kβf ∣∣ι + 同|),
β0	β0=(βf ιβk)∈Rk
where W-k is a shorthand for the matrix containing the rest of the vectors that have not been dis-
carded yet. Removing the vector wk from the first layer increases the loss by a factor that is upper
bounded by E(βp) - E(β), where
βj0	for j < k - 1 ,
βk0 -1 + βk0	otherwise. ,
since now βp is a feasible solution for the pruned first layer.
Let us finally bound E(βp) - E(β).
Since ∠(wk, wk-1) ≤ m, it results from Proposition 2.3 that
(βp )j =
z(wk) =d z(wk-1) + n ,
with E{|n|2} ≤ Cα2 for some constant C independent ofm. By redefining p1 = Y-βpTZ(W-k) -
2 n and p = 2 n, We have
E{∣Y - βpZ(W-k)|2} - E{∣Y - β0TZ(W-k) - βkz(wk)∣2}
E{|p1 +p2|2} - E{|p1 -p2|2}
4E{|p1p2|}
≤t
EUY - βpZ(W—k) - 2n
2 ∣PE{K}
≤ (C + α)α ' m ,
Where C only depends on E{|Y|2}. We also verify that ∣βp∣1 ≤ ∣β0∣1.
It results that removing |Qm | of such vectors incurs an increase of the loss at most |Qm |m '
mηmηn^ = mη+ η-^. Since We picked η such that η + η-1 < 0, this term converges to zero. The
proof is finished.
C Cartoon of Algorithm
Refer to Fig. 2.
D Visualization of Connection
Because the Weight matrices are anyWhere from high to extremely high dimensional, for the pur-
poses of visualization We projected the models on the connecting path into a three dimensionsal sub-
space. Snapshots of the algorithm in progress for the quadratic regression task are indicated in Fig.
3. This Was done by vectorizing all of the Weight matrices for all the beads for a given connecting
path, and then performing principal component analysis to find the three highest Weight projections
for the collection of models that define the endpoints of segments for a connecting path—i.e., the
16
Published as a conference paper at ICLR 2017
e)
Figure 2: A cartoon of the algorithm. a) : The initial two models with approximately the same
loss, Lo. b) : The interpolated loss curve, in red, and its global maximum, occuring at t = t*. c):
The interpolated model Θ(θi, θj,t*) is added and labeled θi,j. d) : Stochastic gradient descent is
performed on the interpolated model until its loss is below aLo. e) : New interpolated loss curves
are calculated between the models, pairwise on a chain. f) : As in step c), a new model is inserted
at the maxima of the interpolated loss curve between θi and θ%,j. g) : As in step d), gradient descent
is performed until the model has low enough loss.
17
Published as a conference paper at ICLR 2017
Figure 3: Snapshots of Dynamic String Sampling in action for the quadratic regression task. The
string’s coordinates are its projections onto the three most important principal axes of the fully
converged string. (Top Left) One step into the algorithm, note the high loss between all of the
vertices of the path. (Top Right) An intermediate step of the algorithm. Portions of the string have
converged, but there are still regions with high interpolated loss. (Bottom Left) Near the end of the
algorithm. Almost the entire string has converged to low loss. (Bottom Right) The algorithm has
finished. A continuous path between the models has been found with low loss.
θi discussed in the algorithm. We then projected the connecting string of models onto these three
directions.
The color of the strings was chosen to be representative of the test loss under a log mapping, so that
extremely high test loss mapped to red, whereas test loss near the threshold mapped to blue. An
animation of the connecting path can be seen on our Github page.
Finally, projections onto pairs of principal components are indicated by the black curves.
E A Disconnection
E.1 A Disconnection
As a sanity check for the algorithm, we also applied it to a problem for which we know that it is not
possible to connect models of equivalent power by the arguments of section 2.3.1. The input data
is 3 points in R2, and the task is to permute the datapoints, i.e. map {x1, x2, x3} → {x2, x3, x1}.
This map requires at least 12 parameters in general for the three linear maps which take xi → xj
for i, j ∈ {{1, 2}, {2, 3}, {3, 1}}. Our archticture was a 2-3-2 fully connected neural network with
a single relu nonlinearity after the hidden layer—a model which clearly has 12 free parameters by
construction. The two models we tried to connect were a single model, θ, and a copy of θ with the
first two neurons in the hidden layer permuted, θσ . The algorithm fails to converge when initialized
with these two models. We provide a visualization of the string of models produced by the algorithm
in Fig. 4.
In general, a persistent high interpolated loss between two neighboring beads on the string of models
could arise from either a slowly converging, connected pair of models or from a truly disconnected
pair of models. “Proving” a disconnection at the level of numerical experiments is intractable in
general, but a collection of negative results—i.e., failures to converge—are highly suggestive of a
true disconnection.
18
Published as a conference paper at ICLR 2017
Figure 4: These three figures are projections of the components of the 12-dimensional weight ma-
trices which comprise the models on the string produced by the DSS algorithm. The axes are the
principal components of the weight matrices, and the colors indicate test error for the model. For
more details on the figure generation, see Appendix D. (Left) The string of models after 1 step. Note
the high error at all points except the middle and the endpoints. (Middle) An intermediate stage
of the algorithm. Part of the string has converged, but a persistent high-error segment still exists.
(Right) Even after running for many steps, the error persists, and the algorithm does not converge.
19