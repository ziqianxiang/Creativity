Published as a conference paper at ICLR 2017
Emergence of foveal image sampling from
LEARNING TO ATTEND IN VISUAL SCENES
Brian Cheung, Eric Weiss, Bruno Olshausen
Redwood Center
UC Berkeley
{bcheung,eaweiss,baolshausen}@berkeley.edu
Ab stract
We describe a neural attention model with a learnable retinal sampling lattice. The
model is trained on a visual search task requiring the classification ofan object em-
bedded in a visual scene amidst background distractors using the smallest number
of fixations. We explore the tiling properties that emerge in the model’s retinal
sampling lattice after training. Specifically, we show that this lattice resembles
the eccentricity dependent sampling lattice of the primate retina, with a high reso-
lution region in the fovea surrounded by a low resolution periphery. Furthermore,
we find conditions where these emergent properties are amplified or eliminated
providing clues to their function.
1	Introduction
A striking design feature of the primate retina is the manner in which images are spatially sampled
by retinal ganglion cells. Sample spacing and receptive fields are smallest in the fovea and then
increase linearly with eccentricity, as shown in Figure 1. Thus, we have highest spatial resolution at
the center of fixation and lowest resolution in the periphery, with a gradual fall-off in resolution as
one proceeds from the center to periphery. The question we attempt to address here is, why is the
retina designed in this manner - i.e., how is it beneficial to vision?
The commonly accepted explanation for this eccentricity dependent sampling is that it provides us
with both high resolution and broad coverage of the visual field with a limited amount of neural re-
sources. The human retina contains 1.5 million ganglion cells, whose axons form the sole output of
the retina. These essentially constitute about 300,000 distinct samples of the image due to the mul-
tiplicity of cell types coding different aspects such as on vs. off channels (Van Essen & Anderson,
1995). If these were packed uniformly at highest resolution (120 samples/deg, the Nyquist-dictated
sampling rate corresponding to the spatial-frequencies admitted by the lens), they would subtend an
image area spanning just 5x5 deg2 . Thus we would have high-resolution but essentially tunnel vi-
sion. Alternatively if they were spread out uniformly over the entire monocular visual field spanning
roughly 150 deg2 we would have wide field of coverage but with very blurry vision, with each sam-
ple subtending 0.25 deg (which would make even the largest letters on a Snellen eye chart illegible).
Thus, the primate solution makes intuitive sense as a way to achieve the best of both of these worlds.
However we are still lacking a quantitative demonstration that such a sampling strategy emerges as
the optimal design for subserving some set of visual tasks.
Here, we explore what is the optimal retinal sampling lattice for an (overt) attentional system per-
forming a simple visual search task requiring the classification of an object. We propose a learnable
retinal sampling lattice to explore what properties are best suited for this task. While evolutionary
pressure has tuned the retinal configurations found in the primate retina, we instead utilize gradi-
ent descent optimization for our in-silico model by constructing a fully differentiable dynamically
controlled model of attention.
Our choice of visual search task follows a paradigm widely used in the study of overt attention in
humans and other primates (Geisler & Cormack, 2011). In many forms of this task, a single target
is randomly located on a display among distractor objects. The goal of the subject is to find the
target as rapidly as possible. Itti & Koch (2000) propose a selection mechanism based on manually
1
Published as a conference paper at ICLR 2017
Edαujl 山百--Oo-j巴Hyl≡0N 3 0
Figure 1: Receptive field size (dendritic field diameter) as a function of eccentricity of Retinal
Ganglion Cells from a macaque monkey (taken from Perry et al. (1984)).
defined low level features of real images to locate various search targets. Here the neural network
must learn what features are most informative for directing attention.
While neural attention models have been applied successfully to a variety of engineering applications
(Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been
little work in relating the properties of these attention mechanisms back to biological vision. An
important property which distinguishes neural networks from most other neurobiological models is
their ability to learn internal (latent) features directly from data.
But existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton
(2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih
et al. (2014) utilize a multi scale glimpse window’ that forms a piece-wise approximation of this
scheme. While it seems reasonable to think that these design choices contribute to the good perfor-
mance of these systems, it remains to be seen if this arrangement emerges as the optimal solution.
We further extend the learning paradigm of neural networks to the structural features of the glimpse
mechanism of an attention model. To explore emergent properties of our learned retinal configura-
tions, we train on artificial datasets where the factors of variation are easily controllable. Despite
this departure from biology and natural stimuli, we find our model learns to create an eccentricity
dependent layout where a distinct central region of high acuity emerges surrounded by a low acuity
periphery. We show that the properties of this layout are highly dependent on the variations present
in the task constraints. When we depart from physiology by augmenting our attention model with
the ability to spatially rescale or zoom on its input, we find our model learns a more uniform layout
which has properties more similar to the glimpse window proposed in Jaderberg et al. (2015); Gre-
gor et al. (2015). These findings help us to understand the task conditions and constraints in which
an eccentricity dependent sampling lattice emerges.
2	Retinal Tiling in Neural Networks with Attention
Attention in neural networks may be formulated in terms of a differentiable feedforward function.
This allows the parameters of these models to be trained jointly with backpropagation. Most for-
mulations of visual attention over the input image assume some structure in the kernel filters. For
example, the recent attention models proposed by Jaderberg et al. (2015); Mnih et al. (2014); Gre-
gor et al. (2015); Ba et al. (2014) assume each kernel filter lies on a rectangular grid. To create a
learnable retinal sampling lattice, we relax this assumption by allowing the kernels to tile the image
independently.
2.1	Generating a Glimpse
We interpret a glimpse as a form of routing where a subset of the visual scene U is sampled to form
a smaller output glimpse G. The routing is defined by a set of kernels k[∙](s), where each kernel i
specifies which part of the input U [•] will contribute to a particular output G[i]. A control variable S
2
Published as a conference paper at ICLR 2017
μx
I
N(m ； ax，刃/
Figure 2: Diagram of single kernel filter parameterized by a mean μ and variance σ.
is used to control the routing by adjusting the position and scale of the entire array of kernels. With
this in mind, many attention models can be reformulated into a generic equation written as
HW
G[i] = XX
U[n, m]k[m, n, i](s)	(1)
where m and n index input pixels of U and i indexes output glimpse features. The pixels in the
input image U are thus mapped to a smaller glimpse G.
2.2 Retinal Glimpse
The centers of each kernel filter μ[i] are calculated With respect to control variables SC and Sz and
learnable offset μ[i]. The control variables specify the position and zoom of the entire glimpse.
μ[i] and σ[i] specify the position and spread respectively of an individual kernel k[-, - , i]. These
parameters are learned during training With backpropagation. We describe hoW the control variables
are computed in the next section. The kernels are thus specified as folloWs:
μ[i] = (sc - μ[i])sz	(2)
σ[i] = σ [i]sz	(3)
k[m,n,i](s) = N(m; μχ[i],σ[i])N(n; μy[i],σ[i])	(4)
We assume kernel filters factorize betWeen the horizontal m and vertical n dimensions of the input
image. This factorization is shoWn in equation 4, Where the kernel is defined as an isotropic gaussian
N. For each kernel filter, given a center μ[i] and scalar variance σ[i], a two dimensional gaussian is
defined over the input image as shoWn in Figure 2. These gaussian kernel filters can be thought of
as a simplified approximation to the receptive fields of retinal ganglion cells in primates (Van Essen
& Anderson, 1995).
While this factored formulation reduces the space of possible transformations from input to output,
it can still form many different mappings from an input U to output G. Figure 3B shows the possible
windows which an input image can be mapped to an output G. The yellow circles denote the central
location of a particular kernel while the size denotes the standard deviation. Each kernel maps to
one of the outputs G[i].
Positional control Sc can be considered analogous to the motor control signals which executes sac-
cades of the eye, whereas Sz would correspond to controlling a zoom lens in the eye (which has
no counterpart in biology). In contrast, training defines structural adjustments to individual ker-
nels which include its position in the lattice as well as its variance. These adjustments are only
possible during training and are fixed afterwards.Training adjustments can be considered analagous
to the incremental adjustments in the layout of the retinal sampling lattice which occur over many
generations, directed by evolutionary pressure in biology.
3
Published as a conference paper at ICLR 2017
A. Optimizing the Retinal Lattice
B. Controlling the Retinal Lattice
t	t+1	t+2
Time
Figure 3: A: Starting from an initial lattice configuration of a uniform grid of kernels, we learn an
optmized configuration from data. B: Attentional fixations generated during inference in the model,
shown unrolled in time (after training).
Table 1: VariantS of the neural attention model
Ability	Fixed Lattice	Translation Only	Translation and Zoom
Translate retina via sc,t	X	X	X
Learnable μ[i], σ[i]		X	X
Zoom retina via sz,t			X
3	Recurrent Neural Architecture for Attention
A glimpSe at a Specific timepoint, Gt, iS proceSSed by a fully-connected recurrent network frnn().
ht = frnn(Gt, ht-1)	(5)
[sc,t; sz,t] = fcontrol (ht)	(6)
The global center sc,t and zoom sz,t are predicted by the control network fcontrol () which iS param-
eterized by a fully-connected neural network.
In thiS work, we inveStigate three variantS of the propoSed recurrent model:
•	Fixed Lattice: The kernel parameters μ[i] and σ[i] for each retinal cell are not learnable.
The model can only tranSlate the kernel filterS sc,t = fcontrol (ht) and the global zoom iS
fixed sz,t = 1.
•	Translation Only: Unlike the fixed lattice model, μ[i] and σ[i] are learnable (via back-
propagation).
•	Translation and Zoom: This model follows equation 6 where it can both zoom and trans-
late the kernels.
A summary for comparing these variants is shown in Table 1.
Prior to training, the kernel filters are initialized as a 12x12 grid (144 kernel filters), tiling uniformly
over the central region of the input image and creating a retinal sampling lattice as shown in Figure
5 before training. Our recurrent network, frnn is a two layer traditional recurrent network with 512-
512 units. Our control network, fcontrol is a fully-connected network with 512-3 units (x,y,zoom)
4
Published as a conference paper at ICLR 2017
Figure 4: Top Row: Examples from our variant of the cluttered MNIST dataset (a.k.a Dataset 1).
Bottom Row: Examples from our dataset with variable sized MNIST digits (a.k.a Dataset 2).
in each layer. Similarly, our prediction networks are fully-connected networks with 512-10 units for
predicting the class. We use ReLU non-linearities for all hidden unit layers.
Our model as shown in Figure 3C are differentiable and trained end-to-end via backpropagation
through time. Note that this allows us to train the control network indirectly from signals backprop-
agated from the task cost. For stochastic gradient descent optimization we use Adam (Kingma &
Ba, 2014) and construct our models in Theano (Bastien et al., 2012).
4	Datasets and Tasks
4.1	Modified Cluttered MNIST Dataset
Example images from of our dataset are shown in Figure 4. Handwritten digits from the original
MNIST dataset LeCun & Cortes (1998) are randomly placed over a 100x100 image with varying
amounts of distractors (clutter). Distractors are generated by extracting random segments of non-
target MNIST digits which are placed randomly with uniform probability over the image. In contrast
to the cluttered MNIST dataset proposed in Mnih et al. (2014), the number of distractors for each
image varies randomly from 0 to 20 pieces. This prevents the attention model from learning a
solution which depends on the number ‘on’ pixels in a given region. In addition, we create another
dataset (Dataset 2) with an additional factor of variation: the original MNIST digit is randomly
resized by a factor of 0.33x to 3.0x. Examples of this dataset are shown in the second row of Figure
4.
4.2	Visual Search Task
We define our visual search task as a recognition task in a cluttered scene. The recurrent attention
model We propose must output the class C of the single MNIST digit appearing in the image via
the prediction network fpredict (). The task loss, L, is specified in equation 8. To minimize the
classification error, We use cross-entropy cost:
^t,n
fpredict (ht,n)
NT
L=	Cnlθg(Ct,n)
(7)
(8)
Analolgous to the visual search experiments performed in physiological studies, We pressure our
attention model to accomplish the visual search as quickly as possible. By applying the task loss to
every timepoint, the model is forced to accurately recognize and localize the target MNIST digit in
as feW iterations as possible. In our classification experiments, the model is given T = 4 glimpses.
5
Published as a conference paper at ICLR 2017
Figure 5: The sampling lattice shown at four different stages during training for a Translation Only
model, from the initial condition (left) to final solution (right). The radius of each dot corresponds
to the standard deviation σi of the kernel.
1.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
Distance from Center (Eccentricity)
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
Figure 6: Top: Learned sampling lattices for four different model configurations. Middle: Resolution
(sampling interval) and Bottom: kernel standard deviation as a function of eccentricity for each
model configuration.
5	Results
Figure 5shows the layouts of the learned kernels for a Translation Only model at different stages
during training. The filters are smoothly transforming from a uniform grid of kernels to an eccen-
tricity dependent lattice. Furthermore, the kernel filters spread their individual centers to create a
sampling lattice which covers the full image. This is sensible as the target MNIST digit can appear
anywhere in the image with uniform probability.
When we include variable sized digits as an additional factor in the dataset, the translation only
model shows an even greater diversity of variances for the kernel filters. This is shown visually in the
first row of Figure 6. Furthermore, the second row shows a highly dependent relationship between
the sampling interval and standard deviatoin of the retinal sampling lattice and eccentricity from the
center. This dependency increases when training on variable sized MNIST digits (Dataset 2). This
6
Published as a conference paper at ICLR 2017
Figure 7: Temporal rollouts of the retinal sampling lattice attending over a test image from Cluttered
MNIST (Dataset 2) after training.
relationship has also been observed in the primate visual system (Perry et al., 1984; Van Essen &
Anderson, 1995).
When the proposed attention model is able to zoom its retinal sampling lattice, a very different
layout emerges. There is much less diversity in the distribution of kernel filter variances as evi-
denced in Figure 6. Both the sampling interval and standard deviation of the retinal sampling lattice
have far less of a dependence on eccentricity. As shown in the last column of Figure 6, we also
trained this model on variable sized digits and noticed no significant differences in sampling lattice
configuration.
Figure 7 shows how each model variant makes use of its retinal sampling lattice after training. The
strategy each variant adopts to solve the visual search task helps explain the drastic difference in
lattice configuration. The translation only variant simply translates its high acuity region to recog-
nize and localize the target digit. The translation and zoom model both rescales and translates its
sampling lattice to fit the target digit. Remarkably, Figure 7 shows that both models detect the digit
early on and make minor corrective adjustments in the following iterations.
Table 2 compares the classification performance of each model variant on the cluttered MNIST
dataset with fixed sized digits (Dataset 1). There is a significant drop in performance when the
retinal sampling lattice is fixed and not learnable, confirming that the model is benefitting from
learning the high-acuity region. The classification performance between the Translation Only and
Translation and Zoom model is competitive. This supports the hypothesis that the functionality of a
high acuity region with a low resolution periphery is similar to that of zoom.
7
Published as a conference paper at ICLR 2017
Table 2: ClaSSification Error on ClUttered MNIST
Sampling Lattice Model	Dataset 1 (%)	Dataset 2 (%)
Fixed Lattice	11.8	31.9
Translation Only	5.1	24.4
Translation and Zoom	4.0	24.1
6 Conclusion
When conStrained to a glimpSe window that can tranSlate only, Similar to the eye, the kernelS con-
verge to a Sampling lattice Similar to that found in the primate retina (Curcio & Allen, 1990; Van ES-
Sen & AnderSon, 1995). ThiS layout iS compoSed of a high acuity region at the center Surrounded
by a wider region of low acuity. Van ESSen & AnderSon (1995) poStulate that the linear relationShip
between eccentricity and Sampling interval leadS to a form of Scale invariance in the primate retina.
Our reSultS from the TranSlation Only model with variable Sized digitS SupportS thiS concluSion.
Additionally, we obServe that zoom appearS to Supplant the need to learn a high acuity region for
the viSual Search taSk. ThiS implieS that the high acuity region ServeS a purpoSe reSembling that of
a zoomable Sampling lattice. The low acuity periphery iS uSed to detect the Search target and the
high acuity ‘fovea’ more finely recognizeS and localizeS the target. TheSe reSultS, while obtained on
an admittedly Simplified domain of viSual SceneS, point to the poSSibility of uSing deep learning aS
a tool to explore the optimal Sample tiling for a retinal in a data driven and taSk-dependent manner.
Exploring how or if theSe reSultS change for more challenging taSkS in naturaliStic viSual SceneS iS a
future goal of our reSearch.
Acknowledgments
We would like to acknowledge everyone at the Redwood Center for their helpful diScuSSion and
commentS. We gratefully acknowledge the Support of NVIDIA Corporation with the donation of the
TeSla K40 GPUS uSed for thiS reSearch.
References
Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with viSual
attention. arXiv preprint arXiv:1412.7755, 2014.
Dzmitry Bahdanau, Kyunghyun Cho, and YoShua Bengio. Neural machine tranSlation by jointly
learning to align and tranSlate. arXiv preprint arXiv:1409.0473, 2014.
Frederic Bastien, Pascal Lamblin, Razvan Pascanu, James BergStra, Ian Goodfellow, AmaUd Berg-
eron, NicolaS Bouchard, David Warde-Farley, and YoShua Bengio. Theano: new featureS and
speed improvements. arXiv preprint arXiv:1211.5590, 2012.
Christine A CUrcio and Kimberly A Allen. Topography of ganglion cells in hUman retina. Journal
OfComparative Neurology, 300(1):5-25, 199θ.
Wilson S Geisler and Lawrence Cormack. Models of overt attention. Oxford handbook of eye
movements, pp. 439-454, 2011.
Alex Graves, Greg Wayne, and Ivo Danihelka. NeUral tUring machines. arXiv preprint
arXiv:1410.5401, 2014.
Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. Draw: A recUrrent neUral network
for image generation. arXiv preprint arXiv:1502.04623, 2015.
LaUrent Itti and Christof Koch. A saliency-based search mechanism for overt and covert shifts of
visUal attention. Vision research, 40(10):1489-1506, 2000.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Ad-
vances in Neural Information Processing Systems, pp. 2008-2016, 2015.
8
Published as a conference paper at ICLR 2017
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Hugo Larochelle and Geoffrey E Hinton. Learning to combine foveal glimpses with a third-order
boltzmann machine. In Advances in neural information processing systems, pp. 1243-1251, 2010.
Yann LeCun and Corinna Cortes. The mnist database of handwritten digits, 1998.
Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In
Advances in Neural Information Processing Systems, pp. 2204-2212, 2014.
VH Perry, R Oehler, and A Cowey. Retinal ganglion cells that project to the dorsal lateral geniculate
nucleus in the macaque monkey. Neuroscience, 12(4):1101-1123, 1984.
David C Van Essen and Charles H Anderson. Information processing strategies and pathways in the
primate visual system. An introduction to neural and electronic networks, 2:45-76, 1995.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and
Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention.
arXiv preprint arXiv:1502.03044, 2015.
9