Published as a conference paper at ICLR 2017
Neural Program Lattices
Chengtao Li *
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
ctli@mit.edu
Daniel Tarlow, Alexander L. Gaunt, Marc Brockschmidt, Nate Kushman
Microsoft Research
Cambridge, CB1 2FB, UK
{dtarlow,algaunt,mabrocks,nkushman}@microsoft.com
Ab stract
We propose the Neural Program Lattice (NPL), a neural network that learns to per-
form complex tasks by composing low-level programs to express high-level pro-
grams. Our starting point is the recent work on Neural Programmer-Interpreters
(NPI), which can only learn from strong supervision that contains the whole hi-
erarchy of low-level and high-level programs. NPLs remove this limitation by
providing the ability to learn from weak supervision consisting only of sequences
of low-level operations. We demonstrate the capability of NPL to learn to perform
long-hand addition and arrange blocks in a grid-world environment. Experiments
show that it performs on par with NPI while using weak supervision in place of
most of the strong supervision, thus indicating its ability to infer the high-level
program structure from examples containing only the low-level operations.
1	Introduction
A critical component of learning to act in a changing and varied world is learning higher-level
abstractions of sequences of elementary tasks. Without such abstractions we would be forced to
reason at the level of individual muscle contractions, making everyday tasks such as getting ready
for work and making dinner almost impossible. Instead, as humans, we learn a hierarchy of skills
starting with basic limb movements and eventually getting to the level of tasks such as get ready
for work or drive to the airport. These abstractions have many different names. For example, in
computer programming they are called functions or subroutines and in reinforcement learning they
are called options or temporally extended actions. They facilitate learning in two important ways.
First, they enable us to learn faster, i.e. with lower sample complexity. Second, they enable us to
strongly generalize from our prior experience so that we can, for example, drive to a new location
once we have learned how to drive to a few other locations.
A primary mechanism used for learning is watching others perform a task. During such demon-
strations, one typically observes the elementary operations performed, such as the movements of
individual limbs or the mouse clicks in a computer interface. In some cases, the demonstrations can
also provide supervision of the abstract operations (i.e., the abstraction hierarchy) that generated
the elementary operations, either through a formal annotation process or through informal natural
language descriptions. Recent work on Neural Programmer-Interpreters, NPI (Reed & de Freitas,
2016), has shown that when the training data includes both elementary and abstract operations,
learning the abstractions results in strong generalization capabilities. This enables, for example, the
ability to add very large numbers when trained only on the addition of relatively small numbers.
Providing supervision of the abstract operations during a demonstration requires significant addi-
tional effort, however, and so in typical real-world scenarios we will observe only the elementary
operations. For example, we can see a person’s limbs move (elementary operations), but we can-
not see the mental states that led to these movements (abstract operations). In the same vein, we
* Work done primarily while author was an intern at Microsoft Research.
1
Published as a conference paper at ICLR 2017
can easily capture a user’s clicks in an online application or their real-world movements using a
skeletal tracking depth camera (Microsoft Corp. Redmond WA). NPI cannot directly be applied on
data like this, however, because the data does not contain the abstraction hierarchy. This motivates
the desire for a model which can learn an abstraction hierarchy from only sequences of elementary
operations, but this is an ill-posed problem that requires either additional modeling assumptions or
some strongly supervised data. In this work, we take a first step by assuming access to a small
number of strongly supervised samples that provide the components of the abstraction hierarchy
and disambiguate which of infinitely many abstraction hierarchies are preferred. While we currently
only consider domains without noise, we believe our work provides a starting point for future re-
search on adding additional modeling assumptions that could remove the need for strong supervision
altogether.
There are several technical issues that arise in developing NPL, which are addressed in this paper.
In section 2, we reformulate the NPI model to explicitly include a program call stack, which is
necessary for the later modeling developments. Next we need to formulate a training objective for
weakly supervised data instances. Ideally we could treat the abstract operations as latent quantities
and optimize the marginalized log probability that arises from summing out the abstract operations.
However, there are exponentially many such abstraction hierarchies, and so this is computationally
intractable. To overcome this challenge, we compute an approximate dynamic program by building
on two ideas from the literature. First, we draw inspiration from Connectionist Temporal Classifica-
tion, CTC (Graves et al., 2006), observing that it provides a method for learning with latent align-
ments. In section 3.1 we reformulate the CTC objective into a feedforward process that executes a
dynamic program. Applying this to our problem, however, requires handling the program call stack.
In section 3.2 we do this through an approximation analogous to that of Stack-Augmented Recurrent
Nets, StackRNNs (Joulin & Mikolov, 2015), resulting in a fully-differentiable feedforward process
that executes a dynamic program to approximately compute the marginalized log probability that we
desire. Finally, we observe in section 3.3 that there are alternative dynamic programs for approxi-
mating the desired marginalized log probability and present one that uses more computation to more
closely resemble the exact (exponentially expensive) dynamic program while remaining tractable.
Our key contributions can be summarized as follows:
•	We show how ideas from CTC and StackRNNs can be adapted and extended to enable the
training of NPI-like models from only flat sequences of elementary operations and world states.
•	We introduce a method to compute a more accurate approximation of marginalized log proba-
bilities in such models.
•	On the long-hand addition task from Reed & de Freitas (2016) and anew task involving arrang-
ing blocks in a grid-world, we demonstrate empirically that using NPL to train with elementary
operation sequences combined with only a few training samples with full program traces can
achieve similar performance to NPI but with weaker supervision.
2	Model Background
The NPI model is based on a Recurrent Neural Network (RNN) which, at each step, either calls an
abstract program, performs an elementary operation, or returns from the current program. To make
this decision, each step of the RNN takes as input: (1) a learnable embedding of program to execute,
(2) embedded arguments for this program, and (3) an embedding of the current world state. Calling
an abstract program resets the LSTM hidden state to zero and updates the program and arguments
provided as input to the following steps. Returning from an abstract program inverts this process,
restoring the hidden state and input program and arguments to those from before the program was
called. Performing an elementary operation updates the world state, but leaves the current program
and arguments in place, and performs the standard LSTM update of the hidden state.
Rather than present the details of the NPI model as in Reed & de Freitas (2016), we will cast it in
the formulation that we will use throughout the paper. The main difference is that our presentation
will explicitly maintain a call stack, which we will refer to as Stack-based NPI. Morally this does
not change the model, but it will enable the extension to weaker supervision described in section 3.
The basic structure of the reformulated model can be seen in Figure 1. The model learns a library of
programs, G, and arguments, R, to these programs, where each program g ∈ Rn and each argument
2
Published as a conference paper at ICLR 2017
t
t+1	t+2	t+3
人
武=PUSH
≡ I
Γ "¾RNN>η I
武+1=OP 兀t+1
武+2 = POP
(empty)
Wt
Wt
S甘i =必# =.。,皆：
Mlξ+1= 0	E
fworld
RNN
Cell
Wt+1
S* = o3-
什2 _ ι,t+1
) 一%Ut
Wt+2
fworld
-3
(empty)
fworld
M


S

Figure 1: Stack-based NPI: Four time steps from the execution of the stack-based NPI model. Each
color/hash pattern represents a unique set of unchanging data values which, over time, move UP and
down (and in and out of) the stack. Operations below the dotted line to calculate the new world state
are executed only at test time, since we do not have access to fworld at training time, and the training
data contains the correct sequence of world states.
r ∈ Rm is represented as an embedding, with n and m as the embedding dimensions. When a
program is Cailed with a list of arguments it performs a sequence of actions, where each action is
one of: OP, PUSH, or POP.OP performs an elementary operation, e.g. move one step. PUSH calls
to another program. POP returns from the current program back to the parent program.
An LSTM-based controller, shown in Figure 2,
is used to generate the sequence of actions, de-
ciding the action at timestep t based on the cur-
rently running program and arguments, gtn, the
LSTM's internal state h& and an observation
of the current world state, wt. To support calls
to and returns from subprograms, the controller
state contains two call stacks, one for the inter-
nal RNN state, which we denote as M (green
in Figure 1), and one for the program and argu-
ments, which we denote as S (red in Figure 1).
Md and Sd refer to the elements at depth-d of
the stacks at timestep t.
The training data for NPI requires full execu-
tion traces. We use ∏ to denote all the observa-
tions recorded in a single full exectution trace.
Specifically, for timestep t in the execution we
define ∏W to be the input world state, and ∏a
to be the decision of which of the following ac-
tions to take:
^out
9out
W
t
Figure 2: RNN Cell: A zoomed in view of the
internals of an RNN cell from Figure 1.
OP: perform elementary operation ∏0
PUSH: push the currently running program and LSTM internal state onto the call stack and call
program πg
POP: return to the parent program by popping the parent,s program state off the top of the call
stack
3
Published as a conference paper at ICLR 2017
Note that, as with the original NPI model, we also include arguments for both the operation and
program calls, but for notational simplicity we subsume those into πot and πgt respectively.
The stack updates are formally defined as:
Jπat = POPKM1t + Jπat = OPKhtout + Jπat = PUSHK0,
Mdt+1 = Jπat = POPKM2t + Jπat = OPKM1t + Jπat = PUSHKhtout,
IJna = POPKMd+ι + Jna = OPKMd+Jna = PUSHKMd-i,
d=0
d=1
d>1
(2.1)
St+1 = Jnat = POPKS1t + Jnat = OPKS0t + Jnat = PUSHKgotut,	d=0
d	Jnat = POPKSdt+1 + Jnat = OPKSdt + Jnat = PUSHKSdt-1, d>0
The conditions in the Iverson brackets choose which type of update should be performed based on
the action type. POPing from the stack moves all items up one location in the stack. Performing an
elementary OP, updates the top element of stack M to contain the new RNN hidden state but other-
wise leaves the stacks unchanged. PUSHing onto the stack pushes the new program and arguments,
got-ut1, onto stack S, pushes a default (zero) hidden state onto stack M, and moves all of the other
elements in the stacks down one location.
The RNN cell inputs are:
hitn = M0t = the current LSTM internal state,
gitn = S0t = the current program and arguments,
wt = nwt = the current world state.
Inside the RNN cell, as shown in Figure 2, gitn and wt are passed through a task specific encoder net-
work, fenc to generate a combined embedding ut which is passed directly into an LSTM (Hochreiter
& Schmidhuber, 1997). Formally,
ut = fenc(wt, gitn), htout = flstm(ut, htin);	(2.2)
The LSTM output is passed in parallel through four different decoder networks to generate the
following probability distributions:
pta =	the action	ptr = the arguments for the program or operation
ptg =	the program to be called pto = the elementary operation to be performed
At test time, we use a greedy decoder that makes the decision with the highest probability for each
choice. Formally:
gotut = φ(ptg) = argmaxγ∈G ptg (γ)
At training time our objective is to find neural network parameters θ which maximize the following
(log) likelihood function:
p(nt) = Jnat = OPK pta(OP)pto(not) + Jnat = PUSHK pta(PUSH)ptg(ngt) + Jnat = POPKpta(POP)
T
p(n) = Y p(nt),
t=1
L(θ) = log p(n)
(2.3)
3	Neural Program Lattices
In this section we introduce our core contribution, a new framework for training NPI-like models
when the training data contains only sequences of elementary actions instead of full program abstrac-
tions. The basis of our framework is the Neural Program Lattice, which approximately computes
marginal probabilities using an end-to-end differentiable neural network.
In this section, the training data is an elementary operation trace λ, which includes a sequence of
elementary steps, λo, and a corresponding sequence of world states, λw . For each elementary step,
λi, the elementary operation performed is λio and the input world state is λiw . We define O as a
many-to-one map from a full execution trace n to it’s elementary operation trace λ. With these
4
Published as a conference paper at ICLR 2017
definitions and p(π) as defined in equation 2.3, our desired (log) marginal likelihood for a single
example becomes
L(θ) = log	p(π).
π∈O-1(λ)
(3.1)
Computing this quantity is intractable because the number of possible executions ∣O-1(λ)∣ is ex-
ponential in the maximum length of π and each execution may have unique stack states. In the
following sections, we describe how to approximately compute this quantity so as to enable learning
from weak supervision. To also learn from strong supervision, we simply add log p(π) terms to the
objective for each strongly supervised example π.
3.1	CTC as a Feed-forward Network
In formulating a loss function which approximates the exponential sum in equation 3.1, the first
challenge is aligning the elementary steps, λi, in the training data, to the timesteps, t, of the model.
Specifically, when the model calls into a program or returns from a program in a given timestep,
it does not perform any elementary operation in that timestep. As a result, the alignment between
elementary steps in the data and the timesteps of the model depends crucially on the choice of high-
level abstraction. To overcome this challenge, we draw inspiration from CTC (Graves et al., 2006).
CTC is an RNN-based neural network architecture used in speech recognition to handle the analo-
gous problem of aligning audio sequence inputs to word sequence outputs. It can be seen as a com-
bination of an RNN and a graphical model. The RNN computes a distribution over possible outputs
for each timestep, while the graphical model consumes those distributions and uses a dynamic pro-
gram to compute the marginal distribution over possible label sequences. A crucial assumption is
that the RNN outputs at each timestep are conditionally independent, i.e. no feedback connections
exist from the output layer back into the rest of the network. This assumption is incompatible with
the NPI model because action decisions from timestep t determine the world state, hidden state, and
program input for the next timestep. In section 3.2 we will adapt the CTC idea to work in the NPI
setting. In this section we prepare by reformulating CTC into a feed forward neural network that
can be trained with standard back propagation.
The main challenge solved by CTC is finding the alignment between the elementary steps, i, ob-
served in the training data and the timesteps, t, of the model. To facilitate alignment discovery, the
output layer in a CTC network is a softmax layer with a unit for each elementary operation in O, the
set of elementary operations, as well as one additional unit for a BLANK output where no elementary
operation is performed because (in our case) the model calls into a new program or returns from the
current program. Define β ∈ O0T as an output sequence over the alphabet O0 = O ∪ BLANK.
Additionally, define the many-to-one map B from an output sequence β to λo the sequence of el-
ementary operations created by removing all of the BLANK outputs from β. As discussed above,
the CTC model assumes that the RNN inputs at time t are independent of the decisions made by
the model, π. Thus for purposes of this subsection, we will assume both that hitn = hto-ut1, and that
w = (w1, . . . , wT) and gin = (gi1n, . . . , giTn) are provided as inputs and are thus independent of the
output decisions. We can then formally define
pt(βt∣W,gin)
pta(POP|w, gin) + pta(PUSH|w, gin),
Ma(OPlw,ginM0Cβtlw, gin),
βt = BLANK
otherwise
|w|
P(β lw,gin) = ∏Pt(βt |w,gin)
t=1
L(θ∖λo,W,gin) = log p(λo∣W,gin) = log E p(β∣W,gin).
β∈B-1 (λo)
The dynamic program used by CTC to compute this likelihood is based on yit , the total probability
that as of timestep t in the model We have generated10：i, the first i elementary actions in λ0. yit is
5
Published as a conference paper at ICLR 2017
calculated from w1:t and gi1n:t, the first t elements in w and gin respectively. Formally,
yit =	X	p(e|w1:t, g⅛)
β∈B-1(λO:"
L(θ∣λo,w,gin) = log y∣∣wθ∣.
We can compute this value recursively as y00 = 1 and
yt =pt(λθ ∣w1t,g1t)yt-1 + Pt(BLANK∣wLt,g1nt)ytT.
This formulation allows the likelihood to be computed in a feed-forward manner and the gradients
of θ to be computed using standard back propagation through time. Note that if there were feedback
connections in the model, then it would not be sufficient to only use yit as the dynamic programming
state; we would need to keep track of all the different possible stack states after having produced the
sequence prefix, which is what leads to the intractability.
3.2	Differentiable Call Stack
In the last section we assumed that the RNN inputs w, and gin were defined independently of the
decisions π made by the model and that hitn = hto-ut1. In this section we show how to relax these
assumptions to handle the full Stack-based NPI model described in section 2. The key idea is that
rather than propagating forward all possible stack states, which leads to a combinatorial explosion,
we will propagate forward a single stack state which is a weighted average of all possible stack
states, where the weights are computed based on local probabilities of actions at each timestep.
This operation is analogous to that used in StackRNNs (Joulin & Mikolov, 2015). The result is a
tractable and differentiable forward execution process that no longer exactly computes the desired
marginal likelihood. However, we will show experimentally that learning with this model for weakly
supervised examples leads to the behavior that we would hope for if we were learning from the true
marginal log likelihood. That is, we can share model parameters while training on strongly and
weakly labeled examples, and adding the weakly labeled data improves generalization performance.
In more detail, we estimate all quantities specified in π but not in λ using a soft-argmax function
that computes deterministic functions of the previously observed or estimated quantities. These
estimated quantities are πa, πg , and implicitly πw . Both πw and πg can be directly replaced with a
soft-argmax as follows:
wt =	yit-1λiw	(3.2)
i∈I
gotut = φsof t (ptg) = Xptg(γ)γ	(3.3)
γ∈G
Replacing decision πat with a soft-argmax changes the stack updates from equation 2.1 into differ-
entiable stack updates similar to those used in Joulin & Mikolov (2015). Formally,
yt =	yit
i∈I
αt (a)
Pi∈Xy"yt+I)Pa(O)PoN), a = OP
(yt/yt+1)pta(a),	a 6= OP
αt(POP)M1t + αt(OP)htout + αt(PUSH)0,
αt(POP)M2t + αt(OP)M1t + αt(PUSH)htout,
αt (POP)Mdt+1 + αt (OP)Mdt + αt (PUSH)Mdt
αt(POP)S1t +αt(OP)S0t +αt(PUSH)gotut,
αt(POP)Sdt+1+αt(OP)Sdt +αt(PUSH)Sdt-1,
-1,
d
d=0
d=1
d>1
0
d>0
with α introduced for notational simplicity. This change enables hitn and gitn to now depend on the
distribution over output decisions at time t - 1 via the stack, as gitn = S0t and hitn = M0t , where S0t
(resp. M0t ) are computed from S0t-1 and S1t-1 (resp. M0t-1 and the LSTM cell’s output at timestep
t).
6
Published as a conference paper at ICLR 2017
Figure 3: NPL lattice: Each slice cor-
responds to one timestep, and each node
in a timestep corresponds to a given call
depth, l, and elementary operation in-
dex, i. A subset of the lattice transitions
are shown with blue arrows for PUSH
transitions, green for OP and orange for
POP.
	Blurred Stack	Blurred World	All Paths Return	Computational Cost	Gradient Accuracy
Execute All Paths	False	False	True	Highest	Exact
NPL	True	False	True	Medium	Medium
CTC+StackRNN	True	True	False	LoWest	LoWest
Table 1: Outlines the tradeoff between representational accuracy and computational cost for two
extreme solutions and NPL.
The last remaining complexity is that λ does not indicate the necessary number of model timesteps.
Thus the likelihood function must sum over all possible execution lengths up to some maximum T
and ensure that the final action is a return, i.e. POP. If We define I = ∣λ0∣ then formally,
L(θ) = log X pta(POP)yIt
t<T
This gives a fully differentiable model for approximately maximizing the marginal probability of λ.
3.3 Lattice of Program States
Although the model We have defined so far is fully differentiable, the difficultly in training smoothed
models of this form has been highlighted in the original Neural Turing Machine Work (Graves et al.,
2014) as Well as much of the folloW on Work (Gaunt et al., 2016; Kurach et al., 2016; Graves
et al., 2016; Neelakantan et al., 2016; Joulin & Mikolov, 2015). To help alleviate this difficulty, We
introduce in this section the neural lattice structure after Which Neural Program Lattices are named.
To motivate the need for this lattice, consider the set of possible program execution paths as a tree
With a branch point for each timestep in the execution and a probability assigned to each path. Exact
gradients could be computed by executing every path in the tree, calculating the gradient for each
path, and then taking an average of the gradients Weighted by the path probabilities. This solution
is impractical hoWever since it requires computation and memory that scales exponentially With
the number of timesteps. To avoid this problem, the NTM and related techniques perform a single
forWard execution Which is meant to approximately represent the simultaneous execution of all of
the paths in the tree. To avoid the exponential explosion, the state at each timestep, i.e. tree depth, is
approximated using a fixed-sized, representation. The approximation representation chosen by both
NTM and Joulin & Mikolov (2015) is a soft-argmax of the states generated by performing each of
the possible actions on the previous approximate state.
We observe that these tWo choices are really extreme points on What is a continuous spectrum of
options. Instead of choosing to maintain a separate state representation for every path, or to group
together all paths into a single representation, We can group together subsets of the paths and main-
tain an approximate state representation for each subset. This alloWs us to move along this spectrum,
by trading higher memory and computational requirements for a hopefully closer approximation of
the marginal probability.
7
Published as a conference paper at ICLR 2017
In our implementation we group together execution paths at each timestep by call depth, l ∈ L,
and number of elementary operations performed so far, i ∈ I , and maintain at each timestep a
separate embedded state representation for each group of execution paths. Thus the unrolled linear
architecture shown in Figure 1 becomes instead a lattice, as shown in Figure 3, with a grid of
approximate program states at each timestep. Each node in this lattice represents the state of all
paths that are at depth l and elementary operation i when they reach timestep t. Each node contains
a soft-argmax of the stack states in M and S and an RNN cell identical to that in Figure 21. For
each node we must also compute yit,l, the probability that at timestep t the execution is at depth l
and at elementary operation i and has output the elementary operation sequence 1上加 As before We
can compute this recursively as:
yit+1,l = pta,,li+1(POP)yit,l+1 + pta,,li-1(OP)pto,,li-1(λio)yit-,l1 + pta,,li-1(PUSH)yit,l-1.
Similarly, the averaged call stack values are computed recursively as folloWs:
αit1,l,1i,2l2(a) = (yit1,l1/yit2+1,l2)pta,,li11(a)
(αt,i+1,l(POP)Mt,i+1 + αQ1,i(θP)pO,,"(λO)hOUt,i-i + αt'i-1,l(PUSH)0,	d = 0
Mdt,+i1,l =	αit,,il+1,l(POP)M2t,,li+1 + αit-,l,1l,i(OP)pto,,li-1(λio)M1t,,il-1 + αit,,il-1,l(PUSH)hto,ul-t,i1-1, d= 1
(αt,i+1,l(POP)Md++； + at%(OP)Po,,"CO)%," + 说「«PUSH)Md--,i,	d > 1
αit,,il+1,l(POP)S1t,,li+1+αit-,l,1l,i(OP)pto,,li-1(λio)S0t,,li-1+αit,,il-1,l(PUSH)got,ul-t,i1,	d=0
αit,il+1,l(POP)Sdt,+l+11i + αit-,l,1l i(OP)pto,li-1(λio)Sdt,li-1 + αit,il-1,l(PUSH)Sdt,-l-11i, d>0
,	+1,i - ,	o, - o ,i-1	,	-1,i
We have left out the boundary conditions from the above updates for readability, the details of these
are discussed in Appendix A.4.
Finally, the likelihood function approximately maximizes the probability of paths Which at any
timestep have correctly generated all elementary operations in λ, are currently at depth 0 and are
returning from the current program. Formally,
L(θ) = logXpta,,0I(POP)yIt,0.	(3.4)
t∈T
Remark: The specific choice to group by elementary operation index, and call depth Was motivated
by the representational advantages each provides. Specifically:
•	Grouping by elementary operation index: alloWs the model to represent the input World
state exactly instead of resorting to the fuzzy World state representation from equation 3.2.
•	Grouping by call depth: alloWs the representation to place probability only on execution
paths that return from all subprograms they execute, and return only once from the top level
program as specified in equation 3.4.
Table 1 summarizes these advantages and the computational trade-offs discussed earlier.
Finally, in practice We find that values of the y’s quickly underfloW, and so We renormalize them at
each timestep, as discussed in Appendix A.3.
4	Experiments
In this section, We demonstrate the capability of NPL to learn on both the long-hand addition
task (Addition) from Reed & de Freitas (2016) and a neWly introduced task involving arranging
blocks in a grid-World (NanoCraft). We shoW that using the NPL to train With mostly the Weak
supervision of elementary operation traces, and very feW full program traces, our technique sig-
nificantly outperforms traditional sequence-to-sequence models, and performs comparably to NPI
models trained entirely With the strong supervision provided by full program traces. Details of the
experimental settings are discussed in Appendix A.5.
8
Published as a conference paper at ICLR 2017
NANOCRAFT,PUSH
MOVE_MANY(right),PUSH
LACT_MOVE(right),STAY
....
<END>, POP
MOVE_MANY(down),PUSH
ACT_MOVE(right),STAY
L+. <END>,POP	D
BUILD_WALL(right),PUSH
PLACE_AND_MOVE(right),PUSH
LACT_MOVE(right),STAY
ACT_PLACE_BLOCK(wood, red),STAY
<END>, OP
PLACE_AND_MOVE(right),PUSH
LACT_MOVE(right),STAY
<END>, OP
<END>, POP
BUILD_WALL(down),PUSH
L....
<END>,POP
<END>, POP
Figure 4: NanoCraft: An
illustrative example program,
where the agent (denoted as
“*”) is required to build 3×4
rectangular red wooden build-
ing at a certain location in
a 6×6 grid world. We can
see that some of the blocks
are already in place in the
initial world-state. To build
the building, the agent (pro-
gram) first makes two calls to
MOVE』ANY to move into place
in the X and Y dimensions, and
then calls build_wall four
times to build the four walls of
the building.
NANOCRAFTWITH FULL WORLD
Figure 5: NanoCraft Sample Complexity: The x-axis varies the number of samples containing
full program abstractions, while the y-axis shows the accuracy. NPL-{64,128,256} shows the accu-
racy of our model when trained with 64/128/256 training samples. NPI shows the accuracy of NPI,
which can utilize only the samples containing full program abstractions. Finally, Seq-{64,128,256}
shows the accuracy of a seq2seq baseline when trained on 64/128/256 samples. It’s performance
does not change as we vary the number of samples with full program abstractions since it cannot
utilize the additional supervision they provide.
4.1	Sample Complexity
Task: We study the sample complexity using a task we call NANOCRAFT. In this task we consider
an environment similar to those utilized in the reinforcement learning literature. The perceptual
input comes from a 2-D grid world where each grid cell can be either empty or contain a block with
both color and material attributes. The task is to move around the grid world and place blocks in
the appropriate grid cells to form a rectangular building. The resulting building must have a set of
provided attributes: (1) color, (2) material, (3) location, and sizes in the (4) X and (5) Y dimensions.
As shown in the example in Figure 4, at each step the agent can take one of two primitive actions,
place a block at the current grid cell with a specific color and material, or move in one of the four
1with additional indexes for i and l on all of the inputs and outputs.
9
Published as a conference paper at ICLR 2017
ADD, PUSH
ADD1,PUSH
ACT_WRITE(3),STAY
CARRY,PUSH
ACT_PTR_MOVE(1, left),STAY
ACT_WRITE(1), STAY	^―
ACT_PTR_MOVE(1, right),STAY
♦ <END>,POP
LSHIFT,PUSH
ACT_PTR_MOVE(0,
ACT_PTR_MOVE(1,
ACT_PTR_MOVE(2,
ACT_PTR_MOVE(3,
♦ <END>,POP
♦ <END>,POP
left),STAY
left),STAY
left),STAY
left),STAY
0
`3*∣
ADD1,PUSH
ACT_WRITE(7),STAY
LSHIFT,PUSH
ACT_PTR_MOVE(0,
ACT_PTR_MOVE(1,
ACT_PTR_MOVE(2,
ACT_PTR_MOVE(3,
♦ <END>,POP
L÷ <END>,POP
T <END>,POP
left),STAY
left),STAY
left),STAY
left),STAY
Figure 6: Addition: An il-
lustrative example program of
the addition of 25 to 48. We
have four pointers (denoted
as “*”), one for each row
of the scratch pad. We re-
peatedly call ADD1 until we
hit the left most entry in the
scratch pad. Each call to
addi We call act_write to
write the result, CARRY to
Write the carry digit (if nec-
essary) and LSHIFT to shift
all four pointers to the left to
Work on the next digit. The
digit sequence on the fourth
roW of scratch pad is the result
of the addition.
0
0
0
0
4
0
8
5
0
3
cardinal directions. We explored both a fully observable setting, and a partially observable setting.
In the fully observable setting, the World is presented as a stack of 3 grids, one indicating the material
of the block at each location (or empty), a similar one for color and a final one-hot grid indicating
the agent’s location. In the partially observable setting, the agent is provided only tWo integers,
indicating the color and material of the block (if any) at the current location. Finally, in both settings
the World input state contains an auxiliary vector specifying the five attributes of the building to
be built. In each sample, a random subset of the necessary blocks have already been placed in the
World, and the agent must Walk right over these locations Without placing a block.
Experiment Setup: We assume that data With full programmatic abstractions is much more diffi-
cult to obtain than data containing only flat operation sequences,2 so We study the sample complexity
in terms of the number of such samples. All experiments Were run With 10 different random seeds,
and the best model Was chosen using a separate validation set Which is one-quarter the size of the
training set.
Results: Figure 5 shoWs the sample complexity for the NANOCRAFT task in the fully observable
setting. We can see that NPL significantly outperforms the NPI baseline (NPI) When only a subset
the total training samples have full abstractions. NPL similarly outperforms a sequence-to-sequence
baseline (Seq-*) trained on all of the available data. We also performed preliminary experiments for
the partially observable setting, and obtained similar results.
4.2	Generalization Ability
Task: We study generalization ability using the ADDITION task from Reed & de Freitas (2016).
The objective of this task is to read in tWo numbers represented as digit sequences and compute the
digit sequence resulting from the summation of these tWo numbers. The goal is to let the model
learn the basic procedure of long-hand addition: repeatedly add tWo one-digit numbers, Write doWn
the result (and the carry bit if necessary) and move to the left until the beginning of the numbers
is reached. The Whole procedure is represented using a four-roW scratch pad, Where the first and
second roWs are input digit sequences, the third roW is the carry digit and the forth roW the result.
The model is provided a World-state observation Which only provides a partial vieW into the full
scratchpad state. Specifically, it is provided the integers at the location of four different pointers,
each in one roW of the scratchpad. The model has tWo possible elementary operations, either move
a pointer left or right, or Write a single digit into one of the four pointer locations. All four pointers
start at the rightmost location (the least significant digit), and are gradually moved to the left by the
2Operation sequences can be obtained by observing a human demonstrating a task, Whereas full abstractions
require additional effort to annotate such traces.
10
Published as a conference paper at ICLR 2017
GENERALIZATION ON ADDITION
Figure 7: Addition Generalization Performance: The x-axis varies the number of input digits
for the samples in the test set, while the y-axis shows the accuracy. All models are trained on addition
programs with inputs of 1 to 10 digits. NPL-16-1 shows the accuracy of our model when trained
with 16 total samples (per number of digits), of which 1 sample (per number of digits) includes full
program abstractions. NPI-1 and NPI-16 show the accuracy of the NPI model when trained with 1
total samples and 16 total samples respectively (per number of digits), all containing full program
abstractions. S2S-Easy-16 and S2S-Easy-32 show the performance of the S2S-Easy baseline when
trained with 16 and 32 samples respectively (per number of digits).
program throughout the execution. Figure 6 gives an example of a full program trace as well as state
of the scratch pad at a particular timestep.
Experiment Setup: A primary advantage of learning programmatic abstractions over sequences
is an increased generalization capability. To evaluate this, we train our model on samples ranging
from 1 to 10 input digits . The training data contains an equal number of samples of each length
(number of digits), and includes full program abstractions for only one randomly chosen sample
for each length such that |FULL| = 10. We then test NPL using samples containing a much larger
number of digits, ranging up to 1,000. On this task we found that both our model and the original
NPI model were somewhat sensitive to the choice of initial seed, so we sample many different seeds
and report both the mean and standard deviation, using a bootstrapping setup (Efron & Tibshirani
(1994)) which is detailed in Appendix A.6.2.
Compared Models: We originally compared to a standard flat LSTM sequence model. However,
we found that even with 32 samples per digit such a model was not able to fit even the training
data for samples with more than 4 or 5 digits, so we did not present these results.3. Instead, we
compare to a model called S2S-Easy, which is the strongest baseline for this task from (Reed &
de Freitas, 2016). This model is custom-designed for learning addition and so it represents a very
strong baseline. We discuss the model details in Appendix A.6.1. For completeness we also compare
to a reimplementation of NPI in two different training regimes.
Results: Figure 7 shows the generalization capabilities of our model on the ADDITION task. Our
model with “one-shot” strong supervision (NPL-16-1) significantly outperforms the S2S-Easy base-
line even when the baseline is provided twice as many training samples (S2S-Easy-32). This is
particularly notable given that the S2S-Easy model is specifically designed for the addition task.
This result highlights the generalization capabilities our model brings by learning the latent struc-
tures which generate the observed sequences of elementary operations. Furthermore, we can see that
3This is consistent with the findings of Reed & de Freitas (2016)
11
Published as a conference paper at ICLR 2017
these latent structures are learned mostly from the unlabeled sequences, since the vanilla NPI model
trained with only 1 sample per digit (NPI-1) cannot generalize beyond the 10-digit data on which
it was trained. Finally, we can see that just a single fully supervised sample is sufficient since it
enables our model to perform comparably with a vanilla NPI model trained with FULL supervision
for all samples (NPI-16).
5	Related Work
We have already discussed the most relevant past work upon which we directly build: CTC (Graves
et al., 2006), StackRNNs (Joulin & Mikolov, 2015) and NPI (Reed & de Freitas, 2016).
Neural Programs Training neural networks to perform algorithmic tasks has been the focus of
much recent research. This work falls into two main categories: weakly supervised methods that
learn from input-output examples, and strongly supervised methods that additionally have access to
the sequence of elementary actions performed to generate the output.
The work on learning neural programs from input-output data was sparked by the surprising effec-
tiveness of the Neural Turing Machine (NTM) (Graves et al., 2014). Similar to NTMs, many of the
proposed architectures have used differentiable memory (Kurach et al., 2016; Graves et al., 2016;
Weston et al., 2014; Sukhbaatar et al., 2015b; Neelakantan et al., 2016; Gaunt et al., 2016; Feser
et al., 2016), while others have used REINFORCE (Williams, 1992) to train neural networks that
use sampling-based components to model memory access (Andrychowicz & Kurach, 2016; Zaremba
& Sutskever, 2015). Some of this work has considered learning addition from input-output samples,
a similar, but more challenging setup than our Addition domain. Zaremba & Sutskever (2014)
makes use of a few training tricks to enable a standard LSTM to learn to add numbers up to length
9 when training on numbers of the same length. Kalchbrenner et al. (2015) proposes an architec-
ture that is able to learn to add 15-digit numbers when trained on numbers of the same length. The
Neural GPU model from (Kaiser & Sutskever, 2015) learns to add binary numbers 100 times longer
than those seen during training, but requires tens of thousands of training samples and extensive
hyperparameter searches. Additionally, using a decimal instead of binary representation with the
Neural GPU model (as in our Addition task) is also reported to have a significant negative impact
on performance.
The work on learning algorithms from sequence data has utilized both related techniques to ours
as well as tackled related tasks. The most related techniques have augmented RNNs with various
attention and memory architectures. In addition to those we have discussed earlier (Reed & de Fre-
itas, 2016; Joulin & Mikolov, 2015), Grefenstette et al. (2015) proposes an alternative method for
augmenting RNNs with a stack. From a task perspective, the most related work has considered vari-
ants of the scratchpad model for long-hand addition, similar or our Addition domain. This work
has focused largely on more standard RNN architectures, starting with Cottrell & Tsung (1993),
which showed that the standard RNN architectures at the time (Jordan, 1997; Elman, 1990) could
successfully generalize to test samples approximately 5 times as long as those seen during training,
if a few longer samples were included in the training set. More recently, Zaremba et al. (2015)
showed that an RNN architecture using modern LSTM or GRU controllers can perfectly generalize
to inputs 20 times as long as than those seen in the training data when trained in either a supervised
or reinforcement learning setting. However this work was focused on trainability rather than data
efficiency and so they utilized hundreds of thousands of samples for training.
NPI (Reed & de Freitas, 2016) and NPL distinguish themselves from the above work with the
explicit modeling of functional abstractions. These abstractions enable our model, with only 16
samples, to perfectly generalize to data sequences about 100 times as long as those in the training
data. Furthermore, concurrent work (Cai, 2016) has shown that an unmodified NPI model can be
trained to perform more complex algorithms such as BubbleSort, QuickSort and topological sorting
by learning recursive procedures, and we expect that our method can be directly applied to reduce
the amount of needed supervision for these tasks as well.
Reinforcement Learning In the reinforcement learning domain the most related work to ours is
the options framework, for building abstractions over elementary actions (Sutton et al., 1999). This
framework bears many similarities to both our model and to NPI. Specifically, at each time step the
12
Published as a conference paper at ICLR 2017
agent can choose either a one-step primitive action or a multi-step action policy called an option.
As with our procedures, each option defines a policy over actions (either primitive or other options)
and terminates according to some function. Much of the work on options has focused on the tabular
setting where the set of possible states is small enough to consider them independently. More recent
work has developed option discovery algorithms where the agent is encouraged to explore regions
that were previously out of reach (Machado & Bowling, 2016) while other work has shown the
benefits of manually chosen abstractions in large state spaces (Kulkarni et al., 2016). However,
option discovery in large state spaces where non-linear state approximations are required is still
an open problem, and our work can be viewed as a method for learning such options from expert
trajectories.
Much work in reinforcement learning has also considered domains similar to ours. Specifically,
grid-world domains similar to NanoCraft are quite standard environments in the reinforcement
learning literature. One recent example is Sukhbaatar et al. (2015a), which showed that even the
strongest technique they considered struggled to successfully perform many of the tasks. Their
results highlight the difficultly of learning complex tasks in a pure reinforcement learning setup. In
future work we would like to explore the use of our model in setups which mix supervised learning
with reinforcement learning.
6	Conclusion
In this paper, we proposed the Neural Program Lattice, a neural network framework that learns a hi-
erarchical program structure based mostly on elementary operation sequences. On the NanoCraft
and Addition tasks, we show that when training with mostly flat operation sequences, NPL is able
to extract the latent programmatic structure in the sequences, and achieve state-of-the-art perfor-
mance with much less supervision than existing models.
References
Making neural programming architectures generalize via recursion. 2016. Under submission to
ICLR 2017.
Marcin Andrychowicz and Karol Kurach. Learning efficient algorithms with hierarchical attentive
memory. arXiv preprint arXiv:1602.03218, 2016.
Garrison W Cottrell and Fu-Sheng Tsung. Learning simple arithmetic procedures. Connection
Science, 5(1):37-58,1993.
Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC press, 1994.
Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.
John K Feser, Marc Brockschmidt, Alexander L Gaunt, and Daniel Tarlow. Neural functional pro-
gramming. arXiv preprint arXiv:1611.01988, 2016.
Alexander L Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan
Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction.
arXiv preprint arXiv:1608.04428, 2016.
Alex Graves, Santiago Fernandez, FaUstino Gomez, and Jurgen Schmidhuber. Connectionist tem-
poral classification: labelling unsegmented sequence data with recurrent neural networks. In Pro-
ceedings of the 23rd international conference on Machine learning, pp. 369-376. ACM, 2006.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538
(7626):471-476, 2016.
13
Published as a conference paper at ICLR 2017
Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to
transduce with unbounded memory. In Advances in Neural Information Processing Systems, pp.
1828-1836, 2015.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Michael I Jordan. Serial order: A Parallel distributed Processing aPProach. Advances in psychology,
121:471-495, 1997.
Armand Joulin and Tomas Mikolov. Inferring algorithmic Patterns with stack-augmented recurrent
nets. In Advances in Neural Information Processing Systems, PP. 190-198, 2015.
Eukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXivpreprint arXiv:151L08228,
2015.
Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. arXiv preprint
arXiv:1507.01526, 2015.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. ICLR, 2015.
Tejas D Kulkarni, Karthik R Narasimhan, Ardavan Saeedi, and Joshua B Tenenbaum. Hierarchical
deeP reinforcement learning: Integrating temPoral abstraction and intrinsic motivation. NIPS,
2016.
Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. ICLR,
2016.
Marlos C Machado and Michael Bowling. Learning PurPoseful behaviour in the absence of rewards.
arXiv preprint arXiv:1605.07700, 2016.
Microsoft CorP. Redmond WA. Kinect for Xbox 360.
Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural Programmer: Inducing latent Programs
with gradient descent. ICLR, 2016.
Scott Reed and Nando de Freitas. Neural Programmer-interPreters. ICLR, 2016.
Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith Chintala, and Rob Fergus. Maze-
base: A sandbox for learning from games. arXiv preprint arXiv:1511.07401, 2015a.
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances
in neural information processing systems, PP. 2440-2448, 2015b.
Richard S Sutton, Doina PrecuP, and Satinder Singh. Between mdPs and semi-mdPs: A framework
for temPoral abstraction in reinforcement learning. Artificial intelligence, 112(1):181-211, 1999.
Jason Weston, Sumit ChoPra, and Antoine Bordes. Memory networks. arXiv preprint
arXiv:1410.3916, 2014.
Ronald J Williams. SimPle statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.
Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines-revised.
arXiv preprint arXiv:1505.00521, 2015.
Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus. Learning simPle algorithms
from examPles. arXiv preprint arXiv:1511.07275, 2015.
14
Published as a conference paper at ICLR 2017
A Appendix
A. 1 Dataset Details
Table 2 lists the set of programs and elementary operations we used to generate the data for Addi-
tion and NanoCraft. The programs and elementary operations for Addition are identical to
those in Reed & de Freitas (2016). Note that when training with weak supervision the training data
contains only the elementary operations and does not contain the programs or arguments.
Programs	Description	Calls
ADD ADD1 CARRY LSHIFT ACT_WRITE ACT_PTR$OVE	Multi-digit addition Single-digit addition Write carry digit Shift four pointers left Write result to environment Move pointer to left/right	ADD1 ACT _WRITE/CARRY/LSHIFT. ACT _PTR$OVE/ACT _WRITE ACJPTRjOVE Elementary Operation Elementary Operation
NANOCRAFT MOVE ^ANY BUILD-WALL PLACEaND』OVE ACT_MOVE ACT_PLACE_BLOCK	Build a rectangular fence Move multiple steps in one direction Build a wall along one direction Move one step and build a block Move one step to a direction Build a block at current location	MOVE^ANY/BUILD.WALL ACT_MOVE PLACEaND』OVE ACT_MOVE/ACT_PLACE_BLOCK Elementary Operation Elementary Operation
Table 2: Programs, arguments and elementary operations used for generating training data of Ad-
dition and NanoCraft tasks.
A.2 Implementation Details
Here we describe the implementation details of the various component neural networks inside our
implementation of the NPL. Note that the mappings are all the same for both Addition and
NANOCRAFT except for fenc which is task dependent.
•	fenc for ADDITION: We represent the environment observation, (latent) programs and ar-
guments as one-hot vectors of discrete states. We feed the concatenation of one-hot vectors
for environment observation and argument through a linear decoder (with bias) to get a uni-
fied arg-env representation. We then embed the programs (via fembed) into an embedding
space. Finally we feed the concatenation of arg-env vector and program vector through a
2-layer MLP with rectified linear (ReLU) hidden activation and linear decoder.
•	fenc for NANOCRAFT: We represent the environment observation as a grid of discrete
states. Here we first embed each entry into an embedding space, and then feed this embed-
ding through two convolutional layers and two MLP layers with ReLU hidden activation
and linear decoder. We represent argument again as one-hot vectors and embed programs
into an embedding space. Finally we feed the concatenation of argument vectors, convolu-
tional vectors of environment observation and program vector through a 2-layer MLP with
ReLU hidden activation and linear decoder.
•	flstm : We employ a two-layer LSTM cell for the mapping. The size of the hidden states is
set to 128 for both Addition and NanoCraft.
•	fprog : This mapping will map the LSTM hidden state to a probability distribution over
programs. The hidden state output of flstm is mapped through a linear projection to an 8-
dimensional space, and then another linear projection (with bias) with softmax generates
ptg.
•	faction and fop : Each of these encoders will output a probability distribution. We feed the
top hidden states by flstm first through a linear projection (with bias) and then a softmax
function to pta and pto respectively.
15
Published as a conference paper at ICLR 2017
A.3 Normalization
When the operation sequence is too long, yit,l will become vanishingly small as t grows. To prevent
our implementation from underflowing, we follow Graves et al. (2006) by renormalizing yit,l at each
timestep and storing the normalized values and normalization constant separately. The new update
rule becomes:
yt+1,l = Jl < LKpa,,li+1(POP)yt,l+1 + J0 < iKpa,,li-ι(θP)pO(λO)yt-1 + J0 < lKpa∖-1(PUSH)y^l-1,
and we normalize the values and maintain a log-summation of the normalization constants
γt = γt-1 + iog(Xyi,1),	yt = ytil/Xyt,l∙
i,l	i,l
Then the original update for yt+1 becomes
log(yt+1) = log_sum_exp(log(yt),log(pI，0(POP)) + log(yI,0) + Yt),
the computation of which can be done robustly.
A.4 Update Equations with B oundary Conditions
In Section 3.3 we did not include the boundary conditions in our discussion to improve the read-
ability. Our implementation, however, must account for the bounds on l, and i, as shown in Iverson
brackets in the full update equations below:
yit+1,l =Jl < LKpta,,li+1(POP)yit,l+1 + J0 < iKpta,,li-1(OP)pto,,li-1(λio)yit-,l1
+ J0 < lKpta,,li-1(PUSH)yit,l-1
αit,l (a) =(yit,l/yit+1,l )pta,,li (a)
,Jl < LKα^l+1 (POP)Mt,li+1 +
Mt+ι,ι = < Jl<LWl+1(P0P)M2,i+1+
Jl < LKαi,l+1(POP)Md++,i+
(Jl<LKαi,l+1(POP)St,,i+1 +
qt,^l - J
Sd,i = (( Jl < LKαit,l+1(POP)Sdt,+l+1,1i+
J0 < iKαit-,l 1(OP)pto,,li-1(λio)hto,ult,i-1+
J0 < lKαit,l-1 (PUSH)0,	d = 0
J0<iKαit-,l1(OP)pto,,li-1(λio)M1t,,li-1+
J0 < lKαit,l-1(PUSH)hto,ul-t,i1-1,	d= 1
J0 < iKαit-,l 1(OP)pto,,li-1(λio)Mdt,,li-1+
J0 < lKαit,l-1(PUSH)Mdt,-l-1,1i,	d> 1
J0 < iKαit-,l1(OP)pto,,li-1(λio)S0t,,li-1+
J0 < lKαit,l-1(PUSH)got,ul-t,i1,	d=0
J0<iKαit-,l1(OP)pto,,li-1(λio)Sdt,,li-1+
J0 < lKαit,l-1(PUSH)Sdt,-l-1,1i,	d> 0
A.5 Experimental Settings
As mentioned before, NPL can be trained jointly with full program abstractions (referred to as
FULL) as well as elementary operation sequences (referred to as OP). When training with FULL
samples, the training procedure is similar to that for NPI and we use this setting as one of our
baselines. For each dataset on which we test NPL, we include mostly OP samples with only a small
number of FULL samples. We pre-train the model solely on FULL samples for a few iterations to
get a good initialization. After that, in each step we train with a batch of data purely from FULL or
OP based on their proportions in the dataset and generate the parameter update in that step using the
corresponding objective. For all tasks, we train the NPL using ADAM (Kingma & Ba, 2015) with
base learning rate of 10-4 and batch size of 1. We decay the learning rate by a factor of 0.95 every
10,000 iterations. These settings were chosen using a manual search based on performance on the
validation data.
16
Published as a conference paper at ICLR 2017
A.6 Experimental Details for Addition
A.6.1 S2S-Easy BASELINE
In our initial seq2seq baseline tests for Addition we represented the data for 90 + 160 = 250 as
the sequence: 90X160X250 However, we found that such a model was not able to fit the training
data even when trained with 32 samples per number of digits. So we instead compared to the much
stronger S2S-Easy baseline presented in Reed & de Freitas (2016). This baseline makes it much
easier to learn addition through the following two modifications to the model: 1) reverse input digits,
and 2) generate reversed output digits immediately at each time step, such that the data sequence
looks like: output: 052 input 1: 090 input 2: 061 This model is quite specific to the Addition task
(and would not work on the NanoCraft task for instance) and results in a very strong baseline.
None-the-less, as we showed in Figure 7, our model still significantly outperforms this baseline.
A.6.2 Bootstrapping
On the Addition task we found that both our model and the original NPI model were somewhat
sensitive to the choice of initial seed. To test this sensitivity we ran our experiments for this task
using a bootstrapping process (Efron & Tibshirani, 1994). We ran all models using 100 different
seeds for each model. We then sampled 25 seed subsets, with replacement. For each subset, we
choose the best seed using a validation set which was one-quarter the size of the original dataset,
but consisted only of 10-digit samples. We performed this resampling procedure 100 times, and in
Figure 7 we report the mean and standard deviation across the resampled seed sets.
17