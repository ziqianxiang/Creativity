title,year,conference
 Theloss surface of multilayer networks,2014, arXiv preprint arXiv:1412
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in Neural Information Processing Systems
 Rmsprop and equilibratedadaptive learning rates for non-convex optimization,2015, arXiv preprint arXiv:1502
 New types of deep neural network learning for speechrecognition and related applications: An overview,2013, In Proc
 Decaf: A deepconvolutional activation feature for generic visual recognition,2014, In Proc
 Local minima and plateaus in hierarchical structures ofmultilayer perceptrons,2000, Neural Networks
 Deep pyramidal residual networks,2016, arXiv preprintarXiv:1610
 Deep residual learning for image recog-nition,2015, arXiv preprint arXiv:1512
 Identity mappings in deep residualnetworks,2016, arXiv preprint arXiv:1603
 Deep networks with stochas-tic depth,2016, arXiv preprint arXiv:1603
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convolutional neuralnetworks,2012, In Proc
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 SGDR: Stochastic Gradient Descent with Restarts,2016, arXiv preprintarXiv:1608
 Restart procedures for the conjugate gradient method,1977, Mathematicalprogramming
 Niching the CMA-ES via nearest-better clustering,2010, In Proceedings of the 12th annualconference companion on Genetic and evolutionary computation
 No more pesky learning rate guessing games,2015, arXiv preprint arXiv:1506
 Cyclical learning rates for training neural networks,2016, arXiv preprintarXiv:1506
 Stochastic subgradient methods with linear convergence for polyhe-dral convex optimization,2015, arXiv preprint arXiv:1510
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Adadelta: An adaptive learning rate method,2012, arXiv preprint arXiv:1212
