title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In ICLR
 Long short-term memory-networks for machinereading,2016, In EMNLP
 Attention-over-attentionneural networks for reading comprehension,2016, arXiv preprint arXiv:1607
 Consensus attention-basedneural networks for chinese reading comprehension,2016, arXiv preprint arXiv:1607
 Gated-attentionreaders for text comprehension,2016, arXiv preprint arXiv:1606
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Dynamic neural turingmachine with soft and hard addressing schemes,2016, arXiv preprint arXiv:1607
 The goldilocks principle: Readingchildrenâ€™s books with explicit memory representations,2016, In ICLR
 Long short-term memory,1997, Neural computation
 Blackout:SPeeding uP recurrent neural network language models with very large vocabularies,2016, In ICLR
 ExPloring thelimits of language modeling,2016, arXiv preprint arXiv:1602
 Text understanding with theattention sum reader network,2016, In ACL
 Adam: A method for stochastic oPtimization,2015, In ICLR
 Recurrentneural network based language model,2010, In Interspeech
 Empiricalevaluation and combination of advanced language modeling techniques,2011, In Interspeech
 Key-value memory networks for directly reading documents,2016, arXiv preprintarXiv:1606
 On the difficulty of training recurrent neuralnetworks,2013, In ICML
 Neural programmer-interpreters,2015, arXiv preprint arXiv:1511
 Learning internal representations byerror propagation,1985, Technical report
 A neural attention model for abstractivesentence summarization,2015, In EMNLP
 Sparse non-negative matrix language modelingfor skip-grams,2015, In Interspeech
 Higher order recurrent neural networks,2016, arXiv preprintarXiv:1605
 Learning matrices and their applications,1963, IEEE Transactions onElectronic Computers
 Natural language comprehensionwith the epireader,2016, arXiv preprint arXiv:1606
 Backpropagation through time: what it does and how to do it,1990, Proceedings of theIEEE
 Memory networks,2015, In ICLR
 Scaling recurrentneural network language models,2015, In 2015 IEEE International Conference on Acoustics
 Reference-aware language models,2016, arXivpreprint arXiv:1611
