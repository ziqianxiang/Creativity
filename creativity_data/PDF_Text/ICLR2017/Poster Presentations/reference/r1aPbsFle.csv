title,year,conference
 On the propertiesof neural machine translation: Encoder-decoder approaches,2014, arXiv PrePrint arXiv:1409
 Learn-ing with a wasserstein loss,2015, In Advances in Neural Information Processing Systems
 A theoretically grounded application of dropout in recurrent neural networks,2015, arXivPrePrint arXiv:1512
 Distilling the knowledge in a neural network,2015, arXivPrePrint arXiv:1503
 Long short-term memory,1997, Neural computation
 Improved learning through augmenting the loss,2016, Stanford CS224D: Deep Learning for Natural Language Processing
 Essai sur la geometrie a n dimensions,1875, Bulletin de la Societe mathematique deFrance
 Character-aware neural languagemodels,2015, arXiv preprint arXiv:1508
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational linguistics
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Recurrentneural network based language model,2010, In Interspeech
 Abstractive text summarizationusing sequence-to-sequence rnns and beyond,2016, 2016
 HoW to construct deeprecurrent neural networks,2013, CoRR
 On the difficulty of training recurrent neuralnetworks,2013, ICML (3)
 Glove: Global vectors for Wordrepresentation,2014, In EMNLP
 Using the output embedding to improve language models,2016, arXiv preprintarXiv:1608
 A neural attention model for abstractivesentence summarization,2015, arXiv preprint arXiv:1509
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, Citeseer
 Recurrenthighway networks,2016, arXiv preprint arXiv:1607
