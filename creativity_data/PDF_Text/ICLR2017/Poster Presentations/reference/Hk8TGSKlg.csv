title,year,conference
 A capacity theory of comprehension: individual differencesin working memory,1992, Psychological review
 Cognitive modeling,2002, MIT Press
 Neural semantic encoders,2016, arXiv preprint arXiv:1607
 Teaching machines to read and comprehend,2015, In NIPS
 The goldilocks principle: Readingchildrenâ€™s books with explicit memory representations,2015, arXiv preprint arXiv:1511
 Who did what: Alarge-scale person-centered cloze dataset,2016, In EMNLP 2016
 Natural language comprehensionwith the epireader,2016, arXiv preprint arXiv:1606
 Iterative alternating neural attention formachine reading,2016, arXiv preprint arXiv:1606
 Text understanding with theattention sum reader network,2016, In ACL 2016
 End-to-end memory networks,2015, In NIPS2015
 Gated-attentionreaders for text comprehension,2016, arXiv preprint arXiv:1606
 Attention-over-attention neural networks for reading comprehension,2016, arXiv preprint arXiv:1607
 Reasonet: Learning to stop readingin machine comprehension,2016, arXiv preprint arXiv:1609
 Neural random-access machines,2016, ICLR2016
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
