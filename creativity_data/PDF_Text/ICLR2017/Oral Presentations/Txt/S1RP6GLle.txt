Published as a conference paper at ICLR 2017
Amortised MAP Inference for
Image Super-resolution
Casper Kaae Sonderby12: Jose Caballero1, Lucas Theis1, Wenzhe Shi1& Ferenc Huszar1
casperkaae@gmail.com, {jcaballero,ltheis,wshi,fhuszar}@twitter.com
1 Twitter, London, UK
2University of Copenhagen, Denmark
Ab stract
Image super-resolution (SR) is an underdetermined inverse problem, where a large
number of plausible high resolution images can explain the same downsampled
image. Most current single image SR methods use empirical risk minimisation,
often with a pixel-wise mean squared error (MSE) loss. However, the outputs from
such methods tend to be blurry, over-smoothed and generally appear implausible.
A more desirable approach would employ Maximum a Posteriori (MAP) infer-
ence, preferring solutions that always have a high probability under the image
prior, and thus appear more plausible. Direct MAP estimation for SR is non-
trivial, as it requires us to build a model for the image prior from samples. Here
we introduce new methods for amortised MAP inference whereby we calculate the
MAP estimate directly using a convolutional neural network. We first introduce a
novel neural network architecture that performs a projection to the affine subspace
of valid SR solutions ensuring that the high resolution output of the network is
always consistent with the low resolution input. Using this architecture, the amor-
tised MAP inference problem reduces to minimising the cross-entropy between
two distributions, similar to training generative models. We propose three methods
to solve this optimisation problem: (1) Generative Adversarial Networks (GAN)
(2) denoiser-guided SR which backpropagates gradient-estimates from denoising
to train the network, and (3) a baseline method using a maximum-likelihood-
trained image prior. Our experiments show that the GAN based approach per-
forms best on real image data. Lastly, we establish a connection between GANs
and amortised variational inference as in e. g. variational autoencoders.
1	Introduction
Image super-resolution (SR) is the underdetermined inverse problem of estimating a high resolution
(HR) image given the corresponding low resolution (LR) input. This problem has recently attracted
significant research interest due to the potential of enhancing the visual experience in many appli-
cations while limiting the amount of raw pixel data that needs to be stored or transmitted. While
SR has many applications in for example medical diagnostics or forensics (Nasrollahi & Moeslund,
2014, and references therein), here we are primarily motivated to improve the perceptual quality
when applied to natural images. Most current single image SR methods use empirical risk minimi-
sation, often with a pixel-wise mean squared error (MSE) loss (Dong et al., 2016; Shi et al., 2016).
However, MSE, and convex loss functions in general, are known to have limitations when presented
with uncertainty in multimodal and nontrivial distributions such as distributions over natural im-
ages. In SR, a large number of plausible images can explain the LR input and the Bayes-optimal
behaviour for any MSE trained model is to output the mean of the plausible solutions weighted ac-
cording to their posterior probability. For natural images this averaging behaviour leads to blurry
and over-smoothed outputs that generally appear implausible, i.e. the produced estimates have low
probability under the natural image prior.
An idealised method for our applications would use a full-reference perceptual loss function that
describes the sensitivity of the human visual perception system to different distortions. However the
* Work done while CKS WaS an intern at TWitter
1
Published as a conference paper at ICLR 2017
cross-entropy	H[qθ, pY]	val-
Figure 1: Illustration of the SR problem via a toy example. Two-dimensional ues. The AffGAN and AffDG
HR data y = [y1 , y2] is drawn from a Swiss-roll distribution (in gray). achieves cross-entropy values
Downsampling is modelled as X = y1 + y2. a) Given observation X = 0.5, close to the MAP solution con-
valid SR solutions lie along the line y2 = 1 - yι (-). The red shading firming that they minimize the
H[qo, Py]	'mse (X, Ay)
MAP	3.15	-
MSE	9.10	1.25 ∙ 10-2
MAE	6.30	4.04 ∙ 10-2
AffGAN	4.10	0.0
SoftGAN	4.25	8.87 ∙ 10-2
AffDG	3.81	0.0
SoftDG	4.19	1.01 ∙ 10-1
Table 1: Directly estimated
illustrates the magnitude of the posterior pY |X=0.5. Bayes-optimal estimates desired quantity. The MSE and
under MSE and MAE as well as the MAP estimate given X = 0.5 are marked MAE models performs worse
with labels. The MAP estimates for different values of X ∈ [-8, 8] are also since they do not minimize
shown (- •一). b) Trained model outputs for X ∈ [-8,8] and estimated gradi- the cross-entropy. Further the
ents from a denoising function trained on PY. Note the AffGAN(- ∙- ) and models using affine projections
AffDG(- ♦- ) models fit the posterior mode well whereas the MSE (- ♦- ) (Aff) performs better than the
and MAE (- ♦- ) model outputs generally fall in low probability regions. soft constrained models.
most widely used loss functions MSE and the related peak-signal-to-noise-ratio (PSNR) metric have
been shown to correlate poorly with human perception of image quality (Laparra et al., 2016; Wang
et al., 2004). Improved perceptual quality metrics have been proposed, the most popular being
structural similarity (SSIM) (Wang et al., 2004) and its multi-scale variants (Wang et al., 2003).
Although the correlation of these metrics with human perception has improved, they still do not
provide a fully satisfactory alternative to MSE for training of neural networks (NN) for SR.
In lieu of a satisfactory perceptual loss function, we leave the empirical risk minimisation framework
and present methods based only on natural image statistics. In this paper we argue that a desirable
approach is to employ amortised Maximum a Posteriori (MAP) inference, preferring solutions that
have a high posterior probability and thus high probability under the image prior while keeping the
computational benefits of amortised inference. To motivate why MAP inference is desirable consider
the toy problem in Figure 1a, where the HR data is two-dimensional y = [y1 , y2] and distributed
according to the Swiss-roll density. The LR observation is defined as the average of the two pixels
X = y1 + y2. Consider observing a LR data point X = 0.5: the set of possible HR solutions is the
line y1 = 2x - y2 , more generally an affine subspace, which is shown by the dashed line in Figure
1a. The posterior distribution p(y|X) is thus degenerate, and corresponds to a slice of the prior along
this line, as shown by the red shading. If one minimise MSE or Mean Absolute Error (MAE), the
Bayes-optimal solution will lie at the mean or the median along the line, respectively. This example
illustrates that MSE and MAE can produce output with very low probability under that data prior
whereas MAP inference would always find the mode which by definition is in a high-probability
region. See Section 5.6 for a discussion of possible limitations of the MAP inference approach.
Our first contribution is a convolutional neural networks (CNN) architecture designed to exploit the
structure of the SR problem. Image downsampling is a linear transformation, and can be modelled as
a strided convolution. As Figure 1a illustrates, the set of HR images y that are compatible with any
LR image X span an affine subspace. We show that by using specifically chosen linear convolution
and deconvolution layers we can implement a projection to this affine subspace. This ensures that our
CNNs always output estimates that are consistent with the inputs. The affine projection layer can be
added to any CNN, or indeed, any other trainable SR algorithm. Using this architecture we show that
training the model for MAP inference reduces to minimising the cross-entropy H[qG , pY ] between
the HR data distribution pY and the implied distribution qG of the model’s output when evaluated
at random LR images. As a result, we don’t need corresponding HR and LR image pairs any more,
and training becomes more akin to training generative models. However direct minimisation of the
cross-entropy is not possible and instead we develop three approaches, all depending on projecting
the model output to the affine subspace of valid solution, to approximate it directly from data:
2
Published as a conference paper at ICLR 2017
1.	We present a variant of the Generative Adversarial Networks (GAN) (Goodfellow et al., 2014)
which approximately minimises the KUllback-Leibler divergence (KL) and cross-entropy be-
tween qG and pY . Our analysis provides theoretical grounding for using GANs in image SR
(Ledig et al., 2016). We also introdUce a trick that we call instance noise that can be generally
applied to address the instability of training GANs.
2.	We employ denoising as a way to captUre natUral image statistics. Bayes-optimal denoising
approximately learn to take a gradient step along the log-probability of the data distribUtion
(Alain & Bengio, 2014). These gradient estimates from denoising can be directly backpropagated
throUgh the network to minimise cross-entropy between qG and pY via gradient descent.
3.	We present an approach where the probability density of data is directly modelled via a generative
model trained by maximUm likelihood. We Use a differentiable generative model based on Pixel-
CNNs (Oord et al., 2016) and MixtUre of Conditional GaUssian Scale MixtUres (MCGSM, Theis
et al., 2012) whose performance we believe is very close to the-state-of-the-art in this category.
In section 5 we empirically demonstrate the behavioUr of the proposed methods on both the two
dimensional toy dataset and on real image datasets. Lastly, in Appendix F we show that a stochastic
version of AffGAN performs amortised variational inference, which for the first time establishes a
connection between GANs and variational inference as in e. g. variational aUtoencoders (Kingma &
Welling, 2014).
2	Related work
The GAN framework was introdUced by Goodfellow et al. (2014) which also showed that these
models minimise the Shannon-Jensen Divergence between qG and pY Under certain conditions. In
Section 3.2, we present an Update rUle that corresponds to minising KL[qGkpY]. Recently, Nowozin
et al. (2016) presented a more general treatment that connects GANs to f -divergence minimisation.
In parallel to oUr contribUtions, theoretical work by Mohamed & Lakshminarayanan (2016) pre-
sented a Unifying view on learning in GAN-style algorithms, of which oUr variant can be regarded a
special case. The focUs of several recent papers on GANs were algorithmic tricks to improve their
stability (Radford et al., 2015; Salimans et al., 2016). In Section 3.2.1 we introdUce another sUch
trick we call instance noise. We discUss theoretical motivations for this and compare it to one-sided
label smoothing proposed by Salimans et al. (2016). We also refer to parallel work by Arjovsky &
BottoU (2017) proposing a similar method. Recently, several attempts have been made to improve
perceptUal qUality of SR Using deep representations of natUral images. BrUna et al. (2016) and Li &
Wand (2016) measUre the EUclidean distance in the nonlinear featUre space ofa deep NN pre-trained
to perform object classification. Dosovitskiy & Brox (2016) and Ledig et al. (2016) Use a similar
approach and also add an adversarial loss term. UnpUblished work by Garcia (2016) explored com-
bining GANs with an L1 penalty between the LR inpUt and the down-sampled oUtpUt. We note
that the soft L2 or L1 penalties Used in these methods can be interpreted as assUming GaUssian and
Laplace observation noise. In contrast, oUr approach assUmes no observation noise and satisfies
the consistency of inpUts and oUtpUts exactly by Using an affine projection as explained in Section
3.1. In other work, Larsen et al. (2015) proposed to replace the pixel-wise MSE Used for training
of variational aUtoencoders with a learned metric from the GAN discriminator. OUr denoiser based
method exploits a fUndamental connection between probabilistic modelling and learning to denoise
(see e. g. Vincent et al., 2008; Alain & Bengio, 2014; Sarela & Valpola, 2005; Rasmus et al., 2015;
Greff et al., 2016): a Bayes-optimal denoiser can be Used to estimate the gradient of the log proba-
bility of data. To our knowledge this work is the first time that the output of a denoiser is explicitly
back-propagated to train another network. Lastly, we note that denoising has been used to solve
inverse problems in compressed sensing as in approximate message passing (Metzler et al., 2015).
3	Theory
Consider a function fθ (x) parametrised by θ which maps a LR observation X to a HR estimate y.
Most current SR methods optimise model parameters via empirical risk minimization:
argminEy,χ['(y, fθ(x))]	(1)
θ
Where y is the true target and ` is some loss function. The loss function is typically a simple convex
function most often MSE 'mse(", y) = ∣∣y - yk2 as in (Dong et al., 2016; Shi et al., 2016). Here,
3
Published as a conference paper at ICLR 2017
we seek to perform MAP inference instead. For a single LR observation the MAP estimate is
y(x) = argmax log PY X (y|x)	(2)
y
Instead of calculating y for each X separately We perform amortised inference, i. e. We would like to
train the SR function fθ(x) to calculate the MAP estimate. A natural loss function for learning the
parameters θ is the average log-posterior:
argmax Ex log PY |X (fθ (x)|x),	(3)
θ
Where the expectation is taken over the distribution of LR observations x. This loss depends on the
unknoWn posterior distribution PY |X . We proceed by decomposing the log-posterior using Bayes’
rule as folloWs.
argmax ExlogPX∣γ(x∣fθ(x))+ ExlogPY(fθ(x)) - ExlogPX(x)	.	(4)
θ I ×------------{z---------} S--------{--------} S-------{----} I
Likelihood	Prior	Marginal Likelihood
3.1	Handling the Likelihood term
Notice that the last term of Eqn. (4), the marginal likelihood, does not depend on θ, so We only
have to deal With the likelihood and image prior. The observation model in SR can be described as
folloWs.
x = A]^,	(5)
Where A is a linear transformation used for image doWnsampling. In general, A can be modelled
as a strided tWo-dimensional convolution. Therefore, the likelihood term in Eqn. (4) is degenerate
p(χ∣fθ(x)) = δ(x - Afθ(x)), and Eqn. (4) can be rewritten as constrained optimisation:
argmax Ex [log PY (fθ (x))]	(6)
θ
∀x: Afθ (x)=x
To satisfy the constraints, we introduce a parametric function class that always guarantees Afθ (x) =
x. Specifically, we propose to use functions of the form
gθ(x) = ΠxAfθ(x) = (I - A+A)fθ(x) +A+x	(7)
where fθ is an arbitrary mapping from LR to HR space, ΠxA a projection to the affine subspace
{y : yA = x}, and A+ is the Moore-Penrose pseudoinverse of A, which satisfies AA+A = A and
A+AA+ = A+ . Conveniently, if A is a strided two-dimensional convolution, then A+ becomes
a deconvolution or up-convolution, which is a standard operation used in deep learning (e. g. Shi
et al., 2016). It is important to stress that the optimal deconvolution A+ is not simply the transpose
of A, Figure 2 illustrates the upsampling kernel (A+) that corresponds to a Gaussian downsampling
kernel (A). For any A the deconvolution A+ can be easily found, here we used numerical methods
as detailed in Appendix B. Intuitively, A+x can be thought of as a baseline SR solution, while
(I - A+A)fθ is the residual. The operation (I - A+A) is a projection to the null-space of A,
therefore when we downsample the residual (I - A+A)fθ we are guaranteed to get 0 no matter
what fθ is. By using functions of this form we can turn Eqn. (6) into an unconstrained optimization
problem.
argmaxExlogPY(ΠxAfθ(x))	(8)
θ
Interestingly, the objective above can be expressed in terms of the probability distribution of the
model output qθ(y) := δ y - ΠxA fθ(x) PX (x)dx as follows.
argmaxExlogPY(∏Afθ(x)) = argmaxEy〜q@ logPY(y) = argminH[qθ,Py],	(9)
θ	θθ
where H[q, P] denotes the cross-entropy between q and P and we used H[qθ, PY] =
Ey〜qθ [- logPY(y)]. To minimise this objective, we do not need matched input-output pairs as
in empirical risk minimisation. Instead we need to match the marginal distribution of reconstructed
images qθ to that of the distribution of HR images. In this respect, the problem becomes more
akin to unsupervised learning or generative modelling. In the following sections we present three
approaches to finding the optimal θ utilising the properties of the affine projection.
4
Published as a conference paper at ICLR 2017
3.2	Affine projected Generative Adversarial Networks
Generative Adversarial Networks (Goodfellow et al., 2014) consist of a generator G that turns noise
sampled from some distribution Z 〜PZ into images G(Z) via a parametric mapping, and a dis-
criminator D that learns to distinguish between real and synthetic images. The generator and dis-
criminator are updated in tandem resulting in the generative distribution qG moving closer to the
distribution of real data pY . The behaviour of GANs depends on the specifics of how the generator
and the discriminator are trained. We use the following objective functions for D and G:
L(D； G) = -Ey~pγ log D(y) - Ez~pz Iog(I- D(G(Z)),	(10)
L(G; D) = -Ez~pz log I DDGG)) .
1 - D(G(Z))
The algorithm iterates two steps: first, it updates D by lowering L(D; G) keeping G fixed, then it
updates G by lowering L(G; D) keeping D fixed. It can be shown that this amounts to minimising
KL[qGkpY ], where qG is the distribution of samples generated by G. See Appendix A for a proof1
In the context of SR, the affine projected SR function ΠxAfθ takes the role of the generator. Instead of
noise, the generator is now fed low-resolution images X 〜PX. Leaving everything else unchanged,
we can deploy the GAN algorithm to minimise KL[qθ kpY ]. We call this algorithm affine projected
GAN or AffGAN for short. Similarly, we introduce notation SoftGAN to denote the GAN algorithm
without the affine projection, which instead uses an additional soft-constraint 'lr = MAE(χ, Ay)
as in (Garcia, 2016). Note that the difference between the cross-entropy and the KL divergence
is the entropy of qθ: H[qθ,PY] - KL[qθkPY] = H[qθ]. Hence, we can expect AffGAN to favour
approximate MAP solutions that lead to higher entropy and thus more diverse solutions overall.
3.2.1	Instance Noise
The theory suggests that GANs should be a convergent algorithm. Ifa unique optimal discriminator
exists and it is reached by optimising D to perfection at each step, technically the whole algorithm
corresponds to gradient descent on an estimate ofKL[qθkPY] with respect to θ. In practice, however,
GANs tend to be highly unstable. So where does the theory go wrong? We think the main reason for
the instability of GANs stems from qθ and PY being concentrated distributions whose support does
not overlap. The distribution of natural images PY is often assumed to concentrate on or around a
low-dimensional manifold. In most cases, qθ is degenerate and manifold-like by construction, such
as in AffGAN. Therefore, odds are that especially before convergence is reached, qθ and PY can
be perfectly separated by several Ds violating a condition for the convergence proof. We try to
remedy this problem by adding instance noise to both SR and true image samples. This amounts to
minimising the divergence dσ(qθ,pγ) = KL [pσ * qθ∣∣Pσ * PY], wherep * qθ denotes convolution
ofqθ with the noise distribution Pσ. The noise level σ can be annealed during training, and the noise
allows us to safely optimise D until convergence in each iteration. The trick is related to one-sided
label noise introduced by Salimans et al. (2016), however without introducing a bias in the optimal
discriminator, and we believe it is a promising technique for stabilising GAN training in general.
For more details please see Appendix C
3.3	Denoiser Guided Super-Resolution
To optimise the criterion Eqn. (6) via gradient descent we need its gradient with respect to θ:
∂	∂	∂
∂θEx[logp(πAfθ(X))] = Ex ∂ logp(y)	∙ πχ ∂θfθ(X)	(II)
∂θ	∂y	y=ΠxAfθ (x)	∂θ
Here 另 fθ are the gradients of the SR function which can be calculated via back-propagation
whereas ∂y logPY (y) requires estimation since PY is unknown. We use results from (Alain &
Bengio, 2014; Sarela & Valpola, 2005) showing that in the limit of infinitesimal Gaussian noise,
optimal denoising functions can be used to estimate this gradient:
fσ = argmin Ey~pγ 'mse(f (y + σe),y) =⇒ f-(y)-y ≈ dy logPY(y),	(12)
1First shown in (HUSzdr, 2016).
5
Published as a conference paper at ICLR 2017
where e 〜N(0, I) is Gaussian white noise, fσ is the Bayes-optimal denoising function for noise
level σ. Using these results we can maximise Eqn. (9) by first training a neural network to denoise
samples from pY and then backpropagate the gradient estimates from Eqn. (12) via the chain rule in
Eqn. (11) to update θ. Well call this method AffDG, as it uses the affine subspace projection and is
guided by the gradient from the DAE. Similar to above we’ll call the similar algorithm soft-enforcing
Eqn. (5) SoftDG.
3.4	Density Guided Super-Resolution
As a more direct baseline model for amortised MAP inference we fit a tractable, yet powerful density
model to pY using maximum likelihood, and then use cross entropy with respect to the generative
model to approximate Eqn. (9). We use a deep generative model similar to the pixelCNN (Oord
et al., 2016) but with a continuous (and differentiable) MCGSM (Theis et al., 2012) likelihood.
These type of models are state-of-the-art in density estimation, are relatively fast to evaluate and
produce visually interesting samples (Oord et al., 2016). We call this method AffLL, as it uses the
affine projection and is guided by the log-likelihood of a density model.
4	Experiments
We designed our experiments to address the following questions:
• Are the methods proposed in Section 3 successful at minimising cross-entropy? → Section 5.1
• Does the affine projection layer hurt the performance of CNNs for image SR? → Section 5.2
• Do the proposed methods produce perceptually superior SR results? → Sections 5.3-5.5
We initially illustrate the behaviour of the proposed algorithms on data where exact MAP infer-
ence is computationally tractable. Here the HR data y = [y1, y2] is drawn from a two-dimensional
noisy Swiss-roll distribution and the one-dimensional LR data x is simply the average of the two
HR pixels. Next we tested the proposed algorithm in a series of experiments on natural images
using 4× downsampling.. For the first dataset, we took random crops from HR images containing
grass texture. SR of random textures is known to be very hard using MSE or MAE loss func-
tions. Finally, we tested the proposed models on real image data of faces (Celeb-A) and natural
images (ImageNet). All models were convolution neural networks implemented using Theano
(Team et al., 2016) and Lasagne (Dieleman et al., 2015). We refer to Appendix D for full experi-
mental details.
5	Results and Discussion
5.1	2D MAP inference: Swiss-Roll
In this experiment we wanted to demonstrate that AffGAN and AffDG are indeed minimising the
MAP objective in Eqn. (9). For this we used the two-dimensional toy problem where pY can be
evaluated using brute-force Monte Carlo. Figure 1b) shows the outputs for x = [-8, 8] for models
trained with different criterion. The AffGAN and AffDG solutions largely fit the dominant mode
similar to MAP inference. For the MSE and MAE models the output generally falls in regions
with low prior density. Table 1 shows the cross-entropy H[qθ , pY ] achieved by different methods,
averaged over 10 independent trials with random initialisation. The cross-entropy values for the
GAN and DAE based models are relatively close to the optimal MAP solution, which in this case
we can find in a brute-force way. As expected the MSE and MAE models perform worse as these
models do not minimize H[qθ , pY ]. We also calculated the average MSE between the network
input and the downsampled network output. For the affine projected models, this error is exactly 0.
The soft constrained models only approximately satisfy this constraint, even after extensive training
(Table 1 second column). Further, we observe that the affine projected models generally found a
lower cross-entropy H[qθ,pY] when compared to soft-constrained versions.
5.2	Affine Projected Networks: Proof of Concept using MSE criterion
Adding the affine projection ΠxA restricts the class of functions that the SR network can model,
so it is important to verify that the network is still capable of achieving the same performance in
6
Published as a conference paper at ICLR 2017
0.004
0.003
0.002
0.91
0.89
0.87
0.85 ɪk---1---1----r
13	5	7
Samples
Figure 2: CelebA performance for MSE models during training. The distance between HR model output y and
true HR image y using MSE in a) and SSIM in b). MSE in LR space between input x and down-sampled model
output Ay in c). The tuple in the legend indicate: ((F)ixed / (T)rainable affine projection, (T)rained / (R)andom
initialised affine projections). The models using pre-trained affine projections (fixed:---, trainable:----)
always performs better in all metrics compared to models using either random initialized affine projections
(----)or no projection (------). Further, a fixed pre-trained affine projection ensures the best consistency
between input and down-sampled output as seen in figure c). A (top) and A+ (bottom) kernels of the affine
projection are seen in d).
SR as unconstrained CNN architectures. To test this, we trained CNNs with and without affine
projections to perform SR on the CelebA dataset using MSE as the objective function. Results
are shown in Figure 2. First note that when using affine projections, a randomly initialised network
starts learning from a lower initial loss as the low-frequency components of the network output
already match those of the target image. We observed that the affine projected networks generally
train faster than unconstrained ones. Furthermore, the affine projected networks tend to find a better
solution as measured by MSE and SSIM (Figure 2a-b). To investigate which aspects of the network
architecture are responsible for the improved performance, we evaluated two further models: In one
variant, we initialise the affine projected CNN to implement the correct projection, but then treat
A+ as a trainable parameter. In the final variant, we keep the architecture the same, but initialise
the final deconvolution layer A+ randomly and allow it to be trained. We found that initialising
A+ to the correct Moore-Penrose inverse is important, and we get the similar results irrespective of
whether or not it is fixed during training. Figure 2c shows the error between the network input and
the downsampled network output. We can see that the exact affine projected network keeps this error
at virtually 0.0 (up to numerical precision), whereas any other network will violate this consistency.
In Figure 2d we show the downsampling kernel A and the corresponding optimal kernel for A+ .
5.3	Grass Textures
Random textures are known to be hard model using MSE loss function. Figure 3 shows 4× SR
of grass texture patches using identical affine projected CNNs trained with different loss functions.
When randomly initialised, affine projected CNNs always produce an output with the correct low-
frequency components,as illustrated by the third panel labelled Affinit in Figure 3. The AffGAN
model produces clearly the sharpest images, and we found the images to be plausible given the LR
inputs. Notice that the reconstruction is not perfect pixel-by-pixel, but it has the correct statistical
properties for the human visual system to recognise it as grass texture. The AffDG and AffLL mod-
els both produced blurry results which we where unable to improve upon using various optimization
methods. Due to these findings we choose not to perform any further experiments with these models
and concentrate on AffGAN instead. We refer to Appendix E for discussion of the results of these
models.
5.4	CelebA Faces
In Figure 4 the SR results are seen for several models trained using different loss functions. The
MSE trained models outputs somewhat generic and over-smoothed images as expected. For the
GAN models the global content is correct for both the affine projected and soft constrained models.
Comparing the AffGAN and SoftGAN outputs the AffGAN model produces slightly sharper images
7
Published as a conference paper at ICLR 2017
Figure 3: 4× SR of grass textures. Top row shows LR model input x, true HR image y and model outputs
according to figure legend. Bottom row shows zoom in on except from the images in the top row. The AffGAN
image is much sharper than the somewhat blurry AffMSE image. Note that both the AffDG and AffLL produces
very blurry results. The Affinit shows the output from an untrained affine projected model, i. e. the baseline
solution, illustrating the effect of the upsampling using A+ .
which however also seem to contain slightly more high frequency noise. We observed some colour
drifting for the soft constrained models. Table 2 shows quantitative results for the same four models
where, in terms of PSNR and SSIM, the MSE model achieves the best scores as expected. The
consistency between input and output clearly shows that the models using the affine projections
satisfy Eqn. (5) better than the soft constrained versions for both MSE and GAN losses.
x	y MSE AffMSE SoftGan AffGAN
Figure 4: 4× SR of CelebA faces. Model input x, target y and model outputs using the affine projections (Aff)
SSIM PSNR	'mse (x,Ay)
MSE	0.90	26.30	8.0 ∙	10-	5
AffMSE	0.91	26.53	1.6 ∙	10-	10
SoftGAN	0.76	21.11	2.3 ∙	10-	3
AffGAN	0.81	23.02	9.1 ∙	10-	10
Table 2: PSNR, SSIM and MSE
scores for the CelebA dataset. In
terms of PSNR and SSIM in HR
space the MSE trained models
achieves the best scores as ex-
pected and the AffGAN performs
better than the SoftGAN. Consid-
ering 'MSE (x, Ay) the models
according to figure legend. Both the AffGAN and SoftGAN produces clearly clearly show better consistency
shaper images than the blurry MSE outputs. We found that AffGAN outputs between input x and down sam-
slightly sharper images compared to SoftGAN, however also with slightly pled model output Ay than mod-
more high-frequency noise.	els not using the projection.
5.5	Natural Images
In Figure 5 we show the results for 4× SR from 32 × 32 to 128 × 128 pixels for AffGAN trained on
natural images from ImageNET. For most of the images the results are sharp and corresponds well
with the LR input. However we still see the high-frequency noise present in most GAN results in
some of the images. Interestingly the snake depicted in the third column is super resolved into water
which is obviously wrong but still a very plausible image considering the LR input image. Further,
water will likely have a higher density under the image prior than snakes which suggests that the
GAN model dreams up reasonable data.
8
Published as a conference paper at ICLR 2017
Figure 5: 4× SR from 32 × 32 to 128 × 128 using AffGAN on the ImageNET. AffGAN outputs (top row), true
HR images y (middle row), model input x (bottom row). Generally the AffGAN produces plausible outputs
which are however still easily distinguishable from true images. Interestingly the snake depicted in the third
column is super resolved into water which is obviously wrong but still a very plausible image considering the
LR input image.
5.6	Criticism and future directions
One argument against MAP inference is that the mode of a distribution is dependent on the represen-
tation: transforming a variable through an invertible transformation and performing MAP inference
in the transformed space may lead to different answers depending on the transformation. As an ex-
treme example, consider transforming a continuous random scalar Y with its cumulative distribution
function F = P(Y ≤ ∙). The resulting variable F(Y) is uniformly distributed, so any value in the
interval (0, 1] can be the mode. Thus, the MAP estimate is not unique if one allows for alternative
representations, and there is no guarantee that the MAP estimate in 24-bit RGB pixel represen-
tation which we seek in this paper is in any way special. One may arrive at a different solution
when performing MAP estimation in the feature space of a convolutional neural network, or even
if merely an alternative colour space is used. Interestingly, AffGAN is more resilient to coordinate
transformations: Eqn. (10) includes the extra term H[qθ] which is effected by transformations the
same way as H[qθ,pY]. The second argument relates to the assumption that MAP estimates appear
plausible. Although by definition the mode lies in a high-probability region, it does not guarantee
that its appearance is anything like that of a random sample. Consider for example data drawn from
a d-dimensional standard Normal distribution. Due to concentration of measure, as d increases the
norm of a typical sample will be approximately dW with very high probability. The mode, however,
has a norm of 0. In this sense, the mode of the distribution is highly atypical. Indeed human ob-
servers can easily tell apart a typical sample from the noise distribution and the mode, but would
have a hard time noticing the difference between two random samples. This argument suggests
that sampling from the posterior pY |X may be a good or even preferable way to obtain plausible
reconstructions. In Appendix F we establish a connection between variational inference, such as in
varational autoencoders (Kingma & Welling, 2014), and a stochastic version of AffGAN, however
leaving emperical studies as further.
6	Conclusion
In this work we developed methods for approximate MAP inference in SR. We first introduced an
architectural restriction to neural networks projecting the model output to the affine subspace of
valid solutions. We then proposed three methods, based on GANs, denoising or density models,
for amortised MAP inference in SR using this affine projection. In high dimensions we empirically
found that the GAN based approach, AffGAN produced the most visually appealing results. Our
work follows successful demonstrations of GAN-based algorithms for image SR (Ledig et al., 2016),
and we provide additional theoretical motivation for why this approach makes sense. In future work
we plan to focus on a stochastic extension of AffGAN which can be seen as performing amortised
variational inference.
9
Published as a conference paper at ICLR 2017
References
Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating distribu-
tion. Journal of Machine Learning Research, 15(1):3563-3593, 2014.
Martin Arjovsky and L6on Bottou. Towards principled methods for training generative adversarial networks.
In International Conference on Learning Representations, 2017.
Joan Bruna, Pablo Sprechmann, and Yann LeCun. Super-resolution with deep convolutional sufficient statistics.
International Conference on Learning Representations, 2016.
Sander Dieleman, Jan Schluter, Colin RaffeL Eben Olson, S0ren Kaae S0nderby, Daniel Nouri, and Eric Bat-
tenberg and. Lasagne: First release., 2015. URL http://dx.doi.org/10.5281/zenodo.27878.
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolu-
tional networks. IEEE Transactions on Pattern Analysis & Machine Intelligence, pp. 295-307, 2016.
Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep
networks. arXiv preprint arXiv:1602.02644, 2016.
David Garcia. Open source code. retrieved on 22 Sept 2016, 2016. URL https://github.com/
david-gpu/srez.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Process-
ing Systems, pp. 2672-2680, 2014.
Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, Jurgen Schmidhuber, and Harri Valpola.
Tagger: Deep unsupervised perceptual grouping. In Advances in Neural Information Processing Systems,
2016.
Gao Huang, Zhuang Liu, and Kilian Q Weinberger. Densely connected convolutional networks. arXiv preprint
arXiv:1608.06993, 2016.
Ferenc Huszdr. An alternative update rule for generative adversarial networks. UnPub-
lished note (retrieved on 7 Oct 2016),	2016. URL http://www.inference.vc/
an- alternative- update- rule- for- generative- adversarial- networks/.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In The International Conference on
Learning Representations, 2014.
Valero Laparra, Johannes Ball6, Alexander Berardino, and Eero P Simoncelli. Perceptual image quality assess-
ment using a normalized laplacian pyramid. In Proc. IS&T Int’l Symposium on Electronic Imaging, Conf.
on Human Vision and Electronic Imaging, 2016.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, and Ole Winther. Autoencoding beyond pixels using a
learned similarity metric. In Proceedings of The 33rd International Conference on Machine Learning, pp.
1558--1566, 2015.
Christian Ledig, Lucas Theis, Ferenc Huszdr, Jose Caballero, Andrew Aitken, Alykhan Tejani, Johannes Totz,
Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial
network. arXiv preprint arXiv:1609.04802, 2016.
Chuan Li and Michael Wand. Combining markov random fields and convolutional neural networks for image
synthesis. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Christopher A Metzler, Arian Maleki, and Richard G Baraniuk. Optimal recovery from compressive measure-
ments via denoising-based approximate message passing. In International Conference on Sampling Theory
and Applications (SampTA), pp. 508-512, 2015.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv preprint
arXiv:1610.03483, 2016.
Kamal Nasrollahi and Thomas B. Moeslund. Super-resolution: a comprehensive survey. Machine Vision and
Applications, pp. 1423-1468, 2014.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using
variational divergence minimization. arXiv preprint arXiv:1606.00709, 2016.
10
Published as a conference paper at ICLR 2017
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In Proceed-
ings of The 33rd International Conference on Machine Learning, pp. 1747—-1756, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. In International Conference on Learning Representations, 2015.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning
With ladder networks. In Advances in Neural Information Processing Systems, pp. 3546-3554, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. In Advances in Neural Information Processing Systems, 2016.
Jaakko Sarela and Harri Valpola. Denoising source separation. Journal of Machine Learning Research, pp.
233-272, 2005.
Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert,
and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolu-
tional neural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1874-1883, 2016.
The Theano Development Team, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller,
Dzmitry Bahdanau, Nicolas Ballas, Fr6d6ric Bastien, Justin Bayer, Anatoly Belikov, et al. Theano: A python
framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688, 2016.
Lucas Theis and Matthias Bethge. Generative image modeling using spatial lstms. In Advances in Neural
Information Processing Systems, pp. 1927-1935, 2015.
Lucas Theis, Reshad Hosseini, and Matthias Bethge. Mixtures of conditional gaussian scale mixtures applied
to multiscale image representations. PLoS ONE, 2012.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing ro-
bust features with denoising autoencoders. In Proceedings of the 25th International Conference on Machine
Learning, pp. 1096-1103, 2008.
Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality assess-
ment. In Conference Record of the 27th Asilomar Conference on Signals, Systems and Computers, volume 2,
pp. 1398-1402, 2003.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error
visibility to structural similarity. IEEE transactions on image processing, pp. 600-612, 2004.
11
Published as a conference paper at ICLR 2017
A Generative Adversarial Networks for minimising
KL-divergence
First note that for a fixed generator G the discriminator D maximises:
Ey~pγ log Dψ(y) + Ez~N log(1 - Dψ (Gθ(Z))
Ey-pγ log Dψ(y) + Ey~QG [iog(ι - Dψ (y))]=
pY (y) log Dψ (y) + qG (y) log(1 - Dψ(y))dy
(13)
(14)
(15)
y
where qG is the generative distribution. A function of the form a log(x) + b log(1 - x) always has maximum
at a++b and We find the Bayes-optimal discriminator to be (assuming equal prior class probabilities)
D*(y)= —pγ(y)—
(y)	PY (y)+ qG(y)
(16)
Let’s assume that this Bayes-optimal discriminator is unique and can be approximated closely by our neural
netWork (see Appendix C for more discussion on this assumption).
Using the modified update rule proposed here the combined optimization problem for the discriminator and
generator is
V(ψ,θ) = maxEy-pγ log Dψ(y) + Ez~N [log(Dψ(Gθ(Z)) - log(1 - Dψ(Gθ(z))]	(17)
ψ,θ
Starting from the definition of KL[qG |pY]
KL[qGlpγ]=Ey~qGlog PG®		(18)
=Eyy log1 DDy )y)	(insert Bayes-optimal classifier)	(19)
≈ Ey-qG log 1 Dψ⅛)y)	=-Ey~qG log 1 DDDyy)	(20)
Which is equal to the terms affecting the generator in Eqn. (17).
B	Affine projection
B.1 Numerical Estimation of the pseudo-inverse
In practice We implement the doWn-sampling projection A as a strided convolution With a fixed Gaussian
smoothing kernel Where the stride corresponds to the doWn-sampling factor. A+ is implemented as a transposed
convolution operation With parameters optimised numerically via stochastic gradient descent on the folloWing
objective function:
'ι(B) = Ey~Nrd kAy - ABAyk2
'2 (B) = Eχ~Nr ∣∣Bx — BABxk2
A+ = argmin {'1(B) + '2(B)}
B
(21)
(22)
(23)
Where Nd is the d-dimensional standard normal distribution, and d is the dimensionality of LR data x. `1 and
`2 can be thought of as a Monte Carlo estimate of the spectral norm of the transformations A - ABA and
B - BAB, respectively. The Monte Carlo formulation above has the advantage that it can be optimised via
stochastic gradient descent. The operation ABA can be thought of as a three-layer fully linear convolutional
neural netWork, Where A corresponds to a strided convolution With fixed kernels, While B is a trainable decon-
volution. We note that for certain doWnsampling kernels A the exact A+ Would have an infinitely large kernel,
although it can alWays be approximated With a local kernel. At convergence We found `1 + `2 to be betWeen
10-12 and 10-8 depending on the doWn-sampling factor, Width of the Gaussian kernel used for A and the filter
sizes of A and B.
12
Published as a conference paper at ICLR 2017
B.2 Gradients
The gradients of the affine projected SR models is derived by applying the chain rule
fθ (x) = (I - A+A)gθ (x) + A+x
∂fθ(x) = ∂fθ(x) ∂gθ(x) =(	a + a) ∂gθ(x)
∂θ	∂gθ(x) ∂θ	∂θ
(24)
(25)
Which is essentially the high-pass filtered version of the gradient of gθ (x).
C	Instance Noise
GANs are notoriously unstable to train, and several papers exist that try to improve their convergence properties
(Salimans et al., 2016; Radford et al., 2015) via various tricks. Consider the following idealised GAN algorithm,
each iteration consisting of the following steps:
1.	we train the discriminator D via logistic regression between qθ vs pY , until convergence
2.	We extract from D an estimate of the logarithmic likelihood ratio s(y) = log qθ(y))
3.	we update θ by taking a stochastic gradient step with objective function Ey〜qθs(y)
If qθ and pY are Well-conditioned distributions in a loW-dimensional space, this algorithm performs gradient
descent on an approximation to the KL divergence, so it should converge. So why is it highly unstable in
practical situations?
Crucially, the convergence of this algorithm relies on a few assumptions that don’t always hold: (1) that the
log-likelihood-ratio log qθ(y) is finite, (2) that the Jensen-Shannon divergence JS[qθ ∣∣p] is a well-behaved func-
tion of θ and (3) that the Bayes-optimal solution to the logistic regression problem is unique. We stipulate that
in real-world situations neither of these holds, mainly because qθ and pY are concentrated distributions whose
support may not overlap. In image modelling, distribution of natural images pY is often assumed to be con-
centrated on or around a lower-dimensional manifold. Similarly, qθ is often degenerate by construction. The
odds that the two distributions share support in high-dimensional space, especially early in training, are very
small. If qθ and pY have non-overlapping support (1) the log-likelihood-ratio and therefore KL divergence is
infinite (2) the Jensen-Shannon divergence is saturated so its maximum value and is locally constant in θ and
(3) there may be a large set of near-optimal discriminators whose logistic regression loss is very close to the
Bayes optimum, but each of these possibly provides very different gradients to the generator. Thus, training the
discriminator D might find a different near-optimal solution each time depending on initialisation, even for a
fixed qθ and pY .
The main ways to avoid these pathologies involve making the discriminator’s job harder. For example, in most
GAN implementations the discriminator is only partially updated in each iteration, rather than trained until
convergence. Another way to cripple the discriminator is adding label noise, or equivalently, one-sided label
smoothing as introduced by Salimans et al. (2016). In this technique the labels in the discriminator’s training
data are randomly flipped. However we do not believe these techniques adequately address all of the concerns
described above.
In Figure 6a we illustrate two almost perfectly separable distributions. Notice how the large gap between the
distributions means that there are large number of possible classifiers that tell the two distributions apart and
achieve similar logistic loss. The Bayes-optimal classifier may not be unique, and the set of near-optimal clas-
sifiers is very large and diverse. In Figure 6b we show the effect of one sided label smoothing or equivalently,
adding label noise. In this technique, the labels of some real data samples y 〜PY are flipped so the dis-
criminator is trained thinking they were samples from qθ . The discriminator indeed has a harder task now,
but all classifiers are penalised almost equally. As a result, there is still a large set of discriminators which
achieve near-optimal loss, it’s just that the near-optimal loss is now larger. Label smoothing does not help if the
Bayes-optimal classifier is not unique.
Instead we propose to add noise to the samples, rather than labels, which we denote instance noise. Using
instance noise the support of the two distributions is broadened and they are no longer perfectly separable as
illustrated in Figure 6c. Adding noise, the Bayes-optimal discriminator becomes unique, the discriminator is
less prone to overfitting because it has a wider training distribution, and the log-likelihood-ratio becomes better
behaved. The Jensen-Shannon divergence between the noisy distributions is now a non-constant function of θ.
Using instance noise, is easy to construct an algorithm that minimises the following divergence:
dσ (qθ ,PY) =KL[pσ * qθ ∣∣Pσ * PY] ,	(26)
13
Published as a conference paper at ICLR 2017
Figure 6: Illustration of samples from two non-overlapping perfectly distributions distributions in a,) with
one-sided label smoothing in b) and instance noise in c). One side-label smoothing shifts the optimal deci-
sion boundary but pY still covers areas with no support in qθ . Instance Noise broadens the support of both
distributions without biasing the optimal discriminator.
where σ is the parameter of the noise distribution. Logistic regression on the noisy samples provides an estimate
of Sσ(x) = log p；：p* . When updating the generator We have to minimise the mean of s(σ) on noisy samples
from qθ . We know that, if pσ is Gaussian, dσ is a Bregman-divergence, and that it is 0 if and only if the two
distributions are equal. Because of the added noise, dσ is less sensitive to local features of the distribution.
We found that in our experiments instance noise helped the convergence of AffGAN. We have not tested the
instance noise in the generative modelling application. Because We don’t have to Worry about over-training the
discriminator, We can train it until convergence, or take more gradient steps betWeen subsequent updates to the
generator. One critical hyper-parameter of this method is the noise distribution. We used additive Gaussian
noise, Whose variance We annealed during training. We propose a heuristic annealing schedule Where the noise
is adapted so as to keep the optimal discriminator’s loss constant during training. It is possible that other
noise distributions such as heavy-tailed or spike-and-slab Would Work better but We have not investigated these
options.
D experimental details
Loss functions
For the GAN models the generative and discriminative parameters Were updated using Eqn. (10). For the
models enforcing Eqn. (5) using a soft-constraint We added an extra MAE loss term to the generative parameters
'mae = ~N Pikxi — Ayi k, where i i runs over the number of data samples N.
The denoiser guided models Were trained in a tWo step procedure. Initially We pre-trained a DAE to denoise
samples from the data distribution by minimising
'dae = N X ky,fDAE(y)k2	(27)
y = y + 3 e 〜N(0,σI)
14
Published as a conference paper at ICLR 2017
During training we anneal the noise level σ and continuously save the model parameters of the DAE f(σDAE)
trained at increasingly smaller noise levels. We then learn the parameters of the generator by following the
gradient in Eqn. (11) using the DAE to estimate 焉 log p(y)
∂θ Ex [log P⑼]=Ex ]Iy log p(y) ∙ ∂θy
Ex
fDAE(y) - y d ʌ
一σ2	∂θy
∂
Θi+1 ― θi + α-Ex [log p(y)]
∂θ
(28)
(29)
(30)
Where α is the learning rate. During training we continuously load parameters of the DAE trained at increas-
ingly low noise levels to get gradients pointing in the approximate correct direction in beginning of training
while covering a large data space and precise gradients close to the data manifold in the end of the training.
For the density guided models we first pre-train a density model by maximising the tractable log-likelihood
L(y) =	log p(yj |y<j)
j
(31)
Where the joint density have been decomposed using the chain rule and j runs over the pixels. Similar to the
DAE we continuously save the parameters of the density model during training. We then learn the parameters
of the generator by directly minimising the negative log-likelihood of the generated samples under the learned
density model.
' =-L(y) = -L(fθ (X))
(32)
2D swiss-roll
The 2D target data y = [y1, y2] was sampled from the 2D Swiss-Roll defined as:
νι 〜N(μι,σι),	V 〜N(μ2,σ2)	(33)
r = 0.4ν1 + ν2	(34)
y = [cos(νι) * r, sin(νι) * r]	(35)
Where μι = 10, σι = 3, μ2 = 0 and σ2 = 0.2. The LR input was defined as x = y1+ y2. The Cross-entropy
H[qθ , pY ] were calculated by estimating the probability density function using a Gaussian kernel density esti-
mator fitted to 50.000 samples from a noiseless Swiss Roll density i.e. σ2 = 0, and setting the bandwidth of
eaCh kernel to σ2 = 0.2. All generator and disCriminators were 2-layered fully ConneCted NNs with 64 units
in eaCh layer. For the AffDG model the DAE was a two layered NN with 256 units in eaCh layer trained while
annealing the standard deviation of the Gaussian noise from 0.5 to 0.25.
Image data
For all image experiments we set A to a Convolution using a Gaussian smoothing kernel of size 9 × 9 using a
stride of 4 Corresponding to 4× down-sampling. A+ were set to a Convolution operation with 42 kernels of size
5 × 5 followed by a reordering of the pixel with the output Corresponding to 4× up-sampling Convolution as
desCribed in (Shi et al., 2016). The parameters of the A+ was optimised numeriCally as desCribed in Appendix
B. All down-sampling were done using the A projeCtion. For all image models we used Convolutional models
using ReLU nonlinearities and batCh normalization in all layers exCept the output. All generators used skip
ConneCtions similar to (Huang et al., 2016) and a final sigmoid non-linearity was applied to output of the model
whiCh were either used direCtly or feed through the affine transformation layers parameterised by A and A+ .
The disCriminators were standard Convolutional networks followed by a final sigmoid layer.
For the grass texture experiments we used randomly extraCted patChes of data from high resolution grass tex-
ture images. The generators used 6 layers of Convolutions with 32, 32, 64, 64, 128 and filter maps and skip
ConneCtions after every seCond layer. The disCriminators had four layers of strided Convolutions with 32, 64,
128 and 256 filter maps. For the AffDG model the DAE was a four layer Convolutional network with 128 filter
maps in eaCh layer trained while annealing the standard deviation of the Gaussian noise from 0.5 to 0.01. The
density model was implemented as a pixelCNN similar to Oord et al. (2016) with four layers of Convolution
15
Published as a conference paper at ICLR 2017
Samples	Samples
Figure 7: PSNR and SSIM results for the AffDG and AffLL models. Note that the step-like behaviour of the
AffDG model is due to change of the DAE model with continuously lower noise levels
with 64 filter map with kernel sizes of 5, except for the first layers which used 7. The original PixelCNN
uses a non-differentiable categorical distribution as the likelihood model why it can not be used for gradient
based optimization. Instead we used a MCGSM as the likelihood model (Theis & Bethge, 2015), which have
been shown to be a good density model for images (Theis et al., 2012), using 32 mixture components and 32
quadratic features to approximate the covariance matrices.
For the CelebA experiments the datasets were split into train, validation and test set using the standard splitting.
All images were center cropped and resized to 64× 64 before down-sampling to 16 ×16 using A. All generators
were 12 layer convolution networks with four layers of 128, 256 and 512 filter maps and skip connections
between every fourth layer. The discriminators were 8 layer convolution nets with two layers of 128, 256, 512
and 1024 filter maps using a stride of 2 for every second layer.
For the ImageNET experiments the 2012 dataset were randomly split into train, validation and test set with 104
samples in the test and validation sets. All images below 20kB were then discarded to remove images with to
low resolution. The images were center cropped and resized to 128 × 128 before down-sampling to 32 × 32
using A. The generator were a 8 layer convolutional network with 4 layers of 128 and 256 filter maps and skip
connections between every second layer. The discriminators were 8 layer convolution nets with two layers of
128, 256, 512 and 1024 filter maps using a stride of 2 for every second layer. To stabilise training we used
Gaussian instance noise linearly annealed from an initial standard deviation of 0.1 to 0. We were unable to
stable train models without this extra regularization.
E Additional results for Denoiser and Density guided
Super-resolution
Figure 7 show the PSNR and SSIM scores during training for the AffDG and AffLL models trained on the
grass textures. Note that the models are converging, but as seen in Figure 3 the images are very blurry. For both
models we had problems with diverging training. For the DAE models with high noise levels the gradients are
only approximately correct but covers a large space around the data manifold whereas for small noise levels
the gradients are more accurate in a small space around the data manifold. For the density model we believe a
similar phenomenon is making the training diverge since for accurate density models the estimated density is
likely very peaked around the data manifold making learning in the beginning of training difficult. To resolve
these issue we started training using models with high noise levels or low log-likelihood values and then loaded
model parameters during training with continuously smaller noise levels or better log-likelihood values. The
effect of this can be clearly seen during training as the step like behavior of the AffDG in Figure 7. We note that
the density model used for training the AffLL achieved a log-likelihood of -4.10 bits per dimension which
is comparable to values obtained in Theis & Bethge (2015) on a texture dataset. Further the AffLL model
achieved high log-likelihood values > -3.5 under this model suggesting that the density model is simply not
providing an accurate enough representation of pY to provide precises scores for training the AffLL model.
16
Published as a conference paper at ICLR 2017
Figure 8: 4× SR from 32 × 32 to 128 × 128 using AffGAN on the ImageNET. AffGAN outputs (top row),
true HR images y (middle row), model input x (bottom row).
F Amortised variational inference using AffGAN
Here we’ll show that a stochastic extension of the AffGAN model approximately minimises an amortised vari-
ational inference criterion as in e. g. variational autoencoders, which for the first time establishes a connection
between adversarial methods of inferences and and variational inference. We introduce a variant of AffGAN
where, in addition to the LR data x, the generator function also takes as input some independent noise variables
z: we establish a connection between GANs and amortised variational
Z〜PZ
y = ∏Afθ(χ, z)
(36)
(37)
Similarly to how we defined qθ in Section 3.1 we introduce the following notation:
qγ；e := Ex〜PXEz〜PZδ (y - ∏Afθ(x, Z))	(38)
qγχιθ ：= Ez〜PZδ (y - ∏Afθ(x, Z))	(39)
qχ,γ；e ：= PX ∙ qY|x；&	(40)
Here the affine projection ensures that under qχ,γ；e, X and y are always consistent. Therefore, under qχ,γ；e,
the conditional of x given y is the same as the likelihood PX|Y = δ(x - Ay) by construction and the following
equality holds:
qx,Y；e = qy；e ∙ Px∣y = Px ∙ qγ∣x；θ
Applying Bayes, rule to Pχ∣γ = PYPYPX and substituting into the above equality we get:
(41)
qy；e ∙ Py∣x=Ay = PY；e ∙ qy|x=Ay；e
The KL divergence that the AffGAN objective minimises can now be rewritten as.
(42)
KL [qy；e ∣∣pγ] = Eq-
Y Y ; log ⅛T	(43)
Xi logqY⅛y) x,y S	PY (y)	(44)
Xi log qYl"ylχ) x,y S	Py |x (y|x)	(45)
X KL [qγ∣χq ∣∣Py∣x]	(46)
Therefore we can conclude that the AffGAN algorithm described in Section 3.2 approximately minimizes the
following amortised variational inference criterion:
argminKL[qy；e ∣∣py ]
θ
argmin Ex 〜PX
θ
KL [qy|x；& ∣∣Py|X]
(47)
and in doing so it only requires samples from PY and PX.
17