Figure 1: Example of meta-learning setup. The top represents the meta-training set Dmeta-train,where inside each gray box is a separate dataset that consists of the training set Dtrain (left side ofdashed line) and the test set Dtest (right side of dashed line). In this illustration, we are consideringthe 1-shot, 5-class classification task where for each dataset, we have one example from each of5 classes (each given a label 1-5) in the training set and 2 examples for evaluation in the test set.
Figure 2: Computational graph for the forward pass of the meta-learner. The dashed line dividesexamples from the training set Dtrain and test set Dtest. Each (Xi , Yi) is the ith batch from thetraining set whereas (X, Y) is all the elements from the test set. The dashed arrows indicate that wedo not back-propagate through that step when training the meta-learner. We refer to the learner asM, where M (X; θ) is the output of learner M using parameters θ for inputs X. We also use Nt asa shorthand for Ne- Lt.
Figure 3: Visualization of the input and forget values output by the meta-learner during the courseof its updates. Layers 1 - 4 represent the values for a randomly selected parameter from the 4convolutional layers and layer 5 represents the values for a random parameter from fully-connectedlayer. The different curves represent training steps on different datasets.
