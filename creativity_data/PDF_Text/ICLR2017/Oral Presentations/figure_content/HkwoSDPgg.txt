Figure 1: Overview of the approach: (1) an ensemble of teachers is trained on disjoint subsets of thesensitive data, (2) a student model is trained on public data labeled using the ensemble.
Figure 2: How much noise can be injectedto a query? Accuracy of the noisy aggrega-tion for three MNIST and SVHN teacher en-sembles and varying γ value per query. Thenoise introduced to achieve a given γ scalesinversely proportionally to the value of γ :small values ofγ on the left of the axis corre-spond to large noise amplitudes and large γvalues on the right to small noise.
Figure 3: How certain is the aggregation ofteacher predictions? Gap between the num-ber of votes assigned to the most and secondmost frequent labels normalized by the num-ber of teachers in an ensemble. Larger gapsindicate that the ensemble is confident in as-signing the labels, and will be robust to morenoise injection. Gaps were computed by av-eraging over the test data.
Figure 4: Utility and privacy of the semi-supervised students: each row is a variant of the stu-dent model trained with generative adversarial networks in a semi-supervised way, with a differentnumber of label queries made to the teachers through the noisy aggregation mechanism. The lastcolumn reports the accuracy of the student and the second and third column the bound ε and failureprobability δ of the (ε, δ) differential privacy guarantee.
Figure 5: Influence of distillation on the accuracy of the SVHN student trained with respect to theinitial number of training samples available to the student. The student is learning from n = 50teachers, whose predictions are aggregated without noise: in case where only the label is returned,we use plurality, and in case a probability vector is returned, we sum the probability vectors outputby each teacher before normalizing the resulting vector.
