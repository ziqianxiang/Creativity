Figure 1: Network structure. The image s, measurements m, and goal g are first processed sep-arately by three input modules. The outputs of these modules are concatenated into a joint repre-Sentation j. This joint representation is processed by two parallel streams that predict the expectedmeasurements E(j) and the normalized action-conditional differences {Ai (j)}, which are then com-bined to produce the final prediction for each action.
Figure 2: Example frames from the four scenarios.
Figure 3: Performance of different approaches during training. DQN, A3C, and DFP achieve sim-ilar performance in the Basic scenario. DFP outperforms the prior approaches in the other threescenarios, with a multiplicative gap in performance in the most complex ones (D3 and D4).
