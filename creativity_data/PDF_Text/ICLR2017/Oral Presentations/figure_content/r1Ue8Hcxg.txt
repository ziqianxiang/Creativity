Figure 1: An overview of Neural Architecture Search.
Figure 2: How our controller recurrent neural network samples a simple convolutional network. Itpredicts filter height, filter width, stride height, stride width, and number of filters for one layer andrepeats. Every prediction is carried out by a softmax classifier and then fed into the next time stepas input.
Figure 3: Distributed training for Neural Architecture Search. We use a set of S parameter serversto store and send parameters to K controller replicas. Each controller replica then samples m archi-tectures and run the multiple child models in parallel. The accuracy of each child model is recordedto compute the gradients with respect to Î¸c, which are then sent back to the parameter servers.
Figure 4: The controller uses anchor points, and set-selection attention to form skip connections.
Figure 5: An example of a recurrent cell constructed from a tree that has two leaf nodes (base 2)and one internal node. Left: the tree that defines the computation steps to be predicted by controller.
Figure 6: Improvement of Neural Architecture Search over random search over time. We plot thedifference between the average of the top k models our controller finds vs. random search every 400models run.
Figure 7: Convolutional architecture discovered by our method, when the search space does nothave strides or pooling layers. FH is filter height, FW is filter width and N is number of filters. Notethat the skip connections are not residual connections. If one layer has many input layers then allinput layers are concatenated in the depth dimension.
Figure 8: A comparison of the original LSTM cell vs. two good cells our model found. Top left:LSTM cell. Top right: Cell found by our model when the search space does not include max andsin. Bottom: Cell found by our model when the search space includes max and sin (the controllerdid not choose to use the sin function).
