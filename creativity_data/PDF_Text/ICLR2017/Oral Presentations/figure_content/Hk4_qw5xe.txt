Figure 1: First, we trained a DCGAN for 1, 10 and 25 epochs. Then, with the generator fixedwe train a discriminator from scratch. We see the error quickly going to 0, even with very fewiterations on the discriminator. This even happens after 25 epochs of the DCGAN, when the samplesare remarkably good and the supports are likely to intersect, pointing to the non-continuity of thedistributions. Note the logarithmic scale. For illustration purposes we also show the accuracy of thediscriminator, which goes to 1 in sometimes less than 50 iterations. This is 1 even for numericalprecision, and the numbers are running averages, pointing towards even faster convergence.
Figure 2: First, we trained a DCGAN for 1, 10 and 25 epochs. Then, with the generator fixed wetrain a discriminator from scratch and measure the gradients with the original cost function. We seethe gradient norms decay quickly, in the best case 5 orders of magnitude after 4000 discriminatoriterations. Note the logarithmic scale.
Figure 3: First, we trained a DCGAN for 1, 10 and 25 epochs. Then, with the generator fixed wetrain a discriminator from scratch and measure the gradients with the - log D cost function. Wesee the gradient norms grow quickly. Furthermore, the noise in the curves shows that the varianceof the gradients is also increasing. All these gradients lead to updates that lower sample qualitynotoriously.
