Table 1: Data used in this paper. Tasks 1-5 were generated using our simulator and share the same KB.
Table 2: Test results across all tasks and methods. For tasks T1-T5 results are given in the standardsetup and the out-of-vocabulary (OOV) setup, where words (e.g. restaurant names) may not have been seenduring training. Task T6 is the Dialog state tracking 2 task with real dialogs, and only has one setup. Bestperforming methods (or methods within 0.1% of best performing) are given in bold for the per-response accuracymetric, with the per-dialog accuracy given in parenthesis. (*) For Concierge, an example is considered correctlyanswered if the correct response is ranked among the top 10 candidates by the bot, to accommodate the muchlarger range of semantically equivalent responses among candidates (see ex. in Tab. 7).⑴ We did not implementMemNNs+match type on Concierge, because this method requires a KB and there is none associated with it.
Table 3: Task 1 (Issue API call) The model learns to direct its attention towards the 4 memories containingthe information key to issue the API call. More hops help to strengthen this signal. <silence> is a special tokenused to indicate that the user did not speak at this turn - the model has to carry out the conversation With noadditional input.
Table 4: Task 2 (Update API call) Out of the multiple memories from the current dialog, the modelcorrectly focuses on the 2 important pieces: the original API call and the utterance giving the update.
Table 5: Task 3 (Displaying options) The model knows it has to display options but the attention is wrong:it should attend on the ratings to select the best option (with highest rating). It cannot learn that properly andmatch type features do not help. It is correct here by luck, the task is not solved overall (see Tab. 2). We do notshow all memories in the table, only those With meaningful attention.
Table 6: Task 4 (Providing extra-information) The model knows it must display a phone or an address,but, as explained in Section A the embeddings mix up the information and make it hard to distinguish betweendifferent phone numbers or addresses, making answering correctly very hard. As shown in the results of Tab. 2,this problem can be solved by adding match type features, that allow to emphasize entities actually appearing inthe history. The attention is globally wrong here.
Table 7: Concierge Data The model is also able to learn from human-human dialogs. <person>, <org>,<number> and <date> are special tokens used to anonymize the data. We report the top 5 answers predicted bythe model. They are all semantically equivalent. Note that the utterances, while all produced by humans, are notperfect English ("rservation", "I’ll check into it")Time	Locutor	Dialog History	Hop #1	Hop #21	User	hey concierge	.189	-^.095-2	User	could you check if i can get a rservation at <org> <date> for brunch	.209	.1783	User	<number> people	.197	.1424	User	<silence>	.187	.1675	Bot	hi <person> unfortunately <org> is fully booked for <date>	.225	.410		and there,s <number> people on the waiting list		User input		when,s the earliest availability		Correct answer		i,ll check		Pred. answer #1		i,m on it	[Incorrect]	Pred. answer #2		i,ll find out	[Incorrect]	Pred. answer #3		i,ll take a look	[Incorrect]	Pred. answer #4		i,ll check	[Correct]	Pred. answer #5		i'll check into it	[Incorrect]	Table 8: Hyperparameters of Supervised Embeddings. When Use History is True, the wholeconversation history is concatenated with the latest user utterance to create the input. If False, only the latest
Table 8: Hyperparameters of Supervised Embeddings. When Use History is True, the wholeconversation history is concatenated with the latest user utterance to create the input. If False, only the latestutterance is used as input.
Table 9: Hyperparameters of Memory Networks. The longer and more complex the dialogs are, themore hops are needed.
Table 10: Test results across all tasks and methods. For tasks T1-T5 results are given in the standardsetup and the out-of-vocabulary (OOV) setup, where words (e.g. restaurant names) may not have been seenduring training. Task T6 is the Dialog state tracking 2 task with real dialogs, and only has one setup. Bestperforming methods (or methods within 0.1% of best performing) are given in bold for the per-response accuracymetric, with the per-dialog accuracy given in parenthesis.
