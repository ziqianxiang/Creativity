Table 1: Network ConfigurationsName	Network Type	Architecture	Data setF1	Fully Connected	Section B.1	MNIST (LeCun et al., 1998a)F2	Fully Connected	Section B.2	TIMIT (Garofolo et al., 1993)C1	(Shallow) Convolutional	Section B.3	CIFAR-10 (Krizhevsky & Hinton, 2009)C2	(Deep) Convolutional	Section B.4	CIFAR-10C3	(Shallow) Convolutional	Section B.3	CIFAR-100 (Krizhevsky & Hinton, 2009)C4	(Deep) Convolutional	Section B.4	CIFAR-100and using other initialization strategies, activation functions, and data sets showed similar behavior.
Table 2: Performance of small-batch (SB) and large-batch (LB) variants of ADAM on the 6 networkslisted in Table 1Name	Training Accuracy		Testing Accuracy		SB	LB	SB	LBF1	99.66% ± 0.05%	99.92% ± 0.01%	98.03% ± 0.07%	97.81% ± 0.07%F2	99.99% ± 0.03%	98.35% ± 2.08%	64.02% ± 0.2%	59.45% ± 1.05%C1	99.89% ± 0.02%	99.66% ± 0.2%	80.04% ± 0.12%	77.26% ± 0.42%C2	99.99% ± 0.04%	99.99% ± 0.01%	89.24% ± 0.12%	87.26% ± 0.07%C3	99.56% ± 0.44%	99.88% ± 0.30%	49.58% ± 0.39%	46.45% ± 0.43%C4	99.10% ± 1.23%	99.57% ± 1.84%	63.08% ± 0.5%	57.81% ± 0.17%We emphasize that the generalization gap is not due to over-fitting or over-training as commonlyobserved in statistics. This phenomenon manifest themselves in the form of a testing accuracy curvethat, at a certain iterate peaks, and then decays due to the model learning idiosyncrasies of thetraining data. This is not What We observe in our experiments; see Figure 2 for the training-testingcurve of the F2 and C1 networks, which are representative of the rest. As such, early-stoppingheuristics aimed at preventing models from over-fitting Would not help reduce the generalizationgap. The difference betWeen the training and testing accuracies for the netWorks is due to thespecific choice of the netWork (e.g. AlexNet, VGGNet etc.) and is not the focus of this study.
Table 3: Sharpness of Minima in Full Space; is defined in (3).
Table 4: Sharpness of Minima in Random Subspaces of Dimension 100	€= SB	10-3 LB	€ = 5∙10-4				SB	LB~w	0.11 ± 0.00	-9.22 ± 0.56-	0.05 ± 0.00	9.17 ± 0.14F2	0.29 ± 0.02	23.63 ± 0.54	0.05 ± 0.00	6.28 ± 0.19C1	2.18 ± 0.23	137.25 ± 21.60	0.71 ± 0.15	29.50 ± 7.48C	0.95 ± 0.34	25.09 ± 2.61	0.31 ± 0.08	5.82 ± 0.52C	17.02 ± 2.20	236.03 ± 31.26	4.03 ± 1.45	86.96 ± 27.39C4	6.05 ± 1.13	72.99 ± 10.96	1.89 ± 0.33	19.85 ± 4.12We conclude this section by noting that the sharp minimizers identified in our experiments do notresemble a cone, i.e., the function does not increase rapidly along all (or even most) directions. Bysampling the loss function in a neighborhood of LB solutions, we observe that it rises steeply onlyalong a small dimensional subspace (e.g. 5% of the whole space); on most other directions, thefunction is relatively flat.
Table 5: Data SetsData Set	# Data Points		# Features	# Classes	Reference	Train	Test			MNIST	60000	10000	28 X 28	10	(LeCUn et al.,1998a;b)TIMIT	721329	310621	360	1973	(GarofoloetaL,1993)CIFAR-10	50000	10000	32 × 32	10	(KrizhevSky & Hinton, 2009)CIFAR-100	50000	10000	32 × 32	100	(KrizhevSky & Hinton, 2009)B	Architecture of NetworksB.1	NETWORK F1For this network, we use a 784-dimensional input layer followed by 5 batch-normalized (Ioffe &Szegedy, 2015) layers of 512 neurons each with ReLU activations. The output layer consists of 10neurons with the softmax activation.
Table 6: Effect of Data AugmentationTesting AccuracyBaseline (SB) Augmented LBSharpness (LB method)e = 10-3	e = 5 ∙ 10-4C1C2C3C483.63% ± 0.14%89.82% ± 0.12%54.55% ± 0.44%63.05% ± 0.5%82.50% ± 0.67%90.26% ± 1.15%53.03% ± 0.33%65.88 ± 0.13%231.77 ± 30.50468.65 ± 47.86103.68 ± 11.93
Table 7: Effect of Conservative Training	Testing Accuracy		Sharpness (LB method)		Baseline (SB)	Conservative LB	E = 10-3	E = 5∙10-4F1	98.03% ± 0.07%	98.12% ± 0.01%	232.25 ± 63.81	46.02 ± 12.58F2	64.02% ± 0.2%	61.94% ± 1.10%	928.40 ± 51.63	190.77 ± 25.33C1	80.04% ± 0.12%	78.41% ± 0.22%	520.34 ± 34.91	171.19 ± 15.13C2	89.24% ± 0.05%	88.495% ± 0.63%	632.01 ± 208.01	108.88 ± 47.36C3	49.58% ± 0.39%	45.98% ± 0.54%	337.92 ± 33.09	110.69 ± 3.88C4	63.08% ± 0.10%	62.51 ± 0.67	354.94 ± 20.23	68.76 ± 16.29E.2 Conservative TrainingIn (Li et al., 2014), the authors argue that the convergence rate of SGD for the large-batch settingcan be improved by obtaining iterates through the following proximal sub-problem.
