Table 1: Number of training examples needed before the GGT-NN model could attain ≤ 5% erroron each of the bAbI tasks. Experiments were run with 50, 100, 250, 500, and 1000 examples.
Table 2: Error rates of various models on the bAbI tasks. Bold indicates ≤ 5% error. For descriptionsof each of the tasks, see Table 1. “GGT-NN + direct ref.” denotes the GGT-NN model with directreference, and “GGT-NN” denotes the version without direct reference. See text for details regardingthe models used for comparison. Results from LSTM and MemNN reproduced from Weston et al.
Table 3: Accuracy of GGT-NN on the Rule 30 Automaton and Turing Machine tasks.
Table 4: Performance of the sequence-extended GGT-NN on the two bAbI tasks with a temporal component.		Algorithm 2 Sequence-Extended Pseudocode		Go — 0 for k from 1 to K do Gk — Th(Gk-ι, i(k)) if direct reference enabled then Gk — Thdirect(Gk , D(k)) end if if intermediate propagation enabled then Gk — Tprop (Gk) end if haGdkd — Trepr (Gk) Gk — Tadd(Gk , [i(k) haGdkd]) Gk — TC(Gk, i(k)) end for answer hsummary — 0 for k from 1 to K do Gk — Thquery(Gk , iquery) if direct reference enabled then Gk — Thquery,direct (Gk, Dquery) end if Gk — Tpqroupery(Gk) haGnkswer — Treqpurery(Gk) answer	answer answer hsummary — fsummarize (hGk , hsummary ) end for return foutput (hsaunmswmearry)		. Initialize G to an empty graph . Process each sentence . Initialize hsaunmswmearry to the zero vector . Process the query for each graphusing a recurrent network such as a GRU layer, from which the output can be produced. The modi-fied pseudocode for this is shown in Algorithm 2.
