title,year,conference
 Normalization propagation: A parametric tech-nique for removing internal covariate shift in deep networks,2016, arXiv preprint
 Layer normalization,2016, arXiv preprint
 Neural machine translation by jointly learning to align andtranslate,2015, ICLR
 Learning long-term dependencies with gradient descent isdifficult,1994, Neural Networks
 Recurrent batch normalization,2016, arXivpreprint
 Draw: A recurrent neuralnetwork for image generation,2015, arXiv preprint
 Untersuchungen zu dynamischen neuronalen netzen,1991, Masterâ€™s thesis
 Long short-term memory,1997, Neural computation
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, CoRR
 Adam: A method for stochastic optimization,2014, arXiv preprint
 Zoneout: Regularizing rnns by randomly preserving hiddenactivations,2016, arXiv preprint
 The neural autoregressive distribution estimator,2011, AISTATS
 Building a large annotated corpus of english:The penn treebank,1993, Comput
 Subword languagemodeling with neural networks,2012, preprint
 On the difficulty of training recurrent neural networks,2012, arXivpreprint
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, arXiv preprint
 Recurrent dropout without memory loss,2016, CoRR
 Theano: A Python framework for fast computation of mathematicalexpressions,2016, arXiv preprint
 Blocks and fuel: Frameworks for deep learning,2015, CoRR
 Accelerating the convergenceof the back-propagation method,1988, Biological Cybernetics
 Describing videos byexploiting temporal structure,2015, In ICCV
