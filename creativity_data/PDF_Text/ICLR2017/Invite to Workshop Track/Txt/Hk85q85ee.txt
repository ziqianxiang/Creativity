Under review as a conference paper at ICLR 2017
Symmetry-Breaking Convergence Analysis of
Certain Two-layered Neural Networks with
ReLU nonlinearity
Yuandong Tian
Facebook AI Research
yuandong@fb.com
Ab stract
In this paper, we use dynamical system to analyze the nonlinear weight dynam-
ics of two-layered bias-free networks in the form of g(x; W) = Pj=I σ(w∣x),
where σ(∙) is ReLU nonlinearity. We assume that the input X follow Gaussian
distribution. The network is trained using gradient descent to mimic the output of
a teacher network of the same size with fixed parameters W using l2 loss. We
first show that when K = 1, the nonlinear dynamics can be written in close form,
and converges to W with at least (1 - e)/2 probability, if random weight ini-
tializations of proper standard derivation (〜 1 /√d) is used, verifying empirical
practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. For net-
works with many ReLU nodes (K ≥ 2), we apply our close form dynamics and
prove that when the teacher parameters {w；}K=I forms orthonormal bases, (1) a
symmetric weight initialization yields a convergence to a saddle point and (2) a
certain symmetry-breaking weight initialization yields global convergence to w；
without local minima. To our knowledge, this is the first proof that shows global
convergence in nonlinear neural network without unrealistic assumptions on the
independence of ReLU activations. In addition, we also give a concise gradient
update formulation for a multilayer ReLU network when it follows a teacher of
the same size with l2 loss. Simulations verify our theoretical analysis.
1 Introduction
Deep learning has made substantial progress in many applications, including Computer Vision [He
et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Nat-
ural Language Processing [Sutskever et al. (2014)] and Speech Recognition [Hinton et al. (2012)].
However, till now, how and why it works remains elusive due to a lack of theoretical understanding.
First, how simple approaches like gradient descent can solve a very complicated non-convex opti-
mization effectively. Second, how the deep models, especially deep convolutional models, achieve
generalization power despite massive parameters.
In this paper, we focus on the first problem and use dynamical system to analyze the nonlinear
gradient descent dynamics of certain two-layered nonlinear network in the following form:
K
g(x； w) = fσ(w∣x)	(1)
j=1
where σ(x) = max(x, 0) is the ReLU nonlinearity. We consider the following setting: a student
network learns the parameters that minimize the l2 distance between its prediction and the super-
vision provided by the teacher network of the same size with a fixed set of parameters w； . We
assume all inputs X to follow Gaussian distribution and thus the network is bias-free. Eqn. 1 is
highly nonconvex and could contain exponential number of symmetrically equivalent solutions.
To analyze this, we first derive novel and concise gradient update rules for multilayer ReLU networks
(See Lemma 2.1) in the teacher-student setting under l2 loss. Then for K = 1, we prove that the
nonlinear gradient dynamics of Eqn. 1 has a close form and converges to w； with at least (1 -
1
Under review as a conference paper at ICLR 2017
Figure 1: (a) We consider the student and teacher network as nonlinear neural networks with ReLU
nonlinearity. The student network updates its weight W from the output of the teacher, whose
weights W are fixed. (b)-(c) The network structure we consider in K = 1 and K ≥ 2 cases.
(d) Notations used in multilayer ReLU gradient update rule (Sec. 2.2)
e)/2 probability, if initialized randomly with standard derivation on the order of 1∕√d, verifying
commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al.
(2012)],. When K ≥ 2, we prove that when the teacher parameters {wj}储 form orthonormal
bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and ⑵ under
a certain symmetric breaking weight initialization, the dynamics converges to w*, without getting
stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to
the origin for a fixed ∣∣w*k, showing that such a convergence behavior is beyond the local convex
structure at w*. To our knowledge, this is the first proof of its kind.
Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)]
analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local
minima is global for multilinear network. Very little theoretical work has been done to analyze
the dynamics of nonlinear networks, especially deep ones. [Mei et al. (2016)] shows the global
convergence when K = 1 with activation function σ(x) when its derivatives σ0, σ00, σ000 are bounded
and σ0 > 0. Similar to our approach, [Saad & Solla (1996)] also uses the student-teacher setting and
analyzes the dynamics of student network when the teacher,s parameters w* forms a orthonomal
bases; however, it uses σ(x) = erf(x) as the nonlinearity and only analyzes the local behaviors of
the two critical points (the saddle point in symmetric initializations, and w*). In contrast, we prove
the global convergence behavior in certain symmetry-breaking cases.
Many previous works analyze nonlinear network based on the assumption of independent activa-
tions: the activations of ReLU (or other nonlinear) nodes are independent of the input and/or mutu-
ally independent. For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network
with spin-glass models when several assumptions hold, including the assumption of independent ac-
tivations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network
is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of
the local minimum in a two-layered ReLU network, by assuming small sample size and applying
independent multiplicative Bernoulli noise on the activations. In practice, the activations are highly
dependent due to their common input. Ignoring such dependency also misses important behaviors,
and may lead to misleading conclusions. In this paper, no assumption of independent activation is
made. For sigmoid activation, [Fukumizu & Amari (2000)] gives quite complicated conditions for
a local minimum to be global when adding a new node to a two-layered network. [Janzamin et al.
(2015)] gives guarantees on recovering the parameters ofa 2-layered neural network learnt with ten-
sor decomposition. In comparison, we analyze ReLU networks trained with gradient descent, which
is a more popular setting in practice.
The paper is organized as follows. Sec. 2 introduces the basic formulation and some interesting
novel properties of ReLU in multilayered ReLU networks. Sec. 3 and Sec. 4 then analyze the two-
layered model Eqn. 1 for K = 1 and K ≥ 2, respectively. Sec. 5 shows that simulation results are
consistent with theoretical analysis. Finally Sec. 7 gives detailed proofs for all theorems.
2	Preliminary
2.1	Notation
Denote X as a N -by-d input data matrix and w* is the parameter of the teacher network with
desired N -by-1 output u = g(X; w*). Now suppose we have an estimator w and the estimated
output V = g(X; w). We want to know with b loss E(w) = ɪ∣∣u 一 v∣2 = 2 ∣∣u 一 g(X; w)∣2,
whether gradient descent will converge to the desired solution w*.
2
Under review as a conference paper at ICLR 2017
The gradient descent update is w(t+1) = w(t) + η∆w(t), where ∆w(t) ≡ -VE(w(t)). If We let
η → 0, then the update rule becomes a first-order differential equation dw/dt = -VE(W),ormore
concisely, W = -VE(w). In this case, E = VE(W)TW = -∣∣VE(w)k2 ≤ 0, i.e., the function
value E is nonincreasing over time. The key is to check whether there exist other critical points
w = w* so that VE(w) = 0.
In our analysis, we assume entries of input X follow Gaussian distribution. In this situation, the gra-
dient is a random variable and ∆w = -E [VE(w)]. The expected E [E(w)] is also nonincreasing
no matter whether we follow the expected gradient or the gradient itself, because
E [e] = -E [VE(w)tVE(w)] ≤ -E [VE(w)]t E [VE(w)] ≤ 0	(2)
Therefore, we analyze the behavior of expected gradient E [VE(w)] rather than VE(w).
2.2	Properties of ReLU
In this paper, we discover a few useful properties of ReLU that make our analysis much simpler.
Denote D = D(w) = diag(Xw > 0) as a N -by-N diagonal matrix. The l-th diagnonal element of
D is a binary variable showing whether the neuron is on for sample l. Using this notation, we could
write σ(Xw) = DXw. Note that D only depends on the direction ofw but not its magnitude.
Note that for ReLU, D is also “tranparent” on derivatives. For example, the Jacobian Jw [σ(Xw)] =
σ0(Xw)X = DX at differentiable regions. This gives a very concise rule for gradient descent in
ReLU network: suppose we have negative gradient inflow vector g (of dimension N -by-1) on the
current ReLU node with weights w, then we can simply write the update ∆w as:
∆w = JW [σ(X w)]lg = X TDg	(3)
This can be easily applied to multilayer ReLU network. Denote j ∈ [c] if node j is in layer c,
dc as the width of layer c, and uj and vj as the output of teacher network and student network,
respectively. A simple deduction yields the following lemma:
Lemma 2.1 For neural network with ReLU nonlinearity and using l2 loss to match with a teacher
network of the same size, the negative gradient inflow gj for node j at layer c has the following
form:
gj = Lj X(Lj*0uj0 - Lj0vj0 )	(4)
j0
where Lj and Lj* are N -by-N diagonal matrices. For any k ∈ [c + 1], Lk =	j∈[c] wjk Dj Lj and
similarly for L*k. For the first layer, L = L* = I.
The intuition here is to start from g = u - v (true for l2 loss) at the top layer and use induction.
With this formulation, we could write the finite dynamics for wc (all parameters in layer c). Denote
the N -by-dc+1 dc matrix Rc = [LjDj]j∈[c]Xc and Rc* = [Lj*Dj*]j∈[c]Xc*. Using gradient descent
rules:
∆wj = XcT Dj gj =XcTDjLj XLj*0Dj*0Xc*wj*0 -XLj0Dj0Xcwj0	(5)
j0	j0
= XcTDjLj(Rc*wc* - Rcwc)	(6)
Therefore we have:
∆wc = RcT (Rc*wc* - Rcwc)	(7)
3	Single ReLU Case
Let’s start with the simplest case where there is only one ReLU node, K = 1. At iteration t,
following Eqn. 3, the gradient update rule is:
∆w(t) = XTD(t)g(t) = XTD(t)(D*Xw* - D(t)Xw(t))	(8)
Note here how the notation ofD(t) comes into play (and D(t) D(t) = D(t)). Indeed, when the neuron
is cut off at sample l, then (D(t))ll is zero and will block the corresponding gradient component.
3
Under review as a conference paper at ICLR 2017
Linear case. In this situation D(t) = D* = I (no gating in either forward or backward propagation)
and:
w(t+1) = W㈤ + WX1X(w* - W㈤)	(9)
where η∕N is the learning rate. When it is sufficiently small so that the spectral radius P(I -
NnX1X) < 1, w(t+1) will converge to w* when t → +∞. Note that this convergence is guaranteed
for any initial condition W(1), ifX|X is full rank with suitable η. This is consistent with its convex
nature. If entries of X follow i.i.d Gaussian distribution, then E [-ɪX|X] = I and the condition
satisfies.
Nonlinear (ReLU) case. In this case, ∆W = X|D(D*XW* - DXW) in which D is a function of
w. Intuitively, this term goes to zero when W → w*, and should be approximated to be N (w* -
W) in the i.i.d Gaussian case, since roughly half of the samples are blocked. However, once we
make such approximation, we lost the nonlinear behavior of the network and would draw the wrong
conclusion of global convergence.
Then how should we analyze it? Notice that in ∆w, both of the two terms have the form F(e, w) =
X | D(e)D(w)X w. Using this form, E [∆w] = E [F (w/kwk, w*)] - E [F (w/kwk, w)]. Here e
is a unit vector called the “projected” weight. In the following, we will show that E [F (e, w)] has
the following close form under i.i.d Gaussian assumption on X :
Lemma 3.1 Denote F (e, w) = X|D(e)D(w)Xw where e is a unit vector, X =
[xι, x2,…，XN]| is N-by-d sample matrix and D(w) = diag(XW > 0) is a binary diagonal
matrix. If Xi 〜 N(0, I) and are i.i.d (and thus bias-free), then:
E [F(e, w)] = — [(π — θ)w + ∣∣wk sin θe]
2π
(10)
where θ = ∠(e, w) ∈ [0, π] is the angle between e and w.
Note that the expectation analysis smooths out the non-differentiable property of ReLU, leaving
only one singularity at e = 0. The intuition is that expectation analysis involves an integration over
the data distribution. With simple algebraic manipulation, E [∆w] takes the following closed form:
E [∆w] = —(w* — w) + ɪ (αsinθw — θw*)
2	2π
(11)
where α = ∣∣w* ∣∕∣w∣ and θ ∈ [0,∏] is the angle between W and w*. The first term is expected
while the last two terms show the nonlinear behavior. Using Lyapunov’s method, we show that the
dynamics (if treated continuously) converges to w* when W(I) ∈ Ω = {w : ∣w - w*∣ < ∣w*∣}:
Lemma 3.2 When W(I) ∈ Ω = {w : ∣w — w*∣ < ∣∣w*∣} ,following the dynamics of Eqn. 11, the
Lyapunov function V (w) = ɪ ∣∣w — w*∣2 has V < 0 and the system is asymptotically stable and
thus w(t) → w* when t → +∞.
See Appendix for the proof. The intuition is to represent V as a 2-by-2 bilinear form of vector
[∣w∣, ∣w* ∣], and the bilinear coefficient matrix is positive definite. One question arises: will the
same approach show the dynamics converges when the initial conditions lie outside the region Ω, in
particular for any region that includes the origin? The answer is probably no. Note that w = 0 is a
singularity in which ∆w is not continuous (if approaching from different directions towards w = 0,
∆w is different). It is due to the fact that ReLU function is not differentiable at the origin. We could
remove this singularity by “smoothing out” ReLU around the origin. This will yield ∆w → 0 when
w → 0. In this case, V(0) = 0 so Lyapunov method could only tell that the dynamics is stable but
not convergent. Note that for ReLU activation, σ0 (x) = 0 for certain negative x even after a local
smoothing, so the global convergence claim in [Mei et al. (2016)] for l2 loss does not apply.
Random Initialization. Then we study how to sample W⑴ so that W⑴ ∈ Ω. We would like
to sample within Ω, but we don,t know where is w*. Sampling around origin with big radius
r ≥ 2∣w* ∣ is inefficient in particular in high-dimensional space. This is because when the sam-
ple is uniform, the probability of hitting the ball is proportional to (r∕∣w*∣)d ≤ 2-d, which is
exponentially small.
4
Under review as a conference paper at ICLR 2017
Figure 2: (a) Sampling strategy to maximize the probability of convergence. (b) Relationship be-
tween sampling range r and desired probability of success (1 - e)/2. (C) Geometry of K = 1 2D
case. There is a singularity at the origin. Initialization with random weights around the origin has
decent probability to converge to w*.
A better idea is to sample around the origin with very small radius (but not at W = 0), so that
the convergent hypersphere behaves like a hyperplane near the origin, and thus almost half of the
samples is useful (Fig. 2(a)), as shown in the following theorem:
Theorem 3.3 The dynamics in Eqn. 11 converges to w* with probability at least (1 — e)/2, if the
initial value W(I) is sampled Uniformlyfrom Br = {w : ∣∣w∣∣ ≤ r} with r ≤ e J^+1 ∣∣w* k.
The intution here is to lower-bound the probability of the shaded area (Fig. 2(b)). From the proof,
the conclusion could be made stronger to show r 〜 1/ʌ/d, consistent with common initialization
techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an
example in the 2D case, in which there is a singularity at the origin, and sampling towards w* yields
the convergence. This is consistent with the analysis above.
4 Multiple ReLUs Case
Now we are ready to analyze the network g(x) = PK=I σ(w|x) for K ≥ 2 (Fig. 1(c)). Theoretical
analysis of such networks is also the main topic in many previous works [Saad & Solla (1996);
Soudry & Carmon (2016); Fukumizu & Amari (2000)]. In this case, Lj = Lj = I for 1 ≤ j ≤ K.
Then we have the following nonlinear dynamics from Eqn. 7:
K
∆wj = X f(wj,wj0,wj*0)	(12)
j0=1
where f = F (wj /∣wj ∣, wj*0) - F (wj /∣wj ∣, wj0). Therefore, using Eqn. 10, its expectation is:
NE f (Wj, wj0, WjO)I =(π - θ*j0)wj0 - (π - θj0)wj0 + ( ∣⅛T Sinθ*j0 - k⅛∣T Sinθj0) Wj
∣wj ∣	∣wj ∣
(13)
00
where θj ≡ ∠(Wj, Wj*0) and θj ≡ ∠(Wj, Wj0).
Eqn. 12 (and its expected version) gives very complicated nonlinear dynamics and could be hard
to solve in general. Unlike K = 1, a similar approach with Lyaponov function does not yield a
decisive conclusion. However, if we consider the symmetric case: Wj = Pj W and Wj* = Pj W*
where Pj is a cyclic permutation matrix that maps index j0 + 1 to (j0 + j mod K) + 1 (and P1 is
the identity matrix), then RHS of the expected version of Eqn. 12 can be simplified as follows:
E[∆Wj] = XE f(Wj,Wj0,Wj*0) = XE[f(PjW,Pj0W,Pj0W*)]
j0	j0
= XE[f(PjW,PjPj00W,PjPj00W*)]	({Pj}jK=1 iS a group)
j00
= Pj XE [f(W,Pj00W,Pj00W*)]	(∣PW1∣ = ∣W1∣, ∠(PW1,PW2) = ∠(W1,W2))
j00
= PjE [∆W1]	(14)
5
Under review as a conference paper at ICLR 2017
which means that if all Wj and Wj are symmetric under the action of cyclic group, so does their
expected gradient. Therefore, the trajectory {w(t)} keeps such cyclic structure. Instead of solving a
system of K equations, we only need to solve one:
K
E [∆W] = X E [f(W, PjW, PjWj)]	(15)
j=1
Surprisingly, there is another layer of symmetry in Eqn. 15 when {Wjj } forms an orthonomal basis
(Wjj0|Wjj = δjj0). In this case, if we start with W(1) = xWj + y Pj6=1 PjWj then we could show
that the trajectory keeps this structure and Eqn. 15 can be further reduced into the following 2D
nonlinear dynamics:
2π E ∆y	= - 卜π - φ)(x - 1+(K - 1)y)] 1] + [φ* - φ +φ x- 1}
+ [(K - 1)(α sin φj -sinφ)+αsinθ]	xy	(16)
Here the symmetrical factor (α ≡ kWjj0 k/kWj k, θ ≡ θjjj, φ	≡	θjj0,	φj	≡ θjjj0)	are defined as
follows:
α = (x2 + (K - 1)y2)-1/2,	cos θ = αx,	cos φj = αy, cos φ = α2 (2xy + (K - 2)y2) (17)
For this 2D dynamics, we thus have the following theorem:
Theorem 4.1 For any K ≥ 2, the 2D dynamics (Eqn. 16) shows the following behaviors:
(1)	Symmetric case. If the initial condition x(1) = y(1) ∈ (0, 1], then the dynamics reduces to
1D and converges to a saddle point X = y = ∏K (√K - 1 一 arccos(1∕√K) + π).
(2)	Symmetry-Breaking. If (X⑴，y(1)) ∈ Ω = {x ∈ (0, 1],y ∈ [0, 1],x > y}, then dynamics
always converges to (X, y) = (1, 0).
From (X(t), y(t)) we could recover W(jt) = X(t)Wjj + y(t) Pj06=j Wjj0. Obviously, a convergence of
Eqn. 16 to (1, 0) means Eqn. 12 converges to {Wjj}, i.e, the teacher parameters are recovered:
Corollary 4.2 For a bias-free two-layeredReLU network g(x; w) = Ej σ(w∣x) that takes Gaus-
sian i.i.d inputs (Fig. 1), if the teacher’s parameters {Wjj } form orthogonal bases, then when
the student parameters is initialized in the form of wj(1) = X(1)wjj + y(1) Pj06=j wjj0 where
(X⑴，y⑴)∈ Ω = {x ∈ (0, 1],y ∈ [0, 1],x > y}, then the dynamics (Eqn. 12) converges to
{wjj } without being trapped into local minima.
When symmetry is broken, since the closure of Ω includes the origin, there exists a path starting
at arbitrarily small neighborhood of origin to wj , regardless of how large kwj k is. In contrast to
traditional convex analysis that only gives the local parameter-dependent convergence basin around
wjj , here we obtain a convergence basin that is parameter-independent. In comparison, [Saad &
Solla (1996)] uses a different activation function (σ(X) = erf(X)) and only analyzes local behaviors
near the two fixed points (the symmetric saddle point and the teacher’s weights wj ), leaving sym-
metry breaking an empirical procedure. Here we show that it is possible to give global convergence
analysis on certain symmetry breaking cases for two-layered ReLU network.
By symmetry, Corollary 4.1 immediately suggests that when w(1) = y(1) PjK=1 wjj + (X(1) -
y(1))wjj0, then the dynamics will converge to Pj0wj. Since X > y but can be arbitrarily close, a
slighest preturbation on the symmetric solution X = y leads to a different fixed point, which is a
permutation of wj . This is very similar to Spontaneously Symmetric-Breaking (SSB) procedure in
physics, in which a high energy state with full symmetry goes to a low energy state and only retains
part of the symmetry. In this case, the energy is the objective function E, the high energy state is the
initialization that is almost symmetrical but with small fluctuation, and the low energy state is the
fixed point the dynamics converges into.
6
Under review as a conference paper at ICLR 2017
(b) Relative RMS error w.r.t #sample (Gaussian distribution)
(a) Distribution of relative RMS error on angle
0.0	0.5	1.0	1.5	2.0	2.5	3.0
Angle (in radius)
10s IO4 IO5 10β	10,
#Samples
(a) Vector field in (x, y) plane (K = 2)
1.0
10，	I。，	105	1O,	IO7
#Samples
(b) Vector field in (x, y) plane (K = 5)
0.8
Figure 3: (a) Distribution of relative RMS error with respect to θ = ∠(w, e). (b) Relative RMS
error decreases with sample size, showing the asympototic behavior of the close form expression
Eqn. 10. (c) Eqn. 10 also works well when the input data X are generated by other zero-mean
distribution X, e.g., uniform distribution in [-1/2, 1/2].
(c) Relative RMS error w.r.t #sample (Uniform distri.)
1O, IO4 IO5 10'	1O,
#Samples
0.6
。：MJSS-P-E6-。M pe-enbs
Figure 4: (a)-(b) Vector field in (x, y) plane following 2D dynamics (Eqn. 16) for K = 2 and
K = 5. Saddle points are visible. The parameters of teacher’s network are at (1, 0). (c) Trajectory
in (x, y) plane for K = 2, K = 5, and K = 10. All trajectories start from (10-3, 0). Even the
starting point are aligned with w*, gradient descent dynamics takes detours. (d) Training curve.
When K is larger the convergence is faster.
From the simulation shown in Fig. 4, we could see that gradient descent takes a detour to reach the
desired solution w*, even when the initialization is aligned with w*. This is because in the first
stage, all ReLU nodes receive the residue and try to explain the data in the same way (both x and
y increases); when the “obvious” component has been explained away, then the residue changes
its direction and pushes some ReLU nodes to explain other components as well (x increases but y
decreases).
Empirically this path also converges to w* under noise. We leave it a conjecture that the system con-
verges in the presence of reasonably large noise. If this conjecture is true, then with high probability
a random initialization stays in the convergence basin and converges to a permutation of w*. The
reason is that a random initialization almost never gives ties. Without a tie, there exists one leading
component which will dominate the convergence.
Conjecture 4.3 When the initialization w(1) = x(1)wj* + y(1) j06=j wj*0 + , where is Gaussian
noise and (x(1),y(1)) ∈ Ω, then the dynamics Eqn. 12 also converges to w* without trapped into
local minima.
5	Simulation
5.1	Close form solution for one ReLU node
We verify our close form expression of E [F (e, w)] = E [X|D(e)D(w)Xw] (Eqn. 10) with sim-
ulation. We randomly pick e and w so that their angle ∠(e, w) is uniformly distributed in [0, π].
We prepare the input data X with standard Gaussian distribution and compare the close form so-
lution E [F (e, w)] with F(e, w), the actual data term in gradient descent without expectation. We
use relative RMS error: err = kE [F (e, w)] - F(e, w)k/kF (e, w)k. As shown in Fig. 3(a), The
error distribution on angles shows the properties of the close-form solution. For small θ, D(w) and
7
Under review as a conference paper at ICLR 2017
0.0
0
1.0
0.8
0.6
0.4
0.0
1.0
0.8
0.6
0.4
0.2
0.2
1.0
0.8
0.6
0.4
0.2
1.0
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
#Iteration
#Iteration
#Iteration
0.0 L-
0
0.0
0
noise = 2.0, top-w = 1
20
40
60
80
100
#Iteration
0.8
0.6
0.4
0.2
20
40
60
80
100
#Iteration
Figure 5: Top row: Convergence when the initial weights deviates from symmetric initialization:
W(I) = 10-3w* + e. Here e 〜N(0,10-3 * noise). The 2-layered network converges to w* until
very large noise is present. Both teacher and student networks use g(x) = PK=I σ(w∣x). Each
experiment has 8 runs. Bottom row: Convergence when We use g2(x) = PK=I ajσ(w∣x). Here
the top weights aj is fixed at different numbers (rather than 1). Large positive aj correponds to fast
convergence. When aj has positive/negative components, the network does not converge to w*.
D(e) overlaps sufficiently, giving a reliable estimation for the gradient. When θ → π, D(w) and
D(e) tend not to overlap, leaving very few data involved in the gradient computation. As a result,
the variance grows. Note that all our analysis operate on θ ∈ [0, n/2] and is not affected by this
behavior. In the following, angles are sampled from [0, ∏∕2].
Fig. 3(a) shows that the close form expression becomes more accurate with more samples. We also
examine other zero-mean distributions ofX, e.g., uniform distribution in [-1∕2, 1∕2]. As shown in
Fig. 3(d), the close form expression still works for large d, showing that it could be quite general.
Note that the error is computed up to a scaling constant, due to the difference in normalization
constants among different distributions. We leave it to the future work to prove its usability for
broader distributions.
5.2	Convergence for multiple ReLU nodes
Fig. 4(a) and (b) shows the 2D vector field given by the 2D dynamics (Eqn. 16) and Fig. 4(c) shows
the 2D trajectory towards convergence to the teacher’s parameters w*. Interestingly, even when we
initialize the weights as (10-3, 0), aligning with w*, the gradient descent takes detours to reach the
destination. One explanation is, at the beginning all nodes move similar direction trying to explain
the data, once the data have been explained partly, specialization follows (y decreases).
Fig. 5 shows empirical convergence for K ≥ 2, when the initialization deviates from symmetric
initialization in Thm. 4.1. Unless the deviation is large, gradient descent converges to w*. We
also check the convergence of a more general network g2(x) = PK=I aj σ(w∣x). When aj > 0
convergence follows; however, when some aj is negative, the network does not converge to w*,
even that the student network already knows the ground truth value of {aj}jK=1.
6	Conclusion and Future Work
In this paper, we analyze the nonlinear dynamical behavior of certain two-layered bias-free ReLU
networks in the form of g(x; w) = PK=I σ(w∣x), where σ = max(x, 0) is the ReLU node. We
assume that the input x follows Gaussian distribution and the output is generated by a teacher net-
work with parameters w* . In K = 1 we show a close-form nonlinear dynamics can be obtained and
its convergence to w* can be proven, if we sample the initialization properly. Such initialization is
consistent with common practice [Glorot & Bengio (2010); He et al. (2015)] and is independent of
the value of w*. For K ≥ 2, when the teacher parameters {wj*} form a orthonormal bases, we prove
that the trajectory from symmetric initialization is trapped into a saddle point, while certain sym-
metric breaking initialization converges to w* without trapped into any local minima. Future work
includes analysis of general cases (or symmetric case plus noise) for K ≥ 2, and a generalization to
multilayer ReLU (or other nonlinear) networks.
8
Under review as a conference paper at ICLR 2017
References
Choromanska, Anna, Henaff, Mikael, Mathieu, Michael, Arous, Gerard Ben, and LeCun, Yann. The
loss surfaces of multilayer networks. In AISTATS, 2015a.
Choromanska, Anna, LeCun, Yann, and Arous, Gerard Ben. Open problem: The landscape of the
loss surfaces of multilayer networks. In Proceedings of The 28th Conference on Learning Theory,
COLT2015, Paris, France, July 3, volume 6,pp.1756-1760, 2015b.
Fukumizu, Kenji and Amari, Shun-ichi. Local minima and plateaus in hierarchical structures of
multilayer perceptrons. Neural Networks,13(3):317-327, 2000.
Glorot, Xavier and Bengio, Yoshua. Understanding the difficulty of training deep feedforward neural
networks. In Aistats, volume 9, pp. 249-256, 2010.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectifiers: Sur-
passing human-level performance on imagenet classification. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pp. 1026-1034, 2015.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image
recognition. Computer Vision anad Pattern Recognition (CVPR), 2016.
Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep,
Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural net-
works for acoustic modeling in speech recognition: The shared views of four research groups.
IEEE Signal Processing Magazine, 29(6):82-97, 2012.
Janzamin, Majid, Sedghi, Hanie, and Anandkumar, Anima. Beating the perils of non-convexity:
Guaranteed training of neural networks using tensor methods. CoRR abs/1506.08473, 2015.
Kawaguchi, Kenji. Deep learning without poor local minima. Advances in Neural Information
Processing Systems, 2016.
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
LeCun, Yann A, Bottou, Leon, Orr, Genevieve B, and Muller, Klaus-Robert. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Mei, Song, Bai, Yu, and Montanari, Andrea. The landscape of empirical risk for non-convex losses.
arXiv preprint arXiv:1607.06534, 2016.
Saad, David and Solla, Sara A. Dynamics of on-line gradient descent learning for multilayer neural
networks. Advances in Neural Information Processing Systems, pp. 302-308, 1996.
Saxe, Andrew M, McClelland, James L, and Ganguli, Surya. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image
recognition. International Conference on Learning Representations (ICLR), 2015.
Soudry, Daniel and Carmon, Yair. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural net-
works. In Advances in neural information processing systems, pp. 3104-3112, 2014.
Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir,
Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions.
In Computer Vision and Pattern Recognition (CVPR), pp. 1-9, 2015.
9
Under review as a conference paper at ICLR 2017
Figure 6: (a)-(b) TWo cases in Lemma 7.2. (C) Convergence analysis in the symmetric two-layered
case.
7 APPENDIX
Here we list all detailed proof for all the theorems.
7.1 Properties of RELU networks
Lemma 7.1 For neural network with ReLU nonlinearity and using l2 loss to match with a teacher
network of the same size, the negative gradient inflow gj for node j at layer c has the following
form:
gj = Lj X(L)UjO- Lj0vj0)	(18)
j0
where Lj and Lj are N-by-N diagonal matrices. Forany k ∈ [c + 1], Lk = Ej∙∈[c] WjkDjLj and
Similarlyfor Lk.
Proof We prove by induction on layer. For the first layer, there is only one node with g = U - v,
therefore Lj = Lj0 = I. suppose the condition holds for all node j ∈ [c]. Then for node k ∈ [c+ 1],
we have:
gk = wjkDjgj =wjkDjLj	Ljj0Uj0 -	Lj0 vj0
j0
j0
wjkDjLj	Ljj0	Djj0wjjk0Uk0 -	Lj0	Dj0wjk0vk0
j0	k0
j0	k0
wjkDjLj	Ljj0Djj0wjjk0Uk0 -wjkDjLj	Lj0Dj0wjk0 vk0
j0	k0
j0	k0
k0 j
wjkDjLj	Ljj0Djj0wjjk0Uk0-	wjk Dj Lj	X	Lj0Dj0wjk0	vk0
j0
k0 j
j0
j
j
j
j
j
setting Lk =	jwjkDjLj and Ljk =	jwjjkDjjLjj (both are diagonal matrices), we thus have:
gk =	LkLjk0Uk0 - Lk Lk0 vk0 = Lk	Lk0 Uk0 - Lk0 vk0
(19)
k0
k0
10
Under review as a conference paper at ICLR 2017
7.2 One ReLU Case
Lemma 7.2 Suppose F(e, w) = X|D(e)D(w)Xw where e is a unit vector and X
[xι, X2,…，XN]l is N-by-d sample matrix. If Xi 〜N(0, I) and are i.i.d, then:
E [F(e, w)] = ɪ ((π — θ)w + ∣∣w∣ sin θe)	∣
2π
where θ ∈ [0, π] is the angle between e and w.
(20)
Proof Note that F can be written in the following form:
F(e, w) =	xixi|w
i*le≥0,xlw≥0
(21)
where Xi are samples so that X = [xι, x2,…，Xn]|. We set UP the axes related to e and W as in
Fig. 6, While the rest of the axis are prependicular to the plane. In this coordinate system, any vector
x = [r Sin φ, r coS φ, x3, . . . , xd]. We have an orthonomal set of bases: e, e⊥
—
e-w/kwk cos θ
Sinθ
(and any set of bases that span the rest of the space). Under the basis, the representation for e and w
is [1, 0d-1] and [kwk coS θ, -kwk Sin θ, 0d-2]. Note that here θ ∈ (-π, π]. The angle θ is positive
when e “chases after” w, and is otherwise negative.
NoW We consider the quality R(φo) = E [* Piiφi∈[o φ0] XiX∣]. If We take the expectation and use
polar coordinate only in the first two dimensions, we have:
R(φ0) = E
N X	Xix
i ; φi∈[0,φo]	.
+∞	+∞ φ0
0	-∞	0
-r Sin φ -
r cos φ
xd
E[xix∣∣φi ∈ [0,φo]] P [φi ∈ [0,φo]]
d
[r Sin φ r coS φ . . . xd] p(r)p(θ)	p(xk)rdrdφdx3 . . . dxd
k=3
where p(r) = e-r2/2 and p(θ) = 1∕2π. Note that R(φo) is a d-by-d matrix. The first 2-by-2 block
can be computed in close form (note that R0+∞ r2p(r)rdr = 2). Any off-diagonal element except
for the first 2-by-2 block is zero due to symmetric property of i.i.d Gaussian variables. Any diagonal
element outside the first 2-by-2 block Will be P [φi ∈ [0, φ0]] = φ0∕2π. Finally, We have:
R(φ0)	= E
N X	XiX
i ; φi∈[0,φo]
1 - cos 2φ0
2φ0 + sin 2φ0
0
0
0
2φ0Id-2
(22)
1
+ 4∏
1
4π
2φ0 - Sin 2φ0
1 - coS 2φ0
0
— Sin 2φ0	1 — coS 2φ0	0
1 — coS 2φ0	Sin 2φ0	0
0	0	0
(23)
With this equation, We could then compute E [F (e, w)]. When θ ≥ 0, the condition {i : Xi|e ≥
0, Xi|w ≥ 0} is equivalent to {i : φi ∈ [θ, π]} (Fig. 6(a)). Using w = [kwk coS θ, -kwk Sin θ, 0d-2]
and We have:
E[F(e,w)] = N (R(π) - R(θ)) w
(24)
4"n-2"N-
==
(π - θ)w + kwk
sin θ
0
((π - θ)w + kwk sin θe)
- Sin 2θ
2(π - θ)w - kwk 1 - coS 2θ
0
1 — coS 2θ	0		coS θ
Sin 2θ	0		— Sin θ
0	0		0
(25)
(26)
(27)
N
B
For θ < 0, the condition {i : xi|e ≥ 0, xi|w ≥ 0} is equivalent to {i : φi ∈ [0, π + θ]} (Fig. 6(b)),
and similarly we get
N
E [F(e, w)] = N (R(∏ + θ) — R(0)) W =——((∏ + θ)w — IlWk Sin θe)
2π
(28)
11
Under review as a conference paper at ICLR 2017
Notice that by abuse of notation, the θ appears in Eqn. 20 is the absolute value and Eqn. 20 follows.
I
Lemma 7.3 In the region ∣∣w(1) 一 w*k < ∣∣w*k ,following the dynamics (Eqn. 11), the Lyapunov
function V (w) = 1 ∣∣w — w*∣2 has V < 0 and the system is asymptotically stable and thus w(t) →
w* when t → +∞.
Proof Denote that Ω = {w : ∣w⑴ 一 w*∣ < ∣∣w*∣∣}. Note that
V = (W — W )l∆w = -y|My
where y = [∣∣w* k, ∣∣w∣∣]1 and M is the following 2-by-2 matrix:
Sin 2θ + 2π — 2θ	—(2π — θ) cos θ — Sin θ
—(2π — θ)cos θ — sin θ	2π
(29)
(30)
M =—
2
In the following we will show that M is positive definite when θ ∈ (0,∏∕2]. It suffices to show that
M11 > 0, M22 > 0 and det(M) > 0. The first two are trivial, while the last one is:
4det(M)	= 2π(sin 2θ + 2π — 2θ) — [(2π — θ) cosθ + sin θ]2	(31)
= 2π(sin 2θ + 2π — 2θ) — [(2π — θ)2 cos2 θ + (2π — θ) sin 2θ + sin2 θ] (32)
=	(4π2 — 1) sin2	θ — 4πθ + 4πθ cos2 θ — θ2 cos2 θ + θsin2θ	(33)
=	(4π2 — 4πθ —	1) sin2 θ + θcosθ(2sinθ — θcos θ)	(34)
Note	that	4π2 —	4πθ	— 1 = 4π(π —	θ) — 1 > 0 for θ ∈ [0,π∕2], and g(θ) = sin θ	—	θcos	θ ≥ 0
for θ	∈	[0, n/2]	since g(0) = 0 and	g0(θ) ≥ 0 in this region. Therefore, when θ ∈	(0,	n/2], M is
positive definite.
When θ = 0, M(θ) = π[1, —1; —1, 1] and is semi-positive definite, with the null eigenvector being
√2[1,1], i.e., ∣w∣ = ∣∣w*∣. However, along θ = 0, the only W that satisfies ∣∣w∣ = ∣∣w*∣ is
W = w*. Therefore, V = —y|My < 0 in Ω. Note that although this region could be expanded to
the entire open half-space H = {w : w|w* > 0}, it is not straightforward to prove the convergence
in H, since the trajectory might go outside H. On the other hand, Ω is the level set V < 2∣∣w*∣2 so
the trajectory starting within Ω remains inside. ∣
Theorem 7.4 The dynamics in Eqn. 11 converges to w* with probability at least (1 — )/2, if the
initial value w(1) is sampled uniformly from Br = {w : ∣w∣ ≤ r} with:
r ≤ e/d+iM*"	(35)
Proof Given a ball of radius r, we first compute the “gap” δ of sphere cap (Fig. 2(b)). First cos θ =
2∣∣W*∣∣, so δ = r cosθ = 2∣∣W*k. Then a sufficient condition for the probability argument to hold,
is to ensure that the volume Khaded of the shaded area is greater than 宁Vd(r), where Vd(r) is the
volume of d-dimensional ball of radius r. Since Vshaded ≥ 11 Vd(r) — δVd-ι, it suffices to have:
which gives
1 Vd(r) — δVd-1 ≥ 1ye Vd(r)
δ≤
e Vd
2 Vd-1
(36)
(37)
Using δ = 2jWιj and Vd(r) = Vd(I)rd, we thus have:
r≤e
Vd⑴
Vd-1(1)
∣w*∣
(38)
where Vd(1) is the volume of the unit ball. Since the volume of d-dimensional unit ball is
“小一∏d/2
Vd ⑴=Γ(d∕2 + 1)
(39)
12
Under review as a conference paper at ICLR 2017
where Γ(x) = R0∞ tx-1e-tdt. So we have
Vd ⑴ =厂 Γ(d∕2+1∕2)
⅛-1(1)= Vn Γ(d∕2+1)
(40)
From Gautschi’s Inequality
x1-s < r(x + 1) < (X + s)1-s	x > 0,0 <s< 1	(41)
Γ(x + s)
with s = 1∕2 and x = d∕2 we have:
(d+1「< Γ(d∕2+1∕2) < (d「/2
I 2 J Γ(d∕2+1)	22j
(42)
Therefore, it suffices to have
r ≤ e{d+1 kw*k	(43)
Note that this upper bound is tight when δ → 0 and d → +∞, since all inequality involved asymp-
totically becomes equal. ∣
7.3 Two Layer Case
Lemma 7.5 For φ*, θ and φ defined in Eqn. 17:
α	≡	(x2 +	(K	- 1)y 2)-1/2	(44)
cos θ	≡	αx	(45)
cos φ	≡	αy	(46)
cos φ	≡	α2 (2xy +	(K - 2)y2)	(47)
we have thefollowing relations in the triangular region Ω∈o = {(x, y) : X ≥ 0, y ≥ 0, x ≥ y + e0}
(Fig. 6(c)):
(1)	φ, φ* ∈ [0, π∕2] and θ ∈ [0, θo) where θo = arccos √K.
(2)	cos φ = 1 — α2(x — y)2 and Sin φ = α(x — y),2 — α2(x — y)2.
(3)	φ* ≥ φ (equality holds only when y = 0) and φ* > θ.
Proof Propositions (1) and (2) are computed by direct calculations. In particular, note that since
cos θ = αx = 1∕p1 + (K — 1)(y∕x)2 and X > y ≥ 0, we have cos θ ∈ (1∕√K, 1] and θ ∈ [0, θ0).
For Preposition (3), φ* = arccos ay > θ = arccos ax because x > y. Finally, for x > y > 0, We
have
cos φ α2(2xy + (K — 2)y2)
-----=------------------------=a(2x + (K — 2)y) > α(x + (K — 1)y) > 1	(48)
cos φ*	αy
The final inequality is because K ≥ 2, x, y > 0 and thus (x + (K - 1)y)2 > x2 + (K 一 1)2y2 >
x2 + (K — 1)y2 = α-2. Therefore φ* > φ. If y = 0 then φ* = φ.	∣
Theorem 7.6 For the dynamics defined in Eqn. 16, there exists 0 > 0 so that the trianglar region
Ωeo = {(x, y) : X ≥ 0, y ≥ 0, x ≥ y + e0} (Fig. 6(c)) is a convergent region. That is, the flow goes
inwards for all three edges and any trajectory starting in Ω∈o stays.
Proof We discuss the three boundaries as follows:
Case 1:	y = 0,0 ≤ X ≤ 1, horizontal line. In this case, θ = 0, φ = π∕2 and φ* = π∕2. The y
component of the dynamics in this line is:
2π	π
fι ≡ N ∆y = — 2(X — 1) ≥ 0	(49)
13
Under review as a conference paper at ICLR 2017
So ∆y points to the interior of Ω.
Case 2:	x = 1, 0 ≤ y ≤ 1, vertical line. In this case, α ≤ 1 and the x component of the dynamics
is:
2π
f2 ≡	— ∆x	=	—(π — φ)(K — 1)y — θ +(K — 1)(αSinφ — Sinφ)	+ αSinθ	(50)
=	—(K — 1) [(π — φ)y — (a sin φ* — sin φ)] + (a sin θ	— θ)	(51)
Note that since α ≤ 1, α sin θ ≤ sin θ ≤ θ, so the second term is non-positive. For the first term,
we only need to check whether (π - φ)y - (a sin φ* - sin φ) is nonnegative. Note that
(π — φ)y — (a sin φ* — sin φ)
= (π — φ)y + α(x — y)，2 — α2(x — y)2 — α ʌ/1 — α2y2
=	y [π	— φ	— α√2 — α2(x — y)2]	+	α [x√2 — α2(x — y)2 — √1 — α2y2]
In Ω we have	(x —	y)2	≤ 1, combined with α	≤	1, we have 1 ≤ √2 — α2 (x — y)2 ≤
(52)
(53)
(54)
√2 and
，1 - α2y2 ≤ 1. Since x = 1, the second term is nonnegative. For the first term, since α ≤ 1,
π — φ — α√2 — α2(x — y)2 ≥ π 一三一√2 > 0
So (π — φ)y — (a sin φ* — sin φ) ≥ 0 and ∆x ≤ 0, pointing inwards.
(55)
Case 3:	x = y + e, 0 ≤ y ≤ 1, diagonal line. We compute the inner product between (∆x, ∆y)
and (1, —1), the inward normal of Ω at the line. Using φ ≤ 2 sin φ for φ ∈ [0,∏∕2] and φ* — θ =
arccos αy — arccos αx ≥ 0 when x ≥ y, we have:
|
2π ∆x
f3(y,e) ≡ N ∆y
1
-1
φ* — θ — eφ + [(K - 1)(αsinφ* — sinφ) + αsinθ] e	(56)
(K-1)
a Sin φ*
α(K - 1)	1 -
π
1 + 2K-Iy JSin φ
α2y2 — e (1 +--.---
1 +2(K - 1),
√2 — a2e2
≥
—
Note that for y > 0:
1
1
1
αy
P(x∕y)2 + (K -Ij = √(1 + e/y)2 + (K-I) ≤ √K
(57)
For y = 0, αy = 0 < √1∕K. So We have √1 — a2y2 ≥ √1 — 1/K. And √2 — a2e2 ≤ √2.
Therefore f3 ≥ eα(K — 1)(Cι —eC2) with Ci ≡ √1 — 1/K > 0 andC2 ≡ √2(1+π∕2(K —1)) >
0. With e = eo > 0 sufficiently small, f3 > 0.	∣
Lemma 7.7 (Reparametrization) Denote e = x — y > 0. The terms αx, αy and αe involved in
the trigometric functions in Eqn. 16 has the following parameterization:
a x = K
β - β2
β+(K- 1)β2
Kβ2
(58)
where β2
√(K - β2)∕(K - 1).	The reverse transformation is given by β
√K - (K - 1)α2e2. Here β ∈ [1, √K) and β2 ∈ (0,1]. In particular, the critical point
(x, y) = (1, 0) corresponds to (β, e) = (1, 1). As a result, all trigometric functions in Eqn. 16
only depend on the single variable β. In particular, the following relationship is useful:
β = cos θ + K— - 1 sin θ
(59)
Proof This transformation can be checked by simple algebraic manipulation. For example:
αK (β - β2)
K
—(K — 1)e2 — e
α2
+ e)2 — e = y (60)
14
Under review as a conference paper at ICLR 2017
To prove Eqn. 59, first we notice that K cos θ = Kαx = β + (K - 1)β2. Therefore, we have
(K cos θ - β)2 - (K - 1)2β22 = 0, which gives β2 - 2β cos θ + 1 - K sin2 θ = 0. Solving this
quadratic equation and notice that β ≥ 1, θ ∈ [0,∏∕2] and We get:
β = cos θ + Pcos2 θ + K sin2 θ 一 1 = cos θ + √K 一 1 Sin θ	(61)
Lemma 7.8 After reparametrization (Eqn. 58), f3(β, ) ≥ 0 for ∈ (0, β2∕β]. Furthermore, the
equality is true only if (β, ) = (1, 1) or (y, ) = (0, 1).
Proof Applying the parametrization (Eqn. 58) to Eqn. 56 and notice that α = β2 = β2 (β), We
could Write
f3 = h1(β) 一 (φ + (K 一 1) sin φ)	(62)
When β is fixed, f3 noW is a monotonously decreasing function With respect to > 0. Therefore,
f3(β, ) ≥ f3(β, 0) for 0 < ≤ 0 ≡ β2∕β. If We could prove f3(β, 0) ≥ 0 and only attain zero at
knoWn critical point (β, ) = (1, 1), the proof is complete.
Denote f3 (β, 0) = f31 + f32 Where
f31(β,C = φ* 一 θ 一 /φ + /α Sin θ	(63)
f32(β,C = (K — 1)(a Sin φ* — Sin φ)e0	(64)
For f32 it suffices to prove that e0(α sin φ* — sin φ) = β2 sin φ* — β2 sin φ ≥ 0, which is equivalent
to sin φ* — sin φ∕β ≥ 0. But this is trivially true since φ* ≥ φ and β ≥ 1. Therefore, f32 ≥ 0.
Note that the equality only holds when φ* = φ and β = 1, which corresponds to the horizontal line
x ∈ (0, 1],y = 0.
For f31, since φ* ≥ φ, φ* > θ and e0 ∈ (0,1], we have the following:
f31 = e'(Φ* — Φ) + (1 — C(φ* — θ) — e,θ	+ β2 sin θ ≥ —e0θ + β2 sin θ ≥ β2 (Sin θ	— ；)	(65)
And it reduces to showing whether β sin θ —	θ is nonnegative.	Using Eqn. 59, we have:
f33(θ) = βsinθ — θ = ∣sin2θ + VK-Isin2 θ — θ	(66)
Note that f33 = cos2θ + √K — 1 sin2θ —	1 = √Kcos(2θ	— θ0) — 1, where θo =	arccos	√=.
By Prepositions 1 in Lemma 7.5, θ ∈ [0, θ0). Therefore, f303	≥ 0 and since f33 (0) =	0, f33	≥ 0.
Again the equity holds when θ = 0, φ* = φ and e0 = 1, which is the critical point (β, E) = (1,1) or
(y, E) = (0, 1). I
Theorem 7.9 For the dynamics defined in Eqn. 16, the only critical point (∆x = 0 and ∆y = 0)
within Ωe is (y, E) = (0,1).
Proof We prove by contradiction. Suppose (β, E) is a critical point other than w*. A necessary
condition for this to hold is f3 = 0 (Eqn. 56). By Lemma 7.8, E > E0 = β2∕β > 0 and
E - 1 + Ky = 1(β2 — α + β - β2) =j=≡^>≡22≤ =0	(67)
α	αα	α
So E — 1 + Ky is strictly greater than zero. On the other hand, the condition f3 = 0 implies that
((K — 1)(α sin φ* — sin φ) + α sin θ) = --(φ* — θ) + φ	(68)
E
Using φ ∈ [0, π∕2], φ* ≥ φ and φ* > θ, we have:
2π
N ∆y =	—(π — Φ)(e — 1 + Ky)	—	(φ	—	φ)	— φy +((K - 1)(α sin φ	— sin φ)	+ α sin θ) y
=	-(∏ - φ)(E - 1 + Ky)	-	(φ*	-	φ)	- U(φ* - θ)y < 0	(69)
E
So the current point (β, E) cannot be a critical point. ∣
15
Under review as a conference paper at ICLR 2017
Theorem 7.10	Any trajectory in Ω∈o converges to (y, e) = (1,0), following the dynamics defined
in Eqn. 16.
Proof We have LyaPonov function V = E [E] so that V = -E [∆wl∆w] ≤ -E [∆w]τ E [∆w] ≤
0. By Thm. 7.9, other than the optimal solution w*, there is no other symmetric critical point,
∆w = 0 and thus V < 0. On the other hand, by Thm. 7.6, the triangular region Ω. is convergent, in
which the 2D dynamics is C∞ differentiable. Therefore, any 2D solution curve ξ(t) will stay within.
By PoincareBendixson theorem, when there is a unique critical point, the curve either converges to a
limit circle or the critical point. However, limit cycle is not possible since V is strictly monotonous
decreasing along the curve. Therefore, ξ(t) will converge to the unique critical point, which is
(y, ) = (1, 0) and so does the symmetric system (Eqn. 12).
Theorem 7.11	When x = y ∈ (0, 1], the 2D dynamics (Eqn. 16) reduces to the following 1D case:
2π
—∆x = —πK (X — x*)
(70)
where x* = ∏K (√K — 1 — arccοs(1∕√K) + π). Furthermore, x* is a convergent CriticaIpoint.
Proof The 1D system can be computed with simple algebraic manipulations (note that when X = y,
φ = 0 and θ = φ* = arccοs(1∕√K)). Note that the 1D system is linear and its close form solution
is x(t) = x0 + Ce-K/2Nt and thus convergent.
16