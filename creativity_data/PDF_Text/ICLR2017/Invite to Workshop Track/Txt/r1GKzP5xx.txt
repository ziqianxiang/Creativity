Under review as a conference paper at ICLR 2017
Recurrent Normalization Propagation
Cesar Laurent, Nicolas Ballas & Pascal Vincent*
Montreal Institute for Learning Algorithms (MILA)
DePartement d,Informatique et de Recherche OPerationnelle
Universite de Montreal
Montreal, Quebec, Canada
{firstname.lastname}@umontreal.ca
Ab stract
We ProPose an LSTM Parametrization that Preserves the means and variances of
the hidden states and memory cells across time. While having training benefits
similar to Recurrent Batch Normalization and Layer Normalization, it does not
need to estimate statistics at each time steP, therefore, requiring fewer comPuta-
tions overall. We also investigate the Parametrization imPact on the gradient flows
and Present a way of initializing the weights accordingly.
We evaluate our ProPosal on language modelling and image generative modelling
tasks. We emPirically show that it Performs similarly or better than other recurrent
normalization aPProaches, while being faster to execute.
1	Introduction
Recurrent neural network have shown remarkably good Performances for sequential modelling tasks
including machine translation (Bahdanau et al., 2015), visual caPtioning (Xu et al., 2015; Yao et al.,
2015) or question answering (Hermann et al., 2015). However, such models remain notoriously
hard to train with gradient backProPagation. As the number of time stePs in the inPut sequence
increases, the contractive or exPanding effects associated with the state-to-state transformation at
each time steP can shrink or grow exPonentially, leading resPectively to vanishing or exPloding
gradients (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2012). In Particular, with gradi-
ent vanishing, states at a given time are not influenced by changes haPPening much earlier in the
sequence, Preventing the model from learning long-term dePendencies.
While the long-term dePendencies Problem is unsolvable in absolute (Hochreiter, 1991; Bengio
et al., 1994), different RNN Parameterizations, such as LSTM or GRU (Hochreiter & Schmidhuber,
1997; Cho et al., 2014) can helP mitigate it. Furthermore, the LSTM Parametrization has been
recently extended to include layer-wise normalization (Cooijmans et al., 2016; Ba et al., 2016),
building uPon Batch Normalization (BN) (Ioffe & Szegedy, 2015). By normalizing the hidden state
distributions to a fix scale and shift through the different time stePs, normalized LSTMs have been
shown to ease training, resulting in a Parametrization that converges faster than a standard LSTM.
However, normalized LSTM introduces extra-comPutations as it involves standardizing the hidden
states, enforcing their means and variances at each time steP. By contrast, we ProPose an LSTM
reParametrization that allows by construction to cheaPly Preserve the normalization of the hidden
states through time. Our aPProach can be seen as the recurrent counterPart to the recent normal-
ization ProPagation aPPlied in feed-forward network (ArPit et al., 2016). It results in faster training
convergence similar to Layer Normalization (LN) and Recurrent Batch Normalization while requir-
ing fewer oPerations Per time steP and generalizing naturally to variable length sequences.
In addition, we investigate the imPact of our Parametrization, and more generally of normalized
LSTM, on the vanishing and exPloding gradient Problems. We observe that layer-wise normalization
Provides a direct way to orient LSTM behaviour toward either gradient exPlosion or vanishing, and
therefore biases the LSTM either towards reliably storing bits of information throughout time or
allowing it to be more sensitive to new inPut changes.
* Associate Fellow, Canadian Institute For Advanced Research (CIFAR)
1
Under review as a conference paper at ICLR 2017
We empirically validate our proposal on character-level language modelling on the Penn Treebank
corpus (Marcus et al., 1993) and on image generative modelling, applying our normalisation to the
DRAW architecture (Gregor et al., 2015).
The paper is structured as follows: section 2 provides a brief overview of the Batch-Normalized
LSTM, in section 3 we derive our Normalized LSTM, section 4 investigates the impact of such
normalization on the gradient flow, section 5 presents some experimental results, and we conclude
in section 5.
2	Pre-requisites
2.1	BN-LSTM
Batch-Normalized Long Short-Term Memory (BN-LSTM) (Cooijmans et al., 2016) is a
reparametrization of LSTM that takes advantage of Batch Normalization (BN) to address the Co-
variate Shift (Shimodaira, 2000) occurring between time steps. Changes in the LSTM output at one
time-step are likely to cause correlated changes in the summed inputs of the sequence next time-
steps. This Temporal Covariate Shift can slow down the training process as the parameters of the
model must not only be updated to minimize the cost of the task at hand but also adapt to the chang-
ing distribution of the inputs. In other words, the latter time steps in an LSTM need to account for
the shifting distribution of the previous hidden states.
BN-LSTM proposes to reduce this temporal covariate shift by fixing the mean and the variance at
each time step, relying on the BN transform
BN(x; γ, β) = γ
X — E [x]
Vdar[x] +
(1)
where E[X], Var[X] are the activation mean and variance estimated from the mini-batch samples.
Given an input sequence X = (X1, X2, . . . , XT), the BN-LSTM defines a sequence of hidden states
ht and memory cell states ct according to
/ ɪt ʌ
i
ft	= BN(Wx xt； γχ,βχ) + BN(Whht-ι; Yh ,βh) + b	⑵
ot
∖ gt /
Ct = σ(it) Θ tanh(gt) + σ(ft) Θ ct-i	(3)
ht = σ(0t) Θ tanh(BN(ct; γc, βc)),	(4)
where Wh ∈ Rdh ×4dh , Wx ∈ Rdx ×4dh , b ∈ R4dh and the initial states h0 ∈ Rdh , c0 ∈ Rdh are
model parameters. σ is the logistic sigmoid function, and Θ denotes the Hadamard product. Ba et al.
(2016) latter extended this parametrization by estimating the normalizing statistics (E[x], Var[x])
using the different feature channels rather than mini-batch samples in order to naturally generalize
to variable length sequences.
2.2	Normalization Propagation
While increasing the training convergence speed relatively to a standard LSTM (Cooijmans et al.,
2016), BN-LSTM needs to perform more computations per sample as it requires to compute 3x the
BN transform at each time step.
On the other hand, Normalization Propagation (Norm Prop) (Arpit et al., 2016) aims at preserve the
normalization of the input throughout the network. Unlike BN, the normalization doesn’t rely on
the statistics of the mini-batch. Instead, it is the structure of the network itself that maintains the
normalization. We therefore propose an LSTM reparametrization that preserves the normalization
through the different time steps in order to avoid those extra computation.
2
Under review as a conference paper at ICLR 2017
3	Normalized LSTM
While Norm Prop properties are appealing for recurrent models, its application to LSTM is not
straightforward due to the memory cell structure. In this section we show how to derive a LSTM
reparametrization that preserves normalization of the state ht through time.
3.1	Construction of the Normalized LSTM
Following (Arpit et al., 2016; Salimans & Kingma, 2016), we will attempt to ensure, through an
analytical reparametrization, that several intermediate quantities in the computation remain approx-
imately standardized. We first compensate for the distribution changes induced by the weight matri-
ces in the gates and cell candidate gt computations
/ it ʌ
f	WT	Wh .
J = Yx	—∏-xt + Yhn^~~∏- ht-ι + b.	(5)
ot	llWx,ill2	lWh,ill2
∖ gt /
where || W∙,i ∣∣2 is the vector of L2-norm of each line of the matrix and Yx and Yh are the trainable
rescaling factors that restore the representation power lost in the rescaling of the weight matrices.
To preserve the constant error carousel mechanism of the LSTM, we use the usual cell update,
ct = σ(it) Θ tanh(gt) + σ(ft) Θ ct-i	(6)
Let us now construct an approximate analytical estimate of Var(ct). The evolution of ct through
time can bee seen as a geometric series, With σ(ft) as constant ratio. Since σ(∙) is upper-bounded by
(and in practice smaller than) 1, ct will converge in expectation to a fixed value. This is the reason
why in BN-LSTM the mini-batch statistics converge to a fixed value after a few time steps (Cooij-
mans et al., 2016). Moreover, if we consider that it, ft, gt and ct-i are (as a rough approximation)
independent1 , we can use the variance product rule of two independent random variables X and Y
Var[XY] = Var[X]Var[Y] +Var[X]E[Y]2 +Var[Y]E[X]2	(7)
to compute Var[ct]. Considering that E[tanh(gt)] ≈ 0 and assuming that the cell has converged i.e.
Var[ct] = Var[ct-i], we have
Var[ct]
Var[tanh(gt)]
一 ~	, ,	一 ∙~	、- C
Var[σ(it)] + E[σ(it)]2
1 — Var[σ(ft)] - E[σ(ft)]2
(8)
We can therefore analytically or numerically compute the mean and variance of each of those ele-
ments, assuming that both input xt and hidden state ht-i are independent drawn from N(0, 1)
E[it] = E[σ(Yxzx + Yhzh)]	(9)
Var[it] = Var[σ(Yxzx + Yhzh)]	(10)
E[gt] = E[tanh(Yxzx + Yhzh)]	(11)
Var[gt] = Var[tanh(Yxzx + Yhzh)]	(12)
where zx,zh 〜N(0,1). The statistics of the gates ot and f can be computed in a similar way. We
can then compute the value to which Var[ct] converges. Using this variance estimate, we compen-
sate ct in order to compute the next hidden state ht
ht = σ(0t) Θ tanh (	Ycct	)	(13)
Var[ct]
Since we assumed that Var[ht-i] = 1, to ensure that we need to correct for the variance induced by
the product of the tanh with the output gate. Using again the variance product rule (equation 7) we
obtain
Var[ht] = Var tanh ^p=c∣==)] (Var[σ(ft)] + E[σ(0t)]2)	(14)
We can estimate this variance through similar computation than equation 12. Scaling ht with
1 / dVar[ht] ensure that its variance is 1 and so the propagation is maintained throughout the re-
currence.
1This assumption is strong, but we don’t have any easy way to model the covariance between those terms
without estimating it from the data.
3
Under review as a conference paper at ICLR 2017
3.2	Proposed Reparametrization
Using equations 5, 6 and 13, we propose the following reparametrization of the LSTM, simply called
the Normalized LSTM
Λt∖
〜
ft
o
∖gt∕
Wx	Wh
Yx ∣∣Wχ,i∣∣2 xt + Yh ∣∣Wh,i∣∣2 ht-1+
ct = σ(it) tanh(gft) + σ(ft)	ct-1
(15)
(16)
1
pvar[ht]
σ(oft) tan
(17)
where Var[ct] and Var[ht] are computed using equations 8 and 14, respectively. Those two vari-
ances are estimated at the initialization of the network (eq. 10 to eq. 12), and are then kept fixed
during the training as in Norp Prop. Yx, Yh and Yc are parameters learned via gradient descent.
Note that the reparametrization of equation 15 is identical to Weight Normalization (Weight Norm)
(Salimans & Kingma, 2016). The main difference comes from equation 17, where we compensate
for the variance of ct, the tanh and σ(oft), which ensures a normalized propagation. Overall, this
reparametrization is equivalent in spirit to the BN-LSTM, but it benefits from the same advantages
that Norm Prop has over BN: There is no dependence on the mini-batch size and the computation is
the same for training and inference. Also, the rescaling of the matrices Wx and Wh can be done
before the recurrence, leading to computation time closer to a vanilla LSTM.
3.3	Weights Initialization
With such reparametrization of the weight matrices, one can think that the scale of the initialization
of the weights doesn’t matter in the learning process anymore. It is actually true for the forward and
backward computation of the layer
_	aWi	_ Wi
'i= ∣∣aWi∣∣2 X = JWKX
∂yi aWi	Wi
----—--------------------
∂X	∣∣aWi∣∣2	∣∣Wi∣∣2
(18)
(19)
and since the variance of both forward and backward passes is fixed, using an initialization scheme
such as Glorot (Glorot & Bengio, 2010) doesn’t make sense with Norm Prop. However, the update
of the parameters is affected by their scale:
∂L
∂yi
(20)
∂yi ∂ L _	1 Γ	Wij -
∂Wij 酝=JW而[xj - yi JW而
The scale of the parameters affect the learning rate of the layer: the bigger the weights, the smaller
the update. This induces a regularization effect in Norm Prop that is also present in BN (Ioffe
& Szegedy, 2015). However, this could possibly be an issue for such parametrization: different
initializations lead to different learning rates, and it is true even with adaptive step rules, such as
Adam (Kingma & Ba, 2014). Moreover, the parameters that are not normalized (such as Y and b)
aren’t affected by this effect, and so they are not regularized. This is the reason why forcing the
weight matrices to have a unit L2 norm of the lines, as proposed in Arpit et al. (2016), helps the
training procedure.
To still benefit from the reduction of the learning rate, which is know to ease the optimization (Vogl
et al., 1988), we propose to simply force the unit L2 norm of the lines of the matrices and combine
it with a global learning rate decay schedule.
4 Gradient Propagation in Normalized LSTM
In this section we study the gradient flow in the Normalized LSTM. Since this reparametrization is
similar to the BN-LSTM, the analysis we do here can be transposed to the BN-LSTM case.
4
Under review as a conference paper at ICLR 2017
4.1	The Exploding and Vanishing Gradients Problem
Given an input sequence X = (x1, x2, . . . , xT), we consider a recurrent network, parametrized by
θ, that defines a sequence of hidden states ht = fθ(ht-1, xt) and cost function L which evaluates
the model performance on a given task. Such network is usually trained using backpropagation
through time, where the backpropagation is applied on the time-unrolled model. The chain rule can
be applied in order to compute the derivative of the loss L with respect to parameters θ.
∂L _	∂Lt _	∂Lt ∂hk ∂ht
而=ι≤t≤τ而=ι≤t≤τ至t酝行丽.
(21)
The factors ∂hk = Qk≤ι≤t a∂hhl 1 transports the error “in time" from step t back to step k and are
also the cause of vanishing or exploding gradient in RNN (Pascanu et al., 2012). Indeed, if the
Jacobian ∂∂hhl 1 has singular value different from 1, the factor ∂hk, which is a product of t 一 k
Jacobian matrices will either explode or vanish.
4.2	Gradient of the Normalized LSTM
To study the gradient propagation of the Normalized LSTM, we first need to derive it. Using equa-
tion 15-17, we can write the gradient of ht with respect to ht-1
at = —, 1 tanh
√Var[ht]
Ycct
pVar[ct]
∂at
=	Gl at + ot Gl	—(
∂ht-1	∂ht-1	∂ht-1
∂it	∂gt	∂ft
∂ht-1G gt + itG ∂ht-1 + ∂ht-1G ct-1
(22)
(23)
As we can see in equation 23 with the normalization, the gradient depends not only on the derivative
of the cell candidate, the gates and the output tanh, but also on on the variance of ht and ct.
If we assume that ht-1 and xt are independent, we can compute the variance of ct . Neglecting the
weight matrices and the effect of the gates, we can write from equations 8 and 14
Var[ct] ≈ Var[gt] = Var[tanh(z)], Z 〜N(0, Yx + Yh)	(24)
Var[ht] = Var[tanh(z)], Z 〜N(0, Y(Yx + Yh))	(25)
In both cases, the variance depends explicitly on the value of the different Y : The bigger the Y, the
higher the variance. Neglecting again the weight matrices, we can now write the equations of the
cell candidates gt and the gates it , ot and ft with respect to ht-1
∂gt	∂ tanh(gt) ∂ gt	2、
=	=(1 一 tanh(YxXt + Yhht-I) ) Yh
∂ ht-ι	∂gt	∂ht-ι
_ _ ~ ,	- ~
d it	dσ(It) dit
=~τ√=-- = σ (YxXt + Yhht-I)(I — σ (YxXt + Yhht-I))Yh
∂ ht-1	∂it ∂ ht-1
(26)
(27)
The gradients ofot and ft can be computed similarly. The effect of the Y here is double: They appear
both in the activation function, where they control the saturation regime, and Yh also appears as a
multiplicative term in the gradient. They should therefore be small enough to prevent the activation
from saturating too much, but at the same time Yh can’t be too small, because it can also make the
gradients vanish. Putting it all together, we have
∂ht	∂ot	∂at	Yc	∂it	∂gt	∂ft
=λ-=后 Yh G	at	+ ot	G	ɪ--t== G Yh	FG	gt	+ it	G 行 + / G ct-i	(28)
∂ht-i	dot	∂at ,Var[ct]	[∂it	∂gt	∂f	」
In this equations we can see that the different Y directly scale the gradient, and they also control
the saturation of the activation functions. Bad initialization of Y could thus lead to saturation or
explosion regimes. Figure 1 shows the norm of the gradient with respect to Yx and Yh in a simulated
LSTM. As we can see, one important parameter is the ratio between Yh and Yx : They control most
of the propagation of the gradients. IfYx > Yh, the network will focus more on the input and so the
gradients will tend to vanish more. On the other hand, if Yh > Yx, the network will tend have less
vanishing gradients, but will focus less on its inputs.
5
Under review as a conference paper at ICLR 2017
IIdhtZdht-Illgamm.a c=0.1,
2
gamma h
1.5
X n∈Enσ
∣∣dht∕dht-l 11 gamma c=1.0
2.1
1.9
5.4
4.8
4.2
3.6
≡
0.6
0.1 0.3 0.5 0.7 0.9 1.1 1.3 1.5 1.7 1.9 2.1
gamma h
Figure 1: Norm of the gradients for one time step in an LSTM with respect to γx and γh (simulation).
Left: γc = 0.1. Right: γc = 1.0.
5 Experiments
5.1	Character-Level Language Modelling
The first task we explore is character-level language modelling on the Penn Treebank corpus (Marcus
et al., 1993). The goal is to predict the next character of the sequence given the previous ones. We
use the same splits as Mikolov et al. (2012) and the same training procedure as Cooijmans et al.
(2016), i.e. we train on sequences of length 100, with random starting point. The model is a
1000 units LSTM followed by a Softmax classifier. We use orthogonal initialization for the weight
matrices. Because Norm Prop requires normalized inputs, we multiply the one-hot inputs vector
with an untrained but fixed orthogonal matrix. This tricks does not only help the optimization of
Norm Prop, but also all other variants.
To compare the convergence properties of Norm Prop against LN and BN, we first ran experiments
using Adam (Kingma & Ba, 2014) with learning rate 2e-3, exponential decay of 1e-3 and gradient
clipping at 1.0. As explained in section 3.3, we rescale the matrices such that they have a unit norm
on the lines. For Norm Prop, we use γx = γh = 2 and γc = 1, for LN all the γ = 1.0 and for BN
all the γ = 0.1. The results are presented in Table 1 and in Figure 2.
Model	Validation	Time
Baseline	1.455	386
Weight Norm	1.438	402
Batch Norm	1.433	545
Layer Norm	1.439	530
Norm Prop	1.422	413
Table 1: Perplexity (bits-per-character) on sequences of length 100 from the Penn Treebank valida-
tion set, and training time (seconds) per epoch.
To show the potential of Norm Prop against other state-of-the-art system, we followed Ha et al.
(2016) and apply dropout on both the input and output layer (p = 0.1) and recurrent dropout inside
the LSTM (p = 0.1). We also used the Batch Data Normalization scheme presented by Arpit et al.
(2016), so we standardize each input example using the mini-batch statistics and use population
statistics at inference time. Finally, we also reduce the learning rate decay to 1e-4, to compensate
for the fact that a network with dropout needs more time to train. The results are presented in Table 2.
As we can see in Figure 2 and in Table 1, Norm Prop compares really well against the other
reparametrization. Also Norm Prop is roughly 30 % computationally faster2 than BN and LN. LN
shows better optimization performances, but also overfits more. We also see that both optimization
and generalization are better than the ones from Weight Norm, which shows the importance of com-
pensating for the variance of ct and ht. Moreover, although Norm Prop doesn’t combine well with
2The GPU used is a NVIDIA GTX 750.
6
Under review as a conference paper at ICLR 2017
Figure 2:	Perplexity (bits-per-character) on sequences of length 100 from the Penn Treebank corpus.
The dashed lines are the training curves, and the solid ones are the validation curves.
Model	Test
Recurrent Dropout LSTM (Semeniuta et al., 2016) Zoneout LSTM (Krueger et al., 2016) Layer Norm LSTM (Ha et al., 2016) HyperLSTM (Ha et al., 2016) Norm Prop LSTM (ours) Layer Norm HyperLSTM (Ha et al., 2016)	1.301 1.27 1.267 1.265 1.262 1.250
Table 2: Perplexity (bits-per-character) of the full Penn Treebank test sequence.
dropout in feed-forward networks (Arpit et al., 2016), it works will with recurrent dropout, as we can
see in Table 2. We believe it is because recurrent dropout is less affecting its output distribution than
dropout in feed forward networks, because we copy the variable at the previous time step instead
of setting it to 0. With such regularization, Norm Prop compares well with other state-of-the-art
approaches.
5.2 DRAW
The second task we explore is a generative modelling task on binarized MNIST (Larochelle &
Murray, 2011) using the Deep Recurrent Attentive Writer (DRAW) (Gregor et al., 2015) architecture.
DRAW is a variational auto-encoder, where both encoder and decoder are LSTMs, and has two
attention mechanisms to select where to read and where to write.
We use Jorg Bomschein's implementation3, with the same hyper-parameters as Gregor et al. (2015),
ie the read and write size are 2x2 and 5x5 respectively, the number of glimpses is 64, the LSTMs
have 256 units and the dimension of z is 100. We use Adam with learning rate of 1e-2, exponential
decay of 1e-3 and mini-batch size of 128. We use orthogonal initialization and force the norm of
the lines of the matrices to be 1. For Norm Prop, we use γx = γh = γc = 0.5. The test variational
bound for the first 100 epochs is presented in Figure 3.
As we can see in Figure 3, both Weight Norm and Norm Prop outperform the baseline network by
a significant margin. Also, as expected, Norm Prop performs better than Weight Norm, showing
one again the importance of the compensation of the variance of ct and ht . Table 3 shows the test
variational bound after 200 epochs of training. Norm Prop also compares favorably against LN.
3https://github.com/jbornschein/draw
7
Under review as a conference paper at ICLR 2017
Figure 3:	Test negative log-likelihood on binarized MNIST.
Model	DRAW
Baseline (ours)	84.30
Layer Norm (Ba et al., 2016)	82.09
Weight Norm (ours)	81.98
Norm Prop (ours)	81.17
Table 3: Test variational log likelihood (nats) after 200 epochs of training.
6 Conclusion
Based on the BN-LSTM, we have shown how to build a Normalized LSTM that is able to preserve
the variance of its output at each time step, by compensating for the variance of the cell and the
hidden state. Such LSTM can be seen as the Norm Prop version of the BN-LSTM, and thus benefits
from the same advantages that Norm Prop has over BN, while being way faster to compute. Also,
we propose a scheme to initialize the weight matrices that takes into account the reparametrization.
Moreover, we have derived the gradients of this LSTM and pointed out the importance of the initial-
ization of the rescaling parameters. We have validated the performances of the Normalized LSTM
on two different tasks, showing similar performances than BN-LSTM and LN-LSTM, while being
significantly faster in computation time. Also, unlike the feed-forward case, this architecture works
well with recurrent dropout, leading to close to state-of-the-art performances on the character-level
language modelling task.
Future work includes trying this architecture on more challenging tasks and also studying the impact
of not keeping the variance estimates of the cell and the hidden states fixed during the learning
process.
Acknowledgments
Part of this work was funded by Samsung. We used Theano (Theano Development Team, 2016),
Blocks and Fuel (van Merrienboer et al., 2015) for our experiments. We also want to thanks Caglar
GUlcehre and Tim Cooijmans for the talks and Jorg Bornschein for his DRAW implementation.
References
D. Arpit, Y. Zhou, B. U Kota, and V. Govindaraju. Normalization propagation: A parametric tech-
nique for removing internal covariate shift in deep networks. arXiv preprint, 2016.
J.	L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint, 2016.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and
translate. ICLR, 2015.
8
Under review as a conference paper at ICLR 2017
Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is
difficult. Neural Networks, IEEE Transactions on, 1994.
K.	Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio.
Learning phrase representations using rnn encoder-decoder for statistical machine translation.
arXiv preprint, 2014.
T. Cooijmans, N. Ballas, C. Laurent, and A. Courville. Recurrent batch normalization. arXiv
preprint, 2016.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Aistats, volume 9, pp. 249-256, 2010.
K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra. Draw: A recurrent neural
network for image generation. arXiv preprint, 2015.
D. Ha, A. Dai, and Q. V Le. Hypernetworks. arXiv preprint, 2016.
K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom.
Teaching machines to read and comprehend. In NIPS, 2015.
S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Master’s thesis, 1991.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8), 1997.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. CoRR, abs/1502.03167, 2015.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint, 2014.
D. Krueger, T. Maharaj, J. Kramar, M. Pezeshki, N. Ballas, N. R. Ke, A. Goyal, Y. Bengio,
H. Larochelle, A. Courville, et al. Zoneout: Regularizing rnns by randomly preserving hidden
activations. arXiv preprint, 2016.
H. Larochelle and I. Murray. The neural autoregressive distribution estimator. AISTATS, 2011.
M. P. Marcus, M. Marcinkiewicz, and B. Santorini. Building a large annotated corpus of english:
The penn treebank. Comput. Linguist., 1993.
T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky. Subword language
modeling with neural networks. preprint, 2012.
R.	Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. arXiv
preprint, 2012.
T. Salimans and D. P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. arXiv preprint, 2016.
S.	Semeniuta, A. Severyn, and E. Barth. Recurrent dropout without memory loss. CoRR,
abs/1603.05118, 2016.
H. Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of statistical planning and inference, 2000.
Theano Development Team. Theano: A Python framework for fast computation of mathematical
expressions. arXiv preprint, 2016.
B. van Merrienboer, D. Bahdanau, V. Dumoulin, D. Serdyuk, D. Warde-Farley, J. Chorowski, and
Y. Bengio. Blocks and fuel: Frameworks for deep learning. CoRR, abs/1506.00619, 2015.
T.	P. Vogl, J. K. Mangis, A. K. Rigler, W. T. Zink, and D. L. Alkon. Accelerating the convergence
of the back-propagation method. Biological Cybernetics, 59(4):257-263, 1988.
K.	Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and
tell: Neural image caption generation with visual attention. arXiv preprint, 2015.
L.	Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville. Describing videos by
exploiting temporal structure. In ICCV, 2015.
9