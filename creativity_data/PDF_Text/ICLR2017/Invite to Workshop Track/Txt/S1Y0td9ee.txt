Under review as a conference paper at ICLR 2017
Shift Aggregate Extract Networks
Francesco Orsini12, Daniele Baracchi2 and Paolo Frasconi2
Department of Computer Science
Katholieke Universiteit Leuven
Celestijnenlaan 200A
3001 Heverlee, Belgium
francesco.orsini@kuleuven.be
Department of Information Engineering
Universita degli StUdi di Firenze
Via di Santa Marta 3
I-50139 Firenze, Italy
daniele.baracchi@unifi.it
paolo.frasconi@unifi.it
Ab stract
The Shift Aggregate Extract Network (saen) is an architectUre for learning repre-
sentations on social network data. saen decomposes inpUt graphs into hierarchies
made of mUltiple strata of objects. Vector representations of each object are learnt
by applying shift, aggregate and extract operations on the vector representations
of its parts. We propose an algorithm for domain compression which takes ad-
vantage of symmetries in hierarchical decompositions to redUce the memory Us-
age and obtain significant speedUps. OUr method is empirically evalUated on real
world social network datasets, oUtperforming the cUrrent state of the art.
1 Introduction
Many different problems in varioUs fields of science reqUire the classification of structured data,
i.e. collections of objects bond together by some kind of relation. A natUral way to represent sUch
strUctUres is throUgh graphs, which are able to encode both the individUal objects composing the
collection (as vertices) and the relationships between them (as edges). A nUmber of approaches to
the graph classification problem has been stUdied in graph kernel and neUral network literatUre.
Graph kernels decompose inpUt graphs in sUbstrUctUres sUch as shortest paths (Borgwardt & Kriegel,
2005), graphlets (Shervashidze et al., 2009) or neighborhood sUbgraph pairs (Costa & De Grave,
2010). The similarity between two graphs is then compUted by comparing the respective sets of
parts. Methods based on recUrsive neUral networks Unfold a neUral network over inpUt graphs and
learn vector representations of their nodes employing backpropagation thoUgh strUctUre (Goller &
KUchler, 1996). RecUrsive neUral networks have been sUccessfUlly applied to domains sUch as nat-
Ural langUage (Socher et al., 2011) and biology (VUllo & Frasconi, 2004; Baldi & Pollastri, 2003).
An advantage of recUrsive neUral networks over graph kernels, is that the vector representations of
the inpUt graphs are learnt rather than handcrafted.
Learning on social network data can be considerably hard dUe to their pecUliar strUctUre: as opposed
to chemical compoUnds and parse trees, the strUctUre of social network graphs is highly irregUlar.
Indeed in social networks it is common to have nodes in the same graph whose degree differs by
orders of magnitUde. This poses a significant challenge for the sUbstrUctUre matching approach Used
by some graph kernels as the variability in connectivity generates a large nUmber of UniqUe patterns
leading to diagonally dominant kernel matrices.
We propose Shift Aggregate Extract Networks (saen), a neUral network architectUre for learning
representations of inpUt graphs. saen decomposes inpUt graphs into H-hierarchies made of mUltiple
strata of objects. Objects in each stratUm are connected by “part-of” relations to the objects to the
stratUm above.
In case we wish to classify graphs we can Use an H-hierarchical decomposition in which the top
stratUm contains the graph G that we want to classify, while the intermediate strata contain sUbgraphs
of G, sUbgraphs of sUbgraphs of G and so on, Until we reach the bottom stratUm which contains the
vertices v of G.
1
Under review as a conference paper at ICLR 2017
Unlike R-convolution relations in kernel methods (which decompose objects into the set of their
parts), H-hierarchical decompositions are deep as they can represent the parts of the parts of an
object.
Recursive neural networks associate to the vertices of the input graphs vector representations impos-
ing that they have identical dimensions. Moreover, the propagation follows the edge connectivity
and weights are shared over the whole input graph. If we consider that vector representations of
nodes (whose number of parents can differ by orders of magnitude) must share the same weights,
learning on social network data with recursive neural networks might be nontrivial.
saen compensates the limitations of recursive neural networks by adding the following degrees of
flexibility:
1.	the saen computation schema unfolds a neural network over H-decompositions instead of the
input graph,
2.	saen imposes weight sharing and fixed size of the learnt vector representations on aper stratum
basis instead of globally.
Indeed saen allows to use vector representations of different sizes for different strata of objects
(e.g. graphs, subgraphs, subgraphs of subgraphs, edges, vertices etc.) The saen schema computes
the vector representation of each object by applying shift, aggregate and extract operations on the
vector representations of its parts.
Another contribution of this paper is the introduction of a domain compression algorithm, that we
use in our experiments to reduce memory usage and runtime. Domain compression collapses objects
in the same stratum of an H-hierarchical decomposition into a compressed one whenever these
objects are indistinguishable for the saen computation schema. In particular objects made of the
same sets of parts are indistinguishable. In order obtain a lossless compression an H-hierarchical
decomposition we store counts on symmetries adopting some mathematical results from lifted linear
programming (Mladenov et al., 2012). The domain compression algorithm is also reminiscent of the
work of Sperduti & Starita (1997) in which common substructures of recursive neural networks are
collapsed in order to reduce the computational cost.
2	Shift-Aggregate-Extract neural networks
We propose a neural network architecture that takes as input an undirected attributed graph G =
(V, E, X) where V is the vertex set, E ⊆ V × V is the edge set, and X = {xv ∈ Rp}v∈V is a
set of p-dimensional vertex attributes. When vertices do not have associated attributes (for example
this happens in some of the social network datasets of § 4.1), we can set xv to some vertex invariant
such as node centrality or betweenness.
2.1	H-HIERARCHICAL decompositions
Most graph kernels decompose graphs into parts by using an R-convolution relation (Haussler,
1999). We extend this approach by decomposing graphs into a hierarchy of π-parametrized “part
of” relations. Formally, an H-hierarchical decomposition is a pair ({Sl}lL=0, {Rl,π}lL=1) where:
•	{Sl}lL=0 are disjoint sets of objects Sl called strata, or levels of the hierarchy. The bottom stratum
S0 contains non-decomposable objects (e.g. individual vertices), while the other strata Sl , l =
1, . . . , L contain composite objects, oi ∈ Sl, whose parts oj ∈ Sl-1 belong to the preceding stratum,
Sl-1.
•	{Rl,π }lL=1 is a set of l, π-parametrized Rl,π-convolution relations. A pair (oi, oj) ∈ Sl × Sl-1
belongs to Rι,∏ iff “oj is part of o% with membership type n”. For notational convenience, the parts
of oi are denoted as Rl-,π1(oi) = {oj |(oj, oi) ∈ Rl,π}.
The membership type π is used to represent the roles of the parts of an object. For example, we
could decompose a graph as a multiset of π-neighborhood subgraphs 1 in which π is the radius of
the neighborhoods (see Figure 1 on the left). Another possible use of the π membership type is to
1The r-neighborhood subgraph (or ego graph) of a vertex v in a graph G is the induced subgraph of G
consisting of all vertices whose shortest-path distance from v is at most r.
2
Under review as a conference paper at ICLR 2017
(stratum S2) decomposed into ego graphs of	Ego graph (stratum S1)
0 and 1 (stratum S1 ).	vertices (stratum S2).
0.	Ego graphs of radius 1
Figure 1: Image of an H-hierarchical decomposition (in particular the egnn explained in § 4.2).
On the left we decompose a graph into rooted ego graphs of radius 0 and 1, while on the right we
decompose an ego graph into the set of its vertices. The directed arrows represent “part of” relations
labeled with their membership type π. The membership type π represents the radius π = 0, 1 of the
ego graphs (decomposition on the left) and the role (i.e. π = root, elem) of a vertex in the ego
graph (decomposition on the right) respectively.
distinguish the root from the other vertices in a rooted neighborhood subgraph (see Figure 1 on the
right).
An H-hierarchical decomposition is a multilevel generalization of R-convolution relations, and it
reduces to an R-convolution relation for L = 1.
2.2	Shift Aggregate Extract schema for learning representations
We propose Shift Aggregate Extract Network (saen) to learn vector representations for all the
objects of all the strata {Sl }lL=0 in an H-hierarchical decomposition. saen unfolds a neural net-
work architecture over an H-hierarchical decomposition by using the Shift Aggregate Extract (sae)
schema.
According to the sae schema the vector representation of each object in the H-hierarchical decom-
position is either computed by applying a neural network on the vertex attributes (for the objects in
bottom stratum) or defined in terms of the vector representations of its parts (for the other objects).
More formally, the sae schema associates a dl-dimensional representation hi ∈ Rdl to each object
oi ∈ Sl of the H-hierarchical decomposition according to the following formula:
fι( E
π∈Πl
、
f0(xvi; Θ0)
E	(z∏ ③ hj)[Θ()
oj ∈R-Π(Oi)	Shift '
_ -	-
^∖Z
Aggregate
________ -	J
^^^^^^^^^^^^^^^^^^^^^^^^^^^
Extract
ifoi ∈ S0
otherwise
(1)
、
/
where fι (∙; Θι), l = 0,...,L are multilayer neural networks with parameters Θι.
With respect to the base case (first branch of Eq. 1) we have that each object oi in the bottom stratum
S0 is in one-to-one correspondence with the vertices vi ∈ V of the graph that we are decomposing.
Indeed the vector representations hi are computed by evaluating fo(∙; Θo) in correspondence of the
vertex attributes xvi ∈ X .
The recursion step (second branch of Eq. 1) follows the Shift Aggregate Extract (sae) schema:
•	Shift: each part representation hj ∈ R。] is remapped into a space R|nidl-1| made of ∣∏ι | slots,
where each slot has dimension dl-1. This transformation shifts part representations hj by using
the Kronecker product 0 between an indicator vector z∏ ∈ R|nl | and the vector representation hj
of part Oj ∈ S1-1. The indicator vector Zn ∈ R|nl| defined as Zi
01 othiferiw=πise. and it is used to
3
Under review as a conference paper at ICLR 2017
H	H
Figure 2: Pictorial representation of the H-hierarchical decomposition of a graph taken from the
IMDB-BINARY dataset (see § 4.1) together with its compressed version.
make sure that vector representations hj of object parts will fall in the same slot if and only if they
have the same membership type π.
•	Aggregate: the shifted representations (z∏ 0 hj) of the parts Oj are then aggregated with a sum.
•	Extract: the aggregated representation is compressed to a dl-dimensional space by a Θl-
parametrized nonlinear map fι(∙, Θι) : Rlπldl-1l → Rdl implemented with a multilayer neural
network.
The shift and aggregate steps, that we have seen so far, are identical to those used in kernel design
when computing the explicit feature of a kernel k(χ,z) derived from a sum E∏∈∏ k∏ (x, Z) of base
kernels kπ(x, z), π ∈ Π. In principle, it would be indeed possible to turn SAEN into a kernel method
by removing the extraction step e from the sae schema. However, such an approach would increase
the dimensionality of the feature space by a multiplicative factor ∣∏ι∣ for each level l of the H-
hierarchical decomposition, thus leading to an exponential number of features. When using saen,
the feature space growth is prevented by exploiting a distributed representation (via a multilayered
neural network) during the e step of the sae schema. As a result, saen can easily cope with H-
hierarchical decompositions consisting of multiple strata.
2.3	Exploiting symmetries for domain compression
In this section we propose a technique, called domain compression, which allows to save memory
and speedup the saen computation. Domain compression exploits symmetries in H-hierarchical de-
compositions by collapsing equivalent objects in each stratum. The greater the number of collapsed
objects the highest the compression ratio.
Two objects a, b in a stratum Sl are collapsable a 〜b if they share the same representation (i.e.
ha = hb) for all the possible values of Θl. A compressed stratum Scomp is the quotient set sl/〜of
stratum Sl w.r.t. the collapsibility relation 〜. We assume that the attributes of the elements in the
bottom stratum S0 are categorical, so that the same vector representation can be shared by multiple
elements with non-zero probability. 2 * While objects in the bottom stratum S0 are collapsable when
their attributes are identical, for all the other strata Sl, l = 1, . . . , L, objects are collapsable if they
are made by the same sets of parts for all the membership types π.
In Figure 2 we provide a pictorial representation of the domain compression of an H-hierarchical
decomposition (egnn, described in § 4.2). On the left we show the H-hierarchical decomposition
ofa graph taken from the imdb-binary dataset (see § 4.1) together with its compressed version on
the right.
2.3.1	Domain compression algorithm
In order to compress H-hierarchical decompositions we adapt the lifted linear programming tech-
nique proposed by Mladenov et al. (2012) to the SAEN architecture. If a matrix M ∈ Rn×p has
2 Vectors of real valued attributes could be discretized using clustering techniques. However, we leave
discretization in saen to future works.
4
Under review as a conference paper at ICLR 2017
m ≤ n distinct rows it can be decomposed as the product DM comp where M comp is a compressed
version of M in which the distinct rows of M appear exactly once. The Boolean decompression
matrix, D, encodes the collapsibility relation among the rows of M so that Dij = 1 iff the ith row
of M falls in the equivalence class j of 〜.A pseudo-inverse C of D can be computed by dividing
the rows of DT by their sum (where DT is the transpose of D).
Example 1 If we look at matrix M in Eq. 2 we notice that row 1 and 4 share the encoding [0, 0, 0],
rows 3 and 5 share the encoding [1, 1, 0] while the encoding [1, 0, 1] appears only once at row 2.
Matrix Mcomp is the compressed version of M.
M
0
1
1
0
1
00
0 1
1 0
00
10
M comp
1
0
0
1
0
00
1 0
0 1
00
01
1/2	0	0	1/2	0
C =	0	1	0	0	0	(2)
0	0	1/2	0	1/2
0
1
1
0
0
1
0
1
0
D
Matrix M can be expressed as the matrix product between the decompression matrix D and the
compressed version of M comp (i.e. M = DM comp), while the matrix multiplication between the
compression matrix C and the M leads to the compressed matrix M comp (i.e.M comp = CM).
To apply domain compression we rewrite Eq. 1 in matrix form as follows:
f0(x；®0)
X----V----'
∣So∣×do
/	∖
ifl = 0
Hl =	fl
[Rl,1, ∙∙∙, Rl,∏ ,∙∙∙, Rl,∣∏ι |]
x---------------V---------------Z
∣Sι∣×∣∏ι∣∣Sι-ι∣
∖
Hl-1 ...	0
. J.	. ；Θι
.	..
0	...	Hl-1
：_______ -	/
I∏1I∣S1-1 l×l∏ι∣dι-ι
otherwise
(3)
/
∣Sι∣×dι
where:
•	H ∈ R1Sll×dl is the matrix that represents the dι-dimensional encodings of the objects in Si.
The rows of Hl are the vector representations hi in Eq. 1, while the rows of Hl-1 are the vector
representations hj in Eq. 1;
•	X ∈ RlSol×p is the matrix that represents the p-dimensional encodings of the vertex attributes in
V (i.e. the rows ofX are the xvi of Eq. 1);
•	fι(∙; Θι) is unchanged w.r.t. Eq. 1 and is applied to its input matrices row-wise;
•	Rι,∏ ∈ RlSll×lSl-1l ∀∏ ∈ ∏ι are the matrix representations of the Rι,∏-convolution relations of
Eq. 1 whose elements are (Rι,π)ij = 1 if (oj, oi) ∈ Rι,π and 0 otherwise.
Domain compression on Eq. 3 is performed by the domain-compression procedure (see Algo-
rithm 3) that takes as input the attribute matrix X and the part-of matrices Rι,π and returns their
compressed versions X comp and the Rιc,oπmp respectively. The algorithm starts by invoking (line 1)
the procedure compute-cd on X to obtain the compression and decompression matrices C0 and
D0 respectively. The compression matrix C0 is used to compress X (line 2) then we start iterating
over the levels l = 0,. . . , L of the H-hierarchical decomposition (line 4) and compress the Rι,π
matrices. The compression of the Rι,π matrices is done by right-multiplying them by the decom-
pression matrix Dι-1 of the previous level l - 1 (line 5). In this way we collapse the parts of relation
Rι,π (i.e. the columns of Rι,π) as these were identified in stratum Sι-1 as identical objects (i.e.
those objects corresponding to the rows of X or Rι-1,π collapsed during the previous step). The
result is a list RCOl-COmp = [Rι,∏Dι-ι, ∀∏ = 1,..., ∣∏ι∣] of column compressed Rι,∏-matrices.
We proceed collapsing equivalent objects in stratum Sι , i.e. those made of identical sets of parts:
we find symmetries in Rcol-comp by invoking COMPUTE-CD (line 6) and obtain a new pair Cι, Dι
of compression, and decompression matrices respectively. Finally the compression matrix Cι is ap-
plied to the column-compressed matrices in R^ol-^omρ in order to obtain the ∏ι compressed matrices
5
Under review as a conference paper at ICLR 2017
domain-compression(X, R)
1	C0, D0 = compute-cd(X)
2	X comp = C0X // Compress the X matrix.
3	Rcomp = {} // Initialize an empty container for compressed matrices.
4	for l = 1 to L
5	Rcol-Comp = [Rι,∏Dι-ι, ∀∏ = 1,..., ∣∏ι∣] // column compression
6	Cι, Di = COMPUTE-CD(RCol-Comp)
7	for ∏ = 1 to ∣∏ι |
8	RCrp = CiRn)I-com // row compression
9	return XCo,mp , RComp
Figure 3: domain-compression
of stratum Si (line 8). Algorithm 3 allows us to compute the domain compressed version of Eq. 3
which can be obtained by replacing: X with XComp = C0X, Ri,π with RiC,oπmp = CiRi,πDi-1 and
Hi with HiComp. Willing to recover the original encodings Hi we just need to employ the decom-
pression matrix Di on the compressed encodings HiComp, indeed Hi = DiHiComp.
As We can see by substituting Si with Scomp, the more are the symmetries (i.e. when ∣SComp∣ C
|Si |) the greater the domain compression will be.
3	Related works
When learning with graph inputs two fundamental design aspects that must be taken into account are:
the choice of the pattern generator and the choice of the matching operator. The former decomposes
the graph input in substructures while the latter allows to compare the substructures.
Among the patterns considered from the graph kernel literature we have paths, shortest paths,
walks (Kashima et al., 2003), subtrees (Ramon & Gartner, 2003; Shervashidze et al., 2011) and
neighborhood subgraphs (Costa & De Grave, 2010). The similarity between graphs G and G is
computed by counting the number of matches between their common the substructures (i.e. a kernel
on the sets of the substructures). The match between two substructures can be defined by using
graph isomorphism or some other weaker graph invariant.
When the number of substructures to enumerate is infinite or exponential with the size of the graph
(perhaps this is the case for random walks and shortest paths respectively) the kernel between the
two graphs is computed without generating an explicit feature map. Learning with an implicit fea-
ture map is not scalable as it has a space complexity quadratic in the number of training examples
(because we need to store in memory the gram matrix).
Other graph kernels such as the Weisfeiler-Lehman Subtree Kernel (wlst) (Shervashidze et al.,
2011) and the Neighborhood Subgraph Pairwise Distance Kernel (nspdk) (Costa & De Grave,
2010) deliberately choose a pattern generator that scales polynomially and produces an explicit
feature map. However the vector representations produced by wlst and nspdk are handcrafted
and not learned.
A recent work by Yanardag & Vishwanathan (2015) proposes to uses pattern generators such as
graphlets, shortest paths and wlst subtrees to transform input graphs into documents. The gener-
ated substructures are then treated as words and embedded in the Euclidean space with a CBOW
or a Skip-gram model. The deep upgrade of existing graph kernels is performed by reweighing the
counts of the substructures by the square root of their word-vector self similarity.
Another recent work by Niepert et al. (2016) upgrades the convolutional neural networks cnns for
images to graphs. While the receptive field of a cnn is usually a square window (Niepert et al.,
2016) employ neighborhood subgraphs as receptive fields. As nodes in graphs do not have a specific
temporal or spatial order, (Niepert et al., 2016) employ vertex invariants to impose an order on the
nodes of the subgraphs/receptive fields.
6
Under review as a conference paper at ICLR 2017
4	Experimental evaluation
We answer to the following experimental questions:
Q1 How does SAEN compare to the state of the art?
Q2 Can SAEN exploit symmetries in social networks to reduce the memory usage and the runtime?
4.1	Datasets
In order to answer the experimental questions we tested our method on six publicly available datasets
first proposed by Yanardag & Vishwanathan (2015).
•	COLLAB is a dataset where each graph represent the ego-network of a researcher, and the task is
to determine the field of study of the researcher between High Energy Physics, Condensed Matter
Physics and Astro Physics.
•	imdb-binary, imdb -multi are datasets derived from IMDB where in each graph the ver-
tices represent actors/actresses and the edges connect people which have performed in the same
movie. Collaboration graphs are generated from movies belonging to genres Action and Romance
for imdb-binaryand Comedy, Romance and Sci-Fi for imdb-multi, and for each actor/actress in
those genres an ego-graph is extracted. The task is to identify the genre from which the ego-graph
has been generated.
•	reddit-binary, reddit-multi5k, reddit-multi12k are datasets where each graph is de-
rived from a discussion thread from Reddit. In those datasets each vertex represent a distinct user
and two users are connected by an edge if one of them has responded to a post of the other in
that discussion. The task in reddit-binaryis to discriminate between threads originating from
a discussion-based subreddit (TrollXChromosomes, atheism) or from a question/answers-based
subreddit (IAmA, AskReddit). The task in reddit-multi5kand reddit-multi 1 2kis a multi-
class classification problem where each graph is labeled with the subreddit where it has originated
(worldnews, videos, AdviceAnimals, aww, mildlyinteresting for reddit-multi5 kand AskReddit,
AdviceAnimals, atheism, aww, IAmA, mildlyinteresting, Showerthoughts, videos, todayilearned,
worldnews, TrollXChromosomes for REDDIT-MULTI 1 2K).
4.2	Experiments
In our experiments we chose an H-hierarchical decomposition called Ego Graph Neural Network
(egnn), that mimics the graph kernel nspdk with the distance parameter set to 0.
Before applying egnn we turn unattributed graphs (V, E) into attributed graphs (V, E, X) by an-
notating their vertices v ∈ V with attributes xv ∈ X . We label vertices v of G with their degree and
encode this information into the attributes xv by employing the 1-hot encoding.
egnn decomposes attributed graphs G = (V, E, X) into a 3 level H-hierarchical decomposition
with the following strata (see Figure 1 for a pictorial representation of egnn):
•	stratum S0 contains objects ov that are in one-to-one correspondence with the vertices v ∈ V .
•	stratum S1 contains vroot-rooted r-neighborhood subgraphs (i.e. ego graphs) e = (vroot, Ve, Ee)
of radius r = 0, 1, . . . , R and has part-of alphabet Π1 = {ROOT, ELEM}. Objects ov ∈ S0 are
“ELEM-part-of” ego graph e if v ∈ Ve \ {vroot}, while the are “ROOT-part-of” ego graph e if
v = vroot.
•	stratum S2 contains the graph G that we want to classify and has part-of alphabet Π2 = {0, 1}
which correspond to the radius of the ego graphs e ∈ S1 of which G is made of.
E1 We experimented with SAEN applying the EGNN H-decomposition on all the datasets. For each
dataset, we manually chose the parameters of saen, i.e. the number of hidden layers for each
stratum, the size of each layer and the maximum radius R. We used the Leaky ReLU (Maas et al.)
activation function on all the units. We report the chosen parameters in Table A1 of the appendix.
In all our experiments we trained the neural networks by using the Adam algorithm to minimize a
cross entropy loss.
The classification accuracy of saen was measured with 10-times 10-fold cross-validation. We man-
ually chose the number of layers and units for each level of the part-of decomposition; the number
of epochs was chosen manually for each dataset and we kept the same value for all the 100 runs of
the 10-times 10-fold cross-validation.
7
Under review as a conference paper at ICLR 2017
Figure 4: Comparison of accuracy results.
DATASET	DGK (Yanardag et al. 2015)	PSCN (Niepert et al., 2016)	SAEN (our method)
COLLAB	73.09 ± 0.25	72.60 ± 2.16	75.63 ± 0.31
imdb-binary	66.96 ± 0.56	71.00 ± 2.29	71.26 ± 0.74
imdb-multi	44.55 ± 0.52	45.23 ± 2.84	49.11 ± 0.64
reddit-binary	78.04 ± 0.39	86.30 ± 1.58	86.08 ± 0.53
reddit-multi5k	41.27 ± 0.18	49.10 ± 0.70	52.24 ± 0.38
reddit-multi 1 2k	32.22 ± 0.10	41.32 ± 0.42	46.72 ± 0.23
Figure 5: Comparison of accuracy on bio-informatics datasets.
DATASET	PSCN (k = 10e) (Niepert et al., 2016)	SAEN (our method)
MUTAG	92.63 ± 4.21	84.99 ± 1.82
PTC	60.00 ± 4.82	57.04 ± 1.30
nci1	78.59 ± 1.89	77.80 ± 0.42
PROTEINS	75.89 ± 2.76	75.31 ± 0.70
d&d	77.12 ± 2.41	77.69 ± 0.96
The mean accuracies and their standard deviations obtained by our method are reported in Ta-
ble 4, where we compare these results with those obtained by Yanardag & Vishwanathan (2015)
and by Niepert et al. (2016).
Although our method was conceived for social network data, it can also handle other types of graphs.
For the sake of completeness in Table 5 we report the mean accuracies obtained with saen on the
molecule and protein datasets studied in previous works (e.g. Niepert et al. (2016)).
Table 1: Comparison of sizes and runtimes of the datasets before and after the compression.
DATASET	SIZE (MB)			ORIGINAL	RUNTIME COMP.	SPEEDUP
	ORIGINAL	COMP.	RATIO			
COLLAB	1Γ90^	-448^	θɪ	43’ 18”	8’ 20”	5ɪ
imdb -binary	68	34	0.50	3’ 9”	0’ 30”	6.3
imdb -multi	74	40	0.54	7’ 41”	1’ 54”	4.0
reddit-binary	326	56	0.17	TO	2’ 35”	≥ 100.0
reddit-multi5k	952	162	0.17	OOM	9’ 51”	—
reddit-multi 1 2k	1788	347	0.19	OOM	29’ 55”	—
E2 In Table 1 we show the file sizes of the preprocessed datasets before and after the compression
together with the data compression ratio. 3 We also estimate the benefit of the relational compression
from a computational time point of view and report the measurement of the runtime for 1 run with
and without compression together with the speedup factor.
For the purpose of this experiment, all tests were run on a computer with two 8-cores Intel Xeon
E5-2665 processors and 94 GB ram. Uncompressed datasets which exhausted our server’s memory
during the test are marked as “oom” (out of memory) in the table, while those who exceeded the
time limit of 100 times the time needed for the uncompressed version are marked as “to” (timeout).
4.3	Discussion
A1 As shown in Table 4, EGNN performs consistently better than the other two methods on all the
social network datasets. This confirms that the chosen H-hierarchical decomposition is effective on
this kind of problems. Also the results for molecule and protein datasets (see Table 5) are in line
with the current state of the art.
A2 The compression algorithm has proven to be effective in improving the computational cost of our
method. Most of the datasets improved their runtimes by a factor of at least 4 while maintaining the
3The size of the uncompressed files are shown for the sole purpose of computing the data compression ratio.
Indeed the last version of our code compresses the files on the fly.
8
Under review as a conference paper at ICLR 2017
same expressive power. Moreover, experiments on reddit-multi5k and reddit-multi 1 2k have
only been possible thanks to the size reduction operated by the algorithm as the script exhausted the
memory while executing the training step on the uncompressed files.
5	Conclusions
We proposed saen, a novel architecture for learning vector representations of H-decompositions
of input graphs. We applied saen for graph classification on 6 real world social network datasets,
outperforming the current state of the art on 4 of them and obtaining state-of-the-art classification
accuracy on the others. Another important contribution of this paper is the domain compression
algorithm which greatly reduces memory usage and allowed us to speedup the training time of a
factor of at least 4.
References
P Baldi and G Pollastri. The principled design of large-scale recursive neural network architectures-
dag-rnns and the protein structure prediction problem. JMach Learn Res, 4(Sep):575-602, 2003.
K M Borgwardt and H-P Kriegel. Shortest-path kernels on graphs. In Proc. of the ICDM-05, pp.
8-pp. IEEE, 2005.
F Costa and K De Grave. Fast neighborhood subgraph pairwise distance kernel. In Proc. of the
ICML-10, pp. 255-262. Omnipress, 2010.
C Goller and A Kuchler. Learning task-dependent distributed representations by backpropagation
through structure. In Neural Networks, 1996., IEEE International Conference on, volume 1, pp.
347-352. IEEE, 1996.
D Haussler. Convolution kernels on discrete structures. Technical report, Citeseer, 1999.
H Kashima, K Tsuda, and A Inokuchi. Marginalized kernels between labeled graphs. In ICML-03,
volume 3, pp. 321-328, 2003.
A L Maas, A Y Hannun, and A Y Ng. Rectifier nonlinearities improve neural network acoustic
models. In Proc. of the ICML-13.
M Mladenov, B Ahmadi, and K Kersting. Lifted linear programming. In AISTATS-12, pp. 788-797,
2012.
M Niepert, M Ahmed, and K Kutzkov. Learning convolutional neural networks for graphs. arXiv
preprint arXiv:1605.05273, 2016.
J Ramon and T Gartner. Expressivity versus efficiency of graph kernels. In First International
Workshop on Mining Graphs, Trees and Sequences, pp. 65-74. Citeseer, 2003.
N Shervashidze, SVN Vishwanathan, T Petri, K Mehlhorn, and K M Borgwardt. Efficient graphlet
kernels for large graph comparison. In AISTATS-09, volume 5, pp. 488-495, 2009.
N Shervashidze, P Schweitzer, E J van Leeuwen, K Mehlhorn, and K M Borgwardt. Weisfeiler-
lehman graph kernels. J Mach Learn Res, 12(Sep):2539-2561, 2011.
R Socher, C C Lin, C Manning, and A Y Ng. Parsing natural scenes and natural language with
recursive neural networks. In Proc. of the ICML-11, pp. 129-136, 2011.
A Sperduti and A Starita. Supervised neural networks for the classification of structures. IEEE
Transactions on Neural Networks, 8(3):714-735, 1997.
A Vullo and P Frasconi. Disulfide connectivity prediction using recursive neural networks and
evolutionary information. Bioinformatics, 20(5):653-659, 2004.
P Yanardag and SVN Vishwanathan. Deep graph kernels. In Proc. of KDD-15, pp. 1365-1374,
2015.
9
Under review as a conference paper at ICLR 2017
Appendix: Shift Aggregate Extract Networks
Francesco Orsini12, Daniele Baracchi2 and Paolo Frasconi2
Department of Computer Science
Katholieke Universiteit Leuven
Celestijnenlaan 200A
3001 Heverlee, Belgium
francesco.orsini@kuleuven.be
Department of Information Engineering
Universita degli StUdi di Firenze
Via di Santa Marta 3
I-50139 Firenze, Italy
daniele.baracchi@unifi.it
paolo.frasconi@unifi.it
A Parameters used in the experiments with egnn
In Table A1 we report for each dataset: the radiUses r of the neighborhood sUbgraphs Used in the
egnn decomposition and the nUmber of Units in the hidden layers for each stratUm.
FigUre A1: Parameters for the neUral networks Used in the experiments.
DATASET	RADIUSES r	So	HIDDEN UNITS S1	S2
COLLAB	^1	15 - 5	5-2	5-3
imdb -b inary	0,1, 2	2	5-2	5-3-1
imdb-multi	0,1, 2	2	5-2	5-3
reddit-binary	0,1	10-5	5-2	5-3-1
reddit-multi5k	0,1	10	10	6-5
reddit-multi 1 2k	0,1	10	10	20-11
MUTAG	0,1,2, 3-	10	5-5	5-5-1
PTC	0,1	15	15	15-1
nci1	0,1,2, 3	15	15	15-10-1
PROTEINS	0,1,2, 3	3-2	6-5-4 6-3- 1
d&d	0,1,2, 3	10	5-2	5-3-1
1