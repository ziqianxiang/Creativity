Under review as a conference paper at ICLR 2017
A Differentiable Physics Engine
for Deep Learning in Robotics
Jonas Degrave, Michiel Hermans； Joni Dambre & Francis Wyffels
Department of Electronics and Information Systems (ELIS)
Ghent University - iMinds, IDLab
Technologiepark-Zwijnaarde 15, B-9052 Ghent, Belgium
{Jonas.Degrave,Joni.Dambre,Francis.wyffels}@UGent.be
Ab stract
One of the most important fields in robotics is the optimization of controllers. Cur-
rently, robots are often treated as a black box in this optimization process, which
is the reason why derivative-free optimization methods such as evolutionary algo-
rithms or reinforcement learning are omnipresent. When gradient-based methods
are used, models are kept small or rely on finite difference approximations for
the Jacobian. This method quickly grows expensive with increasing numbers of
parameters, such as found in deep learning. We propose an implementation of a
modern physics engine, which can differentiate control parameters. This engine
is implemented for both CPU and GPU. Firstly, this paper shows how such an
engine speeds up the optimization process, even for small problems. Furthermore,
it explains why this is an alternative approach to deep Q-learning, for using deep
learning in robotics. Finally, we argue that this is a big step for deep learning in
robotics, as it opens up new possibilities to optimize robots, both in hardware and
software.
1 Introduction
To solve tasks efficiently, robots require an optimization of their control system. This optimization
process can be done in automated testbeds (Degrave et al., 2015), but typically these controllers are
optimized in simulation. Standard methods to optimize these controllers include particle swarms,
reinforcement learning, genetic algorithms and evolutionary strategies. These are all derivative-free
methods.
A recently popular alternative approach is to use deep Q-learning, a reinforcement learning algo-
rithm. This method requires a lot of evaluations in order to train the many parameters (Levine
et al., 2016). However, deep learning experience has taught us that optimizing with a gradient is
often faster and more efficient. This fact is especially true when there are a lot of parameters, as
is common in deep learning. However, in the optimization processes for control systems, the robot
is almost exclusively treated as a non-differentiable black box. The reason for this is that the robot
in hardware is not differentiable, nor are current physics engines able to provide the gradient of
the robot models. The resulting need for derivative-free optimization approaches limits both the
optimization speed and the number of parameters in the controllers.
Recent physics engines, such as mujoco (Todorov et al., 2012), can derive gradients through the
model of a robot but rely on a finite difference method to approximate the gradient. Evaluating finite
difference approximations, however, requires the same number of model evaluations as the number
of states with respect to which is differentiated. Additionally, the gradient is an estimation.
In this paper, we suggest an alternative approach, by introducing a differentiable physics engine
with analytical gradients. This idea is not novel. It has been done before with spring-damper models
in 2D and 3D (Hermans et al., 2014). This technique is also similar to adjoint optimization, a
method widely used in various applications such as thermodynamics (Jarny et al., 1991) and fluid
* Former member, currently unaffiliated
1
Under review as a conference paper at ICLR 2017
dynamics (Iollo et al., 2001). However, modern engines to model robotics are not based on spring-
damper systems. The most commonly used ones are 3D rigid body engines, which rely on impulse-
based velocity stepping methods (Erez et al., 2015). In this paper, we test whether these engines are
also differentiable and whether this gradient is computationally tractable. We will show how this
method does speed up the optimization process tremendously, and give some examples where we
optimize deep learned neural network controllers with millions of parameters.
2 A 3D Rigid Body Engine
The goal is to implement a modern 3D Rigid body engine, in which parameters can be differenti-
ated with respect to the fitness a robot achieves in a simulation, such that these parameters can be
optimized with methods based on gradient descent.
The most frequently used simulation tools for model-based robotics, such as PhysX, Bullet, Havok
and ODE, go back to MathEngine (Erez et al., 2015). These tools are all 3D rigid body engines,
where bodies have 6 degrees of freedom, and the relations between them are defined as constraints.
These bodies exert impulses on each other, but their positions are constrained, e.g. to prevent the
bodies from penetrating each other. The velocities, positions and constraints of the rigid bodies
define a linear complementarity problem (LCP) (Chappuis, 2013), which is then solved using a
Gauss-Seidel projection (GSP) method (Jourdan et al., 1998). The solution of this problem are the
new velocities of the bodies, which are then integrated by semi-implicit Euler integration to get the
new positions (Stewart and Trinkle, 2000). This system is not always numerically stable. Therefore
the constraints are usually softened (Catto, 2009).
The recent growth of automatic differentiation libraries, such as Theano (Al-Rfou et al., 2016),
Caffe (Jia et al., 2014) and Tensorflow (Abadi et al., 2015), has allowed for efficient differentiation
of remarkably complex functions before (Degrave et al., 2016). Therefore, we implemented such
a physics engine as a mathematical expression in Theano, a software library which does automatic
evaluation and differentiation of expressions with a focus on deep learning. The resulting compu-
tational graph to evaluate this expression is then compiled for both CPU and GPU. To be able to
compile for GPU however, we had to limit our implementation to a restricted set of elementary op-
erations. The range of implementable functions is therefore severely capped. However, since the
analytic gradient is determined automatically, the complexity of correctly implementing the differ-
entiation is removed entirely.
One of these limitations with this restricted set of operations, is the limited support for conditionals.
Therefore we needed to implement our physics engine without branching, as this is not yet available
in Theano for GPU. Therefore some sacrifices had to be made. For instance, our system only allows
for contact constraints between different spheres or between spheres and the ground plane. Collision
detection algorithms for cubes typically have a lot of branching (Mirtich, 1998). However, this
sphere based approach can in principle be extended to any other shape (Hubbard, 1996). On the
other hand, we did implement a rather accurate model of servo motors, with gain, maximal torque,
and maximal velocity parameters.
Another design choice was to use rotation matrices rather than the more common quaternions for
representing rotations. Consequently, the states of the bodies are larger, but the operations required
are matrix multiplications. This design reduced the complexity of the graph. However, cumulative
operations on a rotation matrix might move the rotation matrix away from orthogonality. To correct
for this, we renormalize our matrix with the update equation (Premerlani and Bizard, 2009):
A0 = 3A-A；(A ∙ A)	⑴
where A0 is the renormalized version of the rotation matrix A. ‘◦’ denotes the elementwise multi-
plication, and ‘•’ the matrix multiplication.
These design decisions are the most important aspects of difference with the frequently used sim-
ulation tools. In the following section, we will evaluate our physics simulator on some different
problems. We take a look at the speed of computation and the number of evaluations required
before the parameters of are optimized.
2
Under review as a conference paper at ICLR 2017
2.1 Throwing a Ball
To test our engine, we implemented the model of a giant soccer ball in the physics engine, as shown
in Fig. 2a. The ball has a 1m diameter, a friction of μ = 1.0 and restitution e = 0.5. The ball starts
off at position (0, 0). After 5 s it should be at position (10, 0) with zero velocity v and zero angular
velocity ω . We optimized the initial velocity v0 and angular velocity ω0 at time t = 0 s until the
errors at t = 5 s are less than 0.01 m and 0.01 m/s respectively.
Since the quantity we optimize is only know at the end of the simulation, but we need to opti-
mize the parameters at the beginning of the simulation, we need to backpropagate our error through
time (BPTT) (Sutskever, 2013). This approach is similar to the backpropagation through time
method used for optimizing recurrent neural networks (RNN). In our case, every time step in the
simulation can be seen as one pass through a neural network, which transforms the inputs from
this timestep to inputs for the next time step. For finding the gradient, this RNN is unfolded com-
pletely, and the gradient can be obtained by differentiating this unfolded structure. This analytic
differentiation is done automatically by the Theano library.
Optimizing the six parameters in v0 and ω0 took only 88 iterations with gradient descent and back-
propagation through time. Optimizing this problem with CMA-ES (Hansen, 2006), a state of the art
derivative-free optimization method, took 2422 iterations. Even when taking the time to compute the
gradient into account, the optimization with gradient descent takes 16.3 s, compared to 59.9 s with
CMA-ES. This result shows that gradient-based optimization of kinematic systems can in some
cases already outperform gradient-free optimization algorithms from as little as six parameters.
3	Policy Search
To evaluate the relevance of our differentiable physics engine, we use a neural network as a general
controller for a robot, as shown in Figure 1. We consider a general robot model in a discrete-time
dynamical system xt+1 = fph(xt , ut) with a task cost function of l(xt , p), where xt is the state
of the system at time t and ut is the input of the system at time t. p provides some freedom in
parameterizing the loss. If Xt is the trajectory of the state up to time t - 1, the goal is to find a
policy ut = π(Xt ) such that we minimize the loss Lπ .
T
Lπ =	l(xt, p)
t=0
(2)
s.t. xt+1 = fph(xt, π(Xt)) and x0 = xinit
In previous research, finding a gradient for this objective has been described as presenting chal-
lenges (Mordatch and Todorov, 2014). An approximation to tackle these issues has been discussed
in Levine and Koltun (2013).
We implement this equation into an automatic differentiation library, ignoring these challenges in
finding the analytic gradient altogether. The automatic differentiation library, Theano in our case,
analytically derives this equation and compiles code to evaluate both the equation and its gradient.
We define our controller as a deep neural network gdeep with weights W. We do not pass all infor-
mation Xt to this neural network, but only a vector of values st observed by the modeled sensors
s(xt). We also provide our network with (some of the) task-specific parameters p0. Finally, we add
a recurrent connection to the controller in the previous timestep ht . Therefore, our policy is the
following:
π(Xt) = gdeep(s(xt),ht,p0 | W)
s.t.	ht	=	hdeep(s(xt-1), ht-1,	p0	|	W)	and	h0	= 0
(3)
Notice the similarity between equations 2 and 3. Indeed, the equations for recurrent neural net-
works (RNN) in equation 3 are very similar to the ones of the loss ofa physical model in equation 2.
Therefore, we optimize this entire system as an RNN unfolded over time, as illustrated in Figure 4.
The weights W are optimized with stochastic gradient descent. The gradient required for that is the
Jacobian dL/dW, which is found with automatic differentiation software.
3
Under review as a conference paper at ICLR 2017
We have now reduced the problem to a standard deep learning problem. We need to train our
network gdeep on a sufficient amount of samples xinit and for a sufficient amount of sampled tasks p
in order to get adequate generalization. Standard RNN regularization approaches could also improve
this generalization. We reckon that generalization of gdeep to more models fph, in order to ease the
transfer of the controller from the model to the real system, is also possible (Hermans et al., 2014),
but it is outside the scope of this paper.
Figure 1: Illustration of how a closed loop neural network controller would be used to actuate a
robot. The neural network receives sensor signals from the sensors on the robot and uses these to
generate motor signals which are sent to the servo motors. The neural network can also generate a
signal which it can use at the next timestep to control the robot.
3.1	Quadrupedal Robot - Computing Speed
To verify the speed of our engine, we also implemented a small quadrupedal robot model, as illus-
trated in Fig. 2b. This model has a total of 81 sensors, e.g. encoders and an inertial measurement
unit (IMu). The servo motors are controlled in a closed loop by a small neural network gdeep with
a number of parameters, as shown previously in Fig. 4. The gradient is the Jacobian of L, the total
traveled distance of the robot in 10 s , differentiated with respect to all the parameters of the con-
troller W. This Jacobian is found by using BpTT and propagating all 10s back. The time it takes
to compute this traveled distance and the accompanying Jacobian is shown in Table 1. We include
both the computation time with and without the gradient, i.e. both the forward and backward pass
and the forward pass alone. This way, the numbers can be compared to other physics engines, as
those only calculate without gradient. our implementation and our model can probably be made
more efficient, and evaluating the gradient can probably be made faster a similar factor.
(a) Ball model (b) quadruped model (c) robot arm model
Figure 2: (a) Illustration of the ball model used in the first task. (b) Illustration of the quadruped
robot model with 8 actuated degrees of freedom, 1 in each shoulder, 1 in each elbow. The spine of
the robot can collide with the ground, through 4 spheres in the inside of the cuboid. (c) Illustration
of the robot arm model with 4 actuated degrees of freedom.
When only a single controller is optimized, our engine runs more slowly on Gpu than on Cpu. To
tackle this issue, we implemented batch gradient descent, which is commonly used in complex op-
timization problems. In this case, by batching our robot models, we achieve significant acceleration
on Gpu. Although backpropagating the gradient through physics slows down the computations by
roughly a factor 10, this factor only barely increases with the number of parameters in our controller.
Combining this with our previous observation that fewer iterations are needed when using gradi-
ent descent, our approach can enable the use of gradient descent through physics for highly complex
4
Under review as a conference paper at ICLR 2017
deep neural network controllers with millions of parameters. Also note that by using a batch method,
a single GPU can simulate about 864 000 model seconds per day, or 86 400 000 model states. This
should be plenty for deep learning. It also means that a single simulation step of a single robot,
which includes collision detection, solving the LCP problem, integrating the velocities and back-
propagating the gradient through it all, takes about 1 ms on average. Without the backpropagation,
this process is only about seven times faster.
3.2	4 Degree of Freedom Rob ot Arm
As a first test of optimizing robot controllers, we implemented a four degree of freedom robotic arm,
as depicted in Fig. 2c. The bottom of the robot has a 2 degrees of freedom actuated universal joint;
the elbow has a 2 degree of freedom actuated joint as well. The arm is 1 m long, and has a total mass
of 32kg. The servos have a gain of 30s-1, a torque of 30 Nm and a velocity of 45。s-1.
For this robot arm, we train controllers for a task with a gradually increasing amount of difficulty.
To be able to train our parameters, we have to use a couple of tricks often used in the training of
recurrent neural networks.
•	We choose an objective which is evaluated at every time step and then averaged, rather
than at specific points of the simulation. This approach vastly increases the number of
samples over which the gradient is averaged, which in turn makes the gradient direction
more reliable (Sjoberg et al., 1995).
•	The value of the gradient is decreased by a factor α < 1 at every time step. This trick has
the effect of a prior. Namely, events further in the past are less important for influencing
current events, because intermediate events might diminish their influence altogether. It
also improves robustness against exploding gradients (Hermans et al., 2014).
•	We initialize the controller intelligently. We do not want the controller to shake the actua-
tors violently and explore outside the accurate domain of our simulation model. Therefore
our controllers are initialized such that they only output zeros at the start of the simulation.
The initial policy is the zero policy.
•	We constraint the size of the gradient to an L2-norm of 1. This makes sure that gradients
close to discontinuities in the fitness landscape do not push the parameter values too far
away, such that everything which was learned is forgotten (Sutskever, 2013).
3.2.1	Reaching a Fixed Point
A first simple task, is to have a small neural net controller learn to move the controller to a certain
fixed point in space, at coordinates (0.5 m; 0.5 m; 0.5 m). The objective we minimize for this task, is
the distance between the end effector and the target point, averaged over the 8 seconds we simulate
our model.
We provide the controller with a single sensor input, namely the current distance between the end
effector and the target point. Input is not required for this task, as there are solutions for which the
motor signals are constant in time. However, this would not necessarily be the optimal approach for
minimizing the average distance over time, it only solves the distance at the end of the simulation,
but does not minimize the distance during the trajectory to get at the final position.
As a controller, we use a dense neural network with 1 input, 2 hidden layers of 128 units with a
rectifier activation function, and 4 outputs with an identity activation function. This controller has
17 284 parameters in total. We disabled the recurrent connections ht .
We use gradient descent with a batch size of 1 robot for optimization, as the problem is not stochastic
in nature. The parameters are optimized with Adam’s rule (Kingma and Ba, 2014) with a learning
rate of 0.001. Every update step with this method takes about 5 seconds on CPU. We find that the
controller comes within 4 cm of the target in 100 model evaluations, and within 1 cm in 150 model
evaluations, which is small compared to the 1 m arm of the robot. Moreover, the controller does find
a more optimal trajectory which takes into account the sensor information.
Solving problems like these in fewer iteration steps than the number of parameters, is unfeasible
with derivative free methods (Sjoberg et al., 1995). Despite that, We did try to optimize the same
5
Under review as a conference paper at ICLR 2017
problem with CMA-ES. After a week of computing and 60 000 model evaluations, CMA-ES did not
show any sign of convergence, as it cannot handle the sheer amount of parameters.
3.2.2	Reaching a Random Point
As a second task, we sample a random target point in the reachable space of the end effector. We
give this point as input v0 to the controller, and the task is to again minimize the average distance
between the end effector and the target point v. Our objective L is this distance averaged over all
timesteps.
As a controller, we use a dense neural network comparable to the previous section, but this time with
3 inputs. We used 3 hidden layers with 1024 units each, so the controller has 2 107 396 parameters
in total. This is not necessary for this task, but we do it like this to demonstrate the power of this
approach. In order to train for this task, we use a batch size of 128 robots, such that every update step
takes 58 s on GPU. Each simulation takes 8 s with a simulation step of 0.01 s. Therefore, the gradient
on the parameters of the controllers has been averaged over 51 200 timesteps at every update step.
We update the parameters with Adam’s rule, where we scale the learning rate with the average error
achieved in the previous step.
We find that it takes 576 update steps before the millions of parameters are optimized, such that the
end effector of the robot is on average less than 10 cm of target, 2 563 update steps before the error
is less than 5 cm.
3.3 A Quadrupedal Robot - Revisited
Optimizing a gait for a quadrupedal robot is a problem of a different order, something the authors
have extensive experience with (Sproewitz et al., 2013; Degrave et al., 2013; 2015). The problem is
way more challenging and allows for a broad range of possible solutions. in nature, we find a wide
variety of gaits, from hopping over trotting, walking and galloping. With hand tuning on the robot
model shown in Figure 2b, we were able to obtain a trotting motion with an average forward speed
of 0.7 m/s. We found it tricky to find a gait where the robot did not end up like an upside down
turtle, as 75% of the mass of the robot is located in its torso.
As a controller for our quadrupedal robot, we use a neural network with 2 input signals st , namely
a sine and a cosine signal with a frequency of 1.5 Hz. on top of this, we added 2 hidden layers of
128 units and a rectifier activation function. As output layer, we have a dense layer with 8 units and
a linear activation function, which has as input both the input layer and the top layer of the hidden
layers. in total, this controller has 17 952 parameters. Since the problem is not stochastic in nature,
we use a batch size of 1 robot. We initialize the output layer with zero weights, so the robot starts
the optimization in a stand still position.
We optimize these parameters to maximize the average velocity of the spine over the course of 10 s
of time in simulation. This way, the gradient used in the update step is effectively an average of the
1 000 time steps after unrolling the recurrent connections. This objective does not take into account
energy use, or other metrics typically employed in robotic problems.
in only 500 model evaluations or about 1 hour of optimizing on CPu, the optimization with BPTT
comes up with a solution with a speed of 1.17 m/s. This solution is a hopping gait, with a summ-
ersault every 3 steps1, despite limiting the torque of the servos to 4 Nm on this 28.7 kg robot. For
more life-like gaits, energy efficiency could be use as a regularization method. Evaluating these
improvements are however outside the scope of this paper.
3.3.1	The inverted pendulum with a camera as sensor
As a fourth example, we implemented a model of the pendulum-cart system we have in our labora-
torium. This pendulum-cart system is used for the classic control task of the underactuated inverted
pendulum (Vaccaro, 1995). in this example however, a camera which is set up in front of the system
is the only available information for the controller. it therefore has to observe the system it controls
using vision. A frame captured by this camera is shown in Figure 3.
1A video is available at https://goo.gl/5ykZZe
6
Under review as a conference paper at ICLR 2017
In order to build this model, we implemented a differentiable camera in our physics engine. This
camera uses a ray-tracing approach to find where it needs to sample from the textures, and uses
bilinear interpolation to sample from these textures, similar to the one used for the spatial transform
layer (Jaderberg et al., 2015). This interpolation is necessary for making the frame captured by the
camera differentiable to the state of the robot with non-zero derivatives.
Figure 3: A frame captured by the differentiable camera looking at the model of the pendulum-cart
system. The resolution used is 288 by 96 pixels. All the textures are made from pictures of the
actual system.
We minimize the distance from the end of the pendulum to the desired point and regularize the
speed of the pendulum. The memoryless deep controller receives the current image of the camera, in
addition to two images from the past such that it can estimate velocity and acceleration. We observe
that a controller with 1,065,888 parameters is able to learn to swing up and keep the pendulum stable
after only 2420 episodes of 3 model seconds. The complete optimization process took 15 hours. The
resulting controller keeps the pendulum stable for more than one minute2. In order to do this, the
controller has learned to interpret the frames it receives from the camera and found a suitable control
strategy.
4	Discussion
Our results show the first prototype of a differentiable physics engine based on similar algorithms
as those that are commonly used in modern robotics simulators. When initially addressing the
problem, we did not know whether finding the gradient would be computationally tractable, let alone
whether evaluating it would be fast enough to be beneficial for optimization. In this paper, we have
demonstrated that evaluating the gradient is tractable enough to speed up optimization on problems
with as little as six parameters. The speed of this evaluation mainly depends on the complexity
of the physics model and only slightly on the number of parameters to optimize. Therefore, our
results suggest that this cost is dominated by the gain achieved from the combination of using batch
gradient descent and GPU acceleration.
Optimizing the controller of a robot model with gradient-based optimization is equivalent to op-
timizing an RNN. After all, the gradient passes through each parameter at every time step. The
parameter space is therefore very noisy. Consequently, training the parameters of this controller is a
highly non-trivial problem, as it corresponds to training the parameters of an RNN. On top of that,
exploding and vanishing signals and gradients cause far more challenging problems compared to
feed forward networks.
In section 3.2, we already discussed some of the tricks used for optimizing RNNs. Earlier re-
search shows that these methods can be extended to more complicated tasks than the ones discussed
here (Hermans et al., 2014; Sutskever, 2013). Hence, we believe that this approach towards learn-
ing controllers for robotics applies to more complex problems than the illustrative examples in this
paper.
All of the results in this paper will largely depend on showing how these controllers will work on
the physical counterparts of our models. Nonetheless, we would like to conjecture that to a certain
extent, this gradient of a model is close to the gradient of the physical system. The gradient of
the model is more susceptible to high-frequency noise introduced by modeling the system, than
the imaginary gradient of the system itself. Nonetheless, it contains information which might be
indicative, even if it is not perfect. We would theorize that using this noisy gradient is still better than
2https://twitter.com/317070/Status/821062814798331905
7
Under review as a conference paper at ICLR 2017
optimizing in the blind and that the transferability to real robots can be improved by evaluating the
gradients on batches of (slightly) different robots in (slightly) different situations and averaging the
results. This technique has already been applied in (Hermans et al., 2014) as a regularization method
to avoid bifurcations during online learning. If the previous proves to be correct, our approach
can offer an addition or possibly even an alternative to deep Q-learning for deep neural network
controllers in robotics.
We can see the use of this extended approach for a broad range of applications in robotics. Not
only do we think there are multiple ways where recent advances in deep learning could be applied
to robotics more efficiently with a differentiable physics engine, we also see various ways in which
this engine could improve existing angles at which robotics are currently approached:
•	In this paper, we added memory by introducing recurrent connections in the neural network
controller. We reckon that advanced, recurrent connections such as ones with a memory
made out of LSTM cells (Hochreiter and Schmidhuber, 1997) can allow for more powerful
controllers than the controllers described in this paper.
•	Using a differentiable physics engine, we reckon that knowledge of a model can be trans-
ferred more efficiently into a forward or backward model in the form of a neural network,
similar to methods such as used in Johnson et al. (2016) and Dumoulin et al. (2016). By
differentiating through an exact model and defining a relevant error on this model, it should
be possible to transfer knowledge from a forward or backward model in the differentiable
physics engine to a forward or backward neural network model. Neural network mod-
els trained this way might be more robust than the ones learned from generated trajecto-
ries (Christiano et al., 2016). In turn, this neural model could then be used for faster but
approximate evaluation of the model.
•	Although we did not address this in this paper, there is no reason why only control param-
eters could be differentiated. Hardware parameters of the robot have been optimized the
same way before (Jarny et al., 1991; Iollo et al., 2001; Hermans et al., 2014). The authors
reckon that the reverse process is also true. A physics engine can provide a strong prior,
which can be used for robots to learn (or adjust) their robot models based on their hardware
measurements faster than today. You could optimize the model parameters with gradient
descent through physics, to have the model better mimic the actual observations.
•	Where adversarial networks are already showing their use in generating image models,
we believe adversarial robotics training (ART) will create some inventive ways to design
and control robots. Like in generative adversarial nets (GAN) (Goodfellow et al., 2014),
where the gradient is pulled through two competing neural networks, the gradient could be
pulled through multiple competing robots as well. It would form an interesting approach
for swarm robotics, similar to previous results in evolutionary robotics (Sims, 1994; Pfeifer
and Bongard, 2006; Cheney et al., 2014), but possibly faster.
5	Conclusion
In this paper, we show it is possible to build a differentiable physics engine. We implemented a
modern engine which can run a 3D rigid body model, using the same algorithm as other engines
commonly used to simulate robots, but we can additionally differentiate control parameters with
BPTT. Our implementation also runs on GPU, and we show that using GPUs to simulate the physics
can speed up the process for large batches of robots. We show that even complex sensors such as
cameras, can be implemented and differentiated through, allowing for computer vision to be learned
together with a control policy.
We find that these gradients can be computed fast enough for use in applications. We also show that
using gradient descent with BPTT speeds up optimization processes often found in robotics, even
for rather small problems, due to the reduced number of model evaluations required. We show that
this improvement in speed scales to problems with a lot of parameters. We also show that using this
engine, finding policies for robot models can be done faster and in a more straightforward way. This
method should allow for a new approach to apply deep learning techniques in robotics.
8
Under review as a conference paper at ICLR 2017
Acknowledgments
Special thanks to David Pfau for pointing out relevant prior art we were previously unaware of,
and Iryna Korshunova for proofreading the paper. The research leading to these results has re-
ceived funding from the Agency for Innovation by Science and Technology in Flanders (IWT). The
NVIDIA Corporation donated the GTX 1080 used for this research.
References
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin,
M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur,
M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner,
B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P.,
Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. (2015). TensorFlow: Large-scale machine learning on
heterogeneous systems. Software available from tensorflow.org.
Al-Rfou, R., Alain, G., Almahairi, A., Angermueller, C., Bahdanau, D., Ballas, N., Bastien, F., Bayer, J., Be-
likov, A., Belopolsky, A., Bengio, Y., Bergeron, A., Bergstra, J., Bisson, V., Bleecher Snyder, J., Bouchard,
N., Boulanger-Lewandowski, N., Bouthillier, X., de BrebiSson, A., Breuleux, O., Carrier, P.-L., Cho, K.,
Chorowski, J., Christiano, P., Cooijmans, T., Cote, M.-A., Cote, M., Courville, A., Dauphin, Y. N., Delal-
leau, O., Demouth, J., Desjardins, G., Dieleman, S., Dinh, L., Ducoffe, M., Dumoulin, V., Ebrahimi Kahou,
S., Erhan, D., Fan, Z., Firat, O., Germain, M., Glorot, X., Goodfellow, I., Graham, M., Gulcehre, C., Hamel,
P., Harlouchet, I., Heng, J.-P., Hidasi, B., Honari, S., Jain, A., Jean, S., Jia, K., Korobov, M., Kulkarni, V.,
Lamb, A., Lamblin, P., Larsen, E., Laurent, C., Lee, S., Lefrancois, S., Lemieux, S., Leonard, N., Lin, Z.,
Livezey, J. A., Lorenz, C., Lowin, J., Ma, Q., Manzagol, P.-A., Mastropietro, O., McGibbon, R. T., Memi-
sevic, R., van Merrienboer, B., Michalski, V., Mirza, M., Orlandi, A., Pal, C., Pascanu, R., Pezeshki, M.,
Raffel, C., Renshaw, D., Rocklin, M., Romero, A., Roth, M., Sadowski, P., Salvatier, J., Savard, F., Schluter,
J., Schulman, J., Schwartz, G., Serban, I. V., Serdyuk, D., Shabanian, S., Simon, E., Spieckermann, S.,
Subramanyam, S. R., Sygnowski, J., Tanguay, J., van Tulder, G., Turian, J., Urban, S., Vincent, P., Visin, F.,
de Vries, H., Warde-Farley, D., Webb, D. J., Willson, M., Xu, K., Xue, L., Yao, L., Zhang, S., and Zhang,
Y. (2016). Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints,
abs/1605.02688.
Catto, E. (2009). Modeling and solving constraints. In Game Developers Conference.
Chappuis, D. (2013). Constraints derivation for rigid body simulation in 3D.
Cheney, N., Clune, J., and Lipson, H. (2014). Evolved electrophysiological soft robots. In ALIFE, volume 14,
pages 222-229.
Christiano, P., Shah, Z., Mordatch, I., Schneider, J., Blackwell, T., Tobin, J., Abbeel, P., and Zaremba, W.
(2016). Transfer from simulation to real world through learning deep inverse dynamics model. arXiv preprint
arXiv:1610.03518.
Degrave, J., Burm, M., Kindermans, P.-J., Dambre, J., and wyffels, F. (2015). Transfer learning of gaits on a
quadrupedal robot. Adaptive Behavior, page 1059712314563620.
Degrave, J., Burm, M., Waegeman, T., Wyffels, F., and Schrauwen, B. (2013). Comparing trotting and turning
strategies on the quadrupedal oncilla robot. In Robotics and Biomimetics (ROBIO), 2013 IEEE International
Conference on, pages 228-233. IEEE.
Degrave, J., Dieleman, S., Dambre, J., and wyffels, F. (2016). Spatial chirp-Z transformer networks. In
European Symposium on Artificial Neural Networks (ESANN).
Dumoulin, V., Shlens, J., and Kudlur, M. (2016). A learned representation for artistic style. CoRR,
abs/1610.07629.
Erez, T., Tassa, Y., and Todorov, E. (2015). Simulation tools for model-based robotics: Comparison of bullet,
havok, mujoco, ode and physx. In International Conference on Robotics and Automation (ICRA), pages
4397-4404. IEEE.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. (2014). Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672-
2680.
Hansen, N. (2006). The cma evolution strategy: a comparing review. In Towards a new evolutionary computa-
tion, pages 75-102. Springer Berlin Heidelberg.
9
Under review as a conference paper at ICLR 2017
Hermans, M., Schrauwen, B., Bienstman, P., and Dambre, J. (2014). Automated design of complex dynamic
systems. PloS one, 9(1):e86696.
Hochreiter, S. and SChmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8):1735-1780.
Hubbard, P. M. (1996). Approximating polyhedra with spheres for time-critical collision detection. ACM
Transactions on Graphics (TOG), 15(3):179-210.
Iollo, A., Ferlauto, M., and Zannetti, L. (2001). An aerodynamic optimization method based on the inverse
problem adjoint equations. Journal of Computational Physics, 173(1):87-115.
Jaderberg, M., Simonyan, K., Zisserman, A., et al. (2015). Spatial transformer networks. In Advances in Neural
Information Processing Systems, pages 2017-2025.
Jarny, Y., Ozisik, M., and Bardon, J. (1991). A general optimization method using adjoint equation for solving
multidimensional inverse heat conduction. International journal of heat and mass transfer, 34(11):2911-
2919.
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., and Darrell, T. (2014).
Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.
Johnson, J., Alahi, A., and Fei-Fei, L. (2016). Perceptual losses for real-time style transfer and super-resolution.
arXiv preprint arXiv:1603.08155.
Jourdan, F., Alart, P., and Jean, M. (1998). A gauss-seidel like algorithm to solve frictional contact problems.
Computer methods in applied mechanics and engineering, 155(1):31-47.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. Proceedings of the 3rd
International Conference on Learning Representations (ICLR).
Levine, S. and Koltun, V. (2013). Variational policy search via trajectory optimization. In Advances in Neural
Information Processing Systems, pages 207-215.
Levine, S., Pastor, P., Krizhevsky, A., and Quillen, D. (2016). Learning hand-eye coordination for robotic
grasping with deep learning and large-scale data collection. arXiv preprint arXiv:1603.02199.
Mirtich, B. (1998). V-clip: Fast and robust polyhedral collision detection. ACM Transactions On Graphics
(TOG), 17(3):177-208.
Mordatch, I. and Todorov, E. (2014). Combining the benefits of function approximation and trajectory opti-
mization. In Robotics: Science and Systems (RSS).
Pfeifer, R. and Bongard, J. (2006). How the body shapes the way we think: a new view of intelligence. MIT
press.
Premerlani, W. and Bizard, P. (2009). Direction cosine matrix IMU: Theory. DIY DRONE: USA, pages 13-15.
Sims, K. (1994). Evolving 3d morphology and behavior by competition. Artificial life, 1(4):353-372.
Sjoberg, J., Zhang, Q., Ljung, L., Benveniste, A., Delyon, B., Glorennec, P.-Y., Hjalmarsson, H., and Judit-
sky, A. (1995). Nonlinear black-box modeling in system identification: a unified overview. Automatica,
31(12):1691-1724.
Sproewitz, A., Tuleu, A., D,Haene, M., Mockel, R., Degrave, J., Vespignani, M., Gay, S., Ajallooeian, M.,
Schrauwen, B., and Ijspeert, A. J. (2013). Towards dynamically running quadruped robots: performance,
scaling, and comparison. In Adaptive Motion of Animals and Machines, pages 133-135.
Stewart, D. and Trinkle, J. C. (2000). An implicit time-stepping scheme for rigid body dynamics with coulomb
friction. In International Conference on Robotics and Automation (ICRA), volume 1, pages 162-169. IEEE.
Sutskever, I. (2013). Training recurrent neural networks. PhD thesis, University of Toronto.
Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE.
Vaccaro, R. J. (1995). Digital control: a state-space approach, volume 196. McGraw-Hill New York.
10
Under review as a conference paper at ICLR 2017
6 Appendix
6.1 Figures
time t
Figure 4: Illustration of the dynamic system with the robot and controller, after unrolling over time.
The neural networks gdeep and hdeep with weights W receive sensor signals st from the sensors on
the robot and use these to generate motor signals ut which are used by the physics engine fph to
find the next state of the robot in the physical system. These neural networks also have a memory,
implemented with recurrent connections ht . From the state xt of these robots, the loss L can be
found. In order to find dL/dW, every block in this chart needs to be differentiable. The contribution
of this paper, is to implement a differentiable fph , which allows us to optimize W to minimize L
more efficiently than was possible before.
11
Under review as a conference paper at ICLR 2017
6.2 Tables
Table 1: Evaluation of the computing speed of our engine on a robot model controlled by a closed
loop controller with a variable number of parameters. We evaluated both on CPU (i7 5930K) and
GPU (GTX 1080), both for a single robot optimization and for batches of multiple robots in parallel.
The numbers are the time required in seconds for simulating the quadruped robot(s) for 10 s, with
and without updating the controller parameters through gradient descent. The gradient calculated
here is the Jacobian of the total traveled distance of the robot in 10 s, differentiated with respect to all
the parameters of the controller. For comparison, the model has 102 states. It is built from 17 rigid
bodies, each having 6 degrees of freedom. These states are constrained by exactly 100 constraints.
Seconds of computing time required to simulate a batch of robots for 10 seconds
with gradient
CPU GPU
1 robot	1 296 parameters	8.17	69.6
	1 147 904 parameters	13.2	75.0
128 robots
1296 parameters	263	128
1147 904 parameters	311	129
without gradient
CPU	GPU
1.06	9.69
2.04	9.69
	
47.7	17.8
50.4	18.3
Milliseconds of computing time required to perform one time step of one robot.
	with gradient CPU GPU		without gradient CPU	GPU
1 296 parameters 1 robot 1 147 904 parameters	8.17 I 69.6		1.06	9.69
	13.2	75.0		2.04	9.69
			
1 296 parameters 128 robots 1 147 904 parameters	2.05	1.00 2.43	1.01		0.372	0.139 0.394	0.143
12