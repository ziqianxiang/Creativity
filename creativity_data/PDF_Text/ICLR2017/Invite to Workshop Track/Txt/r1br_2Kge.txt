Short and Deep:
Sketching and Neural Networks
Amit Daniely, Nevena Lazic, Yoram Singer, Kunal Talwar*
Google Brain
Ab stract
Data-independent methods for dimensionality reduction such as random projec-
tions, sketches, and feature hashing have become increasingly popular in recent
years. These methods often seek to reduce dimensionality while preserving the
hypothesis class, resulting in inherent lower bounds on the size of projected data.
For example, preserving linear separability requires Ω(1 /γ2) dimensions, where Y
is the margin, and in the case of polynomial functions, the number of required di-
mensions has an exponential dependence on the polynomial degree. Despite these
limitations, we show that the dimensionality can be reduced further while main-
taining performance guarantees, using improper learning with a slightly larger
hypothesis class. In particular, we show that any sparse polynomial function of a
sparse binary vector can be computed from a compact sketch by a single-layer neu-
ral network, where the sketch size has a logarithmic dependence on the polyno-
mial degree. A practical consequence is that networks trained on sketched data are
compact, and therefore suitable for settings with memory and power constraints.
We empirically show that our approach leads to networks with fewer parameters
than related methods such as feature hashing, at equal or better performance.
1	Introduction
In many supervised learning problems, input data are high-dimensional and sparse. The high di-
mensionality may be inherent in the domain, such as a large vocabulary in a language model, or
the result of creating hybrid conjunction features. This setting poses known statistical and compu-
tational challenges for standard supervised learning techniques, as high-dimensional inputs lead to
models with a very large number of parameters.
An increasingly popular approach to reducing model size is to map inputs to a lower-dimensional
space in a data-independent manner, using methods such as random projections, sketches, and hash-
ing. These mappings typically attempt to preserve the hypothesis class, leading to inherent theoret-
ical limitations on size. For example, for linearly separable unit vectors with margin γ, it can be
shown that at least Ω(1∕γ2) dimensions are needed to preserve linear separability, even if one can
use arbitrary input embeddings (see Section D). It would therefore appear that data dimensionality
cannot be reduced beyond this bound.
In this work, we show that using a slightly larger hypothesis class when decoding projections (im-
proper learning) allows us to further reduce dimensionality while maintaining theoretical guarantees.
In particular, we show that any sparse polynomial function ofa sparse binary vector can be computed
from a very compact sketch by a single-layer neural network. The hidden layer allows us to “decode”
inputs from representations that are smaller than in existing work. In the simplest case, we show
that for linearly separable k-sparse d-dimensional inputs, one can create a O(k log 号)-dimensional
sketch of the inputs and guarantee that a single-layer neural network can correctly classify 1 - δ
fraction of the sketched data. In the case of polynomial functions, the required sketch size has a
logarithmic dependence on the polynomial degree.
For binary k-sparse input vectors, we show that it suffices to have a simple feed-forward network
with nonlinearity implemented via the commonly used rectified linear unit (Relu). We extend our
results to real-valued data that is close to being k-sparse, using less conventional min and median
nonlinearities. Furthermore, we show that data can be mapped using sparse sketching matrices.
* Email: kunal@google.com. Author for correspondences.
Under review as a conference paper at ICLR 2017
Thus, our sketches are efficient to compute and do not increase the number of non-zero input values
by much, in contrast to standard dense Gaussian projections.
We empirically evaluate our sketches on real and synthetic datasets. Our approach leads to more
compact neural networks than existing methods such as feature hashing and Gaussian random pro-
jections, at competitive or better performance. This makes our sketches appealing for deployment
in settings with strict memory and power constraints, such as mobile and embedded devices.
2	Previous work
To put our work in context, we next summarize some lines of research related to this work.
Random projections and sketching. Random Gaussian projections are by now a standard tool for
dimensionality reduction. For general vectors, the Johnson-Lindenstrauss (1984) Lemma implies
that a random Gaussian projection into O(log(1∕δ)∕ε2) dimensions preserves the inner product be-
tween a pair of unit vectors up to an additive factor ε, with probability 1 - δ . A long line of work
has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon &
Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson,
2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed
related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estima-
tors for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for
the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min
sketch of Cormode and Muthukrishnan (2005a; 2005b).
Projections in learning. Random projections have been used in machine learning at least since
the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel func-
tions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007)
and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimen-
sionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009)
use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove
strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective
in reducing model size without significantly impacting performance. Hashing has also been used
in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch
in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a
tensor power of a vector could be quickly computed without explicitly computing the tensor power,
and applied it to fast sketching for polynomial kernels.
Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results
in this area, e.g. (Donoho, 2006; Candes & Tao, 20θ6), imply that a k-sparse vector X ∈ Rd can
be reconstructed w.h.p. from a projection of dimension O(k ln d). However, to our knowledge,
no provable decoding algorithms are implementable by a low-depth neural network. Recent work
by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive
sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs.
Parameter reduction in deep learning. Our work can be viewed as a method for reducing the
number of parameters in neural networks. Neural networks have become ubiquitous in many ma-
chine learning applications, including speech recognition, computer vision, and language processing
tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for
a few notable examples). These successes have in part been enabled by recent advances in scaling up
deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al.,
2012). However, a drawback of such large models is that they are very slow to train, and difficult
to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013)
demonstrate significant redundancies in the parameterization of several deep learning architectures,
and they propose training low-rank decompositions of weight matrices. Cheng et al. (2015) im-
pose circulant matrix structure on fully connected layers. Ba and Caruana (2014) train shallow
networks to predict the log-outputs of a large deep network, and Hinton et al. (2015) train a small
network to match smoothed predictions of a complex deep network or an ensemble of such models.
Collins and Kohli (2014) encourage zero-weight connections using sparsity-inducing priors, while
others such as LeCun et al. (1989); Hassibi et al. (1993); Han et al. (2015) use techniques for prun-
ing weights. HashedNets (Chen et al., 2015) enforce parameter sharing between random groups
2
Under review as a conference paper at ICLR 2017
of network parameters. In contrast to these methods, sketching only involves applying a sparse,
linear projection to the inputs, and does not require a specialized learning procedure or network
architecture.
3	S ketching
For simplicity, we first present our results for sparse binary vectors, and extend the discussion to
real-valued vectors to Appendix A. Let Bd,k = {x ∈ {0, 1}d : kxk0 ≤ k} be the set of k-
sparse d-dimensional binary vectors. We sketch such vectors using a family of randomized sketching
algorithms based on the count-min sketch, as described next.
Given a parameter m and a hash function h : [d] → [m], the sketch Sh(x) of a vector x ∈ Bd,k is a
binary vector y where each bit of y is the OR of bits of x that hash to it:
yl =	xi .
i : h(i) = l
We map data using a concatenation of several such sketches. Given an ordered set of hash functions
h1, . . . , ht d=ef h1:t , the sketch Sh1:t (x) is defined as a m × t matrix Y , where the jth column
corresponds to the sketch Shj (x). We define the following procedure for decoding the ith bit of the
input x from its sketch Y :
DhA1N:tD(Y,i)d=ef ^ Yhj(i)j.	(1)
j∈[t]
Thus, the decoded bit i is simply the AND of the t bits of Y that index i hashes to in the sketch.
The following theorem summarizes an important property of these sketches. As a reminder, a set of
hash functions h1:t from [d] to [m] is pairwise independent if for all i 6= j ∈ [d] and a, b ∈ [m],
Pr[h(i) = a ∧ h(j) = b] = m-2.1
Theorem 3.1. Letx ∈ Bd,k and for j ∈ [t] let hj : [d] → [m] be drawn uniformly and independently
from a pairwise independent distribution with m = ek. Then for any i,
Pr[DhA1N:tD(Sh1:t(x),i) 6=xi] ≤e-t.
Proof. Fix a vector x ∈ Bd,k . Let Eh(i) d=ef {i0 6= i : h(i0) = h(i)} denote the collision set of i for
a particular hash function h. Decoding will fail if xi = 0 and for each of the t hash functions, the
collision set of i for contains an index of a non-zero bit of x. For a particular h, the probability of
this event is:
Pr	xi0 = 1
i0∈Eh(i)
where the second inequality follows since the sum is over at most k terms, and each term is m-1
by pairwise independence. Thus Pr[Yhj (i) 6= xi] ≤ e-1 for any j ∈ [t]. Since hash functions
hj are drawn independently, and decoding can fail only if all t hash functions fail, it follows that
PrDANtD(ShLt(x),i)= Xi] ≤ e-t.	□
Let Hd,s denote the set Hd,s = {w ∈ Rd : kwk0 ≤ s}. We have the following corollary:
Corollary 3.2. Let W ∈ Hd,s and X ∈ Bd,k. Fort = log(s∕δ) ,and m = ek, if hι,...,ht are
drawn uniformly and independently from a pairwise independent distribution, then
Pr X wi DhA1N:tD(Sh1:t(x), i) 6= W>x ≤ δ .
i
k1
≤ E Pr[h(i0) = h(i)] ≤ —=—
me
i0:xi0 =1
1 Such hash families can be easily constructed (see e.g. Mitzenmacher & Upfal (2005)), using a O(log m)-
bit seed. Moreover, each hash can be evaluated using O(1) arithmetic operations over O(log d)-sized words.
3
Under review as a conference paper at ICLR 2017
4 Sparse linear functions
Let w ∈ Hd,s, x ∈ Bd,k, and Y =
Sh1:t (x) for m, t satisfying the conditions
of Corollary 3.2. We will now argue that
there exists a one-layer neural network
that takes Y as input and outputs w>x
with high probability (over the random-
ness of the sketching process).
Let Nn (f) denote the family of feed-
forward neural networks with one hidden
layer containing n nodes, nonlinearity f
applied at each hidden unit, and a linear
function at the output layer. We can con-
struct a network in Ns (Relu) such that
each hidden unit implements DhAND (Y, i)
(i.e. decodes bit xi from the sketch) for
each index i in the support of w. We can
x24 x29
Figure 1: Neural-network sketching: sparse vector X maps
to sketch using t = 3 hashes & m = 8; shaded squares
designate 1's; sketching step is random; sketch then used as
input to single-layer net: w>x; nodes labelled “24” & “29”
correspond to decoding of x24 & x29 and shown with non-
zero incoming edges.
then set the output weights of the network to the corresponding non-zero weights wi to get w>x.
it remains to show that a hidden unit can implement DhAND (Y, i). indeed, the AND of t bits can
be implemented using nearly any non-linearity. With Relu(a) = max{0, a}, we can construct the
activation for bit xi, ai = P(l,j) Vlj Ylj + Bi, by setting the appropriate t weights in Vlj to 1, setting
remaining weights to 0, and setting the bias Bi to 1 - t. Using Corollary 3.2, we have the following
theorem.
Theorem 4.1.	For every w ∈ Hd,s there exists a set of weights for a network N ∈ Ns(Relu) such
that for each x ∈ Bd,k,
P rh1:t [N (Sh1:t (x)) = w>x] ≥ 1 - δ ,
as long as m = ek and t = log(s∕δ). Moreover, the weights coming into each node in the hidden
layer are in {0, 1} with at most t non-zeros.
The final property implies that when using w as a linear classifier, we get small generalization error
as long as the number of examples is at least Ω(s(1 + t log mt)). This can be proved, e.g., using
standard compression arguments: each such model can be represented using only st log(mt) bits in
addition to the representation size of w. similar bounds hold when we use `1 bounds on the weight
coming into each unit. Note that even for s = d (i.e. w is unrestricted), we get non-trivial input
compression.
For comparison, we prove the following result for Gaussian projections in the appendix B. in this
case, the model weights in our construction are not sparse.
Theorem 4.2.	For every w ∈ Hd,s there exists a set of weights for a network N ∈ Ns(Relu) such
that for each x ∈ Bd,k,
P rh1:t [N (Gx)) = w>x] ≥ 1 - δ ,
as long as G is a random m X d Gaussian matrix, with m ≥ 4k log(s∕δ).
5 s parse polynomial functions
For boolean inputs, Theorem 4.1 extends immediately to sparse polynomial functions. Note that we
can implement the AND of two bits xi ∧xj as the AND of the corresponding decodings DhAND (Y, i)
and DhAND(Y, j). since each decoding is an AND of t bits, the overall decoding is an AND of at
most 2t locations in the sketch. More generally, we have the following theorem:
Theorem 5.1. Given w ∈ Rs, and sets A1, . . . , As ⊆ [d], letg : {0, 1}d → R denote the polynomial
ss
g(x) =	wj	xi =	wj	xi .
j=1	i∈Aj	j=1	i∈Aj
4
Under review as a conference paper at ICLR 2017
Then there exists a set of weights for a network N ∈ Ns(Relu) such that for each x ∈ Bd,k,
P rh1:t [N (Sh1:t (x)) = g(x)] ≥ 1 - δ ,
as long as m = ek and t = log(∣ ∪j∈[s] Aj ∣∕δ). Moreover, the weights coming into each node in
the hidden layer are in {0,1} with at most t ∙ (Pj∈[s] |Aj|) non-zeros overall. In particular, when
g is a degree-pP polynomial, we can set t = log(ps∕δ), and each hidden unit has at most pt non-zero
weights.
This is a setting where we get a significant advantage over proper learning. To our knowldege,
there is no analog of this result for Gaussian projections. Classical sketching approaches would
use a sketch of X叫 which is a kp-sparse vector over binary vectors of dimension dp. Known
sketching techniques such as Pham & Pagh (2013) would construct a sketch of size Ω(kp). Practical
techniques such as Vowpal Wabbit also construct cross features by explicitly building them and
have this exponential dependence. In stark contrast, neural networks allow us to get away with a
logarithmic dependence on p.
Using polynomial kernels. Theorems 4.1 has a corresponding variants where the neural net is
replaced by a polynomial of degree t. Similarly, the neural net in Theorem 5.1 can be replaced
by a degree-pt polynomial when the polynomial g has degree p. This implies that one can use a
polynomial kernel to get efficient learning.
Deterministic sketching. A natural question that arises is whether the parameters above can im-
proved. We show in App. C that if we allow large scalars in the sketches, one can construct a deter-
ministic (2k + 1)-dimensional sketch from which a shallow network can reconstruct any monomial.
We also show a lower bound of k on the required dimensionality.
Lower bound for proper learning. We can also show, see App. D, that if one does not expand
the hypothesis class, then even in the simplest of settings of linear classifiers over 1-sparse vec-
tors, the required dimensionality of the projection is much larger than the dimension needed for
improper learning. The result is likely folklore and thus we present a short proof in the appendix for
completeness using concrete constants in the theorem and its proof below.
Neural nets on Boolean inputs. We remark that for Boolean inputs (irrespective of sparsity), any
polynomial with s monomials can be represented by a neural network in Ns(Relu) using the con-
struction in Theorem 5.1.
6	Experiments with synthetic data
In this section, we evaluate sketches on synthetically generated datasets for the task of polynomial
regression. In all the experiments here, we assume input dimension d = 104, input sparsity k =
50, hypothesis support s = 300, and n = 2 × 105 examples. We assume that only a subset of
features I ⊆ [d] are relevant for the regression task, with |I| = 50. To generate an hypothesis,
we select s subsets of relevant features A1, . . . , As ⊂ I each of cardinality at most 3, and generate
the corresponding weight vector w by drawing corresponding s non-zero entries from the standard
Gaussian distribution. We generate binary feature vectors X ∈ Bd,k as a mixture of relevant and
other features. Concretely, for each example we draw 12 feature indices uniformly at random from
I, and the remaining indices from [d]. We generate target outputs as g(X) + z, where g(X) is in
the form of the polynomial given in Theorem 5.1, and z is additive Gaussian noise with standard
deviation 0.05. In all experiments, we train on 90% of the examples and evaluate mean squared
error on the rest.
We first examined the effect of the sketching parameters m (hash size) and t (number of hash func-
tions) on sparse linear regression error. We generated synthetic datasets as described above (with all
feature subsets in A having cardinality 1) and trained networks in Ns (Relu). The results are shown
in Figure 2 (left). As expected, increasing t leads to better performance. Using hash size m less
than the input sparsity k leads to poor results, while increasing hash size beyond ek (in this case,
ek u 136) for reasonable t yields only modest improvements.
We next examined the advantages of improper learning. We generated 10 sparse linear regression
datasets and trained linear models and networks in Ns (Relu) on original and sketched features with
5
Under review as a conference paper at ICLR 2017
0.25
0.20
o
0)
I 0-15
to
S-io
S
≡
0.05
0.00
* * linear 0-06
* * N11 (Helu)
03
-ioɪiapaænbs esɪ
■05
■°2
1 2	4	6	8	10	12	14	1 2	4	6	8	10 12 14 no sketch 1 2	4	6	8	10 12 14 no sketch
t	t	t
Figure 2: Left: effect of varying t, m for sketched 1-hidden layer network. Center: sparse linear regression on
sketched data with improper learning. Right: sparse polynomial regression on sketched data.
m = 200 and several values of t. The results are shown in Figure 2 (center). The neural net-
work yields notably better performance than a linear model. This suggests that linear classifiers are
not well-preserved after projections, as the Ω(1∕γ2) projection size required for linear separability
can be large. Applying a neural network to sketched data allows us to use smaller projections.
Table 1: Comparison of sketches and Gaussian
random projections on the sparse linear regression
task (top) and sparse polynomial regression task
(bottom). See text for details.
	1K		2K	3K
Gaussian		0.089	0.057	0.029
Sketch t =	1	0.087	0.049	0.031
Sketch t =	2	0.072	0.041	0.023
Sketch t =	6	0.041	0.033	0.022
Gaussian		0.043	0.037	0.034
Sketch t =	1	0.041	0.036	0.033
Sketch t =	2	0.036	0.027	0.024
Sketch t =	6	0.032	0.022	0.018
We repeated the previous experiment for 10 poly-
nomial regression datasets, generated with feature
subsets in A of cardinality 2 and 3. The results are
shown in Figure 2 (right). The linear model is a bad
fit, showing that g(x) is not well approximated by a
linear function. Neural networks applied to sketches
succeed in learning a small model and achieve sig-
nificantly lower error than a network applied to the
original features for t ≥ 6. This suggests that reduc-
ing the input size, and consequently the number of
model parameters, can lead to better generalization.
Note that previous work on hashing and projections
would imply using significantly larger sketch size for
this setting.
We also compared our sketches to Gaussian random projections. We generated sparse linear and
polynomial regression datasets with the same settings as before, and reduce the dimensionality of the
inputs to 1000, 2000 and 3000 using Gaussian random projections and sketches with t ∈ {1, 2, 6}.
We remark that in this comparison, the column headings correspond to the total sketch size mt.
Thus, e.g., when we take t = 6, m is correspondingly reduced. We report the squared error averaged
across examples and five datasets of one-layer neural networks in Table 1. The results demonstrate
that sketches with t > 1 yield lower error than Gaussian projections. Note also that Gaussian
projections are dense and hence much slower to train.
7 Experiments with language processing tasks
Linear and low degree sparse polynomials are often used for classification. Our results imply that if
we have linear or a sparse polynomial with classification accuracy 1 -ε over some set of examples
in Bd,k × {0, 1}, then neural networks constructed to compute the linear or polynomial function
attain accuracy of at least 1 -ε-δ over the same examples. Moreover, the number of parameters
in the new network is relatively small by enforcing sparsity or `1 bounds for the weights into the
hidden layers. We thus get generalization bounds with negligible degradation with respect to non-
sketched predictor. In this section, we evaluate sketches on the language processing classification
tasks described below.
6
Under review as a conference paper at ICLR 2017
0.901-
Non-zero parameters in 1st layer
Figure 3: Performance vs. number of nonzero parameters in 1st layer for Reuters (left), AG NeWs
(center), and type tagging (right). Each color corresponds to a different sketch size and markers in-
dicate the number of subsketches t. We evaluate each setting for three values of the l1 regularization
parameter λ1.
Entity Type Tagging. Entity type tagging is the task of assigning one or more labels (such as
person, location, organization, event) to mentions of entities in text. We perform type tagging on a
corpus of neW documents containing 110K mentions annotated With 88 labels (on average, 1.7 labels
per mention). Features for each mention include surrounding Words, syntactic and lexical patterns,
leading to a very large dictionary. Similarly to previous Work, We map each string feature to a 32 bit
integer, and then further reduce dimensionality using hashing or sketches. See Gillick et al. (2014)
for more details on features and labels for this task.
Reuters-news Topic Classification. The Reuters RCV1 data set consists of a collection of ap-
proximately 800,000 text articles, each of Which is assigned multiple labels. There are 4 high-level
categories: Economics, Commerce, Medical, and Government (ECAT, CCAT, MCAT, GCAT), and
multiple more specific categories. We focus on training binary classifiers for each of the four major
categories. The input features We use are binary unigram features. Post Word-stemming, We get
data of approximately 113,000 dimensions. The feature vectors are very sparse, hoWever, and most
examples have feWer than 120 non-zero features.
AG-news Topic Classification. We perform topic classification on 680K articles from AG neWs
corpus, labeled With one of 8 neWs categories: Business, Entertainment, Health, Sci/Tech, Sports,
Europe, U.S., World. For each document, We extract binary Word indicator features from the title
and description; in total, there are 210K unique features, and on average, 23 non-zero features per
document.
Experimental Setup. In all experiments, We use tWo-layer feed-forWard netWorks With ReLU ac-
tivations and 100 hidden units in each layer. We use a softmax output for multiclass classification
and multiple binary logistic outputs for multilabel tasks. We experimented With input sizes of 1000,
2000, 5000, and 10,000 and reduced the dimensionality of the original features using sketches With
t ∈ {1, 2, 4, 6, 8, 10, 12, 14} blocks. In addition, We experimented With netWorks trained on the
original features. We encouraged parameter sparsity in the first layer using '1 -norm regularization
and learn parameters using the proximal stochastic gradient method. As before, We trained on 90%
of the examples and evaluated on the remaining 10%. We report accuracy values for multiclass
classification, and F1 score for multilabel tasks, With true positive, false positive, and false negative
counts accumulated across all labels.
Results. Since one motivation for our Work is reducing the number of parameters in neural netWork
models, We plot the performance metrics versus the number of non-zero parameters in the first
layer of the netWork. The results are shoWn in Figure 3 for different sketching configurations and
settings of the 'ι-norm regularization parameters (λι). On the entity type tagging task, we compared
sketches to a single hash function of size 500,000 as the number of the original features is too
large. In this case, sketching allows us to both improve performance and reduce the number of
parameters. On the Reuters task, sketches achieve similar performance to the original features with
fewer parameters. On AG news, sketching results in more compact models at a modest drop in
accuracy. In almost all cases, multiple hash functions yield higher accuracy than a single hash
function for similar model size.
7
Under review as a conference paper at ICLR 2017
8 Conclusions
We have presented a simple sketching algorithm for sparse boolean inputs, which succeeds in sig-
nificantly reducing the dimensionality of inputs. A single-layer neural network on the sketch can
provably model any sparse linear or polynomial function of the original input. For k-sparse vec-
tors in {0,1}d, our sketch of size O(k log s∕δ) allows computing any S-SParse linear or polynomial
function on a 1 - δ fraction of the inputs. The hidden constants are small, and our sketch is sparsity
preserving. Previous work required sketches of size at least Ω(s) in the linear case and size at least
kp for preserving degree-p polynomials. Our results can be viewed as showing a compressed sens-
ing scheme for 0-1 vectors, where the decoding algorithm is a depth-1 neural network. Our scheme
requires O(k log d) measurements, and we leave open the question of whether this can be improved
to O(k log d) in a stable way. We demonstrated empirically that our sketches work well for both
linear and polynomial regression, and that using a neural network does improve over a direct linear
regression. We show that on real datasets, our methods lead to smaller models with similar or better
accuracy for multiclass and multilabel classification problems. In addition, the compact sketches
lead to fewer trainable parameters and faster training.
Acknowledgements
We would like to thank Amir Globerson for numerous fruitful discussion and help with an early
version of the manuscript.
References
Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. J.
Comput. Syst. Sci., 66(4):671-687, 2003.
N. Ailon and B. Chazelle. The fast JL transform and approximate nearest neighbors. SICOMP, 39(1), 2009.
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency moments.
J. Comput. Syst. Sci., 58(1):137-147, 1999.
Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projec-
tion. Machine Learning, 63(2):161-182, 2006.
J.	Ba and R. Caruana. Do deep nets really need to be deep? In NIPS, pp. 2654-2662, 2014.
K.	Do Ba, P. Indyk, E. Price, and D. Woodruff. Lower bounds for sparse recovery. In SODA, 2010.
A. Barron. Approximation and estimation bounds for artificial neural networks. Mach. Learning, 14(1), 1994.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. Information
Theory, IEEE Transactions on, 39(3):930-945, 1993.
Vladimir Braverman, Rafail Ostrovsky, and Yuval Rabani. Rademacher chaos, random Eulerian graphs and the
sparse Johnson-Lindenstrauss transform. CoRR, abs/1011.2590, 2010.
E.J. CandeS and T. Tao. Near optimal signal recovery from random projections: Universal encoding strategies?
IEEE Transactions on Information Theory, 52(12):5406-5425, 2006.
M. Charikar, K. Chen, and M. Farach. Finding frequent items in data streams. Theor. Comp. Sci., 312(1), 2004.
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing convolu-
tional neural networks. CoRR, abs/1506.04449, 2015. URL http://arxiv.org/abs/1506.04449.
Y. Cheng, F. Yu, r. Feris, S. Kumar, A. Choudhary, and S-F. Chang. An exploration of parameter redundancy
in deep networks with circulant projections. In CVPR, pp. 2857-2865, 2015.
K. Clarkson and D. Woodruff. Low rank approximation and regression in input sparsity time. In STOC, 2013.
M.D. Collins and P. Kohli. Memory bounded deep convolutional networks. CoRR, abs/1412.1442, 2014.
G. Cormode and S. Muthukrishnan. An improved data stream summary: the count-min sketch and its applica-
tions. J. Algorithms, 55(1):58-75, 2005a.
G. Cormode and S. Muthukrishnan. Summarizing and mining skewed data streams. In SDM, pp. 44-55, 2005b.
Anirban Dasgupta, Ravi Kumar, and TamaS Sarlos. A sparse Johnson-Lindenstrauss transform. In STOC, pp.
341-350. ACM, 2010.
8
Under review as a conference paper at ICLR 2017
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V Le, MarcAurelio Ranzato, Mark
Mao, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In
Advances in Neural Information Processing Systems, pp. 1223-1231, 2012.
Misha Denil, Babak Shakibi, Laurent Dinh, MarcAurelio Ranzato, and Nando de Freitas. Predicting parameters
in deep learning. In Advances in Neural Information Processing Systems, pp. 2148-2156, 2013.
David L Donoho. Compressed sensing. Information Theory, IEEE Transactions on, 52(4):1289-1306, 2006.
Kuzman Ganchev and Mark Dredze. Small statistical models by random feature mixing. In Workshop on
Mobile NLP at ACL, 2008.
D. Gillick, N. Lazic, K. Ganchev, J. Kirchner, and D. Huynh. Context-dependent fine-grained entity type
tagging. CoRR, abs/1412.1820, 2014. URL http://arxiv.org/abs/1412.1820.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural
network. In Advances in Neural Information Processing Systems, pp. 1135-1143, 2015.
Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon. In Advances in Neural Information
Processing Systems, volume 89, 1993.
G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath,
and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The shared views of
four research groups. Signal Processing Magazine, IEEE, 29(6):82-97, 2012.
G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. CoRR, 1503.02531, 2015.
W.B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemp. Math,
26, 1984.
Daniel M. Kane and Jelani Nelson. Sparser Johnson-Lindenstrauss transforms. J. ACM, 61(1):4:1-4:23, 2014.
Leonid Kontorovich. A universal kernel for learning regular languages. In MLG, 2007.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems, pp. 1097-1105, 2012.
J.	Langford, L. Li, and A. StrehL VoWPal Wabbit online learning project. http://hunch.net/~vw/, 2007.
Yann LeCun, John S Denker, Sara A Solla, Richard E Howard, and Lawrence D Jackel. Optimal brain damage.
In Advances in Neural Information Processing Systems, volume 89, 1989.
J.	Matousek. On variants of the Johnson-Lindenstrauss lemma. Rand. Struct. Algs., 33(2):142-156, 2008.
Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and Probabilistic
Analysis. Cambridge University Press, New York, NY, USA, 2005. ISBN 0521835402.
Ali Mousavi, Ankit B. Patel, and Richard G. Baraniuk. A deep learning approach to structured signal recovery.
arXiv:1508.04065, 2015.
Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. In KDD, pp.
239-247. ACM, 2013.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, pp. 1177-1184, 2007.
Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. Overfeat:
Integrated recognition, localization and detection using convolutional networks. arXiv:1312.6229, 2013.
Q. Shi, J. Petterson, G. Dror, J. Langford, A. J. Smola, A. Strehl, and V. Vishwanathan. Hash kernels. In
Artificial Intelligence and Statistics AISTATS’09, Florida, April 2009.
P.P. Talukdar and W.W. Cohen. Scaling graph-based semi supervised learning to large number of labels using
count-min sketch. In AISTATS, volume 33 of JMLR Proceedings, pp. 940-947. JMLR.org, 2014.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption
generator. CoRR, abs/1411.4555, 2014. URL http://arxiv.org/abs/1411.4555.
K.	Weinberger, A. Dasgupta, J. Attenberg, J. Langford, and A. J. Smola. Feature hashing for large scale
multitask learning. In International Conference on Machine Learning, 2009.
A	General sketches
The results of Section 3 and Section 4 extend naturally to positive and to general real-valued vectors.
We denote by
headk(x) =	arg min kx - yk1
y : kyk0 ≤ k
9
Under review as a conference paper at ICLR 2017
the closest vector to x whose support size is k and the residue tailk(x) = x - headk(x). Let Rd+,k,c
represent the set
Rd+,k,c = {x ∈ Rd+ : ktailk(x)k1 ≤ c},
and let Rd,k,c represent the set
Rd,k,c = {x ∈ Rd : ktailk(x)k1 ≤ c} .
For vectors in Rd+,k,c, we use the following sketching and decoding procedures analogous to the
binary case.
Sh1:t (x) = [y1, ..., yt] where yl = xi
i:h(i)=l
DhM1I:tNd=efmin(yhj(i)).
j∈[t]
Theorem A.1. Let x ∈ Rd+,k,c and let h1, . . . , ht be drawn uniformly and independently from a
pairwise independent distribution,for m = e(k + ε). Thenfor any i,
Pr DhM1I:tN(Sh1:t(x),i) 6∈ [xi,xi + εc] ≤ e-t.
Proof. Fix a vector x ∈ Rd+,k,c. To remind the reader, for a specific i and h, we have defined the set
of collision indices E(i) d=ef {i0 6= i : h(i0) = h(i)}. Note that DhMIN(Sh(x), i) = yh(i), and we can
rewrite yh(i) as yh(i) = xi + Pi0∈E(i) xi0 ≥ xi. We next show that
Pr DhMIN(Sh(x), i) > xi + εc ≤ 1/e .	(2)
We can rewrite
DhMIN(Sh(x),i)-xi =	X xi0	+ Xxi0 .
i0 ∈S (headk (x))∩E (i) i0 ∈S (tailk (x))∩E (i)
By definition of tailk(∙), the expectation of the second term is c/m. Using Markov,s inequality, We
get that
Pr[	^X Xi，	> εc] ≤ —ɪ-.
i0 ∈S (tailk (x))∩E (i)
To bound the first term, note that
Pr	£ 如 = 0	≤ Pr	£ 1 = 0
i0 ∈S (headk (x))∩E (i)	i0 ∈S (headk (x))∩E (i)
≤ E	X 1	≤-.
m
i0 ∈S (headk (x))∩E (i)
Recall that m = e(k + 1∕ε), then, using the union bound establishes (2). The rest of the proof is
identical to Theorem 3.1.	□
Corollary A.2. Let W ∈ Hd,s and X ∈ R+k C. For t = log(s∕δ), and m = e(k + ε), if hι,...,h
are drawn uniformly and independently from a pairwise independent distribution, then
PrI Ewi∙ DMIN(ShI:t (X),i)- w>xl ≥ εckwk1 ≤ δ.
The proof in the case that X may not be non-negative is only slightly more complicated. Let us
define the folloWing additional decoding procedure,
DhM1E:tD(Y,i)d=efMedianYhj(i),j.
j∈[t]
Theorem A.	3. Let X ∈ Rd,k,c, and let h1, . . . , ht be drawn uniformly and independently from a
pairwise independent distribution,for m = 4e2(k + 2∕ε). Thenfor any i,
Pr DhM1E:tD(Sh1:t(X),i) 6∈ [xi - εc, xi + εc] ≤ e-t .
10
Under review as a conference paper at ICLR 2017
Proof. As before, fix a vector x ∈ Rd,k,c and a specific i and h. We once again write
DhMED(Sh(x),i)-xi =	X xi0	+ Xxi0	.
i0 ∈S (headk (x))∩E (i)	i0 ∈S (tailk (x))∩E (i)
By the same argument as the proof of Theorem A.1, the first term is nonzero with probability k/m.
The second term has expectation in [-c/s, c/s]. Once again by Markov’s inequality,
Pr	xi0 > εc
≤ —and Pr
i0 ∈S (tailk (x))∩E (i)
xi0 < -εc
i0 ∈S (tailk (x))∩E (i)
≤ ɪ.
εm
Recalling that m = 4e2(k + 2∕ε),a union bound establishes that for any j,
Pr[DMED(Shj (x),i)-xi∣ >εc] ≤ 4^.
Let Xj be indicator for the event that |DhMED(Shj (x), i) - xi| > εc. Thus X1, . . . , Xt are binomial
random variables with Pr [Xj = 1] ≤ *.Then by Chernoff's bounds
Pr DhM1E:tD(Sh1:t(x),i) 6∈ [xi - εc, xi + εc]
≤ Pr	Xj > t/2
j
≤ exp
lnj1ln
1∕4e2 + 2
≤ exp
ln 2e2 + ɪ ln ɪ
2	2
E) O
≤ exp(-t) .
□
Corollary A.4. Let W ∈ Hd,s and X ∈ Rd,k,c,. For t = log(s∕δ), and m = 4e2(k + ε), if
h1 , . . . , ht are drawn uniformly and independently from a pairwise independent distribution, then
Pr	wiDhM1E:tD(Sh1:t(x),i) - W>x ≥ εckWk1 ≤ δ.
i
To implement DMIN(∙) using hidden units in a neural network, we need to use a slightly non-
conventional non-linearity. For a weight vector z and input vector x, a min gate implements
mini:zi 6=0 zixi . Then, using Corollary A.2, we get the following theorem.
Theorem A.5. For every W ∈ Hd,s there exists a set of weights for a network N ∈ Ns(min) such
that for each x ∈ Rd+,k,c the following holds,
P rh1:t |N (Sh1:t (x)) - W>x| ≥ εckWk1 ≤ δ.
as long as m = e(k + ∣) and t = log(s∕δ). Moreover, the weights coming into each node in the
hidden layer are binary with t non-zeros.
For real vectors x, the non-linearity needs to implement a median. Nonetheless, an analogous result
still holds.
B	Gaussian projections
In this section we describe and analyze a simple decoding algorithm for Gaussian projections.
Theorem B.	1. Let x ∈ Rd, and let G be a random Gaussian matrix in Rd0×d. Then for any i, there
exists a linear function fi such that
EG [(fi(GX)-Xi)2] ≤ kx -d0ieik2
11
Under review as a conference paper at ICLR 2017
Proof. Recall that G ∈ Rd ×d is a random Gaussian matrix where each entry is chosen i.i.d. from
N(0, 1/d0). For any i, conditioned on Gji = gji, we have that the random variable Yj |Gji = gji is
distributed according to the following distribution,
(Yj | Gji = gji ) 〜gjixi +): GjiOxi0
i0 6=i
〜gjixi + N(0, ∣∣χ - xieik2∕d0).
Consider a linear estimator for xi :
ʌ def Σj αji(yj/gji)
xi -	钎K
j αji
for some non-negative aj∙i's. It is easy to verify that for any vector of a's, the expectation of xi,
when taken over the random choices of Gji0 for i0 = i, is xi. Moreover, the variance of xi is
∣∣x - xieik2 Pj(aji/gji)t2
d	(Pj aji)2 .
Minimizing the variance of xi w.r.t aj∙i's gives us a* a gj%. Indeed, the partial derivatives are,
∂ (Pj (aji∕gji)2-λ Pj aji) =2aIf
daji	gji
which is zero at aj-i = λgj"2. This choice of aj-i,s translates to
E[(xi - xi)2]
∣∣x - xiei∣2	1
-d- j,
which in expectation, now taken over the choices of gji,s, is atmost ∣x - xi ei∣2∕d0. Thus, the claim
follows.	□
For comparison, if X ∈ Bd,k, then the expected error in estimating xi is y k-1, so that taking
d = (k - 1) log 1 ∕ε2 suffices to get a error ε estimate of any fixed bit with probablity 1-δ. Setting
ε = 1, we can recover xi with probability 1 - δ for X ∈ Bd,k with d = 4(k - 1) log 1. This
implies Theorem B.2. However, note that the decoding layer is now densely connected to the input
layer. Moreover, for a k-sparse vectors X that are is necessarily binary, the error grows with the
2-norm of the vector X, and can be arbitrarily larger than that for the sparse sketch. Note that G x
still contains sufficient information to recover X with error depending only on ∣X - headk(X)∣2. To
our knowledge, all the known decoders are adaptive algorithms, and we leave open the question of
whether bounds depending on the 2-norm of the residual (X - headk(X)) are achievable by neural
networks of small depth and complexity.
Theorem B.2. For every w ∈ Hd,s there exists a set of weights for a network N ∈ Ns(Relu) such
that for each X ∈ Bd,k,
P rh1:t [N (Gx)) = w>x] ≥ 1 - δ,
as long as G is a random m × d Gaussian matrix, with m ≥ 4k log(s∕δ).
C Deterministic S ketching
First we show that if we allow large scalars in the sketches, we can construct a deterministic (2k+1)-
dimensional sketch from which a shallow network can reconstruct any monomial.
We will also show a lower bound of k on the required dimensionality.
For every X ∈ Bd,k define a degree 2k univariate real polynomial by,
px(z) = 1 - (k + 1) Y (z - i)2.
{i|xi=1}
It is easy to verify that this construction satifies the following.
12
Under review as a conference paper at ICLR 2017
Claim C.1. Suppose that X ∈ Bd,k, and letPχ(∙) be defined as above. If Xj = 1, then Px(j) = Lf
xj = 0, then px(j) ≤ -k.
Let the coeffients of px (z) be ai(X) so that
2k
px(z) d=ef X ai(X)zi .
i=0
Define the deterministic sketch DSkd,k : Bd,k → R2k+1 as follows,
DSkd,k (X) = (ax,0, . . . , ax,2k)	(3)
For a non-empty subset A ⊂ [d] and y ∈ R2k+1 define
2k
XXyiji
DecPolyd,k(y, A) = j£A i=0----.	(4)
|A|
Theorem C.2. For every X ∈ Bd,k and a non-empty set A ⊂ [d] we have
Yxj = Relu(DecPolyd,k(DSkd,k(X), A)).
j∈A
Proof. We have that
2k
X X ax,iji
DecPOIyd,k (DSkd,k (x),A) =	j,Ai=0∣——
|A|
_	Pj∈A Px Cj)
=~|A|~.
In words, the decoding is the average value ofpx(j) over the indices j ∈ A. Now first suppose that
Qj∈A xj = 1. Then for each j ∈ A we have xj = 1 so that by Claim C.1, px(j) = 1. Thus the
average DecP olyd,k(DSkd,k(x), A) = 1.
On the other hand, if Ilj∈a Xj = 0 then for some j ∈ A, say j*, Xj* = 0. In this case, Claim C.1
implies that px(j*) ≤ -k. For every other j, px(j) ≤ 1, and each Px(j) is non-negative only when
Xj = 1, which happens for at most k indices j. Thus the sum over non-negative px(j) can be no
larger than k. Adding px(j*) gives us zero, and any additional j's can only further reduce the sum.
Thus the average in non-positive and hence the Relu is zero, as claimed.	□
The last theorem shows that Bd,k can be sketched in Rq, where q = 2k + 1, such that arbitrary
products of variables be decoded by applying a linear function followed by a ReLU. It is natural to
ask what is the smallest dimension q for which such a sketch exists. The following theorem shows
that q must be at least k. In fact, this is true even ifwe only require to decode single variables.
Theorem C.3. Let Sk : Bd,k → Rq be a mapping such that for every i ∈ [d] there is wi ∈ Rq
satisfying Xi = Relu(hwi, Sk(x)i)for each x ∈ Bd,k, then q is at least k.
Proof. Denote X = {w1, . . . , wd} and let H ⊂ {0, 1}X be the function class consisting of all
functions of the form hx(wi) = sign(hwi, Sk(x)i) for x ∈ Bd,k. On one hand, H is a sub-class of
the class of linear separators over X ⊂ Rq, hence VC(H) ≤ q. On the other hand, we claim that
VC(H) ≥ k which establishes the proof. In order to prove the claim it suffices to show that the set
A = {w1, . . . , wk} is shattered. Let B ⊂ A let x be the indicator vector of B. We claim that the
restriction of hx to A is the indicator function of B . Indeed, we have that,
hx(wi) = sign(hwi, Sk(x)i)
= sign(Relu(hwi, Sk(x)i))
= Xi
=l[i ∈ B]
□
13
Under review as a conference paper at ICLR 2017
We would like to note that both the endcoding and the decoding of determinitic sketched can be
computed efficiently, and the dimension of the sketch is smaller than the dimension of a random
sketch. We get the following corollaries.
Corollary C.4. For every w ∈ Hd,s there exists a set of weights for a network N ∈ Ns(Relu) such
that for each x ∈ Bd,k, N (DSkd,k (x)) = w>x.
Corollary C.5. Given w ∈ Rs, and sets A1 , . . . , As ⊆ [d], let g : {0, 1}d → R denote the
polynomial function
ss
g(x) =	wj	xi =	wj	xi .
j=1	i∈Aj	j=1	i∈Aj
For any such g, there exists a set of weights for a network N ∈ Ns (Relu) such that for each
x ∈ Bd,k, N (DSkh1:t (x)) = g(x).
Known lower bounds for compressed sensing Ba et al. (2010) imply that any linear sketch has size
at least Ω(k log d) to allow stable recovery. We leave open the question of whether one can get the
compactness and decoding properties of our (non-linear) sketch while ensuring stability.
D Lower B ound for Proper Learning
We now show that if one does not expand the hypothesis class, then even in the simplest of settings of
linear classifiers over 1-sparse vectors, the required dimensionality of the projection is much larger
than the dimension needed for improper learning. As stated earlier, the result is likely folklore and
we present a proof for completeness.
Theorem D.1. Suppose that there exists a distribution over maps φ : Bd,1 → Rq and ψ : Bd,s →
Rq such that for any x ∈ Bd,1, w ∈ Bd,s,
Pr Sgn (w>X — J) = Sgn (ψ(w)>Φ(x)) ≥	,
where the probability is taken over sampling φ and ψ from the distribuion. Then q is Ω(s).
Proof. If the error is zero, a lower bound on q would follow from standard VC dimension arguments.
Concretely, the hypothesis class consisting of hw(x) = {ei>x : wi = 1} for all w ∈ Bd,s shatters
the set {eι,...,e§}. If sgn(ψ(w)>φ(ei)) = Sgn(WTX - ɪ) for each W and ei, then the points
φ(ei), i ∈ [s] are shattered by hψ(w)(∙) where W ∈ Bd,s, which is a subclass of linear separators in
Rq . Since linear separators in Rq have VC dimension q, the largest shattered set is no larger, and
thus q ≥ S.
To handle errors, we will use the Sauer-Shelah lemma and show that the set {φ(ei) : i ∈ [S]} has
many partitions. To do so, sample φ, ψ from the distribution promised above and consider the set of
points A = {φ(e1), φ(e2 ), . . . , φ(es)}. Let W = {W1, W2 , . . . , Wκ} be a set of κ vectors in Bd,s
such that,
(a)	S(Wi) ⊆ [S]
(b)	Distance property holds: |S (Wi) 4 S (Wj )| ≥ 4 for i = j.
Such a collection of vectors, with κ = 2cs for a positive constant c, can be shown to exist by a
probabilistic argument or by standard constructions in coding theory. Let H = {hψ(w) : W ∈ W}
be the linear separators defined by ψ(Wi) for i ∈ W. For brevity, we denote hψ(wj) by hj. We will
argue that H induces many different subsets of A.
Let Aj = {y ∈ A : hj(x) = 1} = {x ∈ A : Sgn(ψ(Wj)>x) = 1}. Let Ej ⊆ A be the positions
where the embeddings φ, ψ fail, that is,
Ej = {Φ(ei): i ∈ [s],sgn(W>ei - 2) = sgn(ψ(Wj)>φ(ei)}.
14
Under review as a conference paper at ICLR 2017
Thus Aj = S(Wj) 4 Ej. By assumption, E[∣Ej |] ≤ 击 for each j, where the expectation is taken
over the choice of φ, ψ. Thus Pj E[|Ej|] ≤ sκ∕l0. Renumber the hj's in increasing order of |Ej|
so that |Ei| ≤ ∣E2∣ ≤ ... ≤ ∣Eκ∣. Due to the Ej's being non-empty, not all Aj's are necessarily
distinct. Call aj ∈ [κ] lost if Aj = Aj0 for some j0 ≤ j.
By definition, Aj = S(Wj )4Ej. If Aj = Ajv, then the distance property implies that Ej 4Ej-o ≥ S.
Since the Ej's are increasing in size, it follows that for any lost j, |Ej| ≥ S. Thus in expectation at
most 4κ∕5 of the j's are lost. It follows that there is a choice of φ, ψ in the distribution for which
H induces κ∕5 distinct subsets of A. Since the VC dimension of H is at most q, the Sauer-Shelah
lemma says that
This implies that q ≥ c0 s for some absolute constant c0 .
□
Note that for the setting of the above example, once we scale the Wj's to be unit vectors, the margin
is Θ(√1s). Standard results then imply that projecting to γ⅛ = Θ(s) dimension suffices, so the above
bound is tight.
For this setting, Theorem 4.1 implies that a sketch of size O(log(s∕δ)) suffices to correctly classify
1 -δ fraction of the examples if one allows improper learning as we do.
E Neural Nets on Boolean Inputs
In this short section show that for boolean inputs (irrespective of sparsity), any polynomial with s
monomials can be represented by a neural network with one hidden layer of s hidden units. Our
result is a simple improvement of Barron's theorem (1993; 1994), for the special case of sparse
polynomial functions on 0-1 vectors. In contrast, Barron's theorem, which works for arbitrary inputs,
would require a neural network of size d ∙ S ∙ PO(P) to learn an s-sparse degree-p polynomial. The
proof of the improvement is elementary and provided for completeness.
Theorem E.1. Let x ∈ {0, 1}d, and let g : {0, 1}d → R denote the polynomial function
SS
g(x) =	wj	xi = wj xi .
j=1	i∈Aj	j=1	i∈Aj
Then there exists a set of weights for a network N ∈ NS(Relu) such that for each x ∈ {0, 1}d,
N(x) = g(x). Moreover, the weights coming into each node in the hidden layer are in {0, 1}.
Proof. The jth hidden unit implements hj = Qi∈A xi. As before, for boolean inputs, one can
compute hj as Relu(Pi∈A xi - |Aj | + 1). The output node computes Pj wjhj where hj is the
output of j th hidden unit.	□
15