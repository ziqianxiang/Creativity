Under review as a conference paper at ICLR 2017
Perception Updating Networks: On architec-
TURAL CONSTRAINTS FOR INTERPRETABLE VIDEO
GENERATIVE MODELS
Eder Santana & Jose C Principe *
Department of Electrical and Computer Engineering
University of Florida
Gainesvillle, FL 32611, USA
edersantana@ufl.edu, principe@cnel.ufl.edu
Ab stract
We investigate a neural network architecture and statistical framework that mod-
els frames in videos using principles inspired by computer graphics pipelines. The
proposed model explicitly represents “sprites” or its percepts inferred from maxi-
mum likelihood of the scene and infers its movement independently of its content.
We impose architectural constraints that forces resulting architecture to behave as
a recurrent what-where prediction network.
1 Introduction
The current computer graphics pipelines are the result of efficient implementations required by lim-
ited hardware and high frequency output requirements. These requirements were also achieved with
the use of explicit physics and optic constraints and modeling with constantly improving data struc-
tures (Shirley et al., 2015).
In machine learning on the other hand, for a long time image (Olshausen et al., 1996) and video
(HUrri & Hyvarinen, 2003) generative models had been investigated with statistical approaches that
model images down to the pixel level (Simoncelli & Olshausen, 2001), sometimes assuming neigh-
borhood statistical dependencies (Osindero & Hinton, 2008). In video prediction, the cUrrent state
of the art Uses variations of deep convolUtional recUrrent neUral networks (Kalchbrenner et al., 2016)
(Lotter et al., 2016) (Finn et al., 2016).
As a parallel to the classic machine learning approach to image and video interpretation and pre-
diction is a growing trend in the deep learning literatUre for modeling vision as inverse graphics
(KUlkarni et al., 2015)(Rezende et al., 2016)(Eslami et al., 2016). These approaches can be inter-
preted into two groUps: sUpervised and UnsUpervised vision as inverse graphics. The sUpervised
approach assUmes that dUring training an image is provided with extra information aboUt its rota-
tion, translation, illUmination, etc. The goal of the sUpervised model is to learn an aUto-encoder that
explicitly factors oUt the content of the image and its physical properties. The sUpervised approach
is illUstrated by KUlkarni et al. (2015).
The UnsUpervised approach reqUires extra architectUral constraints, similar to those assUmed in com-
pUter graphics. For example, Reed et al. (2016) modeled the content of a scene with a Generative
Adversarial Network (Goodfellow et al., 2014) and its location with Spatial Transformer Networks
(Jaderberg et al., 2015). The fUll model is adapted end-to-end to generate images whose appear-
ance can be changed by independently modifying the ”what” and/or ”where” variables. A similar
approach was applied to video generation with volUmetric convolUtional neUral networks (Vondrick
et al., 2016).In two papers by Google DeepMind (Rezende et al., 2016) (Eslami et al., 2016) they
improved the ”where” representations of the UnsUpervised approach and modeled the 3D geometry
of the scene. This way they explicitly represented object rotation, translation, camera pose, etc.
Their approaches were also trained end-to-end with REINFORCE-like stochastic gradients to back-
propagate throUgh non-differentiable parts of the graphics pipeline (Rezende et al., 2016) or to coUnt
* Companion code repo coming soon.
1
Under review as a conference paper at ICLR 2017
δXY
10	20	30	40	50	∞
convolution result
10	20	30	40	50	∞
10	20	30	40	50	60
spatial transformer result
Figure 1: How to get similar results using convolutions with delta-functions and spatial transformers.
Input sprite is 8 × 8 pixels and the outputs are 64 × 64 pixels. Note that in the convolution the result
shape is rotated 180 degrees and its center is where the delta equals to one at pixel (x = 16, y = 16).
Note also that the edges of the spatial transformer results are blurred due to bilinear interpolation. A
matrix can be read as “zoom-out” 8 times and translate up and left in a quarter of the resulting size.
the number of objects in the scene (Eslami et al., 2016). Those papers also used Spatial Transformer
Networks to model the position of the objects in the scene, but they extended it to 3D geometry so
it could also model rotation and translation in a volumetric space.
Other approaches inspired by the graphics pipeline and computer vision geometry in machine learn-
ing uses the physics constraints to estimate the depth of each pixel in the scene and camera pose
movements to predict frames in video (Mahjourian et al., 2016) (Godard et al., 2016).
The present paper is closer to the unsupervised approach of vision as inverse graphics. More pre-
cisely, here we investigate frame prediction in video. Contrary to the work by Reed et al. (2016) here
we first limit ourselves to simple synthetic 2D datasets and learning models whose representations
can be visually interpreted. This way we can investigate exactly what the neural network is learning
and validate our statistical assumptions. Also, we investigate the behavior of Spatial Transformer
Networks and question it as the default choice when limited compute resources are available and no
scale invariance is required.
First in the next Section we will pose a statistical model that is appropriate for machine learning but
inspired by the graphics pipeline.
2 A 2D statistical graphics pipeline
This section starts with a high level description of the 2D graphics pipeline, followed by a discussion
of how to implement it with neural network modules, and finally we define a formal statistical model.
The 2D graphics pipeline starts from geometric primitives and follows with modeling transforma-
tions, clipping, viewing transformations and finally scan conversion for generating an image. Here,
we will deal with previously rasterized bitmaps, i.e. sprites, and will model the translation transfor-
mations, rotation and clipping with differential operations. This way, the steps in the pipeline can
be defined as layers of a neural network and the free parameters can be optimized with backpropa-
gation.
2
Under review as a conference paper at ICLR 2017
For our neural network implementation, we assume a finite set of sprites (later we generalize it to
infinite sprites) that will be part of the frames in the video. The image generation network selects a
sprite, s, from a memorized sprite database Si∈{1,...,K} using an addressing signal c:
s =	cj Sj , where
j
cj = 1.
j
(1)
For interpretable results it would be optimal to do one-hot memory addressing where cj = 1 for
Sj = S and cj = 0 otherwise. Note that (1) is differentiable w.r.t to both cj and Sj so we can learn
the individual sprites from data. We can for cj to sum to 1 using the softmax nonlinearity. This
approach was inspired by the recent deep learning literature on attention modules (Bahdanau et al.,
2014) (Graves et al., 2014).
When the number of possible sprites is too large it is more efficient to do a compressed represen-
tation. Instead of using an address value c we use a content addressable memory where the image
generator estimates a code z that is then decoded to the desired sprite with a (possibly nonlinear)
function d(z). If we interpret the addressing value z as a latent representation and the content
addressable memory d(z) as a decoder, we can use the recent advances in neural networks for gen-
erative models to setup our statistical model. We will revisit this later in this section.
The translation transformation can be modeled with a convolution with a Delta function or using
spatial transformers. Note that the translation of an image I(x, y) can be defined as
I(X - Tx,y - Ty) = I(x,y) ?δ(x - TxJy - Ty),	(2)
where ? denotes the image convolution operation. Clipping is naturally handled in such a case. If the
output images have finite dimensions and δ(x -Tx, y -Ty) is non-zero near its border, the translated
image I(x - Tx, y - Ty) will be clipped. Another way of implementing the translation operation is
using Spatial Transformer Networks (STN) (Jaderberg et al., 2015). An implementation of STN can
be defined in two steps: resampling and bilinear interpolation. Resampling is defined by moving the
position of the pixels (x, y) in the original image using a linear transform to new positions (X, y) as
A	A11	A12 A13
A21	A22 A23
(3)
We assume the coordinates in the original image are integers 0 ≤ X < M and 0 ≤ y < N, where
M × N is the size of the image I. Once the new coordinates are defined, we can calculate the values
of the pixels in the new image I using bilinear interpolation:
K~~∖	Tf	∖ .	Tf	∖ .
I(x,y) = Wxi ,yι I(xi,yi) + Wxi ,y2I(xi,y2) +
wx2,y1I(X2,y1) +wx2,y2I(X2,y2)
(4)
where (xι, X2,y1,y2) are integers, xι ≤ X <x2, yι ≤ y <y and
wx1,y1 = (bxc - x)(byc - X)
wx1,y2 = (bxc - x)(byc +1 - y)
wx2,y1 = (bxc + 1 - x)(byc - y)
wx2,y2 = (bxc - x)(byc + 1 - y)
(5)
3
Under review as a conference paper at ICLR 2017
To avoid sampling from outside the image We clip the values [X[ and [X[ + 1 between 0 and M and
the values [y] and [y[ + 1 between 0 and N. We omitted that in (5) for conciseness. Note that (4)
is piecewise differentiable w.r.t I.
We can define translation through operations with
1 0 τx
A = 0 1 τx .	(6)
0 1 τy
Also, we can rotate the image ρ radians counter clockwise with
A
cos ρ sin ρ 0
- sin ρ cosρ 0
(7)
Image rescaling is achieved on that framework by rescaling in the right square submatrix A1:2,1:2 .
We illustrate in Fig. 1 how to get similar results using convolutions with a delta-function and spatial
transformers.
Considering the tools defined above, we can define a statistical model of 2D images the explicitly
represents sprites and their positions in the scene. We can use the free energy of this statistical model
to optimize a neural network. Let us start with a static single frame model and later generalize it to
video.
Let an image I 〜 pθ(I) be composed of sprite S 〜 pθ(S) centered in the (x, y) coordinates in
the larger image I. Denote these coordinates as a random variable δχy 〜pθ, where θ are the
model parameters. pθ (δxy) can be factored in two marginal categorical distributions Cat(δx) and
Cat(δy) that models the probability of each coordinate of the sprite independently. For the finite
sprite dataset, pθ(S) is also a categorical distribution conditioned on the true sprites. For this finite
case the generative model can be factored as
Pθ(I, s, δ) = pθ(s)pθ(δχy)p(I|s, δχy),
(8)
assuming that “what”, S, and “where”, δxy, are statistically independent. Also, in such case the
posterior
Pθ(s, δlI) = Pθ(SII)p(δxy |I)	(9)
is tractable. One could use for instance Expectation-Maximization or greedy approaches like Match-
ing Pursuit to alternate between the search for the position and fitting the best matching shape. For
the infinite number of sprites case, we assume that there is a hidden variable z from which the sprites
are generated as p(s,z) = pθ(z)pθ(s∣z). In such case our full posterior becomes
Pθ(z, s, δlI) = Pθ(z, s|I)p(δxy |I) =
Pθ(ZII )Pθ (slI,z)P(δxy |I).
(10)
We can simplify (10) assuming pθ(zIS) = pθ (zII) for simple images without ambiguity and no
sprite occlusion. For a scalable inference in the case of unknown θ and z and intractable pθ(zIS)
we can use the auto-encoding variational Bayes (VAE) approach proposed by Kingma & Welling
(2013). Using VAE we define an approximate recognition model qφ(zIS). In such case, the log-
likelihood of the i.i.d images I is logpθ(I1, . . . , IT) = PiT logpθ(Ii) and
logpθ(Ii) = DKL(qφ(zISi)IIpθ(zISi))+
DKL(pθ(zISi)IIpθ(zIIi))+
L(θ,φ,δxy,Ii).
(11)
Again, assume that the approximation pθ(zIS) = pθ(zII) we have DKL(pθ(zISi)IIpθ(zIIi)) = 0
and the free energy (or variational lower bound) term equal to
L(θ, φ, δ, I) = -DKL(qφ(zISi)IIpθ(z))+
Eqφ(z∣s,δ)pθ (δ∣I) [log pθ (I|z, δ)],
(12)
4
Under review as a conference paper at ICLR 2017
Figure 2: A schematic block diagram for a Perception Updating Network. This configuration uses
both convolutions with delta functions for translation and spatial transformers for rotation. It also
shows the optional background underlay.
where we dropped the subindices xy and i to avoid clutter. Here we would like to train our model by
maximizing the lower bound (12), again inspired by VAE. We can do so using the reparametrization
trick assuming qφ(z∖s) and the prior pθ(Z) to be Gaussian and sampling
Z = mφ(I) + vφ(I) ∙ ξ,	(13)
where ξ 〜N(0, σI), I is the identity matrix, the functions m(I) and V(I) are deep neural networks
learned from data.
One can argue that given Z and a good approximation to the posterior qφ, estimating δ is still
tractable. Nevertheless, we preemptively avoid Expectation-Maximization or other search ap-
proaches and use instead neural network layers lx and ly :
δχy = Softmax(lχ(I)) 0 Softmax(ly(I)),	(14)
with 0 denoting the outer product of marginals. We also experiment using STNs. Such amortized
inference is also faster in training and test time than EM and will also cover the case where I is
itself a learned low dimensional or latent representation instead ofan observable image. Bear this in
mind while we use this approach even in simple experiments such as those with moving shapes in
the Experiments Section. This will help us to understand what can be learned from this model.
We extend the model above to videos, i.e. sequences of images I(t) = {I (0), I (1), . . .}, assuming
that the conditional log-likelihood logpθ(It∖HIt) = logpθ (It ∖Hδt, Hzt) follows (11), where HIt
is the history of video frames prior to time point t. Also Hδt and Hzt are the history of position
coordinates and the history of latent variables of the sprites respectively. We should observe that one
can make the assumption that the sprites don’t change for a given video I(t) and only estimate one
sprite st=0 or hidden variable Zt=0 . This assumption can be useful for long term predictions, but
requires that the main object moving in the scene doesn’t change.
In the next section, we propose a neural network architecture for maximizing our approximate vari-
ational lower bound 2D videos.
3	Perception Updating Networks
This Section proposes a family of neural architectures for optimizing the lower bound (12). A
schematic diagram is represented in Fig. (2). The core of our method is a Recurrent Neural Network
(RNN) augmented with task specific modules, namely a sprite addressable memory and modeling
transformations layers. RNNs augmented with task specific units were popularized by Graves et al.
(2014) in the context of learning simple differentiable algorithms and served as inspiration for us as
well. Here since we explicitly model the perceived sprites as s or Z and update it and its location
and/or rotation though time we decided to call our method simply Perception Updating Networks.
Here an input frame at time t, It , is fed to the RNN that emits 2 signals: a memory address that
selects a relevant sprite and transformation parameters. If we are doing the translation transformation
using convolutions and delta functions this output is equal to (14). If using STN, the translation
operation returns the matrix A used in (3). Note that we could use both, letting convolutions with
δ to the translation is constraining A as in (7) to do rotation transformations only. We describe the
general case where both δxy and STNs are used in Algorithm 1.
5
Under review as a conference paper at ICLR 2017
Beyond deciding between STNs vs δxy , a few other free parameters of our method are the type of
RNN (e.g. vanilla RNN, LSTM, GRU, ConvRNN, etc), the number of neurons in the hidden state of
the RNN and neural network architectures that infer the correct sprite and modeling transformation
parameters. Our hyperparameter choices are investigated separately in each experiment in the next
Section.
Data: input Videos It, t ∈ {0,1, 2,...}, initial RNN State h°, neural network Iayers m@, v@, d,l, f
Result: video predictions It, t ∈ {1, 2, 3, . . .}
for t ∈ {0, 1, . . .} do
ht J RNN(It,ht-ι)
δχy = softmax(lχ(ht)) 0 softmax(ly(ht))
ρ = f(ht)
A cos ρ sin ρ 0
= - sin ρ cos ρ 0
ξ 〜Pθ (Z)
Zt = mφ(ht) + vφ(ht) ∙ ξ
st = d(zt)
at =STN(st,A)
It+1 = at ? δxy
It+ι = μIt+ι + (1 - μ)B
end
Algorithm 1: Perception Updating Networks. STN denotes spatial transformer operator (3)-(4) and
? denotes convolution. We experimented with several variations of this algorithm, mainly changing
if and how the “where” modules δxy and STN are used. Also changing how the sprite st is calculated
and not using a background B when not necessary.
In the next section we present experiments with the proposed architecture on synthetic datasets.
4	Experiments
In this section we experiment with several implementations of the proposed Perception Updating
Networks. We start with a simple synthetic dataset made of videos where one of 3 moving shapes
moves with constant speed bouncing in the edges of an image. This illustrates the working of the
finite memory and the addressing scheme in (1). Afterwards we show results on the moving MNIST
dataset (Srivastava et al., 2015) commonly used in the literature of generative neural network models
of videos.
4.1	B ouncing Shapes
In this first experiment we generate videos of one of three shapes moving on a non-zero background.
The shapes are a square, triangle and cross. The image size is 20 × 20 pixels and the shapes are 8 × 8
pixels. The pixel values are between 0 and 1. The shapes are picked with equal probability and they
move at constant speed of 1 pixel per frame. The shapes start from random initial positions with and
start moving in random directions as well.
We tested two implementations of the proposed architecture: one using only convolutions, referred
to as convolutional PUN in the figures, and another using using spatial transformers, called spatial
transformer PUN. For the parameters of the convolutional PUN the RNN used was a Long Short
Term Memory (LSTM) with 100 cells. The RNN in the Spatial Transformer PUN had 256 cells. In
the convolutional PUN, the location layers used to calculate δxy, lx and ly, output vectors of size 20
pixels and we used the finite addressable memory described in (1). The background is also learned
from data as weights of neural network. This background served to make the task more difficult and
force the network to avoid just exploiting any non-zero value. After the convolutional composition
It = St ? δχy, we added the background to form a new image using It = μ ∙ It + (1 — μ)B, where
μ is a differentiable mask that accounts for the “transparency” of the image It. B is the learned
20 × 20 pixels background image. For complex shapes this mask shape could be calculated as
another module in the network, similarly to the approach in Vondrick et al. (2016).
6
Under review as a conference paper at ICLR 2017
a)	one step ahead prediction
b)	convolutional PUN learned sprites
10x10 sprites
nr □
)le δχy when sprites 10
6x6 sprites
sample δχy when sprites are 6χ6
Figure 3: Results on the Bouncing Shapes dataset. Three 8x8 sprites (a square, a cross and a triangle)
were used to generate videos. The shapes move in a 20x20 pixels canvas with a Toeplitz background
and bounce on the corners. a) We show one step ahead predictions with the compared methods. b)
We also show the learned sprites for the convolutional implementation of the proposed Perception
Updating Networks when we over- and under-estimate the size of the desired sprites.
In the following experiments, the training videos were 10 frames long. At test time the network is
fed the first 10 frames of a video and asked to predict the next 10. Results for the compared methods
are shown in Fig. ??. For the baseline method, we did a hyperparameter search on conventional
LSTMs with a single linear output layer until we found one that had comparable results at test time.
That network had 256 hidden cells. Also, note that although the scale of the mean square error is
the same, the results from our proposed architecture look smoother than those learned by the LSTM
as shown in Fig. 3.
Given such a simple experiment, it is elucidating to visualize values learned by each piece of the
network. As expected the sprite memory learned the 3 investigated shapes in transposed order since
they are reverted by the convolution operation to compose the frame. We also experimented with
choosing the size of the learned sprites st smaller and larger than the true shapes. We observed that
for larger shapes such as 10 × 10 the sprites converge to the correct shapes but just using part of
the pixels. For smaller shapes such as 6 × 6 pixels, instead of learning a part of the correct shape,
the convolutional Perception Updating Network learned to compensate for the lack of enough pixels
with more than one non-zero value in the location operation δxy (see Fig. 3). This allow us to
suggest to the interested practitioner that in order to get interpretable results it is better to use sprites
larger than the expected size than smaller.
For the spatial transformer PUN the image is calculated as (see Algorithm 1 for context):
A= f(ht),
It+1 = STN(st, A).
(15)
We noticed that the spatial transformer PUN was not able to learn the training videos using an
equivalent architecture to the convolutional PUN one. We had to use multiple layers to define the
function f(ht). In other words, in the convolution based method δxy can be estimated by a single
affine transformation of the state ht but A cannot. We also had to use smaller learning rates to
7
Under review as a conference paper at ICLR 2017
Figure 4: Learning curves in the test task of two implementations of the proposed architecture (conv
PUN and STN PUN) and an equivalent LSTM baseline. Note that the spatial transformer based
PUN was not able to generalize to the test set, i.e. they did not work well for generating videos
when getting its own previous outputs as next step inputs.
Figure 5: Sample rollouts of a 2 layer LSTM convolutional Perception Updating Network with .
guarantee convergence: 0.0001 for STN while the δxy -based model worked with a value 10 times
larger.
If we don’t use the softmax nonlinearity to construct δxy the representations learned by the con-
volutional PUN are no longer visually interpretable. It is interesting to conclude that under this
framework the “what” and “where” can only be distinguished if we impose architectural constraints.
The reason is the commutative property of the convolution operation.
As a note on rotation, we ran experiments where the sprite are rotated by a random angle before
being placed in the image. This new type of videos cannot be learned using only convolutional
based Perception Updating Networks unless we increase the number of sprites proportionally to the
number of possible angles. Spatial transformer based Perception Updating Networks can handle this
new type of video naturally. Nevertheless, if the number of rotation angles is finite or can be dis-
cretized we found that we could learn to generate the videos faster if we combined the convolutional
approach with a mechanism to select the appropriate angle from a set of possibilities. Results on
this experiment are not shown in this paper due to space constraints but they can be reproduced with
the companion code.
4.2 MOVING MNIST
The Moving MNIST benchmark uses videos generated by moving 28× 28 pixel images of hand writ-
ten digits in a 64 × 64 pixels canvas. Just like in the Bouncing Shapes dataset, the digits move with
different different speeds in different directions and can bounce in the walls. Unlike the Bouncing
Shapes dataset, there are 60000 different sprites for training and 10000 for test, making it impracti-
cal to use a discrete memory module. Instead, we use the memory representation denoted by (13)
followed by st = d(zt) as written in Algorithm 1.
We trained a convolutional Perception Updating Network using 2 layer LSTMs each one with 1024
cells for 200 epochs, with 10000 gradient updates per epoch. The latent variable z had 100 dimen-
SionS and the decoder d(∙) WaS a single hidden layer MLP with 1000 hidden neurons and SoftPlUS
8
Under review as a conference paper at ICLR 2017
activation function. The output layer of this MLP has 784 neurons, which is the size of an MNIST
image, and sigmoid activation function. In the test set we obtained a negative log-likelihood of 239
nats with the proposed architecture, while a 2 layer LSTM baseline had 250 nats. Note that the our
method was optimized to minimize the lower bound (12), not only the negative likelihood. These
results are not as good as those obtained by the Video Pixel Networks (Kalchbrenner et al., 2016)
that obtained 87 nats on the test set. Nevertheless, both approaches are not mutually exclusive and
instead of a fully connected decoder we could use a similar PixelCNN decoder to generate sprites
with higher likelihood. In this first paper we decided instead to focus in defining the statistical
framework and interpretable “what” and “where” decoupling.
When running the proposed method in rollout mode, feeding the outputs back as next time step
inputs, we were able to generate high likelihood frames for more time steps than with a baseline
LSTM. Also, since the sprite to be generated and its position in the frame are decoupled, in rollout
mode we can fix the sprite and only use the δxy coming from the network. This way we can generate
realistic looking frames for even longer, but after a few frames we observed the digits stopped
moving or moved in the wrong direction (see video in the companion code repository). This means
that the LSTM RNN was not able to maintain its internal dynamics for too long, thus, there is still
room for improvement in the proposed architecture.
In Fig. 5 we show sample rollout videos. The network was fed with 10 frames and asked to generate
10 more getting its own outputs back as inputs and the companion code repository for an animated
version of this figure.
This experiment also suggests several improvements in the proposed architecture. For example, we
assumed that the internal RNN has to calculate a sprite at every time step, which is inefficient when
the sprites don’t change in the video. We should improve the architecture with an extra memory
unity that snapshots the sprites and avoid the burden of recalculating the sprites at every step. We
believe this would a possible way to free representation power that the internal RNN could use to
model the movement dynamics for even more time steps.
5 Conclusions
This paper introduced a statistical framework for modeling video of 2D scenes inspired by graphics
pipelines and variational auto-encoding Bayes. From this statistical framework we derived a vari-
ational lower bound that decouples sprites and their dynamics in a video. To optimize this lower
bound, we suggested a family of architectures called Perception Updating Networks that can take
advantage of this decoupled representation by memorizing sprites or their percepts and updating in
location in a scene independently. We showed that this architecture could generate videos that are
interpretable and are better suited than baseline RNNs for long video generation.
Acknowledgments
We thank Ryan Burt for several suggestions to the first draft. This work was partially funded by the
University of Florida Graduate Student Fellowship and ONR N00014-14-1-0542.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
SM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, Koray Kavukcuoglu, and Geoffrey E
Hinton. Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint
arXiv:1603.08575, 2016.
Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction
through video prediction. arXiv preprint arXiv:1605.07157, 2016.
Clement Godard, Oisin Mac Aodha, and Gabriel J Brostow. UnsUPervised monocular depth estima-
tion with left-right consistency. arXiv preprint arXiv:1609.03677, 2016.
9
Under review as a conference paper at ICLR 2017
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems,pp. 2672-2680, 2014.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
Jarmo Hurri and Aapo Hyvarinen. SimPle-cell-like receptive fields maximize temporal coherence
in natural video. Neural Computation, 15(3):663-691, 2003.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Ad-
vances in Neural Information Processing Systems, pp. 2017-2025, 2015.
Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex
Graves, and Koray Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527, 2016.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional
inverse graphics network. In Advances in Neural Information Processing Systems, pp. 2539-2547,
2015.
William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video pre-
diction and unsupervised learning. arXiv preprint arXiv:1605.08104, 2016.
Reza Mahjourian, Martin Wicke, and Anelia Angelova. Geometry-based next frame prediction from
monocular video. arXiv preprint arXiv:1609.06377, 2016.
Bruno A Olshausen et al. Emergence of simple-cell receptive field properties by learning a sparse
code for natural images. Nature, 381(6583):607-609, 1996.
Simon Osindero and Geoffrey E Hinton. Modeling image patches with a directed hierarchy of
markov random fields. In Advances in neural information processing systems, pp. 1121-1128,
2008.
Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. Learn-
ing what and where to draw. arXiv preprint arXiv:1610.02454, 2016.
Danilo Jimenez Rezende, SM Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, and Nico-
las Heess. Unsupervised learning of 3d structure from images. arXiv preprint arXiv:1607.00662,
2016.
Peter Shirley, Michael Ashikhmin, and Steve Marschner. Fundamentals of computer graphics. CRC
Press, 2015.
Eero P Simoncelli and Bruno A Olshausen. Natural image statistics and neural representation.
Annual review of neuroscience, 24(1):1193-1216, 2001.
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video
representations using lstms. CoRR, abs/1502.04681, 2, 2015.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.
arXiv preprint arXiv:1609.02612, 2016.
10