Under review as a conference paper at ICLR 2017
Recursive Regression with Neural Networks:
Approximating the HJI PDE Solution
Vicenc Rubies Royo, Claire Tomlin
Department of Electrical Engineering and Computer Sciences
UC Berkeley
Berkeley, California, USA
vrubies@berkeley.edu, tomlin@berkeley.edu
Ab stract
Most machine learning applications using neural networks seek to approximate
some function g(x) by minimizing some cost criterion. In the simplest case, if one
has access to pairs of the form (x, y) where y = g(x), the problem can be framed
as a regression problem. Beyond this family of problems, we find many cases
where the unavailability of data pairs makes this approach unfeasible. However,
similar to what we find in the reinforcement learning literature, if we have some
known properties of the function we are seeking to approximate, there is still hope
to frame the problem as a regression problem. In this context, we present an
algorithm that approximates the solution to a partial differential equation known
as the Hamilton-Jacobi-Isaacs partial differential equation (HJI PDE) and compare
it to current state of the art tools. This PDE, which is found in the fields of control
theory and robotics, is of particular importance in safety critical systems where
guarantees of performance are a must.
1	Introduction
Artificial neural networks are remarkable function approximators used in a myriad of applications
ranging from complex controllers for robotic actuation (Levine et al., 2016) (Schulman et al., 2015)
to simple image classifiers for digit recognition (LeCun et al., 1989) . They even find uses in physics
to find approximations to solutions of PDEs and systems of coupled ordinary differential equations
(ODEs) (Lagaris et al., 1998). Their success is in part achieved by their property of being universal
function approximators (Hornik et al., 1989). In order to train a neural network one usually defines
a cost function which captures the ”goodness” of the choice of parameters in our model, and uses
gradient descent/ascent algorithms to improve them. In supervised learning, for example, input out-
put data pairs are used to define a cost function such as the mean squared error or the mean absolute
error; unfortunately, in many cases the function we want to approximate is unkown. For instance,
in many reinforcement learning settings one wants to find the optimal policy, a function from state
variables to actions1, which maximizes the expected sum of discounted rewards of an agent in some
environment. This function is usually unkown a priori, so this problem can’t readily be framed
as a regression problem using input-output pairs. This assertion becomes blurred, however, when
looking at the work of Mnih et al. (2013), where a deep Q-network learns by generating targets and
minimizing a cost of the form
Li(θi) = Es,a〜ρ[(yi- Q(s,a; θi))2].
(1)
Here, the targets yi are generated from the same Q-network that is being used to approximate the
Q-function, hence the neural network has two purposes: approximation and data generation. In this
work, we show that this same idea can be extended to the domain of approximating solutions to
partial differential equations, and in particular the solution to the Hamiton-Jacobi-Isaacs PDE.
1or states to probabilities over actions
1
Under review as a conference paper at ICLR 2017
2	The Hamilton-Jacobi-Isaacs PDE
In control theory and robotics we often want to know how a system evolves in time given some
input signal. In particular, one would like to know whether there exists an (optimal) input signal that
drives our system to a particular region of interest in our state space and what that input is. For a
deterministic system with continuous states and inputs, this problem can be succinctly expressed as
a partial differential equation known as the Hamilton-Jacobi-Isaacs (HJI) PDE.
Let V : Rn X R- → R. Then, given a time invariant system of the form 篝 =f (x, a, b) and
boundary condition V(x, 0) = l(x), where x ∈ Rn is the state vector and a ∈ A ⊆ Rma and
b ∈ B ⊆ Rmb are inputs to the system2, we wish to find the solution to the minimum-payoff HJI
PDE, associated to the reachability problem:
∂V(x t)
∂χ ) = -min{0,H(x, VxV)},
(2)
where
H(x, Vx V) := max min Vx VTf(x, a, b)	(3)
is known as the Hamiltonian. The boundary condition V(x, 0) = l(x) encodes in its zero sub-level
set (i.e. l(x) ≤ 0) the region of interest in our state space known as the target set T. Lastly, the so-
lution V(x, t) to (2) encodes the information about all the starting states whose induced trajectories
will enter (and possibly leave) T within |t|, given the dynamics and input signals. More precisely,
for some starting state x0 and t ≤ 0, V(x0, t) < 0 if and only if the trajectory starting from x0
enters T within |t|.
To give some intuition as to why V(x, t) encodes the starting states whose trajectories enter T
within t, let Us consider the simpler problem where 篝 =f (x) is an autonomous system without
any inputs. Further, let us write (2) as a finite difference in t. With some rearranging, and absorbing
the gradient into V (i.e. Vx VTf(x)∆t+V(x, t) ≈ V(x+f(x)∆t, t)), one can obtain the following
approximation
V(x, t - ∆t) ≈ min{ V(x, t) , V(x + f (x)∆t, t) }.	(4)
It is straightforward to see from (4) that at time t = 0 all the states outside of T (i.e. V(x, 0) ≥ 0)
but near its boundary, whose induced trajectories enter the target (i.e. V(x + f(x)∆t, 0) < 0) within
∆t, will become negative in V(x, -∆t). Thinking of this update recursively one can intuitively see
how the zero sub-level set of V grows backward in time to include more and more states.
For the case of one input trying to drive our system into T, the approximation becomes
V(x, t - ∆t) ≈ min{ V(x, t) , min V(x + f(x, b)∆t, t) },	(5)
b
and for two competing inputs,
V(x, t - ∆t) ≈ min{ V(x, t) , max min V(x + f(x, a, b)∆t, t) }.	(6)
ab
Using the previous analogy of the autonomous system, one can see how (5) and (6) are essentially
different ways to expand the zero sub-level set backward in time: (5) can be seen as an input trying
to expand the set as fast as possible; (6) can be seen as two inputs with competing goals, where one
input tries to expand the set and the other seeks to prevent its growth. Moreover, this last setting
shows the relevance of the HJI PDE in safety critical systems. By treating input b as a bounded
worse case disturbance and T as some unsafe region, one can establish safety guarantees about the
system and claim which states won’t be driven into T within some time horizon.
2a is usually taken to be the input and b is taken to be some bounded input disturbance
2
Under review as a conference paper at ICLR 2017
Lastly, it is important to note that V(x, t) contains useful information in its gradient VχV(x, t). In
the case where 篝 = f (x, b) has a single input, the argument minimizing the following optimization
problem
b* = argmin VxV(x0,t)Tf(x0,b)	(7)
b∈B
yields the instantaneous optimal input for state x0 at time t to guide the trajectory into T as fast as
possible. Using this fact one can generate an optimal control policy based on the gradient of V. This
idea can then be easily extended to the case of two competing inputs to obtain competing control
policies. Finally, even though (7) need not be a convex problem, in this work we will only deal with
simple dynamical systems, making the optimization problem easy to solve.
3	Approximating S olutions of PDEs
The problem presented in section 2 (as in many other cases with PDEs) is general not straightforward
to solve. For this reason, trying to find a good approximation instead of the actual solution can
be a reasonable approach. Many current state-of-the-art tools used to approximate solutions of
PDEs, including (2), use gridding techniques (Mitchell, 2007) whereby finite differences are used to
iteratively update values on a grid. Another approach (Lagaris et al., 1998) is to train a feedforward
neural network by minimizing the following loss
N
Lθ :=X G(xi, ψθ(xi), Vψθ(xi), V2ψθ(xi))2	(8)
i=1
where G(x, ψ(x), Vψ(x), V2ψ(x)) = 0 is the PDE whose solution ψ(x) we are trying to ap-
proximate and xi are points taken from the discretization of our domain. In (8), the function
ψθ(x) := A(x) + F (x, Nθ(x)) is a candidate approximation which by construction satisfies the
boundary condition, where Nθ(x) is a feedforward neural network. In order to ensure that the con-
ditions at the boundary are satisfied, F(x, Nθ(x)) = 0 at the boundary and A(x) is a fixed function
which satisfies them.
Although this approach is well suited for some problems, special care must be taken when com-
puting the gradient of the loss with respect to the parameters. For instance, following the previous
procedure, the loss for HJI PDE would be written as
Lθ ：= XX(dV(Xi,ti) + min{0,H(% VxV)})2,	(9)
i=1
but the min in the function makes this expression not differentiable everywhere. There exist ways to
circumvent this problem (Djeridane and Lygeros, 2006), however, but they require the cumbersome
definition of many intermediary functions which can become hard to find for complicated dynamical
models.
In this work, we try to tackle the problem of finding an approximate solution to (2) from a different
perspective. We show that a poor approximation to our solution is enough to generate “good enough”
new data for regression, which can in turn be used to improve our model.
4	Self-Generated Data
4.1	Algorithm
In this section we present a simple method for approximating the solution to (2) by utilizing a
feedforward neural network in two ways: as a function approximator and a data generator. We
believe that this parametric approach is better suited for finding good approximations by avoiding
some of the limitations found in gridding/tabular techniques due to the curse of dimesionality. To
3
Under review as a conference paper at ICLR 2017
.1	1	. . 1	1 f' ∙	∙	TV / ∖ . 1	i' . Λ	i'	♦
that end, we start by defining our candidate approximation Vθ (x) to be of the same form as in
(Lagaris et al., 1998); that is, a sum of two terms which help satisfy our boundary condition V (x, 0)
J-，	、	--， 一、	.-,	、
Vθ(x,t) = V(x, 0) + tNθ(x,t),
(10)
where Nθ (x, t) is a neural network mapping from our states and time variables to the real numbers.
Next, we sample N points in the state variable x chosen uniformly at random over some set S which
includes T (the target set), and similarly, sample N points in the time variable t uniformly at random
over the set [-T, 0], where T ≥ 0 is the desired time horizon. By sampling from these distributions,
we seek to find a good approximation to V(x, t) over the set S × [-T, 0]. Once these points have
been gathered, we make use of the update (4),(5) or (6) (depending on our problem) and use Vθ(x, t),
the approximation itself, to generate the new regression points. The complete algorithm 4.1 is shown
using update equation (6), but it should be clear how to modify it for the other cases.
Algorithm 1 Recursive Regression via SGD with Momentum
1:	Input: N, interval, V(x, 0), f(x, a, b), A, B, S, T, K(batch size), γ (momentum decay), η
(learning rate)
2:	Random initialization of the weights and biases of the neural network Nθ (x, t)
3:	Define Vθ (x, t) := V(x, 0) + tNθ (x, t)
4:	Define Lθ := PK=O ∣yfc - V(xk,tk)∣
5:	i — 0, Vi — 0, ∆t — 10—2
6:	while True do (or stopping criterion)
7:	if mod(i,interval) == 0 then
8:	R J empty array of size N
9:	Sample N pairs (x,t)〜Uniform(S X [—T, 0])
10:	for j = 0 to N do
11:	(aj,bj) J argmax argmin Vx^VθTf(x, a, b)
a∈A	b∈B
12:	yj J min{Vθ(xj,tj), Vq(Xj + f(xj, a ^, b^)∆t, tj) }
13:	Rj J ((xj, tj), yj)
14:	b J K elements from R picked at random
15:	νi+1 J γνi + ηVθL(b)
16:	θi+1 J θi - νi+1
17:	i J i + 1
18:	Output： VQ(x, t)
4.2	Comments
Algorithm 4.1 is a type of bootstrapping method in that lines 12 and 13 make use of Vθ(x, t) to
generate points for regression to train Nθ(x, t) which in turn modify Vθ(x, t) itself. At first glance,
it is unclear whether the generated pairs ((xj, tj), yj) will result in a good approximation to the
solution of our PDE after regression; however, given the form of our candidate function (10) we
expect that points sampled near t = 0 will in fact be reasonable approximations of V(x, t) for small
t. Given this assumption, we hypothesize that despite the presence of misleading data, our network
will be able to do a good job at regressing over all points, thus improving our initial model and
allowing the generation of improved data. By repeating this procedure, we expect the accuracy of
the boundary condition to ”propagate” backward in time (possibly with some minor error) in the
form of better and better points for regression.
Another important aspect from line 13 is that we are simulating our dynamics forward in time using
the EUler approximation step Xj + f (xj,a*, b*)∆t. In practice, depending on the variability and
complexity of the dynamics, one might use a Runge-Kutta method or a more involved integration
procedUre. For the experiments in the next sections a RUnge-KUtta method with 4 stages (RK4) was
Used.
4
Under review as a conference paper at ICLR 2017
5	Experiments
In this section we present a few 2-dimensional experiments to demonstrate the validity of our claim
and the effectiveness of the algorithm. To measure the performance of the algorithm, we compare
the difference between our computed approximation and the true analytical solution. In case it is
not straightforward to obtain the solution, a very accurate approximation taken from state-of-the-art
tools is used instead. In particular, we make use of the LevelSet Toolbox from Mitchell (2007), a
powerful computational tool for obtaining good approximations to Hamilton-Jacobi (HJ) PDEs.
The first error metric to be used will be
1M
EI(Vθ(x,t)) := M E ∣V(xi,ti) - %(xi,ti)∣	(11)
i=1
where M are the number of points chosen from our domain to compute the average absolute error
and V(x, t) can denote either the true solution or an accurate approximation. In the case where the
analytical solution is known, the points are taken uniformly at random over S ; otherwise, they are
taken over some grid in S and [-T, 0]. Lastly, we also use a second error metric
E2(vθ (XM = M X | dV⅛⅛ + min{0, H (χi, VxV 川
i=1
(12)
similar to the one defined in (9), which denotes the extent by which (on average) the approximation
is violating the PDE equality. For all experiments M = 3000, all chosen uniformly at random over
S × [-T, 0]. In section 5.4 we also show a visual representation of the approximations.
5.1	A Linear System
In this experiment we study the performance of the algorithm on an autonomous system of the form
--
x = f(x) =	2	- x
(13)
with V(x, 0) = ||x||2 - 1 and T = 1.0. For this simple system, the solution to the HJI PDE can be
found analytically to be V(x, t) = e-t||x||2 - 1. One can easily verify this by checking it satisfies
the boundary condition and (2). For this experiment, a feedforward neural network with a single
hidden layer of 10 units and sigmoid activation functions was used. The number of points sampled
was chosen to be N = 500, uniformly picked over the set S := {(x1, x2)|x1, x2 ∈ [-5, 5]} and
over t ∈ [-T, 0]. The batches were picked to be of size K = 10, momentum decay γ = 0.95 and
learning rate η = 0.1. The interval to renew the regression points was chosen to be 1000 iterations
and the program was halted at 500,000 iterations.
Figure 1: From left to right: the first figure shows the mean absolute error E1 , the second figure
shows the mean absolute PDE error E2 and the third figure shows the loss Lθ as defined in algorithm
4.1 over all the data. The horizontal axis represents the iteration number.
5
Under review as a conference paper at ICLR 2017
The results shown in Fig. 1 where taken over 10 runs of the algorithm concurrently executed over
multiple threads. The overall time to run the 500,000 iterations for all threads was 1521 seconds.
The average E1 error at halting time was in the order of 7 × 10-2, whereas the E2 error was in the
order of 3 × 10-1. The sharp jumps appearing in the loss figure in the majority of cases correspond
to the error after new points are generated and used for regression.
5.2	Pursuit-Evasion Game: Single Input
In this experiment we explore a pursuit-evasion game where a pursuer has to intercept an evader. In
a first simplified approach, we assume the evader has a fixed heading and speed, whereas the pursuer
has the same speed as the evader but has the liberty to change the direction of its heading. Fixing
the evader at the origin with its heading aligned with the x-axis we frame the problem in relative
coordinates between the evader and pursuer, that is x = [xr yr]T , where xr and yr represent the x
and y position of the pursuer relative to the evader. This system’s dynamics are readily encoded in
the following equation
Xr] — f(T b) — Γvpcos(b) - Ve
yr	,∖->)	Vpsin(b)
(14)
where Vp = Ve = 2.0 represent the speed of the pursuer and evader respectively, b ∈ [0, 2π]
represents the input available to the pursuer, which is the angle with respect to the x-axis. In this
simplified pursuit-evasion game we say the pursuer has captured the evader if they are within 1 unit
of distance from each other. Thus, we define our capture condition by defining V (x, 0) = ||x||2 - 1,
which will ensure that our approximation captures all the states from which the pursuer can capture
the evader in within T = 1.0. As in the previous example, we choose the same network architecture
and the same values for the halting time, renewal interval, N,K,γ and η.
Figure 2: From left to right: the first figure shows the mean absolute error E1, the second figure
shows the mean absolute PDE error E2 and the third figure shows the loss Lθ as defined in algorithm
4.1 over all the data. The horizontal axis denotes iteration number.
The results shown in Fig. 2 where also taken over 10 runs of the algorithm like in section 5.2. The
overall time to run the 500,000 iterations was 1952 seconds. The average E1 error at halting time
was also in the order of 7 × 10-2, whereas the E2 error was in the order of 1.5 × 10-1. The points
used to compute E1 were taken from a 51 × 51 grid at t = -0.5 (half of the time horizon), using
a previously computed approximation from the LevelSet Toolbox. The reason why a single time
instance was used to compute E1 was purely to reduce the amount of computation of the error at
run-time.
5.3	Pursuit-Evasion Game: Two Inputs
The last experimental example also consists of a pursuit-evasion game, but in this case the evader
has access to a range of speeds through an input a ∈ [-2, 2]. The system dynamics thus become
xr
y
f (x, a, b)
Vp cos(b) - a
Vpsin(b)
(15)
6
Under review as a conference paper at ICLR 2017
and, similarly, V (x, 0) = ||x||2 - 1 and T = 1.0. As before, vp = 2.0. The interesting behavior we
expect to see from this experiment, in comparison to the single input counterpart, is that this new
available action to the evader will make it more difficult for the pursuer to intercept. This should
then be evident by looking at our approximation V⅞ and its zero sub-level sets at different times. For
this experiment we also chose the same architecture for the network as in the previous experiments
and the same parameters, except for the halting time which was 300,000 iterations.
150000
150000
Figure 3: From left to right: the first figure shows the mean absolute error E1, the second figure
shows the mean absolute PDE error E2 and the third figure shows the loss Lθ as defined in algorithm
4.1 over all the data.
The results shown in Fig. 3 where also taken over 10 runs of the algorithm. The overall time to run
the 300,000 iterations over the all threads was 1028 seconds. The average E1 error at halting time
was in the order of 6 × 10-2, whereas the E2 error was in the order of 1.5 × 10-1. Like in the
single input case, the points used to compute E1 were taken from a 51 × 51 grid at t = -0.5 of a
pre-computed approximation.
5.4	Contour Visualization
In this section we briefly display some of the contours for a neural network picked at random
from those computed in the experimental section. Each line corresponds to the set of states where
Vθ (x, t) = 0 for t = 0, -0.25, -0.5, -0.75, -1.0. These contours enclose within them the states
from which our system can reach the target set T within the absolute value of its associated time.
Figure 4: From left to right: contours for experiment one, experiment two and experiment three. As
one can appreciate, the contours grow according to the specified dynamical model.
As expected, the linear system’s contours expand radially in all directions since the origin is a stable
equilibrium point3 where all trajectories converge. For the pursuit-evasion game of one input, we
also see that the contours grow toward the right, which is a sensible outcome given that the pursuer
can’t catch up with the evader if it starts somewhere where xr < -1.0. Finally, the last set of
contours associated with the pursuer-evader game of two competing inputs also make sense, since
starting states xr < -1.0 or xr > 1.0 should not permit the pursuer to intercept the evader, and so
3with the same negative real part for the eigenvalues
7
Under review as a conference paper at ICLR 2017
the contours should not expand in those directions. As a last comparison, in Fig. 5 we display the
actual contours that would be obtained using the LevelSet Toolbox.
Figure 5: Contours obtained from the LevelSet Toolbox in Matlab.
By comparing Fig. 5 and 4 one can qualitatively see that the neural network has learned an accurate
approximation of V (x, t).
6	Advantages and Disadvantages
The first advantage of using this method over gridding techniques is a dramatic improvement in
memory requirements. For instance, using a standard grid with [51, 51, 10] discretization points per
axis (i.e. 51 in xr, 51 in yr and 10 in t) each of the three previous experiments requires the storage
of 26, 010 numbers, as opposed to 51 weights for our neural network. For the gridding approach
this memory requirement must increase exponentially with the number of dimensions, whereas this
need not be the case for our method. Furthermore, points that do not fall exactly on the grid have
to be interpolated, whereas the neural network is an approximation that assigns values to all points
in the domain. To this we can also add that fact that the neural network can yield the gradient at
any point directly with backpropagation, whereas the gradient must once again be approximated for
gridding techniques.
The main disadvantage of this method, for small dimensional systems in particular, is the time
requirement. Computing values over a grid with the LevelSet Toolbox for the previous systems took
less than 10 seconds. This advantage of gridding/tabular procedures, however, quickly disappears in
higher dimensions (4D, 5D...) due to the curse of dimensionality. Finally, another disadvantage of
using this method is the necessity to tune hyper parameters.
7	Conclusion and Future Work
In this work we focus our attention on the idea that recursive/bootstrapped regression can be used
in some problems where the function we wish to approximate has some known characteristics. In
particular, we show that accurate approximations to the HJI PDE solution can be found by assigning
a neural network two roles, one of them being function approximation, and the other data gener-
ation.To validate our hypothesis three different experiments with three distinct dynamical systems
were performed with satisfactory results.
In this work we did not focus on the architecture of the neural network, but rather on its ability to
perform well on three distinct tasks using the same algorithm. In future work we will try to find
whether one can construct wider or deeper neural networks and obtain better results. We also want
to investigate how well this method scales with the number of state and input dimensions. Positive
results in that front could suppose an important step to further alleviate the effects of the curse of
dimensionality, which are pervasive in griding methods.
8
Under review as a conference paper at ICLR 2017
Acknowledgments
Special thanks to Carlos Florensa for his implementation tips and to Jaime F. Fisac for helping in
the process of writing this work.
References
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-End Training of Deep
Visuomotor Policies. Journal of Machine Learning Research, 17:1-40, 2016. ISSN 15337928.
doi: 10.1007/s13398-014-0173-7.2.
John Schulman, Sergey Levine, Michael Jordan, and Pieter Abbeel. Trust Region Policy Optimiza-
tion. Icml-2015, page 16, 2015. ISSN 2158-3226. doi: 10.1063/1.4927398.
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.
Backpropagation Applied to Handwritten Zip Code Recognition, 1989. ISSN 0899-7667.
I E Lagaris, A Likas, and D I Fotiadis. Artificial neural networks for solving ordinary and partial
differential equations. IEEE Transactions on Neural Networks, 9(5):987-1000, 1998. ISSN 1045-
9227. doi: 10.1109/72.712178.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural Networks, 2(5):359-366, 1989. ISSN 08936080. doi:
10.1016/0893-6080(89)90020-8.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. arXiv preprint
arXiv: ...,pages 1-9, 2013. ISSN 0028-0836. doi: 10.1038/nature14236.
Ian Mitchell. A toolbox of level set methods. Technical report, 2007.
Badis Djeridane and John Lygeros. Neural approximation of PDE solutions: An application to
reachability computations. Proceedings of the 45th IEEE Conference on Decision and Control,
pages 3034-3039, 2006. ISSN 01912216. doi: 10.1109/CDC.2006.377184.
9
Under review as a conference paper at ICLR 2017
8	Extra Experiment
This experiment was designed to test the applicability of the method to problems beyond those
presented in the previous sections. In particular, we show that with small changes we can also
compute an accurate approximation to a pursuit-evasion problem in 3 dimensions. Similar to the
previous examples, we frame the problem in relative coordinates with the x-axis aligned with the
evader’s heading, and give the pursuer and evader control over the rate of rotation. This can be
written as follows:
xr
y
θr
-ve + vp cos(θr) + ayr
f(x, a, b) =	vp sin(θr) - axr
b-a
(16)
For this problem the capture condition is encoded in the boundary condition V (x, 0) =
||[xr yr]T ||2 - 1 (where we ignore θr since the capture condition only depends on the distance)
and we consider a the time horizon T = 1.0s. For this problem we give both pursuer and evader
the same speed vp = ve = 1.0 and the same turning rates a, b ∈ [-1, 1]. Unlike the previous
experiments, we used a neural network with two hidden layers with 10 and 5 units respectively and
sigmoid activations. The number of points sampled was chosen to be N = 2000, uniformly picked
over the set S := {(χr ,yr, θr )|xr, yr ∈ [-5, 5], θr ∈ [-∏,∏]} and overt ∈ [-T, 0]. The batches
were picked to be of size K = 25, momentum decay γ = 0.999 and learning rate η = 0.001. The
interval to renew the regression points was chosen to be 1000 iterations and the program was halted
at 500,000 iterations.
Figure 6: From left to right: the first figure shows the mean absolute error E1, the second figure
shows the mean absolute PDE error E2 and the third figure shows the loss Lθ as defined in algorithm
4.1 over all the data.
As shown in Fig. 6, both error metrics decrease as the algorithm progresses, reaching an average
error for E1 in the order of 5.0 × 10-2 and an average error for E2 in the order of 1.0 × 10-1. The
points used to compute E1 were taken from a 51 × 51 × 50 approximation grid at t = -0.5s. This
set of experiments was run in a different machine4 using 8 threads and the total time for all threads
to finish was 1000 seconds. Finally, Fig. 7 shows the zero level set contour at t = -0.5, which is
now a 3D surface, from side and top perspectives. The first row shows the output of the LevelSet
Toolbox from each perspective, and the second row shows a 3D scatter plot of points on the zero
level-set obtained from one of the 8 neural networks that were trained.
4due to heavy usage of the first machine we had to switch to a different one
10
Under review as a conference paper at ICLR 2017
Figure 7: The first column shows the first side view perpendicular with respect to the x-z plane. The
second column shows the second side view perpendicular with respect to the y-z plane. Finally, the
third column shows the top view which is perpendicular with respect to the x-y plane.
For this experiment, only 111 numbers were needed to store the approximation, as opposed to 51 ×
51 × 50 × 10 = 1300500 numbers (i.e. 51 in xr, 51 in yr, 50 in θr and 10 int) for a [51 × 51 × 50 × 10]
grid approximation.
11