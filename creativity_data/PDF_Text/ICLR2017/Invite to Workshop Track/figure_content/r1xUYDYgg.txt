Figure 1: Example of forward CalCulation of fully-ConneCted layer using Sushi2Most GPGPU kernels are implemented originally for Sushi2, but matrix multipliCation kernel isported from ClBLAS’s5 “sgemm”, beCause it requires advanCed optimization.
Figure 4: Calculation speed for each layer’s computation in VGG16. Measured on NVIDIA K80GPU. For example, forward computation of convL1 is performed by matrix multiplication of(802816, 27) and (27, 64). Forward, backward, gradient computation of cuBLAS and clBLASare shown in different bars.
Figure 5:	Data-parallelism	systemof distributed trainingThe number of clientsThe number of clients^^-8bit firefox^^∏32bit firefox8bit node.js32bit node.jsFigure 6: Computation speed with respect to the number of distributedclients. Left: speed of training LeNet in Nexus 7 Android tablets(Chrome browser). Right: speed of training VGG16 in clients withNVIDIA K80 (Firefox browser / node.js). Measurement includes timeof communication and optimization in the server.
Figure 6: Computation speed with respect to the number of distributedclients. Left: speed of training LeNet in Nexus 7 Android tablets(Chrome browser). Right: speed of training VGG16 in clients withNVIDIA K80 (Firefox browser / node.js). Measurement includes timeof communication and optimization in the server.
