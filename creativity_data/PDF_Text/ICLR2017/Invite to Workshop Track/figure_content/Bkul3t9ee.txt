Figure 1: Method overview. Given a few demonstration videos of the same action, our method discoversintermediate steps, then trains a classifier for each step on top of the mid and high-level representations of apre-trained deep model (in this work, we use all activations starting from the first “mixed” layer that followsthe first 5 convolutional layers). The step classifiers are then combined to produce a single reward function perstep. These intermediate rewards are combined into a single reward function. The reward function is then usedby a real robot to learn the perform the demonstrated task as show in 3.2.
Figure 2: Entire training set for the pouring task (11 demonstrations).
Figure 3: Examples of ”pouring” reward functions. We show here a few successful examples, see Fig. 11for results on the entire test set. In 3a we observe a continuous and incremental reward as the task progressesand saturating as it is completed. 3b increases as the bottle appears but successfully detects that the task is notcompleted, while in 3c it successfully detects that the action is already completed from the start.
Figure 4: Feature selection classification accuracy on the pouring validation set for 2-steps classification. Byvarying the number of features n selected, we show that the method yields good results in the region n =[32, 64], but collapses to 0% accuracy starting at n = 8192.
Figure 5: Robot arm setup. Note that our method does not make use of the sensor on the back handle of thedoor, but it is used in our comparison to train a baseline method with the ground truth reward.
Figure 6: Rewards from human demonstration only. Here we show the rewards produced when trained onhumans only (see Fig. 12). In 6a, we show the reward on a human test video. In 6b, we show what the rewardproduces when the human hands misses opening the door. In 6c, we show the reward successfully saturateswhen the robot opens the door even though it has not seen a robot arm before. Similarly in 6d and 6e we showit still works with some amount of variation of the door which was not seen during training (white door andblack handle, blue door, rotations of the door).
Figure 7: Door opening success rate at each iteration of learning on the real robot. The PI2 baselinemethod uses a ground truth reward function obtained by instrumenting the door. Note that rewards learned byour method, even from videos of humans or different doors, learn comparably or faster when compared to theground truth reward.
Figure 8: Combining intermediate rewards into a single reward function. From top to bottom, we showthe combined reward function (with range [0,2]) followed by the reward function of each individual steps (withrange [0,1]). The first step reward corresponds to the initial resting state of the demonstration and is ignored inthe reward function. The second step corresponds to the pouring action and the third step corresponds to theglass full of liquid.
Figure 9: Qualitative examples of unsupervised discovery of steps for door and pouring tasks in trainingvideos. For each video, we show the detected splits when splitting in 2, 3 or 4 segments. Each segment isdelimited by a different value on the vertical axis of the curves.
Figure 10: Qualitative examples of reward functions for the door task in testing videos. These plots showthe individual sub-goal rewards for either 2 or 3 goals splits. The ”open” or ”closed” door reward functions arefiring quite reliably in all plots, the ”hand on handle” step however can be a weaker and noisier signal as seenin 10b and 10c, or incorrect as shown in 10e. 10d demonstrates how a ”missed” action is correctly recognized.
Figure 11: Entire testing set of ”pouring” reward functions. This testing set is designed to be more chal-lenging than the training set by including ambiguous cases such as pouring into an already full glass (11i and11j) or pouring with a closed bottle (11g and 11h). Despite the ambiguous inputs, the reward functions doproduce reasonably low or high reward based on how full the glass is. 11a, 11b, 11b and 11d are not strictlymonotonically increasing but do overall demonstrate a reasonable progression as the pouring is executed toa saturated maximum reward when the glass is full. 11e also correctly trends upwards but starts with a highreward with an empty glass. 11f is a failure case where the somewhat transparent liquid is not detected.
Figure 12: Entire training set of human demonstrations.
