Figure 1: How to get similar results using convolutions with delta-functions and spatial transformers.
Figure 2: A schematic block diagram for a Perception Updating Network. This configuration usesboth convolutions with delta functions for translation and spatial transformers for rotation. It alsoshows the optional background underlay.
Figure 3: Results on the Bouncing Shapes dataset. Three 8x8 sprites (a square, a cross and a triangle)were used to generate videos. The shapes move in a 20x20 pixels canvas with a Toeplitz backgroundand bounce on the corners. a) We show one step ahead predictions with the compared methods. b)We also show the learned sprites for the convolutional implementation of the proposed PerceptionUpdating Networks when we over- and under-estimate the size of the desired sprites.
Figure 4: Learning curves in the test task of two implementations of the proposed architecture (convPUN and STN PUN) and an equivalent LSTM baseline. Note that the spatial transformer basedPUN was not able to generalize to the test set, i.e. they did not work well for generating videoswhen getting its own previous outputs as next step inputs.
Figure 5: Sample rollouts of a 2 layer LSTM convolutional Perception Updating Network with .
