Figure 1: Diagram of hidden states of a generative RNN as a tree, where xt(n) denotes which of Npossible inputs is encountered at timestep t. Given ht , the starting node of the tree, there will be adifferent possible ht+1 for every xt(+n)1. Similarly, for every ht+1 that can be reached from ht, there isa different possible ht+2 for each xt(n+)2, and so on.
Figure 2:	Hutter prize validation performancein bits/char plotted against number of networkparameters for mLSTM and stacked LSTM.
Figure 3:	Cross entropy loss for mLSTM andstacked LSTM immediately proceeding asurprising inputO×××with the largest average loss taken by mLSTM and stacked LSTM. Both networks perform roughlyequally on this set of surprising characters, with mLSTM and stacked LSTM taking losses of 6.27bits/character and 6.29 bits/character respectively. However, stacked LSTM tended to take muchlarger losses than mLSTM in the timesteps immediately following surprising inputs. One to fourtime-steps after a surprising input occurred, mLSTM and stacked LSTM took average losses of (2.26,2.04, 1.61, 1.51) and (2.48, 2.25, 1.79, 1.67) bits per character respectively, as shown in Figure 3.
