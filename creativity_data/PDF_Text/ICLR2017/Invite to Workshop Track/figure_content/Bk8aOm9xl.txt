Figure 1: Median performance for the continuous control tasks over 10 runs with a fixed set of seeds,with interquartile ranges shown in shaded areas. The x-axis is iterations of training; the y-axis isaverage undiscounted return. AKL-k refers to learning progress (8), NLL to surprisal (5), and PREDto (12). For the first four tasks, η0 = 0.001; for SwimmerGather, η0 = 0.0001. Results for VIME arefrom Houthooft et al. [7], reproduced here with permission. We note that the performance curve forVIME in the SwimmerGather environment represents only 2 random seeds, not 10.
Figure 2: Benchmarking the benchmarks: median performance for the continuous control tasks over10 runs with a fixed set of seeds, with interquartile ranges shown in shaded areas, using the surprisalwithout learning bonus. RAN refers to the fact that this is essentially a random exploration bonus.
Figure 3: Speed test: comparing the performance of VIME against our proposed intrinsic rewardschemes, average compute time over 5 random runs. Tests were run on a Thinkpad T440p withfour physical Intel i7-4700MQ cores, in the sparse HalfCheetah environment. VIME’s greaterinitialization time, which is primarily spent in computation graph compilation, reflects the complexityof the Bayesian neural network model.
Figure 4: Median performance for the Atari RAM tasks over 10 runs with a fixed set of seeds, withinterquartile ranges shown in shaded areas. The x-axis is iterations of training; the y-axis is averageundiscounted return. AKL-k refers to learning progress (8), and NLL to surprisal (5).
