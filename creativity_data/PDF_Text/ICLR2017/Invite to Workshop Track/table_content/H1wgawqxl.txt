Table 1: MNIST baseline comparisonsName	Layers (Units)	Activation	Best Error	Mean ErrorBaseline Neural Net	3 (2048)=	tanh	1.66%	1.79%	,Baseline Neural Net	3 (2048)	ReLU	1.65%	1.76%Baseline NN with Dropout	3 (2048)	ReLU	1.35%	1.42%Baseline CNN	N/A	ReLU	0.88%	0.96%Baseline CNN with Dropout	N/A	ReLU	0.78%	0.84%For the experiments with NPF(L, T) activations, we use the same fully connected and convolutionalarchitectures as in the MNIST baseline experiments. Table 2 contains the results of the experiments.
Table 2: MNIST experiments with Fourier basis expansion. Conv. Act. is the activation functionin the convolutional layers (if present). F.C. Act. is the activation function in the fully connectedlayers. Epochs is the mean number of epochs in training. Results of note appear in bold.
Table 3: CIFAR-10 baseline results. Epochs is average number of epochs trained. There were fiveruns of each experiment.
Table 4: CIFAR10 experiments with Fourier basis. Conv. Act. is the activation function in theconvolutional layers. F.C. Act. is the activation function in the fully connected layers. In both casesit is the activation in the final stage of training (if two stages are used). For two stage training, errorrates and epochs for both first and second stages are given. Results of note appear in bold.
