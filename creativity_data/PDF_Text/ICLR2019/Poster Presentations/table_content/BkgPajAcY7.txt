Table 1: Performance (accuracy for all tasks except SICK-R and STSB, for which we report Pear-son’s r) on all ten downstream tasks where all models have 4096 dimensions with the exception ofBOE (300) and ST-LN (4800). Standard deviations are show in parentheses. InferSent-1 is the paperversion with GloVe (G) embeddings, InferSent-2 has fixed padding and uses FastText (F) embed-dings, and InferSent-3 has fixed padding and uses GloVe embeddings. We also show the differencebetween the best random architecture (BestRand) and InferSent-3 and ST-LN, respectively. The av-erage performance difference between the best random architecture and InferSent-3 and ST-LN is1.7 and -0.4 respectively.
Table 2: Performance (accuracy for all tasks except SICK-R and STSB, for which we report Pear-son’s r) on all ten downstream tasks. Standard deviations are show in parentheses. All modelshave 4096×6 dimensions. ST-LN and InferSent-3 were projected to this dimension with a randomprojection.
Table 3: Performance on a set of probing tasks defined in (Conneau et al., 2018). All randomarchitecture models are 4096 dimensions and were selected by tuning over validation performanceon the classification tasks.
Table 4: Performance (accuracy for all tasks except SICK-R and STSB, for which we report Pear-son’s r) on all ten downstream tasks. Standard deviations are show in parentheses. All models have4096 dimensions and were selected by tuning over validation performance on classification tasks orcorrelation tasks as noted. For RandLSTM this corresponds to a single model that uses max pooling.
Table 5: Accuracy on single-sentence binary classification tasks from SentEval, where max-poolingis done over padded values instead of over the length of the sentence. Experiments are split betweenSorted where sentences are sorted in order of length prior to batching and Unsorted where they arenot.
Table 6: Percentage of total data sparsed due to max-pooling over padded values and the percentageof that data that is the positive class for MR, CR, MPQA, and SUBJ.
Table 7: Performance (accuracy for all tasks except SICK-R and STSB, for which we report Pear-son’s r) on all ten downstream tasks. Standard deviations are show in parentheses. Six differentapproaches to initializing the parameters for each model are explored.
Table 8: Performance (accuracy for all tasks except SICK-R and STSB, for which we report Pear-son’s r) on all ten downstream tasks. Standard deviations are show in parentheses. All parametersfor both the underlying word embeddings and architectures (if applicable) are randomly sampledfrom six different initialization schemes. The last row shows the performance difference betweenthe best performing model (from BOREP and RandLSTM) in Table 7 which uses pre-trained wordembeddings and the best performing model from this table (outside of BOE with 4096 dimensinovectors) which uses randomly initialized embeddings. The average gain from using pre-trained em-beddings is 5.4 points.
