Table 1: Quantitative results in the ‘4layer’ network in both faces and natural scenes cases. For both‘objects’ we compute the SSIM. In both denoising and sparse inpainting, the leftmost evaluation isthe one with corruptions similar to the training, while the one on the right consists of samples withadditional corruptions, e.g. in denoising 35% of the pixels are dropped.
Table 2: Quantitative results in the ‘4layer’ network in both faces and natural scenes cases. In thistable, the `1 loss is reported. In each task, the leftmost evaluation is the one with corruptions similarto the training, while the one on the right consists of samples with additional corruptions, e.g. indenoising 35% of the pixels are dropped.
Table 3: Quantitative results for Imagenet (sec. D.1).				JJJJ	Task	Denoising		Sparse Inpaint.	MethodJJJJJ	25%	35% I	I 50%	75%Baseline-5layer	0.851	0.826	0.819	0.707Ours-5layer	0.890	0.884	0.873	0.818Baseline-6layer	0.859	0.843	0.816	0.727Ours-6layer	0.881	0.865	0.882	0.822Baseline-4layer-skip	0.885	0.863	0.855	0.726Ours-4layer-skip	0.896	0.881	0.859	0.744Table 4: Additional quantitative results (SSIM, see main paper) for the following protocols: i) ‘5layer’network, ii) 50 thousand training images, iii) skip connections.
Table 4: Additional quantitative results (SSIM, see main paper) for the following protocols: i) ‘5layer’network, ii) 50 thousand training images, iii) skip connections.
Table 5: Quantitative results for the semi-supervised training of RoCGAN (sec. D.3). The differenceof the two models is increased (in comparison to the fully supervised case). RoCGAN utilizethe additional unsupervised data to improve the mapping between the domains even with lesscorresponding pairs.
Table 6:	Details of the generator for the ‘4layer’ network baseline. Our modified generator includesin the AE pathway the same parameters. The parameters mentioned below are valid also for the‘4layer-50k’ and the ‘4layer-skip’ networks. ‘Filter size’ denotes the size of the convolutional filters;the last number denotes the number of output filters. BN stands for batch normalization. Conv denotesa convolutional layer, while F-Conv denotes a transposed convolutional layer with fractional-stride.
Table 7:	Details of the generator for the ‘5layer’ network baseline.
Table 8:	Details of the generator for the ‘6layer’ network baseline.
Table 9: Details of the discriminator. The discriminator structure remains the same throughout all theexperiments in this work.
Table 10: Validation of λl values (hyper-parameter choices) in the ‘4layer’ network. Unless explicitlymentioned otherwise, all the quantitative results measure the SSIM value. We notice that for λl largerthan 10 the results are significantly better than the baseline.
Table 11: Validation of λae values (hyper-parameter choices) in the ‘4layer’ network. The networkremains robust for a wide range of values of the hyper-parameter λae ; for λae >= 20 the test resultsdemonstrate a similar performance.
Table 12: Validation of λdecov values (hyper-parameter choices) in the ‘4layer-skip’ network. Thenetwork is more sensitive to the value of the λdecov than the λl and λae .
Table 13: Quantitative results (SSIM) for setting λ* = 0 alternatingly (sec. E.2). In each column, weset the respective hyper-parameter to zero while keeping the rest fixed.
Table 14: Quantitative results for the additional noise experiment (sec. E.3). We test the baselineand our model in additional noise cases; as the noise is incrementally augmented the difference isincreasing with our model much more resilient to additional noise. This is more pronounced in thedifferent type of noise, i.e. the 25/10, 25/20 and 25/25 cases.
Table 15: Quantitative results for the adversarial examples (sec. E.4). The first column correspondsto the ‘4layer’ case and are added for comparison.
