Table 1: Comparisons of normalization methods. First, we compare their types of parameters, numbersof parameters (#params), and hyper-parameters. Second, we compare types of statistics, computational com-plexity to estimate statistics, and numbers of statistics (#statistics). Specifically, γ, β denote the scale andshift parameters. μ, σ, Σ are a vector of means, a vector of standard deviations, and a covariance matrix. μ0represents the moving average. Moreover, p is the momentum of moving average, g in GN is the number ofgroups, e is a small value for numerical stability, and r, d are used in BRN. In SN, k ∈ Ω indicates a set ofdifferent kinds of statistics, Ω = {in, ln, bn}, and Wk is an importance weight of each kind.
Table 2: Comparisons of top-1 accuracies on the validation set of ImageNet, by using ResNet50 trained withSN, BN, and GN in different batch size settings. The bracket (∙, ∙) denotes (#GPUs, #SamPles per GPU).
Table 3: Faster R-CNN+FPN using ResNet50and FPN with 1x LR schedule. BNt represents BNis frozen. The best results are bold.
Table 4: Mask R-CNN using ResNet50 and FPNwith 2x LR schedule. BNt represents BN is frozenwithout finetuning. The best results are bold.
Table 5: Results in ADE20K validation set and C-ityscapes test set by using ResNet50 with dilated con-volutions. ‘ss’ and ‘ms’ indicate single-scale and multi-scale inference. SyncBN represents mutli-GPU synchro-nization of BN. SN finetunes from (8, 2) pretrained mod-el.
Table 6: Results of Kinetics dataset. In training,the clip length of 32 frames is regularly sampledwith a frame interval of 2. We study a batch sizeof 8 or 4 clips per GPU. BN is not synchronizedacross GPUs. SN finetunes from (8, 2) pretrainedmodel.
Table 7: Faster R-CNN for detection in COCO using ResNet50 and RPN. BNtrePreSentS BN is frozenwithout finetuning. The superscript ’1’ indicates the backbones are trained from scratch without pretraining onImageNet.
