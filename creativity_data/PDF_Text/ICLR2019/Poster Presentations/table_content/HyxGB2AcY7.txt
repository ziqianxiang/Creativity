Table 1: Performance of our method and its baselines on Atari games: maximum mean scores(averaged over 40 recent episodes) achieved over total 100M environment timesteps (400M frames)of training, averaged over 3 seeds. The best entry in the group of experiments without supervision isshown in bold. * denotes that A2C+CoEX+RAM acts as a control experiment, which includes somesupervision. More experimental results on A2C+CoEX+RAM are shown in Appendix C.
Table 2: Performance of our method and state-of-the-art exploration methods on Atari games. Forfair comparison, we report the maximum mean score achieved over the specific number of timestepsduring training, averaged over 3 seeds. The best entry is shown in bold. Baselines (for reference) are:DDQN+ and A3C+ (Bellemare et al., 2016), TRPO-AE-SimHash (Tang et al., 2017), Sarsa-φ-EB(Martin et al., 2017), DQN-PixelCNN (Ostrovski et al., 2017), and Curiosity-Driven (Burda et al.,2018). The numbers for DDQN+ were taken from (Tang et al., 2017) or were read from a plot.
Table 3: Performance of PPO and PPO+CoEX: maximum mean scores (average over 40 recentepisodes) achieved over total 500M environment steps (2B frames) of training, averaged over 3 seeds.
Table 4: Network architecture and hyperparametersHyperparameters	Value	Policy and Value Network Architecture	Input: 84x84x1		- Conv(32-8x8-4)	/ReLU	- Conv(64-4x4-2)	/ReLU	- Conv(64-3x3-1)	/ReLU	- FC(512) - FC(|A|), FC(1)	/ReLUADM Encoder Architecture	Input: 160x160x3		- Conv(8-4x4-2)	/LeakyReLU	- Conv(8-3x3-2)	/LeakyReLU	- Conv(16-3x3-2)	/LeakyReLU	- Conv(16-3x3-2)	/LeakyReLUMLP Architecture for et (i, j)	FC(1296,256)	/ReLU	- FC(256,128) - FC(128,|A|)	/ReLUMLP Architecture for αet (i, j)	FC(1296,64)	/ReLU	- FC(64,64) - FC(64,1)	/ReLUλent for Loss	0.001	A2C Discount Factor γ	0.99	Learning Rate (RMSProp)	0.0007	Number of Parallel Environments	16	
Table 5: The list of hyperparameters used for A2C+CoEX in each game. For the four games wherethere is no change of high-level visual context (Freeway, Frostbite, Qbert and Seaquest), wedo not include c in the state representation ψ(s), hence there is no τ. The same values of τ are usedin PPO+CoEX.
Table 6: Summary of the results of the ablation study of the state representation. We report themaximum mean score (averaged over 40 recent episodes) achieved over 100M environment steps,averaged over 3 random seeds.
