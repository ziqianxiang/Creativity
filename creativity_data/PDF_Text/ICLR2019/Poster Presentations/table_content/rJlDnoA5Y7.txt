Table 1: Translation quality experiments (BLEU scores) on IWSLT16 datasetsTable 2 shows results on WMT’16 test set in terms of BLEU and METEOR (Denkowski & Lavie,2014) trained only for best-performing setups in table 1. METEOR uses paraphrase tables andWordNet synonyms for common words. This may explain why METEOR scores, unlike BLEU,close the gap with the baseline models: as we found in the qualitative analysis of outputs, our modelsoften output synonyms of the reference words, which are plausible translations but are penalized byBLEU. 13 Examples are included in the Appendix.
Table 2: Translation quality experiment on WMT16 de-enTraining Time Table 4 shows the average training time per batch. In figure 1 (left), we show howmany samples per second our proposed model can process at training time compared to the baseline.
Table 3: Total convergence times in hours(h)/days(d).
Table 4: Comparison of number of parameters needed for input and output layer, train time perbatch (with batch size of 64) for IWSLT16 fr-en. Numbers in parentheses indicate the fraction ofparameters compared to word/word baseline model.
Table 5: Test set unigram F1 scores of occurrence in the predicted sentences based on their frequen-cies in the training corpus for different models for fr-en.
Table 6: Comparison of softmax alternatives. Red denotes worse than softmax, green denotes betterthan softmax (fractional improvements) and blue denotes huge improvement (more than 2X) oversoftmax.
Table 7: Hyperparameters DetailsPyTorch	0.3.0CPU	Intel(R) Xeon(R) CPU- 2.40GHz (32 Cores)RAM	^T90G#GPUs/experiment	GPU	GeForce GTX TITAN X1Table 8: Infrastructure details. All the exper-iments were run with this configuration8.2	Gradient Computation for NLLvMF lossNLLvMF loss is given asNLLvMF(e；e(w)) = -log(Cmkek)- eτe(w),where Cm (κ) is given as:Cm(κ)Km/2T(2n)m/2Im/2-i(K).
Table 8: Infrastructure details. All the exper-iments were run with this configuration8.2	Gradient Computation for NLLvMF lossNLLvMF loss is given asNLLvMF(e；e(w)) = -log(Cmkek)- eτe(w),where Cm (κ) is given as:Cm(κ)Km/2T(2n)m/2Im/2-i(K).
Table 9: Translation quality experiments using beam search with BPE based baseline models with abeam size of 5With our proposed models, in principle, it is possible to generate candidates for beam search byusing K -Nearest Neighbors. But how to rank the partially generated sequences is not trivial (onecould use the loss values themselves to rank, but initial experiments with this setting did not resultin significant gains). In this work, we focus on enabling training with continuous outputs efficientlyand accurately giving us huge gains in training time. The question of decoding with beam searchrequires substantial investigation and we leave it for future work.
Table 10: Translation examples. Red and blue colors highlight translation errors; red are bad andblue are outputs that are good translations, but are considered as errors by the BLEU metric. Oursystems tend to generate a lot of such “meaningful” errors.
Table 11: Example of fluency errors in the baseline model. Red and blue colors highlight translationerrors; red are bad and blue are outputs that are good translations, but are considered as errors by theBLEU metric.
