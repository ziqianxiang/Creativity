Table 1: Computational complexity of dimension-reduction search. MMACs denotes mega-MACsand BL denotes baseline._________________________________________________________________Layers	Dimension					Operations (MMACs)				nPQ, nCRS, nK	BL	0.3	0.5	0.7	0.9	BL	0.3	0.5	0.7	0.91024, 1152, 128	1152	539	232	148	TTF	^T44^	67.37	29	18.5	14.88256, 1152, 256	1152	616	266	169	^T36^	~2Γ~	38.5	16.63	10.56	8.5256, 2304, 256	2304	616	266	169	^T36^	^T44^	38.5	16.63	10.56	8.564, 2304, 512	2304	693	299	190	^T5^	~7Γ~	21.65	9.34	5.94	4.8164, 4608, 512	4608	693	299	190	^T54^	^T44^	21.65	9.34	5.94	4.81Furthermore, we investigate the influence of the on the computation cost of dimension-reductionsearch for importance estimation. We take several layers from the VGG8 on CIFAR10 as a casestudy, as shown in Table 1. With larger, the dimension-reduction search can achieve lower dimen-sion with much fewer operations. The average reduction of the dimension is 3.6x ( = 0.3), 8.5x( = 0.5), 13.3x ( = 0.7), and 16.5x ( = 0.9). The resulting operation reduction is 3.1x, 7.1x,11.1x, and 13.9x, respectively.
Table 2: Comparison with other structured Sparsification methods for inference. All the results arefrom VGG16 on ImageNet, and the default accuracy is top-1 accuracy. The baseline methods areTaylor Expansion (Molchanov et al., 2016), ThiNet (Luo et al., 2017), Channel Pruning (Hu et al.,2018), AUtoPnmner (LUo & Wu, 2018), and AMC (He et at, 2018b).__________________________________________Methods	Taylor Expansion	ThiNet	Channel Pruning	AutoPrunner	AMC	DSGOperation Sparsity	62.86%	69.81%	69.32%	73.6%	80%	62.92%Accuracy	87%(top-5)	67.34%	70.42%	68.43%	69.1%	71.44%(top-l) 90.56%(top-5)To guarantee the fairness, all the results are from the same network (VGG16) on the same dataset(ImageNet). Since our DSG produces structured sparsity, we also select structured sparsity workas comparison baselines. Different from the previous experiments in this paper, we further take theinput sparsity at each layer into account rather than only count the output sparsity. This is due tothe fact that the baselines consider all zero operands. The results are listed in Table 2, from whichwe can see that DSG is able to achieve a good balance between the operation amount and modelaccuracy.
