Table 1: The table shows error rates on the ILSVRC-2012 validation set of our proposed fullyinvertible RevNet compared to a VGG (Simonyan & Zisserman, 2014) and two ResNet (He et al.,2016) variants, as well as an iRevNet (Jacobsen et al., 2018) with a non-invertible final projectiononto the logits. Our proposed fully invertible RevNet performs roughly on par with others.
Table 2: Results comparing cross-entropy training (CE) with independence cross-entropy training(iCE) from Definition 5 and two architectures from the literature. The accuracy of the logit classi-fiers is on par for the CE and iCE networks, but the train error is higher for CE compared to testerror, indicating less overfitting for iCE. Further, a classifier independently trained on the nuisancevariables is able to reach even smaller error than on the logits for CE, but just 27.70% error for iCE,indicating that we have successfully removed most of the information of the label from the nuisancevariables and fixed the problem of excessive invariance to semantically meaningful variability withno cost in test error.
