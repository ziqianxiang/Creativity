Table 1: Table summarizing the categorization ofdifferent kinds of feature spaces considered.
Table 2: These results compare the mean reward (Â± std-error) after 100 million frames across 3seeds for an agent trained with intrinsic plus extrinsic reward versus extrinsic reward only. Theextrinsic (coefficient 1.0) and intrinsic reward (coefficient 0.01) were directly combined without anyhyper-parameter tuning. We leave the question on how to optimally combine extrinsic and intrinsicrewards up to future work. This is to emphasize that combining extrinsic with intrinsic rewards isnot the focus of the paper, and these experiments are provided just for completeness.
