Table 1: Reward in DMLab tasks (mean ± std) for all compared methods. Higher is better. “ECO”stands for the online version of our method, which trains R-network and the policy at the sametime. We report Grid Oracle reward in tasks with no reward. The Grid Oracle method is given forreference — it uses privileged information unavailable to other methods. Results are averaged over30 random seeds. No seed tuning is performed.
Table S1: Learning locomotion for MuJoCo Ant. For “No reward”, the task reward is 0 (so plainPPO is a random policy), and Grid Oracle rewards are reported (with cell size 5). Results areaveraged over 30 random seeds for “No reward” and over 10 random seeds for “Escape Circle”. Noseed tuning is performed.
Table S2: Hyper-parameters used for VizDoom environment.
Table S3: Hyper-parameters used for DMLab environment.
Table S4:	Hyper-parameters used for MuJoCo Ant “No Reward” environment. For the PPO+1baseline, the curiosity reward is substituted by +1 (optimizes for survival). The curiosity bonusscale is applied to this reward.
Table S5:	Hyper-parameters used for MuJoCo Ant “Escape Circle” environment. For the PPO+1baseline, the curiosity reward is substituted by +1 (optimizes for survival). The curiosity bonusscale is applied to this reward.
Table S6:	Reward on the tasks “No Reward” and “Very Sparse” using a universal R-network. Twobaselines (PPO and PPO + EC with a specialized R-network) are also provided.
Table S7:	Reward on the environments “No Reward” and “Very Sparse” (columns) when the R-network is trained on different environments (rows). We provide a result with a matching R-networkfor reference (bottom).
Table S8:	Reward in the “No Reward” and “Very Sparse“ tasks using different positive examplethresholds k when training the R-network.
Table S9:	Reward for different values of the memory size for the tasks “No Reward” and “VerySparse”.
Table S10: Reward of the policy trained on the “No Reward” and “Very Sparse“ tasks with anR-network trained using a varying number of environment interactions (from 100K to 5M).
Table S11: Reward on the “No Reward” and “Very Sparse“ tasks using ablated versions of theR-network.
Table S12: Reward in the randomized-TV versions of DMLab task “Sparse” (mean ± std) for allcompared methods. Higher is better. “Original” stands for the non-randomized standard version ofthe task which we used in the main text. “ECO” stands for the online version of our method, whichtrains R-network and the policy at the same time. The Grid Oracle method is given for reference —it uses privileged information unavailable to other methods. Results are averaged over 30 randomseeds. No seed tuning is performed.
Table S13: Reward in the randomized-TV versions of DMLab task “Very Sparse” (mean ± std)for all compared methods. Higher is better. “Original” stands for the non-randomized standardversion of the task which we used in the main text. “ECO” stands for the online version of ourmethod, which trains R-network and the policy at the same time. The Grid Oracle method is givenfor reference — it uses privileged information unavailable to other methods. Results are averagedover 30 random seeds. No seed tuning is performed.
