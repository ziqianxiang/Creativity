Table 1: Accuracy of approximation of mean and variance statistics for each layer in a fully trainedLeNet5 (MNIST) tested with noisy input. Observe the following: MC Std σ* is growing significantlyfrom the input to the output; both AP1 and AP2 have a significant drop of accuracy at linear (FCand Conv) layers, due to factorized approximation assumption; AP2 approximation of the standarddeviation is within a factor close to one, and makes a meaningful estimate, although degrading withdepth; AP2 approximation of the mean is more accurate than AP1; the KL divergence from the MCclass posterior is improved with AP2.
Table 2: Accuracy of approximation of mean and variance statistics for each layer in All-CNN(CIFAR-10) trained and tested with dropout. The table shows accuracies after all layers (C-convolution, A-activation, P-average pooling) and the final KL divergence. A similar effect to prop-agating input noise is observed: the MC std σ* grows with depth; a significant drop of accuracy isobserved in convolutional and pooling layers, which rely on the independence assumption.
Table 3: Results for All-CNN on CIFAR-10 test set: negative log likelihood (NLL) and accuracy.
Table C.1: Accuracy of approximation of mean and variance statistics for each layer in a fullytrained ConvPool-CNN-C network with dropout. A significant drop of accuracy is observed as wellafter max pooling, we believe due to the violation of the independence assumption.
