Table 1: Test perplexity on billion word. Adaptive inputs share parameters with an adaptivesoftmax. Training times of Char-CNN and Adaptive input models are measured when training with64 GPUs.
Table 2: Test perplexity on wikitext- 1 03 (cf. Table 1). Training time is based on 8 GPUs.
Table 3: Test perplexity on wikitext- 1 03 for various input and output layer factorizations. Train-ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regimebecause of large memory requirements: the maximum number of tokens per GPU was lowered to1024 from 4096 but the same number of updates were performed by processing four batches beforecommitting a weight update.
Table 4: Test perplexity on billion word. Training speed measured on four 8-GPU machines.
Table 5: Perplexity on wikitext- 1 03 with different context sizes during training and inference.
Table 6: Perplexity on wikitext- 1 03 when regularizing rare words in adaptive softmax.
Table 7: Validation perplexity of our models on wikitext- 1 03.
Table 8: Validation perplexity on wikitext- 1 03 with tied adaptive inputs & outputs. The bandssignify the number of words belonging to each band. In every case, the first band has dimension1024, the second band 256, the third band 64 and the fourth band (if it exists) 16.
