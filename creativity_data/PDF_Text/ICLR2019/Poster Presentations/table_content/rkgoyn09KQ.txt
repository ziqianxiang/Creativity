Table 1: Computation of hi and p(vi |VVi) in DocNADE,ctx-DocNADE and ctx-DocNADEe models, correspondinglyused in estimating log P(V) (Algorithm 1).
Table 2: Data statistics: Short/long texts and/or small/large corpora from diverse domains. Symbols- Avg:average, L: avg text length (#words), |RV | and |FV |: size of reduced (RV) and full vocabulary (FV), C:number of classes, Senti: Sentiment, Indus: Industrial, 'k':thousand and f: multi-label. For short-text, L<25.
Table 3: State-of-the-art comparison: IR (i.e, IR-precision at 0.02 fraction) and classification F1 for short texts,where Avg: average over the row values, the bold and underline: the maximum for IR and F1, respectively.
Table 4: IR-precision at fraction 0.02 and classification F1 for long textsModel PPLDocNADE 980ctx-DocNADE 968ctx-DocNADEe 966DocNADEctx-DocNADEctx-DocNADEe283276272DocNADE 1437ctx-DocNADE 1430ctx-DocNADEe 1427ModelDocNADEctx-DocNADEctx-DocNADEeDocNADEctx-DocNADE
Table 5: Generalization: PPLExperimental Setup: DocNADE is often trained on a reduced vocabulary (RV) after pre-processing(e.g., ignoring functional words, etc.); however, we also investigate training it on full text/vocabulary(FV) (Table 2) and compute document representations to perform different evaluation tasks. The FVsetting preserves the language structure, required by LSTM-LM, and allows a fair comparison ofDocNADE+FV and ctx-DocNADE variants. We use the glove embedding of 200 dimensions. Allthe baselines and proposed models (ctx-DocNADE, ctx-DocNADEe and ctx-DeepDNEe) were runin the FV setting over 200 topics to quantify the quality of the learned representations. To betterinitialize the complementary learning in ctx-DocNADEs, we perform a pre-training for 10 epochswith 位 set to 0. See the appendices for the experimental setup and hyperparameters for the followingtasks, including the ablation over 位 on validation set.
Table 6: Average coherence for short and long texts over 200 topics in FV setting, where DocNADE o DNEWe run TDLM3 (Lau et al., 2017) for all the short-text datasets to evaluate the quality of representa-tions learned in the spare data setting. For a fair comparison, we set 200 topics and hidden size, andinitialize with the same pre-trained word embeddings (i.e., glove) as used in the ctx-DocNADEe.
Table 7: (Left): Topic coherence (NMPI) scores of different models for 50, 100 and 150 topics on BNC dataset.
Table 8: A topic of 20NS dataset with coherence Table 9: Illustration of the top-3 retrievals for an input queryAdditionally, we show the quality of representations learned at different fractions (20%, 40%, 60%,80%, 100%) of training set from TMNtitle data and use the same experimental setup for the IR andclassification tasks, as in section 3.3. In Figure 4, we quantify the quality of representations learnedand demonstrate improvements due to the proposed models, i.e., ctx-DocNADE and ctx-DocNADEeover DocNADE at different fractions of the training data. Observe that the gains in both the tasksare large for smaller fractions of the datasets. For instance, one of the proposed models, i.e., ctx-DocNADEe (vs DocNADE) reports: (1) a precision (at 0.02 fraction) of 0.580 vs 0.444 at 20% and0.595 vs 0.525 at 100% of the training set, and (2) an F1 of 0.711 vs 0.615 at 20% and 0.726 vs0.688 at 100% of the training set. Therefore, the findings conform to our second contribution ofimproving topic models with word embeddings, especially in the sparse data setting.
Table 10: SiROBs data: Example Documents (Requirement Objects) with their types (label).
Table 11: Hyperparameters in Generalization in the DocNADE and ctx-DocNADE variants for 200 topicsThe SiROBs is our industrial corpus, extracted from industrial tender documents. The documentscontain requirement specifications for an industrial project for example, railway metro construction.
Table 12: Hyperparameters in the Document Retrieval task.
Table 13: Perplexity scores for different 位 in Generalization task: Ablation over validation setlabels are not used during training. The class labels are only used to check if the retrieved documentshave the same class label as the query document. To perform document retrieval, we use the sametrain/development/test split of documents discussed in data statistics (experimental section) for allthe datasets during learning.
Table 14: 位 for IR task: Ablation over validation set at retrieval fraction 0.02for the logistic regression classifier). Similarly, for the IR task, similarities were computed based onthe inferred relative topic distribution.
