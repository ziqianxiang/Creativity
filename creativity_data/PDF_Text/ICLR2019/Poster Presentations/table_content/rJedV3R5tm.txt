Table 1: The NLLoracle scores on synthetic data where βmax = 1 for length 20 and βmax = 2 for length 40.
Table 2: The BLEU and NLLgen scores on COCO Image Captions where βmax = 100 and 1000, respectively.
Table 3: The BLEU and NLLgen scores on EMNLP2017 WMT News where βmax = 100 and 1000, respectively.
Table 4: The means and standard deviations of human scores for RelGAN and other models on EMNLP2017WMT News by using Amazon Mechanical Turk. Note that “Real” denotes the human score on the real dataset.
Table 5: The human evaluation scale from 1 to 5 with corresponding criteria and example sentences.
Table 6: Randomly chosen 15 generated samples trained on COCO image caption data with βmax = 100 (toprow) and βmax = 1000 (bottom row). We can see that for βmax = 100, the words “man” and “kitchen” occursevere times even in these small set of generated sentences, meaning some sort of lacking sentence diversity.
Table 7: Randomly chosen 15 generated samples trained on EMNLP WMT News dataset with βmax = 100 (toprow) and βmax = 1000 (bottom row). We can see that in both cases, there is no obvious sign of poor sentencediversity by human eyes.
Table 8: The BLEU and NLLgen scores of RelGAN without pre-training on COCO Image Captions where witha little hyperparameter tuning, we set βmax = 100 for the standard GAN loss and βmax = 1000 for other losses.
Table 9: The BLEU and NLLgen scores of RelGAN with various values of βmax on COCO Image Captions.
