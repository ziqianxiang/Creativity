Table 1: The classification error rate (lower is better) on three datasets with different objectivefunctions and different neural network architectures. CE denotes that the network uses class-specificlabels for training with a multi-class cross-entropy. MCL only uses the binarized similarity forlearning with the meta-classification criterion. KCL is a strong baseline which also uses binarizedsimilarity. The * symbol indicates the worst cases of KCL. The performance in parenthesis meansits network uses a better initialization (VGG16 and VGG8) or a learning schedule which is 10times longer (VGG11). The two treatments are discussed in Section 5.2.1. We only use VGG8 forCIFAR100 since KCL performs the best with it on CIFAR10. Each value is the average of 3 runs.
Table 2: Unsupervised cross-task transfer learning on Omniglot. The performance (higher is better)is averaged across 20 alphabets (datasets), in which each has 20 to 47 letters (classes). The ACC andNMI without brackets have the number of output nodes K equal to the true number of classes in adataset, while columns with "(K=100)" represent the case where the number of classes is unknownand a fixed K = 100 is used.
Table 3: Unsupervised cross-task transfer learning on ImageNet. The values (higher is better) arethe average of three random subsets in ImageN et118. Each subset has 30 classes. The "ACC" hasK = 30. All methods use the features (outputs of average pooling) from Resnet-18 pre-trained withI mageN et882 classification.
Table 4: Test error rates (lower is better) obtained by various semi-supervised learning approacheson CIFAR-10 with all but 4,000 labels removed. Supervised refers to using only 4,000 labeledsamples from CIFAR-10 without any unlabeled data. All the methods use ResNet-18 and standarddata augmentation.
Table 5: Estimates for the number of characters across the 20 datasets in Omnigloteval when Cis unknown. The bold number means the prediction has error smaller or equal to 3. The numberof dominant clusters is defined by NDC = PiK=1 [Ci >=E[Ci]], where [âˆ™] is an Iverson Bracketand Ci is the size of cluster i. For example, E[Ci] will be 10 if the alphabet has 1000 images andK = 100. The ADif represents average difference (Hsu et al., 2018).
