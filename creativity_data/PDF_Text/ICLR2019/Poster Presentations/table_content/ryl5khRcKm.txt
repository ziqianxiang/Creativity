Table 1: Performance comparison of different CNN architectures for protein localization. Li-imatainen et al. (2018) state the winning results of the “Cyto Challenge 2017” on the unpublishedofficial test set. For each method, the table reports the average F1 score and its standard devia-tion across 13 prediction tasks. The third column, p-value, reports the result of a one-sided pairedWilcoxon test with the null hypothesis that GapNet-PL and the respective method perform equallyacross prediction tasks. The consecutive columns report average precision, recall and area underROC (AUC). GapNet-PL has outperformed all competing methods.
Table 2: Predictive performance of GapNet-PL, 3 human experts and the group of scholars. Thetable shows the overall accuracy and average F1 score, precision and recall over 13 tasks. In eachcolumn, the best performing method is written bold. Additionally, results of ensembles of expertsand scholars are given. An ensemble is calculated by majority vote. In case of ties, one of the tiedclasses was chosen randomly. We repeated this random sampling 1,000 times, such that the providedestimates for the ensembles are averages over those 1,000 samplings.
Table 3: Confusion matrix for comparison of GapNet-PL with human experts. p-values are calcu-lated with a McNemar’s test for equality of row and column marginal frequencies.
Table 4: DenseNet architecture variants (Huang et al., 2017). The authors present different variationsof the architecture. Due to memory restrictions the smallest variant, DenseNet-121, was chosen forthe comparison in this work. The reduction rate for the 1x1 convolution transition layer is set to 0.5and the growth rate is k = 32.
Table 5: The architecture configuration taken from Kraus et al. (2017) and the adaptations for ourcomparison.
Table 6: Training times per epoch in seconds and number of parameters for all compared methods.
Table 7: F1 scores per task for each method. Overall, GapNet-PL performs best for 9 tasks, M-CNNfor 3 tasks and DenseNet wins one task.
