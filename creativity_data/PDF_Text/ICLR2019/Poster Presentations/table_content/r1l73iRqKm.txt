Table 1: Dataset statistics of the Wizard of Wikipedia task.
Table 2: Test performance of various methods on the Knowledge Selection Task. The modelsmust select the gold knowledge sentences chosen by humans given the dialogue context.
Table 3: Retrieval methods on the full Wizard task. Models must select relevant knowledge andretrieve a response from the training set as a dialogue response. Using knowledge always helps, andthe Transformer Memory Network with pretraining performs best.
Table 4: Generative models on the full Wizard Task. The Two-stage model performs best usingpredicted knowledge, while the End-to-end (E2E) model performs best with gold knowledge.
Table 5: Human Experiments. Evaluations of the best generative and retrieval models on fulldialogues with humans. Human ratings are reported as mean (stddev). Wiki F1 measures unigramoverlap with the Wikipedia entry for the chosen topic, a measure of knowledge used in conversations.
Table 6: Test performance of the Knowledge Selection Tasks. We also tested the performance ofour models trained to do the full dialogue task (see Section 5.2) on the knowledge selection task. Forour retrieval system, this refers to the performance of the knowledge attention. The results show thatour retrieval system could be improved, and the auxiliary loss clearly helps the generative models.
Table 7: Retrieval methods on the full Wizard task. In addition to the models we tested in thepaper, we also tested a two-stage retrieval system in which we used our best-performing model onthe knowledge selection task to choose a single knowledge sentence to condition on for the dialogueretrieval task. This outperformed our best retrieval method in terms of F1 but not not in termsof Recall@1. Furthermore, we compared these results to a two-stage retrieval system in whichthe dialogue retrieval module is optimized for seeing the gold chosen knowledge sentence. Theperformance of this system on the gold knowledge task suggests that the retrieval system could beimproved by increasing performance on the knowledge selection subtask.
Table 8: Human Experiments. We calculate the Wiki F1 score for the wizard and apprentice asthey appear in the dataset for the sake of comparison to our human evaluations. Note that thisdiffered from the human-human evaluation set-up in the sense that the wizard had direct access toWikipedia passages in the UI, which explains the higher values of Wiki F1 both for the wizard (whouses Wikipedia) and for the apprentice (who would likely reference that use).
