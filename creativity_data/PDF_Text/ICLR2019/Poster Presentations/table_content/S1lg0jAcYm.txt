Table 1:	The constructions of three differently structured discrete variational auto-encoders. The followingSymbols "7"，“]”，)”，and “ -→ ” represent deterministic linear transform, leaky rectified linear units (LeakyReLU)(Maas et al., 2013) nonlinear activation, sigmoid nonlinear activation, and random sampling respectively, in theencoder (a.k.a. recognition network); their reversed versions are used in the decoder (a.k.a. generator).
Table 2:	Test negative log-likelihoods of discrete VAEs trained with a variety of stochastic gradient estimatorson MNIST-StatiC and OMNIGLOT, where *, ?, f, ∣ represent the results reported in Mnih & Gregor (2014),Tucker et al. (2017), Gu et al. (2016), and Grathwohl et al. (2018), respectively. The results for LeGrad (Titsias& Lazaro-Gredilla, 2015) are obtained by running the code provided by the authors. We report the results ofARM using the sample mean and standard deviation over five independent trials with random initializations.
Table 3:	Comparison of the test negative log-likelihoods between ARM and various gradient estimators in Janget al. (2017), for the MNIST conditional distribution estimation benchmark task.
Table 4: Test negative ELBOs of disCrete VAEs trained with four different stoChastiC gradient estimators.
