Table 1: Results (accuracy) for L1-norm based filter pruning (Li et al., 2017). “Pruned Model” is the modelpruned from the large model. Configurations of Model and Pruned Model are both from the original paper.
Table 2: Results (accuracy) for ThiNet (Luo et al., 2017). Names such as “VGG-GAP” and “ResNet50-30%”are pruned models whose configurations are defined in Luo et al. (2017). To accommodate the effects ofdifferent frameworks between our implementation and the original paper’s, we compare relative accuracy dropfrom the unpruned large model. For example, for the pruned model VGG-Conv, -1.23 is relative to 71.03 onthe left, which is the reported accuracy of the unpruned large model VGG-16 in the original paper; -2.75 isrelative to 71.51 on the left, which is VGG-16’s accuracy in our implementation.
Table 3: Results (accuracy) for Regression based Feature Reconstruction (He et al., 2017b). Pruned modelssuch as “VGG-16-5x” are defined in He et al. (2017b). Similar to Table 2, we compare relative accuracy dropfrom unpruned large models.
Table 4: Results (accuracy) for Network Slimming (Liu et al., 2017). “Prune ratio” stands for total percentageof channels that are pruned in the whole network. The same ratios for each model are used as the original paper.
Table 5: Results (accuracy) for residual block pruning using Sparse Structure Selection (Huang & Wang, 2018).
Table 6: Results (accuracy) for unstructured pruning (Han et al., 2015). “Prune Ratio” denotes the percentageof parameters pruned in the set of all convolutional weights.
Table 7: Network architectures obtained by pruning 60% chan-nels on VGG-16 (in total 13 conv-layers) using Network Slim-ming. Width and Width* are number of channels in the originaland pruned architectures, averaged over 5 runs.
Table 8: Comparisons with the Lottery Ticket Hypothesis (Frankle & Carbin, 2019) for one-shot unstructuredpruning (Han et al., 2015) with two initial learning rates: 0.1 and 0.01. The same results are visualized inFigure 7b. Using the winning ticket as initialization only brings improvement when the learning rate is small(0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).
Table 9: Experiments on the Lottery Ticket Hypothesis (Frankle & Carbin, 2019) on a structured pruningmethod (L1-norm based filter pruning (Li et al., 2017)) with two initial learning rates: 0.1 and 0.01. In bothcases, using winning tickets does not bring improvement on accuracy.
Table 10: Results (accuracy) for Soft Filter Pruning (He et al., 2018a) without pretrained models.
Table 11: Results (accuracy) for Soft Filter Pruning (He et al., 2018a) using pretrained models.
Table 12: Results (mAP) for pruning on detection task. The pruned models are chosen from Li et al. (2017).
Table 13: Results (accuracy) for Network Slimming (Liu et al., 2017) when the models are aggressively pruned.
Table 14: Results (accuracy) for L1 -norm based filter pruning (Li et al., 2017) when the models are aggressivelypruned.
Table 15: Results (accuracy) for unstructured pruning (Han et al., 2015) when the models are aggressivelypruned.
Table 16: “Fine-tune-40” stands for fine-tuning 40 epochs and so on. Scratch-E models are trained for 160epochs. We observe that fine-tuning for more epochs does not help improve the accuracy much, and modelstrained from scratch can still perform on par with fine-tuned models.
Table 17: Results for L1-norm filter pruning (Li et al., 2017) when the training schedule of the large model isextended from 160 to 300 epochs.
Table 18: Sparsity patterns of PreResNet-164 pruned on CIFAR-10 by Network Slimming shown in Figure 5(left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio ofchannels to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform(across stages).
Table 19: Average sparsity patterns of 3×3 kernels of PreResNet-110 pruned on CIFAR-100 by unstructuredpruning shown in Figure 5 (middle) under different prune ratio. The top row denotes the total prune ratio. Thevalues denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patternsare close to uniform (across stages).
Table 20: Average sparsity patterns of 3×3 kernels of DenseNet-40 pruned on CIFAR-100 by unstructuredpruning shown in Figure 5 (right) under different prune ratio. The top row denotes the total prune ratio. Thevalues denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patternsare close to uniform (across stages).
Table 21: Sparsity patterns of VGG-16 pruned on CIFAR-10 by Network Slimming shown in Figure 3 (left)under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channelsto be kept. For each prune ratio, the latter stages tend to have more redundancy than earlier stages.
