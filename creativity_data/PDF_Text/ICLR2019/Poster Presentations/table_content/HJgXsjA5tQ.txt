Table 1: Test accuracy (%) of CNN13 on MNIST dataset. CNN13 denotes the original architecturefrom Table 3 while CNN13-skip denotes the corresponding skip-model. There are in total 179, 840hidden neurons from the original CNN13 (see Table 3), out of which we choose a random subset ofN = 55, 000 neurons to connect to the output layer to obtain CNN13-skip.
Table 2: Traning and test accuracy of several CNN architectures with/without skip-connections onCIFAR10 (no data-augmentation). For each original model A, A-skip denotes the correspondingskip-model in which a subset of N hidden neurons “randomly selected” from the hidden layers areconnected to the output units. For Densenet121, these neurons are randomly chosen from the firstdense block. The names in open brackets (rand/SGD) specify how the networks are trained: rand(U is randomized and fixed while V is learned with SGD), SGD (both U and V are optimized withSGD). Additional experimental results with data-augmentation are shown in Table 5 in the appendix.
Table 3: The architecture of CNN13 for MNIST dataset. There are in total 179, 840 hidden neurons.
Table 4: Test accuracy (%) of VGG networks from Table 2 where max-pooling layers are replaced by2x2 convolutional layers of stride 2 (denoted as mp2conv). Other notations are similar to Table 2.
Table 5: Test accuracy (%) of several CNN architectures with/without skip-connections on CIFAR10(+ denotes data augmentation). For each model A, A-skip denotes the corresponding skip-model inwhich a subset of N hidden neurons “randomly selected” from the hidden layers are connected to theoutput units. For Densenet121, these neurons are randomly chosen from the first dense block. Thenames in open brackets (rand/SGD) specify how the networks are trained: rand (U is randomizedand fixed while V is learned with SGD), SGD (both U and V are optimized with SGD).
