Table 1: Results on CIFAR-10 with ResNet-110 (mean/median of 5 runs; lower is better).
Table 2: ImageNet test results using the ResNet architecture. (Lower is better.)4.3	Machine translationTo demonstrate the generality of Fixup, we also apply it to replace layer normalization (Ba et al.,2016) in Transformer (Vaswani et al., 2017), a state-of-the-art neural network for machine trans-lation. Specifically, we use the fairseq library (Gehring et al., 2017) and follow the Fixup tem-plate in Section 3 to modify the baseline model. We evaluate on two standard machine translationdatasets, IWSLT German-English (de-en) and WMT English-German (en-de) following the setupof Ott et al. (2018). For the IWSLT de-en dataset, we cross-validate the dropout probability from{0.3, 0.4, 0.5, 0.6} and find 0.5 to be optimal for both Fixup and the LayerNorm baseline. For theWMTâ€™16 en-de dataset, we use dropout probability 0.4. All models are trained for 200k updates.
Table 3: Comparing Fixup vs. LayerNorm for machine translation tasks. (Higher is better.)5	Related WorkNormalization methods. Normalization methods have enabled training very deep residual net-works, and are currently an essential building block of the most successful deep learning architec-tures. All normalization methods for training neural networks explicitly normalize (i.e. standardize)some component (activations or weights) through dividing activations or weights by some real num-ber computed from its statistics and/or subtracting some real number activation statistics (typicallythe mean) from the activations.6 In contrast, Fixup does not compute statistics (mean, variance ornorm) at initialization or during any phase of training, hence is not a normalization method.
Table 4: Additional results on CIFAR-10, SVHN datasets.
