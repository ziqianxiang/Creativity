Table 1: average distances, δ-hyperbolicities and ratios computed via sampling for the metrics inducedby different h functions, as defined in Eq. (7).
Table 2: Word similarity results for 100-dimensional models. Highlighted: the best and the 2nd best.
Table 3: Nearest neighbors (in terms of Poincare distance) for some words using our 100D hyperbolicembedding model.
Table 4: Word analogy results for 100-dimensional models. Highlighted: the best and the 2nd best.
Table 5: Some words selected from the 100 nearest neighbors and ordered according to the hypernymyscore function for a 50x2D hyperbolic embedding model using h(x) = x2 .
Table 6: Hyperlex results in terms of Spearman correlation for different model types ordered accordingto their difficulty.
Table 7: WBLESS results in terms of accuracy for different model types ordered according to theirdifficulty.
Table 8: Unrestricted (190k) similarity results: models were trained and evaluated on the unrestricted(190k) vocabulary 一 “(init)” refers to the fact that the model was initialized with its counterpart (i.e.
Table 9: Restricted (50k) similarity results: models were trained and evaluated on the restricted (50k)vocabulary - except for the “Vanilla (190k)” baseline, which was trained on the unrestricted (190k)vocabulary and evaluated on the restricted vocabulary.
Table 10: Percentage of word pairs that are dropped when replacing the unrestricted vocabulary of190k words with the restricted one of the 50k most frequent words.
Table 11: Initial number of word pairs in each benchmark similarity dataset.
Table 12: Unrestricted (190k) analogy results: models were trained and evaluated on the unrestricted(190k) vocabulary - “(init)” refers to the fact that the model was initialized with its counterpart (i.e.
Table 13: Restricted (50k) analogy results: models were trained and evaluated on the restricted (50k)vocabulary - except for the “Vanilla (190k)” baseline, which was trained on the unrestricted (190k)vocabulary and evaluated on the restricted vocabulary.
Table 14: Number of test instances in the benchmark analogy datasets initially and after reductions tothe vocabularies of the most frequent 190k and 50k words respectively.
Table 15: Result of the 2-fold cross-validation to determine which t is best in mtd d (see section 6)to answer analogy queries. The (total) Google analogy dataset was randomly split in two partitions.
Table 16: Various properties of similarity benchmark datasets. The frequency index indicates therank of a word in the vocabulary in terms of its frequency: a low index describes a frequent word.
Table 17: Similarity results on the unrestricted (190k) vocabulary for various h functions. This tableshould be read together with Table 1.
