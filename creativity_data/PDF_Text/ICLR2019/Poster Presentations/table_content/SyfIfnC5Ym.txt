Table 1: The accuracy of defense methods on the testing datasets and the adversarial examplesgenerated by various adversaries.
Table 2: The local loss sensitivity analysis for defense methods.
Table 3: The MMD distance across domains in the logit space for defense methods on Fashion-MNIST. D denotes the distribution of the clean testing data; AF GSM and APGD denote the distri-butions of the adversarial testing data generated by the white-box FGSM and PGD, respectively.
Table 4: The accuracy of PAT and PATDA on the testing datasets and the adversarial examplesgenerated by various adversaries. The magnitude of perturbations in '∞ norm is 0.1 for Fashion-MNIST, 0.02 for SVHN, and 4/255 for CIFAR-10 and CIFAR-100.
Table 5: Common settings of attacks for all experimentsAttack	Parameter	NormFGSM PGD R+FGSM MIM	N/A	'∞ Iterated step k = 20, α = e/10	'∞ Random perturbation α = e/2	'∞ Iterated step k = 10, α = e/5, μ = 1.0	'∞A.2 neural network architectures and training hyper-parametersFashion-MNIST. In the training phase, we use Adam optimizer with a learning rate of 0.001 andset the batch size to 64. For Fashion-MNIST, the neural network architectures for the main model,the static pre-trained models and the model held out during training are depicted in Table 6. For alladversarial training methods, the magnitude of perturbations is 0.1 in '∞ norm.
Table 6: Neural network architectures used for the Fashion-MNIST and SVHN datasets. Conv:convolutional layer with Relu, FC: fully connected layer.
Table 7: Neural network architectures used for the CIFAR-10 dataset. Conv: convolutionallayer with Group Normalization and ELU; GAP: global average pooling.
Table 8: Neural network architectures used for the CIFAR-100 dataset. Conv: convolutionallayer with Group Normalization and ELU; GAP: global average pooling.
