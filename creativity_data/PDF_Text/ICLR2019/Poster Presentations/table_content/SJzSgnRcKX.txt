Table 1: Example sentence, spans, and target label for each task. O = OntoNotes, W = Winograd.
Table 2: Comparison of representation models and their respective lexical baselines. Numbersreported are micro-averaged F1 score on respective test sets. Lex. denotes the lexical baseline (ยง 4)for each model, and bold denotes the best performance on each task. Lines in italics are subsetsof the targets from a parent task; these are omitted in the macro average. SRL numbers considercore and non-core roles, but ignore references and continuations. Winograd (DPR) results are theaverage of five runs each using a random sample (without replacement) of 80% of the training data.
Table 3: For each probing task, corpus summary statistics of the number of labels, examples, tokensand targets (split by train/dev/test). Examples generally refer to sentences. For semantic role label-ing, they instead refer to the total number of frames. Targets refer to the total number of classificationtargets (edges or spans, as described in Table 1 and Section 2). For SemEval relation classificationthere is no standard development split, so we use a fixed subset of 15% of the training data and usethe remaining 85% to train.
