Table 1: Results on graph transduction tasks. We have used the same setup that is described in(VeliCkOViC et al., 2017). H-GAT refers to our graph attention network with hyperbolic attentionmechanism. Table shows the mean performance over 100 random seeds, along with 95% confidenceinterVals for this estimate.
Table 2: Results for the WMT14 English to German translation task. Results are computed followingthe procedure in Vaswani et al. (2017). Citations indicate results taken from the literature. Latestis the result of training anew model using an unmodified version of the same code where we addedhyperbolic attention (we have observed that the exact performance of the transformer on this task variesas the Tensor2tensor codebase evolves).
