Table 1: Dataset statistics.
Table 2: Comparison of average accuracy (AT) and worst-case forgetting (Fwst) on the EpisodicMemory (M) and Test Set (DEV ).
Table 3: Comparison of different variations of GEM on MNIST Permutations and Split CIFAR.
Table 4: Comparison with different baselines on Permuted MNIST and Split CIFAR. The value of∞ is assigned to a metric when the model fails to train with the cross-validated values of hyper-parameters found on the subset of the tasks as discussed in Sec. 2 of the main paper. The numbersare averaged across 5 runs using a different seed each time. The results from this table are used togenerate Fig 1 in Sec. 6.1 of the main paper.
Table 5: Average accuracy and forgetting of standard models (left) and joint embedding models(right) on Split CUB. The value of ‘OoM’ is assigned to a metric when the model fails to fit in thememory. The numbers are averaged across 10 runs using a different seed each time. The resultsfrom this table are used to generate Fig 2 in Sec. 6.1 of the main paper.
Table 6: Average accuracy and forgetting of standard models (left) and joint embedding models(right) on Split AWA. The value of ‘OoM’ is assigned to a metric when the model fails to fit in thememory. The numbers are averaged across 10 runs using a different seed each time. The resultsfrom this table are used to generate Fig 2 in Sec. 6.1 of the main paper.
Table 7: Computational cost and memory complexity of different LLL approaches. The timing refersto training time on a GPU device. Memory cost is provided in terms of the total number of parame-ters P, the size of the minibatch B, the total size of the network hidden state H (assuming all methodsuse the same architecture), the size of the episodic memory M per task. The results from this tableare used to generate Fig. 1 and 2 in Sec. 6.1 of the main paper.
