Table 1: Top-1 and top-5 classification error of networks along with the fraction of human ClickMemap variability explained by their features (maps; 100 corresponds to the average similarity betweenhuman ClickMe maps). Performance is reported on the test set of ClickMe. ** denotes P <0.01(statistical testing captures the proportion of image feature maps that exceed the null inter-participantreliability score; see Appendix for details).
Table 1: GALA-ResNet-50 trained with ClickMe selects more similar features to human observersthan the GALA-ResNet-50 trained without ClickMe (the respective fraction of explained ClickMemap variability is 88.56 vs. 64.21, p <0.001 according to a randomization test on the difference inper-image scores between the models, as in Edgington, 1964).
Table 2: ILSVRC12 validation set accuracy for published reference models (Hu et al., 2017)and our re-implementations. Models were evaluated on 224 × 224 image crops from the origi-nal ILSVRC12 encoded into sharded TFRecords (http://serre-lab.clps.brown.edu/resource/clickme).
Table 3: Networks’ classification error and fraction of explained human ClickMe map variability on224 × 224 center crops from the ClickMe test set. The dataset can be downloaded from http://Serre-lab.clps.brown.edu/resource/clickme. ** denotesP <0.01 (statistical testingcaptures the proportion of feature maps that exceed null inter-participant reliability, detailed inInter-rater reliability of the ClickMe maps).
Table 4: Networks trained on the full ILSVRC12 training set, with ClickMe supervision on only asubset of these images (〜16% of all images). This experiment demonstrates that ClickMe maps arenot needed for all training samples to benefit from ClickMe supervision. Network classification erroris reported on 224 × 224 center crops from ILSVRC12 validation5. The dataset can be download athttp://serre-lab.clps.brown.edu/resource/clickme.
