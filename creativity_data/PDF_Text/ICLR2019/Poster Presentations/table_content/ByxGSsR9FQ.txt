Table 1: Accuracies of MNIST classifiers under white-box non-targeted attacks with noise L2-normlimit of 3. MaxIter is the max number of iterations the attacker uses. Model 1 is an ordinarily trainedmodel. Model 2 is the model from Madry et al. (2017). Model 3 is L2NNN without adversarialtraining. Model 4 is L2NNN with adversarial training.
Table 2: Accuracies of CIFAR-10 classifiers under white-box non-targeted attacks with noise L2 -norm limit of 1.5. MaxIter is the max number of iterations the attacker uses, and 1000x10 indicates10 runs each with 1000 iterations. Model 1 is an ordinarily network. Model 2 is the model fromMadry et al. (2017). Model 3 is L2NNN without adversarial training. Model 4 is L2NNN withadversarial training.
Table 3: Ablation studies: MNIST model without weight regularization; one without Lc loss; onewith max-pooling instead of norm-pooling; one without two-sided ReLU; Gap is average confidencegap. R-Accu is under attacks with 1000 iterations and with noise L2-norm limit of 3.
Table 4: Accuracy of L2NNN classifiers under white-box non-targeted attacks with 1000 iterationsand with noise L∞-norm limit of €.___________________________________	€	Model3	Model4MNIST	0.1	90.9%	92.4%MNIST	0.3	7.0%	44.0%CIFAR-10	8/256	32.3%	42.5%Although we primarily focus on defending against L2-bounded adversarial attacks in this work,we achieve some level of robustness against L∞-bounded attacks as a by-product. Table 4 shows6Published as a conference paper at ICLR 2019our results, again measured with the attack code of Carlini & Wagner (2017a). The e values matchthose used in Raghunathan et al. (2018); Kolter & Wong (2017); Madry et al. (2017). Our MNISTL∞ results are on par with Raghunathan et al. (2018); Kolter & Wong (2017) but not as good asMadry et al. (2017). Our CIFAR-10 Model 4 is on par with Madry et al. (2017) for L∞ defense.
Table 5: Accuracy comparison of MNIST classifiers that are trained on noisy data. Rand is thepercentage of training labels that are randomized. WD is weight decay. DR is dropout. ES is earlystopping. Gapl is L2NNN's average confidence gap on training set and Gap2 is that on test set.
Table 6: Training-accuracy-versus-confidence-gap trade-off points of L2NNNs on 50%-scrambledMNIST training labels.
Table 7: Accuracies of non-L2NNN MNIST classifiers that use a 4-layer architecture and that aretrained on training data with various amounts of scrambled labels. Rand is the percentage of traininglabels that are randomized. WD is weight decay. DR is dropout. ES is early stopping.
Table 8: Accuracies of non-L2NNN MNIST classifiers that use a 22-layer architecture and that aretrained on training data with various amounts of scrambled labels. Rand is the percentage of traininglabels that are randomized. WD is weight decay. DR is dropout. ES is early stopping.
Table 9: Training-accuracy-versus-confidence-gap trade-off points of L2NNNs on 25%-scrambledMNIST training labels.
Table 10: Training-accuracy-versus-confidence-gap trade-off points of L2NNNs on 75%-scrambledMNIST training labels.
