Table 1: Complexity study of the Little-Branch (α and β) for bL-ResNet-50.
Table 2: Performance comparison for bL-ResNet, bL-ResNeXt and bL-SEResNeXt.
Table 3: Speech recognition results. We present results on Hub5 and the CallHome portion of Hub5, while the RT-02 Switchboard set was used for selecting decode epoch and HMM prior settings.								Model		FLOPs (109)	Params (106)	WER Avg	Hub5	Hub5 CH1	Baseline: ResNet-22		1.11	3.02	14.67%	11.15%	18.17%2	bL-ResNet-22 (α = 4, β	= 1)	0.68 (1.63×)	3.15	14.72%	11.24%	18.18%3	bL-ResNet-22 (α = 4, β	= 2)	0.66 (1.68×)	3.11	14.47%	10.95%	17.95%4	bL-ResNet-22 (α = 4, β	= 3)	0.65 (1.70×)	3.10	14.66%	11.25%	18.05%5	bL-ResNet-22 (α = 2, β	= 3)	0.77 (1.43×)	3.07	14.46%	11.10%	17.80%6	bL-ResNet-22 (α = 4, β	= 1) cat	0.70(1.58×)	3.18	14.67%	11.31%	18.00%7	bL-PYR-ResNet-22 (α =	4, β = 1)	0.98(1.13×)	3.32	14.50%	11.05%	17.92%(line 1) to the best bL-ResNet-22 (line 5), we see not only a reduction in FLOPs, but also a modestgain in Word Error Rate (WER). Comparing lines 2-4, we see that increasing β (i.e. shorter littlebranches at full resolution) causes no WER degradation, while reducing the number of FLOPs. Fromline 5 we see that, similar to the object recognition ResNet results, decreasing α from 4 to 2 (i.e.
Table 4: Network configurations of bL-ResNet-50. Output size is illustrated in the parenthesis.
Table 5: Network configurations ofbL-ResNets, and α = 2 and β = 4.
Table 6: Performance of ResNets at different input resolutions.
Table 7: Different scales and merging schemes on bL-ResNet.
Table 8: Different number of merges in bL-ResNet. m: number of merges. (α = 2, β = 4)Model	I Top-1 Error FLOPs (109) Params (106)bL-ResNet-50 (m =	4) (baseline)	22.69%	2.85	26.69bL-ResNet-50 (m =	2)	23.48%	2.74	26.66bL-ResNet-50 (m =	1)	24.57%	2.64	26.64bL-ResNet-101 (m	= 4) (baseline)	21.80%	3.89	41.85bL-ResNet-101 (m	= 7)	21.85%	5.21	44.44Number of Merges in bL-NetWe also analyzed the number of merges we needed in the bL-Net. One big difference between ourapproach and others is that bL-Net merges multiple times as opposed to only once in most of the otherapproaches. Below we provide an explanation of why more information exchange is encouraged inour approach and when is the best moment for merging operation.
Table 9: Comparison with ShUffleNetV2 (Ma et al., 2018) (α = 2, β = 2).
Table 10: Objection detection results.
Table 11: Complexity study of the Little-Branch (α and β) for bL-ResNet-50 and bL-ResNet-101.
Table 12: Layers	Network configurations of bL-ResNets applied to speech for acoustic modeling. Output Size I	bL-ResNet-22 (α = 4,β = 1)	∣∣	bL-ResNet-22 (α = 2,β = 3)	∣		Convolution	32 × T T = 49 → 45	5 × 5, 64, s2	bL-module	32 × T T = 45 → 35	3 × 3,64	3 × 3,16 3× × 3, 64； b'× 2	3× × 3,16)J 2 (3 × 3, 64)b × 1	( 3 × 3, 16)l × 1	3 × 3, 64	3 × 3, 32 3× × 3, 64； b'× 2	3× × 3, 32； J (3 × 3, 64)b × 1transition layer	16 × T T = 35 → 33	(3 × 3, 64, s2) × 1	bL-module	16 × T T=33→23	3 3 × 3,128∖	C	3 3 × 3, 32∖	C 13 × 3,128)B × 2	13 × 3, 32)L × (3 × 3, 128)b × 1	(3 × 3, 32)r × 1	3 × 3, 128	3× 3,64 13 × 3,128)B × 2	13 × 3, 64)L × (3 × 3, 128)b × 1transition layer	8 × T T = 23 → 21	(3 × 3,128, s2) × 1	bL-module	8 × T T = 21 → 11	3 ×	×	3,	256、	C	3 ×	×	3, 64、	C 3 ×	×	3,256)b× 2	3 ×	×	3,64)J 2 (3	×	3,	256)b	× 1	( 3	×	3, 64)r × 1	3 × 3, 256	3 × 3, 128 3× × 3, 256； b'× 2 3× × 3,128； J 1 (3 × 3, 256)b × 1transition layer	4 × T T = 11 → 9	(3 × 3, 256, s2) × 1	bL-module	4 × T T = 9 → 1	3× 3, 512	3× 3, 128 3× × 3, 512； b'× 2	3× × 3,128； J 2	3 × 3, 512 3× × 3, 512； × 2Convolution Convolution	1 × 1 1 × 1	4 × 1,512 1 × 1, 32k	For each B block, the first 3 × 3 convolution is with stride 2 (in frequency), and a bilinear upsampling is applied at the end.
