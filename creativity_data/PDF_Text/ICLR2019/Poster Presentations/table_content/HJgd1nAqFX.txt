Table 1: Hyperparameters for training with Rainbow DQN (4 components)Hyperparameter	ValueOptimization algorithm Learning rate Batch Size Discounted factor DQN Target network update period Number of update per frame Number of exploration steps N steps (multi-step) bootstrap Noisy Nets σ0 Use DDQN Easy Tasks: Number of steps for training Medium Tasks: Number of steps for training Hard Tasks: Number of steps for training	Adam (Kingma & Ba, 2014) 0.00015 128 0.99 200 online network updates 1 50 8 0.5 True 5000 50000 2000000	Table 2: Hyperparameters for DOM-Q-NETHyperparameter	ValueVocabulary size: tag Vocabulary size: text Vocabulary size: class Embedding dimension: tag Embedding dimension: text Embedding dimension: class Dimension of Fully Connected(FC) layers Number of FC layers for 3 factorized Q networks Hidden Layer Activation Number of steps for neural message passing Max number of DOMS Max number goal tokens Out of Vocabulary Random vector generation	-80 400 80 16 32 16 128 2	each ReLU 3	(7 for social media task) 160 18 Choose-option, Click-CheckboxesTable 3: Hyperparameters for Replay BufferHyperparameter	Valueα: prioritization exponent	^05β for computing importance sampling weights	0Single Task Buffer Size	15000Multi Task Buffer Size	1000006.2	Goal-Attention Output ModelFigure 8 shows the readout phase of the graph neural network using goal-attention. The graph-levelfeature vector hglobal is computed by the weighted average of node-level representations processedwith T steps of message passing, {h1 hV }. The weights, {α1 αV }, are computed with thegoal vector as the query and node-level features as keys. For our model, we use a scaled dot productattention (Vaswani et al., 2017) with local embeddings as keys and neighbor embeddings as values,as illustrated in 3.3.
Table 2: Hyperparameters for DOM-Q-NETHyperparameter	ValueVocabulary size: tag Vocabulary size: text Vocabulary size: class Embedding dimension: tag Embedding dimension: text Embedding dimension: class Dimension of Fully Connected(FC) layers Number of FC layers for 3 factorized Q networks Hidden Layer Activation Number of steps for neural message passing Max number of DOMS Max number goal tokens Out of Vocabulary Random vector generation	-80 400 80 16 32 16 128 2	each ReLU 3	(7 for social media task) 160 18 Choose-option, Click-CheckboxesTable 3: Hyperparameters for Replay BufferHyperparameter	Valueα: prioritization exponent	^05β for computing importance sampling weights	0Single Task Buffer Size	15000Multi Task Buffer Size	1000006.2	Goal-Attention Output ModelFigure 8 shows the readout phase of the graph neural network using goal-attention. The graph-levelfeature vector hglobal is computed by the weighted average of node-level representations processedwith T steps of message passing, {h1 hV }. The weights, {α1 αV }, are computed with thegoal vector as the query and node-level features as keys. For our model, we use a scaled dot productattention (Vaswani et al., 2017) with local embeddings as keys and neighbor embeddings as values,as illustrated in 3.3.
Table 3: Hyperparameters for Replay BufferHyperparameter	Valueα: prioritization exponent	^05β for computing importance sampling weights	0Single Task Buffer Size	15000Multi Task Buffer Size	1000006.2	Goal-Attention Output ModelFigure 8 shows the readout phase of the graph neural network using goal-attention. The graph-levelfeature vector hglobal is computed by the weighted average of node-level representations processedwith T steps of message passing, {h1 hV }. The weights, {α1 αV }, are computed with thegoal vector as the query and node-level features as keys. For our model, we use a scaled dot productattention (Vaswani et al., 2017) with local embeddings as keys and neighbor embeddings as values,as illustrated in 3.3.
Table 4: Experiment statisticsNumber of tasks Number of tasks concurrently running for multitask Number of goal encoding modules compared	23 9 4	Ni = (23 + 9) * 4 = 128		Number of tasks for ablation study Number of discounted models compared for ablation study	2 3N2 = 2 * 3 = 6	Number of experiments for computing the average of a result	4Number of experiments for 11 multitask learning		Ntota = (128 + 6+ 11)* 4 = 580		6.7	Benchmark ResultsWe present the learning curves of both single-task and multitask agents. We also provide the learningcurves of the model with different goal-encoding modules 6.3. X-axis represents the number oftimesteps, and Y-axis represents the moving average of last 100 rewards. For medium and hard tasks,we also show the fraction of transitions with positive/non-zero rewards in the replay buffer and thenumber of unique positive transitions sampled throughout the training. This is to demonstrate thesparsity of the rewards for each task, and investigate whether the failure comes from exploration.
