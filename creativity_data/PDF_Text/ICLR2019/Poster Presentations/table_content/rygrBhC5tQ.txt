Table 1: Success count for robotic manipulation, comparing our method against baselines with orwithout transition policies (TP). Our method achieves the best performance over both RL baselinesand the ablated variants. Each entry in the table represents average success count and standarddeviation over 50 runs with 3 random seeds.
Table 2: Success count for locomotion, comparing our method against baselines with or withouttransition policies (TP). Our method outperforms all baselines in Patrol and Obstacle course. InHurdle, the reward function for TRPO was extensively engineered, which is not directly compara-ble to our method. Our method outperforms baselines learning from sparse reward, showing theeffectiveness of the proposed proximity predictor. Each entry in the table represents average successcount and standard deviation over 50 runs with 3 random seeds.
Table 3: Hyperparameter values for transition policy, proximity predictor, and primitive policy aswell as TRPO and PPO baselines.
