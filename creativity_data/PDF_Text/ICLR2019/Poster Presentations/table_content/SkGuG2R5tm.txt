Table 1: Comparison of different flavors of the catalyst: with a lattice quantizer (with or withoutend-to-end training), and with OPQ. All results use 64 bits per code. All timings are for BigAnn1Mare on a 2.2 GHz machine with 40 threads. The encoding times associated with the catalyzer includethe forward pass through our neural network. Note, our lattice-based coding scheme is the only onenot requiring external meta-data once the compact code is produced.
Table 2: Performance (1-recall at 10, %) with LSH, on Deep1M and BigAnn1M, as a function of thenumber of bits per index vector. All results are averaged over 5 runs with different random seeds.
Table 3: Optimal values of the regularization parameter Î» for Deep1M, using a fixed radius of r = 10.
