Table 1: Policy generalization on FindObjSY . Agents trained on FindObjS5, and evaluated onFindObjS7 and S10.
Table 2: Transferable exploration strategies on MultiRoomNX SY . InfoBot encoder trained onMultiRoomN2S6. All agents evaluated on MultiRoomN3S4 and N5S4. While several methodsperform well with 3 rooms, InfoBot performs far better as the number of rooms increases to 5.
Table 3: Experiments for training the agent in a 6 × 6 maze environment, and then generalizing to a11 × 11 maze. Comparison of our proposed method to regular actor-critic methods, UVFA and otherhierarchical approaches. Results shown for the % of times agent reaches the goal. The results areaverage over 3 random seeds.
Table 4: Training and test physical reward for setting with comunication, without communication,with limited communication (using InfoBot cost)D Multiagent CommunicationHere, we want to show that by training agents to develop “default behaviours” as well as theknowledge of when to break those behaviours, using an information bottleneck can also help in otherscenarios like multi-agent communication. Consider multiagent communication, where in order tosolve a task, agents require communicating with another agents. Ideally, an agent would would liketo communicate with other agent, only when its essential to communicate, i.e the agents would liketo minimize the communication with another agents. Here we show that selectively deciding when tocommunicate with another agent can result in faster learning.
Table 5: Performance of models trained via reinforcement learning on a held-out set of environmentsand instructions. Policy quality is the true expected normalized reward. We show results from trainingon the local and global instructions both separately and jointly.
