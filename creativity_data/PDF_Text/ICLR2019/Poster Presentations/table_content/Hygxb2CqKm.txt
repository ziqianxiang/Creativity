Table 1: Comparison of stable and unstable models on a variety of sequence modeling tasks. Forall the tasks, stable and unstable RNNs achieve the same performance. For polyphonic music andslot-filling, stable and unstable LSTMs achieve the same results. On language modeling, there isa small gap between stable and unstable LSTMs. We discuss this in Section 4.3. Performance isevaluated on the held-out test set. For negative log-likelihood (nll), bits per character (bpc), andperplexity, lower is better. For F1 score, higher is better.
Table 2: Hyperparameters for all experimentsModelRNN LSTMWord LM	Number layers	1	1	Hidden units	256	1024	Embedding size	1024	512	Dropout	0.25	0.65	Batch size	20	20	Learning rate	2.0	20.
