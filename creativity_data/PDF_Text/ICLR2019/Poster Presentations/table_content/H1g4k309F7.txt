Table 1: Machine learning tasks where W. Ensembling can be applied. Note that W. barycenterallows ensembling different source domains to another target domain as in attributes to category.
Table 2: Sample output (top 15 words) of W. barycenter (Algorithm 1), arithmetic and geometricmeans based on four captioner models. Each row shows a word and a corresponding probability overthe vocabulary (as a percentage). W. Barycenter has higher entropy, spreading the probability massover the synonyms and words related to the top word “car” and downweights the irrelevant objects(exploiting the side information K). Simple averaging techniques, which use only the confidenceinformation, mimick the original model outputs. Figure 4 in Appendix gives a histogram view.
Table 3: Controllable Entropy of regularized Wasserstein Barycenter (Algorithm 1). Output (top 15words) for a synonyms-based similarity matrix K under different regularization ε (which controlsthe distance of K to identity I, kK - IkF). As ε decreases, kK - IkF also decreases, i.e., Kapproaches identity matrix, and the entropy of the output of Algorithm 1 decreases. Note that thelast column, corresponding to very small entropic regularization, coincides with the output fromgeometric mean in Figure 2 (for K = I, the Algorithm 1 outputs geometric mean as a barycenter).
Table 4: Attribute-based classification. The W. barycenter ensembling achieves better accuracy byexploiting the cross-domain similarity matrix K, compared to a simple linear-transform of proba-bility mass from one domain to another as for the original models or their simple averages.
Table 5: Description of our 8 models built on MS-COCOEvaluation Metric. We use the mean Average Precision (mAP) which gives the area under the curveof P = f (R) for precision P and recall R, averaged over each class. mAP performs a sweep of thethreshold used for detecting a positive class and captures a broad view of a multi-label predictorperformance. Performances for our 8 models are reported in Table 6. Precision, Recall and F1 formicro/macro are given in Table 10. Our individual models have reasonable performances overall.
Table 6: Multi-label models performances compared to published results on MS-COCO test set.
Table 7: Performance of GloVe-based W. barycenter on COCO test split using topK beam searchversus Geometric and Arithmetic ensembling. While the generated sentences based on W. barycen-ter do not match exactly the ground truth (lower CIDEr), they remain semantically close to it, whilebeing more diverse (e.g., paraphrased) as indicated by the higher entropy and stable WMD.
Table 8: Sample output (top 20 words) of barycenter for different similarity matrices K based onGloVe (columns titles denote the distance of K from identity kK - I kF and corresponding .).
Table 9: Mapping from a few top words in the barycenter output (for similarity matrix K based onsynonyms) to the input models. For each word in the left columns, the remaining columns show thecontributing words and the percent of contribution.
Table 10: Our multi-label models performances compared to published results on MS-COCO testset. Arithmetic, geometric means and W. barycenter performances are reported as well. [1] (Heet al., 2015) [2] (Zhu et al., 2017)B.2	Weighted Multi-label Prediction EnsemblingEnsembling results given in Tab. 6 are using uniformly weighted models, i.e. λ' = ml where m isthe number of models. However, in practice, arithmetic and geometric mean ensembling usually useweighted ensembles of models The weights are then optimized and established on a small validationset before being used for ensembling on a test set. A well-known embodiment of this type ofapproach is Adaboost (Freund & Schapire, 1999) where weights are dynamically defined at eachpass of training wrt to the accuracy of base models.
Table 11: multi-label models ensembling mAP on MS-COCO test set (35150 images). Performance-based weighting helps both arithmetic and W.Barycenter ensembling, the latter retaining its perfor-mance vantage.
Table 12: Machine learning tasks where W. Barycenter ensembling can be applied: We emphasizethat W. Barycenter has the advantage over alternatives such as arithmetic or geometric means in thatit is able to ensemble models whose histograms are defined on different domains. Moreover, thetarget domain, where the barycenter is defined, can be different from the source domains. This isencountered, for instance, in the attribute-based classification, where models are defined on attributes(multi-labels) and the ensemble is defined on the set of categories. We give here additional tasksthat can benefit from this flexibility of W. Barycenters.
Table 13: Timings (in s) of Wasserstein Barycenter computation compared to Arithmetic and Geo-metric mean computations for the MS-COCO test set (35150 samples).
