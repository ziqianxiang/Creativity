Table 1: Recent binary architectures and their training setup. The Reorder column refers to reorder-ing of blocks in a convolutional layer to make sure pooling layer’s input is full-precision. The 1stLayer column indicates whether the first layer of the network is binary or kept at full precision.
Table 2: Achievable test errors using different optimisers for binary MLP model trained on MNISTand binary CNN model train on CIFAR-10. The hyper-parameters of each optimiser were fine-tunedfor best results.
Table 3: Impact of gradient and/or weight clipping on the final test accuracy of BNNs.				Clipping	None (Vanilla STE)	Weights	Gradients	BothMNIST	1.28%	1.22%	1.17%	1.18%CIFAR-10	10.79%	10.73%	10.53%	10.38%3.3	Impact of Batch NormalisationBatch normalisation (BN) uses mini-batch statistics during training but at inference-time the modelis classifying a single data point. Therefore, each BN layer maintains a running average of mini-batch statistics to use during inference. The default momentum rate for this running average isusually large, e.g. 0.99. We noted that some binary models use smaller values for this hyper-parameter. Binary models are typically trained for more epochs than their non-binary counterpartand training is continued even when there is not a meaningful improvement in loss or accuracy. Thisis consistent with our earlier hypothesis in 3.1. Reducing the momentum rate in BN can help tocancel the effect of long training. The effect is small but consistent. Table 4 shows how differentvalues of BN momentum results in different test accuracies. Krishnamoorthi (2018) also observedthat Batch normalisation should be handled differently when training quantised models in order toachieve better performance.
Table 4: Impact of momentum rate in Batch Normalisation’s moving average on the final test accu-racy of BNNs.	Momentum	0.8	0.85	0.9	0.99 MNIST	1.21%	1.19%	1.22%	1.23% CIFAR-10	10.31%	10.35%	10.53%	10.61%3.4 Impact of Pooling and Learning RateReordering Pooling Block. As can be seen in Table 1 all binary models change the placement ofpooling operation within a convolutional layer. This change makes sense intuitively. For instance,applying MaxPooling to a binary vector results in a vector with almost all ones. We have seen twovariants of block reordering and in both cases (see Figure 3), pooling is done immediately after theconvolution operator where the vector is not binary. In our experiments, not making this changeresulted in significant accuracy loss.
Table 5: Training binary models using pre-trained full-precision models for CIFAR-10 (ResNet-18and VGG-10) and ImageNet (AlexNet-like) datasets.
