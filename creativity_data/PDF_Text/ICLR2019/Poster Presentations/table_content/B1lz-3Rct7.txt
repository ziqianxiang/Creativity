Table 1: Classification results on CIFAR-10 and CIFAR-100. B denotes BN while D denotes data augmentation,including horizontal flip and random crop. WD denotes weight decay regularization. Weight decay regularizationimproves the generalization consistently. Interestingly, we observe that weight decay gives an especially strongperformance boost to the K-FAC optimizer when BN is turned off.
Table 2: Squared Frobenius norm of the input-outputJacobian matrix. K-FAC-G with weight decay signifi-cantly reduces the Jacobian norm.
Table 3: Classification results with different batch sizes. WD denotes weight decay regularization. We tuneweight decay factor and learning rate using held-out validation set.
