Table 1: Performance on continual lifelong learning 20 tasks benchmarks from (Lopez-Paz & Ranzato, 2017).
Table 2: Performance varying the buffer size on continual learning benchmarks (Lopez-Paz & Ranzato, 2017)							unrealistic to assume a system can store a large percentage of previous examples in memory. Assuch, we would like to compare MER to GEM, which is known to perform well with an extremelysmall memory buffer (Lopez-Paz & Ranzato, 2017). We consider a buffer size of 500, that is over 10times smaller than the standard setting on these benchmarks. Additionally, we also consider a buffersize of 200, matching the smallest setting explored in Lopez-Paz & Ranzato (2017). This settingcorresponds to an average storage of 1 example for each combination of task and class. We reportour results in Table 2. The benefits of MER seem to grow as the buffer becomes smaller. In thesmallest setting, MER provides more than a 10% boost in retained accuracy on both benchmarks.
Table 3: Performance on many task non-stationary continual lifelong learning benchmarks.
Table 4: Analysis of the mean dot product across the period of learning between gradients on incomingexamples and gradients on randomly sampled past examples across 5 runs on MNIST based benchmarks.
Table 5: Forward transfer and interference (FTI) experiments on MNIST Rotations.
Table 6: Retained accuracy ablation experiments on MNIST based learning lifelong learning benchmarks.
Table 7: A reproducability comparison of retained accuracy across machines and seeds for the best performinghyperparameters on MNIST Rotations.
Table 8: A reproducability comparison of retained accuracy across machines and seeds for the best performinghyperparameters on MNIST Permutations.
Table 9: A reproducability comparison of retained accuracy across machines and seeds for the best performinghyperparameters on MNIST Many Permutations.
