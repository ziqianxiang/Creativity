Table 1: MNIST classification results. #samples gives the number of kernels that were randomlysampled for the hyperparameter search. “ConvNet GP” and “Residual CNN GP” are random CNNarchitectures with a fixed filter size, whereas “ResNet GP” is a slight modification of the architectureby He et al. (2016b). Entries labelled “SGD” used stochastic gradient descent for tuning hyperpa-rameters, by maximising the likelihood of the training set. The last two methods use parametricneural networks. The hyperparameters of the ResNet GP were not optimised (they were fixed basedon the architecture from He et al., 2016b).
