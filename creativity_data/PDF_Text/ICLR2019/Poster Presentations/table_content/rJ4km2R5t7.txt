Table 1: Task descriptions and statistics. All tasks are single sentence or sentence pair classification,except STS-B, which is a regression task. MNLI has three classes; all other classification tasks havetwo. Test sets shown in bold use labels that have never been made public in any form.
Table 2: The types of linguistic phenomena annotated in the diagnostic dataset, organized under fourmajor categories. For a description of each phenomenon, see Appendix E.
Table 3: Examples from the diagnostic set. Fwd (resp. Bwd) denotes the label when sentence 1(resp. sentence 2) is the premise. Labels are entailment (E), neutral (N), or contradiction (C).
Table 4: Baseline performance on the GLUE task test sets. For MNLI, we report accuracy on thematched and mismatched test sets. For MRPC and Quora, we report accuracy and F1. For STS-B,we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation. For allother tasks we report accuracy. All values are scaled by 100. A similar table is presented on theonline platform.
Table 5: Results on the diagnostic set. We report R3 coefficients between gold and predicted la-bels, scaled by 100. The coarse-grained categories are Lexical Semantics (LS), Predicate-ArgumentStructure (PAS), Logic (L), and Knowledge and Common Sense (K). Our example fine-grained cate-gories are Universal Quantification (UQuant), Morphological Negation (MNeg), Double Negation(2Neg), Anaphora/Coreference (Coref), Restrictivity (Restr), and Downward Monotone (Down).
Table 6: Baseline performance on the GLUE tasksâ€™ development sets. For MNLI, we report accuracyaveraged over the matched and mismatched test sets. For MRPC and QQP, we report accuracy andF1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthewscorrelation. For all other tasks we report accuracy. All values are scaled by 100.
Table 7: Diagnostic dataset statistics by coarse-grained category. Note that some examples may betagged with phenomena belonging to multiple categories.
