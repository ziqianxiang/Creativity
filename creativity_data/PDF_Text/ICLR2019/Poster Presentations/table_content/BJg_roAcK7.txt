Table 1: Relevant feature discovery results for Synthetic datasets with 11 featuresAs demonstrated by Table 1, our method is capable of detecting relevant features on a global level(Syn1, Syn2 and Syn3) as well as on an instance-wise level (Syn4, Syn5 and Syn6) outperformingall other methods in both cases (both global and instance-wise methods). The particularly poorperformance of some global feature selection methods in Syn1, Syn2 and Syn3 (where there is noinstance-wise relevance) is due to the non-linearity of the relationship between features and labels,further details can be found in the Appendix.
Table 2: Detailed comparison of INVASE with L2X in Syn4 and Syn5, highlighting the capabilityof INVASE to select a flexible number of features for each sample. Group 1: X11 < 0, Group 2:X11 ≥ 04.1.3 PredictionIn this experiment we analyze the effect of using feature selection as a pre-processing step for predic-tion. We first perform feature selection (either instance-wise or global) and then train a 3-layer fullyconnected network with Batch Normalization [12] in every layer (to avoid overfitting) to performpredictions on top of the (feature-selected) data. In this setting we compare the two global fea-ture selection methods (LASSO and Tree) and one instance-wise feature selection method (L2X).
Table 3: Prediction performance comparison with and without feature selection methods (L2X,LASSO, Tree, INVASE, and Global). Global is using ground-truth globally relevant features foreach datasettitatively shows that instance-wise feature selection can further improves the predictive model fromground truth global feature selection.
Table 4: Selection probability of overall and patient subgroups by INVASE in MAGGIC dataset.
Table 5: Prediction performance for MAGGIC and PLCO dataset.
Table 6: Summary of the related works. (NN: Neural networks, KL: Kullback-Leibler)Extending INVASE to regressionTo extend our model to the setting where Y is continuous (regression problem), we replace theestimated loss with the reconstruction error as follows.
Table 7: Relevant feature discovery for synthetic datasets with 100 featuresAs can be seen in Table 7, INVASE also works consistently better than all other benchmarks in all 6synthetic datasets in this setting. In fact, we see a significant reduction in performance (compared tothe 11 feature setting) for L2X in Syn1, with the TPR dropping more than 90% leading to an almostcomplete failure of the method to detect any relevant features. In particular, we see that L2X doesnot scale as well as INVASE with the dimensionality of the data, which is particularly limiting for afeature selection method.
Table 8: Comparison of CPU clock time across different instance-wise feature selection methods onaverage across Syn1 to Syn6 with 100 features and 10,000 samples on training/testing, respectivelyHyper-parameter AnalysisIn the following experiment, we provide results for various values of the hyper-parameter, λ, in theSyn4, Syn5, and Syn6 100-dimensional setting. Table 9 gives the results in terms of TPR and FDR.
Table 9: Relevant feature discovery results for various values of the hyper-parameter λ in the Syn4,Syn5, and Syn6 100-dimensional setting.
Table 10: Relevant feature discovery results for complex synthetic datasets (Syn4A, 5A, 6A) With100 features•	Syn4B: If X1X3 < 0, logit = exp(X1X2), otherWise, logit =exp(Pi6=3 Xi2 - 4).
Table 11: Relevant feature discovery results for complex synthetic datasets (Syn4B, 5B, 6B, 7) with100 featuresDataset	Synl		Syn2		Syn3		Syn4		Syn5		Syn6	Metrics (%)	TPR	FDR	TPR	FDR	TPR	FDR	TPR	FDR	TPR	FDR	TPR	FDRINVASE	100.0	0.0	100.0	0.0	100.0	0.0	85.9	0.0	72.9	0.1	81.0	13.2L2X	68.8	31.2	99.9	0.1	83.0	17.0	60.0	31.3	68.3	22.3	73.5	26.5LIME	46.9	53.1	99.9	0.1	87.2	12.8	63.6	24.4	50.2	37.6	68.7	31.3Shapley	73.9	26.1	94.5	5.5	81.0	19.0	65.3	23.9	61.2	29.0	69.9	30.1Knock off	27.5	65.0	77.5	22.5	100.0	0.0	57.0	34.4	56.1	29.8	58.0	42.0Tree	100.0	0.0	100.0	0.0	100.0	0.0	56.3	29.7	51.6	40.2	46.7	53.3SCFS	30.0	70.0	53.0	47.0	100.0	0.0	52.0	39.9	54.0	32.4	64.5	35.5LASSO	25.0	75.0	75.0	25.0	100.0	0.0	60.7	33.1	56.1	29.8	58.2	41.8Table 12: Relevant feature discovery for real datasets with synthetic labels using MAGGIC datasetDataset	Syn1		Syn2		Syn3		Syn4		Syn5		Syn6	Metrics (%)	TPR	FDR	TPR	FDR	TPR	FDR	TPR	FDR	TPR	FDR	TPR	FDRINVASE	35.9	0.0	100.0	0.0	84.0	7.0	59.2	38.6	64.6	31.7	70.0	29.9L2X	0.0	100.0	62.2	37.8	43.6	56.4	41.9	55.4	21.5	76.7	66.9	33.1LIME	1.0	99.0	70.3	29.7	74.9	25.1	43.5	55.9	26.8	68.9	56.8	43.2Shapley	5.4	94.6	68.5	31.5	67.9	32.1	32.7	69.4	39.6	58.6	48.5	51.5Knock off	15.0	50.0	85.0	15.0	100.0	0.0	46.1	52.1	34.5	58.3	60.0	40.0
Table 12: Relevant feature discovery for real datasets with synthetic labels using MAGGIC datasetDataset	Syn1		Syn2		Syn3		Syn4		Syn5		Syn6	Metrics (%)	TPR	FDR	TPR	FDR	TPR	FDR	TPR	FDR	TPR	FDR	TPR	FDRINVASE	35.9	0.0	100.0	0.0	84.0	7.0	59.2	38.6	64.6	31.7	70.0	29.9L2X	0.0	100.0	62.2	37.8	43.6	56.4	41.9	55.4	21.5	76.7	66.9	33.1LIME	1.0	99.0	70.3	29.7	74.9	25.1	43.5	55.9	26.8	68.9	56.8	43.2Shapley	5.4	94.6	68.5	31.5	67.9	32.1	32.7	69.4	39.6	58.6	48.5	51.5Knock off	15.0	50.0	85.0	15.0	100.0	0.0	46.1	52.1	34.5	58.3	60.0	40.0Tree	0.0	100.0	71.0	29.0	75.0	25.0	34.5	66.3	43.8	54.7	36.9	63.1SCFS	10.0	90.0	61.0	39.0	93.8	6.2	43.2	55.7	31.0	63.6	55.5	44.5LASSO	0.0	100.0	72.5	27.5	100.0	0.0	39.2	60.8	33.2	68.2	45.0	55.0Table 13: Relevant feature discovery for real datasets with synthetic labels using PLCO datasetAs demonstrated in Tables 12 and 13, INVASE outperforms all other methods across all 6 of thesynthetic-label settings using real features. This also demonstrates the capability of INVASE insettings where there are unknown correlation structures in the features.
Table 13: Relevant feature discovery for real datasets with synthetic labels using PLCO datasetAs demonstrated in Tables 12 and 13, INVASE outperforms all other methods across all 6 of thesynthetic-label settings using real features. This also demonstrates the capability of INVASE insettings where there are unknown correlation structures in the features.
Table 14: Predictive Performance Comparison on two real-world datasets (MAGGIC and PLCO) interms of AUROC and AUPRC18Published as a conference paper at ICLR 2019Correlations between features and labels in the synthetic andsemi-synthetic experimentsVariables	SynI	Syn2	Syn3	Syn4	Syn5	Syn6X1	0.003	0.008	0.006	0.009	0.007	0.006X2	0.001	0.005	0.006	0.005	0.015	0.005X3	0.006	0.011	0.001	0.017	0.016	0.010X4	0.006	0.003	0.003	0.002	0.000	0.002X5	0.003	0.015	0.022	0.004	0.017	0.028X6	0.003	0.004	0.005	0.002	0.004	0.005X7	0.013	0.009	0.481	0.002	0.242	0.235X8	0.010	0.008	0.012	0.003	0.010	0.022X9	0.001	0.003	0.239	0.002	0.115	0.121X10	0.002	0.003	0.308	0.003	0.149	0.144X11	0.014	0.012	0.004	0.028	0.018	0.002Table 15: Correlation between features and labels in Synthetic datasets with 100 features. Groundtruth (in the global sense) relevant features are given in bold. Features with correlation > 0.05 are
Table 15: Correlation between features and labels in Synthetic datasets with 100 features. Groundtruth (in the global sense) relevant features are given in bold. Features with correlation > 0.05 arehighlighted in red.
Table 16: Correlation between features and labels in MAGGIC datasets. Ground truth relevantfeatures are described in bold. Features with correlation > 0.05 are described in redWe do the same analysis for the MAGGIC dataset; results are given in Table 16. We see that here thelinear correlation with the label is stronger and this is reflected in Tables 12 and 13, where all of thelinear models performed better than in the fully-synthetic settings. However, we note that althoughthey had a better performance, in most cases it was still not comparable with INVASE.
