Table 1: Estimated upper bound on negative log-likelihood along with KL-divergence (in parenthe-sis) in bits per dimension for CIFAR-10 and downsampled ImageNet.
Table 2: The resUlt of oUr text experiments on LM1B in nats / token.
Table 3: Comparison of independent Gaussian delta-VAE and temporal delta-VAE with AR(1) prioron CIFAR-10 both targeting the same rate. While both models achieve a KL around the target rateand perform similarly in the downstream linear classification task, the temporal model with AR(1)prior achieves significantly better marginal likelihood.
Table 4: Hyperparameter values for the models used for experiments. The subscripts e, d, auxrespectively denote the encoder, the decoder, and the LSTM auxiliary prior. l is the number of layers,h is the hidden size of each layer, r is the size of the residual filter, a is the number of attention layersinterspersed with gated convolution layers of PixelCNN, ndmol is the number of components in thediscrete mixture of logistics distribution, dod is the probability of dropout applied to the decoder, zis the dimensionality of the latent variable used for each row, and the alpha column gives the rangeof the AR(1) prior hyper-parameter for each latent.
Table 5:	Hyperparameter values for our LM1B experiments. l is the number of layers, h is thehidden size of each layer, r is the size of the residual filters, do is the probability of dropout, z isthe dimensionality of the latent variable, and the alpha column gives the range of the AR(1) priorhyper-parameter for each latent dimension.
Table 6:	Ablation of anti-causal vs. non-causal structure. l: number of layers, h: hidden size, a:number of attention layers. Subscripts e and d respectively denote encoder and decoder sizes whenthey were different. The low-rate (high-rate) models had latent dimension of 8 (64) with alphalinearly placed in [0.5, 0.95] ([0.5, 0.99]) which gives the total rate of 79.44 (666.6) bits per image.
