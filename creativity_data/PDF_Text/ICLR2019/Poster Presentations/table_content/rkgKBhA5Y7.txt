Table 1: Test errors against current state-of-the-art semi-supervised results. The previous best numbersare obtained from (Tarvainen and Valpola, 2017)1, (Park et al., 2017)2, (Laine and Aila, 2016)3 and(Luo et al., 2018)4. CNN denotes performance on the benchmark 13-layer CNN (see A.8). Rowsmarked * use the Shake-Shake architecture. The result marked ^ are from Î  + fast-SWA, where therest are based on MT + fast-SWA. The settings 50k+500k and 50k+237k* use additional 500k and237k unlabeled data from the Tiny Images dataset (Torralba et al., 2008) where * denotes that we useonly the images that correspond to CIFAR-100 classes.
Table 2: CIFAR-10 semi-supervised errors on test set with a 13-layer CNN. The epoch numbers arereported in parenthesis. The previous results shown in the first section of the table are obtained fromTarvainen and Valpola (2017)1, Park et al. (2017)2, Laine and Aila (2016)3, Miyato et al. (2017)4.
Table 3: CIFAR-100 semi-supervised errors on test set. All models are trained on a 13-layer CNN.
Table 4: CIFAR-10 semi-supervised errors on test set. All models use Shake-Shake Regularization(Gastaldi, 2017) + ResNet-26 (He et al., 2015).
Table 5: CIFAR-100 semi-supervised errors on test set. Our models use Shake-Shake Regularization(Gastaldi, 2017) + ResNet-26 (He et al., 2015).
Table 6: A 13-layer convolutional neural networks for theCNN experiments (CIFAR-10 and CIFAR-100) in Section5.2 and 5.3. Note that the difference from the architectureused in Tarvainen and Valpola (2017) is that we removeda Gaussian noise layer after the horizontal flip.
Table 7: Domain Adaptation from CIFAR-10 to STL. VADA results are from (Shu et al., 2018)and the original SE* is from French et al. (2018). SE is the score with our implementation withoutfast-SWA. fast-SWA 1 performs averaging every epoch and the final result is obtained at epoch 3000.
