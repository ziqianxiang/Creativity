Published as a conference paper at ICLR 2019
Variational Autoencoders with Jointly
Optimized Latent Dependency S tructure
JiaWei He1* & Yu Gong1*	Joseph Marino2
{jha203, gongyug}@sfu.ca	jmarino@caltech.edu
Greg Mori1	Andreas M. Lehrmann
mori@cs.sfu.ca	andreas.lehrmann@gmail.com
1 School of Computing Science, Simon Fraser University
Burnaby, BC, V5B1Z1, Canada
2California Institute of Technology
Pasadena, CA, 91125, USA
Ab stract
We propose a method for learning the dependency structure between latent vari-
ables in deep latent variable models. Our general modeling and inference frame-
work combines the complementary strengths of deep generative models and prob-
abilistic graphical models. In particular, we express the latent variable space of
a variational autoencoder (VAE) in terms of a Bayesian network with a learned,
flexible dependency structure. The network parameters, variational parameters
as well as the latent topology are optimized simultaneously with a single objec-
tive. Inference is formulated via a sampling procedure that produces expectations
over latent variable structures and incorporates top-down and bottom-up reasoning
over latent variable values. We validate our framework in extensive experiments
on MNIST, Omniglot, and CIFAR-10. Comparisons to state-of-the-art structured
variational autoencoder baselines show improvements in terms of the expressive-
ness of the learned model.
1	Introduction
Deep latent variable models offer an effective method for automatically learning structure from
data. By explicitly modeling the data distribution using latent variables, these models are capable
of learning compressed representations that are then relevant for downstream tasks. Such models
have been applied across a wide array of domains, such as images (Gregor et al., 2014; Kingma &
Welling, 2014; Rezende et al., 2014; Gregor et al., 2015), audio (Chung et al., 2015; Fraccaro et al.,
2016), video (He et al., 2018; Yingzhen & Mandt, 2018), and text (Bowman et al., 2016; Krishnan
et al., 2017). However, despite their success, latent variable models are often formulated with simple
(e.g. Gaussian) distributions, making independence assumptions among the latent variables. That is,
each latent variable is sampled independently. Ignoring the dependencies between latent variables
limits the flexibility of these models, negatively impacting the model’s ability to fit the data.
In general, structural dependencies can be incorporated into all phases of a forward process, includ-
ing inference, latent model, and output space: Normalizing flows (Rezende & Mohamed, 2015),
for instance, accounts for dependencies during inference by learning a mapping from a simple dis-
tribution to a more complex distribution that contains these dependencies. Structured output net-
works (Lehrmann & Sigal, 2017), on the other hand, directly predict an expressive non-parametric
output distribution. On the modeling side, one can add dependencies by constructing a hierarchi-
cal latent representation (Dayan et al., 1995). These structures consist of conditional (empirical)
priors, in which one latent variable forms a prior on another latent variable. While this conditional
* Equal Contribution.
1
Published as a conference paper at ICLR 2019
distribution may take a simple form, marginalizing over the parent variable can result in an arbitrar-
ily complex distribution. Models with these more flexible latent dependency structures have been
shown to result in improved performance (S0nderby et al., 2016; BUrda et al., 2016; Kingma et al.,
2016). However, despite the benefits of including additional structure in these models, their depen-
dency structures have so far been predefined, potentially limiting the performance of this approach.
In this work, we propose a method for learning dependency structures in latent variable models.
Structure learning is a difficult task with a long history in the graphical models community (Koller
& Friedman, 2009). Over the years, it has been tackled from several perspectives, including
constraint-based approaches (Cheng et al., 2002; Lehmann & Romano, 2008), optimization of struc-
ture scores (Kass & Raftery, 1995; Heckerman et al., 1995; Barron et al., 1998), Bayesian model
averaging (Heckerman et al., 1999; Koivisto & Sood, 2004), and many more. Unfortunately, the
underlying objectives are often limited to graphs of a particular form (e.g., limited tree width),
prohibitively expensive, or difficult to integrate with the gradient-based optimization techniques of
modern neural networks. Here, we discuss an end-to-end approach for general graph structures in-
troducing minimal complexity overhead. In particular, we introduce a set of binary global variables
to gate the latent dependencies. The whole model (including its structure) is jointly optimized with
a single stochastic variational inference objective. In our experimental validation, we show that the
learned dependency structures result in models that more accurately model the data distribution,
outperforming several common predefined latent dependency structures.
2	Background
2.1	Variational Inference & Variational Autoencoders
A latent variable model, defined by the joint distribution, pθ(x, z) = pθ(x∣z)pθ(z), models each
data example, x, using a local latent variable, z, and global parameters, θ. pθ(x∣z) denotes the con-
ditional likelihood, andpθ(z) denotes the prior. Latent variable models are capable of capturing the
structure present in data, with z forming a compressed representation of each data example. Unfortu-
nately, inferring the posterior, pθ(z∣x), is typically computationally intractable, prompting the use of
approximate inference techniques. Variational inference (Jordan et al., 1999) introduces an approxi-
mate posterior, qφ(z∣x), and optimizes variational parameters, φ, to minimize the KL-divergence to
the true posterior, KL(qφ(z∣x)∣∣pθ(z∣x)). As this quantity cannot be evaluated directly, the follow-
ing relation is used:
log pθ (x) = KL(qφ(z∣x)∣∣pθ (Z∣x)) + L(x; θ,φ),	(1)
where L(x; θ, φ) is the evidence lower bound (ELBO), defined as
L(x; θ, φ) = Eqφ(z∣x) [log pθ (x∣z)] -KL(qφ(z∣x)∣∣pθ(z)).	(2)
In Eq. (1), logpθ (x) is independent of φ, so we can minimize the KL divergence term, i.e. perform
approximate inference, by maximizing L(x; θ, φ) w.r.t. qφ (z∣x). Further, because KL divergence
is non-negative, L(x; θ, φ) is a lower bound on logpθ (x), meaning we can then learn the model
parameters by maximizing L(x; θ, φ) w.r.t. θ.
Variational autoencoders (VAEs) (Kingma & Welling, 2014; Rezende et al., 2014) amortize infer-
ence optimization across data examples by parameterizing qφ(z∣x) as a separate inference model,
then jointly optimizing the model parameters θ and φ. VAEs instantiate both the inference model
and latent variable model with deep networks, allowing them to scale to high-dimensional data.
However, VAEs are typically implemented with basic graphical structures and simple, unimodal
distributions (e.g. Gaussians). For instance, the dimensions of the prior are often assumed to
be independent, pθ(z) = ∏mpθ(zm), with a common assumption being a fixed standard Gaus-
sian: pθ(z) = N(z; 0, I). Similarly, approximate posteriors often make the mean field assump-
tion, qφ(z∣x) = ∏m qφ(zm∣x). Independence assumptions such as these may be overly restrictive,
thereby limiting modeling capabilities.
2.2	Empirical Priors Through Latent Dependency Structure
One technique for improving the expressive capacity of latent variable models is through incorpo-
rating dependency structure among the latent variables, forming a hierarchy (Dayan et al., 1995;
2
Published as a conference paper at ICLR 2019
(a) Traditional Latent Variable Models
Figure 1: Overview: Model Comparison. We show the graphical representations of (a) traditional
latent variable models (VAE, ladder VAE) and (b) the proposed graph VAE. Solid lines denote
generation, dashed lines denote inference, and the dotted area indicates the latent space governed
by variational parameters φ and generative parameters θ. Both VAE and ladder VAE use a fixed
graph structure with limited expressiveness (VAE: independent; ladder VAE: chain-structured). In
contrast, graph VAE jointly optimizes a distribution over latent structures c and model parameters
(φ,θ), allowing test-time sampling of a flexible, data-driven latent structure.
(b) Proposed Model
Rezende et al., 2014; Goyal et al., 2017; Vikram et al., 2018; Webb et al., 2018). These dependen-
cies provide empirical priors, learned priors that are conditioned on other latent variables. With M
latent dimensions, the full prior takes the following auto-regressive form:
M
pθ(z) = ∏ pθ(zm∣zpa(m)),	(3)
m=1
where zpa(m) denotes the vector of latent variables constituting the parents of zm. Each conditional
distribution can be parameterized by deep networks that output the parameters of distributions, e.g.
mean and variance of a Gaussian. While these conditional distributions may be relatively simple, the
marginal empirical prior, pθ (zm) = ∫ pθ(zm∣zpa(m))pθ(zpa(m))dzpa(m), can be arbitrarily complex.
By using more flexible priors, models with latent dependency structure are less restrictive in their
latent representations, hopefully capturing more information and enabling a better fit to the data.
With the added latent dependencies in the model, the independence assumption in the approximate
posterior is even less valid in this setting. While normalizing flows (Rezende & Mohamed, 2015)
offers one technique for overcoming the mean field assumption, a separate line of work has in-
vestigated the use of structured approximate posteriors, particularly in the context of models with
empirical priors (Johnson et al., 2016). This technique introduces dependencies between the dimen-
sions of the approximate posterior, often mirroring the dependency structure of the latent variable
model. An explanation for this was provided by Marino et al. (2018): optimizing the approximate
posterior requires knowledge of the prior, which is especially relevant in models with empirical pri-
ors where the prior can vary with the data. Ladder VAE (S0nderby et al., 2016) incorporates these
prior dependencies by using a structured approximate posterior of the form
M
qφ(z∣x) = ∏ qφ (zm ∣x, zpa(m) ).	(4)
Unlike the mean field approximate posterior, which conditions each dimension only on the data
example, x, the distributions in Eq. (4) account for latent dependencies by conditioning on sam-
ples from the parent variables. Ladder VAE performs this conditioning by reusing the empirical
prior during inference, forming the approximate posterior by combining a “bottom-up” recognition
distribution and the “top-down” prior.
While Eqs. (3) and (4) permit separate latent dependencies for each individual latent dimension, the
dimensions are typically partitioned into a set of nodes, with dimensions within each node sharing
3
Published as a conference paper at ICLR 2019
the same parents. This improves computational efficiency by allowing priors and approximate pos-
teriors within each node to be calculated in parallel. Using zn to denote latent node n of N and
zpa(n) to denote the concatenation of its parent nodes, we can write the ELBO (Eq. (2)) as
L(x;θ, φ) = Eqφ(z∣χ) [logPθ(x∣z)] - ∑ Eqφ(z∣χ) [log qφ(Zn∣χ；*)] .	(5)
n=1	pθ (zn ∣zpa(n) )
Note that the KL divergence term in the ELBO can no longer be evaluated analytically, now requiring
a sampling-based estimate of the expectation (Kingma & Welling, 2014). While this can lead to
higher variance in ELBO estimates and the resulting gradients, models with latent dependencies
still tend to empirically outperform models with independence assumptions (Burda et al., 2016;
S0nderby et al., 2016). However, by increasing the number of nodes, the burden of devising a
suitable dependency structure falls upon the experimental practitioner. This is non-trivial, as the
structure may depend on the data and other model hyperparameters, such as the number of layers
in the deep networks, non-linearities, latent distributions, etc. Rather than relying on pre-defined
fully-connected structures (Kingma et al., 2016) or chain structures (S0nderby et al., 2016), We seek
to automatically learn the latent dependency structure as part of the variational optimization process.
A comparison of these approaches is visualized in Fig. 1.
3	Variational Optimization of Latent Structures
Unlike the model parameters (φ, θ), which are optimized over a continuous domain, the latent de-
pendency structure is discrete, without a clear ordering. The discrete nature of the latent space’s
topological structure introduces discontinuities in the optimization landscape, complicating the
learning process. Fortunately, unlike the related setting of neural architecture search (Zoph & Le,
2016), there is only a finite number of possible dependency structures over a fixed number of latent
dimensions: In a directed graphical model, a fully-connected directed acyclic graph (DAG) models
all possible dependencies. In this model, an ordering is induced over the latent nodes, and the par-
ents of node n (of N) are given as zpa(n) = {zn+1, . . . , zN}.1 Thus, to learn an appropriate latent
dependency structure, we can maintain all dependencies in a fully-connected DAG, modifying their
presence or absence during training. This is accomplished by introducing a set of binary dependency
gates (Section 3.1). We convert discrete optimization over dependency structures into a continuous
optimization problem by parameterizing these gates as samples from Bernoulli distributions, then
learning the distribution parameters (Section 3.3). These gating distributions induce an additional
lower bound on L, which becomes tight when the distribution converges to a delta function, yielding
a single, optimized dependency structure (Section 3.2). Indeed, we observe this process empirically,
with the learned dependency structures outperforming their predefined counterparts (Section 4).
3.1	Gated Dependencies
To control the dependency structure of the model, we introduce a set of binary global variables,
c = {ci,j}i,j, which gate the latent dependencies. The element ci,j denotes the gate variable from
zi to zj (i > j), specifying the presence or absence of this latent dependency. Because each element
ofc takes values in {0, 1}, dependencies can be preserved or removed simply through (broadcasted)
element-wise multiplication with the corresponding parent nodes. Removing a dependency entails
multiplying the corresponding input parent node by 0. Each possible latent dependency structure
can now be expressed through its corresponding value of c.
Each fixed latent dependency structure, c', induces a separate latent variable model pθ(x, z, c)=
Pθ(x∣z, c)pθ(z∣c)δc,c', where δ.,. is the Kronecker delta, which effectively selects a single structure.
Similar to Eq. (3), the prior on the latent variables can now be expressed as
N
pθ (z∣c) = ∏ pθ (zn ∣zpa(n) , cpa(n),n ),	(6)
n=1
where cpa(n),n denotes the gate variables associated with the dependencies between node zn and
its parents, zpa(n). Note that zpa(n) denotes the set of all possible parents of node zn in the fully-
connected DAG, i.e. zpa(n) = {zn+1, . . . , zN}. To give a concrete example of the gating procedure,
1We follow convention (Dayan et al., 1995; Rezende et al., 2014; S0nderby et al., 2016), with parent nodes
having a larger index than their children.
4
Published as a conference paper at ICLR 2019
@ precision-weighted fusion
日 sampling
。multiplication
——bottom-up inference
——top-down inference
Figure 2: Local Distributions. We illustrate the parametrization of a local variable Zn in our struc-
tured representation. The local prior (Eq. (6)) is defined in terms of a top-down process (in black)
predicting the node,s parameters ψn from a gate-modulated sample of Zn,s parents Zpa(n). The local
approximate posterior (Eq. (7)) additionally performs a precision-weighted fusion of these parame-
ters with the result of a bottom-up process using a node-specific MLP to predict input-conditioned
parameters ψn, from a generic encoding of x.
consider the case in which pθ(Zn|zpa(n), Cpa(n),n) is given by a Gaussian density with parameters
ψn = *n, ∑n). We obtain these parameters recursively by multiplying samples of node ZnnS
parent variables Zpa(∩) with their corresponding gating variables cpa(n),n and input a concatenation of
the results of this operation into a multi-layer perceptron MLPnTD) predicting ψn (see Appendix
B for additional details on the MLP architecture). The top-down recursion starts at the root node
ZN Z P(ZN) = N(0, I). An illustration of this process is shown in black in Fig. 2.
The approximate posterior, qφ(Z∣x, c), must approximate pθ (Z∣x, c). We express the approximate
posterior as
N
qφ(Z∣x, c) = ∏ qφ(Zn∣x,
Zpa(n) , cpa(n) ).	(7)
n=1
We note that the dependency structures of the generative model and its corresponding posterior
are, in general, not independent: choosing a particular structure in the generative model induces a
particular structure in the posterior (Webb et al., 2018). A simple way to guarantee enough capacity
in the encoder to account for the dependencies implied by the decoder is thus to keep the encoder
graph fully-connected and learn a decoder graph only. Instead, we share the gating variables c
between approximate posterior and generative model (see Section 3.3), i.e., we assume that the
encoder dependencies mirror those of the decoder. As a consequence, the posterior implied by the
generative model could lie outside of the model class representable by the encoder. In practice, this
is not an issue and we observe significant performance improvements over both traditional VAEs
(where prior and posterior match but are limited in their expressiveness) and Graph VAEs with
fully-connected encoder graph. See Section 4.3 for quantitative experiments and Section 5 for a
discussion on the relationship between learned structures and fully-connected structures.
Parameter prediction for the local factors qφ(Zn∣x, Zpa(n), cpa(n)) consists of a precision-weighted
fusion of the top-down prediction ψn described above and a bottom-up prediction ψn. The latter is
obtained by encoding x into a generic feature that is used as an input to a node-specific multi-layer
perceptron MLPnBU) predicting ψn. This is shown in blue in Fig. 2. Additional details on the fusion
process can be found in Appendix B.3.
5
Published as a conference paper at ICLR 2019
Algorithm 1 Optimizing VAEs with Latent Dependency Structure
Require: Data x, number of latent nodes N, number of dimensions per node N'.
1:	Initialize θ, φ, μ.
2:	repeat
3:	Sample c using Eq. (9) and determine zpa(n) for each zn based on the sampled structure.
4:	For each node, compute qφ(zn∣x, zpa(n)) using Eq. (7).
5:	Sample z from qφ(z∣x) using Eq. (6) and compute pθ(x∣z).
6:	Update θ, φ, μ based on the gradients derived from Eq. (8).
7:	until Convergence.
3.2	Learning Structure by Inducing an Additional Lower B ound
The formulation in Section 3.1 provides the form of the model for a particular configuration of the
latent dependency structure. Finding the optimal structure corresponds to a discrete optimization
over all values of c, potentially optimizing the model parameters of each possible configuration. To
avoid this intractable procedure, we place a distribution over c, then directly optimize the parameters
of this distribution to arrive at a single, learned latent dependency structure. Specifically, we treat
each Cij as an independent random variable, sampled from a Bernoulli distribution with mean μi,j,
i.e. Cij 〜P(Cij) = B(μi,j). We denote the set of these Bernoulli means as μ. Introducing this
distribution allows us to express the following additional lower bound on L, derived in Appendix A:
L ≥ 2 = Ep(C) [Eqφ(z∣χ) [logPθ(x∣z, c)] - KL(qφ(z∣x)∣∣pθ(z∣c))] = Ep(c) [Lc],
(8)
where Lc is the ELBO for a particular value of dependency gating variables. Thus, L can be in-
terpreted as the expected ELBO under the distribution of dependency structures induced by p(c),
which we estimate by sampling C 〜p(c) and evaluating Lc. We note that L is not a proper varia-
tional bound, as it is not guaranteed to recover the marginal likelihood if the approximate posterior
matches the true posterior. Rather, optimizing L provides a method for learning the latent structure.
For fixed parameters (φ, θ), the optimal L w.r.t. μ is a δ-distribution at the MAP configuration, c*,
yielding L = L = Lc*. This is because Lc* is always greater than or equal to the expected ELBO
over all dependency gates, L. In practice, we jointly optimize φ, θ, and μ. While this non-convex
optimization procedure may result in local optima, we find this empirically works well, with p(c)
converging to a fixed distribution (Fig. 3). Thus, by the end of training, we are effectively optimizing
the ELBO for a single dependency structure. The training procedure is outlined in Algorithm 1.
3.3	Learning the Discrete Gating Variable Distributions
For a given latent dependency structure, gradients for the parameters θ and φ can be estimated using
Monte Carlo samples and the reparameterization trick (Kingma & Welling, 2014; Rezende et al.,
2014). To obtain gradients for the gate means, μ, We make use of recent advances in differentiating
through discrete operations (Maddison et al., 2017; Jang et al., 2017), allowing us to differenti-
ate through the sampling of the dependency gating variables, c. Specifically, we recast the gating
variables using the Gumbel-Softmax estimator from Jang et al. (2017), re-expressing Ci,j as:
exP((Iog(Mij) + e)τ)
c =----------:---：----——------------:------：-----.
,	exp((log(μij) + eι)∕τ) + exp((log(1 - μij) + e2)∕τ)
(9)
where 1 and 2 are i.i.d samples drawn from a Gumbel(0, 1) distribution and τ is a temperature
parameter. The Gumbel-Softmax distribution is differentiable for τ > 0, allowing us to estimate the
derivative 等j. For large values of T, we obtain smoothed versions of c, essentially interpolating
°μi,j
between different dependency structures. As τ → 0, we recover binary values for c, yielding the
desired discrete sampling of dependency structures at the cost of high-variance gradient estimates.
Thus, we anneal T during training to learn the dependency gate means, μ, eventually arriving at the
discrete setting.
6
Published as a conference paper at ICLR 2019
(b)
(a)
Figure 3: Structure Learning. In (a), we show training
of the Bernoulli parameters μi,j∙ governing the distribution
over graph structures in architecture space. All edges are
color-coded and can be located in (b), where we show a
random sample from the resulting steady-state distribution
with the same color scheme.
Figure 4: Ablation Study on MNIST.
For a fixed latent dimension M (color-
coded), we report the log-likelihood of
all possible factorizations of node di-
mension N' (x-axis) and number of
nodes N = M/N'.
4	Evaluation
We evaluate the proposed latent dependency learning approach on three benchmark datasets:
MNIST (Lecun et al., 1998; Larochelle & Murray, 2011), Omniglot (Lake et al., 2013), and CIFAR-
10 (Krizhevsky, 2009). After discussing the experimental setup in Section 4.1, we provide a set
of qualitative experiments in Section 4.2 to gain insight into the learning process, hyper-parameter
selection, and the nature of the inferred structures. In Section 4.3, we provide quantitative com-
parisons with common predefined latent dependency structures on benchmark datasets. Additional
results on training robustness and latent space embeddings can be found in Appendix C.
4.1	Experimental Setup
To provide a fair comparison, the encoders of all structured methods use the same MLP architecture
with batch normalization (Ioffe & Szegedy, 2015) and ReLU non-linearities (Nair & Hinton, 2010)
in all experiments.2 Decoder structures are the reverse of the encoders. Likewise, the number of
latent dimensions is the same in all models and experiments (M = 80). As discussed in Section 3.1,
all latent dependencies are modeled by non-linear MLPs as well.
For MNIST and Omniglot, we binarize the data and modelpθ(x∣z) as a Bernoulli distribution, using
a sigmoid non-linearity on the output layer to produce the mean of this distribution. For CIFAR-10,
we model pθ (x∣z) with a Gaussian density, with mean and log-variance predicted by sigmoid and
linear functions, respectively. Further implementation details, including the model architectures and
training criteria, can be found in Appendix B.
4.2	Qualitative Analysis
We first explore the structure learning process. As described in Section 3.2 and Appendix A, op-
timizing L w.r.t. the dependency gating means, μ, should push this lower bound toward L. Thus,
μ should converge to either 0 or 1, yielding a fixed, final latent dependency structure. In Fig. 3,
we visualize this process during training on MNIST. The model has N = 5 nodes and a total latent
dimension of M = 80 (i.e., N' = M/N = 16 dimensions per node). As shown in Fig. 3a, the gating
means converge in practice, with 3 out of 10 edges removed and the rest retained. The resulting
static dependency structure is visualized in Fig. 3b. We observed that the learned structure is stable
across training runs with different seeds for parameter initialization and mini-batch sampling, sup-
porting the hypothesis that the inferred structure indeed depends on the model parameterization and
the dataset.
2We implement classic VAEs using a more complex encoder to match the number of parameters of the
structured methods. All baselines use the same or more parameters than Graph VAE.
7
Published as a conference paper at ICLR 2019
Dataset Method	LL
ISINn l-oβ'≡UIO o'xve□
KL	ELBO
VAE	-87.30 ± 0.04	26.28 ± 0.05	-95.61 ± 0.07
Ladder VAE	-84.98 ± 0.10	24.25 ± 0.12	-92.80 ± 0.14
FC-VAE	-82.80 ± 0.15	22.47 ± 0.09	-88.49 ± 0.12
Graph VAE	-82.58 ± 0.08	22.49 ± 0.10	-88.35 ± 0.14
VAE	-109.62 ± 0.09	32.11 ± 0.07	-115.43 ± 0.12
Ladder VAE	-105.92 ± 0.16	29.46 ± 0.10	-110.62 ± 0.17
FC-VAE	-105.01 ± 0.26	30.74 ± 0.12	-108.79 ± 0.21
Graph VAE	-104.57 ± 0.19	29.93 ± 0.10	-108.17 ± 0.24
VAE
Ladder VAE
FC-VAE
Graph VAE
-6.37 ± △
-6.10 ± 0.02
-5.89 ± 0.04
-5.81 ± 0.04
0.10 ± △	-6.42 ± △
0.08 ± △	-6.13 ± 0.02
0.07 ± △	-5.90 ± 0.04
0.07 ± △	-5.82 ± 0.04
Table 1: Quantitative Analysis. Test log-likelihood (LL), DKL(qφ(z∣x)∣∣pθ(z)) (KL), and ELBO
for the proposed Graph VAE model and the baseline models with predefined dependency structures.
We show mean and standard deviation over 5 independent runs, where △ indicates a value < 0.01.
We next investigate the influence of the total latent dimension, M , and the trade-off between the
number of nodes, N, and the node dimension, N' = M/N. Our results for various models trained
on MNIST are shown in Fig. 4. Models with the same total latent dimension are shown in the
same color. We observe that the performance improves with increasing total latent dimension, likely
resulting from the additional flexibility of the higher-dimensional latent space. We also observe
that, for a fixed number of latent dimensions, models with fewer node dimensions (and therefore
more nodes with a more complex dependency structure) typically perform better. This highlights
the importance of using an expressive dependency structure for obtaining a flexible model.
4.3	Quantitative Comparison
To quantitatively evaluate the improvements due to learning the latent dependency structure, we
compare with a range of common, predefined baseline structures. These baselines include classic
VAEs (Kingma & Welling, 2014; Rezende et al., 2014), which contain no dependencies in the prior,
Ladder VAEs (S0nderby et al., 2016), which contain chain-like dependencies in the prior, and fully-
connected VAEs (FC-VAEs) (cf. Kingma et al. (2016)), which contain all possible dependencies in
the prior (corresponding to all gating variable parameters μi,j set to a fixed value of 1). We note that
our approach is orthogonal and could be complemented by a number of other approaches attempting
to overcome the limitations of classic VAEs. Similar to ladder VAEs, Zhao et al. (2017) use chain-
structured latent dependencies to learn disentangled representations. Normalizing flows (Rezende
& Mohamed, 2015), on the other hand, adds dependencies to the approximate posterior through a
series of invertible transformations.
We evaluate the performance of all models using their test log-likelihood, logpθ(x), in 5 indepen-
dent runs (Table 1). All values were estimated using 5, 000 importance-weighted samples. Follow-
ing standard practice, we report log pθ (x) in nats on MNIST/Omniglot and in bits/input dimension
on CIFAR-10. The learned dependency structure in our proposed Graph VAE consistently outper-
forms models with both fewer (VAE, ladder VAE) and more (FC-VAE) latent dependencies. We
discuss potential reasons in Section 5. To provide further insight into the training objective, Table 1
also reports DKL(qφ(z∣x)∣∣pθ(z)) and the ELBO for each model on the test set.
Encoder-Decoder Relationship. From a purely theoretical point of view, learning the structure
of the generative model implies the need for a fully-connected graph in the approximate posterior
(see Section 3.1). In practice, we share the gating variables c between encoder and decoder (see
Eq. (8)), because we observed improved empirical performance when doing so: training a Graph
VAE decoder with a predefined, fully-connected encoder graph results in a mean test log-likelihood
of -83.40 nats on MNIST, which is worse than the performance of FC-VAE (-82.80 nats) and
Graph VAE (-82.58 nats). We believe these are noteworthy empirical results, but further research is
8
Published as a conference paper at ICLR 2019
required to understand this behavior at a theoretical level. A full visualization of the training process
is provided in Appendix C.1.
5	Discussion
Performance vs. Speed/Memory. As shown in Fig. 4, the number of latent nodes can significantly
impact the performance of our model. While allowing more complex dependency structures through
alow M /N -ratio is typically beneficial, it also has an adverse effect on the training time and memory
consumption. Fortunately, the ability to freely select this ratio allows a simple adaption to the
available processing power, hardware constraints, and application scenarios.
Optimization Order. It is worth noting that the learning process optimizes the model parameters
(c, φ, θ) in a clear temporal order. While the latent structure, governed by c, converges during the
first ≈ 200 epochs (Fig. 3a), it takes over 10× as long until the variational and generative parameters
(φ, θ ) converge. There is no external force enforcing this behaviour, indicating that the loss can
initially most easily be decreased by limiting the latent structure to the complexity prescribed by the
observed training data.
Performance Improvement over Fully-Connected VAEs. There is an intricate relationship be-
tween fully-connected graphs vs. learned graphs along one axis and between prior structures vs.
posterior structures along a separate axis: FC-VAEs model all conditional latent dependencies and
are thus potentially more expressive and flexible than other latent dependency structures. It is there-
fore somewhat surprising that the learned latent structures in Graph VAE consistently outperform
the FC-VAE baseline. We speculate that this may be due to difficulties in optimization, which is
a known problem in hierarchical latent variable models (Bowman et al., 2016; Burda et al., 2016).
Graph VAE and FC-VAE both contain the same hypothesis space of possible models that can be
learned. If all dependencies are needed, Graph VAE could set all dependency gate parameters to 1.
Likewise, if a latent dependency was unnecessary, FC-VAE could set all of the model parameters in
that dependency to 0. However, this would require many coordinated steps along multiple parameter
dimensions. It is plausible that the benefit of learning the dependency structure may stem from the
ability to alter the optimization landscape, using the dependency gates to move through the model
parameter space more coarsely and rapidly. The resulting latent dependency structures may thus be
less expressive, but easier to optimize. While only an intuition, this hypothesis is also in line with
the observations and results in our experiments with fully-connected encoder graphs and learned
decoder graphs, where the theoretically more flexible FC-encoder is outperformed by our parameter
sharing approach. Follow-up work will be required to test this intuition.
6	Conclusion
We presented a novel method for structure learning in latent variable models, which uses depen-
dency gating variables, together with a modified objective and discrete differentiation techniques,
to effectively transform discrete structure learning into a continuous optimization problem. In our
experiments, the learned latent dependency structures improve the performance of latent variable
models over comparable baselines with predefined dependency structures. The approach presented
here provides directions for further research in structure learning for other tasks, including undi-
rected graphical models, time-series models, and discriminative models.
9
Published as a conference paper at ICLR 2019
References
A. Barron, J. Rissanen, and B. Yu. The Minimum Description Length Principle in Coding and
Modeling. In IEEE Transactions on Information Theory, 1998.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio.
Generating sentences from a continuous space. In Proceedings of The 20th SIGNLL Conference
on Computational Natural Language Learning, pp. 10-21, 2016.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In Inter-
national Conference on Learning Representations, 2016.
Jie Cheng, Russell Greiner, Jonathan Kelly, David Bell, and Weiru Liu. Learning Bayesian Networks
from Data: An Information-theoretic Approach. In Artificial Intelligence, 2002.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
gio. A recurrent latent variable model for sequential data. In Advances in neural information
processing systems, pp. 2980-2988, 2015.
Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine.
Neural computation, 7(5):889-904, 1995.
Marco Fraccaro, S0ren Kaae S0nderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. In Advances in neural information processing systems, pp. 2199-2207,
2016.
Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, Eric P Xing, and Carnegie Mellon.
Nonparametric variational auto-encoders for hierarchical representation learning. In ICCV, pp.
5104-5112, 2017.
Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregres-
sive networks. In International Conference on Machine Learning, pp. 1242-1250, 2014.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. Draw: A recurrent
neural network for image generation. In International Conference on Machine Learning, pp.
1462-1471, 2015.
Jiawei He, Andreas Lehrmann, Joseph Marino, Greg Mori, and Leonid Sigal. Probabilistic video
generation using holistic attribute control. In European Conference on Computer Vision, 2018.
D. Heckerman, C. Meek, and G. Cooper. A Bayesian Approach to Causal Discovery. In Computa-
tion, Causation, Discovery, 1999.
David Heckerman, Dan Geiger, and David M. Chickering. Learning Bayesian Networks: The Com-
bination of Knowledge and Statistical Data. In Machine Learning, 1995.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumble-softmax. In
International Conference on Learning Representations, 2017.
Matthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. Com-
posing graphical models with neural networks for structured representations and fast inference.
In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 29, pp. 2946-2954. Curran Associates, Inc., 2016.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
R. Kass and A. Raftery. Bayes Factors. In Journal of the American Statistical Association, 1995.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015.
10
Published as a conference paper at ICLR 2019
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations, 2014.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In Advances in Neural Informa-
tion Processing Systems, pp. 4743-4751, 2016.
M. Koivisto and K. Sood. Exact Bayesian Structure Discovery in Bayesian Networks. In Journal of
Machine Learning Research, 2004.
Daphne Koller and Nir Friedman. Probabilistic Graphical Models. MIT Press, 2009.
Rahul G Krishnan, Dawen Liang, and Matthew Hoffman. On the challenges of learning with infer-
ence networks on sparse, high-dimensional data. arXiv preprint arXiv:1710.06085, 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer,
2009.
Brenden M Lake, Ruslan R Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting
a compositional causal process. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 2526-2534.
Curran Associates, Inc., 2013.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings
of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29-37,
2011.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, Nov 1998. ISSN 0018-9219. doi:
10.1109/5.726791.
Erich L. Lehmann and Joseph P. Romano. Testing Statistical Hypotheses. Springer, 2008.
Andreas M. Lehrmann and Leonid Sigal. Non-parametric structured output networks. In Advances
in neural information processing systems, 2017.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. International Conference on Learning Representations,
2017.
Joseph Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. In International
Conference on Machine Learning, pp. 3400-3409, 2018.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional Conference on Machine Learning, pp. 1530-1538, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, pp. 1278-1286, 2014.
Casper Kaae S0nderby, TaPani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Advances in neural information processing systems, pp. 3738-3746,
2016.
Sharad Vikram, Matthew D Hoffman, and Matthew J Johnson. The loracs prior for vaes: Letting the
trees speak for the data. arXiv preprint arXiv:1810.06891, 2018.
11
Published as a conference paper at ICLR 2019
Stefan Webb, Adam Golinski, Robert Zinkov, N Siddharth, Tom Rainforth, Yee Whye Teh, and
Frank Wood. Faithful inversion of generative models for effective amortized inference. In Ad-
vances in Neural Information Processing Systems, 2018.
Li Yingzhen and Stephan Mandt. Disentangled sequential autoencoder. In International Conference
on Machine Learning,pp. 5656-5665, 2018.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from deep gener-
ative models. In International Conference on Machine Learning, pp. 4091-4099, 2017.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
12
Published as a conference paper at ICLR 2019
A Lower B ound Derivation
Introducing the distribution over dependency gating variables, P(C) = B(μ), modifies the evidence
lower bound (ELBO) from Eq. (2), as we now have an additional set of random variables over which
to marginalize. To see this, we can start by re-expressing Eq. (2) as
L = Eqφ(z∣x) [log pθ (x, z)] -Eqφ(z∣x) [logqφ(z∣x)] .	(10)
pθ(x, z) can be expressed as a marginalization over the gating variables, effectively averaging over
an ensemble of models with a distribution of dependency structures:
pθ (x, z) = ∫ pθ (x, z, C)dC = ∫ pθ(x, z∣C)p(C)dC = Ep(c) [pθ (x, z∣C)] .
Plugging this into Eq. (10):
L = Eqφ(z∣x) [log Ep(c) [pθ (x, z∣C)]] -Eqφ(z∣x) [logqφ(z∣x)] .
Using Jensen's inequality, We bring the log inside of the expectation, Ep(C) [∙], yielding
L ≥ ^L = Eqφ(z∣x) [Ep(C) [log Pθ (x,z∣c)]] - Eqφ(z∣x) [log qφ(z∣x)],
where L is a lower bound on L. Swapping the order of expectation, we rewrite L as
E = Ep(c) [Eqφ(z∣χ) [logPθ(x,z∣c)]] - Eqφ(z∣x) [logqφ(z∣x)],
(11)
(12)
(13)
(14)
and because the second term is independent of p(c), we can include both terms inside of a single
outer expectation:
LE= Ep(C) [Eqφ(z∣x) [logpθ (x, z∣c) - logqφ(z∣x)]]
= Ep(C) [Eqφ(z∣x) [log pθ (x∣z, c)] -KL(qφ(z∣x)∣∣pθ(z∣c))]	(15)
= Ep(C) [LC] ,
where we have defined LC as the ELBO for a given dependency structure. Note that LE is not a
proper variational bound, as it cannot recover the marginal log likelihood. Rather, LE allows us to
optimize the distribution over gating variables and, thus, the model structure. When we arrive at a
fixed structure, we will be optimizing the variational bound for that particular dependency structure.
To see this, note that the bound in Eq. 13 becomes tight when p(c) is any fixed distribution, in which
the dependency gating means, μ, are all either 1 or 0. Plugging in a delta distribution for p(c) at a
particular configuration, c', i.e. p(c) = δc,c', we have
L = Eqφ(z∣χ) [logEδc,c' [Pθ(x,z∣c)]] - Eqφ(z∣x) [logqφ(z∣x)]
=Eqφ(z∣x) [Eδc,c' [logPθ(x,z∣c)]] - Eqφ(z∣x) [logqφ(z∣x)]	(16)
Intuitively, this is simply the case of a latent variable model with predefined, fixed dependencies.
By optimizing L w.r.t. μ, we hope to collapse p(c) to a delta distribution at the optimal (MAP)
configuration, c*, because
Le = Ek。* [Lc] ≥ Ep(C) [Lc] = L	(17)
Effectively, we can search for the dependency structure with the highest ELBO by optimizing the
distribution parameters of p(c). Although this optimization procedure may arrive at locally optimal
dependency structures, our hope is that these learned structures will still perform better than an
arbitrary, predefined dependency structure.
13
Published as a conference paper at ICLR 2019
B Implementation Details
B.1	Network Architecture
We document the network architectures used in our experiments. We use the same network archi-
tectures for all datasets. Input size (pixels per image) is the only difference across datasets. The
input_size is 28 X 28 for MNIST and Omniglot, and 3 X 32 X 32 for CIFAR-10.
Encoders:
fc(input_size,512) → batch,norm → ELU → fc(512,512) → batch,norm → ELU →
fc(512,256) → batch_norm → ELU → fc(256,128)
Latent Dependencies: Latent dependencies are modelled by non-linear MLPs. Note that the top-
down architecture is shared between inference model and generative model, but the MLPs are opti-
mized independently.
bottom-up: For each node with N' dimensions, the local potential is predicted by a mapping from
the encoded feature:
fc(128,128) → batch_norm → ELU → fc(128, N'). The output feature is then mapped to
μ and logvar with two independent fc(N', N') layers, respectively.
top-down: For each node (N' dimensions) with a set of parent nodes, the top-down inference/gen-
eration is implemented as:
fc(sum of parent nodes' dimension, 128) → batch,norm → ELU →
fc(128, N'). The output feature is then mapped to μ and log Var with two independent
fc(N', N') layers, respectively.
Decoders:
fc(N', 256) → batch,norm → ELU → fc(256,512) → batch,norm → ELU →
fc(512,512) → batch,norm → ELU → fc(512,input_size) → Output_function()
output-function for MNIST and Omniglot is Sigmoid (), which predicts the mean μ of
Bernoulli observations; and Sigmoid () predicting μ, fc(input_size,input_size) pre-
dicting log var of Gaussion observations for CIFAR-10.
B.2	Training
All models were implemented with PyTorch (Paszke et al., 2017) and trained using the Adam
(Kingma & Ba, 2015) optimizer with a mini-batch size of 64 and learning rate of 1e-3. Learn-
ing rate is decresed by 0.25 every 200 epochs. The Gumbel-softmax temperature was initialized at
1 and decreased to 0.99epoch at each epoch. MNIST and Omniglot took 2000 epochs to converge,
and CIFAR took 3000 epochs to converge.
B.3	Inference Module Details
Parameter prediction for the local factors qφ(zn∣x, zpa(n)) consists of a precision-weighted fusion
of the top-down prediction ψn and a bottom-up prediction ψn. Specifically, for a latent variable Zn,
ψn ：= {Mn, σn}, and ψn ：= {μn,方4}.
Bottom-up Inference. A high-dimensional input is first mapped to a feature vector hx by an encoder
MLP. hχ is then used to predict μn and σrb with non-linear MLPBU.
Top-down Inference. fin, and σn, are predicted by fin, = MLPTD ([zpa(η)⑦ Cpa(n),n]) and σn,=
MLPTD ([zpa(n) O Cpa(n),n]), respectively. [,] denotes concatenation operation, and Θ denotes
element-wise multiplication.
Precision-weighted fusion. Having ψin and ψσn , the parameters of the local conditional distribution
is given by qφ(Zn∣X, Zpa(n),cpa(n),n )〜N(Zn ∣μn, σnn), with
14
Published as a conference paper at ICLR 2019
_	1
σn = σn2τ¾2,
μn
-2	-2
μnσn + μnσn
σn2 + ^n2
(18)
15
Published as a conference paper at ICLR 2019
C Additional Results
C.1 Robustness of Log-likelihoods
In Fig. 5, we report the averaged test log-likelihoods and associated standard deviations of Graph
VAE and our baselines at different epochs. All calculations are based on 5 independent runs.
Figure 5: Comparison of Test-time Log-Likelihoods on MNIST. Models are trained 5 times indi-
vidually, and test-time performances are evaluated every 300 epochs, with mean log-likelihood and
standard deviation indicated by the colored bars.
C.2 Latent Embeddings
Our training objective optimizes intrinsic structure (which does not necessarily correlate with se-
mantic meaning) and does not incentivize a disentanglement of latent factors. Interestingly, a TSNE-
visualization of the data as well as latent embeddings of Graph VAE and VAE on MNIST (Fig. 6)
shows that the latent embedding of Graph VAE exhibits a large (semantic) gap between different
classes, even though the model is trained in an unsupervised fashion. We will further investigate this
behavior in future work.
(a) Graph VAE
(b) VAE
(c) Data
Figure 6: TSNE-Visualization of Latent Embeddings on MNIST. We visualize embeddings of (a)
Graph VAE, (b) VAE, and (c) the data itself.
16