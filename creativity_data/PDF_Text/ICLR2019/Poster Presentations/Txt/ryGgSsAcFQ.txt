Published as a conference paper at ICLR 2019
Deep, Skinny Neural Networks are not Uni-
versal Approximators
Jesse Johnson
Sanofi
jejo.math@gmail.com
Ab stract
In order to choose a neural network architecture that will be effective for a par-
ticular modeling problem, one must understand the limitations imposed by each
of the potential options. These limitations are typically described in terms of in-
formation theoretic bounds, or by comparing the relative complexity needed to
approximate example functions between different architectures. In this paper, we
examine the topological constraints that the architecture of a neural network im-
poses on the level sets of all the functions that it is able to approximate. This
approach is novel for both the nature of the limitations and the fact that they are
independent of network depth for a broad family of activation functions.
1	Introduction
Neural networks have become the model of choice in a variety of machine learning applications, due
to their flexibility and generality. However, selecting network architectures and other hyperparame-
ters is typically a matter of trial and error. To make the choice of neural network architecture more
straightforward, we need to understand the limits of each architecture, both in terms of what kinds
of functions any given network architecture can approximate and how those limitations impact its
ability to learn functions within those limits.
A number of papers (3; 6; 11; 13) have shown that neural networks with a single hidden layer
are a universal approximator, i.e. that they can approximate any continuous function on a compact
domain to arbitrary accuracy if the hidden layer is allowed to have an arbitrarily high dimension. In
practice, however, the neural networks that have proved most effective tend to have a large number
of relatively low-dimensional hidden layers. This raises the question of whether neural networks
with an arbitrary number of hidden layers of bounded dimension are also a universal approximator.
In this paper we demonstrate a fairly general limitation on functions that can be approximated with
the L∞ norm on compact subsets of a Euclidean input space by layered, fully-connected feed-
forward neural networks of arbitrary depth and activation functions from a broad family including
sigmoids and ReLus, but with layer widths bounded by the dimension of the input space. By a
layered network, we mean that hidden nodes are grouped into successive layers and each node is
only connected to nodes in the previous layer and the next layer. The constraints on the functions
are defined in terms of topological properties of the level sets in the input space.
This analysis is not meant to suggest that deep networks are worse than shallow networks, but
rather to better understand how and why they will perform differently on different data sets. In fact,
these limitations may be part of the reason deep nets have proven more effective on datasets whose
structures are compatible with these limitations.
By a level set, we mean the set of all points in the input space that the model maps to a given value
in the output space. For classification models, a level set is just a decision boundary for a particular
cutoff. For regression problems, level sets don’t have a common interpretation.
The main result of the paper, Theorem 1, states that the deep, skinny neural network architectures
described above cannot approximate any function with a level set that is bounded in the input space.
This can be rephrased as saying that for every function that can be approximated, every level set
must be unbounded, extending off to infinity.
1
Published as a conference paper at ICLR 2019
While a number of recent papers have made impressive progress in understanding the limitations of
different neural network architectures, this result is notable because it is independent of the number
of layers in the network, and because the limitations are defined in terms ofa very simple topological
property. Topological tools have recently been employed to study the properties of data sets within
the field known as Topological Data Analysis (9), but this paper exploits topological ideas to examine
the topology of the models themselves. By demonstrating topological constraints on a widely used
family of models, we suggest that there is further potential to apply topological ideas to understand
the strengths and weaknesses of algorithms and methodologies across machine learning.
After discussing the context and related work in Section 2, we introduce the basic definitions and
notation in Section 3, then state the main Theorem and outline the proof in Section 4. The de-
tailed proof is presented in Sections 5 and 6. We present experimental results that demonstrate the
constraints in Section 7, then in Section 8 we present conclusions from this work.
2	Related Work
A number of papers have demonstrated limitations on the functions that can be approximated by
neural networks with particular architectures (2; 12; 14; 15; 18; 19; 21; 22; 23; 24; 25; 27; 29; 32).
These are typically presented as asymptotic bounds on the size of network needed to approximate
any function in a given family to a given ε.
Lu et al (17) gave the first non-approximation result that is independent of complexity, showing that
there are functions that no ReLu-based deep network of width equal to the dimension of the input
space can approximate, no matter how deep. However, they consider convergence in terms of the L1
norm on the entire space Rn rather than L∞ on a compact subset. This is a much stricter definition
than the one used in this paper so even for ReLu networks, Theorem 1 is a stronger result.
The closest existing result to Theorem 1 is a recent paper by Nguyen, Mukkamala and Hein (26)
which shows that for multi-label classification problems defined by an argmax condition on a higher-
dimensional output function, if all the hidden layers of a neural network have dimension less than
or equal to the input dimension then the region defining each class must be connected. The result
applies to one-to-one activation functions, but could probably be extended to the family of activation
functions in this paper by a similar limiting argument.
Universality results have been proved for a number of variants of the networks described in The-
orem 1. Rojas (28) showed that any two discrete classes of points can be separated by a decision
boundary of a function defined by a deep, skinny network in which each layer has a single perceptron
that is connected both to the previous layer and to the input layer. Because of the connections back
to the input space, such a network is not layered as defined above, so Theorem 1 doesn’t contradict
this result. In fact, to carry out Rojas’ construction with a layered feed-forward network, you would
need to put all the perceptrons in a single hidden layer.
Sutskever and Hinton (30) showed that deep belief networks whose hidden layers have the same
dimension as the input space can approximate any function over binary vectors. This binary input
space can be interpreted as a discrete subset of Euclidean space. So while Theorem 1 does not apply
to belief networks, it’s worth noting that any function on a discrete set can be extended to the full
space in such a way that the resulting function satisfies the constraints in Theorem 1.
This unexpected constraint on skinny deep nets raises the question of whether such networks are
so practically effective despite being more restrictive than wide networks, or because of it. Lin,
Tegmark and Rolnick (16) showed that for data sets with information-theoretic properties that are
common in physics and elsewhere, deep networks are more efficient than shallow networks. This
may be because such networks are restricted to a smaller search space concentrated around functions
that model shapes of data that are more likely to appear in practice. Such a conclusion would
be consistent with a number of papers showing that there are functions defined by deep networks
that can only by approximated by shallow networks with asymptotically much larger number of
nodes (4; 7; 10; 20; 31).
A slightly different phenomenon has been observed for recurrent neural networks, which are univer-
sal approximators of dynamic systems (8). In this setting, Collins, Sohl-Dickstein and Sussillo (5)
showed that many differences that have been reported on the performance of RNNs are due to their
2
Published as a conference paper at ICLR 2019
training effectiveness, rather than the expressiveness of the networks. In other words, the effective-
ness of a given family of models appears to have less to do with whether it includes an accurate
model, and more to do with whether a model search algorithm like gradient descent is likely to find
an accurate model within the search space of possible models.
3	Terminology and Notation
A model family is a subset M of the space C(Rn, Rm) of continuous functions from input space Rn
to output space Rm . For parametric models, this subset is typically defined as the image of a map
Rk → C(Rn, Rm), where Rk is the parameter space. A non-parametric model family is typically
the union of a countably infinite collection of parametric model families. We will not distinguish
between parametric and non-parametric families in this section.
Given a function g : Rn → Rm, a compact subset A ⊂ Rn and a value > 0, we will say
that a second function f (, A)-approximates g if for every x ∈ A, we have |f (x) - g(x)| < .
Similarly, we will say that a model family M (, A)-approximates g if there is a function f in M
that (, A)-approximates g.
More generally, we will say that M approximates f if for every compact A ⊂ Rn and value > 0
there is a function f in M that (, A)-approximates g. This is equivalent to the statement that there
is a sequence of functions fi ∈ M that converges pointwise (though not necessarily uniformly) to g
on all of Rn. However, we will use the (, A) definition throughout this paper.
We’ll describe families of layered neural networks with the following notation: Given an activation
function 夕：R → R and a finite sequence of positive integers n0,nι,..., nκ, let Nφ,no,m,…,nκ be
the family of functions defined by a layered feed-forward neural network with n0 inputs, nκ outputs
and fully connected hidden layers of width n1, . . . , nκ-1.
With this terminology, Hornik et al’s results can be restated as saying that the (non-parametric)
model family defined as the union of all families Nφ,no,m,ι approximates any continuous function.
(Here, κ = 2 and n2 = 1.)
We're interested in deep networks with bounded dimensional layers, so we,ll let N/ be the union
of all the model families Nφ,no,m,...,nκ-ι ,1 such that n ≤ n for all i < κ.
For the main result, we will restrict our attention to a fairly large family of activation functions. We
will say that an activation function 夕 is uniformly approximated by one-to-onefUnctions if there is a
sequence of continuous, one-to-one functions that converge to 夕 uniformly (not just pointwise).
Note that if the activation function is itself one-to-one (such as a sigmoid) then we can let every
function in the sequence be 夕 and it will converge uniformly. For the ReLu function, We need to
replace the the large horizontal portion with a function such as 1 arctan(x). Since this function is
one-to-one and negative for x < 0, each function in this sequence will be one-to-one. Since it’s
bounded between - ɪ and 0, the sequence will converge uniformly to the ReLu function.
4	Outline of the Main Result
The main result of the paper is a topological constraint on the level sets of any function in the family
of models N展,“ To understand this constraint, recall that in topology, a set C is path connected if
any two points in C are connected by a continuous path within C. A path component of a set A is a
subset C ⊂ A that is connected, but is not a proper subset of a larger connected subset of A.
Definition 1. We will say that a function f : Rn → R has unbounded level components if for every
y ∈ R, every path component of f -1(y) is unbounded.
The main result of this paper states that deep, skinny neural networks can only approximate functions
with unbounded level components. Note that this definition is stricter than just requiring that every
level set be bounded. The stricter definition in terms of path components guarantees that the property
is preserved by limits, a fact that we will prove, then use in the proof of Theorem 1. Just having
bounded level sets is not preserved under limits.
Theorem 1. For any integer n ≥ 2 and uniformly continuous activation function 夕：R → R that
can be approximated by one-to-one functions, the family of layered feed-forward neural networks
3
Published as a conference paper at ICLR 2019
Figure 1: A generic function defined by a deep network is the composition of a one-to-one function
and a linear function. Each level set is therefore homeomorphic to the intersection of an open
topological ball with an n - 1-dimensional hyperplane, which forces it to be unbounded.
y
with input dimension n in which each hidden layer has dimension at most n cannot approximate any
function with a level set containing a bounded path component.
The proof of Theorem 1 consists of two steps. In the first step, described in Section 5, we examine
the family of functions defined by deep, skinny neural networks in which the activation is one-to-one
and the transition matrices are all non-singular.
We prove two results about this smaller family of functions: First, Lemma 2 states that any function
that can be approximated by N；,n can be approximated by functions in this smaller family. This
is fairly immediate from the assumptions on 夕 and the fact that singular transition matrices can be
approximated by non-singular ones.
Second, Lemma 4 states that the level sets of these functions have unbounded level components. The
proof of this Lemma is, in many ways, the core argument of the paper and is illustrated in Figure 1.
The idea is that any function in this smaller family can be written as a composition of a one-to-one
function and a linear projection, as in the top row of the Figure.
As suggested in the bottom row, this implies that each level set/decision boundary in the full function
is defined by the intersection of the image of the one-to-one function (the gray patch in the middle)
with a hyperplane that maps to a single point in the second function. Intuitively, this intersection
extends out to the edges of the gray blob, so its preimage in the original space must extend out to
infinity in Euclidean space, i.e. it must be unbounded.
The second part of the proof of Theorem 1, described in Section 5, is Lemma 5 which states that the
limit of functions with unbounded level components also has unbounded level components. This is
4
Published as a conference paper at ICLR 2019
a subtle technical argument, though it should be intuitively unsurprising that unbounded sets cannot
converge to bounded sets.
The proof of Theorem 1 is the concatenation of these three Lemmas: If a function can be approx-
imated by N/ then it can be approximated by the smaller model family (Lemma 2), so it can
be approximated by functions with unbounded level components (Lemma 4), so it must also have
unbounded level components (Lemma 5).
5	Characterizing level sets of “generic” neural net functions
We will say that a function in N^n is non-singular if 夕 is continuous and one-to-one, n = n for all
i < k and the matrix defined by the weights between each pair of layers is nonsingular. Note that if
夕 is not one-to-one, then N^n will not contain any non-singular functions. If it is one-to-one then
N凄n will contain a mix of singular and non-singular functions.
Define the model family of non-singular functions Nn to be the union of all non-singular functions
in families N；n for all activation functions 夕 and a fixed n.
Lemma 2. If g is approximated by N；n for some continuous activation function 夕 that can be
uniformly approximated by one-to-one functions then it is approximated by Nn.
To prove this Lemma, we will employ a technical result from point-set topology, relying on the fact
that a function in N ln can be written as a composition of linear functions defined by the weights
between successive layers, and non-linear functions defined by the activation function 夕.
Lemma 3. Assume f : Rn → Rm is afunction that can be written as a composition f = f ◦∙••◦
f1 ◦ f0 where each fi : Rni → Rni+1 is a continuous function. Let A ⊂ Rn be a compact subset
and choose ε > 0.
Then there is a compact subset Ai ⊂ Rni for each i and δ > 0 such that if gi : Rni → Rni+1 is
a function that (δ, Ai)-approximates f for each i then the COmpOSitiOn g = gκ ◦ ∙∙∙ ◦ gι ◦ go will
(ε, A)-approximate g.
One can prove Lemma 3 by induction on the number of functions in the composition, choosing each
Ai ⊂ Rni to be a closed ε-neighborhood of the image of A in the composition up to i. For each
new function, the δ on the compact set tells you what δ you need to choose for the composition of
the preceding functions. We will not include the details here.
ProofofLemma 2. We,ll prove this Lemma by showing that Nn approximates any given function
in N^n. Then, given ε > 0, a compact set A ⊂ Rn and a function g that is approximated by
N展n, we can choose a function f ∈ N^n that (ε∕2, A)-approximates g and a function in Nn that
(ε∕2, A)-approximates f.
So we will reset the notation, let g be a function in N凄n, let A ⊂ Rn be a compact subset and
choose ε > 0. As noted above, g is a composition g = VK ◦ 'κ ◦ ∙∙∙ ◦ νo ◦ 'o where each ' is a
linear function defined by the weights between consecutive layers and each νi is a nonlinear function
defined by a direct product of the activation function 夕.
If any of the hidden layers in the network defining g have dimension strictly less than n then we can
define the same function with a network in which that layer has dimension exactly n, but the weights
in and out of the added neurons are all zero. Therefore, we can assume without loss of generality
that all the hidden layers in g have dimension exactly n, though the linear functions may be singular.
Let {Ai} and δ > 0 be as defined by Lemma 3. We want to find functions Vi and ' that (δ,Ai)-
approximate each Vi and ' and whose composition is in Nn
For the composition to be in Nn, we need each 'i to be non-singular. If ' is already non-singular,
then we choose `i = `i . Otherwise, we can perturb the weights that define the linear map `i by an
arbitrarily small amount to make it non-singular. In particular, we can choose this arbitrarily small
amount to be small enough that the function values change by less than δ on Ai .
5
Published as a conference paper at ICLR 2019
Similarly, We want each Vi to be a direct product of a continuous, one-to-one activation functions.
By assumption,夕 can be approximated by such functions and we can choose the tolerance for this
approximation to be small enough that Vi (δ, Ai)-approximates ν%. In fact, we can choose a single
activation function for all the nonlinear layers, on each corresponding compact set.
Thus we can choose each ' and an activation function 夕 that defines all the functions Vi, so that the
composition is in Nn and, by Lemma 3, the composition (ε, A)-approximates g.	□
6	Characterizing level sets in a limit of functions.
Lemma 2 implies that if N；n is universal then so is Nn So to prove Theorem 1, we will show that
every function in Nn has level sets with only unbounded components, then show that this property
extends to any function that it approximates.
Lemma 4. If f isa function in Nn then every level set f-1(y) is homeomorphic to an open (possibly
empty) subset ofRn-1. This implies that f has unbounded level components.
Proof. Assume f is a non-singular function in Nn where 夕 is continuous and one-to-one. Let
f : Rn → Rn be the function defined by all but the last layer of the network. Let f : Rn → R be
the function defined by the map from the last hidden layer to the final output layer so that f = f ◦ f.
The function f is a composition of the linear functions defined by the network weights and the non-
linear function at each step defined by applying the activation function to each dimension. Because
f is nonsingular, the linear functions are all one-to one. Because 夕 is continuous and one-to-one,
so are all the non-linear functions. Thus the composition f is also one-to-one, and therefore a
homeomorphism from Rn onto its image I^. Since Rn is homeomorphic to an open n-dimensional
ball, If is an open subset of Rn, as indicated in the top row of Figure 1.
The function f is the composition of a linear function to R with 夕，which is one-to-one by assump-
tion. So the preimage f-1(y) for any y ∈ R is an (n - 1)-dimensional plane in Rn. The preimage
f-1(y) is the preimage in f of this (n - 1)-dimensional plane, or rather the preimage of the inter-
section If ∩ f-1(y), as indicated in the bottom right/center of the Figure. Since I^ is open as a
subset of Rn, the intersection is open as a subset of f-1(y').
Since f is one-to-one, its restriction to this preimage (shown on the bottom left of the Figure) is a
homeomorphism from f-1(y) to this open subset of the (n - 1)-dimensional plane f-1(y). Thus
f-1(y) is homeomorphic to an open subset ofRn-1.
Finally, recall that the preimage in a continuous function of a closed set is closed, so f-1(y) is
closed as a subset of Rn . If it were also bounded, then it would be compact. However, the only
compact, open subset of Rn-1 is the empty set, so f-1(y) is either unbounded or empty. Since each
path component of a subset of Rn-1 is by definition non-empty, this proves that any component of
f is unbounded.	□
All that remains is to show that this property extends to the functions that Nn approximates.
Lemma 5. If M is a model family in which every function has unbounded level components then
any function approximated by M has unbounded level components.
Proof. Let g : Rn → R be a function with a level set g-1(y) containing a bounded path component
C. Note that level sets are closed as subsets of Rn and bounded, closed sets are compact so C is
compact. We can therefore choose a value μ such that any point of g-1(y) outside of C is distance
greater than μ from every point in C.
Let ηc be the set of all points that are distance strictly less than μ∕2 from C. This is an open subset
ofRn, shown as the shaded region in the center of Figure 2, and we will let F be the frontier ofηC
一 the set of all points that are limit points of both ηc and limit points of its complement.
6
Published as a conference paper at ICLR 2019
Figure 2: In the proof of Lemma 2, we choose ε by finding an intercal U = (y - ε, y + ε) whose
preimage contains a bounded component C.
By construction, every point in F is distance μ∕2 from C so F is disjoint from C. Moreover, since
every point in g-1(y) \ C is distance at least μ from C, F is disjoint from the rest of g-1(y) as well,
so y is in the complement of g(F).
The frontier is the intersection of two closed sets, so F is closed. It’s also bounded, since all points
are a bounded distance from C, so F is compact. This implies that g(F) is a compact subset of R,
so its complement is open. Since y is in the complement of g(F), this means that there is an open
interval U = (y - ε, y + ε) that is disjoint from g(F).
Let C be the component of g-1 (U) that contains C, as indicated on the right of the Figure. Note that
this set intersects ηc but is disjoint from its frontier. So C must be contained in ηc, and is therefore
bounded as well. In particular, each level set that intersects C has a compact component in C.
Let x be a point in C ⊂ C. Since C is bounded, there is a value r such that every point in C is
distance at most r from x.
Assume for contradiction that g is approximated by a model family M in which each function has
unbounded level components. Choose R > r and let BR(x) be a closed ball of radius R, centered
at x. Because this is a compact set and g is approximated by M, we can choose a function f ∈ M
that (ε∕2, BR(x))-approximates g.
Then |f (x) - g(x)| < ε∕2 so f(x) ∈ [y - ε∕2, y + ε∕2] ⊂ U and we will define y0 = f(x).
Since f ∈ M, every path component of f-1(y0) is unbounded, so there is a path ` ⊂ f-1(y0) from
x to a point that is distance R from x. If ` passes outside of BR(x)), we can replace ` with the
component of ` ∩ BR(x)) containing x to ensure that ` stays inside of BR(x)), but still reaches a
point that is distance R from x.
Since every point x00 ∈ ` is contained in BR(x), we have |f(x00) - g(x00)| < ε∕2. This implies
g(x00) ∈ [y - ε,y + ε] = U so the path is contained in g-1(U), and thus in the path component C
ofg-1(U).
However, by construction the path ` ends at a point whose distance from xis R > r, contradict-
ing the assumption that every point in C is distance at most r from x. This contradiction proves
that g cannot be approximated by a model family M in which each function has unbounded level
components.	□
ProofofTheorem 1. Let g be a function that is approximated by NJ,。, where 夕 is a continuous
activation function that can be uniformly approximated by one-to-one functions.
By Lemma 2, since g is approximated by NJ,n, it must also be approximated by Nn. By Lemma 4,
every function in Nn has bounded level components, so Lemma 5 implies that every function that
this family approximates has unbounded level sets. Therefore g has unbounded level sets. □
7
Published as a conference paper at ICLR 2019
(a) The decision boundary learned with six,
two-dimensional hidden layers is an un-
bounded curve that extends outside the re-
gion visible in the image.
(b) A network with a single three-
dimensional hidden layer learns a bounded
decision boundary relatively easily.
7	Experiments
To demonstrate the effect of Theorem 1, we used the TensorFlow Neural Network Playground (1)
to train two different networks on a standard synthetic dataset with one class centered at the origin
of the two-dimensional plane, and the other class forming a ring around it. We trained two neural
networks and examined the plot of the resulting functions to characterize the level sets/decision
boundaries. In these plots, the decision boundary is visible as the white region between the blue and
orange regions defining the two labels.
The first network has six two-dimensional hidden layers, the maximum number of layers allowed
in the webapp. As shown in Figure 3a, the decision boundary is an unbounded curve that extends
beyond the region containing all the data points. The ideal decision boundary between the two
classes of points would be a (bounded) loop around the blue points in the middle, but Theorem 1
proves that such a network cannot approximate a function with such a level set. A decision boundary
such as the one shown in the Figure is as close as it can get. The extra hidden layers allow the
decision boundary to curve around and minimize the neck of the blue region, but they do not allow
it to pinch off completely.
The second network has a single hidden layer of dimension three - one more than that of the input
space. As shown in Figure 3b, the decision boundary for the learned function is a loop that approxi-
mates the ideal decision boundary closely. It comes from the three lines defined by the hidden nodes,
which make a triangle that gets rounded off by the activation function. Increasing the dimension of
the hidden layer would make the decision boundary rounder, though in this case the model doesn’t
need the extra flexibility.
Note that this example generalizes to any dimension n, though without the ability to directly graph
the results. In other words, for any Euclidean input space of dimension n, a sigmoid neural network
with one hidden layer of dimension n + 1 can define a function that cannot be approximated by any
deep network with an arbitrary number of hidden layers of dimension at most n. In fact, this will be
the case for any activation function that is bounded above or below, though we will not include the
details of the argument here.
8	Conclusion
In this paper, we describe topological limitations on the types of functions that can be approximated
by deep, skinny neural networks, independent of the number of hidden layers. We prove the result
using standard set theoretic topology, then present examples that visually demonstrate the result.
8
Published as a conference paper at ICLR 2019
This complements a body of existing literature that has demonstrated various limitations on neural
networks that typically take a very different form and are expressed in terms of asymptotic network
complexity. We expect that there is a great deal of remaining potential to explore further topological
constraints on families of models, and to determine to what extent these topological constraints are
simply a different way of describing more fundamental ideas that have been independently demon-
strated elsewhere in other frameworks such as information theory.
References
[1]	Tensorflow neural network playground. https://playground.tensorflow.org/.
Accessed: 2018-04-20.
[2]	J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in neural information
processing Systems, pages 2654-2662, 2014.
[3]	A. R. Barron. Approximation and estimation bounds for artificial neural networks. Machine
learning, 14(1):115-133, 1994.
[4]	N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor
analysis. In Conference on Learning Theory, pages 698-728, 2016.
[5]	J. Collins, J. Sohl-Dickstein, and D. Sussillo. Capacity and trainability in recurrent neural
networks. arXiv preprint arXiv:1611.09913, 2016.
[6]	G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of con-
trol, signals and systems, 2(4):303-314, 1989.
[7]	O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In Advances in Neural
Information Processing Systems, pages 666-674, 2011.
[8]	K. Doya. Universality of fully connected recurrent neural networks. Dept. of Biology, UCSD,
Tech. Rep, 1993.
[9]	H. Edelsbrunner and J. Harer. Persistent homology-a survey. Contemporary mathematics,
453:257-282, 2008.
[10]	R. Eldan and O. Shamir. The power of depth for feedforward neural networks. In Conference
on Learning Theory, pages 907-940, 2016.
[11]	K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks.
Neural networks, 2(3):183-192, 1989.
[12]	N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-dimension bounds for piecewise linear
neural networks. arXiv preprint arXiv:1703.02930, 2017.
[13]	K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal
approximators. Neural networks, 2(5):359-366, 1989.
[14]	V. Khrulkov, A. Novikov, and I. Oseledets. Expressive power of recurrent neural networks.
arXiv preprint arXiv:1711.00811, 2017.
[15]	S. Liang and R. Srikant. Why deep neural networks for function approximation? arXiv preprint
arXiv:1610.04161, 2016.
[16]	H. W. Lin, M. Tegmark, and D. Rolnick. Why does deep and cheap learning work so well?
Journal of Statistical Physics, 168(6):1223-1247, 2017.
[17]	Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural networks: A view
from the width. In Advances in Neural Information Processing Systems, pages 6232-6240,
2017.
[18]	J. Martens, A. Chattopadhya, T. Pitassi, and R. Zemel. On the representational efficiency of
restricted boltzmann machines. In Advances in Neural Information Processing Systems, pages
2877-2885, 2013.
9
Published as a conference paper at ICLR 2019
[19]	J. Martens and V. Medabalimi. On the expressive efficiency of sum product networks. arXiv
preprint arXiv:1411.7717, 2014.
[20]	H. N. Mhaskar and T. Poggio. Deep vs. shallow networks: An approximation theory perspec-
tive. Analysis and Applications,14(06):829-848, 2016.
[21]	G. Montufar, N. Ay, and K. Ghazi-Zahedi. Geometry and expressive power of conditional
restricted boltzmann machines. Journal of Machine Learning Research, 16:2405-2436, 2015.
[22]	G. F. Montufar. Universal approximation depth and errors of narrow belief networks with
discrete units. Neural computation, 26(7):1386-1407, 2014.
[23]	G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep
neural networks. In Advances in neural information processing systems, pages 2924-2932,
2014.
[24]	G. F. Montufar, J. Rauh, and N. Ay. Expressive power and approximation errors of restricted
boltzmann machines. In Advances in neural information processing systems, pages 415-423,
2011.
[25]	Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. arXiv preprint
arXiv:1704.08045, 2017.
[26]	Q. Nguyen, M. Mukkamala, and M. Hein. Neural networks should be wide enough to learn
disconnected decision regions. arXiv preprint arXiv:1803.00094, 2018.
[27]	R. Pascanu, G. Montufar, and Y. Bengio. On the number of response regions of deep feed
forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098, 2013.
[28]	R. Rojas. Networks of width one are universal classifiers. In Neural Networks, 2003. Proceed-
ings of the International Joint Conference on, volume 4, pages 3124-3127. IEEE, 2003.
[29]	D. Rolnick and M. Tegmark. The power of deeper networks for expressing natural functions.
arXiv preprint arXiv:1705.05502, 2017.
[30]	I. Sutskever and G. E. Hinton. Deep, narrow sigmoid belief networks are universal approxi-
mators. Neural computation, 20(11):2629-2636, 2008.
[31]	M. Telgarsky. Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016.
[32]	D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks,
94:103-114, 2017.
10