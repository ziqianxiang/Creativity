Published as a conference paper at ICLR 2019
Minimal Random Code Learning: Getting Bits
Back from Compressed Model Parameters
Marton Havasi
Department of Engineering
University of Cambridge
mh740@cam.ac.uk
Robert Peharz
Department of Engineering
University of Cambridge
rp587@cam.ac.uk
Jose Miguel Hernandez-Lobato
Department of Engineering
University of Cambridge,
Microsoft Research,
Alan Turing Institute
jmh233@cam.ac.uk
Ab stract
While deep neural networks are a highly successful model class, their large mem-
ory footprint puts considerable strain on energy consumption, communication
bandwidth, and storage requirements. Consequently, model size reduction has
become an utmost goal in deep learning. A typical approach is to train a set
of deterministic weights, while applying certain techniques such as pruning and
quantization, in order that the empirical weight distribution becomes amenable to
Shannon-style coding schemes. However, as shown in this paper, relaxing weight
determinism and using a full variational distribution over weights allows for more
efficient coding schemes and consequently higher compression rates. In particular,
following the classical bits-back argument, we encode the network weights using
a random sample, requiring only a number of bits corresponding to the Kullback-
Leibler divergence between the sampled variational distribution and the encoding
distribution. By imposing a constraint on the Kullback-Leibler divergence, we
are able to explicitly control the compression rate, while optimizing the expected
loss on the training set. The employed encoding scheme can be shown to be close
to the optimal information-theoretical lower bound, with respect to the employed
variational family. Our method sets new state-of-the-art in neural network com-
pression, as it strictly dominates previous approaches in a Pareto sense: On the
benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, our approach yields the
best test performance for a fixed memory budget, and vice versa, it achieves the
highest compression rates for a fixed test performance.
1	Introduction
With the celebrated success of deep learning models and their ever increasing presence, it has be-
come a key challenge to increase their efficiency. In particular, the rather substantial memory re-
quirements in neural networks can often conflict with storage and communication constraints, espe-
cially in mobile applications. Moreover, as discussed in Han et al. (2015), memory accesses are up
to three orders of magnitude more costly than arithmetic operations in terms of energy consumption.
Thus, compressing deep learning models has become a priority goal with a beneficial economic and
ecological impact.
Traditional approaches to model compression usually rely on three main techniques: pruning, quan-
tization and coding. For example, Deep Compression (Han et al., 2016) proposes a pipeline employ-
ing all three of these techniques in a systematic manner. From an information-theoretic perspective,
the central routine is coding, while pruning and quantization can be seen as helper heuristics to
reduce the entropy of the empirical weight-distribution, leading to shorter encoding lengths (Shan-
non, 1948). Also, the recently proposed Bayesian Compression (Louizos et al., 2017) falls into this
scheme, despite being motivated by the so-called bits-back argument (Hinton & Van Camp, 1993)
which theoretically allows for higher compression rates.1 While the bits-back argument certainly
1 Recall that the bits-back argument states that, assuming a large dataset and a neural network equipped
With a weight-prior p, the effective coding cost of the network weights is KL(q∣∣p) = Eq[log P], where q is
1
Published as a conference paper at ICLR 2019
motivated the use of variational inference in Bayesian Compression, the downstream encoding is
still akin to Deep Compression (and other approaches). In particular, the variational distribution is
merely used to derive a deterministic set of weights, which is subsequently encoded with Shannon-
style coding. This approach, however, does not fully exploit the coding efficiency postulated by the
bits-back argument.
In this paper, we step aside from the pruning-quantization pipeline and propose a novel coding
method which approximately realizes bits-back efficiency. In particular, we refrain from construct-
ing a deterministic weight-set but rather encode a random weight-set from the full variational pos-
terior. This is fundamentally different from first drawing a weight-set and subsequently encoding it
-this would be no more efficient than previous approaches. Rather, the coding scheme developed
here is allowed to pick a random weight-set which can be cheaply encoded. By using results from
Harsha et al. (2010), we show that such an coding scheme always exists and that the bits-back argu-
ment indeed represents a theoretical lower bound for its coding efficiency. Moreover, we propose a
practical scheme which produces an approximate sample from the variational distribution and which
can indeed be encoded with this efficiency. Since our algorithm learns a distribution over weight-
sets and derives a random message from it, while minimizing the resulting code length, we dub it
Minimal Random Code Learning (MIRACLE).
From a practical perspective, MIRACLE has the advantage that it offers explicit control over the
expected loss and the compression size. This is distinct from previous techniques, which require
tedious tuning of various hyper-parameters and/or thresholds in order to achieve a certain coding
goal. In our method, we can simply control the KL-divergence using a penalty factor, which di-
rectly reflects the achieved code length (plus a small overhead), while simultaneously optimizing
the expected training loss. As a result, we were able to trace the trade-off curve for compression
size versus classification performance (Figure 1). We clearly outperform previous state-of-the-art
in a Pareto sense: For any desired compression rate, our encoding achieves better performance on
the test set; vice versa, for a certain performance on the test set, our method achieves the highest
compression. To summarize, our main contributions are:
•	We introduce MIRACLE, an innovative compression algorithm that exploits the noise resis-
tance of deep learning models by training a variational distribution and efficiently encodes
a random set of weights.
•	Our method is easy to implement and offers explicit control over the loss and the compres-
sion size.
•	We provide theoretical justification that our algorithm gets close to the theoretical lower
bound on the encoding length.
•	The potency of MIRACLE is demonstrated on two common compression tasks, where it
clearly outperforms previous state-of-the-art methods for compressing neural networks.
In the following section, we discuss related work and introduce required background. In Section 3
we introduce our method. Section 4 presents our experimental results and Section 5 concludes the
paper.
2	Related Work
There is an ample amount of research on compressing neural networks, so that we will only discuss
the most prominent ones, and those which are related to our work. An early approach is Optimal
Brain Damage (LeCun et al., 1990) which employs the Hessian of the network weights in order to
determine whether weights can be pruned without significantly impacting training performance. A
related but simpler approach was proposed in Han et al. (2015), where small weights are truncated to
zero, alternated with re-training. This simple approach yielded - somewhat surprisingly - networks
which are one order of magnitude smaller, without impairing performance. The approach was re-
fined into a systematic pipeline called Deep Compression, where magnitude-based weight pruning is
followed by weight quantization (clustering weights) and Huffman coding (Huffman, 1952). While
a variational posterior. However, in order to realize this effective cost, one needs to encode both the network
weights and the training targets, while it remains unclear whether it can also be achieved for network weights
alone.
2
Published as a conference paper at ICLR 2019
its compression ratio (〜50 ×) has been surpassed since, many of the subsequent works took lessons
from this paper.
HashNet proposed by Chen et al. (2015) also follows a simple and surprisingly effective approach:
They exploit the fact that training of neural networks is resistant to imposing random constraints
on the weights. In particular, they use hashing to enforce groups of weights to share the same
value, yielding memory reductions of up to 64× with gracefully degrading performance. Weightless
encoding by Reagen et al. (2018) demonstrates that neural networks are resilient to weight noise, and
exploits this fact for a lossy compression algorithm. The recently proposed Bayesian Compression
(Louizos et al., 2017) uses a Bayesian variational framework and is motivated by the bits-back
argument (Hinton & Van Camp, 1993). Since this work is the closest to ours, albeit with important
differences, we discuss Bayesian Compression and the bits-back argument in more detail.
The basic approach is to equip the network weights w with a prior p and to approximate the posterior
using the standard variational framework, i.e. maximize the evidence lower bound (ELBO) for a
given dataset D
Eqφ[logP(Dw)] - KL(qφl∣p),	(1)
w.r.t. the variational distribution qφ, parameterized by φ. The bits-back argument (Hinton &
Van Camp, 1993) establishes a connection between the Bayesian variational framework and the
Minimum Description Length (MDL) principle (Grunwald, 2007). Assuming a large dataset D of
input-target pairs, we aim to use the neural network to transmit the targets with a minimal message,
while the inputs are assumed to be public. To this end, We draw a weight-set w* from q$, which has
been obtained by maximizing (1); note that knowing a particular weight w* set conveys a message
of length H[qφ] (H refers to the Shannon entropy of the distribution). The weight-set w* is used
to encode the residual of the targets, and is itself encoded with the prior distribution p, yielding a
message of length Eqφ [- log p(D|w)] + Eqφ [log p]. This message allows the receiver to perfectly
reconstruct the original targets, and consequently the variational distribution qφ, by running the same
(deterministic) algorithm as used by the sender. Consequently, with qφ at hand, the receiver is able
to retrieve an auxiliary message encoded in w*. When subtracting the length of this “free message”
from the original Eqe [log p] nats,2 we yield a net cost of KL( qφ ||p) = Eqe [log 专]nats for encoding
the weights, i.e. we recover the ELBO (1) as negative MDL (Hinton & Van Camp, 1993).
In (Hinton & Zemel, 1994; Frey & Hinton, 1997) coding schemes were proposed which practically
exploited the bits-back argument for the purpose of coding data. However, it is not clear how these
free bits can be spent solely for the purpose of model compression, as we only want to store a repre-
sentation of our model, while discarding the training data. Therefore, while Bayesian Compression
is certainly motivated by the bits-back argument, it actually does not strive for the postulated coding
efficiency KL(qφ∣∣p). Rather, this method imposes a sparsity inducing prior distribution to aid the
pruning process. Moreover, high posterior variance is translated into reduced precision which consti-
tutes a heuristic for quantization. In the end, Bayesian Compression merely produces a deterministic
weight-set w* which is encoded similar as in preceding works.
In particular, all previous approaches essentially use the following coding scheme, or a (sometimes
sub-optimal) variant of it. After a deterministic weight-set w* has been obtained, involving poten-
tial pruning and quantization techniques, one interprets w* as a sequence of i.i.d. variables, taking
values from a finite alphabet. Then one assumes the coding distribution p0(w) = N PN=I δw* (w),
where δx denotes the Kronecker delta at x. According to Shannon’s source coding theorem (Shan-
non, 1948), w* can be coded with no less than NH[p0] nats, which is asymptotically achieved by
Huffman coding, like in Han et al. (2016). Note that the Shannon lower bound can be written as
N
N H[P0] = — X log P0(w*) = — log P(W* = X δw*(w)log
i=1
3=KL(JIpO),⑵
w
where we have set p0(w) = ip0(wi). Thus, these Shannon-style coding schemes are in some
sense optimal, when the variational family is restricted to point-measures, i.e. deterministic weights.
By extending the variational family to comprise more general distributions q, the coding length
KL(q||p) could be drastically reduced. In the following, we develop such a method which exploits
the uncertainty represented by q in order to encode a random weight-set with short coding length.
2Unless otherwise stated, we refer to information theoretic measures in nats. For reference, 1 nat
log2 e bits ≈ 1.44 bits
3
Published as a conference paper at ICLR 2019
3 Minimal Random Code Learning
Consider the scenario where we want to train a neural network but our memory budget is constrained
to C nats. As illustrated in the previous section, a variational approach offers - in principle - a
simple and elegant solution. Before we proceed, we note that we do not consider our approach to
be a strictly Bayesian one, but rather based on the MDL principle, although these two are of course
highly related (Griinwald, 2007). In particular, we refer to P as an encoding distribution rather than a
prior, and moreover we will use a framework akin to the β-VAE (Higgins et al., 2017) which better
reflects our goal of efficient coding. The crucial difference to the β-VAE being that we encode
parameters rather than data.
Now, similar to Louizos et al. (2017), we first fix a suitable network architecture, select an encoding
distribution p and a parameterized variational family qφ for the network weights w. We consider,
however, a slightly different variational objective related to the β-VAE:
L(φ)= Eqφ[logP(D∣w)]-β KL(qφl∣p) .	(3)
、-------V-------}	、---------/
negative loss	model complexity
This objective directly reflects our goal of achieving both a good training performance (loss term)
and being able to represent our model with a short code (model complexity), at least according to
the bits-back argument. After obtaining qφ by maximizing (3), a weight-set drawn from qφ will
perform comparable to a deterministically trained network, since the variance of the negative loss
term will be comparatively small to the mean, and since the KL term regularizes the model. Thus,
our declared goal is to draw a sample from qφ such that this sample can be encoded as efficiently as
possible. This problem can be formulated as the following communication problem.
Alice observes a training data set (X, Y ) = D drawn from an unknown distribution p(D). She trains
a variational distribution qφ(w) by optimizing (3) for a given β using a deterministic algorithm.
Subsequently, she wishes to send a message M(D) to Bob, which allows him to generate a sample
distributed according to qφ . How long does this message need to be?
The answer to this question depends on the unknown data distribution p(D), so we need to make an
assumption about it. Since the variational parameters φ depend on the realized dataset D, we can
interpret the variational distribution as a conditional distribution q(w|D) := qφ(w), giving rise to
the joint q(w, D) = q(w|D)p(D). Now, our assumption about p(D) is that q(w |D)p(D) dD =
p(w), that is, the variational distribution qφ yields the assumed encoding distribution p(w), when
averaged over all possible datasets. Note that this a similar strong assumption as in a Bayesian
setting, where we assume that the data distribution is given as p(D) = p(D|w)p(w)dw. In
this setting, it follows immediately from the data processing inequality (Harsha et al., 2010) that in
expectation the message length |M| cannot be smaller than KL(qφ∣∣p):
ED[|M|] ≥ H[M] ≥ I[D : M] ≥ I[D : w]
/KL(q(w∣D)∣∣p(w))dD = ED[KL(q°∣∣p)], (4)
where I refers to the mutual information and in the third inequality we applied the data processing
inequality for Markov chain D → M → w . As discussed by Harsha et al. (2010), the inequal-
ity ED[|M|] ≥ ED[KL(qφ||p)] can be very loose. However, as they further show, the message
length can be brought close to the lower bound, if Alice and Bob are allowed to share a source of
randomness:
Theorem 3.1 (Harsha et al. (2010)) Given random variables D, w and a random string R, let
a protocol Π be defined via a message function M(D, R) and a decoder function w(M, R),
i.e. Π(D) = w(M (D, R), R). Let TΠ (D) := ER [|M (D, R)|] be the expected message length
for data D, and let the minimal expected message length be defined as
T[D : w] := min ED [TΠ(D)],	(5)
where Π ranges over all protocols such that D, w and D, Π(D) have the same distribution. Then
I[D : w] ≤ T[D : w] ≤ I[D : w] + 2 log(I[D : w] + 1) + O(1).	(6)
The results of Harsha et al. (2010) establish a characterization of the mutual information in terms of
minimal coding a conditional sample. For our purposes, Theorem 3.1 guarantees that in principle
4
Published as a conference paper at ICLR 2019
Algorithm 1 Minimal Random Coding
1:
2:
3:
4:
5:
6:
7:
8:
procedure ENCODE(qφ, p)
K — exp(KL(qφl∣p))
draw K samples {wk}K=-o1, Wk 〜P
qφ q_ qe(Wk)
ak — P(Wk)
q(wk)：= PKak— for k ∈{0 ... K - 1}
k0=0 ak0
draw a sample Wk* ~ q
return Wk*, k*
using shared random generator
end procedure
there is an algorithm which realizes near bits-back efficiency. Furthermore, the theorem shows that
this is indeed a fundamental lower bound, i.e. that such an algorithm is optimal for the considered
setting. To this end, we need to refer to a “common ground”, i.e. a shared random source R, where
w.l.o.g. we can assume that this source is an infinite list of samples from our encoding distribution
p. In practice, this can be realized via a pseudo-random generator with a public seed.
3.1	The Basic Algorithm
While Harsha et al. (2010) provide a constructive proof using a variant of rejection sampling (see
Appendix A), this algorithm is in fact intractable, because it requires keeping track of the acceptance
probabilities over the whole sample domain. Therefore, we propose an alternative method to produce
an approximate sample from qφ, depicted in Algorithm 1. This algorithm takes as inputs the trained
variational distribution q@ and the encoding distributionp. We first draw K = exp(KL(qφ∣∣p)) Sam-
ples from p, using the shared random generator. Subsequently, we craft a discrete proxy distribution
q, which has support only on these K samples, and where the probability mass for each sample is
proportional to the importance weights ak = ^Wk,. Finally, we draw a sample from q and return
its index k* and the sample Wk* itself. Since any number 0 ≤ k* < K can be easily encoded with
KL(qφ ||p) nats, we achieve our aimed coding efficiency. Decoding the sample is easy: simply draw
the k*th sample Wk* from the shared random generator (e.g. by resetting the random seed).
While this algorithm is remarkably simple and easy to implement, there is of course the question of
whether it is a correct thing to do. Moreover, an immediate caveat is that the number K of required
samples grows exponentially in KL( qφ ||p), which is clearly infeasible for encoding a practical neural
network. The first point is addressed in the next section, while the latter is discussed in Section 3.3,
together with other practical considerations.
3.2	Theoretical Analysis
The proxy distribution q in Algorithm 1 is based on an importance sampling scheme, as its probabil-
ity masses are defined to be proportional to the usual importance weights ak = *(：：). Under mild
assumptions (qφ, P continuous; a+ < ∞) it is easy to verify that q converges to q@ in distribution
for K → ∞; thus in the limit, Algorithm 1 samples from the correct distribution. However, since
we collect only K = exp(KL(qφ∣∣p)) samples in order to achieve a short coding length, q will be
biased. Fortunately, it turns out that K is just in the right order for this bias to be small.
Theorem 3.2 (Low Bias of Proxy Distribution) Let qφ, P be distributions over W. Let t ≥ 0 and
q be a discrete distribution constructed by drawing K = exp(KL(qφ∣∣p) + t) samples {wk}K=01
from p and defining G(Wk) := P (W)/0)Wk7) ,). Furthermore, let f (W) be a measurable function
and ||f ∣∣qφ = /Eqφ [f2] be its 2-norm under q$. Then it holds that
where
P (∣Eg[f ] - Eqe [f ]∣ ≥ 2f⅛1) ≤ 2e
e = (e-t∕4 + 2 JP (log(qφ/P) > KL(qφ∣∣p) +
1/2
(7)
(8)
5
Published as a conference paper at ICLR 2019
Algorithm 2 Minimal Random Code Learning (MIRACLE)
1	: procedure LEARN(D, model with parameters W, C, Cloc, I0, I)
2	randomly split W into B = dCC-e blocks {w0,... wb-i}
3	:	O J {0, . . . , B 一 1}	. The blocks that have not yet been encoded
4	:	βb J Eβ0, for b ∈ {0, . . . , B 一 1}
5	:	VARIATIONAL UPDATES(I0)
6	while O = 0 do
7	:	draw random b from O
8	:	O J O \ {b}
9	Wbj, kb = ENCODE(qφ(wb), P(Wb))	. from Algorithm 1
10	Wb J Wb (fixing the value of Wb)
11	:	VARIATIONAL UPDATES(I)
12	:	end while
13	:	return [k0, . . . , kB-1]
14	: end procedure
15:	procedure VARIATIONAL UPDATES(I)
16:	LO := Eqφ({wb}b∈O)[logP(DIw)] - Pb∈O βbKL(qφ(Wb) 11P(Wb))
17:	for i ∈ [0, . . . , I - 1] do
18:	Perform stochastic gradient update of LO
19:	for b ∈ O do
20:	if KL(qφ(wb)∣∣p(wb)) > Cloc then
21:	βb J (1 + Ee) X βb
22:	else
23：	Bb J βb∕(I + 6β)
24:	end if
25:	end for
26:	end for
27:	end procedure
Theorem 3.2 is a corollary of Chatterjee & Diaconis (2018), Theorem 1.2, by noting that
E 于 [f]
1
P	qφ(wk0)
乙 k0 P(wk0)
f(Wk)
k
qφ(Wk)
P(Wk) ,
(9)
which is precisely the importance sampling estimator for unnormalized distributions (denoted as
Jn in (Chatterjee & Diaconis, 2018)), i.e. their Theorem 1.2 directly yields Theorem 3.2. Note
that the term e-t/4 decays quickly with t, and, since log qφ∕p is typically concentrated around its
expected value KL(q||P), the second term in (8) also quickly becomes negligible. Thus, roughly
speaking, Theorem 3.2 establishes that Eq6 [f] ≈ E于[f] with high probability, for any measurable
function f. This is in particular true for the function f (w) = logp(D∣w) 一 β log (Ip(W). Note that
the expectation of this function is just the variational objective (3) we optimized to yield qφ in the
first place. Thus, since E于[f] ≈ Eqφ[f] = L(φ), replacing q@ by q is well justified. Thereby, any
sample of q can trivially be encoded with KL(qφ∣∣p) nats, and decoded by simple reference to a
pseudo-random generator.
Note that according to Theorem 3.2 we should actually take a number of samples somewhat larger
than exp(KL(qφ∣∣p)) in order to make E sufficiently small. In particular, the results in (Chatterjee
& Diaconis, 2018) also imply that a too small number of samples will typically be quite off the
targeted expectation (for the worst-case f). However, although our choice of number of samples is
at a critical point, in our experiments this number of samples yielded very good results.
3.3 Practical Implementation
In this section, We describe the application of Algorithm 1 within a practical learning algorithm 一
Minimal Random Code Learning (MIRACLE) - depicted in Algorithm 2. For both q$ and P We used
Gaussians with diagonal covariance matrices. For qφ, all means and standard deviations constituted
6
Published as a conference paper at ICLR 2019
IO4
Pareto-frontier
IO0
3 2 1
Ooo
111
3zUo-SS9JduJ0
0.006	0.007	0.008	0.009	0.010
Error rate
■ Uncompressed model
▼ Deep Compression
▲ Weightless
♦ Bayesian Compression
-∙- MIRACLE (This work)
ιo5
4 3 2
Ooo
111
z-s Uo-SS9JduJ0
Pareto-frontier
■ Uncompressed model
♦ Bayesian Compression
-∙- MIRACLE (This work)
0.011
101 ------------1---------1----------1----------1----------
0.06	0.07	0.08	0.09	0.10	0.11
Error rate
(a) LeNet-5 on MNIST	(b) VGG-16 on CIFAR-10
Figure 1: The error rate and the compression size for various compression methods. Lower left is
better.
the variational parameters φ. The mean of p was fixed to zero, and the standard deviation was shared
within each layer of the encoded network. These shared parameters of p where learned jointly with
qφ, i.e. the encoding distribution was also adapted to the task. This choice of distributions allowed
Us to use the reparameterization trick for effective variational training and furthermore, KL(qφ∣∣p)
can be computed analytically.
Since generating K = exp(KL(qφ∣∣p)) samples is infeasible for any reasonable KL(qφ∣∣p), We di-
vided the overall problem into sub-problems. To this end, we set a global coding goal of C nats
and a local coding goal of Cloc nats. We randomly split the weight vector W into B = d CC-]
Cloc
equally sized blocks, and assigned each block an allowance of Cloc nats. For example, fixing Cloc
to 11.09 nats ≈ 16 bits, corresponds to K = 65536 samples which need to be drawn per block. We
imposed block-wise KL constraints using block-wise penalty factors βb, which were automatically
annealed via multiplication/division with (1 + β) during the variational updates (see Algorithm 2).
Note that the random splitting into B blocks can be efficiently coded via the shared random genera-
tor, and only the number B needs communicated.
Before encoding any weights, we made sure that variational learning had converged by training for
a large number of iterations I0 = 104. After that, we alternated between encoding single blocks and
updating the variational distribution not-yet coded weights, by spending I intermediate variational
iterations. To this end, we define a variational objective LO w.r.t. to blocks which have not been
coded yet, while weights of already encoded blocks were fixed to their encoded value. Intuitively,
this allows to compensate for poor choices in earlier encoded blocks, and was crucial for good
performance. Theoretically, this amounts to a rich auto-regressive variational family qφ, as the
blocks which remain to be updated are effectively conditioned on the weights which have already
been encoded. We also found that the hashing trick (Chen et al., 2015) further improves performance
(not depicted in Algorithm 2 for simplicity). The hashing trick randomly conditions weights to share
the same value. While Chen et al. (2015) apply it to reduce the entropy, in our case it helps to restrict
the optimization space and reduces the dimensionality of both p and qφ. We found that this typically
improves the compression rate by a factor of 〜1.5×.
4	Experimental Results
The experiments3 were conducted on two common benchmarks: LeNet-5 on MNIST and VGG-16
on CIFAR-10. As baselines we used three recent state-of-the-art methods, namely Deep Com-
pression (Han et al., 2016), Weightless encoding (Reagen et al., 2018) and Bayesian Compression
(Louizos et al., 2017). The performance of the baseline methods are quoted from their respective
source materials.
3 The code is publicly available at https://github.com/cambridge-mlg/miracle
7
Published as a conference paper at ICLR 2019
Table 1: Numerical performance of the compression algorithms.
Model	Compression	Size	Ratio	Test error
	Uncompressed model	1720 kB	1×	0.7 %
	Deep Compression	44 kB	39×	0.8 %
LeNet-5 on MNIST	Weightless 5	4.52 kB	382×	1.0%
	Bayesian Compression	2.3 kB	771×	1.0%
	MIRACLE (Lowest error)	3.03 kB	555×	0.69 %
	MIRACLE (Highest compression)	1.52 kB	1110×	0.96 %
	Uncompressed model	60 MB	1×	6.5 %
	Bayesian Compression	642 kB	95×	8.6 %
VGG-16 on CIFAR-10	Bayesian Compression	525 kB	116×	9.2 %
	MIRACLE (Lowest error)	417 kB	147×	6.57 %
	MIRACLE (Highest compression)	168 kB	365×	10.0 %
For training MIRACLE, we used Adam (Kingma & Ba, 2014) with the default learning rate (10-3)
and we set β0 = 10-8 and β = 5 × 10-5. For VGG, the means of the weights were initialized
using a pretrained model.4 5 We recommend applying the hashing trick mainly to reduce the size of
the largest layers. In particular, we applied the hashing trick was to layers 2 and 3 in LeNet-5 to
reduce their sizes by 2× and 64× respectively and to layers 10-16 in VGG to reduce their sizes 8×.
The local coding goal Cloc was fixed at 20 bits for LeNet-5 and it was varied between 15 and 5 bits
for VGG (B was kept constant). For the number of intermediate variational updates I, we used
I = 50 for LeNet-5 and I = 1 for VGG, in order to keep training time reasonable (≈ 1 day on a
single NVIDIA P100 for VGG).
The performance trade-offs (test error rate and compression size) of MIRACLE along with the base-
line methods and the uncompressed model are shown in Figure 1 and Table 1. For MIRACLE we
can easily construct the Pareto frontier, by starting with a large coding goal C (i.e. allowing a large
coding length) and successively reducing it. Constructing such a Pareto frontier for other methods
is delicate, as it requires re-tuning hyper-parameters which are often only indirectly related to the
compression size - for MIRACLE it is directly reflected via the KL-term. We see that MIRACLE is
Pareto-better than the competitors: for a given test error rate, we achieve better compression, while
for a given model size we achieve lower test error.
5	Conclusion
In this paper we followed through the philosophy of the bits-back argument for the goal of coding
model parameters. The basic insight here is that restricting to a single deterministic weight-set and
aiming to coding it in a classic Shannon-style is greedy and in fact sub-optimal. Neural networks
- and other deep learning models - are highly overparameterized, and consequently there are many
“good” parameterizations. Thus, rather than focusing on a single weight set, we showed that this
fact can be exploited for coding, by selecting a “cheap” weight set out of the set of “good” ones.
Our algorithm is backed by solid recent information-theoretic insights, yet it is simple to implement.
We demonstrated that the presented coding algorithm clearly outperforms previous state-of-the-art.
An important question remaining for future work is how efficient MIRACLE can be made in terms
of memory accesses and consequently for energy consumption and inference time. There lies clear
potential in this direction, as any single weight can be recovered by its block-index and relative
index within each block. By smartly keeping track of these addresses, and using pseudo-random
generators as algorithmic lookup-tables, we could design an inference machine which is able to
directly run our compressed models, which might lead to considerable savings in memory accesses.
4For preprocessing the data and pretraining, we followed an open source implementation that can be found
at https://github.com/chengyangfu/pytorch-vgg-cifar10
5Weighless encoding only reports the size of the two largest layers so we assumed that the size of the rest
of the network is negligible in this case.
8
Published as a conference paper at ICLR 2019
Acknowledgements
We want to thank Christian Steinruecken, Oliver Janzer, Kris Stensbo-Smidt and Siddharth SWarooP
for their helpful comments. This project has received funding from the European Union’s Hori-
Zon 2020 research and innovation programme under the Marie SklodoWska-Curie Grant Agreement
No. 797223 — HYBSPN. Furthermore, we acknowledge EPSRC and Intel for their suPPort.
References
S. Chatterjee and P. Diaconis. The sample size required in importance sampling. The Annals of
AppliedProbability, 28(2):1099-1135, 2018.
W. Chen, J. Wilson, S. Tyree, K. Weinberger, and Y. Chen. Compressing neural netWorks With the
hashing trick. In Proceedings of ICML, pp. 2285-2294, 2015.
B. J. Frey and G. E. Hinton. Efficient stochastic source coding and an application to a bayesian
network source model. The ComputerJournal, 40(2_and_3):157—165, 1997.
P. D. Grunwald. The minimum description length principle. MIT press, 2007.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in Neural Information Processing Systems (NIPS), pp.
1135-1143, 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. International Conference on Learning
Representations (ICLR), 2016.
P. Harsha, R. Jain, D. McAllester, and J. Radhakrishnan. The communication complexity of corre-
lation. IEEE Transactions on Information Theory, 1(56):438-449, 2010.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner.
Beta-VAE: Learning basic visual concepts with a constrained variational framework. In ICLR,
2017.
G. E. Hinton and D. Van Camp. Keeping the neural networks simple by minimizing the description
length of the weights. In Proceedings of the sixth annual conference on Computational learning
theory, pp. 5-13. ACM, 1993.
G. E. Hinton and R. S. Zemel. Autoencoders, minimum description length and helmholtz free
energy. In Proceedings of NIPS, pp. 3-10, 1994.
David A Huffman. A method for the construction of minimum-redundancy codes. Proceedings of
the IRE, 40(9):1098-1101, 1952.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Y. LeCun, J. S. Denker, and S. A. Solla. Optimal brain damage. In Proceedings of NIPS, pp.
598-605, 1990.
C. Louizos, K. Ullrich, and M. Welling. Bayesian compression for deep learning. In Proceedings of
NIPS, pp. 3288-3298, 2017.
B.	Reagen, U. Gupta, R. Adolf, M. M. Mitzenmacher, A. M. Rush, G.-Y. Wei, and D. Brooks.
Weightless: Lossy weight encoding for deep neural network compression. International Confer-
ence on Machine Learning, 2018.
C.	E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 27(3):
379-423, 1948.
P.	M. B. Vitanyi and M. Li. An introduction to Kolmogorov complexity and its applications, vol-
ume 34. Springer Heidelberg, 1997.
9
Published as a conference paper at ICLR 2019
Algorithm 3 Greedy Rejection Sampling
1	: procedure SAMPLE(q, P)
2	Po(w) J 0 for W ∈ W
3	pS J 0
4	:	for i J 0 to ∞ do
5	αi(W) J min{q(W) - Pi-i(W), (1 - P匚 Jp(w)}
6	:	Pi(W) JPi-1(W) + αi (W)
7	Pi J ∑w∈wPi(W)
8	draw sample Wi 〜P
9	αi &	ai(Wi) βi	(I-P 之 I)P(Wi)
10	draw E 〜U(0,1)
11	:	if ≤ βi then
12	:	return Wi, i
13	:	end if
14	:	end for
15	: end procedure
A Greedy Rejection Sampling by Harsha et al. (2010)
In order to prove the upper bound, to which Harsha et al. (2010) refer as the ‘one-shot reverse
Shannon theorem’, they exhibit a rejection sampling procedure. However, instead of using the
classical rejection with acceptance probabilities M where M = max P, they propose a greedier
version. The core idea is that every sample should be accepted with as high probability as possible
while keeping the overall acceptance probability of each element below the target distribution.
For this algorithm we assume discrete p and q over the set W and an infinite sequence of samples
{wi }i∞=1 from p.
Let αi (w) with i ∈ N and w ∈ W be the probability that the procedure outputs the ith sample with
wi = w. For the sampling method to be unbiased, we have to ensure that
∞
q(w) = X αi(w) .	(10)
i=0
Let pi (w) = Pij=0 αj (w) be the probability that the procedure halts within j ≤ i iteration and it
outputs Wj = w. Let p* = Pw∈w Pi(W) be the probability that procedure halts within i iterations.
Let
αi(w) = min{q(w) - pi-i(w), (1 - p--i)p(w)}
pi(W) = pi-1(W) + αi(W) .
(11)
Since P(Wi = W)= p(w), α(W) can be at most (1 - PDp(W) The proposed strategy is
greedy because it accepts the ith sample with as high probability as possible under the constraint
that Pi(W) ≤ q(W).
Under the proposed formula for αi(W), the acceptance probability for the ith sample Wi is
βi
αi(Wi)
(1 - Pi-i)P(W)
(12)
The pseudo code is shown in Algorithm 3. Note that the algorithm requires computing αi (W) for
the whole set W in every iteration which makes it intractable for large W .
A.1 Proof outline
For the details of the proof, please refer to the source material (Harsha et al., 2010).
To show that the procedure is unbiased, one has to prove that
q(W) = lim Pi(W) .	(13)
i→∞
10
Published as a conference paper at ICLR 2019
This is shown by proving that q(w) - pi(w) ≤ q(w)(1 - p(w))i for i ∈ N.
In order to bound the encoding length, one has to first show that if the accepted sample has index i*,
then
E[logi*] ≤ KL(q∣∣p) + O(1).
(14)
Following this, one can employ the prefix-free binary encoding of Vitanyi & Li (1997). Let l(n) be
the length of the encoding for n ∈ N using the encoding scheme proposed by Vitanyi & Li (1997).
Their method is proven to have |l(n)| = logn + 2 log log(n + 1) + O(1), from which the upper
bound follows:
TR[D : W] ≤ E∣l(i*)∣ ≤ KL(q∣∣p)+2log(KL(q∣∣p) + 1) + O(1).
(15)
11