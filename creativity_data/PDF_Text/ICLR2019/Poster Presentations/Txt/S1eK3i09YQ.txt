Published as a conference paper at ICLR 2019
Gradient Descent Provably Optimizes
Over-parameterized Neural Networks
Simon S. Du*
Machine Learning Department
Carnegie Mellon University
ssdu@cs.cmu.edu
Xiyu Zhai*
Department of EECS
Massachusetts Institute of Technology
xiyuzhai@mit.edu
Barnabas Poczos
Machine Learning Department
Carnegie Mellon University
bapozos@cs.cmu.edu
Aarti Singh
Machine Learning Department
Carnegie Mellon University
aartisingh@cmu.edu
Ab stract
One of the mysteries in the success of neural networks is randomly initialized first
order methods like gradient descent can achieve zero training loss even though
the objective function is non-convex and non-smooth. This paper demystifies this
surprising phenomenon for two-layer fully connected ReLU activated neural net-
works. For an m hidden node shallow neural network with ReLU activation and n
training data, we show as long as m is large enough and no two inputs are parallel,
randomly initialized gradient descent converges to a globally optimal solution at
a linear convergence rate for the quadratic loss function.
Our analysis relies on the following observation: over-parameterization and ran-
dom initialization jointly restrict every weight vector to be close to its initializa-
tion for all iterations, which allows us to exploit a strong convexity-like property
to show that gradient descent converges at a global linear rate to the global op-
timum. We believe these insights are also useful in analyzing deep models and
other first order methods.
1 Introduction
Neural networks trained by first order methods have achieved a remarkable impact on many ap-
plications, but their theoretical properties are still mysteries. One of the empirical observation is
even though the optimization objective function is non-convex and non-smooth, randomly initialized
first order methods like stochastic gradient descent can still find a global minimum. Surprisingly,
this property is not correlated with labels. In Zhang et al. (2016), authors replaced the true labels
with randomly generated labels, but still found randomly initialized first order methods can always
achieve zero training loss.
A widely believed explanation on why a neural network can fit all training labels is that the neural
network is over-parameterized. For example, Wide ResNet (Zagoruyko and Komodakis) uses 100x
parameters than the number of training data. Thus there must exist one such neural network of this
architecture that can fit all training data. However, the existence does not imply why the network
found by a randomly initialized first order method can fit all the data. The objective function is
neither smooth nor convex, which makes traditional analysis technique from convex optimization not
useful in this setting. To our knowledge, only the convergence to a stationary point is known (Davis
et al., 2018).
* Equal contribution.
1
Published as a conference paper at ICLR 2019
In this paper we demystify this surprising phenomenon on two-layer neural networks with rectified
linear unit (ReLU) activation. Formally, we consider a neural network of the following form.
m
f (W, a, X) = √m X arσ (w>x)
r=1
(1)
where x ∈ Rd is the input, wr ∈ Rd is the weight vector of the first layer, ar ∈ R is the output
weight and σ (∙) is the ReLU activation function: σ (Z) = Z if Z ≥ 0 and σ (z) = 0 if z < 0 .
We focus on the empirical risk minimization problem with a quadratic loss. Given a training data
set {(xi, yi)}in=1, we want to minimize
n1
L(W, a) = ∑ 2 (f (W，a，Xi)-m)2 ∙	⑵
i=1
Our main focus of this paper is to analyze the following procedure. We fix the second layer and
apply gradient descent (GD) to optimize the first layer1
Wk + 1)= w(k)-产 (Wk； a).	⑶
where η> 0 is the step size. Here the gradient formula for each weight vector is 2
dL∂W，a) = √m X(f(W, a, Xi) - yi)arXiI {w>Xi ≥ 0}.
r	i=1
(4)
Though this is only a shallow fully connected neural network, the objective function is still non-
smooth and non-convex due to the use of ReLU activation function. 3 Even for this simple function,
why randomly initialized first order method can achieve zero training error is not known. Many
previous works have tried to answer this question or similar ones. Attempts include landscape anal-
ysis (Soudry and Carmon, 2016), partial differential equations (Mei et al.), analysis of the dynamics
of the algorithm (Li and Yuan, 2017), optimal transport theory (Chizat and Bach, 2018), to name
a few. These results often make strong assumptions on the labels and input distributions or do not
imply why randomly initialized first order method can achieve zero training loss. See Section 2 for
detailed comparisons between our result and previous ones.
In this paper, we rigorously prove that as long as no two inputs are parallel and m is large enough,
with randomly initialized a and W(0), gradient descent achieves zero training loss at a linear con-
vergence rate, i.e., it finds a solution W(K) with L(W(K))≤ in K = O(log (1/)) iterations.4
Thus, our theoretical result not only shows the global convergence but also gives a quantitative
convergence rate in terms of the desired accuracy.
Analysis Technique Overview Our proof relies on the following insights. First we directly an-
alyze the dynamics of each individual prediction f(W, a, Xi) for i = 1, . . . , n. This is different
from many previous work (Du et al., 2017b; Li and Yuan, 2017) which tried to analyze the dynam-
ics of the parameter (W) we are optimizing. Note because the objective function is non-smooth
and non-convex, analysis of the parameter space dynamics is very difficult. In contrast, we find
the dynamics of prediction space is governed by the spectral property of a Gram matrix (which
can vary in each iteration, c.f. Equation (6)) and as long as this Gram matrix’s least eigenvalue is
lower bounded, gradient descent enjoys a linear rate. It is easy to show as long as no two inputs are
parallel, in the initialization phase, this Gram matrix has a lower bounded least eigenvalue. (c.f. The-
orem 3.1). Thus the problem reduces to showing the Gram matrix at later iterations is close to that in
1In Section 3.2, we also extend our technique to analyze the setting where we train both layers jointly.
2 Note ReLU is not continuously differentiable. One can view L(W as a convenient notation for the right
hand side of (4) and this is the update rule used in practice.
3We remark that if one fixes the first layer and only optimizes the output layer, then the problem becomes a
convex and smooth one. Ifm is large enough, one can show the global minimum has zero training loss (Nguyen
and Hein, 2018). Though for both cases (fixing the first layer and fixing the output layer), gradient descent
achieves zero training loss, the learned prediction functions are different.
4Here we omit the polynomial dependency on n and other data-dependent quantities.
2
Published as a conference paper at ICLR 2019
the initialization phase. Our second observation is this Gram matrix is only related to the activation
patterns (I wr>xi ≥ 0 ) and we can use matrix perturbation analysis to show if most of the patterns
do not change, then this Gram matrix is close to its initialization. Our third observation is we find
over-parameterization, random initialization, and the linear convergence jointly restrict every weight
vector wr to be close to its initialization. Then we can use this property to show most of the patterns
do not change. Combining these insights we prove the first global quantitative convergence result of
gradient descent on ReLU activated neural networks for the empirical risk minimization problem.
Notably, our proof only uses linear algebra and standard probability bounds so we believe it can be
easily generalized to analyze deep neural networks.
Notations We let [n] = {1, 2, . . . , n}. Given a set S, we use unif {S} to denote the uniform
distribution over S. Given an event E, we use I {A} to be the indicator on whether this event
happens. We use N(0, I) to denote the standard Gaussian distribution. For a matrix A, we use Aij
to denote its (i,j)-th entry. We use ∣∣∙k2 to denote the Euclidean norm of a vector, and use |卜|| f to
denote the Frobenius norm of a matrix. If a matrix A is positive semi-definite, we use λmin (A) to
denote its smallest eigenvalue. We useh∙,)to denote the standard Euclidean inner product between
two vectors.
2	Comparison with Previous Results
In this section, we survey an incomplete list of previous attempts in analyzing why first order meth-
ods can find a global minimum.
Landscape Analysis A popular way to analyze non-convex optimization problems is to identify
whether the optimization landscape has some good geometric properties. Recently, researchers
found if the objective function is smooth and satisfies (1) all local minima are global and (2) for
every saddle point, there exists a negative curvature, then the noise-injected (stochastic) gradient
descent (Jin et al., 2017; Ge et al., 2015; Du et al., 2017a) can find a global minimum in polynomial
time. This algorithmic finding encouraged researchers to study whether the deep neural networks
also admit these properties.
For the objective function defined in Equation (2), some partial results were obtained. Soudry and
Carmon (2016) showed if md ≥ n, then at every differentiable local minimum, the training error is
zero. However, since the objective is non-smooth, it is hard to show gradient descent convergences to
a differentiable local minimum. Xie et al. (2017) studied the same problem and related the loss to the
gradient norm through the least singular value of the “extended feature matrix” D at the stationary
points. However, they did not prove the convergence rate of the gradient norm. Interestingly, our
analysis relies on the Gram matrix which is DD> .
Landscape analyses of ReLU activated neural networks for other settings have also been studied in
many previous works (Ge et al., 2017; Safran and Shamir, 2016; Zhou and Liang, 2017; Freeman
and Bruna, 2016; Hardt and Ma, 2016; Nguyen and Hein, 2018). These works establish favorable
landscape properties but none of them implies that gradient descent converges to a global minimizer
of the empirical risk. More recently, some negative results have also been discovered (Safran and
Shamir, 2018; Yun et al., 2018a) and new procedures have been proposed to test local optimality
and escape strict saddle points at non-differentiable points (Yun et al., 2018b). However, the new
procedures cannot find global minima as well. For other activation functions, some previous works
showed the landscape does have the desired geometric properties (Du and Lee, 2018; Soltanolkotabi
et al., 2018; Nguyen and Hein, 2017; Kawaguchi, 2016; Haeffele and Vidal, 2015; Andoni et al.,
2014; Venturi et al., 2018; Yun et al., 2018a). However, it is unclear how to extend their analyses to
our setting.
Analysis of Algorithm Dynamics Another way to prove convergence result is to analyze the
dynamics of first order methods directly. Our paper also belongs to this category. Many previous
works assumed (1) the input distribution is Gaussian and (2) the label is generated according to a
planted neural network. Based on these two (unrealistic) conditions, it can be shown that randomly
initialized (stochastic) gradient descent can learn a ReLU (Tian, 2017; Soltanolkotabi, 2017), a
single convolutional filter (Brutzkus and Globerson, 2017), a convolutional neural network with one
filter and one output layer (Du et al., 2018b) and residual network with small spectral norm weight
3
Published as a conference paper at ICLR 2019
matrix (Li and Yuan, 2017).5 Beyond Gaussian input distribution, Du et al. (2017b) showed for
learning a convolutional filter, the Gaussian input distribution assumption can be relaxed but they
still required the label is generated from an underlying true filter. Comparing with these work, our
paper does not try to recover the underlying true neural network. Instead, we focus on providing
theoretical justification on why randomly initialized gradient descent can achieve zero training loss,
which is what we can observe and verify in practice.
Jacot et al. (2018) established an asymptotic result showing for the multilayer fully-connected neural
network with a smooth activation function, if every layer’s weight matrix is infinitely wide, then for
finite training time, the convergence of gradient descent can be characterized by a kernel. Our proof
technique relies on a Gram matrix which is the kernel matrix in their paper. Our paper focuses on
the two-layer neural network with ReLU activation function (non-smooth) and we are able to prove
the Gram matrix is stable for infinite training time.
The most related paper is by Li and Liang (2018) who observed that when training a two-layer
full connected neural network, most of the patterns (I wr>xi ≥ 0 ) do not change over iterations,
which we also use to show the stability of the Gram matrix. They used this observation to obtain the
convergence rate ofGD on a two-layer over-parameterized neural network for the cross-entropy loss.
They need the number of hidden nodes m scales with poly(1/) where is the desired accuracy.
Thus unless the number of hidden nodes m → ∞, their result does not imply GD can achieve zero
training loss. We improve by allowing the amount of over-parameterization to be independent of
the desired accuracy and show GD can achieve zero training loss. Furthermore, our proof is much
simpler and more transparent so we believe it can be easily generalized to analyze other neural
network architectures.
Other Analysis Approaches Chizat and Bach (2018) used optimal transport theory to analyze
continuous time gradient descent on over-parameterized models. They required the second layer to
be infinitely wide and their results on ReLU activated neural network is only at the formal level. Mei
et al. analyzed SGD for optimizing the population loss and showed the dynamics can be captured
by a partial differential equation in the suitable scaling limit. They listed some specific examples on
input distributions including mixture of Gaussians. However, it is still unclear whether this frame-
work can explain why first order methods can minimize the empirical risk. Daniely (2017) built
connection between neural networks with kernel methods and showed stochastic gradient descent
can learn a function that is competitive with the best function in the conjugate kernel space of the
network. Again this work does not imply why first order methods can achieve zero training loss.
3	Continuous Time Analysis
In this section, we present our result for gradient flow, i.e., gradient descent with infinitesimal step
size. The analysis of gradient flow is a stepping stone towards understanding discrete algorithms
and this is the main topic of recent work (Arora et al., 2018; Du et al., 2018a). In the next section,
we will modify the proof and give a quantitative bound for gradient descent with positive step size.
Formally, we consider the ordinary differential equation6 defined by:
dwr (t)	∂L(W(t), a)
dt	∂Wr (t)
for r ∈ [m]. We denote ui (t) = f (W(t), a, xi) the prediction on input xi at time t and we let
u(t) = (u1(t), . . . , un(t)) ∈ Rn be the prediction vector at time t. We state our main assumption.
Assumption 3.1. Define matrix H∞	∈ Rn×n with Hi∞j =
EW〜N©I) [x>xjI {wτXi ≥ 0, w>Xj ≥ 0}]. We assume λ°，λma (H∞) > 0.
H∞ is the Gram matrix induced by the ReLU activation function and the random initialization.
Later we will show that during the training, though the Gram matrix may change (c.f. Equation (6)),
it is still close to H∞. Furthermore, as will be apparent in the proof (c.f. Equation (7)), H∞ is
5Since these work assume the label is realizable, converging to global minimum is equivalent to recovering
the underlying model.
6Strictly speaking, this should be differential inclusion (Davis et al., 2018)
4
Published as a conference paper at ICLR 2019
the fundamental quantity that determines the convergence rate. Interestingly, various properties of
this H∞ matrix has been studied in previous works (Xie et al., 2017; Tsuchida et al., 2017). Now to
justify this assumption, the following theorem shows if no two inputs are parallel the least eigenvalue
is strictly positive.
Theorem 3.1. If for any i 6= j, xi 6k xj, then λ0 > 0.
Note for most real world datasets, no two inputs are parallel, so our assumption holds in general.
Now we are ready to state our main theorem in this section.
Theorem 3.2 (Convergence Rate of Gradient Flow). Suppose Assumption 3.1 holds and for all
i ∈ [n], kxik2 = 1 and |yi| ≤ C for some constant C. Then if we set the number of hidden nodes
m = Ω ^λnδ3) and we i1"initialize Wr 〜N(0,I), a『〜unif[{-1,1}] for r ∈ [m], then with
probability at least 1 - δ over the initialization, we have
ku(t) - yk22 ≤ exp(-λ0t) ku(0) - yk22 .
This theorem establishes that if m is large enough, the training error converges to 0 at a linear rate.
Here we assume kxi k2 = 1 only for simplicity and it is not hard to relax this condition.7 The
bounded label condition also holds for most real world data set. The number of hidden nodes m
required is Ω ^λnδ3)，Which depends on the number of samples n, λo, and the failure probability δ.
Over-parameterization, i.e., the fact m = poly(n, 1∕λ0, 1∕δ), plays a crucial role in guaranteeing
gradient descent to find the global minimum. In this paper, We only use the simplest concentration
inequalities (Hoeffding’s and Markov’s) in order to have the cleanest proof. We believe using a
more advanced concentration analysis We can further improve the dependency. Lastly, We note the
specific convergence rate depends on λ0 but independent of the number of hidden nodes m.
3.1	Proof of Theorem 3.2
Our first step is to calculate the dynamics of each prediction.
m
dtui ⑴=Xh
r=1
n
∂f(W(t), a, xi) dWr (t)
∂Wr (t)
m
dt
j=1
(yj -uj)	h
r=1
∂f(W(t), a, xi) ∂f(W(t),a,xj)
∂Wr (t)
∂Wr (t)
n
i , X(yj 一 uj)Hij (t)
(5)
i
Where H(t) is an n × n matrix With (i, j)-th entry
1m
Hj(t) = mx>Xj XI {x>Wr (t) ≥ O, x>Wr (t) ≥ 0}.
With this H(t) matrix, We can Write the dynamics of predictions in a compact Way:
duu(t) = H(t)(y - u(t)).
dt
(6)
(7)
Remark 3.1. Note Equation (7) completely describes the dynamics ofthe predictions. In the rest of
this section, we will show (1) at initialization ∣∣H(0) 一 H∞∣b is O(,1∕m) and (2) for all t > 0,
∣∣H(t) — H(0)∣2 is O(，1∕m). Therefore, according to Equation ⑺,as m → ∞, the dynamics of
the predictions are characterized by H∞. This is the main reason we believe H∞ is the fundamental
quantity that describes this optimization process.
H(t) is a time-dependent symmetric matrix. We first analyze its property When t = 0. The folloWing
lemma shoWs if m is large then H(0) has a loWer bounded least eigenvalue With high probability.
The proof is by the standard concentration bound so We defer it to the appendix.
7 More precisely, if 0 < clow ≤ kxi k2 ≤ chigh for all i ∈ [n], We only need to change Lemma 3.1-3.3 to
make them depend on Clow and Chigh and the amount of over-parameterization m will depend on Chigh . We
clow
assume kxi k2 = 1 so we can present the cleanest proof and focus on our main analysis technique.
5
Published as a conference paper at ICLR 2019
I
Lemma 3.1. If m
we have with probability at least 1 - δ, ∣H(0) — H∞∣b ≤ λ0
and -min(H(0)) ≥ ∣-o.
Our second step is to show H(t) is stable in terms of W(t). Formally, the following lemma shows
for any W close to W(O), the induced Gram matrix H is close to H(O) and has a lower bounded
least eigenvalue.
Lemma 3.2. If w1 , . . . , wm are i.i.d. generated from N(0, I), then with probability at least 1 - δ,
the following holds. For any set of weight vectors w1 , . . . , wm ∈ Rd that satisfy for any r ∈ [m],
∣∣Wr (0) 一 Wr k2 ≤ c^，R for some small positive constant c, thenthe matrix H ∈ Rn×n defined
by
1m
Hij = — x>Xj XI {w>Xi ≥ 0, w>Xj ≥ 0}
m r=1
satisfies ∣∣H — H(0)∣2 < λ0 and -m® (H) > λ20.
This lemma plays a crucial role in our analysis so we give the proof below.
Proof of Lemma 3.2 We define the event
Air = {∃w : ∣w - wr(0)∣ ≤ R, I {Xi>wr(0) ≥ 0} 6= I {Xi>w ≥ 0}} .
Note this event happens if and only if IWr(0)>xj < R. Recall Wr(0)〜N(0,I). By anti-
concentration inequality of Gaussian, we have P(Air) = PZ〜N(o,i)(IzI < R) ≤ √R. Therefore,
for any set of weight vectors w1, . . . , wm that satisfy the assumption in the lemma, we can bound
the entry-wise deviation on their induced matrix H: for any (i, j) ∈ [n] × [n]
E[IHij(0)-HijI]
1
m
=E
m
X>Xj X (I {Wr(0)>Xi ≥ 0, Wr(0)>Xj ≥ 0} - I {w>Xi ≥ O, W>Xj ≥ 0})
r=1
1 m	4R
≤ 一 EE [I {Air ∪ Ajr }] ≤ √=
m	2π
r=1
where the expectation is taken over the random initialization of w1 (0), . . . , wm(0). Summing over
(i, j),we have E [P，；；；=. 1)IHij- Hij (O) |] ≤ 4√2∏R. Thus by Markov,s inequality, with proba-
bility 1 - δ,we have P(；jn=(i 1)IHij- Hij (0) I ≤ √n2∏R. Next, We use matrix perturbation theory
to bound the deviation from the initialization
kH-H(0)k2≤kH-H(0)kF≤
(n,n)	4n2R
X IHij- Hij(O)I ≤ √=δ.
(i,j)=(1,1)	2πδ
Lastly, we lower bound the smallest eigenvalue by plugging in R
4n2 R	λ
λmin(H) ≥ λmin(H(0))-√∏δ ≥ -0. 口
The next lemma shows two facts if the least eigenvalue of H(t) is lower bounded. First, the loss
converges to 0 at a linear convergence rate. Second, wr (t) is close to the initialization for every
r ∈ [m]. This lemma clearly demonstrates the power of over-parameterization.
Lemma 3.3. Suppose for 0 ≤ S ≤ t, -min (H(s)) ≥ λ20. Then we have ∣∣y - u(t)∣2 ≤
exp(--ot) IIy - u(0)k2 andfor any r ∈ [m], ∣∣Wr (t) - Wr (0)∣2 ≤ √nk√mλ(0)k2，R.
ProofofLemma 3.3 Recall we can write the dynamics of predictions as + u(t) = H(y - u(t)). We
can calculate the loss function dynamics
d ky - u(t)k2 = - 2(y - u(t))> H(t)(y - u(t))
≤ - -o ∣y - u(t)∣22 .
6
Published as a conference paper at ICLR 2019
Thus We have 第 卜xp(λ0t) ∣∣y - u(t)∣∣2) ≤ 0 and exp(λ0t) ∣∣y - u(t)k2 is a decreasing function
with respect to t. Using this fact we can bound the loss
∣y - u(t)∣22 ≤ exp(-λ0t) ∣y - u(0)∣22 .
Therefore, u(t) → y exponentially fast. NoW We bound the gradient norm. Recall for 0 ≤ s ≤ t,
d
dS Wr(S)
2
£(yi - Ui)√= arXiI {wr (s)>Xi ≥ 0}
i=1	m
2
≤
√m X |yi-ui(S)I ≤√√m ky -U(S)k2 ≤ √n eχp(-λ0S) ky-u(O)k2.
i
Integrating the gradient, We can bound the distance from the initialization
∣wr(t) - wr(0)∣2 ≤
d V √nky - u(0)k2
2	≤ - √mλ0-
□
The next lemma shoWs if R0 < R, the conditions in Lemma 3.2 and 3.3 hold for all t ≥ 0. The
proof is by contradiction and We defer it to appendix.
Lemma 3.4. If R < R, we have for all t ≥ 0, λmin(H(t)) ≥ 2λo, for all r ∈ [m],
∣wr(t) - wr(0)∣2 ≤ R0 and ∣y - U(t)∣22 ≤ eχp(-λ0t) ∣y - U(0)∣22.
Thus it is sufficient to show R < R which is equivalent to m = Ω (n 昨；，*2) ∙ We bound
nn
E ∣y - U(0)∣22	= X(yi2	+ yiE	[f (W(0), a, Xi)]	+E	f(W(0), a, Xi)2) = X(yi2	+1) = O(n).
i=1	i=1
Thus by Markov's inequality, we have with probability at least 1 - δ, ky - u(0)k2 = O(δ). Plug-
ging in this bound we prove the theorem.	□
3.2 Jointly Training Both Layers
In this subsection, we showcase our proof technique can be applied to analyze the convergence
of gradient flow for jointly training both layers. Formally, we consider the ordinary differential
equation defined by:
dwr(t)	∂L(W(t),a(t))	d dwr(t)	∂L(W(t),a(t))
dt	∂Wr (t)	an dt	∂ar (t)
for r = 1, . . . , m. The following theorem shows using gradient flow to jointly train both layers, we
can still enjoy linear convergence rate towards zero loss.
Theorem 3.3 (Convergence Rate of Gradient Flow for Training Both Layers). Under the same
assumptions as in Theorem 3.2, if we set the number of hidden nodes m = Ω (n lλgδδm /δ)) and we
i.i.d. initialize Wr 〜N(0,I), a『〜unif[{-1,1}] for r ∈ [m], with probability at least 1 - δ over
the initialization we have
kU(t) - yk22 ≤ eχp(-λ0t) kU(0) - yk22 .
Theorem 3.3 shows under the same assumptions as in Theorem 3.2, we can achieve the same con-
vergence rate as that of only training the first layer. The proof of Theorem 3.3 relies on the same
arguments as the proof of Theorem 3.2. Again we consider the dynamics of the predictions and this
dynamics is characterized by a Gram matrix. We can show for all t > 0, this Gram matrix is close
to the Gram matrix at the initialization phase. We refer readers to appendix for the full proof.
7
Published as a conference paper at ICLR 2019
4 Discrete Time Analysis
In this section, we show randomly initialized gradient descent with a constant positive step size
converges to the global minimum at a linear rate. We first present our main theorem.
Theorem 4.1 (Convergence Rate of Gradient Descent). Under the same assumptions as in Theo-
rem 3.2, if we set the number of hidden nodes m = Ω ^λnδ3)，we ‘工d initialize Wr 〜N(0, I),
ar 〜unif [{-1,1}] for r ∈ [m], and we set the step size η = O (n2)then with probability at least
1 - δ over the random initialization we have for k = 0, 1, 2, . . .
ku(k) - yk2 ≤ (1-	) ku(O) - yk2.
Theorem 4.1 shows even though the objective function is non-smooth and non-convex, gradient
descent with a constant step size still enjoys a linear convergence rate. Our assumptions on the least
eigenvalue and the number of hidden nodes are exactly the same as the theorem for gradient flow.
4.1 Proof of Theorem 4.1
We prove Theorem 4.1 by induction. Our induction hypothesis is just the following convergence
rate of the empirical loss.
Condition 4.1. At the k-th iteration, we have ∣∣y 一 u(k)k2 ≤ (1 一 争)k ∣∣y 一 u(0)k2.
A directly corollary of this condition is the following bound of deviation from the initialization. The
proof is similar to that of Lemma 3.3 so we defer it to appendix.
Corollary 4.1. If Condition 4.1 holds for k0 = 0, . . . , k, then we have for every r ∈ [m]
∣Wr (k + 1) — Wr(0)∣2 ≤	ky-U(0)k2，R.	(8)
Now we show Condition 4.1 holds for every k = 0, 1, ....... For	the base case k = 0, by defini-
tion Condition 4.1 holds. Suppose for k0 = 0, . . . , k, Condition 4.1 holds and we want to show
Condition 4.1 holds for k0 = k + 1.
Our strategy is similar to the proof of Theorem 3.2. We define the event
Air = {∃W ： ∣∣W — Wr (O) k ≤ R, I {x>Wr (O) ≥。} = I {x>W ≥。}}.
where R = cn0 for some small positive constant c. Different from gradient flow, for gradient
descent we need a more refined analysis. We let Si = {r ∈ [m] ： I{Air } = O} and Si⊥ = [m] \ Si.
The following lemma bounds the sum of sizes of Si⊥. The proof is similar to the analysis used in
Lemma 3.2. See Section A for the whole proof.
Lemma 4.1. With probability at least 1 — δ over the initialization, we have Pn=1 ∣S⊥ ∣ ≤ CmnR
for some positive constant C > O.
Next, we calculate the difference of predictions between two consecutive iterations, analogue to
dud(t term in Section 3.
1m
ui(k + 1) — ui(k) = √m £a『(σ (Wr(k + 1)>Xi) — σ (Wr(k)>Xi))
r=1
=√m X ar (σ ((wr(k) 一 η⅛WWF) x) 一 σ (Wr(k)>
8
Published as a conference paper at ICLR 2019
Here we divide the right hand side into two parts. I1i accounts for terms that the pattern does not
change and I2i accounts for terms that pattern may change.
Ii ,√1m Xar (σ ((Wr ⑻-n^LWkL )	x) - σ (Wr ㈤>xi)
I2, √1m X ar (σ ((wr (k)- ηddLW R)
Xi	— σ (Wr (k)>Xi)
We view I2i as a perturbation and bound its magnitude. Because ReLU is a 1-Lipschitz function and
|ar | = 1, we have
IIi I ≤ ɪ X 1(dL(w(k)))> x.
Il √m ɪjja k
≤ η-∣S⊥∣ max
m r∈[m]
dL(w(k))	≤ η |SiL √n ku(k) - y∣b
∂Wr(kL	2	m
To analyze I1i, by Corollary 4.1, we know kWr(kL - Wr(0Lk ≤ R0 and kWr(kL - Wr(0Lk ≤ R0 for
all r ∈ [m]. Furthermore, because R0 < R, we know I Wr(k + 1L>xi ≥ 0 = I Wr(kL>xi ≥ 0
for r ∈ Si . Thus we can find a more convenient expression of I1i for analysis
Ii = - ɪ X x> Xj (Uj- yj) X I {wr (k)>xi ≥ 0, Wr (k)>Xj ≥ 0}
m j =1	r∈Si
n
= - η X(uj - yj)(Hij (k) - Hi⊥j(k))
where Hij(k)	=	m1 Pm=IX>xj∙I{wr(k)>x∙i	≥ 0, WrlkFXj ≥ 0}	is	just	the (i,j)-th
entry of a discrete	version	of Gram matrix	defined in Section	3	and	Hi⊥j (k)	=
ml Pr∈s⊥ x>XjI {Wr(k)>Xi ≥ 0, Wr(k)>Xj ≥ 0} is a perturbation matrix. Let H⊥(k) be the
n × n matrix with (i, j)-th entry being Hi⊥j (k). Using Lemma 4.1, we obtain an upper bound of the
operator norm
∣w(k儿 ≤	X) %(k)∣≤nPnmIS⊥∣ ≤ C⅛R ≤CnR
(i,j)=(1,1)
Similar to the classical analysis of gradient descent, we also need bound the quadratic term.
wk+1)-u(k)k2 ≤ η2X m (X I ^LWkI D ≤ η2n2 ky-mk)k2.
With these estimates at hand, we are ready to prove the induction hypothesis.
ky-u(k+1)k22=ky-u(k)-(u(k+1)-u(k))k22
= ky - u(k)k22 -2(y-u(k))>(u(k+1) - u(k)) + ku(k + 1) - u(k)k22
= ky - u(k)k22 - 2η(y - u(k))> H(k) (y - u(k))
+ 2η(y - u(k))> H(k)⊥ (y - u(k)) - 2 (y - u(k))> I2
+ ku(k + 1) - u(k)k22
≤(1 - ηλo + 2Cf2R + 2Cηn3/2R + η2n2) ky - u(k)k2
δδ
≤(1 - ηλ0) ky - u(k)k2 .
The third equality we used the decomposition of u(k + 1) - u(k). The first inequality we used the
Lemma 3.2, the bound on the step size, the bound on I2, the bound on IH(k)⊥I2 and the bound
on ku(k + 1) - u(k)k22. The last inequality we used the bound of the step size and the bound of R.
Therefore Condition 4.1 holds for k0 = k + 1. Now by induction, We prove Theorem 4.1.	□
9
Published as a conference paper at ICLR 2019
80
2.5 2 1.5 1 0.5
secnatsiD mumixaM
(a) Convergence rates. (b) Percentiles of pattern (c) Maximum distances from ini-
changes.	tialization.
Figure 1: Results on synthetic data.
5	Experiments
In this section, we use synthetic data to corroborate our theoretical findings. We use the initialization
and training procedure described in Section 1. For all experiments, we run 100 epochs of gradient
descent and use a fixed step size. We uniformly generate n = 1000 data points from a d = 1000
dimensional unit sphere and generate labels from a one-dimensional standard Gaussian distribution.
We test three metrics with different widths (m). First, we test how the amount of over-
parameterization affects the convergence rates. Second, we test the relation between the amount of
over-parameterization and the number of pattern changes. Formally, at a given iteration k, we check
Pim=1 Prm=1 I{sign(wr(0)>xi)6=sign(wr(k)>xi)}
—i=1- I——-——mn  ------------------------- (there are mn patterns). This aims to verify Lemma 3.2.
Last, we test the relation between the amount of over-parameterization and the maximum of the dis-
tances between weight vectors and their initializations. Formally, at a given iteration k, we check
maxr∈[m] kwr(k) - wr(0)k2. This aims to verify Lemma 3.3 and Corollary 4.1.
Figure 1a shows as m becomes larger, we have better convergence rate. We believe the reason
is as m becomes larger, H(t) matrix becomes more stable, and thus has larger least eigenvalue.
Figure 1b and Figure 1c show as m becomes larger, the percentiles of pattern changes and the
maximum distance from the initialization become smaller. These empirical findings are consistent
with our theoretical results.
6	Conclusion and Discussion
In this paper we show with over-parameterization, gradient descent provable converges to the global
minimum of the empirical loss at a linear convergence rate. The key proof idea is to show the
over-parameterization makes Gram matrix remain positive definite for all iterations, which in turn
guarantees the linear convergence. Here we list some future directions.
First, we believe our approach can be generalized to deep neural networks. We elaborate the main
idea here for gradient flow. Consider a deep neural network of the form
f (x, W, a) = a>σ (W(H)…σ (W(I)X))
where x ∈ Rd is the input, W(1) ∈ Rm×d is the first layer, W(h) ∈ Rm×m for h = 2, . . . , H are
the middle layers and a ∈ Rm is the output layer. Recall ui is the i-th prediction. If we use the
quadratic loss, we can compute
dW(h)(t) _ ∂L(W(t)) _— ∂u ∂ui(t)
—dt - = - ∂W(h)(t) =Vyi - ui(t)) ∂W(h)(t).
Similar to Equation (5), we can calculate
号=XX h ∂∂W⅛, H i=XX i(t)) XX Gg)
h=1	j=1	h=1
10
Published as a conference paper at ICLR 2019
where G(h)(t) ∈ Rn×n With G(，(t) = h Wuh)(t), Wj((t)). Therefore, similar to Equation (7), We
can write
竽=X G㈤(t)(y-u(t)).
h=1
Note for every h ∈ [H], G(h) is a Gram matrix and thus it is positive semidefinite. If PhH=1 G(h)(t)
has a loWer bounded least eigenvalue for all t, then similar to Section 3, gradient floW converges
to zero training loss at a linear convergence rate. Based on our observations in Remark 3.1, We
conjecture that if m is large enough, PhH=1 G(h) (0) is close to a fixed matrix PhH=1 G(∞h) and
PhH=1 G(h) (t) is close its initialization PhH=1 G(h) (0) for all t > 0. Therefore, using the same
arguments as We used in Section 3, as long as PhH=1 G(∞h) has a loWer bounded least eigenvalue,
gradient floW converges to zero training loss at a linear convergence rate.
Second, We believe the number of hidden nodes m required can be reduced. For example, previous
work (SoUdry and Carmon, 2016) showed m ≥ d is enough to make all differentiable local minima
global. In our setting, using advanced tools from probability and matrix perturbation theory to
analyze H(t), we may be able to tighten the bound.
Lastly, in our paper, we used the empirical loss as a potential function to measure the progress. Ifwe
use another potential function, we may be able to prove the convergence rates of accelerated meth-
ods. This technique has been exploited in Wilson et al. (2016) for analyzing convex optimization.
It would be interesting to bring their idea to analyze other first order methods for optimizing neural
networks.
Acknowledgments
This research was partly funded by AFRL grant FA8750-17-2-0212 and DARPA D17AP00001. We
thank Wei Hu, Jason D. Lee and Ruosong Wang for useful discussions.
References
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with
neural networks. In International Conference on Machine Learning, pages 1908-1916, 2014.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with gaussian
inputs. In International Conference on Machine Learning, pages 605-614, 2017.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pages 2422-2430, 2017.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient
method converges on tame functions. arXiv preprint arXiv:1804.07795, 2018.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with
quadratic activation. Proceedings of the 35th International Conference on Machine Learning,
pages 1329-1338, 2018.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient
descent can take exponential time to escape saddle points. In Advances in Neural Information
Processing Systems, pages 1067-1077, 2017a.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv
preprint arXiv:1709.06129, 2017b.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018a.
11
Published as a conference paper at ICLR 2019
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent
learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. Proceedings of the 35th
International Conference on Machine Learning, pages 1339-1348, 2018b.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
arXiv preprint arXiv:1611.01540, 2016.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pages 797-842, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. arXiv preprint arXiv:1506.07540, 2015.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape sad-
dle points efficiently. In Proceedings of the 34th International Conference on Machine Learning,
pages 1724-1732, 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information
Processing Systems, pages 586-594, 2016.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pages 597-607, 2017.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layers neural networks. Proceedings of the National Academy of Sciences.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Interna-
tional Conference on Machine Learning, pages 2603-2612, 2017.
Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep cnns. In
International Conference on Machine Learning, pages 3727-3736, 2018.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks.
In International Conference on Machine Learning, pages 774-782, 2016.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural net-
works. In International Conference on Machine Learning, pages 4433-4441, 2018.
Mahdi Soltanolkotabi. Learning ReLus via gradient descent. In Advances in Neural Information
Processing Systems, pages 2007-2017, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 2018.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. In International Conference on Machine
Learning, pages 3404-3413, 2017.
12
Published as a conference paper at ICLR 2019
Russell Tsuchida, Farbod Roosta-Khorasani, and Marcus Gallagher. Invariance of weight distribu-
tions in rectified mlps. arXiv preprint arXiv:1711.09090, 2017.
Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension
have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.
Ashia C Wilson, Benjamin Recht, and Michael I Jordan. A Lyapunov analysis of momentum meth-
ods in optimization. arXiv preprint arXiv:1611.02635, 2016.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Artificial
Intelligence and Statistics, pages 1216-1224, 2017.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. A critical view of global optimality in deep learning.
arXiv preprint arXiv:1802.03487, 2018a.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Efficiently testing local optimality and escaping saddles
for relu networks. arXiv preprint arXiv:1809.10858, 2018b.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. NIN, 8:35-67.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Yi Zhou and Yingbin Liang. Critical points of neural networks: Analytical forms and landscape
properties. arXiv preprint arXiv:1710.11205, 2017.
13
Published as a conference paper at ICLR 2019
A Technical Proofs for Section 3
Proof of Theorem 3.1. The proof of this lemma just relies on standard real and functional anal-
ysis. Let H be the Hilbert space of integrable d-dimensional vector fields on Rd : f ∈
H if Ew〜N(0,i) [|f(w)l2] < ∞. The inner product of this space is then hf, giH =
Ew〜N(0,I) [f(w)>g(w)].
ReLU activation induces an infinite-dimensional feature map φ which is defined as for any x ∈ Rd,
(φ(x))(w) = xI w>x ≥ 0 where w can be viewed as the index. Now to prove H∞ is strictly
positive definite, it is equivalent to show φ(x1), . . . , φ(xn) ∈ H are linearly independent. Suppose
that there are αι,…,αn ∈ R such that
αιφ(xi) +------+ αnφ(xn) = 0 in H.
This means that
α1φ(x1)(w) +------+ αnφ(xn)(w) = 0 a.e.
Now we prove αi = 0 for all i.
We define Di = {w ∈ Rd : w>Xi = 0}. This is set of discontinuities of φ(xi). The following
lemma characterizes the basic property of these discontinuity sets.
Lemma A.1. If for any i 6= j, xi 6k xj, then for any i ∈ [m], Di 6⊂	Dj.
j6=i
Now for a fixed i ∈ [n], since Di 6⊂ S Dj , we can choose z ∈ Di \ S Dj . Note Dj , j 6= i are
j 6=i	j 6=i
closed sets. We can pick ro > 0 small enough such that B(z, r) ∩ Dj = 0, ∀j = i,r ≤ r0. Let
B(z, r) = Br+ t Br- where
Br+ = B(z, r) ∩ {w ∈ Rd : w>xi > 0}.
For j 6= i, φ(xj)(w) is continuous in a neighborhood of z, then for any > 0 there is a small
enough r > 0 such that
∀w ∈ B(z,r), ∣φ(xj)(w) - φ(xj)(z)l < e.
Let μ be the Lebesgue measure on Rd. We have
μ(B +) JB + φ(Xj )(W)dw — φ(Xj )(Z) ≤ μ(B +) JB + lφ(xj )(W) — φ(Xj )(Z)I dw < e
and similarly
μ(B-) JB _ φ(Xj )(w)dw — φ(Xj )(z) ≤ μ(B-) JB _ lφ(xj )(w) — φ(Xj )(z)| dw < e.
Thus, we have
lim zp +λ φ φ(xj)(w)dw = lim zp -ʌ φ φ(xj )(w)dw = φ(Xj )(z).
r→0+ μ(Br) Bb+	r→0+ μ(Br ) JBr
Therefore, as r → 0+, by continuity, we have
∀j = i,μ⅛) 1b+ °(Xj)(w)dw - μ⅛ 4-φ(Xj)(w)dw →0	⑼
Next recall that (φ(X))(w) = XI X>w > 0 , so for w ∈ Br+ and Xi, (φ(Xi))(w) = Xi. Then, we
have
pp
lim  L	φ(Xj)(w)dw = lim  L	XidW = Xi
r→0+ μ(B+) 7b+ φ( j八 )	r→0+ μ(B+) JB+ i	i
For w ∈ Br- and Xi, we know (φ(Xi))(w) = 0. Then we have
lim ——-—— φ φ(Xi)(w)dw = lim ——-—— / Odw = 0
r→0+ μ(B-) JB-	K)	r→0+ μ(B-) JBr
(10)
(11)
14
Published as a conference paper at ICLR 2019
Now recall i αiφ(xi) ≡ 0. Using Equation (9), (10) and (11), we have
0=r→m+ μB+ L+ X ɑ φ(xj)(w)dw - r→0+ μ⅛ L- X α φ(x )(w)dw
=X αj (r→+ μ(⅛) B+ φ(xj )(w)dw - r→+ μ⅛ B- φ(xj)(w)dw)
=	αj (δijxi)
j
= αixi
Since xi 6= 0, we must have αi = 0. We complete the proof.
□
ProofofLemma A.1. Let μ be the canonical Lebesgue measure on Di. Wehave Ej= μ(Di∩Dj)=
0 because Di ∩ Dj is a hyperplane in Di . Now we bound
μ(Di ∩ U Dj) ≤ X μ(Di ∩ Dj) = 0.
j 6=i	j6=i
This implies our desired result.	□
Proof of Lemma 3.1. For every fixed (i, j) pair, Hij(0) is an average of independent random vari-
ables. Therefore, by Hoeffding inequality, we have with probability 1 - δ0,
IHij(0)- H∞ι≤ 2pl√¾∕δO).
m
Setting δ0 = n2 δand applying union bound over (i, j ) pairs, we have for every (i, j ) pair with
probability at least 1 - δ
IHij(0)- H∞ι≤ '4p√n".
m
Thus we have
kH(0) - Η∞ k2 ≤ kH(0) - H∞kF ≤ X IHij (0) - H∞∣2 ≤ 16n2lmg(n∕δ).
i,j
Thus if m = Ω (n loλ(n∕6 ) We have the desired result.	□
Proof of Lemma 3.4. Suppose the conclusion does not hold at time t. If there exists r ∈ [m],
kwr(t) - wr(0)k ≥ R0 or ky - u(t)k22 > exp(-λ0t) ky - u(0)k22, then by Lemma 3.3 We knoW
there exists S ≤ t such that λmin(Η(s)) < 2λ0. By Lemma 3.2 we know there exists
t0 = inf t > 0 : max kwr (t) - wr (0)k22 ≥ R .
Thus at t0, there exists r ∈ [m], kwr(t0) - wr(0)k22 = R. Now by Lemma 3.2, we know H(t0) ≥
1 λo for t0 ≤ to. However, by Lemma 3.3, we know ∣∣Wr (to) - Wr (0)∣∣2 < R < R. Contradiction.
For the other case, at time t, λmin(H(t)) < 2λo we know there exists
to = inf t ≥ 0 : max ∣Wr (t) - Wr (0)∣22 ≥ R .
r∈[m]
The rest of the proof is the same as the previous case.	□
15
Published as a conference paper at ICLR 2019
A.1 Proof of Theorem 3.3
In this section we show using gradient flow to jointly train both the first layer and the output layer
we can still achieve 0 training loss. We follow the same approach we used in Section 3. Recall the
gradient for a.
∂L(w, a)_	1 X(f(	,	√σ (w>x,
= √m g(f(w, a, Xi)-yi[σ (Wm Xi)
We compute the dynamics of an individual prediction.
dui(t)	XX I ∂ui(t) ∂Wr (t) XX dui(t) da『(t)
dt	∂wr ∂Wr (t)， ∂t	+2^jdar(t) dt .
Recall we have found a convenient expression for the first term.
XX h ∂WS，胃 i=XX …(t)) Hij (t)
r=1	r	j=1
where
1m
Hij (t) = —X>Xj X ar (t)I {x>Wr (t) ≥ 0, X>Wr (t) ≥ θ}.
m	r=1
For the second term, it easy to derive
XX dui(t) dar (t) XX (	#
工 K ∙ F = IJyj-Uj(t)) Gij⑴
r=1	r=1
where
Gij(t) = 2σ(W>Xi) σ(W>Xj).
Therefore we have
坪= (H(t) + G(t))(y - u(t)).
(12)
(13)
(14)
First use the same concentration arguments as in Lemma 3.1, We can show λmin(H(0)) ≥ 3λ0
with 1 - δ probability over the initialization. In the following, our arguments will base on that
λmin(H(0)) ≥ 3λ0 .
The following lemma shows as long as H(t) has lower bounded least eigenvalue, gradient flow
enjoys a linear convergence rate. The proof is analogue to the first part of the proof of Lemma 3.3.
Lemma A.2. If for 0	≤ S ≤ t, λmin(H(s))	≥ λ0, we have ∣∣y 一 u(t)k2	≤
exp(-λ0t) ky -u(0)k22.
Proof of Lemma A.2. We can calculate the loss function dynamics
d ky - u(t)k2 = - 2(y - u(t))> (H(t) + G(t))(y - u(t))
≤-2(y-u(t))>(H(t))(y-u(t))
≤ - λ0 ∣y - u(t)∣22
where in the first inequality we use the fact that G(t) is Gram matrix thus it is positive.8 Thus
we have £ 卜xp(λ0t) ∣∣y 一 u(t)∣, ≤ 0 and exp(λ0t) ∣∣y 一 u(t)∣22 is a decreasing function with
respect to t. Using this fact we can bound the loss
∣y - u(t)∣22 ≤ exp(-λ0t) ∣y - u(0)∣22 .
□
8In the proof, we have not take the advantage of G(t) being a positive semidefinite matrix. Note if G(t) is
strictly positive definite, we can achieve faster convergence rate.
16
Published as a conference paper at ICLR 2019
We continue to follow the analysis in Section 3. For convenience, we define
R = √πλ0δ R =	R0 = 4√nky - u0k2 RO = 8√nIly - U(O)Il2 PlogCmnM)
W = 32n2 , a = 16n2, W =	再λ	, a =	再λ	.
The first lemma characterizes how the perturbation in a and W affect the Gram matrix.
Lemma A.3. With probability at least 1 - δ over initialization, if a set of weight vectors {wr }rm=1
and the output weight a satisfy for all r ∈ [m], Iwr - wr (0)I2 ≤ RW and |ar - ar (0)| ≤ Ra, then
the matrix H ∈ Rn×n defined by
1m
Hij = 一x> Xj E ar I {w> Xi ≥ 0, w> Xj ≥ 0}
m	r=1
satisfies ∣∣H 一 H(O)k2 ≤ λ0 and λmin (H) > λ20.
Proof of Lemma A.3. Define a surrogate Gram matrix,
m
HO= -X I {w>xi ≥ 0, w>xj ≥ 0}.
m
r=1
Using the same analysis for Lemma 3.3, we know ∣∣H0 一 H(O) k2 ≤ 4√n2Rw . Now we bound H-H0.
For fixed (i, j) ∈ [n] × [n], we have
1m
Hij- Hij = χ> χj m χ(ar - I)I {w> χi ≥ 0,wr>Xj ≥ O} ≤ max ar2 - 1 ≤ 2Ra .
m	r∈[m]
r=1
Therefore, we have ∣H - H0∣2 ≤ P(i,j)∈[n]×[n] Hij - H0ij ≤ 2n2Ra. Combing these two in-
equalities we have ∣∣H - H(0)∣b ≤ 4√2Rw + 2n2Ra ≤ λ0.	□
Lemma A.4. SuPPosefor 0 ≤ S ≤ t, λmin (H(s)) ≥ λ20 and |a『(s) — a『(0)| ≤ Ra. Then we have
∣wr (t) - wr(O)∣2 ≤ R0W.
Proof of Lemma A.4. We bound the gradient. Recall for O ≤ s ≤ t,
d
dt Wr (S)
2
n
= £(yi - Ui	ar (t)XiI {wr (t)>Xi ≥ O}
i=1
1n
≤ √m ɪ2 |yi - Ui(S)||ar (O) + RaI
2
i=1
≤√= ky - U(S) k2 ≤ √= exp(-λ0s/2) Ily - U(O)Il2 .
mm
Integrating the gradient and using Lemma A.2, we can bound the distance from the initialization
kwr⑴-Wr(O)k2 ≤ / U dSwr
(S) dS ≤
2
4 √n ky — U(O)k2
√mλo
□
Lemma A.5. With Probability at least 1 - δ over initialization, the following holds. SuPPose for
O ≤ s ≤ t, λmin (H(s)) ≥ λ0 and IlWr(S) - wr(O)k2 ≤ RW. Then we have ∣a(t) - a『(O)| ≤ Ra
for all r ∈ [m].
ProofofLemmaA.5. Note for any i ∈ [n] and r ∈ [m], w『(O)>Xi 〜 N (0,1). Therefore applying
Gaussian tail bound and union bound we have with probability at least 1 - δ, for all i ∈ [n] and
17
Published as a conference paper at ICLR 2019
r ∈ [m], ∣Wr (0)>Xi∣ ≤ 3 Jlog (mn). Now we bound the gradient. Recall for 0 ≤ S ≤ t,
ar (sr (S)
ds
∣1 n
=√= f (f (w(s), a(s), Xi) — yi) σ (Wr(S)>Xi))
∣ m i=1
≤√U √n ky — u(s)k2 (∣Wr (s)>Xi∣ + Rw)
≤-j= √nky -u(s)k2 (3ι∕log
≤
4√n Ily - u(0)k exp(-λos/2)√log (mn∕δ)
√m
Integrating the gradient, we can bound the distance from the initialization
kar ⑴—ar (O)k2 ≤ / [J ddS Wr
(s)	ds ≤
2
8√n ky — u(0) k2 ,log(mn∕δ)
√mλo
□
LemmaA.6. If Rw < Rw andRa < Ra, Wehaveforallt ≥ 0, λmin(H(t)) ≥ 1 λ°,forallr ∈ [m],
kWr(t) - Wr(0)k2 ≤ R0w, |ar(t) - ar(0)| ≤ R0a and ky - u(t)k22 ≤ exp(-λ0t) ky - u(0)k22.
Proof of Lemma A.6. We prove by contradiction. Let t > 0 be the smallest time that the conclusion
does not hold. Then either λmin(H(t)) < λ0 or there exists r ∈ [m], ∣Wr(t) — Wr(0)∣2 ≤ Rw
or there exists r ∈ [m], % — sr(0)| ≤ Ra. If λmin(H(t)) < λ0, by Lemma A.3, we know
there exists S < t such that either there exists r ∈ [m], kWr(S) - Wr(0)k2 ≤ Rw or there exists
r ∈ [m], |ar(S) — ar(0)| ≤ Ra. However, since R0w < Rw and R0a < Ra. This contradicts with
the minimality oft. If there exists r ∈ [m], kWr — Wr(0)k2 ≤ Rw0 , then by Lemma A.4, we know
there exists s < t such that r ∈ [m], |sr(s) — sr(0)| ≤ Ra or λmin(H(s)) < λ0. However, since
R0w < Rw and R0a < Ra . This contradicts with the minimality of t. The last case is similar for
which we can simply apply Lemma A.5.	□
Based on Lemma A.2, we only need to ensure R0w < Rw and Ra < R0a. By the proof in Section 3,
we know with probability at least δ, ∣y — u(0)∣∣2 ≤ Cδn for some large constant C. Note our
section on m in Theorem 3.3 suffices to ensure R0w < Rw and Ra < Ra0 . We now complete the
proof.
B Technical Proofs for Section 4
Proof of Corollary 4.1. We use the norm of gradient to bound this distance.
k
kWr(k+ 1) —Wr(0)k2 ≤η X
k0=0
∂L(W(k0))
∂Wr (k0)	2
k
≤ηX
k0=0
√n ky — u(k0)k2
√m
≤η 工 √n¾≡ ky - u(k0)k2
≤η W HE ky-u(k0)k2
4√n ky - U(O) k2
√mλo
□
18
Published as a conference paper at ICLR 2019
Proof of Lemma 4.1. For a fixed i ∈ [n] and r ∈ [m], by anti-concentration inequality, we know
P(Air) ≤ √2R∏. Thus We can bound the size of S⊥ in expectation.
E [∣S⊥∣] = XXP(Air) ≤ 2mR.	(15)
2π
r=1
Summing over i = 1, . . . , n, We have
e"X∣s⊥∣] ≤ 2√2∏r.
i=1
Thus by Markov’s inequality, We have With probability at least 1 - δ
n ∣	∣ CmnR
Σ∣S⊥∣≤ -ɪ-.	(16)
i=1
for some large positive constant — > 0.	□
19