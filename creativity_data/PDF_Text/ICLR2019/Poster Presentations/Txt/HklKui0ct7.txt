Published as a conference paper at ICLR 2019
Off-Policy Evaluation and Learning
from Logged Bandit Feedback:
Error Reduction via Surrogate Policy
Yuan Xie*
Indiana University Bloomington
xieyuan@umail.iu.edu
Boyi Liu*
Northwestern University
boyiliu2018@u.northwestern.edu
Qiang Liu
The University of Texas at Austin
lqiang@cs.utexas.edu
Zhaoran Wang
Northwestern University
zhaoranwang@gmail.com
Yuan Zhou
Indiana University Bloomington
yzhoucs@iu.edu
Jian Peng
University of Illinois at Urbana-Champaign
jianpeng@illinois.edu
Ab stract
When learning from a batch of logged bandit feedback, the discrepancy between
the policy to be learned and the off-policy training data imposes statistical and
computational challenges. Unlike classical supervised learning and online learning
settings, in batch contextual bandit learning, one only has access to a collection of
logged feedback from the actions taken by a historical policy, and expect to learn a
policy that takes good actions in possibly unseen contexts. Such a batch learning
setting is ubiquitous in online and interactive systems, such as ad platforms and rec-
ommendation systems. Existing approaches based on inverse propensity weights,
such as Inverse Propensity Scoring (IPS) and Policy Optimizer for Exponential
Models (POEM), enjoy unbiasedness but often suffer from large mean squared
error. In this work, we introduce a new approach named Maximum Likelihood In-
verse Propensity Scoring (MLIPS) for batch learning from logged bandit feedback.
Instead of using the given historical policy as the proposal in inverse propensity
weights, we estimate a maximum likelihood surrogate policy based on the logged
action-context pairs, and then use this surrogate policy as the proposal. We prove
that MLIPS is asymptotically unbiased, and moreover, has a smaller nonasymptotic
mean squared error than IPS. Such an error reduction phenomenon is somewhat
surprising as the estimated surrogate policy is less accurate than the given historical
policy. Results on multi-label classification problems and a large-scale ad place-
ment dataset demonstrate the empirical effectiveness of MLIPS. Furthermore, the
proposed surrogate policy technique is complementary to existing error reduction
techniques, and when combined, is able to consistently boost the performance of
several widely used approaches.
1	Introduction
While maintaining online and interactive systems for information retrieval, news recommendation,
ad placement, and e-commerce, we collect large batches of logs during testing phases and past
deployment periods (Li et al., 2010). Such logs contain rich information that can be used for many
purposes. For example, the logs of an e-commerce system record the decisions (or actions) on which
ads or recommendations are displayed to a user given a context, and the corresponding feedback (or
reward), including whether the user clicks on any of the displayed items and whether a purchase
* equal contribution, the order of the two co-first authors is determined by a coin toss
1
Published as a conference paper at ICLR 2019
occurs afterwards. Although the logs are collected when the online system is deployed with a
historical algorithm (or policy), they are informative for future improvements and design of the
system. One application of such logs is to estimate the performance of new policies, also known
as off-policy evaluation (DUd´k et al., 2014; Li et al., 2011; 2014). Furthermore, the use of logs
allows for an accurate and more efficient way to test and optimize new policies without expensive
trial-and-error cycles in traditional A/B tests, which often take weeks.
Since the logged feedback only provides partial information, policy evaluation and optimization using
logs are often formulated as batch contextual bandit learning. Different from the online learning
setting, batch contextual bandit learning is statistically more challenging because the collected logs
are off-policy, that is, they are generated by a historical policy that differs from the current policy we
intend to evaluate or optimize. To bridge this discrepancy, Inverse Propensity Scoring (IPS) has been
introduced to construct an unbiased counterfactual estimator of policy performance using off-policy
data (Bottou et al., 2013). However, such an estimator tends to have a large mean squared error
when the current policy and the historical policy are very different. To reduce the mean squared
error, a number of regularization and control variate approaches have been introduced (e.g., Dudlk
et al. (2014); Hirano et al. (2003); Ionides (2008); Li et al. (2015)). Notably, Swaminathan &
Joachims (2015a) proposed Policy Optimizer for Exponential Models (POEM), which introduces a
mean squared error regularizer based on the counterfactual risk minimization principle. Empirical
evaluations also suggested the effectiveness of these approaches on policy evaluation and optimization.
In this paper, we introduce a new yet simple parametric approach for mean squared error reduction in
batch contextual bandit learning. Instead of using the given historical policy, we use linear models or
neural network models to estimate a maximum likelihood surrogate policy using the logged action-
context pairs. Then we use such a surrogate policy as the proposal distribution to obtain the inverse
propensity weights in the off-policy estimator, as if the logs are generated by this surrogate policy.
This idea dates back to the “estimated sampler” technique in statistics (Delyon & Portier, 2016;
Henmi et al., 2007). We provide theoretical justification for this approach, which is named Maximum
Likelihood Inverse Propensity Scoring (MLIPS). In particular, we show that the proposed MLIPS
estimator is asymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared
error than the IPS estimator. Such an error reduction effect is counterintuitive as we replace the
known historical policy with a less accurate estimated surrogate policy, which is supposed to increase
the mean squared error. We evaluate the MLIPS estimator on several multi-label classification
problems and a large-scale ad placement dataset to demonstrate its empirical effectiveness on mean
squared error reduction for policy evaluation and optimization. Furthermore, we also combine the
MLIPS estimator with several existing approaches. We find that our surrogate policy technique
is complementary to existing mean squared error reduction techniques and achieves consistently
improved performance.
2	Off-Policy Evaluation and Learning from Logged Bandit
Feedback
In this section, we introduce the background on off-policy evaluation and learning from logged bandit
feedback. In particular, we focus on IPS and present several existing mean squared error reduction
and regularization approaches.
2.1	Inverse Propensity Scoring
The value function used to evaluate a target policy π is defined as
V = En[r]= Ex”Ea〜∏(∙∣ X)Er〜D(∙∣a,x)[r].	(2.1)
Here x is the context drawn from the context distribution λ, which is independent of π, a is the action
taken according to π given context x, and r is the reward (or feedback) received.
In the off-policy setting, we are not able to sample different actions from the target policy π and
obtain reward, which is expensive and time-consuming in practice. Instead, we collect samples from
a logging policy μ, that is, a 〜μ(∙ | x), and expect to use the logs to evaluate a target policy and
further perform policy optimization.
2
Published as a conference paper at ICLR 2019
To bridge the discrepancy between ∏ and μ, the IPS estimator, which is known as the importance
sampling estimator, reweights each logged action-context pair as follows,
e 1 S (	1	1 春 π(ai 1 Xi)	SrVX
VIPS = n ‰ρ(xi,ai) ∙ ri = n2^μ(a∙ ∣ χ,)∙ ri∙	QZ
In the sequel, we use V to denote VIPS for notational simplicity.
It is worth noting that the IPS estimator is an unbiased estimator of V , and thus has been
widely adopted as an objective function for optimizing the target policy π. However, ρ(xi, ai) =
∏(ai | Xi)∕μ(ai | Xi) in (2.2) may induce a large mean squared error, especially when the logging
policy μ is very different from the target policy ∏. As a result, the IPS estimator often leads to
unsatisfactory policy optimization and poor generalization performance.
2.2	Mean S quared Error Reduction and Regularization
To reduce the mean squared error of IPS, several thresholding approaches have been proposed and
studied in the literature (e.g., Bottou et al. (2013); Cortes et al. (2010); Ionides (2008); Strehl et al.
(2010)). One straightforward approach is Propensity Weight Capping (Ionides, 2008), which takes
the form
M = 1 X min{M[ }∙ Tr	(2.3)
where M > 1 is the threshold parameter. Although thresholding reduces the mean squared error
when M is small, it introduces a large bias.
More recently, Swaminathan & Joachims (2015a) proposed the POEM algorithm for batch learning
from bandit feedback. POEM is motivated by the structural risk minimization principle and a
generalization bound that characterizes the mean squared error of VeI(PMS ) based on an empirical
Bernstein argument. The POEM algorithm jointly optimizes VeI(PMS ) and its empirical standard
deviation, which is also known as counterfactual risk minimization.
Let u∏ = min{M,π(ai | Xi)∕μ(ai | Xi)}∙ ri, u∏ = Pn=I u∏/n, and Varn(VPM)) = (Pn=I(Un -
U∏)2)∕(n - 1), the POEM algorithm searches for a policy π that maximizes
VlPM)- λjdr∏(VM))/n.	(2.4)
Here M > 1 and λ ≥ 0 are the thresholding and regularization parameters, respectively. Also note
that when λ = 0, the POEM objective in (2.4) reduces to (2.3).
Motivated by the propensity overfitting problem of IPS and POEM, Swaminathan & Joachims (2015b)
proposed to use control variates, which lead to the following self-normalized estimator
V = 1/n ∙ Pn=I ∏(ai | Xi)∕μ(ai | Xi) ∙ r
SN	1/n ∙ Pn=ι ∏(ai | Xi)∕μ(ai | Xi)
(2.5)
Note that VSN is shifted by a constant C if each ri is shifted to ri + C. In other words, VSN is
equivariant (Hesterberg, 1995) and always lies within the range of ri . In contrast, the IPS estimator
does not have such properties. Self-Normalized POEM (Norm-POEM), a variant of POEM, was also
proposed by Swaminathan & Joachims (2015b). In particular, Norm-POEM searches for a policy
that maximizes the normalized estimator regularized by its empirical standard deviation
___ʌ-.	~	.	,	. x 9
1/n ∙ Pi=ι(ri - VSN)2 ∙ (∏(ai | Xi)∕μ(ai | Xi))
(1/n ∙ Pn=I ∏(ai I Xi)∕μ(ai | Xi))2
(2.6)
In addition, other techniques, such as control variates and doubly-robust estimators Dudlk et al.
(2014) have also been applied to reduce the mean squared error of the off-policy estimator.
3
Published as a conference paper at ICLR 2019
3	Error Reduction via Surrogate Policy
Instead of directly using the logging policy propensities, we propose to refit a surrogate policy and
use it to obtain the inverse propensity weights in the IPS estimator defined in (2.2). We assume that
the logging policy μ(a | x) is parameterized by β ∈ Rd and denote it by μ(a | x; β). In particular,
We assume the logging policy with β = β* generates the logged action-context pairs. Although We
have the access to μ(a | x; β*), We choose to use the maximum likelihood estimator of β* based
on {(ai, Xi)}n=ι as a surrogate policy parameter, which is denoted as β, to replace β* and obtain a
more accurate estimator of the value function V defined in (2.1). In particular, We define the MLIPS
estimator of V as
V _ ι χχ n(ai | Xi)
=	∙r i,
n i=1 μ(ai | Xi； β)
(3.1)
Where {(ai, xi)}in=1 are the logged action-context pairs, Which are independently sampled from
μ(a | x; β*).
Unlike the IPS estimator V in (2.2), the MLIPS estimator V in (3.1) is biased in general. HoWever,
V is asymptotically unbiased, and moreover, in the folloWing subsection We shoW that V achieves a
■~■
smaller mean squared error than V when the logging distribution μ(a | x; β) is properly chosen.
3.1	Theoretical Analysis
In this section, for the simplicity of analysis, we assume that the reward r is determined by a and
x, that is, r = r(a, x). For notational simplicity, in the subsequent analysis we use the following
equivalent definitions of V and V based on joint distributions,
V = 1	χχ	π(ai,xi)	∙ r(a.	x.)	V = 1 χχ	π(ai,xi)	∙ r(a.	x.)
=n = μ(ai，々;β *)	(i, ",	= n = 〃&,% β)
since we have
π(a | x)	π(a, x)∕λ(x)	π(a, x)
μ(a | x; β)	μ(a, x; β)∕λ(x)	μ(a, x; β)
(3.2)
For the ease of notation, we denote E@x)〜*(.,.；e*)[ ∙ ] by E[ ∙ ]. Also, we denote by ∣∣ ∙ ∣∣ the '2-norm
of a vector or the spectral norm of a matrix. For a function f (β), we denote by ∂f (β*)∕∂β its
derivative evaluated at β = β*, which is ∂f (β)∕∂β∣β=β*.
Before we lay out the main theory, we first introduce the following definitions.
Definition 3.1 (Score Function and Fisher Information). The score function S(a, x; β) ∈ Rd and the
Fisher Information matrix I(β) ∈ Rd×d are defined as
S(a, x; β) = d log 力 β),	I (β) = -E [ dS⅛产].	(3.3)
It is worth noting that the reformulation from μ(a | x; β) to μ(a, x; β) does not change the score
function or the Fisher information matrix, since the marginal distribution of the context x in (3.2),
namely λ(x), does not depend on β, and hence its partial derivative with respect to β is zero.
Definition 3.2 (Deviation Function). The difference between π(a, x)∕μ(a, x; β) ∙ r(a, x) and the
true value V of the target policy, which is defined in (2.1), is denoted by
DV(r, a, x; β) = :M XL ∙ r(a, x) 一 V.	(3.4)
μ(a, x; β )
In the sequel, we introduce two assumptions and a condition on an event. The next assumption
ensures that the Fisher information matrix of the joint logging distribution μ(a, x; β*) is well behaved
and the identifiability of the model is thus ensured.
4
Published as a conference paper at ICLR 2019
Assumption 3.3 (Nonsingular Fisher Information). For the joint distribution μ(∙, ∙; β*), We assume
that its Fisher information matrix satisfies
kI-1 (β*)k =卜]ASMdx β*) Hi =。⑴.	(3.5)
The next assumption ensures the score and deviation functions are sufficiently smooth.
Assumption 3.4 (Smoothness of Score and Deviation). For the deviation function DV (r, a, x; β)
defined in (3.4), We assume that
IE [dDV(rdβx β*)H=O(ZDG),卜[序DV∂β尸 βH=O(ZDH),	(3.6)
Where the second equality holds for any β. MeanWhile, for the score function S(a,x; β) defined in
(3.3), We assume that for any v ∈ Rd such that kvk = 1, it holds that
p2S(a,x; β)	I II
E ----∂β---(v, v, ∙) || =O(ZST),	C7)
where ∂2S(α, x; β)∕∂β2 is a tensor consisting of the third-order derivative of logμ(α, x; β) with
respect to β. Here (∂2S(a,x; β0)∕∂β2)(v1,v2, ∙) is the bilinear map Rd X Rd → Rd induced by the
tensor, which maps (v1, v2) to a vector in Rd.
The Assumptions 3.3-3.4 are satisfied, under necessary regularity conditions, by many popular models
such as the multinomial logistic regression model which will be illustrated in detail in §D. Before
introducing the condition, we define several events to simplify the presentation. In the following, we
first define an event on the estimation error of the maximum likelihood estimator β.
Definition 3.5 (Maximum Likelihood Estimation Error). For the maximum likelihood estimator β
and the true parameter β*, we define the event Ee (tg) as
Ee(tβ) = {kb — β*k≤ tβ}∙	(3.8)
Next we define several events on the concentration behavior of the score function defined in (3.3) and
the deviation function defined in (3.4) as well as their derivatives.
Definition 3.6 (Concentration of Score). For the score function with true parameter β*, we define
the following events,
ESG(tSG)
II n XX S (ai,xi; β*)- E[S(α,x; β*)]
≤ tSG ,
(3.9)
ESH(tSH)
f|| 1 S ∂S(a ,xi; β *)
∣l n i=ι-∂	E
∂S(a, x; β*)
_ ∂β _
≤ tSH .
(3.10)
Also, we define
1 /, d (	1 Sd2s(ai,xi；β)z	、IH,∣"d2S(a,x；β)t	\"IL+ 1
EST(tST) = f e,sup=ι n E —酹—(v，…E [	∂β2	(v，v，•)] ∣∣ ≤ tST∫，
where (∂2S(a,x; β)∕∂β2)(v,v, ∙) is defined in Assumption 3.4. Then we define the joint event
ES(tS) = ESG(tSG) ∩ ESH(tSH) ∩ EST(tST), where tS = (tSG, tSH, tST).
Definition 3.7 (Concentration of Deviation). For deviation function with true parameter β*, namely
DV(r, a, x; β*), we define
EDG(tDG)
∂DV (r ,a1,x∖ β*)
∂β
-E
∂DV (r, a, x;
∂β^^
(3.11)
Also, we write
EDH (tDH ) =	sup
e∈Rd
1 XX ∂2DV(ri,ai,xi; β)
n ⅛ 郝
-E
,a2DV(r, a,x e]∣∣ ≤ tDH
Then we define the joint event ED (tD) = EDG (tDG) ∩ EDH (tDH ), where tD = (tDG , tDH ).
We now present a condition on the overall tail behavior of the probability of the events defined above.
5
Published as a conference paper at ICLR 2019
Condition 3.8 (Concentration Tail Behavior). Let t? = (tβ , tS , tD) and
E(t?) =Eβ(tβ) ∩ES(tS) ∩ED(tD),	(3.12)
it holds that P(E(t?)) ≥ 1 - f (t), where t = ∣∣t？∣∣∞ for some decreasing tail function f (t) such that
0∞ tk df (t) is finite and goes to zero as n → ∞ for any nonnegative integer k and decreases along
k. Here f(t) also depends on the dimension d and the sample size n.
Condition 3.8 states that, in addition to the upper bound on the estimation error defined in Definition
3.5, the sample versions of the score function defined in (3.3) and the deviation function defined
in (3.4) as well as their derivatives all well concentrate to their population versions. In particular,
the condition on the integrability of the tail function f (t) characterizes that the tail probability of
concentration decays faster than the polynomial rate. As we will illustrate using the multinomial
logistic regression model in §D, such a superpolynomial tail behavior is typically guaranteed by
exponential concentration inequalities such as Bernstein-type inequalities and their matrix variants.
The following theorem compares the mean squared errors of the MLIPS and IPS estimators defined
in (3.1) and (2.2). Recall that ζDG, ζST, and ζDH are defined in Assumption 3.4. Since ζDG, ζST, and
ζDH are population quantities, they do not scale with the sample size n.
Theorem 3.9 (Mean Squared Error Reduction). Under Condition 3.8 and Assumptions 3.3 and 3.4,
the MLIPS Vb is asymptotically unbiased and it holds that
E[(V - V )2] = E[(V - V )2] - (l∕n∙ Var(∏(r,x,a; β *)) -ξ(n)),	(3.13)
'×{z}
Reduction of MSE
where
Π(a, x; β*) = E[Dv (r, a, x; β*) ∙ S(a,x; β*)>] ∙ I-1(β*) ∙ S (a, x; β *),	(3.14)
and ξ(n) = (E[(V - V)2])1/2 ∙ O((ZDG ∙ (ZST + 1)+ Zdh) ∙ (R∞ t4 df (t))1/2) + O((ZDG ∙ (ZST +
I) + ZDG ∙ ZDH) ∙ R0∞ t3 df (t)).
Proof. See §A for a detailed proof.
□
It is worth noting that the key to our guarantee for MSE reduction lies in the fact that
E ((V - V) - 1 XX ∏(ri,ai,xi; β*))	= E[(V - V )2] - 1/n ∙ Var(∏(r,x,a; β*)),
n i=1
which is actually an orthogonality relation between (V - V) - (1/n) ∙ PZi ∏(ri, ai,xi； β*) and
(1/n) ∙ Pi=ι ∏(ri,ai,xi; β*)∙	一
We interpret the MSE reduction guarantee in Theorem 3.9 as follows. For the simplicity of dis-
cussion, for now we assume that the dimension d does not scale with the sample size n. The
1/n ∙ Var(Π(r, x, a; β*)) term on the right-hand side of (3.13) characterizes the reduction of the
mean squared error. For example, in the multinomial logistic regression model, the ξ(n) term is
roughly of the order 1/n3/2. As a consequence, for a sufficiently large n, we have
1/n ∙ Var(Π(r, x, a; β*)) — ξ(n) ≥ 1/(2n) ∙ Var(∏(r, x, a; β*)).	(3.15)
In other words, the mean squared error of the MLIPS estimator is at least smaller than that of the
IPS estimator by a margin of 1/(2n) ∙ Var(Π(r, x, a; β*)), where Var(Π(r, x, a; β*)) is a population
quantity that does not scale with n. In comparison, the mean squared error is also of the order 1/n in
the multinomial logistic regression model. That is to say, the reduction effect does not vanish even
when n → ∞. See §D for more discussions and a more detailed illustration of Theorem 3.9 using the
multinomial logistic regression model.
3.2	Practical Considerations
Throughout the theoretical analysis, we assume that the parametrization of the logging policy is
known. However, in practice we may not know the class of logging policies that generate the logs, in
which cases we can use universal function approximators such as neural networks to estimate such
surrogate policies. The parametrization of the logging distribution may be misspecified when we
6
Published as a conference paper at ICLR 2019
use those approximators. However, when universal function approximators such as neural networks
are used, the approximation error often diminishes with an increasing number of layers and neurons
(see, for example, Schmidt-Hieber (2017); Telgarsky (2016); Yarotsky (2017)). Such a diminishing
approximation error should enter the Taylor expansions in Lemma A.1-A.2.
In our experiments, we found that neural network policies worked very well on both standard
multi-label classification datasets and a large-scale ad placement dataset.
Due to its simplicity, MLIPS can be easily implemented without much extra effort. Furthermore,
MLIPS is orthogonal to other existing approaches for mean squared error reduction in policy eval-
uation and optimization. Hence, it can often be combined straightforwardly with them. In our
experiments, we observed that it can be used to further improve POEM for policy optimization.
Another advantage of our approach is that it still works even when we have no access to the logging
policy, which often happens in real world due to various reasons. In those cases, IPS may not work at
all, but our approach exhibits superior performance, as shown in our experiments.
4	Experiments
In this section, we empirically evaluate the effectiveness of the proposed method on a number of
datasets. We first check the reduction of the mean squared error enabled by the MLIPS estimator. Then
we apply IPS, MLIPS, POEM and MLPOEM to policy optimization and compare their performance
on several datasets.
4.1	Settings
Datasets. First, we adopt the same experimental design as the one specified in Swaminathan &
Joachims (2015a) to generate batch contextual bandit learning datasets from supervised learning
datasets. Four multi-label datasets are selected from the LibSVM repository (see Swaminathan &
Joachims (2015a) for more details). To construct a batch contextual bandit learning dataset, we take
a supervised learning dataset D* = {(xi, y*)}n=ι, simulate using a logging policy μ by sampling
yi 〜μ(∙ | Xi), and collect the reward as the negative Hamming distance -∆(y*,yi) between the
supervised label and the sampled label. We use a Conditional Random Field (CRF) model (Sutton &
McCallum, 2012) trained on 5% of the data as the logging policy μ. We repeat this procedure four
times to generate the entire bandit feedback dataset D = {(xi, yi, r = -∆(yi,yi), μ(y% | Xi))}n=ι.
Both the training sets and held-out testing sets are constructed in the same way as suggested in
Swaminathan & Joachims (2015a).
In addition to these standard datasets, we also evaluate the effectiveness of our method on a large-scale
ad placement dataset. The dataset is collected by Criteo, a leader in the display advertising industry
(Lefortier et al., 2016). For this dataset, we consider an ad display scenario where a policy selects the
products to be displayed on a website when a user arrives. Provided the candidate set of products and
impression context, we hope the policy to place ads such that the aggregated click-through-rate (CTR)
or the number of clicks is maximized. We consider a subset of the dataset, which is used in the NIPS
2017 Criteo Ad Placement Challenge. Each ad is represented by a 74000-dimensional feature vector.
For each context (or impression), the logged action of selecting a candidate ad, a binary reward value
on whether the displayed product is clicked by the user, and the probability that the logging policy
picks such a specific candidate are recorded. More details can be found at CrowdAI (2017).
Surrogate policy estimation. To fit the surrogate policy, we apply two models, a simple logistic
regression model and a one-hidden-layer perceptron with ten ReLU activation units, both with `2-
regularization. Here five-fold cross validation is used to tune the regularization parameter. We use the
L-BFGS algorithm (Liu & Nocedal, 1989) for optimization.
Policy optimization. We consider IPS, POEM (Swaminathan & Joachims, 2015a), and Norm-
POEM (Swaminathan & Joachims, 2015b). We compare their performance with MLIPS as well
as the combination of our surrogate policy technique with POEM (MLPOEM). There are several
hyperparameters in POEM, Norm-POEM, and MLPOEM, including the thresholding parameter and
the regularization parameter. We use five-fold cross validation on the training set to tune them. For
reward maximization, following the procedures of POEM, we initialize the model weights to zeros,
7
Published as a conference paper at ICLR 2019
use mini-batch AdaGrad (Duchi et al., 2011) with a batch size of 100 and step size η = 1. On all
datasets, we repeat the experiments ten times to report the average generalization performance.
4.2	Mean S quared Error Reduction in Policy Estimation
First, we check whether the MLIPS estimator has a smaller mean squared error than the vanilla IPS
estimator. Instead of comparing the estimation of the value function, we focus on comparing the
performance of MLIPS and IPS on off-policy gradient estimation, whose accuracy is crucial to policy
optimization. We use the LYRL dataset (Swaminathan & Joachims, 2015a) and combine the training
and testing sets to obtain a large bandit feedback dataset. Given a reasonably accurate policy weight
vector (after training with 10000 mini-batch updates), we compute its IPS-based gradient estimator
on the entire dataset to get a rough “ground-truth” gradient. We further construct a series of data
subsets to evaluate the gradient estimation with smaller sample sizes, with 2-1/2 , 2-2/2 , . . . , 2-k/2
of the entire dataset.
For the neural network estimate of surrogate policy
(NN-surrogate), we first fit a logistic regression model
and a one-hidden-layer perceptron with ten ReLU ac-
tivation units to estimate the surrogate policy. After
this step, we compute the Euclidean distance between
the “ground-truth” gradient and the gradient estima-
tors that correspond to smaller sample sizes. Average
Euclidean distances and standard deviations are com-
puted over twenty trials. Results are illustrated in
Figure 1.
We observe that the MLIPS estimator gives much
better gradient estimation than the original IPS es-
timator. In particular, the relative improvement is
significant especially when the sample size is small.
It is also interesting to see that the difference between
Figure 1: Log MSE of estimations for off-
policy gradients made by IPS (logging policy),
MLIPS (linear surrogate) and NN-fitted MLIPS
(NN-surrogate) for logistic regression model on
the LYRL dataset in Swaminathan & Joachims
(2015a).
the linear surrogate policy and the neural network
surrogate policy is very small, possibly because the
actual logging CRF policy is linear. On the Criteo
dataset, in which the logging policy is unknown and
possibly very complicated, we find that the neural network surrogate policy works better than the
linear policy.
4.3	Generalization Performance of Policy Optimization
To evaluate the effectiveness of our method on policy optimization, we train logistic regression
policies with IPS, POEM, Norm-POEM objective functions, and the IPS and POEM versions with
maximum likelihood surrogate policies (MLIPS and MLPOEM). As mentioned before, we use both
the logistic regression model and the one-hidden-layer perceptron model to fit the surrogate policies,
which are respectively marked with “-Lin” and “-NN” suffixes in the names of the training methods.
Results are summarized in Table 1. The test set Hamming losses for all methods are averaged over
ten runs. On each run, both MLIPS and MLPOEM consistently and significantly outperforms their
corresponding original versions.
As a control, we study the scenario where the logging policy is not available. We train the policies
using IPS while treating the logging policy to be uniform. The performance of such a variant of
IPS (the last row in the table with algorithm name “IPS-Uniform”) is significantly worse than our
method, which also does not require knowing the logging policy. Notably, when compared against
(Norm-)POEM, the state-of-the-art algorithm on these datasets, our MLPOEM algorithm can augment
the generalization performance on most datasets. This indicates that our surrogate policy technique
provides a complementary mean squared error reduction effect for existing methods in this field.
Finally, it is worth noting that the methods with neural network surrogate polices and linear policies
are not too much different in terms of performance on these datasets, possibly because the logging
policies used to generate these datasets are linear.
8
Published as a conference paper at ICLR 2019
4.3.1	Criteo Ad Placement Dataset
We also evaluate the performance of our
method on the preprocessed data used in
the NIPS 2017 Workshop Criteo ad place-
ment challenge. Because there is no infor-
mation about the logging policy used to
generate the data, it is reasonable to use a
complex universal function approximator
to fit the surrogate policy. We implement
a neural network model with 21 tanh ac-
tivation units for both the surrogate policy
and the target policy. Due to the massive
size of this dataset and its high-dimensional
and sparse feature representation, we im-
Plement the '2-regularized IPS and MLIPS
in C++ with multi-threading.
Both the maximum likelihood estimation
and Policy oPtimization are Performed us-
ing the L-BFGS algorithm. We sPlit the
dataset into training (80%) and testing
(20%) sets. The regularization Parameter
is chosen by five-fold cross-validation on
Table 1: Evaluations of the Hamming losses for Policy
oPtimizations Performed by imPortance weighed Pol-
icy gradient estimators and their ML-variants on the
datasets of Swaminathan & Joachims (2015a). Eval-
uations of Norm-POEM are also included as general
benchmarks. We include extra results for this exPeri-
ment in APPendix E
Dataset/Alg.	Scene	Yeast	TMC	LYRL
IPS	1.342	4.571	3.023	1.108
POEM	1.143	4.549	2.522	0.981
Norm-POEM	1.045	3.876	2.072	0.799
MLIPS-Lin	1.086	3.778	2.018	1.025
MLIPS-NN	1.086	3.630	2.019	0.930
MLPOEM-Lin	1.086	3.894	2.010	0.949
MLPOEM-NN	1.086	3.477	2.000	0.904
IPS-Uniform	1.086	5.893	6.174	1.463
the training set. Evaluation is Performed by comPuting the inverse ProPensity weights as suggested
by the NIPS challenge. In comParison with IPS, which obtains 0.556 on the training set and 0.551
on the testing set, our method obtains 0.586 on the training set and 0.572 on the testing set. An
imPlementation of our method (0.576 on the training set and 0.568 on the testing set, which are
evaluated on the hold out data for the comPetition) is also ranked among very toP (Prize winning
teams) of the NIPS 2017 challenge leaderboard.
5	Conclusion
We introduce a new but simPle aPProach for mean squared error reduction in Policy evaluation and
oPtimization. Theoretical analysis illustrates that the ProPosed MLIPS estimator is asymPtotically un-
biased, and moreover, has a smaller mean squared error than the classical IPS estimator. ExPerimental
results on several large-scale datasets also demonstrate the emPirical effectiveness of the ProPosed
estimator. Our technique can also be combined with existing aPProaches and further imProves the
Performance.
9
Published as a conference paper at ICLR 2019
References
Leon Bottou, Jonas Peters, JoaqUin Quinonero-Candela, Denis X Charles, D Max Chickering, Elon
Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning
systems: The example of computational advertising. The Journal of Machine Learning Research,
14(1):3207-3260, 2013.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.
In Advances in Neural Information Processing Systems, pp. 442-450, 2010.
CrowdAI. CrowdAI NIPS 2017 Criteo Ad Placement Challenge Starter Kit. https://github.
com/crowdai/crowdai- criteo- ad- placement- challenge- starter- kit,
2017.
Bernard Delyon and Francois Portier. Integral approximation by kernel smoothing. Bernoulli, 22(4):
2177-2208, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Miroslav Dudik, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and
optimization. Statistical Science, pp. 485-511, 2014.
Masayuki Henmi, Ryo Yoshida, and Shinto Eguchi. Importance sampling via the estimated sampler.
Biometrika, 94(4):985-991, 2007.
Tim Hesterberg. Weighted average importance sampling and defensive mixture distributions. Techno-
metrics, 37(2):185-194, 1995.
Keisuke Hirano, Guido W Imbens, and Geert Ridder. Efficient estimation of average treatment effects
using the estimated propensity score. Econometrica, 71(4):1161-1189, 2003.
Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical
Statistics, 17(2):295-311, 2008.
Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims, and Maarten de Rijke. Large-
scale validation of counterfactual learning methods: A test-bed. arXiv preprint arXiv:1612.00367,
2016.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th International Conference
on World Wide Web, pp. 661-670. ACM, 2010.
Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextual-
bandit-based news article recommendation algorithms. In Proceedings of the Fourth ACM Interna-
tional Conference on Web Search and Data Mining, pp. 297-306. ACM, 2011.
Lihong Li, Shunbao Chen, Jim Kleban, and Ankur Gupta. Counterfactual estimation and optimization
of click metrics for search engines. arXiv preprint arXiv:1403.1891, 2014.
Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In
Artificial Intelligence and Statistics, pp. 608-616, 2015.
Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical Programming, 45(1-3):503-528, 1989.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. arXiv preprint arXiv:1708.06633, 2017.
Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. Learning from logged implicit
exploration data. In Advances in Neural Information Processing Systems, pp. 2217-2225, 2010.
Charles Sutton and Andrew McCallum. An Introduction to Conditional Random Fields. Nowpublish-
ers, 2012.
10
Published as a conference paper at ICLR 2019
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged
bandit feedback. In International Conference on Machine Learning, pp. 814-823, 2015a.
Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning.
In Advances in Neural Information Processing Systems, pp. 3231-3239, 2015b.
Matus Telgarsky. Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
11
Published as a conference paper at ICLR 2019
A Proof of Theorem 3.9
The proof of Theorem 3.9 is established based on the following two lemmas. First, we intro-
duce a lemma that relates the errors of the MLIPS and IPS estimators. Recall that, as defined in
Definitions 3.5-3.7 and Condition 3.8, we have t? = (tβ , tS , tD) where tS = (tSG , tSH , tST) and
tD = (tDG, tDH). Also, recall that ζDG, ζDH, and ζST are defined in Assumption 3.4.
Lemma A.1. For the MLIPS estimator V defined in (3.1) and the IPS estimator V defined in (2.2),
under Assumption 3.4 and conditioning on the event E(t?) defined in (3.12), we have
V - V =(V - V) - E[Dv(r, a, x; β*) ∙ S(a, x; β*)>] ∙ G - β*)
+ O(tβ YtDH + ZDH) + tβ ∙ ^⅛g)∙	(A.1)
Proof. See §B for a detailed proof.
□
The following lemma characterizes the β - β* term on the right-hand side of (A.1), which is the
..	..	，公
estimation error of β.
Lemma A.2. For the maximum likelihood estimator β and the true parameter β*, we have
^
β - β*
=I-1(β*) ∙ [1 XX S(ai,Xi; β*)- ∆s(b - β*) + ； XX SldSSa晨i BeS (b - β*,b - β*q
n	2n	∂β2
i=1	i=1
where
八 _1 X SS(ai，Xi； β*)	EJ sS(a,x; β*) ]	R λ 8**( 1 λ 伞	…
△s = n≥^---------∂β--------E[------∂β------]， βs = λsβ + (1 - λs)β.	(A.2)
Here λs ∈ [0,1], I-1(β*) is the inverse of the Fisher information matrix defined in (3.3), and
(∂2S(a, x; β*)∕∂β2)(v1, vd, ∙) is the bilinear map defined in Assumption 3.4.
Proof. See §C for a detailed proof.
□
τ	・ C 「	.	1	.1	.	/ A <、	、T	1	.
Lemma A.2 allows us to more precisely characterize the β - β* term in (A.1). Now we are ready to
prove Theorem 3.9.
Proof of Theorem 3.9. By combining Lemmas A.1 and A.2, under Assumptions 3.3 and 3.4 and
conditioning on the event E(t?) defined in (3.12), we have
1n
V - V = (V - V) - — £ E[Dv(r, a, x; β*) ∙ S (a, x; β*)>] ∙ I -1(β *) ∙ S(a，i, Xi; β *) +η(t?),
i=1、	-{z	J
Π(ri, ai, xi; β*)
(A.3)
where
η(t?) =	O(Zdg ∙ tβ ∙	(Zsτ	∙ tβ	+ tsH)	+ tβ	∙	(tβ	∙ tDG	+ tDH	+	Zdh))∙	(A.4)
First, we show that the MLIPS V is asymptotically unbiased. Note that we have E[S(a, x; β*)] = 0,
the expectation of the second term on the right hand side of (A.3) is zero. Thus, as V is an unbiased
estimator of V, we have
lim E[Vb - V] = lim E[η(t?)].
n→∞	n→∞
(A.5)
By Condition 3.8 and the definition of η(t?) in (A.4), we have
E[η(t?) I E(t?)] = O(ZdG ∙ tβ ∙ (Zsτ ∙ tβ + tsH) + t2β ∙ (tDH + ZDH + t力∙ tDG》
=O(Zdg ∙ (Zst + 1) ∙ td + td ∙ (td +1 + Zdh)),	(A.6)
12
Published as a conference paper at ICLR 2019
where t = ∣∣t？∣∣∞. As defined in Condition 3.8,
Therefore, when n → ∞, from (A.6) we have
E[η(t*)] = E[E[η(t?)∣E (t?)]] = O
0∞ tk df (t) goes to zero when n → ∞ for any k.
∙ (ZST + 1)+ Zdh) ∙/ t2 df(t)),	(A.7)
which gives limn→∞ E[η(t?)] = 0. Thus, we have limn→∞ [Vb - V] = 0, which means that the
MLIPS estimator V is asymptotically unbiased.
Next, we proceed to prove the MSE reducing effect of V. For the mean squared error of the MLIPS
estimator V, we have
2
E(Vb - V)2 = E
-n
-V) - n π(ri, ai,xi; β*) +
i=1
-n
-V) - - E∏(ri, ai,xi;β*)
n i=1
{^^^^^^^^^^^^^^^^^^≡
(i)
2
+E Eη2(t?) E (t?)
)J X-------------------}
^{z
(ii)
-n
-V) - n∑∏(ri
,ai,Xi； β*)	∙ η(t?),
(A.8)
E
X
+ 2 ∙ E
2
}
X
}
""^^^^^^^^^^^^^^^^^^^^^^^^^^∖^^^^^^^^^^^^^^^^^^^^^^^^^"""
(iii)
where for term (ii) we use the law of total expectation. In the sequel, we characterize terms (i)-(iii)
respectively.
Term (i) in (A.8): Note that by Definition 3.2 we have
V - V = n X DV(ri, ai,xi; β*), where DV(r, a,x; β)=M”；"β) ∙ r(a,x) - V.
We reformulate the first term on the right-hand side of (A.8) as
E
- V) -
1n	2
-]T∏(ri,ai,Xi; β*))	= E[(V — V)2] + E
i= 1	' -
i=1
-n	2
(n ∑Sπ(ri,ai,xi;β*))
i=1
-2/n2 ∙ E
DV(ri,ai,xi; β*)	∙
Π(ri, ai,xi; β*)
n
2
(A.9)
Then by the fact that E[S(a, x; β*)] = 0, for the second term on the right-hand side of (A.9) we have
E[∏(r, a, x; β*)] = E[Dv(r, a, x; β*) ∙ S(a,x; β*)>] ∙ I-1 (β*) ∙ E[S(a,x; β*)] = 0. (A.10)
Meanwhile, we have
E
DV (ri,ai,xi; β*)	∙
Π(ri,ai,xi; β*)
n
XE[DV(ri,ai ,xi ； β *) ∙ Π(ri,ai,xi; β*)] + X E[Dv (ri,ai,Xi; β*) ∙ Π(r7- ,aj,xj β*)]
i=1
i6=j
=n ∙ E[Dv(r, a, x; β*) ∙ Π(r, a, x; β*)],	(A.11)
where the second equality follows from (A.10). Hence, we obtain from (A.9) and (A.11) that
-n	2
— V)- n Eπ(ri,ai,xi;β))
(A.12)
i=1
E[(V — V)2] +1∕n ∙ Var (Π(r, x, a; β*)) — 2∕n2 ∙ n ∙ E[Dv(r, a, x; β*) ∙ Π(r, a, x; β*)].
Note that We have the following equality for the Fisher information matrix I(β*),
E[S(a,x;β*) ∙ S(a,x; β*)>] = I(β*) = -∂S(a,x;β*)∕∂β∙
(A.13)
E
13
Published as a conference paper at ICLR 2019
Then by the definition of Π(r, a, x; β*) in (3.14), We have
E[∏2(r, a,x; β*)] = E[Dv(r,a,x;β*) ∙ S(a,x;β*)>] ∙ I-1(β*) ∙ E[S(a,x;β*) ∙ S(a,x; β*)>]
. I-1(β*) ∙ E[Dv(r, a, x; β*) ∙ S(a,x; β*)],
which by (A.13) and the definition of Π(r, a, x; β*) implies
E[∏2(r, a, x; β*)]
=E[Dv (r, a, x; β*) ∙ S(a,x; β*)>] ∙ I -1(β*) ∙ E[Dv (r, a, x; β *) ∙ S (a, x; β*)]
=E[Dv(r, a, x; β*) ∙ Π(r, a, x; β*)].	(A.14)
Note that (A.14) implies for the third term on the right-hand side of (A.12), we have
E[Dv(r, a, x; β*) ∙ Π(r,a,x; β*)] = E[∏2(r,a,x; β*)] =Var(Π(r,x,a; β*)),	(A.15)
where the second equality follows from (A.10). Plugging (A.15) into (A.12), we obtain
E ((V — V) — 1 XX Π(ri,ai,xi; β*))	= E[(V - V )2] — 1/n ∙ Var(∏(r,x,a; β*)).
i=1
(A.16)
Term (ii) in (A.8): By Condition 3.8 and the definition of η(t?) in (A.4), we have
E[η2 (t?) I E(t?)] = O((ZDG ∙ tβ ∙ (ZST ∙ tβ + tSH) + tβ YtDH + ZDH + tβ ∙ tDG))2)
=O((ZDG ∙ (ZST + 1) ∙ t2 + t2 ∙ (t2 + t + ZDH))2),	(A.17)
where t = ∣∣t?k∞. As defined in Condition 3.8, f∞ tk df (t) is a decreasing function of k. Therefore,
from (A.17) we have
∞
t4 df (t) .	(A.18)
E[E[η2(t*)∣E(t?)]i =O((ZDG ∙(Zst + 1)+ Zdh)2 •/
Term (iii) in (A.8): By the law of total expectation, we have
E ((V — V) — ； XX∏(ri,ai,xi; β*)) ∙ η(t?)
i=1
E[(V — V) ∙ n(t?)] — EE
(1 X∏(ri,ai,xi; β*)) ∙ η(t?)
i=1
E(t?)
(A.19)
For the first term on the right-hand side of (A.19), by the Cauchy-Schwartz inequality for expectation,
we have
叫(V — V) ∙ η(t?)] ≤ (e[(V — V )2] ∙ E[E[η2(t*)∣ E (t?)]。"	(A.20)
=(E[(V - V)2])	∙ o((zDG ∙ (ZST + 1) + ZDH) ∙ (/ t4 df (t))),
where the second equality follows from (A.18). Now we consider the second term on the right-hand
side of (A.19). Note that by the definition of Π(r, a, x; β) in (A.3), we have
1n
-V"∏(ri,ai,xi; β*)
n
i=1
1n
―£E[DV(r, a,x; β*) ∙ S(a,x; β*)>] ∙ I-1(β*) ∙ S(ai,xi; β*)
i=1
≤ ∣∣E[Dv(r,a,x; β*) ∙ S(a,x; β*)>] ∣∣ ∙ ∣∣L(β*)k ∙
1n
-V"S(ai,xi; β )
n
i=1
(A.21)
where by (A.23) we have that, conditioning on the event E(t?) defined in (3.12),
1n	∣
-Vs(ai,xi; β*) ≤ tsG.
n∣
i=1
14
Published as a conference paper at ICLR 2019
Under Assumption 3.4, we prove in Lemma B.1 and (B.2) that
∣∣E[Dv (r,a,x; β*) ∙ S(a,x; β* )>] ∣∣ = EdDV(Tdex^ H=O(ZDG)∙	(A.22)
Note that in the event Esg(tsg) defined in (3.9), We have E[S(a, x; β*)] = 0. Hence, equivalently
we have
EsG(tsG)= {||n XS(ai,xi；β*) ≤ tsG j.	(A.23)
Combining (A.4), (A.21), (A.22), (A.23), and Assumption 3.3, under Condition 3.8 We have
E (1 X ∏(ri,ai,xi; β*))∙ η(t?) E(t?)
i=1
=O(ZdG ∙ tsG ∙	(Zdg	∙ tβ ∙	(Zsτ ∙ tβ + tsH) + t2β ∙ (tβ ∙ tDG	+ tDH +	ZDH)))
=O(ZDG ∙ (ZST	+ I) ∙	t3 +	ζDG ∙ t3 ∙ (t2 + t + ZDH力),	(A∙24)
where t = ∣∣t?k∞. By (A.24), using a similar argument to the one used to obtain (A.18), we have
EE (1 X ∏(ri,ai,xi; β *))f(t*) E (t?)
• (ZST + I) + ZDG ∙ ZDH) ∙ / t3 df (t)) ∙
(A.25)
Thus, combining (A.20) and (A.25), for (A.19) we obtain
E
1n
- V) - n Eπ(ri,ai,χi; β)) • η(t?)
i=1
= (E[(Ve - V)21/2 •O
• (ZST +1) + ZDH) •	t4 df (t)
+O
r
• (ZST + 1) + ZDG • ZDH •
0
0
∞ t3 df(t).
(A.26)
∞
Note that term (ii) in (A.8), which is characterized by (A.18), is dominated by the last term on the
right-hand side of (A.25), since 0∞ tk df (t) decreases along k under Condition 3.8. Plugging (A.16),
(A.18), and (A.26) into (A.8), we obtain
E[(V - V)2] = E[(V - V)2] - 1/n • Var (Π(r, x, a; β*))
+ (E[(V - V产])∙O((ZDG •(《ST + 1) + ZDH)
t4 df (t)
+O
• (ZST + 1) + ZDG •
ZDH) •	t3 df (t) ,
which concludes the proof of Theorem 3.9.
□
B Proof of Lemma A.1
Before proving the Lemma A.1, we first introduce the following lemma on the gradient of the
deviation function DV (r, a, x; β).
Lemma B.1. For the score and deviation functions defined in (3.3) and (3.4), we have
E dDV (r∂β,x; β) = -E [dv (r,a,x; β *) • S (a,x； β*)]∙
Proof. By Definition 3.2, we have
∂DV (r, a, x; β*)
而
π(a, x) ∙ r(a, x) ∂μ(a, x; β*)
一E -----7T^-----：~：   ------：------
μ2(a,x; β*)	∂β
(B.1)
15
Published as a conference paper at ICLR 2019
Then using the fact that E[S(a, x; β*)] = 0, we have
-E [Dv (r, a, x; β*) ∙ S (a, x; β *)] = E
V - μ(aU*) , r(a,x)) , S(a, x; β*)
-E
π(a, x) ∙ r(a, x) ∂μ(a, x; β*)
μ2(a, x; β*)
∂β
+ V ∙ E[S(a,x;β*)] = E
∂Dv (r, a, x; β*)
∂β
where the second equality follows from (3.3) and the last equality follows from (B.1). Hence, we
conclude the proof of Lemma B.1.
Note that by Lemma B.1 and Assumption 3.4, we also have the following equality,
∣∣E[Dv(r, a,x; β*) • S(a,x; β*)]∣∣ =O(ZDG).
Now we proceed to prove Lemma A.1.
(B.2)
Proof of Lemma A.1. Recall that we have
Dv (r, a, x;β)=μ∏aaxxβ) , r(a, x)—V,
by which we have
nn
V - V = - ^X DV(ri, ai, xi; β*), V - V = - ^X DV(ri
n i=1	n i=1
, ai, xi; β).
(B.3)
Then by applying the Taylor expansion to the second equality in (B.3), we have
1n
V - V = n y^Dv(ri,ai,xi; β ) +
1n
1X
n i=1
∂Dv(ri,ai,xi; β*)"
∂β
>
,^ ..
• (β - β *)
∂2DV(ri, ai, xi; βD)
∂β2
,^ ,,
∙(β - β*)
Ve
1n
-V + n X
∂Dv(ri, ai,xi； β*)"
i=1
、^
∂β
>
, (e - β*) + O(tβ • (tDH + ZDH)),
(B.4)
where βD = λ□β* + (1 - Xd)β for some Xd ∈ [0,1], and the last equality follows from Assumption
3.4 and the fact that we condition on the event E(t?) defined in (3.12). By Lemma B.1, for the second
term on the right-hand side of (B.4), we have
1n
1X
n i=1
∂Dv(ri, ai,xi； β*)"
∂β
>
,^ ..
∙(β - β*)
□
n
-E[Dv (r, a, x； β*) ∙ S (a, x； β*)>] ∙ (b - β*) + δ>(b - β *),
(B.5)
where
1n
δD = - X
n
i=1
∂Dv (ri,ai,xi; β*)
∂β
-E
∂Dv (r, a, x; β*)
_ ∂β _
Meanwhile, conditioning on the event E(t?) defined in (3.12), we have
δ> (尸-β" ) = O(tβ ∙ tDG ).
Combining (B.6), (B.5), and (B.4), we have
V - V =(V - V) - E[Dv(r, a, x; β*) • S(a, x; β*)>] • G - β*)
+。(t万• tDG + tβ • (tDH + Zdh)),
which concludes the proof of Lemma A.1.
(B.6)
□
16
Published as a conference paper at ICLR 2019
C Proof of Lemma A.2
Proof. First, recall that we have
S(a,x; β) = d lθg μ犷 β).
Since β is the maximum likelihood estimator of the true parameter β* based on {(ai, x%)}n=ι, we
have
n
1
—S , S(ai, xi; β) = 0.
n
i=1
On the other hand, we expand the above sum to obtain
(C.1)
n
1
一 > S(ai,xi; β)
n
i=1
1n	1n
nXS(a,x;β) + nX
i=1
1n
+ 2n X
i=1
i=1
∂2S(a, x; βS )
∂β2
dS(a,x;β*) (b_ β*)
一∂β	(β - β )
,^ ^
(β - β*,β - β*,∙),
(C.2)
一 一 一 一. ,. ..^ _ _ ._ 一 一 一 一 一 . . _ . .. _.
where βs = λsβ* + (1 一 λs)∕b for some λs ∈ [0,1]. Then by the definition of ∆s in (A.2), we
combine (C.1) and (C.2) to obtain
1n
0=-ES(a%,%; β*)+ E
n i=1
∂S(a, x; β*)
∂β
∙(β - β*)- ∆s(β - β*)
^
^
1n
+ 2n X
i=1
∂2S(ai, xi; βS )
∂β2
,^	^
(β - β*,β - β*,∙).
(C.3)
Since we have
E
∂S(a, x; β*)
∂β
Which is invertible by Assumption 3.3, by rearranging the terms in (C.3) We obtain
^
β - β*
n
1n
I(W ∙ n XS(
ai,xi; β ) 一 ∆s(b
i=1
which concludes the proof of Lemma A.2.
1n
2n X
i=1
∂2S(ai, xi; βS)
∂β2
(β - β*,β - β*, ∙),
^
^
□
D Application to Multinomial Logistic Regression
In this section, we consider the setting where the logging distribution μ(a | x; β) is parametrized by
the multinomial logistic regression model. We assume that x ∈ Rp and a ∈ [m]. Let
β= (β1>,...,βm>-1,0>)>, where β1,...,βm-1 ∈Rp.
Thus, the dimension of the parameter β is d = p(m 一 1). The logging distribution μ(a | x; β) is
parametrized by
μ(a | x; β) = —eΧpx⅛—
Pm-IeXP(X>βι) + 1
(D.1)
We denote by λmaχ(∙) and λmin(∙) the largest and the smallest eigenvalue of a matrix, respectively.
Throughout this section, we assume that the following regularity condition holds.
Assumption D.1. We assume that, for all i ∈ [m - 1], βi is bounded by ∣βi∣ ≤ B. Also, We assume
that r(a, x) is bounded for all a and x. Furthermore, recall that in (2.1) we have X 〜V, we assume
that X is bounded by ∣∣χk ≤ X and for its covariance matrix Σ = Ex〜V [xx>], We have
λmaχ(∑) = σ, λmin(Σ) = σ > 0.
(D.2)
17
Published as a conference paper at ICLR 2019
Then the score function S(a, x; β) ∈ Rp(m-1) takes the form
f
S (a, x; β)=
exp(x>β1)
Pm-IeXp(x>βι) + 1
exp(x>β2)
Pm-IeXp(x>βι) + 1
•	X — 1(a = 1) ∙ X
•	x — 1(a = 2) • x
(D.3)
∖
eχp(X>βmτ)	• x - 1(a = m - 1) • X
∖Pm-1exp(x>βι) + 1	/
where 1(∙) is the indicator function, while the Fisher information matrix I(β) ∈ Rρ(m-1)×ρ(m-1) is
a block matrix whose (j, k)-th block, namely [I(β)]j,k, takes the form
[I (β)]j,k = E
1(j=k)-
exp(x>βj )
Pm-I exp(x>βι) + 1
exp(x>βk)	>
__________________ . γγ
Pm=-1 exp(x> βι ) + 1
(D.4)
for j, k ∈ [m - 1]. We give lower and upper bounds for eigenvalues of I(β*) in the following lemma.
Lemma D.2 (Non-singular Fisher Information Matrix). Under Assumption D.1, the Fisher informa-
tion matrix I(β) is non-singular. Furthermore, for the eigenvalues of I(β), we have
2e-4XB σ	4e4XB σ
λmin(I (β)) ≥ ----L,	λmaχ(I (β)) ≤ ———.	(D.5)
m2	m2
Proof. For the ease of notation, We let ∏ = μ(i | x, β) for i ∈ [m — 1], πm = 1 - Pm-I ∏i and
v = (v1, . . . , vm-1)> with vi ∈ Rp. First, for x and βi, i ∈ [m - 1] satisfying Assumption D.1, we
have
e-XB	e-2XB	eXB	e2XB
πi ≥ (m - 1)eXB + 1 ≥ m, πi ≤ (m - 1)e-XB + 1 ≤ m
Then, by (D.4), we have
v>I(β)v =	X	vj> [I(β)]j,kvk
1≤j,k≤m-1
m-1	m-1
E X πj (vj>x)2 + X πj2(vj>x)2 - X (πjvj>x)(πkvk>x)
j =1	j =1	1≤j,k≤m-1
Note that Pkm=1 πk = 1, we have
m-1
X πj (vj>x)2 -
j =1
1≤j,k≤m-1
m-1
=	πjπk(vj>x)2 + πm	πj(vj>x)2 -	(πjvj>x)(πkvk>x)
1≤j,k≤m-1	j =1	1≤j,k≤m-1
1	m-1
=2 E	(∏jv/x - ∏kv>x)2 + ∏m £ ∏j(v>x)2.
1≤j,k≤m-1	j =1
Thus, we have
λmin(l(β)) = min v>I(β)v
kvk=1
(D.6)
(D.7)
(D.8)
1	m-1	m-1
IminIE 2 E	(πjv>x-πkv>x)2+Tmf ∏(v>x)2 + Eπ2(v>x)2
1≤j,k≤m-1	j =1	j =1
2e
≥
-4XB
m2
min E
kvk=1
m-1
j =1
2e-4XB
L λmin(∑)
m2
2e-4XBσ
m2
(D.9)
where the last two equalities are consequences of (D.6) and the Assumption D.1, respectively. This is
saying that the Fisher information matrix I(β) is positive definite. For the largest eigenvalue of I(β),
18
Published as a conference paper at ICLR 2019
we have
λmax I(β) = max E
kvk=1
1	m-1	m-1
2 E	(πj v>x - πk v> X)2 + Tmf πj (V>X)2 + Eπ2 (V> X)2
1≤j,k≤m-1	j=1	j=1
4e4XB
m2
max E
kvk=1
m-1
X(Vj>X)2
j=1
4e4XB σ
m2
which, together with (D.9), concludes the proof.
□
Meanwhile, the deviation function DV (r, a, x; β ) takes the form
DV (r, a, x; β)
∏(a | x) ∙ r(a, x) ∙ (Pm-I exp(x>βι) + 1)
exp(x>βa)
- V.
(D.10)
The gradient of the deviation function takes the form
/
exp(x>βι) - 1(a = 1) ∙ (Pl exp(x>βι) + 1)	∖
∂DV (r, a, x; β)
—∂β—=π(a |X) ∙ r(a,X) •
exp(x>βa)
exp(x>β2) - 1(a = 2) ∙ (Pl exp(x>βl) + 1)
exp(x>βa)
• x
• X
exp(x>βm-1) - 1(a = m - 1) • Pl exp(x>βl) + 1
-- X
exp(x>βa)
(D.11)
The Hessian of the deviation function is a block matrix in Rp(m-1)×p(m-1), whose (j, k)-th block
takes the form
∂2DV(r,a,x; β)
∂β2
xx> • π(a | x) • r (a, x)
(D.12)
j,k
(l(k = j) — 1(k = a))	1(k
∙ —-----------:...— H———
exp x> (βa - βj )
for j, k ∈ [m - 1].
a = j) . (Pi exp(x>βι) + 1)- 1(a = j) . exp(x>βk)"
exp(x>βa)
Note that by (D.3)-(D.4) and (D.11) -(D.12) for the population quantities, which do not scale with n,
in Assumptions 3.3-3.4 are all bounded under Assumption D.1. Hence, in the multinomial logistic
regression model defined in (D.1) with bounded x and β as stated in Assumption D.1, when d is
fixed, we have
ζDG = ζDH = ζST = 1, and t ≤ M,	(D.13)
where t is defined in Condition 3.12 and M is some constant that does not scale with n.
The subsequent lemmas establish the tail behaviors for the events defined in Definitions 3.5-3.7,
which together yield a concrete form of the tail function f(t) defined in Condition 3.8.
Lemma D.3 (Maximum Likelihood Estimation Error). Under the Assumption D.1, for the event
Eβ (tβ) defined in (3.8), we have
P(Ee(tβ)) ≥ 1 — Ce ∙ exp(d — ntIe),
where Cβ is a positive constant independent of n.
19
Published as a conference paper at ICLR 2019
n
Proof. Let 'n(β) be the negative log-likelihood function 'n(β) = -(1/n) ∙ Ei=I logμ(ai, Xi； β),
where μ(a,x; β) is defined in (D.1). ByLemmaD.2, we have λmin ≥ 2e-4XBσ∕m2, which gives
e-4XB σ
≥P(∣λmin"2'n(β*)) - λmin(I(β*))I ≤ Mm ∙tβj
≥PQv%(β*) - I(β*)∣∣ ≤ eMmσ ∙tβ),	(D.14)
where M is defined in (D.13). Then note that ∣∣∂2 log μ(ai, Xi； β*)∕∂β2 — I(β*) ∣∣ ≤ 8e4XBσ∕m2
and III(β*)2k ≤ X2e4XBσm, by matrix Bernstein inequality (see, for example, Tropp (2012);
Vershynin (2010)), we have
P(∣∣v%(β*)-I(β*)∣∣≤ eMm=
(	—ntβ	、
≤" exp((6χ2e4XBMm3 + 16σtβ)Mσm2∕3σ2)
≤ Cβ0 exp(d — nt2β),	(D.15)
where the second inequality holds for a constant Cβ0 when n sufficiently large. Combining (D.14) and
(D.15), we know that 'n(β) is e-4XBσ∕m2-strongly convex with at least probability 1 — Ce exp(d —
nt2β). The rest of the proof considers the case where the e-4XBσ∕m2-strong convexity holds for
'n(β), which means that we have
e-4XBσ
bn(β) ≥ 'nC ) Ke U-*) +	∙kβ - β*k2∙
(D.16)
一 _ . . _ ^ . . . ^ , 一 .一 ^ , . . ^ ,^.	一 _ 一
By definition of MLE, β is the mmimizer for 'n(β), which means 'n(β*) ≥ 'n(β). Therefore, by
Cauchy-Schwarz inequality, we have
e-4XBσ
-^m= ∙kβ — β*k2 ≤ Ve `n (β*)>(β* — β) ≤ kVe 'n(β*)k∙kβ — β *k,
which gives
e-4XB σ
P(kb — β*k ≥ te) ≤ PHIVeb√β*)k ≥ -Im= ∙te}	(d.17)
Note that	E[S(a,x;	β*)]	= 0,	E[S(a,x; β*)S(a,	x; β )>] =	I(β*) and	Vein|(β*)	=	(1/n)	∙
pn=ι S(ai, Xi； β*), by Bernstein,s inequality (see, for example, Tropp (2012)), we have
--4XBσ
P(w>ve'n(β*) ≥	∙ t
≤ exp ([8e8xB σ∕m + 2X (1 + e
for any w such that Iw I = 1, where the second inequality holds for a constant Ce00 when n is large
enough. Let Gd = {w ∈ Rd : IwI = 1}, by taking a union bound over w ∈ Gd in (D.18), we have
P(IlVeb√β")k ≥ e 2m2 σ ∙ te) =P bUp w>Veb√β") ≥ t) ≤ Ce exp(d — nt2e). (D.19)
Plugging (D.19) into (D.17) and recall that (D.16) holds with at least a probability of 1 — Ce0 exp(d —
nt2e), we have
P(Ee(te)) ≤ (1 — Ce eχp(d — nte)) ∙ (1 — Ce exp(d — nte)) ≤ 1 -(Ce + C00) exp(d — nte),
which concludes the proof with Ce = Ce + C%	□
Next, we state two Lemmas on the concentration of score and deviation. The proofs for them are
quite standard in matrix concentration theories.
20
Published as a conference paper at ICLR 2019
Lemma D.4 (Concentration of Score). Under the Assumption D.1, for the logistic regression model
in (D.1), with the event ESG(tSG), ESH(tSH) and EST(tST) defined in Definition 3.6, we have
P(EsG(tsG)) ≥ 1 - Csg ∙ exp(d - ntQ,	P(ESH(tsh)) ≥ 1 - Csh ∙ exp(d - 加Sh)
and
P(EST(tST)) ≥ 1 - Csg ∙ eχp(d - ntST),
where CSG, CSH, and CST are positive constants independent of n.
Proof. Following the arguments made in (D.15) and (D.19), the exponentially decaying tails are
immediate.	□
Lemma D.5 (Concentration of Deviation). Suppose the Assumption D.1 holds and r(a, x) ≤ R for
all a and x. Then, for the logistic regression model (D.1), with the event EDG(tDG) and EDH(tDH)
defined in 3.7, we have
P(EDG(tDG)) ≥ 1 - Cdg ∙ exp(d - ntDα), P(Edh(tdh)) ≥ 1 - CDH ∙ exp(d - 加Dh),
(D.20)
where CDG and CDH are positive constants independent of n.
Proof. Recall that we have πi ≥ e-2XB/m when the Assumption D.1 holds. We have
dDV(L； β ≤ RX [(m - 2)e2XB + me2XB] = 2e2XBRX(m - 1),
∂β
and, by similar arguments in the proof of Lemma D.2, we have
E PDV(r,a,x; β) dDv(r,a,x; β)1 Il R2( m V	4XB R2	2
E [ ∂β	∂β>	Jll ≤R Ie-XB)	= R
Then, the first inequality in (D.20) follows by similar arguments as in (D.17)-(D.18). Next, again by
similar arguments in the proof of Lemma D.2, we also have
∣e[ d 2DV ∂β a,x β) ]| ≤ e2XB Rσ(m +1)
and
|e[( d2DV " a，x； β) )2]| ≤ e2XB R2χ2σ m(m + 1),
Then, similar as (D.15), the second inequality in (D.20) follows by matrix Bernstein inequality. □
Recall the event E(t?) with t? = ktk∞ is defined in Condition 3.8. By Lemmas D.3-D.5, the tail
function f (t) takes the form
f (t) ≤ 6C ∙ exp(d — nt2),	(D.21)
where C = max{Cβ, CSG , CSH , CDG, CDH, CDT} > 0.
In the sequel, we use (D.13) and the tail function in (D.21) to characterize the reduction of the mean
squared error induced by the multinomial logistic regression model.
First, let to = (T + 1) ,d/n with some constant T > 0, we have the following bound on the tail
probability of t based on (D.21),
P(t > to) ≤ f (to) ≤ 6C ∙ e-Td.	(D.22)
Then recall that in Theorem 3.9 we have
ξ(n) =(E [(V - V)2])1/2 ∙ O ((∕∞ t4 df (t)
where by (D.22), we have
∞
t3 df (t) ,
Zt013 df (t) = E[t3 11 ≤ to] ≤ (T +1)3 ∙ (d∕n)3/2 = O((d∕n)3/2),
o
(D.23)
21
Published as a conference paper at ICLR 2019
and
M
t	t3 df (t)	=	E[t3	11 >	t0]	≤ P(t > t0)	∙	M3	≤ 6C ∙ exp(-Td)	∙ M3	=	O(e-Td),	(D.24)
t0
which decays exponentially with d and is therefore dominated by the bound given in (D.23). Hence,
we have
Z t3 df (t) = Z t3df(t)+ Z t3 df (t) = O((d∕n)3∕2).
0	0	t0
Using the same arguments in derivation of (D.25), we also have
(/ ∞t4 df(t)[2 = O(d∕n).
(D.25)
Hence, we obtain
ξ(n) =(E[(V - V)2])1/2 ∙ O(d∕n) + O((d∕n)3/2).
Note that the mean squared error term E[(V - V)2] is of the order 1∕n, which implies that ξ(n) is of
the order (d∕n)3/2. Thus, (3.15) is satisfied when n is sufficiently large, which means that a reduction
of the order 1∕n on the mean squared error is achieved by the multinomial logistic regression model
when the sample size n is sufficiently large. This result implies that the MLIPS in (3.1) is guaranteed
to out perform the IPS estimator in (2.2) when the sample size n is much larger than the dimension d
of the problem. Moreover, since the mean squared error term E[(Ve - V)2] itself is of the order 1∕n,
the effect of reduction does not vanish when n → ∞.
We interpret Theorem 3.9 in a more general way. The ξ(n) term is in general a decreasing function of
the sample size n and is mostly of a higher order than 1∕n in common parametric statistical models.
Correspondingly, for a sufficiently large n, we obtain (3.15), which implies that our method leads to
a 1∕(2n) ∙ Var(Π(r, x, a; β*)) reduction on the mean squared error, which has a non-vanishing effect
when n → ∞.
E Complimentary Results for Section 4.3
We provide the following extra results in Table 2 for the experiment in Section 4.3. The additional
results of standard deviation of the Hamming losses of the policies optimized by using IPS, POEM
and their ML-variants are computed over ten runs. From the Table 2, we can see that our ML-based
approach induces much smaller standard deviations than the improvements made on performances
and, thus, it consistently boosts the performance of IPS and POEM in policy optimization.
Table 2: Hamming losses and standard deviations (‘Std’ columns in the table) for the performances
(evaluated by Hamming loss) of IPS, POEM and their ML-variants on the datasets of Swaminathan &
Joachims (2015a). Here, 10-k is denoted by e-k.
Dataset/Alg.	Scene	(Std)	Yeast	(Std)	TMC	(Std)	LYRL	(Std)
IPS	1.342	(3.14e-1)	4.571	(1.40e-1)	3.023	(N/A)	1.108	(9.18e-2)
POEM	1.143	(2.91e-4)	4.549	(1.05e-1)	2.522	(N/A)	0.981	(3.31e-2)
MLIPS-Lin	1.086	(6.26e-6)	3.778	(9.98e-2)	2.018	(6.23e-3)	1.025	(5.92e-2)
MLIPS-NN	1.086	(4.88e-6)	3.630	(9.10e-2)	2.019	(1.22e-3)	0.930	(2.08e-2)
MLPOEM-Lin	1.086	(3.48e-6)	3.894	(3.16e-1)	2.010	(7.96e-3)	0.949	(4.21e-2)
MLPOEM-NN	1.086	(5.81e-6)	3.477	(2.03e-1)	2.000	(1.22e-2)	0.904	(2.46e-2)
22