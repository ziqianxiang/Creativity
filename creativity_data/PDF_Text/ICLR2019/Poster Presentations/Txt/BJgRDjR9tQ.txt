Published as a conference paper at ICLR 2019
Robust Estimation and Generative Adversarial
Networks
Chao Gao
Department of Statistics
University of Chicago
Chicago, IL 60637 USA
chaogao@galton.uchicago.edu
Jiyi Liu
Department of Statistics and Data Science
Yale University
New Haven, CT 06511 USA
jiyi.liu@yale.edu
Yuan Yao & Weizhi Zhu
Department of Mathematics
Hong Kong University of Science and Technology
Kowloon, Hong Kong
yuany@ust.hk; wzhuai@connect.ust.hk
Ab stract
Robust estimation under Huber’s -contamination model has become an important topic in
statistics and theoretical computer science. Statistically optimal procedures such as Tukey’s
median and other estimators based on depth functions are impractical because of their compu-
tational intractability. In this paper, we establish an intriguing connection between f -GANs
and various depth functions through the lens of f -Learning. Similar to the derivation of f-
GANs, we show that these depth functions that lead to statistically optimal robust estimators
can all be viewed as variational lower bounds of the total variation distance in the frame-
work of f -Learning. This connection opens the door of computing robust estimators using
tools developed for training GANs. In particular, we show in both theory and experiments
that some appropriate structures of discriminator networks with hidden layers in GANs lead
to statistically optimal robust location estimators for both Gaussian distribution and general
elliptical distributions where first moment may not exist.
1	Introduction
In the setting of Huber’s -contamination model (Huber, 1964; 1965), one has i.i.d observations
χι,…,Xn 〜(I - e)Pθ + eQ,
(1)
and the goal is to estimate the model parameter θ. Under the data generating process (1), each observation
has a 1 - e probability to be drawn from Pθ and the other e probability to be drawn from the contamination
distribution Q. The presence of an unknown contamination distribution poses both statistical and computational
challenges to the problem. For example, consider a normal mean estimation problem with Pθ = N(θ, Ip). Due
to the contamination of data, the sample average, which is optimal when e = 0, can be arbitrarily far away
from the true mean if Q charges a positive probability at infinity. Moreover, even robust estimators such as
coordinatewise median and geometric median are proved to be suboptimal under the setting of (1) (Chen et al.,
2018; Diakonikolas et al., 2016a; Lai et al., 2016). The search for both statistically optimal and computationally
feasible procedures has become a fundamental problem in areas including statistics and computer science.
For the normal mean estimation problem, it has been shown in Chen et al. (2018) that the minimax rate with
respect to the squared '2 loss is P ∨ e2, and is achieved by Tukey's median (TUkey, 1975). Despite the statistical
optimality of Tukey’s median, its computation is not tractable. In fact, even an approximate algorithm takes
O(eCp) in time (Amenta et al., 2000; Chan, 2004; Rousseeuw & Struyf, 1998).
1
Published as a conference paper at ICLR 2019
Recent developments in theoretical computer science are focused on the search of computationally tractable
algorithms for estimating θ under Huber’s -contamination model (1). The success of the efforts started from
two fundamental papers Diakonikolas et al. (2016a); Lai et al. (2016), where two different but related compu-
tational strategies “iterative filtering” and “dimension halving” were proposed to robustly estimate the normal
mean. These algorithms can Provably achieve the minimax rate P ∨ e2 up to a poly-logarithmic factor in Poly-
nomial time. The main idea behind the two methods is a critical fact that a good robust moment estimator
can be certified efficiently by higher moments. This idea was later further extended (Diakonikolas et al., 2017;
Du et al., 2017; Diakonikolas et al., 2016b; 2018a;c;b; Kothari et al., 2018) to develop robust and computable
procedures for various other problems.
However, many of the computationally feasible procedures for robust mean estimation in the literature rely
on the knowledge of covariance matrix and sometimes the knowledge of contamination proportion. Even
though these assumptions can be relaxed, nontrivial modifications of the algorithms are required for such
extensions and statistical error rates may also be affected. Compared with these computationally feasible
procedures proposed in the recent literature for robust estimation, Tukey’s median (9) and other depth-based
estimators (RoUsseeUW & Hubert, 1999; Mizera, 2002; Zhang, 2002; Mizera & Muller, 2004; Paindaveine
& Van Bever, 2017) have some indispensable advantages in terms of their statistical properties. First, the
depth-based estimators have clear objective functions that can be interpreted from the perspective of projection
pursuit (Mizera, 2002). Second, the depth-based procedures are adaptive to unknoWn nuisance parameters
in the models such as covariance structures, contamination proportion, and error distributions (Chen et al.,
2018; Gao, 2017). Last but not least, Tukey’s depth and other depth functions are mostly designed for robust
quantile estimation, While the recent advancements in the theoretical computer science literature are all focused
on robust moments estimation. Although this is not an issue When it comes to normal mean estimation, the
difference is fundamental for robust estimation under general settings such as elliptical distributions Where
moments do not necessarily exist.
Given the desirable statistical properties discussed above, this paper is focused on the development of compu-
tational strategies of depth-like procedures. Our key observation is that robust estimators that are maximizers
of depth functions, including halfspace depth, regression depth and covariance matrix depth, can all be derived
under the frameWork off-GAN (NoWozin et al., 2016). As a result, these depth-based estimators can be vieWed
as minimizers of variational loWer bounds of the total variation distance betWeen the empirical measure and the
model distribution (Proposition 2.1). This observation alloWs us to leverage the recent developments in the deep
learning literature to compute these variational loWer bounds through neural netWork approximations. Our the-
oretical results give insights on hoW to choose appropriate neural netWork classes that lead to minimax optimal
robust estimation under Huber’s e-contamination model. In particular, Theorem 3.1 and 3.2 characterize the
netWorks Which can robustly estimate the Gaussian mean by TV-GAN and JS-GAN, respectively; Theorem 4.1
is an extension to robust location estimation under the class of elliptical distributions Which includes Cauchy
distribution Whose mean does not exist. Numerical experiments in Section 5 are provided to shoW the success
of these GANs.
2	ROBUST ESTIMATION AND f-GAN
We start with the definition of f-divergence (Csiszar, 1964; Ali & Silvey, 1966). Given a strictly convex
function f that satisfies f(1) = 0, the f -GAN betWeen tWo probability distributions P and Q is defined by
Df(P kQ) = ∕f(p 'Q.	⑵
Here, we use p(∙) and q(∙) to stand for the density functions of P and Q with respect to some common domi-
nating measure. For a fully rigorous definition, see Polyanskiy & Wu (2017). Let f * be the convex conjugate
of f. That is, f * (t) = suPu∈dom于(Ut - f (u)). A variational lower bound of (2) is
Df (P kQ) ≥ sup [EPT(X) - EQf*(T(X))].	(3)
T∈T
Note that the inequality (3) holds for any class T, and it becomes an equality whenever the class T contains
the function f0 (p/q ) (Nguyen et al., 2010). For notational simplicity, we also use f0 for an arbitrary element of
2
Published as a conference paper at ICLR 2019
the subdifferential when the derivative does not exist. With i.i.d. observations Xi,…，Xn 〜P, the variational
lower bound (3) naturally leads to the following learning method
P = argmin sup
Q∈Q T∈T
1n
n ET(Xi)- EQf *(T(X))
n i=1
(4)
The formula (4) is a powerful and general way to learn the distribution P from its i.i.d. observations. It is
known as f -GAN (Nowozin et al., 2016), an extension of GAN (Goodfellow et al., 2014), which stands for
generative adversarial networks. The idea is to find a Pb so that the best discriminator T in the class T cannot
tell the difference between P and the empirical distribution ɪ Pn=i δχi.
2.1	f-LEARNING: A UNIFIED FRAMEWORK
Our f -Learning framework is based on a special case of the variational lower bound (3). That is,
Df (P kQ) ≥ sup
Qe∈QeQ
(5)
where e(∙) stands for the density function of Q. Note that here We allow the class QQ to depend on the
distribution Q in the second argument of Df (P kQ). Compare (5) with (3), and it is easy to realize that (5) is a
special case of (3) with
T = Tq =卜 0( e : e ∈Qq}.	(6)
Moreover, the inequality (5) becomes an equality as long as P ∈ QQ . The sample version of (5) leads to the
following learning method
Pb = argmin sup
Q∈Q Qe∈QeQ
1X f (q(X) ʌ
n = f Iq(Xi)J
(7)
The learning method (7) will be referred to as f -Learning in the sequel. It is a very general framework that
covers many important learning procedures as special cases. For example, consider the special case where
QeQ = Qe independent of Q, Q = Qe, and f(x) = x log x. Direct calculations give f0 (x) = logx + 1 and
f *(t) = et-i. Therefore, (7) becomes
P = argmin sup 1 XX log 监
Q∈Q Qe∈Q n i=1	q(Xi)
argmax
q∈Q
1n
-£log q(Xi),
n i=1
which is the maximum likelihood estimator (MLE).
2.2	TV-Learning and Depth-Based Estimators
An important generator f that we will discuss here is f(x) = (x -1)+. This leads to the total variation distance
Df(P∣∣Q) = 2 R |p 一 q|. With f0(x) = I{x ≥ 1} and f *(t) = tI{0 ≤ t ≤ 1}, the TV-Learning is given by
P = argmin sup
Q∈Q Qe∈QQ
1n
1 XI
n
i=1
{* ≥ 1}
(8)
A closely related idea was previously explored by Yatracos (1985); Devroye & Lugosi (2012). The following
proposition shows that when QQ approaches to Q in some neighborhood, TV-Learning leads to robust estima-
tors that are defined as the maximizers of various depth functions including Tukey’s depth, regression depth,
and covariance depth.
Proposition 2.1. The TV-Learning (8) includes the following special cases:
3
Published as a conference paper at ICLR 2019
1.	Tukey’s halfspace depth: Take Q = {N (η, Ip) : η ∈ Rp} and Qη = {N (ηe, Ip) : kηe - ηk ≤ r}. As
r → 0, (8) becomes
1n
θ = argmax inf — Y^I{uT(Xi 一 η) ≥ 0}.	(9)
η∈Rp kuk=1 n i=1
2.	Regression depth: Take Q = {Py,χ = Py∣χPχ : Py∖χ = N(XTη, 1), η ∈ Rp},
and Qn = {Py,χ = PyXPχ : Py∣χ = N(XTη, 1), ∣Ie 一 ηk ≤ r}. As r → 0,⑹ becomes
1n
θb = argmax inf -y^I{uTXi(yi - XTη) ≥ 0}.	(10)
η∈Rp kuk=1 n i=1
3.
Covariance matrix depth: Take Q = {N (0, Γ) : Γ ∈ Ep}, where Ep stands for the class of p × p
, ∙ -i X
covariance matrices, and QΓ
(8) becomes
N(0, Γe) : Γe-1 = Γ-1 + reuuT ∈ Ep, |re| ≤ r, IuI = 1 . Asr → 0,
Σb = argmin sup 1-XXI{∣uTXil2 ≤ uTΓu}-P(χ2 ≤ 1))	(11)
Γ∈Ep kuk=1	n i=1
∨(- XI{∣uτXil2 >uTΓu}- P(χ2> 1)).
The formula (9) is recognized as Tukey’s median, the maximizer of Tukey’s halfspace depth. A traditional
understanding of Tukey’s median is that (9) maximizes the halfspace depth (Donoho & Gasko, 1992) so that θb
is close to the centers of all one-dimensional projections of the data. In the f -Learning framework, N(θ, Ip)
is understood to be the minimizer of a variational lower bound of the total variation distance. The formula
(10) gives the estimator that maximizes the regression depth proposed by Rousseeuw & Hubert (1999). It is
worth noting that the derivation of (10) does not depend on the marginal distribution Pχ in the linear regression
model. Finally, (11) is related to the covariance matrix depth (Zhang, 2002; Chen et al., 2018; Paindaveine
& Van Bever, 2017). All of the estimators (9), (10) and (11) are proved to achieve the minimax rate for the
corresponding problems under Huber’s -contamination model (Chen et al., 2018; Gao, 2017).
2.3	FROM f-LEARNING TO f -GAN
The connection to various depth functions shows the importance of TV-Learning in robust estimation. How-
ever, it is well-known that depth-based estimators are very hard to compute (Amenta et al., 2000; van Kreveld
et al., 1999; Rousseeuw & Struyf, 1998), which limits their applications only for very low-dimensional prob-
lems. On the other hand, the general f -GAN framework (4) has been successfully applied to learn complex
distributions and images in practice (Goodfellow et al., 2014; Radford et al., 2015; Salimans et al., 2016). The
major difference that gives the computational advantage to f -GAN is its flexibility in terms of designing the
discriminator class T using neural networks compared with the pre-specified choice (6) in f -Learning. While
f -Learning provides a unified perspective in understanding various depth-based procedures in robust estima-
tion, we can step back into the more general f -GAN for its computational advantages, and to design efficient
computational strategies.
3	Robust Mean Estimation via GAN
In this section, we focus on the problem of robust mean estimation under Huber’s -contamination model. Our
goal is to reveal how the choice of the class of discriminators affects robustness and statistical optimality under
the simplest possible setting. That is, We have i.i.d. observations Xi,…,Xn 〜(1 一 e)N(θ, Ip) + eQ, and We
need to estimate the unknown location θ ∈ Rp with the contaminated data. Our goal is to achieve the minimax
rate P ∨ e2 with respect to the squared '2 loss uniformly over all θ ∈ Rp and all Q.
4
Published as a conference paper at ICLR 2019
Figure 1: Landscape of TV-GAN objective function F (η, w) = supb [EP sigmoid(wX + b) -
EN(η,1)sigmoid(wX + b)], where b is maximized out for visualization. Samples are drawn from P =
(1 - )N (1, 1) + N (10, 1) with = 0.2. Left: a surface plot of F (η, w). The solid curves are marginal
functions for fixed n's: F(1, w) (red) and F(5, W) (blue), and the dash curves are marginal functions for fixed
w’s: F(η, -10) (orange) and F(η, 10) (green). Right: a heatmap of F(η, w). It is clear that F(w) = F(η, w)
has two local maxima for a given η, achieved at w = +∞ and w = -∞. In fact, the global maximum for
F(w) has a phase transition from w = +∞ to w = -∞ as η grows. For example, the maximum is achieved at
w = +∞ when η = 1 (blue solid) and is achieved at w = -∞ when η = 5 (red solid). Unfortunately, even if
we initialize with η0 = 1 and w0 > 0, gradient ascents on η will only increase the value of η (green dash), and
thus as long as the discriminator cannot reach the global maximizer, w will be stuck in the positive half space
{w : w > 0} and further increase the value of η.
3.1	Results for TV-GAN
We start with the total variation GAN (TV-GAN) with f (x) = (x - 1)+ in (4). For the Gaussian location
family, (4) can be written as
θ = argmin max
η∈Rp D∈D
1n
n £D(Xi) - EN(η,ip)D(X)
(12)
with T(x) = D(x) in (4). Now we need to specify the class of discriminators D to solve the classification prob-
lem between N(η, Ip) and the empirical distribution n ɪɪi δχi. One of the simplest discriminator classes is
the logistic regression,
D = {D(x) = Sigmoid(WTX + b) : W ∈ Rp, b ∈ R} .	(13)
With D(x) = sigmoid(wT x + b) = (1 + e-wT x-b)-1 in (13), the procedure (12) can be viewed as a smoothed
version of TV-Learning (8). To be specific, the sigmoid function sigmoid(WT x + b) tends to an indicator
function as kWk → ∞, which leads to a procedure very similar to (9). In fact, the class (13) is richer than the
one used in (9), and thus (12) can be understood as the minimizer of a sharper variational lower bound than
that of (9).
Theorem 3.1. Assume np + e2 ≤ C for some sufficiently small constant c > 0. With i.i.d. observations
Xi,..., Xn 〜(1 — e)N(θ, Ip) + eQ, the estimator θ defined by (12) satisfies
kb — θk2 ≤ C (P V 62),
with probability at least 1 — e-C0(p+n2) uniformly over all θ ∈ Rp and all Q. The constants C, C0 > 0 are
universal.
Though TV-GAN can achieve the minimax rate p V 62 under Huber’s contamination model, it may suffer from
optimization difficulties especially when the distributions Q and N(θ, Ip) are far away from each other, as
shown in Figure 1.
5
Published as a conference paper at ICLR 2019
3.2	Results for JS-GAN
Given the intractable optimization property of TV-GAN, we next turn to Jensen-Shannon GAN (JS-GAN) with
f (x) = X log X - (X + 1) log x++1. The estimator is defined by
*
θ = argmin max
η∈Rp D∈D
1n
—log D(Xi) + EN(η,Ip) IOg(I- D(Xi))
n i=1
+ log 4,
(14)
with T(X) = log D(X) in (4). This is exactly the original GAN (Goodfellow et al., 2014) specialized to the
normal mean estimation problem. The advantages of JS-GAN over other forms of GAN have been studied
extensively in the literature (Lucic et al., 2017; Kurach et al., 2018).
Unlike TV-GAN, our experiment results show that (14) with the logistic regression discriminator class (13) is
not robust to contamination. However, if we replace (13) by a neural network class with one or more hidden
layers, the estimator will be robust and will also work very well numerically.
To understand why and how the class of the discriminators affects the robustness property of JS-GAN, we
introduce a new concept called restricted Jensen-Shannon divergence. Let g : Rp → Rd be a function that
maps a p-dimensional observation to a d-dimensional feature space. The restricted Jensen-Shannon divergence
between two probability distributions P and Q with respect to the feature g is defined as
JSg (P, Q) = max EP log sigmoid(wT g(X)) + EQ log(1 - sigmoid(wTg(X))) + log 4.
In other words, P and Q are distinguished by a logistic regression classifier that uses the feature g(X). It is
easy to see that JSg (P, Q) is a variational lower bound of the original Jensen-Shannon divergence. The key
property of JSg(P, Q) is given by the following proposition.
Proposition 3.1. Assume W is a convex set that contains an open neighborhood of 0. Then, JSg (P, Q) = 0 if
and only if EPg(X) = EQg(X).
The proposition asserts that JSg(∙, ∙) cannot distinguish P and Q if the feature g(X) has the same expected
value under the two distributions. This generalized moment matching effect has also been studied by Liu
et al. (2017) for general f -GANs. However, the linear discriminator class considered in Liu et al. (2017) is
parameterized in a different way compared with the discriminator class here.
When we apply Proposition 3.1 to robust mean estimation, the JS-GAN is trying to match the values of
* Pn=1 g(Xi) and EN(η,ιη)g(X) for the feature g(X) used in the logistic regression classifier. This explains
what we observed in our numerical experiments. A neural net without any hidden layer is equivalent to a lo-
gistic regression with a linear feature g(X) = (XT, I)T ∈ Rp+1. Therefore, whenever η = * P*=ι Xi, We
have JSg (ɪ P*=ι δχi, N(η, Ip)) = 0, which implies that the sample mean is a global maximizer of (14). On
the other hand, a neural net with at least one hidden layers involves a nonlinear feature function g(X), which
is the key that leads to the robustness of (14).
We will show rigorously that a neural net with one hidden layer is sufficient to make (14) robust and optimal.
Consider the following class of discriminators,
D =	D(X) = sigmoid X wjσ(ujTX + bj) : X |wj | ≤ κ, uj ∈ Rp, bj ∈ R .	(15)
[	j≥1	j≥1
The class (15) consists of two-layer neural network functions. While the dimension of the input layer is p, the
dimension of the hidden layer can be arbitrary, as long as the weights have a bounded `1 norm. The nonlinear
activation function σ(∙) is allowed to take 1) indicator: σ(x) = I{x ≥ 1}, 2) sigmoid: σ(x) = 1+e-x, 3)
ramp: σ(X) = max(min(X + 1/2, 1), 0). Other bounded activation functions are also possible, but we do not
exclusively list them. The rectified linear unit (ReLU) will be studied in Appendix A.
Theorem 3.2. Consider the estimator θ defined by (14) with D specified by (15). Assume p + e2 ≤ C for
some sufficiently small constant c > 0, and set K = O (nJ+ + e). With i.i.d. observations Xi,..., Xn 〜
6
Published as a conference paper at ICLR 2019
(1 - )N (θ, Ip) + Q, we have
kb-θk2 ≤ C (n ∨ e2),
with probability at least 1 - e-C0 (p+n2) uniformly over all θ ∈ Rp and all Q. The constants C, C0 > 0 are
universal.
4 Elliptical Distributions
An advantage of Tukey’s median (9) is that it leads to optimal robust location estimation under general elliptical
distributions such as Cauchy distribution whose mean does not exist. In this section, we show that JS-GAN
shares the same property. A random vector X ∈ Rp follows an elliptical distribution if it admits a representation
X = θ+ξAU,
where U is uniformly distributed on the unit sphere {u ∈ Rp : kuk = 1} and ξ ≥ 0 is a random variable
independent of U that determines the shape of the elliptical distribution (Fang, 2017). The center and the
scatter matrix is θ and Σ = AAT .
For a unit vector v, let the density function of ξvT U be h. Note that h is independent of v because of the
symmetry of U. Then, there is a one-to-one relation between the distribution of ξ and h, and thus the triplet
(θ, Σ, h) fully parametrizes an elliptical distribution.
Note that h and Σ = AAT are not identifiable, because ξA = (cξ)(c-1A) for any c > 0. Therefore, without
loss of generality, we can restrict h to be a member of the following class
H = {h: h(t) =…h ≥0, /h = "wad =」.
This makes the parametrization (θ, Σ, h) of an elliptical distribution fully identifiable, and we use EC(θ, Σ, h)
to denote an elliptical distribution parametrized in this way.
The JS-GAN estimator is defined as
∕* a G、	^
(θ, Σ, h) = argmin max
η∈Rp,Γ∈Ep (M),g∈H D∈D
1n
lo Elog D(Xi) + EEC(η,Γ,g) Iog(I- D(X)) + log4,
n i=1
(16)
where Ep(M) is the set of all positive semi-definite matrix with spectral norm bounded by M.
Theorem 4.1. Consider the estimator θ defined above with D specified by (15). Assume M = O(1), P +e2 ≤ C
for some sufficiently small constant c > 0, and set K = O (y/+ + e). With i.i.d. observations Xi,..., Xn 〜
(1 - e)E C (θ, Σ, h) + eQ, we have
kb-θk2 ≤ C (n ∨ e2),
with probability at least 1 - e-C 0 (p+n2) uniformly over all θ ∈ Rp, Σ ∈ Ep(M) and all Q. The constants
C, C0 > 0 are universal.
Remark 4.1. The result of Theorem 4.1 also holds (and is proved) under the strong contamination model
(Diakonikolas et al., 2016a). That is, we have i.i.d. observations Xi,..., Xn 〜P for some P satisfying
TV(P, EC(θ, Σ, h)) ≤ e. See its proof in Appendix D.2.
Note that Theorem 4.1 guarantees the same convergence rate as in the Gaussian case for all elliptical distribu-
tions. This even includes multivariate Cauchy where mean does not exist. Therefore, the location estimator
(16) is fundamentally different from Diakonikolas et al. (2016a); Lai et al. (2016), which is only designed for
robust mean estimation. We will show such a difference in our numerical results.
To achieve rate-optimality for robust location estimation under general elliptical distributions, the estimator (16)
is different from (14) only in the generator class. They share the same discriminator class (15). This underlines
7
Published as a conference paper at ICLR 2019
an important principle for designing GAN estimators: the overall statistical complexity of the estimator is only
determined by the discriminator class.
The estimator (16) also outputs (Σ, h), but we do not claim any theoretical property for
This will be systematically studied in a future project.
in this paper.
5	Numerical Experiments
In this section, we give extensive numerical studies of robust mean estimation via GAN. After introducing the
implementation details in Section 5.1, we verify our theoretical results on minimax estimation with both TV-
GAN and JS-GAN in Section 5.2. Comparison with other methods on robust mean estimation in the literature
is given in Section 5.3. The effects of various network structures are studied in Section 5.4. Adaptation to
unknown covariance is studied in Section 5.5. In all these cases, we assume i.i.d. observations are drawn from
(1 - )N(0p, Ip) + Q with and Q to be specified. Finally, adaptation to elliptical distributions is studied in
Section 5.6.
5.1	Implementations
We adopt the standard algorithmic framework of f -GANs (Nowozin et al., 2016) for the implementation of JS-
GAN and TV-GAN for robust mean estimation. In particular, the generator for mean estimation is Gη (Z) =
Z + η with Z 〜 N(0p, Ip); the discriminator D is a multilayer perceptron (MLP), where each layer consisting
ofa linear map and a sigmoid activation function and the number of nodes will vary in different experiments to
be specified below. Details related to algorithms, tuning, critical hyper-parameters, structures of discriminator
networks and other training tricks for stabilization and acceleration are discussed in Appendix B.1. A PyTorch
implementation is available at https://github.com/zhuwzh/Robust-GAN-Center.
5.2	Numerical Supports for the Minimax Rates
We verify the minimax rates achieved by TV-GAN (Theorem 3.1) and JS-GAN (Theorem 3.2) via numerical
experiments. Two main scenarios We consider here are dp/n < e and ,p/n > e, where in both cases, various
types of contamination distributions Q are considered. Specifically, the choice of contamination distributions
Q includes N(μ * 1p, Ip) with μ ranges in {0.2,0.5,1, 5}, N(0.5 * 1p, Σ) and Cauchy(τ * 1p). Details of the
construction of the covariance matrix Σ is given in Appendix B.2. The distribution Cauchy(τ * 1p) is obtained
by combining p independent one-dimensional standard Cauchy with location parameter τj = 0.5.
τ--∙	An	II r? nil	∙	, ∕ι i`.	r cc	LC CCC ι	CC CL , C cc∖ /— /	∙ ι ιι
Figure 2: '2 error ∣∣θ 一 θ∣∣ against e (left: P = 100, n = 50,000 and e ranges from 0.05 to 0.20), √p (middle:
n = 1,000, e = 0.1 and P ranges from 10 to 100) and 1/√n (right: P = 50, e = 0.1 and n ranges from
50 to 1, 000), respectively. Net structure: One hidden layer with 20 hidden units (JS-GAN), zero hidden layer
(TV-GAN). The vertical bars indicate ± standard deviations.
The main experimental results are summarized in Figure 2, where the `2 error we present is the maximum error
among all choices of Q, and detailed numerical results can be founded in Tables 7, 8 and 9 in Appendix. We
separately explore the relation between the error and one of e, √p and 1 / √n with the other two parameters
fixed. The study of the relation between the '2 error and e is in the regime dp/n < e so that e dominates
8
Published as a conference paper at ICLR 2019
the minimax rate. The scenario，p/n > e is considered in the study of the effects of √p and 1 / √n. As is
shown in Figure 2, the errors are approximately linear against the corresponding parameters in all cases, which
empirically verifies the conclusions of Theorem 3.1 and Theorem 3.2.
5.3	Comparisons with Other Methods
We perform additional experiments to compare with other methods including dimension halving (Lai et al.,
2016) and iterative filtering (Diakonikolas et al., 2017) under various settings. We emphasize that our method
does not require any knowledge about the nuisance parameters such as the contamination proportion e. Tuning
GAN is only a matter of optimization and one can tune parameters based on the objective function only.
Table 1: Comparison of various robust mean estimation methods. Net structure: One-hidden layer network
with 20 hidden units when n = 50, 000 and 2 hidden units when n = 5, 000. The number in each cell is the
average of`2 error kθ - θk with standard deviation in parenthesis estimated from 10 repeated experiments and
the smallest error among four methods is highlighted in bold.
Q	n	P	e	TV-GAN	JS-GAN	Dimension Halving	Iterative Filtering
N(0.5 * 1p,Ip厂	50,000	100	.2	0.0953 (0.0064)	0.1144 (0.0154)	-0.3247 (0.0058)-	0.1472 (0.0071)
N(0.5 * 1p,Ip)	5,000	"Γ00^^	T2-	0.1941 (0.0173)	0.2182 (0.0527)	-0.3568 (0.0197)-	0.2285 (0.0103)
N(0.5 * 1p,Ip厂	50,000	200	.2	0.1108 (0.0093)	0.1573 (0.0815)	-0.3251 (0.0078)-	0.1525 (0.0045)
N(0.5 * 1p,Ip)	50,000	100	.05	0.0913 (0.0527)	0.1390 (0.0050)	-0.0814 (0.0056)-	0.0530 (0.0052)
N (5 * 1p, Ip)	50,000	"Γ00^^	~1Γ~	2.7721 (0.1285)	0.0534 (0.0041)	-0.3229 (0.0087)-	0.1471 (0.0059)
N(0.5 * 1p, Σ)	50,000	100	.2	0.1189 (0.0195)	0.1148 (0.0234)	-0.3241 (0.0088)-	0.1426 (0.0113)
Cauchy(0.5 * 1p)	50,000	100	.2	0.0738 (0.0053T	0.0525(0.00l9T	01045 (0.0071)	0.0633 (0.0042)
Table 1 shows the performances of JS-GAN, TV-GAN, dimension halving, and iterative filtering. The network
structure, for both JS-GAN and TV-GAN, has one hidden layer with 20 hidden units when the sample size
is 50,000 and 2 hidden units when sample size is 5,000. The critical hyper-parameters we apply is given in
Appendix and it turns out that the choice of the hyper-parameter is robust against different models when the
net structures are the same. To summarize, our method outperforms other algorithms in most cases. TV-GAN
is good at cases when Q and N(0p, Ip) are non-separable but fails when Q is far away from N(0p, Ip) due to
optimization issues discussed in Section 3.1 (Figure 1). On the other hand, JS-GAN stably achieves the lowest
error in separable cases and also shows competitive performances for non-separable ones.
5.4	Network Structures
We further study the performance of JS-GAN with various structures of neural networks. The main observation
is tuning networks with one-hidden layer becomes tough as the dimension grows (e.g. p ≥ 200), while a deeper
network can significantly refine the situation perhaps by improving the landscape. Some experiment results are
given in Table 2. On the other hand, one-hidden layer performs not worse than deeper networks when dimension
is not very large (e.g. p ≤ 100). More experiments are given in Appendix B.4. Additional theoretical results
for deep neural nets are given in Appendix A.
Table 2: Experiment results for JS-GAN using networks with different structures in high dimension. Settings:
e = 0.2, P ∈ {200,400} and n = 50,000.________________________________________________
P	200-100-20-1	200-200-100-1	200-100-1	200-20-1
200	0.0910 (0.0056)	0.0790 (0.0026)~~	0.3064 (0.0077)	0.1573 (0.08l5)-
P	400-200-100-50-20-1	400-200-100-20-1	400-200-20-1	-400-200-1
400	0.1477 (0.0053)	0.1732 (0.0397)	0.1393 (0.0090)	0.3604 (0.0990)-
9
Published as a conference paper at ICLR 2019
5.5	Adaptation to Unknown Covariance
The robust mean estimator constructed through JS-GAN can be easily made adaptive to unknown covariance
structure, which is a special case of (16). We define
(θ, Σ) = argmin max
η∈Rp,Γ∈Ep D∈D
1n
lo ɪ2 log D(Xi) + EN(η,Γ) IOg(I- D(Xi))
n i=1
+ log 4,
The estimator θ, as a result, is rate-optimal even when the true covariance matrix is not necessarily identity and
is unknown (see Theorem 4.1). Below, we demonstrate some numerical evidence of the optimality of θbas well
as the error of Σ in Table 3.
Data generating process	Network structure	kθ- 0pk	..— ∣∣ς 一 ςi∣∣op
0.8N(0p, ∑ι) + 0.2N(0.5 * 1p, ∑2)	100-20-1	0.1680 (0.1540)	1.9716(0.7405)
0.8N(θp, ∑ι) + 0.2N(0.5 * 1p, ∑2)	100-20-20-1	0.1824 (0.3034)	1.4495 (0.6028)
0.8N(0p, ∑ι) + 0.2N(1p, ∑2)	100-20-1	0.0817 (0.0213)	1.2753 (0.4523)
-0.8N(0p, Σ1) + 0.2N(6 * 1p, Σ2)	100-20-1	0.1069 (0.0357)	1.1668 (0.1839)
0.8N(0p, ∑ι) + 0.2CaUChy(0.5 * 1p)	100-20-1	0.0797 (0.0257T	4.0653 (0.1569)-
Table 3: Numerical experiments for robust mean estimation with unknown covariance trained with 50, 000
samples. The covariance matrices Σ1 and Σ2 are generated by the same way described in Appendix B.2.
5.6	Adaptation to Elliptical Distributions
We consider the estimation of the location parameter θ in elliptical distribution EC(θ, Σ, h) by the JS-GAN
defined in (16). In particular, we study the case with i.i.d. observations Xi,..., Xn 〜(1-e)Cauchy(θ, Ip)+eQ.
The density function of CaUchy(θ, Σ) is given by p(x; θ, Σ) H ∣∑∣-1/2 (l + (x - θ)TΣ-1(x — θ)) (1+p)/2.
Compared with Algorithm (1), the difference lies in the choice of the generator. We consider the generator
G1 (ξ, U) = gω(ξ)U + θ, where gω(ξ) is a non-negative neural network parametrized by ω and some random
variable ξ. The random vector U is sampled from the uniform distribution on {u ∈ Rp : kuk = 1}. If the
scatter matrix is unknown, we will use the generator G2 (ξ, U) = gω(ξ)AU +θ, with AAT modeling the scatter
matrix.
Table 4 shows the comparison with other methods. Our method still works well under Cauchy distribution,
while the performance of other methods that rely on moment conditions deteriorates in this setting.
Table 4: Comparison of various methods of robust location estimation under Cauchy distributions. Samples
are drawn from (1 - e)Cauchy(0p, Ip) + eQ with e = 0.2, p = 50 and various choices of Q. Sample size:
50,000. Discriminator net structure: 50-50-25-1. Generator gω(ξ) structure: 48-48-32-24-12-1 with absolute
value activation function in the output layer.
Contamination Q	JS-GAN (Gi)	JS-GAN (G2)	Dimension Halving	Iterative Filtering
Cauchy(1.5 * 1p,Ip)	0.0664 (0.0065)	0.0743 (0.0103)	-0.3529 (0.0543)-	0.1244 (0.0114)
Cauchy(5.0 * 1p,Ip)	0.0480 (0.0058)	0.0540 (0.0064)	-0.4855 (0.0616)	0.1687 (0.0310)
Cauchy(1.5 * 1p, 5 * Ip)	0.0754 (0.0135)	0.0742 (0.0111)	-0.3726 (0.0530)	0.1220 (0.0112)
Normal(1.5 * 1p, 5 * Ip)	0.0702 (0.0064-	0.0713 (0.0088T	03915(0.0232)	0.1048 (0.0288))
Acknowledgement
The research of Chao Gao was supported in part by NSF grant DMS-1712957 and NSF Career Award DMS-
1847590. The research of Yuan Yao was supported in part by Hong Kong Research Grant Council (HKRGC)
grant 16303817, National Basic Research Program of China (No. 2015CB85600), National Natural Science
Foundation of China (No. 61370004, 11421110001), as well as awards from Tencent AI Lab, Si Family
Foundation, Baidu Big Data Institute, and Microsoft Research-Asia.
10
Published as a conference paper at ICLR 2019
References
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution from
another. Journal ofthe Royal Statistical Society. Series B (Methodological), pp.131-142,1966.
Nina Amenta, Marshall Bern, David Eppstein, and S-H Teng. Regression depth and center points. Discrete &
Computational Geometry, 23(3):305-323, 2000.
Peter L Bartlett. For valid generalization the size of the weights is more important than the size of the network.
In Advances in neural information processing systems, pp. 134-140, 1997.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Timothy M Chan. An optimal randomized algorithm for maximum tukey depth. In Proceedings of the fifteenth
annual ACM-SIAM symposium on Discrete algorithms, pp. 430-436. Society for Industrial and Applied
Mathematics, 2004.
Mengjie Chen, Chao Gao, and Zhao Ren. Robust covariance and scatter matrix estimation under hubers con-
tamination model. The Annals of Statistics, 46(5):1932-1960, 2018.
Imre Csiszar. Eine informationstheoretische UngleichUng Und ihre anwendung aufbeweis der ergodizitaet Von
markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85-108, 1964.
LUc Devroye and Gabor Lugosi. Combinatorial methods in density estimation. Springer Science & Business
Media, 2012.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust
estimators in high dimensions without the computational intractability. In Foundations of Computer Science
(FOCS), 2016 IEEE 57th Annual Symposium on, pp. 655-664. IEEE, 2016a.
Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Robust learning of fixed-structure bayesian networks.
arXiv preprint arXiv:1606.07384, 2016b.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Being
robust (in high dimensions) can be practical. arXiv preprint arXiv:1703.00893, 2017.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever:
A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815, 2018a.
Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation and learning
mixtures of spherical gaussians. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of
Computing, pp. 1047-1060. ACM, 2018b.
Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for robust linear
regression. arXiv preprint arXiv:1806.00040, 2018c.
David L Donoho and Miriam Gasko. Breakdown properties of location estimates based on halfspace depth and
projected outlyingness. The Annals of Statistics, 20(4):1803-1827, 1992.
Simon S Du, Sivaraman Balakrishnan, and Aarti Singh. Computationally efficient robust estimation of sparse
functionals. arXiv preprint arXiv:1702.07709, 2017.
Kai Wang Fang. Symmetric Multivariate and Related Distributions: 0. Chapman and Hall/CRC, 2017.
Chao Gao. Robust regression via mutivariate regression depth. arXiv preprint arXiv:1702.04656, 2017.
11
Published as a conference paper at ICLR 2019
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-
256, 2010.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information process-
ing systems, pp. 2672-2680, 2014.
Peter J Huber. Robust estimation of a location parameter. The annals of mathematical statistics, 35(1):73-101,
1964.
Peter J Huber. A robust version of the probability ratio test. The Annals of Mathematical Statistics, 36(6):
1753-1758, 1965.
Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering
via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing,
pp. 1035-1046. ACM, 2018.
Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. The gan landscape: Losses,
architectures, regularization, and normalization. arXiv preprint arXiv:1807.04720, 2018.
Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In Foundations
of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pp. 665-674. IEEE, 2016.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of gener-
ative adversarial learning. In Advances in Neural Information Processing Systems, pp. 5545-5553, 2017.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal?
a large-scale study. arXiv preprint arXiv:1711.10337, 2017.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-188, 1989.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative
adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Ivan Mizera. On depth and deep points: a calculus. The Annals of Statistics, 30(6):1681-1736, 2002.
Ivan Mizera and Christine H Muller. Location-scale depth. Journal of the American Statistical Association,
99(468):949-966, 2004.
Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature matching gan. arXiv
preprint arXiv:1702.08398, 2017.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the
likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847-5861,
2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using
variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271-279,
2016.
Davy Paindaveine and Germain Van Bever. Halfspace depths for scatter, concentration and shape matrices.
arXiv preprint arXiv:1704.06160, 2017.
David Pollard. Convergence of stochastic processes. Springer Science & Business Media, 2012.
Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. 2017.
12
Published as a conference paper at ICLR 2019
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolu-
tional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Peter J Rousseeuw and Mia Hubert. Regression depth. Journal of the American Statistical Association, 94
(446):388-402,1999.
Peter J Rousseeuw and Anja Struyf. Computing location depth and regression depth in higher dimensions.
Statistics and Computing, 8(3):193-203, 1998.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234-2242, 2016.
John W Tukey. Mathematics and the picturing of data. In Proceedings of the International Congress of
Mathematicians, Vancouver, 1975, volume 2, pp. 523-531, 1975.
Aad W Van Der Vaart and Jon A Wellner. Weak convergence. In Weak convergence and empirical processes,
pp. 16-28. Springer, 1996.
Marc van Kreveld, Joseph SB Mitchell, Peter Rousseeuw, Micha Sharir, Jack Snoeyink, and Bettina Speck-
mann. Efficient algorithms for maximum regression depth. In Proceedings of the fifteenth annual symposium
on Computational geometry, pp. 31-40. ACM, 1999.
Yannis G Yatracos. Rates of convergence of minimum distance estimators and kolmogorov’s entropy. The
Annals of Statistics, 13(2):768-774, 1985.
Jian Zhang. Some extensions of tukey’s depth function. Journal of Multivariate Analysis, 82(1):134-165,
2002.
13
Published as a conference paper at ICLR 2019
A Additional Theoretical Results
In this section, we investigate the performance of discriminator classes of deep neural nets with the ReLU
activation function. Since our goal is to learn a p-dimensional mean vector, a deep neural network discriminator
without any regularization will certainly lead to overfitting. Therefore, it is crucial to design a network class
with some appropriate regularizations. Inspired by the work of Bartlett (1997); Bartlett & Mendelson (2002),
we consider a network class with `1 regularizations on all layers except for the second last layer with an `2
regularization. With G1H(B) = g(x) = ReLU(vT x) : kvk1 ≤ B , a neural network class with l + 1 layers is
defined as
GlH+1(B)= g(x) = ReLU XH vhgh(x) :XH |vh| ≤B,gh∈GlH(B) .
Combining with the last sigmoid layer, we obtain the following discriminator class,
FLH (κ, τ, B)
D(x) = sigmoid	wj sigmoid	ujhgjh(x) + bj
j≥1	h=1
2p
|wj| ≤ κ,	uj2h
j≥1	h=1
≤ 2,∣bjl ≤ τ,gjh eGH-ι(B)}.
Note that all the activation functions are ReLU(∙) except that We use Sigmoid(∙) in the last layer in the feature
map g(∙). A theoretical guarantees of the class defined above is given by the following theorem.
Theorem A.1. Assume P In P ∨ e2 ≤ C for some sufficiently small constant c > 0. Consider i.i.d. observations
Xi,…，Xn ~ (1 一 e)N(θ, Ip) + eQ and the estimator θ defined by (14) with D = FH(κ,τ, B) with H ≥ 2p,
2 ≤ L = O(1), 2 ≤ B = O⑴,and T = √p logP. We set K = O (jP InP + e). Then, for the estimator θ
defined by (14) with D = FLH (κ, τ, B), we have
kθb 一 θk2 ≤ C
plogp	2
-----V e
n
with probability at least 1 — e-C0(p log p+ne2) uniformly over all θ ∈ RP such that ∣∣θ∣∣∞ ≤ √logP and all Q.
The theorem shows that JS-GAN with a deep ReLU network can achieve the error rate PIog P Ve2 with respect to
the squared '2 loss. The condition ∣∣θ∣∣∞ ≤ √logP for the ReLU network can be easily satisfied with a simple
preprocessing step. We split the data into two halves, whose sizes are log n and n - log n, respectively. Then,
we calculate the coordinatewise median θ using the small half. It is easy to show that ∣∣θ - θ∣∣∞ ≤ JIogp V e
with high probability. Then, for each Xi from the second half, the conditional distribution of Xi - θ given the
first half is (1 - e)N(θ - θ,l0) + eQ. Since JIogn V e ≤ √logP, the condition ∣∣θ - θ∣∣∞ ≤ √logP is satisfied,
and thus we can apply the estimator (14) using the shifted data Xi - θ from the second half. The theoretical
guarantee of Theorem A.1 will be
∣θb-(θ-θe)∣2 ≤C
PlogP
n
V e2
with high probability. Hence, we can use θ + θ as the final estimator to achieve the same rate in Theorem A.1.
On the other hand, our experiments show that this preprocessing step is not needed. We believe that the
assumption ∣∣θ∣∞ ≤ √log P is a technical artifact in the analysis of the Rademacher complexity. It can probably
be dropped by a more careful analysis.
14
Published as a conference paper at ICLR 2019
B Details of Experiments
B.1 Training Details
The implementation for JS-GAN is given in Algorithm 1, and a simple modification of the objective function
leads to that of TV-GAN.
Algorithm IJS-GAN: argmi% maxw [ 1 Pn=1 log Dw(Xi) + Elog(1 - Dw(Gn(Z)))]
Input: Observation set S = {X1, . . . , Xn} ∈ Rp, discriminator network Dw(x), generator network Gη(z) =
z+η, learning rates γd and γg for the discriminator and the generator, batch size m, discriminator steps in each
iteration K, total epochs T , average epochs T0 .
Initialization: Initialize η with coordinatewise median of S. Initialize w with N(0, .05) independently on each
element or Xavier (Glorot & Bengio, 2010).
1:	for t = 1, . . . , T do
2:	for k = 1, . . . , K do
3:	Sample mini-batch {X1, . . . , Xm} from S. Sample {Z1, . . . , Zm} from N(0, Ip)
4:	gw 一 Vw[m1 ∑m=ιlogDw(Xi) + m⅛∑i=1 log(1 - Dw(Gn(Zi)))]
5：	W J W + Ydgw
6:	end for
7:	Sample {Z1, . . . , Zm } from N (0, Ip)
8：	gn J vn[m1 4=1 Iog(I- Dw(Gn(Zi)))]
9:	η J η - γggn
10:	end for
Return: The average estimate η over the last T0 epochs.
Several important implementation details are discussed below.
•	How to tune parameters? The choice of learning rates is crucial to the convergence rate, but the
minimax game is hard to evaluate. We propose a simple strategy to tune hyper-parameters including
∕∖'' ∕∖''
the learning rates. Suppose we have estimators θ1, . . . , θM with corresponding discriminator networks
Dwb1 ,. . . , DwbM . Fixing η = θ, we further apply gradient descent to Dw with a few more epochs (but
not many in order to prevent overfitting, for example 10 epochs) and select the θ with the smallest
value of the objective function (14) (JS-GAN) or (12) (TV-GAN). We note that training discriminator
and generator alternatively usually will not suffer from overfitting since the objective function for
either the discriminator or the generator is always changing. However, we must be careful about the
overfitting issue when training the discriminator alone with a fixed η, and that is why we apply an
early stopping strategy here. Fortunately, the experiments show if the structures of networks are same
(then of course, the dimensions of the inputs are same), the choices of hyper-parameters are robust to
different models and we present the critical parameters in Table 5 to reproduce the experiment results
in Table 1 and Table 2.
•	When to stop training? Judging convergence is a difficult task in GAN trainings, since sometimes
oscillation may occur. In computer vision, people often use a task related measure and stop training
once the requirement based on the measure is achieved. In our experiments below, we simply use a
sufficiently large T which works well, but it is still interesting to explore an efficient early stopping
rule in the future work.
•	How to design the network structure? Although Theorem 3.1 and Theorem 3.2 guarantee the minimax
rates of TV-GAN without hidden layer and JS-GAN with one hidden layer, one may wonder whether
deeper network structures will perform better. From our preliminary experiments, TV-GAN with
one hidden layer is significantly better than TV-GAN without any hidden layer. Moreover, JS-GAN
15
Published as a conference paper at ICLR 2019
Table 5: Choices of hyper-parameters. The parameter λ is the penalty factor for the regularization term (17)
and other parameters are listed in Algorithm 1. We apply Xavier initialization (Glorot & Bengio, 2010) for
both JS-GAN and TV-GAN trainings.___________________________________________________
Structure	Net	Yg	Yd	K	T	To	λ
100-20-1	JS	0.02	0.2		150	~5Γ	0
	TV	0.0001	0.3	2	150	1	0.1
100-2-1	JS	0.01	0.2		150	^5"	0
	TV	0.01	0.1	5	150	1	1
200-20-1	JS	0.02	0.2		200	^5"	0
	TV	0.0001	0.1	2	200	1	0.5
200-200-100-1	JS	0.005	0.1	~2~	200	^5"	0
400-200-20-T-	JS	0.02	0.05	~2~	250	^5"	0.5
with deep network structures can significantly improve over shallow networks especially when the
dimension is large (e.g. p ≥ 200). For a network with one hidden layer, the choice of width may
depend on the sample size. If we only have 5,000 samples of 100 dimensions, two hidden units
performs better than five hidden units, which performs better than twenty hidden units. If we have
50,000 samples, networks with twenty hidden units perform the best.
•	How to stabilize and accelerate TV-GAN? As we have discussed in Section 3.1, TV-GAN has a bad
landscape when N(θ, Ip) and the contamination distribution Q are linearly separable (see Figure 1).
An outlier removal step before training TV-GAN may be helpful. Besides, spectral normalization
(Miyato et al., 2018) is also worth trying since it can prevent the weight from going to infinity and
thus can increase the chance to escape from bad saddle points. To accelerate the optimization of TV-
GAN, in all the numerical experiments below, we adopt a regularized version of TV-GAN inspired
by Proposition 3.1. Since a good feature extractor should match nonlinear moments of P = (1 -
)N (θ, Ip) + Q and N(η, Ip), we use an additional regularization term that can accelerate training
and sometimes even leads to better performances. Specifically, let D(x) = sigmoid(wT Φ(x)) be the
discriminator network with w being the weights of the output layer and ΦD (x) be the corresponding
network after removing the output layer from D(x). The quantity ΦD (x) is usually viewed as a feature
extractor, which naturally leads to the following regularization term (Salimans et al., 2016; Mroueh
et al., 2017), defined as
1 n	2
r(D,η)= -∑T(Φd,Xi) -T(ΦD,N(η,Ip))	,	(17)
i=1
where T(Φ, P) = EPΦ(X) (moment matching) or T(Φ,P) = MedianX〜PΦd (X) (median match-
ing).
B.2	SETTINGS OF CONTAMINATION Q
We introduce the contamination distributions Q used in the experiments. We first consider Q = N(μ,Ip)
with μ ranges in {0.2,0.5,1, 5}. Note that the total variation distance between N(0p, Ip) and N(μ, Ip) is of
order ∣∣0p - μk = kμ∣∣. We hope to use different levels of ∣∣μ∣∣ to test the algorithm and verify the error rate
in the worst case. Second, We consider Q = N(1.5 * 1p, Σ) to be a Gaussian distribution with a non-trivial
covariance matrix Σ. The covariance matrix is generated according to the following steps. First generate a
sparse precision matrix Γ = (γij ) with each entry γij = zij * τij, i ≤ j, where zij and τij are independently
generated from Uniform(0.4,0.8) and Bernoulli(0.1). We then define Yij = Yji for all i > j and Γ =
Γ + (| min eig(Γ) | + 0.05)Ip to make the precision matrix symmetric and positive definite, where min eig(Γ)
is the smallest eigenvalue of Γ. The covariance matrix is Σ = Γ-1. Finally, we consider Q to be a Cauchy
16
Published as a conference paper at ICLR 2019
distribution with independent component, and the jth component takes a standard Cauchy distribution with
location parameter τj = 0.5.
B.3	Comparison Details
In Section 5.3, we compare GANs with the dimension halving (Lai et al., 2016) and iterative filtering (Di-
akonikolas et al., 2017).
•	Dimension Halving. Experiments conducted are based on the code from https://github.com/
kal2000/AgnosticMeanAndCovarianceCode. The only hyper-parameter is the threshold in
the outlier removal step, and we take C = 2 as suggested in the file outRemSperical.m.
•	Iterative Filtering. Experiments conducted are based on the code from https://github.com/
hoonose/robust-filter. We assume is known and take other hyper-parameters as suggested
in the file filterGaussianMean.m.
B.4	Supplementary Experiments for Network Structures
The experiments are conducted with i.i.d. observations drawn from (1 - e)N(0p, Ip) + eN(0.5 * 1p, Ip) with
= 0.2. Table 6 summarizes results for p = 100, n ∈ {5000, 50000} and various network structures. We
observe that TV-GAN that uses neural nets with one hidden layer improves over the performance of that without
any hidden layer. This indicates that the landscape of TV-GAN might be improved by a more complicated
network structure. However, adding one more layer does not improve the results. For JS-GAN, we omit the
results without hidden layer because of its lack of robustness (Proposition 3.1). Deeper networks sometimes
improve over shallow networks, but this is not always true. We also observe that the optimal choice of the
width of the hidden layer depends on the sample size.
Table 6: Experiment results for JS-GAN and TV-GAN with various network structures.
Structure	n	JS-GAN	TV-GAN
100-1	50,000	-	0.1173 (0.0056)
-100-20-1-	50,000	0.0953 (0.0064)	0.1144 (0.0154)
100-50-1	50,000	0.2409 (0.0500)	0.1597 (0.0219)
100-20-20-1	50,000	0.1131 (0.0855T	0.1724 (0.0295)
100-1	5,000	-	0.9818 (0.0417)
-100-2-1	5,000	0.1941 (0.0173)	0.1941 (0.0173)
100-5-1	5,000	0.2148 (0.0241)	0.2244 (0.0238)
100-20-1 ―	5,000	0.3379 (0.02737	0.3336 (0.0186-
B.5	Tables for testing the minimax rates
Tables 7, 8 and 9 show numerical results corresponding to Figure 2.
17
Published as a conference paper at ICLR 2019
Table 7: Scenario I:，p/n < e. Setting: P = 100, n = 50,000, and e from 0.05 to 0.20. Network structure of
JS-GAN: one hidden layer with 5 hidden units. Network structure of TV-GAN: zero-hidden layer. The number
in each cell is the average of `2 error kθ - θk with standard deviation in parenthesis estimated from 10 repeated
experiments. The bold character marks the worst case among our choices of Q at each e level. The results of
TV-GAN for Q = N(5 * 1p, Ip) are highlighted in slanted font. The failure of training in this case is due to the
bad landscape when N(0p, Ip) and Q are linearly separable, as discussed in Section 3.1 (see Figure 1).
Q	Net	e = 0.05	e = 0.10	e = 0.15	e = 0.20
∕∖∏∩ 9 5k 1 T ʌ	^J^	0.1025 (0.0080)	0.1813 (0.0122)	0.2632 (0.0080)	0.3280 (0.0069)
N (0∙2 * 1p, Ip)	TV	0.1110(0.0204)	0.2047 (0.0112)	0.2769 (0.0315)	0.3283 (0.0745)
	^J^	0.1407 (0.0061)	0.1895 (0.0070)	0.1714 (0.0502)	0.1227 (0.0249)
N (0.5 * 1p, Ip)	TV	0.2003 (0.0480)	0.2065 (0.1495)	0.2088 (0.0100)	0.3985 (0.0112)
N (1 I )	^J^	0.0855 (0.0054)	0.1055 (0.0322)	0.0602 (0.0133)	0.0577 (0.0029)
N (1p,Ip)	TV	0.1084 (0.0063)	0.0842 (0.0036)	0.3228 (0.0123)	0.1329 (0.0125)
N(5 * 1 I )	JS	0.0587 (0.0033)	0.0636 (0.0025)	0.0625 (0.0045)	0.0591 (0.0040)
N (5 * 1p, Ip)	TV	1.2886(0.5292)	4.4511 (0.8754)	7.3868 (0.8081)	10.5724 (1.2605)
	^J^	0.0625 (0.0045)	0.0652 (0.0044)	0.0648 (0.0035)	0.0687 (0.0042)
Cauchy(0.5 * 1p )	TV	0.2280 (0.0067)	0.3842 (0.0083)	0.5740 (0.0071)	0.7768 (0.0074)
N(C∖	5k 1 vʌ	^J^	0.1490 (0.0061)	0.1958 (0.0074)	0.2379 (0.0076)	01973(0.0679)
(0.5 * 1p, )	TV	0.2597 (0.0090)	0.4621 (0.0649)	0.6344 (0.0905)	0.7444 (0.3115)
Table 8: Scenario II-a: /p/n > e. Setting: n = 1,000, e = 0.1, and P from 10 to 100. Other details are the
same as above.
Q	Net	p = 10	P = 25	P = 50	P = 75	P = 100
N(0.2 * 1p,Ip)	JS	0.1078 (0.0338)	0.1819 (0.0215)	0.3355 (0.0470)	0.4806 (0.0497)	0.5310(0.0414)
	TV	0.2828 (0.0580)	0.4740 (0.1181)	0.5627 (0.0894)	0.8217 (0.0382)	0.8090 (0.0457)
N(0.5 * 1p,Ip)	JS	0.1587 (0.0438)	0.2684 (0.0386)	0.4213 (0.0356)	0.5355 (0.0634)	0.6825 (0.0981)
	TV	0.2864 (0.0521)	0.5024 (0.1038)	0.6878 (0.1146)	0.9204 (0.0589)	0.9418(0.0551)
N(Ip,1p)	JS	0.1644 (0.0255)	0.2177 (0.0480)	0.3505 (0.0552)	0.4740 (0.0742)	0.6662(0.0611)
	TV	0.3733 (0.0878)	0.5407 (0.0634)	0.9061 (0.1029)	1.0672 (0.0629)	1.1150 (0.0942)
N(5 * lp,Ip)	JS	0.0938 (0.0195)	0.2058 (0.0218)	0.3316(0.0462)	0.4054 (0.0690)	0.5553 (0.0518)
	TV	0.3707 (0.2102)	0.7434 (0.3313)	1.1532(0.3488)	1.1850 (0.3739)	1.3257 (0.1721)
Cauchy(0.5 * 1p)	JS	0.1188 (0.0263)	0.1855 (0.0282)	0.2967 (0.0284)	0.4094 (0.0385)	0.4826 (0.0479)
	TV	0.3198 (0.1543)	0.5205 (0.1049)	0.6240 (0.0652)	0.7536 (0.0673)	0.7612(0.0613)
N(0.5 * 1p, Σ)	JS	0.1805 (0.0220)	0.2692 (0.0318)	0.3885 (0.0339)	0.5144 (0.0547)	0.6833 (0.1094)
	TV	0.3036 (0.0736)	0.5152 (0.0707)	0.7305 (0.0966)	0.9460 (0.0900)	1.0888 (0.0863)
18
Published as a conference paper at ICLR 2019
Table 9: Scenario II-b:，p/n > e. Setting: P = 50, e = 0.1, and n from 50 to 1,000. Other details are the
same as above.
Q	Net	n = 50	n = 100	n = 200	n = 500	n = 1000
N(0.2 * 1p,Ip)	^JΓ^	1.3934 (0.5692)	1.0055 (0.1040)	0.8373 (0.1335)	0.4781 (0.0677)	0.3213 (0.0401)
	TV	1.9714(0.1552)	1.2629 (0.0882)	0.7579 (0.0486)	0.6640 (0.0689)	0.6348 (0.0547)
N(0.5 * 1p,Ip)	JS	1.6422 (0.6822)	1.2101 (0.2826)	0.8374 (0.1021)	0.5832 (0.0595)	0.3930 (0.0485)
	TV	1.9780 (0.2157)	1.2485 (0.0668)	0.8198(0.0778)	0.7597 (0.0456)	0.7346 (0.0750)
N(IP, Ip)	JS	1.8427 (0.9633)	1.2179 (0.2782)	1.0147 (0.2170)	0.5586 (0.1013)	0.3639 (0.0464)
	TV	1.9907 (0.1498)	1.4575 (0.1270)	0.9724 (0.0802)	0.9050 (0.1479)	0.8747 (0.0757)
N(5 * 1p, Ip)	JS	2.6392 (1.3877)	1.3966 (0.5370)	0.9633 (0.1383)	0.5360 (0.0808)	0.3265 (0.0336)
	TV	2.1050 (0.3763)	1.5205 (0.2221)	1.1909 (0.2273)	1.0957 (0.1390)	1.0695 (0.2639)
Cauchy(0.5 * 1p)	JS	1.6563 (0.5246)	1.0857 (0.3613)	0.8944 (0.1759)	0.5363 (0.0593)	0.3832(0.0408)
	TV	2.1031 (0.2300)	1.1712(0.1493)	0.6904 (0.0763)	0.6300 (0.0642)	0.5085 (0.0662)
N(0.5 * 1p, Σ)	^JΓ^	1.2296 (0.3157)	0.7696 (0.0786)	0.5892(0.0931)	0.5015 (0.0831)	0.4085 (0.0209)
	TV	1.9243 (0.2079)	1.2217 (0.0681)	07939 (0.0688)	0.7033 (0.0414)	0.7125 (0.0490)
C Proofs of Proposition 2.1 and Proposition 3.1
In the first example, consider
一 一 ~ 一 . 一. .... .. 一
Q = {N(n,ip)： η ∈ Rp},	Qn = {N(e,ip) ： kη - ηk≤r}∙
In other words, Q is the class of Gaussian location family, and Qη is taken tobea subset in a local neighborhood
一一， 一、 _	_	一， 一、 _ CT 一，〜，-、	_	_______ __ ..	.
of N(η, Ip). Then, with Q = N(η, Ip) and Qe = N (ηe, Ip), the event qe(X)/q(X) ≥ 1 is equivalent to
kX - ηek2 ≤ kX - ηk2. Since kηe - ηk ≤ r, we can write ηe = η + reu for some re ∈ R and u ∈ Rp that satisfy
0 ≤ re ≤ r and kuk = 1. Then, (8) becomes
θ = argmin sup
η∈Rp kuk=1
0≤re≤r
n XXI kT (Xi- η) ≥ e} - p(N(0,1)≥ e
(18)
Letting r → 0, we obtain (9), the exact formula of Tukey’s median.
The next example is a linear model y|X 〜N(XTθ, 1). Consider the following classes
Q	=	{Py,χ =	Py∣xPX	： Py∣x	=	N(XTη,1),η ∈ Rp},
Qn	=	{Py,X =	PyIXPX	: PyIX	=	N(XTe,	I), ke - Ml ≤	r} ∙
Here, Py,X stands for the joint distribution of y and X. The two classes Q and Q share the same marginal
distribution PX and the conditional distributions are specified by N(XTη, 1) and N(XTηe, 1), respectively.
Follow the same derivation of Tukey’s median, let r → 0, and we obtain the exact formula of regression depth
(10). It is worth noting that the derivation of (10) does not depend on the marginal distribution PX.
The last example is on covariance/scatter matrix estimation. For this task, we set Q = {N (0, Γ) : Γ ∈ Ep},
where Ep is the class ofallp ×p covariance matrices. Inspired by the derivations of Tukey depth and regression
depth, it is tempting to choose Q in the neighborhood of N(0, Γ). However, a native choice would lead to a
definition that is not even Fisher consistent. We propose a rank-one neighborhood, given by
QeΓ = nN(0,Γe) : Γe-1 = Γ-1 + reuuT ∈ Ep, |re| ≤r,kuk = 1o.	(19)
Then, a direct calculation gives
I [ dNO (X) ≥ 4= I {e∣uτ XI2 ≤ iog(1 + euTru)}.
dN (0, Γ)
(20)
19
Published as a conference paper at ICLR 2019
Since lime→o 10鼠；+才_。") = 1, the limiting event of (20) is either I{∣uTX∣2 ≤ UTΓu} or I{∣uτX∣2 ≥
uT Γu}, depending on whether re tends to zero from left or from right. Therefore, with the above Q and QΓ, (8)
becomes (11) under the limit r → 0. Even though the definition of (19) is given by a rank-one neighborhood of
the inverse covariance matrix, the formula (11) can also be derived with Γe-1 = Γ-1 + reuuT in (19) replaced
by Γ = Γ + reuuT by applying the Sherman-Morrison formula. A similar formula to (11) in the literature is
given by
W	∙ C
Σ = argmax inf
Γ∈Ep kuk=1
1n
—Vl{∣uτXil2 ≤ βuτΓu} ∧
n
i=1
1n
-XI{∣uτXil2 ≥ βuτΓu}
n i=1
(21)
which is recognized as the maximizer of what is known as the matrix depth function (Zhang, 2002; Chen
et al., 2018; Paindaveine & Van Bever, 2017). The β in (21) is a scalar defined through the equation
P(N(0,1) ≤ √β) = 3/4. It is proved in Chen et al. (2018) that Σ achieves the minimax rate under HU-
ber’s -contamination model. While the formula (11) can be derived from TV-Learning with discriminators
in the form of I { dN(0,Γ) (X) ≥ 1}, a special case of (6), the formula (21) can be derived directly from TV-
GAN with discriminators in the form of I [ dN(0,案(X) ≥ 10
dN (0,βΓ)
argument. This completes the derivation of Proposition 2.1.
by following a similar rank-one neighborhood
To prove Proposition 3.1, we define F(w) = EP log sigmoid(wT g(X)) + EQ log(1 - sigmoid(wT g(X))) +
log 4, so that JSg(P, Q) = maxw∈W F (w). The gradient and Hessian ofF(w) are given by
VF (W)
V2 F (W)
e-wT g(X)	ewT g(X)
EP 1+ e-wTg(X) g(X) - EQ I + ewTg(X) g(X),
-EP
ewTg(X)
(1 + ewT g(X))2
g(X)g(X)T - EQ
e-wT g(X)
(1 + e-wT g(X ))2
g(X)g(X)T.
Therefore, F(W) is concave in W, and maxw∈W F(W) is a convex optimization with a convex W. Suppose
JSg (P, Q) = 0. Then maxw∈W F(W) = 0 = F (0), which implies VF(0) = 0, and thus we have EPg(X) =
EQg(X). Now suppose EPg(X) = EQg(X), which is equivalent to VF (0) = 0. Therefore, W = 0 is a
stationary point of a concave function, and we have JSg (P, Q) = maxw∈W F(W) = F(0) = 0.
D Proofs of Main Results
In this section, we present proofs of all main theorems in the paper. We first establish some useful lemmas in
Section D.1, and the the proofs of main theorems will be given in Section D.2.
D. 1 Some Auxiliary Lemmas
Lemma D.1. Given i.i.d. observations Xi ,…，Xn 〜P and the function class D defined in (13), we have for
any δ > 0,
sup
D∈D
1n
D(Xi) - ED(X)
n
i=1
≤C
log(1∕δ)
n
with probability at least 1 - δ for some universal constant C > 0.
Proof. Let f(X∖,…,Xn = sup°∈D ∣ n Pn=I D(Xi) 一 ED(X)∣. It is clear that f(Xi,…，Xn) satisfies the
bounded difference condition. By McDiarmid’s inequality (McDiarmid, 1989), we have
f (X1,..., Xn) ≤ Ef(XI,…,Xn) + ʌ/log(?)
20
Published as a conference paper at ICLR 2019
with probability at least 1 - δ. Using a standard symmetrization technique (Pollard, 2012), we obtain the
following bound that involves Rademacher complexity,
Ef (X1 , ..., Xn) ≤ 2E sup
D∈D
1n
EiD(Xi)
n i=1
(22)
where E1, ..., En are independent Rademacher random variables. The Rademacher complexity can be bounded
by Dudley’s integral entropy bound, which gives
E sup
D∈D
1n	1	2
nχeiD(Xi) . E√n/
,logN(δ,D,k ∙kn)dδ,
where N(δ, D, ∣∣ ∙	) is the δ-covering number of D with respect to the empirical '2 distance ∣∣f -
g∣∣n = nJ 1 Pn=1(f (Xi) - g(Xi))2. Since the VC-dimension of D is O(P), We have N(δ, DJ ∙
Iln) . P (16e∕δ)O(p) (see Theorem 2.6.7 of Van Der Vaart & Wellner (1996)). This leads to the bound
√n R0 piogN(δ,D, k ∙ In)dδ . Pp, which gives the desired result.	□
Lemma D.2. Given i.i.d. observations Xi,..., Xn 〜P, and the function class D defined in (15), we have for
any δ > 0,
SUp 1 X log D(Xi)- Elog D(X) ≤ CK (r∕p + r log(1∕δ)!,
D∈D n i=1	n	n
with probability at least 1 - δ for some universal constant C > 0.
Proof Let f(Xi,…,Xn) = supD∈D ∣1 Pn=IlOgD(Xi)- ElogD(X)∣. Since
sUp sUp | log(2D(x))| ≤ κ,
D∈D x
we have
SUp	∣f(xi, ∙∙∙,Xn) - f(xi, ...,Xi-1,χi,Xi+1, ...,Xn)∣ ≤ 一∙
x1 ,...,xn ,xi
Therefore, by McDiarmid’s inequality (McDiarmid, 1989), we have
f(X1,…,Xn) ≤ Ef(X1,…,Xn )+ K∖ ∕2log(1∕δ) ,	(23)
n
with probability at least 1 - δ. By the same argument of (22), it is sufficient to bound the Rademacher complex-
ity EsupD∈D 11 Pn=I Ei log(2D(Xi))∣. Since the function ψ(x) = log(2sigmoid(x)) has Lipschitz constant
1 and satisfies ψ(0) = 0, we have
E SUp
D∈D
1n
Ei log(2D(Xi))
n
i=1
≤ 2E	SUp
Pj≥1 |wj l≤κ,uj ∈Rp,bj ∈R
1n
n∑Εi∑ Wj σ(uT Xi + bj)
i=1	j≥1
which uses Theorem 12 of Bartlett & Mendelson (2002). By Holder,s inequality, we further have
E	SUp
Pj≥ι ∣Wj∣≤κ,Uj∈Rp,bj∈R
1n
n∑Εi∑ Wj σ(uT Xi + bj)
i=1	j≥1
≤
κE max	SUp
j≥1 uj ∈Rp,bj ∈R
1n
Xiσ(uT Xi + bj)
i=1
κE SUp
u∈Rp,b∈R
1n
—〉Εiσ(u Xi + b)
n
i=1
21
Published as a conference paper at ICLR 2019
Note that for a monotone function σ : R → [0,1], the VC-dimension of the class {σ(uTx + b) : U ∈ R, b ∈ R}
is O(p). Therefore, by using the same argument of Dudley,s integral entropy bound in the proof Lemma D.1,
We have
E sup
u∈Rp,b∈R
1 n
1 3>σ(uT X, + b)
n i=1
Which leads to the desired result.
□
Lemma D.3. Given i.i.d. observations Xi,.., Xn 〜N(θ,Ip) and the function class FH (κ,τ,B). Assume
∣∣θ∣∣∞ ≤ ʌ/log p and set T = ʌ/p log P. We have for any δ > 0,
sup
D∈FH (κ,τ,B)
1n
-Vlog D(Xi)- Elog D(X)
n z—z
i=1
≤ CK "H∙+rogF)
with probability at least 1 一 δ for some universal constants C > 0.
Proof. Write f (X1, ...,Xn) = supp∈j-H(k,t,b) ∣n Pn=I logD(Xi)- ElogD(X)∣. Then, the inequality
(23) holds with probability at least 1 - δ. It is sufficient to analyze the Rademacher complexity. Using the
fact that the function log(2sigmoid(x)) is Lipschitz and Holder,s inequality, we have
E sup
D∈FH(K,τ,B)
1n
e% log(2D(Xi))
n i=i
2E	sup
IIwkι≤κ,∣∣uj* ∣∣2≤2,电l≤τ,gjh∈GH-I(B)
2κE	sup
kuk2≤2,∣b∣≤τ,gh∈GH-ι(B)
4κE	sup
kuk2≤2,∣b∣≤τ,gh∈GH-ι(B)
8√pκE	sup
g∈GH-ι(B)
Now we
1n
-Σei EwjSigmOid
i=i j≥i
Ujhgjh(Xi) + bj
1 n	2 2P	'
一£eisigmoid I EUhgh(Xi) + b
i=1	∖h=1	,
n Xei (x Uh gh(Xi)+b) ∣
i=i	h=i
1 n	∣ 1 n
n X qg(Xi) +4κτ EIn X
i=1
i=1
use the notation Zi
Xi - θ
Esupg∈GH-ι(B) I 1 Pn=I eig(Zi + θ) ∣ by induction. Since
~	N(0,Ip) for i
1, ..., n.	We bound
≤
≤
≤
≤
n
n
≤
E sup — ^X eig(Zi + θ)
g∈GH(B)n =	,
E sup 1 X eivτ (Zi + θ)
kvkι≤B n M
n
n
≤
B
nn
E 1 X qZi	+ ∣∣θ∣∣∞E 1 X
nn
i=i
i=i
∞
≤
LR√I°gp + llθk∞
CB-----------
n
22
Published as a conference paper at ICLR 2019
and
≤
E sup — X eig(Zi + θ)
g∈GlH+1(B) n i=1
-n H
E	sup	ei	vhgh(Zi + θ)
kvk1 ≤B,gh ∈GlH (B) n i=1 h=1
BE	sup
g∈GlH(B)
≤
≤
n X eig(Zi +
1n
-Eeig(Zi + θ),
i=1
2BE	sup
g∈GlH(B)
we have
E sup
g∈GLH-1(B)
nχ qg(Zi+小 C(2B)L-I √°g⅞n⅛
Combining the above inequalities, we get
E ( SUp	- X ei log D(Zi + θ)] ≤ CK (/(2B)LT √°g^ + kθk∞ +
D∈FH(κ,τ,B)n W	J	n	√n
This leads to the desired result under the conditions on τ and kθk∞ .
□
D.2 Proofs of Main Theorems
Proof of Theorem 3.1. We first introduce some notations. Define F(P, η) = maxw,b Fw,b (P, η), where
Fw,b(P, η) = EP sigmoid(wT X + b) - EN(η,Ip)sigmoid(wT X + b).
With this definition, we have θ = argminη F (Pn, η), where we use Pn for the empirical distribution
1 Pn=ι δχi. We shorthand N(η,Ip) by Pn, and then
一,_ ^ 一,, , _ . ^
F(Pθ, θ) ≤ F((I - e)Pθ + eQ, θ) +e	(24)
≤ F (Pn,b)+e+C (rp+rog(尹!	(25)
≤F (Pn,θ)+e+C (rp+r ι°g(≡!	(26)
≤ F((1 — e)Pθ + eQ,θ) + e + 2C (Tn + Jlo，0 )	(27)
≤ F (Pθ ,θ) + 2e + 2C (rp + J log：0)	Q8)
=2e + 2C (J + 严野).	(29)
With probability at least - δ, the above inequalities hold. We will explain each inequality. Since
F((- - e)Pθ + eQ, η) = max [(- - e)Fw,b (Pθ, η) + eFw,b(Q, η)] ,
w,b
we have
SUP IF((I - e)pθ + eQ,η) - F(Pθ, n)| ≤ e,
η
23
Published as a conference paper at ICLR 2019
which implies (24) and (28). The inequalities (25) and (27) are implied by Lemma D.1 and the fact that
sup ∣F(Pn,η) - F((1 - e)Pθ + eQ,η)∣ ≤ sup
w,b
1n
一	SIgmoId(WTXi + b) — EsIgmoId(WTX + b)
n
i=1
η
El ♦	一,/C,'*	T	,	。，1	1 Γ∙ ∙ , ∙	/ʌ ∕Λ^ -Γ-1∙ 11 • , •	,	,1 , L/C 八、	C
The inequality (26) is a direct consequence of the definition of θ. Finally, it is easy to see that F(Pθ, θ) = 0,
which gives (29). In summary, we have derived that with probability at least 1 - δ,
Fw,b (Pθ, θb) ≤ 2e + 2C
log(1∕δ)
n
for all W ∈ Rp and b ∈ R. For any u ∈ Rp such that kuk = 1, we take W = u and b = -uT θ, and we have
f (0) - f(uT(θ - b)) ≤ 2e + 2C (rp + 严n画),
where f (t) = / [+匕+. φ(z)dz, With φ(∙) being the probability density function of N(0,1). It is not hard to
see that as long as |f (t) - f (0)| ≤ c for some sufficiently small constant c > 0, then |f (t) - f (0)| ≥ c0|t| for
some constant c0 > 0. This implies
kb — θk = sup ∣uτ(θ — θ)∣
kuk=1
≤ 二 SUplf (0) - f(uT(θ - b))∣
c0 kuk=1
.e + rp + 尸画,
nn
with probability at least 1 - δ. The proof is complete.
□
Proof of Theorem 3.2. We continue to use Pη to denote N(η, Ip). Define
F(P,η) =	max	Fw,u,b(P,η),
kwk1≤κ,u,b
where
Fw,u,b(P,η) =EPlogD(X)+EN(η,Ip)log(1-D(X))+log4,
with D(x) = sIgmoId Pj≥1 Wjσ(ujTx + bj) . Then,
F(Pθ,θb)	≤
≤
F((1 - e)Pθ + eQ, θb) + 2κe
≤
F(Pn, θb) + 2κe + Cκ
F(Pn, θ) + 2κe + Cκ
≤
≤
F((1 - e)Pθ + eQ,θ) + 2κe + 2Cκ	+ J0g学))
F (Pθ ,θ)+4κe+2Cκ (rp+r logy)!
4κe+2Cκ (rp+rog(粤!.
(30)
(31)
(32)
(33)
(34)
24
Published as a conference paper at ICLR 2019
The inequalities (30)-(34) follow similar arguments for (24)-(28). To be specific, (31) and (33) are implied by
∕∖''
Lemma D.2, and (32) is a direct consequence of the definition of θ. To see (30) and (34), note that for any w
such that kwk1 ≤ κ, we have
| log(2D(X))| ≤	wj σ(ujT X + bj) ≤ κ.
j≥1
A similar argument gives the same bound for | log(2(1 - D(X)))|. This leads to
sup IF((I -E)Pθ + eQ, η) - F(Pθ, η) | ≤ 2κe,
η
(35)
which further implies (30) and (34). To summarize, we have derived that with probability at least 1 - δ,
Fw,u,b(Pθ,θ) ≤ 4κe + 2Cκ (点 + Jlogn画
for all kwk1 ≤ κ, kuj k ≤ 1 and bj. Take w1 = κ, wj = 0 for all j > 1, u1 = u for some unit vector u and
b1 = -uT θ, and we get
fυτ (θ-θ)(K) ≤ 4κe+2Cκ (rp+r ιog(1")!,	(36)
where
22
fδ ⑴=E lθg 1 + e-tσ(Z) + E lθg 1 + etσ(Z+δ)，	(37)
With Z 〜N(0, 1). Direct calculations give
e-tσ( )	etσ( + )
fδ (t) = E 1 + e-tσ(Z) σ(Z)- E 1+etσ(Z+δ) σ(Z + δ),
e-tσ(Z)	etσ(Z+δ)
fδ (t) = -Eσ(Z )2 (1 + e-tσ(Z))2 - Eσ(Z + δ)2(1+ etσ(Z+δ))2
(38)
Therefore, fδ(0) = 0, fδ(0) = 1 (Eσ(Z) - Eσ(Z + δ)), and fδ0(t) ≥ -1. By the inequality
fδ (K) ≥ fδ (0) + κfδ (0) - 1K2,
We have Kfδ0 (0) ≤ fδ(K) + K2/4. In vieW of (36), We have
≤
σ(z)φ(z)dz - σ(z + uT (θb- θ))φ(z)dz
4κe + 2Cκ (rP + J! + K2.
It is easy to see that for the choices of σ(∙), J σ(z)φ(z)dz - J σ(z + t)φ(z)dz is locally linear with respect to
t. This implies that
Kkb - θk=KkuupI uτ (b -θ.κ (e+rp+r ιog(rδ2)+κ2.
Therefore, with a K . n/+ + e, the proof is complete.
□
Proof of Theorem 4.1. We use Pθ,Σ,h to denote the elliptical distribution EC(θ, Σ, h). Define
F(P, (η, Γ, g)) = max	Fw,u,b(P, (η, Γ, g)),
kwk1 ≤κ,u,b
25
Published as a conference paper at ICLR 2019
where
Fw,u,b(P, (η, Γ, g)) = EP log D(X) + EEC(η,Γ,g) log (1 - D(X)) + log 4,
with D(x) = sigmoid Pj≥1 wj σ(ujT x + bj) . Let P be the data generating process that satisfies
TV(P, Pθ,Σ,h) ≤ , and then there exist probability distributions Q1 and Q2, such that
P + Q1 = Pθ,Σ,h + Q2.
The explicit construction of Q1, Q2 is given in the proof of Theorem 5.1 of Chen et al. (2018). This implies
that
|F (P, (η, Γ, g)) - F (Pθ,Σ,h, (η, Γ, g))|
≤ sup	|Fw,u,b(P, (η, Γ, g)) - Fw,u,b(Pθ,Σ,h, (η, Γ, g))|
kwk1≤κ,u,b
= sup	|EQ2log(2D(X))-EQ1log(2D(X))|
kwk1 ≤κ,u,b
≤ 2κ.	(39)
Then, the same argument in Theorem 3.2 (with (35) replaced by (39)) leads to the fact that with probability at
least 1 - δ,
Fw,u,b(Pθ,∑,h, (b, ∑, h)) ≤ 4κe + 2Cκ ^rn + J叫他)
for all kwk1 ≤ κ, kuj k ≤ 1 and bj. Take w1 = κ, wj = 0 for all j > 1, u1 = u/ uTΣbu for some unit vector
u and b1 = -uT θ/ uTΣbu, and we get
f UT(b-θ) (K) ≤ 4κe + 2Cκ
√uT ΣU
log(1∕δ)
n
where
fδ (t) = / log (1 + e2tσ(∆s)) h(s)ds + / log (1 + eMδ+s)) 及S",
where δ = Uy ^-θ and ∆ = λζuT?.A similar argument to the proof of Theorem 3.2 gives
uTΣbu	uTΣbu
σ(∆S)h(S)dS - σ(δ + S)bh(S)dS
≤ 4κe + 2Cκ
iog(i∕δ)	+ κ2
n	4 .
Since
J σ(∆s)h(s)ds =I = J σ(s)h(s)ds,
the above bound is equivalent to
K (H(0) - H(δ)) ≤ 4κe + 2Cκ
log(1∕δ)	κ2
+τ,
1	TT / r∖	C / r . ∖V^ / ∖ ι El ι ι	FF iff。 k/ tt∕ r∖	ττ ∕<^>∖ ∖ ι	, ∙	,
where H(δ) = / σ(δ + s)h(s)ds. The above bound also holds for 2(H(δ) - H(0)) by a symmetric argument,
and therefore the same bound holds for K |H(δ) - H(0)|. Since H0(0) = R σ(s)(1 - σ(s))h(s)ds = 1, H(δ)
is locally linear at δ = 0, which leads to a desired bound for δ
UT (b-θ)
UTΣbU
. Finally, since uTΣbu ≤ M, we get
the bound for uT (θb- θ). The proof is complete by taking supreme over all unit vector u.
□
26
Published as a conference paper at ICLR 2019
Proof of Theorem A.1. We continue to use Pη to denote N(η, Ip). Define
F (P, η) =	sup	FD (P, η),
D∈FLH (κ,τ,B)
with
FD(P, η) = EP log D(X) + EN(η,Ip) log(1 - D(X)) + log 4.
Follow the same argument in the proof of Theorem 3.2, use Lemma D.3, and we have
Fd (Pθ, b) ≤ Cκ (e + (2B)L-I /学 + ^^),
uniformly over D ∈ FLH (κ, τ, B) with probability at least 1 - δ. Choose w1 = κ and wj = 0 for all wj > 1.
For any unit vector ue ∈ Rp, take u1h = -u1(h+p) = ueh for h = 1, ..., p and b1 = -ueT θ. For h = 1, ..., p,
set g1h(x) = max(xh, 0). For h = p + 1, ..., 2p, set g1h(x) = max(-xh-p, 0). It is obvious that such u
and b satisfy Ph 婢h ≤ 2 and |bi| ≤ ∣∣θk ≤ √p∣∣θ∣∣∞ ≤ √plogP. We need to show both the functions
max(x, 0) and max(-x, 0) are elements of GLH-1(B). This can be proved by induction. It is obvious that
max(xh, 0), max(-xh, 0) ∈ G1H(B) for any h = 1, ..., P. Suppose we have max(xh, 0), max(-xh, 0) ∈
GlH(B) for any h = 1, ..., P. Then,
max (max(xh, 0) - max(-xh, 0), 0)	= max(xh, 0),
max (max(-xh, 0) - max(xh, 0), 0)	= max(-xh, 0).
Therefore, max(xh, 0), max(-xh, 0) ∈ GlH+1(B) as long as B ≥ 2. Hence, the above construction satisfies
D(x) = sigmoid(κsigmoid(ueT (x - θ))) ∈ FLH (κ, τ, B), and we have
fuτ(b-θ) (K) ≤ Cκ (e + (2B尸 H + rl0g(≡! ,	(40)
where the definition of fδ(t) is given by (37) with Z 〜 N(0,1) and σ(∙) is taken as Sigmoid(∙). Apply the a
similar in the proof of Theorem 3.2, We obtain the desired result.	□
27