Published as a conference paper at ICLR 2019
Unsupervised Domain Adaptation
for Distance Metric Learning
Kihyuk Sohn1 Wenling Shang2 Xiang Yu1	Manmohan Chandraker1,3
1NEC Labs America	2University of Amsterdam	3UC San Diego
Ab stract
Unsupervised domain adaptation is a promising avenue to enhance the performance
of deep neural networks on a target domain, using labels only from a source
domain. However, the two predominant methods, domain discrepancy reduction
learning and semi-supervised learning, are not readily applicable when source and
target domains do not share a common label space. This paper addresses the above
scenario by learning a representation space that retains discriminative power on both
the (labeled) source and (unlabeled) target domains while keeping representations
for the two domains well-separated. Inspired by a theoretical analysis, we first
reformulate the disjoint classification task, where the source and target domains
correspond to non-overlapping class labels, to a verification one. To handle both
within and cross domain verifications, we propose a Feature Transfer Network
(FTN) to separate the target feature space from the original source space while
aligned with a transformed source space. Moreover, we present a non-parametric
multi-class entropy minimization loss to further boost the discriminative power of
FTNs on the target domain. In experiments, we first illustrate how FTN works in a
controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes
between the two domains and then demonstrate the effectiveness of FTNs through
state-of-the-art performances on a cross-ethnicity face recognition problem.
1	Introduction
Despite strong performances on facial analysis using deep neural networks (Taigman et al., 2014;
Sun et al., 2014; Schroff et al., 2015; Parkhi et al., 2015), learning a model that generalizes across
variations in attributes like ethnicity, gender or age remains a challenge. For example, it is reported
by Buolamwini & Gebru (2018) that commercial engines tend to make mistakes at detecting gender
for images of darker-skinned females. Such biases have enormous social consequences, such as
conscious or unconscious discrimination in law enforcement, surveillance or security (WIRED,
2018a;b; NYTimes, 2018; GIZMODO, 2018). A typical solution is to collect and annotate more data
along the underrepresented dimension, but such efforts are laborious and time consuming. This paper
proposes a novel deep unsupervised domain adaptation approach to overcome such biases in face
verification and identification.
Deep domain adaptation (Long et al., 2013; 2015; 2016; Tzeng et al., 2015; Ganin et al., 2016; Sohn
et al., 2017; Haeusser et al., 2017; Luo et al., 2017) allows porting a deep neural network to a target
domain without extensive labeling efforts. Currently, there are two predominant approaches to deep
domain adaptation. The first approach, domain divergence reduction learning, is motivated by the
works of (Ben-David et al., 2007; 2010). It aims to reduce the source-target domain divergence using
domain adversarial training (Ganin et al., 2016; Sohn et al., 2017; Tran et al., 2018) or maximum
mean discrepancy minimization (Tzeng et al., 2015; Long et al., 2015; 2016), while leveraging
supervised loss from labeled source examples to maintain feature space discriminative power. Since
the theoretical basis of this approach (Ben-David et al., 2007) assumes a common task between
domains, it is usually applied to a classification problem where the source and target domains share
the same label space and task definition. The second approach considers domain adaptation as a
semi-supervised learning problem and applies techniques such as entropy minimization (Grandvalet
& Bengio, 2005) or self-ensembling (Laine & Aila, 2017; Tarvainen & Valpola, 2017; French et al.,
2018) on target examples to encourage decisive and consistent predictions.
However, neither of those are applicable if the label spaces of source and target domains do not
align. As a motivating example, consider a cross-ethnicity generalization of face recognition problem,
1
Published as a conference paper at ICLR 2019
where the source ethnicity (e.g., Caucasian) contains labeled examples and the target ethnicity (e.g.,
African-American) contains only unlabeled examples. When it is cast as a classification problem, the
tasks of the two domains are different due to disjoint label spaces. Moreover, examples from different
ethnicity domains almost certainly belong to different identity classes. To satisfy such additional label
constraints, representations of examples from different domains should ideally be distant from each
other in the embedding space, which conflicts with the requirements of domain divergence reduction
learning as well as entropy minimization on target examples with source domain class labels.
In this work, we aim at learning a shared representation space between a source and target domain
with disjoint label spaces that not only remains discriminative over both domains but also keep repre-
sentations of examples from different domains well-separated, when provided with additional label
constraints. Firstly, to overcome the limitation of domain adversarial neural network (DANN) (Ganin
et al., 2016), we propose to convert disjoint classification tasks (i.e., the source and target domains
correspond to non-overlapping class labels) into a unified binary verification task. We term adaptation
across such source and target domains as cross-domain distance metric adaptation (CD2MA). We
demonstrate a generalization of the theory of domain adaptation (Ben-David et al., 2007) to our setup,
which bounds the empirical risk for within-domain verification of two examples drawn from the
unlabeled target domain. While the theory does not guarantee verification between examples from
different domains, we propose approaches that also address such cross-domain verification tasks.
To this end, we introduce a Feature Transfer Network (FTN) that separates the target features from the
source features while simultaneously aligning them with an auxiliary domain of transformed source
features. Specifically, we learn a shared feature extractor that maps examples from different domains
to representations far apart. Simultaneously, we learn a feature transfer module that transforms
the source representation space to another space used to align with the target representation space
through a domain adversarial loss. By forging this alignment, the discriminative power from the
augmented source representation space would ideally be transferred to the target representation space.
The verification setup also allows us to introduce a novel entropy minimization loss in the form
of N -pair metric loss (Sohn, 2016), termed multi-class entropy minimization (MCEM), to further
leverage unlabeled target examples whose label structure is not known. MCEM samples pairs of
examples from a discovered label structure within the target domain using an offline hierarchical
clustering algorithm such as HDBSCAN (Campello et al., 2013), computes the N -pair metric loss
among these examples (Sohn, 2016), and backpropagates the resulting error derivatives.
In experiments, we first perform on a controlled setting by adapting between disjoint sets of digit
classes. Specifically, We adapt from 0-4 of MNIST-M (Ganin et al., 2016) dataset to 5-9 of MNIST
dataset and demonstrate the effectiveness of FTN in learning to align and separate domains. Then, we
assess the impact of our proposed unsupervised CD2MA method on a challenging cross-ethnicity face
recognition task, Whose source domain contains face images of Caucasian identities and the target
domain of non-Caucasian identities, such as African-American or East-Asian. This is an important
problem since existing face recognition datasets shoW significant label biases toWards Caucasian
ethnicity, leading to sub-optimal recognition performance for other ethnicities. The proposed method
demonstrates significant improvement in face verification and identification compared to a source-only
baseline model and a standard DANN. Our proposed method also closely matches the performance
upper bounds obtained by training With fully labeled source and target domains.
2	Related Work
Research efforts in deep domain adaptation have explored a proper metric to measure the variational
distance betWeen tWo domains and subsequently regularize neural netWorks to minimize this distance.
For example, maximum mean discrepancy (Long et al., 2013; 2016; Tzeng et al., 2014; Fernando
et al., 2015; Tzeng et al., 2015; Sun & Saenko, 2016) estimates the domain difference based on
kernels. As another example, domain adversarial neural netWorks (Ganin et al., 2016; Bousmalis
et al., 2016; 2017; Sohn et al., 2017; Luo et al., 2017; Tran et al., 2018), measuring the distance
using a trainable and flexible discriminator often parameterized by an MLP, have been successfully
adopted for several computer vision applications, such as semantic segmentation (Hoffman et al.,
2016; Tsai et al., 2018; Zhang et al., 2018) and object detection (Chen et al., 2018). Most of those
Works assume a common classification task betWeen tWo domains, Whereas We tackle a cross-domain
distance metric adaptation problem Where label spaces of source and target domains are different.
Moreover, our problem setting, an adaptation from labeled source to unlabeled target With disjoint
label spaces, contains flavors from both domain adaptation (DA) and transfer learning (TL), folloWing
2
Published as a conference paper at ICLR 2019
the nomenclature of (Pan et al., 2010). The difference in input distribution between source and target
domains and the lack of labels in the target domain are similar to that of DA or transductive TL (Pan
et al., 2010), while the difference in label distribution and task definitions between two domains is
akin to inductive TL (Pan et al., 2010; DaUme III, 2007). In our work, We formalize this problem in
domain adaptation framework using verification as a common task. This is a key contribution that
allows theoretical analysis on the generalization bound as presented in Section 3 and Appendix A,
while allowing novel applications like cross-ethnicity face recognition.
In terms of task objective, (Hu et al., 2015; Ganin et al., 2016; Sohn et al., 2017) also deal with
domain adaptation in distance metric learning, but neither learns a representation space capable of
separating the source and target domains. Resembling CD2MA, Luo et al. (2017) considers domain
adaptation with disjoint label spaces, but the problem is still cast as classification with an assumption
that the target label space is known and a few labeled target examples are provided for training.
In terms of network design, residual transfer network (Long et al., 2016), which learns two classifiers
differ by a residual function for the source and the target domain, is closely related. However, it only
tackles the scenario where source and target domains share a common label space for classification.
3	Revisiting the Theory of Domain Adaptation for Verification
Under the domain adaptation assumption, Ben-David et al. (2007) show that the empirical risk on
the target domain XT is bounded by the empirical risk on the source domain XS and the variational
distance between the two domains, provided that the source and the target domains share the classifiers.
Therefore, this bound is not applicable to our CD2MA setup where the label spaces of two domains are
often different. To generalize those theoretical results to our setting, we reformulate the verification
task as a binary classification task shared across two domains. This new binary classification task
takes a pair of images as an input and predicts the label of 1 if the pair of images shares the same
identity and 0 otherwise. Furthermore, if we now define the new source domain to be pairs of source
images and the new target domain to be pairs of target images, then Theorem 1 and 2 from (Ben-David
et al., 2007) can be directly carried over to bound the new target domain binary classification error in
the same manner. That is, the empirical with-in target domain verification loss is bounded by with-in
source domain verification loss and the variational distance between XS × XS and XT × XT .1 Note
that inputs to the binary classifier are pairs of images from the same domain. Thus, this setup only
addresses adaptation of within-domain verification to unlabeled target domains.
There are two implications from the theoretical insights on domain adaptation using verification as a
shared classification task. Firstly, domain adversarial training, reducing the discrepancy between the
source and the target product spaces, coupled with supervised source domain binary classification
loss (i.e., verification loss using source domain labels) can yield target representations with high
discriminative power when performing within-domain verification. Note that in practice we approxi-
mately reduce the product space discrepancy by generic adversarial learning as done in (Ganin et al.,
2016; Sohn et al., 2017). Secondly, there is no guarantee that the aligned source and target feature
spaces possess any discriminative power for cross-domain verification task. Thus, additional actions
in the form of a feature transfer module and domain separation objective are required to address this
issue. These two consequences together motivate the design of our proposed framework, which is
introduced in the next section.
4	Feature Transfer Net: Learning to Align and S eparate Domains
In this section, we first define the CD2MA problem setup and motivate our proposed feature transfer
network (FTN). Then we elaborate on the training objectives that help our model achieve its desired
properties. Lastly, we provide practical considerations to implement our proposed algorithm.
4.1	Problem Statement and Algorithm Overview
Recall the description of CD2MA, given source and target domain data distributions XS and XT , our
goal is to verify whether two random samples x, x0 drawn from either of the two distributions (and
we do not know which distribution x or x0 come from a priori) belong to the same class.
There are 3 scenarios of constructing a pair: x, x0 ∈ XS, x, x0 ∈ XT, or x ∈ XS, x0 ∈ XT. We refer
the task of the first two cases as within-domain verification and the last as cross-domain verification.
1Mathematical details formalizing our theoretical analysis are in the Appendix A.
3
Published as a conference paper at ICLR 2019
Figure 1: Training of Feature Transfer Network (FTN) for verification, composed of feature generation module
(Gen; f), feature transfer module (Tx; g), and two domain discriminators D1 and D2 . Verification objective
Lvrf’s are applied to source (fs) pairs and transformed source (g(fs))) pairs. Our FTN applies domain adversarial
objective Ladv for domain alignment between transformed source and target domains by D1 and applies Lsep to
distinguish source domain from both target and transformed source domains by D2.
If x, x0 ∈ XS (or XT ), we need a source (or target) domain classifier2. For the source domain, we
are provided with adequate labeled training examples to learn a competent classifier. For the target
domain, we are only given unlabeled examples. However, with our extension of Theorem 1 and 2
from (Ben-David et al., 2007), discriminative power of the classifier can be transferred to the target
domain by adapting the representation spaces of XT × XT and XS × XS, that is, we can utilize the
same competent classifier from the source domain to verify target domain pairs if two domains are
well-aligned. For the third scenario where x ∈ XS but x0 ∈ XT , we assume that the two examples
cannot be of the same class, which is true for problems such as cross-ethnicity face verification.
Our proposed framework, Feature Transfer Network (FTN), is designed to solve all these verification
scenarios in an unified framework. FTN is composed of multiple modules as illustrated in Figure 1.
First, a feature generation module f : X → Z denoted as “Gen” in Figure 1 ideally maps XS and XT
to distinguishable representation spaces, that is, f(XS) and f(XT) are far apart. To achieve this, we
introduce a domain separation objective.3 Next, the feature transfer module g : Z → Z denoted as
“Tx” in Figure 1 transforms f(XS) to g(f(XS)) for it to be aligned with f(XT). To achieve this,
we introduce a domain adversarial objective. Finally, we apply verification losses on f (XS) and
g(f (XS)) using classifiers hf, hg : Z × Z → {0, 1}. During testing, we compare the metric distance
between f(x) and f(x0). Overall, we achieve the following desired capabilities:
•	If x, x0 are from different domains, f(x) and f(x0) will be far away due to the functionality of
the feature generation module.
•	If x, x0 ∈ XS, then f(x) and f(x0) will be close if they belong to the same class and far away
otherwise, due to the discriminative power acquired from optimizing hf .
•	Ifx, x0 ∈ XT, then f(x) and f(x0) will be close if they belong to the same class and far otherwise,
due to the discriminative power acquired by optimizing hg with domain adversarial training.
4.2	Training Objectives
We first define individual learning objectives of the proposed Feature Transfer Network and then
present overall training objectives of FTN. For ease of exposition, all objectives are to be maximized.
Verification Objective. For a pair of source examples, we evaluate the verification losses at two
representations spaces f (XS) and g(f (XS)) using classifiers hf and hg as follows:
Lvrf(f) = E(x1 ,x2)∈XS ×XS y12loghf(f1,f2) + (1-y12) log(1 - hf(f1, f2))	(1)
Lvrf(g) = E(x1,x2)∈XS×XS y12loghg(g1,g2) + (1-y12) log(1 - hg(g1,g2))	(2)
where gi = g(f (xi)), fi = f(xi) and y12 = 1 if x1 and x2 are from the same class and 0 otherwise.
While classifiers hf , hg can be parameterized by neural networks, we aim to learn a generator f and
g whose embeddings can be directly used as a distance metric. Therefore, we use non-parameteric
classifiers hf = σ(f>f2), hg = σ(g>g2) where σ(a) = 1+exp Ja).
2Here we use the term “classifier” to denote a prediction module for the verification of a pair.
3The term “domain separation” indicates that the representation space can be separated with respect to
domain definitions (such as, source or target). This is unrelated to Domain Separation Network (Bousmalis et al.,
2016), where it denotes the separation of the representation space into shared and private subspaces.
4
Published as a conference paper at ICLR 2019
Domain Adversarial Objective. Let D1 : Z → (0, 1) be a domain discriminator. As mentioned
earlier, D1 is trained to discriminate distributions f (XT ) and g(f (XS)) and then produces gradient
for them to be indistinguishable. The learning objectives are written as follows:
LD1 =Ex∈XSlogD1(g)+Ex∈XTlog(1-D1(f)), Ladv=Ex∈XTlogD1(f)	(3)
Note that when feature transform module is an identity mapping, i.e., g(f(x)) = f (x), Equation (3)
defines the training objective of standard DANN.
Domain Separation Objective. The goal of this objective is to distinguish between source and
target at representation spaces of generation module. To this end, we formulate the objective using
another domain discriminator D2 : Z → (0, 1):
Lsep = Ex∈Xs log D2(f) + 2 [Ex∈Xs Iog(I-D2 (O)) + Ex∈Xτ Iog(I-D2(f))]	(4)
Note that, in Lsep, the source spacef(XS) is not only pushed apart from the target spacef(XT) but
also from the augmented source space g(f(XS)) to ensure that g learns meaningful transformation of
source domain representation beyond identity transformation.
Training FTN. Now we are ready to present the overall training objectives Lf and Lg :
Lf = 2 [Lvrf(g) + Lvrf(f)] + λιLadv + λ2Lsep, Lg = Lvrf(g) + λ2Eχs log(l-D2(g))	(5)
with λ1 for domain adversarial objective and λ2 for domain separation objective. We use LD1 in
Equation (3) for D1 and LD2 = Lsep for D2. We alternate updating between D1 and (f, g, D2).
4.3	Practical Considerations
Preventing Mode Collapse via Feature Reconstruction Loss. The mode collapsing phenomenon
with generative adversarial networks (GANs) (Goodfellow et al., 2014) has received much atten-
tion (Salimans et al., 2016). In the context of domain adaptation, we also find it critical to treat the
domain adversarial objective with care to avoid similar optimization instability.
In this work, we prevent the mode collapse issue for domain adversarial learning with an additional
regularization method similar to (Sohn et al., 2017). Assuming the representation of the source
domain is already close to optimal, we regularize the features of source examples to be similar to
those from the reference network fref : X → Z, which is pretrained on labeled source data and fixed
during the training of f. Furthermore, we add a similar but less emphasized (λ4 < λ3) regularization
to target examples, simultaneously avoiding collapsing and allowing more room for target features to
diverge from the original representations. Finally, the feature reconstruction loss is written as follows:
Lrecon = -[λ3 Ex∈XS kf (x) - fref(x)k22 + λ4Ex∈XT kf(x) - fref(x)k22]	(6)
We empirically find that without the feature reconstruction loss, the training would become unstable,
reach an early local optimum and lead to suboptimal performance (see Section 6 and Appendix C).
Thus, we always include the feature reconstruction loss to train DANN or FTN models unless stated
otherwise.
Replacing Verification Loss with N -pair Loss. Our theoretical analysis in Section 3 (and Ap-
pendix A) suggests to use a verification loss that compares similarity between a pair of images. In
practice, however, the pairwise verification loss is too weak to learn a good deep distance metric.
Following (Sohn, 2016), we propose to replace the verification loss with an N -pair loss, defined as
follows:
LN (f )= E{xn,x+ }N=ι,Xn,x+∈Xs[X log Pnf )i, Pnf )= PNxPex(Xnf(X+ ))	⑺
n=1	k=1 exP(f(Xn) f(Xk ))
where Xn and Xn+ are from the same class and Xn and Xk+ , n 6= k, are from different classes. Replacing
Lvrf into LN, the training objective of FTN with N -pair loss is written as follows:
Lf = 2 [LN(g) + LN(f)] + λ1 Ladv + λ2Lsep + Lrecon, Lg = LN(g) + λ2EXs Iog(I-D2(g)) ⑻
5
Published as a conference paper at ICLR 2019
Source: MNIST-M
Target: MNIST
(a) no adaptation
(b) DANN
(c) FTN
Figure 2: t-SNE visualizations of source (0-4 from MNIST-M) and target (5-9 from MNIST) representations by
different learning methods: (a) deep neural network without adaptation, (b) domain adversarial neural network
(DANN) and (c) our feature transfer network (FTN). While domain adversarial learning results in significant
confusion of digits classes between source and target domains (e.g., 3/5, 2/8, 4/9, or 0/6 in (b)), the proposed
FTN transfers discriminative power to target domain while successfully separating them from the source domain.
5	Entropy Minimization via Hierarchical Clustering
Entropy minimization (Grandvalet & Bengio, 2005) is a popular training objective in unsupervised
domain adaptation: unlabeled data is trained to minimize entropy of a class prediction distribution
so as to form features that convey confident decision rules. However, it is less straightforward how
to apply entropy minimization when label spaces for source and target are disjoint. Motivated from
Section 3, we extend entropy minimization for distance metric adaptation using verification as a
common task for both domains:
Levnrft(f) =Ex
i,xj ∈XT pij log pij + (1 - pij ) log(1 - pij )	(9)
where pij ,pij(f) = σ(f(xi)>f(xj)). This formulation encourages a more confident prediction for
verifying two unlabeled images, whether or not coming from the same class.
However, recall that for the source domain, we use N -pair loss instead of pair-wise verification loss
for better representation learning. Therefore, we would like to similarly incorporate the concept of N-
pair loss on the target domain by forging a multi-class entropy minimization (MCEM) objective. This
demands N pair examples to be sampled from the target domain. As the target domain is unlabeled,
we ought to first discover a plausible label structure, which is done off-line via HDBSCAN (Campello
et al., 2013; McInnes et al., 2017), a fast and scalable density-based hierarchical clustering algorithm.
The returned clusters provide pseudo-labels to individual examples of the target domain, allowing us
to sample N pair examples to evaluate the following MCEM objective:
LeNnt(f)=
E{xn,xn+}nN=1,xn,xn+ ∈XT [^X { ^X Pnm logPnm}] , Pnm(f) = PN	(IO)
n=1 m=1	k=1 exp(fn fk )
where xn and xn+ are from the same cluster and xn and xk+, n 6= k are from different clusters. The
objective can be combined with Lf in Equation (8) to optimize f.
6	Experiments
In this section, we first experiment on digit datasets as a proof of concept and compare our proposed
FTN to DANN. Then, we tackle the problem of cross-ethnicity generalization in the context of face
recognition to demonstrate the effectiveness of FTN. In all experiments, we use N -pair loss as defined
in Equation (8) to update f and g for better convergence and improved performance. We also use the
same learning objectives for DANN while fixing g to the identity mapping and λ2 = 0.
6.1	PROOF OF CONCEPT: MNIST-M (0-4) TO MNIST (5-9)
To provide insights on the functionality of FTN, we conduct an experiment adapting the digits 0-4
from MNIST-M (Ganin et al., 2016) to 5-9 from MNIST. In other words, the two domains in our
setting not only differ in foreground and background patterns but also contain non-overlapping digit
classes, contrasting the usual adaptation setup with a shared label space. Our goal is to learn a feature
space that separates the digit classes not only within each domain, but also across the two.
We construct a feature generator f composed of a CNN encoder followed by two fully-connected (FC)
layers and a feature transfer module g composed of MLP with residual connections. Outputs of f and
g are then fed to discriminators D1 and D2 parameterized by MLPs to induce domain adversarial
and domain separation losses respectively. We provide more architecture details in Appendix B.1.
We visualize t-SNE plots of generator features in Figure 2. Without an adaptation (Figure 2(a)),
features of digits from the target domain are heavily mixed with those from the source domain as
6
Published as a conference paper at ICLR 2019
Verification	Identification
Model_______________________________________________________________________
	CAU	AA	EA	ALL	CAU	AA	EA	ALL
SupC	98.39	92.24	93.41	95.58	90.07	69.64	76.37	77.97
SupC,A,E	98.43	97.16	97.05	98.15	90.16	84.02	84.38	85.75
DANN\Lrecon	98.36	94.54	95.02	96.84	90.01	73.05	74.94	77.99
DANN	98.36	95.37	96.36	97.34	90.34	74.88	79.39	79.83
FTN	98.36	95.62	96.64	97.68	90.54	75.35	80.69	81.28
DANN+MCEM	98.39	96.36	97.34	97.88	90.77	80.30	83.07	82.69
FTN+MCEM	98.37	96.76	97.40	98.08	90.95	80.75	83.71	84.16
Table 1: Verification and identification accuracy on the Cross Ethnicity Faces (CEF) dataset. For supervised
models, we report results trained on labeled CAU (SupC) or on labeled CAU, AA, EA domains (SupC,A,E); for
adaptation, we evaluate DANN and FTN, without and with multi-class entropy minimization (MCEM).
Model CAU vs. AA, EA AA vs. CAU EA vs. CAU	Table 2: CroSS domain identification
-------------------------------------------------------------- accuracy on CEF, With CAU evaluated
Sup	91.67	95.42	94.87	against AA + EA combined, AA	against
DANN	89.91	84.78	91.47	CAU and EA against CAU.
FTN	92.29	88.09	92.07
Well as one another. The model reaches 1.3% verification error in the source domain but as high as
27.3% in the target domain. Though DANN in Figure 2(b) shoWs better separation With a reduced
target verification error of 2.2%, there still exists significant overlap betWeen digit classes across tWo
domains, such as 3/5, 4/9, 0/6 and 2/8. As a result, a domain classifier trained to distinguish source
and target on top of generator features can only attain 11.5% classification error. In contrast, the
proposed FTN in Figure 2(c) shoWs 10 clean clusters Without any visual overlap among 10 digits
classes from either source or target domain, implying that it not only separates digits Within the
target domain (2.1% verification error), but also differentiates them across domains (0.3% domain
classification error).
6.2	Cross Ethnicity Face Verification and Recognition
The performances of face recognition engines have significantly improved thanks to recent advances
in deep learning for image recognition (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015;
Szegedy et al., 2015; He et al., 2016) and publicly available large-scale face recognition datasets (Yi
et al., 2014; Guo et al., 2016). HoWever, most public datasets are collected from the Web by querying
celebrities, With significant label bias toWards Caucasian ethnicity. For example, more than 85%
of identities are Caucasian for CASIA Web face dataset (Yi et al., 2014). Similarly, 82% are
Caucasian (CAU) for MS-Celeb-1M (MS-1M) dataset (Guo et al., 2016), While there are only 9.7%
African-American (AA), 6.4% East-Asian (EA) and less than 2% Latino and South-Asian combined.4
Such imbalance across ethnicity in labeled training data can result in significant drop in identification
performance on data-scarce minorities: the second roW of Table 1 shoWs a model trained on Caucasian
dominated dataset performs poorly on the other ethnicities. As expected, if the training data is
composed of only Caucasian identities as source domain, the performance over the target domains
consisting of the other ethnicities further deteriorates (see roW 1 of Table 1). Provided the available
labeled source domain contains only Caucasian identities, We subsequently demonstrate that our
method can effectively leverage unlabeled data from the non-Caucasian target ethnicity to substantially
improve their face verification performances.
Experimental Setup. We perform an adaptation from CAU to a mixture of AA and EA. Our
experiments use the MS-1M dataset. We first remove identities that both appear in the training and
testing sets. The resulting training set consists of 4.04M images from 60K CAU identities, 398K
images from 7K AA identities, and 308K images from 4.6K EA identities. For domain adaptation
experiments, We use labeled CAU images and unlabeled AA, EA images for training. For supervised
experiments to obtain performance loWer and upper bound, We use labeled CAU images to train SupC
and labeled CAU, AA, EA images to train SupC,A,E .
We adopt a 38-layer ResNet (He et al., 2016) for the feature generation module. Feature transfer mod-
ule and discriminators are parameterized With MLPs similarly to Section 6.1. We use 4096-pair loss
for training, including for the supervised CNNs. It is Worth mentioning that our netWork architecture
4We ask AMT to organize the ethnicity of face images into five categories, Caucasian, African-American,
East-Asian, South-Asian and Latino. Sample annotated images are shoWn in Appendix E.
7
Published as a conference paper at ICLR 2019
and training scheme result in strongly competitive face recognition performance, comparing to other
state-of-the-art methods such as FaceNet (Schroff et al., 2015) on YouTube Faces (Wolf et al., 2011)
(97.32% (ours) vs 95.12%) and Neural Aggregation Network (Yang et al., 2017) on IJB-A (see row
2 of Table 3). The complete network architecture and training details are provided in Appendix B.2.
Evaluation. We report the performance of the baseline and our proposed models on two standard
face recognition benchmarks LFW (Huang et al., 2007) and IJB-A (Klare et al., 2015). Note that
these datasets also exhibit significant ethnicity bias.5
To highlight the effectiveness of the proposed adaptation approach, we construct individual test set
for CAU, AA, EA, each of which contains 10 face images from 200 identities. We refer to our
testing set as the Cross-Ethnicity Faces (CEF) dataset. We apply two evaluation metrics on CEF
dataset, verification accuracy and identification accuracy. For verification, following the standard
protocol (Huang et al., 2007), we construct 10 splits, each containing 900 positive and 900 negative
pairs, and compute the accuracy on each split using the threshold found from the other 9 splits.
For identification, a pair composed of the reference and the query images from the same identity
is considered correct if there is no image from different identity that has higher similarity to the
reference image than the query image. We evaluate identification accuracy per ethnicity (200-way) as
well as across all ethnicities (600-way).
Results. The results on CEF are summarized in Table 1. Cross domain identification accuracy is
reported in Table 2, where we use AA and EA as negative classes when evaluating accuracy on CAU
and vice versa, as a measure to indicate domain discrepancy. Among adaptation models, DANN
without feature reconstruction loss (DANN\Lrecon) shows unstable training and easily degenerate,
which leads to only marginal improvement upon SupC. Similar trend is observed while training FTN.
Therefore, to ensure training stability, we impose Lrecon as a regularization term for all adaptation
models. More analysis on the effectiveness of Lrecon is provided in Appendix C.
When testing on AA and EA with model trained on only the labeled source CAU domain (SupC), we
observe significant performance drops in Table 1. Meanwhile, in Table 2, cross domain identification
accuracy is much higher than within domain identification accuracy, i.e., 96.14% of AA vs. CAU
is much higher than 71.92% of AA identification in Table 1, indicating 1) significant discrepancy
between the feature spaces of the source and target domains and 2) lack of discriminative power for
within domain verification task on target ethnicity.
Comparing to SupC , both DANN and FTN show moderate improvement when testing on AA and EA
from CEF (Table 1), demonstrating the effectiveness of domain adversarial learning in transferring
within domain verification capability from labeled source domain to unlabeled target domain. Despite
the improvement, DANN suffers a notable drawback from adversarial objective which attempts to
align identities from different domains, resulting a poor cross domain identification accuracy as
shown in Table 2. In contrast, the proposed FTN achieves much higher cross domain identification
accuracy, demonstrating both within and cross domain discriminative power.
Additionally, in combination with the multi-class entropy minimization (FTN+MCEM), we further
boost the verification and identification accuracy over FTN on AA and EA as well as approach
the accuracy of SupC,A,E, the performance upper bound. This indicates that the HDBSCAN-based
hierarchical clustering provides high quality pseudo-class labels for MCEM to be effective. Indeed,
the clustering algorithm achieves F-score as high as 96.31% and 96.34% on AA and EA. We provide
more in-depth analysis on the clustering strategy in Appendix D.
Finally, Table 3 reports the performance of face recognition models on standard verification and
recognition benchmarks. We observe similar improvements with our proposed distance metric
adaptation when only using labeled CAU, i.e., source domain, as training data. Once the task
becomes more challenging thus demands more discriminative power, the advantage of our method
becomes more evident, such as in the case of open-set recognition and verification at low FAR.
7	Conclusion
We address the challenge of unsupervised domain adaptation when the source and the target domains
have disjoint label spaces by formulating the classification problem into a verification task. We
5We find that LFW dataset is composed of 84.1% of CAU, 9.4% of AA, and 6.5% of EA. IJB-A dataset is
less biased, but still with a dominating 71.6% CAU versus 8.2% AA and 10.6% EA.
8
Published as a conference paper at ICLR 2019
Model	LFW				IJB-A (verification)			IJB-A (id.)	
	VRF	CLS	0.01	0.001	0.01	0.001	0.0001	rank-1	rank-5
SupC	99.57	98.95	86.07	66.61	92.67	76.65	50.32	94.31	97.25
SupC,A,E	99.72	98.79	96.81	91.11	95.57	87.45	76.45	94.73	97.19
DANN\Lrecon	99.43	98.98	96.81	91.44	94.23	86.87	73.80	94.27	97.03
DANN	99.63	98.95	97.15	93.46	95.54	88.64	77.13	94.59	97.31
FTN	99.63	99.11	97.15	92.95	95.07	88.45	77.70	94.48	97.19
DANN+MCEM	99.63	99.08	97.65	94.97	95.28	88.78	77.30	94.75	97.30
FTN+MCEM	99.65	99.14	96.98	93.46	94.63	88.28	77.98	94.79	97.00
Table 3: Face verification and recognition performance on LFW and IJB-A. From left to right, verification
(VRF), closed-set (CLS) and open-set recognition at FAR = 0.01 and 0.001 (Best-Rowden et al., 2014) on LFW,
and verification at different FAR and identification (id.) at rank-k on IJB-A are reported.
propose a Feature Transfer Network, allowing simultaneous optimization of domain adversarial loss
and domain separation loss, as well as a variant of N -pair metric loss for entropy minimization on the
target domain where the ground-truth label structure is unknown, to further improve the adaptation
quality. Our proposed framework excels at both within-domain and cross-domain verification tasks.
As an application, we demonstrate cross-ethnicity face verification that overcomes label biases in
training data, achieving high accuracy even for unlabeled ethnicity domains, which we believe is a
result with vital social significance.
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for
domain adaptation. In NIPS, 2007. 1, 2, 3, 4
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1):151-175, 2010. 1
Lacey Best-Rowden, Hu Han, Christina Otto, Brendan F Klare, and Anubhav K Jain. Unconstrained
face recognition: Identifying a person of interest from a media collection. IEEE Transactions on
Information Forensics and Security, 9(12):2144-2157, 2014. 9
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In NIPS, 2016. 2, 4
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. In CVPR, July
2017. 2
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial
gender classification. In Conference on Fairness, Accountability and Transparency, 2018. 1
Ricardo JGB Campello, Davoud Moulavi, and Jorg Sander. Density-based clustering based on
hierarchical density estimates. In Pacific-Asia conference on knowledge discovery and data mining,
pp. 160-172. Springer, 2013. 2, 6
Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster
r-cnn for object detection in the wild. In CVPR, 2018. 2
Hal DaUme III. Domain adaptation vs. transfer learning. https://nlpers.blogspot.com/
2007/11/domain-adaptation-vs-transfer-learning.html, 2007. 3
Basura Fernando, Tatiana Tommasi, and Tinne Tuytelaars. Joint cross-domain classification and
subspace learning for unsupervised adaptation. Pattern Recognition Letters, 65:60-66, 2015. 2
Brian Forrest. Math 451 course note. 1
Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation.
In ICLR, 2018. 1
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
Journal of Machine Learning Research, 17(59):1-35, 2016. 1, 2, 3, 6
9
Published as a conference paper at ICLR 2019
GIZMODO. How apple says it prevented face id from being racist. https://gizmodo.com/
how-apple-says-it-prevented-face-id-from-being-racist-1819557448,
2018. 1
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 5
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. In ICML, 2013. 2
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NIPS,
2005. 1,6
Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. MS-Celeb-1M: A dataset
and benchmark for large scale face recognition. In ECCV, 2016. 7
Philip Haeusser, Thomas Frerix, Alexander Mordvintsev, and Daniel Cremers. Associative domain
adaptation. In ICCV, 2017. 1, 2
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016. 7
Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell. FCNs in the wild: Pixel-level adversarial
and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016. 2
Junlin Hu, JiWen Lu, and YaP-Peng Tan. Deep transfer metric learning. In CVPR, pp. 325-333, 2015.
3
Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the Wild: A
database for studying face recognition in unconstrained environments. Technical report, University
of Massachusetts, Amherst, 2007. 8
Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In Proceedings
of the Thirtieth international conference on Very large data bases-Volume 30, pp. 180-191. VLDB
EndoWment, 2004. 1
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 3
Brendan F Klare, Ben Klein, Emma Taborsky, Austin Blanton, Jordan Cheney, Kristen Allen, Patrick
Grother, Alan Mah, and Anil K Jain. Pushing the frontiers of unconstrained face detection and
recognition: IARPA Janus Benchmark A. In CVPR, 2015. 8
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification With deep convolu-
tional neural netWorks. In NIPS, 2012. 7
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017. 1
Steven P Lalley. Measure-theoretic probability I, 2017. 1
Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S Yu. Transfer feature
learning With joint distribution adaptation. In ICCV, 2013. 1, 2
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features With
deep adaptation netWorks. In ICML, 2015. 1
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation
With residual transfer netWorks. In NIPS, 2016. 1, 2, 3
Zelun Luo, Yuliang Zou, Judy Hoffman, and Li Fei-Fei. Label efficient learning of transferable
representations across domains and tasks. In NIPS, 2017. 1, 2, 3, 4
Leland McInnes, John Healy, and Steve Astels. hdbscan: Hierarchical density based clustering. The
Journal of Open Source Software, 2(11):205, 2017. 6
10
Published as a conference paper at ICLR 2019
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
ICML, 2010. 2
NYTimes.	Facial recognition is accurate, if you’re a white
guy.	https://www.nytimes.com/2018/02/09/technology/
facial- recognition- race- artificial- intelligence.html, 2018. 1
Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2010. 3
O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In BMVC, 2015. 1
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In NIPS, 2016. 5
Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified embedding for face
recognition and clustering. In CVPR, 2015. 1, 8
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015. 7
Kihyuk Sohn. Improved deep metric learning with multi-class N-pair loss objective. In NIPS, 2016.
2, 5
Kihyuk Sohn, Sifei Liu, Guangyu Zhong, Xiang Yu, Ming-Hsuan Yang, and Manmohan Chandraker.
Unsupervised domain adaptation for face recognition in unlabeled videos. In ICCV, 2017. 1, 2, 3,
5
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
ECCV Workshop, 2016. 2
Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation by joint
identification-verification. In NIPS, 2014. 1
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR,
2015. 7
Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to
human-level performance in face verification. In CVPR, 2014. 1
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results. In NIPS, 2017. 1
Luan Tran, Kihyuk Sohn, Xiang Yu, Xiaoming Liu, and Manmohan Chandraker. Joint pixel and
feature-level domain adaptation in the wild. arXiv preprint arXiv:1803.00068, 2018. 1, 2
Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan
Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018.
2
Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:
Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. 2
Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across
domains and tasks. In ICCV, 2015. 1, 2
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot
learning. In NIPS, 2016. 4
WIRED. Lawmakers can’t ignore facial recognition’s bias anymore. https://www.wired.com/
story/amazon-facial-recognition-congress-bias-law-enforcement/,
2018a. 1
11
Published as a conference paper at ICLR 2019
WIRED. When it comes to gorillas, google photos remains blind. https://www.wired.com/
story/when- it-comes-to-gorillas-google-photos-remains-blind/,
2018b. 1
Lior Wolf, Tal Hassner, and Itay Maoz. Face recognition in unconstrained videos with matched
background similarity. In CVPR, 2011. 8
Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the layers: Fast and accurate cnn object
detector with scale dependent pooling and cascaded rejection classifiers. In CVPR, 2016. 2
Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong Li, and Gang Hua.
Neural aggregation network for video face recognition. In CVPR, 2017. 8
Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv
preprint:1411.7923, 2014. 7
Xiang Yu, Feng Zhou, and Manmohan Chandraker. Deep deformation network for object landmark
localization. In ECCV, 2016. 2
Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, and Tao Mei. Fully convolutional adaptation
networks for semantic segmentation. In CVPR, 2018. 2
12
Published as a conference paper at ICLR 2019
Appendix
A Derivation for Generalization Bound of Target Domain
Verification Loss
Let (X, F) and (Z, G) be measurable input and feature spaces respectively and a feature extractor
R: X →Z be a measurable function. Let μ be a probability measure on X corresponding to the data
distribution. Let (Xi, F, μι) = (X2, F, μ2) = (X, F, μ) and μ12 = μι ×μ2 on X1×X2 be the unique
product measure (Forrest). Similarly, we construct Z1 ×Z2 where (Z1, G) = (Z2, G) = (Z, G). Since
R is measurable, R2 : X1 ×X2 → Z1 ×Z2 where R2(x1, x2) = (R(x1), R(x2)) is also measurable
(see Lemma 1 for the proof). Then we can obtain an induced probability measure for Z1 ×Z2 from
R2, denoted as μ12 = μ12 ◦ (R2)-i (Proposition 1.34 from (Lalley, 2017)).
Let Y : X1 ×X2 → {0, 1}, where 1 represents the pair from the same identity and 0 otherwise,6 be
the stochastic target function for ground truth labeling, φ(x1, x2) = E [Y (x1, x2)] be the expecta-
tion of the label at (x1, x2), and φ(z1, z2) =E [φ(x1, x2)|R(x1)=z1, R(x2)=z2] be the conditional
expectation of φ given the value of R2(x1, x2) = (z1, z2). Now, consider two domains, namely
the source domain with probability measure μs over X and induced probability measure μs over
Z1×Z2, as well as target domain counterparts μτ and μτ. Provided with a deterministic hypothesis
class H ⊆ {g : Z1 ×Z2 → {0, 1}} of VC-dimension d, suppose there exists a function h ∈ H that can
predict both source and target domains reasonably well. Then, we can quantify φ to be λ-close to H:
inf es(h) + eτ(h) ≤ λ, where ∈i(h) = / ∣φ(zι, z2) - h(zi, z2)∣dμi.
h∈H
We are ready to define the variational distance between the two domains with respect to H:
dH(μS,μτ) = 2 SuP ∣μs(A) - μτ(A)∣, A = {Ah = {(z1,z2) ∈Z1×Z2 : h(z1,z2) = 1},h eH}.
A∈A
So far, we have successfully prepared the components in our verification setup to meet the assumptions
and the format required by Theorem 1 from (Ben-David et al., 2007). We may now directly apply the
theorem:
Theorem 1. Randomly sample a labeled set of size m by applying R2 to samples from X1 ×X2 with
labels defined according to Y , with probability at least 1 - δ, ∀h ∈ H
√4 ― ^^2m	^ ^^^ ^^4.
一(dlog ——+ d + log —) + dH(μ , μ ) + λ∙
md	δ
Furthermore, dχ (μs,μτ) can be empirically approximated by finite samples from both domain (Kifer
et al., 2004), using the binary classifier from H that can best distinguishes pairs of samples between
〜
〜
two domains. Following Theorem 2 from (Ben-David et al., 2007), let US and UT consist of n
random pairs of samples from source and target each, with probability at least 1 - δ, we have :
dH(μS, μT) ≤ dH(USET) + j
d log(2n) + log 4
n
where dHUs ,Uτ )=2(1 - 2minh∈H 2n p2=ι ∣h(zι,i, z2,i) - 1{(zι,i, z2,i) ∈ US }∣).
For completeness of our analysis, we formalize and prove in Lemma 1 that R2 is measurable.
Lemma 1. Let (X, F, μ) and (Z, G, μ) be measurable spaces and let (X X X, σ(F X F), μ X μ),
(Z XZ, σ(G x G), μ x μ) be their product spaces with the product measures. Let R: X →Z be a
measurable function, then R2 : X X X → Z X Z where R2(x1, x2) = (R(x1), R(x2)) is also mea-
surable.
Proof. As the σ-algebra of Z X Z is generated by G X G, we only need to show that the
pre-image of any generator is measurable. Let G1 X G2 ∈ G X G, then it is easy to see that
(R2)-1(G1 x G2) = RT(GI)X RT(G2). Since R is a measurable function, hence RT(GI) and
RT(G2) are measurable and so is RT(GI) X RT(G2) measurable.	□
6Rigorously, Y (x1, x2) is a Bernoulli random variable with outcome space containing 0 and 1.
1
Published as a conference paper at ICLR 2019
Figure S1: Network architecture of feature transfer module and domain discriminators.
operation	kernel	output size
Conv1-1 +ReLU	3×3	32×32×32
Conv1-2 + ReLU	3×3	32×32×32
max pooling	2×2	16x16x32
Conv2-1 + ReLU	3×3	16x16x64
Conv2-2 + ReLU	3×3	16x16x64
max pooling	2×2	8x8x64
Conv3-1 +ReLU	3×3	8x8x128
Conv3-2 + ReLU	3×3	8x8x128
max pooling	2×2	4x4x128
FC1 + ReLU	—	128
FC2	—	128
Normalize and Scale (2)	—	128
Table S1: Network architecture for digit experiments.
B Network Architecture and Training Details
B.1	TOY EXPERIMENTS: MNIST-M (0 - 4) TO MNIST (5 - 9)
Following (Haeusser et al., 2017), we preprocess the data by subtracting a channel-wise pixel mean
and dividing by channel-wise standard deviation of pixel values. For MNIST examples, we also apply
color-intensity inversion. All images are resized into 32×32 with 3 channels.
Our feature generator module is composed of 6 convolution layers and 3 max-pooling layers followed
by 2 fully-connected layers. We use ReLU (Nair & Hinton, 2010) after convolution layers. The
output dimension of the feature generator module is 128 and is normalized to have L2-norm of 2.
The full description of the generator module is in Table S1.
The feature transfer module maps 128 dimensional vector into the same dimensional vector using
two fully-connected layers (128 - 256 - 256 - 128) and residual connection as in Figure 1(a).
Discriminator architectures are similar to that in Figure 1(b) but with fully-connected layers whose
output dimensions are 128 instead of 256.
We use Adam stochastic optimizer with learning rate of 0.0003, λ1 = 0.3 and λ2 = 0.03 to train FTN.
B.2	Cross Ethnicity Face Verification and Recognition
Our experimental protocols, such as data preprocessing and network architecture, closely follow
those of (Sohn et al., 2017). We preprocess face images by detecting (Yang et al., 2016), aligning (Yu
et al., 2016), and cropping to provide face images of size 110 × 110. The data is prepared for network
training by random cropping into 100 × 100 with horizontal flip with a 50% chance and converting
into gray-scale.
Our feature generation module contains 38 layers of convolution with several residual blocks and
max pooling layers. We use ReLU (Nair & Hinton, 2010) for most of the layers in combination with
maxout nonlinearities (Goodfellow et al., 2013). We add 7 × 7 average pooling layer on top of the
last convolution layer. The output of the feature generation module is 320 dimensional vector and is
normalized to have L2-norm of size 12. The full description of the model is in Table S2.
2
Published as a conference paper at ICLR 2019
The feature transfer module maps 320 dimensional output vector from feature generation module
into the same dimensional vector using two fully-connected layers and residual connection. The
architecture of feature transfer module is described in Figure 1(a). Discriminators have similar
network architecture besides different numbers of neurons and omitted residual connection.
All models, including supervised CNNs (SupC, SupC,A,E), are trained with 4096-pair loss. For SupC
and SupC,A,E, we use Adam stochastic optimizer (Kingma & Ba, 2015) with the learning rate of
0.0003 for the first 12K updates and 0.0001 and 0.00003 for the next two subsequent 3K updates.
Our feature generation module is initialized with the SupC model, which is also used as a reference
network for feature reconstruction loss as described in Section 4.3. Other modules of our model,
such as feature generation module and discriminators, are initialized randomly. All modules are then
updated with the learning rate of 0.00003. Hyperparameters of different models are summarized in
Table S3.
operation	kernel	output size
Conv1-1 + ReLU	3×3	100x100x32
Conv1-2 + Maxout (2)	3×3	100x100x64
max pooling	2×2	50x50x64
ResBlock + ReLU ×2	3×3, 64 - 64 - 64	50x50x64
Conv2 + Maxout (2)	3×3	50x50x128
max pooling	2×2	25x25x128
ResBlock + ReLU ×4	3×3, 128 - 96 - 128	25x25x128
Conv3 + Maxout (2)	3×3	25x25x192
max pooling	2×2	13x13x192
ResBlock + ReLU ×8	3×3, 192 - 128 - 192	13x13x192
Conv4 + Maxout (2)	3×3	13x13x256
max pooling	2×2	7x7x256
ResBlock + ReLU ×2	3×3, 256 - 160 - 256	7x7x256
Conv5 + Maxout (2)	3×3	7x7x320
avg pooling	7×7	1x1x320
Normalize and Scale (12)	一	320
Table S2: Network architecture for face experiments.
	λι	λ2	λ3	λ4
DANN	0.1	—	0.1	0.01
FTN	0.03	0.1	0.03	0.01
FTN+MCEM	0.03	0.1	0.03	0.003
Table S3: Optimal hyperparameter settings of different adaptation models.
C Impact of Feature Reconstruction Loss on Domain Adversarial
Training
We demonstrate the effectiveness of feature reconstruction loss in stabilizing the domain adversarial
training in DANN framework. We train four different DANN models with different configurations of
λ3 and λ4 . We visualize in Figure S2 the performance curves of identification accuracy evaluated
on the AA, EA, and CAU ethnicities of CEF dataset. Note that we stop training early on when
the performance start to degrade significantly. Therefore, x-axis, the number of training epoch, of
different curves are different. y-axis represents the identification accuracy.
As we see in Figure S2, the performance of all models on the target ethnicities start to improve in
the beginning of training from those of the pretrained reference network. Soon after, however, the
accuracy starts to drop when values of either λ3 or λ4 are set to 0. Note that even in that situation
the performance on the CAU set still remains high, which implies the failure of discriminative
3
Published as a conference paper at ICLR 2019
information transfer. On the other hand, our proposed feature reconstruction loss with non-zero
values of λ3 and λ4 (Figure 2(d)) shows much more stable performance curve. Nonetheless, values of
λ3 and λ4 should be carefully selected since the feature generation module of DANNs or FTNs will
remain almost the same to the reference network when they are set too strong and the effectiveness of
the domain adversarial loss will be reduced. In our experiment, we use λ3 = 0.1 and λ4 = 0.01 for
DANN, λ3 = 0.03 and λ4 = 0.01 for FTN. For FTN with entropy minimization we further reduce
λ4 = 0.003 to give more flexibility in updating model parameters based on entropy loss.
Figure S2:	Performance curves of identification accuracy per ethnicity subset on the CEF datasets. The accuracy
of DANNs with different values of λ3 for λ4 are visualized.
D Performance of Unsupervised Hierarchical Clustering
In this section, we provide analysis on the performance of our clustering strategy by measuring the
clustering accuracy. Specifically, we measure the verification precision and recall as follows:
Precision =
Σ2χ1 ,χ2 ∈XT 1{y1 = y2 ,y1 = y2 }
Σχι,X2∈Xτ 1{y1 = y2}
Recall =
Σ2χ1,χ2∈Xτ 1{y1 = y2, y1 = y2}
x1,x2	∈XT 1{y1 = y2}
(S1)
where yi is the ground-truth class label of an example Xi, and yi is an index of an assigned cluster.
Precision computes the proportion of positive pairs among pairs assigned to the same cluster, i.e.,
purity of returned clusters, and recall computes the proportion of positive pairs assigned to the same
cluster. Ideally, we expect high precision and high recall, i.e., high F-score, to ensure examples with
the same class labels are assigned to the same cluster. Note that we only use clusters of size 5 or
larger as new target classes and discard examples assigned to a cluster whose size is less than 5.
Here, in addition to our proposed clustering strategy, we also evaluate the clustering performance
that clusters target examples by finding a nearest classes or examples from the source domain, which
are shown to be effective for zero-shot learning (Vinyals et al., 2016) or semi-supervised domain
adaptation with disjoint source and target classes (Luo et al., 2017). In this case, we call two examples
from the target domain are assigned to the same cluster if the nearest source examples are the same.
We also measure the clustering performance by matching the nearest source classes.
The summary result is provided in Table S4. Firstly, we observe extremely low precision when using
source domain examples or clusters as a proxy to relate target examples. We believe that this idea of
“clustering by finding the nearest source classes” works under a cross-category similarity assumption
between disjoint classes of source and target domains. In other words, it assumes that there exists
a certain source class closer to examples from certain target class, so that those examples from the
same target class can be clustered around that source class, even though those matching source and
target classes are indeed different (e.g., 3/5, 2/8, 4/9, and 0/6 in Section 6.1). Unfortunately, such an
assumption does not hold for our problem, maybe due to the huge number of identity classes (60K)
in the source domain.
On the other hand, using hierarchical clustering on target features achieves significantly higher
precision and recall. Especially, when using embedding vectors of SupC, we achieve 100% precision,
which means that all clusters are pure even though some ground-truth classes might be separated
into multiple clusters. We observe slightly lower precision using FTN features but much higher
recall, achieving higher F-score overall. Further, the number of examples returned with FTN feature
(253K and 195K for AA and EA, respectively) is higher than with SupC feature (217K and 165K).
Repeating the process using feature of FTN+MCEM model improves the F-score while returning
more target examples that are with cluster assignment (276K and 214K). This not only shows the
4
Published as a conference paper at ICLR 2019
	source example		source center		HDBSCAN		FTN		FTN+MCEM	
	AA	EA	AA	EA	AA	EA	AA	EA	AA	EA
Precision	0.11	0.12	0.30	0.08	100	100	95.36	96.23	95.79	96.10
Recall	25.64	31.11	22.54	48.60	88.66	81.74	97.29	96.46	97.81	96.89
F-score	0.22	0.25	0.58	0.16	93.99	89.95	96.31	96.34	96.79	96.49
Table S4: Verification precision and recall of clustering methods, such as projection to source example or
source class center, or hierarchical clustering using embeddings of SupC (HDBSCAN) or our proposed FTN
model. Furthermore, we repeat the clustering using the FTN with multi-class entropy minimization model
(FTN+MCEM) and report the clustering accuracy.
improved discriminative quality of features by FTNs, but also suggests a potential tool for automatic
labeling of unlabeled data by iterative training of FTN model and hierarchical clustering.
5
Published as a conference paper at ICLR 2019
E Visualization of Ethnicity Annotated Image Samples
We visualize few images from each ethnicity subset in Figure S3 for annotation quality assurance.
(a) Caucasian
(b) African-American
(c) East-Asian
Figure S3:	Face images of Caucasian, African-American, and East-Asian sampled from MS-1M dataset.
6