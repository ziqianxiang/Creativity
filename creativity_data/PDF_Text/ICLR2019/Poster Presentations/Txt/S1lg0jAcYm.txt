Published as a conference paper at ICLR 2019
ARM: Augment-REINFORCE-Merge Gradient
for Stochastic B inary Networks
Mingzhang Yin
Department of Statistics and Data Sciences
The University of Texas at Austin
Austin, TX 78712
mzyin@utexas.edu
Mingyuan Zhou*
Department of IROM, McCombs School of Business
The University of Texas at Austin
Austin, TX 78712
mingyuan.zhou@mccombs.utexas.edu
Ab stract
To backpropagate the gradients through stochastic binary layers, we propose the
augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low
variance, and has low computational complexity. Exploiting variable augmenta-
tion, REINFORCE, and reparameterization, the ARM estimator achieves adaptive
variance reduction for Monte Carlo integration by merging two expectations via
common random numbers. The variance-reduction mechanism of the ARM estima-
tor can also be attributed to either antithetic sampling in an augmented space, or the
use of an optimal anti-symmetric “self-control” baseline function together with the
REINFORCE estimator in that augmented space. Experimental results show the
ARM estimator provides state-of-the-art performance in auto-encoding variational
inference and maximum likelihood estimation, for discrete latent variable models
with one or multiple stochastic binary layers. Python code for reproducible research
is publicly available.
1	Introduction
Given a function f(z) of a random variable z = (z1, . . . , zV )T, which follows a distribution qφ(z)
parameterized by φ, there has been significant recent interest in estimating φ to maximize (or
minimize) the expectation of f (Z) with respect to Z 〜qφ(z), expressed as
E (φ) = Rf (z)qφ(z)dz = Ez^qφ(z)[if (Z)].	⑴
In particular, this expectation objective appears in both maximizing the evidence lower bound (ELBO)
for variational inference (Jordan et al., 1999; Blei et al., 2017) and approximately maximizing the
log marginal likelihood of a hierarchal Bayesian model (Bishop, 1995), two fundamental problems
in statistical inference. To maximize (1), if Rz f (Z) is tractable to compute and Z 〜qφ(z) can be
generated via reparameterization as Z = 7φ(e), E 〜p(e), where E are random noises and Tφ(∙)
denotes a deterministic transform parameterized by φ, then one may apply the reparameterization
trick (Kingma & Welling, 2013; Rezende et al., 2014) to compute the gradient as
VφE (φ) = RφΕsp(e) [f(Tφ(E))] = ESpgVφf(Tφ(E))].	⑵
This trick, however, is often inapplicable to discrete random variables, as widely used to construct
discrete latent variable models such as sigmoid belief networks (Neal, 1992; Saul et al., 1996).
To maximize (1) for discrete z, using the score function Vφ log qφ(Z) = Vφqφ(Z)∕qφ(Z), one may
compute VφE(φ) via REINFORCE (Williams, 1992) as
vφe(φ) = Ez~q#(z)[f(Z)Ve log 9φ(z)] ≈ -1 XK 1 f (Z(JkkU log qφ(Zw,
K	k=1
where z(k) iid qφ(z) are independent, and identically distributed (iid). This unbiased estimator is also
known as (a.k.a.) the score-function (Fu, 2006) or likelihood-ratio estimator (Glynn, 1990). While it is
unbiased and only requires drawing iid random samples from qφ(Z) and computing Vφ log qφ(Z(k)),
* Corresponding author.
1
Published as a conference paper at ICLR 2019
its high Monte-Carlo-integration variance often limits its use in practice. Note that if f (z) depends
on φ, then We assume it is true that Ez〜勺卓⑶[Vφf (z)] = 0. For example, in variational inference, We
need to maximize the ELBO as Ez 〜q@(z)[f (z)], where f (Z) = log[p(x | z)p(z)∕qφ(z)]. Inthiscase,
although f(z) depends on φ, as Ez〜q#(z)[Vφ log qφ(z)] = R Vφqφ(z)dz = Vφ R qφ(z)dz = 0,
we have Ez〜qΦ(z)[Vφf (z)] =0.
To address the high-variance issue, one may introduce an appropriate baseline (a.k.a. control variate)
to reduce the variance of REINFORCE (Paisley et al., 2012; Ranganath et al., 2014; Mnih & Gregor,
2014; Gu et al., 2016; Mnih & Rezende, 2016; Ruiz et al., 2016; Kucukelbir et al., 2017; Naesseth
et al., 2017). Alternatively, one may first relax the discrete random variables with continuous ones
and then apply the reparameterization trick to estimate the gradients, which reduces the variance of
Monte Carlo integration at the expense of introducing bias (Maddison et al., 2017; Jang et al., 2017).
Combining both REINFORCE and the continuous relaxation of discrete random variables, REBAR of
Tucker et al. (2017) and RELAX of Grathwohl et al. (2018) both aim to produce a low-variance and
unbiased gradient estimator by introducing a continuous relaxation based baseline function, whose
parameters, however, need to be estimated at each mini-batch by minimizing the sample variance of
the estimator with stochastic gradient descent (SGD). Estimating the baseline parameters often clearly
increases the computation. Moreover, the potential conflict, between minimizing the sample variance
of the gradient estimate and maximizing the expectation objective, could slow down or even prevent
convergence and increase the risk of overfitting. Another interesting variance-control idea applicable
to discrete latent variables is using local expectation gradients, which estimates the gradients based
on REINFORCE, by performing Monte Carlo integration using a single global sample together with
exact integration of the local variable for each latent dimension (Titsias & Lazaro-Gredilla, 2015).
Distinct from the usual idea of introducing baseline functions and optimizing their parameters to
reduce the estimation variance of REINFORCE, we propose the augment-REINFORCE-merge
(ARM) estimator, a novel unbiased and low-variance gradient estimator for binary latent variables
that is also simple to implement and has low computational complexity. We show by rewriting the
expectation with respect to Bernoulli random variables as one with respect to augmented exponential
random variables, and then expressing the gradient as an expectation via REINFORCE, one can derive
the ARM estimator in the augmented space with the assistance of appropriate reparameterization. In
particular, in the augmented space, one can derive the ARM estimator by using either the strategy of
sharing common random numbers between two expectations, or the strategy of applying antithetic
sampling. Both strategies, as detailedly discussed in Owen (2013), can be used to explain why
the ARM estimator is unbiased and could lead to significant variance reduction. Moreover, we
show that the ARM estimator can be considered as improving the REINFORCE estimator in an
augmented space by introducing an optimal baseline function subject to an anti-symmetric constraint;
this baseline function can be considered as a “self-control” one, as it exploits the function f itself
and correlated random noises for variance reduction, and adds no extra parameters to learn. This
“self-control” feature makes the ARM estimator distinct from both REBAR and RELAX, which rely
on minimizing the sample variance of the gradient estimate to optimize the baseline function.
We perform experiments on a representative toy optimization problem and both auto-encoding varia-
tional inference and maximum likelihood estimation for discrete latent variable models, with one or
multiple binary stochastic layers. Our extensive experiments show that the ARM estimator is unbiased,
exhibits low variance, converges fast, has low computation, and provides state-of-the-art out-of-sample
prediction performance for discrete latent variable models, suggesting the effectiveness of using the
ARM estimator for gradient backpropagation through stochastic binary layers. Python code for repro-
ducible research is available at https://github.com/mingzhang-yin/ARM-gradient.
2	ARM: Augment-REINFORCE-merge estimator
In this section, we first present the key theorem of the paper, and then provide its derivation. With this
theorem, we summarize ARM gradient ascent for multivariate binary latent variables in Algorithm 1,
as shown in Appendix A. Let us denote σ(φ) = eφ∕(1 + eφ) as the sigmoid function and 1[∙] as an
indicator function that equals to one if the argument is true and zero otherwise.
Theorem 1 (ARM). For a vector of V binary random variables z = (z1, . . . , zV )T, the gradient of
E(O)= Ez-QV=I Bern。Uui(Zvb(φv))[f (Z)I	⑶
2
Published as a conference paper at ICLR 2019
with respect to φ = (φ1, . . . , φV )T, the logits of the Bernoulli probability parameters, can be
expressed as
VφE (φ) = Eu 〜QV=
1 Uniform(uv ;0,1) [(f(1[u>σ(-φ)]) - f (1[u<σ(φ)])(u - 1/2) ,	(4)
where 1[u>σ( — φ)] : = (1[υι>σ( — φι)],..., 1[uv >σ(-φv)]) •
2.1	Univariate ARM estimator
Below we will first present the ARM estimator for a univariate binary latent variable (i.e., V = 1),
and then generalize it to a multivariate one (i.e., V > 1). In the univariate case, we need to evaluate
the gradient of E(φ) = Ez〜BernOUUi(σ(φ)) [f (z)] With respect to φ, which has an analytic expression as
VφE(φ) = Vφ[σ(φ)f(1) + σ(-φ)f(0)] = σ(φ)σ(-φ)[f(1) - f(0)].	(5)
Since R0σ(φ)(1 - 2u)du = σ(φ)σ(-φ) and Rσ1(φ)(1 - 2u)du = -σ(φ)σ(-φ), we can rewrite (5) as
VφE(φ) = R0σ(φ) f(1)(1 - 2u)du + Rσ1(φ) f(0)(1 - 2u)du = R01 f(1[u<σ(φ)])(1 - 2u)du
=Eu 〜UnifOrm(0,1)[f (I[u<σ(φ)] )(I - 2u)].	(6)
We refer to (6) as the univariate augment-REINFORCE (AR) estimator, as we initially derived it by
combining variable augmentation (Tanner & Wong, 1987; Van Dyk & Meng, 2001), REINFORCE,
and Rao Blackwellization (Casella & Robert, 1996); we defer the details to Appendix B.
Applying antithetic sampling (Owen, 2013) to the AR estimator in (6), with u = 1 一 u,we have
vφE(O)= Eu〜UnifOrm(0,1)f (I[u<b(0)])(1/2 一 U)]+ Eu〜UnifOrm(0,1)[f (I[u<σ(φ)] )(1/2 - U)]
=Eu〜UnifOrm(0,1)[f (1[u<σ(φ)])(1∕2 — U)+ f (1[u<σ(φ)] )(1/2 - U)]
=Eu〜UnifOrm(0,1) [(f (I[u>σ(-φ)] ) - f (I[u<σ(φ)] )) (U - 1/2)],	⑺
which provides the proof for Theorem 1 for V = 1. We refer to (7) as the univariate ARM estimator,
as we initially derived it by improving the AR estimator with an additional merge step, which shares
random numbers between two expectations to reduce the variance of Monte Carlos integration; we
defer the details to Appendix B.
Note that since Eu〜UnifOrm(0,1)[f (1[u<σ(φ)] )(1/2 一u)] = 一Eu〜UnifOrm(0,1)[f (1[u>σ(-φ)] )(1/2 一u)],
we have Eu 〜UnifOrm(0,1) [(f (1[u<σ(φ)]) + f(1[u>σ(-0)]))(1/2 - u)] = 0, subtracted which from the
AR estimatOr in (6) leads tO the ARM estimatOr in (7). FOr this reasOn, we can alsO cOnsider the
ARM estimatOr as the AR estimatOr subtracted by a zerO-mean baseline functiOn as
b(U) = (f (I[u<σ(φ)]) + f(1㈠6(-0)1))(1/2- u).
This baseline functiOn is distinct frOm previOusly prOpOsed Ones in being parameterized by the
function f itself over two correlated binary latent variables and satisfying b(〃) = -b(1 — u). From
this pOint Of view, the ARM estimatOr can be cOnsidered as a “self-cOntrOl” gradient estimatOr that
exploits the function f itself to control the variance of Monte Carlo integration .
2.2	Multivariate generalization via the law of total expectation
Although the ARM estimator for univariate binary is of little use in practice, as the true gradient,
shown in (5), has an analytic expression, it serves as the foundation for generalizing it to multivariate
settings. Let us denote (∙)∖v as a vector whose vth element is removed. For the expectation in (3),
applying the univariate ARM estimator in (37) together with the law of total expectation, we have
vφvE(φ) = Ez\v~Qj=v Bernoulli(zj∙,σ(φj)){V0vEzv~Bernoulli(σ(φv)[f (Z)]}
Ez\v〜Qj=v Bernoulli(zj∖σ(φj)) {Euv〜UnifOrm(0,1) [(Uv 一 1/2)
× (f (Z\v ,zv = 1[uv >σ(-φv )] ) 一 f (Z\v ,zv = 1[uv <σ(φv )]))]卜
(8)
Since z\v 〜Ilj=V Bernoulli(zj; σ(φj)) can be equivalently generated as z\v = 1[uχv<σ(φ`j] or
as z\V = 1[u∖v>σ(-φ∖°)], where u\v 〜 Qj=V UnifOrm(Uj；0,1), exchanging the order of the two
3
Published as a conference paper at ICLR 2019
expectations in (8) and applying reparameterization, we conclude the proof for (4) of Theorem 1 with
▽°v E(φ) = Euv~Uniform(0,1) {(Uv - 1/2) Ez∖v~Qj=v BernoUlli(ZjRφj))[
f (z\v , zv = 1[uv >σ(-φv)] ) - f (z\v , zv = 1[uv <σ(φv )] )
一Eu〜Qv=ι Uniform(uv;0,1)[(Uv - 1/2)f(z\v = 1[u∖v>σ(-φ∖v)],zv = 1[uv>σ(-φv)])]
-Eu〜QV=I Uniform(uv;0,1)[(uv - 1/2)f (Z\v = 1[u∖v <σ(φ∖v )],zv = 1[uv <σ(φv )])]
=Eu〜QV=I Uniform(uv ;0,1) [(uv - 1/2) (f (1[u>σ(-φ)]) - f(1[u<σ(φ)])	.	(9)
Alternatively, instead of generalizing the univariate ARM gradient as in (8) and (9), we can first do a
multivariate generalization of the univariate AR gradient in (6) as
NΦvE(φ) = Ez\v〜Qj=v BernoUUi(ZjEφj∙)){^φvEzv〜BernoUUi(σ(φv)[f (z)]}
=Ez\v~Qj∙=v BernoUUi(zj∖σ(φj)) {Euv~Uniform(0,1) [(1 - 2uv)f(z∖v,zv = 1[uv<σ(φv)])]}
=Eu〜QV=I Uniform(uv;0,1) [(1 — 2〃v)f (1[u<σ(φ)] )] .	(10)
The same as the derivation of the Univariate ARM estimator, here we can arrive at (4) from (10) by
either adding an antithetic sampling step, or sUbtracting the AR estimator by a baseline fUnction as
b(u) = f(1[u<σ(φ)]) + f(1[u"(F)]))(1/2 - u),	(11)
which has zero mean, satisfies b(u) = -b(1 - u), and is distinct from previoUsly proposed baselines
in taking advantage of “self-control” for variance redUction and adding no extra parameters to learn.
2.3 Effectiveness of ARM for variance reduction
For the Univariate case, we show below that the ARM estimator has smaller worst-case variance than
REINFORCE does. The proof is deferred to Appendix C.
Proposition 2 (Univariate gradient variance). For the objective function EZ〜BernoUUi(σ(φ))[f (z)],
with a single Monte-Carlo sample U 〜 Uniform(0,1) or Z 〜 Bernoulli(σ(φ)), the ARM gradient
is expressed as gARM(U, φ) = f (1[u>σ(-φ)] ) - f (1[u<σ(φ)] ) (U - 1/2), and the REINFORCE
gradient as gR(z, φ) = f(z)Nφ logBernoUlli(z; σ(φ)) = f(z)(z - σ(φ)). Assuming f ≥ 0 (or
f ≤	0)	then supφvar[gARM (u,φ)]	≤ 16	(1	_ 2 f(0)	)2 ≤ 16
f ≤	0),	then suPφVar[gR(u,φ)]	≤ 25	(1	— 2 f(0) + f(1) ) ≤ 25 '
In the general setting, with U(I),..., U(K) R QV=I UnifOrm(0,1), We define the ARM estimate of
Nφv E (φ) with K Monte Carlo samples, denoted as gARMK,v, and the AR estimate with 2K Monte
Carlo samples, denoted as gAR2K,v, Using
gARMκ,v = 2K PK=1(gv (u(k)) + gv (1 — u(k))), gAR2κ,v = 2K Pk=I gv (u® ),	(12)
where gv(U(k)) = f(1[u(k) <σ(φ)] )(1 — 2U(vk)). Similar to the analysis in Owen (2013), the amoUnt
of variance redUction broUght by the ARM estimator can be reflected by the ratio as
var[gARMK ,v] = var[gv (u)] — Cov(-gv (u),gv (1 — u)) = 1-夕
var[gAR2κ ,v]	var[gv (u)]	v
ρv = Corr(—gv (U), gv(1—U)).
Note - gv (U) = f (1[u<σ(φ)])(2uv-1), gv (I-U) = f (1[u>σ(-φ)] )(2uv - 1), and P (1[uv <σ(φv )]=
1[uv>σ(-φv)]) = σ(∣φv|) - σ(-∣φv|). Therefore a strong positive correlation (i.e., Pv → 1) and
hence noticeable variance redUction are likely, especially if φv moves far away from zero dUring
training. Concretely, we have the following proposition.
Proposition 3 (Variance redUction). For the ARM estimate gARMK,v and AR estimate gAR2K,v shown
in (12), the variance of gARMK ,v is guaranteed to be lower than that of gARK ,v; moreover, if f ≥ 0
(or f ≤ 0), then the variance of gARMK,v is guaranteed to be lower than that of gAR2K,v.
We show below that Under the anti-symmetric constraint b(U) = -b(1 - U), which implies that
Eu〜QV 1 Uniform(uτ, ；o,i) [b(υ)] is a vector of zeros, Equation (11) is the optimal baseline function to
be sUbtracted from the AR estimator for variance redUction. The proof is deferred to Appendix C.
4
Published as a conference paper at ICLR 2019
Proposition 4 (Optimal anti-symmetric baseline). For the gradient of Ez〜勺卓⑶[f (z)], the optimal
anti-symmetric baseline function to be subtracted from the AR estimator gAR(u) = f (1[u<σ(φ)] )(1 -
2u), which minimizes the variance of Monte Carlo integration, can be expressed as
arg min var[gAR,v(U) - K(u)] = 1(gAR,v(U) - gAR,v(1 - u)),	(13)
bv(u)∈B	2
where B = {b(U) : bv (U) = -bv (1 - U) for all v} is the set of all zero-mean anti-symmetric
baseline functions. Note the optimal baseline function shown in (13) is exactly the same as (11),
which is subtracted from the AR estimator in (10) to arrive at the ARM estimator in (4).
Corollary 5 (Lower variance than constant baseline). The optimal anti-symmetric baseline function
for the AR estimator, as shown in (13) and also in (11), leads to lower estimation variance than any
constant based baseline function as bv (U) = cv (1/2 - uv), where cv is a dimension-specific constant
whose value can be optimized for variance reduction.
3 Backpropagation through discrete stochastic layers
A latent variable model with multiple stochastic hidden layers can be constructed as
X 〜Pθo (x | bi), bi 〜Pθι (bl | b2),..., b 〜pθt (b | bt+ι),..., bτ 〜Pθτ(bτ),	(14)
WhoSejoint likelihood given the distribution parameters。0：T = {θo,..., θτ} is expressed as
p(x, bi:T | θθ:T) = Pθo (x | bi) h 口二 pθt (bt | bt+i)ipθT (bT).	(15)
In comparison to deterministic feedforWard neural netWorks, stochastic ones can represent complex
distributions and shoW natural resistance to overfitting (Neal, 1992; Saul et al., 1996; Tang & Salakhut-
dinov, 2013; Raiko et al., 2014; Gu et al., 2016; Tang & Salakhutdinov, 2013). HoWever, the training
of the netWork, especially if there are stochastic discrete layers, is often much more challenging.
BeloW We shoW for both auto-encoding variational inference and maximum likelihood estimation,
hoW to apply the ARM estimator for gradient backpropagation in stochastic binary netWorks.
3.1 ARM variational auto-encoder
For auto-encoding variational inference (Kingma & Welling, 2013; Rezende et al., 2014), We construct
a variational distribution as
qw1:T (bi:T | x)
qw1(bi | x)h YtT=-iiqwt+1 (bt+i | bt)i,
With Which the ELBO can be expressed as
E(W1:T) = EbLT〜qw1：T (bi：T |x) [f (bi：T)] , where
f(bi:T) = log Pθo (x | bi) + log Pei：T (bi：T) - log Qw、：T (bi：T | x).
(16)
(17)
Proposition 6 (ARM backpropagation). For a stochastic binary network with T binary stochastic
hidden layers, constructing a variational auto-encoder (VAE) defined with b0 = x and
qwt (bt | bt-i) = Bernoulli(bt; σ(Twt (bt-i)))	(18)
for t = 1, . . . , T, the gradient of the ELBO with respect to wt can be expressed as
Vwt E (Wi:T )= Eq(bi：t-i) [Eut 〜UnifOrm(0,1)[f∆(Ut, Twt &-i), bi:t-i)(ut - 1∕2)Nwt Twt (bt-i)],
where f△(%, Twt (bt —i), bi：t—i) = Ebt+1:T ^q(bt + 1:T | bt), bt = 1[ut>σ(-Twt (bt-ι))] [f (bi:T )]
-Ebt + 1:T ^q(bt + 1:T | bt), bt = 1[ut<σ(Twt (bt-i))] [f(bi:T)].	(19)
The gradient presented in (19) can be estimated with a single Monte Carlo sample as
^"T (b' .) b..» ʃ0,	if bti)= b(2),
, wt ,	:	f(bi:t-i,bt(:iT)) - f (bi:t-i, b(t:2T)),	otherwise,
(20)
5
Published as a conference paper at ICLR 2019
where b(1) = 1[ut>σ(-Twt (bt-ι))], WT 〜q(bt+1:T | btI)), b(2)= 1[ut <σ(Twt (bt-ι))], and
b(2)i：T 〜q(bt+i：T | bt2)). The proof of Proposition 6 is provided in Appendix C. Suppose the
computation complexity of vanilla REINFORCE for a stochastic hidden layer is O(1), which involves
a single evaluation of the function f and gradient backpropagation as VwtTwt (bt-ι), then for a
T -stochastic-hidden-layer network, the computation complexity of vanilla REINFORCE is O(T).
By contrast, if evaluating f is much less expensive in computation than gradient backpropagation,
then the ARM estimator also has O(T ) complexity, whereas if evaluating f dominates gradient
backpropagation in computation, then its worst-case complexity is O(2T).
3.2 ARM maximum likelihood estimation
For maximum likelihood estimation, the log marginal likelihood can be expressed as
logPθ0:T(X) = log EbLT〜PΘ1:T (bi：T)[Pθ0 (X | b1 )]
≥E (θi:T )= Ebi:T~pθ]:T (bi：T)[logpθ0(X | b1)].	(21)
Generalizing Proposition 6 leads to the following proposition.
Proposition 7. For a stochastic binary network defined as
pθt (bt | bt+1) = Bernoulli(bt; σ(Tθt (bt+1))),	(22)
the gradient of the lower bound in (21) with respect to θt can be expressed as
vθtE(θ1：T) = Ep(bt+i:T) [Eut〜UnifOrm(0,1) f∆(Ut, Tθt (bt+1), bt+1:T) (Ut - 1/2)]Tθt (bt+1)],
where f∆(ut, Tet (bt+1), bt+1:T ) = Ebi:t-1 〜p(bi:t-i | bt ), bt = 1[ut>σ(-Tθt (bt+1))])[log Pθο(X | bl)]
-Eb1:t-1~p(b1:t-1 1 bt), bt = 1[ut<σ(Tθt (bt + 1))]) [log pθ0 (X 1 b1)].
4 Experimental results
To illustrate the working mechanism of the ARM estimator, related to Tucker et al. (2017) and
Grathwohl et al. (2018), we consider learning φ to maximize
E(φ) =
Ez 〜Bernoulli(σ(φ)) [(z -p0)2], where p0 ∈ {0.49, 0.499, 0.501, 0.51}.
The optimal solution is σ(φ) = 1[p0<0.5] . The closer p0 is to 0.5, the more challenging the optimiza-
tion becomes. We compare both the AR and ARM estimators to the true gradient as
gφ = (1 - 2p0)σ(φ)(1 - σ(φ))	(23)
and three previously proposed unbiased estimators, including REINFORCE, REBAR (Tucker et al.,
2017), and RELAX (Grathwohl et al., 2018). Since RELAX is closely related to REBAR in introduc-
ing stochastically estimated control variates to improve REINFORCE, and clearly outperforms RE-
BAR in our experiments for this toy problem (as also shown in Grathwohl et al. (2018) for p0 = 0.49),
we omit the results of REBAR for brevity. With a single random sample U 〜Uniform(0,1) for
Monte Carlo integration, the REINFORCE and AR gradients can be expressed as
gφ,REINFORCE = (1[u<σ(φ)] - p0)2(1[u<σ(φ)] - σ(φ)),	gφ,AR = (1[u<σ(φ)] - p0)2 (1 - 2u),
while the ARM gradient can be expressed as
gφ,ARM = (1[u>σ(-φ)] - p0)2 - (1[u<σ(φ)] - p0)2 (u - 1/2).	(24)
See Grathwohl et al. (2018) for the details on RELAX.
As shown in Figure 1, the REINFORCE gradients have large variances. Consequently, a REINFORCE
based gradient ascent algorithm may diverge if the gradient ascent stepsize is not sufficiently small.
For example, when p0 = 0.501, the optimal value for the Bernoulli probability σ(φ) is 0, but the
algorithm with 0.1 as the stepsize infers it to be close to 1 at the end of 2000 iterations of a random
trial. The AR estimator behaves similarly as REINFORCE does. By contrast, both RELAX and
ARM exhibit clearly lower estimation variance. It is interesting to note that the trace plots of the
estimated probability σ(φ) with the univariate ARM estimator almost exactly match these with the
6
Published as a conference paper at ICLR 2019
REINFORCE
True
-0.2
-0.2
-0.2
1.00
1.0
1.0
1.0
0.50-
0.5 -
0.5
0.00
0
0
0.0
2
2
1.0
0.05 -
2
0-
2
0-
0.5
8 -
8 -
0.0
6
0.5
>l o.oo-
9
6 .
0
-0.05 -
0
0
2^=2
2000
0
2^=2
2000
2000
Iteration
0.49
0.499
0.49
True
Figure 1: Comparison of a variety of gradient estimators in maximizing E (φ) = Ez〜BemoUUi(σ(φ)) [(z — Po)2] Via
gradient ascent, where p0 ∈ {0.49, 0.499, 0.501, 0.51}; the optimal solution is σ(φ) = 1(p0 < 0.5). Shown
in Rows 1 and 2 are the trace plots of the true/estimated gradients VφE (φ) and estimated Bernoulli probability
parameters σ(φ), with φ updated via gradient ascent. Shown in Row 3 are the gradient variances for p0 = 0.49,
estimated using K = 5000 Monte Carlo samples at each iteration; the theoretical gradient variances are also
shown if they can be analytically calculated (see Appendices C and D and for related analytic expressions).
true gradients, despite that the trace plots of the ARM gradients are distinct from these of the true
gradients. More specifically, while the true gradients smoothly evolve over iterations, the univariate
ARM gradients are characterized by zeros and random spikes; this distinct behavior is expected
by examining (38) in Appendix C, which suggests that at any given iteration, the univariate ARM
gradient based on a single Monte Carlo sample is either exactly zero, which happens with probability
σ(∣φ∣) - σ(-∣φ∣), or taking |[f (1) - f (0)](1∕2 - u)| as its absolute value. These observations
suggest that by adjusting the frequencies and amplitudes of spike gradients, the univariate ARM
estimator very well approximates the behavior of the true gradient for learning with gradient ascent.
In Figure 4 of Appendix D, we plot the gradient estimated with multiple Monte Carlo samples against
the true gradient at each iteration, further showing the ARM estimator has the lowest estimation
variance given the same number of Monte Carlo samples. Moreover, in Figure 5 of Appendix D, for
each estimator specific column, We plot against the value of φ the sample mean g, sample standard
deviation Sg, and the gradient signal-to-noise ratio defined as SNRg = |g|/sg; for each φ value, We
use K = 1000 single-Monte-Carlo-sample gradient estimates to calculate g, Sg, and SNRg. Both
figures further shoW that the ARM estimator outperforms not only REINFORCE, Which has large
variance, but also RELAX, Which improves REINFORCE With an adaptively estimated baseline.
In Figure 5 of Appendix D, it is also interesting to notice that the gradient signal-to-noise ratio for the
ARM estimator appears to be only a function of φ but not a function of p0 ; this can be verified to be
true using (23) and (39) in Appendix C, as the ratio of the absolute value of the true gradient ∣gφ∣ to
,var[gφ,ARM], the standard deviation of the ARM estimate in (24), can be expressed as
σ(O)(I - σ(φ))/q 16(1 - t)(t3 + 712 + 11 + 1), t = σ(M) - σ(-lφl).	(25)
We find that the values of the ratio shoWn above are almost exactly matched by the values of
SNRg = |g|/sg under the ARM estimator, shown in the bottom right subplot of Figure 5. Therefore,
for this example optimization problem, the ARM estimator exhibits a desirable property in providing
high gradient signal-to-noise ratios regardless of the value ofp0.
4.1 Discrete variational auto-encoders
To optimize a variational auto-encoder (VAE) for a discrete latent variable model, existing solutions
often rely on biased but low-variance stochastic gradient estimators (Bengio et al., 2013; Jang et al.,
2017), unbiased but high-variance ones (Mnih & Gregor, 2014), or unbiased REINFORCE combined
with computationally expensive baselines, whose parameters are estimated by minimizing the sample
variance of the estimator with SGD (Tucker et al., 2017; Grathwohl et al., 2018). By contrast, the
ARM estimator exhibits low variance and is unbiased, efficient to compute, and simple to implement.
7
Published as a conference paper at ICLR 2019
Table 1:	The constructions of three differently structured discrete variational auto-encoders. The following
Symbols "7"，“]”，)”，and “ -→ ” represent deterministic linear transform, leaky rectified linear units (LeakyReLU)
(Maas et al., 2013) nonlinear activation, sigmoid nonlinear activation, and random sampling respectively, in the
encoder (a.k.a. recognition network); their reversed versions are used in the decoder (a.k.a. generator).
Nonlinear	Linear	Linear two layers
Encoder	784→200]→200]→200) -→200	784—200) -→200	784—200) s200—200) -→200
Decoder	784~(784—[200—[200—200	784~(784—200	784~(784—200~(200—200
Table 2:	Test negative log-likelihoods of discrete VAEs trained with a variety of stochastic gradient estimators
on MNIST-StatiC and OMNIGLOT, where *, ?, f, ∣ represent the results reported in Mnih & Gregor (2014),
Tucker et al. (2017), Gu et al. (2016), and Grathwohl et al. (2018), respectively. The results for LeGrad (Titsias
& Lazaro-Gredilla, 2015) are obtained by running the code provided by the authors. We report the results of
ARM using the sample mean and standard deviation over five independent trials with random initializations.
(a) MNIST-static
Linear		Nonlinear		Two layers	
Algorithm	- log p(x)	Algorithm	- log p(x)	Algorithm	- log p(x)
AR	= 164.1	AR	= 114.6	AR	= 162.2
REINFORCE	= 170.1	REINFORCE	= 114.1	REINFORCE	= 159.2
Wake-Sleep*	= 120.8	Wake-Sleep*	-	Wake-Sleep*	= 107.7
NVIL *	= 113.1	NVIL *	= 102.2	NVIL*	= 99.8
LeGrad	≤ 117.5	LeGrad	-	LeGrad	-
MUPrOPt	≤ 113.0	MuProp?	= 99.1	MUPrOPt	≤ 100.4
Concrete?	= 107.2	Concrete?	= 99.6	Concrete?	= 95.6
REBAR?	= 107.7	REBAR?	= 100.7	REBAR?	= 95.7
RELAXt	≤ 113.6	RELAXt	≤ 119.2	RELAXt	≤ 100.9
ARM	=	107.2 ± 0.1	ARM	= 98.4 ± 0.3	ARM	=96.7±0.3
(b) OMNIGLOT
Linear		Nonlinear		Two layers	
Algorithm	一log p(x)	Algorithm	一log p(x)	Algorithm	一 log p(x)
NVIL*	=117.6	NVIL*	=116.6	NVIL*	= 111.4
MuProp?	=117.6	MuProp?	= 117.5	MuProp?	= 111.2
Concrete?	=117.7	Concrete?	= 116.7	Concrete?	= 111.3
REBAR?	=117.7	REBAR?	= 118.0	REBAR?	= 110.8
RELAXt	≤ 122.1	RELAXt	≤ 128.2	RELAXt	≤ 115.4
ARM	=115.8 ± 0.2	ARM	= 117.6 ± 0.4	ARM	= 109.8 ± 0.3
For discrete VAEs, we compare ARM with a variety of representative stochastic gradient estimators
for discrete latent variables, including Wake-Sleep (Hinton et al., 1995), NVIL (Mnih & Gregor,
2014), LeGrad (Titsias & LaZaro-GrediUa, 2015), MuProp (GU et al., 2016), Concrete (Gumbel-
Softmax) (Jang et al., 2017; Maddison et al., 2017), REBAR (Grathwohl et al., 2018), and RELAX
(Tucker et al., 2017). Following the settings in Tucker et al. (2017) and Grathwohl et al. (2018), for the
encoder defined in (15) and decoder defined in (16), we consider three different network architectures,
as summarized in Table 1, including “Nonlinear” that has one stochastic but two Leaky-ReLU (Maas
et al., 2013) deterministic hidden layers, “Linear” that has one stochastic hidden layer, and “Linear two
layers” that has two stochastic hidden layers. We consider a widely used binarization (Salakhutdinov
& Murray, 2008; Larochelle & Murray, 2011; Yin & Zhou, 2018), referred to as MNIST-static and
available at http:〃www.dmi.usherb.ca/~larocheh/mlpython/_modules/datasets/binarized_mnist.html,
making our numerical results directly comparable to those reported in the literature. In addition
to MNIST-static, we also consider MNIST-threshold (van den Oord et al., 2017), which binarizes
MNIST by thresholding each pixel value at 0.5, and the binarized OMNIGLOT dataset.
We train discrete VAEs with 200 conditionally iid Bernoulli random variables as the hidden units
of each stochastic binary layer. We maximize a single-Monte-Carlo-sample ELBO using Adam
(Kingma & Ba, 2014), with the learning rate selected from {5, 1, 0.5} × 10-4 by the validation set.
We set the batch size as 50 for MNIST and 25 for OMNIGLOT. For each dataset, using its default
8
Published as a conference paper at ICLR 2019
MNIST_static
1a0
170
160
130
120
110
100
S150
UJ 140
145
200000 400000 600000 800000 1000001
Steps
——T⅛I∏I∏0(REBAR)
——VaIIdatbn(REBAR)
——ThaInIng(RELA)Q
——VHkiatbn(RELAjq
——T⅛I∏I∏0(ARM)
-——VaIIdatbn(ARM)
TraInIng(Gumbei)
ValWatbn(Gumbei)
MNlST static
140
125
120
S135
Z 130
11引
0	200000 400000 600000 600000 1000000
Steps
——Tralnlna(REBAR)
——VdUetIon(REBAR)
——TreInIng(RELA)Q
VeiWatIon(RELAX)
——TralnIn奴ARM)
——VeiIdetIon(ARM)
---TraInIno(GunibeI)
Veildetlon(Gumbei)
Steps
(a) Nonlinear
(b) Linear
MNlST static
MNIST.static
160
160
150
150
140
120
110
S140
⅛ 130
2000 4000 6000 88。188 12000 1408
Time(Se8∏ds)
IoO
O 25∞ 5000 7500 1000012500 15000 17500 20000
Time(seconds)
(d) Nonlinear
——Training(REBAR)
——Veiiitetion(REBAR)
——Treining(RELA)Q
——VeiidatiOiXRELAX)
----Training(ARM)
-——VdidatioiXARM)
----Training(Gunrfcei)
Vsiidatiori(GijjjJbet)
(c) Linear two layers
——IYaInIng(REBAR)
——VaIIcIatIon(REBAR)
——TfaInm(RELAX)
——VaIIdatIon(REI-AX)
——TfaInIng(ARM)
——VaIIcIatIon(ARM)
----TfaInIng(GumbeI)
ValldatIon(Gumbel)
(e) Linear	(f) Linear two layers
Figure 2: Training and validation negative ELBOs on MNIST-static with respect to the training iterations,
shown in the top row, and with respect to the wall clock times on Tesla-K40 GPU, shown in the bottom row, for
three differently structured Bernoulli VAEs.
Nonlinear
Linear
Figure 3: Trace plots of the log variance of the gradient estimators on the MNIST-static data for “Nonlinear”
and “Linear” network architectures, whose corresponding trace plots of both the training and validation -ELBO
are shown in Figures 2(a) and 2(b), respectively. The variance of the gradient of each element is estimated by
performing exponential smoothing, with the smoothing factor as 0.999, on its first two moments; the logarithm
of the average over all elements’ gradient variances is shown for each stochastic gradient ascent step.
training/validation/testing partition, we train all methods on the training set, calculate the validation
log-likelihood for every epoch, and report the test negative log-likelihood when the validation negative
log-likelihood reaches its minimum within a predefined maximum number of iterations.
We summarize the test negative log-likelihoods in Table 2 for MNIST-static. We also summarize
the test negative ELBOs in Table 4 of the Appendix, and provide related trace plots of the training
and validation negative ELBOs on MNIST-static in Figure 2, and these on MNIST-threshold and
OMNIGLOT in Figures 6 and 7 of the Appendix, respectively. For these trace plots, for a fair
comparison of convergence speed between different algorithms, we use publicly available code and
setting the learning rate of ARM the same as that selected by RELAX in Grathwohl et al. (2018). Note
as shown in Figures 2(a,d) and 7(a,d), both REBAR and RELAX exhibit clear signs of overfitting on
both MNIST-static and Omniglot using the “Nonlinear” architecture; as ARM runs much faster per
iteration than both of them and do not exhibit overfitting given the same number of iterations, we
allow ARM to run more stochastic gradient ascent steps under these two scenarios to check whether
it will eventually overfit the training set.
These results show that ARM provides state-of-the-art performance in delivering not only fast
convergence, but also low negative log-likelihoods and negative ELBOs on both the validation and
test sets, with low computational cost, for all three different network architectures. In comparison to
the vanilla REINFORCE on MNIST-static, as shown in Table 2 (a), ARM achieves significantly lower
test negative log-likelihoods, which can be explained by having much lower variance in its gradient
9
Published as a conference paper at ICLR 2019
Table 3:	Comparison of the test negative log-likelihoods between ARM and various gradient estimators in Jang
et al. (2017), for the MNIST conditional distribution estimation benchmark task.
Gradient estimator ARM ST DARN Annealed ST ST Gumbel-S. SF MuProp
-log p(xι | Xu)	57.9 ± 0.1	58.9 59.7	58.7	59.3	72.0	58.9
estimation, while only costing 20% to 30% more computation time to finish the same number of
iterations.
The trace plots in Figures 2, 6, and 7 show that ARM achieves its objective better or on a par with
the competing methods in all three different network architectures. In particular, the performance
of ARM on MNIST-threshold is significantly better, suggesting ARM is more robust, better resists
overfitting, and has better generalization ability. On both MNIST-static and OMNIGLOT, with the
“Nonlinear” network architecture, both REBAR and RELAX exhibit severe overfitting, which could
be caused by their training procedure, which updates the parameters of the baseline function by
minimizing the sample variance of the gradient estimator using SGD. For less overfitting linear and
two-stochastic-layer networks, ARM overall performs better than both REBAR and RELAX and runs
significantly faster (about 6-8 times faster) in terms of the computation time per iteration.
To understand why ARM has the best overall performance, we examine the trace plots of the logarithm
of the estimated variance of gradient estimates in Figure 3. On the MNIST-static dataset with the
“Nonlinear” network, the left subplot of Figure 3 shows that both REBAR and RELAX exhibit lower
variance than ARM does for their single-Monte-Carlo-sample based gradient estimates; however, the
corresponding trace plots of the validation negative ELBOs, shown in Figure 2(a), suggest they both
severely overfit the training data as the learning progresses; our hypothesis for this phenomenon is
that REBAR and RELAX may favor suboptimal solutions that are associated with lower gradient
variance; in other words, they may have difficulty in converging to local optimal solutions that are
associated with high gradient variance. For the “Linear” network architecture, the right subplot
of Figure 3 shows that ARM exhibits lower variance for its gradient estimate than both REBAR
and RELAX do, and Figure 2(b) shows that none of them exhibit clear signs of overfitting; this
observation could be used to explain why ARM results in the best convergence for both the training
and validation negative ELBOs, as shown in Figure 2(b).
4.2 Maximum likelihood estimation for a stochastic binary network
Denoting xl, xu ∈ R394 as the lower and upper halves of an MNIST digit, respectively, we consider a
standard benchmark task of estimating the conditional distribution pθq2 (xι | Xu) (Raiko et al., 2014;
Bengio et al., 2013; Gu et al., 2016; Jang et al., 2017; Tucker et al., 2017), using a stochastic binary
network with two stochastic binary hidden layers, expressed as
xι 〜BemOUlli(σ(Tθo(bi))), bi 〜BemOUlli(σ(Tθι (b2))), b2 〜BemOUlli(σ(Tθ2 (Xu))). (26)
We set the network structure as 392-200-200-392, which means both b1 and b2 are 200 dimensional
binary vectors and the transformation Tθ are linear so the resUlts are directly comparable with those in
Jang et al. (2017). We approximate log pθ^ (xι | Xu) with log 春 Pk=I BemOUlli(xι; σ(Tθ0 (bf)))),
where b[k) 〜BemOUlli(σ(Tθι (b2k)))), b2k) 〜BemOUlli(σ(Tθ2 (Xu))). We perform training with
K = 1, which can also be considered as optimizing on a single-Monte-Carlo-sample estimate of
the lower boUnd of the log marginal likelihood shown in (21). We Use Adam (Kingma & Ba, 2014),
with the learning rate set as 10-4, mini-batch size as 100, and nUmber of epochs for training as
2000. Given the inferred point estimate of θ03 after training, We evaluate the accuracy of conditional
density estimation by estimating the negative log-likelihood as - logpθq2 (xι | xu), averaging over
the test set Using K = 1000. We show example resUlts of predicting the activation probabilities of
the pixels of Xl given Xu in FigUre 8 of the Appendix.
As shown in Table 3, optimizing a stochastic binary network with the ARM estimator, which is Unbi-
ased and compUtationally efficient, achieves the lowest test negative log-likelihood, oUtperforming
previoUsly proposed biased stochastic gradient estimators on similarly strUctUred stochastic networks,
inclUding DARN (Gregor et al., 2013), straight throUgh (ST) (Bengio et al., 2013), slope-annealed
ST (ChUng et al., 2016), and ST GUmbel-softmax (Jang et al., 2017), and Unbiased ones, inclUding
score-fUnction (SF) and MUProp (GU et al., 2016).
10
Published as a conference paper at ICLR 2019
5 Conclusion
To train a discrete latent variable model with one or multiple stochastic binary layers, we propose
the augment-REINFORCE-merge (ARM) estimator to provide unbiased and low-variance gradient
estimates of the parameters of Bernoulli distributions. With a single Monte Carlo sample, the
estimated gradient is the product of uniform random noises and the difference of a function of two
vectors of correlated binary latent variables. Without relying on estimating a baseline function with
extra learnable parameters for variance reduction, it maintains efficient computation and avoids
increasing the risk of overfitting. Applying the ARM gradient leads to not only fast convergence, but
also low test negative log-likelihoods (and low test negative evidence lower bounds for variational
inference), on both auto-encoding variational inference and maximum likelihood estimation for
stochastic binary feedforward neural networks. Some natural extensions of the proposed ARM
estimator include generalizing it to multivariate categorical latent variables, combining it with a
baseline or local-expectation based variance reduction method, and applying it to reinforcement
learning whose action space is discrete.
Acknowledgments
M. Zhou acknowledges the support of Award IIS-1812699 from the U.S. National Science Foundation,
and support from the McCombs Research Excellence Grant. The authors acknowledge the support
of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research, and the
computational support of Texas Advanced Computing Center.
References
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Christopher M Bishop. Neural Networks for Pattern Recognition. Oxford university press, 1995.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association,112(518):859-877, 2017.
George Casella and Christian P Robert. Rao-blackwellisation of sampling schemes. Biometrika, 83
(1):81-94, 1996.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.
arXiv preprint arXiv:1609.01704, 2016.
Michael C Fu. Gradient estimation. Handbooks in operations research and management science, 13:
575-616, 2006.
Peter W Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the
ACM, 33(10):75-84, 1990.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the Void: Optimizing control variates for black-box gradient estimation. In ICLR, 2018.
Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive
networks. arXiv preprint arXiv:1310.8499, 2013.
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. MuProp: Unbiased backpropagation
for stochastic neural networks. In ICLR, 2016.
Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The “wake-sleep” algorithm
for unsupervised neural networks. Science, 268(5214):1158-1161, 1995.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-softmax. In
ICLR, 2017.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
11
Published as a conference paper at ICLR 2019
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. Automatic
differentiation variational inference. Journal ofMachine Learning Research, 18(14):1-45, 2017.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of
the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29-37, 2011.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network
acoustic models. In ICML, 2013.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete distribution: A continuous
relaxation of discrete random variables. In ICLR, 2017.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
ICML, pp. 1791-1799, 2014.
Andriy Mnih and Danilo J Rezende. Variational inference for Monte Carlo objectives. arXiv preprint
arXiv:1602.06725, 2016.
Christian Naesseth, Francisco Ruiz, Scott Linderman, and David Blei. Reparameterization gradients
through acceptance-rejection sampling algorithms. In AISTATS, pp. 489-498, 2017.
Radford M Neal. Connectionist learning of belief networks. Artificial Intelligence, 56(1):71-113,
1992.
Art B. Owen. Monte Carlo Theory, Methods and Examples, chapter 8 Variance Reduction. 2013.
John Paisley, David M Blei, and Michael I Jordan. Variational Bayesian inference with stochastic
search. In ICML, pp. 1363-1370, 2012.
Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning binary
stochastic feedforward neural networks. arXiv preprint arXiv:1406.2989, 2014.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In AISTATS, pp.
814-822, 2014.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In ICML, pp. 1278-1286, 2014.
Sheldon M. Ross. Introduction to Probability Models. Academic Press, 10th edition, 2006.
Francisco J. R. Ruiz, Michalis K. Titsias, and David M. Blei. The generalized reparameterization
gradient. In NIPS, pp. 460-468, 2016.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In
ICML, pp. 872-879, 2008.
Lawrence K Saul, Tommi Jaakkola, and Michael I Jordan. Mean field theory for sigmoid belief
networks. Journal of Artificial Intelligence Research, 4:61-76, 1996.
Yichuan Tang and Ruslan R Salakhutdinov. Learning stochastic feedforward neural networks. In
NIPS, pp. 530-538, 2013.
Martin A Tanner and Wing Hung Wong. The calculation of posterior distributions by data augmenta-
tion. J. Amer. Statist. Assoc., 82(398):528-540, 1987.
Michalis K Titsias and Miguel Lazaro-Gredilla. Local expectation gradients for black box variational
inference. In NIPS, pp. 2638-2646. MIT Press, 2015.
12
Published as a conference paper at ICLR 2019
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. REBAR:
Low-variance, unbiased gradient estimates for discrete latent variable models. In NIPS, pp.
2624-2633, 2017.
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning.
In NIPS, pp. 6306-6315, 2017.
David A Van Dyk and Xiao-Li Meng. The art of data augmentation. Journal of Computational and
Graphical Statistics, 10(1):1-50, 2001.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. In Reinforcement Learning, pp. 5-32. Springer, 1992.
Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In ICML, pp. 5660-5669,
2018.
Mingyuan Zhou and Lawrence Carin. Negative binomial process count and mixture modeling. arXiv
preprint arXiv:1209.3442v1, 2012.
13
Published as a conference paper at ICLR 2019
Appendix
A The ARM gradient ascent Algorithm
We summarize the algorithm to compute ARM gradient for binary latent variables. Here we show
the gradient with respect to the logits associated with the probability of Bernoulli random variables.
If the logits are further generated by deterministic transform such as neural networks, the gradient
with respect to the transform parameters can be directly computed by the chain rule. For stochastic
transforms, the implementation of ARM gradient is discussed in detail in Section 3 and summarized
in Algorithm 2.
Algorithm 1: ARM gradient for a V -dimensional binary latent vector
input : Bernoulli distribution {qφv (Zv)}v=i：V With probability {σ(φv)}v=i：V, target f (z);
Z = (Z1,…，ZV), φ = (φ1,…，φV)
output ： Φ and ψ that maximize E (φ, ψ) = Ez 〜QV=I qφv (zv )[f (z； ψ)]
Initialize φ, ψ randomly;
while not converged do
Sample a mini-batch of x from the data;
Sample Zv 〜 Bernoulli(σ(φv)) for V = 1, ∙∙∙ , V ;
sample Uv 〜 Uniform(0,1) for V = 1, ∙∙∙ ,V, U = (uι, •…,UV);
gψ = Vψf(z; ψ);
f∆(u,φ) =f 1[u>σ(-φ)] -f 1[u<σ(φ)] ;
gφ = f∆(u, φ)(u - 0.5)
φ = φ + ρtgφ, ψ = ψ + ηtgψ,	With stepsizes ρt, ηt
end
Algorithm 2: ARM gradient for a T -stochastic-hidden-layer binary netWork
input : Inference netWork qw1:T (b1：T | x) = qw1 (b1 | x) QtT=-11 qwt+1 (bt+1 | bt) Where
qwt (bt | bt-1) = Bernoulli(bt; σ(Twt (bt-1))), target f(b1：T; ψ)
output: Wi：T and ψ that maximize E (wi：T, ψ) = EbLT 〜qw>τ [f(bi：T； ψ)]
Initialize w1：T, ψ randomly;
while not converged do
Sample a mini-batch of x from data;
for t = 1:T do
If t ≥ 2, sample bt-i 〜q(bt-1∣bt-2), if t = 2, bi：t-i = bi, else bi：t-i = [bi：t-2, bt-i];
sample Ut 〜 Q Uniform(0,1);
bt = 1[ut >σ(-Twt (bt-1 ))] ;
bt2 = 1[ut <σ(Twt (bt-1))] ;
if bti = bt2 then
I f∆(b1.t-1, bi：T, b2：T) = 0 ;
else
bi+i：T 〜q(bt+i：T电)；
b2+1：T 〜 q(bt+i：T |bt2) ;
f∆(bi∙∙t-i, bi：T, b2：T) = f(bi：t-i, bi：T) - f(bi：t-i, b2：T);
gwt = f∆ (bi：t-i , bti：T,b2：T)(ut- 1 )TVwtTWt(bt-i);
end
wt = wt + ρtgwt With step-size ρt
end
ψ = ψ + ηtVψ f (bi：T; ψ) with step-size η
end
B Original derivation of the AR and ARM estimators
Let us denote t 〜Exp(λ) as an exponential distribution, whose probability density function is
defined as p(t | λ) = λe-λt, where λ > 0 and t > 0. The mean and variance are E[t] = λ-1 and
var[t] = λ-2, respectively. The exponential random variable t 〜 Exp(λ) can be reparameterized as
14
Published as a conference paper at ICLR 2019
t = e∕λ, E 〜 ExP(1). It is well known, e.g., in Ross (2006), that if tι 〜 Exp(λι) and t?〜 Exp(λ2)
are two independent exponential random variables, then the probability that t1 is smaller than t2 can
be expressed as P(tι < t2) = λι∕(λι + λ2); moreover, since tι 〜Exp(λι) is equal in distribution
to E∖∕λ∖, €1 〜Exp(1) and t2 〜Exp(λ2) is equal in distribution to €2/λ2,⑦ 〜Exp(1), we have
P(t1 < t2) = P(El∕λ1 < €2/12) = P(EI < €211/12) = λl∕Cl + λ2).	(27)
B.1	Augmentation of a Bernoulli random variable and Reparameterization
From (27) it becomes clear that the Bernoulli random variable Z 〜 BernOUlli(σ(φ)) can be reparam-
eterized by comparing two augmented exponential random variables as
Z = I[e1<e2eφ], €1 〜Exp(1), €2 〜Exp(1).	(28)
Consequently, the expectation with respect to the Bernoulli random variable can be reparameterized
as one with respect to two augmented exponential random variables as
E(φ) = Ez〜BernOUlli(σ(φ))[f (Z)] = E^ iidExp(1)[f (1&e-φ<e2] )].	(29)
B.2	REINFORCE estimator in the augmented space
Since the indicator function 1[1e-φ<2] is not differentiable, the reparameterization trick in (2) is not
directly applicable to computing the gradient of (29). Fortunately, as tι = e∖e-cφ, €1 〜Exp(1) is
equal in distribution to tι 〜Exp(eφ), the expectation in (29) can be further reparameterized as
E (φ) = Eei,⅛ KEXP(I) [f(l[e1e-Φ<e2])] = Eti 〜Exp(eΦ), & 〜Exp(1)[f(1[t<2])],	(30)
and hence, via REINFORCE and then another reparameterization, we can express the gradient as
VφE(φ) = Eti〜Exp(eΦ), e2〜Exp(l)[f(l[t1<e2])Vφ logExp(tι; eφ)]
=Eti~Exp(eφ), e2~Exp(1)[f (1[ti <e2] )(1 - t1e°)]
=Eeι,e2⅛p(1)f (1[eιe-Φ<e2])(1 - €l)].	(31)
SimiIarly, we have E (φ) = Eeι,e2 ⅛⅛xp(1)f (1[eι<e2eΦ])] = Ee-Exp(I), t2 〜Exp(-eΦ)[f (1 [ei <t2] )], and
hence can also express the gradient as
VφE (φ) = Eei 〜EXP(1), t2 〜Exp(e-Φ)[f(l[e1<t2])Vφ log Exp(t2; e-φ)]
=-EeI〜EXP(I), t2〜Exp(e-φ)f (1[eι<t2])(1 - t2e °)]
=-Eeι,e2⅛p(1)f (1[eιe-Φ<e2])(1 - €2)].	(32)
iid
Note that letting €1, €2 〜Exp(1) is the same in distribution as letting
€1 = €u, €2 = €(1 一 u), where U 〜Uniform(0,1), € 〜Gamma(2,1),	(33)
which can be proved using Exp(1) = Gamma(1,1), (u, 1 一 u)T = Dirichlet (I2), where U 〜
Uniform(0, 1)}, together with Lemma IV.3 of Zhou & Carin (2012); we use “=d” to denote “equal in
distribution.” Thus, (B.2) can be reparameterized as
V φE (φ) = EU 〜UnifOrm(0,1),
e 〜Gamma(2,1) f (1[u<σ(φ)] )(1 一 €U) ,
Applying Rao-Blackwellization (Casella & Robert, 1996), we can further express the gradient as
VφE(φ) = Eu〜Uniform(0,1) [f (1[u<σ(φ)] )(1 一 2u)] .	(34)
Therefore, the gradient estimator shown above, the same as (6), is referred to as the Augment-
REINFORCE (AR) estimator.
15
Published as a conference paper at ICLR 2019
B.3 Merge of REINFORCE gradients
A key observation of the paper is that by swapping the indices of the two iid standard exponential
random variables in (32), the gradient VφE(φ) can be equivalently expressed as
VφE(φ) = -E	iid。⑴[f(1[*-φ<q])(1 -⑴].	(35)
e 1 ,e 2 EXP(1)
As the term inside the expectation in (31) and that in (35) could be highly positively correlated, we are
motivated to merge (31) and (35) by sharing the same set of standard exponential random variables
for Monte Carlo integration, which provides a new opportunity to well control the estimation variance
(Owen, 2013). More specifically, simply taking the average of (31) and (35) leads to
VφE(φ) = EeIqi¾xp⑴[(f(I[e1e-φ<e2]) - f(l[e2e-Φ<q]))(l∕2 - ei/2)] .	(36)
Note one may also take a weighted average of (31) and (35), and optimize the combination weight to
potentially further reduce the variance of the estimator. We leave that for future study.
Note that (36) can be reparameterized as
VφE(φ) = EU〜UnifOrm(0,1), e〜Gamma(2,1) [(f (I[u>σ(-φ)] ) - f (I[u<σ(φ)I))(EU∕2 - 1/2)],
Applying Rao-Blackwellization (Casella & Robert, 1996), we can further express the gradient as
VφE(φ) = Eu〜UnifOrm(0,1) [ff (1[u>σ(-φ)] ) - f (1[u<σ(φ)] )) (u - 1/2)] .	(37)
Therefore, the gradient estimator shown above, the same as (7), is referred to as the Augment-
REINFORCE-merge (ARM) estimatOr.
C Proofs
Proof of Proposition 2. Since the gradients gARM(u, φ), gAR(u, φ), and gR(z, φ) are all unbiased,
their expectatiOns are the same as the true gradient gtrue (φ) = σ(φ)(1 - σ(φ))[f (1) - f (0)]. DenOte
f∆(u, φ) = f (1[u>σ(-φ)] ) - f (1[u<σ(φ)] ). Since
(0,	if σ(TφD < u < σ(IφD,
f∆(u, φ)= f f (1) - f (0),	if U > σ(∣φ∣),	(38)
f(0)- f (1),	if u<σ(-∣φ∣),
The secOnd mOment Of gARM(U, φ) can be expressed as
Eu〜UnifOrm(0,1) [gARM(U, φ)] = Eu〜UnifOrm(0,1) [f∆ (U, φ) (U - 1/2) ]
∕1	[f (1) - f (0)]2(U - 1/2)2dU + Zσ( lφl)[f (0) - f (1)]2(U - 1/2)2dU
√σ(∣φ∣)	√0
=112[1 - (σ(∣Φ∣) - σ(-∣φ∣))3][f (1) - f(0)]2
Denoting t = σ(∣φ∣) - σ(-∣φ∣), we can re-express gtrue(φ) = 4(1 - t2)[f (1) - f(0)]. Thus, the
variance Of gARM (U, φ) can be expressed as
var[gARM(U, φ)] =4 3(I- t3) - 4(1 - t2)2 [f (I)- f (O)K
= W(I- t)(t3 + 7t2 + 1 t + 1)[f (1) - f(O)]2	(39)
16	3	3	3
≤ɪ[f (1) - f (O)]2,
25
which reaches its maximum at 0.039788[f (1) - f (O)]2 when t = √52~1.
FOr the REINFORCE gradient, we have
Ez 〜BernOUlli(σ(φ))[gR(z,Φ)] =Ez〜BernOUlli(σ(φ)) [f 2 (Z)(Z(I- σ(φ)) - σ(φ)(1 - z))2]
=σ(φ)(1 - σ(φ))[(1 - σ(φ))f 2(1) + σ(φ)f 2(O)].
16
Published as a conference paper at ICLR 2019
Therefore the variance can be expressed as
var[gR(u, φ)]
=σ(φ)(1 - σ(φ)) (1 - σ(φ))f2(1) + σ(φ)f2(0) - σ(φ)(1 - σ(φ))[f(1) - f(0)]2
=σ(φ)(1 - σ(φ))[(1 - σ(φ))f(1) + σ(φ)f(0)]2.
The largest variance satisfies
SuPvar[gR(z,φ)] ≥ var[gR(z,0)] = ɪ[f(1) + f(0)]2,
φ	16
and hence when f is always positive or negative, we have
suPφ var[gARM(Z,町 ≤ 16 a - 2 f (0)	)2 ≤ 16
suPφ var[gR(z,Φ)] 一 25( f (0) + f(1)	- 25.
In summary, the ARM gradient has a variance that is bounded by * (f (1) 一 f (0))2, and its worst-case
variance is smaller than that of REINFORCE.	□
Proof of Proposition 3. We only need to prove for K = 1 and the proof for K > 1 automatically
follows. SinceEu[f(1[u<σ(φ)])2(uv - 1/2)2] = Eu[f(1[u>σ(-φ)])2(uv - 1/2)2], we have
var(gARM1,v)-var(gAR1,v) = -3Eu[f(1[u<σ(φ)])2(uv - 1/2)2] + Eu[f(1[u>σ(-φ)])2(uv - 1/2)2]
-	2Eu[f (1[u>σ(-φ)])f (1[u<σ(φ)])(uv - 1/2)2]
= -Eu[f(1[u<σ(φ)])2(uv - 1/2)2] - Eu[f(1[u>σ(-φ)])2(uv - 1/2)2]
-	2Eu[f (1[u>σ(-φ)])f (1[u<σ(φ)])(uv - 1/2)2]
=-Eu [(f(1[u>σ(-φ)]) + f(1[u<σ(φ)]))2(Uν - 1/2)2]
≤ 0,
which shows that the estimation variance ofgARMK,v is guaranteed to be lower than that of the gARK,v,
unless f (1[u>σ(-φ)]) + f (1[u<σ(φ)] ) = 0 almost surely. Furthermore, since
var(gARM1 ,v ) - var(gAR2 ,v ) =Eu [(f (1[u<σ(φ)] ) - f (1[u>σ(-φ)] )) (uv - 1/2) ]
-	Eu(1) ,u(2) [(f (1[u(1)<σ(φ)] )(u(v1) - 1/2) + f (1[u(2)<σ(φ)] )(u(v2) - 1/2))2]
=- 2Eu(i) [f (1[u(i)<σ(φ)])(uV1) - 1∕2)]Eu⑵[f(1[u⑵<σ(φ)])(uV2) - 1/2))]
-	2Eu[f(1[u<σ(φ)])f(1[u>σ(-φ)]))(uv - 1/2)2]
=- 2(Eu[f (1[u<σ(φ)])(uv - 1∕2)])2
-	2Eu[f(1[
u<σ(φ)])f(1[u>σ(-φ)]))(uv - 1/2)2],
when f is always positive or negative, the variance of gARMκ ,v is lower than that of gAR2κ ,v.	□
Proof of Proposition 4. Denoting g(u) = gAR (u) - b(u), we have
var[gv (u)] - var[gAR,v (u)] = -2Eu [gAR,v (u)bv (u)] + Eu [bv2 (u)].
To maximize the variance reduction, it is equivalant to consider the constrained optimization problem
min - 2Eu[gAR,v(u)bv(u)] + Eu[bv2(u)]
bv(u)
subject to: bv(u) = -bv (1 - u),
which is the same as a Lagrangian problem as
min	L(bv (u), λv (u)) = -2Eu [gAR,v (u)bv (u)] + Eu [b2v (u)] +	λv (u)(bv (u) + bv (1 - u))du.
bv (u),λv (u)
17
Published as a conference paper at ICLR 2019
Setting δλL = 0 gives bv(U) + bv(1 - u) = 0. By writing / λv(u)(bv(U) + bv(1 - u))du =
/(λv(U) + λv(1 - u))bv(u)du and setting δδL = 0,we have
[2gAR,v(U) - 2bv (U)]p(U) = λv (U) + λv (1 - U).	(40)
Interchange U and 1 - U gives
[2gAR,v(1 - U) - 2bv (1 - U)]p(1 - U) = λv (1 - U) + λv (U).	(41)
Solving (40) and (41) with bv (U) + bv (1 - U) = 0 and p(U) = p(1 - U), we have the optimal
baseline function as bV (U) = 1 (gAR,v (U) - gAR,v (1 - u)), The proof is completed by noticing that
gAR(u) - b* (U) is the same as the single sample gradient estimate under the ARM estimator. 口
Proof of Corollary 5. Since bv(U) = cv(1 -2u) satisfies the anti-symmetric property, we can directly
arrive at Corollary 5 using Proposition 4. Alternatively, since Eu[f (1[u<σ(φ)] )2(uv - 1/2)2] =
Eu[f(1[u>σ(-φ)])2(uv - 1/2)2] and Eu[f(1[u<σ(φ)])(uv - 1/2)2] = Eu[f(1[u>σ(-φ)])(uv -
1/2)2], for gC,v = (f (1[u<σ(φ)]) - cv)(1 - 2uv), we have
var(gC,v) - var(gARM,v)
= Eu[(f (1[u<σ(φ)]) - cv)2(1 - 2uv)2] - Eu[(f(1[u<σ(φ)]) - f(1[u>σ(-φ)]))2(uv - 1/2)2]
=Eu[(4cV - 8cv f(1[u<σ(φ)]) + 2f 2(1[u<σ(φ)]) + 2f (1[u<σ(φ)] )f (1[u>σ(-φ)] )) (uv - 1/2)2]
=Eu[(f(1[u>σ(-φ)]) + f (1[u<σ(φ)]) - 2Cv)2(Uv- 1/2)2]
≥ 0.
□
Proof of Proposition 6. First, to compute the gradient with respect to w1, since
E(w1:T) =Eq(b1)Eq(b2:T|b1)[f(b1:T)],	(42)
we have
VwiE(wi:T) = Euι~Uniform(0,1) [f∆("1, Twi (X))(UI- 1∕2)NwιTwi (x),	(43)
where
f δ (UL TwI (X)) = Eb2:T ~q(b2:T | bI) , b1=1[uι >σ(-TWi(x))] )[f (b1:T )]
-Eb2: T ~q(b2:T | bI) , b1 = 1[uι<σ(Twι (x))] ) [f (b1:T )] .	(44)
Second, to compute the gradient with respect to wt, where 2 ≤ t ≤ T - 1, since
E(w1:T) = Eq(bi:t-i)Eq(bt | bt-i)Eq(bt+i:T |bt)[f(b1:T)],	(45)
we have
VwtE(W1:T) = Eq(bι∙t-ι)[Eut~Uniform(0,1)[f∆(Ut, Twt (bt-1), b1:t-1)(Ut - 1∕2)]Vw,Twt (bt-1)],
(46)
where
f∆(Ut, Twt (bt-1), bi:t-i) = Ebt
+i:T~q(bt + 1:T | bt), bt = 1[ut>σ(-TWt (bt-ι))] ) [f (b1:T )]
-Ebt + 1：T ~q(bt + LT | bt), bt = 1[ut<σ(TWt (bt-i))])[f (b1：T )].	(47)
Finally, to compute the gradient with respect to wT , we have
VwTE(w1:T) = Eq(bi：T-i) [EUT~Uniform(0,1) f∆ (UT, TwT (bT-1), b1:T-I)(UT - 1/2)]
× VwT TwT (bT-1)] ,	(48)
where
f∆(UT, TwT (bT -1), b1:T-1) = f(b1:T-1, bT = 1[uT >σ(-TwT (bT-i))])
- f(b1:T-1, bT = 1[uT<σ(TwT (bT-i))] ).	(49)
□
18
Published as a conference paper at ICLR 2019
D Additional experimental results
For the univariate AR gradient, we have
σ(φ)	1
Eu 〜UnifOrm(0,1)[gAR(U,。)] =/	f ⑴(I- 2u)) du + J f (0)(I-2u)2 du
=1(f2(0) + f2(1)) + 6(1 - 2σ(φ))3(f2(0) - f2(1)).
Thus its variance can be expressed as
var[gAR(u,φ)] = ∣(f 2(0)+f 2(1))+1(1-2σ(φ))3(f 2 (0)-f 2(1))-σ2(φ)(1-σ(φ))2[f (1)-f (0)]2,
which is used for related plots in Figures 1 and 5.
REINFORCE(K=10
3utu-Pro」9
AR(K=Io)	RELAX(K=Io)	ARM(K=Io)
AR(K=IOO) RELAx(K=IOO) ARM(K=IOO)
AR(K=5000)	RELAX(K=5000)	ARM(K=5000)
REINFΘRCE(K=5000)
O IOOO 2000 O IOOO 2000 O IOOO 2000
Iteration
Figure 4: Comparison of a variety of gradient estimators for estimating the gradient of E(φ) =
Ez〜BernoUlli(σ(φ))[(z -Po)2], Wherepo ∈ {0.49, 0.499, 0.501, 0.51} and the values of φ are updated via gradient
ascent with the true gradients. Shown in Rows 1, 2, and 3 are the trace plots of the estimated gradients using
K = 10, 100, and 5000 Monte Carlo samples, respectively.
⊂SE
0.01
0.00
REINFORCE
AR
Figure 5: Comparison of a variety of gradient estimators for estimating the gradient of E(φ) =
Ez〜BernouUi(σ(φ))[(z - Po)2], Where po ∈ {0.49, 0.499, 0.501, 0.51} and the values of φ range from —2.5
to 2.5. For each φ value, we compute for each estimator K = 1000 single-Monte-Carlo-sample gradient
estimates, and use them to calculate their sample mean g, sample standard deviation Sg, and gradient signal-to-
noise ratio SNRg = |g|/sg. In each estimator specific column, we plot g, Sg, and SNRg in Rows 1, 2, and 3,
respectively. The theoretical gradient standard deviations and gradient signal-to-noise ratios are also shoWn if
they can be analytically calculated (see Eq. 25 and Appendices C and D and for related analytic expressions).
RELAX
-O-Ol
D.OO4
D.002
2.0
1.5
1.0
D.5
ARM
0.00
----0.49
0.499
----0.501
----0.51
——"True
19
Published as a conference paper at ICLR 2019
Table 4: Test negative ELBOs of disCrete VAEs trained with four different stoChastiC gradient estimators.
MNIST-threshold is the binarized MNIST thresholded at 0.5 and MNIST-statiC is the binarized MNIST used in
Salakhutdinov & Murray (2008); LaroChelle & Murray (2011).
	ARM			RELAX	REBAR	ST Gumbel-Softmax
		MNIST-threshold	101.3	110.9	111.6	112.5
	Nonlinear	MNIST-statiC	109.9	112.1	111.8	-
		OMNIGLOT	129.5	128.2	128.3	140.7
		MNIST-threshold	110.3	-^1221 ^^	123.2	129.2
Bernoulli	Linear	MNIST-statiC	116.2	116.7	117.9	-
		OMNIGLOT	124.2	124.4	124.9	129.8
	MNIST-threshold	98.2	114.0	113.7
Two layers	MNIST-statiC	105.8	105.6	105.5
	OMNIGLOT	118.3	119.1	118.8
MNlST
——Tlafclp(REBAR)
——VaMaUen(REBAR)
——Trah 帕(RELAX)
——VaidaiIMi(RELAX)
——TtakIk^(ARM)
——VaMatkKi(ARM)
Trakikig(GurribeI)
VaIdatIon(Gurribd)
100∞0 20080 30080 4880 5∞∞0 6000(
Steps
(a) Nonlinear
150
140
130
120
'0	IOOo8 200∞0 300000 400000 500∞0 60∞C
Steps
140
130
S
111 120
110
100
(C) Linear two layers
03W.
MNIST
2000 «00 6000 0000 10000 12000 14000
Time(se∞πds)
(d) Nonlinear
(b) Linear
170160150140130120110
09—1山‘
(e) Linear	⑴ Linear two layers
Figure 6:	Training and validation negative ELBOs on MNIST-threshold with respeCt to the training iterations,
shown in the top row, and with respeCt to the wall CloCk times on Tesla-K40 GPU, shown in the bottom row, for
three differently struCtured Bernoulli VAEs.
100∞0 200∞0 3000∞ 40080 5∞0∞ 6088
Steps
OmEglot
(a) Nonlinear
Omnigtot
Time(se∞nds)
(d) Nonlinear
----Trainmg(REBAR)
——Vaidation(REBAR)
----Tisirmg(RELAX)
——VaMaticn(RELAX)
----Treirwig(ARM)
Vaidation(ARM)
I(GumbeI)
CXi(Gunbei)
Steps
——Trafc 帕(REBAR)
——VaMetkm(REBAR)
——Trafc 帕(RELAX)
——VaMaOon(RELAX)
——TtBkIk^(ARM)
——VaMaOon(ARM)
Trakikig(Gumbd)
Valdatlon(Ginibel)
(b) Linear
OmnigIOt
140
18。28。3000 4000 SOOO 6000 7000 8000
Time(Se8∏ds)
(e) Linear
140
139
130
125
120
115
O 200000	400000	600∞0 0OO∞O IoOOoOO
Steps
——IYaInIng(REBAR)
——VeIIcIatIon(REBAR)
——TfaInm(RELAX)
——VaIIdatIon(REI-AX)
——TfaInIng(ARM)
——VeIIcIatIon(ARM)
----TfaInIng(GumbeI)
ValldatIon(Gumbel)
(C) Linear two layers
(f) Linear two layers
Figure 7:	Training and validation negative ELBOs on OMNIGLOT with respeCt to the training iterations, shown
in the top row, and with respeCt to the wall CloCk times on Tesla-K40 GPU, shown in the bottom row, for three
differently struCtured Bernoulli VAEs.
20
Published as a conference paper at ICLR 2019
Figure 8: Randomly selected example results of predicting the lower half of a MNIST digit given its
upper half, using a binary stochastic network, which has two binary linear stochastic hidden layers
and is trained by the ARM estimator based maximum likelihood estimation. Red squares highlight
notable variations between two random draws.
H^l⅜7 7λv^6ui
q 7
夕，
互“
r I
3 q
7 o
√ 7
S O
夕O
/ S-
7∕<√6∕p05373
c7∕3j37夕？26s
go//esa:ri r∙6
13 q 4
937$
q 7 a
C "/
夕I q
“ q 5
7 〃 N
v 7 x
S 0 4r
夕o，
/歹2
7∕j6∕p-5 373
c7∕^37彳 226s
0 7/ /727476
OyZr∙75l13q4
7q Z√5 q
21