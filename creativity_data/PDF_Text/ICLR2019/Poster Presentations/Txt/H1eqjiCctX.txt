Published as a conference paper at ICLR 2019
Understanding Composition of Word Embed-
dings via Tensor Decomposition
Abraham Frandsen & Rong Ge
Department of Computer Science
Duke University
Durham, NC 27708, USA
{abef,rongge}@cs.duke.edu
Ab stract
Word embedding is a powerful tool in natural language processing. In this paper
We consider the problem of word embedding composition - given vector represen-
tations of two words, compute a vector for the entire phrase. We give a generative
model that can capture specific syntactic relations between words. Under our
model, we prove that the correlations between three words (measured by their PMI)
form a tensor that has an approximate low rank Tucker decomposition. The result
of the Tucker decomposition gives the word embeddings as well as a core tensor,
which can be used to produce better compositions of the word embeddings. We also
complement our theoretical results with experiments that verify our assumptions,
and demonstrate the effectiveness of the new composition method.
1	Introduction
Word embeddings have become one of the most popular techniques in natural language processing. A
word embedding maps each word in the vocabulary to a low dimensional vector. Several algorithms
(e.g., Mikolov et al. (2013); Pennington et al. (2014)) can produce word embedding vectors whose
distances or inner-products capture semantic relationships between words. The vector representations
are useful for solving many NLP tasks, such as analogy tasks(Mikolov et al., 2013) or serving as
features for supervised learning problems (Maas et al., 2011).
While word embeddings are good at capturing the semantic information of a single word, a key
challenge is the problem of composition: how to combine the embeddings of two co-occurring,
syntactically related words to an embedding of the entire phrase. In practice composition is often
done by simply adding the embeddings of the two words, but this may not be appropriate when the
combined meaning of the two words differ significantly from the meaning of individual words (e.g.,
“complex number” should not just be “complex”+“number”).
In this paper, we try to learn a model for word embeddings that incorporates syntactic information and
naturally leads to better compositions for syntactically related word pairs. Our model is motivated
by the principled approach for understanding word embeddings initiated by Arora et al. (2015), and
models for composition similar to Coecke et al. (2010).
Arora et al. (2015) gave a generative model (RAND-WALK) for word embeddings, and showed
several previous algorithms can be interpreted as finding the hidden parameters of this model.
However, the RAND-WALK model does not treat syntactically related word-pairs differently from
other word pairs. We give a generative model called syntactic RAND-WALK (see Section 3) that is
capable of capturing specific syntactic relations (e.g., adjective-noun or verb-object pairs). Taking
adjective-noun pairs as an example, previous works (Socher et al., 2012; Baroni & Zamparelli, 2010;
Maillard & Clark, 2015) have tried to model the adjective as a linear operator (a matrix) that can
act on the embedding of the noun. However, this would require learning a d × d matrix for each
adjective while the normal embedding only has dimension d. In our model, we use a core tensor
T ∈ Rd×d×d to capture the relations between a pair of words and its context. In particular, using the
tensor T and the word embedding for the adjective, it is possible to define a matrix for the adjective
that can be used as an operator on the embedding of the noun. Therefore our model allows the same
interpretations as many previous models while having much fewer parameters to train.
1
Published as a conference paper at ICLR 2019
One salient feature of our model is that it makes good use of high order statistics. Standard word
embeddings are based on the observation that the semantic information of a word can be captured by
words that appear close to it. Hence most algorithms use pairwise co-occurrence between words to
learn the embeddings. However, for the composition problem, the phrase of interest already has two
words, so it would be natural to consider co-occurrences between at least three words (the two words
in the phrase and their neighbors).
Based on the model, we can prove an elegant relationship between high order co-occurrences of
words and the model parameters. In particular, we show that if we measure the Pointwise Mutual
Information (PMI) between three words, and form an n × n × n tensor that is indexed by three
words a, b, w, then the tensor has a Tucker decomposition that exactly matches our core tensor T and
the word embeddings (see Section 2, Theorem 1, and Corollary 1). This suggests a natural way of
learning our model using a tensor decomposition algorithm.
Our model also allows us to approach the composition problem with more theoretical insights.
Based on our model, if words a, b have the particular syntactic relationships we are modeling, their
composition will be a vector Va + Vb + T(va, vb, ∙). Here va, Vb are the embeddings for word a and b,
and the tensor gives an additional correction term. By choosing different core tensors it is possible to
recover many previous composition methods. We discuss this further in Section 3.
Finally, we train our new model on a large corpus and give experimental evaluations. In the
experiments, we show that the model learned satisfies the new assumptions that we need. We also
give both qualitative and quantitative results for the new embeddings. Our embeddings and the novel
composition method can capture the specific meaning of adjective-noun phrases in a way that is
impossible by simply “adding” the meaning of the individual words. Quantitative experiment also
shows that our composition vector are better correlated with humans on a phrase similarity task.
1.1	Related work
Syntax and word embeddings Many well-known word embedding methods (e.g., Pennington et al.
(2014); Mikolov et al. (2013)) don’t explicitly utilize or model syntactic structure within text. Andreas
& Klein (2014) find that such syntax-blind word embeddings fail to capture syntactic information
above and beyond what a statistical parser can obtain, suggesting that more work is required to build
syntax into word embeddings.
Several syntax-aware embedding algorithms have been proposed to address this. Levy & Goldberg
(2014a) propose a syntax-oriented variant of the well-known skip-gram algorithm of Mikolov et al.
(2013), using contexts generated from syntactic dependency-based contexts obtained with a parser.
Cheng & Kartsaklis (2015) build syntax-awareness into a neural network model for word embeddings
by indroducing a negative set of samples in which the order of the context words is shuffled, in hopes
that the syntactic elements which are sensitive to word order will be captured.
Word embedding composition Several works have addressed the problem of composition for
word embeddings. On the theoretical side, Gittens et al. (2017) give a theoretical justification for
additive embedding composition in word models that satisfy certain assumptions, such as the skip-
gram model, but these assumptions don’t address syntax explicitly. Coecke et al. (2010) present
a mathematical framework for reasoning about syntax-aware word embedding composition that
motivated our syntactic RAND-WALK model. Our new contribution is a concrete and practical
learning algorithm with theoretical guarantees. Mitchell & Lapata (2008; 2010) explore various
composition methods that involve both additive and multiplicative interactions between the component
embeddings, but some of these are limited by the need to learn additional parameters post-hoc in a
supervised fashion.
Guevara (2010) get around this drawback by first training word embeddings for each word and also
for tokenized adjective-noun pairs. Then, the composition model is trained by using the constituent
adjective and noun embeddings as input and the adjective-noun token embedding as the predictive
target. Maillard & Clark (2015) treat adjectives as matrices and nouns as vectors, so that the
composition of an adjective and noun is just matrix-vector multiplication. The matrices and vectors
are learned through an extension of the skip-gram model with negative sampling. In contrast to
these approaches, our model gives rise to a syntax-aware composition function, which can be learned
2
Published as a conference paper at ICLR 2019
along with the word embeddings in an unsupervised fashion, and which generalizes many previous
composition methods (see Section 3.3 for more discussion).
Tensor factorization for word embeddings As Levy & Goldberg (2014b) and Li et al. (2015)
point out, some popular word embedding methods are closely connected matrix factorization problems
involving pointwise mutual information (PMI) and word-word co-occurrences. It is natural to consider
generalizing this basic approach to tensor decomposition. Sharan & Valiant (2017) demonstrate this
technique by performing a CP decomposition on triple word co-occurrence counts. Bailey & Aeron
(2017) explore this idea further by defining a third-order generalization of PMI, and then performing a
symmetric CP decomposition on the resulting tensor. In contrast to these recent works, our approach
arives naturally at the more general Tucker decomposition due to the syntactic structure in our model.
Our model also suggests a different (yet still common) definition of third-order PMI.
2	Preliminaries
Notation For a vector v, we use kvk to denote its Euclidean norm. For vectors u, v we use
hu, vi to denote their inner-product. For a matrix M, we use kM k to denote its spectral norm,
kMkF = Pi,j Mi2,j to denote its Frobenius norm, and Mi,: to denote it’s i-th row. In this paper,
We will also often deal with 3rd order tensors, which are just three-way indexed arrays. We use 0 to
denote the tensor product: if u,v,w ∈ Rd are d-dimensional vectors, T = U 0 V 0 W is a d X d X d
tensor whose entries are Ti,j,k = uivjwk.
Tensor basics Just as matrices are often viewed as bilinear functions, third order tensors can be
interpreted as trilinear functions over three vectors. Concretely, let T be a d X d X d tensor, and let
x, y, z ∈ Rd. We define the scalar T(x, y, z) ∈ R as follows
d
T(x, y, z) = X Ti,j,kx(i)y(j)z(k).
i,j,k=1
This operation is linear in x, y and z . Analogous to applying a matrix M to a vector v (with the result
vector Mv), we can also apply a tensor T to one or two vectors, resulting in a matrix and a vector,
respectively:
dd
T(X,y,Xk)= X Ti,j,kχ(i)y(j),	T(X,∙,∙)j,k = XTi,j,kχ(i)
i,j=1	i=1
We will make use of the simple facts that(z, T(x, y, •))= T(x, y, Z) and [T(x, ∙, ∙)]>y = T(x, y, ∙).
Tensor decompositions Unlike matrices, there are several different definitions for the rank of a
tensor. In this paper we mostly use the notion of Tucker rank Tucker (1966). A tensor T ∈ Rn×n×n
has Tucker rank d, if there exists a core tensor S ∈ Rd×d×d and matrices A, B, C ∈ Rn×d such that
d
Ti,j,k
Si0,j0,k0 Ai,i0 Bj,j0 Ck,k0
i0,j0,k0=1
S(Ai,:, Bj,:, Ck,:),
The equation above is also called a Tucker decomposition of the tensor T. The Tucker decomposition
for a tensor can be computed efficiently.
When the core tensor S is restricted to a diagonal tensor (only nonzero at entries Si,i,i), the decompo-
sition is called a CP decomposition Carroll & Chang (1970); Harshman (1970) which can also be
written as T = Pid=1 Si,i,iAi,: 0 Bi,: 0 Ci,:. In this case, the tensor T is the sum of d rank-1 tensors
(Ai,: 0 Bi,: 0 Ci,:). However, unlike matrix factorizations and the Tucker decomposition, the CP
decomposition of a tensor is hard to compute in the general case (Hastad, 1990; Hillar & Lim, 2013).
Later in Section 4 we will also see why our model for syntactic word embeddings naturally leads to a
Tucker decomposition.
3
Published as a conference paper at ICLR 2019
Figure 1: Graphical models of RAND-WALK (left) and our new model (right), depicting a syntactic
word pair (wt, wt0). Green nodes correspond to observed variables, white nodes to latent variables.
3	S yntactic RAND-WALK model
In this section, we introduce our syntactic RAND-WALK model and present formulas for inference
in the model. We also derive a novel composition technique that emerges from the model.
RAND-WALK model We first briefly review the RAND-WALK model (Arora et al., 2015). In this
model, a corpus of text is considered as a sequence of random variables w1, w2, w3, . . ., where wt
takes values in a vocabulary V of n words. Each word w ∈ V has a word embedding vw ∈ Rd . The
prior for the word embeddings is Vw = S ∙ v, where S is a positive bounded scalar random variable
with constant expectation τ and upper bound κ, and V 〜 N(0,I).
The distribution of each wt is determined in part by a random walk {ct ∈ Rd | t = 1, 2, 3 . . .}, where
Ct - called a discourse vector - represents the topic of the text at position t. This random walk is
slow-moving in the sense that kct+1 - ct k is small, but mixes quickly to a stationary distribution that
is uniform on the unit sphere, which we denote by C .
Let C denote the sequence of discourse vectors, and let V denote the set of word embeddings. Given
these latent variables, the model specifies the following conditional probability distribution:
Pr[wt = W |ct] H exp(hvw, cj∖	(1)
The graphical model depiction of RAND-WALK is shown in Figure 1a.
3.1	S yntactic RAND-WALK
One limitation of RAND-WALK is that it can’t deal with syntactic relationships between words.
Observe that conditioned on ct and V , wt is independent of the other words in the text. However,
in natural language, words can exhibit more complex dependencies, e.g. adjective-noun pairs,
subject-verb-object triples, and other syntactic or grammatical structures.
In our syntactic RAND-WALK model, we start to address this issue by introducing direct pairwise
word dependencies in the model. When there is a direct dependence between two words, we call the
two words a syntactic word pair. In RAND-WALK, the interaction between a word embedding V
and a discourse vector c is mediated by their inner product hV, ci. When modeling a syntactic word
pair, we need to mediate the interaction between three quantities, namely a discourse vector c and the
word embeddings V and V0 of the two relevant words. A natural generalization is to use a trilinear
form defined by a tensor T , i.e.
d
T(V, V0, c) = X Ti,j,kV(i)V0(j)c(k).
i,j,k=1
Here, T ∈ Rd×d×d is also a latent random variable, which we call the composition tensor.
4
Published as a conference paper at ICLR 2019
We model a syntactic word pair as a single semantic unit within the text (e.g. in the case of adjective-
noun phrases). We realize this choice by allowing each discourse vector ct to generate a pair of words
wt , wt0 with some small probability psyn. To generate a syntactic word pair wt , wt0 , we first generate
a root word wt conditioned on ct with probability proportional to exp(hct, wti), and then we draw
wt0 from a conditional distribution defined as follows:
Pr[w0 = b | Wt = a, C, V ] H exp(hct,vbi + T (va,vb,ct)).	(2)
Here exp(hct, vbi) would be proportional to the probability of generating word b in the origi-
nal RAND-WALK model, without considering the syntactic relationship. The additional term
T(va , vb, ct ) can be viewed as an adjustment based on the syntactic relationship.
We call this extended model Syntactic RAND-WALK. Figure 1b gives the graphical model depiction
for a syntactic word pair, and we summarize the model below.
Definition 1 (Syntactic RAND-WALK model). The model consists of the following:
1.	Each word W in vocabulary has a corresponding embedding Vw 〜S ∙ Vw, where S ∈ R≥o is
bounded by K and E[s] = T; Vw 〜N(0,Id×d).
2.	The sequence of discourse vectors ci,…,q are generated by a random walk on the unit
sphere, kct 一 ct+i k ≤ ew∕√d and the stationary distribution is uniform.
3.	For each ct, with probability 1 -psyn, it generates one word Wt with probability proportional
to exp(hct, Vwt i).
4.	For each ct, with probability psyn, it generates a syntactic pair Wt , Wt0 with probability
proportional to exp(hct, Vwt i) and exp(hct, Vw0i + T(Vwt, Vw0 , ct)) respectively, where T
is a d × d × d composition tensor.
3.2	Inference in the model
We now calculate the marginal probabilities of observing pairs and triples of words under the syntactic
RAND-WALK model. We will show that these marginal probabilities are closely related to the model
parameters (word embeddings and the composition tensor). All proofs in this section are deferred to
supplementary material.
Throughout this section, we consider two adjacent context vectors ct and ct+i, and condition on
the event that ct generated a single word and ct+i generated a syntactic pair1. The main bottleneck
in computing the marginal probabilities is that the conditional probailities specified in equations
(1) and (2) are not normalized. Indeed, for these equations to be exact, we would need to divide
by the appropriate partition functions, namely Zct := Pw∈V exp(hVw , cti) for the former and
Zct,a := Pw∈V exp(hct, Vwi + T(Va, Vw, ct)) for the latter. Fortunately, we show that under mild
assumptions these quantities are highly concentrated. To do that we need to control the norm of the
composition tensor.
Definition 2. The composition tensor T is (K, )-bounded, iffor any word embedding Va, Vb, we
have
Kd2
kT(Va, ∙, ∙)+ Ik2 ≤ r1i-； kT(Va, ∙, ∙)+ IkF ≤ Kd	kT(Va,Vb,∙)k2 ≤ Kd.
log n
To make sure exp(hct, Vwi+T(Va, Vw, ct)) are within reasonable ranges, the value K in this definition
should be interpreted as an absolute constant (like 5, similar to previous constants κ and τ). Intuitively
these conditions make sure that the effect of the tensor cannot be too large, while still making sure
the tensor component T(Va, Vb, c) can be comparable (or even larger than) hVb, ci. We have not tried
to optimize the log factors in the constraint for ∣∣T(va, ∙, ∙) + Ik2.
Note that if the tensor component T(va, ∙, ∙) has constant singular values (hence comparable to I),
We know these conditions will be satisfied with K = O(1) and E = O(lo√gn). Later in Section 5
we verify that the tensors we learned indeed satisfy this condition. Now we are ready to state the
concentration of partition functions:
1As we will see in Section 5, in practice it is easy to identify which words form a syntactic pair, so it is
possible to condition on this event in training.
5
Published as a conference paper at ICLR 2019
Lemma 1 (Concentration of partition functions). For the syntactic RAND-WALK model, there exists
a constant Z such that
Pr [(1 - z)Z ≤ Zc ≤ (1 + z)Z] ≥ 1 - δ,
C〜C
for Ez = O(1/√n) and δ = exp(-Ω(log2 n)).
Furthermore, if the tensor T is (K, )-bounded, then for any fixed word a ∈ V, there exists a constant
Za such that
Pr [(1 - Ez,a)Za ≤ Zc,a ≤ (1 + Ez,a)Za] ≥ 1 - δ,
for Ez,a = O(E) + (O(1∕√n) and δ = exp(-Ω(log2 n)).
Using this lemma, we can obtain simple expressions for co-occurrence probabilities. In particular, for
any fixed w, a, b ∈ V , we adopt the following notation:
p(a) := Pr[wt+1 = a] p(w, a) := Pr[wt = w, wt+1 = a]
p([a, b]) := Pr[wt+1 = a, wt0+1 = b] p(w, [a, b]) := Pr[wt = w, wt+1 = a, wt0+1 = b].
Here in particular we use [a, b] to highlight the fact that a and b form a syntactic pair. Note p(w, a)
is the same as the co-occurrence probability of words w and a if both of them are the only word
generated by the discourse vector. Later we will also use p(w, b) to denote Pr[wt = w, wt+1 = b]
(not Pr[wt = w, wt0+1 = b]).
We also require two additional properties of the word embeddings, namely that they are norm-bounded
above by some constant times √d, and that all partition functions are bounded below by a positive
constant. Both of these properties hold with high probability over the word embeddings provided
n dlogd andd log n, as shown in the following lemma:
Lemma 2. Assume that the composition tensor T is (K, E)-bounded, where K is a constant. With
probability at least 1 - δ1 - δ2 over the word vectors, where δ1 = exp(Θ(d log d) - Θ(n)) and
δ2 = exp(Θ(log n) - Θ(d)), there exist positive absolute constants γ and β such that kvik ≤ κγ for
each i ∈ V and Zc ≥ β and Zc,a ≥ βfor any unit vector c ∈ Rd and any word a ∈ V.
We can now state the main result.
Theorem 1. Suppose that the events referred to in Lemma 1 hold. Then
logp(a) = kvadk----log Z ± Ep	(3)
logp(w, a) = kvw +vak------2 log Z ± Ep	(4)
1	k kv Ilva + vb + T(va,vb, ∙)k2 1 ,7 1 ,7 ,	，八
logp([a, b]) =----------2d--------------log Z - log Za ± Ep	(5)
1 k Γ kv Ilvw + va + Vb + T(Va,vb, ∙)∣∣2 Oi 7 1 7 _L	心
logP(w, [a, b]) =-------------2d----------------2 log Z - log Za ± Ep	(6)
Here Ep = O(E + Ew) + O(1∕√n + 1/d), where E is from the (K, e)-boundedness of T and Ew is
from Definition 1.
3.3 Composition
Our model suggests that the latent discourse vectors contain the meaning of the text at each location.
It is therefore reasonable to view the discourse vector c corresponding to a syntactic word pair (a, b)
as a suitable representation for the phrase as a whole. The posterior distribution of c given (a, b)
satisfies
Pr[ct = C | Wt = a, w0 = b] H --ɪ— exp(hVa + Vb + T(va,Vb, ∙),ci) Pr[ct = c].
Zc Zc,a
Since Pr[Ct = C] is constant, and since Zc and Zc,a concentrate on values that don’t depend on C, the
MAP estimate of C given [a, b], which we denote by ^, satisfies
^ ≈ argmaxexp(hva + Vb + T(va,Vb, ∙), Ci)
kck=1
Va + Vb + T(Va,Vb, ∙)
IlVa + Vb + T(Va, Vb, ∙)∣
6
Published as a conference paper at ICLR 2019
Hence, we arrive at our basic tensor composition: for a syntactic word pair (a, b), the composite
embedding for the phrase is Va + Vb + T(va, vb, ∙).
Note that our composition involves the traditional additive composition va + vb , plus a correction
term T(v0, Vb, ∙). We can view T(v0, Vb, ∙) as a matrix-vector multiplication [T(va, ∙, ∙)]>Vb, i.e. the
composition tensor allows us to compactly associate a matrix with each word in the same vein as
Maillard & Clark (2015). Depending on the actual value of T, the term T(va, vb ∙) can also recover
any manner of linear or multiplicative interactions between Va and Vb, such as those proposed in
Mitchell & Lapata (2010).
4 Learning
In this section we discuss how to learn the parameters of the syntactic RAND-WALK model. Theorem
1 provides key insights into the learning problem, since it relates joint probabilities between words
(which can be estimated via co-occurrence counts) to the word embeddings and composition tensor.
By examining these equations, we can derive a particularly simple formula that captures these
relationships. To state this equation, we define the PMI for 3 words as
p(w, [a, b])p(a)p(b)p(w)
PMI3(a, b, w) ：= log1——「~~a “ 小∙	(7)
p(w, a)p(w, b)p([a, b])
We note that this is just one possible generalization of pointwise mutual information (PMI) to several
random variables, but in the context of our model, it is a very natural definition as all the partition
numbers will be canceled out. Indeed, as an immediate corollary of Theorem 1, we have
Corollary 1. Suppose that the events referred to in Lemma 1 hold. Then for p same as Theorem 1
PMI3(a, b, W) = dT(va, Vb, Vw) 土 O4).	(8)
That is, if we consider PMI3(a, b, w) as a n × n × n tensor, Equation equation 8 is exactly a
Tucker decomposition of this tensor of Tucker rank d. Therefore, all the parameters of the syntactic
RAND-WALK model can be obtained by finding the Tucker decomposition of the PMI3 tensor. This
equation also provides a theoretical motivation for using third-order pointwise mutual information in
learning word embeddings.
4.1	Implementation
We now discuss concrete details about our implementation of the learning algorithm.2 *
Corpus. We train our model using a February 2018 dump of the English Wikipedia. The text is
pre-processed to remove non-textual elements, stopwords, and rare words (words that appear less than
1000 within the corpus), resulting in a vocabulary of size 68,279. We generate a matrix of word-word
co-occurrence counts using a window size of 5. To generate the tensors of adjective-noun-word
and verb-object-word co-occurrence counts, we first run the Stanford Dependency Parser (Chen &
Manning, 2014) on the corpus in order to identify all adjective-noun and verb-object word pairs, and
then use context windows that don’t cross sentence boundaries to populate the triple co-occurrence
counts.
Training. We first train the word embeddings according to the RAND-WALK model, following
Arora et al. (2015). Using the learned word embeddings, we next train the composition tensor T via
the following optimization problem
min
T,{Cw},C
E f (X(a,b),w ) (log(X(a,b),w ) - kVw
(a,b),w
+ Va + Vb + T(Va,Vb, ∙)∣∣2
where X(a,b),w denotes the number of co-occurrences of word w with the syntactic word pair (a, b)
(a denotes the noun/object) andf (x) = min(x, 100). This objective function isn’t precisely targeting
2code for preprocessing, training, and experiments can be found at https://github.com/
abefrandsen/syntactic-rand-walk
7
Published as a conference paper at ICLR 2019
Figure 2: Histograms of partition functions Zc,a (x-axis is Zc,a/E[Zc,a])
the Tucker decomposition of the PMI3 tensor, but it is analogous to the training criterion used in
Arora et al. (2015), and can be viewed as a negative log-likelihood for the model. To reduce the
number of parameters, we constrain T to have CP rank 1000. We also trained the embeddings and
tensor jointly, but found that this approach yields very similar results. In all cases, we utilize the
Tensorflow framework (Abadi et al., 2016) with the Adam optimizer (Kingma & Ba, 2014) (using
default parameters), and train for 1-5 epochs.
5	Experimental verification
In this section, we verify and evaluate our model empirically on select qualitative and quantitative
tasks. In all of our experiments, we focus solely on syntactic word pairs formed by adjective-noun
phrases, where the noun is considered the root word.
5.1	Model verification
Arora et al. (2015) empirically verify the model assumptions of RAND-WALK, and since we trained
our embeddings in the same way, we don’t repeat their verifications here. Instead, we verify two key
properties of syntactic RAND-WALK.
Norm of composition tensor We check the assumptions that the tensor T is (K, )-bounded.
Ranging over all adjective-noun pairs in the corpus, We find that d ∣∣T(v。，∙, ∙) +11∣2 has mean 0.052
and maximum 0.248, d ∣∣T(v。, ∙, .) + IkF has mean 1.61 and maximum 3.23, and d ∣∣T(v。, vb, ∙)∣∣2
has mean 0.016 and maximum 0.25. Each of these three quantities has a Well-bounded mean, but
∣∣T(v。，∙，∙) + I∣2 has some larger outliers. If we ignore the log factors (which are likely due to
artifacts in the proof) in Definition 2, the tensor is (K, ) bounded for K = 4 and = 0.25.
Concentration of partition functions In addition to Definition 2, we also directly check its impli-
cations: our model predicts that the partition functions Zc,。 concentrate around their means. To check
this, given a noun a, we draw 1000 random vectors c from the unit sphere, and plot the histogram of
Zc,a.Results for a few randomly selected words a are given in Figure 2. All partition functions that
we inspected exhibited good concentration.
5.2	Qualitative analysis of composition
We test the performance of our new composition for adjective-noun and verb-object pairs by looking
for the words with closest embedding to the composed vector. For a phrase (a, b), we compute
C = Va + Vb + T(v。，vb, ∙), and then retrieve the words W whose embeddings Vw have the largest
cosine similarity to c. We compare our results to the additive composition method. Tables 1 and 2
show results for three adjective-noun and verb-object phrases. In each case, the tensor composition
is able to retrieve some words that are more specifically related to the phrase. However, the tensor
composition also sometimes retrieves words that seem unrelated to either word in the phrase. We
conjecture that this might be due to the sparseness of co-occurrence of three words. We also
observed cases where the tensor composition method was about on par with or inferior to the additive
composition method for retrieving relevant words, particularly in the case of low-frequency phrases.
More results can be found in supplementary material.
8
Published as a conference paper at ICLR 2019
Table 1: Top 10 words relating to various adjective-noun phrases
civil war	complex numbers	national park
additive	tensor	additive	tensor	additive	tensor
war	civil	complex	complex	national	yosemite
civil	somalian	numbers	eigenvalues	park	denali
military	eicher	number	numbers	parks	gunung
army	crimean	function	hermitian	recreation	kenai
conflict	laotian	complexes	quaternions	forest	nps
wars	francoist	functions	marginalia	historic	teton
fought	ulysses	integers	azadi	heritage	refuges
revolutionary	liberian	multiplication	rationals	wildlife	tilden
forces	confederate	algebraic	holomorphic	memorial	snowdonia
outbreak	midst	integer	rhythmically	south	jigme
Table 2: Top 10 words relating to various verb-object phrases
took place	took part	took lead
additive	tensor	additive	tensor	additive	tensor
place	occurred	part	participated	took	equalised
took	scheduled	took	participating	lead	halftime
death	commenced	taking	participate	taking	nailing
take	event	take	culminated	take	kenseth
taking	events	taken	organised	went	fumbled
birth	culminated	takes	participation	led	touchdown
taken	thursday	became	hostilities	taken	furlongs
takes	friday	came	culminating	came	trailed
came	postponed	put	invasion	put	keselowski
held	lasted	whole	undertook	wanted	peloton
9
Published as a conference paper at ICLR 2019
5.3	Phrase Similarity
We also test our tensor composition method on a adjective-noun phrase similarity task using the
dataset introduced by Mitchell & Lapata (2010). The data consists of 108 pairs each of adjective-noun
and verb-object phrases that have been given similarity ratings by a group of 54 humans. The task is
to use the word embeddings to produce similarity scores that correlate well with the human scores;
we use both the Spearman rank correlation and the Pearson correlation as evaluation metrics for this
task. We note that the human similarity judgments are somewhat noisy; intersubject agreement for
the task is 0.52 as reported in Mitchell & Lapata (2010).
Given a phrase (a, b) with embeddings va , vb, respectively, we found that the tensor composition
Va + Vb + T(va,vb, ∙) yields worse performance than the simple additive composition Va + vb. For this
reason, we consider a weighted tensor composition Va + Vb + αT(va, Vb, ∙) with a ≥ 0. Following
Mitchell & Lapata (2010), we split the data into a development set of 18 humans and a test set of
the remaining 36 humans. We use the development set to select the optimal scalar weight for the
weighted tensor composition, and using this fixed parameter, we report the results using the test set.
We repeat this three times, rotating over folds of 18 subjects, and report the average results.
As a baseline, we also report the average results using just the additive composition, as well as a
weighted additive composition βVa + Vb , where β ≥ 0. We select β using the development set
(“weighted1") and the test set (“weighted2"). We allow weighted2 to cheat in this way because
it provides an upper bound on the best possible weighted additive composition. Additionally, we
compare our method to the smoothed inverse frequency (“sif") weighting method that has been
demonstrated to be near state-of-the-art for sentence embedding tasks (Arora et al., 2016). We also
test embeddings of the form P + γωaωbT(v0, Vb, ∙) (“sif+tensor"), where P is the sif embedding for
(a, b), ωa and ωb are the smoothed inverse frequency weights used in the sif embeddings, and γ is a
positive weight selected using the development set. The motivation for this hybrid embedding is to
evaluate the extent to which the sif embedding and tensor component can independently improve
performance on this task.
We perform these same experiments using two other standard sets of pre-computed word embeddings,
namely GloVe3 and carefully optimized cbow vectors4 (Mikolov et al., 2017). We re-trained the
composition tensor using the same corpus and technique as before, but substituting these pre-computed
embeddings in place of the RAND-WALK (rw) embeddings. However, a bit of care must be taken
here, since our syntactic RAND-WALK model constrains the norm of the word embeddings to be
related to the frequency of the words, whereas this is not the case with the pre-computed embeddings.
To deal with this, we rescaled the pre-computed embeddings sets to have the same norms as their
counterparts in the rw embeddings, and then trained the composition tensor using these rescaled
embeddings. At test time, we use the original embeddings to compute the additive components of
our compositions, but use the rescaled versions when computing the tensor components.
The results for adjective-noun phrases are given in Tables 3. We observe that the tensor composition
outperforms the additive compositions on all embedding sets apart from the Spearman correlation
on the cbow vectors, where the weighted additive 2 method has a slight edge. The sif embeddings
outperform the additive and tensor methods, but combining the sif embeddings and the tensor
components yields the best performance across the board, suggesting that the composition tensor
captures additional information beyond the individual word embeddings that is useful for this task.
There was high consistency across the folds for the optimal weight parameter α, with α= 0.4 for the
rw embeddings, α= .2, .3 for the glove embeddings, and α= .3 for the cbow embeddings. For the
sif+tensor embeddings, γ was typically in the range [.1, .2].
The results for verb-object phrases are given in Table 4. Predicting phrase similarity appears to be
harder in this case. Notably, the sif embeddings perform worse than unweighted vector addition.
As before, we can improve the sif embeddings by adding in the tensor component. The tensor
composition method achieves the best results for the glove and cbow vectors, but weighted addition
works best for the randwalk vectors.
Overall, these results demonstrate that the composition tensor can improve the quality of the phrase
embeddings in many cases, and the improvements are at least somewhat orthogonal to improvements
3obtained from https://nlp.stanford.edu/projects/glove/
4obtained from https://fasttext.cc/docs/en/english-vectors.html
10
Published as a conference paper at ICLR 2019
Table 3: Correlation measures between human judgments and embedding-based similarity scores
(Spearman, Pearson) for adjective-noun phrases across three embedding sets (top scores in each row
are bolded)
	additive	weighted1	Weighted2	tensor	sif	sif+tensor
rw	.446, .438	.444, .448	.452, .453	.460, .465	.482, .477	.482, .481
glove	.357, .336	.351,.334	.358, .345	.368, .347	.429, .434	.433, .437
cbow	.471, .452	.469, .451	.476, .456	.474, .471	.489, .482	.492, .484
Table 4: Correlation measures between human judgments and embedding-based similarity scores
(Spearman, Pearson) for verb-object phrases
	additive weighted1 weighted2 tensor	sif	sif+tensor
rW glove cboW	.379, .370	.391,	.385	.392, .387	.379, .370	.378, .351	.378,	.363 .397, .400	.398,	.404	.401, .404	.410,.420	.387, .380	.411,	.409 .423, .414	.423,	.410	.428, .415	.428,.422	.404, .404	.420,	.417
resulting from the sif embedding method. This suggests that a well-trained composition tensor used
in conjunction with high quality word embeddings and additional embedding composition techniques
has the potential to improve performance in downstream NLP tasks.
Acknowledgments
We thank Yingyu Liang, Mohit Bansal, and Eric Bailey for helpful discussions. Support from NSF
CCF-1704656 is gratefully acknowledged.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale
machine learning. In OSDI, volume 16, pp. 265-283, 2016.
Jacob Andreas and Dan Klein. How much do word embeddings encode about syntax? In Proceedings
of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), volume 2, pp. 822-827, 2014.
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Rand-walk: A latent
variable model approach to word embeddings. arXiv preprint arXiv:1502.03520, 2015.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. 2016.
Eric Bailey and Shuchin Aeron. Word embeddings via tensor factorization. arXiv preprint
arXiv:1704.02686, 2017.
Marco Baroni and Roberto Zamparelli. Nouns are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing, pp. 1183-1193. Association for Computational
Linguistics, 2010.
J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling
via an n-way generalization of “eckart-young” decomposition. Psychometrika, 35(3):283-319,
1970.
Danqi Chen and Christopher Manning. A fast and accurate dependency parser using neural networks.
In Proceedings of the 2014 conference on empirical methods in natural language processing
(EMNLP), pp. 740-750, 2014.
11
Published as a conference paper at ICLR 2019
Jianpeng Cheng and Dimitri Kartsaklis. Syntax-aware multi-sense word embeddings for deep
compositional models of meaning. arXiv preprint arXiv:1508.02354, 2015.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical foundations for a composi-
tional distributional model of meaning. arXiv preprint arXiv:1003.4394, 2010.
Alex Gittens, Dimitris Achlioptas, and Michael W Mahoney. Skip-gram-zipf+ uniform= vector
additivity. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1, pp. 69-76, 2017.
Emiliano Guevara. A regression model of adjective-noun compositionality in distributional semantics.
In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, pp.
33-37. Association for Computational Linguistics, 2010.
Richard A Harshman. Foundations of the parafac procedure: Models and conditions for an" explana-
tory" multimodal factor analysis. 1970.
Johan Hastad. Tensor rank is np-complete. Journal of Algorithms ,11(4):644-654, 1990.
Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM
(JACM), 60(6):45, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection.
Annals of Statistics, pp. 1302-1338, 2000.
Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),
volume 2, pp. 302-308, 2014a.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances
in neural information processing systems, pp. 2177-2185, 2014b.
Yitan Li, Linli Xu, Fei Tian, Liang Jiang, Xiaowei Zhong, and Enhong Chen. Word embedding
revisited: A new representation learning and explicit matrix factorization perspective. In IJCAI, pp.
3650-3656, 2015.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015.
Jean Maillard and Stephen Clark. Learning adjective meanings with a tensor-based skip-gram model.
In Proceedings of the Nineteenth Conference on Computational Natural Language Learning, pp.
327-331, 2015.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in neural information processing
systems, pp. 3111-3119, 2013.
Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Advances
in pre-training distributed word representations. arXiv preprint arXiv:1712.09405, 2017.
Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. proceedings of
ACL-08: HLT, pp. 236-244, 2008.
Jeff Mitchell and Mirella Lapata. Composition in distributional models of semantics. Cognitive
science, 34(8):1388-1429, 2010.
Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In "Proceedings of the ACL", 2004.
12
Published as a conference paper at ICLR 2019
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In EMNLP, volume 14, pp. 1532-1543, 2014.
Vatsal Sharan and Gregory Valiant. Orthogonalized als: A theoretically principled tensor decomposi-
tion algorithm for practical use. arXiv preprint arXiv:1703.01804, 2017.
Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 joint conference on empirical
methods in natural language processing and computational natural language learning, pp. 1201-
1211. Association for Computational Linguistics, 2012.
Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):
279-311, 1966.
13
Published as a conference paper at ICLR 2019
Table 5: Top 10 words relating to various verb-object phrases
giving birth		solve problem		changing name	
additive	tensor	additive	tensor	additive	tensor
birth	stillborn	problem	analytically	name	rebrand
giving	unborn	solve	creatively	changing	refocus
place	pregnant	problems	solve	change	redevelop
death	fathered	solving	subconsciously	changed	rebranding
give	litters	solved	devising	names	forgo
date	childbirth	solves	devise	referring	divest
gave	remarry	understand	proactively	title	rechristened
summary	newborn	resolve	solvers	word	afresh
gives	gestation	solution	extrapolate	actually	rebranded
given	eloped	question	rationalize	something	opting
Table 6: Top 10 words relating to various adjective-noun phrases
united states		soviet union		european union	
additive	tensor	additive	tensor	additive	tensor
united	united	union	union	european	eec
states	states	soviet	soviet	union	ebu
us	emigrating	ussr	sfsr	europe	dismemberment
canada	emirates	russian	disintegration	countries	retort
countries	immigrated	communist	lyudmila	federation	detracts
california	cartographic	russia	dismemberment	nations	arguable
usa	extradited	soviets	brezhnev	soviet	kely
america	senate	moscow	ussr	organisations	eea
kingdom	lighthouses	sfsr	perestroika	socialist	geosciences
nations	stateside	ukraine	zhukov	eu	bugzilla
A	Additional qualitiative results
In this section we present additional qualitiative results demonstrating the use of the composition
tensor for the retrieval of words related to adjective-noun and verb-object phrases.
In Table 5, we show results for the phrases “giving birth", “solve problem", and “changing name".
These phrases are all among the top 500 most frequent verb-object phrases appearing in the training
corpus. In these examples, the tensor-based phrase embeddings retrieve words that are generally
markedly more related to the phrase at hand, and there are no strange false positives. These examples
demonstrate how a verb-object phrase can encompass an action that isn’t implied simply by the object
or verb alone. The additive composition doesn’t capture this action as well as the tensor composition.
Moving on to adjective-noun phrases, in Table 6, we show results for the phrases “United States",
“Soviet Union", and “European Union". These phrases, which all occur with comparatively high
frequency in the corpus, were identified as adjective-noun phrases by the tagger, but they function
more as compound proper nouns. In each case, the additive composition retrieves reasonably relevant
words, while the tensor composition is more of a mixed bag. In the case of “European Union", the
tensor composition does retrieve the highly relevant words eec (European Economic Community) and
eea (European Economic Area), which the additive composition misses, but the tensor composition
also produces several false positives. It seems that for these types of phrases, the additive composition
is sufficient to capture the meaning.
In Table 7, we fix the noun “taste" and vary the modifying adjective to highlight different senses of
the noun. In the case of “expensive taste", both compositions retrieve words that seem to be either
related to “expensive" or “taste", but there don’t seem to be words that are intrinsically related to
the phrase as a whole (with the exception, perhaps, of “luxurious", which the tensor composition
14
Published as a conference paper at ICLR 2019
Table 7: Top 10 words relating to various adjective-noun phrases
expensive taste		awful taste		refined taste	
additive	tensor	additive	tensor	additive	tensor
taste	expensive	taste	taste	taste	refined
expensive	taste	awful	awful	refined	taste
cheaper	costly	smell	smell	flavor	sweeter
flavor	prohibitively	unpleasant	disagreeable	tastes	sensuous
tastes	computationally	flavor	fruity	smell	elegant
unpleasant	cheaper	refreshing	aroma	flavour	disagreeable
inexpensive	luxurious	something	fishy	aroma	elegance
smell	sweeter	things	pungent	sour	neoclassicism
costly	inexpensive	really	odor	ingredients	refinement
ingredients	afford	odor	becuase	qualities	perfected
Table 8: Top 10 words relating to various adjective-noun phrases
close friend		best friend		dear friend	
additive	tensor	additive	tensor	additive	tensor
close	confidante	best	confidante	friend	friend
friend	confidant	friend	confides	dear	confidante
friends	coworker	actor	misinterpreting	colleague	coworker
confidant	close	awards	coworker	lover	colleague
colleague	friend	actress	memoirists	friends	dear
closest	confided	award	protege	girlfriend	confidant
collaborator	schoolmates	nominated	presumes	beloved	dearest
confidante	classmate	friends	helpfully	boyfriend	protege
classmate	protege	girlfriend	matth	classmate	confided
brother	cuz	writer	regretfully	roommate	collaborator
retrieves). In the case of “awful taste", both compositions retrieve fairly similar words, which mostly
relate to the physical sense of taste (rather than the more abstract sense of the word). For the phrase
“refined taste", the additive composition fails to capture the sense of the phrase and retrieves many
words related to food taste (which are irrelevant in this context), whereas the tensor composition
retrieves more relevant words.
In Table 8, we fix the noun “friend" and vary the modifying adjective, but in all three cases, the
adjective-noun phrase has basically the same meaning. In the case of “close friend" and “dear
friend", both compositions retrieve fairly relevant and similar words. In the case of “best friend",
both compositions retrieve false positives: the additive composition seems to find words related to
movie awards, while the tensor composition finds unintuitive false positives. We note that in all
three phrases, the tensor composition consistently retrieves the words “confidante", “confided" or
“confides", “coworker", and “protoge", all of which are fairly relevant.
A. 1 Sentiment analysis
We test the effect of using the composition tensor for a sentiment analysis task. We use the movie
review dataset of Pang and Lee (Pang & Lee, 2004) as well as the Large Movie Review dataset
(Maas et al., 2011), which consist of 2,000 movie reviews and 50,000 movie reviews, respectively.
For a fixed review, We identify each adjective-noun pair (a, b) and compute T(va, vb, ∙). We add
these compositions together with the word embeddings for all of the words in the review, and then
normalize the resulting sum. This vector is used as the input to a regularized logistic regression
classifier, which we train using scikit-learn (Pedregosa et al., 2011) with the default parameters. We
also consider a baseline method where we simply add together all of the word embeddings in the
movie review, and then normalize the sum. We evaluate the test accuracy of each method using
15
Published as a conference paper at ICLR 2019
Table 9: Test accuracy for sentiment analysis task (standard deviation reported in parentheses)
Dataset	Additive	Tensor
PangandLee	0.741 (0.018)^^0.759 (0.025)
Large Movie Review	0.793	0.794
5-fold cross-validation on the smaller dataset and the training-test set split provided in the larger
dataset. Results are shown in Table 9. Although the tensor method seems to have a slight edge over
the baseline, the differences are not significant.
B	Omitted proofs for Section 3
In this section we will prove the main Theorem 1, which establishes the connection between the
model parameters and the correlations of pairs/triples of words. As we explained in Section 3, a
crucial step is to analyze the partition function of the model and show that the partition functions are
concentrated. We will do that in Section B.1. We then prove the main theorem in Section B.2. More
details and some technical lemmas are deferred to Section B.3
B.1	Concentration of partition function
In this section we will prove concentrations of partition functions (Lemma 1). Recall that we need
the tensor to be K-bounded (where K is a constant) for this to work.
Definition 3. (Definition 2 restated) The composition tensor T is (K, )-bounded, if for any word
embedding va , vb, we have
Kd2
kT (va,∙,∙) + Ik2 ≤ E ； kT (va, ∙, ∙) + IkF ≤ Kd; kT (va, Vb, ∙)k2 ≤ Kd∙
Note that K here should be considered as an absolute constant (like 5, in fact in Section 5 we show
K is less than 4). We first restate Lemma 1 here:
Lemma 3 (Lemma 1 restated). For the syntactic RAND-WALK model, there exists a constant Z such
that
Pr [(1 - z)Z ≤ Zc ≤ (1 + z)Z] ≥ 1 - δ,
C〜C
for Ez = O(1 /√n) and δ = exp(-Ω(log2 n)).
Furthermore, if the tensor T is (K, )-bounded, then for any fixed word a ∈ V, there exists a constant
Za such that
Pr [(1 - Ez,a)Za ≤ Zc,a ≤ (1 + Ez,a)Za] ≥ 1 - δ,
for Ez,a = O(E) + (O(1∕√n) and δ = exp(-Ω(log2 n)).
In fact, the first part of this Lemma is exactly Lemma 2.1 in Arora et al. (2015). Therefore we will
focus on the proof of the second part.
For the second part, we know the probability of choosing a word bis proportional to exp(T(va, vb, c)+
hc, Vbi) = exp(hT(Va, ∙, C) + c, Vbi).
If the probability of choosing word w is proportional to exp(hr, vwi) for some vector r (think of
r = T(va, ∙,c)+c),then in expectation the partition function should be equal to nEv〜DV [exp(hr, v〉)]
(here DV is the distribution of word embedding). When the number of words is large enough, we
hope that with high probability the partition function is close to its expectation. Since the Gaussian
distribution is spherical, We also know that the expected partition function nEv〜DV [exp( hr, v))]
should only depend on the norm of r. Therefore as long as we can prove the norm of r = T(va, ∙, c)+c
remain similar for most c, we will be able to prove the desired result in the lemma.
We will first show the norm of r = T(va, ∙, c) + C is concentrated if the tensor T is (K, e)-bounded.
Throughout all subsequent proofs, we assume that E < 1 and d ≥ log2 n/E2 .
16
Published as a conference paper at ICLR 2019
Lemma 4. Let va be a fixed word vector, and let c be a random discourse vector. If T is (K, )-
bounded with d ≥ log2 n/2, we have
Pr[kT(va, ∙,c) + ck2∈ L ± O(e)] ≥ 1 - δ,
where 0 ≤ L ≤ K is a constant that depends on Va, and δ = exp(-Ω(log2 n)).
Proof. Since c is a uniform random vector on the unit sphere, we can represent c as c = z/kzk, where
Z 〜N(0, I) is a standard spherical Gaussian vector. For ease of notation, let M = T(va, ∙, ∙) +1, and
write the singular value decomposition of M as M = UΣVT. Note that Σ = diag(λ1, . . . , λd) and
U and V are orthogonal matrices, so that in particular, the random variable y = V T z has the same
distribution as z, i.e. its entries are i.i.d. standard normal random variables. Further, kU xk2 = kxk2
for any vector x, since U is orthogonal. Hence, we have
kT(V …)+ck2=⅛ kMzk2=⅛ kU 叼2=¾f
Since both the numerator and denominator of this quantity are generalized χ2 random variables,
we can apply Lemma 7 to get tail bounds on both. Observe that by assumption, we have λi2 ≤
Kd2/ log2 n for all i, and Pid=1 λi2 ≤ Kd. SetA = Pid=1 λi2yi2 andB = Pid=1 zi2. Let λ2max =
max λi2 . Note that E[A] = Pid 1 λi2 ≤ Kd and E[B] = d.
1≤i≤d i	i=1 i
We will apply Lemma 7 to prove concentration bounds for A, in this case we have
Pr
|A - E[A]| ≥2
≤ 2 exp(-x).
Under Our assumpti0ns, We know λ2naχ ≤ Kde2/ log2 n and ʌ/Pd=I λ ≤ λλmaχ Pd=I λ2 ≤
Kde/log n. Take X =	log2 n, we know 2 Jpd=1 λ4√ + 2λ2rιaxx] ≤ Kde. Therefore
Pr[∣A - E[A]∣ ≥ Kde] ≤ 2exp(-Ω(log2 n)).
Similarly, we can apply Lemma 7 to B (in fact we can apply simpler concentration bounds for
standard χ2 distribution), and we get
Pr[∣B - E[B]∣ ≥ 2√d√X + 2x] ≤ 2exp(-x).
If we take X =+ log2 n, we know 2√d√X + 2x ≤ ed. This implies
Pr[∣B - E[B]∣ ≥ de] ≤ 2exp(-Ω(log2 n)).
When both events happen we know | A - EE[A] | ≤ 4Ke = O(e) (here K is considered as a constant).
This finishes the proof.
□
Using this lemma, we will show that the expected condition number nEv〜DV [exp( hr, v))] (where
r = T(va, ∙, c) + c) is concentrated
Lemma 5. Let Va be a fixed word vector, and let c be a random discourse vector. If T is (K, e)-
bounded, there exists Za such that we have
Pr[nEv〜DV [exp(hT(va,∙,c) + c,v))] ∈ Za(1 ± O(e) ≥ 1 - δ,
where Za = Θ(n) depends on Va, and δ = exp(-Ω(log2 n)).
17
Published as a conference paper at ICLR 2019
Proof. We know V = S ∙ V where V ~ N(0, I) and S is a (random) scaling. Let r = T(va, ∙, C) + c.
Conditioned on s we know hr, vi is equivalent to a Gaussian random variable with standard deviation
σ = krkS. For this random variable we know
1
—^eXp
σ yj12π
eXp(x)dx
E[exp(hr,Vi)|S] =
x
= Zx
1	(x - σ2)2	2
σ√2∏ exp (—-2σ^+σ ∕2)dx
exp(σ2∕2).
Hence,
E[exp(hr, Vi)|S] = exp(S2krk2∕2).
Let g(x) = Es[exρ(s2x∕2)], we know g0(x) = Es[exp(s2x∕2) ∙ (s2∕2)] ≤ κ2∕2 ∙ g(x). Inparticular,
this implies g(x + γ) ≤ exp(κ2γ∕2)g(x) (for small γ).
By Lemma 4, We know with probability at least 1 - Ω(log2 n), ∣∣r∣∣2 ∈ L 士 O(e). Therefore, when
this holds, we have
nEv~Dv [exp(hr, v))] ∈ ng(L -O(E)) ∙ [1, exp(O(eκ2∕2)].
The multiplicative factor on the RHS is bounded by 1 + O() when is small enough (and κ is a
constant). This finishes the proof.	口
Now we know the expected partition function is concentrated (for almost all discourse vectors c), it
remains to show when we have finitely many words the partition function is concentrated around its
expectation. This was already proved in Arora et al. (2015), we use their lemma below:
Lemma 6. For any fixed vector r (whose norm is bounded by a constant), with probability at least
1 — exp(一Ω(log2 n)) over the choices ofthe words, we have
n
EeXp(hr,Vi)) ∈ nEv~Dv [exp((r, v))](1 土 金),
i=1
where Ez = O(1∕√n).
This is essentially Lemma 2.1 in Arora et al. (2015) (see Equation A.32). The version we stated is a
bit different because we allow r to have an arbitrary constant norm (while in their proof vector r is
the discourse vector c and has norm 1). This is a trivial corollary as we can move the norm of r into
the distribution of the scaling factor S for the word embedding.
Finally we are ready to prove Lemma 1.
Proof of Lemma 1. The first part is exactly Lemma 2.1 in Arora et al. (2015).
For the second part, note that the partition function Zc,a = Pn=IhT(v0, ∙, C) + c, Vi). We will use
E[Zc,a] to denote its expectation over the randomness of the word embedding {Vi }. By Lemma 5,
we know for at least 1 - exp(一Ω(log2 n)) fraction of discourse vectors c, the expected partition
function is concentrated (E[Zc,a] ∈ (1 ± O(E))Za). Let S denote the set of C such that Lemma 5
holds. Now by Lemma 6 we know for any X ∈ S, with probability at least 1 - exp(一Ω(log2 n)
Zc,a ∈ (1 ± Ez)E[Zc,a].
Therefore we know if we consider both c and the embedding as random variables, Pr[Zc,a ∈
(1 ± O(E + Ez))Za] ≥ 1 - δ0 where δ0 = exp(-Ω(log2 n)). Let S be the set of word embedding
such that there is at least √77 fraction of C that does not satisfy Zc,a ∈ (1 土 O(E + Ez))Za, We must
have Pr[S] ∙ √δ0 ≤ δ0. Therefore
Pr[S] ≤ √δ0.
That is, with probability at least 1 - √δ7 (over the word embeddings), there is at least 1 - √J7 fraction
of c such that Zc,a ∈ (1 ± O(E + Ez))Za.
□
18
Published as a conference paper at ICLR 2019
B.2	Estimating the correlations
In this section we prove Theorem 1 and Corollary 1. The proof is very similar to the proof of Theorem
2.2 in Arora et al. (2015). We use several lemmas in that proof, and these lemmas are deferred to
Section B.3.
Proof of Theorem 1. Throughout this proof we consider two adjacent discourse vectors c, c0, where c
generated a single word w and c0 generated a syntactic pair (a, b).
The first two results in Theorem 1 are exactly the same as Theorem 2.2 in Arora et al. (2015).
Therefore we only need to prove the result for p([a, b]) and p(w, [a, b]).
For p([a, b]), by definition of the model we know
p([a, b]) = Ec0[ɪʒ^^ exp(hc0, Vai +(。0, Vbi + T(va,vb,c0)]∙
Zc0 Zc0,a
Here Zc0 is the partition function Pin=1 exp(hc0, Vii), and Zc0,a is the partition function
Pin=1 exp(hc0, Vii + T(Va , Vi, c0).
Let F be the event that C satisfies the equations in Lemma 1. Let F be its negation. By Lemma 1 We
know Pr[F] ≥ 1 - exp(-Ω(log2 n)). Using this event, we can write
p([a, b]) =Ec0 h^v^exp(hc0,Vai + hc0, vbi + T(va, Vb, c0))1f]
Zc0 Zc0,a
+ Ec0 /	exp(hc0, Vai + hc0,Vbi + T(Va, Vb,c0))lF].
Zc0 Zc0 ,a
The second term can be bounded by Lemma 8 and the fact that Zc0Zc0,a ≥ β from Lemma 2. We
know
Ec0 [ɪʒ^^ exp(hc0, Vai + hc0,Vbi + T(va,Vb, c0)1f] ≤ exp(-Ω(log1.8 n)).
Zc0 Zc0,a
For the first term, we know by Lemma 1 that there exists Z, Za that are close to Zc0 and Zc0,a .
Therefore
p([a,b]) = Ec0[3^^ exp(hc0, Vai + hc0, Vbi + T(v°, Vb, c0)1f]
Zc0 Zc0,a
+ Ec0[ɪ^^ exp(hc0, Vai + hc0,Vbi + T(Va,Vb,。0)1声].
Zc0 Zc0,a
≤ (1 + Ez)(1 + ez,a)Ec0 [ZZ~ exp(hc0, Vai + hc0, Vbi + T(Va, Vb, CO)1F] + exp(—C(log1.8 n))
≤ (1 + 'z)(1 + ",a) Ec0 [exp(hc0, Vai + hc0, Vbi + T(Va, Vb, c0)] +exp(一Ω(log1∙8 n))
ZZa
≤ (1 + Ez)(1 + ”)(1 + °(1∕d)) exp( kVa + Vb + T(Va,Vb,.)k ) + eχp(-Ω(log1.8n)).
ZZa	2d
Here the last step used Lemma 10. Since both Z and Za can be bounded by O(n), and
kva+vb+Tdva,vb,∙)k is bounded by (4κ + √2K)2, we know the first term is of order Ω(1∕n2),
and the second term is negligible.
19
Published as a conference paper at ICLR 2019
For the lowerbound, we can have
p([a,b]) = Ec0 [3v^exp(hc0,Vai + hc', Vbi + T(va,Vb,c0)lF]
Zc0 Zc0,a
+ Ec0[ɪʒ^^ exp(hc0, Vai + hc0,Vbi + T(Va,Vb,。0)1声].
Zc0 Zc0,a
≥ (1 - Ez)(1 - ^z,a)Ec0 [~^~^~ exp(hc0, Vai + hc, Vbi + T(Va, vb, CO)1F]
Z Za
≥ (I-Cz)(I — ez,a) {Ec0 [exp(hc0, Vai + hc', Vbi + T(Va, Vb, C0))]
ZZa
-Ec0 [exp( hc', Vai + hc', Vbi + T(Va,Vb, c'))1f]}
≥	—Cz)(ICza) {Ec0[exp(hc', Vai + hc', Vbi + T(Va,Vb,c'))] - exp(-Ω(log1.8 n))}
ZZa
≥	(I-Cz)(1-Cz,a)(I- O(1∕d)) [exp( kVa + Vb + T(Va,Vb, ∙)k2 ) - exp(-Ω(log1.8n))∣ .
ZZa	2d
Again the last step is using Lemma 10 and the term exp(-Ω(log1.8 n) is negligible. Combining the
upper and lower bound, we know
1	" Q	||Va + Vb + T(Va,Vb, •州2	]	7	]	7 _l
logp([a, b]) =----------2d--------------log Z - log Za ± Cp,
1	ʌʌ /	.	∖	、 K / r I 1、
where Cp = O(Cz + Cz,a) + O(1∕d).
Now we turn to the most complicated term log p(w, [a, b]). By definition we know
p(w, [a, b]) = Ec,c0[3exp(hc,Vwi)3^^exp(hc',Vai + hc', Vbi + T(v°, Vb,c')].
Zc	Zc0 Zc0 ,a
We will follow similar idea as before. Let F be the event that both c, c' satisfy the equa-
tions in Lemma 1 and F be its negation. By Lemma 1 and union bound we know Pr[F] ≥
1 — exp(-Ω(log2 n)).
We again separate the co-occurrence probability based on the event F:
p(w, [a, b]) =EcQ [；exp(〈c,Vwi)ɪʒɪ^ exp(hc', Vai + hc', vbi + T(Va,Vb,c')lF]
Zc	Zc0 Zc0 ,a
+ Ec,c0 h1 exP(hc, Vw i)
Zc	Zc0 Zc0,a
exp(hc', Vai + hc', Vbi + T(Va, Vb,c')1jF].
For the second term, We can again use Lemma 8 to show that it is bounded by exp(-Ω(log1∙8 n)).
Now, using techniques similar as before, we can prove
p(w, [a, b]) = (1 ± O(Cz + Cz,a))ZZ-Ec,c0 [exp(<c, Vwi)exp((c', v°i + hc', Vb〉+ T(va, Vb, c'))].
a	(9)
Now the final step is to use the fact that c and c' are close to simplify the final formula. Let
A(c') = Ec|c0 [exp(hc, Vwi)], by Lemma 9 we know A(c') ∈ (1 ± Cw) exp(hVw, c'i). Therefore
Ec,c0[exp(hc,Vwi)exp(hc',Vai + hc', Vbi + T(Va,Vb, c'))]
=Ec0[exp(hc',Vai + hc', Vbi + T(Va,Vb, c'))Ec|c0[exp(hc,Vwi)]]
=Ec0[exp(hc',Vai + hc', Vbi + T(Va,Vb, c'))A(c')]
=(1 ± Cw)Ec0 [exp(hc', Vai + hc', Vbi + T(Va, Vb, c') + hc', Vwi)]
/1 I Wl I An /加、	/ kvw + va + vb + T(va,vb, ∙)∣2∖
=(1 ± Cw)(1 ± O(1∕d)) exp(---------------2d--------------).
Here the last step is again by Lemma 10. Combining this with Equation equation 9 gives the
result.	□
20
Published as a conference paper at ICLR 2019
Finally we prove Corollary 1, which is just a simple calculation based on Theorem 1:
Proof of Corollary 1. By the definition of PMI3, we know
PMI3 = log p(w, [a, b]) + log p(a) + log p(b) + log p(w) - log p(w, a) - log p(w, b) - log p([a, b]).
=(kvw + Va + Vb + T(Va,vb' .)『-2logZ - logZ) + (⅛ + ⅛ + ⅛ - 3logZ)
2d	g g a	2d	2d	2d	g
-(⅛v0^ + k^∙⅛‹ - 4log Z)-( kva + Vb + T(va,vb, ∙)k2 - log Z - log Za) ± 7e
2d	2d	2d
T(Va ,vb,vw ) , 7
=± It.
d
□
B.3 Auxiliary lemmas
Tail bound for χ2 distribution We will use the following tail bounds for the generalized χ2-
squared distribution.
Lemma 7. Laurent & Massart (2000) Let y1, . . . , yd be i.i.d. standard normal random variables,
and let a1, . . . , ad be nonnegative real numbers. Set Y = Pid=1 aiyi2 and a = (a1, a2, . . . , ad). Then
the following hold for any positive real number x:
P(Y - E[Y] ≥ 2∣∣ak2√x + 2∣∣ak∞x) ≤ exp(-x)
P (Y - E [Y ] ≤ -2∣∣ak2√X) ≤ exp(-x).
Additional Lemmas We will use several tools developed in Arora et al. (2015). The first lemma
allows us to bound the probabilities the discourse vector c does not satisfy the results of Lemma 1.
Lemma 8. Let F be any event that depends on the discourse vector C with probability at least
1 — exp(一Ω(log2 n)), and F be its negation. Suppose r is a vector of norm O( √d), then
Ec[exp(hr, ci)1产]≤ exp(-Ω(log1∙8 n)).
Further, if we consider two consecutive discourse vectors c, c0, redefine F to be an event that can
depend on both discourse vectors, again with probability at least 1 — exp(一Ω(log2 n)). If r, r0 are
two vectors ofnorm O(√d) we have
Ece [exp(hr, ci)exp(hr0,。0〉)1声]≤ exp(-Ω(log1∙8 n)).
Proof. The proof of this lemma appears on page 20 in Arora et al. (2015), as a step in the proof of
their Theorem 2.2. For completeness, we reproduce (and slightly adapt) their argument here.
Observe that
Ec[exp(hr,。)1声]=Ec[exp(hr,。)1(『©>01F] + Ec[exp(hr, ci)1(r©<o1F].
The second term of B.3 is upper bounded by
Ec[1f] ≤ exp(-Ω(log2 n)).
Note that the first term of B.3 can be bounded as follows:
Ec[exp(hr, ci)1
hr,ci>01f] ≤ Ec[exp(har,ci)1hr,ci>01f] ≤ Ec[exp(hαr, 0)1f]
for α > 1. Therefore, to obtain a bound on Ec[exp(〈r, c))1〈『© >01F] is suffices to bound
吼[exp(hr,。)1F]
when ∣∣rk = Ω(√d).
Let Z denote the random variable hr, c), and let r(z) = 1jf. Using Lemma A.4 in Arora et al. (2015),
we have
Ec [exp(z)r(z)] ≤ Ec[exp(z)1[t,∞] (z)],
21
Published as a conference paper at ICLR 2019
where t satisfies that Ec[1[t,∞](z)] = Pr[z ≥ t] = EJr(z)] ≤ exp(-Ω(log2 n)). Then by Lemma
A.1 of Arora et al. (2015), We have that t ≥ Ω(log.9 n). Finally, applying Corollary A.3 of Arora
et al. (2015), we have
Ec[exp(z)r(z)] ≤ EJexp(z)1[t,∞] (z)] = exp(-Ω(log1∙8 n)),
which completes the proof for the first part of this lemma.
The second part of this lemma can be proved in much the same fashion. By Cauchy-Schwarz,
(Ec,c0 [exp(hr,ci)exp(hr0,c0i)lF])2 ≤ (E。。[exp(hr,。)21Α])(E。,。，[exp(hr0,，'〉)21Α])
≤ (Ec[exp({2r, c〉)Ec，|c[1f]]) (E。，[exp(<2r0,6烟水，[lʃ]]).
Now we bound Ec[exp(h2r,。)旧。，|。[1声]]using the same argument as above in the first part of this
proof, replacing 1/ with 旧。，|。[1F],r with 2r, and r(z) = 1/ with r(z)=旧。，缄[1声].In particular,
we have Ec[exp(h2r,。))旧。，|。[1产]]≤ exp(-Ω(log1∙8 n)). Likewise, we have the same bound for
E。，[exp(h2r0,。0))旧。]。，[1产]].Putting these two together, we conclude that
Ec,c0 [exp( hr, Ci) exp(hr0, c0i〃F] ≤ (E。[exp( Qτ, ci)E。，|。[1f]]) 1/2 (E。，[exp( h2r0, c0))E0∣。，[1F]])1/2
≤ exp(-Ω(log1.8n)),
as desired.
□
The next lemma allows us to handle the difference between two consecutive discourse vectors:
Lemma 9. Let c, C be two discourse vectors that are adjacent, let Vw be a word embedding satisfying
Ilvw k ≤ K0√d, and let A(c) := E。，∣。[exp(hvw,c0))], then we have
A(c) ∈ (1 ± w)exp(hvw,ci).
Proof. The proof of this lemma appears on page 21 in Arora et al. (2015), again as a step in the proof
of their Theorem 2.2. For completeness, we reproduce the argument here.
Since ∣∣vw ∣∣ ≤ K 0 √d for some constant K0, we have that hvw, c - c0) ≤ ∣∣vw ∣∣∣c - c0∣ ≤ K 0 √d∣c -
c0 I. Hence,
A(C)=旧。，|。政「((加，c0〉)]
=€乂「(〈加,。))旧。，|。心乂「(〈加,。0 - c))]
≤ £乂「((加，。))旧。，|。叮0√d∣c - c0k)]
≤ (1 + w) exp(hvw, ci),
where the last inequality follows from our model assumptions.
To get the lower bound, observe that
旧。，|。政「(长0√d∣c - c0k)] + 旧。，|。取「(一长0√d∣c - c0k)] ≥ 2.
Therefore, the model assumptions imply that
Ec，|。IeXP(-K 0√dkc - c0k)] ≥ 1 -Ew .
Hence,
A(c) = €乂「((加，。))旧。，|。伯乂「((加，c0 - c))]
≥ €乂「((加，。))旧。，|。政「(长0√d∣c - c0k)]
≥ (1 - Ew) exp(hvw, c)).
□
The next lemma we use gives bound on EIexp(hv, c))] where c is a uniform vector on the unit sphere.
22
Published as a conference paper at ICLR 2019
Lemma 10. [Lemma A.5 in Arora et al. (2015)] Let v ∈ Rd be a fixed vector with norm kvk
O(√d). For random variable C with uniform distribution over the sphere, we have that
kv k2
log E[exp(hv,ci)] = ^2d- ± 金，
where c = O(1/d).
We end with the proof of Lemma 2.
Proof of Lemma 2. Just for this proof, we use the following notation. Let Id×d be the d-dimensional
identity matrix, and let x1, x2, . . . , xn be i.i.d. draws from N(0, Id×d). Let yi = kxi k2, and note that
yi2 is a standard χ-squared random variable with d degrees of freedom. Let κ be a positive constant,
and let si, s2,..., Sn be i.i.d. draws from a distribution supported on [0, κ]. Let Vi = Si ∙ Xi. Define
Zc = Pin=1exp(hvi,ci), and define Zc,a = Pin=1exp(hvi, ci +T(va,vi, c)).
We first cover the unit sphere by a finite number of metric balls of small radius. Then we show that
with high probability, the partition function at the center of these balls is indeed bounded below by a
constant. Finally, we show that the partition function evaluated at an arbitrary point on the unit sphere
can’t be too far from the partition function at one of the ball centers provided the norms of the vi are
not too large. We finish by appropriately controlling the norms of the vi .
For E > d-i, cover the unit sphere in Rd with N = (∣ + 1)d balls of radius e. Let ci, c2,... ,cn
be the centers of these balls (so that each ci is a unit vector). Let α ≥ 0 be a constant. Note that
hvj ,Cii = hcj, Sj ∙ Cii and〈Vk, cii + T (vι,vk ,ci) = hχ ,Sk (I + T (vι, ∙, ∙))T Cii are Gaussian random
variables with mean 0.
Let Fi be the event that there exists some j, k ∈ [n] such thathVj,Cii ≥ 0 and(Vk +T (va,vk, ∙),Cii ≥
0 . Note that
PrFi] ≤ Pr[∀j ∈ [n], (vj,cii ≤ 0] + Pr[∀k ∈ [n], Ek + T(VaJvk, ∙), Cii ≤ 0]
nn
= Y Pr[hVj, Cii ≤ 0] + Y Pr[hVk + T(va,vk, ∙),Cii ≤ 0]
j=i	k
≤ LL
2n	2n
≤ exp(-Θ(n)).
Let Y > 0. Let Gi be the event that yi < γ√d. Set t =(圭 JY2 — 2 — 2)2d, sothat d +2√dt + 2t =
γ2d. Then by Lemma 7,
Pr[Gi] ≤ exp(-t).
Let E = TiN=i Fi Tin=i Gi . Assume that the word embeddings satisfy the event E . Let Ci be a center
of one of the covering balls such that kC - Ci k2 < E. Let Vj, Vk be vectors that satisfies hxj, Cii ≥ -α
and (vk + T(va, Vk, ∙), Cii ≥ -α. By Cauchy-Schwarz and the definition of E, we have
hVj, Ci = hVj,Cii + hVj,C - Cii
≥ -kVjllkC -Cik
≥ —eγκ√d
=-γκd-1∕2
≥ '
for some appropriate universal constant `. Likewise, using the boundedness property ofT, we have
(Vk + T(Va, Vk, ∙), Ci ≥ -e√K√d
=-√Kd-1/2
≥ `.
23
Published as a conference paper at ICLR 2019
Hence,
n
Zc = EeXp(hvi,ci) ≥ exp(hvj,ci) ≥ exp(-')
i=1
and
n
Zc,a = EeXp(hvi,ci + T(va,vi,c)) ≥ exp(hvk + T(v°,vk, ∙),ci) ≥ exp(-').
i=1
It remains to analyze the probability of E . By the union bound, we have
Pr[E] ≥ 1 — N exp I----- I — n exp(—t)
1
1
—exp(O(d log d) — Θ(n)) — exp(log n — (√= V Y2
— exp(Θ(d log d) — Θ(n)) — exp(Θ(log n) — Θ(d)).
-2)2d)
Note that this is a high probability if n d log d and d	log n.
24