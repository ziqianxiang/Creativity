Published as a conference paper at ICLR 2019
A Mean Field Theory of Batch Normalization
Greg Yang” Jeffrey Pennington。，Vinay Rao。, Jascha Sohl-DiCkstein。，& Samuel S. Schoenholz°
f Microsoft Research AL ◦ Google Brain
gregyang@microsoft.com, {jpennin,vinaysrao,jaschasd,schsam}@google.com
Ab stract
We develop a mean field theory for batch normalization in fully-connected feed-
forward neural networks. In so doing, we provide a precise characterization of
signal propagation and gradient backpropagation in wide batch-normalized net-
works at initialization. Our theory shows that gradient signals grow exponen-
tially in depth and that these exploding gradients cannot be eliminated by tuning
the initial weight variances or by adjusting the nonlinear activation function. In-
deed, batch normalization itself is the cause of gradient explosion. As a result,
vanilla batch-normalized networks without skip connections are not trainable at
large depths for common initialization schemes, a prediction that we verify with a
variety of empirical simulations. While gradient explosion cannot be eliminated,
it can be reduced by tuning the network close to the linear regime, which improves
the trainability of deep batch-normalized networks without residual connections.
Finally, we investigate the learning dynamics of batch-normalized networks and
observe that after a single step of optimization the networks achieve a relatively
stable equilibrium in which gradients have dramatically smaller dynamic range.
Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive
new identities that may be of independent interest.
1	Introduction
Deep neural networks have been enormously successful across a broad range of disciplines. These
successes are often driven by architectural innovations. For example, the combination of convolu-
tions (LeCun et al., 1990), residual connections (He et al., 2015), and batch normalization (Ioffe
& Szegedy, 2015) has allowed for the training of very deep networks and these components have
become essential parts of models in vision (Zoph et al.), language (Chen & Wu, 2017), and rein-
forcement learning (Silver et al., 2017). However, a fundamental problem that has accompanied this
rapid progress is a lack of theoretical clarity. An important consequence of this gap between theory
and experiment is that two important issues become conflated. In particular, it is generally unclear
whether novel neural network components improve generalization or whether they merely increase
the fraction of hyperparameter configurations where good generalization can be achieved. Resolving
this confusion has the promise of allowing researchers to more effectively and deliberately design
neural networks.
Recently, progress has been made (Poole et al., 2016; Schoenholz et al., 2016; Daniely et al., 2016;
Pennington et al., 2017; Hanin & Rolnick, 2018; Yang, 2019) in this direction by considering neural
networks at initialization, before any training has occurred. In this case, the parameters of the
network are random variables which induces a distribution of the activations of the network as
well as the gradients. Studying these distributions is equivalent to understanding the prior over
functions that these random neural networks compute. Picking hyperparameters that correspond
to well-conditioned priors ensures that the neural network will be trainable and this fact has been
extensively verified experimentally. However, to fulfill its promise of making neural network design
less of a black box, these techniques must be applied to neural network architectures that are used
in practice. Over the past year, this gap has closed significantly and theory for networks with skip
connections (Yang & Schoenholz, 2017; 2018), convolutional networks (Xiao et al., 2018), and gated
* Please see https://arxiv.org/abs/1902.08129 for the full and most current version of this
paper
1
Published as a conference paper at ICLR 2019
recurrent networks (Chen et al., 2018; Gilboa et al., 2019) have been developed. More recently,
Yang (2019) devised a formalism that extends this approach to include an even wider range of
architectures.
Before state-of-the-art models can be analyzed in this framework, a slowly-decreasing number of
architectural innovations must be studied. One particularly important component that has thus-far
remained elusive is batch normalization.
Our Contributions. In this paper, we develop a theory of fully-connected networks with batch
normalization whose weights and biases are randomly distributed. A significant complication in the
case of batch normalization (compared to e.g. layer normalization or weight normalization) is that
the statistics of the network depend non-locally on the entire batch. Thus, our first main result is to
recast the theory for random fully-connected networks so that it can be applied to batches of data.
We then extend the theory to include batch normalization explicitly and validate this theory against
Monte-Carlo simulations. We show that as in previous cases we can leverage our theory to predict
valid hyperparameter configurations.
In the process of our investigation, we identify a number of previously unknown properties of batch
normalization that make training unstable. In particular, for most nonlinearities used in practice,
batchnorm in a deep, randomly initialized network induces high degree of symmetry in the embed-
dings of the samples in the batch (Thm 3.4). Whenever this symmetry takes hold, we show that for
any choice of nonlinearity, gradients of fully-connected networks with batch normalization explode
exponentially in the depth of the network (Thm 3.9). This imposes strong limits on the maximum
trainable depth of batch normalized networks. This limit can be lifted partially but not completely
by pushing activation functions to be more linear at initialization. It might seem that such gradient
explosion ought to lead to learning dynamics that are unfavorable. However, we show that networks
with batch normalization causes the scale of the gradients to naturally equilibrate after a single step
of gradient descent (provided the initial gradients are not so large as to cause numerical instabilities).
For shallower networks, this equilibrating effect is sufficient to allow adequate training.
Finally, we note that there is a related vein of research that has emerged that leverages the prior
over functions induced by random networks to perform exact Bayesian inference (Lee et al., 2017;
de G. Matthews et al., 2018; Novak et al., 2019; Garriga-Alonso et al., 2019; Yang, 2019). One of
the natural consequences of this work is that the prior for networks with batch normalization can be
computed exactly in the wide network limit. As such, it is now possible to perform exact Bayesian
inference in the case of wide neural networks with batch normalization.
2	Related Work
Batch normalization has rapidly become an essential part of the deep learning toolkit. Since then, a
number of similar modifications have been proposed including layer normalization (Ba et al., 2016)
and weight normalization (Salimans & Kingma, 2016). Comparisons of performance between these
different schemes have been challenging and inconclusive (Gitman & Ginsburg, 2017). The original
introduction of batchnorm in Ioffe & Szegedy (2015) proposed that batchnorm prevents “internal
covariate shift” as an explanation for its effectiveness. Since then, several papers have approached
batchnorm from a theoretical angle, especially following Ali Rahimi’s catalyzing call to action at
NIPS 2017. Balduzzi et al. (2017) found that batchnorm in resnets allow deep gradient signal prop-
agation in contrast to the case without batchnorm. Santurkar et al. (2018) found that batchnorm
does not help covariate shift but helps by smoothing loss landscape. Bjorck et al. (2018) reached
the opposite conclusion as our paper for residual networks with batchnorm, that batchnorm works in
this setting because it induces beneficial gradient dynamics and thus allows a much bigger learning
rate. Luo et al. (2018) explores similar ideas that batchnorm allows large learning rates and likewise
uses random matrix theory to support their claims. Kohler et al. (2018) identified situations in which
batchnorm can provably induce acceleration in training. Of the above that mathematically analyze
batchnorm, all but Santurkar et al. (2018) make simplifying assumptions on the form of batchnorm
and typically do not have gradients flowing through the batch variance. Even Santurkar et al. (2018)
only analyzes a vanilla network which gets added a single batchnorm at a single moment in train-
ing. Our analysis here on the other hand works for networks with arbitrarily many batchnorm layers
with very general activation functions, and indeed, this deep stacking of batchnorm is precisely what
2
Published as a conference paper at ICLR 2019
leads to gradient explosion. It is an initialization time analysis for networks of infinite width, but we
show experimentally that the resulting insights predict both training and test time behavior.
We remark that Philipp & Carbonell (2018) has also empirically noted gradient explosion happens in
deep batchnorm networks with various nonlinearities. In this work, in contrast, we develop a precise
theoretical characterization of gradient statistics and, as a result, we are able to make significantly
stronger conclusions.
3	Theory
We begin with a brief recapitulation of mean field theory in the fully-connected setting. In ad-
dition to recounting earlier results, we rephrase the formalism developed previously to compute
statistics of neural networks over a batch of data. Later, we will extend the theory to include batch
normalization. We consider a fully-connected network of depth L whose layers have width Nl, ac-
tivation function1 φ, weights Wl ∈ RNl-1 ×Nl, and biases bl ∈ RNl. Given a batch of B inputs2
{xi : Xi ∈ RN0 }i=ι,…,b, the pre-activations of the network are defined by the recurrence relation,
hi1 = W1xi +b1 and	hli = Wlφ(hli-1) +bl ∀l > 1.	(1)
At initialization, We choose the weights and biases to be i.i.d. as Wae 〜 N(0,σW/N1-1) and
bla 〜N(0, σ2). In the following, we use α,β,... for the neuron index, and i, j,... for the batch
index. We will be concerned with understanding the statistics of the pre-activations and the gradients
induced by the randomness in the weights and biases. For ease of exposition we will typically take
the network to have constant width Nl = N.
In the mean field approximation, we iteratively replace the pre-activations in Eq. (2) by Gaussian
random variables with matching first and second moments. In the infinite width limit this ap-
proximation becomes exact (Lee et al., 2017; de G. Matthews et al., 2018; Yang, 2019). Since
the weights are i.i.d. with zero mean it follows that the mean of each pre-activation is zero and
the covariance between distinct neurons are zero. The pre-activation statistics are therefore given
by (hla11,…,hlaeB) —-—---------→ N(0, ∑lδαι∙∙∙ɑB) where Σl are B X B covariance matrices and
δαι, aB is the Kronecker-δ that is one if αι = α2 = ,一=αB and zero otherwise.
Definition 3.1. Let V be the operator on functions φ such that Vφ(∑) = E[φ(h)φ(h)T : h 〜
N(0, Σ)] computes the matrix of uncentered second moments of φ(h) for h 〜N(0, Σ).
Using the above notation, we can express the covariance matrices by the recurrence relation,
Σl = σw2 Vφ(Σl-1) +σb211T	(2)
At first Eq. (2) may seem challenging since the expectation involves a Gaussian integral in RB .
However, each term in the expectation of Vφ involves at most a pair of pre-activations and so the
expectation may be reduced to the evaluation of O(B2 ) two-dimensional integrals. These integrals
can either be performed analytically (Cho & Saul, 2009; Williams, 1997) or efficiently approximated
numerically (Lee et al., 2017), and so Eq. (2) defines a computationally efficient method for com-
puting the statistics of neural networks after random initialization. This theme of dimensionality
reduction will play a prominent role in the forthcoming discussion on batch normalization.
Eq. (2) defines a dynamical system over the space of covariance matrices. Studying the statistics
of random feed-forward networks therefore amounts to investigating this dynamical system and is
an enormous simplification compared with studying the pre-activations of the network directly. As
is common in the dynamical systems literature, a significant amount of insight can be gained by
investigating the behavior of Eq. (2) in the vicinity of its fixed points. For most common activation
functions, Eq. (2) has a fixed point at some Σ*. Moreover, when the inputs are non-degenerate, this
fixed point generally has a simple structure with Σ* = q* [(1 - c*)I + c*11T] owing to permutation
symmetry among elements of the batch. We refer to fixed points with such symmetry as BSB1
(Batch Symmetry Breaking 1 or 1 Block Symmetry Breaking) fixed points. As we will discuss later,
in the context of batch normalization other fixed points with fewer symmetries (“BSBk fixed points”)
1The activation function may be layer dependent, but for ease of exposition we assume that it is not.
2Throughout the text, we assume that all elements of the batch are unique.
3
Published as a conference paper at ICLR 2019
may become preferred. In the fully-connected setting fixed points may efficiently be computed by
solving the fixed point equation induced by Eq. (2) in the special case B = 2. The structure of
this fixed point implies that in asymptotically deep feed-forward neural networks all inputs yield
pre-activations of identical norm with identical angle between them. Neural networks that are deep
enough so that their pre-activation statistics lie in this regime have been shown to be untrainable
(Schoenholz et al., 2016).
Notation As we often talk about matrices and also linear operators over matrices, we write T{Σ}
for an operator T applied to a matrix Σ, and matrix multiplication is still written as juxtaposition.
Composition of matrix operators are denoted with T1 ◦ T2 .
Local Convergence to BSB1 Fixed Point. To understand the behavior of Eq. (2) near its BSB1
fixed point We can consider the Taylor series in the deviation from the fixed point, ∆Σl = Σl 一 Σ*.
To lowest order we generically find,
∆Σl = J{∆Σl-1}	(3)
where J = d∑Φ ∣∑=∑* is the B2 X B2 Jacobian of Vφ. In most prior work where φ was a pointwise
non-linearity, Eq. (3) reduces to the case B = 2 Which naturally gave rise to linearized dynamics in
ql = E[(hli)2] and cl = E[hlizjl]/ql. However, in the case of batch normalization we will see that one
must consider the evolution of Eq. (3) as a whole. This is qualitatively reminiscent of the case of
convolutional networks studied in Xiao et al. (2018) where the evolution of the entire pixel × pixel
covariance matrix had to be evaluated.
The dynamics induced by Eq. (3) will be controlled by the eigenvalues of J: Suppose J has eigen-
values λ% — ordered such that λι ≥ λ? ≥ ∙ ∙ ∙ ≥ 入62 — with associated eigen“vectors" e% (note
that the ei will themselves be B × B matrices). It follows that if ∆Σ0 = Pi ciei for some choice of
constants ci then ∆Σl = Pi ciλliei. Thus, if λi < 1 for all i, ∆Σl will approach zero exponentially
and the fixed-point will be stable. The number of layers over which Σ will approach Σ* will be
given by 一1/ log(λ1). By contrast if λi > 1 for any i then the fixed point will be unstable. In this
case, there is typically a different, stable, fixed point that must be identified. It follows that if the
eigenvalues of J can be computed then the dynamics will follow immediately.
While J may appear to be a complicated object at first, a moment’s thought shows that each di-
agonal element J {∆Σ}ii is only affected by the corresponding diagonal element ∆Σii, and each
off-diagonal element J {∆Σ}ij is only affected by ∆Σii , ∆Σjj , and ∆Σij . Such a Diagonal-Off-
diagonal Semidirect (DOS) operator has a simple eigendecomposition with two eigenspaces corre-
sponding to changes in the off-diagonal and changes in the diagonal (Thm E.72). The associated
eigenvalues are precisely those calculated by Schoenholz et al. (2016) in a simplified analysis. DOS
operators are a particularly simple form of a more general operator possessing an abundance of
symmetries called ultrasymmetric operators, which will play a prominent role in the analysis of
batchnorm below.
Gradient Dynamics. Similar arguments allow us to develop a theory for the statistics of gradients.
The backpropogation algorithm gives an efficient method of propagating gradients from the end of
the network to the earlier layers as,
奈=X δiφ(hi-1)T	δα, = φ'(hlɑi) X W+ 嘴 1.	(4)
iβ
Here L is the loss function and δi = Nh L are Ni -dimensional vectors that describe the error
signal from neurons in the l’th layer due to the i’th element of the batch. The preceding discussion
gave a precise characterization of the statistics of the hli that we can leverage to understand the
statistics of δil. Assuming gradient independence, that is, that an iid set of weights are used during
backpropagation (see Appendix B for more discussions), it is easy to see that E[δαl i] = 0 and
E[δαl iδβl j] = Πlij if α = β and 0 otherwise, where Πl is a covariance matrix and we may once again
drop the neuron index. We can construct a recurrence relation to compute Πl,
Πl = σw2Vφ0(Σl)	Πl+1.	(5)
4
Published as a conference paper at ICLR 2019
Typically, we will be interested in understanding the dynamics of Πl when Σl has converged expo-
nentially towards its fixed point. Thus, we study the approximation,
Πl ≈ σWVφ,(Σ*) Θ Πl+1.	⑹
Since these dynamics are linear (and in fact componentwise), explosion and vanishing of gradients
will be controlled by Vφo (∑*).
3.1	Batch Normalization
We now extend the mean field formalism to include batch normalization. Here, the definition for the
neural network is modified to be the coupled equations,
hi = Wl Φ(hi-1) + bl	hlɑi = Ya 也二上α + βα
σα
(7)
where Ya and βa are parameters, and μa
N Pi hai and σ22 = ∕Nl Pi(ha - μa) + e are
the per-neuron batch statistics. In practice e ≈ 10-5 or so to prevent division by zero, but in this
paper, unless stated otherwise (in the last few sections), e is assumed to be 0. Unlike in the case of
vanilla fully-connected networks, here the pre-activations are invariant to σw2 and σb2 . Without a loss
of generality, we therefore set σw2 = 1 and σb2 = 0 for the remainder of the text. In principal, batch
normalization additionally yields a pair of hyperparameters Y and β which are set to be constants.
However, these may be incorporated into the nonlinearity and so without a loss of generality we set
Y = 1 and β = 0. In order to avoid degenerate results, we assume B ≥ 4 unless stated otherwise;
we shall discuss the small B regime in Appendix J.
If one treats batchnorm as a “batchwise nonlinearity”, then the arguments from the previous section
can proceed identically and we conclude that as the width of the network grows, the pre-activations
will be jointly Gaussian with identically distributed neurons. Thus, we arrive at an analogous ex-
pression to Eq. (2),
Σl = VBφ(Σl-1)
where
Bφ : RB → RB , Bφ(h)
√BGh
l∣G‹
(8)
Here we have introduced the projection operator G = I - -g 11T which is defined SUCh that Gx =
X — μ1 with μ = Pi xi/B. Unlike φ, Bφ does not act component-wise on h. It is therefore not
obvious whether VBφ can be evaluated without performing a B-dimensional Gaussian integral.
Theoretical tools. In this paper, we present several ways to analyze high dimensional integrals
like the above: 1. the Laplace method 2. the Fourier method 3. spherical integration 4. and the
Gegenbauer method. The former two use the Laplace and Fourier transforms to simplify expressions
like the above, where the Laplace method requires that φ be positive homogeneous. Often the
Laplace method will give clean, closed form answers for such φ. Because batchnorm can be thought
of as a linear projection (G) followed by projection to the sphere of radius √B, spherical integration
techniques are often very useful and in fact is typically the most straightforward way of numerically
evaluating quantities. Lastly, the Gegenbauer method expresses objects in terms of the Gegenbauer
coefficients of φ. Briefly, Gegenbauer polynomials {Cl(a) (x)}l∞=0 are orthogonal polynomials with
respect to the measure (1 一 χ2)a-2 on [-1,1]. They are intimately related to spherical harmonics
(a natural basis for functions on a sphere), which explains their appearance in this context. The
Gegenbauer method is the most illuminating amongst them all, and is what allows us to conclude that
gradient explosion happens regardless of nonlinearity under general conditions. See Appendix D for
a more in-depth discussion of these techniques. In what follows, we will mostly present results by
the Laplace method for φ = relu and by the Gegenbauer method for general φ, but give pointers to
the appendix for others.
Back to the topic of Eq. (8), we present a pair of results that expresses Eq. (8) in a more manageable
form. From previous work (Poole et al., 2016), Vφ can be expressed in terms of a two-dimensional
Gaussian integrals independent of B. When φ is degree-α positive homogeneous (e.g. rectified
linear activations) we can relate Vφ and VBφ by the Laplace transform. (see Thm E.5).
5
Published as a conference paper at ICLR 2019
Theorem 3.2.	Suppose φ : R → R is degree-α positive homogeneous. For any positive semi-definite
matrix Σ define the projection ΣG = GΣG. Then
VBφ(Σ)
Bα /∞ dssα-ι Vφ(∑G(I + 2sΣG)-1)
r(a)J0	S S	Pdet(I +2S∑G)
(9)
whenever the integral exists.
Using this parameterization, when Vφ has a closed form solution (like ReLU), VBφ involves only
a single integral. A similar expression can be derived for general φ by applying Fourier transform;
see Appendix E.2. Next we express Eq. (8) as a spherical integral (see Proposition E.34)
Theorem 3.3.	Let e ∈ Rb×b-1 have as columns a set of orthonormal basis vectors of {x ∈ RB :
Gx = x}. Then with SB-2 ⊆ RB-1 denoting the (B - 2)-dimensional sphere,
VB (Σ) = E
八V V〜sb-
φ(√Bev 产2
√ det T∑ Σe (VT (e T Σe)-1v) B-1
(10)
Together these theorems provide analytic recurrence relations for random neural networks with batch
normalization over a wide range of activation functions. By analogy to the fully-connected case we
would like to study the dynamical system over covariance matrices induced by these equations.
We begin by investigating the fixed point structure of Eq. (8). As in the case of feed-forward net-
works, permutation symmetry implies that there exist BSB1 fixed points Σ* = q*[(1-c* )I+C 11t].
We will see that this fixed point is in fact unique, and a clean expression of q* and C can be obtained
in terms of Gegenbauer basis (see Thm F.13).
Theorem 3.4	(Gegenbauer expansions of BSB1 fixed point). If φ(√B - 1x) has Gegenbauer ex-
pansion P∞=o ai — 1ι c C( 2 )(x) where cβ-i,i = BBUi, then
∞
q* = Xai2
i=0
cB-1,i
B-3
F H),
q* c*
∞
Xai2
i=0
1
cB-1,l
1
B⅛)
Thus the entries of the BSB1 fixed point are diagonal quadratic forms of the Gegenbauer coefficients
of φ(√B - 1x). Even more concise closed forms are available when the activation functions are
degree α positive homogeneous (see Thm F.8). In particular, for ReLU we arrive at the following
Theorem 3.5	(BSB1 fixed point for ReLU). When φ = relu, then
q* = 2,	c* = J1 ( B-11) = ∏ - 2(B⅛ + O ((B⅛)	(11)
where Jι(c) = ∏(√Γ—c2 + (π - arccos(c))c) is the arccosine kernel (Cho & Saul, 2009).
In the appendix, Thm F.5 also describes a trigonometric integral formula for general nonlinearities.
In the presence of batch normalization, when the activation function grows quickly, a winner-take-
all phenomenon can occur where a subset of samples in the batch have much bigger activations
than others. This causes the covariance matrix to form blocks of differing magnitude, breaking the
BSB1 symmetry. One notices this, for example, as the degree α of α-ReLU (i.e. αth power of
ReLU) increases past a point αtransition(B) depending on the batch size B (see Fig. 1). However,
we observe, through simulations, that by far most of the nonlinearities used in practice, like ReLU,
leaky ReLU, tanh, sigmoid, etc, all lead to BSB1 fixed points, and we can prove this rigorously
for φ = id (see Corollary F.3). We discuss symmetry-broken fixed points (BSB2 fixed points) in
Appendices J and K, but in the main text, from here on,
Assumption 1. Unless stated otherwise, we assume that any nonlinearity φ mentioned induces Σi to
converge to a BSB1 fixed point under the dynamics of Eq. (8).
3.1.1 Linearized Dynamics
With the fixed point structure for batch normalized networks having been described, we now inves-
tigate the linearized dynamics of Eq. (8) in the vicinity of these fixed points.
6
Published as a conference paper at ICLR 2019
dVB
To determine the eigenvalues of -^∑φ ∣∑=∑* it is helpful to consider the action of batch normal-
ization in more detail. In particular, we notice that Bφ can be decomposed into the composition of
three separate operations, Bφ = φ ◦ n ◦ G. As discussed above, Gh subtracts the mean from h and
we introduce the new function n(h) = √Bh∕∣∣h∣∣ which normalizes a centered h by its standard
deviation. Applying the chain rule, we can rewrite the Jacobian as,
鹏(∑)
dΣ
dV[φ°n](∑G)
-dΣG~
◦ G02
(12)
where ◦ denotes composition and G02 is the natural extension of G to act on matrices as G02{Σ}=
GΣG = Σg. It ends UP being advantageous to study G02 ◦ dV∑n] I	◦ G02 =: G02 ◦ J ◦ G02
dΣ Σ=Σ*
and to note that the nonzero eigenvalues of this object are identical to the nonzero eigenvalues of the
Jacobian (see Lemma F.17).
At face value, this is a complicated object since it simultaneously has large dimension and possesses
an intricate block structure. However, the permutation symmetry of the BSB1 Σ* induces strong
symmetries in J that significantly simplify the analysis (see Appendix F.3). In particular while
ʌ ʌ ʌ
Jijkl is a four-index object, we have Jijkl = Jπ(i)π(j)π(k)π(l) for all permutations π on B and
Jijkl = Jjilk . We call linear operators possessing such symmetries ultrasymmetric (Defn E.53)
and show that all ultrasymmetric operators conjugated by G02 admit an eigendecomposition that
contains three distinct eigenspaces with associated eigenvalues (see Thm E.62).
Theorem 3.6.	Let T be an ultrasymmetric matrix operator. Then on the space of symmetric matri-
ces, G02 ◦ T ◦ G02 has the following orthogonal (under trace inner product) eigendecomposition,
1.	an eigenspace {Σ : ΣG = 0} with eigenvalue 0.
2.	a 1-dimensional eigenspace RG with eigenvalue λGG,T.
3.	a (B - 1)-dimensional eigenspace L := {DG : D diagonal, tr D = 0}, with eigenvalue
λG,T
λL .
4.	a B(B2-3)-dimensional eigenSpace M := {Σ : Σg = Σ, DiagΣ = 0}, with eigenvalue
G,T
λM .
The specific forms of the eigenvalues can be obtained as linear functions of the entries of T; see
Thm E.62 for details. Note that, as the eigenspaces are orthogonal, this implies that G02 ◦ T ◦ G02
is self-adjoint (even when T is not).
In our context with T = J, the eigenspaces can be roughly interpreted as follows: The deviation
∆Σ = Σ - Σ* from the fixed point decomposes as a linear combination of components in each of
the eigenspaces. The RG-component captures the average norm of elements of the batch (the trace
of ∆Σ), the L-component captures the fluctuation of such norms, and the M-component captures
the covariances between elements of the batch.
Because of the explicit normalization of batchnorm, one sees immediately that the RG-component
goes to 0 after 1 step. For positive homogeneous φ, we can use the Laplace method to obtain closed
form expressions for the other eigenvalues (see Thm F.33). The below theorem shows that, as the
batch size becomes larger, a deep ReLU-batchnorm network takes more layers to converge to a
BSB1 fixed point.
Theorem 3.7.	Let φ = relu and B > 3. The eigenVaIues of G02 ◦ J ◦ G02 for L and M are
λ↑
λL
λ↑M
1
2(B +1)μ*
B
2(B +1)μ*
-2) 1 - J1 (-~∖
B-1
+ ɪ J1 (U
B-1 B-1
JI (b⅛) % 2∏⅛ ≈ 0.733
%1
(13)
(14)
where μ* = q*(1 — c*) = 2 (1 — Ji ( B-II) ), and % denotes increasing limit as B → ∞.
7
Published as a conference paper at ICLR 2019
More generally, we can evaluate them for general nonlinearity using spherical integration (Ap-
pendix F.3.1) and, more enlightening, using the Gegenbauer method, is the following
( B — 3)
Theorem 3.8.	If φ( √B — 1x) has GegenbaUer expansion E∞=0 at — 1ι; C∣ 2 (x) ,then
λ↑ _ Σ∞=0 02WB-i,ι + αiαι+2UB-i,ι
λL =	P∞=0 α2vB-i,ι
∞2
λ↑ _ Tl=0 a WB-1,l + aιaι+2UB-i,ι
λM =	P∞=0 a2vB-i,ι
(15)
where the COeffiCientS wb-i,ι,ub-i,ι,W^b-i,ι,Ub-i,ι, vb-i,ι are given in ThmSF.22 and F.24.
A BSB1 fixed point is not locally attracting if λ↑L > 1 or λ↑M > 1. Thus Thm 3.8 yields insight on the
stability of the BSB1 fixed point, which we can interpret heuristically as follows. The specific forms
of the coefficients wb-i,ι,ub-i,ι, ^wb-i,ι,Ub-i,ι,vb-i,ι ShoW that λ∖ is typically much smaller
than λ↑L (but there are exceptions like φ = sin), and wB-1,1 < vB-1,1 but wB-1,ι ≥ vB-1,ι for
all l ≥ 2. Thus one expects that, the larger a1 is, i.e. the “more linear” and less explosive φ is, the
smaller λ↑L is and the more likely that Eq. (8) converges to a BSB1 fixed point. This is consistent
With the “Winner-take-all” intuition for the emergence of BSB2 fixed point explained above. See
Appendix F.3.2 for more discussion.
3.1.2 Gradient Backpropagation
With a mean field theory of the pre-activations of feed-forWard netWorks With batch normalization
having been developed, We turn our attention to the backpropagation of gradients. In contrast to the
case of netWorks Without batch normalization, We Will see that exploding gradients at initialization
are a severe problem here. To this end, one of the main results from this section Will be to shoW
that fully-connected netWorks With batch normalization feature exploding gradients for any choice
of nonlinearity such that Σι converges to a BSB1 fixed point. BeloW, by rate of gradient explosion
We mean the β such that the gradient norm squared groWs as βL+o(L) With depth L. As before, all
computations beloW assumes gradient independenCe (see Appendix B for a discussion).
As a starting point We seek an analog of Eq. (6) in the case of batch normalization. HoWever,
because the activation functions no longer act point-Wise on the pre-activations, the backpropagation
equation becomes,
δαιi=
βj
∂Bφ(hια)j
dhαi
Wβι+α1δβι+j1
(16)
Where hια = (hια1, . . . , hιαB) and We observe the additional sum over the batch. Computing the
resulting covariance matrix Πι, We arrive at the recurrence relation,
Πι = E [f dBφ(h)『Πι+1 dBφ(h) : h 〜N(0, Σι)
dh	dh
:VBφ (Σι )t{∏ι+1}
(17)
where we have defined the linear operator VF(Σ)t{∙} such that VF(Σ)t{Π} = E[FhT∏Fh : h 〜
N(0, Σ)] for any vector-indexed linear operator Fh . As in the case of vanilla feed-forWard netWorks,
here we will be concerned with the behavior of gradients when Σι is close to its fixed point. We
therefore study the asymptotic approximation to Eq. (17) given by Πι = Vbiφ(Σ*)t{∏ι+1}. In
this case the dynamics of Π are linear and are therefore naturally determined by the eigenvalues of
VBφ (∑*)t∙
As in the forward case, batch normalization is the composition of three operations Bφ = φ ◦ n ◦ G.
Applying the chain rule, Eq. (17) can be rewritten as,
「/	T ∖ l-Sl2
VB0(Σ)t = G02 ◦ E I ( d(φ ；n)(Z)	)	: h 〜N(0, ∑) =: Gl»2。F(Σ)
z=Gh
(18)
with F (Σ) appropriately defined. Note that since Gl2 is an idempotent operator, (VB0 (Σ)t)n =
(G02 ◦ F(Σ))n = (G02 ◦ F(Σ) ◦ G02)n-1 ◦ G02 ◦ F(Σ), so that it suffices to study the eigende-
composition of G02 ◦ F(Σ*) ◦ G02. Due to the symmetry of Σ*, F(Σ*) is ultrasymmetric, so that
8
Published as a conference paper at ICLR 2019
G02 ◦ F(Σ*) ◦ G02 has eigenspaces RG, L, M and We can compute its eigenvalues via Thm 3.6.
More illuminating, however, is the Gegenbauer expansion (see Thm G.5). It requires a new identity
Thm E.47 involving Gegenbauer polynomials integrated over a sphere, Which may be of independent
interest.
Theorem 3.9 (Batchnorm causes gradient explosion). Suppose φ(√B - 1χ),φ0(√B - 1 x) ∈
B _ 3	(B - 3
L2((1 — x2)-ɪ).If φ(√B — 1 x) has Gegenbauer expansion E∞=0 αιc~1- Cl 2 (x), then gra-
dients explode at the rate of
,P∞=0 (⅛-33 ∙ l) a2rι
∖÷.
G	P∞=0 a仙
where rι = ci3-1，(C( 2 ) (1) — C( 2 ) (b⅛)) > 0. Consequently, for any non-constant φ
(i.e. there is a j > 0 such that aj 6= 0), λ,G > 1; φ minimizes λ,G iff it is linear (i.e. ai = 0, ∀i ≥ 2),
in which case gradients explode at the rate of B-I.
This contrasts starkly With the case of non-normalized fully-connected netWorks, Which can use the
Weight and bias variances to control its mean field netWork dynamics (Poole et al., 2016; Schoenholz
et al., 2016). As a corollary, We disprove the conjecture of the original batchnorm paper (Ioffe &
Szegedy, 2015) that “Batch Normalization may lead the layer Jacobians to have singular values
close to 1” in the initialization setting, and in fact prove the exact opposite, that batchnorm forces
the layer Jacobian singular values away from 1.
Appendix G.1 discusses the numerical evaluation of all eigenvalues, and as usual, the Laplace
method yields closed forms for positive homogeneous φ (Thm G.12). We highlight the result for
ReLU.
Theorem 3.10. Ina ReLU-batchnorm network, the gradient norm explodes exponentially at the rate
of
1
B - 3
1	- JI (b⅛
(19)
which decreases to ∏-ɪ ≈ 1.467 as B → ∞. In contrast, for a linear batchnorm network, the
gradient norm explodes exponentially at the rate of B-I, which goes to 1 as B → ∞.
Fig. 1 shoWs theory and simulation for ReLU gradient dynamics.
Weight Gradient While all of the above only study the gradient With respect to the hidden preac-
tivations, Appendix L shows that the weight gradient norms at layer l is just h∏l, μ* G)= μ* tr Πl,
and thus by Thm 3.9, the Weight gradients explode as Well at the same rate λ,G.
Effect of as a hyperparameter In practice, is usually treated as small constant and is not
regarded as a hyperparameter to be tuned. Nevertheless, we can investigate its effect on gradient
explosion. A straightforward generalization of the analysis presented above to the case of > 0
suggests somewhat larger values than typically used can ameliorate (but not eliminate) gradient
explosion problems. See Fig. 5(c,d).
3.2 Cross-Batch Dynamics
Forward In addition to analyzing the correlation between preactivations of samples in a batch,
we also study the correlation between those of different batches. The dynamics Eq. (8) can be
generalized to simultaneous propagation of k batches (see Eq. (54) and Appendix H).
∑l =VBek (∑l-i)=	E ~	(Bφ(hι),..., Bφ(hk ))02, with Σl ∈ RkB×kB. (20)
φ	(hι,...,hk)〜N (0,Σl-1)
Here the domain of the dynamics is the space of block matrices, with diagonal blocks and off-
diagonal blocks resp. representing within-batch and cross-batch covariance. We observe empirically
9
Published as a conference paper at ICLR 2019
1',1',1'00b
s3e>u36
E」。U6。-
O O
20	40	60	80	100
layer
10
1
0.100
0.010
0.001
10-4
123456789 10
1 23456789 10
1 23456789 10
123456789 10
Layer 0
Figure 1: Numerical confirmation of theoretical predictions. (a,b) Comparison between theo-
retical prediction (dashed lines) and Monte Carlo simulations (solid lines) for the eigenvalues of
the backwards Jacobian (see Thms 3.10 and G.12) as a function of batch size and the magnitude
of gradients as a function of depth respectively for rectified linear networks. In each case Monte
Carlo simulations are averaged over 200 sample networks of width 1000 and shaded regions denote
1 standard deviation. Dashed lines are shifted slightly for easier comparison. (c,d) Demonstration
of the existence of a BSB1 to BSB2 symmetry breaking transition as a function of a for α-ReLU
(i.e. the ath power of ReLU) activations. In (c) we plot the empirical variance of the diagonal and
off-diagonal entries of the covariance matrix which clearly shows ajump at the transition. In (d) we
plot representative covariance matrices for the two phases (BSB1 bottom, BSB2 top).
.0,8.6/.2.0
Lo.o.o.o.o.
b
Figure 2: Batch norm leads to a chaotic input-output map With increasing depth. A linear
network with batch norm is shown acting on two minibatches of size 64 after random orthogonal
initialization. The datapoints in the minibatch are chosen to form a 2d circle in input space, except
for one datapoint that is perturbed separately in each minibatch (leftmost datapoint at input layer 0).
Because the network is linear, for a given minibatch it performs an affine transformation on its inputs
一 a circle in input space remains an ellipse throughout the network. However, due to batch norm
the coefficients of that affine transformation change nonlinearly as the datapoints in the minibatch
are changed. (a) Each pane shows a scatterplot of activations at a given layer for all datapoints
in the minibatch, projected onto the top two PCA directions. PCA directions are computed using
the concatenation of the two minibatches. Due to the batch norm nonlinearity, minibatches that
are nearly identical in input space grow increasingly dissimilar with depth. Intuitively, this chaotic
input-output map can be understood as the source of exploding gradients when batch norm is applied
to very deep networks, since very small changes in an input correspond to very large movements in
network outputs. (b) The correlation between the two minibatches, as a function of layer, for the
same network. Despite having a correlation near one at the input layer, the two minibatches rapidly
decorrelate with depth. See Appendix H for a theoretical treatment.
that for most nonlinearities used in practice like ReLU, or even for the identity function, ie no
pointwise nonlinearity, the global fixed point of this dynamics is cross-batch BSB1, with diagonal
BSB1 blocks, and off-diagonal entries all equal to the same constant cCb：
/ Σ*
-*
cCB
c*
cCB
c*
cCB
Σ*
c*
cCB
c*
cCB
Σ*
• ∙ ∙
...
...
(21)
∖
10
Published as a conference paper at ICLR 2019
To interpret this phenomenon, note that, after mean centering of the preactivations, this covariance
matrix becomes a multiple of identity. Thus, deep embedding of batches loses the mutual informa-
tion between them in the input space. Qualitatively, this implies that two batches that are similar at
the input to the network will become increasingly dissimilar — i.e. chaotic — as the signal prop-
agates deep into the network. This loss in fact happens exponentially fast, as illustrated in Fig. 2,
and as shown theoretically by Thm H.13, at a rate of λ↑Me < 1. Formally, the linearized dynamics of
Eq. (20) has an eigenspace M given by all block matrices with zero diagonal blocks. The associated
eigenvalue is λM. We shall return to λM shortly and give its GegenbaUer expansion (Thm 3.11).
Backward The gradients of two batches of input are correlated throughout the course of backprop-
agation. We hence also study the generalization of Eq. (17) to the k-batch setting (see Appendix I):
∏l = VB®20 (∑ l)t{∏l+1}
φ
VBe20 (Σ*)t
Bφ0 (x)TΠ1Bφ0 (x)
Bφ0 (y)TΞTBφ0 (x)
Bφ0 (x)T ΞBφ0 (y)
Bφ0 (y)T Π2Bφ0 (y)
(22)
(23)
where for simplicity of exposition and without loss of generality, we take k = 2. Like in Eq. (17), we
ll
have assumed that Σl has converged to its Ilmit Σ*. It is not hard to see that each block of Πl evolves
independently, with the diagonal blocks in particular evolving according to Eq. (17). Analyzing the
off-diagonal blocks might seem unwieldy at first, but it turns out that its dynamics is given simply
by scalar multiplication by a constant λM (Thm I.4). Even more surprisingly,
Theorem 3.11 (Batchnorm causes information loss). The convergence rate of cross-batch preac-
tivation Covariance λM is equal to the decay rate of the cross-batch gradient Covariance λM. If
(B - 3)
φ(λ∕B - 1 x) has Gegenbauer expansion £晨° aicB 1; C 2 ( (x), then both are given by
λ↑ = λ⅛∙=
MM
2∏B(B — 1)a2 Beta (BB, 1)
P=I^oC^ITC^lrGiU
(24)
This quantity is always < 1, and for any fixed B, is maximized by φ = id. Furthermore, for φ = id,
λ↑ = λ- = ⅛1 P (B,1) and increase to 1 from below as B → ∞.
Me	Me	2	2 , 2
Thus, a deep batchnorm network loses correlation information between two input batches, expo-
nentially fast in depth, no matter what nonlinearity (that induces fixed point of the form given by
Eq. (21)). This again contrasts with the case for vanilla networks which can control this rate of
information loss by tweaking the initialization variances for weights and biases Poole et al. (2016);
Schoenholz et al. (2016). The absence of coordinatewise nonlinearity, i.e. φ = id, maximally sup-
presses both this loss of information as well as the gradient explosion, and in the ideal, infinite batch
scenario, can cure both problems.
4	Experiments
Having developed a theory for neural networks with batch normalization at initialization, we now
explore the relationship between the properties of these random networks and their learning dynam-
ics. We will see that the trainability of networks with batch normalization is controlled by gradient
explosion. We quantify the depth scale over which gradients explode by ξ = 1/log λG where, as
above, λG is the largest eigenvalue of the jacobian. Across many different experiments We will see
strong agreement between ξ and the maximum trainable depth.
We first investigate the relationship between trainability and initialization for rectified linear net-
works as a function of batch size. The results of these experiments are shown in Fig. 3 where in
each case we plot the test accuracy after training as a function of the depth and the batch size and
overlay 16ξ in white dashed lines. In Fig. 3 (a) we consider networks trained using SGD on MNIST
where we observe that networks deeper than about 50 layers are untrainable regardless of batch size.
In (b) we compare standard batch normalization with a modified version in which the batch size is
11
Published as a conference paper at ICLR 2019
IO1
0.1	1.0	0.1	0.55
IO2
50 100 150 200 250	50 100 150 200 250
50 100 150 200 250
B	B
Figure 3: Batch normalization strongly limits the maximum trainable depth. Colors show test
accuracy for rectified linear networks with batch normalization and Y = 1, β = 0, e = 10-3,
N = 384, and η = 10-5B. (a) trained on MNIST for 10 epochs (b) trained with fixed batch
size 1000 and batch statistics computed over sub batches of size B. (c) trained using RMSProp.
(d) Trained on CIFAR10 for 50 epochs. In each case, White dashed line indicates a theoretical
prediction of trainable depth, as discussed in the text.
⅛7z∆ 一/⅛*>
20	40	60	80 100 120 140	20	40	60	80 100 120 140	20	40	60	80 100 120 140
LLL
Figure 4: Gradients in networks with batch normalization quickly achieve dynamical equilib-
rium. Plots of the relative magnitudes of (a) the weights (b) the gradients of the loss with respect
to the pre-activations and (c) the gradients of the loss with respect to the weights for rectified linear
networks of varying depths during the first 10 steps of training. Colors show step number from 0
(black) to 10 (green).
held fixed but batch statistics are computed over subsets of size B. This removes subtle gradient
fluctuation effects noted in Smith & Le (2018). In (c) we do the same experiment with RMSProp
and in (d) we train the networks on CIFAR10. In all cases we observe a nearly identical trainable
region.
Itis counter intuitive that training can occur at intermediate depths, from 10 to 50 layers, where there
is significant gradient explosion. To gain insight into the behavior of the network during learning
we record the magnitudes of the weights, the gradients with respect to the pre-activations, and the
gradients with respect to the weights for the first 10 steps of training for networks of different depths.
The result of this experiment is shown in Fig. 4. Here we see that before learning, as expected, the
norm of the weights is constant and independent of layer while the gradients feature exponential
explosion. However, we observe that two related phenomena occur after a single step of learning:
the weights grow exponentially in the depth and the magnitude of the gradients are stable up to
some threshold after which they vanish exponentially in the depth. This is as the result of the
scaling property of batchnorm, where Bφ0 (αh) = α-1Bφ0 (h): The first-step gradients dominate the
weights due to gradient explosion, hence the exponential growth in weight norms, and thereafter,
the gradients are scaled down commensurately. Thus, it seems that although the gradients of batch
normalized networks at initialization are ill-conditioned, the gradients appear to quickly reach a
stable dynamical equilibrium. While this appears to be beneficial for shallower networks, in deeper
ones, the relative gradient vanishing can in fact be so severe as to cause lower layers to mostly stay
constant during training. Aside from numerical issues, this seems to be the primary mechanism
through which gradient explosion causes training problems for networks deeper than 50 layers.
As discussed in the theoretical exposition above, batch normalization necessarily features exploding
gradients for any nonlinearity that converges to a BSB1 fixed point. We performed a number of
experiments exploring different ways of ameliorating this gradient explosion. These experiments
12
Published as a conference paper at ICLR 2019
Figure 5: Three techniques for counteracting gradient explosion. Test accuracy on MNIST as a
function of different hyperparameters along with theoretical predictions (white dashed line) for the
maximum trainable depth. (a) tanh network changing the overall scale of the pre-aCtivations, here
Y → 0 corresponds to the linear regime. (b) Rectified linear network changing the mean of the pre-
activations, here β → ∞ corresponds to the linear regime. (c,d) tanh and rectified linear networks
respectively as a function of e, here We observe a well defined phase transition near e 〜1. Note that
in the case of rectified linear activations we use β = 2 so that the function is locally linear about 0.
We also find initializing β and/or setting e > 0 having positive effect on VGG19 with batchnorm.
SeeFigs.8and9.
are shown in Fig. 5 with theoretical predictions for the maximum trainable depth overlaid; in all
cases we see exceptional agreement. In Fig. 5 (a,b) we explore two different ways of tuning the
degree to which activation functions in a network are nonlinear. In Fig. 5 (a) we tune γ ∈ [0, 2]
for networks with tanh-activations and note that in the γ → 0 limit the function is linear. In Fig. 5
(b) we tune β ∈ [0, 2] for networks with rectified linear activations and we note, similarly, that in
the β → ∞ limit the function is linear. As expected, we see the maximum trainable depth increase
significantly with decreasing γ and increasing β. In Fig. 5 (c,d) we vary e for tanh and rectified
linear networks respectively. In both cases, we observe a critical point at large e where gradients do
not explode and very deep networks are trainable.
5	Conclusion
In this work we have presented a theory for neural networks with batch normalization at initializa-
tion. In the process of doing so, we have uncovered a number of counterintuitive aspects of batch
normalization and - in particular - the fact that at initialization it unavoidably causes gradients to
explode with depth. We have introduced several methods to reduce the degree of gradient explo-
sion, enabling the training of significantly deeper networks in the presence of batch normalization.
Finally, this work paves the way for future work on more advanced, state-of-the-art, network archi-
tectures and topologies.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv:1607.06450
[cs, stat], July 2016. URL http://arxiv.org/abs/1607.06450.
David Balduzzi, Marcus Frean, Lennox Leary, J. P. Lewis, Kurt Wan-Duo Ma, and Brian
McWilliams. The Shattered Gradients Problem: If resnets are the answer, then what is the ques-
tion? In PMLR, pp. 342-350, July 2017. URL http://Proceedings .mlr.ρress∕v70∕
balduzzi17b.html.
Johan Bjorck, Carla Gomes, and Bart Selman. Understanding Batch Normalization. June 2018.
URL https://arxiv.org/abs/1806.02375.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field
theory of RNNs: Gating enables signal propagation in recurrent neural networks. In Jennifer Dy
and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learn-
ing, volume 80 of Proceedings of Machine Learning Research, pp. 873-882, Stockholmsmssan,
13
Published as a conference paper at ICLR 2019
Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://Proceedings .mlr.press/
v80/chen18i.html.
Q. Chen and R. Wu. CNN Is All You Need. ArXiv e-prints, December 2017.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in neu-
ral information processing systems, pp. 342-350, 2009. URL http://papers.nips.cc/
paper/3628- kernel- methods- for- deep- learning.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks:
The Power of Initialization and a Dual View on Expressivity. arXiv:1602.05897 [cs, stat], Febru-
ary 2016. URL http://arxiv.org/abs/1602.05897.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
H1-nGgWC-.
Adria Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional net-
works as shallow gaussian processes. International Conference on Learning Representations,
2019.
Dar Gilboa, Bo Chang, Minmin Chen, Greg Yang, Samuel S. Schoenholz, Ed H. Chi, and Jeffrey
Pennington. Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs. arXiv e-prints,
art. arXiv:1901.08987, Jan 2019.
Igor Gitman and Boris Ginsburg. Comparison of Batch Normalization and Weight Normalization
Algorithms for the Large-scale Image Classification. arXiv:1709.08145 [cs], September 2017.
URL http://arxiv.org/abs/1709.08145.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
arXiv preprint arXiv:1803.01719, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. arXiv:1512.03385 [cs], December 2015. URL http://arxiv.org/abs/
1512.03385.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training
by Reducing Internal Covariate Shift. arXiv:1502.03167 [cs], February 2015. URL http:
//arxiv.org/abs/1502.03167.
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas Hof-
mann. Towards a Theoretical Understanding of Batch Normalization. arXiv:1805.10694 [cs,
stat], May 2018. URL http://arxiv.org/abs/1805.10694.
Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E
Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation net-
work. In Advances in neural information processing systems, pp. 396-404, 1990.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. Understanding Regularization in Batch
Normalization. arXiv:1809.00846 [cs, stat], September 2018. URL http://arxiv.org/
abs/1809.00846.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abo-
lafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are gaussian processes. International Conference of Learning Representations,
2019. URL https://openreview.net/forum?id=B1g30j0qF7.
14
Published as a conference paper at ICLR 2019
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Advances in neural information
processing Systems,pp. 4785-4795, 2017.
George Philipp and Jaime G. Carbonell. The Nonlinearity Coefficient - Predicting Overfitting in
Deep Neural Networks. arXiv:1806.00179 [cs, stat], May 2018. URL http://arxiv.org/
abs/1806.00179.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. arXiv:1606.05340 [cond-mat,
stat], June 2016. URL http://arxiv.org/abs/1606.05340.
Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to
Accelerate Training of Deep Neural Networks. February 2016. URL https://arxiv.org/
abs/1602.07868.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How Does Batch Nor-
malization Help Optimization? (No, It Is Not About Internal Covariate Shift). arXiv:1805.11604
[cs, stat], May 2018. URL http://arxiv.org/abs/1805.11604.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information
Propagation. arXiv:1611.01232 [cs, stat], November 2016. URL http://arxiv.org/abs/
1611.01232.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. 2018.
P.K. Suetin. Ultraspherical polynomials - Encyclopedia of Mathematics. URL https://www.
encyclopediaofmath.org/index.php/Ultraspherical_polynomials.
Eric W. Weisstein. Gegenbauer Polynomial. URL http://mathworld.wolfram.com/
GegenbauerPolynomial.html.
Christopher KI Williams. Computing with infinite networks. In Advances in neural information
processing systems, pp. 295-301, 1997.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla con-
volutional neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 5393-5402, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/xiao18a.html.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Greg Yang and Samuel S. Schoenholz. Meanfield Residual Network: On the Edge of Chaos. In
Advances in neural information processing systems, 2017.
Greg Yang and Samuel S Schoenholz. Deep mean field theory: Layerwise variance and width
variation as methods to control gradient explosion. 2018.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition.
15
Published as a conference paper at ICLR 2019
Layer 0	Layer 2	Layer 4	Layer 6
,.∙∙∙∙∙	.-“F • ・：	.・・・・・,• • ∙ •	a	.∙∙•…∙∙∙∙. • β ∙	∙
∙√ :	∙ ∙	・ ∙	: ∙
工.・・・・・••	*,∙∙	∙∙,	・	a •・・・・・・∙∙	N.•….∙∙
Layer 8	Layer 10	Layer 12	Layer 14
.∙∙”…∙∙. ♦ ∙ ♦ ∙	a∙ ∙ ∙ a F ：［ ・ ∙	C	τ∖
♦	∙ ∙∙.∙	i • *♦•••••	• ♦		
Layer 16	Layer 18	Layer 20	Layer 22
,O ,.,一/	C	— ∙∙Jk∙∙ ∙∙ ∙ ∙	「二飞 6..∙d •・・・・・・・
Layer 24	Layer 26	Layer 28	Layer 30
		S.	/	Λ	「・・
.∙ ∙∙. (a)	y∙	a・・♦. ∙ ∙ f∙∙∙ M ∙∙∙: l,∙∙∙∙U∙∙∙,^ V	.・♦••••・・ (ɜ ： , ∙ ∙ ⅝ ∙ ∙∙∙ ：J
(b)
Figure 6:	Batch norm leads to a chaotic input-output map with increasing depth. A linear
network with batch norm is shown acting on two minibatches of size 64 after random orthogonal
initialization. The datapoints in the minibatch are chosen to form a 2d circle in input space, except
for one datapoint that is perturbed separately in each minibatch (leftmost datapoint at input layer 0).
Because the network is linear, for a given minibatch it performs an affine transformation on its inputs
-a circle in input space remains an ellipse throughout the network. However, due to batch norm
the coefficients of that affine transformation change nonlinearly as the datapoints in the minibatch
are changed. (a) Each pane shows a scatterplot of activations at a given layer for all datapoints
in the minibatch, projected onto the top two PCA directions. PCA directions are computed using
the concatenation of the two minibatches. Due to the batch norm nonlinearity, minibatches that
are nearly identical in input space grow increasingly dissimilar with depth. Intuitively, this chaotic
input-output map can be understood as the source of exploding gradients when batch norm is applied
to very deep networks, since very small changes in an input correspond to very large movements in
network outputs. (b) The correlation between the two minibatches, as a function of layer, for the
same network. Despite having a correlation near one at the input layer, the two minibatches rapidly
decorrelate with depth. See Appendix H for a theoretical treatment.
A VGG19 with Batchnorm on CIFAR 1 00
Even though at initialization time batchnorm causes gradient explosion, after the first few epochs,
the relative gradient norms ∣∣Vθ L∣∣∕∣∣θ∣∣ for weight parameters θ = W or BN scale parameter θ = Y,
equilibrate to about the same magnitude. See Fig. 7.
16
Published as a conference paper at ICLR 2019
O 1
O -
1 M
10→-
10-3∙
0	5	10	15	20	25	30
PaeEeteE β (y«nd M
_£_圭
0	5	10	15 2β 25	30
PaeEeteEetrand M
∙s'J
Figure 7:	relative gradient norms of different parameters in layer order (input to output from left to
right), with γ and W interleaving. From dark to light blue, each curve is separated by (a) 3, (b) 5,
or (c) 10 epochs. We see that after 10 epochs, the relative gradient norms of both γ and W for all
layers become approximately equal despite gradient explosion initially.
We find acceleration effects, especially in initial training, due to setting > 0 and/or initializing
β > 0. See Figs. 8 and 9.
ii
taccl
tacc4
TOO°O.O8.OSO.OT.O"O
-10
-8
-2
BNeps
vaccl
O g «-« rι ιr
° BNeps
vacc4
-36
-3°
24
18
tacclθ
tacc20
-50
«
30
20
10
TOO°O.O8.OSO.OT.O"O
-15
-12
-36
-30
U-24
18
-3
BNeps
O g «-« rι ιr
° BNeps
taccl
tacc4
q y τ rj in
° BNeps
vacclθ
O g «-« rι ιr
° BNeps
-50
30
20
10
q y τ rj In
° BNepS
vacc20
O g «"« rι u
° BNepS
60
-50
40
30
20
10
TOO°O.O8.OSO.OT.O"O
-10
-8
-2
betainit
betainit
-36
-30
u-24
18
12
vaccl
vacc4
tacclθ
tacc20
-50
«
60
30
20
-45
30
15
TOO6O.O8.OSO.OT.O"O
-15
-36
betainit
-12
-3
betainit
10
S 8
?5 3
betainit
betainit
vacclθ
vacc20
18
12
6
^3°
24
-50
40
S 8
?5 3
betainit
betainit
60
tacc40
-45
30
15
60
60
-45
BNepS
vacc40
O g «-J (N IT
BNeps
tacc40
30
15
90
175
60
60
?5 3
betainit
vacc40
30
20
10
30
15
I S ≡
betainit
-45
taccBO
-8°
60
40
20
q y τ rj In
BNeps
vaccBO
O g «-« rι ιr
BNeps
-60
45
30
15
80
taccBO
-45
30
15
40
20
I S ≡
betainit
vaccBO
60
-60
45
30
15
I S ≡
betainit
6
4
9
6
6
4
9
6
6
6
6
«
S 8
S 8
Figure 8: We sweep over different values of learning rate, β initialization, and , in training VGG19
with batchnorm on CIFAR100 with data augmentation. We use 8 random seeds for each combina-
tion, and assign to each combination the median training/validation accuracy over all runs. We then
aggregate these scores here. In the first row we look at training accuracy with different learning rate
vs β initialization at different epochs of training, presenting the max over . In the second row we
do the same for validation accuracy. In the third row, we look at the matrix of training accuracy for
learning rate vs , taking max over β. In the fourth row, we do the same for validation accuracy. B
B Gradient Independence Assumption
Following prior literature Schoenholz et al. (2016); Yang & Schoenholz (2017); Xiao et al. (2018),
in this paper, in regards to computations involving backprop, we assume
Assumption 2. During backpropagation, whenever we multiply by WT for some weight matrix W,
we multiply by an iid copy instead.
17
Published as a conference paper at ICLR 2019
taccl
tacc4
tacclθ
tacc20
tacc40
τo°o°08°50°τ°"o
*
TOO6°O8°5O°T°"O
*
TOO=°O8°5O°T°"O
*
BNeps
vaccl
BNepS
taccl
betainit
vaccl
3
-9.0
90
betainit
-7.5
-30
β∙0
4.5
3.0
1.5
-15.0
.5
10.0
7.5
5.0
-2.5
-10
-2
-15
-12
-3
24
18
12
BNeps
BNeps
vacc4
-30
-24
18
12
BNeps
tacc4
-30
-24
18
12
betainit
vacc4
-36
-30
24
18
12
betainit
vacclθ
BNeps
tacclθ
betainit
vacclθ
betainit
-50
-40
60
-30
-20
10
-50
-30
-20
10
-50
«
30
20
10
-50
30
20
10
-45
30
15
BNepS
BNeps
taccBO
-45
30
15
Lr 75
-«
BNepS
vacc20
vacc40
-50
40
30
20
10
BNeps
BNeps
vaccBO
60
-45
30
15
BNepS
taccBO
W
tacc20
tacc40
betainit
vacc20
betainit
6°
r75
-45
30
15
-50
«
30
20
10
60
-45
30
15
betainit
betainit
60
40
20
vacc40
60
-45
30
15
betainit
vaccBO
-60
30
15
betainit

S O 5 3
6
6
6
4
9
6
S 8
6
6
S 8

«
?5 3
S O 5 3
?5 3
?5 3
Figure 9: In the same setting as Fig. 8, except we don’t take the max over the unseen hyperparameter
but rather set it to 0 (the default value).
As in these previous works, we find excellent agreement between computations made under this
assumption and the simulations (see Fig. 1). Yang (2019) in fact recently rigorously justified this as-
sumption as used in the computation of moments, for a wide variety of architectures, like multilayer
perceptron (Schoenholz et al., 2016), residual networks (Yang & Schoenholz, 2017), and convolu-
tional networks (Xiao et al., 2018) studied previously, but without batchnorm. The reason that their
argument does not extend to batchnorm is because of the singularity in its Jacobian at 0. However,
as observed in our experiments, we expect that a proof can be found to extend Yang & Schoenholz
(2017)’s work to batchnorm.
C Notations
Definition C.1. Let SB be the space of PSD matrices of size B × B. Given a measurable function
Φ : RB → RA, define the integral transform Vφ : SB → Sa by Vφ(∑) = E[Φ(h产2 : h 〜
N(0, Σ)].3 When φ : R → R and B is clear from context, we also write Vφ for V applied to the
function acting coordinatewise by φ.
Definition C.2. For any φ : R → R, let Bφ : RB → RB be batchnorm (applied
to a batch of neuronal activations) followed by coordinatewise applications of φ, Bφ(x)j =
φ ((Xj - AvgX) / q B PB=ι (Xi—AVgX)2)
(here Avg X = B PB=I Xi). When φ = id We will
also write B = Bid .
Definition C.3. Define the matrix GB = I - -B 11T. Let SB be the space ofPSD matrices of size
B × B with zero mean across rows and columns, SBG := {Σ ∈ SB : GB ΣGB = Σ} = {Σ ∈ SB :
Σ1 = 0}.
3This definition of V absorbs the previous definitions of V and W in Yang & Schoenholz (2017) for the
scalar case
18
Published as a conference paper at ICLR 2019
When B is clear from context, we will suppress the subscript/superscript B. In short, for h ∈ RB,
Gh zeros the sample mean of h. G is a projection matrix to the subspace of vectors h ∈ RB of zero
coordinate sum. With the above definitions, We then have Βφ(h) = φ(√BGh∕∣∣Ghk).
We use Γ to denote the Gamma function, P (a, b) := Γ(a + b)∕Γ(a) to denote the Pochhammer
symbol, and Beta(a, b) = ："+') to denote the Beta function. We use(., .〉to denote the dot
product for vectors and trace inner product for matrices. Sometimes When it makes for simpler
notation, we will also use ∙ for dot product.
We adopt the matrix convention that, for a multivariate function f : Rn → Rm, the Jacobian is
∕∂fι	∂fι	∂fι ∖
∂xι	∂x2	∂Xn
j ,	∂f2	∂f2	∂f2
df = ∂xι	∂X2	∂Xn I
dx	.	...	.	，
.	.	..
∂fm	∂fm …∂fm
∂ ∂xι	∂X2	∂Xn /
so that f (X + ∆x) = ddX∆x + o(∆x), where X and ∆x are treated as column vectors. In what
follows we further abbreviate Bφ(x) = dBφJz) [=χ, Bφ(χ)∆χ = dBφ(Z)Iz=X∆χ.
D A Guide to the Rest of the Appendix
As discussed in the main text, we are interested in several closely related dynamics induced by
batchnorm in a fully-connected network with random weights. One is the forward propagation
equation Eq. (43)
Σl = VBφ(Σl-1)
studied in Appendix F. Another is the backward propagation equation Eq. (51)
∏l = VBφ (Σ*)t {∏l+1}
studied in Appendix G. We will also study their generalizations to the simultaneous propagation of
multiple batches in Appendices H and I.
In general, we discover several ways to go about such analyses, each with their own benefits and
drawbacks:
• The Laplace Method (Appendix E.1): If the nonlinearity φ is positive homogeneous, we can
use Schwinger’s parametrization to turn VBφ and VB0 into nicer forms involving only 1 or
2 integrals, as mentioned in the main text. All relevant quantities, such as eigenvalues of the
backward dynamics, can be further simplified to closed forms. Lemma E.2 gives the master
equation for the Laplace method. Because the Laplace method requires φ to be positive
homogeneous to apply, we review relevant results of such functions in Appendices E.1.1
and E.1.2.
• The Fourier Method (Appendix E.2): For polynomially bounded and continuous nonlin-
earity φ (for the most general set of conditions, see Thm E.25), we can obtain similar
simplifications using the Fourier expansion of the delta function, with the penalty of an
additional complex integral.
• Spherical Integration (Appendix E.3): Batchnorm can naturally be interpreted as a linear
projection followed by projection to a sphere of radius √B. Assuming that the forward
dynamics converges to a BSB1 fixed point, one can express VBφ and VB0 as spherical in-
tegrals, for all measurable φ. One can reduce this (B- 1)-dimensional integral, in spherical
angular coordinates, into 1 or 2 dimensions by noting that the integrand depending on φ
only depends on 1 or 2 angles. Thus this method is very suitable for numerical evaluation
of quantities of interest for general φ.
• The Gegenbauer Method (Appendix E.4): Gegenbauer polynomials are orthogonal poly-
nomials under the weight (1 - x2)α for some α. They correspond to a special type of
19
Published as a conference paper at ICLR 2019
spherical harmonics called zonal harmonics (Defn E.44) that has the very useful repro-
ducing property (Fact E.46). By expressing the nonlinearity φ in this basis, we can see
easily that the relevant eigenvalues of the forward and backward dynamics are ratios of
two quadratic forms of φ's GegenbaUer coefficients. In particular, for the largest eigen-
value of the backward dynamics, the two quadratic forms are both diagonal, and in a way
that makes apparent the necessity of gradient explosion (under the BSB1 fixed point as-
sumption). In regards to numerical computations, the Gegenbauer method has the benefit
of only requiring 1-dimensional integrals to obtain the coefficients, but if φ has slowly-
decaying coefficients, then a large number of such integrals may be required to get an
accurate answer.
In each of the following sections, we will attempt to conduct an analysis with each method when it
is feasible.
When studying the convergence rate of the forward dynamics and the gradient explosion of the back-
ward dynamics, we encounter linear operators on matrices that satisfy an abundance of symmetries,
called ultrasymmetric operators. We study these operators in Appendix E.5 and contribute several
structural results Thms E.61, E.62, E.72 and E.72 on their eigendecomposition that are crucial to
deriving the asymptotics of the dynamics.
All of the above will be explained in full in the technique section Appendix E. We then proceed to
study the forward and backward dynamics in detail (Appendices F to I), now armed with tools we
need. Up to this point, we only consider the dynamics assuming Eq. (43) converges to a BSB1 fixed
point, and B ≥ 4. In Appendix J we discuss what happens outside this regime, and in Appendix K
we show our current understanding of BSB2 fixed point dynamics.
Note that the backward dynamics studied in these sections is that of the gradient with respect to
the hidden preactivations. Nevertheless, we can compute the moments of the weight gradients from
this, which is of course what is eventually used in gradient descent. We do so in the final section
Appendix L.
Main Technical Results Corollary F.3 establishes the global convergence of Eq. (43) to BSB1
fixed point for φ = id (as long as the initial covariance is nondegenerate), but we are not able to
prove such a result for more general φ. We thus resort to studying the local convergence properties
of BSB1 fixed points.
First, Thms F.5, F.8 and F.13 respectively compute the BSB1 fixed point from the perspectives of
spherical integration, the Laplace method, and the Gegenbauer method. Thms F.22 and F.24 give
the Gegenbauer expansion of the BSB1 local convergence rate, and Thm F.33 gives a more succinct
form of the same for positive homogeneous φ. Appendix F.3.1 discusses how to do this computation
via spherical integration.
Next, we study the gradient dynamics, and Thms G.5 and G.12 yield the gradient explosion rates
with the Gegenbauer method and the Laplace method (for positive homogeneous φ), respectively.
Appendix G.1.1 discusses how to compute this for general φ using spherical integration.
We then turn to the dynamics of cross batch covariances. Thm H.3 gives the form of the cross
batch fixed point via spherical integration and the Gegenbauer method, and Thm H.6 yields a more
specific form for positive homogeneous φ via the Laplace method. The local convergence rate to
this fixed point is given by Thms H.13 and H.14, respectively using the Gegenbauer method and the
Laplace method. Finally, the correlation between gradients of the two batches is shown to decrease
exponentially in Corollary I.6 using the Gegenbauer method, and the decay rate is given succinctly
for positive homogeneous φ in Thm I.3.
E Techniques
We introduce the key techniques and tools in our analysis in this section.
20
Published as a conference paper at ICLR 2019
E.1 Laplace Method
As discussed above, the Laplace method is useful for deriving closed form expressions for positive
homogeneous φ. The key insight here is to apply Schwinger parametrization to deal with normal-
ization.
Lemma E.1 (Schwinger parametrization). For z > 0 and c > 0,
c
-z
Γ(z)-1 ∞ xz-1e-cx dx
0
The following is the key lemma in the Laplace method.
Lemma E.2 (The Laplace Method Master Equation). For A, A0 ∈ N, let f : RA → RA0 and let
k ≥ 0. Suppose kf (y)k ≤ h(kyk) for some nondecreasing function h : R≥0 → R≥0 such that
E[h(rkyk) ： y ∈ N (0,Ia )] exists for every r ≥ 0. Define 夕(Σ) := E[kyk-2k f (y) : y 〜N (0, ∑)].
Then on {Σ ∈ Sa : rankΣ > 2k},夕(∑) is well-defined and continuous, and furthermore satisfies
夕(∑)
∞
Γ(k)-1
0
ds SkT det(I + 2s∑)-1/2 E[f (y) : y
〜N(0, Σ(I +2s∑)-1)]
(25)
Proof. ρ(∑) is well-defined for full rank Σ because the ∣∣yk-2k singularity at y = 0 is LebesgUe-
integrable in a neighborhood of 0 in dimension A > 2k.
We prove Eq. (25) in the case when ∑ is full rank and then apply a continuity argument.
Proof of Eq. (25) for full rank ∑. First, we will show that we can exchange the order of integration
Z	ds	Z	dy	sk-1f (y)e-2y'3 ι+2sI)y	= Z	dy	Z	ds	sk-1f (y)e-2y'3	ι+2sI)y
0	RA	RA	0
by Fubini-Tonelli’s theorem. Observe that
(2π)-A∕2 det∑-1/2 Z dssk-1 Z dy ∣∣f (y)∣e-2yT3-1+231)"
0	RA
∞ ds SkT det(∑(∑-1 + 2sI))-1/2 E[kf (y)k : y 〜N(0, (Σ-1 + 2sI)-1)]
/
0
Z
0
ds SkT det(I + 2s∑)-1/2 E[kf (y)k : y 〜N(0, Σ(I + 2s∑)-1)]
For λ = kΣk2,
kf (√∑(I + 2s∑)-1y)k ≤ h(∣√Σ(I + 2s∑)-1y∣ ≤ h Qj 1+2^) ≤ h(√λ∣y∣∣)
Because E[h(√λ∣yk) : y ∈ N(0, I)] exists by assumption,
E[kf(y)k : y 〜N(0, ∑(I + 2s∑)-1)]
=E[∣f (√Σ(I +2s∑)-1y)k : y 〜N(0,I)]
→E[kf(0)k : y 〜Nal)] = kf(0)k
as S → ∞, by dominated convergence with dominating function h(√λ∣yk)e-2 kyk2 (2π)-A/2. By
the same reasoning, the function s 7→ E[∣f(y)∣ : y 〜N(0, Σ(I + 2s∑)-1)] is continuous. In
particular this implies that supo≤s≤∞ E[∣∣f (y)k : y 〜N(0, ∑(I + 2s∑)-1)] < ∞. Combined with
the fact that det(I + 2s∑)-1/2 = Θ(s-A/2) as S → ∞,
∞
ds SkT det(I + 2s∑)-1/2
E[∣f(y)∣ : y
〜N(0, Σ(I + 2s∑)-1)]
∞
≤0
ds Θ(sk-1-A/2)
0
21
Published as a conference paper at ICLR 2019
which is bounded by our assumption that A/2 > k. This shows that we can apply Fubini-Tonelli’s
theorem to allow exchanging order of integration.
Thus,
E[kyk-2kf(y) ： y 〜N(0, ∑)]
E[f(y) ∞ ds Γ(k)-1sk-1e-kyk2s : y 〜N(0, Σ)]
0
(2π)-A/2 det Σ-1/2 Z dy e-1 yTςTyf (y) /∞ ds Γ(k)-1sk-1e-kyk2s
(2n)-A/2r(k)T det Σ-1/2 /∞ ds SkT Z dy f (y)e-2yT W)
(by Fubini-Tonelli)
∞
Γ(k)-1
0
ds sk-1 det(Σ(Σ-1 + 2sI))-1/2 E[f(y)
：y 〜N(0, (∑-1 + 2sI)-1)]
∞
Γ(k)-1
0
ds SkT det(I + 2sΣ)-1/2
E[f(y) : y
〜N(0, Σ(I + 2sΣ)-1)]
Domain and continuity of 夕(Σ). The LHS of Eq. (25),夕(∑), is defined and continuous on
rank Σ/2 > k. Indeed, if Σ = MICMT, where M is a full rank A X C matrix with
rank Σ = C ≤ A, then
E[kyk-2kkf(y)k ： y 〜Na ∑)]
=E[kMzk-2kkf(Mz)k ： Z 〜N(0,IC)].
This is integrable in a neighborhood of 0 iff C > 2k, while it’s always integrable outside a ball
around 0 because ∣∣f k by itself already is. So 夕(Σ) is defined whenever rank Σ > 2k. Its continuity
can be established by dominated convergence.
Proofoj Eq. (25) for rankΣ > 2k. Observe that det(I + 2sΣ)-1/2 is continuous in Σ and, by
an application of dominated convergence as in the above, E[f (y) : y 〜 N(0, Σ(I + 2sΣ)-1)] is
continuous in Σ. So the RHS of Eq. (25) is continuous in Σ whenever the integral exists. By the
reasoning above, E[f (y) : y 〜N(0, Σ(I + 2sΣ)-1)] is bounded in S and det(I + 2sΣ)-1/2 =
Θ(s- rank-/2), so that the integral exists iff rank Σ∕2 > k.
To summarize, we have proved that both sides of Eq. (25) are defined and continous for rank Σ >
2k. Because the full rank matrices are dense in this set, by continuity Eq. (25) holds for all rank Σ >
2k.	□
If φ is degree-α positive homogeneous, i.e. φ(ru) = rαφ(u) for any u ∈ R, r ∈ R+, we can apply
Lemma E.2 with k = α,
VBφ(Σ) = E[φ(√BGh∕kGhk)02 : h 〜N(0, Σ)] = Bα E[φ(Gh)02∕∣Ghk2α : h 〜N(0, Σ)]
=Bα E[φ(z)02∕kz∣2α : z 〜N(0, GΣG)]
∞
0
BαΓ(α)-1
ds sα-1
det(I + 2sΣG)T∕2Vφ(G(I + 2sΣG)-1ΣG).
If φ = relu, then
1-21-2
if i = j
otherwise
and, more succinctly, Vφ(∑) = D"Vφ(D-"∑D-")D1/2 where D = Diag(Σ). Here
Jι(c) := ∏1 (√1 一 c2 + (π 一 arccos(c))c) (Cho & Saul, 2009).
22
Published as a conference paper at ICLR 2019
Matrix simplification. We can simplify the expression G(I + 2sΣG)-1ΣG, leveraging the fact
that G is a projection matrix.
Definition E.3. Let e be an B X (B 一 1) matrix whose columns form an orthonormal basis of
im G := {Gv : v ∈ RB } = {w ∈ RB : Pi Wi = 0}. Then the B × B matrix ® = (®|B-1/21) is
an orthogonal matrix. For much of this paper e can be any such basis, but at certain sections We will
consider specific realizations of e for explicit computation.
From easy computations it can be seen that G = ® (IB-I 0) ®T ∙ Suppose Σ = ® (：斤 ：)eT
where ∑□ is (B 一 1) × (B 一 1), V is a column vector and a is a scalar. Then ∑□ = T∑Σe and
ΣG = ® (：□ 0) ®T is block lower triangular, and
(I + 2sΣG)-1 =® ((I +2*)-1 0卜T
(I + 2sΣG)-1ΣG = ® ((I +2s∑□)-1ς□ 0卜T
G(I + 2sΣG)-1ΣG = ® ((I +2s∑□)-1ς□ 0卜T
=e(I + 2s∑□ )-1∑□ Tτ
= GΣG(I + 2sGΣG)-1
=: ΣG(I + 2sΣG)-1
where
Definition E.4. For any matrix Σ, write ΣG := GΣG.
Similarly, det(IB + 2sΣG) = det(IB-1 + 2sΣ□) = det(IB + 2sΣG). So, altogether, we have
Theorem E.5. Suppose φ : R → R is degree-α positive homogeneous. Then for any B × (B 一 1)
matrix e whose columns form an orthonormal basis of im G := {Gv : V ∈ RB} = {w ∈ RB :
Ei Wi = 0}, with ∑□ = TτΣe,
VBφ(Σ)
∞
BαΓ(α)-1
0
ds SaT det(I + 2s∑□)T2Vφ(<B(I + 2s∑□)-1∑□)-1 ©T)
∞
BαΓ(α)-1
0
ds SaT det(I + 2sΣG)-1/2Vφ((I + 2sΣG)-1ΣG)T)
(26)
(27)
Now, in order to use Laplace’s method, we require φ to be positive homogeneous. What do these
functions look like? The most familiar example to a machine learning audience is most likely
ReLU. It turns out that 1-dimensional positive homogeneous functions can always be described
as a linear combination of powers of ReLU and its reflection across the y-axis. Below, we review
known facts about these functions and their integral transforms, starting with the powers of ReLU
in Appendix E.1.1 and then onto the general case in Appendix E.1.2.
E.1.1 α-RELU
Recall that α-ReLUs (Yang & Schoenholz, 2017) are, roughly speaking, the αth power of ReLU.
Definition E.6. The α-ReLU function ρa : R → R sends x 7→ xa when x > 0 and x 7→ 0
otherwise.
This is a continuous function for α > 0 but discontinuous at 0 for all other α.
We briefly review what is currently known about the V and W transforms of ρa (Cho & Saul, 2009;
Yang & Schoenholz, 2017).
Definition E.7. For any α > 一2, define Ca = √∏2a-1Γ(α + ɪ).
23
Published as a conference paper at ICLR 2019
When considering only 1-dimensional Gaussians, Vρα is very simple.
Proposition E.8. If a > 一2,thenfOrany q ∈ Si = R≥0, NPalq) = Caqa
To express results of Vρα on SB for higher B, we first need the following
Definition E.9. Define
Ja(θ) := :ɪ(sin θ)2a+1Γ(α + 1)「/ --dηCosa η
a	2πca	0	(1 一 cosθcosη)1+a
and Ja(c) = Ja(arccos c) for α > 一1/2.
Then
Proposition E.10. FOr any Σ ∈ SB, let D be the diagOnal matrix with the same diagOnal as Σ.
Then
Vρa(Σ) = CaDa/2Ja(DT/2£D-1/2)Da/2
where Ja is applied entrywise.
For example, Ja and Ja for the first few integral α are
J0(θ)
J1(θ)
J2(θ)
π — θ
π
Sin θ + (π — θ) Cos θ
π
3 sin θ cos θ + (π — θ)(1 + 2 cos2 θ)
3π
π — arCCos c
Jο(c)=------------
π
J1(c)
J2(c)
√1 — c2 + (π — arccos c)c
π
3c√1 — c2 + (π —arccos c)(1 + 2c2)
3π
One can observe very easily that (Daniely et al., 2016; Yang & Schoenholz, 2017)
Proposition E.11. FOr each α > —1/2, Ja(c) is an increasing and cOnvex functiOn On c ∈ [0, 1],
i Γ( α +1 )2	T
and is Continous on C ∈ [0,1] and smooth on C ∈ (0,1). Ja(1) = 1, Ja(0) = 2√∏ ,(.十1),and
Ja(-1) = 0.
Yang & Schoenholz (2017) also showed the following fixed point structure
Theorem E.12. For α ∈ [1/2, 1), Ja(C) = C has two solutions: an unstable solution at 1 (”unsta-
ble” meaning Ja(1) > 1) and a Stable solution in c* ∈ (0,1) ("stable” meaning J∖(c*) < 1).
The α-ReLUs satisfy very interesting relations amongst themselves. For example,
Lemma E.13. Suppose α > 1. Then
Ja(θ) = cos θJa-i (θ) + (α — 1)2(2α — 1)-i (2α — 3)-i sin2 θJa-2 (θ)
Ja(C) = CJa-i (C) + (α — 1)2(2α — 1)-i (2α — 3)-i (1 — C2)Ja-2(C)
In additional, surprisingly, one can use differentiation to go from α to α + 1 and from α to α — 1!
Proposition E.14 (Yang & Schoenholz (2017)). Suppose α > 1/2. Then
Ja(θ) = —α2(2α — 1)-i Ja-i(θ) Sinθ
J0a(C) = α2(2α — 1)-iJa-i (C)
so that
a
Ja-n(c) =	Y β-2(2β — 1) (∂∕∂c)nJa(c)
β=a-n+i
24
Published as a conference paper at ICLR 2019
We have the following from Cho & Saul (2009)
Proposition E.15 (Cho & Saul (2009)). For all α ≥ 0 and integer n ≥ 1
Jn+α(c)=2(1 — C2)n+α+ 1(∂∕∂c)n(Jα(c)∕(1 — C2)α+1 )
cn+α
-1
α+n-1
= Y (2β +1)	(1 — c2)n+α+ 2(∂∕∂c)n(Jα(c)∕(1 — c2)α+ 2)
β=α
This implies in particular that we can obtain J0α from Jα and Jα+1.
Proposition E.16. For all α ≥ 0,
J0α(c) = (2α + 1)(1 — c2)-1(J1+α(c) — cJα(c)))
Proof.
Jl+α (C)= 3 (1 — C2)1+α+ 1(∂∕∂c)(Jα(c)∕(1 — C2)α+1 )
c1+α
= (2α + 1)-1(1 — c2)α+3/2 (Jα(c)∕(1 — c2)α+1/2 + 2c(α + 1∕2)Jα(c)∕(1 — c2)α+3/2)
= (2α + 1)-1J0α(c)(1 — c2) + cJα(c)
J0α(c)= (2α + 1)(1 — c2)-1(J1+α(c) — cJα(c))
□
Note that we can also obtain this via Lemma E.13 and Proposition E.14.
E.1.2 Positive-Homogeneous Functions in 1 Dimension
Suppose for some α ∈ R, φ : R → R is degree α positive-homogeneous, i.e. φ(rx) = rαφ(x)
for any x ∈ R, r > 0. The following simple lemma says that we can always express φ as linear
combination of powers of α-ReLUs.
Proposition E.17. Any degree α positive-homogeneous function φ : R → R with φ(0) = 0 can be
written as x → apα(x) — bpa(—x).
Proof. Take a = φ(1) and b = Φ(-1). Then positive-homogeneity determines the value of φ on
R \ {θ} and it coincides with X → aρα(x) — bpa(—x).	□
As a result we can express the V and W transforms of any positive-homogeneous function in terms
of those of α-ReLUs.
Proposition E.18. Suppose φ : R → R is degree α positive-homogeneous. By Proposition E.17, φ
restricted to R \ {0} can be written as x 7→ aρα(x) — bρα (—x) for some a and b. Then for any PSD
2 × 2 matrix M,
Vφ(M)11 = (a2+b2)Vρα(M)11 = cα(a2 + b2)M1α1
Vφ(M)22 = (a2+b2)Vρα(M)22 = cα(a2 + b2)M2α2
Vφ(M)12 = Vφ(M)21 = (a2 + P)NPa (M)12 — 2abVρ° (M0)12
=Cα(M11M22)a∖(a2 + b2)Jα(M12∕PM11M22) — ZablaLMWPMlM^))
where M0 := 10 —01 M
Proof. We directly compute, using the expansion of φ into ραs:
Vφ(M)11 = E[φ(x)2 : x 〜N(0, M11)]
=E[a2pα(x)2 + b2pa(—x)2 + 2abpa(x)pa(—x)]
=a2 E[ρα(x)2] + b2 E[pa(—x)2]
= (a2 + b2) E[ρα(x)2 : x 〜 N(0, M11)]	(28)
= cα(a2 + b2)M1α1
10 —01 .
25
Published as a conference paper at ICLR 2019
where in Eq. (28) we used negation symmetry of centered Gaussians. The case of Vφ (M)22 is
similar.
Vφ(M)12 = E[Φ(χ)Φ(y): (χ,y)〜N(0, M)]
= E[a2ρα (x)ρα (y) + b2ρα(-x)ρα(-y) - abρα(x)ρα(-y) - abρα(-x)ρα(y)]
=(a2+b2)Vρα(M)12 -2abVρα(M0)12
=Ca(M1lM22)a/2((a2 + b2Jα(Ml2/PMlMz) - 2abJ a(-Ml2 / P M11M22))
where in the last equation We have applied Proposition E.10.	□
This then easily generalizes to PSD matrices of arbitrary dimension:
Corollary E.19. Suppose φ : R → R is degree α positive-homogeneous. By Proposition E.17, φ
7→ aρα(x) - bρα(-x) for some a and b. Let Σ ∈ SB. Then
Da/210(。-1/2£。-1/2)Da/2
restricted to R\ {0} can be written as x
Vφ(Σ) = Ca
where Jφ is defined below and is applied entrywise. Explicitly, this means that for all i,
Vφ(Σ)ii = CaΣiaiJφ(1)
vφ(ςW = CaJ0(2j / p，ii ς jj 巨二/2夕力？
Definition E.20. Suppose φ : R → R is degree α positive-homogeneous. By Proposition E.17,
φ restricted to R \ {0} can be written as x 7→ aρa (x) - bρa (-x) for some a and b. Define
Jφ(c) := (a2 + b2)Ja (c) - 2abJa(-c).
Let us immediately make the following easy but important observations.
Proposition E.21. Jφ(1) = a2 + b2 and Jφ(0) = (a2 + b2 - 2ab)Ja(0) = (a - b)2z√∏ ：己；1)，
Proof. Use the fact that Jα(-1) = 0 and Jα(0) = z√∏ ,服+?)by Proposition E.11.	□
As a sanity check, we can easily compute that Jid(c) = 2J1(c) - 2J1(-c) = 2c because id(x) =
relu(x)-relu(-x). By Corollary E.19 and ci = 2, this recovers the obvious fact that V,d)(Σ) = Σ.
We record the partial derivatives of Vφ .
Proposition E.22. Let φ be positive homogeneous of degree α. Then for all i with Σii 6= 0,
dV『ii = Cαα∑α-1Jφ ⑴
∂Σii
For all i 6= j with Σii , Σjj 6= 0,
∂Vφ(∑)ii
∂∑ij
∂Vφ(∑)ij
∂∑ij
∂Vφ(∑)ij
∂∑n
0
ca∑(aτ"2∑jaτ"2Jφ(cij)
1 Ca∑i⅛(a-2)∑jja(αJφ(cij) - CijJφ(Cij))
where Cij = Σj / y∕∑n Σjj and Jφ denotes its derivative.
Proposition E.23. If φ(c) = aρa(c) - bρa(-c) with α > 1/2 on R \ {0}, then
J0φ(c) = (2α - 1)-1Jφ0(c)
= (2α + 1)(1 - c2 )-1 ((a2 + b2 )Ja+1 (c) + 2abJa+1 (-c)) - cJφ(c)
Jφ(0) = (a + b)2 E2 +F
√π Γ(α + 2)
26
Published as a conference paper at ICLR 2019
Proof. We have φ0(c) = aαρα-1 (c) + bαρα-1 (-c). On the other hand, J0φ = (a2 + b2)J0α (c) +
2abJ0α (-c) which by Proposition E.14 is α2 (2α - 1)-1 ((a2 + b2)Jα-1 (c) + 2abJα-1 (-c)) =
(2α - 1)-1Jφ0 (c). This proves the first equation.
With Proposition E.16,
J0φ(c) = (a2 +b2)J0α(c) + 2abJ0α(-c)
= (a2 + b2)(2α + 1)(1 - c2)-1(Jα+1(c) - cJα(c))
+ 2ab(2α + 1)(1 - c2)-1 (Jα+1 (-c) + cJα(-c))
= (2α + 1)(1 - c2)-1 ((a2 + b2)Jα+1(c) + 2abJα+1(-c)) - cJφ(c)
This gives the second equation.
Expanding Jα+1(0) With Proposition E.11, we get
J0φ(0) = (2α + 1)((a2 + b2)Jα+1 (0) + 2abJα+1 (0))
(2α + 1)(a + b)2
1 Γ( 2 + 1)2
2√∏ Γ(α + 2)
Unpacking the definition of Γ(α + 3) then yields the third equation.
□
In general, we can factor diagonal matrices out of Vφ .
Proposition E.24. For any Σ ∈ SB, D any diagonal matrix, and φ positive-homogeneous with
degree α,
Vφ(DΣD) = DαVφ(Σ)Dα
Proof. For any i,
Vφ(D∑D)ii = E[φ(χ)2 : X 〜N(0, D2i∑ii)]
=E[φ(Diiχ)2 : x 〜N(0, Σii)]
=E[D2αΦ(x)2 ： x 〜N(0, ∑ii)]
= Di2iαVφ(Σ)ii
For any i 6= j ,
Vφ(D∑D)ij = E[φ(χ)φ(y): (x, y)〜N(O(DDD∑∑lji DDDΣ∑ij))
=E[φ(Diiχ)φ(Djjy) : (χ, y)〜N(Og：i j))]
=DaDa∙ E[φ(χ)φ(y): (x, y)〜N(0, (Σji ∑jj))]
= DiαiDjαjVφ(Σ)ij
□
E.2 Fourier Method
The Laplace method crucially used the fact that we can pull out the norm factor kGhk out of φ, so
that we can apply Schwinger parametrization. For general φ this is not possible, but we can apply
27
Published as a conference paper at ICLR 2019
some wishful thinking and proceed as follows
Vbφ (∑) = E[φ(√BGh∕kGhk 产1 2 : h 〜N(0, Σ)]
∞
=E[φ(Gh∕r)02 / d(r2)δ(r2 - ||Gh『/B) : h 〜N(0, Σ)]
0
=E[φ(Gh∕r)02 Z∞ d(r2) ZCo S/(M-kGhk2IB) ： h 〜N。, ∑)]
0	-∞ 2π
dh
“ =” /	,________
JRB VZdet(2πΣ)
e -1 hT £Th0(Gh/r 产 £ d(r2) ∕∞ di2""
“ =” Z∞ d(r2) Z∞ dλ Z , dh	e-21 hTLhOGh”产eiMM - Sk2/B)
Jo	-∞ 2∏ JRB Pdet(2πΣ)	'
(29)
,1	Z∞ d(r2) Z∞ dλeiλr2 Z dhφ(Gh∕r产e-1 (hT工—+网叫2/B)
Pdet(2πΣ) Jo	-∞ 2π	RB 〜 C
1
Pdet(2πΣ)
1
(30)
「d(r2/ dλeiλr2 Z dhφ(Gh∕r产e-1 (hT3-1-7B)h)
0	-∞ 2π	RB
√det(2πΣ)
∞ d(r2) Zl dλeiλr2
Vzdet(2π(Σ-1 + 2iλG/B)T) E[φ(Gh∕r)02 : h 〜N(0, Σ-1 + 2iλG∕B)]
∕∞ d(r2) ∕∞ dλeiλr2 det(I + 2iλGΣ/B)T/2	E	φ(h产
Jo	-∞ 2∏	h〜N(0,G(∑τ + 2iλG/B)Gr-2))
(31)
L d(r2) / dλeiλr2 det(I + 2iλGΣ/B)T〃Vφ(G(Σ-1 + 2iλG∕B)Gr-2)
Here all steps are legal except for possibly Eq. (29), Eq. (30), and Eq. (31). In Eq. (29), we “Fourier
expanded” the delta function — this could possibly be justified as a principal value integral, but
this cannot work in combination with Eq. (30) where we have switched the order of integration,
integrating over h first. FUbini's theorem would not apply here because eiλ(r2-kGhk2/B) has norm
1 and is not integrable. Finally, in Eq. (31), we need to extend the definition of Gaussian to complex
covariance matrices, via complex Gaussian integration. This last point is no problem, and we will
just define
hJE0,∑) f(h)：=det(2◎T2 L dhf (h)e-1 hTi
for general complex Σ, whenever Σ is nonsingular and this integral is exists, and
ME	f (h) := det(2π(eTΣe))-1∕2/	dz f (ez)e-1 zT(®T^e) 1z
if T∑ Σe is nonsingular (as in the case above). The definition of Vφ stays the same given the above
extension of the definition of Gaussian expectation.
Nevertheless, the above derivation is not correct mathematically due to the other points. However,
its end result can be justified rigorously by carefully expressing the delta function as the limit of
mollifiers.
Theorem E.25. Let Σ be a positive-definite D × D matrix. Let φ : R → R. Suppose for any
b > a > 0,
1. Ra d(r2)∣φ(za∕r)φ(zb∕r)∣ exists and is finite for any Za,Zb ∈ R (i.e. φ(za∕r)φ(zb∕r) is
locally integrable in r).
28
Published as a conference paper at ICLR 2019
2.	there exists an > 0 such that any γ ∈ [-， ] and for each a， b ∈ [D],
/ 112e[ b]dze-1 zTς-1z∣φ(za∕PPF+^)φ(zb∕PPF+^)|
exists and is uniformly bounded by some number possibly depending on a and b.
3.	for each a，b ∈ [D], RRD dz Rab d(r2 )e- 1 zT ς 1z ∣φ(za∕r)φ(zb∕r)∣ exists and is finite.
4.	R∞∞ dλ ∣det(I + 2iλΣ)-"2Vφ((Σ-1 + 2iλI)-1r-2)∣ exists and isfinitefor each r > 0.
If e-1 zT ς 1z φ(z∕kz∣∣ 产2 is integrable over Z ∈ RD ,then
(2π)-D/2 det Σ-1/2 / dz e-1 zTςTzφ(z∕kz∣∣产
=— lim Zds Z dλ eiλs det(I + 2iλΣ)-"Vφ((Σ-1 + 2iλI)-1s-1).
2π a∖0,b/∞ Ja	J-∞
Similarly,
(2π)-D/2 det Σ-1/2 / dz e-2zTςTZφ(√Dz∕kz∣∣产
ɪ lim Z ds Z dλeiλs det(I + 2iλΣ)-V2Vφ((Σ-1 + 2iλI∕D)TsT).
2π a∖0,b%∞ Ja	J-∞
If G is the mean-centering projection matrix and ΣG = GΣG, then by the same reasoning as in
Thm E.5,
Vbφ (Σ) = (2π)-D/2 det Σ-1/2 / dz e-1 zTςTZφ(√DGz∕kGzk)02
=ɪ lim Zds Z dλeiλs det(I + 2iλΣG∕D)T^Vφ(ΣG(I + 2iλΣG∕D)TsT).
2π a∖0,b/∞ Ja	J-∞
(32)
Note that assumption (1) is satisfied if φ is continuous; assumption (4) is satisfied if D ≥ 3 and
for all Π, kVφ(Π)k ≤ kΠkα for some α ≥ 0; this latter condition, as well as assumptions (2) and
(3), will be satisfied if φ is polynomially bounded. Thus the common coordinatewise nonlinearities
ReLU, identity, tanh, etc all satisfy these assumptions.
Warning: in general, we cannot swap the order of integration as -∞∞ dλ -∞∞ ds. For example, if
φ = id, then
(2π)-D/2 det Σ-1/2 / dz e-1 zTςTzφ(z∕kz∣∣产
=— lim Zds Z dλeiλsdet(I + 2iλΣ)-"2(Σ-1 + 2iλI)-1s-1
2π a∖0,b/∞ Ja	J-∞
=3/ dλ ∞ dseiλs det(I + 2iλΣ)-"2(∑τ + 2iλI)-1s-1
2π -∞	0
because the s-integral in the latter diverges (in a neighborhood of 0).
Proof. We will only prove the first equation; the others follow similarly.
By dominated convergence,
/ dze-1ZT ςTz φ(z∕kz∣∣ 产
a&H/dze-2zTiφ(Z/3产1(32 ∈ [a，b]).
29
Published as a conference paper at ICLR 2019
Let η : R → R be a nonnegative bump function (i.e. compactly supported and smooth) with support
[-1,1] and integral 1 such that η0(θ) = η0θ(0) = 0. Then its Fourier transform η(t) decays like
O(t-2). Furthermore, ηe(x) := e-1η(x∕e) is a mollifier, i.e. for all f ∈ L1(R), f * η → f in L1
and pointwise almost everywhere.
Now, we will show that
/dze-1ZTςTzφ(za∕r)φ(zb∕r)I(kzk2 ∈ [a, b])
=lim Z dze-1 zTς 1z Za d(r2)φ(za∕r)φ(zb∕r)ηe(kzk2 - r2)
by dominated convergence. Pointwise convergence is immediate, because
Z d(r2)φ(za∕r)φ(zb∕r)η (r2 - kzk2) = Z d(r2)φ(za∕r)φ(zb∕r)I(r2 ∈ [a, b])η(kzk2 - r2)
a	-∞
= [(r2 7→ φ(za∕r)φ(zb∕r)I(r2 ∈ [a, b])) * η](kzk2)
→φ(za∕kzk)φ(zb∕kzk)I(kzk2∈[a,b])
as e → 0 (where we used assumption (1) that φ(za∕r)φ(zb∕r)I(r2 ∈ [a, b]) is L1).
Finally, we construct a dominating integrable function. Observe
Z dze-1 zTς 1z Za d(r2)∣φ(za∕r)φ(zb∕r)∣ηe(kzk2 - r2)
Z dze-1 zTςTZ Za d(r2)∣φ(za∕r)φ(zb〃)屣(||/『一r2)
Z dze- 1 zTςTZ Za d(kzk2 + Y)lφ(za∕PPF + Y J(zb∕pkzk2 + YXηe(-Y)
Z dze- 1 zT*TZ Z	dY lφ(za∕PpF + Y)φ(zb/Pkzk2 + YMe(-Y)
a-kzk2
Z	dze-1 zTς-1 Z Z	dY lφ(za∕Pkzk2 + Y)φ(zb∕Pkzk2 + Y)|ne(-Y)
kzk2∈[a-,b+]	a-kzk2
since supp η = [-e, e]
≤ (川-田尸-1 zT ς-1 Z ZF lφ(za∕p^^ "b ∕p^^)限(-')
Le Ei ∙L∈[…H"1 ZT i^/p^ )φ(zb∕pPF^ )l
For small enough e then, this is integrable by assumption (2), and yields a dominating integrable
function for our application of dominated convergence.
In summary, we have just proven that
/ dze-1 zTςTzφ(z∕kzk产
lim lim / dze-1 zTς 瞑 / d(r2)φ(za∕r)φ(zb∕r)ηe(kzk2 — r2)
a∖0,b%∞ e&0 J	Ja
30
Published as a conference paper at ICLR 2019
Now,
dze-2zTς 1z Za d(r2)φ(za∕r)φ(zb∕r)ηe(kzk2 - r2)
/
Z
1T y — ι 个
dze-2Z ς Z
b1
d(r2)Φ(za∕r)Φ(zb∕r)τr
a	2π
-1 d dze-1 zTςTZ
2π
b
d(r2)φ(za∕r)φ(zb∕r)
Z∞
dληe(λ)e-iλ(kzk2-r2)
∞
Z∞
dλη(eλ)e-iXkzk2-M)
∞
Note that the absolute value of the integral is bounded above by
21∏∕
21∏ Z
1 T y — 1 个
dze-2Z ς Z
dze-1 zT ςTZ
a
Za
d(r2)∣Φ(za∕r)Φ(zb∕r)∣
Z∞
dλ ∣η(eλ)e-iλdzk2-T∣
∞
Z∞
dλ lη(eλ)1
∞
≤ CZ dze-1 zTς 1z Za d(r2)lΦ(za∕r)Φ(zb∕r)l
for some C, by our construction of η that η(t) = O(t-2) for large |t|. By assumption (3), this
integral exists. Therefore we can apply the Fubini-Tonelli theorem and swap order of integration.
2∏ Z dze-1 zTς-1Z J： d(r2)φ(za∕r)φ(zb∕r) Z∞ dλ η(eλ)e-iMkzk2-M)
2∏ J： d(r2) Z∞ dλη(eλ)e-iλ(kzk2-r2) Z dzφ(za∕r)φ(zb∕r)e-2zTςTZ
b∞
2∏ J d(r2) J	dλη(eλ)eiλr2 J dzφ(za∕r)φ(zb∕r)e-2ZT3 ι+2iλI)z
2∏ Za d(r2)rD Z dλ η(eλ)eiλr2 Z dzφ(zɑ)φ(zb)e-1 r―TF1+2"1)%
(2π)D∕2τ detΣ1∕2 广 d(r2) Z∞ dλη(eλ)eiλr2 det(I +2iλΣ)-1/2Vφ((∑-1 +2iλI)-1r-2)
a	-∞
By assumption (4),
Zabd(r2) Z∞ dλ ∣η(eλ)eiλr2 det(I + 2iλΣ)T∕2Vφ((Σ-1 + 2iλI)-1r-2)∣
≤ CZa d(r2) Z dλ ∣det(I + 2iλΣ)T∕2Vφ((Σ-1 + 2iλI)-1r-2)∣
is integrable (where We used the fact that η is bounded — which is true for all η ∈ C∞(R)), so by
dominated convergence (applied to & 0),
/ dze-1 zTςTZφ(z∕kz∣∣产
lim lim /dze-1 zTς 叱 / d(r2)φ(za∕r)φ(zb∕r)ηe(kzk2 — r2)
a∖0,b%∞ e&0j	Ja
(2π)D∕2-1 det Σ1∕2	lim	Zbd(r2)Z∞ dλ eiλr2 det(I + 2iλΣ)-1∕2Vφ((Σ-1 + 2iλI)-1r-2)
a∖0,b%∞ Ja	J-∞
where we used the fact that η(0) = JR η(χ) dx = 1 by construction. This gives US the desired result
after putting back in some constants and changing r2 7→ s.
□
31
Published as a conference paper at ICLR 2019
E.3 Spherical Integration
As mentioned above, the scale invariance of batchnorm combined with the BSB1 fixed point as-
sumption will naturally lead us to consider spherical integration. Here we review several basic facts.
Definition E.26. We define the spherical angular coordinates in RB, (r, θ1 , . . . , θB-2) ∈ [0, ∞) ×
A =: [0, π]B-3 × [0, 2π], by
x1 = r cos θ1
x2 = r sin θ1 cos θ2
x3 = r sin θ1 sin θ2 cos θ3
xb-2 = r Sin θι …Sin Θb-3 cos Θb-2
xb-ι = r sin θι …sin Θb-3 sin Θb-2.
Then we set
/	cos θι	∖
sin θι cos θ2
sin θι sin θ2 cos θ3
V ：=	.
.
.
sin θι …sin θB-3 cos θB-2
∖sin θι ∙∙∙ sin θB-3 sin θB-2)
as the unit norm version of x. The integration element in this coordinate satisfies
dxι... dxB-1 = rB-2 SinB-3 θι sinB-4 θ2 …sin θB-3 dr dθι …dθB-2.
Fact E.27. If F ： SB-2 → R, then its expectation on the sphere can be written as
E	F(V) = 2π 12BΓ (22 1) / F(V) SinB-3 θι …sinθB-3 dθι …dθB-2
where A = [0, π]B-3 × [0, 2π] and v are as in Defn E.26.
The following integrals will be helpful for simplifying many expressions involving spherical inte-
gration using angular coordinates.
Lemma E.28. For j, k ≥ 0 and 0 ≤ S ≤ t ≤ π∕2,
Z	sinj θ cosk θ dθ	=J Beta	(cos2 t, cos2 s;	ʌɪ + ɪ, j + ɪ ].
JS	2	,	; 2 , 2 )
By antisymmetry of cos with respect to θ → π 一 θ, if π∕2 ≤ S ≤ t ≤ π,
Z sinj θcosk θ dθ = 2(-1)k Beta (cos2 s, cos2 t; >2 , jɪɪ).
Proof. Set x ：= cos2 θ =⇒ dx = 一2 cosθ sin θ dθ. So the integral in question is
-1 ∕c°s2 1	q j
--	(1 - x) 2 X 2 dx
2	cos2 s
1	2 k+1 j+1	2 k+1 j+1
=2(Beta(Cos2 s； —2~, —2~)- Beta(Cos2 t； —2-, ~1~))
□
As consequences,
Lemma E.29. For j, k ≥ 0,
∕∏ sinj θ cosk θdθ = 1 + (-1)k Beta(j+1, k+1)
0	2	22
1 + (-1)k Γ( 1+j)Γ( 1+k)
Γ( 2+j+k)
2
32
Published as a conference paper at ICLR 2019
Lemma E.30.
S Sinθι dθι ( sin2 θ2 dθ2 …/ Sink θk dθk = ∏kS∕Γ(> + 2).
00	0	2
Proof. By Lemma E.29, R∏ sinr θr dθr = Beta(r++1, 2) = r(^2+2√π. Thus this product of inte-
γ( ~2)
grals is equal to
YY √Γfg) = MΓ(1)∕Γ(k+2) = πk∕2∕Γ(k+2).
r=1	2
□
Lemma E.31. For B > 1,μ > 0, R∞ rBe-r2/2* dr = 2B-IμB2+1 Γ(B2+1)
Proof. Apply change of coordinates Z = r2∕2μ.
□
Definition E.32. Let K(v; Σ, β) := (det T∑Σe)-1/2(vτ(eτΣe)-1v)-B 1+β and K(v; Σ):=
K(v; Σ, 0).
The following proposition allows us to express integrals involving batchnorm as expectations over
the uniform spherical measure.
Proposition E.33. Iff : RB-1 → RB0, B > 1 - β, and Σ ∈ R(B-1)×(B-1) is nonsingular, then
E
h 〜N (0,Σ)
khkβ f(h/khk) = 2e/2P
B-ɪ ,力	E f(v)(detΣ)-"(VT Σ-1v)-B-2+β
2	2 V V〜SB-2
If f : RB → RB0 and Σ ∈ Rb×b is such that T∑Σe is nonsingular (where e is as in Defn E.3) and
GΣG = Σ,
E	kGhkβf (Gh/kGhk) = 2e/2P (一, J) E f V)K(v； ∑, β).
h〜N(0,Σ)	2	2 J V〜SB-2
In particular,
E[f (B(h)) : h 〜N(0, Σ)] = E	f(ev)K(v; Σ)
V〜SB-2
Proof. We will prove the second statement. The others follow trivially.
33
Published as a conference paper at ICLR 2019
E[∣∣Gh∣∣βf (B(h)) : h 〜N(0, Σ)]
E[∣∣χ∣∣βf (eχ∕kχ∣∣) : X 〜N(0, (BTΣe)]
(2π)⅛B(det (BTΣe)-1/2 /	∣∣x∣∣βf (©x∕∣∣x∣∣)e-2XT((ETς(E)TX dx
RBT
(2π) 12B (det ttΣe)-1/2	f (ev) SinB-3 &…sin Θb-3 dr dθι …dθB-2 ×
A
/	rB-2 + β £-2 r2-vτ (® T Σe)-1v
Jo
(spherical coordinates Defn E.26)
, .1 — B	B — 3
(2π)F (det TτΣe)T∕2 X 2FΓ
2，Γ( B-+β)
× ^3TWr
×
/ f (ev) SinB-3 θι ∙∙∙ sin Θb-3 dθι …dθB-2
JA	(VT (e T Σe)-1 V) B-产
2π中Γ	) × 2β∕2r (B -1+ β) Γ (丁
/ f (©v) SinB-3 θι ∙ ∙ ∙ sin Θb-3 dθι ∙ ∙ ∙ dθB-2
X JA	(VT(eTΣe)-1 V)B-+β
2β∕2Γ
B-1+β)γ
(B -1 y1 E _____________________f(®V)______________
V 2 J v~sB—2 (det(ETΣe)i/2(VT(©TΣ(b)-1v)B-2+β
2e/2r (B [ + β) Γ (M)1 E f v)K(v5Σ,β)
∖	2	)	∖	2	) V 〜SB — 2
□
As a straightforward consequence,
Proposition E.34. If tt Σe is nonsingular
V3 (Σ)= E	φ(√Bev 产 2K0Σ)
V〜SB—2
For numerical calculations, it is useful to realize e as the matrix whose columns are @B—m
m
(Im(Im + 1))-1∕2(0,..., 0,—m, 1,1,..., 1)τ, for each m = 1,...,B — 1.
/
1
√B(B-1)
∖

T
1
6-1/2
∖
/-(B - 1)	1	1 ∙∙∙
0	-(B - 2) 1 ∙∙∙
7	.	...
×	.	...
.	...
0	0	0 ∙∙∙
0	0	0 ∙∙∙
2-1/2
11
11
-2	1
0	-1
1∖
1
.
.
.
1
1
(33)
34
Published as a conference paper at ICLR 2019
We have
(e v)i
cos θ1
sin θ1 cos θ2
(e v)2 = cos θι —
B—3
(ev)3 = cos θι + Sin θι CoS θ2 — ∖ --Sm θι Sm θ2 CoS θ3
B—2
B-4
(ev)4 = CoS θι + Sin θι CoS θ2 + Sin θι Sin θ2 CoS θ3 — ∖ ———-Sin θι Sin θ2 Sin θ3 CoS θ4 (34)
B-3
so in particular they only depend on θ1 , . . . , θ4 .
Angular coordinates In many situations, the sparsity of this realization of e combined with SUffi-
cient symmetry allows us to simplify the high-dimensional spherical integral into only a few dimen-
sions, using one of the following lemmas
Lemma E.35. Let f : R4 → RA for some A ∈ N. Suppose B ≥ 6. Then for any k < B — 1
E[r-kf (v1,...,v4): X 〜N(0,Ib-i)]
_ γ((B - 1 - k)/2)2-k/2 -2 /π dθ	/π dθ	θ	θ∖ i B-3 θ i B-6 θ
——Γ((B - 5)/2)-2	π J dθ1 …J dθ4 f (v1 ,∙∙∙,v4)Sin	θ1 ∙∙∙ Sin	θ4
where vi = xi/kxk and r = kxk, and
v1θ = CoS θ1
v2θ = Sin θ1 CoS θ2
v3θ = Sin θ1 Sin θ2 CoS θ3
v4θ = Sin θ1 Sin θ2 Sin θ3 CoS θ4 .
Lemma E.36. Let f : R2 → RA for some A ∈ N. Suppose B ≥ 4. Then for any k < B — 1
E[r-kf (v1,v2) ： x 〜N(0,Ib-i)]
=r，B - 1 - k)/2)2-k/2n-1 Z dθ1 Z dθ2 f (vθ,vθ)sinB-3 θ1 SinB-4 θ2
Γ((B - 3)/2)	Jo 1 Jo	f ( 1, 2)	1	2
where vi = xi/kxk and r = kxk, and
v1θ = CoS θ1
v2θ = Sin θ1 CoS θ2
Lemma E.37. Let f : R → RA for some A ∈ N. Suppose B ≥ 3. Then for any k < B — 1
E[r-kf (v) ： x 〜N(0,Ib-i)]
=γ((B - 1 - k)/2) 2-k∕2π-1∕2 fπ dθ f (CoS θ)sinB-3 θ
=Γ((B - 2)/2) 2 π J0 dθf (CoS θ)sin	θ
where vi = xi/kxk and r = kxk.
We will prove Lemma E.35; those of Lemma E.36 and Lemma E.37 are similar.
35
Published as a conference paper at ICLR 2019
Proof of Lemma E.35.
E[r-kf (v1,...,v4): X 〜N(0,Ib-i)]
=(2π)-(B-I)/2 Zdx r-k f(vι,..., V4)e-r2/2
=(2π)-(B-I)/2 Z (rB-2 sinB-3 θι …sinΘb- dr dθι …dθB-2) r-kf (vθ,..., vθ)e-r2/2
∞
=(2π)-(B-I)/2 J f (vθ,..., vθ) sinB-3 θι …sin Θb-3 dθι …dθB-2 J rB-2-k e-r /2 dr
=(2π)-(BT)/2 / f (vθ,…,vθ) SinB-3 θ1 …sin θB-3 dθ1 …dθB-22(B-3-k"2Γ((B - 1 一 k)/2)
(change of coordinate r J r2 and definition of Γ function)
Γ((B — 1 — k)/2)2-(2+k)/2n-(BT)/2 / f (vθ, ...,v4) sinB-3 θ1 …Sin θB-3 dθ1 …dθB-2
Because V1θ, . . . , V4θ only depends on θ1, . . . , θ4, we can integrate out θ5, . . . , θB-2. By applying
Lemma E.30 to θ5 , . . . θB-3 and noting that R02π dθB-2 = 2π, we get
E[r-kf(vθ,…,v4) ： x 〜N(0,Ib-i)]
=Γ((B — 1 — k)/2)2-(2+k)/2n-(BT)/2 / f (vθ,…，v4) SinB-3 θ1 …sinB-6 θ4π(B-7"2Γ((B — 5)/2)-1 (2π)
=% J —：)/2) 2-k∕2∏-2 Z f (vθ,…，v4) SinB-3 θι …SinB-6。4
—
□
Cartesian coordinates. We can often also simplify the high dimensional spherical integrals with-
out trigonometry, as the next two lemmas show. Both are proved easily using change of coordinates.
Lemma E.38. For any u ∈ Sn,
1
E f(u ∙ V) = ^n-ɪ	dh (1-h2)ɪf(h)
v~sn	ωn	-i
where ωn-ι is hypersurface ofthe (n — 1)-dimensional unit hypersphere.
Lemma E.39. For any u1 , u2 ∈ Sn ,
E f (uι ∙ V,U2 ∙ V)
V〜Sn
ωn-2 I kvk≤1	dv (I — kvk2) n-3 f (U1 ∙ v,u2 ∙ V)
ωn	kvk≤1
v∈span(u1,u2 )
where ωn-1 is hypersurface of the (n - 1)-dimensional unit hypersphere.
E.4 Gegenbauer Expansion
Definition E.40. The Gegenbauer polynomials {Cl(α)(x)}l∞=0, with deg Cl(α)(x) = l, are the
set of orthogonal polynomials with respect to the weight function (1 一 χ2)α-1/2 (i.e. R-I(I 一
x2)α-1∕2Clg)(X)Cma)(X) = 0 if l = m). By convention, they are normalized so that
∕1 hCna)(X)i 2 (1-x2)α-1 dx = UUn+ 2α).
-1	n!(n + α)Γ(α)2
Here are several examples of low degree Gegenbauer polynomials
C0(α) (x) = 1
C1(α) (x) = 2αx
C2(α) (x) = -α + 2α(1 + α)x2
C∖α)(x) = —2α(1 + α)x + 3 α(1 + α)(2 + α)x3.
36
Published as a conference paper at ICLR 2019
They satisfy a few identities summarized below
FactE.41 (Suetin). Cna) (±1) = (±1严广+二-1)
Fact E.42 (Weisstein).
Cn(λ)0(x) = 2λCn(λ-+11)(x)	(35)
xCn(λ)0(x) =nCn(λ)(x)+Cn(λ-)10(x)	(36)
Cn(λ+)10(x) = (n + 2λ)Cn(λ)(x) + xCn(λ)0(x)	(37)
nCn(λ)(x) = 2(n- 1 +λ)xCn(λ-)1(x) - (n - 2 + 2λ)Cn(λ-)2(x), ∀n ≥ 2	(38)
By repeated applications of Eq. (38),
Proposition E.43.
XCnl)(X) = 2(n+λ)((n+I)Cn+1(X)+(n—1+2*Cnλ)ι(X)), ∀n ≥ 1
x2Cn(λ)(x) = κ2(n, λ)Cn(λ+)2(x) + κ0(n, λ)Cn(λ)(x) + κ-2 (n, λ) Cn(λ-)2(x),∀n ≥ 2
XCOl)(X) =X = (C(I)(X)
x2C0λ)(x) = 4λ(11+ λ) (2C(λ)(X)+2λC0λ)(X))
= κ2(0,λ)C2(l)(X)+κ0(0,λ)C0(l)(X)
X2C1(l)(X) = κ2(1,λ)C3(l)(X) +κ0(1,λ)C1(l)(X)
(n+1)(n + 2)	(n+1)(n+2l)	I	(n—1 + 2l)n
where κ2(n, λ) := 4(n+λ)(n+1+λ) ,κ0(n,λ) := 4(n+λ)(n+1+λ) + 4(n+λ)(n-1+λ)，κ-2(n, λ)」
(n-1+2λ)(n-2+2λ)
4(n+λ)(n- 1+λ).
This proposition is useful for the Gegenbauer expansion of the local convergence rate of Eq. (43).
Zonal harmonics. The Gegenbauer polynomials are closely related to a special type of spherical
harmonics called zonal harmonics, which are intuitively those harmonics that only depend on the
“height” of the point along some fixed axis.
Definition E.44. Let ZuN-1,(l) denote the degree l zonal spherical harmonics on the (N - 1)-
dimensional sphere with axis u ∈ S N-1, defined by ZN-1,(I)(V) = CNIlCl ("r~ )(u ∙ V) for any
u,v ∈ SN-1, where CN,l = NN-l-.
The first few zonal spherical harmonics are ZN-1,(O)(V) = 1, ZN-1,(I)(V) = Nu ∙ v,.... We note
a trivial lemma that is useful for simplying many expressions in Gegenbauer expansions.
LemmaE.45. cB+1,l-1 = B-1.
cB-1,l	B-3
Let h-, -iSN denote the inner product induced by the uniform measure on SN . One of the most
important properties of zonal harmonics is their
Fact E.46 (Reproducing property (Suetin)).
hZN T(I) ,ZN TPSN-I= ZN-1, (I)(V) = CNl C N-2) (u ∙ v)
hZuN-1,(l),ZvN-1,(m)iSN-1 =0,ifl 6=m.
In our applications of the reproducing property, U and V will always be ^。,：:= @。,：/||e。,』=
@。,： ʌ/B-ɪ, where e。,： is the ath row of e.
Next, we present a new (to the best of our knowledge) identity showing that a certain quadratic
form of φ, reminiscent of Dirichlet forms, depending only on the derivative φ0 through a spherical
integral, is diagonalized in the Gegenbauer basis. This will be crucial to proving the necessity of
gradient explosion under the BSB1 fixed point assumption.
37
Published as a conference paper at ICLR 2019
Theorem E.47 (Dirichlet-like form of φ diagonalizes over Gegenbauer basis). Let u1 , u2 ∈ SB -2 .
Suppose φ(√B - 1 x),φ0(√B - 1x) ∈ L2((1 - x2)~^-~) and φ(√B - 1x) has Gegenbauer ex-
pansion P∞=o aicB 1ι ι C( 2 )(x). Then
E	(uι ∙ U2 — (uι ∙ v)(u2 ∙ v))φ0( VB-IU1 ∙ v)φ0( VB- 1U2 ∙ v)
v∈SB-2
∞
^Xa α2(l + B — 3)lcB-ι C 2 )(uι ∙ u2)
l=0
This is proved using the following lemmas.
Lemma E.48.
xCl(α) 0 (x) = lCl(α) (x) + X 2(j + α)Cj(α) (x)
j∈[l-2]2
Cl(α)0(x) = X 2(j + α)Cj(α)(x)
j∈[l-1]2
where [n]2 is the sequence {(n%2), (n%2) + 1, . . . , n — 2, n} and n%2 is the remainder of n/2.
Proof. Apply Eq. (36) and Eq. (37) alternatingly.
□
Lemma E.49.
lCl(α)(t) + X 2(j + α)Cj(α)(t) — t X 2(j + α)Cj(α)(t) = 0
j∈[l-2]2
j∈[l-1]2
Proof. Apply Eq. (38) to lCl(α) (t), (l - 2)Cl(-α2) (t), ................
□
Proof of Thm E.47. We have
φ0(√B -1 x) = √==j=
B-1
1
√B-1
1
√B-1
∞
X
l=0
∞
X
l=0
∞
1	B-3 ∖
al-------C( F) 0(x)
cB-1,l
aleɪ	X ⑵+ b - 3)C产)(x)
cB-1,l j∈[l-1]2
1
√B-1
X X al cB-j⑵ + B - 3)CB-1 jCjB-3)(x)
l=0 j∈[l-1]2	cB-1,l	,
∞
X c--ιjCjɪ)(x)	X	al∣B-j-⑵ + B - 3)
j=0	l∈[j+1,∞]2	- ,
where [n, m]2 is the sequence n, n + 2, . . . , m. So for any u1, u2 ∈ SB-2,
E	φ0(7B — 1uι ∙ v)φ0(7B — 1U2 ∙ V)
v∈SB-2
B⅛ X CB-ι,jCjB-3)(U1∙ U2) I X	aι CB-j⑵ +B - 3)
j=0	l∈[j+1,∞]2	B-1,l
2
∞	B-3
B - 1 X CBTj(2j + B - 3) Cj 2	(U1 ∙ u2) I X
j=0	l,m∈[j+1,∞]2
al	am
cB-1,l cB-1,m
38
Published as a conference paper at ICLR 2019
For l < m and m — l even, the coefficient of aι am is
2cB-i,lcB-i,m
B — 1
Σ
j∈[l-1]2
B — 3 ∖
cB-1,j (2j + B — 3) Cj 2	(uι ∙ u2)
2CB『：-1，μ	X (B — 3)(2j + B — 3)cjB-)(uι∙ U2).
—	j∈[l-1]2
For each l, the coefficient of al2 is
-2
H X CB-i,j⑵+ B — 3)2C(年U2)
j∈[l-1]2
c-2
=BB-1 X (B — 3)(2j + B — 3)Cjɪ)(U1∙ U2).
j∈[l-1]2
(39)
(40)
Similarly, if φ0(x) ∈ L2((1 — x2)B-), we have
xφ0(√ B — 1 x)
∞
X αl
l=0
1
CB-1,l
1
√B—
√1^ X a - iclb-)(x)+ X ⑵ + B — 3)cj煤)(x)
√B-I l=0 CBT，l	j∈[l-2]2
∞
—X -T "ajj + l∈jX∞]2
al C^ ⑵+B - 3)
Then
E	(uι ∙ v)(u2 ∙ v)φ0(√B — 1uι ∙ v)φ0 (√B — 1 u2 ∙ V)
v∈SB—2
∞
五----7 X cB-1 jCj	2 )(u1	∙	u2)	(	ajj	+ X	al	B —	(2j	+ B - 3)
B — 1M ，	l∈jT∞]2	CBT，l
2
For l < m and m — l even, the coefficient of al am is
£ "2l + B 一 3)二 CB-1,C 吩)(u1 ∙ W
+2j∈X2]2 C≡⅛ I ⑵ + B - 3)2CBJ(" M 〃2)
2CB-1,lCB-1,mB-3 (lC(ɪ)(uru2)+ X ⑵ + B — 3)cjɪ)(u1 ∙犯)
j∈[l-2]2
(41)
For each l, the coefficient of al2 is
B-I 卜 B-ι,l C ɪ )(uι∙ u2)+	X	( C-j )2 ⑵ + B — 3)2C--ι,j Cj B^ )(uι∙ u2)
Cb-1,l∣z4 (力「；	RCɪ)(uι∙ u2)+ X ⑵ + B — 3)CjB-)(uι∙ u2) I (42)
,B — 1 2 2l + B — 3	J	/
j∈[l-2]2
By Lemma E.49 and Eqs. (39) and (41), the coefficient of a am with m > l in Ev∈sb-2 (uι ∙u2 一 (u「
v)(u2 ∙ v))φ0 (√B — 1 u1 ∙ v)φ0 (√B — 1 u2 ∙v) is 0. Similarly, by LemmaE.49 and Eqs. (40) and (42),
B — 3	B — 3 ∖
the coefficient of a2 is c--1 i——f2l⅛--⅜l0； 2 (uι ∙ u2) = c-- J+--3lCl 2 (u1 ∙ u2). □
39
Published as a conference paper at ICLR 2019
E.5 Symmetry and Ultrasymmetry
As remarked before, all commonly used nonlinearities ReLU, tanh, and so on induce the dynamics
Eq. (43) to converge to BSB1 fixed points, which we formally define as follows.
Definition E.50. We say a matrix Σ ∈ SB is BSB1 (short for “1-Block Symmetry Breaking”) if Σ
has one common entry on the diagonal and one common entry on the off-diagonal, i.e.
/a b b …∖
b a b …
b b a …
I ..... '
We will denote such a matrix as BSB1(a, b). Note that BSB1(a, b) can be written as (a-b)I +b11T.
Its spectrum is given below.
Lemma E.51. A B X B matrix oftheform μI + V 11t has two eigenvalues μ and BV + μ, each with
eigenSpaces {x : Ei Xi = 0} and {x : xι = .一 = XB}. Equivalently, if it has a on the diagonal
and b on the off-diagonal, then the eigenvalues are a - b and (B - 1)b + a.
The following simple lemma will also be very useful
Lemma E.52. Let Σ := BSB1B (a, b). Then GΣG = (a - b)G.
Proof. G and BSB1B (a, b) can be simultaneously diagonalized by Lemma E.51. Note that G zeros
out the eigenspace R1, and is identity on its orthogonal complement. The result then follows from
easy computations.	□
The symmetry of BSB1 covariance matrices, especially the fact that they are isotropic in a codimen-
sion 1 subspace, is crucial to much of our analysis.
Ultrasymmetry. Indeed, as mentioned before, when investigating the asymptotics of the forward
dVB
or backward dynamics, We encounter 4-tensor objects, such as -^∑φ or NB，$, that would Seem
daunting to analyze at first glance. However, under the BSB1 fixed point assumption, these 4-
tensors contain considerable symmetry that somehow makes this possible. We formalize the notion
of symmetry arising from such scenarios:
Definition E.53. Let T : RB×B → RB ×B be a linear operator. Let δi ∈ RB be the vector with 0
everywhere except 1 in coordinate i; then δi δjT is the matrix with 0 everywhere except 1 in position
(i,j). Write [kl|ij] := T (δiδjT)kl . Suppose T has the property that for all i, j, k, l ∈ [B] =
{1,...,B}
•	[kl∣ij] = [∏(k)∏(l)∣∏(i)∏(j)] for all permutation π on [B], and
•	[ij|kl] = [ji|lk].
Then we say T is ultrasymmetric.
Remark E.54. In what follows, we will often “normalize” the representation “[ij|kl]” to the unique
“[i0j0|k0l0]” that is in the same equivalence class according to Defn E.53 and such that i0, j0, k0, l0 ∈
[4] and i0 ≤ j0, k0 ≤ l0 unless i0 = l0, j0 = k0, in which case the normalization is [12|21]. Explicitly,
we have the following equivalence classes and their normalized representations
We will study the eigendecomposition of an ultrasymmetric T as well as the projection G02 ◦T ◦
Ge)2 : Σ → G(T{GΣG})G, respectively with the following domains
Definition E.55. Denote by HB the space of symmetric matrices of dimension B . Also write HBG
for the space of symmetric matrices Σ of dimension B such that GΣG = Σ (which is equivalent to
saying rows of Σ sum up to 0).
As in the case ofS, we omit subscript B when it’s clear from context.
The following subspaces of HB and HG will turn out to comprise the eigenspaces of G02 ◦T◦ G02.
40
Published as a conference paper at ICLR 2019
class	repr.
i = j = k = l	[11|11]
i = j = k or i = j =	l [11|12]
i = j and k = l	[11|22]
i=j	[11|23]
i = k = l or j = k	l [12|11]
i = k and j = l	[12|12]
i = k or i = l	[12|13]
i = l and j = k	[12|21]
k=l	[12|33]
all different	[12|34]
Definition E.56. Let LB := {GDG : D diagonal, tr D = 0} ⊆ HBG and MB := {Σ ∈ HBG :
DiagΣ = 0}. Note that dimLB = B - 1, dimMB = B(J|-3) and RGB ㊉ LB ㊉ MB = HB is an
orthogonal decomposition w.r.t Frobenius inner product.
For the eigenspaces of T, we also need to define
Definition E.57. For any nonzero a, b ∈ R, set
/ a 0	-b -b ∙ ∙ .∖
0	-a b b ...
Lb (a,b) ：=	-	Ib	0	0	…∈ Hb -b	b	0	0	…
.	.	.	.. ..	..	..	..	..
In general, we say a matrix M is LB (a, b)-shaped if M = PLB (a, b)PT for some permutation
matrix P .
Note that
Proposition E.58. L(B - 2, 1)-shaped matrices span LB.
Proposition E.59. L(B - 2,1) = B∙[(-B + 1,1,1,..., 1产2 - (1, -B + 1,1,..., 1产2]=
BG[(1,0,0,...,0产2 - (0,1,0,...,0产2]G
Proof. Straightforward computation.	□
LemmaE.60. Let L = LB (a,b). Then G02{L} = GLG = a+B2bLB(B - 2,1).
Proof. GLG can be written as the sum of outer products
B
GLG = aG1 X G1 - aG2 X G2 + X -bG1 X Gi + bG2 X Gi - bGi X G1 + bGi X G2
i=3
(B - 1Y 1 B
V B - - B2 ,B2
aLB
B
+ bX(-δ1 +δ2) XGi +Gi X (-δ1 +δ2)
i=3
aLB(B - 2,1) + b((-δι + δ2) X V + V X (-δι + δ2))
B
with V
B-2
(-^B-
B - 2 2 2
B ,B,B,..
HLB(B - 2, I) + 微LB(B - 2,1)
BB
a+2bLB (B - 2,1)
B
□
We are now ready to discuss the eigendecomposition of ultrasymmetric operators.
41
Published as a conference paper at ICLR 2019
Theorem E.61 (Eigendecomposition of an ultrasymmetric operator). Let T : RB×B → RB ×B be
an ultrasymmetric linear operator. Then T has the following eigendecomposition.
1.	Two 1-dimensional eigenspace R ∙ BSB1(λTsBi % — α22,α21) with eigenvalue λTsBi % for
i = 1, 2, where
α11 = [11|11] + [11|22](B — 1)
α12 = 2(B — 1)[11|12] + (B — 2)(B — 1)[11|23]
α21 = 2[12|11] + (B — 2)[12|33]
α22 = [12|12] + 4(B — 2)[12|13] + [12|21] + (B — 2)(B — 3)[12|34]
and λTBSB1,1 and λTBSB1,2 are the roots to the quadratic
x2 — (α11 + α22)x + α11α22 — α12α21.
2.	Two (B — 1)-dimensional eigenspaces SB ∙ L(λT, — β22,β21) with eigenvalue λT. for
i = 1,2. Here SB ∙ W denotes the linear span ofthe orbit ofmatrix W under simultaneous
permutation of its column and rows (by the same permutation), and
β11 = [11|11] — [11|22]
β12 = 2(B — 2)([11|23] — [11|12])
β21 = —[12|11] + [12|33]
β22 = [12|21] + [12|12] +2(B — 4)[12|13] — 2(B — 3)[12|34].
and λT 1 and λT 2 are the roots to the quadratic
x2 — (β11 + β22)x + β11β22 — β12 β21.
3.	Eigenspace M (dimension B(B — 3)/2) with eigenvalue λTM := [12|12] + [12|21] —
4[12|13] + 2[12|34].
The proof is by careful, but ultimately straightforward, computation.
Proof. We will use the bracket notation of Defn E.53 to denote entries of T, and implicitly simplify
it according to Remark E.54.
Item 1. Let U ∈ RB×B be the BSB1 matrix. By ultrasymmetry of T and BSB1 symmetry of A,
T{U} is also BSB1. So we proceed to calculate the diagonal and off-diagonal entries of T {G}.
We have
T {BSB1(a, b)}11 = [11|11]a + 2(B — 1)[11|12]b + [11|22](B — 1)a + [11|23](B — 2)(B — 1)b
= ([11|11] + [11|22](B — 1))a + (2(B — 1)[11|12] + (B — 2)(B — 1)[11|23])b
T {BSB1(a, b)}12 = [12|12]b + 2[12|11]a + (B — 2)[12|33]a + 2(B — 2)[12|13]b
+ [12|21]b + 2(B — 2)[12|23]b + (B — 2)(B — 3)[12|34]b
= (2[12|11] + (B — 2)[12|33])a
+ ([12|12] +2(B — 2)[12|13] + [12|21] +2(B — 2)[12|13] + (B — 2)(B — 3)[12|34])b
= (2[12|11] + (B — 2)[12|33])a
+ ([12|12] +4(B — 2)[12|13] + [12|21] +(B — 2)(B — 3)[12|34])b
Thus BSB1(ω1, γ1) and BSB1(ω2, γ2) are the eigenmatrices of T, where (ω1, γ1) and (ω2, γ2) are
the eigenvectors of the matrix
α11 α12
α21 α22
42
Published as a conference paper at ICLR 2019
with	α11 = [11∣11] + [11∣22](B - 1) α12 = 2(B - 1)[11∣12] + (B - 2)(B - 1)[11∣23] α21 = 2[12∣11] + (B - 2)[12∣33] a22 = [12∣12]+ 4(B - 2)[12∣13] + [12∣21] + (B - 2)(B - 3)[12∣34]
The e	igenvalues are the two roots λTsBi 1，Msbi 2 to the quadratic x2 — (αi1 + α22)x +。11。22 — αi2a21
and th	e corresponding eigenvectors are (ω1,γ1) = (λ1 - α22, α21) (ω2,γ2) = (λ2 - α22, α21).
Item Z T{L(	.We will study the image of LB (a, b) (Defn E.57) under T. We have 0,b)}11 = -T{L(α,b)}22 =[11∣11]α + 2(B - 2)[11∣12](-b) + [11∣22](-α) + 2(B - 2)[11∣23]b =([11∣11] - [11∣22])α + 2(B - 2)([11∣23] - [11∣12])b
T{L(	α,b)>12 = T{L(α,b)}21 =[12∣12]0 + [12∣21]0 + [12∣11](α - a) + [12∣13](b - b) + [12∣31](b - b) + [12∣33]0 + [12∣34]0 =0
T{L(	α,b)>33 = T{L(a,b)%, Vi ≥ 3 =[11∣11]0 + [11∣12](2b - 2b) + [11∣22](a - a) + [11∣23](2(B - 3)b - 2(B - 3)b) =0
T{L(	a,b)}34 = T{L(a,b)}j, Vi= j & i,j ≥ 3 =[12∣12]0 + [12∣11]0 + [12∣13](2b - 2b) + [12∣21]0 + [12∣31](2b - 2b) + [12∣33](a - a) + [12∣34](2(B - 4)b - 2(B - 4)b) 0 =0
T{L(	a, b)}13 = T{L(a,b)}j, Vj ≥ 3 =T{L(a, b)}j1, Vj ≥ 3 =[12∣12](-b) + [12∣13](B - 3 - 1)(-b) + [12∣11]a + [12∣21](-b) + [12∣31](B - 3 - 1)(-b) + [12∣33](-a) + [12∣34]2(B - 3)b =([12∣11] - [12∣33])a + (2(B - 3)[12∣34] - 2(B - 4)[12∣13] - [12∣12] - [12∣21])b
Thus	L(a, b) transforms under T by the matrix L- Ld21 —
with	β11 = [11∣11] - [11∣22] β12 = 2(B - 2)([11∣23] - [11∣12]) β21 = -[12∣11] + [12∣33] β22 = [12∣21] + [12∣12]+ 2(B - 4)[12∣13] - 2(B - 3)[12∣34].
So if.	λT1 and λT 2 are the roots of the equation x2 -(e11 + β22)x + e11尸22 -e12尸21
then	T{L(λT1 -e22,e21)} = λT1L(λT1 -e22,尸21) T{L(，T2 -e22,e21)} = λT 2L(λL 2 -e22,尸21)
43
Published as a conference paper at ICLR 2019
Similarly, any image of these eigenvectors under simultaneous permutation of rows and columns
remains eigenvectors with the same eigenvalue. This derives Item 2.
Item 3. Let M ∈ M. We first show that T{M} has zero diagonal. We have
B	BB
T {M}11 = [11|12](X M1i + Mi1) + [11|23]( X Mij- XM1i+Mi1 )
i=2	i,j =1	i=2
=0+0=0
which follows from M1 = 0 by definition of M. Similarly T{M}ii = 0 for all i.
Now we show that M is an eigenmatrix.
T{M}12 = [12|12]M12 + [12|11]0 + [12|33]0 + [12|13] XM1i+XMi2
i=3	i=1
+ [12|21]M21 + [12|31] XB Mi1+XB M2i + [12|34] X Mij
i=3	i=1	i≥3,j≥3
B
= [12|12]M12 -2M12[12|13] +M21[12|21] + [12|31](-2M12) + [12|34](X -Mi1 -M1i)
i=3
= M12([12|12] - 2[12|13] + [12|21] - 2[12|31] + 2[12|34])
= M12([12|12] + [12|21] - 4[12|13] + 2[12|34])
= λTMM12
Similarly T{M}ij = λTMMij for all i 6= j.
□
Note that the eigenspaces described above are in general not orthogonal under trace inner product,
so T is not self-adjoint (relative to the trace inner product) typically. However, as we see next, after
projection by G02, it is self-adjoint.
Theorem E.62 (Eigendecomposition of a projected ultrasymmetric operator). Let T : RB ×B →
RB×B be an ultrasymmetric linear operator. We write G02 ◦ T HBG for the operator HBG →
HBG, Σ 7→ T{Σ} 7→ G(T {Σ})G, restricted to Σ ∈ HBG. Then G02 ◦ T HBG has the following
eigendecomposition.
1.	Eigenspace RG with eigenvalue λGG,T := B-1 ((B - 1)(α11 - α21) - (α12 - α22)),
where as in Thm E.61,
α11 = [11|11] + [11|22](B - 1)
α12 = 2(B - 1)[11|12] + (B - 2)(B - 1)[11|23]
α21 = 2[12|11] + (B - 2)[12|33]
α22 = [12|12] + 4(B - 2)[12|13] + [12|21] + (B - 2)(B - 3)[12|34]
2.	Eigenspace L with eigenvalue λLG,T := B-1 ((B - 2)β11 + β12 + 2(B - 2)β21 + 2β22),
where as in Thm E.61,
β11 = [11|11] - [11|22]
β12 = 2(B - 2)([11|23] - [11|12])
β21 = -[12|11] + [12|33]
β22 = [12|21] + [12|12] + 2(B - 4)[12|13] - 2(B - 3)[12|34]. 3
3. Eigenspace M with eigenvalue λGM,T := λTM = [12|12] + [12|21] - 4[12|13] + 2[12|34].
44
Published as a conference paper at ICLR 2019
Proof. Item 1 As in the proof of Thm E.61, we find
T {BSB1(a, b)} = BSB1
α11
α21
α12	a
α22	b
where
α11 = [11|11] + [11|22](B - 1)
α12 = 2(B - 1)[11|12] + (B - 2)(B - 1)[11|23]
α21 = 2[12|11] + (B - 2)[12|33]
α22 = [12|12] + 4(B - 2)[12|13] + [12|21] + (B - 2)(B - 3)[12|34].
For a = B - 1, b = -1 so that BSB1(B - 1, -1) = BG, we get
T {BSB1(B - 1, -1)} = BSB1 ((B - 1)α11 - α12, (B - 1)α21 - α22)
G(S)2 ◦ T{BSB1(B - 1, -1)} = G BSBI((B - 1)αn - α12, (B - I)a21 - α22) G
= ((B - 1)(α11 - α21) - (α12 - α22)) G
= B-1 ((B - 1)(α11 - α21) - (α12 - α22)) BSB1(B - 1, -1)
= λGG,T BSB1(B - 1, -1)
by Lemma E.52.
Item 2. It suffices to show that L(B - 2, 1) is an eigenmatrix with the eigenvalue λLG,T .
As in the proof of Thm E.61, we find
T{L(a, b)} = L
ββ1211
ββ2122	ab
where
β11 = [11|11] - [11|22]
β12 = 2(B - 2)([11|23] - [11|12])
β21 = -[12|11] + [12|33]
β22 = [12|21] + [12|12] +2(B - 4)[12|13] -2(B - 3)[12|34].
So with a = B - 2, b = 1, we have
T {L(B - 2, 1)} = L((B - 2)β11 + β12, (B - 2)β21 + β22)
GS2 ◦ T{L(B - 2,1)} = G L((B - 2)βu + β12,(B - 2)β21 + β22) G
= B-1((B - 2)β11 + β12 + 2((B - 2)β21 + β22))L(B - 2, 1)
=λLG,TL(B-2,1)
by Lemma E.60.
Item 3. The proof is exactly the same as in that of Thm E.61.	□
Noting that the eigenspaces of Thm E.62 are orthogonal, we have
Proposition E.63. GS2 ◦ T HG for any ultrasymmetric operator T is self-adjoint.
DOS Operators. In some cases, such as the original study of vanilla tanh networks by Schoenholz
et al. (2016), the ultrasymmetric operator involved has a much simpler form:
Definition E.64. Let T : HB → HB be such that for any Σ ∈ HB and any i 6= j ∈ [B],
T {Σ}ii = uΣii
T {Σ}ij = vΣii + vΣjj + wΣij
Then we say that T is diagonal-off-diagonal semidirect, or DOS for short. We write more specifi-
cally T = DOSB (u, v, w).
45
Published as a conference paper at ICLR 2019
Thm E.62 and Thm E.61 still hold for DOS operators, but we can simplify the results and reason
about them in a more direct way.
Lemma E.65. Let T := DOSB (u, v, w). Then T{LB (a, b)} = LB (ua, wb - va).
Proof. Let L := LB (a, b). It suffices to verify the following, each of which is a direct computation.
1.	T {L}3:B,3:B = 0.
2.	T{L}1,2=T{L}2,1=0
3.	T {L}1,1 = -T {L}2,2 = ua
4.	T {L}1,i = -T {L}2,i = T {L}i,1 =-T{L}i,2=va-wb.
□
Lemma E.66. Let T := DOSB (u, v, w). Then LB(w - u, v) and LB (0, 1) are its eigenvectors:
T{LB(w - u, v)} = uLB (w - u, v)
T {LB(0, 1)} = wLB(0, 1)
Proof. The map (a, b) 7→ (ua, wb - va) has eigenvalues u and w with corresponding eigenvectors
(W - u, V) and (0,1). By Lemma E.65, this implies the desired results.	□
LemmaE.67. Let T := DOSB (u,v,w). Thenfor any L ∈ Lb, G02 oT{L} = (u-2v)(B—2)+2w L.
Proof. ByLemmaE.60 and Lemma E.65, G02 °T{L(B — 2,1)} = G02{L(u(B — 1),w — V(B —
1))} = (U-2V)(B-2)+2wLB (B — 2,1). By permutation symmetry, We also have the general result
for any LB (B — 2,1)-shaped matrix L. Since they span LB, this gives the conclusion we want. □
Lemma E.68. Let T := DOSB (u, V, w). Then T {BSB1B (a, b)} = BSB1B (ua, wb + 2Va).
Proof. Direct computation.	□
Lemma E.69. Let T := DOSB (u, V, w). Then BSB1B (w — u, wV) and BSB1B (0, 1) are eigen-
vectors of T.
T {BSB1B (u — w, 2V)} = uBSB1B (u — w, 2V)
T {BSB1B (0, 1)} = wBSB1B (0, 1)
Proof. The linear map (a, b) 7→ (ua, wb + 2Va) has eigenvalues u and w with corresponding eigen-
vectors (U — w, 2v) and (0,1). The result then immediately follows from Lemma E.68.	□
Lemma E.70. Let T := DOSB (u, v, W). Then G02 ◦ T{G} = (B-I)(B-2v)+w g.
Proof. Direct computation with Lemma E.68 and Lemma E.52	□
Definition E.71. Define MB := {Σ ∈ Sb : DiagΣ = 0} (so compared to M, matrices in M do not
need to have zero row sums). In addition, for any a, b ∈ R, set
	a	—b	—b	∙∙∖	
LB (a,b):=	—b	0	0	• ∙	
	—b	0	0	• •	∈Hb
	.. ..	..	. . .	...	
Define LB (a, b) := SPan(PTLB (a, b)Pii : i ∈ [B]) where Pu is the permutation matrix that swap
the first entry with the ith entry, i.e. LB (a, b) is the span of the orbit of LB (a, b) under permuting
rows and columns simultaneously.
46
Published as a conference paper at ICLR 2019
Note that
LB (a, b) =Lb (a,b) - PTLB (a, b)P12
B
BSB1b (a, -2b) = X PTLB (a, b)Pii
i=1
So LB (a, b), BSB1b (a, —2b) ∈ LB (a, b).
Theorem E.72. Let T := DOSB (u, v, w). Suppose w 6= u. Then T	HB has the following
eigendecomposition:
•	MB has eigenvalue W (dimMB = B(B — 1)/2)
•	LB (W — u,v) has eigenvalue of U (dim LB (W — u,v) = B)
If w = u, then LB (w — u,v) ⊆ Mb.
Proof. The case of MB is obvious. We will show that LB (W — u,v) is an eigenspace with eigenvalue
u. Then by dimensionality consideration they are all of the eigenspaces of T.
Let L := LB (w — u,v). Then it,s not hard to see T{L} = LB (a, b) for some a, b ∈ R. It follows
that a has to be U(W — U) and b has to be —(v(w — U) — Wv) = uv, which yields what We want. □
Theorem E.73. Let T := DOSB (u, v, W). Then G02 ◦ T『HB : HB → HB has the following
eigendecomposition:
•	RG has eigenvalue (B-I)(B-2V)+w (dimRG = 1)
•	MB has eigenvalue W (dim MB = B(B — 3)/2)
•	LB has eigenvalue (B-2)(72v)+2w (dimLB = B — 1)
Proof. The case of MB is obvious. The case for RG follows from Lemma E.70. The case for LB
follows from Lemma E.67. By dimensionality considerations these are all of the eigenspaces. □
F Forward Dynamics
In this section we will be interested in studying the dynamics on PSD matrices of the form
Σl = VBφ(Σl-1) = E[φ(√BGh∕kGhk)02 : h 〜N(0, Σl-1)]	(43)
where Σl ∈ SB and φ : R → R.
F.1 Global Convergence
Basic questions regarding the dynamics Eq. (43) are 1) does it converge? 2) What are the limit
points? 3) How fast does it converge? Here we answer these questions definitively when φ = id.
The following is the key lemma in our analysis.
Lemma F.1. Consider the dynamics Σl = E[(h∕kh∣∣)02 : h 〜N(0, Σl-1)] on Σl ∈ Sa. Suppose
Σ0 is full rank. Then
1.	limι→∞ ∑l = AI.
2.	This convergence is exponential in the sense that, for any full rank Σ0, there is a constant
K < 1 such thatλ1(Σl) — λA(Σl) < K(λ1(Σl-1) — λA(Σl-1)) for all l ≥ 2. Here λ1
(resp. λA) denotes that largest (resp. smallest) eigenvalue.
3.	Asymptotically, λι(Σl) — λA(Σl) = O((1 — A+2)l).
47
Published as a conference paper at ICLR 2019
Proof. Let λιl ≥ 入 2 ≥∙∙∙≥ λA be the eigenvalues of Σl. It's easy to see that Pi λj = 1 for all
l ≥ 1. So WLOG we assume l ≥ 1 and this equality holds.
We will show the “exponential convergence" statement; that limι→∞ Σl = ∖I then follows from
the trace condition above.
Proof of Item 2. For brevity, We write suppress the superscript index l, and use 二 to denote ∙l. We
will now compute the eigenvalues λι ≥ ∙∙∙ ≥ λa of Σ.
First, notice that Σ and Σ can be simultaneously diagonalized, and by induction all of {Σl}1≥0 can be
simultaneous diagonalized. Thus we will WLOG assume that Σ is diagonal, Σ = Diag(λι,..., λa),
so that Σ = Diag(Yι,..., YA) for some {γi}i. These {γi}i form the eigenvalues of Σ but a priori
we don't know whether they fall in decreasing order; in fact we will soon see that they do, and
Yi = λi∙
We have
A
π	-A/2	∞
⑸√0A
π	-A/2	∞
⑴J0A
Yi
「X2 2 e-kxk2/2 dx
j λj xj2
A
λiX2e-kxk2/2 dx Z	e-s Pj λjx2 ds
0
Therefore,
(π L/1
2	0A+1
1	Pj(I+2sλj )x2 dx ds
(	2) A/2 Z	λix2e-1 (I+2sλi)x2 ∙ (2) 2 ∏(1 + 2sλj)-1/2 dxi ds
2	02	2	j 6=i
(2)
∞
0
∞2
02
λiX2e-1 (1+2sλi)x2 dχi 口(1 + 2sλj)-1/2 ds
j6=i
λi(1 + 2sλi) 3/2 Y Y(1 + 2sλj) 1/2 ds
j6=i
Z Y(1 + 2sλj)-"λi(1 + 2sλi)T ds
0 j=1
(44)
Yi -	Yk	= Z Y (1 +	2sλj )-1/2 (λi(1	+ 2sλi)-1	- λk (1 + 2sλk)-1 )	ds
0 j=1
=(λi - λk ) Z Y(I + 2sλj 厂1/2 (I + 2sλi厂I(I + 2sλk 厂1 ds
0 j=1
Since the RHS integral is always positive, λ%	≥ λk	=⇒	Yi	≥ Yk	and thus	Yi	=	λi	for each i.
Define T(λ) := R∞ Qj=ι(1 + 2sλι)T∕2(1 + 2sλι)-1(1 + 2sλa)T ds, so that λι - λA =
(λ1 - λA)T(λ).
Note first that QA=i(1 + 2sλj)-1/2(1 + 2sλi)-1(1 + 2sλk)-1 is (strictly) log-convex and hence
(strictly) convex in λ. Furthermore, T(λ) is (strictly) convex because it is an integral of (strictly)
convex functions. Thus T is maximized over any convex region by its extremal points, and only by
its extremal points because of strict convexity.
The convex region we are interested in is given by
A= {(λi)i ： λl ≥ λ2 ≥ ∙∙∙ ≥ λA ≥ 0 & X λi = 1}.
i

48
Published as a conference paper at ICLR 2019
The unit sum condition follows from the normalization h∕khk of the iteration map. The extremal
k times
z------λ-------{
points of A are {ωk := (1∕k, 1∕k,…，1∕k, 0,0,…,0)}A=「For k = 1,..., A - 1, we have
T (ωk) = ∞ (1 + 2s∕k)-k/2-1 ds
0
1 + 2s/k)-k/2
1.
∞
0
But for k = A,
T (ωA) =	(1 + 2s/A)-A/2-2 ds
0
=—2/A— (1 + 2s∕A)-k∕2
-A∕2 - 1( + / )
A
∞
0
A+2
This shows that T(λ) ≤ 1, with equality iff λ = ωk for k = 1, . . . , A - 1. In fact, because every
point λ ∈ A is a convex combination of ωk for k = 1, . . . , A, λ = PkA=1 akωk, by convexity of T ,
we must have
A
T(λ) ≤ X akT (ωk)
k=1
1 -aA
1 - λA
2
A + 2
2
A + 2
(45)
where the last line follows because ωA is the only point with last coordinate nonzero so that aA =
λA.
We now show that the gap λ1 l - λAl → 0 as l → ∞. There are two cases: if λAl is bounded
away from 0 infinitely often, then T (λ) < 1 - infinitely often for a fixed > 0 so that the gap
indeed vanishes with l. Now suppose otherwise, that λAl converges to 0; we will show this leads to
a contradiction. Notice that
Z	Y (1 + 2sλj )-1/2 λA (1 + 2sλA)-1 ds
0 j=1
W ≥ Z Y(1 + 2s(1 - λA)∕(A — 1))-(AT)/2(1 + 2sXa)-3/2 ds
0 j=1
where the first lines is Eq. (44) and the 2nd line follows from the convexity of QjA-I(1 + 2sλj)-1/2
as a function of (λ1, . . . ,λA-1). By a simple application of dominated convergence, as λA → 0,
this integral converges to a particular simple form,
Aa/Aa ≥ Z Y(1 + 2s∕(A - 1))-(AT)/2 ds
0 j =1
=1 + A-3
Thus for large enough l, AAl+1/AAl is at least 1 + for some > 0, but this contradicts the
convergence of AA to 0.
Altogether, this proves that A1l - AAl → 0 and therefore Al → ωA as l → ∞. Consequently, AAl
is bounded from below, say by K0 (where K0 > 0 because by Eq. (44), AAl is never 0), for all l,
49
Published as a conference paper at ICLR 2019
and by Eq. (45), We prove Item 2 by taking K = 1 - K0 a+^ . In addition, asymptotically, the gap
decreases exponentially as T(ωA)l = (A^) , proving Item 3.	□
Theorem F.2. Consider the dynamics Σl = E[(h∕kh∣∣产2 : h 〜N(0, Σl-1)] on Σl ∈ Sa. Suppose
Σ0 = MT DM where M is orthogonal and D is a diagonal matrix. If D has rank C and Dii 6=
0, ∀1 ≤ i ≤ C, then
1.	limι→∞ Σl = CeMTDM where D0 is the diagonal matrix with Dii = I(Dii = 0).
2.	This convergence is exponential in the sense that, for any Σ0 of rank C, there is a constant
K < 1 such thatλ1(Σl) - λC(Σl) < K(λ1(Σl-1) - λC(Σl-1)) for all l ≥ 2. Here λi
denotes the ith largest eigenvalue.
3.	Asymptotically, λι(Σl) — λ°(Σl) = O((1 — C+^)l).
Proof. Note that Σl can alWays be simultaneously diagonalized With Σ0, so that Σl = MTDlM
for some diagonal Dl which has Dii = 0,∀0 ≤ i ≤ C. Then we have (DIy = E[(h∕kh∣∣产2 :
h 〜 N(0, (DIT)0)], where (Dl)0 means the diagonal matrix obtained from Dl by deleting the
dimensions with zero eigenvalues. The proof then finishes by LemmaF.1.	□
From this it easily follows the following characterization of the convergence behavior.
Corollary F.3. Consider the dynamics of Eq. (43) for φ = id: Σl = BE[(Gh∕∣∣Ghk产2 : h 〜
N (0, Σl-1)] on Σl ∈ Sb . Suppose GΣG has rank C < B and factors as ^D ^t where D ∈ RC × C
is a diagonal matrix with no zero diagonal entries and ^ is an B X C matrix whose columns form
an orthonormal basis ofa subspace of im G. Then
1.	liml→∞ Σl = C^IC^T.
2.	This convergence is exponential in the sense that, for any GΣ0 G of rank C, there is a
constant K < 1 such that λ1(Σl) - λC(Σl) < K (λ1 (Σl-1) - λC (Σl-1)) for all l ≥ 2.
Here λi denotes the ith largest eigenvalue.
3.	Asymptotically, λι(Σl) — λC(Σl) = O((1 — C2^)l).
General nonlinearity. We don’t (currently) have a proof of any characterization of the basin of
attraction for Eq. (43) for general φ. Thus we are forced to resort to finding its fixed points manually
and characterize their local convergence properties.
F.2 Limit points
Batchnorm Bφ is permutation-equivariant, in the sense that Bφ (πh) = πBφ(h) for any permtuation
matrix π. Along with the case of φ = id studied above, this suggests that we look into BSB1 fixed
points Σ*. What are the BSB1 fixed points of Eq. (43)?
F.2.1 Spherical Integration
The main result (Thm F.5) of this section is an expression of the BSB1 fixed point diagonal and off-
diagonal entries in terms of 1- and 2-dimensional integrals. This allows one to numerically compute
such fixed points.
By a simple symmetry argument, we have the following
Lemma F.4. Suppose X is a random vector in RB , symmetric in the sense that for any permutation
matrix π and any subset U ofRB measurable with respect to the distribution ofX, P (X ∈ U) =
P(X ∈ π(U)). Let Φ : RB → RB be a symmetric function in the sense that Φ(πx) = πΦ(x) for
any permutation matrix π. Then E Φ(X)Φ(X)t = μI + V 11t for some μ and V.
This lemma implies that VBφ (Σ) is BSB1 whenever Σ is BSB1. In the following, We in fact show
that for any BSB1 Σ, VBφ (Σ) is the same, and this is the unique BSB1 fixed point of Eq. (43).
50
Published as a conference paper at ICLR 2019
Theorem F.5. The BSBIfixedpoint Σ* = μ*I + V*11T to Eq. (43) is unique and satisfies
Γ( B-1 )	∏
μ* + ν* = r(BJ)√π JO dθι sinB-3 θιΦ(√BZι(θι))2
V ,
¾3 R0π dθι R0π dθ2 sinB-3 θι sinB-4 θ2φ(√BZ1(θ1))φ(√BZ2(θ1,θ2)), if B ≥ 4
2∏ J； dθφ(-√2sin θ)φ(sin θ/√2 - cos θ√3/√2),
ifB=3
where
ζ1(θ)
cos θ

ζ2(θ1,θ2) = PB(B - 1) Cos θ1 - B-~isin θ1 Cos θ2
Proof. For any BSB1 Σ, eτΣe = μIB-ι for some μ. Thus We can apply Proposition E.33 to GΣG.
We observe that K(v; GΣG) = 1, so that
Vbφ (∑) = VBφ (G∑G) =	E 2 φ(√Bev产.
V〜SB-2
Note that this is now independent of the specific values of Σ. Now, using the specific form of e
given by Eq. (33), We see that VBφ (Σ)11 only depends on θ1 and VBφ (Σ)12 only depends on θ1 and
θ2 . Then applying Proposition E.33 followed by Lemma E.36 and Lemma E.37 yield the desired
result.	□
F.2.2 Laplace Method
In the case that φ is positive-homogeneous, we can apply Laplace’s method Appendix E.1 and obtain
the BSB1 fixed point in a much nicer form.
Definition F.6. For any α ≥ 0, define Ka,B := CaP (B-1 ,α) 1 (B-1 )α where P (a, b) := Γ(a +
b)∕Γ(a) is the Pochhammer symbol.
Note that
Proposition F.7. limB→∞ Kα,B = cα.
We give a closed form description of the BSB1 fixed point for a positive homogeneous φ in terms of
its J function.
Theorem F.8. Suppose φ : R → R is degree α positive-homogeneous. For any BSB1 Σ ∈
SB, VBφ (Σ) is BSB1. The diagonal entries are Kα,B Jφ (1) and the off-diagonal entries are
Ka,B Jφ (B-⅛). Here Ca is as defined in Defn E.7 and Jφ is as defined in Defn E.20. Thus a
BSB1 fixed point of Eq. (43) exists and is unique.
Proof. Let e bean B X (B 一 1) matrix whose columns form an orthonormal basis of im G := {Gv :
v ∈ RB} = {w ∈ RB : Pi wi = 0}. Let BSB1(a, b) denote a matrix with a on the diagonal and b
on the off-diagnals. Note that Vφ is positive homogeneous of degree a, so for any μ,
Vφ(eμIB-1(IB-1 + 2sμIB-ι)-1eτ)
Vφ(i⅛ ®T)
a
Vφ((E (ET)
by Proposition E.24
μ
1 + 2sμ
μ
a
Vφ(G)
B 一 1、α
1 + 2sμ B
μ B 一 1
1 + 2sμ B
VΦ (BSBI (1, B⅛
Ca Jφ(BSB1 (1,B-1
by Corollary E.19
51
Published as a conference paper at ICLR 2019
So by Eq. (26),
VBφ (® μIe T) = BαΓ(α)-1 [ ds SaT(I + 2sμ)-(BT)/2 (/J- 1 [ c.Jφ (BSBI (l, B-T
Γ(α)-1(B 1)αcαJφ BSB1
1, B-TT) ) / ds sa-1(1 + 2sμ)-(BT)/2-a〃a
Γ(α)-1(B 1)αcαJφ BSB1 1,
B—1	Beta(α, (B - 1)∕2)2-a
Γ((B - 1)/2)
a Γ(α +(B - 1)/2)
(B-Y JΦ (BSB1 八 U))
□
Corollary F.9. Suppose φ : R → R is degree a positive-homogeneous. If Σ* is the BSBIfixedpoint
of Eq. (43) as g^ven by Thm F.8, then GΣ*G = μ*!B-ι where μ* = Ka B (Jφ(1) — Jφ(-1/(B —
1))
By setting α = 1 and φ = relu, we get
CorollaryF.10. For any BSB1 Σ ∈ SB, VBrelU (∑) isBSB1 with diagonal entries ɪ and off-diagonal
entries 1 J1 (B⅛),	so	that	GTvBrelu (ς)} =	G(VBrelU 3))G	=	( 2	- 1 J1	( B-l	G
By setting α = 1 and φ = id = x 7→ relu(x) - relu(-x), we get
Corollary F.11. For any BSB1 Σ ∈ SB, VBid (Σ) is BSB1 with diagonal entries 1 and off-diagonal
entries B-i, so that G02{VBid(∑)} = B∕(B — 1)G.
Remark F.12. One might hope to tweak the Laplace method for computing the fixed point to work
for the Fourier method, but because there is no nice relation between Vφ(cΣ) and Vφ(Σ) in general,
we cannot simplify Eq. (32) as we can Eq. (26) and Eq. (27).
F.2.3 Gegenbauer Expansion
It turns out that the BSB1 fixed point can be described very cleanly using the Gegenbauer coefficients
of φ.
When Σ is BSB1, K(v, Σ) = 1 for all v ∈ SB-2, so that by Proposition E.34,
Σ* =	E	φ( √Bev)02
V〜SB-2
^b =	E	φ(√B<B a,")φ(√B ®b,：v)
V〜SB-2
for any a,b ∈ [B], independent of the actual values of Σ. Here e。,： is the ath row of e.
Theorem F.13. If φ(√B — 1 x)	∈ L2((1 — x2)B-^) has Gegenbauer expansion
P∞=o ai —1 ɪ; C~"r~)(x), then Σ* = μ*I + V*11T with
*
μ
∞
Xai2
i=o
∞
Xai2
i=o
cB-1,i
1
(B —14 +1) - C吩) (B⅛))
1
cB-1,i
ν*
∞
Xai2
i=o
1
cB-1,i
-1
B - 1
52
Published as a conference paper at ICLR 2019
Proof. By the reproducing property Fact E.46, we have
∑Wb = hφ(√B @a,:)。(VB@b,QiSB-2
=hφ(√B - 1-a,:) φ(VB — 16bjBisB-2
1	(B-3)	∞	1	(B-3
a —	C F)(仓。,Q,∑> —	C F
cB-1,l	l=0 cB-1,l
∞
a，ZBa-2,⑷ X aZB-2,(I))
l=0	SB-2
∞
X a2ZB:2,(I)(M:)
l=0
∞
Xal2
l=0
∞
Xal2
l=0
cB-1,l
C(B-3)(h^a,:, M：i)
1「( B-3)(ʃ1
if a = b
SB-2
cB-1,l
-1/(B - 1) else
1
This gives the third equation. Since μ* = Σ" — Σ^, straightforward arithmetic yields the first
equation. The second follows from Fact E.41.	□
We will see later that the rate of gradient explosion also decomposes nicely in terms of Gegenbauer
coefficients (Thm G.5). However, this is not quite true for the eigenvalues of the forward conver-
gence (Thms F.22 and F.24).
F.3 Local Convergence
In this section we consider linearization of the dynamics given in Eq. (43). Thus we must consider
linear operators on the space of PSD linear operators SB. To avoid confusion, we use the following
notation: If T : SB → SB (for example the Jacobian of VBφ) and Σ ∈ SB, then write T{Σ} for the
image of Σ under T.
A priori, the Jacobian of VBφ at its BSB1 fixed point may seem like a very daunting object. But a
moment of thought shows that
dVB
Proposition F.14. The Jacobian d∑φ ∣∑ : HB → Hb is Ultrasymmetricfor any BSB1 Σ.
Now we prepare several helper reults in order to make progress understanding batchnorm.
Definition F.15. Define n(x) = √Βx∕kx∣∣, i.e. division by sample standard deviation.
Batchnorm Bφ can be decomposed as the composition of three steps, φ ◦ n ◦ G, where G is mean-
centering, n is division by standard deviation, and φ is coordinate-wise application of nonlinearity.
We have, as operators H → H,
dVBφ (∑)
d∑
d E[Bφ(z)02 : Z ∈N (0, Σ)]
d∑
dE[(φ ◦ n)(x)02 : X ∈ N(0, GΣG)]
d∑
Definition F.16. With Σ* being
dV(φ°n)(∑G)
d∑G I∑g = GΣ*G
: HG → HG.
dV(φ°n)(∑G)O d∑G
dΣG
dV(φ°n)(∑G)
dΣG
dΣ
◦ G02
the unique BSB1 fixed point, write U :=	G02 ◦
53
Published as a conference paper at ICLR 2019
It turns out to be advantageous to study U first and relate its eigendecomposition back to that of
dvB⅛②) I ∑=∑*，where ∑* is the BSB1 fixed point, by applying Lemma F.17.
Lemma F.17. Let X and Y be two vector spaces. Consider linear operators A : X → Y, B : Y →
X. Then
1.
rank AB = rank BA
2.
3.
If v ∈ Y is an eigenvector of AB with nonzero eigenvalue, then X 3 Bv 6= 0 and Bv is
an eigenvector of BA of the same eigenvalue
Suppose AB has k = rank AB linearly independent eigenvectors {vi }ik=1 of nonzero
eigenvalues {λi}ik=1. Then BA has k linearly independent eigenvectors {Bvi}ik=1 with
the same eigenvalues {λi}ik=1 which are all eigenvectors of BA with nonzero eigenvalues,
up to linear combinations within eigenvectors with the same eigenvalue.
With A = Ge)2 and B = —(φ∑G-), Thm E.61 implies that AB and BA can both be diagonalized,
and this lemma implies that all nonzero eigenvalues of 皿：左3) can be recovered from those of U.
Proof. (Item 1) Observe rank AB = rank ABAB ≤ rank BA. By symmetry the two sides are in
fact equal.
(Item 2) Bv cannot be zero or otherwise ABv = A0 = 0, contradicting the fact that v is an
eigenvector with nonzero eigenvalue. Suppose λ is the eigenvalue associated to v. Then BA(Bv) =
B (ABv) = B(λv) = λBv, so Bv is an eigenvector of BA with the same eigenvalue.
(Item 3) Item 2 shows that {Bvi}ik=1 are eigenvectors BA with the same eigenvalues {λi}ik=1. The
eigenspaces with different eigenvalues are linearly independent, so it suffices to show that if {Bvij }j
are eigenvectors of the same eigenvalue λs, then they are linearly independent. But Pj ajBvij =
0 =⇒ Pj aj vij = 0 because B is injective on eigenvectors by Item 2, so that aj = 0 identically.
Hence {Bvj }j is linearly independent.
Since rank BA = k, these are all of the eigenvectors with nonzero eigenvalues of BA up to linearly
combinations.
□
Lemma F.18. Let f : RB → RA be measurable, and Σ ∈ SB be invertible. Then for any Λ ∈
Rb×b, with〈•，\ denoting trace inner product,
-d E[f (z) ： Z 〜N(0, ∑)]{Λ} = 1 E[f (z) (∑-1zzTΣ-1 - Σ-1, Λ : Z 〜N(0, Σ)]
dΣ	2
If f is in addition twice-differentiable, then
d∑ E[f (z) ： z 〜N(0, ∑)] = 1 E ] dfz) ： z 〜N(0, ∑)
whenever both sides exist.
54
Published as a conference paper at ICLR 2019
Proof. Let ∑t, t ∈ (-e, E) be a smooth path in SB, with ∑0 = Σ. Write %∑t = Σt. Then
-d E[f (z): Z 〜N(0, Σt)]
dt
d(2π)-B/2 detΣ-" / dze-2ZTς-1
(2π)-B/2-21det Σ-1/2 * * * * * tr (ς
+ (2π)-B/2 det Σ-1/2 / dz e-2Z
Z f (z)
t=0
dz e-1ZTς-1z∕2/(z)
T ς-1z -1ZT (-∑-1∑ 0∑-1)zf (z)
-1(2π)-B/2 det Σ-1∕2 / dz e-1ZTς-1zf (z)(tr Σ-1Σ0 -ZTΣ-1Σ0∑-lz)
1(2π)-B/2 det Σ-1/2 / dz e-2ZTςTZf (z) (∑-1zztΣ-1 - Σ-1, Σ0)
.
-1 E[f(z)①1- Σ-1zztΣ-1
:Z 〜N(0, Σ)]
Note that
VT ； e -1ZT ςTZ = -VT ∑-1ze W ZT ςTZ
dz
wτ (ddLe-1 ZT工TZ) v = (WTΣ-1zzτΣ-1v - wτΣ-1v) e-1ZTςTZ
Σ-1 zzτΣ-1 - Σ-1,vwτ〉e-21 ZTςTZ
so that as a cotensor,
里e-1ZTςTZ{Λ} = (∑-1zzτΣ-1 - Σ-1,垃 e-1ZTςTZ
dz2	'	/
for any Λ ∈ RB × B.
Therefore,
d E[f (z) : Z 〜N(0, Σ)]
dΣ
1	d	d2e-1ZT∑-1Z
{Λ} = 2∙(2π)-B/2 det Σ-1/2 / dz f (z)―d^— {Λ}
= 1(2π)-B/2 det∑-1/2 Z dze-2 ZT ςTZ df) {Λ}
2	J	d2z
(by integration by parts)
1E [ fz): z 〜N(0, ∑)l {Λ}
□
Note that for any Λ ∈ HB with ∣∣Λ∣∣op < μ*,
E[φ 0 n(z产2 : z ∈ N(0, μ*G + Λ)] = E[φ ◦ n(Gz尸2 : Z ∈ N(0, μ*I + Λ)]
=E[Bφ(z尸2 : z ∈ N(0, μ*I + Λ)]
55
Published as a conference paper at ICLR 2019
so that we have, for any Λ ∈ HB,
U{Λ}
dVBφ (∑)
dΣ
{Λ}
Σ=μ*I
-μ*-1I, Λ
:Z 〜N(0, μ*I)]
(by Lemma F.18)
2μ*-2 E[Bφ(z尸2hzzT, Λi : Z 〜N(0,μ*I)] - 1 μ*-1Σ*hI, A)
(by Thm F.8)
(2μ*)-1 (E[Bφ(z 产 hzzT, Λi : z 〜NaI)] - ∑hI, Ai)
(Bφ is scale-invariant)
(2μ*)-1 (E[Bφ(z产 (GzztG, Ai : Z 〜N(0,I)] - Σ*(I, Ai)
(A = GAG)
Let’s extend to all matrices by this formula:
Definition F.19. Define
U : rb×b → SB,
A → (2μ*)T (E[Bφ(z)182(GzzTG, A)： Z 〜N(0,I)] - Σ*〈I, Ai)
F.3.1 Spherical Integration
SoU『HB = U ∖ HB. Ultimately We will apply Thm E.61 to G02 ◦lU ∖ HB = G02 ◦ U
DefinitionF.20. WriteTij = ɪ 06： + δjδτ).
Then
U{τj}ki = (2μ*)-1 (E[Bφ(z)kΒφ(z)ι(Gz)i(Gz)j : Z 〜N(0,I)] - Σ*I(i = j))
=(2μ*)-1 (E[φ(n(y))kφ(n(y))ιyiyj : y 〜N(0, G)] - ∑kI(i = j))
=(2〃*)-1 (E[φ(n(y))kφ(n(y))ιyiyj ： y ~N(0，©Tτ)] - -i = j))
(See Defn E.3 for defn of e)
=(2μ*)-1 (E[φ(n(ex))kφ(n(ex))ι(ex)i(ex)j : X 〜N(0,IB-ι)] - ∑M(i = j))
Here we will realize e as the matrix in Eq. (33).
Definition F.21. Define Wjj∖bi := E[φ(n(ex))kφ(n(ex))ι(ex)i(ex)j : x 〜N(0,IB-ι)].
Then U{τj}kι = (2μ*)-1(Wj∖ki — ∑k∕(i = j)). If we can evaluate Wj∖ki then we can use
Thm E.61 to compute the eigenvalues of G02 ◦ U. It’s easy to see that Wij∖ki is ultrasymmetric
Thus WLOG we can take i, j, k, l from {1, 2, 3, 4}.
By Lemma E.35, and the fact that x → ex is an isometry,
Wij∖ki = E[r2φ(√Bev)kφ(√Bev)ι(ev)i(ev)j : x 〜N(0,IB-ι)]
= (B - 5)(B - 3)(B - 1)(2π)-2 ×
d dθι …[dθ4 φ(√Βev)kφ(√Βev)ι(ev)i(ev)j sinB-3 θι …sinB-6 θ4
00
If WLOG we further assume that k, l ∈ {1, 2} (by ultrasymmetry), then there is no dependence on
θ3 and θ4 inside φ. So we can expand (ev)i and (ev)j in trigonometric expressions as in Eq. (34)
and integrate out θ3 and θ4 via Lemma E.29. We will not write out this integral explicitly but instead
focus on other techniques for evaluating the eigenvalues.
56
Published as a conference paper at ICLR 2019
F.3.2 GEGENBAUER EXPANSION
Now let,s compute the local convergence rate via Gegenbauer expansion. By differentiating Propo-
sition E.34 through a path ∑t ∈ Sb , t ∈ (-e, e), We get
-dV%(∑t) =	E φ(√Bev/2-dK(v; ∑t)
dt	V 〜SB-2	dt
=	eb-2 φ(√Bev)θ2K(v; ∑t) (B- 1 (VT∑口-1v)-1vτ∑□-1	∑□ -1v - 1 tr(∑□-1 ^l^
V 〜SB- 2	2	dt	2	at
where ∑□ = ∑□t = tt∑t<e ∈ Sb-1 (as introduced below Defn E.3). At ∑0 = ∑*, the BSB1 fixed
point, we have ∑口0 = μ*I, and
d vbφ (∑t)
=E	φ( √Bev)182K(s∑*)μ*τ
t=0	v~SB-2
μ*-1 E	φ( √Bev)02
V〜SB-2
1 τ d∑□
v dt v
1
2tr(
(46)
If d ∑ιt=0 = G, then the term in the parenthesis vanishes. This shows that ^∑φ ∣	{G} = 0.
Eigenvalue for L. If 捋∑∣t=0 = L(B - 2,1) = BG(δf2 -碍2)G where δι = (1,0,..., 0),δ2 =
(0,1,0,..., 0), both in RB (Proposition E.59), then we know by Thm E.62 that G02 0
dt Vbφ (∑t)∣t=0 is a multiple of L(B - 2,1). We compute
t=0∕ ab
BB - 1 4*-1 E	φ(√Beα,MΦ(√Beb,:v)((æi,：v)2 - (®2,：v)2).
2	V〜SB-2
Clearly,6V% (∑t)∣t=0 is L-shaped.
Now define the quantity A(a, b; c) := EV〜sb-2 φ(√Beα∕v)Φ(√B®b,：v)(®c,：v)2, so that
(SVBφ(∑t)∣t=0)αb = B(Jl-Dμ*-1(A(a, b;1) - A(a, b; 2)). Because e is an isometry,
Pb(@b,：v)2 = Ilvll2 = 1 for all V ∈ Sb-2. Thus
B
X A(α, b; c) =	SE _2φ(√Beα,e)Φ(√Beb,：v) = ∑ab∙	(47)
c=1
By symmetry, A(a, b; a) = A(a, b; b) and for any c, CC ∈ {a, b}, A(α, b; c) = A(α, b; c0). So we
have, for any a, b, C not equal,
A(α, a; a) + (B — 1)A(a, a; c) = ∑aa
2A(a, b; b) + (B — 2)A(a, b; c) = ∑ab
So the eigenvalue associated to L(B - 2,1) is, So by Lemma E.60, G02 0 U{L(B - 2,1)}
λ1L(B - 2,1), where
∖↑ — RB - 1 *-1 (第 vBΦ (∑t) I t = 0)il - 2 陶 vBΦ (∑t) I t=0)i3
λL = B "ɪ μ ---------------------------B----------------------
A(1,1;1) - A(1,1; 2) - 2(A(1,3;1) - A(1, 3; 2))
B
B 彳 μ*τ
一μ*T(A(1,1;1) - π1-(∑aa - A(1,1;1)) - 2(A(1,3; 1) - -1-(∑ab - 2A(1, 3;1))))
2	B-1	B-2
B-1 μ*-1(S A(1,1;1) - ≠T∑aa - 2ɪA(1,3J) + π25∑ab)	(48)
2 B-1	B-1 B-2	B-2
57
Published as a conference paper at ICLR 2019
______ B _ 3	B — 3 ∖
Thus, if φ(√B - 1x) ∈ L2((1 - x2)-ɪ) has Gegenbauer expansion P∞o aι-ɪɪ ； C； 2 (x),
then by Proposition E.43,
x2 φ(√ B — 1x)
∞	1	b 3
^X a；-----x2C(	2)(x)
七	cBτ,l
9	1	，
y½----- K2(l,
七CB-I八
+ κo(l, Bf^)C(ɪ)(x) + κ-2(l,三)C(-f)(x))
(--3 )
where C； 2 (x) is understood to be 0 for l < 0
X C(勺 )(x)
l=0
a al-2 κ2(l- 2,
∖Cb-1,1-2
B-3
—)+
一κo(l,宁)+ -+^-κ-2(l + 2, 一)
CB-1,l	2	Cb-1,i+2	2
where κi(l,
B-3
—2—) is understood to be 0 for l < 0
∞	,—
X Cb-1,1 C("(x)
l=0
(ai-2CB-1，1 κ2(l - 2, B-3) + aικo(l, B-3) + UBfI κ-2(l + 2, B-3)
∖ CB-1,l-2	2	2	Cb-1,1+2	2
We can evaluate, for any a, b (possibly equal),
A(a, b; b) = E— φ(√Bea,v)φ(√Beb,：v)(eb,：v)2
= E_2 Φ(√B - 1^a√v)Φ(√B - 1 ^b,:V)(M：v)
2 B - 1
B
B — 1 X~、	1	(--3)
=-B- Σ>B-ι,ιC( 2 )(h^a,：, M：〉)
l=0
aι ( (IjCB7 κ2(l - 2, B-) + a； κ0(l, 丁) + %2'BT'l κ-2(l + 2, B-)
∖ CB-1,l-2	2	2	Cb-1,1+2	2
(49)
By Eq. (48), the eigenvalue associated with L(B - 2,1) is
Bfiμ*-1(ɪA(IJI)- ɪ
2	B — 1	B — 1
B	2
∑a0- 2 b-D/a 3； 1) + b≡^ ∑ab)
where
B - 1 *-ι
F μ
X CB-I,i(γιC(ɪ)(1) - τιC(吩)(-⅛)
l = 0
γι := aι aai-2cBTI K2(l - 2, 一) + aικ0(l, -≠) + %2'BT,l κ-2(l + 2, 一)
∖ CB-l'l-2	2	2	Cb-1,1+2	2
2(- - 1)
Tl = ~Β^Γ 7l∙

1
B - 1
Thus,
58
Published as a conference paper at ICLR 2019
______ ( B — 3)
Theorem F.22. Suppose φ(√B — 1x) has GegenbaUer expansion E∞=0 ɑι~^~1— Cl 2 (x). Then
the eigenvalue of U with respect to the eigenspace L is a ratio of quadratic forms
where
P∞=0 a2wB-1,l + alal+2uB-1,l
P∞=0 a2vB-i,i
λ↑
λL
vB-1,ι := cB-1-1,ι
wB-1,l := BiI (κ0 (l, Bf2) - B-I>B-ι,ι
2(B — 1) C( B-)( —1 )
B — 2 Cl	(B — 1)
_	l(B — 3 + l)(B — 3)
=(B - 5 + 2l)(B - 1 + 2l)
_ l(B — 3 + l)(B — 3 + 2l)
=(B — 5 + 2l)(B — 1 + 2l)
uB-1,l := -2- c CB-l_+2K--^ fl + 2,
+ cB-1-1,lκ2
(1) —
2(B — 1) C (B-3)
B—2
B-3
ɪ)(1)-
吩 )(1)- ”
B—2
力吩)(£
(B⅛)
2(B — 1) C (B^)( —1 )
B — 2 Cl	(B — 1)
⑴--。君)(U)
B - 1 ((B - 3 + I)(B - 2 + l) C(B-)	2(B — 1)C(B-)( —1 /
2 V (B — 3)(B — 1 + 2l) Il () B — 2 Cl	(B — I)J
+	(l + 1)(l + 2)	(C (B- *) _ 2(B - 1) C(/) ( —1 )ʌ A
+ (B - 3)(B — 1 + 2l)	l+2 () B — 2 Cl+2 (B - 1) .
Note that vB-l,0 = wB-l,0 = uB-l,0 = 0, so that there is in fact no dependence on a0, as expected
since batchnorm is invariant under additive shifts of φ.
We see that λ↑L ≥ 1 iff
∞
0 ≤	al2(wB-l,l — vB-l,l) + alal+2uB-l,l.
l=l
This is a quadratic form on the coefficients {al}l. We now analyze it heuristically and argue that the
eigenvalue is ≥ 1 typically When φ(√B — 1 x) explodes sufficiently as X → 1 or X → —1; in other
words, the more explosive φ is, the less likely it is to induce Eq. (43) to converge to a BSB1 fixed
point.

Heuristically, C、+2[~)(b-1i) is negligible compared to Cl(+^)(1) for sufficiently large
B and l, so that wb-i l - vb-i l ≈	(，(B-3+l))(B-：)八 -1)c-1 -∣ IC(~2~)(1)	=
- ,	- ,	(B-5+2l)(B-l+2l)	B-l,l l+2
(B-7 + O(B-2) + O(l-2)) cB-l lCι+2^)(1) and is positive. For small l, we can calculate
—2B
T+b < 0
B(B + 1)(B2 — 9B + 12)、
-2(B - 1)(B + 3)- ≥ 0，∀B ≥ 10
(B — 3)(B + 3)B2(2B3 — 19B2 + 16B + 13)、
-------6(B - 1)2(B + 1)(B + 5)-- ≥ 0，∀B ≥ 10.
wB-l,l — vB-l,l
wB-l,2 — vB-l,2
wB-l,3 — vB-l,3
In fact, plotting wB-l,l —vB-l,l for various values ofl suggests that for B ≥ 10, wB-l,l —vB-l,l ≥
0 for all l ≥ 1 (Fig. 10).
Thus, the more “linear” φ is, the larger al2 is compared to the rest of {al }l, and the more likely
that the eigenvalue is < 1. In the case that alal+2 = 0 ∀l, then indeed the eigenvalue is < 1
precisely when al2 is sufficiently large. Because higher degree Gegenbauer polynomials explodes
more violently as x → ±1, this is consistent with our claim.
59
Published as a conference paper at ICLR 2019
-l=2
—l=3
—l=4
—l=5
—l=10
—l=100
Figure 10: Plots of wB-1,l - vB-1,l over B for various l
Eigenvalue for M We use a similar technique shows the Gegenbauer expansion of the eigenvalue
for M.
Define Aab(C d) ：= Ev〜sb- Φ(√B@a,：v)O(VZB®b,：v)(®c,：v)(®d,：v). Then Aab(c, C) = A(a, b; c).
Note the ultrasymmetries Aab(c, d) = Aπ(a)π(b) (π(c), π(d)) for any permutation π, and
Aab(c, d) = Aab(d, c) = Aba(c, d) = Aba(d, c).
By Eq. (46), we have
U" }ab
=μ*-1 E_2 φ(√Bea,：v)0(VBeb,：v) (∖ 1 (®c,：v)(®d,：v) - 1 (I(C = d) - B
=μ*-1B-A(a,b； c, d) - μ*-1 11^ (I(C = d) -.
2	2B
By Thm E.62, G02 ◦ U has eigenspace M with eigenvalue
λM = U{，1 δ2}12 + U{δ2δT}12 - 4ZV{διδT}12 + 2U^{δ3δT}12
= 2(U{διδT}12 - 2U{δ1δT}12 + U{δ3δT}12)
B
-1
=------- (A12(12) - A12(13) + A12(34)).
μ
λ∙j
Thus we need to evaluate A12 (12), A12 (13), A12(34). We will do so by exploiting linear depen-
dences between different values of Aab(C, d), the computed values of Aab(C, C) = A(a, b; C), and
the value of Aab(a, b) computed below in Lemma F.23. Indeed, we have
B	B
TAab(c, d) =	E	φ(√B@a,：v)0(√B<eb,：v)(®c,：v) V'(®d,：v) = 0.
^—v	v 〜SB-2	^—↑
d=1	d=1
Leveraging the symmmetries of A, we get
λ∙j
(B - 3)Ai2(34) + 2Ai2(13) + Ai2(33) = 0
λ∙j
(B — 2)vAi2 (13) + A12(11) + A12(12) = 0.
Expressing in terms of A12(11) and A12(12), we get
Al2(13) = B=-2 (A12(11)+ A12(12))
Al2(34) = =1-(2Ai2(13) + Ai2(33))
B-3
=B-13 (B-22 (A"(I1) + A"I2)) + B=2 (ς" - 2A12(22)))
By Eq. (47)
=(B - 3-(1B - 2) ("-4A12(11)- 2A12(12)).
60
Published as a conference paper at ICLR 2019
These relations allow us to simplify
B - 1
(B — 3)μ*
(b⅛
工
百2 +
—―^A12(II) + (B - 1)A12(12)).
B — 2
By Eq.(49), We know the Gegenbauer expansion of A12 (11). The following shows the Gegenbauer
expansion of /12(12).
Lemma F.23. Suppose φ(√B — 1 x), xφ(√B — 1x) ∈ L2 ((1 — x2) B-3) ,and φ(√B — 1 x) has
(B — 3)
GegenbaUereXPanSiOn E∞id at —1 ɪ; Cl 2 ，(x). Then
/12(12) =	E	φ(a/B® 1 ∙v)φ(λ∕B)2 ∙v)(e 1∙v)(e2∙v)
V〜SB-2
∞
Eata2 + βιaι a∕+2
l=0
where
B—1
Ql =————
l B
×
CB-1,1
B
2 CB-1,ι+1
+
(B+⅛⅛)2 CB-1,l-1C-F)(B⅛)
if l ≥ 1
otherwise
B — 1
B
×
βι
2(l(B 1+(B+1)—2) CB-1,ι+1Cι(+τ3 (b⅛).
Proof. Let ψ(x) = xφ(√B — 1 x). Then by Proposition E.43, we have
∞
ψ(x) = EaI cB-1,ι
ι=0
1
B - 3 + 2/
/ + 1)C(+T 3(x) + (B + l - 4)C(BT3
X (aι-1 B⅛
ι=0
l	B + l - 3 ʌ -1 厂(B2—3 3、
+ aι+1 B + 2l _ 3 ) CB7CI	(X).
Then
B—1
/12(12) =	B	SE_2 ψ(^1jv)ψ(^2,v)
=B X (a1 —l— + aι+1 B + l-3 Y c-1 1IC(吩3 (，),
B WiI 1B + 2l — 3	ι+1 B + 2l — 3	BT,ι ι B — 1)
Rearranging the sum in terms of aι gives the result.
□
Combining all of the above, we obtain the following
( B — 3 3
Theorem F.24. Suppose φ(√B — 1x) has Gegenbauer expansion E∞°	— Cl 2 ，(x). Then
the eigenvalue of U with respect to the eigenSPace M is a ratio ofquadraticforms
2
v^∞ 一 2 .~
TI=O aι wb-1,ι + aιaι+2UB-1,ι
P∞0 a2vB-1,ι
61
Published as a conference paper at ICLR 2019
where
U .=	2(B - 1)3	((B - 3 + ∕)(B - 2 + D C(B—!)(」ʌ
BTI .	(B - 3)2(B - 2)B V B - 1 + 2/	l B - 1)
+ (B - 2)(/ +1)(B - 2 + /) C(B-!)( -1、
+ B - 1 + 2/	l+1	B - 1)
+ (/ + 3)(/ + 4)(B - 3 + 2/) C(b-3)(一λ A
+ (B + 2/ + 1)(B + 2/ + 3) l+2	B - 1J J
B-1
WBTI= (B - 3)2(B - 2)
2(B - 1)2(B + 2/ - 3)(2B/ + B + 2/2 - 6/ - 5)
B(B + 2/ - 5)(B + 2/ - 1)
-(B - 3 + 2/))C，(B-3)(b⅛)
+ (B 二 2)(B - 1)2(/ +1)2 C(b-3)(」A
+ B(B + 2/ - 1)	Cl+1	B - 1)
1 (B - 2)(B -1)2(B - 4 + /)2 C(B
+	B(B + 2/ - 5)	1-1
where C-J )(x) = 0 by
convention.
-1
B - 1
Note that UB_1,1 and WB_1,1 depends only on C∙"ɪ) (J-I) which is much smaller than
C∙ "ɪ) (1) for degree larger than 0. Thus the eigenvalue for M is typically much smaller than
the eigenvalue for L (though there are counterexamples, like sin).
F.3.3 Laplace Method
Differentiating Eq. (26) In what follows, let φ be a positive-homogeneous function of degree α.
We begin by studying dv(φ∑炉).We will differentiate Eq. (26) directly at G02{Σ*} = GΣ*G
where Σ* is the BSB1 fixed point given by Thm F.8. To that end, consider a smooth path Σt ∈
SG, t ∈ (-e, e) for some e > 0, with Σ0 = GΣ*G. Set Σ□t = ttΣt<e ∈ Sb-i, so that Σt =
eΣ□teτ and Σ□0 = m*/b—1 where μ* = Kα,b (Jφ(1) - Jφ( B-⅛)) as in Thm F.8. If we write Σ□
.
and Σ for the time derivatives, we have
62
Published as a conference paper at ICLR 2019
B-αΓ(α) d V(0on)((E^D t@T )
t=0
B-αΓ(α) A Vb. (e∑□teτ)
Z∞ ds SaT
0
t=0
d det(I + 2s∑□t)T/2
× Vφ (®∑□ 0(I + 2s∑□0)TeT)
t=0
+ det(I + 2s∑□0) 1/2dtVφ(e∑□t(∕ + 2s∑□t) IeT)
Z∞ ds SaT
0
τ1⅛det(I+2s∑□ 0)-1/2 tr(∑ □0)×
t=0
*	∖a
i+⅛) vφ(G)

+ det(I + 2s∑□0) 1/2”
d∑
Σ=Θ∑□0 (I+2s∑□0 )-1 (BT
{e (I + 2s∑□0)-1∑ □0(I + 2s∑口0)T(BT }
∕∞ ds SaT
0
(apply Lemma F.26 and Lemma F.27)
* ɑ
(1+2SMT(BT)/2 tr (∑□0) × (τ⅛) vφ(G)

I (1 + 2sμ*)-(BT)/2
μ*	α-1 dVφ(∑)
1 + 2sμ*
dΣ
{(1 + 2sμ*)-2 e∑ □ 0e T )
Σ=G I	J
(using fact that dVφ∕d∑ is degree (α — 1)-positive homogeneous)
∞	∖
(1 + 2s〃*)-B--1-aμ*asa)tr (∑□0) Vφ(G)
∞(1 + 2sμ*)-B--1-aμ*a-1sa-1) d^ 1 G 卜∑□0<≡T }
2-1-a Beta (B-1, α + Dtr (∑ 口。) V (G)
+ 4*-12-a Beta (B+1, α) d^P
2	d∑
{∑ 0}
Σ=G '	'
(apply Lemma F.28)
if b21 + 2 > α (precondition for Lemma F.28).
With some trivial simplifications, We obtain the following
Lemma F.25. Let φ be positive-homogeneous of degree a. Consider a smooth path ∑t ∈ SB with
∑0 = μ*G. If 号L1 I 2 > α,then
dV(φ°l1)(Σ)
t=0
dΣ
(∑ 0}
Σ="*G
.
-αBaμ*
-12-1-α P B-I
+ Baμ*-12-aP
2
B +1
2 :
,α + 1)	tr (∑□0) Vφ(G)
“1 *L n∑ 0}
(50)
where P (a, b) = Γ(a + b)∕Γ(a) is the Pochhammer symbol.
Lemma F.26. For any S ∈ R,今 det(I + 2s∑□ )-1/2 = —s det(I + 2s∑□)-1/2 tr((I +
2s∑□)-1d∑□ /dt).
63
Published as a conference paper at ICLR 2019
For ∑□o = μI, this is also equal to 一耳端 det(I + 2s∑□)-1/2 tr(dd∑□) at t = 0.
Proof. Straightforward computation.	□
LemmaF.27. Forany S ∈ R, d∑□(I + 2s∑□)-1 = (I + 2s∑□)智(I +2s∑□)-1.
Proof. Straightforward computation.	□
Lemma F.28. For a > b +1, R∞(1 + 2sμ)-asb ds = (2μ)-1-b Beta(b +1,a — 1 — b).
Proof. Apply change of variables X = 2μ^-.	□
1+2μs
This immediately gives the following consequence.
Theorem F.29. Let φ : R → R be any function with finite first and second Gaussian moments. Then
G02 ◦ dVφ ∣∑=BSBi(a b) : HB → HB has thefollowing eigendeCOmPOsition:
•	RG has eigenvalue (B-I)(B-2V)+w (dimRG = 1)
•	MB has eigenvalue w (dim MB = B(B — 3)/2)
•	LB has eigenvalue (B-2)(UB 2v)+2w (dimLB = B — 1)
where
U = ∂Vφ(Σ)11	V = ∂ Vφ(Σ)12	W = ∂ Vφ(Σ)12
∂	Σ11	∣Σ=BSB1(a,b)	∂Σ11	∣Σ=BSB1(a,b)	∂Σ12 ∣Σ=BSB1(a,b)
Theorem F.30. Let φ : R → R be Positive-homogeneous of degree α. Then for any p 6= 0, c ∈ R,
G02 ◦ dd∑Φ ∣∑=bsbi(p Cp) : HB → HB has thefollowing eigendeCOmPOSition:
•	MB has eigenvalue cαpα-1J0φ (c)
•	LB has eigenvalue CapaTB ((B - 2)ɑ [Jφ(1) — Jφ (c)] + (2 + C(B — 2))Jφ (C))
•	RG has eigenvalue CapaTBB ((B - 1)ɑ [Jφ(1) — Jφ (c)] + (1 + C(B — 1))Jφ (C))
Proof. By Proposition E.22, dd∑φ ∣∑=BSBi(p cp) is DOS(u, v, W) with
∂Vφ(Σ)11 ∣∣
U = -∂∑
∂Σ11	Σ=BSB1(p,cp)
= Caαpa-1Jφ(1)
V	= ∂ Vφ(Σ)12
∂Σ11	∣Σ=BSB1(p,cp)
=	1 CaςF-F; (αJ φ(Cij ) - cij Jφ(cij ))
=JCapaT (°jφ (C)-CJφ (C))
∂Vφ(Σ)12 ∣∣
W = -^ς-------
∂Σ12	Σ=BSB1(p,cp)
(a-1)/2 (a-1)/2 0
=	Ca Σii	Σjj	Jφ (Cij)
=	Capa-1J0φ (C)
With Thm E.73, we can do the computation:
64
Published as a conference paper at ICLR 2019
•	MB has eigenvalue w = cαpα-1J0φ (c)
•	LB has eigenvalue
(U — 2v)(B — 2) + 2w
B
=BB ((B — 2) (CaapaTJφ(1) — CɑPα-1 (αJφ (c) - cJφ (C))) + 2cαPα-1Jφ (C))
=CapaTB ((B - 2)a [Jφ(1) — Jφ (C)] +(2 + c(B — 2))Jφ (c))
•	RG has eigenvalue
(B — 1)(u — 2v) + W
B
=B ((B — 1) (CaapaTJφ(1) — Capa-1 (aJφ (C)-CJφ (C))) + CapaTJφ (C))
=Capa-I B ((B - 1)a Jφ(I)- Jφ (C)] + (1 + C(B — 1))Jφ (C))
□
We record the following consequence which will be used frequently in the sequel.
Theorem F.31. Let φ : R → R be positive-homogeneous of degree a. Then G02 ◦ dV∑Φ ∣ς=g :
HBG → HBG has the following eigendecomposition:
•	MB has eigenvalue λM,φ(B, a) := Ca (B-T)a 1 Jφ (B-T
•	LB has eigenvalue λLG,φ(B, a)
•	RG has eigenvalue λGG,φ (B, a) := Ca
(B-I)a 1 BB ((B - 2)a Jφ(I) - Jφ (b⅛)] + B-TjΦ
，(⅛1 )a a hjφ(I) - Jφ (B⅛)i
B⅛
Proof. Plug inp = B-1 and C = —1/(B — 1) for Thm F.30.	□
Theorem F.32. Let φ : R → R be a degree a positive-homogeneous function. Then for p 6= 0, C ∈
R, dV∑φ∣∑=BSB1(ρ Cp) ： HB → HB has thefollowing eigendecomposition:
•	MB has eigenvalue Capa-1Jφ (c)
•	LB (a,b) has eigenvalue Caapa-1Jφ(1) where a = 2 Jφ (c) — aJφ(1)) and b =
aJφ (C) — CJ0φ (C)
Proof. Use Thm E.72 with the computations from the proof of Thm F.30 as well as the following
computation
w — u = Capa-1J0φ (C) — Caapa-1Jφ(1)
=Capa-I (jφ (C)- ajφ(I))
V = 2Capa-I (ajφ (C)-CJφ (c))
□
Theorem F.33. Let φ be positive-homogeneous with degree a. Assume B-1 > a. The operator
U = G02 ◦ dV(IdC∑Gς )∣∑G=μ*G ： HB → HB has 3 distinct eigenvalues. They are as follows:
65
Published as a conference paper at ICLR 2019
1.	λ↑G(B, α) := 0 with dimension 1 eigenspace RG.
2.
λM(B, α) := Bαμ*T2-αP (B+1, α) ∖Mφ(B, α)
= CaB(B - 1)α-1μ*-12-αP (B+ɪ,α) 1 Jφ (E)
with dimension B(,-3)eigenSPaCe M.
3.
λL(B,α) := Ba4*-12-αP (B+1 ,α
ca(B - 1)α-1μ*"12"αP
-2)α Jφ(1) - jφ
)λfφ(B,α)
(号I
十生jφ (E
with dimension B — 1 eigenspace L.
Proof. Item 1. The case of λ^(B, α). Since (φ ◦ n)(μ*G) = (φ o n)(μ*G + ωG) for any ω, the
operator d"d∑炉)∣∑g=** G sends G to 0.
Item 2. The case of λm(B, α). Assume Σ0 ∈ M, Thm F.31 gives (with Θ denoting Hadamard
product, i.e. entrywise multiplication)
等界	{Σ 0} = λM,φ(B,α)Σ 0
qς	Σ=G
=Ca (彳 O % ( E ) ∑O
Since tr(∑0) = 0, Eq. (50) gives
G02 o
dV(φ°n)(ΣG)
-dΣG
.
(Σ 0}
Σg =μ* G
G02 0 dt v(φon)(Σt)
.
(Σ 0}
t=0
Bαμ*-12-αP (B+1 ,α) G02 0 dvφ(Σ)	∣Σ0O
∖ 2	)	dΣ ς=g I J
Ba4*-12-aP (*,α) ICa (BBI) " (B-I) G02{Σ0}
CaB(B - 1)aT4*-12-aP (B+1, α) 1 Jφ (B-T) Σ0
So the eigenvalue for M is λ[ = CaB(B - 1)a-1μ*-12-aP (B+1, α) 1 Jφ (β⅛
Item 3. The case of λ1(B, α). Let Σ0 = L(B - 2,1). Thm F.31 gives
G02 0 dvdΣ≡	{Σ0} = λG,φ(B,α)Σ0
dΣ Σ=G
66
Published as a conference paper at ICLR 2019
where
λfφ(B,α):
Jφ⑴-Jφ (b^⅛)
Since tr(Σ0) = 0, Eq. (50) gives
GW2。
dV 即 n)(∑G)
dΣG
.
(Σ 0}
∑G=μ*G
G02 ◦ %V(φθn)(∑t)
Bαμ*-12-aP
Bαμ*-12-aP
t=0
b+i ,α"2
B+1 ,α) 1 λ∑ 0
Z dVφ(Σ)
。d∑
{∑ 0}
Σ=G J
(B - 1)α-1cαμ*-12--P B+p-,α
×
-2)a jφ(I)- jφ
+ BBI% (B⅛
.
X ∑ 0
So λL = (B-I)α-%α〃*τ2-αp (B+1 ,a)	((B - 2)a jφ(I)- jφ (B-T)] + B¾ jφ (B⅛)
□
With some routine computation, we obtain
Proposition F.34. With φ and α as above, as B → ∞,
λL(B，°)~ ɑ + B F" - 4α2 - ɑ + I/])) + O(B-"
λM(B，a)〜Jφ(1)φ- Jφ(0)
+ B- /(202-40 + 1)	"___________J(0)
+	+ J Jφ(1) - Jφ(0)	Jφ(1) - Jφ(0)
+ O (B-2)
(jφ(0)	Y!
Jφ(1) - Jφ(0)7
We can compute, by Proposition E.23,
Jφ(0)
jφ(1) - jφ(0)
(a + b)2 1 r( 2+1)2
(O + b) √ r(α+ 2)
≤
(a2 + b2)-(a- b)2号 ⅞W
2γ( 2 + 1)2
√πΓ (α + T)
where the last part is obtained by optimizing over a and b. On α ∈ (-1/2, ∞), this is greater than
α iff ° < 1. Thusfor α ≥ 1, the maximum eigenvalue of U is always achieved by eigenSPaCe L for
large enough B.
G Backward Dynamics
Let,s extend the definition of the V operator:
67
Published as a conference paper at ICLR 2019
Definition G.1. Suppose Tx : Rn → Rm is a linear operator parametrized by x ∈ Rk . Then for a
PSD matrix Σ ∈ Sk, define Vt(Σ) := E[TX 0 TX : X ∈ N(0, Σ)] : Rn×n → Rm×m, which acts on
n × n matrices Π by
VT (Σ){Π} = E[TXΠTXT : x ∈ N (0, Σ)] ∈ Rm×m.
Under this definition, VB0 (Σ) is a linear operator RB×B → RB×B. Recall the notion of adjoint:
Definition G.2. If V is a (real) vector space, then V*, its dual space, is defined as the space oflinear
functionals f : V → R on V. If T : V → W is a linear operator, then Tt, its adjoint, is a linear
operator W * → V t, defined by T * (f) = v → f (T (v)).
If a linear operator is represented by a matrix, with function application represented by matrix-
vector multiplication (matrix on the left, vector on the right), then the adjoint is represented by
matrix transpose.
The backward equation. In this section we are interested in the backward dynamics, given by the
following equation
Πl = VBφ0 (Σl)*{Πl+1}
where Σl is given by the forward dynamics. Particularly, we are interested in the specific case when
we have exponential convergence of Σl to a BSB1 fixed point. Thus we will study the asymptotic
approximation of the above, namely the linear system.
∏l = VBφ (Σ*)t {∏l+1}	(51)
where Σ* is the BSB1 fixed point. Note that after one step of backprop, ΠL-1 = Vbiφ (Σ*)t{∏L}
is in HG. Thus the large L dynamics ofEq. (51) is given by the eigendecomposition of NB，$ (Σ*)t[
HG : HG → HG. It turns out to be much more convenient to study its adjoint G02 ◦ VBψ(∑*),
which has the same eigenvalues (in fact, it will turn out that it is self-adjoint).
G.1 Eigendecomposition of G02 ◦ Vb>φ(Σ*).
By chain rule, Bφ0 (x)
Thus
dφon(z)
dGX
dz z=Gx dx
FLGXG, Bφ (x)δx
FLgxg^x
VBφ (∑)= E [Bφ(x)02 : X 〜N(0, ∑)]
dφ ◦ n(z)
dz
02
z=GxG
:X 〜N(0, Σ)
E dφ ° n(z)
dz
02
◦ G02 : x 〜N(0, Σ)
z=GX
dφ ◦ n(z)
dz
02
:x 〜N(0, Σ) ◦ G
02
z=GX
E
E
V[x→dφ→z)I g ](∑) ◦ G02
If we let F(∑) := V,→ dφ°n(z) ∣	](Σ), then Vb； (Σ) = F(Σ) ° G02. As discussed above, we
will seek the eigendecomposition of G02 ◦ F(Σ) HBG : HBG → HBG.
We first make some basic calculations.
Proposition G.3.
dn(z)∣	= √Br-i(I - vvT
Z ∣z=y
where r = ∣∣yk,v = y/kyk = n(y)∕√B.
),
68
Published as a conference paper at ICLR 2019
Proof. We have 当了 I	= √B dz/kzk I , and
z=y	z=y
∂ 3/kyk)=-—(训 y∣∣∕¾7 )y
∂y =	kyk2
δij	yi ∂√y∙y
=----------Z-----
kyk	kyk2 ∂y
δij	yi yj
=----------Ξ---
kyk	kyk21训
δij	yiyj
=------------
kyk	kyk3
so that
生M	= r-i(I-vvT).
dz
□
By chain rule, this easily gives
Proposition G.4.
dφ ° n(Z)	= √BDr-i(I-vvT),
Z	z=y
where D = Diag(φ0(n(y))),r =IlyIl,v = y∕∣∣y∣∣ = n(y)∕√B.
With V = y∕∣∣y∣∣, r = ∣∣y∣, and D = Diag(φ0(n(y))), We then have
F(Σ) = E D02 O (√B dzd∣z∣ I	)18 : X 〜N(0, ∑)
/	I ∖ 02
=E D02 O ( √B dzd∣z∣ ∣	)	: y 〜N(0, G∑G)
=B E 卜-2 (D02 + (Dvvt)02 - D ㊈(Dvvt) - (Dvvt)㊈ D) : y 〜N(0, GΣG)]
(52)
F(Σ){Λ} = B E 卜-2 (DΛD + DvvtAVVTD - DΛvvtD - DvvtΛD) : y 〜N(0, GΣG)]
G.1.1 Spherical Integration
When Σ = Σ* is the BSB1 fixed point, one can again easily see that F(Σ*) is ultrasymmetric. With
Tij = 1 (δiδj + δjδi) (Defn F.20), we have
F(Σ*){Λ} = B E [r-2 (DΛD + DvvtΛvvtD - DAVVTD - DvvtΛD) : y 〜N(0, n*G)]
=μ*-1B E [r-2 (DλD + DvvtΛvvtD - DAVVTD - DvvtΛD) : y 〜N(0, G)]
F (Σ*){τj }kl = μ*-1B	E	[r-2 fφ0(√Bvfc )φ0(√Bvl) I(k = i & I=)； I(k = j&「)
rv〜N(0,G) |_	∖	2
+ ViVjVkφ0(√Bvk )vιφ0( WBVI)
-2(I(i = k)φ0(√Bvk)Vjvιφ0(√Bvι) + I(j = k)φ0(√Bvk)viVιφ0(√Bvι)
+ I(i = ∕)φz(√Bvι)vj Vk φ0(√Bvk) + I(j = l)φ0(√Bvι )viVkφ0(√Bvk))
μ*-1B E((B - 3)/2) 2-2∕2π-2 Γ d& ∙ ∙ ∙ Γ dθ4 f (v? ,..., vf) SinB-3 θ1 ∙∙∙ SinB-6 θ4
μ	Γ((B - 5)/2)	J0	,J0	i J(I, , 4)	1	4
μ*-1B(B - 5)(2π)-2 / dθ1 ∙ ∙ ∙ ∏ dθ4 f (v?,…，vf) SinB-3 θ1 ∙∙∙ SinB-6 θ4
0	0
69
Published as a conference paper at ICLR 2019
by Lemma E.35, where we assume, WLOG by ultrasymmetry, k, l ∈ {1, 2}; i, j ∈ {1, . . . , 4}, and
f(v1,...,v4) := 1 φ0(√Bev)kφ0(√Bev)1(
I(k = i & l = j) + I(k = j & l = i) + 2(ev)i(ev)j(®v)k(ev)ι
-(I(i = k)(ev)j(®v)ι + I(j = k)(ev)i(ev)ι + I(i = l)(ev)j(®v)k + I(j = l)(ev)i(ev)k)
v1θ = cos θ1
v2θ = sin θ1 cos θ2
v3θ = sin θ1 sin θ2 cos θ3
v4θ = sin θ1 sin θ2 sin θ3 cos θ4 .
ande as inEq.(33).
We can integrate out θ3 and θ4 symbolically by Lemma E.29 since their dependence only appear
outside of φ0. This reduces each entry of F(Σ*){τj}kι to 2-dimensional integrals to evaluate nu-
merically. The eigenvalues of G02 ◦ F(Σ*) can then be obtained from Thm E.62. We omit details
here and instead focus on results from other methods.
G.1.2 Gegenbauer Expansion
Notice that when A = G, the expression for F(Σ*){Λ} significantly simplifies because G acts as
identity on v :
F (∑*){G} = μ* 1B E r-2 IDAD + D VvT GvvT D - D GvvT D - D VvT GD∖ : y 〜N (0, G)
=μ*-1B E 卜-2 (DGD - DVvTD) : y 〜N(0, G)]
Then the diagonal of the image satisfies
F (∑*){G}aa
μ*-1
B E r-2
B-1 φ0(√Bva)2 - vaφ0(√Bvα)2) ： rv 〜N(0, G)
μ*-1B(B - 3)-1	E	(⅛1 φ0(√Bea,:v)2 - (®a,:v)2φ0(√Be。,：。)[ K(v; G, 2)
V〜SB-2 ∖ B
(by Proposition E.33)
B - 1 .	,---- C
μ*-1B(B - 3)-1 v 〜E一(1 - (^a,M2) -B- φ0( √B-I ^a,M2
(K(v; G, 2) = 1)
μ*-1(B - 1)(B - 3)-1 v〜E jI - (^ajv)2) φ0(√B-!^a,:v)2
70
Published as a conference paper at ICLR 2019
And the off-diagonal entries satisfy
F A){G}ab
μ*-1B E r
-(eo√v)3b,:v)Φ0(√Ba,：V)d(eo,：v)] : rV 〜N(0, G)]
μ*-1B(B - 3)-1v E一 (-1 φ(√B≡Γ^a,v)φ,(√B≡Γ^b,:V)

B - 1......................   .	I---	∖
B	(^o√v)(^b,MΦ (VZB - 1 ^α,:v)φ (√B - 1 ^b,e), K(V G, 2)
(by Proposition E.33)
μ*-1B(B - 3)-1	E 万(I + (B -I)(Ja,：V)(M:V))
V〜SB-2 B
φ!(√B - 1 ^a:v)φ0(√B - 1 ^b,:V)
(K(v; G, 2) = 1)
-μ*-1 (B - 3)-1 v IE_2(1 + (B - 1)(仓即：V)(M：v)) φ(√B-16。,并)”(√B-1M：V)
Lifting Spherical Expectation. We first show a way of expressing F(∑*)αα in terms of Gegen-
bauer basis by lifting the spherical integral over SB-I2 to spherical integral over SB. While this
technique cannot be extended to F(Σ*)αb, a = b, we believe it could be of use to future related
problems.
(B — 3
If	φ(√B	-	1x)	has Gegenbauer expansion	P∞o aι- 1ι ；	C；	2	(x),	then	φ0(√B	-	1x)	=
√b P∞o αιc⅛C(ɪ)0(x) =√B-1 P∞0 a"C学 )(x), so that φ0(√Bea,：v)=
—1— p∞ aι B-3 C(B2-1)( v) = —1— p∞ a(B - 3) CB+1,lτ ZB，(lT)(V) Here and
√B-1 乙l=0 alCB-ι,ι	CIT	(e。,：V)=	√B-Γ 乙l=0 al(B	3)	cb-i,i	Z^a,:	(V).	Here	α,:	and
v are treated as points on the B-dimensional sphere with last 2 coordinates 0.
71
Published as a conference paper at ICLR 2019
Thus by Lemmas E.38 and E.39,
F (∑*){G}aa
广1 ωB-(B - 1)(B
3B—2
-3)-1 / dh (1 - h2)B- (1 - h2)φ0(√B-Γh)2
广1 ωB-(B - 1)(B
ωB-2
-1 ωB-3ωB B
μ ---------(B -
ωB-2ωB-1
-3)
1)(B
ZI
dh (1 - h2)⅛2φ0(√B — 1 h)2
1
-3)-1 E φ (√B-1 gw)2
W〜SB
-1 ωB-3ωB	(P
μ ----------(B -
3B—23B-1
1)(B
-3)-1
∞	\ 2
X/B - 3)cb+1^ZJyIT)(W)
士	CB-1，1 y
1
E l ,______
W 〜SB ∖ B B — 1
∞	/	\ 2
〃*-1 ω≡(B -3) X。2 (—) cb+i，i-i
B
∞
〃*-1 ;(B -1) X。2
CB-1,l
by LemmaE.45
1
μ*-1(B -
μ*-1(B -
、2 2 1	(l + B - 3
2)Xa2E l--
2) X a—(l + B - 3
士	CB-H B - 2
μ*-1
μ*-1
∞
X α2
l=0
∞
X α2
l=0
l + B - 3 (l + B - 4
l + B - 3 l (l + B - 4
CB-ι,ι	B — 3 1 B — 4
CB-1,l
B - 3
Dirichlet-Iike form of φ. For F(Σ*){G}αfc, there,s no obvious way of lifting the spherical expec-
tation to a higher dimension with the weight (1 + (B - 1)(白。，卫)(白仇卫)).We will instead leverage
Thm E.47. In Thm E.47, setting u1 =^。，：=u2, We get
B -1	0	.	,---- C
F (∑*){G}aa = 〃*-1 B—3 VJB-2((^a,: ∙ ^a,: ) - (^a,:。产)“(√B≡Γ ^a,v)2
μ*-1
∞
X α2
l=0
l⅛2/Cb-1,iC B-)(1).
Likewise, setting u1 =6a，：，u2 = M：, We get
一一	TB - 1	................... . I-------
F(Σ*){G}ab = μ*-1 B-3 V〜E_2(侬a，： ∙瓯，：)一(LV)(M:。))“(√B—I^a∕)φ'(VB-M：。)
= "*τ X a2⅛⅛lcB-1，lCl(B-3) (B⅛)
l=0
Thus,
Theorem G.5. Suppose φ(√B — 1x),φ0(√B — 1 x) ∈ L2((1 — x2)B-) and φ(√B — 1 x) has
(B — 3)
Gegenbauer expansion E∞° alCB 111C 2 ,(x)∙ Thenfor Σ* being the unique BSB1 fixed point
72
Published as a conference paper at ICLR 2019
of Eq. (43), the eigenvalue of G02 ◦ F(Σ*) corresponding to eigenspace RG is
I	*_ι	2 / + B — 3 -I
λG = μ	2_^ al _B - 3 lcB-1,l
l=0
P∞=0 a2⅛⅛3MB-U
2
2
-1
B - 1
B-2
B-3 .
> 1
This is minimized (over choices of φ) iff φ is linear in which case λG
G.2 Laplace Method
In this section suppose that φ is degree α positive-homogeneous. Set D = Diag(φ0(y)) (and re-
call v = y∕∣∣y∣∣,r = IIyk). Then D is degree Q - 1 positive-homogeneous in x (because φ0 is).
Consequently We can rewrite Eq. (52) as follows,
F (Σ*)
=Ba E	[r-2f r-2(α-1)D02 + r-2(a+1)(DVVT )02
y〜N(0,GΣ*G) L	∖
-r-2αD ㊈(Dvvτ) - r-2a(Dvvτ)㊈ D)
=Ba E	∣r-2aD02 + r-2(a+2)(DyyT)02 - r-2(a+1) (D ③(Dyyτ) + (DyyT)乳 D)]
y〜N(0,μ*G) L
=Ba (A + B - C)	(53)
where Σ* is the BSB1 fixed point of Eq. (43), GΣ*G = μ*G, and
A = E [r-2aD02 : y 〜N(0, n*G)]
B = E [r-2(a+2)(DyyT)02 : y 〜N(0,4*G)]
C = E [r-2(a+1) (D 乳(DyyT) + (DyyT)乳 D) : y 〜N(0, n*G)]
Each term in the sum above is ultrasymmetric, so has the same eigenspaces RG, M, L (this will also
become apparent in the computations below without resorting to Thm E.61). We can thus compute
the eigenvalues for each of them in order.
In all computation below, we apply Lemma E.2 first to relate the quantity in question to Vφ.
Computing A. For two matrices Σ, Λ, write Σ Θ Λ for entrywise multiplication. We have by
Lemma E.2
A{Λ} = E [r-2αDΛD : y 〜N(0, n*G)]
=E [r-2aΛ Θ φ,(y)φ,(y)τ : y 〜N(0, 4*G)]
Γ(α)-1
Γ(α)-1
d ds SaT det(I + 2s〃*G)-1/2A Θ V” (—匕—G )
0	1 + 2s〃* J
「 dssa-1(1 + 2sμ*)-(BT)∕2Λ Θ ()“ ɪ v0,(G)
0	∖1 + 2sμ )
= Γ(α)-1Λ Θ Vφ(G) j ds (μ*s)a-1(1 + 2sμ*)-(BT)/2-a+1
0
=Γ(α)-1Λ Θ Vφ(G) Beta (B-^,°) μ*-12-a
=Λ Θ Ca-1 (BBI)a 1 BSB1 (Jφ(1), Jφ,(B-1)) P (，,α) 1 μ*-12-a
Then Thm E.73 gives the eigendecomposition for BaG02 o A『HG
73
Published as a conference paper at ICLR 2019
Theorem G.6. Let RBa = (2α-1)ca-ιB(B-1)a-1P (B-3,α)-1 2-a. Then BaG。2◦A『HG
has the following eigendecomposition.
λG =…一 (BB1 Jφ(1) + N (B--I))
(2α + B — 3) (α02(B — 1)Jφ ⑴ + (2α — 1)Jφ (I-B))
(2α -I)(B - 3)(b - 1) (J(I)- J (l⅛))
λA = RB,aμ*τ (BB2Jφ(1) + BJ (B⅛))
(2α + B - 3) (α2(B - 2)Jφ (1) + 2(2α - 1)Jφ (τ⅛))
(2α -I)(B - 3)(B -I)(J(I)- J (l⅛
λM = RB,aμ* 1Jφ (B - J )
B(2α + B -
τ⅛
(B - 3)(B -I)(Jφ⑴-Jφ (τ⅛
Proof. We have BaA = (2α - 1)-1RB,αμ*-1DOS J。，(1),0, Jφ0 (b⅛)J . This allows Us to
applyThmE.73. We make a further simplification J。，(c) = (2α-1)Jφ(c) via Proposition E.22. □
Computing B. We simplify
B{A} = E |r-2(a+2)DyyTAyyTD : y 〜N(0, μ*G)]
=E hr-2(a+2)ψ(y)yτAyψ(y)τ : y 〜N(0, μ*G)]
where ψ(y) := yφ0(y), which, in this case of φ being degree α positive-homogeneous, is also equal
to αφ(y).
By Lemma E.2, B{A} equals
Γ(α + 2)-1 / ds sa+1 det(I + 2sμ*G)-1∕2 E[ψ(y)yτAyψ(y)τ : y 〜N(0, —μ--G)]
Jo	1 + 2sμ
This expression naturally leads us to consider the following definition.
Definition G.7. Let ψ : R → R be measurable and Σ ∈ SB. Define Vψ(4)(Σ) : HB → HB by
Vψ4)(∑){A} = E[ψ(y)yτAyψ(y)τ : y 〜N(0, Σ)]
For two matrices Σ, A, write hΣ, Ai := tr(ΣT A). We have the following identity.
Proposition G.8. vφ4)(Σ){A} = Vφ(∑)h∑, Ai + 2dVφ(≡{ΣAΣ}.
Proof. We have
*{A} = (2∏)-B∕2 /
=(2π)-B/2 /
dzφ(z产(d∑det(Σ)T∕2e-2ZT…){A}
dz φ(z产
-21det - + de6T2 TTzzT ς-1
e-1ZTς-1z, A
=2(2π)-B/2 detΣ-1∕2 /dz φ(z产e-1ZTς-1z [{zzτ, Σ-1AΣ-1i - h∑-1, A〉]
=1vφ4)(∑){∑-1A∑-1}- 1vφ(∑)h∑-1, A).
Making the substitution A → ΣAΣ, we get the desired result.
□
74
Published as a conference paper at ICLR 2019
If 夕 is degree α positive-homogeneous, then V(4) is degree α + 1 positive-homogeneous. Thus,
B{Λ} = Γ(α + 2)-1
=Γ(α + 2)-1
I
0
/
0
ds sα+1 det(I + 2s〃*G)T/2VS)
ds sα+1(1 + 2sμ*)-(BT)/2
( μ
1 + 2sμ*
μ*	α+1
1 + 2sμ*
G {Λ}
VS)(G){Λ}
Γ(α + 2)-1vψI)(G){Λ} L ds (μ*s)α+1(1 + 2sμ*)-(BT)/2-a-1
Γ(α + 2)-1V,(G){Λ} Beta(B-3, α + 2)2-α-24*-1
P (—，° + 2)1 2-α-2μ*-1V,(G){Λ}.
By Proposition G.8, VS)(G){Λ} = Vψ(G) trΛ + 2dV^') {Λ} if Λ ∈ Hg. Thus
Theorem G.9. BdGgi2 ◦ B『HB has the following eigendecomposition (note that here μ* is still
with respect to φ, not ψ = ɑφ)
1.	Eigenspace RG with eigenvalue
2
λB = B⅛
2.	Eigenspace L with eigenvalue
λB ：= BaP (F, α + 2)1 2-α-24*-12λB,ψ(B, α)
=	2α3(B - 2)	202BJΦ (B⅛)
(B - 3)(B - I)(B - 1 + 2α)	(B - 3)(B - 1)2(B - 1 + 2α)(Jφ⑴-Jφ (B⅛
3.	Eigenspace M with eigenvalue
λM := BaP (BIP,α + 2)	2-a-2μ*-12λM,ψ(B, a)
2α2BJφ (B⅛)
(B - 3)(B -I)(B - 1 + 2α)(Jφ(I)- jφ (B-1i))
75
Published as a conference paper at ICLR 2019
Proof. The only thing tojustify is the value of λθ. We have
λ∣ = BaP (B-^,α + 2)	2-a-2μ*-1
BαP
× I (B -I)Ca
B-3
F- ,α + 2
B - 1Y
~B~ )
—1
2—a—2
jΨ(I)- jψ (B - 1)) + 2λM,ψ(B, a)
μ*-I(B - 1 + 20)Ca (-B-) (jΨ(1) - jΨ (B - 1
Ca(B - 1)α(B - 1 + 2α)P
B-3
F- ,a + 2
—1
1 2-a―2μ *-1 (Jψ(1) - Jψ (三
Ca(B -1)0P
τ~，a+1	2
-α-1μ*-1 Jψ⑴-Jψ
-1
B - 1
B - 3
B - 3
Ca(B - 1)aP
— a— 1 7— 1 - 2
Ka,B a
工-，0 + 1	2
(since Jψ = 02 Jφ)
α2Ca(B - 1)aP (b-3, α + 1) T 2-a-1
CaP (⅞1 ,α)-1 (b-1 )a
(Defn F.6)
α22-1
(B - 3)/2
02
B-3
□
Computing C. By Lemma E.2,
C{Λ} = E 卜-2(a+1)(DAyyTD + DyyTΛD) : y 〜N(0, n*G)]
Γ(α + 1) -1 / ds sa det(I + 2sμ*G)-1/2 E[DΛyyTD + DyyTΛD : y 〜N(0,
.*
μ
1 + 2sμ*
G)]
「(。+ 1厂1/dssa (I+ 2s“* 厂(BT)"
*
μ
1 + 2sμ*
a
E[DΛyyτD + DyyTΛD : y 〜N(0, G)]
Γ(α + 1) -1 / ds (μ*s)a(1 + 2sμ*)-(BT)/2—a E[DΛyyTD + DyyTΛD : y 〜N(0, G)]
Γ(α + 1)-1 Beta
B-3
2
-1
,α + 1 2-a-1〃 * — 1 E[DΛyyTD + DyyTΛD : y 〜N(0, G)]
P (By^,a +1
2-a-1〃 * — 1 E[DΛyyTD + DyyTΛD : y 〜N(0, G)]
Lemma G.10. Suppose φ is degree α positive-homogeneous. Thenfor Λ ∈ Hb ,
E[D 0 (DyyT) + DyyT 0 D : y 〜N(0, Σ)] = α
dVφ(ΠΣΠ)
dΠ
Π=I
E[DΛyyτD + DyyTΛD : y 〜N(0, Σ)] = α
dVφ(Σ)
dΣ
(ΣΛ + ΛΣ}
where D = Diag(φ0(y)).
76
Published as a conference paper at ICLR 2019
Proof. Let ∏t, t ∈ (-e, e) be a smooth path in HB with ∏0 = I. Write Dt = Diag(φ0(∏ty)), so
.
that D0 = D. Then, using ∏t to denote t derivative,
dVφ(∏t∑∏t) = d E[φ(∏ty)φ(∏ty)τ : y 〜N(0, Σ)]
dt	dt
=E[Dt∏tyφ(∏ty)τ + φ(∏ty)yτ∏tDt : y 〜N(0, Σ)]
dVφ(∏t∑∏t)	= E[D∏0yφ(y)τ + φ(y)yτ∏°D : y 〜N(0, Σ)]
dt	t=0
Because for x ∈ R, ɑφ(x) = xφ0(x), for y ∈ RB We can write φ(y) = α-1 Diag(φ0(y))y. Then
dVφ(Σ)	.	. dVφ(Σ) (d∏t∑∏t I
α	{Σχι 0+∏ 0Σ} =α I
=ɑ 用Vφ(∏tΣ∏t)
dt	t=0
=E[D∏0yyτD + Dyyτ∏°D : y 〜N(0, Σ)]
□
Therefore, for Λ ∈ HG,
C{Λ} = P (B-3,α + 11 2-α-1〃*Tα dV(Σ2 I	{GΛ + ΛG}
∖ 2	/	dΣ I Σ=G
2αP (',α +I)—/——*|	{Λ}
∖ 2	/	dΣ Σ=G
So Thm F.31 gives
Theorem G.11. BaG02 o C has thefollowing eigendeCOmPOSitiOn
1.	eigenspace RG with eigenvalue
λG ：= 2BαaP (B-3, α + 1)	2-α-1μ*-1 ∖Gφ(B, α)
2α2
=B - 3
2.	eigenspace L with eigenvalue
λC := 2BαaP (IB-3, α + 1)	2-α-1μ*-1λG'φ(B, a)
=2α2(B - 2)	2aBjφ (b⅛)
一(B - 3)(B - 1) + (b - 3)(B - 1)2(Jφ(1) - Jφ (b⅛))
3.	eigenspace M with eigenvalue
λM := 2BαaP (BIP, a + 1)	2-α"1μ*-1λM,φ(B, a)
2aB jφ (Bi⅛ )
(B - 3)(B - 1)(jφ(1) - jΦ (B⅛)
Altogether, by Eq.(53) and Thms G.6, G.9 and G.11, this implies
77
Published as a conference paper at ICLR 2019
Theorem G.12. G02 ◦ F(∑*) : HB → HB has eigenspaces RG,M,L respectively with the
following eigenvalues
I	(B - 3 + 2a) ((2a -I)Jφ (B-⅛) + α2(B -I)Jφ(I
'g	7	∖
(2α -I)(B - 3)(B -I)(Jφ⑴-jφ (b⅛) )
ɑ2
B - 3

α2(B - 3 * * * * + 2α) (jφ⑴ + B1-1 jα-1φ0 (b⅛
(2α -I)(B - 3)(jφ(I)- jφ (B⅛,
ɑ2
B - 3

α2 ((B - 2)jφ(I) + (B - 3 + 2α) B1-1 Ja-Iφ0 (B-11) + (2α - I)(Jφ(I) - jφ (B-11
(2α — 1)(B — 3)(Jφ(1) - Jφ (b⅛ ))
λ∣ =	2a2(B - 2)(B - 1 + α)
L = - (B - 3)(B - 1)(B - 1 + 2α)
a2(3B - 4) + α(3B2 - 11B + 8)+ (B - 3)(B - 1)2	jφ (b⅛)
+ 2	(B - 3)(B - 1)2(B - 1 + 2α)	Jφ(1) - Jφ (b⅛)
a2(B - 2)(B - 3 + 2α)	Jφ(1)
十 (2α - I)(B - 3)(B - 1) jφ(1) - jφ (B-T)
, _ B(B2 + 2(α - 2)B + 2(α - 3)α + 3)	jφ (b⅛)
M =	(B - 3)(B - I)(B - 1 + 2α)	Jφ(1)- Jφ (B⅛).
H Cross Batch: Forward Dynamics
In this section, we study the generalization of Eq. (43) to multiple batches.
Definition H.1. For linear operators Ti : Xi → Yi, i = 1, . . . , k, we write Li Ti : Li Xi → Li Yi
for the operator (χ1, ...,xk) → (TT(χι),..., Tk(Xk)). We also write 71φn := L；=i Tl for the
direct sum of n copies of T1 .
For k ≥ 2, now consider the extended (“k-batch”) dynamics on Σ ∈ SkB defined by
∑l =VBek (∑l-1)	(54)
φ
=E[(Bφ(h1:B ), Bφ(hB + L2B ), ..., Bφ(h(k-1)B + LkB ))02 : h 〜N (0, ∑l-1)]
If we restrict the dynamics to just the upper left B × B submatrix (as well as any of the diagonal
B × B blocks) of Σel, then we recover Eq. (43).
H.1 Limit Points
In general, like in the case of Eq. (43), it is difficult to prove global convergence behavior. Thus
we manually look for fixed points and the local convergence behaviors around them. A very natural
extension of the notion of BSB1 matrices is the following
Definition H.2. We say a matrix Σ ∈ SkB is CBSB1 (short for “1-Step Cross-Batch Symmetry
Breaking”) if Σ in block form (k × k blocks, each of size B × B) has one common BSB1 block on
the diagonal and one common constant block on the off-diagonal, i.e.
∕BSB1(a,b) c11T	c11T	…、
c11T	BSB1(a,b)	c11T	…
c11T	c11T BSB1(a,b)…
.	.	..
..	..	..	..
… - - 二. . ...
We will study the fixed point Σ* to Eq. (54) of CBSB1 form.
78
Published as a conference paper at ICLR 2019
H.1.1 Spherical Integration and Gegenbauer Expansion
_ _________ _ : _ _____________________ . 一 ，:、. ___________ , 一一.
Theorem H.3. Let Σ ∈ SkB. If ∑ is CBSB1 then VB㊉k (∑) is CBSB1 and equals Σ :=
φ
Σ∑* c* ∖
(c* ∑* ) Where ∑* is the BSB1 fixed point of Thm F.5 and c* ≥ 0 with √c* =
r((B-1)/2)π-1/2 Rπ dθ φ(一√B — 1 cos θ)sinB-3 θ. If φ(√B — 1 x) has Gegenbauer expansion
Γ((B —2)/2)	0
B — 3 ∖
P∞=0 aiCB⅛ClF)(x), then c* = a2.
Proof. We will prove for k = 2. The general k cases follow from the same reasoning.
Let Σe = c1Σ1T c1Σ1 where Σ = BSB1(a, b). As remarked below Eq. (54), restricting to any
diagonal blocks just recovers the dynamics of Eq. (43), which gives the claim about the diagonal
blocks being Σ* through Thm F.5.
We now look at the off-diagonal blocks.
〜
〜
VB®2 (∑)lB,B + L2B = E[Bφ(zi:B )乳 Bφ(zB + L2B ) : Z ~ N(0, ∑)]
=E[φ ◦ n(yi：B)③ φ ◦ n(yB+i：2B): y 〜N(0, Σg)]
G
where ΣG
-C	- C
G㊉2∑ G㊉2
G0eG0
0GΣ0G
GΣG	cG11T G
cG11T G	GΣG
(a-b)G
0
yB+1:2B , and
(a
(the last step follows from Lemma E.52). Thus y1:B is independent from
Vb®2 (∑)i：B,B+i：2B = E[φ ◦ n(x) : x 〜N(0, (a - b)G)产
By symmetry,
E[φ ◦ n(x) : x 〜N(0, (a — b)G)] = √c* 1
E[φ ◦ n(x) : X 〜N(0, (a — b)G)产2 = c*11T
where √c* := E[φ(n(x)1) : x 〜N(0, (a - b)G)]. We can compute
√c* = E[φ(n(x)ι) : x 〜N(0, G)]
because n is scale-invariant
=E[φ(n(ex)ι): X 〜N(0,Ib-i)]
where e is as in Eq.(33)
=E[φ((en(x))1): x 〜N(0,Ib-i)]
because e is an isometry
=E[φ(√B(ev)ι) : x 〜N(0,Ib-i)]
with v = x/kxk
=γ((B - 1)(2) π-1/2 [∏ dθ φ(-√B-1Cos θ) SinB-3 θ
Γ((B - 2)/2)	J0 φ( B	)
by Lemma E.37. At the same time, by Proposition E.33, we can obtain the Gegenbauer expansion
√c* =	E	φ(√B — 1^ι,:V)
V〜SB-2	，
∞	B-3
= E 2 X a  ---------Ci F (® 1,：V)
V 〜SB-2 1=0 CB-1,1
aiZB-2")」+
aoZBι-2,⑼(^1,:)
a0.
79
Published as a conference paper at ICLR 2019
□
Corollary H.4. With the notation as in Thm H.3, if φ is positive-homogeneous of degree α and
φ(τ) — CC (T)	_ b (_丁)	th	—	(C	_	b) 1 (B _ l)ɑ∕2 γ((B-I)/2)r((a+I)/2)
φ(x) = aρα(x)	- bρa(-x),	then VC	=	(a	-	b) 2√π (B - 1)	Γ((α+B-1)/2)	.
Proof. We compute
Z	dθ (Cos θ)α sinB-3 θ = 2 Beta(0,1; α±~, B2 ?)
by Lemma E.28
= 1Beta (0±1 二)
2	2 ,	2
Z dθ (-cos θ)α sinB-3 θ = Z / dθ (Cos θ)α sinB-3 θ
Jn/2	0
= 1Beta (0±1 二)
2	2 ,	2
So for a positive homogeneous function φ(x) = aρα(x) - bρα(-x),
√ = ；((B — ：)/2)…(a [" dθ (-√B-icos θ)α sinB-3 θ
γ ((B - 2)/2)	∖)n/2
rn/2	,___ ∖
-b	dθ (√B-icos θ)α sinB-3 θ )
=(a - b)r((B - 1)/2)∏-1∕2(B - 1)ɑ∕218^(0±i B-2
=(a b)Γ((B - 2)/2)	(B	1)	2 I 2 , 2
(a- b)2√∏(B - 1)a/2
Γ((B - 1)∕2)Γ((α + 1)/2)
Γ((α + B - 1)/2)
Expanding the beta function and combining with Thm H.3 gives the desired result.
□
H.1.2 Laplace Method
While We don't need to use the LaPlace method to compute C for positive homogeneous functions
(since it is already given by Corollary H.4), going through the computation is instructive for the
machinery for computing the eigenvalues in a later section.
Lemma H.5 (The Laplace Method Cross Batch Master Equation). For A, B, C ∈ N, let f :
RA+B → RC and let a, b ≥ 0. Supposefor any y ∈ RA, Z ∈ RB, kf (y,z)k ≤ h(pkyk2 + kz∣∣2)
for some nondecreasing function h : R≥0 → R≥0 such that E[h(rkzk) : z ∈ N(0, IA+B)] ex-
ists for every r ≥ 0. Define 夕(Σ) := E[kyk-2akzk-2bf (y, z) : (y,z)〜N(0, ∑)]. Then on
{Σ ∈ SA+b : rank Σ > 2(a + b)},夕(∑) is well-defined and continuous, andfurthermore satisfies
夕(Σ)=Γ(a)-1Γ(b)T / ds / dtsa-1tbτ det(IA+B +2Ω)-1/2	E	f(y,z) (55)
o	Jo	(y,z)〜N(0,Π)
where D
√0b)
,Ω
DΣD, andΠ = D-1Ω(I +2Ω)-1D-1.
80
Published as a conference paper at ICLR 2019
Proof. If Σ is full rank, we can show Fubini-Tonelli theorem is valid in the following computation
by the same arguments of the proof of Lemma E.2.
E[kyk-2akzk-2bf(y,z)：(y,z)〜Na ∑)]
=E[f(y,z) ∞ ds Γ(a)-1sa-1e-kyk2s ∞ dt Γ(b)-1tb-1e-kyk2t : (y,z)〜N(0, Σ)]
oo
=(2π)-A+B Γ(a)-T(b)-1 detΣ-1/2
X Z ds Z dtsa-1tbτ Z dy dz f(y,z)e-2(y,z" +炉乂%Z)T
o	o	RA+B
(Fubini-Tonelli)
= Γ(a)-1Γ(b)-1 ∞ ds ∞ dt sa-1tb-1 det(Σ(Σ-1 + 2D2))-1/2 E f (y, z)
oo
where in the last line, (y, Z)〜N(0, (Σ-1 + 2D2)-1). We recover the equation in question with
the folloWing simplifications.
(Σ-1 + 2D2)-1 = Σ(IA+B + 2D2Σ)-1
= Σ(D-1 + 2DΣ)-1D-1
= ΣD(IA+B + 2DΣD)-1D-1
=D-1Ω(Ia+b + 2Ω)-1D-1
det(Σ(Σ-1 + 2D2)) = det(IA+B + 2ΣD2)
= det(IA+B + 2DΣD)
=det( Ia+b + 2Ω)
The case of general Σ with rank Σ > 2(a + b) is given by the same continuity arguments as in
Lemma E.2.
□
〜
〜
Let Σ ∈ S2B , Σ
ΞΣT ΣΞ0 where Σ, Σ0 ∈ SB and Ξ ∈ RB ×B . Consider the off-diagonal block
of VB督(Σ).
一 .. .,. 一 , ~、一
E[Bφ(z) 0Bφ(z0) ： (z,z0)〜N(0, ∑)]
=E[Φ(n(y))③ φ(n(y )) : (y, y )〜N(0, ΣG)]
=Ba E[kyk-αky0k-αφ(y)乳 φ(y0) ： (y, y0)〜Na ∑G)]
=BaΓ(α∕2)-2 ∞ ds ∞ dt (st)a/2T det(I2B + 2Ω)-1/2	E	φ(y)㊈ φ(y0)
Jo	Jo	(y,y"N (Un)
(by Lemma H.5)
BαΓ(α∕2)-2
d ds [ dt (st)α/2-1 det(I2
oo
B +2Ω)-1/2 Vφ(Π)1:B,B+1:2B
(56)
0
sΣG
where ω= (√st(ΞG)T
0 ʌ
√tIBJ
v^tΞG ) and Π = D-1Ω(I + 2Ω)-1D-1 With D = √sIB ㊉ √tIB
tΣ
Theorem H.6 (Rephrasing and adding to Corollary H.4). Let Σ ∈ SkB and φ be positive homoge-
〜
〜
〜
neous of degree a. If Σ is CBSB1 then VB㊉k (Σ) is CBSB1 and equals Σ*
where Σ* is the BSB1 fixed point of Thm F.5 and
* _	(B - lʌa B - 1 a、; c
C = ca ( -1- ) P(F-, 2 ) Jφ(0)
'Σ*	c* 11T
c*11T	Σ*
(a-b/ 4∏(B - 1尸
Γ((B — 1)∕2)Γ((a +1)/2八 2
Γ((α + B - 1)/2)
81
Published as a conference paper at ICLR 2019
Proof. Like in the proof of Corollary H.4, we only need to compute the cross-batch block, which is
Σ
given above by Eq. (56). Recall that Σ = c11T
where Σ = BSB1(a, b). Set μ := a 一 b.
Note that because the off-diagonal block Ξ = c11T by assumption, ΞG = 0, so that
Ω = sΣG ㊉ tΣG
sμG ㊉ tμG
sμ …	tμ C
1 + 2sμ ㊉ 1 + 2tμ°
μ G ㊉-^G
1 + 2sμ	1 + 2tμ
Therefore,
det(I + 2Ω) = (1 + 2sμ)B-1(1 + 2tμ)B-1
丫。(口)="。( B- Y ∆Vφ (B叫,其) bsbi(0, Bk))△
where ∆ =((1+2sμT/2IB(I+2t 0)-α∕2I ). In particular, the off-diagonal block is con-
Stant With entry Cαμα (BJ-I厂(1 + 2sμ)-α^(1 + 2tμ)-α/2 Jφ(0).
Finally, by Eq. (56),
E[Bφ(z)乳 Bφ(z0) ： (z,z0)〜N(0, ∑)]
BaΓ(α∕2)-2 ∞ ds ∞ dt (st).2-1 det(I2B +2Ω)-1∕2Vφ(Π)lb,b+l2b
00
BaΓ(α∕2)-2 [ ds / dt (st)a/2-1 [(1 + 2sμ)(1 + 2tμ)]-B-1
00
B1a
X Caμa (-B- ) [(1 + 2sμ)(1 + 2tμ)]-a∕2Jφ(0)11τ
Ca(B 一 1)aΓ(α∕2)-2 Jφ(0)11T f dσ j dτ (στ)a/2-1(1 + 2σ)- B 11° (1 + 2τ)-B 厂
00
(with σ = μs, τ = μt)
Ca(B — 1)αΓ(α∕2)-2Jφ(0)11T(2-a/2 Beta (B-1, 2))
=Ca ( M y P ( 7 ,l )-2 Jφ (0)11T
Simplification with Defn E.7 and Proposition E.21 gives the result.
□
H.2 Local Convergence
We now look at the linearized dynamics around CBSB1 fixed points of Eq. (54), and investigate
dVB㊉ 2
the eigen-properties of dφ
. By Lemma F.17, its nonzero eigenvalues are exactly those
~ ~
∑=e*
of F := (G出2 产◦
dΣe G
∕~∙J
,F{Λ} = G图2
e G=μ*G ㊉2
dV(φ3n)㊉2 I
de G	lμ*G ㊉2
{A}) G㊉2. Here
UG	G2l …2 L
Σg = G㊉2ΣG㊉2. Thus it suffices to obtain the eigendecomposition of F.
d⅛g≤g G
First, notice that F acts on the diagonal blocks as G02 ◦ dVφGn, and the off-diagonal components of
Λ has no effect on the diagonal blocks of F {Λ}. We formalize this notion as follows
82
Published as a conference paper at ICLR 2019
Definition H.7. Let T : HkB → HkB be a linear operator such that for any block matrix Σ ∈ HkB
with k × k blocks Σij, i,j ∈ [k], of size B, We have
T{∑ }ii = U{∑ ii}
T{∑ 产=V{Σ ii} + V{Σjj }t + W{Σij}, ∀i = j,
where U : HB → Hb, V : HB → RBXB, and W : RBXB → RB×B are linear operators, and
T{Σ}ij denotes the (i,j)th block of T{Σ}. We say that T is blockwise diagonal-off-diagonal
semidirect, or BDOS for short. We write more specifically T = BDOS(U, V, W).
We have F = BDOS(U, V, W) where
U = G®2。dVφ∣n
dΣ ΣG=μ*G
d
d(Σ G)11
(hiN(0,eG) φ(n(h)) ㊈如⑺)
Σ G=μ*G ㊉2
W = 2G02。
d
d(Σ G)12
E 〜φ(n(h)) 0 φ(n(h0))
(h,h0)〜N (0,Σg)
eG =μ* G㊉2
(57)
V = Ge12。
Note that the factor 2 in W is due to the contribution of (ΣG)21 by symmetry. We will first show
that W is multiplication by a constant.
H.2.1 Spherical Integration and Gegenbauer Expansion
_____ ( B — 3)
Theorem H.8. Suppose φ(√B — 1x) has Gegenbauer expansion E∞=0 && [ I Cl 2 ，(x). Then
for any Λ ∈ RBXB satisfying GΛG = Λ,
W{Λ} = 2α2∕Tp
(B — 1 1、2 B ∙
2 2 , 2) B — 1 λ
α11 P (B, 1) 2 B(B - 1)
P∞=0 a2，
「λ
B
B-)
Proof Let J = d(⅛2 E(h,h，)〜N(0,∑G) φ(n(h)) 0 φ(n(h'))[G=“*G®2 ∙ CombiningLemmaF.18
with projection by e, we get
J {Λ} = 1 E	φ (n(© y)) 0 φ (n(© y)) {μt-,2yy'τ — μ*-∣lB-1, ®T Λe)
2 y,y0〜N(0,**Ib—ι)
=ɪ^*-1	E	φ (n(ey)) 0 φ (n(eyz)) (yZTeTΛey — tr Λ)
2	y,y0 〜N (0,Ib —1)
2μ*-1 ( (√2P (-2 1,2))	EB Φ (√Bev) 0 φ (√BevZ)(VZTeTΛeV)
—(tr Λ) EB φ (√BeV) 0 φ (√Bev0))
by Proposition E.33.
Note that E以。,〜SB—2 φ (√Bev) 0 φ (√Bev0) = (Ev〜SB—2 φ (√Bev)) is a constant matrix,
because of the independence of v from vz and the spherical symmetry of the integrand. Thus this
term drops out in G02。J{Λ}.
Next, we analyze H{Λ} = Ev,v,〜SB―2 φ (√Bev) 0 φ (√Bev0) (VZTeTΛev). This is a linear
function in Λ with coefficients
HabIcd = BB 1 (	SE _2 φ(√B — 1 M：v)®,：V)) ( SE _? φ(√B - 1®a；：v)(ed；：v) J .
83
Published as a conference paper at ICLR 2019
By the usual zonal spherical harmonics argument,
V E_2 φ(√B-l^b,：v)(^c,：v)
1
a-------
cB-1,l
B-3 )(hMQ),h^c,:Q)
SB-2
Z B-2,(l)	1	Z B-2,(1)∖
aZ吼,：	,b-1 4c，：	/
SB-2
Ba-1 ZB 2,(1) (M:)
∕⅛ ɪ C(B-3 )(h^b,:, ^c,J)
B -1 cB-1,1
(a1	if b = c
1	B---1 otherwise.
Thus,
H{Λ}ab
X	Λcd
c=b XOR d=a
B- (-2 G1 - B-a⅛	Aab + (a2 - (Ba 1)2) Aab)
because rows and columns of Λ sum to 0
=α2 B _ ι Aab
Therefore,
W{Λ} = 2G02 ◦ J{Λ}
2α2μ*-1P (BfI, 1 )2 4A
2	2 B_1
a11P (B2,1) 2 B(B -1)
p∞=o a c⅛
A
B
B-3) (b⅛
where we used the identity P (b-1 , 2) (b2⅛) =P (B，2)-1
□
Next, we step back a bit to give a full treatment of the eigen-properties of BDOS operators. Then
∙-v
we specialize back to our case and give the result for F .
Definition H.9. Let HGB ：= {∑ ∈ HkB : G㊉k∑G㊉k = ∑} be the subspace of HkB stable under
conjugation by G㊉ k.


Definition H.10. Let Me kB ∈ HkGB be the space of block matrices Σe ∈ HkGB where the diagonal
Fl 1 Hdd	11	T ∙1	♦	1	. TX ιπ	一 Cl	Λ .1	CFll	.	∙	H 一 C，	1
blocks Σii are all zero. Likewise, let MkB ∈ HkB be the space of block matrices Σ ∈ HkB where
ii
the diagonal blocks Σii are all zero. We suppress the subscript kB when it is clear from the context.
Definition H.11. Define the block L-shaped matrix
LkB (∑, A):
Σ	_A
_AT	0
.
84
Published as a conference paper at ICLR 2019
BDOS operators of the form BDOS(U, V, w), where w stands for multiplication by a constant w,
like F, have simple eigendeComPosition,just like DOS operators.
Theorem H.12. If V - wIB is not singular, then BDOS(V, U, w) : HkB → HkB has the following
eigenspaces and eigenvectors
1.	M with associated eigenvalue W.
2.	For each eigenvalue λ and eigenvector Σ ∈ HB of U, the block matrix
/ Σ	(λ - w)-1V{Σ} …、
LkB (∑,-(λ	- w)-1V{Σ}) =	(λ - W)TV3}T	0	∙∙∙	I
...	...	...
along with any matrix obtained from permutating its block columns and rows simultane-
ously. The associated eigenvalue is λ.
If BDOS(V, U, w) also restricts to HkGB → HkGB, then with respect to this signature, the eigende-
composition is obtained by replacing HkB with HkGB in the above:
1.	Block diagonal matrices Σe ∈ HkGB where the diagonal blocks Σe ii are all zero. The associ-
ated eigenvalue is w.
2.	For each eigenvalue λ and eigenvector Σ	∈ HBG of U, the matrix
LkB(∑, 一(λ — W)-IV{Σ}) along with any matrix obtained from Permutating its
block columns and rows simultaneously. The associated eigenvalue is λ.
Proof. The proofs are similar in both cases: One can easily verify that these are eigenvectors and
eigenvalues. Then by dimensionality considerations, these must be all of them.	□
By Thms H.8 and H.12, we easily obtain
( B — 3)
Theorem H.13. Suppose φ(√B — 1 x) has Gegenbauer expansion E∞=id at — 1ι; C∣ 2 (x). The
eigendecomposition of F : HGB → HGB is g^ven below, as long as λM = λ],λM∙
1.	M with associated eigenvalue
λM = 2a2〃*-1p (M ,2 )2 BBl
_ a21 P (B，1) 2 B(B 一 I)
P∞=0 α2 CB⅛
B
ɪ) (B⅛
2.	The space generated by L2B(G, — (λ] — λM) 1V{G}) along with any matrix obtained
from permutating its block columns and rows simultaneously. The associated eigenvalue is
0.
3.	The space generated by L2B(L, — (λ1 — λM) 1V{L}), where L = LB (B — 2,1), along
with any matrix obtained from permutating its block columns and rows simultaneously. The
associated eigenvalue is λ↑L.
4.	The space generated by L2B(M, —(λ1 — λM) 1V{M}), for any M ∈ M, along with
any matrix obtained from permutating its block columns and rows simultaneously. The
associated eigenvalue is λ↑M.
Here, V is as in Eq. (57). The local COnVergenCe dynamics of Eq. (54) to the CBSB1 fixed point Σ*
has the same eigenvalues λ↑Me , λ↑L, λ↑M, and 0.
85
Published as a conference paper at ICLR 2019
Note that, to reason about F extended over k batches, one can replace 2B with kB everywhere in
Thm H.13 and the theorem will still hold. As We will see below, λM is the same as the eigenvalue
governing the dynamics of the cross batch correlations in the gradient propagation.
H.2.2 Laplace Method
As with the single batch case, with positive homogeneous φ, We can obtain a more direct answer of
the new eigenvalue λM in terms of the J function of φ.
Theorem H.	14. Let φ be degree α positive homogeneous. Then the eigenvalue of F on M is
λM
B ”广 P (f ,21 广&力。)
B P (B-ɪ, α)	jφ(0)
Proof. Let Σ*
B -1 P(⅝, 2)2 Jφ(i)- Jφ (B⅛
'Σ*	c*11τ∖
c*]]T	∑* I be the CBSB1 fixed point in Thm H.6. Take a smooth path
∈ S2B,τ ∈ (-e,e) such that ∑0 = Σ*, ∑0
。
YT
Y∖
。
for some
Y ∈ Rb×b. Then with Ωτ
D = √SIb ㊉ √Ib =
(s∑G
l√t(ΞG)T
。∖
VIIB),
√st≡GA
记产
and Πτ = D-1Ωτ(I + 2Ωτ )-1D-1 with
d
〜
赤
Σ
d 一	........ 一 ~ 一
工 E[Bφ(z)0 Bφ(z0) : (z, z0)〜N(0, Σ)]
dT	τ =0
WBaΓ(α∕2)-2 厂 ds 广 dt (st)./2-1 det(I2B + 2Ω)T∕2Vφ(Π)rB,B+L2B
dτ	J0	J0	τ=0
Bar(Q/2)-2 ∞ ds
0
广 dt (St产2-1 ( dT det(i2B + 2。尸/2
vφ(π)lB,B + L2B
T = 0
BɑΓ(α∕2)-2
+ det(12B + 2。) 1/2 工vφ(π)lb,B + L2B
dτ	τ=0
/ ds [ dt (st)a/2-1 ( - det(I + 2Q0)-1/2 tr ((I + 2Ω0)-1Ω0) Vφ(Π)
+ det(I + 2Ω0)-1∕2(D-a产2°j[ -d-Ω(I + 2Ω)-1	(58)
I dτ	T=0 J ) 1:B,B + 1:2B
by chain rule and Lemma F.26, where J = dVφ	. We compute
∑=Ωo(i+2Ω0)τ
Ω0 = μ* (sG ㊉ tG)
Ω0 = √ ((YG)T Y)G)
(I + 2Ω0) 1 = ((1 + 2sμ*) 1G + —11t)㊉((1 + 2tμ*) 1G + —11t)
BB
det(I + 2Ω0) = (1 + 2sμ*)BT(I + 2tμ*)BT
Ω0 (I +2Ω0)-1
sμ*
1 + 2sμ*
G㊉
"G
86
Published as a conference paper at ICLR 2019
So then
s〃* YaT)/2 IB ㊉(t〃*
1 + 2sμ*	1 + 2tμ*
…J2。( 7 y-%
∑=BSB1(1, b-i 产
With
d- Ω(I + 2Ω)-1
dτ
(by LemmaF.27)
(1 + 2sμ*)(1 + 2tμ*)
T
YG
0
J
T = 0
We have
J( -dΩ(I + 2Ω)T
dτ
B - 1YT
~B~
T =0 J
√st
(1 + 2sμ*)(1 + 2tμ*)
sμ*
1 + 2sμ*
a-1
(α-1)∕2
C")
:∖ (α-1)∕2
∖1 + 2t〃* J
(st)" α-1CαJφ(0)
(1 + 2sμ*)S+1”2(1 + 2tμ* )(«+1)/2
.
YG
0
YG
T0
Thus the product (I + 2Ωo)-1Ω0 has zero diagonal blocks so that its trace is 0. Therefore, only the
second term in the sum in Eq. (58) above survives, and we have
×
B - 1
B
d
dτ E[Bφ(z)③ Bφ(z0): (z,z0)〜N (0, ∑)]
τ=0
BαΓ(α∕2)-2 [ ds [ dt (st)a/2-1(1 + 2sμ*)-⅛1 (1 + 2tμ*)-⅛1 ×
00
(st)”(彳Γ
(st)。/2 — a)
B(B - 1)α-1Γ(α∕2)-2 ∞ ds ∞ dt (st)a/2-1
00
(1 + 2sμ*)S+1”2(1 + 2tμ*)S+1”2
μ*α-1CαJφ(0)
YG
(1 + 2s〃*)(B+α”2(1 + 2tμ*)(B+α)/2
YG
Β(Β - 1)α-T(α∕2)-24*TcαJφ(0)
r∞
ζ ddS)
(1 + 2s〃*)(B+a)/2
2
YG
B(B - 1)α-1Γ(α∕2)-2μ*-1CɑJφ(0)(2-a/2 Beta (B, ∣ JYG
B(M LP (B■，2 /广&jφ(0)YG
□
I Cross Batch： Backward Dynamics
For k ≥ 2, we wish to study the following generalization of Eq. (51),
∏l = VBefc 0 (Σl )t{Πl+1}.
φφ
l .
As in the single batch case, we approximate it by taking Σl to its CBSB1 limit Σ*, so that we analyze
印= VBy (Σ *)t{Πl+1}
(59)
87
Published as a conference paper at ICLR 2019
〜
Its dynamics is then given by the eigendeComPosition of VB㊉k，(Σ*), in parallel to the single batch
scenario.
WLOG, we only need to consider the case k = 2.
VB®20 (∑*)
φ
VB㊉20 (Σ*)
φ
E
(x,y)〜N (0,e*)
[(Bφ(x)㊉ Bφ(y)产]
ΞΣT
Ξ	E	Bφ0 (x)ΣBφ0 (x)T
λ) J	(x,y)〜N(0,e*) . Bφ0 (y)ΞT Bφ0 (x)T
Bφ0 (x)ΞBφ0 (y)T
Bφ0 (y)ΛBφ0 (y)T
From this one sees that V[(b㊉2y] acts independently on each block, and consequently so does its
adjoint. The diagonal blocks of Eq. (59) evolves according to Eq. (51) which we studied in Ap-
pendix G. In this section we will study the evolution of the off-diagonal blocks, which is given
by
ξ1 = (χ,y)〜N“HP(X) M -
(x,y)〜N(0,e*)[Bφ⑹T …(X)TH*} (60)
E
(x,y)〜N (0,Σ*)
[Bφ0 (y)T Ξl+1Bφ0 (x)T]
Definition I.1. Let MB := RB×B be the set of all B × B real matrices, and let MBG := {Ξ ∈
MB : GΞG = Ξ}.
Define T := E(Xy)〜N(0 ∑ *)[Bφ (χ)0Bφ (y)]. As in the single batch case, after one stepofbackprop,
ΞL-1 = V * {ΞL} is in Mg. Thus it suffices to study the eigendecomposition of G02 ◦ T *『MG :
MG → Mg. Below, We will show that its adjoint T ◦ G02 acting on MB is in fact multiplication
by a constant, and thus so is it.
We first compute
T= E
(ξ,η)〜N (0,g ㊉ 2∑ *g ㊉2)
E
(ξ,η)~N(0,μ*( G G ))
dφ ◦ n(z)
dz
dφ ◦ n(z)
dz
③
z=ξ
③
z=ξ
dφ ◦ n(z)
dz
dφ ◦ n(z)
dz
z=η
z=η
◦ G02
◦ G02
since Σ* is CBSB1 with diagonal blocks Σ* (Corollary H.4). Then ξ is independent from η, so this
is just
(x,y)〜N (0,e* )[Bφ (X) K(y)]
E
ξ 〜N (0,μ*G)
02
dφ ◦ n(Z)	C「02
----1---- ◦ G
dz	z=ξ
E
ξ 〜N (0,μ*G)
h√BDr-1(I - vvτ)] 02 ◦ G02
(by Proposition G.4)
where D = Diag(φ0(√Bv)),r = ∣∣ξ∣∣,v = ξ∕∣∣ξ∣∣.
Eξ〜N(o,μ*G) [Dr-1(I - VvT)] is actually BSB1, with diagonal
But notice that T
E	r-1 (1 - v2)φ0(VBvi)
rv 〜N (0,μ*G)
E	r-1(1 - v2)φ0(√Bvι),∀i ∈ [B]
rv 〜N (0,μ*G)
and off-diagonal
rv 〜n!L*G)r-"". )φ’(√Bvi)=,。〜n∖*G) Ln ( ^, ∀i = j ∈ B
Thus by Lemma E.51, T has two eigenspaces, {X : GX = X} and R1, with respective eigenvalues
T11 - T12 and T11 + (B - 1)T12. However, because of the composition with G02, only the former
eigenspace survives with nonzero eigenvalue in G02 ◦ T.
88
Published as a conference paper at ICLR 2019
Theorem I.	2. T on	MBG	is just multiplication by λM
2∏Bμ*-1 (J0∏ dθ sinB-1 θφ0(-√B — 1 cos θ))2.
Proof. By the above reasoning, the sole eigenspace with nonzero eigenvalue is {x : Gx = x}02
MBG . It has eigenvalue
B(E[r-1 (1 — v2 + v1v2)φ'(√Bvι) ： rv 〜N(0, μ>G)])
Bμ*-1(E[r-1(1 — v2 + vιv2)φ0(√Bv1) : rv 〜N(0, G)])
Bμ*-1(E[r-1(1 — (ew)1 + (®w)ι(ew)2)φ0(√B(ew)ι) : rw 〜N(0,Ib-i)])
B	Jγ((b- 2)/2) 2-"∏-1
μ	Γ((B - 3)/2)
[dθι [ dθ2 (1 — Z(θ1)2 + Z(Θ1)ω(θ1,θ2))φ0(√BZ(θι)) sinB-3 θι sinB-4 θ2)
where We applied Lemma E.36 with Z(θ) = —\ B-I cos θ and ω(θι, θ2) = / 1	cos θι —
B	、，/	√B(B-1)
B B-2 sin θι cos θ2.
We can further simplify
Z πdθ1
0
Z πdθ1
0
Z
0
dθ2 (1 — ζ(θ1)2 + ζ(Θ1)ω(θ1,θ2))φ0(√BZ(θι)) SinB-3 θι SinB-4 θ
SinB-3 θ1φ0(√BZ(θι))	(1 — Z(θ1)2 + Z(θι) , ɪ n
B(B — 1)
cos θ1	dθ2 sinB-4 θ2
dθ2 sinB-4 θ2 cos θ2
Z dθ1
0
SinB-3 θ1φ0(√BZ(θι)) ((1 — Z(θ1)2 + Z(θι)	1
B(B — 1)
cos θι) Beta ^By3, g))
(by Lemma E.29)
Z dθ1
0
SinB-3 Oi00(Vbz(。I))卜in2 θιBeta (-2-, 2
Beta
0100(—Bb — 1 cos θ1)

so the cross-batch backward off-diagonal eigenvalue is
Bμ*-1 (√2= / dθ1 SinB-I 0100( — dB - 1 cos θ1))
2^Bμ*-1 (Z dθ1 sinB-1 θ1φ/(-，B - 1 cosθ1))
□
For positive homogeneous functions we can evaluate the eigenvalues explicitly.
Theorem I.	3. If φ is positive-homogeneous of degree α with φ(c) = aρa(C) — bρα(-c), then
T : MBG → MGB is just multiplication by
λe = 8-B(B - 1)α-1μ*-1α2(a + b)2 Beta
αB
2 , 2"
2
89
Published as a conference paper at ICLR 2019
α-1 sinBτθ=1Beta(2 (
Proof. As in the proof of Corollary H.4,
Z / dθ (Cosθ)α-1 sinB-1 θ = Z dθ (-cosθ)
00	Jn/2
rπ∕2	2
θ+ b	dθ (cos θ)α-1 sinB-1 θ
□
It is also straightforward to obtain an expression of the eigenvalue in terms of Gegenbauer coeffi-
cients.
( B — 3)
Theorem I.4. If φ(√B - 1 x) has GegenbaUer expansion E∞=0 at — 1ι C Cl 2 '(x), then T :
MBG → MBG is just multiplication by
1	B12
λM = 2∏B(B - 1)μ	aι Beta(E, 2)
2∏ B (B — 1)a2 Beta (B, 1)
P∞=0 a2 c⅛
λ↑ .
Me
B
B-) (B⅛
Proof. It suffices to express the integral in Thm I.2 in Gegenbauer coefficients. Changing coordi-
nates x = - cos θ, so that dx = sin θ dθ, we get
Z dθ sinB-1 θφ0(-√B — 1 Cosθ) = Z dx(1 — x2)B-φ0(√B — 1x).
By Eq. (35),	-
φ0(√B-1x) = (B - 1)-1/2-d-φ(√B-1x)
dx
Then by the orthogonality relations among
∞
(B - I)-"2 X aι ——
l=0	cB-1,l
(B - 1)-1/2 X aι「
l=1	cB-1,l
B-1
B3
ɪ )0(x)
C(B-I )(χ).
1,
Z dθ sinB-1 θφ0(-√B — 1 cosθ) = Z dx(1 — x2)B—2φ0(√B — 1x)
(B - 1) — i/2aiB-3 Z1 hC0B-)(x)i2 (1
-1/2	B - 3 √πγ (BB)
(B -1)	a1 CB-71 TwT
aι √B - 1 Beta ([,；).
-x
B、B-2
:)2 dx
90
Published as a conference paper at ICLR 2019
□
To summarize,
Theorem I.5. The backward cross batch dynamics Eq. (59) over HkGB is separable over each in-
dividual B × B block. The diagonal blocks evolve as in Eq. (51), and this linear dynamics has
eigenvalues λG, λ], λ[ as described by Appendix G. The off-diagonal blocks dynamics is multipli-
cation by λi = λ↑.
Me	Me
Note that
which increases to 1 from below as B → ∞. Thus,
Corollary I.6. For any φ inducing Eq. (54) to converge to a CBSB1 fixed point, λM = λM <
1. For any fixed batch size B, λM is maximized by φ = id. Furthermore, for φ = id, λM =
B-IP (B, 1) 2 and limB→∞ λ∖ = 1, increasing to the limitfrom below.
This corollary indicates that stacking batchnorm in a deep neural network will always cause chaotic
behavior, in the sense that cross batch correlation, both forward and backward, decreases to 0 ex-
ponentially with depth. The φ that can maximally ameliorate the exponential loss of information is
linear.
J Uncommon Regimes
In the above exposition of the mean field theory of batchnorm, we have assumed B ≥ 4 and that φ
induces BSB1 fixed points in Eq. (8).
Small Batch Size What happens when B < 4? It is clear that for B = 1, batchnorm is not
well-defined. For B = 2, Bφ(h) = (±1,千 1) depending on the signs of h. Thus, the gradient
of a batchnorm network with B = 2 is 0. Therefore, we see an abrupt phase transition from the
immediate gradient vanishing of B = 2 to the gradient explosion of B ≥ 4. We empirically see that
B = 3 suffers similar gradient explosion as B = 4 and conjecture that a generalization of Thm 3.9
holds for B = 3.
Batch Symmetry Breaking What about other nonlinearities? Empirically, we observe that if the
fixed point is not BSB1, then it is BSB2, like in Fig. 1, where a submatrix ofΣ (the dominant block)
is much larger in magnitude than everything else (see Defn K.1). If the initial Σ0 is permutation-
invariant, then convergence to this fixed point requires spontaneous symmetry breaking, as the dom-
inant block can appear in any part of Σ along the diagonal. This symmetry breaking is lost when
we take the mean field limit, but in real networks, the symmetry is broken by the network weight
randomness. Because small fluctuation in the input can also direct the dynamics toward one BSB1
fixed point against others, causing large change in the output, the gradient is intuitively large as well.
Additionally, at the BSB2 fixed point, we expect the dominant block goes through a similar dynam-
ics as if it were a BSB1 fixed point for a smaller B, thus suffering from similar gradient explosion.
Appendix K discusses several results on our current understanding of BSB2 fixed points. A specific
form of BSB2 fixed point with a 1 × 1 dominant block can be analyzed much further, and this is
done in Appendix K.1.
91
Published as a conference paper at ICLR 2019
Finite width effect For certain nonlinearities, the favored fixed point can be different between the
large width limit and small width. For example, for φ = c5( "ɪ )(x∕√B - 1)-C( (^ )(χ∕√B - 1)
where B = 10, one can observe that for width 100, Eq. (43) favors BSB2 fixed points, but for width
1000 and more, Eq. (43) favors BSB1 fixed points.
K Toward Understanding BSB2 Fixed Points
Definition K.1. For B1, B2 ≥ 2, a BSB2 matrix is one of the form
BSB2B1,B2(d1,f1,d2,f2,c):= BSB1B1c(d1,f1) BSB1B2c(d2,f2)
up to simultaneous permutation of rows and columns. A BSB2 matrix is a specific kind of BSB2
B0
matrix BSB2B (d, f, c, b) = BSB2B0,B-B0 (d, f, c, b, b), where B0 ∈ [2, B - 1] (where for B0 =
B0
B-1 the lower right hand block is a scalar b). We call the upper left hand block of BSB2B (d, f, c, b)
its main block.
B0
When φ grows quickly, Eq. (43) can converge to BSB2 fixed points of the form BSB2B (d, f, c, b),
up to simultaneous permutation of rows and columns. In this section we present several results on
BSB2 fixed points that sheds light on their structure. We leave the study of the forward Eq. (43) and
backward dynamics Eq. (51) to future work.
First, we give the eigendecomposition of BSB2 matrices.
Theorem K.2. Let Σ = BSB2bi,b2(d1,f1,d2,f2,c), where B1,B2 ≥ 2. Then G02{Σ}=
GΣG = ΣG has the following eigenspaces and eigenvalues.
1.	(B1 - 1)-dimensional eigenspace Z1 := {(x1, . . . , xB1 , 0, . . . , 0) : PiB=11 xi = 0}, with
eigenvalue d1 - f1.
2.	(B2 - 1)-dimensional eigenspace Z2 := {(0, . . . , 0, y1, . . . , yB2 ) : PjB=21 yj = 0}, with
eigenvalue d2 - f2.
3.	1-dimensional Rq with eigenvalue ⑷一丁回+®1-,'；^+31+/2-2。)—, Where q =
B1	B2
Z -、Z 人、
(B1-1,...,B1-1,-B2-1,...,-B2-1).
4.	1-dimensional R1 with eigenvalue 0.
Proof. We can verify all eigenspaces and their eigenvalues in a straightforward manner. These then
must be all of them by a dimensionality argument.	□
Specializing to BSB2 matrices, we get
Corollary K.3. Let Σ = BSB2b (d,f, c, b) with B0 ≥ 2. Then G02{Σ} = GΣG = ΣG has the
following eigenspaces and eigenvalues
•	(B0 - 1)-dimensional eigenspace Z1 := {(x1, . . . , xB0, 0, . . . , 0) : PiB=1 xi = 0}, with
eigenvalue d - f.
•	1-dimensional Rq with eigenvalue (d-f)(B-B )+Bf-C)B(B-B ), where q =
B0	B2
，------A------{/------λ---------{
(	B0-1,...,B0-1, -B2-1,..., -B2-1) where B2 = B - B0.
B0
• (B — B0)-dimensional eigenspace {(-μ,..., 一μ, yι,..., Ub—b，: μ = B-B7 PB=-B yj}
with eigenvalue 0.
92
Published as a conference paper at ICLR 2019
Note also that when B0 = B, then Z1 and Rq become the usual eigenspaces of BSB1(d, f).
B0
For fixed B0 < B, specializing the forward dynamics Eq. (43) to BSB2B fixed points yields a
2-dimensional dynamics on the eigenvalues for the eigenspaces Z1 and Rq . This dynamics in gen-
eral is not degenerate, so that the fixed point seems difficult to obtain analytically. Moreoever, the
Gegenbauer method, essential for proving that gradient explosion is unavoidable when Eq. (43) has
a BSB1 fixed point, does not immediately generalize to the BSB2 case, since Z1 and Rq in general
do not have the same eigenvalues so that we cannot reduce the integrals to that on a sphere. For this
reason, at present we do not have any rigorous result on the BSB2 case.
However, we expect that the main block of the BSB2 fixed point should undergo a similar dynamics
to that of a BSB1 fixed point, leading to similar gradient explosion.
K.1 B\SB21B Fixed Points
When φ grows extremely rapidly, for example φ(x) = relu(x)30, and the width of the network is
1
not too large, then we sometimes observe BSB2B fixed points
1d
B\SB2B (d, c, b) = cd1
where 1 ∈ RB-1 is the all 1s column vector. In these situations we can in fact see pathological
gradient vanishing, in a way reminiscent of the gradient vanishing for B = 2 batchnorm, as we shall
see shortly.
We can extend Corollary K.3 to the B0 = 1 case.
Theorem K.4. BSB2b (d, c, b)G is rank 1 and equal to λ<^02, where λ = B-I (d - 2c + b) and
q=(I, b⅛ ,…,b⅛ )q ⅛1∙
Proof. Straightforward verification.
□
Because B\SB2B under G-projection is rank 1, such a fixed point of Eq. (43) can be determined
B0
(unlike the general BSB2B case).
11
Theorem K.5. There is a unique BSB2b fixed point of Eq. (43), BSB2b (d*, c*,b*) where
d* = 2 (^φ(√B-1)2 + φ(-√B-Γ)2)
c* = 1 (φ(√B-1)φ( √=1=) + φ(-√B-i)φ( √=^=)}
2	B-1	B-1
b* = 2 Q(√B-亍)2+ φ(√B==亍y).
Additionally, BSB2b(d*,c*, b*)G = λ*产2, where
λ* = B-I	φ(√B-1)- φ( √B≡⅛)) + (φ(-√B-1)- φ( √⅛))
1
Proof. If Σ = BSB2B (d, c, b), then
VBφ ⑶=f(Eg∑g) φ(n(y 产
=1 [φ(√Bq产2 + φ(-√B q)02i
since GΣG = λ<^02 for some λ by Thm K.4. The rest then follows from straightforward ComPUta-
tion and another application of Thm K.4.
□
93
Published as a conference paper at ICLR 2019
1
The rank-1 nature of the BSB2B fixed point under G-projection is reminiscent of the case of B = 2
batchnorm. As we will see, gradients of a certain form will vanish similarly.
1
Let δ be a gradient vector. Then by Proposition G.4, for Σ* being the BSB2b fixed point, one step
of backpropagation yields
t
δ = VBrTG(I - VvT)Dδ
Σ=Σ*
where D = Diag(φ0(n(y))),r = ∣∣yk,v = y∕∣∣y∣∣, and y 〜N(0,GΣ*G). Because GΣ*G =
λ*<^02 by Thm K.5, n(y) = √Bv = ±√B(^ with equal probability and r 〜|N(0, λ*)∣ (the
absolute value of a Gaussian). Furthermore, G(I - <^02) = G - <^02 because Gq = (. Thus
t
δ = √B∣r∣-1(G - )^02)(φ0(√Bsgn(r)() Θ δ),r 〜N(0, λ*)
Σ=Σ*
dBφ
^dΣ
dBφ
^dΣ
Now notice the following
Lemma K.6. GB - ^?
0
GB-1
-1
B-T
0
-T
B-T
1____
ɪ B-1
-1
B-T
-0T
B-1
-1
B-T
1____
1 B-TZ
0
0
0
0
0
0
B-T
Proof. Straightforward computation.
□
Thus if δ = (δT, δ2)T ∈ RT × RB-T, then
dBφ
dΣ
t δ = √Bφ0 (∙√sgn(4) ∣r∣T(0,GB-Tδ2)T,r 〜N(0,λ*)
Σ=Σ*	B - 1
If δ2 = 0, then this is deterministically 0. Otherwise, this random variable does not have finite mean
because of the |r|-T term, and this is solely due to Σ* being rank 1. Thus as long as the gradient δ
is not constant in δ2, then we expect severe gradient explosion; otherwise we expect severe gradient
vanishing.
Remark K.7. We would like to note that, in pytorch, with either Float Tensors or Double Tensors,
the gradient empirically vanishes rapidly with generic gradient δ, when φ is rapidly increasing and
leads to a B\SB2B fixed point, e.g. φ(x) = relu(x)30. This is due to numerical issues rather than
mathematical issues, where the computation of (G - <^02)δ in backprop does not kill 6t completely.
L Weight Gradients
Recall weight gradient and the backpropagation equation Eq. (16) of the main text.
dL = X δι XlT
∂wlβ = J δαixβi ,
∂xl
δl = X	αj Wl+1δl+1
δαi =	∂h Wβɑ δβj
βj ∂hαi
where We have used Xei to denote Bφ(h%)i, δli = Vhi L, and subscript i denotes slice over sample
i in the batch, and subscript α denotes slice over neuron α in the layer. Then
E
2
∂L
∂Wβ
2
E
E
δαliXlβ-iT Xlβ-jT
i,j
94
Published as a conference paper at ICLR 2019
By gradient independence assumption (Appendix B),
Eδαlixlβ-i1δαljxlβ-j1=E[δαliδαlj]E[xlβ-i1xlβ-j1]
= Πl Σl-1
= ij ij .
Thus
E
2
∂ L
∂wαβ
ΠlijΣli-j1=hΠl,Σl-1i
i,j
If l is large and φ is well-behaved, then We expect Σl-1 ≈ Σ*, the unique BSB1 fixed point of
Eq. (43). Assuming equality, we then have
E
2
∂ L
∂wαβ
h∏l, Σ*i = hG02{Πl}, Σ*i
=h∏l,G02{Σ* }i = h∏l,μ*G>
=μ trΠl.
Thus in general, we expect the magnitude of weight gradients to follow the magnitude of hidden
unit gradient δil .
95