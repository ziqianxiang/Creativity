Published as a conference paper at ICLR 2019
Universal Stagewise Learning for Non-
Convex Problems with Convergence on Av-
eraged Solutions
Zaiyi Chen *	Zhuoning Yuan *	Jinfeng Yi
University of Science and Technology of China	University of Iowa	JD AI Research
Bowen Zhou	Enhong Chen	Tianbao Yang
JD AI Research	University of Science and Technology of China	University of Iowa
Ab stract
Although stochastic gradient descent (Sgd) method and its variants (e.g., stochas-
tic momentum methods, AdaGrad) are algorithms of choice for solving non-
convex problems (especially deep learning), big gaps still remain between the the-
ory and the practice with many questions unresolved. For example, there is still
a lack of theories of convergence for Sgd and its variants that use stagewise step
size and return an averaged solution in practice. In addition, theoretical insights
of why adaptive step size of AdaGrad could improve non-adaptive step size
of Sgd is still missing for non-convex optimization. This paper aims to address
these questions and fill the gap between theory and practice. We propose a uni-
versal stagewise optimization framework for a broad family of non-smooth non-
convex problems with the following key features: (i) at each stage any suitable
stochastic convex optimization algorithms (e.g., Sgd or AdaGrad) that return
an averaged solution can be employed for minimizing a regularized convex prob-
lem; (ii) the step size is decreased in a stagewise manner; (iii) an averaged solution
is returned as the final solution. Our theoretical results of stagewise AdaGrad
exhibit its adaptive convergence, therefore shed insights on its faster convergence
than stagewise Sgd for problems with slowly growing cumulative stochastic gra-
dients. To the best of our knowledge, this is the first work for addressing the
unresolved issues of existing theories mentioned earlier. Besides theoretical con-
tributions, our empirical studies show that our stagewise Sgd and AdaGrad
improve the generalization performance of existing variants/implementations of
Sgd and AdaGrad.
1 Introduction
Non-convex optimization has recently received increasing attention due to its popularity in emerging
machine learning tasks, particularly for learning deep neural networks. One of the keys to the
success of deep learning for big data problems is the employment of simple stochastic algorithms
such as Sgd or AdaGrad (Krizhevsky et al., 2012; Dean et al., 2012). Analysis of these stochastic
algorithms for non-convex optimization is an important and interesting research topic, which already
attracts much attention from the community of theoreticians (Ghadimi & Lan, 2013; 2016; Yang
et al., 2016; Davis & Drusvyatskiy, 2018b; Ward et al., 2018; Li & Orabona, 2018). However, one
issue that has been largely ignored in existing theoretical analysis is that the employed algorithms in
practice usually differ from their plain versions that are well understood in theory. Below, we will
discuss several important heuristics used in practice for training deep neural networks and the gap
between the practice and the theory to motivate this work.
First, a trick for setting the step size in training deep neural networks is to change it in a stagewise
manner from a large value to a small value, i.e., a constant step size is used in a stage for a number
of iterations and is decreased for the next stage (Krizhevsky et al., 2012; Ren et al., 2018). Although
* Equal contribution. Correspondence to: tianbao-yang@uiowa.edu
1
Published as a conference paper at ICLR 2019
this trick has been adopted by most open-sourced libraries, e.g., Caffe (Jia et al., 2014), Tensor-
Flow (Abadi et al., 2015), Pytorch (Paszke et al., 2017), it still lacks theoretical analysis to date for
non-convex optimization. A related question that stands out is how and when to decrease the step
size. On standard benchmark datasets for academic use such as CIFAR-10, CIFAR-100 (Krizhevsky
et al.), people could follow the setting reported in previous studies to get a good result, which how-
ever might not work well for new datasets. Hence, a better solution to setting the stagewise step size
with insights from theory would be much preferred. However, in the existing literature of theory
for non-convex optimization (Ghadimi & Lan, 2013; Davis & Drusvyatskiy, 2018b), only strategies
based on an iteratively decreasing step size or a small constant step size have been well analyzed.
For example, the existing theory usually suggests an iteratively decreasing step size proportional to
1∕√t at the t-th iteration or a small constant step size, e.g., proportional to e2 with E《1 for finding
an -stationary solution whose gradient’s magnitude (in expectation) is small than .
Second, the averaging heuristic is usually used in practice, i.e., an averaged solution is returned
for prediction (Bottou, 2010), which could yield improved stability and generalization (Hardt et al.,
2016). However, existing theory for many stochastic non-convex optimization algorithms only pro-
vides guarantee on a uniformly sampled solution or a non-uniformly sampled solution with decreas-
ing probabilities for latest solutions (Ghadimi & Lan, 2013; Yang et al., 2016; Davis & Drusvyatskiy,
2018b). In particular, if an iteratively decreasing step size proportional to 1∕√t at the t-th iteration
is employed, the convergence guarantee was provided for a random solution that is non-uniformly
selected from all iterates with a sampling probability proportional to 1∕√t for the t-th iterate. This
means that the latest solution always has the smallest probability to be selected as the final solution,
which contradicts to the common wisdom. If a small constant step size is used, then usually a uni-
formly sampled solution is provided with convergence guarantee. However, both options are rarely
used in practice and cannot justify the heuristic that returns the last solution.
A third common approach in practice is to use adaptive coordinate-wise step size of Ada-
Grad (Dean et al., 2012). Although adaptive step size has been well analyzed for convex problems
(e.g., why and when it could yield faster convergence than Sgd) (Duchi et al., 2011; Chen et al.,
2018b), it still remains an mystery for non-convex optimization with missing insights from theory.
Several recent studies have attempted to analyze AdaGrad for non-convex problems (Ward et al.,
2018; Li & Orabona, 2018; Chen et al., 2018a; Zou & Shen, 2018). Nonetheless, none of them are
able to exhibit the adaptive convergence of AdaGrad to data as in the convex case and its potential
advantage over Sgd for non-convex problems.
To overcome the shortcomings of existing theories for stochastic non-convex optimization, this paper
analyzes new algorithms that employ some or all of these commonly used heuristics in a systematic
framework, aiming to fill the gap between theory and practice. The main results and contributions
are summarized below:
•	We propose a universal stagewise optimization framework for solving a family of non-convex
problems, i.e., weakly convex problems, which is broader than smooth non-convex problems and
includes some non-smooth non-convex problems. At each stage, any suitable stochastic convex
optimization algorithms (e.g., Sgd, AdaGrad) with a constant step size parameter can be em-
ployed for optimizing a regularized convex problem with a number of iterations, which usually
return an averaged solution. The step size parameter is decreased in a stagewise manner following
a polynomial decaying scheme.
•	We analyze several variants of the proposed framework by employing different basic algorithms,
including Sgd, AdaGrad, stochastic heavy-ball (Shb) method, and stochastic Nesterov’s ac-
celerated gradient (Snag) method. We prove the convergence of their stagewise versions for an
averaged solution that is randomly selected from all stagewise averaged solutions.
•	To justify a heuristic approach that returns the last averaged solution in stagewise learning, we
present and analyze a non-uniform sampling strategy over stagewise averaged solutions with sam-
pling probabilities increasing as the stage number.
•	Regarding the convergence results, for stagewise SGD, SHB, SNAG, we establish the same order
of iteration complexity for finding a nearly stationary point as the existing theories of their non-
stagewise variants. For stagewise AdaGrad, we establish an adaptive convergence for finding a
nearly stationary point, which is provably better than (stagewise) Sgd, Shb, and Snag when the
cumulative growth of stochastic gradient is slow.
2
Published as a conference paper at ICLR 2019
•	Besides theoretical contributions, we also empirically verify the effectiveness of the proposed
stagewise algorithms. In particular, our empirical studies show that (i) the stagewise AdaGrad
dramatically improves the generalization performance of existing variants of AdaGrad, (ii)
stagewise Sgd, Shb, Snag also outperform their plain variants with an iteratively decreasing
step size; (iii) the proposed stagewise algorithms achieve similar if not better generalization per-
formance than their heuristic variants implemented in existing libraries on standard benchmark
datasets.
2 Related Work
Sgd for unconstrained smooth non-convex problems was first analyzed by Ghadimi & Lan (2013),
who established an O(1/4) iteration complexity for finding an -stationary point x in expectation
satisfying E[kVf(x)k] ≤ e, where f (∙) denotes the objective function. As mentioned earlier, the
returned solution is either a uniformly sampled solution or a non-uniformly sampled one with sam-
pling probabilities proportional to the decreasing step size. Similar results were established for the
stochastic momentum variants of Sgd (i.e., Shb, Snag) by Yang et al. (2016); Ghadimi & Lan
(2016). Recently, SGD was also analyzed for (constrained) weakly convex problems, whose objec-
tive function is non-convex and not necessarily smooth, by Davis & Drusvyatskiy (2018b). How-
ever, none of these studies provide results for algorithms that return an averaged solution, and these
analyzed algorithms differ significantly from that used in practice for achiving the state-of-the-art
results (Krizhevsky et al., 2012; Ren et al., 2018; Loshchilov & Hutter, 2017).
Although adaptive variants of Sgd, e.g., AdaGrad (Duchi et al., 2011), Adam (Kingma & Ba,
2015; Reddi et al., 2018), were widely used for training deep neural networks, there are few stud-
ies on theoretical analysis of these algorithms for non-convex problems. Several recent studies
attempted to analyze AdaGrad for non-convex problems (Ward et al., 2018; Li & Orabona, 2018;
Chen et al., 2018a; Zou & Shen, 2018). Although these studies have established an iteration com-
plexity of O(1/4) for different variants of ADAGRAD for finding an -stationary solution of a
stochastic non-convex optimization problem, none of them can exhibit the potential adaptive advan-
tage of AdaGrad over Sgd as in the convex case. Besides that, these studies also suffer from the
following shortcomings: (i) they all assume smoothness of the objective function, while we con-
sider non-smooth and non-convex problems; (ii) their convergence is provided on a solution with
minimum magnitude of gradient that is expensive to compute, though their results also imply a con-
vergence on a random solution selected from all iterates with decreasing sampling probabilities. In
contrast, these shortcomings do not exist in this paper. To the best of our knowledge, our result is the
first one that explicitly shows that coordinate-wise adaptive step size could yield faster convergence
than using non-adaptive step size for non-smooth non-convex problems, which is similar to that in
the convex case and was observed in practice for deep learning (Dean et al., 2012).
The proposed stagewise algorithm is similar to several existing algorithms in design (Xu et al.,
2017; Davis & Grimmer, 2017), which are originated from the proximal point algorithm (Rock-
afellar, 1976). I.e., at each stage a regularized convex subproblem is formed and then a stochastic
algorithm is employed for optimizing the regularized subproblem inexactly with a number of itera-
tions. Xu et al. (2017) used this idea for solving problems that satisfy a local error bound condition,
aiming to achieve faster convergence than vanilla Sgd. Davis & Grimmer (2017) are probably the
first who analyzed this idea for solving non-smooth weakly convex problems. In these two pa-
pers Sgd with decreasing step sizes for a strongly convex problem is employed at each stage for
solving the regularized subproblem. Our stagewise algorithm is developed following the similar
idea. The key differences from (Xu et al., 2017; Davis & Grimmer, 2017) are that (i) we focus on
non-convex problems instead of convex problems considered in (Xu et al., 2017); (ii) we analyze a
non-uniform sampling strategy with sampling probabilities increasing as the stage number to justify
a heuristic approach that uses the last averaged solution for prediction, unlike the uniform sampling
used in (Davis & Grimmer, 2017); (iii) we present a unified algorithmic framework and convergence
analysis, which enable one to employ any suitable stochastic convex optimization algorithms at each
stage. Importantly, it brings us several interesting variants including stagewise stochastic momentum
methods and stagewise AdaGrad. Finally, we note that a similar idea that adds a strongly convex
term into the non-convex objective has been considered in several recent works (Allen-Zhu, 2017;
Lan & Yang, 2018; Carmon et al., 2016). However, the key difference between these works and the
present work is that they need to assume the objective function is smooth or has a smooth component.
In addition, Allen-Zhu (2017); Lan & Yang (2018) consider finite-sum problems, and Carmon et al.
3
Published as a conference paper at ICLR 2019
(2016) consider deterministic problems. In contrast, we consider more general stochastic problems
without assuming the objective function is smooth.
3 Preliminaries
The problem of interest in this paper is:
min φ(x) = Eξ[φ(x; ξ)],	(1)
x∈Ω
where Ω ⊆ Rd is a closed convex set, ξ ∈ U is a random variable, φ(x) and φ(x; ξ) are non-convex
functions, with the basic assumptions on the problem given in Assumption 1.
To state the convergence property of an algorithm for solving the above problem. We need to in-
troduce some definitions. These definitions can be also found in related literature, e.g., Davis &
Grimmer (2017); Davis & DrUSvyatSkiy (2018b). In the sequel, We let k ∙ k denote an Euclidean
norm, [S ] = {1,...,S} denote a set, and 6ω(∙) denote the indicator function of the set Ω.
Definition 1. (Frechet subgradient) For a non-smooth and non-convexfunction f (∙),
∂Ff(x) = {v ∈ Rd|f (y) ≥ f(x) +v>(y-x) + o(ky - xk), ∀y ∈ Rd}
denotes the Freechet subgradient of f.
Definition 2. (First-order stationarity) For problem (1), a point X ∈ Ω is a first-order stationary
point if 0 ∈ ∂f (φ + 6q)(x), where 6q denotes the indicator function of Ω. Moreover, a point X
is said to be E-stationary ifdist(0, ∂f(φ + 6ω)(x)) ≤ G where dist denotes the Euclidean distance
from a point to a set.
Definition 3. (Moreau Envelope and Proximal Mapping) For any function f and λ > 0, the follow-
ing function is called a Moreau envelope of f
fλ(x) = minf (z) + ɪ∣∣z - x∣∣2.	(2)
z	2λ
Further, the optimal solution to the above problem denoted by
proxλf (x) = arg min f(z) + 2λ ∣∣z - x∣2
is called a proximal mapping of f.
Definition 4. (Weakly COnvex)AfunCtiOn f is μ-weakly convex (μ > 0), if f (x) + 2∣∣x∣2 is convex.
It is known that if f (x) is μ-weakly convex and Y < μ-1, then its Moreau envelope fγ(x) is
C1-smooth with the gradient given by (see e.g., Davis & Drusvyatskiy (2018b))
VfY(X) = YT(X — Proxγf(X)).
The tool of Moreau envelope is introduced to measure the convergence for optimizing non-smooth
and non-convex functions. A small norm of Vfγ (x) has an interpretation that x is close to a point
that is nearly stationary. In particular for any x ∈ Rd, let xb = proxγ f (x), then we have (Davis &
Drusvyatskiy, 2018b)
f(xb) ≤f(x),	∣x-xb∣ =Y∣Vfγ(x)∣, dist(0, ∂f (xb)) ≤ ∣Vfγ(x)∣,	(3)
where the first inequality follows from the definition of proxλf (x) such that f (b) + = l∣b - x∣2 ≤
f(x) (Please see Appendix H for a detailed proof of (3)). This means that a point x satisfying
∣Vfγ (x)∣ ≤ E is close to a point in distance of O(E) that is E-stationary.
It is notable that for a non-smooth function f (∙), there could exist a sequence of solutions {xk}
such that Vfγ(xk) converges while dist(0, ∂f(xk)) may not converge (Drusvyatskiy & Paquette,
2018). A simple example is to consider minx∈R |x|. As long as x 6= 0, dist(0, ∂f(x)) = 1 6= 0 no
matter how close is x to the stationary point 0. To handle such a challenging issue for non-smooth
non-convex problems, we will follow existing works (Davis & Drusvyatskiy, 2018a; Drusvyatskiy
& Paquette, 2018; Davis & Grimmer, 2017) to prove the near stationarity in terms of Vfγ (x).
In the case when f is smooth, ∣Vfγ (x)∣ is closely related to the magnitude of the gradient. In
4
Published as a conference paper at ICLR 2019
particular, let us define the projected gradient below, which is used as a criterion for non-convex
smooth optimization in the presence of constraints (Ghadimi & Lan, 2016; Allen-Zhu, 2017):
GY (X) = Y(X - Pr0xγδΩ (X - YVf(X))).	(4)
Note that the projected gradient becomes the normal gradient when the constraint X ∈ Ω is absent. It
was shown that when f (∙) is smooth with L-Lipschitz continuous gradient (DrUSvyatSkiy & Lewis,
2016):
(1 - Lγ)kGγ(X)k ≤ kVfγ(X)k ≤ (1 + Lγ)kGγ(X)k,∀X ∈ Ω.	(5)
Thus, the near stationarity in terms of Vfγ (X) implies the near stationarity in terms of Gγ (X) for a
smooth function f (∙) for a properly chosen γ > 0 (e.g., Y = 1∕(2L)). In this work, we define φγ (x)
as the Moreau envelope of φ(x) + 6q(x) as in (2) with f (x) replaced by φ(x) + 6q(x) and study
the convergence in terms of of Vφγ (X).
Now, we are ready to state the basic assumptions of the considered problem (1).
Assumption 1. (i) There is a measurable mapping g : Ω ×U → R such that Eξ [g(x; ξ)] ∈ ∂fφ(x)
for any X ∈ Ω; (ii) for any X ∈ Ω, E[∣∣g(x; ξ)k2] ≤ G2; (iii) there exists ∆φ > 0 such that
φ(x) 一 minz∈Ω φ(z) ≤ ∆φ forany X ∈ Ω; (iv) the objective function φ is μ-Weakly convex;
Remark: Assumption 1(i), (ii) assume a stochastic subgradient is available for the objective function
and its Euclidean norm square is bounded in expectation, which are standard assumptions for non-
smooth optimization. Assumption 1(iii) assumes that the objective value with respect to the optimal
value is bounded. Assumption 1(iv) assumes weak convexity of the objective function, which is
weaker than assuming smoothness. Below, we present some examples of objective functions in
machine learning that are weakly convex.
Ex. 1: Smooth Non-Convex Functions. If φ(∙) is a L-smooth function (i.e., its gradient is L-
Lipschitz continuous), then it is L-weakly convex. This will include the objective function for
neural networks with a smooth activation function (e.g., the sigmoid function) and a smooth loss
function (e.g., softmax loss).
Ex. 2: Convex and Smooth Composition. Consider φ(x; ξ) = h(c(x; ξ)) where h(∙) : Rm → R is
closed convex and M -Lipschitz continuous, and c(X; ξ) : Rd → Rm is nonlinear smooth mapping
with L-Lipschitz continuous gradient. This class of functions has been considered in (Drusvyatskiy
& Paquette, 2018) and it was proved that φ(x; ξ) is M L-weakly convex. An interesting example
is phase retrieval, where φ(x; a, b) = |(x>a)2 - b|. Another example related to deep learning is
that if c(x; ξ) denotes the mapping function of a neural network parameterized by x with a smooth
activation function and h denotes a non-smooth Lipschitz continuous convex loss function (e.g.,
hinge loss), then the resulting loss h(c(x; ξ)) is a weakly convex function of x. More examples of
this class can be found in (Davis & Drusvyatskiy, 2018a).
4 Stagewise Optimization: Algorithms and Convergence
In this section, we will present the proposed stagewise algorithms and their convergence results.
We will first present a Meta algorithmic framework highlighting the key features of the proposed
algorithms and then present several variants of the Meta algorithm.
The Meta algorithmic framework is described in Algorithm 1. There are several key features that
differentiate Algorithm 1 from existing stochastic algorithms that come with theoretical guarantee.
First, the algorithm is run with multiple stages. At each stage, a basic stochastic algorithm (SA)
is called to optimize a regularized problem fs (x) inexactly that consists of the original objective
function and a quadratic term, which is guaranteed to be convex due to the weak convexity of φ
and Y < μ-1. The convexity of fs allows one to employ any suitable existing stochastic algorithms
(cf. Theorem 1) that have convergence guarantee for convex problems. The returned solution from
the (s - 1)-th stage is used as a reference point for constructing fs and as an initial solution for
warm-start. It is notable that SA usually returns an averaged solution xs at each stage. Second, a
decreasing sequence of step size parameters ηs is used. At each stage, the SA uses a constant step
size parameter ηs and runs the updates for a number of Ts iterations. We do not initialize Ts as it
might be adaptive to the data as in stagewise ADAGRAD. The setup ofηs and Ts will depend on the
specific choice of SA, which will be exhibited later for different variants.
5
Published as a conference paper at ICLR 2019
Algorithm 1 A Meta Stagewise Algorithm: Stagewise-SA
1:	Initialize: a sequence of decreasing step size parameters {ηs}, x° ∈ Ω, γ < μ-1
2:	for s = 1, . . . , S do
3:	Let fs(∙) = φ(∙) + 2γ k ∙ -xs-1k2
4:	xs = SA(fs, xs-1, ηs, Ts)	xs is usually an averaged solution
5:	end for
To illustrate that Algorithm 1 is a universal framework such that any suitable SA algorithm can be
employed, we present the following result by assuming that SA has an appropriate convergence for
a convex problem.
Theorem 1. Let f (∙) be a convex function, x* = argminχ∈Ω f (x) and Θ denote some problem
dependent constants. Suppose for x+ = SA(f, x0, η, T ), we have
E[f(x+) - f(x*)] ≤ ε1(η,T,Θ)kx0 - x*k22 + ε2(η, T, Θ)(f(x0) - f(x*)) + ε3(η, T, Θ). (6)
Under Assumption 1(i), (iii) and (iv), by running Algorithm 1 with Y = 1∕(2μ), and with n,TS
satisfying ει(小工,Θ) ≤ 1∕(48γ),ε2(%,Ts, Θ) ≤ l∕2,ε3(ηs,Ts, Θ) ≤ c3∕sforsome c3 > 0, we
have
pΓ∣∣v ʃ 2 、|[2] V 32∆Φ(α +1) 1 48c3(α + I)
E[kvφγ(XT)k ] ≤ Y(S + 1)	+ Y(S + 1),
where T is randomly selected from {0,...,S} with probabilities pτ H (τ + 1)α, α ≥ L
Remark: It is notable that the convergence guarantee is provided on a stagewise average solution
xτ. To justify a heuristic approach that returns the final average solution for prediction, we analyze a
new sampling strategy that samples a solution among all stagewise average solutions with sampling
probabilities increasing as the stage number increases. This sampling strategy is better than uniform
sampling strategy or a strategy with decreasing sampling probabilities in the existing literature. The
convergence upper bound in (6) of SA covers the results of a broad family of stochastic convex
optimization algorithms. When ε2 (ηs, Ts , Θ) = 0 (as in SGD), the upper bound can be improved
by a constant factor. Moreover, We do not optimize the value of Y. Indeed, any γ < 1∕μ will work,
which only has an effect on constant factor in the convergence upper bound.
Next, we present several variants of the Meta algorithm by employing Sgd, AdaGrad, and
stochastic momentum methods as the basic SA algorithm, to which we refer as stagewise Sgd,
stagewise AdaGrad, and stagewise stochastic momentum methods, respectively. It is worth
mentioning that one can follow similar analysis to analyze other stagewise algorithms by using
their basic convergence for stochastic convex optimization, including RMSProp (Mukkamala &
Hein, 2017), AMSGrad (Reddi et al., 2018), stochastic alternating direction methods of multipliers
(ADMM) (Ouyang et al., 2013; Suzuki, 2013).
4.1	Stagewise Sgd
In this subsection, we analyze the convergence of stagewise Sgd, in which Sgd shown in Algo-
rithm 3 in the Appendix is employed in the Meta framework. Besides Assumption 1, we impose the
following bounded domain assumption in this subsection.
Assumption 2. There exists D > 0 such that ∣∣x 一 y k ≤ D for any x, y ∈ Ω.
It is worth mentioning that bounded domain assumption is imposed for simplicity, which is usu-
ally assumed in convex optimization. For machine learning problems, one usually imposes some
bounded norm constraint to achieve a regularization. Nevertheless, the bounded domain assumption
is not essential for the proposed algorithm. We present a more involved analysis in subsection 4.3 for
unbounded domain Ω = Rd. The following is a standard basic convergence result of SGD (Zinke-
vich, 2003), which clearly satisfies the bound in (6).
Lemma 1. For Algorithm 3, assume that f (∙) isconvex and Ekgtk2 ≤ G2, ∀t, then for any X ∈ Ω
E[f(bτ) - f(x)] ≤
kx - χo∣2 , nG2
+
2η(τ + 1)+ 2
The following theorem exhibits the convergence of stagewise Sgd.
6
Published as a conference paper at ICLR 2019
Algorithm 2 ADAGRAD(f, x0, η, *)
1:	Initialize: x1 = x0 , gi:θ = [], Ho ∈ Rd×d
2:	while T does not satisfy the condition in Theorem 3 do
3:	Compute a stochastic subgradient gt for f(xt)
4:	Update g1:t = [g1:t-1, g(xt)], st,i = kg1:t,ik2
5:	Set Ht = Ho + diag(st) and ψt(x) = 1 (X - xι)>Ht(x - xι)
6:	Let xt+ι = arg min ηx> (t PT =ι gτ) + 1 ψt(x)
7:	end while
8:	Output: XbT = PtT=1 Xt/T
Theorem 2. Suppose Assumption 1 and 2 hold. By setting Y = 1∕(2μ),小 =ηo∕s, TS = 12γs∕ηo
where ηo > 0 is a free parameter, then stagewise SGD (Algorithm 1 employing SGD) guarantees
that
τ7r∣∣v7, G ∖∣∣2] Z 16μ∆Φ(α +I) , 24μηo(G2(α +I)
E[kvo,(XT)k ] ≤ s + ι + -E—,
where G2 = 2G2 + 2γ-2D2, and T is Similarly defined as in Theorem 1.
Remark: To find a solution with E kvφγ (XT)k2 ≤ 2, we can set S = O(1∕2) and the total
iteration complexity PsS=1 Ts is in the order of O(1∕4). The above theorem is essentially a corol-
lary of Theorem 1 by applying Lemma 1 to fs(∙) at each stage. We present a complete proof in the
appendix.
4.2	Stagewise AdaGrad
One of the main contributions of the present work is to develop a variant of AdaGrad with adaptive
convergence to data for stochastic non-convex optimization. In this subsection, we analyze stagewise
AdaGrad and establish its adaptive complexity. In particular, we consider the Meta algorithm that
employs ADAGRAD in Algorithm 2 to optimize each fs. The key difference of stagewise ADAGRAD
from stagewise SGD is that the number of iterations Ts at each stage is adaptive to the history of
learning. It is this adaptiveness that makes the proposed stagewise AdaGrad achieve adaptive
convergence. It is worth noting that such adaptive scheme has been also considered in (Chen et al.,
2018b) for solving stochastic strongly convex problems. In contrast, we consider stochastic weakly
convex problems. Similar to previous analysis of AdaGrad (Duchi et al., 2011; Chen et al., 2018b),
we assume ∣∣g(x; ξ)k∞ ≤ G, ∀x ∈ Ω in this subsection. Note that this is stronger than Assumption 1
(ii). We formally state this assumption required in this subsection below.
Assumption 3. ∣∣g(x; ξ)∣∣∞ ≤ G for any X ∈ Ω.
The convergence analysis of stagewise AdaGrad is build on the following lemma, which is at-
tributed to Chen et al. (2018b) with a proof sketch provided in the Appendix.
Lemma 2. Let f(x) be a convex function, Ho = GI with G ≥ maxt ∣gt∣∞, and iteration number
T satisfy T ≥ M max{ G+max2JgLT,ik, C Pd=Ikg上7,/|} for some c > 0. Algorithm 2 returns an
averaged solution xbT such that
E[f (xT)-f(X*)] ≤ M kxo-x*k2 + M
where x* = argmi∏χ∈Ω f(x), gi：t = (g(xι), ∙ ∙ ∙, g(xt)) and gi：t,i denotes the i-th rowof gi：t.
(7)
The convergence property of stagewise AdaGrad is described by following theorem.
Theorem 3. Suppose Assumption 1(i), (iii), (iv), Assumption 2 and Assumption 3 hold. By setting
γ = 1∕(2μ), ηs = ηo∕√s, Ts ≥ Msmaχ{(G + maχi l|gs：Ts,ik)/(2c),cPi=I kgsz,ik} Where
ηo, c > 0 are free parameters, and Msηs ≥ 24γc, then stagewise ADAGRAD guarantees that
Er∣∣v⅛(X >∣∣21	1⅜Δφ^0+J)	4μ2η0(α + 1)
E[kvφγ(XT)k ] ≤ s + ι + c2(s +1),
where G = G + γ-1D, g1s:t,i denotes the cumulative stochastic gradient of the i-th coordinate at
the s-th stage, and τ is similarly defined as in Theorem 1.
7
Published as a conference paper at ICLR 2019
Remark: Note that the free parameter c is introduced to balance the two terms in the lower bound
of Ts due to that (G + maxi |环工/|) H G√T^ and Pd=1 ∣∣gSz,ik H dG√TS have different
orders when d is very large. One way to balance these two terms is to set C in the order of ʌ/ɪ/d,
resulting O(d/(S + 1)) for the second term in the above convergence bound. Another way is to
choose c in the s-th stage such that the two terms in the max of the lower bound of Ts match each
other. One can derive a similar order of O(d/(S + 1)) for the second term in the above convergence
bound. It is obvious that the total number of iterations PsS=1 Ts is adaptive to the data. Next,
let us present more discussion on the iteration complexity. Note that Ms = O(√S) by setting
c as a constant. By the boundness of stochastic gradient kgs：TS ik ≤ O(√TS), therefore Ts in
the order of O(s) will satisfy the condition in Theorem 3. Thus, in the worst case the iteration
complexity for finding E[kVφγ(XT)k2] ≤ a is in the order of PS=I O(S) ≤ O(1∕e4). To show
the potential advantage of adaptive step size as in the convex case, let us consider a good case
when the cumulative growth of stochastic gradient is slow, e.g., assuming kg1s:T ,ik ≤ O(Tsα)
with α < 1/2. Then Ts = O(s1/(2(1-a))) will work, and then the total number of iterations
PS=I Ts ≤ S1+1/(2(1-a)) ≤ O(1/€2+1/(1-a)), which is better than O(1∕e4).
4.3	Stagewise stochastic momentum (SM) methods
Finally, we present stagewise stochastic momentum (SM) methods and their convergence results. In
the literature, there are two popular variants of stochastic momentum methods, namely, stochastic
heavy-ball method (Shb) and stochastic Nesterov’s accelerated gradient method (Snag). Both
methods have been used for training deep neural networks (Krizhevsky et al., 2012; Sutskever et al.,
2013), and have been analyzed by (Yang et al., 2016) for non-convex optimization. To contrast with
the results in (Yang et al., 2016), we will consider the same unified stochastic momentum methods
that subsume Shb, SNAG and SGD as special cases when Ω = Rd. The updates are presented in
Algorithm 4 in the Appendix. There are two additional parameters: β ∈ (0, 1) is the momentum
parameter and ρ is a parameter that can vary between [0, 1/(1 - β)]. By changing the value of ρ,
we can obtain the three variants, SHB (ρ = 0), SNAG (ρ = 1) and SGD (ρ = 1/(1 - β)). Due to the
limit of space, we only present the convergence of stagewise SM methods below.
Theorem 4. Suppose Assumption 1 holds. By setting Y = 1∕(2μ), ηs = (1 一 β)γ∕(96s(ρβ + 1)),
Ts ≥ 2304(ρβ + 1)s, with τ similarly defined as in Theorem 1 stagewise SM methods guarantee
E[kVφγ(Xτ)k2] ≤
16μ∆φ(α + 1)	G2(β + 48(2ρβ + 1)(1 一 β))(α + 1)
STi+	96(ρβ +1)(1 — β)(S + 1)
Remark: The bound in the above theorem is in the same order as that in Theorem 2. The total
iteration complexity for finding a solution Xτ with E kVφγ (Xτ)k2 ≤ 2 is O(1/4) similar to that
achieved in (Yang et al., 2016).
5	Experiments
In this section, we present some empirical results to verify the effectiveness of the proposed stage-
wise algorithms. We use two benchmark datasets, namely CIFAR-10 and CIFAR-100 (Krizhevsky
et al.) for our experiments. We implement the proposed stagewise algorithms in TensorFlow.
We compare different algorithms for learning ResNet-20 (He et al., 2016) with batch normaliza-
tion (Ioffe & Szegedy, 2015) adopted after each convolution and before ReLu activation.
Baselines. We compare the proposed stagewise algorithms with their variants implemented in Ten-
sorFlow. It is notable that AdaGrad has a step size (aka learning rate) parameter 1, which is a
constant in theory (Li & Orabona, 2018; Chen et al., 2018a; Zou & Shen, 2018). However, in the
deep learning community a heuristic fixed frequency decay scheme for the step size parameter is
commonly adopted (Ren et al., 2018; Wilson et al., 2017). We thus compare two implementations
of AdaGrad - one with a constant learning rate parameter and another one with a fixed frequency
decay scheme, which are referred to as AdaGrad (theory) and AdaGrad (heuristic). For each
baseline algorithms of Sgd, Shb, Snag, we also implement two versions - a theory version with
iteratively decreasing size no/ʌ/t suggested by previous theories and a heuristic approach with fixed
frequency decay scheme used in practice, using (theory) and (heuristic) to indicate them. The fixed
1note it is not equivalent to the step size in Sgd.
8
Published as a conference paper at ICLR 2019
——AMSGRAD
ADAGRAD (heuristics)
——ADAGRAD (Bieoiy)
——StageW⅛⅜AD⅛GR⅛D
ResNet-20 (ReLU) CIFAR10
∙lαuujβu=sβl
UW βuw
ResNet-20 (ReLU)CIFARIO
∙lαuujβu=sβl
SHB (heuristics)
——SHB(Bwoiy)
——S⅛gewls⅜~SHB
ResNet-20 ReLU CIFAR10
ResNet-20 (ReLU) CIFAR10
SNAG (heuristics)
——SNAG(theoιy)
----StageWlae~SNAG
——AMSGRAD
ADAGRAD (heuristics)
——ADAGRAD (theory)
——StagWTiADAGRAD
20	40	60	80 ICO
# of Iterations (*800)
ResNet-20 (ReLU) C∣FAR100
ɪ I
.JαuujBssβl
^^1 .
6ssβl
20	40	60	80	1∞
#of Iterations (,800)
ResNet-20 (ReLU)CIFARITO
SHB(heurtetlcs)
——SHB(theoιy)
----StageWtee~s HB
20	40	60	80	18
# of Iterations (*8∞)
ResNet-20 (ReLU)CIFARIOO
20	40	60 SO 100
# of Iterations (*800)
ResNet-20 (ReLU)CIFARIOO
.JαuujBssβl
Figure 1: Comparison of Testing Error on CIFAR-10 (top) and CIFAR-100 (bottom).
20	40	60	80	100
# of Iterations (*800)
20	40	60	80	100
# of Iterations (*8∞)


frequency decay scheme used in the heuristic variants is similar as that in (He et al., 2016), i.e., the
step size parameter is decreased by 10 at 40k, 60k iterations. We also compare stagwise AdaGrad
with AMSGrad (Reddi et al., 2018) - a corrected version of Adam.
Parameters. The stagewise step size η = ηo∕√s is used in stagwise AdaGrad, the num-
ber of iterations Ts in stagwise ADAGRAD is set according to Theorem 3 with some simplifi-
cations for dealing with unknown G^, in particular We set Ts to the smallest value larger than
T0 s maxi kg1s:T ,ik Pi kg1s:T ,ik. For stagewise SGD, SHB, SNAG, the stagewise step size and
iteration number is set to ηs = no/s and Ts = T°s, respectively. The involved parameters in the
compared algorithms are tuned for the best performance, including the initial step size parameter η0
of all algorithms, the value of T0 and γ for our stagewise algorithms.
Results. We consider two settings - with/without an `2 norm regularization on weights. For compar-
ison, we evaluate the training error and testing error of obtained solutions in the process of training.
For our stagewise algorithms, the evaluation is done based on the current averaged solution, and for
other baselines the evaluation is done based on the current solution. Due to the limit of space, we
only show the results of testing error on CIFAR-10 and CIFAR-100 for the setting without regu-
larization in Figure 1. All results are included in the Appendix. From all results, we have several
observations. (i) The proposed stagewise algorithms perform much better in terms of testing error
than the existing theoretical versions reported in the literature (marked with theory in the legend).
This indicates the proposed stagewise step size scheme is better than iteratively decreasing step size
scheme. (ii) The proposed stagewise algorithms achieve similar and sometimes even better testing
error than the heuristic approaches with a fixed frequency decay scheme used in practice. However,
the heuristic approaches usually give smaller training error. This seems to indicate that the proposed
algorithms are less vulnerable to the overfitting. In another word, the proposed algorithms have
smaller generalization error, i.e., the difference between the testing error and the training error.
6	Conclusion & Future Work
In this paper, we have proposed a universal stagewise learning framework for solving stochastic non-
convex optimization problems, which employs well-known tricks in practice that have not been well
analyzed theoretically. We provided theoretical convergence results for the proposed algorithms for
non-smooth non-convex optimization problems. We also established an adaptive convergence of a
stochastic algorithm using data adaptive coordinate-wise step size of AdaGrad, and exhibited its
faster convergence than non-adaptive stepsize when the growth of cumulative stochastic gradients is
slow similar to that in the convex case. For future work, one may consider developing more variants
of the proposed meta algorithm, e.g., stagewise AMSGrad, stagewise RMSProp, etc. We will also
consider the empirical studies on the large-scale ImageNet data set.
References
Mart´n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
9
Published as a conference paper at ICLR 2019
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wat-
tenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learn-
ing on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software
available from tensorflow.org.
Zeyuan Allen-Zhu. Natasha: Faster non-convex stochastic optimization via strongly non-convex
parameter. In Proceedings of the 34th International Conference on Machine Learning (ICML),
pp. 89-97, 2017.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
International Conference on Computational Statistics (COMPSTAT), pp. 177-187, 2010.
Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-
convex optimization. CoRR, abs/1611.00756, 2016.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of adam-type
algorithms for non-convex optimization. CoRR, abs/1808.02941, 2018a.
Zaiyi Chen, Yi Xu, Enhong Chen, and Tianbao Yang. Sadagrad: Strongly adaptive stochastic gradi-
ent methods. In Proceedings of the 35th International Conference on Machine Learning (ICML),
2018b.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex
functions. CoRR, abs/1803.06523, 2018a.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic subgradient method converges at the rate
o(k-1/4) on weakly convex functions. CoRR, /abs/1802.02988, 2018b.
Damek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method for nons-
mooth, nonconvex problems. arXiv preprint arXiv:1707.03505, 2017.
Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao,
Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale
distributed deep networks. In NIPS, pp. 1223-1231, USA, 2012. Curran Associates Inc.
D. Drusvyatskiy and C. Paquette. Efficiency of minimizing compositions of convex functions and
smooth maps. Mathematical Programming, Jul 2018.
Dmitriy Drusvyatskiy and Adrian S. Lewis. Error bounds, quadratic growth, and linear convergence
of proximal methods. arXiv:1602.06661, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Math. Program., 156(1-2):59-99, 2016.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In Proceedings of the 33nd International Conference on Machine Learning
(ICML), pp. 1225-1234, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning, (ICML), pp. 448-456, 2015.
10
Published as a conference paper at ICLR 2019
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Ser-
gio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed-
ding. arXiv preprint arXiv:1408.5093, 2014.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search).
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep con-
volutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pp.
1106-1114, 2012.
Guanghui Lan and Yu Yang. Accelerated stochastic algorithms for nonconvex finite-sum and multi-
block optimization. CoRR, abs/1805.05411, 2018.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. CoRR, abs/1805.08114, 2018.
Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. In ICLR, volume
abs/1608.03983, 2017.
Mahesh Chandra Mukkamala and Matthias Hein. Variants of RMSProp and Adagrad with log-
arithmic regret bounds. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 2545-2553, International Convention Centre, Sydney, Australia, 06-11 Aug 2017.
PMLR.
Hua Ouyang, Niao He, Long Tran, and Alexander Gray. Stochastic alternating direction method of
multipliers. In International Conference on Machine Learning, pp. 80-88, 2013.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. arXiv preprint arXiv:1803.09050, 2018.
R. Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on
Control and Optimization, 14:877-898, 1976.
R.T. Rockafellar. Convex Analysis. Princeton mathematical series. Princeton University Press, 1970.
Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of ini-
tialization and momentum in deep learning. In Proceedings of the 30th International Conference
on Machine Learning (ICML), pp. 1139-1147, 2013.
Taiji Suzuki. Dual averaging and proximal gradient descent for online alternating direction multi-
plier method. In International Conference on Machine Learning, pp. 392-400, 2013.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes. CoRR, abs/1806.01811, 2018.
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In NIPS, pp. 4151-4161, 2017.
Yi Xu, Qihang Lin, and Tianbao Yang. Stochastic convex optimization: Faster local growth implies
faster global convergence. In ICML, pp. 3821 - 3830, 2017.
Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. volume abs/1604.03257, 2016.
11
Published as a conference paper at ICLR 2019
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
ICML,pp. 928-936, 2003.
Fangyu Zou and Li Shen. On the convergence of adagrad with momentum for training deep neural
networks. CoRR, abs/1808.03408, 2018.
12
Published as a conference paper at ICLR 2019
Algorithm 3 SGD(f, x0, η,T)
for t = 0,...,T do
Compute a stochastic subgradient gt for f(xt).
χt+ι = ∏c[χt - ηgt]
end for
Output： XT = PT=O Xt∕(T + 1)
Algorithm 4 Unified Stochastic Momentum Methods: SUM(f, χ0, η, T)
Set parameters: ρ ≥ 0 and β ∈ (0, 1).
for t = 0, . . . , T do
Compute a stochastic subgradient gt for f(χt).
yt+1 = χt - ηgt
yXt+1 = χt - ρηgt
χt+1 = yt+1 + β(yXt+1 - yXt)
end for
Output: χXT = PtT=0χt∕(T+ 1)
A	More Experimental Results
In this section, we present more experimental results. Comparison of training and testing er-
ror in the two settings (w/o regularization) on the two data sets are plotted in Figure 2, 3, 4,
5. We also report the final testing error (after running 80k iterations) of different algorithms in
the two settings on the two datasets in Table 1. For parameter tuning, the initial step sizes of
all algorithms are tuned in {0.1, 0.3, 0.5, 0.7, 0.9}. The value of γ of stagewise algorithms is
tuned in {1, 10, 100, 500, 1000, 1500, 2000, 3000}. The initial value T0 for stagewise SGD, SHB,
SNAG is tuned in {10, 100, 1k, 5k, 6k, 7k, 10k, 20k}, and that for stagewise ADAGRAD is tuned in
{1, 10, 15, 20, 25, 50, 100}.
B Proof of Theorem 1
Proof. Below, we use Es to denote expectation over randomness in the s-th stage given all history
before s-th stage. Define
Zs = arg xx∈in fs(χ) = Proχγ(φ+δα) (Xs-I)	(8)
Then Vφγ(Xs-I) = YT(Xs-I - zs). By applying the convergence bound of SA to fs(x), We have
Es[fs(χs) - fs(zs)] ≤ ε1(ηs,Ts,Θ)kχs-1 - zsk22 + ε2(ηs,Ts, Θ)(fs(χs-1) - fs(zs)) +ε3(ηs,Ts, Θ) .
'----------------------------------------------------{z-------------------------------}
Es
It then folloWs that
Es φ(xs) + 2Yγ∣∣xs - xs-ιk2 ≤ fs(zs) + Es ≤ fs(xs-ι) + Es
≤ φ(Xs-1) +Es	(9)
On the other hand, We have that
∣xs - xs-1∣2 =∣xs - zs +zs - xs-1∣2
=∣xs - zs∣2 + ∣zs - xs-1∣2 + 2hxs - zs, zs - xs-1i
≥(1 - αs-1)∣xs - zs ∣2 + (1 - αs)∣xs-1 - zs ∣2
13
Published as a conference paper at ICLR 2019
Table 1: COmPariSOn of Final TeSting Error(%) on CIFAR-10 and CIFAR-100 DataSetS
I CIFAR-10 I CIFAR-100
Algorithms	∣ with reg. ∣ without reg. ∣ with reg. ∣ without reg.
SGD (theory)	16.25	19.18	43.51	45.78
SGD (heuristic)	8.34	10.81	33.67	37.19
Stagewise-SGD	8.34	9.01	32.25	34.95
SHB (theory)	15.67	16.55	39.15	46.23
SHB (heuristic)	8.58	10.28	33.30	37.56
Stagewise-SHB	8.30	8.61	32.85	34.49
SNAG (theory)	17.64	16.76	39.34	44.21
SNAG (heuristic)	8.85	10.34	33.89	36.84
Stagewise-SNAG	8.00	8.93	31.42	33.29
AMSGrad	10.76	11.13	38.62	39.96
AdaGrad (theory)	12.11	13.96	39.09	44.49
AdaGrad (heuristic)	10.71	13.80	37.04	41.06
Stagewise-AdaGrad	9.09	9.51	33.95	34.62
ResNet-20 (ReLU)ClFARlO
ResNet-20 (ReLU)CIFARIO
ResNet-20 (ReLU)CIFARIO
ResNet-20 (ReLU) CIFAR10
20	40	60	80	100
# of Iterations (*800)
ResNet-20 (ReLU)CIFARIO
20	40	60	80	1∞
# of Iterations (*8∞)
ResNet-20 (ReLU)CIFARIO
——AMSGRAD
ADAGRAD (heuristics)
——ADAGRAD (theory)
——StagWTiADAGRAD
I I
.JαuujBssβl
Bssβl
ResNet-20 (ReLU)CIFARIO
——AMSGRAD
ADAGRAD (heuristics)
——ADAGRAD (Bieoiy)
——StageW⅛⅜AD⅛GR⅛D
.JαuujBU-U-BJl
20	40	60	80	100
# of Iterations (*800)
ResNet-20 (ReLU)CIFARIO
EU-BJl
AMSGRAD
ADAGRAD (heuristics)
——ADAGRAD (theory)
——StagWTiADAGRAD
SHB(heurtetlcs)
——SHB(theoιy)
----StageWtee~s HB
ResNet-20 (ReLU)CIFARIO
.JαuujBssβl
20	40	60 so 100
# of Iterations (*800)
20	40	60	80	100
# of Iterations (*8∞)
Figure 2: Comparison of Training Error (Top) and Testing Error (bottom) on CIFAR-10 without
Regularization.
,c ResNet-20 (ReLU) CIFAR10	ResNet-20 (ReLU)CIFARIO	ResNet-20 (ReLU)CIFARIO	ResNet-20 (ReLU)CIFARIO
ι∞
ResNet-20 (ReLU)CIFARIO
20	40	60	80	1∞
# of Iterations (*8∞)
ResNet-20 (ReLU)CIFARIO
ResNet-20 (ReLU)CIFARIO
.JαuujBu-}sβl
——SHB (heuristics)
——SHB(theoιy)
----Stagewiae-SHB
20	40	60	80	100
# of Iterations (*8∞)
Emi
20	40	60	80	100
# of Iterations (*800)
.JαuujBU-U-BJl

Figure 3: Comparison of Training Error (Top) and Testing Error (bottom) on CIFAR-10 with Regu-
larization. The regularization parameter is set 5e - 4.
where the inequality follows from the Young,s inequality with 0 < αs < 1. Combining the above
inequality with (9) we have that
ES [(1 - αs) kxs-1 - Zsk2] ≤ Es [φ(Xs-1) - φ(xs) + (U - 1IIXS- Zsk2 + ES
L 2Y	」 L	2Y	」
≤ Es
φ(Xs-1 ) - φ(Xs) +
Y(F1」(fs(Xs) - fs (Zs))+ ES
α-1 — γμ
≤ ES 卜(XST)- 2) + E
≤ Es φ(Xs-1) - Φ(Xs)
+ ES
α-1 — γ“
JE"工，⑼kX-
-Zsk2 + ε2(ηs,Ts, Θ)(fs(Xs-1) - fs (Zs)) + ε3(ηs,Ts , Θ)}
14
(10)
Published as a conference paper at ICLR 2019
——AMSGRAD
ADAGRAD (heuristics)
——ADAGRAD (theoiy)
---StagewlBADAGRAD
.JαuujBU-U-BJl
SGD (heuristics)
——SGD(theαy)
——StageWtee~3G D
ResNet-20 ReLU CIFAR100
.JαuujBU-U-BJl
20	40	60	80	100
#	of Iterations (*800)
ResNet-20 (ReLU) C∣FAR100
——AMSGRAD
ADAGRAD (heuristics)
——ADAGRAD (theory)
---StagewtefrADAGRAD
ɪ I
.JαuujBssβl
20	40	60	80	100
# Of Iterations (*800)
^^1 .
6ssβl
——AMSGRAD
ADAGRAD (heuristics)
——ADAGRAD (0ιeoιy)
---Stagewlse-ADAGRAD
.JαuujBU-U-BJl
20	40	60	80	100
# of Iterations (*800)
ResNet-20 (ReLU)CIFARITO
EU-BJl
SHB (heuristics)
SHB (theoiy)
StagewIseHSHB
SNAG (heurtetlcs)
——SNAG (theoiy)
——StageW⅛e~SNAG
ResNet-20 ReLU CIFAR100
ResNet-20 (ReLU)CIFARIoO
20	40	60	80	100
#of Iterations Γ800)
40	60	80	1∞
# Of Iterations (*800)
20	40	60	80	1∞
# Of Iterations (*8∞)
ResNet-20 (ReLU)C∣FAR1∞
I	SGD (heuristics) I
——SGD(Bwoiy)
I——S⅛cew⅛e∙3GD∣
20	40	60	80	100
#of Iterations (*800)
SHB (heuristics)
SHB (theoiy)
StageWtee^HB
SNAG (heuristics)
——SNAG (theoiy)
-----Stagewlee-SNAG
ResNet-20 (ReLU)CIFARIOO
ResNet-20 (ReLU)CIFARIOO
.JαuujBssβl
20	40	60	80	100
#	of Iterations (*8∞)
20	40	60	80	100
#	of Iterations (*800)
Figure 4: Comparison of Training Error (Top) and Testing Error (bottom) on CIFAR-100 without
Regularization.
,c ResNet-20 (ReLU)CIFARIOO	ResNet-20 (ReLU) CIFAR100	ResNet-20 (ReLU)CIFARITO	ResNet-20 (ReLU)CIFARIOO
20	40	60	80	100
#of Iterations Γ800)
40	60	80	1∞
# Of Iterations (*800)
40	60	80	1∞
# Of Iterations (*8∞)
ResNet-20 (ReLU) CIFAR100
ResNet-20 (ReLU)CIFARIOO
——AMSGRAD
ADAGRAD (heuristics)
——ADAGRADjttWOm
---StagewtefrADAGRAD
.Jαuuj6u-}sβl
# of Iterations (*800)
^^1
——SGD (heuristics)
——SGD(th∞ιy)
——Stacewtee-SGD
——SHB (heuristics)
——SHB (theoiy)
——Stagewlae-SHB
ResNet-20 (ReLU)CIFARIOO
I
Emi
20	40	60	80	100
#of Iterations (*800)
# of Iterations (*8∞)
# of Iterations (*800)

^^1
Figure 5:	Comparison of Training Error (Top) and Testing Error (bottom) on CIFAR-100 with Reg-
ularization. The regularization parameter is set 5e - 4.
ResNet-20 (ReLU) CIFAR10
ResNet-20 (ReLU)CIFARIO
,Jew BU-U-BJl
——Stagewise-SGD
Stagewise-ADAGRAD
,' Stagewise-SHB
---Stagawis⅜SNAG
6 4
O O
,Jew 3βθl
——Stagewise-SGD
Stagewise-ADAGRAD
,' Stagewise-SHB
----Stagawis⅜SNAG
20	40	60	80	100
# of iterations (*β00)
20	40	60	80	100
# of iterations (*β00)
ResNet-20 (ReLU) CIFAR100
64.
O O
,JQJ.JW BU-U-BJl
——Stagewise-SHB
Stagewise-ADAGRAD
■' Stagewise-SGD
----Stagawis⅜SNAG
0%	20	40	¢0	80	100
# of iterations (*β00)
ɪ ResNet-20 (ReLU) CIFAR100
-06
∙JOw 3βθl
20	40	60	80
# of iterations (*β00)
100
Figure 6:	Comparison of different stagewise algorithms in terms of Training Error and Testing Error
on CIFAR-10 (top) and CIFAR-100 (bottom) with regularization. The regularization parameter is
set 5e - 4.
where the second inequality uses the strong convexity of ∕s(x), whose strong convexity parameter
is YT - μ. Next, we bound ∕s(xs-ι) - ∕s(zs) given that Xs-ι is fixed. According to the definition
15
Published as a conference paper at ICLR 2019
of fs(∙), We have
fs(Xs-l) - fs(Zs) = φ(Xs-l) — φ(Zs) — 2Y ∣∣Zs - Xs-Ik2
=φ(Xs-I) — φ(Xs) + φ(Xs) — φ(Zs) — 2γ kzs — Xs-Ik2
=[φ(Xs-I) —	φ(Xs)]	+	fs(Xs)	— fs(zs)	+	2γ kzs	— Xs-Ik2	—	2γ llXs — Xs-Ik2	—	2γ kzs —	Xs-Ik2
≤ [φ(Xs-1) — φ(Xs)] + [fs(Xs) — fs(Zs)].
Taking expectation over randomness in the s-th stage on both sides, We have
fs(Xs-1) — fs(zs) ≤ Es[φ(Xs-1) — φ(Xs)] +Es[fs(Xs) — fs(zs)]
≤ E[φ(Xs-1) — φ(Xs)] + ε1(ηs,Ts,Θ)kXs-1 — zsk22 +ε2(ηs,Ts,Θ)(fs(Xs-1) — fs(zs)) + ε3(ηs,Ts, Θ).
Thus,
(1 — ε2(ηs,Ts,⑼)(fs(Xs-I)- fs(zs)) ≤ E[φ(Xs-ι) — φ(Xs)] + ει(ηs,Ts, Θ)∣∣Xs-ι — zsk2 + ε3(ηs,Ts, Θ).
Assuming that ε2(ηs, Ts, Θ) ≤ 1/2, We have
ε2(ηs, Ts, Θ)(fs(Xs-1) — fs(zs)) ≤ Es[φ(Xs-1) — φ(Xs)] + ε1(ηs,Ts, Θ)kXs-1 — zsk22 + ε3(ηs, Ts, Θ).
Plugging this upper bound into (10), We have
Es [(1 - αs) kXs-1 — zsk2l ≤ Es [φ(Xs-ι) — φ(Xs)
2γ
-1
+ EsK …工,⑼kXsT
— zsk2 + φ(Xs-1) — φ(Xs) +2ε3(ηs, Ts, Θ)}
(11)
By setting as = 1/2, Y = 1∕(2μ) and assuming ει(ηs,Ts, Θ) ≤ 1∕(48γ), We have
EsE kXs-1—zsk2
≤ 4Es φ(Xs-1) — φ(Xs) + 6ε3(ηs, Ts, Θ)}
Define ws = sα . Multiplying both sides by ws, We have that
wsγEs[kVφγ(Xs-I)k2] ≤ Es 32ws∆s +48ε3(ηs,Ts, Θ)ws
By summing over s = 1, . . . , S + 1, We have
S+1
X wsE[kVφγ(Xs-1)k2] ≤ E
s=1
32 x+1 ws∆s + 48 X wsε3(ηs,Ts, Θ)
γ s=1	γ s=1
Taking the expectation W.r.t. τ ∈ {0, . . . , S}, We have that
E[kVφγ(Xτ)k2]] ≤ e[32P=+WsA +48PS+l1psS+(ηs,τs,θ))]
γ s=1 ws	γ	s=1 ws
For the first term on the R.H.S, We have that
S+1	S+1	S+1	S+1
∑ ws∆s =	ws(φ(Xs-1) — φ(Xs)) =	(ws-1φ(Xs-1) — wsφ(Xs)) +	(ws — ws-1)φ(Xs-1)
S+1
= w0φ(X0) — wS+1φ(XS+1) +	(ws — ws-1)φ(Xs-1)
s=1
S+1	S+1
=	(ws — ws-1)(φ(Xs-1) — φ(XS+1)) ≤ ∆φ	(ws — ws-1) = ∆φwS+1
16
Published as a conference paper at ICLR 2019
Then,
E[kVφγ (xτ )k2]
32∆φws+ι 48 PS+1 Wsε3(ηs,Ts, Θ)
Y∑S+⅛ + -Y∑S+⅛一
The standard calculus tells that
SS	1
Sα≥sa ≥ J Xadx = —pɪsα+1
S
X sα-1 ≤ SSα-1 = Sα, ∀α ≥ 1,
s=1
S
Xsα-1
s=1
≤ Z xα-1dx
Sα
——,∀0 < α < 1
α
≤
Combining these facts and the assumption ε3(ηs , Ts , Θ) ≤ c/s, we have that
(32δφS+I) + 48c(a+1)	≥ 1
γ(s+i) + γ(s+i)	α ≥ 1
32∆φ(a+1) + 48c(a+1)0 <	1
γ(S+1) + γ(S+1)a	0 < α < 1
In order to have E[kVφγ (XT )k2] ≤ a，we can set S = O(1∕e2). The total number of iterations is
SS
XTs ≤ X12γs ≤ 6γS(S+1) = O(1/4)
s=1	s=1
□
C Proof of Theorem 2
Proof. The proof is almost a duplicate to that of Theorem 1. Define ws = sα. We apply Lemma 1
to each call of Sgd in stagewise Sgd,
E[fs(Xs) - fs(zs)] ≤ kzs /-W + ηs2G2,
Ss^^z}
Es
where G2 is the upper bound of E[kg(x; ξ) + YT(X — Xs-ι)k2], which exists and can be set to
2G2 + 2γ-2D2 due to the Assumption 1-(ii) and the bounded assumption of the domain. Then
following the same analysis as that in the proof of Theorem 1, we have
Es [(ɪɪkXs-1-Zsk2
≤Qφ(Xs-I)- φ(Xs)+ (α-2γ^kXs - zsk2 + Es
≤E
≤E
φ(Xs-1) - φ(Xs) +
γ((Y-[(fs(Xs) - fs(zs))+ Es
α-1 — γ μ
φ(XsT)- φ(xs) + EEsJ	(12)
Combining the above inequalities, we have that
((1 - αS)Y- U:T )Es[kvφγ (Xs-I)k2] ≤ EsEs + (WnsG 2:
Multiplying both sides by ws, we have that
ws ((1 - as)γ - γ2(α-1 - μγ) )Es[∣∣Vφγ(Xs-I)k2] ≤ Es [2ws∆s + (。「- 〃加；涡2
∖	(I- μγ)nsτs J	L	(I- μγ)
By setting as = 1/2 and Y = 1∕(2μ), Tsns ≥ 12γ, We have
4wsγEs[kVφγ(Xs-I)k2] ≤ Es[2ws∆s + 3wsnsG2]
17
Published as a conference paper at ICLR 2019
By summing over s = 1, . . . , S + 1, we have
S+1	S+1	S+1
X WsE[kVφγ(Xs-I)k2] ≤ E 16μ X Ws∆s + 24μ X WsnsG2
s=1	s=1	s=1
Taking the expectation w.r.t. τ ∈ {0, . . . , S}, we have that
E[kVφγ(Xτ)k2]] ≤ E
-16μ PS+1 Ws∆s
-PS+ι1 ws
+
By similar analysis, we have that
E[kVφγ(Xτ)k2] ≤
16μ∆φ(α+1)	24ηoμG2(α+1)
S+1	1	S+1
16μ∆φ(α+1)	24η0μG2(α+1)
S+1	1	(S+1)α-
α≥1
0<α<1
In order to have E[kVφγ (Xτ)k2] ≤ 2, we can set S = O(1/2). The total number of iterations is
SS
XTs ≤ X12γs ≤ 6γS(S+1) = O(1/4)
s=1	s=1
□
D Proof of Theorem 4
We need the following lemma for the convergence bound of stochastic momentum methods for a
strongly convex problem, whose proof is postponed to Section F.
Lemma 3. For Algorithm 4, assume f (x) = φ(x) + 2γ ∣∣x - x0k2 is a λ-strongly convex function,
gt = g(xt；ξ) + 1 (Xt - xo) where g(x; ξ) ∈ ∂fφ(xt,) such that E[∣g(x;ξ)∣2] ≤ G2, and n ≤
(1 一 β)γ2λ∕(8ρβ + 4), then we have that
E[f(bτ) - f(x*)] ≤
(1 — β)∣xo — x*k2
-2n(T + 1)-
+ e(f (XO) — f (X*))
+ (1 - β)(T + 1)
2nG2(2ρβ + 1)
+	1 - β
4ρβ + 4 η
不 2k 2 kx0 一
(1 - β) γ2
x*k2
(13)
+
where XT = PT=O xt∕(1 + T) and x* ∈ arg minχ∈Rd f (x).
Remark: It is notable that in the above result, we do not use the bounded domain assumption since
We consider Ω = Rd for the unified momentum methods in this subsection. The key to get rid of
bounded domain assumption is by exploring the strong convexity of f (x) = φ(x) + 2γ∣∣x 一 xo∣∣2.
Proof. of Theorem 4 According to the definition of zs in (8) and Lemma 3, We have that
Es φ(Xs) + 2γ IlXs - xs-1k2
≤ f (7 ) + β(fs(xs-i)- fs(zs)) , (1 - β)kxs-i - zsk2 , 2nsG2(2ρβ + 1) +ɪ	2
一fs(s)+	(1 - β)(Ts + 1)	+	2ns (Ts + 1)	+	1 - β	+24YksT	sk
X---------------------------------{-----------------------------------}
Es
≤ φ(xs-1) + Es,
Where the last inequality uses the value ofηs = (1 - β)γ∕(96s(ρβ + 1)) ≤ (1 - β)γ∕(96(ρβ + 1)),
which also satisfies the condition in Lemma 3 by noting that λ = γ-1 一 μ = 1∕(2γ). Similar to the
proof of Theorem 1, We have
(1 — a s)..	.. n Lr ,/	∖	,/ ∖ι α— - γ μ c
---∣xs-ι — zsk2 ≤Es[φ(xs-ι) 一 φ(xs)] + -s----------EsEs
2γ	(I- γμ)
(14)
18
Published as a conference paper at ICLR 2019
Plugging the expression of Es and rearranging above inequality, we have that
(1 -αs)γ-
Y2(α-1 - μγ)(1 - β)	α-1 - γμ γ
—
(I- μγ)ns(Ts +I)	(I- γμ) 24
l∣vφγ (χs-ι)k2
ʌ,
≤2Es [∆s] +
2(α-1 - μγ) ∣^β(fs(Xs-I)- fs(zs)) + 2nsGG2(2ρβ + 1)^
(I - μY)
(1 -β)(Ts+1)
1-β
The definition of fs gives that
fs(Xs-i) - fs(zs) = Φ(Xs-i) - Φ(zs) - 2γ∣∣zs - Xs-ik2
On the other hand, the μ-weakly convexity of φ gives that
φ(zs) ≥ Φ(Xs-i) + hg(Xs-i),zs - Xs-ii - μ∣∣zs - Xs-ik2,
where g(Xs-1) ∈ ∂Fφ(Xs-1). Combing these two inequalities we have that
fs(Xs-i) - fs(zs) ≤hg(Xs-i),Xs-i - zsi - μ∣∣zs - Xs-ik2
≤ G2+μ-μ ∣zs - Xs-ι∣2=G2
2μ	2	2μ
where the second inequality follows from Jensen's inequality for ∣∣ ∙ ∣∣ and Young's inequality. Com-
bining above inequalities and multiPlying both side by ws, we have that
ws (1 - αs)Y -
γ2(O-I - μγ)(1 - β)	α-1 - γμ γ
—
(I- μY)ns(Ts +I) (I- γμ) 24
l∣vφγ (Xs-ι)k2
≤2wsEs∆s] + 2wf
βG2
1 - β)(Ts + 1)
2nsG2(2ρβ +1)-
+	1-β — _
(15)
By setting αs = 1/2, ns(Ts + 1) ≥ 24(1 - β)Y, we have that
w4γkVφγ(Xs-I)k2 ≤ 2wsEs[∆s] +
WsnseG2	12wsnsG2(2ρβ + 1)
4(1 - β)2 +
1-β
Summing over s
S+1
1, . . . , S + 1 and rearranging, we have
wslVφγ(Xs-1)l2 = E
s=1
8
—Ws∆s +
Y
wsnsG2(β + 48(2ρβ + 1)(1 -β))
γ(1 - β)2
Following similar analysis as in the proof of Theorem 2, we can finish the proof.
E Proof of Theorem 3
Proof. APPlyingLemma 2 with Ts ≥ Ms max{G+max2kgl:Ts,ik ,cPd=I ∣∣gsz,i∣∣} Ms >
the fact that φ(Xs-ι) ≥ φ(zs) + 2γ ∣∣Xs-ι - zsk2 in Sth stage, We have that
0, and
Es φ(Xs) + -- IlXs - Xs-1 k2
2Ys
≤ fs(zs)+Mn kXs-1- zsk2+M⅛
X----------------------}
{z
Es
≤ φ(xs) + Es
According to (14), we have that
yEsikxs-1- zsk2] ≤φ(XsT)- φ(χs)+
(αs-1 - 1)
≤φ(xs-1) - φ(xs) +
2Y
α-1 - γμ
lXs - zs l2 + Es
(1 - γμ) V Msns
kxs-1- zsk2 + M
□
c
19
Published as a conference paper at ICLR 2019
Rearranging above inequality then multiplying both side by ws, we have that
2γ2c(α-1 - μ))
(I- μ)Msns J
l∣vφγ (Xs-I)k2
≤2wsEs [∆s] +
2wsns(α-1 - μγ)
cMs(l 一 μγ)
By using Msηs ≥ 24γc and summing over s = 1, . . . , S + 1, we have that
S+1	S+1
X wskVφγ(Xs-1)k2 ≤ E X
s=1
s=1
8ws∆s
γ
By the definition of T in the theorem, taking expectation of ∣∣Vφγ(XT)k2 w.r.t. T ∈ {0,...,S} We
have that
8 S+1
E[∣Vφγ (xτ )k2] =E[γ∙E
s=1
ws∆s -
PS+1 Wi J
2 S+1
+2X
CccY ⅛
sα-1
≤ 8δφ (α + 1) +	n0(α + 1)
-Y(S + 1)	c2γ2(S +1)αI(α<1),
where I(α < 1) is 1 if α < 1 and 0 otherwise.
□
F Proof of Lemma 3
Proof. Following the analysis in Yang et al. (2016), we directly have the following inequality,
E[∣∣xk+1 + pk + 1 一 x*∣∣2]=
=E[∣Xk + Pk — x*k2] - -2n-zE[(xk 一 x*)>∂f(xk)] 一TIe∖2E[(xk 一 Xk-ι)>∂f(xQ]
1 一 β	(1 一 β)2
一 Tfρ¾E[g>-ι∂f(Xk)]+ ( 1\)2E[∣gkk2]
(1 一 β)	1 一 β
We also note that
f(xk) — f(x*) ≤ (xk — x*)>∂f(xk) — 2∣∣Xk — x*k2
f(xk) — f(xk-ι) ≤ (Xk — Xk-1 )>∂f(xk) — 2∣∣Xk — Xk-1∣2
-E[g>-ι∂f(Xk)] ≤ E[kgkTk2 + kdf (Xk)k2] ≤ J.∣xk-ι — x0∣2 +— xo∣2 + 2G2
2	γ2	γ2
Ek [lgk 112] ≤ F ∣∣xk — x0∣∣2 + 2G2
γ2
where the first two inequalities are due to the strong convexity of f (∙) and the last three inequalities
are due to the boundness assumption. Thus
E[∣∣xk+1 + pk + 1 一 x∣∣2] ≤ E[∣∣xk + pk 一 x∣∣2] - 1 — BE[(f (xk) 一 f (X))]
一 门 “Br、2 E[(f (Xk ) 一 f (Xk-1))] + ( 1 T o ) (2ρβ + 1)4G2
(1 — β)	1 — β
一 1~⅞ kxk 一 x*k2 一 (1λ"β )2 lxk 一 XkTk2
1 — β	(1 — β)
2ρβ η2	2	2ρβ + 2 η2	2
十 (1 一 β)2 Y2kxk-1 - xok + (1 一 β)2 Y2kxk - xok
20
Published as a conference paper at ICLR 2019
By summarizing the above inequality over k = 0,...,T, we have
-2ηβE [X(J(Xk)- f (x,))l ≤ E[∣∣xo - x*∣∣2] + --2ηβ--2E[f(xo) - f(x*)]
k k k=0	」	(
+ (-⅞) (2pβ + 1)4G2(T + 1)
ηλ Gll	∣∣2 .	4pβ η GIl	∣∣2 . 4pβ + 4 η GIl	II
-1-J ^kXk - x*k + (1 - β)2 γ2 ^kXkT - x*k + (1 - β)2 Y NkXk - x*k
+ (P)+； Y(T+ι)kxo - x*k2
When η ≤ (1 - β)γ2λ/(8pβ + 4), we have
E (f (XT) - f(x*))
≤ (1-β)kxo-x*k2 +
≤	2η(T + 1)	+
β	f(xo) - f(x*) +
(1 - β) —t+1 — +
ɪ (2pβ + 1)2G2
1-β
+
4pβ + 4 η
D 中kxo -
x*∣∣2
□
G Proof of Lemma 2
The proof is almost a duplicate of the proof of Proposition 1 in Chen et al. (2018b). For complete-
ness, we present a proof here.
Proof. Let ψo(x) = 0 and IlXkH = √x>Hx. First, we can see that ψt+ι(x) ≥ ψt(x) for any
t ≥ 0. Define Zt = PT=1 gt and ∆τ = (∂F(Xt) - gt)τ(xt - x). Let 喝 be defined by
Ψt(g) = SUP gτx - 1 ψt(x)
x∈Ω	η
Taking the summation of objective gap in all iterations, we have
TT	TT
£(f(xt)-	f (X))	≤ E	∂f (Xt)T(Xt-X) = E gT(Xt-X)	+ E ∆t
t=1	t=1	t=1	t=1
TT	T
=E g>xt- E g>x - ηψT(X) + ηψT(X) + E Z
1	T	T	(T
≤ ηψT(x) + E g>xt+ E δ + sup j- E g>x
TT
=-Tt (x) + E gJxt + ΨT (-Zt ) + E ∆t
η	t=1	t=1
-1 Ψt (x) ∖
η
Note that
TT
ΨT(-ζτ) = -Eg>xτ +1 - ηTt(XT+1) ≤-∑g>
≤ sup -ZTX - ηψτ-1(x) = TT-1(-Zt)
≤ ψT-1(-Cτ-I) - gTV≠T-1(-Cτ-I) + 2 IlgTllψτ-1
xτ +1 - ηψτ-ι(xτ +1)
21
Published as a conference paper at ICLR 2019
where the last inequality uses the fact that ψt(x) is 1-strongly convex w.r.t ∣∣ ∙ ∣∣ψt = ∣∣ ∙ k% and
consequentially ψJ= (x) is η-smooth wr.t. ∣ ∙ ∣∣ψ* = ∣∣ ∙ ∣∣h-i . Thus, we have
TT
X g> Xt + ψτ (YT) ≤ X g> Xt + ψT-1(YT-I) - g> vψτ-ι(-Cτ-I) + 2 ∣∣gτ ∣∣ψτ-ι
t=1	t=1
T-1
X g> Xt + ψT-1(YT-ι) + 2 IlgT ιιψτ-ι
t=1
By repeating this process, we have
Xgt>Xt +ψT(-ZT) ≤ ψo(-ζo)+2 x 恒”工之 ι = 2 X 恒"品_1
t=1	t=1	t=1
Then
X(f (Xt)- f (Xy) ≤1 ψT(X) + 2 X 叵脸-1
t=1	η	2 t=1
T
+ X ∆t
t=1
(16)
Following the analysis in Duchi et al. (2011), we have
Td
X ∣gt∣Ψl ≤ 2X ∣gnT,i∣2
Thus
X(f(Xt) -f(X)) ≤ GkX-X1k2 + (X-Xl)>diag(ST)(X-X1) + ηX∣ghT,i∣2 + X∆t
t=1	η	η	i=1	t=1
dT
≤ G + maχi∣X -X1∣2 + ηX ∣g.T,i∣2 + X∆t
2η	i=1	t=1
Now bythe value of T ≥ M max{ (G+max2jg1:T，i)k ,c Pd=IkgLT,i∣∣},we have
(G + maxi ∣∣gLT,i∣∣2) ≤ 二
2ηT	一 ηM,
ηPl=I ∣gLT,i∣∣2 ≤ ɪ
T	≤ M
Dividing by T on both sides and setting X = XJ , following the inequality (3) and the convexity of
f(X) we have
c	η 1T
f(X)- f ≤ MkXo - X*k + Mc + T XZ
Let {Ft } be the filtration associated with Algorithm 1 in the paper. Noticing that T is a random
variable with respect to {Ft }, we cannot get rid of the last term directly. Define the Sequence
{Xt }t∈N+ as
1t 1t
Xt = t ∑∆i = t Ehgi- E[gi], Xi- X*i	(17)
i=1	i=1
where E[gi] ∈ ∂f (Xi). Since E[gt+ι - E[gt+ι]] = 0 and Xt+ι = arg min ηX> ɑ PT=1 gτ) +
1 ψt(X), which is measurable with respect to gι,..., gt and xi,..., Xt, it is easy to see {∆t}t∈ N
is a martingale difference sequence with respect to {Ft}, e.g. E[∆t∣Ft-ι] = 0. On the other
hand, since ∣gt ∣2 is upper bounded by G, following the statement of T in the theorem, T ≤ N =
M2 max{ (G+1) , d2G2} < ∞ always holds. Then following Lemma 1 in (Chen et al., 2018b) we
have that E[XT] = 0.
Now taking the expectation we have that
c
Ef (X)-f*]≤ 访 kx0-
X k2 + 受
x*k + M
Then we finish the proof.
□
22
Published as a conference paper at ICLR 2019
H Proof of Inequality (3)
Let us recall the definition of xb = proxγf (x).
ProXYf (x) = argmzin f(z) + 2Y ∣∣z - x∣∣2	(18)
First, note that when f (z) is μ-weakly convex and Y < μ-1, the above problem is strongly convex
and xb = ProXγf (x) is well-defined and unique.
By the optimality condition of b, it is clear that f (X) + 2γ∣∣b 一 x∣2 ≤ f (x), which proves the first
inequality f (b) ≤ f (x). Note that when X = x* ∈ argminz f (z), we can prove that b = X = x*.
This is because that by definition of b, it is unique and satisfies 0 ∈ ∂f (b) + ɪ (b-x). When X = x*,
we have 0 ∈ ∂f (x) and x* clearly satisifes the first-order condition 0 ∈ ∂f (x*) + ɪ(x* 一 x). By
the uniqueness of xb, it follows that xb = x*.
The first-order condition 0 ∈ ∂f (b) + Y(b 一 x) also gives that dist(0, ∂f (b)) ≤ Y∣∣b 一 x∣. Also,
we have NfY (x) = Y (X — x) (Rockafellar, 1970), which implies the second and the third inequality
in (3).
23