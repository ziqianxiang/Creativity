Published as a conference paper at ICLR 2019
Deep Layers as Stochastic Solvers
Adel Bibi *	Bernard Ghanem	Vladlen Koltun	Rene Ranftl
KAUST	KAUST	Intel Labs	Intel Labs
Ab stract
We provide a novel perspective on the forward pass through a block of layers
in a deep network. In particular, we show that a forward pass through a standard
dropout layer followed by a linear layer and a non-linear activation is equivalent to
optimizing a convex objective with a single iteration of a τ -nice Proximal Stochas-
tic Gradient method. We further show that replacing standard Bernoulli dropout
with additive dropout is equivalent to optimizing the same convex objective with
a variance-reduced proximal method. By expressing both fully-connected and
convolutional layers as special cases of a high-order tensor product, we unify the
underlying convex optimization problem in the tensor setting and derive a formula
for the Lipschitz constant L used to determine the optimal step size of the above
proximal methods. We conduct experiments with standard convolutional networks
applied to the CIFAR-10 and CIFAR-100 datasets and show that replacing a block
of layers with multiple iterations of the corresponding solver, with step size set
via L, consistently improves classification accuracy.
1	Introduction
Deep learning has revolutionized computer vision and natural language processing and is increas-
ingly applied throughout science and engineering (LeCun et al., 2015). This has motivated the
mathematical analysis of various aspects of deep networks, such as the capacity and uniqueness of
their representations (Soatto & Chiuso, 2014; Papyan et al., 2018) and their global training conver-
gence properties (Haeffele & Vidal, 2017). However, a complete characterization of deep networks
remains elusive. For example, Bernoulli dropout layers are known to improve generalization (Sri-
vastava et al., 2014), but a thorough theoretical understanding of their behavior remains an open
problem. While basic dropout layers have proven to be effective, there are many other types of
dropout with various desirable properties (Molchanov et al., 2017). This raises many questions. Can
the fundamental block of layers that consists of a dropout layer followed by a linear transformation
and a non-linear activation be further improved for better generalization? Can the choice of dropout
layer be made independently from the linear transformation and non-linear activation? Are there
systematic ways to propose new types of dropout?
We attempt to address some of these questions by establishing a strong connection between the
forward pass through a block of layers in a deep network and the solution of convex optimization
problems of the following form:
1n
minimize F(x) + g(x),	F(x) =f n^Jfi(a>x).	(1)
i
Note that when fi(a>x) = 2(a>X - yi)2 and g(x) = ∣∣x∣∣2, Eq. (1) is standard ridge regression.
When g (x) = kxk1 , Eq. (1) has the form of LASSO regression.
We show that a block of layers that consists of dropout followed by a linear transformation (fully-
connected or convolutional) and a non-linear activation has close connections to applying stochastic
solvers to (1). Interestingly, the choice of the stochastic optimization algorithm gives rise to com-
monly used dropout layers, such as Bernoulli and additive dropout, and to a family of other types
of dropout layers that have not been explored before. As a special case, when the block in question
does not include dropout, the stochastic algorithm reduces to a deterministic one.
Our contributions can be summarized as follows. (i) We show that a forward pass through a block
that consists of Bernoulli dropout followed by a linear transformation and a non-linear activation
*The work was done during an internship at Intel Labs.
1
Published as a conference paper at ICLR 2019
is equivalent to a single iteration of τ -nice Proximal Stochastic Gradient, Prox-SG (Xiao & Zhang,
2014) when itis applied to an instance of (1). We provide various conditions on g that recover (either
exactly or approximately) common non-linearities used in practice. (ii) We show that the same block
with an additive dropout instead of Bernoulli dropout is equivalent to a single iteration of mS2GD
(Konecny et al., 20l6) - a mini-batching form of variance-reduced SGD (Johnson & Zhang, 2013)-
applied to an instance of (1). (iii) By expressing both fully-connected and convolutional layers
(referred to as linear throughout) as special cases of a high-order tensor product (Bibi & Ghanem,
2017), We derive a formula for the LiPsChitz constant L of VF(x). As a consequence, We can
compute the optimal step size for the stochastic solvers that correspond to blocks of layers. We note
that concurrent Work (Sedghi et al., 2019) used a different analysis strategy to derive an equivalent
result for comPuting the singular values of convolutional layers. (iv) We validate our theoretical
analysis exPerimentally by rePlacing blocks of layers in standard image classification netWorks With
corresPonding solvers and shoW that this imProves the accuracy of the models.
2	Related Work
OPtimization algorithms can Provide insight and guidance in the design of deeP netWork archi-
tectures (Vogel & Pock, 2017; Kobler et al., 2017; Yang et al., 2016; Zhang & Ghanem, 2018).
For examPle, Yang et al. (2016) have ProPosed a deeP netWork architecture for comPressed sens-
ing. Their netWork, dubbed ADMM-Net, is insPired by ADMM uPdates (Boyd et al., 2011) on
the comPressed sensing objective. Similarly, Zhang & Ghanem (2018) demonstrated that unrolling
a Proximal gradient descent solver (Beck & Teboulle, 2009) on the same Problem can further im-
Prove Performance. The Work of Kobler et al. (2017) demonstrated a relation betWeen incremental
Proximal methods and ResNet blocks; based on this observation, they ProPosed a neW architecture
(variational netWorks) for the task of image reconstruction. Amos & Kolter (2017) ProPosed to
embed oPtimization Problems, in Particular linearly-constrained quadratic Programs, as structured
layers in deeP netWorks. Meinhardt et al. (2017) rePlaced Proximal oPerators in oPtimization algo-
rithms by neural netWorks. Huang & Van Gool (2017) ProPosed aneW matrix layer, dubbed ReEig,
that aPPlies a thresholding oPeration to the eigenvalues of intermediate feature rePresentations that
are stacked in matrix form. ReEig can be tightly connected to a Proximal oPerator of the set of
Positive semi-definite matrices. Sulam et al. (2018) ProPosed a neW architecture based on a sParse
rePresentation construct, Multi-Layer Convolutional SParse Coding (ML-CSC), initially introduced
by PaPyan et al. (2017). SParsity on the intermediate rePresentations Was enforced by a multi-layer
form of basis Pursuit.
This body of Work has demonstrated the merits of connecting the design of deeP netWorks With
oPtimization algorithms in the form of structured layers. Yet, With feW excePtions (Amos & Kolter,
2017; Sulam et al., 2018), Previous Works ProPose sPecialized architectures for sPecific tasks. Our
Work aims to contribute to a unified frameWork that relates oPtimization algorithms to deeP layers.
A line of Work aims to Provide rigorous interPretation for droPout layers. For examPle, Wager
et al. (2013) showed that dropout is linked to an adaptively balanced '2-regularized loss. Wang
& Manning (2013) shoWed that aPProximating the loss With a normal distribution leads to a faster
form of dropout. Gal & Ghahramani (2016a;b) developed a frameWork that connects dropout With
approximate variational inference in Bayesian models. We provide a complementary perspective, in
Which dropout layers arise naturally in an optimization-driven frameWork for netWork design.
3	Unified Framework
This section is organized as folloWs. We introduce our notation and preliminaries in Section 3.1.
In Section 3.2, We present a motivational example relating a single iteration of proximal gradient
descent (Prox-GD) on (1) to the forWard pass through a fully-connected layer folloWed by a non-
linear activation. We Will shoW that several commonly used non-linear activations can be exactly or
approximately represented as proximal operators of g(x). In Section 3.3, We unify fully-connected
and convolutional layers as special cases of a high-order tensor product. We propose a generic
instance of (1) in a tensor setting, Where We provide a formula for the Lipschitz constant L of the
finite sum structure of (1). In Section 3.4, We derive an intimate relation betWeen stochastic solvers,
namely τ -nice Prox-SG and mS2GD, and tWo types of dropout layers. Figure 1 shoWs an overvieW
of the connections that Will be developed.
2
Published as a conference paper at ICLR 2019
ChoiCe Of StoChaStiC SoIVer
τ-nice Prox-SG
T = (I- p)nɪ
r⅛
m2SGD
AddDropout
Figure 1: An overview of the tight relation between a single iteration of a stochastic solver and
the forward pass through the lth layer in a network that consists of dropout followed by a linear
transformation and a non-linear activation. We study an instance of problem (1) with quadratic
F (x), where xl-1 are the input activations and xl, the variables being optimized, correspond to the
output activations. Varying the type of stochastic solver changes the nature of the dropout layer,
while the prior g(x) on the output activations determines the non-linearity ProX 1 g(.).
3.1	Notation and Preliminaries
As we will be working with tensors, we will follow the tensor notation of Kolda & Bader (2009).
The order of a tensor is the number of its dimensions. In particular, scalars are tensors of order
zero, vectors are tensors of order one, and matrices are tensors of order two. We denote scalars by
lowercase letters a, vectors by bold lowercase letters a, and matrices by bold capital letters A. We
use subscripts ai to refer to individual elements in a vector. Tensors of order three or more will be
denoted by cursive capital letters A ∈ RJ1 × J2 × …XJn. Throughout the paper, We will handle tensors
that are of at most order four. High-order tensors with a second dimension of size equal to one are
traditionally called vector tensors and denoted A~ ∈ RJ1 X1XJ3 XJ4. We use A(i, j, k, z) to refer to
an element in a tensor and A(i, j, k, :) to refer to a slice of a tensor. The inner product between
tensors of the same size is denoted hA, Bi = Pi ,...,i A (i1, . . . , iN) B (i1, . . . , iN). The squared
Frobenius norm of a tensor A is defined as kAk2F = hA, Ai. Lastly, the superscripts > and H are
used to denote the transpose and the Hermitian transpose, respectively.
3.2	M otivational Insight: Non-Linear Activations as Proximal Operators
As a motivating eXample, we consider the lth linear layer in a deep network that is followed by a
non-linear activation ρ, i.e. xl = ρ(Axl-1 + b), where A ∈ Rn2Xn1 and b ∈ Rn2 are the weights
and biases of the layer and xl-1 and xl are the input and output activations, respectively. Now
consider an instance of (1) with a conveX function g(x) and
1	1 n1
F(xl) = _||A>xl - XlT∣∣2 - b>X = - X(A>(i, :)xl - x；-1)2 - b>xl,	(2)
2	2i
where A> (i, :) is the ith row of A>. Such an objective can be optimized iteratively in xl using
ProX-GD with the following update equation:
Xl J Proxιg ((I — 1 AA>) Xl + 1 (AxlT + b)) ,	(3)
where the Lipschitz constant L = λmaX AA> and λmaX(.) denotes the maXimum eigenvalue.
By initializing the iterative optimization at Xl = 0, it becomes clear that a single iteration of (3) is
equivalent to a fully-connected layer followed by a non-linearity that is implemented by the proximal
operator (Fawzi et al., 2015). The choice of g(X) determines the specific form of the non-linearity
ρ. Several popular activation functions can be traced back to their corresponding g(X). The ReLU,
which enforces non-negative output activations, corresponds to the indicator function g(x) = lχ≥o;
the corresponding instance of problem (1) is a non-negative quadratic program. Similar observations
for the ReLU have been made in other contexts (Amos & Kolter, 2017; Papyan et al., 2017). We
3
Published as a conference paper at ICLR 2019
				
g(X)	lχ≥o	2 Pi max2 (-Xi - λ, 0)	-γ Pi log(Xi)	-γ Pi log(1 - Xi2)
Proxg (η)	max(0, η)	(η+λ if η ≤-λ [η	if η ≥ -λ	1 η + 4 4 η2 + Y	Root of cubic polynomial
Shape		J		ʃ
Activation	= ReLU(η)	= LeakyReLU(η)	≈ Softplus(η)	≈ Tanh(η)
Table 1: Different choices of g(x), their corresponding proximal operators, and their relation to
common activation functions. Squared hinge loss regularization of the activations yields a gener-
alized Leaky ReLU. Log-barriers recover smooth activations, such as SoftPlus, Tanh, or Sigmoid.
Derivations can be found in supplementary material.
observe that many other activation functions fit this framework. For example, when g(x) is a squared
hinge loss, i.e. 2 Pi max2 (-Xi - λ, 0), a single update of (3) is equivalent to a linear layer followed
by a Leaky ReLU. Table 1 lists some other choices of g(x) and their induced activations.
Note that g(X) is not required to exhibit a simple, coordinate-wise separable structure. More com-
plex functions can be used, as long as the proximal operator is easy to evaluate. Interesting examples
arise when the output activations have matrix structure. For instance, one can impose nuclear norm
regularization g(X) = k X∣∣ * to encourage X to be low rank. Alternatively, one can enforce positive
semi-definite structure on the matrix X by defining g(X) = IX占0. A similar activation has been
used for higher-order pooling (Huang & Van Gool, 2017).
In what follows, we will show that this connection can be further extended to explain dropout layers.
Interestingly, specific forms of dropout do not arise from particular forms of objective (1), but from
different stochastic optimization algorithms that are applied to it.
3.3 Unifying Fully-Connected and Convolutional Layers
Before presenting our main results on the equivalence between a forward pass through a block of
layers and solving (1) with stochastic algorithms, we provide some key lemmas. These lemmas will
be necessary for a unified treatment of fully-connected and convolutional layers as generic linear
layers. This generic treatment will enable efficient computation of the Lipschitz constant for both
fully-connected and convolutional layers.
Lemma 1. Consider the lth convolutional layer in a deep network with some non-linear activation,
e.g. Proxg(.), where the weights A ∈ Rn2×n1×W×H, biases B~ ∈ Rn2 ×1×W ×H, and input activa-
tions X~ l-1 ∈ Rn1 ×1×W ×H are stacked into 4th-order tensors. We can describe the layer as
X~l = Proxg A ~HO X~ l-1 +B~ ,	(4)
where ~HO is the high-order tensor product. Here n1 is the number of input features, n2 is the num-
ber of output features (number of filters), and W and H are the spatial dimensions of the features.
As a special case, a fully-connected layer follows naturally, since ~HO reduces to a matrix-vector
multiplication when W = H = 1.
The proof can be found in supplementary material. Note that the order of the dimensions is essential
in this notation, as the first dimension in A corresponds to the number of independent filters while
the second corresponds to the input features that will be aggregated after the 2D convolutions. Also
note that according to the definition of ~HO in (Bibi & Ghanem, 2017), the spatial size of the filters
in A, namely W and H, has to match the spatial dimensions of the input activations X~l-1, since
the operator ~HO performs 2D circular convolutions while convolutions in deep networks are 2D
linear convolutions. This is not a restriction, since one can perform linear convolution through a
zero-padded circular convolution. Lastly, we assume that the values in B~ are replicated along the
spatial dimensions W and H in order to recover the behaviour of biases in deep networks.
Given this notation, we will refer to either a fully-connected or a convolutional layer as a linear
layer throughout the rest of the paper. Since we are interested in a generic linear layer followed by
4
Published as a conference paper at ICLR 2019
a non-linearity, we will consider the tensor quadratic version of F (x), denoted F(X~):
1 n1 n
arg min 一 £ 3∣AH(i, ：, ：, ：) ~ho X -Xl 1kF
n1 V I2--------------------{z----------
fi(X~)
- hB~,X~i+g(X~).
(5)
Note that if A ∈ Rn2×n1 ×W×H, then AH ∈ Rn1 ×n2×W×H, where each of the frontal slices of
A(:, :,i,j) is transposed and each filter, A(i,j,:, :),is rotated by 180°. This means that AH ~ho X~
aggregates the n2 filters after performing 2D correlations. This is performed n1 times indepen-
dently. This operation is commonly referred to as a transposed convolution. Details can be found in
supplementary material.
Next, the following lemma provides a practical formula for the computation of the Lipschitz constant
L of the finite sum part of (5):
Lemma 2. The LiPSchitz constant L of VF(X) as defined in (5) is given by
L =	「 maxr η {λmax (A(:, :,i,j) vAH (:, :,i,j)) },	(6)
i∈{1,2,...,W},j∈{1,2,...,H}
where A is the 2D discrete Fourier transform along the spatial dimensions W and H.
The proof can be found in supplementary material. Lemma 2 states that the Lipschitz con-
stant L is the maximum among the set of maximum eigenvalues of all the possible W × H
combinations of the outer product of frontal slices A(:,:, i,j) AH (:, :,i,j). Note that if
W = H = 1, then A = A ∈ Rn2 ×n1 since the 2D discrete Fourier transform of scalars (i.e.
matrices of size 1 × 1) is an identity mapping. As a consequence, we can simplify (6) to
L = maxi=j=1{λmax A(:, :, i, j)AH(:, :, i,j) } = λmax AA> , which recovers the Lipschitz con-
stant for fully-connected layers.
3.4 Dropout Layers as Variants of Stochastic S olvers
In this subsection, we present two propositions. The first shows the relation between standard
Bernoulli dropout (p is the dropout rate), BerDropoutp (Srivastava et al., 2014), and τ -nice Prox-SG.
The second proposition relates additive dropout, AddDropout, to mS2GD (Konecny et al., 2016).
We will first introduce a generic notion of sampling from a set. This is essential as the stochastic
algorithms sample unbiased function estimates from the set of n1 functions in (5).
Definition 3.1. (Gower et al., 2018). A sampling is a random set-valued mapping with val-
ues being the subsets of [n1] = {1, . . . , n1}. A sampling S is τ -nice if it is uniform, i.e.
Prob (i ∈ S) = Prob (j ∈ S) ∀ i,j, and assigns equal probabilities to all subsets of [n1] of cardi-
nality τ and zero probability to all others.
Various other types of sampling can be found in (Gower et al., 2018). We are now ready to present
our first proposition.
Proposition 1. A single iteration of Prox-SG with τ -nice sampling S on (5) with τ = (1 - p)n1,
zero initialization, and unit step size can be shown to exhibit the update
ProxLg	X A(:,i,:,:) ~ηo 炉-1(i,:,:,:)+ Bɔ ,	⑺
τ i∈S
which is equivalent to a forward pass through a BerDropoutp layer that drops exactly n1p input
activations followed by a linear layer and a non-linear activation.
We provide a simplified sketch for fully-connected layers here. The detailed proof is in the supple-
ment. To see how (7) reduces to the functional form of BerDropoutp followed by a fully-connected
layer and a non-linear activation, consider W = H = 1. The argument of Prox * g in (7) (without
the bias term) reduces to
n1 X A(:, i,:,:) ~HO X~l-1(i,:,:,:) = n1 X A(:, i)XJ1(i) = n1 A BerDropoutp (炉-1).
τ i∈S	τ i∈S	τ
(8)
5
Published as a conference paper at ICLR 2019
The first equality follows from the definition of ~HO, while the second equality follows from triv-
ially reparameterizing the sum, with BerDropoutp(.) being equivalent to a mask that zeroes out
exactly pn1 input activations. Note that if τ -nice Prox-SG was replaced with Prox-GD, i.e. τ = n1,
then this corresponds to having a BerDropoutp layer with dropout rate p = 0; thus, (8) reduces to
A BerDropoutp(X~l-1) = AX~ l-1, which recovers our motivating example (3) that relates Prox-
GD with the forward pass through a fully-connected layer followed by a non-linearity. Note that
Proposition 1 directly suggests how to apply dropout to convolutional layers. Specifically, complete
input features from n1 should be dropped and the 2D convolutions should be performed only on the
τ -sampled subset, where τ = (1 - p)n1.
Similarly, the following proposition shows that a form of additive dropout, AddDropout, can be
recovered from a different choice of stochastic solver.
Proposition 2. A single outer-loop iteration of mS2GD (Konecny et al., 2016) with unit step size
and zero initialization is equivalent to a forward pass through an AddDropout layer followed by a
linear layer and a non-linear activation.
The proof is given in the supplement. It is similar to Proposition 1, with mS2GD replacing τ -nice
Prox-SG. Note that any variance-reduced algorithm where one full gradient is computed at least once
can be used here as a replacement for mS2GD. For instance, one can show that the serial sampling
version of mS2GD, S2Gd (Konecny et al., 2016), and SVRG (Johnson & Zhang, 2013) can also
be used. Other algorithms such as Stochastic Coordinate Descent (Richtarik & Takac, 2016) with
arbitrary sampling are discussed in the supplement.
4	Experiments
A natural question arises as a consequence of our framework: If common layers in deep networks
can be understood as a single iteration of an optimization algorithm, what happens if the algorithm
is applied for multiple iterations? We empirically answer this question in our experiments. In
particular, we embed solvers as a replacement to their corresponding blocks of layers and show that
this improves the accuracy of the models without an increase in the number of network parameters.
Experimental setup. We perform experiments on CIFAR-10 and CIFAR-100 (Krizhevsky & Hin-
ton, 2009). In all experiments, training was conducted on 90% of the training set while 10% was
left for validation. The networks used in the experiments are variants of LeNet (LeCun et al., 1999),
AlexNet (Krizhevsky et al., 2012), and VGG16 (Simonyan & Zisserman, 2014). We used stochastic
gradient descent with a momentum of 0.9 and a weight decay of 5 × 10-4. The learning rate was set
to (10-2, 10-3, 10-4) for the first, second, and third 100 epochs, respectively. For finetuning, the
learning rate was initially set to 10-3 and reduced to 10-4 after 100 epochs. Moreover, when a block
of layers is replaced with a deterministic solver, i.e. Prox-GD, the step size is set to the optimal con-
stant 1/L, where L is computed according to Lemma 2 and updated every epoch without any zero
padding as a circular convolution operator approximates a linear convolution in large dimensions
(Zhu & Wakin, 2017). In Prox-SG, a decaying step size is necessary for convergence; therefore, the
step size is exponentially decayed as suggested by Bottou (2012), where the initial step size is again
set according to Lemma 2. Finally, to guarantee convergence of the stochastic solvers, we add the
strongly convex function 21∣∕~∣∣F to the finite sum in (5), where We set λ = 10-3 in all experiments.
Note that for networks that include a stochastic solver, the network will be stochastic at test time.
We thus report the average accuracy and standard deviation over 20 trials.
Replacing fully-connected layers with solvers. In this experiment, we demonstrate that (i) training
networks with solvers replacing one or more blocks of layers can improve accuracy when trained
from scratch, and (ii) the improvement is consistently present when one or more blocks are replaced
with solvers at different layers in the network. To do so, we train a variant of LeNet on the CIFAR-10
dataset with two BerDropoutp layers. The last two layers are fully-connected layers with ReLU
activation. We consider three variants of this network: Both fully-connected layers are augmented
with BerDropoutp (LeNet-D-D), only the last layer is augmented with BerDropoutp (LeNet-ND-D),
and finally only the penultimate layer is augmented with BerDropoutp (LeNet-D-ND). In all cases,
we set the dropout rate to p = 0.5. We replace the BerDropoutp layers with their corresponding
stochastic solvers and run them for 10 iterations with τ = n1/2 (the setting corresponding to a
dropout rate of p = 0.5). We train these networks from scratch using the same procedure as the
baseline networks.
6
Published as a conference paper at ICLR 2019
The results are summarized in Table 2. It can be seen that replacing BerDropoutp with the corre-
sponding stochastic solver (τ -nice Prox-SG) improves performance significantly, for any choice of
layer. The results indicate that networks that incorporate stochastic solvers can be trained stably and
achieve desirable generalization performance.
	LeNet-D-D	LeNet-D-ND	LeNet-ND-D
Baseline	64.39%	71.72%	68.54%
Prox-SG	72.86% ± 0.177	75.20% ± 0.205	76.23% ± 0.206
Table 2: Comparison in accuracy between variants of the LeNet architecture on the CIFAR-10
dataset. The variants differ in the location (D or ND) and number of BerDropoutp layers for both the
baseline networks and their stochastic solver counterpart Prox-SG. Accuracy consistently improves
when Prox-SG is used. Accuracy is reported on the test set.
Convolutional layers and larger networks. We now demonstrate that solvers can be used to im-
prove larger networks. We conduct experiments with variants of AlexNet1 and VGG16 on both
CIFAR-10 and CIFAR-100. We start by training strong baselines for both AlexNet and VGG16,
achieving 77.3% and 92.56% test accuracy on CIFAR-10, respectively. Note that performance on
this dataset is nearly saturated. We then replace the first convolutional layer in AlexNet with the
deterministic Prox-GD solver, since this layer is not preceded by a dropout layer. The results are
summarized in Table 3. We observe that finetuning the baseline network with the solver leads to an
improvement of ≈ 1.2%, without any change in the network’s capacity. A similar improvement is
observed on the harder CIFAR-100 dataset.
	AlexNet	AlexNet-Prox-GD
CIFAR-10	77.30%	78.51%
CIFAR-100	44.20%	45.53%
Table 3: Replacing the first convolutional layer of AlexNet by the deterministic Prox-GD solver
yields consistent improvement in test accuracy on CIFAR-10 and CIFAR-100.
Results on VGG16 are summarized in Table 4. Note that VGG16 has two fully-connected layers,
which are preceded by a BerDropoutp layer with dropout rate p = 0.5. We start by replacing only
the last layer with Prox-SG with 30 iterations and τ = n1/2 (VGG16-Prox-SG-ND-D). We further
replace both fully-connected layers that include BerDropoutp with solvers (VGG16-Prox-SG-D-D).
We observe comparable performance for both settings on CIFAR-10. We conjecture that this might
be due to the dataset being close to saturation. On CIFAR-100, a more pronounced increase in
accuracy is observed, where VGG-16-Prox-SG-ND-D outperforms the baseline by about 0.7%.
We further replace the stochastic solver with a deterministic solver and leave the dropout layers un-
changed. We denote this setting as VGG16-Prox-GD in Table 4. Interestingly, this setting performs
the best on CIFAR-10 and comparably to VGG16-Prox-SG-ND-D on CIFAR-100.
	VGG16	VGG16-Prox-SG-ND-D	VGG16-Prox-SG-D-D	VGG16-Prox-GD
CIFAR-10	92.56%	92.44% ± 0.028	92.57% ± 0.029	92.80%
CIFAR-100	70.27%	70.95% ± 0.042	70.44% ± 0.077	71.10%
Table 4: Experiments with the VGG16 architecture on CIFAR-10 and CIFAR-100. Accuracy is
reported on the test set.
Dropout rate vs. τ -nice sampling. In this experiment, we demonstrate that the improvement in
performance is still consistently present across varying dropout rates. Since Proposition 1 has estab-
lished a tight connection between the dropout rate p and the sampling rate τ in (5), we observe that
for different choices of dropout rate the baseline performance improves upon replacing a block of
layers with a stochastic solver with the corresponding sampling rate τ. We conduct experiments with
1AlexNet (Krizhevsky et al., 2012) was adapted to account for the difference in spatial size of the images
in CIFAR-10 and ImageNet (Deng et al., 2009). The first convolutional layer has a padding of 5, and all
max-pooling layers have a kernel size of 2. A single fully-connected layer follows at the end.
7
Published as a conference paper at ICLR 2019
VGG16 on CIFAR-100. We train four different baseline models with varying choices of dropout rate
p ∈ {0, 0.1, 0.9.0.95} for the last layer. We then replace this block with a stochastic solver with a
sampling rate τ and finetune the network.
Table 5 reports the accuracy of the baselines for varying dropout rates p and compares to the accuracy
of the stochastic solver with corresponding τ (Prox-SG). With a high dropout rate, the performance
of the baseline network drops drastically. When using the stochastic solver, we observe a much more
graceful degradation. For example, with a sampling rate τ that corresponds to an extreme dropout
rate of p = 0.95 (i.e. 95% of all input activations are masked out), the baseline network with
BerDropoutp suffers a 56% reduction in accuracy while the stochastic solver declines by only 5%.
Baseline		Prox-SG	
Dropout rate p	Accuracy	Sampling rate τ	Accuracy
0	70.57%	512	70.87
0.10	70.56%	461	70.51% ± 0.0198
0.50	70.27%	256	70.95% ± 0.0419
0.90	68.34%	51	69.19% ± 0.0589
0.95	30.61%	26	67.42% ± 0.0774
Table 5: Comparison of the VGG16 architecture trained on CIFAR-100 with varying dropout rates
p in the last BerDropoutp layer. We compare the baseline to its stochastic solver counterpart with
corresponding sampling rate τ = (1 - p)n1. Accuracy is reported on the test set.
In summary, our experiments show that replacing common layers in deep networks with stochastic
solvers can lead to better performance without increasing the number of parameters in the network.
The resulting networks are stable to train and exhibit high accuracy in cases where standard dropout
is problematic, such as high dropout rates.
5	Discussion
We have presented equivalences between layers in deep networks and stochastic solvers, and have
shown that this can be leveraged to improve accuracy. The presented relationships open many
doors for future work. For instance, our framework shows an intimate relation between a dropout
layer and the sampling S from the set [n1] in a stochastic algorithm. As a consequence, one
can borrow theory from the stochastic optimization literature to propose new types of dropout
layers. For example, consider a serial importance sampling strategy with Prox-SG to solve (5)
(Zhao & Zhang, 2015; Xiao & Zhang, 2014), where serial sampling is the sampling that satisfies
Prob (i ∈ S, j ∈ S) = 0. A serial importance sampling S from the set of functions fi (X ) is the
sampling such that Prob (i ∈ S) α ∣∣Vfi(X )k α Li, where Liis the Lipschitz constant of Vfi(X),
i.e. each function from the set [n1] is sampled with a probability proportional to the norm of the
gradient of the function. This sampling strategy is the optimal serial sampling S that maximizes the
rate of convergence solving (5) (Zhao & Zhang, 2015). From a deep layer perspective, performing
Prox-SG with importance sampling for a single iteration is equivalent to a forward pass through the
same block of layers with a new dropout layer. Such a dropout layer will keep each input activa-
tion with a non-uniform probability proportional to the norm of the gradient. This is in contrast to
BerDropoutp where all input activations are kept with an equal probability 1 - p. Other types of
dropout arise when considering non-serial importance sampling where |S| = τ > 1.
In summary, we have presented equivalences between stochastic solvers on a particular class of con-
vex optimization problems and a forward pass through a dropout layer followed by a linear layer and
a non-linear activation. Inspired by these equivalences, we have demonstrated empirically on multi-
ple datasets and network architectures that replacing such network blocks with their corresponding
stochastic solvers improves the accuracy of the model. We hope that the presented framework will
contribute to a principled understanding of the theory and practice of deep network architectures.
Acknowledgments. This work was partially supported by the King Abdullah University of Science
and Technology (KAUST) Office of Sponsored Research.
8
Published as a conference paper at ICLR 2019
References
Brandon Amos and J. Zico Kolter. OptNet: Differentiable optimization as a layer in neural networks.
In International Conference on Machine Learning (ICML), 2017.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2009.
Adel Bibi and Bernard Ghanem. High order tensor formulation for convolutional sparse coding. In
International Conference on Computer Vision (ICCV), 2017.
Leon Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks ofthe Trade. Springer,
2012.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers. Foundations and
Trends in Machine Learning, 2011.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), 2009.
Alhussein Fawzi, Mike Davies, and Pascal Frossard. Dictionary learning for fast classification based
on soft-thresholding. International Journal of Computer Vision, 2015.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning (ICML), 2016a.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information Processing Systems (NIPS), 2016b.
Robert M Gower, Peter Richtarik, and Francis Bach. Stochastic quasi-gradient methods: Variance
reduction via Jacobian sketching. arXiv preprint arXiv:1805.02632, 2018.
Benjamin D Haeffele and Rene Vidal. Global optimality in neural network training. In Computer
Vision and Pattern Recognition (CVPR), 2017.
Zhiwu Huang and Luc J Van Gool. A Riemannian network for SPD matrix learning. In Association
for the Advancement of Artificial Intelligence (AAAI), 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems (NIPS), 2013.
Misha E Kilmer and Carla D Martin. Factorization strategies for third-order tensors. Linear Algebra
and its Applications, 2011.
Erich Kobler, Teresa Klatzer, Kerstin Hammernik, and Thomas Pock. Variational networks: Con-
necting variational methods and deep learning. In German Conference on Pattern Recognition,
2017.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM Review, 2009.
Jakub Konecny, Jie Liu, Peter Richtarik, and Martin Takac. Mini-batch semi-stochastic gradient
descent in the proximal setting. Journal of Selected Topics in Signal Processing, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.
Yann LeCun, Patrick Haffner, Leon Bottou, and Yoshua Bengio. Object recognition with gradient-
based learning. In Shape, Contour and Grouping in Computer Vision. Springer, 1999.
Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nature, 2015.
9
Published as a conference paper at ICLR 2019
Tim Meinhardt, Michael Moller, Caner Hazirbas, and Daniel Cremers. Learning proximal operators:
Using denoising networks for regularizing inverse imaging problems. In International Conference
on Computer Vision (ICCV), 2017.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry P. Vetrov. Variational dropout sparsifies deep
neural networks. In International Conference on Machine Learning (ICML), 2017.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via
convolutional sparse coding. Journal of Machine Learning Research, 2017.
Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. Theoretical foundations of deep
learning via sparse representations: A multilayer sparse model and its connection to convolutional
neural networks. IEEE Signal Processing Magazine, 2018.
Peter Richtarik and Martin Takac. On optimal probabilities in stochastic coordinate descent methods.
Optimization Letters, 2016.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. In
International Conference on Learning Representations (ICLR), 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Stefano Soatto and Alessandro Chiuso. Visual representations: Defining properties and deep ap-
proximations. arXiv preprint arXiv:1411.7676, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 2014.
Jeremias Sulam, Aviad Aberdam, and Michael Elad. On multi-layer basis pursuit, efficient algo-
rithms and convolutional neural networks. arXiv preprint arXiv:1806.00701, 2018.
Christoph Vogel and Thomas Pock. A primal dual network for low-level vision problems. In German
Conference on Pattern Recognition, 2017.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In
Advances in Neural Information Processing Systems (NIPS), 2013.
Sida Wang and Christopher Manning. Fast dropout training. In International Conference on Ma-
chine Learning (ICML), 2013.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduc-
tion. SIAM Journal on Optimization, 2014.
Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Deep ADMM-Net for compressive sensing MRI.
In Advances in Neural Information Processing Systems (NIPS), 2016.
Jian Zhang and Bernard Ghanem. ISTA-Net: Iterative shrinkage-thresholding algorithm inspired
deep network for image compressive sensing. In Computer Vision and Pattern Recognition
(CVPR), 2018.
Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss
minimization. In International Conference on Machine Learning (ICML), 2015.
Zhihui Zhu and Michael B Wakin. On the asymptotic equivalence of circulant and Toeplitz matrices.
IEEE Transactions on Information Theory, 2017.
10
Published as a conference paper at ICLR 2019
A Leaky ReLU as a Proximal Operator
Proposition 3. The Leaky ReLU is the solution of the proximal operator of the function
g(x) = 2 Pi max2 (-Xi — λ, 0) with slope ι++γ and shift λ.
Proof. The proximal operator is defined as
Proxg (a) = arg min 口IX - a∣
x2
k2 + Y X max2 (-Xi- λ, 0)
i
、-------------V-----
γ-smooth
Note that the problem is both convex and smooth. The optimality conditions are given by:
X - a - γmax (-X - λ1, 0) = 0
Since the problem is separable in coordinates, we have:
ai	if Xi ≥ -λ	ai	if ai ≥ -λ
Xi = lai++Yλ if Xi ≤-λ → Proxg (ai )= lai+Yλ if ai ≤ -λ
The Leaky ReLU is defined as
ai if ai ≥ 0
LeakyReLU(ai) = αiai if aii ≤0,
which shows that Proxg is a generalized form of the Leaky ReLU with a shift of λ and a slope
α = ι++γ. With λ = 0 and Y = 1-α the standard form of the Leaky ReLU is recovered.
B Approximation of a Soft Plus as a Proximal Operator
Proposition 4. The proximal operator to g(X) = -Y Pi log(Xi) approximates the SoftPlus activa-
tion.
Proof. The proximal operator is defined as
Proxg (a) = arg min 1 IlX — a∣∣2 — Y X log (Xi).
x2
i
(9)
Note that the function g(X) is elementwise separable, convex, and smooth. By equating the gradient
to zero and taking the positive solution of the resulting quadratic polynomial, we arrive at the closed-
form solution:
Proxg(a)
1 X +↑J4 X Θ X + Y,
(10)
}
□
where denotes elementwise multiplication. It is easy to see that this operator is close to zero for
Xi << 0, and close to Xi for Xi >> 0, with a smooth transition for small |Xi |.	□
Note that the function Proxg(a) approximates the activation SoftPlus = log(1 + exp (a)) very well.
An illustrative example is shown in Figure 2.
C	Approximation of TanH and Sigmoid as a Proximal Operator
Proposition 5. The proximal operator to g(X) = -YPi log(1 - Xi Xi) approximates the Tanh
non-linearity.
11
Published as a conference paper at ICLR 2019
Figure 2: Comparison of common activation functions and their approximation via the correspond-
ing proximal operator. Left: SoftPlus activation and corresponding proximal operator with γ = 1.
Right: Tanh activation and corresponding proximal operator with γ = 0.1.
Proof. To simplify the exposition, we derive the proximal operator for the case γ = 1. The general
case for γ > 0 follows analogously. The proximal operator is defined as
ProXg(a) = argmin 1∣∣x — a∣∣2 — X log(1 — Xi Θ Xi).	(11)
x2
i
Note that the logarithm is taken element wise, and the objective is conveX and smooth. By equating
the gradient to zero, it can be seen that the optimal solution is a root of a cubic equation:
xi3 - aixi2 - 3xi + ai = 0,	(12)
which is defined in each coordinate i separately.
Let
a2
P=ai+ι,
—a3
q = 27,
(13)
Since q2 - P3 < 0, ∀ai ∈ R, it is guaranteed that all roots are real and distinct. Consequently, the
roots can be described as
rk = 2√pcos ( 3cos-1
ai3
27 √P3
+2∏k)+a, k∈{o,ι,2}.
Since g(x) is only defined on x ∈ [-1, 1]d, the root that minimizes (11) has to satisfy
-1 ≤ ropt ≤ 1.
y = cos-1 ( 2√√~^ ) implies 0 ≤ y ≤ π . Let
fk =cos 1y y + 2πk-
It is straightforward to check that
fο ∈ 2,1 ,
1
fι ∈ -1, - 2
f2 ∈
11
2, 2
(14)
(15)
(16)
(17)
—
By substituting fk into (14) and checking inequality (15) it becomes clear that the root corresponding
to - = 2 minimizes (11). By using trigonometric identities the root corresponding to - = 2 can be
further simplified to
(ProXg (a))i = -2√psin
3 sin-1
(18)
which has the approXimate shape of the Tanh activation.
□
An eXample of this operator is shown in Figure 2. The proXimal operator corresponding to the
Sigmoid activation can be derived in a similar fashion by setting g(x) = -γ log(x) - γ log(1 - x).
12
Published as a conference paper at ICLR 2019
D Tensor Preliminaries
The exposition presented in the paper requires some definitions related to tensors and opera-
tors on tensors. We summarize the material here. In all subsequent definitions, we assume
D ∈ Rn1 ×n2 ×n3 ×n4 and X~ ∈ Rn2 ×1×n3 ×n4.
Definition D.1. (Bibi & Ghanem, 2017) The t-product between high-order tensors is defined as
D ~HO X~ = foldHO circHO (D) MatVecHO X~	(19)
where circHO (D) ∈ Rn1n3n4×n2n3n4 and MatVecHO X~ ∈ Rn2n3n4×1.
The operator circHO(.) unfolds an input tensor into a structured matrix. On the other hand,
MatVecHO(.) unfolds an input tensor into a vector. The fold and unfold procedures are detailed
in Bibi & Ghanem (2017).
Definition D.2. (Bibi & Ghanem, 2017) The operator
D (:, :, 1, 1) 0n1 ×n2	...	...	0n1 ×n2
..	..	.
...	.	.	...	.
bdiag (D)=	.............. D (:, :,i,j) ...	.	,	(20)
..	..	..	.
...	.	.	.	.
...	...	...	... D (:, :, n3, n4)
where bdiag (.) : Cn1 ×n2×n3×n4 → Cn1n3n4×n2n3n4, maps a tensor to a block diagonal matrix of
all the frontal faces D(:, :, i,j). Note that ifn3 = n4 = 1, bdiag () is an identity mapping. Moreover
if n1 = n2 = 1, bdiag (.) is a diagonal matrix.
Due to the structure of the tensor unfold of circHO(.), the resultant matrix circHO (D) exhibits the
following blockwise diagonalization:
CircHO(D) = (Fn4 乳 Fn3 乳 Ini) bdiag (D) (Fn4 乳 Fn3 乳 In2 )H ,	(21)
where Fn is the n X n Normalized Discrete Fourier Matrix. Note that D has the dimensions n3 and
n4 replaced with the corresponding 2D Discrete Fourier Transforms. That is D(i, j, :, :) is the 2D
Discrete Fourier Transform of D(i, j, :, :).
For more details, the reader is advised to start with third order tensors in the work of Kilmer &
Martin (2011) and move to the work of Bibi & Ghanem (2017) for extension to higher orders.
13
Published as a conference paper at ICLR 2019
E	Proof of Lemma 1
Since non-linear activations commonly used in deep learning are elementwise, we only need to
show that A ~HO X~ l-1 performs a convolution. In particular we need to show, that A ~HO X~ l-1
is equivalent to (i) performing 2D convolutions spatially along the third and fourth dimensions. (ii)
It aggregates the result along the feature dimension n1. (iii) It repeats the procedure for each of the
n2 filters independently. We will show the following using direct manipulation of the properties of
~HO (Bibi & Ghanem, 2017).
n1
A~HOX~l-1 = A(:,i,:,:) ~HO X~ l-1(i,:,:,:)
i
n1
= X foldHO circHO ((A (:, i, :, :))) MatVec X~ l-1 (i, :, :, :)	(22)
i
Note that Equation (22) shows that features are aggregated along the n1 dimension. Now, by show-
ing that A (:, i, :, :) ~HO X~ l-1 (i, :, :, :) performs n2 independent 2D convolutions along on the ith
channel, the Lemma 1 is proven. For ease of notation, consider two tensors U~ ∈ Rn2 ×1×W ×H and
Y~ ∈ R1×1×W ×H then we have the following:
U~ ~HO Y~ (=19) foldHO circHO
MatVecHO Y~
(=) foldHo ( (FH q FW 区 In2) bdiag
(FH 0 FW)H MatVecHO(Y~)
2D-Fourier Transform
foldHO
((FH 0 FW 0 In2) bdiag (U MatVecHO
(=20) foldHO
∖	hu~(:, 1,1,1)i ~(1,1,1,1).
(FH 0 FW 0 In2)	[U(:,1,i,j)i Y(1,1,i,j)
2D-Inverse Fourier Transform with stride of of n2	.
hu~(:, 1,W,H)i ~(1,1,W,H)
(--------------V-------------:
→
∖	G
Note that G is the elementwise product of the 2D Discrete Fourier Transform between a feature of
an input activation Y~ and the 2D Discrete Fourier Transform of every filter of the n2 in Y~. Since
~
(FH 0 FW 0 In2) is the inverse 2D Fourier transform along each of the n2 filters resulting in G.
Thus U~ ~HO Y~ performs 2D convolutions independently along each of the n2 filters, combined with
(22); thus, Lemma 1 is proven.
14
Published as a conference paper at ICLR 2019
F Proof of Lemma 2
Since,
λmaχ 卜2 (l XXX kAH(i, ：, ：, ：) ~HO 不-炉TkF))
(1=) λmaχ 卜2 (1 X kcircHO (AH(i, ：, ：, ：)) MatVeC (X) - MatVeC (XJl-1) kF))
=λmaχ (X CirCHO (AH(i,:,:,:)) CirCHO (AH(i,:,:,:)))
=λmaχ (CirCHO (AH) CirCHO (AH)).
(23)
The seCond equality also follows from the separability of k.k2F where we dropped foldHO. The last
equality follows from the linearity of ~HO. Moreover, from (19), we have:
CirCHO (AH) CirCHO (AH)
(21)
(FH 0 FW 0 Inj bdiag (AH)(FH 0 FW 0 1皿)H
(FH 0 Fw 0 In1) bdiag (AH) (FH 0 FW 0 九)h
(FH 0 FW 0 In2) bdiag (A) bdiag (AH) (FH 0 FW 0 I/ )h
(=20) (FH0FW0In2)
a(:, :,1,1)AH(:,:, 1,1)	0
0	A(:,:, 1, 2)Ah(:, :, 1, 2)
(FH0FW0In2)H
(FH 0 FW 0 In2)
U11
0
...	0
U12	0
Σ11	0	0
0	Σ12	0
UH
U11	. . .
0	Ui⅛
0	...
(FH 0 FW 0 九)H .
0
H
0
0
0
0
|
^^^^{^^^^~
eigenveCtors
}
(24)
The seCond equality follows from the orthogonality of FH 0 FW 0 In1 . The fourth equality follows
from A(:,:, i,j )Ah(:, :, i,j) = UjEjUHH .Thus by Combining (23) and (24), We have:
L = λmaχ V2F∣
“∑11	0
λmax 0	Σi2
0]∖
=	max	(λmax (Σij ))
i∈{1,2,...,W},j∈{1,2,...,H}
max
i∈{1,2,...,W},j∈{1,2,...,H}
15
Published as a conference paper at ICLR 2019
G Proof of Proposition 1
Lemma 3. For τ -nice Prox-SG,
Ψτ(Xk) = 1
τ
X 停 kAH(i, ：, ：, ：) ~HO X~ -炉 1(i, ：, ：, ：)kF
i∈S
- hB~, Xi
is an unbiased estimator to F (X~).
Proof.
[ψτ(Xj)i = E	X gkAH(i,:,:,:) ~ho 不 -炉 1kF
i∈S
hB~,Xi
i∈S
n1	n1
n1 E kAH(i,:,:,:) ~HO 不-不 l-1kF E [1 i∈s ] - T反 Xi Ve [1 i∈s ]
2τ	τ
n1
n1
i=1
n1
1X kAH(i, ：, ：, ：) ~HO X1-X1 l-1kF
→.
- hB~, Xi = F(X~).
i
The first equality follows by introducing an indicator function where li∈s = 1 if i ∈ S and zero
otherwise. The last equality follows from the uniformity across elements of the τ -nice S.
From Lemma 3, and with zero initialization it follows that
VΨτ(X~)
=V I1X nkAH(i, ：, ：, ：) ~HO Xk-『l-1(i, ：, ：, ：)kF - hB,Xi) ∣
X~=0	τ i∈S2	X~=0
(=) X V (n1 IIcircHO (AH(i, ：, ：, ：))) MatVec (X) 一 MatVec
i∈S
X ( 一n1 CirCHO (AH(i, ：, ：, ：)) MatVec
i∈S	τ
X (-nA(：,i, ：, ：) ~HO X7-1(i, ：, ：, ：) - 1B
i∈S	τ	τ
kF-1 ⑻ Xi)L=o
(25)
is an unbiased estimator of VF(X~) ∣1~ =0. The last iteration follows by noting that A = (AH)H.
Therefore, the first iteration ofτ -nice Prox-SGD with zero initialization and unit step size is:
不一PrOX 1 g
(-Vψτ(χi k=0)
」A(:,i,:,:) ~ho Xl 1(i,:,:,:) +—B
Prox 1 g
(26)
E
—
1E
τ
i
→
□
τ
τ
Note that the previous stochastic sum in (26) with τ = (1 - p)n1 can be reparameterized as follows:
X~ = Prox 1〃
L g
Prox 1 g
n1	n1
1 B?X ⅛∈s + n X A(：, i, ：, ：) ~ho ∙~l-1(i, ：, ：, ：)1 i∈s
ii
~ + n1 A ~ho (M Θ 炉T)) = Prox1g (B) + ? A ~ho BerDropoutp (炉T))
(27)
16
Published as a conference paper at ICLR 2019
where M ∈ Rn1 ×1×W ×H is a mask tensor. Note that since τ = (1 - p)n1, M has exactly
pn1 slices M(i, :, :, :) that are completely zero. This equivalent to a dropout layer where the layer
drops exactly pn1 input activations. It follows that (27) is equivalent to a forward pass through a
BerDropoutp layer followed by a linear layer and non-linear activation.
H Proof of Proposition 2
mS2GD (Konecny et al., 2016) with zero initialization at the first epoch defines the following update:
,~ 一 Proxig (∙~ -VFB) - (J XWfi(X)-Vfi(Y")).
(28)
With zero initialization at the first epoch we have Y = 0, therefore
n1
VF(Y)∣Y=0 = n1 X Vfi(Y)IY =0 T(InI-A~HO 炉T-反	(29)
1i
Moreover,
1X Vfi(X) -1X Vfi(Y )∣Y=0
i∈S	i∈S
=X X nIcircHHO (AH(i,:,:,:)) [circHO (AH(i,:,:,:)) MatVec (4~) - MatVec (4~l-1)]-屯
τ i∈S
-X X -nιcircHO (AH(i,:,:,:))H MatVec (4~l-1) - B~
τ i∈S
=n1 X circHHO (AH(i,:,:,:)) circHO (AH(i,:,:,:)) MatVec(X)
τ i∈S
=，X A(:, i,:,:) ~ho AH(i,:,:,:) ~ho -~	(30)
τ i∈S
Substituting both (29) and (30) into (28) we have
不 JPrOX 1 g
X~ + A ~HO X~1-1 + 屯—(? X A(:, i, :, :) ~HO AH (i, :, :, :) ~HO *~))
Prox 1 g	A ~HO XzlT + B + (I- nT1 X A(:,i,:,:) ~HO AH(i,:,:,:) ) ~ho 不 I
Fully-connected/Convolutional	i∈S
(31)
Note that I ∈ Rn2 ×n2 ×W ×H is the identity tensor in which all frontal slices are identity
I(:, :, i, j) = In2×n2 and all other slices are zeros. Following the definition in (19), we have
I~HOX~= X~.
17
Published as a conference paper at ICLR 2019
I Randomized Coordinate Descent Permutes Dropout and Linear
Layer
We present an additional insight to the role of stochastic solvers on (5) in network design. In par-
ticular, we show that performing a randomized coordinate descent, RCD, on (5) ignoring the finite
sum structure, is equivalent to a linear transformation followed by BerDropoutp and a non-linear
activation. That is, performing RCD permutes the order of linear transformation and dropout. For
ease of notation, we show this under the special case of fully connected layers.
Proposition 6. A single iteration of Randomized Coordinate Descent, e.g. NSync (Richtarik &
Takac, 2016), with T-nice sampling of coordinates of (5) with T = (1 - p)n2, unit step sizes along
each partial derivative, and with zero initialization is equivalent to:
PrOx 1 g
A(i, :, :, :) ~HO X~ l-1 + B(i, :, :, :)
which is equivalent to a forward pass through a linear layer followed by a BerDropoutp layer (that
drops exactly n2p output activations) followed by a non-linear activation.
Proof. We provide a sketch of the proof on the simple quadratic F (x) = 1 ∣∣A>x - XlTk2 - b>x
where the linear layer is a fully-connected layer. Considering a randomized coordinate descent, e.g.
NSync, with T -nice sampling of the coordinates we have the following:
x J Proxg x
Proxg X
-X ɪe>VF(x)ej
i∈S vi
-X Je> (A (ATX - Xj) - b) ei
i∈S vi
X=O Proxg ( - X — e> (-Axl-1 - b)
i∈S vi
= Proxg Xei> hDiag (v)-1 Axl-1 + bi ei
i∈S
v=1 Proxg (Dropout? (AXlT + b))
(32)
Note that ei is a vector of all zeros except the ith coordinate which is equal to 1. Moreover, since
the step sizes along each partial derivative is 1, v = 1. Equation (32) is equivalent to a forward pass
through a linear layer followed by a BerDropout? layer and a non-linear activation.	□
18
Published as a conference paper at ICLR 2019
J	ResNet Blocks
In here, we briefly discuss how a ResNet block can be incorporated as an iteration of a solver. The
key observation is that a ResNet block has two non-linearities connected through a skip connection.
In particular, consider the lth ResNet block with two linear operators denotes as A(1) and A(2)
with biases as B(1) and B(2), respectively. The input activations to the ResNet block form the
previous layer. Therefore, a forward pass through a ResNet block are Xk-1 where Xl are the output
activations. A forward pass through a ResNet block exhibits the following functional form:
Xl = ReLU A(2) ~HO ReLU A(1) ~HO Xl-1 + B(1)	+ B(2) + X (l-1)
This can be seen as two blocks of linear followed by nonlinear composed such that:
X = ReLU (A(I) ~HO XlT + B(I))
Xl = ReLU (A⑵ ~ho X + B⑵ + XlT)
Note that a a batch normalization layer is linear during the forward pass through the network and thus
can be absorbed in A and B. Consequently, a forward pass through a ResNet block can be viewed
as performing a single iteration of two consecutive stochastic or deterministic solvers (depending on
the presence or absence of Dropout layers) on (5).
19