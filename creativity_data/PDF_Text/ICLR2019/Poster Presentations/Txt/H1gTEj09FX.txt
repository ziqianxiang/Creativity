Published as a conference paper at ICLR 2019
RotDCF: Decomposition of Convolutional Fil-
ters for Rotation-Equivariant Deep Networks
Xiuyuan Cheng, Qiang Qiu, Robert Calderbank & Guillermo Sapiro
Department of Mathematics, Department of Electrical & Computer Engineering
Duke University
Durham, NC 27708, USA
{xiuyuan.cheng,qiang.qiu,robert.calderbank,guillermo.sapiro}@duke.edu
Ab stract
Explicit encoding of group actions in deep features makes it possible for convo-
lutional neural networks (CNNs) to handle global deformations of images, which
is critical to success in many vision tasks. This paper proposes to decompose
the convolutional filters over joint steerable bases across the space and the group
geometry simultaneously, namely a rotation-equivariant CNN with decomposed
convolutional filters (RotDCF). This decomposition facilitates computing the joint
convolution, which is proved to be necessary for the group equivariance. It sig-
nificantly reduces the model size and computational complexity while preserving
performance, and truncation of the bases expansion serves implicitly to regularize
the filters. On datasets involving in-plane and out-of-plane object rotations, Rot-
DCF deep features demonstrate greater robustness and interpretability than regu-
lar CNNs. The stability of the equivariant representation to input variations is also
proved theoretically. The RotDCF framework can be extended to groups other
than rotations, providing a general approach which achieves both group equivari-
ance and representation stability at a reduced model size.
1	Introduction
While deep convolutional neural networks (CNN) have been widely used in computer vision and
image processing applications, they are not designed to handle large group actions like rotations,
which degrade the performance of CNN in many tasks (Cheng et al., 2016; Hallman & Fowlkes,
2015; Jaderberg et al., 2015b; Laptev et al., 2016; Maninis et al., 2016). The regular convolutional
layer is equivariant to input translations, but not other group actions. An indirect way to encode
group information into the deep representation is to conduct generalized convolutions across the
group as well, as in Cohen & Welling (2016a). In theory, this approach can guarantee the group
equivariance of the learned representations, which provides better interpretability and regularity as
well as the capability of estimating the group action in localization, boundary detection, etc. For
the important case of 2D rotations, group-equivariant CNNs have been constructed in several recent
works, e.g., (Weiler et al., 2017), Harmonic Net (Worrall et al., 2017) and Oriented Response Net
(Zhou et al., 2017). In such networks, the layer-wise output has an extra index representing the
group element (c.f. Table 1), and consequently, the convolution must be across the space and the
group jointly (proved in Section 3.1). This typically incurs a significant increase in the number of
parameters and computational load, even with the adoption of steerable filters (Freeman et al., 1991;
Weiler et al., 2017; Worrall et al., 2017). In parallel, low-rank factorized filters have been proposed
for sparse coding as well as the compression and regularization of deep networks. In particular, Qiu
et al. (2018) showed that decomposing filters under non-adaptive bases can be an effective way to
reduce the model size of CNNs without sacrificing performance. However, these approaches do not
directly apply to be group-equivariant. We review these connections in more detail in Section 1.1.
This paper proposes a truncated bases decomposition of the filters in group-equivariant CNNs, which
we call the rotation-equivariant CNN with decomposed convolutional filters (RotDCF). Since we
need a joint convolution over R2 and SO(2), the bases are also joint across the two geometrical
domains, c.f. Figure 1. The benefits of bases decomposition are three-fold: (1) Reduction of the
number of parameters and computational complexity of rotation-equivariant CNNs, c.f. Section 2.3;
1
Published as a conference paper at ICLR 2019
Figure 1: Decomposition of the convolutional filter across the 2D space (variable u) and the SO (2) rotation
group geometry (variable α) simultaneously. The filter is represented as a truncated expansion under the pre-
fixed bases ψk (u)ψm(α) with adaptive coefficients ak,m learned from data. ψk are FoUrier-BeSSel bases,夕m
are Fourier bases, the first 3 of each are shown. The filter has Nθ group-indexed channels (indexed by α) and
only one inpUt and oUtpUt UnstrUctUred channel (indexed by λ0 and λ respectively) for simplicity. c.f. Table 1.
(	2) Implicit regUlarization of the convolUtional filters, leading to improved robUstness of the learned
deep representation shown experimentally in Section 4; (3) Theoretical gUarantees on stability of
the eqUivariant representation to inpUt deformations, which follow from a generic condition on the
filters in the decomposed form, c.f. Section 3.2.
1.1	Related Work
Learning with factorized filters. In the sparse coding literatUre, RUbinstein et al. (2010) proposed
the factorization of learned dictionaries Under another prefixed dictionary. Separable filters were
Used in Rigamonti et al. (2013) to learn the coding of images. Papyan et al. (2017) interpreted
CNN as an iterated convolUtional sparse coding machine, and in this view, the factorized filters
shoUld correspond to a “dictionary of the dictionary” as in RUbinstein et al. (2010). In the deep
learning literatUre, low-rank factorization of convolUtional filters has been previoUsly Used to remove
redUndancy in trained CNNs (Denton et al., 2014; Jaderberg et al., 2014). The compression of
deep networks has also been stUdied in Chen et al. (2015); Han et al. (2016; 2015), SqUeezeNet
(Iandola et al., 2016), etc., where the low-rank factorization of filters can be Utilized. MobileNets
(Howard et al., 2017) Used depth-wise separable convolUtions to obtain significant compression.
Tensor decomposition of convolUtional layers was Used in Lebedev et al. (2014) for CPU speedUp.
Tai et al. (2015) proposed low-rank-regUlarized filters and obtained improved classification accUracy
with redUced compUtation. QiU et al. (2018) stUdied decomposed-filter CNN with prefixed bases and
trainable expansion coefficients, showing that the trUncated bases decomposition incUrs almost no
decrease in classification accUracy while significantly redUcing the model size and improving the
robUstness of the deep featUres. None of the above networks are groUp eqUivariant.
Group-equivariant deep networks. The encoding of groUp information into network representa-
tions has been stUdied extensively. Among earlier works, transforming aUto-encoders (Hinton et al.,
2011) Used a non-convolUtional network to learn groUp-invariant featUres and compared with hand-
crafted ones. Rotation-invariant descriptors were stUdied in Schmidt & Roth (2012b) with prodUct
models, and in Jaderberg et al. (2015a); Kivinen & Williams (2011); Schmidt & Roth (2012a) by
estimating the specific image transformation. Gonzalez et al. (2016); WU et al. (2015) proposed
rotating conventional filters to perform rotation-invariant textUre and image classification. The joint
convolUtion across space and rotation has been stUdied in the scattering transform (Oyallon & Mallat,
2015; Sifre & Mallat, 2013). GroUp-eqUivariant CNN was considered by Cohen & Welling (2016a),
which handled several finite small-order discrete groUps on the inpUt image. Rotation-eqUivariant
CNN was later developed in Weiler et al. (2017); Worrall et al. (2017); ZhoU et al. (2017) and else-
where. In particUlar, steerable filters were Used in Cohen & Welling (2016b); Weiler et al. (2017);
Worrall et al. (2017). SO(3)-eqUivariant CNN for signals on spheres was stUdied in Cohen et al.
(2018) in a different setting. Overall, the efficiency of eqUivariant CNNs remains to be improved
since the model is typically several times larger than that of a regUlar CNN. The cUrrent paper adopts
bases-decomposed filters previoUsly stUdied in the non-eqUivariant setting (QiU et al., 2018), how-
ever, the joint convolUtion scheme in eqUivariant CNNs is over the two geometries of space and
orientation simUltaneoUsly and neither the approach nor the analysis there can be directly applied.
2
Published as a conference paper at ICLR 2019
2	Rotation-equivariant DCF Net
2.1	Rotation-equivariant CNN
A rotation-equivariant CNN indexes the channels by the SO(2) group (Weiler et al., 2017; Zhou
et al., 2017): The l-th layer output is written as x(l) (u, α, λ), the position u ∈ R2, the rotation
α ∈ S1, and λ ∈ [Ml], Ml being the number of unstructured channel indices. Throughout the
paper, [m] stands for the set {1,…，m}. We denote the group SO(2) also by the circle S1 since the
former is parametrized by the rotation angle. The convolutional filter at the l-th layer is represented
as Wλ(l0),λ(v, α), λ0 ∈ [Ml-1], λ ∈ [Ml], v ∈ R2, α ∈ S1, except for the 1st layer where there is no
indexing of α. In practice, S1 is discretized into Nθ points on (0, 2π). We denote the summation
over U and α by continuous integration, and the notation Jsι (•…)da means 2∏ R2π (•…)dα.
Let the 2D rotation by angle t be denoted by Θt, in the 1st layer of the group-invariant CNN,
x(1) (u, α, λ) = σ
x(0)(u + v0, λ0)Wλ(10,)λ(Θαv0)dv0 + b(1)(λ)
(1)
Note that the 1st layer output has Nθ orientations indexed by α, forming a “channel geometry
(Table 1). For l > 1, the convolution is jointly over R2 and SO(2), which takes the form as
x(l) (u, α, λ) = σ (XI Zsi L2 x(l-1)(u + v , α , λo)wλ0^λ(ΘɑVθ, ɑ0 — α)dv0da0 + b(l) (λ)j .
(2)
The joint convolution over R2 and SO(2) is both sufficient and necessary to guarantee group-
equivariance (Theorem 3.1). While group equivariance is a desirable property, the model size and
computation can be increased significantly due to the extra index α ∈ [Nθ].
2.2	Decomposed Filters Under Steerable Bases
We decompose the filters with respect to u and α simultaneously: Let {ψk}k be a set of bases on the
unit 2D disk, and {ψm}m be bases on S1. At the l-th layer, let jι be the scale of the filter in u, and
ψj,k = 2-2j ψk (2-j u) (the filter is supported on the disk of radius 2jl). Since we use continuous
convolutions, the down-sampling by “pooling” is modeled by the rescaling of the filters in space.
The decomposed filters are of the form
Wλ(10,)λ(v) = X a(λ10),λ(k)ψj1,k(v), Wλ(l0),λ(v, β) = X X a(λl0),λ(k,m)ψjl,k (VWm(Ie), l > 1,⑶
which is illustrated in Figure 1 (for l > 1). We use Fourier-Bessel (FB) bases for {ψk}k and Fourier
bases for {夕m,}m,. Specifically,夕m(α) = eimα, and FB basis has the expression
ψk(r, θ) = cm,qJm(Rm,qr)eimθ, 0 ≤ r ≤ 1,	0 ≤ θ ≤ 2π, k = (m, q)	(4)
where m is the angular frequency, q the radial frequency, Jm the Bessel function (Rm,q the q-throot
of Jm) and cm,q a normalizing constant (Abramowitz & Stegun, 1964). Both bases are “steerable”,
i.e. the operation of rotation is a diagonalized linear transform under both bases. In the complex-
valued version, ψk(Θtv) = e-im(k)tψk(v),夕m(α — t) = e-imt夕m(α). This means that after
the convolutions on R2 X S1 with the bases ψk(V)夕ι(α) are computed for all k and l, both up
to certain truncation, the joint convolution (1), (2) with all rotated filters can be calculated by the
algebraic manipulation of the expansion coefficients a(λl0),λ (k, m), and without any re-computation
of the spatial-rotation joint convolution. Standard real-valued versions of the bases ψk and Wm in
sin’s and cos’s are used in practice. During training, only the expansion coefficients a’s are updated,
and the bases are fixed.
fully-connected layer	regular convolutional layer	CNN with group-indexed channels
x(l-1)(λ0) → x(l)(λ) λ0 → λ: dense	x(l-1)(u0,λ0) → x(l)(u,λ)	x(l-1)(u0,ɑ0,λ0) → x(l)(u,α,λ)
	u0 → u: spatial convolution λ0 → λ: dense	-U → u, & → α: joint convolution λ0 → λ: dense
Table 1: Comparison of a fully-connected layer, a regular convolutional layer, and a rotation-equivariant
convolutional layer with group-indexed channels.
3
Published as a conference paper at ICLR 2019
Apart from the saving of parameters and computation, the bases truncation also regularizes the
convolutional filters by discarding the high frequency components. As a result, DCF Net reduces
response to those components in the input at all layers, which improves the robustness of the learned
feature without affecting recognition performance. The visibly smoother trained filters in RotDCF
are shown in Figure A.1, demonstrating the same regularization effects as in Qiu et al. (2018),
the latter being without the rotation-equivariant setting. The theoretical properties of RotDCF Net,
particularly the representation stability, will be analyzed in Section 3.
2.3	Numbers of Parameters and Computation Flops
Number of trainable parameters: In a regular CNN, a convolutional layer of size L × L × M00 × M0
(L × L being the patch size) has L2M00M0 parameters. In an equivariant CNN, a joint convolutional
filter is of size L × L × Nθ × M0 × M, so that the number of parameters is L2NθM0M. In a RotDCF
Net, K bases are used in space and Kα bases across the angle α, so that the number of parameters is
KKaM0M. This gives a reduction of L ∙ Ka compared to non-bases equivariant CNN. In practice,
L Nθ
after switching from a regular CNN to a RotDCF Net, typically M ≤ 2 Mo or more due to the
adoption of filters in all orientations. The factor L is usually between 8 and 1 depending on the
network and the problem (Qiu et al., 2018). In all the experiments in Section 4, Kα is typically 5,
and Nθ = 8 or 16. This means that RotDCF Net achieves a significant parameter reduction from
the non-bases equivariant CNN (the factor KKNa is 1 or more), and even reduces parameters from a
regular CNN by a factor of 1 or more.
Computation in a forward pass: When the input and output are both W × W in space, the regular
CNN layer needs 2M00M0W2L2 many flops, and a non-bases equivariant convolutional layer needs
about 2M0MW2L2Nθ2. In contrast, the computation in a RotDCF layer is dominated by a term
of 2M0MW2KαKNθ, which is reduced from the non-bases equivariant network by a factor of
LK2 ∙ KNa. Detailed calculations in Appendix A.
L Nθ
In summary, RotDCF Net achieves a reduction of L ∙ KNa from non-bases equivariant CNNs, in
L Nθ
terms of both model size and computation. With typical network architectures, RotDCF Net may be
of a smaller model size than regular CNNs. Numbers for specific networks are shown in Section 4.
3	Theoretical Analysis of Deep Features
This section presents two analytical results: (1) Joint convolution (1), (2) is sufficient and actually
necessary to obtain rotation equivariance; (2) Stability of the equivariant representation with respect
to input variations is proved under generic conditions, which is important in practice since rotations
are never perfect.
3.1	Group-equivariant Property
We consider the change of the l-th layer output when the input image undergoes some arbitrary
rotation. Let rotation around point u0 by angle t be denoted by ρ = ρu0,t, i.e. ρu0,tu = u0 +
Θt(u - u0), for any u ∈ R2, and the transformed image by Dρx(0)(u, λ) = x(0) (ρu0,tu, λ), for any
λ ∈ [M0]. We also define the action Tρ on the l-th layer output x(l), l > 0, as
Tρx(l) (u, α, λ) = x(l)(ρu0,tu, α - t, λ),	∀λ ∈ [Ml].	(5)
The following theorem, proved in Appendix B, shows that the joint convolution scheme (1), (2) not
only produces group-equivariant features at all layers in the sense of
x(l) [Dρx(0)] = Tρx(l) [x(0)],	(6)
but is necessary for a CNN with SO(2)-indexed channels to achieve (6). The sufficiency part is
previously shown in Weiler et al. (2017). The necessity of the joint convolution motivates the design
and the efforts of reducing the complexity of such models. Note that RotDCF is a type of the
channel-indexed CNNs considered in the theorem, so it follows that RotDCF is group-equivariant.
Theorem 3.1. In a CNN with SO(2)-indexed channels, let x(l) [x(0)] be the output at the l-th layer
from input x(0)(u, λ). The relation (6) holds for all l if and only if the convolutional layers are given
by (1), (2).
4
Published as a conference paper at ICLR 2019
3.2	Representation Stability under Input Variations
Assumptions on the RotDCF layers. Following Qiu et al. (2018), we make the following generic
assumptions on the convolutional layers: First,
(A1) Non-expansive sigmoid: σ : R → R is non-expansive.
Second, we also need a boundedness assumption on the convolutional filters W(l) for all l:
(A2) Boundedness of filters: In all layers, Al ≤ 1,
where Al is defined as
Ml-1	M	Ml
Al= π max{sup E kaλl) λkFB, SUp -J-- Ekaλl) λkFB},	⑺
λ λ0=1	,	λ0	Ml λ=1	,
kaλ-,λkFB = Xμk(aλ-,λ(k))2,	kaλl),λkFB = XXμk(aλl),λ(k,m))2, l> 1,⑻
μk being the Dirichlet Laplacian eigenvalues of the unit disk in R2. Note that (A2) bounds the
expansion coefficients, which implies a sequence of boundedness conditions on the convolutional
filters in all layers (Proposition B.1), based upon which the stability results below are derived. Since
μk typically increases in order, (A2) suggests truncating the series to only include low-frequency k
and m’s, which is implemented in Section 4. The validity of the boundedness assumption can be
qualitatively fulfilled by normalization layers which is standard in practice.
Non-expansiveness of the network mapping. Let the L2 norm of x(l) be defined as
kx(l)k2
M X ∣⅛ 1r2 Is、
x(l)(u, α, λ)2dudα,
l≥1
and ∣∣x(0)k2 = 才 £入 吉 ‰2 x(0)(u, λ)2du. Ω is the domain on which x(0) is supported, usually
Ω = [-1,1] X [-1,1] ⊂ R2. The following result is proved in Appendix B:
Proposition 3.2. In a RotDCF Net, under (A1), (A2), for all l,
(a)	The mapping of the l-th convolutional layer (including σ), denoted as x(l) [x(l--)], is non-
expansive, i.e., kx(l) [x-] - x(l) [x2]k ≤ kx- - x2k for arbitrary x- and x2.
(b)	kx(cl) k ≤ kx(cl--) k for all l, where x(cl) (u, α, λ) = x(l)(u, α, λ) - x(0l)(λ) (without index α when
l=1) is the centered version of x(l) by removing x(0l), defined to be the output at the l-th layer from a
zero bottom-layer input. As a result, kx(cl) k ≤ kx(c0) k = kx(0) k.
Insensitivity to input deformation. We consider the deformation of the input “module” to a global
rotation. Specifically, let the deformed input be of the form Dρ ◦ Dτ x(0), where Dρ is as in Section
3.1, ρ = ρu0 ,t being a rigid 2D rotation, and Dτ is a small deformation in space defined by
Dτx(0)(u,λ) =	x(0)(u	- τ(u),	λ),	∀u	∈	R2,	λ ∈	[M0],	(9)
with τ : R2 → R2 is C2. Following Qiu et al. (2018), we assume the small distortion condition:
(A3) Small distortion: ∣Vτ∣∞ = SUpu ∣∣Vτ(u)k < -, with k ∙ k being the operator norm.
The mapping u → U 一 T(U) is locally invertible, and the constant - is chosen for convenience. With
Tρ as defined in (5), the stability result is summarized as
Theorem 3.3. Let ρ = ρu0,t be an arbitrary rotation in R2, around U0 by angle t, and let Dτ be a
small deformation. In a RotDCF Net, under (A1), (A2), (A3), c- = 4, c2 = 2, for any L,
∣∣χ(L)[Dρ ◦ DTX(O)] - TPx(L)[x(0)]k ≤ (2c-L∣Vτ∣∞ + C22-L∣τ∣∞)∣∣x⑼∣∣.
5
Published as a conference paper at ICLR 2019
Figure 2: Representative class activation maps (CAM) on testing images in the rotMNIST transfer learning
experiment. The heatmap indicates the importance of image regions used in recognizing a digit class. The CNN
and RotDCF networks are trained on up-right samples, with no retrain (left) and retraining the fully connected
layers respectively (right) before testing. Testing samples are randomly rotated up to 60 degrees. (c.f. Table 2).
MNIST to rotMNIST MaXRot=30 Degrees
no-retrain fc-retrain
CNN	9261	9471
RotDCF	96.90	98.48
MNIST to rotMNIST MaXRot=60 Degrees
no-retrain fc-retrain
CNN	69.61	85.90
RotDCF	82.36	97.68
Table 2: Test accuracy in the rotMNIST transfer learning eXperiment. The network is trained on 10K up-right
MNIST samples and tested on 50K randomly rotated samples up to the MaXRot degrees.
To prove Theorem 3.3, we firstly establish an approXimate equivariant relation for all layers l (Propo-
sition B.2), which can be of independent interest for estimating the image transformations. All the
proofs are left to AppendiX. Unlike previous stability results for regular CNNs, the above result
allows an arbitrary global rotation ρ with respect to which the RotDCF representation is equivariant,
apart from a small “residual” distortion τ whose influence can be bounded. This is also an important
result in practice, because most often in recognition tasks the image rotation is not a rigid in-plane
one, but is induced by the rotation of the object in 3D space. Thus the actual transformation of
the image may be close to a 2D rotation but is not eXact. The above result guarantees that in such
cases the RotDCF representation undergoes approXimately an equivariant action of Tρ , which im-
plies consistency of the learned deep features up to a rotation. The improved stability of RotDCF
Net over regular CNNs in this situation is observed eXperimentally in Section 4.
4 Experimental Results
In this section, we eXperimentally test the performance of RotDCF Nets on object classification and
face recognition tasks. The advantage of RotDCF Net is demonstrated via improved recognition
accuracy and robustness to rotations of the object, not only with in-plain rotations but with 3D
rotations as well. To illustrate the rotation equivariance of the RotDCF deep features, we show that
a trained auto-encoder with RotDCF encoder layers is able to reconstruct rotated digit images from
“circulated” codes. All codes will be publicly available.
4.1	Object Classification
Non-transfer learning setting. The rotMNIST dataset contains 28 × 28 grayscale images of digits
from 0 to 9, randomly rotated by an angle uniformly distributed from 0 to 2π (Cohen & Welling,
2016a). We use 12,000 and 6,000 training samples, and 50,000 testing samples. The baseline net is
rotMNIST Conv-6, Ntr = 12K
	Test Acc.	# param.	Ratio
CNN M=32	96.54	6.796×105	1.00
DCF M=32, K=5	96.54	1.363×105	0.20
DCF M=32, K=3	96.63	8.194×104	0.12
SFCNN N =8, M=16	-〜98.8-	-	-
RotDCF NO =8			
M=16, K=14, Ka =8	98.61	7.603×105	1.12
M=16, K=5, Ka =8	98.62	2.717×105	0.40
M=16, K=5, Ka =5	98.58	1.699×105	0.25
M=16, K=3, Ka =5	98.43	1.020×105	0.15
M=8, K=14, Ka =8	98.56	1.902×105	0.28
M=8, K=5, Ka =8	98.45	6.799×104	0.10
M=8, K=5, Ka =5	98.40	4.255×104	0.06
M=8, K=3, Ka =5	98.40	2.557×104	0.04
rotMNIST Conv-6, Ntr = 6K		
	Test Acc.	# param.	Ratio
CNN M=32 DCF M =32, K=3	95:17 95.14	
SFCNNNO=8, M=16 RotDCF NO=8 M =16, K =14, Ka=8 M=16,K=5,Ka=5 M=8, K=5, Ka=5 M=8, K=3, Ka=5	-^^983- 98.00 97.95 97.95 97.82	-- (same as left)
CIFAR10 VGG-16, Ntr = 10K		
CNN M = 64	78.40	2.732×106	1.00
RotDCF, NO= 8 M=32, K=3, Ka=7 M=32, K=3, Ka=5	79.44 79.53	1.593 ×106	0.58 1.138×106	0.42
Table 3: Classification accuracy using non-equivariant CNN and rotation-equivariant ones on rotMNIST
and CIFAR10. Baseline DCF (Qiu et al., 2018) and SFCNN (Weiler et al., 2017). “# param.” is number of
parameters in all convolutional layers, and “Ratio” indicates the proportion to the # param. of the regular CNN.
Notice that the reduction from non-bases rotation-equivariant CNNs (the fair comparison case) can be even
smaller, which is the factor of LKa, c.f. Section 2.3. The case of maximum K and Ka (K =14, Ka=8 in
rotMNIST) is mathematically equivalent to using “full” filters without bases truncation.
6
Published as a conference paper at ICLR 2019
Reconstruction
Figure 3: Codes and reconstructions of rotMNIST digits. (Top) A test image is encoded into a 16 × 32 array
in the red box (the intermediate representation), and the code generates 16 copies by circulating the rows.
(Bottom) Images reconstructed from the row-circulated codes above by the decoder.

a CNN with 6 convolutional layers (Table A.2), and we also compare with DCF (Qiu et al., 2018),
which is non-rotation-equivariant but uses bases truncation, and SFCNN (Weiler et al., 2017), which
is rotation-equivariant but does not use bases decompostion. The RotDCF net is made by replacing
the regular convolutional layers with RotDCF layers, with Nθ many rotation-indexed channels and
a reduced number of (unstructured) channels M. K many ψk bases and Ka many ψm are used. The
classification accuracy is shown in Table 3. We see that RotDCF net obtains improved classification
accuracy from CNN with significantly reduced number of parameters, e.g., with 12K training, Rot-
DCF Net with M=8, K=3, Ka=5 improves the test accuracy from 96.54 to 98.40 with less than 击
many parameters of the CNN model and 1 of the DCF model. The performance is also comparable
to SFCNN which has significantly larger model complexity. The results are similar with reduced
training size (5K) and on a shallower net with 3 conv layers, c.f. Table A.3.
The CIFAR10 dataset consists of 32 × 32 colored images from 10 object classes (Krizhevsky,
2009), and we use 10,000 training and 50,000 testing samples. The network architecture is modified
from VGG-16 net (Simonyan & Zisserman, 2014) (Table A.4). As shown in Table 3, RotDCF
Net obtains better testing accuracy with reduced model size from the regular CNN baseline model.
Throughout these experiments, the higher accuracy is due to the group equivariance and the lower
model complexity is due to the bases decomposition.
Transfer learning setting. We train a regular CNN and a RotDCF Net on 10,000 up-right MNIST
data samples, and directly test on 50,000 randomly rotated MNIST samples where the maximum
rotation angle MaxRot=30 or 60 degrees (the “no-retrain” case). We also test after retraining the last
two non-convolutional layers (the “ fc-retrain” case). To visualize the importance of image regions
which contribute to the classification accuracy, we adopt Class Activation Maps (CAM) (Zhou et al.,
2016), and the network is modified accordingly by removing the last pooling layer in the net in Table
A.1. and inserting a “gap” global averaging layer. The test accuracy are listed in Table 2, where the
superiority of RotDCF Net is clearly shown in both the “no-retrain” and “ fc-retrain” cases . The
improved robustness of RotDCF Net is furtherly revealed by the CAM maps (Figure 2): the red-
colored region is more stable for RotDCF Net even in the case with retraining.
4.2	Image Reconstruction
To illustrate the explicit encoding of group actions in the RotDCF Net features, we train a convo-
lutional auto-encoder on the rotated MNIST dataset, where encoder consists of stacked RotDCF
layers, and the decoder consists of stacked transposed-convolutional layers (Table A.5). The en-
coder maps a 28×28 image into an array of 16 × 32, where the first dimension is the discretization
of the rotation angles in [0, 2π], and the second dimension is the unstructured channels. Due to the
rotation equivariant relation, the “circulation” of the rows of the code array should correspond to
Figure 4: Example CAM maps for recognizing faces with in-plane rotations. The heatmap indicates the
importance of different image regions used by respective models in defining a face, and a good CNN model
is expected to select consistent face regions across the same-person images to determine the identity. Across
different in-plane rotated copies, ROtDCF chooses significantly more consistent discriminative regions than
CNN, indicating more stable representations. In this experiment, we obtain 0.54% recognition accuracy using
CNN (nearly random guess), and 97.04% accuracy using RotDCF with feature alignment, on known subjects.
7
Published as a conference paper at ICLR 2019
Figure 6: Example CAM maps for recognizing faces with out-of-plane rotations. Across out-of-plane rotated
copies, the discriminative regions chosen by RotDCF in describing a subject are more consistent, showing better
representation stability than CNN. In this experiment, We obtain 80.79% recognition accuracy using CNN, and
89.66% using ROtDCF, on known subjects.
from a testing image, and the 16 row-circulated copies of it. The bottom panel shows the output of
the decoder fed with the codes in the top panel.
4.3	Face Recognition
As a real-world example, we test RotDCF on the Facescrub dataset (Ng & Winkler, 2014) contain-
ing over 100,000 face images of 530 people. A CNN and a RotDCF Net (Table A.6) are trained
respectively using the gallery images from the 500 known subjects, which are preprocessed to be
near-frontal and upright-only by aligning facial landmarks (Kazemi & Sullivan, 2014). See Ap-
pendix C for data preparation and training details. For the trained deep networks, we remove the last
softmax layer, and then use the network outputs as deep features for faces, which is the typical way
of using deep models for face verification and recognition to support both seen and unseen subjects
(Parkhi et al., 2015). Using deep features generated by the trained networks, a probe image is then
compared with the gallery faces whose identities are known and classified as that of the top match.
Under this gallery-probe face recognition setup, we obtain 94.10% and 96.92% accuracy for known
and unknown subjects respectively using the CNN model; using RotDCF, the accuracies are 93.42%
and 96.92%. Testing on unknown subjects are critical for validating the model representation power
over unseen identities, and the reason for higher accuracy is simply due to the smaller number of
classes. For both cases, RotDCF reports comparable performance as CNN, while the number of
parameters in the RotDCF model is about one-fourth of the CNN model (see Appendix).
In-plane rotation. This experiment demonstrates the rotation-equivariance of the RotDCF features.
We apply in-plane rotations at intervals of ∏ to the probe images (Figure 4), and let the original probe
set be the new gallery, the rotated copies be the new probe set. in this setting, using the RotDCF
model we obtain 97.04% and 97.58% recognition accuracy for known and unknown subjects re-
spectively, after aligning the deep features by circular shifts (using the largest-magnitude α channel
as reference). Notice that the model only sees upright faces. This is due to the rotation-equivariant
property of the RotDCF Net, which means that the face representation is consistent regardless of
its orientations after the group alignment. Lacking such properties, CNN obtains 0.54% and 5.05%
recognition accuracies, which is close to random guess. We further compare CNN and RotDCF
models via the CAM maps, which indicate the image regions relied by the CNN to make a classifi-
cation prediction. in Figure 4, with regular CNN face regions used for prediction varies dramatically
from image to image, and with RotDCF a significantly more consistent region mostly covering eyes
and nose are selected across images, which explains its superior accuracy.
Out-of-plane rotation. To validate our theoretical result on representation stability under input
deformations, we introduce out-of-plane rotations to the probe. Each probe image is fitted to a 3D
face mesh, and rotated copies are rendered at the 10o intervals with -40o to 40o yaw, and -20o to
20o pitch, generating 45 synthesized faces in total (Figure 5). The synthesis faces at two poses
(highlighted in red) are used as the new gallery, and all remaining synthesis faces form the new
probe. The out-of-plane rotations here can be viewed as mild in-plane rotations plus additional
variations, a situation frequently encountered in the real world. With this gallery-probe setup, the
RotDCF model obtains 89.66% and 97.01% recognition accuracy for known and unknown subjects,
and the accuracies are 80.79% and 89.97% with CNN. The CAM plots in Figure 6 also indicate
that RotDCF Net chooses more consistent regions over CNN in describing a subject across different
8
Published as a conference paper at ICLR 2019
poses. Since the out-of-plane rotations as in Figure 5 can be considered as in-plane rotations with
additional variations, the superior performance of RotDCF is consistent with the theory in Section 3.
5 Conclusion and Discussion
This work introduces a decomposition of the filters in rotation-equivariant CNNs under joint steer-
able bases over space and rotations simultaneously, obtaining equivariant deep representations with
significantly reduced model size and an implicit filter regularization. The group equivariant prop-
erty and representation stability are proved theoretically. In experiments, RotDCF demonstrates
improved recognition accuracy and better feature interpretability and stability on synthetic and real-
world datasets involving object rotations, particularly in the transfer learning setting. To extend the
work, implementation issues like parallelism efficiency and memory usage should be considered
before the computational savings can be fully achieved. The framework should also extend to other
groups and joint geometrical domains.
Acknowledgements
Work partially supported by NSF, DoD, NIH and AFOSR.
References
Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions: with formulas, graphs, and
mathematical tables, volume 55. Courier Corporation, 1964.
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural networks
with the hashing trick. In International Conference on Machine Learning, pp. 2285-2294, 2015.
Gong Cheng, Peicheng Zhou, and Junwei Han. Rifd-cnn: Rotation-invariant and fisher discriminative convo-
lutional neural networks for object detection. In Computer Vision and Pattern Recognition (CVPR), 2016
IEEE Conference on, pp. 2884-2893. IEEE, 2016.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In ICML, pp. 2990-2999, 2016a.
URL http://jmlr.org/proceedings/papers/v48/cohenc16.html.
Taco S. Cohen and Max Welling. Steerable cnns. arXiv preprint arXiv:1612.08498, 2016b.
Taco S Cohen, Mario Geiger, Jonas Koehler, and Max Welling. Spherical cnns. arXiv preprint
arXiv:1801.10130, 2018.
Emily L. Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure
within convolutional networks for efficient evaluation. In NIPS, pp. 1269-1277, 2014.
William T Freeman, Edward H Adelson, et al. The design and use of filters. IEEE Transactions on Pattern
analysis and machine intelligence, 13(9):891-906, 1991.
Diego Marcos Gonzalez, Michele Volpi, and Devis Tuia. Learning rotation invariant convolutional filters for
texture classification. CoRR, abs/1604.06720, 2016. URL http://arxiv.org/abs/1604.06720.
Sam Hallman and Charless C Fowlkes. Oriented edge forests for boundary detection. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1732-1740, 2015.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural
network. In Advances in Neural Information Processing Systems, pp. 1135-1143, 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with
pruning, trained quantization and huffman coding. International Conference on Learning Representations
(ICLR), 2016.
Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In International Confer-
ence on Artificial Neural Networks, pp. 44-51. Springer, 2011.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision ap-
plications. arXiv preprint arXiv:1704.04861, 2017.
9
Published as a conference paper at ICLR 2019
Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5mb model size. arXiv:1602.07360,
2016.
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low
rank expansions. arXiv preprint arXiv:1405.3866, 2014.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial trans-
former networks. In NIPS, pp. 2017-2025, 2015a. URL http://papers.nips.cc/paper/
5854-spatial-transformer-networks.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in
neural information processing systems, pp. 2017-2025, 2015b.
Vahid Kazemi and Josephine Sullivan. One millisecond face alignment with an ensemble of regression trees.
In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2014.
Jyri J. Kivinen and Christopher K. I. Williams. Transformation equivariant boltzmann machines. In
ICANN,pp.1-9,2011. doi: 10.1007/978-3-642-21735-7_1. URL http://dx.doi.org/10.10 07/
978-3-642-21735-7_1.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Dmitry Laptev, Nikolay Savinov, Joachim M Buhmann, and Marc Pollefeys. Ti-pooling: transformation-
invariant pooling for feature learning in convolutional neural networks. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp. 289-297, 2016.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. Speeding-up con-
volutional neural networks using fine-tuned cp-decomposition. arXiv preprint arXiv:1412.6553, 2014.
Kevis-Kokitsi Maninis, Jordi Pont-TuseL Pablo Arbeiaez, and LuC Van Gool. Convolutional oriented bound-
aries. In European Conference on Computer Vision, pp. 580-596. Springer, 2016.
Hong-Wei Ng and Stefan Winkler. A data-driven approaCh to Cleaning large faCe datasets. In Image Processing
(ICIP), 2014 IEEE International Conference on, pp. 343-347. IEEE, 2014.
Edouard Oyallon and StePhane Mallat. Deep roto-translation scattering for object classification. In CVPR,
volume 3, pp. 6, 2015.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via convolutional
sparse coding. The Journal of Machine Learning Research, 18(1):2887-2938, 2017.
O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In British Machine Vision Conference,
2015.
Qiang Qiu, Xiuyuan Cheng, Robert Calderbank, and Guillermo Sapiro. Dcfnet: Deep neural network with
decomposed convolutional filters. arXiv preprint arXiv:1802.04145, 2018.
Roberto Rigamonti, Amos Sironi, Vincent Lepetit, and Pascal Fua. Learning separable filters. In Computer
Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 2754-2761. IEEE, 2013.
Ron Rubinstein, Michael Zibulevsky, and Michael Elad. Double sparsity: Learning sparse dictionaries for
sparse signal approximation. IEEE Transactions on signal processing, 58(3):1553-1564, 2010.
Uwe Schmidt and Stefan Roth. Learning rotation-aware features: From invariant priors to equivariant descrip-
tors. In CVPR, pp. 2050-2057, 2012a. doi: 10.1109/CVPR.2012.6247909. URL http://dx.doi.org/
10.1109/CVPR.2012.6247909.
Uwe Schmidt and Stefan Roth. Learning rotation-aware features: From invariant priors to equivariant descrip-
tors. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 2050-2057. IEEE,
2012b.
Laurent Sifre and Stephane Mallat. Rotation, scaling and deformation invariant scattering for texture discrim-
ination. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 1233-1240.
IEEE, 2013.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
10
Published as a conference paper at ICLR 2019
Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regular-
ization. arXiv preprint arXiv:1511.06067, 2015.
Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation equivariant cnns.
arXiv preprint arXiv:1711.07289, 2017.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic networks:
Deep translation and rotation equivariance. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), volume 2, 2017.
Fa Wu, Peijun Hu, and Dexing Kong. Flip-rotate-pooling convolution and split dropout on convolution neu-
ral networks for image classification. CoRR, abs/1507.08754, 2015. URL http://arxiv.org/abs/
1507.08754.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for
discriminative localization. In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR, 2016.
Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. In 2017 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR), pp. 4961-4970. IEEE, 2017.
11
Published as a conference paper at ICLR 2019
Supplementary Material
A	Flops Calculation in Section 2 and Choice of Hyperparameters
When the input and output are both W × W in space, the forward pass in a regular convolutional
layer needs M0 Mo W 2(1 + 2L2)〜2L2M0 Mo W2 flops. (Each convolution with a L X L filter
takes 2L2W2, and there are M00 M0 convolution operations, plus that the summation over λ0 takes
W2Mo0Mo flops.) In a rotation equivariant CNN without using bases, an convolutional layer would
take 〜2M0MW2L2N2 flops.
In a RotDCF layer, the computation consists of three parts: (1) The inner-product with Wm bases
takes W2M0 ∙ 2NθKa flops. (2) The spatial convolution with the ψk bases takes KaM0K ∙ 2L2 W2
flops. (3) The multiplication with aλ0,λ(k, m)e-im(k)α-imα and summation over λ0, k, m takes
MNθ (4KKaM0 + 2W 2KKaM 0) flops (real-valued version). Putting together, the total is
2M0W2Ka(Nθ + L2K + MNθ K), and when M is large, the third term dominates and it gives
2M0MW2KaKNθ . Thus the reduction by using bases-decomposed filters is again a factor of
LK2 ∙ KNα, and the relative ratio with a regular CNN is about MM ∙ KKLN ∙
The choice of hyperparameters K and Ka can be interpreted as the “frequency” truncation both
in R2 and in S1, and it trades-off between filter regularization and filter expressiveness: The more
severe the truncation, the less many trainable parameters, the smoother the filter (Figure A.1) and
less overfitting, while the potential risk of underfitting is larger. This is revealed in Table 3: as K and
Ka are reduced, the accuracy decreases monotonically in the rotMNIST experiment. When Ka is
reduced to 3 then underfitting becomes noticeable and accuracy drops even more (not reported). The
parameter Nθ corresponds to spatial discretization along S1 and, since we adopt a spectral approach,
it does not affect the number of trainable parameters, which is determined by Ka . In other words,
the choice of Nθ is only constrained by memory storage.
B Proofs in Section 3
B.1 Theorem 3.1
Proof of Theorem 3.1. Note: The bases expansion under ψjl ,k and Wm does not affect the form of
convolutional layers, but only impose regularity of the filters, thus the group-equivariant property of
RotDCF follows from the theorem.
Observe that the equivariant relation (6) is equivalent to that
Tρx(l) [x(l-1)] = x(l) [Tρx(l-1)]
for all l, where Tρx(o) means Dρx(o) .
The sufficiency part: When l = 1, by (1),
(A.1)
x(1)[Dρx(o)](u, α, λ)
σ(X	2x(o)(ρu0,tu + v, λ0)Wλ(10,)λ(Θa-tv)dv + b(1)(λ)),
σ(X	x(o)(ρu0,t(u+v),λ0)Wλ(10,)λ(Θav)dv+b(1)(λ)).
λ0	R2
Since ρu0,t(u + v) = ρu0,t + Θtv, we have that Tρx(1) [x(o)] = x(1) [Dρx(o)] by a change of variable
of Θtv 7→ v .
When l > 1, by (2),
Tρ x(l) [x(l-1)](u, α, λ) = σ(X	x(l-1) (ρu0,tu + v, α0, λ0)Wλ(l0),λ(Θa-tv, α0 - α + t)dvdα0 + b(l) (λ)),
λ0 R2 S 1
x(l) [Tρ x(l-1)](u, α, λ) = σ(X	x(l-1) (ρu0,t(u + v), α0 - t, λ0)Wλ(l0),λ(Θav, α0 - α)dvdα0 + b(l) (λ)).
λ0 R2 S1
12
Published as a conference paper at ICLR 2019
Again, inserting ρu0,t(u + v) = ρu0,t + Θtv, the claim follows by changing variables Θtv 7→ v and
α0 - t 7→ α0 .
The necessity part: When l = 1, denote the general convolutional filter as w(1) (v; λ0, λ, α), and
x(1)(u, α, λ) = σ(X	x(0)(u + v, λ0)w(1) (v; λ0, λ, α)dv + b(1)(λ)).
λ0 R2
Recall that
Tρx(1) [x(0)](u, α, λ) = σ(X	x(0) (ρu0,t u + v, λ0)w(1) (v; λ0, λ, α - t)dv + b(1) (λ)),
λ0	R2
x(1) [Dρ x(0)](u, α, λ) = σ(X	x(0) (ρu0,t (u + v), λ0)w(1) (v; λ0, λ, α)dv + b(1)(λ))
λ0	R2
and then (A.1) with l = 1 holding for any x(0) implies that
X	x(0)(ρu0,tu+v, λ0)w(1) (v; λ0, λ, α-t)dv = X	x(0)(ρu0,t(u+v), λ0)w(1) (v; λ0, λ, α)dv.
By that ρu0,t(u + v) = ρu0,t + Θtv, the above equality gives that
w(1) (v; λ0, λ, α - t) = w(1) (Θt-1 v; λ0, λ, α), ∀α, t ∈ S1.
Let t = α, and Fλ0,λ(v) = w(1) (v; λ0, λ, 0), we have that
w(1) (Θα-1v; λ0, λ, α) = Fλ0,λ (v),
and this gives that w(1)(v; λ0, λ, α) = Fλ0,λ(Θαv). This proves that (1) is necessary.
When l > 1, consider the general convolutional filter as w(l) (v; λ0, λ, α0, α). Using a similar argu-
ment, (A.1) implies that
w(l) (v; λ0, λ, α0, α - t) = w(l) (Θt-1v; λ0, λ, α0 + t, α), ∀α, t ∈ S1.
Let t = α, and Fλ0,λ(v, α0) = w(l) (v; λ0, λ, α0, 0), then
Fλ0,λ(v, α0) = w(l)(Θα-1v; λ0, λ, α0 + α, α),
which gives that w(l) (v; λ0, λ, α0, α) = Fλ0,λ (Θαv, α0 - α), which proves that (2) is necessary.
□
B.2 Proposition 3.2
Proposition B.1. For all l,
Bλ(l0),λ, Cλ(l0),λ,2jlDλ(l0),λ ≤ πka(λl0),λkFB,
where
Bλlo)λ := / 1 ∖W(0)λ(v,β)∖dvdβ, l >1, Bλ0)λ := 1 IWλ(1λ(V)Idv
,	R2 S1	,	,	R2	,
Cλl0)λ :=/ I ∖v∖∖VvW^λ(v,β)∖dvdβ, l> 1,	Cλ0)λ =I ∖v∖∖VνWλ1)λ(v)∖dv	(A.2)
R2 S1	R2
Dλlo)λ :=/ [ MWλθ)λ(v,β)∖dvdβ, l> 1, Dλ1)λ =I ∖VvWλ1 λ(v)∖dv
,	R2 S1	,	,	R2	,
As a result,
Bl, Cl, 2jlDl ≤ Al,
13
Published as a conference paper at ICLR 2019
where
Ml-1
Bl := max{sup	Bλ(l0)λ ,
λ λ0=1	,
Ml-1
Cl := max{sup X Cλ(l0)λ,
λ λ0=1	,
Ml-1
Dl := max{sup	Dλ(l0)λ,
λ λ0=1	,
Ml
sup * X BQ，
Ml
sup MMIX Cλl0),λ},
Ml
sup Mw X Dλl0),λ},
(A.3)
and thus (A2) implies that Bl, Cl, 2jl Dl ≤ 1 for all l.
Proof of Proposition B.1. The proof for the case of l = 1 is the same as Lemma 3.5 and Proposition
3.6 of Qiu et al. (2018). We reproduce it for completeness. When l = 1, it suffices to show that for
F(v) = Pkakψk(v),
/ IF(V)|dv, / |v||VF(v)|dv, / VF(V)Idv ≤ π(X μk ak)1/2.
k
(A.4)
Rescaling to ψjl,k in V leads to the desired inequality with the factor of 2jl for Dλ(l0),λ. To prove
(A.4), observe that F is supported on the unit disk, and then kFk1, R IVIIVF(V)IdV ≤ kVF k1 ≤
√∏∣∣VF∣∣2, where ∣∣VF∣∣2 = π Pk μkak due to the orthogonality of ψk.
For l > 1, similarly, we only consider the rescaled filters supported on the unit disk in V. Let
F(v, β) = Pkm ak,mψk(v)夕m(β), β ∈ S1, similarly as above, We have that
F(V, β)IdVdβ,
IVIIVvF (V, β)IdVdβ ≤
IVv F (V, β)IdVdβ
≤(π
|Vv F (v,β)∣2dvdβ )1/2
recalling that J dβ on S1 has the normalization of 2∏. Again, J /IVv F (v,β Ndvde
π Pkm μkak m due to the orthogonality of ψk and 夕m. This proves that
IVvF (v, β)I
dvdβ ≤ n(£μka⅛,m)1/2,
k,m
which leads to the claim after a rescaling ofv.
□
Remark 1 (Remark to Proposition 3.2). The proposition only needs Bl, defined in (A.3), to be less
than 1 for all l, in a rotation-equivariant CNN, which is implied by (A2) by Proposition B.1.
Proof of Proposition 3.2. The proof is similar to that of Proposition 3.1(a) of Qiu et al. (2018).
Specifically, in (a), the argument is the same for l = 1, making use of the fact that
Iw(Θαv)Idv
Iw(v)Idv,
∀α ∈ S 1
and JSI dα = 1 due to the normalization of 2∏. For l > 1, the same technique proceeds with
the new definition of 8心)入 as in (A.2) which involves the integration of RSI (•…)dβ. The detail is
omitted.
14
Published as a conference paper at ICLR 2019
To prove (b), we firstly verify that x(0l) only depends on λ. When l = 1, x(01) (u, α, λ) = σ(b(1) (λ)).
Suppose that it holds for (l - 1), consider l > 1,
x(0l) (u, α, λ) = σ(X	x(0l-1) (u + v, α + β, λ0)Wλ(l0)λ(Θαv, β)dvdβ + b(l) (λ))
λ0 S1 R2
= σ(Xx(0l-1)(λ0)	Wλ(l0),λ(Θαv, β)dvdβ + b(l)(λ))
λ0	S 1 R2
σ(Xx0j)(λ0) ∙
λ0
/ /
S1	R2
Wλ(l0),λ(v0, β)dv0dβ + b(l)(λ))
x(0l)(λ).
Thus x(0l)(u, α, λ) = x(0l)(λ) for all l (without index α for l = 1). The rest of the argument follows
from that ∣∣χCl)k = kx(l) — χ0l)k = kx(l)[x(l-1)] — x(l)[χ0l-1)]k ≤ ∣∣χ(l-1) — χ0l-1)k = kχCl-1)k,
where the inequality is by (a).	□
B.3	Theorem 3.3
Proposition B.2. In a RotDCF Net, under (A1), (A2), (A3), c1 = 4, for any l,
Ilx(I)DP ◦ DTx(0)] — TP ◦ DTx(l)[x⑼]∣∣≤ 2cιl∣Vτ∣∞∣∣χ⑼∣∣,
where Dτ only acts on the space variable u of x(l) similar to (9).
Proof of Proposition B.2. We firstly establish that for all l,
kx(I)[Tρ ◦ DTX(IT)] — TP ◦ DTx(l)[x(lτ)]k ≤ 2cι∣Vτ∣∞kxcl-1)k,	(A.5)
where TP is replaced by DP if applies to x(0) which does not have index α. This is because that
x(l)[TP ◦ DTx(l-1)] = TPx(l) [DTx(l-1)]
by Theorem 3.1, and that
∣TPx(l)[DT x(l-1)] —TP ◦ DTx(l) [x(l-1)]∣ = ∣x(l)[DTx(l-1)] — DTx(l) [x(l-1)]∣
by the definition ofTP (a rigid rotation in u, and a translation in α). This term can be upper bounded
by c1(B1 + Cι)∣Vτ∣∞kxCl-1)k (Lemma B.3), which leads to the desired bound under (A2) by
Proposition B.1.
The rest of the proof is similar to that of Proposition 3.3 of Qiu et al. (2018): Write x(l) [DP ◦
DT x(0)] — TP ◦ DT x(l) [x(0)] as the sum of the differences x(l) [x(j) [DP ◦ DTx(j-1)]] — x(l) [TP ◦
DTx(j)[x(j-1)]] for j = 1,…，l. The norm of the j-th term is bounded by ∣∣x(j)[Dρ ◦ DTx(j-1)] 一
TP ◦ DTx(j) [x(j-1)]∣ due to Proposition 3.2 (a), which, by applying (A.5) together with Proposition
3.2 (b), can be bounded by 2cι ∣Vτ∣∞∣x(O) ∣∣. Summing over j gives the claim.	□
Proof of Theorem 3.3. The proof is similar to that of Theorem 3.8 of Qiu et al. (2018). With the
bound in Proposition B.2, it suffices to show that
∣Tρ ◦ DTX(L) [x(0)] — TPx(L) [x(0)] k ≤ c22-jL ∣τ∣∞kx(O) k.
By the definition ofTP, the l.h.s. equals ∣DT x(L) [x(0)] — x(L) [x(0)]∣, which can be shown to be less
than c2 ∣τ∣∞Dl kxCl-I) k by extending the proof ofProposition 3.4 of QiU et al. (2018), similar to the
argument in proving Lemma B.3. The desired bound then follows by that DL ≤ 2-jL AL ≤ 2-jL
(Proposition B.1 and (A2)) and that kxCl-1) k ≤ kx(O) k (Proposition 3.2 (b)).	□
Lemma B.3. In a rotation-equivariant CNN, Bl, Cl defined as in (A.3), under (A1), (A3), for all
l > 0, with c1 = 4, x(cl) as in Proposition 3.2,
kx(I)DTx(l-1)] — DTx(l)[x(lτ)]k ≤ cι(Bl + Cl)∣Vτ∣∞kxCl-1)k.
15
Published as a conference paper at ICLR 2019
Conv-3 CNN-M
c5x5x1xM ReLu ap2x2
c5x5xM x2M ReLu ap2x2
c5x5x2M x4M ReLu ap2x2
fc64 ReLu fc10 softmax-loss
Conv-3 RotDCF-M
rc5x5x1xM ReLu ap2x2
rc5x5xNθ xM x2M ReLu ap2x2
rc5x5xNθ x2M x4M ReLu ap2x2
fc64 ReLu fc10 softmax-loss
Table A.1: Conv-3 network architectures used in rotMNIST, M = 32, 16 or 8. cLxLxM 0 xM stands for a
convolutional layer of patch size LxL and input (output) channel M0 (M). apLxL stands for LxL average-
pooling. In the RotDCF Net, rcLxLxNθ xM 0 xM stands for a rotation-indexed convolutional layer, which
includes Nθ -times many number of filters except for the 1st rc layer (see Section 2). Batch-normalization
layers (not shown) are used during training.
ConV-6 CNN-M		ConV-6 RotDCF-M	
c5x5x1xM ReLu c5x5xMx1.5M ReLu ap2x2 c5x5x1.5Mx2M ReLu c5x5x2Mx2M ReLu ap2x2 c5x5x2Mx3M ReLu c5x5x3Mx4M ReLu ap2x2 fc64 ReLu fc10 softmax-loss	rc5x5x1xM ReLu rc5x5xNθ xM x1.5M ReLu ap2x2 rc5x5xNθ x1.5M x2M ReLu rc5x5xNθ x2M x2M ReLu ap2x2 rc5x5xNθ x2M x3M ReLu rc5x5xNθ x3M x4M ReLu ap2x2 fc64 ReLu fc10 softmax-loss
Table A.2: Conv-6 network architectures used in rotMNIST. Similar to Table A.1 with 3 more layers.
Proof of Lemma B.3. The proof is similar to that of Lemma 3.2 of Qiu et al. (2018). Specif-
ically, when l = 1, the argument is the same, making use of the fact that / ∣w(θav)∣dv =
/ ∣w(v)∣dv, ∀a ∈ S1 and 卜 dα = 1 due to the normalization of 2∏. When l > 1, the same
technique applies by considering the joint integration of 品 JSI (…)dvdβ instead of just dv. The
only difference is in using the new definitions of Bλ(l0),λ and Cλ(l0),λ for l > 1 as in (A.2), both of which
involve the integration of JSI (•…)dβ. The detail is omitted.	□
C Experimental Details in Section 4
C.1 Object recognition with rotMNIST and CIFAR 1 0
In the experiments on rotMNIST dataset, the network architecture for Conv-3 and Conv-6 nets are
shown in Table A.1 and Table A.2. Stochastic gradient descent (SGD) with momentum is used to
train 100 epochs with decreasing learning rate from 10-2 to 10-4.
In the experiments on CIFAR10 dataset, the VGG16-like network architecture is shown in Table
A.4. SGD with momentum is used to train 100 epochs with decreasing learning rate from 10-2 to
10-4.
rotMNIST COnV-3, Ntr = 10K
	Test Acc.	# param.	Ratio
CNN M=32	-95.67	2.570×105	1.00
DCF M =32, K=5	95.58	5.158× 104	0.20	rotMNIST Conv-3, Ntr = 5K
DCF M =32, K=3	95.69	3.104×104	0.12	Test Acc.~ #param. Ratio
RotDCF Nθ = 8			CNN M=32	94.04
M =16, K=14, Kα=8	97.86	2.871 ×105	1.12	DCF M=32, K=3	94.08
M =16, K=5, Kα=8	97.81	1.026×105	0.40	RotDCF N =8
M =16, K=3, Kα=8	97.77	6.160×104	0.24	M=16, K=3, Kα=5	96.79	(same as left)
M =16, K=5, Kα=5	97.96	6.419×104	0.25	M=8, K=3, Kθ=5	96.53	
M =16, K=3, Kα=5	97.95	3.856×104	0.15
M=8, K=5, Kα=5	97.81	1.610×104	0.06
M=8, K=3, Kα=5	97.59	9.680×103	0.04
Table A.3: Classification accuracy using non-equivariant CNN and rotation-equivariant CNNs on rotMNIST
based on Conv-3, c.f. Table A.1. Similar results to the rotmnist experiments in Table 3 with a net of fewer
(three) layers.
16
Published as a conference paper at ICLR 2019
VGG-16 CNN-M	VGG-16RotDCF-M	
C3x3x3xM ReLu C3x3xM xM ReLu C3x3xM xM ReLu C3x3xM xM ReLu C3x3xM xM ReLu mp2x2 C3x3xM x2M ReLu C3x3x2Mx2M ReLu C3x3x2M x2M ReLu C3x3x2M x2M ReLu mp2x2 C3x3x2M x4M ReLu C3x3x4M x4M ReLu C3x3x4M x4M ReLu C3x3x4M x4M ReLu mp2x2 fC128 ReLu fC10 softmax-loss	rc3x3x3xM ReLurC3x3xNgxMxM ReLu rc3x3xNgxMxM ReLu rc3x3xNθXMXM ReLu rc3x3xNθXMXM ReLu mp2x2 rc3x3xNθXMx2M ReLu rc3x3xNθx2Mx2M ReLu rc3x3xNθx2Mx2M ReLUrC3x3xNθx2Mx2M ReLu mp2x2 rc3x3xNθx2Mx4M ReLUrC3x3xNgx4Mx4M ReLu rc3x3xNθx4Mx4M ReLUrC3x3xNgx4Mx4M ReLu mp2x2 fc128 ReLu fc10 softmax-loss
Table A.4: VGG-16-like network architectures used in CIFAR10, M = 64 or 32. mpLxL stands
for LxL max-pooling, and other notations similar to Table A.1.
RotDCF ConvAE
rc5x5x1x8 ReLu ap2x2
rc5x5xNθ x8x16 ReLu ap2x2
rc5x5xNθ x16x32 ReLu ap2x2
rc5x5xNθx32x32 ReLu	— Encoded representation
fc128 ReLu ct5x5x128x16Nθ ReLu
ct5x5x16Nθ x8Nθ (upsample 2x2) ReLu
ct5x5x8NθXl (upsample 2x2) EuCledian-loss
Table A.5: Convolutional Auto-encoder network used in the image reconstruction experiment. Rot-
DCF layers are used in the enCoder network, with Nθ = 16, K = 5, Kα = 5 (K = 8, Kα = 15 in
the last RotDCF layer), and transposed-Convolutional layers with upsampling are used in the deCoder
net. apLxL stands for LxL average-pooling, and other notations similar to Table A.1.
C.2 Convolutional Auto-encoder for image reconstruction
The network arChiteCture is shown in Table A.5. The network is trained on 50,000 training samples,
the training set is augmented by rotating eaCh sample at 8 random angles, produCing 400k training
set. The network is trained for 10 epoChs, where the learning rate deCreases from 10-3 to 10-6.
C.3 Face recognition on Facescrub
To faCilitate the evaluation on both known and unknown subjeCts, we seleCt the first 500 of the 530
identities as our training subjeCts. The remaining 30 subjeCts are used for validating out of sample
performanCe, namely the unknown subjeCts. The experiment on unknown subjeCts is CritiCal for faCe
models to generate over unseen people. For both known and unknown subjeCts, we hold 10 images
from eaCh person as the probe images, and the remaining as the gallery images. The images are
preproCessed by aligning faCial landmarks using Kazemi & Sullivan (2014). and Crop the aligned
faCe images to 112 × 112 with Color. Thus, both our CNN and RotDCF models are trained with
near-frontal and upright-only faCe images.
The network arChiteCture is shown in Table A.6. ACCording to the formula in SeCtion 2, the number
of trainable parameters in the RotDCF Net is about (2 )2 ∙ L ∙ Ka = 4 of that of the CNN.
C.4 Effects of Regularization by Filter Decomposition
Sample trained filters in the reConstruCtion experiment are shown in Figure A.1, whiCh shows that
RotDCF obtains smoother trained filters as a result of the regularization applied by the bases trunCa-
tion. In the reConstruCtion experiment, inCreasing the number of bases degrades the synthesis (Right
CNN
C5x5x3x32 ReLu mp2x2
C5x5x32x64 ReLu mp2x2
C5x5x64x128 ReLu C5x5x128x128 ReLu mp2x2
C5x5x128x256 ReLu C5x5x256x256 ReLu mp2x2
C5x5x256x256 ReLu C5x5x256x256 ReLu gap13x13
fC softmax
ROtDCF
rc5x5x3x16 ReLu mp2x2
rC5x5xNθ x16x32 ReLu mp2x2
rC5x5xNθ x32x64 ReLu C5x5x64x64 ReLu mp2x2
rC5x5xNθ x64x128 ReLu C5x5x128x128 ReLu mp2x2
rC5x5xNθ x128x128 ReLu C5x5x128x128 ReLu gap13x13
fC softmax
Table A.6: Network arChiteCtures used in faCe experiments, notations as in Table A.1. In the
RotDCF Net, Nθ = 8, K = 5, Kα = 5.
17
Published as a conference paper at ICLR 2019
in Figure A.1). Such improved stability due to the truncated decomposition supports the analysis in
Section 3.


Figure A.1: Trained filters in RotDCF (Left), and in ordinary rotation-equivariant CNNs (Middle) without
truncated decomposition. (Right) Reconstructed digits from row-rotated codes where the AE net is trained with
larger K and Kα , showing more overfitting than Figure 3 in the paper.
18