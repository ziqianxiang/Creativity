Published as a conference paper at ICLR 2019
Sample Efficient Imitation Learning for Con-
tinuous Control
Fumihiro Sasaki, Tetsuya Yohira & Atsuo Kawaguchi
Ricoh Company, Ltd.
{fumihiro.fs.sasaki,tetsuya.yohira,atsuo.kawaguchi}@jp.ricoh.com
Ab stract
The goal of imitation learning (IL) is to enable a learner to imitate expert behavior
given expert demonstrations. Recently, generative adversarial imitation learning
(GAIL) has shown significant progress on IL for complex continuous tasks. How-
ever, GAIL and its extensions require a large number of environment interactions
during training. In real-world environments, the more an IL method requires the
learner to interact with the environment for better imitation, the more training time
it requires, and the more damage it causes to the environments and the learner
itself. We believe that IL algorithms could be more applicable to real-world prob-
lems if the number of interactions could be reduced. In this paper, we propose a
model-free IL algorithm for continuous control. Our algorithm is made up mainly
three changes to the existing adversarial imitation learning (AIL) methods - (a)
adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner pol-
icy, (b) estimating the state-action value using off-policy samples without learn-
ing reward functions, and (c) representing the stochastic policy function so that
its outputs are bounded. Experimental results show that our algorithm achieves
competitive results with GAIL while significantly reducing the environment inter-
actions.
1	Introduction
Recent advances in reinforcement learning (RL) have achieved super-human performance on several
domains (Mnih et al., 2015; Silver et al., 2016; Mnih et al., 2016; Lillicrap et al., 2015). On most of
such domains with the success of RL, the design of reward, that explains what agent’s behavior is
favorable, is obvious for humans. Conversely, on domains where it is unclear how to design the
reward, agents trained by RL algorithms often obtain poor policies and behave worse than what we
expect them to do. Imitation learning (IL) comes in such cases. The goal of IL is to enable the
learner to imitate expert behavior given the expert demonstrations without the reward signal. We
are interested in IL because we desire an algorithm that can be applied to real-world problems for
which it is often hard to design the reward. In addition, since it is generally hard to model a variety
of real-world environments with an algorithm, and the state-action pairs in a vast majority of real-
world applications such as robotics control can be naturally represented in continuous spaces, we
focus on model-free IL for continuous control.
A wide variety of IL methods have been proposed in the last few decades. The simplest IL method
among those is behavioral cloning (BC) (Pomerleau, 1991) which learns an expert policy in a su-
pervised fashion without environment interactions during training. BC can be the first IL option
when enough demonstration is available. However, when only a limited number of demonstrations
are available, BC often fails to imitate the expert behavior because of the problem which is referred
to compounding error (Ross & Bagnell, 2010) - inaccuracies compound over time and can lead the
learner to encounter unseen states in the expert demonstrations. Since it is often hard to obtain
a large number of demonstrations in real-world environments, BC is often not the best choice for
real-world IL scenarios.
Another widely used approach, which overcomes the compounding error problem, is Inverse Rein-
forcement Learning (IRL) (Russell, 1998; Ng & Russell, 2000; Abbeel & Ng, 2004; Ziebart et al.,
2008). Recently, Ho & Ermon (2016) have proposed generative adversarial imitation learning
1
Published as a conference paper at ICLR 2019
(GAIL) which is based on prior IRL works. Since GAIL has achieved state-of-the-art performance
on a variety of continuous control tasks, the adversarial IL (AIL) framework has become a popu-
lar choice for IL (Baram et al., 2017; Hausman et al., 2017; Li et al., 2017). It is known that the
AIL methods are more sample efficient than BC in terms of the expert demonstration. However,
as pointed out by Ho & Ermon (2016), the existing AIL methods have sample complexity in terms
of the environment interaction. That is, even if enough demonstration is given by the expert be-
fore training the learner, the AIL methods require a large number of state-action pairs obtained
through the interaction between the learner and the environment1. The sample complexity keeps
existing AIL from being employed to real-world applications for two reasons. First, the more an
AIL method requires the interactions, the more training time it requires. Second, even if the expert
safely demonstrated, the learner may have policies that damage the environments and the learner it-
self during training. Hence, the more it performs the interactions, the more it raises the possibility of
getting damaged. For the real-world applications, we desire algorithms that can reduce the number
of interactions while keeping the imitation capability satisfied as well as the existing AIL methods
do.
The following three properties of the existing AIL methods which may cause the sample complexity
in terms of the environment interactions:
(a)	Adopting on-policy RL methods which fundamentally have sample complexity in terms of
the environment interactions.
(b)	Alternating three optimization processes — learning reward functions, value estimation
with learned reward functions, and RL to update the learner policy using the estimated
value. In general, as the number of parameterized functions which are related to each other
increases, the training progress may be unstable or slower, and thus more interactions may
be performed during training.
(c)	Adopting Gaussian policy as the learner’s stochastic policy, which has infinite support on
a continuous action space. In common IL settings, we observe action space of the expert
policy from the demonstration where the expert action can take on values within a bounded
(finite) interval. As Chou & Scherer. (2017) suggests, the policy which can select actions
outside the bound may slow down the training progress and make the problem harder to
solve, and thus more interactions may be performed during training.
In this paper, we propose an IL algorithm for continuous control to improve the sample complexity
of the existing AIL methods. Our algorithm is made up mainly three changes to the existing AIL
methods as follows:
(a)	Adopting off-policy actor-critic (Off-PAC) algorithm (Degris et al., 2012) to optimize the
learner policy instead of on-policy RL algorithms. Off-policy learning is commonly known
as the promising approach to improve the complexity.
(b)	Estimating the state-action value using off-policy samples without learning reward func-
tions instead of using on-policy samples with the learned reward functions. Omitting the
reward learning reduces functions to be optimized. It is expected to make training progress
stable and faster and thus reduce the number of interactions during training.
(c)	Representing the stochastic policy function of which outputs are bounded instead of adopt-
ing Gaussian policy. Bounding action values may make the problem easier to solve and
make the training faster, and thus reduce the number of interactions during training.
Experimental results show that our algorithm enables the learner to imitate the expert behavior as
well as GAIL does while significantly reducing the environment interactions. Ablation experimental
results show that (a) adopting the off-policy scheme requires about 100 times fewer environment
interactions to imitate the expert behavior than the one on-policy IL algorithms require, (b) omitting
the reward learning makes the training stable and faster, and (c) bounding action values makes the
training faster.
1Throughout this paper, we refer to “number of interactions“ as the number of state-action pairs obtained
through interaction between the learner and the environment during training the learner.
2
Published as a conference paper at ICLR 2019
2	Background
2.1	Preliminaries
We consider a Markov Decision Process (MDP) which is defined as a tuple {S, A, T, R, d0 , γ},
where S is a set of states, A is a set of possible actions agents can take, T : S ×A×S → [0, 1] is a
transition probability, R : S×A → R is a reward function, d0 : S → [0, 1] is a distribution over ini-
tial states, and γ ∈ [0, 1) is a discount factor. The agent’s behavior is defined by a stochastic policy
π : S×A → [0, 1] and Π denotes a set of the stochastic policies. We denote SE ⊂ S and AE ⊂ A
as sets of states and actions observed in the expert demonstration, and Sπ ⊂ S and Aπ ⊂ A as
sets of those observed in rollouts following a policy π. We will use πE , πθ , β ∈ Π to refer to the
expert policy, the learner policy parameterized by θ, and a behavior policy, respectively. Given a
policy π, performance measure of π is defined as J(π, R) = E[E∞=0 YtR(st, at)∣do, T, ∏] where
st ∈ S is a state that the agent receives at discrete time-step t, and at ∈ A is an action taken by
the agent after receiving st. The performance measure indicates expectation of the discounted re-
turn E∞=o γtR(st, at) When the agent follows the policy π in the MDP. Using discounted state
visitation distribution denoted by ρ∏(S) = E∞=0 YtP(St = s|do, T,∏) where P is a Probabil-
ity that the agent receives the state s at time-step t, the performance measure can be rewritten as
J(∏, R) = Es~ρπ ,a~∏ [R(s, a)]. The state-action value function for the agent following ∏ is defined
as Qn(st,at) = E[EU∞=t Y"-tR(su,au)∣T,∏], and Q∏,ν denotes its approximator parameterized
by ν.
2.2	Adversarial Imitation Learning
We briefly describe objectives of RL, IRL, and AIL below. We refer the readers to Ho & Ermon
(2016) for details. The goal of RL is to find an optimal policy that maximizes the performance
measure. Given the reward function R, the objective of RL with parameterized stochastic policies
πθ : S ×A → [0, 1] is defined as follows:
RL(R) = arg maxθ J(πθ, R)	(1)
The goal of IRL is to find a reward function based on an assumption that the discounted returns
earned by the expert behavior are greater than or equal to those earned by any non-experts behavior.
Technically, the objective of IRL is to find reward functions Rω : S × A → R parameterized by
ω that satisfies J(πE, Rω) ≥ J(π, Rω ) where π denotes the non-expert policy. The existing AIL
methods adopt max-margin IRL (Abbeel & Ng, 2004) of which objective can be defined as follows:
IRL(πE) = arg maxω J(πE,Rω) - J(π,Rω)	(2)
The objective of AIL can be defined as a composition of the objectives (1) and (2) as follows:
AIL(πE) = arg minθ arg maxω J(πE, Rω) - J(πθ, Rω)	(3)
2.3	Off-Policy Actor-Critic
The objective of Off-PAC to train the learner can be described as follows:
argmaxθ 旧§~。3,a~∏g [ Q∏θ,ν(s, a)]	(4)
The learner policy is updated by taking the gradient of the state-action value. Degris et al. (2012)
proposed the gradient as follows:
Es~Pβ ,a~∏θ [Q∏θ,V(s,a)Vθ log ∏θ(a|s)]	(5)
Heess et al. (2015) provided another formula of the gradient using “re-parameterization trick“ in
the case that the learner policy selects the action as a = ∏θ (s,z) with random variables Z 〜Pz
generated by a distribution Pz :
Es~Pβ ,z~Pz [VaQ∏θ ,ν (s, a)lα=∏θ(s,z)Vθ ∏ (s, z)	⑹
3	Algorithm
As mentioned in Section.1, our algorithm (a) adopts Off-PAC algorithms to train the learner policy,
(b) estimates state-action value without learning the reward functions, and (c) represents the stochas-
tic policy function so that its outputs are bounded. In this section, we first introduce (b) in 3.1 and
describe (c) in 3.2, then present how to incorporate (b) and (c) into (a) in 3.3.
3
Published as a conference paper at ICLR 2019
3.1	Value Estimation without Reward Learning
In this subsection, we introduce a new IRL objective to learn the reward function in 3.1.1 and a
new objective to learn the value function approximator in 3.1.2. Then, we show that combining
those objectives derives a novel objective to learn the value function approximator without reward
learning in 3.1.3.
3.1.1	Reward Learning
We define the parameterized reward function as Rω (s, a) = log rω (s, a), with a function rω :
S×A → [0, 1] parameterized by ω. rω(s, a) represents a probability that the state-action pairs (s, a)
belong to SE × AE. In other words, rω (s, a) explains how likely the expert executes the action a
at the state s. With this reward, we can also define a Bernoulli distribution pω : Π×S ×A → [0, 1]
such that pω(∏e|s, a) = rω(s, a) for the expert policy ∏e and pω(n|s, a) = 1 - rω(s, a) for any
other policies ∏ ∈ Π ∖{∏e } which include ∏ and β. A nice property of this definition of the reward
is that the discounted return for a trajectory {(s0, a0), (s1, a1), ...} can be written as a log likelihood
with Pω (∏e |st ,at )：
∞	∞∞
EYtR3 (St ,at) = logɪɪ rt (st, at) = log ∏ PE (∏e ∣st,at)	⑺
t=0	t=0	t=0
Here, We assume Markov property in terms of pω such that pω(∏e|st, at) for t ≥ 1 is independent
of pω (∏e ∣st-u, at-u) for U ∈ {1,…,t}. Under this assumption, the return naturally represents how
likely a trajectory is the one the expert demonstrated. The discount factor γ plays a role to make
sure the return is finite as in standard RL.
The IRL objective (2) can be said to aim at assigning r3 = 1 for state-action pairs (s, a) ∈ SE × AE
and r3 = 0 for (s, a) ∈ Sπ × Aπ when the same definition of the reward R3 (s, a) = log r3 (s, a)
is used. Following this fashion easily leads to a problem where the return earned by the non-expert
policy becomes -∞, since log rω (s,a) = -∞ if r3 (s,a) = 0 and thus log ∏∞=o r3 (st,at) = -∞
for (s, a) ∈ Sπ × Aπ . The existing AIL methods seem to mitigate this problem by trust region
optimization for parameterized value function approximator (Schulman et al., 2015b), and it works
somehow. However, we think this problem should be got rid of in a fundamental way. We propose
a different approach to evaluate state-action pairs (s, a) ∈ Sπ × Aπ. Intuitively, the learner does not
know how the expert behaves in the states s ∈ S \ SE — that is, it is uncertain which actions the
expert executes in the states the expert has not visited. We thereby define a new IRL objective as
follows：
arg maxω Es〜ρ∏E ,a〜∏e [pω (πE |s,a)] + Es〜ρ∏ ,a〜π [H(Pω ds,a))]	(8)
where H denotes entropy of Bernoulli distribution such that：
H(pω(∙∣s,a)) = -Pω(∏e∣s,a)logPω(∏e|s,a) -Pω(∏∣s,a)logPω(∏∣s,a)	(9)
Unlike the existing AIL methods, our IRL objective is to assign pω(∏e|s, a) = pω(π∣s, a) = 0.5
for (s, a) ∈ Sπ × Aπ. This uncertainty P3 (πE |s, a) = 0.5 explicitly makes the return earned by the
non-expert policy finite. On the other hand, the objective is to assign r3 = 1 for (s, a) ∈ SE × AE
as do the existing AIL methods. The optimal solution for the objective (8) satisfies the assumption
of IRL ： J (πE,R3) ≥ J (π,R3), even though the objective does not aim at discriminating between
(s, a) ∈ SE × AE and (s, a) ∈ Sπ × Aπ ,
3.1.2	Value Function Estimation
As we see in Equation (7), the discounted return can be represented as a log likelihood. Therefore, a
value function approximator Qπθ following the learner policy πθ can be formed as a log probability.
We introduce a function qπθ,ν : S×A → [0, 1] parameterized by ν to represent the approximator
Qπθ ,ν as follows：
Qπθ ,ν (st, at) = log qπθ ,ν (st, at)	(10)
The optimal value function following a policy π satisfies the Bellman equation Qπ (st, at) =
R(st,at) + γEst+ι〜T,at+1 〜∏ [Q∏(st+1,at+1)]. Substituting ∏ for π, log r3(st, at) for R(st, at),
4
Published as a conference paper at ICLR 2019
and log qπθ,ν (st , at) for Qπ (st , at), the Bellman equation for the learner policy πθ can be written
as follows:
log q∏θ ,ν (st, at) = Est+ι~T ,at+ι~∏θ [log rω (st, at)q∏θ ,ν(St+1, at+1)]	(II)
We introduce additional Bernoulli distributions PV : ∏ ×S×A :→ [0,1] and Pωνγ ： ∏ ×S×A×
SXA 1 [0,1] as follows:
PV (πist,at)={iπ-⅛πE∣st,at)
if π = πE
otherwise
rω (st, at)qπγθ V(st+1, at+1)	if π = πE
Pωνγ(πlst,at,st+1,at+1) = ∖ . ŋ , θ,,	ʌ ,,
11 — Pωνγ (∏E Ist,at,st+I,at+1) otherwise
Using PV and PωVγ, the loss to satisfy Equation (11) can be rewritten as follows:
L(ω, ν, θ) = Est+1 〜T ,at+ι 〜∏θ [log Pωνγ (∏E |st, at, st+1, at+1)] 一 log PV (∏E |st, at)
/ ∣	Est+ι~T,at+ι~∏θ [PLνγ(πE 1st, at, st+1, at+1)]
≤ og	PV(∏E|st, at)
(12)
(13)
(14)
We use Jensen’s inequality with the concave property of logarithm in Equation (14). Now we see that
the loss L(ω, ν, θ) is bounded by the log likelihood ratio between the two Bernoulli distributions PV
and Pωνγ, and L(ω,ν,θ) = 0 if PV(∏e∣st,at) = Est+ι〜T,at+ι〜∏θ [Rνγ(∏e|st,at,st+ι,at+ι)].
In the end, learning the approximator Qπθ,V turns out to be matching the two Bernoulli distributions.
A natural way to measure the difference between two probability distributions is divergence. We
choose Jensen-Shannon (JS) divergence to measure the difference because we empirically found it
works better, and thereby the objective to optimize Qπθ ,V can be written as follows:
arg min” E DJS (PV(' |st, at) Il Est+ι〜T,at+ι〜∏θ [PL")('|st, at, st+1, at+1)]
(15)
where DJS denotes JS divergence between two Bernoulli distributions.
3.1.3	Value Estimation without Reward Learning
Suppose the optimal reward function Rω* (s,a) = log u (s, a) for the objective (8) can be obtained,
the Bellman equation (11) can be rewritten as follows:
log rω* (st, at) = log q∏θ ,v (st, at) 一 Est+1 〜T,at+1 〜∏θ [log q∏θ ,“(st+L at+1 )]	(16)
Recall that IRL objective	(8)	aims at	assigning rω* (st, at) = 1 for (st, at)	∈	SE × AE and
rω* (st, at) = 0.5 for (st,	at)	∈ Sπ ×	Aπ where π ∈ Π \ {πE}. Therefore,	the	objective (8) is
rewritten as the following objective using the Bellman equation (11) :
argminV Est 〜PnE ,at 〜∏E log q∏θ,V (st, at) 一 Est+1 〜T,at+1 〜∏θ [log 9∏θ,v (st+1, at+1)]
(17)
+ Est~Pn ,at~∏ log qπθ,V(st, at) 一 Es
t+ι~T,at+ι~∏θ [log {q∏θ,v(St+1,at+1)/2}]
Thus, rω* can be obtained by the Bellman equation (16) as long as the solution for the objective (17)
can be obtained. We optimize qπθ,V(st, at) in the same way of objective (15) as follows:
argminV Est 〜PnE ,at 〜∏E
+ Est
〜Pn,at〜π
DJS
DJS
∙∣st,at) Il Est+ι~T,at+1
〜∏θ
[Pγ (Ist+1,at+1)])
(PV (Ist ,at) Il Est+ι~T,at+ι
〜∏θ
[PVγ (∙∣
st+1, at+1
(18)
We use PVγ instead of PωVγ in objective (18) unlike the objective (15). Thus, we omit reward learning
that the existing AIL methods require, while learning qπθ,V(st, at) to obtain rω* .
5
Published as a conference paper at ICLR 2019
UIIbC)Unded	bounded
(a) Gaussian policy representation
(b) Proposed policy representation
Figure 1: Functional representations of (a) Gaussian policy and (b) proposed policy.
Algorithm 1 Overview of our IL algorithm
1:	Initialize parameters ν and θ.
2:	Fulfill a buffer BnE by the expert demonstrations and initialize the replay buffer Be —0.
3:	for episode = 1, M do
4:	Initialize time-step t = 0 and receive initial state s0
5:	while not terminate condition do
6:	Execute an action at = ∏θ(st, z) with Z 〜 Pz and observe new state st+ι
7:	Store a state-action triplet (st, at, st+1) in Bβ.
8:	t = t + 1
9:	end while
10:	for u = 1, t do
11:	Sample mini-batches of triplets (s7t', ai', si'+ι) and (Sj', aj', sj'+ι) from BnE and Be, respectively.
12:	Update V using the sampled gradients of (18) w.r.t V using (stt',a；’, stt'+ι) and (Sj',aj',st'+J.
13:	Sample a mini-batch of triplets (sk', ak', sk'+i) from Be.
14:	Update θ using the sampled policy gradients (6) using (sk', ak', sk'+ι).
15:	end for
16:	end for
3.2 Stochastic Policy Function with Bounded Outputs
Recall that the aim of IL is to imitate the expert behavior. It can be summarized that IL attempts to
obtain a generative model the expert has over A conditioned on states in S . We see that the aim it-
self is equivalent to that of conditional generative adversarial networks (cGANs) (Mirza & Osindero,
2014). The generator of cGANs can generate stochastic outputs of which range are bounded. As
mentioned in Section 1, bounding action values is expected to make the problem easier to solve
and make the training faster. In the end, we adopt the form of the conditional generator to rep-
resent the stochastic learner policy πθ (s, z). The typical Gaussian policy and the proposed policy
representations with neural networks are described in Figure 1.
3.3 Off-policy Actor-Critic Imitation Learning
Algorithm.1 shows the overview of our off-policy actor-critic imitation learning algorithm.
To learn the value function approximator Qπθ ,ν, we adopt a behavior policy β as π in the second
term in objective (18) We employ a mixture of the past learner policies as β and a replay buffer Bβ
(Mnih et al., 2015) to perform sampling St 〜Pn, at 〜π and st+ι 〜T. The buffer Be is a finite
cache and stores the (st, at, st+1) triplets in a first-in-first-out manner while the learner interacts
with the environment.
The approximator Qπθ,ν(st, at) = log qπθ,ν(st, at) takes (-∞, 0]. With the approximator, using
the gradient (5) to update the learner policy always punish (or ignore) the learner’s actions. Instead,
we adopt the gradient (6) which directly uses Jacobian of Qπθ,ν.
As do off-policy RL methods such as Mnih et al. (2015) and Lillicrap et al. (2015), we use the target
value function approximator, of which parameters are updated to track ν , to optimize Qπθ ,ν. We
update Qπθ,ν and πθ at the end of each episode rather than following each step of interaction.
6
Published as a conference paper at ICLR 2019
4	Related Work
In recent years, the connection between generative adversarial networks (GAN) (Goodfellow et al.,
2014) and IL has been pointed out (Ho & Ermon, 2016; Finn et al., 2016a). Ho & Ermon (2016)
show that IRL is a dual problem of RL which can be deemed as a problem to match the learner’s
occupancy measure (Syed et al., 2008) to that of the expert, and that a choice of regularizer for the
cost function yields an objective which is analogous to that of GAN. Their algorithm, namely GAIL,
has become a popular choice for IL and some extensions of GAIL have been proposed (Baram et al.,
2017; Hausman et al., 2017; Li et al., 2017). However, those extensions have never addressed reduc-
ing the number of interactions during training.
There has been a few attempts that try to improve the sample complexity in IL literatures, such as
Guided Cost Learning (GCL) (Finn et al., 2016b). However, those methods have worse imitation ca-
pability in comparison with GAIL, as reported by Fu & Levine (2017). As detailed in section 5, our
algorithm have comparable imitation capability to GAIL while improving the sample complexity.
Hester & Osband (2017) proposed an off-policy algorithm using the expert demonstration. They ad-
dress problems where both demonstration and hand-crafted rewards are given. Whereas, we address
problems where only the expert demonstration is given.
There is another line of IL works where the learner can ask the expert which actions should be taken
during training, such as DAgger(Ross & Bagnell, 2011), SEARN (DaUme & Marcu, 2009), SMILe
(Ross & Bagnell, 2010), and AggreVaTe (Ross & Bagnell, 2014). As opposed to those methods, we
do not suppose that the learner can query the expert during training.
5	Experiments
In our experiments, we aim to answer the following three questions:
Q1. Can our algorithm enable the learner to imitate the expert behavior?
Q2. Is our algorithm more sample efficient than BC in terms of the expert demonstration?
Q3. Is our algorithm more efficient than GAIL in terms of the training time?
5.1	Setup
To answer the questions above, we use five physics-based control tasks that are simulated with
MuJoCo physics simulator (Todorov et al., 2012). See Appendix A for the description of each task.
In the experiments, we compare the performance of our algorithm, BC, GAIL, and GAIL initialized
by BC23. The implementation details can be found in Appendix B. We train an agent on each task
by TRPO (Schulman et al., 2015a) using the rewards defined in the OpenAI Gym (Brockman et al.,
2016), then we use the resulting agent with a stochastic policy as the expert for the IL algorithms.
We store (st, at, st+1) triplets during the expert demonstration, then the triplets are used as training
samples in the IL algorithms. In order to study the sample efficiency of the IL algorithms, we arrange
two setups. The first is sparse sampling setup, where we randomly sample 100 (st, at, st+1)
triplets from each trajectory which contains 1000 triplets. Then we perform the IL algorithms using
datasets that consist of several 100s triplets. Another setup is dense sampling setup, where we
use full (st, at, st+1) triplets in each trajectory, then train the learner using datasets that consist of
several trajectories. Ifan IL algorithm succeeds to imitate the expert behavior in the dense sampling
setup whereas it fails in the sparse sampling setup, we evaluate the algorithm as sample inefficient in
terms of the expert demonstration. The performance of the experts and the learners are measured by
cumulative reward they earned in a trajectory. We run three experiments on each task, and measure
the performance during training.
2Ho & Ermon (2016) suggest that initializing policy parameters with BC could significantly improve learn-
ing speed, but they did not show any results of such initialization.
3We also conducted the same comparison with MGAIL(Baram et al., 2017) using an online available code
provided by the authors. Unfortunately, we never reproduced the same performance as reported in the paper on
all tasks except for Hopper-v1 even if we followed the author’s advice.
7
Published as a conference paper at ICLR 2019
HoPPer-VI	Walker2d-v1	HalfCheetah-v1
drawer evitalumuC
1∙00∙01∙0
)dezilamon(
Ant-v1	Humanoid-v1
Number of trajectories in dataset
0.0	0.0	0 0	0 0 _______________ _____
5 10 15 20 25	5 10 15 20 25	5 10 15 20 25	5 10 15 20 25 0.0 100 150 200 250
∙0
O
Figure 2: The cumulative reward (normalized) vs. the number of trajectories in a dataset. The results
in sparse and dense sampling setup are depicted on top and bottom row, respectively.
HUmanoid-VI
Walker2d-v1
Ant-VI
HoPPer-VI
HalfCheetah-VI
150000
20000
Wall Clock Time (sec)
—GAIL
—BC+GAIL
--Ours
Pjemqj QA-α-numu
20000
20000
(IIh<∣n⅛∙*W>⅛Mf***∣*1

IO4	10β IO4	IO7 IO4	10r IO4	IO7 IO4	IO7
Number of EnVironment Interaction (log scale)
Figure 3:	The cumulative reward (normalized) vs. training time (top row) and the number of envi-
ronment interactions (bottom row).
5.2 Results
Figure 2 shows the experimental results in both sparse and dense sampling setup. In comparison with
GAIL, our algorithm marks worse performance on Walker2d-v1 and Humanoid-v1 with the datasets
of the smallest size in sparse sampling setup, better performance on Ant-v1 in both setups, and
competitive performance on the other tasks in both setups. Overall, we conclude that our algorithm
is competitive with GAIL with regards to performance. That is, our algorithm enables the learner to
imitate the expert behavior as well as GAIL does. BC imitates the expert behavior successfully on
all tasks in the dense sampling setup. However, BC often fails to imitate the expert behavior in the
sparse sampling setup with smaller datasets. Our algorithm achieves better performance than BC
does all over the tasks. It shows that our algorithm is more sample efficient than BC in terms of the
expert demonstration.
Figure 3 shows the performance plot curves over validation rollouts during training in the sparse
sampling setup. The curves on the top row in Figure 3 show that our algorithm denoted by Ours
trains the learner more efficiently than GAIL does in terms of training time. In addition, the curves
on the bottom row in Figure 3 show that our algorithm trains the learner much more efficiently than
GAIL does in terms of the environment interaction. As opposed to Ho & Ermon (2016) suggestion,
GAIL initialized by BC (BC+GAIL) does not improve the sample efficiency, but rather harms the
leaner’s performance significantly.
8
Published as a conference paper at ICLR 2019
)dezilamon(
drawer evitalumu
GAIL
Ours+OnP
Ours+IRL(D)
Ours+IRL(E)
Ours+GP
Ours+DP
Ours
Figure 4:	The cumulative reward (normalized) vs. the number of environment interactions on Ant-v1
in the ablation experiment.
5.3 Ablation Experiments
We conducted additional ablation experiments to demonstrate that our proposed method described in
Section.3 improves the sample efficiency. Figure 4 shows the ablation experimental results on Ant-
v1 task. Ours+OnP, which denotes an on-policy variant of Ours, requires 100 times more interactions
than Ours. The result with Ours+OnP suggests that adopting off-policy learning scheme instead of
on-policy one significantly improves the sample efficiency. Ours+IRL(D) and Ours+IRL(E) are
variants of Ours that learn value function approximators using the learned reward function with the
objective (2) and (8), respectively. The result with Ours+IRL(D) and Ours+IRL(E) suggests that
omitting the reward learning described in 3.1 makes the training stable and faster. The result with
Ours+GP, which denotes a variant of Ours that adopts the Gaussian policy, suggests that bounding
action values described in 3.2 makes the training faster and stable. The result with Ours+DP, which
denotes a variant of Ours that has a deterministic policy with fixed input noises, fails to imitate the
expert behavior. It shows that the input noise variable z in our algorithm plays a role to obtain
stochastic policies.
6	Conclusion
In this paper, we proposed a model-free IL algorithm for continuous control. Experimental results
showed that our algorithm achieves competitive performance with GAIL while significantly reduc-
ing the environment interactions.
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
International Conference on Machine Learning, pp. 1, 2004.
Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end differentiable adversarial imi-
tation learning. In International Conference on Machine Learning, pp. 390-399, 2017.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Daniel Maturana Chou, Po-Wei and Sebastian Scherer. Improving stochastic policy gradients in
continuous control with deep reinforcement learning using the beta distribution. In International
Conference on Machine Learning, pp. 834-843, 2017.
John Langford Daume, Hal and Daniel Marcu. Search-based structured prediction. In Machine
learning, pp. 297-325, 2009.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint
arXiv:1205.4839, 2012.
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative
adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint
arXiv:1611.03852, 2016a.
9
Published as a conference paper at ICLR 2019
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International Conference on Machine Learning, pp. 49-58, 2016b.
Katie Luo Fu, Justin and Sergey Levine. Learning robust rewards with adversarial inverse reinforce-
ment learning. arXiv preprint arXiv:1710.11248, 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence
and Statistics, pp. 249-256, 2010.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, and Joseph Lim. Multi-modal
imitation learning from unstructured demonstrations using generative adversarial nets. arXiv
preprint arXiv:1705.10479, 2017.
Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning
continuous control policies by stochastic value gradients. In Advances in Neural Information
Processing Systems, pp. 2944-2952, 2015.
Vecerik M. Pietquin O. Lanctot M. Schaul T. Piot B. Horgan D. Quan J. Sendonaris A. Dulac-
Arnold G. Hester, T. and I Osband. Deep q-learning from demonstrations. arXiv preprint
arXiv:1704.03732, 2017.
G Hinton, N Srivastava, and K Swersky. Rmsprop: Divide the gradient by a running average of its
recent magnitude. 2012.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, pp. 4565-4573, 2016.
Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual
demonstrations. pp. 3815-3825, 2017.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural net-
work acoustic models. In International Conference on Machine Learning, volume 30, pp. 3,
2013.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In International
Conference on Machine Learning, pp. 663-670, 2000.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. vol-
ume 3, pp. 88-97. MIT Press, 1991.
Gordon G. Ross, S. and D. Bagnell. A reduction of imitation learning and structured prediction to
no-regret online learning. In International Conference on Artificial Intelligence and Statistics, pp.
627-635, 2011.
10
Published as a conference paper at ICLR 2019
StePhane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings ofthe
thirteenth international conference on artificial intelligence and statistics, pp. 661-668, 2010.
StePhane Ross and J. Andrew Bagnell. A reduction of imitation learning and structured Prediction
to no-regret online learning. arXiv preprint arXiv:1406.5979, 2014.
Stuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh annual
conference on Computational learning theory, pp. 101-103, 1998.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015b.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear pro-
gramming. In International Conference on Machine Learning, pp. 1032-1039, 2008.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033, 2012.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Association for the Advancement of Artificial Intelligence, pp. 1433-
1438, 2008.
11
Published as a conference paper at ICLR 2019
A Detailed Description of Experiment
Table 1 summarizes the description of each task, the performance of an agent with random policy,
and the performance of the experts.
Table 1: Description of each task, an agent’s performance with random policy, and the performance
of the experts. dim(S) and dim(A) denote dimensionality of state and action spaces respectively.
Task	dim(S)	dim(A)	RandOm Policy	Expert’s Performance
HalfCheetah-v1	17	6	-282.43 ± 79.53	4130.22 ± 75.51
Hopper-v1	11	3	14.47 ± 7.96	3778.05 ± 3.34
Walker2d-v1	17	6	0.57 ± 4.59	5510.67 ± 74.44
Ant-v1	111	8	-69.68 ± 111.10	4812.93 ± 122.26
Humanoid-v1	376	17	122.87 ± 35.11	10395.51 ± 205.81
B	Implementation Details
We implement our algorithm using two neural networks with two hidden layers. Each network rep-
resents πθ and qν . For convenience, we call those networks for πθ and qω as policy network (PN)
and Q-network (QN), respectively. PN has 100 hidden units in each hidden layer, and its final output
is followed by hyperbolic tangent nonlinearity to bound its action range. QN has 500 hidden units
in each hidden layer and a single output is followed by sigmoid nonlinearity to bound the output be-
tween [0,1]. All hidden layers are followed by leaky rectified nonlinearity (Maas et al., 2013). The
parameters in all layers are initialized by Xavier initialization (Glorot & Bengio, 2010). The input of
PN is given by concatenated vector representations for the state s and noise z. The noise vector, of
which dimensionality corresponds to that of the state vector, generated by zero-mean normal distri-
bution so that Z 〜 PZ = N(0,1). The input of QN is given by concatenated vector representations
for the state s and action a. We employ RMSProp (Hinton et al., 2012) for learning parameters with
a decay rate 0.995 and epsilon 10-8 . The learning rates are initially set to 10-3 for QN and 10-4 for
PN, respectively. The target QN with parameters V are updated so that V = 10-3*ν+(1-10-3)*ν'
at each update of ν. We linearly decrease the learning rates as the training proceeds. We set mini-
batch size of (st, at, st+ι) triplets 64, the replay buffer size ∣Bβ| = 15000, and the discount factor
Y = 0.85. We sample 128 noise vectors for calculating empirical expectation EZ〜Pz of the gradient
(6). We use publicly available code (https://github.com/openai/imitation) for the
implementation of GAIL and BC. Note that, the number of hidden units in PN is the same as that of
networks for GAIL. All experiments are run on a PC with a 3.30 GHz Intel Core i7-5820k Processor,
a GeForce GTX Titan GPU, and 32GB of RAM.
12