Published as a conference paper at ICLR 2019
Optimistic Mirror Descent in Saddle-Point Problems:
Going the Extra (Gradient) Mile
Panayotis Mertikopoulos
Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG
38000 Grenoble, France
panayotis.mertikopoulos@imag.fr
Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar
Institute for Infocomm Research, A*STAR
1 Fusionopolis Way, #21-01 Connexis (South Tower), Singapore
{bruno_lecouat,houssam_zenati,foocs,vijay}@i2r.a-star.edu.sg
Georgios Piliouras
Singapore University of Technology and Design
8 Somapah Road, Singapore
georgios@sutd.edu.sg
Abstract
Owing to their connection with generative adversarial networks (GANs), saddle-
point problems have recently attracted considerable interest in machine learning
and beyond. By necessity, most theoretical guarantees revolve around convex-
concave (or even linear) problems; however, making theoretical inroads towards
efficient GAN training depends crucially on moving beyond this classic framework.
To make piecemeal progress along these lines, we analyze the behavior of mirror
descent (MD) in a class of non-monotone problems whose solutions coincide with
those of a naturally associated variational inequality - a property which We call
coherence. We first show that ordinary, “vanilla” MD converges under a strict
version of this condition, but not otherwise; in particular, it may fail to converge
even in bilinear models with a unique solution. We then show that this deficiency
is mitigated by optimism: by taking an “extra-gradient” step, optimistic mirror
descent (OMD) converges in all coherent problems. Our analysis generalizes and
extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD)
in bilinear problems, and makes concrete headway for provable convergence beyond
convex-concave games. We also provide stochastic analogues of these results, and
we validate our analysis by numerical experiments in a wide array of GAN models
(including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).
Figure 1: Mirror descent (MD) in the non-monotone saddle-point problem f (x1, x2) = (x1 - 1/2)(x2 - 1/2) +
1 exp(-(XI - 1/4)2 - (X2 - 3/4)2). Left: vanilla MD spirals outwards; right: optimistic MD converges.
1
Published as a conference paper at ICLR 2019
1	Introduction
The surge of recent breakthroughs in deep learning has sparked significant interest in solving
optimization problems that are universally considered hard. Accordingly, the need for an effective
theory has two different sides: first, a deeper understanding would help demystify the reasons behind
the success and/or failures of different training algorithms; second, theoretical advances can inspire
effective algorithmic tweaks leading to concrete performance gains. For instance, using tools from the
theory of dynamical systems, Lee et al. [2016, 2019] and Panageas & Piliouras [2017] showed that a
wide variety of first-order methods (including gradient descent and mirror descent) almost always
avoid saddle points. More generally, the optimization and machine learning communities alike have
dedicated significant effort in understanding non-convex landscapes by searching for properties which
could be leveraged for efficient training. As an example, the “strict saddle” property was shown
to hold in a wide range of salient objective functions ranging from low-rank matrix factorization
[Bhojanapalli et al., 2016; Ge et al., 2017] and dictionary learning [Sun et al., 2017a,b], to principal
component analysis [Ge et al., 2015], and many other models.
On the other hand, adversarial deep learning is nowhere near as well understood, especially in the
case of generative adversarial networks (GANs) [Goodfellow et al., 2014]. Despite an immense
amount of recent scrutiny, our theoretical understanding cannot boast similar breakthroughs as in
“single-agent” deep learning. Because of this, a considerable corpus of work has been devoted
to exploring and enhancing the stability of GANs, including techniques as diverse as the use of
Wasserstein metrics [Arjovsky et al., 2017], critic gradient penalties [Gulrajani et al., 2017], feature
matching, minibatch discrimination, etc. [Radford et al., 2016; Salimans et al., 2016].
Even before the advent of GANs, work on adaptive dynamics in general bilinear zero-sum games
(e.g. Rock-Paper-Scissors) established that they lead to persistent, chaotic, recurrent (i.e. cycle-like)
behavior [Sato et al., 2002; Piliouras & Shamma, 2014; Piliouras et al., 2014]. Recently, simple
specific instances of cycle-like behavior in bilinear games have been revisited mainly through the lens
of GANs [Mertikopoulos et al., 2018; Daskalakis et al., 2018; Mescheder et al., 2018; Papadimitriou
& Piliouras, 2018]. Two important recent results have established unified pictures about the behavior
of continuous and discrete-time first order methods in bilinear games: First, Mertikopoulos et al.
[2018] established that continuous-time descent methods in zero-sum games (e.g., gradient descent,
follow-the-regularized-leader and the like) are Poincare recurrent, returning arbitrarily closely to
their initial conditions infinitely many times. Second, Bailey & Piliouras [2018] examined the
discrete-time analogues (gradient descent, multiplicative weights and follow-the-regularized-leader)
showing that orbits spiral slowly outwards. These recurrent systems have formal connections to
Hamiltonian dynamics and do not behave in a gradient-like fashion Balduzzi et al. [2018]; Bailey
& Piliouras [2019]. This is a critical failure of descent methods, but one which Daskalakis et al.
[2018] showed can be overcome through “optimism”, interpreted in this context as an “extra-gradient”
step that pushes the training process further along the incumbent gradient - as a result, optimistic
gradient descent (OGD) succeeds in cases where vanilla gradient descent (GD) fails (specifically,
unconstrained bilinear saddle-point problems).
A common theme in the above is that, to obtain a principled methodology for training GANs, it is
beneficial to first establish improvements in a more restricted setting, and then test whether these
gains carry over to more demanding learning environments. Following these theoretical breadcrumbs,
we focus on a class of non-monotone problems whose solutions are related to those of a naturally
associated variational inequality, a property which we call coherence. Then, hoping to overcome the
shortcomings of ordinary descent methods by exploiting the problem’s geometry, we examine the
convergence of MD in coherent problems. On the positive side, we show that if a problem is strictly
coherent (a condition satisfied by all strictly convex-concave problems), MD converges almost surely,
even in stochastic problems (Theorem 3.1). However, under null coherence (the “saturated” opposite
to strict coherence), MD spirals outwards from the problem’s solutions and may cycle in perpetuity.
The null coherence property covers all bilinear models, so this result encompasses fully the analysis
of Bailey & Piliouras [2018] for GD and follow-the-regularized-leader (FTRL) in general bilinear
zero-sum games within our coherence framework. Thus, in and by themselves, gradient/mirror
descent methods do not suffice for training convoluted, adversarial deep learning models.
To mitigate this deficiency, we consider the addition of an extra-gradient step which looks ahead
and takes an additional step along a “future” gradient. This technique was first introduced by
Korpelevich [1976] and subsequently gained great popularity as the basis of the mirror-prox algorithm
2
Published as a conference paper at ICLR 2019
of Nemirovski [2004] which achieves an optimal O(1/n) convergence rate in Lipschitz monotone
variational inequalities (see also Nesterov, 2007, for a primal-dual variant of the method and Juditsky
et al., 2011, for an extension to stochastic variational inequalities and saddle-point problems).
In the learning literature, the extra-gradient technique (or, sometimes, a variant thereof) is often
referred to as optimistic mirror descent (OMD) [Rakhlin & Sridharan, 2013] and its effectiveness in
GAN training was recently examined by Daskalakis et al. [2018] and Yadav et al. [2018] (the latter
involving a damping mechanism for only one of the players). More recently, Gidel et al. [2018]
considered a variant method which incorporates a mechanism that “extrapolates from the past” in
order to circumvent the need for a second oracle call in the extra-gradient step. Specifically, Gidel
et al. [2018] showed that the extra-gradient algorithm with gradient reuse converges a) geometrically
in strongly monotone, deterministic variational inequalities; and b) ergodiCally in general stochastic
variational inequalities, achieving in that case an oracle complexity bound that is √13∕7∕2 ≈ 68% of
a bound previously established by Juditsky et al. [2011] for the mirror-prox algorithm.
However, beyond convex-concave problems, averaging offers no tangible benefits because there is no
way to relate the value of the ergodic average to the value of the iterates. As a result, moving closer
to GAN training requires changing both the algorithm’s output as well as the accompanying analysis.
With this as our guiding principle, we first show that the last iterate of OMD converges in all coherent
problems, including null-coherent ones. As a special case, this generalizes and extends the results
of Noor et al. [2011] for OGD in pseudo-monotone problems, and also settles in the affirmative an
open question of Daskalakis et al. [2018] concerning the convergence of the last iterate of OGD
in nonlinear problems. Going beyond deterministic problems, we also show that OMD converges
with probability 1 even in stochastic saddle-point problems that are strictly coherent. These results
complement the existing literature on the topic by showing that a cheap extra-gradient add-on can
lead to significant performance gains when applied to state-of-the-art methods (such as Adam). We
validate this prediction for a wide array of standard GAN models in Section 5.
2	Problem setup and preliminaries
Saddle-point problems. Consider a saddle-point problem of the general form
min max f(x1,x2),	(SP)
x1 ∈X1 x2 ∈X2
where each feasible region Xi, i = 1, 2, is a compact convex subset of a finite-dimensional normed
space Vi ≡ ’di, and f : X ≡ X1 × X2 → ’ denotes the problem’s value function. From a game-
theoretic standpoint, (SP) can be seen as a zero-sum game between two optimizing agents (or players):
Player 1 (the minimizer) seeks to incur the least possible loss, while Player 2 (the maximizer) seeks
to obtain the highest possible reward - both determined by f (Xɪ, X2).
To solve (SP), we will focus on incremental processes that exploit the individual loss/reward gradients
of f (assumed throughout to be at least C1-smooth). Since the individual gradients of f will play a
key role in our analysis, we will encode them in a single vector as
g(x) = (g1(x),g2(x)) = (VX1 f (X1, X2),-VX2 f (X1, X2))	(2.1)
and, following standard conventions, we will treat g(X) as an element of Y ≡ V*, the dual of the
ambient space V ≡ V1 × V2, assumed to be endowed with the product norm kXk2 = kX1k2 + kX2k2.
Variational inequalities and coherence. Most of the literature on saddle-point problems has
focused on the monotone case, i.e., when f is conveX-concave. In such problems, it is well known
that solutions of (SP) can be characterized equivalently as solutions of the Stampacchia variational
inequality
hg(X*), X - X*>≥ 0 forall X ∈ X	(SVI)
or, in Minty form:
hg(X), X - X*>≥ 0 for all X ∈ X.	(MVI)
The equivalence between solutions of (SP), (SVI) and (MVI) extends well beyond the realm of
monotone problems: it trivially includes all bilinear problems (f(X1, X2) = X1>MX2), pseudo-monotone
objectives as in Noor et al. [2011], etc. For a concrete example which is not even pseudo-monotone,
consider the problem
min max ( X41 X22 + X12 + 1)(X12X42 - X22 + 1).	(2.2)
X1∈[-1,1] X2∈[-1,1]	1 2	1	1 2	2
3
Published as a conference paper at ICLR 2019
The only saddle-point of f is X* = (0,0): it is easy to check that X* is also the unique solution of
the corresponding variational inequality (VI) problems, despite the fact that f is not even pseudo-
monotone.1 This shows that this equivalence encompasses a wide range of phenomena that are
innately incompatible with convexity/monotonicity, even in the lowest possible dimension; for an
in-depth discussion we refer the reader to Facchinei & Pang [2003].
Motivated by all this, we introduce below the following notion of coherence:
Definition 2.1. We say that (SP) is coherent if:
1.	Every solution of (SVI) also solves (SP).
2.	There exists a solution p of (SP) that satisfies (MVI).
3.	Every solution X* of (SP) satisfies (MVI) locally, i.e., for all X sufficiently close to X*.
In the above, if (MVI) holds as a strict inequality whenever X is not a solution thereof, (SP) will be
called strictly coherent; by contrast, if (MVI) holds as an equality for all X ∈ X, we will say that (SP)
is null-coherent.
The notion of coherence will play a central part in our considerations, so a few remarks are in order.
To the best of our knowledge, its first antecedent is a gradient condition examined by Bottou [1998]
in the context of nonlinear programming; we borrow the term “coherence” from the more recent
paper of Zhou et al. [2017b] who used the term “variational coherence” for a stronger variant of the
above definition.
We should also note here that the set of solutions of a coherent problem does not need to be convex:
for instance, if Player 1 controls (X, y), and the objective function is f(X, y) = X2y2 (i.e., Player 2
has no impact in the game), the set of solutions is the non-convex set X* = {(X, y) : X = 0 or y = 0}.
Moreover, regarding the distinction between coherence and strict coherence, we show in Appendix A
that (SP) is strictly coherent when f is strictly convex-concave. At the other end of the spectrum,
typical examples of problems that are null-coherent are bilinear objectives with an interior solution: for
instance, f(X1, X2) = X1X2 with X1, X2 ∈ [-1, 1] has hg(X), Xi = X1X2 - X2X1 = 0 for all X1, X2 ∈ [-1, 1],
so it is null-coherent. Finally, neither strict, nor null coherence imply a unique solution to (SP), a
property which is particularly relevant for GANs (the first example above is strictly coherent, but
does not admit a unique solution).
3	Mirror descent
The method. Motivated by its prolific success in convex programming, our starting point will be
the well-known mirror descent (MD) method of Nemirovski & Yudin [1983], suitably adapted to our
saddle-point context. Several variants of the method exist, ranging from dual averaging [Nesterov,
2009] to follow-the-regularized-leader; for a survey, we refer the reader to Bubeck [2015].
The basic idea of mirror descent is to generate a new state variable X+ from some starting state X by
taking a “mirror step” along a gradient-like vector y. To do this, let h : X → ’ be a continuous and
K-strongly convex distance-generating function (DGF) on X, i.e.,
h(tx + (1 一 t)x') ≤ th(x) + (1 一 t)h(X) 一 2Kt(1 - t)kX' — x∣∣2,	(3.1)
for all X, X0 ∈ X and all t ∈ [0, 1]. In terms of smoothness (and in a slight abuse of notation), we
also assume that the subdifferential of h admits a continuous selection, i.e., a continuous function
Vh: dom ∂h → Y such that Vh(x) ∈ ∂h(x) for all X ∈ dom ∂h.2 Then, following Bregman [1967], h
generates a pseudo-distance on X via the relation
D(p, x) = h(p) 一 h(x) 一 hVh(x), p 一 xi for all p ∈ X, x ∈ dom ∂h.	(3.2)
This pseudo-distance is known as the Bregman divergence. As we show in Appendix B, we have
D(P, x) ≥ 2KkX — pk2, so the convergence of a sequence Xn to some target point P can be verified by
showing that D(p, Xn) → 0. On the other hand, D(p, x) typically fais to be symmetric and/or satisfy
1To see this, simply note that f(x1, x2) is multi-modal in x2 for certain values of x1 (e.g., when x1 = 1/2).
2Recall here that the subdifferential of h at x ∈ X is defined as ∂h(x) ≡ {y ∈ Y : h(x0) ≥ h(x) + hy, x0 一
xi for all x ∈ V}, with the standard convention h(x) = ∞ for all x ∈ V \ X.
4
Published as a conference paper at ICLR 2019
Algorithm 1: mirror descent (MD) for saddle-point problems
Require: K-strongly convex regularizer h : X → ’, step-size sequence γn > 0
1:	choose X ∈ dom ∂h	# initialization
2:	for n = 1, 2, . . . do
3:	oracle query at X returns g	# gradient feedback
4:	set X — Pχ(-γng)	#new state
5:	end for
6:	return X
the triangle inequality, so it is not a true distance function per se. Moreover, the level sets of D(p, x)
may fail to form a neighborhood basis of p, so the convergence of Xn to p does not necessarily imply
that D(p, Xn) → 0; we provide an example of this behavior in Appendix B. For technical reasons, it
will be convenient to assume that such phenomena do not occur, i.e., that D(p, Xn) → 0 whenever
Xn → p. This mild regularity condition is known in the literature as “Bregman reciprocity” [Chen &
Teboulle, 1993; Alvarez et al., 2004; Mertikopoulos & Staudigl, 2018; Bravo et al., 2018], and it will
be our standing assumption in what follows (note also that it holds trivially for both Examples 3.1
and 3.2 below).
Now, as with standard Euclidean distances, the Bregman divergence generates an associated prox-
mapping defined as
Px(y) = arg min{hy, x - x0i + D(x0, x)} for all x ∈ dom ∂h, y ∈ Y.	(3.3)
x0∈X
In analogy with the Euclidean case (discussed below), the prox-mapping (3.3) produces a feasible
point x+ = Px(y) by starting from x ∈ dom ∂h and taking a step along a dual (gradient-like) vector
y ∈ Y. In this way, we obtain the mirror descent (MD) algorithm
Xn+1 = PXn (-Jng n ),	(MD)
where Yn is a variable step-size sequence and gn is the calculated value of the gradient vector g(Xn) at
the n-th stage of the algorithm (for a pseudocode implementation, see Section 3).
For concreteness, two widely used examples of prox-mappings are as follows:
Example 3.1 (Euclidean projections). When X is endowed with the L2 norm ∣∣∙∣∣2, the archetypal
prox-function is the (square of the) norm itself, i.e., h(X) = 2kX∣∣2∙ In that case, D(P, x) = 2 kX - pk2
and the induced prox-mapping is
Px(y) = Π(x + y),	(3.4)
with Π(x) = argminx0∈Xkx0 - xk2 denoting the ordinary Euclidean projection onto X.
Example 3.2 (Entropic regularization). When X is a d-dimensional simplex, a widely used DGF
is the (negative) Gibbs-Shannon entropy h(x) = P；=1 XjlOg Xj. This function is 1-strongly convex
with respect to the L1 norm [Shalev-Shwartz, 2011] and the associated pseudo-distance is the
Kullback-Leibler divergence DKL(p, X) = Pdj=1 pj log(pj/Xj); in turn, this yields the prox-mapping
(Xjexp(yj))dj=1
Px(y) = r------------j=1 for all X ∈ X0, y ∈ Y.
dj=1 Xj exp(yj)
(3.5)
The update rule X J Pχ(y) is known in the literature as the multiplicative weights (MW) algorithm
[Arora et al., 2012], and is one of the centerpieces for learning in games [Fudenberg & Levine, 1998;
Freund & Schapire, 1999; Cohen et al., 2017], adversarial bandits [Auer et al., 1995], etc.
Regarding the gradient input sequence gn of (MD), we assume that it is obtained by querying a
first-order oracle which outputs an estimate of g(Xn) when called at Xn . This oracle could be either
perfect, returning gn = g(Xn) for all n, or imperfect, providing noisy gradient estimations.3 By
that token, we will make the following standard assumptions for the gradient feedback sequence gn
[Nesterov, 2007; Nemirovski et al., 2009; Juditsky et al., 2011]:
a)	Unbiasedness:	…[gn | Fn] = g(Xn).
b)	Finite mean square error: …[kgn∣∣21 Fn] ≤ G2 for some finite G ≥ 0.
(3.6)
3The reason for this is that, depending on the application at hand, gradients might be difficult to compute
directly e.g., because they require huge amounts of data, the calculation of an unknown expectation, etc.
5
Published as a conference paper at ICLR 2019
In the above, kyk* ≡ sup{hy, ɪ〉： X ∈ V, kXk ≤ 1} denotes the dual norm on Y while Fn represents
the history (natural filtration) of the generating sequence Xn up to stage n (inclusive). Since gn is
generated randomly from Xn at stage n, it is obviously not Fn -measurable, i.e., gn = g(Xn) + Un+1,
where Un is an adapted martingale difference sequence with …[k Un+ιk21 Fn] ≤ σ2 for some finite
σ ≥ 0. Clearly, when σ = 0, we recover the exact gradient feedback framework gn = g(Xn).
Convergence analysis. When (SP) is convex-concave, it is customary to take as the output of (MD)
the so-called ergodic average
Y _ P n =1 YkXk
n = ɪl^r'
(3.7)
or some other average of the sequence Xn where the objective is sampled. The reason for this is
that convexity guarantees - via Jensen,s inequality and gradient monotonicity - that a regret-based
analysis of (MD) can lead to explicit rates for the convergence of Xn to the solution set of (SP)
[Nemirovski, 2004; Nesterov, 2007]. However, when the problem is not convex-concave, the standard
proof techniques for establishing convergence of the method’s ergodic average no longer apply;
instead, we need to examine the convergence properties of the generating sequence Xn of (MD)
directly. With all this in mind, our main result for (MD) may be stated is as follows：
Theorem 3.1. Suppose that (MD) is run with a gradient oracle satisfying (3.6) and a variable
step-size sequence γn such that Pn∞=1 γn = ∞. Then:
a)	If f is strictly coherent and Pn∞=1 γn2 < ∞, Xn converges (a.s.) to a solution of (SP).
b)	Iff is null-coherent, the Sequence …[D (X *, Xn)] is non-deCreasingfor ^very solution x * of (SP).
This result establishes an important dichotomy between strict and null coherence： in strictly coherent
problems, Xn is attracted to the solution set of (SP); in null-coherent problems, Xn drifts away and
cycles without converging. In particular, this dichotomy leads to the following immediate corollaries：
Corollary 3.2. Suppose that f is strictly conveX-concave. Then, with assumptions as above, Xn
converges (a.s.) to the (necessarily unique) solution of (SP).
Corollary 3.3. Suppose that f is bilinear and admits an interior Saddle-POint x* ∈ X°. IfXι , X*
and (MD) is run with eXact gradient input (σ = 0), we have limn→∞ D(X*, Xn) > 0.
Since bilinear models include all finite two-player, zero-sum games, Corollary 3.3 also encapsulates
the non-convergence results of Daskalakis et al. [2018] and Bailey & Piliouras [2018] for gradient
descent and FTRL respectively (for a more comprehensive formulation, see Proposition C.3 in
Appendix C). The failure of (MD) to converge in this case is due to the fact that, witout a mitigating
mechanism in place, a “blind” first-order step could overshoot and spiral outwards, even with a
vanishing step-size. This becomes even more pronounced in GANs where it can lead to mode collapse
and/or cycles between different modes; the next two sections address precisely these issues. 4
4 Extra-gradient analysis
The method. In convex-concave problems, taking an average of the algorithm’s generated samples
as in (3.7) may resolve cycling phenomena by inducing an auxiliary sequence that gravitates towards
the “center of mass” of the driving sequence Xn (which orbits interior solutions). However, this
technique cannot be employed in problems that are not convex-concave because the structure of f
cannot be leveraged to establish convergence of the ergodic average of the process.
In view of this, we replace averaging with an optimistic “extra-gradient” step which uses the obtained
information to amortize the next prox step (possibly by exiting the convex hull of generated states).
The seed of this “extra-gradient” idea dates back to Korpelevich [1976] and Nemirovski [2004], and
has since found wide applications in optimization theory and beyond - for a survey, see Bubeck
[2015] and references therein.
In a nutshell, given a state X, the extra-gradient method first generates an intermediate, “waiting” state
X = Pχ(-γg(x)) by taking a prox step as usual. However, instead of continuing from X, the method
samples g(X) and goes back to the original state X in order to generate a new state X+ = Px(-γg(X)).
Based on this heuristic, we obtain the optimistic mirror descent (OMD) algorithm
Xn+1/2 = PXn (-Y n g n )
Xn +1 = PXn (-Y n g n+1/2)
(OMD)
6
Published as a conference paper at ICLR 2019
Algorithm 2: optimistic mirror descent (OMD) for saddle-point problems
Require: K-strongly convex regularizer h : X → ’, step-size sequence γn > 0
1:	choose X ∈ dom ∂h	# initialization
2:	for n = 1, 2, . . . do
3:	oracle query at X returns g	# gradient feedback
4:	set X+ — Pχ(-γng)	#waiting state
5:	oracle query at X+ returns g+	# gradient feedback
6:	set X — Pχ(-γng+)	#new state
7:	end for
8:	return X
where, in obvious notation, gn and gn+1/2 represent gradient oracle queries at the incumbent and
intermediate states Xn and Xn+1/2 respectively. For a pseudocode implementation, see Algorithm 2;
see also Rakhlin & Sridharan [2013] and Daskalakis et al. [2018] for a variant of the method with
a “momentum” step, and Gidel et al. [2018] for a gradient reuse mechanism that replaces a second
oracle call with a past gradient.
Convergence analysis. In his original analysis, Nemirovski [2004] considered the ergodic average
(3.7) of the algorithm’s iterates and established an O(1/n) convergence rate in monotone problems.
However, as we explained above, even though this kind of averaging is helpful in convex-concave
problems, it does not provide any tangible benefits beyond this class: in more general problems, Xn
appears to be the most natural solution candidate.
Our first result below justifies this choice in the class of coherent problems:
Theorem 4.1. Suppose that (SP) is coherent and g is L-Lipschitz continuous. If (OMD) is run
with exact gradient input (σ = 0) and γn such that 0 < infn γn ≤ supn γn < K/L, the sequence Xn
converges monotonically to a solution x* of (SP), i.e., D(X*, Xn) decreases monotonically to 0.
Corollary 4.2. Suppose that f is bilinear. If (OMD) is run with assumptions as above, the sequence
Xn converges monotonically to a solution of (SP).
Theorem 4.1 includes as a special case the analysis of Facchinei & Pang [2003, Theorem 12.1.11] for
optimistic gradient descent and the more recent results of Noor et al. [2011] for pseudo-monotone
problems. Importantly, Theorem 4.1 shows that the extra-gradient step plays a crucial role in
stabilizing (MD): not only does (OMD) converge in problems where (MD) provably fails, but this
convergence is, in fact, monotonic. In other words, at each iteration, (OMD) comes closer to a
solution of (SP), whereas (MD) may spiral outwards, ultimately converging to a limit cycle. This
phenomenon is seen clearly in Fig. 1, and also in the detailed analysis of Appendix C.
Of course, except for very special cases, the monotonic convergence of Xn cannot hold when the
gradient input to (OMD) is imperfect: a single “bad” sample of gn would suffice to throw Xn off-track.
In this case, we have:
Theorem 4.3. Suppose that (SP) is strictly coherent and (OMD) is run with a gradient oracle
satisfying (3.6) and a variable step-size sequence γn such that Pn∞=1 γn = ∞ and Pn∞=1 γn2 < ∞. Then,
with probability 1, Xn converges to a solution of (SP).
It is worth noting here that the step-size policy in Theorem 4.3 is different than that of Theorem 4.1.
This is due to a) the lack of randomness (which obviates the summability requirement Pn∞=1 γn2 < ∞ in
Theorem 4.1); and b) the lack of Lipschitz continuity assumption (which, in the case of Theorem 4.1
guarantees monotonic decrease at each step, provided the step-size is not too big). Importantly, the
maximum allowable step-size is also controlled by the strong convexity modulus of h, suggesting
that the choice of distance-generating function can be fine-tuned further to allow for more aggressive
step-size policies - a key benefit of mirror descent methods.
5 Experimental results
Gaussian mixture models. For the experimental validation of our theoretical results, we began by
evaluating the extra-gradient add-on in a highly multi-modal mixture of 16 Gaussians arranged in a
4 × 4 grid as in Metz et al. [2017]. The generator and discriminator have 6 fully connected layers
with 384 neurons and Relu activations (plus an additional layer for data space projection), and the
7
Published as a conference paper at ICLR 2019
(a) Vanilla versus optimistic RMS (top and bottom respectively; γ = 3 × 10-4 in both cases).
(b) Vanilla versus optimistic Adam (top and bottom respectively; γ = 4 × 10-5 in both cases).
Figure 2: Different algorithmic benchmarks (RMSprop and Adam): adding an extra-gradient step allows the
training method to accurately learn the target data distribution and eliminates cycling and oscillatory instabilities.
generator generates 2-dimensional vectors. The output after {4000, 8000, 12000, 16000, 20000}
iterations is shown in Fig. 2. The networks were trained with RMSprop [Tieleman & Hinton, 2012]
and Adam [Kingma & Ba, 2014], and the results are compared to the corresponding extra-gradient
variant (for an explicit pseudocode representation in the case of Adam, see Daskalakis et al. [2018]
and Appendix E). Learning rates and hyperparameters were chosen by an inspection of grid search
results so as to enable a fair comparison between each method and its look-ahead version. Overall, the
different optimization strategies without look-ahead exhibit mode collapse or oscillations throughout
the training period (we ran all models for at least 20000 iterations in order to evaluate the hopping
behavior of the generator). In all cases, the extra-gradient add-on performs consistently better in
learning the multi-modal distribution and greatly reduces occurrences of oscillatory behavior.
Experiments with standard datasets. In our experiments with Gaussian mixture models (GMMs),
the most promising training method was Adam with an extra-gradient step (a concrete pseudocode
implementation is provided in Appendix E). Motivated by this, we trained a Wasserstein-GAN on
the CelebA and CIFAR-10 datasets using Adam, both with and without an extra-gradient step. The
architecture employed was a standard DCGAN; hyperparameters and network architecture details
may be found in Appendix E. Subsequently, to quantify the gains of the extra-gradient step, we
employed the widely used inception score and Frechet distance metrics, for which We report the
results in Fig. 3. Under both metrics, the extra-gradient add-on provides consistently higher scores
after an initial warm-up period (and is considerably more stable). For visualization purposes, we also
present in Fig. 4 an ensemble of samples generated at the end of the training period. Overall, the
generated samples provide accurate feature representation and low distortion (especially in CelebA).
6 Conclusions
Our results suggest that the implementation of an optimistic, extra-gradient step is a flexible add-on
that can be easily attached to a wide variety of GAN training methods (RMSProp, Adam, SGA,
etc.), and provides noticeable gains in performance and stability. From a theoretical standpoint, the
dichotomy between strict and null coherence provides a justification of why this is so: optimism
eliminates cycles and, in so doing, stabilizes the method. We find this property particularly appealing
8
Published as a conference paper at ICLR 2019
Figure 3: Left: Inception score (left) and Fr6chet distance (right) on CIFAR-10 When training with Adam (with
and without an extra-gradient step). Results are averaged over 8 sample runs with different random seeds.
Figure 4: Samples generated by Adam with an extra-gradient step on CelebA (left) and CIFAR-10 (right).
because it paves the way to a local analysis with provable convergence guarantees in multi-modal
settings, and beyond zero-sum games; we intend to examine this question in future work.
Acknowledgments
P. Mertikopoulos was partially supported by the French National Research Agency (ANR) grant
ORACLESS(ANR-16-CE33-0004-01), the CNRS PEPS program under grant MixedGAN, and the
FMJH PGMO program under grant HEAVY.NET. G. Piliouras was partially supported by SUTD
grant SRG ESD 2015 097, MOE AcRF Tier 2 grant 2016-T2-1-170, grant PIE-SGP-AI-2018-01 and
NRF fellowship NRF-NRFF2018-07. This work was partly funded by the deep learning 2.0 program
at A*STAR.
References
Felipe Alvarez, J6r6me Bolte, and Olivier Brahic. Hessian Riemannian gradient flows in convex programming.
SIAM Journal on Control and Optimization, 43(2):477-501, 2004.
Mart^n Arjovsky, Soumith Chintala, and L6on Bottou. Wasserstein generative adversarial networks. In Proceed-
ings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017, pp. 214-223, 2017. URL http://proceedings.mlr.press/v70/arjovsky17a.html.
Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: A meta-algorithm and
applications. Theory of Computing, 8(1):121-164, 2012.
Peter Auer, NicolO Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. Gambling in a rigged casino: The
adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of
Computer Science, 1995.
James P Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In Proceedings of the
2018 ACM Conference on Economics and Computation, pp. 321-338. ACM, 2018.
James P Bailey and Georgios Piliouras. Multi-agent learning in network zero-sum games is a Hamiltonian
system. In Int. Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2019.
9
Published as a conference paper at ICLR 2019
David Balduzzi, SCbastien Racan论re, James Martens, Jakob N. Foerster, Karl Tuyls, and Thore GraePeL The
mechanics of n-player differentiable games. In ICML, 2018.
Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert
Spaces. SPringer, New York, NY, USA, 2 edition, 2017.
Srinadh BhojanaPalli, Behnam Neyshabur, and Nati Srebro. Global oPtimality of local search for low rank
matrix recovery. In Advances in Neural Information Processing Systems, pp. 3873-3881, 2016.
LCon Bottou. Online learning and stochastic aPProximations. On-line learning in neural networks, 17(9):142,
1998.
Mario Bravo, David S. Leslie, and Panayotis Mertikopoulos. Bandit learning in concave N-person games. In
NIPS ’18: Proceedings of the 32nd International Conference on Neural Information Processing Systems,
2018.
Lev M. Bregman. The relaxation method of finding the common point of convex sets and its application to the
solution of problems in convex programming. USSR Computational Mathematics and Mathematical Physics,
7(3):200-217, 1967.
SCbastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine
Learning, 8(3-4):231-358, 2015.
Gong Chen and Marc Teboulle. Convergence analysis of a proximal-like minimization algorithm using Bregman
functions. SIAM Journal on Optimization, 3(3):538-543, August 1993.
Johanne Cohen, AmClie HCliou, and Panayotis Mertikopoulos. Learning with bandit feedback in potential games.
In NIPS ’17: Proceedings of the 31st International Conference on Neural Information Processing Systems,
2017.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism.
In ICLR ’18: Proceedings of the 2018 International Conference on Learning Representations, 2018.
Francisco Facchinei and Jong-Shi Pang. Finite-Dimensional Variational Inequalities and Complementarity
Problems. Springer Series in Operations Research. Springer, 2003.
Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic
Behavior, 29:79-103, 1999.
Drew Fudenberg and David K. Levine. The Theory of Learning in Games, volume 2 of Economic learning and
social evolution. MIT Press, Cambridge, MA, 1998.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points online stochastic gradient for
tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified
geometric analysis. In ICML, 2017.
Gauthier Gidel, Hugo Berard, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on
generative adversarial networks. https://arxiv.org/pdf/1802.10551.pdf, February 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. In Advances in Neural Information Processing Systems 30 (NIPS 2017),
pp. 5769-5779. Curran Associates, Inc., December 2017. URL https://papers.nips.cc/paper/
7159-improved-training-of-wasserstein-gans. arxiv: 1704.00028.
P. Hall and C. C. Heyde. Martingale Limit Theory and Its Application. Probability and Mathematical Statistics.
Academic Press, New York, 1980.
Anatoli Juditsky, Arkadi Semen Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic
mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 12 2014.
G. M. Korpelevich. The extragradient method for finding saddle points and other problems. Ekonom. i Mat.
Metody, 12:747-756, 1976.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to
minimizers. In Conference on Learning Theory, pp. 1246-1257, 2016.
Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin Recht.
First-order methods almost always avoid strict saddle points. Mathematical Programming, 2019.
Panayotis Mertikopoulos and Mathias Staudigl. On the convergence of gradient-like flows with noisy gradient
input. SIAM Journal on Optimization, 28(1):163-197, January 2018.
Panayotis Mertikopoulos, Christos H. Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized
learning. In SODA ’18: Proceedings of the 29th annual ACM-SIAM Symposium on Discrete Algorithms,
2018.
10
Published as a conference paper at ICLR 2019
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually
converge? In ICML, 2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. ICLR
Proceedings, 2017.
Arkadi Semen Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with
Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal
on Optimization, 15(1):229-251, 2004.
Arkadi Semen Nemirovski and David Berkovich Yudin. Problem Complexity and Method Efficiency in Opti-
mization. Wiley, New York, NY, 1983.
Arkadi Semen Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approxi-
mation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.
Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and related problems.
Mathematical Programming, 109(2):319-344, 2007.
Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):
221-259, 2009.
Muhammad Aslam Noor, Eisa Al-Said, Khalida Inayat Noor, and Yonghong Yao. Extragradient methods for
solving nonconvex variational inequalities. Journal of Computational and Applied Mathematics, 235(9):
3104-3108, March 2011.
Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update with constant
step-size in congestion games: Convergence, limit cycles and chaos. In NIPS ’17: Proceedings of the 31st
International Conference on Neural Information Processing Systems, 2017.
Ioannis Panageas and Georgios Piliouras. Gradient descent only converges to minimizers: Non-isolated critical
points and invariant regions. In Innovations of Theoretical Computer Science (ITCS), 2017.
Christos Papadimitriou and Georgios Piliouras. From nash equilibria to chain recurrent sets: An algorithmic
solution concept for game theory. Entropy, 20(10), 2018. ISSN 1099-4300. doi: 10.3390/e20100782. URL
http://www.mdpi.com/1099-4300/20/10/782.
Georgios Piliouras and Jeff S Shamma. Optimization despite chaos: Convex relaxations to complex limit sets via
Poincare recurrence. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms,
pp. 861-873. SIAM, 2014.
Georgios Piliouras, Carlos Nieto-Granda, Henrik I. Christensen, and Jeff S. Shamma. Persistent Patterns:
Multi-agent learning beyond equilibrium and utility. In AAMAS, PP. 181-188, 2014.
A. Radford, L. Metz, and S. Chintala. UnsuPervised RePresentation Learning with DeeP Convolutional
Generative Adversarial Networks. In International Conference on Learning Representations (ICLR), 2016.
Alexander Rakhlin and Karthik Sridharan. OPtimization, learning, and games with Predictable sequences. In
NIPS ’13: Proceedings of the 26th International Conference on Neural Information Processing Systems,
2013.
RalPh Tyrrell Rockafellar. Convex Analysis. Princeton University Press, Princeton, NJ, 1970.
Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. ImProved
techniques for training gans. In NIPS, PP. 2234-2242, 2016.
Yuzuru Sato, Eizo Akiyama, and J. Doyne Farmer. Chaos in learning a simPle two-Person game. Proceedings
of the National Academy of Sciences, 99(7):4748-4751, 2002. doi: 10.1073/Pnas.032086299. URL http:
//www.pnas.org/content/99/7/4748.abstract.
Shai Shalev-Shwartz. Online learning and online convex oPtimization. Foundations and Trends in Machine
Learning, 4(2):107-194, 2011.
Ju Sun, Qing Qu, and John Wright. ComPlete dictionary recovery over the sPhere i: Overview and the geometric
Picture. IEEE Transactions on Information Theory, 63(2):853-884, 2017a.
Ju Sun, Qing Qu, and John Wright. ComPlete dictionary recovery over the sPhere ii: Recovery by riemannian
trust-region method. IEEE Transactions on Information Theory, 63(2):885-914, 2017b.
T. Tieleman and G. Hinton. Lecture 6.5 - rmsProP, coursera: Neural networks for machine learning. 2012.
Abhay Yadav, Sohil Shah, Zheng Xu, David Jacobs, and Tom Goldstein. Stabilizing adversarial nets with Predic-
tion methods. In ICLR ’18: Proceedings of the 2018 International Conference on Learning Representations,
2018.
Zhengyuan Zhou, Panayotis MertikoPoulos, Nicholas Bambos, StePhen Boyd, and Peter W. Glynn. On the
convergence of mirror descent beyond stochastic convex Programming. https://arxiv.org/abs/1706.
05681, 2017a.
Zhengyuan Zhou, Panayotis MertikoPoulos, Nicholas Bambos, StePhen Boyd, and Peter W. Glynn. Stochastic
mirror descent for variationally coherent oPtimization Problems. In NIPS ’17: Proceedings of the 31st
International Conference on Neural Information Processing Systems, 2017b.
11
Published as a conference paper at ICLR 2019
A	Coherent saddle-point problems
We begin our discussion with some basic results on coherence:
Proposition A.1. If f is convex-concave, (SP) is coherent. In addition, if f is strictly convex-concave,
(SP) is strictly coherent.
Proof. Let X* be a solution point of (SP). Since f is convex-concave, first-order optimality gives
〈91(片，x2), x 1 - x 1〉=〈VX1 f (x*, x2), x 1 - x 1i ≥ 0,	(A.1a)
and
〈g2(x*1,x2*),x2-x*2〉 = 〈-Vx2 f (x1*, x*2), x2 - x2*〉 ≥0.	(A.1b)
Combining the two, we readily obtain the (Stampacchia) variational inequality
〈g(x*), x - x*〉 ≥ 0 for all x ∈ X.	(A.2)
In addition to the above, the fact that f is convex-concave also implies that g(x) is monotone in the
sense that
〈g(x0) - g(x), x0 - x〉 ≥ 0	(A.3)
for all x, X0 ∈ X [Bauschke & Combettes, 2017]. Thus, setting X0 — X* in (A.3) and invoking (SVI),
we get
〈g(x), x - x*〉 ≥ 〈g(x*), x - x*〉 ≥ 0,	(A.4)
i.e., (MVI) is satisfied.
To establish the converse implication, focus for concreteness on the minimizer, and note that (MVI)
implies that
〈g1(x), x1 - x*1〉 ≥ 0 for all x1 ∈ X1.	(A.5)
Now, if we fix some x1 ∈ X1 and consider the function φ(t) = f(x*1 + t(x1 - x*1), x2*), the inequality
(A.5) yields
φ0 (t) = 〈g(x*1 + t(x1 - x*1),x*2),x1 - x*1〉
=ɪhg(x 1	+ t(x 1 一 x 1), x2), x*	+ t(x 1	一	x 1	一	x 1〉≥	0,	(A.6)
for all t ∈ [0, 1]. This implies that φ is nondecreasing, so f(x1, x*2) = φ(1) ≥ φ(0) = f(x*1, x*2).
The maximizing component follows similarly, showing that x* is a solution of (SP) and, in turn,
establishing that (SP) is coherent.
For the strict part of the claim, the same line of reasoning shows that if 〈g(x), x 一 x*〉 = 0 for some x
that is not a saddle-point of f, the function φ(t) defined above must be constant on [0, 1], indicating
in turn that f cannot be strictly convex-concave, a contradiction.
We proceed to show that the solution set of a coherent saddle-point problem is closed (we will need
this regularity result in the convergence analysis of Appendix C):
Lemma A.2. Let X* denote the solution set of (SP). If (SP) is coherent, X* is closed.
Proof. Let x*n, n = 1, 2, . . . , be a sequence of solutions of (SP) converging to some limit point x* ∈ X .
To show that X* is closed, it suffices to show that x* ∈ X .
Indeed, given that (SP) is coherent, every solution thereof satisfies (MVI), so We have〈g(x), X—Xn〉≥ 0
for all x ∈ X . With xn* → x* as n → ∞, it follows that
〈g(x), x 一 x*〉 = lim 〈g(x), x 一 x*n〉 ≥ 0 for all x ∈ X,	(A.7)
n→∞
i.e., x* satisfies (MVI). By coherence, this implies that x* is a solution of (SP), as claimed.
12
Published as a conference paper at ICLR 2019
B Properties of the Bregman divergence
In this appendix, we provide some auxiliary results and estimates that are used throughout the
convergence analysis of Appendix C. Some of the results we present here (or close variants thereof)
are not new [see e.g., Nemirovski et al., 2009; Juditsky et al., 2011]. However, the hypotheses used to
obtain them vary wildly in the literature, so we provide all the necessary details for completeness.
To begin, recall that the Bregman divergence associated to a K-strongly convex distance-generating
function h : X → ’ is defined as
D(P, x) = h(P) - h(X) -〈Vh(x), P - X〉	(B.1)
With Vh(X) denoting a continuous selection of ∂h(X). The induced prox-mapping is then given by
PX(y) = arg min{〈y, X - X0〉 + D(X0, X)}
X0∈X
= arg max{〈y + Vh(X), X0〉 - h(X0)}	(B.2)
X0∈X
and is defined for all X ∈ dom ∂ h, y ∈ Y (recall here that Y ≡ V * denotes the dual of the ambient
vector space V). In what follows, we will also make frequent use of the convex conjugate h*: Y → '
of h, defined as
h*(y) = max{〈y, X〉 - h(X)}.	(B.3)
X∈X
By standard results in convex analysis [Rockafellar, 1970, Chap. 26], h* is differentiable on Y and its
gradient satisfies the identity
Vh*(y) = arg max{〈y, X〉 - h(X)}.	(B.4)
X∈X
For notational convenience, we will also write
Q(y) = Vh*(y)	(B.5)
and we will refer to Q : Y → X as the mirror maP generated by h. All these notions are related as
follows:
Lemma B.1. Let h be a distance-generating function on X. Then, for all X ∈ dom ∂h, y ∈ Y, we
have:
a)	X = Q(y)	u⇒	y ∈ ∂h(X).	(B.6a)
b)	X+ = Px(y)	-	Vh(x) + y ∈ ∂h(X+)	- X+	= Q(Vh(x) + y).	(B.6b)
Finally, if X = Q(y) and P ∈ X, we have
〈Vh(X), X - P〉 ≤ 〈y,X - P〉.	(B.7)
Remark. By (B.6b), we have ∂h(X+) , 0, i.e., X+ ∈ dom ∂h. As a result, the update rule X J Pχ(y) is
well-Posed, i.e., it can be iterated in perpetuity.
Proof of Lemma B.1. For (B.6a), note that X solves (B.3) if and only if y - ∂h(X) 3 0, i.e., if and only
if y ∈ ∂h(X). Similarly, comparing (B.2) with (B.3), it follows that X+ solves (B.2) if and only if
Vh(X) + y ∈ ∂h(X+), i.e., if and only if X+ = Q(Vh(X) + y).
For (B.7), by a simple continuity argument, it suffices to show that the inequality holds for interior
P ∈ X ◦ . To establish this, let
φ(t) = h(X + t(P - X)) - [h(X) + 〈y, X + t(P - X)〉].	(B.8)
Since h is strongly convex and y ∈ ∂h(X) by (B.6a), it follows that φ(t) ≥ 0 with equality if and
only if t = 0. Since ψ(t) = 〈Vh(X + t(P - X)) - y,P - X〉 is a continuous selection of subgradients
of φ and both φ and ψ are continuous on [0, 1], it follows that φ is continuously differentiable with
φ0 = ψ on [0, 1]. Hence, with φ convex and φ(t) ≥ 0 = φ(0) for all t ∈ [0, 1], we conclude that
φ0 (0) = 〈Vh(X) - y, P - X〉 ≥ 0, which proves our assertion.
We continue with some basic bounds on the Bregman divergence before and after a prox step. The
basic ingredient for these bounds is a generalization of the (Euclidean) law of cosines which is known
in the literature as the “three-point identity” [Chen & Teboulle, 1993]:
13
Published as a conference paper at ICLR 2019
Lemma B.2. Let h be a distance-generating function on X. Then, for all p ∈ X and all x, x0 ∈
dom ∂h, we have
D(P, XZ) = D(p, x) + D(x, X) +〈Vh(X0) - Vh(x), X - P〉.	(B.9)
Proof. By definition, we have:
D(p, X0) = h(p) - h(X0) - 〈Vh(X0), p - X0〉
D(p, X) = h(p) - h(X) - 〈Vh(X), p - X〉	(B.10)
D(X, X0) = h(X) - h(X0) - 〈Vh(X0), X - X0〉.
Our claim then follows by adding the last two lines and subtracting the first.
With this identity at hand, we have the following series of upper and lower bounds:
Proposition B.3. Let h be a K-strongly conveX distance-generating function on X, fiX some p ∈ X,
and let X+ = PX (y) for X ∈ dom ∂h, y ∈ Y. We then have:
D(P, x) ≥ KKkx — Pk2.	(B.11a)
D(p, X+) ≤ D(p, X) - D(X+, X) + 〈y, X+ - p〉	(B.11b)
≤ D (p, χ) + hy, χ — p〉+ ɪ kyk2	(B.ιic)
2K
Proof of (B.11a). By the strong convexity of h, we get
h(P) ≥ h(x) +〈Vh(x), P — x〉+ KK kP — Xk2	(B.12)
so (B.11a) follows by gathering all terms involving h and recalling the definition of D(p, x).
Proof of (B.11b) and (B.11c). By the three-point identity (B.9), we readily obtain
D(P, x) = D(P,x+) +D(x+,x) + 〈Vh(x) -Vh(x+),x+ -P〉.	(B.13)
In turn, this gives
D(P, x+) = D(P, x) - D(x+, x) + 〈Vh( x+) - Vh( x), x+ - P〉
≤ D(P, x) - D(x+, x) + 〈y, x+ - P〉,	(B.14)
where, in the last step, we used (B.7) and the fact that x+ = Px(y), so Vh(x) + y ∈ ∂h(x+). The above
is just (B.11b), so the first part of our proof is complete.
For (B.11c), the bound (B.14) gives
D(P, x+) ≤ D(P, x) + 〈y, x - P〉 + 〈y, x+ - x〉 - D(x+, x).	(B.15)
Therefore, by Young’s inequality [Rockafellar, 1970], we get
hy,X + - X〉≤ kx+x + — xIl2 + TT^kyk2,	(B.16)
2	2K
and hence
1K
D(P,x ) ≤ D(P, x) + hy,x — P〉+ ^ky∣2 + 不1 X — XI2 — D(X ,X)
2K	2
≤ D (P, x) + hy, x — p〉+ ɪ ky∣2,	(B.17)
2K
with the last step following from Lemma B.1 applied to x in place of P.
The first part of Proposition B.3 shows that Xn converges to P if D(P, Xn) → 0. However, as
we mentioned in the main body of the paper, the converse may fail: in particular, we could have
liminfn→∞ D(P, Xn) > 0 even if Xn → P. To see this, let X be the L2 ball of ’d and take h(x)=
— 1 — I xI22 . Then, a straightforward calculation gives
D(P, x) = 1 —〈P,Xi	(B.18)
1	— I xI22
14
Published as a conference paper at ICLR 2019
whenever kpk2 = 1. The corresponding level sets Lc(P) = {X ∈ ’d : D(P, x) = C} of D(P, ∙) are given
by the equation
1-〈p,xi = C Ji - kXk2,	(B.19)
which admits P as a solution for all c ≥ 0 (so P belongs to the closure ofLc(P) even though D(P, P) = 0
by definition). As a result, under this distance-generating function, it is possible to have Xn → p even
when lim infn→∞ D(p, Xn) > 0 (simply take a sequence Xn that converges to p while remaining on the
same level set of D). As we discussed in the main body of the paper, such pathologies are discarded
by the Bregman reciprocity condition
D(p, Xn) → 0 whenever Xn → p.	(B.20)
This condition comes into play at the very last part of the proofs of Theorems 3.1 and 4.1; other than
that, we will not need it in the rest of our analysis.
Finally, for the analysis of the OMD algorithm, we will need to relate prox steps taken along different
directions:
Proposition B.4. Let h be a K-strongly Convex distanCe-generating funCtion on X and fix some
p ∈ X, x ∈ dom ∂h. Then:
a)	For all y1 , y2 ∈ Y, we have:
kPx(y2) - Px(y1)k ≤ Kky2 - y1k*,	(B.21)
i	.e., Px is (1/K)-LipsChitz.
b)	In addition, letting x1+ = Px (y1) and x2+ = Px (y2), we have:
D(p, x2+) ≤ D(p, x) + hy2, x1+ - pi + [hy2, x2+ - x1+i - D(x2+, x)]	(B.22a)
1K
≤ D(P, x) + hy2, x 1 - Pi + 7Γ77ky2- y1k* -不kX1 - XIl .	(B.22b)
2K	2
Proof. We begin with the proof of the Lipschitz property of Px. Indeed, for all P ∈ X , (B.7) gives
〈Vh(x +) -Vh(x) - y1, x + - pi ≤ 0,	(B.23a)
and
〈Vh(x2+) - Vh(x) - y2, x2+ -Pi ≤ 0.	(B.23b)
Therefore, setting P J X + in (B.23a), P J X + in (B.23b) and rearranging, we obtain
〈Vh(x +) - Vh(x +), x+-x +i≤ hy2 - y1, x + - x +i.	(B.24)
By the strong convexity of h, we also have
Kkx2+ - x1+k2 ≤ 〈Vh(x2+) -Vh(x1+),x2+ - x1+i.	(B.25)
Hence, combining (B.24) and (B.25), we get
KkX + - X +k2 ≤ hy2 - y1, X + - X +i ≤ ky2 - y1k*kX + - X +k,	(B.26)
and our assertion follows.
For the second part of our claim, the bound (B.11b) of Proposition B.3 applied to x2+ = Px(y2) readily
gives
D(P, x2+) ≤ D(P, x) - D(x2+ , x) + 〈y2, x2+ - Pi
= D(P, x) + 〈y2, x1+ - Pi + [〈y2, x2+ - x1+i - D(x2+, x)]	(B.27)
thus proving (B.22a). To complete our proof, note that (B.11b) with P J x2+ gives
D(x+, x +) ≤ D(x +, x) + hy1, x + - x +i- D(x +, x),	(B.28)
or, after rearranging,
D(x2+, x) ≥ D(x2+, x1+) + D(x1+, x) + 〈y1, x2+ - x1+i.	(B.29)
We thus obtain
〈y2,x2+ - x1+i - D(x2+, x) ≤ 〈y2 - y1,x2+ - x1+i - D(x2+,x1+) - D(x1+,x)
≤ ky22Ky" + KkX+ - X +k2 - KkX+ - X +k2 - K kX + - Xk2
1K
≤ y77ky2 - y1k* - ^ykX1 - xk ,	(B.30)
2K	2
where we used Young’s inequality and (B.11a) in the second inequality. The bound (B.22b) then
follows by substituting (B.30) in (B.27).
15
Published as a conference paper at ICLR 2019
C Convergence analysis of mirror descent
We begin by recalling the definition of the mirror descent algorithm. With notation as in the previous
section, the algorithm is defined via the recursive scheme
Xn+1 = PXn (-Yng n ),	(MD)
where Yn is a variable step-size sequence and gn is the calculated value of the gradient vector g(Xn) at
the n-th stage of the algorithm. As we discussed in the main body of the paper, the gradient input
sequence gn of (MD) is assumed to satisfy the standard oracle assumptions
a)	Unbiasedness:	…[gn | Fn] = g(Xn).
b)	Finite mean square: …[kgnk21 Fn] ≤ G2 for some finite G ≥ 0.
where Fn represents the history (natural filtration) of the generating sequence Xn up to stage n
(inclusive).
With this preliminaries at hand, our convergence proof for (MD) under strict coherence will hinge on
the following results:
Proposition C.1. Suppose that (SP) is coherent and (MD) is run with a gradient oracle satisfying
(3.6) and a variable step-size γn SUCh that P∞=ι Y2 < ∞. Ifx* ∈ X is a solution of (SP), the Bregman
divergence D(x*, Xn) converges (a.s.) to a random variable D(x*) with …[D(x*)] < ∞.
Proposition C.2. Suppose that (SP) is striCtly Coherent and (MD) is run with a gradient oraCle
satisfying (3.6) and a step-size Yn such that Pn∞=1 Yn = ∞ and Pn∞=1 Yn2 < ∞. Then, with probability 1,
there exists a (possibly random) solution x* of (SP) such that lim infn→∞ D(x*, Xn) = 0.
Proposition C.1 can be seen as a “dichotomy” result: it shows that the Bregman divergence is an
asymptotic constant of motion, so (MD) either converges to a saddle-point x* (if D(x*) = 0) or to
some nonzero level set of the Bregman divergence (with respect to x*). In this way, Proposition C.1
rules out more complicated chaotic or aperiodic behaviors that may arise in general - for instance, as
in the analysis of Palaiopanos et al. [2017] for the long-run behavior of the multiplicative weights
algorithm in two-player games. However, unless this limit value can be somehow predicted (or
estimated) in advance, this result cannot be easily applied. This is the main role of Proposition C.2: it
shows that (MD) admits a subsequence converging to a solution of (SP) so, by (B.20), the limit of
D(x*, Xn) must be zero.
Our first step is to prove Proposition C.2. To do this, we first recall the following law of large numbers
for L2 martingales:
Theorem (Hall & Heyde, 1980, Theorem 2.18). Let Yn = Pkn=1 ζk be a martingale and Tn a non-
decreasing sequence such that limn→∞ τn = ∞. Then,
lim Yn/τn = 0 (a.S.)	(C.1)
n→∞
on the setP∞= τ-2 …[42 | Fn-ι] < ∞.
With this in place, we have:
Proof of Proposition C.2. We begin with the technical observation that the solution set X* of (SP) is
closed - and hence, compact (cf. Lemma A.2 in Appendix A). Clearly, if X* = X, there is nothing to
show; hence, without loss of generality, we may assume in what follows that X* , X.
Assume now ad absurdum that, with positive probability, the sequence Xn generated by (MD) admits
no limit points in X* . Conditioning on this event, and given that X* is compact, there exists a
(nonempty) compact set C ⊂ X such that C ∩ X * = 0 and Xn ∈ C for all sufficiently large n.
Moreover, letting p be as in Definition 2.1, we have hg(x), x - pi > 0 whenever x ∈ C. Therefore, by
the continuity of g and the compactness of X* and C, there exists some a > 0 such that
hg(x), x - pi ≥ a for all x ∈ C.	(C.2)
To proceed, let Dn = D(p, Xn). Then, by Proposition B.3, we have
Y2
Dn+1 = D(P, PXn(-Yngn)) ≤ D(P, Xn) - Ynhgn, Xn- Pi + 7Γ77k⅛gnk
16
Published as a conference paper at ICLR 2019
γ2
=Dn - Ynhg( Xn ), Xn- P i - Y n h Un +1, Xn- P i + ^^b∖∖g^ n IE
2K
Y2
≤ Dn + Ynξn+1 + 笠kgnk2,	(C.3)
2K
where, in the last line, We set Un+1 = gn - g(Xn), ξn +1 = 一〈Un +ι,Xn - P〉，and We invoked the
assumption that (SP) is coherent. Hence, telescoping (C.3) yields the estimate
n	n	n2
Dn +1 ≤ D1-2 Yk〈g(Xk), Xk-P〉+ 2 Ykξk +1 + 2 养 kgkk2.
k=1	k=1	k=1 2K
Subsequently, letting τn = Pkn=1 Yk and using (C.2), We obtain
n -n [	P n=1 Yk ξk +1	(2 K )-1 P n=1 Ykkgkk21
Dn+1 ≤ D1 - Tn a----------------------------------.
(C.4)
τn
τn
(C.5)
By the unbiasedness hypothesis of (3.6) for Un, We have …[ξn+11 Fn]=〈…[Un +11 Fn],Xn - P〉= 0
(recall that Xn is Fn -measurable by construction). Moreover, since Un is bounded in L2 and Yn is `2
summable (by assumption), it folloWs that
∞∞
X Y n E[ξ2+1 IFn ] ≤ X Y nkXn -Pk2 E[k Un +1I2IFn ]
n=1	n=1
∞
≤ diam(X )2σ2 X Yn2 < ∞.	(C.6)
n=1
Therefore, by the laW of large numbers for L2 martingales stated above [Hall & Heyde, 1980,
Theorem 2.18], We conclude that τn-1 Pkn=1 Ykξk+1 converges to 0 With probability 1.
Finally, for the last term of (C.4), let Sn +1 = Pn=1 Y2kgkk2. Since gk is Fn-measurable for all
k = 1, 2, . . . ,n - 1, We have
-n-1	-
E[ Sn +1∣Fn ] = EX Ykkg k k2 + Y nkg n k2 Fn = Sn + Y2 E[kgn k2 I Fn ] ≥ Sn,	(C.7)
k=1
i.e., Sn is a submartingale With respect to Fn. Furthermore, by the laW of total expectation, We also
have
n∞
E[Sn+1] = E[E[Sn +11Fn]] ≤ GG X Yn ≤ G2 X Yn < ∞,	(C.8)
k=1	k=1
so Sn is bounded in L1. Hence, by Doob’s submartingale convergence theorem [Hall & Heyde, 1980,
Theorem 2.5], We conclude that Sn converges to some (almost surely finite) random variable S∞ With
E[S∞] < ∞, implying in turn that limn→∞ Sn+1∕τn = 0 (a.s.).
Applying all of the above, the estimate (C.4) gives Dn +1 ≤ D1 - aTn/2 for sufficiently large n, so
D(P,Xn) → -∞, a contradiction. Going back to our original assumption, this shoWs that, at least one
of the limit points of Xn must lie in X* (a.s.), as claimed.	■
We noW turn to the proof of Proposition C.1:
Proof of ProPosition C.1. Let x* ∈ X* be a limit point of Xn, as guaranteed by Proposition C.2, and
let Dn = D(x*, Xn). Then, by Proposition B.3, We have
D — D r* p z_ -《D * y._ X g _ γ*∖ , YL ,r l,2
Dn+1 = D(X , PXn ( y ng n)) ≤ D(X , Xn) Yn hg n, Xn x〉+ C Okg n k
2K
Y2
=Dn - Ynhg(Xn), Xn- X*〉- Yn〈 Un +1, Xn - X*〉+ 总 kgnk2	(C.9)
2K
and hence, for large enough n:
Y2
Dn +1 ≤ Dn + Y nξn+1 + ^^kg n k*,
2K
(C.10)
17
Published as a conference paper at ICLR 2019
where We used the ansatz that〈g(Xn), Xn - X*〉≤ 0 for sufficiently large n (to be proved below),
and, as in the proof of Proposition C.2, we set Un+1 = gn - g(Xn), ξn+1 = 一〈Un +1, Xn - x*〉. Thus,
conditioning on Fn and taking expectations, we get
γ2	G2
…[Dn +1 | Fn] ≤	Dn	+	…[ξn+1	| Fn]	+ 守…[kgnk* |	Fn]	≤	Dn	+ 7Γ^Yn,	(Cll)
2K	2K
where we used the oracle assumptions (3.6) and the fact that Xn is Fn-measurable (by definition).
Now, letting Rn = Dn + (2K)-1G2 Pk∞=n γk2, the estimate (C.10) gives
G2 ∞	G2 ∞
…[Rn+1 | Fn]	=…[Dn+1	| Fn]	+ TΓT7 /,	Yk	≤	Dn +	/, Yk	= Rn,	(C.12)
2K	2K
k=n+1	k=n
i.e., Rn is an Fn-adapted supermartingale. Since Pn∞=1 Yn2 < ∞, it follows that
G2 ∞
…[Rn]=…[…[Rn | Fn-1]] ≤ …[Rn-1〕≤ …≤ …[尺1〕≤ …。]]+ — Y γ2 < ∞,	(C.13)
2K
n=1
i.e., Rn is uniformly bounded in L1. Thus, by Doob’s convergence theorem for supermartingales [Hall
& Heyde, 1980, Theorem 2.5], it follows that Rn converges (a.s.) to some finite random variable R∞
with …[R∞] < ∞. In turn, by inverting the definition of Rn, this shows that Dn converges (a.s.) to
some random variable D(X*) with …[D(X*)] < ∞, as claimed.
It remains to be shown that〈g(Xn), Xn - X*〉≥ 0 for sufficiently large n. By Definition 2.1, this
amounts to showing that, for all large enough n, Xn lies in a neighborhood U of X* such that (MVI)
holds. Since X* has been chosen so that liminf D(X*, Xn) = 0, it follows that, for all ε > 0, there
exists some n0 such that Pn∞=n0 Yn2 < ε and Xn0 ∈ U. Hence, arguing in the same way as in the proof
of Theorem 5.2 of Zhou et al. [2017a], we conclude that •(Xn ∈ U for all n ≥ n0) = 1, implying in
turn that 〈g(Xn), Xn - X*〉 ≥ 0 for all n ≥ n0. This proves our last claim and concludes our proof.
With all this at hand, we are finally in a position to prove our main result for (MD):
Proof of Theorem 3.1(a). Proposition C.2 shows that, with probability 1, there exists a (possibly
random) solution X* of (SP) such that lim infn→∞kXn - X*k = 0 and, hence, lim infn→∞ D(X*, Xn) = 0
(by Bregman reciprocity). Since limn→∞ D(X*,Xn) exists with probability 1 (by Proposition C.1), it
follows that limn→∞ D(X*, Xn) = lim infn→∞ D(X*, Xn) = 0, i.e., Xn converges to X*.
We proceed with the negative result hinted at in the main body of the paper, namely the failure of
(MD) to converge under null coherence:
Proof of Theorem 3.1(b). The evolution of the Bregman divergence under (MD) satisfies the identity
D(X*, Xn +1) = D(X*, Xn) + D(Xn, Xn+1) + Ynhgn, Xn- X*i
= D(X*, Xn) + D(Xn, Xn+1) + 〈Un+1, Xn -X*〉	(C.14)
where, in the last line, we used the null coherence assumption 〈g(X), X - X*〉 = 0 for all X ∈ X. Since
D(Xn, Xn+1) ≥ 0, taking expecations above shows that D(X*,Xn) is nondecreasing, as claimed.
With Theorem 3.1 at hand, the proof of Corollary 3.2 is an immediate consequence of the fact that
strictly convex-concave problems satisfy strict coherence (Proposition A.1). As for Corollary 3.3, we
provide below a more general result for two-player, zero-sum finite games.
To state it, let Ai = {1, . . . , Ai }, i = 1, 2, be two finite sets of pure strategies, and let Xi = ∆(Ai)
denote the set of miXed strategies of player i. A finite, two-player zero-sum game is then defined by
a matrix M ∈ 'A1×A2 so that the loss of Player 1 and the reward of Player 2 in the mixed strategy
profile X = (X1, X2) ∈ X are concurrently given by
f(X1,X2) =X1>MX2	(C.15)
Then, writing Γ ≡ Γ(A1 , A2, M) for the resulting game, we have:
18
Published as a conference paper at ICLR 2019
Figure 5: Trajectories of vanilla and optimistic mirror descent in a zero-sum game of Matching Pennies (left
and right respectively). Colors represent the contours of the objective, f(x1, x2) = (x1 - 1/2)(x2 - 1/2).
Proposition C.3. Let Γ be a two-player zero-sum game with an interior Nash equilibrium x*. If
Xι , X* and (MD) is run with exact gradient input (σ2 = 0), we have limn→∞ D(X*, Xn) > 0. If in
addition, P ∞=ι Y 2 < ∞, lim n →∞ D (x *, Xn) is finite.
Remark. Note that non-convergence does not require any summability assumptions on γn .
In words, Proposition C.3 states that (MD) does not converge in finite zero-sum games with a unique
interior equilibrium and exact gradient input: instead, Xn cycles at positive Bregman distance from
the game’s Nash equilibrium. Heuristically, the reason for this behavior is that, for small γ → 0, the
incremental step Vγ(x) = Pχ(一γg(X)) 一 X of (MD) is essentially tangent to the level set of D(X*, ∙)
that passes through x.4 For finite γ > 0, things are even worse because Vγ(x) points noticeably away
from x, i.e., towards higher level sets of D. As a result, the “best-case scenario” for (MD) is to orbit
x* (when γ → 0); in practice, for finite γ, the algorithm takes small outward steps throughout its
runtime, eventually converging to some limit cycle farther away from x* .
We make this intuition precise below (for a schematic illustration, see also Fig. 1 above):
Proofof Proposition C.3. Write vi(x) = 一Mx2 and v2(X) = X>M for the players’ payoff vectors
under the mixed strategy profile X = (X1, X2). By construction, We have g(X) = -(vi(X),v2(X)).
Furthermore, since x* is an interior equilibrium of f, elementary game-theoretic considerations show
that v1 (X*) and v2(X*) are both proportional to the constant vector of ones. We thus get
hg(X), X 一 X*i = hv1(X),X1 一 X1*i + hv2(X),X2 一 X*2i
=-x> Mx2 + (x 1)>Mx2 + x> Mx2 - X> Mx2
= 0,	(C.16)
Where, in the last line, We used the fact that X* is interior. This shoWs that f satisfies null coherence,
so our claim folloWs from Theorem 3.1(b).
For our second claim, arguing as above and using (B.11c), We get
D(x*, Xn+1) ≤ D(x*, Xn) + Ynhg(Xn), Xn- x*i + nJkg llg(Xn)k2
2K
Y2G2
≤ D (X *, Xn ) + ⅞f
2K
with G = max/€工,X2∈X2∣∣(-MX2, X>M)k*. Telescoping this last bound yields
∞ Y2G2
sup d(x*, Xn) ≤ D(X*, XI) + ɪʒ 2κ < ∞,
(C.17)
(C.18)
4This observation was also the starting point of Mertikopoulos et al. [2018] who showed that FTRL in
continuous time exhibits a similar cycling behavior in zero-sum games with an interior equilibrium.
19
Published as a conference paper at ICLR 2019
so D(X*, Xn) is also bounded from above. Therefore, with D(X*, Xn) nondecreasing, bounded from
above and D(x*, X1) > 0, it follows that limn→∞ D(x*, Xn) > 0, as claimed.	■
D Convergence analysis of optimistic mirror descent
We now turn to the optimistic mirror descent (OMD) algorithm, as defined by the recursion
Xn+1/2 = Px, (-Y n g n )
Xn +1 = PXn (-Y n g n+1/2)
(OMD)
with X1 initialized arbitrarily in dom ∂h, and gn, gn+1/2 representing gradient oracle queries at the
incumbent and intermediate states Xn and Xn+1/2 respectively.
The heavy lifting for our analysis is provided by Proposition B.4, which leads to the following crucial
lemma:
Lemma D.1. Suppose that (SP) is coherent and g is L-Lipschitz continuous. With notation as above
and eXact gradient input (σ = 0), we have
D(P, Xn +1) ≤ D(P, Xn) - 2( K - YK ! kXn+1/2 - Xn II2,	(D∙1)
with p as in Definition 2.1.
Proof. Substituting X J Xn, y1 <------Yng(Xn), and y2 <---Yng(Xn+1/2) in Proposition B.4, we obtain
the estimate:
D(P, Xn+1) ≤ D(P, Xn) - Ynhg(Xn+1/2), Xn+1/2 - Pi
Y2	K
+ 7ΓT7kg( Xn +1/2)- g( Xn )k* - TTk Xn +1/2 - Xn k
2K	2
Yn2 L2	2 K	2
≤ D(P, Xn) + Go k Xn+1/2 - Xn k - ∣X∖∖ Xn +1/2 - Xn k ,	(D.2)
2K	2
where we used the fact that g is L-Lipschitz and that P is a solution of (SP) such that (MVI) holds for
all X ∈ X .	■
We are now finally in a position to prove Theorem 4.1 (reproduced below for convenience):
Theorem. SuPPose that (SP) is coherent and g is L-LiPschitz continuous. If (OMD) is run with
eXact gradient inPut and a steP-size sequence Yn such that
0 < limn→∞ Yn ≤ supn Yn < K/L,	(D.3)
the sequence Xn converges monotonically to a solution X* of (SP), i.e., D(X*, Xn) is non-increasing
and converges to 0.
Proof. Let P be a solution of (SP) such that (MVI) holds for all X ∈ X (that such a solution exists is
a consequence of Definition 2.1). Then, by the stated assumptions for Yn , Lemma D.1 yields
D(P, Xn+1) ≤ D(P, Xn) - 2K(1 - α2)kXn +1/2 - Xnk2,	(D.4)
where α ∈ (0,1) is such that Y2 < αK/ L for all n (that such an ɑ exists is a consequence of the
assumption that supn Yn < K/L). Now, telescoping (D.1), we obtain
1 n I	γ2 L2 λ
d(P, Xn +1) ≤ D(P, X1) - 2 与［K —K- jkXk+1/2 - Xkk2,	(D.5)
and hence:
χ(1 -)K2 ^kXk +1/2 - Xkk2 ≤ KD(P, X1).	(D.6)
With supn Yn < K/L, the above estimate readily yields Pn∞=1 kXn+1/2 -Xnk2 < ∞, which in turn implies
that kXn+1/2 - Xnk → 0 as n → ∞.
20
Published as a conference paper at ICLR 2019
By the compactness of X, We further infer that Xn admits an accumulation point X*, i.e., there exists
a subsequence n such that Xnk → x* as k → ∞. Since kXnk +1/2 - Xnkk → 0, this also implies that
Xnk+1/2 converges to X* as k → ∞. Further, by passing to a subsequence if necessary, we may also
assume Without loss of generality that γnk converges to some limit value γ > 0. Then, by the Lipschitz
continuity of the prox-mapping (cf. Proposition B.4), we readily obtain
X* = lim Xnk+1/2 = lim PXnk (-γnk g(Xnk)) = PX* (-γg(X*)),	(D.7)
k→∞	k→∞	k
i.e., X* is a solution of (SP).
Since (MVI) holds locally around X* (by Definition 2.1), the same reasoning as above shows that
D(X*, Xn +1) ≤ D(X*, Xn) - 1K(1 - α2)kXn+1/2 - Xnk2 ≤ D(X*, Xn),	(D.8)
for all sufficiently large n. This shows that D(X*, Xn) is non-decreasing; since lim infn→∞ D(X*, Xn) =
0 (by Bregman reciprocity), we ultimately conclude that limn→∞ D(X*, Xn) = 0, i.e., Xn → X*.
Our last result concerns the convergence of (OMD) in strictly coherent problems with a stochastic
gradient oracle:
Proof of Theorem 4.3. Our argument hinges on the inequality
D(X*, Xn+1) ≤ D(X*, Xn) - Ynhgn+1/2, Xn +1/2 - X*i + YYn/(2K) kgn +1/2 - gn k2	(D.9)
which is obtained from the two-point estimate (B.22b) by substituting X J X*, X1 J Xn, y1 J gn,
X + J Xn+1/2 = Px,(-γngn), /2 J gn +1/2, and x+ J Xn = PXn(-γngn+1/2). Then, working as in the
proof of Proposition C.1, we obtain the following estimate for the sequence Dn = D(X*, Xn):
Dn+1 ≤ Dn - Yn〈g(Xn +1/2), Xn+1/2 - X*i - Yn〈 Un+1, Xn- X*i + ^n kgn +1/2 - g 112
2K
≤ Dn + Ynξ++1 + K2 hkgnk2 + kg2+1/2限],	(D.10)
where U∖1 = gn+1/2 - g(Xn +1/2) denotes the martingale part of gn +1/2 and we have set ξ++1 =
〈Un+1, Xn+1/2 - X *〉. Since …[kg n k21 Xn,..., X1] and …[kg n+1/2k21 Xn +1/2,..., X1] are both bounded by
G2 , we get the bound
E[Dn +1 | Fn] ≤ Dn + GYn.	(D.11)
Following the same steps as in the proof of Proposition C.1, it then follows that Dn converges to some
limit value D∞.
To proceed, telescoping (D.10) also yields
n	n	n2
Dn +1 ≤ D1 - X Yk〈g(Xk +1/2), Xk+1/2 - X*i + X Ykξ++1 + X 2Kkgk+1/2 - gkk2.	(D.12)
k=1	k=1	k=1 2K
Each term in the above bound can be controlled in the same way as the corresponding terms in (C.4).
Thus, repeating the steps in the proof of Proposition C.2, it follows that there exists a subsequence
of Xn+1/2 (and hence also of Xn) which converges to X*. Our claim then follows by combining the
two intermediate results above in the same way as in the proof of Theorem 3.1(a); to avoid needless
repetition, we omit the details.
E	Experimental results
E.1 Adam with extra-gradient step
For most of our experiments, the method that seemed to generate the best results was Adam and
its optimistic version [Daskalakis et al., 2018]; for a pseudocode iplementation, see Algorithm 3
below. We also noticed empirically that it was more efficient to use two different sets of moment
estimates (mt, vt) and (mt0, vt0) for the first and the second gradient steps. We used this algorithm for
our experiments with both GMMs and the CelebA/CIFAR-10 datasets.
21
Published as a conference paper at ICLR 2019
Algorithm 3: Adam with extra-gradient add-on （optimistic Adam）
Compute stochastic gradient: Vθ,t
Update biased estimate of 1st momentum: mt =尸 1 m1 + （1 -尸i）Vθ,t
Update biased estimate of 2nd momentum: V t= β2V t-1 + （1 - BW2,t
Compute bias corrected IStmoment: mt = m-
ComPUte bias corrected 2nd moment: ^t = τ⅛
1-β2
Perform: θt = θt-1 - η √+
Compute stochastic gradient: 5,t
Update biased estimate of 1st momentum: m0 = β∖m1 + （1 -尸 1）Vθ∣,t
Update biased estimate of 2nd momentum: v0 =尸2V0-1 + （1 一尸1）丐，t
Compute bias corrected 1stmoment: m0 = m∙
Compute bias corrected 2nd moment: ^0 = ɪ-vet-
Perform: θt = θt-1 - η0 m
v0∕+t+e
Return θt
Table 1:	Generator and discriminator architectures for our images experiments
Generator
latent space 100 （gaussian noise）
dense 4 × 4 × 512 batchnorm ReLU
4×4 conv.T stride=2 256 batchnorm ReLU
4×4 conv.T stride=2 128 batchnorm ReLU
4×4 conv.T stride=2 64 batchnorm ReLU
4×4 conv.T stride=1 3 weightnorm tanh
Discriminator
Input Image 32×32×3
3×3 conv. stride=1 64 lReLU
3×3 conv. stride=2 128 lReLU
3×3 conv. stride=1 128 lReLU
3×3 conv. stride=2 256 lReLU
3×3 conv. stride=1 256 lReLU
3×3 conv. stride=2 512 lReLU
3×3 conv. stride=1 512 lReLU
dense 1
E.2 Experiments with standards datasets
In this section we present the results of our image experiments using OMD training techniques.
Inception and FID scores obtained by our model during training were reported in Fig. 3: as can
be seen there, the extra-gradient add-on improves the performance of GAN training and efficiently
stabilizes the model; without the extra-gradient step, performance tends to drop noticeably after
approximately 100k steps.
For ease of comparison, we provide below a collection of samples generated by Adam and optimistic
Adam in the CelebA and CIFAR-10 datasets. Especially in the case of CelebA, the generated samples
are consistently more representative and faithful to the target data distribution.
E.2.1 Network Architecture and hyperparameters
For the reproducibility of our experiments, we provide Table 1 and Table 2 the network architectures
and the hyperparameters of the GANs that we used. The architecture employed is a standard DCGAN
architecture with a 5-layer generator with batchnorm, and an 8-layer discriminator. The generated
samples were 32×32×3 RGB images.
22
Published as a conference paper at ICLR 2019
(a)	Vanilla versus optimistic Adam training in the CelebA dataset (left and right respectively).
(b)	Vanilla versus optimistic Adam training in the CIFAR-10 dataset (left and right respectively).
Figure 6: GAN training with and without an extra-gradient step in the CelebA and CIFAR-10 datasets.
Table 2:	Image experiments settings
batch size = 64 Adam learning rate = 0.0001
Adam β1 = 0.0
Adam β2 = 0.9
max iterations = 200000
WGAN-GP λ = 1.0
WGAN-GP ndis = 1
GAN objective = ’WGAN-GP’
Optimizer = ’extra-Adam’ or ’Adam’
23