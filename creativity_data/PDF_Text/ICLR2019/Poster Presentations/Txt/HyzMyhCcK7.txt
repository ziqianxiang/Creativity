Published as a conference paper at ICLR 2019
ProxQuant: Quantized Neural Networks via
Proximal Operators
Yu Bai	Yu-Xiang Wang	Edo Liberty
Stanford University	UC Santa-Barbara	Amazon AI
yub@stanford.edu	yuxiangw@cs.ucsb.edu	libertye@amazon.com
Ab stract
To make deep neural networks feasible in resource-constrained environments
(such as mobile devices), itis beneficial to quantize models by using low-precision
weights. One common technique for quantizing neural networks is the straight-
through gradient method, which enables back-propagation through the quantiza-
tion mapping. Despite its empirical success, little is understood about why the
straight-through gradient method works.
Building upon a novel observation that the straight-through gradient method is
in fact identical to Nesterov’s dual-averaging algorithm on a quantization con-
strained optimization problem, we propose a more principled alternative approach,
called ProxQuant, that formulates quantized network training as a regularized
learning problem instead and optimizes it via the prox-gradient method. Prox-
Quant does back-propagation on the underlying full-precision vector and ap-
plies an efficient prox-operator in between stochastic gradient steps to encourage
quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms
state-of-the-art results on binary quantization and is on par with state-of-the-art
on multi-bit quantization. We further perform theoretical analyses showing that
ProxQuant converges to stationary points under mild smoothness assumptions,
whereas variants such as lazy prox-gradient method can fail to converge in the
same setting.
1 Introduction
Deep neural networks (DNNs) have achieved impressive results in various machine learning
tasks (Goodfellow et al., 2016). High-performance DNNs typically have over tens of layers and
millions of parameters, resulting in a high memory usage and a high computational cost at inference
time. However, these networks are often desired in environments with limited memory and compu-
tational power (such as mobile devices), in which case we would like to compress the network into
a smaller, faster network with comparable performance.
A popular way of achieving such compression is through quantization - training networks with low-
precision weights and/or activation functions. In a quantized neural network, each weight and/or
activation can be representable in k bits, with a possible codebook of negligible additional size
compared to the network itself. For example, in a binary neural network (k = 1), the weights are
restricted to be in {±1}. Compared with a 32-bit single precision float, a quantized net reduces
the memory usage to k/32 of a full-precision net with the same architecture (Han et al., 2015;
Courbariaux et al., 2015; Rastegari et al., 2016; Hubara et al., 2017; Zhou et al., 2016; Zhu et al.,
2016). In addition, the structuredness of the quantized weight matrix can often enable faster matrix-
vector product, thereby also accelerating inference (Hubara et al., 2017; Han et al., 2016).
Typically, training a quantized network involves (1) the design of a quantizer q that maps a
full-precision parameter to a k-bit quantized parameter, and (2) the straight-through gradient
method (Courbariaux et al., 2015) that enables back-propagation from the quantized parameter back
onto the original full-precision parameter, which is critical to the success of quantized network train-
ing. With quantizer q, an iterate of the straight-through gradient method (see Figure 1a) proceeds
Code available at https://github.com/allenbai01/ProxQuant.
1
Published as a conference paper at ICLR 2019
as θt+ι = θt - ηtVL(θ)∣θ=q(θt), and q(θ) (for the converged θ) is taken as the output model. For
training binary networks, choosing q(∙) = sign(∙) gives the BinaryConnect method (CoUrbariaUx
et al., 2015).
Though appealingly simple and empirically effective, it is information-theoretically rather myste-
rious why the straight-through gradient method works well, at least in the binary case: while the
goal is to find a parameter θ ∈ {±1}d with low loss, the algorithm only has access to stochastic
gradients at {±1}d. As this is a discrete set, a priori, gradients in this set do not necessarily contain
any information about the function values. Indeed, a simple one-dimensional example (Figure 1b)
shows that BinaryConnect fails to find the minimizer of fairly simple convex Lipschitz functions in
{±1}, due to a lack of gradient information in between.
-注 Eθt))
Straight-through
q(θt)
Hθ
N• **-VL(θt)
ΘP+:
ProxQ
Iant
θt+1
quaι tize
(a)	(b)
Figure 1: (a) Comparison of the straight-through gradient method and our PROXQUANT method. The straight-
through method computes the gradient at the quantized vector and performs the update at the original real
vector; ProxQuant performs a gradient update at the current real vector followed by a prox step which
encourages quantizedness. (b) A two-function toy failure case for BinaryConnect. The two functions are
f1 (x) = |x + 0.5| - 0.5 (blue) and f-1 (x) = |x - 0.5| - 0.5 (orange). The derivatives of f1 and f-1
coincide at {-1, 1}, so any algorithm that only uses this information will have identical behaviors on these two
functions. However, the minimizers in {±1} are x1? = -1 and x?-1 = 1, so the algorithm must fail on one of
them.
In this paper, we formulate the problem of model quantization as a regularized learning problem and
propose to solve it with a proximal gradient method. Our contributions are summarized as follows.
• We present a unified framework for defining regularization functionals that encourage bi-
nary, ternary, and multi-bit quantized parameters, through penalizing the distance to quan-
tized sets (see Section 3.1). For binary quantization, the resulting regularizer is a W -shaped
non-smooth regularizer, which shrinks parameters towards either -1 or 1 in the same way
that the L1 norm regularization shrinks parameters towards 0.
• We propose training quantized networks using PROXQUANT (Algorithm 1) — a stochas-
tic proximal gradient method with a homotopy scheme. Compared with the straight-
through gradient method, ProxQuant has access to additional gradient information at
non-quantized points, which avoids the problem in Figure 1b and its homotopy scheme
prevents potential overshoot early in the training (Section 3.2).
• We demonstrate the effectiveness and flexibility of PROXQUANT through systematic exper-
iments on (1) image classification with ResNets (Section 4.1); (2) language modeling with
LSTMs (Section 4.2). The ProxQuant method outperforms the state-of-the-art results
on binary quantization and is comparable with the state-of-the-art on ternary and multi-bit
quantization.
• We perform a systematic theoretical study of quantization algorithms, showing that our
ProxQuant (standard prox-gradient method) converges to stataionary points under mild
smoothness assumptions (Section 5.1), where as lazy prox-gradient method such as Bina-
ryRelax (Yin et al., 2018) fails to converge in general (Section 5.2). Further, we show that
2
Published as a conference paper at ICLR 2019
BinaryConnect has a very stringent condition to converge to any fixed point (Section 5.3),
which we verify through a sign change experiment (Appendix C).
1.1 Prior work
Methodologies Han et al. (2015) propose Deep Compression, which compresses a DNN via spar-
sification, nearest-neighbor clustering, and Huffman coding. This architecture is then made into
a specially designed hardware for efficient inference (Han et al., 2016). In a parallel line of work,
Courbariaux et al. (2015) propose BinaryConnect that enables the training of binary neural networks,
and Li & Liu (2016); Zhu et al. (2016) extend this method into ternary quantization. Training and
inference on quantized nets can be made more efficient by also quantizing the activation (Hubara
et al., 2017; Rastegari et al., 2016; Zhou et al., 2016), and such networks have achieved impressive
performance on large-scale tasks such as ImageNet classification (Rastegari et al., 2016; Zhu et al.,
2016) and object detection (Yin et al., 2016). In the NLP land, quantized language models have been
successfully trained using alternating multi-bit quantization (Xu et al., 2018).
Theories Li et al. (2017) prove the convergence rate of stochastic rounding and BinaryConnect
on convex problems and demonstrate the advantage of BinaryConnect over stochastic rounding on
non-convex problems. Anderson & Berg (2017) demonstrate the effectiveness of binary networks
through the observation that the angles between high-dimensional vectors are approximately pre-
served when binarized, and thus high-quality feature extraction with binary weights is possible. Ding
et al. (2018) show a universal approximation theorem for quantized ReLU networks.
Principled methods Sun & Sun (2018) perform model quantization through a Wasserstein regu-
larization term and minimize via the adversarial representation, similar as in Wasserstein GANs (Ar-
jovsky et al., 2017). Their method has the potential of generalizing to other generic requirements on
the parameter, but might be hard to tune due to the instability of the inner maximization problem.
Prior to our work, a couple of proximal or regularization based quantization algorithms were pro-
posed as alternatives to the straight-through gradient method, which we now briefly review and
compare with. (Yin et al., 2018) propose BinaryRelax, which corresponds to a lazy proximal gra-
dient descent. (Hou et al., 2017; Hou & Kwok, 2018) propose a proximal Newton method with
a diagonal approximate Hessian. Carreira-Perpinan (2017); Carreira-Perpinan & IdeIbayev (2017)
formulate quantized network training as a constrained optimization problem and propose to solve
them via augmented Lagrangian methods. Our algorithm is different with all the aformentioned
work in using the non-lazy and “soft” proximal gradient descent with a choice of either `1 or `2
regularization, whose advantage over lazy prox-gradient methods is demonstrated both theoretically
(Section 5) and experimentally (Section 4.1 and Appendix C). 2
2	Preliminaries
The optimization difficulty of training quantized models is that they involve a discrete parameter
space and hence efficient local-search methods are often prohibitive. For example, the problem of
training a binary neural network is to minimize L(θ) for θ ∈ {±1}d. Projected SGD on this set
will not move unless with an unreasonably large stepsize (Li et al., 2017), whereas greedy nearest-
neighbor search requires d forward passes which is intractable for neural networks where d is on
the order of millions. Alternatively, quantized training can also be cast as minimizing L(q(θ))
for θ ∈ Rd and an appropriate quantizer q that maps a real vector to a nearby quantized vector, but
θ → q (θ) is often non-differentiable and piecewise constant (such as the binary case q(∙) = sign(∙)),
and thus back-propagation through q does not work.
2.1 The straight-through gradient method
The pioneering work of BinaryConnect (Courbariaux et al., 2015) proposes to solve this problem via
the straight-through gradient method, that is, propagate the gradient with respect to q(θ) unaltered
to θ, i.e. to let ∂θ ：= ∂∂L).One iterate of the straight-through gradient method (with the SGD
optimizer) is
θt+1 = θt - nNL⑹1 θ=q(θt).
3
Published as a conference paper at ICLR 2019
This enables the real vector θ to move in the entire Euclidean space, and taking q(θ) at the end of
training gives a valid quantized model. Such a customized back-propagation rule yields good em-
pirical performance in training quantized nets and has thus become a standard practice (Courbariaux
et al., 2015; Zhu et al., 2016; Xu et al., 2018). However, as we have discussed, it is information
theoretically unclear how the straight-through method works, and it does fail on very simple convex
Lipschitz functions (Figure 1b).
2.2	Straight-through gradient as lazy projection
Our first observation is that the straight-through gradient method is equivalent to a dual-averaging
method, or a lazy projected SGD (Xiao, 2010). In the binary case, we wish to minimize L(θ) over
Q = {±1}d, and the lazy projected SGD proceeds as
(θet = ProjQ(θt) = sign(θt) = q(θt),	(1)
!θt+ι = θt - ηtVL(θt).
Written compactly, this is θt+ι = θt - ηtVL(θ)∣θ=q(θt), which is exactly the straight-through
gradient method: take the gradient at the quantized vector and perform the update on the original
real vector.
2.3	Projection as a limiting proximal operator
We take a broader point of view that a projection is also a limiting proximal operator with a suitable
regularizer, to allow more generality and to motivate our proposed algorithm. Given any set Q, one
could identify a regularizer R : Rd → R≥0 such that the following hold:
R(θ) = 0, ∀θ ∈ Q and R(θ) > 0, ∀θ ∈/ Q.	(2)
In the case Q = {±1}d for example, one could take
d
R(θ) = Rbin(θ) = Emin{∣θj - 1|, ∣θj + 1|}.	(3)
j=1
The proximal operator (or prox operator) (Parikh & Boyd, 2014) with respect to R and strength
λ > 0 is
proxλκ(θ) := argmin 1 1 M- θ∣∣
θe∈Rd	2	2
+ λR(θe)
In the limiting case λ = ∞, the argmin has to satisfy R(θ) = 0, i.e. θ ∈ Q, and the prox operator is
to minimize kθ - θ0 k22 over θ ∈ Q, which is the Euclidean projection onto Q. Hence, projection is
also a prox operator with λ = ∞, and the straight-through gradient estimate is equivalent to a lazy
proximal gradient descent with and λ = ∞.
While the prox operator with λ = ∞ correponds to “hard” projection onto the discrete set Q, when
λ < ∞ it becomes a “soft” projection that moves towards Q. Compared with the hard projection,
a finite λ is less aggressive and has the potential advantage of avoiding overshoot early in training.
Further, as the prox operator does not strictly enforce quantizedness, it is in principle able to query
the gradients at every point in the space, and therefore has access to more information than the
straight-through gradient method.
3	Quantized net training via regularized learning
We propose the ProxQuant algorithm, which adds a quantization-inducing regularizer onto the
loss and optimizes via the (non-lazy) prox-gradient method with a finite λ. The prototypical version
of ProxQuant is described in Algorithm 1.
4
Published as a conference paper at ICLR 2019
Algorithm 1 PROXQUANT: Prox-gradient method for quantized net training
Require: Regularizer R that induces desired quantizedness, initialization θ0, learning rates
{ηt}t≥0, regularization strengths {λt}t≥0
while not converged do
Perform the prox-gradient step
θt+ι = PrOXηtλtR (θt - ηtVLet)) .	(4)
The inner SGD step in eq. (4) can be replaced by any preferred stochastic optimization method
such as Momentum SGD or Adam (Kingma & Ba, 2014).
end while
Compared to usual full-precision training, ProxQuant only adds a prox step after each stochastic
gradient step, hence can be implemented straightforwardly upon existing full-precision training. As
the prox step does not need to know how the gradient step is performed, our method adapts to other
stochastic optimizers as well such as Adam.
In the remainder of this section, we define a flexible class of quantization-inducing regularizers
through “distance to the quantized set”, derive efficient algorithms of their corresponding prox op-
erator, and propose a homotopy method for choosing the regularization strengths. Our regulariza-
tion perspective subsumes most existing algorithms for model-quantization (e.g.,(Courbariaux et al.,
2015; Han et al., 2015; Xu et al., 2018)) as limits of certain regularizers with strength λ → ∞. Our
proposed method can be viewed as a principled generalization of these methods to λ < ∞ with a
non-lazy prox operator.
3.1	Regularization for model quantization
Let Q ⊂ Rd be a set of quantized parameter vectors. An ideal regularizer for quantization would be
to vanish on Q and reflect some type of distance to Q when θ ∈/ Q. To achieve this, we propose L1
and L2 regularizers of the form
R(θ) = inf kθ - θ0k1 or R(θ) = inf kθ-θ0k22.	(5)
θ0∈Q	θ0∈Q
This is a highly flexible framework for designing regularizers, as one could specify any Q and
choose between L1 and L2 . Specifically, Q encodes certain desired quantization structure. By ap-
propriately choosing Q, we can specify which part of the parameter vector to quantize1, the number
of bits to quantize to, whether we allow adaptively-chosen quantization levels and so on. The choice
between {L1, L2} will encourage {“hard”,“soft”} quantization respectively, similar as in standard
regularized learning (Tibshirani, 1996).
In the following, we present a few examples of regularizers under our framework eq. (5) which
induce binary weights, ternary weights and multi-bit quantization. We will also derive efficient
algorithms (or approximation heuristics) for solving the prox operators corresponding to these regu-
larizers, which generalize the projection operators used in the straight-through gradient algorithms.
Binary neural nets In a binary neural net, the entries of θ are in {±1}. A natural choice would
be taking Q = {-1, 1}d. The resulting L1 regularizer is
d
R(θ)=	inf	kθ - θokι = ESJnM ∣θj - [θo]j|
θ0∈{±1}d	1 j=1 [θ0]j∈{±1}
d
二 X min {lθj - 1|, lθj + 1|} = kθ-sign⑹％.
j=1
(6)
This is exactly the binary regularizer Rbin that we discussed earlier in eq. (3). Figure 2 plots the
W-shaped one-dimensional component of Rbin from which we see its effect for inducing {±1}
quantization in analog to L1 regularization for inducing exact sparsity.
1Empirically, itis advantageous to keep the biases of each layers and the BatchNorm layers at full-precision,
which is often a negligible fraction, say 1∕vzd of the total number of parameters
5
Published as a conference paper at ICLR 2019
Figure 2: W-shaped regularizer
for binary quantization.
into
The prox operator with respect to Rbin , despite being a non-convex
optimization problem, admits a simple analytical solution:
proxλRbin (θ) = SoftThreshold(θ, sign(θ), λ)
=sign(θ) +sign(θ — sign(θ)) © [∣θ — sign(θ)∣ - λ]+.
(7)
We note that the choice of the L1 version is not unique: the squared
L2 version works as well, whose prox operator is given by (θ +
λ sign(θ))/(1 + λ). See Appendix A.1 for the derivation of these
prox operators and the definition of the soft thresholding operator.
Multi-bit quantization with adaptive levels. Following (Xu
et al., 2018), we consider k-bit quantized parameters with a struc-
tured adaptively-chosen set of quantization levels, which translates
Q = X αibi : {α1, . . . , αk} ⊂ R, bi ∈ {±1}d = nθ0 = Bα : α ∈ Rk, B ∈ {±1}d×ko.
i=1	(8)
The squared L2 regularizer for this structure is
Rk-bit (θ) =	inf kθ — Bαk22 ,	(9)
α∈Rk,B∈{±1}d×k
which is also the alternating minimization objective in (Xu et al., 2018).
We now derive the prox operator for the regularizer eq. (9). For any θ, we have
proχλRk-bit ⑹=argmin{1 M - θ∣∣2+λ a∈Rk ,Binf±i}d×k Be—Bαll2
argmin α∈Rk,B*}d×k{1M- θ[+λBe - BαB2} .
(10)
This is a joint minimization problem in (θ, B, α), and we adopt an alternating minimization schedule
to solve it:
(1)	Minimize over θ given (B, α), which has a closed-form solution θ
θ+2λBα
1+2λ .
(2)	Minimize over (B, α) given θ, which does not depend on θ0, and can be done via calling the
alternating quantizer of (Xu et al., 2018): Bα = qalt(θ).
Together, the prox operator generalizes the alternating minimization procedure in (Xu et al., 2018),
as λ governs a trade-off between quantization and closeness to θ. To see that this is a strict general-
ization, note that for any λ the solution of eq. (10) will be an interpolation between the input θ and
its Euclidean projection to Q. As λ → +∞, the prox operator collapses to the projection.
Ternary quantization Ternary quantization is a variant of 2-bit quantization, in which weights
are constrained to be in {-α, 0, β} for real values α,β > 0. We defer the derivation of the ternary
prox operator into Appendix A.2.
3.2	Homotopy method for regularization strength
Recall that the larger λt is, the more aggressive θt+1 will move towards the quantized set. An ideal
choice would be to (1) force the net to be exactly quantized upon convergence, and (2) not be too
aggressive such that the quantized net at convergence is sub-optimal.
We let λt be a linearly increasing sequence, i.e. λt := λ ∙ t for some hyper-parameter λ > 0
which we term as the regularization rate. With this choice, the stochastic gradient steps will start
off close to full-precision training and gradually move towards exact quantizedness, hence the name
“homotopy method”. The parameter λ can be tuned by minimizing the validation loss, and controls
the aggressiveness of falling onto the quantization constraint. There is nothing special about the
linear increasing scheme, but it is simple enough and works well as we shall see in the experiments.
6
Published as a conference paper at ICLR 2019
4	Experiments
We evaluate the performance of ProxQuant on two tasks: image classification with ResNets, and
language modeling with LSTMs. On both tasks, we show that the default straight-through gradient
method is not the only choice, and our ProxQuant can achieve the same and often better results.
4.1	Image classification on CIFAR- 1 0
Problem setup We perform image classification on the CIFAR-10 dataset, which contains 50000
training images and 10000 test images of size 32x32. We apply a commonly used data augmentation
strategy (pad by 4 pixels on each side, randomly crop to 32x32, do a horizontal flip with probability
0.5, and normalize). Our models are ResNets (He et al., 2016) of depth 20, 32, 44, and 56 with
weights quantized to binary or ternary.
Method We use PROXQUANT with regularizer eq. (3) in the binary case and eqs. (15) and (16)
in the ternary case, which we respectively denote as PQ-B and PQ-T. We use the homotopy method
λt = λ ∙ t with λ = 10-4 as the regularization strength and Adam with constant learning rate 0.01
as the optimizer.
We compare with BinaryConnect (BC) for binary nets and Trained Ternary Quantization (TTQ) (Zhu
et al., 2016) for ternary nets. For BinaryConnect, we train with the recommended Adam optimizer
with learning rate decay (Courbariaux et al., 2015) (initial learning rate 0.01, multiply by 0.1 at
epoch 81 and 122), which we find leads to the best result for BinaryConnect. For TTQ we compare
with the reported results in (Zhu et al., 2016).
For binary quantization, both BC and our ProxQuant are initialized at the same pre-trained full-
precision nets (warm-start) and trained for 300 epochs for fair comparison. For both methods, we
perform a hard quantization θ 7→ q(θ) at epoch 200 and keeps training till the 300-th epoch to stabi-
lize the BatchNorm layers. We compare in addition the performance drop relative to full precision
nets of BinaryConnect, BinaryRelax (Yin et al., 2018), and our ProxQuant.
Result The top-1 classification errors for binary quantization are reported in Table 1. Our PROX-
Quant consistently yields better results than BinaryConnect. The performance drop of Prox-
QUANT relative to full-precision nets is about 1%, better than BinaryConnect by 0.2% on average
and significantly better than the reported result of BinaryRelax.
Results and additional details for ternary quantization are deferred to Appendix B.1.
Table 1: Top-1 classification error of binarized ResNets on CIFAR-10. Performance is reported in
mean(std) over 4 runs, as well as the (absolute) performance drop of over full-precision nets.
		Classification error		Performance drop over FP net		
Model	FP	BC	PQ-B (ours)	BC	BinaryRelax	PQ-B (ours)
(Bits)	(32)	⑴	(1)	⑴	(1)	⑴
ResNet-20	8.06	9.54 (0.03)	9.35(0.13)	+1.48	+4.84	-+1.29-
ReSNet-32	7.25	8.61 (0.27)	8.53(0.15)	+1.36	+2.75	+1.28
ResNet-44	6.96	8.23 (0.23)	7.95 (0.05)	+1.27	-	+0.99
ResNet-56	6.54	7.97 (0.22)	7.70 (0.06)	+1.43	-	+1.16
4.2	Language modeling with LSTMs
Problem setup We perform language modeling with LSTMs Hochreiter & Schmidhuber (1997)
on the Penn Treebank (PTB) dataset (Marcus et al., 1993), which contains 929K training tokens,
73K validation tokens, and 82K test tokens. Our model is a standard one-hidden-layer LSTM with
embedding dimension 300 and hidden dimension 300. We train quantized LSTMs with the en-
coder, transition matrix, and the decoder quantized to k-bits for k ∈ {1, 2, 3}. The quantization is
performed in a row-wise fashion, so that each row of the matrix has its own codebook {α1, . . . , αk}.
7
Published as a conference paper at ICLR 2019
Method We compare our multi-bit PROXQUANT (eq. (10)) to the state-of-the-art alternating min-
imization algorithm with straight-through gradients (Xu et al., 2018). Training is initialized at a
pre-trained full-precision LSTM. We use the SGD optimizer with initial learning rate 20.0 and de-
cay by a factor of 1.2 when the validation error does not improve over an epoch. We train for 80
epochs with batch size 20, BPTT 30, dropout with probability 0.5, and clip the gradient norms to
0.25. The regularization rate λ is tuned by finding the best performance on the validation set. In
addition to multi-bit quantization, we also report the results for binary LSTMs (weights in {±1}),
comparing BinaryConnect and our ProxQuant-Binary, where both learning rates are tuned on an
exponential grid {2.5, 5, 10, 20, 40}.
Result We report the perplexity-per-word (PPW, lower is better) in Table 2. The performance
of ProxQuant is comparable with the Straight-through gradient method. On Binary LSTMs,
ProxQuant-Binary beats BinaryConnect by a large margin. These results demonstrate that Prox-
Quant offers a powerful alternative for training recurrent networks.
Table 2: PPW of quantized LSTM on Penn Treebank.
MethOd / Number of Bits	1	2	3	FP (32)
BinaryConnect	372.2	-	-	88.5
PROXQUANT-Binary (ours)	288.5	-	-	
ALT Straight-through2	104.7	^902^	^86T	
-ALT-PROXQUANT (ours)-	106.2	290.02	~87Σ^	
5	Theoretical analysis
In this section, we perform a theoretical study on the convergence of quantization algorithms. We
show in Section 5.1 that our ProxQuant algorithm (i.e. non-lazy prox-gradient method) converges
under mild smoothness assumptions on the problem. In Section 5.2, we provide a simple example
showing that the lazy prox-gradient method fails to converge under the same set of assumptions.
In Section 5.3, we show that BinaryConnect has a very stringent condition for converging to a
fixed point. Our theory demonstrates the superiority of our proposed ProxQuant over lazy prox-
gradient type algorithms such as BinaryConnect and BinaryRelax (Yin et al., 2018). All missing
proofs are deferred to Appendix D.
Prox-gradient algorithms (both lazy and non-lazy) with a fixed λ aim to solve the problem
minimizeL(θ) + λR(θ),	(11)
θ∈Rd
and BinaryConnect can be seen as the limiting case of the above with λ = ∞ (cf. Section 2.2).
5.1	A convergence theorem for ProxQuant
We consider PROXQUANT with batch gradient and constant regularization strength λt ≡ λ:
θt+ι = ProxηtλR(θt - ηtVL(θt)).
Theorem 5.1 (Convergence of ProxQuant). Assume that the loss L is β-smooth (i.e. has β-Lipschitz
gradients) and the regularizer R is differentiable. Let Fλ(θ) = L(θ) + λR(θ) be the composite
objective and assume that it is bounded below by F?. Running ProxQuant with batch gradient VL,
constant stepsize η ≡ η =泰 and λt ≡ λ for T steps, we have the convergence guarantee
kVFλ (θTbest )k22 ≤
Cβ(Fλ(θ0) - F?)
T
where Tbest = arg min kθt - θt-1k2 ,	(12)
1≤t≤T
where C > 0 is a universal constant.
2We thank Xu et al. (2018) for sharing the implementation of this method through a personal communi-
cation. There is a very clever trick not mentioned in their paper: after computing the alternating quantization
qalt (θ), they multiply by a constant 0.3 before taking the gradient; in other words, their quantizer is a rescaled
alternating quantizer: θ → 0.3qait(θ). This scaling step gives a significant gain in performance - without SCal-
ing the PPW is {116.7, 94.3, 87.3} for {1, 2, 3} bits. In contrast, our PROXQUANT does not involve a scaling
step and achieves better PPW than this unscaled ALT straight-through method.
8
Published as a conference paper at ICLR 2019
Remark 5.1. The convergence guarantee requires both the loss and the regularizer to be smooth.
Smoothness of the loss can be satisfied if we use a smooth activation function (such as tanh). For
the regularizer, the quantization-inducing regularizers defined in Section 3.1 (such as the W-shaped
regularizer) are non-differentiable. However, we can use a smoothed version of them that is differ-
entiable and point-wise arbitrarily close to R, which will satisfy the assumptions of Theorem 5.1.
The proof of Theorem 5.1 is deferred to Appendix D.1.
5.2	Non-convergence of lazy prox-gradient
The lazy prox-gradient algorithm (e.g. BinaryRelax (Yin et al., 2018)) for solving problem eq. (11) is
a variant where the gradients are taken at proximal points but accumulated at the original sequence:
Θt+1 = θt- η^L(ProxλR(θt)).	(13)
Convergence of the lazy prox-gradient algorithm eq. (13) is only known to hold for convex prob-
lems (Xiao, 2010); on smooth non-convex problems it generally does not converge even in an er-
godic sense. We provide a concrete example that satisfies the assumptions in Theorem 5.1 (so that
ProxQuant converges ergodically) but lazy prox-gradient does not converge.
Theorem 5.2 (Non-convergence of lazy prox-gradient). There exists L and R satisfying the as-
sumptions of Theorem 5.1 such that for any constant stepsize η ≡ η ≤ 击,there exists some
specific initialization θ0 on which the lazy prox-gradient algorithm eq. (13) oscillates between two
non-stataionry points and hence does not converge in the ergodic sense of eq. (12).
Remark 5.2. Our construction is a fairly simple example in one-dimension and not very adversar-
ial: L(θ) = 2θ2 and R is a smoothed W-Shaped regularizer. See Appendix D.2for the details.
5.3	Convergence characterization for BinaryConnect
For BinaryConnect, the concept of stataionry points is no longer sensible (as the target points {±1}d
are isolated and hence every point is stationary). Here, we consider the alternative definition of con-
vergence as converging to a fixed point and show that BinaryConnect has a very stringent conver-
gence condition.
Consider the BinaryConnect method with batch gradients:
St = sign(θt), θt+ι = θt - ηt^L(st).	(14)
Definition 5.1 (Fixed point and convergence). We say that s ∈ {±1}d is a fixed point of the Bina-
ryConnect algorithm, ifs0 = s in eq. (14) implies that st = sfor all t = 1, 2, . We say that the
BinaryConnect algorithm converges if there exists t < ∞ such that st is a fixed point.
Theorem 5.3. Assume that the learning rates satisfy t∞=0 ηt = ∞, then s ∈ {±1}d is a fixed
point for BinaryConnect eq. (14) if and only if sign(VL(s)[i]) = — s[i] for all i ∈ [d] such that
VL(θ)[i] = 0. Such a point may not exist, in which case BinaryConnect does not converge for any
initialization θ0 ∈ Rd.
Remark 5.3. Theorem 5.3 is in appearingly a stark contrast with the convergence result for Bina-
ryConnect in (Li et al., 2017) in the convex case, whose bound involves a an additive error O(∆)
that does not vanish over iterations, where ∆ is the grid size for quantization. Hence, their result is
only useful when ∆ is small. In contrast, we consider the original BinaryConnect with ∆ = 1, in
which case the error makes Li et al. (2017)’s bound vacuous. The proof of Theorem 5.3 is deferred
to Appendix D.3.
Experimental evidence We have already seen that such a fixed point s might not exist in the toy
example in Figure 1b. In Appendix C, we perform a sign change experiment on CIFAR-10, showing
that BinaryConnect indeed fails to converge to a fixed sign pattern, corroborating Theorem 5.3.
6	Conclusion
In this paper, we propose and experiment with the ProxQuant method for training quantized
networks. Our results demonstrate that ProxQuant offers a powerful alternative to the straight-
through gradient method and has theoretically better convergence properties. For future work, it
would be of interest to propose alternative regularizers for ternary and multi-bit ProxQuant and
experiment with our method on larger tasks.
9
Published as a conference paper at ICLR 2019
Acknowledgement
We thank Tong He, Yifei Ma, Zachary Lipton, and John Duchi for their valuable feedback. We thank
Chen Xu and Zhouchen Lin for the insightful discussion on multi-bit quantization and sharing the
implementation of (Xu et al., 2018) with us. We thank Ju Sun for sharing the draft of (Sun & Sun,
2018) and the inspiring discussions on adversarial regularization for quantization. The majority of
this work was performed when YB and YW were at Amazon AI.
References
Alexander G Anderson and Cory P Berg. The high-dimensional geometry of binary neural networks.
arXiv preprint arXiv:1705.07199, 2017.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Miguel A Carreira-Perpinan. Model compression as constrained optimization, with application to
neural nets. part i: General framework. arXiv preprint arXiv:1707.01209, 2017.
Miguel A Carreira-Perpinan and Yerlan Idelbayev. Model compression as constrained optimization,
with application to neural nets. part ii: Quantization. arXiv preprint arXiv:1707.04319, 2017.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. BinaryConnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
SyStemS,pp. 3123-3131, 2015.
Yukun Ding, Jinglan Liu, and Yiyu Shi. On the universal approximability of quantized relu neural
networks. arXiv preprint arXiv:1802.03646, 2018.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J
Dally. EIE: Efficient inference engine on compressed deep neural network. In Computer Archi-
tecture (ISCA), 2016 ACM/IEEE 43rd Annual International SympoSium on, pp. 243-254. IEEE,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In ProceedingS of the IEEE conference on computer viSion and pattern recognition, pp.
770-778, 2016.
Sepp Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Lu Hou and James T. Kwok. Loss-aware weight quantization of deep networks. In International
Conference on Learning RepreSentationS, 2018. URL https://openreview.net/forum?
id=BkrSv0lA-.
Lu Hou, Quanming Yao, and James T Kwok. Loss-aware binarization of deep networks. In Interna-
tional Conference on Learning RepreSentationS, 2017. URL https://openreview.net/
forum?id=S1oWlN9ll.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. Journal
of Machine Learning ReSearch, 18:187-1, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Fengfu Li and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016.
10
Published as a conference paper at ICLR 2019
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training
quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems,
pp. 5811-5821,2017.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and TrendsR in Optimization, 1
(3):127-239, 2014.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European Conference on Computer
Vision, pp. 525-542. Springer, 2016.
Ju Sun and Xiaoxia Sun. Adversarial probabilistic regularization. Unpublished draft, 2018.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pp. 267-288, 1996.
Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11(Oct):2543-2596, 2010.
Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and Hongbin
Zha. Alternating multi-bit quantization for recurrent neural networks. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
S19dR9x0b.
Penghang Yin, Shuai Zhang, Yingyong Qi, and Jack Xin. Quantization and training of low bit-width
convolutional neural networks for object detection. arXiv preprint arXiv:1612.06052, 2016.
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Bina-
ryrelax: A relaxation approach for training deep neural networks with quantized weights. arXiv
preprint arXiv:1801.06313, 2018.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
A Additional results on Regularization
A.1 Prox operators for binary nets
Here we derive the prox operators for the binary regularizer eq. (6) and its squared L2 variant. Recall
that
d
Rbin(θ) = Xmin {∣θj - 1|, ∣θj + 1∣}.
j=1
By definition of the prox operator, we have for any θ ∈ Rd that
ProxλRbin ⑹=argmin1 1 Ue -θ∣2 + λ X min ne - 1|, le + 1
d
E2(0j-θj )2 + λ min {∣e∙- 1∣,∣e∙ + 1
j=1
arg min
θe∈Rd
11
Published as a conference paper at ICLR 2019
This minimization problem is coordinate-wise separable. For each θj , the penalty term remains the
same upon flipping the sign, but the quadratic term is smaller when sign(θj) = sign(θj). Hence,
the solution θ? to the prox satisfies that sign(θj?) = sign(θj ), and the absolute value satisfies
忸?| = argmin {2(t -∣θj |)2 + λ∣t - 1∣} = SoftThreshold(∣θj |, 1, λ) = 1+sign(∣θj∣-1)[∣∣θj ∣-1∣-λ] +
Multiplying by sign(θj?) = sign(θj ), we have
θj? = SoftThreshold(θj , sign(θj), λ),
which gives eq. (7).
For the squared L2 version, by a similar argument, the corresponding regularizer is
d
Rbin(θ) = Emin {(θj - 1)2, (θj + 1)2}.
j=1
For this regularizer we have
(d ι	〜	〜
X 2(θj - θj)2 + λmin{(θj- -1)2, (θj- +1)2}
Using the same argument as in the L1 case, the solution θ? satisfies sign(θj?) = sign(θj), and
忸？| = arg min
t≥0
Multiplying by sign(θj? ) = sign(θj ) gives
θj + λ sign(θj)
ΓΓλ
or, in vector form, θ? = (θ + λsign(θ))∕(1 + λ).
{Γ(t-∣θj∣)2 + λ(t-1)2} = ≡⅛λ
2	1+λ
A.2 Prox operator for ternary quantization
For ternary quantization, we use an approximate version of the alternating prox operator eq. (10):
compute θ = proxλR(θ) by initializing at θ = θ and repeating
θ = q(θ) and θ
θ + 2λb
1 + 2λ ,
(15)


where q is the ternary quantizer defined as
07
q(θ)= θ+1{θ ≥ ∆} + θ-1{θ ≤-∆}, △=彳
kθk1, θ+ =θliιθi≥∆, θ =。k《二—△. (16)
This is a straightforward extension of the TWN quantizer (Li & Liu, 2016) that allows different
levels for positives and negatives. We find that two rounds of alternating computation in eq. (15)
achieves a good performance, which we use in our experiments.
B	Additional experimental results
B.1 Ternary quantization for CIFAR- 1 0
Our models are ResNets of depth 20, 32, and 44. Ternarized training is initialized at pre-trained
full-precision nets. We perform a hard quantization θ 7→ q(θ) at epoch 400 and keeps training till
the600-th epoch to stabilize the BatchNorm layers.
12
Published as a conference paper at ICLR 2019
Table 3: Top-1 classification error of ternarized ResNets on CIFAR-10. Performance is reported in
mean(std) over 4 runs, where for PQ-T we report in addition the best of 4 (Bo4).
Model (Bits)	FP (32)	TTQ (2)	PQ-T (ours) (2)	PQ-T (ours, Bo4) (2)
ResNet-20	8.06	8.87	8.40 (0.13)	822
ResNet-32	7.25	7.63	7.65 (0.15)	7.53
ResNet-44	6.96	7.02	7.05 (0.08)	6.98
Result The top-1 classification errors for ternary quantization are reported in Table 3. Our results
are comparable with the reported results of TTQ,3 and the best performance of our method over 4
runs (from the same initialization) is slightly better than TTQ.
C Sign change experiment
We experimentally compare the training dynamics of ProxQuant-Binary and BinaryConnect
through the sign change metric. The sign change metric between any θ1 and θ2 is the proportion of
their different signs, i.e. the (rescaled) Hamming distance:
SignChange(θ1, θ2)
ksign(θ1) - sign(θ2)k1
2d
∈ [0, 1].
In Rd, the space of all full-precision parameters, the sign change is a natural distance metric that
represents the closeness of the binarization of two parameters.
(a)
Sign_Change-Jayer3.2.conv2.weig ht
(b)
(c)
(d)
Figure 3: SignChange(θ0 , θt) against t (epoch) for BinaryConnect and PROXQUANT, over 4 runs starting
from the same full-precision ResNet-20. ProxQuant has significantly lower sign changes than BinaryCon-
nect while converging to better models. (a) The first conv layer of size 16 × 3 × 3 × 3; (b) The last conv layer
of size 64 × 64 × 3 × 3; (c) The fully connected layer of size 64 × 10; (d) The validation top-1 error of the
binarized nets (with moving average smoothing).
Recall in our CIFAR-10 experiments (Section 4.1), for both BinaryConnect and ProxQuant, we
d
initialize at a good full-precision net θ0 and stop at a converged binary network θ ∈ {±1} . We
are interested in SignChange(θ0, θt) along the training path, as well as SignChange(θ0, θ), i.e. the
distance of the final output model to the initialization.
Our finding is that PROXQUANT produces binary nets with both lower sign changes and higher per-
formances, compared with BinaryConnect. Put differently, around the warm start, there is a good
binary net nearby which can be found by ProxQuant but not BinaryConnect, suggesting that Bi-
naryConnect, and in general the straight-through gradient method, suffers from higher optimization
instability than ProxQuant. This finding is consistent in all layers, across different warm starts,
and across differnent runs from each same warm start (see Figure 3 and Table 4 in Appendix C.1).
This result here is also consistent with Theorem 5.3: the signs in BinaryConnect never stop changing
until we manually freeze the signs at epoch 400.
3We note that our ProxQuant-Ternary and TTQ are not strictly comparable: we have the advantage of
using better initializations; TTQ has the advantage of a stronger quantizer: they train the quantization levels
(θ+ , θ- ) whereas our quantizer eq. (16) pre-computes them from the current full-precision parameter.
13
Published as a conference paper at ICLR 2019
C.1 Raw data for sign change experiment
Table 4: Performances and sign changes on ResNet-20 in mean(std) over 3 full-precision initial-
izations and 4 runs per (initialization x method). Sign changes are computed over all quantized
parameters in the net.
Initialization	Method	Top-1 Error(%)	Sign change
-FP-Net 1-	-BC-	9.489 (0.223)	0.383 (0.006)
(8.06)	PQ-B	9.146 (0.212)	0.276 (0.020)
-FP-Net 2-	-BC-	9.745 (0.422)	0.381 (0.004)
(8.31)	PQ-B	9.444 (0.067)	0.288 (0.002)
-FP-Net 3-	-BC-	9.383 (0.211)	0.359 (0.001)
(7.73)	PQ-B	9.084 (0.24i)-	0.275 (0.001)~
Table 5: Performances and sign changes on ResNet-20 in raw data over 3 full-precision initial-
izations and 4 runs per (initialization x method). Sign changes are computed over all quantized
parameters in the net.
Initialization	Method	Top-1 Error(%)	Sign change
-FP-Net 1-	-BC-	9.664,9.430,9.198,9.663	0.386, 0.377, 0.390, 0.381
(8.06)	PQ-B	9.058, 8.901, 9.388, 9.237	0.288, 0.247, 0.284, 0.285
-FP-Net 2-	-BC-	9.456, 9.530, 9.623, 10.370	0.376, 0.379, 0.382, 0.386
(8.31)	PQ-B	9.522, 9.474, 9.410, 9.370	0.291, 0.287, 0.289, 0.287
-FP-Net 3-	-BC-	9.107,9.558,9.538,9.328	0.360, 0.357, 0.359, 0.360
(7.73)	PQ-B	9.284, 8.866, 9.301, 8.884—	0.275, 0.276, 0.276, 0.275 一
14
Published as a conference paper at ICLR 2019
D Proofs of theoretical results
D.1 Proof of Theorem 5.1
Recall that a function f : Rd → R is said to be β-smooth if it is differentiable and Vf is β-Lipschitz:
for all x, y ∈ Rd we have
kVf (x) -Vf(y)k2 ≤ βkx-yk2.
For any β-smooth function, it satisfies the bound
f (y) ≤ f (x) + hVf (x), y — Xi + 2 ∣∣x — y∣∣2 for all x,y ∈ Rd.
Convergence results like Theorem 5.1 are standard in the literature of proximal algorithms, where
we have convergence to stataionarity without convexity on either L or R but assuming smoothness.
For completeness we provide a proof below. Note that though the convergence is ergodic, the best
index Tbest can be obtained in practice via monitoring the proximity ∣θt - θt-1 ∣2.
Proof of Theorem 5.1 Recall the ProxQuant iterate
θt+1
arθg∈mdin {L(θt)+hθ-θt, vL(θt)i+2η kθ- θt∣2+λR(θ)}.
By the fact that θt+1 minimizes the above objective and applying the smoothness of L, we get that
Fλ(θt) = L(θt) + λR(θt) ≥ L(θt) + hθt+ι - θt, vL(θt)i + 3- ∣∣θt+ι - θt∣∣2 + λR(θt+ι)
2η	2
≥ L(θt+ι) + (而 - 2) ∣θt+ι — θt∣2 + λR(θt+ι) = Fλ(θt+ι) + 2 ιιθt+ι - θt ∣∣2.
Telescoping the above bound for t = 0, . . . , T - 1, we get that
T-1
∣θt+1 - θt ∣22 ≤
t=0
2(Fλ(θo)- Fλ(θτ))
β
2(Fλ(θo)- F?)
β
Therefore we have the proximity guarantee
min 的	^∣∣2< 2(Fλ(θ0) - F?)	门力
o≤min-jθt+1 - θt∣2 ≤ —βτ—.	(D)
We now turn this into a stationarity guarantee. The first-order optimality condition for θt+1 gives
VL(θt) + 1(θt+ι - θt) + λVR(θt+ι) = 0.
η
Combining the above equality and the smoothness of L, we get
∣∣VFλ(θt+ι)∣∣2 = ∣∣VL(θt+ι) + λVR(θt+ι)∣∣2 = 1(θt - 昨。+ VL(θt+ι) - VL(θt)
η2
≤ (η+尸)∣θt+ι - θt∣2=3β ∣θt+ι - θt∣2.
Choosing t = Tbest - 1 and applying the proximity guarantee eq. (17), we get
∣VFλ(θTbest)∣22 ≤ 9β2 ∣θTbest - θTbest-1∣22 = 9β2 0≤mt≤iTn-1 ∣θt+1 - θt∣22 ≤
18β(Fλ(θ0) - F?)
T
This is the desired bound.
□
D.2 Proof of Theorem 5.2
15
Published as a conference paper at ICLR 2019
Let our loss function L : R → R be the quadratic L(θ) = 1 θ2 (So
that L is β-smooth with β = 1). Let the regularizer R : R → R be
a smoothed version of the W-shaped regularizer in eq. (3), defined
as (for ∈ (0, 1/2] being the smoothing radius)
1
-θ-+ + 1 - e,	θ ∈ [0, e)
2
-θ + 1 - 5,	θ ∈ [e, 1 - e)
R(θ) =	1	2
2e (θ -I)2,	θ ∈ [1 -e, 1+e)
、。- 1 - e,	θ ∈ [1 +e, ∞)
Figure 4
and R(-θ) = R(θ) for the negative part. See Figure 4 for an illustration of the loss L and the
regularizer R (with e = 0.2).
It is straightforward to see that R is piecewise quadratic and differ-
entiable on R by computing the derivatives at e and 1 ± e. Further,
by elementary calculus, we can evaluate the prox operator in closed
form: for all λ ≥ 1, we have
proxλR(θ)
eθ + λ sign(θ)
e + λ sign(θ)
for all ∣θ∣ ≤ 1.
Now, suppose We run the lazy prox-gradient method with constant stepsize η ≡ η ≤ ɪ = 2. For
the specific initialization
θo =八工；：-----------∈ (0,1),
2λ + (2 - η )e
we have the equality prox、R(。。)= 2 θo and therefore the next lazy prox-gradient iterate is
θι = θ0 - ηVL(proxiR(θo)) = θo - ηVL ( 2θo ) = θo - η ∙ 2θo = -θ°.
λR	η	η
As both R and L are even functions, a symmetric argument holds for θ1 from which we get θ2 =
-θ1 = θ0 . Therefore the lazy prox-gradient method ends up oscillating between two points:
θt = (-1)tθ0.
On the other hand, it is straightforward to check that the only stationary points of L(θ) + λR(θ)
are 0 and ±^λj, all not equal to ±θ0. Therefore the sequence {θt}t≥o does not have a subsequence
with vanishing gradient and thus does not approach stationarity in the ergodic sense.	□
D.3 Proof of Theorem 5.3
We start with the “=" direction. If S is a fixed point, then by definition there exists θo ∈ Rd such
that θt = θ for all t = 0, 1, 2, .. By the iterates eq. (14)
T
θT = θ0 - X ηtVL(st).
t=0
Take signs on both sides and apply st = s for all t on both sides, we get that
s = sT = sign(θT) = sign θ0 - VL(s) Xηt
Take the limit T → ∞ and apply the assumption that Pt ηt = ∞, we get that for all i ∈ [d] such
that [VL(θ)]i 6= 0,
s[i]
lim sign
T→∞
T
θ0 -VL(s)Xηt
t=0
[i] = - sign(VL(s))[i].
16
Published as a conference paper at ICLR 2019
Now We prove the "u" direction. If θ obeys that sign(VL(s)[i]) = -s[i] for all i ∈ [d] such
that VL(s)[i] = 0, then if we take any θo such that sign(θ0) = s, θt will move in a straight
line towards the direction of -VL(s), which does not change the sign of h0. In other words,
st = sign(θt) = sign(θ0) = s for all t = 0, 1, 2, . Therefore, by definition, s is a fixed point.
17