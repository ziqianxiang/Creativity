Published as a conference paper at ICLR 2019
Learning Latent Superstructures in Varia-
tional Autoencoders for Deep Multidimen-
sional Clustering
Xiaopeng Li1 , Zhourong Chen1 , Leonard K. M. Poon2 and Nevin L. Zhang1
1	Department of Computer Science and Engineering
The Hong Kong University of Science and Technology
2	Department of Mathematics & Information Technology
The Education University of Hong Kong
{xlibo,zchenbb,lzhang}@cse.ust.hk, kmpoon@eduhk.hk
Ab stract
We investigate a variant of variational autoencoders where there is a superstructure
of discrete latent variables on top of the latent features. In general, our superstruc-
ture is a tree structure of multiple super latent variables and it is automatically
learned from data. When there is only one latent variable in the superstructure,
our model reduces to one that assumes the latent features to be generated from a
Gaussian mixture model. We call our model the latent tree variational autoen-
coder (LTVAE). Whereas previous deep learning methods for clustering produce
only one partition of data, LTVAE produces multiple partitions of data, each being
given by one super latent variable. This is desirable because high dimensional data
usually have many different natural facets and can be meaningfully partitioned in
multiple ways.
1	Introduction
Clustering is a fundamental task in unsupervised machine learning, and it is central to many data-
driven application domains. Cluster analysis partitions all the data into disjoint groups, and one can
understand the structure of the data by examining examples in each group. Many clustering methods
have been proposed in the literature (Aggarwal & Reddy, 2013), such as k-means (MacQueen et al.,
1967), Gaussian mixture models (Christopher, 2016) and spectral clustering (Von Luxburg, 2007).
Conventional clustering methods are generally applied directly on the original data space. However,
it is challenging to perform cluster analysis on high dimensional and unstructured data (Steinbach
et al., 2004), such as images. It is not only because the dimensionality is high, but also because the
original data space is too complex to interpret, e.g. there are semantic gaps between pixel values and
objects in images.
Recently, deep learning based clustering methods have been proposed that simultanously learn non-
linear embeddings through deep neural networks and perform cluster analysis on the embedding
space. The representation learning process learns effective high-level representations from high
dimensional data and helps the cluster analysis. This is typically achieved by unsupervised deep
learning methods, such as restricted Boltzmann machine (RBM) (Hinton et al., 2006; Hinton &
Salakhutdinov, 2006), autoencoders (AE) (Vincent et al., 2008; 2010), variational autoencoders
(VAE) (Kingma & Welling, 2014), etc. Previous deep learning based clustering methods (Xie et al.,
2016; Guo et al., 2017; Jiang et al., 2017; Yang et al., 2017) assume one single partition over the data
and that all attributes define that partition. In real-world applications, however, the assumptions are
usually not true. High-dimensional data are often multifaceted and can be meaningfully partitioned
in multiple ways based on subsets of attributes (Chen et al., 2012). For example, a student popula-
tion can be clustered in one way based on course grades and in another way based on extracurricular
activities. Movie reviews can be clustered based on both sentiment (positive or negative) and genre
(comedy, action, war, etc.). It is challenging to discover the multi-facet structures of data, especially
for high-dimensional data.
1
Published as a conference paper at ICLR 2019
To resolve the above issues, we propose an unsupervised learning method, latent tree variational au-
toencoder (LTVAE) to learn latent superstructures in variational autoencoders, and simultaneously
perform representation learning and structure learning. LTVAE is a generative model, where the data
is assumed to be generated from latent features through neural networks, while the latent features
themselves are generated from tree-structured Bayesian networks with another level of latent vari-
ables as shown in Fig. 1. Each of those latent variables defines a facet of clustering. The proposed
method automatically selects subsets of latent features for each facet, and learns the dependency
structure among different facets. This is achieved through systematic structure learning. Conse-
quently, LTVAE is able to discover complex structures of data rather than one partition. We also
propose efficient learning algorithms for LTVAE with gradient descent and Stepwise EM through
message passing.
The rest of the paper is organized as follows. The related works are reviewed in Section 2. We
introduce the proposed method and learning algorithms in Section 3. In Section 4, we present the
empirical results. The conclusion is given in Section 5.
2	Related Works
Clustering has been extensively studied in the literature in many aspects (Aggarwal & Reddy, 2013).
More complex clustering methods related to structure learning using Bayesian nonparametrics have
been proposed, like Dirichlet Process (Blei et al., 2006), Hierarchical Dirichlet Process (HDP) (Teh
et al., 2006). However, those are with conventional clustering methods that apply on raw data.
Recently, deep learning based clustering methods have drawn more and more attention. A simple
two-stage approach is to first learn low-dimensional embeddings using unsupervised feature learning
methods, and then perform cluster analysis on the embeddings. However, without any supervision,
the representation learning do not necessarily reveal the true cluster structure of the data. DEC (Xie
et al., 2016) is a method that simultaneously learns feature representations and cluster assignments
through deep autoencoders. It gradually improves the clustering by driving the deep network to
learn a better mapping. Improved Deep Embedded Clustering (Guo et al., 2017) improves DEC
by keeping the decoder network and adding reconstruction loss to the original clustering loss in
DEC. Variational deep embedding (Jiang et al., 2017) is a generative method that models the data
generative process using a Gaussian mixture model combined with a VAE, and also performs joint
learning of representations and clustering. Similarly, GMVAE (Dilokthanakul et al., 2016) performs
joint learning of a GMM and a VAE, but instead generates the mixture components through neural
networks. Deep clustering network (DCN) (Yang et al., 2017) is another one that jointly learns an
autoencoder and performs k-means clustering. These joint learning methods consistently achieve
better clustering results than conventional ones. The method proposed in (Yang et al., 2016) uses
convolutional neural networks and jointly learns the representations and clustering in a recurrent
framework. All these methods assume flat partitions over the data, and do not attempt the structure
learning issue. An exception is hierarchical nonparametric variational autoencoders proposed in
(Goyal et al., 2017). It uses nCRP as the prior for VAE to allow infinitely deep and branching tree
hierarchy structure and focuses on learning hierarchy of concepts. However, it is still one partition
over the data, only that the partitions in upper levels are more general partitions, while those in
lower levels more fine-grained. Different from it, our work focuses on multifacets of clustering, for
example, the model could make one partition based on identity of subjects, while another partition
based on pose.
3	The Proposed Method
In this section, we present the proposed latent tree variational autoencoder and the learning algo-
rithms for joint representation learning and structure learning for multidimensional clustering.
3.1	Latent Tree Variational Autoencoder
Deep generative models assume that data x is generated from latent continuous variable z through
some random process. The process consists of two steps: (1) a value z is generated from some
prior distribution p(z);(2) the observation X is generated from the conditional distribution pθ(x|z),
which is parameterized through deep neural networks. Thus, it defines the joint distribution between
2
Published as a conference paper at ICLR 2019
Figure 1: Latent Tree Variational Autoencoder
P(K)
P①明）
Clique Tree
P(ZlIyl)	P(Z2∣%) P(Z3∣%) P(ZjH)	P(Zl = Zι∣%) P(Zl = Zl∣%)P(Z3 = Z3∣yr)P(Z4 = Z4∣y2)
Figure 2: Inference and gradient through mes-
sage passing. Solid-arrows denote collecting
message, and dashed-arrows denote distributing
message.
observation x and latent variable z:
p(x,z)= p(z)Pθ (x|z)	(1)
This process is hidden from our view, and we learn this process by maximizing the marginal log-
likelihood p(x) over the parameters θ and latent variable z from data. After the learning, the latent
variable z can be regarded as the deep representations of x since it captures the most relevant infor-
mation of x. Thus, the learning process is also called representation learning.
In order to learn the latent structure of z, for example multidimensional cluster structure, we in-
troduce a set of latent variables Y1, ..., Yl on top of z. A single z or multiple z’s form a node zb.
Suppose variables in Z form B nodes of zι,…，ZB. Each latent variable Y may be only connected
to a subset of nodes, and the dependency of each zb and its parent Y is characterized by a conditional
Gaussian distribution. Furthermore, the latent variables Y1, ..., Yl are connected to each other, and
the dependency of a latent variable Y on its parent Y0 is characterized by a conditional distribution
P(Y|Y0). This essentially forms a Bayesian network. And if we restrict the network to be tree-
structured, the z and Y together form a latent tree model (Zhang, 2004; Poon et al., 2010; 2013;
Mourad et al., 2013; Pearl, 2014; Zhang & Poon, 2017) with z being the observed variables and
Y being the latent variables. For multidimensional clustering, each latent variable Y is taken to be
a discrete variable, where each discrete state y of Y defines a cluster. Each latent variable Y thus
defines a facet partition over the data based on subset of attributes and multiple Y’s define multiple
facets. Given a value y of Y, Zb follows a conditional Gaussian distribution P(zb|y) = N(μy, Σy)
with mean vector μy and covariance matrix Σy. Thus, each Zb and its parent constitute a Gaussian
mixture model (GMM). Suppose the parent of a node is denoted as ∏(∙), the maginal distribution of
z is defined as follows
lB
p(z) = XYp(yjl∏(Yj))γN(Zb∣μ∏(zb), ∑∏(4),	⑵
which sums over all possible combinations of Y states. As a matter of fact, a GMM is a Gaussian
LTM that has only one latent variable connecting to all observed variables.
Let the latent structure of Y be S, defining the number of latent variables in Y, the number of
discrete states in each variable Y and the connectivity structure among all variables in z and Y.
And let the parameters for all conditional probabilities in the latent structure be Θ. Both the latent
structure S and the latent parameters Θ are unknown. We aim to jointly learn data representations
and the latent structure. The proposed LTVAE model is shown in Fig. 1. The latent structure S are
automatically learned from data and will be discussed in a later section.
Due to the existence of the generation network, the inference of the model is intratable. Instead,
we do amortized variational inference for the latent variable z by introducing an inference network
(Kingma & Welling, 2014) and define an approximate posterior q@(z|x). The evidence lower bound
(ELBO) LELBO of the marginal loglikelihood of the data given (S, Θ) is:
LELBO(X) = Eqφ(z∣χ)[lθgPθ(x|z)] + Eqφ(z∣χ)[lθgf PS(z, y;。)] + H[qφ(z|x)],
y
(3)
3
Published as a conference paper at ICLR 2019
where log ypS(z, y; Θ) is the marginal loglikelihood of the latent variable z under the latent tree
model, and HH is the entropy. The conditional generative distribution pθ(x|z) could be a Gaussian
distribution if the input data is real-valued, or a Bernoulli distribution if binary, parameterized by the
generation network. Using Monte Carlo sampling, the ELBO can be asymptotically estimated by
1M
LELBO(X) 'M ElOgPθ(x|z( )) + log£PS(z( ), y；®) + H[qφ(z∣x)],	(4)
i=1	y
where z(i) 〜qφ(z∣x). The term H[qφ(z|x)] can be computed analytically if We choose the form of
qφ(z|x) to be a Gaussian distributionN(z; μχ, σχ): H[qφ(z∣x)] =2 log(2π)+2 PJ=ι(1+log 吟,
where J is the dimensionality of z.
Furthermore, the marginal loglikelihood log PyPS(z(i), y; Θ) can be computed efficiently through
message passing. Message passing is an efficient algorithm for inference in Bayesian networks
(Koller & Friedman, 2009; Poon et al., 2013). In message passing, we first build a clique tree using
the factors in the defined probability density. Because of the tree structure, each zb along with its
parent form a clique with the potential ψ(zb, y) being the corresponding conditional distribution.
This is illustrated in Fig. 2. With the sampled z(i), we can compute the message ψ0(y) by absorbing
the evidence from z. During collecting message phase, the message ψ0(y) are sent towards the pivot.
After receiving all messages, the pivot distributes back messages towards all z. Both the posterior
of Y and the marginal loglikelihood of z(i) thus can be computed in the final normalization step.
3.2	Parameter Learning through Gradient Descent and Stepwise EM with
Message Passing
In this section, we propose efficient learning algorithms for LTVAE through gradient descent and
stepwise EM with message passing.
Given the latent tree model (S, Θ), the parameters of neural networks can be efficiently optimized
through stochastic gradient descent (SGD). However, in order to learn the model, it is important
to efficiently compute the gradient of the marginal loglikelihood log PS (z; Θ) from the latent tree
model, the third term in Eq. 4. Here, we propose an efficient method to compute gradient through
message passing. Let zb be the variables that we want to compute gradient with respect to, and let
Yb be the parent node. The marginal loglikelihood of full z can be written as
logPS(z；®) = logEN(Zbl4yb, %)f(yb)],
yb
(5)
where f(yb) is the collection of all the rest of the terms not containing zb. The gradient gzb of the
marginal loglikelihood log PS (z; Θ) w.r.t zb thus can be computed as
=	1	∂ Eybf (y)N(zb∣μyb, Σyb)
gzb	PS(z；⑼	∂Zb
P(yb|z)
yb
d log[f(yb)N(zb|〃yb，£yb)]
∂Zb
EP(Iyb IZ)∑rt1 (μyb - zb)
yb
(6)
whereP(yb|z) is the posterior probability ofyb and can be computed efficiently with message passing
as described in the previous section. The detailed derivation is in Appendix E. Since z = [z1, ..., zB],
we have
∂ log P(z)	∂ log P(z)
∂ Z	∂ zι
d logP(Z) I= [gz	gz ]
gz	[gZι ,..., gZBj .
(7)
With the efficient computation of the third term in Eq. 4 and its gradient w.r.t z through message
passing, the parameters of inference network and generation network can be efficiently optimized
through SGD.
In order to jointly learn the parameters of the latent tree Θ, we propose Stepwise EM algorithm
based on mini-batch of data. Specifically, we maximize the third term in Eq. 4, i.e. the marginal
loglikelihood of z under the latent tree. In the Stepwise E-step, we compute the distributions
P(y, y0|z, θ(t-1)) and P (y|z, θ(t-1)) for each latent node Y and its parent Y 0. In the Stepwise
4
Published as a conference paper at ICLR 2019
(a) Node insertion
(b) State insertion
(C) Pouching
(d) Node relocation
Figure 3: Structure search operators. The digits above the nodes denote the number of discrete
states. Node deletion, state deletion and unpouching are the inverse of node insertion, state insertion
and pouching, respectively.
M-step, we estimate the new parameter θ(t). Let s(z, y) be a vector the sufficient statistics for
a single data case. Let S = EpS(y|z@[s(z, y)] be the expected sufficient statistics for the data
case, where the expectation is w.r.t the posterior distribution of y with current parameter. And let
μ = PN=I Si be the sum of the expected sufficient statistics. The update of the parameter Θ is
performed as follows:
Si = EpS(yi∣z^Θt)[s(Zi, yi)]
μt+1 = μt + η(Si - μt)	(8)
Θt+1 = arg max l(μt+1, Θ),
Θ
where η is the learning rate and l is the complete data loglikelihood. Each iteration of update
of LTVAE thus is composed of one iteration of gradient descent update for the neural network
parameters and one iteration of Stepwise EM update for the latent tree model parameters with a
mini-batch of data.
3.3	Structure Learning
For the latent structure S, there are four aspects need to determine: the number of latent variables,
the cardinalities of latent variables, the connectivities among variables. We aim at finding the model
m* that maximizes the BIC score (Schwarz et al., 1978; Koller & Friedman, 2009):
BIC(m∣D) = log P(D∣m,θ*) - dm) log N,
where θ* is the MLE of the parameters and d(m) is the number of independent parameters. The
first term is known as the likelihood term. It favors models that fit data well. The second term is
known as the penalty term. It discourages complex models. Hence, the BIC score provides a trade-
off between model fit and model complexity. To this end, we perform systematic searching to find
a structure with a high BIC score. We use the hill-climing algorithm to search for m* as in (Poon
et al., 2010; 2013), and define 7 search operators: node introduction (NI) and node deletion (ND)
to introduce new latent nodes and delete existing nodes, state introduction (SI) and state deletion
(SD) to add a new state and delete a state for existing nodes, node relocation (NR) to change links
of existing nodes, pouching (PO) and unpouching (UP) operators to combine nodes into a single
node and separate variables from a node.. The structure search operators are shown in Fig. 3. Each
operator produces a set of candidates from existing structure, and the best candidate is picked if it
improves the previous one. To reduce the number of possible search candidates, we first perform
SI, NI and PO to expand the structure and pick the best model. Then we perform NR to adjust the
best model. Finally, we perform UP, ND and SD to simplify the current best structure and pick the
best one. Acceleration techniques (Poon et al., 2013) are adopted that make the algorithm efficient
enough. The structure learning is performed iteratively together with the parameter learning of
neural networks.
The overall learning algorithm is illustrated in Algorithm 1. Starting from a pretrained model, we it-
eratively improve the structure and parameters of latent tree model while learning the representations
of data through neural network in a greedy manner. Using current structure St as the initial structure,
5
Published as a conference paper at ICLR 2019
Algorithm 1 Learning Latent Tree Variational Autoencoder
Input: data D, z dim, neural networks, E
θ, φ, S0, Θ0 ― Pretrain(D)
repeat
for e = 1 to E do
for each minibatch X in D do
Compute qφ(z∣μχ,σχ)
Sample z(i)〜q(z∣μχ, σχ)
Compute logPS (z; Θ) and d logps(Z) from Eq. 5 and 7
Compute ELBO from Eq. 4
θ,φ — Back-propagation and SGD step
Θ — StePWiseEM(z(i))
end for
end for
DZ J μD
repeat
S*, Θ* J SearChWith(St-1, Θt-1, {SI, NL PO})
S*, Θ* J SearchWith(S*, Θ* {NR})
St, Θt J SearchWith(S*, Θ*, {UP, ND, SD})
until BIC(St, Θt∣Dz) ≤ BIC(St-1, Θt-1∣Dz)
until stopping criteria
return θ, φ, S , Θ
We search for a better model. With neW latent tree model, We optimize for a better representation
until convergence.
4	Experiments
4.1	Synthetic-Data Demonstration
We first demonstrate the effectiveness of the proposed method through synthetic data. Assume that
the data points have tWo facets Y1 and Y2 , Where each facet controlls a subset of attributes (e.g.
tWo-dimensional domain) and defines one partition over the data. This four-dimensional domain
z = {z1, z2, z3, z4} is a latent representation Which We do not observe. What We observe is x ∈ R100
that is obtained via the folloWing non-linear transformation:
x = σ(Uσ(Wz)),
Where W ∈ R10×4 andU ∈ R100×10 are matrices Whose entries folloW the zero-mean unit-variance
i.i.d. Gaussian distribution, σ(∙) is a sigmoid function to introduce nonlinearity. The generative
model is shoWn in Fig. 4 (a). We define tWo clusters in facet Y1 and tWo clusters in facet Y2, and
generate 5,000 samples of x. Under the above generative model, recovering the tWo facets Y1 and
Y2 structure and the latent z domain from the observation ofx seems very challenging. All previous
DNN-based methods (AE+GMM, DEC, DCN, etc.) are only able to discover one-facet of clustering
(i.e. one partition over the data), and none of these is applicable to solve such a multidimensional
clustering problem. Fig. 4 (b) shoWs the results of the proposed method. As one can see, the LTVAE
successfully discovers the true superstructure of Y1 and Y2 . The 2-d plot of z1 and z2 shoWs the
separable latent space clusters under facet Y1, and it matches the ground-truth cluster assignments.
Additionally, the 2-d plot of z3 and z4 shoWs another separable clusters under facet Y2, and it also
matches the ground-truth cluster assignments Well in the other facet.
4.2	Real-Data Experiment Setup
We evaluate the proposed LTVAE model on tWo image datasets and tWo other datasets, and compare
it against other deep learning based clustering algorithms, including tWo-stage methods, AE+GMM
and VAE+GMM, Which first learn AE/VAE (Kingma & Welling, 2014) models then construct a
GMM on top of them, and joint learning methods, DEC (Xie et al., 2016) and DCN (Yang et al.,
2017). The datasets include MNIST, STL-10, Reuters (Xie et al., 2016; Jiang et al., 2017) and
the Heterogeneity Human Activity Recognition (HHAR) dataset. When evaluating the clustering
6
Published as a conference paper at ICLR 2019
(a) Generative process for synthetic data
(b) Discovered superstructure and latent space
Figure 4: (a) The generative process of synthetic data; (b) The discovered multidimensional super-
structure and the latent space (different colors denote different ground truth clusters in each facet.)
Table 1: Test data loglikelihood for various datasets.
Model	-MNIst-	STL	Reuters	-HHAR
VAE	-86.64±0.20=	-743.72±0.5T=	-1312.40±1.2T=	-17.88±0.26=
IWAE	-85.39±0.13	-742.43±1.10	-1254.29±6.95	-16.53±0.16
LTVAE	-84.75±0.14一	-619.04±7.88~	-1245.71±4.45-	-13.65±0.53一
performance, for fair of comparison, we follow previous works (Xie et al., 2016; Yang et al., 2017)
and use the network structures of d-500-500-2000- 10 for the encoder network and 10-2000-
500 - 500 - d for the decoder network for all datasets, where d is the data-space dimension, which
varies among datasets. All layers are fully-connected. We follow the pretraining procedure as in (Xie
et al., 2016). We first perform greedy layer-wise pretraining in denoising autoencoder manner, then
stack all layers to form deep autoencoder. The deep autoencoder is further finetuned to minimize
the reconstruction loss. The weights of the deep autoencoder are used to intialize the weights of
encoder and decoder networks of above methods. After the pretraining, we optimze the objectives
of those methods. For DEC and DCN, we use the same hyperparameter settings as the original
papers. When initializing the cluster centroids for DEC and DCN, we perform 10 random restarts
and pick the results with the best objective value for k-means/GMM. For the proposed LTVAE, we
use Adam optimzer (Kingma & Ba, 2015) with initial learning rate of 0.001 and mini-batch size of
128. For Stepwise EM, we set the learning rate to be 0.01. As in Algorithm 1, we set E = 5, i.e. we
update the latent tree model every 5 epochs. When optimizing the candidate models during structure
search, we perform 10 random restarts and train with EM for 200 iterations.
4.3	Test Loglikelihood
We first show that, by using the marginal loglikelihood defined by the latent tree model as the prior,
LTVAE better fits the data than conventional VAE and importance weighted autoencoders (IWAE)
(Burda et al., 2016). While alternative quantitative criteria have been proposed (Bounliphone et al.,
2016; Im et al., 2016; Salimans et al., 2016) for generative models, log-likelihood of held-out test
data remains one of the most important measures of a generative model’s performance (Kingma
& Welling, 2014; Burda et al., 2016; Wu et al., 2017; Goyal et al., 2017). For comparison, we
approximate true loglikelihood L5000 using importance sampling (Burda et al., 2016): Lk(x) =
log 1 Pk=1 pθ(X,z)(∣X), where z(i) Z qφ(z|x). The results for all datasets are shown in Table 1. The
proposed LTVAE obtains a higher test data loglikelihood and ELBO, implying that it can better
model the underlying complex data distribution embedded in the image data.
7
Published as a conference paper at ICLR 2019
777777777
∖ 1 ∖ 1 ∖ T ∖ ∖ I
533333333
∕y∕^^yyyy
2ZZZ乙2Z22
MgyggFgVP
DoOoOooOO
ʃʃʃʃʃ^r^ʃ^
仿{√ tp (p Cfi J (£> J
/////////
夕夕夕夕夕夕夕夕夕
HqHqd OMQA/
ʃʃʃʃʃs-erʃr
00 oOO。O 0。。
V \ \ \ \ \ \ \ \ \
4yqqqq7rH7
Y7f77i∕ + 77 4
///j/，///
L> J QQloSQ Q [ Q
(b) Facet 2
Λl<HMMH⅛4∕⅛⅛
∕√∕√∕^√√z∕
TTTTTTW7T
Q4<4>qqqq?
夕夕夕夕夕夕夕夕夕
7γ77γ77T7TV7,
X883g2S883
(a) Facet 1
(c) Pose variation examples
Figure 5: Two facet clustering results from LTVAE are in (a) digit identity and (b) shape and pose.
Each row contains the top 10 scoring elements from one cluster. (c) shows the pose variations by
fixing the digit cluster in facet 1 and changing the cluster in facet 2. It can be seen that up-right,
left-tilted and right-tilted images of the same digits are clearly recognizable.
Table 2: Clustering Accuracy of clustering results.
Model	MNIST	STL-10	Reuters	HHAR
AE+GMM	82.18%	79.83%	68.68%	78.90%
VAE+GMM	76.87%	79.49%	65.85%	67.91%
DEC	84.30%	80.62%	74.32%	79.86%
DCN	83.32%	85.88%	75.05%	81.26%
LTVAE	86.32%	90.00%	80.96%	85.00%
4.4	Multifacet Clustering
The most important features of the proposed model are that it can perform variable selection for
model-based clustering, leading to multiple facets clustering.
We use the standard unsupervised evaluation metric and protocols for evaluations and comparisons
to other algorithms (Yang et al., 2010). For baseline algorithms we set the number of clusters to
the number of ground-truth categories. While for LTVAE, it automatically determines the num-
ber of facets and latent superstructure through structure learning. We evaluate performance with
unsupervised clustering accuracy (ACC):
ACC = max Pn=I 1也=m(Ci)},
mn
where li is the groundtruth label, Ci is the cluster assignment produced by the algorithm, and m
ranges over all possible mappings between clusters and labels. Table 2 show the quantitative clus-
tering results compared with previous works. With z dimension of small value like 10, LTVAE
usually discovers only one facet. It can be seen the, for MNIST dataset LTVAE achieves cluster-
ing accuracy of 86.32%, better than the results of other methods. This is also the case for STL-10,
Reuters and HHAR.
More importantly, the proposed LTVAE does not just give one partition over the data. Instead, it
explains the data in multi-faceted ways. Unlike previous clustering experiments, for this experiment,
we choose the z dimension to be 20. Fig. 5 shows the two facet clustering results for MNIST. It
can be seen that facet 1 gives quite clean clustering over the identity of the digits and the ten digits
are well separated. On the other hand, facet 2 gives a more grand partition based on the shape and
pose. Note how up-right “4” and “9” are similar, and how tilted “4”,“7” and “9” are similar. The
facet meanings are more evident in Fig. 5 (c). Fig. 6 shows four facets discovered for the STL-10
8
Published as a conference paper at ICLR 2019
Figure 6: Clustering results from LTVAE for STL-10
dataset. Each row contains the top 5 scoring elements from
one cluster.
(b) Facet 2
(c) Facet 3
(c) Facet 4
O O
H ¥
5 3
2 1
0 O
i “
3 6
2 Z
/ /
q 9
b &
o。
U,
S O
Z L
/ /
?夕
b 6
O O
9 4
o 5
2 2
(/
q 9
b a
7 I 8 Rrq
7 / sʃʃef
7 / 夕 QrH
71 yav>q
71 g75*q
7 7
I I
g K
3 4
Figure 7: The digits generated by
the proposed model. Digits in the
same row come from the same la-
tent code of the latent tree.
9 9
。6
O “ 3 2
7 7 7
, 2J(
(ʃ Z
dataset. Although it is hard to characterize precisely how the facets differ from each other, there are
visible patterns. For example, the cats, monkeys and birds in facet 2 have clearly visible eyes, while
this is not always true in facet 1. The deers in facet 2 are all showing their antlers/ears, while this is
not true in facet 3. In facet 2 we see frontal views of cars, while in facets 1 and 3 we see side view of
cars. In facet 1, each cluster consists of the same types of objects/animal. In facet 3/4, images in the
same cluster do not necessarily show the same type of objects/animals. However, they have similar
overall feel.
4.5	Image Generation
Since the structure of the data in latent space is automatically learned through the latent tree, we
can sample the data in a more structured way. One way is through ancestral sampling, where we
first sample the root of the latent tree and then hierarchically sample the children variables to get z,
from which the images can be generated through generation network. The other way is to pick one
component from the Gaussian mixture and sample z from that component. This produces samples
from a particular cluster. Fig. 7 shows the samples generated in this way. As it can be seen,
digits sampled from each component has clear semantic meaning and belong to the same category.
Whereas, the samples generated by VAE does not have such structure. Conditional image generation
can also be performed to alter the attributes of the same digit as shown in Appendix B.
5	Discussions
LTVAE learns the dependencies among latent variables Y. In general, latent variables are often
correlated. For example, the social skills and academic skills of a student are generally correlated.
Therefore, its better to model this relationship to better fit the data. Experiments show that removing
such dependencies in LTVAE models results in inferior data loglikelihood.
In this paper, for the inference network, we simply use mean-field inference network with same
structure as the generative network (Kingma & Welling, 2014). However, the limited expressiveness
of the mean-field inference network could restrict the learning in the generative network and the
quality of the learned model (Webb et al., 2018; Rainforth et al., 2018; Cremer et al., 2018). Using
a faithful inference network structure as in (Webb et al., 2018) to incorporate the dependencies
among latent variables in the posterior, for example one parameterized with masked autoencoder
distribution estimator (MADE) model (Germain et al., 2015), could have a significant improvement
in learning. We leave it for future investigation.
9
Published as a conference paper at ICLR 2019
6	Conclusions
In this paper, we propose an unsupervised learning method, latent tree variational autoencoder (LT-
VAE), which simultaneously performs representation learning and multidimensional clustering. Dif-
ferent from previous deep learning based clustering methods, LTVAE learns latent embeddings from
data and discovers multi-facet clustering structure based on subsets of latent features rather than one
partition over data. Experiments show that the proposed method achieves state-of-the-art clustering
performance and reals reasonable multifacet structures of the data.
Acknowledgments
Research on this article was supported by Hong Kong Research Grants Council under grants
16212516 and 16202118.
References
Charu C Aggarwal and Chandan K Reddy. Data clustering: algorithms and applications. CRC
press, 2013.
David M Blei, Michael I Jordan, et al. Variational inference for dirichlet process mixtures. Bayesian
analysis,1(1):121-144, 2006.
Wacha Bounliphone, Eugene Belilovsky, Matthew B Blaschko, Ioannis Antonoglou, and Arthur
Gretton. A test of relative similarity for model selection in generative models. In Proceedings of
the International Conference on Learning Representations, 2016.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In Pro-
ceedings of the International Conference on Learning Representations, 2016.
Tao Chen, Nevin L Zhang, Tengfei Liu, Kin Man Poon, and Yi Wang. Model-based multidimen-
sional clustering of categorical data. Artificial Intelligence, 176(1):2246-2269, 2012.
M Bishop Christopher. Pattern Recognition and Machine Learning. Springer-Verlag New York,
2016.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoen-
coders. In International Conference on Machine Learning, 2018.
Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai
Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture varia-
tional autoencoders. arXiv preprint arXiv:1611.02648, 2016.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for
distribution estimation. In International Conference on Machine Learning, pp. 881-889, 2015.
Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, and Eric Xing. Nonparametric varia-
tional auto-encoders for hierarchical representation learning. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, 2017.
Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with
local structure preservation. In International Joint Conference on Artificial Intelligence (IJCAI-
17), pp. 1753-1759, 2017.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527-1554, 2006.
Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, and Roland Memisevic. Generating images
with recurrent adversarial networks. arXiv preprint arXiv:1602.05110, 2016.
10
Published as a conference paper at ICLR 2019
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep
embedding: An unsupervised and generative approach to clustering. In International Joint Con-
ference on Artificial Intelligence, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the
International Conference on Learning Representations, 2014.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT
press, 2009.
James MacQueen et al. Some methods for classification and analysis of multivariate observations. In
Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1,
pp. 281-297. Oakland, CA, USA, 1967.
Raphael Mourad, Christine Sinoquet, Nevin L Zhang, Tengfei Liu, and PhiliPPe Leray. A survey on
latent tree models and applications. Journal of Artificial Intelligence Research, 2013.
Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan
Kaufmann, 2014.
Leonard Poon, Nevin L Zhang, Tao Chen, and Yi Wang. Variable selection in model-based clus-
tering: To do or to facilitate. In Proceedings of the 27th International Conference on Machine
Learning (ICML-10), pp. 887-894, 2010.
Leonard KM Poon, Nevin L Zhang, Tengfei Liu, and April H Liu. Model-based clustering of
high-dimensional data: Variable selection versus facet determination. International Journal of
Approximate Reasoning, 54(1):196-215, 2013.
Tom Rainforth, Adam R Kosiorek, Tuan Anh Le, Chris J Maddison, Maximilian Igl, Frank Wood,
and Yee Whye Teh. Tighter variational bounds are not necessarily better. In International Con-
ference on Machine Learning, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Gideon Schwarz et al. Estimating the dimension ofa model. The annals of statistics, 6(2):461-464,
1978.
Michael Steinbach, Levent ErtOz, and Vipin Kumar. The challenges of clustering high dimensional
data. In New directions in statistical physics, pp. 273-309. Springer, 2004.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Association, 101(1):1566-1581, 2006.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096-1103. ACM, 2008.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. Journal of Machine Learning Research, 11(Dec):3371-3408, 2010.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395-416,
2007.
Stefan Webb, Adam Golinski, Robert Zinkov, N Siddharth, Tom Rainforth, Yee Whye Teh, and
Frank Wood. Faithful inversion of generative models for effective amortized inference. In Ad-
vances in Neural Information Processing Systems, 2018.
11
Published as a conference paper at ICLR 2019
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of
decoder-based generative models. In Proceedings of the International Conference on Learning
Representations, 2017.
Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.
In International Conference on Machine Learning, pp. 478-487, 2016.
Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces:
Simultaneous deep learning and clustering. In International Conference on Machine Learning,
2017.
Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations
and image clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5147-5156, 2016.
Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting Zhuang. Image clustering using local
discriminant models and global integration. IEEE Transactions on Image Processing, 19(10):
2761-2773, 2010.
Nevin L Zhang. Hierarchical latent class models for cluster analysis. Journal of Machine Learning
Research, 5(6):697-723, 2004.
Nevin L Zhang and Leonard KM Poon. Latent tree analysis. In AAAI, pp. 4891-4898, 2017.
12
Published as a conference paper at ICLR 2019
A Superstructures
For the MNIST dataset, the conditional probability between identity facet Y1 (x-axis) and pose facet
Y2 (y-axis) is shown in Fig. 8. It can be seen that a cluster in Y1 facet could correspond to multiple
clusters in Y2 facet due to the conditional probability, e.g. cluster 0, 4, 5, 11 and 12. However, not
all clusters in Y2 facet are possible for a given cluster in Y1 facet.
Figure 8: Conditional probability of Y1 and Y2 for the two facets of MNIST discovered by LTVAE.
B Conditional Image generation
Here we show more results on conditional image generation. Interestingly, with LTVAE, we can
change the original images by fixing variables in some facet and sampling in other facets. For
example, in MNIST we can fix the variables in identity facet and change the pose of the digit by
sampling in the pose facet. Fig. 9 shows the samples generated in this way. As it can be seen, the
pose of the input digits are changed in the samples generated by the proposed method.
/夕 vɪ//U//
q 夕 q q 9 g 夕 q g q
夕，夕夕，夕夕，夕夕
夕SJ9993223
9夕夕夕989829
Figure 9: Image generation. Left are the original image. Right are generated with the proposed
model by fixing the variables in identity facet and sampling the variables in the pose facet. Digits in
the same row come from the same latent code of the latent tree.
13
Published as a conference paper at ICLR 2019
C Computational time
We compare the computational time of the proposed LTVAE w/ structure learning and that w/ fixed
structure. For LTVAE with fixed structure, we fixed the structure of the latent tree model to be a
single Y connecting to all zs, in which each z node consists of single z variable.
Table 3: Computational time (s) w/ and w/o structure learning.
Model	MNIST	STL-10	Reuters	HHAR
LTVAE w/ structure learning	7,592	3,251	5,756	5,693
LTVAE w/ fixed structure	3,197	-542-	1,442	1,021
D Similarity between learned facets
Different facets learned by LTVAE might have some overlaps among each other. Here we make
quantitative comparison among different facets based on the cluster assignments in each facet. We
evaluate the similarity between two clusterings Y1 and Y2 using normalized mutual information
NMI(Y1;Y2). The NMI is given by
NMI(Y1;Y2)
I (Yι; K)
PH(Y1)H(Y2)
where I(Y1; Y2) is the mutual information between Y1 and Y2 and H(V ) is the entropy of a variable
V . These quantities can be computed from P(Y1; Y2), which in turn is estimated by P(Y1; Y2) =
N PN=ι P(Yl∣di)P(Y2∣di), where di,…，dw are the samples in the test data.
Table 4: NMI between different facets learned by LTVAE and groundtruth for MNIST dataset.
	Groundtruth	Facet 1	Facet 2
Groundtruth	1	=	0.825	0.574
Facet 1	0.825	1	0.682
Facet 2	0.574	0.682	1
Table 5: NMI between different facets learned by LTVAE and groundtruth for STL dataset.
	Groundtruth	Facet 1	Facet 2	Facet 3	Facet 4
Groundtruth	1	=	0.8613	0.6279	0.5758	0.5536
Facet 1	-0.8613-	1	0.6962	0.6314	0.6251
Facet 2	-0.6279-	0.6962	1	0.4675	0.5031
Facet 3	-0.5758-	0.6314	0.4675	1	0.5885
Facet 4	-0.5536-	0.6251	0.5031	0.5885	1
14
Published as a conference paper at ICLR 2019
E Derivation of gradient
Here we give detailed derivation of Equation 6. The gradient gzb of the marginal loglikelihood
log pS (z; Θ) w.r.t zb thus can be computed as
gzb
∂ log pS (z; Θ)
∂zb
1	∂ps⑵㊀)
PS⑵㊀)∂zb
1	∂ Eyb f(加 N(Zb∖μyb , Eyb )
PS⑵㊀)	∂ Zb
X 1	d[f (yb)N(zMμyb, Eyb)]
乙 PS(z；Θ)	∂Zb
(9)
Σ
yb
f(yb)N (zb|〃yb，£yb) d log[f(yb)N (zb|〃yb，Eyb)]
PS (z；Θ)	∂ Zb
P(yb|z)
yb
d log[f(yb)N(Zb Wyb , Eyb )]
∂Zb
p ^p(yb|z)Eyb1 (μyb - Zb)
yb
whereP(yb|Z) is the posterior probability ofyb and can be computed efficiently with message passing
as described in the previous section. Note that
P	∣Z) = fyybNzJμ	ς )	(10)
PS (Z; Θ)
is valid due to PS(z; Θ) = Pyb N(Zb∣μyb, Σybb )f (yb) and the Bayes rule.
15