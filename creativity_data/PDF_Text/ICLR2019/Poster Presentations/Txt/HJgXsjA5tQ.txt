Published as a conference paper at ICLR 2019
On the loss landscape of a class of deep
NEURAL NETWORKS WITH NO BAD LOCAL VALLEYS
Quynh Nguyen
Saarland University, Germany
Mahesh Chandra Mukkamala
Saarland University, Germany
Matthias Hein
University of Tubingen, Germany
Ab stract
We identify a class of over-parameterized deep neural networks with standard
activation functions and cross-entropy loss which provably have no bad local valley,
in the sense that from any point in parameter space there exists a continuous path
on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero.
This implies that these networks have no sub-optimal strict local minima.
1	Introduction
It has been empirically observed in deep learning (Dauphin et al., 2014; Goodfellow et al., 2015) that
the training problem of over-parameterized1 deep CNNs (LeCun et al., 1990; Krizhevsky et al., 2012)
does not seem to have a problem with bad local minima. In many cases, local search algorithms
like stochastic gradient descent (SGD) frequently converge to a solution with zero training error
even though the training objective is known to be non-convex and potentially has many distinct local
minima (Auer et al., 1996; Safran & Shamir, 2018). This indicates that the problem of training
practical over-parameterized neural networks is still far from the worst-case scenario where the
problem is known to be NP-hard (Blum & Rivest., 1989; Sima, 2002; Livni et al., 2014; Shalev-
Shwartz et al., 2017). A possible hypothesis is that the loss landscape of these networks is“well-
behaved” so that it becomes amenable to local search algorithms like SGD and its variants. As not all
neural networks have a well-behaved loss landscape, it is interesting to identify sufficient conditions
on their architecture so that this is guaranteed. In this paper our motivation is to come up with such
a class of networks in a practically relevant setting, that is we study multi-class problems with the
usual empirical cross-entropy loss and deep (convolutional) networks and almost no assumptions on
the training data, in particular no distributional assumptions. Thus our results directly apply to the
networks which we use in the experiments.
Figure 1: An example loss landscape with bad local valleys (left) and without bad local valley (right).
Our contributions. We identify a family of deep networks with skip connections to the output
layer whose loss landscape has no bad local valleys (see Figure 1 for an illustration). Our setting is
1These are the networks which have more parameters than necessary to fit the training data
1
Published as a conference paper at ICLR 2019
for the empirical loss and there are no distributional assumptions on the training data. Moreover, we
study directly the standard cross-entropy loss for multi-class problems. There are little assumptions
on the network structure which can be arbitrarily deep and can have convolutional layers (weight
sharing) and skip-connections between hidden layers. From a practical perspective, one can generate
an architecture which fulfills our conditions by taking an existing CNN architecture and then adding
skip-connections from a random subset of N neurons (N is the number of training samples), possibly
from multiple hidden layers, to the output layer (see Figure 2 for an illustration). For these networks
we show that there always exists a continuous path from any point in parameter space on which the
loss is non-increasing and gets arbitrarily close to zero. We note that this implies the loss landscape
has no strict local minima, but theoretically non-strict local minima can still exist. Beside that, we
show that the loss has also no local maxima.
Beside the theoretical analysis, we show in experiments that despite achieving zero training error,
the aforementioned class of neural networks generalize well in practice when trained with SGD
whereas an alternative training procedure guaranteed to achieve zero training error has significantly
worse generalization performance and is overfitting. Thus we think that the presented class of neural
networks offer an interesting test bed for future work to study the implicit bias/regularization of SGD.
2	Description of network architecture
We consider a family of deep neural networks which have d input units, H hidden units, m output
units and satisfy the following conditions:
1.	Every hidden unit of the first layer can be connected to an arbitrary subset of input units.
2.	Every hidden unit at higher layers can take as input an arbitrary subset of hidden units from
(multiple) lower hidden layers.
3.	Any subgroup of hidden units lying on the same layer can have non-shared or shared weights,
in the later case their number of incoming units have to be equal.
4.	There exist N hidden units which are connected to the output nodes with independent
weights (N denotes the number of training samples).
5.	The output of every hidden unit j in the network, denoted as fj : Rd → R, is given as
fj (x) = σj bj + X fk(x)uk→j
k:kfj
where x ∈ Rd is an input vector of the network, σj : R → R is the activation function of
unit j, bj ∈ R is the bias of unit j, and uk→j ∈ R the weight from unit k to unit j .
This definition covers a class of deep fully connected and convolutional neural networks with an
additional condition on the number of connections to the output layer. In particular, while conventional
architectures have just connections from the last hidden layer to the output, we require in our setting
that there must exist at least N neurons, “regardless” of their hidden layer, that are connected to the
output layer. Essentially, this means that if the last hidden layer of a traditional network has just
L < N neurons then one can add connections from N - L neurons in the hidden layers below it to
the output layer so that the network fulfills our conditions.
Similar skip-connections have been used in DenseNet (Huang et al., 2017) which are different from
identity skip-connections as used in ResNets (He et al., 2016). In Figure 2 we illustrate a network with
and without skip connections to the output layer which is analyzed in this paper. We note that several
architectures like DenseNets Huang et al. (2017) already have skip-connections between hidden
layers in their original architecture, whereas our special skip-connections go from hidden layers
directly to the output layer. As our framework allow both kinds to exist in the same network (see
Figure 2 for an example), we would like to separate them from each other by making the convention
that in the following skip-connections, if not stated otherwise, always refer to ones which connect
hidden neurons to output neurons.
We denote by d the dimension of the input and index all neurons in the network from the input layer
to the output layer as 1, 2, . . . , d, d + 1, . . . , d + H, d + H + 1, . . . , d + H + m which correspond
to d input units, H hidden units and m output units respectively. As we only allow directed
2
Published as a conference paper at ICLR 2019
Figure 2: Left: An example neural network represented as directed acyclic graph. Right: The same
network with skip connections added from a subset of hidden neurons to the output layer. All neurons
with the same color can have shared or non-shared weights.
arcs from lower layers to upper layers, it follows that k < j for every k → j. Let N be the
number of training samples. Suppose that there are M hidden neurons which are directly connected
to the output with independent weights where it holds N ≤ M ≤ H. Let {p1, . . . , pM} with
pj ∈ {d + 1, . . . , d + H} be the set of hidden units which are directly connected to the output units.
Let in(j) be the set of incoming nodes to unit j and uj = [uk→j]k∈in(j) the weight vector of the
j-th unit. Let U = (ud+1, . . . , ud+H, bd+1, . . . , bd+H) denote the set of all weights and biases of all
hidden units in the network. Let V ∈ RM ×m be the weight matrix which connects the M hidden
neurons to the m output units of the network. An important quantity in the following is the matrix
Ψ ∈ RN×M defined as
fpi (XI)	... fPM (XI )
Ψ =	.	.	(1)
..
fpi (XN )	. . . fpM (XN )
As Ψ depends on U, we write ΨU or Ψ(U) as a function of U. Let G ∈ RN×m be the output of the
network for all training samples. In particular, Gij is the value of the j -th output neuron for training
sample Xi . It follows from our definition that
M
Gij = hΨi:, V:ji = Xfpk(Xi)Vkj, ∀i ∈ [N],j ∈ [m]
k=1
Let (Xi, yi)iN=1 be the training set where yi denotes the target class for sample Xi. In the following
we analyze the commonly used cross-entropy loss given as
NG
1	e iyi
φ(U, V) = N X - log (Pm eGik )	(2)
i=1	k=1
We refer to Section C in the appendix for extension of our results to general convex losses. The
cross-entropy loss is bounded from below by zero but this value is not attained. In fact the global
minimum of the cross-entropy loss need not exist e.g. if a classifier achieves zero training error
then by upscaling the function to infinity one can drive the loss arbitrarily close to zero. Due to this
property, we do not study the global minima of the cross-entropy loss but the question if and how one
can achieve zero training error. Moreover, we note that sufficiently small cross-entropy loss implies
zero training error as shown in the following lemma.
Lemma 2.1 If Φ(U,V) < loN2) ,then the training error is zero.
Proof: We note that if Φ(U,V) < loN2), then it holds due to the positivity of the loss,
maxi=1,...,N -log
i=1
< log(2).
3
Published as a conference paper at ICLR 2019
This implies that for all i = 1, . . . , N ,
log(1 + X eGik-Giyi) < log(2)	=⇒	X eGik-Giyi < L
k6=yi	k6=yi
In particular: maxk6=yi eGik-Giyi < 1 and thus maxk6=yi Gik - Giyi < 0 for all i = 1, . . . , N
which implies the result.
3 Main res ult
The following conditions are required for the main result to hold.
Assumption 3.1	1. All activation functions {σd+1, . . . , σd+H} are real analytic and strictly
increasing
2.	Among M neurons {p1 , . . . , pM} which are connected to the output units, there exist
N ≤ M neurons, say w.l.o.g. {p1, . . . ,pN}, such that one of the following conditions hold:
•	For every 1 ≤ j ≤ N : σpj is bounded and limt→-∞ σpj (t) = 0
•	For every 1 ≤ j ≤ N : σpj is the softplus activation (3), and there exists a backward
path from pj to the first hidden layer s.t. on this path there is no neuron which has
skip-connections to the output or shared weights with other skip-connection neurons.
3.	The input patches of different training samples are distinct. In particular, let n1 be the
number of units in the first hidden layer and denote by Si for i ∈ [d + 1, d + n1] their input
support, then for all r 6= s ∈ [N], and i ∈ [d + 1, d + n1], it holds xr |Si 6= xs|Si.
The first condition of Assumption 3.1 is satisfied for softplus, sigmoid, tanh, etc, whereas the
second condition is fulfilled for sigmoid and softplus. For softplus activation function (smooth
approximation of ReLU),
σγ(t) = ɪ log(1 + eγt),	for some γ > 0,	(3)
we require an additional assumption on the network architecture. The third condition is always
satisfied for fully connected networks if the training samples are distinct. For CNNs, this condition
means that the corresponding input patches across different training samples are distinct. This could
be potentially violated if the first convolutional layer has very small receptive fields. However, if this
condition is violated for the given training set then after an arbitrarily small random perturbation of
all training inputs it will be satisfied with probability 1. Note that the M neurons which are directly
connected to the output units can lie on different hidden layers in the network. Also there is no
condition on the width of every individual hidden layer as long as the total number of hidden neurons
in the network is larger than N so that our condition M ≥ N is feasible.
Overall, we would like to stress that Assumption 3.1 covers a quite large class of interesting network
architectures but nevertheless allows us to show quite strong results on their empirical loss landscape.
The following key lemma shows that for almost all U, the matrix Ψ(U) has full rank.
Lemma 3.2 Under Assumption 3.1, the set of U such that Ψ(U) has not full rank N has Lebesgue
measure zero.
Proof: (Proof sketch) Due to space limitation, we can only present below a proof sketch. We refer
the reader to the appendix for the detailed proof. The proof consists of two main steps. First, we
show that there exists U Stthe submatrix Wi：n,i：N has full rank. By Assumption 3.1, all activation
functions are real analytic, thus the determinant of Wi：n,i：N is a real analytic function of the network
parameters which Ψ depends on. By the first result, this determinant function is not identically zero,
thus Lemma A.1 shows that the set of U for which Ψ has not full rank has Lebesgue measure zero.
Now we sketch the proof for the first step, that is to find a U s.t. Ψ has full rank. By Assumption
3.1.3, one can always choose the weight vectors of the first hidden layer so that every neuron at this
4
Published as a conference paper at ICLR 2019
layer has distinct values for different training samples. For higher neurons, we set their initial weight
vectors to be unit vectors with exactly one 1 and 0 elsewhere. Note that the above construction of
weights can be easily done so that all the neurons from the same layer and with the same number of
incoming units can have shared/unshared weights according to our description of network architecture
in Section 2. Let c(j) be the neuron below j s.t. uc(j)→j = 1. To find U, we are going to scale up
each weight vector Uj by a positive scalar aj. The idea is to show that the determinant of Wi：n,i：N is
non-zero for some positive value of {αj}. The biases can be chosen in such a way that the following
holds for some β ∈ R,
Ψij =	fpj (Xi)=	σpj(β	+	αpj	(fc(pj. )(xi)- fc(pj )(xj)))	∀j	∈ [N ]	(4)
where fc(pj)(x) = σc(pj)(αc(pj)σc(c(pj))(. . . fqj (x) . . .)) with qj being the index of some neuron
in the first hidden layer. Note that by our construction the value of unit qj is distinct at different
training samples, and thus it follows from the strict monotonic property of activation functions
from Assumption 3.1 and the positivity of αc(pj ), . . . that fc(pj) (xi) 6= fc(pj ) (xj) for every
i 6= j . Next, we show that the set of training samples can be re-ordered in such a way that it holds
fc(pj)(xi) < fc(pj)(xj) for every i > j. Note that this re-ordering does not affect the rank of Ψ.
Now, the intuition is that if one let αpj go to infinity then Ψij converges to zero for i > j because it
holds for all activations from Assumption 3.1 that limt→-∞ σpj (t) = 0. Thus the determinant of
Wi：i,i：N converges to QN=I σpj (β) which can be chosen to be non-zero by a predefined value of β,
in which case Ψ will have full rank. The detailed proof basically will show how to choose the specific
values of {αj} so that all the above criteria are met. In particular, it is important to make sure that the
weight vectors of two neurons j,j0 from the same layer will be scaled by the same factor αj = αj0 as
we want to maintain any potential weight-sharing conditions. The choice of activation functions from
the second condition of Assumption 3.1 basically determines how the values ofα should be chosen.
While we conjecture that the result of Lemma 3.2 holds for softplus activation function without the
additional condition as mentioned in Assumption 3.1, the proof of this is considerably harder for such
a general class of neural networks since one has to control the output of neurons with skip connection
from different layers which depend on each other. However, please note that the condition is also
not too restrictive as it just might require more connections from lower layers to upper layers but it
does not require that the network is wide. Before presenting our main result, we first need a formal
definition of bad local valleys.
Definition 3.3 The α-sublevel set of Φ is defined as Lα = {(U, V ) | Φ(U, V ) < α} . A local valley
is defined as a connected component of some sublevel set Lα. A bad local valley is a local valley on
which the loss function Φ cannot be made “arbitrarily small”.
Intuitively, a typical example of a bad local valley is a small neighborhood around a sub-optimal
strict local minimum. We are now ready to state our main result.
Theorem 3.4 The following holds under Assumption 3.1:
1.	There exist uncountably many solutions with zero training error.
2.	The loss landscape of Φ does not have any bad local valley.
3.	There exists no suboptimal strict local minimum.
4.	There exists no local maximum.
Proof:
1.	By Lemma 3.2 the set of U such that Ψ(U) has not full rank N has Lebesgue measure zero.
Given U such that Ψ has full rank, the linear system Ψ(U)V = Y has for every possible
target output matrix Y ∈ RN ×m at least one solution V . As this is possible for almost all
U , there exist uncountably many solutions achieving zero training error.
2.	Let C be a non-empty, connected component of some α-sublevel set Lα for α > 0. Suppose
by contradiction that the loss on C cannot be made arbitrarily small, that is there exists
5
Published as a conference paper at ICLR 2019
an > 0 such that Φ(U, V ) ≥ for all (U, V ) ∈ C, where < α. By definition,
Lα can be written as the pre-image of an open set under a continuous function, that is
Lα = Φ-1({a | a < α}), and thus Lα must be an open set (see Proposition A.2). Since C
is a non-empty connected component of Lα, C must be an open set as well, and thus C has
non-zero Lebesgue measure. By Lemma 3.2 the set of U where Ψ(U) has not full rank has
measure zero and thus C must contain a point (U, V ) such that Ψ(U) has full rank. Let Y
be the usual zero-one one-hot encoding of the target network output. As Ψ(U) has full rank,
there always exist V * such that Ψ(U)V * = Yt*, where t* = log ( m-1) Note that the loss
of (U, V*) is	e2
/	°t*	、	U
φ(U, V*) = - log J +(m - J ) = IOg(I + (m - Mt* ) = 2.
As the cross-entropy loss Φ(U, V) is convex in V and Φ(U, V) < α we have for the line
segment V(λ) = λV + (1- λ)V* for λ ∈ [0, 1],
Φ(U,V(λ)) ≤ λΦ(U,V) + (1 - λ)Φ(U,V*) <λα + (1 - λ) j <α.
Thus the whole line segment is contained in Lα and as C is a connected component it has to
be contained in C. However, this contradicts the assumption that for all (U, V) ∈ C it holds
Φ(U, V) ≥ j. Thus on every connected component C of Lα the training loss can be made
arbitrarily close to zero and thus the loss landscape has no bad valleys.
3.	Let (U0, V0) be a strict suboptimal local minimum, then there exists r > 0 such that
Φ(U,V) > Φ(U0,V0) > 0 for all (U, V) ∈ B ((U0,V0),r) ∖{(U0,V0)} where B(∙,r)
denotes a closed ball of radius r. Let a = min,^ʌʃ(e∂b((u V)T) Φ(U V) which exists
as Φ is continuous and the boundary ∂B (U0, V0), r of B (U0, V0), r is compact. Note
that α > Φ(U0, V0) as (U0, V0) is a strict local minimum. Consider the sub-level set
D = L α+Φ(u0,v0). AS Φ(Uo,V0) < α+φ(U0,V0) it holds (U0,V0) ∈ D. Let E be the
connected component of D which contains (U0, V0), that is, (U0, V0) ∈ E ⊆ D. It holds
E ⊂ B((U0,V0),r) as Φ(U,V) < α+φ(U0,V0) < α for all (U, V) ∈ E. Moreover,
Φ(U, V) ≥ Φ(U0, V0) > 0 for all (U, V) ∈ E and thus Φ can not be made arbitrarily small
on a connected component of a sublevel set of Φ and thus E would be a bad local valley
which contradicts 3.3.2.
4.	Suppose by contradiction that (U, V) is a local maximum. Then the Hessian of Φ is negative
semi-definite. However, as principal submatrices of negative semi-definite matrices are
again negative semi-definite, then also the Hessian of Φ w.r.t V must be negative semi-
definite. However, Φ is always convex in V and thus its Hessian restricted to V is positive
semi-definite. The only matrix which is both p.s.d. and n.s.d. is the zero matrix. It follows
that VVΦ(U, V) = 0. One can easily show that
N	eGij	eGij
vVj φ = X P=L (1 - P=L NM
From Assumption 3.1 it holds that there exists j ∈ [N] s.t. σpj is strictly positive, and
Gij
thus some entries of Ψi: must be strictly positive. Moreover, one has Pme 】eGi% ∈ (0,1).
It follows that some entries of V2V Φ must be strictly positive. Thus V2V Φ cannot be
identically zero, leading to a contradiction. Therefore Φ has no local maximum.
Theorem 3.4 shows that there are infinitely many solutions which achieve zero training error, and the
loss landscape is nice in the sense that from any point in the parameter space there exists a continuous
path that drives the loss arbitrarily close to zero (and thus a solution with zero training error) on
which the loss is non-increasing.
While the networks are over-parameterized, we show in the next Section 4 that the modification
of standard networks so that they fulfill our conditions leads nevertheless to good generalization
6
Published as a conference paper at ICLR 2019
performance, often even better than the original network. We would like to note that the proof of
Theorem 3.4 also suggests a different algorithm to achieve zero training error: one initializes all
weights, except the weights to the output layer, randomly (e.g. Gaussian weights), denoted as U, and
then just solves the linear system Ψ(U)V = Y to obtain the weights V to the output layer. Basically,
this algorithm uses the network as a random feature generator and fits the last layer directly to achieve
zero training error. The algorithm is successful with probability 1 due to Lemma 3.2. Note that
from a solution with zero training error one can drive the cross-entropy loss to zero by upscaling
to infinity but this does not change the classifier. We will see, that this simple algorithm shows bad
generalization performance and overfitting, whereas training the full network with SGD leads to good
generalization performance. This might seem counter-intuitive as our networks have more parameters
than the original networks but is inline with recent observations in Zhang et al. (2017) that state-of-the
art networks, also heavily over-parameterized, can fit even random labels but still generalize well on
the original problem. Due to this qualitative difference of SGD and the simple algorithm which both
are able to find solutions with zero training error, we think that our class of networks is an ideal test
bed to study the implicit regularization/bias of SGD, see e.g. Soudry et al. (2018).
4	Experiments
The main purpose of this section is to investigate the generalization ability of practical neural networks
with skip-connections added to the output layer to fulfill Assumption 3.1.
Datasets. We consider MNIST and CIFAR10 datasets. MNIST contains 5.5 × 104 training samples
and 104 test samples, and CIFAR10 has 5 × 104 training samples and 104 test samples. We do not
use any data pre-processing nor data-augmentation in all of our experiments.
Network architectures. For MNIST, we use a plain CNN architecture with 13 layers, denoted as
CNN13 (see Table 3 in the appendix for more details about this architecture). For CIFAR10 we use
VGG11, VGG13, VGG16 (Simonyan & Zisserman, 2015) and DenseNet121 (Huang et al., 2017). As
the VGG models were originally proposed for ImageNet and have very large fully connected layers,
we adapted these layers for CIFAR10 by reducing their width from 4096 to 128. For each given
network, we create the corresponding skip-networks by adding skip-connections to the output so that
our condition M ≥ N from the main theorem is satisfied. In particular, we aggregate all neurons of
all the hidden layers in a pool and randomly choose from there a subset of N neurons to be connected
to the output layer (see e.g. Figure 2 for an illustration). As existing network architectures have a
large number of feature maps per layer, the total number of neurons is often very large compared to
number of training samples, thus it is easy to choose from there a subset of N neurons to connect
to the output. In the following, we test both sigmoid and softplus activation function (γ = 20) for
each network architecture and their skip-variants. We use the standard cross-entropy loss and train
all models with SGD+Nesterov momentum for 300 epochs. The initial learning rate is set to 0.1 for
Densenet121 and 0.01 for the other architectures. Following Huang et al. (2017), we also divide the
learning rate by 10 after 50% and 75% of the total number of training epochs. Note that we do not
use any explicit regularization like weight decay or dropout.
The main goal of our experiments is to investigate the influence of the additional skip-connections
to the output layer on the generalization performance. We report the test accuracy for the original
models and the ones with skip-connections to the output layer. For the latter one we have two different
algorithms: standard SGD for training the full network as described above (SGD) and the randomized
procedure (rand). The latter one uses a slight variant of the simple algorithm described at the end of
the last section: randomly initialize the weights of the network U up to the output layer by drawing
each of them from a truncated Gaussian distribution with zero mean and variance d where d is the
number of weight parameters and the truncation is done after ±2 standard deviations (standard keras
initialization), then use SGD to optimize the weights V for a linear classifier with fixed features Ψ(U)
which is a convex optimization problem.
Our experimental results are summarized in Table 1 for MNIST and Table 2 for CIFAR10. For
skip-models, we report mean and standard deviation over 8 random choices of the subset of N
neurons connected to the output.
Discussion of results. First of all, we note that adding skip connections to the output improves the
test accuracy in almost all networks (with the exception of Densenet121) when the full network is
7
Published as a conference paper at ICLR 2019
	Sigmoid activation function	Softplus activation function
CNN13 CNN13-skip (SGD)	11.35 98.40 ± 0.07	99.20 99.14 ± 0.04
Table 1: Test accuracy (%) of CNN13 on MNIST dataset. CNN13 denotes the original architecture
from Table 3 while CNN13-skip denotes the corresponding skip-model. There are in total 179, 840
hidden neurons from the original CNN13 (see Table 3), out of which we choose a random subset of
N = 55, 000 neurons to connect to the output layer to obtain CNN13-skip.
	Sigmoid activation function		Softplus activation function	
Model	Test acc (%)	Train acc (%)	Test acc (%)	Train acc (%)
VGG11	10	10	78.92	100
VGG11-skip (rand)	62.81 ± 0.39	100	64.49 ± 0.38	100
VGG11-skip (SGD)	72.51 ± 0.35	100	80.57 ± 0.40	100
VGG13	10	10	80.84	100
VGG13-skip (rand)	61.50 ± 0.34	100	61.42 ± 0.40	100
VGG13-skip (SGD)	70.24 ± 0.39	100	81.94 ± 0.40	100
VGG16	10	10	81.33	100
VGG16-skip (rand)	61.57 ± 0.41	100	61.46 ± 0.34	100
VGG16-skip (SGD)	70.61 ± 0.36	100	81.91 ± 0.24	100
Densenet121	86.41	100	89.31	100
Densenet121-SkiP (rand)	52.07 ± 0.48	100	55.39 ± 0.48	100
Densenet121-skip (SGD)	81.47 ± 1.03	100	86.76 ± 0.49	100
Table 2: Traning and test accuracy of several CNN architectures with/without skip-connections on
CIFAR10 (no data-augmentation). For each original model A, A-skip denotes the corresponding
skip-model in which a subset of N hidden neurons “randomly selected” from the hidden layers are
connected to the output units. For Densenet121, these neurons are randomly chosen from the first
dense block. The names in open brackets (rand/SGD) specify how the networks are trained: rand
(U is randomized and fixed while V is learned with SGD), SGD (both U and V are optimized with
SGD). Additional experimental results with data-augmentation are shown in Table 5 in the appendix.
trained with SGD. In particular, for the sigmoid activation function the skip connections allow for
all models except Densenet121 to get reasonable performance whereas training the original model
fails. This effect can be directly related to our result of Theorem 3.4 that the loss landscape of
skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training
error (see Section F in the appendix for more detailed discussions on this issue, as well as Section
E for a visual example which shows why the skip-models can succeed while the original models
fail). The exception is Densenet121 which gets already good performance for the sigmoid activation
function for the original model. We think that the reason is that the original Densenet121 architecture
has already quite a lot of skip-connections between the hidden layers which thus improves the loss
surface already so that the additional connections added to the output units are not necessary anymore.
The second interesting observation is that we do not see any sign of overfitting for the SGD version
even though we have increased for all models the number of parameters by adding skip connections
to the output layer and we know from Theorem 3.4 that for all the skip-models one can easily
achieve zero training error. This is in line with the recent observation of Zhang et al. (2017) that
modern heavily over-parameterized networks can fit everything (random labels, random input) but
nevertheless generalize well on the original training data when trained with SGD. This is currently an
active research area to show that SGD has some implicit bias (Neyshabur et al., 2017; Brutzkus et al.,
2018; Soudry et al., 2018) which leads to a kind of regularization effect similar to the linear least
squares problem where SGD converges to the minimum norm solution. Our results confirm that there
is an implicit bias as we see a strong contrast to the (skip-rand) results obtained by using the network
8
Published as a conference paper at ICLR 2019
as a random feature generator and just fitting the connections to the output units (i.e. V ) which also
leads to solutions with zero training error with probability 1 as shown in Lemma 3.2 and the proof of
Theorem 3.4. For this version we see that the test accuracy gets worse as one is moving from simpler
networks (VGG11) to more complex ones (VGG16 and Densenet121) which is a sign of overfitting.
Thus we think that our class of networks is also an interesting test bed to understand the implicit
regularization effect of SGD. It seems that SGD selects from the infinite pool of solutions with zero
training error one which generalizes well, whereas the randomized feature generator selects one with
much worse generalization performance.
5	Related work
In the literature, many interesting theoretical results have been developed on the loss surface of neural
networks Yu & Chen (1995); Haeffele & Vidal (2017); Choromanska et al. (2015); Kawaguchi (2016);
Safran & Shamir (2016); Hardt & Ma (2017); Yun et al. (2017); Lu & Kawaguchi (2017); Venturi
et al. (2018); Liang et al. (2018b); Zhang et al. (2018); Nouiehed & Razaviyayn (2018). The behavior
of SGD for the minimization of training objective has been also analyzed for various settings (Andoni
et al., 2014; Sedghi & Anandkumar, 2015; Janzamin et al., 2016; Gautier et al., 2016; Brutzkus &
Globerson, 2017; Soltanolkotabi, 2017; Soudry & Hoffer, 2017; Zhong et al., 2017; Tian, 2017; Du
et al., 2018; Wang et al., 2018) to name a few. Most of current results are however limited to shallow
networks (one hidden layer), deep linear networks and/or making simplifying assumptions on the
architecture or the distribution of training data. An interesting recent exception is Liang et al. (2018a)
where they show that for binary classification one neuron with a skip-connection to the output layer
and exponential activation function is enough to eliminate all bad local minima under mild conditions
on the loss function. More closely related in terms of the setting are (Nguyen & Hein, 2017; 2018)
where they study the loss surface of fully connected and convolutional networks if one of the layers
has more neurons than the number of training samples for the standard multi-class problem. However,
the presented results are stronger as we show that our networks do not have any suboptimal local
valley or strict local minima and there is less over-parameterization if the number of classes is small.
6	Conclusion
We have identified a class of deep neural networks whose loss landscape has no bad local valleys.
While our networks are over-parameterized and can easily achieve zero training error, they generalize
well in practice when trained with SGD. Interestingly, a simple different algorithm using the network
as random feature generator also achieves zero training error but has significantly worse generalization
performance. Thus we think that our class of models is an interesting test bed for studying the implicit
regularization effect of SGD.
References
A. Andoni, R. Panigrahy, G. Valiant, and L. Zhang. Learning polynomials with neural networks.
ICML, 2014.
T. M. Apostol. Mathematical analysis. Addison Wesley, 1974.
P. Auer, M. Herbster, and M. K. Warmuth. Exponentially many local minima for single neurons.
NIPS, 1996.
A. Blum and R. L Rivest. Training a 3-node neural network is np-complete. NIPS, 1989.
A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.
ICML, 2017.
A. Brutzkus, A. Globerson, E. Malach, and S. Shalev-Shwartz. Sgd learns over-parameterized
networks that provably generalize on linearly separable data. ICLR, 2018.
A. Choromanska, M. Hena, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer
networks. AISTATS, 2015.
9
Published as a conference paper at ICLR 2019
Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking
the saddle point problem in high-dimensional non-convex optimization. NIPS, 2014.
S. Du, J. Lee, Y. Tian, A. Singh, and B. P6czos. Gradient descent learns one-hidden-layer cnn: Don't
be afraid of spurious local minima. ICML, 2018.
A. Gautier, Q. Nguyen, and M. Hein. Globally optimal training of generalized polynomial neural
networks with nonlinear spectral methods. NIPS, 2016.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
ICML, 2010.
I. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network optimization
problems. ICLR, 2015.
B. D. Haeffele and R. Vidal. Global optimality in neural network training. CVPR, 2017.
M. Hardt and T. Ma. Identity matters in deep learning. ICLR, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CVPR, 2016.
G.	Huang, Z. Liu, L. Maaten, and K. Weinberger. Densely connected convolutional networks. CVPR,
2017.
M. Janzamin, H. Sedghi, and A. Anandkumar. Beating the perils of non-convexity: Guaranteed
training of neural networks using tensor methods. arXiv:1506.08473, 2016.
K. Kawaguchi. Deep learning without poor local minima. NIPS, 2016.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. NIPS, 2012.
Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel.
Handwritten digit recognition with a back-propagation network. NIPS, 1990.
H.	Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein. Visualizing the loss landscape of neural nets. In
ICLR Workshop, 2018.
S. Liang, R. Sun, J. D. Lee, and R. Srikant. Adding one neuron can eliminate all bad local minima.
arXiv:1805.08671, 2018a.
S. Liang, R. Sun, Y. Li, and R. Srikant. Understanding the loss surface of neural networks for binary
classification. In ICML, 2018b.
R. Livni, S. Shalev-Shwartz, and O. Shamir. On the computational efficiency of training neural
networks. NIPS, 2014.
H. Lu and K. Kawaguchi. Depth creates no bad local minima. arXiv:1702.08580, 2017.
B. Mityagin. The zero set of a real analytic function. arXiv:1512.07276, 2015.
B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in deep
learning. NIPS, 2017.
Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. ICML, 2017.
Q. Nguyen and M. Hein. Optimization landscape and expressivity of deep cnns. ICML, 2018.
V. D. Nguyen. Complex powers of analytic functions and meromorphic renormalization in qft.
arXiv:1503.00995, 2015.
M. Nouiehed and M. Razaviyayn. Learning deep models: Critical points and local openness. ICLR
Workshop, 2018.
I. Safran and O. Shamir. On the quality of the initial basin in overspecified networks. ICML, 2016.
10
Published as a conference paper at ICLR 2019
I.	Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks.
ICML, 2018.
H. Sedghi and A. Anandkumar. Provable methods for training neural networks with sparse connectiv-
ity. ICLR Workshop, 2015.
S. Shalev-Shwartz, O. Shamir, and S. Shammah. Failures of gradient-based deep learning. ICML,
2017.
J.	Sima. Training a single sigmoidal neuron is hard. Neural Computation, 14:2709-2728, 2002.
K.	Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
ICLR, 2015.
M. Soltanolkotabi. Learning relus via gradient descent. NIPS, 2017.
D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural
networks. ICLR Workshop 2018, 2017.
D. Soudry, E. Hoffer, M. S. Nacson, and N. Srebro. The implicit bias of gradient descent on separable
data. ICLR, 2018.
Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications
in convergence and critical point analysis. ICML, 2017.
L. Venturi, A. S. Bandeira, and J. Bruna. Spurious valleys in two-layer neural network optimization
landscapes. arXiv:1802.06384, 2018.
G. Wang, G. B. Giannakis, and J. Chen. Learning relu networks on linearly separable data: Algorithm,
optimality, and generalization. arXiv:1808.04685, 2018.
X. Yu and G. Chen. On the local minima free condition of backpropagation learning. IEEE
Transaction on Neural Networks, 6:1300-1303, 1995.
C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. ICLR, 2017.
S. Zagoruyko and N. Komodakis. Wide residual networks. BMCV, 2016.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and Oriol Vinyals. Understanding deep learning requires
re-thinking generalization. ICLR, 2017.
H. Zhang, J. Shao, and R. Salakhutdinov. Deep neural networks with multi-branch architectures are
less non-convex. arXiv:1806.01845, 2018.
K. Zhong, Z. Song, P. Jain, P. Bartlett, and I. Dhillon. Recovery guarantees for one-hidden-layer
neural networks. ICML, 2017.
A	Mathematical Tools
In the proof of Lemma 3.2 we make use of the following property of analytic functions.
Lemma A.1 (Nguyen, 2015; Mityagin, 2015) If f : Rn → R is a real analytic function which is not
identically zero then the set {x ∈ Rn | f (x) = 0} has Lebesgue measure zero.
We recall the following standard result from topology (see e.g. Apostol (1974), Theorem 4.23, p. 82),
which is used in the proof of Theorem 3.4.
Proposition A.2 Let f : Rm → Rn be a continuous function. If U ⊆ Rn is an open set then f-1 (U)
is also open.
11
Published as a conference paper at ICLR 2019
B	Proof of Lemma 3.2
Proof: We assume w.l.o.g. that {p1, . . . ,pN} is a subset of the neurons with skip connections to the
output layer and satisfy Assumption 3.1. In the following, we will show that there exists a weight
configuration U such that the submatrix Wi：n,i：N has full rank. Using then that the determinant is an
analytic function together with Lemma A.1, we will conclude that the set of weight configurations U
such that Ψ has not full rank has Lebesgue measure zero.
We remind that all the hidden units in the network are indexed from the first hidden layer till the
higher layers as d + 1, . . . , d + H. For every hidden neuron j ∈ [d + 1, d + H], uj denotes the
associated weight vector
uj = [uk→j]k∈in(j) ∈ R|in(j)| , where in(j) = the set of incoming units to unit j.
Let n1 be the number of units of the first hidden layer. For every neuron j from the first hidden layer,
let us define the pre-activation output gj ,
gj(xi) =	(xi)kuk→j.
k→j
Due to Assumptions 3.1 (condition 3), we can always choose the weights {ud+1, . . . , ud+n1 } so
that the output of every neuron in the first layer is distinct for different training samples, that is
gj(xi) 6= gj (xi0) for every j ∈ [d + 1, d + n1] and i 6= i0. For every neuron j ∈ [d + n1 + 1, d + H]
in the higher layers we choose the weight vector uj such that it has exactly one 1 and 0 elsewhere.
According to our definition of network in Section 2, the weight vectors of neurons of the same layer
need not have the same dimension, but any subgroup of these neurons can still have shared weights
as long as the dimensions among them agree. Thus the above choice of u is always possible. In the
following, let c(j) denote the neuron below j such that uc(j)→j = 1. This leads to
fk (x)uk→j = fc(j) (x).
k→j
Let α := (αd+1, . . . , αd+H) be a tuple of positive scalars. Let β ∈ R such that σpj (β) 6= 0 for every
j ∈ [N]. We consider a family of configurations of network parameters of the form (αjuj, bj)jd=+dH+1,
where the biases are chosen as
bpj = β - αpj gpj (xj) ∀j ∈ [N],pj ∈ [d + 1, d + n1]
bpj = β - αpj fc(pj) (xj) ∀j ∈ [N],pj ∈/ [d + 1, d+ n1]
bj = 0 ∀j ∈ {d + 1, . . . , d + H} \ {p1 , . . . , pN}
Note that the assignment of biases can be done via a forward pass through the network. By the above
choice of biases and our definition of neurons in Section 2, we have
fPj (Xi)=	σPj(β + αPj (gpj (Xi)-	gpj (Xj))) ,	∀j ∈	[N],Pj	∈	[d	+1,d	+ n1],
fPj (Xi)= σPj(β + αPj (fc(pj )(Xi)- fc(pj)(Xj 力)∀j ∈ [N ],Pj ∈ [d +1,d + n1],
fj(Xi) = σj αjfc(j) (Xi)	∀j ∈ {d+ n1 + 1, . . . , d+ H} \ {p1, . . . ,pN} ,
fj(Xi) = σj αjgj(Xi)	∀j ∈ {d+ 1, . . . , d+ n1} \ {p1, . . . ,pN} .	(5)
One notes that the output of every skip-connection neuron pj is given by the first equation if pj lies
on the first layer and by the second equation if pj lies on higher layers. In the following, to reduce
notational complexity we make a convention that: fc(pj ) = gpj for every pj lies on the first layer.
This allows us to use the second equation for every skip-connection neuron, that is,
fPj (Xi) = σPj (β + αPj (fc(Pj )(Xi)- fc(Pj )(Xj))) ∀j ∈ [N ].
(6)
Now, since α > 0 and all activation functions are strictly increasing by Assumption 3.1, one can
easily show from the above recursive definitions that if pj is a skip-connection neuron which does
12
Published as a conference paper at ICLR 2019
not lie on the first hidden layer then one has the relation: fc(pj) (xi) < fc(pj) (xj) if and only if
gqj (xi) < gqj (xj), where qj is some neuron in the first hidden layer. This means if one sorts the
elements of the set fc(pj) (x1), . . . , fc(pj) (xN) in increasing order then for every positive tuple α,
the order is fully determined by the corresponding order of gqj (x1), . . . , gqj (xN) for some neuron
qj in the first layer. Note that this order can be different for different neurons qj in the first layer, and
thus can be different for different skip-connection neurons pj . Let π be a permutation such that it
holds for every j = 1, 2, . . . , N that
π(j) =	arg max	fc(pj)(xi)	(7)
i∈{1,...,N }∖{π(1),...,π(j-1)}
It follows from above that π is fully determined by the values of g at the first layer. By definition one
has fc(pj) (xπi) < fc(pj) (xπj) for every i > j. Since π is independent of every positive tuple α and
fully determined by the values of g, it can be fixed in the beginning. One can assume w.l.o.g. that π
is the identity permutation as otherwise one can reorder the training samples according to π so that
the rank of Ψ does not change. Thus it holds for every α > 0 that
δij :=fc(pj)(xi)-fc(pj)(xj) <0 ∀i,j∈ [N],i>j	(8)
Now, we are ready to show that there exists a positive tuple α for which Ψ has full rank. We consider
two cases of the activation functions of skip-connection neurons as stated in Assumption 3.1:
• In the first case, the activation functions σpj : R → R for every j ∈ [N] are strictly
increasing, bounded and limt→-∞ σpj (t) = 0. In the following, let l(j) denote the layer
index of the hidden unit j. For every hidden unit j ∈ {d + 1, . . . , d + H} we set αj to be
the maximum of certain bounds (explained later in (10)) associated to all skip-connection
neurons pk lying on the same layer, that is,
)
αj = max 1,
max	maxi>k
k∈[N ]|l(Pk)=Ij)
σ-1 (E) - β
fc(pk) (xi) - fc(pk) (xk)
(9)
where E > 0 is an arbitrarily small constant which will be specified later. There are a
few remarks we want to make for Eq. (9) before proceeding with our proof. First, the
second term in (9) can be empty if there is no skip-connection unit pk which lies on the
same layer as unit j, in which case αj is simply set to 1. Second, αj ’s are well-defined by
constructing the values fc(pk) (xr), r = 1, . . . , N by a forward pass through the network
(note that the network is a directed, acyclic graph; in particular, in the formula of αj , one
has l(c(pk)) < l(pk) = l(j) and thus the computation of αj is feasible given the values
of hidden units lying below the layer of unit j, namely fc(pk)). Third, ifj and j0 are two
neurons from the same layer, i.e. l(j) = l(j0), then it follows from (9) that αj = αj0,
meaning that their corresponding weight vectors are scaled by the same factor, thus any
potential weight sharing conditions imposed on these neurons can still be satisfied.
The main idea of choosing the above values of α is to obtain
Ψij = fpj (xi) ≤ E ∀i,j ∈ [N],i > j.	(10)
To see this, one first observes that the inequality (8) holds for the constructed values of α
since they are all positive. From (9) it holds for every skip-connection unit pj that
σp-1(E) - β
αpj > max -——-ɪ~-——∀—ʌ ∀ j ∈ [N]
i>j fc(pj) (xi) - fc(pj) (xj)
which combined with (8) leads to
αpj (fc(pj) (xi) - fc(pj)(xj)) ≤ σp-j1(E) - β ∀i,j ∈ [N],i > j.
and thus using (6) we obtain (10).
Coming back to the main proof of the lemma, since σpj (j ∈ [N]) are bounded there exists a
finite positive constant C such that it holds that
∣Ψij | ≤ C ∀ i,j ∈ [N]	(11)
13
Published as a conference paper at ICLR 2019
By the Leibniz-formula one has
NN
det(Ψ±N,i:N) = Y σpj(β)+	X	sign(π) Y Ψπ(j)j	(12)
j=1	∏∈Sn ∖{γ}	j=1
where SN is the set of all N! permutations of the set {1, . . . , N} and γ is the identity
permutation. Now, one observes that for every permutation π 6= γ, there always exists at
least one component j where π(j) > j in which case it follows from (10) and (11) that
N
X
sign(π) Y Ψπ(j)j ≤ N ! CN-1
∏∈Sn ∖{γ}	j=1
QjN=1 σpj (β)
By choosing E = ∣ 2nn7 ∣, We get that
N	1N	1N
det(ΨrN,i:N) ≥ Y cp, (β) - 2 Y Cp, (β) = 2 Y Cp, (β) = 0
and thus Ψ has full rank.
• In the second case We consider the softplus activation function Which satisfies our Assump-
tion 3.1 that there exists a backWard path from every skip-connection neuron pj to the first
hidden layer s.t. on this path there is no neuron Which has skip-connections to the output or
shared Weights With other skip-connection neurons.
We choose all the Weights and biases similarly to the first case. The only difference is that
for every skip-connection neuron pj(1 ≤ j ≤ N), the position of 1 in its Weight vector upj
is chosen s.t. the value of neuron pj is determined by the first neuron on the corresponding
backWard path as stated in Assumption 3.1, that is,
fk (xi)uk→pj = fc(pj) (xi ).
k→pj
For skip-connection neurons We set all {αp1 , . . . , αpN} to some scalar variable α, and for
non-skip connection neurons j We set αj = 1. From (6) and equations of (5) We have
fPj (Xi)= σPj (β + α(fc(pj) (Xi)-fc(pj)(xj)	∀j∈ [N],
fj(Xi) = σj fc(j)(Xi)	∀j ∈ {d+ 1, . . . ,d+ H} \ {p1, . . . ,pN} .	(13)
Note that With above construction of u and α, the only case Where our Weight sharing
conditions can be potentially violated is betWeen a skip-connection neuron (αj = α) With a
neuron on a backWard path (αj = 1). HoWever, this is not possible because our assumption
in this case states that there is no Weight sharing betWeen a skip-connection neuron and a
neuron on one of the backWard paths.
Next, by our assumption the recursive backWard path c(k)(pj) does not contain any skip-
connection unit and thus Will eventually end up at some neuron qj ∈ [d + 1, d + n1] in the
first hidden layer after some finite number of steps. Thus We can Write for every j ∈ [N]
fPj (Xi) = σpj (β + αfc(pj )(Xi)- fc(pj )(xj ))),
Where
fc(pj)(Xi) = σc(pj)(σc(c(pj))(. . . (gqj(Xi)) . . .))	∀i ∈ [N].
Moreover, We have from (8) that fc(pj)(Xi) < fc(pj)(Xj) for every i > j. Note that softplus
fulfills for t < 0, σγ(t) ≤ 1 eγt, whereas for t > 0 one has σγ(t) ≤ ɪ + t. The latter
property implies σ(K)(t) ≤ K +1. Finally, this together implies that there exist positive
constants c1, c2, c3, c4 such that it hods
N
| Y Ψ∏(j)jl ≤ cie-αc2 (c3 + α)N-1.
j=1
14
Published as a conference paper at ICLR 2019
This can be made arbitrarily small by increasing α. Thus we get
N
lim det(ΨrNj:N) = Y σpj. (β) = 0
α→∞	j
j=1
So far, we have shown that there always exist U such that Ψ has full rank. Since every activation
function is real analytic by Assumption 3.1, every entry of Ψ is also a real analytic function of the
network parameters where Ψ depends on. The set of low rank matrices Ψ can be characterized
by a system of equations such that all the MN determinants of all N × N sub-matrices of Ψ are
zero. As the determinant is a polynomial in the entries of the matrix and thus an analytic function
of the entries and composition of analytic functions are again analytic, we conclude that each
determinant is an analytic function of U . As shown above, there exists at least one U such that
one of these determinant functions is not identically zero and thus by Lemma A.1, the set of U
where this determinant is zero has measure zero. But as all submatrices need to have low rank
in order that Ψ has low rank, it follows that the set of U where Ψ has low rank has just measure zero.
C Extension of Theorem 3.4 to general convex losses
In this section, we consider a more general training objective, defined as
Φ(U,V )=g(G(U,V))
where G(U, V) = Ψ(U)V ∈ RN×m is the output of the network for all training samples at some
given parameters (U, V), and 夕：RN×m → R the loss function applied on the network output.
Assumption C.1 The loss function 夕：RN×m → R is convex and boundedfrom below.
One can easily check that the following loss functions satisfy Assumption C.1 as they are all convex
and bounded from below by zero:
1.	The cross-entropy loss from Equation 2, in particular:
NG
1	eGiyi
以G) = N X -log (Pm=I Qk),
where (xi, yi)iN=1 is the training data with yi being the ground-truth class of xi.
2.	The standard square loss (for classification/regression tasks)
ψ(G) = 2 kG - Y kF ,	(14)
where Y ∈ RN ×m is the ground-truth matrix.
3.	The multi-class Hinge-loss (for classification tasks)
1N
2(G) = N EmaXj=yi max(0, 1 - (Giyi - Gij)),
i=1
where (xi, yi)iN=1 is the training data with yi being the ground-truth class of xi.
By Assumption C.1,夕 is bounded from below, thus it attains a finite infimum:
p* := inf	2(G) < ∞.
G∈RN ×m
Basically, p* serves as a lower bound on our training objective Φ. For the above examples, it holds
p* = 0. Next, we adapt the definition of “bad local valleys” from Definition 3.3 to the current setting.
15
Published as a conference paper at ICLR 2019
Definition C.2 The α-sublevel set of Φ is defined as Lα = {(U, V ) | Φ(U, V ) < α} . A local valley
is defined as a connected component of some sublevel set Lα. A bad local valley is a local valley on
which the training objective Φ cannot be made arbitrarily close to p*.
The following result extends Theorem 3.4 to general convex losses. The proofs are mostly similar as
before, but we present them below for the completeness and convenience of the reader.
Theorem C.3 The following holds under Assumption 3.1 and Assumption C.1:
1.	There exist uncountably many solutions with zero training error.
2.	The loss landscape of Φ does not have any bad local valley.
3.	There exists no suboptimal strict local minimum.
4.	For cross-entropy loss (2) and square loss (14) there exists no local maximum.
Proof:
1.	By Lemma 3.2 the set of U such that Ψ(U) has not full rank N has Lebesgue measure zero.
Given U such that Ψ has full rank, the linear system Ψ(U)V = Y has for every possible
target output matrix Y ∈ RN ×m at least one solution V . As this is possible for almost all
U , there exist uncountably many solutions achieving zero training error.
2.	Let C be a non-empty, connected component of some sub-level set La where a > p*.
Note that La = 0 if α ≤ p* by Definition C.2. Given any E ∈ (p*, a), We will show that
C always contains a point (U, V ) s.t. Φ(U, V ) ≤ as this would imply that the loss Φ
restricted to C can always attain arbitrarily small value close to p*.
We note that La = Φ-1((-∞, α)) is an open set according to Proposition A.2. Since C is a
non-empty connected component of La, C must also be an open set with non-zero Lebesgue
measure. By Lemma 3.2 the set of U where Ψ(U) has not full rank has measure zero and
thus C must contain a point (U, V) such that Ψ(U) has full rank. By Assumption C.1,夕
attains its infimum at p* < e, and thus by continuity of 夕，there exists G* ∈ RN×m such that
p* ≤ 夕(G*) ≤ e. As Ψ(U) has full rank, there always exist V * such that Ψ(U)V * = G*.
Now, one notes that the loss Φ(U, V)=夕(Ψ(U)V) is convex in V, and that Φ(U, V) < a,
thus we have for the line segment V(λ) = λV + (1 - λ)V* for λ ∈ [0, 1],
Φ(U, V (λ)) ≤ λΦ(U, V) + (1 - λ)Φ(U,V*) < λα+ (1 - λ)E < α.
Thus the whole line segment from (U, V) to (U, V*) is contained in La. Since C is a
connected component of La which contains (U, V), it follows that (U, V*) ∈ C. Moreover,
one has Φ(U, V*)=夕(G*) ≤ e, which thus implies that the loss can always be made
E-small inside the set C for every E ∈ (p*, α).
3.	Let (U0, V0) be a strict suboptimal local minimum, then there exists r > 0 such that
Φ(U,V) > Φ(U0,V0) >p* for all (U, V) ∈ B((U0,V0),r) ∖{(U0,V0)} where B(∙,r)
denotes a closed ball of radius r. Let a = min。⑦CaB((U V)T) Φ(U V) which exists as
Φ is continuous and the boundary ∂B (U0, V0), r of B (U0, V0), r is compact. Note that
Φ(U0, V0) < α as (U0, V0) is a strict local minimum, and thus (U0, V0) ∈ La. Let E be the
connected component of La which contains (U0, V0), that is, (U0, V0) ∈ E ⊆ La. Since
the loss of every point inside E is strictly smaller than α, whereas the loss of every point
on the boundary ∂B (U0, V0), r is greater than or equal to α, E must be contained in the
interior of the ball, that is E ⊂ B((U0, Vo), r). Moreover, Φ(U, V) ≥ Φ(U0, VO) > p* for
every (U, V) ∈ E and thus the values of Φ restricted to E can not be arbitrarily close to p*,
which means that E is a bad local valley, which contradicts G.3.2.
4.	The proof for cross-entropy loss is similar to Theorem 3.4. For square loss, one has
Φ(U,V ) = 2 kΨ(U )V - Y kF = 1 k(Im 氧 Ψ(U)) VeC (V) — vec (Y )k2
where 0 denotes Kronecker product, and Im an m X m identity matrix. The hessian of Φ
w.r.t. V is V2ec(V)Φ = (Im 0 Ψ(U))T(Im 0 Ψ(U)).
16
Published as a conference paper at ICLR 2019
Suppose by contradiction that (U, V ) is a local maximum. Then the Hessian of Φ is negative
semi-definite. As principal submatrices of negative semi-definite matrices are again negative
semi-definite, the Hessian of Φ w.r.t V must be also negative semi-definite. However, Φ is
convex in V thus its Hessian restricted to V must be positive semi-definite. The only matrix
which is both p.s.d. and n.s.d. is the zero matrix. It follows that V2ec(V)Φ(U, V)=0 and
thus Ψ(U) = 0. By Assumption 3.1, there exists j ∈ [M] s.t. σpj is strictly positive, thus
some entries of Ψ(U) must be strictly positive, and so Ψ(U) cannot be identically zero,
leading to a contradiction. Therefore Φ has no local maximum.
D The architecture of CNN13 from Table 1: See Table 3
Layer	Output size	#neurons
Input: 28 × 28	28 × 28 × 1	
3 × 3 ConV — 64, stride 1	28 × 28 × 64	50176
3 × 3 ConV — 64, stride 1	28 × 28 × 64	50176
3 × 3 conv — 64, stride 2	14 × 14 × 64	12544
3 × 3 conv 一 128, stride 1	14 × 14 × 128	25088
3 × 3 conv 一 128, stride 1	14 × 14 × 128	25088
3 × 3 conv 一 128, stride 2	7 × 7 × 128	6272
3 × 3 conv 一 256, stride 1	~7 × 7 × 256~	12544
1 × 1 conv 一 256, stride 1	~7 × 7 × 256~	12544
3 × 3 conv 一 256, stride 2	~4 × 4 × 256~	4096
3 × 3 conv 一 256, stride 1	~4 × 4 × 256~	4096
3 × 3 conv 一 256, stride 2	~2 × 2 × 256~	1024
3 × 3 conv 一 256, stride 1	~2 × 2 × 256~	1024
3 × 3 conv 一 256, stride 2	-1 × 1 × 256~	256
Fully connected, 10 output units		
Table 3: The architecture of CNN13 for MNIST dataset. There are in total 179, 840 hidden neurons.
E Visualization of the loss landscape before and after adding
skip-connections to the output layer
Similar to Li et al. (2018); Goodfellow et al. (2015), we visualize the loss surface restricted to a two
dimensional subspace of the parameter space. The subspace is chosen to go through some point
(U0, V0) learned by SGD and spanned by two random directions (U1, V1) and (U2, V2).
For the purpose of illustration, we train with SGD a two-hidden-layer fully connected network with
784 and 300 hidden units respectively, followed by a 10-way softmax. The training set consists of
1024 images, which are randomly selected from MNIST dataset. After adding skip-connections to
the output, the network fulfills M = N = 1024. Figure 3 shows the heat map of the loss surface
before and after adding skip-connections. One can see a visible effect that skip-connections have
helped to smooth the loss landscape near a small sub-optimal region and allows gradient descent to
flow directly from there to the bottom of the landscape with smaller objective value.
F Discussion of training error in Tab le 2
Training error for the experiment in Table 2. As shown in Table 2, the training error is zero in
all cases, except when the original VGG models are used with sigmoid activation function. The
reason, as noticed in our experiments, is because the learning of these sigmoidal networks converges
quickly to a constant zero classifier (i.e. the output of the last hidden layer converges to zero), which
makes both training and test accuracy converge to 10% and the loss in Equation (2) converges to
17
Published as a conference paper at ICLR 2019
(a) Sigmoid, no skip
-吗-近、
S 5 101520 -201-11
―5>20
(-5 06,
(b) Sigmoid, with skip
Figure 3: Loss surface of a two-hidden-layer network on a small MNIST dataset.
6
4
2
0
-2
-4
-6
-8
9
6
3
0
-3 W
—6 S
S
—9 S
-3
(a) Softplus, no skip
(b) Softplus, with skip
- log(1/10). While we are not aware of a theoretical explanation for this behavior, it is not restricted
to the specific architecture of VGGs but hold in general for plain sigmoidal networks with depth>5 as
pointed out earlier by Glorot & Bengio (2010). As shown in Table 2, Densenets however do not suffer
from this phenomenon, probably because they already have skip-connections between all the hidden
layers of a dense block, thus gradients can easily flow from the output to every layer of a dense block,
which makes the training of this network with sigmoid activation function become feasible.
Discussion of convergence speed. For sigmoid activation, we noticed that skip-models when
trained with the random sampling approach (skip-rand) often converge much slower than when
trained with full SGD (skip-SGD). In our experiments, to be sure that one gets absolute zero training
error, we set the number of training epochs to 5000 for the former case and 1000 for the later. Perhaps
a better learning rate schedule might help to reduce this number, or maybe not, but this is beyond the
scope of this paper. For SoftPlus activation, We noticed a much faster convergence - all models often
converge within 300 epochs to absolute zero training error.
Skip-connections are also helpful for training very deep networks with softplus activation.
Previously We have shoWn that skip-connections are helpful for training deep sigmoidal netWorks. In
this part, We shoW a similar result for softplus activation function. For the purpose of illustration, We
create a small dataset With N = 1000 training images randomly chosen from CIFAR10 dataset. We
use a very deep netWork With 150 fully connected layers, each of Width 10, and softplus activation. A
skip-model is created by adding skip-connections from N randomly chosen neurons to the output
units. We train both netWorks With SGD. The best learning rate for each model is empirically chosen
from 10-2, 10-3, 10-4, 10-5 . We report the training loss and training error of both models in
Figure 5. One can see that the skip-netWork easily converge to zero training error Within 200 epochs,
Whereas the original netWork has stronger fluctuations and fails to converge after 1000 epochs. This
18
Published as a conference paper at ICLR 2019
0.6
0.5
(a) Softplus, no skip	(b) Softplus, with skip
Figure 5: Training progress of a 150-layer neural network with and without skip-connections.
is directly related to our result of Theorem 3.4 in the sense that skip-connections can help to smooth
the loss landscape and enable effective training of very deep networks.
G	Additional experiments: Max-pooling of VGGs are replaced by
2x2 convolutional layers of stride 2
The original VGG Simonyan & Zisserman (2015) and Densenet Huang et al. (2017) contain pooling
layers in their architecture. In particular, original VGGs have max-pooling layers, and original
Densenets have averaging pooling layers. In the following, we will clarify how/if these pooling
layers have been used in our experiments in Table 2, and whether and how our theretical results are
appicable to this case, as well as presenting additional experimental results in this regard.
First of all, we note that Densenets Huang et al. (2017) contain pooling layers only after the first dense
block. Meanwhile, as noted in Table 2, our experiments with Densenets only use skip-connections
from hidden units of the first dense block, and thus Lemma 3.2 and Theorem 3.4 are applicable. The
reason is that one can restrict the full-rank analysis of matrix Ψ in Lemma 3.2 to the hidden units of
the first dense block, so that it follows that the set of parameters of the first dense block where Ψ has
not full rank has Lebesgue measure zero, from which our results of Theorem 3.4 follow immediately.
However for VGGs in Table 2, we kept their max-pooling layers similar to the original architecture
as we wanted to have a fair comparison between our skip-models and the original models. In this
setting, our results are not directly applicable because we lose the analytic property of the entries of
Ψ w.r.t. its dependent parameters, which is crucial to prove Lemma 3.2. Therefore in this section, we
would like to present additional results to Table 2 in which we replace all max-pooling layers of all
VGG models from Table 2 with 2x2 convolutional layers of stride 2. In this case, the whole network
consists of only convolutional and fully connected layers, hence our theoretical results are applicable.
The experimental results are presented in Table 4. Overall, our main observations are similar as
before. The performance gap between original models and their corresponding skip-variants are
approximately the same as in Table 2 or slightly more pronounced in some cases. A one-to-one
comparison with Table 2 also shows that the performance of skip-models themselves have decreased
by 4 - 7% after the replacement of max-pooling layers with 2x2 convolutional layers. This is perhaps
19
Published as a conference paper at ICLR 2019
	Sigmoid activation function		Softplus activation function	
Model	C-10	C-10+	C-10	C-10+
VGGn-mp2conv	10	10	74.53	88.80
VGGn-mp2conv-skip (rand)	53.65 ± 0.66	-	55.51 ± 0.43	-
VGGn-mp2conv-skip (SGD)	64.45 ± 0.31	80.15 ± 0.59	76.18 ± 0.58	89.93 ± 0.19
VGG13-mp2conv	10	10	74.04	90.37
VGG13-mp2conv-skip (rand)	53.45 ± 0.23	-	53.33 ± 0.67	-
VGG13-mp2conv-skip (SGD)	63.53 ± 0.37	82.40 ± 0.23	75.58 ± 0.77	91.04 ± 0.20
VGG16-mp2conv	10	10	74.00	90.38
VGG16-mp2conv-skip (rand)	53.83 ± 0.30	-	55.34 ± 0.64	-
VGG16-mp2conv-skip (SGD)	65.77 ± 0.67	83.06 ± 0.34	76.52 ± 0.78	91.00 ± 0.23
Table 4: Test accuracy (%) of VGG networks from Table 2 where max-pooling layers are replaced by
2x2 convolutional layers of stride 2 (denoted as mp2conv). Other notations are similar to Table 2.
not so surprising because the problem gets potentially harder when the network has more layers to be
learned, especially in case of sigmoid activation where the decrease is sharper. Similar to Table 2,
adding skip-connections to the output units still prove to be very helpful - it improves the result for
softplus while making the training of deep networks with sigmoid activation become possible at all.
Finally, the training of full network with SGD still yields significantly better solutions in terms of
generalization error than the random feature approach. This confirms once again the implicit bias of
SGD towards high quality solutions among infinitely many solutions with zero training error.
H	Data-augmentation results for Table 2
The following Table 5 shows additional results to Table 2 where data-augmentation is used now. For
data-augmentation, we follow the procedure as described in (Zagoruyko & Komodakis, 2016) by
considering random crops of size 32 × 32 after 4 pixel padding on each side of the training images
and random horizontal flips with probability 0.5. For the convenience of the reader, we also repeat
the results of Table 2 in the new table 5.
20
Published as a conference paper at ICLR 2019
	Sigmoid activation function		Softplus activation function	
Model	-	C-10	C-10+	C-10	C-10+
VGGn	10	10	78.92	88.62
VGGn-SkiP (rand)	62.81 ± 0.39	-	64.49 ± 0.38	-
VGGn-SkiP(SGD)	72.51 ± 0.35	85.55 ± 0.09	80.57 ± 0.40	89.32 ± 0.16
VGG13	10	10	80.84	90.58
VGG13-skip (rand)	61.50 ± 0.34	-	61.42 ± 0.40	-
VGG13-skip (SGD)	70.24 ± 0.39	86.48 ± 0.32	81.94 ± 0.40	91.06 ± 0.12
VGG16	10	10	81.33	90.68
VGG16-skip (rand)	61.57 ± 0.41	-	61.46 ± 0.34	-
VGG16-skip (SGD)	70.61 ± 0.36	86.42 ± 0.31	81.91 ± 0.24	91.00 ± 0.22
DenSenet121	86.41	90.93	89.31	94.20
DenSenet121-SkiP (rand)	52.07 ± 0.48	-	55.39 ± 0.48	-
DenSenet121-SkiP (SGD)	81.47 ± 1.03	90.32 ± 0.50	86.76 ± 0.49	93.23 ± 0.42
Table 5: Test accuracy (%) of several CNN architectures with/without skip-connections on CIFAR10
(+ denotes data augmentation). For each model A, A-skip denotes the corresponding skip-model in
which a subset of N hidden neurons “randomly selected” from the hidden layers are connected to the
output units. For Densenet121, these neurons are randomly chosen from the first dense block. The
names in open brackets (rand/SGD) specify how the networks are trained: rand (U is randomized
and fixed while V is learned with SGD), SGD (both U and V are optimized with SGD).
21