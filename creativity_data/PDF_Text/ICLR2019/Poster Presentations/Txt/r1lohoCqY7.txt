Published as a conference paper at ICLR 2019
Learning-Based Frequency Estimation Algorithms
Chen-Yu Hsu, Piotr Indyk, Dina Katabi & Ali Vakilian
Computer Science and Artificial Intelligence Lab
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{cyhsu,indyk,dk,vakilian}@mit.edu
Ab stract
Estimating the frequencies of elements in a data stream is a fundamental task in
data analysis and machine learning. The problem is typically addressed using
streaming algorithms which can process very large data using limited storage.
Today’s streaming algorithms, however, cannot exploit patterns in their input to
improve performance. We propose a new class of algorithms that automatically
learn relevant patterns in the input data and use them to improve its frequency
estimates. The proposed algorithms combine the benefits of machine learning
with the formal guarantees available through algorithm theory. We prove that our
learning-based algorithms have lower estimation errors than their non-learning
counterparts. We also evaluate our algorithms on two real-world datasets and
demonstrate empirically their performance gains.
1.	Introduction
Classical algorithms provide formal guarantees over their performance, but often fail to leverage
useful patterns in their input data to improve their output. On the other hand, deep learning models
are highly successful at capturing and utilizing complex data patterns, but often lack formal error
bounds. The last few years have witnessed a growing effort to bridge this gap and introduce al-
gorithms that can adapt to data properties while delivering worst case guarantees. Deep learning
modules have been integrated into the design of Bloom filters (Kraska et al., 2018; Mitzenmacher,
2018), caching algorithms (Lykouris & Vassilvitskii, 2018), graph optimization (Dai et al., 2017),
similarity search (Salakhutdinov & Hinton, 2009; Weiss et al., 2009) and compressive sensing (Bora
et al., 2017). This paper makes a significant step toward this vision by introducing frequency esti-
mation streaming algorithms that automatically learn to leverage the properties of the input data.
Estimating the frequencies of elements in a data stream is one of the most fundamental subroutines
in data analysis. It has applications in many areas of machine learning, including feature selec-
tion (Aghazadeh et al., 2018), ranking (Dzogang et al., 2015), semi-supervised learning (Talukdar &
Cohen, 2014) and natural language processing (Goyal et al., 2012). It has been also used for network
measurements (Estan & Varghese, 2003; Yu et al., 2013; Liu et al., 2016) and security (Schechter
et al., 2010). Frequency estimation algorithms have been implemented in popular data processing
libraries, such as Algebird at Twitter (Boykin et al., 2016). They can answer practical questions
like: what are the most searched words on the Internet? or how much traffic is sent between any two
machines in a network?
The frequency estimation problem is formalized as follows: given a sequence S of elements from
some universe U, for any element i ∈ U, estimate fi, the number of times i occurs in S. If one
could store all arrivals from the stream S, one could sort the elements and compute their frequencies.
However, in big data applications, the stream is too large (and may be infinite) and cannot be stored.
This challenge has motivated the development of streaming algorithms, which read the elements
of S in a single pass and compute a good estimate of the frequencies using a limited amount of
space.1 Over the last two decades, many such streaming algorithms have been developed, including
1 Specifically, the goal of the problem is as follows. Given a sequence S of elements from U, the desired
algorithm reads S in a single pass while writing into memory C (whose size can be much smaller than the
length of S). Then, given any element i ∈ U, the algorithm reports an estimation of fi based only on the
content of C.
1
Published as a conference paper at ICLR 2019
Count-Sketch (Charikar et al., 2002), Count-Min (Cormode & Muthukrishnan, 2005b) and multi-
stage filters (Estan & Varghese, 2003). The performance guarantees of these algorithms are well-
understood, with upper and lower bounds matching UP to O(∙) factors (JoWhari et al., 2011).
However, such streaming algorithms typically assume generic data and do not leverage useful pat-
terns or properties of their input. For example, in text data, the word frequency is known to be
inversely correlated with the length of the word. Analogously, in network data, certain applications
tend to generate more traffic than others. If such properties can be harnessed, one could design
frequency estimation algorithms that are much more efficient than the existing ones. Yet, it is im-
portant to do so in a general framework that can harness various useful properties, instead of using
handcrafted methods specific to a particular pattern or structure (e.g., word length, application type).
In this paper, we introduce learning-based frequency estimation streaming algorithms. Our algo-
rithms are equipped with a learning model that enables them to exploit data properties without being
specific to a particular pattern or knowing the useful property a priori. We further provide theoretical
analysis of the guarantees associated with such learning-based algorithms.
We focus on the important class of “hashing-based” algorithms, which includes some of the most
used algorithms such as Count-Min, Count-Median and Count-Sketch. Informally, these algorithms
hash data items into B buckets, count the number of items hashed into each bucket, and use the
bucket value as an estimate of item frequency. The process can be repeated using multiple hash
functions to improve accuracy. Hashing-based algorithms have several useful properties. In partic-
ular, they can handle item deletions, which are implemented by decrementing the respective coun-
ters. Furthermore, some of them (notably Count-Min) never underestimate the true frequencies, i.e.,
fi ≥ fi holds always. However, hashing algorithms lead to estimation errors due to collisions: when
two elements are mapped to the same bucket, they affect each others’ estimates. Although collisions
are unavoidable given the space constraints, the overall error significantly depends on the pattern
of collisions. For example, collisions between high-frequency elements (“heavy hitters”) result in
a large estimation error, and ideally should be minimized. The existing algorithms, however, use
random hash functions, which means that collisions are controlled only probabilistically.
Our idea is to use a small subset ofS, call it S0, to learn the heavy hitters. We can then assign heavy
hitters their own buckets to avoid the more costly collisions. It is important to emphasize that we
are learning the properties that identify heavy hitters as opposed to the identities of the heavy hitters
themselves. For example, in the word frequency case, shorter words tend to be more popular. The
subset S0 itself may miss many of the popular words, but whichever words popular in S0 are likely
to be short. Our objective is not to learn the identity of high frequency words using S0 . Rather, we
hope that a learning model trained on S0 learns that short words are more frequent, so that it can
identify popular words even if they did not appear in S0 .
Our main contributions are as follows:
•	We introduce learning-based frequency estimation streaming algorithms, which learn the
properties of heavy hitters in their input and exploit this information to reduce errors
•	We provide performance guarantees showing that our algorithms can deliver a logarithmic
factor improvement in the error bound over their non-learning counterparts. Furthermore,
we show that our learning-based instantiation of Count-Min, a widely used algorithm, is
asymptotically optimal among all instantiations of that algorithm. See Table 4.1 in sec-
tion 4.1 for the details.
•	We evaluate our learning-based algorithms using two real-world datasets: traffic load on
an Internet backbone link and search query popularity. In comparison to their non-learning
counterparts, our algorithms yield performance gains that range from 18% to 71%.
2.	Related Work
Frequency estimation in data streams. Frequency estimation, and the closely related problem of
finding frequent elements in a data stream, are some of the most fundamental and well-studied prob-
lems in streaming algorithms, see Cormode & Hadjieleftheriou (2008) for an overview. Hashing-
based algorithms such as Count-Sketch (Charikar et al., 2002), Count-Min (Cormode & Muthukr-
ishnan, 2005b) and multi-stage filters (Estan & Varghese, 2003) are widely used solutions for these
problems. These algorithms also have close connections to sparse recovery and compressed sens-
2
Published as a conference paper at ICLR 2019
ing (Candes et al., 2006; Donoho, 2006), where the hashing output can be considered as a Com-
pressed representation of the input data (Gilbert & Indyk, 2010).
Several “non-hashing” algorithms for frequency estimation have been also proposed (Misra & Gries,
1982; Demaine et al., 2002; Karp et al., 2003; Metwally et al., 2005). These algorithms do not pos-
sess many of the properties of hashing-based methods listed in the introduction (such as the ability
to handle deletions), but they often have better accuracy/space tradeoffs. For a fair comparison, our
evaluation focuses only on hashing algorithms. However, our approach for learning heavy hitters
should be useful for non-hashing algorithms as well.
Some papers have proposed or analyzed frequency estimation algorithms customized to data that
follows Zipf Law (Charikar et al., 2002; Cormode & Muthukrishnan, 2005a; Metwally et al., 2005;
Minton & Price, 2014; Roy et al., 2016); the last algorithm is somewhat similar to the “lookup
table” implementation of the heavy hitter oracle that we use as a baseline in our experiments. Those
algorithms need to know the data distribution a priori, and apply only to one distribution. In contrast,
our learning-based approach applies to any data property or distribution, and does not need to know
that property or distribution a priori.
Learning-based algorithms. Recently, researchers have begun exploring the idea of integrating
machine learning models into algorithm design. In particular, researchers have proposed improving
compressed sensing algorithms, either by using neural networks to improve sparse recovery algo-
rithms (Mousavi et al., 2017; Bora et al., 2017), or by designing linear measurements that are opti-
mized for a particular class of vectors (Baldassarre et al., 2016; Mousavi et al., 2015), or both. The
latter methods can be viewed as solving a problem similar to ours, as our goal is to design “measure-
ments” of the frequency vector (f1, f2 . . . , f|U|) tailored to a particular class of vectors. However,
the aforementioned methods need to explicitly represent a matrix of size B × |U|, where B is the
number of buckets. Hence, they are unsuitable for streaming algorithms which, by definition, have
space limitations much smaller than the input size.
Another class of problems that benefited from machine learning is distance estimation, i.e., com-
pression of high-dimensional vectors into compact representations from which one can estimate
distances between the original vectors. Early solutions to this problem, such as Locality-Sensitive
Hashing, have been designed for worst case vectors. Over the last decade, numerous methods for
learning such representations have been developed (Salakhutdinov & Hinton, 2009; Weiss et al.,
2009; Jegou et al., 2011; Wang et al., 2016). Although the objective of those papers is similar to
ours, their techniques are not usable in our applications, as they involve a different set of tools and
solve different problems.
More broadly, there have been several recent papers that leverage machine learning to design more
efficient algorithms. The authors of (Dai et al., 2017) show how to use reinforcement learning and
graph embedding to design algorithms for graph optimization (e.g., TSP). Other learning-augmented
combinatorial optimization problems are studied in (He et al., 2014; Balcan et al., 2018; Lykouris &
Vassilvitskii, 2018). More recently, (Kraska et al., 2018; Mitzenmacher, 2018) have used machine
learning to improve indexing data structures, including Bloom filters that (probabilistically) answer
queries of the form “is a given element in the data set?” As in those papers, our algorithms use neural
networks to learn certain properties of the input. However, we differ from those papers both in our
design and theoretical analysis. Our algorithms are designed to reduce collisions between heavy
items, as such collisions greatly increase errors. In contrast, in existence indices, all collisions count
equally. This also leads to our theoretical analysis being very different from that in (Mitzenmacher,
2018).
3.	Preliminaries
3.1.	Estimation Error
We will use ei := |fi - fi| to denote the estimation error for fi. To measure the overall estimation
error between the frequencies F = {f1,f2, ∙∙∙ ,f∣u∣} and their estimates F = {fι, f2,…，f∣u∣},
we will use the expected error Ei〜D 归i], where D models the distribution over the queries to the
data structure. Similar to past work (Roy et al., 2016), we assume the query distribution D is the
same as the distribution of the input stream, i.e., for any j We have Pri〜D[i = j] = fj/N, where N
3
Published as a conference paper at ICLR 2019
is the sum of all frequencies. This leads to the estimation error of F with respect to F:
Err(F, F) := 1/N Efi- fi∣∙ fi	(3.1)
i∈U
We note that the theoretical guarantees of frequency estimation algorithms are typically phrased in
the “亿 δ)-form”, e.g., Pr[∣fi - fi| > EN] < δ for every i (see e.g., Cormode & Muthukrishnan
(2005b)). However, this formulation involves two objectives ( and δ). We believe that the (single
objective) expected error in Equation 3.1 is more natural from the machine learning perspective.
3.2.	Algorithms for Frequency Es timation
In this section, we recap three variants of hashing-based algorithms for frequency estimation.
Single Hash Function. The basic hashing algorithm uses a single uniformly random hash function
h : U → [B], where we use [B] to denote the set {1 . . . B}. The algorithm maintains a one-
dimensional array C [1 . . . B], in which all entries are initialized to 0. Given an element i, the algo-
rithm increments C[h(i)]. It can be seen that at the end of the stream, we have C[b] = Pj:h(j)=b fj.
The estimate f of f is defined as f = C[h(i)] = Pj：h(i)=h(j)fj. Note that it is always the case
- τ „
that fi ≥ fi.
Count-Min. We have k distinct hash functions hi : U → [B] and an array C of size k × B. The
algorithm maintains C, such that at the end of the stream We have C[', b] = Pj%j)=b fj. For each
i ∈ U, the frequency estimate fi is equal to ming≤k C[', h`(i)], and always satisfies fi ≥ f
Count-Sketch. Similarly to Count-Min, we have k distinct hash functions hi : U → [B] and an
array C of size k × B. Additionally, in Count-Sketch, we have k sign functions gi : U → {-1, 1},
and the algorithm maintains C such that C[', b] = Pj%j)=b fj ∙ g`(j). For each i ∈ U, the
frequency estimate fi is equal to the median of {g'(i) ∙ C[', h'(i)]}'≤k. Note that unlike the previous
two methods, here we may have fi < fi .
3.3.	Zipfian Distribution
In our theoretical analysis we assume that the item frequencies follow the Zipf Law. That is, if we
re-order the items so that their frequencies appear in a sorted order fi1 ≥ fi2 ≥ . . . ≥ fin, then
fij a 1/j. To simplify the notation we assume that fi = 1/i.
4.	Learning-Based Frequency Estimation Algorithms
We aim to develop frequency estimation algorithms that exploit data properties for better perfor-
mance. To do so, we learn an oracle that identifies heavy hitters, and use the oracle to assign each
heavy hitter its unique bucket to avoid collisions. Other items are simply hashed using any classic
frequency estimation algorithm (e.g., Count-Min, or Count-Sketch), as shown in the block-diagram
in Figure 4.1. This design has two useful properties: First, it allows us to augment a classic fre-
quency estimation algorithm with learning capabilities, producing a learning-based counterpart that
inherits the original guarantees of the classic algorithm. For example, if the classic algorithm is
Count-Min, the resulting learning-based algorithm never underestimates the frequencies. Second, it
provably reduces the estimation errors, and for the case of Count-Min it is (asymptotically) optimal.
Algorithm 1 provides pseudo code for our design. The design assumes an oracle HH(i) that attempts
to determine whether an item i is a “heavy hitter” or not. All items classified as heavy hitters are
assigned to one of the Br unique buckets reserved for heavy items. All other items are fed to the
remaining B - Br buckets using a conventional frequency estimation algorithm SketchAlg (e.g.,
Count-Min or Count-Sketch).
The estimation procedure is analogous. To compute fi, the algorithm first checks whether i is stored
in a unique bucket, and if so, reports its count. Otherwise, it queries the SketchAlg procedure. Note
that if the element is stored in a unique bucket, its reported count is exact, i.e., fi = fi .
The oracle is constructed using machine learning and trained with a small subset of S, call it S0 .
Note that the oracle learns the properties that identify heavy hitters as opposed to the identities of
the heavy hitters themselves. For example, in the case of word frequency, the oracle would learn that
4
Published as a conference paper at ICLR 2019
shorter words are more frequent, so that it can identify popular words even if they did not appear in
the training set S0 .
4.1.	Analysis
Our algorithms combine simplicity with strong error bounds. Below, we summarize our theoret-
ical results, and leave all theorems, lemmas, and proofs to the appendix. In particular, Table 4.1
lists the results proven in this paper, where each row refers to a specific streaming algorithm, its
corresponding error bound, and the theorem/lemma that proves the bound.
First, we show (Theorem 9.11 and Theorem 9.14) that if the heavy hitter oracle is accurate, then the
error of the learned variant of Count-Min is up to a logarithmic factor smaller than that of its non-
learning counterpart. The improvement is maximized when B is of the same order as n (a common
scenario2). Furthermore, we prove that this result continues to hold even if the learned oracle makes
prediction errors with probability δ, as long as δ = O(1/ lnn) (Lemma 9.15).
Second, we show that, asymptotically, our learned Count-Min algorithm cannot be improved any
further by designing a better hashing scheme. Specifically, for the case of Learned Count-Min with
a perfect oracle, our design achieves the same asymptotic error as the “Ideal Count-Min”, which
optimizes its hash function for the given input (Theorem 10.4).
Finally, we note that the learning-augmented algorithm inherits any (, δ)-guarantees of the original
version. Specifically, its error is not larger than that of SketchAlg with space B - Br, for any input.
Algorithms	Expected Error	Analysis
Single Hash Function (B)	Θ(ln2 n/B)	Lemma 9.2
Count-Min Sketch (k, B)	k+2 Ω(Bnn) and O( klnnl『(喑))	Theorem9.11
Learned Count-Min(B)	“m%(ln2(n/B)/B)	Theorem9.14
Ideal Count-Min(B)	θ(ln2 (n/B)/b)	Theorem 10.4
Table 4.1: Our performance bounds for different algorithms on streams with frequencies obeying
Zipf Law. k is a constant (≥ 2) that refers to the number of hash functions, B is the number of
buckets, and n is the number of distinct elements. The space complexity of all algorithms is the
same, Θ(B). See section 9.4 for non-asymptotic versions of the some of the above bounds
5.	Experiments
Baselines. We compare our learning-based algorithms with their non-learning counterparts. Specif-
ically, we augment Count-Min with a learned oracle using Algorithm 1, and call the learning-
augmented algorithm “Learned Count-Min”. We then compare Learned Count-Min with traditional
Count-Min. We also compare it with “Learned Count-Min with Ideal Oracle” where the neural-
network oracle is replaced with an ideal oracle that knows the identities of the heavy hitters in the test
data, and “Table Lookup with Count-Min” where the heavy hitter oracle is replaced with a lookup
table that memorizes heavy hitters in the training set. The comparison with the latter baseline allows
2For example, Goyal et al. (2012) uses B = 20M, n = 33.5M and B = 50M, n = 94M.
Algorithm 1 Learning-Based Frequency Estimation
1:	procedure LEARNEDSKETCH(B, Br, HH, SketchAlg)
2:	for each stream element i do
3:	if HH(i) = 1 then
4:	if i is already stored in a unique bucket then
5:	increment the count of i
6:	else create a new unique bucket for i and
7:	initialize the count to 1
8:	end if
9:	else
10:	feed i to SketchAlg with B - Br buckets
11:	end if
12:	end for
13:	end procedure
Figure 4.1: Pseudo-code and block-diagram representation of our algorithms
Element i
Learned
Oracle
Heav^^^	^^Not heavy
Unique
Buckets
SketchAlg
(e.g., Count-Min)
5
Published as a conference paper at ICLR 2019
us to show the ability of Learned Count-Min to generalize and detect heavy items unseen in the
training set. We repeat the evaluation where we replace Count-Min (CM) with Count-Sketch (CS)
and the corresponding variants. We use validation data to select the best k for all algorithms.
Training a Heavy Hitter Oracle. We construct the heavy hitter oracle by training a neural network
to predict the heaviness ofan item. Note that the prediction of the network is not the final estimation.
Itis used in Algorithm 1 to decide whether to assign an item to a unique bucket. We train the network
to predict the item counts (or the log of the counts) and minimize the squared loss of the prediction.
Empirically, we found that when the counts of heavy items are few orders of magnitude larger than
the average counts (as is the case for the Internet traffic data set), predicting the log of the counts
leads to more stable training and better results. Once the model is trained, we select the optimal
cutoff threshold using validation data, and use the model as the oracle described in Algorithm 1.
5.1.	Internet Traffic Estimation
For our first experiment, the goal is to estimate the number of packets for each network flow. A flow
is a sequence of packets between two machines on the Internet. It is identified by the IP addresses
of its source and destination and the application ports. Estimating the size of each flow i - i.e., the
number of its packets f -isa basic task in network management (Sivaraman et al., 2017).
Dataset: The traffic data is collected at a backbone link of a Tier1 ISP
between Chicago and Seattle in 2016 (CAIDA). Each recording session is
around one hour. Within each minute, there are around 30 million packets
and 1 million unique flows. For a recording session, we use the first 7
minutes for training, the following minute for validation, and estimate the
packet counts in subsequent minutes. The distribution of packet counts
over Internet flows is heavy tailed, as shown in Figure 5.1.
Model: The patterns of the Internet traffic are very dynamic, i.e., the
flows with heavy traffic change frequently from one minute to the next.
However, we hypothesize that the space ofIP addresses should be smooth
Sorted items (log scale)
Figure 5.1: Frequency of
Internet Flows
in terms of traffic load. For example, data centers at large companies and university campuses
with many students tend to generate heavy traffic. Thus, though the individual flows from these
sites change frequently, we could still discover regions of IP addresses with heavy traffic through a
learning approach.
We trained a neural network to predict the log of the packet counts for each flow. The model takes
as input the IP addresses and ports in each packet. We use two RNNs to encode the source and
destination IP addresses separately. The RNN takes one bit of the IP address at each step, starting
from the most significant bit. We use the final states of the RNN as the feature vector for an IP
address. The reason to use RNN is that the patterns in the bits are hierarchical, i.e., the more
significant bits govern larger regions in the IP space. Additionally, we use two-layer fully-connected
networks to encode the source and destination ports. We then concatenate the encoded IP vectors,
encoded port vectors, and the protocol type as the final features to predict the packet counts 3. The
inference time takes 2.8 microseconds per item on a single GPU without any optimizations4.
Results: We plot the results of two representative test minutes (the 20th and 50th) in Figure 5.2. All
plots in the figure refer to the estimation error (Equation 3.1) as a function of the used space. The
space includes space for storing the buckets and the model. Since we use the same model for all test
minutes, the model space is amortized over the 50-minute testing period.
Figure 5.2 reveals multiple findings. First, the figure shows that our learning-based algorithms
exhibit a better performance than their non-learning counterparts. Specifically, Learned Count-Min,
compared to Count-Min, reduces the the error by 32% with space of 0.5 MB and 42% with space of
1.0 MB (Figure 5.2a). Learned Count-Sketch, compared to Count-Sketch, reduces the error by 52%
3We use RNNs with 64 hidden units. The two-layer fully-connected networks for the ports have 16 and 8
hidden units. The final layer before the prediction has 32 hidden units.
4 Note that new specialized hardware such as Google TPU, hardware accelerators and network compres-
sion (Han et al., 2017; Sze et al., 2017; Chen et al., 2017; Han et al., 2016; 2015) can drastically improve the
inference time. Further, Nvidia has predicted that GPU will get 1000x faster by 2025. Because of these trends,
the overhead of neural network inference is expected to be less significant in the future (Kraska et al., 2018).
6
Published as a conference paper at ICLR 2019
(a) Learned Count-Min - 20th test minute
= 200
O
Φ 175
n
b
2 150
ɪɪ
∈ 125
Φ
∣1∞
O 75
—Count Min
Table lookup CM
----Learned CM (NNet)
,√— Learned CM (Ideal)
Space (MB)
σj
≥25
i 8.0
(b) Learned Count-Min - 50th test minute
(c) Learned Count-Sketch - 20th test minute (d) Learned Count-Sketch - 50th test minute
Figure 5.2: Comparison of our algorithms with Count-Min and Count-Sketch on Internet traffic data.
at 0.5 MB and 57% at 1.0 MB (Figure 5.2c). In our experiments, each regular bucket takes 4 bytes.
For the learned versions, we account for the extra space needed for the unique buckets to store the
item IDs and the counts. One unique bucket takes 8 bytes, twice the space of a normal bucket.5
Second, the figure also shows that our neural-network oracle performs better than memorizing the
heavy hitters in a lookup table. This is likely due to the dynamic nature of Internet traffic -i.e.,
the heavy flows in the training set are significantly different from those in the test data. Hence,
memorization does not work well. On the other hand, our model is able to extract structures in the
input that generalize to unseen test data.
Third, the figure shows that our model’s performance stays roughly the same from the 20th to the
50th minute (Figure 5.2b and Figure 5.2d), showing that it learns properties of the heavy items that
generalize over time.
Lastly, although we achieve significant improvement over Count-Min and Count-Sketch, our scheme
can potentially achieve even better results with an ideal oracle, as shown by the dashed green line in
Figure 5.2. This indicates potential gains from further optimizing the neural network model.
5.2.	Search Query Estimation
For our second experiment, the goal is to estimate the number of times a
search query appears.
Dataset: We use the AOL query log dataset, which consists of 21 million
search queries collected from 650 thousand users over 90 days. The users
are anonymized in the dataset. There are 3.8 million unique queries. Each
query is a search phrase with multiple words (e.g., “periodic table element
poster”). We use the first 5 days for training, the following day for valida-
tion, and estimate the number of times different search queries appear in
subsequent days. The distribution of search query frequency follows the
Zipfian law, as shown in Figure 5.3
Sorted items (log scale)
Figure 5.3: Frequency of
search queries
5By using hashing with open addressing, it suffices to store IDs hashed into log Br + t bits (instead of whole
IDs) to ensure there is no collision with probability 1 - 2-t. log Br + t is comparable to the number of bits
per counter, so the space for a unique bucket is twice the space of a normal bucket.
7
Published as a conference paper at ICLR 2019
(a) Learned Count-Min
50th test day
(b) Learned Count-Min
80th test day
(c) Learned Count-Sketch
50th test day
(d) Learned Count-Sketch
80th test day
Figure 5.4: Comparison of our algorithms with Count-Min and Count-Sketch on search query data.
Model: Unlike traffic data, popular search queries tend to appear more consistently across multiple
days. For example, “google” is the most popular search phrase in most of the days in the dataset.
Simply storing the most popular words can easily construct a reasonable heavy hitter predictor.
However, beyond remembering the popular words, other factors also contribute to the popularity of
a search phrase that we can learn. For example, popular search phrases appearing in slightly different
forms may be related to similar topics. Though not included in the AOL dataset, in general, metadata
of a search query (e.g., the location of the search) can provide useful context of its popularity.
To construct the heavy hitter oracle, we trained a neural network to predict the number of times
a search phrase appears. To process the search phrase, we train an RNN with LSTM cells that
takes characters of a search phrase as input. The final states encoded by the RNN are fed to a
fully-connected layer to predict the query frequency. Our character vocabulary includes lower-case
English alphabets, numbers, punctuation marks, and a token for unknown characters. We map the
character IDs to embedding vectors before feeding them to the RNN6. We choose RNN due to its
effectiveness in processing sequence data (Sutskever et al., 2014; Graves, 2013; Kraska et al., 2018).
Results: We plot the estimation error vs. space for two representative test days (the 50th and 80th
day) in Figure 5.4. As before, the space includes both the bucket space and the space used by the
model. The model space is amortized over the test days since the same model is used for all days.
Similarly, our learned sketches outperforms their conventional counterparts. For Learned Count-
Min, compared to Count-Min, it reduces the loss by 18% at 0.5 MB and 52% at 1.0 MB (Figure 5.4a).
For Learned Count-Sketch, compared to Count-Sketch, it reduces the loss by 24% at 0.5 MB and
71% at 1.0 MB (Figure 5.4c). Further, our algorithm performs similarly for the 50th and the 80th
day (Figure 5.4b and Figure 5.4d), showing that the properties it learns generalize over a long period.
The figures also show an interesting difference from the Internet traffic data: memorizing the heavy
hitters in a lookup table is quite effective in the low space region. This is likely because the search
queries are less dynamic compared to Internet traffic (i.e., top queries in the training set are also
popular on later days). However, as the algorithm is allowed more space, memorization becomes
ineffective.
5.3.	Analyzing Heavy Hitter Models
We analyze the accuracy of the neural network
heavy hitter models to better understand the re-
sults on the two datasets. Specifically, we use
the models to predict whether an item is a heavy
hitter (top 1% in counts) or not, and plot the
ROC curves in Figure 5.5. The figures show
that the model for the Internet traffic data has
learned to predict heavy items more effectively,
with an AUC score of 0.9. As for the model for
search query data, the AUC score is 0.8. This
also explains why we see larger improvements
over non-learning algorithms in Figure 5.2.
(a) Internet traffic
50th test minute
Figure 5.5: ROC curves of the learned models.
(b) Search query
50th test day
6We use an embedding size of 64 dimensions, an RNN with 256 hidden units, and a fully-connected layer
with 32 hidden units.
8
Published as a conference paper at ICLR 2019
6.	Embedding Space Visualization
In this section, we visualize the embedding spaces learned by our heavy hitter models to shed light
on the properties or structures the models learned. Specifically, we take the neural network acti-
vations before the final fully-connected layer, and visualize them in a 2-dimensional space using
t-SNE (Maaten & Hinton, 2008). To illustrate the differences between heavy hitters (top 1% in
counts) and the rest (“light” items), we randomly sample an equal amount of examples from both
classes. We visualize the embedding space for both the Internet traffic and search query datasets.
We show the embedding space learned by the model on the Internet traffic data in Figure 6.1. Each
point in the scatter plot represents one Internet traffic flow. By coloring each flow with its number
of packets in Figure 6.1a, we see that the model separate flows with more packets (green and yellow
clusters) from flows with fewer packets (blue clusters). To understand what structure the model
learns to separate these flows, we color each flow with its destination IP address in Figure 6.1b.
We found that clusters with more packets are often formed by flows sharing similar destination
address prefixes. Interestingly, the model learns to group flows with similar IP prefixes closer in
the embedding space. For example, the dark blue cluster at the upper left of Figure 6.1b shares
a destination IP address prefix “1.96.*.*”. Learning this “structure” from the Internet traffic data
allows the model to generalize to packets unseen in the training set.
8 6 4
Log Of NUmberof PaCketS
(a) Colored by log of number of packets	(b) Colored by the top 8 bits of destination IP
Figure 6.1: Visualization of the embedding space learned by our model on the Internet traffic data.
Each point in the scatter plot represents one network flow.
We show the embedding space learned by the model on the search query data in Figure 6.2. Each
point in the scatter plot represents one search query. Similarly, the model learns to separate frequent
search queries from the rest in Figure 6.2a. By coloring the queries with the number of characters
in Figure 6.2b, we have multiple interesting findings. First, queries with similar length are closer in
the embedding space, and the y-axis forms the dimension representing query length. Second, if we
simply use the query length to predict heavy hitters, many light queries will be misclassified. The
model must have learned other structures to separate heavy hitters in Figure 6.2a.
(a) Colored by log of search query frequency
(b) Colored by query length
Figure 6.2: Visualization of the embedding space learned by our model on the search query data.
Each point in the scatter plot represents one search query.
9
Published as a conference paper at ICLR 2019
7.	Conclusion
We have presented a new approach for designing frequency estimation streaming algorithms by
augmenting them with a learning model that exploits data properties. We have demonstrated the
benefits of our design both analytically and empirically. We envision that our work will motivate a
deeper integration of learning in algorithm design, leading to more efficient algorithms.
8.	Acknowledgements
The authors would like to thank the anonymous reviewers for helpful comments. The work was
partially supported by NSF TRIPODS award #1740751, NSF AITF award #1535851 and Simons
Investigator Award. We also thank the various companies sponsoring the MIT Center for Wireless
Networks and Mobile Computing.
References
Amirali Aghazadeh, Ryan Spring, Daniel LeJeune, Gautam Dasarathy, Anshumali Shrivastava, and
Richard G Baraniuk. Mission: Ultra large-scale feature selection using count-sketches. In Inter-
national Conference on Machine Learning, 2018.
Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch. In
International Conference on Machine Learning, 2018.
LUca Baldassarre, Yen-HUan Li, Jonathan Scarlett, Baran G0zcu, Ilija Bogunovic, and Volkan
Cevher. Learning-based compressive subsampling. IEEE Journal of Selected Topics in Signal
Processing,10(4):809-822, 2016.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing Using genera-
tive models. In International Conference on Machine Learning, pp. 537-546, 2017.
Oscar Boykin, Avi Bryant, Edwin Chen, ellchow, Mike Gagnon, Moses NakamUra, Steven No-
ble, Sam Ritchie, AshUtosh Singhal, and Argyris Zymnis. Algebird. https://twitter.
github.io/algebird/, 2016.
CAIDA. Caida internet traces 2016 chicago. http://www.caida.org/data/monitors/passive-eqUinix-
chicago.xml.
Emmanuel J Candes, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact signal
reconstrUction from highly incomplete freqUency information. IEEE Transactions on information
theory, 52(2):489-509, 2006.
Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams.
In International Colloquium on Automata, Languages, and Programming, pp. 693-703. Springer,
2002.
Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. Eyeriss: An energy-efficient re-
configurable accelerator for deep convolutional neural networks. IEEE Journal of Solid-State
Circuits, 52(1):127-138, 2017.
Graham Cormode and Marios Hadjieleftheriou. Finding frequent items in data streams. Proceedings
of the VLDB Endowment, 1(2):1530-1541, 2008.
Graham Cormode and S Muthukrishnan. Summarizing and mining skewed data streams. In Pro-
ceedings of the 2005 SIAM International Conference on Data Mining, pp. 44-55. SIAM, 2005a.
Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the count-min
sketch and its applications. Journal of Algorithms, 55(1):58-75, 2005b.
Hanjun Dai, Elias Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial op-
timization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351-6361, 2017.
Erik D Demaine, Alejandro Lopez-Ortiz, and J Ian Munro. Frequency estimation of internet packet
streams with limited space. In European Symposium on Algorithms, pp. 348-360. Springer, 2002.
10
Published as a conference paper at ICLR 2019
David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-
1306, 2006.
Fabon Dzogang, Thomas Lansdall-Welfare, Saatviga Sudhahar, and Nello Cristianini. Scalable
preference learning from data streams. In Proceedings of the 24th International Conference on
World Wide Web, pp. 885-890. ACM, 2015.
Cristian Estan and George Varghese. New directions in traffic measurement and accounting: Focus-
ing on the elephants, ignoring the mice. ACM Transactions on Computer Systems (TOCS), 21(3):
270-313, 2003.
Anna Gilbert and Piotr Indyk. Sparse recovery using sparse matrices. Proceedings of the IEEE, 98
(6):937-947, 2010.
Amit Goyal, Hal Daume III, and Graham Cormode. Sketch algorithms for estimating point queries
in nlp. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Language Learning, pp. 1093-1103. Association
for Computational Linguistics, 2012.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850, 2013.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J
Dally. Eie: efficient inference engine on compressed deep neural network. In Computer Archi-
tecture (ISCA), 2016 ACM/IEEE 43rd Annual International Symposium on, pp. 243-254. IEEE,
2016.
Song Han, Junlong Kang, Huizi Mao, Yiming Hu, Xin Li, Yubin Li, Dongliang Xie, Hong Luo,
Song Yao, Yu Wang, et al. Ese: Efficient speech recognition engine with sparse lstm on fpga.
In Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate
Arrays, pp. 75-84. ACM, 2017.
He He, Hal Daume III, and Jason M Eisner. Learning to search in branch and bound algorithms. In
Advances in neural information processing systems, pp. 3293-3301, 2014.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor
search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128, 2011.
Hossein Jowhari, Mert Saglam, and Gabor Tardos. Tight bounds for lp samplers, finding duplicates
in streams, and related problems. In Proceedings of the thirtieth ACM SIGMOD-SIGACT-SIGART
symposium on Principles of database systems, pp. 49-58. ACM, 2011.
Richard M Karp, Scott Shenker, and Christos H Papadimitriou. A simple algorithm for finding
frequent elements in streams and bags. ACM Transactions on Database Systems (TODS), 28(1):
51-55, 2003.
Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned
index structures. In Proceedings of the 2018 International Conference on Management of Data
(SIGMOD), pp. 489-504. ACM, 2018.
Abhishek Kumar, Minho Sung, Jun Jim Xu, and Jia Wang. Data streaming algorithms for efficient
and accurate estimation of flow size distribution. In ACM SIGMETRICS Performance Evaluation
Review, volume 32, pp. 177-188. ACM, 2004.
Zaoxing Liu, Antonis Manousis, Gregory Vorsanger, Vyas Sekar, and Vladimir Braverman. One
sketch to rule them all: Rethinking network flow monitoring with univmon. In Proceedings of the
2016 ACM SIGCOMM Conference, pp. 101-114. ACM, 2016.
Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. In
International Conference on Machine Learning, 2018.
11
Published as a conference paper at ICLR 2019
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. Efficient computation of frequent and
top-k elements in data streams. In International Conference on Database Theory, pp. 398412.
Springer, 2005.
Gregory T Minton and Eric Price. Improved concentration bounds for count-sketch. In Proceedings
of the twenty-fifth annual ACM-SIAM Symposium on Discrete algorithms, pp. 669-686. SIAM,
2014.
Jayadev Misra and David Gries. Finding repeated elements. Science of computer programming, 2
(2):143-152, 1982.
Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. In
Advances in Neural Information Processing Systems, 2018.
Ali Mousavi, Ankit B Patel, and Richard G Baraniuk. A deep learning approach to structured signal
recovery. In Communication, Control, and Computing (Allerton), 2015 53rd Annual Allerton
Conference on, pp. 1336-1343. IEEE, 2015.
Ali Mousavi, Gautam Dasarathy, and Richard G Baraniuk. Deepcodec: Adaptive sensing and re-
covery via deep convolutional neural networks. arXiv preprint arXiv:1707.03386, 2017.
Pratanu Roy, Arijit Khan, and Gustavo Alonso. Augmented sketch: Faster and more accurate stream
processing. In Proceedings of the 2016 International Conference on Management of Data, pp.
1449-1463. ACM, 2016.
Ruslan Salakhutdinov and Geoffrey Hinton. Semantic hashing. International Journal of Approxi-
mate Reasoning, 50(7):969-978, 2009.
Stuart Schechter, Cormac Herley, and Michael Mitzenmacher. Popularity is everything: A new
approach to protecting passwords from statistical-guessing attacks. In Proceedings of the 5th
USENIX conference on Hot topics in security, pp. 1-8. USENIX Association, 2010.
Vibhaalakshmi Sivaraman, Srinivas Narayana, Ori Rottenstreich, S Muthukrishnan, and Jennifer
Rexford. Heavy-hitter detection entirely in the data plane. In Proceedings of the Symposium on
SDN Research, pp. 164-176. ACM, 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep neural
networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295-2329, 2017.
Partha Talukdar and William Cohen. Scaling graph-based semi supervised learning to large number
of labels using count-min sketch. In Artificial Intelligence and Statistics, pp. 940-947, 2014.
Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data - a
survey. Proceedings of the IEEE, 104(1):34-57, 2016.
Yair Weiss, Antonio Torralba, and Rob Fergus. Spectral hashing. In Advances in neural information
processing systems, pp. 1753-1760, 2009.
Minlan Yu, Lavanya Jose, and Rui Miao. Software defined traffic measurement with opensketch. In
NSDI, volume 13, pp. 29-42, 2013.
12
Published as a conference paper at ICLR 2019
9.	Analysis - full proofs
In this section, we analyze the performance of three different approaches, single (uniformly random)
hash function, Count-Min sketch, and Learned Count-Min sketch when the frequency of items is
from Zipfian distribution. For simplicity, we assume that the number of distinct elements n is equal
to the size of the universe |U|, and fi = 1/i. We use [n] to denote the set {1 . . . n}. We also drop
the normalization factor 1/N in the definition of estimation error.
The following observation is useful throughout this section (in particular, in the section on non-
asymptotic analysis).
Observation 9.1. For sufficiently large values of n (i.e., n > 250),
ln(n + I) < X i <lnn+0.58	(9.1)
i=1 i
9.1.	Single Hash Function
Lemma 9.2. The expected error of a single uniformly random hash function h : [n] → [B] (with B
buckets) for estimating thefrequency ofitems whose distribution is ZiPfian is Θ(埠n).
Proof:
E[Err(F, Fh)] = E[ X (f∙ - f) ∙ f ]= X E[ X	fq] ∙ f
j∈[n]	j∈[n]	{q6=j | h[q]=h[j]}
=X (θ(lBn) - B) ∙力=0号).□
j∈[n]
Moreover, since each bucket maintains the frequency of items that are mapped to it under h, the
space complexity of this approach is proportional to the number of buckets which is Θ(B).
9.2.	Count-Min S ketch
Here, we provide an upper bound and lower bound for the expected estimation error of Count-
Min sketch with k hash functions and B buckets per row. In the rest of this section, for each
j ∈ [n], ` ≤ k, we use ej,` and ej respectively to denote the estimation error of fj by h` and
Count-Min sketch. Recall that the expected error of Count-Min sketch is defined as follows:
E[Err(F, FCM)] = E[ X (f∙ - f) ∙ fj] = X E[e∕ ∙ f,	(9.2)
j∈[n]	j∈[n]
Our high-level approach is to partition the interval [0,Bln n] into m + 1 smaller intervals by a
sequence of thresholds Θ(ln1+γ(B)) = r0 ≤ ∙∙∙ ≤ rm, = B lnn where Y is a parameter to be
determined later. Formally, we define the sequence of ris to satisfy the following property:
n
∀' > 0,	r` ：= (ln(五)+lnr'+ι)lnγ r'+ι.	(9.3)
B
Claim 9.3. For each i ≥ 0, ln ri+1 ≥ 2 lnri.
Proof: By (9.3) and assuming lnri+ι ≥ ln(B), r < 2ln1+γ ri+ι. Hence, lnri+ι > (ri) 1+γ >
2 ln ri for sufficiently large values of ri7 assuming γ ≤ 3.
Note that as long as ln ri+ι ≥ ln(B), r ≤ ri+ι. Otherwise, r = o(ln1+γ(B)) < r0.	□
Corollary 9.4. For each i ≥ 1 and c ≥ 1, Pj≥i ln-c rj = O(ln-c ri).
7More precisely, ri ≥ 1.3 × 106.
13
Published as a conference paper at ICLR 2019
Then, to compute (9.2), We rewrite E[ej∙] using the thresholds r0,…，rm as follows:
∞
E[ej] =	Pr(ej ≥ x)dx
0
r rB0	m—1 r riB+1
=J	Pr(ej ≥ x)dx +	P W	Pr(ej ≥ x)dx
(9.4)
≤ O( B
m-1
PMej ≥ x|ej < ɪ) + X PHej ≥ Bq|ej < ɪDdx
q=i+1
Next, we compute Pr(ej∙ ≥ BB∣ej∙ < rB+1) for each t ∈ ∖ri, ri+ι).
Lemma 9.5. For each t ∈ 卜^门十。,Pr(ej- ≥ B匕 < rB1) = O(k(In(Bl:：二：+1)).
Proof: First we prove the following useful observation.
Claim 9.6. For each item j and hashfunction h`, E[ej∙,' | ej∙,' < B ] ≤ 由(njn r ∙
Proof: Note that the condition ej∙,' < B implies that for all items q < B (and q = j), h`(j) = h'(q)
Hence,
I / r〕VXf 1 V ln n - ln( B)	ln( B)+ln r	π
E[ej,' | ej,' < B] ≤ X fq ∙ B ≤ --B----=-------B-----.	□
Thus, by Markov’s inequality, for each item j and hash function h`,
Pr(ej,' >⅛Rj,' ≤ ⅛) ≤ ln(^
BB	t
(9.5)
Now, for each h` in Count-Min sketch, we bound the value of Pr(ej∙,' ≥ B) where t ∈ ∖ri, ri+1):
m—1
PMej,` ≥ B) ≤ X PMej,` ≥ Blej,' < -B+1-)
q=i
≤ X In(B) +ln rq+- = O( ɪ γ1-) B by (9.5) and Corollary 9.4
Hence, for k ≥ 2,
PMej ≥ -e∖< < +)) ≤ kPMej,- ≥ ~e∖-j< < ^^+1 )πk=2 PMej,` ≥ 石)
BB	B	B	B
=O (「第匕；；-))B by(9∙5) and (9.6)
(9.6)
Next, for each item j, we bound the contribution of each interval (B,
ri+1) to E[ej∙], namely
≥ x)dx.
ri+1	n
Claim 9.7. For each i ≥ 0, RLiB Pr(ej- ≥ x)dx = O( B lnYBk-1)-1 ：：])∙
Proof:
ri+1
B^~
Pr(ej ≥ x)dx
Jr
m—-
PHej ≥ x∖ej < -B+1)+ X PHej ≥ B∖ej < ~^+b~
{z
A
}	q=i+- |
{z
Bq
) dx
}
First, we compute JriB Adx.
ri+1	ri+1
P	PMej ≥ x∖ej < ri+1 Idx ≤ f	O(
J ri	B	Jri
k(ln( B) + ln ri+-)
(B lnγ(k--) ri+-)x
)dx
|

k(ln( B) + ln ri+-)
B lnγ(k--)--ri+-
(9.7)
14
Published as a conference paper at ICLR 2019
Similarly, R∏b Pm-；i Bqdx is at most:
ri+1 m-1	ri+1 m-1
Zri B X Pr(ej ≥ B lej < ɪ IdX =	Zr.	B	X O(
B B	q=i+1	B B	q=i+1
ri+1 m-1
=Zr i	B	X θ(
B B	q=i+1
k(ln( B) + ln rq+i) u
rqlnY(kT) rq+1 )
k
E---------)dχ B by (9.3)
lnγk rq+1
ri+1
k
JW	O( ] Yk - )dx B by Corollary 9.4
Hence,
ri+1
Ln，、、，
Pr(ej ≥ x)dx
3
O(
O( B⅛ ) = O(
k(ln( B ) + ln 门+2)
Blnγ(k-D ri+2
)	(9.8)
m-1
Pr(ej ≥ x|ej < *) + X Pr(ej ≥ 得 lej < q+k))) dx
B	BB
k(ln( B ) + ln 门十。
O(
B lnY(kT)T ri+ι
k(ln( B ) + ln 门十。
q=i+1
wOrk(ln( B)+ln ri+2)
)+ O( B lnY(kT) ri+2
) B by (9.7)-(9.8)
B lnY(kT)T ri+ι
B by (9.3)
(9.9)
)
Now, we complete the error analysis of (9.4):
E[ej] ≤
Pr(ej ≥ x|ej <
m-1
ɪ)+ X pr(ej ≥
q=i+1
rq Iej < rB1)) dx
m-1
O(B)+ X O(
i=0
k(ln( B) + ln ri+ι)
B lnY(kT)T ri+ι
B by (9.9)
O( B)+O(kBm⅛⅛)+O(
B lnY(kT)-2
r1)
(9.10)
Note that (9.10) requires γ(k - 1) - 2 ≥ 1 which is satisfied by setting γ = 3/(k - 1) and k ≥ 2.
Thus, for each item j ,
k + 2
E[ej ] = O(ro∕B)=O(In ；(B))
B
(9.11)
)
k
Lemma 9.8. The expected error of Count-Min sketch of size k × B (with k ≥ 2) for estimating
k + 2
items whose frequency distribution is Zipfian is O( Inn ln feB1 (n/B)). In particular, if B = Θ(n),
then E[Err(F,Fcm)] = O(乎).
Proof: By plugging in our upper bound on the estimation error of each item computed in (9.11) in
the definition of expected estimation error of Count-Min (9.2), we have the following.
~	ʌ	ln k+⅛ (n)
E[Err(F,Fcm)] = £E[ej] ∙ fj = O( —BB ∙ lnn).
j=1

Next, we show a lower bound on the expected error of Count-Min sketch with B buckets (more
precisely, of size (k × B/k)) for estimating the frequency of items that follow Zipf Law.
Observation 9.9. For each item j, Pr[ej ≥ 1/(2(B) ln k + 1)] ≥ 1 一 1.
Proof: For each item j, the probability that none of the first 2( B) ln k + 1 items (excluding itself)
collide with it in at least one of the hash functions hi, ∙∙∙ ,hk is at most k(1 一⅛) )2(k )ln k ≤ k.
15
Published as a conference paper at ICLR 2019
Hence, with probability at least 1 - 1, the estimation f includes the frequency of one of the top
2(Bk)ln k + 1 frequent items.	□
Lemma 9.10. The expected error of Count-Min sketch of size k X (BB) for estimating items whose
frequency distribution is ZiPfian is at least 23Fk+k ∙
Proof： Observation 9.9 implies that E®] ≥ (1 - 1)2(君)：口左十]=2君；1+k.Hence,
E[Err(F,Fcm)]= X E[ej] ∙ fj	'
j∈[n]
k-1
≥ (——— -------)ln n B By Observation 9.1
2B ln k + k

Theorem 9.11. The expected error ofCount-Min Sketch ofsize k × (BB) on estimating thefrequency
k + 2
of n itemsfrom Zipfian distribution is at least Ω(BBlnk) and at most O(k lnn EB ,( A)).
In particular, for the case B = Θ(n) and k = O(1), the expected error of Count-Min sketch is
Θ(喑).
Proof: The proof follows from Lemma 9.8 and 9.10. We remark that the bound in Lemma 9.8 is for
the expected estimation error of Count-Min sketch of size k × B . Hence, to get the bound on the
expected error of Count-Min of size k × (BB), we must replace B with B/k.	□
9.3.	Learned Count-Min Sketch
Definition 9.12 (φ-HeavyHitter). Given a set of items I = {iι,…，in} with frequencies f =
hfι,…，fni, an item j is a φ -HeavyHitter of I if fj ≥ φ∣∣f∣∣ι.
Remark 9.13. If the frequency distribution of items I is Zipfian, then the number of φ-HeavyHitters
is at most 1∕(φ ln n). In other words, Br ≤ (φln n)-1.
To recall, in our Learned Count-Min sketch with parameters (Br, B), Br buckets are reserved for
the frequent items returned by HH and the rest of items are fed to a Count-Min sketch of size
k × (B-Br) where k is a parameter to be determined. We emphasize that the space complexity of
Learned Count-Min sketch with parameter (Br, B) is Br + B = O(B).
Theorem 9.14. The optimal expected error of Learned Count-Min sketches with parameters
(Br, B) is at most (In(n/B-B+0.58) .In particular, for Br = Θ(B) = Θ(n), E[Err(F, FL-CM)]=
O( 1).	'
Proof: Since, the count of top Br frequent items are stored in their own buckets, for each j ≤ Br ,
ej = 0. Hence,
E[Err(F, FL-CM)] = X E[ej] ∙ fj
j ∈[n]
=X E[ej] ∙ fj	B ∀j ≤ Br ,ej = 0
j>Br
(ln(n/Br) + 0.58)2
< ʌ—'	------ B By Observation 9.1
B - Br
Note that the last inequality follows from the guarantee of single hash functions; in other words,
setting k = 1 in the Count-Min sketch.	□
9.3.1.	Learned Count-Min S ketch Using Noisy HeavyHitters Oracle
Unlike the previous part, here we assume that we are given a noisy HeavyHitters oracle HHδ such
that for each item j, Pr(H Hδ (j, B^⅛n) = HH0 (j, Bln n)) ≤ δ where HH0 isan ideal HeavyHitter
oracle that detects heavy items with no error.
Lemma 9.15. In an optimal Learned Count-Min sketch with parameters (Br, B) and a noisy
HeavyHitters oracle HHδ, E[Err(F, F(L-CM,δ))] = O(δ ln BB-B (n/Br)).
16
Published as a conference paper at ICLR 2019
Proof: The key observation is that each heavy item, any of Br most frequent items, may only
misclassify with probability δ. Hence, for each item j classified as “not heavy”,
F「/nBr + 1wrln(n∕Br) + ^ n√lnBr + ln(n/Br)
E[ej] ≤ δ ∙ ( B _ B ) + (—BTrB-) = O(--------BTrB-------
B - Br	B - Br	B - Br
),
(9.12)
where the first term denotes the expected contribution of the misclassified heavy items and the
second term denotes the expected contribution of non-heavy items.
The rest of analysis is similar to the proof of Theorem 9.14.
E[Err(F ,F(l-cm,β))]= X E[ej]∙ fj
j∈[n]
< δ X E[ej] ∙ fj + X E[ej] ∙ fj
j≤Br	j>Br
≤ O(δ ∙ln Br + ln(n/Br)) ∙ O(δ ln B+ lmn/Br))	B By(9.12)
B - Br
δ2 ln2 Br +ln2(n∕Br )
≤ ( B - Br	).
__	一	_	, _ .	_	,	一-	一 ，一 ，-	.	一 —一	, 一 ɪ	、 r
Corollary 9.16. Assuming Br = Θ(B) = Θ(n) and δ = O(1∕lnn), E[Err(F, F(L-CM,δ))]
O(1/n).
Space analysis. Here, we compute the amount of space that is required by this approach.
Lemma 9.17. The amount ofspace used by Learned Count-Min sketch ofsize k ∙ (B-Br) with Cutof
(Br reserved buckets) for estimating the frequency of items whose distribution is Zipfian is O(B).
Proof: The amount of space required to store the counters corresponding to functions hi, ∙∙∙ ,hk is
O(k ∙ (B-Br)). Here, We also need to keep a mapping from the heavy items (top Br frequent items
according to HHδ) to the reserved buckets Br which requires extra O(Br ) space; each reserved
buckets stores both the hashmap of its corresponding item and its count.
9.4.	Non-asymptotic Analysis of Count-Min and Learned Count-Min
In this section, We compare the non-asymptotic expected error of Count-Min sketch and out Learned
Count-Min sketch With ideal HeavyHitters oracle. Throughout this section, We assume that the
amount of available space to the frequency estimation algorithms is (1+α)B Words. More precisely,
We compare the expected error of Count-Min sketch With k hash functions and our Learned Count-
Min sketch With Br = αB reserved buckets. Recall that We computed the folloWing bounds on the
expected error of these approaches (Lemma 9.10 and Theorem 9.14):
(k - 1) lnn
[rrCM] ≥ 2(1 + α)B ln(k) + k,
ErErr	]< (ln(OB) + 0.58)2
E[ErrL-CM] ≤	(i-α)B
In the rest of this section, We assume that B ≥ γn and then compute the minimum value of γ that
guarantees E[ErrL-cM] ≤ ι++ε ∙ E[Err°M]. In other words, we compute the minimum amount of
space that is required so that our Learned Count-Min sketch performs better than Count-Min sketch
by a factor of at least (1 + ε).
E[ErrL-C M] ≤
Hence, we must have
(ln( Oγ )+0.58)2/	1
(1 - α)B	≤ 1 + ε
(k - 1) lnn	1
2(1 + α)B ln k + k ≤ 1 + ε . [rrCM],
(0.58 - ln α - lnγ)2 ≤
sponding quadratic equation,
(k-1)(1-α)
(1+ε)(2(1+α)ln k + ( Bk))
• ln n. By solving the corre-
ln2 γ + 2(ln α - 0.58) lnγ + ((ln α - 0.58)2 -
(k — 1)(1 — a)
(1 + ε)(2(1 + α) ln k + (B))
• ln n) ≤ 0.
This implies that ln Y = - ln α + 0.58 - (J(1+/2(-+01-0)+(畲))• ln 九 By setting α = 1, Y ≤
3.58	B
r
e
(k — 1) ln n
2(1+ε)(3ln k + (k∕B))
17
Published as a conference paper at ICLR 2019
Next, we consider different values of k and show that in each case what is the minimum amount of
space in which Learned CM outperforms CM by a factor of 1.06 (setting ε = 0.06).
• k = 1. In this case, we are basically comparing the expected error ofa single hash function
and Learned Count-Min. In particular, in order to get a gap of at least (1 + ε), by a more
careful analysis of Lemma 9.2, γ must satisfy the following condition:
E[ErrL-C M] ≤
(ln(表)+ 0.58)2
(1 — a)B
ln2 n — 1.65 /	1
(1 + α)B ≤ 1 + ε
. E[Errh],
To simplify it further, we require that (ln(2 )+0.58)2 ≤ 3.18 ∙ (ln2 n —1.65) which implies
that γ = Θ(1/ ln n).
1
≤ ---
—1 + ε
• k = 2. In this case, Y ≤ √i58∕4 5, for sufficiently large values of B. Hence, We require
that the total amount of available space is at least √5 j[" 5
k ∈ {3,4}: In this case, Y ≤ √]:的岂 5, for sufficiently large values of B. Hence, we
require that the total amount of available space is at least √5.j7n3 5,
k ≥ 5. In this case, Y ≤ √]亮分 6, for sufficiently large values of B. Hence, we require
that the total amount of available space is at least √5En2 6
We also note that settings where the number of buckets is close to n are quite common in practice.
For example, Goyal et al. (2012) uses ((1 + α)B = 20M, n = 33.5M) and ((1 + α)B = 50M, n =
94M) with k = 3, while Kumar et al. (2004) uses different pairs of ((1 + α)B, n) including (B =
288K, n = 563K). Plugging these values into the exact formulas computed above shows that
Learned CM has a significantly lower loss than CM.
10. Optimal Hash Function
In this section we compute an asymptotic bound error bound of optimal hash functions that map
items [n] to a set of buckets C [1 …B].
Claim 10.1. For a set of items I, let f(I) denote the total frequency of all items in I; f(I) :=
i∈I fi. In any optimal hash function of form {h : I → [BI]} with minimum estimation error, any
item Withfrequency at least f∣) does not collide with any other items in I.
Proof: For each bucket b ∈ [BI], lets fh(b) denotes the frequency of items mapped to b under
h; f (b) := Pi∈[∙h(i)=b fi. Recall that the estimation error of a hash function h is defined as
一 ,一,~ , _、 、	__ . 、 一 , 一, _、	~ , .
Err(F(I), Fh(I)) :=	PiEl fi	∙ (f (h(i))	— i).	Note that we can rewrite Err(F(I),	Fh(I))	as
Err(F(I), Fh(I)) = Xfi ∙ (f (h(i)) — fi) = Xfi ∙ f(h(i)) — X f
= X f(b)2 — Xfi2.	(10.1)
b∈[BI]	i∈I
Note that in (10.1) the second term is independent of h and is a constant. Hence, an optimal hash
function minimizes the first term, Pb∈B f (b)2 .
Suppose that an item i* with frequency at least f(∣) collides with a (non-empty) set of items I * ⊆
I \ {i*} under an optimal hash function h*. Since the total frequency of the items mapped to the
bucket b* containing i* is greater than f∣) (i.e., f (h(i*)) > f(∣)), there exists a bucket b such that
f (b) < f(∣). Next, we define a new hash function h with smaller estimation error compared to h*
which contradicts the optimality of h*:
h ʃ h*(i) ifi ∈ i\i*
'J Ib otherwise.
18
Published as a conference paper at ICLR 2019
Formally,
Err(F(I),Fh*(I))- Err(F(I), FHI)) = fh*(b*)2 + fh*(b)2 - fh(b*)2 - fh(b)2
=(fi* + f (I*))2 + fh*(b)2 -&一(fh*(b) + f(I*))2
= 2fi* ∙ f (I*) - 2fh*(b) ∙ f (I*)
= 2f(I*) ∙(fi* - fh*(b))
> 0 B Since fi* > ʃŋ > > fh*(b).	□
B
Next, we show that in any optimal hash function h* : [n] → [B] and assuming Zipfian input
distribution, Θ(B) most frequent items do not collide with any other items under h*.
Lemma 10.2. Suppose that B = n∕γ where Y ≥ e4.2 is a constant and lets assume that itemsfollow
Zipfian distribution. In any hash function h* : [n] → [B] with minimum estimation error, none of the
2Bγ mostfrequent items collide with any other items (i.e., they are mapped to a singleton bucket).
Proof: Let ij* be the most frequent item that is not mapped to a singleton bucket under h*. If
j* > 2Bγ then the statement holds. Suppose it is not the case and j* ≤ 2Bγ. Let I denote the set
of items with frequency at most fj* = 1/j* (i.e., I = {ij | j ≥ j*}) and let BI denote the number
of buckets that the items with index at least j* mapped to; BI = B - j* + 1. Also note that by
Observation 9.1, f (I) < ln( j) + 1. Next, by Claim 10.1, We show that h* does not hash the items
{j*, ∙∙∙ ,n} to BI optimally. In particular, we show that the frequency of item j* is more than f∣).
To prove this, first we observe that the function g(j) := j ∙ (ln(n/j) + 1) is strictly increasing in
[1, n]. Hence, for any j* ≤ 2Bγ,
nB
j* ∙(ln( j) + 1) ≤ 21nγ ∙ (ln(2γlnγ) + 1)
≤ B ∙ (1 -	—)	B Since ln(2ln γ) + 2 < ln Y for ln Y ≥ 4.2
< BI
Thus, fj* = j1* > ln(nBJ+1 > fBl which implies that an optimal hash function must map j* to a
singleton bucket.
Theorem 10.3.	If n/B ≥ e4.2, then the estimation error of any hash function that maps a set ofn
itemsfollowing Zipfian distribution to B buckets is Ω( In B^b)).
Proof: By Lemma 10.2, in any hash function with minimum estimation error, the (2Bγ) most
frequent items do not collide with any other items (i.e., they are mapped into a singleton bucket)
where Y = n/B > e4.2 .
Hence, the goal is to minimize (10.1) for the set of items I which consist of all items other than
the (2Bγ) most frequent items. Since the sum of squares of m items that summed to S is at least
S2/m, the multi-set loss of any optimal hash function is at least:
Err(F(I), Fh* (I))= E f (b)2 - Efi2
B By (10.1)
b∈[B]
(Pi∈I fi)2
i∈[n]
B(1 -
21nγ)
≥ (In(2Yln Y) - I)
i∈I
)2	2lnY
B
0(*
C(In2(n∕B))
(B ).
B By Observation 9.1
B Since Y > e4
≥
—
—
B
1
+ —
n

19
Published as a conference paper at ICLR 2019
Next, we show a more general statement which basically shows that the estimation error of any
CoUnt-Min sketch with B buckets is Ω(ln(B/B)) no matter how many rows (i.e., the value of k) it
has.
Theorem 10.4.	If n/B ≥ e4.2, then the estimation error of any Count-Min sketch that maps a set
of n itemsfollowing Zipfian distribution to B buckets is Ω( In BIn)).
Proof: We prove the statement by showing a reduction that given a Count-Min sketch CM (k) with
hash functions hi, ∙∙∙ ,hk ∈ {h : [n] → [B/k]} constructs a single hash function h : [n] → [B]
whose estimation error is less than or equal to the estimation error of CM(k).
For each item i, we define C0[i] tobe the bucket whose value is returned by CM(k) as the estimate of
fi; C0[i] := arg minj∈[k] C[j, hj (i)]. Since the total number of buckets in CM (k) is B, |{C0[i] | i ∈
[n]}| ≤ B; in other words, we only consider the subset of buckets that CM(k) uses to report the
estimates of {f | i ∈ [n]} which trivially has size at most B. We define h* as follows:
h*(i) = (j*, hj*(i))	B for each i ∈ [n], where j* = arg min C [j, hj (i)]
j∈[k]
Unlike CM(k), h* maps each item to exactly one bucket in {C[',j] | ' ∈ [k],j ∈ [B/k]}; hence,
for each item i, C0[h*(i)] ≤ C[h*(i)] = fi where f is the estimate of f returned by CM(k).
Moreover, since for each i, C0 [h* (i)] ≥ fi,
Err(F,Fh*) ≤ Err(F, FCM(k)).
Finally, by Theorem 10.3, the estimation error of h* is Ω(ln (B/B)) which implies that the estimation
error of CM(k) is Ω( ln2(B/B)) as well.	□
20