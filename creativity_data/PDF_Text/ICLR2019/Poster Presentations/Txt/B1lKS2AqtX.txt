Published as a conference paper at ICLR 2019
Eidetic 3D LSTM:
A Model for Video Prediction and Beyond
Yunbo Wang1 ∖ Lu Jiang2, Ming-Hsuan Yang2,3, Li-Jia Li4, Mingsheng Long1, Li Fei-Fei4
1Tsinghua University, 2Google AI, 3University of California, Merced, 4Stanford University
Ab stract
Spatiotemporal predictive learning, though long considered to be a promising self-
supervised feature learning method, seldom shows its effectiveness beyond future
video prediction. The reason is that it is difficult to learn good representations
for both short-term frame dependency and long-term high-level relations. We
present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convo-
lutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs
motion-aware and enables the memory cell to store better short-term features. For
long-term relations, we make the present memory state interact with its histori-
cal records via a gate-controlled self-attention module. We describe this memory
transition mechanism eidetic as it is able to effectively recall the stored memo-
ries across multiple time stamps even after long periods of disturbance. We first
evaluate the E3D-LSTM network on widely-used future video prediction datasets
and achieve the state-of-the-art performance. Then we show that the E3D-LSTM
network also performs well on the early activity recognition to infer what is hap-
pening or what will happen after observing only limited frames of video. This
task aligns well with video prediction in modeling action intentions and tendency.
1	Introduction
A fundamental problem in spatiotemporal predictive learning is how to effectively learn good repre-
sentations for video inference or reasoning. Currently, recurrent neural networks (RNNs) remain to
be the most promising models in this field, and have achieved state-of-the-art results on a number of
future video prediction benchmarks (Wang et al., 2018b; Oliu et al., 2018). However, beyond frames
prediction, RNN based models are less effective in learning high-level video representations or cap-
turing long-term relations. On the other hand, recent studies demonstrate that 3D Convolutional
Neural networks (3D-CNNs) surpass RNNs in learning better representations for action classifica-
tion (Carreira & Zisserman, 2017; Tran et al., 2015). For instance, variants of 3D-CNNs, such as
Inflated 3D-CNNs, have significantly increased action classification accuracy over the UCF 101 and
Kinetics datasets. These 3D-CNN architectures have no recurrent structures but instead employ 3D
convolution (3D-Conv) and 3D pooling operations to preserve temporal information of the input
sequences which would be otherwise discarded in classical 2D convolution operations.
Motivated by the recent success of 3D-CNNs, in this paper we propose a new model for spatiotem-
poral predictive learning based on both recurrent modeling (for temporal dependency) and feed-
forward 3D-Conv modeling (for local dynamics). A plausible approach, of course, is to simply
stack 3D-Convs and each RNN unit in a feed-forward way using 3D-Convs for either perceiving
fine-grained features from raw videos or combining high-level representations. However, as shown
in our experiments, these straightforward extensions may not outperform the baseline RNN model.
We attribute these findings to that RNNs and 3D-CNNs represent two very different mechanisms
for the same purpose of spatiotemporal modeling, and connecting them directly fails to exploit their
complementary advantages. Therefore, it remains challenging and requires principled approaches
to design an effective spatiotemporal network.
To this end, we propose a new model called Eidetic 3D LSTM (E3D-LSTM) for spatiotemporal
predictive learning. We introduce an eidetic 3D memory to: a) memorize local appearance and mo-
tion in a short spatiotemporal volume, and b) recall the long-range historical context by learning
* Corresponding author: Wangyb15@mails.tsinghua.edu.cn. Work done in part at Google AL
1
Published as a conference paper at ICLR 2019
to attend to previous memory states. Regarding the short-term dependency, in many cases, spa-
tiotemporal predictive modeling mainly depends on temporally nearby appearances and on-going
short-term motions. All the information is encapsulated into the eidetic 3D memory cell with a
short time convolution window, and used in recurrent transitions. Our experimental results show
that integrating 3D-Conv deep into RNNs is effective for modeling local representations in a con-
secutive manner. On the other hand, for long-term interactions, which is important for predicting
non-stationary or periodical videos as well as learning high-level video representations, we exploit a
self-attention mechanism controlled by revised recurrent gates to recall temporally distant memory.
The current memory state of E3D-LSTM is learned to attend to all previous relevant moments. Our
experimental results verify that this attention mechanism is beneficial for long-term memorization.
We describe this memory transition mechanism eidetic as it is able to effectively recall the stored
memories across multiple time stamps even after long periods of disturbance.
To the best of our knowledge, the proposed E3D-LSTM model is among the first approaches that
leverage 3D-Conv in RNNs. We empirically validate it on standard spatiotemporal predictive tasks
and an early activity recognition task over four benchmarks: a) on future video prediction, it achieves
the best-published accuracy on three classical benchmarks; b) on early activity recognition, it out-
performs the state-of-the-art action recognition methods. In addition, we show that self-supervised
learning can further improve the performance of early activity recognition. We present ablation
studies to verify the effectiveness of all modules in the proposed E3D-LSTM model.
2	Related Work and Problem Context
Spatiotemporal Predictive Learning Models. In recent years, RNNs have been extensively used
in sequence prediction and future frame prediction. Srivastava et al. (2015) extended the LSTM-
based sequence to sequence model (Sutskever et al., 2014) for language modeling to learning video
representations. Shi et al. (2015) proposed the convolutional LSTM by integrating convolutions
into recurrent state transitions for high-dimensional sequence prediction. The convolutional LSTM
model is extended by Finn et al. (2016) to predict future states of robotic environments. Villegas et al.
(2017) leveraged optical flow to help capture short-term video dynamics for video prediction. Xu
et al. (2018) proposed a two-stream RNN that deals with structural video content in separate streams.
Kalchbrenner et al. (2017) introduced a sophisticated model that extends recurrent structures to
estimate local dependencies between adjacent pixels. While this video pixel network (VPN) model
is able to describe image sequences, the computational load is prohibitively high.
The above-mentioned recurrent models predict future frame mainly based on sequentially updated
memory states. When the memory cell is refreshed, older memories will be discarded immediately.
In contrast, the proposed E3D-LSTM model maintains a list of historical memory records and re-
vokes them when necessary, thereby facilitating long-range video reasoning. While in spirit this
idea is similar to the self-attention module in feed-forward networks (Vaswani et al., 2017; Wang
et al., 2018a), we exploit it to correlate long-term and short-term video representations in this work.
Another significant difference between the above-mentioned prior work and the proposed model is
that we use 3D-Convs as basic operations inside the E3D-LSTM instead of fully-connected or 2D
convolution operations. We show using 3D-Convs to model recurrent state-to-state transitions can
significantly improve prediction performance. This idea is motivated by recent advances in video
classification (a high-level representation learning task) (Ji et al., 2013; Tran et al., 2015; Carreira
& Zisserman, 2017). We note that Vondrick et al. (2016) and Tulyakov et al. (2018) also introduced
3D-CNNs for spatiotemporal predictive learning. However, these networks are feed-forward and do
not capture temporal consistency effectively.
Future prediction errors of an imperfect model can be categorized by two factors: a) the “systematic
errors” caused by a lack of modeling ability to the deterministic variations; b) the stochastic, inherent
uncertainty of the future. We aim to minimize the first factor in this work. For the second factor,
numerous methods have applied adversarial training or variational auto-encoders to video prediction,
for example (Mathieu et al., 2016; Vondrick et al., 2016; Denton & Fergus, 2018; Bhattacharjee &
Das, 2017; Tulyakov et al., 2018; Lu et al., 2017; Wichers et al., 2018).
Convolutional Recurrent Networks. Our model is closely related to convolutional recurrent net-
works. In the ConvLSTM network (Shi et al., 2015), all state transitions are implemented with 2D
convolutions. As such, the transition function is no longer permutation invariant and able to better
2
Published as a conference paper at ICLR 2019
(b) 3D-CNN on Top
(c) E3D-LSTM Network
⑶ 3D-CNN at Bottom
Figure 1: Three approaches to integrate 3D-Convs into recurrent networks. Blue arrows indicate data
transition paths with 3D-Convs (for feed-forward features or recurrent hidden states). The diagrams
are simplified for illustration, with fewer layers and RNN states than What are actually used in our
experiments. The classifiers are removed when being trained for future video prediction.
perceive relations in a spatiotemporal neighborhood. The spatiotemporal LSTM (ST-LSTM) is char-
acterized by delivering two memory states separately (Wang et al., 2017): memory M in a zigzag
direction and memory C being passed horizontally (see Appendix A for details). In this model,
M provides greater capability to model short-term motions, and C is adopted from fully-connected
LSTMs (Hochreiter & Schmidhuber, 1997) to ease the vanishing gradient problem. Although the
ST-LSTM performs well on video prediction benchmarks, it does not capture long-term video re-
lations effectively. The forget gates of memory C tend to respond strongly to short-term features,
thereby easily falling into a saturated zone (with values between 0 and 0.1) and interrupting long-
range information flows. We adopt the zigzag updating route of memory M from the ST-LSTM,
while improving the forgetting mechanism in updating the temporal memory C. We also increase
the dimensions of memory states and take 3D-Convs as the basic operators for state transitions.
3	EIDETIC3DLSTM
This section first presents the Eidetic 3D LSTM for perceiving and memorizing both short-term and
long-term representations in videos. We then discuss a scheduled multi-task learning strategy that
uses predictive learning as an auxiliary self-supervised task for activity recognition.
3.1	3D Convolutions in Recurrent Network
An ideal predictive model relies on effective learning of video representations. RNNs and 3D-CNNs
are network architectures of different mechanisms for modeling spatiotemporal data. In this work,
we aim to leverage the strength of each one in a unified architecture and start the discussion with
two plausible extensions of stacking 3D-Convs and RNN units. Figure 1(a) and 1(b) illustrate two
hybrid baseline networks which add 3D-CNNs before or after stacked spatiotemporal LSTMs.
However, we find that integrating the 3D-Convs outside the LSTM unit performs noticeably worse
than the baseline RNN model. To this end, we propose a “deeper” integration of 3D-Convs inside
the LSTM unit in order to incorporate the convolutional features into the recurrent state transition
over time. Figure 1(c) shows the overall encoder-decoder architecture. In this model, a consecutive
of T input frames are first encoded by a few layers of 3D-Convs to obtain high-dimension feature
maps. The 3D-Conv feature maps are directly fed into a novel E3D-LSTM to model the long-
term spatiotemporal interaction. Finally, the E3D-LSTM hidden states are decoded by a number of
stacked 3D-Conv layers to get the predicted video frames. For classification tasks, the hidden states
can be directly used as the learned video representation.
3.2	Eidetic Memory Transition
The architecture of the proposed Eidetic 3D LSTM is illustrated in Figure 2, where the red arrows
indicate short-term information flow and the blue arrows denote long-term information flow. There
are 4 inputs: Xt, the 3D-Conv feature maps from encoders or hidden states from the previous E3D-
3
Published as a conference paper at ICLR 2019
Figure 2: Comparison of (a) the standard memory transition approach in the Spatiotemporal LSTM
and (b) the attentive memory transition approach in the Eidetic 3D LSTM. Red arrows indicate the
short-term information flow. Blue arrows are the attentive memory flow, which potentially enables
our model to capture the long-term relations. Cubes denote higher-dimensional hidden states and
memory states. Cylinders denote higher-dimensional gates. Θ is the Hadamard product. 0 is the
matrix product after reshaping matrices into appropriate 2-dimensional forms.
LSTM layer; Ht_1, the hidden states from previous time stamp; Ck_1, the memory states from
previous time stamp; and Mk_1,the previous spatiotemporal memory states described earlier.
We use recurrent 3D-Convs as motion-aware perceptrons to extract short-term appearance and lo-
cal motions in continuous space-time fields and store them in a small spatiotemporal volume. As
such, video appearance and short-term motions can be encoded in RT ×H ×W ×C tensors, in which
each dimension indicates temporal depth, spatial size, and the number of feature map channels, re-
spectively. By inflating the memory state along the time dimension, we found that the proposed
E3D-LSTM becomes more capable of characterizing and memorizing local or short-term motions.
To capture the long-term frame interactions, we improve the recurrent transition function of the
memory states by proposing a new memory RECALL mechanism:
Rt = σ(Wχr * Xt + Whr * Hk-I + br)
It = σ(Wχi * Xt + Whi * Hk-I + bi)
Gt = tanh(Wxg *Xt + Whg * Htk-1 +bg)	(1)
RECALL(Rt, Ck-T：t_1) = Softmax(Rt ∙ (Ck-T:t-1) ) ∙ Ck-T:t-1
Ctk = It	Gt + LayerNorm(Ctk-1 + RECALL(Rt , Ctk-τ :t-1 )),
where σ is the sigmoid function, * is the 3D-Conv operation, Θ is the Hadamard product, ∙ is the
matrix product after reshaping the recall gate Rt and memory states Ck-T：t-i into RTHW×c and
RτTHW ×C matrices, respectively, and τ is the number of memory states that are concatenated along
the temporal dimension. Three terms are involved in computing Ctk. The first one ItΘ Gt encodes
local video appearance and motions, where It is the input gate and Gt is the input modulation
gate like standard LSTMs. The second one Ctk_1 can be viewed as a short-cut connection from the
previous memory state, which captures short-term changes between adjacent time stamps. In this
process, the accessible memory field is fixed and limited. Therefore, we introduce the third term
of memory transition function, modeling long-term video relations according to local motion and
appearance (as encoded in Xt and Htk_1). The RECALL function is implemented as an attentive
module to compute the relationship between the encoded local patterns and the whole memory
space. A set of parameterized gates Rt, acting as memory access instructions, control where and
what to attend in historical memory records. These two terms are respectively designed for short-
term and long-term video modeling. We integrate them in a unified network by applying layer
normalization (Ba et al., 2016) to their element-wise sum, in order to mitigate the covariant shift
and stabilize the training process, as it has been commonly used in RNNs. The hyper-parameter
τ in Ck-T：t-i decides how many historical memory states are attended by the recall gate Rt. To
involve more long-term relations, in most experiments, we take C1k:t_1 as the inputs of the RECALL
function and do not fix τ. Whereas in particular, we enable online recognition by setting τ to 5.
Unlike the conventional memory transition function, the RECALL function learns the size of tem-
poral interactions. For longer sequences, this allows attending to distant states containing salient in-
4
Published as a conference paper at ICLR 2019
formation. Our work is partially motivated by self-attention mechanisms (Lin et al., 2017; Vaswani
et al., 2017). However, in our model, the attention mechanism is not applied over the output states
but during the memory transitions. It is used to evoke past memories from distant time stamps for
memorizing and distilling useful information from what has been perceived. We show that learning
attention over previous memory states is beneficial in recalling the long-range historical context.
The memory tensor Ctk is named eidetic 3D memory and the entire unit is called E3D-LSTM. We
also exploit the same RECALL method to correlate Mt1:k along the vertical memory transition flow,
but it turns out to be less helpful. With the updated memory state Ctk, the output hidden states are:
I0 = σ(WXi * Xt + Wmi * MkT + bi)
G0 = tanh(Wxg * Xt + Wmg * MkT + bg)
Ft0 =σ(Wx0f *Xt+Wmf *Mtk-1+b0f)
Mtk =It0	Gt0 +Ft0 Mtk-1
Ot =σ(Wxo*Xt+Who * Htk-1 +Wco *Ctk +Wmo *Mtk +bo)
Htk = Ot tanh(W1×1×1 * [Ctk,Mtk]),
(2)
where W1×1×1 is the 1 × 1 × 1 convolutions for the transformation of the channel number. It0, Gt0,
and Ft0 are gate structures of the spatiotemporal memory. Ot is the output gate.
3.3	Self-supervised Auxiliary Learning
For many supervised tasks such as video action recognition, there are often not enough supervisions
or annotations over time for training a satisfactory RNN. As an auxiliary measure to this problem,
future video prediction is considered as a promising representation learning approach that is more
densely supervised over time and might extract useful features to assist video understanding.
We consider two tasks: the pixel-level future frames prediction and another video-level classification
task (early activity recognition in our case). For frames prediction, the objective function is:
2
Lprediction = kX -XbkF+ kX - Xbk1,
(3)
where Xb and X are respectively predicted and ground truth future frames. ∣∣∙∣∣f is the FrobeniUs
norm. For early activity recognition, we make the models for these two tasks share the same network
backbone in the end-to-end training using a multi-task learning objective:
Lrecognition =λkX -Xbk2F+Lce(Y,Yb),
(4)
where Y and Y are high-level predictions and corresponding ground truth classes. Lce is the cross-
entropy loss for classification, and λ is the weight factor.
Although improving both tasks requires proper long short-term contextual representations, there
is no guarantee that features learned with pixel-level supervisions will fully align with any high-
level objectives. We thus introduce a scheduled learning strategy where the objective function is
gradually inclined from one task to the other in a curriculum learning manner (Bengio et al., 2009).
Specifically, we apply a linear decay to λ over the number of iterations i:
λ(i) = max(η, λ(0) — e ∙ i),
(5)
where λ(0) and η are respectively maximum and minimum values of λ(i), controls the decreasing
speed of the role of the auxiliary task. We call this approach the Self-supervised Auxiliary Learning.
4	Experiments
We evaluate the proposed E3D-LSTM model on two tasks: future video prediction and early activ-
ity recognition. These two tasks are of great importance with numerous applications that require
effective spatiotemporal predictive models. We demonstrate that the E3D-LSTM model performs
favorably against the state-of-the-art models on four challenging datasets. The source code and
trained models will be made available to the public.
5
Published as a conference paper at ICLR 2019
Inputs	⅞2			守	*			O	o	
Ground Truth	g			T	T	"ɪ"	J	J		♦
OUrs	g			T	ɪ			%	H	
PredRNN++	g	旨		T		⅞				
PredRNN	Q	Q		3					不	
VPN Baseline	g	5	◎	3		马		ʒ		
ConvLSTM		Q		3			J			
(a) 10 → 10 Prediction
Prior Context (same as Seq. 2)
Seq. 1 Inputs
Seq. 1 Predictions
Seq. 2 Inputs
Seq. 2 Predictions (the first line is the expected ground truth
PredRNN++
ConvLSTM
(b) Copy Test
Figure 3: Video prediction examples on the Moving MNIST dataset.
Table 1: Results on the Moving MNIST dataset. All models, except DFN and VPN, are trained with
a comparable number of parameters. Higher SSIM or lower MSE scores indicate better results.
Model	10 → 10		Copy	
	SSIM	MSE	SSIM	MSE
ConvLSTM (SHI ET al., 2015)	0.713	96.5	0.539	143.2
DFN (De Brabandere et al., 2016)	0.726	89.0	0.598	153.9
CDNA (Finn etal., 2016)	0.728	84.2	0.671	127.1
FRNN (OLIU ETAL., 2018)	0.819	68.4	0.694	110.5
VPN BASELINE (Kalchbrenner ET al., 2017)	0.870	64.1	0.736	78.0
PredRNN (Wang et al., 2017)	0.869	56.5	0.745	80.3
PredRNN++ (Wang et al., 2018b)	0.885	46.3	0.807	69.9
E3D-LSTM	0.910	41.3	0.852	56.8
4.1 FUTURE VIDEO PREDICTION: MOVING MNIST
We first evaluate the E3D-LSTM model against the state-of-the-art video prediction models on a
commonly used synthetic benchmark dataset with moving digits. All experiments are conducted
using TensorFlow (Abadi et al., 2016) and trained with the ADAM optimizer (Kingma & Ba, 2015)
to minimize the l1 + l2 loss over every pixel in the frame. For fair comparisons, we ensure all models
to have comparable numbers of parameters, and apply the same scheduled sampling strategy (Bengio
et al., 2015) in order to reduce the difficulty of training recurrent models.
Dataset and Setup. The moving MNIST dataset is constructed by randomly sampling two digits
from the original MNIST dataset and making them float and bounce at boundaries with a constant
velocity and angle inside a black canvas of 64 × 64 pixels. The whole dataset has a fixed number of
entries, 10, 000 sequences for training, 3, 000 for validation and 5, 000 for test.
We stack 4 E3D-LSTMs in the architecture illustrated in Figure 1(c), leaving out 3D-CNN encoders
for this task. To retain the shape of hidden states over time, the integrated 3D-Conv operators are
composed of a 2 × 5 × 5 (time × height × width) convolutions and a corresponding transposed
convolution with the same filter size. The number of hidden state channels of each E3D-LSTM is
64. The temporal stride is set to 1 and there is one overlapping frame over consecutive time stamps.
A single 3D-Conv layer is used as the decoder to map motion-aware hidden states to output frames.
The E3D-LSTM model is evaluated against the state-of-the-art methods including the ConvLSTM
network (Shi et al., 2015), DFN (De Brabandere et al., 2016), CDNA (Finn et al., 2016), VPN
baseline model with CNN decoders (Kalchbrenner et al., 2017), PredRNN (Wang et al., 2017),
PredRNN++ (Wang et al., 2018b) and FRNN (Oliu et al., 2018).
Main Results. Table 1 shows the performance of the evaluated models using a common setting in
the literature: generating 10 future frames given the previous 10 observations (denoted as 10 → 10).
We use the per-frame structural similarity index measure (SSIM) (Wang et al., 2004) and per-frame
mean squared error (MSE) for evaluation. The SSIM ranges between -1 and 1, representing the
similarity between the generated image and the ground truth. As shown in the second column
(10 → 10) of Table 1, our model performs well against the state-of-the-art methods in both metrics.
The results show that the E3D-LSTM network is effective in modeling spatiotemporal data for video
6
Published as a conference paper at ICLR 2019
Table 2: Ablation study on the Moving MNIST dataset (10 → 10).
Model	SSIM	MSE
Baseline 1: 3D-CNN at b ottom (Figure 1 (a))	0.859	50.6
Baseline 2: 3D-CNN on top (Figure 1 (b))	0.862	53.4
Baseline 3: Ours (w/o 3D convolutions)	0.894	44.2
Baseline 4: Ours (w/o memory attention)	0.880	45.7
E3D-LSTM	0.910	41.3
prediction. Figure 3(a) shows the qualitative comparisons in which our model predicts future frames
from entangled digits better than other methods.
Copy Test. We evaluate the proposed model using the Copy Test setting where the task is to mem-
orize useful information in a longer input sequence when the recurrent disturbance is present. The
input clip consists of three sub-sequences, as illustrated in Figure 3(b). Seq 1 and Seq 2 are com-
pletely irrelevant, and ahead of them, another sub-sequence called prior context is given as the input,
which is exactly the same as Seq 2. Frames marked by black arrows are inputs and those marked by
red arrows are expected outputs. There are two training objective: a) to predict 10 future frames of
Seq 1; and b) to predict 10 future frames of Seq 2. At the test time, we only evaluate the prediction
result of Seq 2. The copy test evaluates the modeling capability of long-range video frame relations.
A well-designed model should make precise predictions regarding Seq 2, as it has seen all frames of
this sequence before. However, this task is difficult for previous LSTM networks. Because Seq 1 is
completely irrelevant, the attempt of making predictions of Seq 1 can erase its memory of Seq 2.
The results are presented in the third column (Copy) of Table 1. All baseline models suffer from the
influence brought by irrelevant frames in Seq 2 and tend to gradually forget the salient information
in the prior context. However, thanks to the eidetic 3D memory, our E3D-LSTM model captures the
long-term video frame interactions and performs well in both metrics. A careful inspection of the
attention weight shows that the E3D-LSTM model can better attend to useful historical representa-
tions across multiple time stamps. The copy test suggests that the E3D-LSTM network is capable of
modeling long-range periodical motions effectively.
Ablation Study. We conduct a series of ablation studies and summarize the results in Table 2.
First, on the first two rows, we show two alternative 3D-LSTM models with 3D-Convs outside the
recurrent unit, including 3D-CNN at Bottom (Figure 1(a)) and 3D-CNN on Top (Figure 1(b)). The
performance drop validates the integration of 3D-Convs and RNN units via the eidetic 3D memory.
Second, the third baseline method is a special case where all 3D convolutional filters in our model
are reduced to 2D. The results demonstrate the effect of capturing local spatiotemporal patterns by
the 3D memory within an individual recurrent state. Furthermore, the contribution of the memory
attention mechanism can be isolated in the fourth baseline method. Note that all evaluated models
are trained with a similar number of parameters for fair comparisons, and the performance gain
comes from design options rather than increased model parameters.
4.2 Future Video Prediction: KTH Action
We evaluate the proposed E3D-LSTM model on video prediction of real-world datasets.
Dataset and Setup. The KTH action dataset (Schuldt et al., 2004) contains 25 individuals per-
forming 6 types of actions, including walking, jogging, running, boxing, hand waving and hand
clapping. On average, each video clip lasts 4 seconds. We follow the experimental setup in (Ville-
gas et al., 2017) by using person 1-16 for training and 17-25 for testing. Each frame is resized to
128 × 128 pixels. We employ the same E3D-LSTM network architecture detailed in Section 4.1.
Models are trained to predict next 10 frames from the previous 10 observations. The prediction
horizon at the test time is extended to 20 or 40 time stamps.
Results. Table 3 shows quantitative results of the proposed model and state-of-the-art methods.
Same as prior work, we use SSIM and PSNR as metrics. Consistent with the observations on the
moving MNIST dataset, the E3D-LSTM model performs favorably against the state-of-the-art meth-
ods across three settings of predicting future 10 frames, 20 frames, and copy test. These empirical
results demonstrate the effectiveness of the E3D-LSTM model for modeling spatiotemporal data.
Figure 4 compares representative generated frames. We select video sequences with relatively com-
plicated spatiotemporal variations (in both moving trajectories and human figure sizes). In the top
7
Published as a conference paper at ICLR 2019
Figure 4: Comparisons of the generated frames on KTH. (Top) predictions of next 40 frames based
on 10 previous observations. (Bottom) the copy test that requires to reproducing prior inputs.
Table 3: Quantitative evaluation of different methods on the KTH human action test set. The metrics
are averaged over the predicted frames. Higher scores indicate better prediction results.
Model	10 → 20		10 → 40		Copy (→ 40)	
	PSNR	SSIM	PSNR	SSIM	PSNR	SSIM
ConvLSTM (SHI ET al., 2015)	23.58	0.712	22.85	0.639	23.49	0.670
DFN (De Brabandere et al., 2016)	27.26	0.794	23.01	0.652	23.37	0.664
MCnet (Villegas et al., 2017)	25.95	0.804	-	-	-	-
FRNN (OLIU ETAL., 2018)	26.12	0.771	23.77	0.678	24.00	0.685
PredRNN (Wang etal., 2017)	27.55	0.839	24.16	0.703	24.45	0.711
PredRNN++ (Wang et al., 2018b)	28.47	0.865	25.21	0.741	25.90	0.759
E3D-LSTM	29.31	0.879	27.24	0.810	30.59	0.874
half (predicting the next 40 frames based on 10 previous frames), E3D-LSTM predicts more ac-
curate motion trajectories into the future, whereas PredRNN++ and ConvLSTM incorrectly predict
the person moving out of the scenes. The lower half shows the copy test providing the expected
outputs as prior inputs. We directly apply models, which are trained under the first setting, to this
test. Without the prior context, it would be difficult to predict human motions for some cases. With
prior inputs, E3D-LSTM benefits the most from its memories and responds well to rapid appear-
ance change. In contrast, PredRNN++ and ConvLSTM are not able to capture useful spatiotemporal
patterns from distant observations due to the lack of modeling long-term data relations.
4.3	A Real Video Prediction Application: Traffic Flow Prediction
We further evaluate our method on the TaxiBJ dataset, which contains real-world traffic flow data in
consecutive heat maps. Predicting urban traffic conditions is a complex setting, as the heat maps are
very noisy and we do not have any underlying or additional information that can facilitate this task.
Table 4: Experimental results on the TaxiBJ dataset. We report MSE at every time stamp.
Model	Frame 1	Frame 2	Frame 3	Frame 4
ST-ResNet (Zhang et al., 2017)	0.688	0.939	1.130	1.288
VPN (Kalchbrenner et al., 2017)	0.744	1.031	1.251	1.444
FRNN (oLIU ET AL., 2018)	0.682	0.823	0.989	1.183
PredRNN (Wang et al., 2017)	0.634	0.934	1.047	1.263
PredRNN++ (Wang et al., 2018b)	0.641	0.855	0.979	1.158
E3D-LSTM	0.620	0.773	0.888	0.984
Dataset and Setup. The TaxiBJ dataset is collected from the chaotic real-world environment using
GPS monitors of taxicabs Beijing. Each frame is a 32 × 32 × 2 heat map. The last dimension denotes
8
Published as a conference paper at ICLR 2019
Figure 5: Prediction results on the TaxiBJ traffic flow dataset. For ease of comparison, We visualize
the differences between the generated heat maps and their corresponding ground truth heat maps.
the entering and leaving traffic flow intensities at the same area. We split the whole dataset into a
training set and a test set as described in (Zhang et al., 2017). We train the networks to predict 4
frames (the next 2 hours) from 4 observations. We use the same network architecture and training
setups as the one on Moving MNIST and KTH datasets.
Results. We report MSE at every time stamp in Table 4 where lower scores indicate better predic-
tion results. We also show a prediction example in Figure 5. Furthermore, we visualize the differ-
ences between the generated heat maps and the ground truth heat maps. Overall, the E3D-LSTM
model outperforms the other methods, with the lowest differences intensities in most areas.
4.4	Early Activity Recognition: Something-Something
To validate that the E3D-LSTM model can learn high-level video representations effectively, we
carry out experiments on early activity recognition. The task is to predict an activity category in
a video after only observing a fraction of frames. We choose not to evaluate on the full-length
video for the activity recognition task, because when a model sees the full-length video, it may
make decisions solely based on the scene information, e.g. seeing only the last frame is enough to
recognize many actions. As a result, the full-length video task may not align well with our previous
video prediction tasks, in which the sequential tendency and causality are important.
Dataset and Setup. The something-something dataset (Goyal et al., 2017) is a recent benchmark
for activity/action recognition (https://20bn.com/datasets/something-something). We
use the standard and official subset which contains 56, 769 short videos for the training set and
7, 503 videos for the validation set on 41 action categories. The video length ranges between 2 and
6 seconds with 24 fps. We adopt the early activity recognition setting (Ma et al., 2016; Zeng et al.,
2017; Zhou et al., 2018), where a model predicts an action type after observing the first 25% or 50%
frames of each video. As these actions appear in diverse scenes and involve interaction with differ-
ent objects, it is challenging to predict actions even for humans (See Figure 6). There are only subtle
differences between some actions in this dataset, such as “Poking a stack of [Something] so that the
stack collapses” versus “Poking a stack of [Something] without the stack collapsing”, or “Pouring
[Something] into [Something]” versus “Trying to pour [Something] into [Something], but missing
so it spills next to it”. To make a correct prediction, a model needs to exploit spatiotemporal cues
to understand the subtle differences between actions. Namely, one can evaluate the model effective-
ness for high-level video tasks. Recognizing early action accurately requires predictions into future
frames, which can only be achieved using an effective model based on historical observations.
Hyper-parameters and Baselines. We use the architecture illustrated in Figure 1(c) as our model,
which consists of2 layers of 3D-CNN encoders, 4 layers of E3D-LSTMs, and 2 layers of 3D-CNN
decoders. The 3D-CNN encoders take 4 consecutive 224 × 224 raw frames, encode them into
2 × 56 × 56 × 64 feature maps at each time stamp, and then feed them into E3D-LSTM. Each
encoder layer has 64 filters (the filter dimensions are 2 × 5 × 5). We use the same hyper-parameters
for E3D-LSTMs as for video prediction. The decoder layers map the output of E3D-LSTMs back to
RGB space, which is an 1 × 3 matrix, predicting the next frame following the inputs. We train the
network to predict the next 10 frames using the front 25% or 50% frames of the video. Note that we
do not extend any predictive states into the future at the test time. For both training and testing, we
concatenate hidden representations of the top recurrent units with respect to the last 16 input time
stamps (considering the first 25% video snippets usually have about 20 to 30 frames), and feed them
into the classifier for activity recognition. The classifier contains 2 layers of 3D-Convs with 128
filters (filter dimensions: 2 × 3 × 3, filter strides: 2 × 2 × 2) followed by a 2 × 2 × 2 pooling layer.
They transform the concatenated recurrent features from 16 × 56 × 56 × 64 to 1 × 7 × 7 × 128,
9
Published as a conference paper at ICLR 2019
0%
25%
50%
100%
Poking a stack of [sth.] so the stack
collapses
3D-CNN
E3D-LSTM
Poking a StaCk …w/o collapsing
Poking a stack... collapses
Poking a stack. collapses
Poking a stack. collapses
Poking a stack of [sth.] without the
stack collapsing
Putting [sth.] onto [sth.]
Poking a StaCk …w/o collapsing
E3D-LSTM
Poking [sth.] so lightly Poking a StaCk…w/o collapsing
3D-CNN
E3D-LSTM
Pouring [sth.] into [sth.] Pouring [sth.] into [sth.]
Pouring [sth.].overflows Pouring [sth.].overflows
3D-CNN
Pouring [sth.] into [sth.] until it
overflows
3D-CNN
E3D-LSTM
Pouring [sth.] out of [sth.] Pouring [sth.] out of [sth.]
Pouring [sth.] into [sth.] Trying to pour. but spills
Trying to pour [sth.] into [sth.], but
missing so it spills next to it
Figure 6: Early activity recognition results given the first 25% and 50% frames of videos on the
Something-Something validation set. The blue bars indicate making correct classifications and the
red bars are incorrect results. The length of the bar denotes the confidence of the result.
Table 5: Early activity recognition accuracy on the 41-category subset of Something-Something.
Model	Front 25%	Front 50%
3D-CNN	9.11	10.30
Separable-CNN: separable-conv at bottom	8.94	9.62
(2+1)D-CNN: separable-conv on top	9.08	10.17
E(2+1)D-LSTM: s eparab le inside units	12.45	19.86
E3D-LSTM	14.59	22.73
then pass them to a 512-channel fully-connected layer followed by a 41-way classification. We
also exploit the self-supervised auxiliary learning approach and train the model with an objective
function in Equation 4. We set λ(i) in Equation 5 to 10 in the beginning (i = 0), and decrease it
with a speed of 2 × 10-5 per iteration, lower bounded by η = 0.1.
We evaluate the E3D-LSTM model against the state-of-the-art feed-forward 3D-Conv architectures
including C3D/I3D (Diba et al., 2016; Carreira & Zisserman, 2017), Separable 3D-CNN (Xie et al.,
2018; Qiu et al., 2017) and (2+1)D-CNN (Tran et al., 2018). These networks achieve the state-
of-the-art results on the UCF-101 and Kinetics benchmark datasets for action recognition. For fair
comparisons, we train these baseline models using similar backbones to the E3D-LSTM network.
Results. Table 5 shows the classification accuracy of the E3D-LSTM network against the state-
of-the-art feed-forward 3D-CNNs. The E3D-LSTM model performs favorably against the other
methods in two settings of using the first 25% and 50% frames, showing its effectiveness in learning
high-level spatiotemporal representations. Figure 6 shows two pairs of video activities that are easy
to confuse, especially with such limited observations. For instance, our model correctly forecasts
the collapse of books, while only a tendency of it has been shown explicitly within the first 25%
frames. This reasoning ability comes from the integrated design of our model to capture both short-
term motions and long-term dependencies. On the other hand, as the feed-forward 3D-CNN models
long-term relations by sampling and assembling, it does not perform well in finding the temporal
dependencies between cause and effect. We note that Zhou et al. (2018) introduced a feed-forward
CNN model and also reported early recognition results on the same dataset. It is not meaningful
to compare these two methods in terms of accuracy as our model is trained only using 25%-50%
frames of a video instead of the entire video in (Zhou et al., 2018). Moreover, the two methods are
trained using different backbone networks and different splits of datasets.
10
Published as a conference paper at ICLR 2019
Table 6: Ablation study of early activity recognition on the Something-Something dataset.
Model	Front 25%	Front 50%
Baseline 1: 3D-CNN at b ottom (Figure 1 (a))	10.28	1 6.05
Baseline 2: 3D-CNN on top (Figure 1 (b))	9.63	14.82
Baseline 3: Ours w/o 3D convolutions	9.58	13.92
Baseline 4: Ours w/o memory attention	11.39	18.84
E3D-LSTM	14.59	22.73
Table 7: Accuracy comparisons of different training strategies on the Something-Something dataset.
Model	Front 25%	Front 50%
Trained only on the primary classification task	13.78	20.91
Pre-trained on the auxiliary task	14.00	22.15
Trained on both tasks with a fixed loss ratio	13.57	20.46
E3D-LSTM (with self-supervised auxiliary learning)	14.59	22.73
A number of recent studies show that separating temporal and spatial convolution operations in a
3D-CNN model leads to better results (Xie et al., 2018; Qiu et al., 2017; Tran et al., 2018). This
observation is validated by our results shown in Table 5. However, it seems counter-intuitive since
such separation leads to a pseudo-3D convolution, in which spatial and temporal filters are inde-
pendent. Interestingly, such separation in our model leads to performance loss, suggesting the 3D
convolution in the E3D-LSTM jointly captures the temporal and spatial information.
Ablation Study. We conduct similar ablation studies as in Section 4.1 and summarize the results
in Table 6. The results from the first two rows show that our deeper integration of 3D-Convs inside
RNNs is helpful not only for pixel-level video prediction, but also for high-level activity recognition.
The results on rows 3 and 4 show the contribution of the two important components in the proposed
Eidetic 3D LSTM: a) 3D convolution features, and b) memory attention mechanism. Both compo-
nents are useful and important for modeling spatiotemporal data effectively. Table 7 shows applying
self-supervised training in different settings. The proposed self-supervised auxiliary learning ap-
proach performs better than other alternatives, including using video prediction models as network
initialization, or training the model under these two tasks with a fixed objective function ratio.
We enable online early activity recognition by making the classifier only depend on a concatenation
of the last 5 recurrent output states. Using Equation 1, we fix the length of the attended memory
states by setting τ to 5. Such settings are applied to both training and testing. Table 8 shows the
experimental results. Despite the slight decrease of accuracy, it enables an online prediction.
Table 8: Online early recognition accuracy: the classifier is built on the last 5 recurrent output states.
Model	Front 25%	Front 50%
Trained only on the primary classification task	13.49	18.94
E3D-LSTM (with self-supervised auxiliary learning)	14.30	20.85
5 Conclusion
Spatiotemporal predictive learning has shown significant improvements in a variety of applications,
such as weather forecasting, traffic flow prediction, and physical interaction simulation. Although
considered to be a promising self-supervised feature learning paradigm, it seldom shows its effec-
tiveness beyond video prediction. In this paper, we presented the E3D-LSTM model based on 3D
convolutional recurrent units for this task. In this model, we integrated 3D-Convs into state transi-
tions to perceive short-term motions and designed a memory attentive module controlled by recur-
rent gates to capture the long-term video frame interaction. Experimental results demonstrate that
the E3D-LSTM model performs favorably against the state-of-the-art methods on video prediction
and early activity recognition tasks.
Acknowledgments
We would like to thank anonymous reviewers for useful comments. Mingsheng Long was supported
by National Natural Science Foundation of China (61772299, 71690231).
11
Published as a conference paper at ICLR 2019
References
Mardn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In NIPS, 2015.
Yoshua Bengio, J6r6me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
ICML, 2009.
Prateep Bhattacharjee and Sukhendu Das. Temporal coherency based criteria for predicting video
frames using deep multi-stage generative adversarial networks. In NIPS, 2017.
Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics
dataset. In CVPR, 2017.
Bert De Brabandere, Xu Jia, Tinne Tuytelaars, and Luc Van Gool. Dynamic filter networks. In
NIPS, 2016.
Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. In ICML, 2018.
Ali Diba, Ali Mohammad Pazandeh, and Luc Van Gool. Efficient two-stream motion and appearance
3d cnns for video classification. In ECCV Workshop, 2016.
Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction
through video prediction. In NIPS, 2016.
Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne West-
phal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.
The "something something" video database for learning and evaluating visual common sense. In
ICCV, 2017.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computaiton, 9(8):
1735-1780,1997.
Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action
recognition. TPAMI, 35(1):221-231, 2013.
Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex
Graves, and Koray Kavukcuoglu. Video Pixel networks. In ICML, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In ICLR, 2015.
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. A structured self-attentive sentence embedding. ICLR, 2017.
Chaochao Lu, Michael Hirsch, and Bernhard Scholkopf. Flexible spatio-temporal networks for
video Prediction. In CVPR, 2017.
Shugao Ma, Leonid Sigal, and Stan Sclaroff. Learning activity progression in lstms for activity
detection and early detection. In CVPR, 2016.
Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond
mean square error. In ICLR, 2016.
Marc Oliu, Javier Selva, and Sergio Escalera. Folded recurrent neural networks for future video
prediction. In ECCV, 2018.
Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d
residual networks. In ICCV, 2017.
12
Published as a conference paper at ICLR 2019
Christian Schuldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: a local svm
approach. In ICPR, 2004.
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional lstm network: A machine learning approach for precipitation nowcasting. In NIPS,
2015.
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video
representations using lstms. In ICML, 2015.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In NIPS, 2014.
Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-
tiotemporal features with 3d convolutional networks. In ICCV, 2015.
Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer
look at spatiotemporal convolutions for action recognition. In CVPR, 2018.
Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion
and content for video generation. In CVPR, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing motion
and content for natural video sequence prediction. In ICLR, 2017.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.
In NIPS, 2016.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
CVPR, 2018a.
Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and S Yu Philip. Predrnn: Recurrent
neural networks for predictive learning using spatiotemporal lstms. In NIPS, 2017.
Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Philip S Yu. Predrnn++: Towards
a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In ICML, 2018b.
Zhou Wang, A. C Bovik, H. R Sheikh, and E. P Simoncelli. Image quality assessment: from error
visibility to structural similarity. TIP, 13(4):600, 2004.
Nevan Wichers, Ruben Villegas, Dumitru Erhan, and Honglak Lee. Hierarchical long-term video
prediction without supervision. In ICML, 2018.
Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotem-
poral feature learning for video understanding. In ECCV, 2018.
Jingwei Xu, Bingbing Ni, Zefan Li, Shuo Cheng, and Xiaokang Yang. Structure preserving video
prediction. In CVPR, 2018.
Kuo-Hao Zeng, William B Shen, De-An Huang, Min Sun, and Juan Carlos Niebles. Visual forecast-
ing by imitating dynamics in natural sequences. In ICCV, 2017.
Junbo Zhang, Yu Zheng, and Dekang Qi. Deep spatio-temporal residual networks for citywide
crowd flows prediction. In AAAI, 2017.
Bolei Zhou, Alex Andonian, and Antonio Torralba. Temporal relational reasoning in videos. In
ECCV, 2018.
13
Published as a conference paper at ICLR 2019
A Key Equations of S patiotemporal LSTM
Operations inside a Spatiotemporal LSTM unit at time stamp t and layer k are shown as follows:
it = σ(Wχi * Xt + Whi * Hk-ι + bi)
gt = tanh(Wxg * Xt + Whg * Hk-I + bg)
ft = σ(Wxf *Xt +Whf * Htk-1 +bf)
i0t =σ(Wx0i *Xt +Wmi * Mtk-1 +b0i)
gt0 = tanh(Wx0g *Xt + Wmg * Mtk-1 +b0g)
ft0 =σ(Wx0f*Xt+Wmf*Mtk-1+b0f)
Ctk = it gt + ft Ctk-1
Mtk = i0t gt0 + ft0 Mtk-1
ot = σ(Wxo * Xt + Who * Htk-1 + Wco * Ctk + Wmo * Mtk + bo)
Htk = ot	tanh(W1×1 * [Ctk, Mtk]),
(6)
where σ is the sigmoid function, * is the convolution operator, and Θ denotes the Hadamard product.
There are four inputs: Xt, the raw frame or hidden states from the previous layer; Mtk-1, the
previous spatiotemporal memory; Htk-1 and Ctk-1, the previous hidden states and memory states.
Two sets of gate structures, including input gate it and i0t , forget gate ft and ft0 , as well as the
output gate ot , control the information flow in space-time domain. All of them can be presented by
RH×W×C dimensional tensors, where the first two dimensions are the width and height of feature
maps, and the last one is the number of feature map channels.
14