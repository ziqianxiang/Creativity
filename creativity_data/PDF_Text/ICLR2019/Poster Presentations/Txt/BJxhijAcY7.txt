Published as a conference paper at ICLR 2019
signSGD with Majority Vote is Communication
Efficient And Fault Tolerant
Jeremy Bernstein1； JiaWei Zhao12*, Kamyar Azizzadenesheli3, Anima Anandkumar1
1Caltech, 2Nanjing University of Aeronautics and Astronautics, 3UC Irvine
bernstein@caltech.edu, jiaweizhao@nuaa.edu.cn,
kazizzad@uci.edu, anima@caltech.edu
Ab stract
Training neural networks on large datasets can be accelerated by distributing the
workload over a network of machines. As datasets grow ever larger, networks of
hundreds or thousands of machines become economically viable. The time cost
of communicating gradients limits the effectiveness of using such large machine
counts, as may the increased chance of network faults. We explore a particu-
larly simple algorithm for robust, communication-efficient learning—signSGD.
Workers transmit only the sign of their gradient vector to a server, and the overall
update is decided by a majority vote. This algorithm uses 32× less communication
per iteration than full-precision, distributed SGD. Under natural conditions veri-
fied by experiment, we prove that SIGNSGD converges in the large and mini-batch
settings, establishing convergence for a parameter regime of Adam as a byprod-
uct. Aggregating sign gradients by majority vote means that no individual worker
has too much power. We prove that unlike SGD, majority vote is robust when
up to 50% of workers behave adversarially. The class of adversaries we consider
includes as special cases those that invert or randomise their gradient estimate.
On the practical side, we built our distributed training system in Pytorch. Bench-
marking against the state of the art collective communications library (NCCL),
our framework—with the parameter server housed entirely on one machine—led
to a 25% reduction in time for training resnet50 on Imagenet when using 15
AWS p3.2xlarge machines.
1	Introduction
The most powerful supercomputer in the world is currently a cluster of over 27,000 GPUs at Oak
Ridge National Labs (TOP500, 2018). Distributed algorithms designed for such large-scale systems
typically involve both computation and communication: worker nodes compute intermediate results
locally, before sharing them with their peers. When devising new machine learning algorithms for
distribution over networks of thousands of workers, we posit the following desiderata:
D1 fast algorithmic convergence;	D3 communication efficiency;
D2 good generalisation performance;	D4 robustness to network faults.
When seeking an algorithm that satisfies all four desiderata D1-4, inevitably some tradeoff must
be made. Stochastic gradient descent (SGD) naturally satisfies D1-2, and this has buoyed recent
advances in deep learning. Yet when it comes to large neural network models with hundreds of
millions of parameters, distributed SGD can suffer large communication overheads. To make mat-
ters worse, any faulty SGD worker can corrupt the entire model at any time by sending an infinite
gradient, meaning that SGD without modification is not robust.
A simple algorithm with aspirations towards all desiderata D1-4 is as follows: workers send the
sign of their gradient up to the parameter server, which aggregates the signs and sends back only the
majority decision. We refer to this algorithm as signSGD with majority vote. All communication
* JB was primary contributor for theory. JZ was primary contributor for large-scale experiments.
1
Published as a conference paper at ICLR 2019
Number of
workers:
—— 1
——3
9
——27
3.0
2.5
2.0
g 1.0
0⅛
100	200	300	400	500
Time step
Proportion of
adversaries:
——48%
——37%
19%
——4%
——0%
⅛ 1.5
Figure 1: Toy experiments. signSGD with majority vote is run on a 1000-dimensional quadratic
with N (0, 1) noise added to each gradient component. Adversarial experiments are run with 27 total
workers. These plots may be reproduced in a web browser by running this Jupyter notebook.
to and from the parameter server is compressed to one bit, so the algorithm certainly gives us D3.
What’s more, in deep learning folklore sign based methods are known to perform well, indeed
inspiring the popular RMSprop and Adam optimisers (Balles & Hennig, 2018), giving hope for
D1. As far as robustness goes, aggregating gradients by a majority vote denies any individual worker
too much power, suggesting it may be a natural way to achieve D4.
In this work, we make the above aspirations rigorous. Whilst D3 is immmediate, we provide the first
convergence guarantees for signSGD in the mini-batch setting, providing theoretical grounds for
D1. We show how theoretically the behaviour of SIGNSGD changes as gradients move from high to
low signal-to-noise ratio. We also extend the theory of majority vote to show that it achieves a notion
of Byzantine fault tolerance. A distributed algorithm is Byzantine fault tolerant (Blanchard et al.,
2017) if its convergence is robust when up to 50% of workers behave adversarially. The class of
adversaries we consider contains interesting special cases, such as robustness to a corrupted worker
sending random bits, or a worker that inverts their gradient estimate. Though our adversarial model
is not the most general, it is interesting as a model of network faults, and so gives us D4.
Next, we embark on a large-scale empirical validation of our theory. We implement majority vote in
the Pytorch deep learning framework, using CUDA kernels to bit pack sign tensors down to one bit.
Our results provide experimental evidence for D1-D4. Comparing our framework to NCCL (the
state of the art communications library), we were able to speed up Imagenet training by 25% when
distributing over 7 to 15 AWS p3.2xlarge machines, albeit at a slight loss in generalisation.
Finally, in an interesting twist, the theoretical tools we develop may be brought to bear on a seem-
ingly unrelated problem in the machine learning literature. Reddi et al. (2018) proved that the ex-
tremely popular Adam optimiser in general does not converge in the mini-batch setting. This result
belies the success of the algorithm in a wide variety of practical applications. signSGD is equiva-
lent to a special case of Adam, and we establish the convergence rate of mini-batch signSGD for
a large class of practically realistic objectives. Therefore, we expect that these tools should carry
over to help understand the success modes of ADAM. Our insight is that gradient noise distributions
in practical problems are often unimodal and symmetric because of the Central Limit Theorem, yet
Reddi et al. (2018)’s construction relies on bimodal noise distributions.
2	Related Work
For decades, neural network researchers have adapted biologically inspired algorithms for efficient
hardware implementation. Hopfield (1982), for example, considered taking the sign of the synaptic
weights of his memory network for readier adaptation into integrated circuits. This past decade,
neural network research has focused on training feedforward networks by gradient descent (LeCun
et al., 2015). It is natural to ask what practical efficiency may accompany simply taking the sign of
the backpropagated gradient. In this section, we explore related work pertaining to this question.
Deep learning: whilst stochastic gradient descent (SGD) is the workhorse of machine learning
(Robbins & Monro, 1951), algorithms like RMSprop (Tieleman & Hinton, 2012) and Adam
(Kingma & Ba, 2015) are also extremely popular neural net optimisers. These algorithms have their
2
Published as a conference paper at ICLR 2019
Algorithm 1 SIGNUM with majority vote, the proposed algorithm for distributed optimisation. Good
default settings for the tested machine learning problems are η = 0.0001 and β = 0.9, though tuning
is recommended. All operations on vectors are element-wise. Setting β = 0 yields SIGNSGD.
Require: learning rate η > 0, momentum constant β ∈ [0, 1), weight decay λ ≥ 0, mini-batch size
n, initial point X held by each of M workers, initial momentum Vm J 0 on mth worker
repeat
on mth worker
gm J n1 En=I StochasticGradient(X)
vm J (1 - β )ggm + βvm
push sign(vm ) to server
on server
V J PmM=1sign(vm)
push sign(V ) to each worker
on every worker
x J x - η(sign(V ) + λx)
until convergence
. mini-batch gradient
. update momentum
. send sign momentum
. aggregate sign momenta
. broadcast majority vote
. update parameters
roots in the Rprop optimiser (Riedmiller & Braun, 1993), which is a sign-based method similar to
signSGD except for a component-wise adaptive learning rate.
Non-convex optimisation: parallel to (and oftentimes in isolation from) advances in deep learning
practice, a sophisticated optimisation literature has developed. Nesterov & Polyak (2006) proposed
cubic regularisation as an algorithm that can escape saddle points and provide guaranteed conver-
gence to local minima of non-convex functions. This has been followed up by more recent works
such as Natasha (Allen-Zhu, 2017) that use other theoretical tricks to escape saddle points. It is
still unclear how relevant these works are to deep learning, since it is not clear to what extent saddle
points are an obstacle in practical problems. We avoid this issue altogether and satisfy ourselves
with establishing convergence to critical points.
Gradient compression: prior work on gradient compression generally falls into two camps. In the
first camp, algorithms like QSGD (Alistarh et al., 2017), TernGrad (Wen et al., 2017) and Atomo
(Wang et al., 2018) use stochastic quantisation schemes to ensure that the compressed stochastic
gradient remains an unbiased approximation to the true gradient. These works are therefore able
to bootstrap existing SGD convergence theory. In the second camp, more heuristic algorithms like
1BITSGD (Seide et al., 2014) and deep gradient compression (Lin et al., 2018) pay less attention to
theoretical guarantees and focus more on practical performance. These algorithms track quantisation
errors and feed them back into subsequent updates. The commonality between the two camps is an
effort to, one way or another, correct for bias in the compression.
signSGD with majority vote takes a different approach to these two existing camps. In directly em-
ploying the sign of the stochastic gradient, the algorithm unabashedly uses a biased approximation
of the stochastic gradient. Carlson et al. (2016) and Bernstein et al. (2018) provide theoretical and
empirical evidence that signed gradient schemes can converge well in spite of their biased nature.
Their theory only applies in the large batch setting, meaning the theoretical results are less relevant
to deep learning practice. Still Bernstein et al. (2018) showed promising experimental results in the
mini-batch setting. An appealing feature of majority vote is that it naturally leads to compression in
both directions of communication between workers and parameter server. As far as we are aware, all
existing gradient compression schemes lose compression before scattering results back to workers.
Byzantine fault tolerant optimisation: the problem of modifying SGD to make it Byzantine fault
tolerant has recently attracted interest in the literature (Yin et al., 2018). For example, Blanchard
et al. (2017) proposed Krum, which operates by detecting and excluding outliers in the gradient ag-
gregation. Alistarh et al. (2018) propose B yzantineS GD which instead focuses on detecting and
eliminating adversaries. Clearly both these strategies incur overheads, and eliminating adversaries
precludes the possibility that they might reform. El Mhamdi et al. (2018) point out that powerful
adversaries may steer convergence to bad local minimisers. We see majority vote as a natural way to
protect against less malign faults such as network errors, and thus satisfy ourselves with convergence
guarantees to critical points without placing guarantees on their quality.
3
Published as a conference paper at ICLR 2019
Figure 2: Gradient distributions for resnet18 on Cifar-10 at mini-batch size 128. At the start of
epochs 0, 1 and 5, we do a full pass over the data and collect the gradients for three randomly chosen
weights (left, middle, right). In all cases the distribution is close to unimodal and symmetric.
3	Theory
3.1	Assumptions
We aim to develop an optimisation theory that is relevant for real problems in deep learning. For this
reason, we are careful about the assumptions we make. For example, we do not assume convexity
because neural network loss functions are typically not convex. Though we allow our objective
function to be non-convex, we insist on a lower bound to enable meaningful convergence results.
Assumption 1 (Lower bound). For all X and some constant f *, we have objective value f (x) ≥ f *.
Our next two assumptions of Lipschitz smoothness and bounded variance are standard in the stochas-
tic optimisation literature (Allen-Zhu, 2017). That said, we give them in a component-wise form.
This allows our convergence results to encode information not just about the total noise level and
overall smoothness, but also about how these quantities are distributed across dimension.
Assumption 2 (Smooth). Let g(x) denote the gradient of the objective f(.) evaluated at point x.
Then ∀x, y we require that for some non-negative constant L := [L1 , ..., Ld]
My)- f (X) + g(X)T(y - X)] I ≤ 2 XLMyi- Xi)2.
i
Assumption 3 (Variance bound). Upon receiving query X ∈ Rd, the stochastic gradient oracle
gives us an independent, unbiased estimate g that has coordinate bounded variance:
E[g(X)] = g(X),	E [(g(X)i- g(X)i)2] ≤ σi2
for a vector of non-negative constants ~σ := [σ1, .., σd].
Our final assumption is non-standard. We assume that the gradient noise is unimodal and symmetric.
Clearly, Gaussian noise is a special case. Note that even for a moderate mini-batch size, we expect
the central limit theorem to kick in rendering typical gradient noise distributions close to Gaussian.
See Figure 2 for noise distributions measured whilst training resnet18 on Cifar-10.
Assumption 4 (Unimodal, symmetric gradient noise). At any given point X, each component of the
stochastic gradient vector gg(X) has a unimodal distribution that is also symmetric about the mean.
Showing how to work with this assumption is a key theoretical contribution of this work. Combin-
ing Assumption 4 with an old tail bound of Gauss (1823) yields Lemma 1, which will be crucial for
guaranteeing mini-batch convergence of signSGD. As will be explained in Section 3.3, this result
also constitutes a convergence proof for a parameter regime of Adam. This suggests that Assump-
tion 4 may more generally be a theoretical fix for Reddi et al. (2018)’s non-convergence proof of
mini-batch Adam, a fix which does not involve modifying the Adam algorithm itself.
4
Published as a conference paper at ICLR 2019
3.0
2.5
2.0
1.5
1.0
0.5
0 0O 5θ' " 100 "z 150 ^-'200
Epoch
bottom 3 quartiles
⅛nrl «	DoniomjquaEies
--- mean over weights
— max over weights
--- critical SNR
Figure 3: Signal-to-noise ratio (SNR) whilst training resnetl8 on Cifar-IO at batch size 128.
At the start of each epoch we compute the SNR for every gradient component. We plot summary
statistics like the mean over weights and the max. By roughly epoch 40, all gradient components
have passed below the critical line (see Theorem 1) and remain there for the rest of training.
3.2 Mini-Batch Convergence of signSGD
With our assumptions in place, we move on to presenting our theoretical results, which are all proved
in Appendix C. Our first result establishes the mini-batch convergence behaviour of signSGD. We
will first state the result and make some remarks. We provide intuition for the proof in Section 3.3.
Theorem 1 (Non-convex convergence rate of mini-batch signSGD). Run the following al-
gorithm for K iterations under Assumptions 1 to 4: x⅛+ι =第k Sign(Ok). Set the learning
rate, η, and mini-batch size, n, as
rn
/o - ∕*
n = 1.
Let Hk be the set Ofgradient components at step k with large signal-to-noise ratio Sb :=
ie Hk := {dS, > 喘}
.We refer to 衰 as the "critical SNR,. Then we have
κ-ι
⅛∑e ∑ k-il+∑⅛
fc=O i∈ H]z	诲 Hk
< 3
∣∣£∣∣ι(∕o - ∕*)
N
where N = K is the total number of stochastic gradient calls up to step K.
Theorem 1 provides a bound on the average gradient norm. The right hand side of the bound decays
IikeO
,establishing convergence to points of the objective where the gradient vanishes.
Remark 1: mini-batch signSGD attains the same O
non-convex convergence rate as SGD.
Remark 2: the gradient appears as a mixed norm: an 八 norm for high SNR components, and a
weighted ⅛ norm for low SNR Compoenents.
Remark 3: we wish to understand the dimension dependence of our bound. We may simplify
matters by assuming that, during the entire course of optimisation, every gradient component lies in
the low SNR regime. Figure 3 shows that this is almost true when training a resnetl8 model. In
this limit, the bound becomes:
IKT 「d 2 ∙
⅛∑e∑⅛
fc=o Li=ι ,
κ-ι
k=0
d
< 3
ll^l∣ι(∕o - ∕*)
N
Further assume that we are in a well-conditioned setting, meaning that the variance is distributed
uniformly across dimension (σf = ɪ), and every weight has the same smoothness constant {Li =
L). σ2 is the total variance bound, and L is the conventional Lipschitz smoothness. These are the
5
Published as a conference paper at ICLR 2019
quantities which appear in the standard analysis of SGD. Then we get
⅛∑‰lll≤3σ∕l≡ll.
k=0
The factors of dimension d have conveniently cancelled. This illustrates that there are problem
geometries where mini-batch SIgnSGD does not pick up an unfavourable dimension dependence.
3.3 The Subtleties of Mini-Batch Convergence
Intuitively, the convergence analysis of SIgnSGD depends on the probability that a given bit of the
sign stochastic gradient vector is incorrect, or P [sign	≠ sign(^)]. Lemma 1 provides a bound
on this quantity under Assumption 4 (unimodal symmetric gradient noise).
Lemma 1 (Bemstein et al. (2018)). Let .及 be an unbiased stochastic approximation to gradi-
ent component。办 with variance bounded by σf. Further assume that the noise distribution is
unimodal and symmetric. Define signal-to-noise ratio Si :=崂.Then we have that
(3-⅛	矿S, > -⅛,
P[sign(^) ≠ Sign3)] ≤心
[与一戏为	otherwise
which is in all cases less than or equal to ∣.
The bound characterises how the failure probability of a sign bit depends on the signal-to-noise
ratio (SNR) of that gradient component. Intuitively as the SNR decreases, the quality of the sign
estimate should degrade. The bound is important since it tells us that, under conditions of unimodal
symmetric gradient noise, even at extremely low SNR we still have that P [sign ≠ sign (^)] ≤ ɪ.
This means that even when the gradient is very small compared to the noise, the sign stochastic
gradient still tells us, on average, useful information about the true gradient direction, allowing us
to guarantee convergence as in Theorem 1.
Without Assumption 4, the mini-batch algorithm may not converge. This is best appreciated with a
simple example. Consider a stochastic gradient component g with bimodal noise:
〜_ 150 with probability 0.1;
g [ —1 with probability 0.9.
The true gradient g = E回=4.1 is positive. But the sign gradient signɑg) is negative with proba-
bility 0.9. Therefore SIgnSGD will tend to move in the wrong direction for this noise distribution.
Note that SIgnSGD is a special case of the Adam algorithm (Bailes & Hennig, 2018). To see this,
SetPl=P2 = c = 0 in Adam, and the Adam update becomes:
G 9	.
F = F = Tgn(g)
This correspondence suggests that Assumption 4 should be useful for obtaining mini-batch conver-
gence guarantees for Adam. Note that when Reddi et al. (2018) construct toy divergence examples
for Adam, they rely on bimodal noise distributions which violate Assumption 4.
We conclude this section by noting that without Assumption 4, SIgnSGD can still be guaranteed to
converge. The trick is to use a “large“ batch size that grows with the number of iterations. This will
ensure that the algorithm stays in the high SNR regime where the failure probability of the sign bit
is low. This is the approach taken by both Carlson et al. (2016) and Bemstein et al. (2018).
3.4 Robustness of convergence
We will now study signSGD,s robustness when distributed by majority vote. We model adversaries
as machines that may manipulate their stochastic gradient as follows.
6
Published as a conference paper at ICLR 2019
Definition 1 (Blind multiplicative adversary). A blind multiplicative adversary may manipulate
their stochastic gradient estimate gt at iteration t by element-wise multiplying gt with any vector vt
of their choice. The vector vt must be chosen before obsemng gt, so the adversary is iblind,. Some
interesting members of this class are:
(i) adversaries that arbitrarily rescale their stochastic gradient estimate;
(H) adversaries that randomise the sign of each coordinate of the stochastic gradient;
(Hi) adversaries that invert their stochastic gradient estimate.
SGD is certainly not robust to rescaling since an adversary could set the gradient to infinity and
corrupt the entire model. Our algorithm, on the other hand, is robust to all adversaries in this class.
For ease of analysis, here we derive large batch results. We make sure to give results in terms of
sample complexity N (and not iteration number K) to enable fair comparison with other algorithms.
Theorem 2 (Non-convex convergence rate of majority vote with adversarial workers). Run
algorithm 1 for K iterations under Assumptions 1 to 4. Switch off momentum and weight
decay (β = λ = 0). Set the learning rate, η, and mini-batch size, n, for each worker as
Assume that a fraction a < ɪ of the M workers behave adversarially according to Definition
1. Then majority vote converges at rate:
- K—l	-] 2	T	2
⅛Σκlkl11 ≤ ⅛ [l-2αl√M + √—一")
_ fc=O	Jl	j
where N = K2 is the total number of stochastic gradient calls per worker up to step K.
The result is intuitive: provided there are more machines sending honest gradients than adversarial
gradients, we expect that the majority vote should come out correct on average.
Remark 1: if we switch off adversaries by setting the proportion of adversaries Q = O, this result
reduces to Theorem 2 in (Bemstein et al., 2018). In this case, we note the variance reduction
that majority vote obtains by distributing over M machines, similar to distributed SGD.
Remark 2: the convergence rate degrades as we ramp up Q from 0 to ɪ.
Remark 3: from an optimisation theory perspective, the large batch size is an advantage. This is
because when using a large batch size, fewer iterations and rounds of communication are theoreti-
cally needed to reach a desired accuracy, since only √W iterations are needed to reach N samples.
But from a practical perspective, workers may be unable to handle such a large batch size in a
timely manner. It should be possible to extend the result to the mini-batch setting by combining the
techniques of Theorems 1 and 2, but we leave this for future work.
4	Experiments
For our experiments, we distributed SlGNUM (Algorithm 1) by majority vote. SlGNUM is the mo-
mentum Counteipart of signSGD, where each worker maintains a momentum and transmits the
sign momentum to the parameter server at each step. The addition of momentum to signSGD is
proposed and studied in (Bailes & Hennig, 2018; Bemstein et al., 2018).
We built SlGNUM with majority vote in the Pytorch deep learning framework (Paszke et al., 2017)
using the Gloo (2018) communication library. Unfortunately Pytorch and Gloo do not natively
support 1-bit tensors, therefore we wrote our own compression code to bit-pack a sign tensor down
to an efficient 1-bit representation. We obtained a performance boost by fusing together smaller
tensors, which saved on compression and communication costs.
7
Published as a conference paper at ICLR 2019
NCCL
all-reduce
Majority
vote
■ compression on workers
■ gather on server
□ majority vote on server
■ broadcast to workers
□ decompression on workers
0	50	100	150	200	250	300
Time per epoch (seconds)
-65 432
Ooooooo
(SJnoll) LPOdB-Ig QEF
O
Figure 4: Timing breakdown for distributing on the cloud. Left: comparing communication (includ-
ing compression) for training resnet50. Right: comparing communication (including compres-
sion) and computation. resnet50 results use 7 p3.2xlarge machines for training Imagenet,
each at batch size 128. alexnet uses 7 p3.2xlarge machines for Imagenet, each at batch size
64. QRNN uses 3 p3.16xlarge machines for training WikiText-103, each at batch size 240.
SSo- 6u'≡'s,l.L
SSO- 6u'≡'s,l.L
5	10	15	20
Time (hours)
20
40poc
E
Oooon
8 6 4 2
(求)AUeJnUUe 4s4do-L
8060
O O
4 2
(求)AUeJnUUe 4so4'do-L
5	10	15	20
Time (hours)
20
40poc
E
Figure 5: Imagenet comparison of Signum with majority vote and SGD distributed with NCCL.
We train resnet50 on Imagenet distributed over 7 to 15 AWS p3.2xlarge machines. Top:
increasing the number of workers participating in the majority vote shows a similar convergence
speedup to distributed SGD. But in terms of wall-clock time, majority vote training is roughly 25%
faster for the same number of epochs. Bottom: in terms of generalisation accuracy, majority vote
shows a slight degradation compared to SGD. Perhaps a better regularisation scheme can fix this.
'0'0'0'°
Oooo
4 3 2 1
x4-xdjd 6u-eJJ.
0	2	4	6	8 10 12
Epoch
。。。。
Oooo
4 3 2 1
exd」@d⅝L
0	2	4	6	8 10 12
Epoch
Oooo
Oooo
4 3 2 1
-Xdjd 6eJJ.
0	2	4	6
Time (hours)
'0'0'0'°
Oooo
4 3 2 1
exd」@d⅝L
0	2	4	6
Time (hours)
Figure 6: Training QRNN across three p3.16xlarge machines on WikiText-103. Each ma-
chine uses a batch size of 240. For Adam, the gradient is aggregated with NCCL. Signum with
majority vote shows some degradation compared to Adam, although an epoch is completed roughly
three times faster. This means that after 2 hours of training, Signum attains a similar perplexity to
Adam. Increasing the per-worker batch size improved Signum’s performance (see Appendix A),
and increasing it beyond 240 may further improve Signum’s performance. Note: the test perplexity
beats training perplexity because dropout was applied during training but not testing.
8
Published as a conference paper at ICLR 2019
Oooo
8 6 4 2
(求)Aalnue C-EF
IOO
80
(东)>UE□uura ⅝ωπ
Ooo
6 4 2
Majority vote O(Md)
L2QSGD	O(M 2√d log d)
max QSGD O(M d)
For d weights, M machines.
Derivations in Appendix B.
Bits sent per iteration
20
60
80
Figure 7: Left: comparing convergence of majority vote to QSGD (Alistarh et al., 2017).
resnet18 is trained on Cifar-10 across M = 3 machines, each at bach size 128. 1-bit QSGD
stochastically snaps gradient components to {0, ±1}. 2-way refers to the compression function Q(.)
being applied in both directions of communication: machine i sends Q(gi) to the server and gets
Q(PM=I Q(Ii)) sent back. Alistarh et al. (2017) develop a theory for L2 QSGD, but experimen-
tally benchmark max QSGD which has much larger communication costs. For this experiment,
1-bit max QSGD gives roughly 5× more compression than the 32× compression of majority vote,
but this further gain turns out to be small relative to the cost of backpropagation. See Appendix A
for QSGD experiments at higher bit-precision. Right: the table gives a theoretical comparison of
the compression cost of each algorithm—see Appendix B for derivations.
Proportion of
adversaries:
——0%
——14%
29%
——43%
O adversaries
1 adversary
2 adversaries
3 adversaries
MaJOrIty vote： negative attack
O Q 。。
8 6 4 2
AUejn8e C-CF
50	100	150	200
Epoch
Figure 8: Imagenet robustness experiments. We used majority vote to train resnet50 distributed
across 7 AWS p3.2xlarge machines. Adversaries invert their sign stochastic gradient. Left:
all experiments are run at identical hyperparameter settings, with weight decay switched off for
simplicity. The network still learns even at 43% adversarial. Right: at 43% adversarial, learning
became slightly unstable. We decreased the learning rate for this setting, and learning stabilised.
MuItI-KrUm: negative attack
MaJOnty vote： random attack
Qg O 。
8 6 4 2
AUeJnUUe C-CF
50 IOO 150	200
Epoch
50	100	150	200
Epoch
Multi-Krum:
attack
50	100	150	200
Epoch
Figure 9: Comparing the robustness of majority vote to Multi-Krum (Blanchard et al., 2017). We
train resnet18 on Cifar-10 across 7 workers, each at batch size 64. Momentum and weight decay
are switched off for simplicity, and for majority vote we divide the learning rate by 10 at epoch
100. Negative adversaries multiply their stochastic gradient estimate by -10. Random adversaries
multiply their stochastic gradient estimate by 10 and then randomise the sign of each coordinate. For
MULTI-KRUM, we use the maximum allowed security level of f = 2. Notice that MULTI-KRUM
fails catastrophically once the number of adversaries exceeds the security level, whereas majority
vote fails more gracefully.
9
Published as a conference paper at ICLR 2019
We test against SGD distributed using the state of the art NCCL (2018) communication library.
NCCL provides an efficient implementation of allreduce. Our framework is often 4× faster in
communication (including the cost of compression) than NCCL, as can be seen in Figure 4. Further
code optimisation should bring the speedup closer to the ideal 32×.
4.1	Communication Efficiency
We first benchmark majority vote on the Imagenet dataset. We train a resnet50 model and dis-
itribute learning over 7 to 15 AWS p3.2xlarge machines. These machines each contain one
Nvidia Tesla V100 GPU, and AWS lists the connection speed between machines as “up to 10 Gbps”.
Results are plotted in Figure 5. Per epoch, distributing by majority vote is able to attain a similar
speedup to distributed SGD. But per hour majority vote is able to process more epochs than NCCL,
meaning it can complete the 80 epoch training job roughly 25% faster. In terms of overall generali-
sation, majority vote reaches a slightly degraded test set accuracy. We hypothesise that this may be
fixed by inventing a better regularisation scheme or tuning momentum, which we did not do.
In Figure 6 we compare majority vote to Adam (distributed by NCCL) for training QRNN (Brad-
bury et al., 2017) on WikiText-103. Majority vote completes an epoch roughly 3 times faster
than Adam, but it reaches a degraded accuracy so that the overall test perplexity after 2 hours ends
up being similar. In Figure 7 we show that majority vote has superior convergence to the ‘theory’
version of QSGD that Alistarh et al. (2017) develop. Convergence is similar for the ‘max’ version
that Alistarh et al. (2017) use in their experiments. Additional results are given in Appendix A.
4.2	Robustness
In this section we test the robustness of Signum with majority vote to Byzantine faults. Again we
run tests on the Imagenet dataset, training resnet50 across 7 AWS p3.2xlarge machines. Our
adversarial workers take the sign of their stochastic gradient calculation, but send the negation to
the parameter server. Our results are plotted in Figure 8. In the left hand plot, all experiments were
carried out using hyperparameters tuned for the 0% adversarial case. Weight decay was not used
in these experiments to simplify matters. We see that learning is tolerant of up to 43% (3 out of 7)
machines behaving adversarially. The 43% adversarial case was slightly unstable (Figure 8, left),
but re-tuning the learning rate for this specific case stabilised learning (Figure 8, right).
In Figure 9 we compare majority vote to Multi- Krum (Blanchard et al., 2017) with a security
level of f = 2. When the number of adversaries exceeds f, MULTI-KRUM fails catastrophically
in our experiments, whereas signSGD fails more gracefully. Note that Multi- Krum requires
2f + 2 < M, therefore f = 2 is the maximum possible security level for these experiments with
M = 7 workers.
5	Discussion and Conclusion
We have analysed the theoretical and empirical properties of a very simple algorithm for distributed,
stochastic optimisation. We have shown that signSGD with majority vote aggregation is robust
and communication efficient, whilst its per-iteration convergence rate is competitive with SGD for
training large-scale convolutional neural nets on image datasets. We believe that it is important to
understand this simple algorithm before going on to devise more complex learning algorithms.
An important takeaway from our theory is that mini-batch signSGD should converge if the gradient
noise is Gaussian. This means that the performance of signSGD may be improved by increasing
the per-worker mini-batch size, since this should make the noise ‘more Gaussian’ according to the
Central Limit Theorem.
We will now give some possible directions for future work. Our implementation of majority vote
may be further optimised by breaking up the parameter server and distributing it across machines.
This would prevent a single machine from becoming a communication bottleneck as in our experi-
ments. Though our framework speeds up Imagenet training, we still have a test set gap. Future work
could attempt to devise new regularisation schemes for signed updates to close this gap. Promising
future work could also explore the link between signSGD and model compression. Signed updates
force the weights to live on a lattice, facilitating compression of the resulting model.
10
Published as a conference paper at ICLR 2019
Acknowledgments
We would like to thank Yu-Xiang Wang, Alexander Sergeev, Soumith Chintala, Pieter Noordhuis,
Hongyi Wang, Scott Sievert and El Mahdi El Mhamdi for useful discussions.
KA is supported in part by NSF Career Award CCF-1254106. AA is supported in part by a Microsoft
Faculty Fellowship, Google Faculty Award, Adobe Grant, NSF Career Award CCF-1254106, and
AFOSR YIP FA9550-15-1-0221.
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
Communication-Efficient SGD via Gradient Quantization and Encoding. In Advances in Neu-
ral Information Processing Systems (NIPS-17), 2017.
Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine Stochastic Gradient Descent.
arXiv:1803.08917, 2018.
Zeyuan Allen-Zhu. Natasha 2: Faster Non-Convex Optimization Than SGD. arXiv:1708.08694,
2017.
Lukas Balles and Philipp Hennig. Dissecting Adam: The Sign, Magnitude and Variance of Stochas-
tic Gradients. In International Conference on Machine Learning (ICML-18), 2018.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
signSGD: Compressed Optimisation for Non-Convex Problems. In International Conference on
Machine Learning (ICML-18), 2018.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine Learning
with Adversaries: Byzantine Tolerant Gradient Descent. In Advances in Neural Information
Processing Systems (NIPS-17), 2017.
James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-Recurrent Neural
Networks. In International Conference on Learning Representations (ICLR-17), 2017.
Francesco Paolo Cantelli. Sui confini della probabilit. Atti del Congresso Internazionale dei Matem-
atici, 1928.
David Carlson, Ya-Ping Hsieh, Edo Collins, Lawrence Carin, and Volkan Cevher. Stochastic spectral
descent for discrete graphical models. IEEE Journal of Selected Topics in Signal Processing, 10
(2):296-311,2016.
El Mahdi El Mhamdi, Rachid Guerraoui, and Sebastien Rouault. The Hidden Vulnerability of
Distributed Learning in Byzantium. In International Conference on Machine Learning (ICML-
18), 2018.
Carl Friedrich Gauss. Theoria combinationis observationum erroribus minimis obnoxiae, pars prior.
Commentationes Societatis Regiae Scientiarum Gottingensis Recentiores, 1823.
Gloo. Gloo Collective Communications Library, 2018. URL https://github.com/
facebookincubator/gloo. Accessed on 9/27/18.
J J Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the National Academy of Sciences, 1982.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International
Conference on Learning Representations (ICLR-15), 2015.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing
the communication bandwidth for distributed training. In International Conference on Learning
Representations (ICLR-18), 2018.
11
Published as a conference paper at ICLR 2019
NCCL. Nvidia Collective Communications Library, 2018. URL https://developer.
nvidia.com/nccl. Accessed on 9/27/18.
Yurii Nesterov and B.T. Polyak. Cubic Regularization of Newton Method and its Global Perfor-
mance. Mathematical Programming, 2006.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic Differentiation in
PyTorch. In Advances in Neural Information Processing Systems, Autodiff Workshop (NIPS-17),
2017.
Friedrich Pukelsheim. The Three Sigma Rule. The American Statistician, 1994.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the Convergence of Adam and Beyond. In
International Conference on Learning Representations (ICLR-18), 2018.
M. Riedmiller and H. Braun. A Direct Adaptive Method for Faster Backpropagation Learning: the
RPROP Algorithm. In International Conference on Neural Networks (ICNN-93), pp. 586-591.
IEEE, 1993.
Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathe-
matical Statistics, 1951.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-Bit Stochastic Gradient Descent
and Application to Data-Parallel Distributed Training of Speech DNNs. In Conference of the
International Speech Communication Association (INTERSPEECH-14), 2014.
Tijmen Tieleman and Geoffrey Hinton. RMSprop. Coursera: Neural Networks for Machine Learn-
ing, Lecture 6.5, 2012.
TOP500. IBM Summit Supercomputer, 2018. URL https://www.top500.org/system/
179397. Accessed on 9/19/18.
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary B. Charles, Dimitris S. Papailiopoulos, and
Stephen Wright. ATOMO: Communication-efficient Learning via Atomic Sparsification. In Ad-
vances in Neural Information Processing Systems (NIPS-18), 2018.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. TernGrad:
Ternary Gradients to Reduce Communication in Distributed Deep Learning. In Advances in Neu-
ral Information Processing Systems (NIPS-17), 2017.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-Robust Distributed
Learning: Towards Optimal Statistical Rates. In International Conference on Machine Learning
(ICML-18), 2018.
12
Published as a conference paper at ICLR 2019
A Additional experiments
Majority vote
1-bit Max QSGD
—— 2-bit Max QSGD
—— 4-bit Max QSGD
8-bit Max QSGD
Majority vote
1-bit L2 QSGD
——2-bit L2 QSGD
——4-bit L2 QSGD
8-bit L2 QSGD
Figure 11: Signum at varying batch sizes. We use a single worker and train a QRNN model on
WikiText-103. Adam is shown for comparison, at batch size 60. The performance of S ignum
is seen to improve with increasing batch size.
Figure 10: QSGD at varying levels of precision. Top row: L2 QSGD. Bottom row: max QSGD.
resnet18 is trained on Cifar-10 across M = 3 machines, each at bach size 128. The one-way
version of QSGD is used, meaning that the compression function is not re-applied after aggregation.
The comparison is given in terms of number of epochs. A comparison in terms of wall-clock time
will depend on details of the systems implementation.
13
Published as a conference paper at ICLR 2019
B Bits Sent Per Iteration: signSGD vs. QSGD
In this section, we perform theoretical calculations of the number of bits sent per iteration in dis-
tributed training. We compare signSGD using majority vote aggregation to two forms of QSGD
(Alistarh et al., 2017). These calculations give the numbers in the table in Figure 7.
The communication cost of SIGNSGD with majority vote is trivially 2Md bits per iterations, since
at each iteration M machines send d-dimensional sign vectors up to the server, and the server sends
back one d-dimensional sign vector to all M machines.
There are two variants of QSGD given in (Alistarh et al., 2017). The first we refer to as L2 QSGD
which is the version developed in the theory section of (Alistarh et al., 2017). The second we refer
to as max QSGD which is the version actually used in their experiments. For each version we
compute the number of bits sent for the highest compression version of the algorithm, which is a
ternary quantisation (snapping gradient components into {0, ±1}). We refer to this as 1-bit QSGD.
The higher precision versions of QSGD will send more bits per iteration.
1-bit L2 QSGD takes a gradient vector g and snaps ith coordinate gi to sign(gi) with probability
名∣∙ and sets it to zero otherwise. Therefore the expected number of bits set to ±1 is bounded by
E[#bits]=X k⅛=乳 ≤√d.
To send a vector compressed in this way, for each non-zero component 1 bit is needed to send the
sign and log d bits are needed to send the index. Therefore sending a vector compressed by 1-bit L2
QSGD requires at most √d(1 + log d) bits.
In the experiments in Figure 7 we see that the ‘2-way’ version of 1-bit L2 QSGD (which recom-
presses the aggregated compressed gradients) converges very poorly. Therefore it makes sense to
use the 1-way version where the aggregated compressed gradient is not recompressed. A sensible
way to enact this is to have each of the M workers broadcast their compressed gradient vector to all
other workers. This has a cost of (M - 1) Vd(I + log d) bits for each of the M workers, and from
this We get the total cost of O(M2√dlog d) for 1-bit L2 QSGD.
The final algorithm to characterise is 1-bit max QSGD. 1-bit max QSGD takes a gradient vector
g and snaps ith coordinate gi to sign(gi) with probability ɪg^ and sets it to zero otherwise. As
noted in (Alistarh et al., 2017), there are no sparsity guarantees for this algorithm, so compression
will generally be much lower than for 1-bit L2 QSGD.
It is easy to see that 1-bit max QSGD requires no more than O(d) bits to compress a d-dimensional
vector, since 2d bits can always store d numbers in {0, ±1}. To see that we can’t generally do better
than O(d) bits, notice that 1-bit max QSGD leaves sign vectors invariant, and thus the compressed
form of a sign vector requires exactly d bits. The natural way to enact 1-bit max QSGD is with a
two-way compression where the M workers each send an O(d)-bit compressed gradient up to the
server, and the server sends back an O(d)-bit compressed aggregated result back to the M workers.
This gives a number of bits sent per iteration of O(M d).
For very sparse vectors 1-bit max QSGD will compress much better than indicated above. For a
vector g with a single non-zero entry, 1-bit max QSGD will set this entry to 1 and keep the rest zero,
thus requiring only log d bits to send the index of the non-zero entry. But it is not clear whether
these extremely sparse vectors appear in deep learning problems. In the experiments in Figure 7,
1-bit max QSGD led to compressed vectors that were 5× more compressed than SIGNSGD—in our
experimental setting this additional improvement turned out to be small relative to the time cost of
backpropagation.
14
Published as a conference paper at ICLR 2019
C Proofs
C.l Accuracy of the Sign Stochastic Gradient
Lemma 1 (Bemstein et al. (2018)). Let gι be an unbiased stochastic approximation to gradi-
ent component g,, with variance bounded by σf. Further assume that the noise distribution is
unimodal and symmetric. Define signal-to-noise ratio Si :=应.Then we have that
P[sign(^) ≠ Sign3)] ≤
which is in all cases less than or equal to ⅜.
1‹ -
2-9 1-2
V S9 > M`
Othemise
Proof. Recall Gauss, inequality for unimodal random variable X with mode U and expected squared
deviation from the mode τ1 (Gauss, 1823; Pukelsheim, 1994):
P[∣X -z∕∣ > k]<
4 τ2	k 、 2
3殍	叱 〉 句
1 —otherwise
By the symmetry assumption, the mode is equal to the mean, so we replace mean μ = ι/ and variance
σ2=τ2.
P[∣X -μ∖>k] ≤
4 σ2	k 、 2
9 lɪ	廿了>适
1 —otherwise
Without loss of generality assume that gi is negative. Then applying symmetry followed by Gauss,
the failure probability for the sign bit satisfies:
∖gl∖]
if 血〉-
σ ■
otherwise
if¾>⅛
otherwise
IP Sign(%) ≠ Sign g
□
C.2 Mini-Batch Convergence Guarantees
Theorem 1 (Non-convex convergence rate of mini-batch SIGNSGD). Run the following al-
gorithm for K iterations under Assumptions 1 to 4: x⅛+ι =第k Sign(Ok). Set the learning
rate, η, and mini-batch size, n, as
I fo — f*
V ∖∖L∖∖1K
n = 1.
Let Hk be the set Ofgradient components at step k with large signal-to-noise ratio Si :
i.e. Hk ：= {/ S, > 表卜 We refer to 衰 as the 'critical SNR,. Then we have
⅛∑e ∑ k-il+∑⅛
fc=0 i∈ H]z	诲 Hk
IIZlll(∕o - ∕*)
N
where N = K is the total number of stochastic gradient calls up to step K.
15
Published as a conference paper at ICLR 2019
Proof. First let’s bound the improvement of the objective during a single step of the algorithm for
one instantiation of the noise. I[.] is the indicator function, gk,i denotes the ith component of the
true gradient g(xk) and Ok is a stochastic sample obeying Assumption 3.
First take Assumption 2, plug in the algorithmic step, and decompose the improvement to expose
the stochasticity-induced error:
dL
fk+1 - fk ≤ gk (xk+1 - Xk) +	^2(xk+1 - Xk )2
=-ηgT Sign(Ok) + η2 X LL
i=1 2
η2	d
=-ηkgk k1 + -ykL k1 + 2n£ |gk,i| I[sign(gk,i) = sign(gk,i)]
2	i=1
Next we find the expected improvement at time k + 1 conditioned on the previous iterate.
η2	d
E[fk + 1 - fk∖xk ] ≤ -ηkgkk1 + ɪ kL k1 + 2η	|gk,i| P[sign(gk,i) = sign(gk,i)]
i=1
By Assumption 4 and Lemma 1 we have the following bound on the failure probability of the sign:
P[sign(gi) = sign(gi)
if Si > √3,
otherwise
if Si > √3,
otherwise
Substituting this in, we get that
E[fk+ι- fk∣χk] ≤-ηkgkkι + %kLkι + 2η X 愣 + 2η X 扇,小
i∈Hk	i6∈Hk
1 -	|gk,i|
2^2√3σ^,
-η X |gk,i| + η^ kL11l + η X 加3'+ η X |gk,i| - η X √3-
i=1	2	i∈Hk 3	i6∈Hk	i6∈Hk	3σi
-2η X |gk,i| - η X √~ + % kL k1
3 i∈Hk	i6∈Hk	3σi	2
Interestingly a mixture between an `1 and a variance weighted `2 norm has appeared. Now substitute
in the learning rate schedule, and we get:
E[fk+ι-fkixk]≤- f⅛ 13 iXk|gk,i1+√3 Xk T
+ f0 - f
+	2K
≤-ff [Xk^+iXk 号
+ f0 - f
+	2K
Now extend the expectation over the randomness in the trajectory and telescope over the iterations:
fo- f * ≥ fo- E[fκ]
K-1
=E Xfk - fk+1
k=0
≥ / f0- f* X E
≥ V3kLkιK ⅛
X lgk,il + X gσf
i∈Hk	i6∈Hk
f0 - f*
-2-
16
Published as a conference paper at ICLR 2019
Finally, rearrange and substitute in N = K to yield the bound
□
C.3 ROBUSTNESS OF MAJORITY VOTE
Theorem 2 (Non-convex convergence rate of majority vote with adversarial workers). Run
algorithm 1 for K iterations under Assumptions 1 to 4. Switch off momentum and weight
decay (β = λ = 0). Set the learning rate, η, and mini-batch size, n, for each worker as
Assume that a fraction a < ɪ of the M workers behave adversarially according to Definition
1. Then majority vote converges at rate:
- K—l	-] 2	T	2
⅛Σκlkl11 ≤ ⅛ [l-2αl√M + √—一")
_ fc=0	Jl	j
where N = K2 is the total number of stochastic gradient calls per worker up to step K.
Proof. We need to bound the failure probability of the vote. We can then use this bound to derive
a convergence rate. We will begin by showing this bound is worst when the adversary inverts the
signs of the sign stochastic gradient.
Given an adversary from the class of blind multiplicative adversaries (Definition 1), the adversary
may manipulate their stochastic gradient estimate gt into the form vt 0 gt. Here vt is a vector of the
adversary,s choosing, and 0 denotes element-wise multiplication. The sign of this quantity obeys:
Sign(% Θ gt) = SigneJ ㊁ sign®。.
Therefore, the only thing that matters is the sign of vt, and rescaling attacks are immediately nulli-
fied. For each component of the stochastic gradient, the adversary must decide (without observing
gt, since the adversary is blind) whether or not they would like to invert the sign of that component.
We will now show that the failure probability of the vote is always larger when the adversary decides
to invert (by setting every component of Sign(OJ to —1). Our analysis will then proceed under this
worst case.
For a gradient component with true value g, let random variable Z ∈ [O, M] denote the number of
correct sign bits received by the parameter server. For a given adversary, we may decompose Z into
the contribution from that adversary and a residual term ɪ from the remaining workers (both regular
and adversarial):
Z(Sign(O)) = X + H[sign(v)sign(,g) = sign(g)],
where g is the adversary,s stochastic gradient estimate for that component, v is the adversary,s
chosen scalar for that component, and H is the 0-1 indicator function. We are considering Z to be a
function of Sign(O).
But by Assumption 4 and Lemma 1, we see that H[÷l × Sign(O) = sign(g)] is a Bernoulli random
variable with success probability p ≥ ɪ. Onthe other hand, H[— 1 × signɑg) = sign(g)] is a Bernoulli
random variable with success probability g = 1 —p ≤ j.
17
Published as a conference paper at ICLR 2019
The essential quantity in our analysis is the probability that more than half the workers provide the
correct sign bit. But from the preceding discussion, this clearly obeys:
P	Z (+1) ≤ M	=P	X +1[+1 X Sign⑼二	.	，M] 二 sign(g)] ≤ -2
		≤P	X +1[—1 × Sign(O)二	M^ 二 sign(g)] ≤ -2
P Z(T) ≤ -2
As we will see below, the implication of this is that our bounds always worse under the setting
v = -1, and so we will adopt v = -1 hereon. It is worth remarking that blindness in the definition
of blind multiplicative adversaries is important to ensure that I[sign(v)sign(0) = sign(g)] is indeed
a random variable as described above. Were the adversary not blind, then the adversary could in
effect deterministically set Sign(V)Sign⑼.Cooperative adversaries, for example, could use this
power to control the vote.
Now we restrict to our worst case blind multiplicative adversaries, that always choose to invert their
sign stochastic gradient estimate. So, we have (1 - α)M good machines and αM adversaries. The
good workers each compute a stochastic gradient estimate, take its sign and transmit this to the
server. The bad workers follow an identical procedure except they negate their sign bits prior to
transmission to the server. It is intuitive that because the proportion of adversaries α < 2, the good
workers will win the vote on average. To make this rigorous, we will need Lemma 1 and Cantelli’s
inequality. Cantelli (1928) tells us that for a random variable X with mean μ and variance σ2:
P[μ - X ≥N ≤ r⅛
1 + σ2
(1)
For a given gradient component, again let random variable Z ∈ [0, M] denote the number of correct
sign bits received by the parameter server. Let random variables G and B denote the number of good
and bad workers (respectively) who (possibly inadvertently) sent the correct sign bit. Then, letting
P be the probability that a good worker computed the correct sign bit, q :=1 - P and E := P - 2 We
can decompose Z as follows:
Z=G+B
G 〜binomial[(1 — a)M,p]
B 〜binomial[αM, q]
E[Z] = (1 — α)Mp + αMq = ɪ + (1 — 2a)Me
Var[Z] = (1 — α)Mpq + αMpq = M ( — — E2 ʌ.
The vote only fails if Z < M which happens with probability
P Z ≤ — = P E[Z] — Z ≥ E[Z]——
≤
1+
1
(EZ-M)2
VarZ
Var[Z]
≤ 2V (E[Z] — MM)
by Cantelli’s inequality
since 1 + x2 ≥ 2x
2
1 / M (1-e2)
2y (1 — 2α)2M2E2
1 ʌ/姿—1
2 (1 — 2α) √M
18
Published as a conference paper at ICLR 2019
We now need to substitute in a bound on . Assumption 4 and Lemma 1 tell us that
e =1-q ≥[2S- 9 S2	ifS>√3,
2 I -⅛	otherwise.
From this We can derive that 力-1 < S2	as follows. First take the case S ≤	√. Then /	≥	S2
and 4j2 -	1 ≤ S32 — 1 <	S42.	Now take	the case S > √. Then E	≥ 2	-	2 S12	and we	have
8	16 1	8
412 - 1 ≤	S2 1 9 181S6 1	< S2 1 9 1 <	S2 by the condition on S.
4e	S	1- 9 S2 + 81 S4 S	1- 9 S2	S
We have now completed the first part of the proof by showing the key statement that for the ith
gradient component with signal to noise ratio Si := lgil, the failure probability of the majority vote
σi
is bounded by
P [vote fails for ith coordinate] = P Zi ≤ M
V	1
≤ (1 - 2α)√MSi
(?)
The second stage of the proof will proceed by straightforwardly substituting this bound into the
convergence analysis of signSGD from Bernstein et al. (2018).
First let’s bound the improvement of the objective during a single step of the algorithm for one
instantiation of the noise. I[.] is the indicator function, gk,i denotes the ith component of the true
gradient g(xk) and sign(Vk) is the outcome of the vote at the kth iteration.
First take Assumption 2, plug in the step from Algorithm 1, and decompose the improvement to
expose the error induced by stochasticity and adversarial workers:
dL
fk + 1 - fk ≤ gk (Xk+1 - Xk) +	^2"(xk+1 - Xk )2
i=1
dL
=-ηgτ Sign(Vk) + η2 X -2
i=1
η2	d
=-ηkgkk1 + -2l∣Lk1 + 2n£ |gk,i|I[sign(V⅛,2) = sign(gk,i)]
i=1
Next we find the expected improvement at time k + 1 conditioned on the previous iterate.
η2	d
E[fk+1 - fk∣Xk] ≤ -ηkgkk1 + -2k~k1 +2n£ ∣gk,i| P[sign(V⅛,i) = sign(gk,i)]
i=1
From (?), we have that the probability of the vote failing for the ith coordinate is bounded by
P[sign(Vk,i) = sign(gk,i)] ≤ (1- 21√M∣gk,i∣
where σk,2 refers to the variance of the kth stochastic gradient estimate, computed over a mini-batch
of size n. Therefore, by Assumption 3, we have that σk,2 ≤ σ2∕√n.
We now substitute these results and our learning rate and mini-batch settings into the expected
improvement:
E[fk+1 - fk |xk] ≤ -ηkgkk1+√n a -k2α)1√M+ητ kL k1
-fK kgk k1+v
f0 - f k~k1
∣∣Lk1K2 (1 - 2α)√M
+ f0 - f*
+	2K
Now extend the expectation over randomness in the trajectory, and perform a telescoping sum over
the iterations:
19
Published as a conference paper at ICLR 2019
fo- f * ≥ f0- E[fκ]
K-1
= X E[fk - fk+1]
k=0
、KX-1 E	Sf0	- f*	Il II _ 2 S	f0	-	f*	kσk1_f0	-	f*
≥ k=0 EW ^raκkgkk1 - 2V 阿K2 (1- 2α)√M - ~K^
S K (fo - f*) "1 K-IlIIJ	D Sfo- f*	同 1	fo
=V ^TL^E [K k=0 k k「2V E (1 - 2α)√M - ―
We can rearrange this inequality to yield the rate:
K XEkgkkI ≤√k [2(1 -l2α1√M+2 PkLMf0-f*).
K-1
1 - I	1	3 _____
E K Ekgk ∣∣1 ≤ √= 2 PkLkI(f0 - f*) + 2k~k1
K k 0	K 2
Since we are growing our mini-batch size, it will take N = O(K2) gradient calls to reach step K.
Substitute this in on the right hand side, square the result, use that 3 < 2, and We are done:
K-1
衣 X EkgkkI
K
k=0
2
4
≤ √N
kσkι
(1 - 2α)√M
+ VzkLk1(f0- f*)
2
□
20