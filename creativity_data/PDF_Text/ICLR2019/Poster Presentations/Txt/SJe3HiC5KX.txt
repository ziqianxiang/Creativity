Published as a conference paper at ICLR 2019
Learning Factorized Representations
for Open-set Domain Adaptation
Mahsa Baktashmotlagh Masoud Faraki* Tom Drummond* Mathieu Salzmann
University of Queensland Monash University Monash University EPFL
Ab stract
Domain adaptation for visual recognition has undergone great progress in the past
few years. Nevertheless, most existing methods work in the so-called closed-set
scenario, assuming that the classes depicted by the target images are exactly the
same as those of the source domain. In this paper, we tackle the more challeng-
ing, yet more realistic case of open-set domain adaptation, where new, unknown
classes can be present in the target data. While, in the unsupervised scenario,
one cannot expect to be able to identify each specific new class, we aim to au-
tomatically detect which samples belong to these new classes and discard them
from the recognition process. To this end, we rely on the intuition that the source
and target samples depicting the known classes can be generated by a shared sub-
space, whereas the target samples from unknown classes come from a different,
private subspace. We therefore introduce a framework that factorizes the data into
shared and private parts, while encouraging the shared representation to be dis-
criminative. Our experiments on standard benchmarks evidence that our approach
outperforms the state of the art in open-set domain adaptation.
1	Introduction
In many practical machine learning scenarios, the test samples are drawn from a different distribu-
tion from the training ones, due to varying acquisition conditions, such as different data sources,
illumination conditions and cameras, in the context of visual recognition. Over the years, great
progress has been achieved to tackle this problem, known has the domain shift. In particular, many
methods aim to align the source (i.e., training) and target (i.e., test) distributions by learning domain-
invariant embeddings (Pan et al., 2011; Gong et al., 2012; Fernando et al., 2013; Sun et al., 2016),
the most recent approaches relying on deep networks (Ganin & Lempitsky, 2014; Long et al., 2015;
Bousmalis et al., 2016; Tzeng et al., 2017; Long et al., 2016a; Yan et al., 2017).
While effective, these methods work under the assumption that the source and target data contain ex-
actly the same classes. In practice, however, this assumption may easily be violated, as the target data
will often contain additional classes that were not present within the source data. For example, when
training a model to recognize office objects from images, as with the popular Office dataset (Saenko
et al., 2010), one should still expect to see new objects, unobserved during training, when deploying
the model in the real world. While one should not expect the model to recognize the specific class
of such objects, at least in unsupervised domain adaptation where no target labels are provided, it
would nonetheless be beneficial to identify these objects as unknown instead of misclassifying them.
This was the task addressed by Busto & Gall (2017) in their so-called open-set domain adaptation
approach. This method aims to learn a mapping from the source samples to a subset of the target
ones corresponding to those identified as coming from known classes. While reasonably effective,
this procedure involves alternatively solving for the mapping and the assignment of the samples to
known/unknown classes, which, as shown in our experiments, can be costly. Recently, Saito et al.
(2018) introduced a deep learning framework for open-set domain adaptation, relying on adversarial
training to separate the samples from the known classes from the unknown ones.
In this paper, we introduce a novel approach to open-set domain adaptation based on learning a
factorized representation of the source and target data. In essence, we seek to model the samples
*This work was supported by the Australian Research Council Centre of Excellence for Robotic Vision
(project number CE14010006).
1
Published as a conference paper at ICLR 2019
from the known classes with a low-dimensional subspace, shared by the source and target domains,
and the target samples from unknown classes with another subspace, specific to the target domain.
We then make use of group sparsity to encourage each target sample to be reconstructed by only
one of these subspaces, which in turns lets us identify if this sample corresponds to a known or
unknown class. We further show that we can obtain a more discriminative shared representation by
jointly learning a linear classifier within our framework. Ultimately, our approach therefore allows
us to jointly separate the target samples between known and unknown classes and represent the
source and target samples within a consistent, shared latent space. Note that our approach is more
intuitive than (Bousmalis et al., 2016) for the open-set DA scenario in the sense that we model each
target sample as being generated by either the shared subspace or the private one, which is crucial
to identify the target samples depicting unknown classes. By contrast, in (Bousmalis et al., 2016),
each sample is encoded as a mixture of shared and private representations, which does not provide
information to discriminate samples from unknown classes.
We demonstrate the effectiveness of our approach on several open-set domain adaptation bench-
marks for visual object recognition. Our method consistently and significantly outperforms the
technique of Busto & Gall (2017) on all benchmarks, as well as the end-to-end learning approach
of Saito et al. (2018) on the Office dataset, thus showing the benefits of learning shared and private
representations corresponding to the known and unknown classes, respectively. Furthermore, it is
faster than the algorithm of Busto & Gall (2017) by an order of magnitude.
2	Related Work
Domain adaptation for visual recognition has become increasingly popular over the past few years,
in large part thanks to the benchmark Office dataset of Saenko et al. (2010). A natural approach to
tackling the domain shift consists of learning a transformation of the data such that the distributions
of the source and target samples are as similar as possible in the resulting space (Baktashmotlagh
et al., 2014; 2013; Sun et al., 2016). Instead of learning a transformation of the data, other methods
have been proposed to re-weight the source samples, so as to rely more strongly on those that look
similar to the target ones (QUiEonero C. et al., 2009; Gong et al., 2013).
With the advent of deep learning for visual recognition, domain adaptation research also eventually
tUrned to exploiting deep networks. While it was initially shown that deep featUres were more
robUst than handcrafted ones to the domain shift (DonahUe et al., 2013), translating the above-
mentioned distribUtion-matching ideas to end-to-end learning proved even more effective (Tzeng
et al., 2014; Long et al., 2015; 2016b; Rozantsev et al., 2018; SUn & Saenko, 2016). In this context,
other ideas were introdUced, sUch as learning intermediate representations to interpolate between
the soUrce and target domains (Chopra S. & R., 2013; Tzeng et al., 2015), the Use of adversarial
domain classifiers (Ganin & Lempitsky, 2014; Tzeng et al., 2017), and additional reconstrUction
loss terms (Ghifary et al., 2016).
Despite achieving great progress to tackle the domain shift, all the aforementioned methods are
designed for the closed-set scenario, where the soUrce and target data depict the exact same set of
classes. Inspired by recent advances in open-set recognition (Bendale & BoUlt, 2015; Scheirer et al.,
2014), the work of BUsto & Gall (2017) constitUtes the first attempt to address the more realistic
case where the target data contains samples from new, Unknown classes. To achieve this, BUsto
& Gall (2017) proposed to jointly learn the assignments of the target samples to known/Unknown
classes and a mapping from the soUrce data to the target samples depicting known classes. The
resUlting learning problem was solved by alternatively optimizing for the assignments and for the
mapping, which can be costly. Very recently, a deep learning approach was proposed for open-set
domain adaptation (Saito et al., 2018), relying on adversarial training to separate the Unknown target
samples from the known ones.
Here, we introdUce a new solUtion to the open-set domain adaptation problem, where we model the
soUrce and target data with sUbspaces. SUbspace-based representations have proven effective for
domain adaptation (Gong et al., 2012; Gopalan et al., 2014; Fernando et al., 2013). Here, however,
we exploit them in a different manner, based on the intUition that soUrce samples and target samples
from the known classes can be generated by a shared sUbspace, whereas target samples from Un-
known classes come from a private sUbspace. While the notion of shared-private representations has
been exploited in the past, e.g., for mUltiview learning (Jia et al., 2010) and for closed-set domain
adaptation (BoUsmalis et al., 2016), the resUlting techniqUes all Use them to encode each sample as
2
Published as a conference paper at ICLR 2019
a mixture of shared and private information. By contrast, here, we aim to model each target sample
as being generated by either the shared subspace or the private one, which is crucial to identify the
target samples depicting unknown classes.
Our experiments evidence that our open-set domain adaptation approach, based on shared-private
representations, is more effective than the one of Busto & Gall (2017), consistently outperforming
it on several datasets, and also faster by an order of magnitude. We also outperform the recent deep
learning open-set domain adaptation framework of Saito et al. (2018) on the Office benchmark.
3	Our Approach
The key idea behind our formulation is to find low-dimensional representations of the data, factor-
ized into a subspace shared by the source samples and the target ones coming from known classes
and another subspace specific to the target samples from unknown classes. Note that, when refer-
ring to target samples from known classes, we do not mean that these samples are labeled, but rather
that they belong to the same set of classes as the source data. As a matter of fact, throughout the
paper, we focus on the unsupervised domain adaptation scenario, where no target annotations are
provided. In the remainder of this section, we first introduce the optimization problem at the heart
of our approach, and then discuss two extensions of this basic formulation.
3.1	FRODA: Factorized Representations for Open-set Domain Adaptation
Given ns source samples, grouped in a matrix Xs ∈ RD×ns and nt target samples represented by
Xt ∈ RD×nt, our goal is to estimate a low-dimensional representation of each sample, such that
the source and target samples coming from the same classes are generated by a shared subspace,
whereas the target data from new, unknown classes are generated by a different, specific subspace.
To this end, let V ∈ RD×d be the matrix encoding the shared subspace, with d D, and U ∈
RD×d the one representing the private subspace. A naive approach to finding the low-dimensional
representations of the data would involve solving
min	kXt-BTk2F+αkXs-VSk2F ,	(1)
U,T,V,S
where α sets the relative influence of both terms, B = [V , U] ∈ RD×2d, and T and S encode the
low-dimensional representations of the target and source data, respectively. This simple formulation,
however, does not aim to separate the target samples belonging to known classes from the unknown
ones, and thus will represent each target sample as a mixture of shared and private information.
Intuitively, we would rather like each target sample to be generated by either the shared subspace V ,
or the private one U. To address this, we propose to make use of a group sparsity regularizer on the
coefficients of the target samples. Specifically, we split the coefficient vector Ti for target sample i
into a part Tiv that corresponds to the shared subspace and a part Tiu that corresponds to the private
one. We then encourage that either of these two parts goes to zero for each sample. To this end, we
therefore write the optimization problem
nt
min	kXt-BTk2F+αkXs-VSk2F+λ1X(kTivk+kTiuk)
U,T,V,S
i=1
dd
s.t. XkUjk2≤1,XkVjk2≤1,	(2)
where the constraints prevent the basis vectors of the subspaces from growing while the coefficients
decrease (Lee et al., 2007), and where λ1 is a scalar controlling the strength of the group sparsity
regularizer. In essence, this formulation allows each target sample to be reconstructed from either
the shared subspace or the private one, which reduces the influence of the samples from unknown
classes on learning the shared representation.
Optimization. To solve equation 2 efficiently, we alternatively update one variable at a time while
keeping the other ones fixed. Below, we describe the different updates.
3
Published as a conference paper at ICLR 2019
Algorithm 1 : FRODA: Factorized Representations for Open-set Domain Adaptation
Input:
Xs ∈ RD×ns : the source samples
Xt ∈ RD×nt: the target samples
d D: the dimensionality of the subspaces
Output:
S ∈ Rd×ns,T ∈ R2d×nt
Initialize:
V — PCA(Xs)
U J Null(V) (i.e., truncated null space of V)
1:	Compute T from equation 5 by proximal gradient descent
2:	Compute S by solving the linear least-squares problem minS kXs - VS k2F
3:	repeat
4:	Compute U from equation 3 by the Lagrange dual method of (Lee et al., 2007)
5:	Compute V from equation 4 by the Lagrange dual method of (Lee et al., 2007)
6:	Compute T from equation 5 by proximal gradient descent
7:	Compute S by solving the linear least-squares problem minS kXs - VS k2F
8:	until convergence
B-minimization: Given the coefficients S and T, we update the tuple (U, V) by solving
d
mUin kA-UTuk2F s.t. X kUj k2 ≤ 1,	(3)
with A = Xt - VTv, and
d
min kA0-VTvk2F + αkXs - VSk2F s.t. X kVj k2 ≤ 1 ,	(4)
V	j=1
with A0 = Xt - UTu. These two sub-problems can be solved efficiently using the Lagrange dual
formulation introduced in (Lee et al., 2007) to update the basis in a standard sparse coding context.
T -minimization: Minimizing equation 2 with respect to T, with all other parameters fixed, yields
nt
mTin kXt-BTk2F+λ1	(kTivk+kTiuk) ,	(5)
i=1
which can be solved efficiently via the proximal gradient method (Mairal et al., 2014). In other
words, for each target sample, we obtain T using group sparse coding to determine to which repre-
sentation, shared or private, each target sample belongs.
S-minimization: Solving equation 2 with respect to S, with all the other parameters fixed, reduces
to a linear least-squares problem, which has a closed-form solution.
To start optimization, we initialize V as the PCA subspace of the source data, and take U as its
truncated null space. We then obtain the corresponding T and S as described above and start
iterating. The pseudo-code of FRODA is provided in Algorithm 1. The steps are repeated until
convergence, which typically occurs around 50 iterations, with each iteration taking roughly 0.05s.
Inference. After obtaining the final 2d-dimensional representation ofT for the target samples, we
determine if each sample i belongs to known classes or unknown ones based on the coefficients Tiu
and Tiv . More specifically, given a threshold ε, a target sample is assigned to the unknown classes
if kTivk/kTiuk ≤ ε, which suggests that it can be well-reconstructed by the private subspace. In
the presence of C known classes, we then train a (C + 1)-way classifier by augmenting the d-
dimensional source data S, i.e. S ∈ Rd×ns, with the target samples Tu identified as unknown.
4
Published as a conference paper at ICLR 2019
3.2	D-FRODA: DISCRIMINATIVE FRODA
The formulation above does not make use of the source labels at all during the representation learn-
ing stage. As such, it does not encourage the representation to be discriminative. To overcome this,
we extend our basic formulation to further account for the classification task at hand. Specifically,
let L = [l1 . . . lns] ∈ RC×ns be the matrix containing the source labels, where li ∈ RC represents
the one-hot encoding of the label of sample i. We then write our D-FRODA formulation as
nt
U TmVinS W kXt-BTk2F+αkXs-VSk2F+βkL-WSk2F+λ1X(kTivk+kTiuk))
U,T,V ,S,W
i=1
dd
s.t. XkUjk2≤1,XkVjk2≤1,	(6)
j=1	j=1
where W ∈ RC ×d is the matrix containing the parameters of a linear classifier for the source data.
Optimization. To optimize equation 6, we follow a similar alternating strategy as before. The B-
minimization and T -minimization steps are unchanged, but the S-minimization now incorporates a
new term and we further need to solve for the classifier parameters W. This translates to:
S-minimization: Minimizing equation 6 with respect to S, with all the other parameters fixed, still
reduces to a linear least-squares problem. The two terms involving S can be grouped into a single
one of the form ∣∣Xnew - VnewSkF, where Xnew = ('√√βXs) and Vnew = (√βW)，and thus
S can be obtained in closed form.
W -minimization: With all the other parameters fixed, finding W	corresponds to a linear least-
squares problem, with a closed-form solution.
Inference. The same inference strategy as before can be followed to label the target samples.
Another option here is to make use ofW to classify the samples identified as belonging to known
classes. We compare these two strategies in our experiments.
3.3	D-FRODA-U: D-FRODA with Unknown Source Classes
Until now, we have tackled the scenario where there are no unknown classes in the source data,
which we believe corresponds to the typical application scenario, since the source data can in general
be fully annotated. Nevertheless, to match the scenario of Busto & Gall (2017), who assume to have
access to additional source samples from unknown classes, yet different from the target unknown
classes, we introduce a modified version of our approach that takes such auxiliary data into account.
Note that, since one knows which source samples are from unknown classes, it is also possible to
simply discard them from training. To nonetheless handle them, we re-write equation 6 as
mi0nS0	0	kXt-BTk2F+αkX0s-B0S0k2F+	βkL-W	0S0k2F
U,T,V ,U ,S ,W
nt	ns
+ λι X (kTvk + kTuk) + λ2 X (ks0Vk + ∣S0Uk)
i=1	i=1
2d	2d
s.t. XkBjk2≤1,XkB0jk2≤1,	(7)
j=1	j=1
where X0 s contains the source samples from both known and unknown classes, and B0 =
[V , U0] ∈ RD×2d denotes the source transformation matrix with U0 ∈ RD×d the private sub-
space for the source data. Note that, similarly to the target coefficients, we have now separated the
source coefficients for each sample S0i into a part corresponding to the shared subspace S0iv and a
part corresponding to the private one S0iu . Note also that the classifier parameters W 0 now account
for C + 1 classes, the additional class corresponding to the unknown samples.
Optimization. We follow a similar iterative procedure to the one used before, with modifications
to update B0 and S0, as discussed below.
5
Published as a conference paper at ICLR 2019
Table 1: Recognition accuracies on the 12 source/target pairs of the BCIS dataset (Tommasi & Tuytelaars,
2014) using a linear SVM classifier. B: Bing, C: Caltech256, I: ImageNet, S: SUN.
Method		B →	C	B→	I_	B	→S	C	→B	C→I	C → S
TCA(PanetaL,2011)= GFK (Gong et al., 2012) SA (Fernando et al., 2013) CORAL (Sun et al.,2016) ATI (Busto & Gall, 2017) AODA (Saito etal.,2018)		62.8 ± 3.8 66.2 ± 4.0 66.0 ± 3.4 68.8 ± 3.3 71.4 ± 2.3 76.2 ± 1.7		-56.6 ± 4.5 58.3 ± 3.1 57.8 ± 3.2 60.9 ± 2.6 69.0 ± 2.8 70.9 ± 3.2		29.6 ± 4.2 23.8 ± 2.0 24.3 ± 2.6 27.2 ± 3.9 37.4 ± 2.6 57.3 ± 1.1		38.9 ± 1.9 40.2 ± 1.8 40.3 ± 1.7 40.7 ± 1.5 45.7 ± 3.0 63.5 ± 2.1		60.2 ± 1.4 62.2 ± 1.5 62.5 ± 0.8 64.0 ± 2.6 67.9 ± 4.2 73.5 ± 0.8	29.7 ± 1.6 28.5 ± 1.0 29.0 ± 1.5 31.4 ± 0.8 37.5 ± 2.7 60.5 ± 0.8
FRODA D-FRODA		73.8 ± 6.1 74.6 ± 5.5		-71.0 ± 2.0 71.4 ± 2.0		54.7 ± 2.9 55.4 ± 2.7		67.5	± 1.4 67.6	± 1.2		74.5 ± 1.7	61.6 ± 2.2 75.0 ± 1.8 61.7 ± 2.1	
Method		I→B		I→C	I → S-		S→	B	S→C	S→I	Avg.
TCA(Panetal.,2011) 二 GFK (Gong et al., 2012) SA (Fernando et al., 2013) CORAL (Sun et al.,2016) ATI (Busto & Gall, 2017) AODA (Saito et al.,2018)	40.9 ± 2.9 42.6 ± 2.4 43.1 ± 1.6 44.6 ± 2.5 48.8 ± 2.3 66.3 ± 0.9		68.6 ± 1.8 73.3 ± 3.6 72.8 ± 3.1 74.5 ± 3.4 77.5 ± 2.2 78.1 ± 0.9		34.5 ± 3.8 32.7 ± 3.6 32.2 ± 3.7 35.4 ± 4.4 43.4 ± 4.8 59.4 ± 1.4		19.4 ± 2.1 16.9 ± 1.5 17.5 ± 1.6 18.7 ± 1.2 23.2 ± 3.2 56.5 ± 2.6		32.0 ± 3.9 28.6 ± 3.8 29.2 ± 4.2 33.6 ± 5.3 47.3 ± 2.9 59.6 ± 3.1	31.1 ± 4.6 26.4 ± 1.1 27.1 ± 1.3 31.3 ± 1.3 33.0 ± 1.1 63.2 ± 1.3	42 ± 3.04 41.6 ± 2.5 41.8 ± 2.4 44.3 ± 2.7 50.2 ± 2.8 65.4 ± 1.7
FRODA D-FRODA	66.0 ± 1.9 66.4 ± 1.7		79.9 ± 1.7 80.5 ± 1.6		59.2 ± 2.1 59.8 ± 2.0		55.7 ± 2.5 55.5 ± 2.4		61.2 ± 1.8	59.4 ± 1.9 61.2 ± 1.9	59.6 ± 2.2		65.4 ± 2.3 65.7 ± 2.2
S0-minimization: To minimize equation 7 w.r.t. S0, with all other parameters fixed, we write
ns
msin αkX Xs-B0S0 kF + β∣∣L - W0 S 0kF + λ? X (∣∣S0V k + ∣∣S 写|) .	(8)
i=1
The first two terms can be grouped into a single squared Frobenius norm, thus resulting in a sparse
group lasso problem, which, as when updating T in FRODA, can be solved via proximal gradient
descent (Mairal et al., 2014).
B0-minimization: Given the coefficients S0v and S0u, we can update U0 by solving
d
mUi0n kA - U 0S0uk2F s.t.XkU0jk2 ≤ 1,
with A = X0s - V S0v, and V by solving
d
mVin kA0-VCk2F s.t. X kVj k2 ≤1 ,
j=1
(9)
(10)
with A0
Xt - UTu
(√ɑ(x Xs-u 0 S 0u)
and C
As in FRODA, these two sub-problems
can be solved efficiently using the Lagrange dual formulation of Lee et al. (2007).
4	Experiments
We evaluate our approach on the task of open-set visual domain adaptation using two benchmark
datasets, and compare its performance against the state-of-the-art open-set domain adaptation meth-
ods on each dataset.1 Note that we also report the results of the methods used as baselines in (Busto
& Gall, 2017). For a dataset with C source classes, we report the accuracy on C + 1 classes, the
additional one corresponding to the unknown case.
Implementation details. Following Busto & Gall (2017), we represent the source and target sam-
ples with 4096-dimensional DeCAF7 features (Donahue et al., 2013) and first reduce their dimen-
sionality by performing PCA jointly on the source and target data and keeping the components
encoding 99% of the data variance. To then determine the dimensionality d of our shared and pri-
vate subspaces, we make use of the subspace disagreement measure of (Gong et al., 2012). For all
our experiments, the hyperparameters of our approach were set as follows: α = 0.1, β = 0.01,
λ1 = 0.001, λ2 = 0.001 and ε = 0.2. For recognition, for the comparison with (Busto & Gall,
2017) to be fair, we employ a linear SVM classifier in a one-vs-one fashion. Nevertheless, we
also report results obtained with a k-Nearest-Neighbor classifier (with k = 3) and with our linear
classifier with parameters W learnt during training.
1Among the different variants of ATI (Busto & Gall, 2017), we report the best one in each experiment.
6
Published as a conference paper at ICLR 2019
Table 2: Recognition accuracies of variants of our approach using linear SVM, nearest neighbor (NN) and our
linear classifier (W) on the 12 source/target pairs of the BCIS dataset (Tommasi & Tuytelaars, 2014). B: Bing,
C: Caltech256, I: ImageNet, S: SUN.
Method	B → C	B→I	B → S	C→B	C→I	C → S
FRODA-SVM	73.8 ± 6.1	71.0 ± 2.0	54.7 ± 2.9	67.5 ± 1.4	74.5 ± 1.7	61.6 ± 2.2
FRODA-NN	67.7 ± 2.8	64.1 ± 2.4	54.4 ± 3.5	65.1 ± 2.9	72.9 ± 1.7	60.5 ± 2.0
D-FRODA-SVM	74.6 ± 5.5	71.4 ± 2.0	55.4 ± 2.7	67.6 ± 1.2	75.0 ± 1.8	61.7 ± 2.1
D-FRODA-W	61.2 ± 1.2	59.3 ± 1.1	53.3 ± 3.0	63.1 ± 0.9	66.2 ± 1.7	60.2 ± 2.1
D-FRODA-NN	67.7 ± 3.3	63.3 ± 2.8	54.0 ± 3.5	65.4 ± 2.8	73.4 ± 1.5	60.3 ± 2.4
D-FRODA-U-SVM	71.9 ± 3.8	69.2 ± 2.7	56.7 ± 3.3	64.8 ± 1.8	72.8 ± 2.7	59.4 ± 2.3
D-FRODA-U-W	55.9 ± 3.6	55.5 ± 4.5	41.7 ± 4.8	52.6 ± 4.7	61.4 ± 3.6	49.5 ± 4.1
D-FRODA-U-NN	57.6 ± 9.1	52.2 ± 5.0	47.5 ± 6.5	51.8 ± 5.7	64.0 ± 5.7	57.7 ± 5.6
Method	I → B	I→C	I → S	S→B	S→C	S→I	Avg.
FRODA-SVM	66.0 ± 1.9	79.9 ± 1.7	59.2 ± 2.1 二	55.7 ± 2.5	61.2 ± 1.8	59.4 ± 1.9	65.4
FRODA-NN	60.9 ± 3.7	77.7 ± 2.8	58.0 ± 2.2	53.4 ± 2.2	61.2 ± 1.4	58.1 ± 1.5	62.8
D-FRODA-SVM-	66.4 ± 1.7	80.5 ± 1.6	59.8 ± 2.0	55.5 ± 2.4	61.2 ± 1.9	59.6 ± 2.2	65.7
D-FRODA-W	62.2 ± 1.6	70.5 ± 2.5	58.1 ± 1.7	56.4 ± 1.9	58.9 ± 1.5	58.5 ± 0.7	60.7
D-FRODA-NN	60.9 ± 4.1	78.7 ± 2.8	57.7 ± 2.0	53.0 ± 2.3	61.2 ± 1.2	57.9 ± 1.6	62.8
D-FRODA-U-SVM	66.0 ± 1.2	76.8 ± 1.9	57.2 ± 4.5	56.3 ± 1.9	61.8 ± 3.0	59.9 ± 1.7	64.4
D-FRODA-U-W	54.6 ± 4.4	66.2 ± 3.8	43.9 ± 5.4	47.6 ± 3.8	53.5 ± 4.9	51.3 ± 3.7	52.8
D-FRODA-U-NN	58.4 ± 6.1	70.9 ± 4.5	52.8 ± 9.0	55.4 ± 2.0	61.5 ± 2.0	60.1 ± 1.7	57.5
Results on the dense cross-dataset benchmark. We first evaluate our approach on the challeng-
ing cross-dataset benchmark of Tommasi & Tuytelaars (2014). This dataset was built using images
depicting 40 object categories and coming from four datasets, namely Bing (B), Caltech256 (C),
ImageNet (I) and SUN (S), hence referred to as BCIS. Following Busto & Gall (2017), we con-
sider the samples from the first 10 classes as known instances, while the samples with class labels
11,12,…，25 and 26, 27,…，40 are taken to be the unknown samples in the source and target do-
mains, respectively. We follow the unsupervised protocol of Tommasi & Tuytelaars (2014), which
relies on 50 source samples per class and 30 target images per class, except when the target data
is coming from SUN, in which case only 20 images per class are employed. Note that only the
DeCAF7 features are publicly available. To nonetheless evaluate the AODA method of Saito et al.
(2018), we made use of a network taking the DeCAF7 features as input and processing them with
two fully-connected layers, with 1024 and 128 units, respectively, and a final classification layer.
In Table 1, we compare the results of our methods with those of the baselines on all 12 domain pairs
of this dataset. Note that our algorithms (both with and without the discriminative term) outperform
all the baselines, and in particular the state-of-the-art one of Busto & Gall (2017) by a large margin.
For instance, the margin exceeds 32, resp. 26, percentage points when going from SUN to Bing and
ImageNet, respectively. This, we believe, clearly evidences the benefits of our factorized represen-
tations, which allow us to separate the unknown target samples from the ones coming from known
classes, thus yielding a better representation for the known classes.
In Table 2, We compare different versions of our
method, corresponding to using different clas-
sifiers and to using additional unknown source
data. Note that the linear SVM classifier, when
used with our framework, tends to perform the
best, followed by the NN one and finally the
learnt linear classifier. This, we believe, can be
explained by the fact that, while the linear clas-
sifier helps to learn a more discriminative rep-
resentation, it remains less powerful than the
other two classifiers to label the target sam-
ples. Note also that the use of unknown source
data does not consistently help in our frame-
work. Nevertheless, the corresponding results
still outperform those of Busto & Gall (2017).
We further evaluate the robustness of our ap-
proach to the choice of threshold ε to separate the target samples from known/unknown classes. In
Figure 1: Sensitivity to ε
7
Published as a conference paper at ICLR 2019
Table 3: Recognition accuracies on the 6 source/target pairs of the Office dataset (Saenko et al., 2010) using a
linear SVM classifier. A: Amazon, W: Webcam, D: DSLR.
Method	A → D	A → W	W→A	W→D	D→A	D→W	Avg.
LSVM	-726-	-575-	49.2	98.8	45.1	88.5	68.6
DAN (Long et al., 2016a)	77.6	72.5	60.8	98.3	57	88.4	75.8
RTN (Long et al., 2016b)	76.6	73	62.4	98.8	57.2	89	76.2
BP (Ganin & Lempitsky, 2014)	78.3	75.9	64	98.7	57.6	89.8	77.4
ADDA (Tzeng et al., 2017)	52.5	58.3	54.1	89.1	45.3	79.1	63.1
DSN (Bousmalis et al., 2016)	58.3	57.2	55.1	79.3	58.1	70.2	63.0
ATI (Busto & Gall, 2017)	79.8	78.4	76.7	98.8	71.3	94.4	83.2
AODA (Saito et al., 2018)	76.6	74.9	81.2	96.9	62.3	94.6	81.1
FRODA	-88.0-	78.7	76.5	98.0	73.7	94.6	84.9
D-FRODA	87.4	78.1	77.1	98.5	73.6	94.4	84.9
Table 4: Recognition accuracies of variants of our approaches using a linear SVM, nearest neighbor (NN) and
our linear classifier (W) on the 6 source/target pairs of the Office dataset (Saenko et al., 2010).
Method	A → D	A→W	W → A	W→D	D→A	D → W	Avg.
FRODA-SVM	88.0	78.7	76.5	98.0	73.7	94.6	84.9
FRODA-NN	83.9	69.5	75.0	97.7	69.0	83.9	79.8
D-FRODA-SVM	87.4	78.1	77.1	98.5	73.6	94.4	84.9
D-FRODA-NN	83.9	70.1	75.1	96.8	69.2	84.5	79.9
D-FRODA-W	71.1	65.3	68.1	83.0	67.3	79.1	72.3
D-FRODA-U-SVM	81.9	83.5	~^75.5~~	96.2	70.6	94.2	83.7
D-FRODA-U-NN	78.1	72.1	69.1	93.6	67.0	75.7	75.9
D-FRODA-U-W	73.4	65.9	62.8	88.0	61.7	65.1	69.5
Fig. 1, we plot the average accuracy over all 12 pairs of the BCIS dataset as a function of the value of
ε. Note that, once a sufficiently large threshold is reached, the results are quite stable. This indicates
that our algorithm is robust to the specific value of this hyperparameter.
Results on the Office dataset. We further evaluate our approach on the slightly less challeng-
ing, although standard Office benchmark (Saenko et al., 2010). This dataset contains three different
domains, namely Amazon (A), DSLR (D) and Webcam (W), sharing 31 object categories, but dif-
fering in data acquisition process. As in (Busto & Gall, 2017), we take all the samples from the
first 1θ classes to represent the known ones, and allthe samples with class labels 11, l2,…，20 and
21,22,…，31 as unknown source and target data, respectively.
We report the results of our algorithms and of the baselines for all 6 domain pairs of this dataset in
Table 3. As before, note that we outperform the baselines in this open-set scenario. This includes the
end-to-end AlexNet-based approach of Saito et al. (2018) for open-set domain adaptation, as well
as the state-of-the-art UDA methods of Tzeng et al. (2017) and Bousmalis et al. (2016), the latter
of which is closest in spirit to our approach. In Table 4, we compare the different variants of our
approach. The conclusions that one can draw from these results are similar to those for the BCIS
dataset, thus showing that our method generalizes well across different domain adaptation datasets.
To evaluate the robustness of our method to the hyper-parameters α, β, and λ1, in Fig. 2, we plot the
average accuracy of D-FRODA-NN (with k = 3) over all 6 pairs of the Office dataset as a function
of the value of α, β, and λ1. Note that our results are stable for large ranges of these values.
Runtimes. As mentioned in Section 3, one iteration of our approach takes on average 0.05 second,
and our algorithm typically takes around 50 iterations to converge. This yields a total runtime of
roughly 2.5 seconds. By contrast, the publicly available implementation of the method of Busto &
Gall (2017) takes on average 8 seconds per iteration and typically converges in 4 iterations, leading
to a total runtime of roughly 32 seconds. Note that these runtimes were measured on the same
computer and that both methods rely on the same input features. Therefore, our approach is not only
significantly more accurate than (Busto & Gall, 2017), but also faster by an order of magnitude.
8
Published as a conference paper at ICLR 2019
Figure 2: Sensitivity to α, β, and λ1.
Further discussion: The experimental setup used in (Saito et al., 2018; Busto & Gall, 2017) and
our work for open-set DA relies on features extracted using a network pre-trained on ImageNet,
which in fact can be argued to already contain semantic information about some of the unknown
classes. To evidence that our approach does not crucially depend on this information, and thus val-
idate our results, we observed that 4 of the unknown classes in the Office dataset, namely Tape dis-
penser, Stapler, Scissors, Punchers, do not appear in ImageNet. We therefore performed additional
experiments with only these classes as unknown ones and the same 10 known classes as before. We
compared the accuracy of our formulations against the SVM baseline and the open-set ATI method
of Busto & Gall (2017). The gap with respect to both baselines remains large: SVM: 76.01%, ATI:
77.4%, D-FRODA: 78.2%, and FRODA: 78.5%. This confirms that our method applies to truly
never-seen-before classes.
Note that among the 15 unknown classes in the setup for BCIS, 8 of them are not shared with
ImageNet, namely Windmill, Steering wheel, Can-soda, Sneaker, Skyscraper, Ladder, Motorcycle,
and Palm tree. This further confirms that our method handles the cases where no categorical or
semantic information is available in the extracted features.
5	Conclusion
We have introduced a novel approach to open-set domain adaptation, based on the intuition that
source and target samples coming from the same, known classes can be represented by a shared sub-
space, while target samples from unknown classes should be modeled with a private subspace. Each
step of the resulting algorithms can be solved efficiently. As demonstrated by our experiments, our
method outperforms the state of the art in open-set domain adaptation and is one order of magnitude
faster than the technique of Busto & Gall (2017). We believe that this clearly evidences the benefits
of learning factorized representations, which allows us to jointly discard the unknown target samples
and learn a better shared representation. In the future, we will investigate ways to make better use
of unknown source data, and to exploit more effective classifiers, such as SVM, directly within our
D-FRODA formulation.
References
M. Baktashmotlagh, M. Harandi, B. Lovell, and M. Salzmann. Unsupervised domain adaptation by
domain invariant projection. In Proc. Int. Conference on Computer Vision, 2013.
M. Baktashmotlagh, M. Harandi, B. Lovell, and M. Salzmann. Domain adaptation on statistical
manifold. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2014.
A. Bendale and T. Boult. Towards open world recognition. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, 2015.
K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan. Domain separation net-
works. In Proc. Advances in Neural Information Processing Systems, 2016.
P. Busto and J. Gall. Open set domain adaptation. In Proc. Int. Conference on Computer Vision,
2017.
9
Published as a conference paper at ICLR 2019
Balakrishnan S. Chopra S. and Gopalan R. Dlid: Deep learning for domain adaptation by interpo-
lating between domains. In ICML workshop on Challenges in Representation Learning, 2013.
J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep con-
volutional activation feature for generic visual recognition. In Proc. Int. Conference on Machine
Learning, 2013.
B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsupervised visual domain adaptation
using subspace alignment. In Proc. Int. Conference on Computer Vision, 2013.
Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. arXiv preprint
arXiv:1409.7495, 2014.
M. Ghifary, Bastiaan W. Kleijn, M. Zhang, D. Balduzzi, and W. Li. Deep reconstruction-
classification networks for unsupervised domain adaptation. In Proc. European Conference on
Computer Vision, 2016.
B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation.
In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2012.
B. Gong, K. Grauman, and F. Sha. Connecting the dots with landmarks: Discriminatively learn-
ing domain-invariant features for unsupervised domain adaptation. In Proc. Int. Conference on
Machine Learning, 2013.
R. Gopalan, R. Li, and R. Chellappa. Unsupervised adaptation across domain shifts by generating
intermediate data representations. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2014.
Y. Jia, M. Salzmann, and T. Darrell. Factorized latent spaces with structured sparsity. In Proc.
Advances in Neural Information Processing Systems, 2010.
H. Lee, A. Battle, R. Raina, and A. Ng. Efficient sparse coding algorithms. In Proc. Advances in
Neural Information Processing Systems, 2007.
M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation
networks. arXiv preprint arXiv:1502.02791, 2015.
M. Long, J. Wang, and M. Jordan. Deep transfer learning with joint adaptation networks. corr, vol.
arXiv preprint arXiv:1605.06636, 2016a.
M. Long, H. Zhu, J. Wang, and M. Jordan. Unsupervised domain adaptation with residual transfer
networks. In Proc. Advances in Neural Information Processing Systems, 2016b.
J.	Mairal, F. Bach, J. Ponce, G. Sapiro, R. Jenatton, and G. Obozinski. Spams: A sparse modeling
software, v2. 3. URL http://spams-devel. gforge. inria. fr/downloads.html, 2014.
S. Pan, I. Tsang, J. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE
Transactions on Neural Networks, 2011.
J Quinonero C., M. Sugiyama, A. Schwaighofer, and N. Lawrence. Covariate shift by kernel mean
matching. Dataset Shift in Machine Learning, 2009.
A. Rozantsev, M. Salzmann, and P. Fua. Beyond sharing weights for deep domain adaptation. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2018.
K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In
Proc. European Conference on Computer Vision, 2010.
K.	Saito, S. Yamamoto, Y. Ushiku, and T. Harada. Open set domain adaptation by backpropagation.
Proc. European Conference on Computer Vision, 2018.
W. J Scheirer, L. Jain, and T. Boult. Probability models for open set recognition. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2014.
10
Published as a conference paper at ICLR 2019
B. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Proc.
European Conference on Computer Vision, 2016.
B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI Conference
on Artificial Intelligence, 2016.
T. Tommasi and T. Tuytelaars. A testbed for cross-dataset analysis. In Proc. European Conference
on Computer Vision, 2014.
E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Deep domain confusion: Maximizing
for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and
tasks. In Proc. Int. Conference on Computer Vision, 2015.
E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In
Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2017.
H. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo. Mind the class weight bias: Weighted maximum
mean discrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1705.00609, 2017.
11