Published as a conference paper at ICLR 2019
An analytic theory of generalization dynam-
ics and transfer learning in deep linear net-
WORKS
Andrew K. Lampinen
Department of Psychology
Stanford University
lampinen@stanford.edu
Surya Ganguli
Department of Applied Physics
Stanford University
and
Google Brain
sganguli@stanford.edu
Ab stract
Much attention has been devoted recently to the generalization puzzle in deep
learning: large, deep networks can generalize well, but existing theories bounding
generalization error are exceedingly loose, and thus cannot explain this striking
performance. Furthermore, a major hope is that knowledge may transfer across
tasks, so that multi-task learning can improve generalization on individual tasks.
However we lack analytic theories that can quantitatively predict how the degree of
knowledge transfer depends on the relationship between the tasks. We develop an
analytic theory of the nonlinear dynamics of generalization in deep linear networks,
both within and across tasks. In particular, our theory provides analytic solutions
to the training and testing error of deep networks as a function of training time,
number of examples, network size and initialization, and the task structure and
SNR. Our theory reveals that deep networks progressively learn the most important
task structure first, so that generalization error at the early stopping time primarily
depends on task structure and is independent of network size. This suggests
any tight bound on generalization error must take into account task structure,
and explains observations about real data being learned faster than random data.
Intriguingly our theory also reveals the existence of a learning algorithm that
proveably out-performs neural network training through gradient descent. Finally,
for transfer learning, our theory reveals that knowledge transfer depends sensitively,
but computably, on the SNRs and input feature alignments of pairs of tasks.
1	Introduction
Many deep learning practitioners closely monitor both training and test errors, hoping to achieve
both a small training error and a small generalization error, or gap between testing and training
errors. Training is usually stopped early, before overfitting sets in and increases the test error.
This procedure often results in large networks that generalize well on structured tasks, raising an
important generalization puzzle (Zhang et al., 2016): many existing theories that upper bound
generalization error (Bartlett & Mendelson, 2002; Neyshabur et al., 2015; Dziugaite & Roy, 2017;
Golowich et al., 2017; Neyshabur et al., 2017; Bartlett et al., 2017; Arora et al., 2018, e.g) in terms of
various measures of network complexity yield very loose bounds. Therefore they cannot explain the
impressive generalization capabilities of deep nets.
In the absence of any such tight and computable theory of deep network generalization error, we
develop an analytic theory of generalization error for deep linear networks. Such networks exhibit
highly nonlinear learning dynamics (Saxe et al., 2013a;b) including many prominent phenomena
like learning plateaus, saddle points, and sudden drops in training error. Moreover, theory developed
for the learning dynamics of deep linear networks directly inspired better initialization schemes
for nonlinear networks (Schoenholz et al., 2016; Pennington et al., 2017; 2018). Here we show
that deep linear networks also provide a good theoretical model for generalization dynamics. In
particular we develop an analytic theory for both the training and test error of a deep linear network
as a function of training time, number of training examples, network architecture, initialization, and
1
Published as a conference paper at ICLR 2019
task structure and SNR. Our theory matches simulations and reveals that deep networks with small
weight initialization learn the most important aspects of a task first. Thus the optimal test error at the
early stopping time depends largely on task structure and SNR, and not on network architecture, as
long as the architecture is expressive enough to attain small training error. Thus our exact analysis
of generalization dynamics reveals the important lesson that any theory that seeks to upper bound
generalization error based only on network architecture, and not on task structure, is likely to yield
exceedingly loose upper bounds. Intriguingly our theory also reveals a non-gradient-descent learning
algorithm that proveably out-performs neural network training through gradient descent.
We also apply our theory to multi-task learning, which enables knowledge transfer from one task to
another, thereby further lowering generalization error (Dong et al., 2015; Rusu et al., 2015; Luong
et al., 2016, e.g.). Moreover, knowledge transfer across tasks may be key to human generalization
capabilities (Hansen et al., 2017; Lampinen et al., 2017). We provide an analytic theory for how
much knowledge is transferred between pairs of tasks, and we find that it displays a sensitive but
computable dependence on the relationship between pairs of tasks, in particular, their SNRs and
feature space alignments.
We note that a related prior work (Advani & Saxe, 2017) studied generalization in shallow and deep
linear networks, but that work was limited to networks with a single output, thereby precluding the
possibility of addressing the issue of transfer learning. Moreover, analyzing networks with a single
output also precludes the possibility of addressing interesting tasks that require higher dimensional
outputs, for example in language (Dong et al., 2015, e.g.), generative models (Goodfellow et al.,
2014, e.g), and reinforcement learning (Mnih et al., 2015; Silver et al., 2016, e.g).
2	Theoretical framework
We work in a student-teacher scenario in which we consider an ensemble of low rank, noisy teacher
networks that generate training data for a potentially more complex student network, and define the
training and test errors whose dynamics we wish to understand.
2.1	An ensemble of low-rank noisy teachers
We first consider an ensemble of 3-layer linear teacher networks with Ni units in layer i, and weight
matrices W21 ∈ RN2 × N1 and W32 ∈ RN3 × N2 between the input to hidden, and hidden to output
layers, respectively. The teacher network thus computes the composite map y = Wx, where
W ≡ W32 W21. Of critical importance is the singular value decomposition (SVD) of W:
N2
W = USVT = X SauavaT,	(1)
α=1
WhereU ∈ RN3×N 2 and V ∈ RN1 ×N 2 are both matrices with orthonormal columns and S is an
N X N diagonal matrix. We construct a random teacher by picking U and V to be random matrices
with orthonormal columns and choosing O⑴ values for the diagonal elements of S. We work in
the limit Ν1,Ν3 → ∞ with an O(1) aspect ratio A = N3/N1 ∈ (0,1] so that the teacher has fewer
outputs than inputs. Also, we hold N2 〜O(1), so the teacher has a low, finite rank, and we study
generalization performance as a function of the N2 teacher singular values.
We further assume the teacher generates noisy outputs from a set of N1 orthonormal inputs:
yμ = Wxμ + zμ for μ = 1,..., Ni.	(2)
This training set yields important second-order training statistics that will guide student learning:
N1	Nr
∑11 ≡ X xμxμT = i,	∑31 ≡ X yμ^μT = W + ZXT	(3)
μ=1	μ=1
Here the input covariance Σ11 is assumed to be white (a common pre-processing step), the input-
output covariance Σ31 is simplified using (2), and Z ∈ RN3×N1 is the noise matrix, whose μ'th
2
Published as a conference paper at ICLR 2019
column is zμ. Its matrix elements zf are drawn iid. from a Gaussian with zero mean and variance
σ2∕Nι. The noise scaling is chosen so the singular values of the teacher W and the noise Z are both
O(1), leading to non-trivial generalization effects. As generalization performance will depend on
the ratio of teacher singular values to the noise variance parameter σz2 , we simply set σz = 1 in the
following. Thus we can think of teacher singular values as signal to noise ratios (SNRs).
Finally, We note that while We focus for ease of exposition in the main paper on the case of one
hidden layer networks and a full orthonormal basis of P = N training inputs in the main paper,
neither of these assumptions are essential to our theory. Indeed in Section 3.4 and App. A we extend
our theory to networks of arbitrary depth, and in App. G we extend our theory to the case of white
inputs with P = N1, obtaining a good match between theory and experiment in both cases.
2.2	Student training and test error
Now consider a student network with Ni UnitS in each layer. We assume the first and last layers match
the teacher (i.e. N = Ni and N3 = N3) but N ≥ N2, allowing the student to have more hidden
units than the teacher. We also consider deeper students (see below and App. A). Now consider any
student whose input-output map is given by y = W32W21 ≡ Wx. Its training error on the teacher
dataset in (2) and its test error over a distribution of new inputs are given by
PN=I ∣∣wxμ -y"∣∣2	—〈||Wx - y||2〉
pN1ι∣∣yμ∣∣2	,	εtest ≡	〈同 2〉
(4)
respectively. Here xμ and yμ are the noisy training set inputs and outputs in (2), whereas x denotes a
random test input drawn from zero mean Gaussian with identity covariance, yμ = Wxμ is noise
free teacher output, and G denotes an average w.r.t the distribution of the test input x. Due to the
orthonormality of the training and isotropy of the test inputs, both εtrain and εtest can be expressed as
E	ECT	CTECT	E	T——	——T ——
_ TrWTW — 2Tr WTΣ3i + Tr Σ3i TΣ3i	_ Tr WTW — 2Tr WTW + Tr W W
εtrain =	Tr Σ3i TΣ3i	, εtest =	TrWTW	.
(5)
Both εtrain and εtest can be further expressed in terms of the student, training data and teacher SVDs,
which we denote by W = USVT, Σ3i = UgVT, and W = US VT respectively. Specifically,
εtrain
N 3
X ^β
β=1
εtest
∑›β
β=1
N2	N 3	N2 N 3
X Sa + X ^β - 2 XX Sɑ^β (Ua ∙ ^β )(vɑ∙ Vβ)
α=1 β =1	α=1 β =1
"N2	N 2	N2 N 2
X Sa + X Sβ - 2 XX Sas (Ua ∙Uβ )(va∙ vβ)
a=1 β =1	a=1 β =1
(6)
(7)
Thus as the student learns, its training and test error dynamics depends on the alignment of the
time-evolving student singular modes {sa, ua, va} with the fixed training data {^a, ^a, Va} and
teacher {sa, ua, va} singular modes respectively.
3	Single task generalization dynamics: theory and experiment
Here we derive and numerically test analytic formulas for both the training and test errors of a student
network as it learns from training data generated from a teacher network. We explore the dependence
of these quantitites on the student network size, student initialization, teacher SNR, and training time.
3.1	Student training dynamics and training-aligned (TA) networks
We assume the student weights undergo batch gradient descent with learning rate λ on the training
error P* ∣∣yμ-W32W21xμ ||2, which for small λ is well approximated by the differential equations:
3
Published as a conference paper at ICLR 2019
Figure 1: Learning dynamics as
a function of singular dimension
strength. (a) shows how modes
of different singular value are
learned, (b) shows that there is
a wave of learning that picks up
singular dimensions with smaller
and smaller singular values as
t → ∞.
τdW21 = W32T (∑31 - W32W21 Σ11) ,	TdW32 = (∑31 - W32W21 Σ11) W21T,
dt	dt
(8)
(where T ≡ 1∕λ), which must be solved from an initial set of student weights at time t = 0 (Saxe
et al., 2013a). We consider two classes of student initializations. The first initialization corresponds
to a random student where the weights W21 and W32 are chosen such that the composite map
W = W32W21 has an SVD W = UVT, where U and V are random singular vector matrices
and all student singular values are . As such a random student learns, the composite map undergoes
a time dependent evolution W(t) = U(t)S(t)V(t)T = PαN=2 1 sα(t)uα(t)vα(t)T. For white inputs,
as t → ∞, W → Σ31, and so the time-dependent student singular modes {sα(t), uα(t), v(t)}
converge to the training data singular modes {Sα, ^α, vα }. However, the explicit dynamics of the
student singular modes can be difficult to obtain analytically from random initial conditions.
Thus we also consider a special class of training aligned (TA) initial conditions in which W21 and
W32 are chosen such that the composite map W = W32 W21 has an SVD W = eUJπVT. That is,
the TA network (henceforth referred to simply as the TA) has the same singular vectors as the training
data covariance Σ31, but has all singular values equal to . As shown in (Saxe et al., 2013a), as the
TA learns according to (8), the singular vectors of its composite map W remain unchanged, while
the singular values evolve as Sa(t) = s(t, Sa), where the learning curve function s(t, S) as well as its
functional inverse t(s, S) is given by
S(t, S)=
Se2^t∕τ
e2St/T — 1 + S/e，
τ τ Tl S/e ― 1
t(S，S) = 2^ln S/S-I.
(9)
Here the function S(t, S) describes analytically how each training set singular value ^ drives the
dynamics of the corresponding TA singular value S, and for notational simplicity, we have suppressed
the dependence of S(t, S) on T and the initial condition e. As shown in Fig. 1A, for each S, S(t, S)
is a sigmoidal learning curve that undergoes a sharp transition around time t∕τ =表 ln(S∕e — 1),
at which it rises from its small initial value of e at t = 0 to its asymptotic value of ^ as t∕τ → ∞.
Alternatively, we can plot S(t, S)∕S as a function of S for different training times t∕τ, as in Fig. 1B.
This shows that TA learning corresponds to a singular mode detection wave which progressively
sweeps from large to small singular values. At any given training time t, training data modes with
singular values S > t∕τ have been learned, while those with singular values S < t∕τ have not.
While the TA is more sophisticated than the random student, since it already knows the singular
vectors of the training data before learning, we will see that the analytic solution for the TA learning
dynamics provides a good approximation to the student learning dynamics, not only for the training
error, as shown in (Saxe et al., 2013a), but also for the generalization error as shown below.
The results in this section assume a single hidden layer, but Saxe et al. (2013a) derived t(S, S) for
networks of arbitrary depth and we apply our theory to some deeper networks. The general differential
equation and derivations for deeper networks can be found in Appendix A.
3.2	How the teacher is buried in the training data: a random matrix analysis
In the previous section, we reviewed an exact analytic solution for the composite map ofaTA network,
namely that its singular modes are related to those of the training data through the relation
Sa (t) = S(t,Sa),	Ua (t) = ^a,	Vv (t) = Va.	(10)
However, computation of the generalization error through (5) then requires understanding how the
teacher singular modes of W are buried within the noisy training data singular modes of Σ31 through
4
Published as a conference paper at ICLR 2019
Figure 2: The teacher’s signal through the noise. Theoretical vs. empirical (a) histogram of singular
values of noisy teacher S. (b) S as a function of S. (C) alignment of noisy teacher and noiseless teacher
singular vectors as a function of S. (N1 = N3 = 100.)
the relation (3). Since the input matrix X is orthonormal, Σ31 is simply a perturbation of the low
rank teacher W by a high dimensional noise matrix Z. The relation between the singular modes of a
low rank matrix and its noise perturbed version has been studied extensively in Benaych-Georges &
Nadakuditi (2012), in the high dimensional limit we are working in, namely N1, N3 → ∞ with the
aspect ratio A = N3/N1 ∈ (0,1], and N2 〜O⑴.
In this limit, the top N2 singular values and vectors of Σ31 converge to S(Sa), where the transfer
function from a teacher singular value S to a training data singular value S is given by the function
“S)= ((S)T p(1+ s2)(A + s2)if S > A1/4	(In
[1 + √A	otherwise.
The associated top N2 singular vectors of Σ31 can also acquire a nontrivial overlap with the N2
modes of the teacher through the relation |ua ∙ ua | ∣va ∙ va | = O(Sa), where the singular vector
overlap function is given by
r h1	_	A(1+S2) i	1/2 h1 _	(A+S2) i	1/2	if Q、A1/4
O (S) =	1 [1	-	s2 (A+s2)	[1 -	s2 (1+s2)	if S > A	(12)
0	otherwise
The rest of the N3 - N2 singular vectors of Σ31 are orthogonal to the top N2 ones, and their singular
values are distributed according to the the Marchenko-Pastur (MP) distribution:
4A-(S2-(1+A 疥
P (^) =	∏As
0
S ∈ [1 -√A, 1 + √A]
otherwise.
(13)
Overall, these equations describe a singular vector phase transition in the training data, as illustrated
in Fig. 2BC. For example in the case of no teacher, the training data is simply noise and the singular
values of Σ31 are distributed as an MP sea spread between 1 ± AJk. When one adds a teacher, how
each teacher singular mode is imprinted on the training data depends crucially on the teacher singular
value S, and the nature of this imprinting undergoes a phase transition at S = A1/4. For S ≤ A1/4, the
teacher mode SNR is too low and this mode is not imprinted in the noisy training data; the associated
training data singular value S remains at the edge of the MP sea at 1 + ∖TA, and the overlap O(S)
between training and teacher singular vectors remains zero.
However, when S > A1/4, this teacher mode is imprinted in the training data; there is an associated
training data singular value ^ that pops out of the MP sea (Fig. 2AB). However, the training data
singular value emerges at a position S > S that is inflated by the noise, though the inflation effect
decreases at larger S, with the ratio S/S approaching the unity line as S becomes large (Fig. 2B).
Similarly, the corresponding training data singular vectors acquire a non-trivial overlap with the
teacher singular vectors when S > A1/4, and the alignment approaches unity as S increases (Fig. 2C).
3.3	Putting it all together: an analytic theory of generalization dynamics
Based on an analytic understanding of how the singular mode structure {Sα, Ua, Va } of the teacher
W is imprinted in the modes {Sa, Ua, Va } of the training data covariance Σ31 through (11), (12) and
(13), and in turn how this training data singular structure drives the time evolving singular modes of a
5
Published as a conference paper at ICLR 2019
Figure 3: Match between theory and experiment for rank 1 (row 1, a-d) and rank 3 (row 2, e-h)
teachers with single-hidden-layer students: (a-b, e-f) log train and test error, respectively, showing
very close match between theory and experiment for TA, and close match for the random student.
(c,g) comparing TA and randomly initialized students minimum generalization errors, showing almost
perfect match. (d,h) comparing TA and randomly initialized students optimal stopping times, showing
small lag due to alignment. (N1 = 100, N2 = 50, N3 = 50.)
D (∙PUEH) I/⅞
0.2	0.3	0.4	0.5
%pt∕τ (TA)
TA network {sα (t), ^α, ^α } of through (9), We Can now derive analytic expressions for εtrain and εtest
in (6) and (7), for a TA network. We will also show that these learning curves closely approximate
those of a random student with time-evolving singular vectors {uα (t), vα (t)}, and match on several
key aspects. First, inserting the TA dynamics in (10) into εtrain in (6), we obtain
εtrain (t) =
N 3
X ^α
α=1
-1
N 2
(N3-N2)hs2iRout +(N2 -N2)h(s(s, t) - s)2iRin + X [sɑ⑴一&a]2
α=1
(14)
Here, Sa(t) = s(Sα,t) as defined in (9) are the TA singular values, and ^α = s(sα) as defined in
(11) are the training data singular values associated with the teacher singular values Sa. Also〈)R
denotes an average with respect to the MP distribution in (13) overa. region R. Two distinct regions
contribute to training error. First Rin contains those top N2 — N2 training data singular values
that do not correspond to the N2 singular values of the teacher but will be learned by a rank N2
student. Second, Rout corresponds to the remaining N3 - N2 lowest training data singular values
that cannot be learned by a rank N2 student. In terms of the MP distribution, Rout = [1 — ∖∕A, f ]
and Rin = [f, 1 + ∖∕A], where f is the point at which the MP density has 1 — N2 /N3 of its mass to
the left and N2 /N3 of its mass to the right. In the simple case of a full rank student, f = 1 — ∖TA, and
one need only integrate over Rin which is the entire range. Equation (14) for εtrain makes it manifest
that it will go to zero for a full rank student as its singular values approach those of the training data.
Of course the test error can behave very differently. Inserting the TA training dynamics in (10) into
εtest in (7), and using (11), (12) and (13) to relate training data to the teacher, we find
εtest (t) =
N 2
X Sa
a=1
-1
(N2-N 2 Ns(S,t)2 iRin +
N 2
X [(Sa(t) - Sa)2 + 2Sa(t)Sa(1 - O(Sa))]
a=1
(15)
Together (14) and (15) constitute a complete theoryj□f generalization dynamics in terms of the
structure of the data distribution (i.e. the teacher rank N2, teacher SNRs {sa }, and the teacher aspect
ratio A = N3/N 1), the architectural complexity of the student (i.e. its rank N2, its number of layers
Nl , and the norm of its initialization), and the training time t. They yield considerable insight into
the dynamics of good generalization early in learning and overfitting later, as we show below.
6
Published as a conference paper at ICLR 2019
B
Fol
U
^ -2
60
t∕τ
120
SNR
■ 10
—Theory
Empirical
■ TA
SNR
I
____ʌ
f -
I...../	Em
60
t∕τ
10
-Theory
Empirical
120 β TA
-5	-4	-3	-2	-1
min In εtest (TA)
D ( ∙PUEH) I/⅞H ( ∙PUECC)401
ɪopt/ɪ (TA)
4	6	8
ɪopt/ɪ (TA)
77/2
V√2
Figure 4: Our theory applies to deeper networks: match between theory and simulation for rank 1
(row 1, a-d) and rank 3 (row 2, e-h) teachers with nl = 5 students: (a-b, e-f) log train and test error,
respectively, showing very close match between theory and experiment for TA. (c,g) comparing TA
and randomly initialized students minimum generalization errors, showing almost perfect match.
(d,h) comparing TA and randomly initialized students optimal stopping times, showing large lag due
to slower alignment in deeper networks. (N1 = 100, N2 = 50, N3 = 50.)
3.4	Numerical tests of the theory of neural network generalization dynamics
Fig. 3 demonstrates an excellent match between the theory and simulations for the TA, and a close
match for random students, for single-hidden-layer students and various teacher ranks N2. Intuitively,
as time t proceeds, learning corresponds to singular mode detection wave sweeping from large to
small training data singular values (i.e. the wave in Fig. 1B sweeps across the training data spectrum
in Fig 2A). Initially, strong singular values associated with large SNR teacher modes are learned
and both εtrain and εtest drop. Fig. 3A-D are for a rank 1 teacher, and so in Fig 3AB we see a single
sharp drop early on, if the teacher SNR is sufficiently high. By contrast, with a rank 3 teacher in Fig.
3E-H, there are several early drops as the three modes are picked up. However, as time progresses,
the singular mode detection wave penetrates the MP sea, and the student picks up noise structure in
the data, so εtrain drops but εtest rises, indicating the onset of overfitting.
The main difference between the random student and TA learning curves is that the random student
learning is slightly delayed relative to the TA, especially late in training. This is understandable
because the TA already knows the singular vectors of the training data, while the random student must
learn them. Nevertheless, two of the most important aspects of learning, namely the optimal early
stopping time togrpatdient ≡ argmintεtest(t) and the minimal test error achieved at this time εgorpatdient ≡
mintεtest(t), match well between TA and random student, as shown in Fig. 3CD. At low teacher
SNRs, the student takes a little longer to learn than the TA, but their optimal test errors match.
Our theory can also be easily extended to describe the learning dynamics deeper networks. Saxe
et al. (2013a) derived t(s, ^) for networks of arbitrary depth, so We only need to adjust this factor
in our formulas, see App. A for details. In Fig. 4 we show that again there is an excellent match
between TA networks and theory for student networks with Nl = 5 layers (i.e. 3 hidden layers).
Randomly-initialized networks show a much longer alignment lag for deeper networks (see App.
B for details), but the curves are qualitatively similar and optimal stopping errors match. We also
demonstrate extensions of our theory to different numbers of training examples (App. G).
Importantly, many of the phenomena we observe in linear networks are qualitatively replicated
in nonlinear networks (Fig. 5), suggesting that our theory may help guide understanding of the
nonlinear case. In particular, features such as stage-like initial learning, followed by a plateau if SNR
is high, and finally followed by overfitting, are replicated. However, there are some discrepancies, in
particular nonlinear networks (especially deeper ones) begin overfitting earlier than linear networks.
This is likely because a mode in a non-linear network can be co-opted by an orthogonal mode,
while in a linear network it cannot. Thus noise modes are able to “stow away” on the strong signal
modes once they are learned. However, overall learning patterns are similar, and we show below that
7
Published as a conference paper at ICLR 2019
many interesting phenomena in nonlinear networks are understandable in the linear case, such as the
(non-)effects of overparameterization, the dynamics of memorization, and the benefits of transfer.
N2 = 3, Nl = 5
N 2 = 1, Nl = 3
N2 = 3, Nl = 3	N2 = 1, Nl = 5
Figure 5: Train (first row, A-D) and test (second row, E-H) error for nonlinear networks (leaky relu at
all hidden layers) with one hidden layer (first two columns) or three hidden layers (last two columns)
trained on the tasks above, with a rank 1 teacher (first and third columns) or a rank 3 teacher (second
and fourth columns). Note that many of the qualitative phenomena observed in linear networks, such
as stage-like improvement in the errors, followed by a plateau, followed by overfitting, also appear in
nonlinear networks. Compare the first column to Fig. 3AB, the second column to Fig. 3EF, the third
to Fig. 4AB, and the fourth to Fig. 4EF. (N1 = 100, N2 = 50, N3 = 50.)
3.5	Randomized data vs. real data: a learning time puzzle
An intriguing observation that resurrected the generalization puzzle in deep learning was the ob-
servation by Zhang et al. (2016) that deep networks can memorize data with the labels randomly
permuted. However, as Arpit et al. (2017) pointed out, the learning dynamics of training error for
randomized labels can be slower than than for structured data. This phenomenon also arises in
deep linear networks, and our theory yields an analytic explanation for why. We randomize data by
choosing orthonormal inputs Xμ as in the structured case, but We choose the outputs yμ to be i.i.d.
Gaussian with zero mean and the same diagonal variance as the structured training data generated by
the teacher. For structured data generated by a low rank teacher with singular values Sa, the diagonal
output variance is given by σ2 = N [PN=2α 反]+ N σ2, where σz is the noise variance, as before.
Since there is no relation between input and output, Σ31 is now distributed as a MP distribution
whose support is [(σr(1 - vzA), σr(1 + AA)]. Thus randomization essentially destroys the outlier
signal singular values in Σ31 reflecting the teacher, and distributes them across all randomized data
Figure 6: Learning randomized data: Comparing (a) singular value distributions and (b) learning
curves for data with a signal vs. random data that preserves basic statistics (mean, variance).
Randomizing the data dilutes the signal singular values, spreading their variance out over many
modes, hence randomly labelled data is learned more slowly. (N1 = 100, N2 = 50, N3 = 50.)
8
Published as a conference paper at ICLR 2019
Relative signal
strength?
Task2
o outputs
Task1
outputs ʌ
Figure 7: Transfer setting- If two different tasks are combined, how well students of the combined
teacher perform on each task depends on the alignment and SNRs of the teachers.
Multi-task outputs
IID Gaussian
noise (fixed
across training)
炉LW
N1 inputs
Training
Multi-task teacher
Student
Figure 8: Transfer benefit TAJB (SA, SB, q) plotted at different values of sA. (a) SA = 0.84 = √A.
Although this task is impossible to learn on its own, with support from another aligned task, especially
one with high SNR, learning can occur. (b) SA = 3. Tasks with modest signals will face interference
from poorly aligned tasks, but benefits from well aligned tasks. These effects are amplified by
SNR. (c) SA = 100. Tasks with very strong signals will show little effect from other tasks (note
y-axis scales), but any impact will be negative unless the tasks are very well aligned. (N1 = 100
N A = N B = 1, N = 50, N = 50.)
10010VARmn
⅝n-.
modes, yielding this stretched MP distribution (compare 6A top and bottom). However, even on this
stretched MP distribution, the right edge will be much smaller than the signal singular values, since
the signal variance will be diluted by spreading it out over many more modes in the randomized data.
Thus the randomized data will lead to slower initial training error drops relative to the structured
data (Fig. 6B) since the singular mode detection wave encounters the first signal singular values in
structured data earlier than it encounters the edge of the stretched MP sea in randomized data.
3.6	Out-performing optimal early stopping through a non-gradient algorithm
For the case of a rank 1 teacher, it is straightforward to derive a good analytic approximation to the
important quantities εgorpatdient and togrpatdient. We assume the teacher SNR is beyond the phase transition
point so its unique singular value S1 > A1/4, yielding a separation between the training data singular
value S1 in (11) and the edge of the MP sea. In this scenario, optimal early stopping will occur at a
time before the detection wave in Fig. 1B penetrates the MP sea, so to minimize test error, we can
neglect the first term in (15). Then optimizing the second term yields the optimal student singular
value S1 = S10(S1). Inserting this value into (15) yields εoptdient = 1 - O(S1)2, and inserting it into
(9) yields togrpatdient. Thus the optimal generalization error with a rank 1 teacher is very simply related
to the alignment of the top training data singular vectors with the teacher singular vectors, and it
decreases as this alignment increases. In App. E, we show this match in the rank 1 case.
With higher rank teachers, εgorpatdient and tgorpatdient must negotiate a more complex trade-off between
teacher modes with different SNRs. For example, as the singular mode detection wave passes the top
training data singular value, S1 (t) → S1 which is greater than the optimal S1 = S10(S1) for mode 1.
Thus as learning progresses, the student overfits on the first mode but learns lower modes. However,
this neural generalization dynamics suggests a superior non-gradient training algorithm that simply
9
Published as a conference paper at ICLR 2019
optimally sets each Sa to SaO(Sa) in (15), yielding an optimal generalization error:
εopt
εnon-gradient
N 2
X Sa
a=1
N 2
X Sa(I-O(Sa)2)
a=1
(16)
Standard gradient descent learning cannot achieve this low generalization error because it cannot
independently adjust all student singular values. A simple algorithm that achieves εonpotn-gradient is as
follows. From the training data covariance Σ31, extract the top singular values Sa that pop-out of the
MP sea, use the functional inverse of (11) to compute Sa(Sa), use (12) to compute the optimal Sa,
and then construct a matrix W with the same top singular vectors as Σ31, but with the outlier singular
values shrunk from ^a to Sa and the rest set to zero. This non-gradient singular value shrinkage
algorithm provably outperforms neural network training with εonoptn-gradient ≤ εgorpatdient.
4	A theory for the transfer of knowledge acros s multiple tasks
Consider two tasks A and B, described by N 3 by N1 teacher maps WA and W5 * * B, of ranks N A and
NB, respectively. Now two student networks can learn from the two teacher networks separately,
each achieving optimal early stopping test errors εoApt and εoBpt. Alternatively, one could construct a
composite teacher (and student) that concatenates the hidden and output units, but shares the same
input units (Fig. 7). The composite student and teacher each have two heads, one for each task, with
N3 neurons per head. Optimal early stopping on each head of the student yields test errors εA[B
and εBpLa. We define the transfer benefit that task B confers on task A to be TAJB ≡ εApt - εALB.
A postive (negative) transfer benefit implies learning tasks A and B simultaneously yields a lower
(higher) optimal test error on task A compared to just learning task A alone.
A foundational question is how the transfer benefit TAJB depends on the two tasks defined by
the teachers W and W . To answer this, consider the SVDS of each teacher alone: W =
UASAVAT and WB = UBSB VBT .From the above, we know that εA depends on WA only
through SA. In App. D we show that the transfer benefit depends on both WA and WB only through
SA, SB, and the NA by N B similarity matrix Q = VA VB. If we think of the columns of each V
as spanning a low dimensional feature space in N1 dimensional input space that is important for each
task, then Q reflects the input feature subspace similarity matrix. Interestingly, the transfer benefit
is independent of output singular vectors U and UB. What matters for knowledge transfer in this
setting are the relevant input features, not how you must respond to them.
We describe the transfer benefit for the simple case of two rank one teachers. Then SA, SB, and Q
are simply scalars sa, SB and q, and we explore the function TAJB (SA, SB ,q) in Fig. 5ABC, which
reveals several interesting features. First, knowledge can be transferred from a high SNR task to a
low SNR task (Fig. 5A) and the degree of transfer increases with task alignment q. This can make
it possible to capture signals from task A which would otherwise sink into the MP sea by learning
jointly with a related task, even if the tasks are only weakly aligned (Fig. 5A). However, if task A
already has a high SNR, task B must be very well aligned to it for transfer to be beneficial - otherwise
there will be interference. The degree of alignment required increases as the task A SNR increases,
but the quantity of benefit or interference decreases correspondingly (Fig. 5BC). In Appendix D we
explain why our theory predicts these results. Furthermore, in Appendix F we demonstrate these
phenomena are qualitatively recapitulated in nonlinear networks, which suggests that our theory may
give insight into how to choose auxiliary tasks.
5	Discussion
In summary, our analytic theory of generalization dynamics in deep linear networks reveals that many
puzzling aspects of generalization in deep learning already arise in the simple linear setting, where
the puzzles can be understood analytically. In particular, deep linear networks learn more important
structure in data first, leading to generalization errors that depend on task structure much more than
network size. Our theory explains why deep linear networks learn randomized data more slowly than
10
Published as a conference paper at ICLR 2019
structured data, and provides a non-gradient based learning method that out-performs gradient descent
learning in the linear case. Finally, we provide an analytic theory of how knowledge is transferred
from one task to another, demonstrating that the degree of alignment of input features important for
each task, but not how one must respond to these features, is critical for facilitating knowledge transfer.
We think these analytic results provide useful insight into the similar generalization and transfer
phenomena observed in the nonlinear case. Among other things, we hope our work will motivate
and enable: (1) the search for tighter upper bounds on generalization error that take into account task
structure; (2) the design of non gradient based training algorithms that outperform gradient-based
learning; and (3) the theory-driven selection of auxiliary tasks that maximize knowledge transfer.
References
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv, pp. 1-32, 2017.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint, pp. 1-39, 2018. URL http://arxiv.
org/abs/1802.05296.
Devansh Arpit, StanislaW Jastrzgbski, NicOlas Ballas, David Krueger, Emmanuel Bengio, Maxinder S
Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien.
A Closer Look at Memorization in Deep Networks. arXiv preprint, 2017. ISSN 1938-7228. URL
http://arxiv.org/abs/1706.05394.
Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint, pp. 1-24, 2017. URL http://arxiv.org/abs/1706.08498.
Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian Complexities : Risk Bounds and
Structural Results. Journal of Machine Learning Research, 3:463-482, 2002.
Florent Benaych-Georges and Raj Rao Nadakuditi. The singular values and vectors of low rank
perturbations of large rectangular random matrices. Journal of Multivariate Analysis, 111:120-135,
2012. ISSN 0047259X. doi: 10.1016/j.jmva.2012.04.019.
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. Multi-Task Learning for Multiple
Language Translation. Acl, pp. 1723-1732, 2015.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing Nonvacuous Generalization Bounds
for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data. arXiv
preprint, 2017. URL http://arxiv.org/abs/1703.11008.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-Independent Sample Complexity of
Neural Networks. arXiv preprint, (1):1-26, 2017. URL http://arxiv.org/abs/1712.
06541.
I.J. Goodfellow, J Pouget-Abadie, and Mehdi Mirza. Generative Adversarial Networks. arXiv
preprint, pp. 1-9, 2014. ISSN 10495258. doi: 10.1001/jamainternmed.2016.8245.
Steven S. Hansen, Andrew Lampinen, Gaurav Suri, and James L. McClelland. Building on prior
knowledge without building it in. Behavioral and Brain Sciences, 40, 2017.
Andrew Lampinen, Shaw Hsu, and James L Mcclelland. Analogies Emerge from Learning Dyamics
in Neural Networks. Proceedings of the 39th Annual Conference of the Cognitive Science Society,
pp. 2512-2517, 2017.
Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
Sequence to Sequence Learning. Iclr, pp. 1-9, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei a Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles
Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane
Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature,
518(7540):529-533, 2015. ISSN 0028-0836. doi: 10.1038/nature14236.
11
Published as a conference paper at ICLR 2019
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. Conference on Learning Theory (COLT),pp. 1376-1401,2015. ISSN 15337928. URL
http://arxiv.org/abs/1503.00036.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian Approach to
Spectrally-Normalized Margin Bounds for Neural Networks. arXiv preprint, (2017):1-9, 2017.
URL http://arxiv.org/abs/1707.09564.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. Advances in Neural Information
Processing Systems 30, (Nips):1-11, 2017. URL http://arxiv.org/abs/1711.04735.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The Emergence of Spectral Uni-
versality in Deep Networks. In AISTATS 2018, 2018. URL http://arxiv.org/abs/1802.
09979.
Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy
Distillation. arXiv, pp. 1-12, 2015. ISSN 0028-0836. doi: 10.1038/nature14236.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. Advances in Neural Information Processing Systems,
pp. 1-9, 2013a.
Andrew M Saxe, James L Mcclelland, and Surya Ganguli. Learning hierarchical category structure
in deep neural networks. Proceedings of the 35th annual meeting of the Cognitive Science Society,
pp. 1271-1276, 2013b.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information
Propagation. In International Conference on Learning Representations (ICLR),, number 2016, pp.
1-18, 2016. URL http://arxiv.org/abs/1611.01232.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, and Koray Kavukcuoglu. Mastering the game of Go with deep neural networks and tree
search. Nature, 529(7585):484-489, 2016. ISSN 0028-0836. doi: 10.1038/nature16961. URL
http://dx.doi.org/10.1038/nature16961.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint, 2016. ISSN 10414347. doi:
10.1109/TKDE.2015.2507132. URL http://arxiv.org/abs/1611.03530.
A Learning dynamics for deeper networks
In the main text, we described the dynamics of how a single-hidden-layer network converges toward
the training data singular modes {^ɑ, ^α, Vα}, which were originally derived in Saxe et al. (2013a).
There it was also proven that for a network with Nl layers (i.e. Nl - 2 hidden layers), the strength of
the mode obeys the differential equation:
TVu = (Nl — 1)u2-2/(Nl-I)(S — U)
This equation is separable and can be integrated for any integer number of layers. In particular, we
consider the case of 5 layers (3 hidden), in which case:
τ
t(S,S) = 2
tanh-1 (p^)
^3/2
s
This expression cannot be analytically inverted to find s(t, S), so we numerically invert it where
necessary.
12
Published as a conference paper at ICLR 2019
B Alignment lag in randomly initialized networks
As noted in the main text, the randomly-initialized networks behave quite similarly to the TA networks,
except that the randomly-initialized networks show a lag due to the time it takes for the network’s
modes to align with the data modes. In fig. 9 we explore this lag by plotting the alignment of the
modes and the increase in the singular value for several randomly initialized networks.
Notice that stronger modes align more quickly. Furthermore, the mode alignment is relatively
independent - whether the teacher is rank 1 or rank 3, the alignment of the modes is similar for the
mode of singular value 2. Most importantly, note how the deeper networks show substantially slower
mode alignment, with alignment not completed until around when the singular value increases. This
explains why deeper networks show a larger lag between randomly-initialized and TA networks - the
alignment process is much slower for deeper networks.
C Train and test errors after a projection
In the case of transfer learning, or more generally when we want to evaluate a network’s loss on a
subset of its outputs, we need to use a slight generalization of the train and test error formulas given
in the main text. Suppose we are interested in the train and test errors after applying a projection
operator P:
PN= ι∣∣pwx” - py”∣∣2	PN= j∣pwχ”- py”∣∣2
εtrain ≡ --------肃-----------------,	ε∙test ≡ -------肃-----------------
PN= 1 I∣py“ll2	PN= 1 l∣Pyμll2
respectively. As in the main text, we can rexpress these as:
_ Tr WTPTPW - 2Tr WTPTPΣ31 + Tr Σ31TPTPΣ31
εtrain =	Tr Σ31T PT PΣ31	,
εtest
EE	T T ——	——T E ——
Tr WtPtPW - 2Tr WTPTPW + Tr W PT PW
Tr WT Pt PW
Using the cyclic property of the trace, we can modify these to get:
εtrain
TrPWWtPt - 2TrPΣ31WtPt + Tr PΣ31Σ31TPt
Tr PΣ31Σ31T Pt
(17)
(18)
(19)
(20)
TrPWWtPt - 2TrPWWtPt + TrPWWTPt	…、
εtest =	T	.	(21)
Tr PWW Pt
As before, we express these in terms of the student, training data and teacher SVDs, W = USVT,
Σ31 = UJfS^VT, and W = U SVT respectively. Specifically,
εtrain = [pN=i ^βI∣P^ɑI图-1 [pN=i ≡α∣∣Puall2 +pN=i ^βl∣P^α∣l2-2PN=I PN=I Sα^β(Puɑ∙P^β)(vɑ∙vβ)i，
(22)
εtest = [PN=1 篷I∣PuαIl2]T[PN=ι ≡α∣∣PuaIl2 +PN2ι篷I∣PuαIl2-2PN=I PN=I KaWe(Puɑ∙Puβ)(vα∙vβ)].
(23)
D Transfer learning derivations & details
Thm 1 (Transfer theorem) The transfer benefit TA~B:
•	Is unaffected by the UA and UB.
•	Is completely determined by only σZ, SA, SB, and the N A by N B similarity matrix Q =
Vat Vb .
13
Published as a conference paper at ICLR 2019
O	75	150
t∕τ
O	75	150
t∕τ
O
75	150
t∕τ
Figure 9: Alignment of randomly-initialized network modes to data modes and growth of singular
values, plotted for 1 hidden layer (first two rows, a-d) and 3 hidden layers (last two rows, e-h), and
for a rank 1 teacher (first and third rows, a & e), or a rank 3 teacher (second and fourth rows, b-d &
f-h). The columns are the different modes, with respective singular values of 6, 4, and 2. σz was set
to 1. The deeper networks show substantially slower mode alignment, with alignment not completed
until around when the singular value increases.
14
Published as a conference paper at ICLR 2019
Proof: We define
N A	N B
"UA I o
_0 I UB
SAB
N A	N B
-SAl 0 -
_o I sb_
VAB
WA+B
N A N B
[VA∣ VB ]
0
^SB
(24)
N ι
Because of the 0 blocks in UAB, the vectors in blocks corresponding to task A and task B are
completely orthogonal, so UAB remains orthonormal. Thus the relationship between the UA and
UA is irrelevant to the transfer. (In our simulations we use arbitrary orthonormal matrices for
UA and UB.) Therefore the transfer effects will be entirely driven by the relationship between the
matrices VA and VB and the singular values.
We define N A by N B similarity matrix Q = VA VB. If we think of the columns of each V
as spanning a low dimensional feature space in N ι dimensional input space that is important for
each task, then Q reflects the input feature subspace similarity matrix. We can now calculate the
singular values of WA+B. First, note that the input singular modes of WA+B are eigenvectors of
WA+B WA+B, and the associated singular values are square roots of the eigenvalues of WA+B.
Now
wa+bt wa+b = VAB sab UABT UAB sab VABT = VAB SAB2VABT
Now if ~c is an eigenvector of this matrix:
VAB SAB2 VABT ~ = λ~
This implies that
VABT VAB SAB2 VABT~ = λVABT~
Hence eigenvalues of VABSAB VAB are also eigenvalues of VAB VABSAB , with the map-
ping between the eigenvectors given by V . Furthermore, this mapping must be a bijection for
eigenvectors with non-zero eigenvalues, since the matrices have the same rank (the rank of V ). To
see this, note that SAB is full rank. From this, it is clear that
rankVABT VAB SAB 2 = rankVABT VAB = rankVAB.
Furthermore, SAB is positive definite, so
rankVAB SAB 2 VABT = rankVAB.
Now that we know the eigenvectors of these matrices are in bijection, note that:
VABT VAB SAB2
[VA IVB ]
IQ
QT	I
0
IB2
0
Because the output modes don’t matter (as noted above), the alignment between the eigenvectors of
VAB SAB VAB and VA, weighted by their respective eigenvalues, gives the transfer benefit.
15
Published as a conference paper at ICLR 2019
For any given tasks, the transfer benefit can be calculated using our theory. However, in certain
special cases, we can give exact answers. For example, in the rank one case with equal singular
values between the tasks (SA = SB = S), the matrix
I	Ql J SA 2 I 0
IQT	I 1[ o I sb2 _
reduces to
^ 1 q ] S2
q1
with eigenvalues S √1 ± q and eigenvectors
11
1 -1
Corresponding to the shared structure between the tasks and the differences between them. We
note that the sign of the alignment q is irrelevant as a special case of the fact (noted above) that any
orthogonal transformation on the output modes does not affect transfer.
D.1 Misalignment and interference
Why is there interference between tasks which are not well aligned? In the rank one case, we are
effectively changing the (input) singular dimensions of YA from VA to VAB. The two singular
modes of V correspond to the shared structure between the tasks (weighted by the relative signal
strengths), and the differences between them, respectively. Although we may be improving our
estimates of the shared mode if q > 0 (by increasing its singular value relative to SA), We are actually
decreasing its alignment with V unless q = 1. This misalignment is captured by the second mode
of V , but the increase in the singular value of the first mode must come at the cost of a decrease
in the singular value of the second mode. See Fig. 10 for a conceptual illustration of this. This means
that the multi-task setting allows the distinctions between the tasks to sink towards the sea of noise,
while pulling out the common structure. In other words, transferring knowledge from one task always
comes at the cost of ignoring differences between the tasks. Furthermore, incorporating a task B
allows its noise to seep into the task A signal. Together, these two effects help to explain why transfer
can be sometimes beneficial but sometimes detrimental.
Lower SNR for
idiosyncratic
structure
HigherSNR
for shared
structure
Figure 10: Conceptual cartoon of how TA-B, the transfer benefit (or cost) arises from alignment
between the task’s input modes.
E	Non-gradient training algorithm
In Fig. 11 we show the match between the error achieved by training the student by gradient descent
and the optimal stopping error predicted by the non-gradient shrinkage algorithm in the case of a
rank-1 teacher.
16
Published as a conference paper at ICLR 2019
Figure 11: Match between optimal stopping error prediction from non-gradient training algorithm
and empirical optimal stopping error for a rank-1 teacher.
F Transfer results generalize to non-linear networks
Since most deep learning practitioners do not train linear networks, it is important that our theoretical
insights generalize beyond this simple case. In this section we show that the transfer patterns
qualitatively generalize to non-linear networks.
Here, We ShoW results from teacher networks with N 1 = 100 N3 = 50, N2 = 4 (thus the task is
higher rank) and leaky relu non-linearities at the hidden and output layers. We train a student with
leaky relu units and N2 = N3 to solve this task. Results qualitatively look quite similar to those in
Fig 5. of the main text for rank one linear teachers, see below. Thus our insights into transfer may
help to understand multi-task benefits in more complicated architectures.
Figure 12: Transfer benefit TAJB (SA, sB, q) for non-linear teachers and students, plotted at different
values of SA. (a) SA = 0.84 = 4/A.. With support from another aligned task, especially one with
moderately higher SNR, performance on a low SNR task will improve. (b) SA = 3. Tasks with
modest signals will face interference from poorly aligned tasks, but benefits from well aligned tasks.
These effects are amplified by SNR. (c) SA = 100. Tasks with very strong signals will show little
effect from other tasks (note y-axis scale), but any impact will be negative unless the tasks are very
well aligned.
G	Varying the number of training examples
In the main text, we focused on the test error dynamics in the case in which the number of examples
equalled the number of inputs. Here we show how the formula for test error curves is modified as
the number of training examples P is varied. For simplicity, when P 6= N1, we focus on the case
of a full rank student with aspect ratio A = 1 (so that N1 = N2 = N3). The more general case of
lower rank students with non-unity aspect ratios can be easily found from this case, but with some
additional bookkeeping.
As before, we assume the teacher generates noisy outputs from a set of P inputs:
yμ = W^μ + zμ for μ = 1,..., P.
(25)
17
Published as a conference paper at ICLR 2019
Figure 13: The effects of varying the number of training examples P. (a) Test error for a student
learning from a rank-1 teacher with an SNR of 3, with different numbers of inputs. (b,c) Minimum
generalization error plotted against ,P/N1 and SNR ∙ ,P/N1, respectively, at different SNRs.
When P ≥ N1, the minimum generalization error is simply determined by SNR，P/N1, so all
curves converge to a single asymptotic line in (c) as P increases. When P < N1, however, the
curves for different SNRs separate because the projection and noise effects depend on initial SNR.
(d) Optimal stopping error for gaussian vs. orthogonal inputs, showing a strong correlation. Thus our
use of orthogonal inputs in the theory also yields insight into the more general case of approximately
unit norm Gaussian inputs. (For all panels N1 = N2 = N3 = 100, N2 = 1.)
This training set yields important second-order training statistics that will guide student learning:
_____________________-1 -1	ʌ ʌ rτι	_o -ʧ	ʌ ʌ rτι	-- ʌ ʌ rτι	ʌ rτι
Σ11 ≡ XXT	Σ31 ≡ YXT = WXXT + ZXT.	(26)
Here X, Y, and Z are each N1 by P, N3 by P, andN3 by P matrices respectively, whose "'th
columns are^μ, yμ, and Zμ, respectively. Σ11 is an N1 by N1 input correlation matrix, and Σ31 is
an N3 by N1 the input-output correlation matrix. We choose the matrix elements z, of the noise
matrix Z to be drawn iid from a Gaussian with zero mean and variance σ2/N 1. The noise scaling is
chosen so the singular values of the teacher W and the noise Z are both O(1), leading to non-trivial
generalization effects. Furthermore, we chose training inputs to be close to unit-norm, and make
the input covariance matrix Σ11 as white as possible (whitening is a common pre-processing step
for inputs). When P > N1, this can be done by choosing the rows of X to be orthonormal and
then scaling up by PP/N 1, so the columns are approximately unit norm. Then Σ11 = P/N 11
is proportional to the identity. On the otherhand, if P < N 1 , we choose the columns of X to be
orthonormal, so that Σ11 = P||, where P|| is a projection operator onto the P dimensional column
space of X spanned by the input examples. Both these choices are intended to approximate the
situation in which the columns of X are chosen to be iid unit-norm vectors. Finally, as generalization
performance will depend on the ratio of teacher singular values to the noise variance parameter σz2 ,
we simply set σz = 1 as in the main text. Thus, given the unit-norm inputs, we can think of the
teacher singular values as signal to noise ratios (SNRs). We now examine how the dynamics of the
test error evolves as we vary the number of training examples P. We split our analyses into two
distinct regimes: (1) the oversampled regime in which the data density D ≡ P/N1 > 1, and (2) the
undersampled regime in which D < 1.
G.1 The oversampled regime
The oversampled regime (D > 1) is relatively simple. First Σ11 is scaled up by a factor of D. And
in the input-output covariance matrix, Σ31 = WXXT + ZXT, the signal component, WXXT is
18
Published as a conference paper at ICLR 2019
scaled UP by a factor of D while the noise component ZXT has the same singular value spectrum
as the D = 1 case, up to an overall scaling by √D (since the rows of XX are orthogonal and all its
singular values are equal to √D). This leads to an increase in the effective SNR by a factor of √D.
Thus overall, the test error curves for the case of D > 1 can be simply obtained from the theory of the
test error curves for D = 1 through two modifications: (1) a boost in the SNR for the D = 1 case by
a multiplicative factor of √D, and (2) and an overall speed up in the learning time by a multiplicative
factor of D.
G.2 The undersampled regime
For the undersampled regime (D < 1), we must account for the fact that the P training inputs do
not span the full N1 dimensional space of all inputs. Thus the projection operator P || onto the
P dimensional column space of XX plays a crucial role. Indeed the input-correlation Σ11 = P||.
And Σ31 = WP|| + ZXXT. This implies that the learning dynamics only transforms the composite
student map W from the P dimensional subspace spanned by the inputs to the N3 dimensional
output space. In contrast, the student map from the N1 - P dimensional subspace orthogonal to the
image of P || remains frozen. Tracing through the equations of the main paper and accounting for the
projection operator P || , we find the effective aspect ratio for this undersampled learning problem
(when N3 = N2 = N1) is no longer A = N3 /N1 but rather D = P/N1. Furthermore, in the limit
N 3,N ι → ∞ while N 2 remains O(1), the singular values of the signal component WP || of Σ31
are attenuated by a factor of √D, making the associated singular vectors more susceptible to noise.
Again tracing through the equations of the main paper, with all of these modifications, we find the
final formula for test error curves in the undersampled measurement regime:
εtest(t)
h(N3 - P)e2 + (P-N2)hs(S, t)2i + PN=1 [(sα(t) - Sα)2 + 2Sα(t)Sα(1 - O(√DSα))]]
hpN=ι Sai
(27)
This equation has several modifications compared to the case P = N1 in (15). First the term in the
numerator involving N3 - P reflects generalization error due to the N3 - P dimensional frozen
subspace, and the initial weight variance e2 contributes to this generalization error. The second term
in the numerator involves all the P - N2 training modes which cannot be correlated with the teacher,
and the average〈•〉is over a Marcenko-Pasteur distribution of singular values (see (13)) except with
the aspect ratio A replaced by D. The third term accounts for learned correlations between the
student and teacher. It involves the transformation from teacher singular values S to training data
singular values S through the formula (11) except with the aspect ratio replacement A → D, and the
effective teacher singular value attenuation S → √Ds. Similarly, the computation of the singular
vector overlap is done through (12) also with the replacements A → D and S → √DS.
G.3 Comparison of theory and experiment for under and over sampled
MEASUREMENT REGIMES
In Fig. 13, we show an excellent match between our theory and empirical simulations for varying
values of P, both in the oversampled and undersampled measurement regimes. There are a number of
interesting features to note. First, although the minimum generalization error improves monotonically
with P, the asymptotic (t → ∞) generalization error does not, because of a frozen subspace (Advani
& Saxe, 2017) of the modes that are not overfit when P < Ni, because the training data rank is ≤ P.
Second, when P ≥ Ni, the minimum generalization error is simply determined by SNR JP/Ni, so
all curves converge to a single asymptotic line as P increases. When P < N1, however, the curves
for different SNRs separate because the projection and noise effects depend on initial SNR. Finally,
in Fig. 13D we show that approximately unit norm i.i.d. gaussian inputs yield similar results to the
orthogonalized data matrices we employed in the theory, although the gaussian inputs do result in
slightly higher optimal stopping error.
H Less than full rank students
Although we generally assumed students were full rank in the main text to simplify the calculations,
our theory remains exact for TA networks of any rank. Furthermore, as shown in Fig. 14, the TA
19
Published as a conference paper at ICLR 2019
and random networks again show very similar optimal stopping generalization error, but with the
optimal stopping time of the random networks lagging behind that of the TA networks. Furthermore,
this lag increases as the rank of the random network decreases (because a low rank network will
have less initial projection onto the random modes, there is is more alignment to be done). However,
reducing the student rank does not change the optimal stopping error (as long as it is still greater than
the teacher rank).
(∙11IUOPUE」)IpOdə 6u-ddols -ElUndO
Min log generalization error (aligned init.)
(a) Best generalization error is quite similar(b) Optimal stopping time is quite similar between aligned and
Empirical results
• Full rank
▲ 50% of full
■ 4% of full
+ 1%of full
-1-2-3必
(・llUJOPUSJCLUe UOnBZ=挹①U ①6 CTO- ⊂=≥
between aligned and random initializations
random initializations
SNR
I
Empirical results
•	Full rank
▲	50% of full
■	4% of full
+	1% of full
—Theory
200	400	600	800	200	400	600	800
Optimal stopping epoch	Optimal stopping epoch
(c) Optimal generalization error vs. optimal(d) Optimal generalization error vs. optimal stopping time for
stopping time for randomly initialized networksinitially aligned networks
Figure 14: Empirical verification that the simplifying assumptions of our theory are approximately
valid in the regime we are considering at different student ranks. Initializations with random initial
weights (random init.) and initializations with initial weight aligned to the noisy data SVD (aligned
init.) are compared across varying student ranks. (a) The minimum generalization errors are almost
identical between the different initializations and different student ranks. (b) The optimal stopping
time in the randomly initialized networks consistently lags behind the aligned networks, because it
takes time for the alignment to occur. This lag increases as the students rank decreases. (c) Randomly
initialized networks of varying ranks obey qualitatively similar trends of increase in optimal stopping
error and optimal stopping time as SNR decreases. (d) The theory predicts the aligned networks
trends of increase in optimal stopping error and optimal stopping time with decreasing SNR almost
perfectly. (All plots are made with a rank 1 teacher and N1 = N3 = 100)
20