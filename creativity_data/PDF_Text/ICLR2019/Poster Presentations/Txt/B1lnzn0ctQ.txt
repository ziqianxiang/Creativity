Published as a conference paper at ICLR 2019
ALISTA: Analytic Weights Are As Good As
Learned Weights in LISTA
Jialin Liu*
Department of Mathematics
University of California, Los Angeles
liujl11@math.ucla.edu
Xiaohan Chen*
Department of Computer Science and Engineering
Texas A&M University
chernxh@tamu.edu
Zhangyang Wang
Department of Computer Science and Engineering
Texas A&M University
atlaswang@tamu.edu
Wotao Yin
Department of Mathematics
University of California, Los Angeles
wotaoyin@math.ucla.edu
Ab stract
Deep neural networks based on unfolding an iterative algorithm, for example,
LISTA (learned iterative shrinkage thresholding algorithm), have been an empir-
ical success for sparse signal recovery. The weights of these neural networks
are currently determined by data-driven “black-box” training. In this work, we
propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is com-
puted as the solution to a data-free optimization problem, leaving only the step-
size and threshold parameters to data-driven learning. This significantly simplifies
the training. Specifically, the data-free optimization problem is based on coher-
ence minimization. We show our ALISTA retains the optimal linear convergence
proved in (Chen et al., 2018) and has a performance comparable to LISTA. Fur-
thermore, we extend ALISTA to convolutional linear operators, again determined
in a data-free manner. We also propose a feed-forward framework that combines
the data-free optimization and ALISTA networks from end to end, one that can be
jointly trained to gain robustness to small perturbations in the encoding model.
1	Introduction
Sparse vector recovery, or sparse coding, is a classical problem in source coding, signal recon-
struction, pattern recognition and feature selection. There is an unknown sparse vector x* =
[x*,…，XM]T ∈ RM. We observe its noisy linear measurements:
M
b = X dmx*m + ε = Dx* + ε,	(1)
m=1
where b ∈ RN, D = [di,…，dM] ∈ RN×M is the dictionary, and ε ∈ RN is additive Gaussian
white noise. For simplicity, each column of D, named as a dictionary kernel, is normalized, that is,
IldmIl2 = ∣∣D=,m∣∣2 = 1, m = 1, 2,…，M. Typically, We have N《M, so Equation (1) is an
under-determined system.
However, when x* is sufficiently sparse, it can be recovered faithfully. A popular approach is to
solve the LASSO problem below (where λ is a scalar):
minimize Ukb - DxI22 + λIxI1
x2
using iterative algorithms such as the iterative shrinkage thresholding algorithm (ISTA):
χ(k+1) = ηλ∕L (x(k) + L DT (b - Dx(k))),	k = 0,1, 2,...
(2)
(3)
* These authors contributed equally and are listed alphabetically.
1
Published as a conference paper at ICLR 2019
where ηθ is the soft-thresholding function1 and L is usually taken as the largest eigenvalue of DTD.
Inspired by ISTA, the authors of (Gregor & LeCun, 2010) proposed to learn the weights in the
matrices in ISTA rather than fixing them. Their methods is called Learned ISTA (LISTA) and
resembles a recurrent neural network (RNN). If the iteration is truncated to K iterations, LISTA
becomes a K-layer feed-forward neural network with side connections. Specifically, LISTA is:
x(k+1) = ηθ(k)(w1k)b + w2k)x(k)),	k = 0,1,…，K - 1.	(4)
If We set WIk) ≡ LDT, Wgk) ≡ I - LDTD, θ(k) ≡ Lλ, then LISTA recovers ISTA.
Given each pair of sparse vector and its noisy measurements (x*, b), applying (4) from some
initial point x(0) and using b as the input yields x(k). Our goal is to choose the parameters
Θ = {W1(k), Ww(k), θ(k)}k=0,1,…,κ-ι such that x(k) is close to x* for all sparse x* following
some distribution P . Therefore, given the distribution P, all parameters in Θ are subject to learning:
minimize Eχ* ,b〜Px(K) Θ, b, x(0) -x*2.	(5)
This problem is approximately solved over a training dataset {(xi*, bi)}iN=1 sampled from P.
Many empirical results, e.g., (Gregor & LeCun, 2010; Sprechmann et al., 2015; Wang et al., 2016b),
show that a trained K-layer LISTA (with K usually set to 10 〜20) or its variants can generalize
more than well to unseen samples (x0, b0) from the same distribution and recover x0 from b0 to
the same accuracy within one or two order-of-magnitude fewer iterations than the original ISTA.
Additionally, the accuracies of the outputs {x(k)} of the layers k = 1, .., K gradually improve.
However, such networks will generalize worse when the input deviates from the training distribution
(e.g., when D varies), in contrast to the classical iterative algorithms such as ISTA that are training-
free and thus agnostic to the input distribution. The Analysis-Synthesis model (Rubinstein & Elad,
2014; Yang et al., 2016) could also be viewed as a special LISTA model with only one layer (K = 1).
More recently, the convolutional sparse coding (CSC), an extension of the sparse coding (1),
gains increasingly attention in the machine learning area. (Sreter & Giryes, 2018) showed that
the CSC could be similarly approximated and accelerated by a LISTA-type feed-forward network.
(Tolooshams et al., 2018) designed a structure of sparse auto-encoder inspired by multi-layer CSC.
(Papyan et al., 2016; Sulam et al., 2017) also revealed CSC as a potentially useful tool for under-
standing general convolutional neural networks (CNNs).
1.1	Related Work
Despite the empirical success (Sprechmann et al., 2015; Wang et al., 2016a;b;c;d; Zhang & Ghanem,
2018; Zhou et al., 2018; Ito et al., 2018) in constructing fast trainable regressors for approximating
iterative sparse solvers, the theoretical understanding of such approximations remains limited.
A handful of recent works have been investigating the theory of LISTA. (Moreau & Bruna, 2017)
re-factorized the Gram matrix of dictionary, by trying to nearly diagonalize the Gram matrix with
a basis, subject to a small `1 perturbation. They thus re-parameterized LISTA a new factorized
architecture that achieved similar acceleration gain to LISTA, hence ending up with an “indirect”
proof. They concluded that LISTA can converge faster than ISTA, but still sublinearly. (Giryes et al.,
2018) interpreted LISTA as a projected gradient descent descent (PGD) where the projection step
was inaccurate, which enables a trade-off between approximation error and convergence speed. The
latest work (Chen et al., 2018) presented the more related results to ours: they introduced necessary
conditions for the LISTA weight structure in order to achieve asymptotic linear convergence of
LISTA, which also proved to be a theoretical convergence rate upper bound. They also introduced
a thresholding scheme for practically improving the convergence speed. Note that, none of the above
works extended their discussions to CSC and its similar LISTA-type architectures.
Several other works examined the theoretical properties of some sibling architectures to LISTA.
(Xin et al., 2016) studied the model proposed by (Wang et al., 2016b), which unfolded/truncated the
iterative hard thresholding (IHT) algorithm instead of ISTA, for approximating the solution to `0-
minimization. They showed that the learnable fast regressor can be obtained by using a transformed
dictionary with improved restricted isometry property (RIP). However, their discussions are not
1Soft- thresholding function is defined in a component-wise way: ηθ (x) = sign(x) max(0, |x| - θ)
2
Published as a conference paper at ICLR 2019
applicable to LISTA directly, although IHT is linearly convergent (Blumensath & Davies, 2009)
under rather strong assumptions. Their discussions were also limited to linear sparse coding and
resulting fully-connected networks only. (Borgerding et al., 2017; Metzler et al., 2017) studied
a similar learning-based model inspired from another LASSO solver, called approximated message
passing (AMP). (Borgerding et al., 2017) showed the MMSE-optimality of an AMP-inspired model,
but not accompanied with any convergence rate result. Also, the popular assumption in analyzing
AMP algorithms (called “state evolution”) does not hold when analyzing ISTA.
1.2	Motivation and Contributions
This paper presents multi-fold contributions in advancing the theoretical understanding of LISTA,
beyond state-of-the-art results. Firstly, We show that the layer-wise weights in LISTA need not
being learned from data. That is based on decoupling LISTA training into a data-free analytic op-
timization stage followed by a lighter-weight data-driven learning stage without compromising the
optimal linear convergence rate proved in (Chen et al., 2018). We establish a minimum-coherence
criterion between the desired LISTA weights and the dictionary D, which leads to an efficient algo-
rithm that can analytically solve the former from the latter, independent of the distribution ofx. The
data-driven training is then reduced to learning layer-wise step sizes and thresholds only, which will
fit the distribution ofx. The new scheme, called Analytic LISTA (ALISTA), provides important in-
sights into the working mechanism of LISTA. Experiments shows ALISTA to perform comparably
with previous LISTA models (Gregor & LeCun, 2010; Chen et al., 2018) with much lighter-weight
training. Then, we extend the above discussions and conclusions to CSC, and introduce an efficient
algorithm to solve the convolutional version of coherence minimization. Further, we introduce a
new robust LISTA learning scheme benefiting from the decoupled structure, by adding perturba-
tions to D during training. The resulting model is shown to possess much stronger robustness when
the input distribution varies, even when D changes to some extent, compared to classical LISTA
models that learn to (over-)fit one specific D.
2	Analytic LISTA: calculating weights without training
We theoretically analyze the LISTA-CPSS model defined in (Chen et al., 2018):
x(k+1) = ηθ(k) x(k) - (W(k))T (Dx(k) - b),	(6)
where W(k) = [w(k),…，w((k)] ∈ RN×M is a linear operator with the same dimensionality with
D, x(k) = [x1k),…，xM)] is the kth layer node. In (6), Θ = {W(k), θ(k)}k are parameters to train.
Model (6) can be derived from (4) with W1(k) = (W(k))T, W2(k) = I -W1(k)D. (Chen et al., 2018)
showed that (6) has the same representation capability with (4) on the sparse recovery problem, with
a specifically light weight structure.
Our theoretical analysis will further define and establish properties of “good” parameters Θ in (6),
and then discuss how to analytically compute those good parameters rather than relying solely on
black-box training. In this way, the LISTA model could be further significantly simplified, with little
performance loss. The proofs of all the theorems in this paper are provided in the appendix.
2.1	Recovery Error Upper bound
We start with an assumption on the “ground truth” signal x* and the noise ε.
Assumption 1 (Basic assumptions). Signal x* is SamPledfrom thefollowing set:
x* ∈X(B,s) , nx*|xi*| ≤B,∀i, kx*k0 ≤so.	(7)
In other words, x* is bounded and s-sParse2 (s ≥ 2). Furthermore, we assume ε = 0.
The zero-noise assumption is for simplicity of the proofs. Our experiments will show that our models
are robust to noisy cases.
The mutual coherence of the dictionary D is a significant concept in compressive sensing (Donoho
& Elad, 2003; Elad, 2007; Lu et al., 2018). A dictionary with small coherence possesses better
sparse recovery performance. Motivated by this point, we introduce the following definition.
2A signal is s-sparse if it has no more than s non-zero entries.
3
Published as a conference paper at ICLR 2019
Definition 1. Given D ∈ RN ×M with each of its column normalized, we define the generalized
mutual coherence:
μ(D)=	infxM m max (W：,i)TD j.	(8)
W∈RN ×M	i6=j
(W：,i)T D∖i = 1,1≤i≤M 1≤i,j≤M
Additionally, We define W(D) = W ∈ RN×M : W attains the infimum given (8) . A weight
matrix W is “good” if W ∈ W (D).
In the above definition, problem (8) is feasible and attainable, i.e., W(D) = 0, which was proven
in Lemma 1 of (Chen et al., 2018).
Theorem 1 (Recovery error upper bound). Take any x* ∈ X (B, S) ,any W ∈ W (D), and any
sequence γ(k) ∈ (0, 2口§-口+1). Using them, define the parameters {W(k),θ(k)}:
W(k) = YIkkW, θ(k) = Y (k)μ(D)	sup	{kχ(k)(x*)-x*kι},	(9)
x* ∈X (B,s)
while the sequence {x(k) (x*)}k∞=1 is generated by (6) using the above parameters and x(0) = 0
(Note that each x(k) (x*) depends only on θ(k-1), θ(k-2), . . . and defines θ(k)). Let Assumption 1
hold with any B > 0 and s < (1 + 1∕μ)∕2. Then, we have
k-1
support(x(k)(x*)) ⊂ S,	kx(k)(x*) -x*k2 ≤ sB exp - X c(τ) , k = 1,2, .. .	(10)
τ=0
where S is the support of x* and c(k) = 一 log ((2"s — μ)γ(k) + |1 一 Y(k)|) is a positive constant.
In Theorem 1, Eqn. (9) defines the properties of “good” parameters:
•	The weights W(k) can be separated as the product of a scalar Y(k) and a matrix W inde-
pendent of layer index k, where W has small coherence with D.
•	Y(k) is bounded in an interval.
•	θ(k)∕γ(k) is proportional to the 'ι error of the output of the kth layer.
The factor c(k) takes the maximum at Y(k) = 1. If Y(k) ≡ 1, the recovery error converges to zero in
a linear rate (Chen et al., 2018):
∣∣x(k)(x*) — x*k2 ≤ SB exp ( — ck),
where C = — log(2"s — μ) ≥ c(k). Although γ(k) ≡ 1 gives the optimal theoretical upper bound if
there are infinitely many layers k = 0,1, 2, ∙∙∙, it is not the optimal choice for finite k. Practically,
there are finitely many layers and Y(k) obtained by learning is bounded in an interval.
2.2	Recovery Error Lower bound
In this subsection, we introduce a lower bound of the recovery error of LISTA, which illustrates that
the parameters analytically given by (9) are optimal in the convergence order (linear).
Assumption 2. The signal x* is a random variable following the distribution PX. Let S =
support(x*). PX satisfies: 2 ≤ | S | ≤ S; S uniformly distributes on the whole index set; non-
zero part xS* satisfies the uniform distribution with bound B: |xi* | ≤ B, ∀i ∈ S. Moreover, the
observation noise ε = 0.
Theorem 1 tells that an ideal weight W ∈ W(D) satisfies I — WTD ≈ 0. But this cannot be met
exactly in the overcomplete D case, i.e., N < M. Definition 2 defines the set of matrices W such
that WT D is bounded away from the identity I. In Appendix D, we discuss the feasibility of (11).
Definition 2. Given D ∈ RN×M, s ≥ 2, σmin > 0, we define a set that W(k) are ChoSenfrom:
W(D,S,σmin) = {W ∈ RN ×M 卜 min(l—(W:,S)T D：,s) ≥ 3min, ∀ S with 2 ≤ | S | ≤ s}. (11)
Based on Definition 2, we define a set that Θ = {W(k), θ(k)}k∞=0 are chosen from:
4
Published as a conference paper at ICLR 2019
Definition 3. Let {x(k)(x*)}∞=ι be generated by(6) with {W(k), θ(k)}∞=o andx(0) = 0. Then we
define T as the set of parameters that guarantee there is no false positive in x(k):
T = {{W(k) ∈ W(D,s,σmin)"%∞=0卜UppOrt(X(k)(x*)) ⊂ S, ∀x* ∈X(B,s), ∀k} (12)
The conclusion (10) demonstrates that T is nonempty because “support(x(k)(x*)) ⊂ S" is Satis-
fied as long as θ(k-1) large enough. Actually, T contains almost all “good” parameters because
considerable false positives lead to large recovery errors. With T defined, we have:
Theorem 2 (Recovery error lower bound). Letthesequence {x(k)(x*)}∞=ι be generated by (6) with
{W(k), θ(k)}k∞=0 and x(0) = 0. Under Assumption 2, for all parameters {W(k), θ(k)}k∞=0 ∈ T and
any sufficient small > 0, we have
kx(k)(x*)- x*k2 ≥d∣x*k2 exp(-ck),	(13)
with probability at least (1 一 Es3/2 — e2), where c = S log(3) 一 log(σmin).
This theorem illustrates that, with high probability, the convergence rate of LISTA cannot be faster
than a linear rate. Thus, the parameters given in (9), that leads to the linear convergence if γk is
bounded within an interval near 1, are optimal with respect to the order of convergence of LISTA.
2.3	Analytic LISTA: less parameters to learn
Following Theorems 1 and 2, we set W(k) = γ(k)W, where γ(k) is a scalar, and propose Tied
LISTA (TiLISTA):	x(k+1) = ηθ(k) x(k) 一 γ(k)WT (Dx(k) 一 b),	(14)
where Θ = {γ(k)}k, {θ(k)}k, W are parameters to train. The matrix W is tied over all the layers.
Further, we notice that the selection of W from W(D) depends on D only. Hence we propose the
analytic LISTA (ALISTA) that decomposes tied-LISTA into two stages:
x(k+1) = ηθ(k) (x(k) — Y(k)WT(Dx(k) — b)),	(15)
where W is pre-computed by solving the following problem (Stage 1)3:
W ∈ arg min IWTDlIF,	s.t. (W：,m)TD：,m = 1, ∀m = 1, 2,…，M,	(16)
W∈RN×M
Then with W fixed, {γ(k), θ(k)}k in (15) are learned from end to end (Stage 2). (16) reformulates
(8) to minimizing the Frobenius norm ofWTD (a quadratic objective), over linear constraints. This
is a standard convex quadratic program, which is easier to solve than to solve (8) directly.
Table 1: Summary: variants of LISTA and the number of parameters to learn.
Vanilla LISTA (4)	LISTA-CPSS (6)	TiLISTA (14)	ALISTA (15)
O(KM2+K+MN)	O(KNM +ɪp	O(NM + Kr	O(K)
3 Convolutional Analytic LISTA
We extend the analytic LISTA to the convolutional case in this section, starting from discussing the
convolutional sparse coding (CSC). Many works studied CSC and proposed efficient algorithms for
that (Bristow et al., 2013; Heide et al., 2015; Wohlberg, 2014; 2016; Papyan et al., 2017; Garcia-
Cardona & Wohlberg, 2018; Wang et al., 2018; Liu et al., 2017; 2018). In CSC, the general linear
transform is replaced by convolutions in order to learn spatially invariant features:
M
b = X dm * Xm + ε,	(17)
m=1
where each dm is a dictionary kernel (or filter). {dm}mM=1 is the dictionary of filters, M denotes the
number of filters. {xm}M=ι is the set of coefficient maps that are assumed to have sparse structure,
3Some details and a complexity analysis of Stage 1 are discussed in Appendix E.1
5
Published as a conference paper at ICLR 2019
and * is the convolution operator. Now We consider 2D convolution and take4 b ∈ RN2, dm ∈
RD2, xm ∈ R(N +D-1)2. Equation (17) is pointwisely defined as5 *:
D-1 D-1 M
b(i, j) =XXX
dm(k, l)xm(i +k,j + l) +ε(i,j),	0 ≤ i,j ≤ N - 1.	(18)
k=0 l=0 m=1
We concatenate dm,s and xm,s: d = [di,…，dM]T, X = [xι,…，XM]T, and rewrite (18) as:
M
b = X DcNonv,m(dm)Xm +ε = DcNonv(d)X+ε,	(19)
m=1
where the matrix DNlnv(d) = [DNOnv,1(d1),…，DN^M(dM)] ∈ RN2×(N +D-1)2M, depending
on the signal size N and the dictionary d, is defined in detail in (48) in Appendix C.2.
From (17), the convolutional LISTA becomes a natural extension of the fully-connected LISTA (6):
M
Xm+i) = ηθ(k) (Xm)-(Wm))0 * (X dm * Xm) - b)), m =1, 2,…，M,	(20)
m=i
where {W(mk)}mM=1 share the same sizes with {dm}M=ι and (∙)0 means a 180 rotation of the filter
(Chalasani et al., 2013). We concatenate the filters together: w(k) = [w(k),…，wM)]T ∈ RD2M.
Parameters to train are Θ = {W(k), θ(k)}k.
Let WcNonv(w(k)) be the matrix induced by dictionary w(k) with the same dimensionality as
DcNonv(d). Since convolution can be written as a matrix form (19), (20) is equivalent to
X(k+1) = ηθ(k) X(k) - (WcNonv(w(k)))T (DcNonv(d)X(k) - b).	(21)
Then by just substituting D, W(k) with DcNonv(d), WcNonv(w(k)) respectively, Theorems 1 and 2
can be applied to the convolutional LISTA.
Proposition 1. Let D = DcNonv(d) and W(k) = WcNonv(w(k)). With Assumption 1 and other
settings the same with those in Theorem 1, (10) holds. With Assumption 2 and other settings the
same with those in Theorem 2, (13) holds.
Similar to the fully connected case (15), based on the results in Proposition 1, we should set wm(k) =
Ymk)Wm, m = 1,2,…，M, where W = [Wi,…，WM]T is chosen from
W ∈ W Nnv=	argmin	II(WNnv(W))T DNlnKd)L.	(22)
w∈RD2M	F
Wm∙dm=1, 1≤m≤M
However, (22) is not as efficient to solve as (16). To see that, matrices DcNlnv(d) and WcNlnv(W) are
both of size N2 X (N + D — 1)2M, the coherence matrix (WNonv(W))TDNnv(d) is thus of size
(N+D - 1)2M × (N+D - 1)2M. In the typical application setting of CSC, b is usually an image
rather than a small patch. For example, if the image size is 100 × 100, dictionary size is 7 × 7 × 64,
N= 100, D = 7,M = 64, then (N+D - 1)2M × (N+D - 1)2M ≈ 5 × 1011.
3.1	Calculating convolutional weights analytically and efficiently
To overcome the computational challenge of solving (22), we exploit the following circular convo-
lution as an efficient approximation:
D-1 D-1 M
b(i,j) = XXX dm(k,l)Xm((i+k)modN, (j+l)modN)+ε(i,j),	0 ≤ i,j ≤ N —1, (23)
k=0 l=0 m=1
4Here, b, dm , xm are vectors. The notion b(i, j) means the (iN + j)th entry of b. Additionally, dm , xm
are defined in the same way for all m = 1,… ,M.
5Strictly speaking, (18) is the cross-correlation rather than convolution. However in TensorFlow, that oper-
ation is named as convolution, and we follow that convention to be consistent with the learning community.
6
Published as a conference paper at ICLR 2019
where b ∈ RN2, dm ∈ RD2 , xm ∈ RN2. Similar to (18), we rewrite (23) in a compact way:
M
b =	DcNir,m(dm)xm +ε = DcNir(d)x + ε,
m=1
where DcNir (d) : RN2M → RN2 is a matrix depending on the signal size N and the dictionary d.
Then the coherence minimization with the circular convolution is given by
W Nr =	arg min	II(WNr(W))T 口焦同匚	(24)
w∈RD2M	F
Wm∙dm = 1, 1≤m≤M
The following theorem motivates us to use the solution to (24) to approximate that of (22).
Theorem 3. The solution sets of (22) and (24) satisfy the following properties:
1.	WcNir = Wc2iDr-1,∀N ≥ 2D - 1.
2.	If at least one of the matrices {DCD-1,…,D2DM1} is non-singular, W 2D-1 involves
only a unique element. Furthermore,
lim WcNonv = Wc2iDr -1
N→∞
(25)
The solution set W cNir is not related with the image size N as long as N ≥ 2D - 1, thus one can
deal with a much smaller-size problem (let N = 2D - 1). Further, (25) indicates that as N gets
(much) larger than D, the boundary condition becomes less important. Thus, one can use Wc2iDr -1
to approximate WcNonv. In Appendix E.2, we introduce the algorithm details of solving (24).
Based on Proposition 1 and Theorem 3, we obtain the convolutional ALISTA:
M
χm+1)=ηθ(k) (Xm)-Ym° (Wm)0* (X dm * Xm)-»), m=1,2,…，m,	(26)
m=ι
where W = [W 1,…，W M ]T ∈ W 2D T and Θ = {{γm)}m,k, {θ(k)}k } are the parameters to train.
(26) is a simplified form, compared to the empirically unfolded CSC model recently proposed in
(Sreter & Giryes, 2018)
4	Robust ALISTA to Model Perturbation
Many applications, such as often found in surveillance video scenarios (Zhao et al., 2011; Han
et al., 2013), can be formulated as sparse coding models whose dictionaries are subject to small
dynamic perturbations (e.g, slowly varied over time). Specifically, the linear system model (1) may
have uncertain D: D = D + Ed , where Ed is some small stochastic perturbation. Classical
LISTA entangles the learning of all its parameters, and the trained model is tied to one static D.
The important contribution of ALISTA is to decompose fitting W w.r.t. D, from adapting other
parameters {γ(k), θ(k)}k to training data.
In this section, we develop a robust variant of ALISTA that is a fast regressor not only for a given
D, but all its randomly perturbations D to some extent. Up to our best knowledge, this approach is
new. Robust ALISTA can be sketched as the following empirical routine (at each iteration):
•	Sample a perturbed dictionary D. Sample X and ε to generate b w.r.t. D.
•	Apply Stage 1 of ALISTA w.r.t. D and obtain W; however, instead of an iterative mini-
mization algorithm, We use a neural network that unfolds that algorithm to produce W.
•	Apply Stage 2 of ALISTA w.r.t. W, D, x, and b to obtain {γ(k),θ(k)}k.
In Robust ALISTA above, D becomes a part of the data for training the neural network that generates
W. This neural network is faster to apply than the minimization algorithm. One might attempt to
use D in the last step, rather than D, but D makes training less stable, potentially because of larger
weight variations between training iterations due to the random perturbations in D. We observe
that using D stabilizes training better and empirically achieves a good prediction. More details of
training Robust ALISTA are given in Appendix G.
7
Published as a conference paper at ICLR 2019
5	Numerical Results
In this section, we conduct extensive experiments on both synthesized and real data to demonstrate:6
•	We experimentally validate Theorems 1 and 2, and show that ALISTA is as effective as
classical LISTA (Gregor & LeCun, 2010; Chen et al., 2018)but is much easier to train.
•	Similar conclusions can be drawn for convolutional analytic LISTA.
•	The robust analytic LISTA further shows remarkable robustness in sparse code prediction,
given that D is randomly perturbed within some extent.
Notation For brevity, we let LISTA denote the vanilla LISTA model (4) in (Gregor & LeCun,
2010); LISTA-CPSS refers to the lately-proposed fast LISTA variant (Chen et al., 2018) with weight
coupling and support selection; TiLISTA is the tied LISTA (14); and ALISTA is our proposed Analytic
LISTA (15). If the model is for convolutional case, then we add “Conv” as the prefix for model name,
such as “Conv ALISTA” that represents the convolutional analytic LISTA.
5.1 Validation of Theorems 1 and 2 (Analytic LISTA)
We follow the same N = 250, M = 500 setting as (Chen et al., 2018) by default. We sample the
entries of D i.i.d. from the standard Gaussian distribution, Dij 〜N(0,1/N) and then normalize its
columns to have the unit `2 norm. We fix a dictionary D in this section. To generate sparse vectors
x*, We decide each of its entry to be non-zero following the Bernoulli distribution with Pb = 0.1.
The values of the non-zero entries are sampled from the standard Gaussian distribution. A test set
of 1000 samples generated in the above manner is fixed for all tests in our simulations. The analytic
weight W that we use in the ALISTA is obtained by solving (16).
All networks used (vanilla LISTA, LISTA-CPSS, TiLISTA and ALISTA) have the same number of
16 layers. We also include two classical iterative solvers: ISTA and FISTA. We train the networks
with four different levels of noises: SNR (Signal-to-Noise Ratio) = 20, 30, 40, and ∞. While our
theory mainly discussed the noise-free case (SNR = ∞), we hope to empirically study the algorithm
performance under noise too. As shown in Figure 1, the x-axes denotes the indices of layers for the
networks, or the number of iterations for the iterative algorithms. The y-axes represent the NMSE
(Normalized Mean Squared Error) in the decibel (dB) unit:
NMSEdB(X,x*) = 10logιo (Ekx - x*k2∕E∣∣x*k2),
where x* is the ground truth and X is the estimated one.
-5 ∙≡
-若
ST -25 -
S	二
LLl -35 —
ω	二
:-45 7
-55 W
-65 —
-75 J
O 1	2 3 4	5 6	7 8	9 10 11 12 13 14 15 16
Iterations / Layers (fc)
(a) Noiseless Case: SNR = ∞
mp)山 SwN
0 -≡
-5 -≡
-10 -≡
-15 ■!
-20 -≡
-25 ■!
-30 -≡
-35 ■!
-40 -≡
-45 ∙!
-50 -≡
0	1	2	3 4	5	6 7	8 9 10 11 12 13 14 15 16
Iterations / Layers (⅛)
(b) Noisy Case: SNR = 40dB
∞p)山S≡N
0 E
-5 ÷
-10 ÷
-15 ÷
-20 W
-25 W
-30 E
-35 ÷ X ISTA X LISTA-CPSS
-40 W
-45 -=
FISTA	Y----TiLISTA
LISTA	-Θ——ALISTA
∞p)山S≡N
ista
fista
lista
→-----lista-cpss
Tl----TiLISTA
-Θ——alista
0	1	2	3 4	5 6	7	8 9 10 11 12 13 14 15 16
Iterations / Layers (fc)
(c)	Noisy Case: SNR = 30dB
0	1	2	3 4	5	6 7	8 9 10 11 12 13 14 15 16
Iterations / Layers (⅛)
(d)	Noisy Case: SNR = 20dB
Figure 1: Justification of Theorems 1 and 2: comparision among LISTA variants.
6Our codes are uploaded to https://github.com/xchen-tamu/alista.
8
Published as a conference paper at ICLR 2019
100 1
10
0.1
0.01
1
1	2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Layer (fc)
⑶Yk
Figure 2: Justification of Theorem 1 (noiseless case): parameters obtained by training satisfy (9).

In Figure 1 (a) noise-less case, all four learned models apparently converge much faster than two
iterative solvers (ISTA/FISTA curves almost overlap in this y-scale, at the small number of iter-
ations). Among the four networks, classical-LISTA is inferior to the other three by an obvious
margin. LISTA-CPSS, TiLISTA and ALISTA perform comparably: ALISTA is observed to eventu-
ally achieve the lowest NMSE. Figure 1(a) also supports Theorem 2, that all networks have at most
linear convergence, regardless of how freely their parameters can be end-to-end learned.
Figure 1 (b) - (d) further show that even in the presence of noise, ALISTA can empirically perform
comparably with LISTA-CPSS and TiLISTA, and stay clearly better than LISTA and ISTA/FISTA.
Always note that ALISTA the smallest amount of parameters to learn from the end-to-end train-
ing (Stage 2). The above results endorse that: i) the optimal LISTA layer-wise weights could be
structured as W(k) = γ(k)W; and ii) W could be analytically solved rather than learned from
data, without incurring performance loss. We also observe the significant reduction of training time
for ALISTA: while LISTA-CPSS of the same depth took 〜1.5 hours to train, ALISTA was trained
within only 6 minutes (0.1 hours) to achieve comparable performance, on the same hardware (one
1080 Ti on server).
We further supply Figures 2 and 3 to justify Theo-
rem 1 from different perspectives. Figure 2 plots the
learned parameters {γ(k) , θ(k) } in ALISTA (Stage 2),
showing that they satisfy the properties proposed in The-
orem 1: γ(k) bounded; θ(k) and γ(k) is proportional
to supχ* ∣∣χ(k)(x*) - x*∣∣ι (“supx*" is taken over the
test set). Figure 3 reports the average magnitude7 of
the false positives and the true positives in Xk (x*) of
→-false positives
—m—true positives
1.2 -∣
1	Tt ” XXXXXXXXXXXXXX
0.8 -
0.6 -
0.4 -
0.2 -
0 4~~)∣t~Ilt~~∣∣i~~y~~)∣t~~)∣t~~)∣t~~¥¥¥¥¥¥¥~~IIC
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Layer (⅛)
Figure 3: Justification of Theorem 1
ALISTA: the “true positives” curve draws the values (noiseless case): Proportion of false
of E{∣∣xk(x*)k2∕kxk(x*)k2} w.r.t. k (the expectation positives vs true positives in Xk(x*).
is taken over the test set), while “false positives” for
E{kχSc (χ* )k2∕kXk(x*)k2}∙ False positives take UP small proportion over the positives, which
supports the Theorem 1 conclusion that SuPPort(Xk (x*)) ⊂ S.
5.2 Validation of Theorem 3 (Convolutional Analytic LISTA)
For convolutional cases, we use real image data to verify Theorem 3. We train a convolutional
dictionary d with D = 7, M = 64 on the BSD500 training set (400 images), using the Algorithm 1
in (Liu et al., 2018). We then use it for problems (22) and (24) and solve them with different Ns.
In Table 2, we take WNr ∈ WNr, W ∈ W50 (consider 50 as large enough) For this example, WNr
has only one element. Table 2 shows that WNr = W for N ≥ 13, i.e., the solution of the problem
(24) is independent of N if N ≥ 2D - 1, justifying the first conclusion in Theorem 3. In Table
3, we take WNnv ∈ WJNOnv and W ∈ w^, where WNnv also has only one element. Table 3
shows WNnv → W, i.e., the solution of the problem (22) converges to that of (24) as N increases,
validating the second conclusion of Theorem 3. Visualized W ∈ w13. is displayed in Appendix F.
7The number and proportion of false alarms are a more straightforward performance metric. However, they
are sensitive to the threshold. We found that, although using a smaller threshold leads to more false alarms,
the final recovery quality is better and those false alarms have small magnitudes and are easy to remove by
thresholding during post-processing. That’s why we chose to show their magnitudes, implying that we get
easy-to-remove false alarms.
9
Published as a conference paper at ICLR 2019
Besides validating Theorem 3, we also present a real image denoising experiment to verify the
effectiveness of Conv ALISTA. The detailed settings and results are presented in Appendix H.
Table 2: Validation of Conclusion 1 in Theorem 3. D = 7. WNr ∈ WNr and W ∈ W5°.
kWNr - W*k3kW*k2
N =10	N=11	N = 12	N = 13	N = 15	N = 20
2.0 X 10-2-	9.3 × 10-3	3.9 × 10-3 -	1.4 × 10T―	8.8 × 10T―	5.9 × 10TL
Table 3: Validation of Conclusion 2 in Theorem 3. D = 7. WNnv ∈ WNonv and W ∈ w1ir.
kWNnv - W*k2/kW*k2
N = 3 N = 5 I^N =10 I N = 15
0.1892^^0.0850--0.0284	0.0161
N = 20
0.0113
(a) σmaχ = 0.02
(b) σmaχ = 0.03
Figure 4: Validation of Robust ALISTA.
5.3 Validation of Robust ALISTA
We empirically verify the effectiveness of Robust ALISTA, by sampling the dictionary perturbation
εD entry-wise i.i.d. from another Gaussian distribution N (0, σm2 ax). We choose σmax = 0.02 and
0.03. Other simulation settings are by default the same as in Section 5.1. We then build the Robust
ALISTA model, following the strategy in Section 4 and using a 4-layer encoder for approximating
its second step (see Appendix G for details). Correspondingly, we compare Robust ALISTA with
TiLISTA and ALISTA with specific data augmentation: we straightforwardly augment their training
sets, by including all data generated with randomly perturbed Ds when training Robust ALISTA.
We also include the data-free FISTA algorithm into the comparison.
Figure 4 plots the results when the trained models are applied on the testing data, generated with
the same dictionary and perturbed by N(0, σt). We vary σt from zero to slightly above σmax . Not
surprisingly, FISTA is unaffected, while the other three data-driven models all slight degrade as σt
increases. Compared to the augmented TiLISTA and ALISTA whose performance are both inferior
to FISTA, the proposed Robust ALISTA appears to be much more favorable in improving robustness
to model perturbations. In both σmax cases, it consistently achieves much lower NMSE than FISTA,
even when σt has slightly surpassed σmax . Although the NMSE of ALISTA may decrease faster
if σt continues growing larger, such decrease could be alleviated by improving σmax in training,
e.g., by comparing σmax = 0.02 and 0.03. Robust ALISTA demonstrates remarkable robustness and
maintains the best NMSE performance, within at least the [0, σmax] range.
6 Conclusions and Future Work
Based on the recent theoretical advances of LISTA, we have made further steps to reduce the train-
ing complexity and improve the robustness of LISTA. Specifically, weno longer train any matrix for
LISTA but directly use the solution to an analytic minimization problem to solve for its layer-wise
weights. Therefore, only two scalar sequences (stepsizes and thresholds) still need to be trained.
Excluding the matrix from training is backed by our theoretical upper and lower bounds. The re-
sulting method, Analytic LISTA or ALISTA, is not only faster to train but performs as well as the
state-of-the-art variant of LISTA by (Chen et al., 2018). This discovery motivates us to further re-
place the minimization algorithm by its unfolding neural network, and train this neural network to
more quickly produce the weight matrix. The resulting algorithm is used to handle perturbations
in the model dictionary — we only train once for a dictionary with all its small perturbations. Our
future work will investigate the theoretical sensitivity of ALISTA (and its convolutional version) to
noisy measurements.
10
Published as a conference paper at ICLR 2019
References
Thomas Blumensath and Mike E. Davies. Iterative hard thresholding for compressed sensing. Ap-
plied and Computational Harmonic Analysis, 27(3):265 - 274, 2009.
Mark Borgerding, Philip Schniter, and Sundeep Rangan. AMP-inspired deep networks for sparse
linear inverse problems. IEEE Transactions on Signal Processing, 65(16):4293-4308, 2017.
Hilton Bristow, Anders P. Eriksson, and Simon Lucey. Fast convolutional sparse coding. 2013 IEEE
Conference on Computer Vision and Pattern Recognition, pp. 391-398, 2013.
Rakesh Chalasani, Jose C Principe, and Naveen Ramakrishnan. A fast proximal method for con-
volutional sparse coding. In Neural Networks (IJCNN), The 2013 International Joint Conference
on, pp. 1-5. IEEE, 2013.
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of
unfolded ista and its practical weights and thresholds. arXiv preprint arXiv:1808.10038, 2018.
David L Donoho and Michael Elad. Optimally sparse representation in general (nonorthogonal)
dictionaries via l1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197-
2202, 2003.
Michael Elad. Optimized projections for compressed sensing. IEEE Transactions on Signal Pro-
cessing, 55(12):5695-5702, 2007.
Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over
learned dictionaries. IEEE Transactions on Image processing, 15(12):3736-3745, 2006.
Cristina Garcia-Cardona and Brendt Wohlberg. Convolutional dictionary learning: A comparative
review and new algorithms. IEEE Transactions on Computational Imaging, 2018.
Raja Giryes, Yonina C Eldar, Alex Bronstein, and Guillermo Sapiro. Tradeoffs between convergence
speed and reconstruction accuracy in inverse problems. IEEE Transactions on Signal Processing,
2018.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the
27th International Conference on International Conference on Machine Learning, pp. 399-406.
Omnipress, 2010.
Sheng Han, Ruiqing Fu, Suzhen Wang, and Xinyu Wu. Online adaptive dictionary learning and
weighted sparse coding for abnormality detection. In Image Processing (ICIP), 2013 20th IEEE
International Conference on, pp. 151-155. IEEE, 2013.
Felix Heide, Wolfgang Heidrich, and Gordon Wetzstein. Fast and flexible convolutional sparse
coding. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5135-
5143, 2015.
Daisuke Ito, Satoshi Takabe, and Tadashi Wadayama. Trainable ista for sparse signal recovery. 2018
IEEE International Conference on Communications Workshops (ICC Workshops), pp. 1-6, 2018.
Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, and Wotao Yin. Online convolutional dictio-
nary learning. 2017 IEEE International Conference on Image Processing (ICIP), pp. 1707-1711,
2017.
Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, and Wotao Yin. First-and second-order meth-
ods for online convolutional dictionary learning. SIAM Journal on Imaging Sciences, 11(2):1589-
1628, 2018.
Canyi Lu, Huan Li, and Zhouchen Lin. Optimized projections for compressed sensing via direct
mutual coherence minimization. Signal Processing, 151:45-55, 2018.
Christopher A Metzler, Ali Mousavi, and Richard G Baraniuk. Learned D-AMP: Principled neu-
ral network based compressive image recovery. In Advances in Neural Information Processing
Systems, pp. 1770-1781, 2017.
11
Published as a conference paper at ICLR 2019
Thomas Moreau and Joan Bruna. Understanding trainable sparse coding with matrix factorization.
In ICLR, 2017.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via
convolutional sparse coding. arXiv preprint arXiv:1607.08194, 2016.
Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. Convolutional dictionary learn-
ing via local processing. 2017 IEEE International Conference on Computer Vision (ICCV), pp.
5306-5314, 2017.
R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science &
Business Media, 2009.
Ron Rubinstein and Michael Elad. Dictionary learning for analysis-synthesis thresholding. IEEE
Transactions on Signal Processing, 62(22):5962-5972, 2014.
Pablo Sprechmann, Alexander M Bronstein, and Guillermo Sapiro. Learning efficient sparse and
low rank models. IEEE transactions on pattern analysis and machine intelligence, 37(9):1821-
1833, 2015.
Hillel Sreter and Raja Giryes. Learned convolutional sparse coding. In 2018 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2191-2195. IEEE, 2018.
Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad. Multi-layer convolutional sparse
modeling: Pursuit and dictionary learning. arXiv preprint arXiv:1708.08705, 2017.
Bahareh Tolooshams, Sourav Dey, and Demba Ba. Scalable convolutional dictionary learning with
constrained recurrent sparse auto-encoders. arXiv preprint arXiv:1807.04734, 2018.
Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Scalable online convolutional
sparse coding. IEEE Transactions on Image Processing, 2018.
Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, and Thomas S Huang. Learning a task-
specific deep architecture for clustering. In Proceedings of the 2016 SIAM International Confer-
ence on Data Mining, pp. 369-377. SIAM, 2016a.
Zhangyang Wang, Qing Ling, and Thomas Huang. Learning deep l0 encoders. In AAAI Conference
on Artificial Intelligence, pp. 2194-2200, 2016b.
Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, and Thomas S Huang. D3:
Deep dual-domain based fast restoration of jpeg-compressed images. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2764-2772, 2016c.
Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, and Thomas S. Huang. Learning a
deep '∞ encoder for hashing. In Proceedings of the Twenty-Fifth International Joint Conference
on Artificial Intelligence, pp. 2174-2180. AAAI Press, 2016d.
Brendt Wohlberg. Efficient convolutional sparse coding. 2014 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 7173-7177, 2014.
Brendt Wohlberg. Efficient algorithms for convolutional sparse representations. IEEE Transactions
on Image Processing, 25:301-315, 2016.
Brendt Wohlberg. Convolutional sparse representations with gradient penalties. In 2018 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6528-6532.
IEEE, 2018.
Bo Xin, Yizhou Wang, Wen Gao, David Wipf, and Baoyuan Wang. Maximal sparsity with deep
networks? In Advances in Neural Information Processing Systems, pp. 4340-4348, 2016.
Meng Yang, Weiyang Liu, Weixin Luo, and Linlin Shen. Analysis-synthesis dictionary learning for
universality-particularity representation based classification. In AAAI, pp. 2251-2257, 2016.
Hui Zhang and Lizhi Cheng. Restricted strong convexity and its applications to convergence analysis
of gradient-type methods in convex optimization. Optimization Letters, 9(5):961-979, 2015.
12
Published as a conference paper at ICLR 2019
Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable optimization-inspired deep network for
image compressive sensing. In IEEE CVPR, 2018.
Bin Zhao, Li Fei-Fei, and Eric P Xing. Online detection of unusual events in videos via dynamic
sparse coding. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on,
pp. 3313-3320. IEEE, 2011.
Joey Tianyi Zhou, Kai Di, Jiawei Du, Xi Peng, Hao Yang, Sinno Jialin Pan, Ivor W Tsang, Yong
Liu, Zheng Qin, and Rick Siow Mong Goh. SC2Net: Sparse LSTMs for sparse coding. In AAAI
Conference on Artificial Intelligence, 2018.
13
Published as a conference paper at ICLR 2019
A Proof of Theorem 1
In this proof, We use the notion x(k) to replace x(k) (x*) for simplicity. We fix D in the proof, μ(D)
can be simply written as μ.
Before proving Theorem 1, We present and prove a lemma.
Lemma 1. With all the settings the same with those in Theorem 1, we have
support(x(k)) ⊂ S, ∀k.	(27)
In another word, there are no false positives in x(k): xi(k) = 0, ∀i ∈/ S, ∀k.
Proof. Take arbitrary x* ∈ X(B, s). We prove Lemma 1 by induction. As k = 0, (27) is satisfied
since x(0) = 0. Fixing k, and assuming support(x(k)) ⊂ S, we have
xi(k+1) =ηθ(k) xi(k) -
Y(k)(W：,i)T(DXE- b))
=ηθ(k) ( - γ(k) X(w：,i)TD：,j(Xjk)- x*)),	∀i ∈ S.
j∈S
By (9), the thresholds are taken as θ(k) = μγ(k) supχ*{∣∣x(k) - x*kι}. Also, since W ∈ W(D),
we have |(W：,i)TD：j | ≤ μ for all j = i. Thus, for all i ∈ S,
θ(k) ≥μγ (k)||x(k) - X*∣∣ι = X	μγ (k)∣xjk) - x* ∣ = X μγ(k)∣χjk) - x* ∣
j∈support(x(k) )	j∈S
≥∣∣∣ - γ(k) X(W:,i)T D:,j (x(jk) -xj*)∣∣∣,
j∈S
which implies xi(k+1) = 0, ∀i ∈/ S by the definition ofηθ(k), i.e.,
support(x(k+1)) ⊂ S
By induction, (27) is proved.	□
With Lemma 1, we are able to prove Theorem 1 now.
Proof of Theorem 1. Take arbitrary x* ∈ X(B, s). For all i ∈ S, by (27), we obtain
X(k+1) = ηθ(k) (χ(k) - Y(k)(w：,i)TD：,S(XSk)-XS))
∈ Xk-YkkNdD：,s(xSk) - xS) - θ(k)∂'ι(χik+1)),
where ∂'ι(x) is the sub-gradient of |x|, X ∈ R:
d` (Q_ J{Sign(X)} if x = 0,
1	[-1, 1]	ifx = 0.
The choice of W ∈ W(D) gives (W:,i)TD:,i = 1. Thus,
xi(k) - γ(k) (W:,i)T D:,S(x(Sk) - xS*)
=xi(k) - γ(k) X (W:,i)TD:,j(x(jk) - xj*) - γ(k)(xi(k) -xi*)
j∈S,j6=i
=xi* - γ(k) X (W:,i)TD:,j(xj(k) - xj*) + (1 - γ(k))(xi(k) -xi*).
j ∈S,j6=i
Then the following inclusion formula holds for all i ∈ S,
x(k+1) - X* ∈ -γ(k) X (W：,i)TD：,j(Xjk)-X*) - θ(k)∂'ι(x(k+1)) + (1 - γ(k))(x(k) - x*).
j∈S,j6=i
14
Published as a conference paper at ICLR 2019
By the definition of ∂'ι, every element in ∂'ι(x), ∀x ∈ R has a magnitude less than or equal to 1.
Thus, for all i ∈ S,
昌k+I)-W | ≤ X Y(k) I (W:,厂DjIxjk)-WI + θ(k) +11 -付)I IMk)-Wl
j∈S,j=i
≤μγ(k) X Ixjk)-吗∣ + θ(k) + ∣ι-γ⑹I Ixik)-W ∣ .
j∈S,j=i
Equation (27) implies ||x(k) - x*∣∣ι = ∣∣xSk) - X和 ι forall k. Then
∣∣χ(k+1)-x*∣∣ι= X Ix(k+1) - WI
i∈S
≤ X(M(k) X Ixjk)-吟 + θ(k) + Ii-γ(k)IIx(k)-x")
i∈S	j∈S,j=i
=μγ(k)(ISI - 1) X Ix(k) - x* I + θ(k)ISI + Ii - Y(k)I|x(k) - x*kι
i∈S
=μγ(k)(ISI - 1)|x(k) - x*∣∣ι + θ(k)ISI + Ii - Y(k)I|x(k) - x*kι.
Taking supremum of the above inequality over x* ∈ X(B, s), by ISI ≤ s,
sup{|x(k+1) - x*kι} ≤ (μ7(k)(s - 1) + I1 - Y(k)I)sup{|x(k) - x*∣∣ι} + θ(k)s.
X*	χ*
By the value of θ(k) given in (9), we have
sup(|x(k+1) - x*kι} ≤(Y(k)(2μs - μ) + I1 - Mk)I)SUP{|x(k) - x*∣∣ι}.
Let C(T)= - log ((2"s - μ)γ(T)+ I1 - Y(T)} Then, by induction,
sup{||x(k+1) - x*∣∣ι} ≤ exp
kk
(-X C(T)) sup{kX(O)- x*kι} ≤ exp ( - X C(T))sB.
T=0	x*	T=0
Since ∣∣x∣∣2 ≤ ∣∣x∣∣ 1 for any X ∈ Rn, We can get the upper bound for '2 norm:
sup{|x(k+1) - x*∣∣2} ≤ sup{|x(k+1) - x*kι} ≤ sB exp
k
(-X C(T)).
T =0
The assumption s < (1 + 1∕μ)∕2 gives 2μs - μ < 1. If 0 < Y(k) ≤ 1, we have c(k) > 0. If
1 < Y(k) < 2/(1 + 2"s -谪,we have
(2“s - ”)Y(k) + I1 - Y(k) I = (2"s - ”)Y(k) + Y(k) - 1 < 1,
which implies c(k) > 0. Theorem 1 is proved.	□
B	Proof of Theorem 2
ProofofTheorem 2. We fix D and sample a x* 〜Pχ.
If we can prove
P((13) does not hold卜UPPOrt(X*) = S) ≤ ^网 + e|S|,	(28)
then the lower bound (13) in Theorem 2 is proved by
P ((13) holds) = X P ((13) holds ∣support(x*) = S)P (support(x*) = S)
S,2≤∣S∣≤s
≥(1 - es3/2 - e2) X P(support(x*) = S)
2≤∣S∣≤s
= 1-es3∕2-e2.
15
Published as a conference paper at ICLR 2019
Now we fix k and prove inequality (28) by three steps:
Step 1: If (13) does not hold, then What condition x* should satisfy?
Fixing k, We define a set X(k)(e), which involves all the x* that does not satisfy (13):
X (k)g = {(13) does not hold} = {x* ∣ ||x(k)(x*) - x*∣∣2 < e∣∣x*k2(察)k }.
Let S = support(x*). For x* ∈ X(k) (), we consider two cases:
1.	|x*| >d∣x*∣∣2(σmin∕3s)k, ∀i ∈ S.
2.	|x*| ≤ ekx* k2(σmin∕3s)k, for some i ∈ S.
If case 1 holds, we obtain that the support of x(k) is exactly the same with that ofx*:
support(x(k)(x*)) = S .
Then the relationship between x(k) and x(k-1) can be reduced to an affine transform:
x(Sk)=ηθ(k)x(Sk-1) - (W:(,kS-1))T (Dx(k-1) - b)
=x(Sk-1) - (W:(,kS-1))TD:,S(x(Sk-1) - xS*) - θ(k-1)sign(x(Sk)).
Subtracting x* from the two sides of (29), we obtain
II(I-(W(,ST))TD:,S)(XSkT)-xS) -θ(I)Sign(XSk))∣L = |思®) -xSk2 = kx(k) -x*k2,
where the last equality is due to Definition 3. Thus, for all x* ∈ X(k) (), if case 1 holds, we have
∣∣(I- (W(,ST))TD：,S)(XSkT)-xS) -θ(kτ)sign(xSk))∣∣2 ≤ d∣x*k2(σmin∕3s)k.	(30)
Multiplying both sides of (30) by (I- (W:(,kS-1))TD:,S)-1,we have
kx(Sk-1) -xS* - θ(k-1)(I - (W:(,kS-1))T D:,S)-1sign(x(Sk))k2
≤k(I- (W(,S-1))TD：,S)Tk2 ∙ d∣x*k2(σmm∕3s)k ≤ ekx*k2(σmin)k-13-ks,
where the last inequality is due to (11). Let X(k-1) denote the bias of x(k-1):
X(I) , Θ(I)(I-(W(,S-1))TD：,S)TSign(XSk)),
then we get a condition that x* satisfies if case 1 holds:
X(I)(e) = {x*∣∣∣XSkT)(x*) - xS - X(kτ)(x*)∣∣2 ≤ ekx*k2(σmin)kT3-ks}.
If case 2 holds, x* belongs to the following set:
X(k)(e) = {x*"*∣ ≤ e∣∣x*k2(σmin∕3s)k, forsome i ∈ s}.
Then for any x* ∈ X(k)(e), either x* ∈ X(k-1)(e) or x* ∈ X(k)(e) holds. In another word,
X(k)(e) ⊂ X(k)(e) ∪X(k-1)(e).
Step 2: By imitating the construction of X(k) (), We construct
X(k-2) (e), X (k-3)(e), ∙∙∙.
Similar to Step 1, we divide X(k-I)(E) into two sets: X(k I)(E) and X(k-2)(e), then we divide
X(k-2)(e) into X(k 2)(e) and X (k-3)(e). Repeating the process, until dividing X(I)(E) into X(I)(E)
and X(0) (E).
16
Published as a conference paper at ICLR 2019
By induction, We have
X (fc)(e) U X (fc)(e) U X (kτ)(e) U X(k-2)(e) U∙∙∙U X(I)(e) U X(0) (e),	(31)
where the sets are defined as follows for all j = 0,1, 2, ∙∙∙ , k:
X (I)(e)={x*"; + X(I)(X*)| <e∣∣x*k2(σmin)I3is, forsome i ∈ S.},	(32)
X(I)(e) ={x* 卜IXSl)(x*) - XS - X(I)(X*)∣∣2 ≤ ekx*∣∣2(σmin)I3is}	(33)
and the bias is defined as following for all j = 0, 1, 2, ∙ ∙ ∙ , k :
X(I)(X*) = XX(I - (w(,S-j+tτyD：,s)	θ(k-j+tT)Sign(XSi+t)(x*)).	(34)
Step 3: Estimating the probabilities of all the sets in (31).
By (31), we have
P(x* ∈ X(fc)(e)卜UPPOrt(x*) = S)
k-1
≤ EP(x* ∈ X(I)(C)卜UPPOrtX) = S) + P(x* ∈ X(O)(C)卜UPPOrtX)= S).
j=ι
Now we have to prove that each of the above terms is small, then P(x* ∈ X(k) (c) ∣support(x*) = S)
is small and (28) will be proved.
Define a set of n-dimensional sign numbers
Si(n) = {(
si,S2,∙∙∙
Sn) j si ∈ {0, -1, 1},∀i = 1, ∙∙∙ , nθ .
Since Sign(XSk-j+t)) ∈ Si(∣S∣) for all t = 1,2, ∙∙∙ , j, {sign(xSk-j+")}3 has finitely possible
values. Let Sign(XSk-j+t)) = s(t) for t = 1,2, ∙∙∙ , j. Then X(k-j)(x*) is independent of x* and
can be written as X(k-j)(S(I), s(2), ∙∙∙ , s(j)). Thus, we have
P(x* ∈ X(k-j)(c)|support(x*) = S)
∙∙∙
i∈S s(1)∈Si(∣S∣) s(2)∈Si(∣S∣)	sj)∈Si(∣S∣)
P (Iw + X(k-j)(x*)| < C∣∣x*k2(σmin)k-j3-ks, Sign(XSk))= s(1), ∙∙∙ , Sign(XSk-j+1))= s(j)卜UPPOrt(X*)
S
≤Σ Σ Σ ∙∙∙ E
i∈S s(1)∈Si(∣S∣) s(2)∈Si(∣S∣)	s(j)∈Si(∣S∣)
P (Iw + x(k-j)(S(I), s(2), ∙∙∙ , s(j))| < e√∣S∣B(σmin)k-j 3-ks ∣ support(x* ) = S)
≤
i∈S s(1)∈Si(∣S∣) s(2)∈Si(∣S∣)
Σ
sj)∈Si(∣S∣)
C 行 B(σmin)k-j 3-ks
B
= |S|3j ∣ s ∣(c√jsj((σmin)k-j3-ks) ≤ c|S|3/2(amin)k-j3(j-k) ∣ s ∣
where the second inequality comes from the uniform distribution of XS (Assumption 2), the last
inequality comes from |S| ≤ s.
17
Published as a conference paper at ICLR 2019
The last term, due to the uniform distribution of XS and x(0) = 0, can be bounded by
P(x* ∈ X⑼(e)∣support(x*) = S)
=P(kx* + X⑼(x*)∣∣2 ≤ ekx*∣∣23-ks卜UPPort(X*) = S)
=X X -X
s(1)∈Si(∣S∣) s(2)∈Si(∣S∣)	s(k)∈Si(∣S∣)
P(kx* + X(O)(X*)∣∣2 ≤ d∣x*k23-ks, Sign(XSI))= S(I),…，Sign(XSk))= s(k)∣support(x*) = S)
≤3klSl ((e3-ks)lSl) ≤ e1S1.
Then we obtain
P(x* ∈ X(k) (e)∣support(x*) = S)
k-1	k
≤ X 啃|3/2年血口产-30-)网 + JSI= X ¥|3/2(bmin)j3-j1S1 + *|
j=0	j=1
=e|S|3/2 1 -mn3]—1S|(1 - (σmin3-lSl)k) + e1S1 ≤ e|S|3/2 + ^S∣
Then (28) is proved.	□
C Proof of Theorem 3
There are two conclusions in Theorem 3. We prove the two conclusions in the following two sub-
sections respectively.
C.1 Proof of Conclusion 1.
Before proving Conclusion 1, we analyze the operator DcNir in detail.
The circular convolution (23) is equivalent With:
N-1 N-1 M
b(i, j) =XXX
DcNir(i, j; k, l, m)xm(k, l),	0 ≤ i, j ≤ N - 1,
k=0 l=0 m=1
Where the circulant matrix is element-Wise defined as:
CN「 . j ]	、_ ʃ dm ((k -	i)modN, (l	- j )modN) ,	0 ≤ (k -	i)modN,	(I - j)modN ≤ D - 1
Dcir(i, j; k, l, m) = 0,	others
(35)
Similarly, the corresponding circulant matrix WcNir(i,j; k, l, m) of dictionary w is:
^W^ N ( ' l> w ʌ _ J"Wm((k	- i)modN,	(l - j)modN) ,	0 ≤ (k	- i)modN, (I	- j )modN	≤ D - 1
Wcir(i, j; k, l, m) = 0,	others
(36)
As We defined in Section 3, b is a vector. With X = [xι,…,XM]T, X is a vector. Then the operator
DcNir is a matrix, where (i, j) is its row index and (k, l, m) is its column index.
Define a function measuring the difference betWeen i and k:
I(i, k), (k - i)modN,	0≤i,k≤N-1.
The coherence betWeen DcNir(i,j; k, l, m) and WcNir(i, j; k, l, m): Bcoh = (DcNir)TWcNir is element-
Wise defined by:
N-1 N-1
Bcoh(k1, l1, m1; k2, l2, m2) =	DcNir(i,j; k1, l1, m1)WcNir(i,j; k2, l2, m2)
i=0 j=0
=X Xdmι (I(i, kι),I(j, l1))wm2 (I(i, k2),I(j, l2)).
i∈I(k1 ,k2) j∈J (l1 ,l2)
18
Published as a conference paper at ICLR 2019
where
I(k1,k2) = {i∣0 ≤ i ≤ N — 1, 0 ≤ I(i, kι) ≤ D - 1, 0 ≤ I(i, k2) ≤ D - 1},
J(l1,l2) = {j|0 ≤ j ≤ N — 1, O ≤ I(j,lι) ≤ D - 1, O ≤ I(j,l2) ≤ D -1}.
Lemma 2. Given N ≥ 2D - 1, it holds that:
(a)	I (k1,k2) = 0 ifandonlyif“ O ≤ (kι- k2)modN ≤ D — 1 ”or “ 0 < Ik — kι)modN ≤ D — 1”
holds.
(b)	J(l1, l2) 6= 0 if and only if“ O ≤ (l1 — l2)modN ≤ D — 1” or “ O < (l2 — l1)modN ≤ D — 1”
holds.
Proof. Now we prove Conclusion (a). Firstly, we prove “if.” If O ≤ (k1 — k2 )modN ≤ D — 1 and
N ≥ 2D — 1, we have
I (k1,k2) = {(kι — δ)modN δ ∈ Z, (k1 — k2)modN ≤ δ ≤ D - 1} = 0.	(37)
If O < (k2 — k1 )modN ≤ D — 1 and N ≥ 2D — 1, we have
I(k1,k2 ) = {(k2 — δ)modN δ ∈ Z, (k2 — k1)modN ≤ δ ≤ D - 1} = 0.	(38)
Secondly, we prove “only if.” If I(k1, k2) 6= 0, we can select an i ∈ I(k1, k2). Let r1 = (k1 —
i)modN and r2 = (k2 — i)modN. By the definition of I(k1, k2), we have O ≤ r1, r2 ≤ D — 1. Two
cases should be considered here. Case 1: r1 ≥ r2. Since O ≤ r1 — r2 ≤ D — 1 ≤ N — 1, it holds
that r1 — r2 = (r1 — r2)modN. Thus,
r1 — r2 = (r1 — r2 )modN = (k1 — i)modN — (k2 — i)modN modN
= ((k1-"-(k2 -")modN
=(k1 — k2)modN.
The equality “O ≤ r1 — r2 ≤ D — 1” leads to the conclusion “O ≤ (k1 — k2)modN ≤ D — 1”. In
case 2 where r1 < r2, we can obtain O < (k2 — k1)modN ≤ D — 1 with the similar arguments.
Conclusion (b) can be proved by the same argument with the proof of (a). Lemma 2 is proved. □
Now we fix k1, l1 and consider what values of k2, l2 give I(k1, k2) 6= 0 and J(l1, l2) 6= 0. Define
four index sets given O ≤ k1, l1 ≤ N — 1:
K(k1) ={k|O ≤ (k1 — k)modN ≤ D — 1}
K(kl) ={k∣0 < (k — kl)modN ≤ D - 1}
L(l1) ={l|O ≤ (l1 — l)modN ≤ D — 1}
C(l1)={l∣0 < (l — li)modN ≤ D - 1}
Lemma 3. IfN ≥ 2D — 1, we have:
(a)	The cardinality of K(kι), K(kι)： | K(kι)∣ = D, | K(kι)∣ = D 一 1.
(b)	K(kι) ∩K(kι) = 0.
(c)	The CardinaIity of L(lι), L(lι): | L(lι)∣ = D, | L(lι)∣ = D — 1.
(d)	L(lι) ∩L(lι) = 0.
Proof. Now we prove Conclusion (a). The set K(k1) can be equivalently written as
K(kI) = {(k1 — rk )modN Irk = 0, 1,…，D — 1}	(39)
Let k(rk) = (k1 — rk)modN. We want to show that k(rk1) 6= k(rk2) as long as rk1 6= rk2. Without loss
of generality, we assume O ≤ rk1 < rk2 ≤ D — 1. By the definition of modulo operation, There exist
two integers q, q0 such that
k(rk1) = qN+ k1 — rk1, k(rk2) = q0N+ k1 — rk2.
19
Published as a conference paper at ICLR 2019
Suppose k(rk1) = k(rk2). Taking the difference between the above two equations, we obtain rk2 -
rk1 = (q0 - q)N, i.e, N divides rk2 - rk1. However, 0 ≤ rk1 < rk2 ≤ D - 1 implies 1 ≤ rk2 - rk1 ≤
D - 1 ≤ N - 1, which contradicts with “N dividing rk2 - rk1.” Thus, it holds that k(rk1) 6= k(rk2).
Then we have | K(k1)| = D.
In the same way, we have
K(kI) = {(k1 + rk )modN |rk = 1, 2L-D - 1}	(4O)
and ∣K(kι)∣ = D - 1. Conclusion (a) is proved.
Now We prove Conclusion (b). Suppose K(kι) ∩ K(kι) = 0. Pick a k2 ∈ K(kι) ∩ K(kι). Let
r3 = (k1 - k2)modN and r4 = (k2 - k1)modN. Then we have 0 ≤ r3 ≤ D - 1 and 0 < r4 ≤ D - 1.
By the definition of modulo operation, There exist two integers q, q0 such that
k1 - k2 = qN + r3,	k2 - k1 = q0N + r4
which imply
r3 + r4 + (q + q0)N = 0.
However, 0 < r3 +r4 ≤ 2D - 2 contradicts with “q ∈ Z, q0 ∈ Z, N ∈ Z, N ≥ 2D - 1.” Conclusion
(b) is proved.
Conclusions (c) and (d) are actually the same with Conclusions (a) and (b) respectively. Thus, it
holds that
L(II)={(II- rl)modN lrl = 0, 1, …,D - 1}	(41)
L(II) ={ (II + rl)modN |rl = 1, 2, …,D - 1}	(42)
and | L(lι)| = D, | L(lι)∣ = D - 1. Lemma 3 is proved.	□
With the preparations, we can prove Conclusion 1 of Theorem 3 now.
ProofofTheorem 3, Conclusion 1. Firstly we fix kι ∈ {0, 1, …，N - 1} and consider k? ∈ K(kι).
Let rk = (k1 - k2)modN. Then equation (37) implies that, for any i ∈ I(k1, k2), there exists a δ
(rk ≤ δ ≤ D - 1) such that
I(i, k1) =	k1	- (k1	- δ)modN	modN = (δ)modN = δ,
I(i, k2) =	k2	- (k1	- δ)modN	modN = (δ - rk)modN =	δ - rk .
(43)
Now we consider another case for k2: k? ∈ K(kι), rk = (k2 - kι)modN. Equation (38) implies
that, for any i ∈ I(k1, k2), there exists a δ (rk ≤ δ ≤ D - 1) such that
I (i, k1 ) =	k1	- (k2	- δ)modN	modN = (δ - rk )modN =	δ - rk ,
I(i, k2) =	k2	- (k2	- δ)modN	modN = (δ)modN = δ.
(44)
Similarly, for any lι ∈ {0, 1, ∙∙∙ ,N - 1} and l2 ∈ L(II) we denote r = (lι - l2)modN. For any
j ∈ J (l1, l2), there exists a δ (rl ≤ δ ≤ D - 1) such that
I(j, l1) = l1 - (l1 - δ)modN modN = (δ)modN = δ,
I(j, l2 ) =(l2 - (II - δ)modN)modN = (δ - rl )modN = δ - rl∙
(45)
Another case for l2: l2 ∈ L(lι), r = (l2 - lι)modN. For any j ∈ J(l1,l2), there exists a δ
(rl ≤ δ ≤ D - 1) such that
I(j,lI) =(l1 - (l2 - δ)modN"odN = (δ - rl)modN = δ - rl,
I(j, l2) = l2 - (l2 - δ)modN modN = (δ)modN = δ.
(46)
Now let us consider the following function. By results in Lemmas 2 and 3, we have
f(k1,l1,m1,m2
N-1 N-1
k2=0 l2=0
(k1,l1,m1;k2,l2,m2
2
=f1 + f2 + f3 + f4,
20
Published as a conference paper at ICLR 2019
where
f1 =	X X	Bcoh(k1,l1,m1; k2, l2, m2)
k2 ∈K(k1) l2 ∈L(l1)
f2 =	X X	Bcoh(k1,l1,m1; k2,l2,m2
k2∈K(kι) l2∈L(l1)
f3 =	X	X	Bcoh(k1,l1,m1; k2,l2,m2
k2∈K(kι) l2∈L(l1)
f4 =	X	X	Bcoh(k1,l1,m1; k2,l2,m2
k2∈冗(kι) l2∈L(l1)
Combining equations (39), (41), (43) and (45), we obtain
2
2
2
D-1 D-1 D-1 D-1	2
f1=XX X X	dm1
(δk, δl)wm2 (δk - rk, δl - rl)	.
rk=0 rl=0 δk =rk δl =rl
Combining (40), (41), (44) and (45), we obtain
D-1 D-1 D-1 D-1	2
f2=XX X X dm1(δk- rk , δl )wm2 (δk , δl - rl )	.
rk=1 rl=0 δk =rk δl =rl
Combining (39), (42), (43) and (46), we obtain
D-1 D-1 D-1 D-1	2
f3=XX X X dm1(δk,δl- rl )wm2 (δk - rk , δl ) .
rk=0 rl=1 δk =rk δl =rl
Combining (40), (42), (44) and (46), we obtain
D-1 D-1 D-1 D-1	2
f4=XX X X dm1(δk- rk , δl - rl )wm2 (δk , δl )	.
rk=1 rl=1 δk =rk δl =rl
By the above explicit formulas of fi, 1 ≤ i ≤ 4, we have f1, f2, f3, f4 are all independent of
k1 , l1 and N. They are only related with m1 , m2 for fixed d and m. Thus, we are able to denote
f(k1, l1, m1, m2) as f(m1, m2) for simplicity. Consequently,
N-1 N-1 N-1 N-1 M M	2
N2k(DNr)TWNrkF = N XXXXX X (Bcoh(k1,l1,m1; k2,l2,m2))
k1=0 l1=0 k2=0 l2=0 m1=1 m2=1
1 N-1 N-1 M M
=N EE EE f(kι,lι,mι,m2)
k1=0 l1=0 m1=1 m2=1
N-1 N-1 M M
=N12 XXX X f(m1,m2)
k1=0 l1=0 m1=1 m2=1
MM	MM
=N2 ∙ N2 ∙ X X f (m1,m2) = X X f(m1,m2)
m1=1 m2=1	m1 =1 m2=1
Thus, N12 Il(DNr)TWNrIlF is dependent of N:
急k(DNr)TWNrkF = MIn2 k(D2DT)TW2iDTkF, ∀N ≥2D - 1,	(47)
N	(2D - 1)
which implies W Nir = W 2D-1,∀N ≥ 2D — 1.	□
21
Published as a conference paper at ICLR 2019
C.2 Proof of Conclusion 2.
Before proving Conclusion 2, let us analyze the relationship between DcNonv and DcNir+D-1.
Similar to Dcir, we use (i, j) as the row index and (k, l, m) as the column index of Dconv. For
0 ≤ i,j ≤ N - 1,1 ≤ m ≤ M,
dm(k - i, l - j), 0 ≤ k - i, l - j ≤ D - 1
0,
k, l taken as others
(48)
Matrix DcNir+D-1 is of dimension (N +D- 1)2 × (N+D- 1)2M,where0 ≤ i,j ≤ N+D-2;
matrix DcNonv is of dimension (N)2 × (N + D - 1)2M, where 0 ≤ i, j ≤ N - 1. Thus, DcNonv is a
block in DcNir+D-1, i.e.,
DN+D-1
cir
DN
conv
∆N
∆D
The matrix ∆ND is of dimension ((N + D - 1)2 - N2) × (N + D - 1)2M:
∆ND = DcNir+D-1(i, j;:,:,:),	(i,j) ∈I∆
where
I∆ =I1∪I2∪I3
I1 ={(i,j)|N ≤i≤N+D-2, 0≤j ≤N-1}
I2 ={(i, j)|0 ≤ i ≤ N - 1, N ≤ j ≤ N + D - 2}
I3 ={(i, j)|N ≤ i ≤ N + D - 2, N ≤ j ≤ N + D - 2}.
Similarly,
N
WcNir+D-1 = W∆cNonv ,	∆NW= WcNir+D-1(i, j;:,:,:),	(i, j) ∈I∆.
∆W
Then,
(DcNir+D-1)T WcNir+D-1 = (DcNonv)TWcNonv + (∆ND)T∆NW.	(49)
Lemma 4. For any (i, j) ∈ I∆, one has
kDcNir+D-1(i, j;:,:,:)k22 =kdk22,	(50)
kWcNir+D-1(i, j;:,:,:)k22 =kwk22.	(51)
Proof. Equation (35) implies that, for (i, j) ∈ I1, 1 ≤ m ≤ M,
(dm(k - i,l - j),	i ≤ k ≤ N + D - 2,j ≤ l ≤ j + D - 1
DcNir+D-1(i, j; k, l, m) = dm(k-i+N+D- 1,l-j), 0 ≤k ≤ i-N,j ≤ l ≤j+D- 1
10,	k, l taken as others
Thus, for any (i, j) ∈ I1,
N+D-2 N+D-2 M
kDcNir+D-1(i,j;:,:,:)k22= X X X DcNir+D-1(i,j;k,l,m)2= kdk22
k=0 l=0 m=1
Similarly,
kDcNir+D-1(i,j;:,:,:)k22= kdk22, (i,j) ∈I2∪I3.
Equation (50) is proved. With the same argument, equation (51) is also proved.	口
Lemma 5. If N ≥ 2D - 1, we have
k(∆Ν)T∆WkF ≤ (2N(D - 1) + (D - 1)2)(2D - 1)2kdk2kwk2.	(52)
22
Published as a conference paper at ICLR 2019
Proof. For simplicity, we denote two row vectors:
di,j , DN+D-1(i,j;:,:,:) ∈ R1×(N +D-1)2M
we,，WN+D-1(i,j;:,:,:) ∈ R1×(N+D-1)2M
Then,
2
k(∆ND)T∆NWk2F =	X diT,jwi,j	= X X	DdiT1,j1wi1,j1,diT2,j2wi2,j2EF,
(i,j)∈I∆	F	(i1,j1)∈I∆ (i2,j2)∈I∆
where
(dT,jl wil,jl , dT,j2 wi2,j2 F= = trace (wT,jl dil,jl dT,j2 W，2 ,j2 ) = (dil,jl	，(Wi1,jl Mj
Since
N-1 N-1 M
di1,j1diT2,j2 = X X X = DcNir+D-1(i1, j1; k, l, m)DcNir+D-1(i2, j2; k, l, m),
k=0 l=0 m=1
with the same argument in Lemma 2, we have: di1,j1diT,j 6= 0 implies
i2 ∈ I0∆ , {i|0 ≤ (i1 - i)mod(N +D-1) ≤ D - 1 or 0 ≤ (i - i1)mod(N+D-1) ≤ D - 1}
j2 ∈ J0∆ , {j |0 ≤ (j1 - j)mod(N+D-1) ≤ D - 1 or 0 ≤ (j - j1)mod(N+D-1) ≤ D - 1}
Then
HOD )T δW k= =	X	X	X	(dii,ji dT,j2 )	∙ (Wi1,j1 wIj )
(i1,j1 )∈I∆ i2∈I0∆ j2∈J0∆
≤	X	X	X	kdk22kwk22
(i1,j1 )∈I∆ i2∈I0∆ j2∈J0∆
=|I ∆ ∣∙∣I ∆ HJ ∆ Hldk2kwk2
=(2N(D - 1) +(D - 1)2)(2D - 1)2∣∣dk2kwk2,
where the inequality in the second line follows from (50) and (51). Inequality (52) is proved. □
With these preparations, we can prove Theorem 3, Conclusion 2 now.
Proof of Theorem 3, Conclusion 2. Define set
W normal = {w ∈ RD2M 卜 m ∙ dm = 1, ∀m =1,…，M}.	(53)
Since d ∈ Wnormal , the set is nonempty:
W normal = °∙	(54)
Define functions FcNonv : RD2M → R, FcNir : RD2M → R.
FNnv(W) = N + D - 1 II(DNnv(d))T WNnv(W “= + IW normal (W)
砒(W) = NkDNr(d))T WNr(W) Il + IWnormal (w)
By the definitions of WcNonv , WcNir, we have
WcNonv = arg min FcNonv (W), WcNir = arg min FcNir(W)
23
Published as a conference paper at ICLR 2019
Step 1: Proving FcNonv(w) uniformly converges to Fc2iDr -1(w) on X ∩ Wnormal for any compact
setX ⊂ RD2M.
We arbitrarily choose such a compact set X. Based on (47), (49) and (52), one has, for all w ∈
X ∩ Wnormal,
|FcNonv(w) - Fc2iDr -1(w)| =|FcNonv(w) - FcNir+D-1(w)|
1
N + D - 1
(DcNir+D-1)TWcNir+D-1F -	(DcNonv)TWcNonvF
≤ nt⅛--i UOD)T ∆W UF
≤
J(2N(D - 1) + (D - 1)2)(2D - 1)
N + D - 1
kdk2kwk2
≤ (2D - 1),2(D - 1)
一	√N + D - 1
kdk2kwk2.
Thus, there exists a constant B > 0, which is independent of N, such that
B
IFNnv(W)- FciD 1(w)l ≤ F SUp	l∣wk,	∀ W ∈ X ∩ W normal.	(55)
N w∈X∩Wnormal
Step 2: Proving FcNonv(W) epigraphically converges8 to Fc2iDr -1 (W).
We want to show, at each point W it holds that
lim inf FcNonv(WN) ≥ Fc2iDr -1(W) for every seqUence WN→ W	(56)
N→∞	cir
lim sUp FcNonv (WN) ≤ Fc2iDr -1(W) for some seqUence WN→ W	(57)
N→∞
Firstly, we prove (56). We arbitrarily pick a sequence {WN}N∞=0 such that WN → W.
If W ∈/ Wnormal, Fc2iDr -1(W) = +∞. Since W normal is a closed set, there exists a N+ such that
WN∈/ W normal for all N ≥ N+. Thus, one has FcNonv (WN) = +∞ for all N ≥ N+, i.e.,
lim inf FcNonv(WN) = Fc2iDr -1(W) = +∞.
N→∞
If W ∈ Wnormal , two cases should be considered. The first case is that any subsequences of
{WN}N∞=0 are not kept within Wnormal, i.e., there exists a N+ such that WN ∈/ Wnormal for
all N ≥ N+. Then we have
lim inf FcNonv(WN) = +∞ > Fc2iDr -1(W).
N→∞
The second case is that there exists a subsequence {WNk}k∞=0 ⊂ {WN}N∞=0 such that
WN ∈W normal,	∀k = 0,1, 2,….
Since WN converges to W, any subsequences should be Cauchy. Given any Cauchy sequence
{WNk}k∞=0 in finite dimensional Euclidean space, there exists a compact set X such that
WNk ∈ X, ∀k = 0,1, 2,…
Let B0 = sUpw∈X∩Wnormal lWl. By (55), we obtain
∣FNnv(wNk) - FciD-1(w)l ≤∣FNnv(wNk) - FciD T(WNk )I + FiD T(WNk) - FciD-1(w)l
≤√NB0 + IFciD T(WNk) - FciD-1(W)∣.
8Epigraphic convergence is a standard tool to prove the convergence of a sequence of minimization prob-
lems. The definition of epigraphic convergence refers to Definition 7.1 and Proposition 7.2 in (Rockafellar &
Wets, 2009).
24
Published as a conference paper at ICLR 2019
For any > 0, by the continuity of Fc2iDr -1, We are able to find a K > 0 such that |Fc2iDr -1(WNk) -
Fc2iDr -1 (W)| < for all k ≥ K. Pick a K0 such that NK0 ≥ (BB0/)2. Then, for all k ≥
max(K, K0), We have |FcNonkv(WNk) - Fc2iDr -1(W)| < 2, i.e.,
lim FcNonkv(WNk) = Fc2iDr -1(W).
k→∞
The above conclusion holds for all subsequences {WNk}k∞=0 ⊂ Wnormal. Fc2iDr -1(W) is an accu-
mulation point of {FcNonv(WN)}N∞=0. All the other accumulation points of {FcNonv (WN)}N∞=0 must
be +∞ because FcNonv(W) = Fc2iDr -1(W) = +∞ for all W ∈/ Wnormal. Thus,
lim inf FcNonv(WN) = Fc2iDr -1 (W) < +∞.
N→∞
Secondly, We prove (57). We set WN = W for all N = 0,1, 2,….Then (57) is a direct result of
(55).
Step 3: proving (25). Define
G(W) = (Dc2iDr-1)TWc2iDr-12F.
We want to show that G(W) is strongly convex.
Let Wi ∈ R(2D-1)2 be the ith column of W2D-1, i.e.,
W2iDT= [w 1, W2,…，W(2D-1)2Mi
Then
(2D-1)2M
G(W)=	X	(Wi)T(d2D-1(d2DT)T)wi.
i=1
Let W ∈ r(2DT)4M vectorize W^-1, i.e.,
W = [(WI)T, (W2)T,…，(W(2D-1)2M)Ti .
Then G(w) can be written as a quadratic form of W:
G(w) = WT QW,
Where
Dc2iDr-1(Dc2iDr-1)T
Q
|
M7-1(d2D T)T)
}
'∙^^^^^^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
totally (2D-1)2M diagonal blocks
As long as at least one of the matrices {D2D,-1,…，D2DM-ι} is non-singular, D2DDT is full row
rank, Which implies that Dc2iDr-1(Dc2iDr-1)T is non-singular. Then Q is positive definite.
The transform between W and W is linear. We denote the transform as T, i.e.,
W = T w.
It,s trivial that ∣∣W∣∣2 = 0 implies ∣∣W2DTkF = 0∙ By the definition of W弋D-1, kW：DTkF = 0
implies kWk22 = 0. Thus, linear operator T is full column rank. Thus, TTQT is positive definite,
and
G(W) = WT(TTQT)W
is strongly convex. Then FC^D-1(w) = ,G(w)+∣wnormaι (w) has only one minimizer, i.e., W2DT
involves only a unique element.
Now we check the conditions of Propositions 7.32(c) and 7.33 in (Rockafellar & Wets, 2009) to
apply them.
25
Published as a conference paper at ICLR 2019
1.	FcNonv -→e Fc2iDr -1 . This is proved in Step 2.
2.	Fc2iDT is level bounded. Since G(W) is strongly convex, FciDT(W) = ,G(w) +
ιWnormal (w) must be level bounded.
3.	FcDT ≡ +∞. Since Wnormal is nonempty (54), dom F2DT = 0, FcDT is not Con-
stantly +∞.
4.	All the level set of FcNonv are connected. This can be derived from the convexity of FcNonv .
5.	FcciDr -1 and FcNonv are all lower semi-continuous and proper. This condition follows from
the fact that the functions FcciDr -1 and FcNonv are all continuous functions defined on a
nonempty closed convex domain Wnormal .
Applying Proposition 7.32(c), we have {FcNonv} is eventually level bounded. If we arbitrarily pick
a WN ∈ WcNonv and let Wcir be the unique point in WcciDr -1. Applying Proposition 7.33, we have
WN → Wcir . By Definition 4.1 in (Rockafellar & Wets, 2009), we obtain the convergence of the
SeqUenCe of sets {WNm}: IimN →∞ W Nmv = W 2D-1.	□
D Discussion of Definition 2 (11)
In this section, We want to numerically jhow that, given typical D and s, there is a 行而口 > 0 SUCh
that a random generated matrix W ∈ W(D, s, σmin). However, given D and W, it,s intractable to
completely check (11):
σmin(I - (W:,S)TD：,s) ≥ σmin, ∀ S With 2 ≤ | S | ≤ s.
The reason is that there are extremely large amount of possible S s. For example, we take M =
250, N = 500, s = 50. There are totally
500	500	500
(50) + (49)+ …+(2)
possible Ss satisfying 2 ≤ | S | ≤ s. It’s impossible to check (11) on all possible Ss.
Instead of checking all possible Ss, we sample 5000 Ss from the whole set:
S0 ⊂ S = {S : S ⊂{1, 2, ∙∙∙ , 500}∣2 ≤ |S| ≤ s},
where S0 is the set of all the samples. Then we estimate 行血口 with the following quantity:
σ(D, W) = min {σmm(I - (W：,S)TD：,s)}
Furthermore, we use the same D as that in Section 5 and generate 1000 Ws with each entry i.i.d
sampled from the normal distribution. Then we normalize each column of the generated Ws. This
technique is commonly used in sparse coding. Finally, we report the distribution of σ0(D, W) with
the fixed D and the 1000 sampled Ws in Figure 5.
Figure 5 demonstrates that, with the fixed D, most of the random generated Ws have a σ0(D, W)
within the interval [0.25, 0.35]. Thus, the numerical results support our claim: with high probability,
a random generated W satisfies
min {σmin(I - (W：,S)TD：,S) ≥ ≥ σmi∏ > 0,
that is, W ∈ W(D,s,σmin).
E efficient algorithm to calculate analytic weights
E.1 An efficient algorithm to solve (16)
In this section, we introduce an algorithm to solve (16) (we copy (16) below to facilitate reading):
min JlWTD∣∣F, s.t. (W：,m)TD：,m = 1, ∀m = 1, 2,…，M,
W∈RN ×M	F
26
Published as a conference paper at ICLR 2019
Figure 5: Discussion of Definition 2: distribution of σ0 (D, W) on random generated Ws.
By the definition of the Frobenius norm, it holds that
kWT Dk2F = k(WT D)T k2F = kDT Wk2F.	(58)
Thus, the above problem is equivalent with
min	IlDTW∣∣F,	s.t. (D：,m)TW：,m = 1, ∀m = 1, 2,…，M.
W∈RN ×M	F
We apply projected gradient descent (PGD) to solve the above problem. The gradient of kDT Wk2F
is W∣DtWkF = DDTW. Denote the Setby
W = {W ∈ RN×M|(D：,m)TW：,m = 1, ∀m = 1, 2,…，M.}
Then the projection onto W can be calculated by
PrOjW(W) = W + ∆W, ∆W = [(1 - (D：J)TW：,i)W：,i,…，(1 - (D：,M)TW：,m)W：,M]
With these formulas, we are able to write down the PGD, which is listed in Algorithm 1.
Algorithm 1: Projected gradient descent for solving (16)
Input: Dictionary D ∈ RN×M.
Initialize: Let W0 = D.
1	for j = 0, 1, 2, . . . until convergence do
2	J Update W by Wj+1 = ProjW (Wj - ηD(D)TWj).
3	end
Output: WJ, where J is the last iterate.
In each step, calculating the gradient has the complexity of O(N2M) because DDT can be pre-
computed. Calculating the projection takes O(NM) time consumptions. Due to the objective func-
tion to minimize in (16) is restricted strongly convex, Algorithm 1 is linear convergent (Zhang &
Cheng, 2015). To get an -accurate solution, PGD takes O(log(1/)) steps. Thus, the complexity of
Algorithm 1 is O(log(1/)N 2M). We should note that the bounds given in Table 1 are the number
of parameters to train, not the training complexity. The training complexity can be estimated by
“Number of iterations × complexity of back-propagation”, i.e., O(IBKNM),where I is the num-
ber of iterations for training, B is the batch size , and K is the number of layers. Actually, Algorithm
1 (Stage 1) only takes a few seconds on an example of D : 250 × 500, while the training process
(Stage 2) of, for example, ALISTA, takes around 0.1 hours.
E.2 An efficient algorithm to solve (24)
In this section, we introduce an algorithm to solve (24) (we copy (24) below to facilitate reading):
min
w∈RD2M
Wm∙dm = 1, 1≤m≤M
|| (WNr(W))T DNr(d)∣F .
27
Published as a conference paper at ICLR 2019
Similarly, by (58), the above problem is equivalent with
min
w∈RD2M
dm∙Wm = 1, 1≤m≤M
(DNr(d))T WNr(W)∣∣2.
(59)
Since the circular convolution is very efficient to calculate in the frequency domain, we consider
solving (59) utilizing the fast Fourier transform (FFT).
Firstly, we introduce the operators DcNir(d), WcNir(W) in the frequency domain. To simplify the
notation, we denote the operators as DcNir and WcNir respectively. Let F be the FFT operator. Thus,
b = DcNirx is equivalent with
Fb =FDcNirFHFx.
Let b = F b, X = F X be the frequency domain signals, let DNr = F DNr FH be the frequency
domain operator. The above equation is:
b = D NrX.
The frequency domain operator DNr is much cheaper to calculate than the operator DNr in the
spacial domain because it is block diagonal (Wohlberg, 2016). Specifically, we zero pad d to N × N
and do FFT: dm = FFT(ZeroPad(dm, N 一 D)) then the above operator can be explicitly written
as:
M
ʌ
b
Ed m Θ
m=1
X m,
Where : means complex conjugate. This is due to DNr is actually cross-correlation, not convolution
(see (18)). Cross-correlation is equal to the transpose of convolution. Thus, there should be complex
conjugate in the frequency domain.
Further, since
IkD Nr)H W Nr∣∣2 = ||( F DNr F H )" F WNr F H^p =|| F (DNr)T WNr F H(
= IkDNr)TWNr IF,
problem (59) is equivalent with
min
W∈RD2M
dm∙Wm = 1, 1≤m≤M
|| (DNr)HWNr ∣∣F,
which can be efficiently solved by the frequency domain ISTA in (Liu et al., 2017). The details are
outlined in Algorithm 2.
F Visualization of the analytic convolutional weights
Fig. 6 visualizes the dictionary d (7 X 7 X 64) and the weights W ∈ W 133., used in the convolutional
A-LISTA simulation of Section 5.2. It is obtained by Algorithm 2 in Appendix E.2.
G Algorithm Details of Training Robust ALISTA
G.1 Model Architecture
Inspired by the a similar unrolling and truncating fashion in LISTA, we can approximately solve the
coherence minimization problem (16) using a similar finite-layer neural network that is unfolded
from iterative algorithms. Because the linear constraints in (16) are hard to enforce in deep neural
networks, we first relax it to the following form:
arg min ||Q	(DTW 一 IM)||2F,	(60)
W∈RN ×M	F
28
Published as a conference paper at ICLR 2019
Algorithm 2: Frequency-domain ISTA for solving (24)
Input: Dictionary d = [di, ∙ ∙ ∙ , dM]T, dm ∈ RD2, m = 1, 2,…，M.
Initialize: Let w0 = d.
1	for j = 0, 1, 2, . . . until convergence do
2	Zeropad and FFT:
W m = FFTkeroPad(Wm, N — D)),m = 1,…，M.
3	ComPute frequency domain gradient:
M
(Vf )m = (X d m © d m © W m, m =1,…，M,
m=1
where 7 represents the conjugate of a complex number.
4	ComPute the next iterate:
wm+i = PrOjWnormal (IFFT(WWm — 4(V/)m)) , m = 1,…，M,
where the set Wnormal is defined in (53).
5 end
Output: WJ, where J is the last iterate.
(a) The dictionary d.	(b) The w obtained by solving (24).
Figure 6: A visualization of convolutional kernels d and W, which is obtained by Algorithm 2 and
used in the convolutional A-LISTA. W keeps the high-frequency texture in d. The support of W is
small, most of the pixels in W are zeros. Then the coherence between shifted d and W is nearly 0.
where © is the Hadamard product and Q is a weight matrix that put more penalty on errors on
diagonals, because entries on the diagonal will be far smaller than off-diagonal. The above relaxed
coherence minimization can be solved using the gradient descent algorithm:
W(k+1) = W(k) - γ(k)D(Q2 © (DT W(k) - IM)).	(61)
By unfolding (61) and truncate to K steps, and considering the γ(k)DT outside the residual as
learnable parameters B, we will have a deep neural network W = E(D) as a coherence minimizer.
We call it a Stage 1 encoder as it encodes a dictionary D into a weight matrix, that can be used in
the Stage 2 of ALISTA, refered as a decoder. One layer of this model is shown in Fig. 7(a).
The illustration of the whole feed-forward robust model is shown in Fig. 7(b). The two parts, the
encoder and the decoder, can be jointly trained to gain the most from data-driven learning. We
further adopt pre-training and curriculum learning to stabilize training, as to be discussed below.
29
Published as a conference paper at ICLR 2019
Figure 7: Feed-Forward Analytic LISTA.
G.2 Model Training
To stabilize the training process, we train the model in two stages: the pre-training stage and cur-
riculum (joint) training stage.
Pre-Training Stage. We first pre-train the encoder and the decoder individually. The pre-training
of the decoder, e.g., ALISTA, follows the standard training procedure in Section 5.1, without both-
ering the perturbations of D. On the other hand, the encoder will always see perturbed dictionaries
D = D + εD, where εD ’s entries are sampled from i.i.d. normal distribution with zero mean and
σp2re variance, and update its weight to minimize loss function defined by (60). The σpre is a hyper-
parameter that we manually select for the pre-training stage, with a default value of 0.01. We use an
exponentially decaying learning rate for encoder pre-training with an initial value αpre = 10-4.
Curriculum (Joint) Training Stage. After the pre-training stage, we concatenate these two parts
and do joint training. However, a direct end-to-end tuning was observed to cause much instability,
due to the randomness in weights. Inspired by the curriculum learning technique, we first perturb
the dictionaries with smaller standard deviations and gradually increase the perturbation level during
training. Specifically, starting from a small standard deviation σt = σ0 , the curriculum joint training
procedure repeats the routine below:
•	First uniformly sample a batch of standard deviations {σi}iB=D1 from [0, σt], where BD is
the batch size for perturbations of the original dictionary D. Use the sampled standard
deviations to sample BD perturbations and apply to D and then normalized them to get
{D }Bι.
•	Sample a batch of sparse codes {xj }jB=x1 from a pre-defined Gaussian-Bernoulli distribu-
tion; the supports of the sparse codes are decided i.i.d. by a Bernoulli distribution to have
around 10% non-zero entries; and the magnitudes are sampled from i.i.d. standard Gaus-
sian. Bx is the batch size for sparse codes in training.
•	Measure yi,j = Dixj . Then (yi,j , xj , Di) forms a tripelet of training sample. Note that
only Robust ALISTA needs Di as part of the training samples.
•	Feed in the data and update the encoder and decoder with learning rates αe and αd , using
the Adam Optimizer, respectively.
•	Increase σt to the next larger value, after C training batches.
•	Repeat the above steps, until the value of σt exceeds the pre-defined σmax , that represents
the maximal standard deviation to sample the dictionary perturbation.
In the experiment, we have BD = 4, Bx = 16, C = 50000, αe = 10-6, αd = 10-4 and σmax ∈
{0.02, 0.03} and σt is obtained by linearly interpolating between [0, σmax] for L - 1 times. We
choose L = 5; hence σt takes 5σmaχ,i = 1, 2,..., 5 in order.
30
Published as a conference paper at ICLR 2019
H Results of Natural Image Denoising Using Conv ALISTA
The natural image denoising experiment is conducted on the same BSD 500 dataset using the 400-
image training set, 50-image validation set and 50-image test set. We convert them all to grayscale,
and then add σ = 20 Gaussian i.i.d. noise. We train both Conv LISTA (i.e., model (20)) and Conv
ALISTA (i.e., model (26)). Both networks have 5 layers, with the same dictionary D obtained from
the training set by solving (24). We reconstruct the denoised images using by convolving the learned
feature maps with the original dictionary D. The mean-square-error (MSE) between denoised and
clean images are adopted as the network training loss, as inspired by (Zhou et al., 2018).
Six popular benchmark images (adding σ = 20 noise) are tested and reported in Table 4. The A-
PSNR denotes the average PSNR over all images and the A-Times represents the average inference
time (in seconds) for denoising one image. We compare Conv LISTA and Conv ALISTA, as well
as the classical KSVD denoising algorithm (Elad & Aharon, 2006) and the recent CSC denoising
algorithm with gradient regularization (CSC-GR) (Wohlberg, 2018). The results show that Conv
LISTA and Conv ALISTA (without heavy tuning done for their optimal performance) can perform
comparably with KSVD and outperforms CSC-GR, but with tremendously faster inference speeds
than KSVD/CSC-GR. More importantly, Conv LISTA and Conv ALISTA only have marginal per-
formance differences, validating again the analytic weights in convolutional cases.
Table 4: Peak Signal to Noise Ratio (PSNR) Comparision between Conv LISTA and Conv ALISTA.
Model		Image PSNR (dB)							A-PSNR	A-Time
	Lenna	House	Pepper	Couple	Boats	Barbara		
KSVD	31.03	33.24	30.97	31.71	31.00	30.47	31.40	24.70
CSC-GR	28.41	29.11	27.39	29.31	28.35	27.19	28.29	7.56-
Conv LISTA	31.26	32.77	31.00	31.89	30.78	29.53	31.21-	0.012
Conv ALISTA	31.01	32.46	30.81	31.85	30.58	29.72	31.07	0.014
I Results of Ablation Studies in Robustness Experiments
As one anonymous reviewer kindly pointed out, Robust ALISTA has larger parameter space over
TiLISTA and ALISTA trained. Therefore, they suggested that we increased the number of layers
in TiLISTA, ALISTA and the baseline model LISTA-CPSS in (Chen et al., 2018) to see if their
performance in this above evaluation setting can be improved in that way. Note that LISTA-CPSS
has tens of layers, hence actually containing more parameters than robust ALISTA. In addition, the
reviewers also suggested a set of ablation studies to investigate whether more layers in the encoder
can endorse the model better adaptivity to higher level perturbations. We conduct the suggested
experiments and present the results in this section.
I.1	Number of Layers in TiLISTA, ALISTA and LISTA-CPSS
As the reviewers pointed out, the comparison we present in Section. 5.3 might be unfair because
robust ALISTA contains much more parameters (because it contains a 4-layer encoder) comparing
to ALISTA, which only learns two series of scalars, and TiLISTA which has just one more matrix
weight than ALISTA. Therefore, we add the following experiments to consolidate our claim on the
effectiveness of the robust ALISTA model:
•	we increase the number of layers of TiLISTA and ALISTA which are then trained in the
same data augmentation setting as we do in Section. 5.3, to see if they could yield compar-
itive robustness against dictionary perturbations;
•	we also compare the robust ALISTA with the baseline LISTA-CPSS model in Chen et al.
(2018), which contains tens of layers of independent weight matrices, thus having even
more parameters than robust ALISTA. This comparison could consolidate our claim that
the outstanding adaptiveness to dictionary perturbations of robust ALISTA is brought by
its encoder-decoder structure rather than its learning capacity alone.
31
Published as a conference paper at ICLR 2019
The results are shown in Table. 5, where the performances are measured with NMSE in dB, which
is defined in Section 5.1. The “Augmented” prefix means the models are trained in the data aug-
mentation setting. σ is the standard deviation of the Gaussian distribution that is used to generate
the dictioanry perturbations. T stands for the number of layers (in the case of robust ALISTA it
means the nubmer of layers of the ALISTA decoder, with a 4-layer encoder). We follow the training
strategy and settings explained in Appendix G, with σmax = 0.02 during training.
On one hand, the comparison of performances of ALISTA, TiLISTA and LISTA-CPSS shows results
that are consistent to the intuition that larger parameter space yields larger learning capacity, and
therefore, better adaptiveness (LISTA-CPSS > TiLISTA > ALISTA). On the other hand, we can
also find that ALISTA with more layers has worse performance. We think this observation is also
reasonable for two reasons: 1) adding more layers in ALISTA does not enlarge the parameter volume
significantly because it has only two scalar parameters in each layer; noting that ALISTA uses a
fixed, analytically solved weight matrix, if this weight matrix is not compatible with the perturbed
dictionary, more layers can even hurt the performance instead of improving. Lastly, it’s clearly
shown that robust ALISTA outperforms LISTA-CPSS, even if it contains less parameters. This
proves that the encoding process that adaptively transforms the perturbed dictiories is necessary to
achieve good robustness against perturbations in dictionaries.
σ of perturbations during testing		0.0001	0.001	0.01	0.015	0.02	0.025
Augmented ALISTA	T=16	-26.58	-25.87	-15.49	-11.71	-8.84	-6.74
	T=20	-24.43	-24.46	-15.39	-11.77	-8.94	-6.82
	T=24	-24.12	-24.00	-15.45	-11.68	-8.81	-6.70
Augmented TiLISTA	T=16	-27.76	-27.18	-16.83	-12.95	-9.81	-7.55
	T=20	-28.13	-28.54	-17.15	-12.98	-9.83	-7.58
	T=24	-26.08	-27.27	-17.34	-13.14	-9.91	-7.61
Augmented LISTA-CPSS	T=16	-27.93	-27.18	-16.96	-12.99	-9.93	-7.70
	T=20	-28.17	-27.33	-16.95	-13.00	-9.94	-7.71
	T=24	-30.30	-29.24	-16.86	-12.97	-9.94	-7.70
Robust ALISTA	T=16	-62.47	-62.41	-62.02	-61.50	-60.67	-45.00
Table 5: The results (recovery NMSE in dB) of ablation study on the influence of model capacity
towards the model robustness against dictionary perturbations.
I.2	Ablation Study on the Depths of Encoders in Robust ALISTA
Another constructive suggestion from the reviewers is to design an ablation study to investigate the
influence of the depth of encoders in robust ALISTA on its adaptivity to dictionary perturbations. A
natural intuition is that, adding more layers to the encoder can increase its ability to sustain larger
perturbation levels. But is this true?
Therefore, we train another two robust ALISTA models, with a 5-layer and a 6-layer encoders
respectively and 16-layer ALISTA decoders for both, and compare them with the originally reported
robust ALISTA model with a 4-layer encoder and a 16-layer ALISTA decoders. All three models
use one pretrained decoder, and pretrain their encoders using the same method (see Appendix G).
We only use σmax = 0.02 during training. One thing to notice is that we observe unstable training
process if we use default initial learning rates αpre = 10-4 in the pre-training stage and αe = 10-6
in the joint training stage when encoders have 5 or 6 layers. Therefore, we use α0pre = 10-5 for the
6-layer encoder in the pre-training stage, and in the joint training stage use a decreased and uniform
initial learning rate α0e = 10-9 for the three encoders while keeping the default initial learning rate
αd = 10-4 for decoders. The other settings remain the same.
Results are shown in Table. 6. The performances are measured with NMSE in dB, defined in Section
5.1. From the table we can see that encoders do show better robustness when they have more layers,
i.e. larger learning capacity.
32
Published as a conference paper at ICLR 2019
# Encoder Layers	σ of perturbations during testing					
	0.0001	0.001	0.01	0.015	0.02	0.025
4	-68.57	-68.56	-67.94	-66.86	-64.84	-56.63
5	-69.34	-69.34	-69.02	-68.49	-67.20	-65.55
6	-70.38	-70.33	-69.92	-69.22	-67.72	-65.60
Table 6: The results of ablation study on the influence of the depths of encoder towards the model
robustness against dictionary perturbations.
33