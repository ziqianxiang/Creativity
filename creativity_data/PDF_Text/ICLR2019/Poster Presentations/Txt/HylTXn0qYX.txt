Published as a conference paper at ICLR 2019
Efficiently testing local optimality and
escaping saddles for ReLU networks
Chulhee Yun, Suvrit Sra & Ali Jadbabaie
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{chulheey,suvrit,jadbabai}@mit.edu
Ab stract
We provide a theoretical algorithm for checking local optimality and escaping
saddles at nondifferentiable points of empirical risks of two-layer ReLU networks.
Our algorithm receives any parameter value and returns: local minimum, second-
order stationary point, ora strict descent direction. The presence ofM data points
on the nondifferentiability of the ReLU divides the parameter space into at most
2M regions, which makes analysis difficult. By exploiting polyhedral geometry,
we reduce the total computation down to one convex quadratic program (QP) for
each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP. For
the last QP, we show that our specific problem can be solved efficiently, in spite
of nonconvexity. In the benign case, we solve one equality constrained QP, and
we prove that projected gradient descent solves it exponentially fast. In the bad
case, we have to solve a few more inequality constrained QPs, but we prove that
the time complexity is exponential only in the number of inequality constraints.
Our experiments show that either benign case or bad case with very few inequality
constraints occurs, implying that our algorithm is efficient in most cases.
1	Introduction
Empirical success of deep neural networks has sparked great interest in the theory of deep models.
From an optimization viewpoint, the biggest mystery is that deep neural networks are successfully
trained by gradient-based algorithms despite their nonconvexity. On the other hand, it has been
known that training neural networks to global optimality is NP-hard (Blum & Rivest, 1988). Itis also
known that even checking local optimality of nonconvex problems can be NP-hard (Murty & Kabadi,
1987). Bridging this gap between theory and practice is a very active area of research, and there have
been many attempts to understand why optimization works well for neural networks, by studying the
loss surface (Baldi & Hornik, 1989; Yu & Chen, 1995; Kawaguchi, 2016; Soudry & Carmon, 2016;
Nguyen & Hein, 2017; 2018; Safran & Shamir, 2018; Laurent & Brecht, 2018; Yun et al., 2019;
2018; Zhou & Liang, 2018; Wu et al., 2018; Shamir, 2018) and the role of (stochastic) gradient-
based methods (Tian, 2017; Brutzkus & Globerson, 2017; Zhong et al., 2017; Soltanolkotabi, 2017;
Li & Yuan, 2017; Zhang et al., 2018; Brutzkus et al., 2018; Wang et al., 2018; Li & Liang, 2018; Du
et al., 2018a;b;c; Allen-Zhu et al., 2018; Zou et al., 2018; Zhou et al., 2019).
One of the most important beneficial features of convex optimization is the existence ofan optimality
test (e.g., norm of the gradient is smaller than a certain threshold) for termination, which gives us
a certificate of (approximate) optimality. In contrast, many practitioners in deep learning rely on
running first-order methods for a fixed number of epochs, without good termination criteria for the
optimization problem. This means that the solutions that we obtain at the end of training are not
necessarily global or even local minima. Yun et al. (2018; 2019) showed efficient and simple global
optimality tests for deep linear neural networks, but such optimality tests cannot be extended to
general nonlinear neural networks, mainly due to nonlinearity in activation functions.
Besides nonlinearity, in case of ReLU networks significant additional challenges in the analysis arise
due to nondifferentiability, and obtaining a precise understanding of the nondifferentiable points is
still elusive. ReLU activation function h(t) = max{t, 0} is nondifferentiable at t = 0. This means
that, for example, the function f(w, b) := (h(wT x + b) - 1)2 is nondifferentiable for any (w, b)
satisfying wTx+b = 0. See Figure 1 for an illustration of how the empirical risk of a ReLU network
1
Published as a conference paper at ICLR 2019
(b) Nondifferentiable points on (w, v) plane.
Figure 1: An illustration of the empirical risk of a ReLU network. The plotted function is f(w, v) :=
(h(w - v + 1) - 2)2 + (h(2w + v + 1) - 1)2 + (h(w + 2v + 1) - 0.5)2, where h is the ReLU
function. (a) A 3-d surface plot of the function. One can see that there are sharp ridges in the
function. (b) A plot of nondifferentiable points on the (w, v) plane. The blue line correspond to the
line w - v + 1 = 0, the red to 2w + v + 1 = 0, and the yellow to w + 2v + 1 = 0.
(a) A 3-d surface plot of f (w, v).
looks like. Although the plotted function does not exactly match the definition of empirical risk we
study in this paper, the figures help us understand that the empirical risk is continuous but piecewise
differentiable, with affine hyperplanes on which the function is nondifferentiable.
Such nondifferentiable points lie in a set of measure zero, so one may be tempted to overlook them
as “non-generic.” However, when studying critical points we cannot do so, as they are precisely
such “non-generic” points. For example, Laurent & Brecht (2018) study one-hidden-layer ReLU
networks with hinge loss and note that except for piecewise constant regions, local minima always
occur on nonsmooth boundaries. Probably due to difficulty in analysis, there have not been other
works that handle such nonsmooth points of losses and prove results that work for all points. Some
theorems (Soudry & Carmon, 2016; Nguyen & Hein, 2018) hold “almost surely”; some assume
differentiability or make statements only for differentiable points (Nguyen & Hein, 2017; Yun et al.,
2019); others analyze population risk, in which case the nondifferentiability disappears after taking
expectation (Tian, 2017; Brutzkus & Globerson, 2017; Du et al., 2018b; Safran & Shamir, 2018; Wu
et al., 2018).
1.1	Summary of our results
In this paper, we take a step towards understanding nondifferentiable points of the empirical risk of
one-hidden-layer ReLU(-like) networks. Specifically, we provide a theoretical algorithm that tests
second-order stationarity for any point of the loss surface. It takes an input point and returns:
(a)	The point is a local minimum; or
(b)	The point is a second-order stationary point (SOSP); or
(c)	A descent direction in which the function value strictly decreases.
Therefore, we can test whether a given point is a SOSP. If not, the test extracts a guaranteed direc-
tion of descent that helps continue minimization. With a proper numerical implementation of our
algorithm (although we leave it for future work), one can run a first-order method until it gets stuck
near a point, and run our algorithm to test for optimality/second-order stationarity. If the point is an
SOSP, we can terminate without further computation over many epochs; if the point has a descent
direction, our algorithm will return a descent direction and we can continue on optimizing. Note that
the descent direction may come from the second-order information; our algorithm even allows us to
escape nonsmooth second-order saddle points. This idea of mixing first and second-order methods
has been explored in differentiable problems (see, for example, Carmon et al. (2016); Reddi et al.
(2017) and references therein), but not for nondifferentiable ReLU networks.
The key computational challenge in constructing our algorithm for nondifferentiable points is posed
by data points that causes input 0 to the ReLU hidden node(s). Such data point bisects the parameter
space into two halfspaces with different “slopes” of the loss surface, so one runs into nondifferen-
2
Published as a conference paper at ICLR 2019
tiability. We define these data points to be boundary data points. For example, in Figure 1b, if the
input to our algorithm is (w, v) = (-2/3, 1/3), then there are two boundary data points: “blue” and
“red.” If there are M such boundary data points, then in the worst case the parameter space divides
into 2M regions, or equivalently, there are 2M “pieces” of the function that surround the input point.
Of course, naively testing each region will be very inefficient; in our algorithm, we overcome this
issue by a clever use of polyhedral geometry. Another challenge comes from the second-order test,
which involves solving nonconvex QPs. Although QP is NP-hard in general (Pardalos & Vavasis,
1991), we prove that the QPs in our algorithm are still solved efficiently in most cases. We further
describe the challenges and key ideas in Section 2.1.
Notation. For a vector v, [v]i denotes its i-th component, and IlvkH := VzvTHv denotes a semi-
norm where H is a positive Semidefinite matrix. Given a matrix A, we let [A]i,j, [A]i,∙, and [A]∙,j
be A’s (i, j)-th entry, the i-th row, and the j-th column, respectively.
2 Problem setting and key ideas
We consider a one-hidden-layer neural network with input dimension dx, hidden layer width dh, and
output dimension dy. We are given m pairs of data points and labels (xi, yi)im=1, where xi ∈ Rdx
and yi ∈ Rdy . Given an input vector x, the output of the network is defined as Y (x) := W2h(W1x+
b1)+b2,whereW2 ∈ Rdy×dh, b2 ∈ Rdy, W1 ∈ Rdh×dx,andb1 ∈ Rdh are the network parameters.
The activation function h is “ReLU-like,” meaning h(t) := max{s+t, 0} + min{s-t, 0}, where
s+ > 0, s- ≥ 0 and s+ 6= s-. Note that ReLU and Leaky-ReLU are members of this class. In
training neural networks, we are interested in minimizing the empirical risk
R((Wj,% )2=1)=Xm=1'(Y (χi),yi)=Xm=1'(W2h(WIxi+bi)+b2,yi),
over the parameters (Wj, bj)j2=1, where `(w, y) : Rdy × Rdy 7→ R is the loss function. We make
the following assumptions on the loss function and the training dataset:
Assumption 1. The loss function `(w, y) is twice differentiable and convex in w.
Assumption 2. No dx + 1 data points lie on the same affine hyperplane.
Assumption 1 is satisfied by many standard loss functions such as squared error loss and cross-
entropy loss. Assumption 2 means, if dx = 2 for example, no three data points are on the same line.
Since real-world datasets contain noise, this assumption is also quite mild.
2.1	Challenges and key ideas
In this section, we explain the difficulties at nondifferentiable points and ideas on overcoming them.
Our algorithm is built from first principles, rather than advanced tools from nonsmooth analysis.
Bisection by boundary data points. Since the activation function h is nondifferentiable at 0, the
behavior of data points at the “boundary” is decisive. Consider a simple example dh = 1, so W1 is a
row vector. If W1xi+b1 6= 0, then the sign of (W1+∆1)xi+(b1+δ1) for any small perturbations ∆1
and δ1 stays invariant. In contrast, when there is a point xi on the “boundary,” i.e., W1xi + b1 = 0,
then the slope depends on the direction of perturbation, leading to nondifferentiability. As mentioned
earlier, we refer to such data points as boundary data points. When ∆1 xi + δ1 ≥ 0,
h((W1 + ∆1)xi + (b1 + δ1)) = h(∆1xi + δ1) = s+(∆1xi + δ1) = h(W1xi + b1) + s+(∆1xi + δ1),
and similarly, the slope is s- for ∆1 xi + δ1 ≤ 0. This means that the “gradient” (as well as higher
order derivatives) of R depends on direction of (∆1, δ1).
Thus, every boundary data point xi bisects the space of perturbations (∆j , δj)j2=1 into two halfs-
paces by introducing a hyperplane through the origin. The situation is even worse if we have M
boundary data points: they lead to a worst case of 2M regions. Does it mean that we need to test all
2M regions separately? We show that there is a way to get around this issue, but before that, we first
describe how to test local minimality or stationarity for each region.
Second-order local optimality conditions. We can expand R((Wj + ∆j, bj +δj)j2=1) and obtain
the following Taylor-like expansion for small enough perturbations (see Lemma 2 for details)
R(Z + η) = R(Z) + g(z, η)T η + 2 ηT H (z, η)η + o(kηk2),	(i)
3
Published as a conference paper at ICLR 2019
where z is a vectorized version of all parameters (Wj , bj )j2=1 and η is the corresponding vector
of perturbations (∆j, δj)j2=1. Notice now that in (1), at nondifferentiable points the usual Taylor
expansion does not exist, but the corresponding “gradient" g(∙) and “Hessian" H(∙) now depend
on the direction of perturbation η. Also, the space of η is divided into at most 2M regions, and
g(z, η) and H(z, η) are piecewise-constant functions of η whose “pieces" correspond to the regions.
One could view this problem as 2M constrained optimization problems and try to solve for KKT
conditions at z; however, we provide an approach that is developed from first principles and solves
all 2M problems efficiently.
Given this expansion (1) and the observation that derivatives stay invariant with respect to scaling
of η, one can note that (a) g(z, η)Tη ≥ 0 for all η, and (b) ηTH(z, η)η ≥ 0 for all η such that
g(z, η)Tη = 0 are necessary conditions for local optimality of z, thus z is a “SOSP" (see Defini-
tion 2.2). The conditions become sufficient if (b) is replaced with ηTH(z, η)η > 0 for all η 6= 0 such
that g(z, η)Tη = 0. In fact, this is a generalized version of second-order necessary (or sufficient)
conditions, i.e., Vf = 0 and V2f 占 0 (or V2f * 0), for twice differentiable f.
Efficiently testing SOSP for exponentially many regions. Motivated from the second-order ex-
pansion (1) and necessary/sufficient conditions, our algorithm consists of three steps:
(a)	Testing first-order stationarity (in the Clarke sense, see Definition 2.1),
(b)	Testing g(z, η)Tη ≥ 0 for all η,
(c)	Testing ηT H(z, η)η ≥ 0 for {η | g(z, η)Tη = 0}.
The tests are executed from Step (a) to (c). Whenever a test fails, we get a strict descent direction
η, and the algorithm returns η and terminates. Below, we briefly outline each step and discuss how
we can efficiently perform the tests. We first check first-order stationarity because it makes Step (b)
easier. Step (a) is done by solving one convex QP per each hidden node. For Step (b), we formulate
linear programs (LPs) per each 2M region, so that checking whether all LPs have minimum cost of
zero is equivalent to checking g(z, η)Tη ≥ 0 for all η. Here, the feasible sets of LPs are pointed
polyhedral cones, whereby it suffices to check only the extreme rays of the cones. It turns out that
there are only 2M extreme rays, each shared by 2M-1 cones, so testing g(z, η)Tη ≥ 0 can be done
with only O(M) inequality/equality tests instead of solving exponentially many LPs. In Step (b),
we also record the flat extreme rays, which are defined to be the extreme rays with g(z, η)Tη = 0,
for later use in Step (c).
In Step (c), we test if the second-order perturbation ητH(∙)η can be negative, for directions where
g(z, η)Tη = 0. Due to the constraint g(z, η)Tη = 0, the second-order test requires solving con-
strained nonconvex QPs. In case where there is no flat extreme ray, we need to solve only one
equality constrained QP (ECQP). If there exist flat extreme rays, a few more inequality constrained
QPs (ICQPs) are solved. Despite NP-hardness of general QPs (Pardalos & Vavasis, 1991), we prove
that the specific form of QPs in our algorithm are still tractable in most cases. More specifically,
we prove that projected gradient descent on ECQPs converges/diverges exponentially fast, and each
step takes O(p2) time (p is the number of parameters). In case of ICQPs, it takes O(p3 +L32L) time
to solve the QP, where L is the number of boundary data points that have flat extreme rays (L ≤ M).
Here, we can see that if L is small enough, the ICQP can still be solved in polynomial time in p. At
the end of the paper, we provide empirical evidences that the number of flat extreme rays is zero or
very few, meaning that in most cases we can solve the QP efficiently.
2.2	Problem-specific notation and definition
In this section, we define a more precise notion of generalized stationary points and introduce some
additional symbols that will be helpful in streamlining the description of our algorithm in Section 3.
Since we are dealing with nondifferentiable points of nonconvex R, usual notions of (sub)gradients
do not work anymore. Here, Clarke subdifferential is a useful generalization (Clarke et al., 2008):
Definition 2.1 (FOSP, Theorem 6.2.5 of Borwein & Lewis (2010)). Suppose that a function f(z) :
Ω → R is locally LipSChitz around the point z* ∈ Ω, and differentiable in Ω \ W where W has
Lebesgue measure zero. Then the Clarke differential of f at z* is
∂zf(z*) := cvxhull{limk Vf(zk) | zk → z* , zk ∈/ W}.
If0 ∈ ∂zf(z*), we say z* is a first-order stationary point (FOSP).
4
Published as a conference paper at ICLR 2019
From the definition, We can note that Clarke subdifferential ∂zR(z*) is the convex hull of all the
possible values of g(z*,η) in (1). For parameters (Wj, bj j=ι, let ∂w,f (z*) and ∂b,f (z*) be the
Clarke differential w.r.t. to Wj and bj, respectively. They are the projection of ∂zf (z*) onto the
space of individual parameters. Whenever the point z* is clear (e.g. our algorithm), we will omit
(z*) from f (z*). Next, we define second-order stationary points for the empirical risk R. Notice
that this generalizes the definition of SOSP for differentiable functions f: Vf = 0 and V2f 占 0.
Definition 2.2 (SOSP). We call z* is a second-order stationary point (SOSP) of R if (1) z* is a
FOSP, (2) g(z*, η)Tη ≥ 0 for all η, and (3) ηT H(z*, η)η ≥ 0 for all η such that g(z*, η)Tη = 0.
Given an input data point x ∈ Rdx, we define O(x) := h(W1x + b1) to be the output of hidden
layer. We note that the notation O(∙) is overloaded with the big O notation, but their meaning will
be clear from the context. Consider perturbing parameters (Wj, bj)j2=1 with (∆j, δj)j2=1, then the
perturbed output Y (x) of the network and the amount of perturbation dY (x) can be expressed as
. . ~ . . . . ... .
dY(x):= Y(X) - Y(X) = ∆2O(x) + δ2 + (W2 + ∆2)J(x)(∆ιx + δι),
where J(x) can be thought informally as the “Jacobian” matrix of the hidden layer. The matrix
J(X) ∈ Rdh×dh is diagonal, and its k-th diagonal entry is given by
[J(X)]k,k
h0([W1X + b1]k)
h0([∆1X + δ1]k)
if [W1X + b1]k 6=0
if [W1X + b1]k = 0,
where h0 is the derivative ofh. We define h0(0) := s+, which is okay because it is always multiplied
with zero in our algorithm. For boundary data points, [J (X)]k,k depends on the direction of pertur-
bations [∆ι 6山,.，as noted in Section 2.1. We additionally define dY1(χ) and dY2(χ) to separate
the terms in dY(X) that are linear in perturbations versus quadratic in perturbations.
dY1(X) := ∆2O(X) +δ2 +W2J(X)(∆1X+δ1), dY2(X) := ∆2J(X)(∆1X + δ1).
For simplicity of notation for the rest of the paper, we define for all i ∈ [m] := {1, . . . , m},
Xi := [χT 1]T ∈ Rdχ+1, v'i := Vw'(Y(Xi),yi), v2'i := vW'(Y(Xi),yi).
In our algorithm and its analysis, we need to give a special treatment to the boundary data points.
To this end, for each node k ∈ [dh] in the hidden layer, define boundary index set Bk as
Bk := {i ∈ [m] | [W1Xi + b1]k = 0} .
The subspace spanned by vectors Xi for in i ∈ Bk plays an important role in our tests; so let us
define a symbol for it, as well as the cardinality of Bk and their sum:
Xdh
Mk.
For k ∈ [dh], let vkT ∈ R1×(dx+1) be the k-th row of [∆1 δ1], and uk ∈ Rdy be the k-th column of
∆2. Next, we define the total number of parameters p, and vectorized perturbations η ∈ Rp:
P :=	dy +	dydh	+ dh(dχ + 1), ητ :=	[δT	UT	…	UTh	VT	…	VTI].
Also let z ∈ Rp be vectorized parameters (Wj, bj)j2=1, packed in the same order as η.
Define a matrix Ck := Pii∕Bk h0(∖W1Xi + bι]k)V'ixT ∈ Rdy ×(dx+1). This quantity appears mul-
tiplie times and does not depend on the perturbation, so it is helpful to have a symbol for it.
We conclude this section by presenting one of the implications of Assumption 2 in the following
lemma, which we will use later. The proof is simple, and is presented in Appendix B.1.
Lemma 1. IfASSUmPtiOn 2 holds, then Mk ≤ dχ and the vectors {Xi}i∈Bk are linearly independent.
3 Test algorithm for second-order stationarity
In this section, we present SOSP-CHECK in Algorithm 1, which takes an arbitrary tuple (Wj, bj)j2=1
of parameters as input and checks whether it is a SOSP. We first present a lemma that shows the ex-
plicit form of the perturbed empiricalriskR(z+η) and identify first and second-order perturbations.
The proof is deferred to Appendix B.2.
5
Published as a conference paper at ICLR 2019
Algorithm 1 SOSP-CHECK (Rough pseudocode)
Input: A tuple (Wj, bj)2= of R(∙).
1:	Testif∂W2R= {0dy×dh}and∂b2R= {0dy}.
2:	for k ∈ [dh] do
3:	if Mk > 0 then
4:	TeStif 0Tχ+ι ∈ dW bι]k,R
5:	Test if gk (z, Vk)TVk ≥ 0 for all Vk via testing extreme rays Vk of polyhedral cones.
6:	Store extreme rays Vk s.t. gk (z, Vk)tVk = 0 for second-order test.
7:	else
8:	TeStif d[Wι bι]k,R ={0Tχ+ι}.
9:	end if
10:	end for
11:	For all n's s.t. g(z,η)τη = 0, test if ητH(z,η)η ≥ 0.
12:	if ∃η 6= 0 s.t. g(z, η)Tη = 0 and ηT H(z, η)η = 0 then
13:	return SOSP.
14:	else
15:	return Local Minimum.
16:	end if
Lemma 2. For small enough perturbation η,
R(Z + η) = R(z) + g(z,η)T η + 1 nτ H (z,η)η + o(kηk* 2 * *),
where g(z, η) and H(z, η) satisfy
dh
g(z,η)τ η = XYT dYι(χi) =(Xi ▽'，O(Xi)T, △2)+(XiV'*〉+ X gk(z,v )τ vk,
k=1
ητ H(Z,η)η = Xi ▽'T dY2(xi)+ 1 XikdYI(Xi)信 `i,
and gk (z,Vk)τ ：= [W2]Tk (Ck + Pi∈Bk h0(xτVk)V'ixτ). Also, g(z,η) and H(z,η) are piece-
wise constant functions of η, which are constant inside each polyhedral cone in space of η.
Rough pseudocode of SOSP-Check is presented in Algorithm 1. As described in Section 2.1, the
algorithm consists of three steps: (a) testing first-order stationarity (b) testing g(Z, η)τη ≥ 0 for all
η, and (c) testing ητ H(Z, η)η ≥ 0 for {η | g(Z, η)τη = 0}. If the input point satisfies the second-
order sufficient conditions for local minimality, the algorithm decides it is a local minimum. If the
point only satisfies second-order necessary conditions, it returns SOSP. If a strict descent direction
η is found, the algorithm terminates immediately and returns η. A brief description will follow, but
the full algorithm (Algorithm 2) and a full proof of correctness are deferred to Appendix A.
3.1 Testing first-order stationarity (lines 1, 4, and 8)
Line 1 of Algorithm 1 corresponds to testing if ∂W2R and ∂b2R are singletons with zero. If not, the
opposite direction is a descent direction. More details are in Appendix A.1.1.
Test for W1 and b1 is more difficult because g(Z, η) depends on △1 and δ1 when there are boundary
0) test if 0dτx+1 is
convex hull of all
data points. For each k ∈ [dh], Line 4 (if Mk > 0), and Line 8 (if Mk =
in ∂[W1 bι]k ∙R. Note from Definition 2.1 and Lemma 2 that ∂[W1	,R is the
possible values of gk(z, vk)τ. If Mk > 0, 0 ∈ ∂[W1 也限.R can be tested by solving a convex QP:
minimize{Si}i∈Bk	k[W2]Tk(Ck + Pi∈Bk siv'iXT)k2
subject to	min{s- , s+} ≤ si ≤ max{s-, s+}, ∀i ∈ Bk.
(2)
If the solution {s* }i∈Bk does not achieve zero objective value, then We can directly return a descent
direction. For details please refer to FO-Subdiff-Zero-Test (Algorithm 3) and Appendix A.1.2.
3.2 Testing g(z, η)τη ≥ 0 for all η (lines 5-6)
Linear program formulation. Lines 5-6 are about testing if gk (z, Vk )τVk ≥ 0 for all directions
of Vk. If θʤ+1 ∈ ∂[W1 bι]k ∙R, with the solution {s*} from QP (2) we can write gk(z, Vk)τ as
Ok (z,Vk)T =[W2]Tk (Ck + Xi∈Bkh0(XT Vk)V'iXT) = [W2]Tk (Xi∈Bfc (h0(XT Vk ) - sjv'iXT).
6
Published as a conference paper at ICLR 2019
Every i ∈ Bk bisects Rdx+1 into two halfspaces, XτVk ≥ 0 and XτVk ≤ 0, in each of which
h0 (XTVk) stays constant. Note that by Lemma 1, Xi ,s for i ∈ Bk are linearly independent. So, given
Mk boundary data points, they divide the space Rdx+1 ofVk into 2Mk polyhedral cones.
sincegk(z, Vk)τ is constant in each polyhedral cones, we can let σi ∈ {-1, +1} for all i ∈ Bk, and
define an LP for each {σi}i∈Bk ∈ {-1, +1}Mk :
minimize	[W2]Tk (Pi∈Bk (Sσi - s∙DV'iχT) Vk
vk
subject to Vk ∈ Vk, σiXTVk ≥ 0, Vi ∈ Bk.
(3)
Solving these LPs and checking if the minimum value is 0 suffices to prove gk (z, Vk)TVk ≥ 0 for all
small enough perturbations. The constraint Vk ∈ Vk is there because any Vk ∈/ Vk is also orthogonal
to gk(z, Vk). It is equivalent to dx + 1 - Mk linearly independent equality constraints. So, the
feasible set of LP (3) has dx + 1 linearly independent constraints, which implies that the feasible set
is a pointed polyhedral cone with vertex at origin. Since any point in a pointed polyhedral cone is a
conical combination (linear combination with nonnegative coefficients) of extreme rays of the cone,
checking nonnegativity of the objective function for all extreme rays suffices. We emphasize that
we do not solve the LPs (3) in our algorithm; we just check the extreme rays.
Computational efficiency. Extreme rays of a pointed polyhedral cone in Rdx+1 are computed
from dχ linearly independent active constraints. For each i ∈ Bk, the extreme ray Vi,k ∈ Vk ∩
span{Xj | j ∈ Bk \ {i}}⊥ must be tested whether gk(z, Vi,k)τVi,k ≥ 0, in both directions. Note
that there are 2Mk extreme rays, and one extreme ray Vi,k is shared by 2Mk-1 polyhedral cones.
Moreover, XTVik = 0 for j ∈ Bk \ {i}, which indicates that
gk(z,Vi,k)τVi,k = (sσi,k - si)[W2]τkV'iXτVi,k, where σi,k = sign(Xτ^i,k),
regardless of {σ7-}j∈Bk∖{i}. Testing an extreme ray can be done with a single inequality test instead
of 2Mk-1 separate tests for all cones! Thus, this extreme ray approach instead of solving individual
LPs greatly reduces computation, from O(2Mk) to O(Mk).
Testing extreme rays. For the details of testing all possible extreme rays, please refer to
FO-Increasing-Test (Algorithm 4) and Appendix A.2. FO-Increasing-Test computes all
possible extreme rays Vk and tests if they satisfy gk (z, Vk)TVk ≥ 0. If the inequality is not satisfied
by an extreme ray Vk, then this is a descent direction, so We return Vk. If the inequality holds with
equality, it means this is a flat extreme ray, and it needs to be checked in second-order test, so we
save this extreme ray for future use.
How many flat extreme rays (gk (z, Vk)τVk = 0) are there? Presence of flat extreme rays introduce
inequality constraints in the QP that we solve in the second-order test. It is ideal not to have them,
because in this case there are only equality constraints, so the QP is easier to solve. Lemma A.1 in
Appendix A.2 shows the conditions for having flat extreme rays; in short, there is a flat extreme ray
if [W2]τk V'i = 0 or Si = s+ or s-. For more details, please refer to Appendix A.2.
3.3 Testing ηTH(z, η)n ≥ 0 FOR {η | g(z, η)τη = 0} (lines 11-16)
The second-order test checks ητH(ζ,η)η ≥ 0 for “flat” η's satisfying g(z,η)τη = 0. This is
done with help of the function sO-TEsT (Algorithm 5). Given its input {σi,k}k∈[dh],i∈Bk, it defines
fixed “Jacobian” matrices Ji for all data points and equality/inequality constraints for boundary data
points, and solves the QP of the following form:
minimizen Pi v'τ∆2Ji(∆1χi + δI) + 1 Pikδ2o(Xi) + δ2 + W2JiQlxi + 6I)I怜2'i,
subject to	[卬2氐Uk = [Wι bι]k,Vk, Vk ∈ [dh],
Xi Vk = 0,
σi,kXTVk ≥ 0,
∀k ∈ [dh],	∀i	∈ Bk	s.t. σi,k	= 0,
∀k ∈ [dh],	∀i	∈ Bk	s.t. σi,k	∈ {-1, +1}.
(4)
Constraints and number of QPs. There are dh equality constraints of the form [W2]TkUk =
[[W1]k,∙	[bι]k] Vk. These equality constraints are due to the nonnegative homogeneous property
of activation h; i.e., scaling [Wι]k,∙ and [bι]k by a > 0 and scaling [W2]∙,k by l∕a yields exactly
7
Published as a conference paper at ICLR 2019
the same network. So, these equality constraints force η to be orthogonal to the loss-invariant di-
rections. This observation is stated more formally in Lemma A.2, which as a corollary shows that
any differentiable FOSP of R always has rank-deficient Hessian. The other constraints make sure
that the union of feasible sets of QPs is exactly {η | g(z, η)Tη = 0} (please see Lemma A.3 in
Appendix A.3 for details). It is also easy to check that these constraints are all linearly independent.
If there is no flat extreme ray, the algorithm solves just one QP with dh + M equality constraints. If
there are flat extreme rays, the algorithm solves one QP with dh + M equality constraints, and 2K
more QPs with dh + M - L equality constraints and L inequality constraints, where
dh	dh
K ：=£ ∣{i ∈ Bk | [W2]TN'i = 0}∣, L ：= E |{i ∈ Bk | Vi,k or -^i,k is a flat ext. ray}∣. (5)
k=1
k=1
Recall from Section 3.2 that i ∈ Bk has a flat extreme ray if [W2]Tk V'i = 0 or 寸=s+ or s-;
thus, K ≤ L ≤ M. Please refer to Appendix A.3 for more details.
Efficiency of solving the QPs (4). Despite NP-hardness of general QPs, our specific form of
QPs (4) can be solved quite efficiently, avoiding exponential complexity in p. After solving QP (4),
there are three (disjoint) termination conditions:
(T1) ηTQη > 0 whenever η ∈ S, η 6= 0, or
(T2) ηTQη ≥ 0 whenever η ∈ S, but ∃η 6= 0 , η ∈ S such that ηTQη	= 0, or
(T3) ∃η such that η ∈ S and ηTQη < 0,
where S is the feasible set of QP. With the following two lemmas, we	show that	the	termination
conditions can be efficiently tested for ECQPs and ICQPs. First, the	ECQPs	can	be	iteratively
solved with projected gradient descent, as stated in the next lemma.
Lemma 3. Consider the QP, where Q ∈ Rp×p is symmetric and A ∈ Rq×p has full row rank:
minimizeη 2ητQη subject to Aη = 0q
Then, projected gradient descent (PGD) updates
η(t+1) = (I - AT(AAT)-1A)(I - αQ)η(t)
with learning rate α < 1∕λmaχ(Q) converges to a solution or diverges to infinity exponentially fast.
Moreover, with random initialization, PGD correctly checks conditions (TI)-(T3) with probability 1.
The proof is an extension of unconstrained case (Lee et al., 2016), and is deferred to Appendix B.3.
Note that it takes O(p2q) time to compute (I - AT(AAT)-1A)(I - αQ) in the beginning, and each
update takes O(p2) time. It is also surprising that the convergence rate does not depend on q.
In the presence of flat extreme rays, we have to solve QPs involving L inequality constraints. We
prove that our ICQP can be solved in O(p3 + L32L) time, which implies that as long as the number
of flat extreme rays is small, the problem can still be solved in polynomial time in p.
Lemma 4. Consider the QP, where Q ∈ Rp×p is symmetric, A ∈ Rq×p and B ∈ Rr×p have full
row rank, and AT BT has rank q + r:
minimizeη ηTQη subject to Aη = 0q, Bη ≥ 0r .
Then, there exists a method that checks whether (T1)-(T3) in O(p3 + r32r) time.
In short, we transform η to define an equivalent problem, and use classical results in copositive
matrices (Martin & Jacobson, 1981; Seeger, 1999; Hiriart-Urruty & Seeger, 2010); the problem can
be solved by computing the eigensystem ofa (p-q-r) × (p-q-r) matrix, and testing copositivity
of an r × r matrix. The proof is presented in Appendix B.4.
Concluding the test. During all calls to S O-TEST, whenever any QP terminated with (T3), then
SOSP-Check immediately returns the direction and terminates. After solving all QPs, if any of
SO-TEST calls finished with (T2), then we conclude SOSP-CHECK with “SOSP.” If all QPs termi-
nated with (T1), then we can return “Local Minimum.”
8
Published as a conference paper at ICLR 2019
Table 1: Summary of experimental results
(dx, dh, m)	# Runs	Sum M (Avg.)	Sum L (Avg.)	Sum K (Avg.)	P{L > 0}
(10, 1, 1000)	40	290 (7.25)	0(0)	0(0)	0
(10, 1, 10000)	40	371 (9.275)	1 (0.025)	0(0)	0.025
(100, 1, 1000)	40	1,452(36.3)	0(0)	0(0)	0
(100, 1, 10000)	40	2,976 (74.4)	2 (0.05)	0(0)	0.05
(100, 10, 10000)	40	24,805 (620.125)	4(0.1)	0(0)	0.1
(1000, 1, 10000)	40	14,194 (354.85)	0(0)	0(0)	0
(1000, 10, 10000)	40	42,334(1,058.35)	37 (0.925)	1 (0.025)	0.625
4	Experiments
For experiments, we used artificial datasets sampled iid from standard normal distribution, and
trained 1-hidden-layer ReLU networks with squared error loss. In practice, it is impossible to get
to the exact nondifferentiable point, because they lie in a set of measure zero. To get close to those
points, we ran Adam (Kingma & Ba, 2014) using full-batch (exact) gradient for 200,000 iterations
and decaying step size (start with 10-3, 0.2× decay every 20,000 iterations). We observed that
decaying step size had the effect of “descending deeper into the valley.”
After running Adam, for each k ∈ [dh], we counted the number of approximate boundary data
points satisfying |[W1xi + b1]k | < 10-5, which gives an estimate of Mk. Moreover, for these
points, we solved the QP (2) using L-BFGS-B (Byrd et al., 1995), to check if the terminated points
are indeed (approximate) FOSPs. We could see that the optimal values of (2) are close to zero
(≤ 10-6 typically, ≤ 10-3 for largest problems). After solving (2), We counted the number of Srs
that ended UP with 0 or 1. The number of such Srs is an estimate of L - K. We also counted the
number of approximate boundary data points satisfying ∣[W2]Tk▽£ | < 10-4, for an estimate of K.
We ran the above-mentioned experiments for different settings of (dx , dh , m), 40 times each. We
fixed dy = 1 for simplicity. For large dh, the optimizer converged to near-zero minima, making
V'i uniformly small, so it was difficult to obtain accurate estimates of K and L. Thus, we had to
perform experiments in settings where the optimizer converged to minima that are far from zero.
Table 1 summarizes the results. Through 280 runs, we observed that there are surprisingly many
boundary data points (M) in general, but usually there are zero or very few (maximum was 3) flat
extreme rays (L). This observation suggests two important messages: (1) many local minima are
on nondifferentiable points, which is the reason why our analysis is meaningful; (2) luckily, L is
usually very small, so we only need to solve ECQPs (L = 0) or ICQPs with very small number of
inequality constraints, which are solved efficiently (Lemmas 3 and 4). We can observe that M , L,
and K indeed increase as model dimensions and training set get larger, but the rate of increase is not
as fast as dx, dh, and m.
5	Discussion and future work
We provided a theoretical algorithm that tests second-order stationarity and escapes saddle points,
for any points (including nondifferentiable ones) of empirical risk of shallow ReLU-like networks.
Despite difficulty raised by boundary data points dividing the parameter space into 2M regions, we
reduced the computation to dh convex QPs, O(M) equality/inequality tests, and one (or a few more)
nonconvex QP. In benign cases, the last QP is equality constrained, which can be efficiently solved
with projected gradient descent. In worse cases, the QP has a few (say L) inequality constraints,
but it can be solved efficiently when L is small. We also provided empirical evidences that L is
usually either zero or very small, suggesting that the test can be done efficiently in most cases. A
limitation of this work is that in practice, exact nondifferentiable points are impossible to reach,
so the algorithm must be extended to apply the nonsmooth analysis for points that are “close” to
nondifferentiable ones. Also, current algorithm only tests for exact SOSP, while it is desirable to
check approximate second-order stationarity. These extensions must be done in order to implement
a robust numerial version of the algorithm, but they require significant amount of additional work;
thus, we leave practical/robust implementation as future work. Also, extending the test to deeper
neural networks is an interesting future direction.
9
Published as a conference paper at ICLR 2019
Acknowledgments
This work was supported by the DARPA Lagrange Program. Suvrit Sra also acknowledges support
from an Amazon Research Award.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, l989.
Avrim Blum and Ronald L Rivest. Training a 3-node neural network is NP-complete. In Proceedings
of the 1st International Conference on Neural Information Processing Systems, pp. 494-501. MIT
Press, 1988.
Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and
examples. Springer Science & Business Media, 2010.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with gaussian
inputs. In International Conference on Machine Learning, pp. 605-614, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations, 2018.
Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for
bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):1190-1208, 1995.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex
optimization. arXiv preprint arXiv:1611.00756, 2016.
Francis H Clarke, Yuri S Ledyaev, Ronald J Stern, and Peter R Wolenski. Nonsmooth analysis and
control theory, volume 178. Springer Science & Business Media, 2008.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Jason D Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent
learns one-hidden-layer CNN: Dont be afraid of spurious local minima. In International Confer-
ence on Machine Learning, pp. 1338-1347, 2018b.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018c.
J-B Hiriart-Urruty and Alberto Seeger. A variational approach to copositive matrices. SIAM review,
52(4):593-629, 2010.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas Laurent and James Brecht. The multilinear structure of ReLU networks. In International
Conference on Machine Learning, pp. 2914-2922, 2018.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on Learning Theory, pp. 1246-1257, 2016.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8168-
8177, 2018.
10
Published as a conference paper at ICLR 2019
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activa-
tion. In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
Duncan Henry Martin and David Harris Jacobson. Copositive matrices and definiteness of quadratic
forms subject to homogeneous linear inequality constraints. Linear Algebra and its Applications,
35:227-258, 1981.
Katta G Murty and Santosh N Kabadi. Some NP-complete problems in quadratic and nonlinear
programming. Mathematical programming, 39(2):117-129, 1987.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Pro-
ceedings of the 34th International Conference on Machine Learning, volume 70, pp. 2603-2612,
2017.
Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep CNNs. In
International Conference on Machine Learning, pp. 3727-3736, 2018.
Panos M Pardalos and Stephen A Vavasis. Quadratic programming with one negative eigenvalue is
NP-hard. Journal of Global Optimization, 1(1):15-22, 1991.
Sashank J Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdi-
nov, and Alexander J Smola. A generic approach for escaping saddle points. arXiv preprint
arXiv:1709.01434, 2017.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural net-
works. In International Conference on Machine Learning, pp. 4430-4438, 2018.
Alberto Seeger. Eigenvalue analysis of equilibrium processes defined by linear complementarity
conditions. Linear Algebra and its Applications, 292(1-3):1-14, 1999.
Ohad Shamir. Are ResNets provably better than linear predictors? In Advances in Neural Informa-
tion Processing Systems, pp. 505-514, 2018.
Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. In Advances in Neural Information
Processing Systems, pp. 2007-2017, 2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. In International Conference on Machine
Learning, pp. 3404-3413, 2017.
Gang Wang, Georgios B Giannakis, and Jie Chen. Learning ReLU networks on linearly separable
data: Algorithm, optimality, and generalization. arXiv preprint arXiv:1808.04685, 2018.
Chenwei Wu, Jiajun Luo, and Jason D Lee. No spurious local minima in a two hidden unit ReLU
network. In International Conference on Learning Representations Workshop, 2018.
Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning.
IEEE Transactions on Neural Networks, 6(5):1300-1303, 1995.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks.
In International Conference on Learning Representations, 2018.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small nonlinearities in activation functions create bad
local minima in neural networks. In International Conference on Learning Representations, 2019.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer ReLU
networks via gradient descent. arXiv preprint arXiv:1806.07808, 2018.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In International Conference on Machine Learning, pp.
4140-4149, 2017.
11
Published as a conference paper at ICLR 2019
Yi Zhou and Yingbin Liang. Critical points of neural networks: Analytical forms and landscape
properties. In International Conference on Learning Representations, 2018.
Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global
minimum in deep learning via star-convex path. In International Conference on Learning Repre-
sentations, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
12
Published as a conference paper at ICLR 2019
Algorithm 2 SOSP-CHECK
Input: A tuple (Wj, bj j=ι of R(∙).
1: if Pm=I ▽' [θ(Xi)T 1] = 0dy ×(dh + 1) then
2： returnQz	δ2]	<--Pm=I ▽'	[θ(Xi)T	1]	,	δ1	-	0dh×dx ,	δ1	- 0dh .
3:	end if
4:	for k ∈ [dh] do
5:	if Mk > 0 then
6:	{Si}i∈Bk — FO-SUBDIFF-ZERO-TEST(k)
7:	VT — [W2]Tk(Ck + Pi∈Bk SiV'ixT).
8:	if Vk = 0dχ+ι then
9:	return Vk《------------Vk, ∀k0 ∈ [dh] \ {k}, vk0 - 0dx + 1, δ2 - 0dy ×dh , δ2 - 0dy.
10:	end if
11:	(decr,Vk, {Si,k}i∈Bk) — FO-INCREASING-TEST(k, {si}i∈Bk).
12:	if decr = True then
13:	return vk — vk, ∀k0 ∈ [dh]∖{k},vk0 — 0dx + 1, δ2 ― 0dy×dh, δ2 — 0dy .
14:	end if
15:	elseif [W2]TkCk = 0Tχ + 1 then
16:	return vk	—--CkT [W2]-,k,	∀k0 ∈	[dh]	\ {k}, vk0 — 0dx + 1,	δ2	―	0dy ×d. ,	δ2	― 0dy .
17:	end if
18:	end for
19:	(decr, sosp, (∆j, δj)j2=1) — SO-TEST({0}k∈[dh],i∈Bk).
20:	if decr = True then return (∆j, δj)j2=1.
21:	end if
22:	if M 6= 0 and {Si,k}k∈[dh],i∈Bk 6= {{0}}k∈[dh],i∈Bk then
23:	for each element {σi,k}k∈[dh],i∈Bk ∈ Qk∈[dh] Qi∈Bk Si,k do
24:	(decr, sospTemp, (∆j, δj)j2=1) — SO-TEST({σi,k}k∈[dh],i∈Bk).
25:	if decr = True then return (∆j, δj)j2=1.
26:	end if
27:	sosp — sosp ∨ sospTemp
28:	end for
29:	end if
30:	if sosp = True then return SOSP.
31:	else return Local Minimum.
32:	end if
Algorithm 3 FO-SUBDIFF-ZERO-TEST
Input: k ∈ [dh]
1:	Solve the following optimization problem and get optimal solution {sii}i∈Bk:
minimize{Si}i∈Bk k[W2]Tk(Ck + Pi∈Bk Siv'ixT)k2
sub ject to	min{s-, s+} ≤ si ≤ max{s- , s+}, ∀i ∈ Bk,
(2)
2:	return {Sii}i∈Bk .
A Full algorithms and proof of correctness
In this section, we present the detailed operation of SOSP-Check (Algorithm 2), and its helper
functions FO-SUBDIFF-ZERO-Test, FO-INCREASING-Test, and SO-Test (Algorithm 3—5).
In the subsequent subsections, we provide a more detailed proof of the correctness of Algorithm 2.
Recall that, by Lemmas 1 and 2, Mk := |Bk | ≤ dχ and vectors {Xi}i∈Bk are linearly independent.
Also, we can expand R(z + η) so that
R(Z + η) = R(Z) + g(z, η)T η + 1 ηT H (z, η)η + o(kηk2),
13
Published as a conference paper at ICLR 2019
Algorithm 4 FO-INCREASING-TEST
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
Input： k ∈ [dh], {s↑}i∈Bk
for all i ∈ Bk do
Define S%,k — 0.
Get a vector Vi,k ∈ Vk ∩ span{Xj | j ∈ Bk \ {i}}⊥.
for Vk ∈ {Vi,k, -Vi,k} do
Define σik — Sign(XT Vk).
if (sσi,k - si)[W2]TkV'ixTvk < 0 then
return (True,vk, {0}i∈Bk)
else if (sσ*k — $：)[卬2晨▽-XTvk = 0 then
Si,k J Si,k ∪ {σi,k}.
end if
end for
If Si,k = 0, Si,k J {0}.
end for
return (False, 0dx+1, {Si,k}i∈Bk).
Algorithm 5 SO-TEST
Input: {σi,k}k∈[dh],i∈Bk
1:	For all i ∈ [m], define diagonal matrices Ji ∈ Rdh ×dh such that for k ∈ [dh],
h0([N(Xi)]k)	ifi∈ [m]\Bk
[Ji]k,k J	sσi,k	if i ∈	Bk	and	σi,k	∈ {—1, +1}
〔0	if i ∈	Bk	and	σ*k	=0.
2:	Solve the following QP. If there is no solution, get a descent direction (∆j, δj)j=∖.
minimize	PV'T∆zJi(∆∖Xi + δι) + 2 Pk∆2O(xi) + δ2 + W2 Ji(∆ιXi+δι)k".
ηi	i	i
subject to	[W2]TkUk = [Wι bι]k,vk, Vk ∈ [dh],
XTvk = 0,	∀k	∈	[dh],∀i	∈	Bk s.t.	σi,k	= 0,
σi,kXTvk ≥ 0,	∀k	∈	[dh],∀i	∈	Bk s.t.	σi,k	∈ { —1,+1}.
(4)
3
4
5
6
if There is no solution then return (True, False, (∆*,δ*)2=1).
else if QP has nonzero minimizers then return (False, True, 0)
else return (False, False, 0)
end if
where g(z, η) and H(z, η) satisfy
dh
g(z, η)T η = Xi v`t dY1 (Xi) =(Xi V'iO(Xi)T, △r + Byi 0 + X gk(z, vk )T vk,
k=1
ηTH(z, η)η =Xi v`t dY2(Xi) +1 XilldY1(Xi)I 监2 `i,
andgk(z,vk)T := [W2]Tk (Ck + Pi∈Bk h(XTvk)V'iXT).
A.1 Testing first-order stationarity (lines 1—3, 6-10 AND 15-17)
A.1.1 TEST oF FiRST-oRDER S TATioNARiTY FoR W2 AND b2 (LiNES 1-3)
Lines 1-3 of Algorithm 2 correspond to testing if ∂W2R = {0dy ×dh} and ∂b2R = {0dy}. if they
are not all zero, the opposite direction is a descent direction, as Line 2 returns. To see why, suppose
pm=ι V'i [O(Xi)T 1]= 0dy×(dh+i).Then choose perturbations
心2 δ2] = 一 Xi=] V'i [O(Xi)T 1] , δi = 0dh×dχ , δ1 = 0dh .
14
Published as a conference paper at ICLR 2019
If we apply perturbation (γ∆j, γδj)j2=1 where γ > 0, we can immediately check that dY1 (xi) =
∆2O(xi) + δ2 and dY2 (xi) = 0. So,
g(z,η)Tη = Xm=1 v'T。2O(Xi) + δ2) = DXm=IV'i [o(χi)T i],[δ2 4])= -O(Y),
ηTH(z, η)η = 1 X，dYι(xi)τV2'idYι(xi) = O(γ2) ≥ 0.
and also that Pim=1 kdY (xi)k22 = O(γ2). Then, by scaling γ sufficiently small we can achieve
R(z + η) < R(z), which disproves that (Wj, bj)j2=1 is a local minimum.
A.1.2 TEST OF FIRST-ORDER STATIONARITY FOR Wi AND bi (LINES 6-10 AND 15-17)
Test for W1 and b1 is more difficult because g(z, η) depends on ∆1 and δ1 when there are boundary
data points. Recall that vkτ (k ∈ [dh]) is the k-th row of [∆i δi]. Then note from Lemma 2 that
Xm 1 V'τ (W2 J(xi)(∆ιXi + δi)) = Xdh gk(z, Vk)τVk,
i=i	k=i
where gk(z, Vk)τ := [W2]τk (Ck + Pii∈Bk h0(xTVk)V'i±T). ThUs We can separate k's and treat
them individually.
Test for zero gradient. Recall the definition Mk := |Bk|. If Mk = 0, there is no boUndary data
point for k-th hidden node, so the Clarke subdifferential with respect to [Wi bι]k,∙, is {C∕[W2]∙,k}.
Lines 15-17 handle this case; if the singleton element in the sUbdifferential is not zero, its opposite
direction is a descent direction, so return that direction, as in Line 16.
Test for zero in subdifferential. For the case Mk > 0, we saw that for boundary data points
i ∈ Bk, h0(∆1Xi + δι]k) = h0(xτVk) ∈ {s-, s+} depends on vk. Lines 6-10 test if θʤ+1 is in
the Clarke subdifferential of R with respect to [Wι]k,∙ and [bi]k. Since the subdifferential is used
many times, we give it a specific name Dk := ∂[W1 比小,R. By observing that Dk is the convex hull
of all possible values of gk (z, Vk)τ,
Dk ：= {W2]τk (Ck+ Xi∈BksiV'iX) | min{s-, s+} ≤ Si ≤ max{s-, s+}, ∀i ∈ Bk}.
Testing 0dτ +i ∈ Dk is done by FO-SUBDIFF-ZERO-TEST in Algorithm 3. It solves a convex
QP (2), and returns {s*}i∈Bk.
If 0Tx + 1 ∈ Dk, {si}i∈Bk will satisfy VT ：= [W2]τk (Ck + Pi∈Bk siv'ixI) = 0Tx + 1. SUPPOse
θʤ+1 ∈ Dk. Then, Vk is the closest vector in Dk from the origin, so ® ,V > 0 for all VT ∈ Dk.
Choose perturbations
Vk =	-Vk,	Vk0	=	0dx+ι for all k0 ∈	[dh]	\ {k},	∆2	= 0dy×dh,	δ2	=	0dy,
and apply perturbation (γ∆j, γδj)j2=1 where γ > 0. With this perturbation, we can check that
g(z, η)τη = X=IV婷dY1(xi) = -Y[W2]Tk (Ck + Xi∈B h0(-xTVk)V'iXT) Vk,
and since h0(-xTVk) ∈ {s-, s+} for i ∈ Bk, we have
[W2]Tk (Ck + XyR h0(-xTVk)V'iXT) ∈ Dk,
i∈Bk
and hVk, Vi > 0 for all vt ∈ Dk shows that g(z, η)τη is strictly negative with magnitude O(Y). It
is easy to see that ηTH(z, η)η = O(γ2 ), so by scaling γ sufficiently small we can disprove local
minimality of (Wj, bj)j2=1.
15
Published as a conference paper at ICLR 2019
A.2 Testing g(z, η)Tη ≥ 0 FOR ALL η (lines 11-14)
Linear program formulation. Lines 11-14 are essentially about testing if gk(z, Vk)TVk ≥ 0 for
all directions Vk. If 0工十]∈ Dk, with the solution {s^}i∈Bk from FO-SUBDIFF-ZERO-TEST We
can write gk (z, Vk)T as
gk(Z,Vk)T = [W2]Tk (Ck + X h0(XTVk)v'ixτ I = [W2]Tk I X (h0(XTVk)-Sjv'ixτ
i∈Bk
i∈Bk
For any i ∈ Bk, h0(XTVk) ∈ {s-, s+} changes whenever the sign of XTVk changes. Every i ∈ Bk
bisects Rdx+1 into two halfspaces, XTVk ≥ 0 and XTVk ≤ 0, in each of which h0(xTVk) stays
constant. Note that by Lemma 1, Xi's for i ∈ Bk are linearly independent. So, given Mk linearly
independent Xi,s, they divide the space Rdx+1 of Vk into 2Mk polyhedral cones.
Since gk(z, Vk)T is constant in each polyhedral cone, we can let σi ∈ {-1, +1} for all i ∈ Bk, and
define an LP for each {σi}i∈Bk ∈ {-1, +1}Mk :
minimize	[W2]Tk PiiBBk (sσi - Si)V'iXT) Vk
vk
subject to Vk ∈ Vk, σiXTVk ≥ 0, ∀i ∈ Bk.
(3)
Solving these LPs and checking if the minimum value is 0 suffices to prove gk(z, Vk)TVk ≥ 0 for all
small enough perturbations. Recall that Vk := span{Xi | i ∈ Bk} and dim(Vk) = Mk. Note that
any component of Vk that is orthogonal to Vk is also orthogonal to gk(z, Vk), so it does not affect
the objective function of any LP (3). Thus, the constraint Vk ∈ Vk is added to the LP (3), which is
equivalent to adding dx+1 -Mk linearly independent equality constraints. The feasible set of LP (3)
has dx + 1 linearly independent equality/inequality constraints, which implies that the feasible set
is a pointed polyhedral cone with vertex at origin. Since any point in a pointed polyhedral cone is a
conical combination (linear combination with nonnegative coefficients) of extreme rays of the cone,
checking nonnegativity of the objective function for all extreme rays suffices. We emphasize that we
do not solve the LPs (3) in our algorithm; we just check the extreme rays.
Computational efficiency. Extreme rays of a pointed polyhedral cone in Rdx+1 are computed
from dx linearly independent active constraints. Line 3 of Algorithm 4 is exactly computing such
extreme rays: Vi,k ∈ Vk ∩ span{Xj | j ∈ Bk \ {i}}⊥ for each i ∈ Bk, tested in both directions.
Note that there are 2Mk extreme rays, and one extreme ray Vi,k is shared by 2Mk-1 polyhedral
cones. Moreover, XTV%,k = 0 for j ∈ Bk \ {i}, which indicates that
gk(z,Vi,k)tVi,k = (Sσi,k — s"[W2]TkV'iXTVi,k, whereσi,k = Sign(XT^ik),
regardless of {σ,}j∈Bk∖{i}. This observation is used in Lines 6 and 8 of Algorithm 4. Testing
gk(z, Vk)TVk ≥ 0 for an extreme ray Vk can be done with a single inequality test instead of 2Mk-1
separate tests for all cones! Thus, this extreme ray approach instead of solving individual LPs greatly
reduces computation, from O(2Mk) to O(Mk).
Algorithm operation in detail. Testing all possible extreme rays is exactly what
FO-Increasing-Test in Algorithm 4 is doing. Output of FO-Increasing-Test is a tu-
ple of three items: a boolean, a (dx + 1)-dimensional vector, and a tuple of Mk sets. Whenever
we have a descent direction, it returns True and the descent direction Vk. If there is no descent
direction, it returns False and the sets {Si,k}i∈Bk.
For both direction of extreme rays Vk = Vi,k and Vk = —Vi,k (Line 4), we check if gk(z,Vk )t Vk ≥ 0.
Whenever it does not hold (Lines 6-7), Vk is a descent direction, so FO-INCREASING-TEST returns
it with True. Line 13 of Algorithm 2 uses that Vk to return perturbations, so that scaling by small
enough γ > 0 will give us a point with R(z + γη) < R(z). If equality holds (Lines 8-9), this means
Vk is a direction of perturbation satisfying g(z, η)Tη = 0, so this direction needs to be checked if
ηTH(z, η)η ≥ 0 too. In this case, we add the sign of boundary data point Xi to Si,k for future use
in the second-order test. The operation with Si,k will be explained in detail in Appendix A.3. After
checking if gk(z,Vk)tVk ≥ 0 holds for all extreme rays, FO-INCREASING-TEST returns False
with {Si,k}i∈Bk .
16
Published as a conference paper at ICLR 2019
Counting flat extreme rays. HoW many of these extreme rays satisfy gk (z, Vk )TVk = 0? Presence
of such flat extreme rays introduce inequality constraints in the QP that we will solve in SO-Test
(Algorithm 5). It is ideal not to have flat extreme rays, because in this case there are only equality
constraints, so the QP is easier to solve. The folloWing lemma shoWs conditions for existence of flat
extreme rays as Well as output of Algorithm 4.
Lemma A.1. Suppose 0Tχ 十]∈ Dk and all extreme rays Vk satisfy gk (z, Vk)TEk ≥ 0. Consider all
i ∈ Bk, and its corresponding Vi,k ∈ Vk ∩ span{Xj | j ∈ Bk \ {i}}⊥.
1.	If [W2]τkV'i = 0, then both extreme rays Vi,k and —Vi,k are flat extreme rays, and S%,k =
{-1, +1} at the end of Algorithm 4.
2.	If [W2]TkV'i = 0 and s* = s+ (or s-), one (and only) of Vk ∈ {Vi,k, —Vi,k} that satisfies
Sign(XTVk) = +1 (or — 1) is a flat extreme ray, and Si,k = {+1} (or { -1}) at the end of
Algorithm 4.
3.	If [W2]Tk V'i = 0 and s* = s±, both Vi,k and -Vi,k are not flat extreme rays, and Si,k =
{0} at the end of Algorithm 4.
Proof First note that we already assumed that all extreme rays Vk satisfy gk (z, Vk)TVk ≥ 0, So
SOSP-CHECK will reach Line 14 at the end. Also note that Xi's in i ∈ Bk are linearly independent
(by Lemma 1), so XTVi,k = 0.
If [W2]TkV'i = 0, then (sσ*k -s()[W2]TkV'iXTVk = 0 regardless of Vk, so both Vi,k and —Vi,k are
flat extreme rays. If [W2]TkV'i = 0 and Sr = s+, Vk ∈ {Vi,k, -Vi,k} that satisfies Sign(XTVk)=
+1 gives σi,k = +1, so sσi,k = s*. Thus, Vk is a flat extreme ray. The case with s* = s- is proved
similarly. If [W2]Tk V' = 0 and s* = s+, none of (s± — s*), [W2]Tk V'i, and XTVi,k are zero, so
Vi,k and —Vi,k cannot be flat.	□
Let Bk(j) ⊆ Bk denote the set of indices i ∈ Bk satisfying conditions in Lemma A.1.j (j = 1, 2, 3).
Note that Bk(j)’s partition the set Bk. We denote the union of Bk(1) and Bk(2) by Bk(1,2), and similarly,
Bk(2,3) := Bk(2) ∪ Bk(3). We can see from the lemma that |Si,k | = 2 for i ∈ Bk(1), and |Si,k | = 1 for
i ∈ Bk(2,3). Also, it follows from the definition of K and L (5) that
dh	dh
K=X|Bk(1)|, L = X |Bk(1)| + |Bk(2)|.
k=1	k=1
Connection to KKT conditions. As a side remark, we provide connections of our tests to the well-
known KKT conditions. Note that the equality gk (z,Vk)T = [W2]Tk (Pi∈B (sσi — s*)V'i±T) for
σiXTVk ≥ 0, ∀i ∈ Bk corresponds to the KKT stationarity condition, where 乐石—s()[W2]Tk V'i's
correspond to the Lagrange multipliers for inequality constraints. Then, testing extreme rays
is equivalent to testing dual feasibility of Lagrange multipliers, and having zero dual variables
([W2]Tk V'i = 0 or s* = s+ or s-, resulting in flat extreme rays) corresponds to having degen-
eracy in the complementary slackness condition.
As mentioned in Section 2.1, given that g(z, η) and H(z, η) are constant functions of η in each
polyhedral cone, one can define inequality constrained optimization problems and try to solve for
KKT conditions for z directly. However, this also requires solving 2M problems. The strength
of our approach is that by solving the QPs (2), we can automatically compute the exact Lagrange
multipliers for all 2M subproblems, and dual feasibility is also tested in O(M) time.
A.3 Testing ηTH(z, η)n ≥ 0 for {η | g(z, η)Tη = 0} (lines 19—32)
The second-order test checks ηTH(z, η)η ≥ 0 for “flat” η's satisfying g(z, η)Tη = 0. This is done
with help of the function SO-TEST in Algorithm 5. Given its input {σi,k}k∈[dh],i∈Bk, it defines
fixed “Jacobian” matrices Ji for all data points and equality/inequality constraints for boundary data
points, and solves the QP (4).
17
Published as a conference paper at ICLR 2019
Equality/inequality constraints. In the QP (4), there are dh equality constraints of the form
[W2]T,k U
[[Wι]k,∙	[bι]k] Vk. These equality constraints are due to the nonnegative homoge-
neous property of activation function h: scaling [Wι]k,∙ and [bι]k by a > 0 and scaling [W2]∙,k
by 1∕ɑ yields exactly the same network. This observation is stated more precisely in the following
lemma.
Lemma A.2. Suppose Z is a FOSP (differentiable or not) of R(∙). Fix any k ∈ [dh], and define
perturbation η as
Uk = -[W2]∙,k, Vk = [[W1]k,∙ [bi]k]T , Uk0 = 0,Vko = 0 forall k0 = k, δ2 = 0.
Then, g(z, η)Tη = ηTH (z, η)η = 0.
The proof of Lemma A.2 can be found in Appendix B.5. A corollary of this lemma is that any
differentiable FOSP of R always has rank-deficient Hessian, and the multiplicity of zero eigenvalue
is at least dh. Hence, these dh equality constraints onuk’s and Vk ’s force η to be orthogonal to the
loss-invariant directions.
The equality constraints of the form XTVk = 0 are introduced when σ3k = 0; this happens for
boundary data points i ∈ Bk(3) . Therefore, there are M - L additional equality constraints. The
(1,2)
inequality constraints come from i ∈ Bk , . So there are L inequality constraints. Now, the
following lemma proves that feasible sets defined by these equality/inequality constraints added
to (4) exactly correspond to the regions where gk (z, Vk)TVk = 0. Recall from Lemma A.1 that
Si,k = {-1, +1} for i ∈ Bk , Si,k = {-1} or {+1} for i ∈ Bk , and Si,k = {0} for i ∈ Bk .
Lemma A.3. Let {σi,k }i∈B(2)
i∈ k
[n
be the only element of i∈B(2) Si,k. Then, in SO-TEST,
Vk | ∀i ∈ Bk3),XTVk = 0, and∀i ∈ B^,2,σi,kXTVk ≥ θ}
{σi,k}	(1) ∈Qi Si,k
i∈Bk
={vk | ∀i ∈ Bk3),xTVk = 0, and ∀i ∈ B，2«i,kXTVk ≥ 0}
= vk | gk (z, vk)T vk = 0 .
The proof of Lemma A.3 is in Appendix B.6.
In total, there are dh + M - L equality constraints and L inequality constraints in each nonconvex
QP. It is also easy to check that these constraints are all linearly independent.
How many QPs do we solve? Note that in Line 19, we call SO-TEST with {σi,k}k∈[dh],i∈Bk = 0,
which results in a QP (4) with dh + M equality constraints. This is done even when we have flat
extreme rays, just to take a quick look if a descent direction can be obtained without having to deal
with inequality constraints.
If there exist flat extreme rays (Line 22), the algorithm calls SO-Test for each element of
Qk∈[dh] Qi∈Bk Si,k. Recall that |Si,k| =2fori ∈ Bk(1), so
k∈[dh] i∈BkSi,k=2K.
In summary, if there is no flat extreme ray, the algorithm solves just one QP with dh + M equality
constraints. If there are flat extreme rays, the algorithm solves one QP with dh + M equality
constraints, and 2K QPs with dh + M - L equality constraints and L inequality constraints. This is
also an improvement from the naive approach of solving 2M QPs.
Concluding the test. After solving the QP, SO-TEST returns result to SOSP-CHECK. The al-
gorithm returns two booleans and one perturbation tuple. The first is to indicate that there is no
solution, i.e., there is a descent direction that leads to -∞. Whenever there was any descent direc-
tion then we immediately return the direction and terminate. The second boolean is to indicate that
there are nonzero η that satisfies ηT H(z, η)η = 0. After solving all QPs, if any of SO-TEST calls
found out η 6= 0 such that g(z, η)T η = 0 and ηT H(z, η)η = 0, then we conclude SOSP-CHECK
with “SOSP.” If all QPs terminated with unique minimum at zero, then we can conclude “Local
Minimum.”
18
Published as a conference paper at ICLR 2019
B Proof of lemmas
B.1 Proof of Lemma 1
By definition, We have ∖Wι]k,∙Xi + [bι]k = 0 for all i ∈ Bk, meaning that they are all on the same
hyperplane [Wι]k,∙X + [bι]k = 0. By the assumption, we cannot have more than dχ points on the
hyperplane.
Next, assume for the sake of contradiction that the Mk := |Bk | data points Xi’s are linearly depen-
dent, i.e., there exists a1, . . . , aMk ∈ R, not all zero, such that
Mk
XXi
ai	1
i=1
Mk
- X ai
i=2
Mk
ai(Xi - X1) = 0,
i=2
where a2, . . . , aMk are not all zero. This implies that these Mk points Xi’s are on the same (Mk -2)-
dimensional affine space. To see why, consider for example the case Mk = 3: a2(X2 - X1) =
-a3(X3 -X1), meaning that they have to be on the same line. By adding any dx + 1 - Mk additional
Xi’s, we can see that dx + 1 points are on the same (dx - 1)-dimensional affine space, i.e., a
hyperplane in Rdx. This contradicts Assumption 2.
B.2	Proof of Lemma 2
From Assumption 1, '(w, y) is twice differentiable and convex in w. By Taylor expansion of '(∙) at
(Y (Xi), yi),
m
R(Z + η) = Ei=ι '(YE) + dY(χi),yi)
=χm '(Y(xi),yi) + V'τdY(xi) + 1 dY(xi)TV2'idY(xi) + o(kηk2)
i=1	2
mm	m
=R(Z) + X V'TdYι(Xi) + X V'TdY2(xi) + 2 X kdYι(xi)恃2'i + o(kηk2),
i=1	i=1	i=1
where the first-order term Pm=I V'TdYι(xi) = Pm=I V'T(∆2O(xi) + δ? + W2 J(χj(∆ιχi + δι))
can be further expanded to show
Xim=1 V'iT (∆2O(Xi) + δ2) = D∆2, Xi V'iO(Xi)T E + Dδ2, Xi V'iE,
χm=1 V'T (W2 J(xi)(∆ιxi + δι)) = tr (XtJ(Xi)WTV'iXT £ )
dh	m	dh
=XW2]Tk X[J(Xi)]k,kV'iXT Vk = XW2]Tk (Ck + Xi∈Bfc h0(XTVk)V'iXT) Vk.
k=1	i=1	k=1	k
Also, note that in each of the 2M divided region (which is a polyhedral cone) of η, J(Xi) stays
constant for all i ∈ [m]; thus, g(Z, η) and H(Z, η) are piece-wise constant functions of η. Specif-
ically, since the parameter space is partitioned into polyhedral cones, we have g(Z, η) = g(Z, γη)
and H(Z, η) = H(Z, γη) for any γ > 0.
B.3	Proof of Lemma 3
Suppose that w1, w2, . . . , wq are orthonormal basis of row(A). Choose wq+1, . . . , wp so that
w1 , w2 , . . . , wp form an orthonormal basis of Rp. Let W be an orthogonal matrix whose columns
are w1,w2,..., wp, and W be an submatrix of W whose columns are wq+ι,..., wp. With this
definition, note that I 一 AT(AAT)-1A = WWT.
Suppose that we are given η(t) satisfying Aη(t) = 0. Then we can write η(t) = Wμ(t), where
μ⑶ ∈ Rp-q and [μ(t)]i = WTrqη(t). Define μ(t+1) likewise. Then, noting η(t) = WWTη(t) gives
Wμ(t+1) = η(t+1) = η(t) 一 αWWTQWμ(t) = W(I — aWTQWW)μ(t).
19
Published as a conference paper at ICLR 2019
Define C := WTQW ∈ R(p-q)×(p-q), and then write its eigen-decomposition C = VSVT and
denote its eigenvectors as ν1, . . . , νp-q and its corresponding eigenvalues λ1, . . . , λp-q. Then note
p-q	p-q
μ(t+1) = (I - αC)μ⑶= (I-QVSVT) X(VTμ(t))νi = X(1 - αλi)(νTμ(t))νi
i=1	i=1
=X(I- αλi)2(νTμ(t-1))νi =…=X(1 - αλi)t+1(VTμ⑼)νi.
i=1	i=1
This proves that this iteration converges or diverges exponentially fast. Starting from the initial
point η(0) = Wμ(0), the component of μ(0) that corresponds to negative eigenvalue blows UP ex-
ponentially fast, those corresponding to positive eigenvalue shrinks to zero exponentially fast (if
α < 1∕λmaχ(C)), and those with zero eigenvalue will stay invariant. Therefore, if there exists
λi < 0, then η(t) blows up to infinity quickly and finds an η such that ηTQη < 0 (T3). If all λi ≥ 0,
it converges exponentially fast to W £分Α=。(不μ(O))Vi (T2). If all λi > 0, η(t) → 0 (T1).
It is left to prove that α < 1∕λmax(Q) guarantees convergence, as stated. To this end, it suffices to
show that λmax(Q) ≥ λmax(C). Note that
C = WTQW = WTWWTQWWTW = [0 I] WTQW [I].
Using the facts that λmax(Q) = λmax(WTQW) and C is a principal submatrix of WTQW,
xTWTQWx	xTWTQWx
λmax(Q) = maχ T ≥ maχ	T = λmaχ(C).
x	xTx	x:[x]1:q =0	xTx
Also, if we start at a random initial point (e.g., sample from a Gaussian in Rp and project to
row(A)⊥), then with probability 1 We have VTμ(0) = 0 for all i ∈ [p - q], so We will get the
correct convergence/divergence result almost surely.
B.4 Proof of Lemma 4
B.4.1	Preliminaries
Before we prove the complexity lemma, we introduce the definitions of copositivity and Pareto
spectrum, which are closely related concepts to our specific form of QP.
Definition B.1. Let Q ∈ Rr×r be a symmetric matrix. We say that Q is copositive if ηTQη ≥ 0 for
all η ≥ 0. Moreover, strict copositivity means that ηTQη > 0 for all η ≥ 0, η 6= 0.
Testing whether Q is not copositive known to be NP-complete (Murty & Kabadi, 1987); it is cer-
tainly a difficult problem. There is a method testing cositivity of Q in O(r32r) time which uses
its Pareto spectrum Π(Q). The following is the definition of Pareto spectrum, taken from Seeger
(1999); Hiriart-Urruty & Seeger (2010).
Definition B.2. Consider the problem
minimize
η≥0,kηk2=1
KKT conditions for the above problem gives us a complementarity system
η ≥ 0, Qη -λη ≥ 0, ηT (Qη -λη) = 0, kηk2 = 1,	(6)
where λ ∈ R is viewed as a Lagrange multiplier associated with kηk2 = 1. The number λ ∈ R is
called a Pareto eigenvalue of Q if (6) admits a solution η. The set of all Pareto eigenvalues of Q,
denoted as Π(Q), is called the Pareto spectrum of Q.
The next lemma reveals the relation of copositivity and Pareto spectrum:
Lemma B.1 (Theorem 4.3 of Hiriart-Urruty & Seeger (2010)). A symmetric matrix Q is copositive
(or strictly copositive) if and only if all the Pareto eigenvalues of Q are nonnegative (or strictly
positive).
20
Published as a conference paper at ICLR 2019
Now, the following lemma tells us how to compute Pareto spectrum of Q.
Lemma B.2 (Theorem 4.1 of Seeger (1999)). Let Q be a matrix of order r. Consider a nonempty
index set J ⊆ [r]. Given J, QJ refers to the principal submatrix of Q with the rows and columns
of Q indexed by J. Let 2[r] \ 0 denote the set of all nonempty subsets of [r]. Then λ ∈ Π(Q) if and
only ifthere exists an index set J ∈ 2[r] \ 0 and a vector ξ ∈ RIJ| such that
QJξ = λξ, ξ ∈ int(RJl), X[Q]i,j [ξ]j ≥ 0 for all i / J.
j∈J
In such a case, the vector η ∈ Rr by
[ξ]j
0
ifj ∈ J,
ifj ∈/ J
is a Pareto-eigenvector of Q associated to the Pareto eigenvalue λ.
These lemmas tell us that the Pareto spectrum of Q can be calculated by computing eigensystems of
all 2r - 1 possible QJ, which takes O(r32r) time in total, and from this we can determine whether
a symmetric Q is copositive.
B.4.2	Proof of the lemma
With the preliminary concepts presented, we now start proving our Lemma 4. We will first transform
η to eliminate the equality constraints and obtain an inequality constrained problem of the form
minimizew：Bw〉o WTRw. From there, We can use the theorems from Martin & Jacobson (1981),
which tell us that by testing positive definiteness of a (p-q-r) × (p-q-r) matrix and copositivity
of a r × r matrix We can determine Which of the three categories the QP falls into. Transforming η
and testing positive definiteness take O(p3) time and testing copositivity takes O(r3 2r) time, so the
test in total is done in O(p3 + r32r) time.
We noW describe hoW to transform η and get an equivalent optimization problem of the form We
Want. We assume Without loss of generality that A = [A1 A2] Where A1 ∈ Rq×q is invertible. If
not, We can permute components of η. Then make a change of variables
η = TA
A1-1
-A-IA2] Pw-
0(p-q)×q
Ip-q	W
so that ATA
[I
W
w
Consequently, the constraint An = 0 becomes W = 0. Now partition B = [Bi B2], where
B1 ∈ Rr×q. Also let R be the principal submatrix of TAT QTA composed With the last p - q roWs
and columns. It is easy to check that
minimizeη ηTQη	≡ minimizew WTRW
subject to Aη = 0q, Bη ≥ 0r.	subject to	(B2 - B1A1-1A2)W ≥ 0r.
Let us quickly check if B2 - B1A1-1A2 has full row rank. One can observe that
A1	A2	Iq	0 A1	A2
B1	B2 = B1A1-1	Ir	0	B2 - B1A1-1A2
It follows from the assumption rank( [AT BT]) = q + r that B := B2 一 B1A-1A2 has rank r,
which means it has full row rank.
Before stating the results from Martin & Jacobson (1981), we will transform the problem a bit fur-
ther. Again, assume without loss of generality that B = [Bi B2] where Bi / Rr×r is invertible.
Define another change of variables as the following:
W = TBν :
. B-1
0(p-q-r)×r
-B-IB2
Ip-q-r
TT RTC —∙ R11 R12 一直
TBRTB=∙[RT2 R22∖ =: r.
Consequently, we get
minimizew WTRW	≡ minimizew
subject to Bw ≥ 0r.	subject to
V T RV = VT R11V1 + 2νT R12V2 + VT R22V2
ν1 ≥ 0r.
Given this transformation, we are ready to state the lemmas.
21
Published as a conference paper at ICLR 2019
Lemma B.3 (Theorem 2.2 of Martin & Jacobson (1981)). If B= [jBi B2], with Bi r X r invert-
ible, then with Rij，s given as above, WT Rw > 0 whenever B W ≥ 0, W = 0 if and only if
•	R22 is positive definite, and
•	R11 — Ri2 R-1RT2 is strictly copositive.
LemmaB.4 (Theorem 2.1 of Martin & Jacobson (1981)). If B = [Bi B2], with Bi r × r invert-
ible, then with Ri/s given as above, wtRw ≥ 0 whenever BW ≥ 0 ifand only if
•	R22 is positive semidefinite, null(R22) ⊆ null(R12), and
•	R11 — R12R22R2 iscopositive,
where R22 is a pseudoinverse of R22.
Using Lemmas B.3 and B.4, we now describe how to test our given QP and declare one of (T1),
(T2), or (T3). First, We compute the eigenSystem of R22 and see which of the following disjoint
categories it belongs to:
(PD1) All eigenvalues λι,..., λp-q- of R22 satisfy λi > 0.
(PD2) ∀i, λi	≥ 0, but	∃i	such that	λi	=	0, and ∀ν2 s.t.	R22ν2	=	0, we have R12ν2	= 0.
(PD3) ∀i, λi	≥ 0, but	∃i	such that	λi	=	0, and	∃ν2 s.t.	R22ν2	=	0 but R12ν2 = 0.
(PD4) ∃i such that λi < 0, i.e., ∃ν2 such that ντR22ν2 < 0.
If the test comes out (PD3) or (PD4), then we can immediately declare (T3) without having to look at
copositivity. This is because if we get (PD4), we can choose νι = 0 so that VTRV = ντR22 ν2 < 0.
In case of (PD3), one Can fix any νι satisfying VTR12V2 = 0, and by scaling ν2 to positive or
negative we can get VTRV → 一∞. Notice that once we have these V satisfying ντRV < 0, we can
recover η from V by backtracking the transformations.
Next, compute the Pareto spectrum of S := R11 — R12R^2 RT2 and check which case S belongs to:
(CP1) S = R11 — R12R^2RRT2 is strictly copositive.
(CP2) S is copositive, but ∃Vi ≥ 0 , Vi 6= 0 such that ViTSVi = 0.
(CP3) ∃Vi ≥ 0 such that ViTSVi < 0.
Here, Vi ’s are Pareto eigenvectors of S defined in Lemma B.2. If we have (CP3), we can declare
(T3) because one can fix V2 = -R22Rτ2Vι and get VTRV = vtSv、< 0. If the tests come out
(PD1) and (CP1), by Lemma B.3 we have (T1). For the remaining cases, we conclude (T2).
B.5 Proof of Lemma A.2
With the given η ,
δ1 =	[W1]k,∙
0(dh-k)×dx
0k-i
δi =	[bi]k
0dh -k
δ2 = [0dy ×(k-1)	-[W2]∙,k
0dy×(dh-k) .
It is straightforward to check that for all i ∈ [m],
dYi(xi) = ∆2O(xi) + W2 J(Xi)(∆ixi + δι) = —[O(xi)]k [W2]∙,k + W2
0k-i
[O(xi)]k	= 0.
0dh-k
From this, g(z, η)τη = Pi V'TdY!(xi) = 0. For the second order terms,
mm
ηTH(z, η)η = X V'TdY2(xi) + 1 X kdY1(xi)怜2'i = Xm=1 v`t∆2J(xi)(∆iXi + δi)
i=i	i=i
=Xm=1 v`t(-[O(χi)]k[W2]∙,k) = — (Xm=IQ(Xi)]kV'T) [W2]∙,k.
From the fact that Z is a FOSP of R, it follows that Pi V'iO(Xi)T = 0, so ητH(z, η)η = 0.
22
Published as a conference paper at ICLR 2019
B.6 Proof of Lemma A.3
The first equality is straightforward, because it follows from Si,k = {-1, +1} for all i ∈ Bk(1) that
taking union of {xTVk ≤ 0} and {xTVk ≥ 0} will eliminate the inequality constraints for i ∈ BkI).
For the next equality, we start by expressing U1 := vk | gk(z, vk)Tvk = 0 as a linear combination
of its linearly independent components. The set U1 can be expressed in the following form:
UI = {v⊥ + X αivi,k + X βivi,k 1 v⊥ ∈V⊥,∀i ∈ Bk) ,αi ∈ R, and ∀i ∈ Bk2,8i ≥ 0},
i∈Bk(1)	i∈Bk(2)
where Vi,k ∈ Vk ∩ span{Xj | j ∈ Bk \ {i}}⊥ for all i ∈ Bk1,2). Additionally, for i ∈ Bk2, Vi,k is
in the direction that satisfies σik = Sign(XTVi,k). To See Why U1 can be expressed in such a form,
first note that at the moment SO-TEST is executed, it is already given that the point z is a FOSP. So,
for any perturbation Vk we have gk(z, Vk) ∈ Vk, and gk(z, Vk)TV⊥ = 0 for any V⊥ ∈ Vk⊥. For the
remaining components, please recall FO-INCREASING-TEST and Lemma A.1; Vik are flat extreme
rays, so they are the ones satisfying gk(z, Vk)TVk = 0.
It remains to show that U := {vk | ∀i ∈ Bki),X,Vk = 0, and ∀i ∈ Bk(2),σ%,kXTVk ≥ 0} = Ui.
We show this by proving U1 ⊆ U2 and U1c ⊆ U2c .
To show the first part, we start by noting that for any v⊥ ∈ V⊥, XTv⊥ = 0 for i ∈ B?3)because
Xi ∈ Vk for these i’s. Also, for all i ∈ Bf2, it follows from the definition of vik that XT Vik = 0 for
all j ∈ Bk2,3). Similarly, for all i ∈ Bk22, XTVik = 0 for all j ∈ Bk2⑸ \ {i}, and σi,kXTVi,k > 0.
Therefore, any Vk ∈ U1 must satisfy all constraints in U2, hence U1 ⊆ U2 .
For the next part, we prove that Vk ∈ U1c violates at least one constraint in U2 . Observe that the
whole vector space Rp can be expressed as
Rp = {v⊥ + X,⑴ αiVi,k + X	⑶βiVi,k + W | W ∈ Vk ∩ span{Vi,k,i ∈ Bk1,2)}⊥,
i∈Bk	i∈Bk
V⊥ ∈ Vk⊥, ∀i ∈ Bk(1), αi ∈ R, and ∀i ∈ Bk(2), βi ∈ R}.
Therefore, any Vk ∈ Uc either has a nonzero component W in Vk ∩ span{Vi,k, i ∈ Bk1,2)}⊥ or
there exists i ∈ B2k)such that βi < 0. By definition, Vi,k ∈ span{Xj | j ∈ Bijk3')}⊥ for any
i ∈ Bp'2), which implies that Vk ∩ span{Vi,k, i ∈ Bk1,2)}⊥ = span{Xj | j ∈ Bfk3')}∙ Thus, a
nonzero component w ∈ span{Xj | j ∈ B,)} will violate some equality constraints in U. Next, in
case where ∃i ∈ B2(k) such that βi < 0, this violates the inequality constraint corresponding to i.
23