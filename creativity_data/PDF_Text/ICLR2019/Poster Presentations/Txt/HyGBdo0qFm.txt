Published as a conference paper at ICLR 2019
On the Turing Completeness of
Modern Neural Network Architectures
Jorge Perez, Javier Marinkovic, Pablo Barcelo
Department of Computer Science, Universidad de Chile & IMFD Chile
{jperez,jmarinkovic,pbarcelo}@dcc.uchile.cl
Ab stract
Alternatives to recurrent neural networks, in particular, architectures based on at-
tention or convolutions, have been gaining momentum for processing input se-
quences. In spite of their relevance, the computational properties of these alter-
natives have not yet been fully explored. We study the computational power of
two of the most paradigmatic architectures exemplifying these mechanisms: the
Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever,
2016). We show both models to be Turing complete exclusively based on their
capacity to compute and access internal dense representations of the data. In par-
ticular, neither the Transformer nor the Neural GPU requires access to an external
memory to become Turing complete. Our study also reveals some minimal sets of
elements needed to obtain these completeness results.
1	Introduction
There is an increasing interest in designing neural network architectures capable of learning algo-
rithms from examples (Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015; Kaiser
& Sutskever, 2016; Kurach et al., 2016; Dehghani et al., 2018). A key requirement for any such an
architecture is thus to have the capacity of implementing arbitrary algorithms, that is, to be Turing
complete. Turing completeness often follows for these networks as they can be seen as a control unit
with access to an unbounded memory; as such, they are capable of simulating any Turing machine.
On the other hand, the work by Siegelmann & Sontag (1995) has established a different way of
looking at the Turing completeness of neural networks. In particular, their work establishes that
recurrent neural networks (RNNs) are Turing complete even if only a bounded number of resources
(i.e., neurons and weights) is allowed. This is based on two conditions: (1) the ability of RNNs to
compute internal dense representations of the data, and (2) the mechanisms they use for accessing
such representations. Hence, the view proposed by Siegelmann & Sontag shows that it is possible to
release the full computational power of RNNs without arbitrarily increasing its model complexity.
Most of the early neural architectures proposed for learning algorithms correspond to extensions of
RNNs - e.g., Neural Turing Machines (Graves et al., 2014) -, and hence they are Turing complete
in the sense of Siegelmann & Sontag. However, a recent trend has shown the benefits of designing
networks that manipulate sequences but do not directly apply a recurrence to sequentially process
their input symbols. Architectures based on attention or convolutions are two prominent examples of
this approach. In this work we look at the problem of Turing completeness a` la Siegelmann & Sontag
for two of the most paradigmatic models exemplifying these features: the Transformer (Vaswani
et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016).
The main contribution of our paper is to show that the Transformer and the Neural GPU are Tur-
ing complete based on their capacity to compute and access internal dense representations of the
data. In particular, neither the Transformer nor the Neural GPU requires access to an external addi-
tional memory to become Turing complete. Thus the completeness holds for bounded architectures
(bounded number of neurons and parameters). To prove this we assume that internal activations are
represented as rational numbers with arbitrary precision. For the case of the Transformer we provide
a direct simulation of a Turing machine, while for the case of the Neural GPU our result follows by
simulating standard sequence-to-sequence RNNs. Our study also reveals some minimal sets of ele-
ments needed to obtain these completeness results. The computational power of Transformers and
1
Published as a conference paper at ICLR 2019
of Neural GPUs has been compared in the current literature (Dehghani et al., 2018), but both are
only informally used. Our paper provides a formal way of approaching this comparison.
For the sake of space, we only include sketch of some proofs in the body of the paper. The details
for every proof can be found in the appendix.
Background work The study of the computational power of neural networks can be traced back
to McCulloch & Pitts (1943) which established an analogy between neurons with hard-threshold
activations and first order logic sentences, and Kleene (1956) that draw a connection between neural
networks and finite automata. As mentioned earlier, the first work showing the Turing completeness
of finite neural networks with linear connections was carried out by Siegelmann & Sontag (1992;
1995). Since being Turing complete does not ensure the ability to actually learn algorithms in prac-
tice, there has been an increasing interest in enhancing RNNs with mechanisms for supporting this
task. One strategy has been the addition of inductive biases in the form of external memory, being
the Neural Turing Machine (NTM) (Graves et al., 2014) a paradigmatic example. To ensure that
NTMs are differentiable, their memory is accessed via a soft attention mechanism (Bahdanau et al.,
2014). Other examples of architectures that extend RNNs with memory are the Stack-RNN (Joulin
& Mikolov, 2015), and the (De)Queue-RNNs (Grefenstette et al., 2015). By Siegelmann & Sontag’s
results, all these architectures are Turing complete.
The Transformer architecture (Vaswani et al., 2017) is almost exclusively based on the attention
mechanism, and it has achieved state of the art results on many language-processing tasks. While
not initially designed to learn general algorithms, Dehghani et al. (2018) have advocated the need for
enriching its architecture with several new features as a way to learn general procedures in practice.
This enrichment is motivated by the empirical observation that the original Transformer architecture
struggles to generalize to input of lengths not seen during training. We, in contrast, show that
the original Transformer architecture is Turing complete, based on different considerations. These
results do not contradict each other, but show the differences that may arise between theory and
practice. For instance, Dehghani et al. (2018) assume fixed precision, while we allow arbitrary
internal precision during computation. We think that both approaches can be complementary as our
theoretical results can shed light on what are the intricacies of the original architecture, which aspects
of it are candidates for change or improvement, and which others are strictly needed. For instance,
our proof uses hard attention while the Transformer is often trained with soft attention (Vaswani
et al., 2017). See Section 3.3 for a discussion on these differences.
The Neural GPU is an architecture that mixes convolutions and gated recurrences over tridimen-
sional tensors. It has been shown that NeuralGPUs are powerful enough to learn decimal multi-
plication from examples (Freivalds & Liepins, 2018), being the first neural architecture capable of
solving this problem end-to-end. The similarity of Neural GPUs and cellular automata has been
used as an argument to state the Turing completeness of the architecture (Kaiser & Sutskever, 2016;
Price et al., 2016). Cellular automata are Turing complete (Smith III, 1971; Ollinger, 2012) and their
completeness is established assuming an unbounded number of cells. In the Neural GPU architec-
ture, in contrast, the number of cells that can be used during a computation is proportional to the
size of the input sequence (Kaiser & Sutskever, 2016). One can cope with the need for more cells
by padding the Neural GPU input with additional (dummy) symbols, as much as needed for a par-
ticular computation. Nevertheless, this is only a partial solution, as for a Turing-complete model of
computation, one cannot decide a priori how much memory is needed to solve a particular problem.
Our results in this paper are somehow orthogonal to the previous argument; we show that one can
leverage the dense representations of the Neural GPU cells to obtain Turing completeness without
requiring to add cells beyond the ones used to store the input.
2	Preliminaries
We assume all weights and activations to be rational numbers of arbitrary precision. Moreover, we
only allow the use of rational functions with rational coefficients. Most of our positive results make
use of the piecewise-linear sigmoidal activation function σ : Q → Q, which is defined as
0 x < 0,
σ(x) =	x 0 ≤ x ≤ 1,
1 x > 1.
(1)
2
Published as a conference paper at ICLR 2019
We are mostly interested in sequence-to-sequence (seq-to-seq) neural network architectures that we
next formalize. A seq-to-seq network N receives as input a sequence X = (x1, . . . , xn) of vectors
xi ∈ Qd, for some d > 0, and produces as output a sequence Y = (y1, . . . , ym) of vectors
yi ∈ Qd . Most of these types of architectures require a seed vector s and some stopping criterion
for determining the length of the output. The latter is usually based on the generation of a particular
output vector called an end of sequence mark. In our formalization instead, we allow a network to
produce a fixed number r ≥ 0 of output vectors. Thus, for convenience we see a general seq-to-
seq network as a function N such that the value N (X, s, r) corresponds to an output sequence of
the form Y = (y1 , y2 , . . . , yr). With this definition, we can view every seq-to-seq network as a
language recognizer of strings as follows.
Definition 2.1. A seq-to-seq language recognizer is a tuple A = (Σ, f, N, s, F), where Σ is a finite
alphabet, f : Σ → Qd is an embedding function, N is a seq-to-seq network, s ∈ Qd is a seed
vector, and F ⊆ Qd isa set of final vectors. We say that A accepts the string W ∈ Σ*, ifthere exists
an integer r ∈ N such that N (f (w), s, r) = (y1, . . . , yr) and yr ∈ F. The language accepted by
A, denoted by L(A), is the set of all strings accepted by A.
We impose two additional restrictions over recognizers. The embedding function f : Σ → Qd
should be computed by a Turing machine in time linear w.r.t. the size of Σ. This covers the two most
typical ways of computing input embeddings from symbols: the one-hot encoding, and embeddings
computed by fixed feed-forward networks. Moreover, the setF should also be recognizable in linear-
time; given a vector f, the membership f ∈ F should be decided by a Turing machine working in
linear time with respect to the size (in bits) of f . This covers the usual way of checking equality
with a fixed end-of-sequence vector. We impose these restrictions to disallow the possibility of
cheating by encoding arbitrary computations in the input embedding or the stop condition, while
being permissive enough to construct meaningful embeddings and stoping criterions.
Finally, a class N of seq-to-seq neural network architectures defines the class LN composed of all
the languages accepted by language recognizers that use networks in N . From these notions, the
formalization of Turing completeness of a class N naturally follows.
Definition 2.2. A class N of seq-to-seq neural network architectures is Turing Complete if LN is
exactly the class of languages recognized by Turing machines.
Given an input sequence X = (x1, . . . , xn), a seed vector y0, and r ∈ N, an encoder-decoder RNN
is given by the following two recursions
h0 = 0,	hi = f1(xiW + hi-1V + b1) (with 1 ≤ i ≤ n)	(2)
g0 = hn,	gt = f2(gt-1U + yt-1R + b2),	yt = O(gt)	(with 1 ≤ t ≤ r)	(3)
where V, W, U, R are matrices, bi and b? are vectors, O(∙) is an output function, and fι and f2
are activations functions. Equation (2) is called the RNN-encoder and (3) the RNN-decoder.
The next Theorem follows by inspection of the proof by Siegelmann & Sontag (1992; 1995) after
adapting it to our formalization of encoder-decoder RNNs.
Theorem 2.3 (Siegelmann & Sontag (1992; 1995)). The class of encoder-decoder RNNs is Turing
complete. Turing completeness holds even if we restrict to the class in which R is the zero matrix,
bi and b2 are the zero vector, O(∙) is the identity function, and fi and f2 are the piecewise-linear
sigmoidal activation σ.
3 The Transformer architecture
In this section we present a formalization of the Transformer architecture (Vaswani et al., 2017),
abstracting away from specific choices of functions and parameters. Our formalization is not meant
to produce an efficient implementation of the Transformer, but to provide a simple setting over which
its mathematical properties can be established in a formal way.
The Transformer is heavily based on the attention mechanism introduced next. Consider a scoring
function score : Qd × Qd → Q and a normalization function ρ : Qn → Qn, for d, n > 0. Assume
that q ∈ Qd, and that K = (ki, . . . , kn) and V = (vi, . . . , vn) are tuples of elements in Qd.
3
Published as a conference paper at ICLR 2019
A q-attention over (K, V ), denoted by Att(q, K, V ), is a vector a ∈ Qd defined as follows.
(s1, . . . ,sn) = ρ(score(q, k1), score(q, k2),. . . ,score(q,kn))	(4)
a = SlVl + S2V2 +--------+ SnVn.	(5)
Usually, q is called the query, K the keys, and V the values. We do not pose any restriction
on the scoring and normalization functions, as some of our results hold in general. We only re-
quire the normalization function to satisfy that there is a function fρ from Q to Q+ such that for
each x = (x1 , . . . , xn) ∈ Qn it is the case that the i-th component ρi (x) of ρ(x) is equal to
fρ(xi)/ Pjn=1 fρ(xj). Thus, a in Equation (5) is a convex combination of the vectors in V .
When proving possibility results, we will need to pick specific scoring and normalization functions.
A usual choice for the scoring function is a feed forward network with input (q, ki) sometimes called
additive attention (Bahdanau et al., 2014). Another possibility is to use the dot product hq, kii called
multiplicative attention (Vaswani et al., 2017). We use a combination of both: multiplicative atten-
tion plus a non linear function. For the normalization function, softmax is a standard choice. Never-
theless, in our proofs we use the hardmax function, which is obtained by setting fhardmax (xi) = 1
ifxi is the maximum value, and fhardmax(xi) = 0 otherwise. Thus, for a vector x in which the max-
imum value occurs r times, We have that hardmaxi(x) = 11 if Xi is the maximum value of x, and
hardmaxi(x) = 0 otherwise. We call it hard attention whenever hardmax is used as normalization
function. As customary, for a function F : Qd → Qd and a sequence X = (x1, x2, . . . , xn), With
xi ∈ Qd, We Write F(X) to denote the sequence (F (x1), . . . , F (xn)).
Transformer Encoder and Decoder A single-layer encoder of the Transformer is a parametric
function Enc(X; θ) receiving a sequence X = (x1, . . . , xn) of vectors in Qd and returning a
sequence Z = (z1, . . . , zn) of the same length of vectors in Qd. In general, We consider the
parameters in Enc(X; θ) as functions Q(∙), K(∙),V(∙), and O(∙), all of them from Qd to Qd. The
single-layer encoder is then defined as folloWs
ai = Att(Q(xi), K(X), V(X)) + xi	(6)
zi = O(ai) + ai	(7)
In practice Q(∙), K(∙), V(∙) are typically matrix multiplications, and O(∙) a feed-forward network.
The + xi and + ai summands are usually called residual connections (He et al., 2016a;b). When
the particular functions used as parameters are not important, we simply write Z = Enc(X).
The Transformer encoder is defined simply as the repeated application of single-layer encoders
(with independent parameters), plus two final transformation functions K(∙) and V(∙) applied to
every vector in the output sequence of the final layer. Thus the L-layer Transformer encoder is
defined by the following recursion (with1 ≤ ` ≤ L- 1 andX1 = X).
X'+1 =Enc(X'; θ`),	K = K(XL),	V = V(XL).	(8)
We use (K, V) = TEncL(X) to denote an L-layer Transformer encoder over the sequence X.
A single-layer decoder is similar to a single-layer encoder but with additional attention to an external
pair of key-value vectors (Ke, Ve). The input for the single-layer decoder is a sequence Y =
(y1, . . . , yk) plus the external pair (Ke, Ve), and the output is a sequence Z = (z1, . . . , zk). When
defining a decoder layer we denote by Yj the sequence (y1, . . . , yj), for 1 ≤ j ≤ k. The layer is
also parameterized by four functions Q(∙), K(∙), V(∙) and O(∙) and is defined as follows.
pi	=	Att(Q(yi),K(Yi),V(Yi))+yi	(9)
ai	=	Att(pi, Ke, Ve) + pi	(10)
zi	=	O(ai) + ai	(11)
Notice that the first (self) attention over (K(Yi), V(Yi)) considers the subsequence of Y only until
index i and is used to generate a query pi to attend the external pair (Ke, Ve). We denote the
single-decoder layer by Dec((Ke, Ve), Y ; θ).
The Transformer decoder is a repeated application of single-layer decoders, plus a transformation
function F : Qd → Qd applied to the final vector of the decoded sequence. Thus, the output of the
decoder is a single vector z ∈ Qd. Formally, the L-layer Transformer decoder is defined as
Y'+1 = Dec((Ke, Ve), Y'; θ`),	Z = F(yk)	(1 ≤ ' ≤ L - 1 and Y1 = Y).	(12)
We use z = TDecL((Ke, Ve), Y) to denote an L-layer Transformer decoder.
4
Published as a conference paper at ICLR 2019
The complete Tansformer A Transformer network receives an input sequence X , a seed vector
y0, and a value r ∈ N. Its output is a sequence Y = (y1, . . . , yr) defined as
yt+1 = TDec(TEnc(X), (y0,y1, . . . ,yt)),	for 0 ≤ t ≤ r - 1.	(13)
We denote the output sequence of the transformer as Y = (y1, y2, . . . , yr) = Trans(X, y0, r).
3.1	Invariance under proportions
The Transformer, as defined above, is order-invariant: two input sequences that are permutations of
each other produce exactly the same output. This is a consequence of the following property of the
attention function: if K = (k1 , . . . , kn), V = (v1 , . . . , vn), and π : {1, . . . , n} → {1, . . . , n} is
a permutation, then Att(q, K, V ) = Att(q, π(K), π(V )) for every query q. This weakness has
motivated the need for including information about the order of the input sequence by other means;
in particular, this is often achieved by using the so-called positional encodings (Vaswani et al., 2017;
Shaw et al., 2018), which we study below.
But before going into positional encodings, a natural question is what languages the Transformer can
recognize without them. As a standard yardstick we use the well-studied class of regular languages,
i.e., languages recognized by finite automata. Order-invariance implies that not every regular lan-
guage can be recognized by a Transformer network. As an example, there is no Transformer network
that can recognize the regular language (ab)*, as the latter is not order-invariant. A reasonable ques-
tion then is whether the Transformer can express all regular languages which are order-invariant.
It is possible to show that this is not the case by proving that the Transformer actually satisfies a
stronger invariance property, which we call proportion invariance.
For a string W ∈ Σ* and a symbol a ∈ Σ,we use prop(a, W) to denote the ratio between the number
of times that a appears in W and the length of w. Consider now the set ProPInv(W) = {u ∈ Σ* |
prop(a, W) = prop(a, u) for every a ∈ Σ}.
Proposition 3.1. Let Trans be a Transformer, s a seed, r ∈ N, and f : Σ → Qd an embedding
function. Then Trans(f (w), s,r) = Trans(f (U), s,r), for each u,w ∈ Σ* with U ∈ ProPInv(W).
As an immediate corollary we obtain the following.
Corollary 3.2. Consider the order-invariant regular language L = {w ∈ {a, b}* | W has an even
number ofa symbols}. Then L cannot be recognized by a Transformer network.
On the other hand, languages recognized by Transformer networks are not necessarily regular.
Proposition 3.3. There is a Transformer network that recognizes the non-regular language S =
{w ∈ {a, b}* | W has strictly more symbols a than symbols b}.
That is, the computational power of Transformer networks without positional encoding is both rather
weak (they do not even contain order-invariant regular languages) and not so easy to capture (as
they can express counting properties that go beyond regularity). As we show in the next section, the
inclusion of positional encodings radically changes the picture.
3.2	Positional Encodings and Completeness of the Transformer
Positional encodings come to remedy the order-invariance issue by providing information about the
absolute positions of the symbols in the input. A positional encoding is just a function Pos : N →
Qd . Function Pos combined with an embedding function f : Σ → Qd give rise to a new embedding
function fpos : Σ × N → Qd such that fpos(a, i) = f(a) + Pos(i). Thus, given an input string
W = a1a2 …an ∈ Σ, the result of the embedding function fpos(W) provides a “new” input
fpos (a1 , 1), fpos (a2 , 2), . . . , fpos (an , n)
to the Transformer encoder. Similarly, the Transformer decoder instead of receiving the sequence
Y = (y0 , y1 , . . . , yt ) as input, it receives now the sequence
Y0 = y0 + Pos(1), y1 + Pos(2), . . . ,yt + Pos(t + 1)
As for the case of the embedding functions, we require the positional encoding Pos(i) to be com-
putable by a Turing machine working in linear time w.r.t. the size (in bits) of i.
The main result of this section is the completeness of Transformers with positional encodings.
5
Published as a conference paper at ICLR 2019
Figure 1: High-level structure of the decoder part of TransM .
Theorem 3.4. The class of Transformer networks with positional encodings is Turing complete.
Proof Sketch. We show that for every Turing machine M = (Q, Σ, δ, qinit, F) there exists a trans-
former that simulates the complete execution of M. We represent a string W = s1s2 …Sn ∈ Σ* as
a sequence X of one-hot vectors with their corresponding positional encodings. Denote by q(t) ∈ Q
the state of M at time t when processing w, and s(t) ∈ Σ the symbol under M’s head at time t.
Similarly, v(t) ∈ Σ is the symbol written by M and m(t) ∈ {—, →} the head direction. We next de-
scribe how to construct a transformer TransM that with input X produces a sequence y0, y1, y2, . . .
such that yi contains information about q(i) and s(i) (encoded as one-hot vectors).
The construction and proof goes by induction. Assume the decoder receives y0 , . . . , yt such that yi
contains q(i) and s(i). To construct yt+1, in the first layer we just implement M’s transition function
δ; note that δ(q(i), s(i)) = (q(i+1), v(i), m(i)) thus, we use (q(i), s(i)) to compute (q(i+1), v(i), m(i))
for every i and store them in the sequence z01, . . . , zt1. This computation can be done with a two-
layer feed-forward network. For the next layer, lets denote by c(i) the index of the cell that M is
pointing to at time i. It can be proved that given z01 , . . . , zt1 one can compute (a representation of)
c(i) and c(i+1) for every i ≤ t with a self-attention layer, and store them in z02, . . . , zt2. In particular,
zt2 contains c(t+1) which is the index to which M is going to be pointing to in the next time step. By
using the residual connections we also store q(i+1) and v(i) in zi2. The final piece of our construction
is to compute the symbol that the tape holds at index c(t+1), that is, the symbol under M’s head at
time t + 1. For this we use the following observation: the symbol at index c(t+1) in time t + 1
coincides with the last symbol written by M at index c(t+1). Thus, we need to find the maximum
value i? ≤ t such that c(i? ) = c(t+1) and then copy v(i? ) which is the symbol that was written
by M at time step i? . This last computation can also be done with a self-attention layer. Thus,
we attend directly to position i? (hard attention plus positional encodings) and copy v(i? ) which is
exactly s(t+1). We finally copy q(t+1) and s(t+1) into the output to construct yt+1. Figure 1 shows
a high-level diagram of the decoder computation.
There are several other details in the construction, in particular, at the beginning of the computation
(first n steps), the decoder needs to attend to the encoder and copy the input symbols so they can later
be processed as described above. Another detail is when M reaches a cell that has not been visited
before, then the symbol under the head has to be set as # (the blank symbol). We show that all these
decisions can be implemented with feed-forward networks plus attention. The complete construction
uses one encoder layer, three decoder layers and vectors of dimension d = 2∣Q∣ + 4∣∑∣ + 11 to store
one-hot representations of states, symbols and some additional working space. All details can be
found in the appendix.	□
3.3 Differences with Vaswani et al. (2017)’s framework
Although the general architecture that we presented closely follows that of Vaswani et al. (2017),
some choices for functions and parameters in our positive results are different to the usual choices in
practice. For instance, we use hard attention which allow us to attend directly to specific positions. In
contrast, Vaswani et al. (2017) use softmax to attend, plus sin-cos functions as positional encodings.
The softmax, sin and cos are not rational functions, and thus, are forbidden in our formalization.
6
Published as a conference paper at ICLR 2019
An interesting line for future work is to consider arbitrary functions but with additional restrictions,
such as finite precision as done by Weiss et al. (2018). Another difference is that for the function
O(∙) in Equation (11) our proof uses a feed-forward network with various layers, while in Vaswani
et al. (2017) only two layers are used.
The need of arbitrary precision Our Turing-complete proof relies on having arbitrary precision
for internal representations, in particular, for storing and manipulating positional encodings. Al-
though having arbitrary precision is a standard assumption when studying the expressive power of
neural networks (Cybenko (1989); Siegelmann & Sontag (1995)) practical implementations rely on
fixed precision hardware. If fixed precision is used, then positional encodings can be seen as func-
tions of the form pos : N → A where A is a finite subset of Qd . Thus, the embedding function fpos
can be seen as a regular embedding function f0 : Σ0 → Qd where Σ0 = Σ × A. Thus, whenever
fixed precision is used, the net effect of having positional encodings is to just increase the size of the
input alphabet. Then from Proposition 3.1 we obtain that the Transformer with positional encodings
and fixed precision is not Turing complete. Although no longer Turing complete, one can still study
the computational power of fixed-precision Transformers. We left this as future work.
4	Neural GPUs
The Neural GPU (Kaiser & Sutskever, 2016) is an architecture that mixes convolutions and gated
recurrences over tridimensional tensors. It is parameterized by three functions U(∙) (update func-
tion), R(∙) (reset function), and F(∙). Given a tensor S ∈ Qh×w×d and a value r ∈ N, it produces a
sequence S1, S2, . . . , Sr given by the following recursive definition (with S0 = S).
Ut = U(St-1),	Rt =	R(St-1),	St	= Ut	St-1	+	(1	-U)F(RtSt-1).
where denotes the element-wise product, and 1 is a tensor with only 1’s. Neural GPUs force
functions U(∙) and R(∙) to produce a tensor of the same shape as its input with all values in [0,1].
Thus, a Neural GPU resembles a gated recurrent unit (Cho et al., 2014), with U working as the
update gate and R as the reset gate. Functions U(∙), R(∙), and F(∙) are defined as a convolution of
its input with a 4-dimensional kernel bank with shape (kH, kW, d, d) plus a bias tensor, followed by
a point-wise transformation
f (K * S + B)	(14)
with different kernels and biases for U(∙), R(∙), and F(∙).
To have an intuition on how the convolution K * S works, it is illustrative to think of S as an
(h × w)-grid of (row) vectors and K as a (kH × kW)-grid of (d × d) matrices. More specifically, let
sij = Si,j,:, and Kij = Ki,j,:,:, then K * S is a regular two-dimensional convolution in which scalar
multiplication has been replaced by vector-matrix multiplication as in the following expression
(K * S)i,j,: =	si+∆1(u),j+∆2(v) Kuv,	(15)
where ∆1(u) = u - bkH /2c - 1 and ∆2 (v) = v - bkW /2c - 1. This intuition makes evident
the similarity between Neural GPUs and cellular automata: S is a grid of cells, and in every itera-
tion each cell is updated considering the values of its neighbors according to a fixed rule given by
K (Kaiser & Sutskever, 2016). As customary, we assume zero-padding when convolving outside S.
4.1 The computational power of Neural GPUs
To study the computational power of Neural GPUs, we cast them as a standard seq-to-seq architec-
ture. Given an input sequence, we put every vector in the first column of the tensor S. We also need
to pick a special cell of S as the output cell from which we read the output vector in every iteration.
We pick the last cell of the first column of S. Formally, given a sequence X = (x1, . . . , xn) with
xi ∈ Qd, and a fixed value w ∈ N, we construct the tensor S ∈ Qn×w×d by leting Si,1,: = xi and
Si,j,: = 0 for j > 1. The output of the Neural GPU, denoted by NGPU(X, r), is the sequence of
vectors Y = (y1, y2, . . . , yr) such that yt = Stn,1,:. Given this definition, we can naturally view
the Neural GPUs as language recognizers (as formalized in Section 2).
7
Published as a conference paper at ICLR 2019
Since the bias tensor B in Equation (14) is of the same size than S, the number of parameters in a
Neural GPU grows with the size of the input. Thus, a Neural GPU cannot be considered as a fixed
architecture. To tackle this issue we introduce the notion of uniform Neural GPU, as one in which
for every bias B there exists a matrix B ∈ Qw×d such that Bi,:,: = B for each i. Thus, uniform
Neural GPUs can be finitely specified (as they have a constant number of parameters, not depending
on the length of the input). We now establish the Turing completeness of this model.
Theorem 4.1. The class of uniform Neural GPUs is Turing complete.
Proof sketch. The proof is based on simulating a seq-to-seq RNN; thus, completeness follows from
Theorem 2.3. Consider an RNN encoder-decoder language recognizer, such that N is of dimension
d and its encoder and decoder are defined by the equations hi = σ(xiW + hi-1V ) and gt =
σ(gt-1U), respectively, where g0 = hn and n is the length of the input. We use a Neural GPU with
input tensor S ∈ Qn×1×3d+3. Let Ei = Si,1,1:d and Di = Si,1,d+1:2d. The idea is to use E for the
encoder and D for the decoder. We use kernel banks of shape (2, 1, 3d + 3, 3d + 3) with uniform
bias tensors to simulate the following computation. In every step t, we first compute the value of
σ(EtW + Et-1V ) and store it in Et, and then reset Et-1 to zero. Similarly, in step t we update the
vector in position Dt-1 storing in it the value σ(Dt-1U + Et-1U) (for the value of Et-1 before the
reset). We use the gating mechanism to ensure a sequential update of the cells such that at time t
we update only positions Ei and Dj for i ≤ t and j ≤ t - 1. Thus the updates on the D are always
one iteration behind the update of E. Since the vectors in D are never reset to zero, they keep being
updated which allows us to simulate an arbitrary long computation. In particular we prove that at
iteration t it holds that Et = ht, and at iteration n + t it holds that Dn = gt. We require 3d + 3
components, as we need to implement several gadgets for properly using the update and reset gates.
In particular, we need to store the value of Et-1 before we reset it. The detailed construction and
the correctness proof can be found in the appendix.	□
The proof above makes use of kernels of shape (2, 1, d, d) to obtain Turing completeness. This is,
in a sense, optimal, as one can easily prove that Neural GPUs with kernels of shape (1, 1, d, d) are
not Turing complete, regardless of the size of d. In fact, for kernels of this shape the value of a cell
of S at time t depends only on the value of the same cell in time t - 1.
Zero padding vs circular convolution The proof of Theorem 4.1 requires the application of zero
padding in convolution. This allows us to clearly differentiate internal cells from cells corresponding
to the endpoints of the input sequence. Interestingly, Turing-completeness is lost if we replace zero
padding with circular convolution. Formally, given S ∈ Qh×w×d, a circular convolution is obtained
by defining Sh+n,:,: = Sn,:,: for n ∈ Z. One can prove that uniform Neural GPUs with circular
convolutions cannot differentiate among periodic sequences of different length; in particular, they
cannot check if a periodic input sequence is of even or odd length. This yields the following:
Proposition 4.2. Uniform Neural GPUs with circular convolutions are not Turing complete.
Related to this last result is the empirical observation by Price et al. (2016) that Neural GPUs that
learn to solve hard problems, e.g., binary multiplication, and which generalize to most of the inputs,
struggle with highly symmetric (and nearly periodic) inputs. Actually, Price et al. (2016) exhibit
examples of the form 11111111 × 11111111 failing for all inputs with eight or more 1s. We leave
as future work to explore the implications of our theoretical results on this practical observation.
Bidimensional tensors and piecewise linear activations Freivalds & Liepins (2018) simplified
Neural GPUs and proved that, by considering piecewise linear activations and bidimensional in-
put tensors instead of the original smooth activations and tridimensional tensors used by Kaiser &
Sutskever (2016), it is possible to achieve substantially better results in terms of training time and
generalization. Our Turing completeness proof also relies on a bidimensional tensor and uses piece-
wise linear activations, thus providing theoretical evidence that these simplifications actually retain
the full expressiveness of Neural GPUs while simplifying its practical applicability.
8
Published as a conference paper at ICLR 2019
5	Final Remarks and Future Work
We have presented an analysis of the Turing completeness of two popular neural architectures for
sequence-processing tasks; namely, the Transformer, based on attention, and the Neural GPU, based
on recurrent convolutions. We plan to further refine this analysis in the future. For example, our
proof of Turing completeness for the Transformer requires the presence of residual connections,
i.e., the +xi, +ai, +yi, and +pi summands in Equations (6-11), while our proof for Neural GPUs
heavily relies on the gating mechanism. We will study whether these features are actually essential
to obtain completeness.
We presented general abstract versions of both architectures in order to prove our theoretical results.
Although we closely follow their original definitions, some choices for functions and parameters
in our positive results are different to the usual choices in practice, most notably, the use of hard
attention for the case of the Transformer, and the piecewise linear activation functions for both
architectures. As we have mentioned, Freivalds & Liepins (2018) showed that for Neural GPUs
piecewise linear activations actually help in practice, but for the case of the Transformer architecture
more experimentation is needed to have a conclusive response. This is part of our future work.
Although our results are mostly of theoretical interest, they might lead to observations of practical
interest. For example, Chen et al. (2018) have established the undecidability of several practical
problems related to probabilistic language modeling with RNNs. This means that such problems can
only be approached in practice via heuristics solutions. Many of the results in Chen et al. (2018) are,
in fact, a consequence of the Turing completeness of RNNs as established by Siegelmann & Sontag
(1995). We plan to study to what extent our analogous undecidability results for Transformers and
Neural GPUs imply undecidability for language modeling problems based on these architectures.
Finally, our results rely on being able to compute internal representations of arbitrary precision. It
would be interesting to perform a theoretical study of the main properties of both architectures in a
setting in which only finite precision is allowed, as have been recently carried out for RNNs (Weiss
et al., 2018). We also plan to tackle this problem in our future work.
Acknowledgements
This work was supported by the Millennium Institute for Foundational Research on Data (IMFD).
9
Published as a conference paper at ICLR 2019
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/
abs/1409.0473.
Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural
networks as weighted language recognizers. In NAACL-HLT2018, pp. 2261-2271, 2018.
KyUnghyUn Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing, EMNLP, pp. 1724-1734, 2014.
George Cybenko. Approximation by superpositions of a sigmoidal function. MCSS, 2(4):303-314,
1989. doi: 10.1007/BF02551274. URL https://doi.org/10.1007/BF02551274.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Univer-
sal transformers. CoRR, abs/1807.03819, 2018. URL https://arxiv.org/abs/1807.
03819.
Karlis Freivalds and Renars Liepins. Improving the neural GPU architecture for algorithm learning.
In Neural Abstract Machines & Program Induction (NAMPI), 2018.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv preprint
arXiv:1410.5401, 2014.
Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to
transduce with unbounded memory. In Advances in Neural Information Processing Systems, pp.
1828-1836, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las
Vegas, NV, USA, June 27-30, 2016, pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual net-
works. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Nether-
lands, October 11-14, 2016, Proceedings, Part IV, pp. 630-645, 2016b.
Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent
nets. In Advances in neural information processing systems, pp. 190-198, 2015.
Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In ICLR, 2016.
S. C. Kleene. Representation of events in nerve nets and finite automata. In Claude Shannon and
John McCarthy (eds.), Automata Studies, pp. 3-41. Princeton University Press, 1956.
Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. In
ICLR, 2016.
Warren McCulloch and Walter Pitts. A logical calculus of ideas immanent in nervous activity.
Bulletin of Mathematical Biophysics, 5:127-147, 1943.
Nicolas Ollinger. Universalities in cellular automata. In Handbook of Natural Computing, pp. 189-
229. 2012.
Eric Price, Wojciech Zaremba, and Ilya Sutskever. Extensions and limitations of the neural GPU.
CoRR, abs/1611.00736, 2016. URL http://arxiv.org/abs/1611.00736.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-
tions. In NAACL-HLT, pp. 464-468, 2018.
Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. In Pro-
ceedings of the Fifth Annual ACM Conference on Computational Learning Theory, COLT, pp.
440-449, 1992.
10
Published as a conference paper at ICLR 2019
Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. J. Comput.
Syst. Sci., 50(1):132-150,1995.
Alvy Ray Smith III. Simple computation-universal cellular spaces. Journal of the ACM (JACM), 18
(3):339-353, 1971.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998-6008, 2017.
Gail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of finite preci-
sion RNNs for language recognition. In ACL 2018, pp. 740-745, 2018.
11
Published as a conference paper at ICLR 2019
A	Proofs for Section 2
A.1 Proof of Theorem 2.3
We first sketch the main idea of Siegelmann & Sontag’s proof. We refer the reader to the original
paper for details. Siegelmann & Sontag show how to simulate a two-stack machine M (and subse-
quently, a Turing machine) with a single RNN N with σ as activation. They first construct a network
Ni that, with 0 as initial state (hN1 = 0) and with a binary string W ∈ {0,1}* as input sequence,
produces a representation of w as a rational number and stores it as one of its internal values. Their
internal representation of strings encodes every w as a rational number between 0 and 1. In particu-
lar, they use base 4 such that, for example, a string w = 100110 is encoded as (0.311331)4 that is,
its encoding is
3 × 4-1 + 1 × 4-2 + 1 × 4-3 + 3 × 4-4 + 3 × 4-5 + 1 × 4-6.
This representation allows one to easily simulate stack operations as affine transformations plus σ
activations. For instance, if Xw is the value representing string W = b1b2 ∙∙∙ bn seen as a stack, then
the top(w) operation can be defined as simply y = σ(4xw - 2), since y = 1 if and only if b1 = 1,
and y = 0 if and only if b1 = 0. Other stack operations can de similarly simulated. Using this
representation, they construct a second network N2 that simulates the two-stacks machine by using
one neuron value to simulate each stack. The input W for the simulated machine M is assumed to
be at an internal value given to N2 as an initial state (h0N2). Thus, N2 expects only zeros as input.
Actually, to make N2 work for r steps, an input of the form 0r should be provided.
Finally, they combine N1 and N2 to construct a network N which expects an input of the following
form: (bι,1,0)(b2,1,0)…(bn, 1,0)(0,0,1)(0,0,0)(0,0,0)…(0,0,0). The idea is that the first
component contains the input string W = b1b2 … bn the second component states when the input
is active, and the third component is 1 only when the input is inactive for the first time. Before the
input vector (0, 0, 1) the network N1 is working. The input (0, 0, 1) is used to simulate a change
from N1 to N2, and the rest of input vectors (0, 0, 0) are provided to continue with N2 for as many
steps as needed. The number neurons that this construction needs to simulate a machine M with m
states, is 10m + 30. 1
Itis clear that Siegelmann & Sontag’s proof resembles a modern encoder-decoder RNN architecture,
where N1 is the encoder and N2 is the decoder, thus itis straightforward to use the same construction
to provide an RNN encoder-decoder N0 and a language recognizer A that uses N0 and simulates the
two-stacks machine M . There are some details that is important to notice. Assume that N0 is given
by the formulas in Equations (2) and (3). First, since N2 in the above construction expects no input,
we can safely assume that R in Equation (3) is the null matrix. Moreover, since A defines its own
embedding function, we can ensure that every vector that we provide for the encoder part of N0 has a
1 in a fixed component, and thus we do not need the bias b1 in Equation (2) since it can be simulated
with one row of matrix V . We can do a similar construction for the bias b2 (Equation (3)). Finally,
Siegelmann & Sontag show that its construction can be modified such that a particular neuron of
N2 , say n?, is always 0 except for the first time an accepting state of M is reached, in which case
n? = 1. Thus, one can consider O(∙) (Equation (3)) as the identity function and add to A the
stopping criterion that just checks ifn? is 1. This completes the proof sketch of Theorem 2.3.
1The idea presented above allows one to linearly simulate M, that is, each step of M is simulated with a
constant number of steps of the corresponding RNN. Siegelmann & Sontag show that, with a refinement of the
above encoding one can simulate M in real-time, that is, a single step of M is simulated with a single step
of the recurrent network. The 10m + 30 is the bound given by a simulation with slow-down of two. See the
original paper for details (Siegelmann & Sontag, 1995).
12
Published as a conference paper at ICLR 2019
B Proofs for Section 3
B.1 Proof of Proposition 3.1
We extend the definition of the function PropInv to sequences of vectors. Given a sequence X =
(x1, . . . , xn) we use vals(X) to denote the set of all vectors occurring in X. Similarly as for strings,
we use prop(v, X) as the number of times that v occurs in X divided by the length of X. Now we
are ready to extend PropInv with the following definition:
PropInv(X) = {X0 | vals(X 0) = vals(X) and prop(v, X) = prop(v, X0) for all v ∈ vals(X)}
Notice that for every embedding function f : Σ → Qd and string W ∈ Σ*, we have that if
u ∈ PropInv(w) then f(u) ∈ PropInv(f (w)). Thus in order to prove that Trans(f (w), s, r) =
Trans(f (u), s, r) for every u ∈ PropInv(w), it is enough to prove that
Trans(X, s, r) = Trans(X 0, s, r) for every X0 ∈ PropInv(X)	(16)
To further simplify the exposition of the proof we introduce another notation. We denote by pvX as
the number of times that vector v occurs in X. Thus we have that X0 ∈ PropInv(X) if and only
if, there exists a value γ ∈ Q+ such that for every v ∈ vals(X) it holds that pvX0 = γpvX.
We now have all the necessary to proceed with the proof of Proposition 3.1. We will prove it by
proving the property in (16). Let X = (x1, . . . , xn) be an arbitrary sequence of vectors, and
let X0 = (x01, . . . , x0m) ∈ PropInv(X). Moreover, let Z = (z1 , . . . , zn) = Enc(X; θ) and
Z0 = (z10 , . . . , zm0 ) = Enc(X0; θ). We first prove the following property:
For every pair of indices (i, j) ∈ {1, . . . , n} × {1, . . . , m}, if xi = x0j then zi = zj0 .	(17)
Lets (i, j) be a pair of indices such that xi = x0j. From Equations (6-7) we have that zi = O(ai)+ai
where ai = Att(Q(xi), K(X), V (X)) + xi. Thus, since xi = x0j, in order to prove zi = zj0 it is
enough to prove that Att(Q(xi), K(X), V (X)) = Att(Q(x0j), K(X0), V (X0)). By equations (4-
5) and the restriction over the form of normalization functions we have that
1n
Att(Q(Xi),K (X ),V (X)) = — EfP(Score(Q(xjK (xg)))V (xg)
α
'=1
where α = Pn=I fρ(score(Q(x'), K(x'))). The above equation can be rewritten as
Att(Q(Xi), K(X), V(X)) = α X	PX fρ(score(Q(χi),K(v)))V(v)
v∈vals(X)
with α = Pv∈vals(X) PvXfρ(score(Q(v), K (v))). By a similar reasoning we can write
Att(Q(Xj),K(X0), V (X 0)) = 1 X	pX0fρ(score(Q(xj),K(v)))V(v)
v∈vals(X0)
with β = Pv∈vals(X0) PvX0fρ(score(Q(v), K(v))). Now, since X0 ∈ PropInv(X) we know that
vals(X) = vals(X0) and there exists a γ ∈ Q+ such that PvX0 = γPvX for every v ∈ vals(X).
Finally, from this last property, plus the fact that Xi = X0j we have
Att(Q(Xj),K(X0), V(X0)) = -1α	X	YPXfρ(score(Q(xj),K(v)))V(v)
γ v∈vals(X )
=1 X	PX fρ(score(Q(xi),K(v)))V(v)
v∈vals(X)
= Att(Q(Xi), K(X), V(X))
Which completes the proof of Property (17) above.
Consider now the complete encoder TEnc. Let (K, V ) = TEnc(X) and (K0, V 0) = TEnc(X 0),
and let q be an arbitrary vector. We will prove now that Att(q, K, V ) = Att(q, K0, V 0). By
13
Published as a conference paper at ICLR 2019
following a similar reasoning as for proving Property (17) (plus induction on the layers of TEnc)
we obtain that if xi = x0j then ki = kj0 and vi = vj0, for every i ∈ {1, . . . , n} and j ∈ {1, . . . , m}.
Thus, there exists a mapping MK : vals(X) → vals(K) such that MK(xi) = ki and MK (x0j) =
kj0 and similarly a mapping MV : vals(X) → vals(V ) such that MV (xi) = vi and MV (x0j) = vj0 ,
for every i ∈ {1, . . . , n} and j ∈ {1, . . . , m}. Lets focus now on Att(q, K, V ). We have:
1n
Att(q, K, V) = — £ fρ(score(q, ki))vi
— i=1
with — = Pin=1 fρ (score(q, ki)). Similarly as before, we can rewrite this as
1n
Att(q, K, V)=—工 fρ(score(q, MK(Xi)))MV(Xi)
=1 X	PX fρ(score(q,Mκ (V)))MV(V)
v∈vals(X)
with — = Pv∈vals(X) pvX fρ(score(q, MK (V))). Similarly for Att(q, K0, V0) we have
1m
Att(q, K, V) = β X fρ(score(q, Mk (Xj)))MV (Xj)
=β X	PX0fρ(score(q, Mk(V)))MV(V)
v∈vals(X0)
And finally using that X0 ∈ PropInv(X) we obtain
Att(q, K0, V0) = 1 X	PX0 fρ(SCore(q, Mk(V)))MV(V)
v∈vals(X0)
=-—X	YPXfρ(score(q, Mk(v)))Mv(v)
γ v∈vals(X)
=―X	PX fρ(score(q,K (v))V (v)
v∈vals(X)
= Att(q, K, V)
which is what we wanted.
To complete the rest proof, consider Trans(X, y0, r) which is defined by the recursion
yk+1	= TDeC(TEnC(X), (y0,y1, . . . , yk))
To prove that Trans(X, y0, r) = Trans(X0, y0, r) we use an inductive argument. We know that
y1 = TDeC(TEnC(X), (y0))
= TDeC((K,V),(y0)).
Now TDeC only access (K, V) via attentions of the form Att(q, K, V) and for the case of y1 the
vector q can only depend on y0, thus, from Att(q, K, V) = Att(q, K0, V0) we have that
y1 = TDeC((K, V), (y0))
= TDeC((K0, V0), (y0))
= TDeC(TEnC(X0), (y0)).
The rest of the steps follow by a simple induction on k.
B.2	Proof of Corollary 3.2
To obtain a contradiction, assume that there is a language recognizer A that uses a Transformer
network and such that L = L(A). Now consider the strings w1 = aabb and w2 = aaabbb. Since
w1 ∈ PropInv(w2) by Proposition 3.1 we have that w1 ∈ L(A) if and only if w2 ∈ L(A) which is
a contradiction since w1 ∈ L but w2 ∈/ L. This completes the proof of the corollary.
14
Published as a conference paper at ICLR 2019
B.3	Proof of Proposition 3.3
We construct a language recognizer A = (Σ, f, Trans, s, F) with Trans a very simple Transformer
network with dimension d = 2 and using just one layer of encoder and one layer of decoder, such
that L(A) = {w ∈ {a,b}* | W has strictly more symbols a than symbols b}. As embedding function,
we use f(a) = [0, 1] and f(b) = [0, -1].
Assume that the output for the encoder part of the transformer is X = (x1, . . . , xn). First we
use an encoder layer that implements the identity function. This can be trivially done using null
functions for the self attention and through the residual connections this encoder layer shall pre-
serve the original Xi values. For the final V(∙) and K(∙) functions of the Transformer encoder
(Equation (8)), we use V(x) = x the identity function and K(x) = [0, 0], giving V e = X and
Ke= ([0,0],[0,0],...,[0,0]).
For the decoder we use a similar approach. We consider the identity in the self attention plus the
residual (which can be done by just using the null functions for the self attention). Considering
the external attention, that is the attention over (Ke, V e), we let score and ρ be arbitrary scoring
and normalization functions. And finally for the function O(∙) (Equation (11)) We use a single
layer neural network implementing the affine transformation O([x, y]) = [y - x, -y] such that
O([x,y]) + [x,y] = [y, 0]. The final function F(∙) isjust the identity function.
In order to complete the proof We introduce some notation. Lets denote by #a (w) as the number of
a's in w, and similarly #b(w) for the number of b's in w. Lets call Cw as the value #a(w)-#b(w).
We now prove that, for any string W ∈ {a, b}* if we consider f (w) = X = (xι,..., Xn) as the
input sequence for Trans and We use initial value s = [0, 0] for the decoder, the complete netWork
shall compute a sequence y1 , y2 , . . . , yr such that:
y = [0, 0] i=0
yi=	[cw,0] i>0
We proceed by induction. The base case trivially holds since y0 = s = [0, 0]. Assume now that we
are at step r and the input for the decoder is (y0, y1, . . . , yr). We will show that yr+1 = [cw, 0].
Since we consider the identity in the self attention (Equation (9)), we have that pi = yi for every
i in {0, . . . , i}. Now considering the external attention, that is the attention over (Ke, V e), Since
all key vectors in Ke are [0, 0], the external attention will produce the same score value for all
positions. That is, score(pi, kj1 ) = score(pi, kj2) for every j1, j2. Lets call this value s?. Thus we
have that
ρ(score(pi, k1), . . . , score(pi, kn)) = ρ(s?, s?, . . . , s?)
=(1,1,...,1).
nn n
Then, since V e = X we have that
1n
Att(Pi, Ke, Ve) = - £xe
n '=1
= -[0, #a(W) - #b(W)]
n
for every i ∈ {0, . . . , r}. The last equality holds since our embedding are f(a) = [0, 1] and f(b) =
[0, -1], and so every a in W sums one and every b subtracts one. Thus, we have that
Att(pi, Ke, Ve)	=	[0,cw].
for every i ∈ {0, . . . , r}. In the next step, after the external attention plus the residual connection
(Equation (10)) we have
ai = Att(pi,Ke,Ve)+pi
= Att(pi,Ke,Ve)+yi
= [0, cw] + [cw, 0]
= [cw , cw]
15
Published as a conference paper at ICLR 2019
Applying function O(∙) plus the residual connection (Equation (11)) We have
zi	=	O(ai) + ai
=	O([cw, cw]) + [cw, cw]
=	[cw - cw , -cw ] + [cw , cw ]
=	[cw , 0]
Finally, yr+1 = F(zr) = zr = [cw, 0] Which is exactly What We Wanted to prove.
To complete the proof, notice that #a (w) > #b(w) if and only if cw > 0. If We define F as
Q+ × Q, the recognizer A = (Σ, f, Trans, s, F) Will accept the string w exactly When cw > 0, that
is, w ∈ L(A) if and only if #a(w) > #b(w). That is exactly the language S, and so the proof is
complete.
16
Published as a conference paper at ICLR 2019
B.4	Proof of Theorem 3.4
Let M = (Q, Σ, δ, qinit, F ) be a Turing machine with a infinite tape and assume that the special
symbol # ∈ Σ is used to mark blank positions in the tape. We make the following assumptions
about how M works when processing an input string:
•	M always moves its head either to the left or to the right (it never stays at the same cell).
•	M begins at state qinit pointing to the cell immediately to the left of the input string.
•	M never makes a transition to the left of the initial position.
•	Q has a special state qread used to read the complete input.
•	Initially (time 0), M makes a transition to state qread and move its head to the right.
•	While in state qread it moves to the right until symbol # is read.
•	There are no transitions going out from accepting states (states in F).
It is easy to prove that every general Turing machine is equivalent to one that satisfies the above
assumptions. We prove that one can construct a transformer network TransM that is able to simulate
M on every possible input string.
The construction is somehow involved and uses several helping values, sequences and intermediate
results. To make the reading more easy we divide the construction and proof in three parts. We first
give a high-level view of the strategy we use. Then we give some details on the architecture of the
encoder and decoder needed to implement our strategy, and finally we formally prove that every part
of our architecture can be actually implemented.
B.4	. 1	Overview of the construction and high-level strategy
In the encoder part of TransM we receive as input the string w = s1s2 . . . sn. We first use an
embedding function to represent every si as a one-hot vector and add a positional encoding for every
index. The encoder produces output (Ke , V e) where Ke = (k1e , . . . , ken) and V e = (v1e , . . . , vne)
are sequences of keys and values such that vie contains the information of si and kie contains the
information of the i-th positional encoding. We later show that this allows us to attend to every
specific position and copy every input symbol from the encoder to the decoder (Lemma B.1).
In the decoder part of TransM We simulate a complete execution of M over W = s1s2 …Sn. For
this we define the following sequences (for i ≥ 0):
q(i) : state of M at time i
s(i) : symbol under the head of M at time i
v(i) : symbol Written by M at time i
m(i) : head direction in the transition of M at time i
For the case of m(i) We assume that -1 represents a movement to the left and 1 represents a move-
ment to the right. In our construction We shoW hoW to build a decoder that computes all the above
values for every time step i using self attention plus attention over the encoder part. Since the above
values contain all the needed information to reconstruct the complete history of the computation, We
can effectively simulate M .
In particular our construction produces the sequence of output vectors y1 , y2 , . . . such that, for
every i, the vector yi contains information about q(i) and s(i) encoded as one-hot vectors. The
construction and proof goes by induction. We begin With an initial vector y0 that represents the state
of the computation before it has started, that is q(0) = qinit and s(0) = #. For the induction step
We assume that We have already computed y1 , . . . , yr such that yi contains information about q(i)
and s(i), and We shoW hoW With input (y0, y1, . . . , yr) the decoder produces the next vector yr+1
containing q(r+1) and s(r+1).
The overvieW of the construction is as folloWs. First notice that the transition function δ relates the
above values With the folloWing equation:
δ(q(i), s(i)) = (q(i+1), v(i), m(i)).	(18)
17
Published as a conference paper at ICLR 2019
We prove that we can use a two-layer feed-forward network to mimic the transition function δ
(Lemma B.2). Thus, given that the input vector yi contains q(i) and s(i), we can produce the values
q(i+1), v(i) and m(i) (and store them as values in the decoder). In particular, since yr is in the input,
we can produce q(r+1) which is part of what we need for yr+1. In order to complete the construction
we also need to compute the value s(r+1), that is, we need to compute the symbol under the head of
machine M at the next time step (time r + 1). We next describe at a high level, how this symbol can
be computed with two additional decoder layers.
We first make some observations about s(i) that are fundamental in our computation. Assume that
at time i the head of M is pointing to the cell at index k. Then we have three possibilities:
1.	If i ≤ n, then s(i) = si since M is still reading its input string.
2.	If i > n and M has never written at index k, then s(i) = #, the blank symbol.
3.	In other case, that is, if i > n and time i is not the first time that M is pointing to index k,
then s(i) is the last symbol written by M at index k.
For the case (1) we can produce s(i) by simply attending to position i in the encoder part. Thus, if
r + 1 ≤ n to produce s(r+1) we can just attend to index r + 1 in the encoder and copy this value to
yr+1. For cases (2) and (3) the solution is a bit more complicated, but almost all the important work
is to compute what is the index that M is going to be pointing to in time r + 1.
To formalize this computation, lets denote by c(i) ∈ Z the following value:
c(i) : the index of the cell to which the head of M is pointing to at time i
Notice that value c(i) satisfies that c(i) = c(i-1) + m(i-1). If we unroll this equation and assuming
that c(0) = 0 we obtain that
c(i) = m(0) + m(1) + …+ m(i-1).
Then, at the step i in the decoder we have all the necessary to compute value c(i) but also the
necessary to compute c(i+1). We actually show that the computation (of a representation) of c(i) and
c(i+1) can be done by using one layer of self attention (Lemma B.3).
We still need to define a final notion. With c(i) one can define the helping value `(i) as follows:
`(i) = max{j | j < i and c(j) = c(i)}.
Thus, '(i) is a value such that C(E(Z)) = C(Z), which means that at time i and at time '(i) the head
of M was pointing to the same cell. Moreover, `(i) is the maximum value less than i that satisfies
such condition. That is `(i) is the last time (previous to i) in which M was pointing to position C(Z).
First notice that in every step, M moves its head either to the right or to the left (it never stays in
the same cell). This implies that for every i it holds that C(Z) 6= C(Z-1), from which we obtain that
`(i) < i - 1. Moreover, in the case that C(Z) is visited for the first time at time step i, the value `(i)
is ill-defined. In such a case we let `(i) = i - 1. This makes `(i) ≤ i - 1 for all i, and allows us to
check that C(Z) is visited for the first time at time step i by just checking that `(i) = i - 1.
We now have all the necessary to explain how we compute our desired s(r+1) value. Assume that
r + 1 > n (the case r + 1 ≤ n was already covered before). We first note that if `(r + 1) = r then
s(r+1) = # since this is the first time that cell C(r+1) is visited. On the other hand, if `(r + 1) < r
then s(r+1) is the value written by M at time '(r + 1) which is exactly V('(r+1)). Thus, in this case
we only need to attend to position '(r +1) and copy the value V('(r+1)) to produce s(r+1). We show
that all this can be done with an additional self-attention decoder layer (Lemma B.4).
We have described at a high-level a decoder that, with input (y0, y1, . . . , yr), computes the values
q(r+1) and s(r+1) which is what we need to produce yr+1. We next show all the details of this
construction.
18
Published as a conference paper at ICLR 2019
B.4.2 DETAILS OF THE ARCHITECTURE OF TransM
In this section we give more details on the architecture of the encoder and decoder needed to im-
plement our strategy. We let several intermediate claims as lemmas that we formally prove in Sec-
tion B.4.3.
Attention mechanism
For our attention mechanism we use the following non-linear function:
ψ(x)
x	x ≤ 0,
-x	x > 0.
(19)
We note that 夕(x) = -|x| and it can be implemented as 夕(x) = - relu(x) - relu(-x). We use
夕(∙)to define a scoring function ScoreW : Rd X Rd → R SUch that
ScoreW(u, V)=g(h% Vi) = -l(u，v〉L
Now, let q ∈ Qd, and K = (k1, . . . , kn) and V = (v1, . . . , vn) be tuples of elements in Qd. We
now describe how Att(q, K, V ) is generally computed when hard attention is considered. Assume
first that there exists a single j? ∈ {1, . . . , n} that maximizes ScoreW (q, kj). In that case we have
that Att(q, K, V ) = Vj? with
j? = arg max ScoreW (q, kj)
1≤j≤n
= arg max -|hq, kji|
1≤j≤n
= arg min |hq, kji|	(20)
1≤j≤n
Thus, when computing hard attention with the function ScoreW(∙) We essentially select the vector Vj
such that the dot product hq, kji is as close to 0 as possible. If there is more than one index, say
indexes j1, j2, . . . , jr, that minimizes the dot product hq, kji then we have that
Att(Q, K, V) = r (Vji + vj2 + ∙∙∙ + Vjr).
Thus, in the extreme case in which all dot products are equal hq, kj i for every index j, attention
behaves just as an average of all value vectors, that is Att(q, K, V) = n1 P；=i Vj. We use all these
properties of the hard attention in our proof.
Vectors and encodings
We now describe the vectors that we use in the encoder and decoder parts of TranSM . The vectors
that we use in the TranSM layers are of dimension d = 2∣Q∣ + 4∣∑∣ + 1l.To simplify the exposition,
whenever we use a vector V ∈ Qd, we write it arranged in four groups of values as follows
V =	[ q1, s1,x1,
q2, s2, x2, x3, x4, x5,
s3, x6, s4, x7
x8, x9, x10, x11	]
where Qi ∈ Q|Q|, Si ∈ Qiςi, and Xi ∈ Q. Whenever in a vector of the above form any of the four
groups of values is composed only of 0’s, we just write ‘0, . . . , 0’ where the length of this sequence
is implicit in the length of the corresponding group. Finally, we denote by 0q the vector in Q|Q| that
has only 0's, and similarly 0§ the vector in Qiςi that has only 0's.
For a symbol S ∈ Σ, we use J S K to denote a one-hot vector in Qiςi that represents s. That is, given
an enumeration ∏ : Σ → {1,..., ∣∑∣}, the vector J S K has a 1 in position ∏(s) and a 0 in all other
positions. Similarly, for q ∈ Q, we use J q K to denote a one-hot vector in Q|Q| that represents q.
19
Published as a conference paper at ICLR 2019
Embeddings and positional encodings
We have the necessary to introduce the embedding and positional encoding used in our construction.
We use an embedding function f : Σ → Qd defined as
f(s) = [ 0, . . . , 0,
0, . . . , 0,
JsiK,0,0s,0,
0, . . . , 0	]
Our construction uses the positional encoding pos : N → Qd such that
pos(i) = [ 0, . . . , 0,
0, . . . , 0,
0, . . . , 0,
1, i, 1/i, 1/i2 ]
Thus, given an input sequence s1s2 …Sn ∈ Σ*, We have that
fpos(si) = f(si) + pos(i)	=	[ 0, . . . , 0,
0, . . . , 0,
J si K, 0, 0s, 0,
1, i, 1/i, 1/i2	]
We denote this last vector by Xi. That is, if M receives the input string W = s1s2 •…Sn, then the
input for TransM is the sequence (x1, x2, . . . , xn). The need for using a positional encoding having
values 1/i and 1/i2 Will be clear When We formally prove the correctness of our construction.
We need a final preliminary notion. In the formal construction of TransM We also use the folloWing
helping sequences:
α(i) =	Si	1≤i≤n
Sn i > n
β(i) = i i≤n
n i>n
These are used to identify When M is still reading the input string.
CONSTRUCTION OF TEncM
The encoder part of TransM is very simple. For TEncM We use a single-layer encoder, such that
TEncM(x1, . . . , xn) = (Ke, V e) Where Ke = (k1, . . . , kn) and V e = (v1, . . . , vn) such that
ki = [ 0, . . . , 0,
0, . . . , 0,
0, . . . , 0,
i,-1,0,0	]
vi = [ 0, . . . , 0,
0, . . . , 0,
J Si K, i, 0s, 0,
0, . . . , 0	]
It is straightforWard to see that these vectors can be produced With a single encoder layer by using a
trivial self attention, taking advantage of the residual connections in Equations (6) and (7), and then
using linear transformations for V(∙) and K(∙) in Equation (8).
When constructing the decoder We use the folloWing property.
Lemma B.1. Let q ∈ Qd be a vector such that q = [_,...,_, 1,j, _, _] where j ∈ N and
denotes an arbitrary value. Then we have that
Att(q, Ke, V e) = [ 0, . . . , 0,
0, . . . , 0,
J α(j) K,β(j),0s,0,
0, . . . , 0	]
20
Published as a conference paper at ICLR 2019
CONSTRUCTION OF TDecM
We next show how to construct the decoder part of TransM to produce the sequence of outputs
y1,y2, . . ., where yi is given by:
yi = [ J q(i) K, J s(i) K, m(i-1),
0, . . . , 0,
0, . . . , 0,
0, . . . , 0	]
That is, yi contains information about the state of M at time i, the symbol under the head of M at
time i, and the last direction followed by M (the direction of the head movement at time i - 1). The
need to include m(i-1) will be clear in the construction.
We consider as the starting vector for the decoder the vector
y0 = [ J qinit K, J # K, 0,
0, . . . , 0,
0, . . . , 0,
0, . . . , 0	]
We are assuming that m(-1) = 0 to represent that previous to time 0 there was no head movement.
Our construction resembles a proof by induction; we describe the architecture piece by piece and
at the same time we show how for every r ≥ 0 our architecture constructs yr+1 from the previous
vectors (y0, . . . , yr).
Thus, assume that y0 , . . . , yr satisfy the properties stated above. Since we are using positional
encodings, the actual input for the first layer of the decoder is the sequence
y0 + pos(1), y1 + pos(2), . . . , yr + pos(r + 1).
We denote by yi the vector y% plus its positional encoding. Thus We have that
y = [ J q(i)K,J S⑺"-1),
0, . . . , 0,
0, . . . , 0,
1,(i+1),1/(i+1),1/(i+1)2 ]
For the first self attention in Equation (9) We just produce the identity Which can be easily imple-
mented With a trivial attention plus the residual connection. Thus, We produce the sequence of
vectors (p0, p1,..., Pr) such that p1 = yi.
Since pɪ is of the form [_,...,_, 1,i +1,_, _] by Lemma B.1 we know that if we use pɪ to attend
over the encoder We obtain
Att(pi1,Ke,Ve) = [ 0,...,0,
0, . . . , 0,
J α(i+1) K, β(i+1), 0s, 0,
0, . . . , 0	]
Thus in Equation (10) we finally produce the vector ai1 given by
ai1 = Att(pi1,Ke, Ve) +pi1 = [ J q(i) K, J S(i) K, m(i-1),
0, . . . , 0,
J α(i+1) K, β(i+1), 0s, 0,
1,(i+1),1/(i+1),1/(i+1)2 ]
(21)
As the final piece of the first decoder layer we use a function Oi (∙) (Equation (11)) that satisfies the
following lemma.
Lemma B.2. There exists a two-layer feed-forward network O1 : Qd → Qd such that with input
vector ai1 (21) produces as output
O1(ai1) = [ -J q(i) K, -J S(i) K, -m(i-1),
J q(i+1) K, J v(i) K, m(i), m(i-1), 0, 0
0, . . . , 0,
0, . . . , 0
]
21
Published as a conference paper at ICLR 2019
That is, function Oι(∙) simulates transition δ(q(i), S⑴)to construct J q(i+1) K, J v(i) K, and m(i)
besides some other linear transformations.
We finally produce as the output of the first decoder layer, the sequence (z01, z11, . . . , zr1) such that
zi1 = O1(ai1) + ai1 = [ 0, . . . ,0,
J q(i+1) K, Jv(i) K,m(i),m(i-1),0,0,	(22
J α(i+1) K,β(i+1),0s,0,	(22
1,(i+1),1/(i+1),1/(i+1)2	]
Notice that zr1 already holds info about q(r+1) and m(r) which we need for constructing vector yr+1.
The single piece of information that we still need to construct is s(r+1), that is, the symbol under
the head of machine M at the next time step (time r + 1). We next describe how this symbol can be
computed with two additional decoder layers.
Recall that c(i) is the cell to which M is pointing to at time i, and that it satisfies that c(i) =
m(0) + m(1) +-------+ m(i-1). We can take advantage of this property to prove the following lemma.
Lemma B.3. Let Zi1 = (z01, z11, . . . ,z1). There existsfunctions Q2(∙), K2(∙), and V2(∙) defined by
feed-forward networks such that
Att(Q2(zi1),K2(Zi1),V2(Zi1))	=	[ 0,...,0,
(i+1)	(i)
0q，0s，0,0, (i+i)，(i+i)，	(23)
0, . . . , 0,
0, . . . , 0	]
Lemma B.3 essentially shows that one can construct a representation for values c(i) and c(i+1) for
every possible index i. In particular we will know the value c(r+1) that represents the cell to which
the machine is pointing to in the next time step.
Continuing with the decoder layer, when using the self attention above and after adding the residual
in Equation (9) we obtain the sequence of vectors (p02,p21, . . . , pr2) such that:
pi2 =	Att(Q2(zi1),K2(Zi1),V2(Zi1))+zi1
= [ 0, . . .,0,
J √i+1) K J v(i) K m(i) m(i-1) c(i+1) 上
J q K, J V K,m ,m ，(i+1)，(i+1)，
J α(i+1) K, β(i+1), 0s, 0,
1,(i+1),1/(i+1),1/(i+1)2	]
From vectors (p20, p21, . . . , pr2) and by using the residual connection in Equation (10) plus the output
function O(∙) in Equation (i1) it is not difficult to produce the sequence of vectors (z2, z2,..., Zr)
such that zi2 = pi2, as the output of the second decoder layer. That is, we have that
zi2 = pi2 = [ 0, . . . , 0,
J √i+1) K J v(i) K m(i) m(i-1) c(i+1) 上il
J q	K, J V K,m , m , (i+1)，(i+1)，
Jα(i+1)K,β(i+1),0s,0,
1,(i+1),1/(i+1),1/(i+1)2	]
We now describe how can we use a third and final decoder layer to produce our desired s(r+1) value
(the symbol under the head of M in the next time step). Recall that `(i) is the last time (previous to
i) in which M was pointing to position c(i), or it is i - 1 if this is the first time that M is pointing to
c(i). We can prove the following lemma.
Lemma B.4. There existsfunctions Q3(∙), K3(∙), and V3(∙) defined by feed-forward networks such
that
Att(Q3(zi2),K3(Zi2),V3(Zi2))	=	[ 0,...,0,
0, . . . , 0,
0s, 0, J V('(i+I))K,'(i+1),
0, . . . , 0	]
22
Published as a conference paper at ICLR 2019
We prove Lemma B.4 by just showing that, for every i one can attend exactly to position `(i+ 1) and
then just copy both values. We do this by taking advantage of the values c(i) and c(i+1) previously
computed for every index i. Then we have that pi3 is given by
pi3 =	Att(Q3(zi2),K3(Zi2),V3(Zi2))+zi2
= [ 0, . . . , 0
(i+1)	(i)	(i)	(i	1)	c(i+1)	c(i)	(24)
J q(i+1) K, J v(i)	K,m(i),m(i	I),	Ci+i),	(⅛),	J
J a(i+1) K,β(i+1), J v('(i+I)) K,'(i + 1),
1,(i+1),1/(i+1),1/(i+1)2	]
From vectors (p30, p31, . . . , pr3) and by using the residual connection in Equation (10) plus the output
function O(∙) in Equation (11)) it is not difficult to produce the sequence of vectors (z3, z3,..., z3)
such that zi3 = pi3 , as the output of the third and final decoder layer, and thus we have that
zi3 = pi3 = [ 0, . . . , 0,
J q(i+1) K, J Vqm^mS),编,后,
J a(i+1) K,β(i+1), J v('(i+1)) K,'(i + 1),
1,(i+1),1/(i+1),1/(i+1)2	]
We finish our construction by using the final transformation function F(∙) in Equation (12) in the
following lemma.
Lemma B.5. There exists a function F : Qd → Qd defined by a feed-forward network such that
F(zr3) = [ J q(r+1) K, J s(r+1) K, m(r),
0, . . . , 0,
0, . . . , 0,
0, . . . , 0	]
= yr+1
We prove Lemma B.5 as follows (details in the next section). We show that one can construct a feed-
forward network that with input zr3 implements the following to produce yr+1. We move J q(r+1) K
and m(r) to its corresponding position in yr+1. Then
1.	if β(r+1)	= r +	1	then we let J s(r+1) K = J α(r+1) K,
2.	if β(r+1)	< r +	1	and `(r + 1) = r, then we let J s(r+1) K	=	J # K, and
3.	if β(r+1)	< r +	1	and '(r +1)= r, then We let J s(r+1) K	=	J v('(r+1))』.
Finally, we move J s(r+1) K	to its corresponding position in yr+1 and we make all other positions 0.
The correctness of the above rules is given by the folloWing argument. If β(r+1) = r + 1 then by the
definition of β(r+1) We have that r + 1 ≤ n Which implies that α(r+1) = sr+1 = s(r+1) and thus
rule (1) above is correct. If β(r+1) < r + 1 then We knoW that r + 1 > n and if `(r + 1) = r then
by the definition of '(∙) we know that c(r+1) is visited by M for the first time at time r + 1, which
implies that s(r+1) = # and thus rule (2) is also correct. Finally, If β(r+1) < r+ 1 and `(r+ 1) 6= r,
then we know that c(r+1) has been visited before at time '(r +1), and thus s(r+1) = v('(r+1)) which
implies the correctness of rule (3).
Final step
We now can use our TransM network to construct the recognizer A = (Σ, fpos, TransM, y0 , F)
such that A accepts w if and only if M = (Q, Σ, δ, qinit, F) accepts w. Notice that M accepts w
if and only if an accepting state qf ∈ F is reached at some time step, say t? . By our construction
above we know that, with input fpos (w) our network TransM produces a vector yt? that contains qf
as a one-hot vector. Thus, we can simply use F as the set of all vectors in Qd that contains a one-hot
representation ofa state in F. Formally, F = {J q K | q ∈ F} × Qd-|Q|. It is straightforward to see
that membership in F can be checked in linear time.
23
Published as a conference paper at ICLR 2019
B.4.3 Detailed proofs of intermediate lemmas
Proof of Lemma B.1. Let q ∈ Qd be a vector such that q = [_,...,_, 1, j, _, _] where j ∈ N
and ‘_, is an arbitrary value. We next prove that
Att(q, Ke, V e) = [ 0, . . . , 0,
0, . . . , 0,
J α(j) K,β(j),0s,0,
0, . . . , 0	]
where α(j) and β(j) are defined as
α(j) =	sj 1≤j≤n
sn j > n
β(j)	= j j≤n
n j>n
Recall that Ke = (k1, . . . , kn) is such that ki = [ 0, . . . , 0, i, -1, 0, 0 ]. Then we have that
SCorer (q, ki) = ψ({q, kii) = -Ihq, kiil = Ti - j|.
Notice that, if j ≤ n, then the above expression is maximized when i = j . Otherwise, if j > n then
the expression is maximized when i = n. Then Att(q, Ke, V e) = vi? where i? = j if j ≤ n and
i? = n ifj > n. We note that i? as just defined is exactly β(j). Thus, given that vi is defined as
vi = [ 0, . . . , 0,
0, . . . , 0,
J si K, i, 0s, 0,
0, . . . , 0	]
we obtain that
Att(q, Ke, V e) = vi?	= [ 0, .. .,0,
0, . . . , 0,
J si? K, i?, 0s, 0,
0, . . . , 0	]
= [ 0, .. .,0,
0, . . . , 0,
J α(j) K, β(j), 0s, 0,
0, . . . , 0	]
which is what We wanted to prove.	口
Proof of Lemma B.2. In order to prove the lemma we need some intermediate notions and proper-
ties. Assume that the enumeration ∏ι : Σ → {1,..., ∣Σ∣} is the one used to construct the one-hot
vectors J s K for s ∈ Σ, and that π2 : Q → {1, . . . , |Q|} is the one used to construct J q K with
q ∈ Q. Using π1 and π2 one can construct an enumeration for the pairs in Q × Σ and then con-
struct one-hot vectors for pairs in this set. Formally, given (q, s) ∈ Q × Σ we denote by J (q, s) K
a one-hot vector with a 1 in position (π1(s) - 1)|Q| + π2(q) and a 0 in every other position. To
simplify the notation we use π(q, s) to denote (π1(s) - 1)|Q| + π2(q). One can similarly con-
struct an enumeration π0 for Q × Σ × {-1, 1} such that π0(q, s, m) = π(q, s) if m = -1 and
π0(q, s, m) = ∣Q∣∣Σ∣ + π(q, S) if m = 1. We denote by J (q, s, m) K the corresponding one-hot
vector for every (q, s, m) ∈ Q × Σ × {-1, 1}. We next prove three helping properties. In every case
q ∈ Q, s ∈ Σ, m ∈ {-1,1}, and δ(∙, ∙) is the transition function of machine M.
1.	There	exists	fι	:	Qiqi+iςi → Qiqiiςi such that fι([ J q K, J S K ]) = J (q,s)	K.
2.	There	exists	fδ	:	Qiqiiςi → Q21qiiςi such that fδ(J (q,s) K) = J δ(q, s)	K.
3.	There	exists	f2	:	Q21qiiςi → Qiqi+iςi + 1 such that f2(J (q,s,m) K) = [	J q	K, J S K,m ].
24
Published as a conference paper at ICLR 2019
To show (1), lets denote by Si, with i ∈ {1,..., ∣∑∣},a matrix of dimensions ∣Σ∣ X |Q| such that
Si has its i-th row with 1’s and it is 0 everywhere else. We note that for every s ∈ Σ it holds that
J s KSi = 1 if and only if i = π1(s) and it is 0 otherwise. Now, consider the vector v(q,s)
V(q,s) = [ J q K + J S KSi, J q K + J S 叵，…，J q K + J S KS∣∑∣ ]
We first note that for every i ∈ {1,..., ∣∑∣}, if i = ∏ι(s) then J q K + J S KSi = J q K + 0 = J q K.
Moreover J q K + J S KSπ1(s) = J q K + 1 is a vector that has a 2 exactly at index π2(q), and it is 1
in all other positions. Thus, the vector v(q,s) has a 2 exactly at position (π1(S) - 1)|Q| + π2(q) and
it is either 0 or 1 in every other position. Now, lets denote by o a vector in Qiqiiςi that has a 1 in
every position and consider the following affine transformation
g1([ J qK, J SK ]) = v(q,s) -o.	(25)
Vector g1([ J q K, J S K ]) has a 1 only at position (π1(S) - 1)|Q| + π2 (q) = π(q, S) and it is less
than or equal to 0 in every other position. Thus, to construct fι(∙) We apply the piecewise-linear
sigmoidal activation σ(∙) (see Equation (1)) to obtain
f1([ J qK, J S K ]) = σ(g1([ J qK, J SK ])) = σ(v(q,s) - o) = J (q, S) K,
which is what we wanted.
Now, to show (2), lets denote by Mδ a matrix of dimensions (∣Q∣∣Σ∣) × (2∣Q∣∣Σ∣) constructed as
follows. For (q, S) ∈ Q × Σ, ifδ(q, S) = (p, r, m) then Mδ has a 1 at position (π(q, S), π0 (p, r, m))
and it has a 0 in every other position, that is
Mπδ(q,s),: = J (p, r, m) K = J δ(q, S) K.
It is straightforward to see that J (q, S) KMδ = J δ(q, S) K, and thus we can define f2(∙) as
f2(J(q,S)K)=J(q,S)KMδ=Jδ(q,S)K.
To show (3), consider the matrix A of dimensions (2∣Q∣∣Σ∣) × (|Q| + ∣Σ∣ + 1) such that
Aπ0(q,s,m),: = [ J q K, J S K, m ].
Then we define f3(∙) as
f3(J (q, S,m) K) = J (q, S,m) KA = [ J qK, J S K,m ].
We are now ready to begin with the proof of the lemma. Recall that ai1 is given by
ai1 = [ J q(i) K, J S(i) K, m(i-1),
0, . . . , 0,
J α(i+1) K, β(i+1), 0s, 0,
1,(i+1),1/(i+1),1/(i+1)2 ]
We need to construct a function O1 : Qd → Qd such that
O1(ai1) = [ -J q(i) K, -J S(i) K, -m(i-1),
J q(i+1) K, J v(i) K, m(i), m(i-1), 0, 0
0, . . . , 0,
0, . . . , 0	]
We first use function hι(∙) that works as follows. Lets denote by m(i-1) the value 11 m(i-i) + 11.
Note that m(i-i) is 0 if m(i-i) = —1, it is ɪ if m(i-i) = 0 and it is 1 if m(i-i) = 1. We use this
transformation just to represent m(i-1) with a value between 0 and 1. Now, consider h1(ai1) defined
by
hi(OI) = [ J q(i)K,J S(i)K,m(i-1),gi([J q(i)K,J S(i)K])]
where gι(∙) is the function defined above in Equation (25). It is clear that hι(∙) is an affine transfor-
mation. Moreover, we note that except for gi([J q(i) K, J S(i) K]) all values in hi(aii) are between 0
and 1. Thus if we apply function σ(∙) to h1(a1) we obtain
σ(hι(a1)) = [ J q(i)K,J S(i)K,m (iT),σ(gi([J q(i)K,J S(i) K]))]
=[J q(i) K, J S(i)K,m(i-1), J (q(i),S(i)) K ]
25
Published as a conference paper at ICLR 2019
Then We can define h2(∙) such that
h2 (σ(hι (a1))) = [ J q(i)K,J s(i)K, 2m (T- l,f2(J (q⑴,s⑴)K)]
=	[Jq(i)K,Js(i)K, m(i-1), J δ(q(i), s(i)) K]
=	[Jq(i)K,Js(i)K, m(i-1),J(q(i+1),v(i),m(i))K]
Now we can define h3(∙) as
h3(h2(σ(h1(ai1)))) = [Jq(i)K,Js(i)K, m(i-1), f3(J (q(i+1), v(i), m(i)) K)]
=	[Jq(i)K,Js(i)K, m(i-1), J q(i+1) K, J v(i) K, m(i)]
Finally we can apply a function h4(∙) to just reorder the values and multiply some components by
-1 to complete our construction
O1(ai1) = h4(h3(h2(σ(h1(ai1))))) = [ -J q(i) K, -J s(i) K, -m(i-1),
J q(i+1) K, J v(i) K, m(i), m(i-1), 0, 0
0, . . . , 0,
0, . . . , 0	]
We note that we applied a single non-linearity and all other functions are affine transformations.
Thus Oι(∙) can be implemented with a two-layer feed-forward network.
□
Proof of Lemma B.3. Recall that zi1 is the following vector
zi1 = [ 0, . . . , 0,
J q(i+1) K, J v(i) K, m(i), m(i-1), 0, 0,
J α(i+1) K, β(i+1), 0s, 0,
1,(i+1),1/(i+1),1/(i+1)2	]
We consider Q2 : Qd → Qd and K2 : Qd → Qd as trivial functions that for every input produce
an output vector composed of only 0’s. Moreover, we consider V2 : Qd → Qd such that for every
j∈{0,1,...,i}
V2(zj1)	=	[ 0,...,0,
0q,0s,0,0, m(j), m(j-1),
0, . . . , 0,
0, . . . , 0	]
Then, since K2(zj) is the vector with only zeros, then ScoreW(Q2(zl),K2(zl)) = 0 for every
j ∈ {0, . . . , i}. Thus, we have that the attention Att(Q2(zi1), K2(Zi1), V2(Zi1)) that we need to
compute is just the average of all the vectors in V2(Zi1) = (V2(z01, . . . , zi1), that is
Att(Q2(zl),K2(Zil),V2(Zi1)) =	FIIy Pj=0 V2(z1)
= [ 0,...,0,
0q, 0s,0, 0, (i+1I) Pj=O m(j), (i+1i) Pj=O m(j-1),
0, . . . , 0,
0, . . . , 0	]
Then, since m(O) + …+ m(i) = c(i+1) and m(T) + m(O) + …+ m(i-1) = c(i) we have that
Att(Q2(zi1),K2(Zi1),V2(Zi1)) = [ 0,...,0,
O 0 0 0 c(i+1)	C⑸
0q，0s,0,0, (i+1)，(i+1)，
0, . . . , 0,
0, . . . , 0	]
which is exactly what we wanted to show.	□
Proof of Lemma B.4. Recall that zi2 is the following vector
zi2	= [ 0, . . . , 0,
J √i+i) K J V⑴ K m⑶ m(i-i) c(i+1) 上匕
J q K, J V K,m , m , (i+1) , (i+1),
J α(i+1) K, β(i+1), 0s, 0,
1,(i+1),1/(i+1),1/(i+1)2
]
26
Published as a conference paper at ICLR 2019
We need to construct functions Q3(∙), K3(∙), and V3(∙) such that
Att(Q3(zi2),K3(Zi2),V3(Zi2))	=	[ 0,...,0,
0, . . . , 0,
0s, 0, J V依i+I))K,'(i+1),
0, . . . , 0	]
We first define the query function Q3 : Qd → Qd such that
Q3(zi2) = [ 0, . . . ,0
0, . . . , 0,
0, . . . , 0,
0 c(i+1)	______1_ ]
0, (i+1) , (i+1) , 3(i+1)2 ]
Now, for every j ∈ {0, 1, . . . , i} we define K3 : Qd → Qd and V3 : Qd → Qd such that
K3(zj2) = [ 0, . . . , 0
0, . . . , 0,
0, . . . , 0,
0 _1_ -Cj, _1_ ]
0, (j+1), (j+1), (j+1)2	]
V3 (zj2) = [ 0, . . . , 0,
0, . . . , 0,
0s,0,Jv(j)K,j,
0, . . . , 0	]
It is clear that the three functions are linear transformations and thus they can be defined by feed-
forward networks. Consider now the attention Att(Q3(zi2), K3(Zi2), V3(Zi2)). In order to compute
this value, and since we are considering hard attention, we need to find the value j ∈ {0, 1, . . . , i}
that maximizes
scoreφ(Q3(z2),K3(z2)) = ^(hQ3(z2),K3(z2)i).
Actually, assumming that such value is unique, lets say j?, then we have that
Att(Q3(zi2),K3(Zi2),V3(Zi2)) =V3(zj2?).
We next show that given our definitions above, it always holds that j? = `(i + 1) and then V3(zj2? )
is exactly the vector that we wanted to obtain.
To simplify the notation, we denote by χij the dot product hQ3(zi2), K3(zj2)i. Thus, we need to find
j? = argmaxj 夕(χj). Moreover, given the definition of 夕(See Equation (20))we have that
arg max 夕(Xj) = arg min ∣χj∣.
j∈{0,...,i}	j∈{0,...,i}
Then, it is enough to prove that
arg min ∣χj | = '(i + 1).
j∈{0,...,i}
Now, by our definition of Q3(∙) and K3(∙) we have that
i _	c(i+1)	Cj)	1
Xj =	(i+1)(j + 1) - (i+1)(j + 1) + 3(i+1)2(j + 1)2
=EiEj ∙ k(i+1) - Cj) + εi3j)
where εk =金+ɪɪ. We next prove the following auxiliary property.
Ifj1 is such that C(j1)	6=	C(i+1) and j2 is such that C(j2)	=	C(i+1),	then	|Xij2 |	<	|Xij1 |.	(26)
In order to prove (26), assume first that j1 ∈ {0, . . . , i} is such that C(j1) 6= C(i+1). Then we have
that |C(i+1) - C(j1) | ≥ 1 since C(i+1) and C(j1) are integer values. From this we have two possibilities
forXij1:
27
Published as a conference paper at ICLR 2019
•	If c(i+1) - c(j1) ≤ -1, then
Xji ≤ -2ι + L
Notice that 1 ≥ £无 ≥ ε > 0. Then We have that εiεjι ≥ ①£无)2 > 1 怎£无)2, and thus
IVil〉r p Gεjι )2
lχji | ≥ εiεji---3
Finally, and using again that 1 ≥ εj1 ≥ εi > 0, from the above equation We obtain that
Ixji I ≥ 3 - (εj2 ≥ (εi)2 - (εf ≥ 2(εf-.
•	If c(i+1) - c(ji) ≥ 1, then Xjl ≥ εiεjι + 1 (ε%εj∙J2 and since 1 ≥ £无 ≥ εi > 0 we obtain
that Iχj11 ≥ εiεjι ≥ εiεi ≥ 3(εj2.
Thus, we have that if c(ji) = c(i+1) then ∣χjι ∣ ≥ 3(εj2.
Now assume j2 ∈ {0, . . . , i} is such that c(j2) = c(i+1). In this case we have that
I i I =	(εiεj2)2 =	(Q)2(£j2)2 ≤	(Q)2
Iχj2 I — q - q — q
We showed that if c(ji) = c(i+1) then ∣χjι ∣ ≥ 3(εj2 and if c(j2) = c(i+1) then ∣χj21 ≤ 3(εj2
which implies that IXij I < IXij I. This completes the proof of the property in (26).
We have now all the necessary to prove that arg minj IXij I = `(i + 1). Recall first that `(i + 1) is
defined as
'(i+1)
max{j I j ≤ i and c(j) = c(i+1)}
if there exists j ≤ i s.t. c(j) = c(i+1) ,
in other case.
Assume first that there exists j ≤ i such that c(j) = c(i+1). By (26) we know that
arg min IXij I j∈{0,...,i}	arg min	IXij I j s.t. c(j)=c(i+i) (εiεj)2 arg min j s.t. c(j)=c(i+i)	3 arg min εj j s.t. c(j) =c(i+i) 1 二	arg min 			 j s.t. c(j)=c(i+i) j + 1 max	j j s.t. c(j) =c(i+i) max{j I c(j) = c(i+1)}
On the contrary, assume that for every j ≤ i it holds that c(j) 6= c(i+1). We will prove that in this
case IXij I < IXiiI for every j < i and thus arg minj∈{0,...,i} IXij I = i. Now, since c(j) 6= c(i+1)
for every j ≤ i, then c(i+1) is a cell that has never been visited before by M . Given that M never
makes a transition to the left of its initial cell, then cell c(i+1) is a cell to the right of every other
previously visited cell. This implies that c(i+1) > c(j) for every j ≤ i. Thus, for every j ≤ i we
have c(i+1) - c(j) ≥ 1. This implies that ∣χj∣ = Xj ≥ ε%εj + 3(εiεj-)2. Moreover, notice that if
j < i then εj > εi and thus, ifj < i we have that
Ixj I ≥ *j + (εj > w + (εεZ = ∣χi∣
28
Published as a conference paper at ICLR 2019
which implies that arg min7-∈{0,...,i} ∣χj | = i. Summing it up, We have shown that
ar min | i	| = max{j | c(j)	= c(i+1)}	if there exists	j ≤ i s.t. c(j)	= c(i+1),
j∈{g0,...,i} χj i	in other case.
which is exactly the definition of `(i + 1). This completes the proof of the lemma.
□
Proof of Lemma B.5. Before going to the proof of Lemma-B.5 we prove the following helping
result that allows us to implement a particular type of if statement with a feed-forward network.
Lemma B.6. Let x ∈ {0, 1}m and y, z ∈ {0, 1}n be binary vectors, and let b ∈ {0, 1}. There
exists a two-layer feed-forward network f : Qm+2n+1 → Qm+n such that
f ([x, y, z, b])
[x, y]
[x, z]
ifb=0,
ifb=1.
Proof. Consider the function f1 : Qm+2n+1 → Qm+2n such that
f1([x, y, z, b]) = [x,y - b1,z +b1 - 1]
where 1 is the n-dimensional vector with only ones. Thus, we have that
f1([x,y,z,b])=[x,y,z-1] ifb=0,
[x,y - 1,z] ifb= 1.
Now, since x, y and z are all binary vectors, it is easy to obtain that
[x, y, 0] if b = 0,
σ(f1([x,y,z,b]))= [[xx,,y0,,z0]] iiff bb == 10,.
Finally, consider the function f2 : Qm+2n → Qm+n such that f2 ([x, y, z]) = [x, y + z]. Then we
have that
[x, y] if b = 0,
f2(σ(f1([x, y, z, b]))) = [x,z] ifb=1.
We note that fι(∙) and f2(∙) are affine transformations, and thus f (∙) = f2(σ(f1(∙))) is a two-layer
feed-forward network. This completes our proof.	□
We can now continue with the proof of Lemma B.5. Recall that zr3 is the following vector
zr3 = [ 0, . . . , 0,
J √r + 1) K J v(r) K m(r) m(r - 1)c(r+1) ^rL
J q	K, J V	K,m ,m , (r + 1) , (r+1),
J a(r+1) K,β(r+1), J V依r+1)) K,'(r + 1),
1,(r+1),1/(r+1),1/(r+1)2	]
Lets denote by J m(r) K a vector such that
Jm(r)K=	[[01,,10]]
if m(r) = 1,
if m(r) = -1.
We first consider the function f1(∙) such that
fι(z3) = [ J q(r+1) K, J m(r) K, J a(r+1) K, (r + 1) - β(r+1), J v('(r+I))K, J # K,'(r + 1) - (r - 1)]
It is straightforward that fι(∙) can be implemented as an affine transformation. Just notice that J # K
is a fixed vector, '(r + 1) - (r -1) = '(r + 1) - (r +1)+2 and that J m(r) K = [ mr, -mr ] + [ ɪ, 2 ].
Moreover, all values in f1(zr3) are binary values except for (r + 1) - β(r+1) and `(r + 1) - (r - 1).
Thus, if we apply function σ(∙) to f1(z3) we obtain
σ(fι(z3)) = [ J q(r+1) K, J m(r) K, J a(r+1) K,bι, J v('(r+I))K, J #』也]
29
Published as a conference paper at ICLR 2019
where bi = σ((r + 1) 一 β(r+1)) and b? = σ('(r + 1) 一 (r - 1)). By the definition of β(r+1) We
know that β(r+1) = r + 1 whenever r + 1 ≤ n, and β(r+1) = n if r + 1 > n. Thus we have that
b =	0 if r + 1 ≤ n
1	1 if r + 1 > n
For the case of b2, since `(r + 1) ≤ r we have that b2 = 1 if `(r + 1) = r and it is 0 otherwise, thus
1 if`(r+ 1) = r
0 if`(r+ 1) 6= r
Then, we can use the if function in Lemma B.6 to implement a function f2(∙) such that
3	*3^= ʃ[ J	q(r+1)	K, J m(r)	K,	J α(r+1)	K,bi, J	v('(r+i))	K ]	if b2=0,
f2( (f1( r ")=[[ J q(r+1)K,J m(r)K,J a(r+1)K,bi,J # K ]	if b? = L
We can use again the if function in Lemma B.6 to implement a function f3(∙) such that
f3(f2 (σ(f1(zr3))))
[Jq(r+1)K,Jm(r)K,Jα(r+1)K]
[J q(r+1)K,J m(r)K,J v('(r+1))K]
[Jq(r+1)K,Jm(r)K,Jα(r+1)K]
[Jq(r+1)K,Jm(r)K,J#K]
if b2 = 0 and b1 = 0,
if b2 = 0 and b1 = 1,
if b2 = 1 and b1 = 0,
if b2 = 1 and b1 = 1,
which can be rewritten as
(I J q(r+1)K,J m(r)K,J a(r+1)K]
f3(f2(σ(f1(zr3))))=	[Jq(r+1)K,Jm(r)K,J#K]
[[J q(r+1)K,J m(r)K,J v('(r+1))K]
if r + 1 ≤ n,
if r + 1 > n and `(r + 1) = r,
if r + 1 > n and `(r + 1) 6= r.
From this, it is easy to prove that
f3(f2(σ(f1(zr3)))) = [ J q(r+1) K, J m(r) K, J s(r+1) K].
This can be obtained from the following observation. If r + 1 ≤ n then α(r+1) = sr+1 = s(r+1).
If r + 1 > n and `(r + 1) = r then we know that c(r+1) is visited by M for the first time at time
r + 1 and it is outside the original input, which implies that s(r+1) = #. Finally, Ifr+1 >n
and `(r + 1) 6= r, then we know that c(r+1) has been visited before at time `(r + 1), and thus
s(r+1) = v('(r+1)).
The final piece of the proof is to just convert J m(r) K back to its value m(r), reorder the values and
add 0's to obtain yr+i. We do all this with a final linear transformation f4(∙) such that
f4(f3(f2(σ(fi(zr3)))))	= [	J q(r+i) K, J s(r+i) K, m(r),
0, . . . , 0,
0, . . . , 0,
0, . . . , 0	]
yr+i
which completes our proof.
□
30
Published as a conference paper at ICLR 2019
C	Proofs for Section 4
C.1 Proof of Theorem 4.1
The formulas of the Neural GPU in detail are as follows (with S0 the initial input tensor):
Ut = U(St-1)
Rt = R(St-1)
St = Ut St-1 + (1 -U)F(RtSt-1)
With U(∙), R(∙), and F(∙) defined as
U(X) = fu(KU * X + BU)
R(X) = fR(KR * X + BR)
F(X) = fF (KF *X+ BF)
Consider now an RNN encoder-decoder N of dimension d and composed of the equations
hi = σ(xiW + hi-1V)
gt = σ(gt-1U)
with h0 = 0 and g0 = hn where n is the length of the input.
CONSTRUCTING THE NEURAL GPU TO SIMULATE N
We construct a Neural GPU network NGPU that simulates N as follows. Assume that the input of
N is X = (x1, . . . , xn). Then we first construct the sequence X0 = (x01, . . . , x0n) such that x0i =
[xi, 0, 0, 1, 1, 0] with 0 ∈ Qd the vector with all values as 0. Notice that x0i ∈ Q3d+3, moreover it
is straightforward that if xi was constructed from an embedding function f : Σ → Qd applied to a
symbol a ∈ Σ, then x0i can also be constructed with an embedding function f0 : Σ → Q3d+3 such
thatf0(a) = [f(a),0,0,1,1,0].
We consider an input tensor S ∈ Qn×1×3d+3 such that for every i ∈ {1, . . . , n} it holds that
Si,1,: = x0i = [xi, 0, 0, 1, 1, 0]. Notice that since we picked w = 1, our tensor S is actually a 2D
grid. Our proof shows that a bi-dimensional tensor is enough for simulating an RNN.
We now describe how to construct the kernel banks KU, KR and KF of shape (2, 1, 3d + 3, 3d + 3).
Notice that for each kernel KX we essentially have to define two matrices K1X,1,:,: and K2X,1,:,: each
one of dimension (3d + 3) × (3d + 3). We begin by defining every matrix in KF as block matrices.
When defining the matrices, all blank spaces are considered to be 0.
K1F,1
where F1 and F2 are 3 × 3 matrices defined by
100
F1 =	0 0 0
000
010
F2 =	0 0 0
000
31
Published as a conference paper at ICLR 2019
Tensors KU and KR are considerable simpler. For the case of KU we have
001
000
000
where U1 and U2 are 3 × 3 matrices defined by
110
U1 =	0 0 0
000
and where A is the 3 × d matrix defined by
Finally, we define KR as
K1R,1
where R2 is the 3 × 3 matrix defined by
U2
-	1	1	…1	■
A =	0	0	•…0
0	0	…	0
100
R2 =	0	1 0
000
and where B is the 3 × d matrix defined by
"0 0	…0	"
B =	11	…1
0 0	…	0
The bias tensors BU and BF are 0 (the tensor with all values 0). Finally, to construct tensor BR we
consider the matrix D of dimension 1 × (3d + 3) such that
D=[0 0 1 0 0 1]
Then we let BiR,:,: = D for all i. Finally, we consider fU = fR = fF = σ. The constructed Neural
GPU is a uniform Neural GPU.
Before continuing with the proof we note that for every kernel KX and tensor S we have that
(KX * S)i,ι,: = Si-i,i,：KXi,：,： + Si,ι,:KXι,:,:
Correctness of the construction
We now prove that the following properties hold for every t ≥ 0:
[0, 0, αit-i, 0, 0, 0] for i < t
[hi,hi,0,0, 1,0] fori = t
[[xi, 0, 0,1,1,0] for i>t
(27)
32
Published as a conference paper at ICLR 2019
where αjk is given by the recurrence α0k = hk and αjk = σ (αjk-1 U). Notice that gj = αjn. That
is, we are going to prove that our construction actually simulates N . By (27) one can see that the
intuition in our construction is to use the first d components to simulate the encoder part, the next
d components to communicate data between the encoder and decoder simulation, and the next d
components to simulate the decoder part. The last three components are needed as gadgets for the
gates to actually simulate a sequencial read of the input, and to ensure that the hidden state of the
encoder and decoder are updated properly.
We prove the above statement by induction in t. First notice that the property trivially holds for S0 .
Now assume that this holds for t - 1 and lets prove it for t. We know that Ut is computed as
Ut = σ(KU * StT + BU) = σ(KU * StT)
Thus we have that:
Uit,1,:	= σ((KU * St-1)i,1,:)
= σ(Sit--11,1,:K1U,1,:,:+Sit,-1,1:K2U,1,:,:)
By the induction hypothesis we have
[0, 0, αit-1-i, 0, 0, 0] fori < t- 1
[hi, hi, 0, 0, 1,0]	fori = t - 1
[xi, 0, 0, 1, 1,0]	fori > t - 1
(28)
Now, notice that K1U,1,:,: and K2U,1,:,: are not zero only in its three last rows, thus we can focus on the
three last components of the vectors in St-1, and then we can compute Uit,1,: as
σ
0
t
σ
0

σ
0
σ
1
K
0]
,
0
K
0]
,
0
K
0]
K
0]
U1,U1,U1,U1,
■ L . L . L . L
I— ,
0]
I— ,
0]
I— ,
0]
I— ,
0]
U2,U2,U2,U2,
+
+
1
1
0
,
0
0
1
+
+
1
1
1
1
1,:,:)	for i < t - 1
1 : : )	for i = t - 1
1,:,:)	fori = t
1,:,:)	for i>t
{σ([0, 0, 0, 0, 0, 0])	for i<t — 1
σ([0,0,0,0,0,0])	fori = t - 1
σ([0,0, 1,0,0, 1]) fori = t
σ([1, 1, 1, 1, 1, 1]) fori >t
[0,0,0,0,0,0] fori < t
[0, 0, 1,0,0, 1]	fori = t
[1,1,1,1, 1, 1]	fori > t
Now, for Rt we have
Rt=σ(KR*St-1+BR)
and thus for Rit,1,: we have
Rit,1,:	= σ((KR * St-1)i,1,: + BiR,1,:)
t-1	R	t-1 R	R
= C(Si-1,1,:K1,1,:,: + Si,1,:K2,1,:,: + Bi,1,:)
=σ(St-1KRι,：,: + B":)
=σ(St-,1KRι,:,: + [0, 0,1,0, 0,1])
33
Published as a conference paper at ICLR 2019
where we deleted the term with K1R,1,:,: since it is the null matrix. Then by using the definition of
Sit,-1,1: above (Equation (28)) we have
	I σ([.,.,., 0,0,0]KR1	,：,：+[0,0,1,0,0,1])	for i < t - 1
Rit,1,：	=I σ([-,.,_, 0,1,0]KR1 一∣ σ([.,.,., 1,1,0]KR1 [σ([.,.,., 1,1,0]KR1	,：,：+[0,0,1,0,0,1]) ,：,：+[0,0,1,0,0,1]) ,：,：+[0,0,1,0,0,1])	for i = t - 1 for i = t for i > t
	( σ([0, 0, 1, 0, 0, 1]) .I σ([0,1,1, 0, 1,1]) = I σ([1, 1, 1, 1, 1, 1]) [ σ([1, 1, 1, 1, 1, 1])	for i < t - 1 for i = t - 1 for i = t for i > t	
	[0,0,1,0,0,1] =	[0,1,1,0,1,1] [1,1,1,1,1,1]	for i < t - 1 for i = t - 1 for i ≥ t	
We can now compute Sit,1,:. By the definition of St we have
Si,ι,: = Ut,ι,: © St-,1： + (I"： - Ui,i,：) © σ((KF * (Rt Θ StT))i,i,：)
where we dropped BF that has only zeros. First, by using what we already computed for Uit,1,： we
have that
(σ((KF * (Rt Θ St-1))’」,：)	for i<t
St,1,: =	{ [0, 0,1, 0, 0,1] Θ St-,1：+ [1,1, 0,1,1, 0] Θ σ((KF * (Rt Θ St-1))i,1,:) for i = t
I St-1	for i>t
When i = t we have that Sit,-1,1： = [xi, 0, 0, 1, 0, 0] (Equation (28)), thus [0, 0, 1, 0, 0, 1] Θ Sit,-1,1：
[0, 0, 0, 0, 0, 0]. Then
σ((KF * (Rt Θ St-1))i,1,：)	fori <t
[1, 1, 0, 1, 1, 0] Θ σ((KF * (Rt Θ St-1))i,1,：) fori=t	(29)
[xi, 0, 0, 1, 1, 0]	for i > t
We are almost done with the inductive step, we only need to compute σ((KF * (Rt Θ St-1))i,1,：).
Given what we have for Rt and St-1 we have that Rt Θ St-1 is
[0,0,1,0,0,	1] Θ	[0, 0, αit-1-i, 0, 0, 0]	fori < t - 1
(Rt Θ St-1)i,1,：	=	[0,1,1,0,1,1]Θ	[hi,hi,0,0,1,0]	fori=t- 1
[1,1,1,1,1,1]Θ [xi,0,0,1,1,0]	fori ≥t
[0, 0, αit-1-i, 0, 0,	0]	for	i	<	t	-	1
[0, hi, 0, 0, 1,0]	fori	=	t	-	1
[xi, 0, 0, 1, 1, 0]	for	i	≥	t
34
Published as a conference paper at ICLR 2019
Lets Tt = σ(KF * (Rt Θ StT)). Notice that from Equation (29) We actually need to know the
values in Tit,1,: only for i ≤ t. Now we have that
Tit,1,: = σ((KF * (Rt Θ St-1))i,1,:)
= σ((Rt Θ St-1))i,1,:K1F,1,:,: + (Rt Θ St-1))i,1,:K2F,1,:,:)
f	σ([0, 0, at-1, 0,0,0]KFι,:,: + [0, 0, at-1, 0,0,0]KF,i,：,：)	for i<t	- 1
=	σ([0, 0, αit--1i, 0, 0, 0]K1F,1,:,: + [0, hi, 0, 0, 1, 0]K2F,1,:,:)	fori=t-1
I	σ([0, hi-ι, 0,0,1,0]Kf 1,：,： + [xi, 0,0,1,1,。]咫1,：,：)	for i = t
f σ([0, 0, at-1-iU, 0, 0, 0])	for i<t — 1
= σ([0,0,hiU,0,0,0])	fori = t - 1
[σ([xiW + hi-ιV, XiW + hi-ιV, 0, 0,1, 0]) for i = t
f [0, 0, αti-i, 0, 0, 0]	fori < t - 1
=	[0,0,αi1,0,0,0]	fori = t - 1
[[hi, hi, 0,0,1,0]	for i = t
[0, 0, αit-i, 0, 0, 0]	fori < t
[hi, hi, 0, 0, 1, 0] for i = t
Putting the value of Tit,1,： in Equation (29) we obtain
f	Tit,1,：	for i < t
st,ι,:	=	(	[1,1, 0,1,1,0]	Θ Tt,ι,:	for i = t
([xi, 0, 0,1,1, 0]	for i>t
f	[0, 0, αit-i, 0, 0, 0]	fori	< t
=	[hi,hi,0,0, 1,0]	fori	= t
(	[xi, 0, 0, 1, 1, 0]	for i	> t
which is exactly what we needed to prove (Equation (27)).
Now, lets focus on Snn,+1t,： for t ≥ 1. By what we have just proved, we obtain that
Snn,+1t,： = [0,0,αtn,0,0,0] = [0,0,gt,0,0,0]
which is the decoder part of the RNN N . Thus, we can simulate the complete network N with a
Neural GPU.
C.2 Proof of Proposition 4.2
We first prove the following claim: Assume that S ∈ Qh×w×d is a tensor that satisfies the following
property: there exists a p ≥ 1 that divides h and such that, for every i ∈ {1, 2, . . . , h - p} it holds
that
Si,：,： = Si+p,：,：.
Given that we will be considering circular convolutions, then we have that for every ` ≤ 0 we
have that S',j,： = Sh+',j,: and for every ' > h we have that S',j,： = Sh-',j,：. With this we have
that for every i ∈ N it holds that Si,：,： = Si+p,：,： that is, we do not need to restrict to values in
i ∈ {1, 2, . . . , h - p}.
Now lets K be an arbitrary kernel bank of shape (kH, kW , d, d). Let T = K ~ S where ~ denotes
the circular convolution. We prove next that
Ti,：,： = Ti+p,：,：
for every i. This is a simple fact that follows from the way in which the convolution is defined. We
use the notation in the body of the paper for T = K ~ S, that is, we denote by sij the vector Si,j,：
and Kij the matrix Ki,j,：,：. Notice that sij = si+p,j for every i ∈ N. Now the circular convolution
is
kH kW
Ti,j,： = (K ~ S)i,j,： =	si+∆1 (u),j+∆2 (v) Kuv
u=1 v=1
35
Published as a conference paper at ICLR 2019
where ∆1(u) = u - bkH /2c - 1 and ∆2 (v) = v - bkW /2c - 1. Then, given that sij = si+p,j for
every i ∈ N we have that
Ti,j,: = (K ~ S)i,j,:
kH kW
ΣΣ si+∆1 (u)+p,j+∆2 (v) Kuv = (K ~ S)i+p,j,: = Ti+p,j,:
u=1 v=1
and then, Ti,:,: = Ti+p,:,:.
Consider now an arbitrary uniform Neural GPU that processes tensor S above, and assume that
S1 , S2 , . . . , Sr is the sequence produced by it. Next we prove that for every t and for every i it
holds that Sit,:,: = Sit+p,:,:. We prove it by induction in t. For the case S0 it holds by definition.
Thus assume that St-1 satisfies the property. Let
Ut = fu(KU * St-1 + BU)
Rt = ∕r(Kr * St-1 + BR)
St = Ut	St-1 + (1 - U)	fF(KF * (Rt	St-1) + BF)
Since we are considering uniform Neural GPUs, we know that there exist three matrices BU, BR
and BF such that for every i it holds that BiU,:,: = BU, BiR,:,: = BR, and BiF,:,: = BF . It is easy
to prove that Uit,:,: = Uit+p,:,:. First note that by inductive hypothesis, we have that Sit,-:,1: = Sit+-p1,:,:
and thus by the property proved above we have that (KU * St-1)i,:,: = (KU * St-1)i+p,:,:. Thus we
have that
Uit,:,: = fU((KU * St-1)i,:,: + BU) = fU((KU * St-1)i+p,:,: + BU) = Uit+p,:,:
With a similar argument we can prove that Rit,:,: = Rit+p,:,:. Moreover, notice that (Rt St-1)i,:,: =
(Rt	St-1)i+p,:,:,	and thus	(KF	*	(Rt	St-1))i,:,: =	(KF *	(Rt	St-1))i+p,:,:. With all this we
finally we have that
,:,:	Sit,-:,1: + (1i,:,: - Ui,:,:)	fF((KF * (Rt	St-1))i,:,: + BF)
+p,:,:	Sit+-p1,:,: + (1i+p,:,: - Ui+p,:,:)	fF ((KF * (Rt	St-1))i+p,:,: + BF)
ti,
S
tititi
UUS
===
p,:,:
This completes the first part of the proof.
We have shown that if the input of a uniform neural GPU is periodic, then the output is also periodic.
We make a final observation. Let N be a uniform Neural GPU, and S ∈ Qkp×w×d be a tensor such
that Si,:,: = Si+p,:,: for every i. Moreover, let T ∈ Qk0p×w×d be a tensor such that Ti,:,: = Ti+p,:,:
for every i, and assume that S1:p,:,: = T1:p,:,:. Lets S1, S2, . . . and T1, T2, . . . be the sequences
produced by N . Then with a similar argument as above it is easy to prove that for every t it holds
that St1:p,:,: = Tt1:p,:,:.
From this it is easy to prove that uniform Neural GPUs will no be able to recognize the length of
periodic inputs. Thus assume that there is a language recognizer A defined by of a uniform neural
GPU N such that L(A) contains all strings of even length. Assume that u is an arbitrary string in Σ
such that |u| = p with p an odd number, and let w = uu and w0 = uuu. Notice that |w| = 2p and
thus w ∈ L(A), but |w0| = 3p and thus w0 6∈ L(A).
Let f : Σ → Qd and let X = f(w) = (x1, x2, . . . , x2p) and X0 = f (w0) = (x01, x02, . . . , x03p).
Consider now the tensor S ∈ Q2p×w×d such that Si,1,: = xi for i ∈ {1, . . . , 2p}, thus Si,:,: =
Si+p,:,:. Similarly, consider T ∈ Q3p×w×d such that such that Ti,1,: = x0i for i ∈ {1, . . . , 3p},
and thus Ti,:,: = Ti+p,:,:. Notice that S1:p,:,: = T1:p,:,: then by the property above we have that
for every t it holds that St1:p,:,: = Tt1:p,:,:. In particular, we have Stp,:,: = Ttp,:,:. We also know
that Stp,:,: = St2p,:,: and that Ttp,:,: = Tt2p,:,: = Tt3p,:,:. Thus we have that for every t it holds that
St2p,1,: = Tt3p,1,:. From this we conclude that the outputs of N for both inputs X and X0 are the
same, and thus if A accepts w then A accepts w0 which is a contradiction.
36