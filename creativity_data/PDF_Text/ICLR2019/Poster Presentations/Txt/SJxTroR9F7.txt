Published as a conference paper at ICLR 2019
Supervised Policy Update for Deep Reinforce-
ment Learning
Quan Vuong
University of California, San Diego
qvuong@ucsd.edu
Yiming Zhang
New York University
yiming.zhang@cs.nyu.edu
Keith Ross
New York University/New York University Shanghai
keithwross@nyu.edu
Ab stract
We propose a new sample-efficient methodology, called Supervised Policy Update
(SPU), for deep reinforcement learning. Starting with data generated by the cur-
rent policy, SPU formulates and solves a constrained optimization problem in the
non-parameterized proximal policy space. Using supervised regression, it then
converts the optimal non-parameterized policy to a parameterized policy, from
which it draws new samples. The methodology is general in that it applies to both
discrete and continuous action spaces, and can handle a wide variety of proximity
constraints for the non-parameterized optimization problem. We show how the
Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) prob-
lems, and the Proximal Policy Optimization (PPO) problem can be addressed by
this methodology. The SPU implementation is much simpler than TRPO. In terms
of sample efficiency, our extensive experiments show SPU outperforms TRPO in
Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.
1	Introduction
The policy gradient problem in deep reinforcement learning (DRL) can be defined as seeking a
parameterized policy with high expected reward. An issue with policy gradient methods is poor
sample efficiency (Kakade, 2003; Schulman et al., 2015a; Wang et al., 2016b; Wu et al., 2017;
Schulman et al., 2017). In algorithms such as REINFORCE (Williams, 1992), new samples are
needed for every gradient step. When generating samples is expensive (such as robotic environments),
sample efficiency is of central concern. The sample efficiency of an algorithm is defined to be the
number of calls to the environment required to attain a specified performance level (Kakade, 2003).
Thus, given the current policy and a fixed number of trajectories (samples) generated, the goal of the
sample efficiency problem is to construct a new policy with the highest performance improvement
possible. To do so, it is desirable to limit the search to policies that are close to the original policy πθk
(Kakade, 2002; Schulman et al., 2015a; Wu et al., 2017; Achiam et al., 2017; Schulman et al., 2017;
Tangkaratt et al., 2018). Intuitively, if the candidate new policy πθ is far from the original policy πθk,
it may not perform better than the original policy because too much emphasis is being placed on the
relatively small batch of new data generated by πθk , and not enough emphasis is being placed on the
relatively large amount of data and effort previously used to construct πθk .
This guideline of limiting the search to nearby policies seems reasonable in principle, but requires a
distance η(πθ, πθk) between the current policy πθk and the candidate new policy πθ, and then attempt
to solve the constrained optimization problem:
maximize J(∏θ | ∏θ , new data)	(1)
θ
subject to η(πθ, πθk) ≤ δ	(2)
where J(∏θ | ∏θk, new data) is an estimate of J(∏θ ), the performance of policy ∏, based on the
previous policy πθk and the batch of fresh data generated by πθk . The objective (1) attempts to
1
Published as a conference paper at ICLR 2019
maximize the performance of the updated policy, and the constraint (2) ensures that the updated policy
is not too far from the policy πθk that was used to generate the data. Several recent papers (Kakade,
2002; Schulman et al., 2015a; 2017; Tangkaratt et al., 2018) belong to the framework (1)-(2).
Our work also strikes the right balance between performance and simplicity. The implementation
is only slightly more involved than PPO (Schulman et al., 2017). Simplicity in RL algorithms has
its own merits. This is especially useful when RL algorithms are used to solve problems outside of
traditional RL testbeds, which is becoming a trend (Zoph & Le, 2016; Mingxing Tan, 2018).
We propose a new methodology, called Supervised Policy Update (SPU), for this sample efficiency
problem. The methodology is general in that it applies to both discrete and continuous action spaces,
and can address a wide variety of constraint types for (2). Starting with data generated by the
current policy, SPU optimizes over a proximal policy space to find an optimal non-parameterized
policy. It then solves a supervised regression problem to convert the non-parameterized policy to
a parameterized policy, from which it draws new samples. We develop a general methodology for
finding an optimal policy in the non-parameterized policy space, and then illustrate the methodology
for three different definitions of proximity. We also show how the Natural Policy Gradient and Trust
Region Policy Optimization (NPG/TRPO) problems and the Proximal Policy Optimization (PPO)
problem can be addressed by this methodology. While SPU is substantially simpler than NPG/TRPO
in terms of mathematics and implementation, our extensive experiments show that SPU is more
sample efficient than TRPO in Mujoco simulated robotic tasks and PPO in Atari video game tasks.
Off-policy RL algorithms generally achieve better sample efficiency than on-policy algorithms
(Haarnoja et al., 2018). However, the performance of an on-policy algorithm can usually be substan-
tially improved by incorporating off-policy training (Mnih et al. (2015), Wang et al. (2016a)). Our
paper focuses on igniting interests in separating finding the optimal policy into a two-step process:
finding the optimal non-parameterized policy, and then parameterizing this optimal policy. We also
wanted to deeply understand the on-policy case before adding off-policy training. We thus compare
with algorithms operating under the same algorithmic constraints, one of which is being on-policy.
We leave the extension to off-policy to future work. We do not claim state-of-the-art results.
2	Preliminaries
We consider a Markov Decision Process (MDP) with state space S , action space A, and reward
function r(s, a), S ∈ S, a ∈ A. Let π = {π(a∣s) : S ∈ S,a ∈ A} denote a policy, let Π be the set of
all policies, and let the expected discounted reward be:
∞
J(π) , E X γtr(St, at)	(3)
T 〜∏	/ “
t=0
where γ ∈ (0, 1) is a discount factor and τ = (S0, a0, S1, . . . ) is a sample trajectory. Let Aπ(S, a)
be the advantage function for policy π (Levine, 2017). Deep reinforcement learning considers a set
of parameterized policies ∏dl = {∏θ ∣θ ∈ Θ} ⊂ Π, where each policy is parameterized by a neural
network called the policy network. In this paper, we will consider optimizing over the parameterized
policies in ΠDL as well as over the non-parameterized policies in Π. For concreteness, we assume
that the state and action spaces are finite. However, our methodology also applies to continuous state
and action spaces, as shown in the Appendix.
One popular approach to maximizing J(πθ) over ΠDL is to apply stochastic gradient ascent. The
gradient of J(πθ) evaluated at a specific θ = θk can be shown to be (Williams, 1992):
∞
Vθ J(∏θk) = E	X YtVθ log∏θk (at∣st)Aπθk(st,at) .	(4)
τ~πθk k	J
We can approximate (4) by sampling N trajectories of length T from πθk :
1 N T-1
Vθ J (∏θk) ≈ N XX Vθ log ∏θk (ait∣Sit)Aπθk(st,at)，gk	(5)
Additionally, define dπ (S) , (1 - γ) Pt∞=0 γtPπ(St = S) for the the future state probability
distribution for policy ∏, and denote ∏(∙∣s) for the probability distribution over the action space A
2
Published as a conference paper at ICLR 2019
when in state s and using policy π. Further denote DKL(π k πθk)[s] for the KL divergence from
∏(∙∣s) to ∏θk (∙∣s), and denote the following as the “aggregated KL divergence”.
DKL(n k πθk) =	E∏ [DKL(π k πθk)[s]]	⑹
S 〜dπθk
2.1	Surrogate Objectives for the Sample Efficiency Problem
For the sample efficiency problem, the objective J(πθ) is typically approximated using samples
generated from πθk (Schulman et al., 2015a; Achiam et al., 2017; Schulman et al., 2017). Two
different approaches are typically used to approximate J(πθ) - J (πθk). We can make a first order
approximation of J(πθ) around θk (Kakade, 2002; Peters & Schaal, 2008a;b; Schulman et al., 2015a):
J(∏θ) - J(∏θk) ≈ (θ - θk)TVθ J(∏θk) ≈ (θ - θk)Tgk	(7)
where gk is the sample estimate (5). The second approach is to approximate the state distribution
dπ(s) with dπθk (s) (Achiam et al., 2017; Schulman et al., 2017; Achiam, 2017):
J (∏)- J (∏θk) ≈L∏θfc (∏)，ɪ %	E-J zvψτ Aπθk G。)]	⑻
k	1 - Y S〜dπθk a〜∏θk (∙∣s) L∏θk (a|s)	J
There is a well-known bound for the approximation (8) (Kakade & Langford, 2002; Achiam et al.,
2017). Furthermore, the approximation Lπθ (πθ) matches J(πθ) - J (πθk ) to the first order with
respect to the parameter θ (Achiam et al., 2017).
3	Related Work
Natural gradient (Amari, 1998) was first introduced to policy gradient by Kakade (Kakade, 2002) and
then in (Peters & Schaal, 2008a;b; Achiam, 2017; Schulman et al., 2015a). referred to collectively
here as NPG/TRPO. Algorithmically, NPG/TRPO finds the gradient update by solving the sample
efficiency problem (1)-(2) with η(∏θ, ∏θk) = Dkl(∏θ ∣∣ ∏θk), i.e., use the aggregate KL-divergence
for the policy proximity constraint (2). NPG/TRPO addresses this problem in the parameter space
θ ∈ Θ. First, it approximates J(∏θ) with the first-order approximation (7) and Dkl(∏θ ∣∣ ∏θk) using
a similar second-order method. Second, it uses samples from πθk to form estimates of these two
approximations. Third, using these estimates (which are functions of θ), it solves for the optimal
θ*. The optimal θ* is a function of gk and of hk, the sample average of the Hessian evaluated at
θk. TRPO also limits the magnitude of the update to ensure Dkl(∏θ k ∏θk) ≤ δ (i.e., ensuring the
sampled estimate of the aggregated KL constraint is met without the second-order approximation).
SPU takes a very different approach by first (i) posing and solving the optimization problem in
the non-parameterized policy space, and then (ii) solving a supervised regression problem to find a
parameterized policy that is near the optimal non-parameterized policy. A recent paper, Guided Actor
Critic (GAC), independently proposed a similar decomposition (Tangkaratt et al., 2018). However,
GAC is much more restricted in that it considers only one specific constraint criterion (aggregated
reverse-KL divergence) and applies only to continuous action spaces. Furthermore, GAC incurs
significantly higher computational complexity, e.g. at every update, it minimizes the dual function to
obtain the dual variables using SLSQP. MPO also independently propose a similar decomposition
(Abbas Abdolmaleki, 2018). MPO uses much more complex machinery, namely, Expectation
Maximization to address the DRL problem. However, MPO has only demonstrates preliminary
results on problems with discrete actions whereas our approach naturally applies to problems with
either discrete or continuous actions. In both GAC and MPO, working in the non-parameterized space
is a by-product of applying the main ideas in those papers to DRL. Our paper demonstrates that the
decomposition alone is a general and useful technique for solving constrained policy optimization.
Clipped-PPO (Schulman et al., 2017) takes a very different approach to TRPO. At each iteration,
PPO makes many gradient steps while only using the data from πθk . Without the clipping, PPO
is the approximation (8). The clipping is analogous to the constraint (2) in that it has the goal of
keeping ∏ close to ∏θk. Indeed, the clipping keeps ∏θ(a |st) from becoming neither much larger
than (1 + )πθk (at|st) nor much smaller than (1 - )πθk (at|st). Thus, although the clipped PPO
objective does not squarely fit into the optimization framework (1)-(2), it is quite similar in spirit. We
note that the PPO paper considers adding the KL penalty to the objective function, whose gradient
3
Published as a conference paper at ICLR 2019
is similar to ours. However, this form of gradient was demonstrated to be inferior to Clipped-PPO.
To the best of our knowledge, it is only until our work that such form of gradient is demonstrated to
outperform Clipped-PPO.
Actor-Critic using Kronecker-Factored Trust Region (ACKTR) (Wu et al., 2017) proposed using
Kronecker-factored approximation curvature (K-FAC) to update both the policy gradient and critic
terms, giving a more computationally efficient method of calculating the natural gradients. ACER
(Wang et al., 2016a) exploits past episodes, linearizes the KL divergence constraint, and maintains an
average policy network to enforce the KL divergence constraint. In future work, it would of interest
to extend the SPU methodology to handle past episodes. In contrast to bounding the KL divergence
on the action distribution as we have done in this work, Relative Entropy Policy Search considers
bounding the joint distribution of state and action and was only demonstrated to work for small
problems (Jan Peters, 2010).
4	SPU Framework
The SPU methodology has two steps. In the first step, for a given constraint criterion η(π, πθk) ≤ δ,
we find the optimal solution to the non-parameterized problem:
maximize Lπθ (π)	(9)
π∈Π	k
subject to η(π, πθk) ≤ δ	(10)
Note that π is not restricted to the set of parameterized policies ΠDL . As commonly done, we
approximate the objective function (8). However, unlike PPO/TRPO, we are not approximating
the constraint (2). We will show below the optimal solution ∏* for the non-parameterized problem
(9)-(10) can be determined nearly in closed form for many natural constraint criteria η(π, πθk) ≤ δ.
In the second step, we attempt to find a policy πθ in the parameterized space ΠDL that is close to the
target policy ∏*. Concretely, to advance from θk to θk+ι, we perform the following steps:
(i)	We first sample N trajectories using policy πθk, giving sample data (si, ai, Ai), i = 1, .., m.
Here Ai is an estimate of the advantage value Aπθk (si , ai). (For simplicity, we index the
samples with i rather than with (i, t) corresponding to the tth sample in the ith trajectory.)
(ii)	For each s” We define the target distribution ∏* to be the optimal solution to the constrained
optimization problem (9)-(10) for a specific constraint η .
(iii)	We then fit the policy network ∏ to the target distributions π*(∙∣Si), i = 1,.., m. Specifically,
to find θk+1, we minimize the following supervised loss function:
m
L(θ) = N X DKL(πθ Il π*)[si]	(II)
For this step, we initialize with the weights for πθk . We minimize the loss function L(θ) with
stochastic gradient descent methods. The resulting θ becomes our θk+1.
5	SPU Applied to Specific Proximity Criteria
To illustrate the SPU methodology, for three different but natural types of proximity constraints, we
solve the corresponding non-parameterized optimization problem and derive the resulting gradient
for the SPU supervised learning problem. We also demonstrate that different constraints lead to very
different but intuitive forms of the gradient update.
4
Published as a conference paper at ICLR 2019
5.1	Forward Aggregate and Disaggregate KL Constraints
We first consider constraint criteria of the form:
maximize π∈Π	dπθk (S)	E	[Aπθk (S, a)]	(12)
subject to	X dπθk (S)DKL (n k πθk )[s] ≤ δ	(13) s Dkl (∏ k ∏θk )[s] ≤ E for all s	(14)
Note that this problem is equivalent to minimizing Lπθ (π) subject to the constraints (13) and (14).
We refer to (13) as the "aggregated KL constraint" and to (14) as the "disaggregated KL constraint".
These two constraints taken together restrict π from deviating too much from πθk. We shall refer to
(12)-(14) as the forward-KL non-parameterized optimization problem.
Note that this problem without the disaggregated constraints is analogous to the TRPO problem.
The TRPO paper actually prefers enforcing the disaggregated constraint to enforcing the aggregated
constraints. However, for mathematical conveniences, they worked with the aggregated constraints:
"While it is motivated by the theory, this problem is impractical to solve due to the large number
of constraints. Instead, we can use a heuristic approximation which considers the average KL
divergence" (Schulman et al., 2015a). The SPU framework allows us to solve the optimization
problem with the disaggregated constraints exactly. Experimentally, we compared against TRPO in
a controlled experimental setting, e.g. using the same advantage estimation scheme, etc. Since we
clearly outperform TRPO, we argue that SPU’s two-process procedure has significant potentials.
For each λ > 0, define: πλ(a∣s)
πθk ⑷S)
Zλ(s)
eA θk (S,a"λ where Zλ(s) is
the normalization term.
Note that πλ(a∣s) is a function of λ. Further, for each s, let λs be such that Dkl(∏'s k ∏θk)[s] = e.
Also let Γλ = {S : DKL(πλ k πθk )[S] ≤ }.
Theorem 1 The optimal solution to the problem (12)-(14) is given by:
~λ∕ I、_ ∫π'(IaIS)	S ∈ rλ	∕1<∖
π (a|S) = < λ / I、	J, λλ	(15)
πλs (a|S)	S ∈/ Γλ
where λ is Chosen so that Es dπθk(s)DKL(∏λ k ∏θ% )[s] = δ (Proof in subsection A.1).
Equation (15) provides the structure of the optimal non-parameterized policy. As part of the SPU
framework, We then seek a parameterized policy ∏θ that is close to ∏λ(a∣s), that is, minimizes the
loss function (11). For each sampled state Si, a straightforward calculation shows (Appendix B):
VθDkl(∏θ k ∏λ)[si] = V©Dkl(∏θ k ∏θ%)[si] - ɪ E	[V©log∏(a∣Si)Aπθk(si,a)]
λsi a~πθk(Tsi)
(16)
〜	〜
where λ% = λ for Si ∈ Γλ and λsi = λ% for Si / Γλ. We estimate the expectation in (16) with the
sampled action ai and approximate Aπθk (Si, ai) as Ai (obtained from the critic network), giving:
VθDkl(∏θ k ∏λ)[Si] ≈ V©Dkl(∏θ k ∏θ%)同-占NV(?Si) Ai	(17)
λ	πθk (ai| Si)
si
To simplify the algorithm, we slightly modify (17). We replace the hyper-parameter δ with the
hyper-parameter λ and tune λ rather than δ. Further, we set λsi = λ for all Si in (17) and introduce
per-state acceptance to enforce the disaggregated constraints, giving the approximate gradient:
vθDKL(πθ k π') ≈ — X[V©DKL(πθ k πθk)[Si] 一 ɪ Vθ∏θ(ai∣Si)A/1°(^忖@)包]々 (18)
m i=1	λ πθk (ai∣Si)	、'' k
We make the approximation that the disaggregated constraints are only enforced on the states
in the sampled trajectories. We use (18) as our gradient for supervised training of the policy
5
Published as a conference paper at ICLR 2019
network. The equation (18) has an intuitive interpretation: the gradient represents a trade-off
1 t 1	. t f-	F / t a 1	1 rθπθ(aiIsi)Z1、 a , F
between the approximate performance of ∏θ (as captured by ʌ------------( .∣ ,)Ai) and how far ∏
diverges from πθk (as captured by RθDKL(πθ k πθk)[si]). For the stopping criterion, we train until
m Pi Dkl(∏θ k ∏θk)[si] ≈ δ.
5.2	Backward KL Constraint
In a similar manner, we can derive the structure of the optimal policy when using the reverse
KL-divergence as the constraint. For simplicity, we provide the result for when there are only
disaggregated constraints. We seek to find the non-parameterized optimal policy by solving:
maximize	dπθk (s)	E	[Aπθk (s, a)]
∏∈∏	/ J	a〜π(∙∣s)
DKL(π k πθk)[s] ≤ for all s
(19)
(20)
Theorem 2 The optimal solution to the problem (19)-(20) is given by:
∏*(a∣s) = ∏θk (a|s)
λ(s)
λ0(s) - Aπθk (s, a)
(21)
where λ(s) > 0 and λ0(s) > maxa Aπθk (s, a) (Proof in subsection A.2).
Note that the structure of the optimal policy with the backward KL constraint is quite different from
that with the forward KL constraint. A straight forward calculation shows (Appendix B):
VθDkl(∏θ k ∏*)[s] = RθDkl(∏θ k ∏θk)[s] - E
a 〜πθk
vθπθ ⑷S) log (______1________]
[∏θk(a∣s)	g lλ0(s)- Aπθk(s,a)力
(22)
The equation (22) has an intuitive interpretation. It increases the probability of action a if
Aπθk (s, a) > λ0(s) - 1 and decreases the probability of action a if Aπθk (s, a) < λ0(s) - 1. (22)
also tries to keep πθ close to πθk by minimizing their KL divergence.
5.3	L∞ CONSTRAINT
In this section we show how a PPO-like objective can be formulated in the context of SPU. Recall
from Section 3 that the the clipping in PPO can be seen as an attempt at keeping πθ(aiIsi) from
becoming neither much larger than ( + )πθk (aiIsi) nor much smaller than (1 - )πθk (aiIsi) for
i =, . . . , m. In this subsection, we consider the constraint function
η(π, πθk)
max |n(ai|si) - πθk(。小，)|
i=1,...,m	πθk (ai Isi)
(23)
which leads us to the following optimization problem:
maximize	X A^ ⑻问)^⅛1
∏(aι∣sι),…,∏(am∣Sm)	i=1	∏k(ai∣si)
π(aiIsi) -πθk(aiIsi)
subject to ---------------k------ ≤ e i = 1,..., m
Xm（ n（ai|si）- πθk ①小，））2 ≤ δ
πθk（ai|si））一
(24)
(25)
(26)
Note that here we are using a variation of the SPU methodology described in Section 4 since here
we first create estimates of the expectations in the objective and constraints and then solve the
optimization problem (rather than first solve the optimization problem and then take samples as done
for Theorems 1 and 2). Note that we have also included an aggregated constraint (26) in addition to
the PPO-like constraint (25), which further ensures that the updated policy is close to πθk .
6
Published as a conference paper at ICLR 2019
Theorem 3 The optimal solution to the optimization problem (24-26) is given by:
∏*(a∕si)
[∏θk (a∕si)min{1 + λAi, 1 + e}
l∏θk (ai∣si) max{1 + λAi, 1 - e}
Ai ≥ 0
Ai < 0
(27)
for some λ > 0 where Ai , Aπθk (si, ai) (Proof in subsection A.3).
To simplify the algorithm, We treat λ as a hyper-parameter rather than δ. After solving for ∏*, We seek
a parameterized policy ∏θ that is close to ∏* by minimizing their mean square error over sampled
states and actions, i.e. by updating θ in the negative direction of Vθ Pi(∏θ(a∕si) - n*(a/si))2.
This loss is used for supervised training instead of the KL because We take estimates before forming
the optimization problem. Thus, the optimal values for the decision variables do not completely
characterize a distribution. We refer to this approach as SPU With the L∞ constraint.
Although We consider three classes of proximity constraint, there may be yet another class that
leads to even better performance. The methodology alloWs researchers to explore other proximity
constraints in the future.
6 Experimental Results
Extensive experimental results demonstrate SPU outperforms recent state-of-the-art methods for
environments With continuous or discrete action spaces. We provide ablation studies to shoW the
importance of the different algorithmic components, and a sensitivity analysis to shoW that SPU’s
performance is relatively insensitive to hyper-parameter choices. There are tWo definitions We use to
conclude A is more sample efficient than B: (i) A takes feWer environment interactions to achieve
a pre-defined performance threshold (Kakade, 2003); (ii) the averaged final performance of A is
higher than that of B given the same number environment interactions (Schulman et al., 2017).
Implementation details are provided in Appendix D.
6.1	Results on Mujoco
The Mujoco (Todorov et al., 2012) simulated robotics environments provided by OpenAI gym
(Brockman et al., 2016) have become a popular benchmark for control problems With continuous
action spaces. In terms of final performance averaged over all available ten Mujoco environments
and ten different seeds in each, SPU With L∞ constraint (Section 5.3) and SPU With forWard KL
constraints (Section 5.1) outperform TRPO by 6% and 27% respectively. Since the forWard-KL
approach is our best performing approach, We focus subsequent analysis on it and hereafter refer to
it as SPU. SPU also outperforms PPO by 17%. Figure 1 illustrates the performance of SPU versus
TRPO, PPO.
To ensure that SPU is not only better than TRPO in terms of performance gain early during training,
We further retrain both policies for 3 million timesteps. Again here, SPU outperforms TRPO by 28%.
Figure 3 in the Appendix illustrates the performance for each environment. Code for the Mujoco
experiments is at https://github.com/quanvuong/Supervised_Policy_Update.
6.2	Ablation Studies for Mujoco
The indicator variable in (18) enforces the disaggregated constraint. We refer to it as per-state
acceptance. Removing this component is equivalent to removing the indicator variable. We refer
to using Pi DKL(πθ k πθk)[si] to determine the number of training epochs as dynamic stopping.
Without this component, the number of training epochs is a hyper-parameter. We also tried removing
VθDKL(πθ k πθk)[si] from the gradient update step in (18). Table 1 illustrates the contribution of
the different components of SPU to the overall performance. The third roW shoWs that the term
VθDKL(πθ k πθk )[si] makes a crucially important contribution to SPU. Furthermore, per-state
acceptance and dynamic stopping are both also important for obtaining high performance, With the
former playing a more central role. When a component is removed, the hyper-parameters are retuned
to ensure that the best possible performance is obtained With the alternative (simpler) algorithm.
7
Published as a conference paper at ICLR 2019
Figure 1: SPU versus TRPO, PPO on 10 Mujoco environments in 1 million timesteps. The x-axis
indicates timesteps. The y-axis indicates the average episode reward of the last 100 episodes.
Table 1:	Ablation study for SPU
APProaCh	Percentage better than TRPO	PerformanCe vs. original algorithm
Original Algorithm	27%	=	0%
No grad KL	4%	-^85%
No dynamic stopping	24%	-71%
No Per-State acceptance	9%	-67%
6.3	Sensitivity Analysis on Mujoco
To demonstrate the practicality of SPU, we show that its high performance is insensitive to hyper-
parameter choice. One way to show this is as follows: for each SPU hyper-parameter, select a
reasonably large interval, randomly sample the value of the hyper parameter from this interval, and
then compare SPU (using the randomly chosen hyper-parameter values) with TRPO. We sampled
100 SPU hyper-parameter vectors (each vector including δ, , λ), and for each one determined the
relative performance with respect to TRPO. First, we found that for all 100 random hyper-parameter
value samples, SPU performed better than TRPO. 75% and 50% of the samples outperformed TRPO
by at least 18% and 21% respectively. The full CDF is given in Figure 4 in the Appendix. We can
conclude that SPU’s superior performance is largely insensitive to hyper-parameter values.
6.4	Results on Atari
(Rajeswaran et al., 2017; Mania et al., 2018) demonstrates that neural networks are not needed to
obtain high performance in many Mujoco environments. To conclusively evaluate SPU, we compare
it against PPO on the Arcade Learning Environments (Bellemare et al., 2012) exposed through
OpenAI gym (Brockman et al., 2016). Using the same network architecture and hyper-parameters,
we learn to play 60 Atari games from raw pixels and rewards. This is highly challenging because of
the diversity in the games and the high dimensionality of the observations.
8
Published as a conference paper at ICLR 2019
Here, we compare SPU against PPO because PPO outperforms TRPO by 9% in Mujoco. Averaged
over 60 Atari environments and 20 seeds, SPU is 55% better than PPO in terms of averaged final
performance. Figure 2 provides a high-level overview of the result. The dots in the shaded area
represent environments where their performances are roughly similar. The dots to the right of the
shaded area represent environment where SPU is more sample efficient than PPO. We can draw
two conclusions: (i) In 36 environments, SPU and PPO perform roughly the same ; SPU clearly
outperforms PPO in 15 environments while PPO clearly outperforms SPU in 9; (ii) In those 15+9
environments, the extent to which SPU outperforms PPO is much larger than the extent to which
PPO outperforms SPU. Figure 5, Figure 6 and Figure 7 in the Appendix illustrate the performance of
SPU vs PPO throughout training. SPU’s high performance in both the Mujoco and Atari domains
demonstrates its high performance and generality.
Percentage Improvement of SPU wrt PPO in one environment
SPU is 20 (or less) % better than PPO and vice versa
500	IOOO	1500	2000
Percentage improvement of SPU over PPO
Figure 2:	High-level overview of results on Atari
7 Acknowledgements
We would like to acknowledge the extremely helpful support by the NYU Shanghai High Performance
Computing Administrator Zhiguo Qi. We also are grateful to OpenAI for open-sourcing their baselines
codes.
9
Published as a conference paper at ICLR 2019
References
Yuval Tassa Remi Munos Nicolas Heess Martin Riedmiller Abbas Abdolmaleki, Jost Tobias Sprin-
genberg. Maximum a posteriori policy optimisation. 2018. URL https://arxiv.org/abs/
1806.06920.
Joshua Achiam. Advanced policy gradient methods. http://rll.berkeley.edu/
deeprlcourse/f17docs/lecture_13_advanced_pg.pdf, 2017.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
International Conference on Machine Learning, pp. 22-31, 2017.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-276,
1998.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning en-
vironment: An evaluation platform for general agents. CoRR, abs/1207.4708, 2012. URL
http://arxiv.org/abs/1207.4708.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/
abs/1606.01540.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/
openai/baselines, 2017.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. CoRR, abs/1604.06778, 2016. URL http:
//arxiv.org/abs/1604.06778.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290,
2018. URL http://arxiv.org/abs/1801.01290.
Yasemin Altun Jan Peters, Katharina Mulling. Relative entropy policy search. 2010. URL https://
www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264.
Sham Kakade. On the sample complexity of reinforcement learning. PhD thesis, University of
London London, England, 2003.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
ICML, volume 2, pp. 267-274, 2002.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Sergey Levine. UC Berkeley CS294 deep reinforcement learning lecture notes. http://rail.
eecs.berkeley.edu/deeprlcourse-fa17/index.html, 2017.
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive
approach to reinforcement learning. arXiv preprint arXiv:1803.07055, 2018.
Ruoming Pang Vijay Vasudevan Quoc V. Le Mingxing Tan, Bo Chen. Mnasnet: Platform-aware neural
architecture search for mobile. 2018. URL https://arxiv.org/abs/1807.11626.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518, 2015. URL http://dx.doi.org/10.1038/nature14236.
10
Published as a conference paper at ICLR 2019
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180-1190, 2008a.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
networks, 21(4):682-697, 2008b.
Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham Kakade. Towards generalization
and simplicity in continuous control. CoRR, abs/1703.02660, 2017. URL http://arxiv.
org/abs/1703.02660.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. CoRR, abs/1506.02438,
2015b. URL http://arxiv.org/abs/1506.02438.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Voot Tangkaratt, Abbas Abdolmaleki, and Masashi Sugiyama. Guide actor-critic for continuous
control. In International Conference on Learning Representations, 2018. URL https://
openreview.net/forum?id=BJk59JZ0b.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based con-
trol. 2012. URL https://ieeexplore.ieee.org/abstract/document/6386109/
authors.
ZiyU Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray KavUkCUoglu, and
Nando de Freitas. Sample efficient actor-critic with experience replay. CoRR, abs/1611.01224,
2016a. URL http://arxiv.org/abs/1611.01224.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint
arXiv:1611.01224, 2016b.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. In Reinforcement Learning, pp. 5-32. Springer, 1992.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
neural information processing systems, pp. 5285-5294, 2017.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR,
abs/1611.01578, 2016. URL http://arxiv.org/abs/1611.01578.
11
Published as a conference paper at ICLR 2019
Appendices
A Proofs for non-parameterized optimization problems
A. 1 Forward KL Aggregated and Disaggregated Constraints
We first show that (12)-(14) is a convex optimization. To this end, first note that the objective (12)
is a linear function of the decision variables π = {π(a∣s) ： S ∈ S, a ∈ A}. The LHS of (14) can
be rewritten as: P°∈a ∏(a∣s) logπ(a∣s) - P°∈a ∏(a∣s) log∏θk (a|s). The second term is a linear
function of π . The first term is a convex function since the second derivative of each summand is
always positive. The LHS of (14) is thus a convex function. By extension, the LHS of (13) is also a
convex function since it is a nonnegative weighted sum of convex functions. The problem (12)-(14)
is thus a convex optimization problem. According to Slater’s constraint qualification, strong duality
holds since πθk is a feasible solution to (12)-(14) where the inequality holds strictly.
We can therefore solve (12)-(14) by solving the related Lagrangian problem. For a fixed λ consider:
maximize X dπθk (s){ E [Aπθk (s, a)] - λDKL(π k πθk)[s]}	(28)
π∈Π	a~π(∙∣s)
s
subject to DKL(π k πθk )[s] ≤ for all s	(29)
The above problem decomposes into separate problems, one for each state s:
maximize E	[Aπθk (s, a)] - λDKL(π k πθk )[s]	(30)
π(∙∣s)	a~π(∙∣s)
subject to DKL(π k πθk)[s] ≤	(31)
Further consider the unconstrained problem (30) without the constraint (31):
maximize	π(a∣s) Aπ% (s,a) — λlog ( "a1?、[	(32)
∏(∙∣s)	a=1	L	∖∏θk (α∖s)J∖
K
subject to X π(a∖s) = 1	(33)
a=1
π(a∖s) ≥ 0,	a = 1, . . . , K	(34)
A simple Lagrange-multiplier argument shows that the opimal solution to (32)-(34) is given by:
πλ(a∖s)
πθk (a∖s)
Zλ(s)
eAπθk (s,a)∕λ
where Zλ(s) is defined so that πλ(∙∖s) is a valid distribution. Now returning to the decomposed
constrained problem (30)-(31), there are two cases to consider. The first case is when DKL(πλ k
πθk )[s] ≤ . In this case, the optimal solution to (30)-(31) is πλ(a∖s). The second case is when
DKL (πλ k πθk)[s] > . In this case the optimal is πλ(a∖s) with λ replaced with λs, where λs is the
solution to DKL (πλ k πθk )[s] = . Thus, an optimal solution to (30)-(31) is given by:
∏λ(a∖s)
πθk ⑷S) .A"。％ (s,a)∕λ
πθk ⑷S) .A"。％ (s,a)∕λs
Z (S)
S ∈ Γλ
S ∈/ Γλ
(35)
where Γλ = {S : DKL(πλ k πθk )[S] ≤ }.
To find the Lagrange multiplier λ, we can then do a line search to find the λ that satisfies:
Edπθk (S)DKL(∏λ k ∏θk)[s] = δ
s
(36)

12
Published as a conference paper at ICLR 2019
A.2 Backward KL Constraint
The problem (19)-(20) decomposes into separate problems, one for each state s ∈ S :
— π(als) 一 ,
maximize E --------------- A " (s, a)
π(∙∣s)	a~πθk (Ts) Lπθfc(a|s)	_
subject to E	log πθk(als) ≤ C
a〜∏θk(Ts) L	∏(a∣s) _
After some algebra, we see that above optimization problem is equivalent to:
maximize
π(Ts)
subject to
K
X Aπθk (s, a)π (a|s)
a=1
K
-E∏θk(a∣s)log∏(a∣s) ≤ J
a=1
K
£n(a|s) = 1
a=1
π(a∣s) ≥ 0, a = 1, ..., K
(37)
(38)
(39)
(40)
(41)
(42)
where C0 = C + entropy(πθk ). (39)-(42) is a convex optimization problem with Slater’s condition
holding. Strong duality thus holds for the problem (39)-(42). Applying standard Lagrange multiplier
arguments, it is easily seen that the solution to (39)-(42) is
∏*(a∣s) = ∏θk (a|s)
λ(s)
λ0(s) - Aπθk (s, a)
where λ(s) and λ0(s) are constants chosen such that the disaggregegated KL constraint is binding
and the sum of the probabilities equals 1. It is easily seen λ(s) > 0 and λ0(s) > maxa Aπθk (s, a)
A.3 L∞ CONSTRAINT
The problem (24-26) is equivalent to:
maximize
π(a1 IsI),…,n(am|sm)
subject to
m
X
i=1
Aπ (si, ai)
∏(ai∣Si)
∏k(ai∣Si)
1-C≤
∏(ai∣Si)
πθk (ai| Si)
≤1+C
i= 1,...,m
m
X
i=1
π(ai∣Si) - ∏θk (ai∣Si)
πθk (ai| Si)
≤δ
(43)
(44)
(45)
2
This problem is clearly convex. πθk (ai|Si), i = 1, . . . , m is a feasible solution where the inequality
constraint holds strictly. Strong duality thus holds according to Slater’s constraint qualification. To
solve (43)-(45), we can therefore solve the related Lagrangian problem for fixed λ:
∑m	ZlnA /	、π(ai∣Si)	ʌ ∏∏(ai∣Si) - ∏θk (a∕si)∖ 2	…、
Aπθk (Si, a,-	— λ ———l---ʒ-—-	(46)
∏(ai∣si),…，∏(am∣sm)分口	∏k(θi∣Si)	∖	∏θk (ai∣Si)	)
subject to 1 - c ≤ π(ailSi) ≤ 1 + c i =1,...,m	(47)
πθk (ai| Si)
which is separable and decomposes into m separate problems, one for each Si :
maximize 4"。《"电)⅛⅛⅛ - λ (n(ai|Si) - 丁(ai|Si)Y	(48)
π(ai∣si)	∏k(ai∣Si)	∖	∏θk (ai∣Si)	)
subject to 1 - C ≤ π(ailSi) ≤ 1 + C	(49)
一∏θk (ai∣Si)一
13
Published as a conference paper at ICLR 2019
The solution to the unconstrained problem (48) without the constraint (49) is:
*/ I、 A I ʌ A , Aπθk (si,ai)
π (aiISi) = πθk (αdSi) 11 + —2λ—
Now consider the constrained problem (48)-(49). If Απθk (si,ai) ≥ 0 and π*(a∕si) > ∏θk (a/si)(1 +
e), the optimal solution is ∏θk(a∕si)(1 + e). Similarly, If Απθk (si,ai) < 0 and ∏*(a∕si) <
∏θk (ai∣Si)(1 - e), the optimal solution is 不§工(a∕si)(1 - e). Rearranging the terms gives Theorem 3.
To obtain λ, we can perform a line search over λ so that the constraint (45) is binding.
14
Published as a conference paper at ICLR 2019
B Derivations the gradient of loss function for SPU
Let CE stands for CrossEntropy.
B.1	Forward-KL
CE (∏θ ∣∣∏λs )[s]
~
~\，，
一 E∏θ (a|s) log πλs (a∣s)(Expanding the definition of cross entropy)
a
-E∏θ(a∣s)log
a
-E∏θ(a∣s)log
a
πθk (a|s)
Z黑(S)
πθk (a|s)
Z黑(S)
eA θk (S,a"λs ) (Expanding the definition of ∏λs)
—Eπθ (ais)
a
Aπθk (s, a)
〜
λs
(Log of product is sum of log)
- X πθ (a|S) log πθk (a|S) +X πθ (a|s)logZ 黑(S)-上 Xπθk(a|s)
λs
a	a	sa
∏θ (a|s)
πθk (Rs)
Aπθk (s, a)
∏θ (a|s)
CE(πθllπθk)[s] + logZ黑(S)-'〜∏E(.1s) [∏θk(a|s)
Aπθk (s, a)
1
⇒ VθCE(∏θ∣∣∏λs )[s] = VθCE(∏θ∣∣∏θk )[s] - 丁
λs a〜∏
"Vθ ∏θ(a∣s)
E(.∣s) [ ∏θk(a|s)
Aπθk (s, a) (Taking gradient on both sides)
⇒ vθDKL(πθ k ππ )[S] = vθDKL(πθ k πθk )[S] - Ea〜∏E(.1」Vn⅛f Aπθk (S，a)
(Adding the gradient of the entropy on both sides and collapse the sum of gradients of cross entropy and entropy
into the gradient of the KL)
B.2	Reverse-KL
CE(∏θ ∣∣∏* )[s]
一 E∏θ(a|s) log π*(a∣s)(Expanding the definition of cross entropy)
a
-E∏θ(a∣s)log
a
πθk (a|S)
λ(s)
λ0(s) - Aπθk(s,a)
(Expanding the definition of π*)
-E∏θ(a∣s)log∏θk(a|s) - E∏θ(a∣s)logλ(s) + E∏θ(a∣s)log(λ0(s) - Aπθk(s,a))
CE(∏θ∣∣∏θk)[s] - λ(s) + Ea〜∏θ% [log(λ0(s) - Aπθk (s,a))
k Lπθk(a|s)	」
CE(πθ llπθk )[s] - λ(S)- Ea~πθk _π⅛⅛ log λ(s) - Aπθk(s,a) _
⇒ VθCE(∏θ∣∣∏*)[s] = VθCE(∏θ∣∣∏θk)[s] - Ea〜^仁
Vθ∏θ(a∣S) l _________1________
∏θk(a∣S) Og λ0(S) - Aπθk(S,a)
(Taking gradient on both sides)
⇒ VθDkl(∏θ k ∏*)[s] = VθDkl(∏θ k ∏θk)[s] - Ea〜∏θk
Vθ∏θ(a∣S) l ___________1________
∏θk (a∣S) Og λ0(S) - Aπθk (s, a)
(Adding the gradient of the entropy on both sides and collapse the sum of gradients of cross entropy and entropy
into the gradient of the KL)
15
Published as a conference paper at ICLR 2019
C Extension to Continuous State and Action S paces
The methodology developed in the body of this paper also applies to continuous state and action
spaces. In this section, we outline the modifications that are necessary for the continuous case.
We first modify the definition of dπ(s) by replacing P∏ (St = S) with 皋P∏(St ≤ S) so that dπ(s)
becomes a density function over the state space. With this modification, the definition of DKL (∏ ∣∣ ∏k)
and the approximation (8) are unchanged. The SPU framework described in Section 4 is also
unchanged.
Consider now the non-parameterized optimization problem with aggregate and disaggregate con-
straints (12-14), but with continuous state and action space:
maximize
π∈Π
subject to
dπθk (S)
E	[Aπθk (S, a)]dS
a 〜π(∙∣s)
dπθk (S)DKL(π ∣ πθk)[S]dS ≤ δ
DKL(π ∣ πθk)[S] ≤ for all S
(50)
(51)
(52)
Theorem 1 holds although its proof needs to be slightly modified as follows. It is straightforward to
show that (50-52) remains a convex optimization problem. We can therefore solve (50-52) by solving
the Lagrangian (28-29) with the sum replaced with an integral. This problem again decomposes with
separate problems for each S ∈ S giving exactly the same equations (30-31). The proof then proceeds
as in the remainder of the proof of Theorem 1.
Theorem 2 and 3 are also unchanged for continuous action spaces. Their proofs require slight
modifications, as in the proof of Theorem 1.
D Implementation Details and Hyperparameters
D.1 Mujoco
As in (Schulman et al., 2017), for Mujoco environments, the policy is parameterized by a fully-
connected feed-forward neural network with two hidden layers, each with 64 units and tanh nonlinear-
ities. The policy outputs the mean of a Gaussian distribution with state-independent variable standard
deviations, following (Schulman et al., 2015a; Duan et al., 2016). The action dimensions are assumed
to be independent. The probability of an action is given by the multivariate Gaussian probability
distribution function. The baseline used in the advantage value calculation is parameterized by a simi-
larly sized neural network, trained to minimize the MSE between the sampled states TD-λ returns
and the their predicted values. For both the policy and baseline network, SPU and TRPO use the same
architecture. To calculate the advantage values, we use Generalized Advantage Estimation (Schulman
et al., 2015b). States are normalized by dividing the running mean and dividing by the running
standard deviation before being fed to any neural networks. The advantage values are normalized by
dividing the batch mean and dividing by the batch standard deviation before being used for policy
update. The TRPO result is obtained by running the TRPO implementation provided by OpenAI
(Dhariwal et al., 2017), commit 3cc7df060800a45890908045b79821a13c4babdb. At every iteration,
SPU collects 2048 samples before updating the policy and the baseline network. For both networks,
gradient descent is performed using Adam (Kingma & Ba, 2014) with step size 0.0003, minibatch size
of 64. The step size is linearly annealed to 0 over the course of training. γ and λ for GAE (Schulman
et al., 2015b) are set to 0.99 and 0.95 respectively. For SPU, δ, , λ and the maximum number of
epochs per iteration are set to 0.05/1.2, 0.05, 1.3 and 30 respectively. Training is performed for 1
million timesteps for both SPU and PPO. In the sensitivity analysis, the ranges of values for the
hyper-parameters δ, , λ and maximum number of epochs are [0.05, 0.07], [0.01, 0.07], [1.0, 1.2] and
[5, 30] respectively.
16
Published as a conference paper at ICLR 2019
D.2 Atari
Unless otherwise mentioned, the hyper-parameter values are the same as in subsection D.1. The
policy is parameterized by a convolutional neural network with the same architecture as described in
Mnih et al. (2015). The output of the network is passed through a relu, linear and softmax layer in
that order to give the action distribution. The output of the network is also passed through a different
linear layer to give the baseline value. States are normalized by dividing by 255 before being fed
into any network. The TRPO result is obtained by running the PPO implementation provided by
OpenAI (Dhariwal et al., 2017), commit 3cc7df060800a45890908045b79821a13c4babdb. 8 different
processes run in parallel to collect timesteps. At every iteration, each process collects 256 samples
before updating the policy and the baseline network. Each process calculates its own update to the
network’s parameters and the updates are averaged over all processes before being used to update
the network’s parameters. Gradient descent is performed using Adam (Kingma & Ba, 2014) with
step size 0.0001. In each process, random number generators are initialized with a different seed
according to the formula Process_seed = experiment_seed + 10000 * Process_rank. Training
is performed for 10 million timesteps for both SPU and PPO. For SPU, δ, , λ and the maximum
number of epochs per iteration are set to 0.02, δ∕1.3,1.1 and 9 respectively.
E Algorithmic Description for SPU
Algorithm 1 Algorithmic description of forward-KL non-parameterized SPU
Require: A neural net πθ that parameterizes the policy.
Require: A neural net Vφ that approximates V πθ .
Require: General hyperparameters: γ, β (advantage estimation using GAE), α (learning rate), N
(number of trajectory per iteration), T (size of each trajectory), M (size of training minibatch).
Require: Algorithm-specific hyperparameters: δ (aggregated KL constraint), (disaggregated con-
straint), λ, ζ (max number of epoch).
1:	for k = 1, 2, . . . do
2:	under policy πθk , sample N trajectories, each of size T (sit, ait, rit, si(t+1)),	i =
1,...,N,t= 1,...,T
3:	Using any advantage value estimation scheme, estimate Ait,	i = 1, . . . , N, t = 1, . . . , T
4:	θ 一 θk
5:	φ 一 φk
6:	for ζ epochs do
7:	Sample M samples from the N trajectories, giving {s1, a1, A1, . . . , sM, aM, AM}
8:	L(φ) = TrP(V targ (sm) - Vφ(Sm))2
Mm
9:	φ 一 φ	—O^ φL(φ)
10:	L(θ) =	M P Ne DKL (πθ k πθk )[sm] - λ ?"：)：厂m Am IDKL(∏θk∏θk )[sm]≤e m	λ πθk (am |sm )
11:	θ 一 θ-	- αL(θ)
12:	iW Pm DKL(π k 3 )[sm]>δthen
13:	Break out of for loop
14:	θk + 1 - θ
15:	Φk+1 - Φ
F	Experimental results
F.1 Results on Mujoco for 3 million timesteps
TRPO and SPU were trained for 1 million timesteps to obtain the results in section 6. To ensure that
SPU is not only better than TRPO in terms of performance gain early during training, we further
retrain both policies for 3 million timesteps. Again here, SPU outperforms TRPO by 28%. Figure 3
illustrates the performance on each environment.
17
Published as a conference paper at ICLR 2019
Figure 3: Performance of SPU versus TRPO on 10 Mujoco environments in 3 million timesteps. The
x-axis indicates timesteps. The y-axis indicates the average episode reward of the last 100 episodes.
F.2 Sensitivity Analysis CDF for Mujoco
When values for SPU hyper-parameter are randomly sampled as is explained in subsection 6.3, the
percentage improvement of SPU over TRPO becomes a random variable. Figure 4 illustrates the
CDF of this random variable.
U QA
15 SO 25 M
Per<em⅛ge better than TRPCl
Sensitivity Aπaly≤⅛
Performartce of 100 different ⅞et⅛ of hyper-pdrameter≤
Each set i≤ ra∏d□rτιly generated
Figure 4: Sensitivity Analysis for SPU
18
Published as a conference paper at ICLR 2019
F.3 Atari Results
Figure 5, Figure 6 and Figure 7 illustrate the performance of SPU vs PPO throughout training in 60
Atari games.
Figure 5: Atari results (Part 1) for SPU vs PPO. The x-axis indicates timesteps. The y-axis indicates
the average episode reward of the last 100 episodes.
19
Published as a conference paper at ICLR 2019
Figure 6: Atari results (Part 2) for SPU vs PPO. The x-axis indicates timesteps. The y-axis indicates
the average episode reward of the last 100 episodes.
20
Published as a conference paper at ICLR 2019
Figure 7: Atari results (Part 3) for SPU vs PPO. The x-axis indicates timesteps. The y-axis indicates
the average episode reward of the last 100 episodes.
21