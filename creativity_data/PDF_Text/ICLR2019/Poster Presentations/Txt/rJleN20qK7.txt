Published as a conference paper at ICLR 2019
Two-Timescale	Networks for	Nonlinear
Value Function Approximation
Wesley Chung, Somjit Nath, Ajin George Joseph and Martha White
Department of Computing Science
University of Alberta
Ab stract
A key component for many reinforcement learning agents is to learn a value
function, either for policy evaluation or control. Many of the algorithms for learning
values, however, are designed for linear function approximation—with a fixed basis
or fixed representation. Though there have been a few sound extensions to nonlinear
function approximation, such as nonlinear gradient temporal difference learning,
these methods have largely not been adopted, eschewed in favour of simpler but not
sound methods like temporal difference learning and Q-learning. In this work, we
provide a two-timescale network (TTN) architecture that enables linear methods to
be used to learn values, with a nonlinear representation learned at a slower timescale.
The approach facilitates the use of algorithms developed for the linear setting, such
as data-efficient least-squares methods, eligibility traces and the myriad of recently
developed linear policy evaluation algorithms, to provide nonlinear value estimates.
We prove convergence for TTNs, with particular care given to ensure convergence
of the fast linear component under potentially dependent features provided by
the learned representation. We empirically demonstrate the benefits of TTNs,
compared to other nonlinear value function approximation algorithms, both for
policy evaluation and control.
1	Introduction
Value function approximation—estimating the expected returns from states for a policy—is heavily
reliant on the quality of the representation of state. One strategy has been to design a basis—such
as radial basis functions (Sutton and Barto, 1998) or a Fourier basis (Konidaris et al., 2011)—for
use with a linear function approximator and temporal difference (TD) learning (Sutton, 1988). For
low-dimensional observation vectors, this approach has been effective, but can be onerous to extend
to high-dimensional observations, potentially requiring significant domain expertise. Another strategy
has been to learn the representation, such as with basis adaptation or neural networks. Though there
is still the need to specify the parametric form, learning these representations alleviates the burden
of expert specification. Further, it is more feasible to scale to high-dimensional observations, such
as images, with neural networks (Mnih et al., 2015; Silver et al., 2016). Learning representations
necessitates algorithms for nonlinear function approximation.
Despite the deficiencies in specification for fixed bases, linear function approximation for estimating
value functions has several benefits over nonlinear estimators. They enable least-squares methods,
which can be much more data-efficient for policy evaluation (Bradtke and Barto, 1996; Szepesvari,
2010; van Seijen and Sutton, 2015), as well as robust to meta-parameters (Pan et al., 2017). Linear
algorithms can also make use of eligibility traces, which can significantly speed learning (Sutton,
1988; Dann et al., 2014; White and White, 2016), but have not been able to be extended to nonlinear
value function approximation. Additionally, there have been a variety of algorithms derived for the
linear setting, both for on-policy and off-policy learning (Sutton et al., 2009; Maei, 2011; van Seijen
and Sutton, 2014; van Hasselt et al., 2014; Mahadevan et al., 2014; Sutton et al., 2016; Mahmood
et al., 2017). These linear methods have also been well-explored theoretically (Tsitsiklis and Van Roy,
1997; Maei, 2011; Mahmood and Sutton, 2015; Yu, 2015) and empirically (Dann et al., 2014; White
and White, 2016), with some insights into improvements from gradient methods (Sutton et al., 2009),
true-online traces (van Seijen and Sutton, 2014) and emphatic weightings (Sutton et al., 2016). These
algorithms are easy to implement, with relatively simple objectives. Objectives for nonlinear value
function approximation, on the other hand, can be quite complex (Maei et al., 2009), resulting in
1
Published as a conference paper at ICLR 2019
more complex algorithms (Menache et al., 2005; Di Castro and Mannor, 2010; Bhatnagar et al., 2013)
or requiring a primal-dual formulation as has been done for control (Dai et al., 2017).
In this work, we pursue a simple strategy to take advantage of the benefits of linear methods, while
still learning the representation. The main idea is to run two learning processes in parallel: the first
learns nonlinear features using a surrogate loss and the second estimates the value function as a linear
function of those features. We show that these Two-timescale Networks (TTNs) converge, because
the features change on a sufficiently slow scale, so that they are effectively fixed for the fast linear
value function estimator.
Similar ideas have previously been explored for basis adaptation, but without this key aspect of
TTNs—namely the separation of the loss for the representation and value function. This separation is
critical because it enables simpler objectives—for which the gradient can be easily sampled—to drive
the representation, but still enables use of the mean squared projected Bellman error (MSPBE)—on
which all the above linear algorithms are based. This separation avoids the complexity of the nonlinear
MSPBE, but maintains the useful properties of the (linear) MSPBE. A variety of basis adaptation
approaches have used a two-timescale approach, but with the same objective for the representation
and the values (Menache et al., 2005; Di Castro and Mannor, 2010; Bhatnagar et al., 2013; J et al.,
2016). Yu and Bertsekas (2009) provided algorithms for basis adaptation using other losses, such
as Bellman error using Monte carlo samples, taking derivatives through fixed point solutions for
the value function. Levine et al. (2017) periodically compute a closed form least-squares solution
for the last layer of neural network, with a Bayesian update to prevent too much change. Because
these methods did not separate the value learn and basis adaptation, the resulting algorithms are more
complex. The strategy of using two different heads—one to drive the representation and one to learn
the values—has yet to be systematically explored.
We show that TTNs are a promising direction for nonlinear function approximation, allowing us to
leverage linear algorithms while retaining the flexibility of nonlinear function approximators. We first
discuss a variety of possible surrogate losses, and their potential for learning a useful representation.
We then show that TTNs converge, despite the fact that a linear algorithm is used with a changing
representation. This proof is similar to previous convergence proofs for policy evaluation, but with a
relaxation on the requirement that features be independent, which is unlikely for learned features.
We then show empirically that TTNs are effective compared to other nonlinear value function
approximations and that they can exploit several benefits of linear value approximations algorithms.
In particular, for both low-dimensional and high-dimensional (image-based) observations, we show
(a) the utility of least-squares (or batch) methods, (b) advantages from eligibility traces and (c) gains
from being able to select amongst different linear policy evaluation algorithms. We demonstrate that
TTNs can be effective for control with neural networks, enabling use of fitted Q-iteration within
TTNs as an alternative to target networks.
2	Background
We assume the agents act in a finite Markov Decision Process (MDP), with notation from (White,
2017). The dynamics of the MDP are defined by the 3-tuple (S, A, P ), where S is the set of states,
A the set of actions and P : S × A × S 7→ [0, 1] the transition probability function. The task in
this environment is defined by a reward function R : S × A × S 7→ R and a discount function
γ : S × A × S 7→ [0, 1]. At each time step, the agent takes an action At according to a policy
π : S × A 7→ [0, 1] and the environment returns reward Rt+1, next state St+1 and discount γt+1.
The goal in policy evaluation is to compute the value function: the expected sum of discounted
rewards from every state under a fixed policy π. The value function Vπ : S → R is defined recursively
from each state s ∈ S as
Vn(S) =ef E[Rt+ι + γt+ιV∏(St+ι)∣St = s] = X π(s,a) X P(s,a,s0)(r + γV∏(s0)).	(1)
a∈A	s0∈S
When using linear function approximation, this goal translates into finding parameters w ∈ Rd to
approximate the value function
V(S) =f x(s)>W ≈ Vn (S)	where X : S → Rd is a feature function.	(2)
More generally, a nonlinear function V(S) could be learned to estimate Vn.
2
Published as a conference paper at ICLR 2019
To formulate this learning problem, We need to consider the objective for learning the function V.
Let Vn, V ∈ R1S1 be the vectors of values for V∏ ,V. The recursive formula (1) defines a Bellman
operator Bπ Where the fixed point satisfies Bπ Vπ = Vπ . Consider a restricted value function class,
such as the set of linear value functions V ∈ F = {Xw | W ∈ Rd} where X ∈ RlSl×d is a matrix
With the i-th roW set to x(s) for ith state s ∈ S. Then, it may no longer be possible to satisfy
the recursion. Instead, an alternative is to find a projected fixed point ΠF Bπ V = V where the
projection operator ΠF projects Bπ V to the space spanned by this linear basis:
∏FV =f argmin kV - Vkd	(3)
V ∈F
where d ∈ R|S| is a vector which weights each state in the weighted norm kV k2d = Ps∈S d(s)V (s)2.
Many linear policy evaluation algorithms estimate this projected fixed point, including TD (Sutton,
1988), least-squares TD (Bradtke and Barto, 1996) and gradient TD (Sutton et al., 2009).
The objective formulated for this projected fixed-point, however, is more complex for nonlinear
function approximation. For linear function approximation, the projection operator simplifies into a
closed form solution involving only the features X. Letting δt = Rt+1 + γV (St+1) - V(St), the
resulting mean-squared projected Bellman error (MSPBE) can be written as
MSPBE(W) =f k∏FBnV - Vkd = E[δtxt] E[xtx>]-1 E[δtXt]	(4)
where E[δtXt] = Ps∈S d(s) E[δt∣St = s]x(s). For nonlinear function classes, the projection does
not have a closed form solution and may be expensive to compute. Further, the projection involves the
value function parameters, so the projection changes as parameters change. The nonlinear MSPBE
and resulting algorithm are more complex (Maei et al., 2009), and have not seen widespread use.
Another option is simply to consider different objectives. However, as we discuss below, other
objectives for learning the value function either are similarly difficult to optimize or provide poor
value estimates. In the next section, we discuss some of these alternatives and introduce Two-timescale
Networks as a different strategy to enable nonlinear value function approximation.
3	Two-timescale Networks and Surrogate Objectives
We first introduce Two-timescale Networks (TTNs), and then describe different surrogate objectives
that can be used in TTNs. We discuss why these surrogate objectives within TTNs are useful to drive
the representation, but are not good replacements for the MSPBE for learning the value function.
TTNs use two concurrent optimization processes: one for the parameters of the network θ and one
for the parameters of the value function w. The value function is approximated as V(S) =f x® (s)>w
where the features xθ : S → Rd are a parametrized function and θ ∈ Rm is adjusted to provide
better features. For a neural network, θ consists of all the parameters in the hidden layers, to produce
the final hidden layer xθ(s). The two optimization processes maintain different time scales, with the
parameters θ for the representation changed as a slow process, and the parameters w for the value
estimate changed as a fast process relative to θ.
The separation between these two processes could be problematic, since the target problem—
estimating the value function—is not influencing the representation! The slow process is driven by a
completely separate objective than the fast process. However, the key is to select this surrogate loss
for the slow process so that it is related to the value estimation process, but still straightforward to
compute the gradient of the loss. We use V(S) as the output of the fast part, which corresponds to the
value estimate used by the agent. To distinguish, Y(S) denotes the output for the slow-part (depicted
in Figure 1), which may or may not be an estimate of the value, as we discuss below.
Consider first the mean-squared TD error (MSTDE), which corresponds to Ps∈S d(s) E[δ2∣St = s].
Notice that this does not correspond to the mean-squared Bellman error (MSBE), for which it is more
difficult to compute gradients ∣∣BπV - Vkd = Ps∈S d(s)(E[δt∣St = s])2. Using the MSTDE as a
surrogate loss, with Y(S) = x®(s)>W, the slow part of the network minimizes
Lslow(θ) = min Xd(S) E[δt(θ, W)2 |St = s]	. δt(θ, W) = Rt+ι + Yt+ix®(St+ι)>W-x®(St)>W.
w∈ s∈S
3
Published as a conference paper at ICLR 2019
This slow part has its own weights W associated with estimating the value function, but learned
instead according to the MSTDE. The advantage here is that stochastic gradient descent on the
MSTDE is straightforward, with gradient δtV{θ,λw} [γt+ιY(St+ι) - Y(St)] where V{θ,-w}Y(St) is
the gradient of the neural network, including the head of the slow part which uses weights W. Using
the MSTDE has been found to provide worse value estimates than the MSPBE—which we re-affirm
in our experiments. It could, nonetheless, play a useful role as a surrogate loss, where it can inform
the representation towards estimating values.
There are a variety of other surrogate losses
that could be considered, related to the value
function. However, many of these losses are
problematic to sample incrementally, without
storing large amounts of data. For example,
the mean-squared return error (MSRE) could
be used, which takes samples of return and min-
imizes mean-squared error to those sampled re-
turns. Obtaining such returns requires waiting
many steps, and so delays updating the represen-
Figure 1: Two-Timescale Network architecture
tation for the current state. Another alternative is
the MSBE. The gradient of the nonlinear MSBE
is not as complex as the gradient of the nonlinear MSPBE, because it does not involve the gradient of
a projection. However, it suffers from the double sampling problem: sampling the gradient requires
two independent samples. For these reasons, we explore the MSTDE as the simplest surrogate loss
involving the value function.
Finally, surrogate losses could also be defined that are not directly related to the value function. Two
natural choices are losses based on predicting the next state and reward. The output of the slow part
could correspond to a vector of values, such as Yt = St+∖ ∈ Rn or 匕=[Rt+； ]. The ability to
predict the next state and reward is intuitively useful for enabling prediction of value, that also has
some theoretical grounding. Szepesvari (2010, Section 3.2.1) shows that the Bellman error is small, if
the features can capture a horizon of immediate rewards and expected next states. For linear encoders,
Song et al. (2016) show that an optimal set of features enables predictions of next state and reward.
More generally, learning representations using auxiliary tasks or self-supervised tasks have had some
successes in RL, such as using pixel control (Jaderberg et al., 2016) or classifying the temporal
distance between frames (Aytar et al., 2018). In computer vision, Gidaris et al. (2018) showed that
using rotated images as self-supervised tasks produced a useful representation for the main loss,
without training the representation with the main loss. Any of these self-supervised tasks could also
be used for the surrogate objective, and motivate that separating out representation learning does not
degrade performance. For now, we restrict focus on simpler surrogate objectives, as the main purpose
of this work is to demonstrate that the separation in TTNs is a sound approach for learning values.
4	Convergence of Two-Timescale Network Algorithm
Training TTNs is fully online, using a single transition from the environment at a time. Projected
stochastic gradient descent is used to reduce the surrogate loss, Lslow(θ) and a linear policy evaluation
algorithm, such as GTD2 or TD(λ), is coupled to the network where the prediction vector W is
callibrated proportional to -VwMSPBEθ(W). The full procedure is summarized in Algorithm 1, in
Appendix A. Regarding the convergence of TTNs, a few remarks are in order:
1.	The network needs to evolve sufficiently slowly relative to the linear prediction weights. In our
theoretical analysis, this is achieved by ensuring that the step sizes ξt and αt of the network and
the linear policy evaluation algorithm respectively decay to zero at different rates. In particular,
ξt∕αt → 0 as t → ∞. With this relative disparity in magnitudes, one can assume that the network is
essentially quasi-static, while the faster linear component is equilibrated relative to the static features.
2.	The linear prediction algorithms need to converge for any set of features provided by the neural
network, particularly linearly dependent features. This induces a technical bottleneck since linear
independence of the features are a necessary condition for the convergence of the prediction methods
GTD and GTD2. We overcome this by following a differential inclusion based analysis for GTD2.
4
Published as a conference paper at ICLR 2019
3.	Finally, we need to guarantee the stability of the iterates (both feature vector θt and the prediction
vector wt) and this is ensured by projecting the iterates to respective compact, convex sets.
The analysis for the convergence of the neural network is general, enabling any network architectures
that are twice continuously differentiable. We prove that the TTNs converge asymptotically to the
stable equilibria of a projected ODE which completely captures the mean dynamics of the algorithm.
We now state our main result (for notations and technical details, please refer Appendix B). The
results are provided for cases when TD(λ) or GTD2 is used as the linear prediction method. However,
note that similar results can be obtained for other linear prediction methods.
Theorem 1. Let θ = (θ, w)> and Θ ⊂ Rm+d be a compact, convex subset with smooth boundary.
Let the projection operator Γθ be Frechet differentiable and Γθ ( — 1 NLsIow )(θ) be Lipschitz con-
tinuous. Also, let Assumptions 1-3 hold. Let K be the set of asymptotically stable equilibria of the
following ODE contained inside Θ:
dtθ(t) = bf(t)(-2NgLslow)(H⑴),夕(O) ∈ θ andt ∈ r+∙
Then the StOChaStiC sequence {&t}t∈N generated by the TTN converges almost Surely to K (sample
path dependent). Further,
TD(λ) Convergence: Under the additional Assumption 4-TD(λ), we obtain the following result: For
any λ ∈ [0, 1], the stochastic sequence {wt}t∈N generated by the TD(λ) algorithm (Algorithm 2)
within the TTN setting converges almost surely to the limit w*, where w* satisfies
Πg*T(I) (Φg* W*) = Φg* w*,	(5)
with θH* ∈ K (sample path dependent).
5	Experiments
We investigate the performance of TTNs versus a variety of other nonlinear policy evaluation
algorithms, as well as the impact of choices within TTNs. We particularly aim to answer (a) is it
beneficial to optimize the MSPBE to obtain value estimates, rather than using value estimates from
surrogate losses like the MSTDE; (b) do TTNs provide gains over other nonlinear policy evaluation
algorithms; and (c) can TTNs benefit from the variety of options in linear algorithms, including least-
squares approaches, eligibility traces and different policy evaluation algorithms. More speculatively,
we also investigate if TTNs can provide a competitive alternative to deep Q-learning in control.
Experiments were performed on-policy in five environments. We use three classic continuous-state
domains: Puddle World, a continuous-state grid world with high-magnitude negative rewards for
walking through a puddle; Acrobot, where a robot has to swing itself up; and Cartpole, which involves
balancing a pole. We also use two game domains: Catcher, which involves catching falling apples;
and Puck World, in which the agent has to chase a puck (Tasfi, 2016). Catcher includes both a
variant with 4-dimensional observations—position and velocity of the paddle, and (x,y) of the apple—
and one with image-based observations—with two consecutive 64-by-64 grayscale images as input.
This domain enables us to analyze the benefit of the algorithms, on the same domain, both with
low-dimensional and high-dimensional observations. We describe the policies evaluated for these
domains in Appendix D. We include a subset of results in the main body, with additional results in
the appendix. Results in Cartpole are similar to Acrobot; Cartpole results are only in the appendix.
The value estimates are evaluated using root-mean-squared value error (RMSVE), where value error
is (V∏(s) — V(S))2. The optimal values for a set of 500 states are obtained using extensive rollouts
from each state and the RMSVE is computed across these 500 states. For the algorithms, we use
the following settings, unless specified otherwise. For the slow part (features), we minimize the
mean-squared TD error (MSTDE) using the AMSGrad optimizer (Reddi et al., 2018) with β1 = 0
and β2 = 0.99. The network weights use Xavier initialization (Glorot and Bengio, 2010); the weights
for the fast part were initialized to 0. In Puddle World, the neural network consists of a single hidden
layer of 128 units with ReLU activations. In the other environments, we use 256 units instead. To
choose hyperparameters, we first did a preliminary sweep on a broad range and then chose a smaller
range where the algorithms usually made progress, summarized in Appendix D. Results are reported
for hyperparameters in the refined range, chosen based on RMSVE over the latter half of a run with
shaded regions corresponding to one standard error.
5
Published as a conference paper at ICLR 2019
Catcher
Puddle world
Puck World
Root
Mean
Square
Error
Figure 2: TTN comparison to other nonlinear value function approximation algorithms. For the TTN, MSTDE
is used as the surrogate loss for the slow part of the network (feature learning) and LSTD is used for the fast part.
TTN vs. competitors. We compare to the following algorithms: nonlinear TD, nonlinear GTD
(Maei et al., 2009), Adaptive Bases (ABBE and ABTD) (Di Castro and Mannor, 2010), nonlinear
TD + LSTD regularization (inspired by Levine et al. (2017)). We describe these algorithms in more
depth in Appendix D. All of the algorithms involve more complex updates compared to TTNs,
except for nonlinear TD, which corresponds to a semi-gradient TD update with nonlinear function
approximation. For TTNs, we use LSTD for the linear, fast part.
In Figure 2, TTN is able to perform as well or better than the competitor algorithms. Especially in
Puddle World, its error is significantly lower than the second best algorithm. Interestingly, Nonlinear
GTD also performs well across domains, suggesting an advantage for theoretically-sound algorithms.
The utility of optimizing the MSPBE. First, we show that the TTN benefits from having a second
head learning at a faster timescale. To do so, we compare the prediction errors of using TTN, with
the fast process optimizing the MSPBE (using LSTD) and the slow one optimizing the MSTDE,
and one trained end-to-end using the MSTDE with AMSGrad. As a baseline, we include TTN with
a fixed representation (a randomly initialized neural network) to highlight that the slow process is
indeed improving the representation. We also include results for optimizing the MSTDE with the
fixed representation.
In Figure 3, we see that optimizing the MSPBE
indeed gives better results than optimizing the
MSTDE. Additionally, we can conclude that us-
ing the MSTDE, despite being a poor objective
to learn the value function, can still be effective
for driving feature-learning since it outperforms
the fixed representation.
Linear algorithms and eligibility traces.
TTNs give us the flexibility to choose any lin-
ear policy evaluation algorithm for the fast part.
We compare several choices: TD, least-squares
TD (LSTD) (Bradtke and Barto, 1996), forgetful
Figure 3: Comparison of MSPBE and MSTDE.
LSTD (FLSTD) (van Seijen and Sutton, 2015), emphatic TD (Sutton et al., 2016), gradient TD (the
TDC variant) (Sutton et al., 2009) and their true-online versions (van Seijen and Sutton, 2014; van
Hasselt et al., 2014) to learn the value function. GTD and ETD are newer temporal difference methods
which have better convergence properties and can offer increased stability. The true-online variants
modify the update rules to improve the behavior of the algorithms when learning online and seem
to outperform their counterparts empirically (van Seijen and Sutton, 2014). Least-squares methods
summarize past interaction, but are often avoided due to quadratic computation in the number of
features. For TTNs, however, there is no computational disadvantage to using LSTD methods, for two
reasons. It is common to choose deep but skinny architectures (Mnih et al., 2015; Hessel et al., 2017).
Furthermore, if the last layer is fully connected, then we already need to store O(d2) weights and use
O(d2) time to compute a forward pass—the same as LSTD. We include FLSTD, which progressively
forgets older interaction, as this could be advantageous when the feature representation changes over
time. For TTN, incremental versions of the least-squares algorithms are used to maintain estimates of
the required quantities online (see appendix D).
All of these linear algorithms can use eligibility traces to increase their sample efficiency by propa-
gating TD errors back in time. The trace parameter λ can also provide a bias-variance tradeoff for the
value estimates (Sutton, 1988; Dann et al., 2014). For nonlinear function approximation, eligibility
traces can no longer be derived for TD. Though invalid, we can naively extend them to this case by
keeping one trace per weight, giving us nonlinear TD(λ).
6
Published as a conference paper at ICLR 2019
(a)	(b)	(c)
Figure 4: a) & b) Linear methods on Puddle World and Catcher. c) Step size sensitivity in Catcher.
The results overall indicate that TTNs can benefit from the ability to use different linear policy
evaluation algorithms and traces, in particular from the use of least-squares methods as shown in
Figure 4 for Puddle World and Catcher. The dominance of LSTD versus the other linear algorithms
is consistent, including in terms of parameter sensitivity, persists for the other three domains. We
additionally investigated sensitivity to λ, and found that most of the TTN variants benefit from a
nonzero λ value and, in many cases, the best setting is high, near 1. One exception is the least-squares
methods, where LSTD performs similarly for most values of λ. Nonlinear TD(λ), on the other hand,
performs markedly worse as λ increases. This is unsurprising considering the naive addition of
eligibility traces is unsound. We include these sensitivity plots in the appendix, in Figure ??.
Surrogate loss functions. For all the previous experiments, we optimized the MSTDE for the slow
part of the network, but as discussed in Section 3, other objectives can be used. We compare a variety
of objectives, by choosing different Yt , including Yt = Rt+1 (Reward); Yt = St+1 (Next State); and
Yt = Rt+1 + Y (St+1). (Semi-gradient MSTDE). In Puck World, in Figure 5 a), we can see that
every auxiliary loss performed well. This does not appear to be universally true, as in Acrobot we
found that the MSTDE was a less effective surrogate loss, leading to slower learning (see Figure 5 b).
Alternate losses such as the semi-gradient MSTDE and next state predictions were more successful
in that domain. These results suggest that there is no universally superior surrogate loss and that
choosing the appropriate one can yield benefits in certain domains.
ACrobot
PUCk World
Root
Mean
Square
Error
Root
Mean
Square
Error
Number of Steps
Number of Steps
Cartpole
10∞
Number of Steps
(a)	(b)	(c)
Figure 5: a), b) & c) Comparison of surrogate losses on Puck World, Acrobot and Cartpole
Control Although the focus of this work is policy evaluation, we also provide some preliminary
results for the control setting. For control, we include some standard additions to competitor learning
algorithms to enable learning with neural networks. The DQN algorithm (Mnih et al., 2015) utilizes
two main tricks to stabilize training: experience replay—storing past transitions and replaying them
multiple times—and a target network—which keeps the value function in the Q-learning targets fixed,
updating the target network infrequently (e.g., every k = 10, 000 steps).
We use an alternative strategy to target networks for TTN. The use of a target network is motivated
by fitted Q-iteration (FQI) (Ernst et al., 2005), which updates towards fixed Q-values with one sweep
through a batch of data. TTNs provide a straightforward mechanism to instead directly use FQI,
where we can solve for the weights on the entire replay buffer, taking advantage of the closed form
solution for linear regression towards the Q-values from the last update. Batch FQI requires storing all
data, whereas we instead have a sliding window of experience. We therefore additionally incorporate
a regularization term, which prevents the weights from changing too significantly between updates,
similarly to Levine et al. (2017). Each FQI iteration requires solving a least squares problem on
the entire buffer, an operation costing O(nd2) computation where d is the number of features in
the last layer of the network and n is the size of the buffer; we update the network every k steps,
which reduces the per-step computation to O(nd2/k). The slow part drives feature-learning by
minimizing the semi-gradient MSTDE for state-action values. As another competitor, we include
7
Published as a conference paper at ICLR 2019
Non-Image Catcher
(a)
(b)
Figure 6: a) Comparison of returns obtained by each algorithm on a) non-image Catcher and b) image Catcher.
LS-DQN (Levine et al., 2017), a DQN variant which also utilizes adjustments to the final layer’s
weights towards the FQI solution, similar to TTN-FQI.
The experimental details differ for control. On nonimage Catcher, we do a sweep over αslow and
λreg, the regularization parameter, for TTN and sweep over the learning rate and the number of steps
over which is annealed for DQN. On image Catcher, runs require significantly more computation so
we only tune hyperparameters by hand. The FQI update in TTNs was done every 1000 (10000) steps
for non-image (image) Catcher. We run each algorithm 10 times (5 times) for 200 thousand steps (10
million steps) on the non-image (image) Catcher.
We see that TTN is able to perform well on both versions of Catcher in Figure 6, particularly learning
more quickly than the DQN variants. This difference is especially pronounced in the image version of
catcher, where TTN is also able to achieve much higher average returns than DQN. Both algorithms
seem to suffer from catastrophic forgetting later during training as the performance dips down after
an initial rise, although TTN still stabilizes on a better policy. Overall, these results suggest that
TTNs are a promising direction for improving sample efficiency in control, whilst still maintaining
stability when training neural networks.
6	Discussion and Conclusion
In this work, we proposed Two-timescale Networks as a new strategy for policy evaluation with
nonlinear function approximation. As opposed to many other algorithms derived for nonlinear value
function approximation, TTNs are intentionally designed to be simple to promote ease-of-use. The
algorithm combines a slow learning process for adapting features and a fast process for learning a
linear value function, both of which are straightforward to train. By leveraging these two timescales,
we are able to prove convergence guarantees for a broad class of choices for both the fast and
slow learning components. We highlighted several cases where the decoupled architecture in TTNs
can improve learning, particularly enabling the use of linear methods—which facilitates use of
least-squares methods and eligibility traces.
This work has only begun the investigation into which combinations for surrogate losses and linear
value function approximation algorithms are most effective. We provided some evidence that, when
using stochastic approximation algorithms rather than least-squares algorithms, the addition of traces
can have a significant effect within TTNs. This contrasts nonlinear TD, where traces were not
effective. The ability to use traces is potentially one of the most exciting outcomes for TTNs, since
traces have been so effective for linear methods. More generally, TTNs provide the opportunity to
investigate the utility of the many linear value function algorithms, in more complex domains with
learned representations. For example, emphatic algorithms have improved asymptotic properties
(Sutton et al., 2016), but to the best of our knowledge, have not been used with neural networks.
Another promising direction for TTNs is for off-policy learning, where many value functions are
learned in parallel. Off-policy learning can suffer from variance due to large magnitude corrections
(importance sampling ratios). With a large collection of value functions, it is more likely that some
of them will cause large updates, potentially destabilizing learning in the network if trained in an
end-to-end fashion. TTNs would not suffer from this problem, because a different objective can be
used to drive learning in the network. We provide some preliminary experiments in the appendix
supporting this hypothesis (Appendix C.7).
8
Published as a conference paper at ICLR 2019
References
Y. Aytar, T. Pfaff, D. Budden, T. Paine, Z. Wang, and N. de Freitas. Playing hard exploration games by watching
youtube. In Advances in Neural Information Processing Systems, pages 2935-2945, 2018.
M. Benaim, J. Hofbauer, and S. Sorin. Stochastic approximations and differential inclusions. SIAM Journal on
Control and Optimization, 44(1):328-348, 2005.
S. Bhatnagar, V. S. Borkar, and P. K. J. Feature Search in the Grassmanian in Online Reinforcement Learning. J.
Sel. Topics Signal Processing, 2013.
V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):291-294, 1997.
V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University Press, 2008.
S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal difference learning. Machine
Learning, 1996.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym.
arXiv preprint arXiv:1606.01540, 2016.
H.-F. Chen. Stochastic approximation and its applications, volume 64. Springer Science & Business Media,
2006.
B.	Dai, A. Shaw, L. Li, L. Xiao, N. He, J. Chen, and L. Song. Smoothed dual embedding control. arXiv preprint
arXiv:1712.10285, 2017.
C.	Dann, G. Neumann, and J. Peters. Policy evaluation with temporal differences: a survey and comparison. The
Journal of Machine Learning Research, 2014.
D.	Di Castro and S. Mannor. Adaptive Bases for Reinforcement Learning. In European Conference on Machine
Learning and Principles and Practice of Knowledge Discovery in Databases, 2010.
D.	Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine
Learning Research, 6(Apr):503-556, 2005.
S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In
International Conference on Learning Representations, 2018.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In
International Conference on Artificial Intelligence and Statistics, 2010.
M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. G. Azar, and
D. Silver. Rainbow - Combining Improvements in Deep Reinforcement Learning. arXiv, 2017.
P. K. J, S. Bhatnagar, and V. S. Borkar. Actor-Critic Algorithms with Online Feature Adaptation. ACM Trans.
Model. Comput. Simul., 2016.
M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement
learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.
G.	Konidaris, S. Osentoski, and P. S. Thomas. Value function approximation in reinforcement learning using the
Fourier basis. In International Conference on Machine Learning, 2011.
H.	Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications, volume 35.
Springer Science & Business Media, 2003.
H.	J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Unconstrained Systems.
Springer Science & Business Media, 2012.
D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017.
N. Levine, T. Zahavy, D. J. Mankowitz, A. Tamar, and S. Mannor. Shallow Updates for Deep Reinforcement
Learning. In International Conference on Learning Systems, 2017.
L. Ljung. Analysis of recursive stochastic algorithms. IEEE transactions on automatic control, 22(4):551-575,
1977.
H.	Maei. Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta, 2011.
9
Published as a conference paper at ICLR 2019
H. Maei, C. Szepesvdri, S. Bhatnagar, D. Precup, D. Silver, and R. S. Sutton. Convergent temporal-difference
learning with arbitrary smooth function approximation. In Advances in Neural Information Processing
Systems, 2009.
S. Mahadevan, B. Liu, P. S. Thomas, W. Dabney, S. Giguere, N. Jacek, I. Gemp, and J. Liu. Proximal reinforce-
ment learning: A new theory of sequential decision making in primal-dual spaces. CoRR abs/1405.6757,
2014.
A. R. Mahmood and R. Sutton. Off-policy learning based on weighted importance sampling with linear
computational complexity. In Conference on Uncertainty in Artificial Intelligence, 2015.
A. R. Mahmood, H. Yu, and R. S. Sutton. Multi-step Off-policy Learning Without Importance Sampling Ratios.
arXiv:1509.01240v2, 2017.
I.	Menache, S. Mannor, and N. Shimkin. Basis function adaptation in temporal difference reinforcement learning.
Annals of Operations Research, 2005.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.
Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra,
S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015.
J.	Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in
atari games. In Advances in Neural Information Processing Systems, pages 2863-2871, 2015.
Y. Pan, E. S. Azer, and M. White. Effective sketching methods for value function approximation. In Conference
on Uncertainty in Artificial Intelligence, Amsterdam, Netherlands, 2017.
A. Ramaswamy and S. Bhatnagar. Stochastic recursive inclusion in two timescales with an application to the
lagrangian dual problem. Stochastics, 88(8):1173-1187, 2016.
S. J. Reddi, S. Kale, and S. Kumar. On the Convergence of Adam and Beyond. In International Conference on
Learning Systems, 2018.
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P.
Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep
neural networks and tree search. Nature, 2016.
Z. Song, R. E. Parr, X. Liao, and L. Carin. Linear feature encoding for reinforcement learning. In Advances in
Neural Information Processing Systems, pages 4224-4232, 2016.
R. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press, 1998.
R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 1988.
R. S. Sutton, H. Maei, D. Precup, and S. Bhatnagar. Fast gradient-descent methods for temporal-difference
learning with linear function approximation. In International Conference on Machine Learning, 2009.
R. S. Sutton, A. R. Mahmood, and M. White. An emphatic approach to the problem of off-policy temporal-
difference learning. The Journal of Machine Learning Research, 2016.
C. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool Publishers, 2010.
N. Tasfi. Pygame Learning Environment. 2016.
J. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE
Transactions on Automatic Control, 1997.
H. van Hasselt, A. R. Mahmood, and R. Sutton. Off-policy TD (λ) with a true online equivalence. In Conference
on Uncertainty in Artificial Intelligence, 2014.
H. van Seijen and R. Sutton. A deeper look at planning as learning from replay. In International Conference on
Machine Learning, 2015.
H. van Seijen and R. S. Sutton. True online TD(lambda). In International Conference on Machine Learning,
2014.
A. M. White and M. White. Investigating practical, linear temporal difference learning. In International
Conference on Autonomous Agents and Multiagent Systems, 2016.
10
Published as a conference paper at ICLR 2019
M. White. Unifying task specification in reinforcement learning. In International Conference on Machine
Learning, pages 3742-3750, 2017.
H. Yu. On convergence of emphatic temporal-difference learning. In Annual Conference on Learning Theory,
2015.
H. Yu and D. P. Bertsekas. Basis function adaptation methods for cost approximation in MDP. In IEEE
Symposium on Adaptive Dynamic Programming and Reinforcement Learning, 2009.
11
Published as a conference paper at ICLR 2019
A TTN Algorithm
Algorithm 1 Training ofTTNs
1:	procedure TRAIN(w, θ, W,π)	.π is a fixed policy
2:	Initialize θ, W with Xavier initialization, W to 0 and thestarting state S according to the
environment
3:	while training do
4:	a J action chosen by π(s)
5:	r, s0 J Environments, a)	. Get reward and next state
6:	θ, W J GradientDescent on Lslow using sample (s, r, s0)
7:	W J Update on Lvalue using sample (s, r, s0)
8:	s J s0
9:	end while
10:	return learned parameters w, θ, W
11:	end procedure
B Convergence proof of two-timescale networks
B.1 Definitions & Notations
-Let R+ denote the set of non-negative real numbers, N = {0,1,2,... } and ∣∣ ∙ k denote the
Euclidean norm or any equivalent norm.
- A map f :	Rd	→	Rd	is Lipschitz continuous if ∣f (x) -	f (y)∣	≤ L(∣x - y∣),	for some
L ∈ (0, ∞), ∀x, y ∈ Rd.
- A set-valued map h : Rd → {subsets ofRd} is called a Marchaud map, ifit satisfies the following
conditions:
1. For each x ∈ Rd, h(x) is convex and compact.
2. For each x ∈ Rd, ∃K ∈ (0, ∞) such that supy∈h(x) ∣y∣ ≤ K(1 + ∣x∣).
3. h is upper-semicontinuous, i.e., if {xn}n∈N → x and {yn}n∈N → y, where xn ∈ Rd,
yn ∈ h(xn), ∀n ∈ N, then y ∈ h(x).
- For x1, x2 ∈ Rd and D ∈ Rk×k a diagonal matrix, we define the inner-product < x1, x2 >D,
x>Dx2. We also define the semi-norm ∣∣x∣∣d，< x, X >D. If all the diagonal elements ofD are
strictly positive, then ∣∣∙∣d is anorm.
- For any set X , let X denote the interior of X and ∂X denote the boundary of X .
-For brevity, let θ = (θ, W)> and Φθ be the feature matrix corresponding to the feature parameter θ,
i.e.
(Xe(sι)>、
八	Xe (s2)>
φe,	.	,	⑹
.
.
∖xe (SISI)>∕∣s∣×d
where Xe(s)> is the row-vector corresponding to state s. Further, define the |S| × |S|-matrix Pπ
as follows:
Psπ,s0 , X π(S, a)P (S, a, S0),	S, S0 ∈ S.	(7)
a∈A
- Also, recall that Lslow(θ) = M ST DE(θ) , E E δt* 1 2 3 |St.
- A function Γ : U ⊆ Rd1 → X ⊆ Rd2 is Frechet differentiable at X ∈ U if there exists a bounded
linear operator Γbx : Rd1 → Rd2 such that the limit
limr(X + Cy)-X
eψ0	€
(8)
•	.	1 ∙	1 .	/	∖	5 T	I - ∙	1 -	1	Λ∙ i'Γ∙	JFI ∙ ∕' 1 -	t .	1	∙ J	∕' I ■	♦.	.
exists and is equal to Γx(y). We say Γ is Frechet differentiable if Frechet derivative of Γ exists at
every point in its domain.
12
Published as a conference paper at ICLR 2019
B.2	Assumptions
Assumption 1: The pre-determined, deterministic, step-size sequence {ξt}t∈N satisfies
ξt >0,∀t∈N,	Xξt=∞,	Xξt2 <∞.
Assumption 2: The Markov chain induced by the given policy π is ergodic, i.e., aperiodic and
irreducible.
Assumption 2 implies that the underlying Markov chain is asymptotically stationary and henceforth it
guarantees the existence of a unique steady-state distribution dπ over the state space S (Levin and
Peres, 2017), i.e., limt→∞ P(St = s) = dπ(s),∀s ∈ S.
Assumption 3: Given a realization of the transition dynamics of the MDP in the form of a
sample trajectory Oπ = {S0, A0, R1, S1, A1, R2, S2, . . . }, where the initial state S0 ∈ S is chosen
arbitrarily, while the action A 3 At 〜∏(St, ∙), the transitioned state S 3 St+ι 〜P (St, At, ∙) and
the reward R 3 Rt+ι = R(St,At,St+ι).
B.3	Convergence Analysis
Figure 7: TTN model
To analyze the long-run behaviour of our algorithm, we employ the ODE based analysis (Borkar,
2008; Kushner and Yin, 2003; Ljung, 1977) of the stochastic recursive algorithms. Here, we consider
a deterministic ordinary differential equation (ODE) whose asymptotic flow is equivalent to the
long-run behaviour of the stochastic recursion. Then we analyze the qualitative behaviour of the
solutions of the ODE to determine the asymptotically stable sets. The ODE-based analysis is elegant
and conclusive and it further guarantees that the limit points of the stochastic recursion will almost
surely belong to the compact connected internally chain transitive invariant set of the equivalent
ODE. since the algorithm follows a multi-timescale stochastic approximation framework, we will
also resort to the more generalized multi-timescale differential inclusion based analysis proposed in
(Borkar, 1997; Ramaswamy and Bhatnagar, 2016).
Note that there exists only a unilateral coupling between the neural network (where the feature vectors
Gt are calibrated by following a stochastic gradient descent w.r.t. Lslow) and the various policy
evaluation algorithms (see Figure 7). This literally implies that the policy evaluation algorithms
depend on the feature vectors θGt but not vice-versa. Therefore, one can independently analyze the
asymptotic behaviour of the feature vectors {θGt }t∈N. Also, as a technical requirement, note that
since one cannot guarantee the stability (almost sure boundedness) of the iterates {θGt}t∈N (which is a
necessary condition required for the ODE based analysis. Please refer Chapter 2 of Borkar (2008)),
we consider the following projected stochastic recursion:
Θt+1= Γθ (θt + ξtδt (Va诒(St)- Yt+iV&诒(St+1))) ,	(9)
where Γθ(∙) is the projection onto a pre-determined compact and convex subset Θ ⊂ Rm+d, i.e.,
ΓΘ (x) = x, for x ∈ Θ, while for x ∈/ Θ, it is the nearest point in Θ w.r.t. the Euclidean distance (or
equivalent metric).
13
Published as a conference paper at ICLR 2019
Define the filtration {Ft}t∈N, a family of increasing natural σ-fields, where Ft ,
σ ({4,Si,Ri；0 ≤ i ≤ t}).
The following lemma characterizes the limiting behaviour of the iterates {&卜三闪：
Lemma 1. Let Assumptions 1-3 hold. Let Θ ⊂ Rm+d be a compact, convex subset with smooth
boundary. Let Γθ be Frechet differentiable. Further, let bΘ ( — 1 PLslow )(θ) be Lipschitz continuous.
Let K be the set of asymptotically stable equilibria of the following ODE contained inside Θ:
dt^(t) = bf(t)(-2▽占Lslow)(H⑴),夕(O) ∈ θ andt ∈ r+∙
Then the stochastic sequence {4t}t∈N generated by the TTN converges almost surely to K.
Proof. We employ here the ODE based analysis as proposed in (Borkar, 2008; Kushner and Clark,
2012). Firstly, we recall here the stochastic recursion which updates θHt ：
Θt+1 = γθ (& + ξtδt (VotYKSt)- γt+2tYKSt+1))),	(10)
where ΓΘ is the projection onto a pre-determined compact and convex subset Θ ⊂ Rm+d . Here,
δt，Rt+ι + γt+ιY0 (St+1) — YX (St) is the temporal difference. Also, V0Y ∈ R(m+d)×lSl is the
Jacobian of Yθo at θ = θt and Vθot Yθo(s) is the column corresponding to state s.
Now the above equation can be rewritten as
Θt+1 = Γθ (θt + ξt (h1(θt) + M1+1 + `l)),	(11)
where h1(θ') ， E [δt (V岳(St)- γt+ιV岳(St+。)], the noise term M1+ι ，
δt (Vdt¾(St) - Yt+ιV呢Ys(St+ι)) - E [δt(V&Yθ(St)-γt+ιVθtYKSt+1)) |Ft] and the
bias '1，E [δt (Vo,Ys(St) - %+»,¾(St+ι)) |Ft] - h1(θt).
Further,
θ = θ■+ ξ Γθ(θt + ξt(h1(^t)+ M1+1 + '1) - Gt)
t+1 t t	ξt
=Gt + ξt (bθt(h1 (θt)) + b0t (M1+1) + bΘt ('1) + o(ξt)) ,	(12)
where ΓbΘ is the Frechet derivative (defined in Eq. (8). Note that ΓΘ is single-valued since Θ is
convex and also the above limit exists since the boundary ∂Θ is assumed smooth. Further, for x ∈ Θ,
we have
bθ(y) = lim 厂(X + 'y)——X = lim X + 翼——X = y (for sufficiently small e),	(13)
x	→0	→0
i	.e., bθ(∙) is an identity map for X ∈ θ.
A few observations are in order：
C1： Γb θΘo (h1(θG)) is a Lipschitz continuous function in θG. This follows from the hypothesis of the
Lemma.
C2: bS (M1+ι) is a truncated martingale difference noise. Indeed, it is easy to verify that the
noise sequence {Mt1+1}t∈N is a martingale-difference noise sequence w.r.t to the filtration
{Ft+1}t∈N, i.e., Mt1+1 is Ft+1-measurable and integrable, ∀t ∈ N and E Mt1+1|Ft = 0
a.s., ∀t ∈ N. Also, since Γθ(∙) is a continuous linear operator, we have bθ(M1+1) to be
Ft+1-measurable and integrable, ∀t ∈ N likewise.
14
Published as a conference paper at ICLR 2019
C3: bθ^ ('1) → 0 as t → ∞ a.s. Indeed,
W* ⑷ Il =
limr1& + “1)- θ
→0
≤ lim
→0
≤ lim
→0
=k'1k.
∣∣rθ (& + e'1) -Γθ (4)||
By taking t → ∞, C3 follows directly from the ergodicity (Levin and Peres, 2017) (Assumption
2) and finiteness of the underlying Markov chain.
C4: o(ξt) → 0 as t → ∞ (follows from Assumption 1).
C5: Iterates {4t}t∈N are stable (forcefully), i.e. bounded almost surely, since θt ∈ Θ, ∀t ∈ N
(ensured by the projection operator ΓΘ) and Θ is compact (i.e., closed and bounded).
C6: ∃K0 ∈ (0, ∞), such that
E [kb* (M1+ι) k2|Fti ≤ Ko (1 + kθtk2) a.s.	(14)
This follows directly from the finiteness of the Markov chain and from the assumption that the
boundary ∂Θ is smooth.
Now, by appealing to Theorem 2, Chapter 2 of (Borkar, 2008)), we conclude that the stochastic
recursion (10) asymptotically tracks the following ODE
dtθ(t = ΓΘ㈤(h1(θ(t)),夕(0) ∈ Θ and t ∈ R+
= Γθ(t)(-2^sLsiow)(θ(t)),	θ(0) ∈ Θ andt ∈ R+.	(15)
In other words, the stochastic recursion (10) converges to the asymptotically stable equilibria of the
ODE (15) contained inside Θ.
□
Remark 1. It is indeed non-trivial to determine the constraint set Θ without prior adequate
knowledge about the limit set of the ODE (15). A pragmatic approach to overcome this concern is to
initiate the stochastic recursion with an arbitrary convex, compact set Θ with a smooth boundary
and gradually spread to the whole of Rm+d (Chen, 2006).
Remark 2. It is also important to characterize the hypothesis of the above lemma (i.e.,
Γθ ( — 1 PLslow )(θ) is Lipschitz continuous) with respect to the features 立.To achieve that one
has to consider the non-projected form of the ODE (15). Apparently, when one considers the
spreading approach proposed in the above remark, then it is essentially encouraged to consider the
non-projected form since the limiting flow of the ODE arising from the projected stochastic recursion
is more likely to lie inside the compact, convex set as Θ becomes larger. Thereupon, it is easy to
observe that the condition Yq IS twice ContInUoUSly-differentiable IS sufficient to ensure the Lipschitz
continuity of bΘ( — 1 VLslow)(θ). Additionally, in that case K = {θ∣V^Lslow(θ) = 0} which is the
set of local extrema of J.
B.4	TD(λ) ALGORITHM
One can directly apply the TD(λ) with linear function approximation algorithm to estimate the
value function with respect to the features provided by the neural network. The TD(λ) algorithm is
provided in Algorithm 2.
Here et, Wt ∈ Rd. Further, δt，Rt+ι + γt+ιw>XQt (St+ι) — w>XQt (St) is the temporal difference.
15
Published as a conference paper at ICLR 2019
Algorithm 2 TD(λ) algorithm
Parameters: αt > 0, λ ∈ [0, 1];
Initialization: w0 = 0, e0 = 0;
For each transition (St, Rt+1, St+1) in Oπ, do:
et+1 = xθt(St) +γt+1λet;	(16)
wt+1 = wt + αt Rt+1 + γt+1wt>xθt (St+1) - wt>xθt (St) et;	(17)
Assumption 4-TD(λ): The pre-determined, deterministic, step-size sequence {αt }t∈N satisfies:
αt > 0, ∀t ∈ N,	αt = ∞,
t∈N
αt2 < ∞,
t∈N
lim ξt- = 0.
t→∞ αt
Note that the step-size schedules {αt}t∈N and {ξt}t∈N satisfy α^t → 0, which implies that {ξt}
converges to 0 relatively faster than {αt}. This disparity in terms of the learning rates induces an
asynchronous convergence behaviour asymptotically (Borkar, 1997), with feature parameter sequence
{&} converging slower relative to the TD(λ) sequence {wt}. The rationale being that the increment
term of the underlying stochastic gradient descent of the neural network is smaller compared to that of
the TD(λ) recursion (17), since the neural network SGD is weighted by the step-size schedule {ξt}t∈N
which is smaller than {αt}t∈N for all but finitely many t. This unique pseudo heterogeneity induces
multiple perspectives, i.e., when viewed from the faster timescale recursion (recursion controlled
by αt), the slower timescale recursion (recursion controlled by ξt) seems quasi-static (‘almost a
constant’), while viewed from the slower timescale, the faster timescale recursion seems equilibrated.
Further, it is analytically admissible (Borkar, 1997) to consider the slow timescale stochastic recursion
(i.e., the neural network SGD) to be quasi-stationary (i.e., Gt ≡ θ, ∀t ∈ N), while analyzing the
asymptotic behaviour of the relatively faster timescale stochastic recursion (17). Thereupon, we
obtain the following directly from Theorem 1 of (Tsitsiklis and Van Roy, 1997).
Lemma 2. Assume θGt ≡ θG, ∀t ∈ N. Let Assumptions 1-3 and 4-TD(λ) hold. Then for any λ ∈ [0, 1],
the stochastic sequence {wt}t∈N generated by the TD(λ) algorithm (Algorithm 2) within the TTN
setting converges almost Surely to the limit w*, where w* satisfies
ΠgT(λ)(Φgw*) = Φρw*,	(18)
withT(λ)J(s) , (1 - λ) Pi∞=0 λiE hPij=0 γ[j]Rj+1 + γ[i+1]J(Si+1)S0 =si andγ[j] = Πij=0γi
(with γo = 1). Also, Πg is defined according to Eq. (3) with F = {Φ^w∣w ∈ Rd}.
For other single-timescale prediction methods like ETD and LSPE, similar results follow. Regarding
the least squares method LSTD, which offers the significant advantage of non-dependency on step-
sizes (albeit computationally expensive) couples smoothly within the TTN setting without any
additional consideration.
B.5	GTD2 Algorithm
However, one cannot directly apply the original GTD2 and TDC algorithms to the TTN setting, since a
necessary condition required for the convergence of these algorithms is the non-singularity of the fea-
ture specific matrices E xθt(St)xθt(St)> and E (xθt(St) - γt+1xθt(St+1)) xθt(St)>. Please
refer Theorem 1 and Theorem 2 of (Sutton et al., 2009). Without the non-singularity assumption,
it is indeed hard to guarantee the almost sure boundedness of the GTD2/TDC iterates. In the TTN
setting that we consider in this paper, one cannot explicitly assure this condition, since the features
are apparently administered by a neural network and it is not directly intuitive on how to control the
neural network to generate a collection of features with the desired non-singularity characteristic.
Henceforth, one has to consider the projected versions of these algorithms. We consider here the
projected GTD2 algorithm provided in Algorithm 3.
16
Published as a conference paper at ICLR 2019
Algorithm 3 GTD2 algorithm
Parameters: αt, βt ;
Initialization: u0 ∈ U, w0 ∈ W ;
For each transition (St, Rt+1, St+1) in Oπ do:
Wt+1 = Γw (Wt + at (δu+ /θt (St)-(Wt>Xθt (St)) x©,(St))) ;	(19)
ut+ι = ΓU (Ut + βt (x©,(St) - Yt+ix©,(St+ι)) (wt>Xθt (St))) ；	(20)
Here ut, Wt ∈ Rd. Further, δU+ι，Rt+i + γt+ιu>Xθ,(St+i) - u>x©,(St) is the temporal
difference.
Here, Γw(∙) is the projection operator onto a pre-determined convex, compact subset W ⊂ Rd with
a smooth boundary ∂W. Therefore, ΓW maps vectors in Rd to the nearest vectors in W w.r.t. the
Euclidean distance (or equivalent metric). Convexity and compactness ensure that the projection is
unique and belongs to W. Similarly, U is a pre-determined convex, compact subset of Rd with a
smooth boundary ∂U.
Projection is required since the stability of the iterates {Wt}t∈N and {Ut }t∈N are hard to guarantee
otherwise.
Assumption 4-GTD2: The pre-determined, deterministic, step-size sequences {αt}t∈N and {βt}t∈N
satisfy
αt , βt > 0, ∀t ∈ N,	X αt = X βt = ∞,
^X (a2	+ β2)	< ∞, lim —	= 0,	lim	∣t	= 0.
t t	t→∞ αt	t→∞	βt
t∈N	t	t
Define the filtration {Ft}t∈N, a family of increasing natural σ-fields, where Ft	,
σ ({wi, Ui,θi,Si,Ri∖0 ≤ i ≤ t}).
Similar to the TD(λ) case, here also we follow the quasi-stationary argument. Henceforth, we
analyze the asymptotic behaviour of GTD2 algorithm under the assumption that feature vector Gt is
quasi-static, i.e. Gt ≡ θ = (θ, W)>.
Lemma 3. Assume θGt ≡ θG = (θ, WG )>, ∀t ∈ N. Let Assumptions 1-3 and 4-GTD2 hold. Then
n(U,W)> litm→∞inf (U, W)> - (Ut, Wt)>o ⊆ [ n(U, W)>W ∈ Auo,	(21)
u∈A*
where A* is the set of asymptotically Stable equilibria ofthefollowing ODE:
du(t) = ΓU(t) (Φ>Ddn(I-Yt+iPπ)Φsu(t)),	u(0) ∈ U, t ∈ R+	(22)
and Au is the asymptotically stable equilibria of the following ODE:
dw(t) = bW(t) ((φ>Ddnδu - φ>Ddn φe) w(t)), W(O) ∈ W and t ∈ R+,
with δu defined in Eq. (29).
Proof. The two equations in the modified GTD2 algorithm constitute a multi-timescale stochastic
approximation recursion, where there exists a bilateral coupling between the stochastic recursions
(19) and (20). Since the step-size sequences {αt}t∈N and {βt}t∈N satisfy β → 0, we have βt → 0
faster than αt → 0. This disparity in terms of the learning rates induces a pseudo heterogeneous
rate of convergence (or timescales) between the individual stochastic recursions which results in a
17
Published as a conference paper at ICLR 2019
pseudo asynchronous convergence behaviour when considered over a finite time window. Also note
that the coherent long-run behaviour of the multi-timescale stochastic recursion will asymptotically
follow this short-term behaviour with the window size extending to infinity(Borkar, 1997; 2008).
This pseudo behaviour induces multiple viewpoints, i.e., when observed from the faster timescale
recursion (recursion controlled by αt), the slower timescale recursion (recursion controlled by βt)
appears quasi-static (‘almost a constant’), while observed from the slower timescale, the faster
timescale recursion seems equilibrated. Further, it is analytically admissible (Borkar, 1997) to
consider the slow timescale stochastic recursion (20) to be quasi-stationary (i.e., ut ≡ u, ∀t ∈ N),
while analyzing the limiting behaviour of the relatively faster timescale stochastic recursion 19.
Analysis of the faster time-scale recursion: The faster time-scale stochastic recursion of the GTD2
algorithm is the following:
Wt+1 = Γw (Wt + at (δu+ /θt(St)-(Wt>Xθt (St)) x©,(St))) .	(23)
Under the previously mentioned quasi-stationary premise that Ut ≡ U and θt ≡ θ = (θ, w)>, ∀t ∈ N,
thereupon, we analyze the long-term behaviour of the following recursion:
wt+ι = Γw (wt + at (δu+ιXt - (wt>xt) xt)) ,	(24)
where xt = xθ(St) and δtu+1 , Rt+1 + γt+1U>xt+1 - U>xt.
The above equation can be rearranged as the following:
wt+1 = rW (wt + αt (h2(Wt) + M2+1 + '2)),
where	the	noise M2+ι	， δJ+ιXt -	(w>xt) Xt	-	E [δJ+ιXt - (w>xt) Xt∣Ft],
h2 (w)	，	E [δJ+ιXt - (w>xt)	xt] and the	bias '	=	E	[64历一(w>xt) xt∣Ft]-
E [δu+1xt - (w>xt) xt].
Similar to Equation (12), we can rewrite the above recursion as follows:
wt = wt + at (bWt (h2(wt)) + bWt (m2+i) + ΓWt ('2) + o(αt)) ,	(25)
where ΓWt (∙) is the Frechet derivative (defined in Equation (8)) of the projection operator ΓW.
A few observations are in order:
D1: The iterates {wt}t∈N are stable, i.e., supt∈N kwt k < ∞ a.s. This immediately follows since
W is bounded.
D2: {bW (M2+ι)}t∈N is a martingale-difference noise sequence with respect to the filtration
{Ft+1}t∈N. This follows directly since {Mt2+1}t∈N is a martingale-difference noise sequence
with respect to the same filtration.
D3: {bWWt (M2+ι)}t∈N are square-integrable and ∃K2 ∈ (0, ∞) such that
E UrWi (m2+i) k2∣Fti ≤ K2 (1 + kwtk2) a.s., t ∈ N.	(26)
This follows directly from the finiteness of the underlying Markov chain and from the assump-
tion that the boundary ∂W is smooth.
D4: ΓbwW (h2(w)) is Lipschitz continuous with respect to w. Proof similar to C1.
D5: bWWt ('2) → 0 as t → ∞ a.s. Proof similar to C3.
Now by appealing to Theorem 2, Chapter 2 of (Borkar, 2008) along with the above observations, we
conclude that the stochastic recursion 23 asymptotically tracks the following ODE almost surely:
ddtw(t) = bW(t)(h2(w(t)), w(0) ∈ W and t ∈ R+.
= bW(t) (e [δu+ιxt] - E [xtx>] w(t)), w(0) ∈ W and t ∈ R+.	(27)
18
Published as a conference paper at ICLR 2019
Therefore, wt converges asymptotically to the stable equilibria of the above ODE contained inside
W almost surely.
Qualitative analysis of the solutions of ODE (27): A trivial qualitative analysis of the long-run
behaviour of the flow induced by the above ODE attests that the stable limit set is indeed the solutions
of the following linear system inside W (This follows since ΓW (y) = y for W ∈ W and also because
ΓW (∙) does not contribute any additional limit points on the boundary other than the roots of h2 since
∂W is smooth).
E xtxt> E δtu+1xt - E xtxt> E xtxt> W = 0.
⇒ E xtxt> W = E δtu+1xt .	(28)
Note that E [xtx>] = Φ>Ddn Φ分
Claim 1: The above linear system of equations is consistent, i.e., E [δU+ιXt] ∈ R(Φ>DdnΦ^), i.e.,
the range-space of Φ> Ddn Φ^: To see that, note that the above system can indeed be viewed as the
least squares solution to the Φ^w = δu with respect to the weighted-norm ∣∣ ∙ ∣∣Dd7r, where
δu(s) = Rn(s)+ Yt+1 X P∏sou>Xθ(s0) - X P∏s0u>Xθ(s0),	(29)
s0∈S	s0∈S
where R is the expected reward. (Note that E [δju+ιx/ = Φ>Ddn δu).
The least-squares solution w0 ∈ Rd (which certainly exists but may not be unique) satisfies
< Φ^w, δu — Φ^wo >DdK = O, ∀w ∈ Rd
⇒ < w,D-1Φ>Ddn (δu — Φ^wo) >Ddn=0, ∀w ∈ Rd.
Now choose W = D-JΦ>Ddn (δu — Φgw°). Then
Φ> Ddn (δu — Φgw°) = 0 ⇒ Φ> Ddn Φgw° = Φ> Ddn δu.	[ End of proof of Claim 1 ]
Since Φ>Ddn Φg may be singular (i.e., Φ>Ddn Φθ is not invertible), the above least squares solution
may not be unique and hence the collection of asymptotically stable equilibria of the flow induced by
the ODE (27) may not be singleton for every u. Let’s denote the asymptotically stable equilibria of
the flow induced by the said ODE to be the set Au, where 0 = Au ⊆ W.
Analysis of the slower time-scale recursion: The slower time-scale stochastic recursion of the
GTD2 algorithm is the following:
Ut+1 =ΓU (Ut + βt (xt — γt+1Xt+1) (wt>Xt)) , Ut ∈ Rd, uo ∈ U.	(30)
Note that since βt → 0, the stochastic recursion (20) is managed on a faster timescale relative to
the the neural network stochastic recursion (10) and henceforth, we continue to maintain here the
quasi-stationary condition θ ≡ θ= (θ, w)>.
Now the above equation can be rearranged as follows:
Ut+1 = Γu (Ut + βt (E [∆Wι] + M3+1 + '3)),	(31)
where ∆w+ι，(Xt — γt+1Xt+1) (wt>xt) = ((Xt — γt+ιxt+ι) x>) W the noise term M3+1，
∆W+ι - E [∆W+ι∣Ft] and the bias '，E [∆邙 |Ft] - E [∆^].
Similar to Equation (12), we can rewrite the above recursion as follows:
ut+1 = Ut + βt (bUt (E [∆W+ι]) + ΓUt (M3+i) + bUt ('3) + o(βt)) ,	(32)
where「；(∙)is the Frechet derivative (defined in Equation (8)) of the projection operator ΓU.
Now the above equation can be interpreted in terms of stochastic recursive inclusion as follows:
Ut+1 = Ut + βt (bUt (E [△*]) + ΓUt (M3+ι) + bUt ('3) + o(βt)) ,	(33)
19
Published as a conference paper at ICLR 2019
with bUt(E [∆W+J) ∈ h3(ut), where the set-valued map h3 : Rd → {subsets of Rd} is defined as
h3(u)，nbbU (E [∆W++ι]), where W ∈ Au}.	(34)
Indeed h3(u) = {bU (BW), where B = E [((xt - γt+1xt+1) x>)] and W ∈ Au}. It is easy to
verify that B = Φ>Ddn(I- Yt+ιPπ)Φ介
Here, one cannot directly apply the multi-timescale stochastic approximation results from (Borkar,
1997) since the said paper assumes that the limit point from the slower timescale recursion is unique
(Please see Chapter 6 of (Borkar, 2008)). But in our setting, the slower timescale recursion (23)
has several limit points (note that the stable limit set Au is not singleton). This is where our
analysis differs from that of the seminal paper on GTD2 algorithm, where it is assumed that both the
matrices E xtxt> and E (xt - γt+1xt+1)xt> are certainly non-singular. However, in our TTN
setting, one cannot guarantee this condition, since the features are apparently provided by a neural
network and it is hard to fabricate the neural network to generate a collection of features with the
desired non-singularity properties. In order to analyze the limiting behaviour of the GTD2 algorithm
under the relaxed singularity setting, henceforth one has to view the stochastic recursion (30) as a
stochastic recursion inclusion (Benaim et al., 2005) and apply the recent results from (Ramaswamy
and Bhatnagar, 2016) which analyzes the asymptotic behaviour of general multi-timescale stochastic
recursive inclusions.
A few observations are in order:
E1: For each u ∈ U, h3(u) is a singleton: This follows from the definition of h3 and Claim 1
above, where we established that each W ∈ Au is the least squares solution to the linear system
of equations Φ^w = δu. Therefore, it further implies that that h3 is a Marchaud map as well.
E2: supt∈N (kWtk + kutk) < ∞ a.s. This follows since W and U are bounded sets.
E3: {ΓUt (M3+J}t∈N is a martingale-difference noise sequence with respect to the filtration
{Ft+1}t∈N. This follows directly since {Mt3+1}t∈N is a martingale-difference noise sequence
with respect to the same filtration.
E4: {ΓUt (M3+J}t∈N are square-integrable and ∃K3 ∈ (0, ∞), such that
E h∣∣ΓUt (M3+1 )∣∣2∣Fti ≤ K3 (1 + kutk2 + kwtk2) a.s., t ∈ N.	(35)
This follows directly from the finiteness of the underlying Markov chain and from the assump-
tion that the boundary ∂U is smooth.
E5: bU ('3t) → 0 as t → ∞ a.s. Proof similar to C3. This implies that the bias is asymptotically
irrelevant.
E6: For each u ∈ U , the set Au is a globally attracting set of the ODE (27) and is also Lyapunov
stable. Further, there exists K4 ∈ (0, ∞) such that supw∈Au kWk ≤ K4(1 + kuk). This
follows since Au ⊆ W and W is bounded.
E7: The set-valued map q : U → {subsets ofRd} given by q(u) = Au is upper-semicontinuous:
Consider the convergent sequences {un}n∈N → u and {Wn}n∈N → W with un ∈ U and
Wn ∈ q(un) = Au. Note that W ∈ W, u ∈ U since W and U are compact. Also
Φ> Ddn ΦgWn = Φ> Ddn δun (from Claim 1). Now taking limits on both sides we get
lim Φ> Ddn ΦθW = lim Φ> Ddn δun	⇒	Φ> Ddn ΦθW = Φ> Ddn δu
n→∞	n→∞
This implies that W ∈ Au = q(u). The claim thus follows.
Thus we have established all the necessary conditions demanded by Theorem 3.10 of (Ramaswamy
and Bhatnagar, 2016) to characterize the limiting behaviour of the stochastic recursive inclusion (33).
20
Published as a conference paper at ICLR 2019
Now by appealing to the said theorem, we obtain the following result on the asymptotic behaviour of
the GTD2 algorithm:
n(u, w)> lim inf (u, w)> - (ut,wt)>o ⊆ [ n(u, w)>w ∈ Auo,	(36)
u∈A*
where A* is the set of asymptotically stable equilibria of the following ODE:
dt u(t) = h3(u(t)),	u(0) ∈U, t ∈ R+.	(37)
□
One can obtain similar results for projected TDC.
We now state our main result:
Theorem 2. Let Θ ⊂ Rm+d be a compact, convex subset with smooth boundary. Let ΓΘ be Frechet
differentiable. Further, let bΘ ( — 1 ^Lfilciw )(θ) be Lipschitz continuous. Also, let Assumptions 1-3
hold. Let K be the set of asymptotically stable equilibria of the following ODE contained inside Θ:
dtθ(t) = bf(t)(-2▽占LsIow)(8(t)),	夕(0) ∈ U andt ∈ R+∙
Then the stochastic sequence {&t}t∈N generated by the TTN converges almost Surely to K (sample
path dependent). Further,
TD(λ) Convergence: Under the additional Assumption 4-TD(λ), we obtain the following result: For
any λ ∈ [0, 1], the stochastic sequence {wt}t∈N generated by the TD(λ) algorithm (Algorithm 2)
within the TTN setting converges almost surely to the limit w* , where w* satisfies
Π(9* T(λ) (Φ(9* w*) = Φ(9* w*,	(38)
with T(λ) defined in Lemma 2 and θ* ∈ K (sample path dependent).
GTD2 Convergence: Let W, U ⊂ Rd be compact, convex subsets with smooth boundaries.
Let Assumption 4-GTD2 hold. Let ΓW and ΓU be Frechet differentiable. Then the stochastic
sequences {wt}t∈N and {ut}t∈N generated by the GTD2 algorithm (Algorithm 3) within the TTN
setting satisfy
(u,w)> litm→∞inf (u, w)> — (ut, wt)>	⊆ [	(u, w)>w ∈ Au ,
u∈A*
where A* is the set of asymptotically stable equilibria of the following ODE:
dt u(t) = rU(t) (φ(>* Dd∏(I — γt+1Pπ)φ(9* u(t)),	U(O) ∈ U, t ∈ R+
and Au is the asymptotically stable equilibria of the following ODE:
dtw(t) = bW(t)((φ;*Ddnδu — φ>*Ddnφ*) w(t)), W(O) ∈W andt ∈ R+,
with θ* ∈ K (sample path dependent) and δu defined in Eq. (29).
21
Published as a conference paper at ICLR 2019
C Additional Experiments
C.1 Nonimage Catcher
Figure 8: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the
fast part of the TTN.
(a)
(b)
Catcher
0∙0⅛
Number of Steps
(c)
Figure 10: a) Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 9: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison of
least-squares methods
(b)
22
Published as a conference paper at ICLR 2019
C.2
Figure 11: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the
fast part of the TTN.
(a)
(b)
PUddIeworId
Number of Steps
(c)
Figure 12: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison of
least-squares methods
(a)	(b)
Figure 13: a) Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
23
Published as a conference paper at ICLR 2019
C.3 Image Catcher
We also ran policy evaluation experiments on image-based catcher with 2 stacked 64x64 frames
as input. The policy evaluated was the same as was used in the non-image setting. Similar to the
non-imaged based catcher experiments, We have similar plots.
Catcher
IOOO	2000	3000	4000	5000
Number of Steps
Figure 14: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the
fast part of the TTN.
(c)
(a)
(b)
(a)
Figure 16: a) Comparison of MPSBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 15: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison of
least-squares methods
(b)
24
Published as a conference paper at ICLR 2019
C.4 Cartpole
In the classic Cartpole environment, the agent has to balance a pole on a cart. The state is given by
vector of 4 numbers (cart position, cart velocity, pole angle, pole velocity). The two available actions
are applying a force towards the left or the right. Rewards are +1 at every timestep and an episode
terminates once the pole dips below a certain angle or the cart moves too far from the center. We use
the OpenAI gym implementation (Brockman et al., 2016).
The policy to be evaluated consists of applying force in the direction the pole is moving with
probability 0.9 (stabilizing the pole) or applying force in the direction of the cart’s velocity with
probability 0.1. We inject some stochasticity so that the resulting policy does not perform overly well,
which would lead to an uninteresting value function.
Figure 17: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the
fast part of the TTN.
Cartpole
Lambda
(a)
(b)
(c)
least-squares methods
Figure 19: a) Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 18: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison of
(b)
25
Published as a conference paper at ICLR 2019
C.5 Acrobot
In the classic Acrobot domain, the agent consisting of two links has to swing up past a certain height.
The agent observes a 4-dimensional state consisting of the angles and the angular velocities of each
link. The avaiable actions are three possible levels of torque to be applied to the joint.
The evaluated policy is obtained by training an agent with true-online Sarsa on a tile coding represen-
tation and then fixing its learned epsilon-greedy policy.
Figure 20: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the
fast part of the TTN.
Acrobot
Lambda
(a)
(b)
ACrobot
Number of Steps
(C)
Figure 21: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison of
least-squares methods
(a)
Figure 22: a) Comparison of Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
26
Published as a conference paper at ICLR 2019
C.6 Puck World
In Puck World (Tasfi, 2016), the agent has to move in a two-dimensional box towards a good puck
while staying away from a bad puck. The 8-dimensional state consists of (player x location, player y
location, player x velocity, player y velocity, good puck x location, good puck y location, bad puck x
location, bad puck y location). Each action increases the agent’s velocity in one of the four cardinal
directions apart from a “None” action which does nothing. The reward is the negative distance to the
good puck plus a penalty of -10 + x if the agent is within a certain radius of the bad puck, where
x ∈ [-2, 0] depends on the distance to the bad puck (the reward is slightly modified from the original
game to make the value function more interesting).
The policy moves the agent towards the good puck, while having a soft cap on the agent’s velocity. In
more detail, to choose one action, it is defined by the following procedure: First, we choose some
eligible actions. The None action is always eligible. The actions which move the agent towards the
good puck are also eligible. For example, if the good puck is Northeast of the agent, the North and
East actions are eligible. If the agent’s velocity in a certain direction is above 30, then the action for
that direction is no longer eligible. Finally, the agent picks uniformly at random from all eligible
actions.
Figure 23: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for the
fast part of the TTN.
(a)
(b)
(c)
(a)
Figure 25: a) Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 24: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison of
least-squares methods
(b)
27
Published as a conference paper at ICLR 2019
C.7 Off-policy Catcher
We run a preliminary experiment to check if TTN can have an advantage in the off-policy setting. The
target policy is the same as the one used for other Catcher experiments (described in Appendix D).
The behaviour policy is slightly different. If the apple is within 20 units (the target policy is 25 units),
then the agent takes the action in the direction of the apple with probability 0.7 and one of the other
two actions with probability 0.15 each. If the apple is not within range, then the agent takes the
None action 10% of the time and one of the other two with equal probability. This combination of
behaviour and target policies results in importance sampling ratios in the range of 0 to 8.7, moderately
large values.
We try TTN with three off-policy algorithms (TD, TDC and LSTD) and compare to off-policy
Nonlinear TD. For TTN, the features are learnt optimizing the MSTDE on the behaviour policy while
the values are learned off-policy. The main difference between TTN and Nonlinear TD is the fact that
Nonlinear TD does off-policy updates to the entire network while TTN only changes the linear part
with off-policy updates.
Figure 26: Comparison of TTN and nonlinear TD
From figure C.7, we see that TTN can outperform nonlinear TD in terms of average error and also
has reduced variance (smaller error bars). This seems to suggest that the TTN architecture can grant
additional stability in the off-policy setting.
D Experimental details
LSTD This version of LSTD was used in the policy evaluation experiments.
Algorithm 4 Incremental LSTD algorithm
Inputs: ηinv , λ, w1 (initial weight vector)
Initialization: A1 = diag(ηinv) ∈ Rd×d, b1 = 0 ∈ Rd, z1 = 0 ∈ Rd , dis the number of features
For each transition (St, Rt+1, St+1) do:
∆t = (x(St) - γx(St+1))T
t+1	Atzt∆tAt
At+1 = T(At- t + ∆tAtZt)
bt+ι = t + ι(zt * rt - bt)
wt+1 = At+1bt+1
zt+1 = λγzt + x(St)
Description of policies The policy evaluated in Puddle World randomly took the north and east
actions with equal probability, with episodes initialized in the southwest corner. The policy evaluated
28
Published as a conference paper at ICLR 2019
in Catcher increased the velocity in the direction of the apple if the agent was within 25 units or else
chose the “None” action with a 20% chance of a random choice.
Competitor details Nonlinear TD uses the semi-gradient TD update with nonlinear function
approximation. This is known not to be theoretically-sound and there exist counterexamples where
the weights and predictions diverge (Tsitsiklis and Van Roy, 1997). Nevertheless, since this is the
simplest algorithm, we use this as a baseline. Nonlinear GTD, conversely, has proven convergence
results as it is an extension of the gradient TD methods to nonlinear function approximation.
Castro et al. proposed three variants of adaptive bases algorithms, each optimizing a different
objective. We include two of them: ABTD, which is based on the plain TD update and ABBE, which
is derived from the MSTDE. We omit ABPBE, which optimizes the MSPBE, since the algorithm is
computationally inefficient, requiring O(d2m) memory, where d is the number of features in the last
layer and m is the number of parameters for the bases (ie. weights in the neural network). Also, the
derivation is similar in spirit to that of nonlinear GTD, so we would expect both to perform similarly.
Levine et al.’s algorithm was proposed for the control setting, combining DQN with periodic LSTD re-
solving for the last layer’s weights. We adapt their idea for policy evaluation by adding regularization
to the last layer’s weights to bias them towards the LSTD solution and using semi-gradient TD on this
objective. Then, we can then train in a fully online manner, which makes the algorithm comparable
to the other competitors. This algorithm is labeled as Nonlinear TD-Reg.
Hyperparameters Here we present the refined hyperparameter ranges that were swept over for the
experimental runs.
First, we outline the hyperparameters of all the algorithms and, afterwards, we give the values tested
for each hyperparameter. There is one range for Puddle World and another for Catcher (both versions)
since different ranges worked well for each domain. For the two-timescale networks, for each
environment, the learning rate for the slow part was set to a fixed value which was sufficiently low for
the fast part to do well but was not tuned extensively. This was done for all the experiments except
for those on surrogate losses, where we swept over a range of values (since different losses could
have different optimization properties).
Two-timescale Networks:
For all algorithms, the learning rate for the slow part αslow .
TD, ETD, TOTD, TOETD : learning rate α, trace parameter λ
TDC : primary weights learning rate α, secondary weights learning rate β, trace parameter λ
LSTD: initializer for the A-1 matrix ηinv , trace parameter λ
FLSTD: initializer for the A matrix η, learning rate α, forgetting rate ψ, trace parameter λ
Castro MSTDE, Castro TD: final layer learning rate α, other layers learning rate β
Nonlinear GTD : primary weights learning rate α, secondary weights learning rate β
Nonlinear TD : learning rate α, trace parameter λ
Nonlinear TD - Reg : learning rate α, learning rate towards LSTD solution (regularization) β,
initializer for the A matrix η
For all algorithms that used eligibility traces, the range swept over was λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1}
The other hyperparameters are shown individually below.
Two-timescale Networks:
-2.5
αslow = 10
TD, ETD, TOTD, TOETD:
α ∈ 10c, c∈ {-3.5, -3.25,..., -1.25}
TDC:
α ∈ 10c, c∈ {-4, -2.75,..., -1.25}
29
Published as a conference paper at ICLR 2019
	β ∈ 10c, C ∈ {-3, -2}
LSTD:	ηinv ∈ 10c, c ∈ {-2,-1.75,..., 0.75}
FLSTD:	η ∈ 10c, c ∈ {-3, -2,-1, 0} α ∈ 10c, c ∈ {-4, -3} ψ ∈ 10c, c ∈ {-5, -4, -3}
Castro MSTDE, Castro TD:	α ∈ 10c, c ∈ {-6, -5.75,..., -4} β ∈ 10c, C ∈ {-6, -5.75,..., -4}
Nonlinear GTD:	α ∈ 10c, C ∈ {-3, -2.75,..., -0.25} β ∈ 10c, C ∈ {-5, -4.5,..., -0.5}
Nonlinear TD:	α ∈ 10c, C ∈ {-4, -3.75,..., -1.25}
Nonlinear TD - Reg:	α ∈ 10c, c ∈ {-6, -5.5,..., -3} β ∈ 10c, c ∈ {-3.5, -3,..., -0.5} η ∈ 10c, c ∈ {-2, -1,0,1}
30
Published as a conference paper at ICLR 2019
Control experiments For both control experiments, we modified the Catcher environment slightly
from its default settings. The agent is given only 1 life and an episode terminates after it fails to catch
a single apple. The reward for catching an apple is +1 and is -1 at the end of an episode. The discount
factor is set to 0.99.
For image-based Catcher, we stack two consecutive frames which we treat as the state. This is done
so that the agent can perceive movements in the paddle, thus making the state Markov.
Both DQN and TTN use an -greedy policy. For DQN, is annealed from 1.0 to 0.01 (0.1) for
nonimage (image) Catcher over a certain number of steps. For TTN, is fixed to a constant value,
0.01 (0.1) for nonimage (image) Catcher.
For both algorithms, we use a replay buffer of size 50000 (200000) for nonimage (image) Catcher.
The buffer is initialized with 5000 (50000) transitions from a random policy. The minibatch size for
DQN and feature learning was 32. TTN uses the AMSGrad optimizer with β1 = 0 and β2 = 0.99
and DQN uses the ADAM optimizer with default settings, β1 = 0.9, β2 = 0.999.
For image catcher, due to the long training times, the hyperparameters were manually tuned. For
nonimage catcher, we concentrated our hyperparameter tuning efforts on the most important ones
and use the following strategy. We first used a preliminary search to find a promising range of values,
followed by a grid search. For TTN with FQI, we focused tuning on the step sizes for the features
and the regularization factor.For DQN, we focused on adjusting the step size and the number of steps
over which , the probability of picking a random action, was annealed. The other hyperparameters
were left to reasonable values.
The final hyperparameters for nonimage catcher:
TTN — αslow = 10-3, λreg = 10-2 (regularization towards previous weights)
DQN — α = 10-3.75, decaying over 20 thousand steps, update the target network every 1000 steps.
For LS-DQN, do a FQI upate every 50,000 steps with a regularization weight of 1.
The final hyperparameters for image catcher:
TTN — αslow = 10-5, λreg = 10-3 (regularization towards previous weights), solve for new
weights using FQI every 10,000 steps.
DQN — α = 10-3.75, decaying over 1 million steps, update the target network every 10,000 steps.
For LS-DQN, do a FQI update every 500,000 steps with a regularization weight of 1.
31
Published as a conference paper at ICLR 2019
Network architectures
For TTN, this describes the neural network which learns the features. The layer marked as “features"
is used to predict state-action values for the fast, linear part.
Image catcher - TTN
To predict the next state, we add an action embedding as done in (Oh et al., 2015).
Input	2x64x64 grayscale image (stacked frames)
Conv1	一	32 filters, 7x7 kernel, stride 4, ReLU
Conv2	64 filters, 5x5 kernel, stride 2, ReLU
Conv3	64 filters, 3x3 kernel, stride 1, ReLU
DenseI (features)	256 units, ReLU	一
From Dense1, Dense2 (reward prediction)	3 units (one per action), Linear
From Dense1, add action embedding	256 units, Linear
Reshape	Into 8x8 image
Deconv1	16 filters, 5x5 kernel, stride 2, ReLU
Deconv2	16 filters, 5x5 kernel, stride 2, ReLU
Deconv3	16 filters, 5x5 kernel, stride 2, ReLU
Conv4 (state prediction)	1 filter, 3x3 kernel, stride 1, Linear
Image catcher - DQN
Input	2x64x64 grayscale image (stacked frames)
Conv1	32 filters, 7x7 kernel, stride 4, ReLU
Conv2	64 filters, 5x5 kernel, stride 2, ReLU
Conv3	64 filters, 3x3 kernel, stride 1, ReLU
Dense1	256 units, ReLU	一
Dense2 (Q-values)	3 units, Linear
Nonimage catcher - TTN
Input	4-dimensional state
Dense1	128 units, ReLU
Dense2	128 units, ReLU
Dense3 (features)	128 units, ReLU
Dense4 (state and reward prediction)	15 units, Linear
Nonimage catcher - DQN
Input	4-dimensional state
Dense1	128 units, ReLU-
Dense2	128 units, ReLU
Dense3	128 units, ReLU
Dense4 (Q-values)	3, Linear
32