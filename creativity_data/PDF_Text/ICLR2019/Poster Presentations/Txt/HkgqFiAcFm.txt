Published as a conference paper at ICLR 2019
Marginal Policy Gradients: A Unified Family
of Estimators for B ounded Action Spaces with
Applications
Carson Eisenach 1,*##, Haichuan Yang2,*#, Ji Liu2,3#, and Han Liu4#
1 Department of Operations Research and Financial Engineering, Princeton University,
Princeton, NJ 08544.
2Department of Computer Science, University of Rochester, Rochester, NY 14627.
3Kwai AI Lab at Seattle, Seattle, WA.
4Department of Electrical Engineering and Computer Science, Northwestern University,
Evanston, IL 60208.
Ab stract
Many complex domains, such as robotics control and real-time strategy (RTS)
games, require an agent to learn a continuous control. In the former, an agent learns
a policy over Rd and in the latter, over a discrete set of actions each of which is
parametrized by a continuous parameter. Such problems are naturally solved using
policy based reinforcement learning (RL) methods, but unfortunately these often
suffer from high variance leading to instability and slow convergence. Unnecessary
variance is introduced whenever policies over bounded action spaces are modeled
using distributions with unbounded support by applying a transformation T to
the sampled action before execution in the environment. Recently, the variance
reduced clipped action policy gradient (CAPG) was introduced for actions in
bounded intervals, but to date no variance reduced methods exist when the action
is a direction, something often seen in RTS games. To this end we introduce the
angular policy gradient (APG), a stochastic policy gradient method for directional
control. With the marginal policy gradients family of estimators we present a
unified analysis of the variance reduction properties of APG and CAPG; our results
provide a stronger guarantee than existing analyses for CAPG. Experimental results
on a popular RTS game and a navigation task show that the APG estimator offers a
substantial improvement over the standard policy gradient.
1	Introduction
Recent work in deep reinforcement learning (RL) has achieved human level-control for complex
tasks like Atari 2600 games and the ancient game of Go. Mnih et al. (2015) show that it is possible to
learn to play Atari 2600 games using end to end reinforcement learning. Other authors (Silver et al.,
2014) derive algorithms tailored to continuous action spaces, such as appear in problems of robotics
control. Today, solving RTS games is a major open problem in RL (Foerster et al., 2016; Usunier
et al., 2017; Vinyals et al., 2017); these are more challenging than previously solved game domains
because the action and state spaces are far larger. In RTS games, actions are no longer chosen from a
relatively small discrete action set as in other game types. Neither is the objective solely learning
a continuous control. Instead the action space typically consists of many discrete actions each of
which has a continuous parameter. For example, a discrete action in an RTS game might be moving
the player controlled by the agent with a parameter specifying the movement direction. Because
the agent must learn a continuous parameter for each discrete action, a policy gradient method is
a natural approach to an RTS game. Unfortunately, obtaining stable, sample-efficient performance
from policy gradients remains a key challenge in model-free RL.
*These authors contributed equally.
^ This work was done while at the TenCent AI Lab, Bellevue, WA 98004.
^ Correspondence to: eisenach@princeton.edu.
1
Published as a conference paper at ICLR 2019
Just as robotics control tasks often have actions restricted to a bounded interval, Multi-player Online
Battle Arena (MOBA) games, an RTS sub-genre, often have actions restricted to the unit sphere which
specify a direction (e.g. to move or attack). The current practice, despite most continuous control
problems having bounded action spaces, is to use a Gaussian distribution to model the policy and then
apply a transformation T to the action a before execution in the environment. This support mismatch
between the sampling action distribution (i.e. the policy π), and the effective action distribution
can both introduce bias to and increase the variance of policy gradient estimates (Chou et al., 2017;
Fujita & Maeda, 2018). For an illustration of how the distribution over actions a is transformed under
T(a) = a/||a||, see Figure 1 in Section 3.
In this paper, motivated by an application to a MOBA game, we study policy gradient methods
in the context of directional actions, something unexplored in the RL literature. Like CAPG for
actions in an interval [α, β], our proposed algorithm, termed angular policy gradient (APG), uses a
variance-reduced, unbiased estimated of the true policy gradient. Since the key step in APG is an
update based on an estimate of the policy gradient, it can easily be combined with other state-of-the art
methodology including value function approximation and generalized advantage estimation (Sutton
et al., 2000; Schulman et al., 2016), as well as used in policy optimization algorithms like TRPO,
A3C, and PPO (Schulman et al., 2015; Mnih et al., 2016; Schulman et al., 2017).
Beyond new methodology, we also introduce the marginal policy gradients (MPG) family of estima-
tors; this general class of estimators contains both APG and CAPG, and we present a unified analysis
of the variance reduction properties of all such methods. Because marginal policy gradient methods
have already been shown to provide substantial benefits for clipped actions (Fujita & Maeda, 2018),
our experimental work focuses only on angular actions; we use a marginal policy gradient method to
learn a policy for the 1 vs. 1 map of the King of Glory game and the Platform2D-v1 navigation task,
demonstrating improvement over several baseline policy gradient approaches.
1.1	Related Work
Model-Free RL. Policy based methods are appealing because unlike value based methods they
can support learning policies over discrete, continuous and parametrized action spaces. It has long
been recognized that policy gradient methods suffer from high variance, hence the introduction of
trust region methods like TRPO and PPO (Schulman et al., 2015; 2017). Mnih et al. (2016) leverage
the independence of asynchronous updating to improve stability in actor-critic methods. See Sutton
& Barto (2018) for a general survey of reinforcement learning algorithms, including policy based
and actor-critic methods. Recent works have applied policy gradient methods to parametrized action
spaces in order to teach an agent to play RoboCup soccer (Hausknecht & Stone, 2016; Masson et al.,
2016). Formally, a parametrized action space A over K discrete, parametrized actions is defined as
A := Uk{(k, ω) : ω ∈ Ωk}, where k ∈ [K] and Ωk is the parameter space for the kth action. See
Appendix B.5 for rigorous discussion of the construction of a distribution over parametrized action
spaces and the corresponding policy gradient algorithms.
Bounded Action Spaces. Though the action space for many problems is bounded, it is nonetheless
common to model a continuous action using the multivariate Gaussian, which has unbounded support
(Hausknecht & Stone, 2016; Florensa et al., 2017; Finn et al., 2017). Until recently, the method for
dealing with this type of action space was to sample according to a Gaussian policy and then either
(1) allow the environment to clip the action and update according to the unclipped action or (2) clip
the action and update according to the clipped action (Chou et al., 2017). The first approach suffers
from unnecessarily high variance, and the second approach is off-policy.
Recent work considers variance reduction when actions are clipped to a bounded interval (Chou et al.,
2017; Fujita & Maeda, 2018). Depending upon the way in which the Q-function is modeled, clipping
has also been shown to introduce bias (Chou et al., 2017). Previous approaches are not applicable to
the case when T is the projection onto the unit sphere; in the case of clipped actions, unlike previous
work, we do not require that each component of the action is independent and obtain much stronger
variance reduction results. Concurrent work (Fellows et al., 2018) also considers angular actions, but
their method cannot be used as a drop in replacement in state of the art methods and the a special
form of the critic qπ is required.
Integrated Policy Gradients. Several recent works have considered, as we do, exploiting an
integrated form of policy gradient (Ciosek & Whiteson, 2018; Asadi et al., 2017; Fujita & Maeda,
2
Published as a conference paper at ICLR 2019
2018; Tamar et al., 2012). Ciosek & Whiteson (2018) introduces a unified theory of policy gradients,
which subsumes both deterministic (Silver et al., 2014) and stochastic policy gradients (Sutton et al.,
2000). They characterize the distinction between different policy gradient methods as a choice
of quadrature for the expectation. Their Expected Policy Gradient algorithm uses a new way of
estimating the expectation for stochastic policies. They prove that the estimator has lower variance
than stochastic policy gradients. Asadi et al. (2017) propose a similar method, but lack theoretical
guarantees. Fujita & Maeda (2018) introduce the clipped action policy gradient (CAPG) which is
a partially integrated form of policy gradient and provide a variance reduction guarantee, but their
result is not tight. By viewing CAPG as a marginal policy gradient we obtain tighter results.
Variance Decomposition. The law of total variance, or variance decomposition, is given by
Var[Y] = E[Var(Y |X)] + Var[E[Y |X]], where X and Y are two random variables on the same
probability space. Our main result can be viewed as a special form of law of total variance, but it
is highly non-trivial to obtain the result directly from the law of total variance. Also related to our
approach is Rao-Blackwellization (Blackwell, 1947) of a statistic to obtain a lower variance estimator.
2	Preliminaries
Notation and Setup. For MDP’s we use the standard notation. S is the state space, A is the
action space, p denotes the transition probability kernel, p0 the initial state distribution, r the reward
function. A policy π(a∣s) is a distribution over actions given a state S ∈ S. A sample trajectory
under π is denoted τ∏ := (s0,a0,r1, S1,a1,...) where s° 〜 po and at 〜 π(∙∣st). The state-
value function is defined as vπ(s) := Eπ[Pt∞=0 γtrt+1 |s0 = s] and the action-value function as
qπ(s, a) := Eπ [Pt∞=0 γtrt+1 |s0 = s, a0 = a]. The objective is to maximize expected cumulative
discounted reward, η(π) = Ep0 [vπ(s0)]. ρπ denotes the improper discounted state occupancy
distribution, defined as ρπ := Pt γtEp0 [P(st = s|s0, π)]. We make the standard assumption of
bounded rewards.
We consider the problem of learning a policy π parametrized by θ ∈ Θ. All gradients are with respect
to θ unless otherwise stated. By convention, We define 0 ∙ ∞ = 0 and 0 = 0. A measurable space
(A, E ) is a set A with a sigma-algebra E of subsets of A. When we refer to a probability distribution
of a random variable taking values in (A, E) we will work directly with the probability measure on
(A, E ) rather than the underlying sample space. For a measurable mapping T from measure space
(A, E, λ) to measurable space (B, F), we denote by Tn the push-forward of λ. Sd-1 denotes the
unit sphere in Rd and for any space A, B(A) denotes the Borel σ-algebra on A. The notation μ《V
signifies the measure μ is absolutely continuous with respect to ν. The function clip is defined as
clip(a, α, β) = min(β, max(α, a)) for a ∈ R. If a ∈ Rd, it is interpreted element-wise.
Variance of Random Vectors. We define the variance of a random vector y as Var(y) = E[(y -
Ey)> (y - Ey)], i.e. the trace of the covariance of y; it is easy to verify standard properties of
the variance still hold. This definition is often used to analyze the variance of gradient estimates
(Greensmith et al., 2004).
Stochastic Policy Gradients. In Section 4 we present marginal policy gradient estimators and work
in the very general setting described below. Let (A, E, μ) be a measure space, where as before A is
the action space of the MDPIn practice, we often encounter (A, E) = (Rd, B(Rd)) with μ as the
Lebesgue measure. The types of policies for which there is a meaningful notation of stochastic policy
gradients are μ-compatible measures (see remarks 2.3 and 2.4).
Definition 2.1 (μ-Compatible Measures). Let (A, E,μ) be a measure space and consider a
parametrized family of measures Π = {∏(∙, θ) : θ ∈ Θ} on the same space. Π is a μ-compatible
family of measures if for all θ:
(a)	π(∙, θ)《μ with density of the form f∏ (∙,θ),
(b)	fπ is differentiable in θ, and
(c)	π satisfies the conditions to apply the Leibniz integral rule for each θ, so that
▽ RA f∏(a)dμ = RA ▽ f∏ (a)dμ.
For μ-compatible policies, Theorem 2.2 gives the stochastic policy gradient, easily estimable from
samples. When μ is the counting measure we recover the discrete policy gradient theorem (Sutton
3
Published as a conference paper at ICLR 2019
et al., 2000). See Appendix A.1 for a more in depth discussion and a proof of Theorem 2.2, which we
include for completeness.
Theorem 2.2 (Stochastic Policy Gradient). Let (A, E, μ) be a measure space and let Π = {∏(∙, θ∣s):
θ ∈ Θ} be a family of μ-compatible probability measures. Denoting by f∏ the density with respect
to μ, We have that
▽n
dρπ(s)
q∏(s, a)Vlogf∏(a∣s)dπ(∙∣s).
In general we want an estimate g of Vn such that it is unbiased (E[g] = Vn) and that has minimal
variance, so that convergence to a (locally) optimal policy is as fast as possible. In the following
sections, we explore a general approach to finding a low variance, unbiased estimator.
Remark 2.3. Under certain choices of T (e.g. clipping) the effective action distribution is a mixture
of a continuous distribution and point masses. Thus, although it adds some technical overhead, it is
necessary that we take a measure theoretic approach in this work.
Remark 2.4. Definition 2.1 is required to ensure the policy gradient is well defined, as it stipulates
the existence of an appropriate reference measure; it also serves to clarify notation and to draw a
distinction between π and its density fπ . Though these details are often minimized they are important
in analyzing the interaction between T and π.
3 Angular Policy Gradients
Consider the task of learning a policy over directions in A = R2, or equivalently learning a policy over
angles [0, 2π). A naive approach is to fit the mean mθ(s), model the angle as normally distributed
about mθ , and then clip the sampled angle before execution in the environment. However, this
approach is asymmetric in that does not place similar probability on mθ(s) - and mθ(s) + for
mθ(s) near to 0 and 2π.
An alternative is to model mg (S) ∈ R2, sample a 〜N(mθ (s), Σ), and then execute T(a) := α∕∣∣α∣∣
in the environment. This method also works for directional control in Rd . The drawback of this
approach is the following: informally speaking, we are sampling from a distribution with d degrees
of freedom, but the environment is affected by an action with only d - 1 degrees of freedom. This
suggests, and indeed we later prove, that the variance of the stochastic policy gradient for this
distribution is unnecessarily high. In this section we introduce the angular policy gradient which can
be used as a drop-in replacement for the policy update step in existing algorithms.
Angular Actions: Density of π
Angular Actions: T(a) = α∕∣∣ɑ∣∣
Angular Actions: Density of T*π
Figure 1: Transformation of a Gaussian policy - (left to right) π(∙∣s), T = a∕∣∣a∣∣, and T*π(∙∣s).
Angular Gaussian Distribution
Instead, we can directly model T(a) ∈ Sd-1 instead of a ∈ Rd. If a 〜N(mg(s), Σg(s)), then T(a)
is distributed according to what is known as the angular Gaussian distribution (Definition 3.1). It can
be derived by a change of variables to spherical coordinates, followed by integration with respect to
4
Published as a conference paper at ICLR 2019
the magnitude of the random vector (Paine et al., 2018). Figure 1 illustrates the transformation of a
Gaussian sampling policy π under T .
Definition 3.1 (Angular Gaussian Distribution). Let a 〜N(m, Σ). Then, with respect to the
spherical measure σ on (Sd-1, B(Sd-1)), x = a/||a|| has density
f(x; m, Σ) = ((2π)d-1∣∑∣(x>∑-1x)d) 1/2 exp (； (α2 — m>Σ-1m)) Md-ι(α),	(3.1)
where α = (χ>∑-ιX 1m/2 and Md-1(x) = (2π)- 1 R∞ ud-1 exp(-(u — x)2∕2)du.
Policy Gradient Method
Although the density in Definition 3.1 does not have a closed form, we can still obtain a stochastic
policy gradient for this type of policy. Define the action space as A := Sd-1 and consider angular
Gaussian policies parametrized by θ := (θm, θΣ), where θm parametrizes m and θΣ parametrizes Σ.
As before, denote the corresponding parametrized family of measures as Π := {∏(∙,θ∣s) : θ ∈ Θ}.
Directly from Definition 3.1, we obtain
log f∏ = 2 (α2 - m>Σ-1m) + logMd-1(α) — 2 [(d - 1)log 2π + log ∣Σ∣ + dlog (x>Σx)].
Though this log-likelihood does not have a closed form, it turns out it is easy to compute the gradient
in practice. It is only necessary that we can evaluate M0d-1(α) and Md(α) easily. Assuming for
now that we can do so, denote by θi the parameters after i gradient updates and define
li(θ):= K (α2 — m>Σ 1m) +	—— α — - [(d — 1)log2π + log ∣Σ∣ + dlog (x>Σx)].
2	Md-1(α(θi))	2
X-----------}
{z
(i)
By design,
▽ log f∏ (θ)∣θ=θi = Vli(θ)∣θ=θi,
thus at update i it suffices to compute the gradient of li , which can be done using standard auto-
differentiation software (Paszke et al., 2017) since term (i) is a constant. From Paine et al. (2018), we
have that M0d(α) = dMd-1 (α), Md+1 (α) = αMd(α) + dMd-1 (α), M1 (α) = αΦ(α) + φ(α)
and M0 (α) = Φ(α), where Φ, φ denote the PDF and CDF of N(0, 1), respectively. Leveraging
these properties, the integral Md(α) can be computed recursively; Algorithm 1 in Appendix B.1
gives psuedo-code for the computation. Importantly it runs in O(d) time and therefore does not effect
the computational cost of the policy update since it is dominated by the cost of computing Vli . In
addition, stochastic gradients of policy loss functions for TRPO or PPO Schulman et al. (2015; 2017)
can be computed in a similar way since we can easily get the derivative of fπ(θ) when Md-1(α)
and M0d-1(α) are known.
4 Marginal Policy Gradient Estimators
In Section 2, we described a general setting in which a stochastic policy gradient theorem holds on a
measure space (A, E, λ) for a family of λ-compatible probability measures, Π = {∏(∙, θ∣s) : θ ∈ Θ}.
As before, we are interested in the case when the dynamics of the environment only depend on a ∈ A
via a function T. That is to say r(s, a) := r(s, T (a)) and p(s, a, s0) := p(s, T (a), s0).
The key idea in Marginal Policy Gradient is to replace the policy gradient estimate based on the
log-likelihood of ∏ with a lower variance estimate, which is based on the log-likelihood of T1√∏. T*∏
can be thought of as (and in some cases is) a marginal distribution, hence the name Marginal Policy
Gradient. For this reason it can easily be used with value function approximation and GAE, as well
as incorporated into algorithms like TRPO, A3C and PPO.
4.1	Setup and Regularity Conditions
For our main results we need regularity Condition 4.1 on the measure space (A, E, λ). Next, let
(B, F) be another measurable space and T : A → B be a measurable mapping. T induces a family of
5
Published as a conference paper at ICLR 2019
probability measures on (B, F), denoted T*Π := {T*∏(∙, θ∣s) : θ ∈ Θ}. We also require regularity
Conditions 4.2 and 4.3 regarding the structure of F and the existence of a suitable reference measure
μ on (B, F). These conditions are all quite mild and are satisfied in all practical settings, to the best
of our knowledge.
Condition 4.1 . A is a metric space and λ is a Radon measure.1
Condition 4.2 . F is countably generated and contains the singleton sets {b}, for all b ∈ B.
Condition 4.3 . There exists a σ-finite measure μ on (B, F) such that TA《μ and T*Π is μ-
compatible.
In statistics, Fisher information is used to capture the variance of a score function. In reinforcement
learning, typically one encounters a score function that has been rescaled by a measurable function
q(a). Definition 4.4 provides a variant of Fisher information for λ-compatible distributions and
rescaled score functions; we defer a discussion of the definition until Section 4.4 after we present our
results in their entirety. If q(a) = 1, Definition 4.4 is the trace of the classical Fisher Information.
Definition 4.4 (Total Scaled Fisher Information). Let (A, E, λ) be a measure space, Π = {∏(∙, θ):
θ ∈ Θ} be a family of λ-compatible probability measures, and q a measurable function on E. The
total scaled fisher information is defined as I∏,λ(q, θ) := E[q(a)2Vlog f∏(a)τV log f∏(a)].
4.2	Variance Reduction Guarantee
From Theorem 2.2 it is immediate that
Vη(θ) = / dρ(s) / q(T(a),s)VTogfn(a∣s)dπ(a∣s)
=Js dρ(s) JB q(b, s)Vlogfτ*∏(b∣s)d(T*∏)(b∣s),
where we dropped the subscripts on ρ and q because the two polices affect the environment in the same
way, and thus have the same value function and discounted state occupancy measure. Denote the two
alternative gradient estimators as gι = q(T(a), s)Vlog f∏(a|s) and g2 = q(b, s)Vlogfτ*∏(b|s).
Just by definition, We have that Eρ,∏ [gι] = Ep∏ [g2]. Lemma 4.5 says something slightly different 一
it says that they are also equivalent in expectation conditional on the state s, a fact we use later.
Lemma 4.5. Let (A, E, λ) and (B, F, μ) be measure spaces, and T : A → B be measurable mapping.
If Π, parametrized by θ, is λ-compatible and T*Π is μ-compatible, then
Eπ∣s [gl] = En|s [g2] = ET*π∣s [g2] ∙
(4.1)
Proof. The result follows immediately from the proof of Theorem 2.2 in Appendix A.1.	□
Because the tWo estimates g1 and g2 are both unbiased, it is alWays preferable to use Whichever
has lower variance. Theorem 4.6 shows that g2 is the lower variance policy gradient estimate. See
Appendix B.3 for the proof. The implication of Theorem 4.6 is that if there is some information loss
via a function T before the action interacts with the dynamics of the environment, then one obtains a
lower variance estimator of the gradient by replacing the density of ∏ with the density of T*∏ in the
expression for the policy gradient.
Theorem 4.6. Let g1 and g2 be as defined above. Then if Conditions 4.1-4.3 are satisfied,
Varp,π(gι) - Varp,τ*π(g2) = Ep,τ*∏ [I∏∣s∣b,λb(q ◦ T, θ)] ≥ 0,
for some family of measures {λb} on A.
4.3	Examples of Marginal Policy Gradient Estimators
Clipped Action Policy Gradient
Consider a control problem where actions in R are clipped to an interval [α, β]. Let λ be an arbitrary
measure on (A, E) := (R, B(R)), and consider any λ-compatible family Π. Following Fujita &
1On a metric space A, a Radon measure is a measure defined on the Borel σ-algebra for which each compact
K ⊂ A, λ(K) < ∞ and for all B ∈ B(A), λ(B) = supK⊆B λ(K) where K is compact.
6
Published as a conference paper at ICLR 2019
Maeda (2018), define the clipped score function
~	(Vlog R(-∞,α] f∏(a,θls)dλ b = α
ψe(s,b, θ) = V log f∏(b, θ∣s)	b ∈ (α,β)
IVlog R[β,∞) f∏(a,θ∣s)dλ	b = β.
We can apply Theorem 4.6 in this setting to obtain Corollary 4.7. It is a strict generalization of the
results in Fujita & Maeda (2018) in that it applies to a larger class of measures and provides a much
stronger variance reduction guarantee. It is possible to obtain this more powerful result precisely
because we require minimal assumptions for Theorem 4.6. Note that the result can be extended to Rd,
but we stick to R for clarity of presentation. See Appendix B.4 for a discussion of which distributions
are λ-compatible and a proof of Corollary 4.7.
Corollary 4.7. Let λ be an arbitrary measure on (A, E) := (R, B(R)), T (a) := clip(a, α, β), and
ψ(s, a, θ) := V log f∏ (a, θ∣s). If Π is a /-compatible family parametrized by θ and the dynamics of
the environment depend only on T (a), then
1.	E∏∣s [q∏(s,a)ψ(s,a,θ)]=
2.	Varρ,π(qπ(s, a)ψ(s, a, θ))
E∏∣s Iqn (s,a)ψe(s,T (a),。)], and
- Varρ,π (qπ (s, a)ψ (s, T (a), θ))
for some family of measures {λb} on A.
EP [Eτ*n|s [I∏∣s∣b,λb(q ◦ T,θ)]],
Angular Policy Gradient
Now consider the case where we sample an action a ∈ Rd and apply T(a) = a/||a|| to map into Sd-1.
Let (A, E) = (Rd, B(Rd)) and let λ be the Lebesgue measure. When Π is a multivariate Gaussian
family parametrized by θ, T*Π is an angular Gaussian family also parametrized by θ (Section 3). If
Π is λ-compatible - here it reduces to ensuring the parametrization is such that f∏ is differentiable in
θ 一 then T*Π is σ-compatible, where σ denotes the spherical measure. Denoting by ∕mv(a, θ∣s) and
fAG(b, θ∣s) the corresponding multivariate and angular Gaussian densities, respectively, We state the
results for this setting as Corollary 4.8. See Appendix B.4 for a proof.
Corollary 4.8. Let λ be the Lebesgue measure on (A, E) = (Rd, B(Rd)), T(a) := a/||a|| and Π
be a multivariate Gaussian family on A parametrized by θ. If the dynamics of the environment only
depend on T(a) and ∕mv(∙, θ∣s), the density corresponding to Π, is differentiable in θ, then
1.	E∏∣s [q∏(s,a)ψ(s,a,θ)]
E∏∣s Iqn (s,a)ψe(s,T (a),。)], and
〜
2.	Varρ,∏(q∏(s,a)ψ(s,a,θ)) - Varp,∏(q∏(s,a)ψ(s,T(a),θ)) = Eρ,τ*∏
Varn|b(qn(s, a)ψr(s, r, 。)) ,
where r = ||a||, f is the conditional density of r, ψ(s,a,θ) := V log ∕mv (a,θ∣s), ψ(s,b,θ)
V log fAG(b,θ∣s), and ψr (s,r,θ) = V log f (r,θ∣s).
Parametrized Action Spaces
As one might expect, our variance reduction result applies to parametrized action spaces when a lossy
transformation Ti is applied to the parameter for discrete action i. See Appendix B.5 for an in depth
discussion of policy gradient methods for parametrized action spaces.
4.4 Discussion
Denoting by g1 the standard policy gradient estimator for a λ-compatible family Π, observe that
Varρ,n(g1) = In,λ(q, 。). We introduce the quantity In,λ because unless T is a coordinate projection
it is not straightforward to write Theorem 4.6 in terms of the density of a conditional distribution.
Corollary 4.8 can be written this way because under a re-parametrization to polar coordinates,
T(a) = a/||a|| can be written as a coordinate projection. In general, by using In,λ we can phrase the
result in terms of a quantity with an intuitive interpretation: a (q-weighted) measure of information
contained in a that does not influence the environment.
Recalling the law of total variance (LOTV), we can observe that Theorem 4.6 is indeed specific
version of that general result. We can not directly apply the LOTV because in the general setting,
it is highly non-trivial to conclude that g2 is a version of the conditional expectation of g1 , and for
arbitrary policies, one must be extremely careful when making the conditioning argument (Chang
7
Published as a conference paper at ICLR 2019
& Pollard, 1997). However for certain special cases, like CAPG, we can check fairly easily that
g2 = E[g1|b].
5	Applications and Discussion
5.1	2D Navigation Task
Because relatively few existing reinforcement learning environments support angular actions, we
implement a navigation task to benchmark our methods2. In this navigation task, the agent is located
on a platform and must navigate from one location to another without falling off. The state space
is S = R2, the action space is A = R2 and the transformation T(a) = a/||a|| is applied to actions
before execution in the environment. Let sG = (1, 1) be the goal (terminal) state. Using the reward
shaping approach (Ng et al., 1999), we define a potential function φ(s) = ||s - sG||2 and a reward
function as r(st, at) = φ(st) - φ(st + at). The start state is fixed at s0 = (-1, -1). One corner of
the platform is located at (-1.5, -1.5) and the other at (1.5, 1.5).
We compare angular Gaussian policies with (1) bivariate Gaussian policies and (2) a 1-dimensional
Gaussian policy where we model the mean of the angle directly, treating angles that differ by 2π as
identical. For all candidate policies, we use A2C (the synchronous version of A3C (Mnih et al., 2016))
to learn the conditional mean m(s; θ) of the sampling distribution by fitting a feed-forward neural
network with tanh activations. The variance of the sampling distribution, σ2I, is fixed. For the critic
we estimate the state value function vπ(s), again using a feed-forward neural network. Appendix C.1
for details on the hyper-parameter settings, network architecture and training procedure.
5.2	Application - KING OF Glory
We implement a marginal policy gradient method for King of Glory (the North American release is
titled Arena of Valor) by Tencent Games. King of Glory has several game types and we focus on the
1v1 version. Our work here is one of the first attempts to solve King of Glory, and MOBA games in
general, using reinforcement learning. Similar MOBA games include Dota 2 and League of Legends.
Game Description. in King of Glory, players are divided into two “camps” located in opposite
corners of the game map. Each player chooses a “hero”, a character with unique abilities, and the
objective is to destroy the opposing team’s “crystal”, located at their game camp. The path to each
camp and crystal is guarded by towers which attack enemies when in range. Each team has a number
of allied “minions”, less powerful characters, to help them destroy the enemy crystal. Only the “hero”
is controlled by the player. During game play, heroes increase in level and obtain gold by killing
enemies. This allows the player to upgrade the level of their hero’s unique skills and buy improved
equipment, resulting in more powerful attacks, increased Hp, and other benefits. Figure 2 shows King
of Glory game play; in the game pictured, both players use the hero “Di Ren Jie”.
Formulation as an MDP. A is a parametrized action space with 7 discrete actions, 4 of which are
parametrized by ω ∈ R2 . These actions include move, attack, and use skills; a detailed description of
all actions and parameters is given in Table 3, Appendix C.2. in our setup, we use rules crafted by
domain experts to manage purchasing equipment and learning skills. The transformation T (a) =
a/||a|| is applied to the action parameter before execution in the environment, so the effective action
parameter spaces are S 1 .
Using information obtained directly from the game engine, we construct a 2701-dimensional state
representation. Features extracted from the game engine include hero locations, hero health, tower
health, skill availability and relative locations to towers and crystals - see Appendix C.2 for details
on the feature extraction process. As in Section 5.1, we define rewards using a potential function. in
particular we define a reward feature mapping ρ and a weighting vector w, and then a linear potential
function as φr(s) = wT ρ(s). information extracted by ρ includes hero health, crystal health, and
game outcome; see Table 5, Appendix C.2 for a complete description of w and ρ. Using φr, we can
define the reward as rt = φr(st) - φr(st-1).
2We have made this environment and the implementation used for the experiments available on-line. We
temporarily removed the link from this paper to preserve anonymity.
8
Published as a conference paper at ICLR 2019
Implementation. We implement the A3C algorithm, and model both the policy π and the value
function vπ using feed-forWard neural netWorks. See Appendix C.2 for more details on hoW We
model and learn the value function and policy. Using the setup described above, We compare:
1.	a standard policy gradient approach for parametrized action spaces, and
2.	a marginal (angular) policy gradient approach, adapted to the parametrized action space Where
Ti(a) = a/||a|| is applied to parameter i.
Additional details on both approaches can be found in Appendix B.5.
5.3	Results
Platform2D-v1
Policy
Angular GaUssian
MUltiVariate Gaussian
1D GaUssian
0	5000	10000	15000	20000	25000
Number of Updates
PIEM9tf①A'βwmuno
Gradient Variance vs.
Sampling Variance - Trained
8 9 0
一-I
King of Glory 1v1
Gradient Variance vs. Sampling Variance - Untrained
2 ∖	Policy
、	----Angular GauSSian
1 ∖	Multivariate GauSSian
0 12
- -
①Ouw.TeA Mou
Policy
Angular GaUssian
Multivariate GaUssian
O.
1000	2000	3000	4000
Number of Episodes
Figure 2: On top are results for Platform2D-v1; on bottom, results for King of Glory 1v1 and a
screenshot of game play.
For the navigation task, the top row of Figure 2 contains, from left to right, cumulative, discounted
reward trajectories, and two plots showing the variances of the competing estimators. We see that the
agent using the angular policy gradient converges faster compared to the multivariate Gaussian due to
the variance reduced gradient estimates. The second baseline also performs worse than APG, likely
due in part to the fact that the critic must approximate a periodic function. Only APG achieves the
maximum possible cumulative, discounted reward. On the King of Glory 1 vs. 1 task, the agent is
trained to play as the hero Di Ren Jie and training occurs by competing with the game’s internal AI,
also playing as Di Ren Jie. The bottom row of Figure 2 shows the results, and as before, the angular
policy gradient outperforms the standard policy gradient by a significant margin both in terms of win
percentage and cumulative discounted reward.
In addition, Figure 2 highlights the effects of Theorem 4.6 in practice. The plot in the center shows
the variance at the start of training, for a fixed random initialization, and the plot on the right shows
the variance for a trained model that converged to the optimal policy. The main difference between
the two settings is that the value function estimate vbπ is highly accurate for the trained model (since
both actor and critic have converged) and highly inaccurate for the untrained model. In both cases,
We See that the variance of the marginal policy gradient estimator is roughly 2 that of the estimator
using the sampling distribution.
5.4	Discussion
Motivated by challenges found in complex control problems, We introduced a general family of
variance reduced policy gradients estimators. This vieW provides the first unified approach to
9
Published as a conference paper at ICLR 2019
problems where the environment only depends on the action through some transformation T , and we
demonstrate that CAPG and APG are members of this family corresponding to different choices of T .
We also show that it can be applied to parametrized action spaces. Because thorough experimental
work has already been done for the CAPG member of the family (Fujita & Maeda, 2018), confirming
the benefits of MPG estimators, we do not reproduce those results here. Instead we focus on the case
when T(a) = a/||a|| and demonstrate the effectiveness of the angular policy gradient approach on
King of Glory and our own Platform2D-v1 environment. Although at this time few RL environments
use directional actions, we anticipate the number will grow as RL is applied to newer and increasingly
complex tasks like MOBA games where such action spaces are common. We also envision that our
methods can be applied to autonomous vehicle, in particular quadcopter, control.
References
Kavosh Asadi, Cameron Allen, Melrose Roderick, Abdel-Rahman Mohamed, George Konidaris, and
Michael Littman. Mean Actor Critic, 2017. arXiv:1709.00503.
David Blackwell. Conditional expectation and unbiased sequential estimation. Annals of Mathemati-
cal Statistics,18(1):105-110,1947.
J T Chang and D Pollard. Conditioning as disintegration. Statistica Neerlandica, 51(3):287-317,
1997.
Po-Wei Chou, Daniel Maturana, and Sebastian Scherer. Improving Stochastic Policy Gradients in
Continuous Control with Deep Reinforcement Learning using the Beta Distribution. In ICML,
2017.
Kamil Ciosek and Shimon Whiteson. Expected Policy Gradients for Reinforcement Learning, 2018.
arXiv:1801.03326.
Matthew Fellows, Kamil Ciosek, and Shimon Whiteson. Fourier Policy Gradients. In ICML, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation
of Deep Networks. In ICML, 2017.
Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic Neural Networks for Hierarchical Rein-
forcement Learning. In ICLR, 2017.
Jakob N Foerster, Yannis M Assael, Nando De Freitas, and Shimon Whiteson. Learning to Commu-
nicate with Deep Multi-Agent Reinforcement Learning. In NIPS, 2016.
Yasuhiro Fujita and Shin-Ichi Maeda. Clipped Action Policy Gradient. In ICML, 2018.
Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance Reduction Techniques for Gradient
Estimates in Reinforcement Learning. Journal of Machine Learning Research, 5:1471-1530, 2004.
Matthew Hausknecht and Peter Stone. Deep Reinforcement Learning In Parameterized Action Space.
In ICLR, 2016.
Diederik P Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimization. In ICLR,
2015.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and SePP Hochreiter. Self-Normalizing
Neural Networks. In NIPS, 2017.
Warwick Masson, Pravesh Ranchod, and George Konidaris. Reinforcement Learning with Parameter-
ized Actions. In AAAI, 2016. ISBN 9781577357605.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles
Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane
Legg, and Demis Hassabis. Human-level control through deeP reinforcement learning. Nature,
(518):529-533, 2015. doi: 10.1038/nature14236.
10
Published as a conference paper at ICLR 2019
Volodymyr Mnih, Adria PUigdomenech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timo-
thy P Lillicrap, David Silver, Koray Kavukcuoglu, Korayk@google Com, and Google Deepmind.
AsynchronoUs Methods for Deep Reinforcement Learning. In ICML, 2016.
Andrew Ng, Daishi Harada, and StUart RUssell. Policy invariance Under reward transformations:
Theory and application to reward shaping. In ICML, 1999.
P. J. Paine, S. P. Preston, M. Tsagris, and Andrew T. A. Wood. An elliptically symmetric angUlar GaUs-
sian distribution. Statistics and Computing, 28:689-697, 2018. doi:10.1007/s11222-017-9756-4.
Adam Paszke, Gregory Chanan, Zeming Lin, Sam Gross, Edward Yang, LUca Antiga, and Zachary
Devito. Automatic differentiation in PyTorch. In NIPS Workshop, 2017.
John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region
Policy Optimization. In ICML, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-Dimensional
Continuous Control Using Generalized Advantage Estimation. In ICLR, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov Openai. Proximal
Policy Optimization Algorithms, 2017. arXiv:1707.06347.
David Silver, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic
Policy Gradient Algorithms. In ICML, 2014.
Richard S Sutton and Andrew G Barto. Reinforcement learning: an introduction. 2018. ISBN
0262193981. doi: 10.1109/TNN.1998.712192.
Richard S Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. Policy Gradient Methods
for Reinforcement Learning with Function Approximation. In NIPS, 2000.
Aviv Tamar, Dotan Di Castro, and Ron Meir. Integrating a partial model into model free reinforcement
learning. Journal of Machine Learning Research, 13:1927-1966, 2012. ISSN 15324435.
Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala. Episodic Exploration for
Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks. In ICLR,
2017.
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle
Yeo, Alireza Makhzani, Heinrich Uttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen
Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado Van Hasselt, David Silver, Timothy
Lillicrap, Deepmind Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders
Ekermo, Jacob Repp, and Rodney Tsing Blizzard. StarCraft II: ANew Challenge for Reinforcement
Learning, 2017. arXiv:1708.04782.
11
Published as a conference paper at ICLR 2019
A Additional Preliminaries
This section contains additional preliminary material and discussion thereof.
A.1 Discussion - Stochastic Policy Gradients
We require a stochastic policy gradient theorem that can be applied to distributions on arbitrary
measurable spaces in order to rigorously analyze the Marginal Policy Gradients framework. Let the
notation be as in section 2. The first ingredient is Proposition A.1, which gives a very general form
of policy gradient, defined for an arbitrary probability measure.
Proposition A.1. [Ciosek & Whiteson (2018)] Let ∏(∙∣s) be a probability measure on (A, E), then
Vη= S
dρ∏(s) Vv∏(S) — I dπ(a∣s) Vq∏(s, a).
A
This is an important step towards the form of stochastic policy gradient theorem we need in order to
present our unified analysis that includes measures with uncountable support and also those which
do not admit a density with respect to Lebesgue measure - something frequently encountered in
practice. To obtain a stochastic policy gradient theorem from Proposition A.1 we simply need to
replace Vvπ(s) with an appropriate expression. As in ciosek & Whiteson (2018), we need to be able
to justify an interchange along the lines of
Vvπ = V/ dπ(a∣s)q∏(s, a) = / daVπ(a∣s)q∏(s, a) + / dπ(a∣s) Vq∏(s,a).	(A.1)
such an expression doesn’t make sense for arbitrary π, so we must be precise regarding the conditions
under which such an expression makes sense and the interchange is permitted, hence Definition 2.1.
Because we did not find a statement with the sort of generality we required in the literature, we give a
proof of our statement the stochastic policy gradient theorem, Theorem 2.2, below.
Proofof Theorem 2.2. The proof follows standard arguments. Because Π is μ-compatible We obtain
that
Vvπ
dπ(a∣s)q∏ (s, a)
V
A
I V [q∏(s,a)fn(a|s)] dμ
A
J q∏(s, a)Vf∏(a∣s)dμ + J Vq∏(s, a)dπ(a∣s).
The result now follows immediately from Proposition A.1.
□
A.2 Disintegration Theorems
The definitions and propositions below are from chang & Pollard (1997), which we include here for
completeness. Let (A, E, λ) be a measure space and (B, F) a measurable space. Let λ be a σ-finite
measure on E and μ be a σ-finite measure on F.
Definition A.2 ((T, μ)-disintegration, Chang & Pollard (1997)). The measure λ has a (T,μ)-
disintegration, denoted {λb} if for all nonnegative measurable f on A
•	λb is a σ-finite measure on E that is concentrated on Eb := {T = b} in the sense that
JAI [A \ Eb]dλb = 0 for μ-almost all b,
•	the function b → T-1(b) fdλb is measurable, and
•	JA fdλ = JB JT-i(b) fdλbdμ.
If μ = T λ, then we call λb a T-disintegration. With some additional assumptions, we have the
existence theorem given below.
12
Published as a conference paper at ICLR 2019
Proposition A.3 (Existence, Chang & Pollard (1997)). Let A be a metric space, λ be a σ-finite
Radon measure, and μ be a σ-finite measure such that Tn《μ. If F is countably generated and
contains the singleton sets {b}, then λ has a (T, μ)-disintegration. The measures {λb} are unique UP
to an almost-sure equivalence in that if {λ计 is another (T, μ)-disintegration, μ({b : λb = λ计)=0.
Lastly, we have ProPosition A.4 which characterizes the ProPerties of disintegrations and how they
relate to densities and Push-forward measures.
Proposition A.4 (Chang & Pollard (1997)). Let λ have a (T, μ)-disintegration {λb}, and let P be
absolutely continuous with respect to λ with a finite density r(a), where each of λ, μ and P is σ-finite.
Then
•	ρ has a (T, μ)-disintegration {ρb} where Pb《λb with density r(a),
•	T*ρ《μ with density rτ(b) := JT-i(b)r(a)dλb,
•	the measures {ρb} are finite for μ almost all b if and only if T*P is σ-finite,
•	the measures {ρb} are probabilities for μ almost all b if and only if μ = T*ρ, and
•	if T*ρ is σ-finite, then T*ρ({b : rT(b) = 0}) = 0 and T*ρ({b : rT(b) = ∞}) = 0. For
T*ρ-almost all b, the measures {eb} defined by
r(a)
J ɪ	f(a)dpb = J ɪ	f (a)rθ∣b(a)dλb and r0∣b(a):= I[0 < rτ(b) < ∞]^(b),
are probability measures that give a T -disintegration of P.
13
Published as a conference paper at ICLR 2019
B Theory and Methodology
This section contains additional theoretical and methodology results, including our crucial scaled
Fisher information decomposition theorem.
B.1	Angular Policy Gradient
Algorithm 1 shows how to compute Md (α), allowing us to easily find the angular policy gradient.
Algorithm 1 Computing Md(α) for Angular Policy Gradient
Input: d, α
Output: Md(α)
1:	Mo - Φ(α)
2:	Mi - αΦ(α) + φ(α)
3:	if d > 1 then
4:	for i = 2, . . . , d do
5:	Mi{—αMi-ι + dMi-2
6:	end for
7:	end if
8:	return Md
B.2	Fisher Information Decomposition
Using the disintegration results stated in Appendix A.2, we now can state and prove our key decom-
position result, Theorem B.1, used in the proof of our main result.
Theorem B.1 (Fisher Information Decomposition). Let (A, E, λ) be a measure space, (B, F) be a
measurable space, T : A → B be a measurable, surjective mapping, and q a measurable function on
F. Consider a λ-compatible family of probability measures Π = {∏(∙,θ) : θ ∈ Θ} on E and denote
T*Π := {T*∏(∙, θ) : θ ∈ Θ}, a family of measures on F. If
(a)	A is a metric space, λ is a Radon measure, and TA《 μ fora σ-finite measure μ on F;
(b)	F is countably generated and contains the singleton sets {b};
(C) T*Π is a μ-compatible family for a measure μ on F;
then
1.	λ has a (T, μ)-disintegration {λb};
2.	Π∣b is a λb-compatible family of probability measures that give a T-disintegration of ∏;
3.	for any measurable function q : B → R,
I∏,λ(q ◦ T,θ) = Eτ*∏ [I∏∣b,λb(q ◦ T, θ)] + Iτ*∏,μ(q,θ).
Proof of Theorem B.1. To simplify matters, we assume without loss of generality that all densities
are strictly positive. This is allowed because if some density is zero on part of its domain, we can just
replace the associated measure with its restriction to sets where the density is non-zero.
The conditions to apply Proposition A.3 are satisfied, so λ has a (T, μ)-disintegration {λb}, which
proves claim 1. Next, denote by g(a) = V log f∏ (a) and h(b) = V log fτ*∏ (b). Because the
14
Published as a conference paper at ICLR 2019
conditions to apply Proposition A.4 are satisfied, we obtain that
/ q(T(a))2g(a)>g(a)d∏(a) = / /	q(T(a))2g(a)>g(a)fθ∣b(a)dλb(a)dT*π(b)
q(b)2
B	T-1(b)
q(b)2
B	T-1(b)
g(a)> g(a)fθ∣b(a)dλb(a)dT*π(b)
[Vlogf∏(a)>Vlogf∏(a)] fθ∣b(a)dλb(a)dTπ(b).
(B.1)
Denoting by π∣b the probability measure with density /。& We see that Π∣b := {π∣b(∙,θ) : θ ∈ Θ} is
a λb-compatible family of probability measures, proving claim 2.
If we denote E∏∣b [g] = JT-汽与 g(a)fa∣b(a)dλb(a), we further obtain that
(B') = / q(b)2E∏∣b[Vlogfa∣b(a)>Vlog fa∣b(α)∖dT*∏(b)
+ 2/ q(b)2Vlogfτ(b)>E∏∣b[Vlogfa∣b(α)]dT*∏(b)
XB-------------------------------------------}
(i)
+ / q(b)2E∏∣b[Vlogfτ(b)>Vlogfτ(b)]dT*π(b).
In the equation above, term (i) is 0 because E∏∣b[V log fa∣b(α)] = 0. Thus we get that
(B.1)	= [ q(b)2E∏∣b[V log fa∣b(a)> V log fa∣b(α)]dT*∏(b) + [ q(b)2V log fτ(b)>V log fτ(b)dT*π(b)
BB
=[q(T(a))2Vlogfa∣b(a)>Vlogfa∣b(a)d∏(a) + [ q(b)2Vlogfτ(b)>Vlog fτ(b)dT*π(b).
AB
(B.2)
Because a density is unique almost-everywhere, we can replace fτ with fτ*∏ in (B.2), giving claim
3:
E∏ [q(T(a))2g(a)>g(a)] = Eτ*∏ [q(b)2E∏∣b [Vlogfa∣b(a)>Vlogfa∣b(α)]] + Et*∏ [q(b)2h(b)>h(b)]
•
I∏,λ(q ◦ T,θ) = Eτ*∏ [I∏∣b,λb(q ◦ T θ)] + 工τ*∏,μ(q网.
□
B.3 Proof of Theorem 4.6
First, we decompose the variance ofg1 as
Varρ,∏(gι) = Varρ [E∏∣s [gι]] + EP [Var∏∣s [gι]] .	(B.3)
A similar decomposition holds for g2. By combining Lemma 4.5 with (B.3) and its equivalent for g2,
we get that
Varρ,∏(gι) - Varρ,τ*∏(g2) = Eρ [Var∏∣s [gι] — VarT*∏∣s [g2]].
For any fixed s, applying the definition of variance given in Section 2 and Lemma 4.5 gives
VarnIs [g1] - VarnIs [g2] = Eπ∣s [g>g1 - g>g2] .	(B.4)
By applying Theorem B.1 (see Appendix B.2), to E∏∣s [g>gι] we obtain
EnIs [g1>g1 - g2>g2] = ET*n [InIb,λb (q ◦T, θ)] .	(B.5)
The result follows from combining (B.4) and (B.5), concluding the proof.
15
Published as a conference paper at ICLR 2019
B.4 Marginal Policy Gradients for Clipped and Normalized Actions
For the clipped action setting, we give an example of a λ-compatible family for which Corollary 4.7
can be applied.
Example B.2 (The Gaussian is λ-compatible). Let (A, E) := (R, B(R)) and λ be the Lebesgue
measure. Consider Π, a Gaussian family parametrized by θ ∈ Θ. If Θ is constrained such that the
variance is lower bounded by > 0, Π is λ-compatible.
Below are proofs of Corollaries 4.7 and 4.8 from Section 4.
Proof of Corollary 4.7. First, it is clear T is measurable, and it is easy to confirm that Conditions
4.1-4.3 hold. Next, define μ = δα + δβ + λ, where λ is understood to be its restriction to (α, β). As
defined, μ is a mixture measure on B and We can easily check that T*Π is μ-compatible. In fact, the
density of T*π(∙, θ∣s) is given by
(R(-∞,α] f∏ (a, θ)dλ b = α
fτ*∏(b,θ)=	f∏(b,θ)	b ∈ (α,β)
[/[β,∞) f∏(a,θ)dλ	b = β.
By applying Theorem 4.6 and observing
Va%∏(q∏(s, a)ψ(s, T(a),θ)) = Va%τ*∏(q∏(s, b)Ψ(s, b,θ)),
the proof is complete.	□
Proof of Corollary 4.8. First, it is clear T is measurable. Second, fMV differentiable in θ implies Π
is λ-compatible. This also implies ∕ag, the density of T*Π, is differentiable in θ and therefore T∏ is
σ-compatible, where σ is the spherical measure on (B, F) = (Sd-1, B(Sd-1)). It is straightforward
to confirm that the remainder of Conditions 4.1-4.3 hold. Applying Theorem 4.6 completes the
proof.	□
B.5 Policy Gradients for Parametrized Action Spaces
First we derive a stochastic policy gradient for parametrized action spaces, which we can do by
writing down the policy distribution and applying 2.2. Recall a parametrized action space with K
discrete actions is defined as
A ：= ∣J{(k,ω) : ω ∈ Ωk},
k
where k ∈ {1, . . . , K}.
Construction of a Policy Family
Masson et al. (2016) gives a definition for a policy over parametrized action spaces, and our definition
is the same in spirit, but for our purposes we need to be careful in formalizing the construction. Our
construction here is also a bit more general.
Informally, we can think of a policy over a parametrized action space as a mixture model, where
k ∈ [K] is a latent state. To formally define a policy family on A, the idea will be to construct a
density function fπ that is differentiable in its parameter θ. We proceed as follows:
1.	Let (Ωk, Ek,μk) be measure spaces.
2.	For k ∈ [K]: specify ∏k = {π(∙,θ∣s) : θ ∈ Θ}, a μk-compatible family of probability
measures on (Ωk, Ek). Denote the corresponding densities by fk.
3.	Denote by μo the counting measure on (Ao, B(Ao)) = (R, B(R)), and specify ∏o =
{∏(∙,θ∣s): θ ∈ Θ} a μo-compatible family of probability measures, parametrized by θo
and supported on [K]. Denote the corresponding density by f0 .
4.	Let θ := (θi)i, and define
fπ((k,ω),θ∣s) := ∣f0(k,θ0ls)fk(ω,θkIs)
if (k, ω) ∈ A
otherwise.
16
Published as a conference paper at ICLR 2019
To finish the policy construction, We need an appropriate σ-algebra E and reference measure μ such
that f∏ is a measurable and JA f∏ dμ = 1. In fact it is not difficult to construct E and μ in terms of
(Ei)i and (μi)i, respectively, but we do not go into detail here. Assuming such a construction exists,
we can define Π a μ-compatible family of policies, parametrized by θ = (θi)i.
Stochastic Policy Gradient
Let (A, E, μ) and Π be as constructed above. By applying Theorem 2.2, Vη(θ) can be estimated
from samples by
g(s,a,θ) := q∏(s,a)Vlog f∏(a,θ∣s) = q∏(s,a)Vlog fo(k,θo∣s) + q∏(s,a)V log fk(ω,θk|s).
(B.6)
Restricted Action Parameters
The second term in (B.6) is simply the policy gradient for a μk-compatible family on (Ωk, Ek,μk).
Let (Bk , Fk ) be a measurable space and consider the setting in which we apply a measurable
function Tk : Ak → Bk to the action parameters before execution in the environment. Assume
the conditions are satisfied to apply Theorem 4.6, and denote by fk产 the density of T*∏k with
respect to an appropriate reference measure. Then we can replace qπ(s, a)V log fk(ω, θk |s) with
q∏(s, a)V log fk,*(Tk (ω), θk |s) in (B.6) to obtain the lower variance estimator
e(s, a, θ) := q∏ (s, a) V Iogfn (a,θ∣s) = q∏ (s, a)V log fo(k, θo∣s)+q∏ (s, a) V log fk,*(Tk (ω),θk |s).
(B.7)
17
Published as a conference paper at ICLR 2019
C Details for Applications
This section contains additional details on the experiments and results in Sections 5.1 and 5.2.
C.1 2D Navigation
We run each setup 24 times from a random initialization. To create the cumulative reward trajectory
plots in Figure 2 we (1) use k-NN regression to interpolate the cumulative discounted rewards on
each run, and (2) using the cumulative discounted rewards from each sample trajectory, plot the
average curve with a 95% confidence band.
Table 1 gives the hyper-parameters used in the experiments on the Platform2D-v1 environment.
Table 1: Hyper-parameter settings used in training
Hyperparameter	Setting
Num. Workers	4
Optimizer	SGD
Learning Rate	0.01
σ	0.1
γ	0.99
No. Layers: Policy Net	2
Width: Policy Net	32
No. Layers: Value Net	2
Width: Value Net	32
C.2 King of Glory
Here we provide details on modeling for King of Glory, the experimental procedure and the tables
referenced in Section 5.2.
State Representation
A detailed description of all the features can be found below in Table 4. After extracting features,
we take the outer product of the feature vector with itself to capture dependencies between features.
To be precise, first define φ0 to be the 74-dimensional initial feature extraction. The featurized state
representation φ(s) is defined by
(φ0(s))i(φ0(s))j	fori 6= j,
(φ(s))i,j := (φ0(s))i	fori=j.
By symmetry, we use only the lower triangular portion of the matrix defined above giving a (74 ×
73)/2 + 74 = 2775 dimensional feature vector that is input to the policy and value networks.
Modeling the Policy and Value Function
The value network is modeled using a feed-forward neural network which takes as input φ(s). The
sampling policy is a mixture, where the mixing distribution is over the 7 discrete actions, and a
Gaussian distribution is used for each parameter space. We model the policy using 5 networks, one of
which represents the distribution over the discrete actions by a fully connected feed-forward neural
network into a 7-way softmax. For the parameters, we model the mean of the sampling distribution
using a feed-forward network. The variance of the sampling distribution for the action parameters is
σ2I where σ is learned by the agent. All action parameters share the same σ and all 5 networks share
weights up to the last layer.
Learning the Policy
The agent is trained to play as the hero Di Ren Jie and training is against the game’s internal AI,
also playing as Di Ren Jie. For both methods, 10 agents are trained for 5000 episodes each. During
18
Published as a conference paper at ICLR 2019
training the cumulative discounted reward of each episode and game outcome are tracked. The
hyper-parameters we used for the neural network structure and the A3C algorithm are shown in
Table 2. To construct the plots in Figure 2, we apply a low pass filter to each trajectory and then plot
the average curve with a 95% confidence band.
Like Mnih et al. (2015) and others do for the Atari Learning Environment, we employ frame-skipping;
two out of every three frames are skipped. Because our reward is defined in terms of a state potential
function, rewards from the skipped states are still captured. For training, we use the Adam algorithm
(Kingma & Ba, 2015). No parameters are shared between different networks and all networks use
SElu activation functions (Klambauer et al., 2017). Table 2 contains various hyper-parameter settings
we used.
Table 2: Hyper-parameter settings used in training
Hyperparameter	Setting
Num. Workers	8
N	128
Optimizer	Adam (β = (0.5, 0.9))
Actor Learning Rate	10-6
Critic Learning Rate	10-3
γ	0.99
No. Hidden Layers: Policy Net	2
Width: Policy Net	(128,96)
Activation: Policy Net	SELU
No. Hidden Layers: Value Net	2
Width: Value Net	(128,96)
Activation: Value Net	SELU
Table 3: Parametrized action space for King of Glory
Action	Parameter Dimension	Description
no action	0	agent does nothing
move	2	move in direction ω
attack	0	hero uses its normal attack
skill 1	2	hero uses skill 1 towards direction ω
skill 2	2	hero uses skill 2 towards direction ω
skill 3	2	hero uses skill 3 towards direction ω
recovery skill	0	hero uses the recovery skill to heal itself
19
Published as a conference paper at ICLR 2019
Table 4: State features for King of Glory
Feature	Dimension	Range	Description
position: our hero	2	[-1, 1]2	x,y coordinates of our hero’s position
position: enemy hero	2	[-1, 1]2	x,y coordinates of enemy hero’s position
position: enemy hero, relative	3	R3	distance, direction to enemy hero
position: enemy tower, relative	4	R4	distance, distance relative to attack range, relative direction to the near- est enemy tower
position: enemy minion, relative	3	R3	distance, relative direction to the nearest enemy minion
position: our spring, relative	3	R3	distance, relative direction to our life spring
in tower range: our hero	3	{0, 1}3	is our hero in the range of the enemy’s towers
in tower range: enemy hero	3	{0, 1}3	is enemy hero in the range of our tower
attacked by tower	3	{0, 1}3	are the enemy towers are attacking our hero
skill cool down: our hero	5	[-1, 1]5	normalized cool down time for our hero’s skills
skill cool down: enemy hero	5	[-1, 1]5	normalized cool down time for enemy hero’s skills
HP: our hero	1	[-1, 1]	our hero’s health points
HP: enemy hero	1	[-1, 1]	enemy hero’s health points
HP: nearest minion	1	[-1, 1]	health points of the nearest enemy minion
HP: nearest tower	1	[-1, 1]	health points of the nearest enemy tower
HP: minions in range	1	R+	sum of HP of all the minions in the attack range of our hero
alive: our hero	1	{0, 1}	whether our hero is alive
alive: enemy hero	1	{0, 1}	whether enemy hero is alive
gold: our hero	1	[0, 1]	our hero’s gold
gold: enemy hero	1	[0, 1]	enemy hero’s gold
gold: ∆	1	[-1, 1]	difference between our hero’s gold and enemy hero’s gold
EP: our hero	1	[-1, 1]	normalized energy points of our hero
EP: enemy hero	1	[-1, 1]	normalized energy points of enemy hero
hero state: our hero	13	[0, 1]13	our hero’s level, experience, current money, kill count, death count, as- sist count, total money, attack range, physical attack, magical attack, move speed, health points, energy points
hero state: enemy hero	13	[0, 1]13	enemy hero’s level, experience, current money, kill count, death count,
assist count, total money, attack range, physical attack, magical attack,
move speed, health points, energy points
Table 5: State features and weights used in reward design
Feature	Weight	Description	Notes
gold difference	0.5	difference between the amount of our hero and enemy hero	
HP (our hero)	0.5	health points of our hero	
hurt to enemy hero	0.5	total amount of hurt from our hero to enemy hero	
hurt to enemy	1.0	total amount of hurt from our hero to all the enemies	
kill dead difference	1.0	difference between kill count and dead count	
distance to our life spring	0.25×(1.0 - HP)	distance from our hero to spring	HP ∈ [0, 1]
distance to enemy	0.125×HP	distance from our hero nearest enemy	HP ∈ [0, 1]
tower HP difference	1.0	difference between HP of our tower and enemy tower	
crystal HP difference	2.0	difference between HP of our crystal and enemy crystal	
skill hit rate	0.15	percent of emitted skills that hit enemy hero	
win/loss	2.0	game result	
20