Published as a conference paper at ICLR 2019
Efficient Training on Very Large Corpora
via Gramian Estimation
Walid Krichene, Nicolas Mayoraz, Steffen Rendle, Li Zhang, Xinyang Yi, Lichan Hong,
Ed H. Chi & John Anderson
Google
{walidk,nmayoraz,srendle,liqzhang,xinyang,lichan,edchi,janders}@google.com
Ab stract
We study the problem of learning similarity functions over very large corpora
using neural network embedding models. These models are typically trained using
SGD with random sampling of unobserved pairs, with a sample size that grows
quadratically with the corpus size, making it expensive to scale. We propose new
efficient methods to train such models without sampling unobserved pairs. Inspired
by matrix factorization, our approach relies on adding a global quadratic penalty
and expressing this term as the inner-product of two Gramians. We show that the
gradient of this term can be efficiently computed by maintaining estimates of the
Gramians, and develop variance reduction schemes to improve the quality of the
estimates. We conduct large-scale experiments that show a significant improvement
in training time and generalization performance compared to sampling methods.
1	Introduction
We consider the problem of learning a similarity function h : X × Y → R, that maps each pair of
items, represented by their feature vectors (x, y) ∈ X × Y, to a real number h(x, y), representing
their similarity. We will refer to x and y as the left and right feature vectors, respectively. Many
problems can be cast in this form: In natural language processing, x represents a context (e.g. a
bag of words), y represents a candidate word, and the target similarity measures the likelihood to
observe y in context x (Mikolov et al., 2013; Pennington et al., 2014; Levy & Goldberg, 2014).
In recommender systems, x represents a user query, y represents a candidate item, and the target
similarity is a measure of relevance of item y to query x, e.g. a movie rating (Agarwal & Chen, 2009),
or the likelihood to watch a given movie (Hu et al., 2008; Rendle, 2010). Other applications include
image similarity, where x and y are pixel-representations of images (Bromley et al., 1993; Chechik
et al., 2010; Schroff et al., 2015), and network embedding models (Grover & Leskovec, 2016; Qiu
et al., 2018), where x and y are nodes in a graph and the similarity is whether an edge connects them.
A popular approach to learning similarity functions is to train an embedding representation of each
item, such that items with high similarity are mapped to vectors that are close in the embedding
space. A common property of such problems is that only a small subset of all possible pairs X × Y is
present in the training set, and those examples typically have high similarity. Training exclusively on
observed examples has been demonstrated to yield poor generalization performance. Intuitively, when
trained only on observed pairs, the model places the embedding of a given item close to similar items,
but does not learn to place it far from dissimilar ones (Shazeer et al., 2016; Xin et al., 2017). Taking
into account unobserved pairs is known to improve the embedding quality in many applications,
including recommendation (Hu et al., 2008; Yu et al., 2017) and word analogy tasks (Shazeer et al.,
2016). This is often achieved by adding a low-similarity prior on all pairs, which acts as a repulsive
force between all embeddings. But because it involves a number of terms quadratic in the corpus size,
this term is computationally intractable (except in the linear case), and it is typically optimized using
sampling: for each observed pair in the training set, a set of random unobserved pairs is sampled and
used to compute an estimate of the repulsive term. But as the corpus size increases, the quality of the
estimates deteriorates unless the sample size is increased, which limits scalability.
In this paper, we address this issue by developing new methods to efficiently estimate the repulsive
term, without sampling unobserved pairs. Our approach is inspired by matrix factorization models,
1
Published as a conference paper at ICLR 2019
which correspond to the special case of linear embedding functions. They are typically trained using
alternating least squares (Hu et al., 2008), or coordinate descent methods (Bayer et al., 2017), which
circumvent the computational burden of the repulsive term by writing it as a matrix-inner-product of
two Gramians, and computing the left Gramian before optimizing over the right embeddings, and vice-
versa. Unfortunately, in non-linear embedding models, each update of the model parameters induces
a simulateneous change in all embeddings, making it impractical to recompute the Gramians at each
iteration. As a result, the Gramian formulation has been largely ignored in the non-linear setting,
where models are instead trained using stochastic gradient methods with sampling of unobserved
pairs, see Chen et al. (2016). Vincent et al. (2015) were, to our knowledge, the first to attempt
leveraging the Gramian formulation in the non-linear case. They consider a model where only one of
the embedding functions is non-linear, and show that the gradient can be computed efficiently in that
case. Their result is remarkable in that it allows exact gradient computation, but this unfortunately
does not generalize to the case where both embedding functions are non-linear.
Contributions We propose new methods that leverage the Gramian formulation in the non-linear
case, and that, unlike previous approaches, are efficient even when both left and right embeddings are
non-linear. Our methods operate by maintaining stochastic estimates of the Gram matrices, and using
different variance reduction schemes to improve the quality of the estimates. We perform several
experiments that show these methods scale far better than traditional sampling approaches on very
large corpora.
We start by reviewing preliminaries in Section 2, then derive the Gramian-based methods and analyze
them in Section 3. We conduct large-scale experiments on the Wikipedia dataset in Section 4, and
provide additional experiments in the appendix. All the proofs are deferred to Appendix A.
2	Preliminaries
2.1	Notation and problem formulation
We consider models that consist of two embedding functions u : Rd ×X → Rk and v : Rd × Y → Rk,
which map a parameter vector1 θ ∈ Rd and feature vectors x, y to embeddings u(θ, x), v(θ, y) ∈ Rk.
The output of the model is the dot product2 of the embeddings hθ (x, y) = hu(θ, x), v(θ, y)i, where
h∙, ∙i denotes the usual inner-product on Rk. Low-rank matrix factorization is a special case, in which
the left and right embedding functions are linear in x and y . Figure 1 illustrates a non-linear model,
in which each embedding function is given by a feed-forward neural network.3
We denote the training set by T = {(xi, yi, si) ∈ X × Y × R}i∈{1,...,n}, where xi, yi are the feature
vectors and si is the target similarity for example i. To make notation more compact, we will use
ui(θ), vi (θ) as a shorthand for u(θ, xi), v(θ, yi), respectively. As discussed in the introduction, we
also assume that we are given a low-similarity prior pij ∈ R for all pairs (i, j) ∈ {1, . . . , n}2. Given
a differentiable scalar loss function ` : R × R → R, the objective function is given by
n	nn
min — X' (hui(θ),vi(θ)i,Si) + — XX((Ui⑻,vj ⑻i- Pij )2,
θ∈Rd n	n2
i=1	i=1 j=1
(1)
where the first term measures the loss on observed data, the second term penalizes deviations from the
prior, and λ is a positive hyper-parameter that trades-off the two terms. To simplify the discussion, we
will assume a uniform zero prior Pij as in (Hu et al., 2008), the general case is treated in Appendix B.
To optimize this objective, existing methods rely on sampling to approximate the second term, and
are usually referred to as negative sampling or candidate sampling, see Chen et al. (2016); Yu et al.
(2017) for a survey. Due to the double sum in (1), the quality of the sampling estimates degrades as
the corpus size increases, which can significantly increase training times. This can be alleviated by
increasing the sample size, but does not scale to very large corpora.
1In many applications, it is desirable for the two embedding functions u, v to share certain parameters, e.g.
embeddings of categorical features common to left and right items; hence, we use the same θ for both.
2This also includes cosine similarity models when the embedding functions u, v are normalized.
3We focus on models with this dot-product structure since they allow efficient retrieval: given x ∈ X, finding
items y ∈ Y with high similarity to x reduces to a maximum inner-product search problem (MIPS), which can
be approximated efficiently (Shrivastava & Li, 2014; Neyshabur & Srebro, 2015).
2
Published as a conference paper at ICLR 2019
u(θ,x) ∈ R"⅞⅞J)
∙∙ ■■■ ∙
u(θ,∙)	∖
X ∈X
hθ(x, y) = <u(θ,χ),v(θ,y)⅛
叵至工v(θ,χ) ∈Rk
(θ,∙)
Y
Figure 1: A dot-product embedding model for a similarity function on X × Y.
2.2 Gramian formulation
A different approach to solving (1), widely popular in matrix factorization, is to rewrite the double
sum as the inner product of two Gram matrices. Let us denote by Uθ ∈ Rn×k the matrix of all left
embeddings such that ui (θ) is the i-th row of Uθ, and similarly for Vθ ∈ Rn×k. Then denoting the
matrix inner-product by hA, Bi = Pi,j Aij Bij, we can rewrite the double sum in (1) as:
g⑹=n X X(UθV> )2j = n <uθ v> , Uθ V>).	⑵
n i=1 j=1	n
Now, using the adjoint property of the inner product, we have UθVθ> , UθVθ> = Uθ> Uθ , Vθ> Vθ ,
and if We denote by U 0 U the outer product of a vector U by itself, and define the Gram matrices
1	1n	1	1n
Gu(O)=	uθ	Uθ	=	ui(θ) 0	Ui⑹，Gv (O)=	Vθ	Vθ	=	Vi(O) 0 Vi⑹，⑶
nn	nn
i=1	i=1
the penalty term becomes
g(O) = hGu(O), Gv(O)i.	(4)
The Gramians are k × k PSD matrices, Where k, the dimension of the embedding space, is much
smaller than n - typically k is smaller than 1000, while n can be arbitrarily large. Thus, the Gramian
formulation (4) has a much loWer computational complexity than the double sum formulation (2), and
this reformulation is at the core of alternating least squares and coordinate descent methods (Hu et al.,
2008; Bayer et al., 2017), which operate by computing the exact Gramian for one side, and solving
for the embeddings on the other. However, these methods do not apply in the non-linear setting due
to the implicit dependence on O, as a change in the model parameters simultaneously changes all
embeddings on both sides, making it intractable to recompute the Gramians at each iteration, so the
Gramian formulation has not been used when training non-linear models. In the next section, we
show that it can in fact be leveraged in the non-linear case.
3	Training Embedding Models using Gramian Estimates
In order to leverage the Gramian formulation in the non-linear case, we start by rewriting the objective
function (1) in terms of the Gramians defined in (3). Let
fi(O) := ` (hUi(O), Vi(O)i, si)	(5)
gi(θ) = ∣[hui(θ),Gv (θ)ui(θ)i + hvi(θ),Gu(θ)vi(θ)i],	(6)
then (1) is equivalent to minj∈Rd 1 Pn=Jfi(θ) + λgi(θ)]. Intuitively, for each example i, -Vfi(θ)
pulls the embeddings Ui and Vi close to each other (assuming a high similarity s", while -Vgi (θ)
creates a repulsive force between Ui and all embeddings {Vj}j∈{1,...,n}, and between Vi and all
{Uj}j∈{1,...,n}, see Appendix D for further discussion, and illustration of the effect of this term.
While the Gramians are expensive to recompute at each iteration, we can maintain PSD estimates
Gu, Gv of Gu(O), Gv(O). Then the gradient of g(O) can be approximated by the gradient (w.r.t. O) of
gi(θ,G u, G v) = DUi (θ), G v %(θ)E + <vi(θ), GuVi(θ)),	(7)
as stated in the following proposition.
Proposition 1. If i is drawn uniformly in {1, . . . , n}, and Gu, Gv are unbiased estimates of
Gu(θ), Gv(θ) and independent of i,then Vθgi(θ,Gu,Gv) Isan unbiased estimate of Vg(θ).
3
Published as a conference paper at ICLR 2019
In a mini-batch setting, one can further average gi over a batch of examples i ∈ B (which We do in
our experiments), but we will omit batches to keep the notation concise. Next, we propose several
methods for computing Gramian estimates Gu , Gv, and discuss their tradeoffs. Since each Gramian
can be written as a sum of rank-one terms, e.g. Gu(θ) = 1 pn=1 ui(θ) 0 ui(θ), a simple unbiased
estimate can be obtained by sampling one term (or a batch) from this sum. We improve on this by
using different variance reduction methods, which we discuss in the next two sections.
3.1	Stochastic Average Gramian
Our first method is inspired by the stochastic average gradient (SAG) method (Roux et al., 2012;
Defazio et al., 2014; Schmidt et al., 2017), which reduces the variance of the gradient estimates by
maintaining a cache of individual gradients, and estimating the full gradient using this cache. Since
each Gramian is a sum of outer-products (see equation (3)), we can apply the same technique to
estimate Gramians. For all i ∈ {l,..., n}, let U%, ^ be a cache of the left and right embeddings
respectively. We will denote by a superscript (t) the value of a variable at iteration t. Let Sut)=
1 Pn=ι u(t) 0 u(t), which corresponds to the Gramian computed with the current caches. At each
iteration t, an example i is drawn uniformly at random and the estimate of the Gramian is given by
Gut) = Sut) + β[υi(θ(t)) 0 Ui(θ⑴)-u(t) 0 u(t)],	(8)
and similarly for GVt). This is summarized in Algorithm 1, where the model parameters are updated
using SGD (line 10), but this update can be replaced with any first-order method. Here β can take
one of the following values: β = 1, following SAG (Schmidt et al., 2017), or β = 1, following
SAGA (Defazio et al., 2014). The choice of β comes with trade-offs that we briefly discuss below.
We will denote the cone of positive semi-definite k × k matrices by S+k .
Proposition 2. Suppose β = 1 in(8). Thenfor all t, GUt), Gvt) ∈ Sf.
Proposition 3. Suppose β = 1 in (8). Thenforall t, GGutis an unbiased estimate of Gu(θ(t)).
While taking β = 1 gives an unbiased estimate, note that it does not guarantee that the estimates
remain in S+k . In practice, this can cause numerical issues, but can be avoided by projecting Gu , Gv
on S+k , using their eigenvalue decompositions. The per-iteration cost of maintaining the Gramian
estimates is O(k) to update the caches, O(k2) to update the estimates Su, Sv, Gu, Gv, and O(k3) for
projecting on S+k . Given the small size of the embedding dimension k, O(k3) remains tractable. The
memory cost is O(nk), since each embedding needs to be cached (plus a negligible O(k2) for storing
the Gramian estimates). This makes SAGram much less expensive than applying the original SAG(A)
methods, which require maintaining caches of the gradients, this would incur a O(nd) memory cost,
where d is the number of parameters of the model, and can be orders of magnitude larger than the
embedding dimension k. However, O(nk) can still be prohibitively expensive when n is very large.
In the next section, we propose a different method which does not incur this additional memory cost.
Algorithm 1 SAGram (Stochastic Average Gramian)
1: Input: Training data {(xi, yi, si)}i∈{1,...,n}, learning rate η > 0.
2: Initialization phase
3: draw θ randomly
4:	Ui J Ui(θ), Vi J Vi(θ) ∀i ∈ {1,...,n}
5:	Su J 1 11 Pn=I Ui 0 Ui, Sv J 1 Pn=1 Vi 0 Vi
6: repeat
7: Update Gramian estimates (i 〜Uniform(n))
8:	Gu	J	Su	+ β[ui(θ)	0 Ui(θ)	—	Ui	0 Ui],	Gv	J	Sv	+ β[Vi(θ)	0 Vi(θ)	— Vi	0 ^i]
9: Update model parameters then update caches (i 〜Uniform(n))
_	_	__ - 一 ，	、	. .	, _ O, O...
10:	θ J θ — ηVθ[fi(θ) + λgi(θ, Gu, Gv)]
Λ	Λ	1 r	,八	ʌ	ʌ r	Λ	Λ	1 r	，八八	ʌ r
11:	Su J Su +	n [Ui(θ) 0 Ui(θ)	— Ui	0 Ui],	Sv	J Sv	+ n [Vi(θ) 0	Vi(θ)	— Vi 0 Vi]
12:	Ui J Ui(θ),	Vi J Vi(θ)
13: until stopping criterion
4
Published as a conference paper at ICLR 2019
3.2	Stochastic Online Gramian
To derive the second method, we reformulate problem (1) as a two-player game. The first player
optimizes over the parameters of the model θ, the second player optimizes over the Gramian estimates
Gu , Gv ∈ S+k , and they seek to minimize the respective losses
{^	^	,	1 __ o-.	„	,	.	, ʌ 人 .-
LGuCv (θ) = n1 Pn=1[fi(θ) + λgi(θ,G u,Gv)]
L2(G u, G v) = 2 IlG U - GU ⑹ kF + 2 IlGv - Gv (θ)kF,
where gi is defined in (7), and k∙kF denotes the FrobeniUs norm. TojUstify this reformulation, We
can characterize its first-order stationary points, as follows.
Proposition 4. (θ,Gu,Gv) ∈ Rd × S+k × S+k is a first-order stationary point for (9) if and only if θ
is a first-order stationary point for problem (1) and Gu = Gu(θ), Gv = Gv (θ).
Several stochastic first-order dynamics can be applied to problem (9), and Algorithm 2 gives a simple
instance where each player implements SGD with a constant learning rate. In this case, the Updates of
the Gramian estimates (line 7) have a particularly simple form, since NGU Lθ(Gu, Gv) = GU 一 Gu(θ)
and can be estimated by GU 一 ui(θ) 0 %(θ), resulting in the update
GUt) = (1 — α)Gut-D + αui(θ⑴)0 ui(θ(t)),	(10)
and similarly for Gv. One advantage of this form is that each update performs a convex combination
between the current estimate and a rank-1 PSD matrix, thus guaranteeing that the estimates remain in
S+k , without the need to project. The per-iteration cost of updating the estimates is O(k* 2), and the
memory cost is O(k2) for storing the Gramians, which are both negligible.
Algorithm 2 SOGram (Stochastic Online Gramian)
1:	Input: Training data {(xi, yi, si)}i∈{1,...,n}, learning rates η > 0, α ∈ (0, 1).
2:	Initialization phase
3:	draw θ randomly, Gu, Gv J 0k×k
4:	repeat
5:	Update Gramian estimates (i 〜Uniform)
6:	Gu	J (1 - α)Gu	+ αui(θ)	0 ui(θ),	Gv	J (1 -	α)Gv	+ αvi (θ)	0 vi (θ)
7:	Update model parameters (i 〜Uniform)
_	_	__ - 一 ，	、	. .	, _ O, O...
8:	θ 一 θ 一 ηVθ [fi(θ) + λgi(θ,G u, Gv)]
9:	until stopping criterion
The update (10) can also be interpreted as computing an online estimate of the Gramian by averaging
rank-1 terms with decaying weights, thus we call the method Stochastic Online Gramian. Indeed,
we have by induction on t, GUt) = PT=1 α(1 一 α)t-τuiτ (θ(τ)) 0 uiτ (θ(τ)). Intuitively, averaging
reduces the variance of the estimator but introduces a bias, and the choice of the hyper-parameter α
trades-off bias and variance. The next proposition quantifies this tradeoff under mild assumptions.
Proposition 5. Let Gy) = PT=1 α(1 - α)t-τGu(θ(τ)). Suppose that there exist σ,δ > 0 such that
for all t, Ei Iui(θ(t)) 0 ui(θ(t)) 一 Gu(θ(t))I2F ≤ σ2 and IGu(θ(t+1)) 一 Gu(θ(t))IF ≤ δ. Then ∀t,
EkGUt)- GUt) kF ≤ σ2 ʌ	(11)
2 — α
kGUt) - GUt)kF ≤ δ(1∕α - 1) + (1 - α)tkGUt)kF.	(12)
The first assumption simply bounds the variance of single-point estimates, while the second bounds
the distance between two consecutive Gramians, a reasonable assumption, since in practice the
changes in Gramians vanish as the trajectory θ(τ) converges. In the limiting case α = 1, GUt) reduces
to a single-point estimate, in which case the bias (12) vanishes and the variance (11) is maximal,
while smaller values of α decrease variance and increase bias. This is confirmed in our experiments,
as discussed in Section 4.2.
5
Published as a conference paper at ICLR 2019
3.3 Comparison with existing stochastic methods
We conclude this section by showing that candidate sampling methods (see Chen et al. (2016); Yu
et al. (2017) for recent surveys) can be reinterpreted in terms of the Gramian formulation (4). These
methods work by approximating the double-sum in (1) using a random sample of pairs. Suppose a
batch of pairs (i, j) ∈ B × B0 is sampled4, and the double sum is approximated by
g(θ)
∣B‰∣ XXμiVj hui(θ),vj(θ)i
| ||	| i∈B j∈B0
(13)
where μ%, Vj are the inverse probabilities of sampling i,j respectively (to guarantee that the estimate
is unbiased). Then applying a similar transformation to Section 2.2, one can show that
g(θ)=D ∣B∣ χ μiui(θ) ® ui(θ), Bi X Vjvj (θ) ® Vj *.
i∈B	j∈B0
(14)
which is equivalent to computing two batch-estimates of the Gramians. Implementing existing
methods using (14) rather than (13) can decrease their computional complexity in the large batch
regime, for the following reason: the double-sum formulation (13) involves a sum of |B||B0| dot
products of vectors in Rk, thus computing its gradient costs O(k|B||B0|). On the other hand, the
Gramian formulation (14) is the inner product of two k × k matrices, each involving a sum over the
batch, thus computing its gradient costs O(k2 max(|B|, |B0|)), which is cheaper when the batch size
is larger than the embedding dimension k, a common situation in practice. With this formulation,
the advantage of SOGram and SAGram becomes clear, as they use more embeddings to estimate
Gramians (by caching or online averaging) than would be possible using candidate sampling.
4 Experiments
In this section, we conduct large-scale experiments on the Wikipedia dataset (Wikimedia Foundation).
Additional experiments on MovieLens (Harper & Konstan, 2015) are given in Appendix F.
4.1	Experimental setup
Datasets We consider the problem of learning the intra-site links between Wikipedia pages. Given
a pair of pages (x, y) ∈ X × X, the target similarity is 1 if there is a link from x to y, and 0
otherwise. Here a page is represented by a feature vector x = (xid, xngrams, xcats), where xid is (a
one-hot encoding of) the page URL, xngrams is a bag-of-words representation of the set of n-grams of
the page’s title, and xcats is a bag-of-words representation of the categories the page belongs to. Note
that the left and right feature spaces coincide in this case, but the target similarity is not necessarily
symmetric (the links are directed edges). We carry out experiments on subsets of the Wikipedia graph
corresponding to three languages: Simple English, French, and English, denoted respectively by
simple, fr, and en. These subgraphs vary in size, and Table 1 shows some basic statistics for each
set. Each set is partitioned into training and validation using a (90%, 10%) split.
language	#pages	# links	# ngrams	# cats
simple	85^	4.6M	8.3K	-6.1^
fr	-1.8M	142M	167.4K	125.3K
en	5.3M	490M	501.0K	403.4K
Table 1: Corpus sizes for each training set.
Models We train non-linear embedding models consisting of a two-tower neural network as in
Figure 1, where the left and right embedding functions map, respectively, the source and destination
page features. The two embedding networks have the same structure: the input feature embeddings are
concatenated then mapped through two hidden layers with ReLU activations. The input embeddings
are shared between the two networks, and their dimensions are 50 for simple, 100 for fr, and 120
for en. The sizes of the hidden layers are [256, 64] for simple and [512, 128] for fr and en.
4The simplest variant uses a uniform distribution, but other methods have been proposed, such adaptive
sampling (Bengio & Senecal, 2008; Bai et al., 2017), and importance sampling (Bengio & Senecal, 2003).
6
Published as a conference paper at ICLR 2019
Training methods The model is trained using a squared error loss, '(s, s0) = 2(S - s0)2, optimized
using SAGram, SOGram, and as baseline, SGD with candidate sampling, using different sampling
strategies. The experiments reported in this section use a learning rate η = 0.01, a penalty coefficient
λ = 10, and batch size 1024. These parameters correspond to the best performance of the baseline
methods; we report additional results with different hyper-parameter settings in Appendix E. For
SAGram and SOGram, a batch B is used in the Gramian updates (line 8 in Algorithm 1 and line 6
in Algorithm 2, where we use a sum of rank-1 terms over the batch), and another batch B0 is used
in the model parameter update. For the sampling baselines, the double sum is approximated by all
pairs in the cross product (i, j) ∈ B × B0, and for efficiency, we implement them using the Gramian
formulation as discussed in Section 3.3, since we operate in a regime where the batch size is an
order of magnitude larger than the embedding dimension k. In the first baseline method, uniform,
items are sampled uniformly from the vocabulary (all pages are sampled with the same probability).
The other baseline methods implement importance sampling similarly to Bengio & Senecal (2003);
Mikolov et al. (2013): in linear, the probability is proportional to the number of occurrences of
the page in the training set, and in sqrt, the probability is proportional to the square root of the
number of occurrences.
Figure 2: Gramian estimation error on a common trajectory (θ(t) ).
∣B∣=1024
SOGrama=O.OOl
→- SOGrama=0.005
-→- SOGrama=O.Ol
→- SOGrama=O.1
IBl = 128
∣B∣ = 1024
■ SOGraml α=O-l
SAGram1 0=ι∕n
(a) SAGram, SOGram and SGD with different sampling strategies. (b) SOGram with different averaging rates.
4.2	Quality of Gramian estimates
In the first set of experiments, we evaluate the quality of the Gramian estimates using each method.
In order to have a meaningful comparison, we fix a trajectory of model parameters (θ(t) )t∈{1,...,T},
and evaluate how well each method tracks the true Gramians Gu (θ(t) ), Gv (θ(t)) on that common
trajectory. This experiment is done on Wikipedia simple (the smallest of the datasets) so that we
can compute the exact Gramians by periodically computing the embeddings ui(θ(t)), vi(θ(t) ) on
the full training set at a given time t. We report in Figure 2 the estimation error for each method,
measured by the normalized Frobenius distance kGuG-,：()；：?kF . In Figure 2a, We can observe that
both variants of SAGram yield the best estimates, and that SOGram yields better estimates than the
baselines. Among the baseline methods, importance sampling (both linear and sqrt) perform
better than uniform. We also vary the batch size to evaluate its impact: increasing the batch size
from 128 to 1024 improves the quality of all estimates, as expected, but it is Worth noting that the
estimates of SOGram With |B| = 128 have comparable quality to baseline estimates With |B| = 1024.
In Appendix E, We shoW that a similar effect can be observed for gradient estimates, and We make a
formal connection betWeen Gramian and gradient estimation errors.
In Figure 2b, We evaluate the bias-variance tradeoff discussed in Section 3.2, by comparing the
estimates of SOGram With different learning rates α. We observe that higher values of α suffer
from higher variance Which persists throughout the trajectory. A loWer α reduces the variance but
introduces a bias, Which is mostly visible during the early iterations.
4.3	Impact on training speed and generalization quality
In order to evaluate the impact of the Gramian estimation quality on training speed and generalization,
We compare the validation performance of SOGram to the sampling baselines, on each dataset (We
7
Published as a conference paper at ICLR 2019
do not use SAGram due to its prohibitive memory cost for corpus sizes of 1M or more). The models
are trained with a fixed time budget of 20 hours for simple, 30 hours for fr and 50 hours for en.
We estimate the mean average precision (MAP) at 10, by scoring, every 5 minutes, left items in the
validation set against 50K random candidates (exhaustively scoring all candidates is prohibitively
expensive, but this gives a reasonable approximation). The results are reported in Figure 3. Compared
to the sampling baselines, SOGram exhibits faster training and better validation performance across
all sampling strategies. Table 2 summarizes the relative improvement of the final validation MAP.
βA32	OJS
OΛlβ ɪ
I
OAlβ
β
-4- uniform sampling
—uniform SOGram (β=0.01)
-Sqrtsampling
→- sqrt SOGram (α=0.01)
-∙- Iinearsampling
-*- linear SOGram (α=0-1)
10
hours
-*- uniform sampling
τ- uniform SOGram (α=0.01)
sqrt sampling
→- sqrt SOGram («=p. 1)
IinearSamPIing
→- linear SOGram (α=0.001)
35	2。 is
hours
-*- uniform sampling
—SOGram uniform (α=0.01)
____〔一▼- Sqrtsampling
i→- SqrtSOGmm (α=0.01)
-*- Iinearsampling
→- SOGram linear (β=0.001)
lβ	2β	30	4«	50
hours
Figure 3: Mean average precision at 10 on the validation set, for different methods, on simple (left),
fr (middle), and en (right). The dashed lines correspond to the baseline methods, and the solid lines
to SOGram. The different colors represent different sampling strategies.
language	uniform sampling	uniform SOGram	sqrt sampling	sqrt SOGram	linear sampling	linear SOGram
simple	0.0255	0.0266 (+4.2%)	0.0247	0.0268 (+8.3%)	00272	0.0300 (+10.7%)
fr	0.1056	01194 (+13.0%)	0.1047	01144 (+9.2%)	0.1004	0.1154 (+15.0%)
en	0.1586	0.1743 (+9.^9%T~	0.1543	0.1723 (+11T7%Γ	0.1504	0.1797 (19.5%)~
Table 2: Final validation MAP on each dataset, and relative improvement compared to the baselines.
The improvement on simple is modest (between 4% and 10%), which can be explained by the
relatively small corpus size (85K unique pages), in which case candidate sampling with a large batch
size already yields decent estimates. On the larger corpora, we obtain more significant improvements:
between 9% and 15% on fr and between 9% and 19% on en. It’s interesting to observe that the
best performance is consistently achieved by SOGram with linear importance sampling, even
though linear performs slightly worse than other strategies in the baseline. SOGram also has a
significant impact on training speed: if we measure the time it takes for SOGram to exceed the final
validation performance of each baseline method, this time is a small fraction of the total budget. In
our experiments, this fraction is between 10% and 17% for simple, between 23% and 30% for fr,
and between 16% and 24% for en. Additional numerical results are provided in Appendix E, where
we evaluate the impact of other parameters, such as the effect of the batch size |B |, the learning
rate η , and the Gramian learning rate α. For example, we show that the relative improvement of
SOGram compared to the baselines is even larger when using smaller batches, and its generalization
performance is more robust to the choice of batch size and learning rate.
5 Conclusion
We showed that the Gramian formulation commonly used in low-rank matrix factorization can be
leveraged for training non-linear embedding models, by maintaining estimates of the Gram matrices
and using them to estimate the gradient. By applying variance reduction techniques to the Gramians,
one can improve the quality of the gradient estimates, without relying on large sample size as is done
in traditional sampling methods. This leads to a significant impact on training time and generalization
quality, as indicated by our experiments. While we focused on problems with very large vocabulary
size, where traditional approaches are inefficient, it will be interesting to evaluate our methods on
other applications such as word-analogy tasks Mikolov et al. (2013); Schnabel et al. (2015). Another
direction of future work is to extend this formulation to a larger family of penalty functions, such as
the spherical loss family studied in (Vincent et al., 2015; de Brebisson & Vincent, 2016).
8
Published as a conference paper at ICLR 2019
References
Deepak Agarwal and Bee-Chung Chen. Regression-based latent factor models. In Proceedings of the
15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD
’09,pp.19-28, New York, NY, USA, 2009. ACM.
Yu Bai, Sally Goldman, and Li Zhang. Tapas: Two-pass approximate adaptive sampling for softmax.
CoRR, abs/1707.03073, 2017.
Immanuel Bayer, Xiangnan He, Bhargav Kanagal, and Steffen Rendle. A generic coordinate descent
framework for learning from implicit feedback. In Proceedings of the 26th International Conference
on World Wide Web, WWW'17, pp. 1341-1350, 2017.
YOshua Bengio and Jean-Sebastien SenecaL Quick training of probabilistic neural nets by importance
sampling. In Proceedings of the Ninth International Workshop on Artificial Intelligence and
Statistics, AISTATS 2003, Key West, Florida, USA, January 3-6, 2003, 2003.
Yoshua Bengio and Jean-Sebastien Senecal. Adaptive importance sampling to accelerate training of
a neural probabilistic language model. IEEE Trans. Neural Networks, 19(4):713-722, 2008.
Jane Bromley, James W. Bentz, Leon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard
SaCkinger, and Roopak Shah. Signature verification using a ”siamese” time delay neural network.
International Journal of Pattern Recognition and Artificial Intelligence, 7(4):669-688, 1993.
Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. Large scale online learning of image
similarity through ranking. J. Mach. Learn. Res., 11:1109-1135, March 2010.
Wenlin Chen, David Grangier, and Michael Auli. Strategies for training large vocabulary neural
language models. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics, ACL 2016, 2016.
Alexandre de BrebiSSOn and Pascal Vincent. An exploration of softmax alternatives belonging to the
spherical loss family. CoRR, abs/1511.05042, 2016.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Process-
ing Systems 27, pp. 1646-1654. Curran Associates, Inc., 2014.
Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In Proceedings
of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ’16, pp. 855-864, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4232-2.
F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM
Transactions on Interactive Intelligent Systems, 2015.
Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback datasets.
In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, ICDM ’08, pp.
263-272, 2008.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In
Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in
Neural Information Processing Systems 27, pp. 2177-2185. Curran Associates, Inc., 2014.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representa-
tions in vector space. CoRR, abs/1301.3781, 2013.
Behnam Neyshabur and Nathan Srebro. On symmetric and asymmetric lshs for inner product search.
In Proceedings of the 32Nd International Conference on International Conference on Machine
Learning - Volume 37, ICML’15, pp. 1926-1934. JMLR.org, 2015.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543,
2014.
9
Published as a conference paper at ICLR 2019
Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as
matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the Eleventh
ACM International Conference on Web Search and Data Mining, WSDM '18,pp. 459-467, New
York, NY, USA, 2018. ACM. ISBN 978-1-4503-5581-0.
Steffen Rendle. Factorization machines. In Proceedings of the 2010 IEEE International Conference
on Data Mining, ICDM ’10, pp. 995-1000, Washington, DC, USA, 2010. IEEE Computer Society.
Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential
convergence rate for finite training sets. In Proceedings of the 25th International Conference on
Neural Information Processing Systems - Volume 2, NIPS’12, pp. 2663-2671, USA, 2012. Curran
Associates Inc.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Math. Program., 162(1-2):83-112, March 2017.
Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. Evaluation methods for
unsupervised word embeddings. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pp. 298-307, 2015.
F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A unified embedding for face recognition and
clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
815-823, June 2015.
Noam Shazeer, Ryan Doherty, Colin Evans, and Chris Waterson. Swivel: Improving embeddings by
noticing what’s missing. CoRR, abs/1602.02215, 2016.
Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner
product search (mips). In Proceedings of the 27th International Conference on Neural Information
Processing Systems - Volume 2, NIPS’14, pp. 2321-2329, Cambridge, MA, USA, 2014. MIT Press.
Pascal Vincent, Alexandre de Brebisson, and Xavier Bouthillier. Efficient exact gradient update for
training deep networks with very large sparse targets. In C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp.
1108-1116. Curran Associates, Inc., 2015.
Wikimedia Foundation. Wikimedia downloads. https://dumps.wikimedia.org/.
Doris Xin, Nicolas Mayoraz, Hubert Pham, Karthik Lakshmanan, and John R. Anderson. Folding:
Why good models sometimes make spurious recommendations. In Proceedings of the Eleventh
ACM Conference on Recommender Systems, RecSys ’17, pp. 201-209, New York, NY, USA, 2017.
ACM.
Hsiang-Fu Yu, Mikhail Bilenko, and Chih-Jen Lin. Selection of negative samples for one-class matrix
factorization. In Proceedings of the 2017 SIAM International Conference on Data Mining, pp.
363-371, 2017.
Xu Zhang, Felix X. Yu, Sanjiv Kumar, and Shih-Fu Chang. Learning spread-out local feature
descriptors. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy,
October 22-29, 2017, pp. 4605-4613, 2017.
10
Published as a conference paper at ICLR 2019
A Proofs
Proof of Proposition 1. Starting from the expression (6) of g(θ) = hGu(θ), Gv(θ)i, and applying
the chain rule, we have
Vg(θ) = VhGu(θ),Gv (θ)i
= Ju(θ)[Gv(θ)] + Jv(θ)[Gu(θ)],	(15)
where Ju(θ) denotes the Jacobian of Gu(θ), an order-three tensor given by
Ju(θ)l,i,j = '°Wj ,	l ∈{1,.. .,d},i,j ∈{1,...,n},
∂θl
and Ju (θ)[Gv (θ)] denotes the vector [Pi,j Ju(θ)l,i,jGv(θ)i,j]l∈{1,...,d}.
Observing that gi(θ,G u,Gv) = GGu u,Ui(θ) 0 %(θ)) +(G v ,vi(θ) 0 vi(θ))，and applying the
chain rule, we have
vθ gi(θ, G u, Gv ) = Ju,i(°)[Gv] + Jv,i(θ)[G^u],
where Ju,i(θ) is the Jacobian of %(θ) 0 Ui(θ), and
(16)
1n
E	[Ju,i(θ)] = - ɪ2 Ju,i(θ) = Ju(θ),
"Uniform	n
i=1
an similarly for Jv,i . We conclude by taking expectations in (16) and using assumption that Gu , Gv
are independent of i.	□
(t)
Proof of Proposition 2. From (8) and the definition of Su , we have,
Gut) = 1 XUj 0 Ut) + 1 Ui(θ(t)) 0 Ui(θ(t)),
un j jn
j6=i
which is a sum of matrices in the PSD cone Sf.	□
Proof of Proposition 3. Denoting by (Ft)t≥0 the filtration generated by the sequence (θ(t))t≥0, and
taking conditional expectations in (8), we have
E[GUt)∣Ft] = Sut) + . πEf	[Ui(θ(t)) 0 Ui(θ(t)) - u(t 0 U(t)∣Ft]
"Uniform
1n
=Sut) +——T[Ui(θ"))0 Ui(θ(t)) — Ui 0 Ui]
n i=1
1n
=—Eui(θ")) 0 Ui(θ(t)) = Gu(θ(t)).
n i=1
□
Proofof Proposition 4. (θ, Gu,Gv) ∈ Rd × Sf × Sf is a first-order stationary point of the game if
and only if
,. ,.-人一 .„ ʌ
Vf (θ) + λ(Ju(θ)[G v ] + Jv(θ)[G u]) = 0	(17)
DGu - Gu(θ),G0 - GuE ≥ 0, ∀G0 ∈Sf	(18)
DGv - Gv(θ),G0 - GvE ≥ 0,	∀G0 ∈ Sf	(19)
The second and third conditions simply states that VG L(Gu, Gv) and VG Lθ(Gu, Gv) define
supporting hyperplanes of Sfk at Gu , Gv , respectively.
Since Gu(θ) ∈ Sf, condition (18) is equivalent to Gu = Gu(θ) (and similarly, (19) is equivalent
to Gv = Gv (θ)). Using the expression (15) of Vg, we get that (17-19) is equivalent to Vf (θ) +
λVg(θ) = 0.	□
11
Published as a conference paper at ICLR 2019
Proof of Proposition 5. We start by proving the first bound (11). As stated in Section 3.2, we have, by
induction on t, Gu) = PT=1 at-τ uiτ (θ(t)) 0 Uiτ(θ(t", where aτ = α(1 - α)τ. And by definition
of G⑴，We have GUt) = PT =1 at-τGu(θ(τ)). Thus,
t
GUt)-GUt) = X at-τ∆Uτ)
τ=1
where ∆(Uτ) = uiτ (θ(τ)) 0 uiτ (θ(τ)) - GU(θ(τ)) are zero-mean random variables. Thus, taking the
second moment, and using the first assumption (which simply states that the variance of ∆(Uτ) is
bounded by σ2), we have
t
EkGUt) - GUt)kF = E Xat-τ∆Uτ)
τ=1
2t
= X at2-τ E k∆(Uτ)k2F
F	τ=1
≤ σ2α2 X(1 - α)2τ
τ=0
σ2α2
1 — (1 — a)2t
1 — (1 — α)2
≤ σ2
α
2 — α
which proves the first inequality (11).
To prove the second inequality, we start from the definition of GU)
t
kGUt) - GUt)kF = k X at-τ(G(U) — GUt))- (1 - α)tGUt)kF
τ=1
t
≤ Xat-τkG(Uτ) -G(Ut)kF+(1-α)tkG(Ut)kF,	(20)
τ=1
where the first equality uses that fact that Ptτ=1 at-τ = 1 - (1 - α)t. Focusing on the first term,
and bounding kG(Uτ ) - G(Ut) kF ≤ (t - τ)δ by the triangle inequality, we get
t-1
∑ at-τ kG(Uτ) - G(Ut))kF ≤ δ	at-τ (t - τ) = δα τ(1 - α)τ
τ=1
τ=1
δα(1 — a)-^-
dα
=δɑ(1 — a)-^-
dα
≤ δα(1 — α) ―2 .
Combining (20) and (21), we get the desired inequality (12).
τ=0
-X(1 - a)τ
τ=0
1 - (1 - α)t
(21)
t
t
	
α
□
12
Published as a conference paper at ICLR 2019
B Generalization to low-rank priors
So far, we have assumed a uniform zero prior to simplify the notation. In this section, we relax this
assumption. Suppose that the prior is given by a low-rank matrix P = QR>, where Q, R ∈ Rn×kP .
In other words, the prior for a given pair (i, j) is given by the dot product of two vectors pij = hqi, rji.
In practice, such a low-rank prior can be obtained, for example, by first training a simple low-rank
matrix approximation of the target similarity matrix.
Given this low-rank prior, the penalty term (2) becomes
nn
gp (θ) =滔 XX[Uθ Vθτ - QR>]2j
n i=1 j=1
=n (Uθ Vθτ — QRT, Uθ Vθ> - QRT)
=n2 [<U>Uθ,Vθ>Vθ〉- 2 (U>Q,VθrR) + c],
where c = QτQ, RτR is a constant that does not depend on θ. Here, we used a superscript P in
gP to disambiguate the zero-prior case.
Now, if we define weighted embedding matrices
[Hu(θ) := 1 UθQ = 1 Pi=1 ui(θ) 0 q
IHv(θ) := nvθR = n Pn=1 vi(θ) 0 ri,
the penalty term becomes
gP(θ) = hGu(θ), Gv (θ)i - 2 hHu(θ), Hv (θ)i + c.
Finally, if we maintain estimates Hu, Hv of Hu(θ), Hv (θ), respectively (using the methods proposed
in Section 3), We can approximate VgP (θ) by the gradient of
Tl ,	ʌ	ʌ	ʌ	ʌ
gP (θ,Gu,Gv ,HU ,Hv ):=
Dui(θ), GVUiME + Dvi(θ),GuVi(θ)) - 2 Dui(θ), HV%)— 2 Dvi(θ), HUriE . (22)
Proposition 1 and Algorithms 1 and 2 can be generalized to the loW-rank prior case by adding updates
for Hu,Hv, and by using expression (22) of gp when computing the gradient estimate.
Proposition 6. Ifi is drawn uniformly in {1, . . . , n}, and Gu, Gv, Hu, Hv are unbiased estimates of
Gu(θ), GV(θ), Hu(θ), HV(θ), respectively, then VθgP(θ, Gu, GV, Hu, HV) is an unbiased estimate
ofVgP(θ).
Proof. Similar to the proof of Proposition 1.
□
The generalized versions of SAGram and SOGram are stated below, where the differences compared
to the zero prior case are highlighted. Note that, unlike the Gramian matrices, the weighted embedding
matrices Hu , HV are not symmetric, thus we do not project their estimates.
13
Published as a conference paper at ICLR 2019
Algorithm 3 SAGram (Stochastic Average Gramian) with low-rank prior
1:	Input: Training data {(xi,yi, si)}i∈{1,...,n}, low-rank priors {qi, ri}i∈{1,...,n}
2:	Initialization phase
3:	draw θ randomly
4:	Ui J Ui(θ), Vi J Vi(θ) ∀i ∈ {1,...,n}
5:	Su J n1 Pn=I Ui % ui, Sv J 1 Pn=I Vi % Vi
6:	TU J n Pn=1 Ui 乳 qi,	TV J n Pn=1 Vi 乳 ri
7:	repeat
8:	Update Gramian estimates (i 〜Uniform(n))
9:	Gu	J	Su +	β[ui(θ) 0 ui(θ)	- ui 0 ui],	Gv	J	Sv	+ β[vi(θ) 0	vi(θ) - vi 0	vi]
10:	Update weighted embedding estimates
11:	HHu J Tu + λ[(ui(θ) — Ui) 0 qi]
12:	HHv J Tv + λ[(vi(θ) — Vi) 0 ri]
13:	Update model parameters then update caches (i 〜Uniform(n))
14:	θ J θ — ηVθ[fi(θ) + λpgP(θ, Gu, Gv, Hu, HV)]
15:	Su J Su + 1 [Ui(θ) 0 Ui(θ) - Ui 0 Ui], Sv J	Sv	+	1 [Vi(θ)	0 Vi(θ) - Vi 0 Vi]
i6：	tu J τu + n[(ui(θ) - IIi) 0 qi], τv J τv	+ n[(Vi(O)	-	Vi) 0	ri]
17:	^i J Ui(θ), Vi J Vi(θ)
18:	until stopping criterion
Algorithm 4 SOGram (Stochastic Online Gramian) with low-rank prior
1:	Input: Training data {(xi,yi,si)}i∈{1,...,n}, low-rank priors {qi, ri}i∈{1,...,n}
2:	Initialization phase
3:	draw θ randomly
4:	Gu,Gv J 0k×k
5:	repeat
6:	Update Gramian estimates (i 〜Uniform(n))
7:	Gu J (1 — α)Gu + αui(θ) 0 Ui(θ),	Gv J (1 — α)Gv + αVi(θ) 0 Vi(θ)
8:	Update weighted embedding estimates
9:	Hu J (1 — α)Hu + αUi (θ) 0 qi,	Hv J (1 — α)Hv + αVi(θ) 0 ri
10:	Update model parameters (i 〜 Uniform(n))
11:	θ J θ — ηVθ[fi(θ) + λpgP(θ, Gu, Gv, Hu, Hν)]
12:	until stopping criterion
C Non-uniform weights
In additional to using a non-uniform prior, it can also be desirable to use non-uniform weights in the
penalty term, for example to balance the contribution of frequent and infrequent items to the penalty
term. We discuss how to adapt our algorithms to the non-uniform weights case. Suppose that the
penalty function is given by
nn
gW (θ) = n XX aibj hui(o),Vj (θ)i2,
n i=1 j=1
where ai , bj are positive left and right weights, respectively. Here we used a superscript W in gW to
disambiguate the uniform-weight case. Then using a similar transformation to Section 2.2, we can
rewrite gW as follows:
gW(θ)
nX X
i=1
aiUi 0 Ui,
n x bj Vj0 G
j=1
i	.e. gW is the inner-product of two weighted Gramians. Both SAGram and SOGram can be
generalized to this case, by maintaining estimates of the weighted Gramians, one simply needs to
scale the contribution of each term Ui 0 Ui by the appropriate embedding weight ai (and similarly
for the right embedding).
14
Published as a conference paper at ICLR 2019
Remark Here we discussed the case of a rank-one weight matrix, i.e. when the unobserved weight
matrix can be written as W = a 0 b for a given left and right weight vectors a, b. The weight
matrix cannot be arbitrary (as specifying n2 individual weights is prohibitively expensive in many
applications such as the experiments of this paper), thus one needs a consice description of the weights
matrix. One such description is the sum of a sparse and low-rank matrix, and one can generalize
SAGram and SOGram to this case: the sparse part of the weight matrix can be optimized explicitly,
and the low-rank part can be optimized using weighted Gramians, by generalizing the argument of
the previous paragraph.
D Interpretations of the Gramian penalty term
In this section, we briefly discuss different interpretations of the Gramian inner-product g(θ). Starting
from the expression (4) of g(θ) and the definition (3) of the Gram matrices, we have
1n	1n
g(θ) = hGu(θ),Gv(θ)i = [- fu,(θ) 0 Ui(θ),Gv(θ)∖ = - Ehui(θ),Gv(θ)w(θ)i, (23)
n i=1	n i=1
which is a quadratic form in the left embeddings ui (and similarly for vj , by symmetry). In particular,
the partial derivative of the Gramian term with respect to an embedding ui is
∂g(θ)	2	2 -
∂~ = ~Gv(θ)ui(θ) = ^ ~ y?vj(θ)0 Vj(θ) ui(θ).
∂ui	n	n n
j=1
Each term (vj 0 vj)ui = vj hvj , uii is simply the projection of ui on vj (scaled by kvj k2). Thus
the gradient of g(θ) with respect to ui is an average of scaled projections of ui on each of the right
embeddings vj , and moving in the direction of the negative gradient simply moves ui away from
regions of the embedding space with a high density of right embeddings. This corresponds to the
intuition discussed in the introduction: the purpose of the g(θ) term is precisely to push left and right
embeddings away from each other, to avoid placing embeddings of dissimilar items near each other,
a phenomenon referred to as folding of the embedding space (Xin et al., 2017).
(a) λ = 10-2, observed pairs.	(b) λ = 10-2, random pairs.
(c) λ = 10, observed pairs.	(d) λ = 10, random pairs.
Figure 4: Evolution of the inner product distribution ui(θ(t)),vj(θ(t)) in the Wikipedia en model
trained with different penalty coefficients λ, for observed pairs (left) and random pairs (right).
15
Published as a conference paper at ICLR 2019
10«
Figure 5: Mean Average Precision of the Wikipedia en model, trained with different values of the
penalty coefficient λ.
In order to illustrate the effect of this term on the embedding distributions, we visualize, in Figure 4,
the distribution of the inner product ui(θ(t)), vj(θ(t)) , for random pairs (i,j), and for observed
pairs (i = j), and how these distributions change as t increases. The plots are generated for the
Wikipedia en model described in Section 4, trained with SOGram (α = 0.01), with two different
values of the penalty coefficient, λ = 10-2 and λ = 10. In both cases, the distribution for observed
pairs remains concentrated around values close to 1, as one expects (recall that the target similarity
is 1 for observed pairs, i.e. pairs of connected pages in the Wikipedia graph). The distributions for
random pairs, however, are very different: with λ = 10, the distribution quickly concentrates around
a value close to 0, while with λ = 10-2 the distribution is more flat, and a large proportion of pairs
have a high inner-product. This indicates that with a lower λ, the model is more likely to fold, i.e.
place embeddings of unrelated items near each other. This is consistent with the validation MAP,
reported in Figure 5. With λ = 10-2, the validation MAP increases very slowly, and remains two
orders of magnitude smaller than the model trained with λ = 10. The figure also shows that when λ
is too large (λ = 103), the model is over-regularized and the MAP decreases.
To conclude this section, we note that our methods also apply to a related regularizer introduced
in (Zhang et al., 2017), called Global Orthogonal Regularization. The authors argue that when learn-
ing feature embedding representations, spreading out the embeddings is helpful for generalization, and
propose to match the second moment of each embedding distribution with that of the uniform distribu-
tion. Formally, and using our notation, they use the penalty term max(gu(θ), 1/k)+max(gv (θ), 1/k),
where k is the embedding dimension, gu(θ) = n12 P2ι Pn=I d，Uji2, and Similarly for gv. They
optimize this term using candidate sampling. We can also apply the same Gramian transformation as
in Section 2.2 to write gu(θ) =hGu (θ), Gu (θ)i, and gv (θ) =hGv (θ), Gv (θ)i, and we can similarly
apply SAGram and SOGram to estimate both Gramians. Formally, the difference here is that one
would penalize the inner-product of each Gramian with itself, instead of the inner-product of the two.
One advantage of this regularizer is that it applies to a broader class of models, as it does not require
the output of the model to be the dot-product of two embedding functions.
E Further experiments on Wikipedia
E.1 Quality of gradient estimates
The experiments in Section 4 indicate that our methods give better estimates of the Gramians, and
a natural question is how this affects gradient estimation quality. First, one can make a formal
connection between the two. Since
∂g(θ)
∂ui
∂g(θ)
∂Ui
2
—GV (θ)ui(θ)
n
2G V Ui(θ),
n
16
Published as a conference paper at ICLR 2019
the estimation error of the gradient with respect to the left embeddings u is
n
l∣Vug - Vugk2 = — ^X k(Gv - Gv)Uill2
n2
i=1
n
=-2 ^X <(Gv - Gv )ui, (Gv - Gv )ui〉
n i=1
n
=-2 ^X (Gv - Gv, (Gv - GV )ui ③ Ui)
n i=1
=- DGv - Gv, (Gv - Gv)GuE .
This last expression can be interpreted as a Frobenius norm of the right Gramian estimation error
Gv 一 Gv, weighted by the left Gramian Gu, thus the gradient error is closely related to the Gramian
error. Figure 6 shows the gradient estimation quality on Wikipedia simple, measured by the
normalized squared norm gu：；|-Jugk2. The results are similar to the Gramian estimation errors
reported in Figure 2.
∣B∣ = 128
1-0
B 6 4 2
a ^_一
=o⅛n≥≥o⅛>ol⅛≥o
4	6
hours
10
0.8
∣B∣ = 1024
―■— SOGram, α=0.1
—	; SAGram, β=l∕n
—	∙— SAGram, β=l
-	uniform sampling
-	▼- linear sampling
-	▲一 sqrt sampling
8

Figure 6: Gradient estimation error on a common trajectory θ(t), with batch sizes |B| = 128 (left)
and |B| = 1024 (right).
E.2 Effect of the sampling distribution
Comparing the different baselines methods on simple (Figure 3), we observe that uniform
sampling performs better than sqrt, despite having a worse Gramian estimate according to Figure 2a.
One possible explanation is that the sampling distribution affects both the quality of the Gramian
estimates, and the frequency at which the item embeddings are updated, which in turn affects the
MAP. In particular, tail items are updated more frequently under uniform than other distributions,
and this may have a positive impact on the MAP.
E.3 EFFECT OF GRAMIAN LEARNING RATE α AND BIAS-VARIANCE TRADEOFF
In addition to the experiments of Section 4, we also evaluated the effect of the Gramian learning
rate α on the quality of the Gramian esimates and generalization performance on Wikipedia en.
Figure 7 shows the validation MAP of the SOGram method for different values of α (together with
the basline for reference). This reflects the bias-variance tradeoff dicussed in Proposition 5: with a
lower α, progress is initially slower (due to the bias introduced in the Gramian estimates), but the
final performance is better. Given a limited training time budget, this suggests that a higher α can be
preferable.
17
Published as a conference paper at ICLR 2019
Figure 7: Validation MAP of SOGram and linear importance sampling, on Wikipedia en, for different
values of the Gramian learning rates α.
We also evaluate the quality of the Gramian estimates, but due to the large vocabulary size in en,
computing the exact Gramians is no longer feasible, so we approximate it using a large sample of 1M
embeddings. The results are reported in Figure 8, which shows the normalized Frobenius distance
between the Gramian estimates Gu and (the large sample approximation of) the true Gramian Gu.
The results are similar to the experiment on simple: with a lower α, the estimation error is initially
high, but decays to a lower value as training progresses, which can be explained by the bias-variance
tradeoff discussed in Proposition 5.
1.0
→- linear SOGRam, α=0.001
→- linear SOGRam, α=0.0i
-∙- linear SOGRam, a=O.i
L-♦ - linear sampling
0.8
-*- linear SOGRam, α=0.001
→- linear SOGRam, α=0.0i
-∙- linear SOGRam, a=O.i
L-♦ - linear sampling
Figure 8: Gramian estimation error on en, for SOGram with different values of α, and different
learning rates. The left and right figures correspond respectively to η = 0.01 and η = 0.002.
The tradeoff is affected by the trajectory of the true Gramians: smaller changes in the Gramians
(captured by the parameter δ in Proposition 5) induce a smaller bias. In particular, changing the
learning rate η of the main algorithm can affect the performance of the Gramian estimates by affecting
the rate of change of the true Gramians. To investiage this effect, we ran the same experiment with
two different learning rates, η = 0.01 as in Section 4, and a lower learning rate η = 0.002. The errors
converge to similar values in both cases, but the error decay occurs much faster with smaller η, which
is consistent with our analysis.
E.4 Effect of batch size and learning rate
In this section, we explore the effect of the batch size |B | and learning rate η on the performance of
SOGram compared to the baselines. We ran the Wikipedia en experiment with different values of
these hyperparameters, and report the final validation MAP in Tables 3 and 4, which correspond to
batch size 128 and 512 respectively.
18
Published as a conference paper at ICLR 2019
	linear sampling	linear SOGram (α = 0.1)	linear SOGram (α = 0.01)	linear SOGram (α = 0.001)
η = 0.02	0.0896	0.1249 (+39.5%)	0.1256 (+40.2%)	0.1683 (+87.9%)
η=0.01	0.1325	0.1286 (-2.9%)	0.1301(-1.8%)	0.1710 (+29.1%)
η = 0.005	0.1290	0.1300 (+0.^8%)~	0.1270 (-1.5%)~	0.1385 (+7.4%)
Table 3: Final validation MAP on Wikipedia en, with batch size |B| = 128.
	linear sampling	linear SOGram (α = 0.1)	linear SOGram (α = 0.01)	linear SOGram (α = 0.001)
η = 0.02	0.0519	0.1331 (+156.4%)	0.1198 (+130.7%)	0.1295 (+149.3%)
η= 0.01	0.1371	-0.1329 (-3.0%)-	0.1505 (+9.8%)	0.1737 (+26.7%)
η= 0.005	0.1346	0.1323 (-1.7%)	0.1299 (-3.5%)	0.1569 (+16.6%)
Table 4: Final validation MAP on Wikipedia en, with batch size |B| = 512.
We can make several observations. First, the best performance is consistently achieved by SOGram
with learning rate α = 0.001. Second, the relative improvement compared to the baseline is, in
general, larger for smaller batch sizes. This can be explained intuitively by the fact that because
of online averaging, the quality of the Gramian estimates with SOGram suffers less than with the
sampling baseline. Finally, we can also observe that the final performance also seems more robust to
the choice of batch size and learning rate, compared to the baseline. For example, with the larger
learning rate η = 0.02, the performance degrades for all methods, but the drop in performance for
the baseline is much more significant than for the SOGram methods.
F Experiment on MovieLens data
In this section, we report experiments on a regression task on MovieLens.
Dataset The MovieLens dataset consists of movie ratings given by a set of users. In our notation,
the left features x represent a user, the right features y represent an item, and the target similarity is
the rating of movie y by user x. The data is partitioned into a training and a validation set using a
(80%-20%) split. Table 5 gives a basic description of the data size. Note that it is comparable to the
simple dataset in the Wikipedia experiments.
Dataset	# users	# movies	# ratings
MovieLens	72K	10K	10M
Table 5: Corpus size of the MovieLens dataset.
Model We train a two-tower neural network model, as described in Figure 1, where each tower
consists of an input layer, a hidden layer, and output embedding dimension k = 35. The left tower
takes as input a one-hot encoding of a unique user id, and the right tower takes as input one-hot
encodings of a unique movie id, the release year of the movie, and a bag-of-words representation
of the genres of the movie. These input embeddings are concatenated and used as input to the right
tower.
Methods The model is trained using a squared loss '(s, s0) = 2(S - s0)2, using SOGram with
different values of α, and sampling as a baseline. We use a learning rate η = 0.05, and penalty
coefficient λ = 1. We measure mean average precision on the trainig set and validation set, following
the same procedure described in Section 4. The results are given in Figure 9.
Results The results are similar to those reported on the Wikipedia simple dataset, which is
comparable in corpus size and number of observations to MovieLens. The best validation mean
average precision is achieved by SOGram with α = 0.1 (for an improvement of 2.9% compared to
the sampling baseline), despite its poor performance on the training set, which indicates that better
19
Published as a conference paper at ICLR 2019
0.0100
0.0095
0.0090
O 0.0085
g 0.0060
X
u
n
ta 0.875
0.87。
0.0065
0.0060
0
hours
0.012
Figure 9: Mean average precision at 10 on the training set (left) and the validation set (right), for
different methods, on the MovieLens dataset.
estimation of g(θ) induces better regularization. The impact on training speed is also remarkable in
this case, SOGram with α = 0.1 achieves a better validation performance in under 1 hour of training
than the sampling baseline in 6 hours.
20