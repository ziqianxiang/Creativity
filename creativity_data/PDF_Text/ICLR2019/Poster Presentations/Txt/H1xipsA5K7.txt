Published as a conference paper at ICLR 2019
Learning Two-layer Neural Networks with
Symmetric Inputs
Rong Ge
Computer Science Department
Duke University
rongge@cs.duke.edu
Zhize Li
Institute for Interdisciplinary Information Sciences
Tsinghua University
zz-li14@mails.tsinghua.edu.cn
Rohith Kuditipudi
Computer Science Department
Duke University
rohith.kuditipudi@duke.edu
Xiang Wang
Computer Science Department
Duke University
xwang@cs.duke.edu
Ab stract
We give a new algorithm for learning a two-layer neural network under a general
class of input distributions. Assuming there is a ground-truth two-layer network
y = Aσ(W x) +ξ,
where A, W are weight matrices, ξ represents noise, and the number of neurons in
the hidden layer is no larger than the input or output, our algorithm is guaranteed to
recover the parameters A, W of the ground-truth network. The only requirement
on the input x is that it is symmetric, which still allows highly complicated and
structured input.
Our algorithm is based on the method-of-moments framework and extends several
results in tensor decompositions. We use spectral algorithms to avoid the compli-
cated non-convex optimization in learning neural networks. Experiments show
that our algorithm can robustly learn the ground-truth neural network with a small
number of samples for many symmetric input distributions.
1	Introduction
Deep neural networks have been extremely successful in many tasks related to images, videos and
reinforcement learning. However, the success of deep learning is still far from being understood in
theory. In particular, learning a neural network is a complicated non-convex optimization problem,
which is hard in the worst-case. The question of whether we can efficiently learn a neural network
still remains generally open, even when the data is drawn from a neural network. Despite a lot of
recent effort, the class of neural networks that we know how to provably learn in polynomial time is
still very limited, and many results require strong assumptions on the input distribution.
In this paper we design a new algorithm that is capable of learning a two-layer1 neural network for
a general class of input distributions. Following standard models for learning neural networks, we
assume there is a ground truth neural network. The input data (x, y) is generated by first sampling
the input x from an input distribution D, then computing y according to the ground truth network that
is unknown to the learner. The learning algorithm will try to find a neural network f such that f (x)
is as close to y as possible over the input distribution D. Learning a neural network is known to be a
hard problem even in some simple settings (Goel et al., 2016; Brutzkus & Globerson, 2017), so we
need to make assumptions on the network structure or the input distribution D, or both. Many works
have worked with a simple input distribution (such as Gaussians) and try to learn more and more
1There are different ways to count the number of layers. Here by two-layer network we refer to a fully-
connected network with two layers of edges (two weight matrices). This is considered to be a three-layer
network if one counts the number of layers for nodes (e.g. in Goel & Klivans (2017)) or a one-hidden layer
network if one just counts the number of hidden layers.
1
Published as a conference paper at ICLR 2019
complex networks (Tian, 2017; Brutzkus & Globerson, 2017; Li & Yuan, 2017; Soltanolkotabi,
2017; Zhong et al., 2017). However, the input distributions in real life are distributions of very
complicated objects such as texts, images or videos. These inputs are highly structured, clearly not
Gaussian and do not even have a simple generative model.
We consider a type of two-layer neural network, where the output y is generated as
y = Aσ(W x) +ξ.	(1)
Here x ∈ Rd is the input, W ∈ Rk×d and A ∈ Rk×k are two weight matrices2. The function σ is
the standard ReLU activation function σ(x) = max{x, 0} applied entry-wise to the vector Wx, and
ξ is a noise vector that has E[ξ] = 0 and is independent of x. Although the network only has two
layers, learning similar networks is far from trivial: even when the input distribution is Gaussian,
Ge et al. (2017b) and Safran & Shamir (2018) showed that standard optimization objective can have
bad local optimal solutions. Ge et al. (2017b) gave a new and more complicated objective function
that does not have bad local minima.
For the input distribution D, our only requirement is that D is symmetric. That is, for any x ∈ Rd, the
probability of observing X 〜D is the same as the probability of observing -X 〜D. A symmetric
distribution can still be very complicated and cannot be represented by a finite number of parameters.
In practice, one can often think of the symmetry requirement as a “factor-2” approximation to an
arbitrary input distribution: if we have arbitrary training samples, it is possible to augment the input
data with their negations to make the input distribution symmetric, and it should take at most twice
the effort in labeling both the original and augmented data. In many cases (such as images) the
augmented data can be interpreted (for images it will just be negated colors) so reasonable labels
can be obtained.
1.1	Our Results
When the input distribution is symmetric, we give the first algorithm that can learn a two-layer
neural network. Our algorithm is based on the method-of-moments approach: first estimate some
correlations between X and y, then use these information to recover the model parameters. More
precisely we have
Theorem 1 (informal). If the data is generated according to Equation (1), and the input distribution
x 〜D is symmetric. Given exact correlations between x, y oforder at most 4, as long as A, W and
input distribution are not degenerate, there is an algorithm that runs in poly(d) time and outputs
a network A, W of the same size that is effectively the same as the ground-truth network: for any
input X, Aσ(WX) = Aσ(W X).
Of course, in practice we only have samples of (X, y) and cannot get the exact correlations. However,
our algorithm is robust to perturbations, and in particular can work with polynomially many samples.
Theorem 2 (informal). If the data is generated according to Equation (1), and the input distribution
x 〜D is symmetric. As long as the weight matrices A, W and input distributions are not degenerate,
there is an algorithm that uses poly(d, 1/) time and number of samples and outputs a network
A, W of the same size that computes an -approximation function to the ground-truth network: for
any input X, ∣∣Aσ(Wx) — Aσ(Wx)k2 ≤ 匕
In fact, the algorithm recovers the original parameters A, W up to scaling and permutations. Here
when we say weight matrices are not degenerate, we mean that the matrices A, W should be full
rank, and in addition a certain distinguishing matrix that we define later in Section 2 is also full rank.
We justify these assumptions using the smoothed analysis framework (Spielman & Teng, 2004).
In smoothed analysis, the input is not purely controlled by an adversary. Instead, the adversary can
first generate an arbitrary instance (in our case, arbitrary weight matrices W, A and symmetric input
distribution D), and the parameters for this instance will be randomly perturbed to yield a perturbed
instance. The algorithm only needs to work with high probability on the perturbed instance. This
limits the power of the adversary and prevents it from creating highly degenerate cases (e.g. choosing
the weight matrices to be much lower rank than k). Roughly speaking, we show
2Here we assume A ∈ Rk×k for simplicity, our results can easily be generalized as long as the dimension
of output is no smaller than the number of hidden units.
2
Published as a conference paper at ICLR 2019
Theorem 3 (informal). There is a simple way to perturb the input distribution, W and A such that
with high probability, the distance between the perturbed instance and original instance is at most
λ, and our algorithm outputs an e-approXimation to the perturbed network with poly(d, 1∕λ, 1/e)
time and number of samples.
In the rest of the paper, we will first review related works. Then in Section 2 we formally define
the network and introduce some notations. Our algorithm is given in Section 3. Finally in Section 4
we run experiments to show that the algorithm can indeed learn the two-layer network efficiently
and robustly. The experiments show that our algorithm works robustly with reasonable number of
samples for different (symmetric) input distributions and weight matrices. Due to space constraints,
the proof for polynomial number of samples (Theorem 2) and smoothed analysis (Theorem 3) are
deferred to the appendix.
1.2	Related Work
There are many works in learning neural networks, and they come in many different styles.
Non-standard Networks Some works focus on networks that do not use standard activation func-
tions. Arora et al. (2014) gave an algorithm that learns a network with discrete variables. Livni
et al. (2014) and follow-up works learn neural networks with polynomial activation functions. Oy-
mak & Soltanolkotabi (2018) used the rank-1 tensor decomposition for learning a non-overlapping
convolutional neural network with differentiable and smooth activation and Gaussian input.
ReLU network, Gaussian input When the input is Gaussian, Ge et al. (2017b) showed that for a
two-layer neural network, although the standard objective does have bad local optimal solutions, one
can construct anew objective whose local optima are all globally optimal. Several other works (Tian,
2017; Du et al., 2017b; Brutzkus & Globerson, 2017; Li & Yuan, 2017; Soltanolkotabi, 2017; Zhong
et al., 2017; Zhang et al., 2018) extend this to different settings.
General input with score functions A closely related work (Janzamin et al., 2015) does not
require the input distribution to be Gaussian, but still relies on knowing the score function of the
input distribution (which in general cannot be estimated efficiently from samples). Recently, Gao
et al. (2018) gave a way to design loss functions with desired properties for one-hidden-layer neural
networks with general input distributions based on a new proposed local likelihood score function
estimator. For general distributions (including symmetric ones) their estimator can still require
number of samples that is exponential in dimension d (as in Assumption 1(d)).
General input distributions There are several lines of work that try to extend the learning results
to more general distributions. Du et al. (2017a) showed how to learn a single neuron or a single
convolutional filter under some conditions for the input distribution. Daniely et al. (2016); Zhang
et al. (2016; 2017); Goel & Klivans (2017); Du & Goel (2018) used kernel methods to learn neural
networks when the norm of the weights and input distributions are both bounded (and in general
the running time and sample complexity in this line of work depend exponentially on the norms of
weights/input). Recently, Du et al. (2018) showed that gradient descent minimizes the training error
in an over-parameterized two-layer neural network. They only consider training error while our
results also apply to testing error. The work that is most similar to our setting is Goel et al. (2018),
where they showed how to learn a single neuron (or a single convolutional filter) for any symmetric
input distribution. Our two-layer neural network model is much more complicated.
Method-of-Moments and Tensor Decomposition Our work uses method-of-moments, which has
already been applied to learn many latent variable models (see Anandkumar et al. (2014) and refer-
ences there). The particular algorithm that we use is inspired by an over-complete tensor decompo-
sition algorithm FOOBI (De Lathauwer et al., 2007). Our smoothed analysis results are inspired by
Bhaskara et al. (2014) and Ma et al. (2016), although our setting is more complicated and we need
several new ideas.
3
Published as a conference paper at ICLR 2019
2	Preliminaries
In this section, we first describe the neural network model that we learn, and then introduce notations
related to matrices and tensors. Finally we will define distinguishing matrix, which is a central object
in our analysis.
2.1	Network Model
We consider two-layer neural networks with d-dimensional input, k hidden units and k-dimensional
output, as shown in Figure 1. We assume that k ≤ d. The input of the neural network is denoted
by x ∈ Rd. Assume that the input x is i.i.d. drawn from a symmetric distribution D3 . Let the
two weight matrices in the neural network be W ∈ Rk×d and A ∈ Rk×k . The output y ∈ Rk is
generated as follows:
y = Aσ(W x) +ξ,	(2)
where σ(∙) is the element-wise ReLU function and ξ ∈ Rk is zero-mean random noise, which is
independent with input x. Let the value of hidden units be h ∈ Rk, which is equal to σ(Wx).
Denote i-th row of matrix W as wi> (i = 1, 2, ..., k). Also, let i-th column of matrix A be ai
(i = 1, 2, ..., k). By property of ReLU activations, for any constant c > 0, scaling the i-th row of
W by c while scaling the i-th column of A by 1/c does not change the function computed by the
network. Therefore without loss of generality, we assume every row vector of W has unit norm.
y=Ah+ξ
A ∈ Rk×k
h = σ(W x) ∈ Rk
W ∈ Rk×d
Figure 1: Network model.
X〜D
2.2	Notations
We use [n] to denote the set {1, 2,…，n}. For two random variables X and Y, We say X == Y if
they come from the same distribution.
In the vector space Rn, we use〈•，•〉to denote the inner product of two vectors, and use k ∙ k to denote
the Euclidean norm. We use ei to denote the i-th standard basis vector. For a matrix A ∈ Rm×n , let
A[i,:] denote its i-th row vector, and let A[:,j] denote its j-th column vector. Let A’s singular values
be σι(A) ≥ σ2(A) ≥ ∙∙∙ ≥ °mE(m,n)(A), and denote the smallest singular value be σmin(A)=
σmin(m,n)(A). The condition number of matrix A is defined as K(A) := σ1(A)∕σmin(A). We use
In to denote the identity matrix with dimension n × n. The spectral norm of a matrix is denoted as
k ∙ k, and the Frobenius norm as ∣∣ ∙ ∣∣f.
We represent a d-dimensional linear subspace S by a matrix S ∈ Rn×d, whose columns form
an orthonormal basis for subspace S . The projection matrix onto the subspace S is denoted by
ProjS = SS>, and the projection matrix onto the orthogonal subspace ofS is denoted by ProjS⊥ =
In - SS> .
For matrix A ∈ Rm1×n1,C ∈ Rm2×n2, let the Kronecker product of A and C be A 0 C ∈
Rm1m2×n1n2, which is defined as (A 0 C)(i1,i2),(j2,j2) = Ai1,i2CjIj2. For a vector X ∈ Rd, the
Kronecker product X 0 X has dimension d2. We denote the p-fold Kronecker product of X as χ0p,
which has dimension dp .
We often need to convert between vectors and matrices. For a matrix A ∈ Rm×n, let vec(A) ∈ Rmn
be the vector obtained by stacking all the columns of A. For a vector a ∈ Rm2, let mat(X) ∈
Rm×m denote the inverse mapping such that vec(mat(a)) = a. Let Rsky×mk be the space of all k × k
3Suppose the density function of distribution D is p(∙), we assume p(x) = p(-x) for any X ∈ Rd
4
Published as a conference paper at ICLR 2019
symmetric matrices, which has dimension k2 + k. For convenience, we denote k2 = k2 . For a
symmetric matrix B ∈ Rk×k, We denote vec*(B) ∈ Rk2+k as the vector obtained by stacking all
the upper triangular entries (including diagonal entries) of B. Note that vec(B) still has dimension
k2. For a vector b ∈ Rk2+k, let mat*(b) ∈ Rf×k denote the inverse mapping of vec*(∙) such that
vec*(mat* (B)) = b.
2.3	Distinguishing Matrix
A central object in our analysis is a large matrix Whose columns are closely related to pairs of hidden
variables. We call this the distinguishing matrix and define it beloW:
Definition 1. Given a weight matrix W of the first layer, and the input distribution D, the distin-
guishing matrix ND ∈ Rd2×k2 is a matrix whose columns are indexed by ij where 1 ≤ i < j ≤ k,
and
ND = Ex〜D [(w>x)(w>x)(x 0 x)l{w>xw>X ≤ 0}].
Another related concept is the augmented distinguishing matrix M, which is a d2 × (k2 + 1) matrix
whose first k2 columns are exactly the same as distinguishing matrix N, and the last column (indexed
by 0) is defined as
MD = Ex 〜D [x 0 x].
For both matrices, when the input distribution is clear from context we use N or M and omit the
superscript.
The exact reason for these definitions Will only be clear after We explain the algorithm in Section 3.
Our algorithm Will require that these matrices are robustly full rank, in the sense that σmin (M)
is loWerbounded. Intuitively, every column NiDj looks at the expectation over samples that have
opposite signs for Weights wi, wj (wi> xwj> x ≤ 0, hence the name distinguishing matrix).
Requiring M and N to be full rank prevents several degenerate cases. For example, if tWo hidden
units are perfectly correlated and alWays share the same sign for every input, this is very unnatural
and requiring the distinguishing matrix tobe full rank prevents such cases. Later in Section C We Will
also shoW that requiring a loWerbound on σmin (M) is not unreasonable: in the smoothed analysis
setting Where the nature can make a small perturbation on the input distribution D, We shoW that for
any input distribution D, there exists simple perturbations D0 that are arbitrarily close to D such that
σmin (M D0 ) is loWerbounded.
3	Our Algorithm
In this section, We describe our algorithm for learning the tWo-layer netWorks defined in Section 2.1.
As a Warm-up, We Will first consider a single-layer neural netWork and recover the results in Goel
et al. (2018) using method-of-moments. This Will also be used as a crucial step in our algorithm. Due
to space constraints We Will only introduce algorithm and proof ideas, the detailed proof is deferred
to Section A in appendix. Throughout this section, when we use EH without further specification
the expectation is over the randomness X 〜D and the noise ξ.
3.1	Warm-up: Learning S ingle-layer Networks
We will first give a simple algorithm for learning a single-layer neural network. More precisely,
suppose we are given samples (x1,y1),…，(xn, yn) where Xi 〜D comes from a symmetric distri-
bution, and the output yiis computed by
yi= σ(w>xi) + ξi.	(3)
Here ξi's are i.i.d. noises that satisfy E[ξi] = 0. Noise ξi is also assumed to be independent with
input xi. The goal is to learn the weight vector w.
The idea of the algorithm is simple: we will estimate the correlations between x and y and the
covariance ofx, and then recover the hidden vector w using these two estimates. The main challenge
here is that y is not a linear function on x. Goel et al. (2018) gave a crucial observation that allows
us to deal with the non-linearity:
5
Published as a conference paper at ICLR 2019
Algorithm 1 Learning Single-layer Neural Networks
Input: Samples (x1, y1), ..., (xn, yn) generated according to Equation (3).
Output: Estimate of weight vector w.
1:	Estimate V = 1 Pn=1 yixi.
2:	Estimate C = 1 Pn=1 xix>
3:	return 2C-1v.
Lemma 1. Suppose X ~ D comes from a symmetric distribution and y is computed as in (3), then
E[yx] = ^E[xx>]w.
Importantly, the right hand side of Lemma 1 does not contain the ReLU function σ. This is true
because if x comes from a symmetric distribution, averaging between x and -x can get rid of non-
linearities like ReLU or leaky-ReLU. Later we will prove a more general version of this lemma
(Lemma 6).
Using this lemma, it is immediate to get a method-of-moments algorithm for learning w: we just
need to estimate E[yx] and E[xx>], then we know w = 2(E[xx>])-1E[yx]. This is summarized in
Algorithm 1.
3.2	Learning Two-layer Networks
In order to learn the weights of the network defined in Section 2.1, a crucial observation is that we
have k outputs as well as k hidden-units. This gives a possible way to reduce the two-layer problem
to the single-layer problem. For simplicity, we will consider the noiseless case in this section, where
y = Aσ(Wx).	(4)
Let u ∈ Rk be a vector and consider u>y, it is clear that u>y = (u>A)σ(W x). Let zi be the
normalized version i-th row of A-1, then we know zi has the property that zi>A = λi ei> where
λi > 0 is a constant and ei is a basis vector.
The key observation here is that if u = zi, then u>A = λi ei> . As a result, u>y = λiei>σ(W x) =
σ(λiwi>x) is the output of a single-layer neural network with weight equal to λiwi. If we know
all the vectors {z1, ..., zk}, the input/output pairs (x, zi>y) correspond to single-layer networks with
weight vectors {λiwi}. We can then apply the algorithm in Section 3.1 (or the algorithm in Goel
et al. (2018)) to learn the weight vectors.
When u> A = λiei , we say that u>y is a pure neuron. Next we will design an algorithm that can
find all vectors {zi }’s that generate pure neurons, and therefore reduce the problem of learning a
two-layer network to learning a single-layer network.
Pure Neuron Detector In order to find the vector u that generates a pure neuron, we will try to
find some property that is true if and only if the output can be represented by a single neuron.
Intuitively, using ideas similar to Lemma 1 we can get a property that holds for all pure neurons:
Lemma 2. Suppose y = σ(w>x), then E[y2] = 1 w>E[xx>]w, and E[yx] = 11 E[xx>]w. As a
result we have
E[y2 ] = 2E[yxτ]E[xxτ]-1E[yx].
As before, the ReLU activation does not appear because of the symmetric input distribution. For
y = uτy, We can estimate all of these moments (E[y2], E[yxτ], E[xxτ]) using samples and check
whether this condition is satisfied. However, the problem with this property is that even if z = uτy
is not pure, it may still satisfy the property. More precisely, if y = Pk=I Ciσ(wτx), then we have
2E[yxτ]E[xxτ]-1E[yx] - E[y2]= ^X CicjE [(wjx)(wjrx) l{wτxwjx ≤ 0}]
1≤i<j≤k
6
Published as a conference paper at ICLR 2019
The additional terms may accidentally cancel each other which leads to a false positive. To address
this problem, we consider a higher order moment:
Lemma 3. Suppose y = σ(w>x), then
E[y2(x 0 x)] = 2E [y ∙ (E[yx>]E[xx>]-1x) ∙ (x 0 x)].
Moreover, if y = Pk=I gσ(w>x) where C ∈ Rk is a k-dimensional vector, we have
2E [y ∙ (E[yx>]E[xx>]-1x) ∙ (x 0 x)] — E[y2(x 0 x)] =	^X CicjNj.
1≤i<j≤k
Here Nij ’s are columns of the distinguishing matrix defined in Definition 1.
The important observation here is that there are k2 =	k2 extra terms in
2E [y ∙ (E[yx>]E[xx>]-1x) ∙ (x 0 x)] — E[y2(x 0 x)] that are multiples of Nj, which are
d2 (or d+2 1 considering their symmetry) dimensional objects. When the distinguishing matrix is
full rank, we know its columns Nij are linearly independent. In that case, if the sum of the extra
terms is 0, then the coefficient in front of each Nij must also be 0. The coefficients are Ci Cj which
will be non-zero if and only if both Ci , Cj are non-zero, therefore to make all the coefficients 0 at
most one of {Ci } can be non-zero. This is summarized in the following Corollary:
Corollary 1 (Pure Neuron Detector). Define f (u) := 2E [(u>y) ∙ (E[(u>y)x>]E[xx>]-1x) ∙ (x 0 x)]
— E[(u>y)2 (x 0 x)]. Suppose the distinguishing matrix is full rank, iff(u) = 0 for unit vector u,
then u must be equal to one of±zi.
We will call the function f(u) a pure neuron detector, as u>y is a pure neuron if and only iff(u) =
0. Therefore, to finish the algorithm we just need to find all solutions for f(u) = 0.
Linearization The main obstacle in solving the system of equations f(u) = 0 is that every entry
of f(u) is a quadratic function in u. The system of equations f(u) = 0 is therefore a system
of quadratic equations. Solving a generic system of quadratic equations is NP-hard. However, in
our case this can be solved by a technique that is very similar to the FOOBI algorithm for tensor
decomposition (De Lathauwer et al., 2007). The key idea is to linearize the function by thinking of
each degree 2 monomial uiuj as a separate variable. Now the number of variables is k2+k = k2 +k
and f is linear in this space. In other words, there exists a matrix T ∈ Rd2 ×(k2 +k) such that
Tvec*(uu>) = f (u). Clearly, if u>y is a pure neuron, then Tvec*(uu>) = f (U) = 0. That is,
{vec* (ziz>)} are all in the nullspace of T. Later in Section A We will prove that the nullspace of T
consists of exactly these vectors (and their combinations):
Lemma 4. Let T be the unique Rd2×(k2+k) matrix that satisfies Tvec*(uu>) = f (u) (Where f (u)
is defined as in Corollary 1), suppose the distinguishing matrix is full rank, then the nullspace ofT
is exactly the span of {vec*(ziz>)}.
Based on Lemma 4, we can just estimate the tensor T from the samples we are given, and its smallest
singular directions would give US the span of {vec* (ziz>)}.
Finding zi’s from span of zizi> ’s In order to reduce the problem to a single-layer problem, the
final step is to find zi’s from span ofzizi>’s. This is also a step that has appeared in FOOBI and more
generally other tensor decomposition algorithms, and can be solved by a simultaneous diagonaliza-
tion. Let Z be the matrix whose rows are zi’s, which means Z = diag(λ)A-1. Let X = Z>DXZ
and Y = Z>DY Z be two random elements in the span ofzizi>, where DX and DY are two random
diagonal matrices. Both matrices X and Y can be diagonalized by matrix Z. In this case, if we
compute XY -1 = Z>DXDY-1(Z>)-1, since zi is a column of Z>, we know
W-1 〜 _7> n _ /ɔ-1( 7> ∖ -1 ~	_ 7> n _ n-1 一	—	DX (i, i) >T C	_	DX (i, i) ~
XY Zi = Z DX DY (Z ) Zi	= Z DX DY ei	=	~Z^^飞 Z ei	=	~z∙^^TYzi.
Y	Y	DY (i, i)	DY (i, i)
That is, Zi is an eigenvector ofX Y -1 ! The matrix X Y -1 can have at most k eigenvectors and there
are k Zi’s, therefore the Zi’s are the only eigenvectors of X Y -1.
7
Published as a conference paper at ICLR 2019
Lemma 5. Given the span of zizi> ’s, let X, Y be two random matrices in this span, with probability
1 the zi ’s are the only eigenvectors of XY -1.
Using this procedure we can find all the zi ’s (up to permutations and sign flip). Without loss of
generality we assume zi>A = λiei> . The only remaining problem is that λi might be negative.
However, this is easily fixable by checking E[zi>y] = λiE[σ(wi>x)]. Since σ(wi>x) is always
nonnegative, E[zi>y] has the same sign as λi, and we can flip zi if E[zi>y] is negative.
3.3 Detailed Algorithm and Guarantees
We can now give the full algorithm, see Algorithm 2. The main steps of this algorithm is as ex-
plained in the previous section. Steps 2 - 5 constructs the pure neuron detector and finds the span of
vec* (ziz>) (as in Corollary 1); Steps 7 - 9 performs simultaneous diagonalization to get all the z，s;
Steps 11, 12 calls Algorithm 1 to solve the single-layer problem and outputs the correct result.
Algorithm 2 Learning Two-layer Neural Networks
Input: Samples (x1, y1), ..., (xn, yn) generated according to Equation (4)
Output: Weight matrices W and A.
1:	{Finding span of vec(zizi> )’s}
2:	Estimate empirical moments E[xx>], E[yx>], E[y 0 x03] and E[y 0 y 0 (X 0 x)].
3:	Compute the vector f(u) = 2E [(uτy) ∙ (E[(uτy)xτ]E[xxτ]-1x) ∙ (x0x)] -E [(uτy)2(x0x)]
where each entry is expressed as a degree-2 polynomial over u.
4:	Construct matrix T ∈ Rd2×(k2+k) such that, Tvec*(uuτ) = f (u).
5:	Compute the k-least right singular vectors of T, denoted by vec* (Ui), vec*(U2),…，vec* (Uk).
Let S be a k2 + k by k matrix, where the i-th column of S is vector vec*(Ui). {Here span S is
equal to span of {vec*(zizτ)}.}
6:	{Finding zi ’s from span}
7:	Let X = mat*(SZι), Y = mat*(SZ2), where Zi and Q are two independently sampled k-
dimensional standard Gaussian vectors.
8:	Let zi, ..., zk be eigenvectors ofXY-i.
9:	For each Zi, if E[zτy] < 0 let Zi《-zi. {Here zi's are normalized rows of A-1.}
10:	{Reduce to 1-Layer Problem}
11:	For each Zi, let vi be the output of Algorithm 1 with input (xi, Ziτyi), ..., (xn, Ziτyn).
12:	Let Z be the matrix whose rows are Ziτ ’s, V be the matrix whose rows are viτ ’s.
13:	return V , Z-i.
We are now ready to state a formal version of Theorem 1:
Theorem 4. Suppose A, W, E[xxτ] and the distinguishing matrix N are all full rank, and Algorith-
m 2 has access to the exact moments, then the network returned by the algorithm computes exactly
the same function as the original neural network.
It is easy to prove this theorem using the lemmas we have.
Proof. By Corollary 1, we know that after Step 5 of Algorithm 2, the span of columns of S is
exactly equal to the span of {vec*(zizτ)}. By Lemma 5, We know the eigenvectors of XYT at
Step 8 are exactly the normalized version of rows of A-i. Without loss of generality, we will fix
the permutation and assume ZiτA = λieiτ . In Step 9, we use the fact that E[Ziτy] = λiE[σ(wiτx)]
where E[σ(wiτx)] is always positive because σ is the ReLU function. Therefore, after Step 9 we
can assume all the λi ’s are positive.
Now the output Ziτy = λiσ(wiτx) = σ(λiwiτx) (again by property of ReLU function σ), by
the design of Algorithm 1 we know vi = λiwi. We also know that Z = diag(λ)A-i, therefore
ZT = Adiag(λ)-i. Notice that Z-1σ(Vx) = Adiag(λ)-1σ(diag(λ)Wx) = Aσ(Wx). These
two scaling factors cancel each other, so the two networks compute the same function.	□
8
Published as a conference paper at ICLR 2019
Figure 2: Error in recovering W , A and outputs (“MSE”) for different numbers of training samples
and different dimensions of W and A. Each point is the result of averaging across five trials, where
on the left W and A are both drawn as random 10 × 10 orthonormal matrices and in the center as
32 × 32 orthonormal matrices. On the right, given 10, 000 training samples we plot the square root
of the algorithm’s error normalized by the dimension of W and A, which are again drawn as random
orthonormal matrices. The input distribution is a spherical Gaussian.
4	Experiments
In this section, we provide experimental results to validate the robustness of our algorithm for both
Gaussian input distributions as well as more general symmetric distributions such as symmetric
mixtures of Gaussians.
There are two important ways in which our implementation differs from our description in Sec-
tion 3.3. First, our description of the simultaneous diagonalization step in our algorithm is mostly
for simplicity of both stating and proving the algorithm. In practice we find it is more robust to draw
10k random samples from the subspace spanned by the last k right-singular vectors of T and com-
pute the CP decomposition of all the samples (reshaped as matrices and stacked together as a tensor)
via alternating least squares (Comon et al., 2009). As alternating least squares can also be unstable
we repeat this step 10 times and select the best one. Second, once we have recovered and fixed A
we use gradient descent to learn W , which compared to Algorithm 1 does a better job of ensuring
the overall error will not explode even if there is significant error in recovering A. Crucially, these
modifications are not necessary when the number of samples is large enough. For example, given
10,000 input samples drawn from a spherical Gaussian and A and W drawn as random 10 × 10
orthogonal matrices, our implementation of the original formulation of the algorithm was still able
to recover both A and W with an average error of approximately 0.15 and achieve close to zero
mean square error across 10 random trials.
4.1	Sample Efficiency
First we show that our algorithm does not require a large number of samples when the matrices are
not degenerate. In particular, we generate random orthonormal matrices A and W as the ground
truth, and use our algorithm to learn the neural network. As illustrated by Figure 2, regardless of
the size of W and A our algorithm is able to recover both weight matrices with minimal error so
long as the number of samples is a few times of the number of parameters. To measure the error
in recovering A and W, we first normalize the columns of A and rows of W for both our learned
parameters and the ground truth, pair corresponding columns and rows together, and then compute
the squared distance between learned and ground truth parameters. Note in the rightmost plot of
Figure 2, in order to compare the performance between different dimensions, we further normalize
the recovering error by the dimension of W and A. It shows that the squared root of normalized
error remains stable as the dimension of A and W grows from 10 to 32. In Figure 2, we also show
the overall mean square error-averaged over all output Units-achieved by our learned parameters.
4.2	Robustness to Noise
Figure 3 demonstrates the robustness of our algorithm to label noise ξ for Gaussian and symmetric
mixture of Gaussians input distributions. In this experiment, we fix the size of both A and W to
be 10 × 10 and again generate both parameters as random orthonormal matrices. The overall mean
square error achieved by our algorithm grows almost perfectly in step with the amount of label noise,
9
Published as a conference paper at ICLR 2019
Figure 3: Error in recovering W, A and outputs (“MSE”) for different amounts of label noise. Each
point is the result of averaging across five trials with 10,000 training samples, where for each trial
W and A are both drawn as 10 × 10 orthonormal matrices. The input distribution on the left is a
spherical Gaussian and on the right a mixture of two Gaussians with one component based at the
all-ones vector and the other component at its reflection.
Figure 4: Error in recovering W , A and outputs (“MSE”), on the left for different levels of con-
ditioning of W and on the right for A. Each point is the result of averaging across five trials with
20,000 training samples, where for each trial one parameter is drawn as a random orthonormal ma-
trix while the other as described in Section 4.3. The input distribution is a mixture of Gaussians with
two components, one based at the all-ones vector and the other at its reflection.
indicating that our algorithm recovers the globally optimal solution regardless of the choice of input
distribution.
4.3	Robustness to Condition Number
We’ve already shown that our algorithm continues to perform well across a range of input distri-
butions and even when A and W are high-dimensional. In all previous experiments however, we
sampled A and W as random orthonormal matrices so as to control for their conditioning. In this
experiment, we take the input distribution to be a random symmetric mixture of two Gaussians and
vary the condition number of either A or W by sampling singular value decompositions UΣV >
such that U and V are random orthonormal matrices and Σii = λ-i, where λ is chosen based on the
desired condition number. Figure 4 respectively demonstrate that the performance of our algorithm
remains steady so long as A and W are reasonably well-conditioned before eventually fluctuating.
Moreover, even with these fluctuations the algorithm still recovers A and W with sufficient accuracy
to keep the overall mean square error low.
5	Conclusion
Optimizing the parameters of a neural network is a difficult problem, especially since the objective
function depends on the input distribution which is often unknown and can be very complicated. In
this paper, we design a new algorithm using method-of-moments and spectral techniques to avoid the
10
Published as a conference paper at ICLR 2019
complicated non-convex optimization for neural networks. Our algorithm can learn a network that is
of similar complexity as the previous works, while allowing much more general input distributions.
There are still many open problems. The current result requires output to have the same (or higher)
dimension than the hidden layer, and the hidden layer does not have a bias term. Removing these
constraints are are immediate directions for future work. Besides the obvious ones of extending our
results to more general distributions and more complicated networks, we are also interested in the
relations to optimization landscape for neural networks. In particular, our algorithm shows there is
a way to find the global optimal network in polynomial time, does that imply anything about the
optimization landscape of the standard objective functions for learning such a neural network, or
does it imply there exists an alternative objective function that does not have any local minima? We
hope this work can lead to new insights for optimizing a neural network.
Acknowledgement
This work was supported by NSF CCF-1704656.
11
Published as a conference paper at ICLR 2019
References
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor
decompositions for learning latent variable models. The Journal of Machine Learning Research,
15(1):2773-2832, 2014.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some
deep representations. In International Conference on Machine Learning, pp. 584-592, 2014.
Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analy-
sis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on Theory
of computing, pp. 594-603. ACM, 2014.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
A Carbery and J Wright. Distributional and lq norm inequalities for polynomials over convex bodies
in Rn. Mathematical research letters, 8(3):233-248, 2001.
Pierre Comon, Xavier Luciani, and Andre LF De Almeida. Tensor decompositions, alternating least
squares and other tales. Journal of Chemometrics: A Journal of the Chemometrics Society, 23
(7-8):393-405, 2009.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Lieven De Lathauwer, Josphine Castaing, and Jean-Franois Cardoso. Fourth-order cumulant-based
blind identification of underdetermined mixtures. IEEE Transactions on Signal Processing, 55
(6):2965-2973, 2007.
Simon S Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks
with overlaps. arXiv preprint arXiv:1805.07798, 2018.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv
preprint arXiv:1709.06129, 2017a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent
learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint arX-
iv:1712.00779, 2017b.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Weihao Gao, Ashok Vardhan Makkuva, Sewoong Oh, and Pramod Viswanath. Learning one-hidden-
layer neural networks under general input distributions. arXiv preprint arXiv:1810.04133, 2018.
Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimen-
sions. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp.
761-770. ACM, 2015.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017a.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017b.
Surbhi Goel and Adam Klivans. Learning depth-three neural networks in polynomial time. arXiv
preprint arXiv:1709.06010, 2017.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polyno-
mial time. arXiv preprint arXiv:1611.10258, 2016.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping
patches. arXiv preprint arXiv:1802.02547, 2018.
12
Published as a conference paper at ICLR 2019
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guar-
anteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training
neural networks. In Advances in Neural Information Processing Systems, pp. 855-863, 2014.
Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-of-
squares. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on,
pp. 438-446. IEEE, 2016.
Samet Oymak and Mahdi Soltanolkotabi. End-to-end learning of a convolutional neural network via
deep tensor decomposition. arXiv preprint arXiv:1805.06523, 2018.
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences, 62(12):1707-1739, 2009.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, 2018.
Mahdi Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information
Processing Systems, pp. 2007-2017, 2017.
Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. Journal of the ACM (JACM), 51(3):385-463, 2004.
Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares
problems. SIAM review, 19(4):634-662, 1977.
GW Stewart and JG Sun. Computer science and scientific computing. matrix perturbation theory,
1990.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu
networks via gradient descent. arXiv preprint arXiv:1806.07808, 2018.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. '1-regularized neural networks are improperly
learnable in polynomial time. In International Conference on Machine Learning, pp. 993-1001,
2016.
Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of fully-
connected neural networks. In Artificial Intelligence and Statistics, pp. 83-91, 2017.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
13
Published as a conference paper at ICLR 2019
A Details of Exact Analysis
In this section, We first provide the missing proofs for the lemmas appeared in Section 3. Then We
discuss how to handle the noise case (i.e. y = σ(Wx) + ξ) and give the corresponding algorithm
(Algorithm 3). At the end we also briefly discuss how to handle the case when the matrix A has
more roWs than columns (more outputs than hidden units).
Again, throughout the section when we write E[∙], the expectation is taken over the randomness of
x 〜D and noise ξ.
A.1 MISSING PROOFS FOR SECTION 3
Single-layer: To get rid of the non-linearities like ReLU, we use the property of the symmetric
distribution (similar to (Goel et al., 2018)). Here we provide a more general version (Lemma 6)
instead of proving the specific Lemma 1. Note that Lemma 1 is the special case when a = w and
p = q = 1 (here ξ does not affect the result since it has zero mean and is independent with x, thus
E[yx] = E[σ(w>x)x]).
Lemma 6. Suppose input X comes from a symmetric distribution, for any vector a ∈ Rd and any
non-negative integers P and q satisfying that P + q is an even number, we have
E[(σ(α>x))px 瓯]=∣ E[(a>x)px 瓯],
where the expectation is taken over the input distribution.
Proof. Since input X comes from a symmetric distribution, we know that E[(σ(ατx))px0q] =
E[(σ(-aτx))p(-x)0q]. Thus, we have
E[(σ(aTX))PX瓯]=∣ (E[(σ(-aTX))P(-x)18q] + E[(σ(aTX))PX18q]).
There are two cases to consider: p and q are both even numbers or both odd numbers.
1.	For the case where p and q are even numbers, we have
1 (E[(σ(-aTX))P(-x)18q ] + E[(σ(aTX))PX瓯])
=1 (E[(σ(-aTX))PX瓯]+ E[(σ(aTX))PX瓯])
=1 E[((σ(-aTX))P + (σ(aTX))P)x18q].
If (aTx) ≤ 0, we know (σ(-aTX))P + (σ(aTX))P = (aTx)P + 0 = (aTx)P. Otherwise,
we have (σ(-aTX))P + (σ(aTX))P = 0 + (aTx)P = (aTx)P. Thus,
E[(σ(aTX))P X 的]=:E[((σ(-aTX))P + (σ(aTX))P)X 的]
=1 E[(aTX)PXl8q ].
2.	For the other case where p and q are odd numbers, we have
1 (E[(σ(-aTX))P(-x)lq ] + E[(σ(aTX))PXIq ])
=1 E[( - (σ(-aTX))P + (σ(aTX))P)XIq].
Similarly, if (aTx) ≤ 0, we know -(σ(-aTX))P + (σ(aTX))P = -(-aTX)P + 0 =
(aTx)p. Otherwise, we have -(σ(-aTX))P + (σ(aTX))P = 0 + (aTx)P = (aTx)p.
Thus,
E[(σ(aTX))PXIq] = ；e[( - (σ(-aTX))P + (σ(aTX))P)XIq]
=1 E[(aTX)PXlq].
14
Published as a conference paper at ICLR 2019
□
Pure neuron detector: The first step in our algorithm is to construct a pure neuron detector based
on Lemma 2 and Lemma 3. We will provide proofs for these two lemmas here.
Proof of Lemma 2. This proof easily follows from Lemma 6. Setting a = w, p = 2 and q = 0 in
Lemma 6, We have E[y2] = ɪwτE[xxτ]w. Similarly, We also know E[yxτ] = ɪwτE[xxτ] and
E[yx] = 2E[xxτ]w. Thus, we have E[y2] = 2E[yxτ]E[xxτ]-1E[yx].	□
Proof of Lemma 3. Here, we only prove the second equation, since the first equation is just a
special case of the second equation. First, we rewrite y = Pk=I ɑσ(wTX) = uτy by letting
UTA = cτ. Then we transform these two terms in the LHS as follows. Let,s look at E [(uτy) ∙
(E[(uτy)xτ]E[xxτ]-1x) ∙ (x 0 x)] first. For E[(uτy)xτ]E[xxτ]-1, we have
E[(uτy)xτ]E[xxτ] 1
=E[(uτAσ(WX))XT]E[xxτ]-1
=guτAW E[xxτ]E[xxτ] 1
=1 UTAW
2
For any vector q ∈ Rd, consider 2E [(uτy) ∙ (qτx) ∙ (x 0 x)]. We have
2E[(uτy) ∙ (qτx) ∙ (x 0 x)]
=2E[(uτAσ(Wx))(qTX)(X 0 x)]
Let qτ = E [(uTy)XT] E [xxτ] 1 = 1 uτAW, we have
2E[(uτy) ∙ (E[(uTy)XT]E[xxτ]-1x) ∙ (x 0 x)]
=:E[(uTAWX)2(x 0 x)]
=:E[{Aτu, Wx>2(x 0 x)]
=2 ^X (Aτu)2E[(wτx)2(x 0 x)] + ^X (Aτu)i(Aτu)jE[(wτx)(wτx)(x 0 x)]
1<i<k	1≤i<j≤k
=g ^X (AτuuτA)iiE[(wτx)2(x 0 x)] +	^X (AτuuτA)ijE[(wτx)(wjx)(x 0 x)].
1≤i≤k	1≤i<j≤k
(5)
where the second equality holds due to Lemma 6.
Now, let,s look at the second term E [(uτy)2 (x 0 x)].
=E[(uτAσ(Wx))2(x 0 x)]
=E[hAτu, σ(Wx)>2(x 0 x)]
=^X (ATU)2E[σ(wJ x)2(x 0 x)] +2 ^X (Aτu)i(Aτu)j E[σ(wτ x)σ(wj x)(x 0 x)]
1≤i≤k	1≤i<j≤k
=^X (ATUUTA)iiE[σ(wJx)2(x 0 x)] + 2 ^X (ATUUTA)ijE[σ(wJx)σ(w/x)(x 0 x)]
1≤i≤k	1≤i<j≤k
=g ^X (AτUUTA)iiE[(wJx)2(x 0 x)] + 2 ^X (AτUUTA) jE[σ(wJx)σ(wτx)(x 0 x)].
2 1≤i≤k	1≤i<j≤k
(6)
15
Published as a conference paper at ICLR 2019
Now, we subtract (5) by (6) to obtain
2E[(uτy) ∙ (E[(uτy)x>]E[xx>]-1x) ∙ (x 0 x)] — E[(uτy)2(x 0 x)]
=	^X (ATuuTA) jE[(wTX)(WTX)(X 0 x)] — 2 ^X (ATuuτA)ijE[σ(wτx)σ(w>χ)(χ 0 x)]
1≤i<j≤k	1≤i<j≤k
=	^X (ATUUTA) jE[[(wTX)(WTX)(X 0 x)l{wTXwTX ≤ θ}]	(7)
1≤i<j≤k
=X (ATUUTA)j Nij,	⑻
1≤i<j≤k
where (7) uses (9) of the following Lemma 7, and (8) uses the definition of distinguishing matrix N
(Definition 1).	□
Lemma 7. Given input X comingfrom a symmetric distribution, for any vector a, b ∈ Rd, we have
^E[(aTx)(bTx)] — E[a(aTx)a(bTx)] = ^E[(aTx)(bTx)l{αTXbTX ≤ 0}]
and
；E[(aTx)(bTX)(X0x)]-E[σ(αTx)σ(bTX)(X0x)] = ；E[(aTx)(bTX)(X0x)l{αTXbTX ≤ 0}],
(9)
where the expectation is taken over the input distribution.
Proof. Here we just prove the first identity, because the proof of the second one is almost identical.
First, we rewrite 2E[(aTx)(bTx)] as follows
；E[(aTx)(bTx)] = ；E[(aTx)(bTX)l{aTxbTx ≤ 0}] + ；E[(aTx)(bTX)l{αTXbTX > 0}].
Thus, we only need to show that ɪE[(aTx)(bTX)l{aTxbTx > 0}] = E[a(aTx)a(bTx)]. It,s
clear that
E[a(aTx)a(bTx)] = E[a(aTx)a(bTx)l{αTXbTX > 0}].
Since input X comes from a symmetric distribution, we have
E[a(aTx)a(bTx)l{αTXbTX > 0}] =E[σ(-aTx)a(—bTx)l{αTXbTX > 0}]
=；(E[0(aTx)o(bTX)l{aTXbTX > 0}]
+ E[σ(-aTx)σ(-bTx)l{αTXbTX > 0}])
=gEh(a(aTx)σ(bTX) + σ(-&Tx)σ(-bTX))I{αTXbTX > 0}]
=；E[(aTx)(bTX)l{αTXbTX > 0}].
When aTXbTX > 0, we know aTx and bTx are both positive or both negative. In either case, we
know that a(aTx)σ(bTX) + σ(-aTx)o(-bTX) = (aTx)(bTx). Thus, we have
E[a(aTx)a(bTx)] =E [a(aTx)a(bTx)l{αTXbTX > 0}]
=；E[(aTx)(bTX)l{αTXbTX > 0}],
which finished our proof.
□
Finding span: Now, we find the span of {vec*(ZizT)}. First, we recall that f (u) = 2E[(uTy) ∙
(E[(uTy)XT]E[xxt]-1x) ∙ (x 0 x)] — E[(uty)2(x 0 x)]. Then, according to (8), we have
f (u) = 2E[(uτy)∙(E[(uTy)XT]E[xxτ]-1x)∙(x0x)]-E[(uτ y)2(x0x)] =	^X (Aτuuτ A)j Nj
1≤i<j≤k
16
Published as a conference paper at ICLR 2019
It is not hard to verify that u>y is a pure neuron if and only if f(u) = 0. Note that f (u) = 0 is a
system of quadratic equations. So we linearize it by increasing the dimension (i.e., consider uiuj as
a single variable) similar to the FOOBI algorithm. Thus the number of variable is k2 + k = k2 + k,
i.e.,
∃T ∈ Rd2×(k2+k) such that Tvec*(U) = f (U)=	X (A>UA)ijNij.	(10)
1≤i<j≤k
Now, We prove the Lemma 4 which shows the null space of T is exactly the span of {vec* (ziz>)}.
Proof of Lemma 4. We divide the proof to the following two cases:
1.	For any vector vec*(U) belongs to the null space of T, we have Tvec*(U) = 0. Note that
the RHS of (10) equals to 0 if and only if A> UA is a diagonal matrix since the distinguish-
ing matrix N is full column rank and A>UA is symmetric. Thus vec*(U) belongs to the
span of {vec* (ziz>)} since U = Z>DZ for some diagonal matrix D.
2.	For any vector vec*(U) belonging to the span of {vec*(ziz>)}, U is a linear combination
of ziz>'s. Furthermore, Tvec* (U) is a linear combination of Tvec* (ziz>). Note that A>Zi
only has one non-zero entry due to the definition of zi, for any i ∈ [k]. Thus all coefficients
in the RHS of (10) are 0. We get Tvec*(U) = 0.
Finding Zi's: Now, we prove the final Lemma 5 which finds all zis from the span of {vec* (ziz>)}
by using simultaneous diagonalization. Given all zi ’s, this two-layer network can be reduced to a
single-layer one. Then one can use Algorithm 1 to recover the first layer parameters wi’s.
Proof of Lemma 5. As we discussed before this lemma, we have XY -1 = Z>DXDY-1(Z>)-1.
According to the following Lemma 8 (i.e., all diagonal elements ofDxDy-1 are non-zero and distinc-
t), the matrix XY -1 have k eigenvectors and there are k zi’s, therefore zi’s are the only eigenvectors
ofXY-1.
Lemma 8. With probability 1, all diagonal elements of DX and DY are non-zero and all diagonal
elements of DX DY-1 are distinct, where X = Z>DXZ and Y = Z>DY Z are defined in Line 7 of
Algorithm 2.
Proof. First, we know there exist diagonal matrices {D% : i ∈ [k]} such that mat* (vec* (Ui))=
Z>DiZ for all i ∈ [k], where {vec*(Ui) : i ∈ [k]} are the k-least right singular vectors of T
(see Line 5 of Algorithm 2). Then, let the vector di ∈ Rk be the diagonal elements of Di , for all
i ∈ k. Let matrix Q ∈ Rk×k be a matrix where its i-th column is di. Then DX = diag(Qζ1) and
DY = diag(Qζ2), where ζ1 and ζ2 are two random k-dimensional standard Gaussian vectors (see
Line 7 of Algorithm 2).
Since {vec*(Ui) : i ∈ [k]} are singular vectors of T, we know di, d2,…，dk are independent.
Thus, Q has full rank and none of its rows are zero vectors. Let i-th row of Q be qi> . Let’s consider
DX first. In order for i-th diagonal element of DX to be zero, we need qi>ζ1 = 0. Since qi is not
a zero vector, we know the solution space of qiζ1 = 0 is a lower-dimension manifold in Rk, which
has zero measure. Since finite union of zero-measure sets still has measure zero, the event that zero
valued elements exist in the diagonal of DX or DY happens with probability zero.
If i-th and j-th diagonal elements of DX DY-1 have same value, we have qi>ζ1(qi>ζ2)-1 =
qj>ζ1(qj>ζ2)-1. Again, we know the solution space is a lower-dimensional manifold in R2k space,
with measure zero. Since finite union of zero-measure sets still has measure zero, the event that
duplicated diagonal elements exist in DX D- happens with probability zero.	□
A.2 Noisy Case
Now, we discuss how to handle the noisy case (i.e. y = σ(W x) + ξ). The corresponding algorithm
is described in Algorithm 3. Note that the noise ξ only affects the first two steps, i.e., pure neuron
detector (Lemma 3) and finding span ofvec*(zizi>) (Lemma 4). It does not affect the last two steps,
i.e., finding zi ’s from the span (Lemma 5) and learning the reduced single-layer network. Because
17
Published as a conference paper at ICLR 2019
Algorithm 3 Learning Two-layer Neural Networks with Noise
Input: Samples (x1, y1), ..., (xn, yn) generated according to Equation (2)
Output: Weight matrices W and A.
1:	{Finding span ofvec*(z%z>)}
2:	Use first half of samples (i.e. {(xi, yi)}n=2) to estimate empirical moments E[xx>], E[yx>],
E[yy 1 ], E[y 0 x03] and E[y 0 y 0 (X 0 x)].
3:	Compute the vector f(u) = 2E[(uτy) ∙ (E[(uτy)xτ]E[xxτ]-1x) ∙ (X 0 x)] — E[(uτy)2(x 0
x)] + (E[(uτy)2] — 2E[(uτy)xτ]E[xxτ] 1E[(uτy)x])E[x 0 x] where each entry is ex-
pressed as a degree-2 polynomial over u.
4:	Construct matrix T ∈ Rd2×(k2+k) such that, Tvec*(UuT) = f (u).
5:	Compute the k-least right singular vectors of T, denoted by vec* (Ui), vec*(U2),…，vec* (Uk).
Let S be a k2 + k by k matrix, where the i-th column of S is vector vec*(Ui). {Here span S is
equal to span of {vec*(zizτ)}.}
6:	{Finding zi ’s from span}
7:	Let X = mat*(SZι), Y = mat*(SZ2), where Zi and Z2 are two independently sampled k-
dimensional standard Gaussian vectors.
8:	Let zi, ..., zk be eigenvectors ofXY-i.
9:	For each zi, use the second half of samples {(xi, yi)}rn=n∕2+1 to estimate E[zτ y]. If E[zτ y] < 0
let Zi《-----Zi. {Here zi's are normalized rows of A-1.}
10:	{Reduce to 1-Layer Problem}
11:	For each Zi, let vi be the output of Algorithm 1 with input {(xj, Ziτyj)}jn=n∕2+i.
12:	Let Z be the matrix whose rows are Ziτ ’s, V be the matrix whose rows are viτ ’s.
13:	return V , Z-i.
Lemma 5 is independent of the model and Lemma 1 is linear wrt. noise ξ, which has zero mean and
is independent of input x.
Many of the steps in Algorithm 3 are designed with the robustness of the algorithm in mind. For
example, in step 5 for the exact case we just need to compute the null space ofT. However ifwe use
the empirical moments the null space might be perturbed so that it has small singular values. The
separation of the input samples into two halves is also to avoid correlations between the steps, and
is not necessary if we have the exact moments.
Modification for pure neuron detector: Recall that in the noiseless case, our pure neuron detector
contains a term E[(uτy)2(x 0 x)], which causes a noise square term in the noisy case. Here, we
modify our pure neuron detector to cancel the extra noise square term. In the following lemma, we
state our modified pure neuron detector in Equation 11, and give it a characterization.
Lemma 9. Suppose y = Aσ(Wx) + ξ,for any u ∈ Rk, we have
f (u) := 2E[(uTy) ∙ (E[(uTy)XT]E[xxτ]-1x) ∙ (x 0 x)] — E[(uτy)2(x 0 x)]
+ E[(uτy)2] — 2E[(uτy)xτ]E[xxτ]-iE[(uτy)x]E[x 0 x]	(11)
= X (Aτ uuτ A)ij Nij — X (Aτuuτ A)ij mij E[x 0 x],	(12)
i≤i<j≤k	i≤i<j≤k
where Nij ’s are columns of the distinguishing matrix (Definition 1), and mij	:=
E [(w7x)(wτx)1{wjxw;x ≤ 0}],
We defer the proof of this lemma to the end of this section. Recall that the augmented distinguishing
matrix M consists of the distinguishing matrix N plus column E[x 0 x]. Now, we need to assume
the augmented distinguishing matrix M is full rank.
Modification for finding span: For Lemma 4, as we discussed above, here we assume the augment-
ed distinguishing matrix M is full rank. The corresponding lemma is stated as follows (the proof is
exactly the same as previous Lemma 4):
18
Published as a conference paper at ICLR 2019
Lemma 10. Let T be the unique Rd2×(k2+k) matrix that satisfies Tvec*(uu>) = f(u) (defined in
Equation 11), suppose the augmented distinguishing matrix is full rank, then the nullspace of T is
exactly the span of {vec*(ziz>)}.
Similar to Theorem 4, we provide the following theorem for the noisy case. The proof is almost the
same as Theorem 4 by using the noisy version lemmas (Lemmas 9 and 10).
Theorem 5. Suppose E[xx>], A, W and the augmented distinguishing matrix M are all full rank,
and Algorithm 3 has access to the exact moments, then the network returned by the algorithm com-
putes exactly the same function as the original neural network.
Now, we only need to prove Lemma 9 to finish this noise case.
Proof of Lemma 9. Similar to (5) and (6), we deduce these three terms in RHS of (11) one by one
as follows. For the first term, it is exactly the same as (5) since the expectation is linear wrt. ξ. Thus,
we have
2E[(u>y) ∙ (E[(u>y)x>]E[xx>]-1x) ∙ (x 0 x)]
=-^X (A>uu>A)iiE[(w>x)2(x 0 x)] + ^X (A>uu>A)ijE[(w>x)(w>x)(x 0 x)].
2 1≤i≤k	1≤i<j≤k
(13)
Now, let’s look at the second term E (u>y)2(x 0 x) which is slightly different from (6) due to the
noise ξ. Particularly, we add the third term to cancel this extra noise square term later.
=E[(u>(Aσ(W x) + ξ))2(x 0 x)]
=E[(u>Aσ(W x))2(x 0 x)] + E[(u>ξ)2(x 0 x)]
二；^X (A>uu>A)iiE[(w>x)2(x 0 x)] +2 ^X (A> uu>A)jE[σ(w>x)σ(w>x)(x 0 x)]
1≤i≤k	1≤i<j≤k
(14)
+ E[(u>ξ)2(x 0 x)],	(15)
where (15) uses (6).
For the third term, we have
E[(u>y)2] - 2E[(u>y)x>]E[xx>]-1E[(u>y)x]
=E[(u>Aσ(Wx))2] + E[(u>ξ)2] - 1 E[hA>u,Wx>2]
=^X (A>uu>A)iiE[σ(w>x)2] — — ^X (A>uu>A)iiE[(w>x)2]
1≤i≤k	2 1≤i≤k
+ 2 X (A>uu>A)ijE[σ(wi>x)σ(wj>x)] - X (A> uu> A)ij E[(wi> x)(wj> x)]
1≤i<j≤k	1≤i<j≤k
+ E[(u>ξ)2]
= - X (A>uu>A)ij E[(w> x)(w> x) l{w> xw> X ≤ 0}] + E[(u>ξ)2]
1≤i<j≤k
= - X (A>uu>A)ijmij + E[(u>ξ)2],	(16)
1≤i<j≤k
where the third equality holds due to Lemma 6 and Lemma 7, and (16) uses the definition of mij.
19
Published as a conference paper at ICLR 2019
Algorithm 4 Learning Two-layer Neural Networks with Non-square A
Input: Samples (x1, y1), ..., (xn, yn) generated according to Equation (2).
Output: Weight matrices W ∈ Rk×d and A ∈l×k.
1:	Using half samples (i.e. {(xi, yi)}n=2) to estimate empirical moments E[yχ>].
2:	Let P be a l X k matrix, which columns are left singular vectors of E[yx>].
3:	Run Algorithm 3 on samples {(xi, P>yi)}in=n/2. Let the output of Algorithm 3 be V, Z-1.
4:	return V , PZ-1.
Finally, We combine these three terms (13-16) as follows:
f(u) = 2E[(u>y) ∙ (E[(u>y)x>]E[xx>]-1x) ∙ (x 0 x)] — E[(u>y)2(x 0 x)]
+(E[(u>y)2] — 2E[(u>y)x>]E[xx>] 1E[(u>y)x])E[x 0 x]
= X (A> uu> A)ij E[(wi> x)(wj> x)(x 0 x)]
1≤i<j≤k
— 2 X (A>uu> A)ij E[σ (wi> x)σ (wj> x)(x 0 x)] — E[(u>ξ)2]E[x 0 x]
1≤i<j≤k
+ — X (A> uu> A)ij mij + E[(u>ξ)2] E[x 0 x]
1≤i<j≤k
= X (A> uu> A)ij Nij — X (A>uu> A)ij mij E[x 0 x],	(17)
1≤i<j≤k	1≤i<j≤k
where (17) uses (9) (same as (7)).
A.3 EXTENSION TO NON-SQUARE A
In this paper, for simplicity, we have assumed that the dimension of output equals the number of
hidden units and thus A is a k × k square matrix. Actually, our algorithm can be easily extended to
the case where the dimension of output is at least the number of hidden units. In this section, we give
an algorithm for this general case, by reducing it to the case where A is square. The pseudo-code is
given in Algorithm 4.
Theorem 6. Suppose E[xx>], W, A and the augmented distinguishing matrix M are all full rank,
and Algorithm 4 has access to the exact moments, then the network returned by the algorithm com-
putes exactly the same function as the original neural network.
Proof. Let the ground truth parameters be A ∈ Rl×k and W ∈ Rk×d . The samples are generated by
y = Aσ(Wx) + ξ, where the noise ξ is independent with input x. We have E[yx>] = 2AWE[xx>].
Since both W and E[xx>] are full-rank, we know the column span ofE[yx>] are exactly the column
span of A. Furthermore, we know the columns of P is a set of orthonormal basis for the column
span of A.
For a ground truth neural network with weight matrices W and P>A, the generated sample will
just be (x, P>y). According to Theorem 5, we know for any input x, we have Z-1σ(V x) =
P>Aσ(Wx). Thus, we have
PZ-1σ(Vx) = PP>Aσ(Wx) =Aσ(Wx),
where the second equality holds since PP> is just the projection matrix to the column span of
A.	□
B	Robustness of Main Algorithm
In this section we will show that even if we do not have access to the exact moments, as long as
the empirical moments are estimated with enough (polynomially many) samples, Algorithm 2 and
20
Published as a conference paper at ICLR 2019
Algorithm 3 can still learn the parameters robustly. We will focus on Algorithm 3 as it is more
general, the result for Algorithm 2 can be viewed as a corollary when the noise ξ = 0. Throughout
this section, we will use V , Z-1 to denote the results of Algorithm 3 with empirical moments, and
use V, Z-1 for the results when the algorithm has access to exact moments, similarly for other
intermediate results. For the robustness of Algorithm 3, we prove the following theorem.
Theorem 7. Assume that the norms of x, ξ, A are bounded by kxk ≤ Γ, kξk ≤ P2, kAk ≤ P1, the
covariance matrix and the weight matrix are robustly full rank: σmin (E[xx>]) ≥ γ, σmin (A) ≥ β.
Further assume that the augmented distinguishing matrix has smallest singular values σmin (M) ≥
α. For any small enough e ,forany δ < 1, given poly (Γ, P1,P2,d, 1/e, 1∕γ, 1∕ɑ, 1∕β, 1∕δ) number
of i.i.d. samples, let the output of Algorithm 3 be V , Z-1, we know with probability at least 1 - δ,
kAσ(Wx) — Z-1σ(Vx)k ≤ e,
for any input x.
In order to prove the above Theorem, we need to show that each step of Algorithm 3 is robust. We
can divide Algorithm 3 into three steps: finding the span of vec* (ziz>)'s; finding Zi's from the span
of vec* (ziz> )'s; recovering first layer using Algorithm 1. We will first state the key lemmas that
prove every step is robust to noise, and finally combine them to show our main theorem.
First, we show that with polynomial number of samples, we can approximate the span of
vec* (ziz> )‘s in arbitrary accuracy. Let T be the empirical estimate of T, which is the pure neu-
ron detector matrix as defined in Algorithm 3. As shown in Lemma 10, the null space of T is
exactly the span of vec* (ziz> )'s. We use standard matrix perturbation theory (see Section D.2) to
show that the null space of T is robust to small perturbations. More precisely, in Lemma 11, we
show that with polynomial number of samples, the span of k least singular vectors of T is close to
the null space of T.
Lemma 11. Under the same assumptions as in Theorem 7, let S ∈ R(k2+k)×k be the matrix whose
k columns are the k least right singular vectors of T. Similarly define S ∈ R(k2+k)×k for empirical
estimate T. Thenfor any E ≤ γ∕2,forany δ < 1, given O(d-r-(rP1 AYk+P2) log(δ)) numberofi.i.d.
samples, we know with probability at least 1 — δ,
kss> - SS>k ≤ √∣.
The proof of the above lemma is in Section B.1. Basically, we need to lowerbound the spectral
gap (k2-th singular value of T) and to upperbound the Frobenius norm of T 一 T. Standard matrix
perturbation bound shows that if the perturbation is much smaller than the spectral gap, then the null
space is preserved.
Next, We show that We can robustly find Zi's from the span of vec* (ziz> )'s. Since this step of the
algorithm is the same as the simultaneous diagonalization algorithm for tensor decompositions, we
use the robustness of simultaneous diagonalization (Bhaskara et al., 2014) to show that we can find
zi’s robustly. The detailed proof is in Section B.2.
Lemma 12. Suppose that ∣∣SS> — SS>∣∣f ≤ 3 ∣∣A∣∣ ≤ Pi, ∣∣ξ∣∣ ≤ P2,σmin(E[xx>]) ≥
Y,σmin(A) ≥ β. Let X = mat*(SZ1), Y = mat*(SZ2), where Zi and Z2 are two inde-
pendent standard Gaussian vectors. Let zi, ∙∙∙ Zk be the normalized row vectors of A-1. Let
zi,..., Zk be the eigenvectors of XY 1 (after sign flip). For any δ > 0 and small enough G with
O( (rP1√k+p2) log(d∕δ)) number ofi.i.d. samples in E[Z>y], with probability at least 1 — δ over the
randomness ofζi , ζ2 and i.i.d. samples, there exists a permutation π(i) ∈ [k] such that
IlZi-Z∏[i]k ≤ poly(d,Pι, 1∕β,e, 1∕δ),
for any 1 ≤ i ≤ k.
Finally, given Zi's, the problem reduces to a one-layer problem. We will first give an analysis for
Algorithm 1 as a warm-up. When we call Algorithm 1 from Algorithm 3, the situation is slightly
different. Note we reserve fresh samples for this step, so that the samples used by Algorithm 1 are
21
Published as a conference paper at ICLR 2019
still independent with the estimate Zi (learned using the other set of samples). However, since Zi is
not equal to Zi, this introduces an additional error term (Zi - Zi)Ty which is not independent of X
and cannot be captured by ξ. We modify the proof for Algorithm 1 to show that the algorithm is still
robust as long as ∣∣Zi - Zik is small enough.
Lemma 13. Assume that kxk ≤ Γ, kAk ≤ P1, kξk ≤ P2 and σmin(E[xxT]) ≥ γ. Suppose that
for each 1 ≤ i ≤ k, k^i — Zik ≤ τ. Thenfor any E ≤ γ∕2 and δ < 1, given O((r +Pγr∣2lo^δ))
number of samples for Algorithm 1, we know with probability at least 1 - δ,
ll r<2τ (ΓPι√k + P2)Γ
kvi - Vik ≤-----------------+ 2e,
γ
for each 1 ≤ i ≤ k.
Combining the above three lemmas, we prove Theorem 7 in Section B.4.
B.1	Robust Analysis for Finding the Span of {vec*(Ziz>)}’s
We first prove that the step of finding the span of {vec*(Ziz>)} is robust. The main idea is based
on standard matrix perturbation bounds (see section D.2). We first give a lowerbound on the k2-th
singular value ofT, giving a spectral gap between the smallest non-zero singular value and the null
space. see the lemma below. The proof is given in section B.1.1.
Lemma 14. Suppose σmin (M) ≥ α, σmin(A) ≥ β, we know that matrix T has rank k2 and the
k2-th singular value ofT is lower bounded by αβ2.
Then We show that with enough samples the estimate T is close enough to T, so Wedin,s Theorem
(Lemma 25) implies the subspace found is also close to the true nullspace ofT . The proof is deferred
to section B.1.2.
Lemma 15. Assume that kxk ≤ Γ, kAk ≤ P1, kξk ≤ P2 and σmin (E[xxT]) ≥ γ > 0, then for any
E ≤ γ∕2 ,for any 1 > δ > 0, given O( d-ɪ—(rP1 ]k+P2) log(δ)) number of i.i.d. samples, we know
kT - TkF ≤ E,
with probability at least 1 - δ.
Finally we combine the above two lemmas and show that the span of the least k right singular vectors
of T is close to the null space of T.
Lemma 11. Under the same assumptions as in Theorem 7, let S ∈ R(k2+k)×k be the matrix whose
k columns are the k least right singular vectors of T. Similarly define S ∈ R(k2+k)×k for empirical
estimate T. Thenfor any E ≤ γ∕2,forany δ < 1, given O(d-ɪ—(rp1 AYk+P2) log(δ)) numberofi.i.d.
samples, we know with probability at least 1 - δ,
∣ss> - ss>∣∣ ≤ αβ∣.
n , A j∙ . τ___________IU . „ ^zd3Γ14(ΓPι√k+P2)6 log© 八_______L	__[
Proof. According to Lemma 15, given O(-------γ^2---≡^δz) number of i.i.d. samples, We
know with probability at least 1 - δ,
∣t - T∣∣f ≤ E∙
According to Lemma 14, we know σk2 (T) ≥ αβ 2 . Then, due to Lemma 27, we have
∣ss> - SS>k ≤
√2kT - TkF
σk2 (T)
√2e
≤ αβ2.
□
22
Published as a conference paper at ICLR 2019
M ∈ Rd2×(k2+1)	recall that k2 := (2)
Figure 5: Characterize T as the product of four matrices.
B.1.1	LOWERBOUNDING k2-TH SINGULAR VALUE OF T
In order to lowerbound the k2-th singular value of T , we first express T as the product of four
simpler matrices, T = MBCF , as illustrated in Figure 5. The definitions of these four matrices
M ∈ Rd2×(k2+1), B ∈ R(k2+1)×k2, C ∈ R(k2×k2), F ∈ Rk2 ×(k2+k) will be introduced later as we
explain their effects. From Lemma 9, we know that
Tvec*(U)=	X (A>UA)ijMij- ( X (A>UA)ijmij)E[x 乳 x],
1≤i<j≤k	1≤i<j≤k
for any symmetric k × k matrix U.
Note that vec*(U) is a (k2 + k)-dimensional vector. For convenience, We first use matrix F to
transform vec*(U) to Vec(U), which has k2 dimensions. Matrix F is defined such that Fvec*(U)=
vec(U), for any k × k symmetric matrix U. Note that this is very easy as We just need to duplicate
all the non-diagonal entries.
Second, we hope to get the coefficients (A> UA)ij’s. Notice that
vec(A>UA) = A> X A>vec(U) = A> X A>Fvec*(U).
Since we only care about the elements of A>UA at the ij-th position for 1 ≤ i < j ≤ k, we just
pick corresponding rows of A> X A> to construct our matrix C, which has dimension k2 × k2 .
The first matrix M is the augmented distinguishing matrix (see Definition 1). In order to better
understand the reason that we need matrix B, let's first re-write Tvec*(U) in the following way:
Tvec* (U)=	^X (A>UA)ij(Mj — mjE[x X x]).
1≤i<j≤k
Thus, Tvec*(U) is just a linear combination of (Mij 一 mjE[x X x]),s with coefficients equal to
(A>UA)ij,s. We have already expressed coefficients (A>UA)j's using CF vec*(U). Now, wejust
need to use matrix B to transform the augmented distinguishing matrix M to a d2 × k2 matrix, with
each column equal to (Mij - mij E[x X x]). In order to achieve this, the first k2 rows of B is just
the identity matrix %, and the last row of B is [-m12, -mi3, •…，-mik, -m23, -m24, ∙∙∙]>.
With above characterization of T, we are ready to show that the k2-th singular value of T is lower
bounded.
Lemma 14. Suppose σmin (M) ≥ α, σmin(A) ≥ β, we know that matrix T has rank k2 and the
k2-th singular value of T is lower bounded by αβ2.
Proof. Since matrix C has dimension k2 × k2, it's clear that the rank of T is at most k2 . We first
prove that the rank of T is exactly k2 .
Since the first k2 rows of B constitute the identity matrix Ik2, we know B is a full-column rank
matrix with rank equal to k2 . We also know that matrix M is a full column rank matrix with rank
k2 + 1. Thus, the product matrix MB is still a full-column rank matrix with rank k2 . If we can
prove that the product matrix CF has full-row rank equal to k2. It's clear that T = MBCF also
has rank k2 . Next, we prove that CF has full-row rank.
23
Published as a conference paper at ICLR 2019
Since σmin(A) ≥ β, We know A> 0 A> is full rank, and a subset of its rows C has full row
rank. For the sake of contradiction, suppose that there exists non-zero vector a ∈ Rk2, such that
Plk=2 1 al(CF)[l,:] = 0. Note that for any 1 ≤ l ≤ k2, (CF)[l,ii] = C[l,ii] for 1 ≤ i ≤ k and
(CF)[l,ij] = C[l,ij] + C[l,ji] for 1 ≤ i < j ≤ k. Since C consists ofa subset of rows ofA> 0 A>,
we know C[l,ij] = C[l,ji] for any l and any i < j. Thus, Plk=2 1 al(CF)[l,:] = 0 simply implies
Plk=2 1 alC[l,:] = 0, which breaks the fact that C is full-row rank. Thus, the assumption is false and
CF has full-row rank.
Now, let’s prove that the k2-th singular value of T is lower bounded. We first show that in the
product characterization ofT, the smallest singular value of each individual matrix is lower bounded.
According to the assumption, we know the smallest singular value of M is lower bounded by α.
Since the first k2 rows of matrix B constitute a k2 × k2 identity matrix, we know
σmin(B) := min kBuk ≥ min kIk2×k2uk =: σmin(Ik2×k2) = 1,
u"∣uk≤1	u"∣uk≤1
where u is any k2-dimensional vector.
Since σmin(A) ≥ β, we know σmin(A> 0A>) ≥ β2. According to the construction ofC, we know
C consists a subset of rows ofA> 0 A>. Denote the indices of the row not picked as S. We have
σmin(C) := min ku>Ck
u"∣uk≤1
=	min	kv>(A> 0 A>)k
v"∣vk≤1 ∧ (Vi=0,∀i∈S)
≥ min kv>(A> 0A>)k
v[∣v∣∣≤1
=: σmin(A> 0 A>)
≥β2,
where u has dimension k2 and v has dimension k2.
We lowerbound the smallest singular value of CF by showing that σmin (CF) ≥ σmin(C). For any
unit vector u ∈ Rk2, we know [u>CF]ii = [u>C]ii for any i and [u>CF]ij = [u>C]ij + [u>C]ji
for any i < j. We also know [u>C]ij = [u>C]ji for i < j. Thus, we know for any unit vector u,
ku>CFk ≥ ku>Ck, which implies σmin(CF) ≥ σmin(C).
Finally, since in the beginning we have proved that matrix T has rank k2, the k2-th singular value
is exactly the smallest non-zero singular value of T . Denote the smallest non-zero singular of T as
σm+in(T), we have
σk2 (T) =σm+in(T)
≥σmin(M)σmin(B)σmin(CF)
≥αβ2,
where the first inequality holds because both M and B has full column rank.	□
B.1.2	Upperbounding ∣∣T — T∣∣f
In this section, we prove that given polynomial number of SamPles, ∣T — T∣∣f is small with high
probability. We do this by standard matrix concentration inequalities. Note that our requirements
on the norm of x is just for convenience, and the same proof works as long as x has reasonable
tail-behavior (e.g. sub-Gaussian).
Lemma 15. Assume that ∣∣x∣ ≤ Γ, ∣A∣ ≤ Pi, ∣ξ∣ ≤ P2 andσmin(E[xx>]) ≥ γ > 0,thenforany
Jgq	d Γ ^Γ 1√	- C∕d3Γ14(ΓPi√k+P2)6lθg( d )、	7	/..7	,	7
E ≤ γ / 2 ,for any 1 > δ > 0, given O(-----γ^- -----) number of i.i.d. samples, we know
kT - TkF ≤ E,
with probability at least 1 — δ.
24
Published as a conference paper at ICLR 2019
Proof. In order to get an upper bound for ∣∣T - T∣∣f, we first show that ∣∣T - T∣∣2 is upper bounded.
Weknow
.. ʌ..	.., ʌ.
∣T - Tk = max	II(T - T)Vk
v∈Rk2+k"∣vk≤1
≤ max	∣∣(T - T)VeC (U)∣.
U ∈R⅛⅛IlU IIF ≤√2
For any k × k symmetric matrix U with eigenvalue decomposition U = Pk=I λiu(i)(u⑴)>,ac-
cording to the definition of T , we know
max	∣∣(T - T)VeC (U)∣ = max	∣∣Tvec (U) - TveC (U)∣
U∈Rk×m：||U∣f≤√2	U∈Rk×k: IlU∣f≤√2
k
≤ max ∣ X %(f(u⑺)-f(u⑴川
"⑸:||u ⑸ k≤√2 M
k
≤ max	X ∣λi∣∣f(u⑺)-f(u叫
"⑸:||u ⑸ k≤√2 M
k
≤(X∣%∣)	max4Lkf(U)-f(U)∣
U ud∣uk≤√2
≤√kUxλ2 maxk∣f(U)- f(U)∣
NM	m∣"k≤√2
=√k∣U∣F max	∣f(u) - f(u)∣
u"∣u∣≤√2
≤√2k max Ilf(u) - f(u)∣
u： ∣u k≤2
1	p / ∖	rτι 士/ ^T ∖	1 .1 i` .λ	1 ∙ .	. 1 z-x 1 CI	♦	1 ∙ . -k τ
where f (u) = Tvec* (UU 1 ) and the fourth inequality uses the Cauchy-Schwarz inequality. Next,
we only need to upper bound maxw∣uk≤2 Ilf (U) - f (u)∣. Recall that
f (u) =2E[(uty) ∙ (E[(U>y)x>]E[xx>]-1x) ∙ (x 0 x)] — E[(uty)2(x 0 x)]
十 (e[(utj)2] — 2E[(uTy)XT]E[xxτ] 1E[(utj)x])E[x 0 x],
and
f (u) =2E[(uTy) ∙ (E[(uTy)XT]E[xxτ]-1x) ∙ (x 0 x)] — E[(U>y)2(x 0 x)]
+(E[(UTy)2] — 2E[(uTy)XT]E[xxτ] 1E[(U>y)x])E[x 0 x].
Notice that
(e[(utj)∙ (E[(uTy)XT]E[xxτ]-1x) ∙ (x0x)]) = E[(uTy)XT]E[xxτ] 1E[(uTy)X(X0x)τ].
We first show that given polynomial number of samples,
∣∣2E [(uTy)XT]E[xxτ] 1E[(uTy)X(X 0 x)τ] — 2E[(uTy)XT]E[xxτ] 1E[(uTy)X(X 0 x)τ] ∣
is upper bounded with high probability.
Since each row of W has unit norm, we have ∣∣ W∣∣ ≤ √k. Due to the assumption that ∣∣x∣ ≤
Γ, Mil ≤ Pι,∣∣ξ∣∣ ≤ P2, we have
II(UTy)XTIl ≤∣∣U∣∣Mσ(Wx) + ξ∣∣∣∣x∣∣
≤kuk(MkkW IIlIXlI + ∣∣ξ∣∣)∣∣x∣∣
≤2Γ(ΓPι√k + P2).
25
Published as a conference paper at ICLR 2019
According to Lemma 24, We know given O( γ "马 Vzk+P2)log(δ)) number of samples,
∣∣E[(uτy)xτ] - E[(uτy)xτ] Il ≤ e,
with probability at least 1 - δ.
Similarly, we can show that given O( γ (rpι√k+p2) log(d)) number of samples,
∣∣E[(uTy)X(X 0 x)τ] — E[(uTy)X(X 0 x)τ] ∣∣ ≤ e,
with probability at least 1 - δ.
Since ∣∣χχτ ∣∣ ≤ Γ2, we know that given O( γ lOg(δ)) number of samples,
∣∣E[xxτ] — E[xxτ] ∣∣ ≤ e,
with probability at least 1 - δ. Suppose that E ≤ γ∕2 ≤ σmi∏(E[xxτ])∕2, we know E[χχτ] has full
rank. According to Lemma 29, we have
∣∣E [xxτ]T - E[xxτ]T∣∣ ≤ 2√2 ∣∣E 巧]工 E [χf]∣∣ ≤ 2√2e∕72,
Ii l 」L j ∣ ∣ ≤	σmin(E[χχτ])	≤ /Y,
with probability at least 1 - δ.
By union bound, we know for any < γ∕2, given O(γ (rpι√k+p2) log(d)) number of samples,
with probability at least 1 - δ, we have
∣∣E[(uτy)xτ] - E[(uτy)xτ] ∣∣ ≤ g
E[xxτ] 1 — E[xxτ] 11∣ ≤ 2√2e∕γ2,
E[(uTy)X(X 0 x)τ] — E[(uTy)X(X 0 x)τ] ∣∣ ≤ e.
Define
Ei := E[(uτy)Xτ]-叫(UTy)XT],
E2 ：= E[xxt]T - E[xxt]T,
E3 := E[(uTy)X(X 0 x)t] — E[(uτ j)x(x 0 x)t].
Then, we have
∣∣2E[(uTy)XT]E[xxt]	1E[(uTy)X(X 0 x)t] — 2E[(uTy)XT]E[xxt]	1E[(uTy)X(X 0 x)t] ∣ ∣
≤2 ∣∣E1∣∣∣∣E2∣∣∣∣E3∣∣+2∣∣E1∣∣∣∣E[xxτ]T∣∣∣∣E[(uTy)X(X 0 x)t]∣∣
+ 2 ∣ ∣ E[(uτy)xτ] ∣ ∣ ∣ ∣ E2∣∣ ∣∣E[(uτy)x(x 0 x)t] ∣∣ + 2∣∣E[(uτy)xτ] ∣∣ ∣∣E[xxt]-11∣ ∣∣E3∣∣
+ 2 ∣∣E1∣∣∣∣E2∣∣∣∣E[(UT y)x(x 0 x)τ]∣∣+2∣∣E[(uτ y)xτ] ∣∣ ∣∣E21∣ ∣∣E3∣∣ + 2∣∣E11∣ ∣∣E[xxt]-1∣∣∣∣E3∣∣
≤2(产
2Γ3(ΓP1 Vk + P2)e	8√2Γ4(ΓP1 √k + P2)% 2Γ(ΓP1 √k + P2)e
I	I	+	I
YY2	Y
l 4√2Γ3(ΓP1 √k + P2)e2 , 4√2Γ(ΓP1 √k + P2)e2 l e2
+	Y	+	Y	+ 7
=O Γ4(ΓP1 √k + P2)25
一O(	2	)
Y2
Thus, given O( γ ^马—蓑2)log(d))number of samples, we know
∣∣2E[(uτy) ∙ (E[(uτy)xτ]E[xxτ]-1x) ∙ (x 0 x)] — 2E[(uτy) ∙ (E[(uτy)xτ]E[xxτ]-1x) ∙ (x 0 x)]
=∣∣2E [(uτy)xτ]E [xxt] 1E [(uτy)x(x 0 x)t] — 2E[(uTy)XT]E[xxt] 1E[(uτy)x(x 0 x)t] ∣ ∣
≤3
26
Published as a conference paper at ICLR 2019
with probability at least 1 - δ.
Now, let,s consider the second term
∣∣E[(uτy)2(x 0 x)] — E[(uτy)2(x 0 x)^]
Since ∣∣(uτy)2(x 0 x)∣∣ ≤ 4Γ2(ΓPι√k + P2)2, according to Lemma 24, we know given
O(
Γ4(ΓP1 √k+P2)4 log(d)
T2
with probability at least 1 - δ.
Next, let,s look at the third term
∣∣E[(uτy)2]E[x 0 x] - E[(uτy)2]E[x 0 x] ∣∣.
Again, using Lemma 24 and union bound, we know given O(γ (rP1 Vzk+P2)log(δ)) number of
samples, we have
∣∣E[(uτy)2] -E[(uτy)2]∣∣ ≤ 3
∣∣E[x 0 x] — E[x 0 x] ∣∣ ≤ e.
Thus, Define
E4 ：= E[(uτy)2] - E[(uτy)2]
E5 := E[x 0 x] — E[x 0 x].
Then, we have
∣∣E[(uτy)2]E[x 0 x] - E[(uτy)2]E[x 0 x]∣∣ ≤∣∣E4∣∣ ∣∣E5∣∣ + ∣∣E4∣∣ ∣∣E[x 0 x]∣∣ + ∣∣E[(uτy)2] ∣∣ ∣∣E5∣∣
≤e2 + Γ2e + 4(ΓP1√k + P2)2e
=O((ΓP1√k + P2)2e).
Thus, we know that given O( γ (rP1 VZk+P2)log(d)) number of samples, we know
∣∣E[(uτy)2]E[x 0 x] - E[(uτy)2]E[x 0 x] ∣ ∣ ≤ €,
with probability at least 1 - δ.
Now, let,s bound the last term,
Similar as the first term, we can show that given O( γ (rP1-蓑2)log(d))number of samples, we
have
∣∣2(E[(uτy)xτ]E[xxτ] 1E[(uτy)x])E[x0x]-2(E[(uTy)XT]E[xxτ] 1E[(uτy)x])E[x0x] ∣ ∣ ≤ €
with probability at least 1 - δ.
Now, we are ready to combine our bound for each of four terms. By union bound, we know given
O( γ "P1-[72) log(d)) number of samples,
2E[(uτy) ∙ (E[(uTy)XT]E[xxτ]-1x) ∙ (x 0 x)] — 2E[(uτy) ∙ (E[(uTy)XT]E[xxτ]-1x) ∙ (x 0 x)] ∣ ∣ ≤ €
E[(uτy)2(x 0 x)] — E[(uτy)2(x 0 x)] ∣∣ ≤ €,
E[(uτy)2]E[x 0 x] - E[(uτy)2]E[x 0 x] ≤ €,
2(E[(uτy)xτ]E[xxτ] 1E[(uτy)x])E[x 0 x] — 2(E[(uTy)XT]E[xxτ] 1E[(uτy)x])E[x 0 x] ∣ ∣ ≤ €,
27
Published as a conference paper at ICLR 2019
hold with probability at least 1 - δ. Thus, we know
max kf(u) - f (u)k ≤ + + + = 4
u： ku k≤2
with probability at least 1 - δ.
Recall that
kT - TkF ≤√k2 + kkT - Tk
≤k√2k max。kf(U)- f(u)k,
u： ku k≤2
where the second inequality holds since ∣∣T - T∣∣ ≤ √2kmaXuRuk≤2 ∣∣f (U) - f (u)∣∣.
Thus, we know given O( γ (rP1√Y+ep2) log(d)) number of samples,
∣T — T∣∣f ≤ k√2ke ≤ d√2de
with probability at least 1 - δ. Thus, given O(d γ (rP1√k+P2) log(d)) number of samples,
∣T - TIIF ≤ E
with probability at least 1 - δ.	□
B.2	Robust Analysis for Simultaneous Diagonalization
In this section, we will show that the simultaneous diagonalization step in our algorithm is robust.
Let S and S be two (k2 + k) by k matrices, whose columns consist of the least k right singular
vectors of T and T respectively.
According to Lemma 11, we know with polynomial number of samples, the Frobenius norm of
SS> - SS⊥ is well bounded. However, due to the rotation issue of subspace basis, We cannot
conclude that ∣S - S∣F is small. Only after appropriate alignment, the difference between S and S
becomes small.
Lemma 16. Let S and S be two (k2 + k) by k matrices, whose columns consist of the least k right
singular vectors of T and T respectively. If ∣SS> - SS> ∣F ≤ E, there exists an rotation matrix
R ∈ Rk×k satisfying RR> = R>R = Ik, such that
..ʌ	..
IlS - SRIIF ≤ 2e.
Proof. Since S has orthonormal columns, we have σk(SS> ) = 1. Then, according to Lemma 35,
we know there exists rotation matrix R such that
kS - SRkF ≤ JSS>- ss>∣f	≤ 2e.
√2(√2 - 1)Pσk(SS>)
□
Let the k columns of S be vec*(U∕ vec*(U2),…，vec*(Uk). Note each Ui can be expressed as
A->DiA-1, where Di is a diagonal matrix. Let Q be a k × k matrix, whose i-th column consists of
the diagonal elements of Di, such that Qij equals the j-th diagonal element of D%. Let vec* (X)=
SRZ1, vec* (Y) = SRZ2, where R is the rotation matrix in Lemma 16 and Zι, Z2 are two independent
standard Gaussian vectors. Let DX = diag(QRζ1) and DY = diag(QRζ2). It’s not hard to check
that X = A->DXA-1 and Y = A->DY A-1. Furthermore, we have XY-1 = A->DXDY-1A>.
Next, we show that the diagonal elements of DX DY-1 are well separated.
Lemma 17. Assume that ∣A∣ ≤ P1, σmin(A) ≥ β. Then for any δ > 0 we know with probability
at least 1 - δ, we have
sep(DX DY-1) ≥ poly(1/d, 1/P1, β, δ),
where sep(DX DY-1) := mini6=j |(DXDY-1)ii - (DX DY-1)jj |.
28
Published as a conference paper at ICLR 2019
Proof. We first show that matrix Q is well-conditioned. Since Ui = A->DiA-1, we have
Vec(Ui)= A-> 0A->vec(Di). Let U bea k2 X k matrix whose columns consist of Vec(Ui)'s. Also
define Q as a k2 × k matrix whose columns are VeC(Di)'s. Note that matrix Q only has k non-zero
rows, which are exactly matrix Q. With the aboVe definition, we haVe U = A-> 0 A->Q. Since
σmin(U) ≤ ∣∣A-> 0 A->kσmin((Q),we have
σmin(Q) ≥
σmin(U)
kA-> 0 A->k.
Notice that a subset of rows of U constitute matrix S, which is an orthonormal matrix. Thus, we
haVe σmin(U) ≥ σmin(S) = 1. Since we assume σmin(A) ≥ β, we haVe
∣A-> 0 A->k = ∣A->k2 = _17τ1 ≤ $.
σmin(A)2	β2
Thus, We have σmin(Q) ≥ β2, which implies σmin(Q) ≥ β2.
We also know ∣∣U∣ ≥ σmin(A-> 0 A-τ)∣(Q∣, thus
kQk ≤
∣Uk
σmin(A-τ 0 A-τ)

Since ∣∣S∣ = 1, we know ∣∣U∣∣ ≤ √2. For the smallest singular value of A-τ 0 A-τ, we have
σmin(A-> 0 a-> )=湍也(AT) = 11P ≥ P .
Thus, we have ∣Q∣ ≤ √2P2, which implies ∣∣Q∣ ≤ √P∖
Now, let's prove that the diagonal elements of DX DY-1 are well-separated. Let qiτ be the i-th row
vector of Q. Then we know the i-th diagonal element of DXD-1 is 2R,1i. Since ∣Q∣ ≤ √2P2,
Y	hqi,Rζ2i
we have ∣∣qi∣ ≤ √2P2 for every row vector.
It's not hard to show that with probability at least 1 一 exp(-dQ(1)), we have ∣hqi, RZ2i∣ ≤
poly(d, P1 ) for each i.
Now given Z2 for which this happens, we have ；；£：：；
hqj,RZ1
hqj,R2
cihqi, Rζ1i 一 cjhqj, Rζ1i, where ci, cj have magnitude as least poly(1/d, 1/P1). Since σmin(Q) ≥
β2, we know ∣Projq⊥ qi∣ ≥ β2 (because otherwise there exists λ such that ∣(ei + λej)∣σmin(Q) ≤
∣(ei + λej)τQ∣ = ∣Projq⊥ qi∣ < β2, which is a contradiction). Let qi⊥,j = Projq⊥ qi = qi 一 λi,j qj,
we can rewrite this as
^RI 一 ⅛⅛j = Ccifq Raj RS = cihq⊥j, RZIi-(Cj+λij ci)hqj, Rζ1i.
By properties of Gaussians, we know hqi⊥,j, Rζ1i is independent of hqj, Rζ1i, so we can first fix
hqj, Rζ1i and apply anti-concentration of Gaussians (see Lemma 38) to hqi⊥,j, Rζ1i. As a result we
know with probability at least 1 一 δ∕k2:
I *∙R4 - 等R41 ≥ poly(1∕d, 1/Pi, β, δ).
hqi, Rζ2i	hqj, Rζ2i
By union bound, we know with probability at least 1 一 δ,
sep(DχD-1) ≥ poly(1∕d, 1∕Pι,β,δ).
□
T . √V	A Λ∙	1 W	A Λ∙ TL T	. 1 . . 1	♦	.	Γ∙ W ——1	1	. . 1
Let X = Sζ1 and Y = Sζ2. Next, we prove that the eigenvectors of XY -1 are close to the
eigenvectors of XY -1.
29
Published as a conference paper at ICLR 2019
Lemma 12. Suppose that	∣∣SS>	— SS>∣∣f	≤ g	∣∣A∣∣	≤	Pi,	∣∣ξ∣∣	≤	P2,σmin(E[xx>])	≥
Y,σmin(A) ≥ β. Let X = mat*(SZ1), Y = mat*(SZ2), where Zi and Z2 are two inde-
pendent standard Gaussian vectors. Let zi, ∙∙∙ Zk be the normalized row vectors of A-1. Let
zi,..., Zk be the eigenVectorS of XY 1 (after Sign flip). For any δ > 0 and small enough G With
O( (rPl√k+P2) log(d∕δ))number ofi.i.d. samples in E[Z>y], with probability at least 1 — δ over the
randomness ofζi , ζ2 and i.i.d. samples, there exists a permutation π(i) ∈ [k] such that
∣∣Zi — z∏[i]k ≤ poly(d,Pι, 1∕β,e, 1∕δ),
for any 1 ≤ i ≤ k.
Proof. Let zi, ∙∙∙ , zk be the eigenvectors of XYT (before sign flip step). Similarly define
Zi,…，zk for XY i. We first prove that the eigenvectors of XY i are close to the eigenvectors
ofXY-i.
_ 一 一 一「-一― 一 一
Let X = X + EX and Y = Y + EY. Then We have
X Y-i = XY-i(I + F) + G,
where F = — Eγ(I + Y-iEγ)-iY-i and G = EXY-i. According to Lemma 34, we have
∣∣Fk ≤ ° . (Y-k∣Ey k and ∣∣Gk ≤ JEXk). In order to bound the perturbation matrices ∣∣F∣∣ and
11 x-»l I	1 . C . 1	1 11 τπ Il 11 τπ Il 1	r∖	/W ∖
∣G∣, we need to first bound ∣EX ∣, ∣EY∣ and σmin(Y), σmin(Y).
Alr~ι	G PT- / A ri -rY∖ λ∙ A	1 ∙ . τ	i x	1	11 A ∕^(c∣∣∕
As we know, EX = X — X = (S — SR)ζi. According to Lemma 16, we have ∣S — SR∣ ≤
∣S — SRkF ≤ 2e. We also know with probability at least 1 — exp(-dQ(I)), ∣Zιk ≤ poly(d). Thus,
we have
..,ʌ .. ʌ	....
∣Eχk = ∣(S — SR)Zik ≤ ∣S — SRkkZik ≤ poly(e,d).
Similarly, with probability at least 1 — exp(—dQ(I)), we also know ∣∣Eγk ≤ poly(e, d).
Now, we lower bound the smallest singular value ofX and Y. Since X = A->DXA-i, we have
σmin (X) ≥σmin(A )σ
min(DX)
=∣A∣2 σmin(DX)
≥ p2 σmin (DX ).
Since DX is a diagonal matrix, its smallest singular value equals the smallest absolute value of
its diagonal element. Recall each diagonal element of DX is hqi, RZi i, which follows a Gaussian
distribution whose standard deviation is at least kqi k ≥ β2 . By anti-concentration property of
Gaussian (see Lemma 38), we know ∣hqi, RZ2i∣ ≥ Ω(δβ2∕k) for all i with probability 1 — δ∕4.
Thus, we have σmin(X) ≥ poly(1∕d, 1∕Pi, β, δ). Similarly we have the same conclusion for Y.
For small enough EY, we have σmin (Y) ≥ σmin (Y) — kEY k.
Thus, for small enough , we have kFk ≤ poly(d, Pi, 1∕β, , 1∕δ) and kGk ≤
poly(d, Pi, 1∕β, , 1∕δ). In order to apply Lemma 33, we also need to bound κ(A->) and kXY-i k.
Since σmin(A->) ≥ 片 and ∣∣A->∣ ≤ 1∕β, we have κ(A->) ≤ Pi∕β. For the norm of XY-i,
we have
kXY-i∣ ≤ σmXY) ≤kX"”,…,
where the second inequality holds because σmin (Y) ≥ poly(1∕d, 1∕Pi, β, δ). Recall that X =
mat*(SRZi). It,s not hard to verify that with probability at least 1 — exp(—dQ(I)), we have ∣∣Xk ≤
poly(d). Thus, we know kXY-i k ≤ poly(d, Pi, 1∕β, 1∕δ). Similarly, we can also prove that
kDXDY-ik ≤ poly(d,Pi, 1∕β, 1∕δ)
Overall, we have
κ(A->)(kXY-iFk + kGk) ≤κ(A->)(kXY-ikkFk + kGk)
≤PI (poly(d, Pi, 1∕β, 1∕δ)poly(d, Pi, 1∕β, e, 1∕δ) + poly(d, Pi, 1∕β, e, 1∕δ))
≤poly(d, Pi, 1∕β, , 1∕δ).
30
Published as a conference paper at ICLR 2019
According to Lemma 17, We know with probability at least 1 - δ∕4, SeP(DXD-1) ≥
poly(1/d, 1/P1, β, δ). Thus, by union bound, we know for small enough , with probability at least
1 - δ,
κ(A->)(kXY-1Fk+kGk) < sep(DXDY-1)/(2k).
According to Lemma 33, we know there exists a permutation π [i] ∈ [k], such that
k夕0 一Ok ≤3 kFkkDχD-1k + kGk
i i 一 σmm(A->)sep(DχD-1)
≤3 poly(d, Pι,∖∕β, e, 1∕δ)poly(d, Pi, 1∕β, 1∕δ) + poly(d, Pι,∖∕β, e, 1∕δ)
≤	1∕Pιpoly(1∕d, 1∕Pι,β, 1∕δ)
≤poly(d,P1,1∕β,,1∕δ).
with probability at least 1 - δ.
According to Lemma 5, the eigenvectors of XY -1 (after sign flip) are exactly the normalized rows
of AT (UP to permutation). Now the only issue is the sign of Z0 By the robustness of sign flip
step (see Lemma 18), we know for small enough e, with O( (rp1√k+P2)2 log(d∕δ)) number of i.i.d.
samples in E[^>y], with probability at least 1 - δ, the sign flip of Zi is consistent with the sign flip
of zi.	□
In the following lemma, we show that the sign flip step of Zi is robust.
Lemma 18. Suppose that kXk ≤ Γ, ∣∣Ak ≤ Pi, kξk ≤ P?. Let zj,…，zk be the eigenvectors of
XY-1 (before sign flip step). Similarly define Zj,…，Zk for X Y-1. Supposefor eachi，kzi-Zik ≤
G where E ≤ 4rq+rβγ√;+ P). We know, for any δ < 1, with O( (rp1√k+P2) log(d∕δ)) number of
i.i.d. samples,
Pr hsign(EKZO,y〉]) = sign(E[h^0,y〉])] ≤ δ.
Proof. We first show that E[hZi0, yi] is bounded away from zero. Let Z0 be a k × k matrix, whose
rows are {Zi0}. Without loss of generality, assume that Z0 = diag(±λ)A-1. Since kA-1k ≤ 1∕β,
we know λi ≥ β, for each i. Thus, we have
|E[hZi0, yi]| = λiE[σ(wi>x)] ≥ βE[σ(wi>x)].
In order to lowerbound E[σ(wi>x)], let’s first look at E[σ(wi>x)x>].
kE[σ(w>x)x>]k =1 kE[w>xx>]k
≥ 1 Ilwikσmin(E[xX>])
≥ Y.
-2
Now, let’s connect kE[σ(wi>x)x>]k with E[σ(wi>x)].
kE[σ(wi>x)x>]k ≤E[kσ(wi>x)x> k]
=E[σ(wi>x)kxk]
≤ΓE[σ(wi>x)].
Thus, we have E[σ(w>x)] ≥ 务,and further |E[〈zi，y〉]| ≥ βγ.
Now, let,s bound ∣∣E[hz°, y〉] - E[h^0, y〉] k.
kE[hzi,yi] - E[hzi,yi]k =kE[hzi,yi] - E[hzi,yi] + E[hzi,yi] - E[hzi,yi]k
≤kE[hzi - ^0,y〉]k + kE[hz0,y〉] -E[h^O,y〉]k
≤(ΓPι√k + P?) + kE[hZ0,y〉] - E[h^O,y〉]k.
31
Published as a conference paper at ICLR 2019
Note that We reserve fresh samples for this step, thus Zi is independent With samples m E[hzi, y〉]].
Since ∣∣hzi, y)k ≤ ΓPι√k + P2, we know with O("马混十与产 log(d∕6) number of samples, we
have
IlEKzi,y〉] -EKzi,yi]]k≤ J
Overall, we have IIEKzO, yi] 一 EKzi,yi]]k ≤ (1 + ΓPι√k + P2)e. Combined with the fact that
∣E[hzi, yi] | ≥ βΓ, we know as long as e ≤ 4r(1+rPγ√k+E), with O( CrP1√k+P2)2 log(d∕") number
of samples, we have
Pr[sign(EKzi,yi]) = sign(EKzi,yi])] ≤ δ∙
□
B.3 Robust Analysis for Recovering First Layer Weights
We will first show that Algorithm 1 is robust.
Theorem 8.	Assume that ∣x∣ ≤ Γ, ∣w∣ ≤ 1, ∣ξ∣ ≤ P2 and σmin(E[xx>]) ≥ Y. Then for any
J ≤ γ/2, for any 1 > δ > 0, given O((r +Pγr∣2lo^δ)) number of i.i.d. samples, we know with
probability at least 1 - δ,
Ilw 一 WIl ≤ J
where W is the learned weight vector.
Proof. We first show that given polynomial number of i.i.d. samples, IE[yx] 一 E[yx]I is upper
bounded with high probability, where E[yx] is the empirical estimate of E[yx]. Due to the assump-
tion that IxI ≤ Γ, IWI ≤ 1, IξI ≤ P2, we have
IyxI =I(σ(W>x) + ξ)xI
≤IW>xxI + IξxI
≤kwkkχk2 + ∣ξ∣kχk
≤Γ2 +P2Γ
According to Lemma 24, we know given O((r +P2：2 log(d)) number of samples, with probability
at least 1 一 δ, we have IE[yx] 一 E[yx]I ≤ J.
Since ∣∣χχ> ∣∣ ≤ Γ2, we know that given O( γ lθg(δ)) number of samples, ∣E[xx>] — E[χχ>]∣ ≤ J
with probability at least 1 — δ. Suppose that J ≤ γ∕2 ≤ σmin(E[xx>])∕2, we know E[χχ>] has full
rank. According to Lemma 29, we have
IlE [xx>]-1 — E[xx>]-1∣∣ ≤ 2√2 胆[xχ]工 E⅛fU∣ ≤ 2√2j∕y2,
Il L 」L j∣ ≤	σmm(E[xx>])	≤ v /Y ,
with probability at least 1 — δ.
By union bound, we know for any J ≤ γ∕2, given O((r +P2：/ log(δ)) number of samples, with
probability at least 1 — δ, we have
∣∣E[xx>]-1 — E[xx>]-1∣ ≤ 2√2j∕γ2
..ʌ r , _一 …
∣∣E[yχ] — E[yχ]k ≤ J
Denote
E1 :=E[xx>]-1 — E[xx>]-1
E2 ：=E[yx] — E[yχ]∙
32
Published as a conference paper at ICLR 2019
Then, we have
∣∣w; — Wk =2∣∣E[xx>]-1E[yx] — E[xx>]-1E[yx] k
≤2∣∣E1k∣∣E2k +2∣Eιk∣E[yx]k +2kE[xx>]-1 ∣∣E21
4√2e2	4√2(Γ2 + P2Γ)e	2e
≤ ^^2	1	Y	+ ~γ
≤o((γ-≡^ ).
γ2
Thus, given O((r +PYNlog(δ)) number of samples, with probability at least 1 一 δ, we have
Ilw 一 Wk ≤ J
□
Now let’s go back to the call to Algorithm 1 in Algorithm 3. Let zi ’s be the normalized rows of
A 1, and let Zi S be the eigenvectors of XY 1 (with correct sign). From Lemma 12, we know {zi}
are close to {zi } with permutation. Without loss of generality, we assume the permutation here is
just an identity mapping, which means ∣Zi — Zik is small for each i.
For each zi, let vi be the output of Algorithm 1 given infinite number of inputs (x, zi>y). For each
Zi, let Vi be the output of Algorithm 1 given only finite number of samples (x, Z>y). In this section,
we show that suppose ∣Zi — ^ik is bounded, with polynomial number of samples, ∣Vi 一 Vik is also
bounded.
The input for Algorithm 1 is (x, Z>y). We view Z>y as the summation of z>y and a noise term
(Zi 一 Zi)Ty. Here, the issue is that the noise term (Zi 一 Zi)>y is not independent with the sample
(x, Z>y), which makes the robust analysis in Theorem 8 not applicable. On the other hand, since we
reserve a separate set of samples for Algorithm 1, the estimate Zi is independent with the samples
(x, y),s used by Algorithm 1. Thus, the samples (x, Zi>y)s here are still i.i.d., which enables us to
use matrix concentration bounds to show the robustness here.
Lemma 13. Assume that kxk ≤ Γ, kAk ≤ P1, kξk ≤ P2 and σmin(E[xxT]) ≥ γ. Suppose that
for each 1 ≤ i ≤ k, ∣∣Zi — Zik ≤ τ. Thenfor any e ≤ γ∕2 and δ < 1, given O((r +Pγr∣2lo^δ))
number of samples for Algorithm 1, we know with probability at least 1 一 δ,
ll r<2τ (ΓPι√k + P2)Γ
kvi 一 Vik ≤-------------------+ 2j,
γ
for each 1 ≤ i ≤ k.
Proof. For any i, let's bound	∣∣Vi	一	Vik.	AS We know,	Vi	=	2E[xx>]-1E[Z>yx]	and	Vi	=
2E[xx>]-1 E[^>yx]. Thus, in order to bound ∣∣Vi 一 Vik, We only need to show
口E[xx>]-1E[Z>yx] 一 E[xx>]-1E[z>yx] ∣∣ and 口E[xx>]-1E[Z>yx] 一 E[xx>]-1E[Z>yx] ∣∣
are both bounded.
The first term can be bounded as follows.
∣∣E[xx>]-1E[Z>yx] 一 E[xx>]-1E[z>yx] ∣∣ =∣∣E[xx>]-1E[(^i 一 Zi)>yx] ∣∣
≤∣∣E[xx>]-1∣∣ ∣∣Zi - Zi∣∣ ∣∣ Aσ(Wx) + ξ∣∣∣∣x∣∣
≤ T (ΓPι√k + P2)Γ
_	Y
We can use standard matrix concentration bounds to upper bound the second term. By similar anal-
ysis of Theorem 1, we know given O((r +Pγr?og(δ)) number of i.i.d. samples, with probability
at least 1 一 δ,
∣∣E[xx>]-1E[^>yx] — E[xx>]-1E[Z>yx]∣∣ ≤ e.
33
Published as a conference paper at ICLR 2019
Overall, we have
IIvi — ViIl =∣∣2E[xx>]-1 E[z> yx] — 2E[xx>]-1E[z> yx] ∣∣
≤2∣∣E[xx>]-1 E[^> yx] — E[xx>]-1E[z> yx] ∣∣ + 2∣∣E[xx>]-1E[^> yx] — E[xx>]-1E[Z> yx] ∣∣
≤ 2τ (FPι√k + P2)r +2e.
By union bound, We know given O((r +胃？® δ)) number of i.i.d. samples, with probability at
least 1 一 δ,
ll	2τ(ΓPι√ + P2)Γɪ9
Ilvi 一 vik ≤+ 2e.
Y
for any 1 ≤ i ≤ k.	□
B.4 Proof of Theorem 7
Proof of Theorem 7. Combining Lemma 11, Lemma 12 and Lemma 13, we know given
Poly (Γ, P1,P2,d, 1/e, 1∕γ, 1∕ɑ, 1∕β, 1/5) number of i.i.d. samples, with probability at least 1 一 δ,
Ilzi - zik ≤ 3 IIvi - Viil ≤ C
for each 1 ≤ i ≤ k.
Let V bea k X d matrix whose rows are vi's. Similarly define matrix V for vi's. Since ∣∣vi 一 vi ∣∣ ≤ E
i`	■	1	. CTτ^ TArI	.	.	< ∙ < ∙	1 ∙ IItt- T1?-11	∕-Γ
for any i, we know every row vector of V 一 V has norm at most C, which implies IV 一 V I ≤	kC.
Let Z be a k × k matrix whose rows are zi's. Similarly define matrix Z for zi's. Again, we have
∣∣Z 一 Zk ≤ √ke. In order to show ∣∣Z T 一 ZTk is small using standard matrix perturbation bounds
(Lemma 29), we need to lower bound σmin(Z). Notice that Z is just matrix A-1 with normalized
row vectors. As we know, σmin(A-1) ≥ 1∕Pι, and kA-1k ≤ 1∕β, which implies that every row
vector of AT has norm at most 1∕β. Let Dz be the diagonal matrix whose i, i-th entry is the norm
of i-th row ofA-1, then Z = Dz-1A-1, and we know σmin(Z) ≥ σmin(Dz-1)σmin(A-1) ≥ β∕P1.
Then, according to Lemma 29, as long as E ≤ 余,we have
kZT- Z-1k ≤ 2√2 kZ -Zk ≤ 2√2p2√ke∕β2.
σmin(Z)
Define
E1 :=ZT - ZT
E2 :=V - V.
We know kE1k ≤ 2√2P2√ke∕β2 and kE2k ≤ √ke. In order to bound kVk, we can bound the
norm of its row vectors. We have,
kvik =k2E[χχ>]-1E[z>yχ]k
≤ 2Γ(ΓPι√k + P2
γ	Y	,
which implies k V k ≤ 2√kr(rp1√k+p2). Now we can bound kZ-1σ(Vx) 一 Z-1σ(V x)k as follows.
kZ-1σ(Vx) - Z-1σ(Vx)k ≤kE1kkE2xk + kE1kkVxk + kZTkkE2xk
2√2P2Γke2	4√2P2Γ2(ΓP1 √k + P2)ke	P1Γ√ke
≤	β2	+	β2γ	+ —β 一,
1	.1 i' . ∙	1 ∙ .	1	1 1	♦	11 ∕τ 7-	∖ 11 IItt- Il 1 11	∕τ^r∖	∕^r τ^ ∖ 11 ∕IItλt^	τ r Il
where the first inequality holds since kσ(V x)k ≤ kVxk and kσ(V x) 一 σ(Vx)k ≤ kVx 一 Vxk.
Thus, we know given poly Γ, P1, P2, d, 1∕E, 1∕Y, 1∕α, 1∕β, 1∕δ number of i.i.d. samples, with
probability at least 1 一 δ,
kAσ(Wx) 一 Z-1σ(Vx)k = kZ-1σ(Vx) 一 Z-1σ(Vx)k ≤ e,
where the first equality holds because Aσ(W x) = Z-1σ(Vx), as shown in Theorem 5.
34
Published as a conference paper at ICLR 2019
C Smoothed Analysis for Distinguishing Matrices
In smoothed analysis, it’s clear that after adding small Gaussian perturbations, matrix A and W
will become robustly full rank with reasonable probability (Lemma 36). In this section, we will
focus on the tricky part, using smoothed analysis framework to show that it is natural to assume
the distinguishing matrix is robustly full rank. We will consider two settings. In the first case, the
input distribution is the Gaussian distribution N (0, Id), and the weights for the first layer matrix
W is perturbed by a small Gaussian noise. In this case we show that the augmented distinguishing
matrix M has smallest singular value σmin (M) that depends polynomially on the dimension and
the amount of perturbation. This shows that for the Gaussian input distribution, σmin (M) is lower
bounded as long as W is in general position. In the second case, we will fix a full rank weight
matrix W , and consider an arbitrary symmetric input distribution D. There is no standard way of
perturbing a symmetric distribution, we give a simple perturbation D0 that can be arbitrarily close to
D, and prove that σmin(M D0 ) is lowerbounded.
Perturbing W for Gaussian Input We first consider the case when the input follows standard
Gaussian distribution N(0, Id). The weight matrix W is perturbed to W where
r≥C _____ _
W = W + ρE.	(18)
kd
Here E ∈ Rk×d is a random matrix whose entries are i.i.d. standard Gaussians. We will use M to
denote the perturbed version of the augmented distinguishing matrix M . Recall that the columns of
M has the form:
Mij = E[(W>x)(W>x)(x 0 x) 1{W>xW>X ≤ 0}],
>
where wei> is the i-th row of W. Also, since M is the augmented distinguishing matrix it has a final
column M0 = vec(Id). We show that the smallest singular value of M is lower bounded with high
probability.
Theorem 9.	Suppose that k ≤ d/5, and the input follows standard Gaussian distribution N (0, Id).
Given any weight matrix W with kwi k ≤ τ for each row vector, let W be a perturbed version
of W according to Equation (18) and M be the perturbed augmented distinguishing matrix. With
probability at least 1 一 exp(-dQ(1)), we have
,ɔr`	一 ，一, .,, 、
σmin(M) ≥ PolyQ/τ l∕d,ρ).
We will prove this Theorem in Section C.1.
Perturbing the Input Distribution Our algorithm works for a general symmetric input distribu-
tion D. However, we cannot hope to get a result like Theorem 9 for every symmetric input distri-
bution D. As a simple example, if D is just concentrated on 0, then we do not get any information
about weights and the problem is highly degenerate. Therefore, we must specify a way to perturb
the input distribution.
We define a perturbation that is parametrized by a random Gaussian matrix Q and a parameter
λ ∈ (0, 1). The random matrix Q is used to generate a Gaussian distribution DQ with a random
covariance matrix. To sample a point in Dq, first sample n 〜 N(0,Id), and then output Qn.
The (Q, λ) perturbation of a distribution D, which we denote by DQ,λ is a mixture between the
distribution D and the distribution DQ. More precisely, to sample x from DQ,λ, pick z as a Bernoulli
random variable where Pr[z = 1] = λ and Pr[z = 0] = 1 - λ; pick x0 according to D and pick
x00 = Qn according to distribution DQ, then let
x0 z = 0
x = x00 z = 1
Intuitively, the (Q, λ) perturbation of a distribution D mixes the distribution D with a Gaussian
distribution DQ with covariance matrix QQ> . Since both D and DQ are symmetric, their mixture
is also symmetric. Also, the TV-distance between D and DQ,λ is bounded by λ. Throughout this
section we will use D0 to denote the perturbed distribution DQ,λ
35
Published as a conference paper at ICLR 2019
We show that given any input distribution, after applying (Q, λ)-perturbation with a random Gaus-
sian matrix Q, the smallest singular value of the augmented distinguishing matrix MD0 is lower
0
bounded. Recall that MD is defined as
MD = Ex〜do [(w>x)(w>x)(x 0 x) l{w>xw>X ≤ 0}],
as the first (2) columns and has Ex〜d，[x 0 x] as the last column.
Theorem 10.	Given weight matrix W with kwi k ≤ τ for each row vector and symmetric input
distribution D. Suppose that k ≤ d/7 and σmin(W) ≥ ρ, after applying (Q, λ)-perturbations to
yield perturbed input distribution D0, where Q is a d × d matrix whose entries are i.i.d. Gaussians,
we have with probability at least 1 一 exp(-dQ(1)) over the randomness of Q,
σmin(MDj ≥ poly(1∕τ, 1/d, ρ, λ).
We will prove this later in Section C.3.
C.1 Smoothed Analysis for Gaussian Inputs
In this section, we will prove Theorem 9, as restated below:
Theorem 9. Suppose that k ≤ d/5, and the input follows standard Gaussian distribution N (0, Id).
Given any weight matrix W with kwi k ≤ τ for each row vector, let W be a perturbed version
of W according to Equation (18) and M be the perturbed augmented distinguishing matrix. With
probability at least 1 — exp(-dQ(1)), we have
,ɔr. 一 ，一,	.,, 、
σmin(M) ≥ PolyQ/τ ι∕d,ρ).
To prove this theorem, recall the definition of Mij :
mat(Mij) = E[(w>x)(w>x)(xx>) l{w>xw>X ≤ 0}].
Since Gaussian distribution is highly symmetric, for every direction u that is orthogonal to both wi
and wj , we have u>mat(Mij )u be a constant. We can compute this constant as
mij := E[(w>x)(w>x)l{w>xw>X ≤ 0}].
This implies that if we consider mat(Mij ) - mij Id, it is going to be a matrix whose rows and
columns are in span of wi and wj . In fact we can compute the matrix explicitly as the following
lemma:
Lemma 19. Suppose input x follows standard Gaussian distribution N(0, Id), and suppose weight
matrix W has full-row rank, then for any 1 ≤ i < j ≤ k, we have
mat(Mij) = - (φj cos(φj) - sin(φij)) ∣IwillIlwjllId + φj(wiw> + Wjw>)
-U (M Wj w> + 骨 Wiw7),	π
-	Iwj I j	Iwi I i
where 0 < φij < - is the angle between weight vectors wi and wj.
Of course, the same lemma would be applicable to W, so we have an explicit formula for Mij. We
will bound the smallest singular value using the idea of leave-one-out distance (as previously used
in Rudelson & Vershynin (2009)).
Leave-one-out Distance Leave-one-out distance is a metric that is closely related to the smallest
singular value but often much easier to estimate.
Definition 2. For a matrix A ∈ Rd×n(d ≥ n), the leave-one-out distance d(A) is defined to be the
smallest distance between a column of A to the span of other columns. More precisely, let Ai be the
i-th column ofA and S-i be the span of all the columns except for Ai, then
d(A) := min I(Id -ProjS-i)Ai I.
36
Published as a conference paper at ICLR 2019
Rudelson & Vershynin (2009) showed that one can lowerbound the smallest singular value of a
matrix by its leave-one-out distance.
Lemma 20 (Rudelson & Vershynin (2009)). For matrix A ∈ Rd×n (d ≥ n), we always have
d(A) ≥ σmin(A) ≥ √d(A).
Therefore, to bound σmin (M) we just need to lowerbound d(M). We use the ideas similar to
Bhaskara et al. (2014) and Ma et al. (2016). Since every column of M (except for M0) is random,
we will try to show that even if we condition on all the other columns, because of the randomness
in Mij , the distance between Mij to the span of other columns is large. However, there are several
obstacles in this approach:
1.	The augmented distinguishing matrix M has a special column M0 = vec(Id) that does not
have any randomness.
2.	The closed form expression for M (as in Lemma 19) has complicated coefficients that are
not linear in the vectors wei and wej .
3.	The columns of Mij are not independent with each other, so ifwe condition on all the other
columns, Mij is no longer random.
To address the first obstacle, we will prove a stronger version of Lemma 20 that allows a special
column.
Lemma 21. Let A ∈ Rd×(n+1) (d ≥ n + 1) be an arbitrary matrix whose columns are
A0, A1 , ..., An. For any i = 1, 2, ..., n, let S-i be the subspace spanned by all the other columns
(including A0) except for Ai, and let d0(A) := mini=1,...,n k(Id - ProjS )Aik. Suppose the column
Ao has norm √d and Ai,..., An has norm at most C, then
σmin (A) ≥ min (∖ IAnnC L , ∖ /. 2 rd	d d0 (A)).
4nC2 + d	4n2 C2 + nd
This lemma shows that if we can bound the leave-one-out distance for all but one column, then the
smallest singular value of the matrix is still lowerbounded as long as the columns do not have very
different norms. We defer the proof to Section C.2.
For the second obstacle, we show that these coefficients are lowerbounded with high probability.
Therefore we can condition on the event that all the coefficients are large enough.
Lemma 22. Given weight vectors wi and wj with norm kwi k, kwj k ≤ τ, let wei = wi + ρεi , wej =
Wj + ρεj where εi,εj are i.i.d. Gaussian random vectors. With probability at least 1 一 exp(-dQ(1)),
we know ∣∣Wik ≤ T + p3ρ2d/2, ∣∣Wj ∣∣ ≤ T + p3ρ2d/2 and
e、√ρ2(d- 2)
φij ≥ √2τ + p3P2d,
where φij is the angle between wei and wej. In particular, if W = W + ρE where E is an i.i.d.
Gaussian random matrix, with probability at least 1 — exp(-dQ(1)) ,forall i, ∣Wik ≤ T + ,3ρ2 d/2,
and for all i < j, the coefficient φij /π in front of the term wei wej> + wej wei> is at least
√P2(d-2)
(√2τ+√3ρ2d)π
This lemma intuitively says that after the perturbation wei and wej cannot be close to co-linear. We
defer the detailed proof to Section C.2.
For the final obstacle, we use ideas very similar to Ma et al. (2016) which decouples the randomness
of the columns.
Proof of Theorem 9. Let Ei be the event that Lemma 22 does not hold. Event Ei will be one of
the bad events (but note that we do not condition on Ei not happening, we use a union bound at the
end).
37
Published as a conference paper at ICLR 2019
We partition [d] into two disjoint subsets L1, L2 of size d/2. Let M0 be the set of rows of M indexed
】r r e， . ♦	.， I	，TV，
by L1 × L2 . That is, the columns of M0 are
--------------------
Mi0j
7
φij
π
(Wi,Lι 0 Wj,L2 + Wj,L1 0 Wi,L2 )-
sin(φij) kweik	kwej k
-iΓ~ ( 百 Wj,L1 0 Wj,L2 + 百 Wi,L1 0 Wi,L2 ),
for i < j, where Wei,L denotes the restriction of vector Wei to the subset L. Note that the restriction
of vec(Id ) to the rows indexed by L1 × L2 is just an all zero vector.
We will focus on a column Mi0j with i < j and try to prove it has a large distance to the span of all
the other columns. Let Vij be the span of all other columns, which is equal to Vij = span{Mk0 l :
k < l ∧ (k, l) 6= (i, j)} (note that we do not need to consider Mf0 because that column is 0 when
restricted to L1 × L2 .
It’s clear that Vij is correlated with Mi0j , which is bad for the proof. To get around this problem, we
follow the idea of Ma et al. (2016) and define the following subspace that contains Vij ,
ʌ
V
Vij
span Wek,L1 0 x,x 0 Wek,L2, Wej,L1 0 x,x 0 Wei,L2 k ∈/ {i, j}, x ∈ Rd/2 .
By definition Vij ⊂ Vij, and thus % ⊂ Vj, where 匕⊥ denotes the orthogonal subspace of Vij.
d .	.	_ .i_ _ . ~	∙~. ■~	~	∙~. ■~	~	∙~.	'~	一	TAr	.i_
Observe that Wej,L1	0 Wei,L2, Wej,L1 0 Wej,L2, Wei,L1	0	Wei,L2 ∈	Vij,	thus
Proj V⊥Mij
^∏jProjV⊥ (Wi,Li
0 Wej,L2 .
TL T .	. 1	.	~	~	∙	∙	1	1	.	∙ .< T1T- > 1	1	TVI	1∙	♦	.	.
Note that WWi,L1 0 WWj,L2 is independent with Vij . Moreover, subspace Vij has dimension at most
(k — 2)d∕2+ (k — 2)d∕2 + d/2 + d/2 = (k — 1)d < 4 ∙ d2. Then by Lemma 31, we know that with
probability at least 1 — exp(—dQ(1)),
ProjV⊥ (Wi，Li
0 WWj,L2 ≥ poly(1/d, ρ).
Let E2 be the event that this inequality does not hold for some i, j .
Let Sij = span{Mf0, Mfkl : k < l ∧ (k, l) 6= (i, j)}. Now we know when neither bad events E1 or
E2 happens, for every pair i < j ,
二1	二 F,
ProjS⊥ Mfij ≥ProjV ⊥ Mfij
ij	ij
_	. ɔr,
≥ProJV⊥Mij
W
=-∏jPrOjV⊥ (Wi,L1 0 Wj,L2)
≥poly(i∕τ, 1/d, ρ).
Currently, we have proved that for any i < j, the distance between column Mij and the span of
other columns is at least inverse polynomial. To use Lemma 21 we just need to give a bound on the
norms of these columns. By Lemma 22, we know when E1 does not happen
∀i, kWWik ≤ τ +
苫
where τ is the uniform upper bound of the norm of every row vector of W . Let τW = τ+ V3Pr ,we
know τW = poly(τ, d, ρ).
38
Published as a conference paper at ICLR 2019
Thus, we have
1
IfijIl ≤∏(φij∣ cos(φij)| +sin(φij))∣IwillIlwjk√d
e
+ φ∏j (kwikkwj k + kwjkkwik)
■~■
sin(φeij )
+ —∏2λ (kwikkwjk + kwj kkwi k)
T 2	L 2	2τ 2
≤ — (π + 1)√d + 2e2 + —.
ππ
EI	.1	∙	.	1/ T ∖	1	.1	. Il W ll/CC	TL ɪ	1	∙	T
Thus, there exists C = poly(τ, d, ρ), such that IMij I ≤ C for every i < j. Now applying Lem-
ma 21 immediately gives the result.
C.2 Proof of Auxiliary Lemmas for Section C.1
We will first prove the characterization for columns in the augmented distinguishing matrix.
Proof of Lemma 19. For simplicity, we start by assuming that every weight vector wi has unit norm.
At the end of the proof we will discuss how to incorporate the norms of wi , wj . Also throughout the
proof we will abuse notation to use Mij as its matrix form mat(Mij).
Let Sij be the subspace spanned by wi and wj . Let Si⊥j be the orthogonal subspace of Sij . Let
{e(1i,j), e(2i,j)} be a set of orthonormal basis for Sij such that e(1i,j) = wi and he(2i,j), wji > 0. We
use matrix Sij ∈ Rd×2 to represent subspace Sij, which matrix has e(1i,j) and e(2i,j) as two columns.
Also, let Si⊥j be ad × (d - 2) matrix, whose columns constitute an orthonormal basis of Si⊥j.
Let ProjS = Sij Si>j , and ProjS⊥ = Id - Sij Si>j . Then, we have
Mij =ProjSi⊥jMij + ProjSij Mij
= (ProjSi⊥j Mij +mijProjSijId) + (ProjSij Mij -mijProjSijId)
= (ProjSi⊥j Mij +mijProjSijId) + (ProjSij Mij -mijSijSi>j)
First, we show that
ProjSi⊥j Mij +mijProjSijId = mijId,
which is equivalent to proving that ProjS⊥ Mij = mij ProjS⊥ Id. It’s obvious that the column span
ofProjS⊥Mij belongs to the subspace Si⊥j. Actually, the row span ofProjS⊥Mij also belongs to the
subspace Si⊥j. To show this, let’s consider u>(ProjS⊥ Mij)v, where u ∈ Si⊥j and v ∈ Sij.
u>(ProjS⊥Mij)v=u>(Id-SijSi>j)Mijv
= (u> - u>SijSi>j )Mijv
= u> Mij v,
where the last equality holds since u ∈ Si⊥j is orthogonal to e(1i,j) and e(2i,j). We also know that
u>MijV = u>E[(w>x)(w>x)(xx>)l{w>xw>X ≤ 0}]V
=E[(w>x)(w>x)(u>x)(v>x)l{w>xw>x ≤ 0}]
=E[(w>x)(w>x)(v>x)l{w>xw;x ≤ 0}]E[(u>x)]
= 0,
where the third equality holds because u> x is independent with wi> x, wj> x and V > . Note since
u is orthogonal with wi , wj , V, we know for standard Gaussian vector x, random variable u> x is
independent with wi> x, wj> x, V> x.
39
Published as a conference paper at ICLR 2019
Since the column span and row span ofProjS⊥Mij both belong to the subspace Si⊥j, there must exist
a (d - 2) × (d - 2) matrix C, such that ProjS⊥ Mij = Si⊥jC(Si⊥j)>. We only need to show this
matrix C must be mijId-2. In order to show this, we prove for any u, v ∈ Si⊥j , u> (ProjS ⊥ Mij)v =
mij u>v.
u> (ProjS⊥ Mij)v = u>Mijv
ij
=u>E[(w>x)(w>x)(xx>)l{w>xw>X ≤ 0}]v
=E[(w>x)(w>x)(u>x)(v>x)l{w>xw>x ≤ 0}]
=E[(w>x)(w>x)l{w>xw>x ≤ 0}]E[u>xv>x]
= mij u> E[xx>]v
= miju>v,
where the fourth equality holds because u>x, v>x are independent with wi>x, wj>x.
Thus, we know
Mij =(ProjSi⊥j Mij +mijProjSijId) + (ProjSij Mij -mijSijSi>j)
=mij Id + (ProjSij Mij - mij Sij Sij ).
Let’s now compute the closed form for mij . Recall that
mj := E[(w>x)(w>x)l{w>xw>X ≤ 0}].
Note, we only need to consider input x within subspace Sij , which subspace has dimension two.
Using the polar representation of two-dimensional Gaussian random variables (r is the radius and θ
is the angle), we have
mij
1	∞ 3	/ r2、τ p
2∏ JjexPL EdT ∏- F
2 cos(θ) cos(θ + φij)dθ = L(φj cos(φj) — sin(φj)).
Next, we compute the closed form of ProjS Mij . Note ProjS Mij is symmetric, because
ProjS Mij = Mij —mijId+mijSijSi>j, and Mij, Id and SijSi>j are all symmetric. It’s obvious that
the column span of ProjS Mij belongs to subspace Sij . Combined with the fact that ProjS Mij
is symmetric, we know the row span of ProjS Mij also belongs to the subspace Sij . Thus, matrix
ProjS Mij can be represented as a linear combination of (e(1i,j))(e(1i,j))> ,
(e(1i,j))(e(2i,j))>, (e(2i,j))(e(1i,j))> and (e(2i,j))(e(2i,j))>, which means
ProjSijMij = c(1i1,j)(e(1i,j))(e(1i,j))>+c(1i2,j)(e(1i,j))(e(2i,j))>+c(2i1,j)(e(2i,j))(e(1i,j))>+c(2i2,j)(e(2i,j))(e(2i,j))>,
where c(1i1,j) , c(1i2,j) , c(2i1,j) and c(2i2,j) are four coefficients. Now, we only need to figure out the four
coefficients of this linear combination. Similar as the computation for mij , we use polar integration
to show that,
c(1i1,j) =hProjSijMij, (e(1i,j))(e(1i,j))T i
=-- r5 exp( — r— )dr	(cos3(θ) cos(θ + φj) +cos(θ)cos3(θ + φj))dθ
2π Jo	2	π--φij
=4∏(12φij cos(φj) — 9sin(φj) — sin(3φj)),
where the first equality holds because e(1i,j) is orthogonal with e(2i,j). Similarly, we can show that
c22,j) =hProjSjj Mij, (e2ij))(e2ij))T i
=L T r5 exp( — — )dr /	(cos(θ) cos(θ + φij) sin2(θ) + cos(θ) cos(θ + φij) sin2(θ + φij))dθ
2π 0o	2	Jn-φij
=4∏ (4φij cos(φij) — 7 sin(φij ) + sin(3φij )),
40
Published as a conference paper at ICLR 2019
and
c(2i1,j) =hProjSijMij, (e(2i,j))(e(1i,j))Ti
=--	r5 exp(-r-)dr	( — cos2(θ) cos(θ + φj) sin(θ) + cos(θ) cos2(θ + φj)sin(θ + φj))dθ
2π Jo	2	π--φij
=1(Φij sin(Φij) - cos(φij)sin2(φj)).
It’s easy to check that c(1i2,j) = c(2i1,j). Let Mi0j be ProjS Mij - mij Sij Sij. Then, according to above
computation, we know
Mi0j =ProjSijMij-mijSijSij
=(c(1i1,j) - mij)(e(1i,j))(e(1i,j))> + c(1i2,j)(e(1i,j))(e(2i,j))> + c(2i1,j)(e(2i,j))(e(1i,j))> + (c(2i2,j) -mij)(e(2i,j))(e(2i,j))>
=4∏ (8 φij cos( φij ) - 5sin(φij ) - sin(3φij )) (e1i，j))(e1i，j))>
+ 4∏ (- 3sin(φij)+sin(3φij ))(e2i,j))(e2i,j))>
+ 1 (Φij sin(Φij) - cos(Φij)sin2(φij))((e1ij))(e2ij))> + (e2i,j))(e1i,j))>).
Since e1i,j) = Wi and e2i,j) = $由(；)Wj - cot(φj∙)Wi, we can also express Mij as a linear combi-
nation of WiWi> , WjWj>, WiWj> and Wj Wi> .
Mij =4∏ (8φij cos(φij) - 5sin(φij) - sin(3φij))WiW>
+ 4∏ ( - 3sin(φij) + sin(3φij))(Sin j .) - cot(φij)Wi)(sin(；. .) - cot(φij)Wi)>
+ 1 (Φij sin(Φij) - cos(Φij)sin2(φij))(Wi(SnwjTy - cot(Φij)Wi)> + (Sinwφ..) - cot(Φij)Wi)W>)
=φij(WiWjr + Wjw>) - sn(φij) (WjWjr + WiW>).
Thus,
Mij =mij Id + Mi0j
=∏(φij cos(φij) - sin(φij))Id + φ∏j(wiW> + Wjw>) - Sinnjij) (Wjw> + Wiw>).
Finally, if the rows Wi, Wj do not have unit norm, let Wi = Wi / k Wi k, Wj = Wj / k wj k, we know
mij = E[(w>x)(w>x) 1{w>xw>x ≤ 0}]
=∣∣WikkWj ∣∣E[(W> x)(W> x) 1{w> xw> X ≤ 0}
=1(φijcos(φij) - sin(φij))kWikkWjk.
Here we used the fact that the indicator variable does not change whether we use Wi, Wj or Wi, Wj.
Similarly,
Mij = E[(w>x)(w>x)xx> 1{w>xwJx ≤ 0}]
=IlWikkWjkE[(W>x)(W>x)xx> 1{w>xw>x ≤ 0}]
=1(φij cos(φij) - sin(φij))kWikkWjkid + φ∏j(WiW> + WjW>) - Sinnφij) (kWi∣WjW> + kjWiW>).
Now we can prove the lemmas used to handle the two obstacles. First we give the stronger leave-
one-out distance bound.
41
Published as a conference paper at ICLR 2019
Proof of Lemma 21. The smallest singular value of A can be defined as follows:
σmin(A) := min kAuk.
u:kuk=1
Suppose U ∈ argm%kuk = IkAuk. Let 遍 be the coordinate corresponding to the column Ai, for
0 ≤ i ≤ n. We consider two cases here. If
then we have
|uo| ≥ 44n4nC+d,
σmin(A) =kAuok
	=llu0oA0 +	uioAi ll 1≤i≤n ≥llu0oA0 ll - ll X uioAi ll 1≤i≤n ≥r≡CC+d √d - (X∣uo∣)C 1≤i≤n ≥ / 4nC2d _	I d C 4nC 4nC2 + d	nnc 4nC2 + d __ I nC2d =V 4nC2 + d,
where the third inequality uses Cauchy-Schwarz inequality.
If lu0 | < ,4nc⅞,weknow Pι≤i≤n |uo|2 ≥ 4nd+d. Let k ∈ argmax1≤i≤n |uio|. We know that
IuO | ≥ ʌIA 2号工 二.Thus,
k 4n2C 2+nd	,
σmin(A) ≥uokAk +	uioAi
i:i6=k
o
=|uk |llAk + X uO Aill
i:i6=k k
≥IukIk(Id-Projs-k)Akk
≥IuO Id0(A)
≥r 4n2C2 + nd d'(A).
Above all, we know that the smallest singular value of A is lower bounded as follows,
σmin(A) ≥ min (∖ IAnnC L , ∖ L 2 J2	d d0 (A)).
4nC2 + d	4n2 C2 + nd
Next we give the bound on the angle between two perturbed vectors wei and wej .
Proof of Lemma 22. According to the definition of ρ-perturbation, we know wei = wi + ρεi, wej =
wj +ρεj, where εi, εj are i.i.d. standard Gaussian vectors. First, we show that with high probability,
the projection of wei on the orthogonal subspace of wej is lower bounded. Denote the subspace
spanned by wej as Swej, and denote the subspace spanned by {wej, wi} as Swej ∪wi. Thus, we have
lProjSw⊥e weil ≥lProjSw⊥e ∪w (wi +ρεi)l
j	ji
=ρlProjSw⊥ej∪wi εil
where Sw⊥e is the orthogonal subspace of Swej .
42
Published as a conference paper at ICLR 2019
Fix εj, then Sw⊥e ∪w is a fixed subspace of Rd with dimension d - 2. Let U be a d × (d - 2)
matrix, whose columns constitute a set of orthonormal basis for the subspace Sw⊥e ∪w . Thus, it’s
not hard to check that ProjS ⊥	εi
=d Uε, where ε ∈ Rd-2 is a standard Gaussian vector. Denote
Y := kProjS⊥	εik2 =d kU εk2 = kεk2, which is a chi-squared random variable with (d - 2)
wej ∪wi
degrees of freedom. According to the tail bound for chi-squared random variable, we have
1	-(d-2)t2
Pr[| d-2 Y - 1| ≥ t] ≤ 2e , ∀t ∈ (0,1).
Let t = 1, we know that with probability at least 1 - 2 exp( -(d-2)),
Y ≥ d-2.
2	2
Thus, we have ∣∣Prcjs⊥ Ieik ≥ PkPrCjs⊥	εik ≥ JPPld-2. Recall that
kProjS⊥ weik = sin(φeij)kweik.
wej
We also know
kweik =kwi + ρεik
≤kwik +ρkεik
=τ +ρkεik,
where the last equality holds since kwi k ≤ τ . Note kεi k2 is another chi-squared random vari-
able with d degrees of freedom. Similar as above, we can show that with probability at least
1 - 2 exp(-d),
kεik2 ≤ 32d.
By union bound, we know with probability at least 1 - 2 exp(-d) — 2 exp( Tg-2)),
・次2…/P2(d- 2)
Sin(Oij)kwik ≥y ——2——
kwei k ≤τ +
Combined with the fact that φij ≥ sin(φij) when φij ∈ [0, π], we know with probability at least
1 - 2exp(-d) - 2exp((d2 2)),
φij ≥ sin(φij)
，~ . .. . ...
_sin(Oij )kweik
=--
≥ q2(ρ
^τ+产
=Pρ2(d- 2)
√2τ + p3ρ2d
~	3	T” ,	L	，	一	♦「八	.	.	.	.	I	I	...
Given W = W + ρE, where E is an i.i.d. Gaussian matrix, by union bound, we know with
probability at least 1 - exp(-dQ(1)),
∀i,kWik≤ T + P3P2d∕2
∀i<j,φij ≥	Fa-ɪ
π	(√2τ + p3ρ2d)π

43
Published as a conference paper at ICLR 2019
C.3 Smoothed Analysis for General Inputs
In this section, we show that starting from any well-conditioned weight matrix W, and any symmet-
ric input distribution D, how to perturb the distribution locally to D0 so that the smallest singular
value of MD0 is at least inverse polynomial.
Recall the definition of (Q, λ)-perturbation: we mix the original distribution D with a distribution
DQ which is just a Gaussian N (0, QQ>). To create a sample x in D0, with probability 1 - λ we
draw a sample from D; otherwise We draw a standard Gaussian n 〜N(0, Id) and let X = Qn. We
will prove Theorem 10 which we restate below:
Theorem 10. Given weight matrix W with kwi k ≤ τ for each row vector and symmetric input
distribution D. Suppose that k ≤ d/7 and σmin(W) ≥ ρ, after applying (Q, λ)-perturbations to
yield perturbed input distribution D0, where Q is a d × d matrix whose entries are i.i.d. Gaussians,
we have with probability at least 1 一 exp(-dQ(1)) over the randomness of Q,
σmin(MDj ≥ poly(1∕τ, 1/d, ρ, λ).
To prove this, let us first take a look at the structure of augmented distinguishing matrix for these
distributions. Let MD, MDQ, MD0 be the augmented distinguishing matrices for distributions D,
DQ and D0 respectively. Since D0 is a mixture of D and DQ , and the augmented distinguishing
matrix is defined as expectations over samples, we immediately have
MD0 = (1 -λ)MD +λMDQ.
Our proof will go in two steps. First we will show that σmin (MDQ ) is large. Then we will show
that even mixing with MD will not significantly reduce the smallest singular value, so σmin (MD0 )
is also large. In addition to the techniques that we developed in Section C.1, we need two ideas that
we call noise domination and subspace decoupling to solve the new challenges here.
Noise Domination First let us focus on σmin (MDQ ). This instance has weight W and input
distribution N(0, QQ>). Let MWQ be the augmented distinguishing matrix for an instance with
weight WQ and input distribution N(0, Id). Our first observation shows that MDQ and MWQ are
closely related, and we only need to analyze the smallest singular value ofMWQ. The problem now
is very similar to what we did in Theorem 9, except that the weight WQ is not an i.i.d. Gaussian
matrix. However, we will still be able to use Theorem 9 as a black-box because the amount of noise
in W Q is in some sense dominating the noise in a standard Gaussian. More precisely, we use the
following simple claim:
Claim 1. Suppose property P holds for N(μ, Id) for any μ, and the property P is convex (in the
sense that if P holds for two distributions it also holds for their mixture), then for any covariance
matrix Σ 占 Id, we know P also holds for N (μ, Σ).
Intuitively the claim says that if the property holds for a Gaussian distribution with smaller variance
regardless of the mean, then it will also hold for a Gaussian distribution with larger variance. The
proof is quite simple:
Proof. Let Σ0 = Σ - Id, by assumption we know Σ0 is still a positive semidefinite matrix. Let
X 〜N(μ, ∑), x0 〜 N(μ, Σ0) and δ 〜 N(0, Id), by property of Gaussians it is easy to see that
X =d X0 + δ. Let dx, dx0 and dδ be the density function for X, X0, δ respectively, then we know for
any point u
dx (U) = ExO〜N(μ,Σ0) [dδ (U - X )].
That is, N(μ, Σ) is a mixture ofN(X0, I). Since property P is true for all N(X0, I), it is also true
for N(μ, ∑).	□
With this claim we can immediately use the result of Theorem 9 to show σmin (MDQ ) is large.
44
Published as a conference paper at ICLR 2019
Subspace Decoupling Next we need to consider the mixture MD0. The worry here is that although
σmin (MDQ ) is large, mixing with D might introduce some cancellations and make σmin(M D0)
much smaller. To prove that this cannot happen with high probability, the key observation is that in
the first step, to prove σmin(MWQ) is large We have only used the property of WQ. IfWe let Q be
the projection of Q to the orthogonal space of row span of W, then Q is still a Gaussian random
matrix even if We condition on the value of WQ! Therefore in the second step We Will use the
additional randomness in Q to show that the cancellation cannot happen. The idea of partitioning
the randomness of Gaussian matrices has been Widely used in analysis of approximate message
passing algorithms. The actual proof is more involved and we will need to partition the Gaussian
matrix Q into more parts in order to handle the special column in the augmented distinguishing
matrix .
Now we are ready to give the full proof of Theorem 10
Proof of Theorem 10. Let us first recall the definition of augmented distinguishing matrix: MD0 is
a d2 by (k2 + 1) matrix, where the first k2 columns consist of
MD := Ex〜do [(w>x)(wjx)(x 0 x)l{w>xwjX ≤ 0}],
and the last column is Ex〜do [x 0 x]. According to the definition of (Q, λ)-perturbation, if we let
DQ be N (0, QQ>), then we have
MDo = (1 -λ)MD +λMDQ.
In the first step, we will try to analyze MDQ . The first k2 columns of this matrix MDQ can be
written as:
MDQ = Ex〜DQ [(w>x)(wjx)(x 0 x)l{w>xwjX ≤ 0}]
=En〜N(0,id) [(w>Qn)(w>Qn)(Qn 0 Qn)l{w>QnwjQn ≤ 0}]
=Q 0 QEn〜N(o1%) [(wjQn)(wjQn)(n 0 n)l{wjQnwjQn ≤ 0}]
for any i < j , and the last column is
Ex〜DQ [x 0 x] = En〜N(0,id) [Qn 0 Qn]
=Q 0 QEn〜N(0,id)[n 0 n].
Except for the factor Q 0 Q, the remainder of these columns are exactly the same as the augmented
distinguishing matrix of a network whose first layer weight matrix is WQ and input distribution is
N(0, Id). We use MWQ to denote the augmented distinguishing matrix of such a network, then we
have
MDQ = Q 0 QMWQ.
WQ
Therefore we can first analyze the smallest singular value of MWQ. Let W = WQ. Note that Q is
a Gaussian matrix, and W is fixed, so WQ is also a Gaussian random matrix except its entries are
not i.i.d. More precisely, there are only correlations within columns of WQ, and for any column of
WQ, the covariance matrix is WWj. Since the smallest singular value of W is at least ρ, we know
σmin (WWj) ≥ ρ2. Let the covariance matrix of WQ be ΣWQ ∈ Rkd×kd, which has smallest
singular value at least ρ2 . Therefore we know ΣWQ ρ2Ikd. It’s not hard to verify that with
probability at least 1 一 exp(-dQ(I)),the norm of every row of WQ is upper bounded by poly(τ, d).
By Claim 1, any convex property that holds for any N (0, ρ2Ikd) perturbation must also hold for
∑wq 4. Thus, we know with probability at least 1 一 exp(-dQ(1)),
σmin(MWQ) ≥ poly(1∕τ, 1∕d,ρ).
To prepare for the next step, we will rewrite MWQ as the product of two matrices. According to the
closed form of MiWj Q in Lemma 19, we know each column of MWQ can be expressed as a linear
combination ofwei0 wej’s and vec(Id). Therefore:
MWQ = hWfj 0 Wfj, vec(Id)iR,
4The conclusion of Theorem 9 is clearly convex because it is a probability, and probabilities are linear in
terms of mixture of distributions.
45
Published as a conference paper at ICLR 2019
where matrix R has dimension (k2 + 1) × (k2 + 1). It’s not hard to verify that
σmin(MWQ) ≤ ∣∣hW> 乳 f>, vec(Id)i ∣∣σmin(R).
Thus,
σ .(R) ≥ —σmin(MWQ)_
(R) ≥∣∣f> ㊈ f>, vec(I<
Note that W is a k × d matrix with kwi k ≤ τ for every row vector, and W = WQ, where Q
is an standard Gaussian matrix. Thus, similar as the proof in Lemma 22, we can show that with
probability at least 1 - exp(-dQ(1)),
∣∣hf> 乳 f>,vec(Id)]∣∣ ≤ ∣∣hf> 乳 W>, VeC(Id川尸 ≤ Poly(T,d).
Thus, we know
σmin(R) ≥ Poly(1∕τ, 1/d, ρ).
Now we will try to perform the seCond step using the idea of subspaCe deCoupling. Let ProjW be
the projection matrix to the row span of W, and let ProjW⊥ = Id — PrCjW. Let Q = ProjW⊥ Q. Let
the Columns of U ∈ Rd×(d-k) be a set of orthonormal basis for the orthogonal subsPaCe W⊥. By
symmetry of Gaussian, ProjW Q is independent with ProjW ⊥ Q. Thus, from now on we will condi-
tion on ProjW Q, and still treat ProjW ⊥ Q as a Gaussian random matrix. More precisely, ProjW ⊥ Q
has the same distribution as UP, where P ∈ R(d-k)×d is a standard Gaussian matrix.
We further decouple the P part into two subspaces (this is done mostly to handle the special column
in augmented distinguishing matrix). Let the rows of V ∈ Rk×d be a set of orthonormal basis for
the row span of W = WQ. And let the rows of V ⊥ ∈ R(d-k)×d be a a set of orthonormal basis
for the orthogonal subspace Wf⊥. We can then decompose UP into the row span of V and V ⊥ as
follows,
UP =d UP1V⊥ + UP2V,
where P1 ∈ R(d-k)×(d-k) and P2 ∈ R(d-k)×k are two independent standard Gaussian matrices.
After this decomposition, we have
Q 乳 Q hf> 乳 f>,vec(Id)]R
=UP ㊈ UP hf > 乳 f >, VeC(Id)] R
=(UPιV⊥ + UP2V)乳(UPιV⊥ + UP2V) hf> 乳 f >,vec(Id)]R
=U ㊈ U (Pi V ⊥ + P2V)氧(PιV ⊥ + P2V) hf> 乳 f>, vec(Id)] R
=U 乳 U[(P1V⊥ + P2V)f > 氧(P1V⊥ + P2V)f >, vec((P1V⊥ + P2V)(P1V⊥ + P2V)>)]r
=U ㊈ U [p2Vf > 乳 P2VW>, VeC(P1P> + P2P>)] R,
⊥>
where the last equality holds beCause the row span of V⊥ is orthogonal to the Column span of W>.
Now, We go back to matrix MD0. Let ProjW⊥蜜W⊥ be ProjW⊥ 0 ProjW⊥. We have,
σmin(MD0 )=σmin ((1 - λ)MD + λMDQ)
=σmin(1 -λ)MD+λQ0QhWf> 0Wf>,VeC(Id)]R
≥σmin ((1 - λ)ProjW⊥0W⊥MD + λProjW⊥0w⊥Q 0 QhWT 0 f >, vec(Id)] R)
=σmin ((1- λ)ProjW⊥0W⊥MD + λQ 0 QhfT 0 W>, VeC(Id)]r)
=σmin ((1 - λ)ProjW⊥0W⊥MD + λU 0 U [P2VW> 0 P2Vf>, vec(P1P> + P2P>)] R)
46
Published as a conference paper at ICLR 2019
Since R has full row rank, We know that the row span of PrCjW⊥0w⊥MD belongs to the row span
of R. According to the definition of U, it's also clear that the column span of ProjW⊥蜜W⊥ MD
belongs to the column span of U 0 U. Thus, there exists matrix C ∈ R(d-k)2×(k2+1) such that
Projw⊥0w⊥MD = U 0 UCR.
Thus,
σmin(M D0 )=σmin ((1 - λ)M D + λM DQ )
≥σmin ((1 - λ)Projw⊥0w⊥ MD + λU 0 U [p)2Vf> 0 P2VWτ, vec(P1P> + P2P>)[ R)
=σmin (λU 0 U (1-λC + [P2Vfτ 0 P2Vf τ, vec(PιP> + P?P> )])R).
Note that C only depends on U and R, U only depends on W , and R only depends on W Q. With
W Q fixed, C is also fixed. Clearly, C is independent with P1 and P2. For convenience, denote
H :=C + [P2Vfτ 0 P2VWfτ, vec(PιP> + P2P>)].
λ
Now, let’s prove that the smallest singular value of matrix H ∈ R(d-k)2×(k2+1) is lower bounded
using leave-one-out distance. Let's first consider its submatrix H which consists of the first k2
τ
columns of H. Note that within random matrix P2VW τ, every row are independent with each
τ
other. Within each row, the covariance matrix is W W τ . Recall that W is a random matrix whose
covariance ΣWQ ρ2Ikd, we can again apply Claim 1 with the property proved in Lemma 37. As
a result, with probability at least 1 一 exp(-dQ(1)),
∕τ`ττ∖ 、	F / r / τ 、
σmin(W ) ≥ poly(1/d, ρ).
τ
Thus the covariance matrix of each row of P2VW τ has smallest singular value at least γ :=
poly(1/d, ρ).
We can view P2 VWWτ as the summation of two independent Gaussian matrix, one of which has
covariance matrix γI(d-k)k. For this matrix, we will do something very similar to Theorem 9 in
order to lowerbound its smallest singular value.
Claim 2. For a random matrix K ∈ R(d-k)×k that is equal to Ko + E where E is a Gaussian
random matrix whose entries have variance γ. If d ≥ 7k, for any subspace SC that is independent
of K and has dimension at most k2 + 1, the leave-one-out distance d(ProjS ⊥ K 0 K) is at least
poly(γ, 1/d).
The proof idea is similar as Theorem 9, and we try to apply Lemma 31 to K 0 K. In the proof we
τ
should think of K := P2VW τ, and denote i-th column of K as Ki. We also think of the space SC
as the column span of C.
As we did in Theorem 9, we partition [d - k] into 2 disjoint subsets L1 and L2 of size (d - k)/2.
Let H0 be the set of rows of H indexed by L1 × L2 .
We fix a column Hi0j, i 6= j ∈ [k]. Let S = span{Hk0 l : (k, l) 6= (i, j)}. It's clear that S is correlated
with HIj. Let C0 be the set of rows of C indexed by Li X L?. Let Sc be the column span of C0,
which has dimension at most k2 + 1. We define the following subspace that contains S,
S = Sco ∪ span{Kj,Lι 0 x,x 0 Ki4 ,Ki,l、0 x,x 0 K1,L2 Ix ∈ R(d-k)/2,l ∈ {i,j}}
Therefor by definition S ⊂ S, and thus S⊥ ⊂ S⊥, where S⊥ denotes the orthogonal subspace of S.
Notice that Hj = Ki,L1 0 Kj,L2 + ɪ-ɪC0j is independent with S, assuming C is fixed. Moreover,
S has dimension at most
(d- k)/2 + (d- k)/2+ (k - 2)(d - k)/2 + (k - 2)(d - k)/2+ k2 + 1 ≤
4(d-k)2
5 -4-
47
Published as a conference paper at ICLR 2019
if k ≤ d/7. Then, according to Lemma 31, We know with probability at least 1 - exp(-dQ(1)),
UPrOjs⊥ Hjll≥ POly(I/d,P).
For the column Hi0i, i ∈ [k], we define subspace S slightly different,
S = Seo ∪ span{K1,L1 0 x,x 0 K1,L2 ∣x ∈ R(d-k"2,l = i}.
Here the dimension of S is also smaller than (d - k)2∕5, assuming that k ≤ d/7. We can similarly
show that with probability at least 1 - exp(-dQ(1)),
∣∣Projs⊥H0i∣∣ ≥POly(I/d,P).
Thus, by union bound, we know that the leave-one-out distance of matrix H0 is lower bounded by
poly(1/d, P).
Now, let’s add the additional column vec(P1P1> + P2P2>) into consideration. For convenience we
denote this column by b. We will first prove that the vector b has large norm when projected to the
orthogonal subspace of columns in H, then we will combine this with the fact that σmin(H) is large
to show that σmin(H) is also large (this last step is very similar to Lemma 21).
We know matrix H only depends on the randomness of P?. Thus, with P2 fixed, all columns in H
are fixed except for b. Now, we rely on the randomness in P1 to show that the distance between b
and the span of other columns in H is lower bounded. In order to get ride of the correlation within
column b, we also need to consider a subset of its rows indexed by L1 × L2, denoted by b0. Let the
first column of Pi be P ∈ Rd-k, and the submatrix consisting of other columns be P∖. Let SH『be
the column span of H0. Let
SH0 = SH0 ∪ Sco ∪ SPan(VeC(PiP>)0, vec(P2P>)0),
where vec(PiPi>)0 is the restriction ofvec(PiPi>) to the rows indexed by Li × L2. The dimension
of SHo is at most k2 + k2 + 1 + 2 ≤ (d - k)2/12, assuming that k ≤ d/7. Clearly,
PrCjs⊥ b0 ≥Projs⊥ b0
=Projs⊥ PLi 0 PL2 ,
where pL is the restriction of p to rows indexed by L. Note that pL1 and pL2 are two independent
standard Gaussian vectors. Thus, according to Lemma 31, we know with probability at least 1 -
exp(-dQ(1)), the distance between b0 and the column span of H0 is at least poly(1/d).
(d-k)2	2	(d-k)2
Claim 3. For any matrix A ∈ R-4- ×k and a vector V ∈ R-4-, if the leave-one-out distance
d(A) ≥ δ, kProja⊥bk ≥ Z, and ∣∣v∣∣ ≤ Ci, let B ∈ R'd ：' ×(k2+1) be the matrix that is the
concatenation ofA and v, then the leave-one-out distance d(B) ≥ poly(ζ, δ, 1/Ci).
The proof idea is similar as the proof in Lemma21. In the proof, we should think of A := H0, v := b0
and B := H0, where H0 is the subset of rows of H indexed by Li X L2. We know that d(H0) ≥
δ = poly(1/d, P), kProjA⊥ bk ≥ ζ = poly(1/d). It’s not hard to show that with probability at least
1 — exp(-dQ(I)),
kb0k ≤poly(d).
Thus, there exists Ci = poly(d), such that kb0k ≤ Ci. We already proved that the leave-one-out
distance of b0 in H0 is lower bounded. We only need to show the leave-one-out distance for the first
k2 columns, which are Hi0j , i, j ∈ [k].
For any i, j ∈ [k], the leave-one-out distance for Hi0j within matrix H0 can be expressed as follows
min k H0j +	X	CklHk 1 + Cbb0k
ckl ,cb
(k,l)6=(i,j)
48
Published as a conference paper at ICLR 2019
Let {ckι ,琮} be one set of the optimal solutions to mmckiG kHj + f(k,i)=@j)Hk ι + cbb k. If
Cb = 0, We immediately have
kHj+	E	CklHkι + cbb0k
(k,l)6=(i,j )
=kHj +	X	CklHklk
(k,l)6=(i,j )
≥minkHj+ χ	CklHklk
(k,l)6=(i,j )
≥δ,
where the last inequality holds because the leave-one-out distance of matrix H0 is lower bounded by
δ.
If Cbb 6= 0, we need to be more careful. In this case, we have,
kH0j+	X	CklHkl + CM
(k,l)6=(i,j)
=1Cb|llCbHij+	X	ClHkl+b°ll
b	(k,l)6=(i,j ) b
≥∣Cb∣ζ,
1	. 1 1	IJlIIF	. 1	1 ∙ .	i' 1 ! . .1	1	i' τ'τ! ♦ 1	F	1 1
where the last inequality holds because the distance ofb0 to the column span ofH0 is lower bounded.
IfICbI ≥ 2cci,wehave IlHj+ P(k,l)=(i,j) CklHkl + Cbb0k ≥ 2ci.
If |瑞| < 含,we have
kH0j+	X	CklHk l + Cbbk
(k,l)6=(i,j )
≥kH0j+	X	CklHk lk-kCbb0k
(k,l)6=(i,j )
≥min kH0j+	X	CklHklk-∣Cb∣kb0k
ckl
(k,l)6=(i,j )
≥δ - 2^Ci
2C1
=δ∕2.
Thus, the leave-one-out distance of H0 is lower bounded by poly(ζ, δ, 1/C1). Recall that with
probability at least 1 一 exp(-dQ(D), we have δ = poly(1∕d,ρ),Z = poly(1∕d),Cι = poly(d).
Thus, we have
d(H0) ≥ poly(1∕d, ρ).
According to Lemma 20, we have
σmin(HZ) ≥ √ 2	= d(H0)
k2 + 1
≥poly(1∕d, ρ).
Finally, we put everything together. Since H0 is a full column rank matrix, we know that σmin (H) ≥
σmin(HZ) ≥ poly(1∕d, ρ). By union bound, we know with probability at least 1 一 exp(-dQ(1)),
σmin (H) ≥poly(1∕d, ρ)
σmin(R) ≥poly(1∕τ, 1∕d, ρ).
49
Published as a conference paper at ICLR 2019
Since U is an orthonormal matrix, We know σmin(U 0 U) = 1. According to Eq. 19, We know with
probability at least 1 一 exp(-dQ(1)),
σmin(MD0) ≥σmin(λU0UHR)
≥λσmin(U 0 U)σmin(H)σmin(R)
≥poly(1∕τ, 1∕d,ρ, λ)
where the second inequality holds since all of U 0 U, H and R have full column rank.
D Tools
In this section, we collect some known results on matrix perturbations and concentration bounds.
Basically, we used matrix concentration bounds to do the robust analysis and used matrix perturba-
tion bounds to do the smoothed analysis. We also proved several corollaries that are useful in our
setting.
D. 1 Matrix Concentration Bounds
Matrix concentration bounds tell us that with enough number of independent samples, the empirical
mean of a random matrix can converge to the mean of this matrix.
Lemma 23 (Matrix Bernstein; Theorem 1.6 in Tropp (2012)). Consider a finite sequence {Zk} of
independent, random matrices with dimension d1 × d2. Assume that each random matrix satisfies
E[Zk] = 0 and kZkk ≤ R almost surely.
Define
σ2 ：= max {∣∣ XE[ZkZ:]∣∣, ∣∣ XE[ZkZk]∣∣}∙
kk
Then, for all t ≥ 0,
Prn∣∣ XZk∣∣ ≥to ≤ (di + d2) exp ( 2 1 //3
As a corollary, we have:
Lemma 24. Consider a finite Sequence {Z1,Z2 •…Zm} of independent, random matrices with
dimension d1 × d2. Assume that each random matrix satisfies
kZk k ≤ R, 1 ≤ k ≤ m.
Then, for all t ≥ 0,
m	t2∕2
Pr 11∣ X(Zk- E[Zk]) ∣∣ ≥ t} ≤ (di + d2)eχp QmR2 + /2Rt)∕3).
Proof. For each k, let Zk0 = Zk - E[Zk] be the new random matrices. It’s clear that E[Zk0 ] = 0 and
kZk0 k ≤ 2R. For the variance,
m
σ2 ≤ X kE[ZkZk*] k	(19)
k=i
m
≤ X 4R2	(20)
k=i
=4mR2.	(21)
(22)
Thus, according to Lemma 23, we have
m	m	t2∕2
pr{∣∣ X(Zk- E[Zk])∣∣ ≥ t} = pr{∣∣ X(Zk)∣∣ ≥ t}≤ (di + d2)eχp(4mR2 + ∕2Rt)/3).
□
50
Published as a conference paper at ICLR 2019
D.2 Matrix Perturbation B ounds
Perturbation Bound for Singular Vectors For singular vectors, the perturbation is bounded by
Wedin’s Theorem.
Lemma 25 (Wedin’s theorem; Theorem 4.1, p.260 in Stewart & Sun (1990).). Given matrices
A, E ∈ Rm×n with m ≥ n. Let A have the singular value decomposition
Σ1	0
A= [U1,U2,U3]	0	Σ2 [V1,V2]>
00
Let A = A + E, with analogous singular value decomposition. Let Φ be the matrix of canonical
angles between the column span of U1 and that of U1, and Θ be the matrix of canonical angles
between the column span ofV1 and that ofV1. Suppose that there exists a δ such that
min ∣[∑ι]i,i - [∑2]jj| > δ and min ∣[∑ι]i,i∣ > δ,
i,j	i
then
k sin(φ)k2 + k sin(θ)k2 ≤ 2k⅛.
δ2
In order to show the robustness of least k right singular vectors of T , we combine Wedin’s theorem
with the following Lemma.
Lemma 26 (Theorem 4.5, p.92 in Stewart & Sun (1990).). Let Φ be the matrix of canonical angles
between the column span ofU and that ofU, then
kProoU - PrOoUk = k Sin阐k.
The exact lemma used in our proof is the following corollary in Ge et al. (2015).
Lemma 27 (Lemma G.5 in Ge et al. (2015)). Given matrix A, E ∈ Rm×n with m ≥ n. Suppose
that the A has rank k. Let S and S be the subspaces spanned by the first k right singular vectors of
A and A = A + E, respectively. Then, we have:
k ProoS⊥ - Projs⊥ k ≤
√⅜F
σk (A)
Perturbation Bound for pseudo-inverse With a lowerbound on σmin(A), we can get bounds for
the perturbation of pseudo-inverse.
Lemma 28 (Theorem 3.4 in Stewart (1977)). Consider the perturbation ofa matrix A ∈ Rm×n :
B = A + E. Assume that rank (A) = rank(B) = n, then
kBt - Atk ≤√2kAtkkBtkkEk.
The following corollary is particularly useful for us.
Lemma 29 (Lemma G.8 in Ge et al. (2015)). Consider the perturbation ofa matrix A ∈ Rm×n :
B = A + E where ∣∣E∣∣ ≤ σmin(A)∕2. Assume that rank (A) = rank(B) = n, then
kBt- Atk ≤ 2√2kEk∕σmin(A)2.
Perturbation Bound for Tensor To lowerbound the leave-one-out distance in augmented distin-
guishing matrix , we use the following Lemma as the main tool.
Lemma 30 (Theorem 3.6 in Bhaskara et al. (2014)). For any constant δ ∈ (0, 1), given any subspace
V of dimension δdl in Rdl, there exist vectors v1,v2, ∙∙∙ ,Vr in V with unit norm, such that for
random (ρ∕√d)-perturbations e(1),e(2), ∙∙∙ , X⑴ ∈ Rd ofany vector x(1),x(2),• 一 , x(l) ∈ Rd,
we
know with probability at least 1 — exp(-6d1/(2l)l),
∃j ∈ [r], hvj,e⑴0e⑵0∙∙∙θe(l)i ≥ρl(1)3l.
51
Published as a conference paper at ICLR 2019
For second-order tensor, we have the following corollary.
Lemma 31. For any constant δ ∈ (0, 1), given any subspace V of dimension δd2 in Rd2, there exist
vectors vι, v2, ∙∙∙ , Vr in V with unit norm, such thatfor random (ρ/√d)-perturbations e(1), e(2) ∈
Rd Ofany vector x(1), x(2) ∈ Rd, we know with probability at least 1 一 exp(-δd1/16),
∃j ∈ [r], hvj,e⑴㊈e⑵i ≥ ρ2(d)9.
Perturbation Bound for Eigendecomposition Here, We restate some generic results from B-
haskara et al. (2014) on the stability of a matrix’s eigendecomposition under perturbation. Let M
and M be two n X n mtrices such that M = UDUT and M = M(I + E) + F.
Definition 3 (Definition A.1 in Bhaskara et al. (2014)). Let sep(D) = mini6=j |Dii 一 Djj|.
The following Lemma guarantees that the eigenvalues of M are distinct if the perturbation are not
too large.
Lemma 32 (Lemma A.2 in Bhaskara et al. (2014)). Ifκ(U)(kMEk + kFk) < sep(D)/2n, then
the eigenvalues of M are distinct and diagonalizable.
The following Lemma further upperbound the difference between corresponding eigenvectors.
Lemma 33 (Lemma A.3 in Bhaskara et al. (2014)). Let uι,..., Un and Ui,..., Un respectively be the
eigenvectors of M and M, ordered by their corresponding eigenvalues. If K(U)(∣∣MEk + ∣∣F∣∣) <
sep(D)∕2n, thenfor all i we have ∣∣Ui 一 ui∣ ≤ 3 "式怒：莅图+^广(尸).
In the setting of simultaneous diagonalization, let Na = Ta + Ea and Nb = Tb + Eb, we have
NaNb-1 =TaTb-1(I+F)+G,
where F = -Eb(I + T-IEb)TT-1 and G = EaN-1. The following lemma bound the maximum
singular value of perturbation matrix F and G.
Lemma 34 (Claim A.5 in Bhaskara et al. (2014)). σmaχ(F) ≤ -----智;;(Eb)	and σmaχ(G) ≤
σmin (Tb )-σmax (Eb )
σmax (Ea )
bmin (Nb )
AI♦	,ccι	∙	i ʌ .	. i	.	♦	.	i i .1	. W r-i All ∙
Alignment of Subspace Basis. Due to the rotation issue, we cannot conclude that ∣S 一 S ∣ is
small even we know ∣∣SS> 一 SS> ∣ is bounded. The following Lemma shows that after appropriate
alignment, S is indeed close to S.
Lemma 35 (Lemma 6 in Ge et al. (2017a)). Given matrices S, S ∈ Rd×r, we have
min
Z> Z=ZZ> =Ir
..ʌ	. . C
∣S-SZkF ≤
∣∣ss> - SS>∣F
2(√2 - 1)σr(SS>)
D.3 Smallest Singular Value of Random Matrices
For a random rectangular matrix where each element is an inpdependent Gaussian variable, Rudel-
son & Vershynin (2009) gives the following result:
Lemma 36 (Theorem 1.1 in Rudelson & Vershynin (2009)). Let A ∈ Rm×n and suppose that
m ≥ n. Assume that the entries ofA are independent standard Gaussian variable, then for every
> 0, with probability at least 1 一 (C )m-n+1 + e-C0n, where C, C0 are two absolute constants,
we have:	_____
σn(A) ≥ e(√m - √n - 1).
However, in our setting, we are more interested in fixed matrices perturbed by Gaussian variables.
The smallest singular value of these “perturbed rectangular matrices” can be bounded as follows.
Lemma 37 (Lemma G.16 in Ge et al. (2015)). Let A ∈ Rm×n and suppose that m ≥ 3n. If all the
entries of A are independently ρ-perturbed to yield A, then for any > 0, with probability at least
1 一 (C)0.25m, for some absolute constant C, the smallest singular value ofA is bounded below by:
σn(A) ≥ EPvzm.
52
Published as a conference paper at ICLR 2019
D.4 Anti-Concentration
We use the anti-concentration property for Gaussian random variables in our proof of Lemma 17.
Lemma 38 (Anti-concentration in Carbery & Wright (2001)). Let x ∈ Rn be a Gaussian variable
x ∈ N(0, I), for any polynomial p(x) of degree d, there exists a constant κ such that
Pr |p(x)| ≤ pVar[p(x)]] ≤ Ke 1/d.
53