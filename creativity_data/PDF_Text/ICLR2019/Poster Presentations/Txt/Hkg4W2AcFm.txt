Published as a conference paper at ICLR 2019
Overcoming the Disentanglement vs Recon-
struction Trade-off via Jacobian Supervision
Jose Lezama
Universidad de la RepUblica, Uruguay
jlezama@fing.edu.uy
Ab stract
A major challenge in learning image representations is the disentangling of the
factors of variation underlying the image formation. This is typically achieved
with an autoencoder architecture where a subset of the latent variables is con-
strained to correspond to specific factors, and the rest of them are considered nui-
sance variables. This approach has an important drawback: as the dimension of
the nuisance variables is increased, image reconstruction is improved, but the de-
coder has the flexibility to ignore the specified factors, thus losing the ability to
condition the output on them. In this work, we propose to overcome this trade-off
by progressively growing the dimension of the latent code, while constraining the
Jacobian of the output image with respect to the disentangled variables to remain
the same. As a result, the obtained models are effective at both disentangling and
reconstruction. We demonstrate the applicability of this method in both unsu-
pervised and supervised scenarios for learning disentangled representations. In a
facial attribute manipulation task, we obtain high quality image generation while
smoothly controlling dozens of attributes with a single model. This is an order of
magnitude more disentangled factors than state-of-the-art methods, while obtain-
ing visually similar or superior results, and avoiding adversarial training1.
1	Introduction
A desired characteristic of deep generative models is the ability to output realistic images while
controlling one or more of the factors of variation underlying the image formation. Moreover, when
each unit in the model’s internal image representation is sensitive to each of these factors, the model
is said to obtain disentangled representations. Learning such models has been approached in the
past by training autoencoders where the latent variables (or a subset of them) are constrained to
correspond to given factors of variation, which can be specified (supervised) or learned from the
data (unsupervised) (Bengio et al., 2013; Mathieu et al., 2016; Hu et al., 2017; Szabo et al., 2018;
Kim & Mnih, 2018). The remaining latent variables are typically considered nuisance variables and
are used by the autoencoder to complete the reconstruction of the image.
There exists one fundamental problem when learning disentangled representations using autoen-
coders, sometimes referred to as the “shortcut problem” (Hu et al., 2017; Szabo et al., 2018). If the
dimension of the latent code is too large, the decoder ignores the latent variables associated to the
specified factors of variation, and achieves the reconstruction by using the capacity available in the
nuisance variables. On the other hand, if the dimension of the latent code is small, the decoder is
encouraged to use the specified variables, but is also limited in the amount of information it can use
for reconstruction, so the reconstructed image is more distorted with respect to the autoencoder’s
input. Szabo et al. (2018) showed that this trade-off between reconstruction and disentangling can
indeed be traversed by varying the dimension of the latent code. However, no principled method
exists to choose the optimal latent code dimension.
The shortcut problem was also addressed by using additional mechanisms to make sure the decoder
output is a function of the specified factors in the latent code. One approach, for example, consists
in swapping the specified part of the latent code between different samples, and using adversarial
1Source code available at https://github.com/jlezama/disentangling-jacobian .
1
Published as a conference paper at ICLR 2019
training to make sure the output distribution is indeed conditioned to the specified factors (Mathieu
et al., 2016; Lample et al., 2017; Szabo et al., 2017; Szabo et al., 2018). However, adversarial
training remains a difficult and unstable optimization problem in practice.
Based on these observations, we propose a method for avoiding the shortcut problem that requires
no adversarial training and achieves good disentanglement and reconstruction at the same time.
Our method consists in first training an autoencoder model, the teacher, where the dimension of the
latent code is small, so that the autoencoder is able to effectively disentangle the factors of variation
and condition its output on them. These factors can be specified in a supervised manner or learned
from the data in an unsupervised way, as we shall demonstrate. After the teacher model is trained,
we construct a student model that has a larger latent code dimension for the nuisance variables. For
the student, we optimize the reconstruction loss as well as an additional loss function that constrains
the variation of the output with respect to the specified latent variables tobe the same as the teacher’s.
In what follows, we consider autoencoder models (E, D), that receive an image x as input and
produce a reconstruction X: D(E(X))= X. We consider that the latent code is always split into a
specified factors part y ∈ Rk and a nuisance variables part Z ∈ Rd: E(X) = (y, z), D (y, Z) = X.
Consider a teacher autoencoder (ET , DT ), with nuisance variables dimension dT, and a student
autoencoder (ES, DS) with nuisance variables dimension dS; dS > dT . Because the dimension
of the nuisance variables of the student is larger than in the teacher model, we expect a better
reconstruction from it (i.e. || x - XS || < ||x - XT || ,for some norm).
At the same time, we want the student model to maintain the same disentangling ability as the
teacher as well as the conditioning of the output on the specified factors. A first order approximation
of this desired goal can be expressed as
∂xS ∂xT
∂yi ≈ ∂y
(1)
where j ∈ {1 …H ∙ W ∙ C}, H, W and C are the dimensions of the output image, and i ∈ {1...k}
indexes over the specified factors of variation.
In this paper we propose a method to impose the first-order constraint in (1), which we term Jacobian
supervision. We show two applications of this method. First, we propose an unsupervised algorithm
that progressively disentangles the principal factors of variation in a dataset of images. Second, we
use the Jacobian supervision to train an autoencoder model for images of faces, in which the factors
of variation to be controlled are facial attributes. Our resulting model outperforms the state-of-the-
art in terms of both reconstruction quality and facial attribute manipulation ability.
2	Related Work
Autoencoders (Hinton & Salakhutdinov, 2006; Bengio et al., 2013; Kingma & Welling, 2014) are
trained to reconstruct an input image while learning an internal low-dimensional representation of
the input. Ideally, this representation should be disentangled, in the sense that each hidden unit in the
latent code should encode one factor of variation in the formation of the input images, and should
control this factor in the output images. There exist extensive literature on learning disentangled
representations (Rifai et al., 2012; Bengio, 2013; Cheung et al., 2014; Kingma et al., 2014; Cogswell
et al., 2015; Chen et al., 2016; Mathieu et al., 2016; Szabo et al., 2017; Perarnau et al., 2016; Hu
et al., 2017; Kim & Mnih, 2018; Burgess et al., 2018).
Disentangled representations have two important applications. One is their use as rich features for
downstream tasks such as classification (Rifai et al., 2012; Tran et al., 2017) or semi-supervised
learning (Kingma et al., 2014). In the face recognition community, for example, disentanglement
is often used to learn viewpoint- or pose-invariant features (Yang et al., 2015; Peng et al., 2017;
Tran et al., 2017). A second important application is in a generative setting, where a disentangled
representation can be used to control the factors of variation in the generated image (Yan et al., 2016;
Perarnau et al., 2016; Higgins et al., 2016; Mathieu et al., 2016; Szabo et al., 2017; Lample et al.,
2017). In this work we concentrate on the second one.
In recent years, with the advent of Generative Adversarial Networks (GANs) (Goodfellow et al.,
2014), a broad family of methods uses adversarial training to learn disentangled representations
2
Published as a conference paper at ICLR 2019
(Mathieu et al., 2016; Szabo et al., 2017; Lample et al., 2017; Perarnau et al., 2016; Chen et al.,
2016). In a generative setting, the adversarial discriminator can be used to assess the quality of a
reconstructed image for which the conditioning factors do not exist in the training set (Mathieu et al.,
2016; Szabo et al., 2017; Chen et al., 2016).
Another alternative, proposed in Fader Networks (Lample et al., 2017), is to apply the adversarial
discriminator on the latent code itself, to prevent it from containing any information pertaining to
the specified factors of variation. Then, the known factors of variation or attributes are appended to
the latent code. This allows to specify directly the amount of variation for each factor, generating
visually pleasing attribute manipulations. Despite being trained on binary attribute labels, Fader
Networks generalize remarkably well to real-valued attribute conditioning.
However, despite recent advances (Arjovsky et al., 2017; Gulrajani et al., 2017), adversarial training
remains a non-trivial min-max optimization problem, that in this work we wish to avoid. Other re-
markable disentangling methods that require no adversarial training are: Cheung et al. (2014), where
the cross-covariance between parts of the latent representation is minimized, so that the hidden fac-
tors of variation can be learned unsupervised and Higgins et al. (2016); Kim & Mnih (2018); Burgess
et al. (2018) where a factorized latent representation is learned using the Variational Autoencoder
(VAE) framework. In particular, the authors of Burgess et al. (2018), propose to overcome the disen-
tangling versus reconstruction trade-off by progressively allowing a larger divergence between the
factorized prior distribution and the latent posterior in a VAE.
Related to the task of varying the factors of image generation is that of domain-transfer (Reed et al.,
2015; Zhu et al., 2017; Isola et al., 2017; Choi et al., 2017; Donahue et al., 2016; Liu & Tuzel, 2016).
Here the challenge is to “translate” an image into a domain for which examples of the original image
are unknown and not available during training. For example, in the face generation task, the target
domain can represent a change of facial attribute such as wearing eyeglasses or not, gender, age, etc.
(Liu & Tuzel, 2016; Perarnau et al., 2016; Yan et al., 2016; Choi et al., 2017).
3	Unsupervised Progressive Learning of Disentangled
Representations
In this section we detail how the Jacobian supervision motivated in Section 1 can be applied, by
ways of a practical example. We will use the Jacobian supervision to learn a disentangled image
representation, where the main factors of variation are progressively discovered and learned unsu-
pervised.
We start with a simple autoencoder model, the teacher T, identified by its encoder and decoder parts
(ET, DT). The output of the encoder (the latent code) is split into two parts. One part corresponds
to the factors of variation y ∈ Rk and the other part corresponds to the nuisance variables, z ∈ Rd .
We begin by using k = 2 and d = 0, meaning that the latent code of the teacher is only 2-
dimensional. We consider the information encoded in these two variables as the two principal factors
of variation in the dataset. This choice was done merely for visualization purposes (Figure 1).
For this example, we trained a 3-layer multi-layer perceptron (MLP) on MNIST digits, using only
the L2 reconstruction loss. We used BatchNorm at the end of the encoder, so that the distribution
of y is normalized inside a mini-batch. In Figure 1 (a) we show the result of sampling this two-
dimensional variable and feeding the samples to the decoder DT. The resulting digits are blurry, but
the hidden variables learned to encode the digit class.
Next, we create a student autoencoder model (ES, DS), similar to the teacher, but with a larger
latent code. Namely, k = 2 and d = 1 instead of d = 0, so that the latent code has now an extra
dimension and the reconstruction can be improved. In order to try to maintain the conditioning of
the digit class by the 2D hidden variable y, we will impose that the Jacobian of the student with
respect to y be the same as that of the teacher, as in (1). How to achieve this is described next.
We take two random samples from the training set x1 and x2, and feed them to the student autoen-
coder, producing two sets of latent codes: (yS, ZS) and (yS, ZS), and two reconstructions XS and
XS, respectively. We then SWaP the parts of the latent code to form (yS, ZS) and (yS, ZS) and feed
them to the student decoder to to obtain their respective reconstructions XSI and XS2. We also feed
3
Published as a conference paper at ICLR 2019
33JM∕∕∕∕∕7
333Jrz∕∕77
333sg7∕∕77
553 SoO3 I 7 7 7
ss6∕bzl 4∙0∕7
GJx342aqc-∙7
OGOAU3J u∙77
O0。。〃，“夕7
ooo4,ʃ夕7 7
OOo0cΓ777
(O6 G 42
&66。〃
ooo^ʃ
。。。乡
0 0。C,
√√
7 7
7 7
&
q
7
7
5
5
5
S
S
j
3 3jʃʃ///77
333JF∕∕∕∕7
333QU0tf∕∕∕77
(a)	(b)
(c)
乙乙乙42.2.2-?.OK
乙乙乙2.2.2.2-?-?-4
乙乙乙 22.2.2.?.?.4
乙乙乙ɪzɪi?.-?.^
乙乙ɪzzzz⅛?.^
乙ɪɪzz Z Z 2⅛⅛
2222222ZZ⅛
ɪɪz Z 2 2 Z Z Z Z
2ZzzzzzznZ
2 2Z2ZZZZZZ
q
7
7
7
Figure 1:	Unsupervised learning of disentangled representations on MNIST digits, using Jacobian
supervision. (a) Output of teacher model (k = 2, d = 0) when varying its two hidden units. (b)
Output of the final student model (k = 2, d = 14), while varying the same hidden units. The
Jacobian supervision makes the model maintain control of the factors of variation of the teacher,
while obtaining significantly better reconstruction. (c) A student model (k = 2, d = 14) trained
without Jacobian supervision loses the control of the factor of variation discovered by the teacher.
(d) Performance curves for the test set, as training of the student progresses. The gray vertical bars
indicate the moments where the latent code was progressively grown by one hidden unit.
the Same pair of images to the teacher autoencoder to obtain yT, yT, XT, XT. Note that the teacher
encoder in this case does not produce a z .
We observe, by a first-order Taylor expansion, that
DT(y2T)=DT(y1T)+JT(y1T)(y2T-y1T)+oT(||y2T-y1T||),	(2)
and
DS(y2S,z1S) =DS(y1S,z1S)+JS(y1S,z1S) (y2S,z1S)-(y1S,z1S) +oS(||(y2S,z1S)-(y1S,z1S)||)
2, 1	1, 1 S 1, 1	2, 1	1, 1	2, 1	1 , 1
(3)
=DS(y1S,z1S)+JS(y1S,z1S)(y2S-y1S,0)+oS(||y2S-y1S||),	(4)
where JT and JS are the Jacobian of the teacher and student decoders respectively.
Suppose
y1S = y1T and y2S = y2T ,	(5)
and
DT (y2T) -DT(y1T) =DS(y2S,z1S) -DS(y1S,z1S)	(6)
then, by simple arithmetic,
JS(y1, z1) [(y2 - y1, 0)] ≈ JT(y1)(y2- y1),	(7)
where, since we assume (5) holds, we dropped the superscripts for clarity.
What (7) expresses is that the partial derivative of the output with respect to the latent variables y in
the direction of (y2 - y1) is approximately the same for the student model and the teacher model.
To achieve this, the proposed method consists essentially in enforcing the assumptions in (5) and (6)
by simple reconstruction losses used during training of the student. Note that one could exhaustively
explore partial derivatives in all the canonical directions of the space. In our case however, by
visiting random pairs during training, we impose the constraint in (7) for random directions sampled
from the data itself. This allows for more efficient training than exhaustive exploration.
Putting everything together, the loss function for training the student autoencoder with Jacobian
supervision is composed of a reconstruction part Lrec and a Jacobian part Ljac :
Lrec(x,ES,DS) := ||x - DS(ES(x))||2 = ||x - DS(yS, ZS)||2 = ||x - XS||2	(8)
Ljac(X,Es ,Ds ) 二 N ||yS - yτ ||2 + λf || (Dt (yT) -DT (yT))-(DS (yS, ZS) -DS (yS, ZS)) ||2
(9)
4
Published as a conference paper at ICLR 2019
d=1
d=2
d=3
d=4
77777777^^
7 7 7 7 7 7,
Figure 2:	3rd to 6th principal factors of variation discovered by our unsupervised algorithm. The
first two factors of variation are learned by the first teacher model (Figure 1 (a)). Each time a hidden
unit is added to the autoencoder, a new factor of variation is discovered and learned. Each row shows
the variation of the newly discovered factor for three different validation samples, while fixing all
the other variables. The unsupervised discovered factors are related to stroke and handwriting style.
where the subscript j indicates a paired random sample. For the experiments in Figure 1 we used
λy = 0.25, λdiff = 0.1. Table 4 in the appendix presents ablation studies on these hyperparameters.
In practice, we found it also helps to add a term computing the cross-covariance between y and z,
to obtain further decorrelation between disentangled features (Cheung et al., 2014):
1M	2
Lxcov(y, z) ：= X 8 X (Zm - Zi)Hm - y)j ,	(⑼
where M is the number of samples in the data batch, m is an index over samples and i, j index
feature dimensions, and zZi and yZj denote means over samples. In our experiments we weigh this
loss with λxcov = 1e-3.
Once the student model is trained, it generates a better reconstructed image than the teacher model,
thanks to the expanded latent code, while maintaining the conditioning of the output that the teacher
had. The extra variable in the student latent code will be exploited by the autoencoder to learn the
next important factor of variation in the dataset. Examples of factors of variations progressively
learned in this way are shown in Figure 2.
To progressively obtain an unsupervised disentangled representation we do the following procedure.
After training of the student with k = 2, d = 1 is finished, we consider this model as a new teacher
(equivalent to k = 3), and we create a new student model with one more hidden unit (equivalent to
k = 3, d = 1). We then repeat the same procedure. Results of repeating this procedure 14 times,
using 100 epochs for each stage are shown in Figure 1. In Figure 1(b), we show how the resulting
final model can maintain the conditioning of the digit class, while obtaining a much better recon-
struction. A model trained progressively until reaching the same latent code dimension but without
Jacobian supervision, and only the cross-covariance loss for disentangling (Cheung et al., 2014), is
shown in Figure 1(c). This model also obtains good reconstruction but loses the conditioning. For
this model we also found λxcov = 1e-3 to give the best result.
To quantitatively evaluate the disentangling performance of each model, we look at how the first two
latent units (k = 2) control the digit class in each model. We take two images of different digits from
the test set, feed them to the encoder, swap their corresponding y subvector and feed the fabricated
latent codes to the decoder. We then run a pre-trained MNIST classifier in the generated image to
see if the class was correctly swapped. The quantitative results are shown in Table 1. We observe
that the reconstruction-disentanglement trade-off is indeed more advantageous for the student with
Jacobian supervision.
To complement this section, we present results of the unsupervised progressive learning of disen-
tangled representations for the SVHN dataset (Netzer et al., 2011) in Section A.5 in the Appendix. 4
4 Application to Facial Attribute Modification
In photographs of human faces, many factors of variation affect the image formation, such as subject
identity, pose, illumination, viewpoint, etc., or even more subtle ones such as gender, age, expres-
sion. Modern facial manipulation algorithms allow the user to control these factors in the generative
process. Our goal here is to obtain a model that has good control of these factors and produces faith-
ful image reconstruction at the same time. We shall do so using the Jacobian supervision introduced
5
Published as a conference paper at ICLR 2019
Table 1: Quantitative comparison of the disentanglement and reconstruction performance of the
unsupervised method on MNIST digits.
Model	d	successful class swaps	reconstruction MSE
Teacher	0	94.3%	0.036
Student with Jacobian supervision	14	61.7%	0.014
Student with Jacobian supervision	18	52.1%	0.012
Student without Jacobian supervision	14	32.6%	0.011
Random weights	14	9.8%	0.116
Figure 3: Diagram of the proposed training procedure for facial attributes disentangling. E and D
always denote the same encoder and decoder module, respectively. Images x1 and x2 are randomly
sampled and do not need to share any attribute or class. Their ground truth attribute labels are y 1 and
y2 respectively. The latent code is split into a vector predicting the attributes y and an unspecified
part z . Shaded E indicates its weights are frozen, i.e., any loss over the indicated output does not
affect its weights.
in Section 3. In this more challenging case, the disentangling will be first learned by a teacher au-
toencoder using available annotations and an original training procedure. After a teacher is trained
to correctly disentangle and control said attributes, a student model will be trained to improve the
visual quality of the reconstruction, while maintaining the attribute manipulation ability.
4.1	Model Architecture and Loss Function
We begin by training a teacher model for effective disentangling at the cost of low quality reconstruc-
tion. Figure 3 shows a diagram of the training architecture for the teacher model. Let x ∈ RH×W×3
be an image with annotated ground truth binary attributes y ∈ {-1,1}k, where k is the number of
attributes for which annotations are available. Our goal is to learn the parameters of the encoder
ET : RH×W×3 → Rk+d and the decoder DT : Rk+d → RH×W×3 such that ET (x) = (y, z) and
DT(y, Z) = X ≈ x (Figure 3, top). Ideally, y ∈ Rk should encode the specified attributes of x,
while z ∈ Rd should encode the remaining information necessary for reconstruction.
The training of the teacher is divided into two steps. First, the autoencoder reconstructs the input
x, while at the same time predicting in y the ground truth labels for the attributes y. Second, the
attributes part of the latent code y is swapped with that of another training sample (Figure 3, bottom).
The randomly fabricated latent code is fed into the decoder to produce a new image. Typically, this
combination of factors and nuisance variables is not represented in the training set, so evaluating
the reconstruction is not possible. Instead, we use the same encoder to assess the new image: If the
disentangling is achieved, the part of the latent code that is not related to the attributes should be
the same for the existing and fabricated images, and the predicted factors should match those of the
sample from which they were copied.
In what follows, we describe step by step the loss function used for training, which consists of the
sum of multiple loss terms. Note that, contrary to relevant recent methods (Mathieu et al., 2016;
Lample et al., 2017; Szabo et al., 2017), the proposed method does not require adversarial training.
Reconstruction loss. The first task of the autoencoder is to reconstruct the input image. The first
term of the loss is given by the L2 reconstruction loss, as in (8).
6
Published as a conference paper at ICLR 2019
Prediction loss. In order to encourage y to encode the original attributes of x indicated in the
ground truth label y, We add the following penalty based on the hinge loss with margin 1:
1k
Lpred(y, y) = k Emax(I - yiyi, 0),	(11)
i=1
where the subscript [i] indicates the ith attribute. Compared to recent related methods (Perarnau
et al., 2016; Lample et al., 2017), the decoder sees the real-valued predicted attributes instead of an
inserted vector of binary attribute labels. This allows the decoder to naturally learn from continuous
attribute variables, leaving a degree of freedom to encode subtle variations of the attributes.
Cycle-consistency loss. Recall our goal is to control variations of the attributes in the generated
image, with the ability to generalize to combinations of content and attributes that are not present in
the training set. Suppose we have two randomly sampled images x1 and x2 as in Figure 3. After
obtaining (y1, z1) = E(x1) and (y2, z2) = E(x2), we form the new artificial latent code (y2, z1).
Ideally, using this code, the decoder should produce an image with the attributes of x2 and the con-
tent of x1 . Such an image typically does not exist in the training set, so using a reconstruction loss
is not possible. Instead, we resort to a cycle-consistency loss (Zhu et al., 2017). We input this image
to the same encoder, which will produce a new code that we denote as (y20 , z10 ) = ET (DT (y2, z1)).
If the decoder correctly generates an image with attributes y2, and the encoder is good at predicting
the input image attributes, then y20 should predict y2 . We use again the hinge loss to enforce this:
1k
LcycI = k EmaX(I- mg1, 0).	(12)
i=1
Here we could have used any random values instead of the sampled y2. However, we found that sam-
pling predictions from the data eases the task of the decoder, as it is given combinations of attributes
that it has already seen. Despite this simplification, the decoder shows remarkable generalization to
unseen values of the specified attributes y during evaluation.
Finally, we add a cycle-consistency check on the unspecified part of the latent code, z1 and z10 :
Lcyc2 = ||z1 - z10 ||2
(13)
Encoder freezing. The training approach we just described presents a major pitfall. The reversed
autoencoder could learn to replicate the input code (y2, z1) by encoding this information inside a
latent image in whatever way it finds easier, that does not induce a natural attribute variation. To
avoid this issue, a key ingredient of the procedure is to freeze the weights of the encoder when back-
propagating Lcyc1 and Lcyc2. This forces the decoder to produce a naturally looking image so that
the encoder correctly classifies its attributes.
Global teacher loss. Overall, the global loss used to train the teacher is the sum of the five terms:
L(θE, θD)
λ1Lrec + λ2 Lpred + λ3Lxcov + λ4Lcyc1
+ λ5 Lcyc2 ,
(14)
where λi ∈ R, i = 1 : 5 represent weights for each term in the sum. Details on how their values
are found and how we optimize (14) in practice are described in the next section. Ablation studies
showing the contribution of each loss are shown in Section A.3 in the appendix.
Student training. After the teacher is trained, we create a student autoencoder model with a larger
dimension for the nuisance variables z and train it using only reconstruction and Jacobian supervi-
sion ((8) and (9)), as detailed in the next section.
4.2	Implementation
We implement both teacher and student autoencoders as Convolutional Neural Networks (CNN).
Further architecture and implementation details are detailed in the Appendix. We train and evaluate
our method on the standard CelebA dataset (Liu et al., 2015), which contains 200,000 aligned faces
of celebrities with 40 annotated attributes.
The unspecified part of latent code (z) of the teacher autoencoder is implemented as a feature map
of 512 channels of size 2×2. To encode the attributes part y, we concatenate an additional k = 40
7
Published as a conference paper at ICLR 2019
channels. At the output of the encoder the values of these 40 channels are averaged, so the actual
latent vector has k = 40 and d = 2048, dimensions for y and z respectively.
The decoder uses a symmetrical architecture and, following Lample et al. (2017), the attribute pre-
diction y is concatenated as constant channels to every feature map of the decoder.
We perform grid search to find the values of the weights in (14) by training for 10 epochs and
evaluating on a hold-out validation set. The values we used in the experiments in this paper are
λ1 = 102,λ2 = 10-1,λ3 = 10-1,λ4 = 10-4,λ5 = 10-5.
4.2.1	Teacher Training
At the beginning of the training of the teacher, the weights of the cycle-consistency losses λ4 and λ5
are set to 0, so the autoencoder is only trained for reconstruction (Lrec), attribute prediction (Lpred)
and linear decorrelation (Lcov ). After 100 training epochs, we resume the training turning on Lcyc1
and Lcyc2 and training for another 100 epochs. At each iteration, we do the parameter updates in two
separate steps. We first update for L1 = λ1Lrec + λ2Lpred + λ3Lcov. Then, freezing the encoder,
we do the update (only for the decoder), for L2 = λ4Lcyc1 + λ5Lcyc2 .
4.2.2	student training
After the teacher autoencoder training is completed, we create the student model by appending new
convolutional filters to the output of the encoder and the input of the decoder, so that the effective
dimension of the latent code is increased.
In this experiment, we first doubled the size of the latent code from d = 2048 to d = 4096 at the
200th epoch and then from d = 4096 to d = 8192 at the 400th epoch. Note that this is different to
the experiment of Section 3, where we grew d by one unit at at time.
We initialize the weights of the student with the weights of the teacher wherever possible. Then, we
train the student using the reconstruction loss (8) and the Jacobian loss (9) as defined in Section 3,
using λy = 1, λdiff = 50, and no prediction nor cycle-consistency loss (λ2 = λ4 = λ5 = 0). The
hyperparameters were found by quantitative and qualitative evaluation on a separate validation set.
4.3	Experimental Results
From CelebA, we use 162,770 images of size 256x256 for training and the rest for validation. All
the result figures in this paper show images from the validation set and were obtained using the same
single model.
For each model, we evaluated quantitatively how well the generated image is conditioned to the
specified factors. To do this, for each image in the CelebA test set, we tried to flip each of the
disentangled attributes, one at a time (e.g. eyeglasses/no eyeglasses). The flipping is done by setting
the latent variable yi to -α ∙ sign(yi), with α > 0 a multiplier to exaggerate the attribute, found in
a separate validation set for each model (α = 40 for all models).
To verify that the attribute was successfully flipped in the generated image, we used an external
classifier trained to predict each of the attributes. We used the classifier provided by the authors of
Fader Networks, which was trained directly on the same training split of the CelebA dataset.
Table 2 and Figure 4 show the quantitative results we obtained. Most notably, at approximately
the same reconstruction performance, the student with Jacobian supervision is significantly better at
flipping attributes than the student without it. With the Jacobian supervision, the student maintains
almost the same disentangling and conditioning ability as the teacher. Note that these numbers could
be higher if we carefully chose a different value of α for each attribute.
To the best of our knowledge, Fader Networks (Lample et al., 2017) constitutes the state-of-the-art
in face image generation with continuous control of the facial attributes. For comparison, we trained
Fader Networks models using the authors’ implementation with d = 2048 and d = 8192 to disen-
tangle the same number of attributes as our model (k = 40), but the training did not converge (using
the same provided optimization hyperparameters). We conjecture that the adversarial discriminator
acting on the latent code harms the reconstruction and makes the optimization unstable. Compar-
isons with these models are shown in Table 2 and in Figures 7 and 8 in the appendix. We also show
8
Published as a conference paper at ICLR 2019
Table 2: Quantitative comparison of the disentanglement and reconstruction performance of the
evaluated models in the facial attribute manipulation task. Disentanglement is measured as the
ability to flip specified attributes by varying the corresponding latent unit.
Model	loss function	d	successful attribute flips	reconstruction MSE
Teacher	cycle-consistency	2048	73.1%	1.82e - 3
Student	Jacobian	8192	72.2%	1.08e - 3
Student	cycle-consistency	8192	42.7%	1.04e - 3
Fader Networks	adversarial	2048	43.1%	3.08e - 3
		8192	44.2%	1.83e - 3
Random weights		2048	20.2%	1.01e - 1
(笆 əjOoSM-τbfiB2fiθ'≡α
Figure 4: Disentanglement versus reconstruction trade-off for the facial attribute manipulation ex-
ample (top-left is better). The disentangling score measures the ability to flip facial attributes by
manipulating the corresponding latent variables.
in Figure 6 that our multiple-attribute model achieves similar performance to the single-attribute
Fader Networks models provided by the authors.
Finally, Figure 5 shows the result of manipulating 32 attributes for eight different subjects, using the
student model with Jacobian supervision. Note that our model is designed to learn the 40 attributes,
however in practice there are 8 of them which the model does not learn to manipulate, possibly
because they are poorly represented in the dataset (e.g. sideburns, wearing necktie) or too difficult
to generate (e.g. wearing hat, wearing earrings).
5 Conclusion
A natural trade-off between disentanglement and reconstruction exists when learning image repre-
sentations using autoencoder architectures. In this work, we showed that it is possible to overcome
this trade-off by first learning a teacher model that is good at disentangling and then imposing the
Jacobian of this model with respect to the disentangled variables to a student model that is good at
reconstruction. The student model then becomes good at both disentangling and reconstruction. We
showed two example applications of this idea. The first one was to progressively learn the principal
factors of variation in a dataset, in an unsupervised manner. The second application is a generative
model that is able to manipulate facial attributes in human faces. The resulting model is able to ma-
nipulate one order of magnitude more facial attributes than state-of-the-art methods, while obtaining
similar or superior visual results, and requiring no adversarial training.
9
Published as a conference paper at ICLR 2019
Reconstruction
Figure 5: Results of attribute manipulation with the student model with Jacobian supervision. All
the images were produced with the same model and belong to the test set.
Genre
Arched Eyebrows	Attractive
Mouth Open
Reconstruction
Age	Rosy Cheeks	Big Lips	Big Nose
Reconstruction
Pale Skin	Makeup	Blurry	Narrow Eyes
Reconstruction
Bushy Eyebrows	Chubby
Double Chin	Eyeglasses
Reconstruction
Goatee
Wearing Lipstick
Blond
Cheekbones
Reconstruction
5 o Clock shadow
Bald
Mustache
Beard
Bags Under Eyes
Oval Face
Black Hair
Pointy nose
Reconstruction
Receding Hairline
Brown hair
Smiling
Bangs
Reconstruction
(a)	(b)	(c)	(d)	(e)
Figure 6: Comparison with single-attribute Fader Networks models (Lample et al., 2017). (a) Orig-
inal image. (b) Reconstruction of Fader Networks with the provided ’eyeglasses’ model. (c) Our
teacher model achieves a sharper reconstruction using the same latent code dimension, and is able
to effectively manipulate up to 32 attributes, instead of only one. (d) Result of amplifying age with
Fader Networks with the provided aging model. (e) Our result for the same task.
10
Published as a conference paper at ICLR 2019
Acknowledgments
This work was supported by CAP-UDELAR Grant BPDN_2018_1. Experiments were partially run
on ClusterUY, National Center for Supercomputing, Uruguay.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Yoshua Bengio. Deep learning of representations: Looking forward. In International Conference
on Statistical Language and Speech Processing, pp. 1-37. Springer, 2013.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-
jardins, and Alexander Lerchner. Understanding disentangling in β-VAE. arXiv preprint
arXiv:1804.03599, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems, pp. 2172-2180, 2016.
Brian Cheung, Jesse A Livezey, Arjun K Bansal, and Bruno A Olshausen. Discovering hidden
factors of variation in deep networks. arXiv preprint arXiv:1412.6583, 2014.
Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Star-
GAN: Unified generative adversarial networks for multi-domain image-to-image translation.
arXiv preprint arXiv:1711.09020, 2017.
Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfit-
ting in deep networks by decorrelating representations. arXiv preprint arXiv:1511.06068, 2015.
Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5769-5779, 2017.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a
constrained variational framework. 2016.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Qiyang Hu, Attila Szabo, Tiziano Portenier, Paolo Favaro, and Matthias Zwicker. Disentangling
factors of variation by mixing them. arXiv preprint arXiv:1711.07410, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. arXiv preprint, 2017.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983,
2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
11
Published as a conference paper at ICLR 2019
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems,
pp. 3581-3589, 2014.
Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic Denoyer, et al. Fader
networks: Manipulating images by sliding attributes. In Advances in Neural Information Process-
ing Systems, pp. 5969-5978, 2017.
Ming-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In Advances in Neural
Information Processing Systems, pp. 469-477, 2016.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann
LeCun. Disentangling factors of variation in deep representation using adversarial training. In
Advances in Neural Information Processing Systems, pp. 5040-5048, 2016.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.
Xi Peng, Xiang Yu, Kihyuk Sohn, Dimitris N Metaxas, and Manmohan Chandraker. Reconstruction-
based disentanglement for pose-invariant face recognition. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pp. 1623-1632, 2017.
Guim Perarnau, Joost van de Weijer, Bogdan Raducanu, and Jose M. Alvarez. Invertible Conditional
GANs for image editing. In NIPS Workshop on Adversarial Training, 2016.
Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In Advances
in Neural Information Processing Systems, pp. 1252-1260, 2015.
Salah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent, and Mehdi Mirza. Disentangling fac-
tors of variation for facial expression recognition. In European Conference on Computer Vision,
pp. 808-822. Springer, 2012.
Attila Szabo, Qiyang Hu, Tiziano Portenier, Matthias Zwicker, and Paolo Favaro. Challenges in
disentangling independent factors of variation. arXiv preprint arXiv:1711.02245, 2017.
Attila Szabo, Qiyang Hu, Tiziano Portenier, Matthias Zwicker, and Paolo Favaro. Understanding
degeneracies and ambiguities in attribute transfer. In The European Conference on Computer
Vision (ECCV), September 2018.
Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled representation learning GAN for pose-invariant
face recognition. In CVPR, volume 3, pp. 7, 2017.
Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image
generation from visual attributes. In European Conference on Computer Vision, pp. 776-791.
Springer, 2016.
Jimei Yang, Scott E Reed, Ming-Hsuan Yang, and Honglak Lee. Weakly-supervised disentangling
with recurrent transformations for 3d view synthesis. In Advances in Neural Information Pro-
cessing Systems, pp. 1099-1107, 2015.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In IEEE International Conference on Computer
Vision, 2017.
12
Published as a conference paper at ICLR 2019
Table 3: Summary comparison of the characteristics of recent related methods. Our method has
advantages over each of them, and together with Fader Networks are the only ones to generate
256×256 images while continuously varying the generated facial attributes.
Method	end-to- end training	requires aligned pairs	requires adversarial training	face image resolution	number of attributes per model	generates continuous attributes
CoGAN	yes ✓	no ✓	yes X	128x128 X	1X	no X
IcGAN	no X	no ✓	yes X	64x64 X	18 ✓	no X
Attribute2Image	no ✓	no ✓	no ✓	64x64 X	1X	yes ✓
StarGAN	yes ✓	no ✓	yes X	128x128 X	7X	no X
Fader Networks	yes ✓	no ✓	yes X	256x256 ✓	3X	yes ✓
This work	yes ✓	no ✓	no ✓	256x256 ✓	32 ✓	yes ✓
A Appendix
A. 1 Implementation Details for Section 3
For the autoencoder utilized for experiments in Section 3, we used the following architecture. For
the encoder:
F (768, 256) → ReLU → F (256, 128) → ReLU → F (128, 64) → ReLU → FC(64, k + d)
where F (I, O) indicates a fully connected layer with I inputs and O outputs. For the first teacher
model (k = 2, d = 0), we also used BatchNorm after the encoder output.
The decoder is the exact symmetric of the encoder, with a Tanh layer appended at the end.
We used Adam (Kingma & Ba, 2014) with a learning rate of 3e-4, a batch size of 128 and weight
decay coefficient 1e-6.
A.2 Implementation Details for Section 4
Following Lample et al. (2017), we used convolutional blocks of Convolution-BatchNorm-ReLU
layers and a geometric reduction in spatial resolution by using stride 2. The convolutional kernels
are all of size 4×4 with padding of 1, and we use Leaky ReLU with slope 0.2. The input to the
encoder is a 256×256 image. Denoting by k the number of attributes, the encoder architecture can
be summarized as:
C(16) → C(32) → C(64) → C(128) → C(256) → C(512) → C(512 + k),
where C(f) indicates a convolutional block with f output channels.
The decoder architecture can be summarized as:
D(512 + k) →D(512+k) →D(256+k) →D(128+k) → D(64 + k) → D(32 + k) →D(16+k),
where D(f) in this case indicates a deconvolutional block doing ×2 upsampling (using transposed
convolutions, BatchNorm and ReLU) with f input channels.
We trained all networks using Adam, with learning rate of 0.002, β1 = 0.5 and β2 = 0.999. We use
a batch size of 128.
Table 3 shows a comparison chart between the proposed and related methods.
A.2.1	Student Model
For the student model, we only need to change the last layer in the encoder from C(512 + k) to
C(1024 + k) in the first stage and C(2048 + k) in the second stage. Similarly, the first layer of the
decoder was changed from D(512 + k) to D(1024 + k) in the first stage and D(2048 + k) in the
second stage.
13
Published as a conference paper at ICLR 2019
A.3 Ablation Studies
Table 4: Ablation study of the weight of each loss term for the unsupervised example of Section 3,
using k = 2 and d = 14 for the student.
λy	λdiff	λxcov	sUccessfUl class sWaps	reconstrUction MSE
0	0	1.0e-1	33.6%	1.15e-2
0	0	1.0e-2	32.6%	1.17e-2
0	0	1.0e-3	32.3%	1.12e-2
0	0	1.0e-4	31.1%	1.13e-2
2.5e-1	1.0e-1	1.0e-1	63.7%	1.55e-2
2.5e-1	1.0e-1	1.0e-2	65.2%	1.55e-2
2.5e-1	1.0e-1	1.0e-3	61.7%	1.42e-2
1.0	1.0	1.0e-3	70.5%	1.86e-2
1.0e-1	1.0	1.0e-3	59.4%	1.70e-2
1.0	1.0e-1	1.0e-3	65.0%	1.58e-2
1.0e-1	1.0e-1	1.0e-3	57.9%	1.38e-2
1.0e-2	1.0e-1	1.0e-3	52.3%	1.30e-2
1.0e-1	1.0e-2	1.0e-3	52.3%	1.28e-2
1.0e-2	1.0e-2	1.0e-3	42.5%	1.17e-2
Table 5: Ablation study of the impact of Jacobian and cycle-consistency losses in the training of
the student model in the facial attribute manipulation task. The results correspond to training a
student with d = 4096 for 50 epochs, and where the teacher was trained only with reconstruction
and Cross-Covariance losses, With d = 2048. All models Used the Same teacher.________
λ4(Lcyc1)	λ5(Lcyc2)	λdiff λy		reconstrUction MSE		sUccessfUl flips
0	0	50	1	1.76e -	3	70.3%
0	0	10	1	1.67e -	3	64.8%
0	0	1	10	1.85e -	3	49.1%
0	0	1	1	1.69e -	3	46.8%
1e-4	1e-5	0	0	1.78e -	3	43.2%
1e-4	1e-5	50	1	1.84e -	3	70.7%
Table 6: Ablation study of the weigh of the cross-covariance loss in the facial attribute manipulation
example. The resUlts Correspond to training a teaCher model With d = 2048, from sCratCh and for
50 epochs.
λxcov	reconstruction MSE	successful flips
1e-3	2.39e - 3	51.4%
1e-2	2.50e - 3	53.3%
1e-1	2.71e - 3	49.9%
14
Published as a conference paper at ICLR 2019
A.4 Extended Qualitative Results for Section 4
Figure 7: Results of image reconstruction on test images. Left to right: original image, reconstruc-
tion by the student model with Jacobian supervision (d = 8192), by the teacher (d = 2048), and by
the Fader Networks model trained for multiple attributes (d = 8192).
15
Published as a conference paper at ICLR 2019
Figure 8: Results of smoothly controlling attributes for our model and Fader Networks. In each row,
our result is shown on the top and Fader Networks’ on the bottom. From top to bottom, the attributes
are: Arched eyebrows, Bags under eyes, Big nose, Pale skin, Age.
16
Published as a conference paper at ICLR 2019
A.5 Unsupervised Progressive Learning of Disentangled Representations on
SVHN Dataset
We applied the procedure described in Section 3 for progressive unsupervised learning of disentan-
gled representations to the Street View House Numbers (SVHN) dataset (Netzer et al., 2011). The
SVHN dataset contains 73,257 32×32 RGB images for training.
For this experiment, the encoder architecture is:
C(64) → C(128) → C(256) → C(k+d) → BatchNorm.
Here, C(n) represents a convolutional block with n 3 × 3 filters and zero padding, ReLU activation
function and average pooling.
The decoder architecture is:
D(256) → D(128) → D(64) → D(3).
Here, D(n) represents a ×2 upconvolution block with n 4 × 4 filters and zero padding, ReLU
activation function and average pooling.
The latent code was started with k = 2 and d = 0 and progressively grown to k = 2, d = 16. Each
stage was trained for 25 epochs. We used λy = 0.025, λdiff = 0.01. We used Adam with a learning
rate of 3e - 4, a batch size of 128 and weight decay coefficient 1e - 6.
The first teacher model (k = 2, d = 0) achieves a reconstruction MSE of 1.94e-2 and the final
student model (k = 2, d = 16) a reconstruction MSE of 4.06e-3.
Figure 9 shows the two principal factors of variation learned by the first teacher model (correspond-
ing to k = 2, d = 0). Contrary to the MNIST example of Section 3, here the two main factors of
variation are not related to the digit class, but to the shading of the digit. The progressive growth of
the latent code is carried on from d = 0 to d = 16. The following factors of variation are related
to lighting, contrast and color (see Figure 10). In this case, the unsupervised progressive method
discovered factors that appear related to the digit class at the 9th and 10th steps of the progression.
Figure 11 shows how the digit class can be controlled by the student with d = 16 by varying these
factors. Because of the Jacobian supervision, the student is able to control the digit class while main-
taining the style of the digit. Finally, in Figure 12 we show that the student also maintains control of
the two main factors of variation discovered by the first teacher.
Figure 9:	The two principal factors of variation learned on SVHN appear related to shading of the
digit. Left to right: darker to lighter. Top to bottom: light color on the left to light color on the right.
17
Published as a conference paper at ICLR 2019
d=1
d=2
d=3
11 8 fl U Illltllll
LiΓΠ Tmiiiiiiiiiii
LJ ♦ ■ LtL⅛l⅛l⅛lg
Figure 10:	Third, fourth and fifth factors of variation automatically discovered on SVHN. Each row
corresponds to one factor and each column corresponds to one sample. Each factor is varied while
maintaining the rest of the latent units fixed.
S( 3∣ 3∣ 34 OIa
Il Il 3( 34 3Ia
31 3134 3434»
2, ” 24 24 2 24
21 %2m2
74 24 24 24 24 21
U Oibl bl b<
W 网 ∣q 191919 K) WKl W
»4 Sq 呜 W W W 用 io WW
* »4 H W W ∣9 访 io K) H>
0 0 M 网 W »9 »9IO 10IO
W H种样种卜9 >9 »9 IO W
H »5 19 »9 19 囚 19 出
H It Il ∣1∣11) 19 19 19 技
(1 Il 11 If n 191319 19 £
∣1 «1 Il H ItAIViniMM
11 Itn π n nn nn j?
44j4∣4∣4∣6464 (U(Hth
44446IH6I6/G，。」
J44∣4i6iH646 阳 0,
444 , 616464 Q(M0,
I I IlISd川8I6J0J6
Itll444W
1111 1」Is9J9J9.
117 11 *29J9J9∙
IllllZZz27
7 7 7 7 777 222
rmnttθiiiɪjɑraif
IKiriaafflflQiraIr
gιcιcιr⅝gqπlι¾W
0333111ElE
谓口皿¾f¾m
”富心L昭mm 口:
,{N / W黑 ⅝f¾∣¾∏j∏H
j⅛ H ,旧m
WlH诵讨诵。彳喇同i
Figure 11:	Factors of variation related to the center digit class appear to emerge on the 9th and
10th discovered factor during the unsupervised progressive procedure described in Section 3. Here
we show how the student model with Jacobian supervision and d = 16 can be used to manipulate
the digit class while approximately maintaining the style of the digit, by varying the latent units
corresponding to those factors. The bottom row shows the original images (reconstructed by the
autoencoder). All images are from the test set and were not seen during training.
Figure 12: Result of the student with Jacobian supervision (d = 16) when varying the two factors
learned by the teacher (Fig. 9), for four different images (whose reconstruction is shown on the
bottom row). The conditioning related to shading is maintained. (Left to right: darker to lighter.
Top to bottom: light color on the left to light color on the right.) All images are from the test set and
were not seen during training.
4MMMM 4M1
4MMMI4j4Mj
4HI4MMJ4MJ
4HMM14J414J
4MI4M14J4J4
4HJ4∣4J4 4 <
I
■MM 2<2<2I2<2(
21 2( 2( 2( 2< 2( 2) 2«
M 阚 2<2<2<2<2<
2 洌到2 2 2 2(2(
Φ 24242 2(242«
[2 2树2t2 242(2(
12<2t2<2t2(2Ut2∣
W242(2(2<2(2∣
2<2<2<2<2<2∏t2∣
5β½U<2<2(2t2∣
18