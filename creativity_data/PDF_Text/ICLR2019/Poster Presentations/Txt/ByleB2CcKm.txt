Published as a conference paper at ICLR 2019
Learning Procedural Abstractions and Eval-
uating Discrete Latent Temporal Structure
Karan Goel
Department of Computer Science
Stanford University
kgoel@cs.stanford.edu
Emma Brunskill
Department of Computer Science
Stanford University
ebrun@cs.stanford.edu
Ab stract
Clustering methods and latent variable models are often used as tools for pat-
tern mining and discovery of latent structure in time-series data. In this work,
we consider the problem of learning procedural abstractions from possibly high-
dimensional observational sequences, such as video demonstrations. Given a
dataset of time-series, the goal is to identify the latent sequence of steps common
to them and label each time-series with the temporal extent of these procedural
steps. We introduce a hierarchical Bayesian model called Prism that models the
realization of a common procedure across multiple time-series, and can recover
procedural abstractions without supervision. We also bring to light two charac-
teristics ignored by traditional evaluation criteria when evaluating latent temporal
labelings (temporal clusterings) - segment structure, and repeated structure - and
develop new metrics tailored to their evaluation. We demonstrate that our met-
rics improve interpretability and ease of analysis for evaluation on benchmark
time-series datasets. Results on benchmark and video datasets indicate that Prism
outperforms standard sequence models as well as state-of-the-art techniques in
identifying procedural abstractions.
1	Introduction
A fundamental problem in machine learning is the discovery of structure in unsupervised data.
In this work we are particularly interested in uncovering the latent structure in procedural data -
potentially high-dimensional observational time-series resulting from some latent sequence of events.
For example, a video showing how to change a tire might involve jacking up the car, removing
one nut from the wheel, removing a second nut, etc. There exists an enormous wealth of videos
of procedures available and inferring the latent structure of such videos could be useful in a huge
range of applications, from supporting search queries (”find all segments where someone jacks up the
car”) to adding to nascent work on learning from observation (Borsa et al., 2017; Stadie et al., 2017;
Torabi et al., 2018) in which robots may be taught merely by observing a procedure performed in a
video. Procedural learning is related to activity recognition and the broader field of latent temporal
dynamical learning, but focuses on the simpler but still ubiquitous setting where the latent activity
sequence is a fixed procedure.
As we started to develop methods for performing latent procedure inference from temporal data, we
considered how to evaluate our resulting methods. One important evaluation protocol for unsupervised
learning methods is external evaluation, where predicted labelings of the discrete variables are
compared to ground-truth labels (Rosenberg & Hirschberg, 2007). But the precise criteria for this
evaluation is critical. Though there are an enormous number of methods for modeling and inferring
latent structure in dynamical systems, including Hidden Markov models, dynamic Bayesian networks,
and temporal clustering for time-series data e.g. Fox et al. (2008b; 2014); Krishnan et al. (2015);
Linderman et al. (2017); Zhou et al. (2008; 2013); Vidal & Favaro (2014), the external evaluation for
such approaches is typically done using the same clustering metrics that are used for non-temporal
data: metrics like the normalized mutual information (NMI) (Strehl & Ghosh, 2002) or by computing
cluster correspondences using the Munkres algorithm (Munkres, 1957). Unfortunately such metrics
disregard temporal information and can therefore make it hard to assess important considerations that
arise in latent temporal structure extraction, including in procedural inference.
1
Published as a conference paper at ICLR 2019
Ground Truth Temporal Clustering
Proposed Temporal Clustering
(b) Repeated structure captured by Prism
at different points in the video (3 left im-
ages) and across videos (right image).
(a) Prediction by PRISM for a cpr video. Note how
repeated structure (cream primitive) is recovered.
Figure 1: Procedural learning. Given an input set of temporal sequences all demonstrating the same
procedure we wish to infer the latent structure of that procedure. Here we show the results of our
method, PRISM, on inferring a cpr procedure from a set of videos in the INRIA Instructional videos
dataset. The ground truth temporal clustering on the upper left represents a human labeling of the
activities involved in a particular cpr video where color denotes the same activity. For example, the
activity denoted by the cream color occurred 4 times across the sequence.
To address the limitations of prior evaluation criteria for the external evaluation of discrete latent
temporal structure, our first contribution is the creation of new criteria. We identify at least two key
aspects of temporal latent structure discovery that are not captured well with existing non-temporal
metrics: segment structure, the temporal durations between transitions from one discrete latent state
to another, and repeated structure, where a particular latent variable may appear in multiple segments.
Figure1 shows an example latent sequence associated with a person performing cardiopulmonary
resuscitation (Cpr) - the colors denote the activity type (such as blowing air into the lungs) and the
temporal duration of a fixed activity type is shown as a continuous block of color. Our new metrics
can evaluate both segment structure and repeated structure, and we later demonstrate how these
metrics may be of broader interest in latent temporal dynamical learning, as they can illustrate and
illuminate different strengths and weaknesses of existing work on common benchmark datasets.
Equipped with new measures for evaluating success, we return to our key goal, inferring the latent
procedure observed in a set of high dimensional demonstrations. Past expressive models like Hidden
Markov models can struggle in this setting, since the resulting procedure may not be Markov: while
the setting can be forced to appear Markov by employing the history as part of the state, this can
reduce sample efficiency (by artificially increasing the latent space size). Additionally assuming the
system is Markov when it is not can mean inferring stochastic latent dynamical structure when it is
in fact deterministic. Recent work (Sener & Yao, 2018) tackles a closely related setting, but does
not allow repeated primitives, which is a key aspect of many procedures. Instead we introduce a
new generative Bayesian model (Prism) for identifying procedural abstractions without supervision.
Results on datasets for procedural tasks - including surgical demonstrations and how-to videos -
show that our method can learn procedural abstractions effectively compared with state-of-the-art
methods and popular baselines, and outperform prior work on both our new metrics and classic
metrics when repeated structure is present.
2	Definitions and Setting
Temporal Clustering. A time-series Xi = (xi1, xi2, . . . , ximi ), i ∈ [n] is a sequence of feature
vectors (or items) of length mi with xit ∈ Rd. A temporal clustering C = (c1 , . . . , cm) is a sequence
of cluster labels with ct ∈ ΓC where ΓC is a set of cluster labels. Temporal clusterings map
each item in the feature trajectory X to a cluster label in ΓC . We use Ci as shorthand to denote
the temporal clustering generated by some method for Xi . The ground truth temporal clustering
for Xi will be denoted by Gi, with corresponding ground truth label set ΓG . For simplicity, we
define Ca:b = (ca, ca+1 , . . . , cb) where 1 ≤ a ≤ b ≤ m are time indices. For α ∈ ΓC, let
C[α] = {t|ct = α, 1 ≤ t ≤ m} be the set of time indices in C whose cluster labels equal α. C[α] is
the cluster or partition (in the standard sense) corresponding to the label α. Temporal clusterings are
distinct from time-series segmentation (Chung et al., 2004) and changepoint detection (Killick et al.,
2012), which consider only identifying boundaries of different dynamic regimes in the time-series.
Segment. A segment is a contiguous sequence of identical cluster labels in C . Each cluster in C may
be split across multiple segments. We represent each segment as a pair of time indices (a, b), where a
represents the start time-index of the segment while b the end time-index (both included). Formally,
we let S(C, α) = {(a, b)|Ca:b = (ca = α, ca+1 = α, . . . , cb = α), 1 ≤ a ≤ b ≤ m} be a function
2
Published as a conference paper at ICLR 2019
(a) G is the ground truth temporal clustering and
C1 , C2 , C3 are possible temporal clusterings.
Figure 3: Problematic cases for traditional metrics, on a simple 6-step time-series.
C1 , C2 , C3 when converted to standard non-
(b)
temporal clusterings for use with traditional metrics.
that maps a temporal clustering C and cluster label α ∈ Γ to the set of segments associated with α in
C. We let SC = Sα∈Γ S(C, α) be the set of all segments in C.
Procedure. Let P be a function that removes running duplicates from a sequence of labels, yield-
ing a sequence of tokens. For instance, P(A, A, A, B, B, B, C, C, C, A, A, B) = (A, B, C, A, B)
yielding 5 tokens. For a temporal clustering C, we define its corresponding procedure to be P(C).
The procedure captures the primitives (as tokens) present in the tem-
poral clustering and the sequence in which they occur. We say that
C exhibits repeated structure if atleast one primitive is reused in C .
This corresponds to having multiple tokens of the same cluster label
in P(C). The procedure (A, B, C, A, B) contains repeated structure
since both A and B are reused. We also define a weight function
W that counts the number of running duplicates in a sequence e.g.
W(A,A,A,B,B,B,C,C,C,A,A,B) = (3,3,3,2,1). Notice that
W computes the length of each segment in the sequence. Weights in
W(C) thus have a one-to-one correspondence with tokens in P(C),
Figure 2: From top - Hme-
series, temporal clustering,
segments and procedure.
equaling the length of each token in the procedure. Taken together W(C), P(C) can be used to
recover C. Lastly, we let H(P1, P2, W1, W2) be a similarity measure between 2 temporal clusterings
represented by P1 , W1 and P2 , W2 respectively.
Fig 2 illustrates these ideas for a temporal clustering defined over the label set Γ = {A, B, C, D}.
The procedure exhibits repeated structure since B has (atleast) 2 tokens/segments corresponding to it.
3 Evaluating Latent Temporal S tructure
External clustering evaluation is the process of comparing a clustering to a ground-truth labeling
in order to evaluate it. Unfortunately, as we discuss below, using traditional clustering evaluation
criteria is problematic for evaluating temporal clusterings.
Existing evaluation criteria. The use of standard clustering metrics for evaluation is preva-
lent in prior work that attempts to evaluate latent temporal structure (Zhou et al., 2008; 2013;
Fox et al., 2008b; 2009; 2014; Krishnan et al., 2015; Sener & Yao, 2018; Hoai & De la Torre,
2012). For a temporal clustering C the widely used purity (Rosenberg & Hirschberg, 2007)
metric penalizes the presence of items from different ground truth labels in the same cluster.
This is a desirable property that a clustering evaluation criterion should have. Purity is defined
as purity = ml Pα∈Γc maxβ∈Γg |G[β] ∩C[α]∣, A related metric is homogeneity (Rosenberg &
Hirschberg, 2007), which captures a similar idea in a different way: homogeneity = 1 -
h(Γg TC)
h(Γg )
where H(Γg) is the entropy of the clustering G when partitioned by the labels Γg, H(Γg ∣Γc) is the
conditional entropy of G given C . Intuitively the conditional entropy term looks inside every cluster
in C, and checks the entropy of the items inside in terms of the ground truth labels. The conditional
entropy will be low (and homogeneity high) if every cluster in C contains items of only a single
ground truth label. Another important metric is completeness (Rosenberg & Hirschberg, 2007), which
prefers clusterings where all items from a ground-truth label lie in the same cluster i.e. the ground
truth label is not split across clusters. Completeness is defined identically to homogeneity, but with
the clusterings swapped G4―→ C.
3
Published as a conference paper at ICLR 2019
Both completeness and homogeneity are considered important criteria for clustering. Maximizing
either at the expense of the other is bad - high homogeneity and low completeness imply a clustering
that is too fine-grained (e.g. every item in its own cluster), while low homogeneity and high
completeness imply a clustering that is too coarse (e.g. a single cluster containing all items).
Therefore, metrics that balance these 2 criterion often perform best (Rosenberg & Hirschberg, 2007)
-normalized mutual information (NMI)(StrehI & Ghosh, 2002) and V-measure (Rosenberg &
Hirschberg, 2007) are examples of such metrics.
Another popular suite of metrics uses the Munkres algorithm (Zhou et al., 2008) - a one-to-one
correspondence between clusters and ground-truth labels is computed so that any classification
criterion (commonly accuracy) under this correspondence is maximized.
Characterizing latent temporal structure. Two major characteristics distinguish the evaluation of
temporal clusterings from the standard clustering case.
•	Repeated structure. The presence of repeated structure in ground-truth - ground-truth labels
that are split across several segments in the trajectory. Ideally, we want that this repeated
structure is identified correctly e.g. in Fig 3 identifying that the segments A0:2 , A4:6 are
repetitions of the same primitive.
•	Segment structure. The locations and sizes of the segments in ground-truth e.g. in Fig 3 we
would like temporal clusterings that switch between primitives at exactly the time points
t = 2, 4 with the same 3 segment structure.
Neither repeated or segment structure are seen in standard clusterings, since they treat data as a bag
of items.
Limitations of existing criteria. Existing metrics cannot score repeated or segment structure
because they convert temporal clusterings to standard clusterings - a contingency matrix (Rosenberg
& Hirschberg, 2007) - before evaluation. This erases the temporal information important to score
these ideas. In fact, qualitatively distinct temporal clusterings can be mapped to the same contingency
matrix, and receive the same score, which is clearly undesirable.
We use Fig 3 as an illustrative example to highlight these shortcomings. In Fig 3, C1 perfectly matches
G, while both C2 and C3 split the cluster G [A] - C2 into C2 [X] and C2 [Z] and C3 into C3 [X] and C3 [Z].
In addition, C2 also misses the repeated structure in A while C3 does not. C3 captures this repeated
structure as XZ, with X0:1Z1:2 corresponding to A0:2 and X4:5Z5:6 corresponding to A4:6. However,
it does so at the cost of creating more segments than C2 - ignoring labels, C2 has a segmentation
exactly like G . Intuitively, we prefer C1 > C3 > C2 if we care more about repeated structure, and
C1 > C2 > C3 , if we care more about the segmentation quality.
However, all 3 temporal clusterings C1 , C2 , C3 in Fig 3 are assigned a perfect purity or homogeneity,
since as seen in Fig 3b, no cluster in any of the 3 contains items from more than one ground-truth label.
Moreover, no traditional clustering criteria can distinguish C2 from C3 since their cluster composition
is identical, as seen in Fig 3b. We now discuss new evaluation criteria that can systematically evaluate
and score both repeated and segment structure.
3.1	Evaluating Repeated Structure
We describe a new evaluation criteria, called the repeated structure score (RSS) below. Algorithmi-
cally, the calculation of the RSS requires the following steps,
1.	Pick a ground-truth label β ∈ ΓG and find the set of segments S(G, β) marked with β in G.
These segments all exhibit repeated structure in G .
2.	Find Ca:b for every (a, b) ∈ S(G, β). This is the predicted temporal clustering restricted to a
particular segment (a, b) in G.
3.	For every such pair Ca1:b1 , Ca2:b2, use a scoring function H to compute H(Ca1:b1 , Ca2:b2).
4.	Aggregate these scores across all such pairs for every ground-truth label β ∈ ΓG ,
RSS
Σβ∈ΓG Σ(a1,b1),(a2,b2)∈S(G,β) H(Ca1:b1 , Ca2：b2 )
2 Pβ∈ΓG∣S (G ,β)l P(a,b)∈s(G,β)(b-a +1)
4
Published as a conference paper at ICLR 2019
where the denominator normalizes RSS to lie in [0, 1].
As a concrete example, in Fig 4a, we could pick A in G for
step 1 with S(G, β = A) = {(0, 2), (4, 7)}. For step 2, we
find C0:2 = (X, X) and C4:7 = (X, Z, Z). For step 3, we
must choose an appropriate scoring function H. While many
choices for H are possible, we let H(Ca1:b1 , Ca2:b2) be the to-
tal weight of the heaviest common sub-sequence or substring
(Jacobson & Vo, 1992) in Ca1:b1 and Ca2:b2 . Fig 4b uses this
choice of H to score the pair of segments We picked - We run
H ((X, X), (X, Z, Z)) and it returns a score of 3.
Intuitively, H tries to match procedures P(Ca1:b1) to P(Ca2:b2)
- finding the (heaviest) sequence of tokens common to both
P(Ca1:b1) and P (Ca2:b2), While respecting the temporal order-
ing of both procedures. At one extreme, if H evaluates to 0
then no overlapping tokens exist in P(Ca1:b1) and P (Ca2:b2).
This implies that C failed to identify repeated structure in
the tWo segments and a score of 0 is appropriate. At the
other extreme, if both segments folloW identical procedures
P(Ca1:b1) = P (Ca2:b2), they receive the maximum possible
score according to H; reflecting that C identified repeated struc-
ture perfectly. H can be computed efficiently using a dynamic
program in O(|P1||P2|).
(a) Example temporal clustering.
(b) Scoring C0:2 , C4:7 With
P(C0:2) = (X), W(C0:2) = (2),
P(C4:7)	=	(X, Z),
W (C4:7) = (1, 2) and running H
yields heaviest sub-sequence (X)
With score 3. C5:7 (dotted) does not
contribute to the score.
Figure 4: Example for repeated
structure scoring.
3.2	Evaluating Segment Structure
We first introduce a neW information-theoretic criteria - the label-agnostic segmentation score (LASS).
The main purpose of LASS is to score hoW Well C predicts the location of transition points betWeen
segments in G. We Want to identify and score cases Where C either oversegments (introduces extra
segments) or undersegments (omits segments) the time-series compared to G. In any temporal
clustering, the transition boundaries do not depend on the actual labeling, but only on Where the labels
sWitch. Therefore, LASS is label-agnostic and only requires the transition points betWeen segments.
Prior Work in changepoint detection and time-series segmentation (Killick et al., 2012) has attempted
to quantify this using detection criteria - e.g. if a changepoint close to one in ground-truth is detected,
a true positive is recorded. HoWever, these criteria (i) require specifying the tolerance interval for
detection, and (ii) don’t degrade gracefully With the segmentation, Which limits their applicability to
temporal clusterings.
Recall that SG, SC are the set of all segments in G and C. To compute LASS, the main quantity We
rely on is H (SC |SG) - a conditional entropy over segments in C. This can be Written as,
H (Sc∣Sg ) = - X	(b-n+1	X
(a,b)∈SG	(a而 ESca:b
(b — a + 1)	(b — α + 1)
(b — a + 1) °g (b — a + 1)
Intuitively, for each ground-truth segment (a, b) ∈ SG, We compute the (Weighted) entropy of C re-
stricted to (a, b). This entropy Will be > 0 only ifC creates neW segments in (a, b). On the other hand,
to detect under-segmentation, We can simply invert the roles of G and C by calculating H (SG |SC).
Finally, noting that H (SC|Sg) ≤ H (SC), We define LASS := 1 一 H(SC|SG)+H(SGISC).OVer-
segmentation (and under-segmentation) can also be measured separately by defining LASS—O :=
1 — HH(SS)G) and LASS-U := 1 — H(SSIS)C). The algebraic form chosen for the criteria relates it
back to existing information-theoretic criteria such as the NMI (Vinh et al., 2010).
Next, We extend the completeness metric defined earlier. While completeness is desirable for any
temporal clustering, using completeness directly has an unintended draWback - it penalizes repeated
structure. An example of this can be seen in Fig 3a, Where C3 is assigned the same completeness
score as C2 despite capturing repeated structure. Since the RSS already proVides a systematic Way of
eValuating presence/absence of repeated structure, We modify the completeness score slightly. We
define Segmemal completeness tobe 1 — HHCrS)G), where the conditioning is now on ground-truth
5
Published as a conference paper at ICLR 2019
segments rather than on ground-truth labels. Equivalently, we can view this as relabeling each
segment in ground-truth to be distinct, and then computing completeness. In this way, segmental
completeness serves a similar function to completeness without interfering with repeated structure
scoring. Homogeneity can also be extended to define segmental homogeneity similarly.
Lastly, we define an aggregate segment structure score (SSS),
SSS := 1 -
H (SC |Sg ) + H(SGISC) + H (Γc |Sg) + H(Γg∣Sc)
H(SC) + H(SG) + H(ΓC) + H(ΓG)
3.3	A Combined Evaluation Criteria
We have introduced several new metrics: (i) the RSS systematically scores repeated structure; (ii) the
LASS scores how well the transition structure is captured; (iii) the SSS provides a unified metric for
assessing segment structure. Note that all of these metrics obey the useful property of n-invariance
(Rosenberg & Hirschberg, 2007; Meila, 2007) - scaling the number of items (stretching the temporal
clustering) does not change the metric values. Finally, we provide a single evaluation measure for
temporal clusterings, the temporal structure score (TSS) that balances the RSS with the SSS.
TSS .= (1 + β) ∙ RSS ∙SSS
:= β ∙ RSS + SSS
While it is preferable to examine the constituent scores individually to determine the strengths and
weaknesses of methods, a unified metric allows us to choose methods that balance these criteria. β
can be tuned to change the influence of the constituent metrics according to the desired application 一
we set β = 1.0 in a problem-agnostic fashion in this paper. This choice is in line with previous work
that defines compound criteria, such as Rosenberg & Hirschberg (2007).
4 Identifying Procedural Abstractions
So far, we have argued the importance of evaluating repeated and segment structure correctly in
temporal data. Our new metrics enable a systematic evaluation of these ideas, whereas existing
metrics did not specifically evaluate or emphasize them. Given this, we now return to our original goal
of identifying procedural abstractions in time-series data, which we term procedure identification.
Concretely, we assume a dataset of time-series such that each demon-
strates the same task or phenomenon. Crucially, we assume that all
time-series share this single, latent procedure - different time-series
are distinct realizations of this underlying procedure. The goal is to
label the temporal structure present in each time-series while identi-
fying the common, latent procedure. For example, this may involve
identifying the common sequence of steps in a set of demonstrations
that follow the same cooking recipe as well as their temporal extent in
each demonstration.
Procedure identification is related to recent efforts in computer vision
that relate to unsupervised activity recognition in videos (Wu et al.,
2015; Yang et al., 2013; Krishnan et al., 2015; Sener & Yao, 2018)
as well as prior work in latent variable modeling (Yang et al., 2013;
Fox et al., 2008b; 2014; Johnson & Willsky, 2013; Linderman et al., Figure 5: PRISM model
2017). These papers typically assume a general setting where activities
or latent tokens may be reused within or across multiple time-series but
there is no onus on methods to discover this structure. Unlike prior work, procedure identification
requires that methods also return a common procedure in addition to the temporal clustering of
each time-series. This additional requirement places a burden on the method to explicitly, and not
implicitly reason about repeated structure.
Formally, finding a common procedure P corresponds to finding temporal clusterings Ci for each
time-series, such that P = P (C1) = P (C2)=…=P (Cn). Since a single procedure consistent
with all temporal clusterings is desired, shared primitives must be identified across all videos. This
requirement also makes evaluation in this setting particularly suited for our metrics. Methods that
6
Published as a conference paper at ICLR 2019
cannot relate primitives across demonstrations (identify repeated structure) or reason about their
temporal extents (identify segment structure) will be unable to recover the underlying procedure and
should therefore be penalized. There may also be additional repeated structure within a time-series,
for example if someone mixes cake batter, adds ingredients, and then mixes the cake batter again.
A closely related setting was tackled by the method outlined in Sener & Yao (2018), who assume that
each time-series is a permutation of a shared bag of activities. However, as they acknowledge, a major
limitation of their method is that they allow each activity to occur atmost once within a time-series,
which limits repeated structure. Our problem setting allows for this possibility, which is often seen in
real-world datasets e.g. the JIGSAWS surgical dataset (Gao et al., 2014) contains videos of surgeons
doing multiple repetitions of primitives such as needle-passing or suturing. However, we do not allow
the procedure to be permuted in our problem setting, which is a useful extension that we hope to
explore in future work.
4.1	Prism: PRocedure Identification with a S egmental Mixture model
We now describe a statistical model called Prism for the procedure identification problem. Prism
is a generative hierarchical Bayesian model that explicitly separates reasoning about the procedure
from its realization in each time-series in the dataset. It utilizes this to create a mixture distribution
over primitives for every observation in the dataset. Fig 5 illustrates the model.
Unlike popular sequence models based on the Hidden Markov model, Prism does not make a
restrictive Markov assumption and can handle any procedure. And in contrast to a Gaussian mixture
model, Prism shares statistical strength across time-series by using a structured prior over the
(discrete) local variables that generate each observation - amortizing the procedure across many time-
series. As we show later in experiments, this structure can help Prism outperform these methods,
especially when the observations are noisy.
Modeling the procedure. PRISM assumes that each time-series Xi follows the same underlying
procedure p = (p1, . . . ,ps), where s is the number of steps in the procedure. p is a vector of discrete,
global latent variables in the model. This implies that each time-series will split into s segments, each
segment corresponding to a single step in the procedure.
Next, let ∣Γc| = K be the number of cluster labels (primitives). Each step in the procedure will
correspond to one of these K labels i.e. pr ∈ [K], 1 ≤ r ≤ s. Thus, p is an ordered sequence of
high-level primitives e.g. to make tea, we would boil water → get cup → pour water and so on.
Formally, we assume a generation process,
λr 〜Dirichlet(αι,..., aκ),	1 ≤ r ≤ S	Pr 〜Categorical(λr),	1 ≤ r ≤ S
Each token in the procedure is independently generated from a separate Dirichlet prior λr , which
allows fine-grained control over the distribution of tokens at each step. Note that we do not disallow
self-transitions, which can be utilized by the model to coalesce adjacent segments.
Modeling realizations in each time-series. Different time-series can be vastly different in how
they implement the procedure e.g. they can be videos of different individuals making tea. These
individuals, while following the same procedure, may spend varying amounts of time on each step
(or even skip steps).
In PRISM, the realization of the procedure in the ith time-series (having mi time-steps) is given by
wi = (wi,1, . . . , wi,mi), a sorted sequence of discrete random variables. Each wi,j ∈ [S] is an index
into the procedure p. Its value dictates the step being followed in the procedure at the jth time-step
in the ith time-series. We assume wi is generated as follows,
Y 〜Dirichlet(βι,..., βs)	Uij 〜Categorical(Y), 1 ≤ i ≤ n, 1 ≤ j ≤ mi
wi = Sort(ui,1, . . .,ui,mi),	1 ≤ i ≤ n
Mathematically, to generate wi , we first generate an unordered bag of mi independent categorical
variables ui,j and then sort them (in ascending order). Each ui,j is generated from the same Dirichlet
prior Y. Intuitively, if we use the stick-breaking metaphor for the Dirichlet distribution, Y is a prior
over how long each token in the procedure is - Yr is the expected relative length of Pr. Thus, Y
allows us to impose a prior on the relative length of each step of the procedure, in every time-series.
An equivalent view into our model is that each wi is directly generated by a Multinomial distribution
7
Published as a conference paper at ICLR 2019
with mi draws on probabilities dictated by γ . However, we find that the view that we adopt yields a
far more efficient inference scheme (see Appendix).
Generating observations. For every time-series, we can combine the common procedure p and the
realization of the procedure wi to recover a temporal clustering zi for that time-series. We write this
temporal clustering as zi = (zi,1, . . . , zi,mi) where zi,j ∈ [K] is a discrete primitive.
For each primitive k ∈ [K], We assume a Gaussian observation model (μk, ∑k) (with a
Normal-Inverse Wishart prior) shared by all time-series. PRISM then generates each xij
independently from its local assignment zi,j and the appropriate observation model. We
use Gibbs sampling (see Appendix) to perform posterior inference of the latent variables
({Pr}r∈[s],{ui,j}i∈[n],j∈[mi],{μk}k∈[κ],{∑k}k∈[κ]) in the model.
5	Experiments with Evaluation Criteria
We first compare our metrics to existing criteria by evaluating several competing methods on real
datasets. We note that the methodology for validating our evaluation criteria is similar to that followed
by Rosenberg & Hirschberg (2007), with the exception that we compare criteria on real-world data
with a large number of methods. We compare to homogeneity (Hom), completeness (Com), NMI since
they give the best combination of performance for clustering evaluation (Rosenberg & Hirschberg,
2007). We also compare to accuracy computed using the Munkres method since it is widely used e.g.
(Sener & Yao, 2018; Zhou et al., 2008) and the adjusted Rand index (ARI) (Vinh et al., 2010).
Methods. We use several methods to generate temporal clusterings: Hierarchical Clustering (AGG);
Hidden Markov Models (HMM); Hierarchical Dirichlet Process HMMs (HDP-HMM) (Teh et al.,
2005); Sticky HDP-HMM (SHDP-HMM) (Fox et al., 2008a); HDP Hidden Semi-Markov Models
(HDP-HSMM) (Johnson & Willsky, 2013); Switching Linear Dynamical Systems (SLDS) (Fox
et al., 2008b); HDP-SLDS (Fox et al., 2008b); Sticky HDP-SLDS (SHDP-SLDS) (Fox et al.,
2008b); Ordered Subspace Clustering (OSC) (Tierney et al., 2014); Temporal SC (TSC) (Li et al.,
2015); Low-Rank SC (LRSC) (Vidal & Favaro, 2014); Sparse SC (SSC) (Elhamifar & Vidal, 2013)
and Spatial SC (SPATSC) (Guo et al., 2013).
Datasets. We use 2 common benchmark datasets (Fox et al., 2008b; 2009; 2014; Zhou et al., 2008;
2013). Bees consists of 6 time-series, each recording the dancing behavior of a bee. Each time-series
is built up of 3 primitive movements - waggle, left-turn and right-turn - repeated several times. For
Bees, we treat each sequence as a separate dataset (Bees1,. . . ,Bees6) and run each method on
them individually, as done in Fox et al. (2008b). Here, we focus on results on Bees1 but results on
Mocap6, a motion-capture dataset and all Bees datasets can be found in the Appendix.
Results. For each of our metrics, we first illustrate that they qualitatively capture the characteristics
that they were designed for. Fig 6 visualizes the best and worst performing methods on Bees1
for each of our metrics. In Fig 6a, SSC is worst on LASS-O since it heavily over-segments the
time-series by introducing many transitions, while TSC introduces far fewer transitions. In Fig 6b,
under-segmentation is penalized and we can see that Agg should be worse since it contains longer
segments and far fewer transitions than SSC. Combining these ideas in Fig 6c yields that HDP-
HSMM has the best LASS - observe that it contains fewer closely spaced transition points and
creates a more even segmentation at the right level of granularity, explaining its score.
Fig 6e shows the result of evaluating with just the RSS - we can clearly see that SpatSC is able to
capture far more repeated structure than SHDP-HMM, which is unable to label segments from the
same ground-truth label consistently. Lastly, we see that SpatSC is judged to be the best method on
BEES1 by TSS in Fig 6f since it has the highest RSS and a SSS of 0.81 which is close to the highest.
Improving scores on evaluation metrics is common practice to demonstrate performance gains due
to a method. We now highlight cases where TSS disagrees with NMI, ARI and/or Munkres in the
assessment of competing methods, since this discrepancy has significant impact on which method
is picked. We find that such disagreements happen very often across all datasets. An illustrative
example for Bees1 is shown in Fig 6h: TSS and Munkres agree that SSC is better than HDP-HSMM,
but NMI and ARI strongly disagree. Before diving into why this may be happening, we note that
examining the RSS and SSS allows us to understand why TSS considers SSC to be better than
HDP-HSMM - it captures more repeated structure. On the other hand Munkres, while it agrees with
8
Published as a conference paper at ICLR 2019
Dataset		Bees							Surgery			
Method		Bees-1 (1)	Bees-2 (1)	Bees-3 (1)	Bees-4 (1)	Bees-5 (1)	Bees-6 (1)	Average	Knot-Tying (10)	Needle-Passing (10)	Suturing (10)	Suturing (5)
GMM		61.21	62.38	44.74	63.86	74.79	66.44	62.24	25.12	21.93	34.75	49.03
		19.97	35.52	12.86	31.20	37.30	36.09	28.82	6.98	2.94	19.96	32.58
HMM (α	0.1)	66.43	70.80	52.35	78.20	87.38	73.62	71.46	22.94	25.44	38.91	49.92
		32.11	37.39	17.14	39.63	65.77	43.27	39.22	5.05	2.99	17.52	27.49
HMM (α	1.0)	65.00	69.16	53.28	73.75	87.45	80.54	71.53	24.48	22.72	38.97	48.72
		29.20	37.82	17.75	36.63	65.93	50.00	39.56	5.50	3.27	18.98	29.93
HMM (α	100.0)	58.69	78.28	53.56	81.51	85.85	86.45	74.06	25.02	22.91	37.73	47.32
		22.14	50.33	18.73	43.59	61.16	61.99	42.99	6.11	3.21	18.38	28.65
Prism		65.90	75.96	54.65	78.27	83.78	82.07	73.44	28.88	28.00	46.68	62.03
		25.04	46.01	18.26	41.57	56.11	53.72	40.12	9.75	15.47	33.26	51.54
Table 1: Comparison with baselines on Bees and the JIGSAWS surgical dataset. For a dataset, values
in parentheses indicate the number of time-series. Upper entries are TSS and lower entries are NMI.
TSS, is unable to give Us this insight. We find that NMI is highly sensitive to the number of clusters -
SSC only uses 2 clusters, which means it is heavily penalized by NMI, in spite of capturing some
repeated structure. ARI is less sensitive than NMI, but is far less interpretable.
Another example is shown in Fig 6g, where the TSS for both SLDS and HDP-HMM is comparable.
Interestingly, NMI/ARI and Munkres are all highly skewed and disagree strongly on the better method.
Since HDP-HMM has more clusters, the clusters are more homogenous and Hom is much higher,
pushing up NMI. On the other hand, since Munkres finds a one-to-one correspondence between
clusters and ground-truth labels, having more clusters means some cannot be matched. This causes
the score to strongly downweigh suchh clusterings. Overall, our metrics provide both interpretability
and the ability to analyze where the method is falling short.
Lastly, we conduct a sensitivity analysis for β as follows: assume that the most appropriate value for
evaluation is β0, but we instead use the default value of β = 1.0. Our main idea is to compute how
well the ranking of methods under β = 1.0 captures that under β0. To do this, we use the Normalized
Discounted Cumulative Gain (NDCG), a common criteria from information retrieval. Varying β0, we
see in Fig 6i that β = 1.0 outputs similar rankings to a relatively wide range of other β values, other
than at the extremes. This confirms that most settings of β will be captured well by our choice of β,
unless there is a strong reason to prefer either the RSS or the SSS only.
6	Experiments with Prism
We now evaluate Prism on complex datasets with noisy features, including video data. For Prism,
we set α = 1.0, β = 0.1 (except BEES) and we set K (number of mixture components) to equal
the number of ground-truth labels for all methods. Results are averaged over 10 random seeds.
Experimental details including hyperparameters can be found in the Appendix.
Before our main results, we briefly conduct a simple check on performance for the Bees dataset
introduced earlier. Though this only has a single example per procedure, we find that our approach
still does well (Table 1) compared to two simple baselines - all approaches use the same observation
model with Gaussian emissions.
Leveraging a fixed procedure. We now evaluate the benefit of our approach on the JIGSAWS
dataset (Gao et al., 2014), which consists of surgeons with varying skill demonstrating common
procedures, such as suturing, needle-passing and knot-tying. We use kinematic features that corre-
spond to the grippers controlled by the surgeon for all methods. For each surgical task, we select
the 10 demonstrations that correspond to the 2 expert surgeons since they adhere most closely to
the prescribed procedures. Even for a single expert, there is considerable variability across the 5
demonstrations they generate, so this is far from an idealized setting. Once again, all methods leverage
the same observational model with the same features. Our baselines are two standard approaches 一 a
Gaussian mixture model (GMM), and variants of a Bayesian Hidden Markov model.
As expected, Prism outperforms these baselines by a considerable margin. On the suturing task,
performance is improve by almost double-digits on both metrics, with performance gains also visible
on the other tasks. Leveraging the common procedure allows Prism to share statistical strength
9
Published as a conference paper at ICLR 2019
Sener (no background model) Prism (no background model) s ener (background model)			
changing tire	25.0	31.2	33.9
coffee	22.1	26.9	29.0
cpr	18.1	28.1	24.9
jump car	5.8	22.4	15.0
repot plant	19.1	24.0	23.9
Table 2: Comparison with Sener & Yao (2018) on the INRIA dataset with the Munkres score.
across demonstrations and make consistent predictions where possible, whereas the other methods
must rely more strongly on the observation model to infer the latent sequence. Without very cleanly
separated features for different primitives, they can often be led astray.
We also experiment with a smaller subset of 5 demonstrations provided by a single expert on the
suturing task. This subset of data has the least amount of variability in terms of the expert’s procedure.
Because of this, We see that Prism gives an improvement of 〜18 points on NMI and 〜10 points
on the TSS. To illustrate PRISM’s sensitivity to hyperparameters, we vary s and K in Fig 7 on this
single-expert suturing task. We find that Prism's performance is quite stable with respect to both 一
this holds as long as s is larger than the number of segments in ground-truth, and K is close to the
true number of ground-truth clusters.
Noisy procedures with video data. Next, we consider labeling demonstrations that are very noisy in
their adherence to the procedure, with a great deal of variability in both the steps being followed, their
order, as well as their relative lengths. Recent work by sener & Yao (2018) achieved state-of-the-art
performance without any form of supervision on two large video datasets - Breakfast actions (Kuehne
et al., 2014) and iNRiA instructional videos (Alayrac et al., 2016). We expect our method to perform
worse on the Breakfast actions dataset, as it has no repeated structure, in which case our model
has no advantages over this prior model, and indeed uses a simpler observation model. Perhaps
encouragingly, in the Breakfast actions dataset, we find that our method only performs slightly worse
than but close to sener & Yao (2018), achieving a Munkres score of 33.5 compared to their score of
34.6. it is likely that this can be improved by tuning the number of segments for each procedure (we
set s = 20 for all procedures).
However a key limitation of their work is that they do not model repeated structure. Therefore in
domains with repeated structure we expect our approach to do well. such structure is present in
some of the activities in the iNRiA dataset. We compare to sener & Yao (2018)’s results on the
iNRiA dataset in Table 2. Prism outperforms their best method on 3 out of the 5 activities despite
not explicitly modeling background information, with a significant difference on cpr and jump car.
On their comparable method which does not model background, Prism is significantly better across
all procedures. This is despite Prism using a much weaker observation model (they simultaneously
learn a feature representation for the videos) and no permutation model. An example for the cpr
procedure is shown in Fig 1 and illustrates that unlike sener & Yao (2018)’s method, Prism can
successfully recover repeated structure both within and across videos. We expect that incorporating a
background model in Prism could enable our approach to yield even stronger performance.
7	Conclusion
We developed Prism, a hierarchical Bayesian model to identify latent procedures in time-series
data. Results on several datasets show that Prism can be used to learn procedural abstractions more
effectively than competing methods. We also introduced new evaluation criteria for the problem of
externally evaluating temporal clusterings. Our metrics address gaps in temporal clustering evaluation,
and provide both interpretability and the potential for easier analysis. in future work, we hope to
extend Prism by incorporating richer observation models, and relaxing the assumption that a single
strictly-ordered procedure is followed by all time-series. Links to code can be found in the Appendix.
8	Acknowledgments
The authors would like to thank the schmidt Foundation and a NsF Career Grant for their support.
10
Published as a conference paper at ICLR 2019
Ground Tinth
Ground TrUth
Prediction by SSC
IHBIIHH IIIH IIII mill III Illi Illll Illll II I lllllll III llllllll IniHllllIIIIIIH
Prediction by TSC
Prediction by Agg
Prediction by SSC
IH Bllll ∏ O 111 HI III NIH !HI U ill III ■! Ill ■! HI I III 11 H
(a)	On LASS-O: SSC (worst) = 0.62, TSC
(best) = 0.84
(b)	On LASS-U: Agg (worst) = 0.85, SSC
(best) = 0.98
Ground Truth
Ground Truth
Prediction by SSC
IH mil ∏ BIH□ O 111 IH III El □lll H ill III ■! ΠJ ■! il D El H
Prediction by HMM
Il ■ Il M Il I ■ Illll ■ ■■■■”■
Prediction by SSC
■ ■■■■■■■■■ III lllllll III Ml IHI HIIII H El H
Prediction by HDP-HSMM
(c)	On LASS: SSC (worst) = 0.76, HDP-
HSMM (best) = 0.86
(d)	On SSS: SSC (worst) = 0.73, HMM (best)
= 0.82
Ground TrUth
Ground TrUth
Prediction by SHDP-HMM
Prediction by SpatSC
(e) On RSS: SHDP-HMM (worst) = 0.39,
(f) On TSS: SHDP-HMM (worst) = 0.52,
(i) Sensitivity to β .
Figure 6: (a)-(f) visualize the best and worst methods on our metrics; (g)-(h) show examples where
we disagree with traditional criteria; (i) illustrates a sensitivity analysis with respect to the TSS
tradeoff parameter β. All results are on the BEES1 dataset.
(a) Varying s.
(b) Varying K .
Figure 7: Effect of varying hyperparameters on model performance. Results are on the JIGSAWS
suturing task. Green marks the value of the hyperparameter in ground-truth.
References
Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Ivan Laptev, Josef Sivic, and Simon
Lacoste-Julien. Unsupervised learning from narrated instruction videos. In Computer Vision and
11
Published as a conference paper at ICLR 2019
Pattern Recognition (CVPR), 2016.
Diana Borsa, Bilal Piot, Remi Munos, and Olivier Pietquin. Observational learning by reinforcement
learning. arXiv preprint arXiv:1706.06617, 2017.
Fu-Lai Chung, Tak-Chung Fu, Vincent Ng, and Robert WP Luk. An evolutionary approach to
pattern-based time series segmentation. IEEE transactions on evolutionary computation, 8(5):
471-489, 2004.
Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm, theory, and applications.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:2765-2781, 2013.
Bernard Fox, Michael C. Hughes, Erik B. Sudderth, and Michael I. Jordan. Joint modeling of multiple
time series via the beta process with application to motion capture segmentation by emily. 2014.
Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, and Alan S. Willsky. An hdp-hmm for systems
with state persistence. In ICML, 2008a.
Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, and Alan S. Willsky. Nonparametric bayesian
learning of switching linear dynamical systems. In NIPS, 2008b.
Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, and Alan S. Willsky. Sharing features among
dynamical systems with beta processes. In NIPS, 2009.
Yixin Gao, S Swaroop Vedula, Carol E Reiley, Narges Ahmidi, Balakrishnan Varadarajan, Henry C
Lin, Lingling Tao, Luca Zappella, Benjamin Bejar, David D Yuh, et al. Jhu-isi gesture and skill
assessment working set (jigsaws): A surgical activity dataset for human motion modeling. In
MICCAI Workshop: M2CAI, volume 3, pp. 3, 2014.
Yi Guo, Junbin Gao, and Feng Li. Spatial subspace clustering for hyperspectral data segmentation.
In The Third International Conference on Digital Information Processing and Communications,
pp. 180-190. The Society of Digital Information and Wireless Communication, 2013.
Minh Hoai and Fernando De la Torre. Maximum margin temporal clustering. In Artificial Intelligence
and Statistics, pp. 520-528, 2012.
Guy Jacobson and Kiem-Phong Vo. Heaviest increasing/common subsequence problems. In Annual
Symposium on Combinatorial Pattern Matching, pp. 52-66. Springer, 1992.
Matthew J Johnson and Alan S Willsky. Bayesian nonparametric hidden semi-markov models.
Journal of Machine Learning Research, 14(Feb):673-701, 2013.
Rebecca Killick, Paul Fearnhead, and Idris A Eckley. Optimal detection of changepoints with a linear
computational cost. Journal of the American Statistical Association, 107(500):1590-1598, 2012.
Sanjay Krishnan, Animesh Garg, Sachin Patil, Colin Lea, Gregory D. Hager, Pieter Abbeel, and
Kenneth Y. Goldberg. Transition state clustering: Unsupervised surgical trajectory segmentation
for robot learning. I. J. Robotics Res., 36:1595-1618, 2015.
Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and
semantics of goal-directed human activities. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 780-787, 2014.
Sheng Li, Kang Li, and Yun Fu. Temporal subspace clustering for human motion segmentation. In
Proceedings of the IEEE International Conference on Computer Vision, pp. 4453-4461, 2015.
Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski.
Bayesian learning and inference in recurrent switching linear dynamical systems. In Artificial
Intelligence and Statistics, pp. 914-922, 2017.
Marina Meila. Comparing clusteringsan information based distance. Journal Ofmultivariate analysis,
98(5):873-895, 2007.
James Munkres. Algorithms for the assignment and transportation problems. Journal of the society
for industrial and applied mathematics, 5(1):32-38, 1957.
12
Published as a conference paper at ICLR 2019
Andrew Rosenberg and Julia Hirschberg. V-measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 joint conference on empirical methods in natural
language processing and computational natural language learning (EMNLP-CoNLL), 2007.
Fadime Sener and Angela Yao. Unsupervised learning and segmentation of complex activities from
video. arXiv preprint arXiv:1803.09490, 2018.
Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. arXiv preprint
arXiv:1703.01703, 2017.
Alexander Strehl and Joydeep Ghosh. Cluster ensembles—a knowledge reuse framework for combin-
ing multiple partitions. Journal ofmaChine learning research, 3(Dec):583-617, 2002.
Yee W Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Sharing clusters among related
groups: Hierarchical dirichlet processes. In Advances in neural information processing systems,
pp. 1385-1392, 2005.
Stephen Tierney, Junbin Gao, and Yi Guo. Subspace clustering for sequential data. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 1019-1026, 2014.
Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv preprint
arXiv:1805.01954, 2018.
Rene Vidal and Paolo Favaro. LoW rank subspace clustering (lrsc). Pattern Recognition Letters, 43:
47-61, 2014.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings
comparison: Variants, properties, normalization and correction for chance. Journal of Machine
Learning Research, 11(Oct):2837-2854, 2010.
Chenxia Wu, Jiemi Zhang, Silvio Savarese, and Ashutosh Saxena. Watch-n-patch: Unsupervised
understanding of actions and relations. In Computer Vision and Pattern Recognition (CVPR), 2015
IEEE Conference on, pp. 4362-4370. IEEE, 2015.
Yang Yang, Imran Saleemi, and Mubarak Shah. Discovering motion primitives for unsupervised
grouping and one-shot learning of human actions, gestures, and expressions. IEEE transactions on
pattern analysis and machine intelligence, 35(7):1635-1648, 2013.
Feng Zhou, Fernando De la Torre, and Jessica K. Hodgins. Aligned cluster analysis for temporal
segmentation of human motion. 2008 8th IEEE International Conference on Automatic Face
Gesture Recognition, pp. 1-7, 2008.
Feng Zhou, Fernando De la Torre, and Jessica K. Hodgins. Hierarchical aligned cluster analysis
for temporal clustering of human motion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 35:582-596, 2013.
13
Published as a conference paper at ICLR 2019
A Algorithm for Repeated S tructure Score
Detailed pseudocode for reproducing the computation of the RSS is given in Algorithm 1.
Algorithm 1 Calculation of RSS
Require: Ground truth G, temporal clustering C, similarity function H
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
\\ Optional Pruning Step
for all α ∈ Γc do βα - argmaxβ∈Γg ∣G[β] ∩ C[α]∣
.find βα which most overlaps ɑ
for all (a, b) ∈ S(G, β), β ∈ ΓG do	. for every segment of every β in G
P(Ca:b),W(Ca:b) := (p1,p2, . . . ,pr), (w1,w2, . . . ,wr)	. locate segment in C
Wi J Wi X I[β = βpi]	. trim weights based on purity
W(Ca：b) J (w1,w2,∙∙∙,wr)	. ConStrUct new weights
\\ Repeated Structure Scoring
score J Pβ∈ΓG P(a1,b1),(a2,b2)∈S(G,β) H(P(Ca1:b1 ), P (Ca2:b2 ), W (Ca1:b1 ), W (Ca2:b2 ))
. score every repeated structure pair for every ground-truth label β
max.score J 2£e三2 |S(G,β)∣∑
return RSS J
SCore
max_score
(a,b)∈S(G,β)
(b-
a+ 1)
B Experiments with Evaluation Criteria
The results presented for the evaluation criteria developed in this paper can be reproduced using
code available at https://github.com/StanfordAI4HI/ICLR2019_evaluating_
discrete_temporal_structure. The temporal clusterings predicted by each method, as
well as ground-truth data is available at the link.
In addition, an implementation of only the newly proposed temporal clustering evaluation criteria can
be found at https://github.com/StanfordAI4HI/tclust-eval.
For completeness, we include the result of evaluation across all methods, datasets and criteria.
method metric	Agg	HDP-HMM	HDP-HSMM	HDP-SLDS	HMM	LRSC	OSC	SHDP-HMM	SHDP-SLDS	SLDS	SSC	SpatSC	TSC
ARI	0.28	0.27	0.22	0.14	0.36	0.10	0.39	0.23	0.14	0.17	0.06	0.39	0.30
Com	0.25	0.24	0.25	0.13	0.30	0.13	0.44	0.25	0.13	0.16	0.05	0.44	0.34
Hom	0.24	0.46	0.51	0.13	0.30	0.13	0.28	0.50	0.13	0.16	0.03	0.28	0.22
LASS	0.84	0.78	0.86	0.82	0.86	0.83	0.84	0.84	0.82	0.84	0.76	0.84	0.86
LASS-O	0.82	0.65	0.80	0.74	0.81	0.73	0.78	0.77	0.80	0.81	0.62	0.78	0.84
LASS-U	0.85	0.97	0.92	0.91	0.92	0.95	0.92	0.94	0.86	0.88	0.98	0.92	0.87
Munk	0.61	0.39	0.41	0.58	0.68	0.50	0.64	0.39	0.58	0.59	0.47	0.64	0.60
NMI	0.24	0.33	0.36	0.13	0.30	0.13	0.35	0.35	0.13	0.16	0.04	0.35	0.27
RSS	0.55	0.46	0.40	0.53	0.57	0.45	0.63	0.39	0.49	0.45	0.49	0.63	0.62
SSS	0.78	0.75	0.81	0.76	0.82	0.79	0.81	0.81	0.76	0.78	0.73	0.81	0.81
TSS	0.65	0.57	0.54	0.62	0.67	0.57	0.71	0.52	0.59	0.58	0.59	0.71	0.70
Table 3: Results on Bees1 across all evaluation criteria and methods.
method metric	Agg	HDP-HMM	HDP-HSMM	HDP-SLDS	HMM	LRSC	OSC	SHDP-HMM	SHDP-SLDS	SLDS	SSC	SpatSC	TSC
ARI	0.26	0.32	0.28	0.14	0.61	-0.19	0.44	0.25	0.12	0.10	0.05	0.44	0.30
Com	0.27	0.33	0.28	0.16	0.54	0.19	0.51	0.29	0.13	0.11	0.04	0.51	0.32
Hom	0.28	0.66	0.53	0.16	0.54	0.20	0.33	0.58	0.20	0.10	0.02	0.33	0.41
LASS	0.79	0.77	0.87	0.83	0.89	0.79	0.84	0.84	0.83	0.82	0.76	0.84	0.87
LASS-O	0.69	0.64	0.82	0.80	0.91	0.67	0.78	0.77	0.77	0.82	0.62	0.78	0.84
LASS-U	0.94	0.96	0.92	0.87	0.88	0.96	0.91	0.93	0.90	0.82	0.98	0.91	0.90
Munk	0.64	0.38	0.44	0.56	0.85	0.60	0.68	0.40	0.37	0.49	0.49	0.68	0.54
NMI	0.27	0.47	0.38	0.16	0.54	0.20	0.41	0.41	0.16	0.10	0.03	0.41	0.37
RSS	0.51	0.60	0.42	0.44	0.77	0.40	0.73	0.57	0.40	0.41	0.49	0.73	0.50
SSS	0.75	0.75	0.82	0.76	0.86	0.76	0.80	0.79	0.76	0.74	0.74	0.80	0.82
TSS	0.61	0.67	0.56	0.56	0.81	0.52	0.76	0.66	0.53	0.53	0.59	0.76	0.62
Table 4: Results on Bees2 across all evaluation criteria and methods.
14
Published as a conference paper at ICLR 2019
method metric	Agg	HDP-HMM	HDP-HSMM	HDP-SLDS	HMM	LRSC	OSC	SHDP-HMM	SLDS	SSC	SpatSC	TSC
ARI	0.00	0.12	0.14	0.17	0.16	0.33	0.08	0.15	0.33	0.05	0.16	0.04
Com	0.02	0.23	0.23	0.14	0.18	0.32	0.14	0.24	0.29	0.13	0.20	0.08
Hom	0.02	0.47	0.49	0.19	0.19	0.21	0.15	0.53	0.30	0.08	0.34	0.10
LASS	0.80	0.79	0.82	0.81	0.80	0.80	0.79	0.80	0.77	0.78	0.80	0.78
LASS-O	0.74	0.69	0.75	0.75	0.82	0.77	0.68	0.71	0.76	0.66	0.71	0.76
LASS-U	0.86	0.94	0.90	0.88	0.78	0.83	0.94	0.91	0.78	0.94	0.93	0.80
Munk	0.41	0.33	0.33	0.48	0.46	0.68	0.48	0.32	0.64	0.47	0.38	0.36
NMI	0.02	0.33	0.34	0.16	0.18	0.26	0.15	0.36	0.29	0.10	0.26	0.09
RSS	0.28	0.42	0.30	0.56	0.45	0.65	0.41	0.37	0.61	0.44	0.40	0.28
SSS	0.73	0.74	0.77	0.74	0.74	0.75	0.73	0.75	0.71	0.74	0.75	0.72
TSS	0.41	0.54	0.43	0.64	0.56	0.70	0.53	0.50	0.66	0.55	0.52	0.40
Table 5: Results on Bees3 across all evaluation criteria and methods.
method metric	Agg	HDP-HMM	HDP-HSMM	HDP-SLDS	HMM	LRSC	OSC	SHDP-HMM	SHDP-SLDS	SLDS	SSC	SpatSC	TSC
ARI	0.28	0.30	0.52	0.25	0.54	-0.12	0.18	0.31	0.29	0.28	0.02	0.18	0.17
Com	0.30	0.28	0.42	0.16	0.48	0.13	0.17	0.26	0.27	0.27	0.02	0.17	0.16
Hom	0.31	0.53	0.62	0.16	0.45	0.14	0.11	0.50	0.18	0.17	0.01	0.11	0.17
LASS	0.87	0.84	0.87	0.81	0.87	0.81	0.81	0.83	0.84	0.84	0.77	0.81	0.78
LASS-O	0.86	0.79	0.84	0.71	0.85	0.73	0.78	0.76	0.77	0.77	0.67	0.78	0.82
LASS-U	0.88	0.91	0.91	0.94	0.90	0.91	0.85	0.91	0.92	0.92	0.92	0.85	0.75
Munk	0.52	0.40	0.63	0.56	0.81	0.52	0.57	0.40	0.62	0.61	0.43	0.57	0.50
NMI	0.30	0.38	0.51	0.16	0.46	0.13	0.14	0.36	0.22	0.22	0.02	0.14	0.17
RSS	0.51	0.43	0.72	0.69	0.85	0.40	0.61	0.50	0.71	0.70	0.49	0.61	0.51
SSS	0.82	0.79	0.83	0.74	0.81	0.77	0.75	0.77	0.79	0.78	0.73	0.75	0.72
TSS	0.62	0.56	0.77	0.71	0.83	0.52	0.68	0.60	0.75	0.74	0.58	0.68	0.60
Table 6: Results on Bees4 across all evaluation criteria and methods.
method metric	Agg	HDP-HMM	HDP-HSMM	HDP-SLDS	HMM	LRSC	OSC	SHDP-HMM	SHDP-SLDS	SLDS	SSC	SpatSC	TSC
ARI	0.14	0.37	0.49	0.24	0.68	-0.31	0.31	0.24	0.24	0.15	0.10	0.31	0.15
Com	0.21	0.38	0.50	0.30	0.66	0.41	0.31	0.30	0.24	0.14	0.15	0.31	0.17
Hom	0.19	0.64	0.67	0.19	0.66	0.26	0.30	0.57	0.18	0.14	0.09	0.30	0.11
LASS	0.82	0.84	0.88	0.85	0.90	0.85	0.84	0.82	0.86	0.83	0.83	0.84	0.84
LASS-O	0.79	0.76	0.86	0.79	0.90	0.88	0.80	0.73	0.81	0.74	0.75	0.80	0.83
LASS-U	0.86	0.93	0.91	0.93	0.91	0.83	0.89	0.93	0.92	0.93	0.92	0.89	0.84
Munk	0.57	0.51	0.67	0.56	0.88	0.61	0.68	0.33	0.54	0.53	0.48	0.68	0.50
NMI	0.20	0.49	0.58	0.24	0.66	0.32	0.30	0.41	0.21	0.14	0.11	0.30	0.14
RSS	0.58	0.78	0.77	0.61	0.90	0.47	0.73	0.63	0.59	0.56	0.57	0.73	0.58
SSS	0.74	0.77	0.83	0.81	0.86	0.80	0.77	0.75	0.81	0.76	0.78	0.77	0.75
TSS	0.65	0.78	0.80	0.69	0.88	0.59	0.75	0.68	0.68	0.64	0.65	0.75	0.66
Table 7: Results on Bees5 across all evaluation criteria and methods.
method metric	Agg	HDP-HMM	HDP-HSMM	HDP-SLDS	HMM	LRSC	OSC	SHDP-HMM	SHDP-SLDS	SLDS	SSC	SpatSC	TSC
ARI	0.21	0.50	0.50	0.26	0.68	-0.13	0.38	0.25	0.24	0.27	0.07	0.38	0.25
Com	0.22	0.42	0.43	0.18	0.62	0.15	0.43	0.32	0.26	0.23	0.07	0.43	0.32
Hom	0.20	0.70	0.70	0.20	0.62	0.27	0.28	0.65	0.17	0.20	0.04	0.28	0.51
LASS	0.82	0.85	0.87	0.79	0.89	0.74	0.85	0.82	0.81	0.81	0.77	0.85	0.80
LASS-O	0.81	0.80	0.84	0.70	0.88	0.60	0.85	0.74	0.75	0.73	0.67	0.85	0.74
LASS-U	0.82	0.90	0.90	0.91	0.90	0.95	0.86	0.92	0.89	0.91	0.90	0.86	0.87
Munk	0.52	0.64	0.63	0.54	0.89	0.33	0.63	0.36	0.57	0.64	0.48	0.63	0.44
NMI	0.21	0.54	0.55	0.19	0.62	0.20	0.35	0.46	0.21	0.21	0.05	0.35	0.40
RSS	0.49	0.68	0.66	0.66	0.89	0.39	0.73	0.53	0.63	0.69	0.49	0.73	0.57
SSS	0.75	0.80	0.83	0.72	0.84	0.69	0.79	0.76	0.76	0.76	0.71	0.79	0.73
TSS	0.59	0.73	0.73	0.69	0.87	0.50	0.76	0.62	0.69	0.72	0.58	0.76	0.64
Table 8: Results on Bees6 across all evaluation criteria and methods.
C Details for Prism
This section contains details related to both the modeling and inference performed by Prism.
15
Published as a conference paper at ICLR 2019
method metric	Agg	HDP-HMM	HDP-HSMM	HDP-SLDS	HMM	LRSC	OSC	SHDP-HMM	SHDP-SLDS	SLDS	SSC	SpatSC	TSC
ARI	0.26	0.47	0.35	0.29	0.46	-0.24	0.17	0.49	0.33	0.48	0.02	0.17	0.16
Com	0.45	0.62	0.54	0.56	0.59	0.51	0.41	0.63	0.56	0.60	0.35	0.41	0.45
Hom	0.48	0.64	0.53	0.32	0.61	0.28	0.28	0.64	0.41	0.56	0.13	0.28	0.21
LASS	0.77	0.87	0.91	0.85	0.85	0.80	0.79	0.87	0.84	0.84	0.81	0.79	0.81
LASS-O	0.64	0.78	0.90	0.75	0.76	0.69	0.66	0.79	0.74	0.74	0.75	0.66	0.77
LASS-U	0.98	0.97	0.92	0.97	0.97	0.95	0.98	0.96	0.97	0.98	0.89	0.98	0.86
Munk	0.40	0.62	0.47	0.46	0.59	0.47	0.36	0.59	0.47	0.61	0.32	0.36	0.30
NMI	0.47	0.63	0.53	0.42	0.60	0.38	0.34	0.63	0.48	0.58	0.21	0.34	0.31
RSS	0.38	0.57	0.47	0.55	0.56	0.42	0.39	0.63	0.50	0.62	0.27	0.39	0.35
SSS	0.77	0.86	0.89	0.84	0.84	0.80	0.79	0.86	0.83	0.83	0.80	0.79	0.79
TSS	0.51	0.69	0.61	0.67	0.68	0.55	0.52	0.73	0.62	0.71	0.41	0.52	0.49
Table 9: Results on Mocap6 across all evaluation criteria and methods.
C.1 Generation in Prism
The generative process used in the hierarchical Bayesian model is given below,
λr 〜Dir(αι, ..., αk),	1 ≤ r ≤ S
Y 〜Dir(βι,...,βs)
Pr 〜Cat(λr),	1 ≤ r ≤ S
Uij 〜Cat(Y),	1 ≤ i ≤ n, 1 ≤ j ≤ m%
wi = Sort(ui,1, . . . , ui,mi ),	1 ≤ i ≤ n
zi = TemporalClustering(p, wi),	1 ≤ i ≤ n
(μk, Σk)〜NIW(μo, ∑o, κ°, ν°), 1 ≤ k ≤ K
Xij 〜Normal(μzi §, ∑zi §), 1 ≤ i ≤ n, 1 ≤ j ≤ mi
C.2 Inference in Prism
We use Gibbs sampling to perform posterior inference of the latent variables
({Pr}r∈[s], {Ui,j}i∈[n],j∈[mi],{μk}k∈[K],{∑k}k∈[K])皿由& model.
Sampling Uij. Let Φ = ({μk}k∈[κ], {∑k}k∈[κ]), and write the complete conditional for Uij,
mi
Pr(Uij= r∣u-i, u-j,...) H Pr(Uij= r∣β, u-i, u-j) ɪɪ Pr(Xij Iuij= r, u-j, p, Φ)
j=1
where in the first term we have collapsed the Dirichlet prior Y . The main difficulty is in the data-
likelihood terms Pr(Xi,jIUi,j = r, . . . ), where we must reason about the effect of changing Ui,j. To
reason about this effect, we must look at wi* * * * * * * * * * * * * * * * * (r) = Sort(Ui,j = r, ui-j), combine it with p to yield zi(r)
(r)
and then compute the observation-likelihood terms. Due to the sorting step that yields wi , changing
a single Ui,j affects the entire sequence zi(r). Naively, we could compute Pr(Xi,j IUi,j = r, . . . ) for
every j ∈ [mi], r ∈ [s] but this would be extremely-ineficient (compute S ∙ m^ terms).
Instead, we note that the assignments zi(r) will be almost identical for every r ∈ [S] since ui-j is
held fixed and only Ui,j is varied. The only points at which the zi(r) are not identical are those right
before and right after segment boundaries - O(S) such locations. We only need to compute the
data-likelihood at these locations since no other locations are changed by sampling Ui,j . Using this
insight, we only need to compute S2 terms, a big reduction since we expect S << mi. We sample all
the Ui,j ’s sequentially using this update.
Sampling pr. To sample pr , we write the complete conditional,
n mi
Pr(pr = kIp-r, α, . . . ) H Pr(pr = kIα)ππPr(Xi,jIpr = k, p-r,wi, Φ)
i=1 j=1
Let zi(k) be the temporal clustering generated by combining (pr = k, p-r) with wi. Note that in
each time-series, setting pr = k affects only those observations that are assigned to pr by wi . We
16
Published as a conference paper at ICLR 2019
thus need to compute the observation-likelihood of only these terms,
n mi
Pr(Pr = k∣p-r, α,...) H Pr(Pr = k∣α)∩ ɪɪ Pr(Xi,j∣μk, ∑k)
i=1 j=1
zi,j =k
We sample the Pr ’s sequentially using this update.
Sampling μk, ∑k. Sampling for the observation model can be done directly from the posterior
of the Normal-Wishart Inverse distribution. We set uninformed priors for the observation model,
μ0 = 0d, ς0 = Id×d, κ0 = 1,ν0 = d + 2.
D Experiments with Prism
The results presented for Prism can be reproduced using code available at https://github.
com/StanfordAI4HI/ICLR2019_prism, which contains a Python implementation of Prism.
D. 1 Datasets
We now provide details for preprocessing each dataset, with data loaders available at the linked code.
JIGSAWS. For the JIGSAWS dataset, we use the first 38 kinematic features. We subsample every
3rd timestep. To preprocess the data, we perform Principal Components Analysis (PCA) for dimen-
sionality reduction to 12 features. We then stack features from 10 time-steps and perform PCA again
to reduce dimensionality to 10 features. Lastly, we standardize features to have zero mean and unit
variance.
Breakfast actions. We use the Fisher vector features taken from the ‘stereo01’ camera without any
other preprocessing. We use the coarse segmentation for evaluation.
INRIA instructional videos. We use PCA to reduce the dimensionality of the features to 64.
Bees. We use 4 features 一 x, y, cos θ and Sin θ.
D.2 Experiment details
We use the hyperparameter settings given in Table 10 for Prism. The Bayesian HMM has a single
hyperparameter α which represents the hyperparameter for the Dirichlet prior over the transition
matrix.
Hyperparameter	Surgical	Breakfast	INRIA	Bees
α	10	10	1.0	1.0
β	0.1	0.1	0.1	1.0
s	25	20	10	200
k	ground-truth	ground-truth	ground-truth	ground-truth
Table 10: Hyperparameter settings used for Prism experiments.
D.3 Results on Breakfast Actions
Detailed results for the Breakfast actions dataset are summarized in Table 11.
Procedure	Cereals	Coffee	Fried Egg	Juice	Milk	Pancakes	Salad	Sandwiches	ScrambledEggs	Tea
NMI	0.10-	0.14	-0.17	0.31	0.16	-0:35-	0.11	0.21	0.25	0.10
Munkres	0.36	0.30	0.29	0.43	0.40	0.34	0.29	0.35	0.29	0.29
TSS	0.43	0.45	0.51	0.57	0.47	0.62	0.38	0.49	0.47	0.40
Table 11: Procedure-wise breakdown of results on the Breakfast actions dataset.
17
Published as a conference paper at ICLR 2019
Method	NMI	TSS
GMM	75.21	73.37
HMM (α = 0.1)	47.68	72.50
HMM (α = 1.0)	56.45	72.39
HMM (α = 100.0)	59.32	51.31
Prism (β = 0.1)	79.04	82.77
Table 12: Comparison of methods in their ability to extract non-Markov procedural structure on
2-dimensional simulated data.
D.4 Non-Markov S imulation Study
We generate 10 time-series using the following simple non-Markov sequence,
AABBAACCAADDAAEEAAFFAAGGAAHHAAAAAAAA
There are 8 distinct latent tokens in the sequence, and we generate noisy Gaussian 2-dimensional
observations for each such token, using a mean of (i, i) for the ith token, and a standard-deviation of
0.35 in either dimension.
We compare the Hidden Markov model variants to Prism and the Gaussian mixture model. All
methods use the same observational model with Gaussian emissions.
We expect that the main advantage of using an HMM vis-a-vis a GMM for recovering the latent
temporal clustering, is in cases where the observations are noisy. In such cases, the learned transition
dynamics can override the (noisy) observational likelihood to yield the correct sequence of tokens, as
long as the procedure is Markov. However, this benefit should disappear in cases where the procedure
is not Markov, like the sequence constructed by us above.
In contrast to these methods, Prism makes no Markov assumption about the underlying procedure.
This allows it to learn any procedure, Markov or otherwise. In addition, by sharing this procedure
across multiple time-series, Prism can avoid being led astray by noisy observations (unlike the
GMM, which relies purely on the observation to infer the local latent token).
This argument is confirmed by the results in Table 12. Prism outperforms all methods on both
evaluation metrics, while the HMM variants suffer due to their inability to abandon the Markov
assumption that is baked into the model.
18