Published as a conference paper at ICLR 2019
Sparse Dictionary Learning by
Dynamical Neural Networks
Tsung-Han Lin, Ping Tak Peter Tang
Intel Corporation
Santa Clara, CA
{tsung-han.lin,peter.tang}@intel.com
Ab stract
A dynamical neural network consists of a set of interconnected neurons that inter-
act over time continuously. It can exhibit computational properties in the sense
that the dynamical system’s evolution and/or limit points in the associated state
space can correspond to numerical solutions to certain mathematical optimization
or learning problems. Such a computational system is particularly attractive in
that it can be mapped to a massively parallel computer architecture for power
and throughput efficiency, especially if each neuron can rely solely on local in-
formation (i.e., local memory). Deriving gradients from the dynamical network’s
various states while conforming to this last constraint, however, is challenging.
We show that by combining ideas of top-down feedback and contrastive learning,
a dynamical network for solving the `1 -minimizing dictionary learning problem
can be constructed, and the true gradients for learning are provably computable by
individual neurons. Using spiking neurons to construct our dynamical network,
we present a learning process, its rigorous mathematical analysis, and numerical
results on several dictionary learning problems.
1 Introduction
A network of simple neural units can form a physical system that exhibits computational properties.
Notable examples include Hopfield network (Hopfield, 1982) and Boltzmann machine (Ackley et al.,
1985). Such systems have global states that evolve over time through only local interactions among
neural units. Typically, one is interested in a system whose motion converges towards locally stable
limit points, with the limit points representing the computational objective of interest. For example, a
Hopfield network’s limit points correspond to stored memory information and that of a Boltzmann
machine, a data representation. These computational systems are interesting for both engineering
and neuroscience research. From a hardware implementation standpoint, such computational models
allow the mapping of neurons to a massively parallel architecture (Davies et al., 2018; Merolla
et al., 2014). By allocating private local memory to each processing element, the so-called von
Neumann memory bottleneck in modern computers can be eliminated, delivering much greater power
and throughput efficiency (e.g., see Kung (1982)). For neuroscience, such computational models
obey the fundamental physical locality constraints of biological neurons, providing a direction for
understanding the brain.
We are interested in using such systems to solve the '「minimizing sparse coding and dictionary
learning problem, which has fundamental importance in many areas, e.g., see Mairal et al. (2014). It
is well-known that even just the sparse coding problem, with a prescribed dictionary, is non-trivial
to solve, mainly due to the non-smooth objective involving an `1 -norm (Efron et al., 2004; Beck
& Teboulle, 2009). Remarkably, a dynamical network known as the LCA network (Rozell et al.,
2008) can be carefully constructed so that its limit points are identical to the solution of the sparse
coding problem. Use of a dynamical network thus provides an alternative and potentially more power
efficient method for sparse coding to standard numerical optimization techniques. Nevertheless, while
extending numerical optimization algorithms to also learning the underlying dictionary is somewhat
straightforward, there is very little understanding in using dynamical networks to learn a dictionary
with provable guarantees due to the challenging locality constraints.
1
Published as a conference paper at ICLR 2019
W
三"A-
Input layer	….
%1	XM
Coding
layer
一(1 - y)λιs1
Input layer
W
1 - y)^ι	(i- y)为M
一(1 - y)^ιsw
YB
(a)	(b)
Figure 1: The network topologies discussed in this work. (a) is known as the LCA network that can
perform sparse coding. We propose the network in (b) for dictionary learning.
In this work, we devise a new network topology and learning rules that enable dictionary learning. In
particular, we show that the gradients for learning are provably computable by individual neurons
using only local information. On a high level, our learning strategy is similar to the contrastive learning
procedure developed in training Boltzmann machines, which also gathers much recent interest in
deriving implementations of backpropagation under the same neuron locality constraints (Ackley et al.,
1985; Movellan, 1990; O’Reilly, 1996; Xie & Seung, 2003; Scellier & Bengio, 2017; Whittington &
Bogacz, 2017). During training, the network is run in two different configurations - a “normal” one
and a “perturbed” one.1 The networks’ limit points under these two configurations will be identical
if the weights to be trained are already optimal, but different otherwise. The learning process is a
scheme to so adjust the weights to minimize the difference in the limit points. In Boltzmann machine,
the weight adjustment can be formulated as minimizing a KL divergence objective function.
For dictionary learning, we adopt a neuron model whose activation function corresponds to the
unbounded ReLU function rather than the bounded sigmoid-like function in Hopfield networks or
Boltzmann machines, and a special network topology where connection weights have dependency.
Interestingly, the learning processes are still similar: We also rely on running our network in two
configurations. The difference in states after a long-enough evolution, called limiting states in
short, is shown to hold the gradient information of a dictionary learning objective function which
the network minimizes, as well as the gradient information for the network to maintain weight
dependency. Comparisons between this work, Hopfield network, and Boltzmann machine can be
found in Appendix C.1.
1.1	Related Work
Dictionary learning is thought to be related to the formation of receptive fields in visual cortex (Ol-
shausen & Field, 1996) and has been widely studied. The typical architecture studied is a feedforward-
only, two-layer neural network with inhibitory lateral connections among the second layer neurons as
shown in Figure 1(a) (Foldiak, 1990; Zylberberg et al., 2011; Brito & Gerstner, 2016; Hu et al., 2014;
Seung & Zung, 2017; Pehlevan et al., 2018; Vertechi et al., 2014; Brendel et al., 2017). The lateral
connections allow the coding neurons to compete among themselves and hence induce sparseness in
neural activities, giving dynamics more complex than the conventional deep neural networks, which
do not have intra-layer connections.2 In Rozell et al. (2008), it is shown that the coding neuron
activations can correspond to a sparse coding solution if the connection weights are set according to
a global dictionary D as F = DT, W = -DTD + I.3 To enable learning in this network (that is,
each neuron locally adjusts their connection weights to adapt the dictionary; see Section 2.2 for the
definition of weight locality), one must address the following two questions:
•	How does individual neuron compute the gradient for learning locally?
•	How do the neurons collectively maintain the global weight consistency between F and W ?
1In Botlzmann machine, the two configurations are called the free-running phase and the clamped phase.
2This should not be confused with the conventional recurrent neural networks. Although RNNs also have
intra-layer connections, these connections are still uni-directional over a sequence of input.
3The exact formulation depends on the neuron model. In the spiking neuron formulation, we in fact have
W - Θ = -DTD where Θ is the firing thresholds. See Section 3.1 for more details.
2
Published as a conference paper at ICLR 2019
The first line of work, Foldiak (1990); Zylberberg et al. (2011); Brito & Gerstner (2016), adopts
the Hebbian/anti-Hebbian heuristics for learning the feedforward and lateral weights, respectively,
and empirically demonstrated that such learning yielded Gabor-like receptive fields if trained with
natural images. However, unlike the network in Rozell et al. (2008), this learning heuristic is not
mathematically derived from a rigorous learning objective, and hence cannot address any of the
two above questions. Recently, it is shown that learning rules resembling the Hebbian/anti-Hebbian
heuristic can be derived from minimizing a “similarity matching” objective function between input
and output correlations (Hu et al., 2014; Pehlevan et al., 2018). This formulation is somewhat different
from the common autoencoder-style dictionary learning formulation discussed in this work.
Another line of work, Vertechi et al. (2014); Brendel et al. (2017), notes the importance of balance
between excitation and inhibition among the coding neurons, and proposes that the learning target
of lateral connections should be to maintain such balance. That is, the inhibitory lateral weights
should grow according to the feedforward excitation, and hence potentially can ensure the weight
consistency between F and W. Nevertheless, similar to the first line of work, both Vertechi et al.
(2014); Brendel et al. (2017) resort to pure Hebbian rule when learning the feedforward weights
F (or equivalently, learning the dictionary). Since the Hebbian rule does not necessarily follow a
descending direction that minimizes the dictionary learning objective function, the convergence of
this learning approach to a local minimum still cannot be guaranteed. Further discussions of prior
work are provided in Appendix C.2.
1.2	Contributions
The major advance in this work is to recognize the inadequacy of the customary feedforward-only
architecture, and to introduce top-down feedback connections shown in Figure 1(b). As will later
be shown, this network structure allows the true learning gradients to be provably computable from
the resulting network dynamics. Further, the existence of feedback allows us to devise a separate
mechanism that acts as an inner loop during learning to continuously ensure weight consistency
among all connections. Combining these two, we can successfully address both the above questions
and the dictionary learning problem.
We will focus our discussion on a network that uses spiking neurons as the basic units that are suited
for digital circuit implementations with high computational efficiency. Note that this does not result
in a loss of generality. The principles of LCA network can be applied to both continuous-valued
and spiking neurons (Shapero et al., 2014; Tang et al., 2017), and similarly the results established in
this paper can be easily applied to construct a network of continuous-valued neurons for dictionary
learning.
Finally, we note that in parallel to this work, various forms of feedback connections and mechanisms
have been proposed to address neural network local learning in the context of other learning problems,
including autoencoder networks (Hinton & McClelland, 1988; Burbank, 2015), deep neural networks
(Guerguiev et al., 2017; Sacramento et al., 2018), and memory networks (Federer & Zylberberg,
2018). This work further shows that feedback connection is also a crucial component for solving the
complex dictionary learning problem.
2	Background
2.1	Integrate-and-Fire Spiking Neuron Model and Network Dynamics
An integrate-and-fire neuron has two internal state variables that govern its dynamics: the current
μ(t) and the potential ρ(t). The key output of a neuron is a time sequence of spikes - spike train 一
that it produces. A neuron’s spike train is generated by its potential ρ(t); ρ(t) is in turn driven by the
current μ(t), which is in turn driven by a constant bias β (bias in short) and the spike trains of other
neurons to which it is connected. Specifically, each neuron has a configured firing threshold θ > 0.
When ρ(t) reaches θ, say at time tk, a spike given by the Dirac delta function δ(t - tk) is generated
and ρ(t) is reset to 0: ρ(tj) = 0. For t >tk and before ρ(t) reaches θ again, ρ(t) = Rtt μ(s) ds.
3
Published as a conference paper at ICLR 2019
In a system of N neurons ni, i = 1, 2, . . . , N, let σj (t) = k δ(t - tj,k) denote the spike train of
neuron n7-. The current μi(t) of n is given in terms of its bias βi and the spike trains {σj (t)}:
μi(t) = βi + Pj= Wij (α * σj)(t),	⑴
where α(t) = 1 e-t/T for t ≥ 0, α(t) = 0 for t < 0 and * is the convolution operator. Neuron nj
inhibits (excites) ni if Wij < 0 (Wij > 0). If Wij = 0, neurons ni and nj are not connected. For
simplicity, we consider only τ = 1 throughout the paper. Equation 1 yields the dynamics
“(t) = β — μ(t) + w ∙ σ(t),	(2)
where the vectors μ(t) and σ(t) denote the N currents and spike trains (see Appendix B.1 for the
full derivation.)
The network dynamics can be studied via the filtered quantities of average current and spike rate:
u(t) def ； J μ(s) ds,	a(t) C=f ； / σ(s) ds.	(3)
In terms of u(t) and a(t), Equation 2 becomes
U(t) = β — u(t) + W a(t) + (μ(0) — u(t))/t	(4)
The trajectory (u(t), a(t)) has interesting properties. In particular, Theorem 1 below (cf. Tang et al.
(2017)) shows that any limit point (u*, a*) satisfies u* — Θa* ≤ 0, a* ≥ 0 and (u* — Θa*) Θ a* = 0
where is elementwise product. These properties are crucial to Section 3.
Theorem 1. Let Θ = diag(θ), θ = [θ1, θ2, . . . , θN], then
u(t) — Θa(t) = β + (W — Θ) ∙ a(t) + ∆(t)	(5)
where max(u(t), 0) — Θa(t) → 0 and ∆(t) → 0.
As with all other theorems, Theorem 1 is given in a conceptual form where the corresponding rigorous
“e-6” versions are detailed in the Appendix.
2.2	Parallel Model of Dynamical Neural Networks
We view the dynamical network as a computational model where each neuron evolves in parallel and
asynchronously. One-sided communication in the form of a one-bit signal from Neuron nj to Neuron
ni occurs only if the two are connected and only when the former spikes. The network therefore can
be mapped to a massively parallel architecture, such as Davies et al. (2018), where the connection
weights are stored distributively in each processing element’s (PE) local memory. In the most general
case, we assume the architecture has the same number of PEs and neurons; each PE hosts one neuron
and stores the weights connected towards this neuron, that is, each PE stores one row of the W
matrix in Equation 2. With proper interconnects among PEs to deliver spike messages, the dynamical
network can be realized to compute sparse coding solutions.
This architectural model imposes a critical weight locality constraint on learning algorithms for
dynamical networks: The connection weights must be adjusted with rules that rely only on locally
available information such as connection weights, a neuron’s internal states, and the rate of spikes it
receives. The goal of this paper is to enable dictionary learning under this locality constraint.
3	Dictionary Learning
In dictionary learning, we are given P images x(p) ∈ R≥M0, p = 1, 2, . . . , P . The goal is to find a
dictionary consisting of a prescribed number of N atoms, D = [d1, d2, . . . , dN], D ∈ RM×N such
that each of the P images can be sparsely coded in D. We focus here on non-negative dictionary and
formulate our minimization problem as
P	1λ
arg min El(D, x(p), a(p)), l(D, x, a) = 5kx — Dak2 + λ1kSa∣∣1 + 5 ∣∣D∣∣F,	(6)
a(p) ≥0,D≥0 p=1	2	2
4
Published as a conference paper at ICLR 2019
S being a positive diagonal scaling matrix.
Computational methods such as stochastic online training (Aharon & Elad, 2008) is known to be
effective for dictionary learning. With this method, one iterates on the following two steps, starting
with a random dictionary.
1.	Pick a random image X J X(P) and obtain sparse code a for the current dictionary D and
image x, that is, solve Equation (6) with D fixed.
2.	Use gradient descent to update D with a learning rate η. The gradient VD with respect to D
is in a simple form and the update of D is
DmeW) J D - η ((Da - χ)aτ + λ[D∖ .
(7)
Implementing these steps with a dynamical network is challenging. First, previous works have only
shown that Step 1 can be solved when the configuration uses the dictionary D in the feedforward
connection weights and DTD as the lateral connection weights (Shapero et al. (2014), c.f. Figure 1(a)
and below). For dictionary learning, both sets of weights evolve without maintaining this exact
relationship, casting doubt if Step 1 can be solved at all. Second, the network in Figure 1(a)
only has F = DT , rendering the needed term Da uncomputable using information local to each
neuron. Note that in general, gradients to minimize certain objective functions in a neural network
can be mathematically derived, but often times they cannot be computed locally, e.g., standard
backpropagation and general gradient calculations for spiking networks (Huh & Sejnowski, 2017).
We now show that our design depicted in Figure 1(b) can indeed implement Steps 1 and 2 and solve
dictionary learning.
3.1 Sparse Coding — Getting a
Non-negative sparse coding (Equation 6 with D fixed) is a constrained optimization problem. The
standard approach (cf. Boyd & Vandenberghe (2004)) is to augment l(D, X, a) with non-negative
slack variables, with which the optimal solutions are characterized by the KKT conditions. Consider
now Figure 1(b) that has explicit feedback weights B whose strength is controlled by a parameter γ.
Equation 5, reflecting the structure of the coding and input neurons, takes the form:
eγ(t)
fγ(t)
def
uγ (t) - Θaγ (t)
vγ(t) - bγ(t)
-(1 - γ)λ1s
(1 - γ)X
-H F
γB -I
aγ (t)
bγ(t)
+ ∆(t)	(8)
+
(u(t), v(t)) and (a(t), b(t)) denote the average currents and spike rates for the coding and input
neurons, respectively, and H d=ef W + Θ. Note that when γ = 0, the network is equivalent to
Figure 1(a). it is established in Tang et al. (2017) that when FT = D, H = DTD and at a limit point
(eɔ, aɔ), Equation 8 is simplified and reduces to eɔ = -λιS 一 DT Da0 + DTX and that eɔ ≤ 0,
aɔ ≥ 0 and eɔ Θ a0 = 0. This shows that aɔ and -e0 are the optimal primal and slack variables that
satisfy the KKT conditions. In particular a0 is the optimal sparse code.
For simplicity, we consider in this section that the feed-forward and feedback weights in Figure 1(b)
are initialized to be symmetric and correspond to a global dictionary D, FT = B = D (see further
discussions in Section 3.3 for the general case.) This network can similarly solve the sparse coding
problem. We extend the previously established result (Tang et al., 2017) in several aspects: (1) γ
can be set to any values in [0,1); all a； are the optimal sparse code, (2) H needs not be FB exactly;
kH - FBk being small suffices, and (3) as long as t is large enough, aγ (t) solves an approximate
sparse coding problem. These are summarized as follows (where the rigorous form is presented in
the Appendix).
Theorem 2.	Let FT = B = D, γ ∈ [0, 1) and kH - FBk be small. Then for t large enough, aγ (t)
is close to an exact solution a to Equation 6 (D fixed) With S replaced by S where ∣∣S — Sk is small.
The significant implication is that despite slight discrepancies between H and FB , the average spike
rate aγ (t) at t large enough is a practical solution to Step 1 of the stochastic learning procedure.
5
Published as a conference paper at ICLR 2019
3.2 Dictionary Adjustment - Updating F, B and H
To obtain the learning gradients, we run the network for a long enough time to sparse code twice: at
Y = 0 and Y = κ > 0, obtaining e0, ek a。, aκ and bo, bκ at those two configurations. We use tilde
to denote the obtained states and loosely call them as limiting states. Denote 1 - κ by κc .
Theorem 3.	The limiting states satisfy
κ(Baκ - x) ≈	gD,	gD =f	bK - bo	(9)
K(H - FB)aκ ≈	gH,	gH =f	κcH(ao - aκ) +	(κce°	- ej	(io)
We now show Theorem 3 lays the foundation for computing all the necessary gradients that we need.
Equation 9 shows that (recall B = D)
DaK - X ≈ κ-1gD.
in other words, the spike rate differences at the input layer reflect the reconstruction error of the
sparse code we just computed. Following Equation 7, this implies that the update to each weight can
be approximated from the spike rates of the two neurons that it connects, while the two spike rates
surely are locally available to the destination neuron that stores the weight. Specifically, each coding
neuron has a row of the matrix F = DT; each input neuron has a row of the matrix B = D. These
neurons each updates its row of matrix via
Fijnew) - Fij- ηD (K 1(aK)i (gD)j + λ2Fij)
B(new) - Bij- ηD (κ-1(aκ)j (gD )i + λ2Bij)
Note that FT = B = D is maintained.
ideally, at this point the W and Θ stored distributively in the coding neurons will be updated to
H (new) where H (new) = F (new)B(new). Unfortunately, each coding neuron only possesses one row
of the matrix F (new) and does not have access to any values of the matrix B(new). To maintain H to
be close to DTD throughout the learning process, we do the following. First we aim to modify H
to be closer to FB (not F(new)B(new)) by reducing the cost function φ(H) = 11 ∣∣(H - FB)aκk2.
The gradient of this cost function is NHφ =(H - FB)aκaT which is computable as follows.
Equation 10 shows that
Vhφ ≈ G C=f κ-1gHaT
Using this approximation, coding neuron nC,i has the information to compute the i-th row of G.
We modify H by -ηHG where ηH is some learning rate. This modification can be thought of as
a catch-up correction because F and B correspond to the updated values from a previous iteration.
Because the magnitude of that update is of the order ofηD, we have ∣H-FB∣ ≈ ηD and ∣G∣ ≈ ηD.
Thus ηH should be bigger than ηD lest ∣ηH G∣ ≈ ηHηD be too small to be an effective correction.
in practice, ηH ≈ 15ηD works very well.
in addition to this catch-up correction, we also make correction of H due to the update of -ηDλ2F
and -ηDλ2B to F and B. These updates lead to a change of -2ηDFB + O(ηD2 ). Consequently,
after Equation 11, we update H by
HF 一 Hij - ηHκ-1(gH)i(aκ)j - 2ηDλ2Hj.	(12)
Note that the update to H involves update to the weights W as well as the thresholds Θ (recall that
H C=ef W + Θ). Combining the above, we summarize the full dictionary learning algorithm below.
3.3 Discussions
Dictionary norm regularization. in dictionary learning, typically one needs to control the norms of
atoms to prevent them from growing arbitrarily large. The most common approach is to constrain
the atoms to be exactly (or at most) of unit norms, achieved by re-normalizing each atom after a
dictionary update. This method however cannot be directly adopted in our distributed setting. Each
input neuron only has a row of the matrix B but not a column of B - an atom - so as to re-normalize.
6
Published as a conference paper at ICLR 2019
Algorithm 1 Dictionary Learning
Initialization: Pick a random dictionary D ≥ 0 with atoms of unit Euclidean norm. Configure
F 一 DT, B 一 D, S 一 [1,1,..., 1]T, and H 一 FB.
repeat
1.	Online input: Pick a random image x from {x(p) }
2.	Sparse coding: Run the network at Y J 0 and at Y J κ > 0.
3.	Dictionary update: Compute the vectors gD and gH distributively according to Equations 9
and 10. Update F, B and H according to Equations 11 and 12. Project the weights to non-
negative quadrant.
4.	Scaling update: Set the scaling vector S to diag(H). This scaling helps maintain each atom
of the dictionary to be of similar norms.
until dictionary is deemed satisfactory
We chose instead to regularize the Frobenius norm of the dictionaries, translating to a simple decay
term in the learning rules. This regularization alone may result in learning degenerate zero-norm atoms
because sparse coding tends to favor larger-norm atoms to be actively updated, leaving smaller-norm
ones subject solely to continual weight decays. By choosing a scaling factor S set to diag(H), sparse
coding favors smaller-norm atoms to be active and effectively mitigates the problem of degeneracy.
Boundedness of network activities. Our proposed network is a feedback nonlinear system, and one
may wonder whether the network activities will remain bounded. While we cannot yet rigorously
guarantee boundedness and stability under some a priori conditions, currents and spike rates remain
bounded throughout learning for all our experiments. One observation is that the feedback excitation
amounts to YFBaγ (t) and the inhibition is H aγ (t). Therefore when H = FB and Y < 1, the
feedback excitation is nullified, keeping the network from growing out of bound.
Network execution in practice. Theoretically, an accurate spike rate can only be measured at a very
large T as precision increases at a rate of O(1/t). In practice, we observed that a small T suffices for
dictionary learning purpose. Stochastic gradient descent is known to be very robust against noise
and thus can tolerate the low-precision spike rates as well as the approximate sparse codes due to the
imperfect H ≈ FB . For faster network convergence, the second network Y = κ is ran right after the
first network Y = 0 with all neuron states preserved.
Weight symmetry. The sparse code and dictionary gradient are computed using the feedforward and
feedback weights respectively. Therefore a symmetry between those weights is the most effective
for credit assignment. We have assumed such symmetry is initialized and the learning rules can
subsequently maintain the symmetry. One interesting observation is that even if the weights are
asymmetric, our learning rules still will symmetrize them. Let Ei(jp) = Fj(ip) - Bi(jp) be the weight
difference at the p-th iteration. It is straightforward to show Ei(jp) = αp-1Ei(j1), α = 1 - ηDλ2.
Hence Ei(jp) → 0 as p gets bigger. In training deep neural networks, symmetric feedforward and
feedback weights are important for similar reasons. The lack of local mechanisms for the symmetry
to emerge makes backpropagation biologically implausible and hardware unfriendly, see for example
Liao et al. (2016) for more discussions. Our learning model may serve as a building block for the
pursuit of biologically plausible deep networks with backpropagation-style learning.
4	Numerical Experiments
We examined the proposed learning algorithm using three datasets. Dataset A. 100K randomly
sampled 8 × 8 patches from the grayscale Lena image to learn 256 atoms. Dataset B. 50K 28 × 28
MNIST images (LeCun et al., 1998) to learn 512 atoms. Dataset C. 200K randomly sampled 16 × 16
patches from whitened natural scenes (Olshausen & Field, 1996) to learn 1024 atoms. These are
standard datasets in image processing (A), machine learning (B), and computational neuroscience
(C).4 For each input, the network is ran with Y = 0 from t = 0 to t = 20 and with Y = 0.7 from
t = 20 to t = 40, both with a discrete time step of 1/32. Note that although this time window of 20
4For Dataset A and C, the patches are further subtracted by the means, normalized, and split into positive and
negative channels to create non-negative inputs (Hoyer, 2004).
7
Published as a conference paper at ICLR 2019
γ=0 (no feedback)	γ=0.7 (with feedback)	γ=0 (no feedback)	γ=0.7 (with feedback)
；≡	≡	≡ Ξ ≡ J
Time	Time
(a) Before learning (training sample No.1)	(b) After learning (training sample No.99900)
Figure 2:	Network spike patterns in practice. In the figures, each row corresponds to one neuron, and
the bars indicate the spike timings. Following Algorithm 1, for each online input the network is run
with γ = 0 from t = 0 to t = 20 (feed-forward only), and γ = 0.7 from t = 20 to t = 40 (perturb
network activities by feedback). The spike rates a, b for computing the gradients are then collected
by counting the number of spikes within the time interval. The figures in the left and right show the
spike patterns before and after learning, respectively. We make two observations: (1) The coding
neuron spike rates remain approximately constant for both intervals t = [0, 20] and t = [20, 40],
illustrating Theorem 2 that feedback perturbation does not alter the computed optimal sparse code.
This property holds throughout learning because our network learns to maintain weight consistency.
(2) For the figure on the left (before learning with a random dictionary), the input neuron spike rates
change significantly when feedback exists. From Theorem 3, this spike rate difference encodes the
reconstruction error of the computed sparse code. This quantity is used for gradient computation and
is expected to be minimized according to the learning objective. Indeed, after learning (figure on the
right), the input neuron spike rates exhibit much less perturbation by the feedback. This shows the
network is able to learn a proper dictionary that minimizes reconstruction error. Data is from learning
with Dataset A; only a subset of the neurons are shown.
is relatively small and yields a spike rate precision of only 0.05, we observed that it is sufficient for
gradient calculation and dictionary learning purpose.
We explored two different connection weight initialization schemes. First, we initialize the weights
to be fully consistent with respect to a random dictionary. Second, we initialized the weights to be
asymmetric. In this case, we set FT and B to be column-normalized random matrices and the entries
of H to be random values between [0, 1.5] with the diagonal set to 1.5.
4.1 Network Dynamics
We first show the spike patterns from a network with fully consistent initial weights in Figure 2. It can
be seen that the spike patterns quickly settle into a steady state, indicating that a small time window
may suffice for spike rate calculations. Further, we can observe that feedback only perturbs the input
neuron spike rates while keeping the coding neuron spike rates approximately the same, validating
our results in Section 3.1 and 3.2.
Another target the algorithm aims at is to approximately maintain the weight consistency H ≈ FB
during learning. Figure 3 shows that this is indeed the case. Note that our learning rule acts as a
catch-up correction, and so an exact consistency cannot be achieved. An interesting observation is
that as learning proceeds, weight consistency becomes easier to maintain as the dictionary gradually
converges.
Although we have limited theoretical understanding for networks with random initial weights, Figure
3 shows that our learning procedure can automatically discover consistent and symmetric weights
with respect to a single global dictionary. This is especially interesting given that the neurons only
learn with local information. No neuron has a global picture of the network weights.
8
Published as a conference paper at ICLR 2019
Figure 3:	Network weight consistency and symmetry during learning. Consistency is measured as
1 - kH - FBkF / kHkF. Symmetry is measured as the average normalized inner product between
the i-th row of F and the i-th column of B for i = 1 . . . N . Data is from learning with Dataset A.
10.5
Dataset A (Lena)
0)
10
9.5
O
9
8.5
φ _
8
7.5
O
7
-Ir-∣ I IΓ-∣ I IΓ-∣ I	I~~rη-
SPikingNet (consistent init) ----
∖ SpikingNet (random init)--------
、	SGD (η=η0) —*—
∖ '∖	SGD (η=2ηD) -B-
\、、、	SGD (η=0.5ηD) -e—
Dataset B (MNIST)
1 09876 543
11
tes tsetfo noitcnuf evitcejbO
102	103	104	105
Number of training samples
101
102	103	104	105
SPikingNet (consistent init) ---
SPikingNet (random init) -----
SGD (η=η°) —*—
SGD (η=2η°) ―B—
SGD (η=0.5η0) -θ-
Number of training samples
Number of training samples
Figure 4:	Comparison of convergence of learning with dynamical neural network and SGD.
4.2 Convergence of Dictionary Learning
The learning problem is non-convex, and hence it is important that our proposed algorithm can find
a satisfying local minimum. We compare the convergence of spiking networks with the standard
stochastic gradient descent (SGD) method with the unit atom norm constraint. For simplicity, both
algorithms use a batch size of 1 for gradient calculations. The quality of the learned dictionary
D = F T is measured using a separate test set of 10K samples to calculate a surrogate dictionary
learning objective (Mairal et al., 2009). For a fair comparison, the weight decay parameters in spiking
networks are chosen so that the average atom norms converge to approximately one.
Figure 4 shows that our algorithm indeed converges and can obtain a solution of similar, if not better,
objective function values to SGD consistently across the datasets. Surprisingly, our algorithm can
even reach a better solution with fewer training samples, while SGD can be stuck at a poor local
minimum especially when the dictionary is large. This can be attributed to the `1 -norm reweighting
heuristic that encourages more dictionary atoms to be actively updated during learning. Finally,
we observe that a network initialized with random non-symmetric weights still manages to reach
objective function values comparable to those initialized with symmetric weights, albeit with slower
convergence due to less accurate gradients. From Figure 3, we see the network weights are not
symmetric before 104 samples for Dataset A. On the other hand, from Figure 4 the network can
already improve the dictionary before 104 samples, showing that perfectly symmetric weights are not
necessary for learning to proceed.
5 Conclusion
We have presented a dynamical neural network formulation that can learn dictionaries for sparse
representations. Our work represents a significant step forward that it not only provides a link
between the well-established dictionary learning problem and dynamical neural networks, but also
demonstrates the contrastive learning approach to be a fruitful direction. We believe there is still much
to be explored in dynamical neural networks. In particular, learning in such networks respects data
locality and therefore has the unique potential, especially with spiking neurons, to enable low-power,
high-throughput training with massively parallel architectures.
9
Published as a conference paper at ICLR 2019
References
David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for Boltzmann
machines. Cognitive science,9(1):147-169, 1985.
Michal Aharon and Michael Elad. Sparse and redundant modeling of image content using an
image-signature-dictionary. SIAM Journal on Imaging Sciences, 1(3):228-247, 2008.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183-202, 2009.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,
Cambridge, UK, 2004.
Wieland Brendel, Ralph Bourdoukan, Pietro Vertechi, Christian K Machens, and Sophie Deneve.
Learning to represent signals spike by spike. arXiv preprint arXiv:1703.03777, 2017.
Carlos S. N. Brito and Wulfram Gerstner. Nonlinear Hebbian learning as a unifying principle in
receptive field formation. PLoS Comput Biol, 12(9):1-24, 2016.
Alfred M Bruckstein, Michael Elad, and Michael Zibulevsky. On the uniqueness of nonnegative
sparse solutions to underdetermined systems of equations. IEEE Transactions on Information
Theory, 54(11):4813-4820, 2008.
Kendra S Burbank. Mirrored STDP implements autoencoder learning in a network of spiking neurons.
PLoS Comput Biol, 11(12):e1004566, 2015.
Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha
Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic
manycore processor with on-chip learning. IEEE Micro, 38(1):82-99, 2018.
Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. Least angle regression. The
Annals of statistics, 32(2):407-499, 2004.
Michael Elad and Michal Aharon. Image denoising via learned dictionaries and sparse representa-
tion. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on,
volume 1, pp. 895-900. IEEE, 2006.
Callie Federer and Joel Zylberberg. A self-organizing short-term dynamical memory network. Neural
Networks, 2018.
Peter Foldiak. Forming sparse representations by local anti-Hebbian learning. Biological cybernetics,
64(2):165-170, 1990.
Yoav Freund and David Haussler. Unsupervised learning of distributions on binary vectors using two
layer networks. In Advances in neural information processing systems, pp. 912-919, 1992.
Jordan Guerguiev, Timothy P Lillicrap, and Blake A Richards. Towards deep learning with segregated
dendrites. ELife, 6:e22901, 2017.
Geoffrey E Hinton and James L McClelland. Learning representations by recirculation. In Neural
information processing systems, pp. 358-366, 1988.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527-1554, 2006.
J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proc. Natl. Acad. Sci., 79(8):2554-2558, 1982.
J. J. Hopfield. Neurons with graded response have collective computational properties like those of
two-state neurons. Proc. Natl. Acad. Sci., 1:3088-3092, 1984.
Patrik O Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine
learning research, 5(Nov):1457-1469, 2004.
10
Published as a conference paper at ICLR 2019
Tao Hu, Cengiz Pehlevan, and Dmitri B Chklovskii. A hebbian/anti-hebbian network for online
sparse dictionary learning derived from symmetric matrix factorization. In 2014 48th Asilomar
Conference on Signals, Systems and Computers, pp. 613-619. IEEE, 2014.
Dongsung Huh and Terrence J Sejnowski. Gradient descent for spiking neural networks. arXiv
preprint arXiv:1706.04698, 2017.
Hsiang-Tsung Kung. Why systolic architectures? IEEE computer, 15(1):37-46, 1982.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Qianli Liao, Joel Z Leibo, and Tomaso A Poggio. How important is weight symmetry in backpropa-
gation? In AAAI, pp. 1837-1844, 2016.
Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse
coding. In Proceedings of the 26th annual international conference on machine learning, pp.
689-696. ACM, 2009.
Julien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing.
Foundations and TrendsR in Computer Graphics and Vision, 8(2-3):85-283, 2014.
Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp
Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spiking-
neuron integrated circuit with a scalable communication network and interface. Science, 345
(6197):668-673, 2014.
Javier R Movellan. Contrastive Hebbian learning in the continuous hopfield model. In Connectionist
models: Proceedings of the 1990 summer school, pp. 10-17, 1990.
Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning
a sparse code for natural images. Nature, 381:13, 1996.
Randall C O’Reilly. Biologically plausible error-driven learning using local activation differences:
The generalized recirculation algorithm. Neural computation, 8(5):895-938, 1996.
Cengiz Pehlevan, Anirvan M Sengupta, and Dmitri B Chklovskii. Why do similarity matching
objectives lead to hebbian/anti-hebbian networks? Neural computation, 30(1):84-124, 2018.
Marc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efficient learning of
sparse representations with an energy-based model. In Advances in neural information processing
systems, pp. 1137-1144, 2007.
Christopher J Rozell, Don H Johnson, Richard G Baraniuk, and Bruno A Olshausen. Sparse coding
via thresholding and local competition in neural circuits. Neural computation, 20(10):2526-2563,
2008.
Ron Rubinstein, Alfred M Bruckstein, and Michael Elad. Dictionaries for sparse representation
modeling. Proceedings of the IEEE, 98(6):1045-1057, 2010.
Joao Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical microcircuits
approximate the backpropagation algorithm. arXiv preprint arXiv:1810.11393, 2018.
Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-
based models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.
H Sebastian Seung and Jonathan Zung. A correlation game for unsupervised learning yields com-
putational interpretations of hebbian excitation, anti-hebbian inhibition, and synapse elimination.
arXiv preprint arXiv:1704.00646, 2017.
Samuel Shapero, Mengchen Zhu, Jennifer Hasler, and Christopher Rozell. Optimal sparse approxi-
mation with integrate and fire neurons. International journal of neural systems, 24(05):1440001,
2014.
11
Published as a conference paper at ICLR 2019
Ping Tak Peter Tang. Convergence of LCA Flows to (C)LASSO Solutions. ArXiv e-prints, March
2016.
Ping Tak Peter Tang, Tsung-Han Lin, and Mike Davies. Sparse coding by spiking neural networks:
Convergence theory and computational results. ArXiv e-prints, 2017.
Pietro Vertechi, Wieland Brendel, and Christian K Machens. Unsupervised learning of an efficient
short-term memory network. In Advances in Neural Information Processing Systems, pp. 3653-
3661, 2014.
James CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm
in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5):
1229-1262, 2017.
Xiaohui Xie and H Sebastian Seung. Equivalence of backpropagation and contrastive Hebbian
learning in a layered network. Neural computation, 15(2):441-454, 2003.
Joel Zylberberg, Jason Timothy Murphy, and Michael Robert DeWeese. A sparse coding model with
synaptically local plasticity and spiking neurons can account for the diverse shapes of v1 simple
cell receptive fields. PLoS Comput Biol, 7(10):e1002250, 2011.
12
Published as a conference paper at ICLR 2019
Appendices
A	Detailed Description of Proposed Network S tructure
We propose a novel network topology with feedback shown in Figure 1(b). The figure shows two
“layers” of neurons. The lower layer consists of M neurons we call input neurons, nI,i for i =
1, 2, . . . , M; the upper layer consists of N neurons we call coding neurons nC,i for i = 1, 2, . . . , N.
Each coding neuron nC,i receives excitatory signals from all the input neurons nI,j with a weight of
Fij ≥ 0. That is, each coding neuron has a row of the matrix F ∈ RN≥0×M. In addition, neuron nC,i
receives inhibitory signals from all other coding neurons nC,j with weight -Wij ≤ 0. W denotes this
matrix of weights: W ∈ R≥N0×N and diag(W) = 0. The firing thresholds are θ = [θ1, θ2, . . . , θN]T
and the matrix W + Θ, Θ = diag(θ), appears often and will denote it as H d=ef W + Θ. Each
neuron nC,i also receives a constant negative bias of -(1 - γ)λ1si where 0 ≤ γ < 1 is an important
parameter that will be varied during the learning process to be detailed momentarily.
Each input neuron nI,i, i = 1, 2, . . . , M, with firing threshold fixed to be 1, receives a bias of
(1 - γ)xi. Typically xi corresponds to the i-th pixel value of an input image in question during the
learning process. In addition, it receives excitatory spikes from each of the coding neurons with
weights γBij ≥ 0. That is each input neuron has a row of the matrix B ∈ R≥M0×N. These excitatory
signals from the coding neurons constitute the crucial feedback mechanism we devised here that
enables dictionary learning.
B	Proof of Theorems
B.1 Theorem 1: SNN dynamics, trajectory, and limit points
In the simplest case when none of the neurons are inter-connected and ρi (0) < θi for all i, then
μi(t) = βi for all i and all t ≥ 0. Hence those neurons n with βi > 0 produces a spike train of
constant inter-spike interval of θi /βi ; those neurons with βi ≤ 0 will have no spiking activities.
When however the neurons are inter-connected, the dynamics becomes non-trivial. It turns out that
one can so describe the dynamics mathematically that useful properties related to the current and
spike train can be derived. Consequently, a network of spiking neurons can be configured to help
solve certain practical problems.
Given a system of N neurons ni, i = 1, 2,...,N, we use vector notations μ(t) and σ(t) to denote
the N currents and spike trains. The vector β and θ are the input biases and firing thresholds. The
convolution (α * σ)(t) is the N-vector whose i-th component is (α * σ∕(t). For simplicity, we
consider only τ = 1 throughout the paper. Thus α(t) = e-t for t ≥ 0 and 0 otherwise. Equation 1 in
vector form is
μ(t) = β + W(α * σ)(t)	(13)
where W ∈ RN ×N and Wii = 0, encodes the inhibitory/excitatory connections among the neurons.
Because S(α * σ)(t) = σ(t) — (α * σ)(t), we have
μ(t) = β — μ(t) + W ∙ σ(t).	(14)
Filtering Equation 14 yields
U(t) = β — u(t) + W a(t) + (μ(0) — u(t))/t
u(t) — Θ a(t) = β + (W — Θ) a(t)
+ (μ(0) — u(t))/t — u(t)	(15)
where Θ = diag(θ). Theorem B.1 has been established previously in Tang et al. (2017) in a
slightly different form. We attach the proof consistent to our notations below for completeness. It is
established under the following assumptions:
• The currents of all neurons remain bounded from above, kμ(t)k∞ ≤ B for all t ≥ 0 for
some B > 0. This implies no neuron can spike arbitrarily fast, and the fact that neurons
cannot spike arbitrarily rapidly implies the currents are bounded from below as well
13
Published as a conference paper at ICLR 2019
• There is a positive number r > 0 such that whenever the numbers ti,k and ti,k+1 exist,
ti,k+1 - ti,k ≤ 1/r. This assumption says that unless a neuron stop spiking althogether
after a certain time, the duration between consecutive spike cannot become arbitrarily long.
Theorem B.1. As t → ∞, U(t), 1 (μ(0) — u(t)) and max(u(t), 0) — Θ a(t) all converge to 0.
Proof. Let
A = { i | neuron-i spikes infinitely often }
(A stands for “active”), and
I = { i | neuron-i stop spiking after a finite time }
(I stands for “inactive”). First consider i ∈ I. Let ti,k be the time of the final spike. For any t > ti,k,
ui(t)
Z Z	μi(s) ds + Z Z μi(s) ds
t 0	t ti,k
~ Z	μi(s) ds + 7pi(t)
t0	t
=θiai(t) + -p,i,(t)
Note that ρi(t) ≤ θi always. If ρi (t) ≥ 0, then
0 ≤ max(ui(t), 0) — θiai(t) ≤ θi/t.
If ρi(t) < 0,
—θiai(t) ≤ max(ui(t), 0) — θiai(t) ≤ 0.
Since i ∈ I, ai(t) → 0 obviously. Thus
max(ui(t), 0) — θiai(t) → 0.
Consider the case of i ∈ A. For any t > 0, let ti,k be the largest spike time that is no bigger than t.
Because i ∈ A, ti,k → ∞ as t → ∞.
ui (t)
1	ti,k
t √0
μi(s) ds + 1 [
t ti,k
μi(s) ds
1t
θiai(t) + - J	μi(s) ds.
Furthermore, note that because of the assumption ti,k+1 — ti,k ≤ 1/r always, where r > 0,
lim inf ai (t) ≥ r. In otherwords, there is a time T large enough such that ai (t) ≥ r/2 for all i ∈ A
and t ≥ T. Moreover, 0 ≤ t — -i,k ≤ ti,k+ι - -i,k ≤ 1/r and μi(t) ∈ [B-,B+]. Thus
1Z t
t	ti,k
μi(s) ds ∈ ɪ [B-, B+]∕r → 0.
When this term is eventually smaller in magnitude than θiai (t), we have
ui(t) — θi ai (t) → 0.
or equivalently,
max(ui(t), 0) — θiai(t) → 0.
Applying Theorem B.1 to Equation 15 yields the following.
Theorem B.2. Given any > 0, there exists T > 0 such that for all t > T,
k(u(t) — Θa(t)) — (β + (W - Θ) a(t))∣∣∞ < e.
The following theorem characterizes limit points of the trajectory (u(t), a(t)). Recall that (u*, a*) is
a limit point if given any > 0, there exists a time T > 0 large enough such that (u(T), a(T)) is
within E to (u*, a*).
Theorem B.3. Given any limit point (u*, a*), we must have β + (W — Θ) a* ≤ 0, a* Θ (β +
(W — Θ) a*) = 0 and a* ≥ 0, where Θ is the elementwise product.
Proof. Theorem B.1 shows that u* — Θ a* ≤ 0 and the elementwise product a* Θ (u* — Θ a*) = 0.
But Theorem B.2 shows that u* — Θ a* = β + (W — Θ) a* and the theorem here is established.
14
Published as a conference paper at ICLR 2019
Figure 5: A 1-layer LCA network for sparse coding.
B.2	NON-NEGATIVE SPARSE CODING BY SPIKING NEURAL NETWORKS
Given a non-negative dictionary D ∈ RMXN, a positive scaling vector S = [si, s2,..., SN ]T ∈ RN0
and an image X ∈ RM0, the non-negative sparse coding problem can be formulated as
a* = arg min l (a), l(a)=工 ∣∣x — D a∣∣2 + λ ∣∣S a∣∣ 1	(16)
a≥0	2
where S = diag(s). Using the well-known KKT condition in optimization theory, see for example
Boyd & Vandenberghe (2004), a* is an optimal solution iff there exists e* ∈ RN such that all of the
following hold:
0 ∈ ∂l(a*) —e*
e*	a* = 0
a* ≥ 0, e* ≥ 0
(stationarity)
(complementarity)
(feasibility)
(17)
where ∂l is the generalized gradient of l. Note that the generalized gradient ∂l(a) is DTDa —
DTX + λs Θ ∂∣a∣ι and that ∂∣a∕ = 1 when a% > 0 and equals the interval [—1,1] when a% = 0.
Straightforward derivation then shows that a* is an optimal solution iff
DT X — λ s — DT Da* ≤ 0 ; and
a* Θ (DT X — λs — DTDa*) = 0.
(18)
We now configure a N -neuron system depicted in Figure 5 so as to solve Equation 16. Set θ =
diag(DT D) as the firing thresholds and set β = DTX — λs as the bias. Define the inhibition matrix
to be —(DTD — Θ), Θ = diag(θ). Thus neuron-j inhibits neuron-i with weight —diT dj ≤ 0. In this
configuration, it is easy to establish ∣∣μ(t)∣∞ ≤ C for all t ≥ 0 for some C > 0 as all connections
are inhibitions. From Theorem B.3, any limit point (u*, a*) of the trajectory (u(t), a(t)) satisfies
β 十 (W — Θ)a* ≤ O and a* Θ (β + (W — Θ)a*) = 0, But β = DTX — λ S and W — Θ = —DTD.
Thus a* solves Equation 16. And in particular, if the solution to Equation 16 is unique, the trajectory
can only have one limit point, which means in fact the trajectory converges to the sparse coding
solution. This result can be easily extended to the network in Figure 1(a) by expanding the bias into
another layer of input neuron with F = DT .
B.3	Theorem 2: sparse coding with feedback perturbation
Equation 5, reflecting the structure of the coding and input neurons, takes the form:
eγ(t)
fγ(t)
def uγ (t) — Θaγ (t)
= vγ(t) — bγ(t)
—(1 — γ)λ1S + —H
(1 — γ)X	γB
aγ(t)
bγ(t)
+ ∆(t)
(19)
(u(t), v(t)) and (a(t), b(t)) denote the average currents and spike rates for the coding and input
neurons, respectively, and H d=ef W +Θ. Note that max(uγ (t), 0) — Θaγ (t), max(vγ (t), 0) — bγ (t)
and ∆(t) all converge to 0 as t → ∞.
Theorem B.4. Consider the configuration FT = B = D and γ ∈ [0, 1). Suppose the soma
currents and thus spike	rates ∣aγ	(t)∣∞ are bounded. Let H = DTD + (λ1γc)-1∆H,	γc =	1 — γ,
be such that 4∣∆H ∣1 ∣aγ (t)∣∞	<	min{si}.	Then, for any > 0 there is T >	0 such that
r 11 1	m 11	/ ɪ ∖	ʌ / ɪ ∖ 11	.	1 ʌ / ɪ ∖	1 1 - . ∙ 1 x ∙. 7 ri ι ι	ι ri	ι
for all t > T, ∣∣aγ(t) — a(t)∣∞	<	E and a(t)	solves Equation 16 With S replaced	by S	where
.._. <∙.. - 、 ∙
∣∣S — S∣∣∞ < mm{si}∕2.
15
Published as a conference paper at ICLR 2019
Proof. Consider τ > 0 and define the vectors a(t) and u(t) for t ≥ 0 by each oftheir components:
(aγ,i(t), θiaγ,i(t))	if aγ,i(t) ≥ τ,
(0,	min(uγ,i(t), 0)) otherwise,
(^i(t),Ui(t))
where θ is the diagonal of DTD. Denote the perturbations ∆a(t) d=f a(t) 一 aγ (t), ∆u(t) d=f
u(t) — Uγ(t), ^γ(t) d=f u(t) — Θa(t), and∆e(t) d=f e(t) — βγ(t). This construction of u(t) anda(t)
ensures k∆a(t)k∞ < G e(t) ≤ 0, and e(t) Θ a(t) = 0. Recall that max(uγ(t), 0) — Θaγ(t) → 0
(Theorem B.1); thus k∆u(t)k∞ < 2τ at t large enough.
Next, observe that vγ (t) ≥ 0 always νγ (t) ≥ 0 always, for any setting γ in [0, 1). Thus Theorem B.1
implies
bγ (t) — [(1 — γ)x + γBaγ(t)] → 0	(20)
as t → ∞. From Equation 19, this implies that
eγ (t) = γc(DT x — λ1s — DTDaγ (t) — λ1∆Haγ(t) + ∆(t))
for some ∆(t) where k∆(t)k∞ → 0. Thus
(γc)-1e(t) = DT X — λιS — DT Da(t)
where S = S — (η(t) + Z(t)), η(t) = Δhaγ(t) and Z(t) = λ-1(DTD∆° (t) + ∆e(t)∕γc + ∆(t)).
By assumption on ∆H, s — η(t) > (3/4)s > 0. Moreover,kζ (t)k∞ can be made arbitrarily small by
taking t and 1∕τ large enough. Thus there exist τ, T > 0 such that for all t > T, k∆a(t)k∞ <
and ∣∣S(t) — s∣∣∞ < min{si}∕2, implying in particular S(t) > s/2 > 0. Finally, note that
a(t) ≥ 0,	(Yc)T^(t) ≤ 0,	(γc)-1e(t) Θ a(t) = 0,
which shows (recall Equation 18) that a(t) solves Equation 16 with S replaced by S and the proof is
complete.
At present, we cannot establish a priori that the currents stay bounded when γ > 0. Nevertheless,
the theorem is applicable in practice as long as the observed currents stay bounded by some C for
0 ≤ t ≤ T and C∕T is small enough. See Section 3.3 for further comments.
B.4	Theorem 3: gradient calculations from contrastive learning
Theorem B.5. Given any > 0, there is a T > 0 such that for all t, t0 > T,
∣κ(Baκ(t0) — X) — (bκ(t0) — b0(t))∣∞ < ,	(21)
∣∣κcH(ao(t) — aκ(t0)) — K(H — FB)aκ(t0) + (κce0(t) — θκ(t0))∣∞ < e. (22)
Proof. Equation 20 implies that
κ(Baκ(t0) — X) — (bκ (t0) — b0(t)) → 0 as t,t0 → ∞,
establishing Equation 21. From Equations 19 and 20
— κcλS — κcHa0(t) + κcFX — κce0(t) → 0, and,
— κcλS — Haκ(t) + κcFX + κFBaκ (t) — eκ(t) → 0.
Equation 22 thus follows.
C Comparisons with Prior Work
C.1 Comparisons of dynamical neural networks
Table 1 provides a summary of the development of three types of dynamical neural networks: Hopfield
network, Boltzmann machine, and sparse coding network.
16
Published as a conference paper at ICLR 2019
Hopfield network	
Neuron model	Binary or continuous (Hopfield, 1982; 1984)
Activation	Binary: Thresholding Continuous: Any bounded, differentiable, strictly increasing function
Topology	Arbitrary symmetric bidirectional connections
Learning	Binary: Hebbian rule Continuous: contrastive learning (Movellan, 1990)
Limit point	Many local minimum
Usage	Associative memory, constraint satisfaction problem
Boltzmann machine	
Neuron model	Binary or continuous (for visible units) (Ackley et al., 1985; Freund & Haussler, 1992)	
Activation	Logistic
Topology	BM: Arbitrary symmetric bidirectional connections RBM:Two-layer with symmetric forward/backward (Hinton et al., 2006)
Learning	BM: contrastive learning RBM: contrastive divergence
Limit point	Many local minimum
Usage	Generative model, constraint satisfaction problem
Sparse coding network	
Neuron model	Continuous or spiking (Rozell et al., 2008; Shapero et al., 2014)
Activation	Rectified linear
Topology	Two-layer with feedforward, lateral, and feedback connections
Learning	Contrastive learning with weight consistency
Limit point	Likely unique (Bruckstein et al., 2008)
Usage	Representation learning with sparse prior, image denoising and super-resolution, compressive sensing
Table 1: Comparison between dynamical neural networks. Text in boldface indicates the new results
established in this work.
C.2 Comparisons of dictionary learning networks
As we discussed in Section 1.1, there are several prior work that qualitatively demonstrate dictionary
learning in dynamical neural networks. The prior work Foldiak (1990); Zylberberg et al. (2011);
Brito & Gerstner (2016); Hu et al. (2014); Seung & Zung (2017); Vertechi et al. (2014); Brendel
et al. (2017) employ a feedforward-only network topology as shown in Figure 1(a), and are unable to
compute the true gradient for dictionary learning from local information. These work hence rely on
additional heuristic or assumptions on input data for learning to work. In contrast, we propose to
introduce feedback connections as shown in Figure 1(b), which allows us to solve the fundamental
problem of estimating the true gradient. Recall the dictionary learning objective function (Equation 6
in the main text)
P	1λ
argmin El(D, x(p), a(p)), l(D, x, a) = 5l∣x - Dak2 + λι∣∣Sa∣∣ι + VIlDIIF,	(23)
a(p)≥0,D≥0 p=1	2	2
and the true stochastic gradient of the learning problem is (Equation 7 in the main text)
DSeW) - D - η ((Da - x)aT + 入2口、.	(24)
17
Published as a conference paper at ICLR 2019
Here we provide a detailed discussion on the difference and limitations of prior work.
The first line of work is the so-called Hebbian/anti-Hebbian network FGldiak (1990); Zylberberg et al.
(2011); Brito & Gerstner (2016); Hu et al. (2014); Seung & Zung (2017). The principle of learning in
these work is to apply Hebbian rules to learn excitatory feedforward weights (strengthen an excitatory
connection if the two connected neurons, input and coding neurons, have strong activations) and
anti-Hebbian learning for inhibitory lateral weights (strengthen an inhibitory connection if the two
connected neurons, both coding neurons, have strong activations). Due to the heuristic nature of this
learning strategy, it is unclear whether this approach can solve the dictionary learning problem in
Equation 23. Zylberberg et al. (2011) argues that the Hebbian rule can approximate the gradient of
the reconstruction error term under a strong assumption that for some batch of successive inputs, the
activities of the coding neurons are uncorrelated (i.e., their computed sparse codes are uncorrelated),
and all the neurons have the same average activations. However, since the anti-Hebbian rule cannot
ensure weight consistency between feedforward and lateral weights, the sparse code computed by the
network may not correspond to any quantity related to the dictionary learning objective function. A
significant gap still exists between this learning strategy and a rigorous learning objective function.
Hu et al. (2014); Pehlevan et al. (2018) argues that this learning framework arises from a different
objective function other than Equation 23. By using the following “similarity matching” objective
function,
arg min kXTX - AT Ak2F ,	(25)
A
where X ∈ RM×P andA ∈ RN×P are formed by stacking the input x and the sparse codes a along
the columns, respectively, they are able to derive learning rules that resemble the principles of the
Hebbian/anti-Hebbian learning heuristic. This formulation is somewhat different from the dictionary
learning objective function we are interested in.
The second line of work Vertechi et al. (2014); Brendel et al. (2017) proposes to learn the lateral
weights according to the feedforward weights instead of using anti-Hebbian rules to address global
weight consistency, although the learning of feedforward weight still follows Hebbian rules, giving
the following update equation,
DmeW) - D + η (XaT - λ2D) .	(26)
It can be seen that Equation 26 is not an unbiased estimate of the true stochastic gradient in Equa-
tion 24. Hence in theory the convergence of learning to an optimal solution cannot be guaranteed.
Nontheless, Vertechi et al. (2014); Brendel et al. (2017) show that under the assumption that the input
is whitened and centered, empirically Equation 26 can progressively learn a dictionary with improved
reconstruction performance. For the general case of non-whitened input, the authors proposed a
modified dictionary learning objective function:
l = 1(Xc- Da)TCT(Xc- Da) + λ1∣∣a∣∣1	(27)
where Xc is the mean-removed training sample and C is the covariance, different from the common
dictionary learning objective that we are interested in Equation 23. Note that in Vertechi et al. (2014);
Brendel et al. (2017), it is unclear whether the non-negative constraints D ≥ 0, a ≥ 0 exist in the
learning objective, although fundamentally the two constraints cannot be omitted: D ≥ 0 is needed
so that the input neurons are only excitatory; a ≥ 0 is needed because spike rates cannot go negative.
Once these two constraints are incorporated, the objective function in Equation 27 may not be very
meaningful: the input to be estimated Xc may have negative entries, while the estimation Da is
always non-negative.
In this work, we directly estimate the true stochastic gradient for dictionary learning, and therefore
we do not need to make additional assumptions on the training input. As discussed in the main
text, obtaining such estimate requires adding the feedback connections with the resulting non-trivial
network dynamics. We provide extensive analysis and proofs and show that dictionary learning can
be solved under this setting.
Finally, we note that the need for feedback has been repeatedly pointed out in training autoencoder
networks (Hinton & McClelland, 1988; Burbank, 2015). Autoencoder networks do not have the
lateral connections as presented in the sparse coding network. Reconstruction errors there are
computed by running the network, alternating between a forward-only and a backward-only phase.
In contrast, we compute reconstruction errors by having our network evolve simultaneously with
18
Published as a conference paper at ICLR 2019
Figure 6: The figure shows a random subset of the dictionaries learned in spiking networks. They
show the expected patterns of edges and textures (Lena), strokes and parts of the digits (MNIST), and
Gabor-like oriented filters (natural scenes), similar to those reported in prior works (Rubinstein et al.,
2010; Ranzato et al., 2007; Hoyer, 2004).
both feedforward and feedback signals tightly coupled together. Nevertheless, these models do not
form strong back-coupled dynamical neural networks. Instead, they rely on staged processing much
similar to a concatenation of feedforward networks. For our network, the dictionary learning relies
only on locations of the dynamics’ trajectories at large time which need not be close to a stable limit
point. Simple computations between these locations that corresponding to two different network
configurations yield the necessary quantities such as reconstruction error or gradients for minimizing
a dictionary learning objective function.
D Additional Numerical Experiment Results
D.1 Visualization of learned dictionaries
In Section 4, we presented the convergence of dictionary learning by dynamical neural networks on
three datasets: Lena, MNIST, and SparseNet. Figure 6 shows the visualization of the respectively
learned dictionaries. Unsurprisingly, these are qualitatively similar to the well-known results from
solving dictionary learning using canonical numerical techniques.
D.2 Image denoising using learned dictionaries
Here we further demonstrate the applicability of the dictionary learned by our dynamical neural
networks. We use the dictionary learned from Dataset A (the Lena image) for a denoising task using
a simple procedure similar to Elad & Aharon (2006): First we extract 8 × 8 overlapping patches
from the noisy 512 × 512 Lena image generated with Gaussian noise. We then solve for the sparse
coefficients of each patch in the non-negative sparse coding problem. Using the sparse coefficients,
we can reconstruct the denoised patches, and a denoised image can be obtained by properly aligning
and averaging these patches. On average, each patch is represented by only 5.9 non-zero sparse
coefficients. Figure 7 shows a comparison between the noisy and the denoised image.
19
Published as a conference paper at ICLR 2019
(a) Noisy image (PSNR=18.69dB)
Figure 7: Image denoising using learned dictionary.
(b) Denoised image (PSNR=29.31dB)
E Relationships between continuous and spiking neuron model
FOR SPARSE CODING
Although in this work we focus our discussions and analysis on spiking neurons, the learning strategy
and mechanism can be applied to networks with continuous-valued neurons. The close relationships
between using spiking and continuous-valued neurons to solve sparse approximation problems has
been discussed by Shapero et al. (2014); Tang et al. (2017). Here we attempt to provide an informal
discussion on the connections between the two neuron models.
Following the derivation in Section 3, the dynamics of the spiking networks can be described using
the average current and spike rates.
u(t) = β - u(t) + Wa(t) + (μ(0) - u(t))/t	(28)
where u(t) and a(t) can be related by Theorem 1 as an “activation function”.
a(t) = Θ-1 max(u(t), 0) + ∆(t),	∆(t) → 0	(29)
Equation 28 and 29 are closedly related to the dynamics of a network of continuous-valued neuron
Rozell et al. (2008).
Uc(t) = βc - Uc(t) + Wac(t)	(30)
ac(t) = max(uc(t), 0)	(31)
where uc(t) is the internal state variable of each neuron, ac(t) is the continuous activation value of
each neuron, βc is the input to each neuron, and W is the connection weight between neurons. One
can immediately see the similarity. Note that although such “ReLU” type, asymmetric activation
function was not discussed in Rozell et al. (2008), it was later shown in Tang (2016) that this network
dynamics can solve a non-negative sparse coding problem.
20