Published as a conference paper at ICLR 2019
NADPEx: An on-policy temporally consistent
EXPLORATION METHOD FOR DEEP REINFORCEMENT
LEARNING
Sirui Xie, Junning Huang, Chunxiao Liu, Lanxin Lei, Zheng Ma, Wei Zhang, Liang Lin
SenseTime
xiesirui@sensetime.com
{huangjunning, liuchunxiao, mazheng, wayne.zhang}@sensetime.com
linliang@ieee.org
Ab stract
Reinforcement learning agents need exploratory behaviors to escape from local
optima. These behaviors may include both immediate dithering perturbation and
temporally consistent exploration. To achieve these, a stochastic policy model that
is inherently consistent through a period of time is in desire, especially for tasks
with either sparse rewards or long term information. In this work, we introduce
a novel on-policy temporally consistent exploration strategy - Neural Adaptive
Dropout Policy Exploration (NADPEx) - for deep reinforcement learning agents.
Modeled as a global random variable for conditional distribution, dropout is incor-
porated to reinforcement learning policies, equipping them with inherent tempo-
ral consistency, even when the reward signals are sparse. Two factors, gradients’
alignment with the objective and KL constraint in policy space, are discussed to
guarantee NADPEx policy’s stable improvement. Our experiments demonstrate
that NADPEx solves tasks with sparse reward while naive exploration and param-
eter noise fail. It yields as well or even faster convergence in the standard mujoco
benchmark for continuous control.
1	Introduction
Exploration remains a challenge in reinforcement learning, in spite of its recent successes in robotic
manipulation and locomotion (Schulman et al., 2015b; Mnih et al., 2016; Duan et al., 2016; Schul-
man et al., 2017b). Most reinforcement learning algorithms explore with stochasticity in stepwise
action space and suffer from low learning efficiency in environments with sparse rewards (Florensa
et al., 2017) or long information chain (Osband et al., 2017). In these environments, temporally
consistent exploration is needed to acquire useful learning signals. Though in off-policy methods,
autocorrelated noises (Lillicrap et al., 2015) or separate samplers (Xu et al., 2018) could be designed
for consistent exploration, on-policy exploration strategies tend to be learnt alongside with the opti-
mal policy, which is non-trivial. A complementary objective term should be constructed because the
purpose of exploration to optimize informational value of possible trajectories (Osband et al., 2016)
is not contained directly in the reward of underlying Markov Decision Process (MDP). This com-
plementary objective is known as reward shaping e.g. optimistic towards uncertainty heuristic and
intrinsic rewards. However, most of them require complex additional structures and strong human
priors when state and action spaces are intractable, and introduce unadjustable bias in end-to-end
learning (Bellemare et al., 2016; Ostrovski et al., 2017; Tang et al., 2017; Fu et al., 2017; Houthooft
et al., 2016; Pathak et al., 2017).
It would not be necessary though, to teach agents to explore consistently through reward shaping,
if the policy model inherits it as a prior. This inspires ones to disentangle policy stochasticity, with
a structure of time scales. One possible solution is policy parameter perturbation in a large time
scale. Though previous attempts were restricted to linear function approximators (Ruckstieβ et al.,
2008; Osband et al., 2014), progress has been made with neural networks, through either network
section duplication (Osband et al., 2016) or adaptive-scale parameter noise injection (Plappert et al.,
2017; Fortunato et al., 2017). However, in Osband et al. (2016) the episode-wise stochasticity is
1
Published as a conference paper at ICLR 2019
unadjustable, and the duplicated modules do not cooperate with each other. Directly optimizing the
mean of distribution of all parameters, Plappert et al. (2017) adjusts the stochasticity to the learning
progress heristically. Besides, as all sampled parameters need to be stored in on-policy exploration,
it is not directly salable to large neural networks, where the trade-off between large memory for
multiple networks and high variance with sinlge network is non-trivial to make.
This work proposes Neural Adaptive Dropout Policy Exploration (NADPEx) as a simple, scalable
and improvement-guaranteed method for on-policy temporally consistent exploration, which gen-
eralizes Osband et al. (2016) and Plappert et al. (2017). Policy stochasticity is disentangled into
two time scales, step-wise action noise and episode-wise stochasticity modeled with a distribution
of subnetworks. With one single subnetwork in an episode, it achieves inherent temporal consis-
tency with only a little computational overhead and no crafted reward, even in environments with
sparse rewards. And with a set of subnetworks in one sampling batch, agents experience diverse
behavioral patterns. This sampling of subnetworks is executed through dropout (Hinton et al., 2012;
Srivastava et al., 2014), which encourages composability of constituents and facilitates the emer-
gence of diverse maneuvers. As dropout could be excerted on connections, neurons, modules and
paths, NADPEx naturally extends to neural networks with various sizes, exploiting modularity at
various levels. To align separately parametrized stochasticity to each other, this sampling is made
differentiable for all possible dropout candidates. We further discuss the effect of a first-order KL
regularizer on dropout policies to improve stability and guarantee policy improvement. Our exper-
iments demonstrate that NAPDEx solves challenging exploration tasks with sparse rewards while
achieving as efficient or even faster convergence in the standard mujoco benchmark for state-of-the-
art PPO agents, which we believe can be generalized to any deep reinforcement learning agents.
Figure 1: (a) Conceptual graph to illustrate the hierarchical stochasticity in NADPEx. Agent spon-
taneously explores towards different directions with different dropouts. (b) Graphical model for an
MDP with NADPEx. z is a global random variable in one episode τ .
2	Preliminaries
We consider a standard discrete-time finite-horizon discounted Markov Decision Process (MDP)
setting for reinforcement learning, represented by a tuple M = (S, A, P, r, ρ0, γ, T), with a state
set S, an action set A, a transitional probability distribution P : S × A × S → R+, a bounded reward
function r : S × A → R+, an initial state distribution ρ0 : S → R+, a discount factor γ ∈ [0, 1] and
horizon T. We denote a policy parametrized by θ as πθ : S×A → R+. The optimization objective of
T
this policy is to maximize the expected discounted return η(πθ) = Eτ[ t=0 γtr(st, at)], where τ =
(so,。。,...) denotes the whole trajectory, so 〜ρ0(s0), at 〜∏θ (at∣st) and s,+ι 〜P (s,+ι ∣st,at). In
T
experimental evaluation, we follow normal setting and use undiscounted return Eτ [ t=o r(st, at)].
2.1	Proximal Policy Optimization (PPO)
Theoretically speaking, NADPEx works with any policy gradient based algorithms. In this work,
we consider the recent advance in on-policy policy gradient method, proximal policy optimization
(PPO) (Schulman et al., 2017b).
2
Published as a conference paper at ICLR 2019
Policy gradient methods were first proposed by Williams (1992) in REINFORCE algorithm to max-
imize the aforementioned objective in a gradient ascent manner. The gradient of an expectation
Vθη(∏θ) is approximated with a Monte Carlo average of policy gradient:
1 NT
vθη(πθ) ≈ nt XX vθlogπθ(at|st)(Ri- bi),	⑴
i=1 t=o
where N is the number of episodes in this batch, Rit = PtT0=t γt0-trti0 and bit is a baseline for
variance reduction.
Schulman et al. (2015b) proposes a policy iteration algorithm and proves its monotonicity in im-
provement. With the πθ ’s corresponding discounted state-visitation frequency ρθ , the sampling
policy πθold and the updated policy πθ, it solves the following constrained optimization problem
with a line search for an appropriate step size within certain bound δKL, called trust region:
argmaxθ
[πθ(a|s)	A…屋〃)]
ρθold,a~πθold [πθoid (a∣s)Aθ	(s，a)]
(2)
S.%.si^^r'j P∣dold [DκL(∏0ld(∙∣s)∣∏θ(∙∣s))] ≤ δκL
where DKl(∙∣∣∙) is the KUllback-Leibler (KL) divergence, Aθoid (s, a) is calculated with Generalized
Advantage Estimation (GAE) (Schulman et al., 2015c).
Proximal Policy Optimization (PPO) transforms (2) to a unconstrained optimization and solves it
with first-order derivatives, embracing established stochastic gradient-based optimizer like Adam
(Kingma & Ba, 2014). Noting that DKL(∏θld(∙∣s)∣∏θ(∙∣s)) is actually a relaxation of total Varia-
tional divergence according to Schulman et al. (2015b), whose first-order derivative is 0 when θ is
close to θold, Dkl(∏θ (∙∣s)∣∣∏θld(∙∣s)) is a natural replacement. Combining the first-order derivative
of Dkl(∏θ(∙∣s)∣∣∏Old(∙∣s)) (Schulman et al., 2017a) (check Appendix A for a proof):
VθDkl(∏θ(∙∣s)∣∏θoid(∙∣s))] = Ea〜∏θ[Vθlog∏θ(a∣s)(log∏θ(a|s) - log∏e°id(a|s))],	(3)
PPO optimizes the following unconstrained objective, called KL PPO loss1:
LKL = Es^ρθoid ,a 〜∏θoid[rθ (s,a)Aθoid (s,a) — 2(log ∏θ (a|s) — log ∏θoid (a|s))2],	(4)
where r®(s, a) = ∏πθ(，：；). Schulman et al. (2017b) also proposes a clipping version PPO, as a
lower bound to (4):
Lclip = Es^Pθθid ,a 〜∏θoid[min(rθ (s, a), clip(rθ (s, a), 1 - 3 1 + e))A®oid (s, a)]	(5)
In this work, we try both KL PPO and clipping PPO.
2.2 Dropout
Dropout is a technique used in deep learning to prevent features from co-adaptation and parameters
from overfitting, by randomly dropping some hidden neuron units in each round of feed-forward
and back-propagation (Hinton et al., 2012; Srivastava et al., 2014). This is modeled by multiplying
a Bernoulli random variable zjk to each hidden unit, i.e. neuron activation hjk, for j = 1...m, where
m is the number of hidden units in kth layer. Then the neuron activation of the k + 1th layer is
hk+1 = σ(W(k+1)T Dzkhk + b(k+1)),	(6)
where Dz = diag(z) ∈ Rm×m, W(k+1) and b(k+1) are weights and biases at k + 1th layer
respectively and we simply denote them with θ =. < W, b >. σ(x) is the nonlinear neuron activation
function. The parameter of this Bernoulli random variable is pjk =. P(zjk = 0), aka the dropout rate
of the jth hidden unit at kth layer. In supervised learning, these dropout parameters are normally
fixed during training in some successful practice (Hinton et al., 2012; Wan et al., 2013). And there
are some variants to dropout connections, modules or paths (Wan et al., 2013).
1Though the concrete form is not provided in Schulman et al. (2017b), it is given in Dhariwal et al. (2017)
publicly released by the authors
3
Published as a conference paper at ICLR 2019
3 Methodology
3.1 Neural Adaptive Dropout Policy Exploration (NADPEx)
Designing an exploration strategy is to introduce a kind of stochasticity during reinforcement learn-
ing agents’ interaction with the environment to help them get rid of some local optima. While action
space noise (parametrized as a stochastic policy πθ) might be sufficient in environments where step-
wise rewards are provided, they have limited effectiveness in more complex environments (Florensa
et al., 2017; Plappert et al., 2017). As their complement, an exploration strategy would be especially
beneficial if it can help either sparse reward signals (Florensa et al., 2017) or significant long-term
information (Osband et al., 2016) to be acquired and back-propagated through time (BPTT) (Sutton
& Barto, 1998). This motivates us to introduce a hierarchy of stochasticity and capture temporal
consistency with a separate parametrization.
Our algorithm, Neural Adaptive Dropout Policy Exploration (NADPEx), models stochasticity at
large time scales with a distribution of plausible subnetworks. For each episode, one specific subnet-
works is drawn from this distribution. Inspected from a large time scale, this encourages exploration
towards different directions among different episodes in one batch of sampling. And from a small
time scale, temporal correlation is enforced for consistent exploration. Policies with a hierarchy
of stochasticity is believed to represent a complex action distribution and larger-scale behavioral
patterns (Florensa et al., 2017).
We achieve the drawing of subnetwork through dropout. As introduced in Section 2.2, originally,
dropout is modeled through multiplying a binary random variable Zj 〜BernouUi(1 - Pj) to each
neuron activation hjk. In Srivastava et al. (2014), this binary dropout is softened as continuous ran-
dom variable dropout, modeled with a Gaussian random variable Z 〜N(L σ2), normally referred
to as Gaussian multiplicative dropout. Here we denote both distributions with qφ(z). Later we will
introduce how both of them could be made differentibale and thus adaptative to learning progress.
During the sampling process, at the beginning of every episode τi, a vector of dropout random vari-
ables Zi is sampled from qφ(z), giving us a dropout policy ∏θ∣zi for step-wise action distribution.
zi is fed to the stochastic computation graph (Schulman et al., 2015a) for the whole episode until
termination, when a new round of drawing initiates. Similar as observations, actions and rewards,
this random variable is stored as sampled data, which will be fed back to the stochastic computation
graph during training. Policies with this hierarchy of stochasticity, i.e. NADPEx policies, can be
represented as a joint distribution:
∏θ,φ(∙, Z) = qφ(z)∏θ∣z(∙),	(7)
making the the objective:
T
η(πθ,φ) = EZ〜q(z)[Eτ|z[£ Ytr(St,at)]].	⑻
t=0
Ifwe only use the stochasticity of network architecture as a bootstrap, as in BootstrapDQN (Osband
et al., 2016), the bootstrapped policy gradient training is to update the network parameters θ with
the following gradients:
1N
Vθη(πθ,φ) = VeEz〜qφ(z)[η(πθ∣z)] = EZ〜qφ(z)[Veη(πe∣z)] ≈ N £VeL(θ, Zi)	⑼
i=1
L(θ, Zi) is the surrogate loss of dropout policy πθ,zi, variates according to the specific type of
reinforcement learning algorithm. In next subsection, we discuss some pitfalls of bootstrap and
provide and our solution to it.
3.2 Training NADPEx
Stochasticity Alignment
One concern in separating the parametrization of stochasticity in different levels is whether they
can adapt to each other elegantly to guarantee policy improvement in terms of objective (8). Policy
4
Published as a conference paper at ICLR 2019
gradients in (9) alway reduces ∏θ∣z's entropy (i.e. Stochasticity at small time scale) as the policy
improves, which corresponds to the increase in agent’s certainty. But this information is not prop-
agated to qφ for stochasticity at large time scale in bootstrap. As in this work qφ is a distribution
of subnetworks, this concern could also be intuitively understood as component space composition
may not guarantee the performance improvement in the resulting policy space, due to the complex
non-linearity of reward function. Gangwani & Peng (2017) observe that a naive crossover in the
parameter space is expected to yield a low-performance composition, for which an extra policy
distillation stage is designed.
We investigate likelihood ratio trick (Ranganath et al., 2014; Schulman et al., 2015a) for gradient
back-propragation from the same objective (8) through discrete distribution e.g. binary dropout and
reparametrization trick (Kingma & Welling, 2013; Rezende et al., 2014) for continuous distribution
e.g. Gaussian multiplicative dropout, thus covering all possible dropout candidates (The proof is
provided in Appendix B):
1N
Vθ,φη(∏θ,φ) ≈ N E(VθL(θ, Zi) + Vφ log qψ(zt)A(s0, a0)),	(10)
N i=1
1N
vθ,Φη(πθ,Φ) ≈ N ɪ2 vθ,ΦL(θ,I+ φ ® W).	(II)
i=1
In (10) φ is the parameters for Bernoulli distributions, A(si0, ai0) is the GAE (Schulman et al., 2015c)
for the ith trajectory from the beginning, i.e. (si0, ai0). In (11) φ = σ thus z = I + φ Wi, is an
element-wise multiplication, and Wi is the dummy random variable ei 〜 N(0, I).
Policy space constraint
However, simply making the dropout ditribution differentiable may not guarantee the policy impro-
ment. As introduced in Section 2.1, Schulman et al. (2015b) reduce the monotonic policy improve-
ment theory to a contraint on KL divergence in policy space. In this work, the analytical form of
NADPEx policy πθ,Φ is obtainable as qΦ is designed to be fully factorizable. Thus ones can leverage
the full-fledged KL constraint to stablize the training of NADPEx.
Here we give an example of PPO-NADPEx. Similar as (3), we leverage a first-order approximated
KL divergence for NADPEx policies. As credit assignment with likelihood ratio trick in (3) may
suffer from curse of dimensionality for qΦ(z), we stop gradients w.r.t. φ from the KL divergence
to reduce variance with a little bias. We further replace πθ,Φ with a representative, the mean policy
∏θ = ∏θ∣z, where Z is the mean of dropout random variable vectors. Then the objective is:
LKDPEX = E[rθ∣z(s, a)Aθoid(s,a) - 2(log∏θ(a|s) - log∏θ0id∣z(a∣s))2]	(12)
where rθ∣z(s, a)= n"：1：，?：]). A proof for the second term is in Appendix C. Intuitively, KL
divergence between dropout polices ∏θ∣z and mean policies ∏θ is added to remedy the omission of
VΦDKL. Optimizing the lower bound of (12), clipping PPO could adjust the clip ratio accordingly.
3.3 Relations to Parameter Noise
Episode-wise parameter noise injection (Plappert et al., 2017) is another way to introduce the hi-
erarchy of stochasticity, which could be regarded as a special case of NADPEx, with Gaussian
mulitplicative dropout on connection and a heurstic adaptation of variance. That dropout at neurons
is a local reparametrization of noisy networks is proved in Kingma et al. (2015). A replication of the
proof is provided in Appendix D. They also prove this local reparametrization trick has a lower vari-
ance in gradients. And this reduction in variance could be enhanced if ones choose to drop modules
or paths in NADPEx.
More importantly, as we scope our discussion down to on-policy exploration strategy, where z from
qΦ(z) need to be stored for training, NADPEx is more scalable to much larger neural networks
with possibly larger mini-batch size for stochastic gradient-based optimizers. As a base case, to
dropout neurons rather than connections, NADPEx has a complexity of O(N m) for both sampling
and memory, comparing to O(N m2) of parameter noise, with m denoting the number of neurons in
one layer. This reduction could also be enhanced if ones choose to drop modules or paths.
5
Published as a conference paper at ICLR 2019
4 Experiments
In this section we evaluate NADPEx by answering the following questions through experiments.
(i)	Can NADPEx achieve state-of-the-art result in standard benchmarks?
(ii)	Does NADPEx drive temporally consistent exploration in sparse reward environments?
(iii)	Is the end-to-end training of the hierarchical stochasticity effective?
(iv)	Is KL regularization effective in preventing divergence between dropout policies and the
mean policy?
We developed NADPEx based on openai/baselines (Dhariwal et al., 2017). We run all exper-
iments with PPO as reinforcement learning algorithm. Especially, we developed NADAPEx based
on the GPU optimized version PPO. Details about network architecture and other hyper-parameters
are available in Appendix E. During our implemented parameter noise in the same framework as
NADPEx, we encountered out of memory error for a mini-batch size 64 and training batch size
2048. This proves our claim of sampling/memory complexity reduction with NADPEx. We had to
make some trade-off between GPU parallelism and more iterations to survive.
For all experiment with NADPEx, we test the following configurations for the dropout hyper-
parameter, the initial dropout rate: (a) pjk = 0.01, (b) pjk = 0.1, (c) pjk = 0.3. For Gaussiasn
dropout, we estimate the dropout rate as pjk = σjk /(1 + σjk). And for experiments with parameter
noise, we set the initial variance accordingly: (a) σjk = 0.001, (b) σjk = 0.01, (c) σjk = 0.05,
to ensure same level of stochasticity. For experiments with fixed KL version PPO, we run with
β = 0.0005. All figures below show statistics of experiments with 5 randomly generated seeds.
4.1	NADPEx in standard environments
First, we evaluate NADPEx’s overall performance in standard environments on the continuous con-
trol environments implemented in OpenAI Gym (Brockman et al., 2016). From Figure 2 we can see
that they show similar pattern with Gaussian dropout and binary dropout, given identical dropout
rates. Between them, Gaussian dropout is slightly better at scores and consistency among initial
dropout rates. With the three initial dropout rates listed above, we find pjk = 0.1 shows constistent
advantage over others. On the one hand, when the initial dropout rate is small (pjk = 0.01), NADPEx
agents learn to earn reward faster than agents with only action space noise. It is even possible that
these agents learn faster than ones with pjk = 0.1 in the beginning. However, their higher variance
between random seeds indicates that some of them are not exploring as efficiently and the NADPEx
policies may not be optimal, therefore normally they will be surpassed by ones with pik = 0.1 in the
middle stage of training. On the other hand, large initial dropout rate (pjk = 0.3) tends to converge
slowly, possibly due to the claim in Molchanov et al. (2017) that a large dropout rate could induce
large varaince in gradients, making a stable convergence more difficult.
Figure 2: Experiments with NADPEx in standard envs, three pjk are presented for Gaussian dropout,
as well as the best of binary dropout. Extensive comparison is given in Appendix F.
6
Published as a conference paper at ICLR 2019
4.2	Temporally consistent exploration
We then evaluate how NADPEx with Gaussian dropout performs in environments where reward sig-
nals are sparse. Comparing with environments with stepwise rewards, these environments are more
challenging as they only yield non-zero reward signals after milestone states are reached. Tempo-
rally consistent exploration therefore plays a crucial role in these environments. As in Plappert et al.
(2017) we run experiments in rllab (Duan et al., 2016), modified according to Houthooft et al.
(2016). Specifically, we have: (a) SparseDoublePendulum, which only yields a reward if the agent
reaches the upright position, and (b) SparseHalfCheetah, which only yields a reward if the agent
crosses a target distance, (c) SparseMountainCar, which only yields a reward if the agent drives up
the hill. We use the same time horizon of T = 500 steps before resetting and gradually increment the
difficulty during repetition until the performance of NADPEx and parameter noise differentiates. We
would like to refer readers to Appendix G for more details about the sparse reward environments.
As shown in Figure 3, we witness success with temporally consistent exploration through NAD-
PEx, while action perturbation fails. In all enviroments we examed, larger initial dropout rates can
achieve faster or better convergence, revealing a stronger exploration capability at large time scales.
Comparing to parameter noise, NADPEx earns higher score in shorter time, possibly indicating a
higher exploration efficiency.
HaIfCheetah
200	400	600	800	1000
nbatch
----Gaussian p=0.01
----Gaussian p=0.1
Oooo
4 3 2 1
φ⊃-ω>
----Gaussian p=0.3	---- pn σ = 0.01	---- ppoclipped
---- pn σ = 0.001	---- pn σ = 0.05
Figure 3: Experiments with NADPEx and parameter noise in sparse reward envs.
DoubIePenduIum
MountainCar
4.3	Effectiveness of stochasticity adaptation
One hypothesis NADPEx builds upon is end-to-end training of separately parametrized stochasticity
can appropriately adapt the temporally-extended exploration with the current learning progress. In
Figure 4 we show our comparison between NADPEx and bootstrap, as introduced in Section 3. And
we can see that though the difference is subtle in simple task like Hopper, the advantage NADPEx
has over bootstrap is obvious in Walker2d, which is a task with higher complexity. In Humanoid,
the task with highest dimension of actions, the empirical result is interesting. Though bootstrap
policy learns almost as fast as NADPEx in the begining, that the dropout is not adaptative drives it to
over-explore when some dropout policies are close to converge. Being trapped at a local optimum,
bootstrap policy earns 500 scores less than NADPEx policy.
Oooo
Oooo
5 0 5 0
2 2 11
① n-B>
200	400	600	800	1000
nbatch
----Gaussian p=0.1	---- Bootstrap Gaussian p=0.1	---- ppoclipped
Figure 4: Comparison between NADPEx and bootstrap with Gaussian dropout.
200	400	600	800	1000
nbatch
7
Published as a conference paper at ICLR 2019
4.4	KL divergence between dropout policies
To evaluate the effect of the KL regularizer, we also run experiments with KL PPO. Though in the
original paper of Schulman et al. (2017b) clipping PPO empirically performs better than KL PPO,
we believe including this KL term explicitly in the objective makes our validation self-explanatory.
Figure 5 left shows the experiment result in Walker2d. Different from clipping PPO, NADPEx with
small initial dropout rate performs best, earning much higher score than action noise. As shown in
Figure 5 right, the KL divergence between dropout polices and mean policies is bounded.
Figure 5: NADPEX KL PPO in Walker2d. Left: learning curves; right: true DKL(πθold∣z∣∣πθ∣z).
5	Related works
On-policy reinforcement learning methods have gained attention in recent years (Schulman et al.,
2015b; Mnih et al., 2016; Schulman et al., 2017b), mainly due to their elegance with theoretical
grounding and stability in policy iteration (Henderson et al., 2017b). Despite of the effectiveness, to
improve their data efficiency remains an active research topic.
In this work, we consider the eXploration strategies for on-policy reinforcement learning methods.
Most of the aforementioned works employ naive eXploration, with stochasticity only in action space.
However, they fail to tackle some tasks with either sparse rewards (Florensa et al., 2017) or longterm
information (Osband et al., 2016), where temporally consistent eXploration is needed.
One solution to this challenge is to shape the reward to encourage more directed eXploration. The
specific direction has various foundations, including but not restricted to state visitation count
(Jaksch et al., 2010; Tang et al., 2017), state density (Bellemare et al., 2016; Ostrovski et al., 2017;
Fu et al., 2017), self-supervised prediction error (Pathak et al., 2017) etc. Some of them share the
Probably ApproXimately Correct (PAC) with discrete and tractable state space (Jaksch et al., 2010).
But when state space and action space are intractable, all of them need additional computational
structures, which take non-trivial efforts to implement and non-negligible resources to eXecute.
Orthogonal to them, methods involving a hierarchy of stochasticity are proposed. Based on hier-
archical reinforcement learning, Florensa et al. (2017) models the stochasticity at large time scales
with a random variable - option - and model low-level policies with Stochastic Neural Networks.
However, authors employ human designed proXy reward and staged training. Almost concurrently,
Osband et al. (2016) and Plappert et al. (2017); Fortunato et al. (2017) propose network section
ensemble and parameter noise injection respetively to disentangle stochasticity. Under the banner
of Stochastic Neural Networks, NADPEX generalize them all. BootstrapDQN is a special case of
NADPEX without stochasticity adapation and parameter noise is a special case with high variance
and compleXlity, as well as some heursitic approXimation. Details are discussed in Section 3.
In NADPEX, this stochasticity at large time scales is captured with a distribution of plausible neural
subnetworks from the same complete network. We achieve this through dropout (Srivastava et al.,
2014). In spite of its success in supervised deep learning literature and Bayesian deep learning
literature, it is the first time to combine dropout to reinforcement learning policies for eXploration.
The closest ones are Gal & Ghahramani (2016); Henderson et al. (2017a), which use dropout in value
network to capture agents’ uncertainty about the environment. According to Osband et al. (2016),
Gal & Ghahramani (2016) even fails in environment requiring temporally consistent eXploration.
8
Published as a conference paper at ICLR 2019
There are also some attempts from the Evolutionary Strategies and Genetic Algorithms literature
(Salimans et al., 2017; Such et al., 2017; Gangwani & Peng, 2017) to continuous control tasks.
Though they model the problem much differently from ours, the relation could be an interesting
topic for future research.
6	Conclusion
Building a hierarchy of stochasticity for reinforcement learning policy is the first step towards more
structured exploration. We presented a method, NADPEx, that models stochasticity at large time
scale with a distribution of plausible subnetworks from the same complete network to achieve on-
policy temporally consistent exploration. These subnetworks are sampled through dropout at the
beginning of episodes, used to explore the environment with diverse and consistent behavioral pat-
terns and updated through simultaneous gradient back-propagation. A learning objective is provided
such that this distribution is also updated in an end-to-end manner to adapt to the action-space pol-
icy. Thanks to the fact that this dropout transformation is differentiable, KL regularizers on policy
space can help to further stabilize it. Our experiments exhibit that NADPEx successfully solves
continuous control tasks, even with strong sparsity in rewards.
References
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, pp.1471-1479, 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/
openai/baselines, 2017.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning,
pp. 1329-1338, 2016.
Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical rein-
forcement learning. arXiv preprint arXiv:1704.03012, 2017.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
arXiv preprint arXiv:1706.10295, 2017.
Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2574-2584,
2017.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059,
2016.
Tanmay Gangwani and Jian Peng. Policy optimization by genetic distillation. arXiv preprint
arXiv:1711.01012, 2017.
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation
for stochastic neural networks. ICLR, 2016.
Peter Henderson, Thang Doan, Riashat Islam, and David Meger. Bayesian policy gradients via alpha
divergence dropout inference. arXiv preprint arXiv:1712.02037, 2017a.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017b.
9
Published as a conference paper at ICLR 2019
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems ,pp.1109-1117, 2016.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparame-
terization trick. In Advances in Neural Information Processing Systems, pp. 2575-2583, 2015.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural
networks. arXiv preprint arXiv:1701.05369, 2017.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized
value functions. arXiv preprint arXiv:1402.0635, 2014.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in neural information processing systems, pp. 4026-4034, 2016.
Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized
value functions. arXiv preprint arXiv:1703.07608, 2017.
Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Remi Munos. Count-based explo-
ration with neural density models. arXiv preprint arXiv:1703.01310, 2017.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning (ICML), volume
2017, 2017.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
arXiv preprint arXiv:1706.01905, 2017.
Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning
binary stochastic feedforward neural networks. ICLR, 2015.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial
Intelligence and Statistics, pp. 814-822, 2014.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
10
Published as a conference paper at ICLR 2019
Thomas Ruckstieβ, Martin Felder, and Jurgen Schmidhuber. State-dependent exploration for policy
gradient methods. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases,pp. 234-249. Springer, 2008.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using
stochastic computation graphs. In Advances in Neural Information Processing Systems, pp. 3528-
3536, 2015a.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015b.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015c.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-
learning. arXiv preprint arXiv:1704.06440, 2017a.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and
Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training
deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT
press Cambridge, 1998.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schul-
man, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for
deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2750-
2759, 2017.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International Conference on Machine Learning, pp. 1058-1066,
2013.
Sida Wang and Christopher Manning. Fast dropout training. In international conference on machine
learning, pp. 118-126, 2013.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint
arXiv:1611.01224, 2016.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. In Reinforcement Learning, pp. 5-32. Springer, 1992.
Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to explore via meta-policy gradient.
In International Conference on Machine Learning, pp. 5459-5468, 2018.
11
Published as a conference paper at ICLR 2019
A KL divergence first order approximation
We derive the first order derivatitve of KL divergence from a refernce policy ∏ to a parametrized
policy πθ to :
Vθ Dkl (∏θ (∙∣s)∣∣∏(∙∣s))
Vθ
J ∏θ(a∣s)(log ∏θ(a|s) — log∏(a∣s))da
/ Vθ(∏θ(a∣s)(log ∏θ(a|s) — log∏(a∣s)))da
J Vθ∏θ(a∣s)(log∏θ(a|s) — log∏(a∣s) + 1)da
(13)
J ∏θ(a∣s)Vθ log ∏θ(a∣s)(log ∏θ(a|s) — log∏(a∣s) + 1)da
Ea〜∏θ [Vθ log ∏θ(a∣s)(log ∏θ(a|s) — log∏(a∣s))]
Note that line 3 derives line 4 with likelihood ratio trick, line 4 derives line 5 as
J ∏θ(a∣s)Vθ log ∏θ(a∣s)da = 0
B Gradients of NADPEx
Here we provide a full derivation of NADPEx’s gradients. Specifically, gradients for two types of
dropout are discussed.
As shown in (7), a NADPEx policy πθ,z is a joint distribution, which could be factorized with a
dropout distribution qφ(z) and a conditional distribution, i.e. the dropout policy, ∏θ∣z, Z 〜qφ(z) is
the dropout random variable vector.
In reinforcement learning with NADPEx, we use gradient based optimization to maximize the ob-
jective (8).
T
vη(πθ,φ) = VEZ〜q(z)[Eτ|z [X Ytr(st,at州.	(I4)
t=0
B.1 Discrete dropout
Normally the dropout distribution qφ(z) is a discrete distribution, for example, Z 〜BernOulli(φ),
ones can use likelihood ratio trick to calculate (14):
T
V θ,φη(πθ,φ) = vθ,φEz~qφ [Eτ∣zXγtr(st, at)]]
t=0
=Vθ,φ / qφ(z) /Pτ∣z,θRdτdz
=/ qφ(z)Vθ /PT∣z,θRTdτdz + Vφ / qφ(Z) /PT∣z,θRTdTdz
=/ qφ(z)Vθ /PT∣z,θRTdτdz + Vφ / qφ(z)Eτ∣z[R(so∣z, ao∣z)]dz
=EZ〜qφ [VθL(θ, z) + Vφ log qφ(z)A(so∣z, ao∣z)]
1N
≈ nn E(VθL(θ, zi) + Vφ log qφ(zi)A(s0, a0)),
(15)
where L(∙) is the surrogate loss, varying from reinforcement learning algorihtms, A(s0, a0) is the
GAE (Schulman et al., 2015c) for the ith trajectory from the beginning i.e. (si0, ai0), such that the
gradient has low variance.
12
Published as a conference paper at ICLR 2019
B.2 Gaussian multiplicative dropout
In Srivastava et al. (2014), Gaussian multiplicative dropout is proposed, where Z 〜 N (I, σ2) is
the multiplicative dropout random variable vector, with the same expectation of discrete dropout
P(Z = 1) = ι+φ. Reparametirzed with a dummy random variable W 〜N(0, I) (Kingma &
Welling, 2013; Rezende et al., 2014), we have z = I + φ , where φ is used to denote σ for
consistency of notation, is an element-wise multiplication, such that (14) could be calculated as:
T
Vθ,φη(πθ,φ) = Ve,φEz 〜N (i,φ2 )[Eτ |z[£ Ytr(St,at)]]
t=0
=vθ,φEz〜N(I,φ2) [η(πθ∣z)]
=vθ,φEe 〜N (0,I)[η(πθ∣I+φΘe)]
=Ee〜N(0,I) [vθ,φη(πθ∣I+φΘ∈)]
=EgN(o,i)[Vθ,φL(θ,I + φ Θ e)]
1N
≈ nn ∑vθ,φ L(θ,I+φΘWi)
N i=1
(16)
C NADPEx KL divergence first order approximation
In NADPEx, the dropout distribution is designed to be fully factorizable, thus the gradient of KL
divergence in the NADPEx policy space is obtainable:
vθ,φdkl5θ,φ(Is)Ilπθold,φold (Is))
=vθ,φ	πθ,φ(a, z|s) log
πθ,φ(α,Z⑸、dadz
πθold,φold (a, zIs)
=vθ,φ! J qφ(z)πθιz (ais) log qφqφ (Z);二(a、dadz
(17)
/ qφ(zRθ / ∏θ∣z(a∣s)log
qφ(z)∏θ∣z (a|s)
qφold(Z)πθold |z(aIs)
dadZ
+ vφ / qφ(z) / πθlz (a|s) log qφqφ(z)nθθz;(a)∣s) dadz.
And thus the same first-order approximation could be done just as in Appendix A. However, note that
likelihood ratio trick is used during the derivation, which should be applied to the random variable
vectors a and Z here.
C.1 Reducing variance in Monte Carlo estimate with MuProp
As normally Z could have a much higher dimension than a, making this likelihood ratio trick suffer
from curse of dimensionality, we inspect into the second term with MuProp (Gu et al., 2016) for a
low-variance estimate. Let
f(Z)=
∏θ∣z (a∣s)log
qφ(z)∏θ∣z(a∣s)	da
qφold(Z)πθold |z (aIs) a
(18)
then we have
gφ = vφ	qφ(Z)f(Z)dZ =	qφvφlogqφ(Z)f(Z)dZ
= Ez [vφ log qφ(Z)f(Z)]
gφ = vφ log qφ(z)f (z) where
(19)
z 〜qφ.
13
Published as a conference paper at ICLR 2019
The idea of MuProp is to use a control variate that corresponds to the first-order Taylor expansion
of f around some fixed value z, i.e. h(z) = f (Z) + f0(z)(z - z), such that
gφM = VφEz [(f(z) - h(z)) + h(z)]
=VφEz[f(z) - f (z) - f0(z)(z - z)] + (f (z) + f0(z)(z - z))]
=Ez [Vφ log qφ(z)[f (z) - h(z)]] + f0(z)VφEz [z]
gφ = Vφlogqφ(z)[f(z)- h(z)] + f0(z)VφEz[z] where Z 〜qφ.
(20)
To further reduce the variance, the first term is sometimes omitted with acceptable biased introduced
(Raiko et al., 2015). And empirically, we find it could be almost negligible comparing with the
gradient from the reinforcement learning objective. For Gaussian dropout, VφEz [z] = Vφ1 = 0,
gφ is thus eliminated. With binary dropout, VφEz [z] = Vφφ = 1, We have
gφ = f 0(z)∣z=Z
d 1
K-log-
∂z
d 1
TT log：
∂z
m-1
X log
i=0
qφ(z)
qφold(z)
l ∂ [	πθ∣z(Hs),
+	/ ∏θ∣z(a∣s)log ——~rrʒda
∂z J	∏θoid∣z(a|s)
Qm-I φzi(I - φi)1-zi	, d /	πθ∣z(Hs)才〜 …、
Qm-I Φoldzi(1 - O")1-"+ ∂zj "θlzJ)g ∏θoid∣z(a∣s) d	(21)
:⅛¾φ3 + ɪ Z ∏θ∣z(a∣s)log	da.
(1 - φi)φold	∂z J	∏θoid∣z(a|s)
Note that in line 3 We assume the probabilistic distribution function to be continuous in an infinites-
imal regions around {z|zi ∈ {0, 1} for ∀zi} to alloW the existence of the first order derivative
for the first term. The first term is then close to zero as φ is close to φold. Another possible relaxation
method is to use Concrete distribution (Maddison et al., 2016):
q (Z)= mY1 λφiZ-λi-1(1-Zi )-λ-1
φ,λ U (φiz-λ+(1-Zi)-λ)2，
where λ is the temperature of this relaxation. Substituting it back to (21), we have
(22)
gM = f 0(z)∣z=z
d ]	qφ,λ(Z)	, d f ( π x1	πθ∣z(a|s),
=log —一L +	∏θ∣z(a∣s)log-----rT^da
∂z	qφoid,λ(z)	∂z J	∏θoid∣z(a∣s)
—d	m-Lj	λφi	.	φiz-λ+(1 -	Zi)	λ ʊ ∣d /	/1 ∖ 1	πθ∣z(Hs)	1
=∂Z ig (IOg 可-g( φoidz-λ + (1-Zi)-λ )) + ∂Z] πθlz(als)log ∏θoid∣z(a∣s)da
=-2X ɪ(log( φiz-λι+ (I-Zi)： )) + ɪ Z ∏θ∣z(a∣s)log πθlz(alsL da
士 ∂ z'g'φoldz-λ + (l-Zi)-λ“	∂zj	θlzL，Foid|z(a|s)
=2λX1(ΦiZ-λ-1 + (1-Zi)-λ-1 - φθldz-λ-1 + (1-Zi)-λ-1)
⅛	ΦiZ-λ + (1-Zi)-λ	φθldz-λ + (1-Zi)-λ
—
l ∂ f	πθ∣z(Hs) T
+	∏θ∣z(a∣s)log------TΓʒda
∂z J	∏θoid∣z(a|s)
(23)
We can reach the same claim as above that the first term could be removed if we set Z = Z → 0.
And the second term could also be eliminated as first order derivative of KL divergence betWeen
two different distributions parametrized with almost but not entirely null networks if Z → 0.
C.2 Relaxing analytical KL divergence with trust region
Ones may argue that the second term in (17) is decomposable to
Vφ / qφ(z)(log qφ(z) - log q@oid (z))dz = PφDκL(qφi(zi )∣∣qφoid (Zi)),	(24)
14
Published as a conference paper at ICLR 2019
Vφ / qφ(z) / ∏θ∣z(a∣s)(log∏θ∣z(a|s) - log∏θoid∣z(a∣s))dadz	(25)
such that (24) has analytical form as q is either diagonal Gaussian or Bernoulli and thus the Monte
Carlo gradient estimate and the MuProp approximation above need only to be exerted on (25). How-
ever, it can be proved that VφDκ L(qφi (zi)∖∖qφoid (Zi)) is still omittable with trust region (Schulman
et al., 2015b). When first proposed in Schulman et al. (2015b), trust region is a relaxation on DKL
to encourage efficient learning, with the idea that step size will only be adapted if the trust region
constraint is violated, i.e. DKL > δ. The adaptation of step size is replaced in Wang et al. (2016)
with a mechanism to clip gradients from surrogate loss adaptively only if DKL > δ to further boost
the efficiency.
We prove below that Dkl will almost never violate the constraint δ = 1 proposed in Wang et al.
(2016). For Gaussian dropout we have:
DKL(qφi(Zi)∖∖qφiold(Zi))	= logQ + _JL__ 1 =log	φold	+ (φold + ∆φ()2 _ 1 φi	2φold2	2	g φOld + ∆φ(	2φold2	2 1	, ∆φ( , ∆φ(2	" ,^∆φi, ɪ ∆φ( , ∆φ(2 TOgr+箭：+ φld + 济=-log(1 + φld) + φld + 济
= - log(1 + x) + x + x2,
(26)
where We let X = ^d.
According to our experiment, as well
as some conclusions in
the literature for dropout (Kingma et al., 2015), φi ∈ (0.005, 0.5).
DKL(qφi(zi)∖∖qφiold(zi)) ∈ (0, 1).
For binary dropout we have:
DKL (qφi (zi)∖∖qφiold (zi))
=φi (log φi - logφiold) + (1 - φi)(log(1 - φi) - log(1 - φiold))
Obviously x
(0, 1),
=[log(1 - φi) - log(1 - φiold)] + φi [log
φi
1 - φi
- log
φiold
1 — φ(OId
(27)
∈
]
= [log(1 - φOld - ∆φi) - log(1 - φOld)] + φi[log I φ* 气
1 - φiold - ∆φi
We can thus discuss how DKL changes with ∆φi and φiold :
∂
∂ ∆φ- DKL(qΦi (zi)∖∖qφold (Zi))
1 φOld
-logτ-φF
].
1
1
1 - φoid - ∆φi+(φold+∆φi)(
ΦOld + ∆φ( + 1 - φold
- ∆φi
φOld + ∆φi
+ (IOg 1 - φold - ∆φi - log
φiold
1 - Φ0ld
= log ""+"i ——log φold	= h(φold + ∆φi) - h(φold).
g 1 - φold - ∆φi	g 1 - φold	`wi + ψ ) iψl )
∂
orDκ DκL(qφi (zi)∖∖qφoid (Zi))
∂φi
r-≠°l1-∆φ"+ T-1φ0d + (φold + △曲乂
Φ0ld + ∆φi + 1 - Φ0ld - ∆φi
+ (φold+∆φi)(焉 + r⅛) + (log	φ0d2 飞也-log rφ⅛)
φiold	1 - φiold	1 - φiold - ∆φi	1 - φiold
—A4/	1 ɪ 1 ʌɪ 1	φOld + ∆φi	1
=-a次(lɪ^ + φold) + log 1 - φold - ∆φi - log
=21! h00(φold)∆φi + 3! h000 (φold)∆φ2 +...
φiold
1 — φold
(28)
(29)
—
—
)
1
1
)
15
Published as a conference paper at ICLR 2019
With h(x) = log ι-xχ ,x ∈ (0.005,0.5), it is easy to see that ∂∆φL = 0 When ∆φi = 0, ∂∆φL > 0
when ∆φi > 0 and ∂∆KL < 0 when ∆φi < 0. Similarly, when φold = 0.5, ∂DKL = 0；
∂∆φi	∂φ
when φold > 0.5, ∂ΦKL > 0; when φold < 0.5, ∂∣⅛ < 0. Hence DKL(qφi(zi)∣∣qφoid(Zi))
reaches its maximum at ∣∆φi∣maχ and ∣φθld - 0.5∣maχ. It is reasonable to set ∣∆φi∣maχ = 0.1,
leading US to DκL(qφi(zi)∣∣qφoid(Zi))max ≈ 0.225 < δ = 1. We reach the same claim that
VφDκL(∏θ,φ(∙∣s)∣∣∏θoZd,φoZd (∙∣s)) could be stopped for the sake of learning efficiency at a cost of
acceptable bias.
C.3 Remedy bias with mean policy
As derived above, the regularization term only back-propagates gradients to θ:
Ne DκL(∏θ,φ(∙∣s)∣∣∏θoRφoid
qφ(z)Vθ / ∏θ∣z(a∣s)log-
qφ (z)Vθ / ∏θ∣z(a∣s)(log
(∙∣s))
qφ(z)πθ∣z(als)
qφold (Z )πθold
∏θ∣z(a∣s)
∏θoid∣z (a|s)
|Z(a|s)
+ log
dadZ
qφ(Z)
qφold(Z)
)dadZ
qφ(z)Vθ / ∏θ∣z(a∣s)(log∏θ∣z(a|s) - log∏θoid∣z(a∣s))dadz
(30)
=Ez~qφ [Vθ Dkl(∏θ∣z (∙∣s)∣∣∏θoid∣z (∙∣s))]
1 N-1
≈ N E vθ DKLg∖Zi (∙ls)llπθoid∣Zi (Is)),
i=0
where N is the number of dropout policies in this batch. Note that from line 3 to line 4 we simply
remove log
qφ(Z)
qφold (Z),
which can be regarded as substracting it as a baseline b(Z) to reduce variance.
Obviously, (30) has similar level of variance as (9), closing our discussion on variance reduction
even though some other techniques could possibly go further.
/
Z
Z
Seeing the lack of regularization on φ due to the gradient omission, we further enforce the idea that
dropout policy had better to be close to each other, which is the supposed role of regularizer on φ.
This could be intuitively understood as a remedy from θ to φ:
1 N-1N-1
N XX vθDKL(ne|zi(Is)llπθold∣Zj(Is)).	(31)
i=0 j=0
Noticing that this term has a complexity of O(N2), we replace ∏e,φ with a mean policy ∏e = ∏e∣z.
In the deep learning literature, it is fairly prevalent to use mean network for dropout training with
some adjustment in gradient back-propagation, due to stability and effiency concern. For instance, it
is proved by Wang & Manning (2013) that the sampling process for dropout network with sigmoid
or ReLU activators could be avoided by integrating a Gaussian approximation at each layer of a
neural network. Therefore, we have
1 N-1
N E vθ DKL(πθ (Is)llπθold∣Zi (Is))
N i=0
1 N-1
N X vθ Z
i=0
∏θ(a∣s)(log∏θ(a|s) - log∏e°idχ(a∣s))da
N NX八
i=0
1 N-1
N X Ea
i=0
1 N-1
N X Ea
i=0
θ(a∣s)Vθ log∏θ(a∣s)(log∏(a|s) - log∏θoid∣zi(a∣s))da
(32)
,〜∏θ∣z[Vθ log∏θ(a∣s)(log∏θ(a|s) - log∏e°idχ(a|s))]
∙~∏θ∣z[1 Vθ(log∏θ(a|s) - log∏θoid∣Zi(a|s))2].
16
Published as a conference paper at ICLR 2019
Empirically, we found little difference between fast dropout back-propagation and normal back-
propagation when (32) is combined into (12).
D	Varitaional dropout and local reparametrization
As introduced in Section 2.1, multiplicative dropout at neuron activation of kth layer can also be
viewed as a multiplicative noise for the input at k + 1th layer:
hk+1 = σ(W(k+1)TDzkhk + b(k+1) ).	(33)
After a simple rearrangement W(k+1)T = W(k+1)TDk, we have:
hk+1 = σ (W(k+I)Thk + b(k+1)).	(34)
That is, the stochastic neuron activation in networks with Gaussian multiplicatice dropout can also
be regarded as stochastic neuron activation in Noisy Networks with correlated Gaussian parameter
noise, whose means equal the corresponding ones in networks with dropout.
E	Hyperparameters
For most of the hyperparamters, we follow the setting in original PPO paper. Though, to emphasis
the scalability of our method, we use 2 parallel enviroments and only 1 minibatch in each epoch.
Table 1: Hyperparamters for PPO
Hyperparameters Value
Horizon	2048
Adam stepsize	3×10
Num. epochs	10
Num. minibatch	1
Discount (γ)	0.99
GAE λ	0.95
PPO clip	0.2
Num. layers	2
Num. hidden units	64
F Experiment results in standard environments
Figure 6: NADPEx in standard envs where Gaussian dropout is used
17
Published as a conference paper at ICLR 2019
Figure 7: NADPEx in standard envs where binary dropout is used
Figure 8: NADPEx in standard envs, camparing the best of Gaussian dropout and binary dropout
G Environments with sparse rewards
We use the same sparse reward environments from rllab Duan et al. (2016), modified by Houthooft
et al. (2016):
•	SparseHalfCheetah (S ⊂ R17, A ⊂ R6), which only yields a reward if the agent crosses a
distance threshold,
•	SparseMountainCar (S ⊂ R2 , A ⊂ R), which only yields a reward if the agent drives up
the hill,
•	SparseDoublePendulum (S ⊂ R6, A ⊂ R), which only yields a reward if the agent reaches
the upright position.
18