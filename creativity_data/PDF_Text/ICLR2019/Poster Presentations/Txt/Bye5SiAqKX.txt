Published as a conference paper at ICLR 2019
Preconditioner on Matrix Lie Group for SGD
Xi-Lin Li
GMEMS Technologies and Spectimbre
366 Fairview Way, Milpitas, CA 95035
lixilinx@gmail.com
Ab stract
We study two types of preconditioners and preconditioned stochastic gradient de-
scent (SGD) methods in a unified framework. We call the first one the Newton type
due to its close relationship to the Newton method, and the second one the Fisher
type as its preconditioner is closely related to the inverse of Fisher information
matrix. Both preconditioners can be derived from one framework, and efficiently
estimated on any matrix Lie groups designated by the user using natural or rela-
tive gradient descent minimizing certain preconditioner estimation criteria. Many
existing preconditioners and methods, e.g., RMSProp, Adam, KFAC, equilibrated
SGD, batch normalization, etc., are special cases of or closely related to either the
Newton type or the Fisher type ones. Experimental results on relatively large scale
machine learning problems are reported for performance study.
1 Introduction
This paper investigates the use of preconditioner for accelerating gradient descent, especially in
large scale machine learning problems. Stochastic gradient descent (SGD) and its variations, e.g.,
momentum (Rumelhart et al., 1986; Nesterov, 1983), RMSProp and Adagrad (John et al., 2011),
Adam (Kingma & Ba, 2015), etc., are popular choices due to their simplicity and wide applicabil-
ity. These simple methods do not use well normalized step size, could converge slow, and might
involve more controlling parameters requiring fine tweaking. Convex optimization is a well studied
field (Boyd & Vandenberghe, 2004). Many off-the-shelf methods there, e.g., (nonlinear) conjugate
gradient descent, quasi-Newton methods, Hessian-free optimizations, etc., can be applied to small
and middle scale machine learning problems without much modifications. However, these convex
optimization methods may have difficulty in handling gradient noise and scaling up to problems
with hundreds of millions of free parameters. For a large family of machine learning problems,
natural gradient with the Fisher information metric is equivalent to a preconditioned gradient using
inverse of the Fisher information matrix as the preconditioner (Amari, 1998). Natural gradient and
its variations, e.g., Kronecker-factored approximate curvature (KFAC) (Martens & Grosse, 2015)
and the one in (Povey et al., 2015), all use such preconditioners. Other less popular choices are the
equilibrated preconditioner (Dauphin et al., 2015) and the one proposed in (Li, 2018). Momentum
or the heavy-ball method provides another independent way to accelerate converge (Nesterov, 1983;
Rumelhart et al., 1986). Furthermore, momentum and preconditioner can be combined to further
accelerate convergence as shown in Adam (Kingma & Ba, 2015).
This paper groups the above mentioned preconditioners and preconditioned SGD methods into two
classes, the Newton type and the Fisher type. The Newton type is closely related to the Newton
method, and is suitable for general purpose optimizations. The Fisher type preconditioner relates to
the inverse of Fisher information matrix, and is limited to a large subclass of stochastic optimiza-
tion problems where the Fish information metric can be well defined. Both preconditioners can be
derived from one framework, and estimated on any matrix Lie groups designated by the user with
almost the same natural or relative gradient descent methods minimizing specific preconditioner
estimation criteria.
1
Published as a conference paper at ICLR 2019
2	Background
2.1	Notations
We consider the minimization of cost function
f(θ) = Ez['(θ,z)]	(1)
where Ez takes expectation over random variable z , ` is a loss function, and θ is the model parameter
vector to be optimized. For example, in a classification problem, ` could be the cross entropy loss, z
is a pair of input feature vector and class label, vector θ consists of all the trainable parameters in the
classification model, and Ez takes average over all samples from the training data set. By assuming
second order differentiable model and loss, we could approximate `(θ, z) as a quadratic function
of θ within a trust region around θ, i.e., `(θ, z) = bzTθ + 0.5θTHzθ + az, where az is the sum of
higher order approximation errors and constant term independent of θ, Hz is a symmetric matrix,
and subscript z in bz , Hz and az reminds us that these three terms depend on z. Clearly, these
three terms depend on θ as well, although we do not explicitly show this dependence to simplify our
notations since we just consider parameter updates in the same trust region. Now, we may rewrite
(1) as f(θ) = bTθ + 0.5θTHθ + a, where b = Ez [bz], H = Ez [Hz], and a = Ez [az]. We do
not impose any assumption, e.g., positive definiteness, on H except for being symmetric. Thus,
the quadratic surface in the trust region could be non-convex. To simplify our notations, we no
longer consider the higher order approximation errors included in a, and simply assume that f(θ) is
a quadratic function of θ in the trust region.
2.2	Preconditioner
Let us consider a certain iteration. Preconditioned SGD updates θ as
θ 一 θ - μP∂f(θ)∕∂θ	(2)
where μ > 0 is the step size, f (θ) is an estimate of f (θ) obtained by replacing expectation With
sample average, and positive definite matrix P could be a fixed or adaptive preconditioner. By
letting θ0 = P -0.5 θ, we can rewrite (2) as
θ J θ0 - μ∂∕(θ)∕∂θ0	(3)
where P -0.5 denotes the principal square root of P. Hence, (3) suggests that preconditioned SGD
is equivalent to SGD in a transformed parameter domain. Within the considered trust region, let us
write the stochastic gradient, ∂f (θ)∕∂θ, explicitly as
ʌ . . . ʌ ʌ
∂f(θ)∕∂θ = H θ + b	(4)
where H and b are estimates of H and b, respectively. Combining (4) and (2) gives the following
linear system
θ J (I — μPH)θ — μPb	(5)
for updating θ within the assumed trust region, where I is the identity matrix. A properly determined
P could significantly accelerate convergence of the locally linear system in (5).
We review a few facts shown in (Li, 2018) before introducing our main contributions. Let δθ be a
random perturbation ofθ, and be small enough such that θ + δθ still resides in the same trust region.
Then, (4) suggests the following resultant perturbation of stochastic gradient,
δg =f ∂f(θ + δθ)∕∂θ — ∂f(θ)∕∂θ = Hδθ = Hδθ + ε	(6)
where ε accounts for the error due to replacing H with H. Note that by definition, δg is a random
vector dependent on both z and δθ. The preconditioner in (Li, 2018) is pursued by minimizing
criterion
C(P ) = Ez,δθ [δgτ Pδg + δθT P -1δθ]	⑺
where subscript δθ in Ez,δθ denotes taking expectation over δθ. Under mild conditions, criterion
(7) determines a unique positive definite P, which is optimal in the sense that it preconditions the
stochastic gradient such that
PEz,δθ [δgδgτ ]P = Eδθ [δθδθτ]	(8)
2
Published as a conference paper at ICLR 2019
which is comparable to relationship H -1δgδgT H -1 = δθδθT, where δg = Hδθ is the perturba-
tion of noiseless gradient, and we assume that H is invertible, but not necessarily positive definite.
Clearly, this preconditioner is comparable to H-1. It perfectly preconditions the gradient such that
the amplitudes of parameter perturbations match that of the associated preconditioned gradient, re-
gardless of the amount of gradient noise. Naturally, preconditioned SGD with this preconditioner
inherits the scale-invariance property of the Newton method.
Note that in the presence of gradient noise, the optimal P and P -1 given by (8) are not unbi-
ased estimates of H-1 and H, respectively. Actually, even if H is positive definite and avail-
able, H-1 may not always be a good preconditioner when H is ill-conditioned since it could
significantly amplify the gradient noise along the directions of the eigenvectors of H associated
with small eigenvalues, and lead to divergence. More specifically, it is shown in (Li, 2018) that
H-1Ez,δθ [δgδ^T]HT ≥ Eδθ [δθδθτ], where A ≥ B means that A - B is nonnegative definite.
3	Two preconditioner estimation criteria
3.1	The Newton type criterion
Preconditioner estimation criterion (7) requires δθ to be small enough such that θ and θ + δθ reside
in the same trust region. In practice, numerical error might be an issue when handling small numbers
with floating point arithmetic. This concern becomes more grave with the popularity of single and
even half precision math in large scale neural network training. Luckily, (6) relates δg to the Hessian-
vector product, which can be efficiently evaluated with automatic differentiation software tools. Let
v be a random vector with the same dimension as θ. Then, (4) suggests the following method for
Hessian-vector product evaluation,
∂θ {[方⑻gθ]T v} = f) V = HV	⑼
Now, replacing (δθ, δg) in (7) with (v,Hv) leads to our following new preconditioner estimation
criterion,
Cn(P ) = Ez,v[vT HPHV + VT P-1v]	(10)
where the subscript V in Ez,v suggests taking expectation over V. We no longer have the need to
assume V to be an arbitrarily small vector. It is important to note that this criterion only requires
the Hessian-vector product. The Hessian itself is not of interest. We call (10) the Newton type
preconditioner estimation criterion as the resultant preconditioned SGD method is closely related to
the Newton method.
3.2	The Fisher type criterion
We consider the machine learning problems where the empirical Fisher information matrix can be
well defined by F =	Ez	d'∂θz)	(d'∂θ,z))	. Replacing (δθ,	δ^)	in (7) with	(v,g	+ λv)	leads to
criterion
Cf(P) = Ez,v[(^ + λv)T P (g + λv) + VT P-1v]	(11)
where ^ = ∂f (θ)∕∂θ is a shorthand for stochastic gradient, and λ ≥ 0 is a damping factor. Clearly,
V is independent of g. Let us further assume that V is drawn from standard multivariate normal
distribution N(0, I), i.e., Ev [V] = 0 and Ev [VVT] = I. Then, we could simplify Cf(P) as
Cf(P )= tr{PEz,v [(g + λV)(g + λV)T ] + PTEv [VVT]}
=tr{P [Ez [ggT ]+ λ2I ]+ P-1}	(12)
By letting the derivative of Cf (P) with respect to P be zero, the optimal positive definite solution
for Cf (P ) is readily shown to be
P = {Ez[g^T ]+ λ2I}-0.5	(13)
3
Published as a conference paper at ICLR 2019
When g is a gradient estimation obtained by taking average over B independent samples, Ez[^gT]
is related to the Fisher information matrix by
Ez[ggT ]= F/B + (B - 1)ggT/B	(14)
We call this preconditioner the Fisher type one due to its close relationship to the Fisher information
matrix. One can easily modify this preconditioner to obtain an unbiased estimation of F-1. Let S
be an exponential moving average of ^. Then, after replacing the g in (13) with g 一 s + s∕√B and
setting λ = 0, P2/B will be an unbiased estimation of F-1. Generally, it might be acceptable to
keep the bias term, (B - 1)ggT /B, in (14) for two reasons: itis nonnegative definite and regularizes
the inversion in (13); it vanishes when the parameters approach a stationary point. Actually, the
Fisher information matrix could be singular for many commonly used models, e.g., finite mixture
models, neural networks, hidden Markov models. We might not be able to inverse F for these
singular statistical models without using regularization or damping. A Fisher type preconditioner
with λ > 0 loses the scale-invariance property ofa Newton type preconditioner. Both P and P2 can
be useful preconditioners when the step size μ and damping factor λ are set properly.
3.3	Properties of the Newton type preconditioner
Following the ideas in (Li, 2018), we can show that (10) determines a unique positive definite pre-
conditioner if and only if Ev [vvτ] is positive definite and {Ev [vvτ]}0∙5Ez,v [HVvTH]{Ev [vvτ]}0∙5
has distinct eigenvalues. Other minimum solutions of criterion (10) are either indefinite or negative
definite, and are not interested for our purpose. The proof itself has limited novelty. We omit it
here. Instead, let us consider the simplest case, where θ is a scalar parameter, to gain some intuitive
understandings of criterion (10). For scalar parameter, it is trivial to show that the optimal solutions
minimizing (10) are
P = ±EvEv[v2]∕Ez,v[h2v2] = ±1∕Jh2 + Ez[(h - h)2]	(15)
where H, H, P, and V are replaced with their plain lower case letters, and We have used the fact that
H ― H and v are independent. For gradient descent, we choose the positive solution, although the
negative one gives the global minimum of (10). With the positive preconditioner, eigenvalue of the
locally linear system in (5) is
h∕,h2 + Ez [(h - h)2]	(16)
Now, it is clear that this optimal preconditioner damps the gradient noise when Ez [(h ― h)2] is large,
and preconditions the locally linear system in (5) such that its eigenvalue has unitary amplitude when
the gradient noise vanishes. Convergence is ensured when a normalized step size, i.e., 0 < μ < 1,
is used. For θ with higher dimensions, eigenvalues of the locally linear system in (5) is normalized
into range [―1, 1] as well, in a way similar to (16).
4	Estimating preconditioners on matrix Lie groups
4.1	Updating rule for the Newton type preconditioner
Let us take the Newton type preconditioner as an example to derive its updating rule. Updating rule
for the Fisher type preconditioner is the same except for replacing the Hessian-vector product with
stochastic gradient. Here, Lie group always refers to the matrix Lie group.
It is inconvenient to optimize P directly as it must be a symmetric and positive definite matrix.
Instead, we represent the preconditioner as P = QTQ, and estimate Q. Now, Q must be a nonsin-
gular matrix as both cn(P) and cf(P) diverge when P is singular. Invertible matrices with the same
dimension form a Lie group. In practice, we are more interested in Lie groups with sparse repre-
sentations. Examples of such groups are given in the next section. Let us consider a proper small
perturbation of Q, δQ, such that Q + δQ still lives on the same Lie group. The distance between Q
and Q + δQ can be naturally defined as dist(Q, Q + δQ) = Ptr(δQQ-1Q-TδQT) (Amari,1998).
Intuitively, this distance is larger for the same amount of perturbation when Q is closer to a singular
matrix. With the above tensor metric definition, natural gradient for optimizing Q has form
VQ = RQ	(17)
4
Published as a conference paper at ICLR 2019
For example, when Q lives on the group of invertible upper triangular matrices, R is given by
R = 2triu{Ez,v[QHVvTH QT — Q-TvvTQ-1]}	(18)
where triu takes the upper triangular part of a matrix. Another way to derive (17) is to let δQ = EQ,
and consider the derivative with respect to E, where E is a proper small matrix such that Q +EQ still
lives on the same Lie group. Gradient derived in this way is known as relative gradient (Cardoso &
Laheld, 1996). For our preconditioner learning problem, relative gradient and natural gradient have
the same form. Now, Q can be updated using natural or relative gradient descent as
Q J Q — μqRQ	(19)
In practice, it is convenient to use the following updating rule with normalized step size,
Q J (I — M0R/kRk)Q	QO)
where 0 < μo < 1, and ∣∣ ∙ ∣∣ takes the norm of a matrix. One simple choice for matrix norm is the
maximum absolute value of a matrix.
Note that natural gradient can take different forms. One should not confuse the natural gradient on
the Lie group derived from a tensor metric with the natural gradient for model parameter learning
derived from a Fisher information metric.
4.2	Summary of the Newton type preconditioned SGD
One iteration of the Newton type preconditioned SGD consists of the following steps.
1.	Evaluate stochastic gradient g.
2.	Draw v from N(0, I), and evaluate Hessian-vector product Hv.
3.	Update preconditioners with Q J (I — μoR∕∣R∣)Q.
4.	Update parameters with θ J θ — μQTQg.
The two step sizes, μ and μo, are normalized. They should take values in range [0,1] with typical
value 0.01. We usually initialize Q to a scaled identity matrix with proper dimension. The specific
form ofR depends on the Lie group to be considered. For example, for upper triangular Q, we have
R = 2triu[QHvvτH QT — Q-T vvT Q-1], where Q-Tv can be efficiently calculated with back
substitution.
4.3	Summary of the Fisher type preconditioned SGD
We only need to replace Hv in the Newton type preconditioned SGD with g+λv to obtain the Fisher
type one. Thus, we do not list its main steps here. Note that only its step size for the preconditioner
updating is normalized. There is no simple way to jointly determine the proper ranges for step size
μ and damping factor λ. Again, R may take different forms on different Lie groups. For upper
triangular Q, we have R = 2tiru[Q(^ + λv)(g + λv)TQT — Q-TvvTQ-1], where v ~ N(0,I).
Here, itis important to note that the natural or relative gradient for cf (P) with the form given in (12)
involves explicit matrix inversion. However, matrix inversion can be avoided by using the cf (P ) in
(11), which includes v as an auxiliary variable. It is highly recommended to avoid explicit matrix
inversion for large Q.
4.4	Variations of preconditioned SGD
There are many ways to modify the above preconditioned SGD methods. Since curvatures typically
evolves slower than gradients, one can update the preconditioner less frequently to save compu-
tations per iteration. With parallel computing available, one might update the preconditioner and
model parameters simultaneously and asynchronously to save wall time per iteration. Combining
preconditioner and momentum may further accelerate convergence. For recurrent neural network
learning, we may need to clip the norm of preconditioned gradients to avoid excessively large pa-
rameter updates. In general, preconditioned gradient clipping relates to the trust region method by
∣∣θ[new] — θ∣∣ = kμPg∕max(1, ∣P^k∕Ω)k ≤ μΩ
5
Published as a conference paper at ICLR 2019
where Ω > 0 is a clipping threshold, comparable to the size of trust region. One may set Ω toa Pos-
itive number proportional to the square root of the number of model parameters. Most importantly,
we can choose different Lie groups for estimating our preconditioners to achieve a good trade off
between performance and complexity.
5	Useful Lie groups with sparse representations
In practice, we seldom consider the Lie group consisting of dense invertible matrices for precon-
ditioner estimation when the problem size is large. Lie groups with sparse structures are of more
interests. To begin with, let us recall a few facts about Lie group. If A and B are two Lie groups,
then AT, A 0 B, and A ㊉ B all are Lie groups, where 0 and ㊉ denote Kronecker product and direct
sum, respectively. Furthermore, for any matrix C with compatible dimensions, block matrix
Q=	A0 CB	(21)
still forms a Lie group. We do not show proofs of the above statements here as they are no more than
a few lines of algebraic operations. These simple rules can be used to design many useful Lie groups
for constructing sparse preconditioners. We already know that invertible upper triangular matrices
form a Lie group. Here, we list a few useful ones with sparse representations.
5.1	Diagonal preconditioner
Diagonal matrices with the same dimension and positive diagonal entries form a Lie group with
reducible representation. Preconditioners learned on this group are called diagonal preconditioners.
5.2	Kronecker product preconditioner
For matrix parameter Θ, we can flatten Θ into a vector, and precondition its gradient using a Kro-
necker product preconditioner with Q having form Q = Q2 0 Q1. Clearly, Q is a Lie group as long
as Q1 and Q2 are two Lie groups. Let us check its role in learning the following affine transformation
y = [Θweight,Θbias]x = Θx	(22)
where x is the input feature vector augmented with 1, and y is the output feature vector. After
reverting the flattened Θ back to its matrix form, the preconditioned SGD learning rule for Θ is
2^g∖
Θ 一 Θ - μQTQιdf∂θQTQ2	(23)
Similar to (3), we introduce coordinate transformation Θ0 = Q1-TΘQ2-1, and rewrite (23) as
Θ0 - Θ0 — μ∂∕(Θ”∂Θ0	(24)
Correspondingly, the affine transformation in (22) is rewritten as y0 = Θ0x0, where y0 = Q1-T y and
x0 = Q2x are the transformed input and output feature vectors, respectively. Hence, the precon-
ditioned SGD in (23) is equivalent to the SGD in (24) with transformed feature vectors x0 and y0 .
We know that feature whitening and normalization could significantly accelerate convergence. A
Kronecker product preconditioner plays a similar role in learning the affine transformation in (22).
5.3	Scaling and normalization preconditioner
This is a special Kronecker product preconditioner by constraining Q1 to be a diagonal matrix, and
Q2 to be a sparse matrix where only its diagonal and last column can have nonzero values. Note that
Q2 with nonzero diagonal entries forms a Lie group. Hence, Q = Q2 0 Q1 is a Lie group as well.
We call it a scaling and normalization preconditioner as it resembles a preconditioner that scales the
output features and normalizes the input features. Let us check the transformed features y0 = Q1-T y
and x0 = Q2x. It is clear that y0 is an element-wisely scaled version ofy as Q1 is a diagonal matrix.
To make x0 a “normalized” feature vector, x needs to be an input feature vector augmented with 1.
6
Published as a conference paper at ICLR 2019
Let us check a simple example to verify this point. We consider an input vector with two features,
and write down its normalized features explicitly as below,
x0i -I	Γ 1∕σι
x02	=	0
10
0	-m1∕σ1
1∕σ2	-m2∕σ2
01
x1
x2
1
(25)
where mi and σi are the mean and standard deviation of xi, respectively. It is straightforward to
show that the feature normalization operation in (25) forms a Lie group with four freedoms. For the
scaling-and-normalization preconditioner, we have no need to force the last diagonal entry of Q2 to
be 1. Hence, the group of feature normalization operation is a subgroup of Q2.
5.4	Scaling and whitening preconditioner
This is another special Kronecker product preconditioner by constraining Q1 to be a diagonal matrix,
and Q2 to be an upper triangular matrix with positive diagonal entries. We call it a scaling-and-
whitening preconditioner since it resembles a preconditioner that scales the output features and
whitens the input features. Again, the input feature vector x must be augmented with 1 such that the
whitening operation forms a Lie group represented by upper triangular matrices with 1 being its last
diagonal entry. This is a subgroup of Q2 as we have no need to fix Q2’s last diagonal entry to 1.
It is not possible to enumerate all kinds of Lie groups suitable for constructing preconditioners.
For example, Kronecker product preconditioner with form Q = Q3 0 Q2 0 Q1 could be used
for preconditioning gradients of a third order tensor. The normalization and whitening groups are
just two special cases of the groups with the form shown in (21), and there are numerous more
choices having sparsities between that of these two. Regardless of the detailed form of Q, all such
preconditioners share the same form of learning rule shown in (20), and they all can be efficiently
learned using natural or relative gradient descent without much tuning effort.
6	Relationship to existing methods
6.1	Relationship to Adagrad, RMSProp and Adam
Adagrad, RMSProp and Adam all use the Fisher type preconditioner living on the group of diag-
onal matrices with positive diagonal entries. This is a simple group. Optimal solution for cf (P)
has closed-form solution P = diag(1 √Ez[^ Θ ^]+ λ2), where Θ and denote element wise
multiplication and division, respectively. In practice, simple exponential moving average is used to
replace the expectation when using this preconditioner.
6.2	Relationship to equilibrated SGD
For diagonal preconditioner, the optimal solution minimizing cn (P ) has closed-form solution P =
diag (J Ev [v Θ v] 0 Ez,v [HV Θ Hv]). For v 〜N (0,I), Ev [v Θ v] reduces to a vector with unit
entries. Then, this optimal solution gives the equilibration preconditioner in (Dauphin et al., 2015).
6.3	Relationship to KFAC and similar methods
The preconditioners considered in (Povey et al., 2015) and (Martens & Grosse, 2015) are closely
related to the Fisher type Kronecker product preconditioners. While KFAC approximates the Fisher
metric of a matrix parameter as a Kronecker product to obtain its approximated inverse in closed-
form solution, our method turns to an iterative solution to approximate this same inverse. Theo-
retically, our method’s accuracy is only limited by the expressive power of the Lie group since no
intermediate approximation is made. In practice, one distinct advantage of our method over KFAC
is that explicit matrix inversion is avoided by introducing auxiliary vector v and using back sub-
stitution, while KFAC typically requires inversion of symmetric matrices. On graphics processing
units (GPU) and with large matrices, parallel back substitution is as computationally cheap as matrix
multiplication, and could be several orders of magnitude faster than inversion of symmetric matrix.
Another advantage is that our method is derived from a unified framework. There is no need to
invent different preconditioner learning rules when we switch the Lie group representations.
7
Published as a conference paper at ICLR 2019
100
10-20
Gradient descent
Fixed momentum
NeSterOv momentum
Fisher type P
- Newton type P
2000	4000	6000	8000	10000
Iterations
Figure 1: Convergence curves of compared methods on minimizing the Rosenbrock function.
6.4 Relationship to batch normalization
Batch normalization can be viewed as preconditioned SGD using a specific scaling-and-
normalization preconditioner with constraint Q1 = I and Q2 from the feature normalization Lie
group. However, we should be aware that explicit input feature normalization is only empirically
shown to accelerate convergence, and has little meaning in certain scenarios, e.g., recurrent neural
network learning where features may not have any stationary first or second order statistic. Both
the Newton and Fisher type preconditioned SGD methods provide a more general and principled
approach to find the optimal preconditioner, and apply to a broader range of applications. Generally,
a scaling-and-normalization preconditioner does not necessarily “normalize” the input features in
the sense of mean removal and variance normalization.
7 Experimental results
We use the square root Fisher type preconditioners in the following experiments since they are less
picky on the damping factor, and seem to be more numerically robust on large scale problems. Still,
as shown in our Pytorch implementation package, the original Fisher type preconditioners could
perform better on small scale problems like the MNIST handwritten digit recognition task.
7.1	Application to mathematical optimization
Let us consider the minimization of Rosenbrock function, f(θ) = 100(θ2 - θ12)2 + (1 - θ1)2,
starting from initial guess θ = [-1, 1]. This is a well known benchmark problem for mathematical
optimization. The compared methods use fixed step size. For each method, the best step size is
selected from sequence {. . . , 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, . . .}. For gradient descent, the best
step size is 0.002. For momentum method, the moving average factor is 0.9, and the best step size is
0.002. For Nesterov momentum, the best step size is 0.001. For preconditioned SGD, Q is initialized
to 0.1I and lives on the group of triangular matrices. For the Fisher type method, we set λ = 0.1, and
step sizes 0.01 and 0.001 for preconditioner and parameter updates, respectively. For the Newton
type method, we set step sizes 0.2 and 0.5 for preconditioner and parameter updates, respectively.
Figure 1 summarizes the results. The Newton type method performs the best, converging to the
optimal solution using about 200 iterations. The Fisher type method does not fit into this problem,
and performs poorly as expected. Mathematical optimization is not our focus. Still, this example
shows that the Newton type preconditioned SGD works well for mathematical optimization.
7.2	ImageNet experiment
We consider the ImageNet ILSVRC2012 database for the image classification task. The well known
AlexNet is considered. We follow the descriptions in (Alex et al., 2012) as closely as possible to
set up our experiment. One main difference is that we do not augment the training data. Another
big difference is that we use a modified local response normalization (LRN). The LRN function
from TensorFlow implementation is not second order differentiable. We have to approximate the
local energy used for LRN with a properly scaled global energy to facilitate Hessian-vector product
evaluation. Note that convolution can be rewritten as correlation between the flattened input image
patches and filter coefficients. In this way, we find that there are eight matrices to be optimized
in the AlexNet, and their shapes are: [96, 11 × 11 × 3 + 1], [256, 5 × 5 × 96 + 1], [384, 3 × 3 ×
256 + 1], [384, 3 × 3 × 384 + 1], [256, 3 × 3 × 384], [4096, 6 × 6 × 256 + 1], [4096, 4096 + 1],
and [1000, 4096 + 1]. We have tried diagonal and scaling-and-normalization preconditioners for
8
Published as a conference paper at ICLR 2019
642386422
- - - - - - -
3 3 3 2 2 2 2
SSo-U-e」J_
-one UO-w0g-sseoUO--ep--e>

Figure 2:	Typical smoothed learning curves from compared methods for the ImageNet
ILSVRC2012 image classification task with AlexNet. Note that batch normalization alters the L2-
regularization. Its training loss is not directly comparable with others.
each matrix. Denser preconditioners, e.g., the Kronecker product one, require hundreds of millions
parameters for representations, and are expensive to run on our platform. Each compared method
is trained with 40 epochs, mini-batch size 128, step size μ for the first 20 epochs, and 0.1μ for
the last 20 epochs. We have compared several methods with multiple settings, and only report the
ones with reasonably good results here. For Adam, the initial step size is set to 0.00005. For batch
normalization, initial step size is 0.002, and its moving average factors for momentum and statistics
used for feature normalization are 0.9 and 0.99, respectively. The momentum method uses initial
step size 0.002, and moving average factor 0.9 for momentum. Preconditioned SGD performs better
with the scaling-and-normalization preconditioner. Its Q is initialized to 0.1I, and updated with
normalized step size 0.01. For the Fisher type preconditioner, we set λ = 0.001 and initial step size
0.00005. For the Newton type preconditioner, its initial step size is 0.01. Figure 2 summarizes the
results. Training loss for batch normalization is only for reference purpose as normalization alters
the L2-regularization term. Batch normalization does not perform well under this setup, maybe due
to its conflict with certain settings like the LRN and L2-regularization. We see that the scaling-
and-normalization preconditioner does accelerate convergence, although it is super sparse. The
Newton type preconditioned SGD performs the best, and achieves top-1 validation accuracy about
56% when using only one crop for testing, while the momentum method may require 90 epochs to
achieve similar performance.
7.3 Word level language modeling experiment
We consider the world level language modeling problem with reference implementation available
from https://github.com/pytorch/examples. The Wikitext-2 database with 33278 to-
kens is considered. The task is to predict the next token from history observations. Our tested
network consists of six layers, i.e., encoding layer, LSTM layer, dropout layer, LSTM layer, dropout
layer, and decoding layer. For each LSTM layer, we put all its coefficients into a single matrix Θ
by defining output and augmented input feature vectors as in [it; ft; gt; ot] = Θ [xt; ht-1 ; 1] , ct =
f t ct-1 + it gt , ht = ot tanh(ct ), where t is a discrete time index, x is the input, h is the hidden
state, and c is the cell state. The encoding layer’s weight matrix is the transpose of that of the decod-
ing layer. Thus, we totally get three matrices to be optimized. With hidden layer size 200, shapes
of these three matrices are [4 × 200, 2 × 200 + 1], [4 × 200, 2 × 200 + 1], and [33278, 200 + 1],
respectively. For all methods, the step size is reduced to one fourth of the current value whenever
the current perplexity on validation set is larger than the best one ever found. For SGD, the initial
step size is 20, and the gradient is clipped with threshold 0.25. The momentum method diverges
easily without clipping. We set momentum 0.9, initial step size 1, and clipping threshold 0.25. We
set initial step size 0.005 and damping factor λ2 = 10-12 for Adam and sparse Adam. Sparse
Adam updates its moments and model parameters only when the corresponding stochastic gradients
are not zeros. We have tried diagonal, scaling-and-normalization and scaling-and-whitening pre-
conditioners for each matrix. The encoding (decoding) matrix is too large to consider KFAC like
preconditioner. The diagonal preconditioner performs the worst, and the other two have compara-
ble performance. For both types of preconditioned SGD, the clipping threshold for preconditioned
gradient is 100, the initial step size is 0.1, and Q is initialized to I. We set λ = 0 for the Fisher
9
Published as a conference paper at ICLR 2019
0	10	20	30	40
Epochs
135
13 130
I 125
ɑ 120
1115
> 110
105
Epochs
Epochs
Figure 3:	Typical learning curves from compared methods for the Wikitext-2 database word level
language modeling task with a LSTM neural network.
type preconditioned SGD. With dropout rate 0.2, the best three test perplexities are 100.7 by SGD,
103.9 by Newton type preconditioned SGD, and 105.8 by Fisher type preconditioned SGD. With
dropout rate 0.35, the best three test perplexities are 100.7 by Newton type preconditioned SGD,
102.3 by SGD, and 103.7 by Fisher type preconditioned SGD. Although SGD performs well on the
test set, both types of preconditioned SGD have significantly lower training losses than SGD with
either dropout rate. Figure 3 summarizes the results when the dropout rate is 0.35. Methods involv-
ing momentum, including Adam and sparse Adam, perform poorly. Note that our preconditioners
preserve the sparsity property of gradients from the encoding and decoding layers (Appendix A).
This saves considerable computations by avoiding update parameters with zero gradients. Again,
both preconditioners accelerate convergence significantly despite their high sparsity.
7.4 Computational complexity and implementation
Compared with SGD, the Fisher type preconditioned SGD adds limited computational complexity
when sparse preconditioners are adopted. The Newton type preconditioned SGD requires Hessian-
vector product, which typically has complexity comparable to that of gradient evaluation. Thus,
using SGD as the base line, the Newton type preconditioned SGD approximately doubles the com-
putational complexity per iteration, while the Fisher type SGD has similar complexity. Wall time
per iteration of preconditioned SGD highly depends on the implementations. Ideally, the precon-
ditioners and parameters could be updated in a parallel and asynchronous way such that SGD and
preconditioned SGD have comparable wall time per iteration.
We have put our TensorFlow and Pytorch implementations on https://github.com/
lixilinx. More experimental results comparing different preconditioners and optimization meth-
ods on diverse benchmark problems can be found there. For the ImageNet experiment, all compared
methods are implemented in Tensorflow, and require two days and a few hours to finish 40 epochs
on a GeForce GTX 1080 Ti GPU. The word level language modeling experiment is implemented in
Pytorch. We have rewritten the word embedding function to enable second order derivative. For this
task, SGD and the Fisher type preconditioned SGD have similar wall time per iteration, while the
Newton type method requires about 80% more wall time per iteration than SGD when running on
the same GPU.
8 Conclusions
Two types of preconditioners and preconditioned SGD methods are studied. The one requiring
Hessian-vector product for preconditioner estimation is suitable for general purpose optimization.
We call it the Newton type preconditioned SGD due to its close relationship to the Newton method.
The other one only requires gradient for preconditioner estimation. We call it the Fisher type pre-
conditioned SGD as its preconditioner is closely related to the inverse of Fisher information matrix.
Both preconditioners can be efficiently learned using natural or relative gradient descent on any
matrix Lie groups designated by the user. The Fisher type preconditioned SGD has lower com-
putational complexity per iteration, but may require more tuning efforts on selecting its step size
and damping factor. The Newton type preconditioned SGD has higher computational complexity
per iteration, but is more user friendly due to its use of normalized step size and built-in gradient
noise damping ability. Both preconditioners, even with very sparse representations, are shown to
considerably accelerate convergence on relatively large scale problems.
10
Published as a conference paper at ICLR 2019
Acknowledgments
The author thanks the reviewers for their comments and Yaroslav Bulatov for his discussions to
improve this paper.
References
K. Alex, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural
networks. InNIPS,pp.1097-1105, 2012.
S. Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251-276, 1998.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
J. F. Cardoso and B. H. Laheld. Equivariant adaptive source separation. IEEE Trans. Signal Process.,
44(12):3017-3030, 1996.
Y. N. Dauphin, H. Vries, and Y. Bengio. Equilibrated adaptive learning rates for non-convex opti-
mization. In NIPS, pp. 1504-1512. MIT Press, 2015.
D. John, H. Elad, and S. Yoram. Adaptive subgradient methods for online learning and stochastic
optimization. The Journal of Machine Learning Research, 12:2121-2159, 2011.
D. P. Kingma and J. L. Ba. Adam: a method for stochastic optimization. In ICLR. Ithaca, NY:
arXiv.org, 2015.
X. L. Li. Preconditioned stochastic gradient descent. IEEE Trans. Neural Networks and Learning
Systems, 29(5):1454-1466, 2018.
J. Martens and R. B. Grosse. Optimizing neural networks with Kronecker-factored approximate
curvature. In ICML, pp. 2408-2417, 2015.
Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/srt(k)).
Soviet Mathematics Doklady, 27:372-376, 1983.
D. Povey, X. Zhang, and S. Khudanpur. Parallel training of DNNs with natural gradient and param-
eter averaging. In ICLR. Ithaca, NY: arXiv.org, 2015.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating
errors. Nature, 323:533-536, 1986.
11
Published as a conference paper at ICLR 2019
Appendix A: Whitening-and-scaling preconditioner preserves
SPARSITY OF GRADIENT OF ENCODING MATRIX
We are to show that the scaling-and-whitening and whitening-and-scaling preconditioners preserve
the sparsity property of gradients from the decoding and encoding layers in the word level language
model considered in Experiment 3, respectively. Clearly, we only need to show this for the encoding
part. Let
W = [w1, w2, . . . ,wI]
be a D × I word embedding matrix, where D is the embedding dimension, I is the number of
tokens, and wi is the vector representation for the ith token. Typically, only a whitening-and-scaling
and sparser preconditioners are affordable since I D > 1. Notably, for sufficiently large I,
the whitening-and-scaling preconditioner could be sparser than the diagonal one. Let us consider
preconditioner
P = QTQ, Q = Q2 乳Qi,	Q2 = diag(q2,1,q2,2,...,q2,ι)
By (23), the preconditioned gradient is
[q2,ιQT Q1g1, q2,2QT Q&,…,q2,ιQT Qigi]
where g% is the stochastic gradient for the ith word embedding vector. Since I》B, most of these
gi's are zeros, where B is the batch size. This preconditioner does not mix UP gradients of different
word embedding vectors. Hence, the sparsity property is preserved.
12