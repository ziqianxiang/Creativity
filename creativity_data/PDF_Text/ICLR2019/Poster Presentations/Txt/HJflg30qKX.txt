Published as a conference paper at ICLR 2019
Gradient descent aligns the layers of deep
LINEAR NETWORKS
Ziwei Ji & Matus Telgarsky
Department of Computer Science
University of Illinois at Urbana-Champaign
{ziweiji2,mjt}@illinois.edu
Ab stract
This paper establishes risk convergence and asymptotic weight matrix alignment
— a form of implicit regularization — of gradient flow and gradient descent when
applied to deep linear networks on linearly separable data. In more detail, for
gradient flow applied to strictly decreasing loss functions (with similar results
for gradient descent with particular decreasing step sizes): (i) the risk converges
to 0; (ii) the normalized ith weight matrix asymptotically equals its rank-1 ap-
proximation uivi> ; (iii) these rank-1 matrices are aligned across layers, meaning
|vi>+1ui| → 1. In the case of the logistic loss (binary cross entropy), more can be
said: the linear function induced by the network — the product of its weight ma-
trices — converges to the same direction as the maximum margin solution. This
last property was identified in prior work, but only under assumptions on gradient
descent which here are implied by the alignment phenomenon.
1	Introduction
Efforts to explain the effectiveness of gradient descent in deep learning have uncovered an exciting
possibility: it not only finds solutions with low error, but also biases the search for low complex-
ity solutions which generalize well (Zhang et al., 2017; Bartlett et al., 2017; Soudry et al., 2017;
Gunasekar et al., 2018).
This paper analyzes the implicit regularization of gradient descent and gradient flow on deep linear
networks and linearly separable data. For strictly decreasing losses, the optimum is at infinity, and
we establish various alignment phenomena:
•	For each weight matrix Wi, the corresponding normalized weight matrix Wi/kWikF asymp-
totically equals its rank-1 approximation uivi>, where the Frobenius norm kWi kF satisfies
kWikF → ∞. In other words, kWik2/kWikF → 1, and asymptotically only the rank-1
approximation of Wi contributes to the final predictor, a form of implicit regularization.
•	Adjacent rank-1 weight matrix approximations are aligned: |vi>+1ui | → 1.
•	For the logistic loss, the first right singular vector v1 ofW1 is aligned with the data, meaning
vι converges to the unique maximum margin predictor U defined by the data. Moreover,
the linear predictor induced by the network, WProd ：= WL …Wι, is also aligned with the
data, meaning wprod/kwp
rod k → U.
Simultaneously, this work proves that the risk is globally optimized: it asymptotes to 0. Alignment
and risk convergence are proved simultaneously; the phenomena are coupled within the proofs.
Since the layers align, they can be viewed as a minimum norm solution: they do not “waste norm”
on components which are killed off when the layers are multiplied together. Said another way,
given data ((xi, yi))in=1, the normalized matrices (W1/kW1 kF, . . . , WL/kWLkF) asymptotically solve
a maximum margin problem which demands all weight matrices be small, not merely their product:
max
WL∈R1×dL-1
kWL kF =1
max	minyi(^WL … Wι)xi.
W1∈Rd1×d0	i
kW1 kF =1
1
Published as a conference paper at ICLR 2019
(a) Margin maximization.
Figure 1: Visualization of margin maximization and self-regularization of layers on synthetic data
with a 4-layer linear network compared to a 1-layer network (a linear predictor). Figure 1a shows the
convergence of 1-layer and 4-layer networks to the same margin-maximizing linear predictor on pos-
itive (blue) and negative (red) separable data. Figure 1b shows the convergence of kWik2/kWikF
to 1 on each layer, plotted against the risk.
(b) Alignment and risk minimization.
The paper is organized as follows. This introduction continues with related work, notation, and
assumptions in Sections 1.1 and 1.2. The analysis of gradient flow is in Section 2, and gradient
descent is analyzed in Section 3. The paper closes with future directions in Section 4; a particular
highlight is a preliminary experiment on CIFAR-10 which establishes empirically that a form of the
alignment phenomenon occurs on the standard nonlinear network AlexNet.
1.1	Related work
On the implicit regularization of gradient descent, Soudry et al. (2017) show that for linear pre-
dictors and linearly separable data, the gradient descent iterates converge to the same direction as
the maximum margin solution. Ji & Telgarsky (2018) further characterize such an implicit bias for
general nonseparable data. Gunasekar et al. (2018) consider gradient descent on fully connected
linear networks and linear convolutional networks. In particular, for the exponential loss, assuming
the risk is minimized to 0 and the gradients converge in direction, they show that the whole network
converges in direction to the maximum margin solution. These two assumptions are on the gradient
descent process itself, and specifically the second one might be hard to interpret and justify. Com-
pared with Gunasekar et al. (2018), this paper proves that the risk converges to 0 and the weight
matrices align; moreover the proof here proves the properties simultaneously, rather than assuming
one and deriving the other. Lastly, Arora et al. (2018) show for deep linear networks (and later Du
et al. (2018) for ReLU networks) that gradient flow does not change the difference between squared
Frobenius norms of any two layers. We use a few of these tools in our proofs; please see Sections 2
and 3 for details.
For a smooth (nonconvex) function, Lee et al. (2016) show that any strict saddle can be avoided
almost surely with small step sizes. If there are only countably many saddle points and they are all
strict, and if gradient descent iterates converge, then this implies (almost surely) they converge to a
local minimum. In the present work, since there is no finite local minimum, gradient descent will go
to infinity and never converge, and thus these results of Lee et al. (2016) do not show that the risk
converges to 0.
There has been a rich literature on linear networks. Saxe et al. (2013) analyze the learning dynamics
of deep linear networks, showing that they exhibit some learning patterns similar to nonlinear net-
works, such as a long plateau followed by a rapid risk drop. Arora et al. (2018) show that depth can
help accelerate optimization. On the landscape properties of deep linear networks, Lu & Kawaguchi
(2017); Laurent & von Brecht (2017) show that under various structural assumptions, all local op-
tima are global. Zhou & Liang (2018) give a necessary and sufficient characterization of critical
points for deep linear networks.
2
Published as a conference paper at ICLR 2019
(b) Margin maximization at layer 1.
(a) Overall margin maximization.
principal ∞mponent 1 of input
(c) Margin maximization at layer 2.	(d) Margin maximization at layer 3.
Figure 2: A visualization of inter-layer alignment on data consisting of two well-separated circles
with a 3-layer linear network. Figure 2a depicts, as in Figure 1a, that optimizing 1- and 3-layer
linear networks finds the same maximum margin solution. The other three plots show the data as it
is mapped through progressively more and more layers. Due to alignment, the product Wi …Wi
becomes Uiu：, where Ui is the top left singular vector of Wi, which means that asymptotically the
mapped data will be well separated and lie along the span of ui , as depicted by the flattening in
Figures 2b to 2d. Additionally, these three subfigures show that the top right singular vector vi+1 of
the subsequent layer is aligned with this ui , which in these plots (with principal component axes)
corresponds to following a horizontal line.
1.2	Notation, setting, and assumptions
Consider a data set {(xi, yi)}in=1, where xi ∈ Rd, kxi k ≤ 1, and yi ∈ {-1, +1}. The data set is
assumed to be linearly separable, i.e., there exists a unit vector u which correctly classifies every data
point: for any 1 ≤ i ≤ n, yihu, xii > 0. Furthermore, let γ := maxkuk=1 min1≤i≤n yihu, xii > 0
denote the maximum margin, and U := argmax^k=] minι≤i≤n y“u,xi〉denote the maximum
margin solution (the solution to the hard-margin SVM).
A linear network of depth L is parameterized by weight matrices WL, . . . , W1, where Wk ∈
Rdk ×dk-1, d0 = d, and dL = 1. Let W = (WL, . . . , W1) denote all parameters of the network. The
(empirical) risk induced by the network is given by
nn
R(W) = R (Wl,...,Wi) = - X ' (yiWL …Wιxi) = - X ' (〈wprod,%)),
n i=1	n i=1
where wprod := (WL …Wi)>, and Zi := y%xi.
The loss ` is assumed to be continuously differentiable, unbounded, and strictly decreasing to 0.
Examples include the exponential loss 'eχp(x) = e-x and the logistic loss 'iog(x) = ln (1 + e-x).
Assumption 1. '0 < 0 is continuous, limχ→-∞ '(x) = ∞ and limχ→∞ '(x) = 0.
This paper considers gradient flow and gradient descent, where gradient flow W (t)t ≥ 0, t ∈ R
can be interpreted as gradient descent with infinitesimal step sizes. It starts from some W(0) at
3
Published as a conference paper at ICLR 2019
t = 0, and proceeds as
dWdtt) = -VR (W (t)).
By contrast, gradient descent W (t)t ≥ 0, t ∈ Z is a discrete-time process given by
W(t + 1) = W⑴-ηtVR (W(t)),
where ηt is the step size at time t.
We assume that the initialization of the network is not a critical point and induces a risk no larger
than the risk of the trivial linear predictor 0.
Assumption 2. The initialization W(0) satisfies VR (W(0)) = 0 and R (W(0)) ≤ R(0) = '(0).
It is natural to require that the initialization is not a critical point, since otherwise gradient
flow/deScent will never make a progress. The requirement R (W(0)) ≤ R(0) can be easily sat-
isfied, for example, by making Wι(0) = 0 and Wl(0)…W2(0) = 0. On the other hand, if
R W(0) > R(0), gradient flow/descent may never minimize the risk to 0. Proofs of those claims
are given in Appendix A.
2 Results for gradient flow
In this section, we consider gradient flow. Although impractical when compared with gradient
descent, gradient flow can simplify the analysis and highlight proof ideas. For convenience, we
usually use W, Wk, and wprod, but they all change with (the continuous time) t. Only proof sketches
are given here; detailed proofs are deferred to Appendix B.
2.1 Risk convergence
One key property of gradient flow is that it never increases the risk:
dRdW) =(VR(W), dW∖ = -kVR(W)k2 = - XX
dt	dt
k=1
∂R
∂Wk
2
≤ 0.
F
(1)
We now state the main result: under Assumptions 1 and 2, gradient flow minimizes the risk, Wk and
wprod all go to infinity, and the alignment phenomenon occurs.
Theorem 1. Under Assumptions 1 and 2, gradient flow iterates satisfy the following properties:
•	limt→∞ R(W) = 0.
•	For any - ≤ k ≤ L, limt→∞ kWk kF = ∞.
•	For any ≤ k ≤ L, letting (uk, vk) denote the first left and right singular vectors of Wk,
lim
t→∞
Wk	>
研而-Ukvk
= 0.
F
Moreover, for any- ≤ k < L, limt→∞ hvk+1, uki = -. As a result,
lim I * -LwProd-----------------------------,vι	1,
t→∞∣∖QL=1 kWk kF
and thus limt→∞ kwProd k = ∞.
Theorem 1 is proved using two lemmas, which may be of independent interest. To show the ideas,
let us first introduce a little more notation. Recall that R(W) denotes the empirical risk induced by
the deep linear network W . Abusing the notation a little, for any linear predictor w ∈ Rd , we also
use R(w) to denote the risk induced by w. With this notation, R(W) = R(wprod), while
1n	1n
VR(WProd) = - X ' (hWprod,Zii) Zi = - X ' (WL …WIzi) Zi
n i=1	n i=1
4
Published as a conference paper at ICLR 2019
is in Rd and different from VR(W), which has PL=I dkdk-1 entries, as given below:
∂R
=W = Wk + 1 …WL VR(WProd) WI …Wk-1∙
∂Wk
Furthermore, for any R > 0, let
B(R) = W max kWkkF ≤R .
1≤k≤L
The first lemma shows that for any R > 0, the time spent by gradient flow in B(R) is finite.
Lemma 1. Under Assumption 1 and 2, for any R > 0, there exists a constant (R) > 0, such that
for any t ≥ 1 and any W ∈ B(R), k∂R∕∂Wι∣∣F ≥ C(R). As a result, gradient flow spends a finite
amount of time in B(R) for any R > 0, and max1≤k≤L kWk kF is unbounded.
Here is the proof sketch. If kWkkF are bounded, then kVR(wprod)k will be lower bounded by a
positive constant, therefore if k∂R∕∂Wι kF = ∣∣ Wl .…W2∣∣∣∣VR(wprod)k can be arbitrarily small,
then k Wl .…W21 and ∣ Wprod ∣ can also be arbitrarily small, and thus R(W) can be arbitrarily close
to R(0). This cannot happen after t = 1, otherwise it will contradict Assumption 2 and eq. (1).
To proceed, we need the following properties of linear networks from prior work (Arora et al., 2018;
Du et al., 2018). For any time t ≥ 0 and any 1 ≤ k < L,
Wk>+1(t)Wk+1(t) - Wk>+1(0)Wk+1(0) = Wk(t)Wk>(t) - Wk(0)Wk>(0).
To see this, just notice that
W>+1 ∂W⅛
∂R
W>+1 ∙∙∙ W>VR(wprod)τW> ∙∙∙ W> = dW W>.
Taking the trace on both sides of eq. (2), we have
Wk+1(t)2F -Wk+1(0)2F =Wk(t)2F -Wk(0)2F.
(2)
(3)
In other words, the difference between the squares of Frobenius norms of any two layers remains a
constant. Together with Lemma 1, it implies that all kWkkF are unbounded.
However, even if kWk kF are large, it does not follow necessarily that kWprod k is also large.
Lemma 2 shows that this is indeed true: for gradient flow, as kWkkF get larger, adjacent layers
also get more aligned to each other, which ensures that their product also has a large norm.
For 1 ≤ k ≤ L, let σk, uk, and vk denote the first singular value (the 2-norm), the first left singular
vector, and the first right singular vector of Wk, respectively. Furthermore, define
L-1
D:= 1≤mka≤xLkWk(0)k2F - kWL(0)k2F+XWk(0)Wkτ(0)-Wkτ+1(0)Wk+1(0)	,
≤ ≤	k=1
which depends only on the initialization. If for any 1 ≤ k < L, Wk(0)Wkτ(0) = Wkτ+1(0)Wk+1(0),
then D = 0.
Lemma 2. The gradient flow iterates satisfy the following properties:
•	For any 1 ≤ k ≤ L, kWkk2F - kWkk22 ≤ D.
•	For any 1 ≤ k < L, <vk+ι, Uk)2 ≥ 1 — (D+kWk+i(O)k2+k Wk(O)k2)∕∣∣Wk+ι ∣∣2.
•	Suppose max1≤k≤L kWkkF → ∞, then wprod/QkL=1 kWkkF,v1 → 1.
The proof is based on eq. (2) and eq. (3). If Wk(0)Wkτ(0) = Wkτ+1(0)Wk+1(0), then eq. (2) gives
that Wk+1 and Wk have the same singular values, and Wk+1’s right singular vectors and Wk’s left
singular vectors are the same. If it is true for any two adjacent layers, since WL is a row vector, all
layers have rank 1. With general initialization, we have similar results when kWkkF is large enough
so that the initialization is negligible. Careful calculations give the exact results in Lemma 2.
An interesting point is that the implicit regularization result in Lemma 2 helps establish risk conver-
gence in Theorem 1. Specifically, by Lemma 2, if all layers have large norms, ∣ Wl …W21 will be
large. If the risk is not minimized to 0, kVR(Wprod)k will be lower bounded by a positive constant,
and thus ∣∂R∕∂Wi∣f = ∣∣Wl …W2∣∣∣∣VR(wprod)k will be large. Invoking eq. (1), Lemma 1 and
eq. (3) gives a contradiction. Since the risk has no finite optimum, kWkkF → ∞.
5
Published as a conference paper at ICLR 2019
2.2 Convergence to the maximum margin solution
Here We focus on the exponential loss 'eχp(x) = e-x and the logistic loss 'iog(x) = ln(1 + e-x).
In addition to risk convergence, these two losses also enable gradient descent to find the maximum
margin solution.
To get such a strong convergence, We need one more assumption on the data set. Recall that γ =
maxkuk=ι minι≤i≤n(u, Zii > 0 denotes the maximum margin, and U denotes the unique maximum
margin predictor which attains this margin γ. Those data points Zi for which (U, zii = Y are called
support vectors.
Assumption 3. The support vectors span the whole space Rd.
Assumption 3 appears in prior work Soudry et al. (2017), and can be satisfied in many cases: for
example, it is almost surely true if the number of support vectors is larger than or equal to d and the
data set is sampled from some density w.r.t. the Lebesgue measure. It can also be relaxed to the
situation that the support vectors and the whole data set span the same space; in this case VR(WProd)
will never leave this space, and we can always restrict our attention to this space.
With Assumption 3, we can state the main theorem.
Theorem 2. Under Assumptions 2 and 3, for almost all data and for losses 'eχp and 'iog,
then limt→∞∣(vι,ui∣ = 1, where vι is the first right singular vector of Wi. As a result,
limt→∞ wProd∕QL=ι kWkkF = U.
Before summarizing the proof, we can simplify both theorems into the following minimum norm
property mentioned in the introduction.
Corollary 1. Under Assumptions 2 and 3, for almost all data andfor losses 'eχp and 'iog,
min yi
i
WL
kWL kF
W1
W⅛J Xi
-----→ max .… max	minyi (AL .…Ai) Xi.
t→∞--AL∈R1×dL-1	A1∈Rd1×d0 i
kAL kF=i	kA1 kF=i
Theorem 2 relies on two structural lemmas. The first one is based on a similar almost-all argument
due to Soudry et al. (2017, Lemma 12). Let S ⊂ {1, . . . , n} denote the set of indices of support
vectors.
Lemma 3. Under Assumption 3, if the data set is sampled from some density w.r.t. the Lebesgue
measure, then with probability 1,
α := min max(ξ, Zii > 0.
∣ξ∣ = 1,ξ⊥U i∈S
Let U⊥ denote the orthogonal complement of SPan(U), and let Π⊥ denote the projection onto U⊥.
We prove that if kΠ⊥wk is large enough, gradient flow starting from w will tend to decrease kΠ⊥wk.
Lemma 4. Under Assumption 3, for almost all data, 'eχp and 'iog, and any W ∈ Rd, if (w, U)≥ 0
and kΠ⊥wk is larger than 1+ln(n)∕ɑ for 'eχp or 2n∕ea for 'iog, then (Π⊥w, VR(W)) ≥ 0.
With Lemma 3 and Lemma 4 in hand, we can prove Theorem 2. Let Π⊥ Wi denote the projection
of rows of Wi onto U⊥. Notice that
Π⊥wprod = (WL ...W2(∏⊥W1))> and dkπ⊥WIkF = -2(Π⊥Wprod, VR(Wprod)).
If kΠ⊥Wi kF is large compared with kWi kF, since layers become aligned, kΠ⊥Wprodk will also be
large, and then Lemma 4 implies that kΠ⊥Wi kF will not increase. At the same time, kWikF → ∞,
and thus for large enough t, kΠ⊥WikF must be very small compared with kWi kF. Many details
need to be handled to make this intuition exact; the proof is given in Appendix B.
3	Results for gradient descent
One key property of gradient flow which is used in the previous proofs is that it never increases
the risk, which is not necessarily true for gradient descent. However, for smooth losses (i.e, with
6
Published as a conference paper at ICLR 2019
Lipschitz continuous derivatives), we can design some decaying step sizes, with which gradient
descent never increases the risk, and basically the same results hold as in the gradient flow case.
Deferred proofs are given in Appendix C.
We make the following additional assumption on the loss, which is satisfied by the logistic loss `log .
Assumption 4. '0 is β-LiPschitz (i.e, ' is β-smooth), and ∣'0∣ ≤ G (i.e., ' is G-Lipschitz).
Under Assumption 4, the risk is also a smooth function of W, if all layers are bounded.
Lemma 5. Suppose ` is β-smooth. If R ≥ 1, then β(R) = 2L2R2L-2 (β + G), and R(W) is a
β(R)-smooth function on the set B(R) = {W∣ k Wk∣∣F ≤ R, 1 ≤ k ≤ L}.
Smoothness ensures that for any W,V ∈ B(R), R(W) - R(V) ≤ hVR(V),W - V〉+
β(R)kW -V k2/2 (see Bubeck et al. (2015) Lemma 3.4). In particular, if we choose some R and
set a constant step size η = 1∕β(R), then as long as W(t + 1) and W(t) are both in B(R),
R (W (t + 1)) -R (W (t)) ≤ DVR (W (t)), -ηtVR (W (t)))+ β(R2)η2∣∣VR (W (t))∣∣2
=-2β⅛ ∣∣VR (W ⑴)∣∣2=- ηt ∣∣VR (W (<・	⑷
In other words, the risk does not increase at this step. However, similar to gradient flow, the gradient
descent iterate will eventually escape B(R), which may increase the risk.
Lemma 6. Under Assumption 1, 2 and 4, suppose gradient descent is run with a constant step size
1∕β(R). Then there exists a time t when W(t) ∈ B(R), in other words, maxι≤k≤L ∣∣ Wk (t)∣∣F > R.
Fortunately, this issue can be handled by adaptively increasing R and correspondingly decreasing
the step sizes, formalized in the following assumption.
Assumption 5. The step size η = min{1∕β(Rt), 1}, where Rt satisfies W(t) ∈ B(Rt 一 1), and if
W(t + 1) ∈ B(Rt - 1), Rt+1 = Rt.
Assumption 5 can be satisfied by a line search, which ensures that the gradient descent update is not
too aggressive and the boundary R is increased properly.
With the additional Assumptions 4 and 5, exactly the same theorems can be proved for gradient
descent. We restate them briefly here.
Theorem 3.	Under Assumption 1, 2, 4, and 5, gradient descent satisfies
∙ limt→∞ R (W(t)) =0.
•	For any 1 ≤ k ≤ L, limt→∞ ∣Wk (t)∣F = ∞.
•	limt→∞ wprod(t)/QkL=1 kWk(t)kF, v1(t) = 1, where v1(t) is the first right singular vector
ofW1(t).
Theorem 4.	Under Assumptions 2, 3 and 5, for the logistic loss `log and almost all data,
limt→∞∣hvι(t),ui∣ = 1, and limt→∞ wprod⑴/QL=ι kWk(t)kF = U.
Corollary 2. Under Assumptions 2, 3 and 5, for the logistic loss `log and almost all data,
WL	W1
min yi nɪʊ H------∣∣T" II	Xi -------→ max ∙∙∙ max min yi(AL •…Ai) Xi.
i	∣WL∣F	∣W1∣F	t→∞	AL∈R1×dL-1	A1∈Rd1×d0 i
kAL kF=1	kA1 kF=1
Proofs of Theorem 3 and 4 are given in Appendix C, and are basically the same as the gradient flow
proofs. The key difference is that an error of Pt∞=0 ηt2 ∣VR(W (t))∣2 will be introduced in many
parts of the proof. However, it is bounded in light of eq. (4):
∞	2∞	2
Xη2∣∣VR(W(t))∣∣ ≤ Xηt∣∣VR(W(t))∣∣ ≤ 2R(w(0)).
t=0	t=0
Since all weight matrices go to infinity, such a bounded error does not matter asymptotically, and
thus proofs still go through.
7
Published as a conference paper at ICLR 2019
(a) Default initialization.
(b) Initialization with the same Frobenius norm.
Figure 3: Risk and alignment of dense layers (the ratio kWik2/kWi kF) of (nonlinear!) AlexNet on
CIFAR-10. Figure 3a uses default PyTorch initialization, while Figure 3b forces initial Frobenius
norms to be equal amongst dense layers.
4 Summary and future directions
This paper rigorously proves that, for deep linear networks on linearly separable data, gradient flow
and gradient descent minimize the risk to 0, align adjacent weight matrices, and align the first right
singular vector of the first layer to the maximum margin solution determined by the data. There are
many potential future directions; a few are as follows.
Convergence rates and practical step sizes. This paper only proves asymptotic convergence,
moreover for adaptive step sizes depending on the current weight matrix norms. A refined analysis
with rates for practical step sizes (e.g., constant step sizes) would allow the algorithm to be compared
to other methods which also globally optimize this objective, would suggest ways to improve step
sizes and initialization, and ideally even exhibit a sensitivity to the network architecture and suggest
how it could be improved.
Nonseparable data and nonlinear networks. Real-world data is generally not linearly separable,
but nonlinear deep networks can reliably decrease the risk to 0, even with random labels (Zhang
et al., 2017). This seems to suggest that a nonlinear notion of separability is at play; is there some
way to adapt the present analysis?
The present analysis is crucially tied to the alignment of weight matrices: alignment and risk are an-
alyzed simultaneously. Motivated by this, consider a preliminary experiment, presented in Figure 3,
where stochastic gradient descent was used to minimize the risk of a standard AlexNet on CIFAR-10
(Krizhevsky et al., 2012; Krizhevsky & Hinton, 2009).
Even though there are ReLUs, max-pooling layers, and convolutional layers, the alignment phe-
nomenon is occurring in a reduced form on the dense layers (the last three layers of the net-
work). Specifically, despite these weight matrices having shape (1024, 4096), (4096, 4096), and
(4096, 10) the key alignment ratios kWi k2 /kWi kF are much larger than their respective lower
bounds (1024-1/2 , 4096-1/2, 10-1/2). Two initializations were tried: default PyTorch initializa-
tion, and a Gaussian initialization forcing all initial Frobenius norms to be just 4, which is suggested
by the norm preservation property in the analysis and removes noise in the weights.
Acknowledgements
The authors are grateful for support from the NSF under grant IIS-1750051. This grant allowed
them to focus on research, and when combined with an NVIDIA GPU grant, led to the creation of
their beloved GPU machine DutchCrunch.
8
Published as a conference paper at ICLR 2019
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.
Peter Bartlett, Dylan Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. NIPS, 2017.
Sebastien BUbeck et al. Convex optimization: Algorithms and complexity. Foundations and
Trends® in Machine Learning, 8(3-4):231-357, 2015.
Simon S DU, Wei HU, and Jason D Lee. Algorithmic regUlarization in learning deep homogeneoUs
models: Layers are aUtomatically balanced. arXiv preprint arXiv:1806.00900, 2018.
SUriya GUnasekar, Jason Lee, Daniel SoUdry, and Nathan Srebro. Implicit bias of gradient descent
on linear convolUtional networks. arXiv preprint arXiv:1806.00468, 2018.
Ziwei Ji and MatUs Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300, 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning mUltiple layers of featUres from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya SUtskever, and Geoffery Hinton. Imagenet classification with deep convolU-
tional neUral networks. In NIPS, 2012.
Thomas LaUrent and James von Brecht. Deep linear neUral networks with arbitrary loss: All local
minima are global. arXiv preprint arXiv:1712.01473, 2017.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges
to minimizers. arXiv preprint arXiv:1602.04915, 2016.
Haihao LU and Kenji KawagUchi. Depth creates no bad local minima. arXiv preprint
arXiv:1702.08580, 2017.
Andrew M Saxe, James L McClelland, and SUrya GangUli. Exact solUtions to the nonlinear dynam-
ics of learning in deep linear neUral networks. arXiv preprint arXiv:1312.6120, 2013.
Daniel SoUdry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. arXiv preprint arXiv:1710.10345v3, 2017.
ChiyUan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning reqUires rethinking generalization. ICLR, 2017.
Yi ZhoU and Yingbin Liang. Critical points of linear neUral networks: Analytical forms and land-
scape properties. 2018.
9
Published as a conference paper at ICLR 2019
A Regarding Assumption 2
Suppose Wι(0) = 0 while Wl(0) •…W2(0) = 0. First of all, Wl(0) •…Wι(0) = 0 and thus
R (W(0)) = R(0). Moreover,
1n
(▽R (WProd(0)) ,U∣ = - X ' (0)hZi, U〉≤ ' (0)γ < 0,
n i=1
which implies ▽R (WProd(0)) =0 and ∂R∕∂W1 = (Wl(0)…W2 (0)) > VR (Wprod (0))> = 0.
On the other hand, if R W(0) > R(0), gradient flow/descent may never minimize the risk to
0. For example, suppose the network has two layers, and both the input and output have dimen-
sion 1; the network just computes the dot product of two vectors W1 and W2. Consider mini-
mizing R(W1, W2) = exp -hW1, W2i . If W1(0) = -W2 (0) 6= 0, then R W1 (0), W2 (0) =
exp kW1 k2 > exp(0). It is easy to verify that for any t, W1(t) = -W2(t), and R W1(t), W2(t) ≥
exp(0) > 0.
B Omitted proofs from Section 2
Proof of Lemma 1. Fix an arbitrary R > 0. If the claim is not true, then for any > 0, there exists
some t ≥ 1 such that kWkkF ≤ R for all k while ∂R∕∂W1 2F ≤ 2, which means
22
= ∣∣W> ∙∙∙ W>VR(wprod)>L = kWL …W2k2∣∣VR(wprod)∣∣2 ≤ e2.
Since kWProd k ≤ RL, we have
1n	1n
hVR(WProd), Ui = —): ' (hwprod, zii) hzi, ui ≤ —): ' (hwprod, Ziy) Y ≤ -Mγ,
n i=1	n i=1
∂R
∂W
where -M = max-RL≤χ≤RL '0(χ). Since ' is continuous and the domain is bounded, the max-
imum is attained and negative, and thus M > 0. Therefore ∣VR(Wprod)∣ ≥ Mγ, and thus
IlWL …W2k ≤ e∕Mγ. Since ∣∣Wi∣∣f ≤ R, We further have ∣∣Wprodk ≤ cR∕Mγ. In other words,
after t = 1, ∣Wprod ∣ may be arbitrarily small, which implies R Wprod can be arbitrarily close to
R (0).
On the other hand, by Assumption 2, dR(W)∕ dt = -IVR(W)I2 < 0 at t = 0. This implies that
R (W (1)) < R (W (0)),andfor any t ≥ 1, R (W (t)) ≤R (W (1)) < R (W (0)) ≤ R(0), which
is a contradiction.
Since the risk is always positive, we have
R
∞L
(W (O)) ≥∕=0 X
∂R
∂Wk
dt
∞
≥
t=0
∞
≥
t=0
∂R
∂W
∂R
∂W1
dt
F
2
1 max IWk IF ≤ R
F	1≤k≤L
dt
2
F
2
max IWk IF ≤ R dt
1≤k≤L
≥ (R)2 Z ∞1
t=1
max IWk IF ≤ R dt,
1≤k≤L
which implies gradient flow only spends a finite amount of time in W max1≤k≤L IWkIF ≤ R .
This directly implies that maxι≤k≤L k Wk ∣∣f is unbounded.	□
10
Published as a conference paper at ICLR 2019
Proof of Lemma 2. The first claim is true for k = L since WL is a row vector. For any 1 ≤ k < L,
recall that Arora et al. (2018); Du et al. (2018) give the following relation:
Wk>+1(t)Wk+1(t) - Wk>+1(0)Wk+1(0) = Wk(t)Wk>(t) - Wk(0)Wk>(0).	(5)
Let Ak,k+1 = Wk(0)Wk>(0) - Wk>+1(0)Wk+1(0). By eq. (5) and the definition of singular vectors
and singular values, we have
σk2 ≥ vk>+1WkWk>vk+1
= vk>+1Wk>+1Wk+1vk+1 + vk>+1Ak,k+1vk+1
= σk2+1 + vk>+1Ak,k+1vk+1
≥ σk2+1 - kAk,k+1 k2.	(6)
Moreover, by taking the trace on both sides of eq. (5), we have
kWkk2F =trWkWk> = tr Wk>+1Wk+1 +trWk(0)Wk>(0) -trWk>+1(0)Wk+1(0)
= kWk+1k2F+kWk(0)k2F - kWk+1(0)k2F.	(7)
Summing eq. (6) and eq. (7) from k to L - 1, we get
L-1
kWkk2F - kWkk22 ≤ kWk(0)k2F - kWL(0)k2F + X kAk0,k0+1k2 ≤ D.	(8)
k0=k
Next we prove that singular vectors get aligned. Consider uk>Wk>+1Wk+1uk. On one hand, similar
to eq. (6), we can get that
uk>Wk>+1Wk+1uk = uk>WkWk>uk - uk>Wk(0)Wk>(0)uk + uk>Wk>+1 (0)Wk+1 (0)uk
≥ uk>WkWk>uk - uk>Wk (0)Wk> (0)uk
≥ σk2 - kWk(0)k22.
On the other hand, it follows from the definition of singular vectors and eq. (8) that
uk>Wk>+1Wk+1uk = huk, vk+1i2σk2+1 +uk> Wk>+1Wk+1 - vk+1σk2+1vk>+1 uk
≤ huk, vk+1i2σk2+1 + kWk+1k2F - kWk+1 k22
≤ huk , vk+1 i σk+1 + D.
Combining eq. (9) and eq. (10), we get
σk2 ≤ huk, vk+1i2σk2+1 + D + kWk(0)k22.
Similar to eq. (9), we can get
σk2 ≥ vk>+1WkWk>vk+1 ≥ σk2+1 - kWk+1(0)k22.
Therefore
σ ≥ 1	kWk+1(0)k2
σ2+1 —	σ2 + 1	.
Combining eq. (11) and eq. (12), we finally get
(9)
(10)
(11)
(12)
huk, vk+1i2 ≥ 1 -
D + kWk (0)k2 + kWk+ι(0)k2
σ22+ι
Regarding the last claim, first recall that since the difference between the squares of Frobenius
norms of any two layers is a constant, max1≤k≤L kWk kF → ∞ implies kWk kF → ∞ for any k.
We further have the following.
•	Since kWkk2F - kWkk22 ≤ D, kWkk2 → ∞ for any k, and Wk/kWkkF → ukvk>.
11
Published as a conference paper at ICLR 2019
•	Since IlWkIl2 → ∞, |〈〃k,vk+ι)| → 1.
As a result,
∣(Q⅛ "Hm a，"
→ ∣*∏ Uivj,vι) 1
→ 1.
□
ProofofTheorem 1. Suppose for some e > 0, R (W) ≥ E for any t. Then there exists some
1 ≤ j ≤ n such that ' (hwprod,zj)) ≥ e, and thus (WPrOd, zj ≤ '-1(e). On the other hand,
since R(W) ≤ R(0) = '(0), ' ((wprod,zj>) ≤ n'(0), and thus (wprod,zj〉≥ '-1 (n'(0)). Let
-M = maxe-ι(n'(0))≤x≤'-i(e∕n) ,(X) < 0, We have for anyt,
1 n
〈▽R(WPrOd), ui = 一〉： ' (hwprod, zii) hzi, ui
n i=1
1 n
≤ -): ' (hwprod, zii) Y
i=1
≤ —' (hwprod, zji) Y
≤-Mγ< 0,
n
and thus ∣∣VR(WPrOd)Il ≥ Mγ∕n.
Similar to the proof of Lemma 2, we can show that if ∣ Wk ∣∣f → ∞,
I / (WL …W2)>	+ I I
I ∖kWk”∙∙∙kW2∣∣F,v2/ ∣→ .
In other words, there exists some C > 0, such that when min1≤k≤L IlWk IlF > C, ∣Wl ∙∙∙ W21 ≥
IlWkIIF ∙∙∙∣∣W2∣∣F∕2 >C l∕2.	——
Lemma 1 shows that gradient flow spends a finite amount of time in {W ∣ max1≤k≤L IlWk IlF ≤ R}
for any R > 0. Since the difference between the squares of Frobenius norms of any two layers is a
constant, gradient flow also spends a finite amount of time in {W∣ min1≤k≤L IlWkkF ≤ C}. Now
we have
r∞ Jl	∂r 2 R(W(O)) ≥ /_0XdWkFdt 八=0k=1	k F f∞	∂R 2 , ≥ /=0 和 F r∞ =L OkWL ∙∙∙ W2k2 IVR(Wprod)I2 dt ∞Q> ≥/ OkWL ∙∙∙ W2k2kVR(Wprod)k21 ≥ (MYY(CL!2 广 1 [w min n ∖ n J ∖2 ) Λ=0	L 1≤k≤L	W min IWk IF > C dt 1≤k≤L IWk IF > C dt
∞,
12
Published as a conference paper at ICLR 2019
which is a contradiction. Therefore R() → 0. This further implies kWkkF → ∞, since R(W) has
no finite optimum. Finally, invoking Lemma 2 proves the final claim of Theorem 1.	□
Proof of Lemma 3. Soudry et al. (2017) Lemma 12 proves that, with probability 1, there are at most
d support vectors, and moreover, the i-th support vector zi has a positive dual variable αi, such that
Pi∈S αizi = u.
Suppose there exists some ξ ⊥ u, such that maxi∈shξ, Zii ≤ 0. Since

αihξ, zii
i∈S
αizi
i∈S
hξ,ui = 0,
we actually have hξ, zi i = 0 for all i ∈ S. This is impossible under Assumption 3, since the support
vectors span the whole space.	□
Proof of Lemma 4. For the sake of presentation, we leave out the subscript in zi and denote a data
point by z generally. For any data point z and predictor w, let z⊥ and w⊥ denote their projection
onto u⊥. Let z0 ∈ arg maxi∈sh-w⊥, z)，and thus h-w⊥, z0i ≥ α∣∣w⊥k.
For 'eχp, We have
hw⊥, VR(w)i = X n h- exp (—〈w, z))i hw⊥, z⊥i
z
=X n [exp (h-w, z))i h—w⊥, z⊥i
z
≥ nnexp (h-w,z0i) h-w⊥,z⊥i + X IeXP (h-w, z)) h-w⊥,z⊥).
hz⊥,w⊥i≥0
The first part can be lower bounded as below (recall that h-w⊥, z⊥0 i = h-w⊥, z0i ≥ αkw⊥ k)
IeXP (h-w,z0i) h-w⊥,z⊥i = IeXP (h-w, γu)) exp (h-w⊥,z⊥)) h-w⊥,z⊥)
≥ ɪ exp (-hw, Yui) exp (α∣∣w⊥∣∣) α∣∣w⊥∣∣.
(13)
(14)
To bound the second part, first notice that since we assume hw, u) ≥ 0, for any z,
hw, z - Yui = hw, z⊥) + hw, z - γu - z⊥) ≥ hw, z⊥) = hw⊥, z⊥).	(15)
The reason is that every data point has margin at least γ, and thus Z - Yu - z⊥ = Cu for some C ≥ 0.
Using eq. (15), we can bound the second part of eq. (13).
X IeXP (h-w,zi) h-w⊥,z⊥i
hz⊥,w⊥i≥0
= X IeXP (h-w, Yui) exp (h-w,z - Yui) h-w⊥,z⊥i
hz⊥,w⊥i≥0
≥ X IeXP (h-w,Yui) exp (h-w⊥,z⊥) h-w⊥,z⊥i
hz⊥,w⊥i≥0
≥ X 1exp(h-w,Yui) (-e)
hz⊥,w⊥i≥0
≥ exp (h-w,Yui) (-1).
(16)
On the third line eq. (15) is applied. The fourth line applies the property that f(x) = -xe-x ≥ -1/e
when x ≥ 0.
13
Published as a conference paper at ICLR 2019
Combining eq. (13), eq. (14) and eq. (16), we get
(w⊥, VR(W)) ≥ exp (〈-w, Yui) (— exp (α∣w⊥ ∣∣) α∣w⊥ ∣∣- ɪ).
As long as ∣∣w⊥∣ ≥ (1 + ln(n))∕α, (w⊥, VR(w)) ≥ 0.
For 'iog, similar to eq. (13), we have
/	1 exp (W	0	L 1 exp (h-w,zi) /	∖
hw⊥, VR(W)i ≥ n 1+exp (〈-w,z，i) h-w⊥,z⊥i [2i≥0 n 1+exp ((-w,z))""
、1 exp ((-w,z))
≥ —
―n 1 + exp
(h-w,z`
〈-w⊥,z⊥i+	^X	nexp (h-w,zi) h-w⊥,z⊥i.
<z⊥,w⊥>≥0
(17)
The second part of eq. (17) can be bounded again by eq. (16). To bound the first part of eq. (17),
first notice that (recall (w, u) ≥ 0)
exp (〈一w,z，〉)= exp ((-w,γu)) exp ((-w⊥,z⊥〉)≤ exp ((-w⊥,z⊥)).	(18)
Using eq. (18), and recall that (-w⊥,z⊥)= (-w⊥, z0) ≥ α∣w⊥∣ ≥ 0, we can bound the first part
of eq. (17) as below.
1 exp (h-w,z'i)	/	0 ∖ __ 1	∕/ N
~τ~. T1	K h-w⊥,z⊥i = —exp (.∖-w,7u∕)
n 1 + exp ((-w,z0))	n
≥ IeXP ((-w,γU))
exp ((-w⊥,z⊥i)
1 +exp ((-w,z0i)
exp ((-w⊥,z⊥i)
h-w⊥,z'⊥ i
1 +exp (h-w⊥,z⊥))
h-w⊥,z'⊥ i
≥ 2~ exp (〈-w, ”〉)h-w⊥,z⊥)
≥ 2- exp (〈一w, ”〉)α∣∣w⊥∣.
Combining eq. (17), eq. (19) and eq. (16), We get
(19)
hw⊥, VR(W) i≥ exp(〈-w，涧)(⅛αkw⊥k-I
As long as ∣∣w⊥k ≥ 2n∕eα, hw⊥, VR(w)) ≥ 0.
□
ProofofTheorem 2. Recall that
dWι
------------------------=
dt
∂ R	T	T	T
而T = -W2 ∙ ∙ ∙ WL VR(WPrOd),
∂ W i
—
and thus
d∣∣Wι∣∣F
dt
dW1
dt = = -2hwprod, VR(WPrOd)i.
(20)
Let ∏u denote the projection onto span(u), and let Π⊥ denote the projection onto u⊥. Also let
∏u Wi and Π⊥Wι denote the projection of rows of Wi onto span(u) and u⊥, respectively. Notice
that
∏u wprod = (WL ∙∙∙ W2(ΠβW1))τ , and ∏⊥wprod = (WL ∙∙∙ W2(Π⊥W1))τ .
We further have
d∣Π⊥WIkF = -2(Π⊥Wprod, VR(Wprod)).
(21)
Let Wi = uiσ1v> + S. We have ∣∣S∣2 ≤ σ1,2 ≤	≤ VZkWIkF - ∣Wι∣2 ≤ √D, where
σ1,2 is the second singular value of Wi and D is the constant introduced in Lemma 2. Then
Π⊥Wι = u1σι (Π⊥vι)τ + Π⊥S,
14
Published as a conference paper at ICLR 2019
and
k∏⊥WιkF ≤ ∣∣uισι (Π⊥vι)>L + k∏⊥S∣∣f = σ"∣Π⊥vj∣ + k∏⊥S∣∣f ≤ σ"∣Π⊥vιk + √dD.
It follows that
k∏⊥ vιk≥≡⊥W⅛ -巫 ≥
σ1	σ1
k∏⊥WιkF	√dD
---------------.
kWlkF---kWlk2
(22)
Fix an arbitrary > 0. By Theorem 1, we can find some t0 large enough such that for any t ≥ t0:
1.	√dD∕kW1k2 ≤ "3.
2.	kwprod/k Wl kF …kWlkF - Vlk ≤ e/3, or kwProd∕kWL kF …kWlkF + vι∣∣ ≤ e/3.
3.	k WLIIF ∙∙∙∣∣ Wi∣∣f ≥ 3K∕e, where K is the threshold given in Lemma 4, i.e., 1+ln(n)∕α
for 'exp, 2n/eα for `log .
4.	R(W) ≤ '(0)∕n, which implies (wprod, zQ ≥ 0 for all 1 ≤ i ≤ n. By Lemma 3, there
always exists a support vector Z for which h∏⊥wprod, Zi ≤ 0, and therefore(wprod, Ui ≥ 0.
Suppose for some t ≥ t0, kΠ⊥W1 kF ∕kW1 kF ≥ . By eq. (22) and bullet 1 above, kΠ⊥v1 k ≥
2e∕3. Bullet 2 above then gives kπ⊥wprod∕kWL∣∣F kW1∣∣F∣∣ ≥ e/3, which together with bullet 3
above implies ∣∣∏⊥wprod∣∣ ≥ K. Since also hwprod, Ui ≥ 0, we can apply Lemma 4 and get that
h∏⊥Wprod, VR(wprod)i ≥ 0. In light ofeq. (21), d∣Π⊥Wι∣F∕ dt ≤ 0.
On the other hand, since after t ≥ t0, hwprod, Zii ≥ 0, we have dkW1 k2F∕ dt ≥ 0 by eq. (20).
Therefore kΠ⊥W1 kF ∕kW1 kF will not increase, and since kW1 kF → ∞, it will eventually drop
below e, and will never exceed e again. Therefore,
Since e is arbitrary, we have
lim sup
t→∞
k∏⊥ Wi∣f
IIWlkF
≤ e.
lim sup
t→∞
k∏⊥ WiIf
IIWIkF
0,
and thus limt→∞ ∣hvι,ui∣ = 1. An application of Theorem 1 gives the other part of Theorem 2.
□
Proof of Corollary 1. By Theorem 2,
WL	W1	>
m yi (丽而…解而尸i m S i
Next, pick arbitrary unit vectors ai ∈ Rdi , and note
max .… max	miny% (AL .…Ai) Xi ≥ miny% ((la>-1)(aL-1a>-2) ∙∙∙ (a2a>)(aιU>)) Xi
A1∈Rd1×d0	AL∈R1×dL-1 i	i
kA1 kF=1	kAL kF=1
=min yiU> Xi.
i
On the other hand, if matrices (AL, . . . , A1) are feasible, then
IlAL …Aik2 ≤ IlALll2 …kA2k2kAlkF ≤ IlALkF …IlAIkF ≤ 1,
whereby
…	4、	…	…	(AL…Ai
miin yi(AL …Ai)xi ≤ IIAl …Ai k2 min y (①...Al k?
≤ 1 ∙ max min y%w>Xi
kwk2=i i i i
=min yiU>Xi.
i
Xi
□
15
Published as a conference paper at ICLR 2019
C Omitted proofs from Section 3
ProofofLemma 5. Given W, V ∈ B(R), we need to show that kVR(W )-VR(V )k ≤ β(R)∣∣W -
Vk for some β(R).
Consider k = 1 first. Let w =
kVR(w)k, kVR(v)k ≤ G. We have
I ∂W- ∂V1∣=IIW
(Wl …Wι)>, and V = (VL …V1)τ. Since |'0|
…W>VR(w) - V> …V> VR(v)
≤ G,
≤ 1 W> …WT VR(w) - VTWT ∙∙∙ WT VR(w)
+1VTWT …WT VR(w) - VT …VT VR(V)
≤ RL-2GkW2 -V2k
十 ∣VT WT …WTVR(W) - VT …VTVR(V) I
≤ RL-2GkW -Vk
+ IVTWT--WTVR(W)- VT ∙∙∙ VTVR(V)∣∣.
Proceeding in this way, we can get
(23)
∂ R
∂W1
∂R
≤ (L - I)RL-2GkW - Vk + RLTkVR(W)- VR(v)∣∣.
∂V1
(24)
—
Since kzi k ≤ 1, `0 is β-Lipschitz, we have
kVR(w) - VR(v)k ≤ βkw - vk ≤ βLRL-1kW - Vk,
(25)
where the last inequality follows from a similar one-by-one replacement procedure as in eq. (23).
Combining eq. (24) and eq. (25), we get for R ≥ 1,
票-dR ≤ ((L - I)RL-2G + βLR2L-2) kW - Vk ≤ 2LR2L-2(β + G)∣∣W - V∣∣.
I	∂ W1	∂ V1 I
The same procedure can be done for other layers, and together
kVR(W) - VR(V)k ≤ 2L2R2L-2(β + G)kW - Vk.
□
ProofofLemma 6. Recall that if W(t), W(t + 1) ∈ B(R) and η = 1∕β(R),
R(W(t + 1))-R(W(t)) ≤ hVR(W(t)),-ηtVR(W(t))i + β(Rη2∣∣VR(W(t))∣∣2
=-2β⅛∣∣VR (W ⑴)∣∣2
=-η2t∣∣VR (W(t))∣∣2.	(26)
Suppose W(t) ∈ B(R) for all t. By Assumption 2 and eq. (26),
R (W (1)) ≤R (W (0)) - -ξ1Hr∣VR (W (0)) ∣∣2 < R (W (0)).
2β (R)
By eq. (26), gradient descent never increases the risk, and thus for all t ≥ 1, R W(t) ≤
R W(1) < R W(0) . In exactly the same way as in the proof of Lemma 1, one can show
that there exists some constant E(R) > 0, so that ∣∣∂R∕∂Wι(t)∣∣F ≥ E(R) for all t. Invoking
eq. (26) again, we will get
∞1
R (w (O)) ≥ X 2β(R)E(R) =∞,
which is a contradiction. Therefore W(t) must go out of B(R) at some time.	口
16
Published as a conference paper at ICLR 2019
Next we prove Theorem 3 and 4. The proofs depend on several lemmas which are similar to the
gradient flow ones. The following Lemma 7 is similar to Lemma 1.
Lemma 7. Under Assumption 1, 2, 4, and 5, gradient descent ensures that
•	max1≤k≤L kWk (t)kF is unbounded.
∞
•	t=0 ηt = ∞.
•	For any R > 0, Pt:W (t)∈B(R) ηt < ∞.
Proof. By Assumption 5, we always have that W(t) ∈ B(Rt). Since β(Rt) = 2L2Rt2L-2(β+G) ≥
RtL-1G, we have for any 1 ≤ k ≤ L,
kWk(t + 1)kF ≤ kWk(t)kF + ηt
∂ R
∂Wk ⑴ F
≤ kWk⑴kF + β(Rt) ∂Wk(t)
≤kWk⑴ kF + β(Rt) RLTG
≤ kWk(t)kF+1.
(27)
Moreover, Lemma 6 shows that Rt → ∞. Since Rt+1 = Rt as long as W(t+1) ∈ B(Rt -1),
max1≤k≤L kWk(t)kF is unbounded.
It then follows that for any t, by Cauchy-Schwarz,
(Xdητ∣ (XXητ卜R(W(T))『)≥ IXdητ卜R(W(T川)→∞.
τ=0	τ=0	τ=0
since by eq. (26),
t-1	2
Xητ∣∣VR (W(τ))∣∣ ≤ 2R (W(0)) - 2R (W⑴)≤ 2R (W(0)),
τ=0
we have	t∞=0 ηt = ∞.
Since under Assumptions 4 and 5 gradient descent never increases the risk, it can be shown in exactly
the same as in the proof of Lemma 1 that, for W(t) ∈ B(R), k∂R∕∂Wι(t)kF ≥ E(R) for some
constant E(R) > 0. Invoking eq. (26) again, We get that PfW(t)∈B(R) ηt < ∞.	□
The next lemma is an analogy to Lemma 2.
Lemma 8. Under Assumption 1 and 4, the gradient descent iterates satisfy the following properties:
•	Forany 1 ≤ k ≤ L, kWkkF -kWkk2 ≤ D + 2R (W(0)).
•	For any1 ≤ k < L, hvk+1, uki2 ≥1 - D+3R(W (0))+kWk+1 (0)k22 +kWk (0)k22/kWk+1 k22.
•	Suppose max1≤k≤L kWkkF → ∞, thenwprod/QkL=1 kWkkF,v1 → 1.
Proof. Recall that for any W,
∂R	∂R
Wk+ι^7R- = Wk+ι …W>VR(wprod)>W> …W> = -- W>.	(28)
∂Wk+1	∂Wk
For gradient descent iterates, summing eq. (28) from 0 to t- 1, We get
Wk+1 ⑴Wk+1 ⑴-Wk+1(O)Wk+1(O) + X 格(∂Wk⅛))	(dWkRT))
=Wk ⑴ WXt)-Wk(O)W>(0)+X 褚(鹏)(∂W⅛ )丁.	(29)
17
Published as a conference paper at ICLR 2019
For any 1 ≤ k ≤ L and any t, let
t-1
Pk(t) =Xητ2
τ=0
∂ R	)( ∂ R )>
∂Wk(τ) ) (∂Wk(T))
and
t-1
Qk(t) = X ητ2
τ=0
(∂ R y
IdWk (T )J
∂ R	ʌ
dWk(T)).
We have kPk(t)k2 = kQk(t)k2 ≤ tr Qk(t) = tr Pk(t) . Moreover, invoking eq. (26),
L	L t-1
X tr (Pk ⑻=XX ηT
k=1	k=1 τ=0
2
F
∂R
∂Wk (τ)
t-1	2
=X ηT∣VR (W (τ 川
τ=0
t-1
≤ Xητ∣∣VR (W(τ))
τ=0
≤ 2R (W(0)) — 2R (W(t))
≤ 2R (W(0)).
(30)
Still let σk(t), uk(t) and vk(t) denote the first singular value, left singular vector and right singular
vector of Wk (t). We can then proceed basically in the same way as in the proof of Lemma 2. For
example, eq. (6) becomes
σk(t) ≥ σk + 1(t) - kAk,k+1(t)k2 - kPk (t)k2 ≥ σk + 1(t) - kAk,k+1(t)k2 - tr (Pk ⑴),(31)
while eq. (7) becomes
kWk(t)kF = kWk+ι(t)kF + kWk(0)kF -kWk+ι(0)kF - tr (Pk(t)) + tr (Qk+ι(t)) .	(32)
Summing eq. (31) and eq. (32) from k to L - 1, and invoke eq. (30), we get
L-1
kWk(t)kF - kWk(t)k2 ≤ D - tr (Pk(t)) + tr (QL(t)) + X tr (Pk，(t)) ≤ D + 2R (W(0)).
k0=k
To prove singular vectors get aligned, we can still proceed in nearly the same way as in the proof of
Lemma 2. eq. (9) becomes
uk>Wk>+1Wk+1uk ≥ σk2 - kWk(0)k22 - kQk+1(t)k2,
while eq. (10) becomes
u> Wk+1 Wk+1uk ≤ huk, vk+1i2σ2+ι + D + 2R (w(0)).
Combining eq. (33) and eq. (34)
σ2 ≤ huk ,vk + 1i2 σ2 + 1 + D + 2R (W (O)) + ∣∣Qk+l(t)∣∣2 + IlWk (0)k2 .
Similar to eq. (33), we can get
σk2 ≥ vk>+1WkWk>vk+1 ≥ σk2+1 - kWk+1(0)k22 - kPk(t)k2,
and thus eq. (12) becomes
σk 、1	kWk+ι(0)k2 + IlPk(t)k2
C ≥1	σk+	.
Combining eq. (35) and eq. (36), we get
(33)
(34)
(35)
(36)
huk, vk+1i2 ≥ 1 -
D + kWk(0)k2 + ∣Wk+ι(0)k2 + 3R (W(0))
σ2+ι
The final claim of Lemma 8 can be proved in exactly the same way as Lemma 2.	□
18
Published as a conference paper at ICLR 2019
Proof of Theorem 3. Summing eq. (32), we know that for any two different layers j > k,
kWk(t)kF -kWj(t)kF = kWk(0)kF -kWj(0)kF - tr (Pk(t)) + tr (Qj(t)).
Recall eq. (30), we know that
∣(kWk(t)kF-kWj(t)kF) - (kWk(0)kF-kWj(0)kF)∣ ≤2R(W(0)).	(37)
In other words, the difference between the squares of Frobenius norms of any two layers is still
bounded.
The proof then goes in the same way as the proof of Theorem 1. Suppose the risk is always above
e > 0. Then there exists some C(E) > 0 such that ∣∣VR(WProd)Il ≥ c(e). By Lemma 8, there exists
some C such that if minι≤k≤L ∣∣Wk(t)∣∣F > C, kWL(t)…W2(t)k ≥ CL/2. By eq. (37) and
Lemma 7, Pt:kWk(t)kF ≤C for some k ηt is finite. On the other hand, by Lemma 7, Pi∞=0 ηt = ∞, and
thus Pt：kWk(t)kF>C for all k %=∞. TherefOre We have, by invoking eq.(26),
∞2
2R(W(0)) ≥ Xηt∣∣R(W(t))∣∣
t=0
∞
≥ X ηt
t=0
∂ R
∂W1(t)
2
CL
≥ C(E)-2^	工 ηt
t:kWk(t)kF>C for all k
= ∞,
Which is a contradiction. Therefore R W(t) → 0, and since it has no finite optimum, kWk kF →
∞. The other results follow from Lemma 7.	□
Proof of Theorem 4. Recall that
∂ R
∂W1
W> …W> VR(Wprod),
and thus
kW1 (t+1)kF = kW1 (t)kF - 2ηt(W1(t),dWRy:+η2 ∂W⅛ F
∂R	2
=∣∣Wl(t)∣∣F - 2η (wprod(t), VR (WProd(t)))+ % ∂Wγ (t)
If hWprod, zii ≥ 0 for all i, then kW1(t + 1)kF ≥ kW1(t)kF.
Also recall that Π⊥W1(t) denote the projection of rows of Wι(t) onto u⊥, the orthogonal comple-
ment of span(u). We have
k∏⊥Wι(t + i)kF ≤ k∏⊥Wι(t)kF - 2ηt (∏⊥Wι(t), dWRy) + η2
2
F
∂ R
∂W1(t)
llπ⊥W1(t)kF — 2ηt Dπ⊥wprod(t), VR (WProd(t)))+ % ∂∖WR(t')
2
.
F
(38)
Invoking eq. (26) again gives
2
η2dWR)p≤ ηt∣∣VR(W(t))∣∣ ≤ 2 (r(W(t))-R(W(t + 1))).	(39)
The proof then goes in almost the same way as the proof of Theorem 2. For any E > 0, we can find
some large enough time t0, such that for any t ≥ t0,
19
Published as a conference paper at ICLR 2019
1.	k∏⊥Wι(t)kF/kWι(t)kF ≥ e implies that (∏⊥wpr°d(t), VR (WProd(t)))≥ 0.
2.	hwprod(t), zii ≥ 0 for all i, and thus kW1(t + 1)kF ≥ kW1(t)kF.
3.	kWl(t)kF ≥ 1 + √2R(W(0))/e.
Suppose at some time tι ≥ to, ∣∣∏⊥W1(t1)∣∣F/∣∣W1(t1)∣∣F ≥ e. As long as this still holds, in
light of bullet (1) above, eq. (38) and eq.(39), k∏⊥WιkF will increase by at most 2R (W(tι)) ≤
2R W(0) . On the other hand, kW1kF → ∞, and thus there exists some t2 > t1 such that
k∏⊥W1(t2)kF/∣∣W1(t2)∣∣F<e.
Let t3 denote the smallest time after t2 such that ∣∣∏⊥W1(t3)∣∣F/∣∣W1(t3)∣∣F ≥ e (if it exists).
Recall that kW1(t + 1)kF ≤ kW1(t)kF + 1 for any t ≥ 0, and kW1(t + 1)kF ≥ kW1(t)kF for any
t ≥ t0, we have
k∏⊥W1(t3)kF ≤ k∏⊥W1(t3)kF ≤ k∏⊥W1(t3 - 1)∣∣F + 1 <	.	1
∣∣W1(t3)∣∣F	— ∣∣W1(t3 - i)∣∣f —	kWi(t3 - i)∣∣f	kW1(t3 - i)∣∣f
Aftert3, k∏⊥Wι kF will increase by at most 2R (W(0)), and thus ∣∣Π⊥W1 ∣∣f will increase by at
most J2R (W(0)). Therefore, for any t4 ≥ t3, as long as ∣∣∏⊥Wι(t4)kF/kW1(t4)kF ≥ e, We
have
k∏⊥W1(t4)kF ≤ k∏⊥W1(t4)kF
IIW1(t4 )∣F	—	kW1(t3)kF
k∏⊥W1(t3)kF + /2R (W(0))
≤	kW1(t3)kF
1	√2R (W(0))
≤ e + kW1(t3- i)If + kW1(t3)kF ≤ 2e,
since ∣∣W1(t)∣∣F ≥ 1+√2r(w(0))/e after to. In other words,
	1.	k∏⊥Wi∣F … lim SUp	“ ——≤ 2e. t→∞	∣W1 ∣F
Since e is arbitrary, we have	1.	k∏⊥Wi∣F n lim tsU∞ ^W1∣^ = 0，
and thus limt→∞∣hvι,ui∣ = 1.
□
Proof of Corollary 2. The proof is analogous to that of Corollary 1, except using Theorem 4 in place
of Theorem 2.	口
20