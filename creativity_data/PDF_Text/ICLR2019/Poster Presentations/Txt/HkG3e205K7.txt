Published as a conference paper at ICLR 2019
Doubly Reparameterized Gradient Estimators
for Monte Carlo Objectives
George Tucker
Google Brain
gjt@google.com
Dieterich Lawson
New York University
jdl404@nyu.edu
Shixiang Gu
Google Brain
shanegu@google.com
Chris J. Maddison
University of Oxford, DeepMind
cmaddis@stats.ox.ac.uk
Ab stract
Deep latent variable models have become a popular model choice due to the
scalable learning algorithms introduced by (Kingma & Welling, 2013; Rezende
et al., 2014). These approaches maximize a variational lower bound on the in-
tractable log likelihood of the observed data. Burda et al. (2015) introduced a
multi-sample variational bound, IWAE, that is at least as tight as the standard
variational lower bound and becomes increasingly tight as the number of samples
increases. Counterintuitively, the typical inference network gradient estimator for
the IWAE bound performs poorly as the number of samples increases (Rainforth
et al., 2018; Le et al., 2018). Roeder et al. (2017) propose an improved gradient
estimator, however, are unable to show it is unbiased. We show that it is in fact bi-
ased and that the bias can be estimated efficiently with a second application of the
reparameterization trick. The doubly reparameterized gradient (DReG) estima-
tor does not suffer as the number of samples increases, resolving the previously
raised issues. The same idea can be used to improve many recently introduced
training techniques for latent variable models. In particular, we show that this
estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep
update (RWS) (Bornschein & Bengio, 2014), and the jackknife variational infer-
ence (JVI) gradient (Nowozin, 2018). Finally, we show that this computationally
efficient, unbiased drop-in gradient estimator translates to improved performance
for all three objectives on several modeling tasks.
1 Introduction
Following the influential work by (Kingma & Welling, 2013; Rezende et al., 2014), deep generative
models with latent variables have been widely used to model data such as natural images (Rezende
& Mohamed, 2015; Kingma et al., 2016; Chen et al., 2016; Gulrajani et al., 2016), speech and music
time-series (Chung et al., 2015; Fraccaro et al., 2016; Krishnan et al., 2015), and video (Babaeizadeh
et al., 2017; Ha & Schmidhuber, 2018; Denton & Fergus, 2018). The power of these models lies
in combining learned nonlinear function approximators with a principled probabilistic approach,
resulting in expressive models that can capture complex distributions. Unfortunately, the nonlinear-
ities that empower these model also make marginalizing the latent variables intractable, rendering
direct maximum likelihood training inapplicable. Instead of directly maximizing the marginal like-
lihood, a common approach is to maximize a tractable lower bound on the likelihood such as the
variational evidence lower bound (ELBO) (Jordan et al., 1999; Blei et al., 2017). The tightness of
the bound is determined by the expressiveness of the variational family. For tractability, a factorized
variational family is commonly used, which can cause the learned model to be overly simplistic.
Burda et al. (2015) introduced a multi-sample bound, IWAE, that is at least as tight as the ELBO
and becomes increasingly tight as the number of samples increases. Counterintuitively, although
Implementation of DReG estimators and code to reproduce experiments: sites.google.com/view/
dregs.
1
Published as a conference paper at ICLR 2019
the bound is tighter, Rainforth et al. (2018) theoretically and empirically showed that the standard
inference network gradient estimator for the IWAE bound performs poorly as the number of sam-
ples increases due to a diminishing signal-to-noise ratio (SNR). This motivates the search for novel
gradient estimators.
Roeder et al. (2017) proposed a lower-variance estimator of the gradient of the IWAE bound. They
speculated that their estimator was unbiased, however, were unable to prove the claim. We show
that it is in fact biased, but that it is possible to construct an unbiased estimator with a second
application of the reparameterization trick which we call the IWAE doubly reparameterized gradient
(DReG) estimator. Our estimator is an unbiased, computationally efficient drop-in replacement, and
does not suffer as the number of samples increases, resolving the counterintuitive behavior from
previous work (Rainforth et al., 2018). Furthermore, our insight is applicable to alternative multi-
sample training techniques for latent variable models: reweighted wake-sleep (RWS) (Bornschein
& Bengio, 2014) and jackknife variational inference (JVI) (Nowozin, 2018).
In this work, we derive DReG estimators for IWAE, RWS, and JVI and demonstrate improved
scaling with the number of samples on a simple example. Then, we evaluate DReG estimators
on MNIST generative modeling, Omniglot generative modeling, and MNIST structured prediction
tasks. In all cases, we demonstrate substantial unbiased variance reduction, which translates to
improved performance over the original estimators.
2 Background
Our goal is to learn a latent variable generative modelpθ (χ,z) = pθ (z)pθ (x|z) where X are observed
data and z are continuous latent variables. The marginal likelihood of the observed data, pθ (x) =
pθ(x, z) dz, is generally intractable. Instead, we maximize a variational lower bound on logpθ (x)
such as the ELBO
logPθ(x) = logEpθ(z)[pθ(x|z)] ≥ Eq(z∣x)
log
Pθ (x,z)
q(ZIx)
(1)
where q(z|x) is a variational distribution. Following the influential work by (Kingma & Welling,
2013; Rezende et al., 2014), We consider the amortized inference setting where qφ(z∣x), referred to
as the inference network, is a learnable function parameterized by φ that maps from x to a distri-
bution over z . The tightness of the bound is coupled to the expressiveness of the variational family
(i.e., {qφ}φ). As a result, limited expressivity of {qφ}φ, can negatively affect the learned model.
Burda et al. (2015) introduced the importance weighted autoencoder (IWAE) bound which alleviates
this coupling
Ezjog( K X p½⅛ !# ≤ log Pp (χ),	⑵
with zi：K 〜Q- qφ(zi∖x). The IWAE bound reduces to the ELBO when K = 1, is non-decreasing
as K increases, and converges to logpθ (x) as K → ∞ under mild conditions (Burda et al., 2015).
When qφ is reparameterizable1, the standard gradient estimator of the IWAE bound is
K
X pjWj vθ,φ log Wi
where Wi = pp(x, Zi)∕qφ(z-∖x). A single sample estimator of this expectation is typically used as
the gradient estimator.
As K increases, the bound becomes increasingly tight, however, Rainforth et al. (2018) show that
the signal-to-noise ratio (SNR) of the inference network gradient estimator goes to 0. This does not
1Meaning that we can express zi as z(i, φ), where z is a deterministic, differentiable function and p(i)
does not depend on θ or φ. This allows gradients to be estimated using the reparameterization trick (Kingma
& Welling, 2013; Rezende et al., 2014). Notably, the recently introduced implicit reparameterization trick has
expanded the class of distributions for which we can compute reparameterization gradients to include Gaussian
mixtures among many others (Graves, 2016; Jankowiak & Obermeyer, 2018; Jankowiak & Karaletsos, 2018;
Figurnov et al., 2018).
2
Published as a conference paper at ICLR 2019
happen for the model parameters (θ). Following up on this work, Le et al. (2018) demonstrate that
this deteriorates the performance of learned models on practical problems.
Because the IWAE bound converges to logpθ (x) (as K → ∞) regardless of qφ, φ's affect on the
bound must diminish as K increases. It may be tempting to conclude that the SNR of the inference
network gradient estimator must also decrease as K → ∞. However, low SNR is a limitation of the
gradient estimator, not necessarily of the bound. Although the magnitude of the gradient converges
to 0, if the variance of the gradient estimator decreases more quickly, then the SNR of the gradient
estimator need not degrade. This motivates the search for lower variance inference network gradient
estimators.
To derive improved gradient estimators for φ, it is informative to expand the total derivative2 of the
IWAE bound with respect to φ
K
X
i=1
(-∂
log qφ(zi∖χ) +
∂log wi dzi
∂zi	dφ
(3)
Previously, Roeder et al. (2017) found that the first term within the parentheses of Eq. 3 can con-
tribute significant variance to the gradient estimator. When K = 1, this term analytically vanishes
in expectation, so when K > 1 they suggested dropping it. Below, we abbreviate this estimator as
STL. As we show in Section 6.1, the STL estimator introduces bias when K > 1.
3 Doubly Reparameterized Gradient Estimators (DReGs)
Our insight is that we can estimate the first term within the parentheses of Eq. 3 efficiently with a
second application of the reparameterization trick. To see this, first note that
Kw∂	K	w∂
EeLK	XpKi—而	log q(ZiIx)	= X EeLK	pKi— ∂d> log q(zi∖x)，
i=1 j=1 wj ∂φ	i=1	j=1 wj ∂φ
so it suffices to focus on one of the K terms. Because the derivative is a partial derivative 篇,it
treats zi = z(i, φ) as a constant, so we can freely change the random variable that the expectation
is over to z1:K. Now,
Ez1:K
Pwwj ∂ log qφ(zi∖x)
Pw⅛ ∂ log qφ(zi∖x)
(4)
where z-i = z1:i-1,i+1:K is the set of z1:K without zi. The inner expectation resembles a REIN-
FORCE gradient term (Williams, 1992), where We interpret PwW as the “reward”. Now, We can
jj
use the following well-known equivalence between the REINFORCE gradient and the reparameter-
ization trick gradient (See Appendix 8.1 for a derivation)
∂	∂f (z) ∂z(, φ)
Eqφ(ZIx) f (Z)羽 log qφ(ZIx) = Ee ~∂Z	∂φ
(5)
This holds even when f depends on φ. Typically, the reparameterization gradient estimator has
lower variance than the REINFORCE gradient estimator because it directly takes advantage of the
derivative of f. Applying the identity from Eq. 5 to the right hand side of Eq. 4 gives
Wi	∂ ∂wi ∂Zi
(Pj Wj)2 )汨而
Ezi
Pwiwj ∂ logqφ(ZiIx)
(6)
This last expression can be efficiently estimated with a single Monte Carlo sample. When Zi is not
reparameterizable (e.g., the models in (Mnih & Rezende, 2016)), we can use a control variate (e.g.,
2log wi depends on φ in two ways: φ and zi = z(i , φ). The total derivative accounts for both sources of
dependence and the partial derivative 焉 considers Zi as a constant.
3
Published as a conference paper at ICLR 2019
六∂φ logqφ(zi∣χ)). In both cases, when K = 1, this term vanishes exactly and We recover the
estimator proposed in (Roeder et al., 2017) for the ELBO. However, when K > 1, there is no reason
to believe this term will analytically vanish.
Substituting Eq. 6 into Eq. 3, we obtain a simplification due to cancellation of terms
QEzi:K
IX
∂ log Wi ∂zi
∂zi ∂φ
(7)
We call the algorithm that uses the single sample Monte Carlo estimator of this expression for the
inference network gradient the IWAE doubly reparameterized gradient estimator (IWAE-DReG).
This estimator has the property that when q(z|x) is optimal (i.e., q(z|x) = p(z|x)), the estimator
vanishes exactly and has zero variance, whereas this does not hold for the standard IWAE gradient
estimator. We provide an asymptotic analysis of the IWAE-DReG estimator in Appendix 8.2. The
conclusion of that analysis is that, in contrast to the standard IWAE gradient estimator, the SNR of
the IWAE-DReG estimator exhibits the same scaling behaviour of O(√K) for both the generation
and inference network gradients (i.e., improving in K).
4 Alternative training algorithms
Now, we review alternative training algorithms for deep generative models and derive their doubly
reparameterized versions.
4.1	Reweighted Wake Sleep (RWS)
Bornschein & Bengio (2014) introduced RWS, an alternative multi-sample update for latent variable
models that uses importance sampling. Computing the gradient of the log marginal likelihood
Vθ logpθ (x)
Vθ Jz pθ(x,z) dz _ Jz pθ(x, z)Vθ logpθ(x,z) dz
pθ(x)
pθ(x)
Epθ(z∣x) [Vθ logpθ(x,z)],
requires samples from pθ(z|x), which is generally intractable. We can approximate the gradient
with a self-normalized importance sampling estimator
K
Epθ(z∣x) [Vθ logPθ(x,z)] ≈ EzLK
Wi
=Pwj
Vθ logpθ (x, zi)
where zi：K 〜 口分 qφ(zi∣χ). Interestingly, this is precisely the same as the IWAE gradient of θ,
so the RWS update for θ can be interpreted as maximizing the IWAE lower bound in terms of θ .
Instead of optimizing a joint objective for p and q, RWS optimizes a separate objective for the
inference network. (Bornschein & Bengio, 2014) propose a “wake” update and a “sleep” update for
the inference network. Le et al. (2018) provide empirical support for solely using the wake update
for the inference network, so we focus on that update.
The wake update approximately minimizes the KL divergence frompθ(z|x) to qφ(z∣χ). The gradient
of the KL term is
VφEpθ(z∣χ) [logPθ(z|x) — log qφ(z|x)] = -Epθ(z∣χ) ]∂φ log 9φ(z∣x) ∙
The wake update of the inference network approximates the intractable expectation by self-
normalized importance sampling
- Epθ (z|x)
log qφ(z∣χ)
≈ -Ez1:K
K
X P⅛ ∂φ log qφ(zi ⑻
(8)
with Zi 〜qφ(zi∣χ). Le et al. (2018) note that this update does not suffer from diminishing SNR
as K increases. However, a downside is that the updates for p and q are not gradients of a unified
objective, so could potentially lead to instability or divergence.
4
Published as a conference paper at ICLR 2019
Doubly Reparameterized Reweighted Wake update
The wake update gradient for the inference network (Eq. 8) can be reparameterized
- Ez1:K
K
X Pi -ξ-log qφ(zi Ix)
i=1 jwj∂φ
∂ log Wi ∂zi
∂zi ∂φ
(9)
We call the algorithm that uses the single sample Monte Carlo estimator of this expression as the
wake update for the inference network RWS-DReG.
Interestingly, the inference network gradient estimator from (Roeder et al., 2017) can be seen as
the sum of the IWAE gradient estimator and the wake update of the inference network (as the wake
update minimizes, we add the negative of Eq. 9). Their positive results motivate further exploration
of convex combinations of IWAE-DReG and RWS-DReG
E1:K
K	wi	wi2	∂ log wi ∂zi
匹(αPWj+ (I-2α)(PWF) kT .
(10)
We refer to the algorithm that uses the single sample Monte Carlo estimator of this expression as
DReG(α). When α = 1, this reduces to RWS-DReG, when α = 0, this reduces to IWAE-DReG
and when α = 0.5, this reduces STL.
4.2	Jackknife Variational Inference (JVI)
Alternatively, Nowozin (2018) reinterprets the IWAE lower bound as a biased estimator for the log
marginal likelihood. He analyzes the bias and introduces a novel family of estimators, Jackknife
Variational Inference (JVI), which trade off reduction in bias for increased variance. This additional
flexibility comes at the cost of no longer being a stochastic lower bound on the log marginal like-
lihood. The first-order JVI has significantly reduced bias compared to IWAE, which empirically
results in a better estimate of the log marginal likelihood with fewer samples (Nowozin, 2018). For
simplicity, we focus on the first-order JVI estimator
K × Ez1:K
1 K	K-1 K
log (K 工 WiJ ] - ~iΓ ∑Ez-i
It is straightforward to apply our approach to higher order JVI estimators.
Doubly Reparameterized Jackknife Variational Inference (JVI)
The JVI estimator is a linear combination of K and K - 1 sample IWAE estimators, so we can use
the doubly reparameterized gradient estimator (Eq. 7) for each term.
5	Related Work
Mnih & Rezende (2016) introduced a generalized framework of Monte Carlo objectives (MCO). The
log of an unbiased marginal likelihood estimator is a lower bound on the log marginal likelihood by
Jensen’s inequality. In this view, the ELBO can be seen as the MCO corresponding to a single im-
portance sample estimator of the marginal likelihood with qθ as the proposal distribution. Similarly,
IWAE corresponds to the K-sample estimator. Maddison et al. (2017) show that the tightness of an
MCO is directly related to the variance of the underlying estimator of the marginal likelihood.
However, Rainforth et al. (2018) point out issues with gradient estimators of multi-sample lower
bounds. In particular, they show that although the IWAE bound is tighter, the standard IWAE gradi-
ent estimator’s SNR scales poorly with large numbers of samples, leading to degraded performance.
Le et al. (2018) experimentally investigate this phenomenon and provide empirical evidence of this
degradation across multiple tasks. They find that RWS (Bornschein & Bengio, 2014) does not suffer
from this issue and find that it can outperform models trained with the IWAE bound. We conclude
that it is not sufficient to just tighten the bound; it is important to understand the gradient estimators
of the tighter bound as well.
5
Published as a conference paper at ICLR 2019
Wake-sleep is an alternative approach to fitting deep generative models, first introduced in (Hinton
et al., 1995) as a method for training Hemholtz machines. It was extended to the multi-sample
setting by (Bornschein & Bengio, 2014) and the sequential setting in (Gu et al., 2015). It has been
applied to generative modeling of images (Ba et al., 2015).
6	Experiments
To evaluate DReG estimators, we first measure variance and signal-to-noise ratio (SNR)3 of gradient
estimators on a toy example which we can carefully control. Then, we evaluate gradient variance
and model learning on MNIST generative modeling, Omniglot generative modeling, and MNIST
structured prediction tasks.
6.1	Toy Gaussian
We reimplemented the Gaussian example from (Rainforth et al., 2018). Consider the generative
model With Z 〜 N(θ, I) and x|z 〜 N(z, I) and inference network qφ(z∣χ) 〜 N(Ax + b, 21),
where φ = {A, b}. As in (Rainforth et al., 2018), we sample a set of parameters for the model
and inference network close to the optimal parameters (perturbed by zero-mean Gaussian noise
with standard deviation 0.01), then estimate the gradient of the inference network parameters for
increasing number of samples (K).
In addition to signal-to-noise ratio (SNR), we plot the squared bias and variance of the gradient
estimators4 in Fig. 1. The bias is computed relative to the expected value of the IWAE gradient
estimator. As a result, although the average ofK ELBO gradient estimators is an unbiased estimator
of the ELBO gradient, it is a biased gradient estimator of the IWAE objective. Importantly, SNR
does not penalize estimators that are biased, so trivial constant estimators can have infinite SNR.
Thus, it is important to consider additional evaluation measures as well. As K increases, the SNR of
the IWAE-DReG estimator increases, whereas the SNR of the standard gradient estimator of IWAE
goes to 0, as previously reported. Furthermore, we can see the bias present in the STL estimator. As a
check of our implementation, we verified that the observed “bias” for IWAE-DReG was statistically
indistinguishable from 0 with a paired t-test. For the biased estimators (e.g., STL), we could easily
reject the null hypothesis with few samples.
IOP ——K ELBOs
IO7 ——IWAE
10^β ——STL (Roeder et al. 2017)
10^9 ——IWAE-DReG
Ow
IO0 IO1 IO2
K
Figure 1: Signal-to-noise ratios (SNR), bias squared, and variance of gradient estimators with in-
creasing K over 10 random trials with 1000 measurement samples per trial (mean in bold). The
observed “bias” for IWAE-DReG is not statistically significant under a paired t-test (as expected
because IWAE-DReG is unbiased). IWAE-DReG is unbiased, its SNR increases with K, and it has
the lowest variance of the estimators considered here.
6.2	Generative modeling
Training generative models of the binarized MNIST digits dataset is a standard benchmark task for
latent variable models. For this evaluation, we used the single latent layer architecture from (Burda
et al., 2015). The generative model used 50 Gaussian latent variables with an isotropic prior and
3Defined as the mean of the estimator divided by the standard deviation.
4All dimensions of φ behaved qualitatively similarly, so for clarity, we show curves for a single randomly
chosen dimension of φ.
6
Published as a conference paper at ICLR 2019
passed z through two deterministic layers of 200 tanh units to parameterize factorized Bernoulli
outputs. The inference network passed x through two deterministic layers of 200 tanh units to pa-
rameterize a factorized Gaussian distribution over z. Because our interest was in improved gradient
estimators and optimization performance, we used the dynamically binarized MNIST dataset, which
minimally suffers from overfitting. We used the standard split of MNIST into train, validation, and
test sets.
We trained models with the IWAE gradient, the RWS wake update, and with the JVI estimator.
In all three cases, the doubly reparameterized gradient estimator reduced variance5 and as a result
substantially improved performance (Fig. 2).
0.014
0.012
(u
C 0.010
® 0.008
c 0.006 ，
g 0.004
0 0.002 /
0.000 I
O
- RWS
——RWS-DReG
		
----JVI-DReG
2000 3000 4000 5000
Steps (thousands)
-86.5
UJ)<u>ti更 qott<υH
IOOO 2000 3000 4000 5000
Steps (thousands)
50505050
α7.7.& cd9.9.口
88888889
--------
(SaeU) φ>-tiω⅞ ttωH
0.09
0.08
8 0.07
,∣ 0.06
9 0.05
s 0-°4 l
:5 0.03
£ 0 02 /
0.01 !
0.00 I
- JVI
——JVI-DReG
IOOO 2000 3000 4000 5000
Steps (thousands)
/''k
Figure 2: MNIST generative modeling trained according to IWAE (left), RWS (middle), and JVI
(right). The top row compares the variance of the original gradient estimator (dashed) with the
variance of the doubly reparameterized gradient estimator (solid). The bottom row compares test
performance. The left and middle plots show the IWAE (stochastic) lower bound on the test set.
The right plot shows the JVI estimator (which is not a bound) on the test set. The bold lines are the
average over three trials, and individual trials are displayed as semi-transparent). All methods used
K = 64.
We found similar behavior with different numbers of samples (Fig. 3 and Appendix Fig. 8). Inter-
estingly, the biased gradient estimators STL and RWS-DReG perform best on this task with RWS-
DReG slightly outperforming STL. As observed in (Le et al., 2018), RWS increasingly outperforms
IWAE as K increases. Finally, we experimented with convex combinations of IWAE-DReG and
RWS-DReG (right Fig. 3). On this dataset, convex combinations that heavily weighted RWS-DReG
had the best performance. However, as we show below, this is task dependent.
Next, we performed the analogous experiment with the dynamically binarized Omniglot dataset
using the same model architecture. Again, we found that the doubly reparameterized gradient esti-
mator reduced variance and as a result improved test performance (Figs. 5 and 6 in the Appendix).
6.3	Structured prediction on MNIST
Structured prediction is another common benchmark task for latent variable models. In this task, our
goal is to model a complex observation x given a context c (i.e., model the conditional distribution
p(χ∣c)). We can use a conditional latent variable model pθ(x, z|c) = pθ(x|z, c)pθ(z|c), however, as
before, computing the marginal likelihood is generally intractable. It is straightforward to adapt the
bounds and techniques from the previous section to this problem.
5We summarized the Covariance matrix of the gradient estimators by reporting the trace of the Covariance
normalized by the number of parameters. To estimate this quantity, we estimated moments from exponential
moving averages with decay=0.99 (we found that the results were robust to the exact value of the decay).
7
Published as a conference paper at ICLR 2019
0es<υ>-e2qo⅛<υH
-86.5
-87.0
-87.5
-88.0
-88.5
-89.0
-89.5
-90.0
0
(saeu)<υ>-e2qo⅛<υH
<υ>μp2qo⅛i3<υ>Aeω0:
----alρha=0.5, STL
----a∣ρha=0.7
----a∣ρha=0.8
----a∣ρha=0.9
——alpha=l, RWS-DReG
1000 2000 3000 4000 5000
Steps (thousands)
Figure 3:	Log-likelihood lower bounds for generative modeling on MNIST. The left and middle
plots compare performance with different number of samples K = 32, 256. For clarity the legend
is shared between the plots. The bold lines are the average over three trials, and individual trials
are displayed as semi-transparent). The right plot compares performance as the convex combination
between IWAE-DReG and RWS-DReG is varied (Eq. 10). To highlight differences, we plot the
difference between the test IWAE bound and the test IWAE bound IWAE-DReG achieved at that
step.
To evaluate our method in this context, we use the standard task of modeling the bottom half of a
binarized MNIST digit from the top half. We use a similar architecture, but now learn a conditional
prior distribution pθ(z|c) where C is the top half of the MNIST digit. The conditional prior feeds
c to two deterministic layers of 200 tanh units to parameterize a factorized Gaussian distribution
over z. To model the conditional distribution pθ(x|c, z), we concatenate Z with C and feed it to two
deterministic layers of 200 tanh units to parameterize factorized Bernoulli outputs.
As in the previous tasks, the doubly reparameterized gradient estimator improves across all three
updates (IWAE, RWS, and JVI; Appendix Fig. 7). However, on this task, the biased estimators
(STL and RWS) underperform unbiased IWAE gradient estimators (Fig. 4). In particular, RWS
becomes unstable later in training. We suspect that this is because RWS does not directly optimize
a consistent objective.
-40.0
K= 64
(SNe S ttωH
——RWS-DReG
——STl(Roeder 2017)
1000
2000
3000
Steps (thousands)
UJ)<u>ti.⅛qott<υ4 ω>-⅛-ωtt
Figure 4:	Log-likelihood lower bounds for structured prediction on MNIST. The left plot uses K =
64 samples and the right plot uses K = 256 samples. For clarity the legend is shared between
the plots. The bold lines are the average over three trials, and individual trials are displayed as
semi-transparent). The right plot compares performance as the convex combination between IWAE-
DReG and RWS-DReG is varied (Eq. 10). To highlight differences, we plot the difference between
the test IWAE bound and the test IWAE bound IWAE-DReG achieved at that step.
7 Discussion
In this work, we introduce doubly reparameterized estimators for the updates in IWAE, RWS, and
JVI. We demonstrate that across tasks they provide unbiased variance reduction, which leads to
improved performance. Furthermore, DReG estimators have the same computational cost as the
original estimators. As a result, we recommend that DReG estimators be used instead of the typical
gradient estimators.
Variational Sequential Monte Carlo (Maddison et al., 2017; Naesseth et al., 2018; Le et al., 2018)
and Neural Adapative Sequential Monte Carlo (Gu et al., 2015) extend IWAE and RWS to sequential
8
Published as a conference paper at ICLR 2019
latent variable models, respectively. It would be interesting to develop DReG estimators for these
approaches as well.
We found that a convex combination of IWAE-DReG and RWS-DReG performed best, however, the
weighting was task dependent. In future work, we intend to apply ideas from (Baydin et al., 2017)
to automatically adapt the weighting based on the data.
Finally, the form of the IWAE-DReG estimator (Eq. 7) is surprisingly simple and suggests that there
may be a more direct derivation that is applicable to general MCOs.
Acknowledgments
We thank Ben Poole and Diederik P. Kingma for helpful discussion and comments on drafts of this
paper. We thank Sergey Levine and Jascha Sohl-Dickstein for insightful discussion.
References
Jimmy Ba, Ruslan R Salakhutdinov, Roger B Grosse, and Brendan J Frey. Learning Wake-Sleep
Recurrent Attention Models. In C Cortes, N D Lawrence, D D Lee, M Sugiyama, and R Garnett
(eds.), Advances in Neural Information Processing Systems 28, pp. 2593-2601. 2015.
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine.
Stochastic variational video prediction. International Conference on Learning Representations,
2017.
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood.
Online learning rate adaptation with hypergradient descent. arXiv preprint arXiv:1703.04782,
2017.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal of the American Statistical Association, 2017.
Jorg Bomschein and Yoshua Bengio. ReWeighted wake-sleep. International Conference on Learn-
ing Representations, 2014.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. nterna-
tional Conference on Learning Representations, 2015.
George Casella and Roger L Berger. Statistical inference, volume 2. Duxbury Pacific Grove, CA.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. International Conference on Learn-
ing Representations, 2016.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
gio. A recurrent latent variable model for sequential data. In Advances in neural information
processing systems, pp. 2980-2988, 2015.
Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. International
Conference on Machine Learning, 2018.
Michael Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. arXiv
preprint arXiv:1805.08498, 2018.
Marco Fraccaro, S0ren Kaae S0nderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. In Advances in neural information processing systems, pp. 2199-2207,
2016.
Alex Graves. Stochastic backpropagation through mixture density distributions. arXiv preprint
arXiv:1607.05690, 2016.
Shixiang Gu, Zoubin Ghahramani, and Richard E Turner. Neural adaptive sequential monte carlo.
In Advances in Neural Information Processing Systems, pp. 2629-2637, 2015.
9
Published as a conference paper at ICLR 2019
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez,
and Aaron Courville. Pixelvae: A latent variable model for natural images. International Confer-
ence on Learning Representations, 2016.
David Ha and Jurgen Schmidhuber. World models. Advances in neural information processing
systems, 2018.
Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The” wake-sleep” algorithm
for unsupervised neural networks. Science, 268(5214):1158-1161, 1995.
Martin Jankowiak and Theofanis Karaletsos. Pathwise derivatives for multivariate distributions.
arXiv preprint arXiv:1806.01856, 2018.
Martin Jankowiak and Fritz Obermeyer. Pathwise derivatives beyond the reparameterization trick.
arXiv preprint arXiv:1806.01851, 2018.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. nternational Conference
on Learning Representations, 2013.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In Advances in Neural Informa-
tion Processing Systems, pp. 4743-4751, 2016.
Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint
arXiv:1511.05121, 2015.
Tuan Anh Le, Adam R Kosiorek, N Siddharth, Yee Whye Teh, and Frank Wood. Revisiting
reweighted wake-sleep. arXiv preprint arXiv:1805.10469, 2018.
Chris J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih,
Arnaud Doucet, and Yee Teh. Filtering variational objectives. In Advances in Neural Information
Processing Systems, pp. 6573-6583, 2017.
Andriy Mnih and Danilo J Rezende. Variational inference for monte carlo objectives. International
Conference on Machine Learning, 2016.
Christian Naesseth, Scott Linderman, Rajesh Ranganath, and David Blei. Variational sequential
monte carlo. In International Conference on Artificial Intelligence and Statistics, pp. 968-977,
2018.
Sebastian Nowozin. Debiasing evidence approximations: On importance-weighted autoencoders
and jackknife variational inference. International Conference on Learning Representations, 2018.
Tom Rainforth, Adam R Kosiorek, Tuan Anh Le, Chris J Maddison, Maximilian Igl, Frank Wood,
and Yee Whye Teh. Tighter variational bounds are not necessarily better. International Conference
on Machine Learning, 2018.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional Conference on Machine Learning, pp. 1530-1538, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, pp. 1278-1286, 2014.
Geoffrey Roeder, Yuhuai Wu, and David Duvenaud. Sticking the landing: An asymptotically zero-
variance gradient estimator for variational inference. Advances in Neural Information Processing
Systems, 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
10
Published as a conference paper at ICLR 2019
8 Appendix
0.008
<υuue"⊂e> 4u<υ-peJ9
0.014
0.012
0.010
0.008
0.006
Steps (thousands)
0.007
(u
2 0.006
■E 0.005
(D
> 0.004
1 0.003
P
E 0.002
9
0.001
0.000
- RWS
——RWS-DReG
0.045
-105
《J2e£ttωH
ttωH
2000 3000 4000 5000
Steps (thousands)
ttωH
Steps (thousands)
Figure 5: Omniglot generative modeling trained according to IWAE (left), RWS (middle), and JVI
(right). The top row compares the variance of the original gradient estimator (dashed) with the
variance of the doubly reparameterized gradient estimator (solid). The bottom row compares test
performance. The left and middle plots show the IWAE (stochastic) lower bound on the test set.
The right plot shows the JVI estimator (which is not a bound) on the test set. The bold lines are the
average over three trials, and individual trials are displayed as semi-transparent). All methods used
K = 64.
(aeu) 3>«&q。ttəj.
(aeu) 3>ttθ{qott3h-
K= 256
* ...-,,
喈L ` IWAE
---IWAE-DReG
- RWS
—— RWS-DReG
----STL(Roeder2017)
IOOO 2000 3000 4000 5000
Steps (thousands)
K= 64
alpha=0.1
----alpha=0.2
----alpha=0.3
----alpha=0.4
----alpha=0.5, STL
----alpha=0.6
----alpha=0.7
----alpha=O.B
----alpha=0.9
O IOOO 2000 3000 4000 5000
Steps (thousands)
----alpha=l, RWS-DReG
Figure 6: Log-likelihood lower bounds for structured prediction on Omniglot. The left plot uses
K = 64 samples and the right plot uses K = 256 samples. For clarity the legend is shared between
the plots. The bold lines are the average over three trials, and individual trials are displayed as
semi-transparent). The right plot compares performance as the convex combination between IWAE-
DReG and RWS-DReG is varied. To highlight differences, we plot the difference between the test
IWAE bound and the test IWAE bound IWAE-DReG achieved at that step.
8.1 Equivalence between REINFORCE gradient and reparameterization trick
GRADIENT
Given a function f (z, φ), we have
κ L/ 八 d log qφ(Z)
Eqφ(Z) f (z,φ)	∂φ
,∂f(z,φ) ∂z(e,φ)
∂ ∂φ
(aeu) 0>RO∂fqo Ba 3>ReQJH
11
Published as a conference paper at ICLR 2019
gue"ce> 4u<υ-peJ9
0.0014
0.0012
0.0010 ∣ι
0.0008
0.0006
- IWAE
---IWAE-DReG
0.0004
0.0002
0.006
g θ∙005
S 0.004 1
乙 0.003 ,
c ∙
(u ,
:5 0.002
e
o 0.001 L
0.000
Steps (thousands)
- JVl
----JVI-DReG
1000 2000 3000 4000 5000
Steps (thousands)
0.0000 I
0	1000 2000 3000 4000 5000
Steps (thousands)
(S4esEttwqotttuH
Steps (thousands)
(S4esEttwqotttuH
-40.5
- RWS
——RWS-DReG
-41.0
《£leu》<υ>μp2qott<υH
- JVI
----JVI-DReG
1000 2000 3000 4000 5000
Steps (thousands)
Figure 7:	Structured prediction on MNIST according to IWAE (left), RWS (middle), and JVI (right).
The top row compares the variance of the original gradient estimator (dashed) with the variance of
the doubly reparameterized gradient estimator (solid). The bottom row compares test performance.
The left and middle plots show the IWAE (stochastic) lower bound on the test set. The right plot
shows the JVI estimator (which is not a bound) on the test set. The bold lines are the average over
three trials, and individual trials are displayed as semi-transparent). All methods used K = 64.
Steps (thousands)
K= 32
K= 64
K= 128
K= 256
Steps (thousands)

Figure 8:	Variance of the gradient estimators on the MNIST generative modeling task. We plot
the trace of the variance of the doubly reparameterized gradient estimator relative to the original
gradient estimator for IWAE (left), RWS (middle), and JVI (right) as the number of samples (K) is
varied.
for a reparameterizable distribution qφ(z). To see this, note that
d	∂	∂∂
dφ J qφ(z)f(z,φ) dz = J ∂φqφ(z)f(z,φ)dz = Jf(z,φ)∂φqφ(z)+q©(Z)∂φf(z,φ)dz
f(z, φ)qφ(z)
z
Eqφ(z)
f(z, φ)
Id log qφ(z)
∂φ
∂ log qφ(z)∙
∂φ
dz + Eqφ(z)
+ Eqφ (z)
Γ∂f (z,φ)^
'[∂φ _
∂f(z,φ)]
∂φ ],
via the REINFORCE gradient. On the other hand,
ddφ / qφ(z)f (z,φ) dz = dφEqφ(Z) [f (Z，φ)] =
[f (z(, φ), φ)] = E
∂f(z,Φ) dz(gφ)
∂Z	∂φ
∂f(z,φ) dz(gφ)
∂Z	∂φ
∂f(z, φ)
+ Eqφ (z)
∂φ z=z(,φ)
一 ∂f(z,φ) ^
∂φ ,
, φ), φ)
E
12
Published as a conference paper at ICLR 2019
via the reparameterization trick. Thus, we conclude that
W ∂lo Nlogqφ(z)"Lw	pf(Z⑷]W pf(z,φ) azɑφ)Lw	pf(z,φ)
Eqφ(z) f(z,φ) ∂φ J+ Eqφ(z) [-^r = = Ee [F~~∂T]+ Eqφ(z) [-^r
from which the identity follows.
8.2	Asymptotic analysis
At a high level, Rainforth et al. (2018) show that the expected value of the IWAE gradient of the
inference network collapses to zero with rate 1/K, while its standard deviation is only shrinking at
a rate of 1/√K. This is the essence of the problem that results in the SNR (expectation divided by
standard deviation) of the inference network gradients going to zero at a rate O((1∕K )/(1/√K))=
O(1∕√K), worsening with K. In contrast, Rainforth et al. (2018) show that the generation network
gradients scales like O(√K), improving with K.
Because the IWAE-DReG estimator is unbiased, we cannot hope to change the scaling of the ex-
pected value in K, but we can hope to change the scaling of the variance. In particular, in this
subsection, we provide an informal argument, via the delta method, that the standard deviation of
IWAE-DReG scales like K-3∕2, which results in an overall scaling of O(√K) for the inference
network gradient’s SNR (i.e., increasing with K). Thus, the SNR of the IWAE-DReG estimator
improves similarly in K for both inference and generation networks.
We will appeal to the delta method on a two-variable function g : R2 → R. Define the following
notation for the partials of g evaluated at the mean of random variables X, Y ,
gχ(χ,γ )=笔M
(x,y)=(E(X),E(Y))
The delta method approximation of Var(g(X, Y)) is given by (Section 5.5 of Casella & Berger),
Var(g(X, Y)) ≈ gx(X, Y)2Var(X) + 2gx(X, Y)gy(X, Y)Cov(X, Y) + gy(X, Y)2Var(Y)
Now, assume without loss of generality that φ is a single real-valued parameter. Let ui =
W2 d l∂giwi ∂φ, X = Pκ=1 Ui, and Y = PK=1 wi. Let g(X,Y) = X/Y2, then g(X,Y) is the
IWAE-DReG estimator whose variance we seek to understand. Letting Z = E(wi) and U = E(ui)
we get in this case after cancellations,
Var(g(X,Y)) ≈ 吃 Vr(X)
Z4 K4
4U Cov(X, Y)	4U2 Var(Y)
Z5 ―K4 — + ^z6"—K 4
Because wi are all mutually independent, we get Var(Y) = KVar(wi). Similarly for Var(X)
and ui. Because the wi and ui are identically distributed and independent for i 6= j , we have
Cov(X, Y) = KCov(wi, ui). All together we can see that Var(g(X, Y)) scales like K-3. Thus,
the standard deviation scales like K-3∕2 .
8.3	Unified S urrogate Objectives for Estimators
In the main text, we assumed that θ and φ were disjoint, however, it can be helpful to share parame-
ters between p and q (e.g., (Fraccaro et al., 2016)). With the IWAE bound, we differentiate a single
objective with respect to both the p and q parameters. Thus it is straightforward to adapt IWAE and
IWAE-DReG to the shared parameter setting. In this section, we discuss how to deal with shared
parameters in RWS.
Suppose that both p and q are parameterized by θ. If we denote the unshared parameters of q by
φ, then we can restrict the RWS wake update to only φ. Alternatively, with a modified RWS wake
update, we can derive a single surrogate objective for each scenario such that taking the gradient with
respect to θ results in the proper update. For clarity, we introduce the following modifier notation
for pθ (x, Zi), qθ (zi∣x), and Wi which are functions of θ and Zi = z(θ, ei). We use X to mean X with
stopped gradients with respect to zɪ, X to mean X with stopped gradients with respect to θ (but not
θ is not stopped in z(θ, g)), and X to mean X with stopped gradients for all variables. Then, we
can use the following surrogate objectives:
13
Published as a conference paper at ICLR 2019
IWAE:
LIW AE (θ) = E1:K
K
XPi logWi
匕 Ej Wj
(11)
DReG IWAE:
LDReG-I W AE (θ) = E1:K
KW
X P - logpθ(x, Zi) +
W
(12)
RWS:
LRW S (θ) = E1:K
DReG RWS:
LDReG-RW S (θ) = E1:K
STL:
DReG(α):
K
X
i=1
(logPθ(x, Zi) + log Qθ(Zi|x))
X PwWj log 讥 (x,zi)+(∑w⅛ - (∑w⅛!) logWi
LST L(θ) = E1:K
K
X
i=1
(log Pθ (x,Zi ) + log Wi)
(13)
(14)
(15)
LDReG(α) (θ) = E1:K
X PWi_ logPθ(x,Zi) + ( αPwI +(1 — 2α) (Pwi ) ) logWi
i=1 j Wj	j Wj	j Wj
(16)
The only subtle difference is that DReG(α = 0.5) does not correspond exactly to STL due to the
scaling between terms:
LDReG(α=0.5) (θ) = E1:K
K
X
i=1
(log Pθ (x,Zi) + 0.5log Wi)
(17)
14