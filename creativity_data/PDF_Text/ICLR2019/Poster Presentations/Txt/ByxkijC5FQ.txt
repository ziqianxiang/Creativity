Published as a conference paper at ICLR 2019
Neural Persistence: A Complexity Measure
for Deep Neural Networks Using Algebraic
Topology
Bastian Rieck112 J, Matteo Togninalli112 J, Christian Bock1121,
Michael Moor1,2, Max Horn1,2, Thomas Gumbsch1,2, Karsten Borwardt1,2
1 Department of Biosystems Science and Engineering, ETH Zurich, Switzerland
2SIB Swiss Institute of Bioinformatics, Switzerland
* These authors contributed equally
Ab stract
While many approaches to make neural networks more fathomable have been pro-
posed, they are restricted to interrogating the network with input data. Measures
for characterizing and monitoring structural properties, however, have not been
developed. In this work, we propose neural persistence, a complexity measure for
neural network architectures based on topological data analysis on weighted strat-
ified graphs. To demonstrate the usefulness of our approach, we show that neural
persistence reflects best practices developed in the deep learning community such
as dropout and batch normalization. Moreover, we derive a neural persistence-
based stopping criterion that shortens the training process while achieving com-
parable accuracies as early stopping based on validation loss.
1	Introduction
The practical successes of deep learning in various fields such as image processing (Simonyan &
Zisserman, 2015; He et al., 2016; Hu et al., 2018), biomedicine (Ching et al., 2018; Rajpurkar et al.,
2017; Rajkomar et al., 2018), and language translation (Bahdanau et al., 2015; Sutskever et al.,
2014; Wu et al., 2016) still outpace our theoretical understanding. While hyperparameter adjustment
strategies exist (Bengio, 2012), formal measures for assessing the generalization capabilities of deep
neural networks have yet to be identified (Zhang et al., 2017). Previous approaches for improving
theoretical and practical comprehension focus on interrogating networks with input data. These
methods include i) feature visualization of deep convolutional neural networks (Zeiler & Fergus,
2014; Springenberg et al., 2015), ii) sensitivity and relevance analysis of features (Montavon et al.,
2017), iii) a descriptive analysis of the training process based on information theory (Tishby &
Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017; Saxe et al., 2018; Achille & Soatto, 2018), and
iv) a statistical analysis of interactions of the learned weights (Tsang et al., 2018). Additionally,
Raghu et al. (2017) develop a measure of expressivity of a neural network and use it to explore
the empirical success of batch normalization, as well as for the definition of a new regularization
method. They note that one key challenge remains, namely to provide meaningful insights while
maintaining theoretical generality. This paper presents a method for elucidating neural networks in
light of both aspects.
We develop neural persistence, a novel measure for characterizing neural network structural com-
plexity. In doing so, we adopt a new perspective that integrates both network weights and connectiv-
ity while not relying on interrogating networks through input data. Neural persistence builds on com-
putational techniques from algebraic topology, specifically topological data analysis (TDA), which
was already shown to be beneficial for feature extraction in deep learning (Hofer et al., 2017) and
describing the complexity of GAN sample spaces (Khrulkov & Oseledets, 2018). More precisely,
we rephrase deep networks with fully-connected layers into the language of algebraic topology and
develop a measure for assessing the structural complexity of i) individual layers, and ii) the entire
network. In this work, we present the following contributions:
1
Published as a conference paper at ICLR 2019
-	We introduce neural persistence, a novel measure for characterizing the structural complexity of
neural networks that can be efficiently computed.
-	We prove its theoretical properties, such as upper and lower bounds, thereby arriving at a normal-
ization for comparing neural networks of varying sizes.
-	We demonstrate the practical utility of neural persistence in two scenarios: i) it correctly captures
the benefits of dropout and batch normalization during the training process, and ii) it can be easily
used as a competitive early stopping criterion that does not require validation data.
2	Background: Topological data analysis
Topological data analysis (TDA) recently emerged as a field that provides computational tools for
analysing complex data within a rigorous mathematical framework that is based on algebraic to-
pology. This paper uses persistent homology, a theory that was developed to understand high-
dimensional manifolds (Edelsbrunner et al., 2002; Edelsbrunner & Harer, 2010), and has since been
successfully employed in characterizing graphs (Sizemore et al., 2017; Rieck et al., 2018), finding
relevant features in unstructured data (Lum et al., 2013), and analysing image manifolds (Carlsson
et al., 2008). This section gives a brief summary of the key concepts; please refer to Edelsbrunner &
Harer (2010) for an extensive introduction.
Simplicial homology The central object in algebraic topology is a simplicial complex K, i.e. a
high-dimensional generalization of a graph, which is typically used to describe complex objects such
as manifolds. Various notions to describe the connectivity of K exist, one of them being simplicial
homology. Briefly put, simplicial homology uses matrix reduction algorithms (Munkres, 1996) to
derive a set of groups, the homology groups, for a given simplicial complex K. Homology groups
describe topological features—colloquially also referred to as holes—of a certain dimension d, such
as connected components (d = 0), tunnels (d = 1), and voids (d = 2). The information from
the dth homology group is summarized in a simple complexity measure, the dth Betti number βd,
which merely counts the number of d-dimensional features: a circle, for example, has Betti numbers
(1, 1), i.e. one connected component and one tunnel, while a filled circle has Betti numbers (1, 0),
i.e. one connected component but no tunnel. In the context of analysing simple feedforward neural
networks for two classes, Bianchini & Scarselli (2014) calculated bounds of Betti numbers of the
decision region belonging to the positive class, and were thus able to show the implications of
different activation functions. These ideas were extended by Guss & Salakhutdinov (2018) to obtain
a measure of the topological complexity of decision boundaries.
Persistent homology For the analysis of real-world data sets, however, Betti numbers turn out
to be of limited use because their representation is too coarse and unstable. This prompted the
development of persistent homology. Given a simplicial complex K with an additional set of
weights a0 ≤ a 1 ≤ ∙∙∙ ≤ a，m-1 ≤ a，m, which are commonly thought to represent the idea
of a scale, it is possible to put K in a filtration, i.e. a nested sequence of simplicial complexes
0 = Ko ⊆ Ki ⊆ …⊆ Km-1 ⊆ Km = K. This filtration is thought to represent the ‘growth, of K
as the scale is being changed. During this growth process, topological features can be created (new
vertices may be added, for example, which creates a new connected component) or destroyed (two
connected components may merge into one). Persistent homology tracks these changes and repres-
ents the creation and destruction of a feature as a point (ai , aj ) ∈ R2 for indices i ≤ j with respect
to the filtration. The collection of all points corresponding to d-dimensional topological features is
called the dth persistence diagram Dd . It can be seen as a collection of Betti numbers at multiple
scales. Given a point (x, y) ∈ Dd, the quantity pers(x, y) := |y - x| is referred to as its persist-
ence. Typically, high persistence is considered to correspond to features, while low persistence is
considered to indicate noise (Edelsbrunner et al., 2002). 3
3 A novel measure for neural network complexity
This section details neural persistence, our novel measure for assessing the structural complexity of
neural networks. By exploiting both network structure and weight information through persistent
homology, our measure captures network expressiveness and goes beyond mere connectivity prop-
erties. Subsequently, we describe its calculation, provide theorems for theoretical and empirical
2
Published as a conference paper at ICLR 2019
w20 = 0
l0	l1
⊆
Figure 1: Illustrating the neural persistence calculation of a network with two layers (l0 and l1 ).
Colours indicate connected components per layer. The filtration process is depicted by colouring
connected components that are created or merged when the respective weights are greater than or
equal to the threshold wi0. As wi0 decreases, network connectivity increases. Creation and destruction
thresholds are collected in one persistence diagram per layer (right), and summarized according to
Equation 1 for calculating neural persistence.
bounds, and show the existence of neural networks complexity regimes. To summarize this section,
Figure 1 illustrates how our method treats a neural network.
3.1	Neural persistence
Given a feedforward neural network with an arrangement of neurons and their connections E , let
W refer to the set of weights. Since W is typically changing during training, we require a function
φ: E → W that maps a specific edge to a weight. Fixing an activation function, the connections
form a stratified graph.
Definition 1 (Stratified graph and layers). A stratified graph is a multipartite graph G = (V, E)
satisfying V = V0 t V1 t . . . , such that if u ∈ Vi, v ∈ Vj, and (u, v) ∈ E, we have j = i + 1.
Hence, edges are only permitted between adjacent vertex sets. Given k ∈ N, the kth layer of a
stratified graph is the unique subgraph Gk := (Vk t Vk+1, Ek := E ∩ {Vk × Vk+1}).
This enables calculating the persistent homology of G and each Gk, using the filtration induced
by sorting all weights, which is common practice in topology-based network analysis (Carstens
& Horadam, 2013; Horak et al., 2009) where weights often represent closeness or node similarity.
However, our context requires a novel filtration because the weights arise from an incremental fitting
procedure, namely the training, which could theoretically lead to unbounded values. When analysing
geometrical data with persistent homology, one typically selects a filtration based on the (Euclidean)
distance between data points (Bubenik, 2015). The filtration then connects points that are increas-
ingly distant from each other, starting from points that are direct neighbours. Our network filtration
aims to mimic this behaviour in the context of fully-connected neural networks. Our framework
does not explicitly take activation functions into account; however, activation functions influence the
evolution of weights during training.
Filtration Given the set of weights W for one training step, let wmax := maxw∈W |w|. Further-
more, let W0 := {|w|/wmax | w ∈ W} be the set of transformed weights, indexed in non-ascending
order, such that 1 = w0 ≥ w1 ≥ ∙∙∙ ≥ 0. This permits Us to define a filtration for the kth layer
Gk as G k°k G	⊆ ..., where G (i)	:=	(Vk	Ll Vk +匕{(u,v) ∣ (u,v) ∈	Ek	∧	W (u,v) ≥	Wi}) and
φ0(u, V) ∈ W0 denotes the transformed weight of an edge. We tailored this filtration towards the
analysis of neural networks, for which large (absolute) weights indicate that certain neurons exert a
larger influence over the final activation ofa layer. The strength ofa connection is thus preserved by
the filtration, and weaker weights with ∣w∣ ≈ 0 remain close to 0. Moreover, since w0 ∈ [0, 1] holds
for the transformed weights, this filtration makes the network invariant to scaling, which simplifies
the comparison of different networks.
Persistence diagrams Having set up the filtration, we can calculate persistent homology for every
layer Gk. As the filtration contains at most 1-simplices (edges), we capture zero-dimensional to-
pological information, i.e. how connected components are created and merged during the filtration.
These information are structurally equivalent to calculating a maximum spanning tree using the
3
Published as a conference paper at ICLR 2019
Algorithm 1 Neural persistence calculation
Require: Neural network with l layers and weights W
1： wmax J maxw∈w ∣w∣	> Determine largest absolute weight
2:	W0 J {∣w∣∕wmax ∣ w ∈ W}	> Transform weights for filtration
3： for k ∈ {0, . . . , l - 1} do
4:	Fk J G(k0) ⊆ G(k1) ⊆ ...	. Establish filtration of kth layer
5:	Dk J PERSISTENTHOMOLOGY(Fk)	. Calculate persistence diagram
6:	end for
7:	return {kD0kp , . . . , kDl-1 kp}	. Calculate neural persistence for each layer
weights, or performing hierarchical clustering with a specific setup (Carlsson & Memoli, 2010).
While it would theoretically be possible to include higher-dimensional information about each layer
Gk, for example in the form of cliques (Rieck et al., 2018), we focus on zero-dimensional informa-
tion in this paper, because of the following advantages: i) the resulting values are easily interpretable
as they essentially describe the clustering of the network at multiple weight thresholds, ii) previous
research (Rieck & Leitte, 2016; Hofer et al., 2017) indicates that zero-dimensional topological in-
formation is already capturing a large amount of information, and iii) persistent homology calcula-
tions are highly efficient in this regime (see below). We thus calculate zero-dimensional persistent
homology with this filtration. The resulting persistence diagrams have a special structure: since
our filtration solely sorts edges, all vertices are present at the beginning of the filtration, i.e. they
are already part of G(k0) for each k. As a consequence, they are assigned a weight of 1, resulting in
∣Vk × Vk+1 ∣ connected components. Hence, entries in the corresponding persistence diagram Dk are
of the form (1, x), with x ∈ W0, and will be situated below the diagonal, similar to superlevel set fil-
trations (Bubenik, 2015; Cohen-Steiner et al., 2009). Using the p-norm ofa persistence diagram, as
introduced by Cohen-Steiner et al. (2010), we obtain the following definition for neural persistence.
Definition 2 (Neural persistence). The neural persistence of the kth layer Gk, denoted by NP(Gk),
is the p-norm of the persistence diagram Dk resulting from our previously-introduced filtration, i.e.
NP(Gk)：= kDkkp ：= ( X pers(c,d)p,	(1)
(c,d)∈Dk
which (for p = 2) captures the Euclidean distance of points in Dk to the diagonal.
The p-norm is known to be a stable summary (Cohen-Steiner et al., 2010) of topological features in
a persistence diagram. For neural persistence to be a meaningful measure of structural complexity,
it should increase as a neural network is learning. We evaluate this and other properties in Section 4.
Algorithm 1 provides pseudocode for the calculation process. It is highly efficient: the filtra-
tion (line 4) amounts to sorting all n weights of a network, which has a computational complexity
of O(n log n). Calculating persistent homology of this filtration (line 5) can be realized using an
algorithm based on union-find data structures Edelsbrunner et al. (2002). This has a computational
complexity of O (n ∙ α (n)), where α(∙) refers to the extremely slow-growing inverse of the Acker-
mann function (Cormen et al., 2009, Chapter 22). We make our implementation and experiments
available under https://github.com/BorgwardtLab/Neural-Persistence.
3.2	Properties of neural persistence
We elucidate properties about neural persistence to permit the comparison of networks with different
architectures. As a first step, we derive bounds for the neural persistence ofa single layer Gk.
Theorem 1.	Let Gk be a layer of a neural network according to Definition 1. Furthermore, let
Ψk : Ek → W0 denote the function that assigns each edge of Gk a transformed weight. Using the
filtration from Section 3.1 to calculate persistent homology, the neural persistence NP(Gk) of the
kth layer satisfies
0 ≤ NP(Gk) ≤ (max Ψk(e) — min Ψk(e)) (∣Vk × Vk+ι∣ — 1)1,	(2)
e∈Ek	e∈Ek
where ∣Vk × Vk+1 ∣ denotes the cardinality of the vertex set, i.e. the number of neurons in the layer.
4
Published as a conference paper at ICLR 2019
Proof. We prove this constructively and show that the bounds can be realized. For the lower bound,
let G- be a fully-connected layer with ∖Vk∖ vertices and, given θ ∈ [0, 1], let φk(e) := θ for
every edge e. Since a vertex v is created before its incident edges, the filtration degenerates to a
lexicographical ordering of vertices and edges, and all points in Dk will be of the form (θ, θ). Thus,
NP(Gk-) = 0. For the upper bound, let Gk+ again be a fully-connected layer with ∖Vk ∖ ≥ 3 vertices
and let a, b ∈ [0, 1] with a < b. Select one edge e0 at random and define a weight function as
φ(e) := b and φ(e) := a otherwise. In the filtration, the addition of the first edge will create a pair
of the form (b, b), while all other pairs will be of the form (b, a). Consequently, we have
1
NP(G+) = (Pers(b, b)p + (n - 1) ∙ pers(b, a)p^ p =(b - a) ∙ (n - 1)P	(3)
=(max φ(e) - min φ(e) )(∖Vk ∖ - 1)P,	(4)
e∈Ek	e∈Ek
so our upper bound can be realized. To show that this term cannot be exceeded by NP(G) for any
G, suppose We perturb the weight function e(e) := φ(e) + E ∈ [0,1]. This cannot increase NP,
however, because each difference b - a in Equation 3 is maximized by max φ(e) 一 min φ(e). 口
We can use the upper bound of Theorem 1 to normalize the neural persistence of a layer, making it
possible to compare layers (and neural networks) that feature different architectures, i.e. a different
number of neurons.
Definition 3 (Normalized neural persistence). Fora layer Gk following Definition 1, using the upper
bound of Theorem 1, the normalized neural persistence NP(Gk) is defined as the neural persistence
ʃ-—`-—-
of Gk divided by its upper bound, i.e. NP(Gk):= NP(Gk) ∙ NP(G+)-1.
The normalized neural persistence of a layer permits us to extend the definition to an entire network.
While this is more complex than using a single filtration for a neural network, this permits us to
side-step the problem of different layers having different scales.
Definition 4 (Mean normalized neural persistence). Considering a network as a stratified graph
G according to Definition 1, we sum the neural persistence values per layer to obtain the mean
normalized neural persistence, i.e. NP(G) := 1 /l ∙ PIk=O NP(Gk).
While Theorem 1 gives a lower and upper bound in a general setting, it is possible to obtain empirical
bounds when we consider the tuples that result from the computation of a persistence diagram.
Recall that our filtration ensures that the persistence diagram of a layer contains tuples of the form
(1, wi), with wi ∈ [0, 1] being a transformed weight. Exploiting this structure permits us to obtain
bounds that could be used prior to calculating the actual neural persistence value in order to make
the implementation more efficient.
Theorem 2.	Let Gk be a layer of a neural network as in Theorem 1 with n vertices and m edges
whose edge weights are sorted in non-descending order, i.e. W0 ≤ W2 ≤ •… ≤ Wm-1. Then
NP(Gk) can be empirically bounded by
k1 -wmaxkp ≤ NP(Gk) ≤ k1 - wminkp,	(5)
where wmax = (Wm-1, Wm-2, . . . , Wm-n)T and wmin = (W0, W2, . . . , Wn-1)T are the vectors
containing the n largest and n smallest weights, respectively.
Proof. See Section A.2 in the appendix.
Complexity regimes in neural persistence As an application of the two theorems, we briefly
take a look at how neural persistence changes for different classes of simple neural networks. To
this end, we train a perceptron on the ‘MNIST’ data set. Since our measure uses the weight mat-
rix of a perceptron, we can compare its neural persistence with the neural persistence of random
weight matrices, drawn from different distributions. Moreover, we can compare trained networks
with respect to their initial parameters. Figure 2 depicts the neural persistence values as well as the
lower bounds according to Theorem 2 for different settings. We can see that a network in which the
optimizer diverges (due to improperly selected parameters) is similar to a random Gaussian matrix.
5
Published as a conference paper at ICLR 2019
JsEf
⅛*51⅝*
0	2	4	6	8	10	12	14	16	18	20	22	24	26	28
Neural persistence
Figure 2: Neural persistence values of trained perceptrons (green), diverging ones (yellow), random
Gaussian matrices (red), and random uniform matrices (black). We performed 100 runs per cat-
egory; dots indicate neural persistence while crosses indicate the predicted lower bound according
to Theorem 2. The bounds according to Theorem 1 are shown as dashed lines.
Trained networks, on the other hand, are clearly distinguished from all other networks. Uniform
matrices have a significantly lower neural persistence than Gaussian ones. This is in line with the
intuition that the latter type of networks induces functional sparsity because few neurons have large
absolute weights. For clarity, we refrain from showing the empirical upper bounds because most
weight distributions are highly right-tailed; the bound will not be as tight as the lower bound. These
results are in line with a previous analysis (Sizemore et al., 2017) of small weighted networks, in
which persistent homology is seen to outperform traditional graph-theoretical complexity measures
such as the clustering coefficient (see also Section A.1 in the appendix). For deeper networks,
additional experiments discuss the relation between validation accuracy and neural persistence (Sec-
tion A.5), the impact of different data distributions, as well as the variability of neural persistence
for architectures of varying depth (Section A.6).
4	Experiments
This section demonstrates the utility and relevance of neural persistence for fully connected deep
neural networks. We examine how commonly used regularization techniques (batch normalization
and dropout) affect neural persistence of trained networks. Furthermore, we develop an early stop-
ping criterion based on neural persistence and we compare it to the traditional criterion based on
validation loss. We used different architectures with ReLU activation functions across experiments.
The brackets denote the number of units per hidden layer. In addition, the Adam optimizer with hy-
perparameters tuned via cross-validation was used unless noted otherwise. Please refer to Table A.1
in the appendix for further details about the experiments.
4.1	Deep learning best practices in light of neural persistence
We compare the mean normalized neural persistence (see Definition 4) of a two-layer (with an ar-
chitecture of [650, 650]) neural network to two models where batch normalization (Ioffe & Szegedy,
2015) or dropout (Srivastava et al., 2014) are applied. Figure 3 shows that the networks designed
according to best practices yield higher normalized neural persistence values on the ‘MNIST’ data
set in comparison to an unmodified network. The effect of dropout on the mean normalized neural
persistence is more pronounced and this trend is directly analogous to the observed accuracy on
0.48	0.50	0.52	0.54	0.56	0.58	0.60	0.62	0.64
Mean normalized neural persistence
Figure 3: Comparison of mean normalized neural persistence for trained networks without modific-
ations (green), with batch normalization (yellow), and with 50% of the neurons dropped out during
training (red) for the ‘MNIST’ data set (50 runs per setting).
6
Published as a conference paper at ICLR 2019
the test set. These results are consistent with expectations if we consider dropout to be similar to
ensemble learning (Hara et al., 2016). As individual parts of the network are trained independently,
a higher degree of per-layer redundancy is expected, resulting in a different structural complexity.
Overall, these results indicate that for a fixed architecture approaches targeted at increasing the
neural persistence during the training process may be of particular interest.
4.2	Early stopping based on neural persistence
Neural persistence can be used as an early stopping criterion that does not require a validation data
set to prevent overfitting: if the mean normalized neural persistence does not increase by more
than ∆min during a certain number of epochs g, the training process is stopped. This procedure is
called ‘patience’ and Algorithm 2 describes it in detail. A similar variant of this algorithm, using
validation loss instead of persistence, is the state-of-the-art for early stopping in training (Bengio,
2012; Chollet et al., 2015). To evaluate the efficacy of our measure, we compare it against validation
loss in an extensive set of scenarios. More precisely, for a training process with at most G epochs,
we define a G × G parameter grid consisting of the ‘patience’ parameter g and a burn-in rate b (both
measured in epochs). b defines the number of epochs after which an early stopping criterion starts
monitoring, thereby preventing underfitting. Subsequently, we set ∆min = 0 for all measures to
remain comparable and scale-invariant, as non-zero values could implicitly favour one of them due
to scaling. For each data set, we perform 100 training runs of the same architecture, monitoring
validation loss and mean normalized neural persistence every quarter epoch. The early stopping
behaviour of both measures is simulated for each combination of b and g and their performance
over all runs is summarized in terms of median test accuracy and median stopping epoch; if a
criterion is not triggered for one run, we report the test accuracy at the end of the training and
the number of training epochs. This results in a scatterplot, where each point (corresponding to a
single parameter combination) shows the difference in epochs and the absolute difference in test
accuracy (measured in percent). The quadrants permit an intuitive explanation: Q2, for example,
contains all configurations for which our measure stops earlier, while achieving a higher accuracy.
Since b and g are typically chosen to be small in an early stopping scenario, we use grey points
to indicate uncommon configurations for which b or g is larger than half of the total number of
epochs. Furthermore, to summarize the performance of our measure, we calculate the barycentre of
all configurations (green square).
Figure 4a depicts the comparison with validation loss for the ‘Fashion-MNIST’ (Xiao et al., 2017)
data set; please refer to Section A.3 in the appendix for more data sets. Here, we observe that
most common configurations are in Q2 or in Q3, i.e our criterion stops earlier. The barycentre
is at (-0.53, -0.08), showing that out of 625 configurations, on average we stop half an epoch
earlier than validation loss, while losing virtually no accuracy (0.08%). Figure 4c depicts detailed
differences in accuracy and epoch for our measure when compared to validation loss; each cell in
a heatmap corresponds to a single parameter configuration of b and g . In the heatmap of accuracy
differences, blue, white, and red represent parameter combinations for which we obtain higher, equal,
or lower accuracy, respectively, than with validation loss for the same parameters. Similarly, in the
Algorithm 2 Early stopping based on mean normalized neural persistence
Require: Weighted neural network N, patience g, ∆min
1:	P J 0, G J 0	> Initialize highest observed value and patience counter
2:	ProcedureFARLYSTOPping(N, g, ∆min)	> Callback that monitors training at every epoch
3:	P0 J NP(N)
4:	if P0 > P + ∆min then . Update mean normalized neural persistence and reset counter
5:	P J P0, G J 0
6:	else	. Update patience counter
7:	G J G + 1
8:	end if
9:	if G ≥ g then	. Patience criterion has been triggered
10:	return P	. Stop training and return highest observed value
11:	end if
12:	end procedure
7
Published as a conference paper at ICLR 2019
(a) Fashion-MNIST
Data set	Barycentre	Final test accuracy
Fashion-MNIST (-0.53, -0.08)	86.72 ±	0.43
MNIST	(+0.17, -0.06)	96.16 ±	0.24
CIFAR-10	(-1.33, -1.13)	52.19 ±	3.40
IMDB	(-1.68, +0.07)	87.35 ±	0.03
(c) Accuracy and epoch differences
NP
24
20
16
q 12
8
4
0
0	4	8 12 16 20 24
g
(d) Number of triggers
(b) Summary
Figure 4: The visualizations depict the differences in accuracy and epoch for all comparison scen-
arios of mean normalized neural persistence versus validation loss, while the table summarizes the
results on other data sets. Final test accuracies are shown irrespectively of early stopping to put the
accuracy differences into context.
heatmap of epoch differences, green represents parameter combinations for which we stop earlier
than validation loss. For b ≤ 8, we stop earlier (0.62 epochs on average), while losing only 0.06%
accuracy. Finally, Figure 4d shows how often each measure is triggered. Ideally, each measure
should consist of a dark green triangle, as this would indicate that each configuration stops all the
time. For this data set, we observe that our method stops for more parameter combinations than
validation loss, but not as frequently for all of them. To ensure comparability across scenarios, we
did not use the validation data as additional training data when stopping with neural persistence;
we refer to Section A.7 for additional experiments in data scarcity scenarios. We observe that our
method stops earlier when overfitting can occur, and it stops later when longer training is beneficial.
5	Discussion
In this work, we presented neural persistence, a novel topological measure of the structural com-
plexity of deep neural networks. We showed that this measure captures topological information
that pertains to deep learning performance. Being rooted in a rich body of research, our meas-
ure is theoretically well-defined and, in contrast to previous work, generally applicable as well as
computationally efficient. We showed that our measure correctly identifies networks that employ
best practices such as dropout and batch normalization. Moreover, we developed an early stopping
criterion that exhibits competitive performance while not relying on a separate validation data set.
Thus, by saving valuable data for training, we managed to boost accuracy, which can be crucial for
enabling deep learning in regimes of smaller sample sizes. Following Theorem 2, we also experi-
mented with using the p-norm of all weights of the neural network as a proxy for neural persistence.
However, this did not yield an early stopping measure because it was never triggered, thereby sug-
gesting that neural persistence captures salient information that would otherwise be hidden among
all the weights of a network. We extended our framework to convolutional neural networks (see
Section A.4) by deriving a closed-form approximation, and observed that an early stopping criterion
based on neural persistence for convolutional layers will require additional work. Furthermore, we
conjecture that assessing dissimilarities of networks by means of persistence diagrams (making use
of higher-dimensional topological features), for example, will lead to further insights regarding their
generalization and learning abilities. Another interesting avenue for future research would concern
the analysis of the ‘function space’ learned by a neural network. On a more general level, neural
persistence demonstrates the great potential of topological data analysis in machine learning.
8
Published as a conference paper at ICLR 2019
References
Mardn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Man6, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wat-
tenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learn-
ing on heterogeneous systems: Simple, end-to-end, LeNet-5-like convolutional MNIST model
example, 2015. URL https://github.com/tensorflow/models/blob/master/
tutorials/image/mnist/convolutional.py.
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. Journal ofMachine Learning Research, 18:1-34, 2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations (ICLR),
2015.
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In
Gregoire Montavon, Genevieve B. Orr, and Klaus-Robert Muller (eds.), Neural Networks: Tricks
of the Trade, volume 7700 of Lecture Notes in Computer Science, pp. 437-478. Springer, Heidel-
berg, Germany, 2012.
Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A com-
parison between shallow and deep architectures. IEEE Transactions on Neural Networks and
Learning Systems, 25(8):1553-1565, 2014.
Peter Bubenik. Statistical topological data analysis using persistence landscapes. Journal of Machine
Learning Research, 16:77-102, 2015.
Gunnar Carlsson and Facundo Memoli. Characterization, stability and convergence of hierarchical
clustering methods. Journal of Machine Learning Research, 11:1425-1470, 2010.
Gunnar Carlsson, Tigran Ishkhanov, Vin de Silva, and Afra Zomorodian. On the local behavior of
spaces of natural images. International Journal of Computer Vision, 76(1):1-12, 2008.
Corrie J. Carstens and Kathy J. Horadam. Persistent homology of collaboration networks. Mathem-
atical Problems in Engineering, 2013:815035, 2013.
Travers Ching, Daniel S. Himmelstein, Brett K. Beaulieu-Jones, Alexandr A. Kalinin, Brian T. Do,
Gregory P. Way, Enrico Ferrero, Paul-Michael Agapow, Michael Zietz, Michael M. Hoffman,
Weil Xie, Gail L. Rosen, Benjamin J. Lengerich, Johnny Israeli, Jack Lanchantin, Stephen Wo-
loszynek, Anne E. Carpenter, Avanti Shrikumar, Jinbo Xu, Evan M. Cofer, Christopher A. Lav-
ender, Srinivas C. Turaga, Amr M. Alexandri, Zhiyong Lu, David J. Harris, Dave DeCaprio,
Yanjun Qi, Anshul Kundaje, Yifan Peng, Laura K. Wiley, Marwin H.S. Segler, Simina M. Boca,
S. Joshua Swamidass, Austin Huang, Anthony Gitter, and Casey S. Greene. Opportunities and
obstacles for deep learning in biology and medicine. Journal of The Royal Society Interface, 15
(141):20170387, 2018.
FrangoisCholletetal. Keras. https://keras.io, 2015.
David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Extending persistence using Poincare
and Lefschetz duality. Foundations of Computational Mathematics, 9(1):79-103, 2009.
David Cohen-Steiner, Herbert Edelsbrunner, John Harer, and Yuriy Mileyko. Lipschitz functions
have Lp-stable persistence. Foundations of Computational Mathematics, 10(2):127-139, 2010.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to
algorithms. MIT Press, Cambridge, MA, USA, 3rd edition, 2009.
Herbert Edelsbrunner and John Harer. Computational topology: An introduction. American Math-
ematical Society, Providence, RI, USA, 2010.
9
Published as a conference paper at ICLR 2019
Herbert Edelsbrunner, David Letscher, and Afra J. Zomorodian. Topological persistence and simpli-
fication. Discrete & Computational Geometry, 28(4):511-533, 2002.
William H Guss and Ruslan Salakhutdinov. On characterizing the capacity of neural networks using
algebraic topology. arXiv preprint arXiv:1802.04443, 2018.
Kazuyuki Hara, Daisuke Saitoh, and Hayaru Shouno. Analysis of dropout learning regarded as
ensemble learning. In Alessandro E.P. Villa, Paolo Masulli, and Antonio Javier Pons Rivero
(eds.), Artificial Neural Networks and Machine Learning (ICANN), number 9887 in Lecture Notes
in Computer Science, pp. 72-79, Cham, Switzerland, 2016. Springer.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image re-
cognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016.
Christoph Hofer, Roland Kwitt, Marc Niethammer, and Andreas Uhl. Deep learning with topolo-
gical signatures. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1633-
1643, 2017.
Danijela Horak, Slobodan Maletic, and Milan Rajkovic. Persistent homology of complex networks.
Journal of Statistical Mechanics: Theory and Experiment, 2009(03):P03034, 2009.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 7132-7141, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 448-456. PMLR, 2015.
Valentin Khrulkov and Ivan Oseledets. Geometry score: A method for comparing generative ad-
versarial networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re-
search, pp. 2621-2629. PMLR, 2018.
Pek Y. Lum, Gurjeet Singh, Alan Lehman, Tigran Ishkanov, Mikael Vejdemo-Johansson, Muthu
Alagappan, John Carlsson, and Gunnar Carlsson. Extracting insights from the shape of complex
data using topology. Scientific Reports, 3:1-8, 2013.
GregOire Montavon, WCjciech Samek, and KlaUs-RObert Muller. Methods for interpreting and Un-
derstanding deep neural networks. Digital Signal Processing, 73:1-15, 2017.
James R. MUnkres. Elements of algebraic topology. CRC Press, Boca Raton, FL, USA, 1996.
Maithra RaghU, Ben Poole, Jon Kleinberg, SUrya GangUli, and Jascha Sohl-Dickstein. On the ex-
pressive power of deep neUral networks. In Doina PrecUp and Yee Whye Teh (eds.), Proceedings
of the 34th International Conference on Machine Learning, volUme 70 of Proceedings of Machine
Learning Research, pp. 2847-2854. PMLR, 2017.
Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M. Dai, Nissan Hajaj, Michaela Hardt, Peter J LiU,
Xiaobing LiU, Jake MarcUs, Mimi SUn, et al. Scalable and accUrate deep learning with electronic
health records. npj Digital Medicine, 1(1):18, 2018.
Pranav RajpUrkar, Jeremy Irvin, Kaylie ZhU, Brandon Yang, Hershel Mehta, Tony DUan, Daisy
Ding, Aarti BagUl, CUrtis Langlotz, Katie Shpanskaya, Matthew LUngren, and Andrew Y. Ng.
CheXNet: Radiologist-level pneUmonia detection on chest X-rays with deep learning. arXiv
preprint arXiv:1711.05225, 2017.
Bastian Rieck and Heike Leitte. Exploring and comparing clUsterings of mUltivariate data sets Using
persistent homology. Computer Graphics Forum, 35(3):81-90, 2016.
Bastian Rieck, Ulderico FUgacci, Jonas LUkasczyk, and Heike Leitte. CliqUe commUnity persistence:
A topological visUal analysis approach for complex networks. IEEE Transactions on Visualization
and Computer Graphics, 24(1):822-831, 2018.
10
Published as a conference paper at ICLR 2019
Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky,
Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep
learning. In International Conference on Learning Representations (ICLR), 2018.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations (ICLR), 2015.
Ann Sizemore, Chad Giusti, and Danielle S. Bassett. Classification of weighted networks through
mesoscale homological features. Journal of Complex Networks, 5(2):245-273, 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. In Workshop Track of the International Conference on
Learning Representations (ICLR), 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(1):1929-1958, 2014.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 3104-3112, 2014.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
IEEE Information Theory Workshop (ITW), pp. 1-5, 2015.
Michael Tsang, Dehua Cheng, and Yan Liu. Detecting statistical interactions from neural network
weights. In International Conference on Learning Representations (ICLR), 2018.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), European Conference
on Computer Vision (ECCV), volume 8689 of Lecture Notes in Computer Science, pp. 818-833,
Cham, Switzerland, 2014. Springer.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations (ICLR), 2017.
11
Published as a conference paper at ICLR 2019
0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1
Clustering coefficient
—
14	15	16	17	18	19	20	21	22	23	24	25
Neural persistence
Figure A.1: Traditional graph measures (top), such as the clustering coefficient, fail to detect dif-
ferences in the complexity of neural networks. Our novel neural persistence measure (bottom), by
contrast, shows that trained networks with η = 0.5 (green), which have an accuracy of ≈ 0.91, obey
a different distribution than networks trained with η = 1 × 10-0.5 (yellow), which have accuracies
ranging from 0.38-0.65.
A	Appendix
A.1 Comparison with graph-theoretical measures
Traditional complexity/structural measures from graph theory, such as the clustering coefficient, the
average shortest path length, and global/local efficiency are already known to be insufficiently accur-
ate to characterize different models of complex random networks Sizemore et al. (2017). Our exper-
iments indicate that this holds true for (deep) neural networks, too. As a brief example, we trained a
perceptron on the MNIST data set with batch stochastic gradient descent (η = 0.5), achieving a test
accuracy of ≈ 0.91. Moreover, we intentionally ‘sabotaged’ the training by setting η = 1 × 10-5
such that SGD is unable to converge properly. This leads to networks with accuracies ranging from
0.38-0.65. A complexity measure should be capable of distinguishing both classes of networks.
However, as Figure A.1 (top) shows, this is not the case for the clustering coefficient. Neural persist-
ence (bottom), on the other hand, results in two regimes that can clearly be distinguished, with the
trained networks having a significantly smaller variance.
A.2 Proof of Theorem 2
Proof. We may consider the filtration from Section 3.1 to be a subset selection problem with con-
straints, where we select n out of m weights. The neural persistence NP(Gk) of a layer thus only
depends on the selected weights that appear as tuples of the form (1, wi) in Dk. Letting we denote the
vector of selected weights arising from the persistence diagram calculation, we can rewrite neural
persistence as NP(Gk) = k1 - wekp. Furthermore, we satisfies kwminkp ≤ kwe kp ≤ kwmaxkp.
Since all transformed weights are non-negative in our filtration, it follows that (note the reversal of
the two terms)
k1 -wmaxkp ≤ NP(Gk) ≤ k1 - wminkp,	(6)
and the claim follows.	□
A.3 Additional visualizations and analyses for early stopping
Due to space constraints and the large number of configurations that we investigated for our early
stopping experiments, this section contains additional plots that follow the same schematic: the top
row shows the differences in accuracy and epoch for our measure when compared to the commonly-
used validation loss. Each cell in the heatmap corresponds to a single configuration of b and g .
In the heatmap of accuracy differences, blue represents parameter combinations for which we ob-
tain a higher accuracy than validation loss for the same parameters; white indicates combinations
for which we obtain the same accuracy, while red highlights combinations in which our accuracy
decreases. Similarly, in the heatmap of epoch differences, green represents parameter combina-
tions for which we stop earlier than validation loss for the same parameter. The scatterplots in
12
Published as a conference paper at ICLR 2019
Section 4.2 show an ‘unrolled’ version of this heat map, making it possible to count how many para-
meter combinations result in early stops while also increasing accuracy, for example. The heatmaps,
by contrast, make it possible to compare the behaviour of the two measures with respect to each
parameter combination. Finally, the bottom row of every plot shows how many times each measure
was triggered for every parameter combination. We consider a measure to be triggered if its stopping
condition is satisfied prior to the last training epoch. Due to the way the parameter grid is set up,
no configuration above the diagonal can stop, because b + g would be larger than the total number
of training epochs. This permits us to compare the ‘slopes’ of cells for each measure. Ideally, each
measure should consist of a dark green triangle, as this would indicate that parameter configuration
stops all the time.
MNIST Please refer to Figures A.2 and A.3. The colours in the difference matrix of the top row
are slightly skewed because in a certain configuration, our measure loses 0.8% of accuracy when
stopping. However, there are many other configurations in which virtually no accuracy is lost and
in which we are able to stop more than four epochs earlier. The heatmaps in the bottom row again
indicate that neural persistence is capable of stopping for more parameter combinations in general.
We do not trigger as often for some of them, though.
CIFAR-10 Please refer to Figure A.4. In general, we observe that this data set is more sensitive
with respect to the parameters for early stopping. While there are several configurations in which
neural persistence stops with an increase of almost 10% in accuracy, there are also scenarios in
which we cannot stop training earlier, or have to train longer (up to 15 epochs out of 80 epochs
in total). The second row of plots shows our measure triggers reliably for more configurations
than validation loss. Overall, the scatterplot of all scenarios (Figure A.5) shows that most practical
configurations are again located in Q2 and Q3 . While we may thus find certain configurations in
which we reliably outperform validation loss as an early stopping criterion, we also want to point
out that our measures behaves correctly for many practical configurations. Points in Q1, where we
train longer and achieve a higher accuracy, are characterized by a high patience g of approximately
40 epochs and a low burn-in rate b, or vice versa. This is caused by the training for CIFAR-10,
which does not reliably converge for FCNs. Figure A.6 demonstrates this by showing loss curves
and the mean normalized neural persistence curves of five runs over training (loss curves have been
averaged over all runs; standard deviations are shown in grey; we show the first half of the training to
highlight the behaviour for practical early stopping conditions). For 'Fashion-MNIST', We observe
that NP exhibits clear change points during the training process, which can be exploited for early
stopping. For ‘CIFAR-10’, We observe a rather incremental groWth for some runs (With no clearly-
defined maximum), making it harder to derive a generic early stopping criterion that does not depend
on fine-tuned parameters. Hence, we hypothesize that neural persistence cannot be used reliably
in scenarios where the architecture is incapable of learning the data set. In the future, we plan
to experiment with deliberately selected ‘bad’ and ‘good’ architectures in order to evaluate to what
extent our topological measure is capable of assessing their suitability for training, but this is beyond
the scope of this paper.
IMDB Please refer to Figure A.7. For this data set, we observe that most parameter configurations
result in earlier stopping (up to two epochs earlier than validation loss), with accuracy increases
of up to 0.10%. This is also shown in the scatterplot A.8. Only a single configuration, viz. g = 1
and b = 0, results in a severe loss of accuracy; we removed it from the scatterplot for reasons of
clarity, as its accuracy difference of -21% would skew the display of the remaining configurations
too much (this is also why the legends do not include this outlier).
13
Published as a conference paper at ICLR 2019
Q
0.8
0.6
0.4
0.2
0.0
-0.2
-0.4
-0.6
-0.8
24 Epoch difference
0	4	8 12 16 20 24
g
(a) Accuracy and epoch differences
g
g
(b) Number of triggers
Figure A.2: Additional visualizations for the 'MNIST’ data set.
0.4
0.2
0
—0.2
—0.4
AccUraCy difference
—8	- 6	- 4	- 2	0	2	4	6	8
Epoch difference
Figure A.3: Scatterplot of epoch and accuracy differences for ‘MNIST’.
14
Published as a conference paper at ICLR 2019
Accuracy difference
Epoch difference
(a) Accuracy and epoch differences
(b) Number of triggers
Figure A.4: Additional visualizations for the 'CIFAR-10’ data set.
ωou°J°tllp"BJnOO-V
10
8
6
4
2
0
-	2
-	4
-	6
-	8
-10
-20	-15	-10	- 5	0	5	10	15	20
Epoch difference
Figure A.5: Scatterplot of epoch and accuracy differences for ‘CIFAR-10’.
15
Published as a conference paper at ICLR 2019
Figure A.6: A comparison of mean normalized neural persistence curves that we obtain during the
training of ‘CIFAR-10’ and ‘Fashion-MNIST’.
g
0.10
0.05
0.00
-0.05
-0.10
24
Epoch difference
20
16
12
8
4
2
1
0
-1
-2
0
0	4	8 12 16 20 24
g
(a)	Accuracy and epoch differences
Validation loss
24
NP
20
16
q 12
8
4
50
40
30
20
10
」0
0
0	4	8 12 16 20 24
g
24
20
16
12
8
4
50
40
30
20
10
」0
0
0	4	8	12 16 20 24
g
(b)	Number of triggers
Figure A.7: Additional visualizations for the ‘IMDB’ data set.
16
Published as a conference paper at ICLR 2019
0.2
uouu,wieJP&EIn。。V
—0.2
-3
0.15
0. 1
5 ∙ 10 — 2
—5 ∙ 10
-2
I B t Q 2 2 卜ɪ: •」I	q 1
* ∙ . Q 3 			* . Q 4 		
—10	1
Epoch difference
23
Figure A.8: Scatterplot of epoch and accuracy differences for 'IMDB’.
17
Published as a conference paper at ICLR 2019
A.4 Neural Persistence for Convolutional Layers
In principle, the proposed filtration process could be applied to any bipartite graph. Hence, we
can directly apply our framework to convolutional layers, provided we represent them properly.
Specifically, for layer l we represent the convolution of its ith input feature map ai(l-1) ∈ Rhin×win
with the jth filter Hj ∈ Rp×q as one bipartite graph Gi,j parametrized by a sparse weight matrix
W(lj ∈ R(hour仅out)×(hinWin), which in each row contains the P ∙ q unrolled values of Hj on the
diagonal, with hin - p zeros padded in between after each p values of vec(Hj). This way, the
flattened pre-activation can be described as vec( zj = Wj ∙ vec( a(l-1)) + bij ∙ 1( h。世.仅 OUt) × 1.
Since flattening does not change the topology of our bipartite graph, we compute the normalized
neural persistence on this sparse weight matrix Wi(,lj) as the unrolled analogue of the fully-connected
network’s weight matrix. Averaging over all filters then gives a per-layer measure, similar to the
way we derived mean normalized neural persistence in the main paper.
When studying the unrolled adjacency matrix Wi(,lj), it becomes clear that the edge filtration process
can be approximated in a closed form. Specifically, for m and n input and output neurons we
initialize τ = m + n connected components. When using zero padding, the additional dummy input
neurons have to included in m. For all τ tuples in the persistence diagram the creation event c = 1.
Notably, each output neuron shares the same set of edge weights.
Due to this, the destruction events—except for a few special cases—simplify to a list of length τ
containing the largest filter values (each value is contained n times) in descending order until the
list is filled. This simplification of neural persistence of a convolution with one filter is shown as a
closed expression in Equations 7-11, and our implementation is sketched in Algorithm 3. We thus
obtain
where we use
with
NP(Gi,j) = k1-wekp,
Il河Ip ≤ ∣∣(0, WT, wTφ, VeC(Aφ)T, VeC(Bφ)T )T∣p ,
φ
Ax
By
- dim(Wc) - 1,
1： b xC Z) 1 n— 1,
byC +ι 0 1 y mod n,
(7)
(8)
(9)
(10)
(11)
τ
W
W
where 10 := 0. Following this notation, Equation 7 expresses neural persistence of the bipartite
graph Gi,j, with We denoting the vector of selected weights (i.e. the destruction events) when cal-
culating the persistence diagram. We use W to denote the flattened and sorted weight values (in
descending order) of the convolutional filter Hj, while Wc represents the vector of all weights that
are located in a corner of Hj, whereas Wcφ is the vector of all weights which do not originate from
the corner of the filter while still belonging to the first (and thus largest)
we denote by W
1： b ∙n c ■
闾
weights in W, which
For the subsequent experiments (see below), we use a simple CNN that employs 32 + 2048 filters.
Hence, by using the shortcut described above, we do not have to unroll 2080 weight matrices expli-
citly, thereby gaining both in memory efficiency and run time, as compared to the naive approach:
on average, a naive exact computation based on unrolling required 8.77 s per convolutional filter and
evaluation step, whereas the approximation only took about 0.000 38 s while showing very similar
behaviour up to a constant offset.
For our experiments, we used an off-the-shelf ‘LeNet-like’ CNN model architecture (two convo-
lutional layers each with max pooling and ReLU, 1 fully-connected and softmax) as described in
Abadi et al. (2015). We trained the model on ‘Fashion-MNIST’ and included this setup in the early
stopping experiments (100 runs of 20 epochs). In Figure A.9, we observe that stopping based on
the neural persistence of a convolutional layer typically only incurs a considerable loss of accuracy:
given a final test accuracy of91.73±0.13, stopping with this naive extension of our measure reduces
accuracy by up to 4%. Furthermore, in contrast to early stopping on a fully-connected architecture,
18
Published as a conference paper at ICLR 2019
Algorithm 3 Approximating Neural Persistence of Convolutions per filter
Require: filter H ∈ Rp×q ; number of input and output neurons as m, n
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
T ~ 8	. Initialize set of tuples for persistence diagram
T J m + n, t J 0, i J 0	. Initialize number of tuples, tuple counter, weight index
hmax J maxh∈H ∣h∣	. Determine largest absolute weight
H0 J {|h|/hmax | h ∈ H}	. Transform weights for filtration
s J sort(vec(H0))	. Sort weights in descending order
Hc0 J {h00,0, h00,q-1, h0p-1,0, h0p-1,q-1} . Determine the set of all corner weights of filter H0
T J (1, 0), t J t + 1	. Add tuple for surviving component
for h0c ∈ Hc0 do	. Each corner of H0 merges components
T J (1, h0c ), t J t + 1
end for
while 1 do	. Create the remaining tuples (Approximation step)
n0 = n - Ind(s[i] ∈ Hc0)	. if current weight is a corner weight, write one less tuple
if t + n0 ≤ τ then . if there are at least n0 more tuples, set their merge value to s[i]
repeat n0 times
T J (1, s[i])	. approximative as s[i] does not always add n0 merges due to loops
t J t + n0, i J i + 1
else	. otherwise, process the remaining tuples similarly
repeat (τ - t) times
TJ(1,s[i])
break
end if
end while
return kTkp	. Compute norm of approximated persistence diagram
we do not observe any parameter combinations that stop early and increase accuracy. In fact, there
is no configuration that results in an increased accuracy. This empirically confirms our theoretical
scepticism towards naively applying our edge-focused filtration scheme to CNNs.
A.5 Relationship between neural persistence and validation accuracy
Motivated by Figure 2, which shows the different ‘regimes’ of neural persistence for a perceptron
network, we investigate a possible correlation of (high) neural persistence with (high) predictive
accuracy. For deeper networks, we find that neural persistence measures structural properties that
arise from different parameters (such as training procedures or initializations), and no correlation
can be observed.
For our experiments, we constructed neural networks with a high neural persistence prior to training.
More precisely, following the theorems in this paper, we initialized most weights of each layer with
very low values and reserved high values for very few weights. This was achieved by sampling the
weights from a beta distribution With α = 0.005 and β = 0.5. Using this procedure, We are able to
initialize [20,20,20] networks with NP ≈ 0.90 ± 0.003 compared to the same networks that have
NP ≈ 0.38 ± 0.004 when initialized by Xavier initialization. The mean validation accuracy of these
untrained networks on the ‘Fashion-MNIST’ data set is 0.10 ± 0.01 and 0.09 ± 0.03, respectively.
Figure A.10 depicts how both types of networks converge to similar regimes of validation accuracy,
while the mean normalized neural persistence achieved at the end of the training varies. For networks
initialized with high NP (Figure A.10, left) the validation accuracy of networks with final 0.9 ≤
NP ≤ 0.95 ranges from 0.098 (not shown) to 0.863. For Xavier initialization (Figure A.10, right),
the lack of correlation can also be observed. Furthermore, comparing the two plots, there are no clear
advantages in initializing networks with high NP. This observation further motivates the proposed
early stopping criterion, which checks for changes in the NP value, and considers stagnating values
to be indicative of a trained network.
19
Published as a conference paper at ICLR 2019
Epoch difference
9
(a)	Accuracy and epoch differences
100
80
60
40
20
0
(b)	Number of triggers
Figure A.9: Additional visualizations for the ‘Fashion-MNIST’ data set, following the preliminary
examination of convolutional layers. Here, the approximated neural persistence calculation for the
first convolutional layer was used. However, we also ran few runs of the same experiment using the
exact method which showed the same results. Employing the second convolutional layer or both did
not improve this result.
Mean normalized neural persistence
Mean normalized neural persistence
Figure A.10: Each cluster of points represent the last two training epochs (sampled every quarter
epoch) of a [20,20,20] network trained on the ‘Fashion-MNIST’ data set. We observe no correlation
between validation accuracy and normalized total persistence
20
Published as a conference paper at ICLR 2019
12W
.50
0.55	0.60	0.65	0.70	0.75	0.80	0.85	0.90
Normalized Neural Persistence
∙87
O O
uus&d -2nN PZ-E∙ION
°,5^	[20]
[3×20]	[6x20]	[8×20]	[16x20]	[20x20]
Figure A.11: (left) Histogram of the final normalized neural persistence of a [50, 50, 20] network for
100 runs and 25 epochs of training. (right) Normalized neural persistence after 15 epochs of training
on MNIST for different architectures with increasing depth. Deeper architectures are denoted as
[n × 20] where n is the number of hidden layers.
A.6 Neural persistence for different data distributions and deeper FCN
ARCHITECTURES
Neural persistence captures information about different data distributions during training. The
weights tuned via backpropagation are directly influenced by the input data (as well as their labels)
and neural persistence tracks those changes. To demonstrate this, we trained the same architecture ,
i.e. [50, 50, 20], on two data sets with the same dimensions but different properties: MNIST and
‘Fashion-MNIST’. Each data set has the same image size (28 × 28 pixels, one channel) but lay on
differentImanifolds. Figure A.11 (left) shows a histogram of the mean normalized neural persist-
ence (NP) after 25 epochs of training over 100 different runs. The distributions have a similar shape
but are shifted, indicating that the two datasets lead the network to different topological regimes.
We also investigated the effect of depth on neural persistence. We selected a fixed layer size (20
hidden units) and increased the number of hidden layers. Figure A.11 (right) depicts the boxplots of
mean NP for multiple architectures after 15 epochs of training on MNIST. Adding layers initially
increases the variability of NP by enabling the network to converge to different regimes (essentially,
there are many more valid configurations in which a trained neural network might end up in). How-
ever, this effect is reduced after a certain depth: networks with deeper architectures exhibit less
variability in NP.
21
Published as a conference paper at ICLR 2019
A.7 Early stopping in data scarcity scenarios
Labelled data is expensive in most domains of interest, which results in small data sets or low qual-
ity of the labels. We investigate the following experimental set-ups: (1) Reducing the training data
set size and (2) Permuting a fraction of the training labels. We train a fully connected network
([500, 500, 200] architecture) on ‘MNIST’ and ‘Fashion-MNIST’. In the experiments, we compare
the following measures for stopping the training: i) Stopping at the optimal test accuracy. ii) Fixed
stopping after the burn in period. iii) Neural persistence patience criterion. iv) Training loss patience
criterion. v) Validation loss patience criterion. For a description of the patience criterion, see Al-
gorithm 2. All measures, except validation loss, include the validation datasets (20%) in the training
process to simulate a larger data set when no cross-validation is required. We report the accuracy
on the non-reduced, non-permuted test sets. The batch size is 32 training instances. The stopping
measures are evaluated every quarter epoch.
Figure A.12 shows the results averaged over 10 runs (the error is the standard deviation). The
difference between the top and the bottom panel is the data set and the patience parameters. The
x-axis depicts the fraction of the data set, which is warped for better accessibility. In each panel, the
left-hand side subplots depict the results of the reduced data set experiment where the right-hand
side subplots depict the result of the permutation experiments. The y-axis of the top subplot shows
the accuracy on the non-reduced, non-permuted test set. The y-axis of the bottom subplot shows
when the stopping criterion was triggered.
We note the following observations, which hold for both panels: More, non-permuted data yields
higher test accuracy. Also, as expected, the optimal stopping gives the highest test accuracy. The
fixed early stopping results in inferior test accuracy when only a fraction of the data is available. The
neural persistence based stopping is triggered late when only a fraction of the data is available which
results in a slightly better test accuracy compared to training and validation loss. The training loss
stopping achieves similar test accuracies compared to the persistence based stopping (for all regimes
except the very small data set) with shorter training, on average. We note that, it is generally not
advisable to use training loss as a measure for stopping because the stability of this criterion also
depends on the batch size. When only a fraction of the data is available, the validation loss based
stopping stops on average after the same number of training epochs as the training loss, which
results in inferior test accuracy because the network has seen in total fewer training samples. Most
strikingly, validation loss based stopping is is triggered later (sometimes never) when most training
and validation labels are randomly permuted which results in overfitting and poor test accuracy.
To conclude, the neural persistence based stopping achieves good performance without being af-
fected by the batch size and noisy labels. The authors also note that the result is consistent for
multiple architectures and most patience parameters.
22

• sqoodo jə:ɪenb uτ pəuodəj əib sjə:əuibib(i uτ UJnq PUB əɔuəpBd əm `sɪəgnɪ pə:nuiɪəd Jo uoμoBJj q而q
B γe gmuτBJi jəguoɪ jə:jb Aobjuoob isəi jə/ʌoɪ oι spnəɪ qoτqM 3 4gmddoιs ssoɪ uoμBp∏BΛ gψ o： ISBiJUoɔ
m 'Q[qυιs sutbuiəj dN J0 既u[ddois oψ 4(opτs PUBqTq而D sɪəgnɪ guτuren oψ m əstou gmsŋəjɔm joj
•AoBjnooBisəi joqgτψeuτ s:ɪnsəjqoτqM (əpɪsPlreqTJəjQjqBjTBABəib səɪduɪŋs gmureu jəməjuoqM ssoɪ
gmureu PUB uoμBp∏BΛ uυψ jə:nɪ SdolS (ənɪg m) dN JLSINW-UOiqsem puυ isinw UO γχ*y əjngɪj
uo∏epι∣eΛ
6uιuιejχ
3Uθ4S∣SJθd
Pθ×!d
∣eιuπd∩
}壬壬壬千
s∣eqe∣ pə4niujəd JO uoιpejj
OT 90 VO ZO TO
O = u∣ uj∏q
9τ = əɔuəQed
[00 乙'00UOO3
ISINW
uo∏epι∣eΛ
6uιuιejχ
3Uθ4S∣SJθd
Pθ×!d
∣eιuπd∩
}壬壬壬壬
O-T 66 0 6 0 9 0 VO T O TO O O O
test accuracy
-SZ O
-OSO
-SZ. 0
OO-T
s∣eqe∣ pəiɪiuuəd JO uoιpejj
O-T 66 0 6 0 9 0 TO T O TO O O O
-O
-OT
-OZ
e;ep JO uo∏□ejj
OT 90 VO ZO TO
O T 66 0 6 0 9 0 VO T OTO OO O O-T 9 0 VO Z O T O
3 = ui uj∏q
t7 = əɔuəQed
[00 乙'00UOOS]
ISINN uoiqsej
-SlO
-OSO
-SZ.-0
-ooτ
-Z60
-WO
-960
-860
test accuracy
610% mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Table A.l: Parameters and hyperparameters for the experiment on best practices and neural persistence. Dropout and batch normalization were applied after the
first hidden layer. Throughout the networks, ReLU was the activation function of choice.
Data set	# Runs	# Epochs	Architecture	Optimizer	Batch Size	Hyperparameters
						η = 0.0003 BI = 0.9, β2 = 0.999, e = 1 × 10-8
MNIST	50	40	[650, 650]	Adam	32	η = 0.0003 ∕31 = 0.9, ∕⅜ = 0.999, e = 1 × IO-8, Batch Normalization
						η = 0.0003 ∕31 = 0.9, β2 = 0.999, e=l× 10-8, Dropout 50%
PUbliShed as a ConferenCe PaPersICLR 2019
圣	Table A.2: Parameters and hyperparameters for the experiment on early stopping. Throughout the networks, ReLU was the activation function of choice.
Data set	#Runs	# Epochs	Architecture	Optimizer	Batch Size	Hyperparameters
		10	Perceptron	Minibatch SGD	100	η = 0.5
(Fashion-)MNIST	100		[50, 50, 20]			
		40	[300,100]	Adam	32	η = 0.0003 βι = 0.9, β2 = 0.999, e = 1 × 10-8
			[20, 20, 20]			
CIFAR-IO	10	80	[800, 300, 800]	Adam	128	η = 0.0003 βι = 0.9, β2 = 0.999, e = 1 × 10-8
IMDB	5	25	[128, 64,16]	Adam	128	τ? = 1 × 10—5 β[ = o.9, β2 = 0.999, e = 1 × 10-8
Published as a conference paper at ICLR 2019

0.980	0.981	0.982	0.983	0.984	0.985	0.986	0.987	0.988
Test set accuracy
Figure A.13: Comparison of test set accuracy for trained networks without modifications (green),
with batch normalization (yellow), and with 50% of the neurons dropped out during training (red)
for the MNIST data set.
A.8 Testing accuracy of differently regularized models
We showed in the main text that neural persistence is capable of distinguishing between networks
trained with/without batch normalization and/or dropout. Figure A.13 additionally shows test set
accuracies.
25