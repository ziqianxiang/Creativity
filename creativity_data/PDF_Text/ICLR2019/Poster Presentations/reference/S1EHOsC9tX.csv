title,year,conference
 Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Estimating or propagating gradients through stochasticneurons for conditional computation,2013, arXiv preprint arXiv:1308
 Comment on “biologically inspired protection of deep networks from adversarialattacks”,2017, arXiv preprint arXiv:1704
 Decision-based adversarial attacks: Reliable attacksagainst black-box machine learning models,2018, In International Conference on Learning Representations
 Thermometer encoding: One hot way toresist adversarial examples,2018, In International Conference on Learning Representations
" Magnet and"" efficient defenses against adversarial attacks"" are not robust toadversarial examples",2017, arXiv preprint arXiv:1711
 Boostingadversarial attacks with momentum,2017, arxiv PrePrint
 A generative visionmodel that trains with high data efficiency and breaks text-based captchas,2017, Science
 Countering adversarial imagesusing input transformations,2018, In International Conference on Learning Representations
 Formal guarantees on the robustness of a classifier againstadversarial manipulation,2017, In Advances in Neural Information Processing Systems 30
 The robust manifolddefense: Adversarial training using generative models,2017, arXiv preprint arXiv:1712
 Black-box adversarial attacks with limitedqueries and information,2018, arXiv preprint arXiv:1804
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, arXiv preprint arXiv:1502
 Vectordefense: Vectorization as a defense toadversarial examples,2018, arXiv preprint arXiv:1804
 Adam: A method for stochastic optimization,2014, arXiv preprint arXiv:1412
 Auto-encoding variational bayes,2013, arXiv preprint arXiv:1312
 Adversarial examples in the physical world,2016, arXiv preprintarXiv:1607
 Defense against adversarialattacks using high-level representation guided denoiser,2017, arXiv preprint arXiv:1712
 Towards deeplearning models resistant to adversarial attacks,2018, In International Conference on Learning Representations
 Magnet: a two-pronged defense against adversarial examples,2017, In Proceedings ofthe 2017 ACM SIGSAC Conference on Computer and Communications Security
 Deep neural networks are easily fooled: High confidencepredictions for unrecognizable images,2015, In The IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Deflecting adversarialattacks with pixel deflection,2018, arXiv preprint arXiv:1801
 Foolbox: A python toolbox to benchmark the robustnessof machine learning models,2017, arXiv preprint arXiv:1707
 Defense-GAN: Protecting classifiers against adversarialattacks using generative models,2018, In International Conference on Learning Representations
 Adversariallyrobust generalization requires more data,2018, CoRR
 Ape-gan: Adversarial perturbation elimination withgan,2017, arXiv preprint arXiv:1707
 Pixeldefend: Leveraginggenerative models to understand and defend against adversarial examples,2018, In International Conference onLearning Representations
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Ensembleadversarial training: Attacks and defenses,2017, arXiv preprint arXiv:1705
 Mitigating adversarial effectsthrough randomization,2018, In International Conference on Learning Representations
 Distributionally adversarial attack,2018, arXiv preprintarXiv:1808
