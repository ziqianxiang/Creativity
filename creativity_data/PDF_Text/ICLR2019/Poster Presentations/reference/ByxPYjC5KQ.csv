title,year,conference
 Do GANs learn the distribution? some theory andempirics,2018, In International Conference on Learning Representations
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Many paths to equilibrium: GANs do not need to decrease a divergence at everystep,2018, In International Conference on Learning Representations
 Generative adversarial nets,2014, In Z
 Im-proved training of wasserstein gans,2017, In I
 In I,2017, Guyon
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Francis Bach and David Blei (eds
 Adam: A method for stochastic optimization,2014, CoRR
 Mixed batches and symmet-ric discriminators for GAN training,2018, In Jennifer Dy and Andreas Krause (eds
 Spectral normalizationfor generative adversarial networks,2018, In International Conference on Learning Representations
 Fisher gan,2017, In I
 Gradient descent gan optimization is locally stable,2017, InI
 Automatic differentiation inpytorch,2017, 2017
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2015, CoRR
 Stabilizing training ofgenerative adversarial networks through regularization,2017, In I
 Veegan:Reducing mode collapse in gans using implicit variational learning,2017, In I
 Wasserstein diver-gence for gans,2018, In Vittorio Ferrari
 On the discrimination-generalization tradeoff in GANs,2018, In International Conference on Learning Representations
003 for both G and DLearning rate TTUR-	0,1000,003 for G
