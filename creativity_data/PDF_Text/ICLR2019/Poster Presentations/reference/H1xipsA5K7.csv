title,year,conference
 Provable bounds for learning somedeep representations,2014, In International Conference on Machine Learning
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, arXiv preprint arXiv:1702
 Distributional and lq norm inequalities for polynomials over convex bodiesin Rn,2001, Mathematical research letters
 Improved learning of one-hidden-layer convolutional neural networkswith overlaps,2018, arXiv preprint arXiv:1805
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Learning one-hidden-layer neural networks under general input distributions,2018, arXiv preprint arXiv:1810
 No spurious local minima in nonconvex low rank problems: Aunified geometric analysis,2017, arXiv preprint arXiv:1704
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Learning depth-three neural networks in polynomial time,2017, arXivpreprint arXiv:1709
 Reliably learning the relu in polyno-mial time,2016, arXiv preprint arXiv:1611
 Learning one convolutional layer with overlappingpatches,2018, arXiv preprint arXiv:1802
 Beating the perils of non-convexity: Guar-anteed training of neural networks using tensor methods,2015, arXiv preprint arXiv:1506
 On the computational efficiency of trainingneural networks,2014, In Advances in Neural Information Processing Systems
 Polynomial-time tensor decompositions with sum-of-squares,2016, In Foundations of Computer Science (FOCS)
 End-to-end learning of a convolutional neural network viadeep tensor decomposition,2018, arXiv preprint arXiv:1805
 Learning relus via gradient descent,2017, In Advances in Neural InformationProcessing Systems
 Computer science and scientific computing,1990, matrix perturbation theory
 An analytical formula of population gradient for two-layered relu network and itsapplications in convergence and critical point analysis,2017, arXiv preprint arXiv:1703
 User-friendly tail bounds for sums of random matrices,2012, Foundations of computationalmathematics
 Learning one-hidden-layer relunetworks via gradient descent,2018, arXiv preprint arXiv:1806
 '1-regularized neural networks are improperlylearnable in polynomial time,2016, In International Conference on Machine Learning
 On the learnability of fully-connected neural networks,2017, In Artificial Intelligence and Statistics
 Recovery guaranteesfor one-hidden-layer neural networks,2017, arXiv preprint arXiv:1706
