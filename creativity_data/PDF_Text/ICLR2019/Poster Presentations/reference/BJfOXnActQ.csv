title,year,conference
 Learning to learn by gradient descent by gradient descent,2016, In Advances in NeuralInformation Processing Systems
 On the optimization ofa synaptic learningrule,1992, In Preprints Conf
 Multitask learning,1997, Machine learning
 A LEARNED REPRESENTATION FORARTISTIC STYLE,2017, ICLR
 Feature-wise transformations,2018, Distill
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In International Conference on Machine Learning
 Dynamic few-shot visual learning without forgetting,2018, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Learning to learn using gradient descent,2001, InInternational Conference on Artificial Neural Networks
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, arXiv preprint arXiv:1502
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Siamese neural networks for one-shot imagerecognition,2015, In ICML Deep Learning Workshop
 Learning to optimize neural nets,2017, arXiv preprint arXiv:1703
 Meta-learning with temporalconvolutions,2017, arXiv preprint arXiv:1707
 Meta networks,2017, arXiv preprint arXiv:1703
 Rapid adaptation withconditionally shifted neurons,2018, In International Conference on Machine Learning
 Tadam: Task dependent adaptive metric forimproved few-shot learning,2018, arXiv preprint arXiv:1805
 Film: Visualreasoning with a general conditioning layer,2017, arXiv preprint arXiv:1709
 Optimization as a model for few-shot learning,2016, 2016
 Meta-learning for semi-supervised few-shot classification,2018, arXivpreprint arXiv:1803
 Meta-learning with latent embedding optimization,2018, arXiv preprint arXiv:1807
 Prototypical networks for few-shot learning,2017, CoRR
 A perspective view and survey of meta-learning,2002, Artificial IntelligenceReview
 Matching networks for one shotlearning,2016, In Advances in Neural Information Processing Systems
