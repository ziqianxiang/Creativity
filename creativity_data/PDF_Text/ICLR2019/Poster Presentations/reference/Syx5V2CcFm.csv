title,year,conference
 Natasha: Faster non-convex stochastic optimization via strongly non-convexparameter,2017, In Proceedings of the 34th International Conference on Machine Learning (ICML)
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofInternational Conference on Computational Statistics (COMPSTAT)
 Accelerated methods for non-convex optimization,2016, CoRR
 On the convergence ofa class of adam-typealgorithms for non-convex optimization,2018, CoRR
 Sadagrad: Strongly adaptive stochastic gradi-ent methods,2018, In Proceedings of the 35th International Conference on Machine Learning (ICML)
 Large scaledistributed deep networks,2012, In NIPS
 Efficiency of minimizing compositions of convex functions andsmooth maps,2018, Mathematical Programming
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Math
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proceedings of the 32nd International Conference on MachineLearning
 Caffe: Convolutional architecture for fast feature embed-ding,2014, arXiv preprint arXiv:1408
 Adam: A method for stochastic optimization,2015, InternationalConference on Learning Representations
 Accelerated stochastic algorithms for nonconvex finite-sum and multi-block optimization,2018, CoRR
 On the convergence of stochastic gradient descent with adaptivestepsizes,2018, CoRR
 SGDR: stochastic gradient descent with restarts,2017, In ICLR
 Variants of RMSProp and Adagrad with log-arithmic regret bounds,2017, In Doina Precup and Yee Whye Teh (eds
 Stochastic alternating direction method ofmultipliers,2013, In International Conference on Machine Learning
 Automatic differentiation inpytorch,2017, 2017
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Convex Analysis,1970, Princeton mathematical series
 On the importance of ini-tialization and momentum in deep learning,2013, In Proceedings of the 30th International Conferenceon Machine Learning (ICML)
 Dual averaging and proximal gradient descent for online alternating direction multi-plier method,2013, In International Conference on Machine Learning
 Adagrad stepsizes: Sharp convergence over nonconvexlandscapes,2018, CoRR
 The marginalvalue of adaptive gradient methods in machine learning,2017, In NIPS
 Stochastic convex optimization: Faster local growth impliesfaster global convergence,2017, In ICML
 Online convex programming and generalized infinitesimal gradient ascent,2003, InICML
 On the convergence of adagrad with momentum for training deep neuralnetworks,2018, CoRR
 Following the analysis in Yang et al,2019, (2016)
