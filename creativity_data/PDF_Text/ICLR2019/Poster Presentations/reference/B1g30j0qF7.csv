title,year,conference
 Tensorflow: Large-scale machinelearning on heterogeneous distributed systems,2016, arXiv preprint arXiv:1603
 A gaussian process perspective on convolutional neural networks,2018, arXivpreprint arXiv:1810
 An analysis of deep neural networkmodels for practical applications,2016, arXiv preprint arXiv:1605
 Dynamical isometry and a mean fieldtheory of RNNs: Gating enables signal propagation in recurrent neural networks,2018, In Proceedingsof the 35th International Conference on Machine Learning
 Kernel methods for deep learning,2009, In Advances in neuralinformation processing systems
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Deep gaussian processes,2013, In Artificial Intelligence andStatistics
 Deep convolutionalnetworks as shallow Gaussian processes,2018, arXiv preprint arXiv:1808
 Google vizier: A service for black-box optimization,1487, In Proceedings of the 23rd ACMSIGKDD International Conference on Knowledge Discovery and Data Mining
 Qualitatively characterizing neural networkoptimization problems,2015, International Conference on Learning Representations
 Adam: A method for stochastic optimization,2015, 3rd InternationalConference for Learning Representations
 Learning multiple layers of features from tiny images,2009, Technical report
 Deep gaussian processes withconvolutional kernels,2018, arXiv preprint arXiv:1806
 Hierarchical gaussian process latent variable models,2007, InProceedings of the 24th international conference on Machine learning
 Continuous neural networks,2007, In Artificial Intelligence andStatistics
 Generalization and network design strategies,1989, In Connectionism in perspective
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Deep neural networks as gaussian processes,2018, In International Conference on LearningRepresentations
 On the effect of the activation function on the distribution ofhidden nodes in a deep network,2019, arXiv preprint arXiv:1901
 BAYESIAN LEARNING FOR NEURAL NETWORKS,1995, PhD thesis
 In search of the real inductive bias: Onthe role of implicit regularization in deep learning,2015, Proceeding of the international Conferenceon Learning Representations workshop track
 Feature visualization,2017, Distill
 Wavenet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 On the saddle point problemfor non-convex optimization,2014, arXiv preprint arXiv:1405
 Expo-nential expressivity in deep neural networks through transient chaos,2016, In Advances In NeuralInformation Processing Systems
 Learning internal representationsby error propagation,1985, Technical report
 Deep informationpropagation,2017, ICLR
 Mastering the game of gowithout human knowledge,2017, Nature
 Deep inside convolutional networks:Visualising image classification models and saliency maps,2014, ICLR Workshop
 Markovian architectural bias of recurrentneural networks,2004, IEEE Transactions on Neural Networks
 Variational learning of inducing variables in sparse gaussian processes,2009, In ArtificialIntelligence and Statistics
 Convolutional gaussian pro-cesses,2017, In Advances in Neural Information Processing Systems 30
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
 Computing with infinite networks,1997, In Advances in neural informationprocessing systems
 Stochastic variationaldeep kernel learning,2016, In Advances in Neural Information Processing Systems
 Training ultra-deep cnnswith critical initialization,2017, In NIPS Workshop
 Deep Mean Field Theory: Layerwise Variance and WidthVariation as Methods to Control Gradient Explosion,2018, ICLR Workshop
 Mean field residual networks: On the edge of chaos,2017, InAdvances in neural information processing systems
 A meanfield theory of batch normalization,2018, ICLR
 Visualizing and understanding convolutional networks,2014, InEuropean conference on computer vision
 GPs are regularized in the same fashion as in Â§G,1010,1
