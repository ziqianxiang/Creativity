title,year,conference
 Learning polynomials withneural networks,2014, In International Conference on Machine Learning
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Globally optimal gradient descent for a ConvNet with gaussianinputs,2017, In International Conference on Machine Learning
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, arXiv preprint arXiv:1805
 Stochastic subgradientmethod converges on tame functions,2018, arXiv preprint arXiv:1804
 Gradientdescent can take exponential time to escape saddle points,2017, In Advances in Neural InformationProcessing Systems
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, arXiv preprint arXiv:1806
 Gradient descentlearns one-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2018, Proceedings of the 35thInternational Conference on Machine Learning
 Escaping from saddle points - online stochasticgradient for tensor decomposition,2015, In Proceedings of The 28th Conference on Learning Theory
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, arXiv preprint arXiv:1806
 How to escape sad-dle points efficiently,2017, In Proceedings of the 34th International Conference on Machine Learning
 Deep learning without poor local minima,2016, In Advances In Neural InformationProcessing Systems
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
 Optimization landscape and expressivity of deep cnns,2018, InInternational Conference on Machine Learning
 Spurious local minima are common in two-layer ReLU neural net-works,2018, In International Conference on Machine Learning
 Learning ReLus via gradient descent,2017, In Advances in Neural InformationProcessing Systems
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 No bad local minima: Data independent training error guaranteesfor multilayer neural networks,2016, arXiv preprint arXiv:1605
 An analytical formula of population gradient for two-layered ReLU network and itsapplications in convergence and critical point analysis,2017, In International Conference on MachineLearning
 Neural networks with finite intrinsic dimensionhave no spurious valleys,2018, arXiv preprint arXiv:1802
 A Lyapunov analysis of momentum meth-ods in optimization,2016, arXiv preprint arXiv:1611
 Diverse neural network learns true target functions,2017, In ArtificialIntelligence and Statistics
 Efficiently testing local optimality and escaping saddlesfor relu networks,2018, arXiv preprint arXiv:1809
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Critical points of neural networks: Analytical forms and landscapeproperties,2017, arXiv preprint arXiv:1710
