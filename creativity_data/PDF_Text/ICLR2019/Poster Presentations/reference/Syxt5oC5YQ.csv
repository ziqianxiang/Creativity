title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Training deep nets with sublinearmemory cost,2016, arXiv preprint arXiv:1604
 Why momentum really works,2017, Distill
 Classical mechanics,2011, Pearson Education India
 The reversible residual network:Backpropagation without storing activations,2017, In Advances in Neural Information ProcessingSystems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Long short-term memory,1997, Neural computation
 Batch normalization: Accelerating deeP network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 On the insufficiency ofexisting momentum schemes for stochastic oPtimization,2018, arXiv preprint arXiv:1803
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Learning multiPle layers of features from tiny images,2009, 2009
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational linguistics
 Deep learning via hessian-free optimization,2010, In ICML
 New insights and perspectives on the natural gradient method,2014, arXiv preprintarXiv:1412
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Regularizing and optimizing lstmlanguage models,2017, arXiv preprint arXiv:1708
 Automatic differentiation inpytorch,2017, 2017
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Adine: an adaptivemomentum method for stochastic gradient descent,2018, In Proceedings of the ACM India JointInternational Conference on Data Science and Management of Data
 A differential equation for modeling nesterovsaccelerated gradient method: Theory and insights,2014, In Advances in Neural Information ProcessingSystems
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Backpropagation through time: what it does and how to do it,1990, Proceedings of theIEEE
 On accelerated methods in optimization,2015, arXiv preprintarXiv:1509
 A variational perspective on acceleratedmethods in optimization,2016, Proceedings of the National Academy of Sciences
 A lyapunov analysis of momentum methodsin optimization,2016, arXiv preprint arXiv:1611
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
