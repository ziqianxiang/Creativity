title,year,conference
 Layer normalization,2016, arXiv:1607
 Globally optimal gradient descent for a convnet with gaussian inputs,2017, InICML
 Robust Implicit Backpropagation,2018, In arXiv:1808
 Comparison of Batch Normalization and Weight Normalization Algorithmsfor the Large-scale Image Classification,2017, arXiv:1709
 Deep residual learning for image recognition,2016, InCVPR
 Densely connected convolutionalnetworks,2017, In CVPR
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In ICML
 Deep learning without poor local minima,2016, In NIPS
 Learning multiple layers of features from tiny images,2009, In Technical Report
 Imagenet classification with deep convolutional neuralnetworks,2012, In NIPS
 Sgdr: Stochastic gradient descent with warm restarts,2016, In arXiv:1608
 Eigennet: Towards fast and structural learning of deep neural networks,2017, IJCAI
 Learning deep architectures via generalized whitened neural networks,2017, ICML
 Differentiable learning-to-normalize viaswitchable normalization,2019, ICLR
 Stochastic Gradient Descent as Approximate BayesianInference,2017, arXiv:1704
 The landscape of empirical risk for non-convex losses,2016, InarXiv:1607
 On the importance of singledirections for generalization,2018, In ICLR
 Switchable whitening for deeprepresentation learning,2019, In arXiv:1904
 Geometry of neural network loss surfaces via random matrix theory,2017, InICML
 Adding noise to the input of a model trained witha regularized objective,2011, arXiv:1104
 Imagenet large scale visualrecognition challenge,2015, In ICJV
 Dynamics of on-line gradient descent learning for multilayer neural networks,1996, InNIPS
 Weight normalization: A simple reparameterization to accelerate trainingof deep neural networks,2016, In arXiv:1602
 Statistical mechanics of learning from examples,1992, PhysicalReview A
 Ssn: Learningsparse switchable normalization via sparsestmax,2019, In CVPR
 Rethinkingthe Inception Architecture for Computer Vision,1512, arXiv:1512
 Bayesian uncertainty estimation for batch normalized deepnetworks,2018, In ICML
 An analytical formula of population gradient for two-layered relu network and its applicationsin convergence and critical point analysis,2017, In ICML
 Instance normalization: The missing ingredient for faststylization,2016, arXiv:1607
 Dropout Training as Adaptive Regularization,1307, arXiv:1307
 Group normalization,2018, arXiv:1803
 Understanding deep learningrequires rethinking generalization,2017, In ICLR
 The most difficult process in Eqn,2019,19 is to solve the inner integration over h3
