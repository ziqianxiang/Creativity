title,year,conference
 A pid controllerapproach for stochastic optimization of deep networks,2018, In The IEEE Conference on ComputerVision and Pattern Recognition (CVPR)
 On-line learning rate adaptation with hypergradient descent,2018, In International Conference on LearningRepresentations
 Openai gym,2016, CoRR
 EMNIST: an extensionof MNIST to handwritten letters,2017, CoRR
 A robust accelerated optimizationalgorithm for strongly convex functions,2018, In 2018 Annual American Control Conference
 Language modeling withgated convolutional networks,2016, arXiv preprint arXiv:1612
 Incorporating nesterov momentum into adam,2016, ICLR Workshop
 Addressing function approximation error inactor-critic methods,2018, arXiv preprint arXiv:1802
 Convolutionalsequence to sequence learning,2017, arXiv preprint arXiv:1705
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proceedings of the 32nd International Conference on MachineLearning
 Acceleratingstochastic gradient descent,2017, CoRR
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems 26: 27th Annual Conferenceon Neural Information Processing Systems 2013
 On the insufficiency of ex-isting momentum schemes for stochastic optimization,2018, In International Conference on LearningRepresentations
 Adam: A method for stochastic optimization,2015, In Proceedingsof 3rd International Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Technical report
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems
 The mnist database of handwritten digits,1998, 1998
 Aggregated momentum: Stability throughpassive damping,2018, CoRR
 Pointer sentinel mixturemodels,2016, CoRR
 Scaling neural machinetranslation,2018, arXiv preprint arXiv:1806
 Automatic differentiation inpytorch,2017, 2017
 On second-best national saving and game-equilibriumgrowth,1968, The Review of Economic Studies
 The best things in life are model free,2018, argmin (personal blog)
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 An overview of gradient descent optimization algorithms,2016, arXiv preprintarXiv:1609
 Minimizing finite sums with the stochas-tic average gradient,2013, CoRR
 Fast stochastic variance reducedgradient method with momentum acceleration for machine learning,2017, CoRR
 Qanet: Combining local convolution with global self-attention for reading compre-hension,2018, arXiv preprint arXiv:1804
7 and PyTorch 0,2017,4
