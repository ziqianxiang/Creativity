title,year,conference
 Learning to learn by gradient descent by gradientdescent,2016, Neural Information Processing Systems
 A closer look at memorization in deep netWorks,2017, International Conference on MachineLearning
 Distributed second-order optimization using kronecker-factored approXimations,2017, International Conference on Learning Representations
 Trusting SVM for pieceWise linearCNNs,2017, International Conference on Learning Representations
 Smooth loss functions for deep top-kclassification,2018, International Conference on Learning Representations
 Practical gauss-neWton optimisation for deeplearning,2017, International Conference on Machine Learning
 A large annotatedcorpus for learning natural language inference,2015, Conference on Empirical Methods in NaturalLanguage Processing
 Convex optimization: Algorithms and complexity,2015, Foundations and Trends inMachine Learning
 Supervisedlearning of universal sentence representations from natural language inference data,2017, Conferenceon Empirical Methods in Natural Language Processing
 Natural neural netWorks,2015, NeuralInformation Processing Systems
 An algorithm for quadratic programming,1956, Naval ResearchLogistics Quarterly
 Proximal backpropaga-tion,2018, International Conference on Learning Representations
 Reliably learning the ReLU inpolynomial time,2017, Conference on Learning Theory
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, Conference on Computer Vision and Pattern Recognition
 Densely connectedconvolutional networks,2017, Conference on Computer Vision and Pattern Recognition
 Adam: A method for stochastic optimization,2015, InternationalConference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Technical Report
 Block-coordinate Frank-Wolfe optimization for structural SVMs,2013, International Conference on Machine Learning
 A proximal method for composite minimization,2016, MathematicalProgramming
 Optimizing neural networks with Kronecker-factored approximatecurvature,2015, International Conference on Machine Learning
 Training deep and recurrent networks with Hessian-free optimiza-tion,2012, Neural Networks: Tricks of the Trade
 Kronecker-factored curvature approximations forrecurrent neural networks,2018, International Conference on Learning Representations
 Partial linearization basedoptimization for multi-class SVM,2016, European Conference on Computer Vision
 Path-sgd: Path-normalized optimiza-tion in deep neural networks,2015, Neural Information Processing Systems
 Path-normalized opti-mization of recurrent neural networks with relu activations,2016, Neural Information ProcessingSystems
 Exploring generaliza-tion in deep learning,2017, Neural Information Processing Systems
 Automatic differentiation inpytorch,2017, NIPS Autodiff Workshop
 Optimization as a model for few-shot learning,2017, InternationalConference on Learning Representations
 Topmoumoute online natural gradientalgorithm,2008, Neural Information Processing Systems
 Learning representations by back-propagating errors,1986, Nature
 No more pesky learning rates,2013, International Conferenceon Machine Learning
 Very deep convolutional networks for large-scale imagerecognition,2015, International Conference on Learning Representations
 Max-margin Markov networks,2003, NeuralInformation Processing Systems
 Trainingneural networks without gradients: A scalable ADMM approach,2016, International Conference onMachine Learning
 Support vectormachine learning for interdependent and structured output spaces,2004, International Conference onMachine Learning
 Learned optimizers that scale andgeneralize,2017, International Conference on Machine Learning
 The marginalvalue of adaptive gradient methods in machine learning,2017, Neural Information Processing Systems
 ADADELTA: an adaptive learning rate method,2012, arXiv preprint
 Bpgrad: Towards global optimality in deeplearning via branch and pruning,2017, Conference on Computer Vision and Pattern Recognition
 The initializationstep is not informative about the optimization problem,2019, Therefore
