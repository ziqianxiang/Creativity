title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Understanding batch normal-ization,2018, In S
 Incorporatingsecond-order functional knowledge for better option pricing,2001, In T
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Yee Whye Teh and Mike Titterington (eds
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, In S
 Densely connectedconvolutional networks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Batch renormalization: Towards reducing minibatch dependence in batch-normalizedmodels,2017, In I
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Towards a theoretical understanding of batch normalization,2018, arXiv preprintarXiv:1805
 Data-dependent initializationsof convolutional neural networks,2015, arXiv preprint arXiv:1511
 On the convergence of stochastic gradient descent with adaptivestepsizes,2018, arXiv preprint arXiv:1805
 Rectifier nonlinearities improve neural net-work acoustic models,2013, In in ICML Workshop on Deep Learning for Audio
 All you need is a good init,2015, arXiv preprint arXiv:1511
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, In D
 How does batch nor-malization help optimization? In S,2018, Bengio
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Instance normalization: The missing in-gredient for fast stylization,2016, arXiv preprint arXiv:1607
 WNGrad: Learn the Learning Rate in Gradient De-scent,2018, arXiv preprint arXiv:1803
 Group normalization,2018, In The European Conference on Computer Vision(ECCV)
 On the convergence ofadaptive gradient methods for nonconvex optimization,2018, arXiv preprint arXiv:1808
 On the convergence of adagrad with momentum for training deep neuralnetworks,2018, arXiv preprint arXiv:1808
