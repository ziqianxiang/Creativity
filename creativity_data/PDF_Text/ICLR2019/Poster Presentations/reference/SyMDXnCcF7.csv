title,year,conference
 Layer Normalization,2016, arXiv:1607
 Understanding Batch Normalization,2018, June 2018
 Dynamical isometry and a mean fieldtheory of RNNs: Gating enables signal propagation in recurrent neural networks,2018, In Jennifer Dyand Andreas Krause (eds
 Deep convolutional net-works as shallow gaussian processes,2019, International Conference on Learning Representations
 Comparison of Batch Normalization and Weight NormalizationAlgorithms for the Large-scale Image Classification,2017, arXiv:1709
 Deep Residual Learning for ImageRecognition,2015, arXiv:1512
 Batch Normalization: Accelerating Deep Network Trainingby Reducing Internal Covariate Shift,2015, arXiv:1502
 Towards a Theoretical Understanding of Batch Normalization,2018, arXiv:1805
 Handwritten digit recognition with a back-propagation net-work,1990, In Advances in neural information processing systems
 Deep neural networks as gaussian processes,2017, arXiv preprint arXiv:1711
 Understanding Regularization in BatchNormalization,2018, arXiv:1809
 Bayesian deep convolutional networks withmany channels are gaussian processes,2019, International Conference of Learning Representations
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Advances in neural informationprocessing Systems
 The Nonlinearity Coefficient - Predicting Overfitting inDeep Neural Networks,2018, arXiv:1806
 Exponen-tial expressivity in deep neural networks through transient chaos,2016, arXiv:1606
 Weight Normalization: A Simple Reparameterization toAccelerate Training of Deep Neural Networks,2016, February 2016
 Deep InformationPropagation,2016, arXiv:1611
 Mastering the game of gowithout human knowledge,2017, Nature
 A bayesian perspective on generalization and stochastic gradientdescent,2018, 2018
 Computing with infinite networks,1997, In Advances in neural informationprocessing systems
 Meanfield Residual Network: On the Edge of Chaos,2017, InAdvances in neural information processing systems
 Deep mean field theory: Layerwise variance and widthvariation as methods to control gradient explosion,2018, 2018
 ThusTheorem G,2019,9
