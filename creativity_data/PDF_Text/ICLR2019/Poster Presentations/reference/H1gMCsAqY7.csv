title,year,conference
 Impatient dnns-deep neural networks withdynamic time budgets,2016, arXiv preprint arXiv:1610
 Layer normalization,2016, arXiv preprintarXiv:1607
 XcePtion: Deep learning with depthwise separable convolutions,2016, arXiv preprintarXiv:1610
 Imagenet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition
 A learned representation for artisticstyle,2016, arXiv preprint arXiv:1610
 Feature-wise transformations,2018, Distill
 Rich feature hierarchies for ac-curate object detection and semantic segmentation,2014, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Channel pruning for accelerating very deep neural net-works,1398, In Computer Vision (ICCV)
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Anytime neural networksvia joint optimization of auxiliary losses,2017, arXiv preprint arXiv:1708
 Arbitrary style transfer in real-time with adaptive instance nor-malization,2017, InICCV
 Ai benchmark: Running deep neural networks on android smart-phones,2018, arXiv preprint arXiv:1810
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Learning nested sparse structures in deep neuralnetworks,2017, arXiv preprint arXiv:1712
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Revisiting batch normaliza-tion for practical domain adaptation,2016, arXiv preprint arXiv:1603
 Demystifying neural style transfer,2017, arXivpreprint arXiv:1701
 Universal styletransfer via feature transforms,2017, In Advances in Neural Information Processing Systems
 Runtime neural pruning,2017, In Advances in NeuralInformation Processing Systems
 Dynamic deep neUral networks: Optimizing accUracy-efficiency trade-offsby selective execUtion,2017, arXiv preprint arXiv:1701
 Learn-ing efficient convolUtional networks throUgh network slimming,2017, In Computer Vision (ICCV)
 Thinet: A filter level prUning method for deep neUralnetwork compression,2017, arXiv preprint arXiv:1707
 PrUning convolUtionalneUral networks for resoUrce efficient inference,2016, arXiv preprint arXiv:1611
 LearningvisUal reasoning withoUt strong priors,2017, arXiv preprint arXiv:1707
 Film: VisUalreasoning with a general conditioning layer,2017, arXiv preprint arXiv:1709
 UnsUpervised representation learning with deepconvolUtional generative adversarial networks,2015, arXiv preprint arXiv:1511
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Mnasnet: Platform-aware neural architecture search for mobile,2018, arXiv preprint arXiv:1807
 Convolutional networks with adaptive computation graphs,2017, arXivpreprint arXiv:1711
 Skipnet: Learning dynamic routing inconvolutional networks,2017, arXiv preprint arXiv:1711
 Learning structured sparsity indeep neural networks,2016, In Advances in Neural Information Processing Systems
 Blockdrop: Dynamic inference paths in residual networks,2017, arXiv preprintarXiv:1711
 Aggregated residual trans-formations for deep neural networks,2017, In Computer Vision and Pattern Recognition (CVPR)
 Efficientvideo object segmentation via network modulation,2018, arXiv preprint arXiv:1802
 Rethinking the smaller-norm-less-informativeassumption in channel pruning of convolution layers,2018, arXiv preprint arXiv:1802
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2017, arXiv preprint arXiv:1707
 Towards effective low-bitwidth convolutional neural networks,2018, In other words
