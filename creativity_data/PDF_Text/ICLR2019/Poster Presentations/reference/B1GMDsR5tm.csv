title,year,conference
 Feedfor-ward initialization for fast inference of deep generative networks is biologically plausible,2016, arXivpreprint arXiv:1606
 Distributed optimization of deeply nested systems,2014, InArtificial Intelligence and Statistics
 Inference suboptimality in variational autoen-coders,2018, arXiv preprint arXiv:1801
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Neurons with graded response have collective computational properties like thoseof two-state neurons,1984, Proceedings of the national academy of sciences
 Decoupled neural interfaces using synthetic gradients,2016, arXivpreprint arXiv:1608
 Deep directed generative models with energy-based probabilityestimation,2016, arXiv preprint arXiv:1606
 Semi-amortizedvariational autoencoders,2018, arXiv preprint arXiv:1802
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Error forward-propagation: Reusingfeedforward connections to propagate errors in deep learning,2018, arXiv preprint arXiv:1808
 Approximate inference with amortised mcmc,2017, arXivpreprint arXiv:1702
 Iterative amortized inference,2018, arXiv preprintarXiv:1807
 Conditional generative adversarial nets,2014, arXiv preprintarXiv:1411
 Trainingneural networks without gradients: A scalable admm approach,2016, In International Conference onMachine Learning
 Generative adversarial networks asvariational training of energy based models,2016, arXiv preprint arXiv:1611
