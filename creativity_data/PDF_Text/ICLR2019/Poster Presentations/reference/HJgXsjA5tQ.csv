title,year,conference
 Mathematical analysis,1974, Addison Wesley
 Training a 3-node neural network is np-complete,1989, NIPS
 Sgd learns over-parameterizednetworks that provably generalize on linearly separable data,2018, ICLR
 The loss surfaces of multilayernetworks,2015, AISTATS
 Identifying and attackingthe saddle point problem in high-dimensional non-convex optimization,2014, NIPS
 Gradient descent learns one-hidden-layer cnn: Don'tbe afraid of spurious local minima,2018, ICML
 Globally optimal training of generalized polynomial neuralnetworks with nonlinear spectral methods,2016, NIPS
 Qualitatively characterizing neural network optimizationproblems,2015, ICLR
 Global optimality in neural network training,2017, CVPR
 Identity matters in deep learning,2017, ICLR
 Deep residual learning for image recognition,2016, CVPR
 Densely connected convolutional networks,2017, CVPR
 Beating the perils of non-convexity: Guaranteedtraining of neural networks using tensor methods,2016, arXiv:1506
 Deep learning without poor local minima,2016, NIPS
 Imagenet classification with deep convolutional neuralnetworks,2012, NIPS
 Visualizing the loss landscape of neural nets,2018, InICLR Workshop
 Understanding the loss surface of neural networks for binaryclassification,2018, In ICML
 On the computational efficiency of training neuralnetworks,2014, NIPS
 Depth creates no bad local minima,2017, arXiv:1702
 The zero set of a real analytic function,2015, arXiv:1512
 Exploring generalization in deeplearning,2017, NIPS
 The loss surface of deep and wide neural networks,2017, ICML
 Optimization landscape and expressivity of deep cnns,2018, ICML
 Learning deep models: Critical points and local openness,2018, ICLRWorkshop
 On the quality of the initial basin in overspecified networks,2016, ICML
 Provable methods for training neural networks with sparse connectiv-ity,2015, ICLR Workshop
 Failures of gradient-based deep learning,2017, ICML
 Training a single sigmoidal neuron is hard,2002, Neural Computation
 Learning relus via gradient descent,2017, NIPS
 Exponentially vanishing sub-optimal local minima in multilayer neuralnetworks,2017, ICLR Workshop 2018
 The implicit bias of gradient descent on separabledata,2018, ICLR
 An analytical formula of population gradient for two-layered relu network and its applicationsin convergence and critical point analysis,2017, ICML
 Spurious valleys in two-layer neural network optimizationlandscapes,2018, arXiv:1802
 On the local minima free condition of backpropagation learning,1995, IEEETransaction on Neural Networks
 Global optimality conditions for deep neural networks,2017, ICLR
 Wide residual networks,2016, BMCV
 Understanding deep learning requiresre-thinking generalization,2017, ICLR
 Deep neural networks with multi-branch architectures areless non-convex,2018, arXiv:1806
 Recovery guarantees for one-hidden-layerneural networks,2017, ICML
