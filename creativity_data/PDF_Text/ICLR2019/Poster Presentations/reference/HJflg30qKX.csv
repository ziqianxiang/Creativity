title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Spectrally-normalized margin bounds for neuralnetworks,2017, NIPS
 Convex optimization: Algorithms and complexity,2015, Foundations andTrendsÂ® in Machine Learning
 Algorithmic regUlarization in learning deep homogeneoUsmodels: Layers are aUtomatically balanced,2018, arXiv preprint arXiv:1806
 Implicit bias of gradient descenton linear convolUtional networks,2018, arXiv preprint arXiv:1806
 Risk and parameter convergence of logistic regression,2018, arXiv preprintarXiv:1803
 Imagenet classification with deep convolU-tional neUral networks,2012, In NIPS
 Deep linear neUral networks with arbitrary loss: All localminima are global,2017, arXiv preprint arXiv:1712
 Gradient descent convergesto minimizers,2016, arXiv preprint arXiv:1602
 Depth creates no bad local minima,2017, arXiv preprintarXiv:1702
 Exact solUtions to the nonlinear dynam-ics of learning in deep linear neUral networks,2013, arXiv preprint arXiv:1312
 The implicit bias of gradient descent on separabledata,2017, arXiv preprint arXiv:1710
 Understandingdeep learning reqUires rethinking generalization,2017, ICLR
 Critical points of linear neUral networks: Analytical forms and land-scape properties,2018, 2018
