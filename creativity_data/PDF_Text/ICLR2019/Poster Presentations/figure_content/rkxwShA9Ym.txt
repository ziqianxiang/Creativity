Figure 1: An Illustration of land cover data and label super-resolution. Our method takes an inputimage (x) with low-resolution labels (z) and outputs a set of super-resolved label predictions (y),utilizing the statistical descriptions between low-resolution and high-resolution labels (Appendix B)e.g., one low-resolution class designates areas of low-intensity development, with 20% to 49% ofimpervious surfaces (such as houses or roads).
Figure 2: Proposed statistical matching loss function for label super-resolution shown with exampleimages from our land cover labeling application. The model’s high-resolution predictions in eachlow-resolution block are summarized by a label counting layer and matched with the distributionsdictated by the low-resolution labels.
Figure 3: Our model is useful detecting land cover change over years, at the same geographicallocation, which cannot be achieved effectively by directly comparing satellite images. For a detaileddescription of how we detect land cover change, see Appendix D.
Figure 4: Land cover segmentation examples. The SR model, while never shown pixel-level data intraining, finds sharper edges of buildings than the high-res model and even identifies some featuresalong the shoreline that the high-res model misses. For more qualitative examples, see Appendix E.
Figure 5: The effect of adding high-resolution data in training of super-resolution models. We showthe baseline (HR only and SR only) and the results of HR+SR models with varying number of high-res label data seen in training, both overall and in developed classes (as in Table 1). All resultspresented are average of 5 experiments with different random samples of high-res data.
Figure 6: Our method is able to super-resolve the low-resolution probabilities of lymphocyte infil-tration into pixel-level lymphocyte segmentation. Lymphocytes are dark, rounded small cells. Ourmethod gives reasonable lymphocyte segmentation results (in green contours).
Figure 7: Left: the assignment to all pixels of the distribution over labels that corresponds to theNLCD patches containing them. Next: the inferred land cover map y for an location near Richmond,Virginia, at 1m resolution at three different stages of training. As the learning progresses (on theentire state of Maryland), the labels get refined. Interestingly, the model flips the interpretations ofthe land use multiple times until it converges to the approximately correct mix of labels. Our modelallows for this as the constraints on assignments for each input image are soft, as opposed to themodels that enforce the constraints in each iteration of learning, e.g., (Papandreou et al. (2015)).
Figure 8: Web application that lets you interactively query our best HR+SR model for any areain the United States. The application shows the NAIP input imagery which the model is run on,the corresponding low-resolution NLCD labels for that area, and our high-resolution model out-put. Users can click on the map which will run model inference on-the-fly and “paint” the re-sulting land cover predictions over the ESRI basemap. This tool can be found online at http://landcovermap.eastus.cloudapp.azure.com:4040/.
Figure 9: Segmentations of pedestrians from the Cityscapes dataset, small U-Net model. Rows topto bottom: input image, ground-truth segmentation, super-res model output, high-res model output.
Figure 10: Segmentations of pedestrians from the Cityscapes dataset, large U-Net model. Rows topto bottom: input image, ground-truth segmentation, quantized frequencies of “pedestrian” pixels in13 × 20 blocks (label data seen in super-res training), super-res model output, high-res model output.
Figure 11: Probabilistic index map with 8 indices (segments) learned using (Jojic & Caspi (2004))is shown on the left. The map is a prior for segmentation of individual images, each followinga different palette: a different image-specific Gaussian mixture with 8 components. On the right,We show inferred grouping of the 8 segments into two - foreground and background, based on theassumption that the left edge and the right edge are most likely background.
Figure 12: Unsupervised segmentation maps obtained using the probabilistic index map modelfrom Fig. 11 are shown in the first row for images in the second row. The third row shows fore-ground/background segmentation based on the segment grouping in Fig. 11, and the final row showsblock estimates of statistics on foreground pixel counts, which can then be used as coarse labels inFig. 10. This is just one example in which coarse labels can be created in an unsupervised manner.
