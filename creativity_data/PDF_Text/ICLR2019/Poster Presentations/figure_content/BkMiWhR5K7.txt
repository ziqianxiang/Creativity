Figure 1: The fraction of correctly estimated coordinates of sgn(VχL(χ, y)) required to successfullyexecute the single-step PGD (also known as FGSM) attack, with = 0.05. In the experiment, for eachk, the top k percent - chosen either by magnitude (top-k) or randomly (random-k) - of the signsof the coordinates are set correctly, and the rest are set to +1 or -1 at random. The adversariality rateis the portion of 1,000 random ImageNet images misclassified after one FGSM step. For example,estimating only 20% of coordinates correctly leads to misclassification for > 60% of images.
Figure 3: Cosine similarity of “tiled” image gra-dient with original image gradient versus thelength of the square tiles, averaged over 5,000randomly selected ImageNet images.
Figure 2: Cosine similarity between the gradi-ents at the current and previous steps along theoptimization trajectory of NES PGD attacks, av-eraged over 1000 random ImageNet images.
Figure 4: (left) Average number of queries per successful image as a function of the number oftotal successful images; at any desired success rate, our methods use significantly less queries persuccessful image than NES, and the trend suggests that this gap increases with the desired successrate. (center) The loss over time, averaged over all images; (right) The correlation of the latentvector with the true gradient g, which is precisely the gradient estimation objective we define.
Figure 5: Sparsity in standard, wavelet (db4 wavelets), and PCA-constructed bases for the gradientsof 5,000 randomly chosen example images in the ImageNet validation set. The y-axis shows themean fraction of `2 weight held by the largest k vectors over the set of 5,000 chosen images. Thex-axis varies k . The gradients are taken through a standardly trained Inception v3 network. None ofthe bases explored induce significant sparsity.
Figure 6: Average blurred gradient with kernel size or “tile length” 5. The original gradient can beseen in 6a, and the “tiled” or average blurred gradient can be seen in 6b18Published as a conference paper at ICLR 2019B.3	Time-dependent Priors at Higher Step SizesWe show in Figure 7 that the correlation between successive gradients on the NES trajectory aresignficantly correlated, even at much higher step sizes (up to `2 norm of 4.0, which is a typical valuefor ε, the total adversarial perturbation bound and thus an absolute bound on step size). This servesas further motivation for the time-dependent prior.
Figure 7:	Figure 2 repeated for several step sizes, showing that the successive correlation betweengradients continues even at higher step sizes.
Figure 8:	Average loss and cosine distance versus number of queries used over the approaches’optimization trajectories in the two threat models. We average each cosine distance and loss point ateach query number over 100 images from the evaluation set.
Figure 9:	Cumulative distribution functions for the number of queries required to create an adversarialexample in the '2 and '∞ settingsfor the NES, bandits with time prior (BanditsT), and bandits withtime and data-dependent priors (BanditsT D) approaches. Note that the CDFs do not converge to one,as the approaches sometimes cannot find an adversarial example in less than 10,000 queries.
Figure 10:	The average number of queries used per successful image for each method when reachinga specified success rate: we compare NES Ilyas et al. (2017), BanditsT (our method with timeprior only), and BanditsT D (our method with both data and time priors) and find that our methodsstrictly dominate NES—that is, for any desired sucess rate, our methods take strictly less queries persuccessful image than NES.
