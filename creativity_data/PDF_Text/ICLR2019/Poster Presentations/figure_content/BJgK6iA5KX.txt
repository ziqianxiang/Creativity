Figure 1: AutoLoss reaches good convergence regard-less of λ for both regression and MLP classification.
Figure 2: AUTOLOSS vs. 4 best performed baselines in terms of training progress (IS vs. epochs). Each curvecorresponds to 3 runs of experiments and the variances are illustrated as vertical bar.
Figure 3: (L) Validaton PPL w.r.t. training epochs on the NMT task; (M) Visualiza-tion of the trained controller’s policy on the NMT task; (R) AutoLoss vs. DGS interms of MSE w.r.t. scanned batches on the regression task.
Figure 4: (a) Transfer a trained controller for MLP clas-sification to different data distributions. (b) ComParingthe final convergence (IS) on training randomly sam-Pled DCGAN architectures w/ and w/o AutoLoss. (c)ComParing the training Progress (IS vs. ePochs) ofGAN 1:K, GAN K:1 and an AutoLoss-guided GAN onCIFAR-10 with the controller trained on MNIST.
Figure 5: Images generated by a DCGAN trained under AutoLoss’ controller on MNIST. The controller istrained along with the DCGAN training.
Figure 6: Images generated by an AutoLoss-guided DCGAN. The controller is trained on MNIST dataset andapplied to guide the training of GANs on CIFAR-10.
