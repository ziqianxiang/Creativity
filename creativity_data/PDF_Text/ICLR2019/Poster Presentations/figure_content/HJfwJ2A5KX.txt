Figure 1: Evaluation of drop in classification accuracy after compression against the MNIST, CIFAR, andFashionMNIST datasets With varying number of hidden layers (L) and number of neurons per hidden layer(η*). Shaded region corresponds to values within one standard deviation of the mean.
Figure 2: Evaluation of relative error after compression against the MNIST, CIFAR, and FashionMNIST datasetsWith varying number of hidden layers (L) and number of neurons per hidden layer (η*).
Figure 3:	Evaluations against the MNIST dataset with varying number of hidden layers (L) and number ofneurons per hidden layer (η*). Shaded region corresponds to values within one standard deviation of the mean.
Figure 4:	Evaluations against the CIFAR-10 dataset with varying number of hidden layers (L) and number ofneurons per hidden layer (η*). The trend of our algorithm's improved relative performance as the number ofparameters increases (previously depicted in Fig. 3) also holds for the CIFAR-10 data set.
Figure 5:	Evaluations against the FashionMNIST dataset with varying number of hidden layers (L) and numberof neurons per hidden layer (η*).
Figure 6:	Evaluations against the MNIST dataset with varying number of hidden layers (L) and number ofneurons per hidden layer (η*). Shaded region corresponds to values within one standard deviation of the mean.
Figure 7:	Evaluations against the CIFAR-10 dataset with varying number of hidden layers (L) and number ofneurons per hidden layer (η*). The trend of our algorithm's improved relative performance as the number ofparameters increases (previously depicted in Fig. 6) also holds for the CIFAR-10 data set.
Figure 8:	Evaluations against the FashionMNIST dataset with varying number of hidden layers (L) and numberof neurons per hidden layer (η*).
