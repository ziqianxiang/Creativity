Figure 1: Actionable representa-tions: 3 houses A, B, C can onlybe reached by indicated roads. Theactions taken to reach A, B, C areshown by arrows. Although A, Bare very close in space, they arefunctionally different. The car hasto take a completely different roadto reach A, compared to B and C.
Figure 2: An illustration of actionable representations. For a pair of states s1 , s2, the divergencebetween the goal-conditioned action distributions they induce defines the actionable distance DAct,which in turn is used to learn representation φ.
Figure 3: Hierarchical RL with ARC.
Figure 4: The tasks in our evaluation. The 2D navigation tasks allow for easy visualization andanalysis, while the more complex tasks allow us to investigate how well ARC and prior methods candiscern the most functionally-relevant features of the state.
Figure 5: Visualization of ARC for 2D navigation. The states in the environment are colored tohelp visualize their position in representation space. For the wall task, points on opposite sides ofthe wall are clearly separated in ARC space (c). For four rooms, we see that ARCs provide a cleardecomposition into room clusters (f), while VAEs do not (e).
Figure 6: Perturbation analysis (Section 6.4): Effective representations vary significantly with per-turbations to functionally relevant elements of state (shown in orange), and less for secondary ele-ments (shown in purple). ARC exhibits this property, with a spread orange region - robot CoM orobject position, and a suppressed purple region - joint angles and other secondary elements. TheVAE and naive state representations do not capture this saliency, containing spread purple regions.
Figure 7: Learning neW tasks With reWard shaping in representation space. ARC representations aremore effective than other methods, and match the performance of a hand-specified shaping.
Figure 9: Waypointand multi-room HRLWe evaluate the two schemes for hierarchical reasoning with ARCs detailed tasksin Section 4.3: commanding directly in representation space or through a k-means clustering of therepresentation space. We train a high-level controller πh with TRPO which outputs as actions eithera direct point in the latent space zh or a cluster index ch, from which a goal gh is decoded andpassed to the goal-conditioned policy to follow for 50 timesteps. Exact specifications and details arein Appendix A.5 and A.6.
Figure 10: Comparison on hierarchical tasks. ARCs perform significantly better than other repre-sentation methods, option-critic, and commanding goals in state spaceUsing a hierarchical meta-policy with ARCs performs significantly better than those using alterna-tive representations which do not properly capture abstraction and environment dynamics (Fig 10).
