Figure 1: We define novelty through reach-ability. The nodes in the graph are observa-tions, the edges — possible transitions. Theblue nodes are already in memory, the greennodes are reachable from the memory withink = 2 steps (not novel), the orange nodesare further away — take more than k stepsto reach (novel). In practice, the full possibletransition graph is not available, so We traina neural network approximator to predict ifthe distance in steps between observations islarger or smaller than k.
Figure 2: Left: siamese architecture of reachability (R) network. Right: R-network is trained basedon a sequence of observations that the agent encounters while acting. The temporally close (withinthreshold) pairs of observations are positive examples, while temporally far ones — negatives.
Figure 3: The use of episodic curiosity (EC) module for reward bonus computation. The moduletake a current observation as input and computes a reward bonus which is higher for novel observa-tions. This bonus is later summed up with the task reward and used for training an RL agent.
Figure 4: Examples of tasks considered in our experiments: (a) VizDoom static maze goal reaching,(b) DMLab randomized maze goal reaching, (c) DMLab key-door puzzle, (d) MuJoCo ant locomo-tion out of first-person-view curiosity.
Figure 5: Examples of maze types used in our experiments: (a) VizDoom static maze goal reaching,(b) DMLab randomized maze goal reaching, (c) DMLab randomized maze goal reaching with doors.
Figure 6: Task reward as a function of training step for VizDoom tasks. Higher is better. We use theoffline version of our algorithm and shift the curves for our method by the number of environmentsteps used to train R-network — so the comparison is fair. We run every method with a repeat of 3(same as in prior work (Pathak et al., 2017)) and show all runs. No seed tuning is performed.
Figure 7: Reward as a function of training step for DMLab tasks. Higher is better. “ECO” standsfor the online version of our method, which trains R-network and the policy at the same time. Werun every method 30 times and show 5 randomly selected runs. No seed tuning is performed.
Figure S1:	Examples of randomized environments: (a) Image Action, (b) Noise.
Figure S2:	Reward as a function of training stepfor the DMLab task “Dense 2”. Higher is better.
