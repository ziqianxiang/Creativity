Figure 1: Distributed weight and gradient quantization with data parallelism.
Figure 2: F (c) vs clipping factor c.
Figure 3: Convergence of the weight-quantized models with different d’s.
Figure 4: Convergence with different numbers of bits for the weights on CIFAR-10. The gradient isfull-precision (denoted G(FP)) or m-bit quantized (denoted G(SQm)) without gradient clipping.
Figure 5: Convergence with different numbers of bits for the gradients on CIFAR-10. The weight isbinarized (denoted W(LAB)) or m-bit quantized (denoted W(LAQm)). Gradients are not clipped.
Figure 6: Results for LAQ2 with SQ2 on CIFAR-10 with two workers. (a) Histograms of gradientsat different Cifarnet layers before clipping (visualized by Tensorboard); (b) Average ∣∣gtk2/kgt∣2(for non-clipped gradients) and ∣∣gt∣2/∣gt∣2 (for clipped gradients); and (C) Training curves.
Figure 8: Speedup of ImageNet training on a 16-node GPU cluster. Each node has 4 1080ti GPUswith one PCI switch.
