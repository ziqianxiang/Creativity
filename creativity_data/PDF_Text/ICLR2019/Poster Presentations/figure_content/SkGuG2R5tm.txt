Figure 1: Our method learns a network that encodes the input space Rd into a code c(x). It islearned end-to-end, yet the part of the network in charge of the discretization operation is fixed inadvance, thereby avoiding optimization problems. The learnable function f, namely the “catalyzer”,is optimized to increase the quality of the subsequent coding stage.
Figure 2: Illustration of our method, which takes as input a set of samples from an unknowndistribution. We learn a neural network that aims at preserving the neighborhood structure in the inputspace while best covering the output space (uniformly). This trade-off is controlled by a parameterλ. The case λ = 0 keeps the locality of the neighbors but does not cover the output space. On theopposite, when the loss degenerates to the differential entropic regularizer (λ → ∞), the neighborsare not maintained by the mapping. Intermediate values offer different trade-offs between neighborfidelity and uniformity, which is proper input for an efficient lattice quantizer (depicted here by thehexagonal lattice A2).
Figure 3: Histograms of the distance between a query point and its 1st (resp. 100th) nearest neighbors,in the original space (left) and after our catalyzer (right). In the original space, the two histogramshave a significant overlap, which means that a 100-th nearest neighbor for a query has often a distancelower that the 1st neighbor for another query. This gap is significantly reduced by our catalyzer.
Figure 4: Impact of the regularizer on the output distri-bution. Each column corresponds to a different amountof regularization (left: λ = 0, middle: λ = 0.02, right:λ = 1). Each line corresponds to a different randomprojection of the empirical distribution, parametrized byan angle in [0, 2π]. The marginal distributions for thesetwo views are much more uniform with our KoLeo regu-larizer, which is a consequence of the higher uniformityin the high-dimensional latent space.
Figure 5: Comparison of the performance of the product lattice vs OPQ on Deep1M (left) andBigAnn1M (right). Our method maps the input vectors to a dout -dimensional space, that is thenquantized with a lattice of radius r. We obtain the curves by varying the radius r.
Figure 6: Number of atoms of the hyper-sphere of %.(linear scale), and the corresponding numberof points on the hyper-sphere (log scale).
Figure 7: Agreement between nearest neigh-bor and range search: average number of re-sults per query for given values of ε (indicatedon the curve), and corresponding recall values.
