Figure 1: Policy architecture.
Figure 2: MultiRoomNX SY and FindObjSY MiniGrid environments. See text for details.
Figure 3:	Policy generalization on MultiRoomNXSY . Success is measured by the percent of timethe agent can find the goal in an unseen maze. Error bars are standard deviations across runs. Baselineis a vanilla goal-conditioned A2C agent.
Figure 4:	Transferable exploration strategies on MultiRoomNX SY . As the number of roomsincreases (from left to right), a count-based exploration bonus alone cannot solve the task, whereasthe proposed exploration bonus, by being tuned to task structure, enables success on these moredifficult tasks.
Figure 5:	Goal based MiniPacMan navigation task: We train on a 6 × 6 environment, and evaluatethe generalization performance in a 11 × 11 maze. The agent is represented by white color and has toreach the goal (light green marker).
Figure 6: Goal based MiniPacMan navigation task: Here the agent gets a full observation ofenvironment. We follow the similar setup as in Imagination Augmented agents. In this, the output ofthe imagination core is treated as a contextual information by the policy. We treat this contextualinformation as the “goal” in the InfoBot setup. Here, we want to see, where the policy wants to accessthe information provided by running the imagination module. Ideally, only at the decision states (i.epotential sub-goals) policy should access the output of the imagination module. We show the outputof DKL [penc(Z | st, gt) | q(Z | st)], where gt refers to the output of imagination module. High KLis represented by lighter color.
Figure 7: Transferable exploration strategies on Humanoid, Walker2D, and Hopper. The ”base-line” is PPO (Schulman et al., 2017). Experiments are run with 5 random seeds and averaged overthe runs.
Figure 8:	Transferable exploration strategies on Pong, Qbert, Seaquest, and Breakout. Thebaseline is a vanilla A2C agent. Results averaged over three random seeds.
Figure 9:	Transfer across ALE Games (Pong, Qbert and Freeway) using egocentric encoder toprovide exploration bonus, trained from Seaquest. Comparison of InfoBot (A2C + KL Regularizer)with a Baseline A2C. Experiment results averaged over four random seeds. See Section B.
Figure 10: Visitation Count: Effect on visitation count as a result of giving KL as an explorationbonus. As top figure shows, that after giving KL as an exploration bouns, agent visits more diversestates. Here, the agent is trained on a smaller 6 x 6 maze, and evaluated on more complex 11 * 11maze. The "blueness" quanitifies the states visited by the agent.
Figure 11: InfoBot Comparison with state of the art off poliCy algorithm (SAC) on sparse rewardmujoCo envs.
Figure 12: Visualizations of randomly generated worldsH	MiniGrid Environments for OpenAI GymThe FindObj and MultiRoom environments used for this research are part of MiniGrid, which is anopen source gridworld package. This package includes a family reinforcement learning environmentscompatible with the OpenAI Gym framework. Many of these environments are parameterizable sothat the difficulty of tasks can be adjusted (eg: the size of rooms is often adjustable).
