Figure 1: Mars Rover domain and policy illustration. As α decreases, the agent is requiredto learn a safer policy.
Figure 2: RCPO vs Lagrange comparison. The reward is (-) the average number of stepsit takes to reach the goal. Results are considered valid if and only if they are at or belowthe threshold.
Figure 3: Mujoco with torque constraints. The dashed line represents the maximal allowedvalue. Results are considered valid only if they are at or below the threshold. RCPO is ourapproach, whereas each λ value is a PPO simulation with a fixed penalty coefficient. Y axisis the average reward and the X axis represents the number of samples (steps).
