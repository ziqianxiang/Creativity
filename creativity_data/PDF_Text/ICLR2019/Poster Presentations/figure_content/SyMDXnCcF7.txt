Figure 1: Numerical confirmation of theoretical predictions. (a,b) Comparison between theo-retical prediction (dashed lines) and Monte Carlo simulations (solid lines) for the eigenvalues ofthe backwards Jacobian (see Thms 3.10 and G.12) as a function of batch size and the magnitudeof gradients as a function of depth respectively for rectified linear networks. In each case MonteCarlo simulations are averaged over 200 sample networks of width 1000 and shaded regions denote1 standard deviation. Dashed lines are shifted slightly for easier comparison. (c,d) Demonstrationof the existence of a BSB1 to BSB2 symmetry breaking transition as a function of a for α-ReLU(i.e. the ath power of ReLU) activations. In (c) we plot the empirical variance of the diagonal andoff-diagonal entries of the covariance matrix which clearly shows ajump at the transition. In (d) weplot representative covariance matrices for the two phases (BSB1 bottom, BSB2 top).
Figure 2: Batch norm leads to a chaotic input-output map With increasing depth. A linearnetwork with batch norm is shown acting on two minibatches of size 64 after random orthogonalinitialization. The datapoints in the minibatch are chosen to form a 2d circle in input space, exceptfor one datapoint that is perturbed separately in each minibatch (leftmost datapoint at input layer 0).
Figure 3: Batch normalization strongly limits the maximum trainable depth. Colors show testaccuracy for rectified linear networks with batch normalization and Y = 1, β = 0, e = 10-3,N = 384, and η = 10-5B. (a) trained on MNIST for 10 epochs (b) trained with fixed batchsize 1000 and batch statistics computed over sub batches of size B. (c) trained using RMSProp.
Figure 4: Gradients in networks with batch normalization quickly achieve dynamical equilib-rium. Plots of the relative magnitudes of (a) the weights (b) the gradients of the loss with respectto the pre-activations and (c) the gradients of the loss with respect to the weights for rectified linearnetworks of varying depths during the first 10 steps of training. Colors show step number from 0(black) to 10 (green).
Figure 5: Three techniques for counteracting gradient explosion. Test accuracy on MNIST as afunction of different hyperparameters along with theoretical predictions (white dashed line) for themaximum trainable depth. (a) tanh network changing the overall scale of the pre-aCtivations, hereY → 0 corresponds to the linear regime. (b) Rectified linear network changing the mean of the pre-activations, here β → ∞ corresponds to the linear regime. (c,d) tanh and rectified linear networksrespectively as a function of e, here We observe a well defined phase transition near e 〜1. Note thatin the case of rectified linear activations we use β = 2 so that the function is locally linear about 0.
Figure 6:	Batch norm leads to a chaotic input-output map with increasing depth. A linearnetwork with batch norm is shown acting on two minibatches of size 64 after random orthogonalinitialization. The datapoints in the minibatch are chosen to form a 2d circle in input space, exceptfor one datapoint that is perturbed separately in each minibatch (leftmost datapoint at input layer 0).
Figure 7:	relative gradient norms of different parameters in layer order (input to output from left toright), with γ and W interleaving. From dark to light blue, each curve is separated by (a) 3, (b) 5,or (c) 10 epochs. We see that after 10 epochs, the relative gradient norms of both γ and W for alllayers become approximately equal despite gradient explosion initially.
Figure 8: We sweep over different values of learning rate, β initialization, and , in training VGG19with batchnorm on CIFAR100 with data augmentation. We use 8 random seeds for each combina-tion, and assign to each combination the median training/validation accuracy over all runs. We thenaggregate these scores here. In the first row we look at training accuracy with different learning ratevs β initialization at different epochs of training, presenting the max over . In the second row wedo the same for validation accuracy. In the third row, we look at the matrix of training accuracy forlearning rate vs , taking max over β. In the fourth row, we do the same for validation accuracy. BB Gradient Independence AssumptionFollowing prior literature Schoenholz et al. (2016); Yang & Schoenholz (2017); Xiao et al. (2018),in this paper, in regards to computations involving backprop, we assumeAssumption 2. During backpropagation, whenever we multiply by WT for some weight matrix W,we multiply by an iid copy instead.
Figure 9: In the same setting as Fig. 8, except we don’t take the max over the unseen hyperparameterbut rather set it to 0 (the default value).
Figure 10: Plots of wB-1,l - vB-1,l over B for various lEigenvalue for M We use a similar technique shows the Gegenbauer expansion of the eigenvaluefor M.
