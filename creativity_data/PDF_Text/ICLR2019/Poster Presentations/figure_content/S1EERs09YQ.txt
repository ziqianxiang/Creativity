Figure 1: We discover the most activated sentences and aligned concepts to the units in hidden rep-resentations of deep convolutional networks. Aligned concepts appear frequently in most activatedsentences, implying that those units respond selectively to specific natural language concepts.
Figure 2: Mean and variance of selectivity values over all units in the learned representation for eachdataset. Sentences including the concepts that our alignment method discovers always activate unitssignificantly more than random sentences. See section 4.2 for details.
Figure 3: Examples of top activated sentences and aligned concepts to some units in several encodinglayers of ByteNet and VDCNN. For each unit, concepts and their presence in top K sentences areshown in the same color. [#] symbol denotes morpheme concepts. See section 4.3 for details.
Figure 4: 30 concepts selected by the number of aligned units in three encoding layers of ByteNetlearned on the Europarl translation dataset (left column) and VDCNN learned on AG-News (rightcolumn). [#] symbol denotes morpheme concepts. See section 4.4 for details.
Figure 5: Aligned concepts are divided into six different levels of granularity: morphemes, wordsand N-gram phrases (N = 2, 3, 4, 5) and shown layerwise across multiple datasets and tasks. Thenumber of units increases with layers in the classification models (i.e. [64, 128, 256, 512]), but intranslation the number is constant (i.e. 1024) across all layers.
Figure 6: BLEU scores onthe validation data for threetranslation models. We trainByteNet from scratch on eachdataset by varying the numberof encoder layers.
Figure 7: Correlations between the number of units per conceptand (a) document frequency and (b) delta of expected loss. Pear-son correlation coefficients are measured at the final layer of theAG News sentence classification task. Some concepts alignedto many units are annotated. Results on other tasks are availablein Appendix F.
Figure 8: Mean and variance of selectivity values for different M = [1, 3, 5, 10], where M is thenumber of selected concepts per unit. In all settings, the selectivity of replicate ones is the highest,that of one instance ones is runner-up, and that of random is the lowest near 0.
Figure 9: Ratio of interpretable units in layer-wise. Across all datasets, there are more than 90% ofunits are interpretable. See Section D for more details.
Figure 10: Examples of non-interpretable units, their concepts and top 5 activated sentences. Unitsare from representations learned on english-to-french translation dataset. See section D for details.
Figure 11: Concept clusters of the last layer representations learned in each task. The more distinctthe diagonal blocks are, the stronger the tendency that concepts aligned to the same unit share similarmeanings or semantics. See Appendix E for details.
Figure 12: Averaged pairwise distances of concepts in each layer per task. Top and bottom rowshow the concept distances of translation models and classification models, respectively. For pro-jecting concepts into the embedding space, we use three pretrained embedding: Glove, ConceptNet,FastText. See Appendix E.2 for more details.
Figure 13:	The number of unique concepts in each layer. It increases with the layer depth, whichimplies that the units in a deeper layer represent more diverse concepts.
Figure 14:	The Pearson correlations between the number of units per concept and (i) documentfrequency (top row), (ii) delta of expected loss (bottom row). They are measured at the final layerrepresentation.
Figure 15: 30 concepts selected by the number of aligned units in four encoding layers in VD-CNN learned on Yelp Review dataset and DBpedia ontology dataset, and ByteNet learned on theEngIiSh-to-German, English-to-French, and English-to-Czech parallel corpus. [#] symbol denotesmorpheme concept.
