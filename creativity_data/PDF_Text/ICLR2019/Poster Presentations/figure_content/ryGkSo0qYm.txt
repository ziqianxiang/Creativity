Figure 1: Time comparison of different ways to compute a graph. Left: Graph between 10,000 mostfrequent English words using a word2vec representation. Right: Graph between 1,000,000 nodesfrom 68 features (US Census 1990). Scalable algorithms benefit from a small average node degree k.
Figure 2: Theoretical bounds of θ for a given sparsity level on 1000 images from MNIST. Left:Solving (8) for only one column of Z. Theorem 3 applies and for each k gives the bounds of θ (blue).
Figure 5: Connectivity across classes of MNIST. Left: A-NN graph. Middle: `2 model (4) neglectsdigits with larger distance. Right: log model (5) does not neglect to connect any cluster.
Figure 3: Approximation error betweenour large scale log model and the exactlog model by Kalofolias (2016).
Figure 4: Effectiveness of θ boundseq. (17). Requested versus obtained de-gree, "spherical" data (262, 000 nodes).
Figure 6: Left: Edge accuracy of large scale models for MNIST. Right: Digit classification errorwith 1% labels. Dashed lines represent nodes in components without known labels (non-classifiable).
Figure 7: Detail from the manifolds recovered by `2 and log models from "spherical data" (262, 144nodes, 1920 signals). Corner (blue) and middle (green) parts of the manifold. Left: `2 model,k = 4.70. Right: log model, k = 4.73. See Figure 15 for the big picture.
Figure 8: Graph diameter measures manifold recovery quality. Left: small spherical data: 4096nodes, 1920 signals. Middle: Same data, 40 signals. Right: word2vec: 10,000 nodes, 300 features.
Figure 9: A 2-hop sub-graph of the word ”use”. Left: A-NN (k = 5.4). Center: k-NN graph(k = 5.0). Right: Large scale log (k = 5.7) being manifold-like only reaches relevant terms.
Figure 10: Label frequency (left) and average squared distribution (right) of MNIST train data(60000 nodes). The distances between digits “1” are significantly smaller than distances betweenother digits.
Figure 11: Robustness of the theoretical bounds of θ in the existence of outliers or duplicate nodes.
Figure 13: Connectivity across different classes of MNIST (60000 nodes). The graph is normalizedso that kW k1,1 = 1. We measure the percentage of the total weight for connected pairs of each label.
Figure 14: Time needed for learning a graph of 60000 nodes (MNIST images) using the large-scaleversion of (3). Our algorithm converged after 250 to 450 iterations with a tolerance of 1e - 4. Thetime needed is linear to the number of variables, that is linear to the average degree of the graph.
Figure 15: Spherical data, ground truth and recovered manifolds. Up left: The ground truth manifoldis on the sphere. We have colored the nodes that correspond to the middle of the 2-D grid and thelower corner so that we track where they are mapped in the recovered manifolds. In Figure 7 we keeponly the subgraphs of the green or blue nodes. Up, right: Recovered by A-NN, k = 4.31. Down,left: Recovered by the `2 model, k = 4.70. The middle region is mixed with nodes outside thevery center. The corners are much more dense, the blue region is barely visible on the bottom. Notethat 46 nodes were disconnected so they are not mapped at all. Down, right: Recovered by the logmodel, k = 4.73. The middle region is much better mapped. The corners are still very dense, wehave to zoom-in for the blue region (Figure 7).
