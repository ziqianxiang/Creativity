Figure 1: Comprehensive motivation illustration. (a) Using larger mini-batch size helps improvethroughput until it is compute-bound; (b) Limited memory capacity on a single computing nodeprohibits the use of large mini-batch size; (c) Neuronal activation dominates the representational costwhen mini-batch size becomes large; (d) BN is indispensable for maintaining accuracy; (e) Upperand lower one are the feature maps before and after BN, respectively. However, using BN damagesthe sparsity through information fusion; (f) There exists such great representational redundancy thatmore than 80% of activations are close to zero.
Figure 2: (a) Illustration of dynamic and sparse graph (DSG); (b) Dimension-reduction search forconstruction of DSG; (c) Double-mask selection for BN compatibility. ‘DRS’ denotes dimension-reduction search.
Figure 3: Compressive and accelerative DSG. (a) Original dense convolution; (b) Converted accel-erative VMM operation; (c) Zero-value compression.
Figure 4: Structured selection via dynamic dimension-reduction search for producing sparse patternof neuronal activations.
Figure 5: Comprehensive analysis on sparsity v.s. accuracy. (a) & (b) Accuracy using DSG; (c)Influence of the graph selection strategy; (d) Influence of the dimension-reduction degree; (e) In-fluence of the double-mask selection for BN compatibility; (f) Influence of the network depth andwidth. ‘DRS’ denotes dimension-reduction search.
Figure 6: Memory footprint comparisons for (a) training and (b) inference.
Figure 7: Computational complexity comparisons for (a) training and (b) inference. ‘DRS’ denotesdimension-reduction search.
Figure 8: On VGG8: (a) Layer-wise execution time comparison; (b) Validation accuracy v.s. trainingtime of different models: large-sparse ones and smaller-dense ones with equivalent MACs.
Figure 9:	Selection mask generation: using a top-k search on the first input sample X(1) withineach mini-batch to obtain a top-k threshold which is shared by the following samples. Then, weapply thresholding on the whole output activation tensor to generate the importance mask for thesame mini-batch.
Figure 10:	Accuracy convergence. (a) Training curve with validation accuracy of VGG8 on CI-FAR10; (b) Training curve with top-5 validation accuracy of ResNet-18 on ImageNet; (c) Dis-tribution of pairwise difference between the original high-dimensional inner product and the low-dimensional one for the CONV5 layer in VGG8.
Figure 11: Selection mask convergence. (a) Average L1-norm value of the difference mask tensorsbetween adjacent training epochs across all samples in one mini-batch; (b) Average L1-norm valueof the difference mask tensors between adjacent samples after training.
Figure 12: Comparison with smaller-dense models with equivalent MACs using ResNet8 on CI-FAR 10 and AlexNet on ImageNet.
