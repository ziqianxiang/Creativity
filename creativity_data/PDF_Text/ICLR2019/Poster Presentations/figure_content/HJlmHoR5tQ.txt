Figure 1: Transfer learning problems. Fig. (a) represents a problem where agent dynamics aremodified during testing, i.e., a reward learned on a quadruped-ant (left) is transferred to a crippled-ant (right). Fig (b) represents a problem where environment structure is modified during testing, i.e.,a reward learned on a maze with left-passage is transferred to a maze with right-passage to the goal(green).
Figure 2: The performance of policies obtained from maximizing the learned rewards in the transferlearning problems. It can be seen that our method performs significantly better than AIRL (Fu et al.,2017) and exhibits expert-like performance in all five randomly-seeded trials which imply that ourmethod learns near-optimal, transferable reward functions.
Figure 3: Benchmark control tasks for imitation learningAIRL(s) and AIRL(s, a) in matching an expertâ€™s performance, thus showing no downside to theEAIRL approach.
Figure 5: The top and bottom rows show the path followed by a 2D point-mass agent (yellow) toreach the target (green) in training and testing environment, respectively.
