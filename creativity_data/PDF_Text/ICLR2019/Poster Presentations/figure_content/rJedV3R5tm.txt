Figure 1: The self-attention mechanism for updating the memory from Mt to Mt+ι by incorporating newobservation xt, where each row of the memory matrix Mt is a memory slot, and Q(h), Kthh and Vt(h) denotethe queries, keys and values, respectively. Note that the softmax function is performed on each row, andZ denotes the dot product. The concatenation (denoted by “concat”) of Mt and Xt is row-wise where theembedded input is first passed through a linear layer to make Xt match the row dimension of Mt.
Figure 2: The proposed discriminator framework with multiple embedded representations. The input is eitherthe real sentence [ri : … ：rτ] where rt denotes the t-th one-hot token, or the generated (approximate)sentence [^ι :… :yτ] where yt is from (6). Also, S embedding matrices {We(s) }S=i map each input into Sembedded representations, each of which is passed through discriminator independently to get the related loss.
Figure 3: The training curves of NLLgen scores (left) and NLLoracle scores (right) on synthetic data of length20 with different values of maximum inverse temperature βmax ∈ {1, 2, 5, 10, 100}. The vertical dash linerepresents the end of pre-training. With the increase of βmax, NLLgen becomes lower but NLLoracle becomeshigher. For both the NLLgen and NLLoracle scores, the lower the better.
Figure 4: (Left) Training curves of the BLEU-4 score on COCO Image Captions with different generatorarchitectures 一 relational memory (RM), LSTM-32 and LSTM-512. (Right) Training curves of the BLEU-2score on COCO Image Captions with Gumbel-Softmax relaxation and the vanilla REINFORCE method. Allthe results are obtained by taking the average of 6 runs with different random seeds.
Figure 5: (Left) The best NLLoraCle score on the synthetic data varies With different number of embeddedpresentations S = {1, 2,4, 8,16, 32, 64} where βmax = 10. (Right) The training curves of BLEU-3 score onCOCO Image Captions with the number of embedded representations S = 1 and S = 64, respectively, whereβmax = 1000. All results are obtained by taking the average of 6 runs with different random seeds.
Figure 6: Training curves of BLEU scores on COCO Image Captions with different loss functions: RSGAN(Jolicoeur-Martineau, 2018), standard GAN (the non-saturating version) (Goodfellow et al., 2014) and hingeloss (Nowozin et al., 2016; Zhang et al., 2018), where βmx = 1000 and We use two different optimizers - (a)Adam and (b) RMSProp. All the results are obtained by taking the average of 6 runs with different randomseeds. The vertical dash line represents the end of pre-training. We can see that RelGAN works well withdifferent commonly-used loss functions of GANs and different optimization methods. In this scenario, theperformance of RSGAN and standard GAN outperforms the hinge loss version.
Figure 7: Training curves of BLEU scores on COCO Image Captions with different generator architectures -relational memory (RM), LSTM-32 and LSTM-512, where βmax = 1000. We can see that the BLEU scoresof relational memory are consistently better than those of LSTM-32 and LSTM-512, which demonstrates theadvantages of using relational memory as generator in RelGAN.
Figure 8: Training curves of BLEU scores on COCO Image Captions with different gradient relaxations forGANs on discrete data - Gumbel-Softmax relaxation and REINFORCE method. We can see that the BLEUscores of Gumbel-Softmax relaxation are consistently better than those of REINFORCE method, which demon-strates the advantages of using Gumbel-Softmax relaxation to deal with non-differentiable issues in RelGAN.
Figure 9: Training curves of BLEU scores on COCO Image Captions with different number of embeddedrepresentations S = 1 and S = 64, where βmax = 1000. We can see that the BLEU scores of S = 64are consistently better than those of S = 1, which demonstrates the advantages of using multiple embeddedrepresentations for discriminator in RelGAN.
Figure 10: The training curve of NLLgen in RelGAN on coco image captions, where βmax = 1000. We cansee that during the adversarial training, the NLLgen score first increases and then decreases after around 800iterations. The turning point matches well with those in the training curves of BLEU scores.
Figure 11: The BLEU-4 (Left) and NLLgen (Right) scores with error bars in RelGAN on COCO Image Captionswith varying maximum inverse temperature βmaχ .
