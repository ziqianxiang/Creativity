Figure 1: The same image, decoded with a model computing the prior using integer arithmetic (left),and the same model using floating point arithmetic (right). The image was decoded correctly, begin-ning in the top-left corner, until floating point round-off error caused a small discrepancy betweenthe sender’s and the receiver’s copy of the prior, at which point the error propagated catastrophically.
Figure 2: Left: Example nonlinearity implementing a saturating rectifier for 4-bit unsigned integeroutputs, given by gQReLU(v) = max(min(v, 15), 0). This nonlinearity can be implemented deter-ministically either using a lookup table or simply using a clipping operation. The correspondingscaled cumulative of a generalized Gaussian with β = 4 used for computing gradients is plotted incyan, and other choices of β in gray. Right: Example nonlinearity approximating hyperbolic tan-gent for 4-bit signed integer outputs, given by gQtanh(v) = Q(7tanh(考)).This nonlinearity can beimplemented deterministically using a lookup table. The corresponding scaled hyperbolic tangentused for computing gradients is plotted in cyan.
Figure 3: Rate-distortion performance of image compression models with integer priors (left andUP is better). Left: performance of Bane et al. (2018) model vs. the same model with an integerprior. The performance is identical, but the latter can be reliably deployed across different hardwareplatforms. Right: performance of Balle (2018) ReLU model with 128 filters per layer vs. the samemodel, with integer transforms and QReLU activation functions and 128 or 256 filters per layer. Theapproximation capacity of integer networks is diminished vs. floating point networks, but doublingthe number of filters per layer more than compensates for the loss.
Figure 4: Loss function across training of Bane (2018) model, evaluated on Kodak (1993), corre-sponding to the rate point at approximately 0.7 bits per pixel in figure 3, right panel. Generally,training of integer models takes somewhat longer and is somewhat noisier than training of floatingpoint models. When matching floating point and integer networks for asymptotic performance (128vs. 256 filters, respectively), integer networks take longer to converge (likely due to their largernumber of filters). When matching by number of filters (128), it appears that the training time toconvergence is about the same, but the performance ends up worse.
