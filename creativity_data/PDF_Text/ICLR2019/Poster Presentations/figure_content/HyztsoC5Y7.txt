Figure 1: We implement our sample-efficient meta-reinforcement learning algorithm on a real legged millirobot,enabling online adaptation to new tasks and unexpected occurrences such as losing a leg (shown here), novelterrains and slopes, errors in pose estimation, and pulling payloads.
Figure 2: Two real-world and four simulated en-vironments on which our method is evaluated andadaptation is crucial for success (e.g., adapting todifferent slopes and leg failures)5 Model-Based Meta-Reinforcement LearningNow that we have discussed our approach for enabling online adaptation, we next propose how tobuild upon this idea to develop a model-based meta-reinforcement learning algorithm. First, weexplain how the agent can use the adapted model to perform a task, given parameters θ* and ψ* fromoptimizing the meta-learning objective.
Figure 3: Histogram of normalized K -step model predictionerrors of GrBAL, showing the improvement of the post-updatemodel’s predictions over the pre-update ones.
Figure 4: Compared to model-free RL, model-free meta-RL, and model-based RL methods, our model-basedmeta-RL methods achieve good performance with 1000× less data. Dotted lines indicate performance atconvergence. For MB+DE+MPPI, we perform dynamic evaluation at test time on the final MB+MPPI model.
Figure 5: Simulated results in a variety ofdynamic test environments. GrBAL outper-forms other methods, even the MB oracle,in all experiments where fast adaptation isnecessary. These results highlight the dif-ficulty of training a global model, and theimportance of adaptation.
Figure 6: GrBAL clearly outperforms both MB andMB+DE, when tested on environments that (1) re-quire online adaptation, and/or (2) were never seenduring training.
Figure 7: The dotted black line indicates the desired trajectory in the xy plane. By effectively adaptingonline, our method prevents drift from a missing leg, prevents sliding sideways down a slope, accounts for posemiscalibration errors, and adjusts to pulling payloads (left to right). Note that none of these tasks/environmentswere seen during training time, and they require fast and effective online adaptation for success.
Figure 8: Histogram of the K step normalized error across different tasks. GrBAL accomplishes lower modelerror when using the parameters given by the update rule.
Figure 9: At each time-step we show the K step normalized error across different tasks. GrBAL accomplisheslower model error using the parameters given by the update rule.
Figure 10: Effect of the meta-training distribution on test performanceB Effect of Meta-Training DistributionTo see how training distribution affects test performance, we ran an experiment that used GrBAL totrain models of the 7-DOF arm, where each model was trained on the same number of datapointsduring meta-training, but those datapoints came from different ranges of force perturbations. Weobserve (in the plot below) that1.	Seeing more during training is helpful during testing — a model that saw a large range of forceperturbations during training performed the best2.	A model that saw no perturbation forces during training did the worst3.	The middle 3 models show comparable performance in the "constant force = 4" case, which is anout-of-distribution task for those models. Thus, there is not actually a strong restriction on what needsto be seen during training in order for adaptation to occur at train time (though there is a general trendthat more is better)15Published as a conference paper at ICLR 2019C Sensitivity of K and MIn this section we analyze how sensitive is our algorithm w.r.t the hyperparameters K and M . Inall experiments of the paper, we set K equal to M. Figure 11 shows the average return of GrBALacross meta-training iterations of our algorithm for different values of K = M . The performanceof the agent is largely unaffected for different values of these hyperparameters, suggesting that our
Figure 11: Learning curves, for different values of K = M, of GrBAL in the half-cheetah disabled and slopedterrain environments. The x-axis shows data aggreation iterations during meta-training, whereas the y-axis showsthe average return achieved when running online adaptation with the meta-learned model from the particulariteration. The curves suggest that GrBAL performance is fairly robust to the values of these hyperparameters.
