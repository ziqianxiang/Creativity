Figure 1: Learning Moore Machine Networks. (1.) Learn an RNN policy. (2.) Learn QBN’s toquantize memory and observations. (3.) Insertion and Fine Tuning. The modules labelled O, R areobservation feature-extraction and recurrent modules, respectivelyWe represent a QBN via a continuous multilayer encoder E, which maps inputs x to a latent encod-ing E(x), and a corresponding multilayer decoder D. To quantize the encoding, the QBN output isgiven byb(x) = D(quantize(E(x)))In our case, we use 3-level quantization in the form of +1, 0 and -1 using the quantize function,which assumes the outputs of E(x) are in the range [-1, 1].1 One choice for the output nodes ofE(x) would be the tanh activation. However, since the gradient of tanh is close to 1 near 0, it canbe difficult to produce quantization level 0 during learning. Thus, as suggested in Pitis (2017), tosupport 3-valued quantization we use the following activation function, which is flatter in the regionaround zero input.
Figure 2: Moore Machine representation for Atari policiesnear equivalent Moore Machine Network (MMN) which has quantized memory and observationfeatures. From the MMN we then extract a discrete Moore machine that can then be transformedinto an equivalent minimal machine for analysis and usage. Our results on two environments wherethe ground truth machines are known show that our approach is able to accurately extract the groundtruth. We also show experiments in six Atari games, where we have no prior insight into the groundtruth machines. We show that, in most cases, the learned MMNs maintain similar performance tothe original RNN policies. Further, the extracted machines provide insight into the memory usageof the policies. First, we see that the number of required memory states and observations is surpris-ingly small. Second, we can identify cases where the policy did not use memory in a significantway (e.g. Pong) and policies that relied only on memory and ignored the observations (e.g. Bowlingand Freeway). To our knowledge, this is the first work where this type of insight was reported forpolicies in such complex domains. A key direction for future work is to develop tools and visualiza-tions for attaching meaning to the discrete observations and in turn states, which will allow for anadditional level of insight into the policies. It is also worth considering the use of tools for analyzingfinite-state machine structure to gain further insight and analyze formal properties of the policies.
Figure 3: Moore machine representation of Mode Counter Environments (MCE). We use ‘m’ toindicate the activate mode/action required in that state. Given M = 4 , we have 4 observationsclasses, oi = (0,0.25] , 02 = (0.25, 0.5], 03 = (0.5, 0.75] and 04 = (0.75,1]. Also, o* implies thatthe transaction is valid for all observations. The minimal moore machines extracted by our approachfrom trained RNN policies exactly matches these ground truth machines.
Figure 4: Extracted Moore machine representation for Tomita Grammar policies where Bh = 16.
