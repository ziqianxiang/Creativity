Figure 1: A high-level overview of Deep Graph Infomax. Refer to Section 3.4 for more details.
Figure 2: The DGI setup on large graphs (such as Reddit). Summary vectors, ~s, are obtained bycombining several subsampled patch representations, ~hi (here obtained by sampling three and twoneighbors in the first and second level, respectively).
Figure 3: t-SNE embeddings of the nodes in the Cora dataset from the raw features (left), featuresfrom a randomly initialized DGI model (middle), and a learned DGI model (right). The clusters ofthe learned DGI model’s embeddings are clearly defined, with a Silhouette score of 0.234.
Figure 4: Discriminator scores,attributed to each node in the Cora dataset shown over aDt-SNE of the DGI algorithm. Shown for both the original graph (left) and a negative sample (right).
Figure 5: The learnt embeddings of the highest-scored positive examples (upper half ), and thelowest-scored negative examples (lower half ).
Figure 6: Classification performance (in terms of test accuracy of logistic regression; left) anddiscriminator performance (in terms of number of poorly discriminated positive/negative examples;right) on the learnt DGI embeddings, after removing a certain number of dimensions from theembedding—either starting with most distinguishing (p ↑) or least distinguishing (p 1).
Figure 7:	DGI also works under a corruption function that modifies only the adjacency matrix(A 6= A) on the Cora dataset. The left range (ρ → 0) corresponds to no modifications of theadjacency matrix—therein, performance approaches that of the randomly initialized DGI model.
Figure 8:	DGI is stable and robust under a corruption function that modifies both the feature matrix(X 6= X) and the adjacency matrix (A 6= A) on the Cora dataset. Corruption functions that preservesparsity (P ≈ 得)perform the best. However, DGI still performs well even with large disruptions(where edges are added or removed with probabilities approaching 1). N.B. log scale used for ρ.
