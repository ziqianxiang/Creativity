Figure 1: Our proposed Big-Little Net (bL-Net) for efficient multi-scale feature representations. (a)The bL-Net stacks several Big-Little Modules. A bL-module include K branches (K = 2 in thisillustration) where the kth branch represents an image scale of 1/2k. ‘M’ here denotes a mergingoperation. (b) Our implementation of the Big-Little Module includes two branches. The Big-Branchhas the same structure as the baseline model while the Little-Branch reduces the convolutional layersand feature maps by α and β, respectively. Larger values of α and β lead to lower computationalcomplexity in Big-Little Net.
Figure 2: Comparison with the ResNet and ResNeXt related works.
Figure 3: Comparison performance among other types of networks. (a) FLOPs. (b) GPU Speed.
Figure 4: Prediction results for bL-ResNet-50 and ResNet-50-lowres. True labels, predicted labelsand their probability are listed in the table. When both models predicts correctly ((a) and (b)),bL-ResNet-50 achieves much higher probability; on the other hand, bL-ResNet-50 captures thedetails on the object and then predicts correctly ((c) and (d)).
