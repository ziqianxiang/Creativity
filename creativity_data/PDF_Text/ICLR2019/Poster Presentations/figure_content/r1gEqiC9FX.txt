Figure 1: Matrices Wk and Wk+ι are updated by multiplying the columns of the first matrix withrescaling coefficients. The rows of the second matrix are inversely rescaled to ensure that the productof the two matrices is unchanged. The rescaling coefficients are strictly positive to ensure functionalequivalence when the matrices are interleaved with ReLUs. This rescaling is applied iterativelyto each pair of adjacent matrices. In this paper, we address the more complex cases of biases,convolutions, max-pooling or skip-connections to be able to balance modern CNN architectures.
Figure 2: Rescaling the weights of two consecutive convolutional layers that preserves the functionimplemented by the CNN. Layer k scales channel number i of the input activations by Yi and layerk+1 cancels this scaling with the inverse scalar so that the activations after layer k+1 are unchanged.
Figure 3: Rescaling an elementary block within a ResNet-18 consists of 3 steps. (1) Conv1 and Con-vSkip are left-rescaled using the rescaling coefficients between blocks k - 1 and k; (2) Conv1 andConv2 are rescaled as two usual convolutional layers; (3) Conv2 and ConvSkip are right-rescaled us-ing the rescaling coefficients between blocks k and k +1. Identical colors denote the same rescalingcoefficients D. Coefficients between blocks are rescaled as detailed in Section C.2.
Figure 4: MNIST auto-encoder results(lower is better).
Figure 5: CIFAR-10 fully-connected results(higher is better).
Figure 6: ReSNet-18 results on the ImageNet dataset, batch size 64.
Figure 7: Energy of the network ('2-norm of the weights), before ENorm. Each dot represents thenorm of one column in the layer’s weight matrix.
Figure 8: Energy of the network through successive ENorm iterations (without training). One colordenotes one iteration. The darker the color, the higher the iteration number.
Figure 9: Iterating ENorm cycles on a randomly initialized ResNet-18 with no training.
Figure 10: Training a fully-connected network on CIFAR-10, with (ENorm-1) or without (Baseline)Equi-normalization.
Figure 11: Uniform vs adaptive scaling onMNIST, without BN.
Figure 12: Uniform vs adaptive scaling onMNIST, with BN.
