Figure 1: Comparison of predictions given by a fully trained NP and Attentive NP (ANP) in 1D func-tion regression (left) / 2D image regression (right). The contexts (crosses/top half pixels) are usedto predict the target outputs (y-values of all x ∈ [-2, 2]/all pixels in image). The ANP predictionsare noticeably more accurate than for NP at the context points.
Figure 2: Model architecture for the NP (left) and Attentive NP (right)______ DeterministicPathLatent......Path⅛m MeanFigure 2 describes how attention is incorporated into NP to give the Attentive NP (ANP). In sum-mary, self-attention is applied to the context points to compute representations of each (x, y) pair,and the target input attends to these context representations (cross-attention) to predict the targetoutput. In detail, the representation of each context pair (xi , yi)i∈C before the mean-aggregationstep is computed by a self-attention mechanism, in both the deterministic and latent path. The intu-ition for the self-attention is to model interactions between the context points. For example, if manycontext points overlap, then the query need not attend to all of these points, but only give high weightto one or a few. The self-attention will help obtain richer representations of the context points thatencode these types of relations between the context points. We model higher order interactions bysimply stacking the self-attention, as is done in Vaswani et al. (2017).
Figure 3: Qualitative and quantitative results of different attention mechanisms for 1D GP functionregression with random kernel hyperparameters. Left: moving average of context reconstructionerror (top) and target negative log likelihood (NLL) given contexts (bottom) plotted against trainingiterations (left) and wall clock time (right). d denotes the bottleneck size i.e. hidden layer size of allMLPs and the dimensionality of r and z. Right: predictive mean and variance of different attentionmechanisms given the same context. Best viewed in colour.
Figure 4: Qualitative and quantitative results on test set for 2D CelebA function regression.
Figure 5: Reconstruction of full image from top half. The CelebA results use the same models (withthe same parameter values) as Figure 4a.
Figure 6: Mapping between different resolutions by the same model (with the same parametervalues) as Stacked Multihead ANP in Figures 4a, 5b. The two rightmost columns show the resultsof baseline methods, namely linear and cubic interpolation to 256 × 256.
Figure 7: Pixels attendedto by each head of multi-head attention in MultiheadANP given a target pixel.
Figure 8:	The model architecture for NP and ANP for both 1D and 2D regression.
Figure 9:	Same as right of Figure 3 but also comparing against the oracle GP from which contextwas drawn.
Figure 10: Same as Figure 3 but for fixed kernel hyperparameters.
Figure 11: KL term in NP loss throughout training for data generated from a GP with fixed (left)and random (right) kernel hyperparameters, using the same colour scheme as Figure 10.
Figure 12: Simple and cumula-tive regret for BO.
Figure 13: Qualitative and quantitative results of different attention mechanisms on test set for 2DMNIST function regression.
Figure 14:	More MNIST reconstruction of full image from top half.
Figure 15:	More CelebA reconstruction of full image from top half.
Figure 16: Visualisation of pixels attended by each head of multihead attention in the NP givena target pixel and a separate context of 100 random pixels. Each head is given a different colour(consistent with the colours in Figure 7 and the target pixel is marked by a cross.
Figure 17: Same as Figure 4a but for a different image.
Figure 18:FigutxetnoC 1 elpmaS 2 elpmaS 3 elpmaSfor a different image.
Figure 19: Mapping from 32 × 32 to 256 × 256 for different images.
