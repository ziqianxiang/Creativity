Figure 1: Illustration of adversarial reprogramming. (a) Mapping of ImageNet labels to adversar-ial task labels (squares count in an image). (b) Two examples of images from the adversarial task(left) are embedded at the center of an adversarial program (middle), yielding adversarial images(right). The adversarial program shown repurposes an Inception V3 network to count squares inimages. (c) Illustration of inference with adversarial images. The network when presented withadversarial images will predict ImageNet labels that map to the adversarial task labels.
Figure 2: Examples of adversarial programs for MNIST classification. Adversarial programswhich cause six ImageNet models to function as MNIST classifiers. Each program is shown beingapplied to an MNIST digit.
Figure 3: Adversarial programs may be limited in size or concealed. In all panels, an InceptionV3 model pretrained on ImageNet is reprogrammed to classify MNIST digits. Example images(a) with adversarial programs of different sizes, and (b) with adversarial programs of differentperturbation scales. In (c), the adversarial data + program (right) are hidden inside a normal imagefrom ImageNet (left), yielding an adversarial image (center) that is able to reprogram the networkto function as an MNIST classifier. The pixels of the adversarial data are shuffled to conceal itsstructure.
