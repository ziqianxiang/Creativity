Figure 1: An overview of DARTS: (a) Operations on the edges are initially unknown. (b) Continuousrelaxation of the search space by placing a mixture of candidate operations on each edge. (c) Jointoptimization of the mixing probabilities and the network weights by solving a bilevel optimizationproblem. (d) Inducing the final architecture from the learned mixing probabilities.
Figure 2: Learning dynamics of our iterative algorithmwhen Lval (w, α) = αw - 2α + 1 and Ltrain (w, α) =w2 -2αw+α2, starting from (α(0), w(0)) = (2, -2). Theanalytical solution for the corresponding bilevel optimiza-tion problem is (α*,w*) = (1,1), which is highlightedin the red circle. The dashed red line indicates the fea-sible set where constraint equation 4 is satisfied exactly(namely, weights in w are optimal for the given architec-ture α). The example shows that a suitable choice of ξhelps to converge to a better local optimum.
Figure 3: Search progress of DARTS for convolutional cells on CIFAR-10 and recurrent cells onPenn Treebank. We keep track of the most recent architectures over time. Each architecture snapshotis re-trained from scratch using the training set (for 100 epochs on CIFAR-10 and for 300 epochs onPTB) and then evaluated on the validation set. For each task, we repeat the experiments for 4 timeswith different random seeds, and report the median and the best (per run) validation performance ofthe architectures over time. As references, we also report the results (under the same evaluation setup;with comparable number of parameters) of the best existing cells discovered using RL or evolution,including NASNet-A (Zoph et al., 2018) (2000 GPU days), AmoebaNet-A (3150 GPU days) (Realet al., 2018) and ENAS (0.5 GPU day) (Pham et al., 2018b).
Figure 4: Normal cell learned on CIFAR-10.
Figure 5: Reduction cell learned on CIFAR-10.
Figure 6: Recurrent cell learned on PTB.
