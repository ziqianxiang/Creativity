Figure 1: Framework of CapsGNN. At first, GNN is used to extract node embeddings and formprimary capsules. Attention module is used to scale node embeddings which is followed by DynamicRouting to generate graph capsules. At the last stage, Dynamic Routing is applied again to performgraph classification.
Figure 2: The structure of Attention Module. We first flatten primary capsules and apply two layerfully-connected neural network to generate attention value for each capsule. Node-based normal-ization (normalize each row here) is applied to generate final attention value. Scaled capsules arecalculated by multiplying the normalized value with primary capsules.
Figure 3: Comparison of efficiency in feature representation. The horizontal axis represents thesetting of tested architectures. The vertical axis represents classification accuracy on NCI1.
Figure 4: The structure of Coordinate Addition Module. We take the capsules extracted from thefinal layer of GNN as the position indicators of corresponding nodes by concatenating it with eachcapsule of the node. The node capsules votes generated in this way contain more position informa-tion.
