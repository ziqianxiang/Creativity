Figure 1: Examples of tasks solved with POLO. A 2D point agent navigating a maze without anydirected reward signal, a complex 3D humanoid standing up from the floor, pushing a box, and in-hand re-positioning of a cube to various orientations with a five-fingered hand. Video demonstrationof our results can be found at: https://sites.google.com/view/polo-mpc.
Figure 2: 2D point mass navigation task in a world with no rewards. Fig. (a) describes the percentageof an occupancy grid covered by the agent, averaged over 10 random seeds. Fig. (b) depicts an agentover 1000 timesteps; red indicates regions of high value (uncertainty) while blue denotes low. Thevalue function learns to assign the true, low values to regions visited and preserves high values tounexplored regions; uncertainty and long horizons are observed to be critical for exploration.
Figure 3: Performance as a function of planning horizon for the humanoid getup (left), and in-hand manipulation task (middle). POLO was trained for 12000 and 2500 environment timesteps,respectively. We test POLO with the learned terminal value function against pure MPC and compareaverage reward obtained over 3 trials in the getup task and 1000 steps in the manipulation task. Onthe right, a value function trained with POLO is used by MPC without per-time-step rewards. Theagent’s height increases, indicating a task-relevant value function. For comparison, we also includethe trace of POLO with dense rewards and multiple trials (dashed vertical lines)2.03.2	Value function approximation for trajectory optimizationNext, we study if value learning helps to reduce the planning horizon for MPC. To this end, weconsider two high dimensional tasks: humanoid getup where a 3D humanoid needs to learn to standup from the ground, and in-hand manipulation where a five-fingered hand needs to re-orient a cubeto a desired configuration that is randomized every 75 timesteps. For simplicity, we use the MPPIalgorithm (Williams et al., 2016) for trajectory optimization. In Figure 3, we consider MPC and thefull POLO algorithm of the same horizon, and compare their performance after T steps of learning inthe world. We find that POLO uniformly dominates MPC, indicating that the agent is consolidatingexperience from the world into the value function. With even the longest planning horizon, thehumanoid getup task has a local solution where it can quickly sit up, but cannot discover a chain ofactions required to stand upright. POLO’s exploration allows the agent to escape the local solution,and consolidate the experiences to consistently stand up. To further test if the learned value function
Figure 4: Usefulness of trajectory optimization for value function learning. (a) illustrates that N -steptrajectory optimization accelerates the learning of the value function. N=1 corresponds to trajectorycentric fitted value iteration. A difference of 0.2 reward to MPC amounts to approximately 50%performance improvement. (b) value function trained for the nominal model (head size of 1.0) usedwith MPC for models with larger sizes.
