Figure 1: Structural causal models (SCMs) model environments using random variables U (cir-cles, ‘scenarios’), that summarize immutable aspects, some of which are observed (grey), some not(white). These are fed into deterministic functions fi (black squares) that approximate causal mech-anisms. Left: SCM for a contextual bandit with context Uc, action A, feedback O and scenario Uo .
Figure 2: Experimental results on PO-SOKOBAN environment. Left: Policy evaluation. Policyevaluation error decreases with amount of off-policy data available (in #transitions per episode) forinferring scenarios (levels) Us1 that are used for counterfactual evaluation. No data (data pointson the very left) corresponds to standard model-based policy evaluation (MB-PE), yielding largeerrors, whereas Counterfactual policy evaluation yields more accurate results. This holds for allthree policies with different true performances. Right: Policy search. Counterfactually-GuidedPolicy Search (CF-GPS) outperforms a naive model-based RL (MB-PS) algorithm as well as aversion of standard Guided Policy Search (‘GPS-like’) on PO-SOKOBAN.
Figure 3: Top: PO-SOKOBAN. Shown on the left is a procedurally generated initial state. Theagent is shown in green, boxes in yellow, targets in blue and walls in red. The agent does notobserve this state but a sequence of observations, which are masked by iid noise with 0.9 probability,except a 3x3 window around the agent. Bottom: Inference model. For counterfactual inference inPO-SOKOBAN, we need the (approximate) inference distribution p(Us1|hT) over the initial stateUs1 = S1, conditioned on the history of observations hT. We model this distribution using a DRAWgenerative model with latent variables Z, which are conditioned on the output of a backward RNNsummarizing the observation history.
Figure 4: Analysis of the model mismatch of the learned inference distributions p(Us1 |ht) over theinitial PO-SOKOBAN state Us1, for three different amounts of observations t = 0, 1 and 50. Shownare two dimensions of the learned latent representation Z of Us1. The (whitened) learned priorp(Z|ht) is indicated by a red contour of one standard deviation. The inferred mean embedding ofthe true levels are show as crosses, and their aggregated density is shown in blue. With increasingamount data that the model is conditioned on, the learned distributions match the data better.
