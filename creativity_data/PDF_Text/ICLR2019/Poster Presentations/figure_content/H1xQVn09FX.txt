Figure 1: Frame-based estimation of audio waveforms. Much of sound is made up of locally-coherent waves with a local periodicity, pictured as the red-yellow sinusoid with black dots at thestart of each cycle. Frame-based techniques, whether they be transposed convolutions or STFTs,have a given frame size and stride, here depicted as equal with boundaries at the dotted lines. Thealignment between the two (phase, indicated by the solid black line and yellow boxes), precesses intime since the periodicity of the audio and the output stride are not exactly the same. Transposedconvolutional filters thus have the difficult task of covering all the necessary frequencies and allpossible phase alignments to preserve phase coherence. For an STFT, we can unwrap the phase overthe 2π boundary (orange boxes) and take its derivative to get the instantaneous radial frequency (redboxes), which expresses the constant relationship between audio frequency and frame frequency.
Figure 2: Number of wins on pair-wise comparison across different output representations and base-lines. Ablation comparing highest performing models of each type. Higher scores represent betterperceptual quality to participants. The ranking observed here correlates well with the evaluation onquantitative metrics as in Table 1.
Figure 3: Phase coherence. Examples are selected to be roughly similar between the models forillustrative purposes. The top row shows the waveform modulo the fundamental periodicity of thenote (MIDI C60), for 1028 examples taken in the middle of the note. Notice that the real datacompletely overlaps itself as the waveform is extremely periodic. The WaveGAN and PhaseGAN,however, have many phase irregularities, creating a blurry web of lines. The IFGAN is much morecoherent, having only small variations from cycle-to-cycle. In the Rainbowgrams below, the realdata and IF models have coherent waveforms that result in strong consistent colors for each har-monic, while the PhaseGAN has many speckles due to phase discontinuities, and the WaveGANmodel is quite irregular.
Figure 4: Global interpolation. Examples available for listening4. Interpolating between waveformsperceptually results in crossfading the volumes of two distinct sounds (rainbowgrams at top). TheWaveNet autoencoder (middle) only has local conditioning distributed in time, and no compactprior over those time series, so linear interpolation ventures off the true prior / data manifold, andproduces in-between sounds that are less realistic examples and feature the default failure mode ofautoregressive wavenets (feedback harmonics). Meanwhile, the IF-Mel GAN (bottom) has globalconditioning so interpolating in perceptual attributes while staying along the prior at all intermediatepoints, so they produce high-fidelity audio examples like the endpoints.
Figure 5: NDB bin proportions for the IF-Mel + H model and the WaveGAN baseline (evaluatedwith examples of pitch 60).
Figure 6: NDB bin proportions for the WaveNet baseline (evaluated with examples of pitch 60).
Figure 7: The first 20 seconds (10 seconds per a row) of the prelude to Bach’s Suite No. 1 in Gmajor 9, for pitches synthesized with a single latent vector (top), and with spherical interpolation inlatent space (bottom). The timbre is constant for a single latent vector, shown by the consistencyof the upper harmonic structure, while it varies dramatically as the latent vector changes. Listeningexamples are provided at https://goo.gl/magenta/gansynth-examples14Published as a conference paper at ICLR 2019C Baseline Model ComparisonsTable 2: Comparison of models generating waveforms directly. Our Waveform GAN baseline per-forms similar to the WaveGAN baseline, but the progressive training does not improve performance,so we only compare to the WaveGAN baseline for the paper. The 8-bit categorical WaveNet outper-forms the 16-bit mixture of logistics, likely due to the decreased stability of the 16-bit model withonly pitch conditioning, despite the increased fidelity.
