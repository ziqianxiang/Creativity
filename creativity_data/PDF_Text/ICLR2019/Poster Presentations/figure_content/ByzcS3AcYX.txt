Figure 1: Schematic diagrams of our model. (a) A content encoder Encc encodes xtxt into a textembedding c1:T , and a style encoder Encs encodes both paired and unpaired audio samples, xa+udand xa-ud, into style embeddings s+ and s-, respectively. The decoder Dec takes each of the twoconditions (c, s+) and (c, s-), and generates x+ 〜pθ(x|c, s+) and X- 〜pθ(x|c, s-). All thelosses involved in solving the adversarial game (D) and collaborative game (R and C) are indicatedby dashed lines. (b) Given xtxt and xaud , we synthesize audio with Encc , Encs and Dec. Note thatxaud does not have to be paired with xtxt . See Appendix A for more details of each module.
Figure 2: t-SNE visualization of the learned latent spaces for (a) EMT-4 and (b) VCTK datasets.
Figure 3: Encoder network architectures. (Left) The content encoder (Encs): A sequence of 128-D character-level embeddings is fed into two fully-connected layers which have [256, 128] units,respectively. Each layer is followed by a ReLU activation and dropout with a 50% chance. Theoutput is fed into a CBHG block Wang et al. (2017b). Inside in the CBHG block, the Conv1D bankhas 16 layers, where each layer has 128 units and comes with a ReLU activation. Next, the max-pooling layer has a stride of 1 and with a width of 2. The Conv1D projection has three layers, eachwith 128 units and a ReLU activation. After the residual connection is four fully-connected layers,each with 128 units and a ReLU activation. The final Bidirectional GRU has 128 cells. (Right) Thestyle encoder (Encs ) and the inference Network (C): The style encoder consists of a referenceencoder and style token layers. The reference encoder takes a N × Tmel × 80 mel-spectrogram asinput, where N is batch size, Tmel is length of mel-spectrogram, and 80 is the dimension. The sixConv2D layers have [32, 32, 64, 64, 128, 128] filters, respectively, each with a kernel size 3 × 3and a stride of 2 × 2. Each layer is followed by a ReLU activation and batch normalization. Nextis a single-layer GRU with 128 units. The final state from the GRU is fed into a fully-connectedlayer with 128 units and a tanh activation; this produces the reference embedding. In the style tokenlayers, 10 global style tokens (GSTs) are randomly initialized. The reference embedding is used asa query for a multi-head attention unit. A learned linear weight is then output from the multi-headattention unit, and the style embedding is computed as a weighted sum. The inference network (C)shares the same architecture and parameters with Encs , except that a new N-way classifier (whichconsists of a fully connected layer followed by Softmas) is added on top.
Figure 4: Decoder network architecture. The decoder (Dec) takes as input a concatenation of acontent embedding sequence C1:T and the style embedding (replicated T times). We then unrolleach of the T time slices by feeding them into two fully-connected layers, with [256, 128] units,respectively, followed by an attention RNN and a decoder RNN. The attention RNN has 2-layerresidual-GRUs, each with 256 cells. The decoder RNN has a 256 cell one-layer GRU. As an outputof each time step, 5 spectrogram slices are predicted (r = 5), and they are fed into the CBHGblock (see Figure 3(left) for detail). The final output of the decoder is the predicted spectrogram. Avocoder is used to synthesize voice audios from the spectrograms. In this work, we use the Griffin-Lim algorithm Griffin & Lim (1984) to achieve fast waveform generation.
Figure 5: Discriminator network architecture. The main computation body is similar to the referenceencoder (part of the style encoder in Figure 3), as shown on the right. The difference is that, insteadof having only spectrograms as input, it has a combination of spectrograms (either ground truth orsynthesized) and the content information (output from Encc), both shown on the left. Here we adoptglobal sentence embedding to represent the content information. The output content embeddingfrom the Content Encoder Encc is averaged along time over the whole sequence, which produces aN × 1 × 128 single embedding. To match with the dimension of the spectrogram, the single contentembedding is replicated according to the spectrograms time step (Tmel), and they are concatenatedtogether as the combined input.
Figure 6: Attention alignments by different reference sentence lengths. From left to right, thesentence length are short, medium and long, respectively. The first row is obtained by using theStyle Encoder Encs as shown in 3. The second row is obtained by only using the Reference Encoder(remove the Style token layers in Encs). As can be seen, adding the global style token layer madethe network more robust to the variance in the length of the reference audio.
