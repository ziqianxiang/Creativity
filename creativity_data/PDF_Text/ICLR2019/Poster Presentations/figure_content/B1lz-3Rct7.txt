Figure 1: Comparison of test accuracy of the networks trained with different optimizers on both CIFAR10and CIFAR100. We compare Weight Decay regularization to L2 regularization and the Baseline (which usedneither). Here, BN+Aug denotes the use of BN and data augmentation. K-FAC-G and K-FAC-F denote K-FACusing Gauss-Newton and Fisher matrices as the preconditioner, respectively. The results suggest that weightdecay leads to improved performance across different optimizers and settings.
Figure 2: Test accuracy as a function of training epoch for SGD and Adam on CIFAR-100 with different weightdecay regularization schemes. baseline is the model without weight decay; wd-conv is the model with weightdecay applied to all convolutional layers; wd-all is the model with weight decay applied to all layers; wd-fc isthe model with weight decay applied to the last layer (fc). Most of the generalization effect of weight decay isdue to applying it to layers with BN.
Figure 3: Effective learning rate of thefirst layer of ResNet32 trained with SGDon CIFAR-100. Without weight decay reg-ularization, the effective learning rate de-creases quickly in the beginning.
Figure 4: The curves of test accuracies ofResNet32 on CIFAR-100. To be noted, weuse wd and wn to denote weight decay andweight normalization respectively.
Figure 5: Relationship between K-FAC GN norm and Jacobian norm for practical deep neural networks. Eachpoint corresponds to a network trained to 100% training accuracy. Even for (nonlinear) classification networks,the K-FAC GN norm is highly correlated with both the squared Frobenius norm of the input-output Jacobian andthe generalization gap.
Figure 6: Test accuracy as a function of training epoch for K-FAC on CIFAR-100 with different weight decayregularization schemes. baseline is the model without weight decay regularization; wd-conv is the model withweight decay applied to all convolutional layers; wd-all is the model with weight decay applied to all layers;wd-fc is the model with weight decay applied to the last layer (fc). Consistent with the Jacobian regularizationhypothesis, applying weight decay to the non-BN layers have the largest regularization effect. However, applyingweight decay to the BN layers also lead to noticeable gains.
Figure 7: Trace norm of Fisher matrix and Gauss-Newton matrix of the first layer (Normalized) ofResNet32. The model was trained on CIFAR-10with K-FAC-F and BN.
Figure 8: Test accuracy as a function of training epoch. We plot baseline vs L2 regularization vs weight decayregularization on CIFAR-10 and CIFAR-100 datasets. The ’+’ denotes with BN and data augmentation. Notethat training accuracies of all the models are 100% in the end of the training. We smooth all the curves for visualclarity.
Figure 9: CIFAR-10 image classification task.
