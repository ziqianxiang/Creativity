Figure 1: (a) shows that SN adapts to various networks and tasks by learning importance ratios to selectnormalizers. In (a), a ratio is between 0 and 1 and all ratios of each task sum to 1. (b) shows the top-1 accuraciesof ResNet50 trained with SN on ImageNet and compared with BN and GN in different batch settings. Thegradients in training are averaged over all GPUs and the statistics of normalizers are estimated in each GPU.
Figure 2: The size of featuremaps is N × C × H × W (N = 4in this example). Different nor-malizers estimate statistics alongdifferent axes.
Figure 3: Geometric view of directions andlengths of the filters in IN, BN, LN, and SNby comparing them to WN.
Figure 4: Importance weights v.s. batch sizes. Thebracket (∙, ∙) indicates (#GPUs, #samples per GPU).
Figure 5: Comparisons of ‘BN’, ‘SN with movingaverage’, and ‘SN with batch average’, when trainingResNet50 on ImageNet in (8, 32). We see that SN withbatch average produces faster and more stable conver-gence than the other methods.
Figure 6: Comparisons of learning curves. (a) visualizes the validation curves of SN with different settingsof batch size. The bracket (∙, ∙) denotes (#GPUs, #samples per GPU). (b) compares the top-1 train andvalidation curves on ImageNet of SN, BN, and GN in the batch size of (8,32). (c) compares the train andvalidation curves of SN and GN in the batch size of (8,2).
Figure 7:	Selected operations of each SN layer in ResNet50. There are 53 SN layers. (a,b) show the im-PortanCe weights for μ and σ of (8, 32), while (c,d) show those of (8, 2). The y-axis represents the importanceweights that sum to 1, while the x-axis shows different residual blocks of ResNet50. The SN layers in differentplaces are highlighted differently. For example, the SN layers follow the 3 × 3 conv layers are outlined byshaded color, those in the shortcuts are marked with ‘’, while those follow the 1 × 1 conv layers are in flatcolor. The first SN layer follows a 7 × 7 conv layer. We see that SN learns distinct importance weights fordifferent normalization methods as well as μ and σ, adapting to different batch sizes, places, and depths of adeep network.
Figure 8:	Average precision (AP) curves of Faster R-CNN on the 2017 val set of COCO. (a) plots the resultsof finetuning pretrained networks. (b) shows training the models from scratch.
Figure 9: Selected normalizers of each SN layer in ResNet50 for semantic image parsing in ADE20Kand Cityscapes. There are 53 SN layers. (a,b) show the importance weights for μ and σ of (8, 2) in ADE20K,while (c,d) show those of (8, 2) in Cityscapes. The y-axis represents the importance weights that sum to 1,while the x-axis shows different residual blocks of ResNet50. The SN layers in different places are highlighteddifferently. For example, the SN layers follow the 3 × 3 conv layers are outlined by shaded color, those in theshortcuts are marked with ‘’, while those follow the 1 × 1 conv layers are in flat color.
Figure 10: (a) shows the losses of BN, IN, and SN in the task of image stylization. SN converges fasterthan IN and BN. As shown in Fig.1 and the supplementary material, SN adapts its importance weight to INwhile producing comparable stylization results. (b) plots the accuracy on the validation set of CIFAR-10 whensearching network architectures.
Figure 11: Results of Image Stylization. The first column visualizes the content and the style images. Thesecond and third columns are the results of IN and SN respectively. SN works comparably well with IN in thistask.
