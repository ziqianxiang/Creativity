Figure 1: The proposed discretization process. (a) Given a distribution p(X) over the real line Wepartition it into K intervals of width α where the center of each of the intervals is a grid point gi . Theshaded area corresponds to the probability of X falling inside the interval containing that specific gi.
Figure 2: Best viewed in color. Illustration of the inductive bias obtained via training with theproposed quantizer; means of the logistic distribution over the weights for each layer of the LeNet-5when trained with 2 bits per weight and activation. Each color corresponds to an assignment to aparticular grid point and the vertical dashed lines correspond to the grid points (β = 0). We canclearly see that the real valued weights are naturally encouraged through training to cluster intomultiple modes, one for each grid point. It should also be mentioned, that for the right and leftmostgrid points the probability of selecting them is maximized by moving the corresponding weightfurthest right or left respectively. Interestingly, we observe that the network converged to ternaryweights for the input and (almost) binary weights for the output layer.
Figure 3: Local grid constructionn∕~ / ∣~ P P P I[ C ]、	P (X ≤ C) - P (X < [x] - δσ)P(x ≤ c|* * * * * * * * * X ∈ (bXe - δσ, bXe + δσD= 4~ τ I ]、~~=~ / I 1~~-f7	(6)P(x ≤ bx∣ + δσ) — P(x < bx∣ - δσ)X =	):	zigi	(7)gi ∈(bxe-δσ,bxe+δσ]2.3 Relation to Stochastic RoundingOne of the pioneering works in neural network quantization has been the work of Gupta et al. (2015);it introduced stochastic rounding, a technique that is one of the most popular approaches for trainingneural networks with reduced numerical precision. Instead of rounding to the nearest representablevalue, the stochastic rounding procedure selects one of the two closest grid points with probabilitydepending on the distance of the high precision input from these grid points. In fact, we can viewstochastic rounding as a special case of RQ where p(X) = U(x — 2, x + 2). This uniform distributioncentered at X of width equal to the grid width α generally has support only for the closest grid point.
Figure 4: Best viewed in color. Comparison of various methods on Resnet-18 and Mobilenetaccording to top-1 error (on the y-axis) and bit operations (on the x-axis) computed according tothe formula described in Baskin et al. (2018). Each dashed line corresponds to employing a specificbit configuration for every layer’s weights and activations. Values for top-1 and top-5 errors aregiven in Table 2 in the Appendix. We compare against multiple works that employ fixed-pointquantization: SR+DR (Gupta et al., 2015; Gysel et al., 2018), LR Net (Shayer et al., 2018), Jacobet al. (2017), TWN (Li et al., 2016), INQ (Zhou et al., 2017), BWN (Rastegari et al., 2016), XNOR-net (Rastegari et al., 2016), DoReFa (Zhou et al., 2016), HWGQ (Cai et al., 2017), ELQ Zhou et al.
Figure 5: Learning curves for the VGG on CIFAR 10.
