Figure 1: Our system diagram. A: A neural network architecture is randomly sampled, forming aGHN. B: After graph propagation, each node in the GHN generates its own weight parameters. C:The GHN is trained to minimize the training loss of the sampled network with the generated weights.
Figure 2: Stacked GHN alongthe depth dimension.
Figure 3: Comparison with state-of-the-arthuman-designed networks on CIFAR-10.
Figure 4: Comparison between random 10 andtop 10 networks on CIFAR-10.
Figure 5: GHN when varying the number of nodes and propagation schemeshot model proposed by Pham et al. (2018), where nodes store a set of shared parameters for eachpossible operation. Unlike GHN, which is compatible with varying number of nodes, the one-shotmodel must be trained with N = 17 nodes to match the evaluation. The GHN is trained withN = 7, T = 5 using forward-backward propagation. These GHN parameters are selected based onthe results found in Section 5.4.
Figure 6: Comparison for 100 randomly sampled architectures.
Figure 7: Best block found for classification7.5.2 Anytime PredictionFigures 8, 9 and 10 show blocks 1 2 and 3 of the best architecture found in the anytime experiments.
Figure 10: Block 3 for anytime network. Red color denotes early exit.
