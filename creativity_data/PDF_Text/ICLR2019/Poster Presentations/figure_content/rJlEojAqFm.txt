Figure 1: (a) The RFM module stacks a GN Encoder, a Graph GRU and a GN Decoder to obtaina relational reasoning module that holds state information across time steps. (b) Example of anenvironment graph representation. Edges connect agents (magenta and orange) to all entities and arecolor-coded according to the identity of the receiver. (c) RFM-augmented agents, the output of thethe RFM module is appended to the original observation input to the policy network. The on-boardRFM module is trained with full supervision and alongside the policy network.
Figure 2: Action prediction performance of our RFM module and baseline models. The reportedquantity is the mean number of environment steps for which the predicted actions matched theground truth exactly, for all agents. Mean across 128 episodes, bars indicate standard deviation acrossepisodes. Alternative measures of model performance show similar results (Fig. 10).
Figure 3: Edge analysis. Top-row: the norm of output edge activations is predictive of future behavior.
Figure 4: Return analysis: we trained our RFM model to predict the return (until the end of the episode)received by each agent. We trained our model on graphs with and without edges connecting the twoagents. (Left) ground truth and predicted return (using both graphs) for a sample episode. (Middle)aaRFau1ll graph - RPar1uned Graph around the time a stag is captured. Positive value indicates that the modelaaestimates that the social influence has a positive marginal utility. (Right) RFau1ll graph - RPar1uned Graphright before and right after a stag is captured: agents’ influence are most beneficial for each otherwhen they a capture a stag. Episodes ran for 128 steps for this analysis.
Figure 5: Training curves for A2C agents with and without on-board RFM modules. Allowing agentsto access the output of a RFM module results in agents that learn to coordinate faster than baselineagents. This also scales to different number of agents. Importantly, the on-board RFM module istrained alongside the policy network, and there is no sharing of parameters or gradients between theagents. We also show curves for training alongside learning teammates in Fig. 8. Embedding an RFMis also more beneficial than embedding an MLP+LSTM (see Fig. 7.)3	RFM-augmented agents3.1	MethodsWe have shown that relational reasoning modules capture information about the social dynamics ofmulti-agent environments. We now detail how these modules’ predictions can be useful for improvingMARL agents’ speed of learning. We extended the agent architecture (described in Sec. 2.1.2) byembedding a RFM module in each agent, and augmenting the policy network’s observations with theRFM’s output. This agent architecture is depicted in Fig. 1c.
Figure 6: Coin collection analysis in the Coin Game.
Figure 7: Augmenting agents with predictions from a non-relational model.
Figure 8: Agents training with non-expert teammates. Reward shown as the average return per agent,averaged over four agent seeds.
Figure 9: If coordination is not required to collect stags, or if agents are not interested in collectingstags, the edge norm between the two agents is not affected by the appearance of available stags.
Figure 10: Next-step action classification accuracy.
