Figure 1: A neural network architecture for input-output program synthesis (e.g., (Bunel et al.,2018)). At each timestep t, the decoder LSTM generates a program token gt conditioned on both theinput-output pairs {IOK} and the previous program token gt-1. Each IO pair is fed into the LSTMindividually, and a max-pooling operation is performed over the hidden states {htk}kK=1 of the lastlayer of LSTM for all IO pairs. The resulted vector is fed into a softmax layer to obtain a predictionprobability distribution over all the possible program tokens in the vocabulary. More details can befound in Appendix C.
Figure 2: An example of the execution of partial programs to reach the target state in the Kareldomain. The blue dot denotes the marker put by the Karel robot.
Figure 3: Results of the ensemble model trained with Exec + RL approach. Left: generalizationaccuracy. Right: exact match accuracy. The corresponding figures using models trained with Execapproach can be found in Appendix D.2.
Figure 4: Grammar for the Karel task.
Figure 5: An example of the predicted program that generalizes to all input-output examples, but isdifferent from the ground truth. Here, we only include 2 out of 5 input-output examples for simplicity.
Figure 6: Results of the ensemble model trained with our Exec approach. Left: generalizationaccuracy. Right: exact match accuracy.
