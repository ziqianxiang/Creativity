Figure 1: Generative Transformer Memory Network. An IR system provides knowledge candi-dates from Wikipedia. Dialogue Context and Knowledge are encoded using a shared encoder. In theTwo-stage model, the dialogue and knowledge are re-encoded after knowledge selection.
Figure 2: Selected conversations between humans and models.
Figure 3: The Wizard of Wikipedia dataset. Examples of collected conversations from the dataset,where both wizard and apprentice are humans. The wizard has access to an information retrievalsystem over Wikipedia, so that they can ask and answer questions, and make statements relevantto the discussion. For each utterance, knowledge retrieval is performed based on dialogue history,giving ã€œ61 knowledge candidates per turn, with wizards clicking no sentence used 6.2% of the time.
Figure 4:	Retriever with Knowledge conversations. Selected conversations with a human. (*)indicates clear factual mistakes by the model.
Figure 5:	Two-stage Generator conversations. Selected conversations with a human. (*) indicatesclear factual mistakes by the model.
