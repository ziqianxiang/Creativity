Figure 1: (a) Comparison of the straight-through gradient method and our PROXQUANT method. The straight-through method computes the gradient at the quantized vector and performs the update at the original realvector; ProxQuant performs a gradient update at the current real vector followed by a prox step whichencourages quantizedness. (b) A two-function toy failure case for BinaryConnect. The two functions aref1 (x) = |x + 0.5| - 0.5 (blue) and f-1 (x) = |x - 0.5| - 0.5 (orange). The derivatives of f1 and f-1coincide at {-1, 1}, so any algorithm that only uses this information will have identical behaviors on these twofunctions. However, the minimizers in {±1} are x1? = -1 and x?-1 = 1, so the algorithm must fail on one ofthem.
Figure 2: W-shaped regularizerfor binary quantization.
Figure 3: SignChange(θ0 , θt) against t (epoch) for BinaryConnect and PROXQUANT, over 4 runs startingfrom the same full-precision ResNet-20. ProxQuant has significantly lower sign changes than BinaryCon-nect while converging to better models. (a) The first conv layer of size 16 × 3 × 3 × 3; (b) The last conv layerof size 64 × 64 × 3 × 3; (c) The fully connected layer of size 64 × 10; (d) The validation top-1 error of thebinarized nets (with moving average smoothing).
