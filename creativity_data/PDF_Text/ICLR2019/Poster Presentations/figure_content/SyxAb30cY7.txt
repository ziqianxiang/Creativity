Figure 1: Comparison of the standard accuracy of models trained against an '2-bounded adversaryas a function of size of the training dataset. We observe that when training with few samples,adversarial training has a positive effect on model generalization (especially on MNIST). However,as training data increase, the standard accuracy of robust models drops below that of the standardmodel (£b。讪=0). Similar results for '∞ trained networks are shown in Figure 6 of Appendix G.
Figure 2: Visualization of the loss gradient with respect to input pixels. Recall that these gradientshighlight the input features which affect the loss most strongly, and thus are important for the classi-fier’s prediction. We observe that the gradients are significantly more interpretable for adversariallytrained networks - they align well with perceptually relevant features. In contrast, for standardnetworks they appear very noisy. We observe that gradients of '∞-trained models tend to be sparserthan those of `2 -trained models. (For MNIST, blue and red pixels denote positive and negativegradient regions respectively. For CIFAR-10 and ImageNet, we clip gradients to within ±3σ andrescale them to lie in the [0, 1] range.) Additional visualizations are in Figure 10 of Appendix G.
Figure 3: Visualizing large-ε adversarial examples for standard and robust ('2 /'∞-adversarialtraining) models. We construct these examples by iteratively following the (negative) loss gradientwhile staying with '2-distance of ε from the original image. We observe that the images producedfor robust models effectively capture salient data characteristics and appear similar to examples of adifferent class. (The value of ε is equal for all models and much larger than the one used for training.)Additional examples are visualized in Figure 8 and 9 of Appendix G.
Figure 4: Interpolation between original image and large-ε adversarial example as in Figure 3.
Figure 5: Analysis of linear classifier trained on a binary MNIST task (5 vs. 7). (Details in AppendixTable 5.) (a) Visualization of network weights per input feature. (b) Comparison of feature-labelcorrelation to the weight assigned to the feature by each network. Adversarially trained networksput weights only on a small number of strongly-correlated or “robust” features. (c) Performance of amodel trained using standard training only on the most robust features. Specifically, we sort featuresbased on decreasing correlation with the label and train using only the most correlated ones. Beyonda certain threshold, we observe that as more non-robust or (weakly correlated) features are availableto the model, the standard accuracy increases at the cost of robustness.
Figure 6: Comparison of standard accuracies of models trained against an '∞-bounded adversaryas a function of the size of the training dataset. We observe that in the low-data regime, adversarialtraining has an effect similar to data augmentation and helps with generalization in certain cases(particularly on MNIST). However, in the limit of sufficient training data, we see that the standardaccuracy of robust models is less than that of the standard model (εtrain = 0), which supports thetheoretical analysis in Section 2.1.
Figure 7: Standard test accuracy of adversarially trained classifiers. The adversary used duringtraining is constrained within some `p-ball of radius εtrain (details in Appendix A). We observe aconsistent decrease in accuracy as the strength of the adversary increases.
Figure 8: Large-ε adversarial examples, bounded in '∞-norm, similar to those in Figure 3.
Figure 9: Large-ε adversarial examples, bounded in '2-norm, similar to those in Figure 3.
Figure 10: Visualization of the gradient of the loss with respect to input features (pixels) for standardand adversarially trained networks for 10 randomly chosen samples, similar to those in Figure 2.
