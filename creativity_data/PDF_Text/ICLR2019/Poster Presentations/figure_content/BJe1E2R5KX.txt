Figure 1: Comparison between SLBO (ours), SLBO with squared `2 model loss (SLBO-MSE),vanilla model-based TRPO (MB-TRPO), model-free TRPO (MF-TRPO), and Soft Actor-Critic(SAC). We average the results over 10 different random seeds, where the solid lines indicate the meanand shaded areas indicate one standard deviation. The dotted reference lines are the total rewards ofMF-TRPO after 8 million steps.
Figure 2: Ablation study on multi-step model training. All the experiments are average over 10random seeds. The x-axis shows the total amount of real samples from the environment. The y-axisshows the averaged return from execution of our learned policy. The solid line is the mean of thetotal rewards from each seed. The shaded area is one-standard deviation.
Figure 3: Ablation study on entropy regularization. λ is the coefficient of entropy regularization inthe TRPO’s objective. All the experiments are averaged over 10 random seeds. The x-axis showsthe total amount of real samples from the environment. The y-axis shows the averaged return fromexecution of our learned policy. The solid line is the mean of the total rewards from each seed. Theshaded area is one-standard deviation.
Figure 4: Comparison among SLBO (ours), SLBO with squared `2 model loss (SLBO-MSE), vanillamodel-based TRPO (MB-TRPO), model-free TRPO (MF-TRPO), and Soft Actor-Critic (SAC) withmore samples than in Figure 1. SLBO, SAC, MF-TRPO are trained with 4 million real samples. Weaverage the results over 10 different random seeds, where the solid lines indicate the mean and shadedareas indicate one standard deviation. The dotted reference lines are the total rewards of MF-TRPOafter 8 million steps.
