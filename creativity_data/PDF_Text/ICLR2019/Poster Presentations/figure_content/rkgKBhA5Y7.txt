Figure 1: (a): The evolution of the gradient norm for the consistency regularization term (Cons) andthe cross-entropy term (CE) in the Π, MT, and standard supervised (CE only) models during training.
Figure 2: (a): Illustration of a convex and non-convex function and Jensen,s inequality. (b): Scatterplot of the decrease in error Cavg for weight averaging versus distance. (c): Scatter plot of the decreasein error Cens for prediction ensembling versus diversity. (d): Train error surface (orange) and Testerror surface (blue). The SGD solutions (red dots) around a locally flat minimum are far apart dueto the flatness of the train surface (see Figure 1b) which leads to large error reduction of the SWAsolution (blue dot).
Figure 3: Left: Cyclical cosine learning rate schedule and SWA and fast-SWA averaging strategies.
Figure 4: Prediction errors of Π and MT models with and without fast-SWA. (a) CIFAR-10 withCNN (b) CIFAR-100 with CNN. 50k+ and 50k+* CorresPondto 50k+500k and 50k+237k* settings(c) CIFAR-10 with ResNet + Shake-Shake using the short schedule (d) CIFAR-10 with ResNet +Shake-Shake using the long schedule.
Figure 5: Prediction errors of base models and their weight averages (fast-SWA and SWA) for CNNon (left) CIFAR-10 with 4k labels, (middle) CIFAR-100 with 10k labels, and (right) CIFAR-10050k labels and extra 500k unlabeled data from Tiny Images (Torralba et al., 2008).
Figure 6: All plots are a obtained using the 13-layer CNN on CIFAR-10 with 4k labeled and 46kunlabeled data points unless specified otherwise. Left: Test error as a function of distance alongrandom rays for the Π model with 0, 4k, 10k, 20k or 46k unlabeled data points, and standard fullysupervised training which uses only the cross entropy loss. All methods use 4k labeled examples.
Figure 7: (Left): The evolution of the gradient covariance trace in the Π, MT, and supervised modelsduring training. (Middle): Scatter plot of the decrease in error Cavg for weight averaging versusdiversity. (Right): Scatter plot of the distance between pairs of weights versus diversity in theirpredictions.
Figure 8: Test errors as a function of training epoch for baseline models, SWA and fast-SWA onCIFAR-10 trained using 1k, 2k, 4k, and 10k labels for (top) the MT model (bottom) the Π model.
Figure 9: Test errors versus training epoch for baseline models, SWA and fast-SWA on CIFAR-100trained using 10k, 50k, 50k+500k, and 50k+237k* labels for (top) the MT model (bottom) the Πmodel. All models are trained using the 13-layer CNN.
Figure 10: The plots are generated using the MT model with CNN trained on CIFAR-10. Werandomly select 5k of the 50k train images as a validation set. The remaining 45k images are splittedinto 4k labeled and 41k unlabeled data points. (a) Validation accuracy as a function of training epochfor different cycle lengths c (b) fast-SWA with constant learning rate. The “learning rate epoch”corresponds to the epoch in the unmodified cosine annealing schedule (Figure 3, left) at which thelearning rate is evaluated. We use this fixed learning rate for all epochs i ≥ `.
Figure 11: (left) Comparison of different averaging methods. The y axis corresponds to the increasederror with respect to the MT model with fast-SWA solution (which has y =0). All numbers aretaken from epoch 180. (right) The effects of using SWA as a teacher. w-t model corresponds to theperformance of a model with weight w using a model with a teacher being t.
