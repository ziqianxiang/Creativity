Figure 1: A snapshot of the 54 environments investigated in the paper. We show that agents are able to makeprogress using no extrinsic reward, or end-of-episode signal, and only using curiosity. Video results, code andmodels at https://doubleblindsupplementary.github.io/large-curiosity/.
Figure 2: A comparison of feature learning methods on 8 selected Atari games and the Super Mario Bros.
Figure 3: (a) Left: A comparison of the RF method on Mario with different batch sizes. Results are withoutusing extrinsic reward. (b) Center: Number of ball bounces in the Juggling (Roboschool) environment. (c)Right: Mean episode length in the multiplayer Pong environment. The discontinuous jump on the graph corre-sponds to the agent reaching a limit of the environment - after a certain number of steps in the environment theAtari Pong emulator starts randomly cycling through background colors and becomes unresponsive to agentâ€™sactions(b) Juggling (Roboschool)(c) Two-player Pongtraining using 128 and 1024 parallel environment threads in Figure 3(a). As apparent from thegraph, training with large batch-size using 1024 parallel environment threads performs much better.
Figure 4: Mario generalization experiments. On theleft we show transfer results from Level 1-1 to Level 1-2, and on the right we show transfer results from Level1-1 to Level 1-3. Underneath each plot is a map of thesource and target environments. All agents are trainedwithout extrinsic reward.
Figure 5: Mean extrinsic reward inthe Unity environment while trainingwith terminal extrinsic + curiosity re-ward. Note that the curve for extrinsicreward only training is constantly zero.
Figure 6: We add a noisy TV to the unity environ-ment in Section 3.3. We compare IDF and RF withand without the TV.
Figure 7: (a) Left: Best extrinsic returns on eight Atari games and Mario. (c) Right: Mean episode lengths oneight Atari games and Mario.
Figure 8: Pure curiosity-driven exploration (no extrinsic reward, or end-of-episode signal) on 48 Atari games.
Figure 9: Best extrinsic returns on the Mario scaling experiments. We observe that larger batches allow theagent to explore more effectively, reaching the same performance in less parameter updates, and also achievingbetter ultimate scores.
