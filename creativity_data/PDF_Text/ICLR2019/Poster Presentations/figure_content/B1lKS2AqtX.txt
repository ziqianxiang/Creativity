Figure 1: Three approaches to integrate 3D-Convs into recurrent networks. Blue arrows indicate datatransition paths with 3D-Convs (for feed-forward features or recurrent hidden states). The diagramsare simplified for illustration, with fewer layers and RNN states than What are actually used in ourexperiments. The classifiers are removed when being trained for future video prediction.
Figure 2: Comparison of (a) the standard memory transition approach in the Spatiotemporal LSTMand (b) the attentive memory transition approach in the Eidetic 3D LSTM. Red arrows indicate theshort-term information flow. Blue arrows are the attentive memory flow, which potentially enablesour model to capture the long-term relations. Cubes denote higher-dimensional hidden states andmemory states. Cylinders denote higher-dimensional gates. Î˜ is the Hadamard product. 0 is thematrix product after reshaping matrices into appropriate 2-dimensional forms.
Figure 3: Video prediction examples on the Moving MNIST dataset.
Figure 4: Comparisons of the generated frames on KTH. (Top) predictions of next 40 frames basedon 10 previous observations. (Bottom) the copy test that requires to reproducing prior inputs.
Figure 5: Prediction results on the TaxiBJ traffic flow dataset. For ease of comparison, We visualizethe differences between the generated heat maps and their corresponding ground truth heat maps.
Figure 6: Early activity recognition results given the first 25% and 50% frames of videos on theSomething-Something validation set. The blue bars indicate making correct classifications and thered bars are incorrect results. The length of the bar denotes the confidence of the result.
