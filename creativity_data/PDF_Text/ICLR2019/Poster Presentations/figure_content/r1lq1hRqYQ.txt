Figure 1: A task where an agent (green triangle)must execute the command “go to the fruit bowl.”This is a simple example where the reward func-tion is easier to specify than the policy.
Figure 2: Our reward function architecture. Our network receives as input a panoramic semanticimage (4 views) and a language command represented as a sequence of one-hot word vectors, andoutputs a scalar reward.
Figure 4: Example first-person RGB (left) andsemantic (right) images from the bedroom in-side the house depicted in Figure 3. We onlyuse the semantic labels as input to our model.
Figure 3: An example task. The green segmentcorresponds to the solution of a NAV task, ”goto the cup”, where the cup is circled in green.
Figure 5: Top row: First-person view of an agent executing the task: “move Vase to living room”.
Figure 7: Learned rewards and a corresponding birds-eye view rollout for the task ”move pan toliving room”.
