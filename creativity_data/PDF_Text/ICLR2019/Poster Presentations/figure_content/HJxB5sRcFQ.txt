Figure 1: Overall architecture of LayoutGAN. The generator takes as input graphic elements withrandomly sampled class probabilities and geometric parameters from Uniform and Gaussian distri-bution respectively. An encoder embeds the input and feeds them into the stacked relation module,which refines the embedded features of each element in a coordinative manner by considering itssemantic and spatial relations with allthe other elements. Finally, a decoder decodes the refined fea-tures back to class probabilities and geometric parameters. The wireframe rendering discriminatorfeeds the generated results to a differentiable wireframe rendering layer which raterizes the inputgraphic elements into 2D wireframe images, upon which a CNN is applied for layout optimization.
Figure 2: Wireframe rendering of different polygons (point, rectangle and triangle). The black gridsrepresent grids of target image. The orange dots/dotted lines represent the graphic element mappedonto the image grid. The blue solid lines represent the rasterized wireframes expressed as differen-tiable functions of graphic elements in terms of both class probilities and geometric parameters.
Figure 3: Results on MNIST digit generation.
Figure 4: Document layout comparison.
Figure 6: Discriminator loss landscapes.
Figure 9: Training progression of tangram graphic design generation (from left to right).
Figure 1: Visualization of document layout samples and their corresponding real document pages.
Figure 2: Document layout generation.
Figure 3: Clipart abstract scene generation.
Figure 4: Tangram graphic design.
Figure 5: Visualization of mobile app layout samples and their corresponding real mobile appscreenshots.
Figure 6: Mobile app layout design.
