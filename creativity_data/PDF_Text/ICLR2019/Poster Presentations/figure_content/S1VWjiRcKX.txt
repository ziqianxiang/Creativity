Figure 2: Trip MDP: [Left] Depiction of MDP. [Right] Optimality gap (Difference between optimalreturn and the return obtained by the different models) at different times in the training process.
Figure 3: Environment.
Figure 4:	Zero-shot generalisation performance, across different models, on a sample of test tasksw0 ∈ M0 after training on M. Shaded areas represent one standard deviation over 10 runs.
Figure 5:	Generalisation performance on sample test tasks w0 ∈ M0 after training on M, withDz = N(w, σ I), for σ = 0.1 and σ = 0.5 (larger coverage of the z space). Average over 3 runs.
Figure 6: Optimal value space as a function of a scalar task description wSuppose now we are in the scenario studied in the paper, where after training on a set of tasksM the agent should generalise to a test task w0 . Specifically, let us consider three points in thisspace M = {wι, w2, w3}- three tasks We are going to consider for learning and approximatingtheir optimal policies {QWι, Qwl, QW1}. Given these three points we are going to fit a parametricfunction that aims to generalise in the space of w. A depiction of this is included in Figure 7(a). NoW,given a new point w0 we can obtain a zero-shot estimate QwO for QW - see Figure 7(b). Due toapproximation error under a very limited number of training points, this estimate will typically notrecover perfectly QW = 0. In the case of UVFA (and other FAS trying to generalise in task space),we are going to get a guess based on optimal value function we have built, and we are going to takedecision based on this estimate Qwo.
Figure 7: UVFA-like generalisation.
Figure 8: GPI generalisation.
Figure 9: [Sample run] Performance of the different methods (in this order, starting with the secondsubplot): UVFA, SF&GPIon the perfect SFs induced by M, USFA with C = random(5) and USFAwith C = {w0 } as compared to the optimal performance one could get in this MDP (first plot). Thesecorrespond to one sample run, where we trained the UVFA and USFA for 1000 episodes. The optimalperformance and the SF&GPIwere computed exactly.
Figure 10: [Sample run] Optimality gap over the whole task space. These correspond to the samesample run as above, where we trained the UVFA and USFA for 1000 episodes. We can now seemore clearly that USFAs manage to recover better policies and optimality across a much greaterportion of the task space. The last two plots correspond to the same USFA just using different choicesof the candidate set C. Something to note here is that by having a more diverse choice in C, we canrecover an optimal policy even in areas of the space where our approximation has not yet optimallygeneralised (like the upper-left corner in the w-space in the figures above).
Figure 11: Zero-shot performance on the diagonal: Optimality gap for M0 = {w0|w10 = w20 , w1 ∈[0, 1]}. These results were averaged over 10 runs.
Figure 12: USFA architectureAs highlighted in Section 4.2, our agent comprises of three main modules:•	Input processing module: computes a state representation f(ht) from observation ot. Thismodule is made up of three convolutional layers (structure identical to the one used in (Mnihet al., 2015)), the output of which then serves as input to a LSTM (256). This LSTM takesas input the previously executed action at-1. The output of the LSTM is passed through anon-linearity f (chosen here to be a ReLu) to produce a vector of 128 units, f (ht).
Figure 13:	Zero-shot performance on the easy evaluation set: Average reward per episode on testtasks not shown in the main paper. This is Comparing a USFA agent trained on the CanoniCal trainingset M = {1000, 0100, 0010, 0001}, with Dz = N(w, 0.1I) and the two UVFA agents: one trainedon-poliCy, one employing off-poliCy.
Figure 14:	Zero-shot performance on harder tasks: Average reward per episode on test tasks notshown in the main paper. This is comparing a USFA agent trained on the canonical training setM = {1000, 0100, 0010, 0001}, with Dz = N(w, 0.1I) and the two UVFA agents: one trainedon-policy, one employing off-policy. (Part 1)20Published as a conference paper at ICLR 2019PJeMQJ əposo,山Task -11000.2	0.4	0.6	0.8	1.0Environment frame le9PJeMQJ əposo,山Task-11010.2	0.4	0.6	0.8	1.0Environment frame le9(a) Task-1100(b) Task -1101Task-11-1025	505 OPJeMQJ əposo,山
Figure 15:	Zero-shot performance on harder tasks: Average reward per epiSode on teSt taSkS notShown in the main paper. ThiS iS Comparing a USFA agent trained on the CanoniCal training SetM = {1000, 0100, 0010, 0001}, with Dz = N(w, 0.1I) and the two UVFA agentS: one trainedon-poliCy, one employing off-poliCy. (Part 2)21Published as a conference paper at ICLR 2019D.2 Canonical basis: USFAs in different training regimes.
Figure 16:	Different DZ - Zero-shot performance on harder tasks: Average reward per episodeon test tasks not shown in the main paper. This is comparing the generalisations of two USFA agenttrained on the canonical training set M = {1000, 0100, 0010, 0001}, with DZ = N(w, 0.1I), andDZ = N(w, 0.5I). (Part 1)22Published as a conference paper at ICLR 2019(a) Task -1100.
Figure 17:	Different DZ - Zero-shot performance on harder tasks: Average reward per episodeon test tasks not shown in the main paper. This is comparing a USFA agent trained on the canonicaltraining set M = {1000, 0100, 0010, 0001}, with DZ = N(w, 0.1I), and DZ = N(w, 0.5I). (Part2)23Published as a conference paper at ICLR 2019D.3 Larger collection of training tasksWe also trained our USFA agent on a larger set of training tasks that include the previous canonicaltasks, as well as four other tasks that contain both positive and negative reward M = {1000, 0100,0010, 0001, 1-100, 01-10, 001-1, -1000}. Thus we expect this agent to generalises better as a resultof its training. A selection of these results and sample performance in training are included in Fig. 18.
Figure 18:	Large M. Learning curves for training task [1000] ∈ M and generalisation performanceon a sample of test tasks w0 ∈ M0 after training on all the tasks M. This is a selection of the hardevaluation tasks. Results are average over 10 training runs.
