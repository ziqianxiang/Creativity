Figure 1: Attention based encoder. Input nodesare embedded and processed by N sequentiallayers, each consisting of a multi-head attention(MHA) and node-wise feed-forward (FF) sub-layer. The graph embedding is computed as themean of node embeddings. Best viewed in color.
Figure 2: Attention based decoder for the TSP problem. The decoder takes as input the graphembedding and node embeddings. At each time step t, the context consist of the graph embeddingand the embeddings of the first and last (previously output) node of the partial tour, where learnedplaceholders are used if t = 1. Nodes that cannot be visited (since they are already visited) aremasked. The example shows how a tour π = (3, 1, 2, 4) is constructed. Best viewed in color.
Figure 3: Held-out validation set optimalitygap as a function of the number of epochs forthe Attention Model (AM) and Pointer Net-work (PN) with different baselines (two dif-ferent seeds).
Figure 4: Illustration of weighted message passing using a dot-attention mechanism. Only compu-tation of messages received by node 1 are shown for clarity. Best viewed in color.
Figure 5: Optimality gap of different methods as a function of problem size n ∈{5, 10, 15, 20, 25, 30, 40, 50, 60, 75, 100, 125}. General baselines are drawn using dashed lineswhile learned algorithms are drawn with a solid line. Algorithms (general and learned) that per-form search or sampling are plotted without connecting lines for clarity. The *, **, *** and ****indicate that values are reported from Bello et al. (2016), Vinyals et al. (2015), Dai et al. (2017) andNowak et al. (2017) respectively. Best viewed in color.
Figure 6: Validation set optimality gap as a function of the number of epochs for different η .
Figure 7: Example greedy solutions for the CVRP (n = 100). Edges from and to depot omittedfor clarity. Legend order/coloring and arcs indicate the order in which the solution was generated.
