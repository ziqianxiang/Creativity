Figure 1: We reconstruct an image x from its tomographic measurements. In moderately ill-posedproblems, conventional methods based on the pseudoinverse and regularized non-negative leastsquares (x ∈ [0, 1]N , N is image dimension) give correct structural information. In fact, totalvariation (TV) approaches give very good results. A neural network (Jin et al. (2016)) can be trainedto directly invert and remove the artifacts (NN). In a severely ill-posed problem on the other hand(explained in Figure 4) with insufficient ground truth training data, neither the classical techniquesnor a neural network recover salient geometric features.
Figure 2: Regularization by Λ random projections: 1) each orthogonal projection is approximated bya convolutional neural network which maps from a non-negative least squares reconstruction of animage to its projection onto a lower dimension subspace of Delaunay triangulations; 2) projectionsare combined to estimate the original image using regularized least squares.
Figure 3: Illustration of the expected kernel κ(u, v) with varying subspace dimension, K, and numberof subspaces, Λ. Reconstruction of a sparse three-pixel image (left) and the cameraman image (right).
Figure 4: Linearized traveltime tomography illustration: On the left we show a sample model, withred crosses indicating 25 sensor locations and dashed blue lines indicating linearized travel paths; onthe right we show a reconstruction from 225 = 300 measurements by non-negative least squares.
Figure 5: a) Reconstructions for different combinations of training and testing input SNR. The outputSNR is indicated for each reconstruction. Our method stands out when the training and testingnoise levels do not match; b) reconstructions with erasures with probability 1, ɪθ and 4.Thereconstructions are obtained from networks which are trained with input SNR of 10 dB. The directnetwork cannot produce a reasonable image in any of the cases.
Figure 6: Reconstructions from networkstrained on different datasets (LSUN, CelebAand Shapes) with 10dB training SNR.
Figure 7: Reconstructions on checkerboards and x-rays with 10dB measurement SNR tested on 10dBtrained networks. Red annotations highlight where the direct net fails to reconstruct correct geometry.
Figure 8: Orthogonal vs. oblique projections. There is no linear operator acting on y or on theorthogonal projection y = Pr(a*)x = Aty that can compute the orthogonal projection into S.
Figure 9: Comparison between perfect orthogonal projection, ProjNet projections and obliqueprojection. The projections of an image, x, (same as Figure 5) are obtained using ProjNet and thelinear oblique projection method. The mean-squared errors (MSE) between the obtained projectionsand the perfect projections are stated. The subspaces used in this figure were used in the ProjNetreconstructions.
Figure 10: We try hard to get the best reconstruction from the linear approach. SNRs are indicatedin the bottom-left of each reconstruction. In the linear approach, coefficients are obtained using thelinear oblique projection method. Once coefficients are obtained, they are non-linearly reconstructedaccording to (2). Both linear approach reconstructions use the box-constraint (BC) mentioned in (2).
Figure 11: a) ProjNet architecture; b) SubNet architecture. In both cases, the input is a non-negativeleast squares reconstruction and the network is trained to reconstruct a projection into one subspace.
Figure 12: Geophysics image patches taken from BP2004 dataset. Our method especially gets correctglobal shapes with better accuracy even when tested on noise levels different from training.
Figure 13: Reconstructions from erasures on x-ray images with erasure probability P = 8.
Figure 15: Examples from the random shapes dataset which is used in Figure 6.
Figure 16: Reconstructions of the original image from 10 individually trained direct inversionnetworks for 10dB noise under the P = 1 erasure corruptions model (described in Figure 5b). 9 outof the 10 reconstructions fail to capture the key structure of the original image.
