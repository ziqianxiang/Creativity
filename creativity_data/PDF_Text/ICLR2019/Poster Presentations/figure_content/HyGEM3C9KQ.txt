Figure 1: Simplified block diagram of DNC’s memory access module with single read head. Yellowboxes denote the inputs from the previous time step, orange boxes are the corresponding outputs tothe next time step. Green boxes are the control inputs from the controller. Blue, rounded boxes aremodules responsible for a specific function. wtw denotes the write address, wtr the read address, Ltthe temporal linkage matrix. Mt is the memory. Arrow ”r” denotes the output of the memory read.
Figure 2: Block diagram of read address generation in DNC with key masking and sharpnessenhancement. Blue parts indicate new components absent in standard DNC. CMP is a cosinesimilarity-based comparator. Memory and key are compared after a novel masking step. Be-fore combining temporal links and content-based address distribution, sharpness enhancement takesplace.
Figure 3: (a) Mean training loss on the associative recall task. The shaded area shows the ±2σ mark(12 seeds/model). Masking improves convergence speed. (b) An example read mask of DNC-M inthe key-value retrieval task. Yellow values indicate parts of the key the network searches for, theblue values indicate parts that need to be retrieved form memory. When the query switches fromW1 to W2 , the mask changes. In the bottom third (in) the input is stored (look-up is not used). Forin middle third (q1) W1 is presented in random order and W2 is retrieved. In the last third (q2) W2is presented in random order and W1 is retrieved.
Figure 4: (a) Input (top), ground truth (middle), and network output (bottom) of DNC on big repeatcopy tasks. DNC fails to solve the task; the output is blurry. The problem is especially apparentstarting from t = 50. (b) De-allocating and sharpness enhancement substantially improves conver-gence speed. The improvement by the masking is marginal, probably because the task uses temporallinks.
Figure 5: (a), (b) Example forward link distribution. Each row is an address distribution acrossall memory cells. Blue cells are not read, yellow cells are read with a large weight. (a) DNC-D:without sharpness enhancement the distributions are blurred, rarely having peaks near 1.0. Theproblem becomes worse over time. 3 repeats are shown. Notice the more intense blocks for t ∈[9, 18], [34, 46] and [54, 62]. (b) Sharpness enhancement (DNC-DS) makes the distribution sharpduring the read, peaking near 1.0. Note that (a) and (b) have identical input data. (c), (d) The πt1distribution for (a) and (b). Columns are the weighting of the backward links, the content basedlook up, and the forward links, respectively. (c) The forward links are barely used without sharpnessenhancement. (d) With sharpness enhancement the forward links are used for every block.
Figure 6: Mean test error of various models during the training. Shadowed area shows ±2σ.
