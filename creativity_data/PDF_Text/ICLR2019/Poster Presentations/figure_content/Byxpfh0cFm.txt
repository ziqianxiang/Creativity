Figure 1: Distribution of influence functionvalues on initial training set for translate aug-mentations. Most values are not influential andcan therefore be augmented with low priority.
Figure 2: Influence distribution on initial train-ing set (x-axis) vs. final training set (y-axis) fortranslate augmentations. Points that are unin-fluential typically remain uninfluential.
Figure 3:	The performance of random policies using influence and loss vs. the baseline (simplerandom sampling). Random sampling based on loss/influence consistently outperforms the baseline.
Figure 4:	The performance of policies when point downweighting is used or augmentation scoresare updated.
Figure 5: Points with highest influence / loss(top) and lowest influence / loss (bottom).
Figure 6: Distribution of log loss values on initial training set for translate augmentations. Thedistributions Seem to have similar shape, but with different scales. Most values are not influentialand can be augmented with low priority.
Figure 7: Distribution of influence values on initial training set for translate augmentations. Thedistributions seem to have similar shape, but with different scales. Most values are not influentialand can be augmented with low priority.
Figure 8: Distribution of influence values on initial training set (x-axis) vs. final training set (y-axis)for translate augmentations. The distributions seem to have similar shape, but with different scales.
Figure 9: The performance of randomized policies using influence.
Figure 10: The performance of deterministic policies using influence.
Figure 11: From top to bottom: high influence, high loss, low influence, and low loss for MNIST.
Figure 12: From top to bottom: high influence, high loss, low influence, and low loss for CIFAR10.
Figure 13: From top to bottom: high influence, high loss, low influence, and low loss for NORB.
Figure 14: The performance of randomized policies using SVM margin.
Figure 15: The performance of randomized policies using standard and clustered sampling.
Figure 16: The performance of CIFAR10 k-DPP policies using bottleneck features or a combinationof influence and bottleneck features in the kernel. Only 250 augmented points were used due to thecomputational expense of samplings a larger amount of points.
Figure 17: The time it takes (in seconds) to perform a single ResNet50v2 epoch with respect totraining set size. The training relationship is linear with low variance.
