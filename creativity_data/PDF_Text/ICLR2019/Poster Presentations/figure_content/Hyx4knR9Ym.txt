Figure 1: Adversarial training performance with and without spectral normalization (SN) for AlexNetfit on CIFAR10. The gain in the final test accuracies for FGM, PGM, and WRM after spectralnormalization are 0.09, 0.11, and 0.04, respectively (see Table 1 in the Appendix). For FGM andPGM, perturbations have `2 magnitude 2.44.
Figure 2: Validation of SN implementation and distribution of the gradient norms using AlexNettrained on CIFAR10.
Figure 3: Effect of SN on distributions of unnormalized (leftmost column) and normalized (otherthree columns) margins for AlexNet fit on CIFAR10. The normalization factor is described by thecapacity norm Φ reported in Theorems 1-4.
Figure 4: Fitting random and true labels on CIFAR10 with AlexNet using adversarial training.
Figure 5: Robustness of AlexNet trained on CIFAR10 to various adversarial attacks.
Figure 6: Test accuracy improvement after SN for various datasets and network architectures.
Figure 7: Adversarial training performance with and without spectral normalization for AlexNet fiton CIFAR10.
Figure 8:	Adversarial training performance with and without spectral normalization for Inception andResNet fit on CIFAR10.
Figure 9:	Adversarial training performance with and without spectral normalization for AlexNet withELU activation functions fit on CIFAR10.
Figure 10: Adversarial training performance with proposed SN versus Miyato et al. (2018)’s SNfor AlexNet fit on CIFAR10 using PGM. The final train and validation accuracies for the proposedmethod are 0.92 and 0.60. The final train and validation accuracies for Miyato et al. (2018)’s are 1.00and 0.55.
Figure 11: Adversarial training performance with proposed SN versus batch normalization, weightdecay, and dropout for AlexNet fit on CIFAR10 using PGM. The dropout rate was 0.8, and theamount of weight decay was 5e-4 for all weights. The leftmost plot is from Figure 1 and comparesfinal performance of no regularization (train accuracy 1.00, validation accuracy 0.48) to that of SN(train accuracy 0.92, validation accuracy 0.60). The final train and validation accuracies for batchnormalization are 1.00 and 0.54; the final train and validation accuracies for weight decay are 0.84and 0.55; and the final train and validation accuracies for dropout are 0.99 and 0.52.
