Figure 1: SPIGAN example inputs and outputs. From left to right: input images from a simulator;adapted images from SPIGAN’s generator network; predictions from SPIGAN’s privileged network(depth layers); semantic segmentation predictions from the target task network.
Figure 2: SPIGAN learning algorithm from unlabeled real-world images xr and the unpaired outputof a simulator (synthetic images xs , their labels ys , e.g. semantic segmentation ground truth, andPrivileged Information PI zs, e.g., depth from the z-buffer) modeled as random variables. Fournetworks are learned jointly: (i) a generator G(Xs) ~ Xr, (ii) a discriminator D between G(Xs)=Xf and Xr, (iii) a perception task network T(Xr) ~ y, which is the main target output of SPIGAN(e.g., a semantic segmentation deep net), and (iv) a privileged network P to support the learning ofT by predicting the simulator’s PI zs .
Figure 3: Loss curves for the task, perceptual,and privileged parts of the learning objectiveduring the training of SYNTHIA-to-Cityscapes.
Figure 4: Early stopping at the iteration whenthe discriminator loss is significantly and con-sistently better than the generator loss (90 here).
Figure 5: Adaptation from SYNTHIA to Cityscapes. (a) Examples of images from the sourcedomain. (b) Source images after the adaptation process w/o Privileged Information. (c) Sourceimages after the adaptation process using SPIGAN.
Figure 6: Adaptation from SYNTHIA to Vistas. (a) Examples of images from the source domain.
Figure 7: Semantic segmentation results on Cityscapes. For a set of real images (a) we show ex-amples of predicted semantic segmentation masks. SPIGAN predictions (c) are more accurate (i.e.,closer to the ground truth (d)) than those produced without PI during training (b).
Figure 8: Semantic segmentation results on Vistas. For a set of real images (a) we show examplesof predicted semantic segmentation masks. SPIGAN predictions (c) are more accurate (i.e., closerto the ground truth (d)) than those produced without PI during training (b).
