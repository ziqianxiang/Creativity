Figure 1: (a) Probability versus rank of each of the functions (ranked by probability) from a sam-ple of 1010 (blue) or 107 (others) parameters. The labels are different parameter distributions. (b)Probability versus Lempel-Ziv complexity. Probabilities are estimated from a sample of 108 param-eters. Points with a frequency of 10-8 are removed for clarity because these suffer from finite-sizeeffects (see Appendix G). The red line is the simplicity bias bound of Eq.(1) (d) Generalization errorincreases with the Lempel-Ziv complexity of different target functions, when training the networkwith advSGD - See Appendix A.
Figure 2: (a) Probability (using GP approximation) versus critical sample ratio (CSR) of labellingsof 1000 random CIFAR10 inputs, produced by 250 random samples of parameters. The networkis a 4 layer CNN. (b) Comparing the empirical frequency of different labelings for a sample of mMNIST images, obtained from randomly sampling parameters from a neural neural network, versusthat obtained by sampling from the corresponding GP. The network has 2 fully connected hiddenlayers of 784 ReLU neurons each. σw = σb = 1.0. Sample size is 107, and only points obtained inboth samples are displayed. These figures also demonstrate significant (simplicity) bias for P (f).
Figure 3: Mean generalization error and corresponding PAC-Bayes bound versus percentage oflabel corruption, for three datasets and a training set of size 10000. Training set error is 0 in allexperiments. Note that the bounds follow the same trends as the true generalization errors. Theempirical errors are averaged over 8 initializations. The Gaussian process parameters were σw =1.0, σb = 1.0 for the CNN and σw = 10.0, σb = 10.0 for the FC. Insets show the marginallikelihood of the data as computed by the Gaussian process approximation (in natural log scale),versus the label corruption.
Figure 4: Average probability of finding a function for a variant of SGD, versus average probabilityof finding a function when using the Gaussian process approximation. This is done for a randomlychosen, but fixed, target Boolean function of Lempel-Ziv complexity 84.0. See Appendix D fordetails. The Gaussian process parameters are σw = 10.0, and σb = 10.0. For advSGD, we haveremoved functions which only appared once in the whole sample, to avoid finite-size effects. In thecaptions, ρ refers to the 2-tailed Pearson correlation coefficient, and p to its corresponding p value.
Figure 5: Comparing the empirical frequency of different labellings for a sample of 10 MNISTimages obtained from randomly sampling parameters from a neural neural network, versus the ap-proximate marginal likelihood from the corresponding Gaussian process. Orange dots correspondto the expectation-propagation approximation, and blue dots to the Laplace approximation. The net-work has 2 fully connected hidden layers of 784 ReLU neurons each. The weight and bias variancesare 1.0.
Figure 6: Dependence of PAC-Bayes bound on variance hyperparameters. We plot the value of thePAC-Bayes bound versus the standard deviation parameter for the weights and biases, for a sampleof 10000 instances from different datasets, and a two-layer fully connected network (with the layersof the same size as input). The fixed parameter is put to 1.0 in all cases.
Figure 7: Dependence of PAC-Bayes bound on variance hyperparameters. PAC-Bayes bound ver-sus the standard deviation parameter for the weights and biases, for a sample of 10000 instancesfrom different datasets, and a four-layer convolutional network. The fixed parameter is put to 1.0 inall cases.
Figure 8: We plot hP(f)i for a variant of SGD, versus hP(f)i computed using the ABI methoddescribed above (which approximates uniform sampling on parameter space on the zero-error re-gion). The Gaussian process parameters (see Section 5.1 in main text) are σw = 1.0, and σb = 1.0.
Figure 9: Scatter matrix showing the correlation between the different complexity measures usedin this paper On the diagonal, a histogram (in grey) of frequency versus complexity is depicted. Thefunctions are from the sample of 108 parameters for the (7, 40, 40, 1) network.
Figure 10: Probability versus different measures of complexity (see main text for Lempel-Ziv),estimated from a sample of 108 parameters, for a network of shape (7, 40, 40, 1). Points with afrequency of 10-8 are removed for clarity because these suffer from finite-size effects (see Ap-pendix G). The measures of complexity are described in Appendix F.
Figure 11: Histogram of functions in the probability versus Lempel-Ziv complexity plane, weightedaccording to their probability. Probabilities are estimated from a sample of 108 parameters, for anetwork of shape (7, 40, 40, 1)20	40	60LZ complexityFigure 12: Probability versus LZ complexity for network of shape (7, 40, 40, 1) and varying sam-Pling distributions. Samples are of size 107. (a) Weights are sampled from a Gaussian with variance1 / √n where n is the input dimension of each layer. (b) Weights are sampled from a Gaussian withvariance 2.526Published as a conference paper at ICLR 2019F.4 Effects of target function complexity on learning for differentCOMPLEXITY MEASURESHere we show the effect of the complexity of the target function on learning, as well as other com-plementary results. Here we compare neural network learning to random guessing, which we call“unbiased learner”. Note that both probably have the same hypothesis class as we tested that theneural network used here can fit random functions.
Figure 12: Probability versus LZ complexity for network of shape (7, 40, 40, 1) and varying sam-Pling distributions. Samples are of size 107. (a) Weights are sampled from a Gaussian with variance1 / √n where n is the input dimension of each layer. (b) Weights are sampled from a Gaussian withvariance 2.526Published as a conference paper at ICLR 2019F.4 Effects of target function complexity on learning for differentCOMPLEXITY MEASURESHere we show the effect of the complexity of the target function on learning, as well as other com-plementary results. Here we compare neural network learning to random guessing, which we call“unbiased learner”. Note that both probably have the same hypothesis class as we tested that theneural network used here can fit random functions.
Figure 13: Different learning metrics versus the LZ complexity of the target function, when learningwith a network of shape (7, 40, 40, 1). Dots represent the means, while the shaded envelope corre-sponds to piecewise linear interpolation of the standard deviation, over 500 random initializationsand training sets.
Figure 14: Different learning metrics versus the generalization complexity of the target function,when learning with a network of shape (7, 40, 40, 1). Dots represent the means, while the shadedenvelope corresponds to piecewise linear interpolation of the standard deviation, over 500 randominitializations and training sets.
Figure 15: Different learning metrics versus the Boolean complexity of the target function, whenlearning with a network of shape (7, 40, 40, 1). Dots represent the means, while the shaded envelopecorresponds to piecewise linear interpolation of the standard deviation, over 500 random initializa-tions and training sets.
Figure 16: Different learning metrics versus the entropy of the target function, when learning with anetwork of shape (7, 40, 40, 1). Dots represent the means, while the shaded envelope corresponds topiecewise linear interpolation of the standard deviation, over 500 random initializations and trainingsets.
Figure 17: Generalization error of learned function versus the complexity of the target function fortarget functions with fixed entropy 1.0, for a network of shape (7, 20, 20, 1). Complexity measuresare (a) LZ and (b) generalisation complexity. Here the training set size was of size 64, but sampledwith replacement, and the generalization error is over the whole input space. Note that despite thefixed entropy there is still variation in generalization error, which correlates with the complexity ofthe function. These figures demonstrate that entropy is a less accurate complexity measure than LZor generalisation complexity, for predicting generalization performance.
Figure 18: Probability (calculated from frequency) versus Lempel-Ziv complexity for a neuralnetwork of shape (7, 40, 40, 1), and sample sizes N = 106, 107, 108. The lowest frequency functionsfor a given sample size can be seen to suffer from finite-size effects, causing them to have a higherfrequency than their true probability.
Figure 19: Probability versus LZ complexity for networks with different number of layers. Samplesare of size 106 (a) & (b) A perceptron with 7 input neurons (complexity is capped at 80 in (a) toaid comparison with the other figures). (c) & (d) A network with 1 hidden layer of 40 neurons (e)& (f) A network with 2 hidden layer of 40 neurons (g) & (h) A network with 5 hidden layers of 40neurons each. (i) & (j) A network with 8 hidden layers of 40 neurons each(d) 1 hidden layer(f) 2 hidden layers(h) 5 hidden layers(j) 8 hidden layers33Published as a conference paper at ICLR 2019J	Other related workThe topic of generalization in neural networks has been extensively studied both in theory and exper-iment, and the literature is vast. Theoretical approaches to generalization include classical notionslike VC dimension Baum & Haussler (1989); Harvey et al. (2017) and Rademacher complexity Sunet al. (2016), but also more modern concepts such as stability Hardt et al. (2016), robustness Xu &Mannor (2012), compression Arora et al. (2018) as well as studies on the relation between general-ization and properties of stochastic gradient descent (SGD) algorithms Zhang et al. (2017b); Soudryet al. (2017); Advani & Saxe (2017).
