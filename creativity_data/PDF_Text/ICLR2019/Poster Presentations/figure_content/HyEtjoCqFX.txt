Figure 1: Grid world experiments. Left column: The top shows a corridor grid world with 3 × 20cells where the goal is on the right. The bottom shows an 8 × 8 grid world where an importantaction (left) has to be made exactly once to arrive at the goal. Middle column: Evaluation of Q-learning (QL), SQL with standard uniform exploration and with marginal exploration (SQL_m), andMIRL. We clearly see that MIRL outperforms the baselines on the corridor, and is comparable to thebaselines on the 8 × 8 world. Right column: We see that MIRL is able to identify the correct action(go right) faster than the baselines in the corrider (top). The bottom reports how having infrequentbut important actions does not affect the performance of MIRL.
Figure 2: Left panel: Median normalized score across 19 Atari games. Comparison between ourmethod mutual information RL (MIRL), SQL and DQN, demonstrating MIRL’s superior perfor-mance. Right panels: Top figures show the raw score for 2 example games reporting MIRL’ssuperior performance on RoadRunner and Seaquest. The bottom plots show the evolution of the es-timated prior over actions. For RoadRunner the prior converges to stable values during training. InSeaquest, the algorithm seems not to have converged yet after 50 million environment steps whichis why the prior probabilities have not converged yet either (however, the formation of separatetrajectory clusters towards the end of training indicates the ongoing process of convergence). SeeAppendix for details and plots for all environments. The curves are smoothed with an exponentialmoving average with effective window size of 106 environment steps.
Figure 3: Normalized score for eight gamescomparing MIRL against standard SQL anda modified version of SQL that exploreswith the marginal distribution over actions(SQL_m). The exploration method slightlyimproves SQL but not sufficiently enough toachieve MIRL’s performance. See individualplots and games in the Appendix.
Figure 4:	Prior Evolution for all games. We can see that MIRL’s prior has fully converged for somegames whereas for other games it is still about to converge.
Figure 5:	Scores for all games on the evaluation snapshots.
Figure 6:	Comparison between standard SQL (SQL_u), SQL with marginal exploration (SQL_m),and MIRL.
Figure 7: Beta evolution over time.
Figure 8: Evolution of β × maxQ over time while training. Specifically, for an environment step i,we compute the βi maxa Qθi (si, a), where βi is the current β-value, Qθi the current approximationof Q and si is the state at the step i.
