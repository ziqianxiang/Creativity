Figure 1: Best-response architecture for an L2-Jacobian regularized two-layer linear network.
Figure 2: The effect of the sampled neighborhood. Left: If the sampled neighborhood is too small (e.g., apoint mass) the approximation learned will only match the exact best-response at the current hyperparameter,with no guarantee that its gradient matches that of the best-response. Middle: If the sampled neighborhood isnot too small or too wide, the gradient of the approximation will match that of the best-response. Right: If thesampled neighborhood is too wide, the approximation will be insufficiently flexible to model the best-response,and again the gradients will not match.
Figure 4: (a) A comparison of the best validation perplexity achieved on PTB over time, by grid search,random search, Bayesian optimization, and STNs. STNs achieve better (lower) validation perplexity in lesstime than the other methods. (b) The hyperparameter schedule found by the STN for each type of dropout. (c)The hyperparameter schedule found by the STN for the coefficients of activation regularization and temporalactivation regularization.
Figure 6: The hyperparameter schedule prescribed by the STN while training for image classification. Thedropouts are indexed by the convolutional layer they are applied to. FC dropout is for the fully-connectedlayers.
Figure 5: A comparison of the best validationloss achieved on CIFAR-10 over time, by gridsearch, random search, Bayesian optimization,and STNs. STNs outperform other methods formany computational budgets.
Figure 7:	Validation performance of a baseline LSTM given different settings of input and outputdropout, at various epochs during training. (a), (b), and (c) show the validation performance on PTB givendifferent hyperparameter settings, at epochs 1, 10, and 25, respectively. Darker colors represent lower (better)validation perplexity.
Figure 8:	Grid search-derived schedule for output dropout.
Figure 9:	Comparison of output dropout schedules. (a) Gaussian-perturbed output dropout rates around thebest value found by grid search, 0.68; (b) sinusoid-perturbed output dropout rates with amplitude 0.1 and aperiod of 1200 mini-batches; (c) the output dropout schedule found by the ST-LSTM.
Figure 10:	The effect of using a different number of train/val steps. For the CNN, we include BayesianOptimization and the reported STN parameters for comparison. During these experiments we found scheduleswhich achieve better final loss with CNNs.
Figure 11:	The effect of using different perturbation scales. For the CNN, we include Bayesian Optimizationand the reported STN parameters for comparison. For (a), the perturbation scales are fixed.
