Figure 1: (Best seen as PDF) Density plots (top) and box plots (bottom) of normalized margin of threeconvolutional networks trained with cross-entropy loss on CIFAR-10 with varying test accuracy: left:55.2%, middle: 70.6%, right: 85.1%. The left network was trained with 20% corrupted labels. Trainaccuracy of all above networks are close to 100%, and training losses close to zero. The densities andbox plots are computed on the training set. Normalized margin distributions are strongly correlatedwith test accuracy (moving to the right as accuracy increases). This motivates our use of normalizedmargins at all layers. The (Tukey) box plots show the median and other order statistics (see section3.2 for details), and motivates their use as features to summarize the distributions.
Figure 2: (Best seen as PDF) Regression models to predict generalization gap. Left: regression modelfit in log space for the full 20-dimensional feature space (R2 = 0.94); Middle: fit for a subset of only4 features, 2 each from 2 of the hidden layers (RR = 0.89); Right: fit for features extracted from thenormalized margin distribution as used in Bartlett et al. (2017) (R2 = 0.72).
Figure 3: (Best seen as PDF) Left: Regression model fit in log space for the full 20-dimensionalfeature space for 216 residual networks (R = 0.87) on QFAR-10; Middle: Log density plot ofnormalized margins of a particular residual network that achieves 91.7% test accuracy without dataaugmentation; Right: Log density plot of normalized margins of a CNN that achieves 87.2% withdata augmentation. We see that the resnet achieves larger margins, especially at the hidden layers,and this is reflected in the higher test accuracy.
Figure 4: (Best seen as PDF) Left: Regression model fit in log space for the full 20-dimensionalfeature space for 300 residual networks (R2 = 0.97) on CIFAR-100; Middle: density plot ofnormalized margins of a particular residual network trained on CIFAR-100 that achieves 44% testaccuracy; Right: Density plot of normalized margins of a residual network trained on CIFAR-10 thatachieves 61%.
Figure 5: Residual plots for all explanatory variables, row: h0, h1, h2, h3, column: lower fence, Q1,Q2, Q3, upper fence. lower fence is clipped because distance cannot be smaller than 0. The residualis fairly evenly distributed around 0.
Figure 6:	Residual plots for all explanatory variables, row: h0, h1, h2, h3, column: lower fence, Q1,Q2, Q3, upper fence. lower fence is clipped because distance cannot be smaller than 0. The residualis less evenly distributed as are in other two settings; this fact is well reflected in the cluster along theX axis and in the RR2; We speculate that this is due to not having diverse enough generalization gap inthe models trained to cover the entire space of the “model” unlike in the other two settings.
Figure 7:	Residual plots for all explanatory variables, row: h0, h1, h2, h3, column: lower fence, Q1,Q2, Q3, upper fence. lower fence is clipped because distance cannot be smaller than 0. The residualis fairly evenly distributed around 0. There is one outlier in this experimental setting as shown in theplots.
Figure 8: Scatter Plots9.2	Cross Dataset ComparisonWe perform regression analysis with ResNet32 on both CIFAR-10 and CIFAR-100. The resultingR2 = 0.96 and the k-fold R2 = 0.95. This suggests that the same coefficient works generally wellacross dataset of the same architecture.
Figure 9: Scatter Plots18Published as a conference paper at ICLR 20199.3	Cross EverythingWe join all our experiment data and the resulting The resulting R2 = 0.93 and the k-fold R2 = 0.93.
Figure 10: Scatter Plots9.4	Implications on Generalization BoundsWe believe that the method developed here can be used in complementary with existing generalizationbound; more sophisticated engineering of the predictor may be used to actually verify what kindof function the generalization bound should look like up to constant factor or exponents; it may behelpful for developing generalization bound tighter than the existing ones.
