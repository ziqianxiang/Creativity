Figure 1:Visualization of the trajectories of three ran-dom initializations of a network throughfunction space, left, and parameter space,right. The network is a convolutional net-work trained on a 5,000 image subset ofCIFAR-10. At each epoch, we computethe L2 and `2 distances between all pre-vious epochs, forming two distance matri-ces, and then recompute the 2D embed-ding from these matrices using multidimen-sional scaling. Each point on the plots rep-resents the network at a new epoch of train-ing.The black arrows represent the directionof movement.
Figure 2: Parameter distances is sometimes, but not always, representative of function distances.
Figure 3: The variance of the the L2 estimator is small enough that it can be reasonably estimatedfrom a few hundred examples. In panels A and D, we reproduced L2 distances seen in the panelsof Fig. 2. As we increase the number of validation examples these distances are computed over,the estimations become more accurate. Panels B and E show the 95% confidence bounds for theestimation; on 95% of batches, the value will lie bewteen these bounds. These bounds can be obtainedfrom the standard deviation of the L2 distance on single examples. In panel C we show that thestandard deviation scales linearly with the L2 distance when measured between updates, meaningthat a fixed batch size will often give similar percentage errors. This is not true for the distance frominitialization, in panel F; early optimization has higher variance relative to magnitude, meaning thatmore examples are needed for the same uncertainty. In the Appendix, we also display the convergenceof the L2 distance estimator between epochs.
Figure 4: Regularizing the L2 distance fromold tasks (calculated over a working mem-ory cache of size 1024) can successfully pre-vent catastrophic forgetting. Here we dis-play the test performance on the first task as7 subsequent tasks are learned. Our methodoutperforms simply retraining on the samecache (ADAM+retrain), which potentiallyoverfits to the cache. Also displayed areADAM without modifications, EWC, andSI.
Figure 5: Results of aSqueezenet v1.1 trained onCIFAR10. The learning rateis decreased by a factor of10 at epoch 150. For the trainerror we overlay the runningaverage of each trace for clar-ity.
Figure 6: Results of a single-layer LSTM with 128 hid-den units trained on the se-quential MNIST task withpermuted pixels. Shown arethe traces for SGD and Adam(both with learning rate 0.01).
