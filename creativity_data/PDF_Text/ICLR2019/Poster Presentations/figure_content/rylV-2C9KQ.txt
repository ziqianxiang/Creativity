Figure 1: The deep decoder (depicted on the right) enables concise image representations, on-par with state-of-the-art wavelet based compression. The crosses on the left depict the PSNRs for100 randomly chosen ImageNet-images represented with few wavelet coefficients and with a deepdecoder with an equal number of parameters. A cross above the red line means the correspondingimage has a smaller representation error when represented with the deep decoder. The deep de-coder is particularly simple, as each layer has the same structure, consisting of a pixel-wise linearcombination of channels, upsampling, ReLU nonlinearities, and channelwise normalization (CN).
Figure 2: An application of the deep decoder for denoising the astronaut test image. The deepdecoder has performance on-par with state of the art untrained denoising methods, such as the DIPmethod (Ulyanov et al., 2018) and the BM3D algorithm (Dabov et al., 2007).
Figure 3: An application of the deep decoder for recovering an inpainted image. For this example,the deep decoder and the deep image perform almost equally well.
Figure 4: Denoising with the deep decoder and the deep image prior. The first two panels shows theMSE of the output of the DD or DIP for a noisy or noiseless image relative to the noiseless image.
Figure 5: The blue curves show a one-dimensional piecewise smooth signal, and the red crossesshow estimates of this signal by a one-dimensional deep decoder with either linear or convex up-sampling. We see that linear upsampling acts as an indirect signal prior that promotes piecewisesmoothness.
Figure 6: The left panel shows an image reconstruction after training a deep decoder on the MRIphantom image (PSNR is 51dB). The right panel shows how the deep decoder builds up an imagestarting from a random input. From top to bottom are the input to the network and the activationmaps (i.e., relu(BiCi)) for eight out of the 64 channels in layers one to six.
Figure 7: Sensitivity to parameter perturbations of the weights in each layer, and images generatedby perturbing the weights in different layers, and keeping the weights in the other layers constant.
Figure 8: Distribution of the weights for fitting the test image Barbara along with a Gaussian fit:The distribution of the weighs is approximately Gaussian.
