Figure 1: A typical three-stage network pruningpipeline.
Figure 2: Difference between predefined and auto-matically discovered target architectures, in channelpruning as an example. The pruning ratio X is user-specified, while a,b,c,d are determined by the prun-ing algorithm. Unstructured sparse pruning can alsobe viewed as automatic.
Figure 3: Pruned architectures obtained by different approaches, all trained from scratch, averaged over 5runs. Architectures obtained by automatic pruning methods (Left: Network Slimming (Liu et al., 2017), Right:Unstructured pruning (Han et al., 2015)) have better parameter efficiency than uniformly pruning channels orsparsifying weights in the whole network.
Figure 4: The average sparsity pattern of all3Ã—3 convolutional kernels in certain layerstages in a unstructured pruned VGG-16.
Figure 5: Pruned architectures obtained by different approaches, all trained from scratch, averaged over 5runs. Left: Results for PreResNet-164 pruned on CIFAR-10 by Network Slimming (Liu et al., 2017). Middleand Right: Results for PreResNet-110 and DenseNet-40 pruned on CIFAR-100 by unstructured pruning (Hanet al., 2015).
Figure 6: Pruned architectures obtained by different approaches, all trained from scratch, averaged over 5 runs.
Figure 7: Comparisons with the Lottery Ticket Hypothesis (Frankle & Carbin, 2019) for iterative/one-shotunstructured pruning (Han et al., 2015) with two initial learning rates 0.1 and 0.01, on CIFAR-10 dataset. Eachpoint is averaged over 5 runs. Using the winning ticket as initialization only brings improvement when thelearning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely usedlarge learning rate (0.1).
Figure 8: Weight distribution of convolutional layers for different pruning methods. We use VGG-16 andCiFAR-10 for this visualization. We compare the weight distribution of unpruned models, fine-tuned mod-els and scratch-trained models. Top: Results for Network slimming (Liu et al., 2017). Bottom: Results forunstructured pruning (Han et al., 2015).
