Figure 1: Overview: Model Comparison. We show the graphical representations of (a) traditionallatent variable models (VAE, ladder VAE) and (b) the proposed graph VAE. Solid lines denotegeneration, dashed lines denote inference, and the dotted area indicates the latent space governedby variational parameters φ and generative parameters θ. Both VAE and ladder VAE use a fixedgraph structure with limited expressiveness (VAE: independent; ladder VAE: chain-structured). Incontrast, graph VAE jointly optimizes a distribution over latent structures c and model parameters(φ,θ), allowing test-time sampling of a flexible, data-driven latent structure.
Figure 2: Local Distributions. We illustrate the parametrization of a local variable Zn in our struc-tured representation. The local prior (Eq. (6)) is defined in terms of a top-down process (in black)predicting the node,s parameters ψn from a gate-modulated sample of Zn,s parents Zpa(n). The localapproximate posterior (Eq. (7)) additionally performs a precision-weighted fusion of these parame-ters with the result of a bottom-up process using a node-specific MLP to predict input-conditionedparameters ψn, from a generic encoding of x.
Figure 3: Structure Learning. In (a), we show trainingof the Bernoulli parameters μi,j∙ governing the distributionover graph structures in architecture space. All edges arecolor-coded and can be located in (b), where we show arandom sample from the resulting steady-state distributionwith the same color scheme.
Figure 4: Ablation Study on MNIST.
Figure 5: Comparison of Test-time Log-Likelihoods on MNIST. Models are trained 5 times indi-vidually, and test-time performances are evaluated every 300 epochs, with mean log-likelihood andstandard deviation indicated by the colored bars.
Figure 6: TSNE-Visualization of Latent Embeddings on MNIST. We visualize embeddings of (a)Graph VAE, (b) VAE, and (c) the data itself.
