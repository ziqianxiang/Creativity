Figure 1: Two-Timescale Network architecturetation for the current state. Another alternative isthe MSBE. The gradient of the nonlinear MSBEis not as complex as the gradient of the nonlinear MSPBE, because it does not involve the gradient ofa projection. However, it suffers from the double sampling problem: sampling the gradient requirestwo independent samples. For these reasons, we explore the MSTDE as the simplest surrogate lossinvolving the value function.
Figure 2: TTN comparison to other nonlinear value function approximation algorithms. For the TTN, MSTDEis used as the surrogate loss for the slow part of the network (feature learning) and LSTD is used for the fast part.
Figure 3: Comparison of MSPBE and MSTDE.
Figure 4: a) & b) Linear methods on Puddle World and Catcher. c) Step size sensitivity in Catcher.
Figure 5: a), b) & c) Comparison of surrogate losses on Puck World, Acrobot and CartpoleControl Although the focus of this work is policy evaluation, we also provide some preliminaryresults for the control setting. For control, we include some standard additions to competitor learningalgorithms to enable learning with neural networks. The DQN algorithm (Mnih et al., 2015) utilizestwo main tricks to stabilize training: experience replay—storing past transitions and replaying themmultiple times—and a target network—which keeps the value function in the Q-learning targets fixed,updating the target network infrequently (e.g., every k = 10, 000 steps).
Figure 6: a) Comparison of returns obtained by each algorithm on a) non-image Catcher and b) image Catcher.
Figure 7: TTN modelTo analyze the long-run behaviour of our algorithm, we employ the ODE based analysis (Borkar,2008; Kushner and Yin, 2003; Ljung, 1977) of the stochastic recursive algorithms. Here, we considera deterministic ordinary differential equation (ODE) whose asymptotic flow is equivalent to thelong-run behaviour of the stochastic recursion. Then we analyze the qualitative behaviour of thesolutions of the ODE to determine the asymptotically stable sets. The ODE-based analysis is elegantand conclusive and it further guarantees that the limit points of the stochastic recursion will almostsurely belong to the compact connected internally chain transitive invariant set of the equivalentODE. since the algorithm follows a multi-timescale stochastic approximation framework, we willalso resort to the more generalized multi-timescale differential inclusion based analysis proposed in(Borkar, 1997; Ramaswamy and Bhatnagar, 2016).
Figure 8: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for thefast part of the TTN.
Figure 10: a) Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 9: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison ofleast-squares methods(b)22Published as a conference paper at ICLR 2019C.2Figure 11: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for thefast part of the TTN.
Figure 11: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for thefast part of the TTN.
Figure 12: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison ofleast-squares methods(a)	(b)Figure 13: a) Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 13: a) Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 14: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for thefast part of the TTN.
Figure 16: a) Comparison of MPSBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 15: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison ofleast-squares methods(b)24Published as a conference paper at ICLR 2019C.4 CartpoleIn the classic Cartpole environment, the agent has to balance a pole on a cart. The state is given byvector of 4 numbers (cart position, cart velocity, pole angle, pole velocity). The two available actionsare applying a force towards the left or the right. Rewards are +1 at every timestep and an episodeterminates once the pole dips below a certain angle or the cart moves too far from the center. We usethe OpenAI gym implementation (Brockman et al., 2016).
Figure 17: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for thefast part of the TTN.
Figure 19: a) Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 18: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison of(b)25Published as a conference paper at ICLR 2019C.5 AcrobotIn the classic Acrobot domain, the agent consisting of two links has to swing up past a certain height.
Figure 20: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for thefast part of the TTN.
Figure 21: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison ofleast-squares methods(a)Figure 22: a) Comparison of Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 22: a) Comparison of Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 23: TTN comparison to other nonlinear value function approximation algorithms. We use LSTD for thefast part of the TTN.
Figure 25: a) Comparison of MSPBE and MSTDE. b) Comparison of surrogate loss functions.
Figure 24: a) Sensitivity plots for λ ∈ {0, 0.3, 0.5, 0.9, 0.99, 1} b) Step size sensitivities. c) Comparison ofleast-squares methods(b)27Published as a conference paper at ICLR 2019C.7 Off-policy CatcherWe run a preliminary experiment to check if TTN can have an advantage in the off-policy setting. Thetarget policy is the same as the one used for other Catcher experiments (described in Appendix D).
Figure 26: Comparison of TTN and nonlinear TDFrom figure C.7, we see that TTN can outperform nonlinear TD in terms of average error and alsohas reduced variance (smaller error bars). This seems to suggest that the TTN architecture can grantadditional stability in the off-policy setting.
