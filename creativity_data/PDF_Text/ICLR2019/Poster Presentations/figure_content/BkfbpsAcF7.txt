Figure 1: All images shown cause a competitive ImageNet-trained network to output the exact sameprobabilities over all 1000 classes (logits shown above each image). The leftmost image is from theImageNet validation set; all other images are constructed such that they match the non-class relatedinformation of images taken from other classes (for details see section 2.1). The excessive invariancerevealed by this set of adversarial examples demonstrates that the logits contain only a small fractionof the information perceptually relevant to humans for discrimination between the classes.
Figure 2: Connection between (1) invariance-based (long pink arrow) and (2) perturbation-basedadversarial examples (short orange arrow). Class distributions are shown in green and blue; dashedline is the decision-boundary of a classifier. All adversarial examples can be reached either by cross-ing the decision-boundary of the classifier via perturbations, or by moving within the pre-image ofthe classifier to mis-classified regions. The two viewpoints are complementary to one another andhighlight that adversarial vulnerability is not only caused by excessive sensitivity to semanticallymeaningless perturbations, but also by excessive insensitivity to semantically meaningful transfor-mations.
Figure 3: The fully in-vertible RevNet, a hybridof Glow and iRevNet withsimple readout structure.
Figure 4: Left: Decision-boundaries in 2D subspace spanned by two random data points x1, x2.
Figure 6: Left: Mutual information under distribution Dtrain, Right: Effect of distributional shiftto DAdv. Each case under training with cross-entropy (CE) and independence cross-entropy (iCE).
Figure 7: Samples X = FT(Zs,Zn) with logit activations Zs taken from original image and Znobtained by linearly interpolating from the original nuisance zn (first row) to the nuisance of a tar-get example Zn (last row upper block). The used target example is shown at the bottom. Whentraining with cross-entropy, virtually any image can be turned into any class without changing thelogits Zs , illustrating strong vulnerability to invariance-based adversaries. Yet, training with inde-pendence cross-entropy solves the problem and interpolations between nuisances Zn and Zn preservethe semantic content of the image.
Figure 8: ShiftMNIST experiments. (a): Binary ShiftMNIST, where the class is additionally encodedwith a location-based binary code on the left border of the image (highlighted with red circles). Theshifted adversarial test distribution does not have the binary class encoding. (b): Texture shiftM-NIST, where the class is additionally encoded in background texture type. The texture-class couplingis randomized in the shifted adversarial test distribution. Right: Results of CE-trained ResNet, fullyinvertible RevNet and iCE-trained fully invertible RevNet. The CE-based models build excessiveinvariance with respect to the digit identity on Dtrain and fail on DAdv. Difference denotes thelargest improvement between CE-trained and iCE-trained model. The iCE model is more resilientto removing informative features, and reduces the error on DAdv up to 38%.
Figure 9: Here we show a batch of randomly sampled metamers from our ImageNet-trained fullyinvertible RevNet-48. The quality is generally similar, sometimes colored artifacts appear.
