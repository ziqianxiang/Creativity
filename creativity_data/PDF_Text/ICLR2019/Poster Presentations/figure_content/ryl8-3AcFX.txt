Figure 1: We illustrate the architecture for learning the EPI-Policy. Trajectories Tepi generated fromthe EPI policy are passed through an embedding network to obtain the environment embeddings Ïˆ .
Figure 2: Striker Environment: (a) Illustration of the environment (b) Training curves of the EPI-policy comparing to baselines. The x-axis shows training iterations for TRPO. The y-axis shows theaverage reward of an episode.(c) Two-dimensional environment embedding of the EPI-policy aftertraining. Embeddings from the same environment have the same color.
Figure 3: Hopper Environment: (a) Illustration of the environment (b) Training curves of the EPI-Policy comparing to baselines. The x-axis shows training iterations. The y-axis shows the averagereward of an episode. (c) t-SNE of the 8-dimensional Environment embedding of the EPI-policyafter training. To visualize, the color is given by (R,G,B) where R:average of mass, Gfriction,B:average of damping.
Figure 4: Generalizability of the EPI-Policy on Hopper. The grey area represents the training rangewhile the rest represents testing environments.
