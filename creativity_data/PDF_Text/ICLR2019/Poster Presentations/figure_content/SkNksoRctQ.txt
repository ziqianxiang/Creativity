Figure 1: Approaches toward stationarity during the initial trainings for the MLP on the MNISTdata (a) and for the CNN on the CIFAR-10 data (b). Top panels depict the half-running averagef B(t) (dark green) and the instantaneous value f B(t) (light green) of the mini-batch loss. Bottompanels depict the convergence of the half-running averages of the observables OL = θ ∙ Vf B andOR = 2(1+μV) ηv2, whose stationary-state averages should agree according to the relation (FDR1').
Figure 2: Approaches toward stationarity during the sequential runs for various learning rates η, seenthrough the half-running averages of the observables OL = θ ∙ Vf B (solid) and OR = 2(1+))^V2(dotted light-colored). They agree at sufficiently long times but the relaxation time to reach such astationary regime increases as the learning rate η decreases.
Figure 3: The stationary-state average of the full-batch observable OFB as a function of the learningrate η, estimated through half-running averages. Dots and error bars denote mean values and 95%confidence intervals over several distinct runs, respectively. The straight red line connects the originand the point with the smallest η explored. (a) For the MLP on the MNIST data, linear dependenceon η for η . 0.01 supports the validity of the harmonic approximation there. (b) For the CNN onthe CIFAR-10 data, anharmonicity is pronounced even down to η 〜0.001.
Figure 4: Comparison of preset training schedule (black) and adaptive training schedule (blue),employing SGD without momentum both for the MLP on the MNIST data (a) and the CNN on theCIFAR-10 data (b), along with the AMSGrad algorithm (green). From top to bottom, plotted are thelearning rate η, the full-batch training loss f , and prediction accuracies on the training-set images(solid) and the 10000 test-set images (dashed).
Figure S1:	Comparison of AMSGrad (green) and Adam (orange) algorithms for the MLP on theMNIST data (a) and the CNN on the CIFAR-10 data (b). Top rows plot the full-batch training lossf while bottom rows plot prediction accuracies on the training-set images (solid) and the 10000test-set images (dashed).
Figure S2:	Comparison of preset training schedule (black) and adaptive training schedule (purple)-now with the scheduling hyperparameters X = 0.1 and Y = 0.3 - employing SGD withoutmomentum, and the AMSGrad algorithm (green), for the CNN on the CIFAR-10 data with the sameinitial seed as in the main text (a) and three different initial seeds (b-d). From top to bottom, plottedare the learning rate η, the full-batch training loss f , and prediction accuracies on the training-setimages (solid) and the 10000 test-set images (dashed).
