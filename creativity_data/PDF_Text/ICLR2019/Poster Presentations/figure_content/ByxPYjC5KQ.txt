Figure 1: Value surfaces of discriminators trained for 10,000 iterations with different gradient penal-ties, on samples from two Gaussian distributions. The discriminator is a 2 hidden layer MLP with 64hidden neurons.(a) No GP. (b) No GP with more samples. (c) One-centered GP (1-GP) with λ = 1.
Figure 2: Gradient w.r.t. the input of the discriminator of a GAN trained with different gradientpenalties. The vector associated with a datapoint v points in the direction that increases the valueof log (D(v)) the fastest. The discriminator is a 2 hidden layer MLP with 512 hidden neurons. Thediscriminator is updated once every generator update. SGD is used for optimization. (a), (b) NoGP, iter. 1000 and 10,000. (c), (d) No GP with TTUR, iter. 1,000 and 10,000. (e) Our 0-GP withλ = 10, iter. 10,000. (f), (g) Our 0-GP with TTUR and λ = 10, iter. 10,000 and 20,000. (h) 1-GPwith λ = 10, iter. 10,000. (i) 0-GP-sample with λ = 10, iter. 10,000.
Figure 3: Result on MNIST. The networks have the same architectures with networks used in syn-thetic experiment. Batch normalization (Ioffe & Szegedy, 2015) was not used. Adam optimizer(Kingma & Ba, 2014) with β1 = 0.5, β2 = 0.9 was used. (a) No GP, iter. 1,000. (b) 0-GP-sample,λ = 100, iter. 1,000. (c) 1-GP, λ = 100, iter. 1,000. (d), (e) 0-GP, λ = 100, iter. 1,000 and 10,000.
Figure 4: Inception score (Salimans et al., 2016) on ImageNet of GAN-0-GP, GAN-0-GP-sample,and WGAN-GP. The code for this experiment is adapted from Mescheder et al. (2018). We usedλ = 10 for all GANs as recommended by Mescheder et al. The critic in WGAN-GP was updated 5times per generator update. To improve convergence, we used TTUR with learning rates of 0.0001and 0.0003 for the generator and discriminator, respectively.
Figure 5:	Evolution of GAN-0-GP with λ = 100 on 8 Gaussians dataset.
Figure 6:	GANs trained with different gradient penalty on swissroll dataset. Although GAN-1-GPis able to learn the distribution, the gradient field has bad pattern. GAN-1-GP is more sensitive tochange in hyper parameters and optimizers. GAN-1-GP fails to learn the scaled up version of thedistribution.
Figure 7:	Result on MNIST. Adam was initialized with β1 = 0.5, β2 = 0.9. (a) No GP, iteration10,000. (b) Zero-centered GP on real samples only with λ = 100, iteration 10,000. (c) One-centeredGP with λ = 100, iteration 10,000. (d) Our zero-centered GP with λ = 100, iteration 10,000.
Figure 8:	Result on MNIST. Adam was initialized with β1 = 0.9, β2 = 0.99. (a) Zero-centeredGP on real samples only with λ = 100, iteration 10,000. (b) Our zero-centered GP with λ = 100,iteration 10,000.
Figure 9:	Linear latent space interpolation between two random samples. (a) GAN with 0-GP-sample cannot perform smooth interpolation between modes. Small changes in input latent variableresult in big difference in the output (red boxes). The result suggest that 0-GP-sample makes GANsto remember the training dataset and do not generalize to the region between samples in the trainingdataset. (b) GAN with our 0-GP can perform smooth interpolation between modes. The behaviorimplies that GANs with our 0-GP have better generalization.
Figure 10: Samples from GAN-0-GP trained on ImageNet.
