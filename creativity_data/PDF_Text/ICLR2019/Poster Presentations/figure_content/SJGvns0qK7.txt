Figure 1: An overview of Bayesian Policy Optimization. The policy is simulated on multiplelatent models. At each timestep of the simulation, a black-box Bayes filter updates theposterior belief and inputs the state-belief to the policy (Figure 1a). Belief (b) and state (s)are independently encoded before being pushed into the policy network (Figure 1b)Veness, 2010; Guez et al., 2012) are also prohibitively expensive in continuous state-actionspaces: the width of the search tree after a single iteration is too large, preventing anadequate search depth from being reached.
Figure 2:	(a) Comparison of BPO with belief-agnostic, robust RL algorithms. BPO signifi-cantly outperforms benchmarks when belief-awareness and explicit information gathering arenecessary (Tiger, LightDark). It is competitive with UP-MLE when passive estimation oruniversal robustness is sufficient (Chain, MuJoCo). (b) Scalability of BPO with respect tolatent state space discretization for the Chain problem.
Figure 3:	Visualization of different algorithms on the LightDark environment. The dashedline indicates the light source. Blue circles are one standard deviation for per-step estimates.
Figure 4: (a) Comparison of BPO and TRPO trained on the nominal environment for adifferent environment. The task is to move to the right along the x-axis. However, the modelat test time differs from the one TRPO trained with: one leg is 20% longer, another is 20%shorter. (b) Comparison of average entropy per timestep by BPO and UP-MLE. The beliefdistribution collapses more quickly under the BPO policy. (c) Belief distribution at t = 20during a BPO rollout.
Figure 5: Pairwise performance comparison of algorithms on MuJoCo BAMDPs. Eachpoint represents an MDP, and its (x, y)-coordinates correspond to the long-term reward by(baseline, BPO). The farther a point is above the line y = x, the more BPO outperformsthat baseline. Colors indicate which algorithm achieved higher reward: BPO (red), EPOpt(green), UP-MLE (blue), or TRPO (purple).
