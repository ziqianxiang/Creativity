Figure 1: The network topologies discussed in this work. (a) is known as the LCA network that canperform sparse coding. We propose the network in (b) for dictionary learning.
Figure 2:	Network spike patterns in practice. In the figures, each row corresponds to one neuron, andthe bars indicate the spike timings. Following Algorithm 1, for each online input the network is runwith γ = 0 from t = 0 to t = 20 (feed-forward only), and γ = 0.7 from t = 20 to t = 40 (perturbnetwork activities by feedback). The spike rates a, b for computing the gradients are then collectedby counting the number of spikes within the time interval. The figures in the left and right show thespike patterns before and after learning, respectively. We make two observations: (1) The codingneuron spike rates remain approximately constant for both intervals t = [0, 20] and t = [20, 40],illustrating Theorem 2 that feedback perturbation does not alter the computed optimal sparse code.
Figure 3:	Network weight consistency and symmetry during learning. Consistency is measured as1 - kH - FBkF / kHkF. Symmetry is measured as the average normalized inner product betweenthe i-th row of F and the i-th column of B for i = 1 . . . N . Data is from learning with Dataset A.
Figure 4:	Comparison of convergence of learning with dynamical neural network and SGD.
Figure 5: A 1-layer LCA network for sparse coding.
Figure 6: The figure shows a random subset of the dictionaries learned in spiking networks. Theyshow the expected patterns of edges and textures (Lena), strokes and parts of the digits (MNIST), andGabor-like oriented filters (natural scenes), similar to those reported in prior works (Rubinstein et al.,2010; Ranzato et al., 2007; Hoyer, 2004).
Figure 7: Image denoising using learned dictionary.
