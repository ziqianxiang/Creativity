Figure 1: Toy experiments. signSGD with majority vote is run on a 1000-dimensional quadraticwith N (0, 1) noise added to each gradient component. Adversarial experiments are run with 27 totalworkers. These plots may be reproduced in a web browser by running this Jupyter notebook.
Figure 2: Gradient distributions for resnet18 on Cifar-10 at mini-batch size 128. At the start ofepochs 0, 1 and 5, we do a full pass over the data and collect the gradients for three randomly chosenweights (left, middle, right). In all cases the distribution is close to unimodal and symmetric.
Figure 3: Signal-to-noise ratio (SNR) whilst training resnetl8 on Cifar-IO at batch size 128.
Figure 4: Timing breakdown for distributing on the cloud. Left: comparing communication (includ-ing compression) for training resnet50. Right: comparing communication (including compres-sion) and computation. resnet50 results use 7 p3.2xlarge machines for training Imagenet,each at batch size 128. alexnet uses 7 p3.2xlarge machines for Imagenet, each at batch size64. QRNN uses 3 p3.16xlarge machines for training WikiText-103, each at batch size 240.
Figure 5: Imagenet comparison of Signum with majority vote and SGD distributed with NCCL.
Figure 6: Training QRNN across three p3.16xlarge machines on WikiText-103. Each ma-chine uses a batch size of 240. For Adam, the gradient is aggregated with NCCL. Signum withmajority vote shows some degradation compared to Adam, although an epoch is completed roughlythree times faster. This means that after 2 hours of training, Signum attains a similar perplexity toAdam. Increasing the per-worker batch size improved Signum’s performance (see Appendix A),and increasing it beyond 240 may further improve Signum’s performance. Note: the test perplexitybeats training perplexity because dropout was applied during training but not testing.
Figure 7: Left: comparing convergence of majority vote to QSGD (Alistarh et al., 2017).
Figure 8: Imagenet robustness experiments. We used majority vote to train resnet50 distributedacross 7 AWS p3.2xlarge machines. Adversaries invert their sign stochastic gradient. Left:all experiments are run at identical hyperparameter settings, with weight decay switched off forsimplicity. The network still learns even at 43% adversarial. Right: at 43% adversarial, learningbecame slightly unstable. We decreased the learning rate for this setting, and learning stabilised.
Figure 9: Comparing the robustness of majority vote to Multi-Krum (Blanchard et al., 2017). Wetrain resnet18 on Cifar-10 across 7 workers, each at batch size 64. Momentum and weight decayare switched off for simplicity, and for majority vote we divide the learning rate by 10 at epoch100. Negative adversaries multiply their stochastic gradient estimate by -10. Random adversariesmultiply their stochastic gradient estimate by 10 and then randomise the sign of each coordinate. ForMULTI-KRUM, we use the maximum allowed security level of f = 2. Notice that MULTI-KRUMfails catastrophically once the number of adversaries exceeds the security level, whereas majorityvote fails more gracefully.
Figure 11: Signum at varying batch sizes. We use a single worker and train a QRNN model onWikiText-103. Adam is shown for comparison, at batch size 60. The performance of S ignumis seen to improve with increasing batch size.
Figure 10: QSGD at varying levels of precision. Top row: L2 QSGD. Bottom row: max QSGD.
