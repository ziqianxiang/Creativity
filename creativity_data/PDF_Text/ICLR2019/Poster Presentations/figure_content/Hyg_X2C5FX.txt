Figure 1: Overview: (a) Realistic outdoor church images generated by Progressive GANs (Karraset al., 2018). (b) Given a pre-trained GAN model, we identify a set of interpretable units whosefeaturemap is correlated to an object class across different images. For example, one unit in layer4localizes tree regions with diverse visual appearance. (c) We force the activation of the units to bezero and quantify the average casual effect of the ablation. Here we successfully remove trees fromchurch images. (d) We activate tree causal units in other locations. These same units synthesize newtrees, visually compatible with their surrounding context. In addition, our method can diagnose andimprove GANs by identifying artifact-causing units (e). We can remove the artifacts that appear (f)and significantly improve the results by ablating the “artifact” units (g). Please see our demo video.
Figure 2: Measuring the relationship between representation units and trees in the output using (a)dissection and (b) intervention. Dissection measures agreement between a unit u and a concept c bycomparing its thresholded upsampled heatmap with a semantic segmentation of the generated imagesc(x). Intervention measures the causal effect of a set of units U on a concept c by comparing theeffect of forcing these units on (unit insertion) and off (unit ablation). The segmentation sc revealsthat trees increase after insertion and decrease after ablation. The average difference in the tree pixelsmeasures the average causal effect. In this figure, interventions are applied to the entire featuremap P,but insertions and ablations can also apply to any subset of pixels P ⊂ P.
Figure 3: Visualizing the activations of individual units in two GANs. Top ten activating imagesare shown, and IoU is measured over a sample of 1000 images. In each image, the unit feature isupsampled and thresholded as described in Eqn. 2.
Figure 4: Ablating successively larger sets of tree-causal units from a GAN trained on LSUN outdoorchurch images, showing that the more units are removed, the more trees are reduced, while buildingsremain. The choice of units to ablate is specific to the tree class and does not depend on the image.
Figure 5: Comparing representations learned by progressive GANs trained on different scene types.
Figure 6: Comparing layers of a progressive GAN trained to generate LSUN living room images.
Figure 7: Comparing layer4 representations learned by different training variations. SlicedWasserstein Distance (SWD) is a GAN quality metric suggested by Karras et al. (2018): lower SWDindicates more realistic image statistics. Note that as the quality of the model improves, the numberof interpretable units also rises. Progressive GANs apply several innovations including making thediscriminator aware of minibatch statistics, and pixelwise normalization at each layer. We can seebatch awareness increases the number of object classes matched by units, and pixel norm (applied inaddition to batch stddev) increases the number of units matching objects.
Figure 8: (a) We show two example units that are responsible for visual artifacts in GAN results.
Figure 9: Measuring the effect of ablating units in a GAN trained on conference room images. Fivedifferent sets of units have been ablated related to a specific object class. In each case, 20 (out of512) units are ablated from the same GAN model. The 20 units are specific to the object class andindependent of the image. The average causal effect is reported as the portion of pixels that areremoved in 1 000 randomly generated images. We observe that some object classes are easier toremove cleanly than others: a small ablation can erase most pixels for people, curtains, and windows,whereas a similar ablation for tables and chairs only reduces object sizes without deleting them.
Figure 10: Comparing the effect of ablating 20 window-causal units in GANs trained on five scenecategories. In each case, the 20 ablated units are specific to the class and the generator and independentof the image. In some scenes, windows are reduced in size or number rather than eliminated, orreplaced by visually similar objects such as paintings.
Figure 11: Inserting door units by setting 20 causal units to a fixed high value at one pixel in therepresentation. Whether the door units can cause the generation of doors is dependent on its localcontext: we highlight every location that is responsive to insertions of door units on top of the originalimage, including two separate locations in (b) (we intervene at left). The same units are insertedin every case, but the door that appears has a size, alignment, and color appropriate to the location.
Figure 12: At left, visualizations of the highest-activating image patches (from a sample of 1000) forthree units. (a) the loWeSt-FID unit that is manually flagged as showing artifacts (b) the highest-FIDunit that is not manually flagged (c) the highest-FID unit overall, which is also manually flagged. Atright, the precision-recall curve for unit FID as a predictor of the manually flagged artifact units. AFID threshold selecting the top 20 FID units will identify 10 (of 20) of the manually flagged units.
Figure 13: The effects of ablating high-FID units compared to manually-flagged units: (a) generatedimages with artifacts, without intervention; (b) those images generated after ablating the 20-highestFID units; (c) those images generated after ablating the 20 manually-chosen artifact units.
Figure 14: Two examples of generator units that our dissection method labels differently from humans.
Figure 15: TWo examples of units that correlate with unrealistic images that confuse a semanticsegmentation network. Both units are taken from a WGAN-GP for LSUN bedrooms.
Figure 16: Comparing a dissection of units for a WGAN-GP trained on LSUN bedrooms, consideringall units (at left) and considering only “realistic” units with FID < 55 (at right). Filtering units byFID scores removes spurious detected concepts such as ‘sky’, ‘ground', and ‘building’.
Figure 17: Tracing the effect of inserting door units on downstream layers. An identical ”door”intervention at layer4 of each pixel in the featuremap has a different effect on later feature layers,depending on the location of the intervention. In the heatmap, brighter colors indicate a strongereffect on the layer14 feature. A request for a door has a larger effect in locations of a building,and a smaller effect near trees and sky. At right, the magnitude of feature effects at every layer isshown, measured by the changes of mean-normalized features. In the line plot, feature changes forinterventions that result in human-visible changes are separated from interventions that do not resultin noticeable changes in the output.
Figure 18: The evolution of layer4 of a Progressive GAN bedroom generator as training proceeds.
Figure 19: All layers of awood layer13 #23	iou=0.17--UnProgressive GAN trained to generate LSUN livingroom images.
