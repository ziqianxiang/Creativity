Figure 1: Input-driven environments: (a) load-balancing heterogeneous servers (Harchol-Balter & Vesilo, 2010)with stochastic job arrival as the input process; (b) adaptive bitrate video streaming (Mao et al., 2017) withstochastic network bandwidth as the input process; (c) Walker2d in wind with a stochastic force (wind) appliedto the walker as the input process; (d) HalfCheetah on floating tiles with the stochastic process that controlsthe buoyancy of the tiles as the input process; (e) 7-DoF arm tracking moving target with the stochastic targetposition as the input process. Environments (c)-(e) use the MuJoCo physics simulator (Todorov et al., 2012).
Figure 2: Load balancing over two servers. (a) Job sizes follow a Pareto distribution and jobs arrive as a Poissonprocess; the RL agent observes the queue lengths and picks a server for an incoming job. (b) The input-dependentbaseline (blue) results in a 50ร lower policy gradient variance (left) and a 33% higher test reward (right) thanthe standard, state-dependent baseline (green). (c) The probability heatmap of picking server 1 shows that usingthe input-dependent baseline (left) yields a more precise policy than using the state-dependent baseline (right).
Figure 3: Graphical model ofinput-driven MDPs.
Figure 4: In continuous-action MuJoCo environments, TRPO (Schulman et al., 2015a) with input-dependentbaselines achieve 25%-3 X better testing reward than with a standard State-dependent baseline. Learning curvesare on 100 testing episodes with unseen input sequences; shaded area spans one standard deviation.
Figure 5: In environments with discrete action spaces, A2C (Mnih et al., 2016) with input-dependent baselinesoutperforms the best heuristic and achieves 25-33% better testing reward than vanilla A2C (Mnih et al., 2016).
Figure 6: An LSTM-based input-dependent baseline (green) does not provide significant performance gain overstandard state-dependent baseline (red) for load balancing and Walker2d with wind environments.
Figure 7: Input-dependent base-line improves TRPO performancein the POMDP version of theWalker2d with wind environment.
Figure 8: In continuous-action MuJoCo environments (ยง6.1), PPO (Schulman et al., 2017) with input-dependentbaselines achieves 42%-3.5x better testing reward than PPO with a standard state-dependent baseline. Learningcurves are on 100 testing episodes with unseen input sequences; shaded area spans one standard deviation.
Figure 9: The input-dependent baseline technique is complementary and orthogonal to RARL (Pinto et al.,2017). The implementation of input-dependent baseline is MAML (ยง5). Left: learning curves of testing rewards;shaded area spans one standard deviation; the input-dependent baseline improves the policy optimization forboth TRPO and RARL, while RARL improves TRPO in the Walker2d environment with wind disturbance.
Figure 10: The input-dependent baseline technique is complementary to MPO (Clavera et al., 2018b). Theimplementation of input-dependent baseline is MAML (ยง5). Left: learning curves in the testing Walker2denvironment with wind disturbance; MPO is tested with adapted policy in each testing instance of the windinput; shaded area spans one standard deviation; the input-dependent baseline improves the policy optimizationfor both TRPO and MPO, while MPO improves TRPO. Right: meta policy adaptation at training timestep 5e7;adapting the policy in specific input instances help boosting the performance (comparing yellow with green, andred with blue); applying input-dependent baseline generally improves the policy performance.
