Figure 1: Top row shows Q-value discrepancy ∆Q as a measure for recurrent state staleness. (a)Diagram of how ∆Q is computed, with green box indicating a whole sequence sampled from replay.
Figure 2: Atari-57 results. Left: median human-normalized scores and training times of variousagent architectures. Diagram reproduced and extended from (Horgan et al., 2018). Right: Exampleindividual learning curves of R2D2, averaged over 3 seeds, and Ape-X, single seed.
Figure 3: DMLab-30 comparison of R2D2 and R2D2+ with our re-run of IMPALA shallow anddeep in terms of mean-capped human-normalized score (Espeholt et al., 2018).
Figure 4: Ablations with reward clipping instead of value function rescaling (Clipped), smallerdiscount factor of γ = 0.99 (Discount), and feed-forward (Feed-Forward) variants of R2D2.
Figure 5: Effect of restricting R2D2's policy’s memory on Ms.Pacman and EMSTM_WATERMAZE.
Figure 6: Left: Parameter lag experienced with distributed prioritized replay with (top) 256 and(bottom) 64 actors on four DMLab levels: explore obstructed goals large (eogl), explore objectrewards many (eorm), lasertag three opponents small (lots), rooms watermaze (rw). Center: initial-state and Right: final-state Q-value discrepancy for the same set of experiments.
Figure 7: Ablation results with standard deviations shown by shading (3 seeds). ‘Clipped’ refers tothe agent variant using clipped rewards (instead of value function rescaling), ‘discount’ refers to theuse of a discount value of 0.99 (instead of 0.997).
Figure 8: Ablation results for the use of life loss as episode termination on Atari, with standarddeviations shown by shading (3 seeds). ‘reset’ refers to the agent variant using life losses as fullepisode terminations (preventing value function bootstrapping across life loss events, as well asresetting the LSTM state), whereas ‘roll’ only prevents value function bootstrapping, but unrolls theLSTM for the duration of a full episode (potentially spanning multiple life losses).
Figure 9: Comparing sample efficiency between state-of-the-art agents on Atari-57. We observea general trend of increasing final performance being negatively correlated with sample efficiency,which holds for all four algorithms compared.
Figure 10: Learning curves on 57 Atari games: R2D2 (3 seeds), APe-X and R2D2-feed-forward (1seed each).
Figure 11: Per-level breakdown of DMLab-30 performance. Comparison between R2D2 and IM-PALA trained for the same number of environment frames. The shallow-network and deep-networkversions are overlaid for each algorithm.
