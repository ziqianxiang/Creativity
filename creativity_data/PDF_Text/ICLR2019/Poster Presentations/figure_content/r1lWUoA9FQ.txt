Figure 1: Adversarial examples with different norm constraints formed via the projected gradient method (Madry et al.,2017) on Resnet50, along with the distance between the base image and the adversarial example, and the top class label.
Figure 2: Sparse adversarial examples perturb a small subset of pixels and can hide adversarial “fuzz” inside high-frequency image regions. The original image (left) is classified as an "ox." Under '∞-norm perturbations, it is classifiedas “traffic light”, but the perturbations visibly distort smooth regions of the image (the sky). These effects are hidden inthe grass using `0 -norm (sparse) perturbations limited to a small subset of pixels.
Figure 3: The -expansion of a half sphere nearly covers the whole sphere for small and large n. Visualizations showthe fraction of the sphere captured within units of a half sphere in different dimensions. Results from a near-exactexperimental method are compared to the theoretical lower bound in Lemma 2.
Figure 4: (a) Robustness of MNIST and “big” MNIST classifiers as a function of . Naturally trained classifiers areless robust with increased dimensionality. (b) With adVersarial training, susceptibility curVes behaVe as predicted byTheorems 2 and 5. (c) The susceptibility of CIFAR-10 is compared to big MNIST. Both datasets haVe similar dimension,but the higher complexity of CIFAR-10 results in far worse susceptibility. Perturbations are measured in the '2-norm.
