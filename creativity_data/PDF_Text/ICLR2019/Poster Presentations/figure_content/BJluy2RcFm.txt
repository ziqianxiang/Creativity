Figure 1: A neural network with a single Janossypooling layer. The embedding h is permutedin all |h|! possible ways, and for each permuta-*tion hπ, f (|h|, hπ; θ(f)) is computed. These aresummed and passed to a second function ρ(∙; θ(ρ))which gives the final permutation-invariant outputy(x; θ(ρ), θ(f), θ(h)); the gray rectangle representsJanossy pooling. We discuss how this can be madethe Supplementary Material.
Figure 2: Mean performance Vs number of permutations sampled at test time, PPI taskB.2	Implementation and Experiment DetailsSequence tasks We extended the code from Zaheer et al. (2017), which was written inKeras(Chollet et al., 2015), and subsequently ported to PyTorch. For k-ary models with k ∈ {2, 3},we always sort the sequence x beforehand to reduce the number of combinations we need to sumover. In the notation of Figure 1, h is an Embedding with dimension of floor( 1k0) (to keep the total*number of parameters consistent for each k as discussed below), f is either an MLP with a singlehidden layer or an RNN depending on the model (k-ary Janossy or full-Janossy, respectively), andρ is either a linear dense layer or one hidden layer followed by a linear dense layer. The MLPs inf have 30 neurons whereas the MLPs in ρ have 100 neurons, the LSTMs have 50 neurons, and theGRUs have 80 hidden neurons. All activations are tanh except for the output layer which is linear.
