Figure 1: Abstract comparison of standard approach (left) and ours (right). Thestandard direct approach requires a discriminative, supervised loss (SL) term betweenpredicted and true x, causing problems when y → x is ambiguous. Our network uses asupervised loss only for the well-defined forward process x → y. Generated x are requiredto follow the prior p(x) by an unsupervised loss (USL), while the latent variables z are madeto follow a Gaussian distribution, also by an unsupervised loss. See details in Section 3.3.
Figure 2: Viability of INN for a basic inverse problem. The task is to produce thecorrect (multi-modal) distribution of 2D points x, given only the color label y*. Whentrained with all loss terms from Sec. 3.3, the INN output matches ground truth almostexactly (2nd image). The ablations (3rd and 4th image) show that we need Ly and Lz tolearn the conditioning correctly, whereas Lx helps us remain faithful to the prior.
Figure 3: Distribution over articulated poses x, conditioned on the end point y*.
Figure 4: Sampled posterior of 5 parameters for fixed y* in medical application.
Figure 5: Astrophysics application. Properties x of star clusters in interstellar gas cloudsare inferred from multispectral measurements y. We train an INN on simulated data, andshow the sampled posterior of 5 parameters for one y* (colors as in Fig. 4, second row). Thepeculiar shape of the prior is due to the dynamic nature of these simulations. We includethis application as a real-world example for the INN’s ability to recover multiple posteriormodes, and strong correlations in P(X | y*), see details in appendix, Sec. 5.
Figure 6: Results of several existing methods for the Gaussian mixture toy example.
Figure 7: Abstraction of the cVAE-IAF training scheme compared to our INN from Fig. 1.
Figure 8: Layout of INN latent space for one fixed label y*, colored by mode closestto x = g(y*, z). For each latent position z, the hue encodes which mode the corresponding Xbelongs to and the luminosity encodes hoW close X is to this mode. Note that colors usedhere do not relate to those in Fig. 2, and encode the position x instead of the label y. Thefirst three columns correspond to labels green, blue and red Fig. 2. White circles mark areasthat contain 50% and 90% of the probability mass of latent prior p(z).
Figure 9: Posteriors generated for less challenging observations y* than in Fig. 3.
Figure 10: INN applied to real footage to predict oxygenation so2 and uncertainty.
Figure 11: Calibration curves for all four methods compared in Sec. 4.2.
