Figure 1: When images from the ImageNet validation dataset are shown to a pre-trainedResNet-18 (He et al., 2016), the outputs from certain channel neurons may vary drastically.
Figure 2: A high level view of a convolutional layer with FBS. By way of illustration, weuse the lth layer with 8-channel input and output features, where channels are colored toindicate different Saliencies, and the white blocks (Sl) represent all-zero channels.
Figure 3: Experimental results on M-CifarNet. We compare in (a) the accuracy/MACstrade-off between FBS, NS and FBS+NS. The baseline is emphasized by the circle . Theheat map in (b) reveals the individual probability of skipping a channel for each channel(x-axis), when an image of a category (y-axis) is shown to the network with d = 1.
Figure 4: The training history of a convolutional layer conv4 in M-CifarNet. The historyis visualized by the 12 skipping probabilites heat maps, where the heights denote the 10categories in CIFAR-10, and channels in conv4 occupy the width.
Figure 5: The accuracy/performance trade-off comparison between NS and FBS for ResNet-18 on the ImageNet ILSVRC2012 validation set.
