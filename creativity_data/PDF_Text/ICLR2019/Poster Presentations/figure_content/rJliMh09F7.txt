Figure 1: Impact of our regularization on multi-modal conditional generation.
Figure 2: Diverse outputs generated by DSGAN. The first and second column shows ground-truthand input images, while the rest columns are generated images with different latent codes.
Figure 3: Qualitative comparison for high-resolution image synthesis (1024 × 512 px.).
Figure 5: Stochastic image inpainting results. Given an input image with missing region (first row),We generate multiple faces by sampling different Z (second-fifth rows). Each row is generated fromthe same z , and exhibits similar face attributes.
Figure B: Qualitative comparisons of Table 1.
Figure C: Qualitative comparisons of Table 3.
Figure D: Interpolation in latent space of DSGAN on Cityscapes dataset.
Figure E:	Comparison of latent space interpolation results between DSGAN and BicycleGAN.
Figure F:	Image inpainting results with different λ. We observe more diversity emerges from thegenrator outputs as we increase the weights for our regularization.
Figure G: Interpolation results on image inpainting task. For each row, we sample the two latentcodes (leftmost and rightmost images), and generate the images from the interpolated latent codesfrom one latent code to another.
Figure H: Stochastic video prediction results. In both datasets, our method presents diverse predic-tion, whereas SAVP generate less diverse result especially in the KTH dataset. Interestingly, as youcan see from the dotted orange box, our model can explore not only the original condition (handclapping) but also other cases (hand waving) if the context is not too strong. Please check our webpage to see the videos: https://sites.google.com/view/iclr19-dsgan/22Published as a conference paper at ICLR 2019Random SamplesFigure I: Qualitative comparison of various video prediction methods. Both baseline cGAN andSAVP exhibit some noises in the predicted videos due to the failures in separating the moving fore-ground object from the background clutters (red arrow). Compared to this, our method tends togenerate more clear predictions on both foreground and background. Interestingly, SAVP some-times fail to predict interaction between objects (magenta arrows). For instance, the objects on atable stay in the same position even after pushed by the robot arm. On the other hand, our method isable to capture such interactions more precisely (blue arrows).
Figure I: Qualitative comparison of various video prediction methods. Both baseline cGAN andSAVP exhibit some noises in the predicted videos due to the failures in separating the moving fore-ground object from the background clutters (red arrow). Compared to this, our method tends togenerate more clear predictions on both foreground and background. Interestingly, SAVP some-times fail to predict interaction between objects (magenta arrows). For instance, the objects on atable stay in the same position even after pushed by the robot arm. On the other hand, our method isable to capture such interactions more precisely (blue arrows).
