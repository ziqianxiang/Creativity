Figure 1: We train a 6-layer network with parameters φ to predict layerwise targets generated by an-other network with random parameters φ*. Left: We compare the convergence of the global loss oftwo training runs starting from the same initial conditions and identical (untuned) hyperparamters: Anetwork with parameters φlocal trained using only local losses anda network with parameters φglobaltrained directly on the global loss. We note that the locally trained network converges significantlyfaster, suggesting that optimization is easier in the absence of the “confusing” distant-gradient sig-nals from the not-yet-converged higher layers. Right: We plot the cosine-similarity of local anddistant components of the gradient of φlocal as training progresses. We see that as we approachconvergence (as φiocai → φ*), the local and distant gradients tend to align.
Figure 2: Learning Curves on MNIST comparing the performance of Equilibrium Propagation (EqProp: s-), the Forward-Pass in Initialized Equilibrium Propagation (Fwd Eq Prop: sf) (Algorithm2) and the Negative Phase in Initialized Equilibrium Propagation (Fwd Eq Prop: s- ) (Algorithm 3)Numbers indicate error at the final test. Left Column: A shallow network with a single hidden layerof 500 units. Right Column: A deeper network with 3 layers of [500, 500, 500] hidden units. TopRow: Training with a small-number of negative-phase steps (4 for the shallow network, 20 for thedeeper) shows that feedfoward initialization makes training more stable by providing a good startingpoint for the negative phase optimization. The Eq Prop s- lines on the upper plots are shortenedbecause we terminate training when the network fails to converge. Bottom Row: Training withmore negative-phase steps shows that when the baseline Equilibrium Propagation network is givensufficient time to converge, it performs comparably with our feedforward network (Note that they-axis scale differs from the top).
Figure 3: Test scores and gradient alignment on [784-500-500-500-10] network trained on MNISTLeft: We compare the performance of Initialized Equilibrium Propagation when the feedforwardnetwork is trained using only local losses vs the global loss (i.e. using backpropagation). sf denotesthe forward pass and s- denotes the state at the end of the negative phase. Note that we observeno disadvantage when we only use local losses. Right: We observe the same effect as for our toyproblem (see Figure 1). Early on in training, the local error gradients tend to align with gradientscoming from higher layers.
Figure 4: Here we scan the λ parameter and plot the final score at the end of training. Each pointin each plot corresponds to the final score of a network with parameter λ fixed at the given valuethroughout training. The top row of plots is a for a small network with one hidden layer of 500hidden units. The bottom is for a large network with 3 layers of [500, 500, 500] hidden units. Eachcolumn is for a different number of steps of negative-phase convergence.
