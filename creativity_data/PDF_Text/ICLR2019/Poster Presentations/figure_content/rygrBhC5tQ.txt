Figure 1: Concept of a transition policy. Composing complex skills using primitive skills requiressmooth transition between primitive skills since a following primitive skill might not be robust toending states of the previous one. In this example, the ending states (red circles) of the primitivepolicy pjump are not good initial states to execute the following policy pwalk. Therefore, executingPwalk from these states will fail (red arrow). To smoothly connect the two primitive policies, wepropose a transition policy which navigates an agent to suitable initial states for pwalk (dashed arrow),leading to a successful execution of Pwalk (green arrow).
Figure 2: Our modular network augmented with transition policies. To perform a complex task,our model repeats the following steps: (1) The meta-PoliCy chooses a primitive policy of index c;(2) The corresponding transition policy helps initiate the chosen primitive policy; (3) The primitivepolicy executes the skill; and (4) A success or failure signal for the primitive skill is produced.
Figure 3: Training of transition policies and proximity predictors. After executing a primitive policy,a previously performed transition trajectory is labeled and added to a replay buffer based on theexecution success. A proximity predictor is trained on states sampled from the two buffers to outputthe proximity to the initiation set. The predicted proximity serves as a reward to encourage thetransition policy to move toward good initial states for the corresponding primitive policy.
Figure 4: Tasks and success count curves of our model (blue), TRPO (purple), PPO(magenta), andtransition policies (TP) trained on task reward (green) and sparse proximity reward (yellow).OUrmodel achieves the best performance and convergence time. Note that TRPO and PPO are trained5 times longer than ours with dense rewards since TRPO and PPO do not have primitive skills andlearn from scratch. In the success count curves, different temporal scales are used for TRPO andPPO (bottom x-axis) and ours (top x-axis).
Figure 5: Average transition length and average proximity reward of transition trajectories overtraining on Manipulation (left) and Patrol (right).
Figure 6: Visualization of transition trajectories of (a) Repetitive picking up and (b) Patrol. TOPand Bottom rows: contain rendered frames of transition trajectories. Middle row: containsstates extracted from each primitive skill execution projected onto PCA space. The dots connectedwith lines are extracted from the same transition trajectory, where the marker color indicates theproximity prediction P (s). A higher P(s) value indicates proximity to states suitable for initializingthe next primitive skill. Left: two picking up transition trajectories demonstrate that the transitionpolicy learns to navigate from terminate states s0 and s1 to t0 and t1. RIGHT: the forward to balancetransition moves between the forward and balance state distributions and the balance to backwardtransition moves from the balancing states close to the backward states.
Figure 7: Success count curves of our model with exponentially discounted proximity functionand linearly discounted proximity function over training on Obstacle course (left) and Repetitivecatching (right).
