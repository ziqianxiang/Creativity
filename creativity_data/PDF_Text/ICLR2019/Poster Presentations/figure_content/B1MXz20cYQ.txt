Figure 1: Graphical models. (1a) pM(c|x) is classifier whose behavior we wish to analyze. (1b) Toexplain its response to a particular input x we partition the input x into masked (unobserved) regionxr and their complement x = xr ∪ x\r . Then we replace the xr with uninformative reference value^r to test which region Xr is important for classifier,s output PM(c∣Xr, x\r). Heuristic in-filling(Fong & Vedaldi, 2017) computes Xr ad-hoc such as image blur. This biases the explanation whensamples [Xr, x\r] deviate from the data distributionP(Xr, x\r). (1c) We instead sample Xr efficientlyfrom a conditional generative model Xr 〜PG(Xr ∣X∖r) that respects the data distribution.
Figure 2: Computed saliency for a variety of in-filling techniques. The classifier predicts thecorrect label, “drake”. Each saliency map (top row) results from maximizing in-class confidence ofmixing a minimal region (red) of the original image with some reference image in the complementary(blue) region. The resulting mixture (bottom row) is fed to the classifier. We compare 6 methodsfor computing the reference, 3 heuristics and 3 generative models. We argue that strong generativemodels—e.g., Contextual Attention GAN (CA) (Yu et al., 2018)—ameliorate in-fill artifacts, makingexplanations more plausible under the data distribution.
Figure 3: Visualization of reference value infilling methods under centered mask. The ResNetoutput probability of the correct class is shown (as a percentage) for each imputed image.
Figure 4: Classifier confidence of infilled images. Given an input, FIDO-CA finds a minimal pixelregion that preserves the classifier score following in-fill by CA-GAN (Yu et al., 2018). Dabkowski& Gal (2017) (Realtime) assigns saliency coarsely around the central object, and the heuristic infillreduces the classifier score. We mask further regions (head and body) of the FIDO-CA saliency mapby hand, and observe a drop in the infilled classifier score. The label for this image is “goose”.
Figure 5: Pseudo code comparison. Differences between the approaches are shown in blue.
Figure 6: Choice of objective between LSDR and LSSR . The classifier (ResNet) gives correctpredictions for all the images. We show the LSDR and LSSR saliency maps under 2 infilling methods:Mean and CA. Here the red means important and blue means non-important. We find that LSDR ismore susceptible to artifacts in the resulting saliency maps than LSSR .
Figure 7: Comparison of saliency map under different infilling methods by FIDO SSR usingResNet. Heuristics baselines (Mean, Blur and Random) tend to produce more artifacts, whilegenerative approaches (Local, VAE, CA) produce more focused explanations on the targets.
Figure 8: Proportion of saliency map outside bounding box. Different in-filing methods evaluatingResNet trained on ImageNet, 1, 971 images. The lower the better.
Figure 9: Comparison of saliency maps for several classifier architectures. We compare 3 net-works: AlexNet, Vgg and ResNet using FIDO-CA with λ = 10-3)4.5	Quantitative EvaluationWe follow Fong & Vedaldi (2017) and Shrikumar et al. (2017) in measuring the classifier’s sensitivityto successively altering pixels in order of their saliency scores. Intuitively, the “best” saliency mapshould compactly identify relevant pixels, so that the predictions are changed with a minimum numberof altered pixels. Whereas previous works flipped salient pixel values or set them to zero, we notethat this moves the classifier inputs out of distribution. We instead dropout pixels in saliency orderand infill their values with our strongest generative model, CA-GAN. To make the log-odds scoresuppression comparable between images, we normalize per-image by the final log-odds suppressionscore (all pixels infilled). In Figure 10 we evaluate on ResNet and carry out our scoring procedure on1, 533 randomly-selected correctly-predicted ImageNet validation images, and report the number ofpixels required to reduce the normalized log-odds score by a given percent. We evaluate FIDO undervarious in-filling strategies as well as BBMP with Blur and Random in-filling strategies. We put bothalgorithms on equal footing by using λ = 1e-3 for FIDO and BBMP (see Section A.1 for furthercomparisons). We find that strong generative infilling (VAE and CA) yields more parsimonioussaliency maps, which is consistent with our qualitative comparisons. FIDO-CA can achieve a givennormalized log-odds score suppression using fewer pixels than competing methods. While FIDO-CA may be better adapted to evaluation using CA-GAN, we note that other generative in-fillingapproaches (FIDO-Local and FIDO-VAE) still out-perform heuristic in-filling when evaluated with
Figure 10: Number of salient pixels required to change normalized classification score. Pixelsare sorted by saliency score and successively replaced with CA-GAN in-filled values. We selectλ = 1e-3 for BBMP and FIDO. The lower the better.
Figure 11: Examples from the ablation study. We show how each of our two innovations, FIDOand generative infilling, improve from previous methods that adopts BBMP with hueristics infilling(e.g. Blur and Random). Specifically, we compare with a new variant BBMP-CA that uses stronggenerative in-filling CA-GAN via thresholding the continous masks: we test a variety of decreasingthresholds. We find both FIDO (searching over Bernoulli masks) and generative in-filling (CAGAN)are needed to produce compact saliency maps (the right-most column) that retain class information.
Figure 13: Comparisons of upsampling effect in Mean and CA infilling methods with no totalvariation penalty. We show the upsampling regularization removes the artifacts especially in theweaker infilling method Mean.
Figure 14: Testing the stability of our method with 4 different random seeds. They produce similarsaliency maps. (Using CA infilling method with ResNet and λ = 10-3)A.4 Total Variation EffectHere we test the effect of total variation prior regularization in Figure 15. We find the total variationcan reduce the adversarial artifacts further, while risking losing signals when the total variationpenalty is too strong.
Figure 15: Total Variation Regularization Effect. We show the saliency maps of 4 increasing totalvariation (TV) regularization. We show that strong regularization risks removing signal.
Figure 16: Box plot of the classifier probability under different infilling with respect to randommasked pixels using ResNet under 1, 000 images. We show that generative models (VAE and CA)performs much better in terms of classifier probability.
Figure 17: Batch size effects of final saliency output. We observed unsatisfactory results for batchsize less than 4.
Figure 18: Number of salient pixels required to change normalized classification score withcomparison to BBMP-CA across variety of thresholds. Pixels are sorted by saliency score andsuccessively replaced with CA-GAN in-filled values. The lower the better.
Figure 19: More examples of classifier confidence on infilled images. Realtime denotes the methodof Dabkowski & Gal (2017); FIDO-CA is our method with CA-GAN infilling (Yu et al., 2018).
Figure 20: Additional Saliency Maps for FIDO under total variation 0.01 with a variety of in-fillingmethods. We include the method from Dabkowski & Gal (2017) in the right-most column.
Figure 21: Additional Saliency Maps for FIDO under total variation 0.01 with a variety of in-fillingmethods. We include the method from Dabkowski & Gal (2017) in the right-most column.
Figure 22: Additional examples of ablation study.
