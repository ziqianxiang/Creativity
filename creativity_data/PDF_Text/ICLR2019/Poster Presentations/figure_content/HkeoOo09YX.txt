Figure 1: Comparing the computation graphs of SGHMC and the meta-learned sampler (withmodified forward Euler discretization). Here Fφ and Gφ transformations are defined by Eq. (7).
Figure 2: (Left) Sampler’s bias measured by KL. (Middle) NNSGHMC trajectory plot on a 2D-Gaussian with manually injected gradient noise. (Right) SGHMC plot for the same settings.
Figure 3: Learning curves on test error (top) and negative test LL (bottom).
Figure 4: (Left) The contour plot of function fφQ (Middle) The contour plot for fφD for dimension 1and 2 With fixed -VθU(θ) (Right) The same plot for f φD for dimension 2 and 3 With fixed energy.
Figure 5: Learning curves on test error (top) and negative test LL/100 (bottom).
Figure 6: Test NLL learning curve (with zoom-in for sampling methods) and the best performance.
Figure 7: (Left) The unrolled scheme of the meta sampler updates. Stop gradient operations areapplied to the dashed arrows. (Right) A visualization of cross-chain in-chain training. The grey arearepresents samples across multiple chains, and we compute the cross chain loss for every 5 timesteps. The purple area indicates the samples taken across time with sub-sampled chains 1 and 3. Inthis visualization the initial 15 samples are discarded for burn-in, and the thinning length is τ = 1(effectively no thinning).
Figure 8: We only test the Network Generalization and Activation function generalization. The upperpart indicates the test error plot and lower part are the negative test LL curvefφo Contour with gradient 0.12EmU ① EoWfψo Contour with Momentum -1I-O-⅛3 一 Ps924	-1-16 I8	-2-°	0：0	Q.5	l：0	1；5	2.Q	2.5Energy1-180160-140-120-100-80-60-40
Figure 9: The contour plots of fφD for other input values.
