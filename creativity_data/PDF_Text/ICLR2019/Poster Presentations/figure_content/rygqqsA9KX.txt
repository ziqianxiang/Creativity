Figure 1: Illustration of the proposed Multimodal Factorization Model (MFM) with three modalities. MFMfactorizes multimodal representations into multimodal discriminative factors Fy and modality-specific generativefactors Fa{1.M}. (a) MFM Generative Network with latent variables {Zy, Za{1.M}}, factors {Fy, Fa{1.M}},generated multimodal data Xi：3 and labels Y. (b) MFM Inference Network. (c) MFM Neural Architecture.
Figure 2: (a) MFM generative network for multimodal image dataset SVHN+MNIST, (b) unimodal andmultimodal classification accuracies, and (c) conditional generation for SVHN and MNIST digits. MFM showsimproved capabilities in digit prediction as well as flexible generation of both images based on labels and styles.
Figure 3: Models used in the ablation studies of MFM. Each model removes a design component from ourmodel. Modality reconstruction and sentiment prediction results are reported on CMU-MOSI with best resultsin bold. Factorizing multimodal representations into multimodal discriminative factors and modality-specificgenerative factors are crucial for improved performance.
Figure 4: Analyzing the multimodal represen-tations learnt in MFM via information-based(entire dataset) and gradient-based interpreta-tion methods (single video) on CMU-MOSI.
Figure 5: Recurrent neural architecture for MFM. The encoder Q(ZyIXIM) Can be parametrized byany model that performs multimodal fusion (Nojavanasghari et al., 2016; Zadeh et al., 2018a). Weuse encoder LSTM networks and decoder LSTM networks (Cho et al., 2014) to parametrize functionsQ(ZaIMXi：m) and Fim respectively, and FCNNs to parametrize functions Gy, Ga{iM} and D.
Figure 6: The surrogate inference graphical model to deal with missing modalities in MFM. Redlines denote original inference in MFM and green lines denote surrogate inference to infer latentcodes given present modalities.
