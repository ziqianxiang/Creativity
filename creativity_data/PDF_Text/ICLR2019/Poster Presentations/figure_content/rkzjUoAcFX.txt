Figure 1: Architecture of the WaveNet model for few-shot voice adaptation.
Figure 2: Training (slow, lots of data), adaptation (fast, few data) and inference stages for the SEA-All architecture. The components with bold pink outlines are fine-tuned during the adaptation phase.
Figure 3:	t-SNE visualization of the d-vector embeddings of real and SEA-ALL-generated utterances,for both the LibriSpeech (T ≤ 5 mins) and VCTK (T ≤ 10 mins) evaluation datasets.
Figure 4:	Detection error trade-off (DET) curve for speaker verification in percentage, using theTI-SV speaker verification model (Wan et al., 2018). The utterances were generated using T ≤ 5 andT ≤ 10 minute samples from LibriSpeech and VCTK respectively. EER is marked with a dot.
Figure 5: Cosine similarity of real and generated utterances to the real enrollment set.
Figure 6: ROC curve for real versus generated utterance detection. The utterances were generatedusing models with 5 and 10 minutes of training data per speaker from LibriSpeech and VCTKrespectively. Lower curve indicate that the verification system is having a harder time distinguishingreal from generated samples.
Figure 7: Encoder network architecture for predicting speaker embeddings.
Figure 8:	Detection error trade-off (DET) curve for speaker verification, using the TI-SV speakerverification model (Wan et al., 2018). The utterances were generated using 1 minute or 10 seconds ofutterance from LibriSpeech and VCTK. EER is marked with a dot.
Figure 9:	ROC curve for real vs. generated utterance detection. The utterances were generated using1 minute or 10 seconds of utterance from LibriSpeech and VCTK. Lower curve suggests harder todistinguish real from generated samples.
