Figure 1: The Universal Transformer repeatedly refines a series of vector representations for eachposition of the sequence in parallel, by combining information from different positions usingself-attention (see Eqn 2) and applying a recurrent transition function (see Eqn 4) across all timesteps 1 ≤ t ≤ T. We show this process over two recurrent time-steps. Arrows denote dependenciesbetween operations. Initially, h0 is initialized with the embedding for each symbol in the sequence.
Figure 2: The recurrent blocks of the Universal Transformer encoder and decoder. This diagram omitsposition and time-step encodings as well as dropout, residual connections and layer normalization.
Figure 3: Ponder time ofUT with dynamic halting for encoding facts in a story and question in a bAbItask requiring three supporting facts.
Figure 4: The Universal Transformer with position and step embeddings as well as dropout and layernormalization.
Figure 5: Visualization of the attention distributions, when encoding the question: “Where is Mary?”.
Figure 6: Visualization of the attention distributions, when encoding the question: “Where is theapple?”.
Figure 7: Visualization of the attention distributions, when encoding the question: “Where is the milk?”.
Figure 7: Visualization of the attention distributions, when encoding the question: “Where was theapple before the bathroom?”.
