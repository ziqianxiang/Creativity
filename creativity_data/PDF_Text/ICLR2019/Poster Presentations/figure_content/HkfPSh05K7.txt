Figure 1: Our framework unrolled for two steps. The initial query is encoded and the retriever sendsthe top-k paragraphs to the reader. The multi-step-reasoner component of our model takes in theinternal state of the reader model and the previous query vector and does a gated update to producea reformulated query. This new query vector is used by the retriever to re-rank the paragraphs andsend different paragraphs to the reader. Thus the multi-step-reasoner facilitates iterative interactionbetween the retriever (search engine) and the reader (QA model)Lin et al., 2018) and hence can be computed and stored offline. Given an input question, the retrieverperforms fast inner product search to find the most relevant contexts. The highest ranked contextsare then passed to the neural MRC model. Our architecture is agnostic to the choice of the readerarchitecture and we show that multi-step-reasoning increases performance of two state-of-the-artMRC architectures - DrQA (Chen et al., 2017) and BiDAF (Seo et al., 2017).
Figure 2: Scalability of retriever.
Figure 3: F1 score w.r.t number of steps.
Figure 4: Examples of how multi-step-reasoner iteratively modifies the query by reading context tofind more relevant paragraphs. Figure (left) shows an example where the initial retrieved context didnot have the answer but the context provided enough hint to get more relevant paragraph in the nextstep. In figure (right), both the retrieved paragraph have the answer string leading to a boost in thescore of the answer span because of score aggregation of spans (ยง2.2).
