Figure 1: Propagating an input perturbed with Gaussian noise N(0,0.1) through a fully trainedLeNet. When the same image is perturbed with different noise samples, We observe in the hiddenunits and on the output an empirical distributions shown as Monte Carlo (MC) histograms. Propa-gating the clean image results in the estimate denoted AP1 which may be away from the MC mean.
Figure 2: Propagation for the Heaviside function: Y = [[A≥0]], ReLU: Y = max(0, A) and leakyReLU: Y = max(αA, A). Red: activation function. Black: an exemplary input distribution withmean μ = 3, variance σ2 = 1 shown on the support μ ± 3σ. Dashed blue: the approximate meanμ of the output versus the input mean μ. The variance of the output is shown as blue shaded areaμ ± 3σ0.
Figure 3: Comparison of analytic AP2 dropout with baselines. All methods use AP2 normalizationduring training. Analytic dropout converges to similar values of stochastic dropout and is faster initerations. Both methods are efficient in preventing overfitting as seen in the right plot.
