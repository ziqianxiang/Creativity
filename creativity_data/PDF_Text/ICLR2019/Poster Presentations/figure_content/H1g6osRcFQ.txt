Figure 1: The environments used in our experiments. Environments in the top row are sourceenvironments and environments in the bottom row are the target environments we want to transferthe policy to. (a) Hopper from DART to MuJoCo. (b) Walker2d from DART to MuJoCo withlatency. (c) HalfCheetah from DART to MuJoCo with latency. (d) Minitaur robot from inaccuratemotor modeling to accurate motor modeling. (e) Hopper from rigid to soft foot.
Figure 2: Transfer performance vs Sample number in target environment for the Hopper example.
Figure 3: Transfer performance for the Hopper example. Policies are traiend to transfer from DARTto MuJoCo with different ankle joint limits (horizontal axis). All trials run with total sample numberof 30, 000 in the target environment.
Figure 4: Transfer performance for the Walker2d example. (a) Transfer performance vs samplenumber in target environment on flat surface. (b) Transfer performance vs foot mass, trained with30, 000 samples in the target environment.
Figure 5: Transfer performance for the HalfCheetah example. (a) Transfer performance vs samplenumber in target environment on flat surface. (b) Transfer performance vs surface slope, trainedwith 30, 000 samples in the target environment.
Figure 6: Transfer performance for the Quadruped example (a) and the Soft-foot Hopper example(b).
Figure 7: Comparison of DART and MuJoCo environments under the same control signals. The redcurves represent position or velocity in the forward direction and the green curves represent positionor velocity in the upward direction.
Figure 8: Comparison of SO-CMA and SO-BA for Hopper and Quadruped examples.
Figure 9: Comparison of SO-CMA and SO-MB for Hopper DART-to-MuJoCo transfer.
