Figure 1: DIAYN Algorithm: We update the discriminator to better predict the skill, and update theskill to visit diverse states that make it more discriminable.
Figure 2: (Left) DIAYN skills in a simple navigation environment; (Center) skills can overlap if theyeventually become distinguishable; (Right) diversity of the rewards increases throughout training.
Figure 3: Locomotion skills: Without any reward, DIAYN discovers skills for running, walking,hopping, flipping, and gliding. It is challenging to craft reward functions that elicit these behaviors.
Figure 4: Why use a fixed prior? In contrast toprior work, DIAYN continues to sample all skillsthroughout training.
Figure 5: Policy Initialization: Using a DIAYN skill to initialize weights in a policy accelerateslearning, suggesting that pretraining with DIAYN may be especially useful in resource constrainedsettings. Results are averages across 5 random seeds.
Figure 6: Hierarchical RLeach random seed on all goals. We also compared to Variational Information Maximizing Exploration(VIME) (Houthooft et al., 2016). Note that even the best random seed from VIME significantlyunder-performs DIAYN. This is not surprising: whereas DIAYN learns a set of skills that effectivelypartition the state space, VIME attempts to learn a single policy that visits many states.
Figure 8: DIAYN for Hierarchical RL: By learning a meta-controller to compose skills learned byDIAYN, cheetah quickly learns to jump over hurdles and ant solves a sparse-reward navigation task.
Figure 9: Imitating an expert: DIAYN imitates an expert standing upright, flipping, and faceplanting,but fails to imitate a handstand.
Figure 10: Optimum for Gridworlds: For gridworld environments, we can compute an analyticsolution to the DIAYN objective.
Figure 11: The DIAYN objective prefers skills that (Left) partition states into sets with short bordersand (Right) which correspond to bottleneck states.
Figure 12: Objectives: We plot the two terms from our objective (Eq. 1) throughout training. Whilethe entropy regularizer (blue) quickly plateaus, the discriminability term (orange) term continues toincrease, indicating that our skills become increasingly diverse without collapsing to deterministicpolicies. This plot shows the mean and standard deviation across 5 seeds for learning 20 skills in halfcheetah environment. Note that log2(1/20) ≈ -3, setting a lower bound for log qφ(z | s).
Figure 13: We repeated the experiment from Figure 2 with 5 random seeds to illustrate the robustnessof our method to random seed.
Figure 15: Task reward of skills learned without reward: While our skills are learned without thetask reward function, we evaluate each with the task reward function for analysis. The wide range ofrewards shows the diversity of the learned skills. In the hopper and half cheetah tasks, many skillsachieve large task reward, despite not observing the task reward during training. As discussed in priorwork (Henderson et al., 2017; Duan et al., 2016), standard model-free algorithms trained directly onthe task reward converge to scores of 1000 - 3000 on hopper, 1000 - 5000 on cheetah, and 700 - 2000on ant.
Figure 16: Exploration: We take DIAYN skills learned without a reward function, and evaluateon three natural reward functions: running, jumping, and moving away from the origin. For alltasks, DIAYN learns some skills that perform well. In contrast, a single policy that maximizes anexploration bonus (VIME) performs poorly on all tasks.
Figure 17: Effect of learning p(z): We plot the effective number of skills that are sampled fromthe skill distribution p(z) throughout training. Note how learning p(z) greatly reduces the effectivenumber on inverted pendulum and mountain car. We show results from 3 random seeds for eachenvironment.
Figure 18: Learning p(z) with varying number of skills: We repeat the experiment in Figure 4 forvarying sizes of z. Regardless of the size of z, learning p(z) causes the effective number of skillsto drop to less than 10. The two subplots show the same data (Left) on a linear scale and (Right)logarithmic scale. We plot the mean and standard deviation across 3 random seeds.
Figure 19: Visualizing Skills: For every skill, we collect one trajectory and plot the agent’s Xcoordinate across time. For inverted pendulum (top), we only plot skills that balance the pendulum.
Figure 20: Half cheetah skills: We show skills learned by half-cheetah with no reward.
Figure 21: Hopper Skills: We show skills learned by hopper with no reward.
Figure 22: Ant skills: We show skills the ant learns without any supervision. Ant learns (top row) tomove right, (middle row) to move left, (bottom row, left to right) to move up, to move down, to flipon its back, and to rotate in place.
Figure 23: Imitating an expert: Across 600 imitation tasks, we find our method more closelymatches the expert than all baselines.
