Figure 1: (a) shows generalizationerror v.s. effective load α using a linearstudent (identity units). ‘WN+gammadecay, has two curves Z = ɪ and2Mζ = 0.25. BN is trained with M =32. (b) shows generalization error v.s.
Figure 2: (a) & (b) compare the loss (both training and evaluation) and validation accuracy between BN andPN on CIFAR10 using a ResNet18 network; (c) & (d) compare the training and validation loss curve with WN +mean-only BN and WN + variance-only BN; (e) & (f) validate the regularization effect of BN on both γ2 andthe validation loss with different batch sizes; (g) & (h) show the loss and top-1 validation accuracy of ResNet18with additional regularization (dropout) on large-batch training of BN and WN.
Figure 3: Results of downsampled ImageNet. (a) plots training and evaluation loss. (b) shows validationaccuracy. The models are trained on 8 GPUs.
Figure 4: Study of parameter norm. Vanilla SGD is finetuned from a network pretrained by BN on CIFAR10.
