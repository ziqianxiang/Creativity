Table 1: Accuracy of domain classification and digit classificationTrain	Test	Baseline	HEX	HEX-ADV	HEX-ALLA, W	D	0.405±0.016	0.343±0.030	0.343±0.030	0.216±0.119D, W	A	0.112±0.008	0.147±0.004	0.147±0.004	0.055±0.004A, D	W	0.400±0.016	0.378±0.034	0.378±0.034	0.151±0.008Table 2: Accuracy on Office data set with extracted features. The Baseline refers to MLP withSURF features. The HEX methods refer to adding another MLP with features extracted by tradi-tional GLCM methods. Because D and W are similar domains (same obejcts even share the samebackground), we believe these results favor the HEX method (see Section 4.1.2) for duscussion).
Table 2: Accuracy on Office data set with extracted features. The Baseline refers to MLP withSURF features. The HEX methods refer to adding another MLP with features extracted by tradi-tional GLCM methods. Because D and W are similar domains (same obejcts even share the samebackground), we believe these results favor the HEX method (see Section 4.1.2) for duscussion).
Table 3: Accuracy on MNIST-Rotation data set4.4	MNIST with Rotation as DomainWe continue to compare HEX with other state-of-the-art DG methods (that use distribution labels)on popular DG data sets. We experimented with the MNIST-rotation data set, on which many DGmethods have been tested. The images are rotated with different degrees to create different domains.
Table 4: Testing Accuracy on PACSsee slight drop of performance because NGLCM also learns some semantic information, which isthen projected out. Also, training all the model parameters simultaneously may lead into a trivialsolution where FG (in Equation 3) learns garbage information and HEX degenerates to the baselinemodel. To overcome these limitations, we invented several training heuristics, such as optimizingFP and FG sequentially and then fix some weights. However, we did not report results with trainingheuristics (expect for PACS experiment) because we hope to simplify the methods. Another limita-tion we observe is that sometimes the training performance of HEX fluctuates dramatically duringtraining, but fortunately, the model picked up by highest validation accuracy generally performs bet-ter than competing methods. Despite these limitations, we still achieved impressive performance onboth synthetic and popular DG data sets.
Table A1: Accuracy in classifying semantic and superficial informationFrom results in Table A1, we can see that GLCM suits our goal very well: GLCM outperforms othermethods in most cases in classifying textural patterns while predicts least well in the semantic tasks.
