title,year,conference
 Weighted transformer network for ma-chine translation,2017, arxiv
 Layer normalization,2016,	arXiv
 Neural machine translation by jointlylearning to align and translate,2015, In Proc
 Deep communicating agents forabstractive summarization,2018, In Proc
 Locally-connected and convolutional neural networks for small footprint speakerrecognition,2015, In Proc
 Xception: Deep learning with depthwise separable convolutions,2017, In Proc
 Language modeling with gatedconvolutional networks,2017, In Proc
 Latent alignment andvariational attention,2018, arXiv
 Classicalstructured prediction losses for sequence to sequence learning,2018, In Proc
 Pervasive attention: 2d convolutional neuralnetworks for sequence-to-sequence prediction,2018, arXiv
 Controllable abstractive summarization,2017, arXiv
 A Convolutional EncoderModel for Neural Machine Translation,2016, arXiv
 ConvolutionalSequence to Sequence Learning,2017, In Proc
 Convolutional inter-action network for natural language inference,2018, In Proc
 Efficientsoftmax approximation for gpus,2016, arXiv
 Non-autoregressiveneural machine translation,2018, In International Conference on Learning Representations
 Non-autoregressive neuralmachine translation with enhanced decoder input,2019, AAAI
 Achieving human parity onautomatic chinese to english news translation,2018, arXiv
 Deep Residual Learning for ImageRecognition,2015, In Proc
 Teaching machines to read and comprehend,2015, In Proc
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Exploring thelimits of language modeling,2016, arXiv
 Depthwise separable convolutions for neuralmachine translation,2017, arXiv
 Fast decoding in sequence models using discrete latent variables,2018, In Proceedings of the35th International Conference on Machine Learning (ICML)
 Neural Machine Translation in Linear Time,2016, arXiv
 Adam: A Method for Stochastic Optimization,2015, In Proc
 Gradient-based learning applied todocument recognition,1998, Proc
 Deterministic non-autoregressive neural se-quence modeling by iterative refinement,2018, arXiv
 Rouge: A package for automatic evaluation of summaries,2004, In Workshop on TextSummarization Branches Out
 Generating wikipedia by summarizing long sequences,2018, arXiv
 SGDR: Stochastic Gradient Descent with Warm Restarts,2016, arXiv
 Effective approaches to attention-basedneural machine translation,2015, In Proc
 Abstractive text summarizationusing sequence-to-sequence rnns and beyond,2016, In Proc
 On the difficulty of training recurrent neuralnetworks,2013, In Proc
 A deep reinforced model for abstractivesummarization,2017, arXiv
 Regular-izing neural networks by penalizing confident output distributions,2017, In Proc
 Using the output embedding to improve language models,2017, In Proc
 Get to the point: Summarization with pointer-generator networks,2017, In Proc
 Neural machine translation of rare words withsubword units,2016, In Proc
 Self-attention with relative position representa-tions,2018, In Proc
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, InProc
 Learning context-sensitiveconvolutional filters for text processing,2018, In Proc
 Bi-directional block self-attention for fast and memory-efficient sequence modeling,2018, arXiv
 Fast directional self-attention mechanism,2018, arXiv
 Rigid-motion scattering for image classification,2014, Ph
 On the importance ofinitialization and momentum in deep learning,2013, In ICML
 Sequence to Sequence Learning with Neural Net-works,2014, In Proc
 Re-thinking the Inception Architecture for Computer Vision,2015, arXiv
 Deepface: Closing the gap tohuman-level performance in face verification,2014, In Proc
 Why self-attention? a targetedevaluation of neural machine translation architectures,2018, In Proc
 Attention Is All You Need,2017, In Proc
 Regularization of neuralnetworks using dropconnect,2013, In Proc
 Accelerating neural transformer via an average attentionnetwork,2018, arXiv
