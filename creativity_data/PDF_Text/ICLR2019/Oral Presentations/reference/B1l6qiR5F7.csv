title,year,conference
 Tree-structured decoding with doubly-recurrent neuralnetworks,2016, 2016
 Learning deep architectures for ai,2009, Foundations and trendsR in MachineLearning
 Recursive neural networks canlearn logical semantics,2014, arXiv preprint arXiv:1406
 Tree-structured composition inneural networks without tree-structured architectures,2015, arXiv preprint arXiv:1506
 A fast unified model for parsing and sentence understanding,2016, arXiv preprintarXiv:1603
 Immediate-head parsing for language models,2001, In Proceedings of the 39th AnnualMeeting on Association for Computational Linguistics
 Bayesian grammar induction for language modeling,1995, In Proceedings of the 33rdannual meeting on Association for Computational Linguistics
 Three models for the description of language,1956, IRE Transactions on informationtheory
 Aspects of the Theory of Syntax,1965, The MIT Press
 Hierarchical multiscale recurrent neural net-works,2016, arXiv preprint arXiv:1609
 Unsupervised structure prediction with non-parallel multilingual guidance,2011, In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing
 Hierarchical recurrent neural networks for long-term dependen-cies,1996, In Advances in neural information processing systems
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Advances in neural information processing systems
 Lstm recurrent networks learn simple context-free and context-sensitive languages,2001, IEEE Transactions on Neural Networks
 Improving neural language models with acontinuous cache,2016, arXiv preprint arXiv:1612
 Colorlessgreen recurrent networks dream hierarchically,2018, In Proc
 Grammar induction with neural languagemodels: An unusual replication,2018, arXiv preprint arXiv:1808
 Tying word vectors and word classifiers: Aloss framework for language modeling,2016, arXiv preprint arXiv:1611
 Learning hierarchicalstructures on-the-fly with a recurrent-recursive model for sequences,2018, In Proceedings of The ThirdWorkshop on Representation Learning for NLP
 Inferring algorithmic patterns with stack-augmented recurrentnets,2015, In Advances in neural information processing systems
 Character-aware neural languagemodels,2016, In AAAI
 A generative constituent-context model for improved gram-mar induction,2002, In Proceedings of the 40th Annual Meeting on Association for ComputationalLinguistics
 Natural language grammar induction with a generativeconstituent-context model,2005, Pattern recognition
 A clockwork rnn,2014, arXivpreprint arXiv:1402
 Deep learning,2015, Nature
 Learning long-term dependencies is notas difficult with narx recurrent neural networks,1998, Technical report
 Assessing the ability of lstms to learn syntax-sensitive dependencies,2016, arXiv preprint arXiv:1611
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational linguistics
 Targeted syntactic evaluation of language models,2018, arXiv preprintarXiv:1808
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Regularizing and Optimizing LSTMLanguage Models,2017, arXiv preprint arXiv:1708
 Statistical language models based on neural networks,2012, Presentation at Google
 Using the output embedding to improve language models,2017, In Proceedingsof the 15th Conference of the European Chapter of the Association for Computational Linguistics:Volume 2
 Learning ordered representations with nesteddropout,2014, In International Conference on Machine Learning
 Neural sequence chunkers,1991, 1991
 Deep learning in neural networks: An overview,2015, Neural networks
 Gradient estimation usingstochastic computation graphs,2015, In Advances in Neural Information Processing Systems
 Neural language modeling byjointly learning syntax and lexicon,2017, arXiv preprint arXiv:1711
 On tree-based neural sentence modeling,2018, arXivpreprint arXiv:1808
 Improved semantic representationsfrom tree-structured long short-term memory networks,2015, arXiv preprint arXiv:1503
 A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
 Sequence-to-dependencyneural machine translation,2017, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Breaking the softmaxbottleneck: A high-rank rnn language model,2017, arXiv preprint arXiv:1711
 Learning tocompose words into sentences with reinforcement learning,2016, arXiv preprint arXiv:1611
 Memory architectures in recurrent neural network language models,2018, 2018
 Generativeneural machine for tree structures,2017, CoRR
 Recurrenthighway networks,2016, arXiv preprint arXiv:1607
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
