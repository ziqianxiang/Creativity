title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, ICML
 A closer look atmemorization in deep networks,2017, In International Conference on Machine Learning
 Understanding dropout,2013, In Advances in neural informationprocessing systems
 Deep rewiring: Trainingvery sparse deep networks,2018, Proceedings of ICLR
 ConveXneural networks,2006, In Advances in neural information processing systems
 Randomout: Using a convolutional gradient norm towin the filter lottery,2016, ICLR Workshop
 Inductive bias of deep convolutional networks through poolinggeometry,2016, arXiv preprint arXiv:1605
 Predicting parameters in deeplearning,2013, In Advances in neural information processing systems
 Learning to prune deep neural networks via layer-wiseoptimal brain surgeon,2017, In Advances in Neural Information Processing Systems
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Dropout as a bayesian approXimation: Representing modeluncertainty in deep learning,2016, In international conference on machine learning
 Concrete dropout,2017, In Advances in Neural InformationProcessing Systems
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Dynamic network surgery for efficient dnns,2016, In AdvancesIn Neural Information Processing Systems
 Dsd: Regularizing deep neural networks with dense-sparse-densetraining flow,2017, Proceedings of ICLR
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, In Advances in neural information processing systems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Network trimming: A data-drivenneuron pruning approach towards efficient deep architectures,2016, arXiv preprint arXiv:1607
 Training skinny deep neural networkswith iterative hard thresholding methods,2016, arXiv preprint arXiv:1607
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Variational dropout and the local reparameteri-zation trick,2015, In Advances in Neural Information Processing Systems
 Learning multiple layers of features from tiny images,2009, 2009
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Measuring the intrinsic dimensionof objective landscapes,2018, Proceedings of ICLR
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Rethinking the valueof network pruning,2019, In International Conference on Learning Representations
 Bayesian compression for deep learning,2017, InAdvances in Neural Information Processing Systems
 Learning sparse neural networks throughl_0 regularization,2018, Proceedings of ICLR
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, arXiv preprint arXiv:1707
 Diversity networks,2016, Proceedings of ICLR
 Variational dropout sparsifies deep neuralnetworks,2017, arXiv preprint arXiv:1701
 Exploring sparsity in recurrentneural networks,2017, Proceedings of ICLR
 Structured bayesianpruning via log-normal multiplicative noise,2017, In Advances in Neural Information ProcessingSystems
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Occamâ€™s razor,2001, In T
 Stochastic complexity and modeling,1986, The annals of statistics
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Data-free parameter pruning for deep neural networks,2015, arXivpreprint arXiv:1507
 Generalized dropout,2016, arXiv preprint arXiv:1611
 Regularization of neuralnetworks using dropconnect,2013, In International Conference on Machine Learning
 Designing energy-efficient convolutional neuralnetworks using energy-aware pruning,2017, arXiv preprint
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Compressibilityand generalization in large-scale deep learning,2018, arXiv preprint arXiv:1804
