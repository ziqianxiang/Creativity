Figure 1:	Humans learn visual concepts, words, and semantic parsing jointly and incrementally.
Figure 2:	We propose to use neural symbolic reasoning as a bridge to jointly learn visual concepts,words, and semantic parsing of sentences.
Figure 3: We treat attributes such as Shape and Color as neural operators. The operators mapobject representations into a visual-semantic space. We use similarity-based metric to classify objects.
Figure 4: A. Demonstration of the curriculum learning of visual concepts, words, and semantic parsingof sentences by watching images and reading paired questions and answers. Scenes and questions ofdifferent complexities are illustrated to the learner in an incremental manner. B. Illustration of ourneuro-symbolic inference model for VQA. The perception module begins with parsing visual scenesinto object-based deep representations, while the semantic parser parse sentences into executableprograms. A symbolic execution process bridges two modules.
Figure 5: We test the combinatorial gen-eralization w.r.t. the number of objectsin scenes and the complexity of ques-tions (i.e. the depth of the program trees).
Figure 6: Samples collected from four splits in Section 4.3 for illustration. Models are trained onsplit A but evaluated on all splits for testing the combinatorial generalization.
Figure 7: Left: An example image-question pair from the VQS dataset and the correspondingexecution trace of NS-CL. Right: Results on the VQS test set. Our model achieves a comparableresults with the baselines.
Figure 8: Concepts learned from VQS, including object categories, attributes, and relations.
Figure 9:	An example image and a related question-answering pair from the Minecraft dataset.
Figure 10:	An example image from the VQS dataset. The orange bounding boxes are object proposals.
Figure 11: Visualization of the execution trace generated by our Neuro-Symbolic Concept Learneron the CLEVR dataset. Example A and B are successful executions that generate correct answers.
Figure 12: Exemplar execution trace generated by our Neuro-Symbolic Concept Learner on theMinecraft reasoning dataset. Example A, B and C are successful execution. Example C demonstratesthe semantics of the FilterMost operation. Example D shows a failure case: the detection model failsto detect a pig hiding behind the big tree.
Figure 13: Illustrative execution trace generated by our Neuro-Symbolic Concept Learner on theVQS dataset. Execution traces A and B shown in the figure leads to the correct answer to the question.
Figure 14: Concepts learned on the CLEVR dataset.
Figure 15: Concepts learned on the Minecraft dataset.
