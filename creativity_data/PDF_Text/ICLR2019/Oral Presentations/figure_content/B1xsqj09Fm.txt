Figure 1: Class-conditional samples generated by our model.
Figure 2: (a) The effects of increasing truncation. From left to right, the threshold is set to 2, 1, 0.5,0.04. (b) Saturation artifacts from applying truncation to a poorly conditioned model.
Figure 3: A typical plot of the first singular value σ0 in the layers of G (a) and D (b) before SpectralNormalization. Most layers in G have well-behaved spectra, but without constraints a small sub-set grow throughout training and explode at collapse. D’s spectra are noisier but otherwise better-behaved. Colors from red to violet indicate increasing depth.
Figure 4: Samples from our BigGAN model with truncation threshold 0.5 (a-c) and an example ofclass leakage in a partially trained model (d).
Figure 5: Samples generated by our BigGAN model at 256×256 resolution.
Figure 6: Samples generated by our BigGAN model at 512×512 resolution.
Figure 7: Comparing easy classes (a) with difficult classes (b) at 512×512. Classes such as dogswhich are largely textural, and common in the dataset, are far easier to model than classes involvingunaligned human faces or crowds. Such classes are more dynamic and structured, and often havedetails to which human observers are more sensitive. The difficulty of modeling global structure isfurther exacerbated when producing high-resolution images, even with non-local blocks.
Figure 8: Interpolations between z, c pairs.
Figure 9: Interpolations between c with z held constant. Pose semantics are frequently maintainedbetween endpoints (particularly in the final row). Row 2 demonstrates that grayscale is encoded inthe joint z, c space, rather than in z.
Figure 10: Nearest neighbors in VGG-16-fc7 (Simonyan & Zisserman, 2015) feature space. Thegenerated image is in the top left.
Figure 11: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space. The generatedimage is in the top left.
Figure 12: Nearest neighbors in pixel space. The generated image is in the top left.
Figure 13: Nearest neighbors in VGG-16-fc7 (Simonyan & Zisserman, 2015) feature space. Thegenerated image is in the top left.
Figure 14: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space. The generatedimage is in the top left.
Figure 15: (a) A typical architectural layout for BigGAN,s G; details are in the following tables.
Figure 16: (a) A typical architectural layout for BigGAN-deep's G; details are in the followingtables. (b) A Residual Block (ResBlock up) in BigGAN-deep's G. (c) A Residual Block (ResBlockdown) in BigGAN-deep’s D. A ResBlock (without up or d^wn) in BigGAN-deep does not includethe UPsamPle or Average Pooling layers, and has identity skip connections.
Figure 17: IS vs. FID at 128×128. Scores are averaged across three random seeds.
Figure 18: IS vs. FID at 256 and 512 pixels. Scores are averaged across three random seeds for 256.
Figure 19: JFT-300M IS vs(top) and from σ = 0.5 tocurve labeled with baselinetechniques disabled), whilecapacities (Ch).
Figure 20: Training statistics for a typical model without special modifications. Collapse occursafter 200000 iterations.
Figure 21: G training statistics with σ0 in G regularized towards 1. Collapse occurs after 125000iterations.
Figure 22: D training statistics with σ0 in G regularized towards 1. Collapse occurs after 125000iterations.
Figure 23: G training statistics with an R1 Gradient Penalty of strength 10 on D. This model doesnot collapse, but only reaches a maximum IS of 55.
Figure 24: D training statistics with an R1 Gradient Penalty of strength 10 on D. This model doesnot collapse, but only reaches a maximum IS of 55.
Figure 25: G training statistics with Dropout (keep probability 0.8) applied to the last feature layerof D. This model does not collapse, but only reaches a maximum IS of 70.
Figure 26: D training statistics with Dropout (keep probability 0.8) applied to the last feature layerof D. This model does not collapse, but only reaches a maximum IS of 70.
Figure 27: Additional training statistics for a typical model without special modifications. Collapseoccurs after 200000 iterations.
Figure 28: Additional training statistics with an R1 Gradient Penalty of strength 10 on D. This modeldoes not collapse, but only reaches a maximum IS of 55.
Figure 29: A closeup of D’s spectra at a noise spike.
