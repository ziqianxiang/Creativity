Figure 1: Diagram of TD-VAE. Follow the red panels for an explanation of the architecture. Forsuccinctness, we use the notation pD to denote the decoder p(x|z), pT to denote the transitiondistribution p(st2 |st1), qS for the smoothing distribution and pB for the belief distribution.
Figure 2: MiniPacman. Left: A full frame from the game (size 15 × 19). Pacman (green) isnavigating the maze trying to eat all the food (blue) while being chased by a ghost (red). Top right:A sequence of observations, consisting of consecutive 5 × 5 windows around Pacman. Bottom right:ELBO and estimated negative log probability on a test set of MiniPacman sequences. Lower is better.
Figure 3: Moving MNIST. Left: Rows are example input sequences. Right: Jumpy rollouts fromthe model. We see that the model is able to roll forward by skipping frames, keeping the correct digitand the direction of motion.
Figure 4: Skip-state prediction for 1D signal. The input is generated by a noisy harmonic oscillator.
Figure 5: Beliefs of the model. Left: Independent samples z1, z2, z3 from current belief; all 3 decodeto roughly the same frame. Right: Multiple predicted futures for each sample. The frames are similarfor each zi, but different across zi ’s.
Figure 6: Rollout from the model. The model was trained on steps uniformly distributed in [1, 5].
Figure 7: Recurrent variational auto-encoder. General recurrent variational auto-encoder, obtainedby imposing recurrent structure, forward sampling and allowing all potential connections. Note thatthe encoder can have several alternating layers of forward and backward RNNs. Also note that theconnection 1 has to be absent if the backwards encoder is used. Possible skip connections are notshown as they can directly be implemented in the RNN weights. If connections 2 are absent, themodel is capable of forward sampling in latent space without going back to observations.
Figure 8: Deep version of the model from Figure 1. A deep version of the model is formed bycreating a layer similar to the shallow model of Figure 1 and replicating it. Both sampling andinference proceed downwards through the layers. Circles have the same meaning as in Figure 1 andare implemented using neural networks, such as LSTMs.
