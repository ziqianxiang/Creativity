Published as a conference paper at ICLR 2019
Deterministic Variational Inference for
Robust Bayesian Neural Networks
AnqiWu1； Sebastian Nowozin2^ Edward Meeds4,
Richard E. Turner3,4, Jose Miguel Hernandez-Lobato3,4 &
Alexander L. Gaunt4
1	Princeton Neuroscience Institute, Princeton University
2	Google AI Berlin
3	Department of Engineering, University of Cambridge
4	Microsoft Research, Cambridge
anqiw@princeton.edu, nowozin@google.com,
{ret26,jmh233}@cam.ac.uk, {ted.meeds, algaunt}@microsoft.com
Ab stract
Bayesian neural networks (BNNs) hold great promise as a flexible and principled
solution to deal with uncertainty when learning from finite data. Among approaches
to realize probabilistic inference in deep neural networks, variational Bayes (VB)
is theoretically grounded, generally applicable, and computationally efficient. With
wide recognition of potential advantages, why is it that variational Bayes has seen
very limited practical use for BNNs in real applications? We argue that variational
inference in neural networks is fragile: successful implementations require careful
initialization and tuning of prior variances, as well as controlling the variance of
Monte Carlo gradient estimates. We provide two innovations that aim to turn VB
into a robust inference tool for Bayesian neural networks: first, we introduce a novel
deterministic method to approximate moments in neural networks, eliminating
gradient variance; second, we introduce a hierarchical prior for parameters and
a novel Empirical Bayes procedure for automatically selecting prior variances.
Combining these two innovations, the resulting method is highly efficient and
robust. On the application of heteroscedastic regression we demonstrate good
predictive performance over alternative approaches.
1	Introduction
Bayesian approaches to neural network training marry the representational flexibility of deep neural
networks with principled parameter estimation in probabilistic models. Compared to “standard”
parameter estimation by maximum likelihood, the Bayesian framework promises to bring key
advantages such as better uncertainty estimates on predictions and automatic model regularization
(MacKay, 1992; Graves, 2011). These features are often crucial for informing downstream decision
tasks and reducing overfitting, particularly on small datasets. However, despite potential advantages,
such Bayesian neural networks (BNNs) are often overlooked due to two limitations: First, posterior
inference in deep neural networks is analytically intractable and approximate inference with Monte
Carlo (MC) techniques can suffer from crippling variance given only a reasonable computation
budget (Kingma et al., 2015; Molchanov et al., 2017; Miller et al., 2017; Zhu et al., 2018). Second,
performance of the Bayesian approach is sensitive to the choice of prior (Neal, 1993), and although
we may have a priori knowledge concerning the function represented by a neural network, it is
generally difficult to translate this into a meaningful prior on neural network weights. Sensitivity to
priors and initialization makes BNNs non-robust and thus often irrelevant in practice.
In this paper, we describe a novel approach for inference in feed-forward BNNs that is simple to
implement and aims to solve these two limitations. We adopt the paradigm of variational Bayes (VB)
for BNNs (Hinton & van Camp, 1993; MacKay, 1995c) which is normally deployed using Monte
*Work done during an internship at Microsoft Research, Cambridge.
^ Work done while at Microsoft Research, Cambridge.
1
Published as a conference paper at ICLR 2019
Carlo variational inference (MCVI) (Graves, 2011; Blundell et al., 2015). Within this paradigm we
address the two shortcomings of current practice outlined above: First, we address the issue of high
variance in MCVI, by reducing this variance to zero through novel deterministic approximations to
variational inference in neural networks. Second, we derive a general and robust Empirical Bayes (EB)
approach to prior choice using hierarchical priors. By exploiting conjugacy we derive data-adaptive
closed-form variance priors for neural network weights, which we experimentally demonstrate to be
remarkably effective.
Combining these two novel ingredients gives us a performant and robust BNN inference scheme
that we refer to as “deterministic variational inference” (DVI). We demonstrate robustness and
improved predictive performance in the context of non-linear regression models, deriving novel
closed-form results for expected log-likelihoods in homoscedastic and heteroscedastic regression
(similar derivations for classification can be found in the appendix).
Experiments on standard regression datasets from the UCI repository, (Dheeru & Karra Taniskidou,
2017), show that for identical models DVI converges to local optima with better predictive log-
likelihoods than existing methods based on MCVI. In direct comparisons, we show that our Empirical
Bayes formulation automatically provides better or comparable test performance than manual tuning
of the prior and that heteroscedastic models consistently outperform the homoscedastic models.
Concretely, our contributions are:
•	Development of a deterministic procedure for propagating uncertain activations through
neural networks with uncertain weights and ReLU or Heaviside activation functions.
•	Development of an EB method for principled tuning of weight priors during BNN training.
•	Experimental results showing the accuracy and efficiency of our method and applicability to
heteroscedastic and homoscedastic regression on real datasets.
2	Variational Inference in Bayesian Neural Networks
We start by describing the inference task that our method must solve to successfully train a BNN.
Given a model M parameterized by weights w and a dataset D = (x, y), the inference task is
to discover the posterior distribution p(w|x, y). A variational approach acknowledges that this
posterior generally does not have an analytic form, and introduces a variational distribution q(w; θ)
parameterized by θ to approximate p(w|x, y). The approximation is considered optimal within the
variational family for θ* that minimizes the KUllback-Leibler (KL) divergence between q and the
true posterior.
θ* = argminDKL [q(w;θ)∣∣p(w∣x,y)].
θ
IntrodUcing a prior p(w) and applying Bayes rUle allows Us to rewrite this as optimization of the
qUantity known as the evidence lower boUnd (ELBO):
θ* = argmax{Ew〜q [logp(y∣w, x)] - DKL [q(w; θ)∣∣p(w)]} .	(1)
θ
Analytic resUlts exist for the KL term in the ELBO for carefUl choice of prior and variational
distribUtions (e.g. GaUssian families). However, when M is a non-linear neUral network, the first
term in eqUation 1 (referred to as the reconstrUction term) cannot be compUted exactly: this is where
MC approximations with finite sample size S are typically employed:
1S
Ew〜q [logp(y∣w, x)] ≈ s ElOgp(y∣w(s), x), W⑶〜q(w; θ).	⑵
s=1
OUr goal in the next section is to develop an explicit and accUrate approximation for this expectation,
which provides a deterministic, closed-form expectation calcUlation, stabilizing BNN training by
removing all stochasticity dUe to Monte Carlo sampling.
3	Deterministic Variational Approximation
Figure 1 shows the architecture of the computation of Ew〜q [logp(D∣w)] for a feed-forward neural
network. The compUtation can be divided into two parts: first, propagation of activations thoUgh
2
Published as a conference paper at ICLR 2019
Figure 1: Architecture of a Bayesian
neural network. Computation is divided
into (a) propagation of activations (a)
from an input x and (b) computation of
a log-likelihood function L for outputs
y. Weights are represented as high di-
mensional variational distributions (blue)
that induce distributions over activations
(yellow). MCVI computes using sam-
ples (dots); our method propagates a full
distribution.
parameterized layers and second, evaluation of an unparameterized log-likelihood function (L). In
this section, we describe how each of these stages is handled in our deterministic framework.
3.1	Moment Propagation
We begin by considering activation propagation (figure 1(a)), with the aim of deriving the form
of an approximation q(aL) to the final layer activation distribution q(aL) that will be passed to
the likelihood computation. We compute aL by sequentially computing the distributions for the
activations in the preceding layers. Concretely, we define the action of the lth layer that maps a(l-1)
to al as follows:
hl = f(a(l-1)),
al =hlWl+bl,
where f is a non-linearity and {Wl, bl} ⊂ w are random variables representing the weights and
biases of the lth layer that are assumed independent from weights in other layers. For notational
clarity, in the following we will suppress the explicit layer index l, and use primed symbols to denote
variables from the (l - 1)th layer, e.g. a0 = a(l-1). Note that we have made the non-conventional
choice to draw the boundaries of the layers such that the linear transform is applied after the non-
linearity. This is to emphasize that al is constructed by linear combination of many distinct elements
of h0 , and in the limit of vanishing correlation between terms in this combination, we can appeal
to the central limit theorem (CLT). Under the CLT, for a large enough hidden dimension and for
variational distributions with finite first and second moments, elements ai will be normally distributed
regardless of the potentially complicated distribution for hj induced by f1. We empirically observe
that this claim is approximately valid even when (weak) correlations appear between the elements of
h during training (see section 3.1.1).
Having argued that a adopts a Gaussian form, it remains to compute the first and second moments. In
general, these cannot be computed exactly, so we develop an approximate expression. An overview
of this derivation is presented here with more details in appendix A. First, we model W, b and h as
independent random variables, allowing us to write:
haii = hhji hWjii + hbii ,
Cov(ai, ak) = hhjhli Cov(Wji, Wlk) + hWjii Cov(hj, hl) hWlki + Cov(bi, bk),	(3)
where we have employed the Einstein summation convention and used angle brackets to indicate
expectation over q. If we choose a variational family with analytic forms for weight means and
covariances (e.g. Gaussian with variational parameters hWjii and Cov(Wji, Wlk)), then the only
difficult terms are the moments of h:
hhji ∞ / f(αj)exp - (% 2∑tji)	dαj,	(4)
hhjhιi ~ /f(%)fSI)eχp - 2 (H)(∑ij II；)i(αj-ha" daj dαι ,⑸
1We are also required to choose a Gaussian variational approximation for b to preserve the Gaussian
distribution of a.
3
Published as a conference paper at ICLR 2019
	A(〃1,〃2 ,P)	Q(M1,μ2,P)	
Heaviside	Φ(μ1)Φ(μ2) U	SR(〃1)SR(μ2) ReLU	+ PΦ(μ1)Φ(μ2)	-log(ghp) + 2g⅛ hμ1 + μ - 1⅛μ1μ2i + O(M4) Tog(g∏∙) + hE) (〃2 + 〃2) -ar号产μ1μ2i + O(μ4)
Table 1: Forms for the components of the approximation in equation 6 for Heaviside and ReLU
non-linearities. Φ is the CDF of a standard Gaussian, SR is a “soft ReLU" that We define as SR(X)=
φ(x) + xΦ(x) where φ is a standard Gaussian, P =，1 - ρ2, gh = arcsin P and gr = gh + 1++^
Where We have used the Gaussian form of a0 parameterized by mean ha0i and covariance Σ0, and
for brevity we have omitted the normalizing constants. Closed form solutions for the integral in
equation 4 exist for Heaviside or ReLU choices of non-linearity f (see appendix A). Furthermore, for
these non-linearities, the a0j → ±∞ and ha0li → ±∞ asymptotes of the integral in equation 5 have
closed form. Figure 2 shows schematically how these asymptotes can be used as a first approximation
for equation 5. This approximation is improved by considering that (by definition) the residual decays
to zero far from the origin in the ( a0j , ha0li) plane, and so is well modelled by a decaying function
exp[-Q( a0j , ha0li , Σ0)], where Q is a polynomial in ha0i with a dominant positive even term. In
practice we truncate Q at the quadratic term, and calculate the polynomial coefficients by matching
the moments of the resulting Gaussian with the analytic moments of the residual. Specifically, using
dimensionless variables μi =(4)/∖∕∑ii and Pij = ∑j∕∖ Σ[i∑jj, this improved approximation
takes the form
hhjhl i = Sj1 {4μj,μl, PjI) + eχp [-Q(μj , μi, PjI)]},
(6)
where the expressions for the dimensionless asymptote A
and quadratic Q are given in table table 1 and derived in
appendix A.2.1 and A.2.2. The dimensionful scale fac-
tor Sj0 l is 1 for a Heaviside non-linearity or Σ0jl /P0jl for
ReLU. Using equation 6 in equation 3 gives a closed form
approximation for the moments of a as a function of mo-
ments ofa0. Since a is approximately normally distributed
by the CLT, this is sufficient information to sequentially
propagate moments all the way through the network to
compute the mean and covariances of q(aL), our explicit
multivariate Gaussian approximation to q(aL). Any deep
learning framework supporting special functions arcsin
and Φ will immediately support backpropagation through
the deterministic expressions we have presented. Below
we briefly empirically verify the presented approximation,
and in section 3.2 we will show how it is used to compute
an approximate log-likelihood and posterior predictive
distribution for regression and classification tasks.
3.1.1	Empirical Verification
Approximation accuracy The approximation derived
above relies on three assumptions. First, that some form of
CLT holds for the hidden units during training where the
iid assumption of the classic CLT is not strictly enforced;
second, that a quadratic truncation of Q is sufficient2 ; and
third that there are only weak correlation between layers
so that they can be represented using independent vari-
ables in the variational distribution. To provide evidence
that these assumptions hold in practice, we train a small
ReLU network with two hidden layers each of 128 units
Figure 2: Approximation of hhjhli using
an asymptote and Gaussian correction for
(a) Heaviside and (b) ReLU non-linearities.
Yellow functions have closed-forms, and blue
indicates residuals. The examples are plotted
for —6 < μ < 6 and Pjl = 0.5, and the
relative magnitude of each correction term is
indicated on the vertical axis.
2Additional Taylor expansion terms can be computed if this assumption fails.
4
Published as a conference paper at ICLR 2019
(C)
O 1
y 一
Cb)2
Data samples
-- Data 1-std
口 Model 1-std ,
00ug≡IRIHalojem
q(m) r 口 MC —ours Jl	IL
-10	0	10
5
—0
▲
O
lX
-
00ug≡wH自JV 一
-20 -10	0	10	20
Ki~1 Jl	
6
-Om
8
I
Figure 3: Empirical accuracy of our approximation on toy 1-dimensional data. (a) We train a 2 layer ReLU
network to perform heteroscedastic regression on the dataset shown in (b) and obtain the fit shown in blue. (c)
The output distributions for the activation units m and ` evaluated at x = 0.25 are in excellent agreement with
Monte Carlo (MC) integration with a large number (20k) of samples both before and after training.
to perform 1D heteroscedastic regression on a toy dataset of 500 points drawn from the distribution
shown in figure 3(b). Deeper networks and skip connections are considered in appendix C. The
training objective is taken from section 4, and the only detail required here is that aL is a 2-element
vector where the elements are labelled as (m, `). We use a diagonal Gaussian variational family to
represent the weights, but we preserve the full covariance of a during propagation. Using an input
x = 0.25 (see arrow, Figure 3(b)) we compute the distributions for m and` both at the start of training
(where we expect the iid assumption to hold) and at convergence (where iid does not necessarily
hold). Figure 3(c) shows the comparison between aL distributions reported by our deterministic
approximation and MC evaluation using 20k samples from q(w; θ). This comparison is qualitatively
excellent for all cases considered.
Computational efficiency In traditional MCVI,
propagation of S samples of d-dimensional activa-
tions through a layer containing a d × d-dimensional
transformation requires O(Sd2) compute and O(Sd)
memory. Our DVI method approximates the S → ∞
limit, while only demanding O(d3) compute and
O(d2) memory (the additional factor of d arises from
manipulation of the quadratically large covariance
matrix Cov[hj , hl]). Whereas MCVI can always
trade compute and memory for accuracy by choosing
a small value for S, the inherent scaling of DVI with
d could potentially limit its practical use for networks
with large hidden size. To avoid this limitation, we
also consider the case where only the diagonal entries
Cov(hj , hj ) are computed and stored at each layer.
We refer to this method as “diagonal-DVI” (dDVI),
and in section 6 we show the surprising result that
the strong test performance of DVI is largely retained
by dDVI across a range of datasets. Figure 4 shows
the time required to propagate activations through a
single layer using the MCVI, DVI and dDVI methods
hidden dimension, d
Figure 4: Runtime performance of VI methods.
We show the time to propagate a batch of 10 ac-
tivation vectors through a single d × d layer. For
MCVI we label curves with the number of sam-
ples used, and we show quadratic and cubic scal-
ing guides-to-the-eye (black). Black dots indicate
where our implementation runs out of memory
(16GB).
on a Tesla V100 GPU. As a rough rule of thumb (on this hardware), for layer sizes of practical
relevance, we see that absolute DVI runtimes roughly equate to MCVI with S = 300 and dDVI
runtime equates to S = 1.
3.2	Log-likelihood Evaluation
To use the moment propagation procedure derived above for training BNNs, we need to build a
function L that maps final layer activations aL to the expected log-likelihood term in equation 1 (see
figure 1(b)). In appendix B.1 we show the intuitive result that this expected log-likelihood over q(w)
5
Published as a conference paper at ICLR 2019
can be rewritten as an expectation over q(aL).
Ew〜q [logp(y∣x, w)] = EaL〜q(aL) [logp(y∣aL)] .	(7)
With this form we can derive closed forms for specific tasks; for brevity we focus on the regression
case and refer the reader to appendices B.4 and B.5 for the classification case.
Regression Case For simplicity we consider scalar y and a Gaussian noise model parameterized
by mean m(x; W) and heteroscedastic log-variance logσ2(x) = '(x; w). The parameters of
this Gaussian are read off as the elements of a 2-dimensional output layer aL = (m, `) so that
p(y|aL) = N y|m, e` . Recall that these parameters themselves are uncertain and the statistics
aL and ΣL can be computed following section 3.1. Inserting the Gaussian forms for p(y|aL) and
q(aL ) into equation 7 and performing the integral (see appendix B.2) gives a closed form expression
for the ELBO reconstruction term:
EaL〜虱aL) [logp(y∣aL)] = -2 [log2∏ + h'i + Xmmehimi-RLy) ] .	(8)
This heteroscedastic model can be made homoscedastic by setting h' = Σ'' = Σm' = 0. The ex-
pression in equation 8 completes the derivations required to implement the closed form approximation
to the ELBO reconstruction term for training a network. In addition, we can also compute a closed
form approximation to the predictive distribution that is used at test-time to produce predictions that
incorporate all parameter uncertainties. By approximating the moments of the posterior predictive
and assuming normality (see appendix B.3), we find:
p(y) ≈ /p(y∣aL) q(aL) daL
≈ N (y∣ hmi , ∑mm + eh')"'"2).
(9)
4 Empirical Bayes for Variational BNNs
So far, we have described methods for deterministic approximation of the reconstruction term in the
ELBO. We now turn to the KL term. For a d-dimensional Gaussian prior p(w) = N(μp, Σp), the
KL divergence with the Gaussian variational distribution q = N(μq, Σq) has closed form:
DKL [q||p] = 2 [log l∑pl - d + Tr (E-1Eq) + (μp - μq)> £-13P- μq)i . (IO)
However, this requires selection of (μp, Σp) for which there is usually little intuition beyond arguing
μp = 0 by symmetry and choosing Σp to preserve the expected magnitude of the propagated
activations (Glorot & Bengio, 2010; He et al., 2015). In practice, variational Bayes for neural network
parameters is sensitive to the choice of prior variance parameters, and we will demonstrate this
problem empirically in section 6 (figure 5).
To make variational Bayes robust we parameterize the prior hierarchically, retaining a conditional
diagonal Gaussian prior and variational distribution on the weights. The hierarchical prior takes the
form S 〜p(s); W 〜p(w∣s), using an inverse gamma distribution on S as the conjugate prior to the
elements of the diagonal Gaussian variance. We partition the weights into sets {λ} that typically
coincide with the layer partitioning3, and assign a single element in S to each set:
Sλ 〜Inv-Gamma(α,β) , wλ 〜N(0,sλ),	(11)
for shape α and scale β, and where wiλ is the ith weight in set λ.
Rather than taking the fully Bayesian approach, we adopt an empirical Bayes approach (Type-2
MAP), optimizing sλ , assuming that the integral is dominated by a contribution from this optimal
value sλ = s). We use the data to inform the optimal setting of s) to produce the tightest ELBO:
ELBO = Ew~q [logp(y∣hL(w))] - {dkl [q(w; θ)∣∣p(w∣s*)p(s*)]}
=⇒ sλ = argmin {dkl [q(w; θ)∣∣p(wλ∣sλ)] - logp(sλ)}	(12)
sλ
3In general, any arbitrary partitioning can be used
6
Published as a conference paper at ICLR 2019
Writing out the integral for the KL in equation 12, substituting in the forms of the distributions in
equation 11 and differentiating to find the optimum gives
λ= Tr [∑λ + μλ("λ)>]+2β
s* = Ωλ + 2α + 2
(13)
where Ωλ is the number of weights in the set λ. The influence of the data on the choice of s) is
made explicit here through dependence on the learned variational parameters Σq and μq. Using sλ
to populate the elements of the diagonal prior variance Σp, we can evaluate the KL in equation 10
under the empirical Bayes prior. Optimization of the resulting ELBO then simultaneously tunes the
variational distribution and prior.
In the experiments we will demonstrate that the proposed empirical Bayes approach works well;
however, it only approximates the full Bayesian solution, and it could fail if we were to allow too
many degrees of freedom. To see this, assume we were to use one prior per weight element, and we
would also define a hyperprior for each prior mean. Then, adjusting both the prior variance and prior
mean using empirical Bayes would always lead to a KL-divergence of zero and the ELBO objective
would degenerate into maximum likelihood.
5	Related Work
Bayesian neural networks have a rich history. In a 1992 landmark paper David MacKay demonstrated
the many potential benefits of a Bayesian approach to neural network learning (MacKay, 1992);
in particular, this work contained a convincing demonstration of naturally accounting for model
flexibility in the form of the Bayesian Occam’s razor, facilitating comparison between different
models, accurate calibration of predictive uncertainty, and to perform learning robust to overfitting.
However, at the time Bayesian inference was achieved only for small and shallow neural networks
using a comparatively crude Laplace approximation. Another early review article summarizing
advantages and challenges in Bayesian neural network learning is (MacKay, 1995c).
This initial excitement around Bayesian neural networks led to two main methods being developed;
First, Hinton & van Camp (1993) and MacKay (1995b) developed the variational Bayes (VB) ap-
proach for posterior inference. Whereas Hinton & van Camp (1993) were motivated from a minimum
description length (MDL) compression perspective, MacKay (1995b) motivated his equivalent en-
semble learning method from a statistical physics perspective of variational free energy minimization.
Barber & Bishop (1998) extended the methodology for two-layer neural networks to use general
multivariate Normal variational distributions. Second, Neal (1993) developed efficient gradient-based
Monte Carlo methods in the form of “hybrid Monte Carlo”, now known as Hamiltonian Monte Carlo,
and also raised the question of prior design and limiting behaviour of Bayesian neural networks.
Rebirth of Bayesian neural networks. After more than a decade of no further work on Bayesian
neural networks Graves (2011) revived the field by using Monte Carlo variational inference (MCVI)
to make VB practical and scalable, demonstrating gains in predictive performance on real world
tasks.
Since 2015 the VB approach to Bayesian neural networks is mainstream (Blundell et al., 2015);
key research drivers since then are the problems of high variance in MCVI and the search for
useful variational families. One approach to reduce variance in feedforward networks is the local
reparameterization trick (Kingma et al., 2015) (see appendix E). To enhance the variational families
more complicated distributions such as Matrix Gaussian posteriors (Louizos & Welling, 2016),
multiplicative posteriors (Kingma et al., 2015), and hierarchical posteriors (Louizos & Welling,
2017) are used. Both our methods, the deterministic moment approximation and the empirical Bayes
estimation, can potentially be extended to these richer families.
Prior choice. Choosing priors in Bayesian neural networks remains an open issue. The hierarchical
priors for feedforward neural networks that we use have been investigated before by Neal (1993)
and MacKay (1995a), the latter proposing a “cheap and cheerful” heuristic, alternating optimization
of weights and inverse variance parameters. Barber & Bishop (1998) also used a hierarchical prior
and an efficient closed-form factored VB approximation; our approach can be seen as a point estimate
to their approach in order to enable use of our closed-form moment approximation. Note that Barber
& Bishop (1998) manipulate an expression for hhjhli into a one-dimensional integral, whereas our
7
Published as a conference paper at ICLR 2019
approach gives closed form approximations for this integral without need for numerical integration.
Graves (2011) also used hierarchical Gaussian priors with flat hyperpriors, deriving a closed-form
update for the prior mean and variance. Compared to these prior works our approach is rigorous
and with sufficient data accurately approximates the Bayesian approach of integrating over the prior
parameters.
Alternative inference procedures. As an alternative to variational Bayes, probabilistic backprop-
agation (PBP) (Hemandez-Lobato & Adams, 2015) applies approximate inference in the form of
assumed density filtering (ADF) to refine a Gaussian posterior approximation. Like in our work, each
update to the approximate posterior requires propagating means and variances of activations through
the network. (Hernandez-Lobato & Adams, 2015) only consider the diagonal propagation case and
homoscedastic regression. Since the original work, PBP has been generalized to classification (Ghosh
et al., 2016) and richer posterior families such as the matrix variate Normal posteriors (Sun et al.,
2017). Our moment approximation could be used to improve the inference accuracy of PBP, and
since we handle minibatches of data rather than processing one data point at a time, our method is
more computationally efficient.
Gaussianity in neural networks. Our demonstration of Gaussianity of ReLU network activations
is also directly relevant to recent work on Gaussian process interpretations of deep neural net-
works (Matthews et al., 2018; Lee et al., 2017), validating the insight that activations in deep neural
networks are closely approximated by Gaussian processes. Two recent works derived deterministic
moment approximations for deep neural networks: Bibi et al. (2018), using Price’s theorem, derived
exact first and second moment expressions for ReLU activations but limit themselves to the case
of zero-mean Gaussian activations. Kandemir et al. (2018) also derive closed-form solutions to the
ELBO for the case of diagonal Gaussian variational families. However, their approach is limited to
linear layers without bias.
Markov chain Monte Carlo approaches. Another rich class of approximate inference methods for
Bayesian neural networks are stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods.
These methods allow for approximate posterior parameter inference using unbiased log-likelihood
estimates. Stochastic gradient Langevin dynamics (SGLD) was the first method in this class (Welling
& Teh, 2011). SGLD is particularly simple and efficient to implement, but recent methods increase
efficiency in the case of correlated posteriors by estimating the Fisher information matrix (Ahn et al.,
2012) and extend Hamiltonian Monte Carlo to the stochastic gradient case (Chen et al., 2014). A
complete characterization of SG-MCMC methods is given by (Ma et al., 2015; Gong et al., 2018).
However, despite this progress, important theoretical questions regarding approximation guarantees
for practical computational budgets remain (Nagapetyan et al., 2017). Moreover, while SG-MCMC
methods work robustly in practice, they remain computationally inefficient, especially because
evaluation of the posterior predictive requires evaluating an ensemble of models.
Wild approximations. The above methods are principled but often require sophisticated imple-
mentations; recently, a few methods aim to provide “cheap” approximations to the Bayes posterior.
Dropout has been interpreted by Gal & Ghahramani (2016) to approximately correspond to varia-
tional inference. Likewise, Bootstrap posteriors (Lakshminarayanan et al., 2017; Fushiki et al., 2005;
Harris, 1989) have been proposed as a general, robust, and accurate method for posterior inference.
However, obtaining a bootstrap posterior ensemble of size k is computationally intense at k times the
computation of training a single model.
6	Experiments
We implement4 deterministic variational inference (DVI) as described above to train small ReLU
networks on UCI regression datasets (Dheeru & Karra Taniskidou, 2017). The experiments address
the claims that our methods for eliminating gradient variance and automatic tuning of the prior
improve the performance of the final trained model. In Appendix D we present extended results to
demonstrate that our method is competitive against a variety of models and inference schemes.
4Our implementation in TensorFlow is available at https://github.com/Microsoft/
deterministic-variational-inference
8
Published as a conference paper at ICLR 2019
Dataset	|D|	dx	DVI	dDVI	MCVI	hoDVI
bost	506	13	-2.41 ± 0.02	-2.42 ± 0.02	-2.46 ± 0.02	-2.58 ± 0.04
conc	1030	8	-3.06 ± 0.01	-3.07 ± 0.02	-3.07 ± 0.01	-3.23 ± 0.01
ener	768	8	-1.01 ± 0.06	-1.06 ± 0.06	-1.03 ± 0.04	-2.09 ± 0.06
kin8	8192	8	1.13 ± 0.00	1.13 ± 0.00	1.14 ± 0.00	1.01 ± 0.01
nava	11934	16	6.29 ± 0.04	6.22 ± 0.06	5.94 ± 0.05	5.84 ± 0.06
powe	9568	4	-2.80 ± 0.00	-2.80 ± 0.00	-2.80 ± 0.00	-2.82 ± 0.00
prot	45730	9	-2.85 ± 0.01	-2.84 ± 0.01	-2.87 ± 0.01	-2.94 ± 0.00
wine	1588	11	-0.90 ± 0.01	-0.91 ± 0.02	-0.92 ± 0.01	-0.96 ± 0.01
yach	308	6	-0.47 ± 0.03	-0.47 ± 0.03	-0.68 ± 0.03	-1.41 ± 0.03
Table 2: Average test log-likelihood on UCI datasets. |D| is the dataset size, and dx is the input dimension.
Deterministic vs. Stochastic We compare DVI with MCVI from equation 2 with S = 10 samples
(we consider vanilla MCVI and discuss the local reparameterization trick in appendix E). The same
model is used for each inference method: a single hidden layer of 50 units for each dataset considered,
extending this to 100 units in the special case of the larger protein structure dataset, prot. Although
neither DVI nor MCVI is limited to a particular choice of variational family q(w; θ), we use a
factorized Gaussian family (i.e. a diagonal Cov(Wji , Wlk)). Factorization reduces the computational
complexity of terms involving Cov(Wji, Wlk) in DVI5 from O(N 2) to O(N), where N is the
number of elements in W (see appendix A.1). Additionally, both methods use the same EB prior
from equation 13 with a broad inverse Gamma hyperprior (α = 1, β = 10) and an independent sλ for
each linear transformation. Each dataset is split into random training and test sets with 90% and 10%
of the data respectively. This splitting process is repeated 20 times and the average test performance
of each method at convergence is reported in table 2 (see also learning curves in appendix F). We
see that DVI consistently outperforms MCVI, by up to 0.35 nats per data point on some datasets.
The computationally efficient diagonal-DVI (dDVI) surprisingly retains much of this performance.
By default we use the heteroscedastic model, and we observe that this uniformly delivers better
results than a homoscedastic model (hoDVI; rightmost column in table 2) on these datasets with no
overfitting issues6.
Empirical Bayes In Figure 5 we
compare the performance of networks
trained with manual tuning of a fixed
Gaussian prior to networks trained
with the automatic EB tuning. We
find that the EB method consistently
finds priors that produce models with
competitive or significantly improved
test log-likelihood relative to the best
manual setting. Since this observation
holds across all datasets considered,
we say that our method is “robust”.
Note that the EB method can outper-
form manual tuning because it au-
tomatically finds different prior vari-
ances for each weight matrix, whereas
in the manual tuning case we search
over a single hyperparameter control-
-3.05
-0.95-
1.13
1.12
111
prior variance
Figure 5: Comparison of converged test log-likelihood with a
manually tuned prior variance (orange) or empirical Bayes (blue).
63
62
6.1
-0.90-
ling all prior variances. An additional ablation study showing the relative contribution of our
deterministic approach and the EB prior are shown in appendix D.1.
5For MCVI with full rank covariance, Cholesky decomposition required for sampling is O(N 3).
6Note that this result is non-trivial because heteroscedastic models are more complex and could result in
poorer approximate inference leading to worse test performance
9
Published as a conference paper at ICLR 2019
7	Conclusion
We introduced two innovations to make variational inference for neural networks more robust: 1. an
effective deterministic approximation to the moments of activations of a neural networks; and 2. a
simple empirical Bayes hyperparameter update. We demonstrate that together these innovations make
variational Bayes a competitive method for Bayesian inference in neural heteroscedastic regression
models.
Bayesian neural networks have been shown to substantially improve upon standard networks in these
settings where calibrated predictive uncertainty estimates, sequential decision making, or continual
learning without catastrophic forgetting are required (see e.g. Oliveira et al. (2016); Gal et al. (2017);
Nguyen et al. (2018)). In future work, the new innovations proposed in this paper can be applied
to these areas. In the sequential decision making and continual learning applications, approximate
Bayesian inference must be run as an inner loop of a larger algorithm. This requires a robust and
automated version of BNN training: this is precisely where we believe the innovations in this paper
will have large impact since they pave the way to automated and robust deployment of BBNs that do
not involve an expert in-the-loop.
References
Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian posterior sampling via stochastic
gradient Fisher scoring. arXiv preprint arXiv:1206.6380, 2012.
David Barber and Christopher M Bishop. Ensemble learning in Bayesian neural networks. NATO
ASI SERIES F COMPUTER AND SYSTEMS SCIENCES,168:215-238,1998.
Adel Bibi, Modar Alfadly, and Bernard Ghanem. Analytic expressions for probabilistic moments
of PL-DNN with Gaussian input. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
Thang Bui, Daniel Hernandez-Lobato, Jose Hernandez-Lobato, Yingzhen Li, and Richard Turner.
Deep Gaussian processes for regression using approximate expectation propagation. In Interna-
tional Conference on Machine Learning, pp.1472-1481, 2016.
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In
International Conference on Machine Learning, pp. 1683-1691, 2014.
Dua Dheeru and Efi Karra Taniskidou. UCI machine learning repository, 2017. URL http:
//archive.ics.uci.edu/ml.
Tadayoshi Fushiki, Fumiyasu Komaki, Kazuyuki Aihara, et al. Nonparametric bootstrap prediction.
Bernoulli,11(2):293-307, 2005.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059,
2016.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
In International Conference on Machine Learning, 2017.
Soumya Ghosh, Francesco Maria Delle Fave, and Jonathan S Yedidia. Assumed density filtering
methods for learning Bayesian neural networks. In AAAI., pp. 1589-1595, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Wenbo Gong, Yingzhen Li, and Jose Miguel Hernandez-Lobato. Meta-learning for stochastic gradient
MCMC. arXiv preprint arXiv:1806.04522, 2018.
10
Published as a conference paper at ICLR 2019
Alex Graves. Practical variational inference for neural networks. In Advances in neural information
processing Systems,pp. 2348-2356, 2011.
Ian R Harris. Predictive fit for natural exponential families. Biometrika, 76(4):675-684, 1989.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning
of Bayesian neural networks. In International Conference on Machine Learning, pp. 1861-1869,
2015.
GE Hinton and Drew van Camp. Keeping neural networks simple by minimising the description
length of weights. In Proceedings of COLT-93, pp. 5-13, 1993.
Melih Kandemir, Manuel Haussmann, and Fred A Hamprecht. Sampling-free variational inference
of Bayesian neural nets. arXiv preprint arXiv:1805.07654, 2018.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-
zation trick. In Advances in Neural Information Processing Systems, pp. 2575-2583, 2015.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, pp. 6402-6413, 2017.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix
Gaussian posteriors. In International Conference on Machine Learning, pp. 1708-1716, 2016.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational Bayesian neural
networks. arXiv preprint arXiv:1703.01961, 2017.
Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient MCMC. In
Advances in Neural Information Processing Systems, pp. 2917-2925, 2015.
David JC MacKay. A practical Bayesian framework for backpropagation networks. Neural computa-
tion, 4(3):448-472, 1992.
David JC MacKay. Bayesian neural networks and density networks. Nuclear Instruments and
Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated
Equipment, 354(1):73-80, 1995a.
David JC MacKay. Developments in probabilistic modelling with neural networks—ensemble
learning. In Neural Networks: Artificial Intelligence and Industrial Applications, pp. 191-198.
Springer, 1995b.
David JC MacKay. Probable networks and plausible predictionsa review of practical Bayesian
methods for supervised neural networks. Network: Computation in Neural Systems, 6(3):469-505,
1995c.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.
Andrew Miller, Nick Foti, Alexander D’Amour, and Ryan P Adams. Reducing reparameterization
gradient variance. In Advances in Neural Information Processing Systems, pp. 3708-3718, 2017.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural
networks. arXiv preprint arXiv:1701.05369, 2017.
11
Published as a conference paper at ICLR 2019
Tigran Nagapetyan, Andrew B Duncan, Leonard Hasenclever, Sebastian J Vollmer, Lukasz Szpruch,
and Konstantinos Zygalakis. The true cost of stochastic gradient Langevin dynamics. arXiv
preprint arXiv:1706.02692, 2017.
Radford M Neal. Bayesian learning via stochastic dynamics. In Advances in neural information
processing systems,pp. 475-482, 1993.
Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning.
International Conference on Learning Representations (ICLR), 2018.
Ramon Oliveira, Pedro Tabacof, and Eduardo Valle. Known unknowns: Uncertainty quality in
bayesian neural networks. In NIPS Bayesian Deep Learning Workshop, 2016.
Christopher G. Small. Expansions and Asymptotics for Statistics. CRC Press, 2010.
Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning structured weight uncertainty in
Bayesian neural networks. In Artificial Intelligence and Statistics, pp. 1283-1292, 2017.
Jarno Vanhatalo and Aki Vehtari. Mcmc methods for MLP-network and Gaussian process and
stuff—a documentation for Matlab toolbox MCMCstuff. Laboratory of computational engineering,
Helsinki university of technology, 2006.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681-688,
2011.
Zhanxing Zhu, Ruosi Wan, and Mingjun Zhong. Neural control variates for variance reduction. arXiv
preprint arXiv:1806.00159, 2018.
12
Published as a conference paper at ICLR 2019
Appendix
A MOMENTS OF THE ACTIVATION VARIABLES a`
Under assumption of independence of h, W and b, we can write:
haii = hhjWjii+hbii = hhji hWjii + hbii	(14)
Cov(ai, ak) = Cov(hj Wji, hlWlk) + Cov(bi, bk)
= hhjWji hlWlki - hhjWjii hhlWlki + Cov(bi, bk)
= hhjhli hWjiWlki - hhji hhli hWjii hWlki + Cov(bi, bk)
= hhjhli [Cov(WjiWlk) + hWjii hWlki]
- hhji hhli hWjii hWlki + Cov(bi, bk)
= hhjhli Cov(Wji, Wlk) + hWjii Cov(hj, hl) hWlki + Cov(bi, bk),	(15)
which is seen in the main text as equation 3. For Heaviside and ReLU activation functions, closed
forms exist for hhji in equation 14:
Heaviside
ReLU
hhji
hhji
where SR(X) ∙= φ(χ) + xΦ(x) is a “soft ReLU”，φ and Φ represent the standard Gaussian PDF and
CDF, and We have introduced the dimensionless variables μj =(aj) /ʌ Σjj.. These results are is
sufficient to evaluate equation 14, so in the following sections we turn to each term from equation 15.
A.1 EVALUATION OF TERM 1: hhjhli Cov(Wji, Wlk)
In the general case, we can use the results from section A.2 to evaluate off-diagonal hhj hli. However,
in our experiments we always consider the the special case where Cov(Wji, Wlk) is diagonal. In this
case we can write the first term in equation 15 as (reintroducing the explicit summation):
X hhjhli Cov(Wji, Wlk) =XhhjhliδjlδikVar(Wji)
=δik	hzjzjiVar(Wji)
j
= diag [vVar(W )]
i.e. this term is a diagonal matrix with the diagonal given by the left product of the vector vj = hhjhji
with the matrix Var(Wki). Note that hhjhji can be evaluated analytically for Heaviside and ReLU
activation functions:
Heaviside	hhj hji	=	√2∏ςo= ZO ∞	e-F SjTaji) dαj = φ (μj)
ReLU	hhjhji	=	√2∏∑0 /	αje 2Wj( j' j" dαj =当彳[μjφ(μj)	+ (I + μj2)φ(μj)]
A.2 EVALUATION OF TERM 2: hWjii Cov(hj , hl) hWlki
Evaluation of Cov(hj, hl) requires an expression for hhjhli. From equation 5, we write:
hhjhι〉a / f (αj)f (αι)exp [-2P(αj,α?; a0, Σ0)] dαjdα?,	(16)
13
Published as a conference paper at ICLR 2019
where P is the quadratic form:
P(αj, αl; a0, Σ0)
αj -ha0j i	Σ0jj Σ0jl -1 αj -ha0j i
αl-ha0li	Σ0lj Σ0ll	αl -ha0l i
(ηj-μj「( 1 ρji「1 (%-μj∖
I ηι-μl J IPlj 1 ) I ηι-μl 广
Here We haye introduced further dimensionless variables η = α,/ʌ Σj η = αι∕P∑∣ and
ρ0jl = Σ0jl/ Σ0jjΣ0ll. We can then rewrite equation 16 in terms of a dimensionless integral I using a
scale factor Sj0l that is 1 for the Heaviside non-linearity orΣ0jl∕ρ0jl for ReLU:
hhjhli = SjlI(μj, μl ,ρjι) ; I
Z / f (ηj )f (ηl)eχp [- 1 P (ηj ,ηl;μ, ρ0)]
dηj dηl.
The normalization constant, Z, is evaluated by integrating over e-P/2 and is explicitly written as
Z = 2∏ρjl, where Pjl = 1- - ρ0jl. Now, following equation 6, we have the task to write I as an
asymptote A plus a decaying correction e-Q. To evaluate A and Q, we have to insert the explicit
form of the non-linearity f, which we do for Heaviside and ReLU functions in the next sections.
A.2.1 Heaviside non-linearity
For the Heaviside activation, we can represent the integral I as the shaded area under the Gaussian in
the upper-left quadrant shown below. In general, this integral does not have a closed form. However,
for μj → ∞, vanishing weight appears under the Gaussian in the upper-right quadrant, so we can
write down the asymptote of the integral in this limit:
1∞	∞
μl→m∞I = ZJ	J oexp[- 1 P(ηj,ηl;μ0,ρjl)] dηjdηl =φ(μl)
Here we performed the integral by noticing that the outer integral over ηj marginalizes out ηj from
the bivariate Gaussian, leaving the inner integral as the definition of the Gaussian CDF. By symmetry,
we also have limμo→∞ I = Φ(μj) and limμ< l→-∞ I = 0. We can then write down the following
symmetrized form that satisfies all the limits required to qualify as an asymptote:
a = Φ(μj)Φ(μl)
To compute the correction factor we evaluate the derivatives of (I - A) at the origin up to second
order to match the moments of e-Q for quadratic Q. Description of this process is found below
Zeroth derivative At the origin μj∙ = μl = 0, we can diagonalize the quadratic form P:
P(ηj,ηl; 0,ρjl) = 2/102 (η2 - 2Pjlnjnl + η2) = 4p02 (ξ+ + ξ-),
where ξ± = √1 千 P(n1 ± 小).Performing this change of variables in the integral gives:
I =4∏⅛ L exp h 4⅛ (ξ+ + ξ- )i dξ+dξ- = @/n
where we integrated in polar coordinates over the region H in which the Heaviside function is
non-zero. The angle ψ can be found from the coordinate transform between n and ξ as7:
ψ = arctan '1+pjl = ∏2 — 11 arccosρjl.
Since A∣μ,=o = Φ(0)Φ(0) = 1/4, we can evaluate:
(I - A)lμ0 = 0 = π2 - 12 arccθs Pjl - 4
1
=1π arcsin Pjl
7Here we use the identity cos(2 arctan x) = cos2 arctan X — sin2 arctan X = 1-χ2
14
Published as a conference paper at ICLR 2019
First derivative Performing a change of variables Xi = η - μi, We can write I as:
I = Z / H(Xj + μj)H(xι + μl)exp[-2P(Xj,X1 ； 0,ρjι)]
dXj dXl
where H is the Heaviside function. Now, using ∂xH(X) = δ(X), we have:
μθ = 0 1 = Z I
δ(xj)H(xι)exp[- 1 P(Xj,X1；0,ρjι)] dxjdxι = 2√12∏.	(17)
In addition, using ∂xΦ(X) = φ(X), we have:
dμjlμo=o A = 2√⅛	=⇒	d⅛L=o (I- A)=0.
By symmetry (I - A) also has zero gradient with respect to μ∣ at the origin. Therefore Q has no
linear term in μ0.
Second derivative Taking another derivative in equation 17 gives:
∂μ02∣ z_0I = -Z/δ(Xj)H(Xl)Pjlχιeχp [- 1 P(Xj,χι;0,Pjl)]
dXj dXl
Pj
2πpjι.
where we used the identity f(X)∂xδ(X)dX = - δ(X)∂xf(X)dX, which holds for arbitrary f. In
addition, we have:
嘉 ∣ A = 0
W2 lμ0=0
Ho=。(I - A) = - 2⅜ .
and the same result holds for the second derivative w.r.t. μ∣. To complete the Hessian, it is a simple
extension of previous results to show that:
∂2
∂μj ∂μ0
lμ0=0 (I - A) = W .
Now that we have obtained derivatives of the residual (I - A) up to second order we propose a
correction factor of the form e-Q where Q is truncated at quadratic terms:
Q = - log 2α∏ + β (μj2 + μl2) + γμj2 μl2.
We then find the coefficients {α,β,γ} by matching (=) derivatives at μ = 0:
	e-Q	lμ=0 二	α 一 2∏
∂ dμi	e-Q	। μ=0	0
∂2 ∂μ2	e-Q	। μ=0	二-2β2∏
∂2 ∂μj ∂μ0	e-Q	। μ=0	二 Y 2π
! arcsin p0jl
=	2∏
!
=! 0
J-	PjI
————：—
2πPjι
J_ ∖-pj
-- -
2πPjι
=⇒ α = arcsin ρ0jl
=⇒ β = 2⅜
=⇒ γ
I-PjI
αpjl
This yields the expression seen in table 1 of the main text.
A.2.2 ReLU non-linearity
As in the Heaviside case, we begin by computing the asymptote of I by inspecting the limit as
μj → ∞:
1∞	∞
μl→m∞I = ZJ J Onjηleχp[- 1 p(η,ηl;μ0,ρjl)] dηjdηl
∞	1	02	0	∞	1	02
=√2∏	μjηle-2(ηι-μι) dηl + √2∏	ηl(ηl - μl)e-2(ηι-μι) dηl
ηι=0	ηι=0
=μj SR(Ml) + ρjl φ(μ∣)
(18)
15
Published as a conference paper at ICLR 2019
Now, we construct a full 2-dimensional asymptote by symmetrizing equation 18 (using proper-
ties SR(x) → x and Φ(x) → 1 as x → ∞ to check that the correct limits are preserved after
symmetrizing):
A = SR(Mj)SR(Ml) + ρjιφ(μj )φ(μ0)
Next we compute the correction factor e-Q. The details of this procedure closely follow those for
the Heaviside non-linearity of the previous section, so we omit them here (and in practice we use
Mathematica to perform the intermediate calculations). The final result is presented in table 1 of
the main text.
B Log-Likelihood and Posterior Predictive Computation
Here we give derivations of expressions quoted in section 3.2. In section B.1 we justify the intuitive
result that expectation of the ELBO reconstruction term over q(w; θ) can be re-written as an expecta-
tion over q(aL). We then derive expected log-likelihoods and posterior predictive distributions for
the cases of univariate Gaussian regression and classification. The latter sections are arranged as
follows:
I Regression		Classification
Log-likelihood	section B.2	section B.4
Posterior predictive	section B.3	section B.5
B.1	LOG-LIKELIHOODS: FROM Ew TO EaL
We begin by rewriting the reconstruction term for data point (x, y) in terms of aL:
Ew〜q [logp(y∣w)]
q(w) log p(y|w) dw
q(aL)q(w|aL) log p(y|w) dw daL
where we have suppressed explicit conditioning on x for brevity. Our goal now is to perform the
integral over w, leaving the expectation in terms of aL only, thus allowing it to be evaluated using
the approximation q(aL) from section 3.1.
To eliminate w, consider the case where the output of the model is a distribution p(y|aL) that is a
parameter-free transformation of aL (e.g. aL are logits of a softmax distribution for classification or
the moments of a Gaussian for regression). Since the model output is conditioned only on aL, we
must have p(y|w) = p(y|aL) for all configurations w that satisfy the deterministic transformation
aL = M(x; w), where M is the neural network (i.e p(y|w) = p(y|aL) for all w where q(w|aL) is
non-zero). This allows us to write:
q(w|aL) log p(y|x, w) dw = logp(y|aL)	q(w|aL) dw = log p(y|aL),
so the reconstruction term becomes:
Ew〜q [logp(y∣χ, w)]
q(aL) log p(y|aL)daL
This establishes the equivalence given in equation 7 in the main text. Since we are using an
approximation to q, We will actually compute EaL〜虱0l)[logp(y∣aL)].
B.2	Univariate Regression: Log-likelihood
Here we give a derivation of equation 8 from the main text. Throughout this section we label the
2 elements of the final activation vector as aL = (m, `). We first insert the Gaussian form for
p(y|aL)〜N [m, e'] into the log-likelihood expression:
EaL〜虱aL) [logp(y∣aL)] = -2EaL〜虱aL) [log(2∏ exp('))+exp(-')(y - m)2]
=-2 log2π - 2 h'i - EaL 〜虱aL) [eχp(-')(y - m)2] .	(19)
16
Published as a conference paper at ICLR 2019
Now We use the Gaussian form of q(aL):
q(aL) « exp [-1X>(∑L)-1X] ； X = (m'-hmi ).
and note that
/exp(-')exp [-1X>(ΣL)-1X] dmd'
exp (- h`i)
/exp [-2X>(ΣL)-1X - (' - h'i)] dmd'
exp [-1X>(ΣL)-1X - e>X] dmd'
exp
(-h`i)
exp [-1 (X> + e>ΣL)(ΣL)T(X + e'ΣL)] dmd',	(20)
where e> = (0,1) is the unit vector in the ' coordinate, and we completed the square to obtain the
final line. Inserting equation 20 into equation 19 and marginalizing out the ' coordinate gives:
EaL〜q(aL) [logp(y∣aL)]	= - 1	log 2∏	+	h'i	+	eɪ/j'i	/(y - m)2 exp	(- [m-(hmi-*" )	dm
q	2	2πΣmm	2 mm
Finally, performing the integral over m gives the result seen in equation 8.
B.3	Univariate Regression: Posterior Predictive Distribution
Here we give a derivation of equation 9 from the main text. We first calculate the first and second
moments of the predictive distribution under the approximation q(aL) ≈ ⅞(aL):
Ey〜P(y)[y] = Z yp(y∣aL项aL) dy daL	Var© = Var[Ey~p⑶QL)⑻]+Ey~p⑼[Var(y|aL)]
J	= Var[m] + Ey〜p(y) [e']
=J /yp(y|aL)dy q(aL)daL	=∑mm + / e'p(y|aL)q(aL) daL dy
=/ mq(aL) daL	= Σmm + / e'虱aL) daL
=hmi	= ∑mm + exp(h'i + ∑''∕2)
where the final integral in the variance computation is performed by inserting the Gaussian form for
q(aL) and completing the square. Then we assume normality of the predictive distribution to obtain
the result in equation 9.
B.4	Classification: Log-likelihood
There is no exact form for the expected log-likelihood for multivariate classification with logits aL .
However, using the second-order Delta method (Small, 2010), we find the expansion
EaL〜虱aL) [logp(y∣aL)] =(aL)一 EaL〜虱aL) [logsumexp(aL)]
≈〈aL)— logsumexp(〈aL))— 1 (p> diag(ΣL) — p>ΣLp) , (21)
To derive this expansion, we first state the second order expansion for the expectation of a function g
of random variable x using the Delta method as follows8:
E[g(X)] ≈ g (E[x]) + 1 XhCjdx⅛j i =E[ ] ,	(22)
ij	x= [x]
where Cij = Cov(xi, xj). Now we note that the logsumexp function has a simple Hessian
dx∂∂xj logsumexp(x) = δjPi - Pipj,
where p = softmax(x). Putting these results together allows us to write:
E [logsumexp(x)] ≈ logsumexp (E[x]) + 1 [p> diag(C) - p>Cp] x=E[x],
This result is sufficient to complete the derivation of equation 21 and enable training of a classifier
using our method.
8This result is obtained by Taylor expansion inside the expectation.
17
Published as a conference paper at ICLR 2019
B.5	Classification: Posterior Predictive Distribution
Using the same second-order Delta method, we find the following expansion for the posterior
predictive distribution:
p(y) = EaL〜q(aL) [p(y∣aL)] ≈ P Θ [1 + P>∑LP — ∑LP + 1 diag(ΣL) - 1 p> diag(ΣL)].
(23)
where P = softmax( aL ).
For this expansion, we begin by computing the Hessian:
RNj [p] k = ∂Xi∂xj Pk = [2piPj - (δkipj + δkjpi) + δikδjk - δijpi] Pk,
where P = Softmax(x), and We used the intermediate result Vj [p] k = δjdPk 一 PjPk. Then We can
form the product:
Tr [CVVp] =p	[2P>CP- 2CP + diag(C) - P> diag(C)]
and insert this into equation 22 to obtain equation 23.
Preliminary experiments shoW that good results are obtained either using these approximations or a
lightWeight MC approximation just to perform the mapping of aL to (log)P after the deterministic
heavy-lifting of computing aL . In this Work We are primarily concerned With demonstrating the
benefits of the moment propagation method from section 3.1, so We limit our experiments to regression
examples Without additional complication from approximation of the likelihood function.
C Deeper Networks
Here We consider the applicability of our method to the regime of deep, narroW netWorks. This
regime is challenging because for small hidden dimension the Gaussian approximation for a (reliant
on the CLT) breaks doWn, and these errors accumulate as the net becomes deep. We empirically
explore this potential problem by investigating deep netWorks containing 5 layers of only 5, 25 or
125 units each. Figure 6 shoWs results analogous to figure 3 that qualitatively illustrate hoW Well our
approximation matches the true variational distribution of output activations both at the start and end
of training. We see that our CLT-based approximation is good in the 125- and 25-unit cases, but is
poor in the 5-unit case. Since it is generally considered that optimization of neural netWorks only
Works Well in the high dimensional setting With at least a feW tens of hidden units, these empirical
observations suggest that our approximation is applicable in practically relevant architectures.
C.1 Skip connections
Training deep netWorks is considered difficult even in the traditional maximum-likelihood setting due
to the problems of exploding and vanishing gradients. A popular approach to combat these issues is to
add skip connections to the architecture. Here We derive the necessary results to add skip connections
to our deterministic BNN.
We consider a simple layer With skip connections of the folloWing form:
h= f(a0),
δ = hW + b,
a = a0 + δ.
The moment propagation expressions for this layer are (using the bilinearity of Cov):
haii = ha0ii + hδii ,
Cov(ai, ak) = Cov(a0i, a0k) + Cov(δi, δk) + Cov(a0i, δk) + Cov(δi, a0k),
Where hδii and Cov(δi, δk) can be computed using analogy to equations 14 and 15. This just leaves
computation of Cov(a0i, δk) and its transpose, Which can be performed analytically using integral
18
Published as a conference paper at ICLR 2019
results and methods borrowed from appendix A.
Cov(a0i, δk) = Cov(a0i, X f (a0j)Wjk) + Cov(a0i, bk)
j
=X(<aif(aj)> - ha"f(aj)>) hWjki
j
=X hWjk i(%) n ("；：?HRvLUde
j
Using this result, we implement a 5-layer, 25-unit network with skip connections. In figure 6(d) we
qualitatively verify the validity of our approximation on this architecture by observing a good match
with Monte Carlo simulations using 20k samples.
19
Published as a conference paper at ICLR 2019
(a)
Data samples
--Data 1-std
口 Model 1-std /
2
1
y
0
-1
-0.5	0.0	0.5
X
0du-≡-ejHəjojən
q(m)	厅 口 MC	/ 一ours J	L
Q(I)/	L
'O
'5
'O
5
'-
O
(b)
2
1
y
0
-1
Data samples
-- Data 1-std
口 Model 1-std
d)
bnu-ureUJBJV
-1.5	-1.0 -0.5	0.0
m
wu-ureJH3JOJ3m
q(m) 口 MC	/ —ours / A	,L
O 1
y 一
Data samples
-- Data 1-std
口 Model 1-std
O
1
-
Mu-u-巴Həuojəa
6
■
O
8
■
‘ O
wu-ureUJBJV 一
	
q(m)
口 MC
—ours
q(m) A	
10
wu-ureJH3J0J3m一wu-ureJHJ3七V
Data samples
--Data 1-std
口 Model 1-std /
q(m)
口 MC
—ours
10
，10
q(i)	/ 1	___ k.—
-0.8	-0.6	-0.4 -3.5	-3.0	-2.5	-2.0
m	I
0
O
'O
5
0
2
I

m
Figure 6: Empirical accuracy of our approximation for 5-layer networks trained analogously to
figure 3. Progressively narrower networks of (a) 125 unit (b) 25 unit and (c) 5 unit are trained and our
CLT-based approximation is only seen to significantly break down in the 5-unit case. (d) Qualitative
verification of our approximation applied to an architecture with skip connections (orange).
20
D Extended Results
Here we include comparison with a number of different models and inference schemes on the 9 UCI datasets considered in the main text. We report test log-likelihoods
at convergence and find that our method is competitive or superior to a range of state-of-the-art techniques (reproduced from Bui et al. (2016)).
Dataset	bost	conc	ener	kin8	nava	powe	prot	wine	yach
ID	506	1030	768	8192	11934	9568	45730	1588	308
dχ	13	8	8	8	16	4	9	11	6
GP50	-2.22 ± 0.07	-2.85 ± 0.02	-1.29 ± 0.01	1.31 ± 0.01	4.86 ± 0.04	-2.66 ± 0.01	-2.95 ± 0.05	-0.67 ± 0.01	-1.15 ± 0.03
DGP-1 50	-2.33 ± 0.06	-3.13 ± 0.03	-1.32 ± 0.03	0.68 ± 0.07	3.60 ± 0.33	-2.81 ± 0.01	-2.55 ± 0.03	-0.35 ± 0.04	-1.39 ± 0.14
DGP-2 50	-2.17 ± 0.10	-2.61 ± 0.02	-0.95 ± 0.01	1.79 ± 0.02	4.77 ± 0.32	-2.58 ± 0.01	-2.11 ± 0.04	-0.10 ± 0.03	-0.99 ± 0.07
DGP-3 50	-2.09 ± 0.07	-2.63 ± 0.03	-0.95 ± 0.01	1.93 ± 0.01	5.11 ± 0.23	-2.58 ± 0.01	-2.03 ± 0.07	-0.13 ± 0.02	-0.94 ± 0.05
GP 100	-2.16 ± 0.07	-2.65 ± 0.02	-1.11 ± 0.02	1.68 ± 0.01	5.51 ± 0.03	-2.55 ± 0.01	-2.52 ± 0.07	-0.57 ± 0.02	-1.26 ± 0.03
DGP-1 100	-2.37 ± 0.10	-2.92 ± 0.03	-1.21 ± 0.02	1.09 ± 0.04	3.75 ± 0.37	-2.67 ± 0.02	-2.18 ± 0.06	0.07 ± 0.03	-1.34 ± 0.10
DGP-2 100	-2.09 ± 0.06	-2.43 ± 0.02	-0.90 ± 0.01	2.31 ± 0.01	5.13 ± 0.27	-2.39 ± 0.02	-1.51 ± 0.09	0.37 ± 0.02	-0.96 ± 0.06
DGP-3 100	-2.13 ± 0.09	-2.44 ± 0.02	-0.91 ± 0.01	2.46 ± 0.01	5.78 ± 0.05	-2.37 ± 0.02	-1.32 ± 0.06	0.25 ± 0.03	-0.80 ± 0.04
VI(KW)-2	-2.64 ± 0.02	-3.07 ± 0.02	-1.89 ± 0.07	2.91 ± 0.10	6.10 ± 0.19	-2.28 ± 0.02	-0.42 ± 0.31	-0.85 ± 0.01	-1.92 ± 0.03
i	SGLD-2	-2.38 ± 0.06	-3.01 ± 0.03	-2.21 ± 0.01	1.68 ± 0.00	3.21 ± 0.02	-2.61 ± 0.01	-1.23 ± 0.01	0.14 ± 0.02	-3.23 ± 0.03
SGLD-1	-2.40 ± 0.05	-3.08 ± 0.03	-2.39 ± 0.01	1.28 ± 0.00	3.33 ± 0.01	-2.67 ± 0.00	-3.11 ± 0.02	-0.41 ± 0.01	-2.90 ± 0.02
HMC-1	-2.27 ± 0.03	-2.72 ± 0.02	-0.93 ± 0.01	1.35 ± 0.00	7.31 ± 0.00	-2.70 ± 0.00	-2.77 ± 0.00	-0.91 ± 0.02	-1.62 ± 0.01
PBP-1	-2.57 ± 0.09	-3.16 ± 0.02	-2.04 ± 0.02	0.90 ± 0.01	3.73 ± 0.01	-2.84 ± 0.01	-2.97 ± 0.00	-0.97 ± 0.01	-1.63 ± 0.02
VI(G)-1	-2.90 ± 0.07	-3.39 ± 0.02	-2.39 ± 0.03	0.90 ± 0.01	3.73 ± 0.12	-2.89 ± 0.01	-2.99 ± 0.01	-0.98 ± 0.01	-3.44 ± 0.16
VI(KW)-1	-2.43 ± 0.03	-3.04 ± 0.02	-2.38 ± 0.02	2.40 ± 0.05	5.87 ± 0.29	-2.66 ± 0.01	-1.84 ± 0.07	-0.78 ± 0.02	-1.68 ± 0.04
Dropout-1	-2.46 ± 0.25	-3.04 ± 0.09	-1.99 ± 0.09	0.95 ± 0.03	3.80 ± 0.05	-2.89 ± 0.01	-2.80 ± 0.05	-0.93 ± 0.06	-1.55 ± 0.12
DVI	-2.41 ± 0.02	-3.06 ± 0.01	-1.01 ± 0.06	1.13 ± 0.00	6.29 ± 0.04	-2.80 ± 0.00	-2.85 ± 0.01	-0.90 ± 0.01	-0.47 ± 0.03
dDVI	-2.42 ± 0.02	-3.07 ± 0.02	-1.06 ± 0.06	1.13 ± 0.00	6.22 ± 0.06	-2.80 ± 0.00	-2.84 ± 0.01	-0.91 ± 0.02	-0.47 ± 0.03
DVI-MC	-2.41 ± 0.02	-3.05 ± 0.01	-1.00 ± 0.06	1.13 ± 0.00	6.25 ± 0.03	-2.80 ± 0.00	-2.85 ± 0.01	-0.93 ± 0.04	-0.55 ± 0.03
DVI-MC-softplus	-2.42 ± 0.02	-3.06 ± 0.02	-1.03 ± 0.05	1.13 ± 0.00	6.20 ± 0.04	-2.80 ± 0.01	-2.85 ± 0.01	-0.89 ± 0.01	-0.54 ± 0.03
MCVI	-2.46 ± 0.02	-3.07 ± 0.01	-1.03 ± 0.04	1.14 ± 0.00	5.94 ± 0.05	-2.80 ± 0.00	-2.87 ± 0.01	-0.92 ± 0.01	-0.68 ± 0.03
MCVI-softplus	-2.47 ± 0.02	-3.08 ± 0.02	-1.02 ± 0.05	1.14 ± 0.00	5.99 ± 0.02	-2.80 ± 0.00	-2.85 ± 0.01	-0.92 ± 0.01	-0.69 ± 0.03
Table 3: Average test log likelihood on UCI datasets. See table 4 for model glossary and implementation references.
Published as a conference paper at ICLR 2019
Published as a conference paper at ICLR 2019
Abbreviation	Description	Implementation
GP N	Gaussian process regression	Bui et al. (2016)
DGP-L N	Deep Gaussian process regression	Bui et al. (2016)
VI(KW)-L	BNN with the variational free energy evaluated using the reparameterization trick (KW = Kingma + Welling)	Bui et al. (2016)
SGLD-L	Stochastic Gradient Langevin Dynamics	Bui et al. (2016)
HMC-L	Hamiltonian Monte Carlo	Bui et al. (2016) using toolbox Vanhatalo & Vehtari (2006)
PBP-L	probabilistic back-propagation	Hernandez-Lobato & Adams (2015)
VI(G)-L	scalable variational inference (VI) method for neural networks (G = Graves)	Graves (2011)
Dropout-L	a technique that employs dropout during training as well as at prediction time.	Gal & Ghahramani (2016)
DVI	Our method	ours
dDVI	same as DVI, but with diagonal activation covariance	ours
DVI-MC	same as DVI, but with a light-weight Monte Carlo inte- gration only for computing the predictive distribution	ours
DVI-MC-softplus	same as DVI-MC, but uses softplus(`) to model het- eroscedastic observation variance rather than e` . Note that in this case there is no closed form for the log- likelihood, so the lightweight final MC step is required	ours
MCVI-exp	our implementation of SVI using e`	ours
MCVI-softplus	our implementation of SVI using softplus(`)	ours
Table 4: Glossary of methods displayed in table 3 with references. Note: L = number of hidden layers; N
= number of Gaussian process pseudo-points. Please refer to Bui et al. (2016) for more descriptions of other
state-of-the-art methods.
D. 1 Ablation study
Here we provide an ablation study that indicates the individual contributions of (1) the deterministic
approximation and (2) the the empirical Bayes prior. We consider all combinations of DVI or
MCVI with and without empirical Bayes. In the DVI-fixed and MCVI-fixed cases without empirical
Bayes we use a fixed zero-mean Gaussian prior during training and we perform separate runs to tune
the prior variance, reporting the best performance achieved (cf. figure 5)9. Since the EB approach
requires no hyperparameter tuning between the datasets shown, these results hide the considerable
computational advantaged that the EB approach brings.
E	Variance Reduction and the Local Reparameterization Trick
By eliminating MC sampling and its associated variance entirely, our method directly tackles the
problem of high variance gradient estimates that hinder MC approaches to training of BNNs. Al-
ternative methods that only reduce variance have been considered, and among these, the local
9Note that the EB method can out perform manual tuning because it automatically finds different prior
variances for each weight matrix, whereas manual tuning case searches over a single hyperparameter controlling
all prior variances.
22
Published as a conference paper at ICLR 2019
Dataset	DVI	DVI-fixed	MCVI	MCVI-fixed
bost	-2.41 ± 0.02	-2.46 ± 0.02	-2.46 ± 0.02	-2.48 ± 0.02
conc	-3.06 ± 0.01	-3.07 ± 0.01	-3.07 ± 0.01	-3.07 ± 0.01
ener	-1.01 ± 0.06	-1.07 ± 0.04	-1.03 ± 0.04	-1.07 ± 0.04
kin8	1.13 ± 0.00	1.12 ± 0.00	1.14 ± 0.00	1.13 ± 0.00
nava	6.29 ± 0.04	6.32 ± 0.04	5.94 ± 0.05	6.00 ± 0.02
powe	-2.80 ± 0.00	-2.80 ± 0.01	-2.80 ± 0.00	-2.80 ± 0.00
prot	-2.85 ± 0.01	-2.84 ± 0.01	-2.87 ± 0.01	-2.89 ± 0.01
wine	-0.90 ± 0.01	-0.94 ± 0.01	-0.92 ± 0.01	-0.94 ± 0.01
yach	-0.47 ± 0.03	-0.49 ± 0.03	-0.68 ± 0.03	-0.56 ± 0.03
Table 5: Ablation study of all combinations of DVI and MCVI with EB or a fixed prior. One standard deviation
error in the last significant digit is shown in paraentheses.
(a)
Figure 7:	Performance of MCVI vs rMCVI. (a) Gradient variance for the model shown in figure 3 with batch
size B = 1. Variance values are normalized such that MCVI with 1 sample appears at unit relative variance.
For this model, rMCVI achieves the same variance as MCVI with roughly 5× fewer samples (b) Runtime
performance of rMCVI evaluated under the conditions of figure 4. For this model, rMCVI runs with roughly
10× more samples in the same time as MCVI.
reparameterization trick (Kingma et al., 2015) is particularly popular. Similar to our approach, the
local reparameterization trick maps the uncertainty in the weights to an uncertainty in activations,
however, unlike the fully deterministic DVI, MC methods are then used to propagate this uncertainty
through non-linearities. The benefits of MCVI with the reparameterization trick (rMCVI) over vanilla
MCVI are two-fold:
•	The variance of the gradient estimates during back propagation are reduced (see details in
Kingma et al. (2015)).
•	Since the sampling dimension in rMCVI only appears on the activations and not on the
weights, an H0 × H linear transform can be implemented using SB × H0 by H0 × H matrix
multiplies (where S is the number of samples and B is the batch size). This contrasts with
the S × B × H0 by S × H0 × H batched matrix multiply required for MCVI. Although both
of these algorithms have the same asymptotic complexity O(SBH0H), a single large matrix
multiplication is generally more efficient on GPUs than smaller batched matrix multiplies.
Figure 7 shows empirical studies of the gradient variance and runtime for rMCVI vs. MCVI applied
to the model described in section 3.1.1 and figure 3. To evaluate the gradient variance, we initialize
the model with partially trained weights and measure the variance of the gradient of the ELBO
reconstruction term L with respect to variational parameters. Specifically, we inspect the gradient
with respect to the parameters ΣqL describing the variance of the q distribution for the weight matrix
in the final layer.
Gradient variance ∙= mean [Var (dL)]
s∈ΣqL	s
The plots in figure 7 serve to show that rMCVI is not fundamentally different from MCVI, and the
performance of one (on either the speed or variance metric) can be transformed into the other by
varying the number of samples. A comparison of DVI with rMCVI is included in table 3 using the
implementation labelled as “VI(KW)-1”.
23
Published as a conference paper at ICLR 2019
bost
U①s①q S一一①q∙sp三 pοο≡-①*一-IBOO-
-2.9-
-3.0-
conc
ener
nava
6-
5-
4-
Wine
PoWe
-2.80 ]
-2.85-
-2.90-
yach
0	20	40
epoch
DVI
MCVI
0	10	20
20
40
0
Figure 8:	Learning trajectories for the models from table 2.
F	Learning Curves
Figure 8 shows the test log-likelihood during the training of the models from table 2 using DVI and
MCVI inference algorithms. Since the underlying model is identical, both methods should achieve
the same test log-likelihood given infinite time and infinite MC samples (or a suitable learning rate
schedule) to mitigate the increased variance of the MCVI method. However, since we use only 10
samples and do not employ a leaning rate schedule, we find that MCVI converges to a log-likelihood
that is consistently worse than that achieved by DVI.
24