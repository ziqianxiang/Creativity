Published as a conference paper at ICLR 2019
Learning deep representations by mutual in-
FORMATION ESTIMATION AND MAXIMIZATION
R Devon Hjelm	Alex Fedorov
MSR Montreal, MILA, UdeM, IVADO MRN, UNM
devon.hjelm@microsoft.com
Samuel Lavoie-Marchildon
MILA, UdeM
Karan Grewal
U Toronto
Phil Bachman
MSR Montreal
Adam Trischler	Yoshua Bengio
MSR Montreal	MILA, UdeM, IVADO, CIFAR
Ab stract
This work investigates unsupervised learning of representations by maximizing
mutual information between an input and the output of a deep neural network en-
coder. Importantly, we show that structure matters: incorporating knowledge about
locality in the input into the objective can significantly improve a representation’s
suitability for downstream tasks. We further control characteristics of the repre-
sentation by matching to a prior distribution adversarially. Our method, which we
call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning
methods and compares favorably with fully-supervised learning on several clas-
sification tasks in with some standard architectures. DIM opens new avenues for
unsupervised learning of representations and is an important step towards flexible
formulations of representation learning objectives for specific end-goals.
1	Introduction
One core objective of deep learning is to discover useful representations, and the simple idea explored
here is to train a representation-learning function, i.e. an encoder, to maximize the mutual information
(MI) between its inputs and outputs. MI is notoriously difficult to compute, particularly in continuous
and high-dimensional settings. Fortunately, recent advances enable effective computation of MI
between high dimensional input/output pairs of deep neural networks (Belghazi et al., 2018). We
leverage MI estimation for representation learning and show that, depending on the downstream
task, maximizing MI between the complete input and the encoder output (i.e., global MI) is often
insufficient for learning useful representations. Rather, structure matters: maximizing the average
MI between the representation and local regions of the input (e.g. patches rather than the complete
image) can greatly improve the representation’s quality for, e.g., classification tasks, while global MI
plays a stronger role in the ability to reconstruct the full input given the representation.
Usefulness of a representation is not just a matter of information content: representational char-
acteristics like independence also play an important role (Gretton et al., 2012; Hyvarinen & Oja,
2000; Hinton, 2002; Schmidhuber, 1992; Bengio et al., 2013; Thomas et al., 2017). We combine MI
maximization with prior matching in a manner similar to adversarial autoencoders (AAE, Makhzani
et al., 2015) to constrain representations according to desired statistical properties. This approach is
closely related to the infomax optimization principle (Linsker, 1988; Bell & Sejnowski, 1995), so we
call our method Deep InfoMax (DIM). Our main contributions are the following:
•	We formalize Deep InfoMax (DIM), which simultaneously estimates and maximizes the
mutual information between input data and learned high-level representations.
•	Our mutual information maximization procedure can prioritize global or local information,
which we show can be used to tune the suitability of learned representations for classification
or reconstruction-style tasks.
•	We use adversarial learning (a` la Makhzani et al., 2015) to constrain the representation to
have desired statistical characteristics specific to a prior.
1
Published as a conference paper at ICLR 2019
•	We introduce two new measures of representation quality, one based on Mutual Information
Neural Estimation (MINE, Belghazi et al., 2018) and a neural dependency measure (NDM)
based on the work by Brakel & Bengio (2017), and we use these to bolster our comparison
of DIM to different unsupervised methods.
2	Related Work
There are many popular methods for learning representations. Classic methods, such as independent
component analysis (ICA, Bell & Sejnowski, 1995) and self-organizing maps (Kohonen, 1998),
generally lack the representational capacity of deep neural networks. More recent approaches
include deep volume-preserving maps (Dinh et al., 2014; 2016), deep clustering (Xie et al., 2016;
Chang et al., 2017), noise as targets (NAT, Bojanowski & Joulin, 2017), and self-supervised or
co-learning (Doersch & Zisserman, 2017; Dosovitskiy et al., 2016; Sajjadi et al., 2016).
Generative models are also commonly used for building representations (Vincent et al., 2010; Kingma
et al., 2014; Salimans et al., 2016; Rezende et al., 2016; Donahue et al., 2016), and mutual information
(MI) plays an important role in the quality of the representations they learn. In generative models that
rely on reconstruction (e.g., denoising, variational, and adversarial autoencoders, Vincent et al., 2008;
Rifai et al., 2012; Kingma & Welling, 2013; Makhzani et al., 2015), the reconstruction error can be
related to the MI as follows:
Ie(X, Y) = He(X ) — He(X |Y) ≥ He(X ) — Re,d (X|Y),	(1)
where X and Y denote the input and output of an encoder which is applied to inputs sampled from
some source distribution. Re,d(X |Y) denotes the expected reconstruction error of X given the codes
Y. He(X) andHe(X|Y) denote the marginal and conditional entropy ofX in the distribution formed
by applying the encoder to inputs sampled from the source distribution. Thus, in typical settings,
models with reconstruction-type objectives provide some guarantees on the amount of information
encoded in their intermediate representations. Similar guarantees exist for bi-directional adversarial
models (Dumoulin et al., 2016; Donahue et al., 2016), which adversarially train an encoder / decoder
to match their respective joint distributions or to minimize the reconstruction error (Chen et al., 2016).
Mutual-information estimation Methods based on mutual information have a long history in
unsupervised feature learning. The infomax principle (Linsker, 1988; Bell & Sejnowski, 1995),
as prescribed for neural networks, advocates maximizing MI between the input and output. This
is the basis of numerous ICA algorithms, which can be nonlinear (Hyvarinen & Pajunen, 1999;
Almeida, 2003) but are often hard to adapt for use with deep networks. Mutual Information Neural
Estimation (MINE, Belghazi et al., 2018) learns an estimate of the MI of continuous variables, is
strongly consistent, and can be used to learn better implicit bi-directional generative models. Deep
InfoMax (DIM) follows MINE in this regard, though we find that the generator is unnecessary.
We also find it unnecessary to use the exact KL-based formulation of MI. For example, a simple
alternative based on the Jensen-Shannon divergence (JSD) is more stable and provides better results.
We will show that DIM can work with various MI estimators. Most significantly, DIM can leverage
local structure in the input to improve the suitability of representations for classification.
Leveraging known structure in the input when designing objectives based on MI maximization is
nothing new (Becker, 1992; 1996; Wiskott & Sejnowski, 2002), and some very recent works also
follow this intuition. It has been shown in the case of discrete MI that data augmentations and other
transformations can be used to avoid degenerate solutions (Hu et al., 2017). Unsupervised clustering
and segmentation is attainable by maximizing the MI between images associated by transforms or
spatial proximity (Ji et al., 2018). Our work investigates the suitability of representations learned
across two different MI objectives that focus on local or global structure, a flexibility we believe is
necessary for training representations intended for different applications.
Proposed independently of DIM, Contrastive Predictive Coding (CPC, Oord et al., 2018) is a MI-
based approach that, like DIM, maximizes MI between global and local representation pairs. CPC
shares some motivations and computations with DIM, but there are important ways in which CPC and
DIM differ. CPC processes local features sequentially to build partial “summary features”, which are
used to make predictions about specific local features in the “future” of each summary feature. This
equates to ordered autoregression over the local features, and requires training separate estimators
2
Published as a conference paper at ICLR 2019
for each temporal offset at which one would like to predict the future. In contrast, the basic version
of DIM uses a single summary feature that is a function of all local features, and this “global” feature
predicts all local features simultaneously in a single step using a single estimator. Note that, when
using occlusions during training (see Section 4.3 for details), DIM performs both “self” predictions
and orderless autoregression.
3	Deep InfoMax
MXM feature map (see Figure 1)
Score
Figure 1:	The base encoder model in the
context of image data. An image (in this
case) is encoded using a convnet until reach-
ing a feature map of M × M feature vec-
tors corresponding to M × M input patches.
These vectors are summarized into a single
feature vector, Y . Our goal is to train this net-
work such that useful information about the
input is easily extracted from the high-level
features.
Figure 2:	Deep InfoMax (DIM) with a
global MI(X; Y ) objective. Here, we pass
both the high-level feature vector, Y , and the
lower-level M ×M feature map (see Figure 1)
through a discriminator to get the score. Fake
samples are drawn by combining the same
feature vector with a M × M feature map
from another image.
MXM features drawn from another image
Here We outline the general setting of training an encoder to maximize mutual information between
its input and output. Let X and Y be the domain and range of a continuous and (almost everywhere)
differentiable parametric function, Eψ : X → Y with parameters ψ (e.g., a neural network). These
parameters define a family of encoders, EΦ = {Eψ }ψ∈Ψ over Ψ. Assume that we are given a set of
training examples on an input space, X: X := {x(i) ∈ X}iN=1, with empirical probability distribution
P. We define Uψ,P to be the marginal distribution induced by pushing samples from P through Eψ .
I.e., Uψ,P is the distribution over encodings y ∈ Y produced by sampling observations X 〜X and
then sampling y 〜Eψ (x).
An example encoder for image data is given in Figure 1, which will be used in the following sections,
but this approach can easily be adapted for temporal data. Similar to the infomax optimization
principle (Linsker, 1988), we assert our encoder should be trained according to the following criteria:
•	Mutual information maximization: Find the set of parameters, ψ , such that the mutual
information, I(X; Eψ (X)), is maximized. Depending on the end-goal, this maximization
can be done over the complete input, X, or some structured or “local” subset.
•	Statistical constraints: Depending on the end-goal for the representation, the marginal
Uψ,P should match a prior distribution, V. Roughly speaking, this can be used to encourage
the output of the encoder to have desired characteristics (e.g., independence).
The formulation of these two objectives covered below we call Deep InfoMax (DIM).
3.1 Mutual information estimation and maximization
Our basic mutual information maximization framework is presented in Figure 2. The approach
follows Mutual Information Neural Estimation (MINE, Belghazi et al., 2018), which estimates mutual
information by training a classifier to distinguish between samples coming from the joint, J, and the
3
Published as a conference paper at ICLR 2019
Figure 3: Maximizing mutual information
between local features and global features.
First we encode the image to a feature map
that reflects some structural aspect of the data,
e.g. spatial locality, and we further summarize
this feature map into a global feature vector
(see Figure 1). We then concatenate this fea-
ture vector with the lower-level feature map
at every location. A score is produced for
each local-global pair through an additional
function (see the Appendix A.2 for details).
product of marginals, M, of random variables X and Y . MINE uses a lower-bound to the MI based
on the Donsker-Varadhan representation (DV, Donsker & Varadhan, 1983) of the KL-divergence,
I(X; Y) := Dkl(J∣∣M) ≥ ¾dv)(X; Y) := EJ[Tω(x,y)] - logEM[eTω(x,y)],	(2)
where Tω : X × Y → R is a discriminator function modeled by a neural network with parameters ω .
At a high level, we optimize Eψ by simultaneously estimating and maximizing I(X, Eψ (X)),
(ω ,Ψ)g = argmax 13 (X ； Eψ (X)),	(3)
ω,ψ
where the subscript G denotes “global” for reasons that will be clear later. However, there are some
important differences that distinguish our approach from MINE. First, because the encoder and
mutual information estimator are optimizing the same objective and require similar computations, we
share layers between these functions, so that Eψ = fψ ◦ Cψ and Tψ,3 = D3 ◦ g ◦ (Cψ , Eψ),1 where
g is a function that combines the encoder output with the lower layer.
Second, as we are primarily interested in maximizing MI, and not concerned with its precise value,
we can rely on non-KL divergences which may offer favourable trade-offs. For example, one could
define a Jensen-Shannon MI estimator (following the formulation of Nowozin et al., 2016),
I3JSD)(X； Eψ(X)) ：= EP[-sp(-Tψ,ω(x,Eψ(x)))] - Ep×f [sp(Tψ,3(x0,Eψ(x)))],	(4)
where X is an input sample, x0 is an input sampled from P = P, and SP(Z) = log(1+ez) is the softplus
function. A similar estimator appeared in Brakel & Bengio (2017) in the context of minimizing the
total correlation, and it amounts to the familiar binary cross-entropy. This is well-understood in terms
of neural network optimization and we find works better in practice (e.g., is more stable) than the
DV-based objective (e.g., see App. A.3). Intuitively, the Jensen-Shannon-based estimator should
behave similarly to the DV-based estimator in Eq. 2, since both act like classifiers whose objectives
maximize the expected log-ratio of the joint over the product of marginals. We show in App. A.1 the
relationship between the JSD estimator and the formal definition of mutual information.
Noise-Contrastive Estimation (NCE, Gutmann & Hyvarinen, 2010; 2012) was first used as a bound
on MI in Oord et al. (and called “infoNCE”, 2018), and this loss can also be used with DIM by
maximizing:
I3nψoNCE)(X； Eψ(X)) := EP Tψ,3(χ,Eψ(x)) - EP
For DIM, a key difference between the DV, JSD, and infoNCE formulations is whether an expectation
over P/P appears inside or outside of a log. In fact, the JSD-based objective mirrors the original
NCE formulation in Gutmann & HyVarinen (2010), which phrased unnormalized density estimation
as binary classification between the data distribution and a noise distribution. DIM sets the noise
distribution to the product of marginals over X/Y, and the data distribution to the true joint. The
infoNCE formulation in Eq. 5 follows a softmax-based version of NCE (Jozefowicz et al., 2016),
similar to ones used in the language modeling community (Mnih & Kavukcuoglu, 2013; Mikolov et al.,
1Here we slightly abuse the notation and use ψ for both parts of Eψ .
logXeTψ,ω(x0,Eψ(x))	.
(5)
4
Published as a conference paper at ICLR 2019
2013), and which has strong connections to the binary cross-entropy in the context of noise-contrastive
learning (Ma & Collins, 2018). In practice, implementations of these estimators appear quite similar
and can reuse most of the same code. We investigate JSD and infoNCE in our experiments, and
find that using infoNCE often outperforms JSD on downstream tasks, though this effect diminishes
with more challenging data. However, as we show in the App. (A.3), infoNCE and DV require a
∙-v
large number of negative samples (samples from P) to be competitive. We generate negative samples
using all combinations of global and local features at all locations of the relevant feature map, across
all images in a batch. For a batch of size B, that gives O(B × M2) negative samples per positive
example, which quickly becomes cumbersome with increasing batch size. We found that DIM with
the JSD loss is insensitive to the number of negative samples, and in fact outperforms infoNCE as the
number of negative samples becomes smaller.
3.2 Local mutual information maximization
The objective in Eq. 3 can be used to maximize MI between input and output, but ultimately this
may be undesirable depending on the task. For example, trivial pixel-level noise is useless for image
classification, so a representation may not benefit from encoding this information (e.g., in zero-shot
learning, transfer learning, etc.). In order to obtain a representation more suitable for classification,
we can instead maximize the average MI between the high-level representation and local patches of
the image. Because the same representation is encouraged to have high MI with all the patches, this
favours encoding aspects of the data that are shared across patches.
Suppose the feature vector is of limited capacity (number of units and range) and assume the encoder
does not support infinite output configurations. For maximizing the MI between the whole input and
the representation, the encoder can pick and choose what type of information in the input is passed
through the encoder, such as noise specific to local patches or pixels. However, if the encoder passes
information specific to only some parts of the input, this does not increase the MI with any of the
other patches that do not contain said noise. This encourages the encoder to prefer information that is
shared across the input, and this hypothesis is supported in our experiments below.
Our local DIM framework is presented in Figure 3. First we encode the input to a feature map,
Cψ(x) := {Cψ(i)}iM=×1 M that reflects useful structure in the data (e.g., spatial locality), indexed in this
case by i. Next, we summarize this local feature map into a global feature, Eψ (x) = fψ ◦ Cψ (x).
We then define our MI estimator on global/local pairs, maximizing the average estimated MI:
1 M2
(ω,ψ)L = arg max M XIω,ψ (cψi)(X)； Eψ (X)).	(6)
ω,ψ M	i=1
We found success optimizing this “local” objective with multiple easy-to-implement architectures,
and further implementation details are provided in the App. (A.2).
3.3 Matching representations to a prior distribution
Absolute magnitude of information is only one desirable property of a representation; depending on
the application, good representations can be compact (Gretton et al., 2012), independent (Hyvarinen
& Oja, 2000; Hinton, 2002; Dinh et al., 2014; Brakel & Bengio, 2017), disentangled (Schmidhuber,
1992; Rifai et al., 2012; Bengio et al., 2013; Chen et al., 2018; Gonzalez-Garcia et al., 2018), or
independently controllable (Thomas et al., 2017). DIM imposes statistical constraints onto learned
representations by implicitly training the encoder so that the push-forward distribution, Uψ,P, matches
a prior, V. This is done (see Figure 7 in the App. A.2) by training a discriminator, Dφ : Y → R, to
estimate the divergence, D(V∣∣Uψ,p), then training the encoder to minimize this estimate:
(ω ,ψ)p = arg min arg max Dφ(V∣∣Uψ,p) = Ev[log Dφ(y)]+ Ep[log(1 - Dφ(Eψ (x)))].	(7)
ψφ
This approach is similar to what is done in adversarial autoencoders (AAE, Makhzani et al., 2015),
but without a generator. It is also similar to noise as targets (Bojanowski & Joulin, 2017), but trains
the encoder to match the noise implicitly rather than using a priori noise samples as targets.
5
Published as a conference paper at ICLR 2019
All three objectives - global and local MI maximization and prior matching - can be used together,
and doing so we arrive at our complete objective for Deep InfoMax (DIM):
β M2
arg max Slωι,ψ (X； Eψ (X)) + M EIω2,ψ
ω1,ω2,ψ	M i=1
(X (i); Eψ (X))) + arg min arg max γ Dφ(V∣∣Uψ,p),
ψ
φ
(8)
where ω1 and ω2 are the discriminator parameters for the global and local objectives, respectively,
and α, β, and γ are hyperparameters. We will show below that choices in these hyperparameters
affect the learned representations in meaningful ways. As an interesting aside, we also show in the
App. (A.8) that this prior matching can be used alone to train a generator of image data.
4	Experiments
We test Deep InfoMax (DIM) on four imaging datasets to evaluate its representational properties:
•	CIFAR10 and CIFAR100 (Krizhevsky & Hinton, 2009): two small-scale labeled datasets
composed of 32 × 32 images with 10 and 100 classes respectively.
•	Tiny ImageNet: A reduced version of ImageNet (Krizhevsky & Hinton, 2009) images scaled
down to 64 × 64 with a total of 200 classes.
•	STL-10 (Coates et al., 2011): a dataset derived from ImageNet composed of 96 × 96 images
with a mixture of 100000 unlabeled training examples and 500 labeled examples per class.
We use data augmentation with this dataset, taking random 64 × 64 crops and flipping
horizontally during unsupervised learning.
•	CelebA (Yang et al., 2015, Appendix A.5 only): An image dataset composed of faces labeled
with 40 binary attributes. This dataset evaluates DIM’s ability to capture information that is
more fine-grained than the class label and coarser than individual pixels.
For our experiments, we compare DIM against various unsupervised methods: Variational AutoEn-
coders (VAE, Kingma & Welling, 2013), β-VAE (Higgins et al., 2016; Alemi et al., 2016), Adversarial
AutoEncoders (AAE, Makhzani et al., 2015), BiGAN (a.k.a. adversarially learned inference with
a deterministic encoder: Donahue et al., 2016; Dumoulin et al., 2016), Noise As Targets (NAT,
Bojanowski & Joulin, 2017), and Contrastive Predictive Coding (CPC, Oord et al., 2018). Note
that we take CPC to mean ordered autoregression using summary features to predict “future” local
features, independent of the constrastive loss used to evaluate the predictions (JSD, infoNCE, or DV).
See the App. (A.2) for details of the neural net architectures used in the experiments.
4.1	How do we evaluate the quality of a representation?
Evaluation of representations is case-driven and relies on various proxies. Linear separability is
commonly used as a proxy for disentanglement and mutual information (MI) between representations
and class labels. Unfortunately, this will not show whether the representation has high MI with
the class labels when the representation is not disentangled. Other works (Bojanowski & Joulin,
2017) have looked at transfer learning classification tasks by freezing the weights of the encoder and
training a small fully-connected neural network classifier using the representation as input. Others
still have more directly measured the MI between the labels and the representation (Rifai et al., 2012;
Chen et al., 2018), which can also reveal the representation’s degree of entanglement.
Class labels have limited use in evaluating representations, as we are often interested in information
encoded in the representation that is unknown to us. However, we can use mutual information neural
estimation (MINE, Belghazi et al., 2018) to more directly measure the MI between the input and
output of the encoder.
Next, we can directly measure the independence of the representation using a discriminator. Given a
batch of representations, we generate a factor-wise independent distribution with the same per-factor
marginals by randomly shuffling each factor along the batch dimension. A similar trick has been used
for learning maximally independent representations for sequential data (Brakel & Bengio, 2017). We
can train a discriminator to estimate the KL-divergence between the original representations (joint
6
Published as a conference paper at ICLR 2019
distribution of the factors) and the shuffled representations (product of the marginals, see Figure 12).
The higher the KL divergence, the more dependent the factors. We call this evaluation method Neural
Dependency Measure (NDM) and show that it is sensible and empirically consistent in the App.
(A.6).
To summarize, we use the following metrics for evaluating representations. For each of these, the
encoder is held fixed unless noted otherwise:
•	Linear classification using a support vector machine (SVM). This is simultaneously a
proxy for MI of the representation with linear separability.
•	Non-linear classification using a single hidden layer neural network (200 units) with
dropout. This is a proxy on MI of the representation with the labels separate from linear
separability as measured with the SVM above.
•	Semi-supervised learning (STL-10 here), that is, fine-tuning the complete encoder by
adding a small neural network on top of the last convolutional layer (matching architectures
with a standard fully-supervised classifier).
•	MS-SSIM (Wang et al., 2003), using a decoder trained on the L2 reconstruction loss. This
is a proxy for the total MI between the input and the representation and can indicate the
amount of encoded pixel-level information.
.—.
•	Mutual information neural estimate (MINE), Iρ(X, Eψ (x)), between the input, X, and
the output representation, Eψ(x), by training a discriminator with parameters ρ to maximize
the DV estimator of the KL-divergence.
•	Neural dependency measure (NDM) using a second discriminator that measures the KL
between Eψ (x) and a batch-wise shuffled version of Eψ (x).
For the neural network classification evaluation above, we performed experiments on all datasets
except CelebA, while for other measures we only looked at CIFAR10. For all classification tasks,
we built separate classifiers on the high-level vector representation (Y), the output of the previous
fully-connected layer (fc) and the last convolutional layer (conv). Model selection for the classifiers
was done by averaging the last 100 epochs of optimization, and the dropout rate and decaying learning
rate schedule was set uniformly to alleviate over-fitting on the test set across all models.
4.2	Representation learning comparison across models
In the following experiments, DIM(G) refers to DIM with a global-only objective (α = 1, β = 0, γ =
1) and DIM(L) refers to DIM with a local-only objective (α = 0, β = 1, γ = 0.1), the latter chosen
from the results of an ablation study presented in the App. (A.5). For the prior, we chose a compact
uniform distribution on [0, 1]64, which worked better in practice than other priors, such as Gaussian,
unit ball, or unit sphere.
Classification comparisons Our classification results can be found in Tables 1, 2, and 3. In general,
DIM with the local objective, DIM(L), outperformed all models presented here by a significant margin
on all datasets, regardless of which layer the representation was drawn from, with exception to CPC.
For the specific settings presented (architectures, no data augmentation for datasets except for STL-
10), DIM(L) performs as well as or outperforms a fully-supervised classifier without fine-tuning,
which indicates that the representations are nearly as good as or better than the raw pixels given the
model constraints in this setting. Note, however, that a fully supervised classifier can perform much
better on all of these benchmarks, especially when specialized architectures and carefully-chosen
data augmentations are used. Competitive or better results on CIFAR10 also exist (albeit in different
settings, e.g., Coates et al., 2011; Dosovitskiy et al., 2016), but to our knowledge our STL-10 results
are state-of-the-art for unsupervised learning. The results in this setting support the hypothesis that
our local DIM objective is suitable for extracting class information.
Our results show that infoNCE tends to perform best, but differences between infoNCE and JSD
diminish with larger datasets. DV can compete with JSD with smaller datasets, but DV performs
much worse with larger datasets.
For CPC, we were only able to achieve marginally better performance than BiGAN with the settings
above. However, when we adopted the strided crop architecture found in Oord et al. (2018), both
7
Published as a conference paper at ICLR 2019
Table 1: Classification accuracy (top 1) results on CIFAR10 and CIFAR100. DIM(L) (i.e., with the
local-only objective) outperforms all other unsupervised methods presented by a wide margin. In
addition, DIM(L) approaches or even surpasses a fully-supervised classifier with similar architecture.
DIM with the global-only objective is competitive with some models across tasks, but falls short
when compared to generative models and DIM(L) on CIFAR100. Fully-supervised classification
results are provided for comparison.
Model	conv	CIFAR10 fc (1024)	Y (64)	Conv	CIFAR100 fc (1024)	Y (64)
Fully supervised		75.39					42.27			
VAAE	60.71	60.54	54.61	37.21	34.05	24.22
AE	62.19	55.78	54.47	31.50	23.89	27.44
β-VAE	62.4	57.89	55.43	32.28	26.89	28.96
AAE	59.44	57.19	52.81	36.22	33.38	23.25
BiGAN	62.57	62.74	52.54	37.59	33.34	21.49
NAT	56.19	51.29	31.16	29.18	24.57	9.72
DIM(G)	52.2	52.84	43.17	27.68	24.35	19.98
DIM(L)(DV)	72.66	70.60	64.71	48.52	44.44	39.27
DIM(L)(JSD)	73.25	73.62	66.96	48.13	45.92	39.60
DIM(L)(infoNCE)	75.21	75.57	69.13	49.74	47.72	41.61
Table 2: Classification accuracy (top 1) results on Tiny ImageNet and STL-10. For Tiny ImageNet,
DIM with the local objective outperforms all other models presented by a large margin, and approaches
accuracy of a fully-supervised classifier similar to the Alexnet architecture used here.
	conv	Tiny ImageNet fc (4096)	Y (64)	STL-10 (random crop pretraining)			
				ConV	fc (4096)	Y (64)	SS
Fully supervised		36.60					687				
VAEE	18.63	16.88	11.93	58.27	56.72	46.47	68.65
AE	19.07	16.39	11.82	58.19	55.57	46.82	70.29
β-VAE	19.29	16.77	12.43	57.15	55.14	46.87	70.53
AAE	18.04	17.27	11.49	59.54	54.47	43.89	64.15
BiGAN	24.38	20.21	13.06	71.53	67.18	58.48	74.77
NAT	13.70	11.62	1.20	64.32	61.43	48.84	70.75
DIM(G)	11.32	6.34	4.95	42.03	30.82	28.09	51.36
DIM(L)(DV)	30.35	29.51	28.18	69.15	63.81	61.92	71.22
DIM(L)(JSD)	33.54	36.88	31.66	72.86	70.85	65.93	76.96
DIM(L)(infoNCE)	34.21	38.09	33.33	72.57	70.00	67.08	76.81
CPC and DIM performance improved considerably. We chose a crop size of 25% of the image size
in width and depth with a stride of 12.5% the image size (e.g., 8 × 8 crops with 4 × 4 strides for
CIFAR10, 16 × 16 crops with 8 × 8 strides for STL-10), so that there were a total of 7 × 7 local
features. For both DIM(L) and CPC, we used infoNCE as well as the same “encode-and-dot-product”
architecture (tantamount to a deep bilinear model), rather than the shallow bilinear model used in
Oord et al. (2018). For CPC, we used a total of 3 such networks, where each network for CPC is
used for a separate prediction task of local feature maps in the next 3 rows of a summary predictor
feature within each column.2 For simplicity, we omitted the prior term, β, from DIM. Without data
augmentation on CIFAR10, CPC performs worse than DIM(L) with a ResNet-50 (He et al., 2016)
type architecture. For experiments we ran on STL-10 with data augmentation (using the same encoder
architecture as Table 2), CPC and DIM were competitive, with CPC performing slightly better.
CPC makes predictions based on multiple summary features, each of which contains different amounts
of information about the full input. We can add similar behavior to DIM by computing less global
features which condition on 3 × 3 blocks of local features sampled at random from the full 7 × 7
sets of local features. We then maximize mutual information between these less global features and
the full sets of local features. We share a single MI estimator across all possible 3 × 3 blocks of
local features when using this version of DIM. This represents a particular instance of the occlusion
technique described in Section 4.3. The resulting model gave a significant performance boost to
2Note that this is slightly different from the setup used in Oord et al. (2018), which used a total of 5 such
predictors, though we found other configurations performed similarly.
8
Published as a conference paper at ICLR 2019
Table 3: Comparisons of DIM with Contrastive Predictive Coding (CPC, Oord et al., 2018). These
experiments used a strided-crop architecture similar to the one used in Oord et al. (2018). For
CIFAR10 we used a ResNet-50 encoder, and for STL-10 we used the same architecture as for Table 2.
We also tested a version of DIM that computes the global representation from a 3x3 block of local
features randomly selected from the full 7x7 set of local features. This is a particular instance of the
occlusions described in Section 4.3. DIM(L) is competitive with CPC in these settings.
Model	CIFAR10 (no data augmentation)	STL10 (random crop pretraining)
DIM(L) single global	8095	76:97
CPC	77.45	77.81
DIM(L) multiple globals		77.51			78.21	
Table 4: Extended comparisons on CIFAR10. Linear classification results using SVM are over five
runs. MS-SSIM is estimated by training a separate decoder using the fixed representation as input and
minimizing the L2 loss with the original input. Mutual information estimates were done using MINE
and the neural dependence measure (NDM) were trained using a discriminator between unshuffled
and shuffled representations.
Model	Proxies				Neural Estimators ^	
	SVM (conv)	SVM (fc)	SVM (Y)	MS-SSIM	Ip(x,y )	NDM
^VAE	53.83 ± 0.62	42.14 ± 3.69	39.59 ± 0.01	072	93.02	1.62
AAE	55.22 ± 0.06	43.34 ± 1.10	37.76 ± 0.18	0.67	87.48	0.03
BiGAN	56.40 ± 1.12	38.42 ± 6.86	44.90 ± 0.13	0.46	37.69	24.49
NAT	48.62 ± 0.02	42.63 ± 3.69	39.59 ± 0.01	0.29	6.04	0.02
DIM(G)	46.8 ± 2.29	28.79 ± 7.29	29.08 ± 0.24	0.49	49.63	9.96
DIM(L+G)	57.55 ± 1.442	45.56 ± 4.18	18.63 ± 4.79	0.53	101.65	22.89
DIM(L)	63.25 ± 0.86	54.06 ± 3.6	49.62 ± 0.3	0.37	45.09	9.18
DIM for STL-10. Surprisingly, this same architecture performed worse than using the fully global
representation with CIFAR10. Overall DIM only slightly outperforms CPC in this setting, which
suggests that the strictly ordered autoregression of CPC may be unnecessary for some tasks.
Extended comparisons Tables 4 shows results on linear separability, reconstruction (MS-SSIM),
mutual information, and dependence (NDM) with the CIFAR10 dataset. We did not compare to CPC
due to the divergence of architectures. For linear classifier results (SVC), we trained five support
vector machines with a simple hinge loss for each model, averaging the test accuracy. For MINE,
we used a decaying learning rate schedule, which helped reduce variance in estimates and provided
faster convergence.
MS-SSIM correlated well with the MI estimate provided by MINE, indicating that these models
encoded pixel-wise information well. Overall, all models showed much lower dependence than
BiGAN, indicating the marginal of the encoder output is not matching to the generator’s spherical
Gaussian input prior, though the mixed local/global version of DIM is close. For MI, reconstruction-
based models like VAE and AAE have high scores, and we found that combining local and global
DIM objectives had very high scores (α = 0.5, β = 0.1 is presented here as DIM(L+G)). For more
in-depth analyses, please see the ablation studies and the nearest-neighbor analysis in the App. (A.4,
A.5).
4.3	Adding coordinate information and occlusions
Maximizing MI between global and local features is not the only way to leverage image structure.
We consider augmenting DIM by adding input occlusion when computing global features and by
adding auxiliary tasks which maximize MI between local features and absolute or relative spatial
coordinates given a global feature. These additions improve classification results (see Table 5).
For occlusion, we randomly occlude part of the input when computing the global features, but
compute local features using the full input. Maximizing MI between occluded global features and
unoccluded local features aggressively encourages the global features to encode information which
is shared across the entire image. For coordinate prediction, we maximize the model’s ability to
predict the coordinates (i, j) of a local feature c(i,j) = Cψ(i,j)(x) after computing the global features
9
Published as a conference paper at ICLR 2019
Table 5: Augmenting infoNCE DIM with additional structural information - adding coordinate
prediction tasks or occluding input patches when computing the global feature vector in DIM can
improve the classification accuracy, particularly with the highly-compressed global features.
Model	Y (64)	CIFAR10 fc (1024)	conv	Y (64)	CIFAR100 fc (1024)	conv
-DM	70.65	~~73:33^^	77.46	44.27	47.96	49.90
DIM (coord)	71.56	73.89	77.28	45.37	48.61	50.27
DIM (occlude)	72.87	74.45	76.77	44.89	47.65	48.87
DIM (coord + occlude)	73.99	75.15	77.27	45.96	48.00	48.72
y = Eψ(x). To accomplish this, we maximize E[logpθ((i,j)∣y, c(i,j))] (i.e., minimize the cross-
entropy). We can extend the task to maximize conditional MI given global features y between pairs
of local features (c(i,j), c(i0,j0)) and their relative coordinates (i - i0,j - j0). This objective can be
written as E[logpθ((i - i0, j - j0)|y, c(i,j), c(i0,j0))]. We use both these objectives in our results.
Additional implementation details can be found in the App. (A.7). Roughly speaking, our input
occlusions and coordinate prediction tasks can be interpreted as generalizations of inpainting (Pathak
et al., 2016) and context prediction (Doersch et al., 2015) tasks which have previously been proposed
for self-supervised feature learning. Augmenting DIM with these tasks helps move our method
further towards learning representations which encode images (or other types of inputs) not just in
terms of compressing their low-level (e.g. pixel) content, but in terms of distributions over relations
among higher-level features extracted from their lower-level content.
5	Conclusion
In this work, we introduced Deep InfoMax (DIM), a new method for learning unsupervised represen-
tations by maximizing mutual information, allowing for representations that contain locally-consistent
information across structural “locations” (e.g., patches in an image). This provides a straightforward
and flexible way to learn representations that perform well on a variety of tasks. We believe that this
is an important direction in learning higher-level representations.
6	Acknowledgements
RDH received partial support from IVADO, NIH grants 2R01EB005846, P20GM103472,
P30GM122734, and R01EB020407, and NSF grant 1539067. AF received partial support from
NIH grants R01EB020407, R01EB006841, P20GM103472, P30GM122734. We would also like to
thank Geoff Gordon (MSR), Ishmael Belghazi (MILA), Marc Bellemare (Google Brain), Mikolaj
BinkOWSki (Imperial College London), Simon Sebbagh, and Aaron Courville (MILA) for their useful
input at various points through the course of this research.
References
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Luls B Almeida. Linear and nonlinear ica based on mutual information. The Journal of Machine
Learning Research, 4:1297-1318, 2003.
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In International Conference on Learning Representations, 2017.
Suzanna Becker. An information-theoretic unsupervised learning algorithm for neural networks.
University of Toronto, 1992.
Suzanna Becker. Mutual information maximization: models of cortical self-organization. Network:
Computation in neural systems, 7(1):7-31, 1996.
10
Published as a conference paper at ICLR 2019
Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and
R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062,
ICML’2018, 2018.
Anthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind separation
and blind deconvolution. Neural computation, 7(6):1129-1159, 1995.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI), 35(8):1798-1828,
2013.
Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. arXiv preprint
arXiv:1704.05310, 2017.
Philemon Brakel and Yoshua Bengio. Learning independent features with adversarial nets for
non-linear ica. arXiv preprint arXiv:1710.05050, 2017.
Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep adaptive
image clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5879-5887, 2017.
Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement
in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelligence
and statistics, pp. 215-223, 2011.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In The IEEE
International Conference on Computer Vision (ICCV), 2017.
Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In Proceedings of the IEEE International Conference on Computer Vision,
2015.
Jeff Donahue, PhiliPP Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
M.D Donsker and S.R.S Varadhan. AsymPtotic evaluation of certain markov Process exPectations for
large time, iv. Communications on Pure and Applied Mathematics, 36(2):183-212, 1983.
Alexey Dosovitskiy, PhiliPP Fischer, Jost Tobias SPringenberg, Martin Riedmiller, and Thomas Brox.
Discriminative unsuPervised feature learning with exemPlar convolutional neural networks. IEEE
transactions on pattern analysis and machine intelligence, 38(9):1734-1747, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier MastroPietro,
and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Abel Gonzalez-Garcia, Joost van de Weijer, and Yoshua Bengio. Image-to-image translation for
cross-domain disentanglement. arXiv preprint arXiv:1805.09730, 2018.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A
kernel two-samPle test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
11
Published as a conference paper at ICLR 2019
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Michael Gutmann and AaPo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics, pp. 297-304, 2010.
Michael U Gutmann and Aapo Hyvarinen. Noise-contrastive estimation of unnormalized statistical
models, with applications to natural image statistics. Journal of Machine Learning Research, 13
(Feb):307-361, 2012.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. Openreview, 2016.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
R Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, and Yoshua Bengio.
Boundary-seeking generative adversarial networks. In International Conference on Learning
Representations, 2018.
Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning
discrete representations via information maximizing self-augmented training. arXiv preprint
arXiv:1702.08720, 2017.
Aapo Hyvarinen and Erkki Oja. Independent component analysis: algorithms and applications.
Neural networks, 13(4):411-430, 2000.
Aapo Hyvarinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and
uniqueness results. Neural Networks, 12(3):429-439, 1999.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information distillation for unsupervised
image segmentation and clustering. arXiv preprint arXiv:1807.06653, 2018.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the
limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems, pp.
3581-3589, 2014.
Teuvo Kohonen. The self-organizing map. Neurocomputing, 21(1-3):1-6, 1998.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Ralph Linsker. Self-organization in a perceptual network. IEEE Computer, 21(3):105-117, 1988.
doi: 10.1109/2.36. URL https://doi.org/10.1109/2.36.
12
Published as a conference paper at ICLR 2019
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional
models: Consistency and statistical efficiency. arXiv preprint arXiv:1809.01812, 2018.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In International Conference on Machine Learning, pp. 3478-3487, 2018.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in neural information processing
systems, pp. 3111-3119, 2013.
Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive
estimation. In Advances in neural information processing systems, pp. 2265-2273, 2013.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Systems,
pp. 271-279, 2016.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
arXiv preprint arXiv:1601.06759, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Deepak Pathak, Philipp KrahenbuhL Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wierstra. One-
shot generalization in deep generative models. arXiv preprint arXiv:1603.05106, 2016.
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-
encoders: Explicit invariance during feature extraction. In Proceedings of the 28th International
Conference on International Conference on Machine Learning, pp. 833-840. Omnipress, 2011.
Salah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent, and Mehdi Mirza. Disentangling
factors of variation for facial expression recognition. In European Conference on Computer Vision,
pp. 808-822. Springer, 2012.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transforma-
tions and perturbations for deep semi-supervised learning. In Advances in Neural Information
Processing Systems, pp. 1163-1171, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp.
2234-2242, 2016.
JUrgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation,
4(6):863-879, 1992.
Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean
Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently controllable features.
arXiv preprint arXiv:1708.01289, 2017.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096-1103. ACM, 2008.
13
Published as a conference paper at ICLR 2019
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. Journal ofmaChine learning research, 11(Dec):3371-3408, 2010.
Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In Signals, Systems and Computers, 2004. Conference Record of the Thirty-Seventh
Asilomar Conference on, volume 2, pp. 1398-1402. Ieee, 2003.
Laurenz Wiskott and Terrence J Sejnowski. Slow feature analysis: Unsupervised learning of
invariances. Neural computation, 14(4):715-770, 2002.
Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.
In International conference on machine learning, pp. 478-487, 2016.
Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. From facial parts responses to face
detection: A deep learning approach. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 3676-3684, 2015.
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-
scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,
2015.
14
Published as a conference paper at ICLR 2019
A Appendix
A.1 On the Jensen-Shannon divergence and mutual information
Here we show the relationship between the Jensen-Shannon divergence (JSD) between the joint
and the product of marginals and the pointwise mutual information (PMI). Let p(x) and p(y) be
two marginal densities, and define p(y|x) and p(x, y) = p(y|x)p(x) as the conditional and joint
distribution, respectively. Construct a probability mixture density, m(χ, y) = 1 (p(χ)p(y) + p(x, y)).
It follows that m(x) = p(x), m(y) = p(y), and m(y∣x) = 1 (p(y) + p(y∣x)).
Note that:
log m(y∣x) = log (ɪ(p(y) + p(y∣x))) = log 2 + logp(y) + log(1 + Ppyyx)) .	(9)
Discarding some constants:
J S D(p(x, y)||p(x)p(y))
“Ex〜p(x) [Ey〜p(y∣x)
“Ex 〜p(x)
“Ex 〜p(x)
“Ex 〜P(X)
“Ex 〜P(X)
[Ey~p(y∣x)
[Ey~p(y∣x)
[Ey~p(y∣x)
Ey~p(y∣x)
hlo p(y|x)P(X) i + E / ∖ hloɑ. P(V)P(X) ii
[log m(y∣x)m(x) J + Ey〜P(V)[log m(y∣x)m(x)
l^lcσ∙ P(V|x) lno∙ Λ i P(y|x)\] ι E /、∣^ lnσ∙ 11 ∣ P(y|x)All
[log^T(yπ - log (1 + ^T(yπJJ + Ey〜P(y) [Tog (1 + -^)ΓJ∖∖
l^lnσ. P(y∖x)↑	2E „ l x l^lnσ. 11 i P(y|x)All
[log~KyT∖ - 2Ey〜m(y∖x) [log (1 + ~K)TJ∖∖
heC P(y|x)	2m(y∖x) lcθ∙ 11 -U P(y∖x)']^∣
[log P(y) - 2 P(y∖x) log(1+ P(y)川
l^lnσ∙P(y∖x)	∩ i P(y) ∖lcσ∙(ι jP(y∖x)']^∣	门八、
[log再T - (1 + Py同)log(1 + 再τy)JJ .	(IO)
The quantity inside the expectation of Eqn. 10 is a concave, monotonically increasing function of the
ratio PPyyX), which is exactly ePMI(x,y). Note this relationship does not hold for the JSD of arbitrary
distributions, as the the joint and product of marginals are intimately coupled.
We can verify our theoretical observation by plotting the JSD and KL divergences between the joint
and the product of marginals, the latter of which is the formal definition of mutual information (MI).
As computing the continuous MI is difficult, we assume a discrete input with uniform probability,
p(x) (e.g., these could be one-hot variables indicating one of N i.i.d. random samples), and a
randomly initialized N × M joint distribution, p(x, y), such that PjM=1p(xi, yj) = 1 ∀i. For this
joint distribution, we sample from a uniform distribution, then apply dropout to encourage sparsity to
simulate the situation when there is no bijective function between x and y, then apply a softmax. As
the distributions are discrete, we can compute the KL and JSD between p(x, y) and p(x)p(y).
We ran these experiments with matched input / output dimensions of 8, 16, 32, 64, and 128, randomly
drawing 1000 joint distributions, and computed the KL and JSD divergences directly. Our results
(Figure A.1) indicate that the KL (traditional definition of mutual information) and the JSD have an
approximately monotonic relationship. Overall, the distributions with the highest mutual information
also have the highest JSD.
A.2 Experiment and architecture details
Here we provide architectural details for our experiments. Example code for running Deep Infomax
(DIM) can be found at https://github.com/rdevon/DIM.
Encoder We used an encoder similar to a deep convolutional GAN (DCGAN, Radford et al., 2015)
discriminator for CIFAR10 and CIFAR100, and for all other datasets we used an Alexnet (Krizhevsky
et al., 2012) architecture similar to that found in Donahue et al. (2016). ReLU activations and batch
norm (Ioffe & Szegedy, 2015) were used on every hidden layer. For the DCGAN architecture, a
single hidden layer with 1024 units was used after the final convolutional layer, and for the Alexnet
architecture it was two hidden layers with 4096. For all experiments, the output of all encoders was a
64 dimensional vector.
15
Published as a conference paper at ICLR 2019
Figure 4:	Scatter plots of MI(x; y) versus
JSD(p(x, y)||p(x)p(y))
with discrete inputs and a given ran-
domized and sparse joint distribution,
p(x, y). 8 × 8 indicates a square joint
distribution with 8 rows and 8 columns.
Our experiments indicate a strong mono-
tonic relationship between MI(x; y) and
JSD(p(x, y)||p(x)p(y)) Overall, the distri-
butions with the highest MI(x; y) have the
highest JSD(p(x, y)||p(x)p(y)).
Mutual information discriminators For the global mutual information objective, we first encode
the input into a feature map, Cψ (x), which in this case is the output of the last convolutional layer.
We then encode this representation further using linear layers as detailed above to get Eψ (x). Cψ (x)
is then flattened, then concatenated with Eψ(x). We then pass this to a fully-connected network with
two 512-unit hidden layers (see Table 6).
Table 6: Global DIM network architecture
Operation	Size	Activation
Input → Linear layer	512	-ReLU-
Linear layer	512	ReLU
Linear layer	1	
We tested two different architectures for the local objective. The first (Figure 5) concatenated the
global feature vector with the feature map at every location, i.e., {[Cψ(i)(x), Eψ(x)]}iM=×1 M. A 1 × 1
convolutional discriminator is then used to score the (feature map, feature vector) pair,
Tψ(i,)ω(x,y(x))=Dω([Cψ(i)(x),Eψ(x)]).	(11)
Fake samples are generated by combining global feature vectors with local feature maps coming from
different images, x0 :
Tψ(i,)ω(x0,Eψ(x))=Dω([Cψ(i)(x0),Eψ(x)]).	(12)
This architecture is featured in the results of Table 4, as well as the ablation and nearest-neighbor
studies below. We used a 1 × 1 convnet with two 512-unit hidden layers as discriminator (Table 7).
Table 7: Local DIM Concat-and-convolve network architecture
Operation	Size	Activation
Input → 1 × 1 conv	512	-ReLU-
1 × 1conv	512	ReLU
1 × 1conv	1	
The other architecture we tested (Figure 6) is based on non-linearly embedding the global and
local features in a (much) higher-dimensional space, and then computing pair-wise scores using
dot products between their high-dimensional embeddings. This enables efficient evaluation of a
large number of pair-wise scores, thus allowing us to use large numbers of positive/negative samples.
Given a sufficiently high-dimensional embedding space, this approach can represent (almost) arbitrary
classes of pair-wise functions that are non-linear in the original, lower-dimensional features. For
more information, refer to Reproducing Kernel Hilbert Spaces. We pass the global feature through a
16
Published as a conference paper at ICLR 2019
MXM features	MXM Scores
+ replicated feature vector
MXM features drawn from another image
Figure 5:	Concat-and-convolve architec-
ture. The global feature vector is concate-
nated with the lower-level feature map at ev-
ery location. A 1 × 1 convolutional discrimi-
nator is then used to score the “real” feature
map / feature vector pair, while the “fake” pair
is produced by pairing the feature vector with
a feature map from another image.
Figure 6:	Encode-and-dot-product archi-
tecture. The global feature vector is en-
coded using a fully-connected network, and
the lower-level feature map is encoded us-
ing 1x1 convolutions, but with the same num-
ber of output features. We then take the dot-
product between the feature at each location
of the feature map encoding and the encoded
global vector for scores.
Figure 7:	Matching the output of the en-
coder to a prior. “Real” samples are drawn
from a prior while “fake” samples from the en-
coder output are sent to a discriminator. The
discriminator is trained to distinguish between
(classify) these sets of samples. The encoder
is trained to “fool” the discriminator.
fully connected neural network to get the encoded global feature, Sω (Eψ (x)). In our experiments,
we used a single hidden layer network with a linear shortcut (See Table 8).
Table 8: Local DIM encoder-and-dot architecture for global feature
Operation	Size	Activation	Output
Input → Linear Linear	2048 2048	-ReLU-	Output 1
Input → Linear Output 1 + Output 2	2048	ReLU	Output 2
We embed each local feature in the local feature map Cψ (x) using an architecture which matches the
one for global feature embedding. We apply it via 1 × 1 convolutions. Details are in Table 9.
17
Published as a conference paper at ICLR 2019
Table 9: Local DIM encoder-and-dot architecture for local features
Operation	Size	Activation	Output
Input → 1 × 1conv 1 × 1 conv	2048 2048	-ReLU-	Output 1
Input → 1 × 1 Conv Output 1 + Output 2 Block Layer Norm	2048	ReLU	Output 2
Finally, the outputs of these two networks are combined by matrix multiplication, summing over the
feature dimension (2048 in the example above). As this is computed over a batch, this allows us to
efficiently compute both positive and negative examples simultaneously. This architecture is featured
in our main classification results in Tables 1, 2, and 5.
For the local objective, the feature map, Cψ(x), can be taken from any level of the encoder, Eψ. For
the global objective, this is the last convolutional layer, and this objective was insensitive to which
layer we used. For the local objectives, we found that using the next-to-last layer worked best for
CIFAR10 and CIFAR100, while for the other larger datasets it was the previous layer. This sensitivity
is likely due to the relative size of the of the receptive fields, and further analysis is necessary to better
understand this effect. Note that all feature maps used for DIM included the final batch normalization
and ReLU activation.
Prior matching Figure 7 shows a high-level overview of the prior matching architecture. The
discriminator used to match the prior in DIM was a fully-connected network with two hidden layers
of 1000 and 200 units (Table 10).
Table 10: Prior matching network architecture
Operation	Size	Activation
Input → Linear layer	1000	-ReLU-
Linear layer	200	ReLU
Linear layer	1	
Generative models For generative models, we used a similar setup as that found in Donahue et al.
(2016) for the generators / decoders, where we used a generator from DCGAN in all experiments.
All models were trained using Adam with a learning rate of 1 × 10-4 for 1000 epochs for CIFAR10
and CIFAR100 and for 200 epochs for all other datasets.
Contrastive Predictive Coding For Contrastive Predictive Coding (CPC, Oord et al., 2018), we
used a simple a GRU-based PixelRNN (Oord et al., 2016) with the same number of hidden units as
the feature map depth. All experiments with CPC had the global state dimension matched with the
size of these recurrent hidden units.
A.3 Sampling strategies
We found both infoNCE and the DV-based estimators were sensitive to negative sampling strategies,
while the JSD-based estimator was insensitive. JSD worked better (1 - 2% accuracy improvement) by
excluding positive samples from the product of marginals, so we exclude them in our implementation.
It is quite likely that this is because our batchwise sampling strategy overestimate the frequency of
positive examples as measured across the complete dataset. infoNCE was highly sensitive to the
number of negative samples for estimating the log-expectation term (see Figure 9). With high sample
size, infoNCE outperformed JSD on many tasks, but performance drops quickly as we reduce the
number of images used for this estimation. This may become more problematic for larger datasets and
networks where available memory is an issue. DV was outperformed by JSD even with the maximum
number of negative samples used in these experiments, and even worse was highly unstable as the
number of negative samples dropped.
18
Published as a conference paper at ICLR 2019
Figure 8: Nearest-neighbor using the L1 distance on the encoded Tiny ImageNet images, with
DIM(G) and DIM(L). The images on the far left are randomly-selected reference images (query)
from the training set and the four images their nearest-neighbor from the test set as measured in the
representation, sorted by proximity. The nearest neighbors from DIM(L) are much more interpretable
than those with the purely global objective.
Figure 9: Classification accuracies (left: global representation, Y , right: convolutional layer) for
CIFAR10, first training DIM, then training a classifier for 1000 epochs, keeping the encoder fixed.
Accuracies shown averaged over the last 100 epochs, averaged over 3 runs, for the infoNCE, JSD,
and DV DIM losses. x-axis is log base-2 of the number of negative samples (0 mean one negative
sample per positive sample). JSD is insensitive to the number of negative samples, while infoNCE
shows a decline as the number of negative samples decreases. DV also declines, but becomes unstable
as the number of negative samples becomes too low.
A.4 Nearest-neighbor analysis
In order to better understand the metric structure of DIM’s representations, we did a nearest-neighbor
analysis, randomly choosing a sample from each class in the test set, ordering the test set in terms of
L1 distance in the representation space (to reflect the uniform prior), then selecting the four with the
lowest distance. Our results in Figure 8 show that DIM with a local-only objective, DIM(L), learns
a representation with a much more interpretable structure across the image. However, our result
potentially highlights an issue with using only consistent information across patches, as many of the
nearest neighbors share patterns (colors, shapes, texture) but not class.
19
Published as a conference paper at ICLR 2019
Y = 0.1: Classifier Accuracy
2.00
1.75
1.50
1.25
t¾ 1.0O
0.75
0.50
0.25
0.00
2.00
1.75
1.50
1.25
迎 1.00
0.75
0.50
0.25
0.00
0.0
0.5
1.0
1.5
2.0
Y
2.00
2.00
1.75
1.75
1.50
1.50
α
1.25
1.00
0.75
0.50
0.25
1.25
t¾ 1.00
0.75
0.50
0.25
0.00
65
60
55
50
45
40
35
1.0
Y
Figure 10: Results from the ablation studies with DIM on CIFAR10. Values calculated are points on
the grid, and the heatmaps were derived by bilinear interpolation. Heatmaps were thresholded at the
minimum value (or maximum for NDM) for visual clarity. Highest (or lowest) value is marked on
the grid. NDM here was measured without the sigmoid function.
Figure 11: Ablation study on CelebA over the global and local parameters, α and β . The classification
task is multinomial, so provided is the average, minimum, and maximum class accuracies across
attibutes. While the local objective is crucial, the global objective plays a stronger role here than with
other datasets.
A.5 Ablation studies
To better understand the effects of hyperparameters α, β, and γ on the representational characteristics
of the encoder, we performed several ablation studies. These illuminate the relative importance of
global verses local mutual information objectives as well as the role of the prior.
Local versus global mutual information maximization The results of our ablation study for
DIM on CIFAR10 are presented in Figure 10. In general, good classification performance is highly
dependent on the local term, β, while good reconstruction is highly dependent on the global term,
α. However, a small amount of α helps in classification accuracy and a small about of β improves
reconstruction. For mutual information, we found that having a combination of α and β yielded
higher MINE estimates. Finally, for CelebA (Figure 11), where the classification task is more
fine-grained (is composed of potentially locally-specified labels, such as “lipstick” or “smiling”), the
global objective plays a stronger role than with classification on other datasets (e.g., CIFAR10).
20
Published as a conference paper at ICLR 2019
Figure 12:	A schematic of learning the
Neural Dependency Measure. For a
given batch of inputs, we encode this
into a set of representations. We then
shuffle each feature (dimension of the
feature vector) across the batch axis.
The original version is sent to the dis-
criminator and given the label “real”,
while the shuffled version is labeled as
“fake”. The easier this task, the more
dependent the components of the repre-
sentation.
Figure 13:	Neural Dependency Mea-
sures (NDMs) for various β-VAE (Alemi
et al., 2016; Higgins et al., 2016) models
(0.1, 0.5, 1.0, 1.5, 2.0, 4.0). Error bars are
provided over five runs of each VAE and es-
timating NDM with 10 different networks.
We find that there is a strong trend as we in-
crease the value of β and that the estimates
are relatively consistent and informative w.r.t.
independence as expected.
The effect of the prior We found including the prior term, γ, was absolutely necessary for ensuring
low dependence between components of the high-level representation, Eψ (x), as measured by NDM.
In addition, a small amount of the prior term helps improve classification results when used with the
local term, β . This may be because the additional constraints imposed on the representation help to
encourage the local term to focus on consistent, rather than trivial, information.
A.6 Empirical consistency of Neural Dependency Measure (NDM)
Here we evaluate the Neural Dependency Measure (NDM) over a range of β-VAE (Alemi et al., 2016;
Higgins et al., 2016) models. β-VAE encourages disentangled representations by increasing the role
of the KL-divergence term in the ELBO objective. We hypthesized that NDM would consistenly
measure lower dependence (lower NDM) as the β values increase, and our results in Figure A.6
confirm this. As we increase β, there is a strong downward trend in the NDM, though β = 0.5 and
21
Published as a conference paper at ICLR 2019
β = 1.0 give similar numbers. In addition, the variance over estimates and models is relatively low,
meaning the estimator is empirically consistent in this setting.
A.7 Additional details on occlusion and coordinate prediction experiments
Here we present experimental details on the occlusion and coordinate prediction tasks.
Training without occlusion	Training with occlusion
Figure 14: Visualizing model behaviour when computing global features with and without occlusion,
for NCE-based DIM. The images in each block of images come in pairs. The left image in each
pair shows the model input when computing the global feature vector. The right image shows the
NCE loss suffered by the score between that global feature vector and the local feature vector at each
location in the 8 × 8 local feature map computed from the unoccluded image. This loss is equal to
minus the value in Equation 5. With occluded inputs, this loss tends to be highest for local features
with receptive fields that overlap the occluded region.
Occlusions. For the occlusion experiments, the sampling distribution for patches to occlude was
ad-hoc. Roughly, we randomly occlude the input image under the constraint that at least one 10 × 10
block of pixels remains visible and at least one 10 × 10 block of pixels is fully occluded. We
chose 10 × 10 based on the receptive fields of local features in our encoder, since it guarantees
that occlusion leaves at least one local feature fully observed and at least one local feature fully
unobserved. Figure 14 shows the distribution of occlusions used in our tests.
Absolute coordinate prediction For absolute coordinate prediction, the global features y and local
features c(i,j) are sampled by 1) feeding an image from the data distribution through the feature
encoder, and 2) sampling a random spatial location (i, j) from which to take the local features c(i,j) .
Given y and c(i,j), we treat the coordinates i and j as independent categorical variables and measure
the required log probability using a sum of categorical cross-entropies. In practice, we implement the
prediction function pθ as an MLP with two hidden layers, each with 512 units, ReLU activations,
and batchnorm. We marginalize this objective over all local features associated with a given global
feature when computing gradients.
Relative coordinate prediction For relative coordinate prediction, the global features y and local
features c(i,j)/c(i0,j0) are sampled by 1) feeding an image from the data distribution through the
feature encoder, 2) sampling a random spatial location (i, j) from which to take source local features
c(i,j), and 3) sampling another random location (i0, j0) from which to take target local features c(i0,j0).
In practice, our predictive model for this task uses the same architecture as for the task described
previously. For each global feature y we select one source feature c(i,j) and marginalize over all
possible target features c(i0,j0) when computing gradients.
22
Published as a conference paper at ICLR 2019
SqdEeS Jo %
姒x)	IX)	私)
a) GAN	b) Proxy	C) LSGAN
O(X)	向
d) WGAN e) Our method
Figure 15: Histograms depiCting the disCriminator’s unnormalized output distribution for the standard
GAN, GAN with - log D loss, Least Squares GAN, Wasserstein GAN and our proposed method
when trained with a 50:1 training ratio.

A.8 Training a generator by matching to a prior implicitly
We show here and in our experiments below that we Can use prior objeCtive in DIM (Equation 7)
to train a high-quality generator of images by training Uψ,P to map to a one-dimensional mixture
of two Gaussians impliCitly. One Component of this mixture will be a target for the push-forward
distribution of P through the enCoder while the other will be a target for the push-forward distribution
of the generator, Qθ , through the same enCoder.
Let Gθ : Z → X be a generator funCtion, where the input z ∈ Z is drawn from a simple prior,
p(z) (suCh as a spheriCal Gaussian). Let Qθ be the generated distribution and P be the empiriCal
distribution of the training set. Like in GANs, we will pass the samples of the generator or the training
data through another funCtion, Eψ, in order to get gradients to find the parameters, θ. However, unlike
GANs, we will not play the minimax game between the generator and this funCtion. Rather Eψ will
be trained to generate a mixture of Gaussians Conditioned on whether the input sample Came from P
or Qθ:
VP = N (μp,1), VQ = N (μQ, 1), Uψ,p =4#石矽,Uψ,Q = Qθ #石矽,	(13)
where N(μP, 1) and N(μQ, 1) are normal distributions with unit variances and means μP and μQ
respeCtively. In order to find the parameters ψ , we introduCe two disCriminators, TφP , TφQ : Y → R,
and use the lower bounds following defined by the JSD f-GAN:
(Ψ,Φp,Φq) = arg min arg maxLd(VP, Uψ,P,TPp) + Ld(Vq, Uψ,Q,TQQ).	(14)
ψ	φP,φQ
The generator is trained to move the first-order moment of EUψ,Q [y] = Ep(Z) [Eψ(Gθ(Z))] to μP:
θ = argmin(Ep(z)[Eψ(Gθ(Z))] - μp)2.	(15)
Some intuition might help understand why this might work. As discussed in Arjovsky & Bottou
(2017), if P and Qθ have support on a low-dimensional manifolds on X , unless they are perfectly
aligned, there exists a discriminator that will be able to perfectly distinguish between samples coming
from P and Qθ, which means that Uψ,P and Uψ,Q must also be disjoint.
However, to train the generator, Uψ,P and Uψ,Q need to share support on Y in order to ensure stable
and non-zero gradients for the generator. Our own experiments by overtraining the discriminator
(Figure 15) confirm that lack of overlap between the two modes of the discriminator is symptomatic
of poor training.
Suppose we start with the assumption that the encoder targets, VP and VQ , should overlap. Unless P
and Qθ are perfectly aligned (which according to Arjovsky & Bottou (2017) is almost guaranteed not
to happen with natural images), then the discriminator can always accomplish this task by discarding
information about P or Qθ . This means that, by choosing the overlap, we fix the strength of the
encoder.
23
Published as a conference paper at ICLR 2019
Table 11: Generation scores on the Tiny Imagenet dataset for non-saturating GAN with contractive
penalty (NS-GAN-CP), Wasserstein GAN with gradient penalty (WGAN-GP) and our method. Our
encoder WaS penalized using CP
Model	Inception score	FID
Real data	31.21 ± .68	4.03
IE (ours)	7.41 ± .10	55.15
NS-GAN-CP	8.65 ± .08	40.17
WGAN-GP	8.38 ± 0.18	42.30
A.9 Generation experiments and results
For the generator and encoder, We use a ResNet architecture (He et al., 2016) identical to the one
found in Gulrajani et al. (2017). We used the contractive penalty (found in Mescheder et al. (2018)
but first introduced in contractive autoencoders (Rifai et al., 2011)) on the encoder, gradient clipping
on the discriminators, and no regularization on the generator. Batch norm (Ioffe & Szegedy, 2015)
Was used on the generator, but not on the discriminator. We trained on 64 × 64 dimensional LSUN (Yu
et al., 2015), CelebA (Liu et al., 2015), and Tiny Imagenet dataset.
Figure 16: Samples of generated results used to get scores in Table 11. For every methods, the sample
are generated after 100 epochs and the models are the same. Qualitative results from these three
methods shoW no qualitative difference.
Here, We train a generator mapping to tWo Gaussian implicitly as described in Section A.8. Our
results (Figure 16) shoW highly realistic images qualitatively competitive to other methods (Gulrajani
et al., 2017; Hjelm et al., 2018). In order to quantitatively compare our method to GANs, We trained a
non-saturating GAN With contractive penalty (NS-GAN-CP) and WGAN-GP (Gulrajani et al., 2017)
With identical architectures and training procedures. Our results (Table 11) shoW that, While our
mehtod did not surpass NS-GAN-CP or WGAN-GP in our experiments, they came reasonably close.
24