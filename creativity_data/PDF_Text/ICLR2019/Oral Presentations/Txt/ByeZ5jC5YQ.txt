Published as a conference paper at ICLR 2019
KnockoffGAN: Generating Knockoffs for
Feature Selection using Generative Adver-
sarial Networks
James Jordon
Engineering Science Department
University of Oxford, UK
james.jordon@wolfson.ox.ac.uk
Jinsung Yoon
Department of Electrical and Computer Engineering
UCLA, California, USA
jsyoon0823@g.ucla.edu
Mihaela van der Schaar
University of Cambridge, UK
Department of Electrical and Computer Engineering, UCLA, California, USA
Alan Turing Institute, London, UK
mihaela@ee.ucla.edu
Ab stract
Feature selection is a pervasive problem. The discovery of relevant features can be
as important for performing a particular task (such as to avoid overfitting in pre-
diction) as it can be for understanding the underlying processes governing the true
label (such as discovering relevant genetic factors for a disease). Machine learning
driven feature selection can enable discovery from large, high-dimensional, non-
linear observational datasets by creating a subset of features for experts to focus
on. In order to use expert time most efficiently, we need a principled method-
ology capable of controlling the False Discovery Rate. In this work, we build
on the promising Knockoff framework by developing a flexible knockoff genera-
tion model. We adapt the Generative Adversarial Networks framework to allow
us to generate knockoffs with no assumptions on the feature distribution. Our
model consists of 4 networks, a generator, a discriminator, a stability network and
a power network. We demonstrate the capability of our model to perform fea-
ture selection, showing that it performs as well as the originally proposed knock-
off generation model in the Gaussian setting and that it outperforms the original
model in non-Gaussian settings, including on a real-world dataset.
1	Introduction
Feature selection is a pervasive problem. Often the goal is to discover features that are relevant to
a particular outcome, either for the sake of discovery itself or to aid in prediction [16; 25]. When
the focus is on discovery, feature selection methods typically focus on trying to control either the
Family-Wise Error Rate (FWER) or the False Discovery Rate (FDR). The FWER measures the
probability of making a single false discovery (a Type I error) among the selected features (i.e.
selecting one which is not relevant), whereas the FDR measures the proportion of false discoveries
made (i.e. the proportion of selected features which are false). Controlling FWER, however, leads
to reduced power (i.e. selecting fewer relevant variables) since it controls the probability of making
any false discovery, whereas FDR tries to control the proportion of false discoveries.
Controlling the FDR is important [5; 6; 3]. Often, data-driven feature selection will be used to select
a set of candidate features for further investigation. When further investigation is expensive (for
example when further investigation would involve conducting new experiments and collecting more
data), a method that cannot control the FDR may result in a large amount of wasted resources, with
no guarantee that anything meaningful will be discovered. On the other hand, being able to control
the FDR at, say, 10% ensures that at most, 10% of the spent resources are wasted, and 90% are
in fact spent on discovering positive, useful results. It should be noted, however, that estimating
1
Published as a conference paper at ICLR 2019
the FDR of a method empirically is hard in practice, since we do not have access to the ground
truth relevance. As such, a theoretical analysis of the method and its (potential) FDR-controlling
properties must be carried out, which does not exist for many existing feature selection methods.
[3]	is the seminal paper on the knockoff framework, which is an innovative FDR-controlling feature
selection method. Knockoffs are features that are generated to “look like” the real features but
be conditionally independent of the label given the real features. Feature statistics (such as the
coefficients of a LASSO [32]) are compared between the real features and their knockoffs and a
selection is made when this difference is sufficiently large. Performing the selection in this way
allows for an estimate of the FDR to be obtained and the selection threshold can be adjusted to
control the FDR at the selected level. In the original paper, the relationship between the label
and the features is constrained to be of a very specific form; in [7], they remove this constraint
and instead provide a theoretical analysis that shifts the burden of knowledge onto knowing the
underlying feature distribution. Unfortunately, while the theoretical results hold for any feature
distribution, they rely on being able to generate valid knockoffs, for which [7] only provide a method
for generating knockoffs when the distribution is a (known) multivariate Gaussian distribution. In
this paper, we modify the Generative Adversarial Networks (GAN) [11] framework to address this
problem, allowing us to generate knockoffs for any distribution (and without any prior knowledge of
it). GANs have been shown to be a powerful method for learning to generate complex distributions
[24; 20; 2].
Our main contribution is in modifying the discriminator used in the GAN framework in such a way
that the generator learns to generate knockoffs satisfying the necessary swap condition [7] which
requires that when a feature and its knockoff are swapped, the joint distribution remains unchanged.
In addition, we propose a method for maximizing the power of our model using Mutual Information
Neural Estimation (MINE) [4] and investigate a regularization method to improve the stability of
training. Our model consists of four networks: (1) a generator network that takes as input noise and
the real features, and outputs a set of candidate knockoff features; (2) a discriminator network taking
as input “swapped” feature-knockoff features that attempts to determine which variables have been
swapped; (3) a Wasserstein GAN discriminator that we use as a regularization term; and (4) a MINE
network that estimates the mutual information between each feature-knockoff pair allowing us to
maximize the power of the knockoff procedure.
2	Related Works
Feature selection is a well-studied problem with a wealth of related works ([12; 31; 17; 22] provide
a summary of a lot of existing literature); however, most methods do not attempt to control the FDR.
The most common feature selection method for FDR control is the Benjamini-Hochberg (BHq)
procedure and its variants [5; 6], which relies on obtaining valid marginal p-values for each selection.
Knockoffs are an active area of research [9; 18; 10]. The notion of a knockoff was first introduced
in [3] with the theory there requiring that the relationship between the features and the label be
of a specific form. In [7], they build on the knockoff framework, removing this requirement but
instead shifting the requirement to one of knowing the distribution of the features. As noted in the
introduction, the theory in [7] holds independent of the distribution of the features - relying only on
being able to generate valid knockoffs (which exist for any distribution of features). However, they
only propose a method for generating knockoffs when the distribution of features is jointly Gaussian.
While they do propose a method for generating approximate knockoffs in the non-Gaussian setting
(by simply approximating the features as Gaussian), the guarantees on FDR control do not hold
for their approximate knockoffs. In [26] and [10], they add to the class of constructible knockoffs,
describing methods for constructing knockoffs for Markov Chains, Hidden Markov Models and
Gaussian Mixture Models. Though once again, knowledge of the full distribution is still necessary
for their construction.
In this paper we use a framework motivated by GANs [11] to learn to generate knockoffs without
any assumptions on the distribution of the features. To do this, we modify the discriminator so
that rather than trying to determine whether a sample is real or fake, it attempts to identify which
components have been “swapped”. In [38], an unconventional discriminator is used that performs
component-wise discrimination for the purpose of imputation. While the problem addressed in that
paper is different to the one here, the key idea relies on a similar modification to the discriminator
to be able to appropriately guide the generator.
2
Published as a conference paper at ICLR 2019
In order to maximize the power of our variable selection mechanism, it will be desirable that the
feature-knockoff pairs are ”as independent as possible” (this is discussed in [7]). In order to achieve
this we will investigate the use of a promising recent paper, MINE [4]. MINE proposes a neural
architecture and training procedure capable of estimating the mutual information between two ran-
dom variables. As the mutual information between two random variables is zero only when they are
independent, we will use this as a measure of independence and attempt to minimize it during the
training of our modified GAN.
3	Background
In this section we introduce our notation and define knockoffs as in [7]. Let us denote the feature
space by X and the label space by Y. Let the dimension of X be d. Suppose that X = (X1, ..., Xd)
and Y are random variables over X and Y . As in [7], we will work with the notion of a null set.
Definition 1. A variable Xj is said to be ”null” if and only if Y is independent of Xj conditional
on {Xi : i 6= j}. We define H0 to be the set of all null variables.
Our goal will be to discover as many relevant features as possible while controlling the FDR. For a
given (potentially random) selection procedure that selects S ⊂ {1, ..., d}, we define the FDR to be
口 |S ∩Hol
FDR = E	.
⑸
Note that this agrees with the usual notion of FDR (i.e. when defined in terms of the Markov blanket)
under mild assumptions (for a more thorough discussion see [7]).
3.1	Knockoffs
Definition 2. A knockoff [7] for X is a random variable X ∈ X satisfying the following two
properties:
(X, X) = (X, X)swap(S)	⑴
~ .
X ⊥⊥ Y|X	(2)
for all S ⊂ {1,...,d} where (∙, ∙)swap(s)denotes the vector obtained by swapping the ith component
with the (i + d)th component for each i ∈ S and =d. is equality in distribution.
In order to use knockoffs for feature selection, we must define an appropriate feature statistic, Wj ,
that depends on X, X and Y, i.e. Wj = wj ((X, X), Y) for some function wj. This function wj
must satisfy the following flip-sign property:
wj ((X, X)swap(S) , Y)
[wj ((X, X),Y) if j/S
[-Wj((X,X),Y) if j / S.
(3)
One of the procedures used in [7] to construct these statistics is to perform LASSO, treating the
augmented feature-knockoffs as the features on which to regress. This gives LASSO coefficients
b1, ..., b2d, and the statistic Wj is set to be the LASSO Coefficient Difference given by
Wj = |bj| - |bj+d|.
Note that the FDR control guarantees hold independently of the choice of statistic, but a poorly
chosen statistic can significantly impact the power of the test. In particular, using the LASSO Coef-
ficient Difference in non-linear settings can yield few discoveries. The focus of this paper, however,
is on generating the knockoffs, not on the statistic used on top of the generated knockoffs and so in
our synthetic experiments, we use a linear model for Y to be able to draw fair comparisons between
our model and [7]. In the real world data experiment, we use a statistic based on Random Forests
for both methods [37].
The following result from [7] depends only on having obtained knockoffs that satisfy Definition 2
and feature statistics satisfying (3) (and in particular do not depend specifically on using LASSO to
obtain the statistics).
3
Published as a conference paper at ICLR 2019
Theorem 1. Let q ∈ [0, 1]. Given test statistics, W1, ..., Wd, satisfying (3), let
τ = min t > 0 :
1 + |{j : Wj≤-t}∣ ≤ 1
∣(j: Wj ≥ t?∣	≤ qj.
Then the procedure selecting the variables
S = {j : Wj ≥ T}
controls the FDR at level q, i.e.
.^ ..
∣S^ ∩Ho∣
.ʌ .
∣s∣∨ 1
≤ q.
E
4 KnockoffGAN
It should be noted that in order to satisfy equation (2), it simply needs to be the case that the knock-
offs are constructed without looking at the label, Y . In order to satisfy equation (1) we use a modified
GAN framework, which gives us the flexibility to learn to generate knockoffs without any assump-
tions on the distribution of the original features.
Figure 1: KnockoffGAN Block Diagram
4.1	Generator
The generator, G, will be a function G(∙, ∙; φ) : X X [0,1]c → X, parametrized by φ that takes a
realization X of X and random noise, Z 〜U([0,1]c), as inputs and outputs knockoff features x. We
define X := G(X, z). We model G as a fully connected neural network with weights φ.
4.2	Discriminator
The main innovation of our paper is in defining the discriminator. Equation (1) imposes a condition
on the joint distribution of (X, X) and as such we must define a discriminator with a loss that is
(not necessarily uniquely) minimized only for joint distributions satisfying this condition. To that
end, the discriminator, D, will be a function D(∙; ψ) : X × X → [0,1]d that takes as input a
swapped sample-knockoff pair (x, X)SWaP(S) and outputs a vector in [0,1]d with the ith component
of D((x, X)SwaP(S)) corresponding to the probability that i ∈ S. The discriminator is attempting
to detect which variables have been swapped and, intuitively, when the discriminator is unable to
determine this, the swapped and unswapped joint distributions must be the same.
The loss we use to train the discriminator is the multi-output cross-entropy loss given by
X
EX〜Px[Eχ〜PX(X)[S∙log(D((X, X)SwaP(S)))+(1-S)∙log(1-D((X, X)SwaP(S)))]]
S∈{0,1}d
(4)
4
Published as a conference paper at ICLR 2019
where ∙ is the standard dot, 1 = (1,…，1), S = (Si,…，Sd) with Si = I(i ∈ S) (I is the indicator
function) and log is taken element-wise. The following theorem is our main theoretical result, which
states that the training regime employed by KnockoffGAN will result in a procedure that generates
valid knockoffs.
Theorem 2. Equation (4) is maximized (with respect to G) if and only if equation (1) is satisfied by
G.
Proof. The proof, alongside supporting theoretical results, can be found in the Appendix. □
In practice, the sum is too computationally expensive (O(2d)) to calculate and so we perform
stochastic gradient descent using minibatches with S sampled uniformly from {0, 1}d, indepen-
dently for each sample in the minibatch.
We also found that training with respect to the full loss resulted in a poor performance, particularly
when d is large. We found that the discriminator struggled to learn anything when asked to find
the full swap vector, and the poor discriminator resulted in a poorly trained generator. In order to
overcome this, we introduce a hint vector - first introduced in [38] - that we use to reveal partial
information to the discriminator about the swap vector. We do this by using the hint to reveal some,
but not all, of the components of S to the discriminator. In doing so, we reduce the burden of the
discriminator from needing to determine the entire swap vector to only needing to determine some
of the swap vector.
Formally, the hint, H, will be a random variable depending on S, that we pass to the discriminator,
alongside (X, X)swap(S). We use the hint to control the amount of information we pass to D about
S before asking D to predict S. In practice, our hinting mechanism involves sampling a multivariate
Bernoulli random variable, B from i.i.d. components, which each take value 1 with probability
0.9. The hint is then constructed by setting Hi = Si if Bi = 1 and Hi = 0.5 if Bi = 0. The
discriminator is therefore being asked only to predict the values of S for which Bi = 0; the others,
D is able to directly infer from Hi. In order to avoid overfitting to the hint, it becomes necessary to
remove these terms from our loss. Our loss now becomes
LD =	X	EX〜Pχ[Eχ〜PX(X) [Eh〜Phis	[(S	©	(1	- B))	∙log(D((X, X)SWaP(S), H))	⑸
S∈{0,1}d
+ ((1 - S) © (1 - B)) ∙ log(1 - D((X, X)SWaP(S), H))]]]
where © denotes element-wise multiplication and the expectation over B is implicit in the expecta-
tion over H.
4.3	Stability
We found that adding a regularization term in the form of a Wasserstein GAN discriminator (with
Gaussian Process (GP) regularization) [2], f, aided performance. We note that when equation (1)
holds, we must have that X = X and so the addition of this regularizing term does not affect the
optimal solution to our loss. We model f as a fully connect neural network with weights ν. The loss
is given by
Lf = E [f(X) - f(X) - η(l∣Vχf(X)∣∣2 - i)2i
where E 〜U[0,1], X = CX +(1 — E)X and η is a hyper-parameter (set to 10 in practice). Note
that we have rewritten the loss to be the negative of the one given in [2], allowing us to write our
overall objective as a minimax problem. This loss is added to the generator loss as an additional
regularization term.
4.4	Maximizing Power
As noted in [7], itis intuitive that in order to maximize the power of the knockoff selection procedure,
we wish to make Xj and Xj as ”independent” as possible. Doing so ensures that as little as possible
of the dependence between the real feature and the label is present between the knockoff and the
label; this allows us to determine whether or not the relationship between the feature and label is
only through the feature’s correlation with other features, or is in fact a true signal.
5
Published as a conference paper at ICLR 2019
In order to achieve maximal independence, we look to minimize the mutual information between
each feature and its knockoff. Actually computing the true mutual information requires access to
both the joint density of the feature-knockoff pairs and to the marginal densities of each feature and
knockoff, which we do not have.
Instead, we look to a promising recent work, Mutual Information Neural Estimation (MINE [4]),
that provides a framework for estimating the mutual information using neural networks. To do so,
they estimate the mutual information between random variables U and V by performing gradient
ascent on the following objective:
Sup EpUnV [Tθ] - log(EPUn) 0pVn) [eTθ D
where PUV denotes the joint measure of (U, V ) with PU = V dPUV and PV = U dPUV denoting
the marginal measures. (n) denotes the empircal distribution associated with n i.i.d. samples.
Using MINE we approximate the mutual information between each pair Xj and Xj by using d neural
networks1, T1 , ..., Td, each parametrized by θ1 , ..., θd, that we refer to collectively as the power
network, and will write P to denote the collection of networks T1 , ..., Td. The mutual information
is added using a trade-off parameter λ to the loss for G. Formally, define LP by
dn	n
Lp = XI X(Tθj (Xi)Mi)))- IOg(X eχp(Tθj (X(K(i)),χji)/
j=1 i=1	i=1
where κ is a random permutation of [n]2 and (i) denotes the ith sample - noting that dependence on
G is through X.
4.5	Final Objective
The resulting minimax game played by G, D, W and P is given by
min ( max(LD) + λ max(Lp) + μ max(Lf)
Where λ, μ are hyper-parameters (Set to 1 in the experiments section).
We train each of G, D, W and P iteratively. Pseudo-code of our knockoff construction algorithm
can be found in Algorithm 1 and a visual representation of our architecture in Fig. 1.
After generating knockoffs, feature statistics are computed according to some procedure (in the
synthetic experiments We use LASSO and in the real data experiment We use a Random Forest-
based statistic [37]). Features are then selected based on these statistics according to Theorem 1.
5	Experiments
In this section We demonstrate the capability of our method to match the results of [7] in settings
Where their model is correctly specified (i.e. When the underlying feature distribution is Gaussian)
and then go on to shoW, in settings Where the underlying feature distribution is non-Gaussian, that
our method is able to outperform their Gaussian approximation. We compare to tWo versions of the
BHq method [5; 6] to provide a baseline.
We also perform a qualitative analysis of KnockoffGAN on a real-World dataset. We compare fea-
tures found by KnockoffGAN to PubMed literature and shoW that KnockoffGAN discovers several
meaningful features for 2 different disease outcomes.
1In practice We use a single neural netWork With diagonalized Weights to parallelize these netWorks.
2In our pseudo-code, We Write U (Sn) to denote the uniform distribution over the set of all permutations of
[n] = {1,..., n}.
6
Published as a conference paper at ICLR 2019
Algorithm 1 Pseudo-code of KnockoffGAN
1:	Inputs: mini-batch size nmb > 0, Initialize parameters φ, ψ, ν, θ1 , ..., θd
2:	while Converge do
3:	Discriminator Update
4:	Sample xι,…,Xnmb from D, zι,…,Znmb ~ PZ
5:	Sample Si,…,Snmbikd U({0,1}d), bi,…,"皿.〜Ber(0⑼
6:	for i = 1, ..., nmb do
7：	Xi — G(xi, Zi； φ)
8:	hi = Si bi + 0.5(1 - bi)
9:	Update D by ascending its stochastic gradient
nmb
Vψ X [(Si Θ (1 - bi)) ∙ log(D((xi, Xi)SwaP(S)), hi)
i=i
+ ((I - Si) θ (I - bi)) ∙ lOg(I - D((Xi, Xi)swap(S), hi))i
10:	MINE Update
11:	SampleXi, ..., Xnmb from D, zi, ..., znmb k PZ, κk U (Snmb)
12:	for i = 1, ..., nmb do
13：	Xi — G(Xi, Zi； φ)
14:	for j = 1, ..., d do
15:	Update Tj by ascending its stochastic gradient
Vθj (PnmIbTθj (Xji), xji))) - log (PnmIb exp(Tθj(Xji), xjK(i)))))
16:	WGAN-GP Update
17:	SampleXi, ..., Xnmb from D, Zi, ..., Znmb k PZ
18:	for i = 1, ..., nmb do
19:	Sample k U[0, 1]
20：	Xi — G(Xi, Zi； φ)
21：	Xi = Exi + (1 — E)Xi
22:	Update f by ascending its stochastic gradient
VV PnmIbhf(Xi) - f(Xi) — η(l∣vxi f (Xi)∣∣2 - i)2i
23：	Generator Update
24：	SampleXi, ..., Xnmb from D, Zi, ..., Znmb k PZ
25： Sample Si,..., Snmb i.ki.dU({0,1}d),κkU(Snmb)
26： for i = 1, ..., nmb do
27：	Xi — G(Xi, Zi； φ)
28： Update G by descending its stochastic gradient
Vφ (LD + λLp + μLf)
5.1	Synthetic Data Experiments
5.1.1	S imulation settings
Evaluating feature selection methods on real data is difficult as we do not have access to the ground
truth. To evaluate KnockoffGAN, we conduct a series of experiments using synthetic data, replicat-
ing those carried out in [7] and extending them to more general settings. In each of the following
synthetic experiments, we set the feature dimension to be d = 1000 and the number of samples to
be n = 3000. For each feature distribution we perform two experiments：
1. Y-Logit： P(Y = 1|X)
exp(m(X))
(1+exp(m(X)))
2. Y-Gaussian： Y k N (m(X), 1)
7
Published as a conference paper at ICLR 2019
where m(X) = Pi6=0 1 αδiXi with δi ∈ {-1, 1} sampled uniformly and then fixed for each exper-
iment. α controls the strength of the influence that X has on Y , and in the experiments we vary
this (as in [7]). Note that for the auto-regressive settings (found in Section 5.1.2 and the Appendix)
the relevant variables are sampled uniformly at random from among the 1000 features (rather than
being the first 60); in the non-auto-regressive settings this is not necessary.
We report the True Positive Rate (TPR), which is also commonly referred to as the power, defined
as
TPR
∣S ∩s *∣
∣s *1
(6)
where S * = {1, ..., d} \ H0 is the set of all non-null features. We also report the FDR to verify that
the methods do indeed control it at the specified level which we set to be 10%. Note that we are not
using FDR as a metric - a lower FDR is not desirable when we set the threshold to 10%. In fact, we
want the methods to be as close to 10% as possible (so that they are achieving maximum power).
We perform 100 replications of each experiment and report the average TPR and FDR.
5.1.2 Gaussian settings
We begin by replicating the setup from [7] in which the underlying feature distribution is Gaussian.
In this setting, we do not expect KnockoffGAN to perform better than the original knockoff frame-
work as the original framework assumes a Gaussian distribution. Our goal here is simply to achieve
a similar performance, demonstrating that little performance is lost even when the distribution is
known to be Gaussian.
In the first experiment that we replicate from [7], the features are set to be auto-regressive (AR(1))
i.i.d.
With GaUssian marginal distributions, i.e. Xi = φXi-ι + Zi With Zi being chosen such that Xi 〜
N(0, n). In this experiment We vary φ, which determines the correlation between features, rather
than α. We fix α = 3.5 for Y-Gaussian and α
10 for Y-Logit. The results are reported in Fig. 2.
Figure 2: Comparison of KnockoffGAN With the benchmarks for X distributed as an auto-regressive distribu-
tion With Gaussian marginal distributions. TPR is used to quantify performance and FDR is reported to verify
that it is at the specified threshold (10%).
Y-GaUss
As in [7], We observe that BHq Marginal, Which tests for marginal independence of the feature from
Y , suffers from severely increased FDR as We increase the correlation, invalidating the seemingly
good TPR. To make the remaining results clearer, We omit BHq marginal from the rest of this
section. Aside from this, We see in Fig. 2, that the other methods control the FDR at or very close to
the specified 10% threshold. We also see that across the entire range of α, KnockoffGAN achieves
a very similar TPR to the original Knockoff frameWork.
In the second experiment, We set the underlying feature distribution to be i.i.d. Gaussian. We found
in this case also that KnockoffGAN Was able to control the FDR and achieve a similar TPR to the
8
Published as a conference paper at ICLR 2019
original knockoff framework. More details of this experiment and the results for it can be found in
the Appendix
5.1.3	Non-Gaussian settings
We now move on to the key results for the paper in which the underlying feature distribution is no
longer Gaussian. In this setting, we expect to outperform the original Knockoff framework due to
the fact that they approximate the distribution as Gaussian. In particular, when this approximation
is poor, the knockoffs are no longer valid and as such no FDR guarantees can be given. On the other
hand, KnockoffGAN does not place any requirements on the distribution of the features and as such
is able to generate valid knockoffs.
We performed experiments for several different underlying feature distributions, and found that
KnockoffGAN achieved a higher TPR than the original knockoff framework in all cases, while
controlling the FDR at the specified level. We give the results for X coming from a 4-Gaussian
mixture model in Fig. 3 - results for Uniform, Dirichlet, and other (2 and 3) Gaussian mixture
models can be found in the Appendix.
To create our 4-mixture model, we set the means (m1, m2, m3, m4) of the 4 Gaussians to be:
•	mi1 =	1 for i = 1	to 100 and 0 for i = 101 to 1000,
•	mi2 =	1 for i = 1	to 50 and -1 for i = 51 to 100 and 0 for i	=	101 to	1000,
•	mi3 =	-1 for i =	1 to 50 and 1 for i = 51 to 100 and 0 for	i	=	101 to	1000,
•	mi4 =	-1 fori =	1 to 100 and0fori = 101 to 1000.
We scale the variance of each GaUssian to be such that the overall variance of each feature is 1.
Y-Logit	Y-GaUSS
Figure 3: Comparison of KnockoffGAN with the benchmarks forX distributed as a 4-mixture Gaussian mixture
model. TPR is used to quantify performance and FDR is reported to verify that it is at the specified threshold
(10%).
We see in Fig. 3 that KnockoffGAN consistently outperforms the original knockoff framework,
achieving a higher TPR across the entire range of α while consistently controlling the FDR at 10%.
In fact, in the Y -Gaussian setting we see that the original knockoff framework performs almost
identically to BHq Maximum Likelihood.
5.1.4	Impact of WGAN regularization
We conclude the synthetic experiments by demonstrating the effect of the WGAN regularize13. We
conduct this experiment using an auto-regressive model with U(- Vz3∕η, Vz3∕n) marginal distribu-
tions. We fix α = 5 for Y-Logit and α = 2.5 for Y-Gauss.
3The WGAN regularizer was included in all previous experiments.
9
Published as a conference paper at ICLR 2019
Y-GaUSS
Figure 4: A comparison of the performance of KnoCkoffGAN With and without the WGAN regularize] for X
distributed as an auto-regressive distribution with U(一，3/n, ʌ/ɛ/n) marginal distributions. TPR is used to
quantify performance and FDR is reported to verify that it is at the specified threshold (10%).
As we see in Fig. 4, the WGAN regularizer has a significant effect on the results, with the im-
provement in some places being almost as much as KnockoffGAN without WGAN makes over the
original knockoff framework. As noted in Section 4.3, there is no trade-off introduced by the inclu-
sion of this regularizer; the optimal solution to the loss is unchanged and therefore this regularization
is ”free” in terms of FDR control, but as demonstrated improves TPR performance.
5.2 Real Data Experiment
In this section we use a biobank dataset (with 387 dimensions) to qualitatively analyze the perfor-
mance of KnockoffGAN. We use KnockoffGAN to select features for two different outcomes: (1)
Cardiovascular Disease (CVD) and (2) Diabetes and then use PubMed literature to asses the validity
of the selected features.
We found that the original knockoff framework was unable to select even the most well-known fea-
tures (such as Age and Sex for CVD [15]), even when the FDR threshold was increased to 20%.
Therefore, there are no relevant features to report for the original knockoff framework and so Table
1 contains only the features selected by KnockoffGAN that were deemed relevant by PubMed liter-
ature. For this the FDR threshold was set to 5% so that the number of discoveries was manageable
for cross-reference with PubMed.
No I Cardiovascular Disease ∣ PubMed ref. ∣∣ Diabetes ∣ PubMed ref.
1	Age	[15]	Lipid-lowering drugs	[35]
2	Sex	[15]	Comparative body size	[27]
3	Daily smoking	[1]	Home owned	[13]
4	FEV1	[28]	Insomnia	[34]
5	Diastolic blood pressure	[33]	Anti-hypertensive drugs	[8]
6	Diabetes	[29]	Asthma	[30]
7	Father chronic bronchitis	[14; 23]	Height	[19; 27]
8	Alcohol intake	[21]	Alcohol intake	[36]
9	Long-standing illness			
Table 1: Discovered features using KnockoffGAN framework, verified using PubMed literature. The FDR
threshold was set to 5%.
As we see in Table 1, KnockoffGAN discovers 9 relevant features for CVD and 8 relevant features
for diabetes. Some of the relevant features, such as Age, Sex and Long-standing illness for CVD are
10
Published as a conference paper at ICLR 2019
well-known relevant features. The remaining features are all supported by the literature in PubMed.
While this is a qualitative result (it relies on using PubMed as the ground truth), we do believe this
demonstrates that KnockoffGAN is a significant improvement over the original knockoff generation
procedure.
6 Conclusion
In this paper we built on the knockoff framework introduced in [3] by developing a novel GAN
framework, KnockoffGAN, capable of generating knockoffs with no assumptions on the underlying
data. We demonstrated through a series of experiments on a range of synthetic datasets and on a real
world dataset that our method improves on the performance of the original knockoff framework.
While we feel this is a significant step towards being able to generate knockoffs for any data, there
is still more work to be done. In particular, generalizing this method to time-series data would be
non-trivial, and would be an interesting avenue for further investigation.
Acknowledgement
The authors would like to thank the reviewers for their helpful comments. The research presented
in this paper was supported by the Office of Naval Research (ONR) and the NSF (Grant number:
ECCS1462245, ECCS1533983, and ECCS1407712).
References
[1]	John A Ambrose and Rajat S Barua. The pathophysiology of cigarette smoking and cardiovas-
CUlar disease: an update. Journal of the American college of cardiology, 43(10):1731-1737,
2004.
[2]	Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
[3]	Rina Foygel Barber and Emmanuel J Candes. A knockoff filter for high-dimensional selective
inference. arXiv preprint arXiv:1602.03574, 2016.
[4]	Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio,
Devon Hjelm, and Aaron Courville. Mutual information neural estimation. In Proceedings of
the 35th International Conference on Machine Learning, pp. 530-539, 2018.
[5]	Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and pow-
erful approach to multiple testing. Journal of the royal statistical society. Series B (Method-
ological), pp. 289-300, 1995.
[6]	Yoav Benjamini and Daniel Yekutieli. The control of the false discovery rate in multiple testing
under dependency. Annals of statistics, pp. 1165-1188, 2001.
[7]	Emmanuel Candes, Yingying Fan, Lucas Janson, and Jinchi Lv. Panning for gold:
Model-free knockoffs for high-dimensional controlled variable selection. arXiv preprint
arXiv:1610.02351, 2016.
[8]	Antonio Ceriello, Dario Giugliano, Antonio Quatraro, and Pierre J Lefebvre. Anti-oxidants
show an anti-hypertensive effect in diabetic and hypertensive subjects. Clinical Science, 81(6):
739-742, 1991.
[9]	Yingying Fan, Emre Demirkaya, Gaorong Li, and Jinchi Lv. Rank: large-scale inference with
graphical nonlinear knockoffs. arXiv preprint arXiv:1709.00092, 2017.
[10]	Jaime Roquero Gimenez, Amirata Ghorbani, and James Zou. Knockoffs for the mass: new
feature importance statistics with false discovery guarantees. arXiv preprint arXiv:1807.06214,
2018.
11
Published as a conference paper at ICLR 2019
[11]	Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in
neural information processing Systems, pp. 2672-2680, 2014.
[12]	Isabelle Guyon and Andre Elisseeff. An introduction to variable and feature selection. Journal
of machine learning research, 3(Mar):1157-1182, 2003.
[13]	C JafioL F Thomas, K Bean, B Jego, and N Danchin. Impact of socioeconomic status on dia-
betes and cardiovascular risk factors: results of a large french survey. Diabetes & metabolism,
39(1):56-62, 2013.
[14]	Pekka Jousilahti, Erkki Vartiainen, Jaakko Tuomilehto, and Pekka Puska. Symptoms of chronic
bronchitis and the risk of coronary disease. The Lancet, 348(9027):567-572, 1996.
[15]	Pekka Jousilahti, Erkki Vartiainen, Jaakko Tuomilehto, and Pekka Puska. Sex, age, cardio-
vascular risk factors, and coronary heart disease: a prospective follow-up study of 14 786
middle-aged men and women in finland. Circulation, 99(9):1165-1172, 1999.
[16]	Alan Jovic, Karla Brkic, and Nikola Bogunovic. A review of feature selection methods with
applications. In Information and Communication Technology, Electronics and Microelectron-
ics (MIPRO), 2015 38th International Convention on, pp. 1200-1205. IEEE, 2015.
[17]	Alan Jovic, Karla Brkic, and Nikola Bogunovic. A review of feature selection methods with
applications. In Information and Communication Technology, Electronics and Microelectron-
ics (MIPRO), 2015 38th International Convention on, pp. 1200-1205. IEEE, 2015.
[18]	Eugene Katsevich and Chiara Sabatti. Multilayer knockoff filter: Controlled variable selection
at multiple resolutions. arXiv preprint arXiv:1706.09375, 2017.
[19]	D Lawlor, S Ebrahim, and G Davey Smith. The association between components of adult
height and type ii diabetes and insulin resistance: British women’s heart and health study.
Diabetologia, 45(8):1097-1106, 2002.
[20]	Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic
single image super-resolution using a generative adversarial network. In CVPR, volume 2, pp.
4, 2017.
[21]	Michael Marmot and Eric Brunner. Alcohol and cardiovascular disease: the status of the u
shaped curve. BMJ: British Medical Journal, 303(6802):565, 1991.
[22]	Tahir Mehmood, Kristian Hovde Liland, Lars Snipen, and Solve sæbθ. A review of variable
selection methods in partial least squares regression. Chemometrics and Intelligent Laboratory
SyStemS, 118:62-69, 2012.
[23]	Howraman Meteran, Vibeke Backer, Kirsten Ohm Kyvik, Axel Skytthe, and Simon Francis
Thomsen. Heredity of chronic bronchitis: a registry-based twin study. Respiratory medicine,
108(9):1321-1326, 2014.
[24]	Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural
samplers using variational divergence minimization. In Advances in Neural Information Pro-
cessing Systems, pp. 271-279, 2016.
[25]	Carl Rietschel, Jinsung Yoon, and Mihaela van der Schaar. Feature selection for survival
analysis with competing risks using deep learning. arXiv preprint arXiv:1811.09317, 2018.
[26]	Matteo Sesia, Chiara Sabatti, and Emmanuel J Candes. Gene hunting with knockoffs for
hidden markov models. arXiv preprint arXiv:1706.04677, 2017.
[27]	Suzanne M Shoff and Polly A Newcomb. Diabetes, body size, and risk of endometrial cancer.
Americanjournalofepidemiology, 148(3):234—240, 1998.
[28]	Don D Sin and SF Paul Man. Chronic obstructive pulmonary disease: a novel risk factor for
cardiovascular disease. Canadian journal ofphysiology and pharmacology, 83(1):8-13, 2005.
12
Published as a conference paper at ICLR 2019
[29]	James R Sowers and Melvin A Lester. Diabetes and cardiovascular disease. Diabetes care, 22:
C14, 1999.
[30]	Lars C Stene and Per Nafstad. Relation between occurrence of type 1 diabetes and asthma.
The Lancet, 357(9256):607-608, 2001.
[31]	Jiliang Tang, Salem Alelyani, and Huan Liu. Feature selection for classification: A review.
Data classification: Algorithms and applications, pp. 37, 2014.
[32]	Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society. Series B (Methodological), pp. 267-288, 1996.
[33]	Ramachandran S Vasan, Martin G Larson, Eric P Leip, Jane C Evans, Christopher J O’donnell,
William B Kannel, and Daniel Levy. Impact of high-normal blood pressure on the risk of
cardiovascular disease. New England journal of medicine, 345(18):1291-1297, 2001.
[34]	Alexandros N Vgontzas, Duanping Liao, Slobodanka Pejovic, Susan Calhoun, Maria
Karataraki, and Edward O Bixler. Insomnia with objective short sleep duration is associated
with type 2 diabetes: a population-based study. Diabetes care, 2009.
[35]	Sandeep Vijan and Rodney A Hayward. Pharmacologic lipid-lowering therapy in type 2 dia-
betes mellitus: background paper for the american college of physicians. Annals of Internal
Medicine, 140(8):650-658, 2004.
[36]	SG Wannamethee, AG Shaper, IJ Perry, and KGMM Alberti. Alcohol consumption and the
incidence of type ii diabetes. Journal of Epidemiology & Community Health, 56(7):542-548,
2002.
[37]	Marvin N Wright, Theresa Dankowski, and Andreas Ziegler. Unbiased split variable selection
for random survival forests using maximally selected rank statistics. Statistics in medicine, 36
(8):1272-1284, 2017.
[38]	Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GAIN: Missing data imputation
using generative adversarial nets. In Proceedings of the 35th International Conference on
Machine Learning, pp. 5689-5698, 2018.
13
Published as a conference paper at ICLR 2019
Appendix
Theoretical Results
In order to prove Theorem 2, we use similar techniques to those used in the original GAN paper
[11]. In what follows, we analyze the minimax game defined by:
X
EX 〜Pχ[Eχ 〜PX(X) [S∙log(D((X, X)Swap(S)))+(1-S)∙log(1-D((X, X)Swap(S)))]]
S∈{0,1}d
(7)
where we have uSed the verSion of LD given by equation (4) in the main manuScript (i.e. without
hinting). The theoretical reSultS that follow are proven only for thiS verSion of the loSS, though we
do believe that the theorem holdS more generally for the hinting verSion of the loSS - thiS iS backed
up by our empirical reSultS demonStrating Strict FDR control while uSing the hint mechaniSm. After
proving that the optimal Solution to thiS game doeS indeed provide uS with valid knockoffS, we will
then Show that the additional term Lf doeS not change the optimal Solution. Let p be the denSity of
(X, X).
We begin by Stating a lemma, that followS from a Similar proof to PropoSition 1 in [11].
Lemma 1. Let (x, X) ∈ X × X. Thenfor a fixed generator G, the ith component of the optimal
discriminator D*((x, X)) is given by
D*((x X)), = ______P((X, X))"αpHiD)_____ (8)
((, ))i p((x, X)swap({i}))+ p((x, X))	M
for each i ∈ {1, ..., d}.
Proof. The proof of thiS involveS Some baSic integral manipulation to get that the objective in (7)
can be rewritten aS
d
i=1	X×X
log D((x, X))ip((x, X)Swap({i})) +log(1 - D((x, X))i)p((x, X))dx.
We then observe that y → a log y + b log(1 - y) achieves its maximum in [0,1] at a^ and so the
objective iS maximized (with reSpect to D, for fixed G) when
D*((x x)) =	P((X, X)Swap({i}))
(( , ))i = P((x, X)Swap({i}))+ P((x, X))
for each i ∈ {1,…，d}.	□
With this lemma, we are now able to prove our key result.
Theorem 3. Equation (7) is maximized (with respect to G) if and only if equation (1) (in the main
paper) is satisfied by G.
Proof. We begin by rewriting our loss, substituting in D*, to give US the following loss for G:
LG
Σ
S⊂{1,...,d}
ex,X (X log
i∈S
P((X, X)Swap(S∖i))
P((X, X)Swap(S∖i)) + P((X, X)Swap(S))
P((X, X)Swap(S))
P((X, X)Swap(S∪i)) + P((X, X)Swap(S))
+ log
i∈S
where we note that
((x, x)swap(S))swap({i})
ʃ (x, X)Swap(S'i) if i ∈ S
I(X, x)swap(S∪i) if i ∈ S.
14
Published as a conference paper at ICLR 2019
Then by inspecting each term in the sum, we see that each term is a KL-divergence term that is
minimized only when
P((X, X)Swap({S∖i})) = P((X, X)Swap(S))	⑼
and
P((X, x)swap({S∪i})) = P((X, X)Swap(S))
for every i ∈ {1,…，d}, every S ⊂ {1,…，d} and each (x, X) ∈ X ×X.
By iteratively applying equation (9), we get that
P((X, X)Swap(S)) = P((X, X)Swap(S∖1))=…=P((X, X)Swap(S\{1,…,d-1}))
=P((X, X)Swap(S\{1,…,d})) = P((X, X)
proving the theorem.	□
Lemma 2. The addition of the term Lf to our loss, does not affect the optimal solution to it.
Proof. By theorem 2, it SufficeS to Show that any diStribution SatiSfying equation (1) alSo minimizeS
maxf Lf. But we note that, aS Shown in [2], maxf Lf (or rather supf Lf) iS the WaSSerStein
d.
diStance between X and X, which iS 0 (and minimal) when X = X. It therefore SufficeS to Show
d.
that equation (1) implieS X = X.
d
Let S = {1,…，d}. Then if (X, X) SatiSfy equation (1), We get that (X, X) = (X, X). Since the
joint diStributionS are equal, it followS that the marginal diStributionS are equal and So by projecting
Iykd.
onto the first d variables, We get that X = X.	□
15
Published as a conference paper at ICLR 2019
MINE
We state the key theory used by MINE to estimate the mutual information. For full details see the
original paper, [4].
The mutual information is defined as
I (U ； V ) = Lbg dPdPUVP^ 电V
where U and V are random variables over some spaces U and V, respectively with joint measure
PUV and marginal measures PU= V dPUV and PV = U dPUV, respectively.
The mutual information can also be characterized by the Kullback-Leibler divergence, DKL, as
I (U ； V ) = Dkl(Puv ||Pu 0 PV)
The Donsker-Varadhan representation then gives us for any two probability measures P and Q over
a probability space Ω
DKL(P||Q) = sup EP[T] - log(EQ[eT])
τq→R
where the supremum is taken over all functions T such that the two expectations are finite.
A simple corollary of this is that fixing a class F of functions (such as a parametrized class {Tθ : θ ∈
Θ}) over which the supremum is taken will provide us with a lower bound for the mutual information
that approaches the true mutual information as the class becomes sufficiently rich. MINE [4] fix the
class to be parametric in this way - given a fixed neural network architecture, they let F = {Tθ :
θ ∈ Θ} be the set of all functions parametrized by this network.
Implementation of KnockoffGAN
In the experiments, the depth of the generator, discriminator and WGAN-GP networks is set to 4
and power network is set to 3. The number of hidden nodes in each layer is d/4, d/16 d/4 for the
generator, discriminator, and WGAN-GP, respectively. For the power network, we use 2 diagonal
matrices for each layer to make two hidden nodes for each feature separately. We use ReLu and tanh
as the activation functions for each layer except for the output layer where we use a linear activation
function for the generator, power network and WGAN-GP networks and sigmoid activation function
for the discriminator network. The number of samples in each mini-batch is 128. KnockoffGAN is
implemented in tensorflow.
Details of Benchmarks
We use the following links for the implementations of 3 benchmarks.
•	Knockoff:	http://web.stanford.edu/group/candes/knockoffs/
software/knockoff/index.html
•	BHq Max:	http://web.stanford.edu/group/candes/knockoffs/
software/knockoff/tutorial-4-r.html
•	BHq Marginal: Modifying the original code of BHq Max in http://web.stanford.
edu/group/candes/knockoffs/software/knockoff/tutorial-4-r.
html
Except for the knockoff generation step, KnockoffGAN follows the same procedures as the
original knockoff framework described at http://web.stanford.edu/group/candes/
knockoffs/software/knockoff/tutorial-2- r.html to select features.
16
Published as a conference paper at ICLR 2019
Additional Experiments
Independent Gaussians
In the following experiment, features were taken to be i.i.d. Gaussian, with mean 0 and variance 1,
i.e. X 〜N(0,11n). The results are reported in Fig. 5.
Y-Logit	Y-Gauss
Figure 5: Comparison of KnockoffGAN with the benchmarks for X 〜N(0,11n). TPR is used to quantify
performance and FDR is reported to verify that it is at the specified threshold (10%).
As we see in Fig. 5, all methods control the FDR at or very close to the specified 10% threshold. We
also see that across the entire range of α, KnockoffGAN achieves a very similar TPR to the original
knockoff framework.
17
Published as a conference paper at ICLR 2019
Non-Gaussian settings
Independent Uniform
In this experiment we set the feature distribution to be a Uniform distribution with mean 0 and
variance 1 (to be consistent with the Gaussian experiments). Fig. 6 displays the results for each
component of X being i.i.d. U(- ʤ/n, vz3∕n) Once again We see that KnockoffGAN consis-
tently outperforms the original knockoff framework, achieving a higher TPR across the entire range
of α in both settings.
Figure 6: Comparison of KnockoffGAN with the benchmarks for X 〜U(—,3/n,，3/n). TPR is used to
quantify performance and FDR is reported to verify that it is at the specified threshold (10%).
18
Published as a conference paper at ICLR 2019
Dirichlet
In this experiment we set the feature distribution to be a Dirichlet(1, ..., 1) distribution - i.e. the
uniform distribution over the (d - 1)-simplex. Correlation here exists through the requirement that
Pid=1 Xi = 1.
Y-Gauss
Figure 7: Comparison of KnoCkoffGAN With the benchmarks for X 〜 DiriChlet(1,…，1). TPR is used to
quantify performance and FDR is reported to verify that it is at the specified threshold (10%).
19
Published as a conference paper at ICLR 2019
Gaussian Mixture Models
For the GMM2 model we set the means (m1, m2) of the 2 Gaussians to be:
•	mi1 = 1 for i = 1 to 100 and 0 for i = 101 to 1000,
•	mi2 = -1 for i = 1 to 100 and 0 for i = 101 to 1000.
Figure 8: Comparison of KnoCkoffGAN With the benchmarks for X 〜GMM2. TPR is used to quantify
performance and FDR is reported to verify that it is at the specified threshold (10%).
20
Published as a conference paper at ICLR 2019
For the GMM3 model we set the means (m1, m2, m3) of the 3 Gaussians to be:
•	mi1 = 1 for i = 1 to 100 and 0 for i = 101 to 1000,
•	mi2 = 1 for i = 1 to 50 and -1 for i = 51 to 100 and 0 for i = 101 to 1000,
•	mi3 = -1 for i = 1 to 100 and 0 for i = 101 to 1000.
Y-Logit	Y-Gauss
Figure 9: Comparison of KnoCkoffGAN With the benchmarks for X 〜GMM3. TPR is used to quantify
performance and FDR is reported to verify that it is at the specified threshold (10%).
21
Published as a conference paper at ICLR 2019
Hyper-parameter Analysis
Hyper-parameter selection in the feature selection problem is difficult; hyper-parameter selection
cannot be performed using cross-validation as we do not have access to ground truth. The hyper-
parameters must therefore be fixed a priori. For this We believe that λ = 1 and μ = 1 are perhaps
the most canonical choice we could make and thus these were used in the main manuscript. In the
following experiments, we investigate the sensitivity of the results to various settings of λ and μ.
The results beloW are in the auto-regressive Uniform setting With Y-Logit and FDR set to 10%.
TPR I	λ
I 0			0.1	0.5	1	5	10
	0.1	70.3	76.2	75.7	77.0	75.1	76.0
φ	0.2	68.7	72.6	71.9	72.7	72.9	73.7
	0.4	42.4	55.1	51.1	53.1	53.8	53.2
	0.8	22.4	27.4	25.3	27.7	27.5	27.6
Table 2: Evaluation of the hyper-parameter λ in the auto-regressive uniform setting with Y-Logit and FDR
threshold 10%
TPR I	μ
I 0			0.1	0.5	1	5	10
	0.1	70.3	76.2	75.7	77.0	75.1	76.0
φ	0.2	68.7	72.6	71.9	72.7	72.9	73.7
	0.4	42.4	55.1	51.1	53.1	53.8	53.2
	0.8	22.4	27.4	25.3	27.7	27.5	27.6
Table 3: Evaluation of the hyper-parameter μ in the auto-regressive uniform setting with Y-Logit and FDR
threshold 10%
As can be seen in Table 2 and 3, the performance of KnockoffGAN is not sensitive to the value of
λ and μ. The only significant difference can be seen when either λ or μ is set to 0 which represents
the exclusion of either the power network or WGAN-discriminator network, respectively, from the
model. In particular, the lack of sensitivity to μ aligns with Lemma 2 in which we see that there is
not a trade-off between Lf and LD but rather the two can be simultaneously minimised.
To further understand the effects of the WGAN network, we report the final values of the other losses
(LD and LP) when μ = 1 and μ = 0 (i.e. with and without the WGAN regularisation). The results
are given below in the auto-regressive Uniform setting with Y-Logit and FDR set to 10%.
Loss I LD I LP
	μ = 0	μ =1	μ = 0	μ =1
— 0.1	0.6894	0.6962	0.0014	0.0203
0.2	0.7005	0.6964	0.0018	0.0115
0.4	0.6919	0.6955	0.0046	0.0180
0.8	0.7013	0.6960	0.0068	0.0311
Table 4: Final values of the other training losses when μ = 0 or μ = 1 in the auto-regressive uniform setting
with Y-Logit and FDR threshold 10%
As can be seen in Table 4, the inclusion of the WGAN network improves the final value of LD,
bringing it significantly closer to its optimal value of log(2) ≈ 0.693. On the other hand, the power
network loss is increased, however, this trade-off is expected and acceptable - the FDR guarantees
rely on the discriminator loss (LD) and not the power network. As we see in Table 3, the TPR does
not suffer from this increased loss for the power network.
22
Published as a conference paper at ICLR 2019
For our final hyper-parameter evaluation, we investigate the effect of varying the hinting probability,
p, which determines the probability with which Bi = 1. We note that this hyper-parameter is used
to trade-off between speed of learning and optimality of the learned solution. A low probability
makes for fast convergence, but suboptimal convergence whereas a high probability makes for slow
convergence (but a more optimal solution). We chose 0.9 to balance this, following the implementa-
tion of [38]. To demonstrate this trade-off we vary this hyper-parameter (from 0 to 0.9). The results
below are for the auto-regressive Uniform distribution with Y-Logit and FDR set to 10%.
TPR 		P					
	0	0.2	0.4	0.6	0.8	0.9
0.1	73.3	74.7	75.0	75.3	75.7	77.0
工 0.2	67.7	68.4	69.1	69.7	72.4	72.7
φ 0.4	51.2	51.6	51.7	51.9	52.9	53.1
0.8	18.0	20.4	21.4	22.4	25.4	27.7
Table 5: Evaluation of the hinting probability, p, in the auto-regressive uniform setting with Y-Logit and FDR
threshold 10%
Results with FDR threshold 5%
In all of the synthetic experiments above, the FDR threshold is set to 10% (which is the level most
thoroughly investigated by [3]). In this final experiment, we set the FDR threshold to 5% (which
aligns with our real-world experiment) and verify that KnockoffGAN is capable of controlling the
FDR at this level and still outperforms the original knockoff framework. We conduct the experiments
on (1) an auto-regressive distribution with Gaussian marginal distributions, (2) an auto-regressive
distribution with Uniform marginal distributions, and (3) a 4-mixture Gaussian mixture model.
Y-Logit	Y-GaUSS
Figure 10: A comparison of the performance of KnockoffGAN for X distributed as an auto-regressive distribu-
tion with Gaussian marginal distributions. TPR is used to quantify performance and FDR is reported to verify
that it is at the specified threshold (5%).
23
Published as a conference paper at ICLR 2019
Y-Logit
Y-GaUSS
Figure 11: A comparison of the performance of KnockoffGAN for X distributed as an auto-regressive distribu-
tion with Uniform marginal distributions. TPR is used to quantify performance and FDR is reported to verify
that it is at the specified threshold (5%).
Y-Gauss
Figure 12: A comparison of the performance of KnockoffGAN for X distributed as 4-mixture Gaussian mixture
model. TPR is used to quantify performance and FDR is reported to verify that it is at the specified threshold
(5%).
24
Published as a conference paper at ICLR 2019
Statistics of real-world biobank dataset
To preserve anonymity of the authors, the full details of this dataset will be given upon acceptance
of the paper. In Table 1, we provide the basic statistics of the real-world biobank dataset.
Staticstics	Values
No of patients	86082
No of features	387
Pearson correlation across the features	25%: 0.0042, 50%: 0.0120 ,75%: 0.0332
Pearson correlation with the outcome	CVD: 25%: 0.0033, 50%: 0.0091,75%: 0.0199 Diabetes: 25%: 0.0068, 50%: 0.0179 ,75%: 0.0439
Label distribution	CVD: 1252Patients(L5%) Diabetes: 3932 patients (4.6%)
Table 6: Basic statistics of real-world biobank dataset (% means percentile in Pearson correlation rows)
25