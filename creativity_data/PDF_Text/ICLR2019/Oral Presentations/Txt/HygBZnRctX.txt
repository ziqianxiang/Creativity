Published as a conference paper at ICLR 2019
Transferring Knowledge
across Learning Processes
Sebastian Flennerhag*
The Alan Turing Institute
London, UK
sflennerhag@turing.ac.uk
Pablo G. Moreno
Amazon
Cambridge, UK
morepabl@amazon.com
Neil D. Lawrence
Amazon
Cambridge, UK
lawrennd@amazon.com
Andreas Damianou
Amazon
Cambridge, UK
damianou@amazon.com
Ab stract
In complex transfer learning scenarios new tasks might not be tightly linked to
previous tasks. Approaches that transfer information contained only in the final
parameters of a source model will therefore struggle. Instead, transfer learning at a
higher level of abstraction is needed. We propose Leap, a framework that achieves
this by transferring knowledge across learning processes. We associate each task
with a manifold on which the training process travels from initialization to final
parameters and construct a meta-learning objective that minimizes the expected
length of this path. Our framework leverages only information obtained during
training and can be computed on the fly at negligible cost. We demonstrate that our
framework outperforms competing methods, both in meta-learning and transfer
learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can
transfer knowledge across learning processes in demanding reinforcement learning
environments (Atari) that involve millions of gradient steps.
1	Introduction
Transfer learning is the process of transferring knowledge encoded in one model trained on one set
of tasks to another model that is applied to a new task. Since a trained model encodes information
in its learned parameters, transfer learning typically transfers knowledge by encouraging the target
model’s parameters to resemble those of a previous (set of) model(s) (Pan & Yang, 2009). This
approach limits transfer learning to settings where good parameters for a new task can be found
in the neighborhood of parameters that were learned from a previous task. For this to be a viable
assumption, the two tasks must have a high degree of structural affinity, such as when a new task
can be learned by extracting features from a pretrained model (Girshick et al., 2014; He et al., 2017;
Mahajan et al., 2018). If not, this approach has been observed to limit knowledge transfer since the
training process on one task will discard information that was irrelevant for the task at hand, but that
would be relevant for another task (Higgins et al., 2017; Achille et al., 2018).
We argue that such information can be harnessed, even when the downstream task is unknown, by
transferring knowledge of the learning process itself. In particular, we propose a meta-learning
framework for aggregating information across task geometries as they are observed during training.
These geometries, formalized as the loss surface, encode all information seen during training and thus
avoid catastrophic information loss. Moreover, by transferring knowledge across learning processes,
information from previous tasks is distilled to explicitly facilitate the learning of new tasks.
Meta learning frames the learning of a new task as a learning problem itself, typically in the
few-shot learning paradigm (Lake et al., 2011; Santoro et al., 2016; Vinyals et al., 2016). In this
* Work done while at Amazon.
1
Published as a conference paper at ICLR 2019
environment, learning is a problem of rapid adaptation and can be solved by training a meta-learner by
backpropagating through the entire training process (Ravi & Larochelle, 2016; Andrychowicz et al.,
2016; Finn et al., 2017). For more demanding tasks, meta-learning in this manner is challenging;
backpropagating through thousands of gradient steps is both impractical and susceptible to instability.
On the other hand, truncating backpropagation to a few initial steps induces a short-horizon bias (Wu
et al., 2018). We argue that as the training process grows longer in terms of the distance traversed on
the loss landscape, the geometry of this landscape grows increasingly important. When adapting to
a new task through a single or a handful of gradient steps, the geometry can largely be ignored. In
contrast, with more gradient steps, it is the dominant feature of the training process.
To scale meta-learning beyond few-shot learning, we propose Leap, a light-weight framework for
meta-learning over task manifolds that does not need any forward- or backward-passes beyond those
already performed by the underlying training process. We demonstrate empirically that Leap is a
superior method to similar meta and transfer learning methods when learning a task requires more than
a handful of training steps. Finally, we evaluate Leap in a reinforcement Learning environment (Atari
2600; Bellemare et al., 2013), demonstrating that it can transfer knowledge across learning processes
that require millions of gradient steps to converge.
2	Transferring Knowledge across Learning Processes
We start in section 2.1 by introducing the gradient descent algorithm from a geometric perspective. Sec-
tion 2.2 builds a framework for transfer learning and explains how we can leverage geometrical
quantities to transfer knowledge across learning processes by guiding gradient descent. We focus
on the point of initialization for simplicity, but our framework can readily be extended. Section 2.3
presents Leap, our lightweight algorithm for transfer learning across learning processes.
2.1	Gradient Paths on Task Manifolds
Central to our framework is the notion of a learning process; the harder a task is to learn, the harder
it is for the learning process to navigate on the loss surface (fig. 1). Our framework is based on the
idea that transfer learning can be achieved by leveraging information contained in similar learning
processes. Exploiting that this information is encoded in the geometry of the loss surface, we leverage
geometrical quantities to facilitate the learning process with respect to new tasks. We focus on the
supervised learning setting for simplicity, though our framework applies more generally. Given a
learning objective f that consumes an input x ∈ Rm and a target y ∈ Rc and maps a parameterization
θ ∈ Rn to a scalar loss value, we have the gradient descent update as
θi+1 = θi - αiSiVf (θi),	(1)
where Vf (θi) = Eχ,y〜p(χ,y) [Vf (θi, x, y)]. We take the learning rate schedule {αih and Precon-
ditioning matrices {Si}i as given, but our framework can be extended to learn these jointly with
the initialization. Different schemes rePresent different oPtimizers; for instance αi = α, Si = In
yields gradient descent, while defining Si as the inverse Fisher matrix results in natural gradient
descent (Amari, 1998). We assume this Process converges to a stationary Point after K gradient stePs.
To distinguish different learning Processes originating from the same initialization, we need a notion of
their length. The longer the Process, the worse the initialization is (conditional on reaching equivalent
Performance, discussed further below). Measuring the Euclidean distance between initialization and
final Parameters is misleading as it ignores the actual Path taken. This becomes crucial when we
comPare Paths from different tasks, as gradient Paths from different tasks can originate from the same
initialization and converge to similar final Parameters, but take very different Paths. Therefore, to
caPture the length of a learning Process we must associate it with the loss surface it traversed.
The Process of learning a task can be seen as a curve on a sPecific task manifold M . While this
manifold can be constructed in a variety of ways, here we exPloit that, by definition, any learning
Process traverses the loss surface of f . As such, to accurately describe the length of a gradient-based
learning Process, itis sufficient to define the task manifold as the loss surface. In Particular, because the
learning Process in eq. 1 follows the gradient trajectory, it constantly Provides information about the
2
Published as a conference paper at ICLR 2019
Figure 1: Example of gradient paths on a manifold described by the loss surface. Leap learns an
initialization with shorter expected gradient path that improves performance.
geometry of the loss surface. Gradients that largely point in the same direction indicate a well-behaved
loss surface, whereas gradients with frequently opposing directions indicate an ill-conditioned loss
surface—something we would like to avoid. Leveraging this insight, we propose a framework for
transfer learning that exploits the accumulation of geometric information by constructing a meta
objective that minimizes the expected length of the gradient descent path across tasks. In doing so,
the meta objective intrinsically balances local geometries across tasks and encourages an initialization
that makes the learning process as short as possible.
To formalize the notion of the distance of a learning process, we define a task manifold M as
a submanifold of Rn+1 given by the graph of f . Every point p = (θ, f (θ)) ∈ M is locally
homeomorphic to a Euclidean subspace, described by the tangent space TpM . Taking Rn+1 to be
Euclidean, it is a Riemann manifold. By virtue of being a submanifold ofRn+1, M is also a Riemann
manifold. As such, M comes equipped with an smoothly varying inner product gp : Tp M × Tp M 7→
R on tangent spaces, allowing us to measure the length of a path on M . In particular, the length (or
energy) of any curve γ : [0, 1] 7→ M is defined by accumulating infinitesimal changes along the
trajectory,
Length(γ)
gγ(t) (Y⑴,Y(U) dt,
Energy(γ) =
0
gγ(t)(Y⑴,Y(I)) dt,
(2)
where Y(t)=今γ(t) ∈ TY(t)M is a tangent vector of γ(t) = (θ(t),f (θ(t))) ∈ M. WeuseParenthe-
ses (i.e. Y(t)) to differentiate discrete and continuous domains. With M being a submanifold ofRn+1,
the induced metric on M is defined by gγ(t)(Y(), Y(t)) = hY(t), Y(t)i. Different constructions of
M yield different Riemann metrics. In Particular, if the model underlying f admits a Predictive Prob-
ability distribution P(y | x), the task manifold can be given an information geometric interPretation
by choosing the Fisher matrix as Riemann metric, in which case the task manifold is defined over the
sPace of Probability distributions (Amari & Nagaoka, 2007). If eq. 1 is defined as natural gradient
descent, the learning Process corresPonds to gradient descent on this manifold (Amari, 1998; Martens,
2010; Pascanu & Bengio, 2014; Luk & Grosse, 2018).
Having a comPlete descriPtion of a task manifold, we can measure the length of a learning Process
by noting that gradient descent can be seen as a discrete aPProximation to the scaled gradient flow
θ(t) = -S(t)Vf(θ(t)). This flow describes a curve that originates in γ(0) = (θ0,f(θ0)) and
follows the gradient at each Point. Going forward, we define Y to be this unique curve and refer to it
as the gradient path from θ0 on M. The metrics in eq. 2 can be comPuted exactly, but in Practice
we observe a discrete learning Process. Analogously to how the gradient uPdate rule aPProximates
the gradient flow, the gradient Path length or energy can be aPProximated by the cumulative chordal
distance (Ahlberg et al., 1967),
K-1
dp(θ0,M)= X kYi+1-Yikp2,	p∈ {1, 2}.	(3)
i=0
3
Published as a conference paper at ICLR 2019
Figure 2: Left: illustration of Leap (algorithm 1) for two tasks, τ and τ0. From an initialization θ0, the
learning process of each task generates gradient paths, Ψτ and Ψτ0 , which Leap uses to minimize the
expected path length. Iterating the process, Leap converges to a locally Pareto optimal initialization.
Right: the pull-forward objective (eq. 6) used to minimize the expected gradient path length. Any
gradient path Ψτ = {ψτi }iK=τ1 acts on θ0 by pulling each θτi towards ψτi+1.
We write d when the distinction between the length or energy metric is immaterial. Using the energy
yields a slightly simpler objective, but the length normalizes each length segment and as such protects
against differences in scale between task objectives. In appendix C, we conduct an ablation study and
find that they perform similarly, though using the length leads to faster convergence. Importantly,
d involves only terms seen during task training. We exploit this later when we construct the meta
gradient, enabling us to perform gradient descent on the meta objective at negligible cost (eq. 8).
We now turn to the transfer learning setting where we face a set of tasks, each with a distinct task
manifold. Our framework is built on the idea that we can transfer knowledge across learning processes
via the local geometry by aggregating information obtained along observed gradient paths. As such,
Leap finds an initialization from which learning converges as rapidly as possible in expectation.
2.2	Meta Learning across Task Manifolds
Formally, we define a task τ = (fτ,pτ, uτ) as the process of learning to approximate the relationship
x 7→ y through samples from the data distribution pτ (x, y). This process is defined by the gradient
update rule uτ (as defined in eq. 1), applied Kτ times to minimize the task objective fτ. Thus, a learn-
ing process starts at θτ0 = θ0 and progresses via θτi+1 = uτ (θτi ) until θτKτ is obtained. The sequence
{θτi }iK=τ0 defines an approximate gradient path on the task manifold Mτ with distance d(θ0 ; Mτ ).
To understand how d transfers knowledge across learning processes, consider two distinct tasks.
We can transfer knowledge across these tasks’ learning processes by measuring how good a shared
initialization is. Assuming two candidate initializations converge to limit points with equivalent
performance on each task, the initialization with shortest expected gradient path distance encodes
more knowledge sharing. In particular, if both tasks have convex loss surfaces a unique optimal
initialization exists that achieves Pareto optimality in terms of total path distance. This can be crucial
in data sparse regimes: rapid convergence may be the difference between learning a task and failing
due to overfitting (Finn et al., 2017).
Given a distribution of tasks p(τ), each candidate initialization θ0 is associated with a measure
of its expected gradient path distance, ET〜p(τ) [d(θ0； MT)], that summarizes the suitability of the
initialization to the task distribution. The initialization (or a set thereof) with shortest expected
gradient path distance maximally transfers knowledge across learning processes and is Pareto optimal
in this regard. Above, we have assumed that all candidate initializations converge to limit points of
equal performance. If the task objective fT is non-convex this is not a trivial assumption and the
gradient path distance itself does not differentiate between different levels of final performance.
As such, it is necessary to introduce a feasibility constraint to ensure only initializations with some
minimum level of performance are considered. We leverage that transfer learning never happens
in a vacuum; we always have a second-best option, such as starting from a random initialization
or a pretrained model. This “second-best” initialization, ψ0, provides us with the performance we
4
Published as a conference paper at ICLR 2019
Algorithm 1 Leap: Transferring Knowledge over Learning Processes
Require: p(τ), τ = (fτ , uτ , pτ): distribution over tasks
Require: β: step size
1:	randomly initialize θ0
2:	while not done do
3:	NF J 0: initialize meta gradient
4:	sample task batch B from p(τ)
5:	for all τ ∈ B do
6:	ψτ0 J θ0: initialize task baseline
7:	for all i ∈ {0, . . . , Kτ - 1} do
8:	ψτi+1 J uτ (ψτi ): update baseline
9:	θτi J ψτi : follow baseline (recall ψτ0 = θ0)
10:	increment NF using the pull-forward gradient (eq. 8)
11:	end for
12:	end for
13:	θ0 J θ0 一 曲NF: update initialization
14:	end while * *
would obtain on a given task in the absence of knowledge transfer. As such, performance obtained
by initializing from ψ0 provides us with an upper bound for each task: a candidate solution θ0 must
achieve at least as good performance to be a viable solution. Formally, this implies the task-specific
requirement that a candidate θ0 must satisfy fτ (θτKτ) ≤ fτ (ψτKτ). As this must hold for every task,
we obtain the canonical meta objective
min
θ0
s.t.
F(θ0)= Eip(T)[d(θ0; MT)]
θτi+1 =uτ(θτi), θτ0 =θ0,
θ0 ∈ Θ = ∩τ {θ0 fτ(θKτ ) ≤ fτ(ψKτ ) }.
(4)
This meta objective is robust to variations in the geometry of loss surfaces, as it balances comple-
mentary and competing learning processes (fig. 2). For instance, there may be an initialization that
can solve a small subset of tasks in a handful of gradient steps, but would be catastrophic for other
related tasks. When transferring knowledge via the initialization, we must trade off commonalities
and differences between gradient paths. In eq. 4 these trade-offs arise naturally. For instance, as the
number of tasks whose gradient paths move in the same direction increases, so does their pull on
the initialization. Conversely, as the updates to the initialization renders some gradient paths longer,
these act as springs that exert increasingly strong pressure on the initialization. The solution to eq. 4
thus achieves an equilibrium between these competing forces.
Solving eq. 4 naively requires training to convergence on each task to determine whether an initial-
ization satisfies the feasibility constraint, which can be very costly. Fortunately, because we have
access to a second-best initialization, we can solve eq. 4 more efficiently by obtaining gradient paths
from ψ0 and use these as baselines that we incrementally improve upon. This improved initialization
converges to the same limit points, but with shorter expected gradient paths (theorem 1). As such,
it becomes the new second-best option; Leap (algorithm 1) repeats this process of improving upon
increasingly demanding baselines, ultimately finding a solution to the canonical meta objective.
2.3 Leap
Leap starts from a given second-best initialization ψ0 , shared across all tasks, and constructs baseline
gradient paths ΨT = {ψTi }iK=τ0 for each task τ in a batch B. These provide a set of baselines
Ψ = {ΨT }T ∈B. Recall that all tasks share the same initialization, ψT0 = ψ0 ∈ Θ. We use these
baselines, corresponding to task-specific learning processes, to modify the gradient path distance
metric in eq. 3 by freezing the forward point γTi+1 in all norms,
5
Published as a conference paper at ICLR 2019
Kτ-1
dp(θ0; MT, Ψτ)= X 后T+1-yTkP,	(5)
i=0
where Y = (ΨT, f (ΨT)) represents the frozen forward point from the baseline and γT = (θT, f (θT))
the point on the gradient path originating from θ0 . This surrogate distance metric encodes the
feasibility constraint; optimizing θ0 with respect to Ψ pulls the initialization forward along each
task-specific gradient path in an unconstrained variant of eq. 4 that replaces Θ with Ψ,
min F(θ0N) = Eτ~p(τ) [d(θ°; MT, Ψτ)],
s.t.	θτi+1 = uτ (θτi ), θτ0 = θ0.
(6)
We refer to eq. 6 as the pull-forward objective. Incrementally improving θ0 over ψ0 leads to a new
second-best option that Leap uses to generate a new set of more demanding baselines, to further
improve the initialization. Iterating this process, Leap produces a sequence of candidate solutions
to eq. 4, all in Θ, with incrementally shorter gradient paths. While the pull-forward objective can be
solved with any optimization algorithm, we consider gradient-based methods. In theorem 1, we show
that gradient descent on F yields solutions that always lie in Θ. In principle, F can be evaluated at
any θ0, but a more efficient strategy is to evaluate θ0 at ψ0. In this case, d= d, so that F = F.
Theorem 1 (Pull-forward). Define a sequence of initializations {θs0}s∈N by
θ0+1= θ0 - βs^F(θ0,Ψs),	θ0 ∈ Θ,	⑺
with ψs0 = θs0 for all s. For βs > 0 sufficiently small, there exist learning rates schedules {αiT}iK=τ1
for all tasks such that θk0→∞ is a limit point in Θ.
Proof: see appendix A. Because the meta gradient requires differentiating the learning process,
we must adopt an approximation. In doing so, we obtain a meta-gradient that can be computed
analytically on the fly during task training. Differentiating F, we have
Kτ -1
vF(θ0, Ψ) = -P Eτ~p(τ)	X	JT(θ0)t	(∆fT	vfτ(θT)	+	∆θT) (kγT+1	- γTkP)p-2	(8)
i=0
where JTi denotes the Jacobian of θTi with respect to the initialization, ∆fTi = fT (ψTi+1) - fT (θTi )
and ∆θTi = ψTi+1 - θTi . To render the meta gradient tractable, we need to approximate the Jacobians,
as these are costly to compute. Empirical evidence suggest that they are largely redundant (Finn
et al., 2017; Nichol et al., 2018). Nichol et al. (2018) further shows that an identity approximation
yields a meta-gradient that remains faithful to the original meta objective. We provide some further
support for this approximation (see appendix B). First, we note that the learning rate directly controls
the quality of the approximation; for any KT , the identity approximation can be made arbitrarily
accurate by choosing a sufficiently small learning rates. We conduct an ablation study to ascertain
how severe this limitation is and find that it is relatively loose. For the best-performing learning
rate, the identity approximation is accurate to four decimal places and shows no signs of significant
deterioration as the number of training steps increases. As such, we assume Ji ≈ In throughout.
Finally, by evaluating VF at θ0 = ψ0, the meta gradient contains only terms seen during standard
training and can be computed asynchronously on the fly at negligible cost.
In practice, we use stochastic gradient descent during task training. This injects noise in f as well
as in its gradient, resulting in a noisy gradient path. Noise in the gradient path does not prevent
Leap from converging. However, noise reduces the rate of convergence, in particular when a noisy
gradient step results in fT(ψTs+1) - fT (θTi ) > 0. If the gradient estimator is reasonably accurate, this
causes the term ∆fTi vfT (θTi ) in eq. 8 to point in the steepest ascent direction. We found that adding
a stabilizer to ensure we always follow the descent direction significantly speeds up convergence and
allows us to use larger learning rates. In this paper, we augment F with a stabilizer of the form
6
Published as a conference paper at ICLR 2019
μ (fτ(θτ); fτ(ψτ+1)) = {-2(fτ(ψT+1) - fτ(θT))2
if fτ(ψτi+1) ≤ fτ(θτi ),
else.
Adding Vμ (re-scaled if necessary) to the meta-gradient is equivalent to replacing ∆fT with -∣∆fT |
in eq. 8. This ensures that we never follow VfT(ΘT) in the ascent direction, instead reinforcing the
descent direction at that point. This stabilizer is a heuristic, there are many others that could prove
helpful. In appendix C we perform an ablation study and find that the stabilizer is not necessary for
Leap to converge, but it does speed up convergence significantly.
3 Related Work
Transfer learning has been explored in a variety of settings, the most typical approach attempting
to infuse knowledge in a target model’s parameters by encouraging them to lie close to those of
a pretrained source model (Pan & Yang, 2009). Because such approaches can limit knowledge
transfer (Higgins et al., 2017; Achille et al., 2018), applying standard transfer learning techniques
leads to catastrophic forgetting, by which the model is rendered unable to perform a previously
mastered task (McCloskey & Cohen, 1989; Goodfellow et al., 2013). These problems are further
accentuated when there is a larger degree of diversity among tasks that push optimal parameterizations
further apart. In these cases, transfer learning can in fact be worse than training from scratch.
Recent approaches extend standard finetuning by adding regularizing terms to the training objective
that encourage the model to learn parameters that both solve a new task and retain high performance
on previous tasks. These regularizers operate by protecting the parameters that affect the loss function
the most (Miconi et al., 2018; Zenke et al., 2017; Kirkpatrick et al., 2017; Lee et al., 2017; Serra
et al., 2018). Because these approaches use a single model to encode both global task-general
information and local task-specific information, they can over-regularize, preventing the model from
learning further tasks. More importantly, Schwarz et al. (2018) found that while these approaches
mitigate catastrophic forgetting, they are unable to facilitate knowledge transfer on the benchmark
they considered. Ultimately, if a single model must encode both task-generic and task-specific
information, it must either saturate or grow in size (Rusu et al., 2016).
In contrast, meta-learning aims to learn the learning process itself (Schmidhuber, 1987; Bengio
et al., 1991; Santoro et al., 2016; Ravi & Larochelle, 2016; Andrychowicz et al., 2016; Vinyals et al.,
2016; Finn et al., 2017). The literature focuses primarily on few-shot learning, where a task is some
variation on a common theme, such as subsets of classes drawn from a shared pool of data (Lake
et al., 2015; Vinyals et al., 2016). The meta-learning algorithm adapts a model to a new task given
a handful of samples. Recent attention has been devoted to three main approaches. One trains the
meta-learner to adapt to a new task by comparing an input to samples from previous tasks (Vinyals
et al., 2016; Mishra et al., 2018; Snell et al., 2017). More relevant to our framework are approaches
that parameterize the training process through a recurrent neural network that takes the gradient
as input and produces a new set of parameters (Ravi & Larochelle, 2016; Santoro et al., 2016;
Andrychowicz et al., 2016; Hochreiter et al., 2001). The approach most closely related to us learns an
initialization such that the model can adapt to a new task through one or a few gradient updates (Finn
et al., 2017; Nichol et al., 2018; Al-Shedivat et al., 2017; Lee & Choi, 2018). In contrast to our work,
these methods focus exclusively on few-shot learning, where the gradient path is trivial as only a
single or a handful of training steps are allowed, limiting them to settings where the current task is
closely related to previous ones.
It is worth noting that the Model Agnostic Meta Learner (MAML: Finn et al., 2017) can be written as
ET〜p(τ)[fτ(θK)] .1 As such, it arises as a special case of Leap where only the final parameterization
is evaluated in terms of its final performance. Similarly, the Reptile algorithm (Nichol et al., 2018),
which proposes to update rule θ0 - θ0 + e (ET〜P(T)[°K] - θ0), can be seen as a naive version of
Leap that assumes all task geometries are Euclidean. In particular, Leap reduces to Reptile if fT
is removed from the task manifold and the energy metric without stabilizer is used. We find this
configuration to perform significantly worse than any other (see section 4.1 and appendix C).
1MAML differs from Leap in that it evaluates the meta objective on a held-out test set.
7
Published as a conference paper at ICLR 2019
No pretraining
FOMAML
MAML
Finetuning
Reptile
Leap
Figure 3: Results on Omniglot. Left: Comparison of average learning curves on held-out tasks (across
10 seeds) for 25 tasks in the meta-training set. Curves are moving averages with window size 5.
Shading: standard deviation within window. Right: AUC across number of tasks in the meta-training
set. Shading: standard deviation across 10 seeds.
Related work studying models from a geometric perspective have explored how to interpolate in a
generative model’s learned latent space (Tosi et al., 2014; Shao et al., 2017; Arvanitidis et al., 2018;
Chen et al., 2018; Kumar et al., 2017). Riemann manifolds have also garnered attention in the context
of optimization, as a preconditioning matrix can be understood as the instantiation of some Riemann
metric (Amari & Nagaoka, 2007; Abbati et al., 2018; Luk & Grosse, 2018).
4	Empirical Results
We consider three experiments with increasingly complex knowledge transfer. We measure transfer
learning in terms of final performance and speed of convergence, where the latter is defined as the
area under the training error curve. We compare Leap to competing meta-learning methods on the
Omniglot dataset by transferring knowledge across alphabets (section 4.1). We study Leap’s ability
to transfer knowledge over more complex and diverse tasks in a Multi-CV experiment (section 4.2)
and finally evaluate Leap on in a demanding reinforcement environment (section 4.3).
4.1	Omniglot
The Omniglot (Lake et al., 2015) dataset consists of 50 alphabets, which we define to be distinct tasks.
We hold 10 alphabets out for final evaluation and use subsets of the remaining alphabets for meta-
learning or pretraining. We vary the number of alphabets used for meta-learning / pretraining from 1 to
25 and compare final performance and rate of convergence on held-out tasks. We compare against no
pretraining, multi-headed finetuning, MAML, the first-order approximation of MAML (FOMAML;
Finn et al., 2017), and Reptile. We train on a given task for 100 steps, with the exception of
MAML where we backpropagate through 5 training steps during meta-training. For Leap, we
report performance under the length metric (d1 ); see appendix C for an ablation study on Leap
hyper-parameters. For further details, see appendix D.
Any type of knowledge transfer significantly improves upon a random initialization. MAML exhibits
a considerable short-horizon bias (Wu et al., 2018). While FOMAML is trained full trajectories, but
because it only leverages gradient information at final iteration, which may be arbitrarily uninforma-
tive, it does worse. Multi-headed finetuning is a tough benchmark to beat as tasks are very similar.
Nevertheless, for sufficiently rich task distributions, both Reptile and Leap outperform finetuning,
with Leap outperforming Reptile as the complexity grows. Notably, the AUC gap between Reptile
and Leap grows in the number of training steps (fig. 3), amounting to a 4 percentage point difference
in final validation error (table 2). Overall, the relative performance of meta-learners underscores the
importance of leveraging geometric information in meta-learning.
8
Published as a conference paper at ICLR 2019
Table 1: Results on Multi-CV benchmark. All methods are trained until convergence on held-out tasks.
FinetUning is multiheaded. * Area under training error curve; scaled to 0-1001Our implementation.
MNIST results omitted; see appendix E, table 4.
Held-out task	Method	Test (%)	Train (%)	AUC*
Facescrub	Leap	19.9	0.0	11.6
	Finetuning	32.7	0.0	13.2
	Progressive NetS^	18.0	0.0	8.9
	HAR	25.6	0.1	14.6
	No pretraining	18.2	0.0	10.5
Cifar10	Leap	21.2	10.8	17.5
	Finetuning	27.4	13.3	20.7
	Progressive NetS^	24.2	15.2	24.0
	HAR	27.7	21.2	27.3
	No pretraining	26.2	13.1	23.0
SVHN	Leap	8.4	5.6	7.5
	Finetuning	10.9	6.1	10.5
	Progressive NetS^	10.1	6.3	13.8
	HAR	10.5	5.7	8.5
	No pretraining	10.3	6.9	11.5
Cifar100	Leap	52.0	30.5	43.4
	Finetuning	59.2	31.5	44.1
	Progressive NetS^	55.7	42.1	54.6
	HAR	62.0	49.8	58.4
	No pretraining	54.8	33.1	50.1
Traffic Signs	Leap	2.9	0.0	1.2
	Finetuning	5.7	0.0	1.7
	Progressive NetS^	3.6	0.0	4.0
	HAR	5.4	0.0	2.3
	No pretraining	3.6	0.0	2.4
4.2	Multi-CV
Inspired by Serra et al. (2018), We consider a set of computer vision datasets as distinct tasks. We
pretrain on all but one task, which is held out for final evaluation. For details, see appendix E. To
reduce the computational burden during meta training, We pretrain on each task in the meta batch for
one epoch using the energy metric (d2). We found this to reach equivalent performance to training on
longer gradient paths or using the length metric. This indicates that it is sufficient for Leap to see a
partial trajectory to correctly infer shared structures across task geometries.
We compare Leap against a random initialization, multi-headed finetuning, a non-sequential version of
HAT (Serra et al., 2018) (i.e. alloWing revisits) and a non-sequential version of Progressive Nets (Rusu
et al., 2016), Where We alloW lateral connection betWeen every task. Note that this makes Progressive
Nets over 8 times larger in terms of learnable parameters.
The Multi-CV experiment is more challenging both due to greater task diversity and greater complex-
ity among tasks. We report results on held-out tasks in table 1. Leap outperforms all baselines on all
but one transfer learning tasks (Facescrub), Where Progressive Nets does marginally better than a
random initialization oWing to its increased parameter count. Notably, While Leap does marginally
Worse than a random initialization, finetuning and HAT leads to a substantial drop in performance. On
all other tasks, Leap converges faster to optimal performance and achieves superior final performance.
9
Published as a conference paper at ICLR 2019
Figure 4: Mean normalized episode scores on Atari games across training steps. Shaded regions depict
two standard deviations across ten seeds. Leap (orange) generally outperforms a random initialization
(blue), even when the action space is twice as large as during pretraining (table 6, appendix F).
4.3	Atari
To demonstrate that Leap can scale to large problems, both in computational terms and in task com-
plexity, we apply it in a reinforcement learning environment, specifically Atari 2600 games (Bellemare
et al., 2013). We use an actor-critic architecture (Sutton et al., 1998) with the policy and the value
function sharing a convolutional encoder. We apply Leap with respect to the encoder using the energy
metric (d2). During meta training, we sample mini-batches from 27 games that have an action space
dimensionality of at most 10, holding out two games with similar action space dimensionality for
evaluation, as well as games with larger action spaces (table 6). During meta-training, we train on
each task for five million training steps. See appendix F for details.
We train for 100 meta training steps, which is sufficient to see a distinct improvement; we expect
a longer meta-training phase to yield further gains. We find that Leap generally outperforms a
random initialization. This performance gain is primarily driven by less volatile exploration, as seen
by the confidence intervals in fig. 4 (see also fig. 8). Leap finds a useful exploration space faster
and more consistently, demonstrating that Leap can find shared structures across a diverse set of
complex learning processes. We note that these gains may not cater equally to all tasks. In the case of
WizardOfWor (part of the meta-training set), Leap exhibits two modes: in one it performs on par with
the baseline, in the other exploration is protracted (fig. 8). This phenomena stems from randomness
in the learning process, which renders an observed gradient path relatively less representative. Such
randomness can be marginalized by training for longer.
That Leap can outperform a random initialization on the pretraining set (AirRaid, UpNDown) is
perhaps not surprising. More striking is that it exhibits the same behavior on out-of-distribution tasks.
In particular, Alien, Gravitar and RoadRunner all have at least 50% larger state space than anything
encountered during pretraining (appendix F, table 6), yet Leap outperforms a random initialization.
This suggests that transferring knowledge at a higher level of abstraction, such as in the space of
gradient paths, generalizes to unseen task variations as long as underlying learning dynamics agree.
5	Conclusions
Transfer learning typically ignores the learning process itself, restricting knowledge transfer to
scenarios where target tasks are very similar to source tasks. In this paper, we present Leap, a
framework for knowledge transfer at a higher level of abstraction. By formalizing knowledge transfer
as minimizing the expected length of gradient paths, we propose a method for meta-learning that
scales to highly demanding problems. We find empirically that Leap has superior generalizing
properties to finetuning and competing meta-learners.
10
Published as a conference paper at ICLR 2019
Acknowledgments
The authors would like to thank anonymous reviewers for their comments. This work was supported
by The Alan Turing Institute under the EPSRC grant EP/N510129/1.
References
Gabriele Abbati, Alessandra Tosi, Michael Osborne, and Seth Flaxman. Adageo: Adaptive geometric
learning for optimization and sampling. In International Conference on Artificial Intelligence and
Statistics,pp. 226-234, 2018.
Alessandro Achille, Tom Eccles, Loic Matthey, Christopher P. Burgess, Nick Watters, Alexander
Lerchner, and Irina Higgins. Life-long disentangled representation learning with cross-domain
latent homologies. arXiv preprint arXiv:1808.06508, 2018.
J Harold Ahlberg, Edwin Norman Nilson, and Joseph Leonard Walsh. The Theory of Splines and
Their Applications. Academic Press, 1967. p. 51.
Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel.
Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments. In
International Conference on Learning Representations, 2017.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-276,
1998.
Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry, volume 191. American
Mathematical Society, 2007.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in Neural Information Processing Systems, 2016.
Georgios Arvanitidis, Lars Kai Hansen, and S0ren Hauberg. Latent Space Oddity: on the Curvature
of Deep Generative Models. In International Conference on Learning Representations, 2018.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279,
2013.
Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a SynaptiC learning rule. UniverSite
de Montreal, DePartement d,informatique et de recherche OPeratiOnnelle, 1991.
Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, and Patrick van der Smagt.
Metrics for Deep Generative Models. In International Conference on Artificial Intelligence and
Statistics, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation
of Deep Networks. In International Conference on Machine Learning, 2017.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In International Conference on Computer Vision and
Pattern Recognition, pp. 580-587, 2014.
Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empiri-
cal investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint
arXiv:1312.6211, 2013.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In International
Conference on Computer Vision, pp. 2980-2988, 2017.
11
Published as a conference paper at ICLR 2019
Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot
transfer in reinforcement learning. arXiv preprint arXiv:1707.08475, 2017.
Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.
In International Conference on Artificial Neural Networks, 2001.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International
Conference on Learning Representations, 2015.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences, 2017.
Abhishek Kumar, Prasanna Sattigeri, and P Thomas Fletcher. Improved Semi-supervised Learning
with GANs using Manifold Invariances. In Advances in Neural Information Processing Systems,
2017.
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of
simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society,
2011.
Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Sang-Woo Lee, Jin-Hwa Kim, JungWoo Ha, and Byoung-Tak Zhang. Overcoming Catastrophic
Forgetting by Incremental Moment Matching. In Advances in Neural Information Processing
Systems, 2017.
Yoonho Lee and Seungjin Choi. Meta-Learning with Adaptive Layerwise Metric and Subspace. In
International Conference on Machine Learning, 2018.
Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. In International
Conference on Learning Representations, 2017.
Kevin Luk and Roger Grosse. A coordinate-free construction of scalable natural gradient. arXiv
preprint arXiv:1808.10340, 2018.
Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan
Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised
pretraining. arXiv preprint arXiv:1805.00932, 2018.
James Martens. Deep learning via hessian-free optimization. In International Conference on Machine
Learning, 2010.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of Learning and Motivation, volume 24, pp. 109-165.
Elsevier, 1989.
Thomas Miconi, Jeff Clune, and Kenneth O. Stanley. Differentiable plasticity: training plastic neural
networks with backpropagation. International Conference on Machine Learning, 2018.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A Simple Neural Attentive
Meta-Learner. In International Conference on Learning Representations, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
12
Published as a conference paper at ICLR 2019
Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms. arXiv
preprint ArXiv:1803.02999, 2018.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge &
Data Engineering, (10):1345-1359, 2009.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In International
Conference on Learning Representations, 2014.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International
Conference on Learning Representations, 2016.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International Conference on Machine
Learning, 2016.
Jurgen Schmidhuber. Evolutionary principles in self-referential learning. PhD thesis, Technische
UniverSit泣MUnChen,1987.
Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska,
Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework
for continual learning. In International Conference on Machine Learning, 2018.
Joan Serrħ, DidaC Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In International Conference on Machine Learning, 2018.
Hang Shao, Abhishek Kumar, and P Thomas Fletcher. The Riemannian Geometry of Deep Generative
Models. arXiv preprint ArXiv:1711.08014, 2017.
Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical Networks for Few-shot Learning. In
Advances in Neural Information Processing Systems, 2017.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT Press,
Cambridge, 1998.
Alessandra Tosi, S0ren Hauberg, Alfredo Vellido, and Neil D Lawrence. Metrics for Probabilistic
Geometries. Conference on Uncertainty in Artificial Intelligence, 2014.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing Networks for One Shot Learning. In Advances in Neural Information Processing Systems,
2016.
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger B. Grosse. Understanding short-horizon bias in
stochastic meta-optimization. In International Conference on Learning Representations, 2018.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual Learning Through Synaptic Intelligence.
In International Conference on Machine Learning, 2017.
13
Published as a conference paper at ICLR 2019
Appendix
A	Proof of theorem 1
Proof. We first establish that, for all s,
ET d(θ0+ι,Mτ) = F %) = F(θ0+∕Ψs+ι) ≤ F(θ0; Ψs) = F (θ0) = ET d(θ0, MT),
with strict inequality for at least some s. Because {βs}s∞=1 satisfies the gradient descent criteria,
it follows that the sequence {θs0 }s∞=1 is convergent. To complete the proof we must show that this
limit point lies in Θ. To this end, we show that for βs sufficiently small, for all s, limi→∞ θsi +1 =
limi→∞ θsi . That is, each updated initialization incrementally reduces the expected gradient path
length while converging to the same limit point as θ00. Since θ00 ∈ Θ by assumption, we obtain θs0 ∈ Θ
for all s as an immediate consequence.
To establish ET d(θs0+1, MT) ≤ ET d(θs0, MT), with strict inequality for some s, let
zTi = (θTs,i,fT(θTs,i))	xiT = (θTs+1,i,fT(θTs+1,i))
hiT=(ψTs,i+1,fT(ψTs,i+1))	yTi =(ψTs+1,i+1,fT(ψTs+1,i+1)),
with ψT,i+1 = θS,i+1. Denote by E「,i the expectation over gradient paths, ET~p(τ) PK「Note that
F(θ0, Ψs) = Eτ,i khT - zTkP	F(θ0, Ψs+ι) = Eτ,i kyT - zTkP
F(θ0+1, Ψs) = ET,i khT - XTkp	F(θ0+1, Ψs+1) = ET,i kyT - XTkp
with p = 2 defining the meta objective in terms of the gradient path energy and p = 1 in terms
of the gradient path length. As we are exclusively concerned with the Euclidean norm, we omit
the subscript. By assumption, every βs is sufficiently small to satisfy the gradient descent criteria
F(θ0Ns) ≥ F(θ0+ιNs). Adding and subtracting F(θ0+ι, Ψ.+ι) to the RHS, we have
ET,ikhiT-zTikp≥ET,ikhiT-XiTkp
= ET,i kyTi -XiTkp+khiT-XiTkp-kyTi -XiTkp.
It follows that ET d(θs0, MT) ≥ ET d(θs0+1, MT) if ET,i khiT - XiT kp ≥ ET,i kyTi - XiTkp. As our main
concern is existence, we will show something stronger, namely that there exists αiT such that
khiT-XiTkp≥ kyTi -XiTkp	∀i,τ,s,p
with at least one such inequality strict for some i, τ, s, in which case dp(θs0+1, MT) < dp(θs0, MT)
for any p ∈ {1, 2}. We proceed by establishing the inequality for p = 2 and obtain p = 1 as an
immediate consequence of monotonicity of the square root. Expanding khiT - XiT k2 we have
khiT-XiTk2-kyTi -XiTk2=k(hiT-zTi)+(zTi -XiT)k2-kyTi -XiTk2
=khiT-zTik2+2hhiT-zTi,zTi -XiTi+kzTi -XiTk2-kyTi -XiTk2.
Every term except kzTi - XiT k2 can be minimized by choosing αiT small, whereas kzTi - XiT k2 is
controlled by βs. Thus, our strategy is to make all terms except kzTi - XiT k2 small, for a given βs,
by placing an upper bound on αTi . We first show that khiT - zTi k2 - kyTi - XiT k2 = O αiT2 . Some
care is needed as the (n + 1)th dimension is the loss value associated with the other n dimensions.
Define ZT = θS,i, so that ZT = (^T, f (ZT)). Similarly define XT, hT, and yT to obtain
14
Published as a conference paper at ICLR 2019
2
khT-zTk2 = khτ- ZTk2 + (fτ(hT)- fτ(^τ))
kyT-xTk2 = kyτ - XTk2 + (fτ(yτ)-fτ(xT))2
2hhT -ZT, ZT - XTi = 2hhT - ZT, ZT - XTi + (fτ(hT) - fτ(ZT))(fτ(ZT) - fτ(XT)).
Consider ∣∣hT - ZTk2 - kyT - XTk2∙ Note that hT = ZT - aTg(^), where g(^T) = ST,Nf(ZT), and
similarly yT = XT 一 αTg(XT) With g(XT) = ST+1,iNf(Xiτ). Thus, ∣hT 一 ZTk2 = αT2∣∣g(ZT)k2 and
similarly for ∣∣yT - XTk2,so
khT - ZTk2 - kyT - XTk2 = (αT)2 (kg(^T)k2 -kg(XT)k2) = o(ST)2).
22
Now consider (f(hT) - f(^T)) - (f(yT) - f(XT)) . Using the above identities and first-order
Taylor series expansion, we have
(fT(hT) - fT(^T))2 =卜NfT(ZTT(h‘T - ZT) + O (αT))2
=(-αTNfT(ZT)Tg(ZT) + O (αT))2 = O (⑶)2),
and similarly for (fT(yT) - fT(XT))2. AS such, khT - zTk2 - kyT - ±Tk2 = O ((αT)2).
Finally, consider the inner producthhT - zT,zT -XTi∙ From above we have that (∕r(hT) -f (ZT))2 =
-aT (VfT(ZT)tg(ZT) - RT) = -αTξT, where RT denotes an upper bound on the residual. We extend
g to operate on ZT by defining O(ZT) = (g(ZT), ξT). Returning to MT - xTk2 - kyT - xTk2, We have
khT - XTk2 - kyT - XTk2 = kzT - XTk2 + 2hhT - ZT,ZT-XTi + o (ST)2)
=kzT - xTk2 - 2αThg(zT),zT - xTi + O ((αT)2).
The first term is non-negative, and importantly, always non-zero whenever βs 6= 0. Furthermore,
αiT can always be made sufficiently small for kZTi - XiT k2 to dominate the residual, so we can
focus on the inner product hgO(ZTi), ZTi - XiT i. If it is negative, all terms are positive and we have
khiT - XiT k2 ≥ kyTi - XiT k2 as desired. If not, kZTi - XiT k2 dominates if
i V	IIzT - xTk2	U m
a_ ≤	∈ (0, ∞).
T — 2hg(zT),zT - XTi '''
Thus, for αiT sufficiently small, we have khiT - XiT k2 ≥ kyTi - XiT k2 ∀i, τ, s, with strict inequality
whenever hgO(ZTi ), ZTi - XiT i < 0 or the bound on αiT holds strictly. This establishes d2(θs0+1, MT) ≤
d2(θs0, MT) for all τ, s, with strict inequality for at least some τ, s. To also establish it for the gradient
path length (p = 1), taking square roots on both sides of khiT - XiT k2 ≥ kyTi - XiT k2 yields the desired
results, and so khiT - XiT kp ≥ kyTi - XiT kp forp ∈ {1, 2}, and therefore
d(θ0+ι, MT) = F(θ0+ιNs+ι) ≤ F(θ0Ns) = d(θ0, MT) ∀τ, S
with strict inequality for at least some τ, s, in particular whenever βs 6= 0 and αiT sufficiently small.
Then, to see that the limit point of Ψs+1 is the same as that of Ψs for βs sufficiently small, note that
XiT = yTi-1. As before, by the gradient descent criteria, βsis such that
E”i MT -xTkp = E”i MT -yT-1 kp ≤ ET,i MT - zTkp = E”i (α"pkg(zT)kp∙
15
Published as a conference paper at ICLR 2019
Define iτ as the noise residual from the expectation; each yτi-1 is bounded by khiτ - yτi-1 kp ≤
(αT)p Ilg(ZT)kp + eT. For βs small this noise component vanishes, and since {αT}i is a converging
sequence, the bound on yτi-1 grows increasingly tight. It follows then that {θsi+1}i∞=1 converges to
the same limit point as {θsi}i∞=1, yielding θs0+1 ∈ Θ for all s, as desired.
B	ABLATION STUDY: APPROXIMATING JACOBIANS Ji (θ0)
To understand the role of the Jacobians, note that (we drop task subscripts for simplicity)
i
Ji+1(θ0) = In - αiSiHf (θi) Ji(θ0) = Y In - αjSjHf(θj)
j=0
i
=In — XαiSiHf (θi)+ O ((αi)2),
j=0
where Hf (θj ) denotes the Hessian of f at θj . Thus, changes to θi+1 are translated into θ0 via
all intermediary Hessians. This makes the Jacobians memoryless up to second-order curvature.
Importantly, the effect of curvature can directly be controlled via αi , and by choosing αi small we
can ensure Ji(θ0) ≈ In to be a arbitrary precision. In practice, this approximation works well (c.f.
Finn et al., 2017; Nichol et al., 2018). Moreover, as a practical matter, if the alternative is some other
approximation to the Hessians, the amount of noise injected grows exponentially with every iteration.
The problem of devising an accurate low-variance estimator for the Ji (θ0) is highly challenging and
beyond the scope of this paper.
UOI-ɔaɪd
0.01	0,1	0.5
10-2
10-3
10-4
10-5
10-6
1	5	10	15	20
step
Figure 5: Relative precision of Jacobian approximation. Precision is calculated for the Jacobian of
the first layer, across different learning rates (colors) and gradient steps.
To understand how this approximation limits our choice of learning rates αi , we conduct an ablation
study in the Omniglot experiment setting. We are interested in the relative precision of the identity
approximation under different learning rates and across time steps, which we define as
ρ i, {αj}ij=0 =
IIn- Ji(θ0)kl
J i(θ0)kι
where the norm is the Schatten 1-norm. We use the same four-layer convolutional neural network as
in the Omniglot experiment (appendix D). For each choice of learning rate, we train a model from a
random initialization for 20 steps and compute ρ every 5 steps. Due to exponential growth of memory
16
Published as a conference paper at ICLR 2019
0.5-
0
2.5-
2.0-
1.5-
1.0-
ssol gniniart egarev
100	200	300	400	500
Meta training steps
p=2, μ=0, fτ = 1
p=2, μ=1, fτ = 1
p=2, μ=0, fτ=0
p=1, μ=0, fτ = 1
p=1, μ=1, fτ = 1
p=1, μ=0, fτ=0
Figure 6:	Average task training loss over meta-training steps. P denotes the dp used in the meta
objective, μ = 1 the use of the stabilizer, and f = 1 the inclusion of the loss in the task manifold.
consumption, we were unable to compute ρ for more than 20 gradient steps. We report the relative
precision of the first convolutional layer. We do not report the Jacobian with respect to other layers,
all being considerably larger, as computing their Jacobians was too costly. We computed ρ for all
layers on the first five gradient steps and found no significant variation in precision across layers.
Consequently, we prioritize reporting how precision varies with the number of gradient steps. As in
the main experiments, we use stochastic gradient descent. We evaluate αi = α ∈ {0.01, 0.1, 0.5}
across 5 different tasks. Figure 5 summarizes our results.
Reassuringly, we find the identity approximation to be accurate to at least the fourth decimal for
learning rates we use in practice, and to the third decimal for the largest learning rate (0.5) we
were able to converge with. Importantly, except for the smallest learning rate, the quality of the
approximation is constant in the number of gradient steps. The smallest learning rate that exhibits
some deterioration on the fifth decimal, however larger learning rates provide an upper bound that is
constant on the fourth decimal, indicating that this is of minor concern. Finally, we note that while
these results suggest the identity approximation to be a reasonable approach on the class of problems
we consider, other settings may put stricter limits on the effective size of learning rates.
C Ablation Study: Leap Hyper-Parameters
As Leap is a general framework, we have several degrees of freedom in specifying a meta learner. In
particular, we are free to choose the task manifold structure, the gradient path distance metric, dp ,
and whether to incorporate stabilizers. These are non-trivial choices and to ascertain the importance
of each, we conduct an ablation study. We vary (a) the task manifold between using the full loss
surface and only parameter space, (b) the gradient path distance metric between using the energy or
length, and (C) inclusion of the stabilizer μ in the meta objective. We stay as close as possible to the
set-up used in the Omniglot experiment (appendix D), fixing the number of pretraining tasks to 20
and perform 500 meta gradient updates. All other hyper-parameters are the same.
Our ablation study indicates that the richer the task manifold and the more accurate the gradient path
length is approximated, the better Leap performs (fig. 6). Further, adding a stabilizer has the intended
effect and leads to significantly faster convergence. The simplest configuration, defined in terms
of the gradient path energy and with the task manifold identifies as parameter space, yields a meta
gradient equivalent to the update rule used in Reptile. We find this configuration to be less efficient
in terms of convergence and we observe a significant deterioration in performance. Extending the
task manifold to the loss surface does not improve meta-training convergence speed, but does cut
prediction error in half. Adding the stabilizer significantly speeds up convergence. These conclusions
also hold under the gradient path length as distance measure, and in general using the gradient path
length does better than using the gradient path energy as the distance measure.
17
Published as a conference paper at ICLR 2019
D Experiment details: Omniglot
Table 2: Mean test error after 100 training steps on held out evaluation tasksJMulti-headed finetuning.
Method No. Pretraining tasks	Leap	Reptile	FinetUningt	MAML	FOMAML	No pretraining
1	62.3	59.8	46.5	64.0	64.5	82.3
3	46.5	46.5	36.0	56.2	59.0	82.3
5	40.3	41.4	32.5	50.1	53.0	82.5
10	32.6	35.6	28.7	49.3	49.6	82.9
15	29.6	33.3	26.9	45.5	47.8	82.6
20	26.0	30.8	24.7	41.7	45.4	82.6
25	24.8	29.4	23.5	42.9	44.0	82.8
Table 3: Summary of hyper-parameters for Omniglot. “Meta” refers to the outer training loop, “task
refers to the inner training loop.
	Leap	Finetuning	Reptile	MAML	FOMAML	No pretraining
Meta training						
Learning rate	0.1	—	0.1	0.5	0.5	—
Training steps	1000	1000	1000	1000	1000	—
Batch size (tasks)	20	20	20	20	20	—
Task training						
Learning rate	0.1	0.1	0.1	0.1	0.1	—
Training steps	100	100	100	5	100	—
Batch size (samples)	20	20	20	20	20	—
Task evaluation						
Learning rate	0.1	0.1	0.1	0.1	0.1	0.1
Training steps	100	100	100	100	100	100
Batch size (samples)	20	20	20	20	20	20
Omniglot contains 50 alphabets, each with a set of characters that in turn have 20 unique samples.
We treat each alphabet as a distinct task and pretrain on up to 25 alphabets, holding out 10 out
for final evaluation. We use data augmentation on all tasks to render the problem challenging. In
particular, we augment any image with a random affine transformation by (a) random sampling a
scaling factor between [0.8, 1.2], (b) random rotation between [0, 360), and (c) randomly cropping the
height and width by a factor between [-0.2, 0.2] in each dimension. This setup differs significantly
from previous protocols (Vinyals et al., 2016; Finn et al., 2017), where tasks are defined by selecting
different permutations of characters and restricting the number of samples available for each character.
We use the same convolutional neural network architecture as in previous works (Vinyals et al., 2016;
Schwarz et al., 2018). This model stacks a module, comprised of a 3 × 3 convolution with 64 filters,
followed by batch-normalization, ReLU activation and 2 × 2 max-pooling, four times. All images are
downsampled to 28 × 28, resulting in a 1 × 1 × 64 feature map that is passed on to a final linear layer.
We define a task as a 20-class classification problem with classes drawn from a distinct alphabet.
For alphabets with more than 20 characters, we pick 20 characters at random, alphabets with fewer
characters (4) are dropped from the task set. On each task, we train a model using stochastic gradient
descent. For each model, we evaluated learning rates in the range [0.001, 0.01, 0.1, 0.5]; we found
0.1 to be the best choice in all cases. See table 3 for further hyper-parameters.
18
Published as a conference paper at ICLR 2019
Table 4: Transfer learning results on Multi-CV benchmark. All methods are trained until convergence
on held-out tasks. ^Area under training error curve; scaled to 0-100. ^OUr implementation.
Held-out task	Method	Test (%)	Train (%)	AUCt
Facescrub	Leap	19.9	0.0	11.6
	Finetuning	32.7	0.0	13.2
	Progressive Nets ^	18.0	0.0	8.9
	HA/	25.6	0.1	14.6
	No pretraining	18.2	0.0	10.5
NotMNIST	Leap	5.3	0.6	2.9
	Finetuning	5.4	2.0	4.4
	Progressive Nets ^	5.4	3.1	3.7
	HA/	6.0	2.8	5.4
	No pretraining	5.4	2.6	5.1
MNIST	Leap	0.7	0.1	0.6
	Finetuning	0.9	0.1	0.8
	Progressive Nets ^	0.8	0.0	0.7
	HA/	0.8	0.3	1.2
	No pretraining	0.9	0.2	1.0
Fashion MNIST	Leap	8.0	4.2	6.8
	Finetuning	8.9	3.8	7.0
	Progressive Nets ^	8.7	5.4	9.2
	HA/	9.5	5.5	8.1
	No pretraining	8.4	4.7	7.8
Cifar10	Leap	21.2	10.8	17.5
	Finetuning	27.4	13.3	20.7
	Progressive Nets ^	24.2	15.2	24.0
	HA/	27.7	21.2	27.3
	No pretraining	26.2	13.1	23.0
SVHN	Leap	8.4	5.6	7.5
	Finetuning	10.9	6.1	10.5
	Progressive Nets ^	10.1	6.3	13.8
	HA/	10.5	5.7	8.5
	No pretraining	10.3	6.9	11.5
Cifar100	Leap	52.0	30.5	43.4
	Finetuning	59.2	31.5	44.1
	Progressive Nets ^	55.7	42.1	54.6
	HA/	62.0	49.8	58.4
	No pretraining	54.8	33.1	50.1
Traffic Signs	Leap	2.9	0.0	1.2
	Finetuning	5.7	0.0	1.7
	Progressive Nets ^	3.6	0.0	4.0
	HA/	5.4	0.0	2.3
	No pretraining	3.6	0.0	2.4
We meta-train for 1000 steps unless otherwise noted; on each task we train for 100 steps. Increasing
the number of steps used for task training yields similar results, albeit at greater computational
expense. For each character in an alphabet, we hold out 5 samples in order to create a task validation
set.
19
Published as a conference paper at ICLR 2019
Table 5: Summary of hyper-parameters for Multi-CV.“Meta” refers to the outer training loop, ‘task
refers to the inner training loop.
	Leap	FinetUning	Progressive Nets	HAT	No pretraining
Meta training					
Learning rate	0.01	—	—	—	—
Training steps	1000	1000	1000	1000	—
Batch size	10	10	10	10	—
Task training					
Learning rate	0.1	0.1	0.1	0.1	—
Max epochs	1	1	1	1	—
Batch size	32	32	32	32	—
Task evalUation					
Learning rate	0.1	0.1	0.1	0.1	0.1
Training epochs	100	100	100	100	100
Batch size	32	32	32	32	32
E Experiment details: Multi-CV
We allow different architectures between tasks by using different final linear layers for each task.
We use the same convolutional encoder as in the Omniglot experiment (appendix D). Leap learns an
initialization for the convolutional encoder; on each task, the final linear layer is always randomly
initialized. We compare Leap against (a) a baseline with no pretraining, (b) multitask finetuning, (c)
HAT (Serra et al., 2018), and (d) Progressive Nets (RUsU et al., 2016). For HAT, We use the original
formulation, but allow multiple task revisits (until convergence). For Progressive Nets, we allow
lateral connections betWeen all tasks and mUltiple task revisits (Until convergence). Note that this
makes Progressive Nets over 8 times larger in terms of learnable parameters than the other models.
inproceedings We train Using stochastic gradient descent With cosine annealing (Loshchilov & HUtter,
2017). DUring meta training, We sample a batch of 10 tasks at random from the pretraining set and
train Until the early stopping criterion is triggered or the maximUm amoUnt of epochs is reached
(see table 5). We Used the same interval for selecting learning rates as in the Omniglot experiment
(appendix D). Only Leap benefited from Using more than 1 epoch as the Upper limit on task training
steps dUring pretraining. In the case of Leap, the initialization is Updated after all tasks in the meta
batch has been trained to convergence; for other models, there is no distinction betWeen initialization
and task parameters. On a given task, training is stopped if the maximUm nUmber of epochs is reached
(table 5) or if the validation error fails to improve over 10 consecUtive gradient steps. Similarly, meta
training is stopped once the mean validation error fails to improve over 10 consecUtive meta training
batches. We Use Adam (Kingma & Ba, 2015) for the meta gradient Update With a constant learning
rate of 0.01. We Use no dataset aUgmentation. MNIST images are zero padded to have 32 × 32
images; We Use the same normalizations as Serra et al. (2018).
F Experiment details: Atari
We Use the same netWork as in Mnih et al. (2013), adopting it to actor-critic algorithms by estimating
both valUe fUnction and policy throUgh linear layers connected to the final oUtpUt of a shared
convolUtional netWork. FolloWing standard practice, We Use doWnsampled 84 × 84 × 3 RGB images
as inpUt. Leap is applied With respect to the convolUtional encoder (as final linear layers vary in size
across environments). We Use all environments With an action space of at most 10 as oUr pretraining
pool, holding oUt BreakoUt and SpaceInvaders. DUring meta training, We sample a batch of 16 games
at random from a pretraining pool of 27 games. On each game in the batch, a netWork is initialized
Using the shared initialization and trained independently for 5 million steps, accUmUlating the meta
gradient across games on the fly. ConseqUently, in any given episode, the baseline and Leap differs
20
Published as a conference paper at ICLR 2019
only with respect to the initialization of the convolutional encoder. We trained Leap for 100 steps,
equivalent to training 1600 agents for 5 million steps. The meta learned initialization was evaluated
on the held-out games, a random selection of games seen during pretraining, and a random selection
of games with action spaces larger than 10 (table 6). On each task, we use a batch size of 32, an
unroll length of 5 and update the model parameters with RMSProp (using = 10-4, α = 0.99) with
a learning rate of 10-4. We set the entropy cost to 0.01 and clip the absolute value of the rewards to
maximum 5.0. We use a discounting factor of 0.99.
Table 6: Evaluation environment characteristics. ^Calculated on baseline (no pretraining) data.
Environment	Action Space	Mean ReWardt	Standard Deviationt	Pretraining Env
AirRaid	6	2538	624	Y
UpNDown	6	52417	2797	Y
WizardOfWor	10	2531	182	Y
Breakout	4	338	13	N
SpaceInvaders	6	1065	103	N
Asteroids	14	1760	139	N
Alien	18	1280	182	N
Gravitar	18	329	15	N
RoadRunner	18	29593	2890	N
21
Published as a conference paper at ICLR 2019
Figure 7:	Mean normalized episode scores on Atari games across training steps. Scores are reported
as moving average over 500 episodes. Shaded regions depict two standard deviations across ten
seeds. KungFuMaster, RoadRunner and Krull have action state spaces that are twice as large as the
largest action state encountered during pretraining. Leap (orange) generally outperforms a random
initialization, except for WizardOfWor, where a random initialization does better on average due to
outlying runs under Leap’s initialization.
22
Published as a conference paper at ICLR 2019
6000-
4000
2000-
0	12	3	4
2500
2000
1500-
1000-
500-
2500-
2250-
2000-
1750-
1500-
1250-
0	12	3	4
0	12	3	4
2500-
2000-
1500-
1000-
500-
0	12	3	4
500-
400-
300-
200-
100
0-
0	12	3	4
50000
40000-
30000
20000
10000
0-
0	12	3	4
Figure 8: Mean episode scores on Atari games across training steps for different runs. Scores are
reported as moving average over 500 episodes. Leap (orange) outperforms a random initialization by
being less volatile.
23