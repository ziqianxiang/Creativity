Published as a conference paper at ICLR 2019
On Random Deep Weight-Tied Autoencoders:
Exact Asymptotic Analysis, Phase Transi-
tions, and Implications to Training
Ping Li* Phan-Minh Nguyent
Ab stract
We study the behavior of weight-tied multilayer vanilla autoencoders under the
assumption of random weights. Via an exact characterization in the limit of large
dimensions, our analysis reveals interesting phase transition phenomena when the
depth becomes large. This, in particular, provides quantitative answers and in-
sights to three questions that were yet fully understood in the literature. Firstly,
we provide a precise answer on how the random deep weight-tied autoencoder
model performs “approximate inference” as posed by Scellier et al. (2018), and
its connection to reversibility considered by several theoretical studies. Secondly,
we show that deep autoencoders display a higher degree of sensitivity to pertur-
bations in the parameters, distinct from the shallow counterparts. Thirdly, we
obtain insights on pitfalls in training initialization practice, and demonstrate ex-
perimentally that it is possible to train a deep autoencoder, even with the tanh
activation and a depth as large as 200 layers, without resorting to techniques such
as layer-wise pre-training or batch normalization. Our analysis is not specific to
any depths or any Lipschitz activations, and our analytical techniques may have
broader applicability.
1	Introduction
The autoencoder is a cornerstone in machine learning, first as a response to the unsupervised learning
problem (Rumelhart & Zipser (1985)), then with applications to dimensionality reduction (Hinton
& Salakhutdinov (2006)), unsupervised pre-training (Erhan et al. (2010)), and also as a precursor to
many modern generative models (Goodfellow et al. (2016)). Its reconstruction power is well utilized
in applications such as anomaly detection (Chandola et al. (2009)) and image recovery (Mousavi
et al. (2015)). With the surge of deep learning, thousands of papers have studied multilayer vari-
ants of this architecture, but theoretical understanding has been limited, since analyzing the learning
dynamics of a highly nonlinear structure is typically a difficult problem even for the shallow autoen-
coder. To get around this, we tackle the task with a critical assumption: the weights are random
and the autoencoder is weight-tied. One enjoys much analytical tractability from the randomness
assumption, whereas weight tying enforces the random autoencoder to perform “autoencoding”. We
also study this in the high-dimensional setting, where all dimensions are comparably large and ide-
ally jointly approaching infinity. We consider the simplest setting: vanilla autoencoders (i.e., ones
with fully connected layers only) and their reconstruction capability. This is done for the sake of
understanding the effect of depth, while we note our techniques may have broader applicability.
The aforementioned assumptions are not without justifications. There is a growing literature on
deep neural networks with random weights, (Li & Saad (2018); Giryes et al. (2016); Poole et al.
(2016); Schoenholz et al. (2016); Gabrie et al. (2018); Amari et al. (2018)) to name a few, revealing
certain properties of deep feedforward networks* 1. Several recent works have also studied random
multilayer feedforward networks through the lens of statistical inference (Manoel et al. (2017);
* Cognitive Computing Lab (CCL), Baidu Research, Bellevue, WA 98004, USA.
liping11@baidu.com.
,Stanford University, Stanford, CA 94305, USA. pm.nguyen@stanford.edu. Work performed at
Baidu Research as Summer Intern in 2018.
1It is worth mentioning that several characteristics of deep random feedforward networks were implicitly
investigated a few decades ago, in the form of (continuous- or discrete-time) recurrent dynamics of shallow
random neural networks (Sompolinsky et al. (1988); Cessac et al. (1994); Cessac (1995)) under the “local
chaos” hypothesis (Amari (1972)).
1
Published as a conference paper at ICLR 2019
Reeves (2017); Fletcher et al. (2018)). The idea of weight tying is considered in the important
paper Vincent et al. (2010) with an empirical finding that autoencoders with and without weight
tying perform comparably, and has become standard in autoencoders. Similar features of random
connection and symmetry also appear in other neural models (Lillicrap et al. (2016); Scellier et al.
(2018)). Finally the high-dimensional setting is common in recent statistical learning advances
(BUhlmann & Van De Geer (2011)), and not too far from the actual practice where many large
datasets have dimensions of at least a few hundreds and are harnessed by large-scaled models.
We seek quantitative answers to three specific questions that are motivated by previous works:
•	In exactly what way does the (vanilla) random weight-tied autoencoder perform “approxi-
mate inference”? This term is coined in Scellier et al. (2018) in connection with the theo-
retical results in Arora et al. (2015), which implicitly studies the said model. In particular,
Arora et al. (2015) proves an upper bound on ∣∣x - x∣∣2, where X and X are the input
and the output of the network, but is limited in the number of layers and specific to the
ReLU activation. This direction has been recently extended by Gilbert et al. (2017). In
our work, we establish precisely what this approximate inference is by obtaining a general
and asymptotically exact characterization2 of X, for any number oflayers and any LiPschitz
continuous activations (Theorem 1 and Section 3.3). Theorem 1 is the key theoretical result
of our work and lays the foundation for all analyses that follow.
•	In what way is the deep autoencoder different from the shallow counterpart? Li & Saad
(2018); Poole et al. (2016) reveal this in terms of the candidate function space and expres-
sivity for feedforward networks. It is unclear how these notions are applicable to weight-
tied autoencoders, which seek replication of the input rather than a generic mapping. In
this work, we show that the deep autoencoder exhibits a higher order of sensitivity to per-
turbations of the parameters (Section 3.4).
•	Does it have any implications to the training practice? Many recent works3 Glorot & Ben-
gio (2010); He et al. (2015); Schoenholz et al. (2016); Pennington et al. (2017); Yang &
Schoenholz (2017); Xiao et al. (2018); Chen et al. (2018); Hayou et al. (2018); Hanin &
Rolnick (2018); Hanin (2018); Burkholz & Dubatovka (2018) demonstrate a connection
between the study of random networks, or ones at initialization, and their trainability. Note
that these works either do not study weight-tied structures, or assume the analysis of the
untying case for weight-tied structures. In our work, we derive and experimentally ver-
ify insights on how (not) to initialize deep weight-tied autoencoders, demonstrating that
it is possible to train them without resorting to techniques such as greedy layer-wise pre-
training, drop-out and batch normalization (Section 3.5). Specifically we experiment with
200-layer autoencoders.
No prior works have attempted all three tasks. The quantitative difference between weight-tied and
weight-untied networks is in fact not negligible, yet the analysis is non-trivial due to the weight tying
constraint (Arora et al. (2015); Chen et al. (2018)). To address this issue and obtain Theorem 1, we
apply the Gaussian conditioning technique, which first appears in the studies of TAP equations in
spin glass theory (Bolthausen (2014)) and is extensively used in the approximate message passing
algorithm literature (Bayati & Montanari (2011); Javanmard & Montanari (2013); Berthier et al.
(2017)). This should be contrasted with untied random networks, whose analysis is typically more
straightforward. More importantly, the difference is not only analytical: the overall picture of deep
random weight-tied autoencoders is rich and drastically different from that of feedforward networks.
An analysis in the limit of infinite depth reveals three fundamental equations governing the picture
(Section 3.1), which displays multiple phase transition phenomena (Section 3.2).
Finally let us quickly mention other recent theoretical works on autoencoders: Arora et al. (2014);
Arpit et al. (2015); Rangamani et al. (2017); Nguyen et al. (2018) studying the learned autoencoders
in specific settings, Baldi (2012); Alain & Bengio (2014); Bengio et al. (2013) taking a framework
where the encoder and the decoder are generic mappings, and Le Roux & Bengio (2008); Sutskever
2Roughly speaking, We prove that X = ci X + c2z, for ci and c2 scalars and Z an independent Gaussian
vector. It is then straightforward to deduce ∣∣X — x∣∣2. This quantity is hence meaningful when ci = 1.
3Many of these works refer to the random network framework as the mean-field analysis. Here we avoid
using the term “mean-field” since it is also used to refer to the analysis of the learned infinitely-wide neural
networks (Mei et al. (2018)), which is a different setting.
2
Published as a conference paper at ICLR 2019
& Hinton (2008); Montufar & Ay (2011) exploring representational properties of related architec-
tures. These works head in different directions from ours.
2	Setting and main theorem
2.1	Setting and assumptions
Consider the following 2L-layers autoencoder with weight tying:
X =0(W>0 (.“L-1 (W>中L (WLbL-I (…σι (W 1σ0 (x) + b1) + …)+ Bl) + VL) + …)+ v1
Here X ∈ Rn0 is the input, W` ∈ Rn'×n'-1 is the weight, b` ∈ Rn' is the encoder bias, and
v` ∈ Rn'-1 is the decoder bias, for ' = 1,…，L. Also 夕' : R → R and b` : R → R are the
activations (where for a vector U ∈ Rn and a function 夕： R → R, we write 夕(U) to denote the
vector (φ (u1),…，夕(un))>). It is usually the case in practice that σ0 (U) = U the identity function.
We introduce some convenient quantities inductively:
x0 = x,	x' = Wr 'σ'-1 (x'-1) + b`,	' = 1,…,L,
XL = W>夕L (XL) + VL,	X' = W>夕' (X'+1) + v`, ' = L - 1,∙∙∙, 1.
Note that X =夕0 (Xι). See Fig. 5 of Appendix A.1 for a schematic diagram. We assume weights
are random. Specifically we generate the weights and biases according to
(W')ij 〜N(0, nlɪ卜id, (b')i 〜N (0, σb' i.i.d.,	(v`)i 〜N(0, σV,j i.i.d.
independently of each other. The scaling of the variances accords with the literature and actual
practice (Glorot & Bengio (2010); Vincent et al. (2010)). We also consider the asymptotic high-
dimensional regime, indexed by n:
n`
n` = n` (n),-------> ɑ` > 0 and n` → ∞ as n → ∞, ∀'.
n`-1
Here σw,', og`,	σv,'	and	ɑ`	are finite constants independent of	n.	We enforce σw,'	>	0,	but
allow σb,' and σv,' to be zero. We assume that all activations are Lipschitz continuous, and the
encoder activations σ∕s are non-trivial in the sense that for any τ > 0, Ez
{σ' (Tz)2}
> 0 where
z 〜N (0, 1). We also assume that n^ ∣∣σo (x)『 tends to a finite and strictly positive constant as
n → ∞. We refer to Appendix A.1 for more clarifications of notations.
2.2 Main theorem
We motivate our main result via a simplified shallow autoencoder: X = W>夕(WX) =
Pm=I Wi2(w>x), where W ∈ Rm×n, Wi ∈ Rn. Notice X is a sum of independent terms,
and by Stein,s lemma (cf. Appendix E.2), E {X} α E {Pm==ι 夕0 (w>x) /m} x. One thus expects
X ≈ cιx + c2z for scalars ci, c2 and Z 〜N (0, In), for large n and m. It is then important to
specify exactly c1 and c2. Theorem 1 formalizes this intuition with precise formulas for the scalars.
We now define some scalar sequences, which will then be related to the (vector) quantities of the
autoencoders in Theorem 1. First we define {τe}g=ι L and {Te}'=° L inductively:
τ2=n kσ0(X) k2,
τ2 = σW,iτ2,
τ' = τ' + σb,',	' = 1,…,L,
τ' = σW,'Ez {σ'-1 (τ⅛-1z) } ,	' = 2,…,L,
for z 〜 N (0, 1). Next, we define {γ', P'}'=2	l+1 inductively:
γL+1
{TLzi夕L (TLzi)},
PL+1 = EzI {夕L (TLzi)2},
Y' = T2— Ezι,z2 {τe-izi2'-i (αgσW,'Y'+ισ'-i (Te-izi) + ja`ow,`p`/i + σ∖'z2)},
p` = Ez1,z2	2'-i (αgσW,'Y'+ib'-i (Te-izi) + Ja/W,`p/i + q/z2) ∖ ,	' = L — 1,..., 2,
3
Published as a conference paper at ICLR 2019
for z1,z2 〜 N (0,1) independently. With these sequences defined, We can state the main theorem.
Its statement uses the relational operator =, which is defined formally in Appendix A.1. Roughly
speaking, an = bn means an and bn are asymptotically equal in distribution as n → ∞.
Theorem 1. Consider the settings and assumptions as in Section 2.1, and the Sequences {τe,Te}
and {γ', ρ'} defined as above. Then in the limit n → ∞:
(a)	{T'} describes the behavior ofthe encoder output X':
χ' = τ'z,	' = 1,…,L,
for Z =N (0, I n`).
(b)	{T', γ', ρ'} describes the behavior ofthe decoder output X `:
X` = αgσW,'T'+ισ'-i (τ'-iZι) +
√α'σW,'P'+ι + σV,'z2,
` = 2, ..., L,
for zι, z2 =N(0, In`-i) independently. One can replace Te-ιzι with X'-ι in the above,
with Z2 independent of X'-ι, in which case the statement also holdsfor ' = 1 with xo = X.
(c)	For the autoencoder,s output X,
X =夕0 (αισWjγ2σ0 (x) +
α1σW2 ,1ρ2 +σv2,1z2
for z2 = N (0, In0) independent ofX.
The proof of the theorem, as well as an outline of the key ideas, are in Appendix A. The theorem
says that X', X` and X admit simple descriptions which are tracked by scalar sequences {T', γ', ρ'}.
Hence we can learn about the autoencoder by analyzing {T', γ', ρ'}, which is generally a simpler
task than studying X', X` and X directly. Numerical simulations in Appendix B suggest that, al-
though the theorem’s statement is in the infinite dimension limit, the agreement is already good for
dimensions of a few hundreds. We note that while the theorem assumes Gaussian biases, the same
proof technique allows to obtain a similar result with a more relaxed condition on the biases.
Remark 2. While the theorem is specific to W ` following the Gaussian distribution, simulations
in Appendix B suggest that the conclusion holds for a much broader class of distributions. We
conjecture that it should hold so long as each W` has i.i.d. entries and is independent of each other,
its distribution has bounded k-th moment for some sufficiently large k, and the activations as well
as the input X satisfy certain mild regularity conditions.
Remark 3. We comment on the range of p` and γ'. We have p` ≥ 0, which is obvious, and if
k2'-ιk∞ ≤ C, then p` ≤ C2. By Stein,s lemma (cf. Appendix E.2),
YL+1 = Ez {夕L (TLzI)},
Y' = αgσW,'Y'+ιEzι,z2 {夕'—i (αgσW,∩'+∖U'-∖ (T'-izι) + Jagσ%,`p`+[ + σ],gz2) σ'7(方—izi)}.
If the activations are non-decreasing, then γ' ≥ 0. Furthermore, if the activations are Lipschitz, then
∣Y'∣ ≤ Cc for some constants C and c.
3	An analysis at infinite depth
In the following, we adopt a semi-rigorous approach, with an emphasis on the overall picture.
3.1	Infinite depth simplification
We make several analytical simplifications. First consider α' = α > 0, σW2 ,' = σW2 > 0, σb2,' =
σb2 ≥ 0,夕' =夕 and σ' = σ all independent of ', except for 夕L which is chosen separately (but we
shall see that the specific choice of 夕L is largely immaterial). We also assume that σV ` = 0, and
σo and 夕0 are the identity4. We introduce a parameter T ≥ 0, whose role will be clear shortly, and
which satisfies:
T2 = T (τ2) ≡ T (τ2; σW,σ2,σ) ,	T (τ2) = σWE {σ (Tz)2 } + σ2,	(1)
4The assumption σV,' = 0 aligns well with several recent theoretical analyses of the autoencoders (Arpit
et al. (2015); Rangamani et al. (2017); Gilbert et al. (2017); Nguyen et al. (2018)) which disregard the decoder’s
4
Published as a conference paper at ICLR 2019
for Z 〜N(0,1). Note that this implies σW
≤ σW,max = T2/E {σ (Tz)2 ).
If this cannot be
satisfied, We set T2 = +∞. In addition, let β = ασ2v > 0. With these, let Us consider the following
two fixed point equations:
Y = G (γ,ρ) ≡ G (γ, ρ; β, τ2, σ, ψ),
P = R (Y,ρ) ≡ R (γ, ρ; β,τ2,σ, ψ),
G(Y, P) = T2E {τz1∕ (βYσ (TzI) + √βpz2)},⑵
R (γ,ρ) = E ∖ φ ∖γσσ (Tzι) + βρzpzι] ∖,	⑶
for zι,z2 〜N (0,1) independently. Then Eq. (1), (2) and (3) together form the fundamental
equations for deep random weight-tied aUtoencoders, in either one of the following senses:
Interpretation 1. For 1	` L, in the limit L → ∞ (and ` → ∞ at a pace sUfficiently slow
compared to L), we have t` → T, γ' → Y and p` → p, where T is a stable fixed point solution to
T* 2 * * = T (t2) , and (γ, p) is thenjointly a stable fixed point solution to Y = G (γ, p) and P = R (γ,p).
In light of Theorem 1, (T, γ, p) describes the behavior of an intermediate X`:
X' = Ssigσ (X'-l) + Svarz, Ssig = βγ,	Svar = /YP,
where Z = N(0, I n`-j independent of X'-ι, and X'-ι = Tz0 for z0 = N(0, I n`-j. If the
convergences t` → T, γ' → Y and p` → P are fast, then the majority of the intermediate layers are
well approximately described by the above, in the regime of large L.
Interpretation 2. Suppose that for To = ∣∣xk /√n0, we impose the constraint Tl2 = E {σ (Tz)2}.
This should be interpreted as follows: starting with a chosen T, we normalize the input data X
according to T02 = E {σ (Tz)2}; then we choose σW ≤ σW,maχ and σ2 according to Eq. (1). Under
this constraint, it is easy to see that t` = T for all' ≥ 1, and hence the norm of the input to each of
the encoder’s hidden layers is preserved by Claim (a) of Theorem 1. We then have that as L → ∞,
at small ' ≥ 2 (i.e., at the layers near the two ends of the autoencoder), t` → Y and p` → p, where
(Y, p) is jointly a stable fixed point of Y = G (Y, p) and p = R (Y, p). The autoencoder’s output is
then, in this limit,
X = SsigX + SvarZ, Ssig = Yy,	Svar = /Yp,
for z = N (0, In0) independent of X.
In either cases, we see that X` or X is a composition of a signal component and a Gaussian variation
component. Their respective strengths Ssig and Svar admit simple expressions, thanks to the infinite-
L simplification5. We note that G and R do not take σb2 as a parameter. We refer to Appendix C.1
for the computation of Y and p. Fig. 1 shows that our asymptotic simplification is quite accurate
already for L on the order of a few tens.
We also remark that the equation T2 = T (τ2) is known in the signal propagation analysis of random
feedforward networks (Poole et al. (2016)), but the equations Y = G (Y, p) andp = R (Y, p) are new.
We also observe that in these equations, there is the presence of α (through γ), which is typically
missing from such analyses. Hence unlike untied structures, one may expect to see architectural
constraints in analyses of weight-tied structures.
3.2	Phase transition of the fixed point
With the infinite depth simplification, one question is on the existence of the solutions to Eq. (2) and
(3), and how these fixed points look like. (Eq. (1) is well-studied, cf. Poole et al. (2016); Hayou
et al. (2018), and we will not analyze it here.) We note that Y = 0 is always a solution to Eq. (2), in
biases. In addition, it is a common practice to initialize an untrained neural network (hence a random one) with
zero biases (Goodfellow et al. (2016)). This assumption also helps making the analysis more tractable. Other
assumptions seem restrictive, but can be relaxed without affecting the analysis that follows, and hence are made
for ease of presentation. For example, the assumption ɑ` = α for all ' may be relaxed to ɑ` = α for most'
such that 1 ` L.
5Recall we consider the limit of n → ∞ before taking L → ∞. Without a proof, we expect that our results
hold for L = o (log n), so practically this means 1 4 L 冬 Ω (log n).
5
Published as a conference paper at ICLR 2019
Figure 1: The gaps ∣γ2 - Y| and ∣ρ2 - ρ∣ versus the depth L, where Y and ρ2 (which are dependent
on L) are as in Section 2.2, and γ and ρ (the infinite-L limits of γ2 and ρ2) are from Eq. (2) and
(3). Here all activations are tanh, T2 = 1.2, σ2 = 0.211, σW = 2.312 ‹。54ɑ乂 ≈ 2.806, and
τ2 ≈ 0.4276, which satisfies τ2 = E
{σ (Tz)2 }. From left to right: a
0.9, α
1.0 and α = 1.5.
The gaps decrease exponentially with the depth L.
1
能0.5
0
Figure 2: The mapping γ → G (γ, P) for T2 = 1 and β = 5 (blue), β = 2.7 (red), β = 0.8 (green).
The color intensity varies with ρ ∈ [0.1, 1] with equal spacings, where the darkest curve corresponds
to ρ = 0.1, and the lightest is P = 1. From left to right:夕，σ are ReLU;夕 is ReLU, σ is tanh;
夕，σ are tanh;夕 is tanh, σ is ReLU. A fixed point is an intersection between this mapping and the
identity line (black dashed).
which case Eq. (3) also has a solution, for instance, when 夕(0) = 0 such as ReLU or tanh (which
admits P = 0). However γ = 0 is trivial, since it implies Ssig is zero. We will be interested in the
existence of non-trivial and stable fixed points. To ease visualization, for the moment, let us consider
Eq. (2) only. Fig. 2 shows γ → G (γ, ρ) for different ρ, β,夕 and σ for T2 = 1. For a given 夕 and
σ, depending on β and P, one may observe one or more fixed points, one of which is at γ = 0 and
can be stable or unstable. When γ = 0 is the only fixed point but is unstable, we have γ = ∞ as the
“stable solution” to Eq. (2). The solution landscape changes drastically with β; for instance, when
σ =夕=tanh, Y = 0 is the only and stable fixed point when β is small, but it becomes unstable
and a new fixed point at γ > 0 emerges when β is sufficiently large. This hints at certain phase
transition behaviors as β varies.
In Appendix C.2, we perform a detailed analysis of Eq. (2) and (3), supported by several rigorously
proven properties. In the following, by an initialization for Y and P, we mean YL+1 and PL+1 as
in Section 2.2, and by convergence to Y and P, we mean the convergences as in Section 3.1. We
highlight some results from the analysis for specific pairs of 夕 and σ:
ReLU φ and σ. We have two phase transitions at β = 2 and at β = 4. When β < 2, with any
initialization, we have convergence to Y = 0 and P = 0. When 2 < β < 4, we have, with certain
initializations, convergence to Y = 0 and divergence to P = +∞, and with certain other initializa-
tions, divergence to Y = +∞ and P = +∞. These include almost all possible initializations. When
β ≥ 4, with any non-zero initialization, we have divergence to Y = +∞ and P = +∞.
ReLU φ and tanh σ. We have two phase transitions at β = 2 and β = β0 (τ) ∈ (2, ∞). When
β < 2 , with any initialization, we have convergence to Y = 0 and P = 0. When 2 < β < β0 ,
with any non-zero initialization, we have convergence to Y = 0 and divergence to P = +∞. When
β > β0, with any non-zero initialization, we have divergence to Y = +∞ and hence P = +∞.
6
Published as a conference paper at ICLR 2019
Figure 3: γ and ρ versus β, as solved with Eq. (2) and (3). The vertical dotted line is β = σW2 ,max.
From left to right: (1)夕=σ = tanh and T2 = 0.0259, (2)夕=σ = tanh and T2 = 1.2, and (3)
夕=tanh, σ is the ReLU, and T2 = 0.2.
tanh φ and σ. We have two phase transitions at β = 1 and β = βo (T) > 1. When β ≤ 1,
we have convergence to γ = ρ = 0. When 1 < β < β0, with any non-zero initialization, we have
convergence to γ = 0 and ρ ∈ (0, 1). When β > β0, with any non-zero initialization, we have
convergence to γ > 0 and P ∈ (0,1). For T > 0, Y cannot grow to +∞ as β varies. We note that
βo → 1 if T2 → 0, and in the case α = 1, this implies σW → 1. With respect to Eq. (1), we then
have σb2 → 0. An illustration is given in Fig. 3.
tanh φ and ReLU σ.	We have a picture similar to the case 夕=σ = tanh, with a crucial
difference that one cannot have β0 be close to 1. An illustration is given in Fig. 3.
γ and P thus exhibit phase transitions, depend crucially on the choice of activations (especially the
decoder activation 夕),and can be trivialized (i.e., being zero or infinity) as in the case of ReLU 夕
and σ . It is remarkable that the above pictures are general for many other activations, as suggested
by our analysis. In the next sections, we explore the implications of these behaviors.
3.3 Approximate inference at infinite depth
Theorem 1 gives a quantitatively exact sense of how the random weight-tied autoencoder performs
“approximate inference”. Here we will be interested in stronger notions. A first question is: does
it explain reversibility? Reversibility, as mathematically formalized in previous works Arora et al.
(2015); Gilbert et al. (2017), quantitatively concerns with how small the quantity E = ∣∣x - X∣∣2 /n0
is. The smaller it is, the better the decoder “reverses” the encoder. This formalized notion is an
attempt to give a theoretical understanding of empirical findings that the input could be reproduced
from the values of hidden layers of a trained feedforward network. Let us now consider the infinite
depth simplification under Interpretation 2 of Section 3.1. We have E ` (Ssig - 1)2 T02 + 5/&-
As such, for E ≈ 0 with high probability, one must have Ssig ≈ 1 and Svar ≈ 0, hence P ≈ 0.
Consequently, E
{ψ (σ (TzI))2}
≈ 0. For T > 0, E {夕(σ (Tzι))2 }
0 is impossible for any
non-trivial activations (unless the activation outputs zero almost everywhere). Strikingly, in light
of Section 3.2, when φ and σ are both ReLU, we have that Ssig and Svar are either 0 or +∞, in
which case E ≥ T2 and can become unbounded. While this does not contradict the results in Arora
et al. (2015) (which also concerns with ReLU activations, but with specific choices of the biases and
limited depth, and hence is in a different setting), our discussion suggests that random weight-tied
models may be insufficient to explain reversibility.
A second question is: does the model perform signal recovery? In this case, we are interested in
whether X = cx for some constant C not necessarily 1. Similar to the above, this requires Svar = 0,
hence P
0, and E {夕(βγσ (TzI))2}
0. For non-trivial 夕 and σ, this requires 夕(0) = 0 and
γ = 0. Many activations do not conform with the former, and the latter implies X = 0 undesirably.
This provides a negative answer to the question.
A critic may argue that in expectation, E {X} ≈ Ssigx, and as per Section 3.2, there are cases where
γ > 0 and hence Ssig > 0. Yet in fact, this in-expectation property can already be observed in the
simple setting of linear shallow autoencoders (Arora et al. (2015)). What is ignored in such argument
is that in many cases, Svar > 0 whenever Ssig > 0, in light Section 3.2. Our analysis hence mitigates
7
Published as a conference paper at ICLR 2019
the shortcoming of the in-expectation approach, and gives a more precise understanding of what the
random weight-tied autoencoder can and cannot achieve when the depth becomes large.
3.4	Comparison with the shallow case
Our result also allows for the case of L = 1 a shallow autoencoder. In particular, taking a parallel
setting with Section 3.1 (in particular, Interpretation 2), by Theorem 1,
X = Ssigx + Svarz,
Ssig = βγ,	Svar = √βρ,
in which, with 夕hid being the activation in the hidden layer,
Y = τ12E {τz夕hid (τz)} , P = E {夕hid (τz)2}.
Some observations follow. In the shallow case, γ > 0 and ρ > 0 and both are bounded regardless
of the parameters, except for trivial edge cases such as τ2 = 0 or 夕^d (∙) = 0. Furthermore, Y
and ρ are independent of β, for a fixed τ. As such, there is no phase transition in Y and P as β
changes. We also have Ssig (β) α β and Svar (β) α √β. Hence, the signal component dominates
with Ssig/Svar α √β. Again this happens regardless of parameter choices.
In comparison with the infinite depth case, for φ = σ = tanh or φ = tanh and σ being the
ReLU, as observed from Fig. 3, in certain regimes, γ, P and γ∕√ρ can grow (sublinearly) with
β, and hence Ssig (β) = Ω (β) 6, Svar (β) = Ω (√β), and the signal component dominates with
Ssig/Svar = Ω (√β). In particular, near the phase transition of γ, Ssig/Svar = Ω (β1∙5). Recalling
that β = ασW2 , this implies for the infinite depth case, as compared to the shallow one, firstly a
slight perturbation in σW2 may result in a larger perturbation in the signal’s strength, and secondly an
architecture using larger α may gain more in terms of amplification of the signals. In short, the deep
autoencoder is more sensitive to slight changes in the parameters. As evident in Section 3.2, the case
夕 being the ReLU also exhibits extreme sensitivity, in that it is possible for a slight perturbation in
β to drastically change Y and P. As suggested by Fig. 1, it should be the case already for L about
a few tens. It is, however, at the expense of much care in the selection of parameters, since there
are continuous regimes in which the infinite depth diminishes Ssig and Svar to zero or boost them to
infinity, a situation that never occurs in the shallow case.
Remark 4. Sensitivity to perturbations is implied by expressivity, a notion put forth in Poole et al.
(2016) in the study of random feedforward networks. Hence we expect that sensitivity is a common
feature of various types of deep neural networks.
3.5	Implications to training initialization
We examine the implications of Interpretation 1 in Section 3.1 to trainability of the weight-tied
autoencoder. We first state our hypothesis, then test it with experiments.
The hypothesis. Since the majority of intermediate layers can be described approximately by Y
and ρ (as well as τ) and the random weight-tied autoencoder is in fact one at initialization, appro-
priate values of γ, P and τ (by a suitable choice of σW and σ2) should lead to better trainability.
In particular, if one of them is ∞, we expect numerical errors or too large values resulting in quick
saturation, both of which render the autoencoder untrainable. IfY = P = 0 in a neighborhood of the
chosen σW2 and σb2, we expect that the progress is slowed down in the beginning. If such pitfalls are
avoided, the autoencoder is expected to show a faster progress.
Our analysis in Section 3.2 shows that those pitfalls can occur for a wide range of parameters. If the
hypothesis is true, σW2 and σb2 must then be chosen carefully when L is large. As a special remark
on the case 夕 is the ReLU, when α = 1, the hypothesis suggests taking σW = 2 and σ2 = 0.
This coincides with the celebrated He initialization (He et al. (2015)), which however considers
feedforward networks only. Interestingly this does require σ to be the ReLU; for instance, σ can be
tanh, in which case the argument in (He et al. (2015)) is not applicable.
We shall also examine edge of chaos (EOC) initializations (Schoenholz et al. (2016); Pennington
et al. (2017)) (see Appendix E.1). The EOC initialization enables better signal propagation in deep
feedforward networks, and in our context, is relevant to the encoder part with the activation σ.
6By f (β) = Ω(g(β)) in a certain range of β, we mean 言[f (β)∕g(β)] ≥ 0 on this range.
8
Published as a conference paper at ICLR 2019
No.	中	σ	σW	σ2	τ2	EOC	Trainable	Slowed	Inf
-τ-^	ReLU	ReLU	2.0	0.0	-	x	X		
~^Γ~	ReLU	ReLU	1.0	0.1	0.2			x	
3^Γ	ReLU	ReLU	2.5	0.0	∞				x
4^~	ReLU	tanh	2.0	0.0	—	0.618		X		
5~Γ~	ReLU	tanh	1.05	2.01 X 10-5	0.0259	xx		x	
6~β~	ReLU	tanh	2.505	0.3	—	1.460				X
Tl--	tanh	tanh	1.05	2.01 × 10-5	0.0259	xx	x		
~~Γ~	tanh	tanh	0.5	-0.0136-	0.0259			x	
9~9~	tanh	tanh	2.312	0.211	-1.2-	x	x		
^T0-	tanh	tanh	0.5	0.986	-1.2-			x	
~~∏~	tanh	tanh	1.0	0771	1.2		X		
~2T~	tanh	ReLU	2.0	0.0	-	x	X		
-T3-	tanh	ReLU	1.0	0.1	-0.2-		X		
	tanh	ReLU	0.5	0.15	—	0.2			x	
Table 1: List of initialization schemes for each pair of φ and σ, for α = 1. Here “-” indicates a
positive finite value that depends on the choice of 夕L (for which we choose the ReLU), but its exact
value is irrelevant for our purpose. “EOC” indicates whether the scheme is an EOC initialization
with respect to σ , and “xx” indicates an EOC scheme that is found to be the better one among
all EOC initializations with Gaussian weights (Pennington et al. (2017)). “Trainable” indicates
better trainability in the beginning as predicted by our theory. “Slowed” indicates γ = ρ = 0 in a
neighborhood. “Inf” indicates either Y → ∞ or P → ∞. The schemes with 夕=tanh should be
reflected against Fig. 3.
Experiments. Table 1 lists several initialization schemes with α = 1, which are chosen such that
the hypothesis can be tested separately for each pair 夕 and σ. We perform simple experiments on a
weight-tied vanilla autoencoder as described in Section 3.1: L = 100, all hidden dimensions of 400,
identity input activation σ0, and decoder biases initialized to zero. This sets ɑ` = α = 1 for ' ≥ 2;
here α1 6= 1 is irrelevant in light of Interpretation 1. We train the autoencoder on the MNIST dataset
with mini-batch gradient descent with a batch size of 250 and without regularizations, for 5 × 105
iterations (equivalent to 2500 epochs). We perform the experiments in two settings:
•	Setting 1: The output activation 夕0 is tanh, MNIST images are normalized to [-1, +1],
and the learning rate is fixed at 5 × 10-3. This is standard for MNIST.
•	Setting 2:夕0 is the identity, MNIST images are unnormalized (i.e., normalized to [0, +1]),
and the learning rate is fixed at 3 × 10-3. This is common for regression.
These learning rates are chosen so that the learning dynamics is typically smooth, in light of recent
works Mei et al. (2018); Smith & Le (2018). We use the normalized '2 loss ∣∣x - x『/ IIx『，and
are primarily interested in this loss as a quality measure, since we only focus on trainability7. We
also do not apply techniques such as greedy layer-wise pre-training, drop-out or batch normalization.
The results are plotted in Fig. 4. See also Appendix D.1 for visualization of the reconstructions, and
Appendix D.2 for the evolution over a broader range of parameters. Note that we plot the evolution
in the logarithmic scale of time, since it is typically smooth and revealing on this scale, as found in
prior works Baity-Jesi et al. (2018); Mei et al. (2018) and also evident from the plots.
Discussion. The results are in good agreement with our hypothesis. (Recall we test the hypothesis
separately for each pair 夕 and σ, for which the involved schemes share the same architecture and
only differ in the initialization.) Note that as predicted, in Setting 2, Scheme 3 and 6 are trapped
with numerical errors, and in Setting 1, they saturate quickly at a high loss. As such, we do not
include the results of Scheme 3 and 6 in Fig. 4.
7The chosen loss is slightly different from the traditional '2 loss ∣∣x — x∣∣2. On one hand, We found from
our experiments that these two losses perform comparably, with the normalized loss typically yielding slight
improvements, provided that the learning rates are scaled appropriately. On the other hand, the normalized loss
allows ease for interpretation.
9
Published as a conference paper at ICLR 2019
1.2
0.8
S
S
O
工 0.6
ω
φ
I—
0.4
0.2
—1
--2
—4
--5
—7
--8
—9
--10
--11
—12
--13
—m— 14
0」
IO0
Scheme 11, 13
Scheme 2, 5,
8, 10, 14
0.8
S
S
O
二 0.6
Iteration
1.2
1
0.4
0.2
0
IO0
Iteration
1
Figure 4:	Test loss ∣∣x - x∣∣2 / ||x『 of the schemes from Table 1. Left: the setting with 夕0 = tanh
(Setting 1). Right: the setting where 夕0 is the identity (Setting 2).
We see from the figure that Scheme 2, 5, 8, 10 and 14 show much slower progresses, by a factor
of 3 to 10 times in terms of training iterations to reach the same loss. Hence a good amount of
training time can be saved by an appropriate initialization. Interestingly Scheme 5 is in fact a special
EOC initialization that Pennington et al. (2017) found to be the better one among all EOC schemes
with Gaussian weights for tanh activation. This last observation shows that having good signal
propagation through the encoder is far from being a sufficient condition for trainability.
Among the schemes, only Scheme 1 and 4 in Setting 1 and only Scheme 1, 4 and 7 in Setting 2
have their eventual trained networks produce meaningful reconstructions, whereas the rest always
output some “average” of the training set regardless of the input, at the end of 5 × 104 5 & iterations
(see Appendix D.1). It is unclear whether this is a bad local minimum, or whether these schemes
take much longer to show further progresses. An explanation is beyond our current theory, and it is
an open question how to create a scheme with meaningful trainability. Remarkably all the schemes
that show slower initial progresses (Scheme 2, 5, 8, 10 and 14) are among those that could not yield
meaningful reconstructions.
We observe that in Setting 2, the tanh network under Scheme 7 is best performing in terms of the
reconstruction loss, and its progress does not seem to reach a plateau after 5 × 105 iterations. In both
settings, Scheme 4, which is a hybrid of ReLU and tanh activations, shows slight improvements
over Scheme 1, which is a purely ReLU network. This extends the conclusion in Pennington et al.
(2017) to the context of weight-tied autoencoders: reasonable training at a large depth is possible
even for the notoriously difficult tanh activation, and this necessarily requires careful initializations.
Overall we see that our experiments confirm the hypothesis, showing an intimate connection be-
tween the phase transition behaviors found by our theory and trainability of the autoencoders.
4 Discussion
This paper has shown quantitative answers to the three questions posed in Section 1. This feat is
enabled by an exact analysis via Theorem 1. The theorem is stated in a general setting, allowing
varying activations, weight variances, etc, but our analyses in Section 3 have made several sim-
plifications. This leaves a question of whether these simplifications can be relaxed, and how the
picture changes accordingly, for instance, when the parameters vary across layers, similar to Yang
& Schoenholz (2018). Many other questions also remain. For example, what would be the covari-
ance structure between the outputs of two distinct inputs? How does the network’s Jacobian matrix
look like? These questions have been answered in the feedforward case (Poole et al. (2016); Pen-
nington et al. (2017)), but we believe answering them is more technically involved in our case. We
have also seen that an autoencoder that shows initial progress may not necessarily produce mean-
ingful reconstruction eventually after training, and hence much more work is needed to understand
the training dynamics far beyond initialization. Recent works Mei et al. (2018); Rotskoff & Vanden-
Eijnden (2018); Sirignano & Spiliopoulos (2018); Chizat & Bach (2018) have made progresses in
this direction for shallow networks.
10
Published as a conference paper at ICLR 2019
References
Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating
distribution. Journal of Machine Learning Research ,15(1):3563-3593, 2014.
Shun-Ichi Amari. Characteristics of random nets of analog neuron-like elements. IEEE Transactions
on systems, man, and cybernetics, (5):643-657, 1972.
Shun-ichi Amari, Ryo Karakida, and Masafumi Oizumi. Statistical neurodynamics of deep net-
works: Geometry of signal spaces. arXiv preprint arXiv:1808.07169, 2018.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some
deep representations. In International Conference on Machine Learning, pp. 584-592, 2014.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. Why are deep nets reversible: A simple theory, with
implications for training. arXiv preprint arXiv:1511.05653, 2015.
Devansh Arpit, Yingbo Zhou, Hung Ngo, and Venu Govindaraju. Why regularized auto-encoders
learn sparse representation? arXiv preprint arXiv:1505.05561, 2015.
Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gerard Ben Arous, Chiara Cam-
marota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing dynamics: Deep neural
networks versus glassy systems. arXiv preprint arXiv:1803.06969, 2018.
Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In Proceedings of ICML
Workshop on Unsupervised and Transfer Learning, pp. 37-49, 2012.
Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with
applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764-785,
2011.
Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders
as generative models. In Advances in Neural Information Processing Systems, pp. 899-907, 2013.
Raphael Berthier, Andrea Montanari, and Phan-Minh Nguyen. State evolution for approximate
message passing with non-separable functions. arXiv preprint arXiv:1708.03950, 2017.
Erwin Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington-
Kirkpatrick model. Communications in Mathematical Physics, 325(1):333-366, 2014.
Peter Buhlmann and Sara Van De Geer. Statistics for high-dimensional data: methods, theory and
applications. Springer Science &amp; Business Media, 2011.
Rebekka Burkholz and Alina Dubatovka. Exact information propagation through fully-connected
feed forward neural networks. arXiv preprint arXiv:1806.06362, 2018.
Bruno Cessac. Increase in complexity in random neural networks. Journal de Physique I, 5(3):
409-432, 1995.
Bruno Cessac, Bernard Doyon, Mathias Quoy, and Manuel Samuelides. Mean-field equations, bifur-
cation map and route to chaos in discrete time neural networks. Physica D: Nonlinear Phenomena,
74(1-2):24-44, 1994.
Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM com-
puting surveys (CSUR), 41(3):15, 2009.
Minmin Chen, Jeffrey Pennington, and Samuel S Schoenholz. Dynamical isometry and a mean field
theory of rnns: Gating enables signal propagation in recurrent neural networks. arXiv preprint
arXiv:1806.05394, 2018.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. Emnist: an extension of
mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.
11
Published as a conference paper at ICLR 2019
Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and
Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine
Learning Research,11(Feb):625-660, 2010.
Alyson K Fletcher, Sundeep Rangan, and Philip Schniter. Inference in deep networks in high di-
mensions. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 1884-1888.
IEEE, 2018.
Marylou Gabria Andre Manoel, Clement Luneau, Jean Barbier, Nicolas Macris, Florent Krzakala,
and Lenka Zdeborov叁 Entropy and mutual information in models of deep neural networks. arXiv
preprint arXiv:1805.09785, 2018.
Anna C Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, and Honglak Lee. Towards understanding the
invertibility of convolutional neural networks. arXiv preprint arXiv:1705.08664, 2017.
Raja Giryes, Guillermo Sapiro, and Alexander M Bronstein. Deep neural networks with random
gaussian weights: a universal classification strategy? IEEE Trans. Signal Processing, 64(13):
3444-3457, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? arXiv
preprint arXiv:1801.03744, 2018.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
arXiv preprint arXiv:1803.01719, 2018.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the selection of initialization and activa-
tion function for deep neural networks. arXiv preprint arXiv:1805.08266, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. Science, 313(5786):504-507, 2006.
Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing
algorithms, with applications to spatial coupling. Information and Inference: A Journal of the
IMA, 2(2):115-144, 2013.
Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann machines and
deep belief networks. Neural computation, 20(6):1631-1649, 2008.
Bo Li and David Saad. Exploring the function space of deep-learning machines. Physical Review
Letters, 120(24):248301, 2018.
Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic
feedback weights support error backpropagation for deep learning. Nature communications, 7:
13276, 2016.
Cosme Louart, Zhenyu Liao, and Romain Couillet. A random matrix approach to neural networks.
Ann. Appl. Probab., 28(2):1190-1248, 04 2018.
Andre Manoel, Florent Krzakala, Marc Mezard, and Lenka Zdeborovd. Multi-layer generalized
linear estimation. In 2017 IEEE International Symposium on Information Theory (ISIT), pp.
2098-2102. IEEE, 2017.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences, 07 2018.
12
Published as a conference paper at ICLR 2019
Guido Montufar and Nihat Ay. Refinements of universal approximation results for deep belief net-
works and restricted boltzmann machines. Neural COmputatiOn, 23(5):1306-1319, 2011.
Ali Mousavi, Ankit B Patel, and Richard G Baraniuk. A deep learning approach to structured signal
recovery. In 2015 53rd Annual AllertOn COnference On COmmunicatiOn, COntrOl, and COmputing
(AllertOn), pp. 1336-1343. IEEE, 2015.
Thanh V Nguyen, Raymond KW Wong, and Chinmay Hegde. Autoencoders learn generative linear
models. arXiv preprint arXiv:1806.00572, 2018.
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In Ad-
vances in Neural InfOrmatiOn PrOcessing Systems, pp. 2637-2646, 2017.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Advances in neural infOrmatiOn
prOcessing systems, pp. 4785-4795, 2017.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. In Advances in neural infOrma-
tiOn prOcessing systems, pp. 3360-3368, 2016.
Akshay Rangamani, Anirbit Mukherjee, Ashish Arora, Tejaswini Ganapathy, Amitabh Basu, Sang
Chin, and Trac D Tran. Sparse coding and autoencoders. arXiv preprint arXiv:1708.03735, 2017.
Galen Reeves. Additivity of information in multilayer networks via additive gaussian noise trans-
forms. In 2017 55th Annual AllertOn COnference On COmmunicatiOn, COntrOl, and COmputing
(AllertOn), pp. 1064-1070. IEEE, 2017.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error.
arXiv preprint arXiv:1805.00915, 2018.
David E Rumelhart and David Zipser. Feature discovery by competitive learning. COgnitive science,
9(1):75-112, 1985.
Benjamin Scellier, Anirudh Goyal, Jonathan Binas, Thomas Mesnard, and Yoshua Bengio. Extend-
ing the framework of equilibrium propagation to general dynamics. 2018.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. arXiv preprint arXiv:1611.01232, 2016.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv
preprint arXiv:1805.01053, 2018.
Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient
descent. 2018.
Haim Sompolinsky, Andrea Crisanti, and Hans-Jurgen Sommers. Chaos in random neural networks.
Physical review letters, 61(3):259, 1988.
Ilya Sutskever and Geoffrey E Hinton. Deep, narrow sigmoid belief networks are universal approx-
imators. Neural cOmputatiOn, 20(11):2629-2636, 2008.
Roman Vershynin. IntrOductiOn tO the nOn-asymptOtic analysis Of randOm matrices, pp. 210-268.
Cambridge University Press, 2012.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. JOurnal Of Machine Learning Research, 11(Dec):3371-3408, 2010.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.
13
Published as a conference paper at ICLR 2019
Greg Yang and Sam S Schoenholz. Deep mean field theory: Layerwise variance and width variation
as methods to control gradient explosion. 2018.
Greg Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In
Advances in neural information processing systems, pp. 7103-7114, 2017.
ENCODER
DECODER
dimension: nL
dimension: nL-ι
dimension: n`
dimension: n`-i
dimension: nι
dimension: no
dimension: n°
Figure 5:	Schematic diagram of the weight-tied autoencoder as described in Section 2.1. Left: the
encoder. Center: the decoder. Right: dimensions of the corresponding vectors.
14
Published as a conference paper at ICLR 2019
A Proof of Theorem 1
In the following, we give an outline of the proof of Theorem 1, and the complete proof. First, we
start with a few notations and definitions.
A. 1 Definitions and Notations
We recall the setting in Section 2.1 (see also Fig. 5 for a schematic diagram of the autoencoder). We
define [g` : Rn'-1 → Rn'-1 }t=r L inductively as follows:
gL (U) = W>夕L (WLσL-1 (u) + bL) + vL,
g` (u) =	W>夕' (g'+ι	(W'σ'-i (u) +	b`)) + v`,	' = L - 1,..., 1.
It is easy to see that X`	= g` (x'-ι)	for ' = 1,..., L.	Essentially	g`'s represent the	autoencoding
mappings computed by the inner layers.
We use bold-face letters (e.g. x, W) to denote vectors or matrices. We use C throughout to denote
an arbitrary (and immaterial) constant that is independent of the dimensions. For two vectors x
and y, hx, yi denotes their inner product. We use I for the identity matrix, and In to emphasize
its dimensions n X n. We use ∣∣∙k to denote the usual Euclidean norm, ∣∣∙k∞ the infinity norm,
and kM k2 the maximum singular value of a matrix M . For a sigma-algebra F and two random
variables X and Y , X |F =d Y means that for any integrable function φ and any F -measurable
bounded random variable Z, E {φ (X) Z} = E {φ (Y ) Z}. We write X =d Y when F is the trivial
sigma-algebra.
A sequence of functions φn : Rn 7→ R is uniformly pseudo-Lipschitz if there exists a constant C,
independent of n, such that for any x, y ∈ Rn ,
(IJkxkJ kyk ʌ kx - yk
lφn (X) - φn (U)I ≤ C 1 + ~~Γ + -^=——^-.
nn n
A sequence of functions φn : Rn 7→ Rn is uniformly Lipschitz if there exists a constant C,
independent ofn, such that for any x, y ∈ Rn, kφn (x) - φn (y)k ≤ C kx - yk . These definitions
are adopted from Berthier et al. (2017). For two sequences of random variables Xn ∈ R and Yn ∈ R
indexed by n, we write Xn ' Yn to mean that Xn - Yn → 0 in probability. The same meaning
holds when Yn is deterministic. For two sequences of random vectors Xn ∈ Rn and Y n ∈ Rn
indexed by n, We write Xn = Yn if for any sequences of uniformly PSeUdO-LiPSchitz test functions
φn : Rn 7→ R, φn (Xn) ' E {φn (Yn)} (and hence in this context, we do not need Xn and Yn
to be defined on a joint Probability sPace).
A.2 Outline of the proof of Theorem 1
We state several results that are key to Prove Theorem 1.
Proposition 5. Consider the asymptotic setting n → ∞, with some sequence m = m (n) such that
m/n → α > 0 as n → ∞. Let 夕: R → R and σ : R → R be LipSChitz continuous scalar
functions. Consider a sequence of uniformly Lipschitz functions g : Rm 7→ Rm, and sequences of
vectors b ∈ Rm, v ∈ Rn, u ∈ Rn such that
也≤c, 0<c≤⅛r≤°,固≤C
m	nn
for all sufficiently large n. Let us define
f (u) = W 4 (g (W σ (u) + b)) + v,
where W ∈ Rm×n with Wij 〜N(0, σW/n) i.i.d. for σw > 0. Let
Y = e≡ 1 —ɪ hττz,中(g(T2 + b))i], P = E≡ 1 — kψ(g(Tz + b))『],τ2 = σWkσ(U)k ,
mτ2	m	n
for z 〜N (0, Im). Then:
f (U)
=ασWγσ (u) +
ασW2 ρz + v ,
where z = N (0, In) (independent ofU, b and v). Here the randomness is solely due to W.
15
Published as a conference paper at ICLR 2019
A corollary follows from this proposition.
Corollary 6. Consider the same setting as in Proposition 5. Further assume that b 〜 N 0, σb2Im ,
V 〜N(0, σV I n) and U 〜N (0,σU I n), independent of each other and of W. Then:
f (U) = ασWγσ (b“z) 十 小ασWP + σ2z',
for z, z0 〜N (0, In) independently. In particular, for φ : R → R Lipschitz continuous,
1 kφ (f (u))k2 ' Ez1,z2
n⅛ hu,φ (f (U))i' Ez1,z2
φ (ασWγσ (b“zi) + Jασ%P + σVz2) },
σ~zιφ (ασWγσ 0"zi) + Jασ%P + σVz2) },
(4)
(5)
in which
Y = E {m⅛ E(g (T 汕〉
ρ = E≡ ∣m1 kψ (g(T汾)『), T = σW Ezi {σ (σuzι)2}+σ2,
where Z 〜N (0, Im), and zι, z? are independently distributed as N (0,1).
In a nutshell, the proposition and the corollary consider a random weight-tied “autoencoder” with
a single hidden layer, with a mapping g in the middle. Since g is not a separable function (i.e., g
does not apply entry-wise), this is different from the usual shallow autoencoder, a case that has been
investigated in Pennington & Worah (2017); Louart et al. (2018) with techniques and objectives
different from ours. By understanding this structure, one can understand the random weight-tied
multi-layer autoencoder. Indeed, for each ',the proposition applies with U being X'-ι, f (u) being
X` and g being g'+ι. In other words, this studies the mapping g`. One can start with gL, then
progressively move to outer layers gL-1, gL-2, etc, and hence analyze the autoencoder completely
by repeating the same procedure L times. We note that in doing so, at each step, one requires certain
information about the inner layers to perform calculations for the outer layers, and this is worked
out in Corollary 6 via a simple recursive relation. In particular, the left-hand sides of Eq. (4) and Eq.
(5) play the role of P and γ of the outer layer, whereas their right-hand sides involve P and γ from
the inner layers. It is also important to note that the assumption that the weights at different layers
are independent is crucial in making U, g and (W , b, v ) to be independent, allowing the proposition
to be applicable at all steps. This is the idea behind the proof of Theorem 1.
We quickly mention the key proof technique for Proposition 5. The main technical challenge in
working with the weight-tied structure W>h (W U), for some h : Rm 7→ Rm and W ∈ Rm×n
Gaussian, is that whereas y = WU is Gaussian with zero mean thanks to independence between
U and W, W>h (y) is not since y is correlated with W. It is observed in Bolthausen (2014)
that conditioning on a linear constraint y = WU (or the sigma-algebra F generated by y and U),
one has that W is distributed as a conditional projection component plus an independent Gaussian
component:
W |F = E {W |F} + W P⊥,
where Pu⊥ is an appropriate projection. Here E {W |F} is what propagates the information about U.
In addition, W is Gaussian and independent of F, and hence exact calculations can be then worked
out. We note that the assumption W is Gaussian is crucial for this identity to hold.
A.3 Proof of Proposition 5
We state the Gaussian Poincare inequality, which will be used multiple times throughout the proof.
We remark that the use of the Gaussian Poincare inequality is unlikely to lead to a tight non-
asymptotic result, but is sufficient for our asymptotic analysis.
Theorem 7 (Gaussian Poincare inequality). For z 〜 N (0,σ2In) and φ : Rn → R con-
tinuous and weakly differentiable, there exists a universal constant C such that Var {φ (z)} ≤
Cσ2E {∣∣Vφ (Z)『}.
Now we are ready for the proof.
16
Published as a conference paper at ICLR 2019
Step 1. We perform Gaussian conditioning. Let y = Wσ (u). Let F be the sigma-algebra
generated by y and u. Conditioning on F is equivalent to conditioning on the linear constraint
y = Wσ (u). Following Bayati & Montanari (2011), we have
W |F = W Pσ(u) + W P⊥(u),
where W = W and is independent of F, Pσ(u) = σ (U) σ (u)> / ∣∣σ (U)『the projection onto
σ (u), and P⊥u = I-Pσ(u) the corresponding orthogonal projection. As such, since 夕(g (y + b))
is F -measurable,
f (U)IF = ∣σ(U)∣2 hWσ(U),ψ(g(y + b))iσ(U) + P⊥u)W>ψ(g(y + b)) + V
=∣σ (U)『hy, ψ (g (y+b))i σ (U)+P⊥u)W >。(g (y+b))+v.
For a sequence of uniformly pseudo-Lipschitz functions φn : Rn 7→ R:
φn (f (U)) |F =d φn
T产σ (U)+P⊥(u)W % (g (y+b))+v
(6)
Up to this point, there is no need for the asymptotics n → ∞. The rest of the proof focuses on Φn.
Step 2. We show that τ, γ and ρ are uniformly bounded as n → ∞. This is trivial for τ , and we
note τ > 0. Consider ρ. We have for any r ∈ Rm,
√1=闷(g (r + b))k ≤√U 闷(g (r + b))-中(g (0))k + √1=	(g (0))k
mm	m
C1
≤√mkr+bk + √m 闷(g(0))k
CC1
邑〒 krk + -ɛ kb∣ + -ɛ M (g (0))k
mmm
C
邑〒 krk+ C	(7)
m
for sufficiently large m. It is then easy to see that ρ is uniformly bounded. Regarding γ,
∣γ∣ T ≤ EL kzk∣L (g(τ2 + b))k} ≤{m kz『} P = √ρ,
by Cauchy-Schwarz inequality. Therefore γ is also uniformly bounded.
Step 3.	We analyze the first term in Φn. Notice that y = τz for some W 〜 N (0, Im). In addition,
the mapping y →(y,夕(g (y + b))i /m is uniformly PSeUdO-LiPSChitz, by noticing that
m hyι,ψ (g (yι+b))i—m hy2,ψ (g (y2+b))i
≤ -1 kψ(g(yι + b))k kyι — y2k + — ky2k kψ(g®i + b))一中(gIy2 + b))k,
along with the fact that y → 夕(g (y + b)) is uniformly Lipschitz, and Eq. (7). As such, by Theorem
7,
Var { m hy,φ (g (y + b))i
m
1+√m kyk +
≤ 3Cτ2E (1 + 1 kyk2 + ɪ kbk
m
m
which tends to 0 as n → ∞. By Chebyshev’s inequality, we thus have
-1 (y, Ψ (g (y + b))i ' E [ -1 hy,φ (g (y + b))i∣ = E 1 — (τ z, φ (g (τ Z + b))i ∖ = γτ2.
m
m
m
17
Published as a conference paper at ICLR 2019
Step 4. We analyze the second term in Φn . We have:
p⊥(u)Ww >ψ (g (y + b)) = WW >ψ (g (y + b))-
k⅛⅛ Dσ (U)，W %(g (y+b))E.
(8)
Note that the mapping y → ml ∣∣夕(g (y + b))『is uniformly PseUdo-LiPschitz by a similar argu-
ment. Hence again by Theorem 7, Var
{表闷 (g (y + b))『}
O (1/m), which yields
m 陷 (g (y+b川2 ` E{ m 陷 (g (y+b 川 2}=E{ m 闷 (g (T 三十一『)=ρ.
Recall that W is independent of F, and as such, there exists a random variable z0 〜 N (0,1)
independent of y such that
1 Dσ (U)，W % (g (y+助〉=n√nkσ (U)Hk (g (y+b))k σWz0'1kσ (U)k √αρσW z0,
which converges to 0 in probability, where we have used the fact that ρ is uniformly bounded from
Step 2. Furthermore, there also exists a random vector Z 〜N (0, In) independent of F such that
>1
W φ (g (y + b)) = -J= kφ (g (y + b))k σwz.
(9)
Step 5. We finish the proof. From the definition of uniform pseudo-Lipschitz functions and Eq.
(8) and (9), we obtain:
Φn - φn (ασWγσ (u) + q∕ασ∣WρZ + V) |
≤ C 1+
lhy,ψ(g(y + b))i| l 2 l∣σ(u)k l
+ασWWk +
+ JασWP √zk +
∣∣vW∣∣ 比(g(y+则
×
hy,ψ(g(y + b))i	2	∣ kσ (u)k l
kσ (u)∣2	-	W γ∣V^ +
k= ιι^ (g (y + b))k- √αρ∣ σw z-γ=
+ lH⅛∣1 Dσ (U) ,W > 中 (g (y + b))E
Here notice that ||修。)∣∣ = 1, and as a standard fact from the random matrix theory, ∣ WW∣∣ ≤
c α, σW2 a constant with high probability (see e.g. Vershynin (2012)). Furthermore, √n 闫 ≤ C
with high probability due to the law of large numbers. Combining with the facts from Step 2, 3 and
4, it is then easy to see that
Φn - φn
ασWγσ (U) + ∕ασ%PZ + v) | ` 0.
(10)
Finally, since γ and P are uniformly bounded from Step 2, it is easy to show that the mapping
Z → φn 卜σWγσ (u) + q∕ασWPZ + V)
is uniformly pseudo-Lipschitz. Hence by Theorem 7,
Var ∣φn QσWγσ (u) + ∕ασ%PZ + V) } = O (；),
which yields
φn QσWγσ (u) + q∕ασWPZ + V) ` E {φn QσWγσ (u) + ∕ασWPZ + V) }.
Together with Eq. (6) and (10), this completes the proof.
18
Published as a conference paper at ICLR 2019
A.4 Proof of Corollary 6
By Proposition 5, for any sequence of uniformly pseudo-Lipschitz φn : Rn 7→ R:
φn (f (u)) ' Ez φn
ασWγσ (U) +
α ασWPz + V
for z 〜N (0, In) independent of u, V and b, in which
Y = Y (τ,b) = EW
hτz,φ (g (T2 + b))i
ρ = P(τ,b) = Ew [-1 k2(g (T2 + b))k2
m
τ2 — τ2 (U) — σ2 kσ (U)『
T — T (U) — σW	,
n
for W 〜N (0,1m) independent of U and b. It is easy to see that T2 ' T2 — T2 - σb2. For any
t ∈ (0, ∞), the mapping b → Y (t, b) is uniformly PseUdo-LiPschitz, and hence by Theorem 7,
Varb {γ (t, b)} — O (1/m), which yields Y (t, b) ` Eb {γ (t, b)}. Recall T is independent of b.
As such, γ (t, b) ` Eb {γ (t, b)}. Furthermore, the mapping t → Eb {γ (t, b)} is continuous and
hence Eb {γ (τ, b)} ` Eb {γ (τ, b)}. Performing a similar argument for P (τ, b), We thus have:
P ' P,
Y ' Ez,b
hτW,2(g (τz + b))i
We note by Stein’s lemma,
Ew,b{<τW,夕(g (τz + b))i} — τ2EjS《div (夕。g) ( JT2 + σ2W
Ezb {hb” (g (τ Z + b))i} — σ2Es《div (夕。g) Jτ2 + σ2z
Also notice that
EZ,b{hτw + b” (g (tw + b))i} — EZ {+ σ2z,P(g HT2 + σ2W)),}.
Direct algebras then yield
EZ,b {m⅛ hτ W，°(g (τ w+b))i
and hence Y ` 7. Therefore,
φn (f (u)) ′ Ez {φn (ασWY (u) + ^oσ2wPzZ + V) ≡ ≡ h (u, v).
The mapping (U, V) 7→ h (U, V) is uniformly pseudo-Lipschitz and hence by Theorem 7,
Varu,v {h (U, V)} — O (1/n), which implies
h (U, V) ' Eu,v,z	φn
oσW2 Yσ (U) +
oσW2 Pz + V
oσW2 Yσ (σu z) +
oσW2 P + σv2 z0
which follows from the fact that U, V and z are independent and normally distributed. The first
claim is hence proven.
Eq. (4) follows immediately from the above. The proof of Eq. (5) is similar and hence omitted.
19
Published as a conference paper at ICLR 2019
A.5 Proof of Theorem 1
The following lemma states that, roughly speaking, g` is uniformly Lipschitz (indexed by n) with
high probability. First, recall that W` = W` (n) is indexed by n, and one can easily construct a
joint probability space on which the sequence {W` (n)}n≥ι is defined. The exact construction is
immaterial, so long as for each n, the marginal distribution satisfies the setting in Section 2.1. We
work in this joint space in the following.
Lemma 8. For each ' = 1,...,L, there exists a finite constant e` > 0 such that P {EN} → 0 as
N → ∞, where the event EN is defined as
EN = ( sup	kg'	(u1)g'	(u2)k >	e`) ,	D' = {n>N, uι, u ∈ Rn'-1,	uι	= u2}.
(n,u1,u2)∈D'	lluι - u2k
Proof. Consider ' = L. Recall that gL (U) = W>夕L (Wlol-∖ (U) + 6工)+ vl. Then for any
u1 and u2,
kgL (Ui) — gL (U2)k = ||W>ΨL (WLσL-1(Ul) + bl) - W>ψL (WL©L-1(U2) + bL)||
≤ kWLk2 ILl (WLσL-i (uι) + bl) — 中L (Wlcl- (u2) + bι)k
(i)
≤ C kW Lk2 kW LσL-1 (U1) — W LσL-1 (U2)k
≤ C kWLk22 kσL-1 (U1) — σL-1 (U2)k
(ii)	2
≤ C kWL k22 kU1 — U2 k ,
where steps (i) and (ii) are because 夕L and σι-ι are Lipschitz. It is a standard fact in the random
matrix theory that, for each ' = 1,…,L, there exist finite constants n` = n` (a`, σW,, > 0 and
c' = c' (α',σ%') > 0 such that P {∣∣W ` k2 > n`j ≤ e-c'n (see e.g. Vershynin (2012)). As such,
using this fact for ` = L, by the union bound,
P {EL} ≤ X e-cLn ≤ ∕∞ e-cLudu = [e-cLN,
n>N	u=N	cL
which proves the claim for ` = L. To see the claim for general `, we have from a similar calculation:
kg` (Ui) — g` (u2)k ≤ k W'k2 kg'+i (W'σ'-i (Ui) + b`) — g'+i (W`o`-i (u2) + b`)k
≤ C L-'+i (∏kW r k2) kUi - U2k .
r='
Therefore,
sup	kg' (UI)- g' (U2)k ≤ CL-'+i YYY ηr
(n,u1,u2)∈D'	ku1 — u2 k	∖r='
with probability at least
LL
1 - X X e-crN ≥ 1---e-c/N,	c0 = min c',
乙乙	一	c0	,	i≤'≤L ',
n>N r='
by the union bound. This proves the claim.	□
Lemma 9. For each ' ≥ 0, τ'+ι and T'+ι are finite and strictly positive. Furthermore,
n7 kσ'(x')k2 ' τ'+∕σW,'+i, and x'+i = τ'+iz for Z 〜N (0,In'+ι).
Proof. We prove the lemma by induction. The claim is trivially true for ` = 0 by assumption.
Assume the claim for some ` ≥ 0. Due to independence, conditioning on x' ,
d
x'+i|x'
j。*'+1 kσ' (x')k2 + σ2,'+iz,
20
Published as a conference paper at ICLR 2019
where Z ~ N(0,1nc+1) independent of xg. Since 嵩 ∣∣σ` (xg)∣∣2 ` τ'2+1∕σ2z,'+1, we then have
x'+ι = √τ2+ι + σ2,'+ιz = τ'+ιz,
which implies
六"ι ")∣2→ Ez0 {σ`+1 (Ez0)2}
for z0 ~ N (0,1). By the induction hypothesis,方+ι > 0. Hence by assumption, the right-hand side
is finite and strictly positive. One also recognizes that it is equal to 卷2∕σW'+2. Since σw,`+2 is
strictly positive and finite, so is 及+2, and hence so is T'+2. ThiS completes the proof.	口
We are now ready for the proof of Theorem 1. Claim (a) of the theorem, which follows directly from
Lemma 9, is just a forward pass through the encoder and hence is the same as in the case of random
feedforward networks (without weight tying) Poole et al. (2016).
For ' = 2,..., L, let H' denote the following three claims:
(1):
(2)：
⑶：
X` = ɑ`σW,'Y'+ισ'-i (方-1Z1) + J。'。％,'P'+1 + */2,
-ɪ I32-i (g` (Te-IZ`-I))II2 ` p`
n`-i
----〈方-1 五-i,夕'-i (g` (方-1 幻-1)))' γ',
n'-iT2-ι
for z1, Z2,〜N(0,1 ng_i) independently, and z`-i 〜N(0,1 n`-j independent of g`, with the
understanding that gL+1 is the identity. We prove them by induction, and Claim (b) then follows
immediately. We first note that with high probability, for any ', 嵩 ∣∣b'∣2 and n1γ ∣W'∣∣2 are
bounded by the law of large numbers. Consider Hl. Note that xl-i is independent of gL, and that
η1γ ∣∣σL-ι (XL-1)『converges to a finite non-zero constant in probability by Lemma 9. Hence
by Corollary 6, with g being the identity, f being gL, U being xl-i (recalling XL = gL (XL-I))
and φ being 夕l-i, and the fact xl-i = Tl-iZ'-i from Claim (a), HL is proven.
Assuming	H'+ι	for some	',	we prove	H'.	Recall that	ge+ι	is independent of	θ`	=
{W`, b`, v`, X'-ι, Z'-i), and g` is independent of X'-ι and i⅛-ι. Also from Claim (a), X'-ι =
T'-1Z'-1. Consider some N ∈ N and let ENI denote the event as defined in Lemma 8. On the
event -ENi (the complement of EJ+ι), by Corollary 6, with respect to the randomness of θ`,
X` = ασW,'Yσ-i (方-1Zi) + yaσ2z,`P + σV,'Z2,
----I2'-i (g' (t'-i2'-i)) ∣∣2 ' Ezi,.
n`-i
3'-i"'-i (g` (方-1 幻-1))) ' Ezi,.
n`-i
{2'-1 (ασW,'Yσ'-i (方-izι) + JασW,'P + 脸Zj
{ziφ (αgσW,'γσ'-i (T'-1Z1) + {。2飞钞 + / j`的)]
in which
Y = Ez0 ∣nl∑2 EZ'"' (。'+1 (T'z'))〉},
p = Ez0 {5 忻(g'+1 (TgZO)) ∣2} .
Note that here Y and P are functions of g- and hence random, and z0 = N (0,1n`) independent
of g'+1. On the event -Εg+1, with the fact that t` is finite and non-zero by Lemma 9, we have the
mapping
ZZ → -To 品Z,夕' (g'+1 (TZZ))) ≡ h (Z)
n'T2
is uniformly pseudo-Lipschitz, and hence applying Theorem 7, one obtains that
Varz0 {h (zz)} = O 1	,
21
Published as a conference paper at ICLR 2019
with the right-hand side independent of g'+ι for n > N. Consequently, due to Lemma 8 and
Chebyshev’s inequality, for any > 0,
P {|h (ZO)-Y| > e, -E+ι} = E nPz0 {|h (ZO)-YI > e} 1-E'+1}
≤ E { e2Varz0 {h (z0)} I-Ej+1 } = On (1)(1* (1)) = On (1),
where On (1) → 0 as n → ∞. On the other hand, since z0 is independent of g'+ι, invoking H'+ι,
P {|h (z0) — Y'+ι∣>e} = on (1).
As such,
P {∣Y - Y'+ι∣ >e}≤ P {|h (ZO)-Y'+1∣ > e} + P {∣h (ZO)-YI > e}
≤ P {∣h (ZO)-Y'+1∣ >e} + P {|h (ZO)-Y I >e, /J + P {E+1}
= On (1) + On (1) + ON (1) .
Letting n → ∞ then N → ∞, we then have Y ` Y'+ι. Similarly, We also have P ` P'+ι. It is then
easy to deduce h`, recalling the definitions of Y' and p`.
The claim that Te-ιZι can be replaced with X'-ι in the expression for X` in Claim (b) can be
recognized easily by doing the same replacement in the proof of Corollary 6 and the above proof.
The proof of Claim (c) is similar. We omit these repetitive steps.
B Numerical verification of Theorem 1
We perform simple simulations to verify Theorem 1 at finite dimensions. In particular, we simulate
a random weight-tied autoencoder, as described in Section 2.1, with L = 50, σW` = 2.312, σ2 ` =
0.211, σV ` = 0,夕' = σ' = tanh, identity 夕0 and σ0, ɑ` = 1, and consequently n` = n0, for
an input x ∈ Rn0 whose first half of the entries are +1 and the rest are -1. Then we compute the
following:
(X',σ'-i (x'-i)i
α'σW,` kσ'-i (x'-i)k2
ρ'+ι
1
^^^Ξ2-
α' σW,'
kX ` k2
α2σW,'Y+
n`-i
kσ'-1 (χ'-i)k2 - σV,'
—
We also compute {y', P'}'=2 l+i as in Section 2.2, and Y and P as in Section 3.1. Theorem 1
predicts that ^' ` Y' and p` ` p`. Section 3.1 also asserts that Y' ≈ Y and p` ≈ P for '《L. Fig.
6 shows the results for n0 = 500 and n0 = 2000. We observe a quantitative agreement already for
n0 = 500, and this improves with larger n0 .
Next we verify the normality of the variation component in X. We compute
2
Z _ x - α1σW,1Y2X
Ja1σW,ιρ2 + σV,ι
Its empirical distribution should be close to N (0, 1), in light of Theorem 1. We make this compari-
son in Fig. 7, and again observe good agreement already for n0 = 500.
Finally we re-simulate the autoencoder with different distributions of the weights W'. In particular,
we try with the Bernoulli distribution, the uniform distribution, and the Laplace distribution, with
their means and variances adjusted to zero and σW'∕n'-ι respectively. The results for no = 2000
are plotted in Fig. 8 and 9. We observe good quantitative agreements between the simulations for
these non-Gaussian distributions and the prediction as in Theorem 1, although the theorem is proven
only for Gaussian weights.
22
Published as a conference paper at ICLR 2019
I
Figure 6: The agreement among γ', γ' and γ, and among p`, p` and ρ, for ' = 2,…,51 and
Gaussian weights. The setting is described in Appendix B. We take a single run for the simulation
I
of the autoencoder. Here n0 = 500 (left) and n0
2000 (right).
4 2 O-2
ZsUBn
-4
-4	-2	0	2	4
Standard GaUssian quantiles
4 2 O-2
Bs-Ifuen°
-4
-4	-2	0	2	4
Standard GaUssian quantiles
Figure 7: Quantile-quantile plots for the empirical distribution of z, described in Appendix B, versus
the standard Gaussian distribution. Here n0 = 500 (left) and n0 = 2000 (right).
i
Figure 8:	The agreement among Y', γ' and γ, and among p`, p` and ρ, for ' = 2,..., 51, for different
distributions of the weights. The setting is described in Appendix B. We take a single run for the
simulation of the autoencoder. Here n0 = 2000.
-4
Bernoulli
4 2 O-2
Z。考石=
-2	0	2
Standard GaUssian quantiles
Uniform
4 2 O-2
Z。-考石=0
-2	0	2
Standard GaUssian quantiles
-4
LaPIaCe
4 2 O-2
Z。考石=
-2	0	2
Standard GaUssian quantiles
Figure 9:	Quantile-quantile plots for the empirical distribution of z, described in Appendix B, versus
the standard Gaussian distribution, for different distributions of the weights. Here n0 = 2000.
23
Published as a conference paper at ICLR 2019
C ON THE FIXED POINT EQUATIONS OF γ AND ρ
C.1 COMPUTATION OF γ AND ρ
Computing γ and ρ as from Eq. (2) and (3) amounts to evaluating double integrals. For simple
夕，such as the ReLU, the integrals f (a, b)
E {φ (a + bz)} and gφ (a, b)
E {夕(a + bz)2 },
for Z 〜 N (0,1), can be calculated in closed forms. In such cases, one can make reduction to
one-dimensional integrals:
Y = ɪE {τzιfw (ασWγσ (τzι) , q∕ασWρ) } , ρ = E {g^ (ασ%γσ (τzι) , q∕ασWρ) }.
We proceed with computing γ andρ as follows. From a random initialization γ(0) > 0 and ρ(0) > 0,
one iteratively updates γ(t+1) = G γ(t), ρ(t) and ρ(t+1) = R γ(t) , ρ(t) , and stops when the
incremental update is negligible or the number of iterations exceeds a threshold. Upon convergence,
this procedure finds a stable fixed point.
C.2 PROPERTIES OF γ AND ρ
We prove several properties of γ and ρ. We recall G (γ, ρ) and R (γ, ρ) from Eq. (2) and (3) for
ease of reading:
G(γ,ρ) = JE 卜zιP(βγσ (Tzι) + pβpz2)} = βγE {20 (βγσ (τz1) + pβpz2) σ0 (Tzι)},
R (Y, P)= E ［夕(βYσ (TZI) + Pβpz2) ∖ ,
for z1,z2 〜 N (0,1) independently, where (i) is due to Stein,s lemma. Recall that the fixed points
equations are Y = G (Y, ρ) and ρ = R (Y, ρ). We also recall that β = ασW2 > 0. In light of
Remark 3, We will consider Lipschitz continuous, non-decreasing 夕 and σ, so that Y ≥ 0 (and of
course, ρ ≥ 0). We will study these equations, first by stating some propositions, then discussing
their implications, although we caution that the link between the propositions and the suggested
implications is not entirely rigorous. All the proofs are deferred to Section C.3. We note that while
the discussions concern with ReLU or tanh activations, the propositions apply to broader classes of
functions.
In the following, when we say an initialization for Y and ρ, we mean either an initialization in the
context of an iterative process to find the fixed points as in Section C.1, or YL+1 and ρL+1 as in
Section 2.2 in the context of autoencoders with L → ∞. We also say Y is a fixed point, without
referencing to ρ, to mean that it is only a fixed point of Y = G (Y, ρ) for a given ρ, and similarly for
ρ. When we mention both Y and ρ as a fixed point, we mean a fixed point to both Y = G (Y, ρ) and
ρ = R (Y, ρ). We will use ∂ukf to denote the kth-order partial derivative of f with respect to u.
C.2.1 THE CASE OF RELU 夕
The following result is exclusive to ReLU 夕.
Proposition 10. Consider that 夕 is the ReLU and σ is Lipschitz continuous and non-decreasing:
(a)	Y = 0 and ρ = 0 is a fixed point. Furthermore, at Y = 0 and β 6= 2, the mapping
ρ 7→ R (0, ρ) admits ρ = 0 as the only fixed point, which is stable if β < 2 and unstable if
β > 2. Also at Y = 0 and β = 2, any ρ is a stable fixed point.
(b)	Assume σ is positive on a set of positive Lebesgue measure. If Y → +∞, it must be that
ρ → +∞.
(c)	Consider τ ∈ (0, ∞). Assume σ is non-zero on a set of positive Lebesgue measure, and
that σ (u) = 0for all u ≤ 0. Then no Y > 0 is a stable fixed point. Furthermore,
•	if βE {σ0 (TZ1)} ≤ 1, there is only one fixed point at Y = 0, which is stable;
•	if βE {σ0 (τzι)} ∈ (1, 2), there are two: one at Y = 0, which is stable, and the other
at Y > 0, which is unstable;
24
Published as a conference paper at ICLR 2019
•	if βE {σ0 (Tzι)} ≥ 2, there is only one fixed point at Y = 0, which is unstable.
(d)	Assume σ is non-zero on a set of positive Lebesgue measure, and that σ is an odd function.
Then for any ρ, the mapping γ 7→ G (γ, ρ) is a straight line through the point (0, 0).
Furthermore, if βE {σ0 (Tzι)} = 2, there is only one fixed point at Y = 0, which is stable
for βE {σ0 (Tzι)} < 2 and unstable for βE {σ0 (Tzι)} > 2; if βE {σ0 (Tzι)} = 2, any Y
is a fixed point.
(e)	Assume σ is odd, and consider β > 2. Given Y ≥ 0, we have R (Y, ρ) > ρ for all ρ ≥ 0.
The proposition suggests the following picture. First consider σ is ReLU. Since E {σ0 (Tzι)} =
P (TzI ≥ 0) = 0.5, we have two phase transitions at β = 2 and at β = 4. In particular, based on
Proposition 10:
•	When β < 2, with any initialization, we have convergence to Y = 0 and ρ = 0. This is
based on Claim (a) and (c).
•	When β ∈ (2, 4), with certain initializations, we have convergence to Y = 0 and divergence
to ρ = +∞; with certain other initializations, we have divergence to Y = +∞ and ρ =
+∞. This excludes a special initialization at the unstable fixed point, which is a singleton
and essentially a rare case. This is based on Claim (a), (b) and (c).
•	When β ≥ 4, with any non-zero initialization, we have divergence to Y = +∞ and hence
ρ = +∞. This is based on Claim (b) and (c).
Now we consider σ is tanh. Let βo = βo (T) = 2/E{σ0 (τzι)}, and it is easy to see that βo > 2
since σ = tanh. The following picture is then expected:
•	When β < 2, with any initialization, we have convergence to Y = ρ = 0. This is based on
Claim (a) and (d).
•	When β ∈ (2, β0), with any non-zero initialization, we have convergence to Y = 0 and
divergence to ρ = +∞. This is based on Claim (a) and (d).
•	When β > β0, with any non-zero initialization, we have divergence to Y = +∞ and
ρ = +∞. This is based on Claim (b) and (d).
•	When β = β0, we have that Y is unchanged from the initialization. Since β0 > 2, we then
have divergence to ρ = +∞. This is based on Claim (b) and (e).
One crucial property of the ReLU is that it is unbounded at infinity and its derivative at infinity is
bounded away from zero. This allows Y and ρ to grow to infinity. This is a stark contrast to the case
夕 is bounded, for instance,夕=tanh as we shall see.
C.2.2 THE CASE OF tanh 夕
We state a result that is relevant to 夕=tanh.
Proposition 11. Assume that 夕 thrice-differentiable with 夕(0) = 0,夕0 (0) = K, and Udk)L ≤ C
for k = 0,..., 3, where 夕(k) is the k-th derivative of φ. Assume σ is Lipschitz continuous, non-
decreasing. Then:
(a)	Y = 0 and ρ = 0 is a fixed point. Furthermore, assuming that σ is non-zero on a set of
positive Lebesgue measure,夕 is non-zero almost everywhere and τ ∈ (0, ∞), we have if
ρ = 0, it must be that Y = 0; in other words, ifY > 0, then ρ > 0.
(b)	If 夕 is bounded, then 0 ≤ P ≤ C, and ∣γ| ≤ C/T.
(c)	Given Y = 0, if β ‹ 1 /κ2, P = 0 isa stable fixed point, and if β › 1 /κ2, P = 0 is unstable.
(d)	Consider T ∈ (0, ∞). Assume that
•	σ is positive on a set of positive Lebesgue measure, and E {σ0 (τz)} > 0 for z 〜
N (0, 1),
•	either
25
Published as a conference paper at ICLR 2019
-case 1: σ (U) = 0 for U ≤ 0, and ∆ρ (u, t) < 0 for u,t,ρ > 0, or
-case 2: σ is an odd function, and ∆ρ (u, t) < ∆ρ (—u, t) for u,t,ρ > 0,
in which ∆ρ (u, t)=夕0 [βu + √βρt) 一 夕0 [βu 一 √βρt),
•	夕 satisfies E {zφ (z)} → +∞ for Z 〜N (0,s2) and S → ∞,
•	夕 satisfies E {zI (z)} > 0 for Z is any Gaussian with zero mean and non-zero vari-
ance, and I (u)=夕(u) + U夕0 (u).
Thenfor any given ρ > 0, there exists β* = β* (ρ, τ) > 0 finite such that if β ≤ β*, then
Y = 0 is the only fixed point of the equation Y = G (γ, P) and is stable; if β > β*, Y = 0
is unstable, and there is one more fixed point at γ > 0 finite, which is stable.
(e) Consider T ∈ (0, ∞). The same conclusion as in Claim (c) holds for P = 0 with β* =
1/ (KE {σ0 (Tzι)}), assuming
• σ is positive on a set of positive Lebesgue measure, and E {σ0 (τz)} > 0 for Z 〜
N(0,1),
•夕”(u) < 0 for U > 0, and either
- case 1: σ (u) = 0for u ≤ 0 , or
-case 2:夕00 and σ are odd functions.
The assumption ∣∣^(k) ∣∣∞ ≤ C for k = 0,..., 3 is not critical, only serves to ensure integrability of
various terms in the proof and is likely relaxable, but is made for simplicity. The following lemma
establishes certain properties of the tanh function.
Lemma 12. Consider 夕=tanh. Then:
(a)	For ∆ (u, t)=夕0 (u + t)—夕0 (u — t), we have ∆ (u, t) < 0 for u,t > 0, and ∆ (u, t) <
∆ (—u, t) for u, t > 0.
(b)	lims→∞ E {zφ (z)} = +∞ for Z 〜N(0, s2).
(c)	E {zI (z)} > 0 for z 〜N(0, s2), s = 0, and I (U) =夕(u) + U夕0 (u).
Now let us consider 夕=σ = tanh. Note that tanh0 (u) ∈ (0,1) for any u = 0, tanh0 (0) = 1 and
E {tanh0 (Tzι)} < 1 unless T = 0. By Lemma 12, Proposition 11 applies. The following picture is
suggested based on Proposition 11:
•	When β < 1, we have convergence to Y = P = 0. This is based on Claim (c) and (e).
•	The phase transition for P locates atβ = 1, above which we have convergence to P ∈ (0, 1)
given a non-zero initialization, and below which P = 0. Here P < 1 since tanh is bounded
by 1. This is based on Claim (a), (b), and (c).
•	The phase transition of Y locates at some β > 1, above which we have convergence to
γ > 0 given a non-zero initialization, and below which Y = 0. For T > 0, Y cannot grow
to+∞ as β varies. This is based on Claim (a), (b) and (d).
The proposition also suggests that the two phase transitions are close to each other if E {σ0 (Tzι)} ≈
1. This requires that T2 ≈ 0, and σW ≈ 1 in the case α = 1. With respect to Eq. (1), we then have
σb2 ≈ 0. Remarkably this is reminiscent of the context of random feedforward networks with tanh
activations, in which (Pennington et al. (2017)) finds that an initialization at σW2 ≈ 1 and σb2 ≈ 0
works better than most edge-of-chaos initialization schemes with Gaussian weights.
We also expect from the proposition a similar picture forσ being the ReLU, with a crucial difference.
In this case, E {σ0 (Tzι)} = 0.5, and therefore one cannot have that the two phase transitions being
close to each other.
Interestingly Claim (a) implies that the phase transition of P never occurs before thatofY, regardless
of the specific 夕 and σ. One way for the phase transitions to be close to each other is, as above,
taking 夕=σ = tanh and T2 ≈ 0. Claim (a), (c) and (e) of the proposition also suggests that
if E {σ0 (Tzι)} > 夕0 (0), then Y and P will share the exact same location of the phase transitions,
below which they are zero and above which they are positive.
26
Published as a conference paper at ICLR 2019
C.3 Proofs for Section C.2
Proofof Proposition 10. Let θ = βγσ (Tzι) + √βρz2 for brevity.
Claim (a). We have G (0,ρ) = 0 and R (0,0) = 0. Simple calculations yield R (0,ρ) = βρ∕2.
Claim (a) is then immediate.
Claim (b). To study ρ in the case γ → ∞, we calculate:
PR (Y,ρ)
UEl Z	(βγσ (τzι) + p∕βρzɔ φ (Z) dz I
P [Jz=-γσ(τzι)'β∕ρ '	，	J
E I jpγσ (τzi) φ ( V ργσ (τzi)
+(1 + pγI 2σ (τzι)2) Φ (yβpγσ (Tzι)) }
where φ (U) = √2∏ exp {-u2∕2} and Φ (u) = R=-∞ φ (t) dt the Gaussian PDF and CDF. Recall
that We must have ɪR (γ, ρ) = 1 unless P = ∞. Also notice that φ (∞) = 0 and Φ (+∞) = 1.
Since σ is positive on a set of positive Lebesgue measure, if γ2∕ρ → ∞ as Y → ∞, P R (γ,ρ) → ∞,
and hence it must be that P → ∞ when γ → ∞.
Claim (c). We compute the first partial derivative ∂γG (γ, ρ), using the fact 夕"(u) = δ (u = 0) (in
the distributional sense) for the ReLU:
∂γG (γ,ρ) = βE {20 (θ) σ (τzι)} + β2YE {d0 (θ) σ (τzι) σ (Tzι)}
=βE {夕0 (θ) σ0 (Tzι)} + β2YE {δ (θ = 0) σ0 (Tzι) σ (Tzι)}
)
=βE {，(θ) σ (τzι)} + β2YE
βγ2σ (Tzι)2
2ρ
σ
(τzι) σ (τzι)
where in the last step, we use the fact that z1 and z2 are independent. Hence some algebras yield the
second partial derivative:
∏2β2E 1(τzι) σ (τzι)exp [-β [J) } (l - β2ρσ (τzι)2
=JIβ2E {10 (σ (Tzι))σ0 (Tzι)}
(=)J2β2E{I (σ (τzι)) zι}
V π τ
where in step (i), we define
I (U) = ɪu2 exp — - ^ɪu2
2	2ρ
in step (ii), we use Stein,s lemma. Recall T ∈ (0, ∞). Since σ (u) = 0 for all U ≤ 0, we have
I (σ (Tz)) = 0 for Z ≤ 0. In addition, I (u) > 0 almost everywhere and σ is non-zero on a set of
positive Lebesgue measure. It follows that ∂γ2G (Y, ρ) > 0. Therefore for any fixed ρ, the mapping
Y 7→ G (Y, ρ) is strictly convex. Notice that G (0, ρ) = 0. Hence it cannot be that there is a stable
fixed point at Y > 0.
Next we have:
∂γG (+∞, ρ) = lim βE {20 (β)σ (TZI)) σ0 (τzι)} = βE {σ0 (τzι)},
γ→+∞
∂γG (0,ρ) = βE {" (pβρz2) σ0 (Tzι)} = ∣βE {σ0 (Tzι)}.
Recall that y → G (y, ρ) is strictly convex. As such, when βE {σ0 (τzι)} ≤ 1, there is only one
fixed point at Y = 0, which is stable. When βE {σ0 (τzι)} ∈ (1, 2), there are two: one at Y = 0,
which is stable, and the other at y > 0, which is unstable. When βE {σ0 (τzι)} ≥ 2, there is only
one fixed point at Y = 0, which is unstable.
27
Published as a conference paper at ICLR 2019
Claim (d). In the case that σ is an odd function, u → I (U) is even and hence z → I (σ (Tz))
is even. Then from the calculation of Claim (c), it is easy to see that ∂γ2G (γ, ρ) = 0. Also,
G (0, ρ) = 0. Therefore for each ρ, the mapping γ 7→ G (γ, ρ) is a straight line which passes
through the point (0,0). Since ∂γG (0, P) = 1 βE {σ0 (τz1)}, the claim is then immediate.
Claim (e). We recall the formula for R (γ, ρ) derived in the proof of Claim (b). Since σ is odd, φ is
even and Φ - 0.5 is odd, we have
PR(γ,P) = βE{ jpYσ (TzI) φ (jpYσ (TzI)) + (1 + pγ2σ
+e {2 (ι+PY 2σ (TzI)2)}
=E {2 (ι+Pγ2σ (TzI)2)}.
Then ifβ > 2, we always have R (γ , P) > P.
(TzI)2) (φ( ∖∕lγσ (TzI)) - 2!)
□
Proofof Proposition 11. Let θ = βγσ (TzI) + √βρz2 for brevity.
Claim (a). The fact γ = P = 0 is a fixed point is obvious by assumption. Note that in order that
R (γ, 0) = 0, one must have E {夕(βγσ (Tz1))2} = 0. By assumption, this requires Y = 0.
Claim (b). The fact that 0 ≤ P ≤ C is obvious. Note that
|Y| = T2 IE ʤ (βYσ (TzI) + √βρz2)}∣ ≤ CE{|zi|} = C W,
which yields the claim.
Claim (c). We have:
∂ρR (γ,ρ) = jPE 2 (θ) / (θ) z2} = βE {/ (θ)2 + ,0 (θ)中(θ)},
where (i) is due to Stein’s lemma. As such, ∂ρR (0, 0) = βκ2. The claim is then immediate.
Claim (d). Consider P > 0. We have:
∂γG (γ,ρ) = βE {20 (θ) σ0 (Tzι)} + β2YE {200 (θ) σ0 (Tzi) σ (Tzι)},
and therefore,
d：G (γ,ρ) = 2β2E {.00 (θ) σ0 (Tzi) σ (Tzi)} + β3γE {.000 (θ) σ0 (Tzi) σ (Tzi)2 }
=βE {∂ζι (200 (βγσ (Tzi) + pβpz2) σ (Tzi)2)}
=βEζ2 {Ezι {"0 (βγσ (Tzi) + pβpz2) ziσ (Tzi)2}}
=βEzi {Ez2 {"0 (βγσ (Tzi) + pβpz2)} ziσ (Tzi)2}
where we use Stein’s lemma and the fact zi and z2 are independent. Notice that, for βP > 0, by
Stein’s lemma,
Ez2 {^00 (a + Pp∕βρz2)O = √βρ∙
1
Eζ2 卜夕0 (a + pβPz2) }
√2∏βρ
Zt	[/ (a + √βpt) - ψ (a - pβpt)i t exp {— g } dt.
Consider case 1. Since σ (u) = 0 for all u ≤ 0,
β2	+∞
dγ G (γ,ρ) = 2ΠT√βp L
+∞
∆	∆ρ (γσ (Tz), t) tzσ (Tz)2 exp
t=0
—
⅜f} dtdz.
28
Published as a conference paper at ICLR 2019
Since ∆ρ (u, t) < 0 for any u > 0 and t > 0, and by the assumption that σ is positive on a set of
positive Lebesgue measure (which cannot intersect with (-∞; 0] since σ (u) = 0 for all u ≤ 0), we
have ∂γ2G (γ, ρ) < 0 for γ > 0. In case 2, since σ is an odd function,
∂γ2G(γ,ρ)
β2	+∞ +∞
2∏T√βp L /=0 δp (γσ (Tz) ,t)
—∆ρ (-γσ (Tz), t)] tzσ (Tz)2 exp
+ t2
2
dtdz.
z
2
—
Since ∆ρ (u, t) < ∆ρ (-u, t) for u > 0 and t > 0 and σ (u) ≥ 0 for u ≥ 0 (for σ being odd and
non-decreasing), we again have ∂γ2G (γ, ρ) < 0 for γ > 0. As such, the mapping γ 7→ G (γ, ρ), for
ρ > 0, is strictly concave on (0, ∞).
Next we have
∂γG(0,ρ) = βE {20 (pβpz2) σ0 (τzι)} = βE {20 (pβpz2) } E {σ0 (Tzι)},
since z1 and z2 are independent. Notice that by Stein’s lemma,
dβ (βE {“ (pβρz2)})=IE nI 0 (pβpz2)}=21PE {pβpz2I (pβpz2)}.
By assumption, We thus have that βE {夕0 (√βρzι)} and hence ∂γG (0,ρ) are increasing in β.
Furthermore, ∂γG (0, ρ) → 0 when β → 0, and
lim ∂γ G (0, ρ)
β→∞
=E {σ0 (Tzι)} Jm PE {Pβpz22(pβpz2)}
(=ii)
+∞
where (i) is by Stein's lemma, and (ii) is by assumption. Therefore, there must exist β* =
β* (ρ,τ) > 0 finite such that ∂γG (0, ρ) = 1 at β = β*, ∂γG (0,ρ) < 1 for β < β* and
∂γG (0,ρ) > 1 for β > β*. Since Y → G (γ,ρ) is strictly concave on (0, ∞) and Y = 0 is a
fixed point by Claim (a), β * is the threshold as in the claim.
Claim (e). Consider = 0, in which case:
dYG (Y, 0) = τ√= Z	Ψ,' (βYσ (Tz)) zσ (Tz)2 eχP [-z- \ dz.
T V 2π J ζ=-∞	I 2 )
Consider case 1. Since σ (u) = 0 for all u ≤ 0,
∂YG (y, 0) = T√2∏ / ɔ ψ" (βγσ (Tz)) zσ (Tz)2 exp {-z } dz.
Since 夕00 (u) < 0 for u > 0 and σ is positive on a set of positive Lebesgue measure (which cannot
intersect (-∞; 0] since σ (u) = 0 for all u ≤ 0), it is easy to see that ∂γ2G (Y, 0) < 0 for Y > 0. In
case 2, since 夕00 and σ are odd, the mapping z → 夕00 (βγσ (Tz)) is odd, and hence
d；G (γ, 0) = T2β2∏ / Ψ,0 (βγσ (Tz)) zσ (Tz)2 exp { -z } dz.
Again we have ∂γ2G (Y, 0) < 0 for Y > 0. As such, Y 7→ G (Y, P) is strictly concave on (0, ∞).
Notice that ∂γG (0,0) = βκE {σ0 (Tzι)}, linearly increasing in β. This proves the claim with
β* = 1/ (κE{σ0 (Tzι)}).	□
Proof of Lemma 12. We prove the first claim:
∆ρ (u, t) = tanh2 (u - t) - tanh2 (u + t) .
Since tanh2 is even and increasing on (0, +∞), ∆ρ (u, t) < 0 for u, t > 0 and ∆ρ (u, t) > 0 for
u > 0 and t < 0. Simple algebra yields
∆ρ (-u, t) - ∆ρ (u, t) = 2∆ρ (u, -t) .
Hence ∆ρ (u, t) < ∆ρ (-u, t) for u > 0 and t > 0. To see the second claim, for z 〜N (0,1),
lim E {sz夕(sz)} = lim E {s |z|} =+∞.
s→∞	s→∞
To see the third claim, since I is odd,
E {zI(z)} = 2E z tanh (z) + z 1 - tanh (z)2 I (z > 0) > 0.
□
29
Published as a conference paper at ICLR 2019
C0 O 9
bg 00 g
rH<⅝∙
S 4 ・
g 8 g g
7 7 7・
6 GG ∙
ʃʃʃ f
V 9 V ∙
3 3 3 9
λax∙
I .11- f
,040
Figure 10:	The reconstructions by the schemes from Table 1, as described in Appendix D.1, in
Setting 1 (i.e.,夕0 = tanh). From the top row: original images, reconstructions from Scheme 1,
4 and 2. We omit the reconstructions from other schemes, since they are almost identical to those
of Scheme 2. For each digit/letter category, the image is selected from the test set by ranking the
reconstruction loss, averaged across Scheme 1 and 4, and picking one at the 75% percentile.
Cooi
V000
Λ;说∙
qq
尸Zr∙ ∙
mɔe
G Gs■
5 S ʃ 0
4 4 4 9
33 3 9
72 a・
Figure 11:	Description similar to Fig. 10. The chosen images are the 25% percentile.
D Visualization for Section 3.5
D. 1 Visualization of the reconstructions
In Fig. 10, 11, 12 and 13, we show the reconstructions of several images by the trained networks
after 5 × 105 training iterations, under the schemes from Table 1, in the experiments of Section 3.5.
We draw 10 digit images from the MNIST test set, as well as 3 letter images from the EMNIST
Letters test set (Cohen et al. (2017)). Note that the networks are not trained with any letter images
from the EMNIST data set. The reconstruction quality is visually imperfect even after intensive
training, which is entirely expected for vanilla autoencoders and regression problems.
Observe that for the schemes that yield meaningful reconstructions, they output recognizable digits
for digit images, while for letter images, most of their reconstructions are hardly recognizable as
letters. As such, the trained networks of these schemes do not simply approximate the identity
function, but rather capture some low-dimensional structures of the data. An exception is Scheme 7
under Setting 2, which is not surprising since it is a purely tanh network and tanh is almost identity
near zero.
D.2 Visualization of the evolution
We show the evolution of the test reconstruction loss on the plane σW2 , σb2 , in conjunction with the
experiments of Section 3.5. To make the computation more manageable, we opt for L = 50 with
less iterations, while maintaining other parameters the same as in Section 3.5. The results are shown
in Fig. 14 and 15. Several patterns emerge in good agreement with our hypotheses. Firstly, for
夕=ReLU, the evolution starts earliest near σW = 2, shows almost no progress or is numerically
unstable when σW》2, and is much slower when σW《 2. Secondly, for 夕 =tanh, the evolution
is much slower when σW2	1. Intriguingly the evolution is almost insensitive to σb2.
30
Published as a conference paper at ICLR 2019
夕0B
华5 b 7
I A 3
O
C O 5 C
夕夕夕
Figure 12: Description similar to Fig. 10. The setting is Setting 2 (i.e., identity 夕0). The reconstruc-
tions are of Scheme 1, 4, 7 and 2. The chosen images are the 75% percentile. The ranking is by
averaging over Scheme 1, 4, and 7.
。I 7 3 *5G 7 8
OlJ 3 4 5 G 7 8
0 I 二 3 夕 5 G 7 X
Figure 13: Description similar to Fig. 10. The setting is Setting 2 (i.e., identity 夕0). The reconstruc-
tions are of Scheme 1, 4, 7 and 2. The chosen images are the 25% percentile. The ranking is by
averaging over Scheme 1, 4, and 7.
3 Iter. 300
ɔ ∣⅜er.500
3 Iter. 1000
ɔ _ Iter. 2000
3 Iter. 3000
2.2
1.7
2
2
2
2
2
1
1
1
0	0.5	1 0	0.5	1 0	0.5	1 0	0.5	1 0	0.5	1
0.3
o Iter. 200
3 lteι^OO
ɜ IteMOOO
o Iter. 2000
3 lteι^OOO
2.1
1.5
0.9
1
0	0.5
1
1 0	0.5
1
1 0	0.5
1
1 0	0.5
1
1 0	0.5	1
0.3
3 Iter. 100
3 lteι^OO
3 Iter. 700
3 Iter. 1000
3 Iter. 3000
1.2
0.9
0.6
2
2
2
2
2
2
2
2
2
2
1
1
1
1
1
0	0.5	1 0	0.5	1 0	0.5	1 0	0.5	1 0	0.5	1
0.3
2.0
∣te∣^∞ 2 0
Iter. 600 C C
2.0
IteMOOO 20
Iter. 2000 2 θ Iter. 3000
0.9
1.5
1.5
1.5
0.6
0.5^^―
0	0.5
1 0	0.5
1 0	0.5
1 0	0.5
1 0	0.5	1
0.3
1
1
1
1
1
Figure 14:	The test loss for various pairs σW2 , σb2 at different iterations, as described in Appendix
D.2, in Setting 1 (i.e.,夕0 = tanh). The horizontal axis is σ2, and the vertical axis is σW. First
row: φ = σ = ReLU; second row:夕=ReLU and σ = tanh; third row:夕=σ = tanh; fourth
row:夕=tanh and σ = ReLU. Red indicates higher loss, and black indicates lower loss. White
indicates a numerical error.
31
Published as a conference paper at ICLR 2019
C Iter. 1000 C ∣ter. 1400 C ∣ter. 2000 C ∣ter. 2400 C ∣ter. 3000
0.5
l0.5
l0.5
l0.5
l0.5
0	0.5	1 0	0.5	1 0	0.5	1 0	0.5	1 0	0.5	1
0.9
0.75
0.6
2
2
2
2
2.0
1.5
0.5
1J
J
0.9
0.75
0.6
3
0.8
0.6
0.9
0.75
0.6
Figure 15:	The test loss for various pairs σW2 , σb2 at different iterations, as described in Appendix
D.2, in Setting 2 (i.e.,夕0 is the identity). The description is similar to Fig. 14. White indicates
either a very large value or a numerical error, which are due to the fact that the chosen learning rate
is not sufficiently small. Note that in this setting with 夕=ReLU, the training process is trapped in
numerical errors for σW2 > 2 as we expect, and hence the test loss at σW2 > 2 is not plotted.
E	Miscellanies
E.1 Edge of chaos initialization
We quickly review the edge of chaos (EOC) initialization (Schoenholz et al. (2016)). For an activa-
tion σ, any (σW, σ2) such that there exists a finite τ ≥ 0 for which
τ2 = σWE {σ (τz)20 + σ2,
σW = 1/E {σ0 (Tz)2}
is said to be an EOC initialization scheme, where Z ~ N (0,1). This is based on the order-to-chaos
phenomenon found in (Poole et al. (2016)). For ReLU σ , σW2 = 2 and σb2 = 0 is the only EOC
initialization (Hayou et al. (2018)) and coincides with the He initialization (He et al. (2015)). For
σ = tanh, there can be multiple pairs that form EOC initialization. (Pennington et al. (2017)) argues
that a better EOC scheme should be closer to dynamical isometry and suggests taking σW2 ≈ 1.05
and σb2 ≈ 2.01 × 10-5 for σ = tanh. The EOC initialization is applicable to feedforward networks.
E.2 Useful facts
We state several useful facts, which are used in various places throughout. First, for a Gaussian
matrix W and an independent vector u, we have Wu is also Gaussian. In particular, if entries of
W are i.i.d. N(0, s2), then Wu ~ N 仅，s2 ||u『I). The second fact is Stein,s lemma:
Lemma 13 (Stein's). E {zf (z)} = E {f 0 (z)} for Z ~ N (0,1) and f : R → R weakly differen-
tiable whenever the expectations are defined.
32