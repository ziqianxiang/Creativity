Under review as a conference paper at ICLR 2019
Detecting Adversarial Examples Via Neural
Fingerprinting
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks are vulnerable to adversarial examples: input data that has
been manipulated to cause dramatic model output errors. To defend against such
attacks, we propose NeuralFingerprinting: a simple, yet effective method to detect
adversarial examples that verifies whether model behavior is consistent with a set of
fingerprints. These fingerprints are encoded into the model response during training
and are inspired by the use of biometric and cryptographic signatures. In contrast
to previous defenses, our method does not rely on knowledge of the adversary and
can scale to large networks and input data. In this work, we 1) theoretically analyze
NeuralFingerprinting for linear models and 2) show that NeuralFingerprinting
significantly improves on state-of-the-art detection mechanisms for deep neural
networks, by detecting the strongest known adversarial attacks with 98-100% AUC-
ROC scores on the MNIST, CIFAR-10 and MiniImagenet (20 classes) datasets. In
particular, we consider several threat models, including the most conservative one
in which the attacker has full knowledge of the defender’s strategy. In all settings,
the detection accuracy of NeuralFingerprinting generalizes well to unseen test-data
and is robust over a wide range of hyperparameters.
1	Introduction
Deep neural networks (DNNs) are highly effective pattern-recognition models for a wide range of
tasks, e.g., computer vision (He et al., 2015), speech recognition (Xiong et al., 2016) and sequential
decision-making (Silver et al., 2016). However, DNNs are vulnerable to adversarial examples: an at-
tacker can add (small) perturbations to input data that are imperceptible to humans, and can drastically
change the model’s output, introducing catastrophic errors(Szegedy et al., 2013; Goodfellow et al.,
2014). A key challenge then is to make DNNs robust to adversarial attacks, so DNNs can be reliably
used in noisy environments or mission-critical applications, e.g., in autonomous vehicles. There
are two broad classes of solution approaches: making robust predictions and detecting adversarial
examples. A major bottleneck with robust predictions can be the inherently larger sample complexity
associated with robust learning (Schmidt et al., 2018). Hence, we focus on the latter and propose
NeuralFingerprinting (NeuralFP): a fast, secure and effective method to detect adversarial examples.
The core idea of NeuralFP is to encode fingerprint patterns into the response of a neural network
around real (i.e., non-adversarial) data. These patterns characterize the model’s expected behavior
around real data and can thus be used to detect adversarial examples, around which the model outputs
are not consistent with the expected fingerprint outputs (Figure 1). This approach is attractive as
encoding fingerprints is feasible and simple to implement during training, and evaluating fingerprints
is computationally inexpensive. Furthermore, NeuralFP is agnostic of the adversary’s attack mecha-
nism, and differs from recent methods (Meng & Chen, 2017; Ma et al., 2018; Lee et al., 2018) that
rely on auxiliary classifiers for detecting adversarial examples.
In this work, we extensively analyze and evaluate NeuralFP under various settings and threat models.
We theoretically analyze the feasibility and effectiveness of NeuralFP for linear models. Furthermore,
for DNNs, we experimentally validate under multiple (including the most conservative) threat
models that 1) NeuralFP achieves almost perfect detection AUC-ROC scores against state-of-the-art
adversarial attacks on various datasets and 2) adaptive attackers with knowledge of the fingerprints
fail to craft successful attacks. To summarize, our key contributions are:
1
Under review as a conference paper at ICLR 2019
Figure 1: Detecting adversarial examples
using NeuralFP with N = 2 fingerprints,
for K-class classification. 夕(x) is the
model output. NeuralFP separates real
data x (top) from adversarial data x0 =
x + η (bottom) by comparing the sensi-
tivity of the model to certain predefined
perturbations around unseen inputs with a
reference sensitivity encoded around the
manifold of real images during training.
•	We present NeuralFP: a simple and secure method to detect adversarial examples that does
not rely on knowledge of the attack mechanism.
•	We formally characterize the effectiveness of NeuralFP for linear classification.
•	We empirically show on vision tasks that NeuralFP achieves state-of-the-art near-perfect
AUC-ROC scores on detecting and separating unseen test data and the strongest known
adversarial attacks.
•	We empirically show that the performance of NeuralFP is robust to the choice of fingerprints
and is effective for a wide range of choices of hyperparameters.
•	Finally, we show that NeuralFP can be robust to known adaptive-whitebox-attacks, where
an attacker has full knowledge of the fingerprint data1.
1.1 Related Work
Several forms of defense to adversarial examples have been proposed, primarily in the setting of
robust prediction, including adversarial training, detection and reconstructing images using adversarial
networks (Meng & Chen, 2017). Instead, our focus is on detecting adversarial attacks.
Robust Prediction. Raghunathan et al. (2018); Kolter & Wong (2017) train on convex relaxations of
the network to maximize robustness and are able to formally certify robustness for small perturbations.
These methods do not yet scale to large models or high-resolution inputs. Several other defenses
attempt to make robust predictions: by relying on randomization (Xie et al., 2018), introducing non-
linearity that is not differentiable (Buckman et al., 2018) and by relying on Generative Adversarial
Networks (Song et al., 2018; Pouya Samangouei, 2018) for denoising images. However, recent
work has shown that several of these defenses - including (Buckman et al., 2018; Meng & Chen,
2017; Song et al., 2018), amongst others - are not secure, when subject to a stronger adversary
(Athalye et al., 2018; Uesato et al., 2018; Carlini & Wagner, 2017a;b). The reliance of these defenses
on obfuscated gradients and obscurity is used to render the defenses inviable. Madry et al. (2017)
employs robust-optimization techniques to minimize the maximal loss that can be achieved through
first-order attacks, and has been shown to not cause obfuscated gradients (Athalye et al., 2018).
Robust Detection. Amongst defenses that study the detection of adversarial examples, Ma et al.
(2018) detects adversarial samples using an auxiliary classifier trained to use an expansion-based
measure, local intrinsic dimensionality (LID). Similar detection methods based on Kernel Density
(KD), Bayesian-Uncertainty (BU) (Feinman et al., 2017) and the Mahalonobis Distance (MD) (Lee
et al., 2018) using artifacts from trained networks have been considered. (Carlini & Wagner, 2017a;
Athalye et al., 2018) showed LID, KD and BU to be ineffective against stronger attacks. In contrast,
NeuralFP does not depend on auxiliary classifiers and performs significantly better than LID, KD
and BU: 1) when the attack mechanism is unknown and 2) is robust to stronger attacks that break
LID, KD and BU. A recent defense, (Lee et al., 2018), remains to be extensively studied under a
stronger threat model and for signs of gradient obfuscation and security through obscurity (Athalye
et al., 2018; Uesato et al., 2018), where the dynamic attacker is aware of the defense mechanism.2
1Code to reproduce experiments and the model weights: https://www.dropbox.com/sh/iq0yub74gquz1od/
AADZXUVabMvZasPrt- 6-c-Wpa?dl=0.
2 Due to unavailable implementation code, we were not able to baseline against this defense.
2
Under review as a conference paper at ICLR 2019
Further, all of the above mentioned defenses (including MD) rely on auxiliary models for detection,
which often adds to the vulnerability of the defense.
Adversarial Examples in Other Domains. We evaluate NeuralFP on computer vision tasks, a
domain in which DNNs have proved to be effective and adversarial examples have been extensively
studied. NeuralFP could potentially be employed to secure DNNs against attacks in domains
such as speech recognition (Carlini & Wagner, 2018) and text comprehension (Jia & Liang, 2017).
Adversarial attacks have also been studied in domains such as detection of malware (Grosse et al.,
2017), spam (Nelson et al., 2008) and intrusions (Wagner & Soto, 2002). Data-poisoning (Alfeld
et al., 2016) is another form of attack where maliciously crafted data is injected during training.
2 Fingerprinting for Detection of Adversarial Examples
We consider supervised classification, where we aim to learn a model f (x; θ) from real data
{(xi,y*i)}i∈z, where X ∈ Rl is an input example (e.g., an image) and y* is a 1-hot label vec-
tor y* ∈ {0,1}K over K classes. Here, we assume the data is sampled from a data distribution
Pdata(x, y). For example, a neural network model f predicts class probabilities P (y|x; θ) as:
f(χ; θ)j = P(yj∣χ; θ)
exp h(x; θ)j
Pl exp h(x; θ)l
(1)
where h(χ; θ) ∈ RK are called logits and the most likely class is chosen. The optimal θ* can be
learned by minimizing a loss function L(x, y; θ), e.g., cross-entropy loss. In this setting, an attacker
attempts to construct adversarial examples x0, such that y = argmaxl P(yl∣x0; θ) is an incorrect
class prediction (i.e., Pdata (χ0, y) = 0). In this work, We focus on bounded adversarial attacks, which
produce small perturbations η that cause mis-classification. This is a standard threat model, for an
extensive review see (Akhtar & Mian, 2018). More generally, a bounded adversarial example causes
a large change in model output, i.e. for δ, ρ > 0:
kηk ≤ δ,	kf(x+η) - f (x)k > ρ,
such that the class predicted by the model changes: argmaxf (x + η)j 6= argmaxf (x)j. An example
of a bounded attack is the Fast-Gradient Sign Method (FGSM) Goodfellow et al. (2014) which uses
an input-space gradient: η H sign dLXχy⑻.Since the perturbation η is bounded and small, if X is an
image, x0 can be indistinguishable from x but still cause very different predictions. Hence, our goal
in this work is to efficiently detect and separate real from adversarial examples.
Neural Fingerprinting. To defend DNNs against
adversarial attacks, we propose NeuralFP: a method
that detects whether an input example X is real or ad-
versarial. This algorithm is summarized in Algorithm
1 and Figure 1. The key idea of NeuralFP is to detect
adversarial examples by using a form of consistency
check of the model output around the input. More pre-
cisely, a defender using NeuralFP chooses (a set of)
input perturbation(s) ∆X around X and checks whether
the model output on X + ∆X changes according to a
chosen ∆y. These chosen output-changes are encoded
Algorithm 1 NeuralFP
1:	Input: example x, comparison function
D (see Eqn 3), threshold τ > 0, χi,j
(see Eqn 2), model f.
2:	Output: accept / reject.
3:	if ∃j : D(X, f, χi,j ) ≤ τ then
4:	Return: accept # X is real
5:	else
6:	Return: reject # X is not real
7:	end if
into the network during training. We will discuss
strategies for choosing the fingerprints (∆X, ∆y) in Section 2.2.
Formally, we define a fingerprint χ as the tuple χ , (∆X, ∆y). For K-class classification, we define
a set of N fingerprints:
χi,j = (∆Xi, ∆yi,j), i= 1,...,N, j= 1,...,K,	(2)
where χi,j is the ith fingerprint for class j . As noted before, the ∆Xi (∆yi,j ) are chosen by the
defender. Note that we use the same directions ∆Xi for each class j = 1, 2 . . . , K, and that ∆yi,j
can be either discrete or continuous depending on f(X; θ).
3
Under review as a conference paper at ICLR 2019
To characterize sensitivity, we define the function F (x, ∆xi) to measure the change in model output.
A simple choice could be F(x, ∆xi) = f(x+∆xi) -f(x) (although we will use variations hereafter).
To compare F (x, ∆xi ) with the reference output-perturbation ∆yi , we use a comparison function D:
1N
D(x,f, Xj，NN ∑ kF(x, ∆xi) - ∆yi,j∣∣2.	⑶
Hence, the goal of a defender is to minimize D(x, f, χi,j) for real data x.
Detecting Adversarial Examples. NeuralFP classifies a new input x0 as real if the change in
model output is close to the ∆yi,j for some class j, for all i. Here, we use a comparison function
D and threshold τ > 0 to define the level of agreement required, i.e., we declare x0 real when D is
below a threshold τ .
x0 is real ⇔ ∃j : D (x0, f, Xj ≤ τ.	(4)
Hence, the NeuralFP test is defined by the data: NFP = ^{χi,j }i=1 N j=ι K ,D,τ).
Encoding Fingerprints. Once a defender has constructed a set of desired fingerprints χ, the chosen
fingerprints can be embedded into the network’s response by adding a fingerprint regression loss
during training. Given a classification model, the fingerprint loss is:
N
Lfp(x, y, χ; θ) = X kF (x, ∆xi) - ∆yi,kk22,	(5)
i=1
where k is the ground truth class for example x and ∆yi,k are the fingerprint outputs. Note that we
only train on the fingerprints for the ground truth class. The total training objective then is:
min(Lo(x, y； θ) + αLfp(x, y, χ; θ)),
θ
(x,y)
where L0 is a loss function for the task (e.g. cross-entropy loss for classification) and α a positive
scalar. In practice, we choose α such that it balances the task and fingerprint losses. The procedure
trains the model so that the function D has low values around the real data, e.g., the train-set. We
then exploit this characterization to detect adversarial test-set examples using D.
Complexity. Extra computation comes from Algorithm 1 which requires O(N K) forward passes
to compute the differences F(x, ∆xi). A straightforward implementation is to check (4) iteratively
for all classes, and stop whenever an agreement is seen or all classes have been exhausted. However,
this can be parallelized and performed in minibatches for real-time applications.
2.1	Threat Model Analysis
We study NeuralFP under various threat models (Table 1), where the
attacker has varying levels of knowledge of NFP and model f(x; θ).
In the partial-whitebox-attack (PWA) setting, the attacker has ac-
cess to θ, can query f(x; θ) and its derivatives, but does not know
NFP. We evaluate under this setting in Section 3. This setting is
the most commonly studied setting, and most defenses reported in
θ	NFP
×	×	blackbox-attack
X	×	partial-whitebox-attack
X	X	adaptive-whitebox-attack
Table 1: Threat models: at-
tacker knows θ and / or NFP.
Section 1, including (Ma et al., 2018; Song et al., 2018; Lee et al., 2018), study attacks under this
threat model. The PWA setting is relevant, for instance, when the attacker has a copy of the model,
but the fingerprints are kept private, in accordance with Kerchhoff’s principle (Shannon, 1949) (e.g.
when the NeuralFP test is run on a cloud). Although PWA is not the strongest threat scenario, we
study it because of its widespread study in recent literature (Ma et al., 2018; Song et al., 2018) and it
provides a direct comparison of NeuralFP against established baselines.
Reverse engineering the NFP by brute-force search can be combinatorially hard. To see this, consider
a simple setting where only the ∆yi.j are unknown, and that the attacker knows that each fingerprint
4
Under review as a conference paper at ICLR 2019
〈叫 6)+ b > O
{w,为〉+ 6 < O
(w, ɪ) + 6 = O
37jIaaJ
Figure 2: Geometry of fingerprints for SVMs with lin-
early separable data. Let d(x) be the distance of x to
the decision boundary (see Thm 1). δ±max (δ±min) de-
note the maximal (minimal) distances of the positive
(x+) and negative (x-) examples to the separating hy-
perplane hw, xi + b = 0. The fingerprint ∆x1 with
(△x1,W〉= δmin will have f (x- + ∆x) < 0 and
f(x- ) < 0 for all x- in the data distribution (red
region). Hence, △x1 will flag all x0 in the regions
-δ-min < d(x0) < 0 as not real (d(x0) is the signed
distance of x0 from the boundary), since for those x0 it
will always see a change in predicted class. Similarly,
△x2 with h∆x2 ,Wi = δmax always sees a class change
for real x-, flagging all x0 : d(x0) < -δ-max as not real.
is discrete, i.e. each component △yki.j = ±1. Then the attacker would have to search over combina-
torially (O(2N K)) many △y to find the subset of △y that satisfies the detection criterion in equation
(4). Further, smaller τ s reduce the volume of inputs accepted as real.
However, the strongest threat model assumes the attacker has full information about the defender, for
e.g., when the attacker can either reverse-engineer or has access to the fingerprint data NFP, so that
stronger attacks could be possible. Hence, we study this scenario extensively as well.
In the adaptive-whitebox-attack (AWA) setting, the adversary has perfect knowledge of NFP in
addition to the information available under PWA. For this setting, first, in Section 2.2 we analyze
NeuralFP for binary classification with linear models (SVMs), by characterizing the region of inputs
that will be judged as real for a given set of fingerprints χi.j and the correspondence with the data
distribution Pdata(x, y) (see Theorem 1). Second, we empirically show the robustness of NeuralFP
for DNNs to adaptive attacks in Section 3.2. Ma et al. (2018); Xu et al. (2018) are other recent
defenses that investigate the robustness to such adaptive attacks.
The blackbox-attack setting is the weakest setting, where the adversary has no knowledge of the
model parameters or NFP. We evaluate NeuralFP under this setting in Appendix F. Additionally,
we define the notion of whitebox-defense (the defender is aware of the attack mechanism) and
blackbox-defense (defender has no knowledge about attacker). As such, NeuralFP is a blackbox-
defense. There has been considerable progress in the blackbox-attack and whitebox-defense settings
(e.g. (Liao et al., 2017)). However, progress in the blackbox-defense threat model is relatively limited.
2.2	Choosing and Characterizing Fingerprints: Linear Models
We first analyze NeuralFP on binary classification with data {(xi, y*i) }分三1 and linear model (SVM):
f (x) = hw,x^i + b, y = sign f(x) ∈{-1,1},
on inputs xi ∈ Rn, where n 1 (e.g., n = 900 for MNIST). The SVM defines a hyperplane
f (x) = 0 to separate positive (y = +1) from negative examples (y = -1). We will assume that the
positive and negative examples are linearly separable by a hyperplane defined by a normal W = k^.
We define the minimal and maximal distance from the examples to the hyperplane along W as:
δmin = min ∣hxi,ι^i∣ ,δmax = max ∣hxi,W)∣
i:yi=±1	i:yi=±1
In this setting, the set of xi classified as real by fingerprints is determined by the geometry of f (x).
Here, for detection, we measure the exact change in predicted class using
F(x, △x) = △y = sign (hW, x + △xi + b) - sign (hW, xi + b) ∈ {-2, 0, 2} .
Theorem 1 (Fingerprint Detection for SVM). Consider an SVM with W = ^^ and separable data,
and the following criteria:
(∆x1 = δminW, ∆y1,- = 0),	(6)	(∆x3	=	-δmaxW, ∆y3,+	= -2),	(8)
(∆x2 = δmaxW, ∆y2,- = +2),	(7)	(∆x4	=	-δmint^, Ay4，+	= 0).	(9)
5
Under review as a conference paper at ICLR 2019
An adversarial input x0 = x± + η for which one of the following holds:
d(x0) > δ+max,	0 < d(x0) < δ+min,	d(x0) < -δ-max, -δ-min < d(x0) < 0,	(10)
will satisfy one ofthe above listed criteria. Here, d(x0) = hx1,Wj+b represents the signed distance of
x0 from the separating hyperplane.
The proof for two fingerprints is shown in Figure 2. For the full proof, see the Appendix. Theorem 1 by
itself does not prevent attacks parallel to the decision boundary. An adversary could push a negative
example x- across the boundary to a region outside the data distribution (Pdata(x- + η, y) = 0),
but within distances δ+min and δ+max of the boundary. This would still be judged as real by using
fingerprints. However, such examples could still be detected by also checking the distance of x- + η
to the nearest x+ in the dataset.
2.3 Choosing and Characterizing Fingerprints: DNNs
In contrast to the linear setting, in general NeuralFP utilizes a softer notion of fingerprint matching
by checking whether the model outputs match changes in normalized-logits. Specifically, for
classification models f(x; θ) with logits h(x; θ) (see Eqn 1), where F is defined as:
F(X, δXi),2(X + δXi)-2(X),	夕(X) , Ilh(x; θ)∣∣,
kh(x; θ)k
where 夕 are the normalized logits. The logits are normalized so the DNN does not fit the ∆y by
making the weights arbitrarily large. Here, we use D(X, ∆Xi) as in (3). Note that here ∆yi,j ∈ RK.
Choosing ∆y For our experiments, we choose the ∆y so that the normalized-logit of the true class
either increases or decreases along the ∆Xi (analogous to the linear case). For e.g., for a 10-class
classification task, if X is in class k we choose ∆y of the form:
∆ylk6=k = -α,	∆ylk=k = β, k = 1, . . . , 10,	(11)
where α, β ∈ R+. We found that reasonable choices are α = 0.25 and β = 0.75, and that the method
is not sensitive to these specific choices. Further, we experimented with randomizing the signs of
∆yk and found that NeuralFP’s performance is robust to this randomization as well. 3
Choosing ∆X For nonlinear models (e.g., DNNs), the best fingerprint-direction choice is not
obvious. To overcome this, we propose a straightforward extension from the linear case, using
randomly sampled fingerprints. Randomization minimizes structural assumptions that may make
NeuralFP exploitable. We empirically found that this provides effective detection.
For all experiments, we sampled the fingerprint directions ∆X from a uniform distribution (∆X 〜
U(-ε, ε)l ), where l is the input dimension, and each pixel is uniformly randomly sampled in the
range [-ε, ε]. Our experiments (see Figure 6) suggest that NeuralFP is not sensitive to the random
values sampled. This also suggests that the ∆Xi could be chosen based on a different approach.
Visualizing Fingerprints. To understand the behavior of fingerprinted non-linear models we
trained a neural network (two hidden layers with 200 ReLU nodes each) to distinguish between
two Gaussian balls in a 2D space (Figure 3, left). Without fingerprints, the model learns an almost
linear boundary separating the two balls (compare with Figure 2). When we train to encode the
fingerprints (negative of ∆ys in (11)), we observe that NeuralFP causes the model to learn a highly
non-linear boundary (Figure 3, center) forming pockets of low fingerprint-loss characterizing the data-
distribution (Figure 3,right). In this simple setting, NeuralFP learns to delineate the data-distribution,
where the darker regions are accepted as real and the rest is rejected (Figure 4).
3 Using ∆yi=kk = —α ∙ (2p — 1), ∆yl=kk = β ∙ (2p — 1),p 〜Bern (1) , NeuralFP achieves AUC-ROC
> 95% on PWAs. Here p ∈ {0, 1} is a Bernoulli random variable that is resampled for each ∆xi.
6
Under review as a conference paper at ICLR 2019
Figure 3: Left: decision boundary without fingerprints. Center: with fingerprints, red arrows indicate
fingerprint-directions. The decision boundary is significantly more non-linear. Right: contour plot of
fingerprint loss. NeuralFP detects dark regions as “real”, while lighter regions are “fake” (tunable
through τ). Fingerprinting create valleys of low-loss delineating the data-distribution from outliers.
3	EVALUATING NeuralFP ON DETECTION OF ADVERSARIAL ATTACKS
We now empirically validate the effectiveness of NeuralFP on a number of vision data-sets, as well
as analyze the behavior and robustness of NeuralFP under the various threat models. We evaluate
NeuralFP on distinguishing between unseen real images and adversarial images, for data and models
of varying scales in the three threat-models discussed earlier (PWA, AWA, BA). Further, we study
the sensitivity of NeuralFP to varying hyperparameters. The study of the blackbox-attack setting is
deferred to Appendix F. We empirically find that:
•	the defense is robust under all the three threat models, and
•	using NeuralFP does not diminish test accuracy.
3.1	Detection of Partial-Whitebox Attacks
We report the AUC-ROC of NeuralFP on MNIST, CIFAR-10 and MiniImagenet-20 against four
state-of-the-art partial-whitebox attacks (Table 2):
•	Fast Gradient Method (FGSM)Goodfellow et al. (2014) and Basic Iterative Method (BIM)
Kurakin et al. (2016) are both gradient based attacks with BIM being an iterative variant
of FGSM. We consider both BIM-a (iterates until misclassification has been achieved) and
BIM-b (iterates 50 times).
•	Jacobian-based Saliency Map Attack (JSMA) Papernot et al. (2015) perturbs pixels using a
saliency map.
•	Carlini-Wagner Attack (CW-L2): an optimization-based attack, is one of the strongest known
attacks (Carlini & Wagner, 2016; Carlini & Wagner, 2017a), and optimizes to minimize the
perturbation needed for misclassification.
Following Ma et al. (2018), for each dataset we consider a randomly sampled pre-test-set of unseen
1328 test-set images, and discard misclassified pre-test images. For the test-set of remaining images,
we generate adversarial perturbations by applying each of the above mentioned attacks. We report
AUC-ROC on sets composed in equal parts of the test-set and test-set adversarial samples. The
AUC-ROC is computed by varying the threshold τ. See Appendix for model and dataset details.
The baselines are LID Ma et al. (2018), a recent detection based defense; KD; BU Feinman et al.
(2017); all trained on FGSM, as in Ma et al. (2018). The FGSM, BIM-a, BIM-b and JSMA attacks are
untargeted. We use published code for the attacks and code from Ma et al. (2018) for the baselines.
MNIST. We trained a 5-layer ConvNet to 99.2 ± 0.1% test-accuracy. The set of ∆xi ∈ R28×28 is
chosen at random, with each pixel perturbation chosen uniformly in [-ε, ε]. For each ∆xi, if x is
of label-class k, ∆yi,k ∈ R10 is chosen to be such that ∆yli6=,kk = -0.235 and ∆yli=,kk = 0.73, with
k∆yk2 = 1. The AUC-ROCs for the best N and ε using grid-search are reported in Table 4. We see
that NeuralFP achieves near-perfect detection with AUC-ROC of 99 - 100% across all attacks.
7
Under review as a conference paper at ICLR 2019
Data	Attack	Test Accuracy	Bound on Adversarial Perturbation η
MNIST	FGSM BIM-a	11.87% 0.00%	kηk∞ ≤ 0.4 kηk∞ ≤0.4
	BIM-b	0.00%	kηk∞ ≤0.4
	JSMA	1.73%	
	CW-L2	0.00%	
CIFAR	FGSM BIM-a	11.39 % 0.00%	kηk∞ ≤ 0.05 kηk∞ ≤ 0.05
	BIM-b	0.00%	kηk∞ ≤ 0.05
	JSMA	13.33%	
	CW-L2	0.00%	
MiniImagenet	FGSM	-100%-	kηk∞ ≤ 16/255 kηk∞ ≤ 16/255
	BIM-b	100%	
Table 2: Parameters and model test-accuracy on PW-
attacks for different datasets (without NeuralFP test). CW-
L2 and JSMA attacks are unbounded. The bounds are
relative to images with pixel intensities in [0, 1].
1.4
S 1 ■)
ω 1∙2
0.4
0.2
J 1.0
⊂
β∙0.8
Φ
6
E 0∙6
	* +	* +
+	+	Test
+	*	Adversarial
	+	
Figure 4: Fingerprint losses (mean over
N fingerprints) on 100 random test
(blue) and adversarial (red) CIFAR-10
images. We see a clear separation in
loss, illustrating that NeuralFP is effec-
tive across many thresholds τ .
Data	Method	FGSM	JSMA	BIM-a	BIM-b	CW-L2
	LID	99.68	96.36	99.05	99.72	98.66
MNIST	KD	81.84	66.69	99.39	99.84	96.94
	BU	27.21	12.27	6.55	23.30	19.09
	KD+BU	82.93	47.33	95.98	99.82	85.68
	NeuralFP	100.0	99.97	99.94	99.98	99.74
r^'T∏Λ ŋ In	LID	82.38	89.93	82.51	91.61	93.32
CIFAR-10	KD	62.76	84.54	69.08	89.66	90.77
	BU	71.73	84.95	82.23	3.26	89.89
	KD+BU	71.40	84.49	82.07	1.1	89.30
	NeuralFP	99.96	99.91	99.91	99.95	98.87
Table 4: Detection AUC-ROC of blackbox-defenders (do not know attack strategy) against partial-
whitebox-attackers (know model f(x; θ), but not defense details; see Section 2.1), on MNIST,
CIFAR-10 on test-set (“real”) and corresponding adversarial (“fake”) samples (1328 pre-test samples
each). NeuralFP outperforms baselines (LID, KD, BU) on MNIST & CIFAR-10 across attacks.
CIFAR-10. For CIFAR-10, we trained a 7-layer ConvNet (similar to (Carlini & Wagner, 2016)) to
85 ± 1% accuracy. The ∆xi and ∆yi,j are chosen similarly as for MNIST. Across attacks, NeuralFP
outperforms LID on average by 11.77% and KD+BU, KD, BU even more substantially (Table 4).
Even compared to LID-whitebox (where LID is aware of the attackers mechanism but NeuralFP is
not), NeuralFP outperforms LID-whitebox on average by 8.0% (Appendix, Table 10).
MiniImagenet-20. We also test on MiniImagenet-20 with
20 classes randomly chosen from the 100 MiniImagenet
classes (Vinyals et al., 2016) and trained an AlexNet network
on 10,600 images (not downsampled) with 91.1% top-1
accuracy. We generated test-set adversarial examples using
BIM-b with 50 steps (NIP) and FGSM. NeuralFP achieves
Data	FGSM BIM-b
MiniImagenet-20~~9996	99.68
Table 5: Detection AUC-ROC of Neu-
ralFP vs partial-whitebox-attacks on
MiniImagenet-20, N = 20, ε = 0.05.
AUC-ROCs of > 99.5% (Table 5). We could not get results for JSMA and CW-L2, which require
too much computation for tasks of this size. Results for other defenses are not reported due to time
constraints & unavailable implementations.
Visualization. Figure 4 shows that fingerprint-loss differs significantly for most test and adversarial
samples (across the 5 attacks in Table 2), enabling NeuralFP to achieve close to 100% AUC-ROC.
3.2	Detection of Adaptive-Whitebox Attacks
The strongest threat-model, AWA, is one where the adversary has access to the parameters of
NeuralFP. To evaluate whether NeuralFP is robust in this setting, or if it relies on gradient obfuscation
or obscurity (for a detailed analysis, see Section 4), we consider adaptive variants of FGSM, BIM-b,
CW-L2, and SPSA (Uesato et al., 2018). Under AWA, the attacker tries to find an adversarial example
8
Under review as a conference paper at ICLR 2019
Data Method AdaPtiVe-FGSM AdaPtiVe-BIM-b AdaPtiVe-CW-L2 AdaPtiVe-CW-L2 (γ2 = 1) AdaPtiVe-SPSA
MNIST	NeuralFP	99.91	99.37	95.04	99.17	99.94
CIFAR-10	NeuralFP	99.99	99.92	97.19	97.56	99.99
Table 6: Detection AUC-ROC for adaPtiVe attacks on datasets MNIST and CIFAR-10. Other
defenses such as (Song et al., 2018; Liao et al., 2017), including the baselines KD and BU, fail
under adaPtiVe-attacks (< 10% accuracy). For MNIST, the NeuralFP Parameters for FGSM, SPSA
are (ε, N) = (0.1, 10) and (ε, N) = (0.05, 20) for the BIM-b, CW-L2 attacks. For CIFAR-10, the
Parameters are set at (ε, N) = (0.003, 30) across attacks.
x0 that also minimizes the fingerPrint-loss (5), while attacking the model trained with NeuralFP. We
find that NeuralFP is robust across all AW-attacks, achieVing AUC-ROCs of 96-100% (See Table 6)
Adaptive-FGSM, Adaptive-BIM-b, Adaptive-SPSA For the FGSM, BIM-b and SPSA (untar-
geted) attacks we mount an adaPtiVe attack with a modified oPtimization objectiVe as in (Uesato et al.,
2018). SPecifically, for SPSA, the loss function to minimize is:
JadV(X,y*,θ) + YLfP(X0,y*,χ; θ),
where Jθadv is the original adVersarial objectiVe from (Uesato et al., 2018). For the gradient-based
FGSM and BIM-B attacks, we use gradients of the following loss function:
LCE(X, y, θ) - YLfP(X, y*,χ; θ),
where LCE (x, y*) is the cross-entropy loss. For each of the attacks and for each data-point, We
choose the largest Y ∈ [10-3,104] that results in a successful attack with a bisection search oVer Y 一
note that larger Y Values increase the Priority for minimizing Lfp . For the three adaPtiVe attacks, the
Perturbation bounds are kηk∞ ≤ 0.4 for MNIST and kηk∞ ≤ 0.05 for CIFAR-10.
Adaptive-CW-L2 We consider two adaPtiVe Variants of the CW-L2 attack. The first Variant we
consider is with the modified objectiVe function:
min ∣∣x - x0∣∣2 + γι (LCW (x0) + Mf (x0,y*,χ; θ)).	(12)
x0
Here, y* is the label-Vector, γι ∈ [10-3,106] and Y ∈ [10-3,104] are scalar coefficients, LfP is
the fingerPrint-loss we trained on and LCW is an objectiVe encouraging misclassification. To find
Y1 and Y2 we do a bisection search, first decreasing Y1 (as in Carlini & Wagner (2017a)) and then
increasing Y2 gradually in a similar manner. Note that increasing Y2 increases the imPortance giVen
to minimizing LfP. The successful attack with the smallest LfP during our search is chosen.
The second Variant is similar to the one considered in (Carlini & Wagner, 2017a). Here Y2 is held at
1.0 and the successful attack with the smallest ∣X - X0∣2 is chosen during a bisection search oVer Y1.
4	Discussion and Analysis of Empirical Results
Robustness Across Attacks. In addition to being robust to blackbox attacks, we find that NeuralFP
is robust across a set of adaPtiVe attacks. The fingerPrint loss Lfp is differentiable, and accordingly,
the gradient-free adaPtiVe attack (SPSA) does not Perform better than the gradient based ones (CW-L2,
BIM-b, FGSM). Other recent defenses such as (Song et al., 2018; Liao et al., 2017), including the
baselines KD and BU, were shown as not Viable (< 10% accuracy) under adaPtiVe attacks similar to
the ones we consider in our work (Carlini & Wagner, 2017a; Uesato et al., 2018).
AdVersarial training and robust oPtimization based defenses (Madry et al., 2017; Kannan et al.,
2018) that Perform robust Prediction show decaying accuracy with increasing adVersarial distortion.
HoweVer, NeuralFP shows robust Performance across Varying adVersarial distortion (See Figure 5).
A Possible exPlanation is that NeuralFP functions by rejecting regions that are further away from the
data-distribution (Figure 3). EmPirically, NeuralFP is less sensitiVe to small Perturbations and more
robust to larger distortions, and this is shown by the degradation in Performance against the adaPtiVe
BIM & CW-L2 attacks (which Produce significantly smaller distortions relatiVe to SPSA & FGSM).
9
Under review as a conference paper at ICLR 2019
99.5
100.0
oottlo⊃<
0.05	0.10	0.15	0.20	0.25
Adversarial Perturbation Magnitude Imlle
oott.o⊃<
FGSM	JSMA	BIM-A	BIM-B	CW-I2
Attacks
Figure 6: AUC-ROC mean μ and Standard-
deviation σ for 32 randomly sampled finger-
prints (including randomizing N ) for CIFAR-
10. The AUC-ROC across all PWAs varies little
(σ < 1%), with σ highest for CW-L2.
Figure 5: AUC-ROC with varying adversarial
distortion (300 pre-test images). Unlike robust
prediction(Madry et al., 2017; Kannan et al.,
2018), NeuralFP shows no performance decline
with increasing distortion (AUC-ROC >99%).
-fgsm
cw-12
-A- bim-a
bim-b
jsma
0.0025 0.003
0.006	0.01
ε
*aZ-----φ----∙
--*-- fgsm
cw-12
-A- bim-a
bim-b
jsma
(a)	Varying fingerprint magnitude ε
(N = 10)
5	10	15	20	25	30
Number of Fingerprint Directions
(b)	Varying no. fingerprints (N)
(ε = 0.01)
0.05	0.10	0.15	0.20	0.25	0.30	0.35	0«
False Positive Rate
(c) ROC Curve
Varying Threshold (τ)
Figure 7: AUC-ROC for different hyperparameters (left, middle) and ROC curves (right) on CIFAR-10
for partial-whitebox attacks. For analysis on MNIST, see Appendix. NeuralFP is robust across attacks
& hyperparameters with an AUC-ROC between 95 - 100%. Increasing N improves performance,
indicating more fingerprints are harder to fool. Increasing the magnitude ε decreases AUC on CW-L2
only, suggesting that as adversarial perturbations become of smaller magnitude, NeuralFP requires ε.
Obfuscating Gradients and Obscurity. Athalye et al. (2018) argued that several previous defenses
are vulnerable as they mask the true gradients (“gradient obfuscation”). A similar phenomenon
was observed in Uesato et al. (2018), where such defenses were successfully attacked (i.e., <10%
defense success rate) by exploiting their reliance on obscurity and masked gradients. In contrast, our
experiments suggest that NeuralFP does not rely on either obfuscating gradients or obscurity. First,
Athalye et al. (2018) argue that defenses that rely on obfuscating gradients likely perform worse on
iterative attacks (e.g., BIM) compared to non-iterative ones (e.g., FGSM). However, NeuralFP is
robust to both types of attack (Tables 4, 10). Second, NeuralFP is robust against gradient-free (e.g.,
SPSA) adaptive attacks, which suggests that it does not only rely on obscured gradients. Gradient-
free attacks can be used to accurately measure robustness in the presence of obscured gradients.
Finally, Carlini & Wagner (2017a); Uesato et al. (2018) argue that defenses that rely on obscurity are
vulnerable to adaptive attacks. However, NeuralFP is robust against a variety of adaptive attacks.
Sensitivity and Efficiency Analysis Next, we study the effect of changing N (number of finger-
print directions) and ε (magnitude of fingerprint-perturbation ∆x) on the AUC-ROC for CIFAR-10
(for analysis on MNIST, see the Appendix). Figure 7 and 9 show that NeuralFP performs well across
a wide range of hyperparameters and is robust to variation in the hyperparameters for PWAs. With
increasing ε, the AUC-ROC for CW-L2 decreases. As discussed before, a possible explanation is
that CW-L2 produces smaller adversarial perturbations than other attacks, and for larger fingerprint-
distortions ε, the fingerprints are less sensitive to those small adversarial perturbations. However, the
degradation in performance is not substantial (〜4 - 8%) as We increase ε over an order of magnitude.
With increasing N, the AUC-ROC generally increases across attacks. We conjecture that larger sets
of fingerprints can detect perturbations in more directions and results in better detection.
10
Under review as a conference paper at ICLR 2019
Figure 6 shows that NeuralFP achieves mean AUC-ROC of 98% - 100% against all PWA, with
standard deviation < 1%. This suggests that NeuralFP is not very sensitive to the chosen fingerprints.
5	Conclusion
Our experiments suggest that NeuralFP is an effective method for detecting the strongest known state-
of-the-art adversarial attacks, and the high AUC-ROC scores indicate that the fingerprints generalize
well to the test-set, but not to adversarial examples. Open questions are if there are attacks that can
fool NeuralFP or if it is provably robust to certain attacks. Learning the fingerprints during training,
and studying NeuralFP within a robust optimization framework are other interesting directions.
References
NIPS 2017: Non-targeted Adversarial Attack, url = https://www.kaggle.com/c/
nips-2017-non-targeted-adversarial-attack/, note = Accessed: 2017-02-07.
N. Akhtar and A. Mian. Threat of adversarial attacks on deep learning in computer vision: A survey.
IEEEAccess, 6:14410-14430, 2018. ISSN 2169-3536. doi:10.1109/ACCESS.2018.2807385.
Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models.
In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, pp. 1452-
1458. AAAI Press, 2016. URL http://dl.acm.org/citation.cfm?id=3016100.
3016102.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, July 2018. URL https://arxiv.org/abs/
1802.00420.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot
way to resist adversarial examples. International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=S18Su--CW.
N. Carlini and D. Wagner. Towards Evaluating the Robustness of Neural Networks. ArXiv e-prints,
August 2016.
Nicholas Carlini and David A. Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. CoRR, abs/1705.07263, 2017a. URL http://arxiv.org/abs/1705.
07263.
Nicholas Carlini and David A. Wagner. Magnet and "efficient defenses against adversarial attacks"
are not robust to adversarial examples. CoRR, abs/1711.08478, 2017b. URL http://arxiv.
org/abs/1711.08478.
Nicholas Carlini and David A. Wagner. Audio adversarial examples: Targeted attacks on speech-to-
text. CoRR, abs/1801.01944, 2018. URL http://arxiv.org/abs/1801.01944.
R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner. Detecting Adversarial Samples from
Artifacts. ArXiv e-prints, March 2017.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. CoRR, abs/1412.6572, 2014.
Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel.
Adversarial examples for malware detection. In Simon N. Foley, Dieter Gollmann, and Einar
Snekkenes (eds.), Computer Security - ESORICS 2017,pp. 62-79, Cham, 2017. Springer Interna-
tional Publishing. ISBN 978-3-319-66399-9.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. CoRR, abs/1502.01852, 2015. URL http:
//arxiv.org/abs/1502.01852.
11
Under review as a conference paper at ICLR 2019
R. Jia and P. Liang. Adversarial examples for evaluating reading comprehension systems. In Empirical
Methods in Natural Language Processing (EMNLP), 2017.
Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. CoRR,
abs/1803.06373, 2018. URL http://arxiv.org/abs/1803.06373.
J.	Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. CoRR, abs/1711.00851, 2017. URL http://arxiv.org/abs/1711.
00851.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
CoRR, abs/1607.02533, 2016. URL http://arxiv.org/abs/1607.02533.
K.	Lee, K. Lee, H. Lee, and J. Shin. A Simple Unified Framework for Detecting Out-of-Distribution
Samples and Adversarial Attacks. Advances in Neural Information Processing Systems 31,
December 2018.
Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Jun Zhu, and Xiaolin Hu. Defense against
adversarial attacks using high-level representation guided denoiser. CoRR, abs/1712.02976, 2017.
URL http://arxiv.org/abs/1712.02976.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Michael E. Houle, Dawn Song, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=B1gJ1L2aW. accepted as oral presentation.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards Deep Learning Models
Resistant to Adversarial Attacks. ArXiv e-prints, June 2017.
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. CoRR,
abs/1705.09064, 2017. URL http://arxiv.org/abs/1705.09064.
Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D. Joseph, Benjamin I. P. Rubinstein,
Udam Saini, Charles Sutton, J. D. Tygar, and Kai Xia. Exploiting machine learning to subvert your
spam filter. In Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent
Threats, LEET'08,pp. 7:1-7:9, Berkeley, CA, USA, 2008. USENIX Association. URL http:
//dl.acm.org/citation.cfm?id=1387709.1387716.
Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Anan-
thram Swami. The limitations of deep learning in adversarial settings. CoRR, abs/1511.07528,
2015. URL http://arxiv.org/abs/1511.07528.
Rama Chellappa Pouya Samangouei, Maya Kabkab. Defense-GAN: Protecting classifiers against
adversarial attacks using generative models. International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=BkJ3ibb0-.
A. Raghunathan, J. Steinhardt, and P. Liang. Certified Defenses against Adversarial Examples. ArXiv
e-prints, January 2018.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry.
Adversarially robust generalization requires more data. CoRR, abs/1804.11285, 2018. URL
http://arxiv.org/abs/1804.11285.
C. E. Shannon. Communication theory of secrecy systems. The Bell System Technical Journal, 28(4):
656-715, Oct 1949. ISSN 0005-8580. doi: 10.1002/j.1538-7305.1949.tb00928.x.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go
with deep neural networks and tree search. Nature, 529(7587):484-489, January 2016. doi:
10.1038/nature16961.
12
Under review as a conference paper at ICLR 2019
(w,x) -^-b> O
4殍in
k ：	i
ʌ : smin /
Δ^ι∖Y - /
{w,x} + b< O	/
〈到 z)+ b = O
一色3S
Figure 8: Geometry of fingerprints for SVMs with
linearly separable data. Let d(x) be the distance
of x to the decision boundary (see Thm 1). δ±max
(δ±min) denote the maximal (minimal) distances of
the positive (x+) and negative (x-) examples to
the separating hyperplane hw, xi + b = 0. The
fingerprint ∆x1 with h∆x1, ei = δ-min will have
f(x- + ∆x) < 0 and f(x-) < 0 for all x- in
the data distribution (red region). Hence, ∆x1 will
flag all x0 in the regions -δ-min < d(x0) < 0 as
“fake”, since for those x0 it will always see a change
in predicted class. Similarly, ∆x2 with h∆x2 , ei =
δ-max always sees a class change for real x-, thus
flagging all x0 with d(x0) < -δ-max as “fake”.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=rJUYGxbCW.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Jonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aaron van den Oord. Adversarial risk
and the dangers of evaluating against weak attacks. In Jennifer Dy and Andreas Krause (eds.),
Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pp. 5025-5034, Stockholmsmassan, Stockholm Sweden, 10-15
Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/uesato18a.html.
Oriol Vinyals, Charles Blundell, Tim Lillicrap, koray kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29,
pp. 3630-3638. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6385-matching-networks-for-one-shot-learning.pdf.
David Wagner and Paolo Soto. Mimicry attacks on host-based intrusion detection systems. In
Proceedings of the 9th ACM Conference on Computer and Communications Security, CCS ’02, pp.
255-264, New York, NY, USA, 2002. ACM. ISBN 1-58113-612-9. doi: 10.1145/586110.586145.
URL http://doi.acm.org/10.1145/586110.586145.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial
effects through randomization. International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=Sk9yuql0Z.
Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong
Yu, and Geoffrey Zweig. Achieving human parity in conversational speech recognition. CoRR,
abs/1610.05256, 2016. URL http://arxiv.org/abs/1610.05256.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. In Network and Distributed Systems Security Symposium (NDSS) 2018, volume
abs/1704.01155, 2018.
Appendix A	Proof for Theorem 1
Proof. Also see Figure 8. Consider any perturbation η = λe that is positively aligned with w, and
has hη, ei = δ-min. Then for any negative example (x-, -1) (except for the support vectors that lie
exactly δ-min from the hyperplane), adding the perturbation η does not change the class prediction:
signf(x-) = -1,	signf(x- -η) = -1.	(13)
13
Under review as a conference paper at ICLR 2019
Layer	Parameters
Convolution + ReLU + BatchNorm	11	× 11 × 64
MaxPool Convolution + ReLU + BatchNorm	5	3×3 × 5 × 192
MaxPool Convolution + ReLU + BatchNorm	3	3×3 × 3 × 384
MaxPool Convolution + ReLU + BatchNorm	3	3×3 × 3 × 256
MaxPool Convolution + ReLU + BatchNorm	3	3×3 × 3 × 156
MaxPool Fully Connected + ReLU + BatchNorm Dropout Fully Connected + ReLU + BatchNorm Dropout Softmax		3×3 3072 - 1024 - 20
Table 7: MiniImagenet-20 Model Used
Layer	Parameters
Convolution + ReLU + BatchNorm	5× 5 ×32
MaxPool	2×2
Convolution + ReLU + BatchNorm	5 × 5 × 64
MaxPool	2×2
Fully Connected + ReLU + BatchNorm	200
Fully Connected + ReLU + BatchNorm	200
Softmax	10
Table 8: MNIST Model Used
The fingerprint in (6) is an example of such an η . However, if λ is large enough, that is:
hη, ei = δ-max,	(14)
(e.g. the fingerprint in (7)), for all negative examples (x- , -1) the class prediction will always
change (except for the x- that lie exactly δ-max from the hyperplane):
signf(x-) = -1,	signf(x- +η) = +1,	(15)
Note that if η has a component smaller (or larger) than δ±min , it will exclude fewer (more) examples,
e.g. those that lie closer to (farther from) the hyperplane. Similar observations hold for fingerprints
(8) and (9) and the positive examples x+. Hence, it follows that for any x that lies too close to the
hyperplane (closer than δ±min), or too far (farther than δ±max), the model output after adding the four
fingerprints will never perfectly correspond to their behavior on examples x from the data distribution.
For instance, for any x that is closer than δ+min to the hyperplane, (9) will always cause a change in
class, while none was expected. Similar observations hold for the other regions in (10). Since the
SVM is translation invariant parallel to the hyperplane, the fingerprints can only distinguish examples
based on their distance perpendicular to the hyperplane. Hence, this choice of λs is optimal.	□
Appendix B	Randomized Fingerprints
Instead of simple ∆yi,j , we can encode more complex fingerprints that are harder to guess for an
adversary. For instance, we were able to train a network on CIFAR-10 using random ∆yi,j: for each
∆xi, ifx is in class k:
∆yli6=,kk = -0.235p, ∆yli=,kk = 0.7p.	(16)
14
Under review as a conference paper at ICLR 2019
Layer	Parameters
Convolution + ReLU + BatchNorm	3	× 3 × 32
Convolution + ReLU + BatchNorm	3	× 3 × 64
MaxPool		2×2
Convolution + ReLU + BatchNorm	3	× 3 × 128
Convolution + ReLU + BatchNorm	3	× 3 × 128
MaxPool		2×2
Fully Connected + ReLU + BatchNorm		256
Fully Connected + ReLU + BatchNorm		256
Softmax		10
Table 9: CIFAR Model Used
Here p is a random variable with Pr(p = 1) = 0.5 and Pr(p = -1) = 0.5 that is resampled
for each ∆xi, making it prohibitively hard for a brute-force attacker to guess. For this NFP, we
achieve AUC-ROCs of > 95% across attacks with N = 30, ε = 0.05, without extensive tuning. The
high model-capacity of neural networks allows to learn such complex patterns, that can be hard to
reverse engineer. This also indicates that the proposed approach is robust and the specific choice
of fingerprints or the distributions they are sampled from do not actually influence the detection
performance to a large extent.
Appendix C	Models for Evaluation
Note: Code for CW-adaptive is based on code fromhttps://github.com/carlini/nn
robust_attacks. Code for the other attacks was obtained from the paper (Ma et al., 2018).
C.1 MNIST
For MNIST, we use the model described in Table 8.
C.2 CIFAR- 1 0
For CIFAR-10, we use the model described in Table 9
C.3 MiniImagenet-20
MiniAlexNet Model We use a model similar to AlexNet for MiniImagenet-20. The model used is
described in Table 7
MiniImagenet-20 classes We use images from the following 20 ImageNet classes for our
experiments:
n01558993,
n02110063,
n02443484,
n02795169,
n02950826,
n02981792,
n03062245,
n03146219,
n03207743,
n03272010,
n03400231,
n03476684,
n03980874,
n04146614,
n04443257,
n04515003
n04612504,
n07697537
15
Under review as a conference paper at ICLR 2019
Data	Method	FGM	JSMA	BIM-a	BIM-b	CW-L2
MNIST	LID	99.68	98.67	99.61	99.90	99.55
	NeuralFP	100.0	99.97	99.94	99.98	99.74
CIFAR-10	LID	82.38	95.87	82.30	99.78	98.94
	NeuralFP	99.96	99.91	99.91	99.95	98.87
Table 10: DeteCtion AUC-ROC for NeuralFP,whitebox-LID against whitebox-attackers (know model
f(x; θ), but not fingerprints; see SeCtion 2.1), on MNIST, CIFAR-10 tasks on test-set (“real”) and
Corresponding adversarial (“fake”) samples (1328 pre-test samples eaCh). NeuralFP outperforms
the baselines (LID, KD, BU) on MNIST and CIFAR-10 aCross all attaCks, exCept CW-L2 where
it performs Comparably. A possibly explanation for LID’s improved performanCe against stronger,
iterative attaCks is gradient masking Athalye et al. (2018).
Appendix D	Sensitivity analysis
φ⅛uw uo^.u⊃<
--*-- fgsm
cw-12
-A- bim-a
bim-b
-♦- jsma
φ⅛⅛ 00^--0⊃<
4	6 B 10	12	14
Number of Finaerorint Directions
(b)	Varying N (ε = 0.03)
0.03	0.06	0.1	0.3
ε
(a)	Varying ε (N = 10)
0.05	0.10	0.15	0.20	0.25	0.30	0.35	0.40
False Positive Rate
(C) ROC Curve
Figure 9: AUC-ROC performance for different hyperparameter settings (left, middle) and ROC
Curves (right) on MNIST. We see that the performanCe of NeuralFP is robust aCross attaCks and
hyperparameters, with the AUC-ROC between 90 - 100% for most settings. The AUC-ROC is lowest
versus CW-L2, whiCh is one of the strongest known attaCk.
APPENDIX E	NeuralFP VS WHITEBOX-LID
We also Compare LID against NeuralFP in the setting where LID is aware of the attaCk meChanism
and trains against the speCifiC attaCk meChanism, while NeuralFP still does not use any attaCker
information(Table 9). In this unfair setting, we see LID performs Comparably on CW-L2, and
NeuralFP outperforms LID against all other attaCks. Athalye et al. (2018) provides a possible
explanation for this behavior for LID, where defenses relying on obfusCated gradients perform better
against stronger attaCks (CW-L2) Compared to one-step weaker attaCks.
APPENDIX F	EVALUATING NeuralFP AGAINST BLACKBOX ATTACKS
In Athalye et al. (2018), the authors indiCate that testing against blaCkbox attaCks is useful to gauge
if the defense is relying on gradient masking. To explore this aspeCt of our defense, we ConstruCt
adversarial examples for models trained without fingerprinting and then test the ability of fingerprinted
models to distinguish these from unseen test-data. We find that the performanCe does not degrade
from the whitebox-attaCk setting, in Contrast to other defenses evaluated in Athalye et al. (2018),
where the performanCe degrades against blaCkbox-attaCks.
Appendix G	Evaluated Attack Hyperparameters
Table 12 summarizes the hyperparameter settings Corresponding to the experiments in Tables 4,6.
16
Under review as a conference paper at ICLR 2019
Data	Method FGM
MNIST	NeuralFP^^9996
CIFAR-10^^NeuralFP^^9992
Table 11: Detection AUC-ROC for NeuralFP against blackbox-attackers (know dataset but not
model or fingerprints), on MNIST, CIFAR-10 tasks on test-set (“real”) and corresponding blackbox
adversarial (“fake”) samples (1328 pre-test samples each). NeuralFP achieves near perfect AUC-
ROC scores in this setting against FGM. Iterative attacks are known to not transfer well in the
blackbox setting. For CIFAR-10, the hyperparameters are N = 30, ε = 0.003 and for MNIST, the
hyperparameters are N = 10, ε = 0.03. We did not tune the parameters because this setting in itself
achieved near perfect detection rates.
Method	Parameters
CW-L - 2	Bisection-steps (γι)=9, Bisection-steps (γ2)=5, Max Iteration Steps = 1000, L2_ABORT_EARLY = True, L2_LEARNING_RATE = 1e-2, L2_TARGETED = True, L2_CONFIDENCE = 0, L2_INITIAL_CONST = 1e-3 (γ1 initial value) L2_INITIAL_CONST_2 = 0.1 (γ2 initial value)
SPSA	Bisection steps=6, Upper-bound for bisection = 50.0, spsa-iters=1, spsa-samples=128, Lower-bound=0.001 Iterations = 100, lr=0.01,dr=0.01
BIM (MNIST) BIM (CIFAR)	eps-iter=0.010 eps-iter=0.005 Table 12: Attack hyperparameters corresponding to Tables 4,6.
Appendix H
Adaptive-PGD
To further investigate NeuralFP’s dependence on gradient obfuscation, we evaluate the detection
performance on l0 and l2 variants of an adaptive attack with projected gradient descent (PGD) (Madry
et al., 2017) as the optimization method. The adaptive attack is set-up in a similar manner to adaptive-
FGSM. PGD attack fails across all sets of hyperparameters explored (See Table 13). Even at distortion
bounds of kηk∞ ≤ 1.0 and kηk2 ≤ 60.0 PGD does not succeed in degrading the performance of
NeuralFP. Obviously, at these large distortion bounds (unbounded), adversarial examples exist as
part of the training dataset itself. This indicates that the optimization procedure is not performing
optimally, and it remains an open question if optimization procedures can be developed to render
NeuralFP vulnerable.
Method	Variant	Distortion Bound	Restarts	Iters	eps-iter	bisection steps	AUC-ROC (%)
PGD	l0	16/255	1	1000	0.005	6	99.74
PGD	l0	16/255	50	50	0.005	6	99.21
PGD	l0	0.25	1	1000	0.005	6	99.71
PGD	l0	1.0	5	150	0.005	6	99.48
PGD	l2	10.0	5	100	0.5	6	99.55
PGD	l2	60.0	5	150	0.5	6	99.37
Table 13: The detection AUC-ROC for NeuralFP against adaptive-PGD (112 pre-test samples each).
NeuralFP achieves near perfect AUC-ROC scores across settings. For CIFAR-10, the hyperparameters
for NeuralFP are N = 30, ε = 0.003.
APPENDIX I ADAPTIVE-CW-L2 - MORE ITERATIONS, MORE BISECTION STEPS
As a sanity check to see if the defense only succeeds because of the limited number of iteration
steps during our preliminary investigation, we run Adaptive-CW-L2 for 20000 iteration steps with
20 bisection search steps for the value of γ1 and 10 bisection search steps for the value of γ2. For a
batch of 16 samples, this takes approximately 10 hours on an NVIDIA Tesla K80 for CIFAR-10 with
N = 30. We find that even with these large number of iterations/bisection steps, NeuralFP is quite
robust and the AUC-ROC does not degrade substantially. We evaluate Adaptive-CW-L2 with these
hyperparameters for two sets of NeuralFP hyperparameters. Table 14 summarizes the results from
these experiments. Note that since the number of samples for these evaluations is quite small, the
17
Under review as a conference paper at ICLR 2019
AUC-ROC often fluctuates a few percent depending on the set of samples randomly chosen from
the unseen test-set. Increasing the number of iterations causes a drop in AUC-ROC of about 2-4%
when moving from 1000 iteration steps to 20000 steps. For NeuralFP with (N = 50, ε = 0.003),
the AUC-ROC corresponding to the settings in Table 12 is 96.2%. An interesting observation is that
the perturbation size (mean l2 distortion) seems to grow with the number of increased iteration steps.
For the vanilla CW-L2 attack (non-adaptive), the mean l2 distortion is in the range 0.15 - 0.25. For
further comparison, the defense in (Madry et al., 2017) has a robust-prediction accuracy of < 5% at a
l2 distortion of 0.45 when evaluated against PGD.
NeuralFP Parameters	Iteration Steps	Bisection Steps (γ1)	Bisection Steps (γ2)	Mean l2 Distortion Size	Number of samples	AUC-ROC
N = 30, ε =	0.003	20000	20	^T0	0.47	112	94.12
N = 30, ε =	0.003	15000	15	9	0.31	64	95.48
N = 50, ε =	0.003	20000	20	10	0.45	64	95.56
Table 14: The detection AUC-ROC for NeuralFP against Adaptive-CW-L2 with varying hyperparam-
eter settings.
Appendix J Adaptive-SPSA with 1000 iterations
For SPSA we investigate if increasing the number of iterations degrades performance for 3 sets of
hyperparameters (See Table 15) with randomly sampled 112 unseen test-samples. We find that even
with increased iterations the detection performance largely remains unaffected.
Iteration Steps	Bisection Steps	Distortion Bound	Learning Rate	Delta	AUC-ROC (%)
1000	20	-005	0.01	0.01	99.84
1000	20	0.05	0.005	0.005	99.71
1000	20	0.25	0.01	0.01	99.32
Table 15: The detection AUC-ROC for NeuralFP against SPSA with varying hyperparameter settings.
Appendix K Randomized sampling to check for gradient
OBFUSCATION
A test suggested in (Athalye et al., 2018) to check for gradient-masking is sampling randomly in
-balls around unseen test-data to find adversarial examples. They recommend sampling 105 points or
more, and checking if any of these points can pass through the defense. Accordingly, for 100 MNIST
test samples, we sample 106 points uniformly from the l∞-ball with kηk∞ ≤ 0.3 and for 50 CIFAR-
10 test samples, we sample 5 × 105 points uniformly from the l∞-ball with kηk∞ ≤ 0.25. Amongst
these randomly sampled points for each test point, we compute the smallest min (Lfp (x, i, ξ; θ)) (let
Us call this value Lfp) across points that are adversarial (i.e. cause a misclassification). Note that Lfp
for each test point is the minimum fingerprint-loss over all adversarial samples sampled around the
point and across all labels. For CIFAR-10, we are able to find adversarial examples for 47 points and
for MNIST, we are able to find adversarial examples for 52 points. Figure 10 shows the distribution
of Lfp across the test points and Lfp across the corresponding randomly sampled adversarial points.
We observe that for MNIST, one single test sample has a high fingerprint-loss while all adversarial
samples have high fingerprint-losses. The remaining test-samples have fingerprint-losses that are
roughly 3 - 30 times smaller than the adversarial examples. For CIFAR-10, the test and adversarial
points are well separated, with most test samples having losses significantly smaller (roughly 10
times smaller) than the randomly sampled adversarial examples. This indicates that it is likely that
NeuralFP does not simply function by misleading the gradient based attacks.
18
Under review as a conference paper at ICLR 2019
50
40
302010
sφ-dEes Jo -φqEnN
3	4	5	6	7	8
⅛ (Fingerprint Loss)
50
40
Randomly Sampled Adversarial Examples
Correctly Classified Clean Examples
sφ-dEes Jo -φqEnN
(b) CIFAR-10
Figure 10: Histograms depicting the distribution of losses for randomly sampled adversarial examples
and test-data. Randomly sampled adversarial examples are well separated from unseen test examples.
For NeuralFP, hyperparameters are (ε, N) = (0.1, 10) and (ε, N) = (0.003, 30) for MNIST and
CIFAR-10 respectively.
(a) MNIST
19