Under review as a conference paper at ICLR 2019
Logically-Constrained Neural Fitted Q-iteration
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes a method for efficient training of Q-function for continuous-
state Markov Decision Processes (MDP), such that the traces of the resulting
policies satisfy a Linear Temporal Logic (LTL) property. LTL, a modal logic, can
express a wide range of time-dependent logical properties including safety and
liveness. We convert the LTL property into a limit deterministic Buchi automaton
with which a synchronized product MDP is constructed. The control policy is
then synthesised by a reinforcement learning algorithm assuming that no prior
knowledge is available from the MDP. The proposed method is evaluated in a
numerical study to test the quality of the generated control policy and is com-
pared against conventional methods for policy synthesis such as MDP abstraction
(Voronoi quantizer) and approximate dynamic programming (fitted value iteration).
1	Introduction
Markov Decision Processes (MDPs) are extensively used as a family of stochastic processes in
automatic control, computer science, economics, etc. to model sequential decision-making problems.
Reinforcement Learning (RL) is a machine learning algorithm that is widely used to train an agent
to interact with an MDP when the stochastic behaviour of the MDP is initially unknown. However,
conventional RL is mostly focused on problems in which MDP states and actions are finite. Nonethe-
less, many interesting real-world tasks, require actions to be taken in response to high-dimensional
or real-valued sensory inputs (Doya, 2000). For example, consider the problem of drone control in
which the drone state is represented as its Euclidean position (x, y, z) ∈ R3.
Apart from state space discretisation and then running vanilla RL on the abstracted MDP, an alternative
solution is to use an approximation function which is achieved via regression over the set of samples.
At a given state, this function is able to estimate the value of the expected reward. Therefore, in
continuous-state RL, this approximation replaces conventional RL state-action-reward look-up table
which is used in finite-state MDPs. A number of methods are available to approximate the expected
reward, e.g. CMACs (Sutton, 1996), kernel-based modelling (Ormoneit & Sen, 2002), tree-based
regression (Ernst et al., 2005), basis functions (Busoniu et al., 2010), etc. Among these methods,
neural networks offer great promise in reward modelling due to their ability to approximate any
non-linear function (Hornik, 1991). There exist numerous successful applications of neural networks
in RL for infinite or large-state space MDPs, e.g. Deep Q-networks (Mnih et al., 2015), TD-Gammon
(Tesauro, 1995), Asynchronous Deep RL (Mnih et al., 2016), Neural Fitted Q-iteration (Riedmiller,
2005), CACLA (Van Hasselt & Wiering, 2007).
In this paper, we propose to employ feedforward networks (multi-layer perceptrons) to synthesise
a control policy for infinite-state MDPs such that the generated traces satisfy a Linear Temporal
Logic (LTL) property. LTL allows to specify complex mission tasks in a rich time-dependent
formal language. By employing LTL we are able to express complex high-level control objectives
that are hard to express and achieve for other methods from vanilla RL (Sutton & Barto, 1998;
Smith et al., 2011) to more recent developments such as Policy Sketching (Andreas et al., 2017).
Examples include liveness and cyclic properties, where the agent is required to make progress while
concurrently executing components, to take turns in critical sections or to execute a sequence of tasks
periodically. The purpose of this work is to show that the proposed architecture efficiently performs
and is compatible with RL algorithms that are core of recent developments in the community.
Unfortunately, in the domain of continuous-state MDPs, to the best of our knowledge, no research has
been done to enable RL to generate policies according to full LTL properties. On the other hand, the
1
Under review as a conference paper at ICLR 2019
problem of control synthesis in finite-state MDPs for temporal logic has been considered in a number
of works. In (Wolff et al., 2012), the property of interest is an LTL property, which is converted to a
Deterministic Rabin Automaton (DRA). A modified Dynamic Programming (DP) algorithm is then
proposed to maximise the worst-case probability of satisfying the specification over all transition
probabilities. Notice that in this work the MDP must be known a priori. (Fu & Topcu, 2014) and
(Brazdil et al., 2014) assume that the given MDP has unknown transition probabilities and build a
Probably Approximately Correct MDP (PAC MDP), which is producted with the logical property
after conversion to DRA. The goal is to calculate the value function for each state such that the value
is within an error bound of the actual state value where the value is the probability of satisfying
the given LTL property. The PAC MDP is generated via an RL-like algorithm and standard value
iteration is applied to calculate the values of states.
Moving away from full LTL logic, scLTL is proposed for mission specification, with which a linear
programming solver is used to find optimal policies. The concept of shielding is employed in
(Alshiekh et al., 2017) to synthesise a reactive system that ensures that the agent stays safe during
and after learning. However, unlike our focus on full LTL expressivity, (Alshiekh et al., 2017)
adopted the safety fragment of LTL as the specification language. This approach is closely related to
teacher-guided RL (Thomaz & Breazeal, 2008), since a shield can be considered as a teacher, which
provides safe actions only if absolutely necessary. The generated policy always needs the shield to
be online, as the shield maps every unsafe action to a safe action. Almost all other approaches in
safe RL either rely on ergodicity of the underlying MDP, e.g. (Moldovan & Abbeel, 2012), which
guarantees that any state is reachable from any other state, or they rely on initial or partial knowledge
about the MDP, e.g. (Song et al., 2012) and (Lope & Martin, 2009).
2	Background
2.1	Problem Framework
Definition 2.1 (Continuous-state Space MDP) The tuple M = (S, A, s0, P, AP, L) is an MDP
over a set of states S = Rn, where A is a finite set of actions, s0 is the initial state and P :
B(Rn) × S × A → [0, 1] is a Borel-measurable transition kernel which assigns to any state and any
action a probability measure on the Borel space (Rn, B(Rn)) (Durrett, 1999). AP is a finite set of
atomic propositions and a labelling function L : S → 2AP assigns to each state s ∈ S a set of atomic
propositions L(s) ⊆ 2AP Durrett (1999).	y
A finite-state MDP is a special case of continuous-state space MDP in which |S| < ∞ and P :
S × A × S → [0, 1] is the transition probability function. The transition function P induces a matrix
which is usually known as transition probability matrix in the literature.
Theorem 2.1 In any MDP M with bounded reward function and finite action space, if there exists an
optimal policy, then that policy is stationary and deterministic. (Puterman, 2014) (Cavazos-Cadena
et al., 2000).	y
An MDP M is said to be solved if the agent discovers an optimal policy Pol* : S → A to maximize
the expected reward. From Definitions A.3 and A.4 in Appendix, it means that the agent has to take
actions that return the highest expected reward. Note that the reward function for us as the designer
is known in the sense that we know over which state (or under what circumstances) the agent will
receive a given reward. The reward function specifies what the agent needs to achieve but not how
to achieve it. Thus, the objective is that the agent itself comes up with an optimal policy. In the
supplementary materials in Section A.2, we present fundamentals of approaches introduced in this
paper for solving infinite-state MDPs.
3	Linear Temporal Logic Properties
In order to specify a set of desirable constraints (i.e. properties) over the agent policy we employ
Linear Temporal Logic (LTL) (Pnueli, 1977). An LTL formula can express a wide range of properties,
such as safety and persistence. LTL formulas over a given set of atomic propositions AP are
2
Under review as a conference paper at ICLR 2019
syntactically defined as
φ ::= true | α ∈ AP | φ ∧ φ | —夕 | Q 夕 | 夕 ∪ 夕.
(1)
We define the semantics of LTL formula next, as interpreted over MDPs. Given a path ρ, the i-th
state of ρ is denoted by ρ[i] where ρ[i] = si. Furthermore, the i-th suffix of ρ is ρ[i..] where
ρ[i..] = si -a→i si+1
ai+1
---→ si+2
ai+2
ai+3
si+3 ---→
Definition 3.1 (LTL Semantics) For an LTLformula φ and for a path ρ,the satisfaction relation
Pi=夕 is defined as
Pi= a ∈ AP ^⇒ a ∈ L(ρ[0])	P I= Q夕 ^⇒ ρ[1..] I= φ
Pl=2 1	∧ 22	Q⇒ Pl=2 1 ∧ P =22	Pl=2 1 ∪ 22 Q⇒	∃j	∈ N ∪	{θ}	S.t.
P = -2 y⇒ P = 2	P[j..]=22 & ∀i, 0 ≤ i < j, P[i..]=2 1
Using the until operator We are able to define two temporal modalities: (1) eventually, ♦夕=true∪ 夕;
and (2) always, 白夕 = -♦-夕.LTL extends propositional logic with the temporal modalities until ∪,
eventually ♦, and always . For example, in a robot control problem, statements such as “eventually
get to this point” or “always stay safe” are expressible by these modalities and can be combined via
logical connectives and nesting to provide general and complex task specifications. Any LTL task
specification 夕 over AP expresses the following set of words: Words(夕)={σ ∈ (2AP)ω s.t. σ =
ψ}.
Definition 3.2 (Policy Satisfaction) We say that a stationary deterministic policy Pol satisfies an
LTLformula 夕 if:
P[L(s0)L(s1)L(s2)... ∈ Words(φ)] > 0,
where every transition si → si+1, i = 0, 1, ... is constructed by taking action Pol (si) at state si. y
For an LTL formula 夕，an alternative method to express the set of associated words, i.e., Words(φ),
is to employ an automaton. Limit Deterministic Buchi Automatons (LDBA) (Sickert et al., 2016) are
one of the most succinct and simplest automatons for that purpose (Sickert & Kretlnsky, 2016). We
need to first define a Generalized Buchi Automaton (GBA) and then we formally introduce an LDBA.
Definition 3.3 (Generalized Buchi Automaton) A GBA N = (Q, q0, Σ, F, ∆) is a structure where
Q is a finite set of states, q0 ⊆ Q is the set of initial states, Σ = 2AP is a finite alphabet, F =
{F1, ..., Ff} is the set of accepting conditions where Fj ⊂ Q, 1 ≤ j ≤ f, and ∆ : Q × Σ → 2Q is a
transition relation.	y
Let Σω be the set of all infinite words over Σ. An infinite word w ∈ Σω is accepted by a GBA N
if there exists an infinite run θ ∈ Qω starting from q0 where θ[i + 1] ∈ ∆(θ[i], ω[i]), i ≥ 0 and for
each Fj ∈ F
inf (θ) ∩ Fj = 0,	⑵
where inf (θ) is the set of states that are visited infinitely often in the sequence θ.
Definition 3.4 (LDBA) A GBA N = (Q, q0 , Σ, F, ∆) is limit deterministic if Q can be partitioned
into two disjoint sets Q = QN ∪ QD, such that (Sickert et al., 2016):
•	∆(q, α) ⊆ QD and l∆(q, α)l = 1 for every state q ∈ QD and for every corresponding
α ∈ Σ,
•	for every Fj ∈ F, Fj ⊂ QD.	y
An LDBA is a GBA that has two partitions: initial (QN) and accepting (QD). The accepting part
includes all the accepting states and has deterministic transitions.
4	Logically-Constrained Neural Fitted Q-iteration
In this section, we propose an algorithm based on Neural Fitted Q-iteration (NFQ) that is able
to synthesize a policy that satisfies a temporal logic property. We call this algorithm Logically-
Constrained NFQ (LCNFQ). We relate the notion of MDP and automaton by synchronizing them to
create a new structure that is first of all compatible with RL and second that embraces the logical
property.
3
Under review as a conference paper at ICLR 2019
Definition 4.1 (Product MDP) Given an MDP M(S, A, s0 , P, AP, L) and an LDBA
N (Q,qo, ∑, F, ∆) with Σ = 2AP ,the product MDP is defined as (M 0 N) = MN (S 巴 A,
s『，P巴 AP0, L0, F0), where S宏=S X Q, Sj = (so, qo), APe) = Q, L = S X Q → 2q such that
L® (s,q)= q and F0 ⊆ S0 is the set ofaccepting states such thatfor each s0 = (s,q) ∈ F0, q ∈ F.
The intuition behind the transition kernel P0 is that given the current state (si, qi) and action a the
new state is (sj,qj) where Sj 〜P(∙∣Si, a) and q§ ∈ ∆(qi, L(Sj)).	y
By constructing the product MDP we add an extra dimension to the state space of the original MDP.
The role of the added dimension is to track the automaton state and, hence, to synchronize the current
state of the MDP with the state of the automaton and thus to evaluate the satisfaction of the associated
LTL property.
Definition 4.2 (Absorbing Set) We define the set A ∈ B(S0) to be an absorbing set if
P0(A|S0, a) = 1 for all S0 ∈ A and for all a ∈ A. An absorbing set is called accepting if
it includes F0. We denote the set of all accepting absorbing sets by A.	y
Note that the defined notion of absorbing set in continuous-state MDPs is equivalent to the notion
of maximum end components in finite-state MDPs. In another word, once a trace ends up in an
absorbing set (or a maximum end component) it can never escape from it (Tkachev et al., 2017).
The product MDP encompasses transition relations of the original MDP and the structure of the
Buchi automaton, thus it inherits characteristics ofboth. Therefore, a proper reward function can lead
the RL agent to find a policy that is optimal and that respects both the original MDP and the LTL
property 夕.In this paper, We propose an on-the-fly random variable reward function that observes the
current state S0, the action a and observes the subsequent state S00 and gives the agent a scalar value
according to the following rule:
R(S0, a)
if S0 ∈/ A, S00 ∈ A,
otherwise,
(3)
where rp = M + y X m X rand(S0) is a positive reward and rn = y X m X rand (S0) is a neutral
reward where y ∈ {0, 1} is a constant, 0 < m M, and rand : S0 → (0, 1) is a function that
generates a random number in (0, 1) for each state S0 each time R is being evaluated. The role of
function rand is to break the symmetry in LCNFQ neural nets. Note that parameter y essentially acts
as a switch to bypass the effect of the rand function on R. As we will see later, this switch is only
active for LCNFQ.
In LCNFQ, the temporal logic property is initially specified as a high-level LTL formula 夕.The LTL
formula is then converted to an LDBA N to form a product MDP MN (see Definition 4.1). In order
to use the experience replay technique we let the agent explore the MDP and reinitialize it when a
positive reward is received or when no positive reward is received after th iterations. The parameter
th is set manually according to the MDP such that allows the agent to explore the MDP and also to
prevent the sample set to explode in size. All episode traces, i.e. experiences, are stored in the form
of (S0, a, S00, R(S0, a), q). Here S0 = (S, q) is the current state in the product MDP, a is the chosen
action, S00 = (S0, q0) is the resulting state, and R(S0, a) is the reward. The set of past experiences is
called the sample set E.
Once the exploration phase is finished and the sample set is created, we move forward to the learning
phase. In the learning phase, we employ n separate multi-layer perceptrons with just one hidden layer
where n = |Q| and Q is the finite cardinality of the automaton N. Each neural net is associated with
a state in the LDBA and together the neural nets approximate the Q-function in the product MDP.
For each automaton state qi ∈ Q the associated neural net is called Bqi : S0 X A → R. Once the
agent is at state S0 = (S, qi) the neural net Bqi is used for the local Q-function approximation. The
set of neural nets acts as a global hybrid Q-function approximator Q : S0 X A → R. Note that the
neural nets are not fully decoupled. For example, assume that by taking action a in state S0 = (S, qi)
the agent is moved to state S00 = (S0, qj) where qi 6= qj. According to (13) the weights of Bqi
are updated such that Bqi (S0, a) has minimum possible error to R(S0, a) + γ maxa0 Bqj (S00, a0).
Therefore, the value of Bqj (S00, a0) affects Bqi (S0, a).
Let qi ∈ Q be a state in the LDBA. Then define Eqi := {(∙, ∙, ∙, ∙,x) ∈ E|x = q# as the set of
experiences within E that is associated with state qi, i.e., Eqi is the projection of E onto qi. Once the
4
Under review as a conference paper at ICLR 2019
Algorithm 1: LCNFQ
input : MDP M, a set of transition samples E
output : Approximated Q-function
1	initialize all neural nets Bqi with (s0, qi, a) as the input and rn as the output where a ∈ A is a random
action
2	repeat
3	for qi = |Q| to 1 do
4	Pqi = {(inputl , targetl ), l = 1, ..., |Eqi |)}
5	inputι = (S 产，aι)
6	targetι = R(sι0,aι) + YmaxQ(S产，a0)
a0
7	Where(S产，aι,s产0,R(s产，aι),qi) ∈ Eqi
8	Bqi - RProP(Pqi)
9	end
10	until end of trial
exPerience set E is gathered, each neural net Bqi is trained by its associated exPerience set Eqi . At
each iteration a Pattern set Pqi is generated based on Eqi :
Pqi = {(inputl , targetl ), l = 1, ..., |Eqi |)},
where input =	(Sl0,aι) and target =	R(Sl^),aι) + YmaxQ(Sl00, a0) such that
a0
(Sl室,al, sl0 , R(Sl0, al), qi) ∈ Eqi. The pattern set is used to train the neural net Bqi. We use
RProP (Riedmiller & Braun, 1993) to uPdate the weights in each neural net, as it is known to be a fast
and efficient method for batch learning (Riedmiller, 2005). In each cycle of LCNFQ (Algorithm 1),
the training schedule starts from networks that are associated with accePting states of the automaton
and goes backward until it reaches the networks that are associated to the initial states. In this way
we allow the Q-value to back-ProPagate through the networks. LCNFQ stoPs when the generated
Policy satisfies the LTL ProPerty and stoPs imProving for long enough.
Remark 4.1 We tried different embeddings such as one hot encoding (Harris & Harris, 2010)
and integer encoding in order to approximate the global Q-function with a single feedforward net.
However, we observed poor performance since these encoding allows the network to assume an
ordinal relationship between automaton states. Therefore, we turned to the final solution of employing
n separate neural nets that work together in a hybrid manner to approximate the global Q-function.y
Recall that the reward function (3) only returns a Positive value when the agent has a transition to an
accePting state in the Product MDP. Therefore, if accePting states are reachable, by following this
reward function the agent is able to Come up with a policy Pol0 that leads to the accepting states.
This means that the trace of read labels over S (see Definition 4.1) results in an automaton state to
be accepting. Therefore, the trace over the original MDP is a trace that satisfies the given logical
property. Recall that the optimal policy has the highest expected reward comparing to other policies.
Consequently, the optimal policy has the highest expected probability of reaching to the accepting
set, i.e. satisfying the LTL property.
The next section studies state space discretization as the most popular alternative approach to solving
infinite-state MDPs.
5	Voronoi Quantizer
Inspired by (Lee & Lau, 2004), we propose a version of Voronoi quantizer that is able to discretize
the state space of the product MDP S0. In the beginning, C is initialized to consist of just one ci,
which corresponds to the initial state. This means that the agent views the entire state space as a
homogeneous region when no apriori knowledge is available. Subsequently, when the agent explores,
the Euclidean distance between each newly visited state and its nearest neighbor is calculated. If
this distance is greater than a threshold value ∆ called “minimum resolution”, or if the new state S0
has a never-visited automaton state then the newly visited state is appended to C. Therefore, as the
agent continues to explore, the size of C would increase until the relevant parts of the state space are
5
Under review as a conference paper at ICLR 2019
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Algorithm 2: Episodic VQ
input : MDP M, minimum resolution ∆
output : Approximated Q-function Q
initialize Q(c1 , a) = 0, ∀a ∈ A
repeat
initialize c1 = initial state
set c = c1
α = arg maxa∈A Q(c, a)
repeat
execute action α and observe the next state (s0, q)
if Cq is empty then
I append Cnew = (s0,q) to Cq
I initialize Q(Cnew,a) = 0, ∀a ∈ A
else
determine the nearest neighbor Cnew within Cq
if Cnew = C then
if ||C - (s0, q)||2 > ∆ then
I append Cnew = (s0,q) to Cq
I initialize Q(Cnew,a) = 0, ∀a ∈ A
end
else
I Q(c,α) = (1 - μ)Q(c,α) + μ[R(c,α) + Ymax(Q(cnew,a0))]
a0
end
end
C = Cnew
until end of trial
until end of trial
partitioned. In our algorithm, the set C has n disjoint subsets where n = |Q| and Q is the finite set of
states of the automaton. Each subset Cqj , j = 1, ..., n contains the centroids of those Voronoi cells
that have the form of Cqj = (∙, qj), i.e. Um Cqj = Cqj and C = U；=i Cqj. Therefore, a Voronoi cell
{(s,q∙) ∈ S%∣∣(s,qj) - Cqj ||2 ≤∣∣(s,%∙ )-cqj∣∣2}
is defined by the nearest neighbor rule for any i0 6= i. The VQ algorithm is presented in Algorithm
2. The proposed algorithm consist of several resets at which the agent is forced to re-localize to its
initial state s0. Each reset is called an episode, as such in the rest of the paper we call this algorithm
episodic VQ.
6 Fitted Value Iteration
In this section we propose a modified version of FVI that can handle the product MDP. The global
value function V : S0 → R, or more specifically V : S X Q → R, consists of n number of sub-value
functions where n = |Q|. For each qj ∈ Q, the sub-value function vqj : S → R returns the value
the states of the form (s, qj). As we will see shortly, in a same manner as LCNFQ, the sub-value
functions are not decoupled.
Let P 0(dy|s0, a) be the distribution over S0 for the successive state given that the current state is
s0 and the current action is a. For each state (s, qj), the Bellman update over each sub-value function
Vqj is defined as:
T Vqj (s)
sup{
a∈A
V(y)P0(dy|(s,qj),a)},
(4)
where T is the Bellman operator (HernandeZ-Lerma & Lasserre, 2012). The update in (4) is a special
case of general Bellman update as it does not have a running reward and the (terminal) reward is
embedded via value function initialization. The value function is initialized according to the following
rule:
if s0 ∈ A,
otherwise.
(5)
6
Under review as a conference paper at ICLR 2019
Algorithm 3: FVI
input : MDP M, a set of samples {sf}k=ι = {(si,qj)}k=ι for each qj ∈ Q, Monte Carlo sampling
number Z, smoothing parameter h
output : approximated value function Lv
1	initialize Lv
2	sample YaZ (si, qj), ∀qj ∈ Q, ∀i = 1, ..., k , ∀a ∈ A
3	repeat
4	for j = |Q| to 1 do
5	Yqj ∈ Q, ∀i = 1,...,k, ∀a ∈ A calculate Ia((Si,qj)) = 1/ZPy∈γz(si,qj)Lv(y) using (6)
6	for each state (si,qj), update Vqj (Si) = supa∈A{Ia((si,qj))} in (6)
7	end
8	until end of trial
where rp and rn are defined in (3). The main hurdle in executing the Bellman operator in continuous
state MDPs, as in (4), is that no analytical representation of the value function v and also sub-value
functions vqj , qj ∈ Q is available. Therefore, we employ an approximation method by introducing
the operator L. The operator L constructs an approximation of the value function denoted by Lv
and of each sub-value function vqj which we denote by Lvqj . For each qj ∈ Q the approximation
is based on a set of points {(si, qj)}k=ι ⊂ S0 which are called centers. For each qj, the centers
i = 1, ..., k are distributed uniformly over S such that they uniformly cover S.
We employ a kernel-based approximator for our FVI algorithm. Kernel-based approximators have
attracted a lot of attention mostly because they perform very well in high-dimensional state spaces
(Stachurski, 2008). One of these methods is the kernel averager in which for any state (s, qj) the
approximate value function is represented by
LvGqj) = Lvqj (S) = Pi=I K (Si-S)Vqj (Si)
j	Pk=ι κ(Si - S)
(6)
where the kernel K : S → R is a radial basis function, such as e-|s-si|/h, and h is smoothing
parameter. Each kernel has a center si and the value of it decays to zero as s diverges from si . This
means that for each qj ∈ Q the approximation operator L is a convex combination of the values of
the centers {si}ik=1 with larger weight given to those values vqj (si) for which si is close to s. Note
that the smoothing parameter h controls the weight assigned to more distant values (see Section A.3).
In order to approximate the integral in Bellman update (4) we use a Monte Carlo sampling technique
(Shonkwiler & Mendivil, 2009). For each center (si, qj) and for each action a, we sample the next
state yaz(si, qj) for z = 1, ..., Z times and append it to set of Z subsequent states YaZ(si, qj). We then
replace the integral with
1Z
Ia(si,qj) = Zf* LMya(St,qj)).
z=1
(7)
The approximate value function Lv is initialized according to (5). In each cycle of FVI, the ap-
proximate Bellman update is first performed over the sub-value functions that are associated with
accepting states of the automaton, i.e. those that have initial value of rp, and then goes backward until
it reaches the sub-value functions that are associated to the initial states. In this manner, we allow the
state values to back-propagate through the transitions that connects the sub-value function via (7).
Once we have the approximated value function, we can generate the optimal policy by following the
maximum value (Algorithm 3).
7 Experimental Results
We describe a mission planning architecture for an autonomous Mars-rover that uses LCNFQ to
follow a mission on Mars. The scenario of interest is that we start with an image from the surface of
Mars and then we add the desired labels from 2AP, e.g. safe or unsafe, to the image. We assume that
we know the highest possible disturbance caused by different factors (such as sand storms) on the
7
Under review as a conference paper at ICLR 2019
(a) Melas Chasma	(b) Coprates Chasma
Figure 1: Melas Chasma and Coprates Chasma, in the central and eastern portions of Valles Marineris.
Map color spectrum represents elevation, where red is high and blue is low. (Image courtesy of
NASA, JPL, Caltech and University of Arizona.)
rover motion. This assumption can be set to be very conservative given the fact that there might be
some unforeseen factors that we did not take into account.
The next step is to express the desired mission in LTL format and run LCNFQ on the labeled image
before sending the rover to Mars. We would like the rover to satisfy the given LTL property with the
highest probability possible starting from any random initial state (as we can not predict the landing
location exactly). Once LCNFQ is trained we use the network to guide the rover on the Mars surface.
We compare LCNFQ with Voronoi quantizer and FVI and we show that LCNFQ outperforms these
methods.
7.1 MDP Structure
In this numerical experiment the area of interest on Mars is Coprates quadrangle, which is named
after the Coprates River in ancient Persia (see Section A.4). There exist a significant number of
signs of water, with ancient river valleys and networks of stream channels showing up as sinuous
and meandering ridges and lakes. We consider two parts of Valles Marineris, a canyon system in
Coprates quadrangle (Fig. 1). The blue dots, provided by NASA, indicate locations of recurring
slope lineae (RSL) in the canyon network. RSL are seasonal dark streaks regarded as the strongest
evidence for the possibility of liquid water on the surface of Mars. RSL extend downslope during a
warm season and then disappear in the colder part of the Martian year (McEwen et al., 2014). The
two areas mapped in Fig. 1, Melas Chasma and Coprates Chasma, have the highest density of known
RSL.
For each case, let the entire area be our MDP state space S, where the rover location is a single
state s ∈ S. At each state s ∈ S, the rover has a set of actions A = {left, right, up, down, stay}
by which it is able to move to other states: at each state s ∈ S, when the rover takes an action
a ∈ {left, right, up, down} it is moved to another state (e.g., s0) towards the direction of the action
with a range of movement that is randomly drawn from (0, D] unless the rover hits the boundary of
the area which forces the rover to remain on the boundary. In the case when the rover chooses action
a = stay it is again moved to a random place within a circle centered at its current state and with
radius d D . Again, d captures disturbances on the surface of Mars and can be tuned accordingly.
With S and A defined we are only left with the labelling function L : S → 2AP which assigns to
each state s ∈ S a set of atomic propositions L(s) ⊆ 2AP. With the labelling function, we are able
to divide the area into different regions and define a logical property over the traces that the agent
generates. In this particular experiment, we divide areas into three main regions: neutral, unsafe and
target. The target label goes on RSL (blue dots), the unsafe label lays on the parts with very high
elevation (red coloured) and the rest is neutral. In this example we assume that the labels do not
overlap each other.
Note that when the rover is deployed to its real mission, the precise landing location is not known.
Therefore, we should take into account the randomness of the initial state s0 . The dimensions of the
area of interest in Fig. 1.a are 456.98 × 322.58 km and in Fig. 1.b are 323.47 × 215.05 km. The
8
Under review as a conference paper at ICLR 2019
(a) LDBA expressing
(8)
(b) LDBA expressing (9)
Figure 2: Generated LDBAs
Table 1: Simulation results
MelaS Chasma					
Algorithm	Sample Complexity	UPor(So)	Success Ratet	Training Time*(s)	Iteration Num.
LCNFQ	7168 samples	0.0203	99%	95.64	40
VQ (∆ = 0.4)	27886 samples	0.0015	99%	1732.35	2195
VQ (∆ = 1.2)	7996 samples	0.0104	97%	273.049	913
VQ (∆ = 2)		0	0%		
FVI	40000 samples	0.0133	98%	4.12	80
Coprates Chasma					
Algorithm	Sample Complexity	Upol+(so)	Success Ratet	Training Time*(s)	Iteration Num.
LCNFQ	2680 samples	0.1094	98%	166.13	40
VQ (∆ = 0.4)	8040 samples	0.0082	98%	3666.18	3870
VQ (∆ = 1.2)	3140 samples	0.0562	96%	931.33	2778
VQ (∆ = 2)		0	0%		
FVI	25000 samples	0.0717	97%	2.16	80
t Testing the trained agent (for 100 trials) * Average for 10 trainings
diameter of each RSL is 19.12 km. Other parameters in this numerical example have been set as
D = 2 km, d = 0.02 km, the reward function parameter y = 1 for LCNFQ and y = 0 for VQ and
FVL M = 1, m = 0.05 and AP = {neutral, unsafe, target_1, target^}.
7.2 Specifications
The first control objective in this numerical example is expressed by the following LTL formula over
Melas Chasma (Fig. 1.a):
♦(t1 ∧	♦t2) ∧	(t2	→	t2)	∧	(u	→	u),	(8)
where n stands for “neutral”, t1 stands for “target 1”, t2 stands for “target 2” and u stands for “unsafe”.
Target 1 are the RSL (blue bots) on the right with a lower risk of the rover going to unsafe region
and the target 2 label goes on the left RSL that are a bit riskier to explore. Conforming to (8) the
rover has to visit the target 1 (any of the right dots) at least once and then proceed to the target 2 (left
dots) while avoiding unsafe areas. Note that according to (u → u) in (8) the agent is able to go
to unsafe area u (by climbing up the slope) but it is not able to come back due to the risk of falling.
With (8) We can build the associated Buchi automaton as in Fig. 2.a.
The second formula focuses more on safety and we are going to employ it in exploring Coprates
Chasma (Fig. 1.b) Where a critical unsafe slope exists in the middle of this region.
♦t ∧ (t → t) ∧ (u → u)	(9)
In (9), t refers to “target”, i.e. RSL in the map, and u stands for “unsafe”. According to this LTL
formula, the agent has to eventually reach the target (♦t) and stays there ((t → t)). HoWever, if
the agent hits the unsafe area it can never comes back and remains there forever ((u → u)). With
(9) we can build the associated Buchi automaton as in Fig. 2.b. Having the Buchi automaton for each
formula, We are able to use Definition 4.1 to build product MDPs and run LCNFQ on both.
9
Under review as a conference paper at ICLR 2019
7.3 Simulation Results
This section presents the simulation results. All simulations are carried on a machine with a 3.2GHz
Core i5 processor and 8GB of RAM, running Windows 7. LCNFQ has four feedforward neural
networks for (8) and three feedforward neural networks for (9), each associated with an automaton
state in Fig. 2.a and Fig. 2.b. We assume that the rover lands on a random safe place and has to find
its way to satisfy the given property in the face of uncertainty. The learning discount factor γ is also
set to be equal to 0.9.
Fig. 4 in Section A.5 gives the results of learning for LTL formulas (8) and (9). At each state S巴
the robot picks an action that yields highest Q(S巴∙)and by doing so the robot is able to generate a
control policy Pol0 over the state space S巴 The control policy Pol0 induces a policy Pol* over
the state space S and its performance is shown in Fig. 4.
Next, we investigate the episodic VQ algorithm as an alternative solution to LCNFQ. Three different
resolutions (∆ = 0.4, 1.2, 2 km) are used to see the effect of the resolution on the quality of the
generated policy. The results are presented in Table 1, where VQ with ∆ = 2 km fails to find
a satisfying policy in both regions, due to the coarseness of the resulted discretisation. A coarse
partitioning result in the RL not to be able to efficiently back-propagate the reward or the agent to be
stuck in some random-action loop as sometimes the agent’s current cell is large enough that all actions
have the same value. In Table 1, training time is the empirical time that is taken to train the algorithm
and travel distance is the distance that agent traverses from initial state to final state. We show the
generated policy for ∆ = 1.2 km in Fig. 5 in Section A.5. Additionally, Fig. 7 in Section A.6 depicts
the resulted Voronoi discretisation after implementing the VQ algorithm. Note that with VQ only
those parts of the state space that are relevant to satisfying the property are accurately partitioned.
Finally, we present the results of FVI method in Fig 6 in Section A.5 for the LTL formulas (8) and (9).
The FVI smoothing parameter is h = 0.18 and the sampling time is Z = 25 for both regions where
both are empirically adjusted to have the minimum possible value for FVI to generate satisfying
policies. The number of basis points also is set to be 100, so the sample complexity of FVI is
100 × Z × |A| × (|Q| - 1). We do not sample the states in the product automaton that are associated
to the accepting state of the automaton since when we reach the accepting state the property is
satisfied and there is no need for further exploration. Hence, the last term is (|Q| - 1). However,
if the property of interest produces an automaton that has multiple accepting states, then we need
to sample those states as well. Note that in Table 1, in terms of timing, FVI outperforms the other
methods. However, we have to remember that FVI is an approximate DP algorithm, which inherently
needs an approximation of the transition probabilities. Therefore, as we have seen in Section 6 in (7),
for the set of basis points we need to sample the subsequent states. This reduces FVI applicability as
it might not be possible in practice.
Additionally, both FVI and episodic VQ need careful hyper-parameter tuning to generate a satisfying
policy, i.e., h and Z for FVI and ∆ for VQ. The big merit of LCNFQ is that it does not need any
external intervention. Further, as in Table 1, LCNFQ succeeds to efficiently generate a better policy
compared to FVI and VQ. LCNFQ has less sample complexity while at the same time produces
policies that are more reliable and also has better expected reward, i.e. higher probability of satisfying
the given property.
8 Conclusion
This paper proposes LCNFQ, a method to train Q-function in a continuous-state MDP such that the
resulting traces satisfy a logical property. The proposed algorithm uses hybrid modes to automatically
switch between neural nets when it is necessary. LCNFQ is successfully tested in a numerical
example to verify its performance.
10
Under review as a conference paper at ICLR 2019
References
Mohammed Alshiekh, Roderick Bloem, RUediger Ehlers, Bettina KOnighofer, Scott Niekum, and
Ufuk Topcu. Safe reinforcement learning via shielding. arXiv preprint arXiv:1708.08611, 2017.
Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy
sketches. In ICML, volume 70,pp. 166-175, 2017.
TomaW BrazdiL Krishnendu Chatterjee, Martin Chmellk, VojteCh Forejt, Jan Kretinsky, Marta
Kwiatkowska, David Parker, and Mateusz Ujma. Verification of Markov decision processes
using learning algorithms. In ATVA, pp. 98-114. Springer, 2014.
Lucian Busoniu, Robert Babuska, Bart De Schutter, and Damien Ernst. Reinforcement Learning and
Dynamic Programming Using Function Approximators, volume 39. CRC press, 2010.
Rolando Cavazos-Cadena, Eugene A Feinberg, and RaUl Montes-De-Oca. A note on the existence
of optimal policies in total reward dynamic programs with compact action sets. Mathematics of
Operations Research, 25(4):657-666, 2000.
Kenji Doya. Reinforcement learning in continuous time and space. Neural computation, 12(1):
219-245, 2000.
Richard Durrett. Essentials of stochastic processes, volume 1. Springer, 1999.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
JMLR, 6(Apr):503-556, 2005.
Jie Fu and Ufuk Topcu. Probably approximately correct MDP learning and control with temporal
logic constraints. In Robotics: Science and Systems X, 2014.
Geoffrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning,
pp. 261-268. Elsevier, 1995.
R. Gray. Vector quantization. IEEE ASSP Magazine, 1(2):4-29, 1984.
David Harris and Sarah Harris. Digital design and computer architecture. Morgan Kaufmann, 2010.
OneSimo Hernandez-Lerma and Jean B Lasserre. Further topics on discrete-time Markov control
processes, volume 42. Springer Science & Business Media, 2012.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):
251-257, 1991.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359-366, 1989.
Ivan SK Lee and Henry YK Lau. Adaptive state space partitioning for reinforcement learning.
Engineering applications of artificial intelligence, 17(6):577-588, 2004.
Long-H Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machine learning, 8(3/4):69-97, 1992.
Javier Lope and Jose Martin. Learning autonomous helicopter flight with evolutionary reinforcement
learning. In International Conference on Computer Aided Systems Theory, pp. 75-82. Springer,
2009.
Alfred S McEwen, Colin M Dundas, Sarah S Mattson, Anthony D Toigo, Lujendra Ojha, James J
Wray, Matthew Chojnacki, Shane Byrne, Scott L Murchie, and Nicolas Thomas. Recurring slope
lineae in equatorial regions of Mars. Nature Geoscience, 7(1):53, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
11
Under review as a conference paper at ICLR 2019
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. InICML,pp.1928-1937, 2016.
Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in Markov decision processes. arXiv
preprint arXiv:1205.4810, 2012.
Dirk Ormoneit and Saunak Sen. Kernel-based reinforcement learning. Machine learning, 49(2):
161-178, 2002.
Amir Pnueli. The temporal logic of programs. In Foundations of Computer Science, pp. 46-57. IEEE,
1977.
Martin L Puterman. Markov decision processes: Discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Martin Riedmiller. Concepts and facilities of a neural reinforcement learning control architecture for
technical process control. Neural computing & applications, 8(4):323-338, 1999.
Martin Riedmiller. Neural fitted Q iteration-first experiences with a data efficient neural reinforcement
learning method. In ECML, volume 3720, pp. 317-328. Springer, 2005.
Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation learning:
The RPROP algorithm. In Neural networks, pp. 586-591. IEEE, 1993.
Ronald W Shonkwiler and Franklin Mendivil. Explorations in Monte Carlo Methods. Springer
Science & Business Media, 2009.
Salomon Sickert and Jan Kfetlnsky. MoChiBA: Probabilistic LTL model checking using limit-
deterministic Buchi automata. In ATVA, pp. 130-137. Springer, 2016.
Salomon Sickert, Javier Esparza, Stefan Jaax, and Jan Kfetlnsky. Limit-deterministic BUchi automata
for linear temporal logic. In CAV, pp. 312-332. Springer, 2016.
Stephen L Smith, Jana Tumova, Calin Belta, and Daniela Rus. Optimal path planning for surveillance
with temporal-logic constraints. The International Journal of Robotics Research, 30(14):1695-
1708, 2011.
Yong Song, Yi-bin Li, Cai-hong Li, and Gui-fang Zhang. An efficient initialization approach of
Q-learning for mobile robots. International Journal of Control, Automation and Systems, 10(1):
166-172, 2012.
John Stachurski. Continuous state dynamic programming via nonexpansive approximation. Compu-
tational Economics, 31(2):141-160, 2008.
Richard S Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse
coding. In NIPS, pp. 1038-1044, 1996.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT
press Cambridge, 1998.
Gerald Tesauro. TD-Gammon: A self-teaching Backgammon program. In Applications of Neural
Networks, pp. 267-285. Springer, 1995.
Andrea L Thomaz and Cynthia Breazeal. Teachable robots: Understanding human teaching behavior
to build more effective robot learners. Artificial Intelligence, 172(6-7):716-737, 2008.
Ilya Tkachev, Alexandru Mereacre, Joost-Pieter Katoen, and Alessandro Abate. Quantitative model-
checking of controlled discrete-time Markov processes. Information and Computation, 253:1-35,
2017.
Hado Van Hasselt and Marco A Wiering. Reinforcement learning in continuous action spaces. In
ADPRL, pp. 272-279. IEEE, 2007.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Eric M Wolff, Ufuk Topcu, and Richard M Murray. Robust control of uncertain Markov decision
processes with temporal logic specifications. In CDC, pp. 3372-3379. IEEE, 2012.
12
Under review as a conference paper at ICLR 2019
A S upplementary Materials
A.1 Preliminaries
Definition A.1 (Path) In a continuous-state MDP M, an infinite path ρ starting at s0 is a sequence of
states ρ = s0 -a→0 s1 -a→1 ... such that every transition si -a→i si+1 is possible in M, i.e. si+1 belongs
to the smallest Borel set B such that P(B|si, ai) = 1 (or in a discrete MDP, P (si+1 |si, ai) > 0).
We might also denote ρ as s0 .. to emphasize that ρ starts from s0.	y
Definition A.2 (Stationary Policy) A stationary (randomized) policy Pol : S × A → [0, 1] is a
mapping from each state s ∈ S, and action a ∈ A to the probability of taking action a in state s. A
deterministic policy is a degenerate case of a randomized policy which outputs a single action at a
given state, that is ∀s ∈ S, ∃a ∈ A, Pol (s, a) = 1.	y
In an MDP M, we define a function R : S × A → R0+ that denotes the immediate scalar bounded
reward received by the agent from the environment after performing action a ∈ A in state s ∈ S.
Definition A.3 (Expected (Infinite-Horizon) Discounted Reward) For a policy Pol on an MDP
M, the expected discounted reward is defined as (Sutton & Barto, 1998):
UPoO(S) = EPol[XX Yn R(Sn,Pθl(Sn))∣S0 = s],	(10)
n=0
where EPol [∙] denotes the expected value given that the agent follows policy Pol, Y ∈ [0,1) is a
discount factor and s0, ..., sn is the sequence of states generated by policy Pol up to time step n. y
Definition A.4 (Optimal Policy) OPtimal policy Por is defined as follows:
Por(S) = arg sup UPoI(S),
P ol∈D
where D is the set of all stationary deterministic policies over the state space S.	y
13
Under review as a conference paper at ICLR 2019
A.2 Policy Synthesis
The simplest way to solve an infinite-state MDP with RL is to discretise the state space and then
to use the conventional methods in RL to find the optimal policy (Stachurski, 2008). Although this
method can work well for many problems, the resulting discrete MDP is often inaccurate and may
not capture the full dynamics of the original MDP. One might argue that by increasing the number
of discrete states the latter problem can be resolved. However, the more states we have the more
expensive and time-consuming our computations will be. Thus, MDP discretisation has to always
deal with the trade off between accuracy and the curse of dimensionality.
A.2.1 Classical Q-learning
Let the MDP M be a finite-state MDP. Q-learning (QL), a sub-class of RL algorithms, is extensively
used to find the optimal policy for a given finite-state MDP (Sutton & Barto, 1998). For each state
s ∈ S and for any available action a ∈ A, QL assigns a quantitative value Q : S × A → R, which is
initialized with an arbitrary and finite value for all state-action pairs. As the agent starts learning and
receiving rewards, the Q-function is updated by the following rule when the agent takes action a at
state s:
Q(s,a) - Q(s,a) + μ[R(s,a) + Y max(Q(s0,a0)) - Q(s,a)],	(11)
a0∈A
where Q(s,a) is the Q-ValUe corresponding to state-action (s,a), 0 < μ ≤ 1 is called learning rate
or step size, R(s, a) is the reward obtained for performing action a in state s, γ is the discount factor,
and s0 is the state obtained after performing action a. Q-fUnction for the rest of the state-action pairs
remains Unchanged.
Under mild assUmptions, for finite-state and finite-action spaces QL converges to a UniqUe limit,
as long as every state action pair is visited infinitely often (Watkins & Dayan, 1992). Once QL
converges, the optimal policy Pol* : S → A can be generated by selecting the action that yields the
highest Q, i.e.,
Pol* (s) = argmax Q(s, a),
a∈A
where Pol* is the same optimal policy that can be generated via DP with Bellman operation. This
means that when QL converges, we have
∞
Q(s,a) = R(S,a)+EPol*[X YnR(Sn,Pol*(sn))∣sι = s0],	(12)
n=1
where S0 ∈ B is the agent new state after choosing action a at S sUch that P(B|S, a) = 1.
A.2.2 Neural Fitted Q-iteration
Recall the QL Update rUle (11), in which the agent stores the Q-valUes for all possible state-action
pairs. In the case when the MDP has a continUoUs state space it is not possible to directly Use standard
QL since it is practically infeasible to store Q(S, a) for every S ∈ S and a ∈ A. ThUs, we have to tUrn
to fUnction approximators in order to approximate the Q-valUes of different state-action pairs of the
Q-fUnction. NeUral Fitted Q-iteration (NFQ) (Riedmiller, 2005) is an algorithm that employs neUral
networks (Hornik et al., 1989) to approximate the Q-fUnction, dUe to the ability of neUral networks to
generalize and exploit the set of samples. NFQ, is the core behind Google famoUs algorithm Deep
Reinforcement Learning (Mnih et al., 2015).
The Update rUle in (11) can be directly implemented in NFQ. In order to do so, a loss fUnction has to
be introdUced that measUres the error between the cUrrent Q-valUe and the new valUe that has to be
assigned to the cUrrent Q-valUe, namely
L = (Q(S, a) - (R(S, a) + YmaxQ(S0, a0)))2.	(13)
a0
Over this error, common gradient descent techniqUes can be applied to adjUst the weights of the
neUral network, so that the error is minimized.
In classical QL, the Q-fUnction is Updated whenever a state-action pair is visited. In the continUoUs
state-space case, we may Update the approximation in the same way, i.e., Update the neUral net weights
once a new state-action pair is visited. However, in practice, a large nUmber of trainings might need
14
Under review as a conference paper at ICLR 2019
to be carried out until an optimal or near optimal policy is found. This is due to the uncontrollable
changes occurring in the Q-function approximation caused by unpredictable changes in the network
weights when the weights are adjusted for one certain state-action pair (Riedmiller, 1999). More
specifically, if at each iteration we only introduce a single sample point the training algorithm tries
to adjust the weights of the neural network such that the loss function becomes minimum for that
specific sample point. This might result in some changes in the network weights such that the error
between the network output and the previous output of sample points becomes large and failure to
approximate the Q-function correctly. Therefore, we have to make sure that when we update the
weights of the neural network, we explicitly introduce previous samples as well: this technique is
called “experience replay” (Lin, 1992) and detailed later.
The core idea underlying NFQ is to store all previous experiences and then reuse this data every time
the neural Q-function is updated. NFQ can be seen as a batch learning method in which there exists a
training set that is repeatedly used to train the agent. In this sense NFQ is an offline algorithm as
experience gathering and learning happens separately.
We would like to emphasize that neural-net-based algorithms exploit the positive effects of general-
ization in approximation while at the same time avoid the negative effects of disturbing previously
learned experiences when the network properly learns (Riedmiller, 2005). The positive effect of
generalization is that the learning algorithm requires less experience and the learning process is highly
data efficient.
A.2.3 Voronoi Quantizer
As stated earlier, many existing RL algorithms, e.g. QL, assume a finite state space, which means
that they are not directly applicable to continuous state-space MDPs. Therefore, if classical RL is
employed to solve an infinite-state MDP, the state space has to be discretized first and then the new
discrete version of the problem has to be tackled. The discretization can be done manually over the
state space. However, one of the most appealing features of RL is its autonomy. In other words, RL
is able to achieve its goal, defined by the reward function, with minimum supervision from a human.
Therefore, the state space discretization should be performed as part of the learning task, instead of
being fixed at the start of the learning process.
Nearest neighbor vector quantization is a method for discretizing the state space into a set of disjoint
regions (Gray, 1984). The Voronoi Quantizer (VQ) (Lee & Lau, 2004), a nearest neighbor quantizer,
maps the state space S onto a finite set of disjoint regions called Voronoi cells. The set of centroids
of these cells is denoted by C = {ci}im=1, ci ∈ S, where m is the number of the cells. Therefore,
designing a nearest neighbor vector quantizer boils down to coming up with the set C. With C, we are
able to use QL and find an approximation of the optimal policy for a continuous-state space MDP.
The details of how the set of centroids C is generated as part of the learning task in discussed in the
body of the paper.
A.2.4 Fitted Value Iteration
Finally, this section introduces Fitted Value Iteration (FVI) for continuous-state numerical dynamic
programming using a function approximator (Gordon, 1995). In standard value iteration the goal is to
find a mapping (called value function) from the state space to R such that it can lead the agent to find
the optimal policy. The value function in our setup is (10) when Pol is the optimal policy, i.e. U PoI *.
In continuous state spaces, no analytical representation of the value function is in general available.
Thus, an approximation can be obtained numerically through approximate value iteration, which
involves approximately iterating the Bellman operator T on some initial value function (Stachurski,
2008). FVI is explored more in the paper.
A.3 Kernel Averager
It has been proven that FVI is stable and converging when the approximation operator is non-expansive
(Gordon, 1995). The operator L is said to be non-expansive if:
sup |Lvtq+j 1(s) - Lvtqj (s)| ≤ sup |vtq+j1(s) - vtqj (s)|,
s∈S	s∈S
15
Under review as a conference paper at ICLR 2019
where Lvtqj (s) is the approximated value function at (s, qj) at iteration t of the algorithm. The kernel
averager is a non-expansive approximator (Stachurski, 2008).
16
Under review as a conference paper at ICLR 2019
A.4 Coprates Quadrangle
Figure 3: Coprates quadrangle (Image courtesy of NASA, JPL and USGS.)
17
Under review as a conference paper at ICLR 2019
A.5 Generated Policies
(a)	The generated path in Melas Chasma when
the landing location, i.e. the black rectangle, is
(118, 85)
(b)	The generated path in Coprates Chasma when
the landing location, i.e. the the black rectangle,
is (194, 74)
Figure 4: Results of learning with LCNFQ
(a)	The generated path in Melas Chasma when
the landing location, i.e. the black rectangle, is
(118, 85)
(b)	The generated path in Coprates Chasma when
the landing location, i.e. the black rectangle, is
(194, 74)
Figure 5: Results of learning with episodic VQ
(a)	The generated path in Melas Chasma when
the landing location, i.e. the black rectangle, is
(118, 85)
(b)	The generated path in Coprates Chasma when
the landing location, i.e. the black rectangle, is
(194, 74)
Figure 6: Results of learning with FVI
18
Under review as a conference paper at ICLR 2019
A.6 Generated Voronoi Cells
Figure 7: VQ generated cells in Melas Chasma for different resolutions
19