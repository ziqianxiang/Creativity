Under review as a conference paper at ICLR 2019
Decoupling Gating from Linearity
Anonymous authors
Paper under double-blind review
Ab stract
The gap between the empirical success of deep learning and the lack of strong the-
oretical guarantees calls for studying simpler models. By observing that a ReLU
neuron is a product of a linear function with a gate (the latter determines whether
the neuron is active or not), where both share a jointly trained weight vector, we
propose to decouple the two. We introduce GaLU networks — networks in which
each neuron is a product of a Linear Unit, defined by a weight vector which is
being trained, with a Gate, defined by a different weight vector which is not being
trained. Generally speaking, given a base model and a simpler version of it, the
two parameters that determine the quality of the simpler version are whether its
practical performance is close enough to the base model and whether it is easier to
analyze it theoretically. We show that GaLU networks perform similarly to ReLU
networks on standard datasets and we initiate a study of their theoretical proper-
ties, demonstrating that they are indeed easier to analyze. We believe that further
research of GaLU networks may be fruitful for the development of a theory of
deep learning.
1	Introduction
An artificial neuron with the ReLU activation function is the function fw (x) : Rd → R such that
fw(x) = max{x>w, 0} = (lχ>w≥0) ∙ (x>w).
The latter formulation demonstrates that the parameter vector w has a dual role; it acts both as a filter
or a gate that decides if the neuron is active or not, and as linear weights that control the value of
the neuron if it is active. We introduce an alternative neuron, called Gated Linear Unit or GaLU for
short, which decouples between those roles. A 0 - 1 GaLU neuron is a function gw,u(x) : Rd → R
such that
gw,u(X) = (1χ>u≥θ) ∙ (x>w) .	(1)
GaLU neurons, and therefore GaLU networks, are at least as expressive as their ReLU counterparts,
since fw = gw,w . On the other hand, GaLU networks appear problematic from an optimization
perspective, because the parameter u cannot be trained using gradient based optimization (since
Vugw,u(x) is always zero). In other words, training GaLU networks with gradient based algorithms
is equivalent to initializing the vector u and keeping it constant thereafter. A more general definition
of a GaLU network is given in section 2.
The main claim of the paper is that GaLU networks are on one hand as effective as ReLU networks
on real world datasets (section 3) while on the other hand they are easier to analyze and understand
(section 4).
1.1	Related Work
Many recent works attempt to understand deep learning by considering simpler models, that would
allow theoretical analysis while preserving some of the properties of networks of practical utility.
Our model is most closely related to two such proposals: linear networks and non-linear networks
in which only the readout layer is being trained.
Deep linear networks is a popular model for analysis that lead to impressive theoretical results (e.g.
Saxe et al. (2013); Kawaguchi (2016); Lu & Kawaguchi (2017)). Linear networks are useful in order
to understand how well gradient-based optimization algorithms work on non-convex problems. The
1
Under review as a conference paper at ICLR 2019
weakness of linear network is that their expressive power is very limited: linear networks can only
express linear functions. It means that their usefulness to understand the practical success of standard
networks is somewhat limited.
Training only the readout layer is an alternative attempt to understand deep learning through simpler
models, that also gave theoretical interesting results (e.g. Saxe et al. (2011); Mairal et al. (2014);
Daniely et al. (2016)). The idea is that all the layers but the last one implement a non-linear constant
transformation, and the last layer is learning a linear function on top of this transformation. The
weakness of this model is that there is a big practical difference between training all the layers of a
network and training only the last one.
Our model is similar in certain aspects to both of those models, but it enjoys a much better practical
utility than either one. See section 3 for an empirical comparison.
2	GaLU Networks
Recall the definition of a basic GaLU neuron given in equation 1. We consider a more general GaLU
neuron of the form
gw,u,σ (X)= σ(x>u) ∙ (x>w)
for some non-linear scalar function σ : R → R. If σ is differentiable, we could train the vectors u
with gradient based algorithms, but the focus of this paper is on untrained gates. That is, we assume
that the vectors {u} are kept to their initial values throughout the optimization procedure and only
the linear part of the GaLU neurons is being optimized.
GaLU networks with a single hidden layer have the following property: for any given example, the
values of the gates in the network remain constant. In networks with more than one hidden layer
this not true. Consider a standard fully connected feed-forward network, let x(0) be the input to the
network and let x(1), x(2), . . . be the inputs to intermediate layers of the network. The output of a
GaLU neuron at layer i will be σ(x(i-1)>u) ∙ (x(i-1)>w). So while the filter parameter vector, u,
is not optimized upon, the value of the gate, σ(x(i-1)>u), can change as x(i-1) changes. This adds
an additional complication to the dynamics of the optimization that we wish to avoid.
An alternative way to define a GaLU neuron at layer i is σ(x(0)>u) ∙ (x(i-1)> W). In that case, the
value of the gate is determined by the original input, and only the linear part depends on the output
of the previous layer of the network. We call such a neuron a GaLU0 neuron, and a GaLU0 network
is a network where all the neurons are GaLU0 neurons. In GaLU0 networks the gate values remain
constant along the training, producing simpler dynamics.
3	Empirical Success
In order to check the hypothesis that effectiveness of ReLU networks stems mostly from the ability
to train the linear part of the neurons, and not the gate part, we tested both GaLU01 and ReLU
networks on the standard MNIST (LeCun & Cortes, 2010) and Fashion-MNIST (Xiao et al., 2017)
datasets. For both, we used PCA to reduce the input dimension to 64, and then trained a two hidden
layers fully-conneted networks on them, with k hidden neurons at each hidden layer. Figure 1
summarizes the results, showing that GaLU0 and ReLU achieve similar results, both outperforming
linear networks of the same size. Training only the readout layer of a ReLU network gave much
poorer results (which were omitted from the graphs for clarity).
4	THEORETICAL SIMPLICITY: S OLVING Rd → R1 PROBLEMS WITH ONE
Hidden Layer Networks
We now turn to some very basic theoretical analysis of GaLU networks with a single hidden layer.
Our goal is to show that GaLU networks are simpler to analyze than standard networks.
1We used the logistic sigmoid function, as it gave better results on the test set than the binary gate function.
2
Under review as a conference paper at ICLR 2019
0.00
MNIST
0.10
0.02
864
000
...
000
ycaruccA tseT
20
40
60	80	100
k
Fashion MNIST
0.20
0.10
0.12
864
111
...
000
ycaruccA tseT
ReLu ReLU
—GaLU0
—θ— Linear
60	80	100
k
20	40
Figure 1: Comparison between 3 deep learning models on the MNIST and Fashion- MNIST datasets.
All models were trained using the same architectures: two fully connected hidden layers with k
neurons. The input dimension was reduced to 64 with PCA.
Consider a GaLU network with a single hidden layer of k neurons: N (x) = Pjk=1 αjgwj,uj (x). A
convenient property of a GaLU neuron is that it is linear in the weights wj, hence, αj gwj ,uj (x) =
gαjwj,uj (x). It means that the network can be rewritten as
k	kk
N(X) =〉： αjgwj,uj (X) =〉： ga wj,uj (X) =〉： gWj ,uj (X)
With Wj = αjWj. Because we want to optimize over the weights wι,..., Wk,αι,...,ak, we might
as well optimize over the reparameterization Wi,..., Wk without losing expressive power. It means
that in a GaLU network of this form, it is sufficient to train the first layer of the network, as the
readout layer adds nothing to the expressiveness of the network.
The previous term can be further simplified:
kk
N(X) =)： gWj,uj (X) =)： σ (XTUj) XTWj
j=1	j=1
σ X>u1 X>	σ X>u2 X>	. . . σ X>uk X>
W1
W2
.
.
.
Wk
Φu(X)>W
where
Φu(X)
σ
σ
u1 X
u2 X
W1
W2
,W
, u = [u1	u2
uk] .
σ X>uk X
Wk
So it turns out that a GaLU network is nothing more than a random non-linear transformation Φu :
Rd → Rkd and then a linear function.
4.1 Expressivity
There are different notions for the expressivity of a model, and one of the simplest ones is the finite-
sample expressivity over a random sample. This notion fits well to our model, because we are not
interested in the absolute expressivity ofa GaLU network, but of the expressivity ofa GaLU network
with random filters. So the question is how well does a randomly-initialized network can fit a random
sample. Note that given the constant filters, solving for the best weights is a convex problem.
3
Under review as a conference paper at ICLR 2019
Hence, there is no “expressivity - optimization gap" in GaLU networks - every expressivity results
is immediately also an optimization result.
Let S = {(xι, yj, (x2, yj,..., (xm, ym)} be a random sample, such that xι,..., Xm 〜N (0, Id)
and yι,..., ymj 〜N(0,1), all of which are independent. Clearly, it is impossible to generalize
from the sample to unseen examples; the best possible test loss is 1, and is achieved by the constant
prediction 0. However, itis an interesting problem because it allows us to measure the expressivity of
GaLU networks, by showing how much overfit we can expect from the network for a non-adversarial
sample. Equivalently, it tells us how well the network can perform memorization tasks, where the
only solution is to memorize the entire sample. We train the network for the standard mean-square-
error regression loss.
Because the network is simply linear function over a constant non-linear transformation, and
because we use the MSE loss, there is a closed form solution to the optimization problem
mi□w m Pm=I(N(Xi) - yi)2 which is
X
Φu(x1)>
Φu(x2)>
w* = X+
y1
y2
Φu(xm)
ym
with X+ being a pseudo-inverse of X. This gives US
Theorem 1 Let xι,..., Xm ∈ Rd, uι,..., Uk ∈ Rd be arbitrary vectors. Define X as above. Let
yι,..., ym 〜N(0,1) be independent random normal variables. Define the expected squared loss
on the training set, for weights w, as LS (w). Then,
E[min LS (w)] = 1 -
rank (X)
w
m
Proof Every vector y = (y1 , . . . , ym ) ∈ Rm can be decomposed to a sum y = a + b
where a is in the span of the columns of X and b is in the null space of X. It follows that
minw LS(W) = ∣∣bk2∕m. The claim follows because if y 〜N(0, Im) then the expected value of
∣∣bk2 is m 一 rank (X).	■
It is always true that rank(X) ≤ min{m,kd}. Empirical experimentation shows that if
xι,..., Xm, uι,..., Uk 〜 N(0, Id) then with high probability rank(X) = min{m, kd}.
4.1.1	Comparison to ReLU Networks
The fact that the GaLU network turned out to be only a linear function on top ofa non-linear trans-
formation seems to be a peculiar mathematical accident, with little relevance to standard networks.
So we empirically tested the behavior of both ReLU and GaLU networks on the above model. It
turns out that ReLU outperforms GaLU by a small margin - itis never better than GaLU with double
the number of neurons, and is often worse than that.
ReLU can outperform GaLU, even though it is less expressive, because we don’t train the value of
the the filters u1 , . . . , uk at all for the GaLU networks. It turns out that SGD over a ReLU network
converges to better filters than a simple random initialization. One way to measure how much better
those filters are is by trying to improve the initial filters of the GaLU network by randomly replacing
them. Consider for example the simple algorithm given in algorithm 1.
Running this algorithm improves the results of the GaLU networks, making them more competitive
with the ReLU ones. Figure 2 summarizes our results.
4.2 Generalization
An important fact about artificial neural networks is that they have small generalization error in
many real-life problems. Otherwise they wouldn’t be very useful as a learning algorithm. Zhang
4
Under review as a conference paper at ICLR 2019
Algorithm 1 Improve GaLU filters
Input: A sample S, number of neurons k, number of iterations n.
Initialize u1 , u2 , . . . , uk randomly.
Find an optimal solution w1 , . . . , wk .
for i = 1 to n do
Pick j 〜Uniform {1, 2,…,k}.
Pick Uj randomly.
Find an optimal solution for a GaLU network with filters uι,..., Uj-1, Uj, uj+ι,..., Uk.
If the new solution is better than the current one, update Uj = Uj-.
end for
m
Figure 2: Comparison of GaLU and ReLU networks with a single hidden layer and output in R1 .
GaLU(512) stands for GaLU networks after 512 steps of algorithm 1.
et al. (2016) have shown empirically that many classical attempts, such as model capacity, explicit
regularization and even the properties of the optimization algorithm cannot explain this behavior.
One of the main experiments they run was to train the network over a sample with randomized labels,
and to observe that the network still achieved small training loss (but large test loss, naturally). So
any generalization bound that can be applied to the randomized sample is necessarily too weak to
explain the generalization of the natural sample.
As our goal is to show that GaLU networks exhibit similar phenomena as ReLU networks, but
may be easier to analyze, we first construct a similar experiment to that of Zhang et al. (2016) and
compare the performance of GaLU and ReLU networks. Consider the following natural model. Let
ci,..., Cn 〜N(0, Id) be n clusters centers, each one with a random labels bi,..., bn. A data point
(x, y) is generated by picking a random index i 〜 Uniform {1, 2,..., n}, and setting X = Ci + ξ for
ξ 〜N(0, σ%Id). y is a noisy version of bi. This model can be used for both regression problems
(with bi 〜N(0,1) and y = bi + e, e 〜N(0, σj)) and classification problems (with bi 〜Ber(2),
and y = bi ㊉ e, e 〜Ber(p)).
We fixed the number of samples m = 1000, the input dimension d = 30, the number of clusters
n = 30, the number of hidden neurons k = 30 and σx = 0.1. We calculated the train and test errors
for different values of σy and p and for a GaLU and ReLU networks. The results are summarized in
figure 3. We can clearly see that GaLU and ReLU have similar statistical behavior, and that while
the train error is always small, as the labels become noisier the generalization error increases. This
matches the spirit of experiments reported in Zhang et al. (2016).
4.2.1	Generalization Error and Norms
Next, we turn to an analysis of this phenomenon. Since one hidden layer GaLU networks can be cast
as linear predictors, we can rely on classic norm-based generalization bounds for linear predictors. In
particular, forp ∈ {1, 2}, consider the class of linear predictors Hp = {x 7→ x>w : kwkp ≤ Bp}.
Given a sequence of instances S = {xi, . . . , xm}, where kxik∞ ≤ 1, the Rademacher complexity
5
Under review as a conference paper at ICLR 2019
Regression
Binary Classification
Label Error σy
Figure 3: Train and test errors for the two models from section 4.2, with n = k = d = 30, m =
1000, σx = 0.1. Both of those graphs show that the generalization error is highly correlated with the
optimal error: it is not true that there is a constant difference between the train error and test error.
Note that in the regression problem, the number of SGD steps can drastically change the test error.
More steps mean larger test error. Similar optimization issues might also account for the apparent
difference between GaLU and ReLU in the regression model.
of all the predictions Hp induces on S is upper bounded by yJB maxi IlxilI2/m for P = 2 and by
∙√2log(2d)B2 maxi Ilxik∞/m, where dis the dimension, forp = 1. See for example Section 26.2
in Shalev-Shwartz & Ben-David (2014). This also induces an upper bound on the gap between the
test and train loss (see again Shalev-Shwartz & Ben-David (2014) for Lipschitz loss functions and
see Srebro et al. (2010) for the relation between Rademacher complexity and the generalization of
smooth losses such as the squared loss). The question is whether the `1 /`2 norm of w is correlated
with the amount of noise in the data. To study this, we depict the gap between train and test error as
a function of the norm of w for GaLU networks. As can be seen in figure 4, for both the `1 and `2
norm, there is a clear linear relation between IwI2p and the generalization gap. While the constants
are far from what the bounds state, the linear correlation is very clear.
Note that figure 4 deals with GaLU networks that were trained as linear functions (by using the
closed form solution for the MSE loss), and indeed shows that such network with such training
behave as the theory states for linear predictors. We do not get the same behavior when we (unnec-
essarily) train both layers of the network using SGD. This matches the discussion in Section 5 of
Zhang et al. (2016), where the correlation between the `2 norm of the weights in a ReLU network
and the test loss is discussed, and it is argued that there are more factors that affect the generaliza-
tion properties. Indeed, many followup works show different capacity measures that may be more
adequate for studying the generalization of deep learning (See for example Bartlett et al. (2017);
Neyshabur et al. (2017b; 2018); Arora et al. (2018); Neyshabur et al. (2017a); Kawaguchi et al.
(2017)). We next show a rather different analysis for a particular instance of linear regression.
4.2.2	Alternative Approach
Consider a simple linear regression using the MSE, and denote the train and test loss by
LS (W)	=	-1	X	(x>w	—	y)2	；	LD (W)= E(x,y)〜D	(x>w	—	y)2	.
(x,y)∈S
Given a training set S, the MSE estimator is defined as W(S) := arg minw LS (W).
We start with the following lemma.
Lemma 1 (Follows from Corollary 2 of Rosset & Tibshirani (2018)) For a scalar σ ≥ 0 and a
vector β ∈ Rd, let Dσ,β be the distribution over Rd × R which is defined by the following generative
6
Under review as a conference paper at ICLR 2019
SSo-U 更1，Sso-Is2
Figure 4:	A linear correlation between the generalization gap and the squared norm of the solution.
This was generated for GaLU network with a fixed readout layer, and the solution was calculated
analytically.
model: pick X 〜 N(0, Id), and pick y = x>β + σε where ε
have:
〜N(0,1). Then, for m > d + 1, we
S EDm [LS(w(S))] = σ2
〜 σ,β
S〜Em [Ke (W(S))] = σ2 (1 + md +
σ,β
d d + 1
m m -d-1
This lemma provides a complete analysis for the following experiment, which is similar to the
experiments reported by Zhang et al. (2016). We compare two distributions, the first is Dσ,β for
some vector β ∈ Rd and forσ being close to 0, and the second is D1,0. Note that the first distribution
corresponds to a case in which we would like to be able to generalize, while the second distribution
corresponds to a case in which we are fitting random noise and do not expect to generalize. We set
the training set size to be m = d + 2 and we analyze the MSE estimator, w(S). As the lemma
shows, the expected training losses on the first and second distributions are
σ2(1-9)=2σ2
mm
2
m
respectively. Hence, the training loss should be small on both of the distributions. In contrast, the
expected test loss on the first distribution is
σ2(1 + - + - d + 1 1 ≤ ≤ (3 + d)σ2
m m m -d-1
while the expected test loss on the second distribution is
1 + — +
m
d d +1
m m -d-
≥1.
We see that while the train loss can be small on both distributions, in the test loss We see a big gap
between the first distribution (assuming σ《1∕√d) and the second distribution of purely random
labels. This is exactly the type of phenomenon reported in Zhang et al. (2016) — a sample with a
small amount of noise achieves both small train and test losses, but a sample with random labels
achieves a small train loss but a large test loss. Note that this is a natural property of the least
squares solution, without any explicit regularization, picking a minimal-norm solution or using a
specific algorithm for solving the problem.
Lemma 1 gives us a very sharp analysis of linear regression. Unfortunately, the assumptions of
Lemma 1 (which are based on the assumptions of Corollary 2 in Rosset & Tibshirani (2018)) are
too strong — we need that m > d + 1 and that the instances will be generated based on a Gaussian
distribution. While Rosset & Tibshirani (2018) also includes asymptotic results that are applicable
for a larger set of distributions, we leave the application of them to GaLU networks for future work.
7
Under review as a conference paper at ICLR 2019
5 A FEW WORDS ABOUT Rd → Rd0 PROBLEMS WITH ONE HIDDEN LAYER
Networks
In the analysis of the Rd → R case we used the fact that a GaLU neuron gw,u is linear in the
parameter w, and it allowed us to rephrase the problem as a convex problem. In the Rd → Rd0 case
the situation is not as simple. In this case, every hidden neuron has d0 outgoing edges, and so we
cannot use the same reparametrization trick as before.
Even so, the output of a GaLU neuron is still linear in the parameter w. It means that for convex
loss functions, finding the optimal weights for the first layer, keeping the weights of the second one
constant, is a convex problem. The same doesn’t hold for ReLU networks. Finding the optimal
weights for the second layer, keeping the weights of the first one constant, is also a convex problem.
Even more specifically, the optimization problem over the two layers is biconvex (see Gorski et al.
(2007) for a survey). So instead of applying SGD, we can apply biconvex optimization algorithms,
such as Alternate Convex Search (ACS). In the case of the MSE loss, there is a closed form solution
for each step of ACS, and using it outperforms SGD for small enough samples2. Even though it is
of limited practical use, this algorithm might be interesting for the derivation of theoretical bounds
for such networks.
In addition, it turns out that as we increase the output dimension d0, GaLU and ReLU networks
becomes more similar. In section 4.1.1 we measured the difference between ReLU and GaLU for
the problem where all the variables are i.i.d. N(0, 1), and it turned out that ReLU outperforms GaLU
to a small extent. We repeated this experiment with larger d0, and saw that the difference between
the two vanished quickly (see figure 5).
Figure 5:	We empirically found the minimal num-
ber of neurons k such that a one hidden layer network
achieves MSE< 0.3 on the random regression problem.
As the output dimension d0 grows, more neurons are
needed. As demonstrated in figure 2, GaLU networks
needs more neurons than ReLU networks for output di-
mension d0 = 1. For larger d0 GaLU is slightly better,
but it is clear that the two networks exhibit very simi-
lar behavior. We used fixed sample size (m = 1024)
and input dimension (d = 32) in the generation of this
graph.
6 Conclusion & Further Work
The standard paradigm in deep learning is to use neurons of the form σ x>w for some differ-
entiable non linear function σ : R → R. In this article we proposed a different kind of neurons,
σi,j ∙ x>w, where σi,j is some function of the example and the neuron index that remains constant
along the training. Those networks achieve similar results to those of their standard counterparts,
and they are easier to analyze and understand.
To the extent that our arguments are convincing, it gives new directions for further research. Better
understanding of the one hidden layer case (from section 5) seems feasible. And as GaLU and
ReLU networks behave identically for this problem, it gives us reasons to hope that understanding
the behavior of GaLU networks would also explain ReLU networks and maybe other non-linearities
as well. As for deeper network, it is also not beyond hope that GaLU0 networks would allow some
better theoretical analysis than what we have so far.
2As it requires inverting a matrix, it is infeasible for large samples.
8
Under review as a conference paper at ICLR 2019
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Jochen Gorski, Frank Pfeuffer, and Kathrin Klamroth. Biconvex sets and optimization with biconvex
functions: a survey and extensions. Mathematical Methods of Operations Research, 66(3):373-
407, Dec 2007. ISSN 1432-5217. doi: 10.1007/s00186-007-0161-1. URL https://doi.
org/10.1007/s00186-007-0161-1.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Yann LeCun and Corinna Cortes.	MNIST handwritten digit database.
http://yann.lecun.com/exdb/mnist/, 2010.
Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. CoRR, abs/1702.08580, 2017.
URL http://arxiv.org/abs/1702.08580.
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.
In Advances in neural information processing systems, pp. 2627-2635, 2014.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017a.
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of opti-
mization and implicit regularization in deep learning. arXiv preprint arXiv:1705.03071, 2017b.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. To-
wards understanding the role of over-parametrization in generalization of neural networks. arXiv
preprint arXiv:1805.12076, 2018.
Saharon Rosset and Ryan J. Tibshirani. From fixed-x to random-x regression: Bias-variance
decompositions, covariance penalties, and prediction error estimation. Journal of the Amer-
ican Statistical Association, 0(ja):0-0, 2018. doi: 10.1080/01621459.2018.1424632. URL
https://doi.org/10.1080/01621459.2018.1424632.
Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y
Ng. On random weights and unsupervised feature learning. In ICML, pp. 1089-1096, 2011.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlin-
ear dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013. URL
http://arxiv.org/abs/1312.6120.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast
rates. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and
A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 2199-
2207. Curran Associates, Inc., 2010. URL http://papers.nips.cc/paper/
3894-smoothness-low-noise-and-fast-rates.pdf.
9
Under review as a conference paper at ICLR 2019
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. https://arxiv.org/abs/1708.07747, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http:
//arxiv.org/abs/1611.03530.
10