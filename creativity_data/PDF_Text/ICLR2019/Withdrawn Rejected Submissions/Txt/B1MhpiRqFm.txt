Under review as a conference paper at ICLR 2019
B oltzmann Weighting Done Right in Rein-
forcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
The Boltzmann softmax operator can trade-off well between exploration and ex-
ploitation according to current estimation in an exponential weighting scheme,
which is a promising way to address the exploration-exploitation dilemma in re-
inforcement learning. Unfortunately, the Boltzmann softmax operator is not a
non-expansion, which may lead to unstable or even divergent learning behavior
when used in estimating the value function. The convergence of value iteration
is guaranteed in a restricted set of non-expansive operators and how to character-
ize the effect of such non-expansive operators in value iteration remains an open
problem. In this paper, we propose a new technique to analyze the error bound
of value iteration with the the Boltzmann softmax operator. We then propose the
dynamic Boltzmann softmax(DBS) operator to enable the convergence to the op-
timal value function in value iteration. We also present convergence rate analysis
of the algorithm. Using Q-learning as an application, we show that the DBS oper-
ator can be applied in a model-free reinforcement learning algorithm. Finally, we
demonstrate the effectiveness of the DBS operator in a toy problem called Grid-
World and a suite of Atari games. Experimental results show that outperforms
DQN substantially in benchmark games.
1	Introduction
In sequential decision making problem, an agent learns to find an optimal policy that maximizes the
expected discounted long-term reward, which can be modeled by a Markov decision process (MDP).
In an MDP, the optimal value function is the fixed point of the Bellman operator. Thus, the opti-
mal value function can be computed by iterative updates from arbitrary initial value function, i.e.,
value iteration (Bellman (2013)). Littman & Szepesvari (1996) proposed the generalized MDP Con-
sidering generalized action selection operators, according to which the value function is estimated.
If the generalized action selection operator satisfies the non-expansion property, the uniqueness of
the fixed-point of the generalized Bellman operator is guaranteed, thus ensuring the convergence of
generalized value iteration algorithm. Examples of non-expansive operators include the max and the
mean operators.
Without full information about transition dynamics and reward function of the environment, in rein-
forcement learning, the agent aims to learn an optimal policy by interacting with the unknown envi-
ronment from experience. Reinforcement learning has achieved groundbreaking success for many
decision making problems, both in discrete and continuous domains, including robotics (Kober et al.
(2013)), game playing (Mnih et al. (2015)), and many others. One of the most fundamental chal-
lenges is how to balance exploration and exploitation, where the exploration-exploitation dilemma
occurs in action selection and value function optimization (Asadi & Littman (2016)). For value func-
tion optimization, the agent estimates the value function according to the action selection operator
and then updates the estimated value. A number of effective algorithms to address exploration-
exploitation dilemma have their root in alternating the action selection operator, which falls in the
generalized MDP framework.
One of the most important reinforcement learning algorithm, Q-learning, employs the the max oper-
ator for value function optimization. The max operator always greedily selects the action that gives
the best value and updates current estimation according to the value. As the current estimation for
value functions is not accurate, such greedy selection may lead to misbehavior, e.g., the overesti-
1
Under review as a conference paper at ICLR 2019
mation phenomenon (Hasselt (2010)). In fact, the max operator is the extreme case of exploitation,
lacking the ability to explore or consider other choices. On the other hand, the mean operator esti-
mates the value by computing the average. Thus, the mean operator solely explores which fails to
utilize current estimation.
The Boltzmann softmax operator is a natural summary operator that has been widely applied (Cesa-
Bianchi et al. (2017)). Specifically, it is an exponentially weighting scheme, where the weights are
computed according to the current estimation and its parameter β . The parameter β trades off be-
tween exploration and exploitation. When β → ∞, it behaves as the max operator which solely
favors exploitation while it behaves as the mean operator when β → 0 which purely focuses on
exploration. However, despite from the advantages, it is very challenging to apply the operator in
value function optimization. First, the parameter β is difficult to choose (Sutton et al. (1998)). Sec-
ond, as shown in (Littman & Szepesvari (1996); Asadi & Littman (2016)), the Boltzmann Softmax
operator is not a non-expansion, which may lead to multiple fixed-points and thus the value function
of this policy is not well-defined. The non-expansive property is vital to guarantee the uniqueness
of the fix-point and the convergence of the learning algorithm. Without such property, the learning
algorithm may misbehave or even diverge. Thus, it is widely believed that the Boltzmann soft-
max operator cannot be used directly due to the violation of the non-expansive property (Littman &
Szepesvari (1996); Asadi & Littman (2016)). In fact, as far as We know, how to characterize the use
of the Boltzmann softmax operator, which violates the property of non-expansion in value iteration,
remains an open problem (Littman (1996)).
In this paper, we study the property of the Boltzmann softmax operator and propose anew technique
to characterize its error bound in value iteration with fixed parameter β . To be specific, we show that
for the error bound between the value function induced by the Boltzmann softmax operator and the
optimal value function, there remains a term related to β that will not converge to 0. Thus, although
the Boltzmann softmax operator guarantees the approximate convergence of value iteration, it would
converge to a sub-optimal policy unfortunately. Indeed, the direct use of the Boltzmann softmax
operator inevitably introduces performance drop in value iteration.
We then take a step further and study an essential problem, is there a way that the Boltzmann softmax
operator be applied in value iteration which guarantees the convergence to the optimal policy?
Based on this technique, we propose the dynamic Boltzmann softmax operator boltzβt, termed the
DBS operator, to eliminate the loss and enable the convergence of value iteration to the optimal
value function. Our core idea is to dynamically change βt in value iteration and present its conver-
gence rate analysis. Then, we propose the DBS Q -learning algorithm with the application of the
DBS operator in a popular model-free reinforcement learning algorithm, i.e., Q-learning (Watkins
& Dayan (1992)), and prove the convergence of the DBS Q-learning.
We conduct experiments to verify the effectiveness and efficiency of our proposed dynamic Boltz-
mann softmax operator. We first evaluate DBS value iteration and DBS Q-learning on a tabular
case, the GridWorld. Results show that the DBS operator leads to smaller error and faster conver-
gence. We then demonstrate that the DBS operator can be extended to large scale problems, Atari
games. Using DQN as baseline, we show that DQN with the dynamic Boltzmann softmax operator
(abbreviated as DBS-DQN) substantially outperforms DQN in a suite of Atari benchmark games.
2	Preliminaries
A Markov decision process (MDP) is defined by a 5-tuple (S, A, p, r, γ), where S and A denote
the set of states and actions, p(s0|s, a) represents the transition probability from state s to state s0
under action a, and r(s, a) is the corresponding immediate reward. The discount factor is denoted
by γ ∈ [0, 1), which controls the degree of importance of future rewards.
At each time, the agent interacts with the environment with its policy π, a mapping from state to
action. The objective is to find an optimal policy that maximizes the expected discounted long-term
reward E[P∞=0 γtrt∣∏], which can be solved by estimating value functions. The state value of S
and state-action value of S and a under policy ∏ are defined as Vπ (S) = En [P∞=0 γtrt ∣so = s] and
Qn(s, a) = En[P∞=o γtrt∣so = s,a0 = a]. The optimal value functions are defined as V*(s)=
max∏ Vn(s) and Q*(s, a) = max∏ Qn(s, a).
2
Under review as a conference paper at ICLR 2019
Littman & Szepesvari (1996) proposed a general framework for reinforcement learning, where the
generalized action selection operator is denoted by N. The optimal value function V * satisfies the
generalized Bellman equation, which is defined recursively as in Equation (1):
V* (s) = O r(s, a) + X p(s0 |s, a)γV* (s0)	(1)
a∈A	s0∈S
Starting from arbitrary initial value function V0, the optimal value function V* can be computed
by value iteration (Bellman (2013)) according to an iterative update: Vk+1 = TVk, where T is the
generalized the Bellman operator as defined in Equation (2).
(T V)(s) = O r(s, a) + X p(s0|s, a)γV(s0).	(2)
a∈A	s0∈S
The convergence of value iteration is guaranteed if N is a non-expansion, which guarantees the
unique solution of the generalized Bellman equation (1), i.e., TV* = V*. The non-expansion is
defined as:
∣0Qι(s,a) — 孕 Q2(s,a)∣ ≤ IIQι(s,∙)-Q2(s,∙)ll∞,	(3)
where || ∙ ∣∣∞ denotes the '∞-norm.
The Boltzmann softmax operator is one kind of the action selection operator N, which is defined
as:
Pn x eβxi
boltzβ(X)= 2p1xiβχi .	(4)
3	Analysis of the B oltzmann S oftmax Operator
In this section, we first analyze the property of the Boltzmann softmax operator and then propose a
new technique to analyze its error bound in value iteration.
It has been shown that the Boltzmann softmax operator is not a non-expansion ((Littman &
Szepesvari (1996); Asadi & Littman (2016))) as it does not satisfy Inequality (3). Indeed, the
non-expansive property is vital to the convergence of the learning algorithm, which guarantees the
uniqueness of the fixed point. We first analyze the property of the Boltzmann softmax operator,
which paves the path for studying the effect of using such operators that violates the non-expansive
property in value iteration.
Proposition 1 For β > 0 and X, Y ∈ Rn, the Boltzmann softmax operator satisfies the following
property:
|boltzβ(X) — boltzβ(Y)I ≤ ||X — γ∣∣∞ + 2^°g(n),	(5)
β
where ∣∣∙∣∣∞ is the '∞-norm in Rn.
In Proposition 1, we show that although the Boltzmann softmax operator is not a non-expansive
operator, the degree of the violation of the non-expansive property is controlled by β. The larger the
value ofβ is, the closer it is to the non-expansion. Due to space limit, we put the proof of Proposition
1 in Appendix A.
Next, we propose a new technique to characterize the error bound of value iteration with Boltzmann
softmax operator in value iteration, where the full proof is referred to Appendix B.
Theorem 1 (Error bound of value iteration with Boltzmann softmax operator) Let Vt be the
value function computed by the Boltzmann softmax operator at the t-th iteration and V0 denote the
initial value. After t iterations,
∣∣v v*∣∣ L t∣∣v v*∣∣ , 4log(IAI)(I- Yt)	小
||Vt - V ii∞ ≤ γ 11VO - V ii∞ + 户(1 - Y)--------.	(6)
Taking the limit of t in both sides of Inequality (6), we obtain the following result:
3
Under review as a conference paper at ICLR 2019
Corollary 1 For the Boltzmann softmax operator boltzβ, the error of value functions is
limt→∞ ∣∣Vt+ι — V*∣∣≤ 4⅛⅛芈.
Corollary 1 characterizes the error bound of value iteration with the Boltzmann softmax operator.
With a fixed parameter β, the error is upper bounded by 蓝；-：/, which decreases with an in-
creasing value of β . Thus, the direct use of the Boltzmann softmax operator inevitably introduces
performance drop in practice. Motivated by the theoretical findings, we propose the dynamic Boltz-
mann softmax operator, which enables the convergence to the optimal.
4 Dynamic B oltzmann S oftmax Operator
In this section, we propose the dynamic Boltzmann softmax (DBS) operator boltzβt to eliminate
the error in value iteration. Next, we give theoretical analysis of the proposed DBS value iteration
algorithm. We prove that it converges to the optimal policy ifβt approaches ∞, as shown in Theorem
2. We then present the convergence rate analysis in Theorem 3. Finally, we show that the DBS
operator can be applied in a prominent model-free reinforcement learning algorithm, Q-learning,
with convergence guarantee.
From Corollary 1, although the Boltzmann softmax operator can converge, it may suffer from error
due to the violation of the non-expansive property. We propose the dynamic Boltzmann softmax
(DBS) operator to eliminate the error, which is motivated by Corollary 1 that although boltzβ is not
a non-expansive operator, it performs very close to the non-expansion when β is large enough.
Based on the DBS operator, we design the corresponding DBS value iteration algorithm. DBS value
iteration algorithm admits a dynamically changing series {βt} (line 1) and update the value function
according to the dynamic Boltzmann softmax operator boltzβt (line 6). Thus, the way to update the
value function is according to the exponential weighting scheme, which is related to both the current
estimation value and the parameter βt .
Algorithm 1: DBS Value Iteration
Input: An increasing series {βt }; termination condition θ
1	Initialize V (s), ∀s ∈ S arbitrarily
2	for each episode t = 1, 2, ... do
3	∆ - 0
4	for each s ∈ S do
5	V — V (S)
6	V (S) ― boltzβt(Ps0,r p(s0,r∣s,a)[r + YV (s0)])
7	∆ — max(∆, |v — V(s)|)
8	if ∆ < θ then
9	L break
4.1	Convergence Analysis
In Theorem 2, we demonstrate that the DBS operator can enable the convergence of DBS value
iteration to the optimal. Due to space limit, see Appendix C for proof of the theorem.
Theorem 2 (Convergence of value iteration with the DBS operator) For a sequence of dynamic
Boltzmann Sofmax operator {βt}, if βt → ∞, Vt converges to V *, where Vt and V * denote the
value function after t iterations and the optimal value function.
Theorem 2 implies that DBS value iteration does converge to the optimal policy if βt approaches
infinity. Although the Boltzmann softmax operator may violate the non-expansive property for some
values of β , we only need β approaches infinity to guarantee the convergence.
The convergence rate of the DBS operator is shown in Theorem 3, where the proof is provided in
Appendix C.
4
Under review as a conference paper at ICLR 2019
Theorem 3 (Convergence rate of value iteration with the DBS operator) For any power
series	βt	=	tp(p	>	0),	we have that for any ∈	(0, min{0.25,	||R||-1}),	after
max{O(log"log詈(1-γ) ),O(((1⅛) 1)} steps, the error ||Vt - V*|| ≤ e.
For the larger value of p, the convergence rate is faster. Note that when p approaches ∞, the conver-
gence bound is dominated by the first term.
From the above theoretical analysis of the DBS operator, we demonstrate that value iteration can still
converge to the optimal even with an operator violating the non-expansive property. Such finding
generalizes previous understanding of the convergence of value iteration, which is restricted to a
class of non-expansive operators. In addition, the convergence rate is of the same order as the
standard Bellman operator. This implies that the DBS operator will not lose too much in terms of
the convergence rate in value iteration. These findings provide theoretical background and pave
the way for the study of the use of the DBS operator in reinforcement learning algorithms, e.g., Q-
learning, which does not have full information about the model. In the absence of full information
about the model, the agent has to explores in the environment and exploits the optimal strategy.
4.2	Application: Q-learning
In this section, we show that the DBS operator can be applied in a model-free Q-learning algorithm
(Algorithm 9), which requires careful trade-off between exploration and exploitation.
The DBS Q-learning updates the Q-value according to the DBS operator, where it admits a dynam-
ically changing series βt . It is worth noting that in Theorem 3, the larger value of p results in faster
convergence rate in value iteration. However, this is not the case in Q-learning. Indeed, Q-learning
differs from value iteration in that it knows nothing about the environment, which means the agent
has to learn from experience. Thus, the agent needs to balance between exploration and exploitation.
If p is too large, it quickly approximates the max operator that favors pure exploitation.
Algorithm 2: DBS Q-learning
Input: An increasing series {βt}
ι Initialize Q(s,a), ∀s ∈ S, a ∈ A arbitrarily, and Q(terminal)-(state, ∙) = 0
2	for each episode	t	=	1, 2,	...	do
3	Initialize s
4	βt = f(t)
5	for each step of	episode	do
6	choose a from s using policy derived from Q
7	take action a, observe r, s0
8	Q(s, a) — Q(s, a) + α[r + Yboltzet (Q(s, ∙)) - Q(s, a)]
9	S — a
In Theorem 4, we prove that DBS Q-learning converges to the optimal policy under the same ad-
ditional condition as in DBS value iteration. The proof is based on the stochastic approximation
lemma in (Singh et al. (2000)), and the full proof is referred to Appendix E.
Theorem 4 (Convergence of DBS Q-learning) The Q-learning algorithm with dynamic Boltzmann
softmax policy given by
Qt+ι(st,at) = (1 - αt(st, at))Qt(st, at) + αt(st, at)[rt + Yboltzet (Qt(st+1, •))]	(7)
converges to the optimal Q*(s, a) values if
1.	The state and action spaces are finite.
2.	t αt (s, a) = ∞ and t αt2 (s, a) < ∞
3.	limt→∞ βt = ∞
4.	Var(r(s, a)) is bounded.
5
Under review as a conference paper at ICLR 2019
(a)
Environment.
----Q-Iearning
---- DBS Q-Ieaming (βt = t)
——DBS Q-Ieaming {βt≈t2)
DBS Q-Ieaming (βt = t3)
—— DBS Q-Ieaming (βt = t7)
O IOO 200	300	400	500
Episode
(b) Results of Q-learning.
Figure 1: The grid world experiment.
"SOT8=6O一
(c) Dynamic βt .	(d) Convergence bound.
Figure 2: Value iteration in the GridWorld.
(a) Training loss.	(b) Static β .
Note that different from value iteration, Q-learning does not know the full information about the
model and has to learn from experience. Thus, it is vital to trade-off exploration and exploitation.
Unlike the max operator, the DBS operator enables exploration in the begining of learning with a
small value of βt . Since the estimated value function is not accurate in the begining of learning, it is
better to weight possible choices according to the current estimation rather than greedily selecting
the maximum estimated value. The DBS favors exploitation more as βt increases, meaning that it is
able to utilize the information of current estimation.
5	Experiments
5.1	GridWorld
We first evaluate the performace of DBS value iteration DBS Q-learning in a toy problem, the
GridWorld (Figure 1(a)), which is a larger variant of the environment of (O’Donoghue et al. (2016)).
The GridWorld consists of 10 × 10 grids, with the dark grids representing walls. The agent starts
at the upper left corner and aims to eat the apple at the bottom right corner upon receiving a reward
of +1. Otherwise, the reward is 0. An episode ends if the agent successfully eats the apple or a
maximum number of steps 300 is reached. For this experiment, we consider the discount factor
γ = 0.9.
The training loss of value iteration is shown in Figure 2(a). As expected, larger value of β leads to
smaller loss. Figure 2(b) and Figure 2(c) demonstrate the training loss in logarithmic form for the
last episode. For static β, the value iteration algorithm suffers from some loss which decreases as β
increases. For dynamic βt , the performance of t2 and t3 are the same and achieve the smallest loss.
The convergence rate is illustrated in Figure 2(d). For higher order p of βt = tp , the convergence
rate is faster. We also see that the convergence rate of t2 and t10 is very close as discussed before.
Figure 1(b) demonstrates the number of steps the agent spent in each episode. DBS Q-learning with
βt = t2 achieves the best performance as it best trades off between exploration and exploitation.
When the power p of βt = tp increases, it performs closer to the max operator for exploitation.
When p = 1, it performs worse than Q-learning in this simple game as it explores more. Thus,
considering trading off between exploration and exploitation, we choose p = 2 in the following
experiments.
6
Under review as a conference paper at ICLR 2019
	Mean	Median
DQN	495.76%	84.72%
DBS-DQN	1611.49%	103.95%
Table 1: Summary of Atari games.
整S
三嗡W
l±Γ O
晋d纭国2
∙≥ ω <
s5s?求/s?s?/打国楼常誉常国公资
⅞ ? ? ? ? ? ξ ≡ H ÷ ≡ ≡ ≡ ⅛«
σ>
(S
-σ
TO
I

φ = t
I ⅛ ?
冬 O
Figure 3:	Relative human normalized score on Atari games.
5.2 Atari
We evaluate the DBS-DQN algorithm on 49 Atari games from the Arcade Learning Environment
(Bellemare et al. (2013)) by comparing it with DQN. For fair comparison, we use the same setup of
network architectures and hyper-parameters as in Mnih et al. (2015) for both DQN and DBS-DQN.
Note that DBS-DQN estimates the value for the next state according to the DBS operator, where
βt = ct2 and c is the coefficient. See Appendix F for full implementation details. For each game,
we train each algorithm for 50M steps. The evaluation procedure is identical to Mnih et al. (2015),
30 no-op evaluation, where the agent performs a random number (up to 30) of “do nothing” actions
in the beginning of an episode.
Table 1 shows the summary of results in human normalized score, which is defined as (Van Hasselt
et al. (2016)):
scoreagent - scorerandom
× 100%,
(8)
scorehuman - scorerandom
where human score and random score are taken from Wang et al. (2015). As illustrated in Table 1,
DBS-DQN significantly outperforms DQN in terms of both the mean and the median of the human
normalized score. To better characterize the effectiveness of DBS-DQN, its improvement over DQN
is shown in Figure 3, where the improvement is defined as the relative human normalized score:
Scoreagent - Scorebaseline
max{Scorehuman, Scorebaseline} -
Scorerandom
× 100%,
(9)
with DQN serving as the baseline. In all, DBS-DQN exceeds the performace of DQN in 33 out of
49 Atari games. Full scores of comparison is referred to Appendix G. Figure 4 shows the learning
curves for each algorithm. The results provide emprical evidence that the DBS operator trades-off
well exploration and exploitation in value function optimization.
6 Related Work
In reinforcement learning, avoiding the exploration-exploitation dilemma is a vital task. The Boltz-
mann softmax operator is a popular way to balance exploration and exploitation by exponentially
7
Under review as a conference paper at ICLR 2019
Assault
O 25	50	75 IOO 125	150	175	200
Training Epochs
(a) Assault
Enduro
0	25	50	75	100	125	150	175	200
Training Epcdis
(b) Enduro
DQN
QBSDQN
3poss∙Ead a>8"36eaλv
Rlverrald
0	25	50	75	100	125	150	175	200
Training ERChS
(c) Riverraid
3poss∙Ead a>8"36eaλv
0	25	50	75	100	125	150	175	200
Training Epochs
Seaquest
0	25	50	75	100	125	150	175	200
Training Epcdis
Zaxxon
0	25	50	75 IOO 125	150	175	200
Training ERChS
——DQN
——DBS-OQN
(d) RoadRunner	(e) Seaquest	(f) Zaxxon
Figure 4:	Learning curves in Atari games.
weighting its current estimation (Kaelbling et al. (1996)). To address the problem, a line of research
focuses on the exploration strategy where the agent should exploit current best action on the one
hand, but it needs to explore whether there are better possibilities on the other hand. (Singh et al.
(2000)) studied the convergence of on-policy algorithm, i.e., Sarsa. They show that the strategy
needs to be greedy in the limit, which guarantees that the optimal action can be selected. Although
they considered a dynamic parameter of β in the Boltzmann softmax operator, it depends on the
state, which is impractical in complex problems as Atari games. The other line studies the use
of alternative operators in value function estimation. To better trade-off between exploration and
exploitation, Asadi & Littman (2016) proposed the “Mellowmax” operator, where the degree of
exploration and exploitation is controlled by its parameter. However, although the “Mellowmax”
operator can approximate maximization in the limit, it converges to a sub-optimal policy rather than
the optimal policy. Haarnoja et al. (2017) utilized the log-sum-exp operator, which enables better
exploration and learns deep energy-based policies.
It is worth noting that meta learning cannot be applied to solve the problem since the Boltzmann
softmax operator with a fixed parameter would inevitably lead to error in value iteration, so there is
no optimal value of β.
7 Conclusion
We provide a new theoretical technique to analyze the error bound of the value iteration algorithm
with the Boltzmann softmax operator. Then, we develop the DBS value iteration algorithm based
on our proposed dynamic Boltzmann softmax (DBS) operator which enables convergence to the op-
timal value function and present convergence rate analysis. We show that the DBS operator can be
applied in a model-free reinforcement learning algorithm, Q-learning. Experimental results demon-
strate the effectiveness of the DBS operator and show that it can be extended to complex problems as
Atari games. For future work, it is worth studying the sample complexity of our proposed DBS Q-
learning algorithm. Itis also a promising direction to apply the DBS operator to other state-of-the-art
DQN-based algorithms, such as Rainbow (Hessel et al. (2017)).
8
Under review as a conference paper at ICLR 2019
References
Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning.
In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,
NS叱 Australia, 6-11 August 2017, pp. 243-252, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Richard Bellman. Dynamic programming. Courier Corporation, 2013.
Nicolo Cesa-Bianchi, ClaUdio Gentile, Gabor Lugosi, and Gergely Neu. Boltzmann exploration
done right. In Advances in Neural Information Processing Systems, pp. 6284-6293, 2017.
Chelo Ferreira and Jose L Lopez. Asymptotic expansions of the hurwitz-lerch zeta function. Journal
of Mathematical Analysis and Applications, 298(1):210-224, 2004.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Hado V Hasselt. Double q-learning. In Advances in Neural Information Processing Systems, pp.
2613-2621, 2010.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.
Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A
survey. Journal of artificial intelligence research, 4:237-285, 1996.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Michael L Littman and Csaba Szepesvari. A generalized reinforcement-learning model: Conver-
gence and applications. In Machine Learning, Proceedings of the Thirteenth International Con-
ference (ICML ’96), Bari, Italy, July 3-6, 1996, pp. 310-318, 1996.
Michael Lederman Littman. Algorithms for sequential decision making. 1996.
David JC MacKay and David JC Mac Kay. Information theory, inference and learning algorithms.
Cambridge university press, 2003.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Brendan O’Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy
gradient and q-learning. arXiv preprint arXiv:1611.01626, 2016.
Satinder Singh, Tommi Jaakkola, Michael L Littman, and Csaba Szepesvari. Convergence results
for single-step on-policy reinforcement-learning algorithms. Machine learning, 38(3):287-308,
2000.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press,
1998.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In AAAI, volume 2, pp. 5. Phoenix, AZ, 2016.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas.
Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581,
2015.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
9
Under review as a conference paper at ICLR 2019
A Property of the B oltzmann Softmax Operator
Proposition 1 For β > 0 and X, Y ∈ Rn, the Boltzmann softmax operator satisfies the following
property:
|boitzβ(X)-boltzβ(Y)I ≤ ||x-Yιι∞ +—β L	(IO)
where ∣∣∙∣∣∞ is the '∞-norm in Rn.
Proof 1 Let Lβ denote a log-sum-eXPfunction, i.e., Lβ (X) = β log(£n=1 eβxi)
Iboltzβ(X) - boltzβ(Y)I = 1(boltzβ(X) - Lβ(XX- (boltzβ(Y) - Lβ(Y)) + (Lβ(X)- Lβ(Y))∣
≤ Iboltzβ(X) - Lβ(X)I + Iboltzβ(Y) - Lβ(Y)I + ∣Lβ(X) - Lβ(Y)I
(11)
From MacKay & Mac Kay (2003), we have Iboltzβ(X) - Lβ(X)I ≤ hog(βXl)
Substitute the above inequality into Equation (11), we have
Iboltzβ(X) - boltzβ(Y)I ≤ 2⅛n) + 11log(XXeβxi) - 1log(χXeβyi)I
β β	i=1	β	i=1
(12)
「 e P⅛
xi
Let ∆i = Ixi - yi I and assume	in=1 eβxi ≥	in=1 eβyi without loss of generality. Then, we have
1	Pn eβxi	1	Pn eβ(yi+∆i)
β I log( PEI eβy	≤ β Ilog( P=I eβy	) I
1	Pn eβ(yi+l∣∆i∣l∞)
≤ ”g(UPitI eβyi	)I	(13)
≤ II∆iII∞
= IIx - yII∞
So we have
Iboltzβ(X) - boltzβ(Y)I ≤ 21og(n) + "x - y"∞	(14)
β
B Error B ound of the B oltzmann S oftmax Operator
Theorem 1 (Error bound of value iteration with Boltzmann softmax operator) Let Vt be the
value function comPuted by the Boltzmann softmax oPerator at the t-th iteration and V0 denote the
initial value. After t iterations, IIVt - V*II∞ ≤ YtIIV0 - V*II∞ + 41个《-；—γ ).
Proof 2 Let Tβ and Tm denote the dynamic Programming oPerators for the Boltzmann softmax
oPerator boltzβ and the max oPerator resPectively, so
(TeV)(s) = boltzβ(Q(s, ∙)), (TmV)(s) = max(Q(s, ∙))	(15)
Thus,
II(TβV1) -(TmV2)II∞ ≤ II(TβV1) - (TβV2)II∞+II(TβV2) -(TmV2)II∞	(16)
{} X{}
(A)	(B)
10
Under review as a conference paper at ICLR 2019
For the term (A), we have
∖∖(TβV1) - (TβtV2)∣∣∞ = max ∖boltzβ(Qι(s, ∙)) - boltzβ(Q2(s, ∙))∣
S
≤ maxmax ∖Q1 - Q2∖ + 2lθg(∣AI)
≤ max max Y	p(s0∖s, α)∖½(sz) — V2(s0)∖ + ?bg(""	( Q)
一Sa 上 β
≤ Y∖∖V1- ½∖∖∞ + 2loBAI)
P
For the term (B), we have
∖∖(TβVI)-(TmV1)∖∖∞ = max ∖boltzβ(Qι(s, ∙)) - max(Q1(s, ∙))∖
S
≤ max [∖boltz^β(Qι(s, ∙)) - Lβ(Qι(s, ∙))∖ + ∖Lβ(Qι(s, ∙)) - max(Q1(s, ∙))∖]
≤ 2log(∖A∖)
一β
(18)
Thus,
∖∖(τβ H)-(Tm V2)∖∖∞ ≤ 7∖∖Vι - V2∖∖∞ +4log(IAI)	(19)
β
As for the max operator; Tm is a contraction mapping, then from Banach fixed-point theorem we
have TmV * = V *
Let Tt denote the dynamic programming operator for a SeqUenCe of Boltzmann softmax operators,
then we have
∖M - V*∖∖∞ = ∖∖T% -TmV*∖∖∞
≤ y∖∖(T'TVd -TmTV*∖∖∞ + 4号IAI)
β
≤ ...
≤ YlM - V*II∞ + 4log(IAI) χχ Yi
β
X	k = 1
(20)
γtII% - V*II∞ +
4log(∖AI)(1- Y)
β(1 - Y)
C Convergence of DBS Value Iteration
Theorem 2 (Convergence of value iteration with the DBS operator) For a sequence ofdynamic
Boltzmann softmax operator {βt}, if βt → ∞, Vt converges to V *, where Vt and V * denote the
value function after t iterations and the optimal value function.
Proof 3 Let Tβt and Tm denote the dynamic programming operators for dynamic Boltz-
mann softmax operator boltzβt and the max operator respectively, so (TβtV)(s)	=
boltzβt (Q(s, ∙)), (TmV)(s) = max(Q(s, ∙)). Thus, we have
∖∖(Tβt VI)-(TmV2)∖∖∞ ≤ ∖∖(Tβt VI)-(Tβt V2)∖∖∞ + ∖∖(Tβt V2)- (TmV2)∖∖∞	(21)
`---------y---------' `----------y---------'
(A)	(B)
By similar techniques as in Theorem 6, we have
∖∖(TβtH)-(Tm匕)∖∖∞ ≤ Y∖∖V1- V2∖I∞ + 4log(IAI)	(22)
βt
11
Under review as a conference paper at ICLR 2019
As the max operator Tm is a contraction mapping, then from Banach fixed-point theorem we have
TmV * = V *
Let Tb denote the dynamic programming operator for a SequenCe of dynamic Boltzmann softmax
operators, so Tb = TetTβt_1 ∙∙∙Tβ1, then we have
IM - V*∣∣∞ = IITbtK -TmV*∣∣∞
=∣∣(Tβt ∙∙∙Tβι )V0 -(Tm ...Tm)V *∣∣∞
≤ Y∣∣(Tβt-1 ∙∙Tβ1 )V0 - (Tm..∙Tm)V*∣∣∞ + 4lθg(IAI)
β	(23)
≤ ...
t 〜t — k
≤ YtIM - V*∣∣∞ +4log(∣A∣) X γ-
k=1 Pk
Since limk→∞ *=0, we have that V∈ι > 0, ∃K > 0, such that Vk > K, ∣* ∣ < eɪ. Thus,
t 〜t —k K(eD t —fe	t
X = = X 丁 ' J)+1
k=1	k = 1	k=K(e1)⅛1
γt-k
丁
1	K("
≤ ɪ X Yt-k + €1
min p
k k=1
t
X Yl
k=K(e1) + 1
(24)
1 Yt—K(e1)(1 - yk(61))
min β	1 - Y	十七1
V 1 / Yt-K(CI)	)
- 1 — Y( min β + "
1(1 - γt-k(€1))
I- Y
If t > ι°g(⑸(I-Yg—")min β + K (€1) and €1 < €2(1 - Y) ,then ∑k=1 展 <€ -
_ t	t ,t — k ,
So we obtain that V€2 > 0, ∃T > 0, SUCh that Vt > T, ∣ Ek=I ⅛71 < €2.
t—k
Thus, limt→∞ Ek=I ⅛T = 0∙
Taking the limit of Equation (23), we have that
t t—k
lim IIVt+1 - V*II∞ ≤ lim [Yt∣IV1 - V*II∞ + 4log(IAI) X —3—] = 0
t→∞	t→∞	β—β βk
k=1
(25)
D Convergence Bound
Theorem 3 (Convergence rate of value iteration with the DBS operator) For any power
series βt = tp(p > 0), we have that for any € ∈ (0, min{0.25, ∣∣R∣∣T}), after
maχ{θ(log(1 lo+*E)),o(((1⅛)1)} StePS, the error IM - V*II ≤ €.
Proof 4
t	t-k	∞	-1
X Y— = Yt[X Y--
kp	kp
k=1	k=1
∞	_1
X -
J	kP
k=t+1
Polylogarithm
-y—(t+1) $(y—1 ,p,t + 1)]
'-----------------V-------}
Lerch transcendent
(26)
By Ferreira & Lopez (2004), we have
t γ-(t+1)	1	1
Ein(26) ≈Y γ≡1-ɪEy = (1 - Y)(t + 1)p
(27)
12
Under review as a conference paper at ICLR 2019
From Theorem 2 we have
I∣vt - V*|| ≤ YlM - V*|| +
(1 -lYg(t工。≤ 2maxSIk- V*|1，
4log(∣A∣)
(1 - Y)(t +1)。
} (28)
Thus, for any e > 0, after at most t = max{ klg( ‘ )+log( 1-γog+jo,1 |R| l)+log(4)，(8；；-(YAI))P — 1}
steps, we have ||Vt - V * || ≤ e.
E Convergence of DBS Q-Learning
Theorem 4 (Convergence of DBS Q-learning) The Q-learning algorithm with dynamic Boltzmann
softmax policy given by
Qt+ι(st, at) = (1 - αt(st, at))Qt(st, at) + αt(st, at)[rt + Yboltzet (Qt(St+1，・))]	(29)
converges to the optimal Q* (s， a) values if
1.	The state and action spaces are finite.
2.	t αt (s， a) = ∞ and	t αt2 (s， a) < ∞
3.	limt→∞ βt = ∞
4.	Var(r(s， a)) is bounded.
Proof 5 Let ∆t(s, a) = Qt(s, a) - Q*(s, a) and Ft(s, a) = Irt + Yboltzet (Qt(St+1，∙)) 一 Q*(s, a)
Thus, from (29) we have ∆t+ι(s, a) = (1 — αt(s, a))∆t(s, a) + αt(s, a)Ft(s, a), which has the
same form as the process defined in Lemma 2.
Next, we verify Ft(S， a) meets the required properties.
Ft(s, a) = Irt + Yboltzet (Qt(St+1，∙)) - Q*(s, a)
=(rt + YmaχQt(St+1，at+1) - Q*(s，a)) + γ(boltzβt(Qt(St+1，∙)) - maxQt(St+1，at+ι))
a+1	at+1
= Gt(S， a) + Ht(S， a)
(30)
For Gt, it is indeed the Ft function as that in Q-learning with static exploration parameters, which
satisfies
||E[Gt(S，a)]|Pt||w ≤ Y∣∣∆t∣∣w	(31)
For Ht, we have
|E[Ht(S，a)]| = y∣ Ep(SlS，a)[boltzet(Qt(S0，∙)) - maXQt(S0，a0)]]
s0
≤ Y∣ max[boltzet(Qt(S0，∙)) - maxQt(S0，a0)]∣
s0	a0
≤ Y max Iboltzet (Qt(S0，∙)) 一 max Qt(S0，a0)|
s0	a0
=Y max ∣(boltzβt(Qt(S0，∙)) - Let (Qt(S0，•))) + (Let (Qt(S0，∙)) - maX Qt(S0，a0))|
≤ Y max (Iboltzet (Qt(S0，∙)) - Let (Qt(S0，∙))∣ + |Let (Qt(S0，∙)) - max Qt(S0，a0)|)
s0	a0
≤
Y log(|AI) + Y max ∣ ɪlog( X eetQt(s0'α0)) - max Qt(S0，a0)|
βt	s βt	a0∈A	a
≤ 2Y log(|AI)
一	βt
(32)
Let ht = 2γ log,AI), so we have
||E[Ft(S，a)]|Pt||w ≤ Y∣∣∆t∣∣w + ht，	(33)
where ht converges to 0
13
Under review as a conference paper at ICLR 2019
F Implementation Details
The network architecture is the same as in (Mnih et al. (2015)). The input to the network is a raw
pixel image, which is pre-processed into a size of 84 × 84 × 4. Table 2 summarizes the network
architecture.
LAYER	TYPE	CONFIGURATION	ACTIVATION FUNCTION
1st	convolutional	#filters=32, size=8 × 8, Stride=4	ReLU
2nd	convolutional	#filters=64, size=4 × 4, stride=2	ReLU
3rd	convolutional	#filters=64, size=3 × 3, Stride=1	ReLU
4th	fully-connected	#Units=512	ReLU
output	fully-connected	#Units=#actions	—
Table 2: Network architecture.
14
Under review as a conference paper at ICLR 2019
G Atari Scores
GAMES	DQN	DBS-DQN
Alien	20.18	25.84
Amidar	56.73	67.26
Assault	780.99	851.52
Asterix	50.03	56.69
Asteroids	1.38	1.68
Atlantis	1651.23	23211.93
Bank Heist	59.66	81.08
Battle Zone	79.08	103.46
Beam Rider	49.89	55.04
Bowling	19.84	27.72
Boxing	732.5	730.25
Breakout	1332.64	1336.32
Centipede	25.86	37.16
Chopper Command	80.81	12.0
Crazy Climber	399.15	419.03
Demon Attack	659.59	473.09
Double Dunk	545.45	432.58
Enduro	84.72	108.93
Fishing Derby	163.77	196.0
Freeway	104.05	103.95
Frostbite	17.15	40.37
Gopher	395.37	556.44
Gravitar	9.44	7.89
H.E.R.O.	65.14	64.75
Ice Hockey	76.86	75.8
James Bond	270.09	295.29
Kangaroo	241.6	425.36
Krull	639.28	574.89
Kung-Fu Master	114.78	129.87
Montezumas Revenge	0.0	8.42
Ms. Pac-Man	41.81	37.45
Name This Game	102.76	110.91
Pong	113.88	116.19
Private Eye	0.18	2.98
Q*Bert	97.46	80.44
River Raid	38.27	41.63
Road Runner	504.66	573.03
Robotank	636.08	409.02
Seaquest	13.8	14.97
Space Invaders	101.55	76.54
Star Gunner	559.34	391.4
Tennis	232.26	166.45
Time Pilot	78.38	163.0
Tutankham	36.3	170.04
Up and Down	84.74	181.62
Venture	13.73	8.67
Video Pinball	12792.59	45791.36
Wizard Of Wor	51.05	54.68
Zaxxon	58.32	62.15
Figure 5: Human normalized scores across all games, starting with 30 no-op actions.
15