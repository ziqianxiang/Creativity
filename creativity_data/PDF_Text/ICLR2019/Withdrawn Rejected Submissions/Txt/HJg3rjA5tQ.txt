Under review as a conference paper at ICLR 2019
Spread Divergences
Anonymous authors
Paper under double-blind review
Ab stract
For distributions p and q with different support, the divergence D(p||q) generally
will not exist. We define a spread divergence D(p||q) on modified p and q and
describe sufficient conditions for the existence of such a divergence. We give
examples of using a spread divergence to train implicit generative models, includ-
ing linear models (Principal Components Analysis and Independent Components
Analysis) and non-linear models (Deep Generative Networks).
1 Introduction
A divergence D(p||q) (see, for example Dragomir (2005)) is a measure of the difference between
two distributions p and q with the property
D(p||q) ≥ 0, and	D(p||q) = 0	⇔ p = q	(1)
Some of our results are specific to the f -divergence, defined as
Df(PIIq)= Eq(X)	f(M
(2)
where f(x) is a convex function with f(1) = 0. An important special case of an f -divergence is the
well-known Kullback-Leibler divergence KL(pIIq)
Ep(x) [log 翳]
which is widely used to train
models using maximum likelihood. We are interested in situations in which the supports of the two
distributions are different, supp (p) 6= supp (q). In this case the divergence may not be defined. For
example, for p(x) being an empirical data distribution on continuous dataset x1 , . . . ,xN, p(x) =
N PnN=I δ (x - Xn) where δ (∙) is the Dirac Delta function. For a model q(χ) with support R, then
KL(qIIp) is not formally defined. This is a challenge since implicit generative models of the form
q(x) = δ (x - gθ(z)) p(z)dz only have limited support; in this case maximum likelihood to learn
the model parameter θ is not available and alternative approaches are required - see Mohamed &
Lakshminarayanan (2016) fora recent survey.
2 Spread Divergences
The aim is, from q(χ) andp(x) to define new distributions q(y) andp(y) that have the same support1.
Using the notation Jx to denote integration / (∙) dx for continuous x, and Pχ∈χ for discrete X with
domain X , we define a random variable y with the same domain as χ and distributions
P(y) = p p(y∣χ)p(χ),
x
q(y) = / p(y∣χ)q(χ)
x
(3)
where p(y∣χ) is a ‘noise, process designed to ‘spread, the mass of P and q such thatp(y) and q(y)
have the same support. For example, if We use a Gaussianp(y∣χ) = N (y∣χ,σ2), thenP and q both
have support R. We therefore use noise with the property that, despite D(p∣∣q) not existing, D(p||q)
does exist and we define the Spread Divergence
~ , ..
D(p∣∣q) ≡ D(p∣∣q)
(4)
Note that this satisfies the divergence requirement D(PIIq) ≥ 0. The second requirement, D(PIIq)
0 ⇔ P = q, is guaranteed for certain ‘noise, processes, as described in section(2.1).
1For simplicity, we use univariate x, with the extension to the multivariate setting being straightforward.
1
Under review as a conference paper at ICLR 2019
Spread divergences have many potential applications. For example, for a model q(χ∣θ) with param-
eter θ and empirical data distribution p(x), maximum likelihood training corresponds to minimising
KL(p||q) with respect to θ. However, for implicit models, the divergence KL(p||q) does not exist.
However, if a spread divergence exists, provided that the data is distributed according the model
p(x) = q(x∣θo) for some unknown parameter θo, the spread divergence D(P(X)∣∣q(χ∣θ)) has a
minimum at θ = θ0 . That is (for identifiable models) we can correctly learn the underlying data
generating process, even when the original divergence is not defined.
2.1 Noise Requirements for a S pread Divergence
Our main interest is in using noise to define a new divergence in situations in which the original
divergence D(p||q) is itself not defined. For discrete variables x ∈ {1, . . . , n}, y ∈ {1, . . . , n}, the
noise Pij = p(y = i|x = j) must be a distribution Pi Pij = 1, Pij ≥ 0 and
Pijpj =	Pij qj	∀i	⇒	pj = qj	∀j	(5)
jj
which is equivalent to the requirement that the matrix P is invertible, see appendix(B). There is an
additional requirement that the spread divergence exists. In the case of f -divergences, the spread
divergence exists provided that P and q have the same support. This is guaranteed if
PijPj > 0,	Pij qj > 0	∀i
jj
(6)
which is satisfied if Pij >0. In general, therefore, there is a space of noise distributions P(y|x) that
define a valid spread divergence. The ‘antifreeze’ method of Furmston & Barber (2009) is a special
form of spread noise to define a valid Kullback-Leibler divergence (see also Barber (2012)).
For continuous variables, in order that D(P||q) = 0 ⇒ P = q, the noise P(y|x), with dim(Y )
dim(X) must be a probability density and satisfy
P(y |x)P(x)dx =	P(y|x)q(x)dx	∀y ∈ Y
This is satisfied if there exists a transform P-1 such that
P-1 (x0 |y)P(y|x)dy = δ (x0 - x)
⇒ P(x) = q (x)	∀x ∈ X (7)
(8)
where δ (∙) is the Dirac delta function. As for the discrete case, the spread divergence exists provided
that P and q have the same support, which is guaranteed if p(y∣χ)>0. A well known example of such
an invertible integral transform is the Weierstrass Transform p(y∣x) = N (y∣x, σ2), which has an
explicit representation for P-1. In general, however, we can demonstrate the existence of a spread
divergence without the need for an explicit representation of P-1. As we will see below, the noise
requirements for defining a valid spread divergence such thatD(p∣0 = 0 ⇔ P = q are analogous to
the requirements on kernels such that the Maximum Mean Discrepancy MMD(P, q) = 0 ⇔ P = q,
see Sriperumbudur et al. (2011) and Sriperumbudur et al. (2012).
3 Stationary Spread Divergences
Consider stationary noise P(y|x) = K(y - x) where K(x) is a probability density function with
K(χ)>0, x ∈ R. In this caseP and q are defined as a convolution
P(y) = /
K(y — x)P(x)dx = (K * P) (y),
q(y)
/ K(y — x)q(x)dx = (K * q)(y) (9)
Since K>0, P and q are guaranteed to have the same support R. A sufficient condition for the exis-
tence of the Fourier Transform F {f} ofa function f(x) for real x is that f is absolutely integrable.
All distributions P(x) are absolutely integrable, so that both F {P} and F {q} are guaranteed to
exist. Assuming F {K} exists, we can use the convolution theorem to write
F{P} = F{K}F{P},	F {q} = F{K}F {q}
(10)
2
Under review as a conference paper at ICLR 2019
Hence, we can write
D(p∣∣q)=0 ⇔ P = q ⇔F{K}F{p} = F{K}F{q}⇔F{p} = F{q}⇔ P = q (11)
where we used the invertibility of the Fourier transform and assumed that F {K} 6= 0, or equiva-
lently2, F{K} >0. Hence, provided that K(x)>0 and F {K} >0 then K(x) defines a valid spread
divergence. Note that other transforms have a corresponding convolution theorem3 and the above
derivation holds, with the requirement that the corresponding transform of K(x) is non-zero. As an
example of such a noise process, consider Gaussian noise,
K(y - X) = √=1= e-2σ2(y-x)2	(12)
2πσ2
leading to a positive Fourier Transform:
1	∞	1 2	σ2 ω2
F{K} (ω) =	____ eiωxe-旬X dx = e-F > 0	(13)
2πσ2 -∞
Similarly, for Laplace noise K(x)=克e- 1|x|
p(y∣x) = K(y - x) = ɪe-b1y-x1,	F{K} (ω) = ʌ/ɪ, b-1 2 > 0	(14)
2b	π b-2 + ω 2
Since K>0 and F{K} >0, this also defines a valid spread divergence over R.
3.1	Invertible Mappings
Consider p(y|x) = K(y - f(x)) for strictly monotonic f. Then, using the change of variables
Py) = ∕κ(y -Z)Pz(Z)dz, Pz(z) = Px(f-1(z)) (∖J (X = f-1(z))∣)-1	(15)
where J is the Jacobian of f. For distributions p(x) with bounded domain, for example x ∈ [0, 1],
we can use a logit function, f(x) = - log x-1 - 1 , which maps the interval [0, 1] to R. Us-
ing then, for example, Gaussian spread noise P(y∖x) = N (y ∣f (x), σ2), both P(y) and q(y) have
support R. If D(PP∖∖qP) is zero then P = q on the domain [0, 1].
3.2	Maximising the Spread
From the data processing inequality (see appendix(A)), spread noise will always decrease the f-
divergence Df (PP(y)∖∖qP(y)) ≤ Df (P(x)∖∖q(x)). If we are to use a spread divergence to train a
model, there is the danger that adding too much noise may make the spreaded empirical distri-
bution and spreaded model distribution so similar that it becomes difficult to numerically distin-
guish them, impeding training. In general, therefore, it would be useful to add noise such that we
define a valid spread divergence, but can maximally still discern the difference between the two
distributions. To gain intuition, we define P and q to generate data in separated linear subspaces,
p(x) = J δ (x — a — AZ) P(z)dz, q(x) = J δ (x — b — BZ) P(z)dz, P(Z) = N (z ∣0,Iz). Using
Gaussian spread, P(y∖x) = N(y∖μ, ∑), What is the optimal μ, Σ that maximises the divergence?
Clearly, as Σ tends to zero, the divergence increases to infinity, meaning that we must at least con-
strain the entropy of P(y∖x) to be finite. In this case the spreaded distributions are given by
P(y)= N (y∣μ + a,AAT + ∑) ,	q(y) = N (y∣μ + b,BBT + ∑)	(16)
We define a simple Factor Analysis noise model With Σ = σ2I + uuT, Where σ2 is fixed and
uTu = 1. The entropy of P(y∖x) is then fixed and independent of u. Also, for simplicity, We
assume A = B. It is straightforWard to shoW that the spread divergence KL(PP∖∖qP) is maximised
for u pointing orthogonal to the vector AAT + σ2I -1 (b - a). Then u optimally points along the
direction in Which the support lies. The support of P(y∖x) must be the Whole space but to maximise
the divergence the noise preferentially spreads along directions defined by P and q, see figure(1).
2If F {K} can change sign, by continuity, there must exist a point at Which F {K} = 0.
3This includes the Laplace, Mellin and Hartley transforms.
3
Under review as a conference paper at ICLR 2019
Figure 1: Left: The lower dotted line denotes Gaussian distributed data p(x) with support only along
the linear subspace defined by the origin a and direction A. The upper dotted line denotes Gaussian
distributed data q(x) with support different from p(x). Optimally, to maximise the spread divergence
between the two distributions, for fixed noise entropy, we should add noise that preferentially spreads
out along the directions defined by p and q, as denoted by the ellipses.
4 Mercer Spread Divergence
We showed in section(3) how to define one form of spread divergence, with the result that stationary
noise distributions must have strictly positive Fourier Transforms. A natural question is whether, for
continuous x, there are other easily definable noise distributions that are non-stationary. To examine
this question, let x ∈ [a, b], y ∈ [a, b] and K(x, y) = K(y, x) be square integrable, K(x, y) ∈ L2 .
We define Mercer noise p(y|x) = K(x, y)/K(x), where K(x) = K(x, y)dy. For strictly positive
definite K, by Mercer’s Theorem, it admits an expansion
K(x, y) =	λnφn(x)φn(y)
n
(17)
where the eigenfunctions φn form a complete orthogonal set of L2 [a, b] and all λn >0, see for ex-
ample Sriperumbudur et al. (2011). Then
P⑻=XX ∕λnMx)(My)需 dx,
q(y) = X [ λnφn(x)φn(y) ^x)dx (18)
n	K (x)
and p(y) = q(y) is equivalent to the requirement
X / λn φn(x)φn(y) ^(X) dx = X / λnφn(x)φn(y) qx^ dx
n	K (x) n	K (x)
Multiplying both sides by φm(y) and integrating over y we obtain
φ Φm(x) -p(xT dx = φ Φm(x) -q(x) dx
K (x)	K (x)
(19)
(20)
If p(x)/K (x) and q(x)/K (x) are in L2[a, b] then, from Mercer’s Theorem, they can be expressed
as orthogonal expansions
p(x)	q(x)
衍=Σγn φn(x),	K(x)= Σγn φn(X)	(21)
Then, equation(20) is
φm(x) Xγnpφn(x)dx	= φm(x) Xγnqφn(x)dx
(22)
which reduces to (using orthonormality), γmp = γmq ⇒ p = q. Hence, provided K (x, y) =K (y,x)
is square integrable on [a, b] and strictly positive definite, then K (x, y)/K (x, y)dy defines valid
spread noise. For example, K	(x,	y)	= exp -λ1(x2 + y2) +exp	-λ2(x2 +	y2)	defines a strictly
positive non-stationary square integrable kernel on [a, b]. Provided p(x)/K (x) and q(x)/K (x) are
in L2[a, b] then the spread noisep(y|x) =K (x, y)/K (x) defines a valid spread divergence.
4
Under review as a conference paper at ICLR 2019
5 Applications
We demonstrate using a spread divergence to train implicit models
/
pθ(x)
δ (x - gθ (z)) p(z)dz
(23)
where θ are the parameters of the encoder g. We show that, despite the likelihood not being defined,
we can nevertheless successfully train the models using an EM style algorithm, see for example
Barber (2012). We then show how to train a deterministic non-linear generative model using a
variational approximation.
5.1	Deterministic Linear Latent Model
For observation noise γ, the Probabilistic PCA model (Tipping & Bishop, 1999) for X-dimensional
observations and Z-dimensional latent is
X = Fz + Yg Z 〜N(0,Iz),	E 〜N(0,Iχ),	pθ(X)= N (y ∣0, FFT + γ2Iχ) (24)
When γ = 0, the generative mapping from z to x is deterministic and the model pθ (x) has support
only on a subset ofRX and the data likelihood is in general not defined. In the following we consider
general γ, setting γ to zero at the end of the calculation. To fit the model to iid data {X1, . . . , XN}
using maximum likelihood, the only information required from the dataset is the data covariance
Σ. The maximum Iikeihood solution for PPCA is then F = UZ (ʌz - γ2Iz)2 R, where Λz, UZ
are the Z largest eigenvalues, eigenvectors of Σ; R is an arbitrary orthogonal matrix. Using spread
noisep(y∣x) = N (y∣x, σ2Iχ), the spreaded distributionpθ(y) is a Gaussian
Pθ (y) = N (y∣ 0,FF T + (γ2 + σ2)lχ)
(25)
Thus, pθ (y) is of the same form as PPCA, albeit with an inflated covariance matrix. Adding Gaussian
spread noise to the data also simply inflates the sample covariance to Σ0 = Σ + σ2Iχ. Since the
eigenvalues of Σ0 ≡ ∑+σ2Iχ are simply Λ0 = ʌ+σ2Iχ, with unchanged eigenvectors, the optimal
1	1
deterministic (γ = 0) latent linear model has solution F = UZ (Λ% - σ2Iz)2 R = UZΛZZR.
Unsurprisingly, this is the standard PCA solution; however, the derivation is non-standard since the
likelihood of the deterministic latent linear model is not defined. Nevertheless, using the spread
divergence, we learn a sensible model and recover the true data generating process if the data were
exactly generated according to the deterministic model.
5.2	Deterministic Independent Components Analysis
ICA corresponds to the model
p(X, z) = p(X|z)	p(zi)	(26)
i
where the independent components zi follow a non-Gaussian distribution. For Gaussian noise ICA
an observation X is assumed to be generated by the process
P(XIz)= YN (Xjlgj(z),Y2)	(27)
j
where gi(z) mixes the independent latent process z. In standard linear ICA, gj (z) = ajTz where aj is
thejth column on the mixing matrix A. For small observation noise γ2, the EM algorithm (Bermond
& Cardoso, 1999) becomes ineffective. To see this, consider X = Z and invertible mixing matrix
A, X = Az. At iteration k the EM algorithm has an estimate Ak of the mixing matrix. The M-step
updates Ak to
Ak+1 = E hXzTiE hzzTi	(28)
5
Under review as a conference paper at ICLR 2019
(a) Relative error |Aiejst - Aitjrue|/|Aitjrue| as a
function of the model noise standard deviation γ.
n
(b) Relative error |Aj - Airue|/|AtrueI as a
function of the number of datapoints N.
Figure 2: (a) For X = 20 observations and Z = 10 latent variables, we generate N = 20000
datapoints from the model x = Az, for independent zero mean unit variance Laplace components
on z. The elements of A used to generate the data are uniform random ±1. We use Sy = 1,
Sz = 1000 samples and 2000 EM iterations to estimate the mixing matrix. The relative error is
averaged over all i, j and 10 random experiments. We also plot standard errors around the mean
relative error. In blue we show the error in learning the underlying parameter using the standard EM
algorithm. As expected, as γ → 0, the error blows up as the EM algorithm ‘freezes’. In orange we
plot the error for EM using spread noise, as described in section(5.2.1); no slowing down appears as
the model noise γ decreases. As the model noise increases, the quality of the learned model under
spread noise decreases gradually. In (b) we show that, apart from very small N , the error for the
spread EM algorithm is lower than for the standard EM algorithm. Here Z = 5, X = 10, S = 1000,
γ = 0.2, with 500 EM updates used. Results are averaged over 50 runs of randomly drawn A.
where, for noiseless data (γ = 0),
E hxzTi = N XXn (A-Ixn)=SA-T,	E hzzTi = A-ISA-T	(29)
n
T1T
where S ≡ N En xnxn is the moment matrix of the data. Thus, Ak+ι = SA- A-k 1SA- ) =
Ak. and the algorithm ‘freezes’. Similarly, for low noise γ	1 progress critically slows down.
Whilst over-relaxation methods, see for example Winther & Petersen (2007) can help in the case of
small noise, for zero noise γ = 0, over-relaxation is ofno benefit.
5.2.1	Healing Critical Slowing Down
To deal with small noise and the limiting case of a deterministic model (γ = 0), we consider
Gaussian spread noise p(y∣x) = N (y ∣x, σ2Iχ) to give
p(y, z) =
p(y |x)p(x, z)dx = ∏N (y∣gi(Z), (γ2 + σ2) Iχ) ∏p(Zi)
ii
The empirical distribution is replaced by the spreaded empirical distribution
Py) = Nn X N (y∣xn, σ2Iχ)
n
The M-step has the same form as equation(28) but with modified statistics
E IyZTi= N χ/N (y∣xn,σ2) p(z∣y)yzTdzdy,
E 卜ZTi= N X N N (y∣xn,σ2) p(z∣y)zzT dzdy
The E-step optimally sets
P(ZIy) = Z^(y)N (zM(y), ς) Y P(Zi),	Zq (y) = / N (zμ(y), ς) Y P(Zi)dz
(30)
(31)
(32)
(33)
(34)
6
Under review as a conference paper at ICLR 2019
2 3 7 /O7 FRC /
.64 *4 q^¾M /%
s0.⅛m夕
方。O/6 ∖r g *
7 6，5 9 5ng r /
~>, 60O ⅛ #¾f *
NS 2* &r/ 8
,da Terq &M244
/⅛¼>7& q 彳&，「
y XI90 3 77 3。&
A65ΛbΛΛd19
SGy4475 95M
H35 3 3 / ə Z 幺 3
7/“^7 $4;s1 4
⅛ /to<3^li∙r∙739
Aa4。。/k>9fi
⅛“ζ6 q-认 4 45s
7ΛJ ΠΛ ʃ ? 6 7>41s
ZZ576∕q75
3√5g12>5"54
?。夕7，/3335
Cb夕 4X⅛G∕ 5J2ʃo
IA 3CΓ9JGqf
9J/ 7 5 y¾4∕ U-
7¼4∕67 夕Q +Z?
4»Sxn 7D75/G/
73λvλx∕</
∕26Bqr产7。D
r6f + 8c3¾s7
Γy2393M23ς
(a) δVAE samples	(b) Noisy VAE means	(c) Noisy VAE samples
Figure 3: Comparison of deep generative models trained on the MNIST digits after 300K iterations.
(a) Samples from the trained deterministic model. (b) Means from a standard (noisy) VAE with
fixed observation noise. (c) Samples from the standard noisy VAE model.
where Zq(y) is a normaliser and
Σ = (γ2 + σ2) (ATA)	,	μ(y) = (ATA) Ay	(35)
We can rewrite the expectations required for the E-step of the EM algorithm as
E IyzTi= Nn XZN (y∣xn,σ2) N(Zmy),刀,Pzi)yzTdzdy
E 卜Ti= N XZN(y∣χn,σ2)N(z∣μ(y), ∑) ɪzpzi)ZzTdzdy
(36)
(37)
Generally the posterior p(z∣y) will be peaked around N(z∖μ(y), Σ) and writing the expectations
with respect to N (z∖μ(y), Σ) allows for an effective sampling approximation focussed on regions
of high probability. We implement this update by drawing Sy samples from N (y∣Xn,σ2l) and, for
each y sample, We draw Sz samples from N (z∣μ(y), Σ). This scheme has the advantage over more
standard variational approaches, see for example Winther & Petersen (2007), in that we obtain a
consistent estimator of the M-step update for A4. We show results for a toy experiment in figure(2),
learning the underlying mixing matrix in a deterministic non-square setting. Note that standard
algorithms such as FastICA (Hyvarinen, 1999) fail in this setting. The noise value is set to σ =
max(0.001,2.5 * sqrt(mean(AAT))), for estimated mixing matrix A of the underlying deterministic
model xn = Azn , n = 1, . . . , N . The EM algorithm learns a good approximation of the unknown
mixing matrix and latent components zn , with no critical slowing down.
5.3	Training Implicit Non-linear Models
For a deterministic non-linear implicit model, we set p(z) = N(z∣0,I) and parameterise gθ(x)
by a deep neural network. The likelihood equation(23) is in general intractable and it is natural to
consider a variational approximation (Kingma & Welling, 2013),
logpθ(χ) ≥-/ qφ(z∣χ)
(log qφ(z∣χ) + log (pθ (χ∣z)p(z))) dz
(38)
However, since pθ(x|z) = δ (x 一 gθ(z)) this bound is not well defined. Instead, we minimise the
spread divergence KL(p(y)∣∣pθ(y)). The approach is a straightforward extension of the standard
4We focus on demonstrating how the spread divergences heals critical slowing down, rather than deriving
a state-of-the-art approximation of p(z|y). The importance sampling approach has fast run time and works
well, even for large latent dimensions, Z = 50. We also implemented a variational factorised approximation
ofp(z|y) but found this to be relatively slow and ineffective. A variational Gaussian approximation of p(z|y)
improves on the factorised approximation, but is still slow compared to the importance sampling scheme.
7
Under review as a conference paper at ICLR 2019
Figure 4: Comparison of training approaches for the CelebA dataset. All models had the same
structure and were trained using the same Adam settings, as in the MNIST experiment.
variational autoencoder and in appendix(C) we provide details of how to do this, along with higher
resolution images of samples from the generative model. We dub this model and associated spread
divergence training the 'δVAE'. As a demonstration, We trained a generative network on the MNIST
dataset, see figure(3) and appendix(D). We used Gaussian spread noise σ = 1 for the δVAE and
observation noise σ = 0.5 for the standard noisy VAE. The network gθ(x) contains 8 layers, each
layer with 400 units and relu activation function and latent dimension Z = 64. We also trained
a deep convolutional generative model on the CelebA dataset (Liu et al., 2015), see figure(4) and
appendix(E). We pre-process CelebA images by first taking 140x140 centre crops and then resizing
to 64x64. Pixel values were then rescaled to lie in [0, 1]. We use Gaussian spread noise σ = 0.5 for
the δVAE and observation noise σ = 0.5 for the standard noisy VAE.
6 Summary
We described an approach to defining a divergence, even when two distributions to not have the same
support. The method introduces a ‘noise’ variable to ‘spread’ mass from each distribution to cover
the same domain. Previous approaches (Furmston & Barber, 2009; S0nderby et al., 2016) can be
seen as special cases. We showed that defining divergences this way enables us to train deterministic
generative models using standard ‘likelihood’ based approaches. Indeed, for simple models such as
Independent Components Analysis, we showed how we can implement a principled learning method
based on classical EM training, without the standard difficulty of critical slowing down in the case
of small (or zero) observation noise.
Introducing noise means that an additional expectation is required. This can be carried out, in part,
exactly, although additional approximations using perturbation theory are possible, similar to Roth
et al. (2017). Spread divergences have deep connections to other approaches to define measures
of disagreement between distributions. In particular, one can view the spread divergence as the
probabilistic analogue of MMD, with conditions required for the existence of the spread divergence
closely related to the universality requirement on MMD kernels (Micchelli et al., 2006).
Theoretically, we can learn the underlying true data generating process by the use of any valid
spread divergence — for example for fixed Gaussian spread noise. In practice, however, the quality
of the learned model can depend on the choice of spread noise. In this work we fixed the spread
noise, but showed that if we were to learn the spread noise, it would preferentially spread mass
across the manifolds defining the two distributions. In future work, we will investigate learning
spread noise to maximally discriminate two distributions, which would involve a minimax model
training objective, with an inner maximisation over the spread noise and an outer maximisation
over the model parameters. This would bring our work much closer to adversarial training methods
(Goodfellow, 2017).
8
Under review as a conference paper at ICLR 2019
References
D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, New York,
NY, USA, 2012. ISBN 0521518148, 9780521518147.
O. Bermond and J. F. Cardoso. Approximate likelihood for noisy mixtures. In Proc. ICA 99, pp.
325-330,1999.
S.	S. Dragomir. Some general divergence measures for probability distributions. Acta Mathematica
Hungarica, 109(4):331-345, Nov 2005. ISSN 1588-2632. doi: 10.1007/s10474-005-0251-6.
T.	Furmston and D. Barber. Solving deterministic policy (PO)MPDs using Expectation-
Maximisation and Antifreeze. In First international workshop on learning and data mining for
robotics (LEMIR), pp. 56-70, 2009. In conjunction with ECML/PKDD-2009.
S. Gerchinovitz, P. Menard, and G. Stoltz. Fano's inequality for random variables. arXiv, 2018. doi:
arXiv:1702.05985v2.
I. J. Goodfellow. NIPS 2016 Tutorial: Generative Adversarial Networks. CoRR, abs/1701.00160,
2017.
A. Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE
Transactions on Neural Networks, 10(3):626-634, May 1999. ISSN 1045-9227. doi: 10.1109/
72.761722.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. arXiv:1312.6114 [stat.ML], 2013.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep Learning Face Attributes in the Wild. In Proceedings
of International Conference on Computer Vision (ICCV), 2015.
C. A. Micchelli, Y. Xu, and H. Zhang. Universal Kernels. Journal of Machine Learning Research,
6:2651-2667, 2006.
S. Mohamed and B. Lakshminarayanan. Learning in implicit generative models. arXiv preprint,
2016. doi: arXiv:1610.03483.
K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. Stabilizing Training of Generative Adversarial
Networks through Regularization . arXiv:1705.09367, 2017.
C. K. S0nderby, J. Caballero, L. Theis, W. Shi, and F. Huszar. Amortised map inference for image
super-resolution. arXiv preprint arXiv:1610.04490, 2016.
B. Sriperumbudur, K. Fukumizu, A. Gretton, B. Scholkopf, and G. Lanckriet. On the Empirical
Estimation of Integral Probability Metrics. Electronic Journal of Statistics, 6:1550-1599, 2012.
B. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet. Universality, Characteristic Kernels
and RKHS Embedding of Measures. J. Mach. Learn. Res., 12:2389-2410, July 2011. ISSN
1532-4435.
M. E. Tipping and C. Bishop. Probabilistic principal component analysis. Journal of the Royal
Statistical Society, Series B, 21/3:611622, January 1999.
S. L. Warner. Randomised response: a survey technique for eliminating evasive answer bias. Journal
of the American Statistical Association, 60(309):63-69, 1965.
O. Winther and K. B. Petersen. Bayesian independent component analysis: Variational methods and
non-negative decompositions. Digital Signal Processing, 17(5):858 - 872, 2007. ISSN 1051-
2004. Special Issue on Bayesian Source Separation.
M. Zhang, T. Bird, R. Habib, T. Xu, and D. Barber. Training Generative Latent Models by Varia-
tional f -Divergence Minimization. arXiv preprint, 2018.
9
Under review as a conference paper at ICLR 2019
A Spread noise makes distributions more similar
The data processing inequality for f -divergences (see for example Gerchinovitz et al. (2018)) states
thatDf(p(y)∣∣q(y)) ≤ Df(P(X)||q(x)). For completeness, we provide here an elementary proof of
this result. We consider the following joint distributions
q(y, x) = p(y|x)q(x),	p(y,x) = p(y|x)p(x)	(39)
whose marginals are the spreaded distributions
Ip(V) = p p(ylx)p(χ),
x
q(y) = / p(y|X)q(X)
x
(40)
The divergence between the two joint distributions is
D Jr)(行 x)|| o(y X))= / n(1j Xf /p(y|X)P(X)) — D Jr)(X)|| O(X))
Df(P(y,x)llq(y,X)) = J q(y, X)J <p(y∣X)q(X), = Df(P(X 川 q(X))
(41)
Thef-divergence between two marginal distributions is no larger than thef-divergence between the
joint (see also Zhang et al. (2018)). To see this, consider
Df (p(u, v)||q(u, v))
q(u)
q(v|u)f
P(u,v)
.q(u, V)
dydu
≥ ZqIUuf (∕q(VIu) q(⅛)d小U
q(u)f
du = Df (P(u)||q(u))
Hence,
Df(P(y)I∣q(y)) ≤Df(P(y,X)I∣q(y,X)) = Df(P(X)IIq(X))	(42)
Intuitively, spreading two distributions increases their overlap, reducing the divergence. When P and
q do not have the same support, Df (q(X)IIP(X)) can be infinite or not well-defined.
B	Injective Linear Mappings
Consider an injective linear mapping T from space V to W. From the rank nullity theorem for finite
dimensional spaces,
dim(image(T)) + dim(kernel(T)) = dim(V )	(43)
IfT is injective, then dim(kernel(T)) = 0. If dim(V ) = dim(W) then
dim(image(T )) = dim(W )	(44)
Since image(T) ⊆ W, it must be that image(T) = W. Hence, injective linear maps between
between two (finite dimensional) spaces of the same dimension are surjective; equivalently, they are
invertible.
In the context of spread noise, since the domain of X and y are equal and P(y) is defined through
a linear transformation of P(X), the requirement in (5) that the mapping is injective is equivalent to
the requirement that the mapping is invertible.
C S pread Divergence for Deterministic Deep Generative Models
Instead of minimising the likelihood, we train an implicit generative model by minimising the spread
divergence
min KL(P(y)IIPθ (y))	(45)
θ
10
Under review as a conference paper at ICLR 2019
where
1N
Py) = N EN (y∣xn,σ2Iχ)
n=1
and
Pθ (y) = / p(y∣χ)pθ (χ)dχ
/N (y∣gθ(z),σ2Iχ) p(z)dz = /Pθ(y∣z)p(z)dz
According to our general theory,
min KL(P(y)∣∣Pθ (y)) =0	⇔	P(X) = pθ (x)
θ
(46)
(47)
(48)
Here
KL(P(y)Hpθ(y)) = N X ZN (y∣xn,σ2Ix) logq(y)dy + const.
N n=1
(49)
Typically, the integral over y will be intractable and we resort to an unbiased sampled estimate
(though see below for Gaussian q). Neglecting constants, the KL divergence estimator is
1NS
ns XX log q(yn)
n=1 s=1
(50)
where yn is a noisy sample of xn, namely yn 〜N (yf ∣Xn, σ2Iχ). In most cases of interest, with
non-linear g, the distribution qP(y) is intractable. We therefore use the variational lower bound
log PPθ (y) ≥
/ qφ(z∖y) (-log qφ(z∣y)log(Pθ(y I Z)P(Z))) dz
(51)
Parameterising the variational distribution as a Gaussian,
qφ(z∖y) = N (z∣μφ(y), £¢3))
(52)
then we can reparameterise and write
logPθ(y) ≥ H(∑φ) + EN(e∣0,1) [log (pθ(y∖z = μφ + Cφθqz(z = μφ + Cφe))]	(53)
where H is the entropy of a Gaussian with covariance Σφ . For Gaussian spread noise in D dimen-
sions, this is (ignoring constants)
logPθ(y) ≥ H(∑φ)+En(β∣0,1)
1
(2σ2)D∕2
(y - gθ (μφ(y) + Cφe))2 + logqz(Z = μφ(y) + Cφe)
(54)
where Cφ is the Cholesky decomposition of Σφ .
The overall procedure is therefore a straightforward modification of the standard VAE method
Kingma & Welling (2013) in which both the model and data are corrupted by noise:
1.	Choose a noise corruption variance σ2 .
2.	Choose a tractable family for the variational distribution, for example qφ(Z∖y)
N (z∖μφ(y), Σφ(y)) and initialise φ.
3.	We then sample a noisy version yn for each datapoint (if we’re using S = 1 samples)
4.	Draw samples to estimate logPPθ(yn), equation(54)
5.	Do a gradient ascent step in (θ, φ).
6.	Go to 3 and repeat until convergence.
11
Under review as a conference paper at ICLR 2019
We note that for Σφ independent of y, we can partially integrate equation(54) over y to give the
bound
/N (y∣x,σ2Iχ) logP(y) ≥ H(∑φ) + EN(e∣0,1)[log qz(z = μφ(y)+ Cφ6)]
[EN (y∣x,σ2Iχ)
- ^D2EN9 0,I)
h(y - gθ (μφ(y) + Cφe))2ii
(55)
(56)
where
EN(y∣x,σ2Iχ) [(y - gθ (μφ(y) + Cφe))2]
=σ2 - 2EN(∈χ ∣o,iχ) [eχgθ(μφ(X + σ5))] + EN(∈χ ∣o,iχ)卜/ - gθ(μφ (X + Qax)))2]
(57)
Similar to Roth et al. (2017), in principle, one can form a perturbation approximation of the above
to second order in ax and express the integral over the spread noise as a form of regularisation;
however, in our experiments We found that the above works well - We therefore leave such analysis
for future work.
D MNIST Experiment
We first scaled the MNIST data to lie in [0.05, 0.95] and then transformed using the logit (inverse
logistic sigmoid) of the pixel value in order to use Gaussian spread noise. We use Gaussian spread
noise σ = 1 for the δVAE and observation noise σ = 0.5 for the standard noisy VAE. The network
gθ(X) contains 8 layers, each layer with 400 units and relu activation function and latent dimension
Z = 64. The variational inference network qφ(z∣y) = N (z∣μg(y), σφIZ) has a similar structure
for the mean network μφ(y). Learning was done using the Adam optimiser with learning rate 10-4
and exponential decay rate of 0.96 every 10000 iterations.
E CelebA Experiment
Both encoder and decoder used fully convolutional architectures with 5x5 convolutional filters and
used vertical and horizontal strides 2 except the last deconvolution layer we used stride 1. Here
Convk stands for a convolution with k filters, DeConvk for a deconvolution with k filters, BN for
the batch normalization Ioffe & Szegedy (2015), ReLU for the rectified linear units, and FCk for the
fully connected layer mapping to Rk .
X ∈ R64×64×3 → Conv128 →BN→ Relu
→ Conv256 → BN → Relu
→ Conv512 → BN → Relu
→ Conv1024 → BN → Relu → FC64
z ∈ R64 → FC8×8×1024
→ DeConv512 → BN → Relu
→ DeConv256 → BN → Relu
→ DeConv128 → BN → Relu
→ DeConv64 → BN → Relu → DeConv3
12
Under review as a conference paper at ICLR 2019
?。夕7//3335
fe夕 8d>5JZrΛ
--ix4 Mq 夕 JS 夕夕
ɔʃ/ 75q¾4/，
7¼4,67 夕 Q +Z?
4身<7d!k3∕⅛/
^r^367361A∕t/
/ 7- ə ⅛ 产78。
Γ6rg8,3¾5 1
ry2393M?3G
Figure 5: δVAE samples
266Λbɔod「夕
5 0 y 4 4 7 5 夕 3 t⅛
H rʌ 5 3 今 / a Z ± *⅛
y∕s⅛7 % 」 S ) 4
Gbəɜli-r73t
0α4 "。/%>*fi
⅛〃zb qNd o-tcs
7ΛJ 7J⅛r,6 794S
£>7 457 GJ。，5
Figure 6: Noisy VAE means
13
Under review as a conference paper at ICLR 2019
Figure 7: Noisy VAE samples
Figure 8: δVAE samples
14
Under review as a conference paper at ICLR 2019
Figure 9: Noisy VAE means
Figure 10: Noisy VAE samples
15