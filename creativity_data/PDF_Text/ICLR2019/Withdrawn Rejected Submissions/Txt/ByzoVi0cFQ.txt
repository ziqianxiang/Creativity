Under review as a conference paper at ICLR 2019

TRANSFER  LEARNING  FOR  ESTIMATING  CAUSAL  EF-

FECTS  USING  NEURAL  NETWORKS

Anonymous authors

Paper under double-blind review

ABSTRACT

We develop new algorithms for estimating heterogeneous treatment effects, combin-
ing recent developments in transfer learning for neural networks with insights from
the causal inference literature.  By taking advantage of transfer learning, we are
able to efficiently use different data sources that are related to the same underlying
causal mechanisms. We compare our algorithms with those in the extant literature
using extensive simulation studies based on large-scale voter persuasion experi-
ments and the MNIST database. Our methods can perform an order of magnitude
better than existing benchmarks while using a fraction of the data.

1    INTRODUCTION

The rise of massive datasets that provide fine-grained information about human beings and their
behavior provides unprecedented opportunities for evaluating the effectiveness of treatments. Re-
searchers want to exploit these large and heterogeneous datasets, and they often seek to estimate
how well a given treatment works for individuals conditioning on their observed covariates.  This
problem is important in medicine (where it is sometimes called personalized medicine) (Henderson
et al., 2016; Powers et al., 2018), digital experiments (Taddy et al., 2016), economics (Athey and
Imbens, 2016), political science (Green and Kern, 2012), statistics (Tian et al., 2014), and many
other fields. Although a large number of articles are being written on this topic, many outstanding
questions remain. We present the first paper that applies transfer learning to this problem.

In the simplest case, treatment effects are estimated by splitting a training set into a treatment 
and
a control group.  The treatment group receives the treatment,  while the control group does not.
The outcomes in those groups are then used to construct an estimator for the Conditional Average
Treatment Effect (CATE), which is defined as the expected outcome under treatment minus the
expected outcome under control given a particular feature vector (Athey and Imbens, 2015). This is a
challenging task because, for every unit, we either observe its outcome under treatment or control,
but   never both. Assumptions, such as the random assignment of treatment and additional regularity
conditions, are needed to make progress. Even with these assumptions, the resulting estimates are
often noisy and unstable because the CATE is a vector parameter. Recent research has shown that it 
is
important to use estimators which consider both treatment groups simultaneously (Künzel et al., 
2017;
Wager and Athey, 2017; Nie and Wager, 2017; Hill, 2011). Unfortunately, these recent advances are
often still insufficient to train robust CATE estimators because of the large sample sizes required
when the number of covariates is not small.

In this paper, we show how these difficulties in estimating the CATE can sometimes be overcome
through the use of transfer learning. In particular, we provide several strategies for utilizing 
ancillary
datasets that are related to the causal mechanism under investigation.  Examples of such datasets
include observations from:  experiments in different locations on different populations, different
treatment arms, different outcomes, and non-experimental observational studies.  We show that,
by transferring information from these ancillary datasets, CATE estimators can converge to better
solutions with fewer samples.  This is particularly important for CATE estimation, as the cost of
collecting additional data is quite high and often requires real-world data collection.

Our contributions are as follows:

1.  We introduce the new problem of transfer learning for estimating heterogeneous treat-
ment effects.

1


Under review as a conference paper at ICLR 2019

2.  MLRW  Transfer  for  CATE  Estimation  adapts  the  idea  of  meta-learning  regression
weights (MLRW) to CATE estimation. By using a learned initialization, regression problems
can be optimized much more quickly than with random initializations. Though a variety of
MLRW algorithms exist, it is not immediately obvious how one should use these methods for
CATE estimation. The principal difficulty is that CATE estimation requires the simultaneous
estimation of outcomes under both treatment and control, but we only observe one of the
outcomes for any individual unit.  Most MLRW transfer methods optimize on a per-task
basis   to estimate a single quantity. We show that one can overcome this problem with clever
use of the Reptile algorithm (Nichol et al., 2018).

3.  We  provide  several  additional  methods  for  transfer  learning  for  CATE  estimation:

warm start, frozen-features, multi-head, and joint training.

4.  We apply our methods to difficult data problems and show that they perform better
than existing benchmarks.  We reanalyze a set of large field experiments that evaluate
the effect of a mailer on voter turnout in the 2014 U.S. midterm elections (Gerber et al.,
2017). This includes 17 experiments with 1.96 million individuals in total. We also simulate
several randomized controlled trials using image data of handwritten digits found in the
MNIST database (LeCun, 1998). We show that our methods, MLRW in particular, obtain
better than state-of-the-art performance in estimating CATE, and that they require far fewer
observations than extant methods.

5.  We provide open source code for our algorithms.1

2    CATE ESTIMATION

We begin by formally introducing the CATE estimation problem. Following the potential outcomes
framework (Rubin, 1974), assume there exists a single experiment wherein we observe N  i.i.d.
distributed units from some super population, (Yi(0), Yi(1), Xi, Wi)          . Yi(0)      R 
denotes the
potential outcome of unit i if it is in the control group, Yi(1)      R is the potential outcome of 
i if it is
in     the treatment group, Xi     Rd  is a d-dimensional feature vector, and Wi      0, 1   is the 
treatment
assignment.  For each unit in the treatment group (Wi  =  1), we only observe the outcome under
treatment, Yi(1). For each unit under control (Wi = 0), we only observe the outcome under control.
Crucially, there cannot exist overlap between the set of units for which Wi = 1 and the set for 
which
Wi     = 0. It is impossible to observe both potential outcomes for any unit. This is commonly 
referred
to as the fundamental problem of causal inference.

However, not all hope is lost. We can still estimate the Conditional Average Treatment Effect (CATE)
of the treatment. Let x be an individual feature vector. Then the CATE of x, denoted ⌧(x), is 
defined
by

⌧(x) = E[Y (1) — Y (0)|X = x].

Estimating   ⌧   is   impossible   without   making   further   assumptions   on   the   
distribution   of

(Yi(0), Yi(1), Xi, Wi). In particular, we need to place two assumptions on our data.

Assumption 1 (Strong Ignorability, Rosenbaum and Rubin (1983))

(Yi(1), Yi(0)) ? W |X.

Assumption 2 (Overlap)  Define the propensity score of x as,

e(x) := P(W  = 1|X = x).

Then there exists constant 0 < emin,  emax < 1 such that for all x 2 Support(X),

0 < emin < e(x) < emax < 1.

In words, e(x) is bounded away from 0 and 1.

¹The software will be released once anonymity is no longer needed. We can also provide an anynomized
copy to reviewers upon request.

2


Under review as a conference paper at ICLR 2019

Assumption 1 ensures that there is no unobserved confounder, a random variable which influences both
the probability of treatment and the potential outcomes, which would make the CATE unidentifiable.
The assumption is particularly strong and difficult to check in applications. Meanwhile, Assumption
2 rectifies the situation wherein a certain part of the population is always treated or always in 
the
control group. If, for example, all women were in the control group, one cannot identify the 
treatment
effect for women. Though both assumptions are strong, they are nevertheless satisfied by design in
randomized controlled trials.  While the estimators we discuss would be sensible in observational
studies when the assumptions are satisfied, we warn practitioners to be cautious in such studies,
especially when the number of covariates is large (D’Amour et al., 2017).

Given these two assumptions, there exist many valid CATE estimators. The crux of these methods is
to estimate two quantities: the control response function,

µ₀(x) = E[Y (0)|X = x],

and the treatment response function,

µ₁(x) = E[Y (1)|X = x].

If we denote our learned estimates as µˆ₀(x) and µˆ₁(x), then we can form the CATE estimate as the
difference between the two

⌧ˆ(x) = µˆ₁(x) — µˆ₀(x).

The astute reader may be wondering why we don’t simply estimate µ₀  and µ₁  with our favorite
function approximation algorithm at this point and then all go home. After all, we have access to 
the
ground truths µ₀ and µ₁ and the corresponding inputs x. In fact, it is commonplace to do exactly 
that.
When people directly estimate µ₀ and µ₁ with their favorite model, we call the procedure a T-learner
(Künzel et al., 2017). Common choices of models include linear models and random forests, though
neural networks have recently been considered (Nie and Wager, 2017).

A practitioner of deep learning might find the T-learner quite trivial. After all, it amounts to 
using
neural networks to fit two quantities, µ₀ and µ₁. However, it is important to note that the 
T-learner is
a baseline method. We use it in this paper only to ease exposition, especially as it relates to 
transfer
learning.  The T-learner has many drawbacks (Athey and Imbens, 2015).  It is almost always an
inefficient estimator. For example, it will often perform poorly when one can borrow information
across the treatment conditions. For these reasons, more sophisticated learners such as the S, X, 
T, R,
and Y learners are almost always used instead of the T-learner (Hill, 2011; Athey and Imbens, 2016;
Nie and Wager, 2017; Künzel et al., 2017; Stadie et al., 2018).  Although much of our exposition
will focus on transfer learning in the context of the T-learner, in practice we extend the discussed
methods to these other more advanced learners, as shown in the Evaluation section. Descriptions of
these more advanced estimators are given in the appendix.

3    TRANSFER  LEARNING

In this section, we consider a scenario wherein one has access to many related causal inference
experiments.  The goal is to use the results from some old experiments to obtain faster training
with     less data on other new experiments.  Since direct transfer between different populations is
wrought with difficulty, we will instead achieve transfer by using previous experiments to help find
an initialization for new experiments which leads to faster optimization.²

We consider two kinds of algorithms. First, there are transfer algorithms that sit on top of 
existing
CATE estimators. These transfer algorithms take a CATE estimation strategy, such as the S-learner,
and provide a recipe for transforming it into a transfer learning CATE estimator. The second class 
of
algorithms does not sit on top of existing CATE estimation strategies. Instead, they are built from 
the
ground up to take advantage of transfer learning. These algorithms are joint training and MLRW.

Across all experiments, the input space X is the same. Let i index an experiment. Each experiment
has its own distinct outcome when treatment is received, µ₁,i(x), and when no treatment is received,
µ₀,i(x). Together, these quantities define the CATE ⌧ ,i(x) = µ₁,i(x)      µ₀,i(x), In standard CATE
estimation, we define a strategy that takes x as input and outputs predictions µˆ₀,i(x) and 
µˆ₁,i(x).

²By faster optimization, we mean that starting from the learned initialization will allow the 
problem to be
solved in fewer optimization epochs and with less data than starting with random weights.

3


Under review as a conference paper at ICLR 2019


random initialization

random initialization

warm   start

random initialization


0                 π  0

0                     0

0                 π  0

1                     1

π  0                  ˆ 0

0                     0

L0

π  0                  ˆ 0

1                     1

0                 π

0                     0

0                 πγ

1

π  0                  ˆ 0

0                     0

L0

π  0                  ˆ 0

1                     1

0                 π

0                     0

0                 πγ

1

π  0                  ˆ 0

0                     0

L0

π  0                  ˆ 0

1                     1


warm start

1                 π  1                    π  1

0                     0                          0

1                 π  1                    π  1

1                     1                          1

backprop

ˆ 1

0

L1

ˆ 1

1

backprop

1                 π

0                     0

1                 πγ

1

backprop

π  1                  ˆ 1

0                     0

L1

π  1                  ˆ 1

1                     1

backprop

warm  start

0                 πγ0                    π  1

1                 π                 π  1

1                                            1

backprop

ˆ 1

0

L1

ˆ 1

1

backprop

freeze during backprop

Warm start method                                               Frozen­features method              
                                 Multi­head method

Figure 1: Warm start, frozen-features, and multi-head methods for CATE transfer learning. For these
figures, we use the T-learner as the base learner for simplicity. All three methods attempt to reuse
neural network features from previous experiments.

In transfer learning, the hope is that we can transfer knowledge between experiments. The model
parameters that allowed us predict µ₀,i(x), µ₁,i(x) and ⌧·,i(x) from experiment i should help us
predict µ₁,j(x), µ₀,j(x), and ⌧·,j(x) from experiment j.

Let ⇡✓ be a generic expression for a neural network parameterized by ✓. Parameters will have two
subscripts.  The index on the left indicates if their neural network predicts treatment or control 
(0
for control and 1 for treatment).  The index on the right is for the experiment.  For example, ✓₀,₂
parametrizes ⇡✓0,2 (x) to predict µ₀,₂(x), the outcome under control for Experiment 2.  All of the
transfer algorithms described here are presented in full detail as pseudo-code in the appendix.

3.0.1    ALGORITHMS THAT EXTEND EXISTING CATE ESTIMATORS TO TRANSFER LEARNING

These algorithms extend existing CATE estimation techniques to the transfer learning setting. The
following exposition is largely motivated by transfer learning with the T-Learner as a base CATE
estimator. This is only for ease of exposition. The discussed procedures can extend to other, more
complicated, CATE estimators such as the R, X, Y, and S learners.

Warm start: Experiment 0 predicts ⇡✓0,0 (x) = µˆ₀,₀(x) and ⇡✓1,0 (x) = µˆ₁,₀(x) to form the CATE
estimator ⌧ˆ,₀ = µˆ₁,₀(x)      µˆ₀,₀(x). Suppose ✓₀,₀, ✓₁,₀ are fully trained and produce a good 
CATE
estimate. For experiment 1, the input space X is identical to the input space for experiment 0, but 
the
outcomes µ₀,₁(x) and µ₁,₁(x) are different. However, we suspect the underlying data representations
learned by ⇡✓0,0  and ⇡✓1,0  are still useful.  Hence, rather than randomly initializing ✓₀,₁  and 
✓₁,₁


for experiment 1,  we set ✓₀,₁   =  ✓₀,₀  and ✓₁,₁   =  ✓₁,₀.   We then train ⇡✓0,1 (x)  =

⇡✓1,1 (x) = µˆ₁,₁(x). See Figure 1 and Algorithm 8 in the appendix.

µˆ₀,₁(x)  and

Frozen-features: Begin by training ⇡✓0,0  and ⇡✓1,0  to produce good CATE estimates for experiment

0.  Assuming ✓₀,₀  and ✓₁,₀  have more than k layers,  let μ₀  be the parameters corresponding to
the first k layers of ✓₀,₀.  Define μ₁  analogously.  Since we think the features encoded by ⇡çi (X)
would make a more informative input than the raw features X, we want to use those features as a
transformed input space for ⇡✓0,1  and ⇡✓1,1 . To wit, set z₀ = ⇡ç0 (x) and z₁ = ⇡ç1 (x). Then form 
the
estimates ⇡✓0,1 (z₀) = µˆ₀,₁(x) and ⇡✓1,1 (z₁) = µˆ₁,₁(x). During training of experiment 1, we only
backpropagate through ✓₀,₁ and ✓₁,₁ and not through μ₀ and μ₁. See Figure 1 and Algorithm 9 in the

appendix.

Multi-head: In this setup, all experiments share base layers that are followed by 
experiment-specific
layers.   The intuition is that the base layers should learn general features,  and the experiment-
specific layers should transform those features into estimates of µj,i(x).  More concretely, let μ₀
and μ₁  be shared base layers for estimating µ₀,·(x)  and µ₁,·(x)  respectively.  Set z₀  =  ⇡ç0 
(x₀)

and z₁  =  ⇡ç1 (x₁).  The base layers are followed by experiment-specific layers $₀,i and $₁,i.  
Let

✓j,i   =  [μj, $j,i].   Then  ⇡✓j,i (x)  =  ⇡$j,i  .⇡çj (x)Σ  =  ⇡$j,i (zj)  =  µˆj,i(x).   
Training  alternates

between experiments: each ✓₀,i and ✓₁,i is trained for some small number of iterations, and then the
experiment and head being trained are switched. Every head is usually trained several times. See
Figure 1 Algorithm 10 in the appendix.

4


Under review as a conference paper at ICLR 2019

SF Reptile transfer for CATE estimators:  Pick your favorite CATE estimator.  The goal is to
learn an initialization for that CATE estimator’s weights that leads to fast convergence on new
experiments. More concretely, starting from good initializers ✓₀ and ✓₁, one can train neural 
networks

⇡✓0  and ⇡✓1  to estimate µ₀,i(x) and µ₁,i(x) much faster and with less data than starting from 
random

initializations. To learn these good initializations, we use a transfer learning technique called 
Reptile.
The idea is to perform experiment-specific inner updates U(✓) and then aggregate them into outer
updates of the form ✓nₑw  =  ✏   U(✓) + (1      ✏)   ✓.  In this paper, we consider a slight 
variation of
Reptile. In standard Reptile, ✏ is either a scalar or correlated to per-parameter weights furnished 
via
SGD. For our problem, we would like to encourage our network layers to learn at different rates.
The hope is that the lower layers can learn more general, slowly-changing features like in the 
frozen
features method, and the higher layers can learn comparatively faster features that more quickly 
adapt
to new tasks after ingesting the stable lower-level features. To accomplish this, we take the path 
of
least resistance and make ✏ a vector which assigns a different learning rate to each neural network
layer. Because our intuition involves slow and fast weights, we will refer to this modification in 
this
paper as SF Reptile: Slow Fast Reptile. Though this change is seemingly small, we found it boosted
performance on our problems. See Algorithm 11.

3.0.2    TRANSFER LEARNING ALGORITHMS THAT DO NOT EXTEND CATE ESTIMATION
STRATEGIES

Joint  training:   All  predictions  share  base  layers  ✓.    From  these  base  layers,  there  
are  two
heads  per  experiment  i:   one  to  predict  µ₀,i(x)  and  one  to  predict  µ₁,i(x).    Every  
head  and
the  base  features  are  trained  simultaneously  by  optimizing  with  respect  to  the  loss  
function

=       i     (µˆ₀,i(x)      µ₀,i(x))    +    (µˆ₁,i(x)      µ₁,i(x))    and minimizing over all 
weights.  This
will encourage the base layers to learn generally applicable features and the heads to learn 
features
specific to predicting a single µj,i(x). See Algorithm 6.

MLRW transfer: In this method, there exists one single set of weights ✓. There are no experiment-
specific weights.  Furthermore, we do not use separate networks to estimate µ₀  and µ₁.  Instead,

⇡✓ is trained to estimate one µi,j(x) at a time.  We train ✓ with SF Reptile so that in the future 
⇡✓
requires minimal samples to fit µi,j(x) from any experiment. To actually form the CATE estimate,
we use a small number of training samples to fit ⇡✓ to µ₀,i(x) and then a small number of training
samples to fit ⇡✓ to µ₁,i(x). We call ✓ meta-learned regression weights (MLRW) because they are
meta-learned over many experiments to quickly regress onto any µi,j(x). The full MLRW algorithm
is presented as Algorithm 5.

4    EVALUATION

We evaluate our transfer learning estimators on both real and simulated data. In our data example,
we consider the important problem of voter encouragement.  Analyzing a large data set of 1.96
million potential voters, we show how transfer learning across elections and geographic regions
can dramatically improve our CATE estimators.  To the best of our knowledge, this is the first
successful demonstration of transfer learning for CATE estimation. The simulated data has been
intentionally chosen to be different in character from our real-world example.  In particular, the
simulated input space is images and the estimated outcome variable is continuous.

4.1    GOTV EXPERIMENT

To evaluate transfer learning for CATE estimation on real data, we reanalyze a set of large field
experiments with more than 1.96 million potential voters (Gerber et al., 2017). The authors 
conducted
17 experiments to evaluate the effect of a mailer on voter turnout in the 2014 U.S. Midterm 
Elections.
The mailer informs the targeted individual whether or not they voted in the past four major 
elections
(2006, 2008, 2010, and 2012), and it compares their voting behavior with that of the people in the
same state. The mailer finishes with a reminder that their voting behavior will be monitored. The 
idea
is that social pressure—i.e., the social norm of voting—will encourage people to vote. The 
likelihood
of voting increases by about 2.2% (s.e.=0.001) when given the mailer.

Each of the experiments targets a different state. This results in different populations, different 
ballots,
and different electoral environments. In addition to this, the treatment is slightly different in 
each

5


Under review as a conference paper at ICLR 2019

experiment, as the median voting behavior in each state is different. However, there are still many
similarities across the experiments, so there should be gains from transferring information.

In this example, the input X is a voter’s demographic data including age, past voting turnout in
2006, 2008, 2009, 2010, 2011, 2012, and 2013, marital status, race, and gender.  The treatment
response function µˆ₁(x) estimates the voting propensity for a potential voter who receives a mailer
encouraging them to vote. The control response function µˆ₀ estimates the voting propensity if that
voter did not receive a mailer. The CATE ⌧ is thus the change in the probability of voting when a 
unit
receives a mailer. The complete dataset has this data over 17 different states. Treating each state 
as a
separate experiment, we can perform transfer learning across them.

x                          outcome                 µ₀                               µ₁              
                 ⌧


A voter profile

The voter’s
propensity to
vote

The voter’s
propensity to
vote when
they do not
receive

a mailer

The voter’s
propensity to
vote when
they do
receive

a mailer

Change in the
voter’s
propensity to
vote after
receiving a
mailer

Being able to estimate the treatment effect of sending a mailer is an important problem in 
elections.
We may wish to only treat people whose likelihood of voting would significantly increase when
receiving the mailer, to justify the cost for these mailers.  Furthermore, we wish to avoid sending
mailers to voters who will respond negatively to them. This negative response has been previously
observed and is therefore feasible and a relevant problem—e.g., some recipients call their Secretary
of State’s office or local election registrar to complain (Mann, 2010; Michelson, 2016).

Evaluating a CATE estimator on real data is difficult. The primary difficulty is that we do not get
to observe the true CATE for any unit, due to the fundamental problem of causal inference.  By
definition, only one of the two outcomes is observed for any unit. One could use the original 
features
and simulate the outcome features, but this would require us to create a response model. Instead, we
estimate the "truth" on the real data using linear models (version 1) or random forests (version 
2). We
then construct the data based on these estimates. For a detailed description, see Appendix A.2. We
then ask the question: How do the various methods perform when they have less data than the entire
sample?

RESULTS

We evaluate all the algorithms discussed in section 3 on the GOTV dataset. For the algorithms in
section 3.0.1 that require a base CATE estimator, we use the Y learner because we found it delivered
the best performance.³ For baselines, we compare against the non-transfer Y-learner and the S 
learner
with random forests.⁴ In previous work, state of the art results on this problem have been achieved
with both non-transfer tree-based estimators such as S-RF (Künzel et al., 2017; Green and Kern,
2012) and neural-network-based learners such as the R and Y-learners (Nie and Wager, 2017; Stadie
et al., 2018).

The best estimator is MLRW. This algorithm consistently converges to a very good solution with
very few observations. Looking at Tables 1, 2, and 3, we observe that MLRW is the best performing
transfer learner for GOTV version 1 in 8 out of 17 trials.  In GOTV version 2, it is the best in 11
out  of 17 trials.  In Figure 2, its average performance is dominant over all other algorithms.  We
hypothesize that this method does best because it does not try to artificially bottleneck the flow 
of
information between outcomes and experiments. MLRW also seems more resilient to data-poisoning
when      it encounters outlier data, though we did not concretely test against this. We also 
observe that
multi-head, frozen-features, and SF all generally improve upon non-transfer baselines. The faster
learning rate of these algorithms indicates that positive transfer between experiments is occurring.
Warm start, however, does not work well and often even leads to worse results than the baseline
estimators. This is consistent with prior findings on warm start (Finn et al., 2017).

See Tables 1, 2, and 3 and Figures 5, 6, and 7 for full results with the X, Y, S, and T-learners as 
base CATE

estimators.

⁴We use the S learner because the Y learner is not compatible with random forests.

6


Under review as a conference paper at ICLR 2019

Figure 2: Social Pressure and Voter Turnout, Version 1 (real data as a linear model) and Version 2
(real data as random forest). Our transfer learning algorithms far exceed the previous state of the 
art
non-transfer baselines, which are represented here as S-RF and Baseline (Y-NN).

4.2    MNIST EXAMPLE

In the previous experiment, we observed that the MLRW estimator performed most favorably, and
transfer learning significantly improved upon the baseline.  To confirm that this conclusion is not
specific to voter persuasion studies, we intentionally consider a very different type of data. 
Recently,
(Nie and Wager, 2017) introduced a simulation study wherein MNIST digits are rotated by some
number of degrees ↵; with ↵ furnished via a single data generating process that depends on the
value of the depicted digit. They then attempt to do CATE estimation to measure the heterogeneous
treatment effect of a digit’s label.

Motivated by this example, we develop a data generating process using MNIST digits wherein
transfer learning for CATE estimation is applicable.  In our example, the input X is an MNIST
image. We have k data-generating processes which return different outcomes for each input when
given either treatment or control. Thus, under some fixed data-generating process, µ₀ represents the
outcome when the input image X is given the control, µ₁ represents the outcome when X is given the
treatment, and ⌧ is the difference in outcomes given the placement of X in the treatment or control
group. Each data-generating process has different response functions (µ₀ and µ₁) and thus different
CATEs (⌧ ), but each of these functions only depends on the label presented in the image X. We thus
hope that transfer learning could expedite the process of learning features which are indicative of 
the
label. See Appendix A for full details of the data generation process. In Figure 3, we confirm that 
a
transfer learning strategy outperforms its non-transfer learning counterpart, even on image data. We
also see that MLRW performs well, though in this case multi-head is competitive. We also see that
several of the transfer methods are worse than non-transfer baselines.

Figure 3: MNIST task. The baseline is the S-learner. All transfer CATE estimators for this task are
built on top the S-learner, rather than the Y-learner, because we found it delivered better 
performance
for this problem.

7


Under review as a conference paper at ICLR 2019

5    RELATED  WORKS  AND  DISCUSSION

5.0.1    RELATED WORKS

In this paper, we proposed the problem of transfer learning for CATE estimation. One immediate
question the reader may be left with is why we chose the transfer learning techniques we did.
We  only  considered  two  common  types  of  transfer:  (1)  Basic  fine  tuning  and  weights  
sharing
techniques common in the computer vision literature (Welinder et al., 2010; Saenko and Darrell,
2010; Bourdev et al., 2011; Donahue et al., 2014; Koch, 2015), (2) Techniques for learning an
initialization that can be quickly optimized (Finn et al., 2017; Ravi and Larochelle, 2017; Nichol
et            al., 2018). However, many further techniques exist. Yet, transfer learning is an 
extensively studied
and perennial problem (Schmidhuber, 1992; Bengio et al., 1992; Thrun, 1996; Thrun and Pratt, 1998;
Taylor and Stone, 2009; Silver et al., 2013). In (Vinyals et al., 2016), the authors attempt to 
combine
feature embeddings that can be utilized with non-parametric methods for transfer. (Snell et al., 
2017)
is   an extension of this work that modifies the procedure for sampling examples from the support 
set
during training. (Andrychowicz et al., 2016) and related techniques try to meta-learn an optimizer
that can more quickly solve new tasks. (Rusu et al., 2016) attempts to overcome forgetting during
transfer by systematically introducing new network layers with lateral connections to old frozen
layers.  (Munkhdalai and Yu, 2017) uses networks with memory to adapt to new tasks.  We invite
the reader to review (Finn et al., 2017) for an excellent overview of the current transfer learning
landscape. Though the majority of the discussed techniques could be extended to CATE estimation,
our implementations of (Rusu et al., 2016; Andrychowicz et al., 2016) proved difficult to tune and
consequently learned very little. Furthermore, we were not able to successfully adapt (Snell et al.,
2017)      to the problem of regression.  We decided to instead focus our attention on algorithms 
for
obtaining good initializations, which were easy to adapt to our problem and quickly delivered good
results without extensive tuning.

On the topic of using neural networks to improve causal inference algorithms, a flurry of relevant
work exists (Ramachandra, 2018; Magliacane et al., 2017; Johansson et al., 2016; Louizos et al.,
2017; Alaa et al., 2017; Shalit et al., 2017; Nie and Wager, 2017). We found that these papers 
either
did not allow us to better estimate the CATE, or else provided worse performance than the baseline
methods we did consider in this paper. Extending transfer to other causal inference algorithms is an
ongoing and interesting area of research.

5.0.2    CLOSING REMARKS

We are left with several open questions. Can transfer learning still be applied to CATE estimation
when the experiment input spaces differ? How should one properly deal with missing and incomplete
data? Do there exist better methods for interpretability, highlighting which features are most 
important
for transfer and why? Can these techniques be extended to causal models outside of CATE estimation?
How can one properly encode causal relationships into a neural network? Answering these questions
would have a positive impact on fields such as causal inference, deep learning, and reinforcement
learning.

8


Under review as a conference paper at ICLR 2019

REFERENCES

Alaa, A., Weisz, M., and M., V. D. S. (2017). Deep counterfactual networks with propensity-dropout.

1706.05966.

Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W., Pfau, D., Schaul, T., and de Freitas, N.
(2016). Learning to learn by gradient descent by gradient descent. Neural Information Processing
Systems (NIPS).

Athey, S. and Imbens, G. W. (2015). Machine learning methods for estimating heterogeneous causal
effects. stat, 1050(5).

Athey,  S.  and  Imbens,  G.  W.  (2016).   Recursive  partitioning  for  heterogeneous  causal  
effects.
Proceedings of the National Academy of Sciences of the United States of America, 113(27):7353–
60.

Bengio, S., Bengio, Y., Cloutier, J., and Gecsei, J. (1992). On the optimization of a synaptic 
learning
rule. Biological Neural Networks.

Bourdev, L., Maji, S., and Malik, J. (2011). Describing people: A poselet-based approach to 
attribute
classification. ICCV.

D’Amour, A., Ding, P., Feller, A., Lei, L., and Sekhon, J. (2017). Overlap in observational studies
with high-dimensional covariates. arXiv preprint arXiv:1711.02582.

Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2014). 
Decaf: A
deep convolutional activation feature for generic visual recognition. International Conference on
Machine Learning (ICML).

Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic metalearning for fast adaptation of deep
networks. ICML.

Gerber, A. S., Huber, G. A., Fang, A. H., and Gooch, A. (2017).  The generalizability of social
pressure effects on turnout across high-salience electoral contexts: Field experimental evidence
from 1.96 million citizens in 17 states. American Politics Research, 45(4):533–559.

Green, D. P. and Kern, H. L. (2012). Modeling heterogeneous treatment effects in survey experiments
with bayesian additive regression trees. Public opinion quarterly, 76(3):491–511.

Henderson, N. C., Louis, T. A., Wang, C., and Varadhan, R. (2016). Bayesian analysis of heteroge-
neous treatment effects for patient-centered outcomes research.  Health Services and Outcomes
Research Methodology, 16(4):213–233.

Hill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 20(1):217–240.

Johansson, F. D., Shalit, U., and Sontag, D. (2016).  Learning representations for counterfactual
inference. ICML.

Koch, G. (2015). Siamese neural networks for one-shot image recognition. ICML Deep Learning
Workshop.

Künzel, S., Sekhon, J., Bickel, P., and Yu, B. (2017).  Meta-learners for estimating heterogeneous
treatment effects using machine learning. arXiv preprint arXiv:1706.03461.

LeCun, Y. (1998). The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/.

Louizos, C., Shalit, U., Mooij, J., Sontag, D., Zemel, R., and Welling, M. (2017).  Causal effect
inference with deep latent-variable models. NIPS.

Magliacane, S., Van Ommen, T., Claassen, T., Bongers, S., Versteeg, P., and Mooij, J. (2017). Domain
adaptation by using causal inference to predict invariant conditional distributions. 1707.06422.

Mann, C. B. (2010).  Is there backlash to social pressure?  a large-scale field experiment on voter
mobilization. Political Behavior, 32(3):387–407.

9


Under review as a conference paper at ICLR 2019

Michelson, M. R. (2016). The risk of over-reliance on the institutional review board: An approved
project is not always an ethical project. PS: Political Science & Politics, 49(02):299–303.

Munkhdalai, T. and Yu, H. (2017). Meta networks. ICML.

Nichol, A., Achiam, J., and Schulman, J. (2018).  On first-order meta-learning algorithms.  CoRR,
abs/1803.02999.

Nie, X. and Wager, S. (2017).  Learning objectives for treatment effect estimation.  arXiv preprint
arXiv:1712.04912.

Powers, S., Qian, J., Jung, K., Schuler, A., Shah, N. H., Hastie, T., and Tibshirani, R. (2018). 
Some
methods for heterogeneous treatment effect estimation in high dimensions. Statistics in medicine.

Ramachandra, V. (2018). Deep learning for causal inference. 1803.00149.

Ravi, S. and Larochelle, H. (2017).  Optimization as a model for few-shot learning.  International
Conference on Learning Representations (ICLR).

Rosenbaum, P. R. and Rubin, D. B. (1983). The central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1):41–55.

Rubin, D. B. (1974).  Estimating causal effects of treatments in randomized and nonrandomized
studies. Journal of educational Psychology, 66(5):688.

Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., 
Pascanu,
R., and Hadsell, R. (2016). Progressive neural networks. CoRR, vol. abs/1606.04671.

Saenko, K., K. B. F. M. and Darrell, T. (2010).  Adapting visual category models to new domains.

ECCV.

Schmidhuber,  J. (1992).   Learning to control fast-weight memories:  An alternative to dynamic
recurrent networks. Neural Computation.

Shalit, U., Johansson, F., and Sontag, D. (2017). Estimating individual treatment effect: 
generalization
bounds and algorithms. ICML.

Silver, Yand, and Li (2013). Lifelong machine learning systems: Beyond learning algorithms. DAAAI
Spring Symposium-Technical Report, 2013.

Snell, J.,  , Swersky, K., and Zemel, R. (2017). Prototypical networks for few-shot learning. arXiv
preprint arXiv:1703.05175.

Stadie, B. C., Künzel, S. R., Vemuri, N., Ramakrishnan, V., Sekhon, J. S., and Abbeel, P. (2018).
Estimating heterogenous treatment effects with the y-learner. arXiv.

Taddy,  M.,  Gardner,  M.,  Chen,  L.,  and Draper,  D. (2016).   A nonparametric bayesian analysis
of heterogenous treatment effects in digital experimentation.  Journal of Business & Economic
Statistics, 34(4):661–672.

Taylor and Stone (2009). Transfer learning for reinforcement learning domains: A survey. DAAAI
Spring Symposium-Technical Report, 2013.

Thrun (1996). Is learning the n-th thing any easier than learning the first?  NIPS.

Thrun, S. and Pratt, L. (1998). Learning to learn. Springer Science and Business Media.

Tian, L., Alizadeh, A. A., Gentles, A. J., and Tibshirani, R. (2014). A simple method for estimating
interactions between a treatment and a large number of covariates.   Journal of the American
Statistical Association, 109(508):1517–1532.

Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., and et al (2016).  Matching networks for one
shot learning. Neural Information Processing Systems (NIPS).

Wager, S. and Athey, S. (2017). Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association.

Welinder,  P.,  Branson,  S.,  Mita,  T.,  Wah,  C.,  Schroff,  F.,  Belongie,  S.,  and  Perona,  
P.  (2010).
Caltech-ucsd birds 200. technical report cns-tr-2010-001. California Institute of Technology.

10


Under review as a conference paper at ICLR 2019

A    APPENDIX: SIMULATION  STUDIES  AND  APPLICATION

A.1    MNIST SIMULATION

For our MNIST simulation study (Section 4.2), we used the MNIST database (LeCun, 1998) which
contains labeled handwritten images. We follow here the notation of Nie and Wager (2017), who
introduce a very similar simulation study which is not trying to evaluate transfer learning for CATE
estimation, but instead emulates a RCT with the goal to evaluate different CATE estimators.

The MNIST data set contains labeled image data (Xi, Ci), where Xi denotes the raw image of i and
Ci       0,. .., 9   denotes its label. We create k Data Generating Processes (DGPs), D₁,  . . . ,  
Dk,
each of which specifies a distribution of (Yi(0), Yi(1), Wi, Xi) and represents different CATE esti-
mation problems.

In this simulation, we let Wi = 0 if the image Xi is placed in the control, and Wi = 1 if the image

Xi is placed in the treatment. Yi(Wi) quantifies the the outcome of Xi under Wi.

To generate a DGP Dj , we first sample weights in the following way,

mʲ(0),  mʲ(1),. ..,  mʲ(9) i⇠ⁱᵈ Unif(—3,  3),

tʲ(0),  tʲ(1),. ..,  tʲ(9) i⇠ⁱᵈ Unif(—1,  1),

pʲ(0),  pʲ(1),. ..,  pʲ(9) i⇠ⁱᵈ Unif(0.3,  0.7),

and we define the response functions and the propensity score as

µʲ (Ci) = mʲ(Ci) + 3Ci,
µʲ (Ci) = µʲ (Ci) + tʲ(Ci),

1                         0

eʲ(Ci) = pʲ(Ci).

To generate (Yi(0), Yi(1), Wi, Xi) from Dj, we fist sample a (Xi, Ci) from the MNIST data set, and
we then generate Yi(0), Yi(1), and Wi in the following way:

"i  ⇠ N(0, 1)

Yi(0) = µ₀(Ci) + "i

Yi(1) = µ₁(Ci) + "i

Wi ⇠ Bern(e(Ci)).

During training, Xi, Wi, and Yi are made available to the convolutional neural network, which then
predicts ⌧ˆ given a test image Xi and a treatment Wi.  ⌧ is the difference in the outcome given the
difference in treatment and control.

Having access to multiple DGPs can be interpreted as having access to prior experiments done on a
similar population of images, allowing us to explore the effects of different transfer learning 
methods
when predicting the effect of a treatment in a new image.

A.2    GOTV DATA EXAMPLE AND SIMULATION

In this section, we describe how the simulations for the GOTV example in the main paper were
done and we discuss the results of a much bigger simulation study with 51 experiments which is
summarized in Tables 1, 2, and 3.

A.2.1    DATA GENERATING PROCESSES FOR OUR REAL WORLD EXAMPLE

For our data example, we took one of the experiments conducted by Gerber et al. (2017). The study
took place in 2014 in Alaska and 252,576 potential voters were randomly assigned in a control and a
treatment group. Subjects in the treatment group were sent a mailer as described in the main text 
and
their voting turnout was recorded.

To evaluate the performance of different CATE estimators we need to know the true CATEs, which
are unknown due to the fundamental problem of causal inference. To still be able to evaluate CATE

11


Under review as a conference paper at ICLR 2019

estimators researchers usually estimate the potential outcomes using some machine learning method
and then generate the data from this estimate. This is to some extend also a simulation, but unlike
classical simulation studies it is not up to the researcher to determine the data generating 
distribution.
The only choice of the researcher lies in the type of estimator she uses to estimate the response
functions. To avoid being mislead by artifacts created by a particular method, we used a linear 
model
in    Real World Data Set 1 and random forests estimator in Real World Data Set 2.

Specifically, we generate for each experiment a true CATE and we simulate new observed outcomes
based on the real data in four steps.

1.  We first use the estimator of choice (e.g., a random forests estimator) and train it on the
treated units and on the control units separately to get estimates for the response functions,
µ₀ and µ₁.

2.  Next, we sample N units from the underlying experiment to get the features and the treatment

assignment of our samples (Xi, Wi)N   .

3.  We then generate the true underlying CATE for each unit using ⌧i = ⌧(Xi) = µ₁(Xi)

µ₀(Xi).

4.  Finally we generate the observed outcome by sampling a Bernoulli distributed variable
around mean µi.


Y ᵒᵇˢ ⇠ Bern(µ ),               µ

= ⇢µ₀(Xi)  if  W  = 0,

After this procedure, we have 17 data sets corresponding to the 17 experiments for which we know
the true CATE function, which we can now use to evaluate CATE estimators and CATE transfer
learners.

A.2.2    DATA GENERATING PROCESSES FOR OUR SIMULATION STUDY

Simulations motivated by real-world experiments are important to assess whether our methods work
well for voter persuasion data sets, but it is important to also consider other settings to 
evaluate the
generalizability of our conclusions.

To do this, we first specify the control response function, µ₀(x) = E[Y (0)|X = x] 2 [0, 1], and the
treatment response function, µ₁(x) = E[Y (1)|X = x] 2 [0, 1].

We then use each of the 17 experiments to generate a simulated experiment in the following way:

1.  We sample N units from the underlying experiment to get the features and the treatment

assignment of our samples (Xi, Wi)N   .

2.  We then generate the true underlying CATE for each unit using ⌧i = ⌧(Xi) = µ₁(Xi)

µ₀(Xi).

3.  Finally we generate the observed outcome by sampling a Bernoulli distributed variable
around mean µi.


Y ᵒᵇˢ ⇠ Bern(µ ),               µ

= ⇢µ₀(Xi)  if  W  = 0,

The experiments range in size from 5,000 units to 400,000 units per experiment and the covariate
vector is 11 dimensional and the same as in the main part of the paper. We will present here three
different setup.

Simulation LM (Table 1): We choose here N to be all units in the corresponding experiment. Sample

Ø⁰ = (Ø⁰,. .., Ø⁰) i⇠ⁱᵈ N(0, 1) and Ø¹ = (Ø¹,. .., Ø¹) i⇠ⁱᵈ N(0, 1) and define,

µ₀(x) = logistic  xØ⁰   ,
µ₁(x) = logistic .xØ¹Σ .

Simulation RF (Table 2): We choose here N to be all units in the corresponding experiment.

12


Under review as a conference paper at ICLR 2019

1.  Train a random forests estimator on the real data set and define µ₀  to be the resulting
estimator,

2.  Sample a covariate f (e.g., age),

3.  ample a random value in the support of f (e.g., 38),

4.  Sample a shift s ⇠ N(0, 4).

Now define the potential outcomes as follows:

µ₀(x) = trained Random Forests algorithm

µ₁(x) = logistic (logit (µ₀(x) + s ⇤ 1f≤v))

Simulation RFt (Table 3): This experiment is the same as Simulation RF, but use only one percent
of the data, N =  #ᵘⁿⁱᵗˢ .

13


Under review as a conference paper at ICLR 2019

B    PSEUDO  CODE  FOR  CATE ESTIMATORS

In this section, we will present pseudo code for the CATE estimators in this paper. We present code
for the meta learning algorithms in Section C. We denote by Y ⁰ and Y ¹ the observed outcomes for
the control and the treated group.  For example, Y ¹ is the observed outcome of the ith unit in the
treated group. X⁰ and X¹ are the features of the control and treated units, and hence, X¹ 
corresponds
to the feature vector of the ith unit in the treated group. Mk(Y      X) is the notation for a 
regression
estimator, which estimates x      E[Y  X = x]. It can be any regression/machine learning estimator,
but      in this paper we only choose it to be a neural network or random forest.

These algorithms first appeared in (Künzel et al., 2017; Stadie et al., 2018). We reproduce them 
here
for completeness.

Algorithm 1 T-learner

1:  procedure T–LEARNER(X, Y ᵒᵇˢ,W )

2:         µˆ₀ = M₀(Y ⁰ ⇠ X⁰)

3:         µˆ₁ = M₁(Y ¹ ⇠ X¹)

4:         ⌧ˆ(x) = µˆ₁(x) — µˆ₀(x)

5:  end procedure

M0 and M1 are here some, possibly different machine learning/regression algorithms.

Algorithm 2 S-learner

1:  procedure S–LEARNER(X, Y ᵒᵇˢ,W )

2:         µˆ = M(Y ᵒᵇˢ ⇠ (X, W ))

3:         ⌧ˆ(x) = µˆ(x, 1) — µˆ(x, 0)

4:  end procedure

M(Y ᵒᵇˢ     (X, W )) is the notation for estimating (x, w)      E[Y  X  = x, W  = w] while treating 
W  as a
0,1–valued feature.

Algorithm 3 X–learner

1:  procedure X–LEARNER(X, Y ᵒᵇˢ, W, g)

2:         µˆ₀ = M₁(Y ⁰ ⇠ X⁰)                                                                       
   . Estimate response function

3:         µˆ₁ = M₂(Y ¹ ⇠ X¹)

4:         D˜1  = Y ¹ — µˆ₀(X¹)                                                             . 
Compute imputed treatment effects

5:         D˜0  = µˆ₁(X⁰) — Y ⁰

6:         ⌧ˆ₁ = M₃(D˜1  ⇠ X¹)                                                      . Estimate CATE 
for treated and control

7:         ⌧ˆ₀ = M₄(D˜0  ⇠ X⁰)

8:         ⌧ˆ(x) = g(x)⌧ˆ₀(x) + (1 — g(x))⌧ˆ₁(x)                                                    
  . Average the estimates

9:  end procedure

g(x)    [0, 1] is a weighing function which is chosen to minimize the variance of ⌧ˆ(x). It is 
sometimes possible
to estimate Cov(⌧0(x), ⌧1(x)), and compute the best g based on this estimate. However, we have made 
good
experiences by choosing g to be an estimate of the propensity score, but also choosing it to be 
constant and equal
to         the ratio of treated units usually leads to a good estimator of the CATE.

14


Under review as a conference paper at ICLR 2019

Algorithm 4 Y-Learner Pseudo Code

1:  if Wi == 0 then

2:         Update the network ⇡✓0  to predict Y

obs

3:         Update the network ⇡✓1  to predict Y      + ⇡⌧ (Xi)

obs

4:         Update the network ⇡⌧ to predict ⇡✓  (Xi) — Y ᵒᵇˢ

1                               i

5:  end if

6:  if Wi == 1 then


7:         Update the network ⇡✓0

8:         Update the network ⇡✓1

to predict Y ᵒᵇˢ — ⇡⌧ (Xi)

i

9:         Update the network ⇡⌧ to predict Y ᵒᵇˢ — ⇡✓  (Xi)

i                   0

10:  end if

This process describes training the Y-Learner for one step given a data point (Y ᵒᵇˢ, Xi, Wi)

15


Under review as a conference paper at ICLR 2019

C    EXPLICIT  TRANSFER  LEARNING  ALGORITHMS  FOR  CATE ESTIMATION

C.1    MLRW TRANSFER FOR CATE ESTIMATION

Algorithm 5 MLRW Transfer for Cate Estimation.

1:  Let µ⁽ⁱ⁾ and µ⁽ⁱ⁾ be the outcome under treatment and control for experiment i.

0                    1

2:  Let numexps be the number of experiments.

3:  Let ⇡✓ be an N layer neural network parameterized by ✓ = [✓₀,. .., ✓N ].
4:  Let ✏ = [✏₀,. .., ✏N ] be a vector, where N is the number of layers in ⇡✓.
5:  Let outeriters be the total number of training iterations.

6:  Let inneriters be the number of inner loop training iterations.

7:  for oiter < outeriters do

8:         for i < numexps do

9:               Sample X₀ and X₁: control and treatment units from experiment i

10:               for j = [0, 1] do                                                    . j 
iterating over treatment and control

11:                      Let U₀(✓) = ✓

12:                      for k < inneriters do

13:                                 =    ⇡Uk (✓)(Xj)      µj(Xj)

14:                             Compute    ✓   .

15:                             Use ADAM with    ✓    to obtain Uk₊₁(✓).

16:                             Uk(✓) = Uk₊₁(✓)

17:                      end for

18:                      for  p < N  do

19:                             ✓p = ✏p   Uk(✓p) + (1      ✏p)   ✓p.

20:                      end for

21:               end for

22:         end for

23:  end for

24:  To Evaluate CATE estimate, do

25:  C = []

26:  for i < numexps do

27:         Sample X₀ and X₁: control and treatment units from experiment i

28:         Sample X: test units from experiment i.

29:         for j = [0, 1] do                                                          . j 
iterating over treatment and control

30:               for k < innteriters do

31:                          =    ⇡Uk (✓)(Xj)      µj(Xj)

32:                      Compute    ✓   .

33:                      Use ADAM with    ✓    to obtain Uk₊₁(✓).

34:                      Uk(✓) = Uk₊₁(✓)

35:               end for

36:               µˆj = ⇡Uk (✓)(X)

37:         end for

38:         ⌧ˆi = µˆ₀ — µˆ₁

39:         C.append(⌧ˆi)

40:  end for

41:  return C

16


Under review as a conference paper at ICLR 2019

C.2    JOINT TRAINING

Algorithm 6 Joint Training

1:  Let µ⁽ⁱ⁾ and µ⁽ⁱ⁾ be the outcome under control and treatment for experiment i.

0                    1

2:  Let numexps be the number of experiments.

3:  Let ⇡⇢ be a generic expression for a neural network parameterized by ⇢.

4:  Let ✓ be base neural network layers shared by all experiments.

5:  Let $⁽ⁱ⁾ be neural network layers predicting µ⁽ⁱ⁾ in experiment i.

0                                                                                                   
   0

6:  Let $⁽ⁱ⁾ be neural network layers predicting µ⁽ⁱ⁾ in experiment i.


1

7:  Let !⁽ⁱ⁾ =

h✓, $(i)i

1

be the full prediction network for µ₀ in experiment i.


1

9:  Let ⌦ = S1

1

Snumexps !⁽ⁱ⁾ be all trainable parameters.

10:  Let numiters be the total number of training iterations

11:  for iter < numiters do

12:             =0 

13:         for i < numexps do

14:               Sample X₀ and X₁: control and treatment units from experiment i

15:               for j = [0, 1] do                                                    . j 
iterating over treatment and control

16:                      L⁽ⁱ⁾ = k⇡  (i) (Xj) — µj(Xj)k

!

17:                      L = L + L⁽ⁱ⁾


18:               end for

19:         end for

P  P  @L(i)

		

21:         Apply ADAM with gradients given by    ⌦   .

22:         for i < numexps do

23:               Sample X: test units from experiment i

24:         end for

25:  end for

26:  µˆ₀ = ⇡  (i) (X)

0

27:  µˆ₁ = ⇡!(i) (X)

28:  return CATE estimate ⌧ˆ = µˆ₁ — µˆ₀

Figure  4:  Joint  Training  -  Unlike  the
Multi-head method which differentiates
base  layers  for  treatment  and  control,
the Joint Training method has all obser-
vations and experiments (regardless of
treatment and control) share the same
base network, which extracts general low
level features from the data.

17


Under review as a conference paper at ICLR 2019

C.3    T-LEARNER TRANSFER CATE ESTIMATORS

Here, we present full pseudo code for the algorithms from Section 3 using the T-learner as a base
learner. All of these algorithms can be extended to other learners including S, R, X, and Y . See 
the
released code for implementations.

Algorithm 7 Vanilla T-learner (also referred to as Baseline T-learner)

1:  Let µ₀ and µ₁ be the outcome under treatment and control.

2:  Let X be the experimental data. Let Xt be the test data.

3:  Let ⇡✓0  and ⇡✓1  be a neural networks parameterized by ✓₀ and ✓₁.

4:  Let ✓ = ✓₀     ✓₁.

5:  Let numiters be the total number of training iterations.

6:  Let batchsize be the number of units sampled. We use 64.

7:  for i < numiters do

8:         Sample X₀ and X₁: control and treatment units. Sample batchsize units.

9:         L₀ = k⇡✓(X₀) — µ₀(X₀)k

10:         L₁ = k⇡✓(X₁) — µ₁(X₁)k

11:         L = L₀ + L₁        @L

12:         Compute    ✓    =  @✓ .

13:         Apply ADAM with gradients given by    ✓   .

14:  end for

15:  µˆ₀ = ⇡✓0 (Xt)

16:  µˆ₁ = ⇡✓1 (Xt)

17:  return CATE estimate ⌧ˆ = µˆ₁ — µˆ₀

Algorithm 8 Warm Start T-learner

1:  Let µⁱ  and µⁱ  be the outcome under treatment and control for experiment i.

0                 1

2:  Let Xⁱ be the data for experiment i. Let Xⁱ be the test data for experiment i.

3:  Let ⇡✓0  and ⇡✓1  be a neural networks parameterized by ✓₀ and ✓₁.

4:  Let ✓ = ✓₀     ✓₁.

5:  Let numiters be the total number of training iterations.

6:  Let batchsize be the number of units sampled. We use 64.

7:  for i < numiters do

8:         Sample X⁰ and X⁰: control and treatment units for experiment 0. Sample batchsize units.

0                   1

9:         L₀ = k⇡✓  (X⁰) — µ₀(X⁰)k

10:         L₁ = k⇡✓  (X⁰) — µ₁(X⁰)k

1       1                          1

11:         L = L₀ + L₁        @L

12:         Compute    ✓    =  @✓ .

13:         Apply ADAM with gradients given by    ✓   .

14:  end for

15:  for i < numiters do

16:         Sample X¹ and X¹: control and treatment units for experiment 1. Sample batchsize units.

0                   1

17:         L₀ = k⇡✓  (X¹) — µ₀(X¹)k

18:         L₁ = k⇡✓  (X¹) — µ₁(X¹)k

1       1                          1

19:         L = L₀ + L₁        @L

20:         Compute    ✓    =  @✓ .

21:         Apply ADAM with gradients given by    ✓   .

22:  end for

23:  µˆ₀ = ⇡✓  (X¹)

0       t

24:  µˆ₁ = ⇡✓  (X¹)

1       t

25:  return CATE estimate ⌧ˆ = µˆ₁ — µˆ₀

18


Under review as a conference paper at ICLR 2019

Algorithm 9 Frozen Features T-learner

1:  Let µⁱ  and µⁱ  be the outcome under treatment and control for experiment i.

0                 1

2:  Let Xⁱ be the data for experiment i. Let Xⁱ be the test data for experiment i.

3:  Let ⇡⇢ be a generic expression for a neural network parameterized by ⇢.

4:  Let ✓⁰, ✓¹, ✓⁰, ✓¹ be neural network parameters. The subscript indicates the outcome that ✓ is

0      0      1      1

associated with predicting (0 for control and 1 for treatment) and the superscript indexes the
experiment.

5:  Let μ₀ be the first k layers of ⇡✓0 . Define μ₁ analogously.

6:  Let ✓ⁱ = ✓ⁱ [ ✓ⁱ .

7:  Let numiters be the total number of training iterations.

8:  Let batchsize be the number of units sampled. We use 64.

9:  for i < numiters do

10:         Sample X⁰ and X⁰: control and treatment units for experiment 0. Sample batchsize units.

0                   1

0                          0

11:         L₀ = k⇡✓0 (X0 ) — µ₀(X0 )k

0                          0

12:         L₁ = k⇡✓0 (X1 ) — µ₁(X1 )k

13:         L = L₀ + L₁        @L

14:         Compute    ✓    =  @✓ .

15:         Apply ADAM with gradients given by    ✓0    .

16:  end for

17:  for i < numiters do

18:         Sample X¹ and X¹: control and treatment units for experiment 1. Sample batchsize units.

0                   1

19:         Compute Z¹ = ⇡ç(X¹) and Z¹ = ⇡ç(X¹)

0                        0                    1                        1

1                         1

20:         L₀ = k⇡✓1 (Z0 ) — µ₀(Z0 )k

1                         1

21:         L₁ = k⇡✓1 (Z1 ) — µ₁(Z1 )k


22:         L = L₀ + L₁

 @L                                                                                                 
      0


23:         Compute    ✓1      =  @✓1 . Do not compute gradients with respect to ✓

24:         Apply ADAM with gradients given by    ✓1    .

25:  end for

26:  Compute Z¹ = ⇡ç(X¹).

parameters.

t                    t

1

27:  µˆ₀ = ⇡✓1 (Zt )

28:  µˆ₁ = ⇡✓1 (Zt )

29:  return CATE estimate ⌧ˆ = µˆ₁ — µˆ₀

19


Under review as a conference paper at ICLR 2019

Algorithm 10 Multi-Head T-learner

1:  Let µⁱ  and µⁱ  be the outcome under treatment and control for experiment i.

0                 1

2:  Let Xⁱ be the data for experiment i. Let Xⁱ be the test data for experiment i.

3:  Let ⇡⇢ be a generic expression for a neural network parameterized by ⇢.

4:  Let ✓₀ be base neural network layers shared by all experiments for predicting outcomes under
control.

5:  Let ✓₁ be base neural network layers shared by all experiments for predicting outcomes under
treatment.

6:  Let $⁽ⁱ⁾ be neural network layers receiving ⇡✓  (xⁱ ) as input and predicting µ⁽ⁱ⁾(xⁱ ) in 
experiment

0                                                                                                  
0      0                                                                  0          0

i.

7:  Let $⁽ⁱ⁾ be neural network layers receiving ⇡✓  (xⁱ ) as input and predicting µ⁽ⁱ⁾(xⁱ ) in 
experiment

1                                                                                                  
1      1                                                                  1          1

i.

8:  Let !⁽ⁱ⁾ = h✓, $⁽ⁱ⁾i be all trainable parameters used to predict µⁱ .

9:  Let !⁽ⁱ⁾ = h✓, $⁽ⁱ⁾i be all trainable parameters used to predict µⁱ .

10:  Let ⌦ⁱ = !⁽ⁱ⁾ [ !⁽ⁱ⁾.

11:  Let numiters be the total number of training iterations.

12:  Let batchsize be the number of units sampled. We use 64.

13:  Let numexps be the number of experiments.

14:  for i < numiters do

15:         for j < numexps do

16:               Sample Xʲ and Xʲ:  control and treatment units for experiment j.  Sample 
batchsize

0                    1

units.

17:               Compute Zʲ = ⇡✓  (Xʲ) and Zʲ = ⇡✓  (Xʲ)


0               0       0

j                    j

1               1       1

j                    j

18:               Compute µˆ0  = ⇡$j (z0 ) and µˆ1  = ⇡$j (z1 )

0                                         1

19:               L₀ = kµˆʲ — µʲ (Xʲ)k

20:               L₁ = kµˆʲ — µʲ (Xʲ)k


1            1

21:               L = L₀ + L₁

1

 @L 

22:               Compute    ⌦i       =  @⌦i .

23:               Apply ADAM with gradients given by    ✓   .

24:         end for

25:  end for

26:  Let C = []

27:  for j < numexps do

28:         Compute Zʲ = ⇡✓  (Xʲ) and Zʲ = ⇡✓  (Xʲ)


0               0       t

j                    j

1               1       t

j                    j

29:         Compute µˆ0  = ⇡$j (z0 ) and µˆ1  = ⇡$j (z1 )

0                                         1

30:         Estimate CATE ⌧ˆ = µˆʲ — µˆʲ .

1            0

31:         C.append(⌧ˆ)

32:  end for

33:  return C

20


Under review as a conference paper at ICLR 2019

Algorithm 11 SF Reptile T-learner

1:  Let µⁱ  and µⁱ  be the outcome under treatment and control for experiment i.

0                 1

2:  Let Xⁱ be the data for experiment i. Let Xⁱ be the test data for experiment i.

3:  Let ⇡✓0  and ⇡✓1  be a neural networks parameterized by ✓₀ and ✓₁.

4:  Let ✓ = ✓₀     ✓₁.

5:  Let ✏ = [✏₀,. .., ✏N ] be a vector, where N is the number of layers in ⇡✓i .

6:  Let numouteriters be the total number of outer training iterations.

7:  Let numinneriters be the total number of inner training iterations.

8:  Let numexps be the number of experiments.

9:  Let batchsize be the number of units sampled. We use 64.

10:  for iouter < numouteriters do

11:         for i < numexps do

12:               U₀(✓₀) = ✓₀

13:               U₀(✓₁) = ✓₁.

14:               for k< numinneriters do

15:                      Sample Xⁱ and Xⁱ: control and treatment units. Sample batchsize units.

0                  1

16:                      L₀ = k⇡U  ₍✓ ₎(Xⁱ) — µ₀(Xⁱ)k

17:                      L₁ = k⇡U  ₍✓ ₎(Xⁱ) — µ₁(Xⁱ)k

18:                      L = L₀ + L₁        @L

19:                      Compute    ✓    =  @✓ .

20:                      Use ADAM with gradients given by    ✓    to obtain Uk₊₁(✓₀) and Uk₊₁(✓₁).

21:                      Set Uk(✓₀) = Uk₊₁(✓₀) and Uk(✓₁) = Uk₊₁(✓₁)

22:               end for

23:               for  p < N  do

24:                      ✓p = ✏p   Uk(✓p) + (1      ✏p)   ✓p.

25:               end for

26:         end for

27:  end for

28:  To Evaluate CATE estimate, do

29:  C = [].

30:  for i < numexps do

31:         U₀(✓₀) = ✓₀

32:         U₀(✓₁) = ✓₁.

33:         for k< numinneriters do

34:               Sample Xⁱ and Xⁱ: control and treatment units. Sample batchsize units.

0                  1

35:               L₀ = k⇡U  ₍✓ ₎(Xⁱ) — µ₀(Xⁱ)k

36:               L₁ = k⇡U  ₍✓ ₎(Xⁱ) — µ₁(Xⁱ)k

37:               L = L₀ + L₁        @L

38:               Compute    ✓    =  @✓ .

39:               Use ADAM with gradients given by    ✓    to obtain Uk₊₁(✓₀) and Uk₊₁(✓₁).

40:               Set Uk(✓₀) = Uk₊₁(✓₀) and Uk(✓₁) = Uk₊₁(✓₁)

41:         end for

42:         µˆⁱ  = ⇡U  ₍✓ ₎(Xⁱ)

0                k     0             0


43:

µˆⁱ  = ⇡U  ₍✓ ₎(Xⁱ)

1                k     1             1

44:         ⌧ˆⁱ = µˆⁱ — µˆⁱ

45:         C.append(⌧ˆⁱ).

46:  end for

47:  return C.

21


Under review as a conference paper at ICLR 2019

D    FULL  RESULTS

Below, we include the full results for the GOTV and MNIST experiments. In particular, we use show
results for transfer CATE learners with S, T, X, and Y base learners. We also provide full tables of
more comprehensive results for all methods and all train-test splits.

22


Under review as a conference paper at ICLR 2019

S−NN                                            T−NN                                            
Y−NN                                    other estimators

baseline

T−RF

0.15


0.10

baseline

warm

frozen                             SF

joint
S−RF


0.05

0.00

multi head

SF

frozen
multi head

baseline

SF

warm
multi head

frozen

MLRW

0         25        50        75       100   0         25        50        75       100   0         
25        50        75       100   0         25        50        75       100

Number of units in the training set (in 1000)

Figure 5: Social Pressure and Voter Turnout, Version 1.  Our results far exceed the previous state
of the art, which are represented here as S-RF, T-RF, and the baseline method for S-NN and T-NN.
Our new methods are Y-NN and the transfer learning methods: warm, frozen, multi-head, joint, SF
Reptile, and MLRW.


0.125

S−NN                                           T−NN                                           Y−NN  
                                 other estimators

baseline

T−RF

0.100


0.075

0.050

0.025

SF multi head  frozen

SF

frozen

baseline

multi head
SF

S−RF

joint


0.000

baseline

warm

multi head

warm

frozen

MLRW

0         25        50        75       100   0         25        50        75       100   0         
25        50        75       100   0         25        50        75       100

Number of units in the training set (in 1000)

Figure 6: Social Pressure and Voter Turnout, Version 2. Our results exceed the previous state of the
art results, which are represented here as S-RF, T-RF, and the baseline method for S-NN and T-NN.
Our new methods are Y-NN and the transfer learning methods: warm, frozen, multi-head, joint, SF
Reptile, and MLRW.

23


Under review as a conference paper at ICLR 2019

S−NN                                         Y−NN

baseline

1.6

0.8


0.4

0.2

baseline
frozen

warm

multi head

warm
MLRW

0              5             10                0              5             10

Number of units in the training set in 1000 units

Figure 7: MNIST task

24


Under review as a conference paper at ICLR 2019

25


Under review as a conference paper at ICLR 2019

26


Under review as a conference paper at ICLR 2019

27

