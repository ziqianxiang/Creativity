Under review as a conference paper at ICLR 2019
Spatial-Winograd Pruning Enabling
Sparse Winograd Convolution
Anonymous authors
Paper under double-blind review
Ab stract
Deep convolutional neural networks (CNNs) are deployed in various applications
but demand immense computational requirements. Pruning techniques and Wino-
grad convolution are two typical methods to reduce the CNN computation. How-
ever, they cannot be directly combined because Winograd transformation fills
in the sparsity resulting from pruning. Li et al. (2017) propose sparse Wino-
grad convolution in which weights are directly pruned in the Winograd domain,
but this technique is not very practical because Winograd-domain retraining re-
quires low learning rates and hence significantly longer training time. Besides,
Liu et al. (2018) move the ReLU function into the Winograd domain, which can
help increase the weight sparsity but requires changes in the network structure.
To achieve a high Winograd-domain weight sparsity without changing network
structures, we propose a new pruning method, spatial-Winograd pruning. As
the first step, spatial-domain weights are pruned in a structured way, which effi-
ciently transfers the spatial-domain sparsity into the Winograd domain and avoids
Winograd-domain retraining. For the next step, we also perform pruning and re-
training directly in the Winograd domain but propose to use an importance factor
matrix to adjust weight importance and weight gradients. This adjustment makes
it possible to effectively retrain the pruned Winograd-domain network without
changing the network structure. For the three models on the datasets of CIFAR-
10, CIFAR-100, and ImageNet, our proposed method can achieve the Winograd-
domain sparsities of 63%, 50%, and 74%, respectively.
1	Introduction
Deep convolutional neural networks (CNNs) have been ubiquitously utilized in various application
domains. However, their performance comes at the cost of a significant amount of computation
which keeps growing over time. As an example, for the ImageNet challenge (Russakovsky et al.,
2015), Krizhevsky et al. (2012) proposed AlexNet which requires more than 1.1 × 109 multipli-
cations. Later, in 2016, the ResNet-152 model (He et al., 2016) increased the computation cost to
11.3 × 109 multiplications. This high computation cost limits the deployment of larger and deeper
CNN models.
There are two primary methods to reduce the required computation of CNN models: pruning tech-
niques and Winograd/FFT convolution. Pruning removes redundant weight parameters, inducing
sparsity into the network. On the other hand, Winograd convolution (Lavin & Gray, 2016) and
FFT convolution (Mathieu et al., 2013) transform the computation into different domains. The
convolution operations can then be replaced by element-wise multiplications. For the typical con-
volution kernel size of 3 × 3, Winograd convolution can achieve more than twofold speedup over
highly optimized spatial convolution algorithms, and typically requires fewer flops than FFT-based
approaches (Li et al., 2017). Therefore, in this paper, we focus on the Winograd convolution.
The pruning techniques and Winograd convolution are not directly compatible with each other.
Sparse weight matrices, which are generated by pruning, lose most of the sparsity after the Winograd
transformation from the spatial (original) domain to the Winograd domain. The remaining sparsity
is much lower than what we need for improving computation performance.
To increase the Winograd-domain sparsity, Li et al. (2017) propose to perform pruning and retraining
directly on Winograd-domain weights. However, it requires using an extremely small learning rate,
1
Under review as a conference paper at ICLR 2019
(a) Conventional Winograd convolution.
Figure 1: Conventional Winograd convolution and sparse Winograd convolution (m = 4, n = 3).
(b) Sparse Winograd convolution.
e.g., 200x smaller for AlexNet, in retraining and is difficult to be applied to deep networks. Besides,
Winograd-ReLU pruning (Liu et al., 2018) moves ReLU function into the Winograd domain, which
helps increase Winograd-domain sparsity but requires changes in the network structure.
In this paper, to further improve the sparsity of Winograd-domain weights without changing the
network structure, we propose a new pruning method, spatial-Winograd pruning. It includes two
parts: spatial structured pruning and Winograd direct pruning. In spatial structured pruning, we
prune the spatial-domain weights in a structured way, in which the structures are designed to transfer
the spatial-domain sparsity into the Winograd domain efficiently. After spatial structured pruning,
weights of the pruned layers will be converted to and kept in the Winograd domain. Then, for
Winograd direct pruning, we perform pruning and retraining entirely in the Winograd domain to
improve the sparsity further.
This paper makes the following contributions:
•	We propose a new pruning method, spatial-Winograd pruning. Without changing the net-
work structure, it can achieve higher sparsity in Winograd-domain weights compared with
previous methods.
•	As the first part of spatial-Winograd pruning, we provide a structured pruning method to
transfer the spatial-domain sparsity into the Winograd domain efficiently. It can help avoid
Winograd-domain retraining in this part and accelerate the pruning process.
•	In the second part, to perform pruning directly in the Winograd domain, we present a
new approach to measuring the importance of each Winograd-domain weight based on its
impact on output activations. Also, we propose to use an importance factor matrix to adjust
the gradients of Winograd-domain weights, which makes it much faster to retrain deep
networks directly in the Winograd domain without changing the network structure.
2	Preliminary and Related Work
Winograd convolution (Lavin & Gray, 2016) is a typical algorithm to reduce the arithmetic com-
plexity of CNNs. It transforms the computation into the Winograd domain, and the convolution
operations can then be replaced by element-wise multiplications. We call the domain, in which the
conventional convolution operation is executed, to be the spatial domain.
The basic block of Winograd convolution works on a 2D input tile, I, with a size of m × m and a
2D weight filter, W , with a size of n × n. In this case, the 2D output tile generated, O, will have
a size of (m - n + 1) × (m - n + 1). For a typical convolutional layer, the input feature maps
are first disassembled into input tiles and, after the Winograd convolution, the output tiles will be
reassembled into the output feature maps.
Figure 1a shows how conventional Winograd convolution works. As the first step, the weight filter
W and the input tile I are converted into the Winograd domain using the predefined matrices G
and B. Element-wise multiplication is then applied to the Winograd-domain weight filter, GWG>,
and input tile, B>I B, to generate the Winograd-domain output tile with a size of m × m. In the
last step, the output tile is converted back into the spatial domain with another predefined matrix A.
With as the Hadamard product (element-wise multiplication), the entire process can be written as
O = A> [(GW G>)	(B>I B)]A	(1)
2
Under review as a conference paper at ICLR 2019
Figure 2: Overview of the SPatial-Winograd pruning.
The transform/inverse-transform matrices A, B and G are only determined by m and n. These
matrices contain many repeating elements and applying them requires only few multiplications. In
this case, considering only the element-wise multiplication between GWG> and B I B, the
Winograd convolution can reduce the number of multiplications from (m 一 n + 1)2n2 to m2.
In addition to Winograd convolution, pruning is also a well-explored method to reduce CNN com-
putation. Han et al. (2015b;a) propose to perform pruning and retraining iteratively, which can help
reduce the computation by up to 5×. To fully utilize the sparsity incurred by pruning to acceler-
ate CNN computation, Wen et al. (2016) and Yu et al. (2017) prune networks in a structured way:
weights are clustered into groups with hardware-friendly structures and then get pruned in groups.
However, Winograd convolution is not directly compatible with conventional pruning algorithms.
The transformation GWG> fills in the zeros in the sparse weight filters generated by pruning.
There have been several research attempts to solve this problem.
Liu & Turakhia (2016) propose to directly mask out Winograd-domain weights and use backprop-
agation to train the spatial-domain weights. However, compared with spatial-domain weights,
Winograd-domain weights are in a higher-dimensional space. Directly setting Winograd-domain
weights to zero will cause an inconsistency between the spatial domain and the Winograd do-
main. This inconsistency will lead to a significant accuracy loss or a low sparsity on networks,
e.g., AlexNet, for large datasets (Li et al., 2017).
To address the inconsistency between the spatial and Winograd domain, Li et al. (2017) propose
the sparse Winograd convolution. Figure. 1b shows how it works. Weight values are stored in
the Winograd domain instead of the spatial domain. Both pruning and retraining are applied di-
rectly to Winograd-domain weights. This native pruning algorithm achieves > 90% sparsity on
AlexNet (Krizhevsky et al., 2012) but cannot provide a high sparsity for deep networks (Liu et al.,
2018). Also, direct retraining in the Winograd domain requires an extremely small learning rate,
e.g., 200x smaller for AlexNet, which makes the retraining much slower.
Based on sparse Winograd convolution, Liu et al. (2018) introduce the Winograd-ReLU pruning.
It moves the ReLU function from the spatial domain into the Winograd domain. In this case, the
computation of Winograd convolution becomes
O = A> [(GWG>)	ReLU(B>I B)]A	(2)
The Winograd-domain inputs also become sparse, which helps further reduce the required compu-
tation. Besides, a higher sparsity in Winograd-domain weights can be achieved. However, with
weight filters being sparse, it is challenging to utilize both the weight and input sparsity for CNN
acceleration on general-purpose processors due to more irregularity in the access pattern and control
flow. Also, Winograd-ReLU pruning cannot be applied to conventional CNN models since the new
computation in Equation 2 does not correspond to the original convolution operation. It requires
changing the network structure and retraining the network from scratch.
3	Spatial-Winograd Pruning
In this paper, to achieve a high Winograd-domain weight sparsity on deep CNN models without
changing network structures, we propose the spatial-Winograd pruning. As shown in Figure 2, it
consists of two parts: spatial structured pruning and Winograd direct pruning.
In spatial structured pruning, spatial-domain weights are pruned in a structured way and then re-
trained to regain the original accuracy. After spatial structured pruning, the weights of the pruned
model will be transferred into and kept in the Winograd domain. The Winograd direct pruning then
3
Under review as a conference paper at ICLR 2019
performs pruning and retraining directly onto the weights in the Winograd domain. The pruning and
retraining steps in both spatial structured pruning and Winograd direct pruning will be iteratively
executed until we achieve the desired sparsity or the produced model loses much accuracy.
3.1	Spatial S tructured Pruning
The first part of the spatial-Winograd pruning is spatial structured pruning. Spatial-domain weights
which affect the same Winograd-domain weight are clustered into the same group. Less important
weight groups are removed, and the pruned network will be retrained to regain the accuracy. This
structured pruning method can help transfer more spatial-domain sparsity into the Winograd domain.
Spatial Pruning For each spatial-domain filter W, we need to generate a mask M spatial to indi-
cate the redundant weights. Assuming W has a size of n × n and the Winograd-domain filter Q is
m × m, we have Q = GWG>. Each element of the Winograd-domain filter, Qi,j, is the weighted
sum of the spatial-domain weights:
Qij=	X	(Sij,u,v ∙ Wu,v)	0 6 i,j 6 m - 1	(3)
06u,v6n-1
where S is a 4D tensor containing the weight coefficients of the spatial-domain weights and is only
determined by m and n. Details about the calculation of S can be found in Appendix A.1.
For each Winograd-domain weight Qi,j , we can create a set Di,j containing the spatial-domain
weights which affect the value of Qi,j. Di,j is defined as
Di,j = {Wu,v Si,j,u,v 6= 0, 0 6 u,v 6 n - 1}	(4)
In this case, for each weight group Di,j, we use a function h(Di,j) to measure its importance. In this
paper, we use the maximum norm function as h
h(Di,j) = max({ |Wu,v| Wu,v ∈ Di,j})	(5)
With a specific threshold tspatial, if h(Di,j) < tspatial, then Di,j is considered as redundant and all
weights included need to be removed. In this case, the corresponding Qi,j will be fixed to 0 and
also removed. The set of redundant weights for entire W is the union of all redundant Di,j and can
be calculated as
D =	Di,j	(6)
06i,j 6m-1,h(Di,j)<tspatial
Here we define Di,j in a structured way based on the relation between spatial-domain weights and
Winograd-domain weights. It helps transfer as much spatial-domain sparsity into Winograd-domain
sparsity as possible.
The mask matrix M spatial can be generated by
Mus,pvatial = 10 WWuu,,vv ∈∈/ DD 06u,v6n-1	(7)
Spatial Retraining After spatial pruning, we can perform the spatial retraining with conventional
training algorithms, e.g., stochastic gradient descent (SGD). The removed weights are fixed to 0 by
applying W = W M spatial after each training iteration. is element-wise multiplication. The
steps of spatial pruning and spatial retraining will be iteratively performed until the retrained model
loses much accuracy. The threshold tspatial is gradually increased to incur more sparsity into the
network.
In spatial structured pruning, both pruning and retraining steps are performed in the spatial domain.
It helps avoid the Winograd-domain retraining to accelerate the pruning process but, at the same
time, incurs high Winograd-domain sparsity.
3.2 Winograd Direct Pruning
After spatial structured pruning, as in sparse Winograd convolution, weights of the pruned model
will be transferred into and kept in the Winograd domain. In Winograd direct pruning, we measure
4
Under review as a conference paper at ICLR 2019
the importance of each weight based on its impact on output activations, and unimportant weights
are removed. The pruned network is then retrained in the Winograd domain, and an importance
factor matrix is deployed to adjust the weight gradients.
Winograd Pruning Similar to spatial pruning, in Winograd pruning, we need to generate a mask
matrix M W inograd for each Winograd-domain filter Q to indicate the redundant weights. With the
weight filter Q in the Winograd domain, the output tile O is calculated as
O=A>[Q(B>IB)]A
(8)
Each output element can be considered as the weighted sum of the products of weights and inputs
Ox,y =	):	(HX,y,i,j,s,t ∙ Qi,j ∙ Is,t)	0 6 x,y 6 m - n	(9)
06i,j,s,t6m-1
where H is a 6D tensor containing the weight coefficients of different products (Qi,j ∙ Is,t) and is
only determined by m and n. Details about the calculation of H can be found in Appendix A.2.
By removing one weight Qi,j , the change on each output Ox,y is
△Ox,y lQi,j = -1 ∙	(Hx,y,i,j,s,t ∙ Qi,j ∙ Is,t)	0 6 x,y 6 m - n
06s,t6m-1
(10)
In Winograd pruning, we need to remove a certain amount of weights while minimizing the change
of the output activations ∣∣∆O∣∣2. Removing an important weight will lead to a larger change in
output activations. Therefore, we propose to measure the importance of each weight Qi,j by the
expected value of ∣∣ΔO∣q, ∕∣2∙ In this case, We have
E(||∆O|Qi,j ll2) = E ( X h X	(Hx,y,i,j,s,t ∙ Qi,j ∙ Is,t)i )
06x,y6m-n	06s,t6m-1
Q2,j ∙ (	E	[Hx,y,i,j,s,t ∙E(Is2,t)+
06x,y6m-n,06s,t6m-1
(11)
HX	[Hx,y,i,j,s,t ∙ Hx,y,i,j,s0,t0 ∙ E(Is,t ∙ Is0,tO)] ∖
06x,y6m-n
06s,t,s0,t0 6m-1
(s,t)6=(s0,t0)
For simplicity, we can assume input values are independent and identically distributed (i.i.d.), and
have expected values of 0. With this assumption, we have
E(Is,t ∙ Is0,t0) = E(Is,t) ∙ E(Is0,t0) = 0	(s,t) = (s0,t0)	(12)
Since the importance of weights are relative numbers, we can assume E(Is2,t) = 1. In this case,
E(∣∆O∣Qi,j∣∣2) = Q2,j∙	E
Hx,y,i,j,s,t
06x,y6m-n,06s,t6m-1
(13)
Based on Equation. 13, we can generate an importance factor matrix F, where
Fij=Ej==S
Hx2,y,i,j,s,t	0 6i,j 6 m-1	(14)
06x,y 6m-n,06s,t6m-1
Therefore, F is only determined by m and n, and keeps the same for all 2D Winograd-domain filters
Q in a specific layer. Then Equation. 13 can be simplified to
E (go∣Qi,jll2) = Q2,j∙琮 j	(15)
In this case, with a specific threshold tW inograd, we can generate the mask matrix M W inograd as
WiWigrograd _ ∫0 Qj Fj < tWinograd	∩「y 1
^Mi,j	= J 1 q2 ∙ F2 _ > tWinograd	0 6 i,j 6 m 1
(16)
5
Under review as a conference paper at ICLR 2019
For a specific weight Qi,j, conventional pruning algorithms (Han et al., 2015b; Guo et al., 2016) use
its absolute value |Qi,j | as the weight importance, which is equivalent to using Qi2,j . Therefore, in
Equation 16, the employed weight importance, Q2,j ∙ Fj can be considered as using the importance
factor matrix F to adjust the conventional weight importance Qi2,j .
Winograd Retraining As the same with the spatial retraining, we fix the removed Winograd-
domain weights to 0 by applying Q = Q M Winograd after each training iteration.
However, using conventional SGD to retrain the Winograd-domain parameters will lead to diver-
gence. This is because, as shown in Equation. 14, different locations of Winograd-domain weights
have different importance and, therefore, require different learning speeds. Using an extremely small
learning rate can avoid the divergence but makes the retraining much slower.
To address this problem, in Winograd retraining, we propose to adjust the gradients of Winograd-
domain weights with the importance factor matrix F . Assume loss to be the loss value. At the
training step k, after the backward computation, the gradients of Q, d∂Qs | ^, will be adjusted by
dloss I VldjUsted	dloss I	ETOa
£)	=Wk0 F
(17)
where 0 and ◦a are the Hadamard division (element-wise division) and Hadamard power (element-
wise power of α) function, respectively. In this paper, based on empirical results, α is fixed to 1.5.
In this case, with the learning rate of η, the SGD update for the Winograd-domain weights Q at the
training step k becomes
Q|k+1 = Q|k - η ∙ ( ∂Q ∖k 0 F°a)	(18)
4 Experiments
To evaluate the spatial-Winograd pruning, we perform the experiments on three datasets: CIFAR-
10, CIFAR-100 (Krizhevsky, 2009) and ImageNet (ILSVRC-2012) (Russakovsky et al., 2015). Py-
Torch (Paszke et al., 2017) is used to implement the pruning framework.
We use the Winograd-ReLU pruning (Liu et al., 2018) as the baseline pruning technique. To show
the effectiveness of our proposed method, we test the same models as in Winograd-ReLU prun-
ing: VGG-nagadomi (Nagadomi, 2014), ConvPool-CNN-C (Springenberg et al., 2014) and ResNet-
18 (He et al., 2016) on the three datasets tested, respectively. Those models are chosen since the
majority of the included convolutional layers use 3 × 3 kernels.
For 3×3 kernels, we set the input tile size m to 6 instead of4. A larger input tile size can help achieve
higher computation speedup. With our proposed method, we expect that lower input tile sizes can
lead to a similar or higher sparsity. This is because, with lower input tile sizes, the spatial-domain
weights have less correlation between each other and the spatial structured pruning can achieve a
higher sparsity.
4.1	CIFAR-10: VGG-NAGADOMI
For the CIFAR-10 dataset, we test the VGG-nagadomi model (Nagadomi, 2014). It contains 8
convolutional layers with 3 × 3 kernels. We use batch normalization instead of dropout to regularize
the convolutional layers. The original model has a prediction accuracy of 93.96%. We prune the first
convolutional layer with a fixed Winograd-domain sparsity of 20%. For the remaining convolutional
layers, we incur a uniform Winograd-domain sparsity, increasing from 20% to 80%, for simplicity.
Figure 3a shows the pruning results. The baseline result reported in (Liu et al., 2018) is shown as
the dashed line. With < 0.1% accuracy loss, it achieves a sparsity of 60%. With spatial-Winograd
pruning, we can achieve a Winograd-domain sparsity of 63%. It is similar to Winograd-ReLU
pruning, but spatial-Winograd pruning does not require changing the network structure.
4.2	CIFAR-100: CONVPOOL-CNN-C
For the CIFAR-100 dataset, the ConvPool-CNN-C model (Springenberg et al., 2014) is tested. It
contains 9 convolutional layers, in which 7 layers use 3 × 3 kernels. The original model has a
6
Under review as a conference paper at ICLR 2019
(a) VGG-nagadomi on CIFAR-10.
Figure 3: Pruning of (a) VGG-nagadomi on CIFAR-10 (b) ConvPool-CNN-C on CIFAR-100 with
uniform sparsity across layers.
(东)ABnXV e15-3d;
(b) ConvPool-CNN-C on CIFAR-100.
・ ・ Winograd-ReLU Pruning ⅛-Λ Winograd Direct Pruning - 0.65
Spatial Structured Pruning ♦♦ Winograd Direct Pruning - 0.70
• ♦ Winograd-ReLU Pruning A-A Winograd Direct Pruning - 0.65
Spatial Structured Pruning ♦ Winograd Direct Pruning - 0.70
Wιnograd-domaιn Sparsity	Wιnograd-domaιn Sparsity
(a) Top-1 accuracy against sparsity.	(b) Top-5 accuracy against sparsity.
Figure 4: Pruning of ResNet-18 on ImageNet with uniform sparsity across the pruned layers.
prediction accuracy of 69.95%. We prune the first convolutional layer with a fixed Winograd-domain
sparsity of 20%. Similar to the VGG-nagadomi model, the remaining 6 convolutional layers with
3 × 3 kernels are iteratively pruned and retrained with uniform Winograd-domain sparsities.
Figure. 3b shows the result of the relative accuracy against the Winograd-domain sparsity. The
baseline result reported in (Liu et al., 2018) is shown as the dashed line. With <0.1% accuracy loss,
it achieves a sparsity of 40%. Winograd direct pruning is applied to the model pruned by spatial
structured pruning with 30% sparsity. With no accuracy loss, spatial-Winograd pruning can reach a
sparsity of 50%, which is 10% higher than Winograd-ReLU pruning.
4.3	ImageNet: ResNet- 1 8
We test the ResNet-18 model on the ImageNet (ILSVRC-2012) dataset. As the same with Winograd-
ReLU pruning, we replace each 2 × 2-stride 3 × 3 convolutional layer with a 2 × 2-stride max-pooling
layer followed by a 1 × 1-stride 3 × 3 convolutional layer. This change makes it easier to apply
Winograd convolution on most of the convolutional layers.
The original model has a top-1/top-5 prediction accuracy of 69.82%/89.55%. However, for
Winograd-ReLU pruning, Liu et al. (2018) use the model with the original top-1/top-5 accuracy
of only 66.67%/87.42%. Despite this, we still use the relative accuracies reported in (Liu et al.,
2018) as the baseline.
We prune the 16 convolutional layers in the residual blocks with the same Winograd-domain sparsity.
The first convolutional layer and the downsample layers are kept intact. Figure. 4 shows the results of
the relative accuracy against the Winograd-domain sparsity. As the dashed line show, the Winograd-
ReLU pruning achieves a sparsity of 70%/65% with <0.1% top-1/top-5 accuracy loss.
We apply Winograd direct pruning to the models pruned by spatial structured pruning with 65% and
70% Winograd-domain sparsity, annotated as Winograd direct pruning - 0.65 and 0.70, respectively.
As shown in the figure, with <0.1% top-1/top-5 accuracy loss, applying Winograd direct pruning to
the model with 70% Winograd-domain sparsity can achieve a higher sparsity of 74%/72%. This is
7
Under review as a conference paper at ICLR 2019
S >UE⊃UU‹ E1s-3α
(a) Winograd pruning: unadjusted or adjusted
weight importance for different locations.
Figure 5: Effectiveness of employing importance factor matrix F in (a) Winograd pruning and (b)
Winograd retraining.
♦ - Unadjusted (LR=le-7)	♦ ■ Unadjusted (LR=le-9)
♦ Unadjusted (LR=le-8) ɪ Adjusted (LR=le-6)
(％) Aue.Inba 3>=j23H
(b) Winograd retraining: unadjusted or adjusted
gradients for different locations..
W
because, with the sparsity increasing, Winograd direct pruning makes the prediction accuracy drop
much faster than spatial structured pruning. Although we can use the importance factor matrix F
to adjust the weight gradients to accelerate the Winograd-domain retraining, the learning rate still
needs to be much lower than in the spatial retraining. In this case, the accuracy loss recovered
through Winograd retraining is limited, which makes the accuracy drop much faster when applying
Winograd direct pruning.
4.4	Effectiveness of Importance Factor Matrix
In Winograd direct pruning, we use the importance factor matrix F to adjust the weight importance
and gradients for different locations of Winograd-domain weights. Here we test the effectiveness of
employing the importance factor matrix in both the Winograd pruning and retraining.
We first test how the importance factor matrix F helps in Winograd pruning. Winograd pruning
without retraining is applied to the model pruned by spatial structured pruning with 70% sparsity.
Figure 5a shows the relative accuracy against the sparsity when pruning with weight importance
unadjusted or adjusted with F . As shown in the figure, adjusting the weight importance with the
importance factor matrix can dramatically reduce the accuracy loss when performing Winograd
pruning. When pruning the model to 76% sparsity, using the absolute value as the weight importance
will cause a 22% accuracy loss. In comparison, using the importance factor matrix to adjust the
weight importance can help reduce the accuracy loss to 10%.
We also test the effectiveness of the importance factor matrix in Winograd retraining. For the model
pruned with spatial structured pruning (70% sparsity), Winograd pruning is applied to increase the
sparsity to 74%. We then perform Winograd retraining for 10 epochs. Figure 5b shows the rela-
tive accuracy against the retraining epochs with unadjusted and adjusted gradients. For unadjusted
gradients, we try three learning rates of 1e-7, 1e-8 and 1e-9. Higher learning rates, e.g., 1e-6, will
lead to an accuracy drop through retraining. As shown in the figure, adjusting the gradients with
the importance factor matrix can substantially accelerate the convergence. With retraining of only
10 epochs, it reduces the accuracy loss to 0.2% while retraining without gradient adjustment only
reduces the accuracy loss to 0.7%.
5 Conclusion
In this paper, we present a new pruning method, spatial-Winograd pruning, to improve the
Winograd-domain weight sparsity without changing network structures. It includes two steps: spa-
tial structured pruning and Winograd direct pruning. In spatial structured pruning, we prune the
spatial-domain weights based on the internal structure in the Winograd transformation. It can help
efficiently transfer the spatial-domain sparsity into the Winograd domain. For Winograd direct prun-
ing, we perform both pruning and retraining in the Winograd domain. An importance factor matrix
is proposed to adjust the weight gradients in Winograd retraining, which makes it possible to ef-
fectively retrain the Winograd-domain network to regain the original accuracy without changing the
network structure. We evaluate spatial-Winograd pruning on three datasets, CIFAR-10, CIFAR-100,
ImageNet, and it can achieve the Winograd-domain sparsities of 63%, 50%, and 74%, respectively.
8
Under review as a conference paper at ICLR 2019
References
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In
Advances In Neural Information Processing Systems, pp.1379-1387, 2016.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in neural information processing systems, pp. 1135-1143,
2015b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer,
2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4013-4021, 2016.
Sheng Li, Jongsoo Park, and Ping Tak Peter Tang. Enabling sparse winograd convolution by native
pruning. arXiv preprint arXiv:1702.08597, 2017.
Xingyu Liu and Yatish Turakhia. Pruning of winograd and fft based convolution algorithm. 2016.
Xingyu Liu, Jeff Pool, Song Han, and William J Dally. Efficient sparse-winograd convolutional
neural networks. arXiv preprint arXiv:1802.06367, 2018.
Michael Mathieu, Mikael Henaff, and Yann LeCun. Fast training of convolutional networks through
ffts. arXiv preprint arXiv:1312.5851, 2013.
Nagadomi. Code for kaggle-cifar10 competition. 5th place. https://github.com/
nagadomi/kaggle-cifar10-torch7, 2014.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074-2082,
2016.
Jiecao Yu, Andrew Lukefahr, David Palframan, Ganesh Dasika, Reetuparna Das, and Scott Mahlke.
Scalpel: Customizing dnn pruning to the underlying hardware parallelism. In ACM SIGARCH
Computer Architecture News, volume 45, pp. 548-560. ACM, 2017.
9
Under review as a conference paper at ICLR 2019
A COEFFICIENT TENSORS S AND H
The coefficient tensors S and H contain the weight coefficients in Equation 3 and Equation 9. They
are only determined by the input tile size m and the weight filter size n.
A.1 COEFFICIENT TENSOR S
To calculate the tensor S, we first introduce an equivalent transformation: for two vectors a and b
with a size of m, and a matrix C with a size of m × m, we have
a>Cb = ~1 >[(ab>)	C] ~1	(19)
where ~1 is a vector of size m and all its entries are 1.
For matrix S, with the weight transform matrix G, we have Q = GWG>. Each element in
Winograd-domain weight filter Q is calculated as
Qij = Gi,: ∙ W ∙ (Gj,：)>
=1 >[((Gi,：)>Gj,：) Θ W] ~
= X (Gi,u ∙Gj,v ∙ Wu,v)
06u,v6n-1
(20)
where 0 6 i, j 6 m - 1. In this case, compared with Equation 3, each element in the coefficient
tensor S can be calculated as
Si,j
,u,v
Gi,u∙Gj,v
(21)
where 0 6 i, j 6 m -
1, 0 6 u, v 6 n - 1.
A.2 COEFFICIENT TENSOR H
With the Winograd-domain weight filter Q, the output tile O is calculated as
O=A>[QΘ(B>IB)]A
Each element Ox,y is calculated as
Ox,y = (A:,x)> [Q Θ (B>I B)]A:,y
where 0 6 x, y 6 m - n. Based on Equation 19, we have
Ox,y = ~1 > [(A:,x(A:,y)>) Θ Q Θ (B>I B)] ~1
LetV = (A:,x(A:,y)>) Θ Q Θ (B>I B), then
Vi,j = (Ai,xAj,y) ∙ Qi,j ∙ (BI B)i,j
where 0 6 i,j 6 m - 1. The element (B>I B)i,j can be calculated as
(B>IB)i,j = (B:,i)>I B:,j
=1 >[(B：,i(B：,j)>) Θ I] 1
= X	[(Bs,iBt,j ) ∙ Is,t]
06s,t6m-1
Based on Equation 24, 25 and 26, we have
Ox,y =	Vi,j
06i,j 6m-1
= X	[(Ai,χAj,y) ∙ Qi,j ∙ (B>I B)i,j]
06i,j 6m-1
= X	h(Ai,χAj,y) ∙Qi,j∙ X	[(Βs,iΒt,j) ∙ Is/
06i,j 6m-1	06s,t6m-1
= E	[(Ai,xAj,yBs,iBtj) ∙ Qi,j ∙ Is,t]
06i,j,s,t6m-1
(22)
(23)
(24)
(25)
(26)
(27)
10
Under review as a conference paper at ICLR 2019
Figure 6: Accuracy loss of ResNet-18 when incurring 60% Winograd-domain sparsity into different
layers. Spatial structured pruning is applied with no retraining.
Layer	Spatial Structured Pruning		Winograd Direct Pruning
	Winograd Sparsity	Corr. Spatial Sparsity	Winograd Sparsity
0-b	78.0%	-888%-	85.0%
1-b	77.1 %	-87.9%-	84.0%
2-b	76.4%	-88.5%-	84.4%
3-b	85.2%	-93ΓΓ%-	90.4%
4-b	76.9%	-88.9%-	847%
5-b	88.6%	-95.1%-	93.0%
6-b	74.4%	-88.3%-	82.4%
7-b	82.6%	-87.8%-	92.3%
Average	79.4%	88.9%	87.6%
Top-1 Acc.	69.92 %		69.94 %
Top-5 Acc.	89.34 %		89.51 %
Table 1: The sparsity for the pruned convolutional layers when pruning the second convolutional
layer in each residual block of ResNet-18.
Therefore, compared with Equation 9, each element in the coefficient tensor H is calculated as
Hx,y,i,j,s,t = Ai,x Aj,y Bs,i Bt,j	(28)
where 0 6 x, y 6 m - n, 0 6 i, j, s, t 6 m - 1.
B	ResNet- 1 8 Pruning with Varied Sparsities acros s Layers
In addition to pruning ResNet-18 with the same sparsity across all targeting layers, we experiment
incurring different sparsities into different layers with spatial-Winograd pruning.
For spatial structured pruning, we first test the pruning sensitivity of each convolutional layer to
decide which layers need to be pruned and the corresponding thresholds. To choose the targeting
layers, we measure the accuracy loss when 60% of Winograd-domain weights are pruned for each
layer. Only one layer is pruned at one time, and other layers are kept intact. Figure. 6 shows the
results. In ResNet-18, the i-th residual block contains two convolutional layers, i-a and i-b. As
shown in the figure, the first layer in each residual block is much more sensitive to pruning than the
second layer. Therefore, we will only prune the second convolutional layer, i-b, in each residual
block.
For each targeting layer i-b, we determine the corresponding pruning threshold tispatial based on its
pruning sensitivity. We gradually increase the threshold until the validation accuracy drops by 2%
and the threshold is recorded as tispatial, 2% loss. Then in spatial structured pruning, we can calculate
the threshold used for layer i-b as tSpatial = β ∙ tspatial, 2% loss where β is a multiplier shared across
all targeting layers. With a larger β, the threshold and, therefore, the sparsity will be higher. Also, in
Winograd direct pruning, we use the same strategy to choose the thresholds used for different layers.
11
Under review as a conference paper at ICLR 2019
(％)s」£EAM Joα,6jsuα,xlα,d
5	10	15	20	25	30
Num of Pruned Weights in a 2D Filter
62
Figure 8: Sparsity of different locations of
Winograd-domain weights for: (a) filters
with 20 weights pruned; (b) filters with at
least one weight remaining. Darker locations
have higher sparsities.
35
Figure 7: Distribution of 2D filters with dif-
ferent numbers of weights pruned.
Table 1 lists the pruning results. After spatial structured pruning, we can reach an average Winograd-
domain sparsity of 79.4% for the pruned layers. The corresponding spatial-domain sparsity is 88.9%
which is 9.5% higher. Winograd direct pruning can further improve the Winograd-domain sparsity
to 87.6% and layer 5-b has the highest sparsity of 93.0%.
C Sparsity Distribution
For the pruned ResNet-18 model, we analyze more detailed sparsity distribution across and inside
2D weight filters. Here each Winograd-domain weight matrix Q is considered as a 2D filter. We
use the last convolutional layer (7-b) as an example. The model with a uniform sparsity of 74%
across all pruned layers, which corresponds to point P in Figure 4a, is tested. Figure 7 shows the
sparsity distribution across the filters. As shown in the figure, more than half (62%) of the filters
have all weights removed. An interesting observation is that a large portion of the filters have exact
20 weights removed.
To explain why many filters have exact 20 weights removed, we visualize the sparsity distribution
inside the filters. Figure 8a shows the sparsity of different locations for the filters with 20 weights
removed. Darker locations have higher sparsities where more weights are removed. The border
part of the 6 × 6 filter, which includes 20 weights, has much higher sparsity than the central part.
It means the border part of the Winograd-domain weights is much less important than weights in
the central part. A potential reason is that the weights in the central part are correlated to more
spatial-domain weights and, therefore, removing them will lead to a larger difference in the output
activations. In Figure 8b, we also visualize the sparsity distribution inside the filters with at least
one weight remaining, and it shows a similar pattern.
12