Under review as a conference paper at ICLR 2019
Provable Defenses against Spatially Transformed Ad-
versarial Inputs: Impossibility and Possibility Results
Anonymous authors
Paper under double-blind review
Abstract
One intriguing property of neural networks is their inherent vulnerability to adversarial inputs, which
are maliciously crafted samples to trigger target networks to misbehave. The state-of-the-art attacks
generate adversarial inputs using either pixel perturbation or spatial transformation. Thus far, several
provable defenses have been proposed against pixel perturbation-based attacks; yet, little is known
about whether such solutions exist for spatial transformation-based attacks. This paper bridges
this striking gap by conducting the first systematic study on provable defenses against spatially
transformed adversarial inputs. Our findings convey mixed messages. On the impossibility side,
we show that such defenses may not exist in practice: for any given networks, it is possible to
find legitimate inputs and imperceptible transformations to generate adversarial inputs that force
arbitrarily large errors. On the possibility side, we show that it is still feasible to construct adversarial
training methods to significantly improve the resilience of networks against adversarial inputs over
empirical datasets. We believe our findings provide insights for designing more effective defenses
against spatially transformed adversarial inputs.
1	Introduction
Despite their tremendous success in computer vision and pattern recognition (LeCun et al., 2015),
neural networks are inherently vulnerable to adversarial inputs - those maliciously crafted samples
to trigger target networks to misbehave - which hinders their application in security-critical domains
(Szegedy et al., 2014; Goodfellow et al., 2015; Sharif et al., 2016; Eykholt et al., 2017). This seri-
ous vulnerability has spurred intensive research effort, leading to an arms race between developing
more powerful attacks (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2015; Papernot et al., 2016a;
Carlini & Wagner, 2017; Meng & Chen, 2017; Xu et al., 2018) and designing more robust defenses
(Papernot et al., 2016b; Madry et al., 2018; Tramer et al., 2018; Kannan et al., 2018). For example,
defensive distillation (Papernot et al., 2016b), originally considered as an effective defense, was later
shown to be ineffective against stronger attacks (Carlini & Wagner, 2017).
To end this constant arms race, several provable defense methods (Bastani et al., 2016; Raghunathan
et al., 2018; Wong & Zico Kolter, 2018) have been proposed recently. Motivated by that most exist-
ing attacks generate adversarial inputs by directly modifying the pixel values of benign inputs, where
the perturbation “imperceptibility“ is often quantified by its Lp norm, such provable defenses, by
reducing the upper bound of the worst-case loss that can be caused by norm-bounded perturbation,
provides the following guaranteed protection: for a given network and test input, no attack is able to
force the error to exceed a certain threshold.
The effectiveness of these provable defenses hinges on modeling the upper bound of the worst-
case loss under norm-bounded perturbation. While most existing attacks indeed use Lp norm to
quantify the perturbation imperceptibility, it has long been recognized that this is not an ideal metric
(Johnson et al., 2016; Isola et al., 2016). Recently, spatial transformation has been proposed as a
new approach for generating adversarial inputs (Xiao et al., 2018; Engstrom et al., 2017b): instead
of directly modifying pixel values, it changes their spatial positions (e.g., via rotation and shifting),
which is shown to better preserve the identity and structure of the original image (Zhou et al.,
2016). It is observed that spatial transformation-based adversarial inputs are often perceptually
less distinguishable from pixel perturbation-based counterparts, as shown in Figure1. However,
for spatial transformation, it is insensible to measure the perturbation imperceptibility using Lp
norm; rather, alternative metrics such as the total variation (Rudin et al., 1992) are used to quantify
1
Under review as a conference paper at ICLR 2019
(a)	(B)
(c)
(d)
Figure 1: (a) benign input, (b) pixel perturbation-based adversarial input, (C) spatial transformation-
based adversarial input, and (d) flow vector of spatial transformation.
the magnitude of spatial deformation. In other words, spatially transformed adversarial inputs are
inherently not norm-bounded, which thus raises an important and intriguing question:
Does there exist provable defenses that provide a given network with guaranteed
protection against spatially transformed adversarial inputs?
In this paper, we present a systematic investigation to answer this question. Under the setting of
a neural network with one hidden layer, we conduct both analytical and empirical studies on the
existence of provable defenses. Our findings convey mixed messages. The impossibility results
indicate that such defenses may not exist in practice. We show that for any given networks, it is
possible to find benign inputs and imperceptible transformations to produce adversarial inputs that
force arbitrarily large errors. Meanwhile, the possibility results imply that while the worst case
can be arbitrarily bad, it is still possible to construct adversarial training methods to significantly
improve the robustness of given networks over empirical datasets.
In summary, to our best knowledge, this work represents the first in-depth study on provable defenses
against spatial transformation-based adversarial attacks. We provide both impossibility and possi-
bility results regarding the existence of such defenses. We believe that our findings will highlight
the fundamental difference of spatial transformation- and pixel perturbation-based attacks from the
defender,s perspective, and inspire designing more effective defenses against spatially transformed
adversarial inputs.
2	Background
2.1	Spatial Transformation
We model a network as a function f. For simplicity, we focus on a binary classification task (‘+’ and
‘-')(the extension to the multi-class setting given in the appendix), in which the network predicts the
class of a given input X as f (x) > 0 as '+' and f (x) < 0 as '-'. In adversarial attacks, the adversary
crafts an adversarial input X by applying perturbation r to a benign input x: X = g(r; x), with the
objective of forcing X to be misclassified. To maximize the attack evasiveness, the adversary also
desires to preserve x's perceptual quality in x.
According to their perturbation types, existing adversarial attacks can be categorized as either pixel
perturbation-based attacks or spatial transformation-based attacks. Figure 1 compares two versions
of adversarial inputs with respect to the same benign input.
In spatial transformation attacks, instead of directly modifying x,s pixel values, the adversary applies
a spatial transformation (e.g., shifting and rotation) over X to generate x. Specifically, we use the
per-pixel flow field (displacement) r to synthesize X using the pixels of x. Let Xi be X,s i-th pixel
and (Ui, Vi) be its position in X. We optimize the amount of displacement with a flow vector r :=
(∆ui, ∆vi), which goes from X to x. The position of Xi,s corresponding pixel in X is derived as:
(ui, Vi) = (Ui + △%, Vi + ∆vi). As (%, Vi) may be fractional numbers and do not necessarily lie
on the integer grid, we use the differentiable bilinear interpolation to compute Xi,s pixel value:
Xi = ɪ2	Xj	max (°, 1 —	|Ui	+ ∆ui	—	UjI) max (0,1 —	|Vi + ∆vi	— vj |)	(1)
j∈N (i)
2
Under review as a conference paper at ICLR 2019
where j iterates over N (i): the set of pixels adjacent to (ui, vi) (top-left, top-right, bottom-left, and
bottom-right) in x. Under this setting, `diff can be instantiated with the total variation Rudin et al.
(1992), which compares the spatial movement distance for any two adjacent pixels:
`diff (r)
E (IIδUi-δUjIl2 + ∣∣δVi-
j∈N(i)
vjII22)
(2)
i
where i iterates over all the pixels.
We now derive its matrix form. Let r ∈ Rd×2 : r = [r(u) , r(v)] be the flow field, where r(u) and r(v)
respectively represent the transformation along the horizontal and vertical directions. We assume
r(u) = r(v) and will justify this assumption in § 3. With a little abuse of notations, let r = r(u) =
r(v). Let M(t) be the top neighbor matrix, with the i-th row being the top neighbor j of the i-th
pixel (i.e., the one-hot encoding of j). Similarly, we define M(b), M(l), and M(r) respectively as
the bottom, left, and right neighbor matrices. We have the proposition (proof in appendix):
Proposition 1. The magnitude of spatial transformation can be specified as:
'diff (r) = √2r>M > Mr
where M>M = Pi∈{t,b,ι,r} Q - M⑴ Y(I- M⑴)∙
Therefore, the constraint on the spatial transformation magnitude can be given as:
`diff (r) = kMrk ≤	(3)
2.2	Adversarial Loss
Let g(r; x) be the adversarial attack that applies the transformation r to x to generate the adversarial
input x, i.e., X = g(r; x) and X = g(0; x). Therefore, for a given benign input x, We define the
margin between f (x) and f (x) as the adversarial loss. Formally,
'adv(x,r) =f f (x) - f (x)
f ◦ g(r; x) - f(x)
2
Note that 'adv(x, r) is a function of r.
In the case of bilinear interpolation as defined in Eqn (1), g is differentiable with respect to r. Let
gj be the j-th element of g and ri be r's i-th element. Note that in Eqn (1), Xi only depends on ri.
Thus, dj = 0 for i = j. Meanwhile,
∂gi	0	0	lui + ri- Ujl	≥ 1
∂-l =工 Xj max	(0,1 -|Vi +	r - Vj ∣)	J	1	Ui + r ≥ Uj
ri	j	- -1 Ui + Iri < Uj
where sub-gradients are used (Jaderberg et al., 2015) due to the discontinuities of interpolation.
The Jacobian Dg of g is a diagonal matrix with its i-th diagonal element given by dgi. Further, the
∂ri
gradient of f with respect to g is given as: Vf
Putting everything together using the chain rule, we have the gradient of f ◦ g with respect to r as:
V(f ◦ g)(r; x) = (Dg)> Vf
We now compute 'adv(r; x) using the integration along the line from X to X = f ◦ g(r; x):
'adv(x,r) = / (V(f ◦ g)(zr; x))> rdz
0
(4)
3 Impossibility Results
Next we present the negative results for provable defenses against spatially transformed adversarial
inputs. Intuitively, we show that given a classifier f and a threshold δ > 0, if the perturbation magni-
tude is allowed to be reasonably large, the adversary is able to find an input x* and its corresponding
3
Under review as a conference paper at ICLR 2019
spatial transformation L such that the adversarial input causes f to misclassify the adversary input
g(r*; x*) with a large margin 'adv(x*,r*) ≥ δ.
Specifically, we consider the maximum influence of spatial transformation on the adversarial loss
via the following supremum:
sup	'adv (x,r)
kxk∞≤1,kMrk≤
We then find a particular input-transformation pair (x*, r*) such that f ◦g(r*; x*) -f(x*) represents
a lower bound of this supremum. Apparently, if f ◦ g(r*; x*) - f(x*) ≥ 2δ, the supremum of
'adv(x, r) must exceed δ. Next we present the concrete attack for a network with one hidden layer.
3.1 Lower Bound Attack
For a network with one hidden layer, we have f(x) = v>σ(W x) where W and v respectively
represent the weights of the first and second layers of the network, and σ is the activation function
(e.g., ReLU). In this case, We have Vf = W> diag(v)Vσ, where (Vσ)i = d∂(yi) With yi =
∂yi
(W g(r; x))i. Substituting it into Eqn (4), we have:
`adv
(x, r) =
0
(Vσ)> diag(v)W Dg(zr)rdz
(5)
We consider a linear approximation of 'adv(x, r):
2adv(x,r) = (Vσ(Wx))> diag(v)W diag(x)r
∙-v
The following theorem finds a lower bound of the supremum of `adv (x, r) (proof in appendix).
Theorem 1. For a given network f, assuming W has rank d and Wd is a rank-d submatrix of W,
∙-v
sup	'adv(x, r) ≥ EkM-1 diag(x*)W> diag(v)Vσ(Wx*)k
kxk∞ ≤1,kM rk≤
where x* = π∞(Wd-11) (1 is an all-one vector). The lower bound is attained when
r* = EM-1π2(M-1 diag(x*)W diag(v)Vσ(Wx*)).	(6)
Theorem 1 gives the initial setting of (x*, r*). We further increase the adversarial loss by improving
both x* and r*. Let r(i) = (22-1) r. We compute the midpoint approximation of Eqn (5):
1n
`adv (x, r) ≈ 一 VJi Vσ (Wg (r(i); x)) diag(v)WDg (r(i); x) r	(7)
i=1
|--------------------{z-------------------}
Summation
where we divide the interval [0, 1] into n intervals.
Updating x* To update x*,we consider X as a symbolic vector. Yet, itis difficult to optimize Eqn (7)
with respect to x directly as Vσ also depends on x. Instead, we update Vσ and x alternatively.
Let xk be the current setting ofx*. We update x* as follows. Let αi be the coefficient of x’s i-th ele-
ment in Eqn (7) with Vσ computed based on xk. We set xk+1 = [sign(α1), sign(α2), . . . , sign(αd)]>,
which maximizes Eqn (7) under fixed r, fixed Vσ and the constraint of kxk∞ ≤ 1. We repeat this
procedure until convergence.
Updating r* Meanwhile, for fixed x* , we may also increase the adversarial loss by refining r* using
the integration approximation. Let rk be the current setting ofr*. We update r* using the rule of:
rk+1 = eM-1π2 (M-1 X Dg(rki); x) Wdiag(v)Vσ (Wg(rf); x)))	(8)
Intuitively, we find rk+1 that aligns with the summation in Eqn (7) under kM rk+1 k ≤ E.
Algorithm 1 sketches the attack against a given network. After initialization (line 1-3), it updates x*
and r* alternatively until convergence (line 4-8).
4
Under review as a conference paper at ICLR 2019
Algorithm 1: Lower Bound Attack.
Input: v, W: f's weights; e: threshold of perturbation magnitude; M: neighboring matrix; α:
update step; n: parameter of midpoint approximation
Output: x*: genuine input; r*: spatial transformation
// initialization
ι Wd J rank-d submartrix of W;
2	x* J π∞(Wj11);
3	initialize r* according to Eqn(6);
4	while not converged do
// refinement of x*
5	while not ConVerged do
6	compute Eqn (7) with symbolic x;
// αi: coefficient of x's i-th element in Eqn (7)
7	x* J [sign(αι), sign(a2),..., sign(ad)]>;
// refinement of r*
8	update r* according to Eqn (8);
9	return (x*, r*);
3.2 Empirical Evaluation
Next We empirically validate the above analytical result. We mainly use MNIST (LeCun et al.,
1998) as the benchmark dataset and a network with one hidden layer (with 512 hidden neurons) as
the target model f. We consider three variants of f: (i) fnt - f is normally trained, (ii) f^ 一 f is
randomly initialized, and (iii) fat - f is adversarially trained (Madry et al., 2018).
Figure 2: Lower bounds of adversarial loss by Algorithm 1 (w.r.t. 'adv) and Theorem 1 (w.r.t. 'adv).
Benign Input	Adversarial Input	Flow Vector
Figure 3: Samples of benign and adversarial inputs found by Algorithm 1.
5
Under review as a conference paper at ICLR 2019
Figure 4: Convergence of estimation of adversarial loss by Algorithm 1.
We first measure the quality of lower bounds given by Algorithm 1. With respect to each of the
networks, under different setting of , we measure the estimation by Algorithm 1 (with respect to
`adv) and that by Theorem 1 (with respect to `adv). Figure 2 shows the results. It is clear that as
varies from 0 to 7.84, the adversarial loss increases in an exponential manner, indicating that with
reasonably transformation magnitude, it is possible to find (x*, r*) that forces arbitrarily large error.
Also note that Algorithm 1 provides much higher-quality estimation of adversarial loss compared
with the initial estimation by Theorem 1. Figure 3 shows a set of samples of benign and adversarial
inputs (and their associated flow fields).
Figure4 shows that Algorithm 1 typically converges fast under varied setting of network and , which
validates our analysis of the performance of Algorithm 1.
4 Possibility Results
Despite the impossibility results for provable defenses against spatial transformation-based attacks,
in this section, we show that it is still possible to construct adversarial training methods that signifi-
cantly improve DNN resilience on empirical datasets.
4.1 Adversarial Training
To this end, we establish an upper bound on 'adv(χ, r) and reduce this upper bound in training f to
improve its robustness. Specifically, we consider the integration form of `adv (x, r) in Eqn (4).
f
0
`adv (x, r)
(▽(f ◦ g)(zr))> rdz
max
z∈[0,1],kM rk≤
(▽(f ◦ g)(zr; x))> r
≤
In the following, we derive its upper bound form for a network with one hidden layer.
We have the following derivation regarding 'adv(χ, r):
'adv(x,r)	≤ max (Vσ)> diag(v)WDgr
Vσ,Dg,r
(i)
≤ max s> diag(v)W Dgr
llsll∞≤1,Dg,r
(ii)
≤ 2 max	s> diag(v)W diag(λ)t
一	∣∣s∣∣∞≤ι,∣∣t∣∣∞≤ι
In (i), we use the assumption that the non-linear activation function applied element-wise has
bounded gradients Vσ ∈ [0, 1], which holds in many settings (e.g., for ReLU, Vσ ∈ [0, 1]; for
sigmoid, Vσ ∈ [0, 1/4]). In (ii), we use the following results. As kMrk = defines an ellipsoid E
centered at 0, diag(z)r with kzk∞ ≤ 1 corresponds to the union of the set of ellipsoids mirrored to
E along the axises. We can thus bound diag(z)r using the axis aligned bounding box (AABB) of
these ellipsoids. We have the following proposition (proof in appendix):
Proposition 2. The AABB of the ellipsoid defined by kMrk = is given by [-λi, λi] (1 ≤ i ≤ d),
where λi2 is the i-th diagonal element of the matrix (M>M)-1.
6
Under review as a conference paper at ICLR 2019
def
z=
P	d=ef
we obtain the formulation:
Because diag(v)W diag(λ) is not necessarily positive (negative) semidefinite, the above formula-
tion is in general a non-convex optimization problem, which is similar to the NP-hard MAXCUT
problem. We thus resort to the Semidefinite Programming (SDP) relaxation.
We first re-parameterize the variables as:
s
t
0	diag(v)W diag(λ)
diag(λ)W > diag(v)	0
`adv (x, r) ≤ max	z>P z
—I∣z∣l∞≤ι
Using the fact that z>Pz = tr(zz>P), we first have:
max z>Pz =	max tr(ZP)
∣∣z∣∣∞≤1	Z=zz>,kzk∞≤1
By relaxing Z = zz> and kzk∞ ≤ 1 with Z 0 and diag(Z) ≤ 1 (Boyd & Vandenberghe, 2004),
we have the following convex SDP problem:
max tr(ZP)
t	diag(Z) ≤ 1
s.t.	Z 0
which is efficiently solvable using off-the-shelf SDP optimizers.
4.2 Empirical Evaluation
Next we empirically validate the above analytical results. Following the same setting as in § 3, we
mainly use MNIST (LeCun et al., 1998) as the benchmark dataset and a two layer neural network as
the target network model f. We consider four variants of f: (i) fnt - f is normally trained, (ii) fat
-f is adversarially trained (Madry et al., 2018), (iii) f is trained with regularized bounds, and (iv)
fstn - f is trained with spatial transformer network. For (iii), we consider three types of bounds:
Frobenius - ffro , spectral - fspe , and SDP fsdp (our proposed method). The implementation details
Figure 5: 0/1 adversarial loss for different variants off on MNIST based on Frobenius, spectral and
SDP upper bound.
Upper Bounds We evaluate the different upper bounds on adversarial loss. For each of the net-
works described above, we compute the 0/1 loss based on the Frobenius, spectral, and SDP bounds
respectively, with results shown in Figure 5. It is observed that all the bounds are fairly loose: the
estimated adversarial loss goes to 1 as reaches 0.1. This again echos the impossibility results found
in § 3. However, our proposed SDP bound is slightly tighter than alternative bounds under fsdp .
Model Robustness We also observe that fsdf is more robust than alternative models under varying
perturbation budget. Specifically, we plot both the success rates of spatial transformation attacks
and the test accuracy of different models under varying τ in Figure 6. Here τ controls the allowed
magnitude of spatial transformation, with τ = 0.05 used in (Xiao et al., 2018). For fsdp, it is
observed that there are the reduction of around 0.1 for attack success rates and the gain of around
0.08 for adversarial test accuracy, compared with other training objectives.
7
Under review as a conference paper at ICLR 2019
.8 .6 .4
000
etarsseccus kcattA
0 0.025
0.1
0.05	0.075
0.8
n
U
U
0.6
0.4
I
S
S
02
0.2
0.05	0.075
0.1
∞
0 0.025
Hyper-parameter 7
(a)	(b)
Figure 6: (a) Spatial transformation attack success rate versus τ under different training objectives.
(b) Test accuracy of spatial transformed adversarial inputs for different training objectives and τ
(τ = ∞ corresponds to benign cases).
5	Related Work
Adversarial Inputs Due to their increasing use in security-critical domains, machine learning mod-
els are becoming the targets of malicious attacks (Barreno et al., 2010; Biggio & Roli, 2018). Com-
pared with simple models (e.g., decision tree, support vector machine, and logistic regression),
securing deep neural networks deployed in adversarial settings poses even more challenges due to
their significantly higher model complexity (LeCun et al., 2015). A variety of adversarial attacks
have been proposed, including both white-box attacks (Szegedy et al., 2014; Goodfellow et al., 2015;
Moosavi-Dezfooli et al., 2015; Papernot et al., 2016a; Kurakin et al., 2016; Carlini & Wagner, 2017;
Meng & Chen, 2017; Xu et al., 2018) and black-box attacks (Papernot et al., 2016; Liu et al., 2016;
Reddy Mopuri et al., 2017).
Spatial Transformation While most state-of-the-art attacks directly modify the pixel values of
benign images, recent work (Xiao et al., 2018; Engstrom et al., 2017b) has proposed to use spa-
tial deformation as an alternative to generate adversarial inputs. Compared with pixel perturbation,
spatial transformation tends to better preserve the perceptual quality of original images. Yet, be-
sides their perceptual superiority, thus far little is known about the security properties of spatially
transformed adversarial inputs.
Provable Defenses Another line of research has focused on improving model resilience against
adversarial attacks by developing new training and inference strategies (Goodfellow et al., 2015;
Huang et al., 2015; PaPemot et al., 2016b; Ji et al., 2018; Madry et al., 2018; Tramer et al., 2018;
Kannan et al., 2018). Yet, the fundamental challenges of defending against adversarial attacks stem
from their adaPtive nature. Existing defenses, once dePloyed, can be easily circumvented by adaPtive
attacks. This arms race between attacks and defenses has motivated the develoPment of Provable
defenses (Bastani et al., 2016; Raghunathan et al., 2018; Wong & Zico Kolter, 2018), which ensure
that for a given network, no attack is able to force the error to exceed a certain threshold. However,
existing Provable defenses all assume norm-bounded Perturbation. It is an oPen question whether
such defenses exist for sPatial transformation-based attacks.
6	Conclusion
This work rePresents the first systematic study on Provable defenses against sPatial transformation-
based adversarial attacks. Our findings include both Possibility and imPossibility results. We show
that such Provable defenses do not exist in Practice. For any given networks, the adversary is able
to find legitimate inPuts and imPercePtible transformations to generate adversarial inPuts that force
arbitrarily large errors. Yet, we also show that it is still Possible to construct adversarial training
methods to significantly imProve the robustness of given networks against adversarial inPuts over
emPirical datasets. We hoPe this work may insPire designing more effective defenses against sPatial
transformation-based attacks and adversarial attacks in general.
8
Under review as a conference paper at ICLR 2019
References
Marco Barreno, Blaine Nelson, Anthony D. Joseph, and J. D. Tygar. The Security of Machine
Learning. Mach. Learn., 81(2):121-148, 2010.
O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori, and A. Criminisi. Measuring
Neural Net Robustness with Constraints. In Proceedings of Advances in Neural Information
Processing Systems (NIPS), 2016.
Battista Biggio and Fabio Roli. Wild Patterns: Ten Years after the Rise of Adversarial Machine
Learning. Pattern Recognition, 84:317-331, 2018.
Battista Biggio, Igino Corona, Davide Maiorca, Blame Nelson, Nedim Srndic, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Pro-
ceedings of European Conference on Machine Learning and Knowledge Discovery in Databases
(ECML/PKDD), 2013.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, New
York, NY, USA, 2004.
N. Carlini and D. Wagner. Towards Evaluating the Robustness of Neural Networks. In Proceedings
of IEEE Symposium on Security and Privacy (S&P), 2017.
Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. Adversarial classi-
fication. In Proceedings of ACM International Conference on Knowledge Discovery and Data
Mining (KDD), 2004.
L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. A Rotation and a Translation Suffice:
Fooling CNNs with Simple Transformations. ArXiv e-prints, 2017a.
L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. A Rotation and a Translation Suffice:
Fooling CNNs with Simple Transformations. ArXiv e-prints, 2017b.
K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and
D. Song. Robust Physical-World Attacks on Deep Learning Models. ArXiv e-prints, 2017.
I.	J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial Examples. In
Proceedings of International Conference on Learning Representations (ICLR), 2015.
R. Huang, B. Xu, D. Schuurmans, and C. Szepesvari. Learning with a Strong Adversary. ArXiv
e-prints, 2015.
P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-Image Translation with Conditional Adver-
sarial Networks. ArXiv e-prints, 2016.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer
networks. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2015.
Y. Ji, X. Zhang, and T. Wang. EagleEye: Attack-Agnostic Defense against Adversarial Inputs. ArXiv
e-prints, 2018.
J.	Johnson, A. Alahi, and L. Fei-Fei. Perceptual Losses for Real-Time Style Transfer and Super-
Resolution. ArXiv e-prints, 2016.
H. Kannan, A. Kurakin, and I. Goodfellow. Adversarial Logit Pairing. ArXiv e-prints, 2018.
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. ArXiv
e-prints, 2016.
Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
9
Under review as a conference paper at ICLR 2019
D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Math.
Program. ,45(3):503-528,1989.
Y. Liu, X. Chen, C. Liu, and D. Song. Delving into Transferable Adversarial Examples and Black-
box Attacks. ArXiv e-prints, 2016.
Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of ACM International
Conference on Knowledge Discovery and Data Mining (KDD), 2005.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards Deep Learning Models
Resistant to Adversarial Attacks. In Proceedings of International Conference on Learning Rep-
resentations (ICLR), 2018.
Dongyu Meng and Hao Chen. Magnet: A two-pronged defense against adversarial examples. In
Proceedings of ACM SAC Conference on Computer and Communications (CCS), 2017.
S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. DeepFool: a simple and accurate method to
fool deep neural networks. In Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2015.
Blaine Nelson, Benjamin I. P. Rubinstein, Ling Huang, Anthony D. Joseph, Steven J. Lee, Satish
Rao, and J. D. Tygar. Query strategies for evading convex-inducing classifiers. J. Mach. Learn.
Res., 13:1293-1332, 2012.
N. Papernot, P. McDaniel, and I. Goodfellow. Transferability in Machine Learning: from Phenom-
ena to Black-Box Attacks using Adversarial Samples. ArXiv e-prints, 2016.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Anan-
thram Swamil. The limitations of deep learning in adversarial settings. In Proceedings of IEEE
European Symposium on Security and Privacy (Euro S&P), 2016a.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as
a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE
Symposium on Security and Privacy (S&P), 2016b.
A. Raghunathan, J. Steinhardt, and P. Liang. Certified Defenses against Adversarial Examples. In
Proceedings of International Conference on Learning Representations (ICLR), 2018.
K. Reddy Mopuri, U. Garg, and R. Venkatesh Babu. Fast Feature Fool: A data independent approach
to universal adversarial perturbations. ArXiv e-prints, 2017.
Leonid I. Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal
algorithms. Phys. D, 60(1-4):259-268, 1992.
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Accessorize to a crime:
Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of ACM SAC Con-
ference on Computer and Communications (CCS), 2016.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
Properties of Neural Networks. In Proceedings of International Conference on Learning Repre-
sentations (ICLR), 2014.
F. Tramer, A. Kurakin, N. PaPemoL I. Goodfellow, D. Boneh, and P. McDaniel. Ensemble Adver-
sarial Training: Attacks and Defenses. In Proceedings of International Conference on Learning
Representations (ICLR), 2018.
E. Wong and J. Zico Kolter. Provable defenses against adversarial examPles via the convex outer
adversarial PolytoPe. In Proceedings of IEEE Conference on Machine Learning (ICML), 2018.
C. Xiao, J.-Y. Zhu, B. Li, W. He, M. Liu, and D. Song. SPatially Transformed Adversarial ExamPles.
In Proceedings of International Conference on Learning Representations (ICLR), 2018.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examPles in deeP
neural networks. In Proceedings of Network and Distributed System Security Symposium (NDSS),
2018.
10
Under review as a conference paper at ICLR 2019
T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View Synthesis by Appearance Flow. ArXiv
e-prints, 2016.
Appendix
6.1	Notations and Symbols
Symbol	Description
x	genuine input
X	adversarial input
f(∙)	classifier
r	flow vector
g(r; x)	spatial transformation r on X
∆f (x, r)	margin between f ◦ g(r; x) and f (x)
Vf (∙)	gradient of f
Dg(∙)	Jacobian matrix of g
π2(∙)	L projection ∏2(z) = k^
∏∞(∙)	L∞ projection π∞(z) = kzk∞
Table 1: Symbols and notations.
6.2	Proofs of Propositions, Lemmas, and Theorems
(Unless noted otherwise, We use ∣∣∙∣∣ to indicate L2 norm.)
Proof of Proposition 3
Proposition 3. The extreme values of Eqn (4) are achieved when r(u) = r(v).
Proof. (Proposition 3) Consider the first-order approximation of Eqn (4). Recall that r = [r(u), r(v)]
and notice that Vg(0) = [x, x] according to Eqn (2.2). We can rewrite the above expression as:
'adv(x,r)	= tr ((diag(Vf(X)) [x,x])b(u),r(v)])
ʃ √r(u)>M-2r(u) + r(v)>M2r(v) ≤ e
s.t.	kxk∞≤1
Due to the symmetry of r(u) and r(v), the extreme values of ∆f(x, r) is attained when r(u) = r(v).
□
Proof. (Proposition 1)
We derive the following matrix form for Eqn (2):
'diff (r) =	2r>	E	(I - M⑷)>(I - M⑴)r	(9)
i∈{t,b,l,r}
where I denotes the identity matrix, and tr is the trace operator. We have the following proposition
to simplify the notation (proof in Appendix).
We first prove that Pi∈{l,r,t,b}(I - M(i))> (I - M(i)) is diagonally dominant. It suffices to show
that each matrix (I - M(i))>(I - M(i)) is diagonally dominant. Without loss of generality, assume
M(i) corresponds to the top neighbor matrix.
Let A = I - M(i). Consider the i-th pixel. We differentiate three cases: (i) if i is on the top
boundary, it must be the top neighbor of another pixel j ; (ii) if i is on the bottom boundary, it must
be the bottom neighbor of another pixel k; (iii) otherwise, i is the top neighbor of some j and the
bottom neighbor of some k.
11
Under review as a conference paper at ICLR 2019
Consider the i-th column of A. We have: (i) Aii = 0, Aji = -1, and Ari = 0 for r 6= i, j; (ii)
Aii = 1, Ari = 0 for r 6= i; (iii) Aii = 1, Aji = -1, and Ari = 0 for r 6= i, j. Now we compute
Z = A>A. First consider the diagonal element Zii: (i) Zii = 1; (ii) Zii = 1; and (iii) Zii = 2.
Then consider the off-diagonal element Zij: (i) if i is j’s top neighbor, then Zij = -1, and Zij = 0
otherwise; (ii) if i is j’s bottom neighbor, Zij = -1, and Zij = 0 otherwise; (iii) if i is j’s top
neighbor or bottom neighbor, Zij = -1, and Zij = 0 otherwise.
In all the cases, Zii ≥ 0 and |Zii| ≥ Pj6=i |Zij |, i.e., Z is diagonally dominant with real non-
negative diagonal elements. Thus, Pi∈{l,r,t,b}(I-M(i))>(I-M(i)) is positive semidefinite. Using
the spectral decomposition, we can rewrite it as: Pi∈{l,r,t,b}(I -M (i))>(I -M (i)) = M>M, where
M is a square matrix of the same rank as Pi∈" r tb}(I - M(i))>(I - M(i)).	口
Proof of Theorem 2
Proof. Recall that kMrk = defines an ellipsoid. It is straightforward to see that the supremum
∙-v
of ∆f (x, r) = v> diag(x)r is attained only if r lies on the ellipsoid boundary. Otherwise, suppose
∙-v
r0 = arg maxr ∆f (x, r) and kMr0k < . Note that f(x, r) ≥ 0. We may extend r0 to the ellipsoid
∙-v
boundary to further increase ∆f (x, r). Thus, we only need to consider the case of kMrk = . For
convenience, we rewrite the ellipsoid in the following form: r = M-1s with ksk = .
Thus, supχ,r /adv(x, r) = suPχ,s v> diag(x)M-1s, which represents the maximum projection of S
on M-1 diag(x)v. Note that s lies on the ball of ksk = and that M-1 diag(x)v = M-1 diag(v)x.
We have the formulation as follows:
sup	/adv(x, r) = C sup ∣∣M-1 diag(v)x∣∣	(10)
kxk∞≤1,kMrk≤	kxk∞≤1
which is attained by s = Cπ2 (M-1 diag(v)x).
It is noted that without the constraint of ∣x∣∞ ≤ 1, the supremum above is attained by assigning x
the primary eigenvector v* of diag(v)M-2 diag(v). We thus set X = π∞(v*), which leads to:
∙-v
SUp	'adv(x,r) ≥ C∣MT diag(v)π∞(v*)k
kxk∞≤1,kMrk≤
□
Proof of Theorem 1
∙-v
Proof. Similar to the proof of Theorem 2, it is easy to see that the supremum of ∆f (x, r) is attained
only if r lies on the ellipsoid boundary of ∣Mr∣ = C. For convenience, we redefine the ellipsoid as:
r = M-1s with ∣s∣ = C.
According to the definition, we have:
SUP	2adv(x,r)=	SUP	(Vσ)> diag(v)W diag(x)M-1s
kxk∞≤1,kM rk≤	ksk≤,kxk∞≤1
As s lies on the ball of ∣s∣ = C, for fixed x, we have:
sup	/adv(x,r) = c sup ∣∣M-1 diag(x)W> diag(v)Vσ∣
kxk∞≤1,kMrk≤	kxk∞≤1
which is achieved by s = Cπ2 (M-1 diag(x)W diag(v)Vσ).
We then define the following auxiliary function:
φ(x) = 1∣MT diag(x)W> diag(v)Vσ∣2
Recall that in the case that σ is ReLU, (Vσ)i = 1 if (W x)i > 0 or (Vσ)i = 0 otherwise. Observe
that φ(x) has the absolute lower bound of0, which can be achieved when x = 0, i.e., all the neurons
in the first layer are inactive. We attempt to increase φ(x) by maximizing the number of active
neurons.
12
Under review as a conference paper at ICLR 2019
For a practical neural network f, we may consider each column of W as a basis function. We thus
assume W has full column rank, i.e., rank(W) = d. Therefore, by properly setting x, we may
make at least d dimensions of Wx larger than 0. Let Wd be the submatrix of W that consists of d
independent rows of W. We consider x* = ∏∞(W-11) where 1 is an all-one vector. It is easy to
verify that (i) Wdx* > 0, i.e., Vσ(WdX*) = 1 and ∣∣xok∞ ≤ 1. That is, x* is a valid input and
activates at least d neurons in the first layer of the network. We have the following lower bound:
∙-v	__
sup	'adv(x, r) ≥ EkMT diag(x*)W> diag(v)Vσ(Wx*)∣.
kxk∞≤1,kMrk≤
□
6.3	Results for Linear Classifiers
6.4	Impossibility Results
In the case of single-layer neural network (i.e., linear classifier), we have f(x) = v>x, where v is
the weights of the network. In this case, we have the following result:
`adv
(x, r) =
0
v>Dg(zr; x)rdz
(11)
To find x* and r*, we consider a linear approximation of `adv (r; x):
?adv(x, r) = v>Dg(0; x)r = v> diag(x)r
where Dg(0; x) = diag(x) according to Eqn (1). Theorem 2 finds a lower bound of the supremum
∙-v
of `adv (x, r) (proof in appendix).
Theorem 2. For a single-layer network f, we have
sup	2adv(x, r) ≥ EkM-1 diag(v)x* k
kxk∞≤1,kMrk≤
where x* = π∞(v*) and v* represents the primary eigenvector of diag(v)M -2 diag(v). The bound
is attained when
r* = EM-1π2(M-1 diag(v)x*).	(12)
While Theorem 2 finds high-quality setting of (x*, r*), we may further increase the adversarial loss
by approximating Eqn(11). Let r(i) = 2i-1 r. We compute the midpoint approximation ofEqn(11):
1n
'adv(x,r) ≈ - £v>Dg (r(i); x) r	(13)
n i=1
|--------{z-------}
Summation (i)
where we divide the interval [0, 1] into n subintervals.
Updating x*
We first update x* to maximize Eqn (13) With fixed r. To this end, we consider x as a sym-
bolic vector in Eqn (13). Let αi be the coefficient of x’s i-th element in Eqn (13). We set
x* = [sign(α1), sign(α2), . . . , sign(αd)]>, which maximizes Eqn (13) under fixed r and the con-
straint of kxk∞ ≤ 1.
Updating r*
Let rk be the setting of r* at the k-th iteration. With x* fixed, we may update r* as follows:
rk+ι = EM-1∏2 (MT X Dg (rki); x*) V)	(14)
Intuitively, to maximize Eqn (13), we find rk+1 that aligns with the direction of the summation (i) in
Eqn (13) under kM rk+1 k ≤ E.
13
Under review as a conference paper at ICLR 2019
Figure 7: Bounds of diag(s)r, where the dashed area corresponds to diag(s)r that satisfies ||Mr|| ≤
e and ∣∣s∣∣∞ ≤ L
Attack
Algorithm 2 sketches the attack against a given single-layer network. After initialization (line 1-3),
it updates x* and r* in an interleaving manner (line 4-7).
Algorithm 2: Attack against single-layer networks.
Input: v: fs weights; e: threshold of perturbation magnitude; M: neighboring matrix; n:
parameter of midpoint approximation
Output: x*: genuine input; r*: spatial transformation
// initialization
ι v* J primary eigenvector of diag(V)M-2 diag(v);
2	X* - ∏∞(V*);
3	set r* according to Eqn(12);
4	while not converged do
// refinement of x*
5	compute Eqn (13) with symbolic x;
// αi:	coefficient of x's i-th dimension in Eqn (13)
6	X* J [sign(αι), sign(a2),..., sign(ad)]>;
// refinement of r*
7	update r* according to Eqn (14);
8	return (x*,r*);
6.5	Possibility Results
In the case of single-layer network (i.e., linear classifier), we have:
'adv(x, r)	≤
max v>Dg(zr; x)r
z∈[0,1],∣∣Mrk≤e	'	，
≤ 2 max	v> diag(s)r
一	∣∣s∣∣∞ ≤1J∣Mrk≤e	' '
(15)
where the last inequality follows from the fact that as defined in Eqn (1), under bilinear interpolation,
the diagonal elements of Dg(zr; x) vary within the interval of [-2, 2].
Let λ = [λ1, λ2, . . . , λd]>. We have the following derivation of Eqn (15) (note that we re-
parameterize r with t):
`adv (x, r)	≤
2 max ev> diag(λ)t
ktk∞≤1
2e kv> diag(λ)k1
(16)
where the upper-bound is attained when t = sign(v> diag(λ)).
Note that this upper bound is irrelevant to the original input x and solely dependent on the network’s
properties (i.e., v and λ). Thus, it only needs to be computed once for a given network.
14
Under review as a conference paper at ICLR 2019
6.6	Extension to Multi-Class Cases (Impossibility)
We can extend Algorithm 2 and Algorithm 1 for a K-class classifier f (∙). We Considerthe CW
attack and take κ = 0:
'adv(r) = maxσy(g(r; x)) - σy(g(r; x))	(17)
For the most popular gradient-based adversarial attack methods, their update rules of r at t-th itera-
tion are:
rt+1 = Irt - α^∣0dv (r)	(18)
∂r
where α is learning rate. Let y? such that argmaxy=y σy (g(r; x)) = y?, then according to the chain
rule, We have d maxy=y σ0g (r; x)= 4；09 (r; x). Then We derive a simple work around: at each
iteration, we only update based on the two classes case between class y?(-) and y(+).
6.7	Extension to Multi-Class Cases (Possibility)
We extend the proposed SDP training objective to multiple classes case. Here we only consider a
two-layer neural network, it is straightforward to apply the extension to a one-layer network. Similar
to Eqn (9), we define:
ij def	0
diag(λ)W > diag(vi - vj)
diag(vi - vj)W diag(λ)
0
for all pair of classes i(+),j(-). Then we have
'ldv(x,r) ≤
max	tr(ZPij) d=ef qij(x)
diag(Z)≤1,Z0
For an image X with class y, the attack is failed if and only if maxi=y 'Odv(x) ≤ 0. Therefore, we
take maxi6=y qiy(x) as the upper bound.
6.8	Additional Experimental Results
Implementation details
For the empirical evaluation (ii), we follow the experimental setup in Raghunathan et al. (2018) and
evaluate the part with a two-layer network f with 500 hidden units. We construct a set of variants
of f using different training methods. We report our training objectives and details below:
1.	fnt - f is normally trained, with cross-entropy loss and without explicit regularization.
2.	ffro — f is trained with Hinge loss Frobenius norm regularization (Fro-NN) and
λ(kWkF + kvk2) with λ = 0.08.
3.	fspe — f is trained with hinge loss and a regularizer λ (k W∣∣2 + ∣∣v∣∣2) with λ = 0.09.
4.	fstn— f is trained with hinge loss and a three layer spatial transformer network with 100
hidden units processes input images before feeding them to classification network.
5.	fat - f is trained with cross-entropy loss with an adversarial training regularization against
spatial transformed adversarial examples. We take regularization coefficient λr = 0.3 for
training and λ = 0.05 for regularizing the `diff. Due to Xiao et al. (2018) taking L-BFGS
for solving the optimization problem, it is very slow to generate adversarial examples on
the fly. Instead, we generate a pool of adversarial examples every k = 3 epochs, and sample
adversarial example from pools during training.
6.	fsdp - f is the upper bound we derived in Eqn (9), and we follow the implementation
of SDP-NN in Raghunathan et al. (2018). In particular, we also optimize the dual form
of Eqn (9) so that we can backpropagate through both the classification loss and the SDP
regularization term.
The other naive bounds used in the § 4:
15
Under review as a conference paper at ICLR 2019
2. FrobeniUS bound
0 8 6 4 2
1 Ss-一BMBSJ①APa
2
4
6
8
Transformation threshold €
Figure 8: Lower bounds estimated with respect to different networks.
1. Spectral bound
'diff(x,r) ≤ EkWk2kvk2
'diff(x,r) ≤ EkWkFkvk2
e = 0.05
6 = 0.5
6 = 5
Adversarial Input
Benign Input
Figure 9: Samples of benign and adversarial inputs found by Algorithm 1 under frd .
Flow Vector
16
Under review as a conference paper at ICLR 2019
e = 0.5
Benign Input	Adversarial Input	Flow Vector
If
Figure 10: Samples of benign and adversarial inputs found by Algorithm 1 under fat.
Transformation threshold €
(a)	(b)
Figure 11: Upper bounds on 0/1 adversarial loss for different networks on MNIST.
17