Under review as a conference paper at ICLR 2019
Mental Fatigue Monitoring using
Brain Dynamics Preferences
Anonymous authors
Paper under double-blind review
Ab stract
Driver’s cognitive state of mental fatigue significantly affects driving performance
and more importantly public safety. Previous studies leverage the response time
(RT) as the metric for mental fatigue and aim at estimating the exact value of RT
using electroencephalogram (EEG) signals within a regression model. However,
due to the easily corrupted EEG signals and also non-smooth RTs during data
collection, regular regression methods generally suffer from poor generalization
performance. Considering that human response time is the reflection of brain dy-
namics preference rather than a single value, a novel model called Brain Dynamics
ranking (BDrank) has been proposed. BDrank could learn from brain dynamics
preferences using EEG data robustly and preserve the ordering corresponding to
RTs. BDrank model is based on the regularized alternative ordinal classification
comparing to regular regression based practices. Furthermore, a transition ma-
trix is introduced to characterize the reliability of each channel used in EEG data,
which helps in learning brain dynamics preferences only from informative EEG
channels. In order to handle large-scale EEG signals and obtain higher gener-
alization, an online-generalized Expectation Maximum (OnlineGEM) algorithm
also has been proposed to update BDrank in an online fashion. Comprehensive
empirical analysis on EEG signals from 44 participants shows that BDrank to-
gether with OnlineGEM achieves substantial improvements in reliability while
simultaneously detecting possible less informative and noisy EEG channels.
1 Introduction
As reported by sleep health report (Adams et al., 2017), mental fatigue is a major cause in 33% -
45% of all road accidents. In general, mental fatigue (Boksem & Tops, 2008) refers to the inability to
maintain optimal cognitive performance in continuous task of the high demand of cognitive activity.
Such inability in the context of driver could lead to accidents with severe consequences. Individuals
may find themselves in a mental fatigue state because of lack of sleep, continuous driving for long-
time, midnight driving, monotonous driving, and driving during the influence of sleeping drugs or
sleep disorders (Ji et al., 2004; Ting et al., 2008).
In response to these critical issues, several methods (Jap et al., 2009; Wascher et al., 2014; Cook
et al., 2007; Lal et al., 2003; de Naurois et al., 2017) have been proposed to estimate and predict
the mental fatigue based on EEG and RT (Fig. 1a). Some of these methods, however, performed
considerably well for some participants but failed for others due to lack of robustness. There are
several challenges behind such instability and one of such problem is how to use RT effectively. RT
is the most resourceful piece of information to predict mental fatigue. However, it is easily affected
by the instrumental error, mind wandering or any other task non-related factor. A previous study
(Wei et al., 2015) tried to overcome this problem by providing different techniques to adjust RT, they
also tried removing outliers nevertheless, failed to make it work for all participants. The regression
assumption of this method between EEG signals and RT is not correct. Human’s RT is usually the
result of preference (Izuma & Adolphs, 2013) in brain dynamics during the task, than just a single
value. These preferences of human can be affected by different cognition (MockeI et al., 2015) like
mind wandering (Lin et al., 2016), and/or lower level of attention (Chuang et al., 2018). Therefore,
the relationship between EEG and RT including the extreme/abnormal RT should be taken care in
the way that reflect human brain dynamics preferences by the developed technique itself.
1
Under review as a conference paper at ICLR 2019
EEG Signals
Response
Time
Triali
⅝λ⅛vM内
I
RTt
Trialj
川jjV∣⅛⅛
I
RTJ'
Brain Dynamics
Preferences
Pairwise RT
Comparison
Triali	Trialj
下(	,	)
1
RTi > RTj
(a) Regression
(b) BDrank
Figure 1: (a) Regression model with EEG signals. (b) BDrank model with brain dynamics prefer-
ences.
Another important problem is related to different brain regions, which are normally responsible for
different functionalities. There was an attempt to choose different brain regions (Wascher et al.,
2014) for a method during mental fatigue prediction but these regions of the brain are not necessary
same for all participants (Gramann et al., 2006). For example, existing work (Wascher et al., 2014)
used frontal theta to represent a different level of mental fatigue for all participants. In such case
learning model’s reliability would inevitably degrade because of possibly noisy channels chosen, on
different brain regions, by the method. Some previous work (de Naurois et al., 2017), attempted
to solve this issue using artificial neural network models but still failed to provide convincing re-
sults. Again these mentioned work reflect developed methods should be based on brain dynamics
preferences rather than fix model or regions of brain.
To overcome the above-mentioned problems, a new approach has been proposed. We called it
BDrank such that brain dynamics ranking. This approach not only learns from brain dynamics
preferences for mental fatigue but also other cognitive states (Lal et al., 2003), while effectively
preserving the exact ordering of RT (Fig. 1b). This approach surprisingly improved over defects of
previous models and their performance due to noisy and extreme RT. Furthermore, BDrank model
also proposes to use transition matrix to evaluate the high confidence BDrank (HC-BDrank) sources
among different EEG channels, which contributes highly toward task performance. In order to
handle large-scale EEG signals and obtain higher generalization, an online-generalized Expectation
Maximum (OnlineGEM) (Roche, 2011) algorithm also has been proposed. Comprehensive empiri-
cal experiments on EEG signals from 44 participants show that BDrank model together with Onli-
neGEM algorithm delivers substantial improvements and robust performance while simultaneously
providing the information about noisy or less informative EEG channels.
2 Materials and Methods:
a)	Experiment Paradigm: This
paper utilized the 33-channels EEG
data recorded in the previous study
(Huang et al., 2015) from 44 adult
participants while performing long
sustained attention task. The ex-
periment has been conducted us-
ing a virtual-reality (VR) dynamic
driving simulator (Fig. 2D-E). The
task involves driving on the four-
lane highway while lane-departure
events were randomly induced de-
viation toward the side of the
road from the original position.
Each participant was instructed to
quickly respond to steer back to
original position. A complete trial
Figure 2: Sustained-attention driving task
in this study (Fig. 2A), includes 10s baseline, deviation onset, response onset, and response offset
(Fig. 2B-C). The next trial occurs within an interval of 5-10s after finishing the current trial. Each
participant completed T trials within 1.5h. For each trial i, the EEG signals {xn,i}nN=1 from N dif-
ferent channels were recorded simultaneously and the corresponding response time RTi was also
collected afterwards. If a participant fell asleep during the experiment, there was no feedback to
wake him up.
2
Under review as a conference paper at ICLR 2019
The response time is an intuitive indicator used to assess human mental fatigue. Therefore, the
common practice for mental fatigue monitoring is to find a robust mapping for humans’ response
time (RT) to an emergent situation using the EEG signals recorded beforehand. The natural way
to forecast the response time with the EEG signals is to formulate it as a regression task (Fig. 1a),
namely finding a nonlinear mapping (e.g. neural network, SVR) from the EEG signals x to the cor-
responding RT. However, due to the easily corrupted properties of the EEG signals and the existence
of extreme values in response times during data collection, focusing on predicting the exact value of
a noisy and non-smooth measurement (the response time) is easier to create a near-perfectly fitted
model with poor generalization performance (See Table 1 and Fig. 3 for more details). This creates
a dilemma: it requires a powerful learning model to predict response time with the complex EEG
signals (indeed, it is exactly our target) but it is not so significant to excessively approximate the
exact value of response time, especially the extreme values.
Here comes the problem, how to find an efficient way to learn from the noisy response time while
the exact value is not so significant? Actually, the RTs are defined in the totally ordered space R. The
totally ordered space owns its structure meanings, which are preserved by the pairwise comparisons
between the RTs. The pairwise comparisons indeed preserve the whole relative structure informa-
tion between the response times while ignoring their absolute numerical information. Therefore,
predicting the orderings of the pairwise comparisons can be accepted as a regularized alternative of
previous regression model. Further, it makes it more flexible to consider more powerful learning
unit in the proposed model.
Therefore, there is no longer requirement to estimate the RT with a regression model. Instead, the
proposed approach will transform it into an ordinal classification problem, and focus on correctly
preserving the whole orderings between the pairwise RT comparisons (Fig. 1b). First, the preference
propositions1 could be constructed as follows,
D1 = {ρm}mM=1 1 = {RTm,1 >RTm,2}mM=11,	D2 = {ρm0}mM02 *=1 = {RTm0,1 ≈RTm0,2}mM02=1.	(1)
where M1 and M2 denote the number of type-1 and type-2 preference propositions. D1 denotes the
type-1 preference propositions that the orderings between the RTs are significant; D2 denotes the
type-2 preference propositions that the RTs in each comparison are comparable. Then, the brain
dynamics preference was constructed for each proposition with the corresponding pairwise EEG
signals recorded from each channel. For brevity of notations, new index (the notation in Eq.1)
is adopted in the following, instead of the original index used in sustained-attention driving task
(Fig. 1). Namely, the m-th proposition ρm ∈ D1 SD2 denotes the pairwise comparison RTm,1 > RTm,2
or RTm,1 ≈ RTm,2. And the pairwise brain dynamics preference (xn1,m, xn2,m) denotes the features
recorded within the n-th channel for each preference proposition Pm ∀m = 1,2, •…，Mι + M2.
In this paper, the 10s baseline (Fig. 2B) as the feature vector has been adopted, which is assumed
to be long enough to detect any significant changes in brain activity (Zhang, 2000). This followed
by exploring the relation between the 10s baseline x (∈ Rk) and the preference proposition ρm under
the following four assumptions: (1) different participants are independent during the data collec-
tion process; (2) Different EEG sensors used for recording are recorded independently from scalp
without influencing other sensors (Homan et al., 1987; Teplan et al., 2002); (3) Different trials are
conducted independently during the data collection process; (4) The collected response time are
slightly corrupted by inherent (basically irremovable) sources of noise, but the ranking relationships
are preserved to some extents.
b)	Brain Dynamics Preferences: Again, instead of modelling the dependence between the RT and
the 10s baseline as a regression problem, we aim to predict the orderings of the pairwise comparisons
using the brain dynamics preferences, namely f(x1,x2) → ρ. Further, a preference proposition ρ has
three states: 1, 0, -1, denoting win (RT1 > RT2), tie (RT1 ≈ RT2) and loss (RT1 < RT2), respectively.
However, classical classification models, e.g. logistic ordinal regression (Harrell, 2001), are infea-
sible for our problem, due to the lack of a normalized probability definition for three states. Since
two types of preference propositions D1 and D2 are considered in our problem, we tailor-define
a normalized probability definition, namely first normalizing the probabilities of the states (1, -1)
1We used the term “preference” intentionally to show that brain dynamics keep changing w.r.t. human
behaviours and it happens because the human brain prefers one decision over others. Therefore, we prefer to
call it“preference” than “classification”.
3
Under review as a conference paper at ICLR 2019
(exclusively to D1) to 1, then generalize the probability definition to state 0. To be specific, it can be
mathematically formulated as follows,
(σ (wτ ∆x )[1 — κ (wτ ∆x)]	P = 1
κ (wT ∆x)	ρ = 0
σ (-wT ∆x)[1 - κ (wT ∆x)] ρ = -1
(2)
where σ(z) = 1/(1 + e-z) is the sigmoid function and σ (-z) = 1 - σ (z). The symbol ∆x denotes
the subtraction (x1 -x2) between the brain dynamics preferences (x1,x2). Following Weng & Lin
(2011), the probability of a tie is modelled as the geometric mean between a Win and a loss, namely
K (wT ∆x) = σ σ (wτ ∆x) [1 — σ (wτ ∆x)].
c)	Preference State Transition: Furthermore, considering the different functions of different re-
gions in the human brain, the relative contributions of different channels to human response time
may vary a lot. For example, the information conveyed by positive channels is positive related to the
RT, while negative channels may convey the information which is negative related to the RT. There
are also some noisy (non-relevant) channels which are independent to the learning task. Therefore,
if we directly model the EEG preferences recorded in each channel without making any distinctions
about the channel state (i.e. positive, noisy and negative), the model’s reliability would inevitably
degrade. Note that a channel is called as “noise” if the current algorithms could not extract useful
brain information with EEG signals from this channel (Alharbi, 2018; Lin et al., 2018).
In the following, a transition matrix Πn is introduced to characterize the reliability of each channel n
w.r.t the corresponding task. Let ρ denote the preference proposition and ρ (n) denote the prediction
from the n-th channel. They are defined on the finite state space S = {1, 0, —1}. Then we have
∏n	0	(1 - ∏n )
∏ n = P (P ∣P S ))=	0	1	0
(1 -πn )	0	πn
(3)
wherePi,j(P|P(n)) =P(P = Sj|P(n) =Si). Note that we do not consider the transition between the type-
1 and type-2 preference propositions (i.e. P(P = {1, -1}|P(n) =0) = 0 and P(P = 0|P(n) = {1, -1}) = 0),
since the equal cases are hard to measure when do prediction. A promising approach to generalize
the transition matrix Πn (Eq.3) is to introduce the concept of the confidence region to measure the
equal cases (Pregibon et al., 1981).
Remark: The parameter πn in the transition matrix Πn actually indicates the reliability of the n-th
channel ∀n = 1,2, •…，N. It additionally helps to divide the channels into three states: (1) positive
channels with πn close to 1, the ranking model (Eq.2) can extract enough information from the
n-th channel, and exactly predict the state of the preference proposition. (2) Noisy channels with
πn approximating to 0.5, the ranking model can not extract any useful information from the n-th
channel. (3) Negative channels with πn close to 0, the ranking model can extract enough information
from the n-th channel, but the prediction states are exactly opposite to the proposition states. The
identified positive and negative channels are all considered as informative EEG channels, which
helps in learning reliable models for the corresponding task.	■
With the introduced transition matrix Πn , the marginal likelihood function for each proposition P
can be further represented as P(P) = EP(n) P(P|P(n))P(P(n)). Specifically,
{[∏nn σ (WT ∆x ) + (1 - ∏n )σ (-WT ∆x )][1 - K (WT ∆x n, m )] P = 1
κ (wT ∆xn,m)	P = 0	(4)
[(1 - πn)σ (WT ∆x) + πnσ (-WT ∆x)][1 - K (WT ∆xn,m)] P = -1
For sake of simplicity, the subscripts (n, m) are omitted. Let D = D1 SD2 denotes all the preference
propositions and X represents the recorded EEG signals from N different channels. We further
extend our robust ordinal classification (Eq.4) for BDrank into the Bayesian version. A Gaussian
prior is introduced for W (i.e., W 〜N(μ, Σ)). Since the transition matrix Πn only depends on the
parameter ∏, we only focus on estimate the parameter ∏ ∀n = 1,2, •…，N in the following. Let π
denote {πnn}N=1, and we introduce a Beta prior for each πnn (i.e., π 〜B(α,β) = ∏N=1 B(αn,βn)). Then,
4
Under review as a conference paper at ICLR 2019
our BDrank can be formulated into a maximum a posteriori (MAP) estimate as follows,
argmax Pι(∏) Po( W) P(D ∣∏,% X )= Pι(∏) P√ W) P (Dι∣∏, ^, X) P(D2∣π,* X)
π,w
N	M1	M2
=Pθ(∏ ) Pθ( W ) ∏ [∏ P (P m = 1∣∏n, W, ∆x % m ) ∏ P (pm 0 = 0∣∏n , W, ∆x% m 0 )J
n=1 m=1	m0=1
(5)
N	M1	M2
=B (π ∣α, β) N (W ∣μ, Σ)∏∏([πn σ (WT ∆x ) + (1 - πn )σ (-WT ∆x )][1 - K (WT ∆x n, m )]) ∏ K ( WT ∆x n, m 0 )J.
n=1 m=1	m0=1
Note that due to the symmetry of the state probability (Eq.2) and transition matrix (Eq.3) w.r.t. state
1 and -1, the resultant marginal likelihood (Eq.4) remains symmetry w.r.t. state 1 and -1. For
simplicity, D1 is constructed using the preference propositions with state ρ = 1 only. M1 and M2
denote the number of type-1 and type-2 preference propositions, namely |D1 | = M1 and |D2| = M2.
The variable n iterates over the channels. m and m0 iterate over two types of preference propositions,
respectively. Now our aim is to estimate the model parameters (W and π) by maximizing Eq.5. In
principle, any solution strategies for MAP can be considered to solve this problem. See Section 3
for optimization details.
d)	Reliability analysis and channel state estimation Note that our BDrank (Eq.5) indeed trains
a mixture of two complementary classifiers, which share the same parameter W. It is different from
classical mixture models, since it clusters at the channel level instead of the sample level. In particu-
lar, in terms of the positive channels with πn close to 1, BDrank relies on the first classifier to update
the shared parameter W. In terms of the negative channels with πn close to 0, Eq.5 automatically
switches to the opposite classifier which can extract correct information from the negative channels
and update the shared parameter W accordingly. Further, BDrank is robust to the noisy channels with
πn approximating to 0.5, because Eq.5 gives up extracting information from the noisy channels by
assigning a constant likelihood (i.e. 0.5) to each brain dynamics preference. Note that the estimated
πn can be leveraged as the indicator to detect noisy channels with πn ≈ 0.5,∀n = 1,2, •…，N. See
Fig. 4 in the experimental section for more details.
3 Optimization Methods
In this section, we describe a generalized Expectation-Maximization (EM) algorithm (Dempster
et al., 1977) to solve the proposed BDrank (Eq.5). Since the feasible region of πn is restricted to
[0, 1], the gradient-based optimization methods would make our solution inaccurate and inefficient.
The EM algorithm is an efficient iterative procedure to compute the MAP problem in presence of
latent variable (ρm(n) in Eq.5). EM avoids calculating the derivative to the expectation of the latent
variable directly, and resorts to a surrogate lower bound to optimize. Therefore, EM, a silver bullet
for MAP with latent variable, can significantly simplify the optimization over parameter πn for Eq.5.
3.1 Generalized EM for BDrank
For each type-1 proposition ρm, we introduce an auxiliary variable δm(n)(∈ {1, 0}) for each channel,
representing the consistency between the preference proposition ρm and the prediction ρm(n) given
by the n-th channel. Specifically,
1 denotes the prediction ρm(n) given by the first classifier
is consistent with the preference proposition ρm, and δm(n) = 0 denotes the prediction ρm(n) estimated
by the second classifier is consistent with the preference proposition ρm . We can therefore find an
equivalent formulation of our BDrank model involving the auxiliary variable Ξ = {δm(n)}mM=1 1.
P (D, Ξ, π, W | X )= P0(π) P0( w ) P (D1, Ξ∣π,W, X) P (D2∣ w, π, X)
N	M1	( )	M2
=P0(π) P)( W ) ∏ [∏ P (Pm = 1, δmn )∣πn , W, ∆x n, m ) ∏ P(Pm m = 0∣πn, W, ∆x n, m，)]	(6)
n=1 m=1	m0=1
N	M1	δ(n)	1 δ(n)	M2
=P0(π)P0(W) ∏ ∏ πnσ(WT∆xn,m) m (1 - πn)σ (-WT ∆xn,m) - m [1 - K(WT∆xn,m)] ∏ K (WT ∆xn,m0) .
n=1 m=1	m0=1
5
Under review as a conference paper at ICLR 2019
Now, we can deal with the joint distribution directly, which leads to significant simplifications for
optimization. The complete log likelihood can be written as
N M1	N M2
log P (D, Ξ, π,M X )= log Po(π)+ log p√ W)+ ∑ ∑ log[1 - K (wτ ∆x% m)]+ ∑ ∑ log κ (wτ ∆x% m 0)	(7)
n=1 m=1	n=1 m0=1
+
N M1	( )	( )
∑ ∑ δm logπnσ(wT∆xn,m)+ (1 -δm ) log(1 -πn)σ(-wT∆xn,m) .
n=1 m=1
Expectation Step In the expectation step, we first calculate the expected value of the auxiliary
variable δ,(n) w.r.t its posterior distribution P(δm) ∣π,w,Pm = 1,χ*m) ∀n = 1,2,…，N,∀m = 1,2,…，Mɪ:
E	屈n)] = P(Pm = 1, δm)∣∏n, W,∆xn, m ) =	1
P(δmm)lπ,w,Pm=Ixnm)∣OmP(Pm = 1∣∏n,W,∆xnm)	— 1 ∣	(1-πn)σ(-wTδnm川-K(wTδnm)]
Wm 1 n, ,	n,m	1 + ^^nσ(wτ∆xn,m)iτ-κ(wτ∆xn,m)^
, γ(δm(n)),
(8)
where γ(δm(n)) denotes the degree of the consistency between the prediction Pm(n) and the preference
proposition Pm . Then, the expected value of the complete-data log likelihood function w.r.t the
posterior expectation of the auxiliary variable Ξ = {δm(n)}mM=1 1 can be represented as follows:
N	1	N M1
L (W, ∏) = ∑ [(an- 1) log ∏n + (βn - 1)log(1 - ∏n )] — 万(W - μ ) T Σ-1( W - μ )+ ∑ ∑ log[1 - K (WT ∆x n, m )]
n=1	n=1 m=1
N M2	N M1
+ ∑ ∑ logK(WT∆xn,m0)+ ∑ ∑ γ(δm )logπn+(1 -γ(δm )) log(1 -πn)	(9)
n=1 m0=1	n=1 m=1
N M1	(n)	T	(n)	T
+ ∑ ∑ γ(δm )logσ(WT∆xn,m) + (1 - γ(δm )) log σ (-WT ∆xn,m) + const ant.
n=1 m=1
Maximization Step In the maximization step, we maximize the objective function Eq.9 w.r.t the
model parameters π and W, respectively. In terms of π, we set the gradient of Eq.9 w.r.t πn to zero
and obtain the following estimate for πn :
πneW= ∑M=I Y(⅞m*)) + αn - 1
n	M1+ an + β n - 2	,
n = 1,2,・・・,N.
(10)
In terms of W, due to the complexity of the sigmoid function, we do not have a closed form solution
for W and we need to use gradient-based optimization methods. In the following, we adopt the L-
BFGS to optimize W, the objective function L (W) consists of the second, the third, the fourth and
the sixth parts of Eq.9 and the gradient function g(W) can be represented as follows,
N	M1	1 2σ (WT ∆x )	N	M1
g (W) =	∑ 1 (W	μ) + ∑	∑1	1 - " 1	,一— δxn, m +	∑	∑	(γ(δm(n))	- σ (WT∆xn,m))∆xn,m
n =1 m=1 2(1 - κ(WT∆xn,m) )	n =1 m =1
1 N M2
+ ∑ ∑ ∑ (I - 2σ(w δxn,mO))δxn,m0.	(II)
n=1 m0=1
WneW can be solved with L-BFGS using L (W) and g(W). The EM algorithm then iterates the E-step
and M-step until convergence is achieved.
3.2 Online GEM for BDrank
The generalized EM approach introduced in Section 3.1 is inefficient for large-scale datasets, be-
cause we need to iteratively calculate the gradient with respect to parameters π and W over all
samples during each maximization step. Motivated from the stochastic approximation literature, we
introduce an online-generalized Expectation Maximization (OnlineGEM) approach, which resorts
to stochastic mini-batch optimisation to learn the parameters. To be specific, OnlineGEM approx-
imates the updated π and W in batch EM with a single sample or mini-batch samples. Since a
mini-batch samples cannot be a perfect approximation for the whole dataset, we interpolate between
the new and former estimations with a decreasing step-size ηk2, as in Liang & Klein (2009).
2ηk = (k+ 2)-τ0, where k is the number of iterations and 0.5 < τ0 < 1. The smaller the τ0, the larger the
update ηk, and the more quickly we forget (decay) our old parameters. This can lead to swift progress but also
generates instability.
6
Under review as a conference paper at ICLR 2019
Before the k-th iteration, we randomly downsample a mini-batch Dk from the preference proposi-
tions D. The number of two types preference propositions in Dk are Mɪ and M^, which are much
smaller than the corresponding total size M1 and M2, respectively.
The expectation step remains similar. The difference is that we only need to calculate the posterior
expectation of the auxiliary variable δm(n) over the mini-batch Dk.
In the maximization step, we maximize the objective function, calculated on the mini-batch Dk ,
with regard to model parameters π and w. In terms of parameter πn, since its marginal distribution
belongs to exponential family, we could perform the stochastic update in the space of sufficient
statistics (CaPPe & Moulines, 2009). φ； denotes the noisy estimate of the sufficient statistic for πn.
φn
M ∑ Y(δmn)),
1 m∈Dk
φ n n)=(1 - η k )φnk -1)+η k φn,
∏new = Mφ+α+--2,	n=1,2,…,N
(12a)
(12b)
(12c)
In terms of parameter w, the above practice is infeasible due to its non-exponential marginal distri-
bution. Inspired by the stochastic gradient EM algorithms in CaPPe & Moulines (2009), we perform
the stochastic update in the original space. First, a local optima regression weight w(k) can be
estimated via iterative optimization over the mini-batch Dk, using L-BFGS algorithm. Then we
interpolate between the local optima and former estimations to form a global approximation.
w(k) =L-BFGS(L(w),g(w),Dk)
wnew = (1 - ηk)wold + ηkw(k).
(13a)
(13b)
The convergence issues of the proposed online generalized EM algorithm are the analogues of the
discussion given by CaPPe & Moulines (2009) for their stochastic gradient EM Algorithms. The
existence of such links is hardly surprising. In view of the discussions in Section 3 of CaPPe &
Moulines (2009), the online update rule (Eq.13b) could also be seen as a stochastic gradient descent
formula, namely wnew = wold + ηk (w(k) -wold) .
4	Empirical Analysis
In this section, we demonstrate the reliability of the proposed BDrank (Eq.5) with EEG signals from
forty four participants.
Data Preprocessing: EEG preferences for each participant has been generated as follows: (1) the
trials of each participant were randomly divides into two parts: 50% for training and 50% for
test; (2) EEG preferences were constructed according to the pairwise comparisons between the
RTs. To be specific, the type-1 preference propositions D1 were constructed with RT compar-
isons (RTm,1,RTm,2), which satisfies RTm,2 < min(RTm,2 + τ1 ,τ2 ×RTm,2) < RTm,1; the type-2 preference
propositions D2 were constructed with RT comparisons (RTm0,1,RTm0,2), which satisfies RTm0,2 <
RTm0,1 < min(RTm0,2 + τ3,τ4 × RTm0,2). It is notable that τ1 > τ3 > 0 and τ2 > τ4 > 1 control the differ-
ence in the RT comparisons simultaneously, we empirically set τ1 = 1; τ2 = 1.5; τ3 = 0.8; τ4 = 1.3
for all participants in our experiment setting. Considering the time delay among the channels in the
time domain, Fourier transform (Welch, 1967) has been applied to EEG signals to transform time-
series into frequency domain. Further, to avoid overhead computation, EEG power within 0-30Hz
has been selected, which is considered to be the most relevant to the RTs (Huang et al., 2015).
Baselines and Metrics: We compared BDrank with one regression model Support Vector Regres-
sion (SVR) (Smola & Scholkopf, 2004) and one ordinal classification model Logistics Ordinal Re-
gression (LOR) (Harrell, 2001). First, we aggregate the predictions from different channels using
a simple voting scheme, namely pm = sign(∑N=I P勿) ^1-[τnln > K) — 1(πn < 1 — K)]). P勿) denotes the
predicted state (1 means win and -1 means loss) for (RTm1 , RTm2) by the n-th channel, using the brain
dynamics preference (χ1,m,x2,m). ρ^m is the final estimated order for (RTm,RTm) by aggregating the
predictions Pm(n) over all channels. 1() is an indicator that returns one if the argument is valid and
returns zero otherwise.
7
Under review as a conference paper at ICLR 2019
Then, we introduce two metrics to measure the performance of BDrank model from different per-
spectives. First, we adapted the Wilcoxon-Mann-Whitney statistics (Yan et al., 2003) to evaluate the
accuracy (in %, higher is better) over all pairs, namely 去 ∑M=γ 1(ρm = ρ^m). Further, We investigate
the reliability of BDrank in terms of preserving the global ordering w.r.t RTs. Note that a totally
ordered set could be equally represented by a fully directed graph, Where the fully directed graph
can be further encoded by its degree sequence. Therefore, We first collected the indegree sequences3 *
(Becirovic, 2017) of the constructed directed graph using the predicted RTs and then measured the
discrepancy betWeen the predicted indegree sequences and ground truth using the root-mean-squared
errors (smaller is better). See Supplementary for the detailed description.
Note that We only trust the predictions from informative channels With reliability πn > κ or πn <
1 - κ. κ is set to 0.85 for all participants in our experiment. In terms of SVR and LOR, consider the
scale difference betWeen the EEG signals betWeen different channels, We train a SVR/LOR model
for each channel and aggregate the results from different channels to calculate the final predictions
using the majority voting scheme. Since there is no mechanism for SVR and LOR to evaluate the
channel state, We trust all the channels by default. Further, We only calculate the tWo metrics on the
type-1 preference propositions D1, since the state of the type-2 preference propositions D2 is hard
to evaluate When do prediction.
Parameter Initialization: In terms of the Weight w, We randomly initialized w in [-1, 1]. In terms
of the channel reliability πn , We assumed all channels Were relevant to the task beforehand, and ran-
domly initialized the channel reliability ∏, ∀ n = 1,2,…，N in [0.5,1]. TheL-BFGS implementation
Was doWnloaded from GranzoW, Where We used the default parameters. In terms of the hyperparam-
eters (μ,Σ) for W, we adopted the standard Gaussian distribution, namely W 〜N(0,1). In terms of
the hyperparameters (αn,βn), as We intended to eliminate the effects of noisy channels, We adopted
a strong non-informative prior for πn, namely On = βn = 20, ∀n = 1,2,…，N, according to Bishop
(2006). In terms of the maximum iteration number, we set MaxIter = 30 in our experiment to ensure
the algorithm converged for each participant. In terms of the minibatch size R, we set the sample
ratio to 10% and 5% for type-1 and type-2 preference propositions, respectively. The parameters for
LOR were optimized with L-BFGS with the default setting for all participants. In terms of SVR, we
resorted to 5-fold cross validation to find the best parameters for each participant.
4.1 Empirical results of BDrank on brain dynamics preferences
The accuracies of LOR and BDrank for training and test EEG preferences are presented in Table 1.
For SVR, we followed standard procedure and optimization for each participant to predict RTs with
training and test EEG signals. We adopted the Wilcoxon-Mann-Whitney statistics to measure the
accuracy of the predicted RTs w.r.t. ground truth (shown in Table 1).
Table 1: Training and test accuracy (in %). Higher is better, the best is marked in gray
Participant	P1	P2	P3	P4	P5	rɪ	rɪ	P8	P9	P10	P11	P12	P13	P14	P15	P16	P17	P18	P19	P20	P21	P22
’SYR	98.16	98.45	100	99.40	100	100	100	100	100	100	100	98.31	98.73	99.82	99.69	100	100	100	100	98.78	99.64	100
Train LOR	91.66	94.73	77.24	88.65	81.31	97.10	96.37	85.42	84.35	73.67	82.42	89.97	89.79	85.19	91.83	99.01	88.38	86.06	91.90	91.65	88.66	84.83
BDrank	98.96	97.13	95.26	99.08	94.92	99.85	97.45	97.40	97.33	93.60	99.43	97.99	98.48	96.47	98.60	99.88	98.95	100.00	99.68	99.03	96.23	85.95
SVR	68.56	76.81	68.49	61.20	76.04	78.04	69.59	66.90	67.01	79.23	73.34	67.58	69.45	69.47	85.38	73.73	75.38	55.60	66.55	71.87	75.03	68.22
IeSt	LOR一	71.22	69.12	53.12	61.63	64.63	68.46	70.99	54.97	67.18	59.18	52.96	69.87	70.50	61.18	76.27	69.52	50.35	60.87	82.73	60.67	65.10	69.98
BDrank	77.04	77.93	84.05	71.93	81.21	68.06	73.45	79.64	72.97	82.09	68.92	76.24	72.95	67.87	87.64	73.74	63.08	65.53	83.85	74.62	74.38	77.28
Participant	P23	P24	P25	P26	P27	P28	P29	P30	P31	P32	P33	P34	P35	P36	P37	P38	P39	P40	P41	P42	P43	P44
’SYR	100	100	97.13	100	99.47	100	99.88	99.98	100	100	100	100	99.93	99.97	99.95	100	98.21	99.09	100	100	100	99.59
Train LOR	91.63	98.08	88.86	92.10	90.97	84.00	86.91	92.00	87.36	90.84	80.20	86.10	91.40	89.03	93.47	93.52	90.86	92.19	93.42	82.37	81.87	81.80
BDrank	97.13	99.79	94.00	99.19	95.90	92.29	98.59	96.60	94.64	92.61	85.70	92.32	99.09	96.37	98.64	99.21	97.89	95.94	96.74	93.41	96.20	76.04
SVR-	69.70	72.84	67.25	80.89	72.71	66.17	77.24	76.59	81.11	79.54	80.64	73.83	78.39	70.99	75.34	61.9	86.92	76.51	69.89	68.21	65.09	63.06
Iest LOR	69.80	74.24	72.02	63.80	73.62	66.46	63.05	77.12	73.09	72.15	73.20	69.21	69.53	71.31	70.01	84.42	69.85	65.87	70.02	77.20	84.89	70.41
	 BDrank	71.40	82.06	74.06	72.61	79.12	68.56	81.69	79.99	75.57	77.67	79.34	79.30	80.11	73.97	76.14	86.93	87.23	77.47	70.40	77.38	88.47	74.16
From Table 1, we observe that: (1) SVR achieves the highest accuracies (100% accuracy) on the
training EEG data for almost all participants. However, test accuracies for most participants are
inferior to other models, 21 participants for LOR and 35 participants for BDrank out of 44 partic-
ipants. This observation is consistent with our previous discussion that regression-based models
are easily overfitting to the training datasets, especially when extreme values (RTs in our problem)
exist. (2) BDrank shows significant improvements over SVR and LOR in terms of test accuracies.
3We only consider the indegree sequence because the indegree and outdegree of a vertex can be uniquely
determined when the overall degree of each vertex is fixed. The indegree of vertex vt can be calculated
as deg-(Vt) = ∑m∈N1(vt)[1(Pm = 1)+ 0.5 × 1(Pm = 0)]+ ∑m∈N2(vt)[1(Pm = -1)+ 0∙5 × I(Pm = 0)], Where
N1 (vt),N2(vt) denote the index set of the pairwise comparisons with the RT of trial t (vertex vt) appearing in
the first and second position, respectively.
8
Under review as a conference paper at ICLR 2019
In particular, the number of participants with test accuracy above 75% are 17 and 25 for SVR and
BDrank, respectively. This observation is consistent with our statement that classification-based
models, served as a regularized alternative for the regression-based models, can effectively circum-
vent the overfitting caused by non-smooth RTs and preserve the ordering corresponding to RTs.
(3) However, simple classification-based extensions (e.g. LOR) do not enjoy the benefits, which
only achieves comparable test accuracies with SVR (outperforming SVR on only 21/44 partici-
pants). It is because they are not robust to the noisy data, while it is a common phenomenon when
dealing with EEG signals. (4) Note that, compared to the fine-tuned SVR for each participant, fixed
parameters were adopted for all participants in the experiment of LOR and BDrank. It further veri-
fies the superior robustness of ordinal classification models compared to regression-based models.
To further investigate the reliability of BDrank in terms of preserving the global ordering corre-
sponding to RTs, we first collected the indegree sequences of the constructed directed graph using
the predicted RTs and then measured the indegree discrepancy between the calculated indegree se-
quences and the ground truth using the root-mean-squared error (RMSE) (shown in Table. 2).
Table 2: Training and test RMSE (in #). Smaller is better, the best is marked in gray
Participant	P1	P2	P3	P4	ΓP≡	Γp6	Γp7^	P8	P9	P10	P11	P12	P13	P14	P15	P16	P17	P18	P19	P20	P21	P22
’svr	1.52	2.51	0.00	1.23	0.00	0.00	0.00	0.00	0.00	0.00	0.00	2.01	1.26	0.39	0.64	0.00	0.00	0.00	0.00	3.84	1.56	0.00
Train LOR	4.06	4.69	16.60	9.54	29.77	3.31	5.30	13.79	28.86	42.19	16.04	6.38	5.53	11.66	8.42	1.25	8.12	4.90	8.63	15.02	20.88	31.94
BDrank	0.92	3.78	3.16	0.74	7.52	0.11	4.67	1.82	3.85	11.38	0.65	1.73	0.91	3.18	1.71	0.39	0.52	0.00	2.91	2.15	7.51	34.12
SVR	14.30	19.52	22.95	30.11	39.45	16.80	29.69	27.01	53.23	38.90	23.56	20.32	13.71	21.55	14.42	26.66	16.40	16.24	28.40	47.88	39.85	60.94
Test	LOR	13.74	23.08	31.36	27.96	53.02	22.61	33.03	34.91	46.77	62.14	35.40	20.02	13.71	26.61	23.03	30.98	28.80	15.43	24.52	62.99	56.10	59.79
BDrank	10.94	19.45	10.68	19.37	25.12	10.60	28.59	12.31	32.92	27.63	16.13	14.82	12.67	21.75	11.54	26.32	12.95	3.35	19.38	32.09	40.92	52.11
Participant	P23	P24	P25	P26	P27	P28	P29	P30	P31	P32	P33	P34	P35	P36	P37	P38	P39	P40	P41	P42	P43	P44
SVR	0.00	0.00	6.08	0.00	2.22	0.00	0.98	0.22	0.00	0.00	0.00	0.00	0.26	0.19	0.26	0.00	2.10	1.38	0.00	0.00	0.00	0.67
Train LOR	16.60	3.31	15.98	19.20	17.59	27.85	22.97	14.68	28.61	17.74	36.52	23.72	7.71	11.56	7.46	6.22	6.05	5.88	11.70	22.95	15.26	11.39
BDrank	5.05	0.45	7.70	1.94	9.65	13.36	2.79	6.21	10.07	16.75	28.45	13.62	1.02	4.07	1.20	1.18	1.81	3.39	6.58	22.79	12.28	15.43
SVR	42.89	25.60	40.55	41.12	44.93	49.09	39.85	36.77	39.03	37.63	35.72	39.49	16.90	24.19	21.14	15.44	9.15	21.64	46.41	35.09	26.92	20.37
Test	LOR	46.80	22.47	36.34	64.61	41.59	48.48	31.45	40.34	53.32	48.30	47.65	35.17	22.46	26.83	25.55	13.59	18.60	22.05	47.83	26.00	20.95	18.10
BDrank	44.58	14.89	32.64	32.46	37.91	39.16	24.13	29.46	38.94	40.12	39.23	34.7	15.26	24.18	18.03	8.8	9.13	14.26	42.25	23.86	9.09	16.40
From Table. 2, we observe that: (1) On the training datasets, SVR could exactly recover the indegree
sequence (RMSE equalling to 0), which means SVR preserves the global ordering corresponding
to RTs; while LOR and BDrank could not fully fit the training datasets. (2) Note that, on the test
datasets, BDrank shows surprisingly good generalization performance and maximum preserves the
global ordering corresponding to RTs.
To further demonstrate the superiority of our BDrank, we calculated the indegree sequences using
the predicted RTs for participants P19, P38, P42, P43 with the most representative performance
(Fig. 3). Similar observations can be found for other participants.
200
150
150
50
SVR 28.4
Drank: 19.38
W 100
50
100	150	200
Trial Index (P19)
Ground Truth
SVR Test
BDrank Test
SJ 100
50
SVR: 15.44
BDrank 8.8
50
100
150
Trial Index (P38)
Ground Truth
SVR Test
BDrank Test
Trial Index (P42)
Trial Index (P43)
0
0
0
0
Figure 3:	Indegree sequence for BDrank and SVR (closer is better). The root-mean-squared error
(RMSE) was also measured between the estimated indegree sequences and the ground truth.
From Fig. 3, we observe that: (1) On the training datasets, the indegree sequences predicted by SVR
completely overlaps with the ground truth; while the indegree sequence predicted by BDrank fluctu-
ates locally but retains the overall trend. (2) On the test datasets, the indegree sequences predicted by
BDrank still closely aligns with the ground truth with slight fluctuates; while the indegree sequences
predicted by SVR fluctuates significantly and fails to maintain the trend with the ground truth, e.g.
P19, P43. (3) It is interesting to note that the indegree sequences predicted by SVR usually fluc-
tuates heavily for low indegree trials (denoting small RTs) and high indegree trials (denoting large
9
Under review as a conference paper at ICLR 2019
RTs). It means that SVR over-estimates the RTs with small values and under-estimates the RTs with
large values. It is consistent with our claim that regression-based model is not suitable for the tasks
with non-smooth response variable.
4.2 Noisy channel detection
We also investigated the reliability of our BDrank from the perspective of noisy channel detection.
According to our analysis, the parameter πn in the transition matrix Πn actually indicates the channel
reliability. Hereafter, we leverage πn as the channel reliability indicator to detect noisy channels.
Fig. 4 lists the noisy channels (marked in red) detected with 0.15 ≤ πn ≤ 0.85 , ∀n = 1,2,…,N.
C35
C30
C25
C20
C15
C10
C5
C0
P9
P18	P27	P36	P45
Positive
• Noisy
口 Negative
P0
Figure 4:	Reliability of different channels for forty four participants estimated by BDrank. Each
column denotes the states of33 channels for each participant. The channels with estimated reliability
0.15 ≤ πn ≤ 0.85 are marked in red.
Fig. 4 shows that: (1) the noisy channels is universally exist among the EEG signals. There are total
42 among 44 participants with at least one noisy channels detected. For example, the 33-th channel
is recognized as the noisy channel by BDrank for almost all participants. It is reasonable since the
33-th channel is the non-EEG channel, which is generally acknowledged as the non-relevant channel
to any tasks; (2) For each participant, most channels are reliable, which ensures we can always find
enough support to training our BDrank; and (3) The detected noisy channels varies from participant
to participant, and do not possess the transitivity property between participants. Because the noise
can arise due to (i) intrinsic non-EEG channel, e.g. the 33-th channel; (ii) channels for lateral
mastoid references, e.g. the 23-th and 29-th channel (Chatrian et al., 1985); and (iii) improper
experimentation or artifacts (Lin et al., 2018).
5 Conclusion
This work proposes a BDrank model to assess the state of mental fatigue. The efficacy of BDrank
model was demonstrated using EEG data collected in sustained driving task from 44 participants.
This model has been further combined with an online-generalized Expectation Maximization (On-
lineGEM) algorithm to provide a continuous update in the model. BDrank model utilized a unique
methodology with a regularized alternative, i.e. ordinal classification, to circumvent overfitting to
the extreme values of RTs. It has been demonstrated that the overall performance of BDrank can
be significantly improved with the introduction of the transition matrix, which enables the tech-
nique to evaluate the reliability of informative EEG channels while detecting noisy EEG channels.
Empirical results show that BDrank combined with the OnlineGEM algorithm delivers significant
improvement over SVR and LOR in terms of global ranking preservation.
In this work, the cooperation mechanism among channels is simplified as a weighted voting system,
while different trials are viewed independently. We intend to further formulate it with more complex
mechanisms, such as Markov decision process (MDP), to conduct learning and decision making
simultaneously. Some previous (Chen et al., 2016; 2015) studied the decision making process among
crowd (noisy) workers, which is promising to our setting to investigate the cooperation mechanism
among noisy channels. Efforts are underway to apply this approaches in future work.
10
Under review as a conference paper at ICLR 2019
References
Robert J Adams, Sarah L Appleton, Anne W Taylor, Tiffany K Gill, Carol Lang, R Douglas McEvoy,
and Nick A Antic. Sleep health of australian adults in 2016: results of the 2016 sleep health
foundation national survey. Sleep Health: Journal ofthe National Sleep Foundation, 3(1):35-42,
2017.
Nader Alharbi. A novel approach for noise removal and distinction of eeg recordings. Biomedical
signal processing and control, 39:23-33, 2018.
Ema Becirovic. On social choice in social networks, 2017.
Christopher M Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
Maarten AS Boksem and Mattie Tops. Mental fatigue: costs and benefits. Brain research reviews,
59(1):125-139, 2008.
Olivier CaPPe and Eric Moulines. On-line expectation-maximization algorithm for latent data mod-
els. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593-613,
2009.
GE Chatrian, E Lettich, and PL Nelson. Ten Percent electrode system for toPograPhic studies of
sPontaneous and evoked eeg activities. American Journal of EEG technology, 25(2):83-92, 1985.
Xi Chen, Qihang Lin, and Dengyong Zhou. Statistical decision making for oPtimal budget allocation
in crowd labeling. Journal of Machine Learning Research, 16(1):1-46, 2015.
Xi Chen, Kevin Jiao, and Qihang Lin. Bayesian decision Process for cost-efficient dynamic ranking
via crowdsourcing. Journal of Machine Learning Research, 17(217):1-40, 2016.
Chun-Hsiang Chuang, Zehong Cao, Jung-Tai King, Bing-Syun Wu, Yu-Kai Wang, and Chin-Teng
Lin. Brain electrodynamic and hemodynamic signatures against fatigue during driving. Frontiers
in neuroscience, 12:181, 2018.
Dane B Cook, Patrick J OConnor, Gudrun Lange, and Jason Steffener. Functional neuroimaging
correlates of mental fatigue induced by cognition among chronic fatigue syndrome Patients and
controls. Neuroimage, 36(1):108-122, 2007.
Charlotte Jacobe de Naurois, Christophe Bourdin, Anca Stratulat, Emmanuelle Diaz, and Jean-Louis
Vercher. Detection and Prediction of driver drowsiness using artificial neural network models.
Accident Analysis & Prevention, 2017.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data
via the em algorithm. Journal of the royal statistical society. Series B (methodological), pp. 1-38,
1977.
Klaus Gramann, HJ Muller, B SChOnebeck, and G Debus. The neural basis of ego-and allocentric
reference frames in spatial navigation: Evidence from spatio-temporal coupled current density
reconstruction. Brain research, 1118(1):116-129, 2006.
Brian Granzow. A matlab implementation of L-BFGS-B. https://github.com/bgranzow/
L-BFGS-B. Accessed July 4, 2017.
Frank E Harrell. Ordinal logistic regression. In Regression modeling strategies, pp. 331-343.
Springer, 2001.
Richard W Homan, John Herman, and Phillip Purdy. Cerebral location of international 10-20 sys-
tem electrode placement. Electroencephalography and clinical neurophysiology, 66(4):376-382,
1987.
Chih-Sheng Huang, Nikhil R Pal, Chun-Hsiang Chuang, and Chin-Teng Lin. Identifying changes
in eeg information transfer during drowsy driving by transfer entropy. Frontiers in human neuro-
science, 9:570, 2015.
11
Under review as a conference paper at ICLR 2019
Keise Izuma and Ralph Adolphs. Social manipulation of preference in the human brain. Neuron, 78
(3):563-573,2013.
Budi Thomas Jap, Sara Lal, Peter Fischer, and Evangelos Bekiaris. Using eeg spectral components
to assess algorithms for detecting fatigue. Expert Systems with Applications, 36(2):2352-2359,
2009.
Qiang Ji, Zhiwei Zhu, and Peilin Lan. Real-time nonintrusive monitoring and prediction of driver
fatigue. IEEE transactions on vehicular technology, 53(4):1052-1068, 2004.
Saroj KL Lal, Ashley Craig, Peter Boord, Les Kirkup, and Hung Nguyen. Development of an
algorithm for an eeg-based driver fatigue countermeasure. Journal of safety Research, 34(3):
321-328, 2003.
Percy Liang and Dan Klein. Online em for unsupervised models. In Proceedings of human language
technologies: The 2009 annual conference of the North American chapter of the association for
computational linguistics, pp. 611-619. Association for Computational Linguistics, 2009.
Chin-Teng Lin, Chun-Hsiang Chuang, Scott Kerick, Tim Mullen, Tzyy-Ping Jung, Li-Wei Ko, Shi-
An Chen, Jung-Tai King, and Kaleb McDowell. Mind-wandering tends to occur under low per-
ceptual demands during driving. Scientific reports, 6:21353, 2016.
Chin-Teng Lin, Chih-Sheng Huang, Wen-Yu Yang, Avinash Kumar Singh, Chun-Hsiang Chuang,
and Yu-Kai Wang. Real-time eeg signal enhancement using canonical correlation analysis and
gaussian mixture clustering. Journal of Healthcare Engineering, 2018.
Tina MockeL Christian Beste, and Edmund Wascher. The effects of time on task in response
selection-an erp study of mental fatigue. Scientific reports, 5:10113, 2015.
Daryl Pregibon et al. Logistic regression diagnostics. The Annals of Statistics, 9(4):705-724, 1981.
Alexis Roche. Em algorithm and variants: An informal tutorial. arXiv preprint arXiv:1105.1476,
2011.
Alex J Smola and Bernhard Scholkopf. A tutorial on support vector regression. Statistics and
computing, 14(3):199-222, 2004.
Michal Teplan et al. Fundamentals of eeg measurement. Measurement science review, 2(2):1-11,
2002.
Ping-Huang Ting, Jiun-Ren Hwang, Ji-Liang Doong, and Ming-Chang Jeng. Driver fatigue and
highway driving: A simulator study. Physiology & behavior, 94(3):448-453, 2008.
Edmund Wascher, Bjorn Rasch, Jessica Sanger, Sven Hoffmann, Daniel Schneider, Gerhard Rinke-
nauer, Herbert Heuer, and Ingmar Gutberlet. Frontal theta activity reflects distinct aspects of
mental fatigue. Biological psychology, 96:57-65, 2014.
Chun-Shu Wei, Yuan-Pin Lin, Yu-Te Wang, Tzyy-Ping Jung, Nima Bigdely-Shamlo, and Chin-
Teng Lin. Selective transfer learning for eeg-based drowsiness detection. In Systems, Man, and
Cybernetics (SMC), 2015 IEEE International Conference on, pp. 3229-3232. IEEE, 2015.
Peter Welch. The use of fast fourier transform for the estimation of power spectra: a method based
on time averaging over short, modified periodograms. IEEE Transactions on audio and electroa-
coustics, 15(2):70-73, 1967.
Ruby C Weng and Chih-Jen Lin. A bayesian approximation method for online ranking. Journal of
Machine Learning Research, 12(Jan):267-300, 2011.
Lian Yan, Robert H Dodier, Michael Mozer, and Richard H Wolniewicz. Optimizing classifier
performance via an approximation to the wilcoxon-mann-whitney statistic. In Proceedings of the
20th International Conference on Machine Learning (ICML-03), pp. 848-855, 2003.
Guoqiang Peter Zhang. Neural networks for classification: a survey. IEEE Transactions on Systems,
Man, and Cybernetics, Part C (Applications and Reviews), 30(4):451-462, 2000.
12
Under review as a conference paper at ICLR 2019
Supplementary:
The discrepancy between the predicted indegree sequence and the ground truth:
Assume there exists a ranking list S : a > b > C > d, we can constructed 6 (= 4×3) preference
propositions {ρi}i6=1 with the pairwise comparisons. Further, these could be equally represented by
the fully directed graph with four vertices as Fig.5a. Then, the indegree of each vertex vi can be
calculated following deg- (vi) =∑m∈N1(vi)1(ρm= 1)+∑m∈N2(vi)1(ρm= -1), whereN1(vi),N2(vi) denote
the index set of the pairwise comparisons with the vertex i appearing in the first and second position,
respectively. Therefore, the indegree sequence (in ascending order) of Fig.5a can be represented as
X = [0, 1,2,3] for the corresponding vertex sequence dcba. Note that we only consider the indegree
sequence because the indegree and outdegree of a vertex can be uniquely determined when the
overall degree of each vertex is fixed.
Figure 5: Directed graph representation of the ranking list S : a > b > c > d. Left Panel: the ground
truth, where {ρi}i6=1 is the constructed preference proposition. Right Panel: the prediction, where
{ρ^i} 6=1 is the predicted order for each pairwise comparison. Note that we use the edges marked in
red (e.g. {ρ^ι, ρ^3,何}) to denote the wrong predictions.
As for each prediction model, we first calculate the final estimation {pi}6=1 for each pairwise com-
parisons using a simple voting scheme, detailed in Section 4. The corresponding fully directed graph
is shown in Fig.5b. We calculate the indegree of each vertex Vi following deg-(vi) = ∑m∈N1(vi.)[l(pm =
1) + 0.5 × 1(pm = 0)] + ∑m∈N2(vi.)[1(pm = -1) + 0.5 × 1(f)m = 0)]. Note that we consider the tie case
(e.g. ρ = 0) here since the number of votes for state 1 and -1 may be comparable. Next, the
predicted indegree sequence for the vertex sequence dcba is X = [1,0.5,2.5,2]. Therefore the dis-
crepancy between the predicted indegree sequences and ground truth using the root-mean-squared
errors (RMSD) can be calculated as follows,
RMSD(X)
j∑ T=1( XT - X) = 0.7906.
Then, we can use this value to investigate the reliability of the proposed method in terms of preserv-
ing the global ordering.
13