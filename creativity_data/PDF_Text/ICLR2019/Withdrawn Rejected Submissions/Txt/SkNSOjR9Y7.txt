Under review as a conference paper at ICLR 2019
Training Variational Auto Encoders with Dis-
crete Latent Representations using
Importance Sampling
Anonymous authors
Paper under double-blind review
Ab stract
The Variational Auto Encoder (VAE) is a popular generative latent variable model
that is often applied for representation learning. Standard VAEs assume con-
tinuous valued latent variables and are trained by maximization of the evidence
lower bound (ELBO). Conventional methods obtain a differentiable estimate of
the ELBO with reparametrized sampling and optimize it with Stochastic Gradient
Descend (SGD). However, this is not possible if we want to train VAEs with dis-
crete valued latent variables, since reparametrized sampling is not possible. Till
now, there exist no simple solutions to circumvent this problem. In this paper, we
propose an easy method to train VAEs with binary or categorically valued latent
representations. Therefore, we use a differentiable estimator for the ELBO which
is based on importance sampling. In experiments, we verify the approach and train
two different VAEs architectures with Bernoulli and Categorically distributed la-
tent representations on two different benchmark datasets.
1 The Variational Auto Encoder
The variational auto encoder (VAE) is a generative model which it is trained to approximate the
true data generating distribution p(x) of an observed random vector x from a given training set
D = {x1, ..., xN} (Kingma & Welling (2013); Kingma et al. (2016)). It is an especially suited
model if x is high dimensional or has highly nonlinear dependent elements. Therefore, the VAE
is oftenly used for tasks like density estimation, data generation, data interpolation (White (2016)),
outlier and anomaly detection (An & Cho (2015); Xu et al. (2018)) or clustering (Jiang et al. (2016);
Dilokthanakul et al. (2016)).
As shown in Fig. 1, the VAE is an easy latent variable model, where the observations X 〜p(x∣z)
are dependent on latent variables Z 〜p(z).
Figure 1: The latent variable model of a VAE with latent variables z and observations X.
During training, the VAE maximizes the probability p(X) to observe the data X. Therefore, the
negative evidence lower bound (ELBO)
L(θ) = -Eq(z|x) [ln p(X|z)] + DKL(q(z|X)||p(z))	(1)
≥	- ln p(X) + DKL (q(z|X)||p(z|X))	(2)
is minimized, where p(z|X) = p(X|z)p(z)/ p(X|z)p(z)dz is the true but intractable posterior dis-
tribution the model assigns to z, q(z|X) is the corresponding tractable variational approximation
and DKL (q(z|X)||p(z|X)) is the Kullback-Leibler (KL) divergence between p(z|X) and q(z|X). Be-
cause DKL(q(z|X)||p(z|X)) > 0, minimizing L(θ) means to maximize the probability p(X) the
model assigns to observations X. Therefore, DKL(q(z|X)||p(z|X)) must be as as close as possible
to 0, meaning that after training q(z|X) is a very good approximation of the true posterior p(z|X).
1
Under review as a conference paper at ICLR 2019
Kingma & Welling (2013) proposed to minimize L(θ), using stochastic gradient descent on a train-
ing data set, which they called Stochastic Gradient Variational Bayes (SGVB).
The VAE uses parametric distributions that are parametrized by an encoder network with parameters
θE and a decoder network with parameters θD for both q(z|x) and p(x|z), respectively. This leads to
the well known encoder-decoder structure in Fig. 2. The data likelihood is a distribution with mean
x, that is the output of the decoder network. Further, We assume in this paper, that the variational
posterior q(z|x) is a distribution from the exponential family
q(z|x) = exp(ηT (x; θE)T (z) - A(η(x; θE)))	(3)
with natural parameters η(x; θE), sufficient statistic T(z) and log partition function A(η(x; θE)).
This gives us the flexibility to study training with different q(z|x) in the same mathematical frame-
work. As shown in Fig. 2, the natural parameters η are the output of the encoder network, where we
drop the arguments x, θE for shorter notations in the remainder of the paper.
L
K
L
而
ɪ加ɪθ
==
Figure 2: The encoder-decoder structure ofa VAE. The encoder parametrizes q(z|x) as an exponen-
tial family distribution with natural parameters η and the decoder parametrizes p(x|z) with mean
x.
The conventional VAE proposed in (Kingma & Welling (2013); Kingma et al. (2016)) learns
continuous latent representations z ∈ Rc. It uses i.i.d. Gaussian distributed z, meaning that
η = [μ"σ2,-1/(2由,…,μJσ2,-142σ2产,T(Z) = [zι,z2,…, zc, zc2]T and A(η) is chosen
such that q(z∣x) integrates to one. The likelihood is also Gaussian, with p(x∣z)〜N(x, 1). But in
many applications learning discrete rather than continuous representations is advantageous. Binary
representations z ∈ {0, 1}c can for example be used very efficiently for hashing, what is a power-
ful method for large-scale visual search (Liong et al. (2015)). Learning Categorical representations
z ∈ {e1, ..., ec} is interesting, because this naturally lead to clustering of the data x, as shown in the
experiments. Further, for both binary and categorical z it is easy to find entropy based heuristics to
choose the size of the latent space, because the entropy is bounded for discrete z.
However, training VAEs with discrete latent representations is problematic, since standard SGVB
can not be applied for optimization. Because SGVB is a gradient based method, we need to calculate
the derivative of the two cost terms with respect to the encoder and decoder parameters
DKL (q(z|x)||p(z))	(4)
Eq(z|x) [ln p(x|z)] ,	(5)
where LKL(θ) only depends on the encoder parameters and the expected log likelihood term LL (θ)
depends on both encoder and decoder parameters. For a suited choice of p(z) and q(z|x), LKL(θ)
can be calculated in closed form. However, Ll(Θ) contains an expectation over Z 〜q(z∣x) that has
to be estimated during training. A good estimator LL(θ) for LD (θ) that is unbiased, differentiable
with respect to θ and that has low variance is the key to train VAEs. SGVB uses an estimator LR(θ)
that is based on reparametrization of q(z|x) and sampling (Kingma & Welling (2013)). However, as
described in section 2, this method places many restrictions on the form of q(z|x) and fails if q(z|x)
can not be reparametrized. This is the case ifz is discrete, for example.
In this paper, we propose a simple and differentiable estimator LL (θ) for Ll(Θ) that is based on
importance sampling. Because no reparametrization is needed, it can be used to train VAEs with bi-
nary or categorical latent representations. Compared to previously proposed methods like the Vector
Quantised-Variational Auto Encoder (VQ-VAE) (van den Oord et al. (2017)), which is based on a
straight-through estimator for the gradient of LL(θ) (Bengio et al. (2013)), our proposed estimator
has two advantages. Itis unbiased and its variance approaches zero the closer we are to the optimum.
2
Under review as a conference paper at ICLR 2019
2 Estimating the expected log-likelihood with reparametrized
SAMPLING
The standard estimator LR (θ) proposed in Kingma & Welling (2013) is based on reparametrized
sampling
∂θ LL(θ)	∂ =∂θEq(z∣χ) [lnP(XIZ)I	(6)
	∂ = 而Epg [lnP(XIZ = f(3 θ))] ∂θ ∂1M ≈ ∂θ M ElnP(XIZ = f(em, θ))	(7)
		(8)
	=∂θ LR(θ)	(9)
where is a random variable with the distribution p(), m are samples from this distribution and
f (e, θ) is a reparametrization function, such that Z = f (e, θ)〜q(z∣x). This estimator can be used
to train VAEs with SGVB if two conditions are fulfilled: I) There exists a distribution p() and a
reparametrization function f 亿 θ), such that Z = f (e, θ)〜q(z∣x). II) The derivative of Eq. 5 must
exist. With Eq. 8, we obtain
∂	1M∂	∂
∂θ LL (θ) = M X ∂z lnP(X|Z = f (K θ)) ∂θZ，
m=1
(10)
meaning that both the reparametrization function f(, θ) and ln p(x|Z) must be differentiable
with respect to Z and θ, respectively, to allow direct backpropagation of the gradient through the
reparametrized sampling operator. If these conditions are fulfilled, the gradient can flow directly
from the output to the input layer of the VAE, as shown in Fig. 3. Distributions over discrete latent
representations Z can not be reparametrized this way. Therefore, this estimator can not be used to
train VAEs with such representations.
Figure 3: The gradient flow through the VAE, using LR(θ) based on reparametrized sampling. The
gradient is propagated directly through the reparametrized sampling operator.
3
Under review as a conference paper at ICLR 2019
3 Estimating the expected log-likelihood with importance
SAMPLING
We propose an estimator LL(θ) which is based on importance sampling and can also be used to
train VAEs with binary or categorical latent representations z. Expanding Eq. 4 leads to
∂∂
∂θLL(θ) = ∂θ J lnp(x|z)q⑵x)dz
=∂ Zhn P(XIz)磊 qI (Z)dz
≈ ∂ M (XjPIM)给
=∂θLL("
(11)
(12)
(13)
(14)
where qI (z) is an arbitrary distribution that is of the same form as q(z|X) which is independent
from the parameters θ. Zm 〜qI(z) are samples from this distribution. The estimator computes a
weighted sum of the log likelihood ln P(X|zm ) with the weighting q(zm |X)/qI (z).
The benefit is that the log likelihood ln P(X|zm ) depends on the decoder parameters θD only and
not on the encoder parameters θE whereas the weighting q(zm |X)/qI (z) depends only on θD and
not on Θe . Therefore, calculation of the gradient of LL (θ) can be separated
∂θLL(θ)= kθb Ll® 焉比⑻
(15)
with
∂ ∂θ~ LL(O)= ∂θE	1 二 M	M X m=1	ln p(x∣zm)焉 qZmX ∂θE qI (zm )	(16)
∂ ∂θ~ LL(θ)	= ∂θD	1 二 M	M X m=1	q(Zm⅛ɪ lnP(x∣zm). qI (zm ) ∂θD	(17)
As shown in Fig. 4, gradient backpropagation is split into two separate parts. ∂∂^LlL⑻ back-
propagates the error lnp(x∣z) from the output of the VAE to the sampling operator and ∂^LlL (θ)
backpropagates the error q(Z(χ from the sampling operator to the input layer of the VAE.
Figure 4: Gradient flow through the VAE when using LL(θ), based on importance sampling.
Compared to LR(θ), We do not need to find a differentiable reparametrization for q(z∣x), because
We do not propagate the gradient through the sampling operator. Therefore, LlL(θ) can also be
used if no reparametrization function exists for q(z|x), e.g. if it is a Bernoulli or a Categorical
distribution.
4
Under review as a conference paper at ICLR 2019
4 VAE WITH B ERNOULLI DISTRIBUTED z (BVAE)
Assume the latent representation z has i.i.d. components that are Bernoulli distributed, i.e. both the
variational posterior distribution q(z|x) and qI (z) have the form
q(z|x) = exp(ηT z - A(η))
qI (z) = exp(ξT z - A(ξ)),
(18)
(19)
where z ∈ {0, 1}c, η = [ln(q1/(1 - q1)), ..., ln(qc/(1 - qc))] is the output vector of the encoder
that contains the logits of the independent Bernoulli distributions and A(η) = 1T ln(1 + eη) are the
corresponding log-partition functions.
Hence, Eq. 17 is
焉LL(θ)	=	14	X	lnP(XIZm) dL exP	(⑺	-	ξ)TZm	-	(A㈤-A(E)))	(ɪη)
∂θE	M	∂η	∂ θE
m=1
1M	∂
= M E lnP(XIZm) eχp ((η - ξ) Zm - (A(η) - A(E)))(Z - q产(犷〃
m=1	E
where q = [qι,…，q∕T = ι+—η contains the probabilities q(zi = 1∣x). The variance of the
estimator LL(θ) heavily depends on the choice of the natural parameters ξ of the distribution qι(z).
We choose E = η, leading to a gradient of the very simple form
∂	1M
荷 LL(θ) =Kf ln P(XIZm)(Zm - q)
∂ θE	M
m=1
(∂θEη).
(20)
This estimator of the gradient has two desirable properties for training, which can be easily seen in
the one dimensional case with z ∈ {0, 1}. The mean of the estimator is
EqI(ZidlE LL(θ)
EqI (z)
1M
M E lnP(XIz)(Zm — q)
m=1
q(1 - q)(ln P(XIz = 1) - ln(P(XIz = 0))
∂θE LL(θ),
(∂θE η)
(21)
(22)
(23)
meaning that the estimator is unbiased. Further, the variance of ∂∂^ LIL⑻ reduces to zero, the
closer q is to 0 or 1, because q(1 - q) → 0 and hence the variance of the estimator approaches 0.
That is desirable, since there are only three interesting cases during training that are shown in Fig.
5:
(a)	ln P(XIz = 0) = ln P(XIz = 1): In this case the expected log likelihood
Eq(z|x) [ln P(XIz = 0)] = ln P(XIz = 0) we want to maximize is independent of the choice
of q.
(b)	ln P(XIz = 0)	>	ln P(XIz	= 1):	In this case the expected log likelihood
Eq(z|x) [ln P(XIz =	0)]	= (1 - q)	ln P(XIz =	0) + q ln P(XIz = 1) is maximized for q = 0.
(c)	ln P(XIz = 0)	<	ln P(XIz	= 1):	In this case the expected log likelihood
Eq(z|x) [ln P(XIz =	0)]	= (1 - q)	ln P(XIz =	0) + q ln P(XIz = 1) is maximized for q = 1.
This means, that the only candidate points that maximize the log likelihood are q = 0 or q = 1 lie
near q(z = 1IX) = 0/1. Therefore, the longer we train, the more accurate the gradient estimate will
be.
5
Under review as a conference paper at ICLR 2019
ln p(x∣z)	Aln p(x∣z)	Aln p(x∣z)
z=0	z=1 z z=0	z=1 z z=0	z=1 z
(a)	(b)	(c)
Figure 5: Three different cases how the log-likelihood can vary over z. The decoder network defines
ln(x|z) over z ∈ R. However, only the points z = 0 and z = 1 are of interest. (a) ln(x|z =
0) = ln(x|z = 1), meaning Eq(z|x) [ln(x|z)] is independent of q. (b) ln(x|z = 0) > ln(x|z = 1),
meaning Eq(z|x) [ln(x|z)] is maximized for q = 0. (c) ln(x|z = 0) < ln(x|z = 1), meaning
Eq(z|x) [ln(x|z)] is maximized for q = 1.
5 VAE WITH CATEGORICALLY DISTRIBUTED z (CVAE)
For Categorically distributed z both the variational posterior distribution q(z|x) and qI (z), again
have the form
q(z|x) = exp(ηT z - A(η))
qI (z) = exp(ξT z - A(ξ)),
(24)
(25)
but now z ∈ {e1, ..., ec} can assume only c different values. The vector of natural parameters is
η = [ln(p1/pc), ..., ln(pc-1/pc), 0)] and the log partition function is A(η) = ln 1Teη.
With the formulas above, we arrive at the same easy form of the expected gradient of the log likeli-
hood
∂	1M	T ∂
^-LL(O)=而 £ lnP(XIzm)(Zm - q) (而~η ) ,	(26)
∂θE	M m=1	∂θE
but now with q=	softmax(η) that consists of the probabilities qi=	q(z=	ei), where Pic=1 qi=	1.
6 Experiments
In the following section, we show our preliminary experiments on the MNIST and Fashion MNIST
datasets LeCun & Cortes (2010); Xiao et al. (2017). Two different kinds of VAEs have been evalu-
ated:
1.	The BVAE with Bernoulli distributed z ∈ {0, 1}c.
2.	The CVAE with Categorically distributed z ∈ {e1, ..., ec}.
To train both architectures, the estimator LL (θ) derived in Sec. 5 is used.
Both BVAE and CVAE are tested with two different architectures given in Tab. 1. The fully con-
nected architecture has 2 dense encoder and decoder layers. The encoder and decoder networks of
the convolutional architecture consist of 4 convolutional layers and one dense layer each.
In our first experiment We train a FCBVAE with C = 50, i.e. z ∈ {0,1}50 and a FCCVAE with
c 100, i.e. z ∈ {z1, ..., z100}. We train them for 300 epochs on the MNIST dataset, using
6
Under review as a conference paper at ICLR 2019
Table 1: The architectural details of the trained VAEs. FC, Conv and Conv-1 are the fully con-
nected, convolutional and deconvolutional layer, respectively. The shape of the convolutional layers
is given in the form DimA × DimB × Channel/Stride/Activation.
Architecture	In/Out	Encoder	Latent	Decoder
-FC-BVAE- orFC.CVAE	-784-	FC 1024ZReLU FC c/linear	Z ∈ Re 〜 Ber(i+：-n )~ or z ∈ Rc 〜Cat( -ITnn)	FC 1024ZReLU FC 784/sigmoid
CNN_BVAE or CNN-CVAE	28x28x1	Conv 3x3x32/2ZReLU Conv 3x3x64/2ZReLU Conv 3x3x64/2ZReLU Conv 3x3x128/2/ReLu flatten FC c/linear	Z ∈ Rc ~ Ber( ιj-n )~ or z ∈ Rc 〜Cat( -ITnn)	FC c/ReLu reshape ConVT 3x3x64/2ZReLU ConVT 3x3x64/2ZReLU ConVT 3x3x32/2ZReLU ConV-I 3x3x1/2/sigmoid
SGVB with our proposed estimator LlL(θ) to estimate the expected log likelihood, and ADAM
as optimizer. Fig. 6 shows the convergence of the loss, the log likelihood the VAEs assign to the
training data lnp(x∣z) and the variance of the estimator LL(θ) for a learning rate of 1e - 3 and
a batch size of 2048. During training, the loss decreases steadily without oscillation. We observe
that the variance of the estimator LL(θ) decreases the longer We train and the closer We get to the
optimum. This is consistent with our theoretically considerations in Sec. 4. The results of the
corresponding simulations with the CNN-BVAE and the CNN-CVAE are shown in Fig. 9, in the
appendix.
log-likelihood
gradient variance
loss
Figure 6: Convergence of the loss, log-likelihood and gradient variance over 300 epochs of training
on the MNIST dataset.
The performance of the FCCVAE is worse than the performance of the FC_BVAE. Training con-
verges to a lower log likelihood ln p(x|z), because the maximal information content HCV AE (z) ≤
ln(100) of the latent variables of the FC-CVAE is much less than the maximal information content
HBVAE (Z) ≤ C ln(2) ofthe latent variables of the FC_BVAE. The FC_CVAE can at maximum learn
to generate 100 different handwritten digits, what is a small number compared to the 250 different
images that the FC-CVAE can learn to generate.
Fig. 7 shows handwritten digits that are generated by the FC-BVAE and the FC-CVAE if we sample
z from the variational posterior q(z|x). To draw samples from q(z|x), we feed test data which
has not been seen during training to the encoders. The test data is shown in Fig. 7a and Fig. 7c.
The corresponding reconstructions generated by the decoders are shown in Fig. 7b and Fig. 7d.
Both input and reconstructed images are very similar in case of the FC-BVAE, meaning that it can
approximate the data generating distribution P(X) well. However, in case of the FC-CVAE, the
generated are blurry and look very different than the input of the encoder. In some cases, the class
of the generated digit is even flipped. This happens because of the the very limited model capacity.
Similar results for the CNN-BVAE and CNN-CVAE are shown in Fig. 10 in the appendix.
Fig. 8a shows generated images of the FC-BVAE if we sample Z 〜p(z) from the prior distribu-
tion. A few generated images look like templates of handwritten digits and the remaining generated
images seem to resemble mixtures of different digits. This is similar to the behaviour of a VAE
with continuous latent variables, where we can interpolate between or generate mixtures of different
7
Under review as a conference paper at ICLR 2019
(a) input FC_B VAE
q i 7
q a 7
。¥ q
管SJ
δ 3 &
Q £ ?
出发
Oeq
7/ I Qsr4
77*7Q6 5》/
/∕≤73to∕
7σ-3 77TNa
Ql 757/ / I
IqQ 2tpTo?
Ul 2 ʃ of S / OJI
0/737// 1 /z
lσ~or⅞19^7(J36-
H2s, Sf4lq^7
O 7/ I"73<6 1F
77Γ-CΓ6 lnx∕"G
/,5731Q∕r⅛a
36O∖4-Γ9r6>v
£%7U/C
q
7
N
ʒ
o
s 9 5
07
Vq
夕O、
H ɛ 1
7 6 8
O < 6
3 3 7
I H 3
q / A
3 OG 3 I 7s∕w r 7
7to 7ywaz5*
CQg3c00cγ5597
gqg7JI0435
37Qiαi∕sz∙ 7jλ
goɪOgG44 gl
$3<r9¥¥o**ll9
S 夕37 9 72338
3 fe 019rgr0y
r8ly9 74 7, / 6
(b) generated FuBVAE
(c) input FC-CVAE
(d) generated FCCVAE

9 0o<Γ9HI 4x ɔ X 9
q
3 5G 5/7三& ʃ 7
fo r-≠y〃 W z3jl
Figure 7: Test input images and generated handwritten images of the FCBVAE and FCCVAE
digits by traveling through the latent space Kingma & Welling (2013). However, in comparison to
VAEs with continuous latent variables, we can only generate discrete mixtures for VAEs if the latent
variables z are Bernoulli.
Fig. 8b shows generated images of the FC-CVAE if We sample Z 〜p(z) from the prior distribution.
Since the FC-CVAE can only learn to generate 100 different images, its decoder learns to generate
template images that fit well to all the training images. We observe that some latent representations
are decoded to meaningless patterns that just fit well to the data in avarage. However, the decoder
also learned to generate at least one template image for each class of handwritten digits. Hence,
the categorical latent representation can be interpreted as the cluster affiliation and the encoder of
the FC-CVAE automatically learns to cluster the data. Similar results for the CNN_BVAE and
CNN-CVAE are shown in Fig. 11 in the appendix.
A major drawback of the FC-CVAE is, that the latent space of the FC-CVAE can encode only very
little information and thus its generative capabilites are poor. However, we think that they can be
increased considerably if we allow a hybrid latent space with some continuous latent variables, as
proposed in Chen et al. (2016). This could lead to a powerfull model for nonlinear clustering.
(a) samples FC_BVAE
(b) samples FC.CVAE
Figure 8: Handwritten digits generated by the FC-BVAE and FC_CVAE, if Z is sampled from the
prior distribution p(z).
p7 7 5 3 2 Q 4 3 公
JS,fc-Γ⅛TΛ-JF/
9-Atteτgγ*4a
TS3 W 夕夕z2¾,
, qs0 f⅛3∕*''≠
9 9？舄 ic
R36<r>τ8g 弓 3
62∕∙l 十I
dG*227τffr
幺 TCJvg7,2下二•
∙-oq--q
O ð 00 ʒ
O 5 3 r oo : ∙.
I 3 9 夕 K，8 g
a79 3 夕 6 G
/ O g 8 q 2 :
2 勺 2，9 7，
7 Conclusion
In this paper, we derived an easy estimator for the ELBO, which does not rely on reparametrized
sampling and therefore can be used to obtain differentiable estimates, even if reparametrization is
not possible, e.g. if the latent variables Z are Bernoulli or Categorically distributed. We have shown
theoretically and in experiments, close to the optimal parameter configuration, the variance of the
estimator approaches zero. This is a very desirable property for training.
8
Under review as a conference paper at ICLR 2019
References
Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using reconstruc-
tion probability. 2015.
Yoshua Bengio, Nicholas Leonard, and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL
http://arxiv.org/abs/1308.3432.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-
gan: Interpretable representation learning by information maximizing generative adversarial nets.
CoRR, abs/1606.03657, 2016. URL http://arxiv.org/abs/1606.03657.
Nat Dilokthanakul, Pedro A. M. Mediano, Marta Garnelo, Matthew C. H. Lee, Hugh Salimbeni,
Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture
variational autoencoders. CoRR, abs/1611.02648, 2016. URL http://arxiv.org/abs/
1611.02648.
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep
embedding: A generative approach to clustering. CoRR, abs/1611.05148, 2016. URL http:
//arxiv.org/abs/1611.05148.
D. P Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints, December 2013.
Diederik P. Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse
autoregressive flow. CoRR, abs/1606.04934, 2016. URL http://arxiv.org/abs/1606.
04934.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
V. E. Liong, Jiwen Lu, Gang Wang, P. Moulin, and Jie Zhou. Deep hashing for compact binary
codes learning. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 2475-2483, June 2015. doi: 10.1109/CVPR.2015.7298862.
Aaron van den Oord, Oriol Vinyals, and Koray KavUkcUoglu. Neural discrete representation learn-
ing. CoRR, abs/1711.00937, 2017. URL http://arxiv.org/abs/1711.00937.
Tom White. Sampling generative networks: Notes on a few effective techniques. CoRR,
abs/1609.04468, 2016. URL http://arxiv.org/abs/1609.04468.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/
abs/1708.07747.
Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Liu, You-
jian Zhao, Dan Pei, Yang Feng, Jie Chen, Zhaogang Wang, and Honglin Qiao. Unsupervised
anomaly detection via variational auto-encoder for seasonal kpis in web applications. CoRR,
abs/1802.03903, 2018. URL http://arxiv.org/abs/1802.03903.
9
Under review as a conference paper at ICLR 2019
A Results for CNNBVAE and CNNCVAE on MNIST
——CNN_BVAE ——CNN-CVAE
Figure 9: Convergence of the loss and the log-likelihood over 300 epochs, while training on the
MNIST dataset.
(a) input CNN_BVAE
&12S 9/ 5/ 7Z
3VN932y731
WS3?47,⅞>2>8/
/0*F*αgN — 03M
?98n夕)，h工『
夕夕 7。5y⅛g Jq
74 3/316 0/3
I7ls^7∕⅛∖z?
(b) generated CNNBVAE
(c) input CNNCVAE
55Γf5∕DI55r
52^97795^1
2722/35夕Q3
I G t?夕 3 5A I N I
q2q夕22夕/ ɑ
332q613f79
57^0^51939
(d) generated CNN-CVAE
Figure 10: Test input images and generated handwritten images of the CNN_BVAE and CNN-CVAE
(a) samples CNN_CVAE
(b) samples CNN_BVAE
βg03∙ogβ*68
7i*16y7o9pq
t>ss 1c902C g
SI¢5 3 94 岁3G
fo a Hw 夕夕FeΓ÷
⅛⅛yα 32ft∖>89f
Gy4τf0596f
c∕⅛>c¾≠∕i‰7^0q
占 4 87aq9βs 歹

Figure 11:	Handwritten digits generated by the CNN_BVAE and CNN-CVAE, if Z is sampled from
the prior distribution p(z).
10
Under review as a conference paper at ICLR 2019
B Results for CNN_BVAE ON fashion MNIST
As shown in Fig. 12, the gradient variance approaches 0, the closer we get to the optimum. This
is the same behaviour as for the MNIST dataset. As shown in Fig. 13, the FC-BVAE Can correctly
reconstruct the shape of the given clothes with high accuracy. However, details like texture are lost.
This is due to the limited model capacity, i.e. the latent representation z ∈ {0, 1}50 of the given
VAE can at most encode 50Bits of information.
FC-BVAE
Figure 12:	Convergence of the loss and gradient variance over 300 epochs of training on the fashion
MNIST dataset.
(a) input FC-BVAE	(b) generated FC-BVAE
Figure 13: Test input images and generated images of the FC-BVAE on the fashion MNIST dataset.
11