Under review as a conference paper at ICLR 2019
Revisiting Reweighted Wake-Sleep
Anonymous authors
Paper under double-blind review
Ab stract
Discrete latent-variable models, while applicable in a variety of settings, can of-
ten be difficult to learn. Sampling discrete latent variables can result in high-
variance gradient estimators for two primary reasons: 1. branching on the samples
within the model, and 2. the lack of a pathwise derivative for the samples. While
current state-of-the-art methods employ control-variate schemes for the former
and continuous-relaxation methods for the latter, their utility is limited by the
complexities of implementing and training effective control-variate schemes and
the necessity of evaluating (potentially exponentially) many branch paths in the
model. Here, we revisit the reweighted wake-sleep (rws) (Bornschein & Ben-
gio, 2015) algorithm, and through extensive evaluations, show that it circumvents
both these issues, outperforming current state-of-the-art methods in learning dis-
crete latent-variable models. Moreover, we observe that, unlike the importance
weighted autoencoder, RWS learns better models and inference networks with in-
creasing numbers of particles, and that its benefits extend to continuous latent-
variable models as well. Our results suggest that rws is a competitive, often
preferable, alternative for learning deep generative models.
1	Introduction
Learning deep generative models with discrete latent variables opens up an avenue for solving a wide
range of tasks including tracking and prediction (Neiswanger et al., 2014), clustering (Rasmussen,
2000), model structure learning (Adams et al., 2010), speech modeling (Juang & Rabiner, 1991),
topic modeling (Blei et al., 2003), language modeling (Chater & Manning, 2006), and concept learn-
ing (Kemp et al., 2006; Lake et al., 2018). Furthermore, recent deep-learning approaches addressing
counting (Eslami et al., 2016), attention (Xu et al., 2015), adaptive computation time (Graves et al.,
2014), and differentiable data structures (Graves et al., 2014; 2016; Grefenstette et al., 2015), under-
score the importance of models with conditional branching induced by discrete latent variables.
Current state-of-the-art methods optimize the evidence lower bound (elbo) based on the importance
weighted autoencoder (iwae) (Burda et al., 2016) by using either reparameterization (Kingma &
Welling, 2014; Rezende et al., 2014), continuous relaxations of the discrete latents (Maddison et al.,
2017; Jang et al., 2017) or the reinforce method (Williams, 1992) with control variates (Mnih &
Gregor, 2014; Mnih & Rezende, 2016; Gu et al., 2016; Tucker et al., 2017; Grathwohl et al., 2018).
Despite the effective large-scale learning made possible by these methods, several challenges re-
main. First, with increasing number of particles, the iwae elbo estimator adversely impacts
inference-network quality, consequently impeding learning of the generative model (Rainforth et al.,
2018). Second, using continuous relaxations results in a biased gradient estimator, and in models
with stochastic branching, forces evaluation of potentially exponential number of branching paths.
For example, a continuous relaxation of the cluster identity in a Gaussian mixture model (gmm)
(Section 4.3) forces the evaluation of a weighted average of likelihood parameters over all clusters
instead of selecting the parameters based on just one. Finally, while control-variate methods may be
employed to reduce variance, their practical efficacy can be somewhat limited as in some cases they
involve designing and jointly optimizing a separate neural network which can be difficult to tune
(Section 4.3).
To address these challenges, we revisit the reweighted wake-sleep (rws) algorithm (Bornschein &
Bengio, 2015), comparing it extensively with state-of-the-art methods for learning discrete latent-
1
Under review as a conference paper at ICLR 2019
variable models, and demonstrate its efficacy in learning better generative models and inference
networks, and improving the variance of the gradient estimators, over a range of particle budgets.
Going forward, we review the current state-of-the-art methods for learning deep generative models
with discrete latent variables (Section 2), revisit rws (Section 3), and present an extensive evaluation
of these methods (Section 4) on (i) the Attend, Infer, Repeat (air) model (Eslami et al., 2016)
to perceive and localise multiple mnist digits, (ii) a continuous latent-variable model on mnist,
and (iii) a pedagogical gmm example, exposing a shortcoming of rws that we fix using defensive
importance sampling (Hesterberg, 1995). Our experiments confirm that rws is a competitive, often
preferable, alternative that unlike iwae, learns better models and inference networks with increasing
particle budgets.
2	Background
Consider data (x(n))nN=1 sampled from a true (unknown) generative model p(x), a family of gen-
erative models pθ (z, x) of latent variable z and observation x parameterized by θ and a family of
inference networks qφ(z∣χ) parameterized by φ. We would like to learn the generative model by
maximizing the marginal likelihood over data: θ* = argmax6 焉 PN=IlogPθ(x(n)). We would
simultaneously also like to learn an inference network qφ(z∣χ) that amortizes inference given obser-
vation x; i.e., qφ(z∣χ) maps an observation X to an approximation of pθ* (z|x). Amortization ensures
that evaluation of this function is cheaper than performing approximate inference of pθ* (z|x) from
scratch. Our focus here is on such joint learning of the generative model and the inference network,
here referred to as “learning a deep generative model”, although we note that other approaches ex-
ist that learn just the generative model (Goodfellow et al., 2014; Mohamed & Lakshminarayanan,
2016) or the inference network (Paige & Wood, 2016; Le et al., 2017) in isolation.
We begin with a review of iwaes (Burda et al., 2016) as a general approach for learning deep
generative models using stochastic gradient descent (sgd) methods, focusing on generative-model
families with discrete latent variables, for which the high variance of the naive gradient estima-
tor impedes learning. We will also review control-variate and continuous-relaxation methods for
gradient-variance reduction. The iwae used alongside such gradient-variance reduction methods is
currently the dominant approach for learning deep generative models with discrete latent variables.
2.1	Importance Weighted Autoencoders
Burda et al. (2016) introduced the iwae which maximizes an average of elbos over data,
N PN=I ELBOK (θ, Φ, x(n)), where, given number of particles K,
ELBOKG φ, X)= EQφ(zi:K|x)
Pθ (Zk, X)	(1)
qφ(zk |x).
When K = 1, this reduces to the variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende
et al., 2014). Burda et al. (2016) show that ELBOIKS (θ, φ, X) is a lower bound on log pθ (X) and that in-
creasing K leads to a tighter lower bound. Further, tighter lower bounds arising from increasing K
improve learning of the generative model, but worsen learning of the inference network (Rain-
forth et al., 2018), as the signal-to-noise ratio of θ's gradient estimator is 0(√K) whereas φ's is
0(1/√K). Moreover, poor learning of the inference network, beyond a certain point (large K), can
actually worsen learning of the generative model as well; a finding we explore in Section 4.3.
Optimizing the iwae objective using sgd methods requires unbiased gradient estima-
tors of ELBOIKS (θ, φ, X) with respect to θ and φ (Robbins & Monro, 1951). The θ
gradient Vθ ELBOK(θ,Φ,x) is estimated by sampling zi：K 〜 Qφ(∙∣χ) and evaluating
Vθ log (六 PK=I wk). While Vφ ELBOK (θ, φ, x) is estimated similarly for models with reparame-
terizable latents, discrete (and other non-reparameterizable) latents require the reinforce gradient
2
Under review as a conference paper at ICLR 2019
estimator (Williams, 1992)
gREINFORCE
log A X Wk) Vφ log Qφ(zi:K|x) + Vφ log
*{z
①
}|
K XXWk
k=1
{z^^^^^^^^^
②
(2)
|
}
2.2	Continuous Relaxation and Control Variate Methods
Since the gradient estimator in Eq. (2) can often suffer from high variance, mainly due to the ef-
fect of ①，a number of approaches have been developed to ameliorate the issue. When employing
continuous relaxation methods, all discrete latent variables in the model can be replaced by the Con-
crete (Maddison et al., 2017) or Gumbel-Softmax (Jang et al., 2017) distribution, whose gradients
can be approximated using the reparameterization trick (Kingma & Welling, 2014; Rezende et al.,
2014). The main practical difficulty here is tuning the temperature parameter: low temperatures re-
duce the gradient estimator bias, but rapidly increase its variance. Moreover, since such relaxations
are defined on a simplex, applying them to models that exhibit branching requires the computation-
ally expensive evaluation of all (potentially exponentially many) paths in the generative model.
Variational inference for Monte Carlo objectives (vimco) (Mnih & Rezende, 2016) is a method that
doesn’t require designing an explicit control variate, as it exploits the particle set obtained in iwae.
It replaces ① with
①
gVIMCO
XX	⅛ XX Wk)-log( ⅛
'=1	k = 1	/	∖
'-----{------} `-----
A
eK-T Pk=' lθg wk + X Wk)) V Vφ log qφ(z'∣x),
-	k='	)))
一■――	,
{z
B
where term B is independent of z` and highly correlated with term A.
Finally, assuming zk is a discrete random variable with C categories1 * , REBAR (Tucker et al., 2017)
and relax (Grathwohl et al., 2018) improve on the methods of Mnih & Gregor (2014) and Gu et al.
(2016), and replaces ① with
gRELAX
①
-cρ(g1:K) I vφ log Qφ(z1:K IX) + V0cp(g1:K) - V0cp(g1:K),
where gk is a C-dimensional vector of reparameterized Gumbel random variates, zk is a one-hot
argmax function of gk, and gk is a vector of reparameterized conditional Gumbel random variates
conditioned on zk. The conditional Gumbel random variates are a form of Rao-Blackwellization
used to reduce variance. The control variate cρ, parameterized by ρ, is optimized to minimize the
gradient variance estimates concurrently with the main elbo optimization, leading to state-of-the-
art performance on, for example, sigmoid belief networks (Neal, 1992). The main practical difficulty
in using this method is choosing a suitable family of cρ, as some choices lead to higher variance
despite the concurrent gradient variance minimization. Moreover, the objective for the concurrent
optimization requires evaluating a Jacobian-vector product that induces an overhead of O(DφDρ)
where Dφ, Dρ are number of inference network and control-variate parameters respectively.
3	Revisiting Reweighted Wake-Sleep
Reweighted wake-sleep (rws) (Bornschein & Bengio, 2015) comes from another family of algo-
rithms (Hinton et al., 1995; Dayan et al., 1995) for learning deep generative models, eschewing a
single objective over parameters θ and φ in favour of individual objectives for each. We review the
rws algorithm and discuss its advantages and disadvantages.
1The assumption is needed only for notational convenience. However, using more structured latents leads
to difficulties in picking the control-variate architecture.
3
Under review as a conference paper at ICLR 2019
3.1	Reweighted Wake-Sleep
Reweighted wake-sleep (rws) (Bornschein & Bengio, 2015) is an extension of the wake-sleep al-
gorithm (Hinton et al., 1995; Dayan et al., 1995) both of which, like iwae, jointly learn a generative
model and an inference network given data. While iwae targets a single objective, rws alternates
between objectives, updating the generative model parameters θ using a wake-phase θ update and
the inference network parameters φ using either a sleep- or a wake-phase φ update (or both).
Wake-phase θ update. Given a current value of φ, θ is updated using an unbiased estimate of
Vθ (得 PnN=I ELBOK(θ, Φ, x(n))), which can be obtained directly without needing reparameteri-
zation or control variates as the sampling distribution Qφ(∙∣χ) is independent of θ.2
Sleep-phase φ update. Here, φ is updated to maximize the negative Kullback-Leibler (KL) diver-
gence between the posteriors under the generative model and the inference network, averaged over
the data distribution of the current generative model
Epθ(x)[-DκL(pθ(z∣x),qφ(z∣x))] = Epθ(z,x)[logqφ(z∣x) - logPθ(z|x)].	(3)
The gradient of this objective is Ep@(z,x)[Vφ log qφ(z∣χ)] and can be estimated by sampling z, X from
the generative model pθ(z,χ) and evaluating Vφ log qφ(z∣χ). The variance of such an estimator can
be reduced at a standard Monte Carlo rate by increasing the number of samples of z, x.
Wake-phase φ update. Here, φ is updated to maximize the negative KL divergence between the
posteriors under the generative model and the inference network, averaged over the true data distri-
bution
ep(x)[-dkl(pθ (ZIx),qφ(ZIx))] = Ep(x)[Epθ (z∣x)[log qφ(HX)- log pθ (ZIx)]].	(4)
The outer expectation of the gradient Ep(X) [Ep@ (z∣χ) [Vφ log qφ(z∣x)]] canbe estimated using a single
sample x from the true data distribution p(x), given which, the inner expectation can be estimated
using a self-normalized importance sampler with K particles using qφ(ZIx) as the proposal distri-
bution. This results in the following estimator
KK
wk/	w` Vφlogqφ(ZkIx),	(5)
k = 1 '	'=1	)
where x 〜 p(x), Zk 〜 qφ(zk∣x), and Wk = pθ(Zk,x)∕qφ(Zk∣x), in a similar fashion to Eq. (1).
Note that equation 5 is negative of the second term of the reinforce estimator of the iwae elbo
in equation 2. The crucial difference of the wake-phase φ update to the sleep-phase φ update is that
the expectation in Eq. (4) is over the true data distribution p(x) unlike the expectation in Eq. (3)
which is under the current model distribution pθ(x). The former is desirable from the perspective
of amortizing inference over data from p(x). Although the estimator in Eq. (5) is biased, this bias
decreases as K increases.
While Bornschein & Bengio (2015) refer to the use of the sleep-phase φ update followed by an
equally-weighted mixture of the wake-phase and sleep-phase updates ofφ, we here use RWS to refer
to the variant that employs just the wake-phase φ update, and not the mixture. The rationale for our
preference will be made clear from the empirical evaluations (Section 4).
3.2	Advantages of Reweighted Wake-Sleep
While the gradient update of θ targets the same objective as IWAE, the gradient update of φ targets
the objective in Eq. (3) in the sleep case and Eq. (4) in the wake case. This leads to two advantages
for rws over iwae. First, since we don’t need to use reinforce, using rws leads to much lower
variance of gradient estimators of φ. Second, the φ updates in RWS directly target minimization
of the expected kl divergences from true to approximate posteriors. Increasing the computational
budget (using more Monte Carlo samples in the sleep-phase φ update case and higher number of
2We assume that the deterministic mappings induced by the parameters θ, φ are themselves differentiable,
such that they are amenable to gradient-based learning.
4
Under review as a conference paper at ICLR 2019
particles K in the wake-phase φ update) results in a better estimator of these expected KL diver-
gences. This is different to iwae, where optimizing elb oKIS targets a kl divergence on an extended
sampling space (Le et al., 2018) which for K > 1 doesn’t correspond to a KL divergence between
true and approximate posteriors (in any order). Consequently, increasing number of particles in
iwae leads to worse learning of inference networks (Rainforth et al., 2018).
3.3	Disadvantages of Reweighted Wake-Sleep
The objective of the sleep-phase φ update in equation 3 is an expectation of KL under the current
model distribution pθ (x) rather than the true one p(x). This makes the sleep-phase φ update sub-
optimal since the inference network must always follow a “doubly moving” target (both pθ (x) and
Pθ (z|x) change during the optimization).
4	Experiments
The iwae and rws algorithms have primarily been applied to problems with continuous latent
variables and/or discrete latent variables that do not actually induce branching such as sigmoid
belief networks (Neal, 1992). The purpose of the following experiments is to compare rws to
iwae used alongside the control variate and continuous relaxation methods described in Section 3
on models with conditional branching, where, as we will show, the various control-variate schemes
underperform in relation to rws. In several ways, including elbos achieved and average distance
between true and amortized posteriors, we empirically demonstrate that increasing the number of
particles K hurts learning in IWAE but improves learning in RWS.
The first experiment, using the deep generative model from Attend, Infer, Repeat (air) (Eslami
et al., 2016), demonstrates better learning of the generative model in a model containing both dis-
crete latent variables used for branching as well as continuous latent variables in a complex visual
data domain (Section 4.1). The next experiment on mnist (Section 4.2) does so in a model with con-
tinuous latent variables. Finally, a gmm experiment (Section 4.3) serves as a pedagogical example
to understand sources of advantage for rws in more detail.
Notationally, the different variants of rws will be referred to as wake-sleep (ws) and wake-wake
(WW). The wake-phase θ update is always used. We refer to using it in conjunction with the
sleep-phase φ update as WS and using it in conjunction with the wake-phase φ update as WW. We
tried using both wake- and sleep-phase φ updates however, in addition to doubling the amount of
sampling, found that doing so only improves performance on the continuous latent variable model.
The number of particles K used for the wake-phase θ and φ updates will always be specified. The
sleep phase φ update will also use K samples from pθ (z, x).
4.1	Attend, Infer, Repeat
First, we evaluate ww and vimco on air (Eslami et al., 2016), a structured deep generative model
with both discrete and continuous latent variables. Air uses the discrete variable to decide how many
continuous variables are necessary to explain an image (see supplementary material for details).
The sequential inference procedure of air poses a difficult problem, since it implies a sequential
decision process with possible branching. See (Eslami et al., 2016) for the details of the model (see
supplementary material for the model in our notation).
We set the maximum number of inference steps in AIR to three and we train it on images of size 50 ×
50 with zero, one or two MNIST digits. The training and testing data sets consist of 60000 and 10000
images, respectively, which are generated from the respective mnist train/test datasets. Unlike air,
which used Gaussian likelihood with fixed standard deviation and continuous inputs (i.e., input
x ∈ [0, 1]50×50), we use a Bernoulli likelihood and binarized data; the stochastic binarization is the
same as in Burda et al. (2016). This choice is motivated by initial experiments, which have shown
that the original setup is detrimental to the sleep phase of WS - samples from the generative model
did not look similar to true data even after the training has converged. Training is performed over
two million iterations by RmsProp (Tieleman & Hinton, 2012) with the learning rate of 10-5, which
is divided by three after 400k and 1000k training iterations. We set the glimpse size to 20 × 20.
5
Under review as a conference paper at ICLR 2019
We first evaluate the generative model via the average test log marginal where each log marginal is
estimated by a one-sample, 5000-particle IWAE estimate. The inference network is then evaluated
via the average test kl from the inference network to the posterior under the current model where
each DκL(qφ(z∣χ),Pθ(z|x)) is estimated as a difference between the log marginal estimate above
and a 5000-sample, one-particle IWAE estimate. Note that this KL estimate is merely a proxy to the
desired KL from the inference network to the posterior under the true model.
This experiment confirms (Fig. 1) that increasing number of particles hurts learning in vimco but
improves learning in WW. Increasing K improves WW monotonically but VIMCO only up to a point.
Ww also results in significantly lower variance and better inference networks than vimco.
4.2	Continuous Latent Variable Model
rws has typically only been considered for learning models with discrete latent variables. In order
to gauge its applicability to a wider class of problems, we evaluate it on a variational autoencoder
with normally distributed latent variables for mnist. To do this, we use the training procedure and
the model with a single stochastic layer of Burda et al. (2016) and their stochastic binarization of
data.
To train the model, we use the Adam optimizer (Kingma & Ba,
2015) with the learning rate of 10-3 annealed over the course
of about 3000 epochs (9840000 iterations) to the value of 10-4
with the batch size of 32. We evaluate performance in the same
way as we evaluate the air model. Additionally, we evaluate
the number of active latent units (Burda et al., 2016).
This experiment confirms (Fig. 2 and Table 1) that increasing
number of particles hurts learning in iwae but improves learn-
ing in WW. Increasing K does improve the marginal likelihood
Table 1: Number of active latent
units evaluated on the test set.
K IWAE	WW
8	23.00 ± 0.00
16	23.75 ± 0.43
32	24.50 ± 0.50
64	25.00 ± 0.00
23.75 ± 0.83
25.00 ± 0.00
25.00 ± 0.00
26.50 ± 0.50
attained by ww and iwae with reparameterization. However, the latter learns a better model only
up to a point (K = 128) — further increase in the number of particles has diminishing returns. WW
also results in better inference networks than iwae as showed by the kl plot on the right of Fig. 2.
4.3 WHY?
To see what is going on, we study a gmm which branches on a discrete
latent variable to select cluster assignments. The generative model and
inference network are defined as
Pθ(Z) = Categorical(z∣softmax(θ)), p(x∣z) = Normal(x∣μz,σZ),
qφ(z∣x) = Categorical(z∣softmax(ηφ(x))),
0	190
Figure 3: True gmm.
where Z ∈ {0,...,C -1}, C is the number of clusters and μc, σ2 are fixed to μc = 10c and σ2 = 52.
The generative model parameters are θ ∈ RC . The inference network consists of a multilayer
perceptron ηφ : R → RC, with the 1-16-C architecture and the tanh nonlinearity, parameterized by
φ. The chosen family of inference networks is empirically expressive enough to capture the posterior
0	200	400	600	800	1000
epoch
τ)s90-
IWAE+VIMCO
WW
5	10	20	40	80
number of particles
0	200	400	600	800	1000
epoch
Figure 1: Training of AIR. (Left) Training curves: training with VIMCO leads to larger variance
in training than WW. (Middle) Log evidence values at the end of training: increasing number of
particles improves WW monotonically but improves VIMCO only up to a point (K = 10 is the best).
(Right) WW results in significantly lower variance and better inference networks than VIMCO. Note
that KL is between the inference network and the current generative model.
((X-Z)8= (Xaod
6
6
Under review as a conference paper at ICLR 2019
Figure 2: Training of a continuous latent variable model on MNIST. (Left) Training curves: the
variance in training is comparable between WW and IWAE with reparameterization. (Middle) Log
evidence values at the end of training: increasing number of particles improves both ww and iwae
with reparameterization; the latter learns a better model only up to a point (K = 128) and suffers
from diminishing returns afterwards. (Right) WW results in better inference networks than IWAE.
Note that KL is between the inference network and the current generative model.
Iol
1010V
1
.1S3 lupe-le
6 J。 ∙pls

0	Iteration 100000 0	Iteration 100000 0	Iteration 100000 0	Iteration 100000
---Concrete  RELAX  REINFORCE  VIMCO   WS   WW   δ-WW
Figure 4: GMM training. Median and interquartile ranges from 10 repeats shown. (Top) Quality
of the generative model: ws and ww improve with larger particle budget thanks to lower variance
and lower bias estimators of the gradient respectively. Iwae methods suffer with a larger particle
budget (Rainforth et al., 2018). Ws performs the worst as a consequence of computing the expected
KL under model distribution pθ(x) equation 3 instead of the true data distribution p(x) as with
ww equation 4. Ww suffers from zero-forcing (described in text) in low-particle regimes, but learns
the best model fastest in the many-particle regime; δ-WW additionally learns well in the low-particle
regime. (Middle) The quality of the inference network develops identically to that of the generative
model. (Bottom) WW and WS have lower-variance gradient estimators of φ than IWAE, as they don’t
include the high-variance term ① in equation 2. This is a necessary, but not sufficient, condition for
efficient learning with other important factors being gradient direction and the ability to escape local
optima.
under the true model. The true model is set to pθtrue (x) where softmax(θtrue)c = (c+5)/ PiC=1(i+5)
(c = 0, . . . , C - 1), i.e. the mixture probabilities are linearly increasing with the z (Fig. 3). We fix
the mixture parameters in order to study the important features of the problem at hand in isolation.
Iwae was trained with reinforce, relax, vimco and the Concrete distribution. We also train
using WS and WW. We fix C = 20 and increase number of particles from K = 2 to 20. We
use the Adam optimizer with the learning rate 10-3 and default β parameters. At each iteration,
a batch of 100 data points is generated from the true model to train. Having searched over sev-
eral temperature schedules for the Concrete distribution, we use the one with the lowest trainable
terminal temperature (linearly annealing from 3 to 0.5). We found that using the control variate
Cp(gi：K) = Kk PK=I MLPρ([χ, gk]), where the architecture of the multilayer perceptron (MLP) is
7
Under review as a conference paper at ICLR 2019
(1 + C)-16-16-1 (with tanh nonlinearity) led to most stable training (see supplementary material
for more details).
We evaluate the generative model, inference network and the variance of the gradient estimator of φ.
The generative model is evaluated via the L2 distance between the probability mass functions (PMFs)
of its prior and true prior as ksoftmax(θ) - softmax(θtrue)k. The inference network is evaluated via
the L2 distance between PMFs of the current and true posteriors, averaged over a fixed set (M = 100)
of observations (X(Im))M=1 from the true model: M PM=1 kqφ(ZIx(Im)) - Pθtπje(ZIx(Im))k. Finany,
φ's gradient-estimator standard deviation is given by Dφ PDφ∖ std(gd) where gd is the dth element
of one of φ's gradient estimators (e.g. equation 2 for reinforce) and std(∙) is estimated using 10
samples.
Here, we demonstrate that using ws and ww with larger particle budgets leads to a better inference
networks whereas this is not the case for iwae methods (Fig. 4, Middle). Recall that the former is
because using more samples to estimate the gradient of the sleep φ objective equation 3 for WS re-
duces variance at a standard Monte Carlo rate and that using more particles in equation 5 to estimate
the gradient of the wake φ objective results in a lower bias. The latter is because using more particles
results in the signal-to-noise of iWAE's φ gradient estimator to drop at the rate O(1/√K) (Rainforth
et al., 2018).
Learning of the generative model, as a consequence of inference-network learning, is also better for
ws and ww, but worse for iwae methods when the particle budget is increased. This is because the
the θ gradient estimator (common to all methods), Vθ ELBOK (θ, Φ, x) Can be seen as an importance
sampling estimator whose quality is tied to the proposal distribution (inference network).
Ww and ws have lower variance gradient estimators than iwae, even if used with control-variate
and continuous-relaxation methods. This is because φ's gradient estimators for WW and WS don't
include the high-variance term ① in equation 2. This is a necessary but not sufficient condition for
efficient learning with other important factors being gradient direction and the ability to escape local
optima (explored below). Employing the Concrete distribution gives low-variance gradients for φ
to begin with, but the model learns poorly due to the high gradients bias (due to high temperature
hyperparameter).
K = 2	K = 20
Pθ(z)	qφ(z∣x =	0)	qφ(z∣x = 95)	qφ(z∣x = 190)	Pθ(z)	qφ(z∣x = 0)	qφ(z∣x = 95)	qφ(z∣x = 190)
0 上 。。06寸」3± OoO66 -3上
ZZZZZZZZ
RELAX	REINFORCE	VIMCO	WS	WW	δ-WW
Figure 5: Generative model and inference network during gmm training shown as Hinton diagrams
where areas are proportional to probability. Z goes from 0 to 19, left to right. Rows correspond
to start, middle and end of optimization. (Left half) Learning with few particles leads to the zero-
forcing (described in text) of the inference network (shown as conditional PMF given different x) and
the generative model (first column of each half) for all methods except δ-WW. Concrete distribution
fails. (Right half) Learning with many particles leads to zero-forcing only for WS; WW and δ-WW
succeed where iwae fails, learning a suboptimal final generative model.
We now describe a failure mode, that affects ws, ww, vimco and reinforce, which we will refer
to as zero-forcing. it is best illustrated by inspecting the generative model and the inference network
8
Under review as a conference paper at ICLR 2019
as the training progresses, focusing on the low-particle (K = 2) regime (Fig. 5). For WS, the
generative model pθ(Z) peaks at Z = 9 and puts zero mass for z > 9; the inference network qφ(z∣x)
becomes the posterior for this model which, in this model, also has support at most {0, . . . , 9}
for all x. This is a local optimum for WS because (i) the inference network already approximates
the posterior of the model pθ (Z, x) well, and (ii) the generative model pθ (Z), being trained using
samples from qφ(z∣χ), has no samples outside of its current support ({0,..., 9}). Similar failure
mode occurs for WW and VIMCO/REINFORCE although the support of the locally optimal pθ (Z) is
larger ({0, . . . , 14} and {0, . . . , 17} respectively).
While this failure mode is a particular feature of the gmm, we hypothesize that ws and ww suffer
from it more, as they alternate between two different objectives for optimizing θ and φ. WS attempts
to amortize inference for the current model distribution pθ (x) which reinforces the coupling between
the generative model and the inference network, making it easier to get stuck in a local optimum.
WW with few particles (say K = 1) on the other hand, results in a highly-biased gradient estimator
equation 5 that samples Z from qΦ(∙∣χ) and evaluates Vφ log qφ(z∣χ); this encourages the inference
network to concentrate mass. This behavior is not seen in ww with many particles where it is the
best algorithm at learning both a good generative model and inference network (Fig. 4; Fig. 5, right).
We propose a simple extension of WW, denoted δ-WW, that mitigates this shortcoming by chang-
ing the proposal of the self-normalized importance sampling estimator in Eq. (5) to qφ,δ(z∣χ) =
(1 一 δ)qφ(z∣x) + δUniform(z). We use δ = 0.2, noting that the method is robust to a range of
values. Using a different proposal than the inference network qφ(z∣χ) means that using the low-
particle estimator in Eq. (5) no longer leads to zero-forcing. This is known as defensive importance
sampling (Hesterberg, 1995), and is used to better estimate integrands that have long tails using
short-tailed proposals. Using δ-WW outperforms all other algorithms in learning both the generative
model and the inference network.
5 Conclusion
Our experiments suggest that rws learns both better generative models and inference networks
in models that involve discrete latent variables, while performing just as well as state-of-the-art
on continuous-variable models as well. The air experiment (Section 4.1) shows that the trained
inference networks are unusable when trained with high number of particles. Moreover, the mnist
experiment (Section 4.2) suggests that rws is competitive even on models with continuous latent
variables, especially for high number of particles where iwae elbo starts suffering from worse
inference networks. The gmm experiment (Section 4.3) illustrates that this is at least at least in part
due to a lower variance gradient estimator for the inference network and the fact that for rws—
unlike the case of optimizing iwae elbo (Rainforth et al., 2018)—increasing number of particles
actually improves the inference network. In the low-particle regime, the gmm suffers from zero-
forcing of the generative model and the inference network, which is ameliorated using defensive
rws. Finally, all experiments show that, beyond a certain point, increasing the particle budget starts
to affect the quality of the generative model for iwae elbo whereas this is not the case for rws. As
a consequence of our findings, we recommend reconsidering using rws for learning deep generative
models, especially those containing discrete latent variables that induce branching.
References
Ryan Adams, Hanna Wallach, and Zoubin Ghahramani. Learning the structure of deep sparse graph-
ical models. In International Conference on Artificial Intelligence and Statistics, 2010.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine
Learning research, 3(Jan):993-1022, 2003.
Jorg Bornschein and Yoshua Bengio. Reweighted wake-sleep. In International Conference on
Learning Representations, 2015.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In Inter-
national Conference on Learning Representations, 2016.
Nick Chater and Christopher D Manning. Probabilistic models of language processing and acquisi-
tion. Trends in cognitive sciences, 10(7):335-344, 2006.
9
Under review as a conference paper at ICLR 2019
Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The Helmholtz machine.
Neural computation,7(5):889-904, 1995.
S.	M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray
Kavukcuoglu, and Geoffrey E. Hinton. Attend, infer, repeat: Fast scene understanding with
generative models. In Advances in Neural Information Processing Systems, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. In International
Conference on Learning Representations, 2018.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing machines. arXiv preprint
arXiv:1410.5401, 2014.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538
(7626):471, 2016.
Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to
transduce with unbounded memory. In Advances in Neural Information Processing Systems, pp.
1828-1836, 2015.
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation
for stochastic neural networks. In International Conference on Learning Representations, 2016.
Tim Hesterberg. Weighted average importance sampling and defensive mixture distributions. Tech-
nometrics, 37(2):185-194, 1995.
Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The “wake-sleep” algorithm
for unsupervised neural networks. Science, 268(5214):1158-1161, 1995.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-softmax. In
International Conference on Learning Representations, 2017.
Biing Hwang Juang and Laurence R Rabiner. Hidden markov models for speech recognition. Tech-
nometrics, 33(3):251-272, 1991.
Charles Kemp, Joshua B Tenenbaum, Thomas L Griffiths, Takeshi Yamada, and Naonori Ueda.
Learning systems of concepts with an infinite relational model. 2006.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference
on Learning Representations, 2014.
Brenden M Lake, Neil D Lawrence, and Joshua B Tenenbaum. The emergence of organizing struc-
ture in conceptual representation. Cognitive science, 2018.
Tuan Anh Le, Atilim Gunes Baydin, and Frank Wood. Inference compilation and universal proba-
bilistic programming. In International Conference on Artificial Intelligence and Statistics, 2017.
Tuan Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and Frank Wood. Auto-encoding sequential
Monte Carlo. In International Conference on Learning Representations, 2018.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous re-
laxation of discrete random variables. In International Conference on Learning Representations,
2017.
10
Under review as a conference paper at ICLR 2019
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
International Conference on Machine Learning, pp. 1791-1799, 2014.
Andriy Mnih and Danilo Rezende. Variational inference for Monte Carlo objectives. In International
Conference on Machine Learning, pp. 2188-2196, 2016.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv
preprint arXiv:1610.03483, 2016.
Radford M Neal. Connectionist learning of belief networks. Artificial intelligence, 56(1):71-113,
1992.
Willie Neiswanger, Frank Wood, and Eric Xing. The dependent Dirichlet process mixture of objects
for detection-free tracking and object modeling. In Artificial Intelligence and Statistics, pp. 660-
668, 2014.
Brooks Paige and Frank Wood. Inference networks for sequential monte carlo in graphical models.
In International Conference on Machine Learning, pp. 3040-3049, 2016.
Tom Rainforth, Adam R Kosiorek, Tuan Anh Le, Chris J Maddison, Maximilian Igl, Frank Wood,
and Yee Whye Teh. Tighter variational bounds are not necessarily better. In International Con-
ference on Machine Learning, 2018.
Carl Edward Rasmussen. The infinite Gaussian mixture model. In Advances in neural information
processing systems, pp. 554-560, 2000.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
T.	Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar:
Low-variance, unbiased gradient estimates for discrete latent variable models. In Advances in
Neural Information Processing Systems, pp. 2624-2633, 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International Conference on Machine Learning, pp. 2048-2057, 2015.
11