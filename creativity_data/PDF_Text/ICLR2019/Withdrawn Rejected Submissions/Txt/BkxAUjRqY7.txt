Under review as a conference paper at ICLR 2019
AN INFORMATION-THEORETIC METRIC OF
TRANSFERABILITY FOR TASK TRANSFER LEARNING
Anonymous authors
Paper under double-blind review
ABSTRACT
An important question in task transfer learning is to determine task transferabilï¿¾ity, i.e. given a common input domain, estimating to what extent representations
learned from a source task can help in learning a target task. Typically, transferabilï¿¾ity is either measured experimentally or inferred through task relatedness, which
is often defined without a clear operational meaning. In this paper, we present a
novel metric, H-score, an easily-computable evaluation function that estimates the
performance of transferred representations from one task to another in classification
problems. Inspired by a principled information theoretic approach, H-score has
a direct connection to the asymptotic error probability of the decision function
based on the transferred feature. This formulation of transferability can further be
used to select a suitable set of source tasks in task transfer learning problems or to
devise efficient transfer learning policies. Experiments using both synthetic and
real image data show that not only our formulation of transferability is meaningful
in practice, but also it can generalize to inference problems beyond classification,
such as recognition tasks for 3D indoor-scene understanding.
1 INTRODUCTION
Transfer learning is a learning paradigm that exploits relatedness between different learning tasks in
order to gain certain benefits, e.g. reducing the demand for supervision (Pratt (1993)). In task transfer
learning, we assume that the input domain of the different tasks are the same. Then for a target task
TT , instead of learning a model from scratch, we can initialize the parameters from a previously
trained model for some related source task TS. For example, deep convolutional neural networks
trained for the ImageNet classification task have been used as the source network in transfer learning
for target tasks with fewer labeled data (Donahue et al. (2014)), such as medical image analysis (Shie
et al. (2015)) and structural damage recognition in buildings (Gao & Mosalam).
An imperative question in task transfer learning is transferability, i.e. when a transfer may work
and to what extent. Given a metric capable of efficiently and accurately measuring transferability
across arbitrary tasks, the problem of task transfer learning, to a large extent, is simplified to search
procedures over potential transfer sources and targets as quantified by the metric. Traditionally,
transferability is measured purely empirically using model loss or accuracy on the validation set
(Yosinski et al. (2014); Zamir et al. (2018); Conneau et al. (2017)). There have been theoretical studies
that focus on task relatedness (Baxter (2000); Maurer (2009); Pentina & Lampert (2014); Ben-David
et al. (2003)). However, they either cannot be computed explicitly from data or do not directly explain
task transfer performance. In this study, we aim to estimate transferability analytically, directly from
the training data.
We quantify the transferability of feature representations across tasks via an approach grounded in
statistics and information theory. The key idea of our method is to show that the error probability of
using a feature of the input data to solve a learning task can be characterized by a linear projection of
this feature between the input and output domains. Hence we adopt the projection length as a metric
of the featureâ€™s effectiveness for the given task, and refer to it as the H-score of the feature. More
generally, H-score can be applied to evaluate the performance of features in different tasks, and is
particularly useful to quantify feature transferability among tasks. Using this idea, we define task
transferability as the normalized H-score of the optimal source feature with respect to the target task.
1
Under review as a conference paper at ICLR 2019
As we demonstrate in this paper, the advantage of our transferability metric is threefold. (i) it has a
strong operational meaning rooted in statistics and information theory; (ii) it can be computed directly
and efficiently from the input data, with fewer samples than those needed for empirical learning;
(iii) it can be shown to be strongly consistent with empirical transferability measurements.
In this paper, we will first present the theoretical results of the proposed transferability metric in
Section 2-4. Section 5 presents several experiments on real image data , including image classificaton
tasks using the Cifar 100 dataset and 3D indoor scene understanding tasks using the Taskonomy
dataset created by Zamir et al. (2018). A brief review of the related works is included in Section 6.
2 BACKGROUND
In this section, we will introduce the notations used throughout this paper, as well as some related
concepts in Euclidean information theory and statistics.
X, x, X and PX represent a random variable, a value, the alphabet and the probability distribution
respectively. âˆšPX denotes the vector with entries p PX(x) and [âˆšPX] the diagonal matrix of
p
PX(X). For joint distribution PY X, PY X represents the |Y| Ã— |X | probability matrix. Depending
on the context, f(X) is either a |X |-dimensional vector whose entries are f(x), or a |X | Ã— k feature
matrix. Further, we define a task to be a tuple T = (X, Y, PXY ), where X is the training features
and Y is the training label, and PXY the joint probability (possibly unknown). Subscripts S and T
are used to distinguish the source task from the target task.
2.1 LOCAL INFORMATION GEOMETRY AND HYPOTHESIS TESTING
Our definiton of transferability uses concepts in local information geometry developed by Makur et al.
(2015), which characterizes probability distributions as vectors in the information space. Consider
the following binary hypothesis testing problem: test whether i.i.d. samples xm = {x(i)}mi=1 are
drawn from distribution P1 or distribution P2, where P1, P2 belong to an  -neighborhood N (P0) , {P|P xâˆˆX
(P (x)âˆ’P0(x))2 P0(x) â‰¤  2} centered at a reference distributon P0. And denote Ï†i = Pi(x)âˆ’P0(x)  âˆšP0(x)
as the information vector corresponding to Pi for i = 1, 2.
Given k normalized feature functions f(x) = [f1(x), . . . , fk(x)], let Î(x) = [Î¾1(x), . . . , Î¾k(x)] be
the matrix of feature vectors Î¾i(x) = p P0(x)fi(x). Given observations xm, feature f is associated
with a k-dimensional statistics l(xm) = 1m P mi=1(f(x(i)
)) for the binary hypothesis testing problem.
Let Ef be the error exponent of decision region {xm | l(xm) > T} for T â‰¥ 0, which characterizes
the asymptotic error probability Pe of l (i.e. limmâ†’âˆ âˆ’ 1m log(Pe) = Efk
). Ef can be written as the
squared length of a projection:
Efk = kXi=1
Efi = kXi=l  28 h Î¾i
, Ï†1 âˆ’ Ï†2i 2 + o( 2) (1)
When f(x) = log P1(x) P2(x)
is the log likelihood ratio, l is the minimum sufficient statistics that achieves
the largest error exponent Efk = P ki=1

28
||Ï†1 âˆ’ Ï†2||2 + o( 2) by the Chernoff theorem. (See
Appendix A for details.) In the rest of this paper, we assume  is small.
2.2 DIVERGENCE TRANSITION MATRIX
Definition 1. Matrix ËœB is the Divergence Transition Matrix (DTM) of a joint probability PY X if
ËœB =  âˆšPY  âˆ’1 PY X  âˆšPX âˆ’1 âˆ’ âˆšPY âˆšPXT.
The singular values of ËœB satisfy that 1 â‰¥ Ïƒ1 â‰¥ Â· Â· Â· â‰¥ ÏƒK = 0, K , min{|X |, |Y|}. Let Î¨ =
[Ïˆ1, . . . , ÏˆK] and Î¦ = [Ï†1, . . . , Ï†K] be the left and right singular vectors of ËœB. Define functions
fiâˆ—(x) = Ï†i(x) âˆšPX(x)
and giâˆ—(y) = Ïˆi(y) âˆšPY (y)
for each i = 1, . . . , K âˆ’ 1. Makur et al. (2015) further
2
Under review as a conference paper at ICLR 2019
proved that fiâˆ—
and giâˆ—
are solutions to the maximal HGR correlation problem studied by Hirschfeld
(1935); Gebelein (1941); RÂ´enyi (1959), defined as follows:
Ï(X; Y ) = sup
f : X â†’ Rk
, g : Y â†’ Rk E[f(X)] = E[g(Y )] = 0
E[f(X)f(X)T
] = I E[f(X)T g(Y )] (2)
The maximal HGR problem finds the K strongest, independent modes in PXY from data. It can
be solved efficiently using the Alternating Conditional Expectation (ACE) algorithm with provable
error bound (see Appendix B). Huang et al. (2017) further showed that f âˆ—
and gâˆ—
are the universal
minimum error probability features in the sense that they can achieve the smallest error probability
over all possible inference tasks.
3 MEASURING THE EFFECTIVENESS OF FEATURES IN CLASSIFICATION TASKS
In this section, we present a performance metric of a given feature representation for a learning task.
3.1 H-SCORE
For a classification task involving input variable X and label Y , most learning algorithms work
by finding a k-dimensional functional representation f(x) of X that is most discriminative for the
classification. To measure how effective f(x) is in predicting Y , rather than train the model via
gradient descent and evaluate its accuracy, we present an analytical approach based on the definition
below:
Definition 2. Given data matrix X âˆˆ RmÃ—d and label Y âˆˆ {1, . . . , |Y|}. Let f(x) be a kï¿¾dimensional, zero-mean feature function. The H-Score of f(x) with respect to the learning task
represented by PY X is:
H(f) = tr(cov(f(X))âˆ’1
cov(EPX|Y [f(X)|Y ]))
This definition is intuitive from a nearest neighbor search perspective. i.e. a high H-score implies the
inter-class variance cov(EPX|Y [f(X)|Y ]) of f is large, while feature redundancy tr(cov(f(X))) is
small. Such a feature is discriminative and efficient for learning label Y. More importantly, H(f) has
a deeper operational meaning related to the asymptotic error probability for a decision rule based on
f in the hypothesis testing context, discussed in the next section.
3.2 OPERATIONAL MEANING OF H-SCORE
Without loss of generality, we consider the binary classification task as a hypothesis testing problem
defined in Section 2.1, with P1 = PX|Y =0, and P2 = PX|Y =1. For any k-dimensional feature
representation f(x), we can quantify its performance with respect to the learning task using its error
exponent Efk.
Theorem 1. Given PX|Y =0, PX|Y =1 âˆˆ N X (P0,X) and features f such that E [f(X)] = 0 and
E[f(X)f(X)T] = I, there exists some constant c independent of f such that Ekf = cH(f).
See Appendix C for the proof. The above theorem shows that H-score H(f) is proportional to the
error exponent of the decision region based on f(x) when f(x) is zero-mean with identity covariance.
To compute the H-score of arbitrary f, we can center the features fS(x) âˆ’ E[fS(x)], and incorporate
normalization into the computation of the error exponent, which results in Definition 2. The details
are presented in Appendix D.
The proof for Theorem 1 uses the fact that H(f) = k ËœBÎk 2F
, where ËœB is the DTM matrix, Î , [Î¾1 Â· Â· Â· Î¾k] is the matrix composed of information vectors Î¾i and c is a constant. This allows us to
infer an upper bound for the H-score of a given learning task:
Corollary 1. For all normalized feature f(x) = [f1(x), . . . , fk(x)], its H-score satisfies H(f) â‰¤ P ki=1 Ïƒi2 â‰¤ k, where Ïƒi
is the ith singular value of ËœB. 3
Under review as a conference paper at ICLR 2019
YT fS(x) 
input X
Ys
source task
target task
frozen layers
free layers
Figure 1: A simple neural network topology for transfer learning
The first inequality is achieved when Î is composed of the right singular vectors of ËœB, i.e.
maxÎ || ËœBÎ||2F = || ËœBT ||2F
. The corresponding feature functions fopt(X)is in fact the same as the
universal minimum error probability features from the maximum HGR correlation problem. The final
inequality in Corollary 1 is due to the fact all singular values of ËœB are less than or equal to 1.
4 TRANSFERABILITY
Next, we apply H-score to efficiently measure the effectiveness of task transfer learning. We will also
discuss how this approach can be used to solve the source task selection problem.
4.1 TASK TRANSFERABILITY
A typical way to transfer knowledge from the source task TS to target task TT is to train the target task
using source feature representation fS(x). In a neural network setting, this idea can be implemented
by copying the parameters from the first N layers in the trained source model to the target model,
assuming the model architecture on those layers are the same. The target classifier then can be trained
while freezing parameters in the copied layers (Figure 1). Under this model, a natural way to quantify
transferability is as follows:
Definition 3 (Task transferability). Given source task TS and target task TT , and trained source
feature representation fS(x), the transferability from TS to TT is T(S, T) , HT (fS ) HT (fTopt )
, where
fTopt(x) is the minimum error probability feature of the target task.
The statement T(S, T) = r means the error exponent of transfering from TS via feature representation
fS is 1r
of the optimal error exponent for predicting the target label YT . This definition also implies
0 â‰¤ T(S, T) â‰¤ 1, which satisfies the data processing inequality if we consider the transferred feature
fS(X) as post-processing of input X for solving the target task. And it can not increase information
about predicting the target task T .
A common technique in task transfer learning is fine-tuning, which adds before the target classifier
additional free layers, whose parameters are optimized with respect to the target label. For the
operational meaning of transferability to hold exactly, we require the fine tuning layers consist of only
linear transformations, such as the model illustrated in Figure 2.a. It can be shown that under the local
assumption, H-score is equivalent to the log-loss of the linear transfer model up to a constant offset
(Appendix E). Nevertheless, later we will demonstrate empirically that this transferability metric can
still be used for comparing the relative task transferability with fine-tuning.
4.2 EFFICIENT COMPUTATION OF TRANSFERABILITY
With a known f, computing H-score from m sample data only takes O(mk2) time, where k is the
dimension of f(x) for k < m. The majority of the computation time is spent on computing the
sample covariance matrix cov(f(X)).
The remaining question is how to obtain fTopt efficiently. We use the fact that HT (fopt) = || ËœBT ||2F = E[f(X)Tg(Y )], where f and g are the solutions of the HGR-Maximum Correlation problem. This
problem can be solved efficiently using the ACE algorithm for discrete variable X. For a continuï¿¾ous random variable X, we can obtain fopt through a different formulation of the HGR maximal
4
Under review as a conference paper at ICLR 2019
W YT fS(x) 
input X
Ys
source task
fS(x)W target task
gs(y) gs(y) XY f(x) 
f
encoder
g
encoder
g(y) 
! " # $% & âˆ’ 12 *+(-./(" # -./ % & L
(a) (b)
Figure 2: a) Network topology of linear feature transfer. b) Architecï¿¾ture for the neural network implementation of the soft HGR problem
1 2 3 4 5 6
dim(f(x))
0.0
0.1
0.2
0.3
H-score: HY(f(x)) 2I(X; Y)
Figure 3: H-score and muï¿¾tual information.
correlation problem:
max
f:Xâ†’Rk,g:Yâ†’Rk E[f(X)Tg(Y )] âˆ’ 12
tr(cov(f(X))cov(g(Y )))
s.t. E[f(X)] = 0, E[g(Y )] = 0
(3)
This is also known as the soft HGR problem studied by Wang et al. (2019), who reformulated the
original maximal HGR correlation objective to eliminate the whitening constraints while having
theoretically equivalent solution. In practice, we can utilize neural network layers to model functions
f and g, as shown in Figure 2.b. Given two branches of k output units for both f and g, the loss
function can be evaluated in O(mk2), where m is the batch size. Makur et al. (2015) showed that the
sample complexity of ACE is only 1/k of the complexity of estimating PY,X directly. This result
also applies to the soft HGR problem due to their theoretical equivalence. Hence transferability can
be computed with much less samples than actually training the transfer network.
Itâ€™s also worth noting that, when f is fixed, maximizing the objective in Equation 3 with respect to
zero-mean function g results in the definition of H-score. In many cases though, the computation of
HT (fopt) can even be skipped entirely, such as the problem below:
Definition 4 (Source task selection). Given N source tasks TS1
, . . . , TSN with labels YS1
, . . . , YSN
and a target task TT with label YT . Let fS1
, . . . , fSN be the minimum error probability feature
functions of the source tasks. Find the source task TSi
that maximizes the testing accuracy of
predicting YT with feature fSi .
We can solve this problem by selecting the source task with the largest transferability to TT . In fact, we
only need to compute the numerator in the transferability definition since the denominator is the same
for all source tasks, i.e. argmaxi T(Si
, T) = argmaxi(HT (fSi,opt)/HT (fTopt) = argmaxi H(fSi ).
4.3 RELATIONSHIP WITH MUTUAL INFORMATION
Under the local assumption that PX|Y âˆˆ N (PX), we can show that mutual information I(X; Y ) =
12
||B||2F + o( 2). (See Appendix F for details.) Hence H-score is related to mutual information by
H(f(x)) â‰¤ 2I(X; Y ) for any zero-mean features f(x) satisfying the aforementioned conditions.
Figure 3 compares the optimal H-score of a synthesized task when |Y| = 6 with the mutual
information between input and output variables, when the feature dimension k changes. The value of
H-score increases as k increases, but reaches the upper bound when k â‰¥ 6, since the rank of the joint
probability between X and YT , as well as the rank of its DTM is 6. As expected, the H-score values
are below 2I(X; Y ), with a gap due to the constant o( 2). This relationship shows that H-score is
consistent with mutual information with a sufficiently large feature dimension k.
In practice, H-score is much easier to compute than mutual information when the input variable
X (or fS(X)) is continous, as mutual information are either computed based on binning, which
has extra bias due to bin size, or more sophisticated methods such as kernel density estimation or
neural networks (GabriÂ´e et al. (2018)). On the other hand, H-score only needs to estimate conditional
expectations, which requires less samples.
5
Under review as a conference paper at ICLR 2019
5 EXPERIMENTS
In this section, we validate and analyze our transferability metric through experiments on real image
data. 1 The tasks considered cover both object classification and non-classification tasks in computer
vision, such as depth estimation and 3D (occlusion) edge detection.
5.1 VALIDATION OF TRANSFER PERFORMANCE
To demonstrate that our transferability metric is indeed a suitable measurement for task transfer perforï¿¾mance, we compare it with the empirical performance of transfering features learned from ImageNet
1000-class classification (Krizhevsky et al. (2012)) to Cifar 100-class classification (Krizhevsky &
Hinton (2009)), using a network similar to Figure 1. Comparing to ImageNet-1000, Cifar-100 has
smaller sample size and its images have lower resolution. Therefore it is considered to be a more
challenging task than ImageNet, making it a suitable case for transfer learning. In addition, we use a
pretrained ResNet-50 as the source model due to its high performance and regular structure.
Validation of H-score. The training data for the target task in this experiemnt consists of 20, 000
images randomly sampled from Cifar-100. It is further split 9:1 into a training set and a testing set.
The transfer network was trained using stochastic gradient descent with batch size 20, 000 for 100
epochs.
Figure 4.a compares the H-score of transferring from five different layers (4a-4f) in the source network
with the target log-loss and test accuracy of the respective features. As H-score increases, log-loss of
the target network decreases almost linearly while the training and testing accuracy increase. Such
behavior is consistent with our expectation that H-score reflects the learning performance of the target
task. We also demonstrated that target sample size does not affect the relationship between H-score
and log-loss (Figure 4.b).
This experiment also showed another potential application of H-score for selecting the most suitable
layer for fine-tuning in transfer learning. In the example, transfer performance is better when an
upper layer of the source networks is transferred. This could be because the target task and the source
task are inherently similar such that the representation learned for one task can still be discriminative
for the other.
20 25 30 35 40 45
Log-loss
1.0
1.5
2.0
5K
10K
20K
30K
50K
(a) (b) (c)
Figure 4: H-score and transferability vs. the empirical transfer performance measured by log-loss,
training and testing accuracy. a.) Performance of ImageNet-1000 features from layers 4a-4f for Cifar-
100 classification. b.) Effect of sample size (5K-50K) on H-score for Cifar 100. c.): Transferability
from ImageNet-1000 to 4 different target tasks based on Cifar-100.
Validation of Transferability. We further tested our transferability metric for selecting the best
target task for a given source task. In particular, we constructed 4 target classification tasks with
3, 5, 10, and 20 object categories from the Cifar-100 dataset. We then computed the transferability
from ImageNet-1000 (using the feature representation of layer 4f) to the target tasks. The results are
compared to the empirical transfer performance trained with batch size 64 for 50 epochs in Figure4.c.
We observe a similar behavior as the H-score in the case of a single target task in Figure 4.a, showing
that transferability can directly predict the empirical transfer performance.
5.2 TASK TRANSFER FOR 3D SCENE UNDERSTANDING
In this experiment, we solve the source task selection problem for a collection of 3D sceneï¿¾understanding tasks using the Taskonomy dataset from Zamir et al. (2018). In the following, we will
1Test data and code can be found at https://goo.gl/uoXj8m
6
Hscore
Under review as a conference paper at ICLR 2019
introduce the experiment setting and explain how we adapt the transferability metric to pixel-to-pixel
recognition tasks. Then we compare transferability with task affinity, an empirical transferability
metric proposed by Zamir et al. (2018).
Data and Tasks. The Taskonomy dataset contains 4,000,000 images of indoor scenes of 600
buildings. Every image has annotations for 26 computer vision tasks. We randomly sampled 20, 000
images as training data. Eight tasks were chosen for this experiment, covering both classifications
and lower-level pixel-to-pixel tasks. Table 6 summaries the specifications of these tasks and sample
outputs are shown in Figure 5.
Figure 5: Ground truth of different tasks for a given query image.
Feature Extraction and Data Preprocessing. For each task, Zamir et al. (2018) trained a fully
supervised network with an encoder-decoder structure. When testing the transfer performance from
TS to TT , the encoder output of TS is used for training the decoder of TT . For a fair comparison,
we use the same trained encoders to extract source features. The output dimension of all encoders
are 16 Ã— 16 Ã— 8 and we flatten the output into a vector of length 2048. To reduce the computational
complexity, we also resize the ground truth images into 64 Ã— 64.
For classification tasks, H-score can be easily calculated given the source features. But for pixel-toï¿¾pixel tasks such as Edges and Depth, their ground truths are represented as images, which can not
be quantized easily. As a workaround, we cluster the pixel values in the ground truth images into
a palette of 16 colors. Then compute the H-score of the source features with respect to each pixel,
before aggregating them into a final score by averaging over the whole image.
We ran the experiment on a workstation with 3.40 GHz Ã—8 CPU and 16 GB memory. Each pairwise
H-score computation finished in less than one hour including preprocessing. Then we rank the source
tasks according to their H-scores of a given target task and compare the ranking with that in Zamir
et al. (2018).
Tasks k Output Quantize
-level
2D Edges 2048 images 16
3D Occlusion
Edges
2048 images 16
2D Keypoints 2048 images 16
3D Keypoints 2048 images 16
Reshading 2048 images 16
Depth 2048 images 16
Object Class. 2048 labels none
Scene Class. 2048 labels none
Figure 6: Task descriptions
Transferability Task Hierarchy Affinity Ranking Correlation
DCG
Figure 7: Ranking comparison between transferability and affinity
score.
Pairwise Transfer Results. Source task ranking results using transferability and affinity are
visualized side by side in Figure 7, with columns representing source tasks and rows representing
target tasks. For classification tasks (the bottom two rows in the transferability matrix), the top two
transferable source tasks are identical for both methods. The best source task is the target task itself,
as the encoder is trained on a task-specific network with much larger sample size. Scene Class. and
Object Class. are ranked second for each other, as they are semantically related. Similar observations
can be found in 2D pixel-to-pixel tasks (top two rows). The results on lower rankings are noisier.
A slightly larger difference between the two rankings can be found in 3D pixel-to-pixel tasks,
especially 3D Occlusion Edges and 3D Keypoints. Though the top four ranked tasks of both methods
7
Under review as a conference paper at ICLR 2019
are exactly the four 3D tasks. It could indicate that these low level vision tasks are closely related
to each other so that the transferability among them are inherently ambiguous. We also computed
the ranking correlations between transferability and affinity using Spearmanâ€™s R and Discounted
Cumulative Gain (DCG). Both criterion show positive correlations for all target tasks. The correlation
is especially strong with DCG as higher ranking entities are given larger weights.
The above observations inspire us to define a notion of task relatedness, as some tasks are frequently
ranked high for each other. Specifically, we represent each task with a vector consisting of H-scores
of all the source tasks, then apply agglomerative clustering over the task vectors. As shown in the
dendrogram in Figure 7, 2D tasks and most 3D tasks are grouped into different clusters, but on a
higher level, all pixel-to-pixel tasks are considered one category compared to the classifications tasks.
Figure 8: Ranking of second order transferability for all recogï¿¾nition tasks
Figure 9: First and second order
pixel-wise transferability to Depth.
Higher Order Transfer. Sometimes we need to combine features from two or more source tasks
for better transfer performance. A common way to combine features from multiple models in deep
neural networks is feature concatenation. For such problems, our transferability definition can be
easily adapted to high order feature transfer, by computing the H-score of the concatenated features.
Figure 8 shows the ranking results of all combinations of source task pairs for each target task.
For all tasks except for 3D Occlusion Edges and Depth, the best seond-order source feature is the
combination of the top two tasks of the first-order ranking. We examine the exception in Figure 9, by
visualizing the pixel-by-pixel H-scores of first and second order transfers to Depth using a heatmap
(lighter color implies a higher H-score). Note that different source tasks can be good at predicting
different parts of the image. The top row shows the results of combining tasks with two different
â€œtransferability patternsâ€ while the bottom row shows those with similar patterns. Combining tasks
with different transferability patterns has a more significant improvement to the overall performance
of the target task.
6 RELATED WORKS
Transfer learning. Transfer learning can be devided into two categories: domain adaptation, where
knowledge transfer is achieved by making representations learned from one input domain work on a
different input domain, e.g. adapt models for RGB images to infrared images (Wang & Deng (2018));
and task transfer learning, where knowledge is transferred between different tasks on the same input
domain (Torrey & Shavlik (2010)). Our paper focus on the latter prolem.
Empirical studies on transferability. Yosinski et al. (2014) compared the transfer accuracy of
features from different layers in a neural network between image classification tasks. A similar
study was performed for NLP tasks by Conneau et al. (2017). Zamir et al. (2018) determined the
optimal transfer hierarchy over a collection of perceptual indoor scene understanidng tasks, while
transferability was measured by a non-parameteric score called â€task affinityâ€ derived from neural
network transfer losses coupled with an ordinal normalization scheme.
Task relatedness. One approach to define task relatedness is based on task generation. Generalizaï¿¾tion bounds have been derived for multi-task learning (Baxter (2000)), learning-to-learn (Maurer
(2009)) and life-long learning (Pentina & Lampert (2014)). Although these studies show theoretical
results on transferability, it is hard to infer from data whether the assumptions are satisfied. Another
approach is estimating task relatedness from data, either explicitly (Bonilla et al. (2008); Zhang
8
Under review as a conference paper at ICLR 2019
(2013)) or implicitly as a regularization term on the network weights (Xue et al. (2007); Jacob et al.
(2009)). Most works in this category are limited to shallow ones in terms of the model parameters.
Representation learning and evaluation. Selecting optimal features for a given task is traditionally
performed via feature subset selection or feature weight learning. Subset selection chooses features
with maximal relevance and minimal redundancy according to information theoretic or statistical
criteria (Peng et al. (2005); Hall (1999)). The feature weight approach learns the task while reguï¿¾larizing feature weights with sparsity constraints, which is common in multi-task learningLiao &
Carin (2006); Argyriou et al. (2007). In a different perspective, Huang et al. (2017) consider the
universal feature selection problem, which finds the most informative features from data when the
exact inference problem is unknown. When the target task is given, the universal feature is equivalent
to the minimum error probability feature used in this work.
7 CONCLUSION
In this paper, we presented H-score, an information theoretic approach to estimating the performance
of features when transferred across classification tasks. Then we used it to define a notion of task
transferability in multi-task transfer learning problems, that is both time and sample complexity
efficient. The resulting transferability metric also has a strong operational meaning as the ratio
between the best achievable error exponent of the transferred representation and the minium error
exponent of the target task.
Our transferability score successfully predicted the performance for transfering features from
ImageNet-1000 classification task to Cifar-100 task. Moreover, we showed how the transferability
metric can be applied to a set of diverse computer vision tasks using the Taskonomy dataset.
In future works, we plan to extend our theoretical results to non-classification tasks, as well as
relaxing the local assumptions on the conditional distributions of the tasks. We will also investigate
properties of higher order transferability, developing more scalable algorithms that avoid computing
the H-score of all task pairs. On the application side, as transferability tells us how different tasks are
related, we hope to use this information to design better task hierarchies for transfer learning.
REFERENCES
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. In
Advances in neural information processing systems, pp. 41â€“48, 2007.
Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149â€“198, 2000.
Shai Ben-David, Reba Schuller, et al. Exploiting task relatedness for multiple task learning. Lecture
notes in computer science, pp. 567â€“580, 2003.
Edwin V Bonilla, Kian M Chai, and Christopher Williams. Multi-task gaussian process prediction.
In Advances in neural information processing systems, pp. 153â€“160, 2008.
Leo Breiman and Jerome H Friedman. Estimating optimal transformations for multiple regression
and correlation. Journal of the American statistical Association, 80(391):580â€“598, 1985.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised
learning of universal sentence representations from natural language inference data. arXiv preprint
arXiv:1705.02364, 2017.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley-Interscience, New
York, NY, USA, 1991. ISBN 0-471-06259-6.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In
International conference on machine learning, pp. 647â€“655, 2014.
Marylou GabriÂ´e, Andre Manoel, ClÂ´ement Luneau, Jean Barbier, Nicolas Macris, Florent Krzakala,
and Lenka ZdeborovÂ´a. Entropy and mutual information in models of deep neural networks. CoRR,
abs/1805.09785, 2018. URL http://arxiv.org/abs/1805.09785. 9
Under review as a conference paper at ICLR 2019
Yuqing Gao and Khalid M Mosalam. Deep transfer learning for image-based structural damage
recognition. Computer-Aided Civil and Infrastructure Engineering.
Hans Gebelein. Das statistische problem der korrelation als variations-und eigenwertproblem und
sein zusammenhang mit der ausgleichsrechnung. ZAMM-Journal of Applied Mathematics and
Mechanics/Zeitschrift fÂ¨ur Angewandte Mathematik und Mechanik, 21(6):364â€“379, 1941.
Mark Andrew Hall. Correlation-based feature selection for machine learning. 1999.
Hermann O Hirschfeld. A connection between correlation and contingency. Mathematical Proceedï¿¾ings of the Cambridge Philosophical Society, 31(04):520â€“524, 1935.
Shao-Lun Huang, Anuran Makur, Lizhong Zheng, and Gregory W Wornell. An information-theoretic
approach to universal feature selection in high-dimensional inference. In Information Theory
(ISIT), 2017 IEEE International Symposium on, pp. 1336â€“1340. IEEE, 2017.
Laurent Jacob, Jean-philippe Vert, and Francis R Bach. Clustered multi-task learning: A convex
formulation. In Advances in neural information processing systems, pp. 745â€“752, 2009.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convoluï¿¾tional neural networks. In Advances in neural information processing systems, pp. 1097â€“1105,
2012.
Xuejun Liao and Lawrence Carin. Radial basis function network for multi-task learning. In Advances
in Neural Information Processing Systems, pp. 792â€“802, 2006.
Anuran Makur, FabiÂ´an Kozynski, Shao-Lun Huang, and Lizhong Zheng. An efficient algorithm for
information decomposition and extraction. In Communication, Control, and Computing (Allerton),
2015 53rd Annual Allerton Conference on, pp. 972â€“979. IEEE, 2015.
Andreas Maurer. Transfer bounds for linear feature learning. Machine learning, 75(3):327â€“350,
2009.
Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information criteria
of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on pattern analysis
and machine intelligence, 27(8):1226â€“1238, 2005.
Anastasia Pentina and Christoph H. Lampert. A pac-bayesian bound for lifelong learning. In
International Conference on International Conference on Machine Learning, pp. IIâ€“991, 2014.
Lorien Y Pratt. Discriminability-based transfer between neural networks. In Advances in neural
information processing systems, pp. 204â€“211, 1993.
AlfrÂ´ed RÂ´enyi. On measures of dependence. Acta mathematica hungarica, 10(3-4):441â€“451, 1959.
C. K. Shie, C. H. Chuang, C. N. Chou, M. H. Wu, and E. Y. Chang. Transfer representation
learning for medical image analysis. In 2015 37th Annual International Conference of the
IEEE Engineering in Medicine and Biology Society (EMBC), pp. 711â€“714, Aug 2015. doi:
10.1109/EMBC.2015.7318461.
Lisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research on machine learning
applications and trends: algorithms, methods, and techniques, pp. 242â€“264. IGI Global, 2010.
Lichen Wang, Jiaxiang Wu, Shao-Lun Huang, Lizhong Zheng, Xiangxiang Xu, Lin Zhang, and
Junzhou Huang. An efficient approach to informative feature extraction from multimodal data.
AAAI, 2019. URL https://arxiv.org/abs/1811.08979.
Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 2018.
Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krishnapuram. Multi-task learning for classification
with dirichlet process priors. Journal of Machine Learning Research, 8(Jan):35â€“63, 2007.
10
Under review as a conference paper at ICLR 2019
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in neural information processing systems, pp. 3320â€“3328, 2014.
Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.
Taskonomy: Disentangling task transfer learning. Computer Vision and Pattern Recognition
(CVPR), 2018.
Yu Zhang. Heterogeneous-neighborhood-based multi-task local learning algorithms. In Advances in
neural information processing systems, pp. 1896â€“1904, 2013.
APPENDIX A INFERENCE USING LOCAL INFORMATION GEOMETRY
A.1 A PRIMER ON ERROR EXPONENTS
To begin with, consider the binary hypothesis testing problem over m i.i.d. sampled observations
{x(i)}mi=1 , xm with the following hypotheses: H0 : xm âˆ¼ P1 or H1 : xm âˆ¼ P2. Let Pxm be the
empirical distribution of the samples. The optimal test, i.e., the log likelihood ratio test can be stated
in terms of information-theoretic quantities as follows:
log P1(xm) P2(xm) = m[D(Pxm||P2) âˆ’ D(Pxm||P1)]
H0 â‰·H1
log T H0 : P1 H1 : P2 Î² = P2(A) Î± = P1(Ac) A: fail to reject H0 Ac
:reject H0 Pxm : D(Pxmk P2) âˆ’ D(Pxmk P1) = log
mT
Figure 10: The binary hypothesis testing problem. The blue curves shows the probility density
functions for P1 and P2. The rejection region and the acceptance region are highlighted in red and
blue, respectively. The vertical line indicates the decision threshold.
Further, using Sannovâ€™s theorem, we have that asymptotically the probability of type I error
Î± = P1(Ac) â‰ˆ 2âˆ’mD(P1âˆ—
||P1)
where P1âˆ— = argminP âˆˆAc D(P||P1) and Ac(T) = {xm : D(Pxm||P2) âˆ’ D(Pxm||P1) < 1m log T}
denotes the rejection region. Similarly, for type II error
Î² = P2(A) â‰ˆ 2âˆ’mD(P2âˆ—
||P2)
where P2âˆ— = argminP âˆˆA D(P||P2) and A = {xm : D(Pxm||P2) âˆ’ D(Pxm||P1) > 1m log T}
represents the acceptance region. (See Figure 10) The overall probability of error is P(m) e =
Î±P r(H0) + Î²P r(H1) and the best achievable exponent in the Bayesian probability of error (a.k.a.
Chernoff exponent) is defined as:
E = lim mâ†’âˆ
min
AâŠ†X m âˆ’ 1m
log Pe(m)
See Cover & Thomas (1991) for more background information on error exponents and its related
theorems.
11
Under review as a conference paper at ICLR 2019
A.2 LOCAL INFORMATION GEOMETRY
Now consider the same binary hypothesis testing problem, but with the local constraint P1, P2 âˆˆ N (P0X ). Let Ï†i = Pi(x)âˆ’P X0 (x)  âˆšP X0 (x)
denote the information vectors corresponding to Pi for i = 1, 2.
Makur et al. (2015) uses local information geometry to connect the error exponent in hypothesis
testing to the length of certain information vectors, summarized in the following two lemmas.
Lemma 1. Given zero-mean, unit variance feature function f(x) : X â†’ R, the optimal error
exponent (a.k.a. Chernoff exponent) of this hypothesis testing problem is
E =  28 k Ï†1 âˆ’ Ï†2k 2 + o( 2)
Lemma 2. Given zero-mean, unit variance feature function f(x) : X â†’ R, the error exponent of a
mismatched decision function of the form l = 1m P mi=1(f(x(i)
)) is
Ef =  28 h
Î¾, Ï†1 âˆ’ Ï†2i 2 + o( 2)
where Î¾(x) = p P0(x)f(x) is the feature vector associated with f(x).
As our discussion of transferability mostly concerns with multi-dimensional features, we present the
k-dimensional generalization of Lemma 2 below: (Equation 1 in the main paper.)
Lemma 3. Given k normalized feature functions f(x) = [f1(x), . . . , fk(x)], such that E[fi(X)] = 0
for all i, and cov(f(X)) = I , we define a k-d statistics of the form lk = (l1, ..., lk) where
li = 1m P ml=1 fi(x(l)). Let Î(x) = [Î¾1(x), . . . , Î¾k(x)] be the corresponding feature vectors with
Î¾i(x) = p PX(x)fi(x). Efk = kXi=1
Efi = kXi=l  28 h Î¾i
, Ï†1 âˆ’ Ï†2i 2 + o( 2) (4)
Proof. According to CramÂ´erâ€™s theorem, the error exponent under Pi
is
Ei(Î») = min
P âˆˆÎ´(Î») D(P||Pi)
where Î´ (Î») ,  P : EP [f(X)k
] = Î»EP1 [f(X)k
] + (1 âˆ’ Î»)EP2 [f(X)k]	 . With the techniques deï¿¾veloped in local information geometry, the above above problem is equivalent to the following
problem:
Ei(Î») = min
Ï†ËœÎ¸k âˆˆÏƒ(Î») 12k Ï†ËœÎ¸k âˆ’ Ï†ik 2
where Ïƒ(Î») , n Ï†ËœÎ¸k : D Ï†ËœÎ¸k , Î¾lE = h Î»Ï†1 + (1 âˆ’ Î»)Ï†2, Î¾li
, l = 1, ..., ko and i = 1, 2. Further, using
the equation Ï†ËœÎ¸k (x) = P kl=1 Î¸lÎ¾l(x)+Ï†i(x)âˆ’Î±(Î¸k)p P0(x)+o( ) and the equation Î±(Î¸k
) = o( 2),
it is easy to show that E1(Î») = E1(Î») when Î» = 12
. Then the overall error probability has the
exponent as shown in Equation (1).
APPENDIX B HGR MAXIMAL CORRELATION AND THE ACE ALGORITHM
Given random variables X and Y , the HGR maximal correlation Ï(X; Y ) defined in Equation 2 is
a generalization of the Pearsonâ€™s correlation coefficient to capture non-linear dependence between
random variables. According to RÂ´enyi (1959), it satisfies all seven natural postulates of a suitable
dependence measure. Some notable properties are listed below:
12
Under review as a conference paper at ICLR 2019
1. Ï(X; Y ) is defined for any pair of random variables X and Y
2. 0 â‰¤ Ï(X; Y ) â‰¤ 1;
3. Ï(X; Y ) = 0 if and only if X and Y are independent
4. Ï(X; Y ) = 1 if and only if X and Y are strictly dependent.
When the feature dimension is 1, the solution of the maximal HGR correlation is Ï(X; Y ) = Ïƒ1,
the largest singular value of the DTM matrix ËœB. For k-dimensional features, Ï(X; Y ) = P ki (Ïƒi).
However, computing ËœB requires estimating the joint probability PY X from data, which is inpractical
in real applications. Breiman & Friedman (1985) proposed an efficient algorithm, alternating
condition expectation (ACE), inspired by the power method for computing matrix singular values.
Algorithm 1 The ACE algorithm
Require: training samples {((x(i)
, y(i)
) : i = 1, . . . , m}
1: Initialize g(y) âˆˆ Rk, âˆ€y âˆˆ Y randomly
2: Center g(y) â† g(y) âˆ’ E[g(y)]
3: repeat
4: f(x) â† E[g(Y ) | X = x], âˆ€x âˆˆ X
5: f(x) â† f(x)E[f(X)f(X)T ]âˆ’ 12 , âˆ€x âˆˆ X {Normalize f(x)}
6: g(y) â† E[f(X) | Y = y], âˆ€y âˆˆ Y
7: g(y) â† g(x)E[g(Y )g(Y )T ]âˆ’ 12 , âˆ€y âˆˆ Y {Normalize g(y)}
8: until E[f(X)T g(Y )] stops increasing
In Algorithm 1, we first initialize g as a random k-dimensional zero-mean function. Then iteratively
update f(x) and g(y) for all x âˆˆ X and y âˆˆ Y . The conditional distributions on Line 4 and 6 are
computed as the empirical average over m samples. The normalization steps on Lines 5 and 7 can
also be implemented using the Gram-Schmidt process. Note that the ACE algorithm has several
variations in previous works, including a kernel-smoothed version (Breiman & Friedman (1985)) and
a parallel version with improved efficiency (Huang et al. (2017)). An alternative formulation that
supports continuous X and large feature dimension k has also been proposed recently (Wang et al.
(2019)).
Next we look at the convergence property of the ACE algorithm. Let f(X), g(Y ) be the true maximal
correlation functions, and let fËœ(X), gËœ(Y ) be estimations computed with Algorithm 1 from m i.i.d.
sampled training data. Similarly, denote by E[f(X)g(Y )] and Ë†Em[fËœ(X)Ëœg(Y )] the true and estimated
maximal correlations, respectively. Using Sanovâ€™s Theorem, we can show that for a small âˆ† > 0,
the probability that the ratio between the true and estimated maximal correlation is within 1 Â± âˆ†
drops exponentially as the number of samples increases. Hence the ACE algorithm converges in
exponential time. The following theorem gives the precise sampling complexity for k = 1.
Theorem 2. For any random variables X and Y with joint probability distribution PY X, if X and Y
are not independent, then for any f : X â†’ R and g : Y â†’ R such that E[f 2(X)] = E[g2(Y )] = 1,
we have that
âˆ’ lim
âˆ†â†’0+ 1Î´2
lim mâ†’âˆ
1m
log " P (      Ë†Em[fËœ(X)Ëœg(Y )]
E[f(X)g(Y )] âˆ’ 1 â‰¥ âˆ†
)# = 12 E[f(X)g(Y )]2
var[f(X)g(Y )]
for any given âˆ† > 0. APPENDIX C PROOF OF THEOREM 1
To simplify the proof, we first consider the case when the feature function is 1-dimensional.i.e.
f : X â†’ R. We have the following lemma:
13
Under review as a conference paper at ICLR 2019
Lemma 4. Let f : X â†’ R be an arbitrary feature function of X such that E[f(X)] = 0 and
E[f(X)2
] = 1 , then || ËœBÎ¾||2F = EPY
[(E[f(X)|Y ])2] where Î¾ is the feature vector corresponding to
f.
Proof. Since Î¾(x) = p PX(x)f(x), we have
|| ËœBÎ¾||2F = Î¾T ËœBT ËœBÎ¾
= X yâˆˆY xXâˆˆX 
PY X(y, x) p PX(x)p PY (y) âˆ’ p PX(x)p PY (y)! Â· p PX(x)f(x)! 2 = X yâˆˆY xXâˆˆX
PY X(y, x) PY (y) Â· f(x) âˆ’ xXâˆˆX
f(x)PX(x)! 2 Â· PY (y) = X yâˆˆY
(E[f(X)|Y = y] âˆ’ E[f(x)])2 Â· PY (y) = EPY
[(E[f(X)|Y ])2]
The last equality uses the assumption that E[f(x)] = 0.
Theorem 3 (1D version of Theorem 1). Given PX|Y =0, PX|Y =1 âˆˆ N X (P0,X) and features f : X â†’ R such that E [f(X)] = 0 and E[f(X)2
] = 1, then there exists some constant c independent
of f such that
Ef = ck ËœBÎ¾k 2F (5)
Proof. From E[f(X)] = PY (0)E[f(X)|Y = 0] + PY (1)E[f(X)|Y = 1] = 0, we first derive the
following properties of the conditional expectations of f(x): E[f(X)|Y = 1] = âˆ’  PY (0)
PY (1) E[f(X)|Y = 0]
E[f(X)|Y = 0] = âˆ’  PY (1)
PY (0) E[f(X)|Y = 1]
On the R.H.S. of Equation 5, we apply Lemma 4 to write
k
ËœBÎ¾k 2F = EPY  (E [f(X)|Y ])2 =PY (0)PY (1) + PY (1)2 PY (0) (E [f(X)|Y = 1])2
Next consider the L.H.S. of the equation, by Lemma 2, we have E(h) =  28 h
Î¾, Ï†1 âˆ’ Ï†2i 2 + o( 2
) =
c0h Î¾, Ï†1 âˆ’ Ï†2i 2
for some constant c0. c0 h Î¾, Ï†P1 âˆ’ Ï†P2 i 2 =c0 xXâˆˆX
PX|Y =0(x) âˆ’ PX|Y =1(x) p PX(x) Â· p PX(x)f(x)! 2 =c0 (E [f(X)|Y = 0] âˆ’ E [f(X)|Y = 1])2 =c0 âˆ’ (PY (0) + PY (1))2 PY (0)PY (1) Â· (E [f(X)|Y = 1])(E [f(X)|Y = 0])
=c0  PY (0) + PY (1)
PY (0) 
2 Â· (E [f(X)|Y = 1])2 =c0 PY (0) + PY (1)
PY (0)PY (1) 
PY (0)PY (1) + PY (1)2 PY (0)  Â· (E [f(X)|Y = 1])2 =c || ËœBÎ¾||2F
14
Under review as a conference paper at ICLR 2019
For k â‰¥ 2, Lemma 4 can be restated as follows:
Lemma 5. Let f : X â†’ Rk be a k-dimensional feature function of X, where f(x) =
[f1(x), . . . , fk(x)]. Further we assume that E[f(X)] = 0 and cov(f(X)) = E[f(X)Tf(X)] = I .
Then || Ë†BÎ||2F = tr(cov(E[f(X)|Y ])) where columns of Î are the information vectors corresponding
to f1(x), . . . , fk(x).
Proof. First, note that
ËœBÎ =  h p PY i âˆ’1 PY X h p PXi âˆ’1 âˆ’ p PY p PXT h p PXi f(X) = h p PY
i  [PY ]âˆ’1 PY Xf(X) âˆ’ 1 Â· E[f(X)]T
where 1 is a column vector with all entries 1 and length |Y|. Since E[f(X)] = 0, we have
ËœBÎ = h p PY
i  [PY ]âˆ’1 PY Xf(X)
It follows that
|| ËœBÎ||2F =tr(ÎT ËœBT ËœBÎ)
=tr   [PY ]âˆ’1 PY Xf(X) T
[PY ]  [PY ]âˆ’1 PY Xf(X)  =tr ï¿¾ EPY  (E[f(X)|Y ]]T)T(E[f(X)|Y ]]T)
 =tr(cov(E[f(X)|Y ]))
Finally, we derive the multi-dimensional case for Theorem 1.
Proof of Theorem 1. Using Lemma 5 and a similar argument as in the simplified proof, the R.H.S of
the equation becomes
|| ËœBÎ||2F =tr(cov(E[f(X)|Y ]))
=tr ï¿¾ EPY  (E[f(X)|Y ]]T)T(E[f(X)|Y ]]T)
 =PY (0)E[f(X)|Y = 0]]TE[f(X)|Y = 0]] + PY (1)E[f(X)|Y = 1]]TE[f(X)|Y = 1]]
=PY (0)PY (1) + PY (1)2 PY (0) (E [f(X)|Y = 1])T(E [f(X)|Y = 1])
By Lemma 3, the L.H.S. of the equation can be written as Efk = c0 P ki=l h Î¾i
, Ï†1 âˆ’ Ï†2i 2
for some
constant c0. It follows that
c0 kXi=l h Î¾i
, Ï†1 âˆ’ Ï†2i 2 =c0 
ï¿¾ï¿¾ PX|Y =0 âˆ’ PX|Y =1
T
f(X)
 
ï¿¾ï¿¾ PX|Y =0 âˆ’ PX|Y =1
T
f(X) T =c0 (E[f(X)|Y = 0] âˆ’ E[f(X)|Y = 1])T (E[f(X)|Y = 0] âˆ’ E[f(X)|Y = 1])
=c0 PY (0) + PY (1)
PY (0)PY (1) 
PY (0)PY (1) + PY (1)2 PY (0)  Â· E [f(X)|Y = 1]T E [f(X)|Y = 1]
=c k ËœBÎk 2F
15
Under review as a conference paper at ICLR 2019
APPENDIX D GENERALIZATION OF H-SCORE FOR ARBITRARY FEATURE SET
Since Î =  âˆšPX f(X) and E[f(X)] = 0, we have
ÎTÎ = h p PXi f(X) T h p PXi f(X) = E[f(X)Tf(X)] = cov(f(X)) (6)
Equation (6) gives a more understandable expression of the normalization term. We can also write
ËœBÎ as follows:
ËœBÎ =  h p PY i âˆ’1 PY X h p PXi âˆ’1 âˆ’ p PY p PXT h p PXi f(X) = h p PY
i  [PY ]âˆ’1 PY Xf(X) âˆ’ 1 Â· E[f(X)]T
where 1 is a column vector with all entries 1 and length |Y|, we have
ÎT ËœBT ËœBÎ =  [PY ]âˆ’1 PY Xf(X) âˆ’ 1 Â· E[f(X)]T T
[PY ]  [PY ]âˆ’1 PY Xf(X) âˆ’ 1 Â· E[f(X)]T =EPY  (E[f(X)|Y ] âˆ’ 1 Â· E[f(X)]T)T(E[f(X)|Y ] âˆ’ 1 Â· E[f(X)]T) =cov (E[f(X)|Y ]) (7)
On the other hand,
|| ËœBÎ(ÎTÎ)âˆ’ 12 ||2F =tr  (ÎTÎ)âˆ’ 12 ÎT ËœBT ËœBÎ(ÎTÎ)âˆ’ 12  =tr  (ÎTÎ)âˆ’1ÎT ËœBT ËœBÎ (8)
=tr ï¿¾ cov(f(X))âˆ’1
cov(E[f(X)|Y ])
The last equality is derived by substituting (6) and (7) into (8).
APPENDIX E H-SCORE AND SOFTMAX REGRESSION
In softmax regression, given m training examples {(x(i)
, y(i))}mi=1, the cross-entropy loss of the
model is
`
(f, Î¸) = âˆ’ mXi=1
|Y|
kX=1
1{y(i) = k} log eâˆ’Î¸Tk f(x(i)) P Cj=1 eâˆ’Î¸Tj f(x(i))
(9)
,
âˆ’ mXi=1
log QY |X(y(i)|x(i)) (10)
= âˆ’ EPY X [log QY |X(Y |X)] (11)
Minimizing ` is equivalent to minimizing D ï¿¾ PY X||PXQY |X where PY X is the joint empirical
distribution of (X, Y ).
Using information geometry, it can be shown that under a local assumption
argmin
f,Î¸
D ï¿¾ PY X||PXQY |X = argmin
Î¨,Î¦ 12k ËœB âˆ’ Î¨Î¦Tk 2F + o( 2) (12)
which reveals a close connection between log loss and the modal decomposition of ËœB. In consequence,
it is reasonable to measure the classification performance with k ËœB âˆ’ Î¨Î¦Tk 2F
given a pair of (f, Î¸)
associated with (Î¨, Î¦).
In the context of estimating transferability, we are interested in a one-sided problem, where Î¦S is
given by the source feature and Î¨ becomes the only variable.
min
Î¨ k ËœBT âˆ’ Î¨Î¦TS k
(13)
16
Under review as a conference paper at ICLR 2019
Figure 11: Comparing task transferability (H-score) and transfer log-loss on synthesized tasks.
Training the network is equivalent to finding the optimal weight Î¨âˆ—
that minimizes the log-loss. By
taking the derivative of the Objective function with respect to Î¨, we get
Î¨âˆ— = ËœBT Î¦S(Î¦TSÎ¦S)âˆ’1
(14)
Substituting (14) in the Objective of (13), we can derive the following close-form solution for the log
loss.
k
ËœBTT ËœBT k 2F âˆ’ k ËœBT Î¦S(Î¦TSÎ¦S)âˆ’ 12 k 2F (15)
The first term in (15) is fixed given TT while the second term has exactly the form of H-score. This
implies that log loss is negatively linearly related to H-score.
We demonstrates this relationship experimentally, using a collection of synthesized tasks (Figure 11).
In particular, the target task is generated based on a random stochastic matrix PY0|X, and 20 source
tasks are generated with the conditional probability matrix PYi|X = PY0|X + iÎ»I for some positive
constant Î».
The universal minimum error probability features for each source task are used as the source features
fSi (x), while the respective log-loss are obtained through training a simple neural network in Figure 2
with cross-entropy loss. The relationship is clearly linear with a constant offset.
APPENDIX F DTM AND MUTUAL INFORMATION
Proposition 1. Under the local assumption that PX|Y âˆˆ N (PX), mutual information I(X; Y ) =
12
|| ËœB||2F + o( 2) , where ËœB of the DTM matrix of X and Y .
Proof. First, we define Ï†Xy |Y (x) = PX|Y (x|y)âˆ’PX(x)  âˆšPX(x)
and let Î¦X|Y âˆˆ R
|X |Ã—|Y| denote its matrix
version. Then we have

Î¦X|Y [p PY ] = [p PX]âˆ’1(PXY âˆ’ PX)[p PY ] = ËœBT
Next, we express the mutual information in terms of information vector Ï†X|Y , I(X; Y ) = I(Y ; X) = X
yâˆˆY
PY (y)DKL(PX|Y ||PX) =  22 X yâˆˆY
PY (y)||Ï†Xy |Y
||2 + o( 2) = 12
|| Î¦X|Y [p PY ]||2F + o( 2) = 12
|| ËœB||2F + o( 2)
17
Under review as a conference paper at ICLR 2019
APPENDIX G SUPPLEMENTARY RESULTS ON EXPERIMENT 5.2
G.1 COMPARISON OF H-SCORES AND AFFINITIES
Table 1, with columns representing source tasks and rows representing target tasks. For each target
task, the upper row shows our results while the lower one shows the results in Zamir et al. (2018).
Score values are included in parenteses.
Table 1: Transferability ranking comparison, between H-scoreâ€™s estimation and task affinity
Tasks 2D Edges 2D Keypoints 3D Edges 3D Keypoints Reshading Depth Object Class. Scene Class.
2D Edges 1 (1.8216) 2 (1.7334) 5 (1.5704) 6 (1.5696) 4 (1.6146) 3 (1.6201) 7 (1.5097) 8 (1.4402)
1 (0.0389) 2 (0.0117) 4 (5.8920e-5) 3 (8.8011e-5) 7 (2.9001e-5) 8 (2.2110e-5) 5 (4.9141e-5) 6 (4.8720e-5)
2D Keypoints 2 (1.6698) 1 (1.7859) 7 (1.5248) 5 (1.5287) 4 (1.5481) 3 (1.5632) 6 (1.5253) 8 (1.4725)
2 (0.0002) 1 (0.0542) 7 (7.7797e-5) 5 (8.1029e-5) 6 (7.8464e-5) 8 (7.2724e-5) 3 (0.0002) 4 (0.0001)
3D Edges 5 (1.4828) 4 (1.4910) 3 (1.5167) 7 (1.4701) 2 (1.5405) 1 (1.6739) 8 (1.4644) 6 (1.4730)
6 (0.0117) 7 (0.0108) 1 (0.1179) 2 (0.0734) 4 (0.0622) 3 (0.0636) 8 (0.0094) 5 (0.0151)
3D Keypoints 6 (1.5375) 5 (1.5466) 4 (1.5910) 3 (1.6456) 1 (1.7198) 2 (1.7122) 7 (1.4709) 8 (1.4121)
5 (0.0141) 6 (0.0136) 2 (0.0531) 1(0.1275) 3 (0.0400) 4 (0.0247) 7 (0.0132) 8 (0.0121)
Reshading 5 (1.5504) 6 (1.5426) 3 (1.8174) 4 (1.7990) 1 (2.2339) 2 (2.1200) 7 (1.4774) 8 (1.3804)
6 (0.0147) 8 (0.0143) 2 (0.0781) 4(0.0545) 1 (0.1121) 3 (0.0765) 7 (0.0144) 5 (0.0174)
Depth 6 (1.6542) 5 (1.6870) 3 (1.8504) 4 (1.8176) 2 (2.1700) 1 (2.2441) 7 (1.6008) 8 (1.5099)
7 (0.0175) 8 (0.0154) 3 (0.0595) 4 (0.0617) 2 (0.0867) 1 (0.0989) 6 (0.0217) 5 (0.0237)
Object Class. 5 (22.866) 4 (23.627) 7 (22.371) 8 (21.950) 6 (22.452) 3 (23.697) 1 (33.468) 2 (28.013)
7 (0.0205) 6 (0.0217) 3 (0.0350) 4 (0.0318) 5 (0.0286) 8 (0.0147) 1 (0.0959) 2 (0.0774)
Scene Class. 5 (14.575) 4 (15.074) 7 (14.206) 8 (13.801) 6 (14.332) 3 (15.474) 2 (25.750) 1 (25.962)
8 (0.0149) 7 (0.0165) 3 (0.0335) 4 (0.0305) 5 (0.0263) 6 (0.0198) 2 (0.0504) 1 (0.1474)
Here we present some detailed results on the comparison between H-score and the affinity score in
Zamir et al. (2018) for pairwise transfer.
The results of the classification tasks are shown in Figure 12 and the results of Depth is shown in 13.
We can see in general, although affinity and transferability have totally different value ranges, they
tend to agree on the top few ranked tasks.
Figure 12: Source task transferability ranking for classification tasks. For each target task, the left
figure shows H-score results, and the right figure shows task affinity results.
G.2 LABEL QUANTIZATION
During the quantization process of the pixel-to-pixel task labels (ground truth images), we are
primarily concerned with two factors: computational complexity and information loss. Too much
information loss will lead to bad approximation of the original problems. On the other hand, having
little information loss requires larger cluster size and computation cost.
Figure (14) shows that even after quantization, much of the information in the images are retained.
To test the sensitivity of the cluster size, we used cluster centroids to recover the ground truth image
pixel-by-pixel. The 3D occlusion Edge detection results on a sample image is shown in Figure 15.
When the cluster number is set to N = 5 (right), most detected Edges in the ground truth image (left)
are lost. We found that N = 16 strikes a good balance between recoverability and computation cost.
18
Under review as a conference paper at ICLR 2019
Figure 13: Comparison between source task rankings for Depth. with H-score results on the left and
affinity scores Zamir et al. (2018) on the right. The Top 3 transferable source tasks in both methods
are the same: Depth, Image Reshading and 3D Occlusion Edges.
Figure 14: Quantization. Recover is done with the centroid of corresponding cluster of each pixel.
Figure 15: Effect of quantization cluster size for 3D occlusion Edge detection.
19
