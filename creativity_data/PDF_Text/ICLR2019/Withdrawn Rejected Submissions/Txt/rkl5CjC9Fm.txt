Under review as a conference paper at ICLR 2019
Dual Importance Weight GAN
Anonymous authors
Paper under double-blind review
Ab stract
Generative Adversarial Networks (GAN) are trained to generate a sample image
of interest. To this end, generative network of GAN learns implicit distribution
of true dataset from the classification samples with candidate generated samples.
However, in real implementation of GAN, training the generative network with
limited number of candidate samples guarantees to properly represent neither true
distribution nor the distribution of generator outputs. In this paper, we propose
dual importance weights for the candidate samples represented in the latent space
of auto-encoder. The auto-encoder is pre-trained with real target dataset. There-
fore, the latent space representation allows us to compare real distribution and the
distribution of generated samples explicitly. Dual importance weights iteratively
maximize the representation of generated samples for both distributions: current
generator outputs and real dataset. Proposed generative model not only resolves
mode collapse problem of GAN but also improves the convergence on target distri-
bution. Experimental evaluation shows that the proposed network learns complete
modes of target distribution more stable and faster than state of the art methods.
1	Introduction
Recently, generative networks have been widely studied thanks to the explosive and successful ap-
plications of Generative Adversarial Networks (GAN) proposed by Goodfellow et al. (2014). Main
difficulty of training step in the generative model is evaluating high-dimensional original probability
distribution. Estimation of the probability distribution of target dataset from sparse samples is not a
trivial task that plays a critical role in the quality of generated samples.
GAN is composed of two main networks: discriminator and generator. Core idea is learning the
target distribution through two-player minimax game theory. The discriminator helps generator to
learn the representation of the target distribution by distinguishing the difference between generated
and real samples. As a result, generator is able to produce samples which resemble target real data.
Although GAN has been successfully implemented in many applications, it suffers from ill-training
problems such as oscillation between modes, mode collapsing, etc. Especially, mode collapsing
results in the generation of samples only from a single or a few modes of target data distribution
losing diversity of target dataset. Main reason of mode collapsing problem is that the discriminator
is incapable of delivering any information about samples’ diversity. Once the generator finds optimal
point of a fixed discriminator in each generator training step, the network produces same samples
driving the expectation value in objective function becomes minimum regardless of input noise
vectors. Consequently, generator repeatedly generates certain good instance rather than diverse
instances from entire data distribution.
Metz et al. (2016) proposes unrolled GAN that trains generator using unrolled multiple samples
rather than single target at the generator output, which alleviate mode collapsing. VEEGAN in Sri-
vastava et al. (2017) employs a reconstruction network which is not only mapping real data distribu-
tion to a Gaussian in encoded space but also approximating reverse action of the generator. Because
VEEGAN estimates implicit probability of real dataset, it prevents mode collapsing problem and
produces more realistic samples. Bang & Shim (2018) propose a manifold guided generative ad-
versarial network(MGGAN) for guiding a generator by adding another adversarial loss in manifold
space transformed by pre-trained encoder network. This enables the generator to learn all modes of
target distribution without impairing the quality of image. Although real data distribution is repre-
sented in the manifold loss of their second discriminator, it is not able to estimate explicit distance
between the distributions. OT-GAN(Salimans et al. (2018)) expands generator loss function using
1
Under review as a conference paper at ICLR 2019
optimal transport theory. They define a new metric, M inibatchEnergyDistance, over probability
distribution in a adversarially learned feature space. Although they make the condition of GAN to
be more stable than other state-of-the-art research, the computational cost of their large mini-batches
is expensive to be practical in real applications.
In all such previous approaches, using increased number of samples or larger mini-batches for the
training of generator network allows more stable and improved performance. On the other hand, they
require hugely increased computation cost and training time. Our idea is that given limited number
of generated samples in the training step, representation ability of the samples can be evaluated and
maximized for both distributions (current generator outputs and real dataset) adopting importance
weights estimation of the samples.
In this paper, we propose dual importance weights for the generated candidate samples in generator
network training step represented in the latent space of auto-encoder. The auto-encoder is pre-trained
with real target dataset. We assume that auto-encoder trained with target dataset has ability to rep-
resent the distribution of target dataset in the latent space under optimally minimized dimensions.
Therefore, the latent space representation allows us to compare real distribution and the distribution
of generated samples explicitly. Our dual importance weights on generated samples iteratively max-
imize the representation ability for the generator and guide the generator to find complete modes
of real data distribution. To this end, we expand generator objective function to save the diversity
of target distribution via adopted auto-encoder which works as a bridge between data and manifold
space. As a result, proposed dual importance weight GAN not only resolves mode collapse problem
of GANs but also improves the convergence on target distribution. Transformed distributions to the
latent space enables explicit representation of generated and real data. We calculate actual distance
between the two distributions and achieve fast and robust convergence to optimal states avoiding
mode collapse problem.
2	GAN with Auto-encoder
GAN is motivated by game theory that two players (generator and discriminator) compete with each
other in a zero-sum game framework. The generator learns the target distribution via an adversarial
training process with discriminator. When the generator transform a noise vector z into a data vector
G(z) on target space, the discriminator tries to estimate the probability if input sample comes from
target distribution P(x) or not. At the end of the training process, the generator produces outputs of
P (x). GAN defines this adversarial problem as follow:
min max LGAN(D,G) = Eχ~Pdata log D(x) + Ez~Pz log(1 - D(G(Z)))	(1)
where the goal of GAN is to find the optimal parameter set of G and D in equation (1) minimizing
the generator loss and maximizing the discriminator loss.
Auto-encoder is a neural network that learns optimal representation of unlabeled input data in en-
coded space. It generates original input through decoding network. Auto-encoder not only decreases
the dimensionality of input data but also finds optimal abstraction of input data for better discrimina-
tion between subjects. Generative networks with auto-encoder and adversarial learning algorithms
have been proposed to improve GAN. Adversarial auto-encoder (AAE) Makhzani et al. (2015) is a
type of auto-encoder combined with a discriminator and learns target data distribution using both
reconstruction and adversarial losses. The adversarial loss let the encoder learn a posterior which
follows imposed prior during adversarial training. More recently, Rosca et al. (2017) suggests a
α-GAN which combines variational auto-encoder (VAE, Kingma & Welling (2013)) and GAN. Us-
ing reconstruction and adversarial losses, α-GAN tries to address mode collapse problem of GAN
producing blurry images of VAE. Proposed generative network extends original GAN Goodfellow
et al. (2014) with auto-encoder. Auto-encoder trained with real dataset optimally reduces the dimen-
sion of the dataset. In the encoded space, auto-encoder extracts essential feature set to represent all
modes and aspects of original data distribution which is ideal to guide generative network.
3	Sampling with Dual Importance Weight
Based on our GAN with auto-encoder, obtaining good samples is a critical for improved perfor-
mance. Importance sampling is a statistical technique for estimating characteristics of distribution
2
Under review as a conference paper at ICLR 2019
Pre-training of Auto-encoder
Sampling in the Latent Space
E : encoder	Bat
D : decoder
E* : pre-trained encoder
Sr : real sample set
Sg : generated sample set
Figure 1: Proposed generative network with dual importance weights
Target Samples
in Encoded Space
X Target Sample
• Generated Sample
Two Samples
Generated Samples
in Encoded Space
Figure 2: Distance calculation between generated and real data samples in encoded space
with the samples generated from different distribution. The basic idea of importance sampling is to
give weights to the samples for the proper representation of target distribution. This weight adjusts
the contribution of each sample to the representation so that we are able to improve the approx-
imation quality. In GAN, we assume that current generated samples represent the distribution of
complete outputs of the generator. And we expect the generated samples ultimately represent the
distribution of real data. Diesendruck et al. (2018) proposes importance weighted generative net-
works with modified adversarial loss function from differently contributing samples of mini-batch.
Importance weighted auto-encoders (IWAE; Burda et al. (2015)) is a variant of variational auto-
encoder (VAE). An improved loss function of log-likelihood lower bound derived from importance
weighting has tighter boundary than VAE.
We define a new distance Da(sr, sg) which measures the distance between paired real and generated
samples in the latent space of auto-encoder.
Da (Sr,Sg ) = Wk Wk J(Sr - Sg )2	(2)
where sr ∈ Sr , sg ∈ Sg denote samples from real and generated sample sets in encoded space.
Real sample Sr is extracted from the distribution of real data in the encoded space and the the
distribution of real data is obtained using Kernel density estimation (KDE) on the complete real
dataset transformed to the encoded space of pre-trained auto-encoder.
3
Under review as a conference paper at ICLR 2019
We define the expectation of euclidean distance over mini-batch samples as the distance between
real and generated samples. wrk and wgk are target importance weight and generator importance
weight, respectively. Target importance weight is assigned to each target real sample and generator
importance weight is assigned to each generated sample. First, generator importance weight wg
indicates how well current generated sample represents generator outputs. In order to obtain the
weight for each sample, we accumulate past N generated sample sets constructing the distribution
of generator outputs using Kernel density estimation (KDE). A generated sample of high generator
importance weight contributes more to the distance calculation in equation (2). When the current
generated samples poorly represent the current generator distribution, they gets smaller weights
and are ignored in the training. In this manner, iterative sample quality evaluation makes stable
environment of GAN training procedure. Secondly, target real importance weight wr indicates how
well all generated samples contribute to the estimation of current real data sample. We assign the
weight for each target real sample based on the degree of how much the real sample is covered by
generated samples. Calculated distance of a real sample from paired generated samples in the past
iterations add importance to the real sample. In other words, if a real sample has higher distances
to all generated samples during the past training steps, the real sample gets higher real importance
weight. And then, in the next training step, network is updated to generated this isolated real sample
due to the higher loss value. Finally, both importance weights encourage generated samples to
cover entire target real data samples. At each training step, both weights are updated adjusting the
importance of generated and target samples. Due to the elaborate evaluation of the generated sample
quality at each training step, proposed network converges to target real distribution faster without
mode collapse problem.
Our generator loss function is shown in equation (3). If GAN unexpectedly generates samples from
single mode of real dataset, second term in equation (3) increases forcing the network to learn other
modes.
min -Ez〜Pzlog D(G(Z)) + α X Esr〜5,%〜SgDa(Sr, Sg)	(3)
Overall training procedures are summarized in Figure 6. First we train auto-encoder with target
real dataset. The target data distribution in the encoded space is estimated by KDE(Kernel density
estimation) from entire dataset transformed to the encoded space. And then, we obtain same number
of good real samples of batch size which reflects target distribution. Note that the real samples are
extracted once and used for all iterations of the training. For optimal generated and real sample pair
matching in distance Da calculation, we calculate distances among all real samples to all generated
samples and assign pair one by one minimizing average distance. Based on generated candidate
samples, our approach tries to find all aspects of real distribution qualifying generated samples.
Better generated samples evaluated by generator importance weight understand generator better.
Better target importance understands target real dataset better. Finally, our method let the generator
understand target real dataset better.
4	Experimental Evaluation
We perform quantitative and qualitative evaluation on three datasets: Mixture of Gaussians,
(stacked) MNIST, and Cifar10. Several similarity metrics are used to quantify generation perfor-
mance.
4.1	Mixture of Gaussians
We have created several test distributions made of Gaussians: mixture of eight 2D Gaussians located
in a ring, 25 Gaussians located in a grid and 25 Gaussians randomly located, 27 Gaussians in a
3D cube. 25 random Gaussians are anisotropic. VEEGAN(Srivastava et al. (2017)) and Unrolled
GAN(Metz et al. (2016)) are compared with our method. Identical network architecture is used
for fair comparison. Generator has three layers of fully connected MLPs with 128 nodes without
dropout and batch normalization and discriminator has two layers of fully connected MLPs without
dropout and batch normalization. We choose the dimension of encoded space of auto-encoder that
properly reproduces original data. Following Srivastava et al. (2017), we employ two metrics for
quantitative evaluation. First, number of modes found are counted. Secondly, the quality of samples
are measured by high quality sample ratio (HQS). However it’s hard to evaluate how the generated
samples are able to cover the entire real distribution just using number of modes and percentage
4
Under review as a conference paper at ICLR 2019
of high quality samples. Thus we measure the distance between estimated target and generated
distribution. We map the sample to canonical space and count the number of points within each
canonical unit. By measuring Jensen-Shannon divergence(JSD) between the distributions (Pr, Pg),
we evaluate how well generator follows target distribution. Figure 3 shows results on our mixture of
Gaussians dataset. Proposed method outperforms over two state of the art methods showing much
faster convergence without mode collapse problem. Table 1 summarizes quantitative results showing
outstanding performance of our generative network.
• ∙ ∙ ∙ ∙
• ∙ ∙ ∙ ∙
• ∙ ∙ ∙ ∙
• ••♦・
• ∙ ∙ ∙ ∙
• ∙ ∙ ∙ ∙
♦， ∙
♦ , ∙
Figure 3: Experimental results compared to VEEGAN(Srivastava et al. (2017)) and Unrolled
GAN(Metz et al. (2016)): 2D Ring, 2D Grid(isotropic and unisotropic), and 3D Cube testset. Pro-
posed method converges faster than state-of-the-art methods.
∙∙∙∙∙
5
Under review as a conference paper at ICLR 2019
Table 1: Quantitative Evaluation: Number of modes found, HQS(High Quality Sample), and
JSD(Jensen-Shannon divergence) between real and generated sample distributions. Best result is
indicated bold face font.
	METRIC	Unrolled GAN(Std)	VEEGAN(std)	Proposed(std)
	Modes(Max 8)	8(0)	8(0)	8(0)
2D Ring	% HQS	30.9(0.006)	60.4(0.005)	85.5(0.006)
	JSD(realkgenerated)	0.254(0.004)	0.19(0.005)	0.172(0.004)
	Modes(Max 25)	25(0)	24.1(0.31)	25(0)
2D Grid	% HQS	14.4(0.006)	65.4(0.006)	82.5(0.004)
(uniform)	JSD(realkgenerated)	0.47(0.006)	0.21(0.004)	0.12(0.004)
	Modes(Max 25)	24.4(0.5)	24.3(0.637)	25(0)
2D Grid	% HQS	15.9(0.005)	63.7(0.007)	83.4(0.006)
(random)	JSD(realkgenerated)	0.38(0.007)	0.32(0.007)	0.15(0.004)
	Modes(Max 27)	27(0)	26.6(0.5)	27(0)
3D Cube	% HQS	85.0(0.348)	43.3(0.007)	80.0(0.005)
	JSD(realkgenerated)	0.194(0.004)	0.31(0.005)	0.125(0.004)
4.2	MNIST
In this experiment, we expand MoG dataset to image space with MNIST and stacked MNIST. Eval-
uation on MNIST data (Figure 4) shows that proposed method generates more number of digits than
original GAN. Stacked MNIST is designed for high complexity evaluation with extended number
of modes. Stacked MNIST is synthesized by stacking different MNIST digits in respective colors.
This synthesized dataset has 1000 modes which are the combinations of 10 classes in 3 channel. We
use implemented generator architecture of standard DGGAN. For the discriminator, we use same
architecture with DCGAN and Unrolled GAN. For VEEGAN, the architecture of discriminator is
designed following Srivastava et al. (2017). Auto-encoder has three convolutional layers with 5 by 5
filters, 2 fully connected layers for encoder part and one fully connected layer, 3 transposed convo-
lutional layers are used for decoder. First, we train the classifier using MNIST dataset which assigns
the mode to generated images. In this evaluation, we use 20,000 generated samples and they are
given the mode from pre-trained classifier. For example, one sample has 3 channels which represent
different digit. Digit for each sample in channel is determined by MNIST classifier.
6
Under review as a conference paper at ICLR 2019
,3 壕.6≡够出⅛l'⅛l
⅛≡I¾SC4
二？⅛3*3
g普逸够障1≡
II €f ⅛
您物≡邃施Z，城
，，/二；障段3蟋.？
60r)2-0eGJ
Q 力64。，巳 D
6巧 GQ4266
。9方夕G幺66
6亍「？幺6匕G
G2 G 6 76&σ
e9，,N-3匕Ir
匕力登夕「90£
强 Ev-7σ6 q 6
2g0Qag#3
9<t>q9p 单70,
79 53 年 IOa7
I弓夕于上5
J163夕5邛58
y0 6 8© q$4
(b) DCGAN
(a) True Data
3iβ夕e夕 chθ
/ð4— 夕βg 7
7 夕夕y7⅛*
€ tya9I夕京
，7 9 979—£
/ 3βa甘，/拿
4 Ole夕, 76
，，7彳7i7 A-
qc⅜ ʃ ¾ $ β⅛r
JN 备层 €><0?&
S奇。，Oq3。
4 6Γ<of,443
so，*sr45
S6S4 2M>82
4 9 4 £，，U5,
4 7 * 必。3 q q
T£母写母母写工
s-iM色军年药t5m
s:写^i-5"F4τ
于E年干XT您∙ T制
监T写筝r写筝T
r写零年事ɪr^-
τ,F筝彩S-，«?
写«"写τi-τFr
(d) Unrolled	(e) VEEGAN	(f) Proposed
Figure 5: Generated stacked MNIST samples from trained models. We refer these images from
Srivastava et al. (2017) paper except our result.
Figure 6: Experimental results are compared to VEEGAN and DCGAN using CIFAR 10 dataset:
VEEGAN and DCGAN frequently suffer from mode collapsing in yellow circles.
4.3 CIFAR 10
We also evaluate our method using CIFAR-10 dataset which includes 32x32 color images with
10 classes collected by Krizhevsky & Hinton (2009). The architecture is same to 4.2. Generated
results of VEEGAN and DCGAN are collected from Srivastava et al. (2017). As shown in Figure 6,
VEEGAN and DCGAN frequently include identical samples suffering from mode collapsing (see
yellow circles).
7
Under review as a conference paper at ICLR 2019
5 Conclusion
In this paper, we propose dual importance weights for the candidate samples represented in the
latent space of auto-encoder. Dual importance weights iteratively maximize the representation of
generated samples for both distributions: current generator outputs and real dataset. Evaluation
and comparison are extensively performed on three datasets showing promising performance of the
proposed method. On the other hand, our method involves additional computation for sample pair
matching. Structure of auto-encoder and the dimension of encoded space can be further optimized
for improved quality of generated samples.
Acknowledgments
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including
those to funding agencies, go at the end of the paper.
References
Duhyeon Bang and Hyunjung Shim. Mggan: Solving mode collapse using manifold guided training.
arXiv preprint arXiv:1804.04391, 2018.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
Maurice Diesendruck, Ethan R Elenberg, Rajat Sen, Guy W Cole, Sanjay Shakkottai, and Sinead A
Williamson. Importance weighted generative networks. arXiv preprint arXiv:1806.02512, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems,pp. 2672-2680, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. arXiv preprint arXiv:1611.02163, 2016.
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational
approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987,
2017.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving gans using optimal
transport. arXiv preprint arXiv:1803.05573, 2018.
Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan:
Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Infor-
mation Processing Systems, pp. 3308-3318, 2017.
8