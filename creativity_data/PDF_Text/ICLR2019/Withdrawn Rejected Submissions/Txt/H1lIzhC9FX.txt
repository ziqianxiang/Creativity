Under review as a conference paper at ICLR 2019
Learning to remember: Dynamic Generative
Memory for Continual Learning
Anonymous authors
Paper under double-blind review
Ab stract
Continuously trainable models should be able to learn from a stream of data over
an undefined period of time. This becomes even more difficult in a strictly in-
cremental context, where data access to previously seen categories is not possi-
ble. To that end, we propose making use of a conditional generative adversarial
model where the generator is used as a memory module through neural masking
to emulate neural plasticity in the human brain. This memory module is fur-
ther associated with a dynamic capacity expansion mechanism. Taken together,
this method facilitates a resource efficient capacity adaption to accommodate new
tasks, while retaining previously attained knowledge. The proposed approach out-
performs state-of-the-art algorithms on publicly available datasets, overcoming
catastrophic forgetting.
1	Introduction
Continual lifelong learning is unquestionably one of the most important cognitive skills of humans
and artificial neural networks alike. This can be largely attributed to the fact that every time some-
thing new is learned, a little of bit of old knowledge gets overwritten. Therefore, addressing this is
of particular relevance in a continual learning setting where forgetting is largely unavoidable. In this
regard, this phenomenon in context of artificial neural network algorithms is commonly referred to
as catastrophic forgetting (McCloskey & Cohen, 1989; Ratcliff, 1990; French, 1999).
Catastrophic forgetting is believed to occur due to a lack of a plastic component in the neuron con-
nections (Kirkpatrick et al., 2016; Zenke et al., 2017). In analogy with biological systems, neural or
synaptic plasticity is the capability of a neuron or synapse to “lock” itself to a state or a connection,
thus retaining previously attained knowledge. Indeed, it has been shown that this plasticity is re-
sponsible for maintaining previously acquired structure in the neo-cortical circuits of brains (Cichon
& Gan, 2015; Hayashi-Takagi et al., 2015).
Several recent approaches try to mitigate forgetting by simulating neuroplasticity in artificial neural
networks. To that end, one possibility is to identify critically important parameters of a network w.r.t.
a given task, and imposing a penalty for changing them for learning another task (Kirkpatrick et al.,
2016; Chaudhry et al., 2018; Zenke et al., 2017; Aljundi et al., 2017). Common to all these methods
is that they seek to find a point in the model parameter space that minimizes the loss jointly for all
previously learned tasks. However, the existence of such point is not always guaranteed, hence the
hard attention to the task (HAT) mechanism was proposed Serra et al. (2018). HAT finds a parameter
subspace for each task that can overlap with other tasks’ parameter subspaces. The optimal solution
is then found in the corresponding parameter subspace of each task. These solutions can be seen as
reserving sub-spaces of the neural network parameter space for each task at hand and driving the
subsequent network updates into the free-capacity space.
Another important factor in the continual learning setting is the ability to scale, i.e. to maintain
sufficient capacity to accommodate for a continuously growing number of tasks. Given equitable
resource constraints, it is inevitable that with a growing number of tasks to learn, the model capacity
is depleted at one point in time. Without a smart and adaptive capacity strategy network expansion
can swiftly go beyond existing hardware limits, or unnecessarily waste resources by reservation.
In order to address catastrophic forgetting in a continual learning setting some approaches rely on
storing raw samples of previously seen data and making use of replay strategies during the training
1
Under review as a conference paper at ICLR 2019
of subsequent tasks. However, this starkly contrasts with the natural learning mechanisms of the
brain, which does not feature the retrieval of raw information identical to originally exposed impres-
sions Mayford et al. (2012). What is more, storing raw samples of previous data may violate data
privacy and memory restrictions in the real world applications. Instead of storing raw samples, the
proposed approach relies upon on the idea of training a deep generative model to memorize pre-
viously seen data distributions. Simultaneous with the adversarial training on each incoming data
batch, we learn a sparse mask for the layer activations of the generator network. This serves the pur-
pose of encouraging parameter re-usability across tasks. The values of the learned mask correspond
to the plasticity of the connections between the layers. Thus units with low plasticity are reserved
for previous tasks and can be reused but not changed during the subsequent training.
Our contribution is twofold: (a) we address the catastrophic forgetting problem in continual learn-
ing; we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative
network endowed with neuronal plasticity through efficient learning of a sparse parameter attention
mask.1 for layer activations; a single generator is able to incrementally learn new information dur-
ing normal adversarial training without the need to replay previous knowledge; (b)W e address the
scalability problem in continual learning proposing an adaptive network expansion mechanism,
facilitating resource efficient continual learning. We compare the proposed method to state-of-the-
art approaches for continual learning.
2	Related Work
Among the first works dealing with catastrophic forgetting in the context of lifelong learning are
(French, 1999; McCloskey & Cohen, 1989; Ratcliff, 1990). In contrast to the proposed approach,
these methods tackle this problem by employing shallow neural networks, whereas our method
makes use of modern deep architectures. In this regard, lately a wealth of works dealing with
catastrophic forgetting in context of deep neural networks have appeared in the literature, see e.g.,
(Kirkpatrick et al., 2017; Zenke et al., 2017; Li & Hoiem, 2016; Serra et al., 2018; AljUndi et al.,
2017). However, all of these methods have been proposed for a “task-incremental learning” setup,
where a seqUence of disjoint tasks shoUld to be learned one after the other by a single network. In oUr
work, however, a completely different incremental methodology is Used. We specifically propose
a method to overcome catastrophic forgetting within the “class-incremental learning” setUp. The
key difference in the task-incremental setUp is that the model learns a separate classifier for each
task (i.e., mUlti-head), whereas in the latter the model learns only a single classifier for all of the
observed classes of all tasks (i.e., single-head). Several continUoUs learning approaches (RebUffi
et al., 2016; NgUyen et al., 2017; Kemker & Kanan, 2017), address catastrophic forgetting in the
class-incremental setting, i.e. by storing raw samples of previoUsly seen data and making Use of
them dUring the training of sUbseqUent tasks.
There is a growing interest in employing deep generative models for memorizing previoUsly seen
data distribUtions instead of storing old samples. ThUs Shin et al. (2017); WU et al. (2018) rely on
the idea of generative replay, which reqUires retraining the generator from scratch at each time step
on a mixtUre of synthesized images of previoUs classes and new real samples. Apart from being
inefficient for training, it is severely prone to “semantic drifting”. Namely, the qUality of images
generated at every memory replay point highly depends on the images generated dUring previoUs
replays, which can resUlt in loss of qUality over time. In contrast to the methods described above, we
propose to Utilize a single generator that is able to incrementally learn new information dUring the
normal adversarial training withoUt the need to replay previoUs knowledge. We achieve it thoUgh
efficiently learning a sparse mask for the layer activations of the generator network.
Similar to oUr method, Seff et al. (2017) proposed to avoid retraining the generator at every time-
step on the previoUs classes, by applying per-parameter regUlarization selectively in the generative
network. We pUrsUe a similar goal with the key difference that we expand the capacity of generator
dynamically (with increasing amoUnt of attained knowledge) instead of regUlarizing parameters of
previoUs tasks. A similar strategy has been proposed in Yoon et al. (2018). However, they propose to
keep track of the semantic drift in every neUron and then expand the network by dUplicating neUrons
that are sUbject to sharp changes. In contrast, we compUte weights importance concUrrently dUring
the coUrse of network training by modeling the neUron behavior Using an explicit binary mask. As
1Represented as a learnable binary mask applied to the network’s activations.
2
Under review as a conference paper at ICLR 2019
a result, our method explicitly does not require any further network retraining after adding new ca-
pacity. Another approach relying on dynamic network extension was proposed by Rusu et al. (2016)
and improved by Schwarz et al. (2018). In Progressive Neural Networks Rusu et al. (2016) authors
propose to assign a separate column (network) to every new task, while insure positive knowledge
transfer by maintaining trainable connections from the pretrained columns of previous task to the
current task’s column. Schwarz et al. (2018) address the scalability problem of Progressive Net-
works by only keeping two columns: a knowledge base and an active column while distilling the
knowledge continuously from the later to the former one. In our approach we only keep a single
column for all tasks in which knowledge transfer happens naturally, since parameters of old tasks
can be freely reused by the new ones.
Other approaches like (Kemker & Kanan, 2017; Kamra et al., 2017) try to explicitly model short
and long term memory with separate networks while transferring the knowledge from the former
one to the later in a separate “sleeping” phase. In contrast to these methods our approach does
not explicitly keep two separate memory locations, but models it implicitly in a single memory
network. The memory transfer occurs during the binary mask learning from non-binary (short term)
to completely binary (long term) values.
Most related to our paper is the work by Serra et al. (2018), which considers a task-based hard
attention mechanism that preserves previous tasks’ information without affecting the current task’s
learning. In this regard, they propose to learn a binary ask for the parameters of a base network
similar to Courbariaux et al. (2015) and Mallya & Lazebnik (2018) and Mancini et al. (2018). In
this fashion, while the parameters of the network are kept constant, the binary mask is used to
find a task specific perturbation of the base network that maximizes performance on incrementally
introduced new tasks. We build upon this idea and extend it by a mask for the neurons to be jointly
learned with the network parameters directly within the generator, thus allowing it to work in class-
incremental setup without storing samples of previous tasks. Such binary mask also provides us
with an efficient growth mechanism for network capacity by explicit modeling of plasticity and
abandoning of over-capacity.
3	Preliminaries
Adopting the notation of Chaudhry et al. (2018), let Dk = {(xik, yik)}in=k1 denote a collection of data
belonging to the task k ∈ K, where xik ∈ X is the input data and yik ∈ yk are the ground truth labels.
While in the (standard) non-incremental multi-task setup the entire data D = ∪kK=0Dk is available
at once, in a task-incremental setup the incoming dataset Dk becomes available to the model con-
tinuously and specifically only during the learning of task k . Thereby, Dk can be composed of a
collection of class items from the same task, or more generally only from a single class. We ad-
dress the latter class-incremental setup, in which the model is presented data from a single class at
a time. Furthermore, during test time the output space covers all the labels observed, independently
from the task they belong to: Yk = ∪jk=1yj. Such an evaluation scenario is often referred to as
a single-head evaluation (Chaudhry et al., 2018; Rebuffi et al., 2016) and is predominantly used in
class-incremental learning.
4	Method
In the continuous learning setup the task at a time t is to learn the parameters θ of the predictive
model fθ : X → Yt, where Yt = ∪tj =1yj denote all classes seen so far. This requires solving the
following problem:
min L(θ,Dt)+Ω(θ),	(1)
θ
where t is the task index, L is the loss function and Ω(θ) is the regularization term. It should be
noted that if Dt = {(xk ,yt)}, i.e. the training data of task t only contains a single class, e.g.
|{y}| = 1 , the loss can not be computed without referring data from previously seen classes.
However, storing any training data violates the strictly incremental setup. In order to overcome this,
3
Under review as a conference paper at ICLR 2019
Figure 1: Dynamic generative memory: The AC-GAN Odena et al. (2016) architecture allows simul-
taneous adversarial and auxiliary training, in which auxiliary output is trained on the real samples
of the current task and synthesized sample of previously seen tasks. During the adversarial train-
ing with real and fake samples of current class a connection plasticity in the generator is learned
simulataniously with the weights of the base network. Connections plasticity is represented by task
specific, trainable binary masks for base netwrok’s activations.
a generator G(yt, z, mt) based on AC-GAN (Odena et al., 2016) is employed to synthesize samples
of the previously seen classes.
However, in order to facilitate learning in an incremental setting and to mitigate catastrophic for-
getting, the generator G is further associated with: (i) a learned binary mask - see Sec. 4.1, (ii) a
dynamic capacity expansion mechanism - see Sec. 4.2.
The learned binary masks models the plasticity of neurons and thus avoids overwriting important
units by directing SGD updates in segments of the neural network that correspond to free capacity,
or capacity that can be shared. The dynamic network expansion facilitates adding neurons in order
to provide sufficient capacity to deal with new tasks, capacity that is efficiently allocated based on
task-wise mask allocation.
To this end, in every SGD step t we learn a binary mask Mt = [mt1, ..., mlt] based on the previous
layer’s activations of the generator G(yt, z, mt). The binary mask Mt is then modulated with the
layer activation, yielding e.g.
yt = mt Θ (W1 ∙ x),
(2)
for a fully connected layer l, where mlt is an n-element vector and Wl is the weight matrix between
layer l and l - 1 shaped as m × n.
Subsequently, a network HΘ serves both as a discriminator for newly synthesised samples and
classifier for the overall learning problem. HΘ is reinitialized for every task t and retrained using
synthesized samples of the previously seen tasks.
4.1	Learning a binary mask
Similarly to other approaches that aim to learn a binary mask for the network parameters (Cour-
bariaux et al., 2015; Serra et al., 2018; Mallya & Lazebnik, 2018), We make use of real valued mask
embeddings elt , which are scaled by a positive scaling parameter s, and passed through a threshold
function σ(x) ∈ [0, 1]. The binary mask is thus given by mlt = σ(selt)
We use sigmoid function as a pseudo step-function such that a gradient floW to train mask embed-
dings e is ensured. As pointed out by Serra et al. (2018), the scaling parameter S controls the degree
of binarization of the mask m. Specifically, the higher the value of s, the higher the extent of bi-
narization, and vise versa, i.e. mlt → {0, 1} for s → ∞, mlt → 0.5 for s → 0. Over the course
of training, s is incrementally annealed from 0 to smax , largely folloWing the scheme proposed by
Serra et al. (2018):
S = Smax+(smax - Smax) 匕,	⑶
4
Under review as a conference paper at ICLR 2019
where i corresponds to the current epoch and I is the total number of epochs.
In order to prevent the overwriting of the knowledge related to previously seen classes in the gen-
erator network, the gradients gl of the weights of each layer l are multiplied by the reverse of the
cummulated binary masks for all previous tasks:
gl0 = [1 - ml≤,mt ×n]gl,
ml≤t = max(mlt, mlt-1),
(4)
(5)
where gl0 corresponds to the new weights gradient and ml≤,mt ×n is a cummulated mask expanded to
the shape of gl (n times duplication of m≤t to match sizes).
In the cummulated attention mask the parameters that are important for the previously learned
classes are masked with values of ”1” (or close to 1), whereas free parameters that can be modified
during further training are masked with ”0”. Parameters reserved for previous tasks (marked with
”1”) can be used during further training, however, their values can not be modified. The higher the
sparsity of the mask, the higher the number of parameters that can be modified during consecutive
training steps.
Similarly to Serra et al. (2018), We promote sparsity of the binary mask by adding a regularization
term Rt to the loss function LG of the AC-GAN generator:
Rt(Mt, Mt-1
PL-11 PNli 1 - m<t
(6)
Where Nt is the number of parameters of the layer l. Parameters that Were reserved previously
by other tasks are not regularized, While parameters that have never been used are. This promotes
reusing neurons over reserving neW ones and promotes efficiency in neuron allocation per task.
Joint training: The system must learn to perform three tasks: A generative, a discriminative and
finally a classification task in the strictly incremental class setup.
As such, using task labels as conditions, the generator netWork (G) must learn from a training set
Xt = {X1t, ..., XNt } to generate images for task t. AC-GAN’s conditional generator synthesizes
images xt = Gθt (t, z, Mt), Where θGt represents the parameters of the generator netWork at task
t, z a random noise vector, and Mt the computed binarization mask for task t. The discriminator
netWork is used to perform tWo tasks. First, a discriminative task, determining Whether sample xf is
real or fake. Second, a classification task, indicating Whether xf can be labelled as part of the task
t. To achieve both, the final layer of the netWork has tWo branches corresponding to each task, and
the base netWork added to each layer is parameterized With θHt and θCt respectively. The parameters
corresponding to the three tasks, θGt , θHt , θCt are optimized in an alternating fashion. As such, the
generator optimization problem can be seen as minimizing LG = Lts - Ltc + λRURt, With Lc a
classification error on the auxiliary output, Ls a discriminative loss function used on the binary
output layer of the netWork, and λRU Rt the regularizer term expanded upon in Equation 6.
To promote efficient neuron use taking into consideration the amount of the netWork already in use,
the regularization weight λ is multiplied by the ratio a = SSt-, where St is the size of the network
Sf ree
before training the task t, and Sf ree is the number of free neurons. This ensures that less parameters
are reused during the beginning stages of training, and more during the later stages.
The discriminator is optimized similarly through minimizing LD = Ltc - Lts + λGP Ltgp, where Ltgp
represents a gradient penalty term implemented as in Gulrajani et al. (2017), to ensure a more stable
training process.
4.2	Dynamic network expansion
As discussed by Mancini et al. (2018) the more significant the domain shift between the learned data
batches, the quicker the network capacity will be exhausted and the effects related to catastrophic
forgetting will manifest. This can be attributed to the overall decline in sparsity of the of the accu-
mulated mask ml≤t over the course of training. In order to avoid this effect, we keep the sparsity of
the binary mask equal for each training cycle t.
5
Under review as a conference paper at ICLR 2019
Assuming a network layer l with input vector of size m and output vector of size n. At the beginning
of the initial training cycle on D0 the binary mask ml is initialized at size n, with zero sparsity. Thus,
all neurons of the layer will be used, with all n values of the binary mask set to 0.5 (real-valued
embeddings e are initialized with 0).
After the initial training cycle with the sparsity regularizer R0, the sparsity of the mask will decrease
to n - δt , where δt is the number of parameters reserved for the generation task t. Subsequently,
after the training cycle Do, We expand the number of output units n of the layer l by δt∕m. This
guarantees that the free capacity of the layer is kept constant at n for each learning cycle.
5	Experimental Results
In the folloWing section We provide a qualitative and quantitative evaluation of our method on a
number of publicly available datasets. Furthermore, We provide a discussion upon the performance
of the different components of our system.
5.1	Experiments
We perform experiments measuring the classification accuracy of our system in a strictly class incre-
mental setup on three benchmark datasets: MNIST (LeCun, 1998), SVHN (Netzer et al., 2011) and
CIFAR-10 (Krizhevsky et al., 2014). Both a 10-class and 5-class accuracy is reported, to provide a
high level impression on the criticality of the catastrophic forgetting problem: ideally a system that
does not forget Will see results that are stable, or even increasing as it learns to generalize better
from previously seen tasks.
All datasets are used to train a classification netWork in the strictly incremental setup, and the per-
formance of our method is evaluated quantitatively through comparison With benchmark methods.
Note that We compare to both type of methods; (i) Where the strictly incremental constraints are
relaxed, Rebuffi et al. (2016), Kirkpatrick et al. (2016), Chaudhry et al. (2018), Zenke et al. (2017),
(ii) methods that strictly adhere to the setup, making use of a type of memory: Seff et al. (2017),
Shin et al. (2017), Wu et al. (2018). Further, We present a qualitative evaluation of CIFAR-10 using
a perceptual metric (Heusel et al., 2017).
Datasets: The MNIST and SVHN datasets are composed of 60000 and 99289 images respectively,
containing digits. The main difference is in the complexity and variance of the data used. SVHN’s
images are cropped photos containing house numbers and as such present varying vieWpoints, il-
luminations, etc. All images are further resized to 32 x 32 before use. Finally, CIFAR10 contains
60000 32 × 32 labelled images, split in 10 classes, roughly 6k images per class.
Architectures: We make use of the same architecture for the MNIST and SVHN experiments, a
3-layer DCGAN (Radford et al. (2015)), With the generator’s number of parameters modified to
be proportionally smaller than the architecture reported in Wu et al. (2018). The projection and
reshape operation is further performed With a convolutional layer instead of a fully connected one.
For the CIFAR-10 experiments, We use the CIFAR-10 ResNet architecture proposed by Radford
et al. (2015). Note that all architectures used have been modified to function as an AC-GAN, i.e. the
discriminator has both a discriminative and a classification role.
5.2	Results
A quantitative comparison of the proposed DGM approach With other methods that operate in a
single-head regime on the MNIST dataset is listed in Table 1. The first set of methods do not adhere
to the strictly incremental setup, and thus make use of stored samples. As such these methods are
not generative, and have difficulties in generalizing across different tasks due to inability to capture
sufficient diversity. They are sharply outperformed by our method, despite of having access to
previously observed data during training.
The second set of methods We compare With use a single-head evaluation protocol, and beyond that
do not store any additional data. Our method significantly outperforms Seff et al. (2017) and Shin
et al. (2017) on the MNIST benchmark through the integration of the memory learning mechanism
directly into the generator netWork, and the expansion of said netWork as it saturates to accommodate
6
Under review as a conference paper at ICLR 2019
Method	MNIST		SVHN		CIFAR10	
	A10(%)	A5 (%)	A10(%)	A5(%)	A10(%)	A5(%)
iCarl-S (Rebuffi et al., 2016)	-	55.8	-	-	-	-
EWC-S(Kirkpatrick et al., 2016)	-	79.7	-	-	-	-
RWalk-S(Chaudhry et al., 2018)	-	82.5	-	-	-	-
PI-S (Zenke et al., 2017)	-	78.7	-	-	-	-
EWC-M (Seff et al., 2017)	77.03	70.62	33.02	39.84	-	-
DGR-M (Shin et al., 2017)	85.4	90.39	47.28	61.29	-	-
MeRGAN-M (Wu et al., 2018)	97.0	98.19	66.78	80.90	-	-
Joint Training	98.1	97.66	84.82	85.30	64.2	82.2
DGM (ours)	99.17	98.14	68.36	84.18	50.8	62.5
Table 1: Comparison to the benchmark presented by [15] of approaches that make use of generative
memory. Joint training (JT) represents the the performance of the discriminator trained in non-
incremental fashion. Performance of DGM is presented after incremental learning of 5 and 10
classes for MNIST, SVHN and CIFAR10 datasets.
new tasks. We yield an increase in performance over Wu et al. (2018) , a method that is based on a
similar architecture but does not provide dynamic network expansion and what is more makes use
of a replay strategy for memory purposes, losing any information not sampled during training. As it
can be observed for both our method and MeRGAN, the accuracy reported between 5-tasks and 10-
tasks has changed little, suggesting that for this dataset and evaluation methodology both methods
have largely curbed the effects of catastrophic forgetting.
Interestingly, DGM outperforms joint training on the MNIST dataset using the same architecture.
This suggests that the strictly incremental training methodology indeed forced the network to learn
better generalizations compared to what it would learn given all the data.
Given the high accuracies reached on the MNIST dataset largely give rise to questions about satura-
tion. We thus perform further evaluation on the more visually diverse SVHN dataset (Netzer et al.
(2011)), where increased data diversity translates to a more difficult generation and susceptibility
to catastrophic forgetting. In fact, as can be seen in Table 1, the difference between 5-task and
10-task accuracies is significantly larger in all methods than what can be observed in the MNIST
experiments.
Our system sharply outperforms all other methods on this task. This can be attributed to a network
that is initialized with a small size preventing overfitting on single tasks. At the same time, the
network will expand at minimal capacity requirement in order to accommodate joint distribution
expansion by integrating the next tasks w.r.t. to all previously seen classes. DGM thus becomes
more stable in the face of catastrophic forgetting.
The quality of the generated images after 10 stages of incremental training for both MNIST and
SVHN can be observed in Fig. 2. The generative network is able to provide an informative and
diverse set of samples for all previously seen classes without catastrophic forgetting.
7
Under review as a conference paper at ICLR 2019
Figure 4: Evolution of Frechet Inception Dis-
tance (FID), the lower the better. FID decreases
during the absence for forgetting is evidenced by
the fact that FID does not increase during the
learning subsequent task.
Figure 3: Evolution of the classification accuracy
over the 10 tasks of the CIFAR10 dataset. Per-
formance of the currently learned task is always
high (real samples are available), it drops during
the subsequent tasks due to increasing problem
complexity.
Figure 5: Samples generated for CIFAR10 after 5th task
Finally, on the CIFAR-10 dataset we provide a Frechet Inception Distance (Heusel et al., 2017)
metric of the generated images over the first 5 classes in order to asses the perceptual quality of the
generation. Figure 4 provides the dynamics of this metric while training across the different tasks.
In conjunction with the qualitative results shown in Fig. 5 it can be observed that little to no quality
is lost for generating historical task samples. For each newly learned task the discriminator network
is reinitialized, but the knowledge contained in the generator is quickly leveraged thus making re-
learning a fast process. With the more complex CIFAR samples, more of the task is forgotten, but
the task knowledge is maintained as observed in the qualitative results.
5.3 Discussion
The primary strength of our proposed method is an efficient network expansion component, whose
performance is directly related to how neurons are reserved in the strictly incremental setup.
SUoJnou Jo JoqumN
Figure 6: Neural capacity analysis
8
Under review as a conference paper at ICLR 2019
Figure 7: Neuron plasticity during the training of a task t. The standard deviation of the mask values
is shown as a blue shadow.
We first analyze how learning is accomplished within a given task t, and how this further affects
the wider algorithm. For a given task t, its corresponding binary mask Mt is initialized with the
scaling parameter s = 0. As observed in Fig. 7 (A-1), at task initialization the mask is completely
non-binary, with no new neurons set aside to learn the new task.
As training progresses for the given task, both the scaling parameter s and regularization scaling
parameter λ are annealed and increased. With heavy regularization and increasing binarization, the
network is therefore encouraged to make use of the smallest possible number of neurons, behaviour
observed in region (A-2).
But with most mask values near 0, the network’s capacity to learn is greatly curtailed. The opti-
mization process pushes the mask to become less binary so that neurons can have better mobility,
and a short-term memory is formed. The number of mask values corresponding to useful neurons is
steadily increased with the binarization bias, a trend observed in region (B) as the standard deviation
of the mask values corresponding to each neuron. This behaviour can be seen as a state transition
where the most representative neurons are selected and reserved by the network with neurons that
have not made this transition being essentially unused for learning task t.
Consequently, neurons can be broadly divided into three types for a given task t: (i) neurons that
are not used at all (U), which can be seen as a network’s free capacity, (ii) neurons that are newly
blocked (NB), (iii) neurons that have been reused from previous tasks (R).
Figure 6 presents the evolution of the ratio of the (NB) and (R) types over the total number of
neurons. Of particular importance is that the ratio of reused neurons is increasing between tasks,
while the ratio of newly blocked neurons is decreasing. These trends can be justified by the network
learning to generalize better, leading to a more efficient capacity allocation for new tasks.
An issue with reserving the representative neurons of each task is that the network will eventually
run out of capacity - and as such we maintain a constant number Nf ree of unused neurons (Figure
6, shaded area). This is achieved by expanding the generation network after each task with the
number of newly blocked neurons denoted as NB(t). Network grow is thus directly linked to how
efficiently neurons can be reserved per task, which ultimately depends on how well the generative
network can generalize from previously learned tasks. We can observe the efficiency of our network
growth method compared to a worst case scenario, where for every task we add the same number of
parameters Ninit to the network.
Indeed, the manner in which G is learned can be seen as learning for both representative and dis-
criminative features. This learning regime has the capacity of generalizing better compared to a
network that is simply trained with the entirety of the dataset, as seen in Table 1, where our method
outperforms the multi-label baseline on MNIST.
9
Under review as a conference paper at ICLR 2019
6 conclusion
In this work we study the continual learning problem in a single-head, strictly incremental context.
Our results suggest that the proposed approach successfully overcomes catastrophic forgetting sce-
narios by making use of a conditional generative adversarial model where the generator is used as a
memory module through neural masking. Furthermore, we show that the proposed dynamic memory
expansion mechanism facilitates a resource efficient adaption in order to successfully accommodate
learning new tasks. For future work we plan to expand plasticity to also incorporate weights as well
as to refine the dynamic capacity expansion.
References
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget. CoRR, abs/1711.09601, 2017. URL
http://arxiv.org/abs/1711.09601.
Arslan Chaudhry, Puneet Kumar Dokania, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Rie-
mannian walk for incremental learning: Understanding forgetting and intransigence. CoRR,
abs/1801.10112, 2018. URL http://arxiv.org/abs/1801.10112.
Joseph Cichon and Wen-Biao Gan. Branch-specific dendritic ca2+ spikes cause persistent synaptic
plasticity. Nature, 520(7546):180-185, 2015.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre Binaryconnect David. Training deep
neural networks with binary weights during propagations. arxiv preprint. arXiv preprint
arXiv:1511.00363, 2015.
Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences,
3(4):128-135, 1999.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Akiko Hayashi-Takagi, Sho Yagishita, Mayumi Nakamura, Fukutoshi Shirai, Yi I Wu, Amanda L
Loshbaugh, Brian Kuhlman, Klaus M Hahn, and Haruo Kasai. Labelling and optical erasure of
synaptic memory traces in the motor cortex. Nature, 525(7569):333, 2015.
Martin HeUseL HUbert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and
Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium.
arXiv preprint arXiv:1706.08500, 2017.
Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for continual
learning. arXiv preprint arXiv:1710.10368, 2017.
Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning.
arXiv preprint arXiv:1711.10563, 2017.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks. CoRR, abs/1612.00796, 2016. URL http://arxiv.org/abs/
1612.00796.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
pp. 201611835, 2017.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online: http://www. cs.
toronto. edu/kriz/cifar. html, 2014.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
10
Under review as a conference paper at ICLR 2019
Zhizhong Li and Derek Hoiem. Learning without forgetting. CoRR, abs/1606.09282, 2016. URL
http://arxiv.org/abs/1606.09282.
Arun Mallya and Svetlana Lazebnik. Piggyback: Adding multiple tasks to a single, fixed network
by learning to mask. arXiv preprint arXiv:1801.06519, 2018.
Massimiliano Mancini, Elisa Ricci, Barbara Caputo, and Samuel Rota Bulo. Adding new tasks to a
single network with weight trasformations using binary masks. arXiv preprint arXiv:1805.11119,
2018.
Mark Mayford, Steven A Siegelbaum, and Eric R Kandel. Synapses and memory storage. Cold
Spring Harbor perspectives in biology, pp. a005751, 2012.
Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. volume 24 of Psychology of Learning and Motivation, pp. 109 —
165. Academic Press, 1989. doi: https://doi.org/10.1016/S0079-7421(08)60536-8. URL http:
//www.sciencedirect.com/science/article/pii/S0079742108605368.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.
Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning.
arXiv preprint arXiv:1710.10628, 2017.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-
iary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Roger Ratcliff. Connectionist models of recognition memory: Constraints imposed by learning and
forgetting functions. Psychological Review, pp. 285-308,1990.
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, and Christoph H. Lampert. icarl: Incremental
classifier and representation learning. CoRR, abs/1611.07725, 2016. URL http://arxiv.
org/abs/1611.07725.
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR,
abs/1606.04671, 2016. URL http://arxiv.org/abs/1606.04671.
Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska,
Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame-
work for continual learning. arXiv preprint arXiv:1805.06370, 2018.
Ari Seff, Alex Beatson, Daniel Suo, and Han Liu. Continual learning in generative adversarial nets.
arXiv preprint arXiv:1705.08395, 2017.
Joan Serra, Dldac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018. URL http://arxiv.
org/abs/1801.01423.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In Advances in Neural Information Processing Systems, pp. 2990-2999, 2017.
C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs:
learning to generate images from new categories without forgetting. In Advances In Neural Infor-
mation Processing Systems, 2018.
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically
expandable networks. 2018.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Improved multitask learning through synaptic
intelligence. CoRR, abs/1703.04200, 2017. URL http://arxiv.org/abs/1703.04200.
11