Under review as a conference paper at ICLR 2019
Theoretical and Empirical Study of Adversar-
ial Examples
Anonymous authors
Paper under double-blind review
Ab stract
Many techniques are developed to defend against adversarial examples at scale. So
far, the most successful defenses generate adversarial examples during each training
step and add them to the training data. Yet, this brings significant computational
overhead. In this paper, we investigate defenses against adversarial attacks. First,
we propose feature smoothing, a simple data augmentation method with little
computational overhead. Essentially, feature smoothing trains a neural network
on virtual training data as an interpolation of features from a pair of samples,
with the new label remaining the same as the dominant data point. The intuition
behind feature smoothing is to generate virtual data points as close as adversarial
examples, and to avoid the computational burden of generating data during training.
Our experiments on MNIST and CIFAR10 datasets explore different combinations
of known regularization and data augmentation methods and show that feature
smoothing with logit squeezing performs best for both adversarial and clean
accuracy. Second, we propose an unified framework to understand the connections
and differences among different efficient methods by analyzing the biases and
variances of decision boundary. We show that under some symmetrical assumptions,
label smoothing, logit squeezing, weight decay, mix up and feature smoothing all
produce an unbiased estimation of the decision boundary with smaller estimated
variance. All of those methods except weight decay are also stable when the
assumptions no longer hold.
1	Introduction
Machine learning models are often vulnerable to adversarial examples, which are maliciously
designed to cause misclassification. In the area of computer vision, for instance, object recognition
classifiers are much more likely to incorrectly classify images that have been modified with small,
often inpreceptible perturbations. Similar problems also occur in natural language processing area,
see (Miyato et al., 2017), where small perturbations of text can easily fool a label classification
model. It is therefore important to develop machine learning models that are resistant to adversarial
examples in situations where attacker may attemp to interfere, for example with autonomous vehicles
(Papernot et al., 2017). Understanding the design mechanisms of adversarial examples can also help
researchers to gain a better understanding of the performance of machine learning, especially deep
learning models. In this paper, we introduce an efficient feature smoothing method to improve the
adversarial robustness of neural networks and also build a theoretical framework to understand how
different approaches help with the adversarial accuracy.
Different adversarial training methods have been proposed to increase robustness by augmenting
training data with adversarial examples. Goodfellow et al. (2015) developed the fast gradient signed
method (FGSM), which efficiently generated adversarial example by a “single-step” attack based
on a linearization of the model’s loss. Their trained model is robust to single-step perturbations but
remains vulnerable to more costly “multi-step” attacks. Madry et al. (2017) extended FGSM by
proposing a multi-step variant FGSM, which is essentially projected gradient descent(PGD). They
suggested that adversarial training with the PGD attack is a universal first order adversary defense,
which means that models trained against PGD attacks are also resistant against many other first order
attacks. Their PGD attacks consists of initializing the search for an adversarial examples at a random
point within the allowed norm ball, then running several iterations of the basic iterative method to
find an adversarial examples. Kannan et al. (2018) then introduced a logit pairing method (ALP)
1
Under review as a conference paper at ICLR 2019
which encourages the logits for pairs of examples and their corresponding adversarial examples to be
similar. Logit pairing improves accuracy on adversarial examples over trainings based on PGD.
The above successful approaches performed data augmentation by generating adversarial examples
during each training step, which will unfortunately bring significant computational burden to the
training process. In contrast, more “efficient” training methods without hindering the training speed
have also been shown to improve adversarial robustness (In this paper we refer “efficient” methods as
data augmentation and regularization methods including mixup, label smoothing, logit squeezing,
weight decay, and our proposed feature smoothing). Szegedy et al. (2016) proposed label smoothing,
which trains a classifier using soft targets for the cross-entropy loss rather than the hard targets. The
correct class is given a target probability of 1 - α and the remaining α probability mass is divided
uniformly among incorrect classes. Label smoothing reduces overfitting by preventing a network
from assigning full probability to each training data, and also offers a small amount of robustness to
adversarial examples (Kannan et al., 2018). Kannan et al. (2018) proposed a logit squeezing method
which penalizes the logit of each input example. It is showed that combined with adding Gaussian
noise into input examples, logit squeezing gave even better results than ALP in some datasets, for
example MNIST and SHNV. Zhang et al. (2018) performed data augmentation by training the model
on virtual input points as interpolation of two random examples from the training set and their labels,
resulting in increasing both the robustness of adversarial examples and the accuracy in clean test data.
In parallel, many theorems have also been proposed to understand the power and existence of
adversarial examples. Transferability is shown to be a common property of adversarial examples.
Szegedy et al. (2014) and Papernot et al. (2016) found that adversarial examples generated based
on a specific neural network can fool both the same neural network trained with different datasets
and different neural networks trained with the same dataset. The existence of adversarial examples is
still an open question. Possible reasons have been suggested in recent papers, such as low density
(Szegedy et al., 2014; Pei et al., 2017), decision boundary too close to the training data (Tanay &
Griffin, 2016). However, there are few papers theoretically explaining the similarities and differences
between those methods, especially based on their estimation of decision boundaries. Goodfellow et al.
(2015) discussed the differences between weight decay and adversarial training by comparing their
loss functions in logistic regression, but didn’t show how these two methods affect the estimation and
accuracy.
The above discussion leaves us two questions:
•	Without adding any computational burden during training, these “efficient” methods mainly
benefit from data augmentation and regularization, and as a result, resist against adversarial
examples to some extent. As most of them are not specifically designed for resisting against
adversarial examples, can we develop an “efficient” approach specifically designed to be
robust to adversarial examples?
•	What are the connections and differences among these “efficient” methods? Can we build a
unified framework to analyze them?
Motivated by these two questions, we investigate defenses against adversarial attacks, and the
contribution is two-fold. We first propose feature smoothing, a data interpolation method that
softens the features of input. We show that feature smoothing obtains better performance than other
“efficient” approaches on both MNIST and CIFAR10. We also observe the best performance when
combining our feature smoothing method and logit squeezing strategy, among all “efficient” methods.
We also propose a unified framework to understand how different “efficient” approaches influence
the estimation of decision boundary. In particular, based on both simulations and theoretical analysis
of logistic regression, we show that under some symmetrical assumption, label smoothing, logit
squeezing, weight decay, mixup, feature smoothing and data extrapolation all give an unbiased
estimation of boundary with smaller estimation variance. But regularization with weight decay is
more sensitive when the assumption may not hold. We believe it is the reason weight decay can hurt
the accuracy in clean test data. Our framework are also partially extended to deep convolutional
neural networks.
The paper is organized as follows. Section 2 presents our proposed method and other related “efficient”
methods. Section 3 reports the performance of feature smoothing against other “efficient” methods.
We conduct theoretical analysis and explore the connections and differences among different methods
in Section 4. The last section concludes.
2
Under review as a conference paper at ICLR 2019
(a) Clean data (b) Adversarial training (c) Data interpolation (d) Gaussian noise
Figure 1: Toy examples of binary classification problem with blue circles and red rhombus repre-
senting two classes of data. The blue dash lines and solid lines show true boundary and estimated
boundary respectively. (a) The original data; (b) Adversarial examples added into training data; (c)
Virtual data interpolated between classes added in training data; (d) Gaussian random noise added
into input.
2	Method
Following the idea of adversarial training, we propose feature smoothing method which also adds
new data into the training set to improve the robustness. Other than generating adversarial examples
based on current model, feature smoothing mimics adversarial examples by data interpolation
and adding Gaussian noise directly based on the original training data. We will introduce feature
smoothing and discuss several related methods in the following.
2.1	Feature smoothing
In a classification problem, we aim to recover the unknown decision boundary based on the training
data (Figure 1(a)). As long as the decision boundary is correctly estimated, there will be no adversarial
examples. Tanay & Griffin (2016) suggested that neural networks which estimate decision boundary
too close to the training data causes adversarial problems. The incorrect estimation of boundary may
be caused by low density (Szegedy et al., 2014) of input data where adversarial examples exists. In
adversarial training, the estimation is improved by adding adversarial examples into input (Figure
1(b)) during each step.
Based on this idea, if we are able to generate ‘low density’ data directly based on the original training
set, we can also improve the estimation as what adversarial training does but with much smaller
computational cost. We now introduce feature smoothing, a simple data augmentation approach
which generates new virtual training data as interpolation of features from a pair of random samples.
Virtual training data are constructed as follows:
X(i) = (1 - α)x(i) + αx(j),	y(i) = y(i),
where (x(i), y(i)) and (x(j), y(j)) are two examples drawn randomly from our training data, and
0 ≤ α < 0.5. When x(i) and x(j) belong to different classes, and the interval between these two data
points intercept with the decision boundary only once, Xi is closer to the boundary than Xi or Xj.
Figure 1(c) shows that adding new data interpolated between classes can help with the estimation of
decision boundary.
Furthermore, Gaussian noise also helps extend the range ofx. Figure 1(d) shows that adding Gaussian
random noise with proper variance into input can also push the estimated boundary closer to the
true boundary compared against original clean data. Hence we add Gaussian noise into our feature
smoothing method as well:
X(i) = P (x(i) + €),	y(i) = y⑴，
where € 〜Normal(0,σ2) and P(x) projects X to the range of original data. To distinguish
data interpolation part and Gaussian noise part, we use ‘feature smoothing’ only referring to data
interpolation and ‘noise’ for the Gaussian noise part in the following. A detailed illustration of how
feature smoothing helps the estimation of boundary is discussed in Section 4.
3
Under review as a conference paper at ICLR 2019
2.2	Related methods
Though starting from different intuitions, feature smoothing turns to be very similar with mixup
(Zhang et al., 2018). In mixup, additional virtual data points are generated by interpolating both
features and labels of the original training data:
X = (1 — α)x(i) + αx(j),	y = (1 — α)y(i) + αy(j),
where α ∈ [0, 1]. Mixup can be understood as a form of data augmentation that encourages the
model to behave linearly in-between training examples. Zhang et al. (2018) argued that this linear
behavior reduces the amount of undesirable oscillations when predicting data outside the training
examples. On the contrary, our feature smoothing method includes the interpolations with new label
remaining the same as the dominant data point, which maintains the S-shaped curve of logistic model
and also allow feature smoothing easier to be combined with regularization methods. More detailed
comparison can be found in Sec. 4.
Label smoothing (LaS) and logit squezzing (LoS) are other two efficient ways which improve the
adversarial accuracy. Let y ∈ RK be one-hot label for K classes, label smoothing (Szegedy et al.,
2016) softens the target by replacing y with
y = ττδ-j-(I - y) +(I - δ)y,
K-1
where δ = 0.1 is shown to be the best choice (Pereyra et al., 2017). Assume we train a model with
parameters θ on a batch of m data points {(x(i), y(i)), i = 1, 2, . . . , m}, y(i) ∈ {0, 1}K. Let f(x; θ)
denote the mapping function from x to logits of the model. Let L(clean) denote the cross entropy
loss for the batch of data points as:
mK
L(Clean)=」XX yji) iog(Pθ (yji)∣χ(i))).
m i=1 j=1
The loss function of label smoothing can also be achieved by some calculation:
m K	m K 1 K (i)
LLaS = -	XXyji) Iog(Pθ{yj ∖x m	m	K-- K - 1 订j'，°，.
m	m	-
i=1 j=1	i=1 j=1
Notice that if we assume our model obtains a good estimation of f(x, θ), then when yj = 0,
fj (x, θ) < 0 and when y = 1, fj(x, θ) > 0. In a binary classification case, LLaS can be written
as L(clean) + δ∣f (x, θ)∣, which further indicates that label smoothing predicts logits with smaller
magnitude and therefore avoids overfitting.
Similarly, logit squeezing (Kannan et al., 2018) applies a L2 norm on the logits directly as a penalty
of over-confidence:
λm
LLoS = LClean + - X ||f (x(i))∣∣2,
m	i=1
where LClean is the original loss of neural networks and f(x(i)) is the logit of image x(i) as above.
Weight decay is another well known regularizer which efficiently reduces overfitting of neural
networks by adding L1 or L2 penalty of weight w ,
Lwd = L(Clean) + λ∣∣w∣∣2.
However, weight decay is shown to be not very helpful for adversarial examples compared to label
smoothing and logit squeezing, which will be discussed in Sec 4.
Combination of different approaches In feature smoothing and mixup, we generate new data
points as linear combination of xi and xj . For generating the exact y value of these virtual points,
mixup uses a linear interpolation for estimation, while feature smoothing chooses the dominant label.
Nevertheless, it is also possible that feature smoothing or mixup also adds mislabeled noises into the
training data, especially when xi and xj are not symmetric to the boundary. In that situation, label
smoothing and logit squeezing are better ways to avoid overfitting. So we also consider to combine
these methods together to gain a better test and adversarial accuracy.
4
Under review as a conference paper at ICLR 2019
3 Experiments
3.1	Results on MNIST
We experiment feature smoothing, label smoothing, mix up, logit squeezing and their possible
combinations on MNIST, with the results summarized in Table 1. We find that combining feature
smoothing and logit squeezing give the best performance in both clean test data and adversarial
examples. For all experiments in this section, we train our models for 200 epochs and use Adam for
our optimizer with a learning rate at 10-4. Random noises are added into the training data in several
methods, with the same σ value of 0.5.
For MNIST, when α ranges between 0.2 and 0.4 we observe similar performance for feature smooth-
ing, whereas for large α at 0.5, too much noise in data label brings underfitting for feature smoothing.
We use a final α value of 0.3 for reporting results in Table 1. Chosen by cross validation, we use
α ∈ Beta(8, 8) for mixup, and δ = 0.1 for label smoothing. In logit squeezing, we use the weight λ
of 0.2 as experimented in Kannan et al. (2018). In feature smoothing and mixup, 10 new data points
are generated on each batch with batch size m = 50.
We use the LeNet model as Madry et al. (2017) and also apply the same attack parameters as they
provided. After scaling the range of images pixels into [0, 1] (divided by 255), we apply perturbation
per step of 0.01, 40 total attack steps with 1 random start, and the total adversarial perturbation
threshold set as 0.3. Similar with Madry et al. (2017), we also generate black box examples for
MNIST by independently initializing and training a copy of the LeNet model, then generate PGD
attack based on that model. Both cross entropy loss and correct-wrong loss are used.
Each single method improves a small amount of the adversarial accuracy, but combinations of them
lead to a much better performance (Table 1). Logit squeezing combined with feature smoothing and
Gaussian random noise achieves the best performance among all those “efficient” methods.
Method	PGD	PGD-cw	BlaCkBox-PGD	BlaCkBox-CW	Clean
Clean	0.00	-^0.00~~	83:04	83.95	99.17
Gaussian random noise	21.02	20.34	84.57	84.26	99.20
Logit Squeezing(LoS)	2.23	0.06	85.12	84.07	99.21
Label smoothing(LaS)	0.06	0.08	83.13	83.07	99.22
Mix up	1.06	1.04	83.67	84.08	99.22
Feature smoothing(FS, ours)	4.68	4.76	84.54	84.98	99.26
LoS + noise	71.49	69.04	89.01	85.34	99.22
LaS + noise	66.26	66.62	85.99	85.26	99.24
LoS + noise + mix-up	70.25	68.13	84.78	83.55	99.20
LoS + noise + FS	76.97	75.38	90.12	89.16	99.26
Table 1: Accuracies of different methods on MNIST. PGD: projected gradient descent with cross
entropy loss; PGD-cw: PGD with correct-wrong loss; Clean: test with original test data. For logit
squeezing, we also applied PGD with original cross entropy loss since it gives smaller adversarial
accuracy.
3.2	Results on CIFAR 1 0
We follow Madry’s lab for the experiments in CIFAR10. For all experiments in this section, we train
our models for 80000 global steps with batch size m = 128 in each step. We use Momentum at 0.9
for our optimizer with a learning rate at 0.1 for the begining, 0.01 after 40000 global steps and 0.004
after 60000 global steps. Weight decay with λ = 0.0002 is also applied to all experiments. We use
α ∈ Beta(8, 8) for mixup, δ = 0.1 for label smoothing, λ = 0.1 for logit squeezing , α = 0.2 for
feature smoothing, and σ = 0.5 for Gaussian random noise. In feature smoothing and mixup, 10 new
data points are randomly generated on each batch with batch size m = 128.
We apply the ResNet model and the same attack parameters as they used. We use perturbation per
step of 2.0, 20 total attack steps with 1 random start and the total adversarial perturbation threshold
set as 8. The black box adversarial examples are also generated by independently initializing and
training a same model. Logit squeezing combined with feature smoothing and Gaussian random
noise still performs the best among all of the ‘efficient’ methods (Table 2).
5
Under review as a conference paper at ICLR 2019
Method	PGD	PGD-cw	BIaCkBox-PGD	BIaCkBox-CW	Clean
clean	0.00	-^000~~	973	924	94.07
LoS + noise	29.13	3.81	32.49	12.40	94.17
LaS + noise	29.20	3.78	33.79	11.54	94.41
LoS + noise + mix-up	27.49	3.85	43.22	15.24	94.62
LaS + noise + mix-up	27.12	3.85	43.40	15.09	94.62
LoS + noise + FS	32.71	9.03	48.24	17.36	95.0
LaS + noise + FS	30.1	5.31	49.11	16.72	94.8
Table 2: Top 1 accuracy in CIFAR10.
4	Theoretical explanations
In this section, we show that the above “efficient” methods increase neural networks’ adversarial
robustness by improving the estimation of the decision boundaries. The improvement relies on two
components: (1) unbiased estimation of boundary; (2) smaller estimation of variance. Given the
same training data, these methods estimate the boundary closer to the true boundary than the original
neural networks. Our simulations and theoretical results mainly focus on logistic regression. The
idea is then discussed with deep convolutional neural networks.
To gain some intuitions on how the above methods improve the estimation, we start from logistic
regression model with binary classes. Assume a feature vector x follows some distribution Px in Rd ,
w ∈ Rd and b ∈ R, then the corresponding label y follows a Bernoulli distribution with probabilities
given by:
P := P(y = 1) =--------,----TT-,	P(y = 0) =----------.	(1)
1	+ e-(wx+b),	1 + ewx+b
Based on the changing the loss function, we divide these methods into two categories: (1) regulariza-
tion methods: label smoothing (LaS), logit squeezing (LoS), and weight decay (wd); (2) augmentation
methods: mixup and feature smoothing. Regularization methods add penalty term to loss function
directly, while augmentation methods modify the loss function by adding new virtual data into it. We
analyze the properties of these methods based on the two categories in the following subsections and
the proofs of the theorems are included in Appendix.
4.1	Regularization methods
Our main theorem shows that all of the regularization methods estimate the decision boundary
with smaller variance and the estimation is unbiased when x is symmetric with the boundary.
With one-dimensional x and binary classes, the variance of decision boundary can be defined as:
var( W), w = 0. Let P denote the estimated probability. The confidence interval of p, which indicates
the confidence interval of boundary, is narrowed down with the regularization methods, especially
when the support of the distribution of x is far away from boundary (Figure 2(a)). As the value of
w increases, the corresponding variances for w, b and decision boundary are also better controlled
with regularization methods than the vanilla logistic regression (Figure. 3). For the vanilla logistic
regression, when w is large enough, the variance of boundary grows in an exponential rate with
w. But with these regularization methods, variance keeps decreasing even when w is really large.
This observation is also true with higher dimensions and multiple classes (Figure 5). Inspired by
our observation in the simulation study, we prove the following theorems in one dimension (1-D) to
further explain the phenomena in the simulations.
Theorem 4.1.	Label smoothing, logit squeezing, and weight decay all estimate the decision boundary
with smaller variance in logistic regression model in 1-D.
Theorem 4.2.	When x is symmetric with respect to boundary, label smoothing, logit squeezing, and
weight decay have unbiased estimation of boundary in logistic regression model in 1-D.
The symmetric assumption is not unrealistic for imaging classification problems, since we can always
assume the true boundary is in the middle of two classes. However, the stability of those methods
when this assumption cannot hold is also important. We also show that label smoothing and logit
squeezing is relatively more stable than weight decay when x is asymmetric (Figure 7).
6
Under review as a conference paper at ICLR 2019
E
tn
LU
-1.0 -0.5 0.0	0.5	1.0	-1.0 -0.5 0.0	0.5	1.0
(b) Uniform:x∈[-1,1]
Figure 2: Mean estimated probabilities (solid lines) with 99% confidence intervals (dash lines)
obtained from 1000 realizations for regularization methods, with N = 300 data points sampled from
(a) U nif orm([-1, -0.9] ∪ [0.9, 1]), w = 4, b = 0. (b) U nif orm([0, 1]), w = 8, b = 4.
Figure 3: Mean variance of estimated parameters obtained from 1000 realizations with true w
increasing from 1 to 6. x ∈ U nif orm([-1, -0.9] ∪ [0.9, 1]. The legend ‘regularizers’ represents all
regularization methods, since they produce almost identical plots. Here we use δ = λ = 0.1 for label
smoothing and logit squeezing; β = 0.01 for weight decay.
4.2	Augmentation methods
Other than adding regularization to the loss function directly, adversarial training, mixup and feature
smoothing all ‘improve’ the loss function by changing the distribution of x. Figure 4 shows how the
distribution of x influences the estimation of the boundary in different cases. It is natural to see that
when the data are pushed closer to the true boundary, the boundary estimation becomes better due to
reduced variances. Following the same analysis above, x being around boundary leads to smaller
estimated p(1 - p), which yields smaller variance for w, b, and the boundary. When the symmetrical
assumption is violated, more careful selection of original data points is needed to avoid adding too
much noise into training set.
Following the above explanation, our theorem 4.3 shows that adding data around boundary with
labels generated from the true distribution into training can narrow down the variance of boundary,
even though the sample size remains the same. Adversarial training, mixup and feature smoothing
estimated the labels in different ways. We further show that feature smoothing achieve smaller
variance than mixup when α is properly chosen (sec A.4).
Theorem 4.3. Adding data around boundary narrow down the variance of boundary estimation by
making the distribution of x closer to boundary. The estimation is unbiased if all labels for the new
data are balanced/correctly assigned.
E
tn
LU
1.0-Γ
0.8-
0.6-
0.4-
0.2-
0.0-
-1.0 -0.5 0.0	0.5	1.0	-1.0 -0.5 0.0	0.5	1.0
Figure 4: Mean estimated probabilities (solid lines) with 95% confidence intervals (dash lines)
obtained from 1000 realizations for augmentation methods, with N = 300 data points sampled from
(a) U nif orm([-1, -0.9] ∪ [0.9, 1]), w = 4, b = 0. (b) U nif orm([0, 1]), w = 8, b = 4. In both
cases, we use α = 0.3 for feature smoothing and mixup, and σ = 0.3 for generating random noise.
(b) Uniform x: x∈[-l, 1]
-1.0 -0.5
7
Under review as a conference paper at ICLR 2019
4.3	Extension to neural networks
In more complex models like convolution neural network (CNN), the model can be divided into two
parts: hidden layers which transform the input data x → f (x) and the classification model which
applies the softmax function (or sigmoid function for binary classification) on f (x). Our results can
be extended to CNN for regularization methods since softmax function is just a multi-classes logistic
regression. For augmentation methods, we also believe that an interpolation of input data implies an
interpolation of transformed data after hidden layers. For simplicity, we assume the nonlinear layers
in the CNN only consist of ReLU and max-pooling. Obviously, both ReLU and max-pooling satisfy
the following properties: let X = αx(i) + (1 — α)x(j), then
0 ≤ ReLU(X) ≤ αReLU(Xei)) + (1 — α)ReLU(Xj)),
g(αmax-p(x(i)) + (1 — α)max-p(x(j))) ≤ max-p(X) ≤ αmax-p(x(i)) + (1 — α)max-p(x(j)),
(2)
where max-p represents max-pooling. The first inequality in (2) holds when each argument of
x(i), x(j) is non-negative. Given the pooling layer after the ReLU layer, the assumption is valid. It
further implies that
f (X) ≤ αf (x(i)) + (1 — α)f (x(j)),
which means augmentation methods on the data can be considered as augmentation on the logits.
Then we may use our framework on the logistic regression.
5 Discussion
We have proposed feature smoothing, a straightforward data augmentation method as an efficient
way to increase adversarial robustness of neural networks. In our experiments, feature smoothing
combined with logit squeezing shows the best performance in both MNIST and CIFAR10. We found
that α ∈ [0.2, 0.4] shows similar results when we apply PGD attack with total perturbation threshold
as e = 0.3. If we use smaller perturbations, smaller α, for example α = 0.1 for e = 0.1, we also
observe good results. As a future plan, more possibilities of combinations of different techniques can
still be further explored.
We also built a framework to explain how different regularization methods and augmentation methods
improve the estimation of decision boundaries for logistic regression. Our main theorems show that
all of these methods achieve smaller estimation variance of the decision boundary while keeping
the unbiasedness of the estimation. In some extreme cases, for example, correctly labeled data
around boundary for one specific class (7), the vanilla logistic regression is incorrectly estimated the
boundary for sure but all of the above methods resolve the problem. We also extend the analysis
to neural networks based on two facts: (1) the softmax regression is a generalized form of logistic
regression in multi-class classification problem; (2)the activation functions like Relu and max-pooling
can both keep linear inequalities Eq. (2).
Acknowledgement
We would like to thank Dr. Jean-Marc Langlois and Dr. Alyssa Glass for their valuable inputs. We
thank Dr. Harini Hannan for providing detailed explanation of her work, so we can successfully
replicate experiment results from her paper. We also thank Weiqiang Shi for providing engineering
support and Dr. Hua Guo for helpful feedback on drafts of this article.
References
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Proceedings of the International Conference on Learning Representations, ICLR ’15,
2015.
Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018. URL http://arxiv.org/abs/1803.06373.
8
Under review as a conference paper at ICLR 2019
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017. URL https://arxiv.org/abs/1706.06083.
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised
text classification. In Proceedings of the International Conference on Learning Representations,
ICLR ’17, 2017.
Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning:
from phenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016.
URL http://arxiv.org/abs/1605.07277.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on
Asia Conference on Computer and Communications Security, pp. 506-519. ACM, 2017.
Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore: Automated whitebox testing of
deep learning systems. In Proceedings of the 26th Symposium on Operating Systems Principles,
Shanghai, China, October 28-31, 2017, pp. 1-18, 2017. doi: 10.1145/3132747.3132785. URL
http://doi.acm.org/10.1145/3132747.3132785.
Gabriel Pereyra, George Tucker, Jan Chorowski, Eukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing confident output distributions. In Proceedings of the International
Conference on Learning Representations, ICLR ’17, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In Proceedings of the International
Conference on Learning Representations, ICLR ’14, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, CVPR ’16, pp. 2818-2826, 2016.
Thomas Tanay and Lewis D. Griffin. A boundary tilting persepective on the phenomenon of
adversarial examples. CoRR, abs/1608.07690, 2016.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. Mixup: Beyond empirical
risk minimization. In Proceedings of the International Conference on Learning Representations,
ICLR ’18, 2018.
9
Under review as a conference paper at ICLR 2019
A Proofs
The proofs of Theorem 4.1 〜4.3 are derived in this section. We first focus on binary logistic regression
in the proofs.
A.1 Proof Theorem 4.1
Let W and b denote the estimates of W and b. The decreasing of variance is mainly achieved by two
parts: (1) estimated W and b with smaller magnitude; (2) bias-variance trade-off. We first show that
adding regularizers always produce W with smaller magnitude, which lead to smaller variance. Then
we show that the bias of P introduced by penalties also leads to a smaller variance, essentially when P
is closer to 0.5 than the true p.
Based on the Fisher S Information, when estimated parameters are MLE, the variances of W and b are
given by:
Var(W) = (Eχx2 [p(1 - P)])-1,	(3)
,Λ,	, 一 - - ，	.一、 -ι
var(b) = (Eχ[p(1 - P)])	.	(4)
∕^∖
The decision boundary is {x : Wx + b = 0}. So We use var( V) to measure the variance of estimation
of boundary and by delta method
ʌ
var(--T)
W
2
var(b) b2
w2	var(1∕W)
+ o(1).
XT τ∙ . 1	. 1	,'	1 ∙ ,	,' , 1	T	/-∖	1	/-∖ El	♦	i- T, / ʌ ∙ . 1	1 .
Without loss of generality, we further assume b = 0 and w > 0. The variance of b/w is then equal to
1
w2Eχ[p(1 - P)]
τ∕' . <	i' ♦	1 1 .	♦	I ->	¢- . 1	♦	/' T' / ʌ	F i' . Λ	∙..
If the distribution of x is a delta mass, i.e., Px = δχ, the variance of b/w can be further written as
…	1
g(W) = FTl-------C,
w2 P(1 - P)
and the derivative with respect to w is
g0(w)
-2w — w2x(1 — 2P)
w4p(1 — P)
Given our assumption that w > 0, it follows immediately that x(1 - 2P) < 0 and wx(1 - 2P) is
monotonically decreasing. Moreover, as W → ∞, it yields wx(1 - 2P) → -∞. Therefore, there
exists a constant C only depending on x so that for w > C we have -2w - w2x(1 - 2P) > 0. We
proved so far that if the estimation is MLE, the variance of boundary is increasing with w when
w > C.
However, since we add one more regularization term to the original loss function, the estimator is no
longer MLE. An approximation of var(b) is
Ey(y - P)2
P2(1 — P)2 ,
where y 〜Bernoulli(Px). With regularization methods and based on our assumption of w, 0.5 ‹
P < Px for x > 0 andPx < P < 0.5 for x < 0. Therefore
Ey (y - P)2 ≤	1
P2(1 — P)2 - P(1 — P)，
which indicates a smaller variance of boundary than MLE estimation with the same W and b.
10
Under review as a conference paper at ICLR 2019
A.2 Proof of Theorem 4.2
When {x} is symmetric to the boundary x = 0, the data set can then be splitted into two groups,
{xp} containing positive values and {xn} containing negative values, which are symmetric to each
other. We further assume the corresponding labels are also approximately symmetric, which is easy
to achieve when the sample size is large enough. The loss function is then automatically divided
into: Lp = L({xp}) and Ln = L({xn}). The mιmmιzer W and b of Lp and Ln have to be the same
since the input data are symmetric. Then W and b are also the mimmizers of the whole loss function
L = Lp + Ln. That means for any x in positive part and its corresponding image x0 in negative part,
the estimated pj(y∣x) + Pθ(y0∣x0) is equal one, which indicates to an unbiased estimation of the
boundary.
A.3 Proof of Theorem 4.3
As mentioned in section 2, new data points
X = ɑx(i) + (1 — α)x(j),
are added as input in both methods, while the corresponding y are estimated by either linear interpo-
lation or dominant point of x(i) and x(j). Let P denote the true probability of X, we claim that adding
X and y 〜Bernoulli(P) into input decreases the variance of boundary.
Same as above, the variance of boundary can be estimated as
…/ b、	1
var(-W = w2P(1- P)，
where X is generated from a distribution Pχ. When y are generated from the true probability, the
MLE estimation do not change. On the other hand, if X(i) and X(j) are from different classes and
symmetric to the decision boundary, X is closer to the boundary than X(i) and X(j), and therefore
P(I — P) >Pχ(i)(1 — Pχ(i)).
A.4 Remark of Theorem 4.3
As mentioned above, given x from a delta mass distribution, the first derivative of variance with
repect to W is given by:
o —2w — w2x(1 — 2p)
g (W) = -w4p(1 - P)-.
Without loss of generality, we still assume x > 0, b = 0,w > 0, then g0(w) = 0 gives W =方.-2/).
g0(w) < 0 if w < x(i-2p)while g0(w) > 0 otherwise. X close to the boundary (0 when b = 0) also
leads to probability P close to 0.5,i.e. (1 - 2) close to 0. Therefore in a large range of W ∈ (0,方.々/)),
the variance is decreasing with W. As a results, feature smoothing gives even smaller variance than
the original MLE with X.
B S imulation results
B.1	Logistic regression with high-dimensional feature and multiple classes
B.2	In-symmetric feature
However, in real world, we can never have the perfect scenario that {X} is strictly symmetric
distributed with respect to the boundary. We further argue that regularizers based on WX + b
including label smoothing and logit squeezing are more tolerant to unbalanced data than weight
decay regularized on W only. Figure 6 shows how these four methods perform when {X} are not
symmetrically generated, in two scenarios: (a) data size is unbalanced with respect to decision
boundary; (b) data distribution is unbalanced with respect to boundary. It is easy to see that label
smoothing and logit squeezing are less sensitive to the distribution of X in both scenarios. In contrast,
vanilla logistic regression and weight decay are more sensitive. Confidence intervals for vanilla
11
Under review as a conference paper at ICLR 2019
Estimated value and confidence interval of b/w in 3-d multiple classes
Figure 5: The confidence interval of averaged ,boundary, -K PK=I b/wi With X ∈ R2 and K = 3.
-1.0 -0.5 0.0	0.5	1.0	-1.0 -0.5 0.0	0.5	1.0
(b) Unbalanced distribution: x∈[-l, 0.9] u [0.5,0.6]
-1.0 -0.5 0.0	0.5	1.0	-1.0 -0.5 0.0	0.5	1.0
Figure 6: Mean estimated probabilities (solid lines) With 95% confidence intervals (dash lines)
obtained from 1000 realizations in the case of in-symmetric data. Data Were generated from model 1
With w = 4, b = 0. (a) N1 = 200 and N2 = 100 data points Were sampled from [-1, -0.9] and
[0.9, 1] respectively; (b) both N1= 150 and N2 = 150 data points Were sampled from [-1, -0.9]
and [0.6, 0.7] respectively.
logistic regression become Wider and do not behave consistently as x value changes; estimated mean
decision boundary (p = 0.5) for weight decay deviate from the true one, not as robust as other
methods.
B.3	Another realistic case
Now let us consider another data unbalance scenario under a different data generation mechanism and
see how different methods perform. Note that all of our analysis above assumed that the data label y
given data feature x were all generated from the true model 1. In other words, y are random numbers
following Bernoulli (Multinomial for multi classes) distribution. Now let us consider another data
generation mechanism which is also quite common in real world. Given input x, y is deterministic by
an identity function y = I(wx + b) instead of following a distribution.
But some classes have data around boundary and some do not, i.e. the distribution of x is unbalanced,
for example most of x ∈ [-1, -0.9] ∪ [0.9, 1] but some x ∈ [0, 0.1]. Vanilla logistic regression fails
to detect the true boundary in this case but both regularization methods and augmentation methods
can improve the estimation (Figure 7).
12
Under review as a conference paper at ICLR 2019
-1.0 -0.5 0.0	0.5	1.0
-1.0 -0.5 0.0	0.5	1.0
Figure 7: Mean estimated probabilities (solid lines) with 95% confidence intervals (dash lines)
obtained from 1000 realizations in the case of in-symmetric data. x ∈ [-1, -0.9] ∪ [0.9, 1] with size
300. y = 1 J p > 0.5. Then 10 another input x0 = 0.1, y0 = 1 or 10 input of x0 = —0.1, y0 = 0 are
added randomly with probability 0.5 to the training.
13