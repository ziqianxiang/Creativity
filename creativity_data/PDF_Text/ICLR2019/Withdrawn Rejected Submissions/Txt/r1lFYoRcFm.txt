Under review as a conference paper at ICLR 2019
Quantile Regression Reinforcement Learning
with State Aligned Vector Rewards
Anonymous authors
Paper under double-blind review
Ab stract
Learning from a scalar reward in continuous action space environments is diffi-
cult and often requires millions if not billions of interactions. We introduce state
aligned vector rewards, which are easily defined in metric state spaces and allow
our deep reinforcement learning agent to tackle the curse of dimensionality. Our
agent learns to map from action distributions to state change distributions implic-
itly defined in a quantile function neural network. We further introduce a new
reinforcement learning technique inspired by quantile regression which does not
limit agents to explicitly parameterized action distributions. Our results in high
dimensional state spaces show that training with vector rewards allows our agent
to learn multiple times faster than an agent training with scalar rewards.
1	Introduction
Reinforcement learning (Sutton et al., 1998) is a powerful paradigm in which an agent learns about
an environment through interaction. The common formulation consists of a Markov Decision Pro-
cess (MDP) modeled as a 5-tuple (S, A, P, r, γ) where S is the (possibly infinite) set of states, A is
the (possibly infinite) set of actions available to the agent, P : (S × A × S) → [0, 1] : P (s0|s, a)
is the transition probability of reaching state s0 ∈ S given state s ∈ S and action a ∈ A,
r : (S × A) → R : r(s, a) is the reward received for taking action a in state s and γ is the
reward discount factor. The goal of the agent is to maximize the cumulative discounted reward
R = Pt∞=0 γtr(st, at) by choosing actions at according to some (possibly stochastic) policy
∏ : (S X A) → [0,1] : ∏(at∣st). Sometimes it is further useful to make a distinction be-
tween the actual state space S and the correlated observation space O of the agent. In this case
∏ : (O × A) → [0,1] : ∏(at∣θt) with ot ∈ O. The use of deep neural networks allowed this
formulation to scale to high dimensional visual inputs approaching continuity in state space (Mnih
et al., 2015) while others extended deep reinforcement learning to continuous action spaces (Lilli-
crap et al., 2015; Mnih et al., 2016). While neural networks are powerful function approximators,
they require large amounts of training data to converge. In the case of reinforcement learning this
means interactions with the environment, a requirement easy to fulfill in simulation, yet impractical
when the agent should interact with the real world. This problem is aggravated by the weak training
signal of classical reinforcement learning - a simple scalar reward.
While originally the dopamine activity in mammal brains was linked to general “rewarding” events,
Pinto & Lammel (2017) point out that the diversity of dopamine circuits in the mid brain is better
modeled by viral vector strategies. Gershman et al. (2009) also show that human reinforcement
learning incorporates effector specific value estimations to cope with the high dimensional action
space. Inspired by these biological insights, we improve the sample efficiency of a deep reinforce-
ment learning algorithm in this work by modeling a d-dimensional vector reward. A vector reward
can in some domains easily be defined in alignment with the state space. We say that two vec-
tor spaces are aligned if their dimensions correlate and show that if state and action space are not
aligned, a mapping from action distribution to state change distribution can be learned.
As a motivating example, consider the agent in Figure 1(a) trying to reach the goal (marked by the
blue dot). If we take p as the position vector of the agent relative to the goal, a sensible reward to
guide the agent to the goal in this environment would be r = ∣∣p∣∣-∣∣p + a|| where ∣∣∙∣∣ can be any
norm in the vector space of the environment. For illustration purposes we’ll focus on the L1 norm in
this example and throughout this paper. During training the agent might try action a which moves
1
Under review as a conference paper at ICLR 2019
Figure 1: (a) An agent freely moving in a 2D world might try to reach a goal at position (0, 0) by
taking action a. A sensible reward in this environment is the change in absolute distance to the goal.
With a scalar reward this would be summarized as r = rx - ry, whereas a vector reward would keep
the two reward dimensions distinguishable. (b) In most cases action and state space are however not
aligned, therefore a mapping from action to state change must be learned.
it closer to the goal in x direction, but a bit further away from the goal in y direction. The scalar
reward would then just convey the information, that the action was rather positive (since the agent
got closer to the goal) but miss out on the distinction that the action was good in x-direction but bad
in y-direction. To provide this distinction, a more informative reward would keep the dimensions
separate and therefore be a vector itself: r = |p| - |p + a| where | ∙ | denotes the element-wise
absolute value here. Note that this reward is dimension wise aligned with the position p, the state,
of the agent. Since we focus on reaching problems in this work, we’ll use the terms “position” and
“state” interchangeably.
The problem with such a state aligned vector reward is however that the action space is in most
cases not state aligned. To see this, consider the schematic robot arm in Figure 1(b): The action
dimensions a1 and a2 correspond to the torques of the robot arm and do not directly translate to
a shift in x and y dimension, respectively. To address this issue we use the method proposed by
Dabney et al. (2018b;a) to train a deep neural network to approximate the quantile function, in our
case of the position change, given the current observation and quantile target. Additionally we give
a parameterization of the action probability distribution as input to this position change prediction
network (short PCPN). We then train the agent, parameterized by another neural network which
maps from observations to action probability distributions, through a new reinforcement learning
method we call quantile regression reinforcement learning (short QRRL). A schematic overview of
our setup can be seen in Figure 2.
To summarize, the contributions of this paper are the following:
•	We extend the reinforcement learning paradigm to allow for faster training based on more
informative state aligned vector rewards.
•	We present an architecture that learns a probability distribution over possible state changes
based on a probability distribution over possible actions.
•	We introduce a new reinforcement learning algorithm to train stochastic continuous action
policies with arbitrary action probability distributions.
2	Quantile Regression and Implicit Quantile Networks
Quantile regression (Koenker, 2005) discusses approximation techniques for the inverse cumulative
distribution function FY-1, i.e., the quantile function, of some probability distribution Y . Recent
work (Dabney et al., 2018a; Ostrovski et al., 2018) shows that a neural network can learn to ap-
proximate the quantile function by mapping a uniformly sampled quantile target T 〜U([0,1]) to its
corresponding quantile function value FY-1(τ) ∈ R. Thereby the trained neural network implicitly
models the full probability distribution Y .
2
Under review as a conference paper at ICLR 2019
Observation o φ
Observation o
Quantile target 工力
PCPN
Agent
Action
probability Φ
distribution
Position
change
prediction for
quantile 工
Training with
state aligned
vector
reward r
Sampled action a
Figure 2: An overview of the architecture setup. The agent and the position change prediction
network (PCPN) are instantiated with neural networks and trained through quantile regression rein-
forcement learning and quantile regression, respectively.
More formally, let Wp(U, Y ) be the p-Wasserstein metric Wp(U, Y )	=
(R1 IF?1 (ω) - F-1(ω)∣pdω) '' of distributions U and Y, also characterized as the Lp
metric of quantile functions (Muller, 1997). Dabney et al.(2018b) show that the quantile regression
loss (Koenker & Hallock, 2001)
Pτ(δ) = (T - 1δ<o) ∙ δ
(1)
minimizes the 1-Wasserstein distance of a scalar probability distribution Y to a uniform mix-
ture of Diracs U. Here, δ = y - U with y 〜 Y and U 〜 U is the quantile sample er-
ror. Ostrovski et al. (2018) generalized this result by showing that the expected quantile loss
ET〜U([0,1]) [Ez〜Z [ρτ(z - Qθ(T川
of a parameterized quantile function Q θ aproximating the
quantile function FZ-1 of some distribution Z is equal to the quantile divergence
q(Z,Qθ):
FQ-θ1(τ)
01	FZ-1(τ)
(FZ (x) - T)dx
dT
1
0
plus some constant not depending on the parameters θ. Here, Qθ is the distribution implicitly defined
by Qθ. Therefore, training a neural network Qθ(T) to minimize ρτ(z - Qθ(T)) with z sampled from
the target probability distribution Z effectively minimizes the quantile divergence q(Z, Qθ) and
thereby models an approximate distribution Qθ of Z implicitly in the network parameters θ of the
neural network Qθ (T).
By approximating the quantile function instead of a parameterized probability distribution, as com-
mon in many deep learning models (Kingma & Welling, 2013; Lillicrap et al., 2015; Ha & Schmid-
huber, 2018), we do not enforce any constraint on the probability distribution Z, e.g., Z can be
multi-modal, not continuous and non-Gaussian. This is crucial for our case as a position change
distribution given a certain action distribution can have any shape.
In our setup, we train the PCPN with the quantile regression loss (1) to approximate a position
change quantile function per position dimension. Aside from a target quantile T 〜U([0,1]) per po-
sition dimension, the network input consists of an observation o ∈ O and a multi-variate Gaussian
action probability distribution A = N(μ, Σ) with diagonal covariance matrix Σ = Iσ, parameter-
ized by the mean μ and variance σ vectors. Therefore, the PCPN has the ability to implicitly learn
the conditional position change distribution given an observation and an action distribution. 3
3 Quantile Regression Reinforcement Learning
The dominant reinforcement learning algorithms are either value based methods, e.g., Q-learning
(Watkins, 1989; Mnih et al., 2015), policy gradient based methods, e.g., REINFORCE (Williams,
1992; Sutton et al., 1999), or a combination of both, e.g., actor-critic methods (Sutton et al., 1998;
Mnih et al., 2016). In this section we establish a new reinforcement learning objective based on
3
Under review as a conference paper at ICLR 2019
quantile regression. In contrast to Q-learning we allow for a continuous action space similar to
policy based algorithms, but in contrast to policy based algorithms we do not limit ourselves to
explicitly parameterized policies. Note that the technique described in this section is generally
applicable to reinforcement learning problems and we therefore use the terms “action” and “policy”
in the common reinforcement learning meaning, which is distinct from the meaning of “action” and
“policy” in the rest of the paper. We later link in Section 4 the position change estimation of the
PCPN to this new meaning of “action” to connect with the rest of the paper.
The main idea behind Quantile Regression Reinforcement Learning (or QRRL in short) is to model
the policy implicitly in the network parameters, therefore allowing for complex stochastic policies
which can find multi-modal stochastic optima in policy space. For this we model for each action
1
dimension the quantile function Qθ(τ, o) = FQ-1(τ, o) of the implicitly defined action distribution
Qθ (o) by a neural network with an observation o and a target quantile τ ∈ [0, 1] as input. From
the full network Qξ : [0,1]d X O → Rd, with d being the number of action dimensions, an action
a ∈ A ≡ Rd for a given observation o can be sampled by sampling T 〜 U([0,1]d) and taking the
network output as action. Since the network approximates quantile functions, the network output
of a uniformly at random sampled quantile target is a sample from the implicitly defined action
distribution. The question left to address is how to train the network, such that it (a) approximates
the quantile functions of the action dimensions and (b) the implicitly defined policy maximizes the
expected (discounted) reward R.
Here quantile regression comes in handy. Informally put, quantile regression is linked to the Wasser-
stein metric which is also sometimes refered to as earth movers distance. Imagine a pile of earth
representing probability mass. In reinforcement learning we essentially want to move probability
mass towards actions that were good and away from actions that were bad, where “good” and “bad”
are measured by discounted accumulative (bootstrapped) reward achieved. Quantile regression can
achieve this neatly by shaping the pile of earth according to an advantage estimation and the con-
straint of monotonicity (which is a core property of quantile functions).
More formally, we are interested in the effect of the quantile regression loss (1) on action probabili-
ties when the error δ is between two samples from the same network, one representing the action that
was played in the environment (which resulted from some sample quantile τ) and one representing
the quantile function value at some τ0 where the network tries to approximate the quantile function
of the optimal action distribution. We focus in this analysis on the simple case of a single action
dimension with scalar quantile target input and scalar quantile function output. The generalization to
multidimensional action-/quantile-functions with independent action dimensions follows trivially.
First, let us denote the action taken in the environment by aτ := Qθ(τ, o) = Qθ(τ), where we
hide the dependence of Qθ (τ, o) on the observation o hereafter for notation simplicity. To train the
quantile network we sample different τ0 and consider the loss
pτ 0 (aτ - Q θ (T O)) = (T 0 - 1aτ-Qθ(τ 0 )<0) ∙ (T- Q θ (T O)).
As we are interested in the effect of the loss on the probability of aτ consider a τ0 close to τ, i.e.,
T0 = T ± for some > 0 and < T < 1 - . For T0 = T - the loss reduces to
^	^ (	∖∖	_(	^ ^	^ (	∖∖	_ a a	aτ	- Qθ(T	- E) e→0∖ / δQθ(T)
Pτ0(aτ - qΘ (T	- E))	= (T -	E)	∙ (aτ	- qΘ (T	- E))	= C(T -	E) --------------→ dT1-7----
E	δT
where dT1 is an infinitely small but positive value Similarly we get for T0 = T + E
∕^Jτ I ,-ʌ	C	X∕^Jτ∖
Pτ0(aτ-Qθ(t+e)) = (t+e-1)∙(aτ-Qθ(t+e)) = e(1-t-e)--------------------τ --→ dr2--——
E	δT
with dT2 an infinitely small but positive value. Therefore, the quantile regression loss is positively
correlated to the slope of Qθ at T. The partial derivative of a quantile function Qθ (T) with respect
to the quantile T is however also known as the sparsity function (Tukey, 1965) or quantile-density
function (Parzen, 1979) and has the interesting property (Jones, 1992):
δ 右 ,、	1	1
—Qθ (τ) = -----τ------- = ---7 7
δτ	PQθ (Qθ(τ))	PQθ (aτ)
where pQθ is the probability density function of distribution Qθ. Hence, the quantile regression loss
is inverse proportional to the probability of action a「, which implies the following, given that Qθ is
the quantile function of distribution Qθ :
4
Under review as a conference paper at ICLR 2019
1.	Minimizing the quantile loss (1) for a given action aτ increases the likelihood of action aτ .
2.	Maximizing the quantile loss for a given action aτ decreases the likelihood of action aτ .
This leads us to the QRRL actor objective
minEa [E「0 [ρτ，(。「 一 Qθ(T0, ot)) ∙ (Rtn - Vψ (ot)]] s.t. Qθ is monotonically increasing with T0
θτ
where (Rt,n - Vψ(ot)) is an advantage estimation (Mnih et al., 2016) with Rt,n = γnVψ(ot+n) +
Pt+=nt Yt TrtO being the n-step estimate of the discounted future reward and Vψ (∙) being the output
of a critic network with parameters ψ, which is trained to approximate Rt,n . Note that QRRL is a
constraint optimization (since Qθ must be a proper quantile function, i.e., monotonically increasing).
In our experiments we found it however sufficient to add this constraint as an additional loss term
Lmon(τ,τ 0, θ) = LHuber(Q θ (T ) - Qθ (T O)) ∙ 1τ<τ 0 & Qθ (T )>Qθ (T 0)orτ>τ0 & Qθ (T )<Qθ (T0)
.L rκ / c	δ δ2	∣δ∣ ≤ K
with LκHuber (δ ) =	1
[∣δ∣- 1 κ otherwise
We weight this additional loss term with a constant Lagrange multiplier λmon in the full loss term
LQRRL (aT , ot, T, T , θ, ψ) = La (aT, ot , T , θ) + λcLc(ot, ψ) + λmonLmon (T, T , θ)	(2)
With La (aτ, ot,τ ,θ) = PT0 (aτ - Qθ (T , O)) ∙ (Rt,n - Vψ (Ot)) and Lc(ot, ψ) = (Rt,n - Vψ (Ot)).
λc is another constant Lagrange multiplier to weight the critic loss Lc against the actor loss La .
Although we focus in our experiments on an on-policy method with multiple actors, QRRL can
easily be adapted to the off-policy setting.
4	State Aligned Vector Reward Agent
In our State Aligned VEctor Reward (SAVER) agent, we use QRRL to train the agent network
ANn through the PCPN. For this We feed the action probability distribution output μ, σ of the
agent network to a pretrained PCPN and train on the QRRL loss (2) with respect to the agent net-
work parameters η, where we take the actual position change ∆p introduced by a sampled action
a 〜N(μ, Iσ) as QRRL action target a「= ∆p and compare it to K sampled position change
estimations ∆p(τ(i)), τ(i) ~ U([0,1]d) for i ∈ {1,…，K}. Note that We pretrain the PCPN in our
setup with random observations and action probability distributions and freeze the PCPN weights
during agent training. Therefore, one could potentially train several agents to solve different tasks
in the same environment using the same PCPN.
Since the output of the PCPN can be seen as state aligned action, training on vector rewards is
straight forward. Instead of having a scalar critic estimating the value function V(∙) ∈ R we estimate
a value per action dimension, i.e., V(∙) ∈ Rd. Similarly, the vector rewards can be summed to a
vector n-step discounted reward estimation Rt,n = γnV(Ot+n) + Ptt0+=nt γt0-trt0, which leads us
to a vector advantage (Rt,n - V (Ot)) at timestep t. Therefore we have an advantage estimation
for each position change dimension individually and can apply loss (2) to each position change
estimation dimension individually. 5
5 Experiments and Results
To test our ideas, we implement three simple experiments, two to test our approach in high dimen-
sional metric spaces and one to show the applicability of our approach to a real world problem by
modeling the robot arm shown in Figure 1(b). For reproducibility and to stimulate further research
in this area, our code is publicly available.1 We compare our SAVER agent against an A2C agent,
the synchronous variant of A3C (Mnih et al., 2016). We choose this baseline since it is most similar
to our SAVER implementation and we believe that SAVER can benefit in the future from state-of-
the-art additions to A3C as presented by Espeholt et al. (2018). Here however, we want to focus on
the benefit of using vector rewards instead of scalar rewards. We implement a simple feed forward
5
Under review as a conference paper at ICLR 2019
1,000
500
0
012
——A2C
——SAVER
Figure 3: Average number of steps needed to reach the goal over the course of training of an agent
that chooses pairwise angle and step size for pairwise grouped dimensions. Results are shown for
a 2, 4, 8 and 16 dimensional environment, corresponding plots are ordered from left to right. The
x-axis shows the number of training steps in millions while the y-axis shows the average episode
length over the last 100 episodes. Training length was fixed to 2,560,000 steps. Plotted is the
average of 3 training runs with the shaded area indicating the standard deviation between runs. The
plots suggest that the higher dimensional the environment, the more apparent the gain of training on
vector rewards is.
neural architecture architecture and fix most hyperparameters to values that work well for SAVER
and A2C. Details can be found in Appendix A.
In a first experiment, we let the agent move freely in all directions within a d-dimensional hypercube
by choosing the action space to represent a set of (angle, step size) pairs for moving in the pairwise
grouped environment dimensions. Note that this setup is similar to the depiction in Figure 1(a), but
the action dimensions are not aligned with the state-/position-change. The initial position of the
agent is sampled uniformly at random fromU([-1, 1]d) and the goal of the agent is to reach the ori-
gin of the d-dimensional space. The agent’s observation is its position and movement of the agent is
stopped at the boundaries of the hypercube defined by [-1, 1]d. Step sizes are re-scaled and clipped
to a maximal value of 0.1 and episodes are terminated after 1,000 steps if the agent didn’t manage
to get into close proximity of the goal beforehand. We pretrain the PCPN with 100,000 batches of
128 transitions each by randomly sampling o 〜 U([-1,1]d), μ 〜U([-1,1]d) and σ 〜U([0,1]d)
where actions between -1 and 1 correspond to step sizes between -0.1 and 0.1 and angles between
0 and π, respectively.1 2 We performed a small hyperparameter search for the learning rate in the
8-dimensional hypercube and settled for a learning rate of 0.01 (chosen from {0.01, 0.003, 0.001})
for SAVER and a learning rate of 0.0003 (chosen from {0.001, 0.0003, 0.0001}) for A2C.3 We mea-
sure the performance of the agents by the mean length of the last 100 episodes. This mean episode
length is plotted in Figure 3 over the course of training for hypercubes of different dimensionality.
Average and standard deviation of three training runs is shown. As can be seen from the plots, the
higher the dimensionality d of the environment, the more apparent the advantage of training on vec-
tor rewards gets. SAVER trains faster in high dimensional cubes and is even able to find the goal in
a 16-dimensional cube given the step limit of 1,000 steps.
As a second experiment, we keep the environment specifications the same but change the action
representation. Here the agent’s action consists of two parts: a softmax distribution from which the
dimension to be manipulated is chosen (discrete action part) and a scalar Gaussian distribution de-
fined by network outputs μ and σ from which the step size is sampled (continuous action part). Note
that this action composition leads to a not continuous position change distribution when regarding
the position change dimensions isolated, since in each dimension the probability of a change equal
to 0 is more likely then position changes close to 0. We show with this experiment, that the quan-
tile regression based PCPN can learn to implicitly model this complex position change distribution
based on the composed action distribution input. For this we pretrain the PCPN with 10,000 batches
of 128 transitions each by sampling softmax logits and μ uniformly at random from U([-1,1]) and
1Our code can be found at: https://goo.gl/rMuadg (Anonymous Google Drive for double blind review)
2Note that even though SAVER requires pretraining, pretraining is task independent and other tasks could be
learned by reusing a trained PCPN. Therefore we do not include the pretraining in our plots. In our experiments
however, we retrain the PCPN for each training run to show algorithmic stability.
3We found in informal early experiments, that A2C requires a much smaller initial learning rate than the
QRRL based SAVER for stable learning.
6
Under review as a conference paper at ICLR 2019
Figure 4: Average number of steps needed to reach the goal over the course of training of an agent
that can select the dimension it wants to move in and the amount by how much it wants to move.
Results are shown for a 3, 4, 5 and 6 dimensional environment, corresponding plots are ordered
from left to right. The x-axis shows the number of training steps in millions while the y-axis shows
the average episode length over the last 100 episodes. Training length was fixed to 12,80,000 steps.
Plotted is the average of 3 training runs with the shaded area indicating the standard deviation be-
tween runs. Again we see a benefit of training on vector rewards in high dimensional environments.
Figure 5: Average number of steps needed to reach the goal over the course of training of an agent
that controls a robot arm as depicted in Figure 1(b). The x-axis shows the number of training steps
in millions while the y-axis shows the average episode length over the last 100 episodes. Plotted is
the average of 5 training runs with the shaded area indicating the standard deviation between runs.
σ fromU([0, 1]). In this experiment we searched for an appropriat learning rate in the 4 dimensional
hypercube and settled for 0.01 for SAVER (chosen from {0.01, 0.003, 0.001}) and 0.0001 for A2C
(chosen from {0.001, 0.0003, 0.0001}). The corresponding mean episode lengths over the course of
agent training are plotted in Figure 4. Again we find that the higher dimensional the hypercube is,
the more advantageous it is to train with vector rewards.
In our last experiment, we model the 2-joint robot arm depicted in Figure 1(b). Here an action
sampled from the 2-dimensional Gaussian a 〜N(μ, Iσ) is translated into an angle change between
-0.02 ∙ ∏ and 0.02 ∙ ∏ of the joints. The agent,s observation o consist here additionally to the
hand position also of a goal position and the current angle of the joints. At the beginning of each
episode, the initial joint angles and the goal position are randomly sampled from U([-π, π]2) and
U ([-1, 1]2), respectively. We choose the learning rate of SAVER as 0.003 and A2C as 0.0003
based on a search over {0.1, 0.01, 0.003, 0.001} and {0.01, 0.001, 0.0003, 0.0001}, respectively.
We pretrain the PCPN with 10,000 batches of 128 transitions each by sampling μ 〜 U([-1, 1]d)
and σ ~ U([0,1]d). The corresponding mean episode lengths over the course of training are plotted
in Figure 5. Even in this low dimensional problem, SAVER trains slightly faster than A2C due to
the more informative vector rewards. 6
6 Related Work
Using a vector of rewards instead of a scalar reward is most common in the literature on Multi-
Objective Reinforcement Learning (MORL) (Liu et al., 2015) and Multi-Objective Sequential Deci-
sion Making (Roijers et al., 2013; Roijers & Whiteson, 2017). However, since most of this literature
focuses on classical reinforcement learning and conflicting objectives, the methods discussed either
7
Under review as a conference paper at ICLR 2019
reduce the multiple objectives to a single objective or learn different policies for each objective ad-
ditionally to a superpolicy deciding on which policy to use when. Recent work (Mossalam et al.,
2016; Tajmajer, 2017; Nguyen, 2018) also applied these techniques to deep reinforcement learning
agents. In contrast, we directly use the vector reward as more informative training signal for a neural
network which implicitly learns to trade off different objectives.
Other adjacent areas are multi-agent and multi-task deep reinforcement learning (Arulkumaran et al.,
2017). While former considers multiple agents with potentially different observations, later ad-
dresses how a single agent best solves multiple tasks. In contrast, our work considers a single agent
solving a single task by leveraging a multi-dimensional reward.
Klinkhammer (2018) discusses multiple problem aligned rewards for better learning, while Brys
et al. (2014) discuss the effect of correlated rewards on learning performance. Brys et al. (2014) also
suggest multiple reward shapings of the same reward function for faster learning. Van Seijen et al.
(2017) decompose the reward function into multiple rewards and train an architecture similar to ours
with deep Q learning, assigning a Q-value output to each reward. While all three approaches come to
the same conclusion as we do, i.e., increased training performance, they do require hand engineered
reward functions, reward shapings and/or reward decompositions. In contrast, our approach is based
on the fact that many state spaces are metric spaces and therefore allow for a straight forward vector
reward interpretation. This makes our approach easier to apply to a larger set of tasks. While state
proximity was already used by McCallum (1992) for faster backpropagation of rewards in tabular
reinforcement learning, we are unaware of any deep learning algorithm capitalizing on state aligned
vector rewards as we do.
Many recent approaches to deep reinforcement learning learn the environment dynamics to have a
richer training signal (Dosovitskiy & Koltun, 2016), imagine action consequences (Racaniere et al.,
2017), improve exploration (Pathak et al., 2017) or dream up solutions (Ha & Schmidhuber, 2018).
An interesting line of research (Zhang et al., 2017; Barreto et al., 2017; Ma et al., 2018; Barreto
et al., 2018) in this direction uses successor features to share knowledge between tasks in the same
environment. In contrast to most of these works, which often only predict one possible next state or
successor feature, our PCPN incorporates the full probability distribution of possible state changes.
While Ha & Schmidhuber (2018) also predict the full probability distribution of their next state
representation by a Gaussian mixture model, our approach is more general in that it is also able to
approximate non-Gaussian probability distributions.
Dabney et al. (2018b) were the first to use quantile regression in connection with deep reinforcement
learning. In their work, including their followup work (Dabney et al., 2018a), they focused on
approximating the full probability distribution of the value function. In contrast, with QRRL we
explore possibilities of using quantile regression to approximate richer policies by not constraining
the action distribution to an explicitly parameterized distribution. Ostrovski et al. (2018) showed that
quantile networks can also be used for generative modeling. In general, we see quantile regression
in combination with deep learning to have a lot of potential for future work.
7 Conclusion
In this work we present the idea of state aligned vector rewards for faster reinforcement learning.
While the idea is straight forward and simple, we are unaware of any work that addresses it so far.
Additionally, we also present a new reinforcement learning technique based on quantile regression in
this work which we term QRRL. QRRL allows for complex stochastic policies in continuous action
spaces, not limiting agents to Gaussian actions. Combining both, we show that the agent network in
our SAVER agent can be trained through a quantile network pretrained in the environment. We show
that SAVER is capable of training orders of magnitudes faster in high dimensional metric spaces.
While d-dimensional metric spaces are mainly mathematical constructs for d > 3, we see a lot of
potential in SAVER to be applied to problems in mathematics and related fields, including the field
of deep (reinforcement) learning itself.
Acknowledgments
Paper under double-blind review.
8
Under review as a conference paper at ICLR 2019
References
Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief
survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866, 2017.
Andre Barreto, Will Dabney, Remi Munos, Jonathan J. Hunt, Tom SchaUL David Silver, and
Hado P. van Hasselt. Successor features for transfer in reinforcement learning. In Ad-
vances in Neural Information Processing Systems 30: Annual Conference on Neural In-
formation Processing Systems 2017, 2017. URL http://papers.nips.cc/paper/
6994-successor-features-for-transfer-in-reinforcement-learning.
Andre Barreto, Diana Borsa, John Quan, Tom SchaUL David Silver, Matteo Hessel, Daniel J.
Mankowitz, Augustin z´dek, and Remi Munos. Transfer in deep reinforcement learning using
successor features and generalised policy improvement. In Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, 2018. URL http://proceedings.mlr.
press/v80/barreto18a.html.
Tim Brys, Ann Nowe, Daniel Kudenko, and Matthew E. Taylor. Combining multiple correlated
reward and shaping signals by measuring confidence. In Proceedings of the Twenty-Eighth AAAI
Conference OnArtificialIntelligence, July27 -31, 2014, Quebec City, Quebec, Canada.,pp.1687-
1693, 2014. URL http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/
view/8390.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. In Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, pp.
1104-1113, 2018a. URL http://Proceedings .mlr.press∕v80∕dabney18a.html.
Will Dabney, Mark Rowland, Marc G. Bellemare, and Remi Munos. Distributional reinforce-
ment learning with quantile regression. In Proceedings of the Thirty-Second AAAI Conference
on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018, 2018b. URL
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17184.
Alexey Dosovitskiy and Vladlen Koltun. Learning to act by predicting the future. arXiv preprint
arXiv:1611.01779, 2016.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IM-
PALA: scalable distributed deep-rl with importance weighted actor-learner architectures. In
Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stock-
holmsmassan, Stockholm, Sweden, July 10-15, 2018, pp. 1406-1415, 2018. URL http:
//proceedings.mlr.press/v80/espeholt18a.html.
Samuel J. Gershman, Bijan Pesaran, and Nathaniel D. Daw. Human reinforcement learning sub-
divides structured action spaces by learning effector-specific values. Journal of Neuroscience,
29(43):13524-13531, 2009. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.2469-09.2009. URL
http://www.jneurosci.org/content/29/43/13524.
D. Ha and J. Schmidhuber. World models. 2018. doi: 10.5281/zenodo.1207631. URL https:
//worldmodels.github.io.
M. C. Jones. Estimating densities, quantiles, quantile densities and density quantiles. Annals of the
Institute of Statistical Mathematics, 44(4):721-727, Dec 1992. ISSN 1572-9052. doi: 10.1007/
BF00053400. URL https://doi.org/10.1007/BF00053400.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Eric R Klinkhammer. Learning in complex domains: Leveraging multiple rewards through align-
ment. 2018.
Roger Koenker. Quantile Regression. Econometric Society Monographs. Cambridge University
Press, 2005. doi: 10.1017/CBO9780511754098.
9
Under review as a conference paper at ICLR 2019
Roger Koenker and Kevin Hallock. Quantile regression: An introduction. Journal of Economic
Perspectives,15(4):43-56, 2001.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
C. Liu, X. Xu, and D. Hu. Multiobjective reinforcement learning: A comprehensive overview.
IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45(3):385-398, March 2015.
ISSN 2168-2216. doi: 10.1109/TSMC.2014.2358639.
Chen Ma, Junfeng Wen, and Yoshua Bengio. Universal successor representations for transfer rein-
forcement learning. CoRR, abs/1804.03758, 2018. URL http://arxiv.org/abs/1804.
03758.
R. Andrew McCallum. Using transitional proximity for faster reinforcement learning. In Derek
Sleeman and Peter Edwards (eds.), Machine Learning Proceedings 1992, pp. 316 - 321.
Morgan Kaufmann, San Francisco (CA), 1992. ISBN 978-1-55860-247-2. doi: https://doi.
org/10.1016/B978-1-55860-247-2.50045-0. URL http://www.sciencedirect.com/
science/article/pii/B9781558602472500450.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529-533, 2015. doi: 10.1038/nature14236. URL
https://doi.org/10.1038/nature14236.
Volodymyr Mnih, Adria PUigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016,
New York City, NY, USA, June 19-24, 2016, pp. 1928-1937, 2016. URL http://jmlr.org/
proceedings/papers/v48/mniha16.html.
Hossam Mossalam, Yannis M Assael, Diederik M Roijers, and Shimon Whiteson. MUlti-objective
deep reinforcement learning. arXiv preprint arXiv:1610.02707, 2016.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2):429-443, 1997.
Thanh Thi Nguyen. A multi-objective deep reinforcement learning framework. arXiv preprint
arXiv:1803.02965, 2018.
Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative
modeling. In Proceedings of the 35th International Conference on Machine Learning, ICML
2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, pp. 3933-3942, 2018. URL
http://proceedings.mlr.press/v80/ostrovski18a.html.
Emanuel Parzen. Nonparametric statistical data modeling. Journal of the American statistical asso-
ciation, 74(365):105-121, 1979.
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 2778-2787, 2017. URL
http://proceedings.mlr.press/v70/pathak17a.html.
Daniel F. Cardozo Pinto and Stephan Lammel. Viral vector strategies for investigating midbrain
dopamine circuits underlying motivated behaviors. Pharmacology Biochemistry and Behavior,
2017. ISSN 0091-3057. doi: https://doi.org/10.1016/j.pbb.2017.02.006. URL http://www.
sciencedirect.com/science/article/pii/S0091305716303185.
10
Under review as a conference paper at ICLR 2019
Sebastien Racaniere, TheoPhane Weber, David P. Reichert, Lars Buesing, Arthur
Guez, Danilo Jimenez Rezende, Adria PUigdomenech Badia, Oriol Vinyals, Nico-
las Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Sil-
ver, and Daan Wierstra. Imagination-augmented agents for deeP reinforcement learn-
ing. In Advances in Neural Information Processing Systems 30: Annual Confer-
ence on Neural Information Processing Systems 2017, 4-9 December 2017, Long
Beach, CA, USA, pp. 5694-5705, 2017. URL http://papers.nips.cc/paper/
7152-imagination-augmented-agents-for-deep-reinforcement-learning.
Diederik M. Roijers and Shimon Whiteson. Multi-Objective Decision Making.	Synthesis
Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,
2017. doi: 10.2200/S00765ED1V01Y201704AIM034. URL https://doi.org/10.2200/
S00765ED1V01Y201704AIM034.
Diederik M. Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multi-
objective sequential decision-making. J. Artif. Intell. Res., 48:67-113, 2013. doi: 10.1613/jair.
3987. URL https://doi.org/10.1613/jair.3987.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press,
1998.
Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural In-
formation Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 -
December 4, 1999], pp. 1057-1063, 1999.
Tomasz Tajmajer. Multi-objective deep q-learning with subsumption architecture. arXiv preprint
arXiv:1704.06676, 2017.
John W Tukey. Which part of the sample contains the information? Proceedings of the National
Academy of Sciences, 53(1):127-134, 1965.
Harm Van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes,
and Jeffrey Tsang. Hybrid reward architecture for reinforcement learning. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5392-
5402. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7123- hybrid- reward- architecture- for- reinforcement- learning.pdf.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, King’s
College, Cambridge, 1989.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8:229-256, 1992. doi: 10.1007/BF00992696. URL https://
doi.org/10.1007/BF00992696.
Jingwei Zhang, Jost Tobias Springenberg, Joschka Boedecker, and Wolfram Burgard. Deep rein-
forcement learning with successor features for navigation across similar environments. In 2017
IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2017, 2017. doi: 10.
1109/IROS.2017.8206049. URL https://doi.org/10.1109/IROS.2017.8206049.
11
Under review as a conference paper at ICLR 2019
Table 1: List of hyperparameters used in all experiments
Hyperparameter
Number of actors
Rollout length n
A2C entropy β
Critic λc
Pretrain learning rate
Monotonic λmon
Discount factor γ
Quantile samples K
Optimizer
Non-linearities
Value
16
8
0.0
0.5
0.1 ∙ Learning rate
1.0
0.99
32
RMSProp
ReLu
A	Architecture details
As agent network, we used a simple layer-wise fully connected network with 3 hidden representa-
tions of size 256, 128 and 64, respectively. From the last hidden representation we map with a linear
layer to the action mean μ, while the action standard deviation σ is initialized to e-1 ∙ [1,…，1]T
and kept independent of the observation input. For the A2C implementation, the last hidden rep-
resentation of the agent network is also mapped through a linear layer to the scalar value estimate
V (o). For the PCPN We first map from μ, σ and o toa hidden representation of size 256. From this
hidden representation we map to d hidden representations, each of size 128, where d is the number
of state dimensions. We then multiply each of these d hidden representation with a 128-dimensional
cosine embedding of corresponding quantile target τj with j ∈ {1, ..., d}. Each of this new hidden
representations is then fed through a fully connected layer with 64 neurons before being linearly
projected to a scalar value representing the position change estimation of dimension j for quantile
τj. We implement the QRRL critic as a separate network with 3 hidden representations of size 256,
128 and 64 and the same input as fed to the PCPN’s first layer. Besides the action distribution repre-
sentation which is ajusted corresponding to the experiment, we keep this architecture and all of the
hyperparameters (as listed in Table 1) fixed for all experiments. The only hyperparameter we adjust
is the learning rate, as described in the main text.
12