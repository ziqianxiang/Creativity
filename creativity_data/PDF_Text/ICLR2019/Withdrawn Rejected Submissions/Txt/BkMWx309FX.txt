Under review as a conference paper at ICLR 2019
Reinforcement Learning with
Perturbed Rewards
Anonymous authors
Paper under double-blind review
Ab stract
Recent studies have shown that reinforcement learning (RL) models can be vulner-
able in various scenarios, where noises from different sources could appear. For
instance, the observed reward channel is often subject to noise in practice (e.g.,
when observed rewards are collected through sensors), and thus observed rewards
may not be credible. Also, in applications such as robotics, a deep reinforce-
ment learning (DRL) algorithm can be manipulated to produce arbitrary errors. In
this paper, we consider noisy RL problems where observed rewards by RL agents
are generated with a reward confusion matrix. We call such observed rewards as
perturbed rewards. We develop an unbiased reward estimator aided robust RL
framework that enables RL agents to learn in noisy environments while observing
only perturbed rewards. Our framework draws upon approaches for supervised
learning with noisy data. The core ideas of our solution include estimating a re-
ward confusion matrix and defining a set of unbiased surrogate rewards. We prove
the convergence and sample complexity of our approach. Extensive experiments
on different DRL platforms show that policies based on our estimated surrogate
reward can achieve higher expected rewards, and converge faster than existing
baselines. For instance, the state-of-the-art PPO algorithm is able to obtain 67.5%
and 46.7% improvements in average on five Atari games, when the error rates are
10% and 30% respectively.
1	Introduction
Designing a suitable reward function plays a critical role in building reinforcement learning mod-
els for real-world applications. Ideally, one would want to customize reward functions to achieve
application-specific goals (Hadfield-Menell et al., 2017). In practice, however, it is difficult to de-
sign a function that produces credible rewards in the presence of noise. This is because the output
from any reward function is subject to multiple kinds of randomness:
•	Inherent Noise. For instance, sensors on a robot will be affected by physical conditions such as
temperature and lighting, and therefore will report back noisy observed rewards.
•	Application-Specific Noise. In machine teaching tasks (Thomaz et al., 2006; Loftin et al., 2014),
when an RL agent receives feedback/instructions from people, different human instructors might
provide drastically different feedback due to their personal styles and capabilities. This way the
RL agent (machine) will obtain reward with bias.
•	Adversarial Noise. Adversarial perturbation has been widely explored in different learning tasks
and shows strong attack power against different machine learning models. For instance, Huang
et al. (2017) has shown that by adding adversarial perturbation to each frame of the game, they
can mislead RL policies arbitrarily.
Assuming an arbitrary noise model makes solving this noisy RL problem extremely challenging.
Instead, we focus on a specific noisy reward model which we call perturbed rewards, where the
observed rewards by RL agents are generated according to a reward confusion matrix. This is not a
very restrictive setting to start with, even considering that the noise could be adversarial: Given that
arbitrary pixel value manipulation attack in RL is not very practical, adversaries in the real-world
have high incentives to inject adversarial perturbation to the reward value by slightly modifying it.
For instance, adversaries can manipulate sensors via reversing the reward value.
1
Under review as a conference paper at ICLR 2019
In this paper, we develop an unbiased reward estimator aided robust framework that enables an RL
agent to learn in a noisy environment with observing only perturbed rewards. Our solution frame-
work builds on existing reinforcement learning algorithms, including the recently developed DRL
ones (Q-Learning (Watkins, 1989; Watkins & Dayan, 1992), Cross-Entropy Method (CEM) (Szita
& Lorincz, 2006), Deep SARSA (Sutton & Barto, 1998), Deep Q-NetWork (DQN) (Mnih et al.,
2013; 2015; van Hasselt et al., 2016), Dueling DQN (DDQN) (Wang et al., 2016), Deep Determin-
istic Policy Gradient (DDPG) (Lillicrap et al., 2015), Continuous DQN (NAF) (Gu et al., 2016) and
Proximal Policy Optimization (PPO) (Schulman et al., 2017) algorithms).
The main challenge is that the observed reWards are likely to be biased, and in RL or DRL the
accumulated errors could amplify the reWard estimation error over time. We do not require any
assumption on knoWing the true distribution of reWard or adversarial strategies, other than the fact
that the generation of noises folloW an unknoWn reWard confusion matrix. Instead, We address the
issue of estimating the reWard confusion matrices by proposing an efficient and flexible estimation
module. Everitt et al. (2017) provided preliminary studies for the noisy reWard problem and gave
some general negative results. The authors proved a No Free Lunch theorem, Which is, Without
any assumption about What the reWard corruption is, all agents can be misled. Our results do not
contradict With the results therein, as We consider a specific noise generation model (that leads to a
set of perturbed reWards). We analyze the convergence and sample complexity for the policy trained
based on our proposed method using surrogate reWards in RL, using Q-Learning as an example.
We conduct extensive experiments on OpenAI Gym (Brockman et al., 2016) (AirRaid, Alien, Car-
nival, MsPacman, Pong, Phoenix, Seaquest) and shoW that the proposed reWard robust RL method
achieves comparable performance With the policy trained using the true reWards. In some cases, our
method even achieves higher cumulative reWard - this is surprising to us at first, but We conjecture
that the inserted noise together With our noisy-removal unbiased estimator adds another layer of
exploration, Which proves to be beneficial in some settings. This merits a future study.
Our contributions are summarized as folloWs: (1) We adapt and generalize the idea of defining
a simple but effective unbiased estimator for true reWards using observed and perturbed reWards
to the reinforcement learning setting. The proposed estimator helps guarantee the convergence to
the optimal policy even When the RL agents only have noisy observations of the reWards. (2) We
analyze the convergence to the optimal policy and finite sample complexity of our reWard robust RL
methods, using Q-Learning as the running example. (3) Extensive experiments on OpenAI Gym
shoW that our proposed algorithms perform robustly even at high noise rates.
1.1	Related Work
Robust Reinforcement Learning It is knoWn that RL algorithms are vulnerable to noisy envi-
ronments (Irpan, 2018). Recent studies (Huang et al., 2017; Kos & Song, 2017; Lin et al., 2017)
shoW that learned RL policies can be easily misled With small perturbations in observations. The
presence of noise is very common in real-World environments, especially in robotics-relevant appli-
cations. Consequently, robust (adversarial) reinforcement learning (RRL/RARL) algorithms have
been Widely studied, aiming to train a robust policy Which is capable of Withstanding perturbed
observations (Teh et al., 2017; Pinto et al., 2017; Gu et al., 2018) or transferring to unseen envi-
ronments (RajesWaran et al., 2016; Fu et al., 2017). HoWever, these robust RL algorithms mainly
focus on noisy vision observations, instead of the observed reWards. A couple of recent Works (Lim
et al., 2016; Roy et al., 2017) have also looked into a rather parallel question of training robust RL
algorithms With uncertainty in models.
Learning with Noisy Data Learning appropriately With biased data has received quite a bit of
attention in recent machine learning studies Natarajan et al. (2013); Scott et al. (2013); Scott (2015);
Sukhbaatar & Fergus (2014); van Rooyen & Williamson (2015); Menon et al. (2015). The idea of
above line of Works is to define unbiased surrogate loss function to recover the true loss using the
knoWledge of the noises. We adapt these approaches to reinforcement learning. Though intuitively
the idea should apply in our RL settings, our Work is the first one to formally establish this extension
both theoretically and empirically. Our quantitative understandings Will provide practical insights
When implementing reinforcement learning algorithms in noisy environments.
2
Under review as a conference paper at ICLR 2019
2	Problem formulation and preliminaries
In this section, we define our problem of learning from perturbed rewards in reinforcement learning.
Throughout this paper, we will use perturbed reward and noisy reward interchangeably, as each
time step of our sequential decision making setting is similar to the “learning with noisy data”
setting in supervised learning (Natarajan et al., 2013; Scott et al., 2013; Scott, 2015; Sukhbaatar &
Fergus, 2014). In what follows, we formulate our Markov Decision Process (MDP) problem and the
reinforcement learning (RL) problem with perturbed (noisy) rewards.
2.1	Reinforcement Learning: The Noise-Free Setting
Our RL agent interacts with an unknown environment and attempts to maximize the total of his
collected reward. The environment is formalized as a Markov Decision Process (MDP), denot-
ing as M = hS, A, R, P, γi. At each time t, the agent in state st ∈ S takes an action at ∈ A,
which returns a reward r(st, at, st+1) ∈ R (which we will also shorthand as rt), and leads to
the next state st+1 ∈ S according to a transition probability kernel P, which encodes the proba-
bility Pa(st, st+1). Commonly P is unknown to the agent. The agent’s goal is to learn the opti-
mal policy, a conditional distribution π(a∣s) that maximizes the state's value function. The value
function calculates the cumulative reward the agent is expected to receive given he would follow
the current policy π after observing the current state st: Vπ(s) = Eπ Pk∞=1 γkrt+k |st=s ,
where 0 ≤ γ ≤ 11 is a discount factor. Intuitively, the agent evaluates how preferable each
state is given the current policy. From the Bellman Equation, the optimal value function is given
by V*(s) = maχa∈A Ps叶i∈s Pa(st,st+ι) [rt + γV*(st+ι)]. It is a standard practice for RL al-
gorithms to learn a state-action value function, also called the Q-function. Q-function denotes
the expected cumulative reward if agent chooses a in the current state and follows π thereafter:
Qπ(s, a) = Eπ [r(st, at, st+1) + γVπ (st+1) | st = s, at = a] .
2.2	Perturbed Reward in RL
In many practical settings, our RL agent does not observe the reward feedback perfectly. We con-
sider the following MDP with perturbed reward, denoting as M = hS, A, R, C, P, γi2: instead
of observing rt ∈ R at each time t directly (following his action), our RL agent only observes a
perturbed version of rt, denoting as r% ∈ R. For most of our presentations, we focus on the cases
where R, R are finite sets; but our results generalize to the continuous reward settings.
The generation of r follows a certain function C : S×R → R. To let our presentation stay focused,
we consider the following simple state-independent3 flipping error rates model: if the rewards are
binary (consider r+ and r-), r(st, at, st+ι) (rt) can be characterized by the following noise rate
ParameterS e+,e-:	e+ =	P(r(st,at, st+ι) =	r-lr(st,at,st+ι) =	r +),	e—	=	P(r(St,at,	st+1)=
r+ |r(st, at, st+1) = r-). When the signal levels are beyond binary, suppose there are M outcomes
in total, denoting as [Ro, Ri,…，RM-ι]. rt will be generated according to the following confusion
matrix CM ×M where each entry cj,k indicates the flipping probability for generating a perturbed
outcome: cj,k = P(rt = Rk卜t = Rj). Again we'd like to note that we focus on settings with finite
reward levels for most of our paper, but we provide discussions in Section 3.1 on how to handle
continuous rewards with discretizations.
In the paper, we do not assume knowing the noise rates (i.e., the reward confusion matrices), which
is different from the assumption of knowing them as adopted in many supervised learning works
Natarajan et al. (2013). Instead we will estimate the confusion matrices (Section 3.3).
1γ = 1 indicates an undiscounted MDP setting (Schwartz, 1993; Sobel, 1994; Kakade, 2003).
2The MDP with perturbed reward can equivalently be defined as a tuple MM = hS, A, R, R, P, γ), with the
perturbation function C implicitly defined as the difference between R and R.
3The case of state-dependent perturbed reward is discussed in Appendix C.3
3
Under review as a conference paper at ICLR 2019
3	Learning with Perturbed Rewards
In this section, we first introduce an unbiased estimator for binary rewards in our reinforcement
learning setting when the error rates are known. This idea is inspired by Natarajan et al. (2013), but
we will extend the method to the multi-outcome, as well as the continuous reward settings.
3.1	Unbiased Estimator for True Reward
With the knowledge of noise rates (reward confusion matrices), we are able to establish an unbiased
approximation of the true reward in a similar way as done in Natarajan et al. (2013). We will
call such a constructed unbiased reward as a surrogate reward. To give an intuition, we start with
replicating the results for binary reward R = {r-, r+} in our RL setting:
Lemma 1. Let r be bounded. Then, if we define,
r(st,at,st+ι)
((I-e->r+-e+∙r-
J	1-e+-e-
) (1 —e+)∙r——e-∙r+
I	1-e+-e-
(r(st ,at,st+ι) = r+)
(T(St ,at,st+ι) = r-)
(1)
we havefor any r(st, at, st+ι), Er∣r [r(st, at, st+ι)] = r(st, at, st+ι).
In the standard supervised learning setting, the above property guarantees convergence - as more
training data are collected, the empirical surrogate risk converges to its expectation, which is the
same as the expectation of the true risk (due to unbiased estimators). This is also the intuition why
we would like to replace the reward terms with surrogate rewards in our RL algorithms.
The above idea can be generalized to the multi-outcome setting in a fairly straight-forward way.
Define R := [r(r = R0), r(r = Ri),…,r(r = RM-i)], where r(r = Rm) denotes the value of the
surrogate reward when the observed reward is Rk. Let R = [Ro； Ri；…；RM-ι] be the bounded
reward matrix with M values. We have the following results:
Lemma 2. Suppose CM×M is invertible. With defining:
R = C-1 ∙ R.	(2)
we haveforany r(st, at, st+i), Er∣r [r(st, at, st+i)] = r(st, at, st+i).
Continuous reward When the reward signal is continuous, we discretize it into M intervals and
view each interval as a reward level, with its value approximated by its middle point. With increasing
M, this quantization error can be made arbitrarily small. Our method is then the same as the solution
for the multi-outcome setting, except for replacing rewards with discretized ones. Note that the finer-
degree quantization we take, the smaller the quantization error - but we would suffer from learning
a bigger reward confusion matrix. This is a trade-off question that can be addressed empirically.
So far we have assumed knowing the confusion matrices, but we will address this additional estima-
tion issue in Section 3.3, and present our complete algorithm therein.
3.2	CONVERGENCE AND SAMPLE COMPLEXITY: Q-LEARNING
We now analyze the convergence and sample complexity of our surrogate reward based RL algo-
rithms (with assuming knowing C), taking Q-Learning as an example.
Convergence guarantee First, the convergence guarantee is stated in the following theorem:
Theorem 1. Given a finite MDP, denoting as M = hS, A, R, P, γi, the Q-learning algorithm with
surrogate rewards, given by the update rule,
Qt+ι(st,at) = (1 - at)Q(st,at) + α ^ + YmaxQ(st+ι,b) ,	(3)
b∈A
converges w.p.1 to the optimal Q-function as long as t αt = ∞ and t αt2 < ∞.
Note that the term on the right hand of Eqn. (3) includes surrogate reward r estimated using Eqn. (1)
and Eqn. (2). Theorem 1 states that that agents will converge to the optimal policy w.p.1 with
replacing the rewards with surrogate rewards, despite of the noises in observing rewards. This result
is not surprising - though the surrogate rewards introduce larger variance, we are grateful of their
unbiasedness, which grants us the convergence. In other words, the addition of the perturbed reward
does not destroy the convergence guarantees of Q-Learning.
4
Under review as a conference paper at ICLR 2019
Sample complexity To establish our sample complexity results, we first introduce a generative
model following previous literature (Kearns & Singh, 1998; 2000; Kearns et al., 1999). This is a
practical MDP setting to simplify the analysis.
Definition 1. A generative model G(M) for an MDP M is a sampling model which takes a state-
action pair (st, at) as input, and outputs the corresponding reward r(st, at) and the next state st+1
randomly with the probability of Pa(st, st+ι), i.e., st+ι 〜P(∙∣s, a).
Exact value iteration is impractical if the agents follow the generative models above exactly (Kakade,
2003). Consequently, we introduce a phased Q-Learning which is similar to the ones presented
in Kakade (2003); Kearns & Singh (1998) for the convenience of proving our sample complexity
results. We briefly outline phased Q-Learning as follows - the complete description (Algorithm 2)
can be found in Appendix A.
Definition 2. Phased Q-Learning algorithm takes m samples per phase by calling generative model
G(M). It uses the collected m samples to estimate the transition probability P and update the
estimated value function per phase. Calling generative model G(MM) means that surrogate rewards
are returned and used to update value function per phase.
The sample complexity of Phased Q-Learning is given as follows:
Theorem 2. (Upper Bound) Let r ∈ [0, Rmax] be bounded reward, C be an invertible reward
confusion matrix with det(C) denoting its determinant. For an appropriate choice ofm, the Phased
Q-Learning algorithm calls the generative model G(MM) O Q(i-：)AT(C)2 log 1S11A1T) times in
T epochs, and returns a policy such that for all state S ∈ S, ∣V∏ (S) — V *(s)∣ ≤ e, e > 0, w.p.
≥ 1 - δ, 0 < δ < 1.
Theorem 2 states that, to guarantee the convergence to the optimal policy, the number of samples
needed is no more than O(1/det(C)2) times of the one needed when the RL agent observes true
rewards perfectly. This additional constant is the price we pay for the noise presented in our learning
environment. When the noise level is high, we expect to see a much higher 1/det(C)2; otherwise
when we are in a low-noise regime , Q-Learning can be very efficient with surrogate reward (Kearns
& Singh, 2000). Note that Theorem 2 gives the upper bound in discounted MDP setting; for undis-
counted setting (Y = 1), the upper bound is at the order of O (岛战& log 1S11A1T ). Lower bound
result is omitted due to the lack of space. The idea of constructing MDP in which learning is difficult
and the algorithm must make (1S11A1T log 1)calls to G(MM), is similar to Kakade (2003).
While the surrogate reward guarantees the unbiasedness, we sacrifice the variance at each of our
learning steps, and this in turn delays the convergence (as also evidenced in the sample complexity
bound). It can be verified that the variance of surrogate reward is bounded when C is invertible, and
it is always higher than the variance of true reward. This is summarized in the following theorem:
Theorem 3. Let r ∈ [0, Rmax] be bounded reward and confusion matrix C is invertible. Then, the
variance ofsurrogate reward r is bounded asfollows: Var(r) ≤ Var(r) ≤ &©选产∙ R2naχ.
To give an intuition of the bound, when we have binary reward, the variance for surrogate reward
bounds as follows: Var(r) ≤ Var(r) ≤ .-4Rmax_尸.AS e- + e+ → 1, the variance becomes
unbounded and the proposed estimator is no longer effective, nor will itbe well-defined. In practice,
there is a trade-off question between bias and variance by tuning a linear combination of R and R,
i.e., Rproxy = ηR + (1 — η)R, and choosing an appropriate η ∈ [0, 1].
3.3	Estimation of Confusion Matrices
In Section 3.1 we have assumed the knowledge of reward confusion matrices, in order to compute
the surrogate reward. This knowledge is often not available in practice. Estimating these confusion
matrices is challenging without knowing any ground truth reward information; but we’d like to note
that efficient algorithms have been developed to estimate the confusion matrices in supervised learn-
ing settings (Bekker & Goldberger, 2016; Liu & Liu, 2017; Khetan et al., 2017; Hendrycks et al.,
2018). The idea in these algorithms is to dynamically refine the error rates based on aggregated re-
wards. Note this approach is not different from the inference methods in aggregating crowdsourcing
5
Under review as a conference paper at ICLR 2019
labels, as referred in the literature (Dawid & Skene, 1979; Karger et al., 2011; Liu et al., 2012). We
adapt this idea to our reinforcement learning setting, which is detailed as follows.
At each training step, the RL agent collects the noisy reward and the current state-action pair. Then,
for each pair in S × A, the agent predicts the true reward based on accumulated historical observa-
tions of reward for the corresponding state-action pair via, e.g., averaging (majority voting). Finally,
with the predicted true reward and the accuracy (error rate) for each state-action pair, the estimated
reward confusion matrices C are given by
〜.=P(s,a)∈S×A # [r(4 s, a) = Rj lr(s, a) = Ri]
P(s,a)∈S ×A#[尸(s,α) = Ri]
(4)
where in above # H denotes the number of state-action pair that satisfies the condition [∙] in the
set of observed rewards R(s, a) (see Algorithm 1 and 3); r(s, a) and r(s, a) denote predicted true
rewards (using majority voting) and observed rewards when the state-action pair is (s, a). The above
procedure of updating Ci,j continues indefinitely as more observation arrives.
Our final definition of surrogate
reward replaces a known reward
confusion C in Eqn. (2) with our
estimated one C. We denote this
estimated surrogate reward as r.
We present (Reward Robust RL)
in Algorithm 14. Note that the
algorithm is rather generic, and
we can plug in any exisitng RL
algorithm into our reward robust
one, with only changes in re-
placing the rewards with our es-
timated surrogate rewards.
4 Experiments
Algorithm 1 Reward Robust RL (sketch)
Input: M, α, β, R(s, a)
Output: Q(s), π(s, t)
Initialize value function Q(s, a) arbitrarily.
while Q is not converged do
Initialize state s ∈ S
while s is not terminal do
Choose a from s using policy derived from Q
Take action a, observe s' and noisy reward r
if collecting enough F for every S ×A pair then
Get predicted true reward 尸 using majority voting
Estimate confusion matrix C based on r and r (Eqn. 4)
1
Obtain surrogate reward r (R = (1 — η) ∙ R + η ∙ C 1R)
Update Q using surrogate reward
S — s0
return Q(s) and π(s)
In this section, reward robust RL is tested in different games, with different noise settings. Due to
space limit, more experimental results can be found in Appendix D.
4.1	Experimental Setup
Environments and RL Algorithms To fully test the performance under different environments,
we evaluate the proposed robust reward RL method on two classic control games (CartPole, Pendu-
lum) and seven Atari 2600 games (AirRaid, Alien, Carnival, MsPacman, Pong, Phoenix, Seaquest),
which encompass a large variety of environments, as well as rewards. Specifically, the rewards
could be unary (CartPole), binary (most of Atari games), multivariate (Pong) and even continu-
ous (Pendulum). A set of state-of-the-art reinforcement learning algorithms are experimented with
while training under different amounts of noise (See Table 3)5. For each game and algorithm, three
policies are trained based on different random initialization to decrease the variance.
Reward Post-Processing For each game and RL algorithm, we test the performances for learning
with true rewards, learning with noisy rewards and learning with surrogate rewards. Both symmet-
ric and asymmetric noise settings with different noise levels are tested. For symmetric noise, the
confusion matrices are symmetric. As for asymmetric noise, two types of random noise are tested:
1) rand-one, each reward level can only be perturbed into another reward; 2) rand-all, each reward
could be perturbed to any other reward, via adding a random noise matrix. To measure the amount
of noise w.r.t confusion matrices, we define the weight of noise ω in Appendix B.2. The larger ω is,
the higher the noise rates are.
4One complete Q-Learning implementation (Algorithm 3) is provided in Appendix C.1.
5The detailed settings are accessible in Appendix B.
6
Under review as a conference paper at ICLR 2019
4.2	Robustness Evaluation
CartPole The goal in CartPole is to prevent the pole from falling by controlling the cart’s direction
and velocity. The reward is +1 for every step taken, including the termination step. When the cart or
pole deviates too much or the episode length is longer than 200, the episode terminates. Due to the
unary reward {+1} in CartPole, a corrupted reward -1 is added as the unexpected error (e- = 0).
As a result, the reward space R is extended to {+1, -1}. Five algorithms Q-Learning (1992),
CEM (2006), SARSA (1998), DQN (2016) and DDQN (2016) are evaluated.
(a) Q-Learning
(b) CEM
(c) SARSA
(d) DQN
(e) DDQN
Figure 1: Learning curves from five RL algorithms on CartPole game with true rewards (r) ■, noisy
rewards (r) ■ and estimated surrogate rewards (r) (η = 1) ■. Note that reward confusion matrices
C are unknown to the agents here. Full results are in Appendix D.2 (Figure 6).
In Figure 1, we show that our estimator successfully produces meaningful surrogate rewards that
adapt the underlying RL algorithms to the noisy settings, without any assumption of the true distri-
bution of rewards. With the noise rate increasing (from 0.1 to 0.9), the models with noisy rewards
converge slower due to larger biases. However, we observe that the models always converge to the
best score 200 with the help of surrogate rewards.
In some circumstances (slight noise - see Figure 6a, 6b, 6c, 6d), the surrogate rewards even lead to
faster convergence. This points out an interesting observation: learning with surrogate reward even
outperforms the case with observing the true reward. We conjecture that the way of adding noise
and then removing the bias introduces implicit exploration. This implies that for settings even with
true reward, we might consider manually adding noise and then remove it in expectation.
Pendulum The goal in Pendulum is to keep a frictionless pendulum standing up. Different from
the CartPole setting, the rewards in pendulum are continuous: r ∈ (-16.28, 0.0]. The closer the
reward is to zero, the better performance the model achieves. Following our extension (see Sec-
tion 3.1), the (-17,0] is firstly discretized into 17 intervals: (-17, -16], (-16, -15],…,(-1,0],
with its value approximated using its maximum point. After the quantization step, the surrogate
rewards can be estimated using multi-outcome extensions presented in Section 3.1.
Table 1: Average scores of various RL algorithms on CartPole and Pendulum with noisy rewards ⑺
and surrogate rewards under known (^) or estimated (r) noise rates. Note that the results for last two
algorithms DDPG (rand-one) & NAF (rand-all) are on Pendulum, but the others are on CartPole.
Noise Rate	I Reward ∣	Q-Learn	CEM	SARSA	DQN	DDQN DDPG		NAF
	r	170.0	98.1	165.2	187.2	187.8	-1.03	-4.48
ω = 0.1	r	165.8	108.9	173.6	200.0	181.4	-0.87	-0.89
	r	181.9	99.3	171.5	200.0	185.6	-0.90	-1.13
	r	134.9	28.8	144.4	173.4	168.6	-1.23	-4.52
ω = 0.3	r	149.3	85.9	152.4	175.3	198.7	-1.03	-1.15
	r	161.1	82.2	159.6	186.7	200.0	-1.05	-1.36
We experiment two popular algorithms, DDPG (2015) and NAF (2016) in this game. In Figure 2,
both algorithms perform well with surrogate rewards under different amounts of noise. In most
cases, the biases were corrected in the long-term, even when the amount of noise is extensive (e.g.,
ω = 0.7). The quantitative scores on CartPole and Pendulum are given in Table 1, where the
7
Under review as a conference paper at ICLR 2019
①POMd①」odPBM①」
∞0 = 3
①POMdm」ad PJeMə.i
=3
(b) DDPG (rand-one)
(c) DDPG (rand-all)
(a) DDPG (symmetric)
Figure 2: Learning curves from DDPG and NAF on Pendulum game With true rewards (r) ■, noisy
rewards (r) and surrogate rewards (r) (η = 1) . Both symmetric and asymmetric noise are
conduced in the experiments. Full results are in Appendix D.2 (Figure 8).
O 200	400	600
episode
(d) NAF (rand-all)
scores are averaged based on the last thirty episodes. The full results (ω > 0.5) can be found in
Appendix D.1, so does Table 2. Our reward robust RL method is able to achieve consistently good
scores.
Atari We validate our algorithm on seven Atari 2600 games using the state-of-the-art algorithm
PPO (Schulman et al., 2017). The games are chosen to cover a variety of environments. The rewards
in the Atari games are clipped into {-1, 0, 1}. We leave the detailed settings to Appendix B.
Figure 3: Learning curves from PPO on Pong-v4 game with true rewards (r) ■, noisy rewards (F) I
and surrogate rewards (η = 1) (r)	. The noise rates increase from 0.1 to 0.9, with a step of 0.1.
Results for PPO on Pong-v4 in symmetric noise setting are presented in Figure 3. Due to limited
space, more results on other Atari games and noise settings are given in Appendix D.3. Similar to
previous results, our surrogate estimator performs consistently well and helps PPO converge to the
optimal policy. Table 2 shows the average scores of PPO on five selected Atari games with different
amounts of noise (symmetric & asymmetric). In particular, when the noise rates e+ = e- > 0.3,
agents with surrogate rewards obtain significant amounts of improvements in average scores. We do
not present the results for the case with unknown C because the state-space (image-input) is very
large for Atari games, which is difficult to handle with the solution given in Section 3.3.
5	Conclusion
Only an underwhelming amount of reinforcement learning studies have focused on the settings with
perturbed and noisy rewards, despite the fact that such noises are common when exploring a real-
world scenario, that faces sensor errors or adversarial examples. We adapt the ideas from supervised
8
Under review as a conference paper at ICLR 2019
Table 2: Average scores of PPO on five selected games with noisy rewards (r) and surrogate rewards
(r). The experiments are repeated three times with different random seeds.
Noise Rate	Reward Lift (↑)	Mean Alien Carnival Phoenix MsPacman Seaquest
e- = e+ = 0.1	^	-	2044.2^^1814.8^^1239.2	4608.9	1709.1	849.2 r	67.5% ↑ 3423.1	1741.0	3630.3	7586.3	2547.3	1610.6
e- = 0.1, e+ = 0.3	^	-	770.5	893.3	841.8	250.7	1151.1	715.7 r	20.3% ↑	926.6	973.7	955.2	643.9	1307.1	753.1
e- = e+ = 0.3	^	-	1180.1 ~~543.1	919.8	2600.3	1109.6	727.8 r	46.7%↑	1730.8	1637.7	966.1	4171.5	1470.2	408.6
learning with noisy examples (Natarajan et al., 2013), and propose a simple but effective RL frame-
work for dealing with noisy rewards. The convergence guarantee and finite sample complexity of
Q-Learning (or its variant) with estimated surrogate rewards are given. To validate the effective-
ness of our approach, extensive experiments are conducted on OpenAI Gym, showing that surrogate
rewards successfully rescue models from misleading rewards even at high noise rates.
References
Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable
labels. In ICASSP, pp. 2682-2686. IEEE, 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates
using the em algorithm. Applied statistics, pp. 20-28, 1979.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/
openai/baselines, 2017.
Tom Everitt, Victoria Krakovna, Laurent Orseau, and Shane Legg. Reinforcement learning with a
corrupted reward channel. In IJCAI, pp. 4705-4713. ijcai.org, 2017.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-
forcement learning. CoRR, abs/1710.11248, 2017.
Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning
with model-based acceleration. In ICML, volume 48 of JMLR Workshop and Conference Pro-
ceedings, pp. 2829-2838. JMLR.org, 2016.
Zhaoyuan Gu, Zhenzhong Jia, and Howie Choset. Adversary a3c for robust reinforcement learning,
2018. URL https://openreview.net/forum?id=SJvrXqvaZ.
Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse
reward design. In Advances in Neural Information Processing Systems, pp. 6765-6774, 2017.
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train
deep networks on labels corrupted by severe noise. CoRR, abs/1802.05300, 2018.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017.
Alex Irpan. Deep reinforcement learning doesn’t work yet. https://www.alexirpan.com/
2018/02/14/rl-hard.html, 2018.
Tommi S. Jaakkola, Michael I. Jordan, and Satinder P. Singh. Convergence of stochastic iterative
dynamic programming algorithms. In NIPS, pp. 703-710. Morgan Kaufmann, 1993.
9
Under review as a conference paper at ICLR 2019
Sham Machandranath Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis,
University of London, 2003.
David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing
systems. In Advances in neural information processing Systems, pp. 1953-1961, 2011.
Michael J. Kearns and Satinder P. Singh. Finite-sample convergence rates for q-learning and indirect
algorithms. In NIPS, pp. 996-1002. The MIT Press, 1998.
Michael J. Kearns and Satinder P. Singh. Bias-variance error bounds for temporal difference updates.
In COLT, pp. 142-147. Morgan Kaufmann, 2000.
Michael J. Kearns, Yishay Mansour, and Andrew Y. Ng. A sparse sampling algorithm for near-
optimal planning in large markov decision processes. In IJCAI, pp. 1324-1231. Morgan Kauf-
mann, 1999.
Ashish Khetan, Zachary C. Lipton, and Anima Anandkumar. Learning from noisy singly-labeled
data. CoRR, abs/1712.04577, 2017.
Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. CoRR,
abs/1705.06452, 2017.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR,
abs/1509.02971, 2015.
Shiau Hong Lim, Huan Xu, and Shie Mannor. Reinforcement learning in robust markov decision
processes. Math. Oper. Res., 41(4):1325-1353, 2016.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun.
Tactics of adversarial attack on deep reinforcement learning agents. In IJCAI, pp. 3756-3762.
ijcai.org, 2017.
Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In Proc. of
NIPS, 2012.
Yang Liu and Mingyan Liu. An online learning approach to improving the quality of crowd-
sourcing. IEEE/ACM Transactions on Networking, 25(4):2166-2179, Aug 2017.
R. Loftin, B. Peng, J. MacGlashan, M. L. Littman, M. E. Taylor, J. Huang, and D. L. Roberts.
Learning something from nothing: Leveraging implicit human feedback strategies. In The 23rd
IEEE International Symposium on Robot and Human Interactive Communication, pp. 607-612,
Aug 2014.
Aditya Menon, Brendan Van Rooyen, Cheng Soon Ong, and Bob Williamson. Learning from cor-
rupted binary labels via class-probability estimation. In International Conference on Machine
Learning, pp. 125-134, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,
abs/1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial rein-
forcement learning. In ICML, volume 70 of Proceedings of Machine Learning Research, pp.
2817-2826. PMLR, 2017.
Matthias Plappert. keras-rl. https://github.com/keras-rl/keras-rl, 2016.
10
Under review as a conference paper at ICLR 2019
Aravind Rajeswaran, Sarvjeet Ghotra, Sergey Levine, and Balaraman Ravindran. Epopt: Learning
robust neural network policies using model ensembles. CoRR, abs/1610.01283, 2016.
Joshua Romoff, Alexandre Piche, Peter Henderson, Vincent Frangois-Lavet, and Joelle Pineau. Re-
ward estimation for variance reduction in deep reinforcement learning. CoRR, abs/1805.03359,
2018.
Aurko Roy, Huan Xu, and Sebastian Pokutta. Reinforcement learning under model mismatch.
CoRR, abs/1706.04711, 2017.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017.
Anton Schwartz. A reinforcement learning method for maximizing undiscounted rewards. In ICML,
pp. 298-305. Morgan KaUfmann, 1993.
Clayton Scott. A rate of convergence for mixture proportion estimation, with application to learning
from noisy labels. In AISTATS, 2015.
Clayton Scott, Gilles Blanchard, Gregory Handy, Sara Pozzi, and Marek Flaska. Classification with
asymmetric label noise: Consistency and maximal denoising. In COLT, pp. 489-511, 2013.
Matthew J. Sobel. Mean-variance tradeoffs in an undiscounted MDP. Operations Research, 42(1):
175-183, 1994.
Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv
preprint arXiv:1406.2080, 2(3):4, 2014.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive com-
putation and machine learning. MIT Press, 1998.
Istvan Szita and Andras Lorincz. Learning tetris using the noisy cross-entropy method. Neural
Computation, 18(12):2936-2941, 2006.
Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,
Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In NIPS,
pp. 4499-4509, 2017.
Andrea Lockerd Thomaz, Cynthia Breazeal, et al. Reinforcement learning with human teachers:
Evidence of feedback and guidance with implications for learning performance. 2006.
John N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine Learning, 16
(3):185-202, 1994.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In AAAI, pp. 2094-2100. AAAI Press, 2016.
Brendan van Rooyen and Robert C Williamson. Learning in the presence of corruption. arXiv
preprint arXiv:1504.00091, 2015.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.
Dueling network architectures for deep reinforcement learning. In ICML, volume 48 of JMLR
Workshop and Conference Proceedings, pp. 1995-2003. JMLR.org, 2016.
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. In Machine Learning, pp. 279-292,
1992.
Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards. PhD thesis, King’s
College, Cambridge, UK, May 1989.
Yuchen Zhang, Xi Chen, Denny Zhou, and Michael I Jordan. Spectral methods meet em: A provably
optimal algorithm for crowdsourcing. In Advances in neural information processing systems, pp.
1260-1268, 2014.
11
Under review as a conference paper at ICLR 2019
A Proofs
ProofofLemma 1. For simplicity, We shorthand r(st,at,st+1),r(st,at,st+1),r(st, at,st+ι) as
r, r, r, and let r+,r-,r+, r- denote the general reward levels and corresponding surrogate ones:
Er∣r(r) = Pr∣r(r = r-)r- + Pr∣r(r = r+ )r+.	(5)
When r = r+ , from the definition in Lemma 1:
PrIr(r = r-) = e+, PrIr(r = r+) = 1 - e+.
Taking the definition of surrogate rewards Eqn. (1) into Eqn. (5), we have
ErIr(r) = e+ ∙ r- + (I - e+) ∙ r+
e+ ∙
(I - e+)r- - e-r+	( (I - e-)r+ - e+r-
1-e--e+	+(1 …^ I—+
r+.
Similarly, when r = r-, it also verifies Er∣r [r(st, at, st+ι)] = r(st, at, st+ι).
□
Proof of Lemma 2. The idea of constructing unbiased estimator is easily adapted to multi-outcome
reward settings via writing out the conditions for the unbiasedness property (s.t. Er∣r [r] = r.). For
simplicity, we shorthand r(r = Ri) as Ri in the following proofs. Similar to Lemma 1, we need to
solve the following set of functions to obtain ^
D	_ C E>	I	E>	I	IC	E>
∫R0 = c0,0 ∙ R0 + c0,1 ∙ R1 + …+ c0,M-1 ∙ RM-1
I >	A	I	A	I	I	A
R1 = c1,0 ∙ R0 + c1,1 ∙ R1 + …+ c1,M-1 ∙ RM-1
D	_ c	E> I	E> I	IC	E>
RM-1 = CM-1,0 ∙ R0 + CM-1,1 ∙ RI + …+ CM-1,M-1 ∙ RM-1
where Ri denotes the value of the surrogate reward when the observed reward is Ri. Define R :=
[Ro； R1； •…；RM-1], and R := [Ro, R1,…,RM-1], then the above equations are equivalent to:
R = C ∙ R. If the confusion matrix C is invertible, we obtain the surrogate reward:
R = C-1 ∙ R.
According to above definition, for any true reward level Ri, i = 0,1,…，M - 1, we have
E	Γ^l	Δ 1	Δ 1	1	7⅛	I >
ErIr=Ri[r] = ci,0 ∙ R0 + ci,1 ∙ R1 + …+ ci,M-1 ∙ RM-1 = Ri
□
Furthermore, the probabilities for observing surrogate rewards can be written as follows:
P = [p1,p2,…，pM] = Epjcj,1, £巧cj,2,…，Epjcj,M ,
jj	j
where p = Pj PjCj,i, and pi, Pi represent the probabilities of occurrence for surrogate reward Ri
and true reward Ri respectively.
Corollary 1. Let Pi and Pi denote the probabilities of occurrence for surrogate reward r(r = Ri)
and true reward Ri. Then the surrogate reward satisfies,
Pa(st,st+1)r(st,a,st+1) = EPj Rj = EPj Rj.	⑹
s0∈S	j	j
Proof of Corollary 1. From Lemma 2, we have,
Pa(st, st+1)r(st, a, st+1) =	Pa(st, st+1,Rj)Rj
st∈S	st+ι∈S;Rj ∈R
=	Pa(st, st+1)Rj =	PjRj =	PjRj.
Rj∈Rst+1∈S	Rj∈R	j
12
Under review as a conference paper at ICLR 2019
Consequently,
∑Pj Rj = E ∑Pk Ckj Rj =>£ Ckj Rj
= pkRk=	Pa(st, st+1)r(st, a, st+1).
k	st∈S
□
To establish Theorem 1, we need an auxiliary result (Lemma 3) from stochastic process approxi-
mation, which is widely adopted for the convergence proof for Q-Learning (Jaakkola et al., 1993;
Tsitsiklis, 1994).
Lemma 3. The random process {∆t} taking values in Rn and defined as
∆t+1 (x) = (1 - αt(x))∆t(x) + αt(x)Ft(x)
converges to zero w.p.1 under the following assumptions:
•	0 ≤ αt ≤ 1,	t αt(x) = ∞ and t αt(x)2 < ∞;
•	||E [Ft(χ)∣Ft] ||w ≤ γ∣∣∆t∣∣, with Y < 1 ；
•	var [Ft(x)Ft] ≤ C(1 + ∣∣∆t∣∣W) ,for C> 0∙
Here Ft = {∆t, ∆t-ι, ∙∙∙ , Ft-ι ∙∙∙ ,at, ∙∙∙} stands for the past at Step t, αt(x) is allowed to
depend on the past insofar as the above conditions remain valid. The notation ∣∣∙∣∣ w refers to some
weighted maximum norm.
ProofofLemma 3. See previous literature (Jaakkola et al., 1993; Tsitsiklis, 1994).	□
Proofof Theorem 1. For simplicity, We abbreviate st, st+ι, Qt, Qt+ι, rt, Irt and at as s, s0, Q, Q0,
r, r, and α, respectively.
Subtracting from both sides the quantity Q*(s, a) in Eqn. (3):
Q0(s, a) - Q*(s, a) = (1 - α)(Q(s, a) - Q*(s, a)) + α r + YmaxQ(s0, b) - Q*(s, a).
b∈A
Let ∆t(s, a) = Q(s, a) — Q*(s, a) and Ft(s, a) = r + Y maXb∈A Q(s0, b) — Q*(s, a).
∆t+1(s0, a) = (1 - α)∆t(s, a) + αFt(s, a).
In consequence,
E [Ft(x)∣Ft] =	X	Pa(s,s0,r) r + YmaxQ(s0,b) - Q*(s,a)
s0∈S∖r∈R	b	b∈A
= X	Pa(s,s0,r)r+ X Pa(s,s0)
s0∈S[r∈R	s0∈S
=X	Pa(s,s0,r)r - X Pa(s,s0)r + EPa(S,s0)γ
s0∈S[r∈R	s0∈S	s0∈S
=XPjrj - X Pa(s, CT + X Pa(s, SO)Y
j	s0∈S	s0∈S
= X Pa(S, S0)Y
s0∈S
≤ Y X Pa(s, s0)八 max JQ(S0, b) - Q*(s0, b)|
s0∈S b∈A,s0∈S
=Y X Pa(s, SO)IIQ - Q*ll∞ = YIIQ - Q*ll∞ = Y11δM∞.
s0∈S
Y max Q(s0, b) — r — Y max Q*(s0, b)
maxQ(s0, b) — maxQ*(s0, b)
b∈A	b∈A
maxQ(s0, b) — maxQ*(s0, b)
b∈A	b∈A
maxQ(s0, b) — maxQ*(s0, b)	(using Eqn. (6))
13
Under review as a conference paper at ICLR 2019
Finally,
Var [Ft(x)∣Ft] = E I Ir + Ymax Q(s0,b) — ɪ2 P0(s,s0,r)
∖	^0∈∈S	s0es；reR
r + γ max Q(s0, b)
Var r + γmaxQ(s0, b)∣Ft
Because r is bounded, it can be clearly verified that
Var [Ft(x)∣Ft] ≤ C(1 + ∣∣∆t∣∣W)
for some constant C. Then, due to the Lemma 3, ∆t converges to zero w.p.1, i.e., Q0(s, a) converges
to Q* (s,a).	□
The procedure of Phased Q-Learning is described as Algorithm 2:
Algorithm 2 Phased Q-Learning
Input: G(M): generative model of M = (S, A, R, P, γ), T: number of iterations.
Output: V(s): value function, π(s, t): policy function.
1:	Set VT(s) = 0
2:	for t = T 一 1,…，0 do
1.	Calling G(M) m times for each state-action pair.
Pa (St,st+I)=#[(st，at) → st+l]
m
2.	Set
V(s)=mχ X Pa(st,St+l)bt + γV(st+l)]
st+1 ∈S
∏(s,t) = arg max V(S)
aA
3:	return V(S) and ∏(s,t)
Note that P here is the estimated transition probability, which is different from P in Eqn. (6).
To obtain the sample complexity results, the range of our surrogate reward needs to be known.
Assuming reward r is bounded in [0, Rmax], Lemma 4 below states that the surrogate reward is also
bounded, when the confusion matrices are invertible:
Lemma 4. Letr ∈ [0, Rmax] be bounded, where Rmax is a constant; suppose CM×M, the confusion
matrix, is invertible with its determinant denoting as det(C). Then the surrogate reward satisfies
0 ≤∣r∣≤
M
det(C)
max.
(7)
R
Proof of Lemma 4. From Eqn. (2), we have,
R= C-1 ∙ R
adj(C)
det(C)
∙ R,
where adj(C) is the adjugate matrix of C; det(C) is the determinant of C. It is known from linear
algebra that,
adj(C)j = (-1)i+j ∙ Mji,
14
Under review as a conference paper at ICLR 2019
where Mji is the determinant of the (M — 1) × (M — 1) matrix that results from deleting row j and
column i of C. Therefore, Mji is also bounded:
(∖	M-1 (M-1	∖
∣sgn(σ)∣∏ cm,σn	≤ ∏ X CS =IM = 1,
m=1	)	m=0 ∖ n=0	)
where the sum is computed over all permutations σ of the set {0,1,…，M — 2}; C is the element
of Mji; sgn(σ) returns a value that is +1 whenever the reordering given by σ can be achieved by
successively interchanging two entries an even number of times, and —1 whenever it can not.
Consequently,
I Pj ∣adj(C)ij ∣ ∙ ∣Rj ∣	MR
i∖ =	EC	≤ det(C) . max.
□
ProofofTheorem 2. From HOefding’s inequality, we obtain:
P I I E Pa(st, st+1)匕+1(st+1) — E Pa(st, st+1)匕+1(st+1) ≥ E I
∖Jst 十i∈s	St 十i∈s	)
——2me2(1 — Y)2 ∖
≤ 2exp (	R「) )，
because Vt(st) is bounded within RI-X. In the same way, ^ is bounded by deM(c) ∙ RmaX from
Lemma 4. We then have,
P I I X Pα(st, st+1,rt)rt — X Pα(st, st+1,rt)rt
s 1 st+1 ∈S	st+1 ∈s
∖ i rt∈R	rt∈τR
≤ 2 exp
—2me2det(C)2
M 2R2ax
ImaX
∖
≥ E
I
Further, due to the unbiasedness of surrogate rewards, we have
X Pα(st,st+1)rt = X	Pα(st, st+1√^t)rt∙
st+1∈S	st十ι∈S∙ft∈R
As a result,
I V7(S)— Vi(S)I= max X Pa (st,St+1)[rt + YVt+1(st+1)]
st + 1 ∈s
—max X Pa(St, st+1) [rt + Y匕+1(st+1)]
st + 1 ∈s
≤ E1 + Y max X Pa(St, st+1)vt+1(st+1) — X Pa(St, st+1)Vt+1(St+1)
st+1∈S	st+1∈S
+ maχ £ Pa(st, st+1)rt — E	Pa(st,st+1 √^t)rt
st+1∈S	st+1∈s∙r ∈R
≤ Y max
一s∈S
I vt+1(S)- Vt+1(S) 1
+ E1 + YE2
In the same way,
i Vi(S) - Vi(S) i ≤
Y max
s∈S
I Vt+1(S)- Vt+1(S) i
+ E1 + YE2
15
Under review as a conference paper at ICLR 2019
Recursing the two equations in two directions (0 → T), we get
max IV*(s) — V(S)I ≤ & + γs) + γ(6ι + γ62) +--------+ YT-1(6ι + YS)
s∈S
(1 + Y2)(1 — YT)
1-γ
(1 + γ2)(1 - γT)
1-γ
Combining these two inequalities above we have:
max |V*(s) — V(s)| ≤ 2
s∈S
(S + YS)(I - YT) ≤ 2 (S + Y⑴
1-γ
1-γ
Let 6ι = 62, so maxs∈s |V*(s) — V(s)| ≤ C as long as
1 = 2 ≤
(1 - γ)
2(1+ TV
For arbitrarily small e, by choosing m appropriately, there always exists ∈ι =⑦=2(-+Y) such that
the policy error is bounded within . That is to say, the Phased Q-Learning algorithm can converge
to the near optimal policy within finite steps using our proposed surrogate rewards.
Finally, there are ∣S∣∣A∣T transitions under which these conditions must hold, where | ∙ | represent
the number of elements in a specific set. Using a union bound, the probability of failure in any
condition is smaller than
2iSiiA1T ∙eχp (-mt2((I-Y))2 ∙ min{(ι - γ)2, deMC-}).
We set the error rate less than δ, and m should satisfy that
1
∣s∣∣A∣τ
δ
m = OEfdE log
In consequence, after m∣S∣∣A∣T calls, which is, O Q(OAeT(C尸 log 1S11A1T ) ,the value function
converges to the optimal one for every state s, with probability greater than 1 - δ.
□
The above bound is for discounted MDP setting with 0 ≤ Y < 1. For undiscounted setting Y = 1,
since the total error (for entire trajectory ofT time-steps) has to be bounded by C, therefore, the error
for each time step has to be bounded by T. Repeating our anayslis, We obtain the following upper
bound:
O ( ∣s∣IA∣τ3 lnJSllAlTʌ
底2det(C)2 g δ ).
Proof of Theorem 3.
Var(r) — Var(r) = E [(r — E[r])2] — E [(r — E[r])2]
=E[r2] — E[r]2 + E[r2] — E[r]2
2
Xpj R2
j
∑Pj R
PjRj2 —	PjRj
XPjR2 - XPjRj2 = XXpiCi,jR2 -XPj	Cj,iRi丫
X PjX Cj,iR -
ji
cj,i Ri
—
—
j
j
j
j
j
j
i
16
Under review as a conference paper at ICLR 2019
Using the CauchySchwarz inequality,
X Cj,iR = X E ∙ X (√jj冗)2 ≥
i	ii
So we get,
In addition,
Var(r) — Var(r) ≥ 0.
Var(r) = XPjRj2 -(XPjRj ≤ XPjRjj
jjj
≤ X p, M2	∙ R2	= M2	∙ R2 .
≤ 勺Pj det(C)2	max	det(C)2	max
□
B Experimental Setup
We set up our experiments within the popular OpenAI baselines (Dhariwal et al., 2017) and keras-
rl (Plappert, 2016) framework. Specifically, we integrate the algorithms and interact with OpenAI
Gym (Brockman et al., 2016) environments (Table 3).
B.1	RL Algorithms
A set of state-of-the-art reinforcement learning algorithms are ex-
perimented with while training under different amounts of noise,
including Q-Learning (Watkins, 1989; Watkins & Dayan, 1992),
Cross-EntroPy Method (CEM) (Szita & Lorincz, 2006), Deep
SARSA (Sutton & Barto, 1998), Deep Q-Network (DQN) (Mnih
et al., 2013; 2015; van Hasselt et al., 2016), Dueling DQN
(DDQN) (Wang et al., 2016), Deep Deterministic Policy Gradi-
ent (DDPG) (Lillicrap et al., 2015), Continuous DQN (NAF) (Gu
et al., 2016) and Proximal Policy Optimization (PPO) (Schulman
et al., 2017) algorithms. For each game and algorithm, three poli-
cies are trained based on different random initialization to de-
crease the variance in experiments.
Table 3: RL algorithms utilized
in the robustness evaluation.
Environment	RL Algorithm
CartPole	Q-Learning (1989) CEM (2006) SARSA (1998) DQN (2013; 2015) DDQN (2016)
Pendulum	-DDPG (2015) NAF (2016)
Atari Games	PPO(2017)-
B.2	Post-Processing Rewards
We explore both symmetric and asymmetric noise of different noise levels. For symmetric noise,
the confusion matrices are symmetric, which means the probabilities of corruption for each reward
choice are equivalent. For instance, a confusion matrix
0.8 0.2
C = 0.2 0.8
says that r1 could be corrupted into r2 with a probability of 0.2 and so does r2 (weight = 0.2).
As for asymmetric noise, two types of random noise are tested: 1) rand-one, each reward level can
only be perturbed into another reward; 2) rand-all, each reward could be perturbed to any other
reward. To measure the amount of noise w.r.t confusion matrices, we define the weight of noise as
follows:
C = (1 — ω) ∙ I + ω∙ N, ω ∈ [0,1],
where ω controls the weight of noise; I and N denote the identity and noise matrix respectively.
Suppose there are M outcomes for true rewards, N writes as:
-no,o no,ι	•… no,M-1 -
N =	…	......... …	,
_nM-1,0 nM-1,1	•… nM-1,M-L
17
Under review as a conference paper at ICLR 2019
where for each row i, 1) rand-one: randomly choose j, s.t ni,j = 1 and ni,k 6= 0 if k 6= j; 2) rand-
all: generate M random numbers that sum to 1, i.e., j ni,j = 1. For the simplicity, for symmetric
noise, we choose N as an anti-identity matrix. As a result, ci,j = 0, if i 6= j or i + j 6= M.
B.3 Perturbed-Reward MDP Example
To obtain an intuitive view of the reward perturbation model, where the observed rewards are gen-
erated based on a reward confusion matrix, we constructed a simple MDP and evaluated the per-
formance of robust reward Q-Learning (Algorithm 1) on different noise ratios (both symmetric and
asymmetric). The finite MDP is formulated as Figure 4a: when the agent reaches state 5, it gets an
instant reward of r+ = 1, otherwise a zero reward r- = 0. During the explorations, the rewards are
perturbed according to the confusion matrix C2×2 = [1 - e-, e-; e+, 1 - e+].
(a) Finite MDP (six-state)
(b) Estimation process in time-variant noise
Training step (×102)
Figure 4: Perturbed-Reward MDP Example
There are two experiments conducted in this setting: 1) performance of Q-Learning under different
noise rates (Table 4); 2) robustness of estimation module in time-variant noise (Figure 4b). As shown
in Table 4, Q-Learning achieved better results consistently with the guidance of surrogate rewards
and the confusion matrix estimation algorithm. For time-variant noise, we generated varying amount
of noise at different training stages: 1) e- = 0.1, e+ = 0.3 (0 to 1e4 steps); 2) e- = 0.2, e+ = 0.1
(1e4 to 3e4 steps); 3) e- = 0.3, e+ = 0.2 (3e4 to 5e4 steps); 4) e- = 0.1, e+ = 0.2 (5e4 to 7e4
steps). In Figure 4b, we show that Algorithm 1 is robust against time-variant noise, which dynam-
ically adjusts the estimated C after the noise distribution changes. Note that we set a maximum
memory size for collected noisy rewards to let the agents only learn with recent observations.
Table 4: Average performance of Q-Learning on Perturbed MDP example (Figure 4a) with noisy
rewards (r), surrogate rewards under known (r) or estimated (r) noise rates. Note that the success
means the agents can find the optimal policy at every initial state according to learned Q-function.
We repeated the experiments 1,000 times to calculate the successful rate for each noisy setting.
Noise Rate		Reward	Lift (↑)	Success	Noise Rate		Reward	Lift (↑)	Success
		r	-	93.4%			r	-	89.5%
e- = e+	0.1	r	2.7% ↑	96.1%	e- = 0.1,e+ =	0.3	r	3.4% ↑	92.9%
		r	3.2% ↑	96.6%			r	2.9% ↑	92.4%
		r	-	85.5%			r	-	90.3%
e- = e+	0.3	r	1.4% ↑	86.9%	e- = 0.3, e+ =	0.1	r	0.9% ↑	91.2%
		r	0.9% ↑	86.4%			r	1.2% ↑	91.5%
B.4 Training Details
CartPole and Pendulum The policies use the default network from keras-rl framework. which
is a five-layer fully connected network6. There are three hidden layers, each of which has 16 units
and followed by a rectified nonlinearity. The last output layer is activated by the linear function. For
6https://github.com/keras-rl/keras- rl/examples
18
Under review as a conference paper at ICLR 2019
CartPole, We trained the models using Adam optimizer with the learning rate of 1e-3 for 10,000
steps. The exploration strategy is Boltzmann policy. For DQN and Dueling-DQN, the update rate of
target model and the memory size are 1e-2 and 50, 000. For Pendulum, We trained DDPG and NAF
using Adam optimizer with the learning rate of 5e-4 for 150, 000 steps. the update rate of target
model and the memory size are 1e-3 and 100, 000.
Atari Games We adopt the pre-processing steps as well as the network architecture from Mnih
et al. (2015). Specifically, the input to the network is 84 × 84 × 4, which is a concatenation of the last
4 frames and converted into 84 × 84 gray-scale. The network comprises three convolutional layers
and two fully connected layers7. The kernel size of three convolutional layer are 8 × 8 with stride
4 (32 filters), 4 × 4 with stride 2 (64 filters) and 3 × 3 with stride 1 (64 filters), respectively. Each
hidden layer is followed by a rectified nonlinearity. Except for Pong where we train the policies
for 3e7 steps, all the games are trained for 5e7 steps with the learning rate of 3e-4 . Note that the
rewards in the Atari games are discrete and clipped into {-1, 0, 1}. Except for Pong game, in which
r = -1 means missing the ball hit by the adversary, the agents in other games attempt to get higher
scores in the episode with binary rewards 0 and 1.
C Estimation of Confusion Matrices
C.1 Reward Robust RL Algorithms
As stated in Section 3.3, the confusion matrix can be estimated dynamically based on the aggregated
answers, similar to previous literature in supervised learning (Khetan et al., 2017). To get a concrete
view, we take Q-Learning for an example, and the algorithm is called Reward Robust Q-Learning
(Algorithm 3). Note that is can be extended to other RL algorithms by plugging confusion matrix
estimation steps and the computed surrogate rewards, as shown in the experiments (Figure 6).
Algorithm 3 Reward Robust Q-Learning
Input:
M = (S, A, R, P, γ): MDP with corrupted reward channel
T : transition function T : S × A → S
N ∈ N: upper bound of collected noisy rewards
α ∈ (0, 1): learning rate in the update rule
η ∈ (0, 1): weight of unbiased surrogate reward
R(s, a): set of observed rewards when the state-action pair is (s, a).
Output: Q(s): value function; π(s, t): policy function
Initialize Q: S × A → R arbitrarily
Set confusion matrix C as zero
while Q is not converged do
Start in state s ∈ S
while s is not terminal do
Calculate π according to Q and exploration strategy
a J π(s)
s0 J T(s, a)
Observe noisy reward r(s, a) and add it to R(s, a)
_____ . ~, 、. _________
if P(s,a) IR(S,a)| ≥ N then
Get predicted true reward r(s,a) using majority voting in every R(s, a)
Estimate confusion matrix C based on r(s, a) and r(s, a) (Eqn.(4))
Empty all the sets of observed rewards R(S, a)
Obtain surrogate reward r(s,a) using Rproxy = (1 一 η) ∙ R + η ∙ CTR
Q(s0, a) J (1 — α) ∙ Q(s, a) + α ∙ (r(s, a) + Y ∙ max。，Q(s0, a0))
S J S0
return Q(S) and π(S)
7https://github.com/openai/baselines/tree/master/baselines/common
19
Under review as a conference paper at ICLR 2019
C.2 Expectation-Maximization in Estimation
In Algorithm 3, the predicted true reward r(s, a) is derived from majority voting in collected noisy
sets R(s, a) for every state-action pair (s, a) ∈ S × A, which is a simple but efficient way of
leveraging the expectation of aggregated rewards without assumptions on prior distribution of noise.
In the following, we adopt standard Expectation-Maximization (EM) idea in the our estimation
framework (arguably a simple version of it), inspired by previous works (Zhang et al., 2014).
Assuming the observed noisy rewards are independent conditional on the true reward, we can com-
pute the posterior probability of true reward from the Bayes’ theorem:
P(	_R	l~(1) _ R	~(	) _ R _	P(r(1) =	Ri,…，r(n)	=	Rn|r	=	Ri)	∙ P(r	= Ri)
PCr = RiIr(I) = R1,…，r(n) = Rn)= Pj p(r(i)= Ri,…，r(n) = Rn∣r = Rj) ∙ P(r = Rj)
—	P(r = Ri) ∙Qn=ιP(r(k) = Rk|r = Ri)
=Pj [P(r = Rj) ∙ Qn=i P(r(k) = RkIr = Rj)]	()
where P(r = Rj) is the prior of true rewards, and P(r = Rk ∣r = Rj) is estimated by current
estimated confusion matrix C: P(r = Rk ∣r = Rj) = Cj,i. Note that the inference should be con-
ducted for each state-action pair (s, a) ∈ S ×A in every iteration, i.e., P(r(s, a) = Ri∣r(s, a, 1)=
Ri,…，r(s, a, n) = Rn), abbreviated as P(f(s, a) = Ri), which requires relatively greater com-
putation costs compared to the majority voting policy. It also points out an interesting direction to
check online EM algorithms for our perturbed-RL problem.
After the inference steps in Eqn. (8), the confusion matrix C is then updated based on the posterior
probabilities:
〜	=E(s,a) PTG a) = Ri) ∙ # [r(s, a) = Rj lr(s, a) = Ri]
Ci，j	P(s,a) P(r(s, a) = Ri) ∙ #[r(s, a) = Ri]
(9)
where P(f(s, a) = Ri) denotes the inference probabilities of true rewards based on collected noisy
rewards sets R(s, a). To utilize EM algorithms in the robust reward algorithms (e.g., Algorithm 3),
we need to replace Eqn. (4) by Eqn. (9) for the estimation of reward confusion matrix.
C.3 State-Dependent Perturbed Reward
In previous sections, to let our presentation stay focused, we consider the state-independent per-
turbed reward environments, which share the same confusion matrix for all states. In other words,
the noise for different states is generated within the same distribution. More generally, the gener-
ation of r follows a certain function C : S × R → R, where different states may correspond to
varied noise distributions (also varied confusion matrices). However, our algorithm is still applica-
ble except for maintaining different confusion matrices Cs for different states. It is worthy to notice
that Theorem 1 holds because the surrogate rewards produce an unbiased estimation of true rewards
for each state, i.e., Er∣r,st [r(st, at, st+i)] = r(st, at, st+i). Furthermore, Theorem 2 and 3 can be
revised as:
Theorem 4.	(Upper bound) Let r ∈ [0, Rmax] be bounded reward, Cs be invertible reward confu-
sion matrices with det(Cs) denoting its determinant. For an appropriate choice of m, the Phased
Q-Learning algorithm calls the generative model G(M)
O (_________SJAT_______________log ISIIAT)
"2(1-γ)2 m⅛3∈s{det(Cs)}2 g δ )
times in T epochs, and returns a policy such that for all state S ∈ S, ∖V∏ (s) 一 V *(s)I ≤ e, e > 0,
w.p. ≥ 1 - δ, 0 < δ < 1.
Theorem 5.	Let r ∈ [0, Rmax] be bounded reward and all confusion matrices Cs are invertible.
Then, the variance OfSurrOgate reward r is bounded asfollows:
M2
Var(r) ≤ Var(r) ≤ —--------J “ ∙ Rmax.
mins∈S{det(Cs)}2	max
20
Under review as a conference paper at ICLR 2019
C.4 Variance Reduction in Estimation
As illustrated in Theorem 3, our surrogate rewards introduce larger variance while conducting un-
biased estimation which are likely to decrease the stability of RL algorithms. Apart from the linear
combination idea (appropriate trade-off), some variance reduction techniques in statistics (e.g., cor-
related sampling) can also be applied into our surrogate rewards. Specially, Romoff et al. (2018)
proposed to a reward estimator to compensate for stochastic corrupted reward signals. It is worthy
to notice that their method is designed for variance reduction under stochastic (zero-mean) noise,
which is no longer efficacious in more general perturbed-reward setting. However, it is potential to
integrate their method with our robust-reward RL framework because surrogate rewards guarantee
unbiasedness in reward expectation.
To verify this idea, we repeated the experiments of Cartpole in Section 4.2 but included variance
reduction step for estimated surrogate rewards. Following Romoff et al. (2018), we adopted sample
mean as a simple approximator during the training and set sequence length as 100. As shown in Fig-
ure 5, the models with only variance reduction technique (red lines) suffer from huge biases when
the noise is large, and cannot converge to the optimal policies like those under noisy rewards. Nev-
ertheless, they benefits from variance reduction for surrogate rewards (purple lines), which achieve
faster convergence or better performance in many cases (e.g., Figure 5a (ω = 0.7), 5b (ω = 0.3)).
It is also not surprising that the integrated algorithm (purple lines) outperforms better as the noise
rate increases (indicating larger variance from Theorem 3, e.g., ω = 0.9). Similarly, Table 5 pro-
vides quantitative results which show that our surrogate benefits from variance reduction techniques
(“ours + VRT”), especially when the noise rate is large.
(a) Q-Learning
(b) CEM
(c) SARSA
(d) DQN
(e) DDQN
Figure 5: Learning curves from five reward robust RL algorithms (See Algorithm 3) on CartPole
game with true rewards (r) ■, noisy rewards ⑺(η = 1) ■, sample-mean noisy rewards (η = 1) ■,
estimated surrogate rewards (T) ■ and sample-mean estimated surrogate rewards ■. Note that
confusion matrices C are unknown to the agents here. From top to the bottom, the noise rates are
0.1, 0.3, 0.7 and 0.9. Here we repeated each experiment 10 times with different random seeds and
plotted 10% to 90% percentile area with its mean highlighted.
C.5 Experimental Results
To validate the effectiveness of robust reward algorithms (like Algorithm 3), where the noise rates
are unknown to the agents, we conduct extensive experiments in CartPole. It is worthwhile to
notice that the noisy rates are unknown in the explorations of RL agents. Besides, we discretize the
21
Under review as a conference paper at ICLR 2019
Table 5: Average scores of various RL algorithms on CartPole with sample-mean reward using
variance reduction technique (VRT), surrogate rewards (ours) and the combination of them (ours +
VRT). Note that the reward confusion matrices are unknown to the agents and the experiments are
repeated three times with different random seeds.
Noise Rate	I Reward	I Q-Learn	CEM	SARSA	DQN	DDQN
	VRT	173.5	99.7	167.3	181.9	187.4
ω=0.1	ours (r)	181.9	99.3	171.5	200.0	185.6
	ours + VRT	184.5	98.2	174.2	199.3	186.5
	VRT	140.4	439	ms	182.7	^^177.6
ω=0.3	ours (r)	161.1	81.8	159.6	186.7	200.0
	ours + VRT	161.6	82.2	159.8	188.4	198.2
	VRT	71.1	16.1	132	15.6	14.7
ω=0.7	ours (r)	172.1	83.0	174.4	189.3	191.3
	ours + VRT	182.3	79.5	178.9	195.9	194.2
observation (velocity, angle, etc.) to construct a set of states and implement like Algorithm 3. The η
is set 1.0 in the experiments.
Figure 6 provides learning curves from five algorithms with different kinds of rewards. The proposed
estimation algorithms successfully obtain the approximate confusion matrices, and are robust in the
unknown noise environments. From Figure 7, we can observe that the estimation of confusion
matrices converges very fast. The results are inspiring because we don’t assume any additional
knowledge about noise or true reward distribution in the implementation.
Figure 6:	Complete learning curves from five reward robust RL algorithms (see Algorithm 3) on
CartPole game with true rewards (r) ■, noisy rewards ⑺(η = 1) ■ and estimated surrogate
rewards (r) ■. Note that confusion matrices C are unknown to the agents here. From top to the
bottom, the noise rates are 0.1, 0.3, 0.7 and 0.9. Here we repeated each experiment 10 times with
different random seeds and plotted 10% to 90% percentile area with its mean highlighted.
22
Under review as a conference paper at ICLR 2019
(a) Q-Learning	(b) CEM
(c) SARSA	(d) DQN (e) Dueling-DQN
Figure 7:	Estimation analysis from five reward robust RL algorithms (see Algorithm 3) on CartPole
game. The upper figures are the convergence curves of estimated error rates (from 0.1 to 0.9), where
the solid and dashed lines are ground truth and estimation, respectively; The lower figures are the
absolute difference between the estimation and ground truth of confusion matrix C (normalized
matrix norm).
D Supplementary Experimental Results
D.1 Supplementary Quantitative Results
Table 6: Complete average scores of various RL algorithms on CartPole and Pendulum with noisy
rewards (r) and surrogate rewards under known (r) or estimated (T) confusion matrices. Note that
the results for last two algorithms DDPG (rand-one) & NAF (rand-all) are on Pendulum, but the
others are on CartPole. The experiments are repeated three times with different random seeds.
Noise Rate ∣ Reward ∣ Q-Learn				CEM	SARSA	DQN	DDQN DDPG		NAF
		r	170.0	98.1	165.2	187.2	187.8	-1.03	-4.48
ω	0.1	r	165.8	108.9	173.6	200.0	181.4	-0.87	-0.89
		r	181.9	99.3	171.5	200.0	185.6	-0.90	-1.13
		r	134.9	288	144.4	m4	168.6	-1.23	-4.52
ω	0.3	r	149.3	85.9	152.4	175.3	198.7	-1.03	-1.15
		r	161.1	81.8	159.6	186.7	200.0	-1.05	-1.36
		r	56.6	192	126	17.2	11.8	-8.76	-7.35
ω	0.7	r	177.6	87.1	151.4	185.8	195.2	-1.09	-2.26
		r	172.1	83.0	174.4	189.3	191.3	—	—
Table 7: Complete average scores of PPO on five selected Atari games with noisy rewards (r) and
surrogate rewards (r). The experiments are repeated three times with different random seeds.
Noise Rate	Reward	Lift (↑)	Mean	Alien	Carnival	Phoenix	MsPacman	Seaquest
e- = e+ = 0.1	r	—	2044.2	1814.8	1239.2	4608.9	1709.1	849.2
	r	67.5%↑	3423.1	1741.0	3630.3	7586.3	2547.3	1610.6
e- = 0.1, e+ = 0.3	r	—	770.5	893.3	841.8	250.7	1151.1	715.7
	r	20.3%↑	926.6	973.7	955.2	643.9	1307.1	753.1
e- = e+ = 0.3	r	—	1180.1	5431	919.8	2600.3	1109.6	727.8
	r	46.7%↑	1730.8	1637.7	966.1	4171.5	1470.2	408.6
e- = e+ = 0.7	r	—	296.8	485.4	380.3	126.5	491.5	0.0
	r	557.3%↑	1951.0	1799.2	1045.2	4970.4	1447.8	492.5
e- = 0.9, e+ = 0.7	r	—	382.6	410.2	67.0	174.1	620.1	641.7
	r	106.9%↑	791.5	693.5	918.1	298.9	1312.0	735.1
e- = e+ = 0.9	r	—	588.8	540.6	6.3	1410.8	535.4	588.8
	r	482.3%↑	3428.8	1901.3	4261.7	6758.6	2515.1	1707.1
23
Under review as a conference paper at ICLR 2019
D.2 Visualizations on Control Games
(a) DDPG (symmetric) (b) DDPG (rand-one) (c) DDPG (rand-all)
Figure 8: Complete learning curves from DDPG and NAF on Pendulum game with true rewards
(r) B, noisy rewards (r) ■ and surrogate rewards (^) (η = 1) ■. Both symmetric and asymmetric
noise are conduced in the experiments. From top to the bottom, the noise rates are 0.1, 0.3, 0.7 and
0.9, respectively. Here we repeated each experiment 6 times with different random seeds and plotted
10% to 90% percentile area with its mean highlighted.
2 4 6 8
- - - -
əpoMdə」9d PJeMg
2 4 6 8
- - - -
① PoMdə .iəd-≡eM ①」
(d) NAF (rand-all)
200	400	600
episode
D.3 Visualizations on Atari Games8
D.3.1 Pong
Asymmetric Noise (rand-one)
8For the clarity purpose, we remove the learning curves (blue ones in previous figures) with true rewards
except for Pong-v4 game.
24
Under review as a conference paper at ICLR 2019
w = 0.1
LV = 0.6	LV = 0.7	IV= 0.8
w = 0.9
Asymmetric Noise (rand-all)
D.3.2 AirRaid
sp-JraMBct:①pos-d山 sp-JraMBct:①pos-d山
D.3.3 Alien
25
Under review as a conference paper at ICLR 2019
D.3.4 Carnival
Oooo
Oooo
Oooo
8 6 4 2
SPJPMBa ①POMd山
Ooo
Ooo
Ooo
6 4 2
SP」raM①工①POMd山
D.3.5
MsPacman
D.3.6
Ooo
Ooo
Ooo
6 4 2
SPJBM ①=① POMd 山
Oo
8L
Ooo
Ooo
Ooo
6 4 2
SPJPMBa ①POMd山
O
2	3
Phoenix
5
le7
26
Under review as a conference paper at ICLR 2019
D.3.7 Seaquest
27