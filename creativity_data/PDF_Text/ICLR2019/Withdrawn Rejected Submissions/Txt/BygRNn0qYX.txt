Under review as a conference paper at ICLR 2019
P2IR: Universal Deep Node Representation via
Partial Permutation Invariant Set Functions
Anonymous authors
Paper under double-blind review
Ab stract
Graph node representation learning is a central problem in social network analysis,
aiming to learn the vector representation for each node in a graph. The key problem
is how to model the dependence of each node to its neighbor nodes since the
neighborhood can uniquely characterize a graph. Most existing approaches rely on
defining the specific neighborhood dependence as the computation mechanism of
representations, which may exclude important subtle structures within the graph and
dependence among neighbors. Instead, we propose a novel graph node embedding
method (namely P2IR) via developing a novel notion, namely partial permutation
invariant set function to learn those subtle structures. Our method can 1) learn
an arbitrary form of the representation function from the neighborhood, without
losing any potential dependence structures, 2) automatically decide the significance
of neighbors at different distances, and 3) be applicable to both homogeneous
and heterogeneous graph embedding, which may contain multiple types of nodes.
Theoretical guarantee for the representation capability of our method has been
proved for general homogeneous and heterogeneous graphs. Evaluation results on
benchmark data sets show that the proposed P2IR outperforms the state-of-the-art
approaches on producing node vectors for classification tasks.
1	Introduction
Graph node representation learning (or graph embedding in some literature) is to learn the numerical
representation of each node in a graph by vectors in a Euclidean space, where the geometric relation-
ship reflects the structure of the original graph. Nodes that are “close” in the graph are embedded
to have similar vector representations (Cai et al., 2017). The learned node vectors benefit a number
of graph analysis tasks, such as node classification (Bhagat et al., 2011), link prediction (Liben-
Nowell & Kleinberg, 2007), community detection (Fortunato, 2010), and many others (Hamilton
et al., 2017b). In order to preserve the node geometric relations in an embedded space, the similar-
ity/proximity/distance of a node to its neighborhood is generally taken as input to different graph
embedding approaches. For example, matrix-factorization approaches work on pre-defined pairwise
similarity measures (e.g., different order of adjacency matrix) (Zhang et al., 2018). Deepwalk (Perozzi
et al., 2014), node2vec (Grover & Leskovec, 2016) and other recent approaches (Dong et al., 2017)
consider flexible, stochastic measure of node similarity by the node co-occurrence on short random
walks over the graph (Goyal & Ferrara, 2017). Neighborhood autoencoder methods compress the
information about a node’s local neighborhood that is described as a neighborhood vector containing
the node’s pairwise similarity to all other nodes in the graph (Wang et al., 2016; Cao et al., 2016;
BojcheVski & Gunnemann, 2018). Neural network based approaches such as graph convolutional
networks (GCN) and GraphSAGE apply convolution like functions on its surrounding neighborhood
for aggregating neighborhood information (Kipf & Welling, 2016a; Hamilton et al., 2017a; Tu et al.,
2018).
However, most existing methods either explicitly or implicitly restrict the dependence form of each
node to its neighbors and also the depth of neighbors. Therefore, some important topology structures
within the graph and subtle dependence between neighbors may not be captured in the embedded
space. For example, the methods in the family of stochastic walk lose the control of influence of
further neighbors in a network since they usually utilize a fixed hyper-parameter as a window size, or
need to specify the degree of neighborhood proximity; GCN losses the flexibility of neighborhood
information since it fixes the two-layer aggregation function for the representation computation.
1
Under review as a conference paper at ICLR 2019
In this work, we propose a Partial Permutation Invariant Representation method (P2IR) by developing
a new notion of partial permutation invariant set function, that can
•	learn node representation via a universal graph embedding function f, without pre-defining
pairwise similarity, specifying random walk parameters, or choosing aggregation functions
among element-wise mean, a max-pooling neural network, or LSTMs;
•	capture the arbitrary relationship of each node to its neighbors;
•	automatically decide the significance of nodes at different distances;
•	be generally applied to any graphs from simple homogeneous graphs to heterogeneous
graphs with complicated types of nodes.
Evaluation results on benchmark data sets show that the proposed P2IR outperforms the state-of-the-
art approaches on producing node vectors for classification tasks.
2	Related Work
The main difference among various graph embedding methods lies in how they define the “closeness”
between two nodes (Cai et al., 2017). First-order proximity, second-order proximity or even high-order
proximity have been widely studied for capturing the structural relationship between nodes (Tang
et al., 2015b; Yang et al., 2017; Zhu et al., 2018). Comprehensive reviews of graph embedding have
been done by Cai et al. (2017); Hamilton et al. (2017b); Goyal & Ferrara (2017); Yang et al. (2017).
In this section, we discuss the relevant graph embedding approaches in terms of how node closeness
to neighboring nodes is measured, for highlighting our contribution on utilizing neighboring nodes in
a most general manner.
Matrix Analysis on Graph Embedding As early as 2011, a spectral clustering method (Tang &
Liu, 2011) was proposed to take the eigenvalue decomposition of a normalized Laplacian matrix of a
graph as an effective approach to obtain the embeddings of nodes. Other similar approaches choose
different similar matrix (from Laplacian matrix) to make a trade-off between modeling the “first-order
similarity” and “higher-order similarity” (Cao et al., 2015; Ou et al., 2016; Rossi et al., 2018). Node
content information can also be easily fused in the pairwise similarity measure, e.g., in TADW (Yang
et al., 2015), as well as node label information, which resulting in semi-supervised graph embedding
methods, e.g., MMDW (Tu et al., 2016). Recently, an arbitrary-order proximity preserved network
embedding method is introduced in (Zhang et al., 2018) based on matrix eigen-decomposition, which
is applied to a pre-defined high-order proximity matrix. For heterogeneous networks, Huang et al.
(2017) proposed a label involved matrix analysis to learn the classification result of each vertex within
a semi-supervised framework.
Random Walk on a Graph to Node Representation Learning Both deepwalk (Perozzi et al.,
2014) and node2vec (Grover & Leskovec, 2016) are outstanding graph embedding methods to solve
the node representation learning problem. They convert the graph structures into a sequential context
format with random walk (LoVgsz, 1993). Thanks to the invention of Mikolov et al. (2013) for
word representation learning of sentences, deepwalk inherited the learning framework for words
representation learning in paragraphs to generate the representation of nodes in random walk context.
And then node2vec evolved such the idea with additional hyper-parameters tuning for the trade-off
between DFS and WFS to control the direction of random walk. Struc2vec (Ribeiro et al., 2017)
also utilizes the multilayer graph to construct the graph node representations. Gao & Huang (2018)
propose a self-paced network embedding by introducing a dynamic negative sampling method to
select difficult negative context nodes in the training process. Planetoid (Yang et al., 2016) proposed
a semi-supervised learning framework by guiding random walk with available node label information.
The heterogeneity of graph nodes is often handled by a heterogeneous random walk procedure (Dong
et al., 2017), or selected relation pairs (Chang et al., 2015). Tang et al. (2015a) considered the
predictive text embedding problem on a large-scale heterogeneous text network and the proposed
method is also based on pre-defined heterogeneous random walks.
Neighborhood Encoders to Graph Embedding There are also methods focusing on aggregating
or encoding the neighbors’ information to generate node embeddings. DNGR (Cao et al., 2016) and
2
Under review as a conference paper at ICLR 2019
SDNE (Wang et al., 2016) introduce the autoencoder to construct the similarity function between the
neighborhood vectors and the embedding of the target node. DNGR defines neighborhood vectors
based on random walks and SDNE introduces adjacency matrix and Laplacian eigenmaps to the
definition of neighborhood vectors. GraphWave (Donnat et al., 2018) learns the representation of
each node’s neighborhood via leveraging heat wavelet diffusion patterns. Although the idea of
autoencoder is a great improvement, these methods are painful when the scale of the graph is up to
millions of nodes. Therefore, methods with neighborhood aggregation and convolutional encoders
are involved to construct a local aggregation for node embedding, such as GCN (Kipf & Welling,
2016a;b; Schlichtkrull et al., 2018; van den Berg et al., 2017), FastGCN (Chen et al., 2018), column
networks (Pham et al., 2017), the GraphSAGE algorithm (Hamilton et al., 2017a), GAT (Velickovic
et al., 2017) and a recent DRNE (Tu et al., 2018) method using layer normalized LSTM to approximate
the embedding of a target node by the aggregation of its neighbors’ embeddings. The main idea
of these methods is involving an iterative or recursive aggregation procedure e.g., convolutional
kernels or pooling procedures to generate the embedding vectors for all nodes and such aggregation
procedures are shared by all nodes in a graph.
The above-mentioned methods work differently on using neighboring nodes for node representation
learning. They require on pre-defining pairwise similarity measure between nodes, or specifying
random walk parameters, or choosing aggregation functions. In practice, it takes non-trivial effort to
tune these parameters or try different measures, especially when graphs are complicated with nodes
in multiple types, i.e., heterogeneous graphs. This work hence targets on making neighboring nodes
play their roles in a most general manner such that their contributions are learned but not user-defined.
The resultant embedding method has the flexibility to work on any types of homogeneous and
heterogeneous graph.
Our proposed method P2IR has a natural advantage on avoiding any manual manipulation of random
walking strategies or designs for the relationships between different types of nodes. To the invention
of set functions by Zaheer et al. (2017), all existing valid mapping strategies from neighborhood to
the target nodes can be represented by the set functions which are learnt by P2IR automatically.
3	The Proposed P2IR method
In this section, we first formally define the problem, followed by introducing a new notion - partial
permutation invariant function set function. This section ends up with the overall P2IR method.
3.1	Problem setup
We target on designing graph embedding models for the most general graph that may include K
different types of nodes (k=1 for homogeneous graphs). Formally, a graph G = {V, E}, where the
node set V = ∪kK=1Vk, i.e., V is composed of K disjoint types of nodes. One instance of such a graph
is the academic publication network, which includes different types of nodes for papers, publication
venues, author names, author affiliations, research domains etc. Given such a graph G, our goal is to
learn the embedding vector for each node in this graph.
3.2	The proposed universal graph embedding model
As we know, the position of a node in the embedded space is collaboratively determined by its
neighboring nodes. Therefore, the key issue to address for graph embedding is how to model the
dependence of each node to its neighbors. Most existing approaches need to pre-specify the neighbors
(usually via the distance on the graph), and either explicitly or implicitly define a specific form to
characterize the dependence between each node and its neighbors.
We propose a universal graph embedding model that neither requires to pre-define neighbors nor to
specify the dependence form between each node and its neighbors. The embedding vector xv of
node v ∈ Vk can be represented by its neighbors’ embedding vectors via a function f
Xv = f(X1, Xv,…，XK), ∀v ∈ Vk, ∀k ∈{1,2,..., K}
where Xkv is a matrix with column vectors corresponding to the embedding of node v’s neighbors in
type k. Note that the neighbors can include step-1 (or immediate) neighbors, step-2 neighbors, and
even further neighbors.
3
Under review as a conference paper at ICLR 2019
How to characterize an arbitrary dependence to neighbors? The key observation is that all
neighboring nodes reachable from a target node at the same step are not distinguishable from the
VieW of the target node. Therefore, function f (∙) should be a partially permutation invariantfunction.
That is, if we swap any elements in a Xkv , the function value remains the same. Unfortunately, the set
function is not simply learn-able because the permutation property is hard to guarantee directly.
One straightforWard idea to represent the partially permutation inVariant function is to define it in the
folloWing form
f(xv,…，XK)：=	χ X … X	τ(XvPi,…，XKPK)	(1)
P1∈P∣V1∣ P2∈P∣V2∣	PK∈p∣Vk|
Where Pn denotes the set of n-dimensional permutation matrices. X1vP1 is to permute the columns
in X1v . It is easy to Verify that the function defined in equation 1 is partially permutation inVariant,
but it is intractable because it inVolVes QkN=1(|Vk| !) “sum” items.
Our solution of learning function f is based on the folloWing important theorem, Which giVes a neat
Way to represent any partial permutation inVariant function. The proof is proVided in the Appendix.
Theorem 3.1. [Partial permutation invariant function representation theorem] Let f be a con-
tinuous real-valued function defined on a compact set X with the following form
/
} 1
}
|
}
|
\
f
xi,1,xi,2, ∙∙∙ ,Xl,Nι ,X2,1,X2,2, ∙∙∙ ,X2,N2 , ∙∙∙ ,XK,1,XK,2,…，XK,Nk
{z
G1
{z^
G2
{z^
GK
If function f is partially permutation invariant, that is, any permutations of the values within the
group Gk for any k does not change the function value, then there must exist functions h(∙) and
{gk (∙)}K=ι to approximate f with arbitrary precision in the following form
N1	N2	NK
h	g1(x1,n),	g2(x2,n), …，	gK (xK,n)	.	(2)
n=1	n=1	n=1
The rigorous proof is proVided in Appendix. This result essentially suggests a neat but uniVersal
Way to represent any partially permutation inVariant function. For instance, a popular permutation
invariant function widely used in deep learning max(∙) can be approximated in an arbitrary precision
by
max(x1 , x2 ,
, xN) ≈ h Xg(xi)
with g(z) = [exp(kz)z, exp(kz)], and h([z, z0]) = z/z0, as long as k is large enough. This is
because
XN-1 N
exp(kxi) I	eχP exp(kxi) ∙ xi.
i=1
Therefore, based on Theorem 3.1, we only need to parameterize h(∙) and {gk(∙)}3ι to learn the node
embedding function, which makes the partial permutation function finally learn-able and tractable.
To make equation 2 approximate an arbitrary partial permutation invariant function, we only need
to ensure that functions h and all g’s can represent (or approximate) arbitrary real functions. As
we know, deep neural network provides an efficient way to approximate an arbitrary continuous
real function, because 3-layer neural networks is able to approximate any function (Cybenko, 1989;
Hornik, 1991; Haykin, 1994). We next formulate the embedding model when considering different
order of neighborhood.
How to automatically learn the importance of neighbors at different distances?
(1-step neighbors) From Theorem 3.1, any mapping function of a node v ∈ Vk can be characterized
by appropriately defined functions ψ1,ψ2,…,ψκ, and φk
Xv =	φk	I X	ψi (Xu), X	ψ2	(Xu),…，X ψκ	(Xu))	∀v ∈ Vk,	Vk ∈{1,...,K},
∖u∈Ωv,ι	u三Ωv,2	u∈Ωv,κ	)
4
Under review as a conference paper at ICLR 2019
where。*,卜:=Ωn ∩ Vk denotes the step-n neighbors of node V in node type k.
(Multi-step neighbors) High order proximity has been shown to be beneficial on generating high
quality embedding vectors (Yang et al., 2017). Extending the 1-step neighbor model, we can have
the more general model where the representation of each node could depend on immediate (1-step)
neighbors, 2-step neighbors, 3-step neighbors, and even infinite-step neighbors.
(∞	∞	∞
X an X ψl (Xu) , X an	X ψ2 (Xu),…，X a” X	ψκ (Xu )1	⑶
n=0 u∈ΩV ι	n=0	u∈ΩV	2	n=0 u∈Ωv τ.
n,1	n,2	n,K
∀v ∈ Vk ,∀k ∈ {1,2,...,K}.
where ɑι, a2,…,a∞ are the weights for neighbors at different steps. Let A ∈ {0,1}lVl×lVl be
the adjacent matrix indicating all edges by 1. We involve the polynomial matrix function on the
adjacency matrix A to represent the structures of various steps neighborhood with tuning the weight
among 1-step to infinite-step neighbors automatically. If we define the polynomial matrix function
B(∙) on the adjacent matrix A as B(A) = P∞=0 anAn, we can cast equation 3 into its matrix form
x = φk (ψ1(XI)[B(A)]Vι,v, ψ2(X2HB(A)]V2,v,…，ψK (XK HB(A)]Vκ ,v )	(4)
∀v ∈ Vk ,∀k ∈ {1,2,...,K},
where Xk denotes the representation matrix for nodes in type k, [B(A)]Vk,v denotes the sub-matrix
of B(A) indexed by column V and rows in Vk, and function ψ (with 7 on the top of function ψ(∙)) is
defined as the function extension
Ψ(X):= [Ψ(X1),Ψ(X2),…，Ψ(xn)],
where ψ(∙) takes the output column vectors from a function ψ(∙) with each column of X to form
a new matrix with N columns. For a specific node V, we apply the scale |Vi,v | as N for the type i
neighbors of node V in the equation above. Therefore, with the polynomial matrix function B(A), and
multiple embedding vector mapping ψ(∙)s, we integrate the subtle structural knowledge of various
types and steps neighbors surrounding a node to compute the node representation. Note that the
embedding vectors for different types of nodes may be with different dimensions. Homogeneous
graph is a special case of the heterogeneous graph with K = 1. The above proposed model is thus
naturally usable on a homogeneous graph.
To avoid optimizing infinite number of coefficients, we propose to use a 1-dimensional NN function
ρ(∙) : R → R to equivalently represent the function B(∙) to reduce the number of parameters based
on the following observations
X∞∞
anσ% …,X anσN U> = Udiag (ρ(σι),…，ρ(σN)) U>,
n=0
where A = Udiag(σι, ∙∙∙ , qn)U> is the singular value decomposition (SVD). We parameterize
ρ : R 7→ R using 1-dimensional NN function, which allows us easily controlling the number of
variables to optimize in ρ by choosing the number of layers and the number of nodes in each layer.
Remark 3.2. Acute readers may worry about the computational complexity of SVD when the size
of A is large. In general, the full SVD takes the cubic computational complexity O(N 3) where
N is the total number of nodes in the network, which is pretty expensive if N is large. However,
since our adjacency matrix of the network is typically a very sparse matrix (the number of edges
is typically far less than O(N 2)), the complexity of the matrix decomposition can be reduced from
O(N 3) to O(N × #e) where #e is the number of edges in the network. Furthermore, we can use the
low rank approximation approach (that is, top k eigenvalues and eigenvectors) to approximate the
original large sparse matrix. Therefore, the approximated version of the matrix decomposition can
be completed in O(k × #e) complexity. We apply the low rank approximation in our experiments.
In particular, such approximation has been used in large datasets including Pubmed, DBLP, and
BlogCatalog. We also tested it on the small datasets including Cora, Citeseer, and Wiki by only using
top eigenvalues in the range of top 15% to 50% and did not found obvious drop on the performance.
5
Under review as a conference paper at ICLR 2019
3.3	The overall P2IR model
For short, we denote the representation function for xv in equation 4 by
xv =Rρ,φk,{ψk}kK=1(v) ∀v∈Vk ,∀k∈ {1,2,...,K}.
To fulfill the requirement of a specific learning task, we propose the complete P2IRmodel by involving
a supervised component
min
{xv}v∈V,ρ,
{φk}kK=1,{ψk}kK=
K 1	2	1
Xλk∣vk∣ X Ilx -Rp,φk,{ψk}K=ι(u)ll + JViabej X '(x ,y)	(5)
k=1	u∈Vk	v∈Vlabel
where Vlabel ⊂ V denotes the set of labeled nodes, and λk > 0 balances the representation error and
prediction error. The first unsupervised learning component restricts the representation error between
the target node and its neighbors with L2 norm since it is allowed to have noise in a practical graph.
And the supervised component is flexible to be replaced with any designed learning task on the
nodes in a graph. For example, to a regression problem, a least square loss can be chosen to replace
'(x, y) and a cross entropy loss can be used to formulate a classification learning problem. To solve
the problem in equation 5, we apply a stochastic gradient descent algorithm (SGD) to compute the
effective solutions for the learning variables simultaneously.
4	Experiments
This section reports experimental results to validate the proposed method, comparing to state-of-the-
art algorithms on benchmark datasets including both homogeneous and heterogeneous graphs, for
evaluating the applicability of our proposed method on embedding problem of general graphs.
4.1	Comparison on homogeneous graphs
We consider the multi-class classification problem over the homogeneous graphs. Given a graph with
partially labeled nodes, the goal is to learn the representation for each node for predicting the class
for unlabeled nodes.
Datasets We evaluate the performance of P2IR and methods for comparison on five datasets.
•	Cora (McCallum et al., 2000) is a paper citation network. Each node is a paper. There are 2708 papers
and 5429 citation links in total. Each paper is associated with one of 7 classes.
•	CiteSeer (Giles et al., 1998) is another paper citation network. CiteSeer contains 3,312 papers and
4,732 citations in total. All these papers have been classified into 6 classes.
•	Pubmed (Sen et al., 2008) is a larger and more complex citation networks compared to previous two
datasets. There are 19,717 vertexes and 88,651 citation links. Papers are classified into 3 classes.
•	Wikipedia (Sen et al., 2008) contains 2,405 online web-pages. The 17,981 links are undirected
between pairs of them. All these pages are from 17 categories.
•	Email-eu (Leskovec et al., 2007) is an Email communication network which illustrates the email
communication relationships between researchers in a large European research institution. There
are 1,005 researchers and 25,571 links between them. Department affiliations (42 in total) of the
researchers are considered as labels to predict.
Baseline methods The compared baseline algorithms are listed below:
•	Deepwalk (Perozzi et al., 2014) is an unsupervised graph embedding method which relies on the
random walk and word2vec method. For each vertex, we take 80 random walks with length 40, and set
window size as 10. Since deepwalk is unsupervised, we apply a logistic regression on the generated
embeddings for node classification.
•	Node2vec (Grover & Leskovec, 2016) is an improved graph embedding method based on deepwalk.
We set the window size as 10, the walk length as 80 and the number of walks for each node is set to
100. Similarly, the node2vec is unsupervised as well. We apply the same evaluation procedure on the
embeddings of node2vec as what we did for deepwalk.
6
Under review as a conference paper at ICLR 2019
•	Struc2vec (Ribeiro et al., 2017) chooses the window size as 10, the walking length as 80, the number
of walks from each node as 10, and 5 iterations in total for SGD.
•	GraphWave (Donnat et al., 2018) chooses the heat coefficient as 1000, the number of characteristic
functions as 50, the number of Chebyshev approximations as 100, and the number of steps as 20.
•	WYS (Watch-Your-Step) (Abu-El-Haija et al., 2017) chooses the learning rate as 0.2, the highest
power of normalized adjacency matrix as 5, the regularization co-efficient as 0.1, and uses the “Log
Graph Likelihood” as objective function.
•	MMDW (Tu et al., 2016) is a semi-supervised learning framework of graph embedding which
combines matrix decomposition and SVM classification. We tune the method multiple times and take
0.01 as the hyper-parameter η in the method which is recommended by the authors.
•	Planetoid (Yang et al., 2016) is a semi-supervised learning framework. We set the batch size as 200,
learning rate as 0.01, the batch size for label context loss as 200, and mute the node attributes as input
while using softmax for the model output.
•	GCN (Graph Convolutional Networks) (Kipf & Welling, 2016a) chooses the convolutional neural
networks into the semi-supervised embedding learning of graph. We eliminate the node attributes for
fairness as well.
•	GAT (Graph Attention Networks) (Velickovic et al., 2017) chooses the learning rate as 0.005, the
co-efficient of the regularization as 0.0005, and the number of hidden units as 64. To make the
comparison fair, we mute the node attributes in the training of GAT as well.
Experiment setup and results For fair comparison, the dimension of representation vectors is
chosen to be the same for all algorithms (the dimension is 64). The hyper-parameters are fine-tuned
for all of them. The details of P2IR for multi-class case are as follows.
•	Supervised Component: Softmax function is chosen to formulate our supervised component in
equation 5 which is defined as σ : RK → {z ∈ RK | PiK=1 zi = 1, zi > 0}. For an arbitrary
embedding X ∈ Rd, We have the probability term as P(y = j|x) = PKxpxwjwx++：.)for SUch node
to be predicted as class j, where (wj ∈ Rd, bj ∈ R) is a classifier for class j. Therefore, the whole
supervised component in equation 5 is - Pv∈V	PjK yj log P(y = j|xv) +λwR(w), Where R(w)
is an L2 regularization for w and λw is chosen to be 10-3.
•	Unsupervised embedding mapping Component: We design a tWo-layer NN With hidden dimension
64 to form the mapping from embeddings of neighbors to the target node and We also form a tWo-layer
1-to-1 NN With a 3 dimensional hidden layer to construct the matrix function for the adjacency matrix.
We pre-process the matrix A With an eigenvalue decomposition Where the procedure preserve the
highest 1000 eigenvalues in default. The balance hyper-parameter λ1 is set to be 10-3.
Table 1: Accuracy (%) of multi-class classification experiments in homogeneous graphs
	training%	10.00%	20.00%	30.00%	40.00%	50.00%	60.00%	70.00%	80.00%	90.00%
	deepwalk	72.73	75.50	76.72	77.34	78.38	78.41	78.82	79.15	79.70
		±2.02	士1.12	士0.99	士0.52	±0.82	±1.47	±1.12	±1.04	±2.32
E J O U	node2vec	73.46	75.70	76.45	78.07	78.42	78.65	78.94	79.08	81.04
		±0.87	士1.33	士1.11	士1.03	±1.14	±1.46	±1.43	±1.44	±0.99
	GraphWave	30.27	30.44	30.41	30.11	30.47	30.29	30.47	31.09	31.19
		±0.11	士0.29	士0.30	士1.02	±1.93	±0.59	±0.45	±0.73	±3.15
	struc2vec	26.89	28.47	29.69	29.53	30.06	30.01	30.34	31.09	31.19
		±0.71	士0.32	士0.39	士1.02	±2.01	±0.88	±0.59	±0.66	±3.09
	WYS	70.24	72.82	74.17	74.79	75.51	75.18	75.96	76.01	77.11
		±1.55	士0.99	士1.00	士0.30	±1.37	±0.92	±1.29	±1.04	±2.53
	MMDW	74.88	79.18	81.20	82.19	83.10	84.62	85.54	85.27	87.82
		±0.23*	士0.10*	士0.11*	士0.25*	±0.41*	±0.09*	±0.44*	±0.22*	±0.37*
	planetoid	40.87	52.63	52.23	52.07	53.06	51.86	51.91	51.29	54.32
		±15.61	士0.44	士1.47	士0.55	±1.99	±1.90	±1.09	±2.76	±3.48
	GCN	59.10	67.08	73.06	75.61	79.08	80.11	82.61	84.29	85.18
		±1.48	士1.98	士1.07	士1.02	±1.23	±1.15	±1.00	±1.88	±1.66
	GAT	29.57	30.49	29.91	29.85	30.44	30.22	30.53	30.74	30.55
		士2.11	士0.67	士0.58	±0.81	±0.78	±1.73	±0.79	±2.33	±1.41
	P2IR	76.28	80.10	82.95	83.99	85.13	85.95	86.87	87.84	88.59
		士1.35	±1.36	±1.44	±1.14	±0.71	±0.81	±0.97	±0.35	±1.58
	deepwalk	-4652-	-49.18-	-4928-	-50.41 ^^	-5043-	-5079-	-4963-	-5006-	-5233^
		士1.45	士0.49	士0.72	±0.87	±1.27	±0.99	±0.75	±1.89	±0.89
Citeseer	node2vec	46.38	47.65	48.68	48.12	48.37	48.13	47.67	48.55	48.82
		士1.32	士0.88	士0.71	±0.64	±1.14	±0.98	±1.38	±1.11	±2.46
	GraphWave	18.03	18.03	17.96	18.40	18.14	17.67	17.76	18.13	19.58
		士0.30	士0.28	士0.17	±0.46	±0.70	±1.06	±0.35	±1.91	±1.92
7
Under review as a conference paper at ICLR 2019
pəuIqn^
gpədpnM
n9,I-sUIω
struc2vec
WYS
MMDW
planetoid
GCN
GAT
P2IR
deepwalk
node2vec
GraphWave
struc2vec
WYS
MMDW
planetoid
GCN
GAT
P2IR
deepwalk
node2vec
GraphWave
struc2vec
WYS
MMDW
planetoid
GCN
GAT
P2IR
17.72	17.73	17.79	17.91	18.16	17.52	17.76	18.10	18.79
±0.35	±0.34	±0.44	±0.72	±0.88	±1.07	±0.41	±2.10	±1.85
44.23	44.14	44.77	44.38	43.84	44.15	43.63	44.32	46.59
±1.56	±0.42	±1.15	±1.13	±1.31	±1.26	±1.53	±1.75	±1.44
55.36	60.98	62.00	63.89	66.59	69.00	69.72 *	70.40	70.64
±0.60	±0.56	±0.17	±0.12	±0.27	尸 ±0.15	±0.82	±0.93	±0.31
38.44	41.12	40.67	38.59	39.32	37.96	39.07	36.98	36.75
±0.89	±0.98	±1.42	±1.50	±2.13	±1.60	±0.72	±1.59	±3.32
39.78	47.89	53.85	57.92	62.00	64.74	67.59	68.04	73.65
±0.34	±1.21	±1.38	±1.21	±0.72	±1.09	±1.32	±1.56	±1.03
20.45	21.82	23.21	22.50	25.26	23.76	25.19	24.32	25.83
±0.60	±3.02	±4.34	±3.48	±5.25	±5.38	±8.41	±6.32	±7.68
53∙98*	60.34*	63.55	66.34	69.41	71.63	73.11	72.99	77.16
±1.64	±0.81	±0.97	±0.63	±1.10	±0.92	±1.82	±1.47	±1.82
75.51 ^^	-75.84-	-76.00-	-76.15-	-76.13-	-7622-	-76.05-	-7670-	-7603^
±0.34	±0.20	±0.19	±0.22	±0.38	±0.26	±0.37	±0.83	±1.08
76.17 *	76.46 *	76.41 .	76.67 *	76.96 *	76.82 .	76.69 *	77.41 *	76.85 .
±0.26*	±0.26*	±0.17	±0.31*	±0.48*	±0.42	±0.54*	±0.96*	±1.29
38.80	39.22	38.87	39.17	39.31	39.17	39.21	39.35	38.77
±0.66	±0.90	±0.31	±0.35	±0.44	±0.43	±0.39	±0.46	±0.79
- 75.09	- 75.48	- 75.52	- 75.82	- 75.88	- 75.81	- 75.72	- 76.28	- 75.42
±0.47	±0.29	±0.32	±0.30	±0.60	±0.55	±0.50	±1.01	±1.14
68.57	66.18	67.39	68.95	71.84	71.44	53.65	69.11	50.90
±1.01	±4.13	±5.02	±1.53	±2.93	±2.33	±14.69	±6.45	±16.62
40.27	40.48	40.30	40.33	40.33	40.79	40.66	40.43	40.92
±0.50	±0.23	±0.50	±0.23	±0.49	±0.55	±0.48	±0.31	±1.35
57.17	60.92	63.70	65.76	66.96	67.80	68.06	69.39	70.24
±0.44	±0.70	±0.33	±0.35	±0.43	±0.65	±0.61	±0.30	±0.95
41.62	43.44	44.47	41.99	46.64	47.05	45.45	48.68	40.97
士4.26	±4.72	±6.04	±4.75	±8.10	±6.85	±6.86	±6.78	±2.55
76.91	78.50	79.38	80.26	80.54	81.28	81.58	81.99	82.17
士0.73	±0.40	±0.23	±0.27	±0.52	±0.46	±0.37	±0.71	±0.60
45.82-	-51.22-	-52.18-	-54.05-	-54.98-	-54.45-	-5637-	-5634-	-5700^
±1.05	±1.05	±0.89	±1.13	±0.79	±1.46	±2.19	±2.11	±2.91
45.04	49.70	52.17	52.45	53.76	53.41	54.87	55.93	53.83
±0.74	±0.82	±0.98	±1.80	±0.85	±1.26	±1.98	±1.64	±4.16
4.73	5.04	5.03	5.49	5.34	5.11	5.58	5.16	6.00
±0.38	±0.53	±0.43	±0.32	±0.69	±0.37	±0.49	±0.90	±1.83
11.42	12.64	11.78	12.31	12.61	12.45	12.43	12.39	12.50
±1.39	±1.08	±0.75	±1.44	±0.62	±0.81	±1.68	±1.64	±2.24
45.24	48.25	49.64	49.49	50.08	49.75	50.29	51.52	50.17
±1.70	±1.04	±0.92	±1.55	±1.03	±1.15	±2.54	±1.20	±2.69
53.05 .	59.45	62.85 .	62.42	64.26	66.46	67.50	67.37	70.20
±0.54	±0.35*	±0.55	±0.07	±1.09	±0.85	±0.48	±0.56	±1.22
49.78	51.44	51.61	50.35	50.32	49.89	51.00	50.50	52.03
±1.81	±0.93	±0.55	±1.38	±0.59	±2.02	±1.89	±3.01	±2.60
52.93	59.12	62.63	66.21	67.46	69.00	69.74 *	71.85	72.58
±1.91	±1.16	±1.27	±0.59	±1.07	±1.40	±2.28	±0.67	±3.79
16.83	15.70	16.30	16.84	15.84	15.47	15.90	17.18	15.52
±0.98	±2.50	±1.01	±0.90	±1.99	±3.60	±1.84	±2.26	±2.73
56.02	63.96	65.92	68.99	69.80	71.27	71.57	73.81	74.33
士1.00	±0.62	±0.59	±0.80	±0.80	±0.89	±2.59	±1.81	±2.85
deepwalk	28.87	42.59	48.02	52.07	56.81	57.26	60.00	59.80	58.80
	±2.40	±2.95	±2.10	±0.71	±1.72	±1.10	±2.44	±2.55	±5.07
node2vec	29.82	43.93	48.42	52.90	56.81	56.87	59.38	59.50	60.00
	±1.79	±3.72	±2.03	±0.78	±2.08	±1.87	±3.17	±2.32	±6.44
GraphWave	6.50	6.64	6.69	6.14	6.57	6.37	6.71	7.26	7.80
	±0.21	±0.47	±0.52	±0.59	±0.45	±0.72	±2.12	±2.27	±1.48
struc2vec	6.79	6.94	6.83	6.24	6.85	6.37	6.71	7.66	7.80
	±0.33	±0.52	±0.41	±0.46	±0.36	±0.62	±2.12	±2.24	±1.30
WYS	46.84	53.11	54.68	56.25	61.24	60.10	59.73	60.50	60.60
	±1.79	±2.30	±3.12	±1.02	±3.32	±2.13	±3.49	±1.52	±5.37
MMDW	36.76	40.72	43.22	43.01	46.11	44.94	48.08	53.62	65.50
	±0.90	±2.54	±0.61	±1.52	±1.23	±0.38	±1.21	±0.83	±1.34
planetoid	53.50	61.26	62.75	65.19	67.02	66.41	67.51	68.44	69.06
	±2.60	±1.13	±2.39	±1.09	±1.72	±2.46	±2.14	±2.90	±3.95
GCN	58.94	64.93	69.13	70.65	72.87 -ɪ`	74.13 *	75.35	76.12	77.60
	±3.57	±2.33	±1.38	±2.13	±1.55	±2.96	±1.87	±1.99	±3.51
GAT	10.71	9.47	10.04	9.40	10.26	10.37	11.46	12.18	14.26
	±0.21	±2.39	±1.89	±2.37	±0.73	±1.22	±2.13	±2.15	±6.63
P2IR	63.05	67.19	71.81	73.23	76.33	75.57	76.41	77.81	79.00
	±5.74	±3.39	±2.39	±2.73	±1.56	±0.96	±3.46	±2.35	±4.53
We take experiments on each data set and compare the performance among all baseline methods
mentioned above. Since it is the multi-class classification scenario, we use Accuracy as the evaluation
criterion. The percentage of labeled samples is chosen from 10% to 90% and the remaining samples
8
Under review as a conference paper at ICLR 2019
are used for evaluation. All experiments are repeated for five times and we report the mean and
standard deviation of their performance in the Table 1 . We highlight the best performance for each
dataset with bold font style and the second best results with a “*”. We can observe that in most cases,
our method outperforms other methods.
4.2	Comparison on heterogeneous graphs
We next conduct evaluation on heterogeneous graphs, where learned node embedding vectors are
used for multi-label classification.
Datasets The used datasets include
•	BlogCatalog (Wang et al., 2010) is a social media network with 55,814 users and according to the
interests of users, they are classified into multiple overlapped groups. We take the five largest groups
to evaluate the performance of methods. Users and tags are two types of nodes. The 5,413 tags are
generated by users with their blogs as keywords. Therefore, tags are shared with different users and
also have connections since some tags are generated from the same blogs. The number of edges
between users, between tags and between users and tags are about 1.4M, 619K and 343K respectively.
•	DBLP (Ji et al., 2010) is an academic community network. Here we obtain a subset of the large
network with two types of nodes, authors and key words from authors’ publications. The generated
subgraph includes 27K (authors) + 3.7K (key words) vertexes. The link between a pair of author
indicates the coauthor relationships, and the link between an author and a word means the word
belongs to at least one publication of this author. There are 66,832 edges between pairs of authors and
338,210 edges between authors and words. Each node can have multiple labels out of four in total.
Baseline Methods To illustrate the validation of the performance ofP2IR on heterogeneous graphs,
we conduct the experiments on two stages: (1) comparing P2IR with Deepwalk (Perozzi et al.,
2014) and node2vec (Grover & Leskovec, 2016) on the graphs by treating all nodes as the same
type (P2IR with K = 1 in a homogeneous setting); (2) comparing P2IR with the state-of-the-art
heterogeneous graph embedding method, metapath2vec (Dong et al., 2017), in a heterogeneous
setting. The hyper-parameters of the method are fine-tuned and metapath2vec++ is chosen as the
option for the comparison.
Experiment Setup and Results For fair comparison, the dimension of representation vectors is
chosen to be the same for all algorithms (the dimension is 64). We fine-tune the hyper-parameter for
all of them. The details of P2IR for multi-label case are as follows.
•	Supervised Component: Since it is a multi-label classification problem, each label can be treated
as a binary classification problem. Therefore, we apply logistic regression for each label and for
an arbitrary instance x and the i-th label yi, the supervised component is formulated as l(x, yi) =
log(1 + exp(wi> x +bi)) -yi(wi>x+bi), where (wi ∈ Rd, bi ∈ R) is the classifier for the i-th label.
Therefore, the supervised component in equation 5 is defined as Pv∈V	Pi l(x, yi) + λw R(wi)
and R(wi) is the regularization component for wi, where λw is chosen to be 10-4.
•	Unsupervised Embedding Mapping Component: We design a two-layes NN with a 64-dimensional
hidden layer for each type of nodes with the types of nodes in its neighborhood to formulate the
mapping from embedding of neighbors to the embedding of the target node. We also form a two-layer
1-to-1 NN wth a 3 dimensional hidden layer to construct the matrix function for the adjacency matrix
A for the whole graph. We pre-process the matrix A with an eigenvalue decomposition by preserving
the highest 1000 eigenvalues of magnitude in default. We denote the nodes to be classified as type 1
and the other type as type 2. The balance hyper-parameter [λ1, λ2] is set to be [0.2, 200].
For the datasets DBLP and BlogCatalog, we carry out the experiments on each of them and compare
the performance among all methods mentioned above. Since it is a multi-label classification task, we
take f1-score(macro, micro) as the evaluation score for the comparison. The percentage of labeled
samples is chosen from 10% to 90%, while the remaining samples are used for evaluation. We repeat
all experiments for three times and report the mean and standard deviation of their performance in the
Table 2. We can observe that in most cases, P2IR in heterogeneous setting has the best performance.
P2IR in homogeneous setting is better than deepwalk and node2vec in the same homogeneous setting,
and is even better than metapath2vec++ in heterogeneous setting (achieving the second best results).
Overall, the superior performance ofP2IR in Table 1 and 2 demonstrates the validity of our proposed
universal graph embedding mechanism.
9
Under review as a conference paper at ICLR 2019
)orcam( golataCgolB )orcim( golataCgolB )orcam( PLBD
training%	10.00%	20.00%	30.00%	40.00%	50.00%	60.00%	70.00%	80.00%	90.00%
Table 2: F1-score (macro, micro) (%) of multi-label classification in heterogeneous graphs
deepwalk	45.13	44.64	44.52	44.64	44.32	44.36	44.78	44.33	44.49
	±0.68	±0.21	士0.42	士0.23	士0.21	士0.46	士0.48	士0.62	士0.86
node2vec	45.78	45.42	45.28	45.41	45.17	45.19	45.57	45.04	44.96
	±0.68	±0.30	士0.32	士0.18	士0.20	士0.36	士0.13	士0.31	士0.55
metapath2vec++	37.46	36.72	36.69	36.58	36.74	36.90	36.89	36.42	36.16
	±0.61	±0.36	士0.29	士0.28	士0.17	士0.33	士0.32	士0.41	士0.98
P2IR	47.63 *	50.99 *	51.70 .	50.04 .	50.60 .	50.34 .	52.20 .	51.88 .	51.36 .
(Homogeneous)	±3.16	士0.09	士0.19	士1.90	士1.73	士0.55	士1.11	士1.08	士0.26
P2IR	49.65	51.47	52.69	53.37	53.73	53.97	53.83	54.07	53.36
(Heterogeneous)	士0.63	士0.40	士0.24	士0.45	士0.01	±0.43	±0.62	士0.40	士0.84
deepwalk	47.93	47.36	47.25	47.30	47.07	47.09	47.39	47.02	47.27
	士0.48	士0.15	士0.45	士0.19	士0.16	士0.48	士0.49	士0.72	士0.82
node2vec	48.52	48.20	48.01	48.18	47.97	48.04	48.22	47.83	47.95
	士0.50	士0.18	士0.31	士0.17	士0.25	士0.39	士0.15	士0.32	士0.55
metapath2vec++	40.90	40.06	40.10	39.97	40.04	40.22	40.21	39.82	39.66
	士0.34	士0.21	士0.25	士0.22	士0.17	士0.29	士0.22	士0.44	士0.82
P2IR	50.75	54.26	55.08	53.41	53.88	53.57	55.36	54.93	55.03
(Homogeneous)	士2.74	士0.12	士0.08	±1.92	士1.70	士0.50	士1.30	士1.07	士0.52
P2IR	53.06	54.77	55.93	56.41	56.86	57.28	57.13	57.43	56.98
(Heterogeneous)	±0.45	±0.21	±0.17	±0.31	±0.06	±0.38	±0.43	士0.14	±0.53
Deepwalk	74.48	74.87	74.95	75.10	75.07	75.44	75.33	74.75	75.36
	士0.34*	士0.14	士0.15	士0.22	士0.22	士0.22	士0.33	士0.31	士0.73
Node2vec	73.37	73.94	74.00	74.25	74.06	74.52	74.52	74.32	74.55
	士0.24	士0.11	士0.18	士0.23	士0.31	士0.21	士0.35	士0.26	士0.57
Metapath2vec++	74.82	75.27	75.55	75.63	75.53	75.92	75.92	75.56	76.09
	士0.30	士0.08	士0.12	士0.27	士0.22	士0.24	士0.42	士0.36	士0.46
P2IR	72.51	76.89	79.91	82.14	84.60	86.34	87.49	88.21	89.61
(Homogeneous)	士0.91	士0.06	士0.09	±0.26	士0.42	±0.52	士0.31	±0.25	士0.58
P2IR	74.06	78.43	81.00	83.18	84.95	86.91	88.30	89.18	90.37
(Heterogeneous)	士0.37	±0.61	±0.19	±0.16	±0.22	±0.54	±0.29	±0.17	±0.38
)orcim( PLB
deepwalk	76.65	77.03	77.15	77.21	77.20	77.60	77.44	76.87	77.54
	士0.25	士0.19	士0.15	士0.15	士0.17	士0.20	士0.31	士0.35	士0.72
node2vec	75.65	76.21	76.30	76.48	76.33	76.82	76.76	76.44	76.73
	士0.16	士0.12	士0.16	士0.18	士0.25	士0.22	士0.33	士0.34	士0.55
metapath2vec++	76.98	77.38	77.66	77.70	77.61	78.04	77.95	77.54	78.02
	士0.21*	士0.10	士0.08	士0.21	士0.18	士0.18	士0.39	士0.36	士0.47
P2IR	74.37	78.52	81.47	83.56	85.81	87.51	88.44	89.16	90.54
(Homogeneous)	士0.88	士0.09	士0.11	士0.28	士0.42	士0.54	±0.27	±0.22	士0.50
P2IR	77.06	80.67	82.87	84.75	86.29	88.09	89.27	90.11	91.22
(Heterogeneous)	±0.29	±0.45	±0.13	±0.16	±0.22	±0.47	±0.25	±0.13	±0.43
5 Conclusion and Future Work
To summarize the whole work, we propose P2IR, a general graph embedding solution with the
novel notion of partial permutation invariant set function, in principle that it can capture an arbitrary
dependence between neighbors and automatically decide the significance of neighbor nodes at
different distance for both homogeneous and heterogeneous graphs. We provide a theoretical
guarantee for the effectiveness of the whole model. Through conducting extensive experimental
evaluation, we show P2IR has better performance on both homogeneous and heterogeneous graphs,
comparing to the stochastic trajectories based, matrix analytics based and graph neural network based
state-of-the-art algorithms. For the future work, our model can be extended to more general cases,
e.g., involving the rich content information out of graph neighborhood structures.
10
Under review as a conference paper at ICLR 2019
References
Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alex Alemi. Watch your step: Learning
graph embeddings through attention. arXiv preprint arXiv:1710.09599, 2017.
Smriti Bhagat, Graham Cormode, and S Muthukrishnan. Node classification in social networks. In
Social network data analytics, pp. 115-148. Springer, 2011.
Aleksandar Bojchevski and Stephan Gunnemann. Deep gaussian embedding of graphs: Unsupervised
inductive learning via ranking. In International Conference on Learning Representations (ICLR),
2018.
HongYun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph
embedding: Problems, techniques and applications. CoRR, abs/1709.07604, 2017.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global
structural information. In Proceedings of the 24th ACM International on Conference on Information
and Knowledge Management, CIKM ’15, pp. 891-900, 2015. ISBN 978-1-4503-3794-6.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph representations.
In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, pp. 1145-
1152, 2016.
Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and Thomas S Huang.
Heterogeneous network embedding via deep architectures. In Proceedings of the 21st ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 119-128. ACM,
2015.
Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks via
importance sampling. In International Conference on Learning Representations (ICLR), 2018.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation
learning for heterogeneous networks. In KDD ’17, pp. 135-144. ACM, 2017.
Claire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning structural node embed-
dings via diffusion wavelets. 2018.
Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75-174, 2010.
Hongchang Gao and Heng Huang. Self-paced network embedding. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’18, pp.
1406-1415, 2018.
Clyde Lee Giles, Kurt Dewitt Bollacker, and Steve Lawrence. Citeseer: An automatic citation
indexing system. In Proceedings of the third ACM conference on Digital libraries, pp. 89-98.
ACM, 1998.
Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications, and performance: A
survey. CoRR, abs/1705.02801, 2017.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 855-864. ACM, 2016.
William Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In NIPS, pp. 1024-1034. 2017a.
William Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and
applications. CoRR, abs/1709.05584, 2017b.
Simon Haykin. Neural networks: a comprehensive foundation. Prentice Hall PTR, 1994.
11
Under review as a conference paper at ICLR 2019
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):
251-257,1991.
Xiao Huang, Jundong Li, and Xia Hu. Label informed attributed network embedding. In Proceedings
of the Tenth ACM International Conference on Web Search and Data Mining, pp. 731-739. ACM,
2017.
Ming Ji, Yizhou Sun, Marina Danilevsky, Jiawei Han, and Jing Gao. Graph regularized transductive
classification on heterogeneous information networks. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 570-586. Springer, 2010.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016a.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308,
2016b.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densification and shrinking
diameters. ACM Transactions on Knowledge Discovery from Data (TKDD), 1(1):2, 2007.
David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. journal of
the Association for Information Science and Technology, 58(7):1019-1031, 2007.
Lgszl6 Lovdsz. Random walks on graphs. Combinatorics, Paul erdos is eighty, 2(1-46):4, 1993.
Ian Grant Macdonald. Symmetric functions and Hall polynomials. Oxford university press, 1998.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the
construction of internet portals with machine learning. Information Retrieval, 3(2):127-163, 2000.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems, pp. 3111-3119, 2013.
Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. Asymmetric transitivity preserving
graph embedding. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’16, pp. 1105-1114, 2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of the 20th ACM SIGKDD international Conference on Knowledge Discovery
and Data Mining, pp. 701-710. ACM, 2014.
Trang Pham, Truyen Tran, Dinh Q Phung, and Svetha Venkatesh. Column networks for collective
classification. In AAAI, pp. 2485-2491, 2017.
Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node
representations from structural identity. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 385-394. ACM, 2017.
Ryan A Rossi, Nesreen K Ahmed, and Eunyee Koh. Higher-order network representation learning.
In Companion of the The Web Conference 2018 on The Web Conference 2018, pp. 3-4, 2018.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In European Semantic Web
Conference, pp. 593-607. Springer, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93, 2008.
Richard P Stanley. Enumerative combinatorics, volume 2. 2001.
Marshall H Stone. Applications of the theory of boolean rings to general topology. Transactions of
the American Mathematical Society, 41(3):375-481, 1937.
12
Under review as a conference paper at ICLR 2019
Marshall H Stone. The generalized weierstrass approximation theorem. Mathematics Magazine, 21
(5):237-254,1948.
Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale hetero-
geneous text networks. In Proceedings of the 21st ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 1165-1174. ACM, 2015a.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proceedings of the 24th International Conference on World
Wide Web, pp. 1067-1077, 2015b.
Lei Tang and Huan Liu. Leveraging social media networks for classification. Data Mining and
Knowledge Discovery, 23(3):447-478, 2011.
Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and Maosong Sun. Max-margin deepwalk: Discrimina-
tive learning of network representation. In IJCAI, pp. 3889-3895, 2016.
Ke Tu, Peng Cui, Xiao Wang, Philip S. Yu, and Wenwu Zhu. Deep recursive network embedding
with regular equivalence. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’18, pp. 2357-2366, 2018.
Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix completion.
stat, 1050:7, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 1(2), 2017.
Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD
’16, pp. 1225-1234, 2016.
Xufei Wang, Lei Tang, Huiji Gao, and Huan Liu. Discovering overlapping groups in social media. In
the 10th IEEE International Conference on Data Mining series (ICDM2010), Sydney, Australia,
December 14- 17 2010.
Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation
learning with rich text information. In IJCAI, pp. 2111-2117, 2015.
Cheng Yang, Maosong Sun, Zhiyuan Liu, and Cunchao Tu. Fast network embedding enhancement
via high order proximity approximation. In Proceedings of the Twenty-Sixth International Joint
Conference on Artificial Intelligence, IJCAI, pp. 19-25, 2017.
Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. In Proceedings of the 33rd International Conference on International
Conference on Machine Learning - Volume 48, ICML’16, pp. 40-48. JMLR.org, 2016.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp.
3394-3404, 2017.
Ziwei Zhang, Peng Cui, Xiao Wang, Jian Pei, Xuanrong Yao, and Wenwu Zhu. Arbitrary-order
proximity preserved network embedding. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD ’18, pp. 2778-2786, 2018.
Dingyuan Zhu, Peng Cui, Daixin Wang, and Wenwu Zhu. Deep variational network embedding
in wasserstein space. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’18, pp. 2827-2836, 2018.
13
Under review as a conference paper at ICLR 2019
Appendix
We provide the proof to Theorem 3.1 in the appendix. The proof in our paper borrows some idea
from the proof sketch for a special case of our theorem in (Zaheer et al., 2017). While (Zaheer et al.,
2017) only provides a small paragraph to explain the raw idea on how to prove a special case, we
provide complete and rigorous proof and nontrivial extension to the more general case.
Definition 5.1 (Power Sum Symmetric Polynomials). For every integer k ≥ 0, the k-th power sum
symmetric polynomial in variables x1,x2, ∙∙∙ ,Xn is defined as
Pk (X1,X2 ,…，Xn)= Exk .
i=1
Definition 5.2 (Monomial Symmetric Polynomials). For λ = [λ1, λ2, . . . , λn]> ∈ Rn where
λι ≥ λ2 ≥ ∙∙∙ ≥ λn ≥ 0, let Ωλ be the Set of all permutations of the entries in λ. For λ, the
monomial symmetric polynomials is defined as
mλ = X	x：1 xλ2 …xnn
[λ1,…,λn]>∈Qλ
Lemma 5.3. If f : X → R is a continuous real-valued function defined on the compact set X ⊂ Rn,
then ∀ > 0, there exists a polynomial function p : X → R, such that supx∈X |f (X) - p(X)| < .
Proof. This lemma is a direct application of Stone-Weierstrass Theorem (Stone, 1937; 1948) for the
compact HaUsdorff space.	□
Corollary 5.3.1. For any continuous real-valued function f : X → R of the form defined in
Theorem 3.1,
} I
f (x1,1, x1,2, ∙ ∙
'---------------
G1
,X1,Ni ,X2,1,X2,2, ∙∙∙ ,X2,N2 , ∙∙∙ ,XK,1,XK,2, ♦…
, XK,NK ),
G2
GK
there exist polynomial functions p : X → R also permutation invariant within each Gk to closely
approximate f.
Proof. Let Sk be the set of all possible permutations of {1,…，N} for the group Gk, where
k ∈ {1, 2, . . . , K}. SUppose σk = [σ1k, σ2k, . . . , σNk ] ∈ Sk is some permUtation for Gk. Then we
know that ∀σk ∈ Sk with k ∈ {1, 2, . . . , K}, we have
f (xi, X2,…，XK ) = f([xi]σi, [x2]σ2,…，[xκ ]σK )
where we let Xk to denote [χk,ι, Xk,2,…，Xk,Nk] for Gk and [xk]σ-k to denote the permuted Xk for
simplicity. There are in total QkK=1(Nk!) permutations.
By lemma 5.3, we can know that ∀ > 0, there exists a polynomial function q : X → R such
that sup |f (xι, X2,…，XK) - q(xι, X2,…，XK)| ≤ e. This further implies that ∀σk ∈ Sk with
k ∈ {1, 2, . . . , K}, we have
SUP ∣f([Xl]σi, [X2]σ2,…，[XK ]σK ) - q([Xl]σi,住2必2 ,…，[XK ]σK )| ≤ E
14
Under review as a conference paper at ICLR 2019
Welet p(xι, x2,…，XK) = QKINPσi∈sι,…,σK∈SK q([xι]σi, [x2]σ2,…，[xκ]σK) and by
the property of permutation invariant within each Gk of the function f, we have that
SuP |f (xi, X2, •…，XK ) - p(xi, X2, •…，XK )|
SuP f(X1, X2,…
,XK) - TrK	E	q([xl]σ1 ,[χ2]σ2 , ….,[xK]σK )
IIk = I(Nk !) σ1∈Sl ,…，σK ∈SK
SuPIrrK INn	X	f ([χ1]σ1，^^^，[XK ]σK ) - FrK	X	Q([xl]σ1 ,….，[xK ]σ
IIk=I(Nk !) σ1 ∈S1,…，σK ∈SK	IIk = I(Nk ・) σ1∈Sl ,…，σκ ∈SK
K)||
1
≤~~r7------
- QK=i(Nk!)
1
≤-w------
-QK=ι(Nk!)
X	SuP |f ([xi]σ1 , [X2]σ2 , ∙∙∙ , [XK ]σK ) - q([xi]σ1 ,[况2 ]σ2 ,…。，[XK ]σK )|
σ1∈S1,…，σK ∈SK
X
σ1∈S1,…，σK ∈SK
Therefore, We can see that the polynomial function p(xι, x2,…，XK) is permutation invariant
within each group and can closely approximate the function f (xι, x2,…,XK).
□
Lemma 5.4. Suppose p : X → R is a real-valued polynomial in the form
}
p(x
I
1,1,X1,2, ∙∙∙ ,Xl,Nι, ∙∙∙ ,XK,1,XK,2,…，XKN ),
{z^
G1
}I
{z^
GK
which is permutation invariant within each group Gk, k ∈ {1, 2, . . . , K}. Then, p can be represented
as
}
p(x
I
}I
1,1,x1,2, ∙∙ ∙ , x1,Nι,…，xK,1,xK,2,…，xK,Nκ
{z^
G1
^{z
GK
where ci is the rational coefficient, mkλK denotes a monomial symmetric polynomial of variables
xk,ι ,xk,2,…,Xk,Nk within group Gk and λK ∈ Rn are some certain exponents for the monomial
symmetric polynomial.
Proof. Suppose that the polynomial is expressed as a summation of monomials
N1 αr N2 αr	NK αr
P(X1,1,x1,2,…，x1,Nι,,…,χK,1,xK,2,…，xK,NK) = Ecr ɪɪ xι j,j ɪɪ x2jj ,…,U XKKjj
I	、，	} I	、，	}	-- 二 二	二
^{z
G1
^{z
GK
r j=1 j=1
j=1
where αrk,j is the exponent of xk,j for the r-th monomial.
rr	r
We consider the term c『Qj=I X]j,j Qj=I X2jj …Qj=KI XKjj for a certain r in the above
summation. Since p(∙) is partially symmetric within the group Gι, then there must exist
terms having exponents of permuted αr1,j, ∀j ∈ {1, . . . , N1} on X1,j while the other factors
r
Qj=K1 xKK,j,j remain the same. (Otherwise, the polynomial is not permutation-
invariant w.r.t. group G1.) By summing up those terms together, we get a term with a factor of
monomial symmetric polynomial as cmf1 QN=I Xajj …QNKI XKjj. Based on this term, we
consider that the polynomial p(∙) is also permuted invariant within G2, which implies that there must
exist terms having exponents of permuted αr2,j, ∀j ∈ {1, . . . , N2} on X2,j while the other factors
crm；1 QN=ι Xajj … QNKL XKjj remain the same. Therefore, adding up all those terms together,
we have crmf1 mj2 QN=ι Xajj …QNKI XCKjj. Carrying out the above procedures recursively,
αr αr	αr
we can eventually have crm11 m22 ... mκκ. For all the remaining terms in the polynomial p(∙),
performing the same steps will lead to completion of our proof.
□
15
Under review as a conference paper at ICLR 2019
Example 5.5. p(x11, x21, x12, x22) is a polynomial that is permutation invariant within each group,
G1 = {X11, X12} and G2 = {X12, X22}. We let
P(X1,1, X1,2, X2,1, X2,2) =X1,1X12,2X2,1X22,2 + X21,1X1,2X22,1X2,2 + X1,1X12,2X22,1X2,2 + X21,1X1,2X2,1X22,2
2334	3243	2343	3234
+ X1,1X1,2X2,1X2,2 + X1,1X1,2X2,1X2,2 + X1,1X1,2X2,1X2,2 + X1,1X1,2X2,1X2,2.
It is easy to observe thatP(X1,1, X1,2, X2,1, X2,2) can be rewritten as
P(X1,1, X1,2, X2,1, X2,2) =(X1,1X21,2 + X12,1X1,2)(X2,1X22,2 + X22,1X2,2)
+ (X21,1X31,2 + X31,1X21,2)(X32,1X42,2 + X24,1X23,2)
(2,1)	(2,1)	(3,2)	(4,3)
= m1	m2	+ m1	m2	.
Lemma 5.6. For a symmetric polynomial P(X1, . . . , Xn) of n variables, it can be expressed by a
polynomial in the power sum symmetric polynomials Pk(X1, . . . , Xn) for 1 ≤ k ≤ n with rational
coefficients.
Proof. This lemma is a direct result of the fact thatP1,P2, . . . ,Pn are algebraically independent and
the ring of symmetric polynomials with rational coefficients can be generated as a Q-algebra, i.e.
Q[P1,P2, . . . ,Pn]. (Macdonald, 1998; Stanley, 2001) This lemma can also be easily proved by the
combination of the fundamental theorem of symmetric polynomials and newton's identities. □
Proof of Theorem 3.1
Proof. By Corollary 5.3.1, for any function f : X → R that are permutation-invariant w.r.t. the
variables within each group Gk, ∀k ∈ {1, 2, . . . , K}, which is in the form
/ Y
}
f (x1,1, χ1,2, ∙ ∙
X-------------
G1
,Xl,Nι, X2,1, X2,2, ∙∙∙ , X2,N2 , ∙∙∙ ,XK,1,XK,2,…，XK,Nk ),
{-^
G2
}X
{-^
GK
we can always find a polynomial function P : X → R that are also permutation-invariant w.r.t. the
variables within each group Gk to approximate f closely in a given small error tolerance .
Here we first define the function gk : R → RNk in the following form,
gk(Xk,n)
Xk,n
X
Nk
k,n
which thus leads to
I
,n ,n ,n
k, 2k, 3k,
XXX
1 1 1.
Nkn=Nkn=Nkn=
nn n
PPP
Pι(xk,ι,…，Xk,N)
P2(Xk,1,…，Xk,Nk )
P3(xk,1,…，xk,Nk )
_PNk (Xk,1,…，xk,N ).
=
)
,n
Xk,
k
g
NkX n=1
Therefore, we generate a sequence of power sums basis by PnN=k 1 gk (xk,n).
ByLemma 5.4, the polynomial function p(x1,1, x1,2,…，xi,nl ,…，xκ,1,xκ,2,…，xk,Nk) Can
be expressed as Pi=1 ci m1λ1 m2λ2 . . . mλKK. Note that each mkλk is a symmetric polynomial, which
thus can be rewritten as a polynomial expression of power sum basis,
PI(Xk,1, …，xk,Nk ),P2(xk,1, ∙ ∙ ∙ ,xk,Nk ),…，PNk (xk,1,.∙. ,xk,Nk ).
which has been generated by PnN=k 1 gk (xk,n).
16
Under review as a conference paper at ICLR 2019
Hence the polynomial p(χι,ι, χ1,2, ∙∙∙ , xi,nl, ∙∙∙ , χκ,ι, xk,2, ∙∙∙ , xk,NK) is also a function of
PnN=k 1 gk(xk,n), ∀k ∈ {1, 2, . . . , K}, which will be expressed as
N1	N2	NK
P(X1,1,χ1,2,…,χ1,Nι,…,χK,1,χK,2,…,xK,Nκ ) = h(E gl(χ1,n),y^g2(χ2,n), …,£ gK (χK,n )).
n=1	n=1	n=1
Thus, the function f could be approximate in any given error tolerance by a polynomial
h(PN= 1 g1(x1,n), PN= 1 g2(x2,n),…，PNKI gK(xκ,n)), WhiCh finishes our proof.
□
17