Under review as a conference paper at ICLR 2019
PIE: Pseudo-Invertible Encoder
Anonymous authors
Paper under double-blind review
Ab stract
We consider the problem of information compression from high dimensional data.
Where many studies consider the problem of compression by non-invertible trans-
formations, we emphasize the importance of invertible compression. We introduce
new class of likelihood-based Auto-Encoders with pseudo bijective architecture,
which we call Pseudo Invertible Encoders. We provide the theoretical explanation
of their principles. We evaluate Gaussian Pseudo Invertible Encoder on MNIST,
where our model outperforms WAE and VAE in sharpness of the generated im-
ages.
1	Introduction
We consider the problem of information compression from high dimensional data. Where many
studies consider the problem of compression by non-invertible transformations, we emphasize the
importance of invertible compression as there are many cases where one cannot or will not decide
a priori what part of the information is important and what part is not. Compression of images for
person ID in a small company requires less resolution then person ID at an airport. To loose part
of the information without harm to the future purpose of viewing the picture requires knowing the
purpose upfront. Therefore, the fundamental advantage of invertible information compression is that
compression can be undone if a future purpose so requires.
Recent advances of classification models have demonstrated that deep learning architectures of
proper design do not lead to information loss while still being able to achieve state-of-the-art in
classification performance. These i-RevNet models Jacobsen et al. (2018) implement a small but es-
sential modification of the popular RevNet models while achieving invertibility and a performance
similar to the standard RevNet Gomez et al. (2017). This is of great interest as it contradicts the
intuition that information loss is essential to achieve good performance in classification Tishby &
Zaslavsky (2015). Despite the requirement of the invertibility, flow-based generating models Dinh
et al. (2014; 2016); Rezende & Mohamed (2015); Kingma & Dhariwal (2018) demonstrate that the
combination of bijective mappings allows one to transform the raw distribution of the input data to
any desired distribution and perform the manipulation of the data.
On the other hand, Auto-Encoders have provided the ideal mechanism to reduce the data to the
bare minimum while retaining all essential information for a specific task, the one implemented
in the loss function. Variational Auto Encoders (VAE) Kingma & Welling (2013) and Wasserstein
Auto Encoders (WAE) Tolstikhin et al. (2018) are performing best. They provide an approach for
stable training of autoencoders, which demonstrate good results at reconstruction and generation.
However, both of these methods involve the optimization of the objective defined on the pixel level.
We would emphasise the importance of avoiding the separate decoder part and training the model
without relying on the reconstuction quality directly.
Combining the best of Invertible mappings and Auto-Encoders, we introduce Pseudo Invertible En-
coder. Our model combines bijectives with restriction and extension of the mappings to the depen-
dent sub-manifolds Fig. 1. The main contributions of this paper are the following:
•	We introduce new class of likelihood-based Auto-Encoders, which we call Pseudo Invert-
ible Encoders. We provide the theoretical explanation of their principles.
•	We demonstrate the properties of Gaussian Pseudo Invertible Encoder in manifold learning.
•	We compare our model with WAE and VAE on MNIST, and report that the sharpness of
the images, generated by our models is better.
1
Under review as a conference paper at ICLR 2019
mapping F projection & restriction
mapping F~1 extension
Figure 1: Schematic representation of the proposed mechanism of dimensionality reduction.
2	Related Work
2.1	Invertible models
ResNets He et al. (2016) enable Networks to grow even more and thus memory consumption be-
comes a bottleneck. Gomez et al. (2017) propose a Reversible Residual Network (RevNet) where
each layer’s activations can be reconstructed from the activations of the next layer. By replacing
the residual blocks with coupling layers, they mimic the behaviour of residual blocks while being
able to retrieve the original input of the layer. RevNet replaces the residual blocks of ResNets, but
also accommodates non-invertible components to train more efficiently. By adding a downsampling
operator to the coupling layer, i-RevNet circumvents these non-invertible modules (Jacobsen et al.,
2018). With this they show that losing information is not a necessary condition to learn representa-
tions that generalize well on complicated problems. Although i-RevNet circumvents non-invertible
modules, data is not compressed and the model is only invertible up to the last layer. All their meth-
ods do not allow dimensionality reduction. In current research we build a pseudo invertible model
which performs dimensionality reduction.
2.2	Autoencoders
Auto-Encoders were first introduced by Rummerhart (1986) as an unsupervised learning algorithm.
They are now widely used as a technique for dimension reduction by compressing input data. By
training an encoder and a decoder network, and measuring the distance between original and re-
constructed data, data can be represented in a latent space. This latent space can then be used for
supervised learning algorithms. Instead of learning a compressed representation of the input data
Kingma & Welling (2013) propose to learn the parameters of a probability distribution that repre-
sent the data. tol introduced new class of models - Wasserstein Auto Encoders, which use Optimal
Transport to be trained. These methods require the optimization of the objective function which
includes the terms defined on pixel level. Our model does not require such optimization. Moreover,
it only perform encoding at training time.
3	Theory
Here we introduce the approach for obtaining dimensionality reduction invertible mappings. Our
method is based on the restriction of the mappings to low-dimensional manifolds, and extension of
the inverse mappings with certain constraints (Fig. 2).
3.1	Restriction-Extension Approach
Given data xi ∈ X ⊂ RD . Assuming that X is a d-dimensional manifold, with d < D, we seek
to find a mapping G : RD → Rd invertible on X . In other words, we are looking for a pair of
2
Under review as a conference paper at ICLR 2019
Figure 2: The schematic representation of the Restriction-Extension approach. The invertible map-
Ping X - Z is preformed by using the dependent sub-manifold R = g(Z) and a pair extended
functions G, G-1.
associated functions G and G-1 such that
G(X) = Z ⊂ Rd
G-1(Z) = X
(1)
Let R be an open set in RD-d. We use this residual manifold in order to match the dimensionalities
of the hidden and initial spaces. Here we introduce the function g : Rd → RD-d. With no loss of
generality We can say that R = g(Z). We use the pair of extended functions G : RD → Rd X RD-d
and GT : Rd × RD-d → RD to rewrite Eq. 1:
(G(X) = Z×R
(G-1(Z ×R) = X
(2)
Rather than searching for the invertible dimensionality reduction mapping directly, we seek to find
G, the invertible transformation with certain constraints, expressed by R.
In search for G, we focus on Fθ : RD → RD, Fθ ∈ F, where F is a parametric family of functions
invertible on RD . We select the function Fθ with parameters θ which satisfy the constraint:
Fθ-1 ◦ PRd×R ◦ Fθ =idX	(3)
where PRd×R is the orthogonal projection from Rd × RD-d to Rd × R.
Taking into account constraint 3, we derive Fθ(x) = [z, r], where z ∈ Z and r ∈ R. By combining
this with Eq. 2 we have the desired pair of functions:
G(x) = z,
G-1(z) =Fθ-1([z,g(z)])
(4)
The obtained function G is Pseudo Invertible Endocer, or shortly PIE.
3.2 Log Likelihood Maximization
As we are interested in high dimensional data such as images, the explicit choice of parameters θ is
impossible. We choose θ* as a maximizer of the log likelihood of the observed data given the prior
pθ(x):
θ* = arg max[log Pe (x)]
θ
(5)
After a change of variables according to Eq. 4 we obtain
p(x) = p(Fθ(x)) det
(6)
3
Under review as a conference paper at ICLR 2019
(a) General Flow
(b) Multi-scale RealNVP
(c) PIE
Figure 3: Schematic representation of three types of bijective mappings currently used in normal-
izing flows. The circles represent the variables. The basic invertible mappings are depicted with
blue edges. Green edges represent the aggregation of the variables in objective function. In general
normalizing flow (a) all the variables are mapped in the same manner and are propagated through
the same number of flows. The multi-scale architecture used in RealNVP (b) transform different
variables with different number of flows and afterwards map them to the same distribution. Our
model (c) progressively discards part of the variables by hardly constraining their distributions.
Taking into account the constraint 3 we derive the joint distribution for Fθ (x) = [z, r]
p(Fθ(x)) = p(z, r) = p(r|z)p(z)
p(Fθ (x))dx =	p(r|z)p(z)drdz =	δ(r - g(z))p(z)drdz
(7)
(8)
X
R=g(Z) Z
RZ
p(Fθ(x)) = δ(r - g(z))p(z)	(9)
Dirac’s delta function can be viewed as a limit of sequence of Gaussians:
δ(x) = lim N (x|0, 2I)	(10)
Let us fix 2 = 02	1. Then
δ(x) ≈ N(x|0,02I)	(11)
δ(r - g(z)) ≈ N(r|g(z),02I)	(12)
Finally, for the log likelihood we have:
log P(X) ≈ log p(z) + log N (r|g (z), ∈01) + log ∣ det ∂χTττ) ∖	(13)
We choose prior distribution p(z) as Standard Gaussian. We search for the parameters by using
Gradient Descent.
3.3	Composition of Bijectives
The method relies on the function Fθ . This choice is challenging by itself. The currently known
classes of real-value bijectives are limited. To overcome this issue, we approximate Fθ with a
composition of basic bijectives from certain classes:
F = FK ◦ Fk-1 ◦ . . . ◦ F2 ◦ F1	(14)
where Fj = Fj (∙∣θj) ∈ Fj, j = 1... K.
Taking into account that a composition of PIE is also PIE, we create a final dimensionality reduction
mapping from a sequence of PIEs:
X o Yi o Y2 —…—Yl o Zi
(15)
4
Under review as a conference paper at ICLR 2019
[	)Downsampling	[ j 1 × 1 Conv, Linear	[ j Coupling
(]Split method
Figure 4: Architecture of the Pseudo-Invertible Encoder. PIE consists of convolutional and linear
blocks which can be repeated multiple times, as denoted by the three dots between the block structure
at the bottom. Each block has Kl repetitions of coupling layers and 1 × 1 convolutions.
such that
D > dim Y1 > dim Y2 > . . . > dim YL > d
(16)
where L < D - d.
Then the log likelihood is represented as
L	L Ki
log P(X) ≈ log P(Z) + X log N (rι∣gι(zι), e2I) + XX log | det(Jki)∣	(17)
l=1	l=1 k=1
where Jkl is the Jacobian of the k-th function of the l-th PIE. The approximation error here depends
only on e, according to the Eq. 10. For the simplicity we will now refer to the whole model as PIE.
The building blocks of this model are PIE blocks.
3.4	Relation to Normalizing Flows
If We choose the distribution p(z) in Eq. 17 as Standard Gaussian, gι(∙) = 0, ∀l and e0 = 1, then
the model can be viewed as Normalizing Flow with multi-scale architecture Dinh et al. (2016) Fig.
3	. It was demonstrated in Dinh et al. (2016) that the model with such architecture achieves semantic
compression.
4	Pseudo-Invertible Encoder
This section introduces the basic bijectives for the Pseudo-Invertible Encoder (PIE). We explain
what each building bijective consists of and how it fits in the global architecture as shown in Fig. 4.
4.1	Architecture
PIE is composed ofa series of convolutional blocks followed by linear blocks, as depicted in Fig. 4.
The convolutional PIE blocks consist of series of coupling layers and 1×1 convolutions. We perform
invertible downsampling of the image at the beginning of the convolutional block, by reducing the
spatial resolution and increasing the number of channels, keeping the overall number of the variables
the same. At the end of the convolutional PIE block, the split of variables is performed. One part
of the variables is projected to the residual manifold R while others is feed to the next block. The
linear PIE blocks are constructed in the same manner. However, the downsampling is not performed
and 1 × 1 convolutions are replaced invertible linear mappings.
5
Under review as a conference paper at ICLR 2019
(a) Forward
(b) Inverse
Figure 5: Structure of a coupling block. P partitions the input into two groups of equal length.
U unites these group together. In the inverse P -1 and U-1 are the reverse of these operations
respectively.
4.2	Coupling layer
In order to enhance the flexibility of the model, we utilize affine coupling layers Fig. 5. We modify
the version, introduced in Dinh et al. (2016).
Given input data x, the output y is obtained by using the mapping:
y1 = s1(x2) x1 + b1(x2)
y2 = s2(y1) x2 +b2(y1)
x2 = (y2 - b2(y1))/s2(y1)
x1 = (y1 - b1(x2))/s1(x2)
(18)
^⇒
Here multiplication and division are performed element-wise. The scalings s1 , s2 and the biases
b1, b2 are the functions, parametrized with neural networks. The invertibility is not required for this
functions. x1, x2 are the non-intersecting partitions of x. For convolutional blocks we partition the
tensors by splitting them into halves along the channels. In case of the linear blocks, we just split
the features into halves.
The log determinant of the Jacobian of coupling layer is given by:
log det
(∂Fθ )1 =
sum(log |s1|) + sum(log |s2|)
where log | ∙ | is calculated element-wise.
4.3	INVERTIBLE 1 × 1 CONVOLUTION AND LINEAR TRANSFORMATION
The affine couplings operate on non-intersecting parts of the tensor. In order to capture the various
correlations between channels and features, the different mechanism of channel permutations were
proposed. Kingma & Dhariwal (2018) demonstrated that invertible 1 × 1 convolutions perform better
than fixed permutations and reversing of the order of channels Dinh et al. (2016).
We parametrize Invertible 1 × 1 Convolutions and invertible linear mappings with Householder
Matrices Householder (1958). Given the vector v, the Householder Matrix is computed as:
T
H(V)= I - 2丁	(19)
vTv
6
Under review as a conference paper at ICLR 2019
x
δ(r - g(z))
g(z) ÷
(a) Forward
(b) Inverse
z
Figure 6: Structure of the split method. P partitions the input into two sub samples. P -1 unites
these sub samples together.
The obtained matrix is orthogonal. Therefore, its inverse is just its transpose, which makes the
computation of the inverse easier comparing to Kingma & Dhariwal (2018). The log determinant of
the Jacobian of such transformation is equal to 0.
4.4	Downsampling
We use invertible downsampling to progressively reduce the spatial size of the tensor and increase
the number of its channels. The downsampling with the checkerboard patterns Jacobsen et al.
(2018); Dinh et al. (2016) transforms the tensor of size C X H XW into a tensor of size 4C X H X 号,
where H, W are the height and the width of the image, and C is the number of the channels. The
log determinant of the Jacobian of Downsampling is 0 as it just performs permutation.
4.5	Split
All the discussed blocks transform the data while preserving its dimensionality. Here we introduce
Split block Fig. 6, which is responsible for the projection, restrictions and extension, described
in Section 3. It reduces the dimensionality of the data by splitting the variables into two non-
intersecting parts z, r of dimensionalities d and D - d, respectively. z is kept and is to be processed
by the subsequent blocks. r is constrained to match N(r|g(z), 20I). The mappings is defined as
z = x|Rd
r → N(r|g(z),20I)
Q⇒ X = [z,g(z)]
(20)
5	Experiments
5.1	Manifold learning
For this experiment we trained a Gaussian PIE on the MNIST digits dataset. We build PIE with
2 convolutional blocks, each splitting the data in the last layer to 50% of the input size. Next
we add three linear blocks to PIE, reducing the dimensions to 64, 10 and the last block does not
reduce the dimensions any further. For each affine transformation we use the three biggest possible
Householder reflections. For this experiment we set Kl equal to 3. Optimization is done with the
Adam optimizer Kingma & Ba (2014). The model diminishes the number of dimensions from R784
to R10.
This experiment shows the ability of PIE to learn a manifold with three different constraints; 2 =
0.01, 2 = 0.1 and 2 = 1.0. The results are shown in Fig. 7. As the constraint gets to loose, as
shown in the right column, the model is not able to reconstruct anymore (Fig. 7a). Lower values for
2 perform better in terms of reconstruction. Too low values, however, sample fuzzy images (Fig.
7b). Narrowing down the distribution to sample from increases the models probability to produce
accurate images. This is shown in Fig. 7c where samples are taken from N (0, 0.5). For both
2 = 0.01 and 2 = 0.1 reconstructed images are more accurate.
7
Under review as a conference paper at ICLR 2019
(a)
(b)
4 q
ʒ S
二 7
/ L
O 5
9
Oo
7
U
S
0/234
3733夕
S b7 2 q
3 3 3
歹G O O
/ S O 5
iLo i
7 6 / I
SGC夕
3 9 7L
a ' 8 ?
/ Au 0 K
e6 h g
K Pg7
(c)
8β3 377
3 g , 677
,，g N m
8 Cc .
7，g 8”
OBOP
OODO
Λv
OO夕O
g 9 8 0

(d)
(e)
2 = 0.01
IlILqqqqqH
夕夕夕歹夕VgQG66
DDDDQqqqqqq
7777777 — 99
6600。09?夕夕夕
2 = 0.1
夕夕，夕,9 88868
夕夕夕夕/夕夕夕夕9。
88880999999
3333习习习习习33
gaaesgggp夕夕
2 = 1.0
Figure 7: Experiment on MNIST dataset with dim(z) = 10. (a) shows reconstructions on test
data. Row 1 and 3 are original images, row 2 and 4 are reconstructions from z-space. (b) and (c)
both show reconstruction of a samples z-space. (b) is sampled from N(0, 1), (c) is sampled from
N(0, 0.5). (d) is a linear interpolation between a picture on the left and the right of the image. All
digits shown are reconstructed from z-space. At last (e) shows UMAP from dim(z) = 10.
8
Under review as a conference paper at ICLR 2019
	Sharpness
True	-0.18
VAE	0.08
WAE	0.07
PIE	0.49
Table 1: Results for experiment on sharpness on three different models and original images. For
all three models a sample of 8 dimensions was taken. The generated images where convolved with
Laplace filter and then the variance of activations was averaged over 10000 samples images. Higher
values are better.
Fig. 7d shows for each model the linear interpolation from one latent space to another. Both lower
values of 2 (0.01, 0.1) show digits that are quite accurate. When the constraint is loosened to
= 1.0 the interpolation is unable to show distinct values.
This experiment shows that tightening the constraint by decreasing 2 increases the power of the
manifold learned by the model. This is shown again in Fig. 7e where we diminished the number
of dimensions even further from R10 to R2 utilizing UMAP (McInnes et al., 2018). With 2 = 1.0
UMAP created a manifold with a good Gaussian distribution. However, from the manifold created
by PIE it was not able to separate distinct digits from each other. Tightening the constraint with a
lower 2 moves the manifold created by UMAP further away from a Gaussian distribution, while it
is better able to separate classes from each other.
5.2	Image sharpness
It is a well-known problem in VAEs that generated images are smoothened. WAE Tolstikhin et al.
(2018) improves over VAEs by utilizing Wasserstein distance function. To test the sharpness of
generated images we convolve the grey-scaled images with the Laplace filter. This filter acts as an
edge detector. We compute the variance of the activations and average them over 10000 sampled
images. If an image is blurry, it means there are less edges and thus more activations will be close
to zero, leading to a smaller variance. In this experiment we compare the sharpness of images
generated by PIE with WAE, VAE and the sharpeness of the original images. For VAE and WAE
we take the architecture as described in Radford et al. (2015). For PIE we take the architecture as
described in section 5.1.
Table 1 shows the results for this experiment. PIE outperforms both VAE and WAE in terms of
sharpeness of generated images. Images generated by PIE are even more sharp then original images
from the MNIST dataset. An explanation for this is the use of a checkerboard pattern in the down-
sampling layer of the PIE convolutional block. With this technique we capture intrinsic properties
of the data and are thus able to reconstruct sharper images.
6	Conclusion
In this paper we have proposed the new class of Auto Encoders, which we call Pseudo Invertible
Encoder. We provided a theory which bridges the gap between Auto Encoders and Normalizing
Flows. The experiments demonstrate that the proposed model learns the manifold structure and
generates sharp images.
7	Acknowledgements
One of the authors is sponsored by STW project “Imagine: in search for the unknown”.
References
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components esti-
mation. CoRR, abs/1410.8516, 2014. URL http://arxiv.org/abs/1410.8516.
9
Under review as a conference paper at ICLR 2019
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. CoRR,
abs/1605.08803, 2016. URL http://arxiv.org/abs/1605.08803.
Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net-
work: Backpropagation without storing activations. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 30, pp. 2214-2224. Curran Associates, Inc., 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Alston S. Householder. Unitary triangularization of a nonsymmetric matrix. J. ACM, 5(4):339-
342, October 1958. ISSN 0004-5411. doi: 10.1145/320941.320947. URL http://doi.acm.
org/10.1145/320941.320947.
Jorn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks.
In ICLR 2018-International Conference on Learning Representations, 2018.
D. P. Kingma and P. Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. ArXiv
e-prints, July 2018.
D. P Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints, December 2013.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Leland McInnes, John Healy, Nathaniel Saul, and LUkaS GroBberger. Umap: uniform manifold
approximation and projection. The Journal of Open Source Software, 3(29):861, 2018.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015. URL http://
arxiv.org/abs/1511.06434.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional Conference on Machine Learning, pp. 1530-1538, 2015.
D. E. Rummerhart. Learning internal representations by error propagation. Parallel Distributed
Processing: I. Foundations, pp. 318-362, 1986. URL https://ci.nii.ac.jp/naid/
10009703828/en/.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
Information Theory Workshop (ITW), 2015 IEEE, pp. 1-5. IEEE, 2015.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=HkL7n1- 0b.
10