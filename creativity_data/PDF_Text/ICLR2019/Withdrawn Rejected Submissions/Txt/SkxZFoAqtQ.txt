Under review as a conference paper at ICLR 2019
Improving Composition of Sentence Embed-
dings through the Lens of Statistical Rela-
tional Learning
Anonymous authors
Paper under double-blind review
Ab stract
Various NLP problems - such as the prediction of sentence similarity, entailment,
and discourse relations - are all instances of the same general task: the modeling
of semantic relations between a pair of textual elements. We call them textual
relational problems. A popular model for textual relational problems is to embed
sentences into fixed size vectors and use composition functions (e.g. difference or
concatenation) of those vectors as features for the prediction. Meanwhile, com-
position of embeddings has been a main focus within the field of Statistical Rela-
tional Learning (SRL) whose goal is to predict relations between entities (typically
from knowledge base triples). In this work, we show that textual relational mod-
els implicitly use compositions from baseline SRL models. We show that such
compositions are not expressive enough for several tasks (e.g. natural language
inference). We build on recent SRL models to address textual relational problems,
showing that they are more expressive, and can alleviate issues from simpler com-
positions. The resulting models significantly improve the state of the art in both
transferable sentence representation learning and relation prediction.
1	Introduction
Predicting relations between textual units is a widespread task, essential for discourse analysis, dia-
log systems, information retrieval, or paraphrase detection. Since relation prediction often requires a
form of understanding, it can also be used as a proxy to learn transferable sentence representations.
Several tasks that are useful to build sentence representations are derived directly from text struc-
ture, without human annotation: sentence order prediction (Logeswaran et al., 2016; Jernite et al.,
2017), the prediction of previous and subsequent sentences (Kiros et al., 2015; Jernite et al., 2017),
or the prediction of explicit discourse markers between sentence pairs (Nie et al., 2017; Jernite et al.,
2017). Human labeled relations between sentences can also be used for that purpose, e.g. inferential
relations (Conneau et al., 2017). While most work on sentence similarity estimation, entailment de-
tection, answer selection, or discourse relation prediction seemingly uses task-specific models, they
all involve predicting whether a relation R holds between two sentences s1 and s2 . This generic-
ity has been noticed in the literature before (BaUd诵 et al., 2016) and it has been leveraged for the
evaluation of sentence embeddings within the SentEval framework (Conneau et al., 2017).
A straightforward way to predict the probability of (s1, R, s2) being true is to represent s1 and s2
with d-dimensional embeddings h1 and h2, and to compute sentence pair features f(h1, h2), where
f is a composition function (e.g. concatenation, product, . . . ). A softmax classifier gθ can learn to
predict R with those features. gθ ◦ f can be seen as a reasoning based on the content of h1 and
h2 (Socher et al., 2013). We address here limitations of existing compositions for textual relational
learning.
Our contributions are as follows:
-	we review composition functions used in textual relational learning and show that they lack
expressiveness (section 2);
-	we draw analogies with existing SRL models (section 3) and design new compositions
inspired from SRL (section 4);
1
Under review as a conference paper at ICLR 2019
-	We perform extensive experiments to test composition functions and show that some of
them can improve the learning of representations and their downstream uses (section 6).
2	Composition functions for relation prediction
We review here popular composition functions used for relation prediction based on sentence em-
beddings. Ideally, they should simultaneously fulfill the following minimal requirements:
-	make use of interactions between representations of sentences to relate;
-	allow for the learning of asymmetric relations (e.g. entailment, order);
-	be usable with high dimensionalities (θ and f parameters should fit in GPU memory, e.g.
12GB).
If the main goal is transferable sentence representation learning, compositions should also incen-
tivize sentences with a monotonic attributes change (e.g. sentences subjectivity increasing) to lie on
a linear manifold, since transfer usually uses linear models. This use case is the main focus of this
paper. Another goal can be learning of transferable relation representation. Concretely, a sentence
encoder and f can be trained on a base task, and f(h1, h2) can be used as features for transfer in
another task. In that case, the geometry of the sentence embedding space is less relevant, as long as
f(h1, h2) space works well for transfer learning. This case will also occur in our evaluations.
A straightforward instantiation of f is concatenation (Hooda & Kosseim, 2017):
f[,](h1,h2) = [h1,h2]	(1)
However, interactions between s1 and s2 cannot be modeled with f[,] followed by a softmax re-
gression. Using a multi-layer perceptron before the softmax would solve this issue, but it harms
sentence representation learning (Conneau et al., 2017; Logeswaran & Lee, 2018), possibly because
the perceptron allows for accurate predictions even if the sentence embeddings lie in a convoluted
space. Consider a paraphrase detection task: given a sentence s1, the sentence s2 maximizing the
probability of s2 being an s1 paraphrase does not even depend on s1 with this composition. This
effect has been noticed in Levy et al. (2015) regarding lexical relations. To promote interactions
between hi and h2, element-wise product has been used in Baudis et al. (2016):
f(h1,h2)=h1h2	(2)
Absolute difference is another solution for sentence similarity (Mueller & Thyagarajan, 2016), and
its element-wise variation may equally be used to compute informative features:
f-(h1,h2) = |h1 - h2|	(3)
The latter two were combined into a popular instantiation (Tai et al., 2015; Kiros et al., 2015; Mou
et al., 2015):
f-(h1, h2) = [h1	h2, |h2 - h1|]	(4)
Although effective for certain similarity tasks, f- is symmetrical, and should be a poor choice
for tasks like entailment prediction or prediction of discourse relations. For instance, if Re denotes
entailment and (s1, s2)= (“It just rained”, “The ground is wet”), (s1, Re, s2) should hold but not
(s2, Re, s1). The f- composition function is nonetheless used to train/evaluate models on entail-
ment (Conneau et al., 2017) or discourse relation prediction (Nie et al., 2017).
Sometimes [h1, h2] is concatenated to f- (h1, h2) (Ampomah et al., 2016; Nie et al., 2017; Con-
neau et al., 2017; Shen et al., 2017). While the resulting composition is asymmetrical, that asym-
metrical component involves no interaction as noted previously.
An outer product 0 has been used instead for asymmetric multiplicative interaction (Jernite et al.,
2017):
f0(h1, h2) = hi 0 h2 where (hi 0 h2)i,j = hiih2j	(5)
This formulation is expressive but it forces gθ to have d2 parameters per relation, which is prohibitive
when there are many relations and d is high. It can also hinder sentence representation learning. The
problems outlined above are well known in SRL.
To sum up, existing compositions (except f0) can only model relations superficially for tasks cur-
rently used to train state of the art sentence encoders, like NLI or discourse connectives prediction.
2
Under review as a conference paper at ICLR 2019
Model	Scoring function	Relation parameters
Unstructured (Bordes et al., 2013a)	||e1 - e2||p	-
TransE (Bordes et al., 2013b)	||e1 +wr - e2||p	wr ∈ Rd
RESCAL (Nickel et al., 2011)	e1T Wre2	Wr ∈ Rd2
DistMult (Yang et al., 2015)	< e1 , wr, e2 >	wr ∈ Rd
ComplEx (Trouillon et al., 2016)	Re < eι,Wr, e2 >	wr ∈ Cd
Table 1
Selected relational learning models. Following Trouillon et al. (2016), < a, b, c > denotes
Pk akbkck. Re(x) is the real part of x, andp is commonly set to 1.
3	Statistical Relational Learning models
In this section we introduce the context of statistical relational learning (SRL) and relevant models.
Recently, SRL has focused on efficient and expressive relation prediction based on embeddings.
A core goal of SRL (Getoor & Taskar, 2007) is to induce whether a relation R holds between two
arbitrary entities e1, e2. As an example, we would like to assign a score to (e1 , R, e2) = (Paris,
LOCATEd_in, France) that reflects a high probability. In embedding-based SRL models, entities ei
have vector representations in Rd and a scoring function reflects truth values of relations. The scor-
ing function should allow for relation-dependent reasoning over the latent space of entities. Scoring
functions can have relation-specific parameters, which can be interpreted as relation embeddings.
Table 1 presents an overview of a number of state of the art relational models. We can distinguish
two families of models: subtractive and multiplicative.
The TransE scoring function is motivated by the idea that translations in latent space can model
analogical reasoning and hierarchical relationships. Dense word embeddings trained on tasks related
to the distributional hypothesis naturally allow for analogical reasoning with translations without
explicit training on this task (Mikolov et al., 2013). TransE can be seen as a generalization of the
older Unstructured model. We call them subtractive models.
The RESCAL, Distmult, and ComplEx scoring functions can be seen as dot product matching be-
tween e1 and a relation-specific linear transformation of e2 (Liu et al., 2017). This transformation
helps checking whether e1 matches with some aspects of e2 . This is well suited for similarity
learning or to check associations of characteristics between entities. RESCAL allows a full linear
mapping Wre2 but has a high complexity, while Distmult is restricted to a component-wise weight-
ing wr e2 . ComplEx has fewer parameters than RESCAL but still allows for the modeling of
asymmetrical relations. It can be interpreted as a variation of Distmult using a Hermitian product on
complex embeddings instead of a dot product on real embeddings. As shown in Liu et al. (2017),
ComplEx boils down to a restriction of RESCAL where Wr is a block diagonal matrix. These
blocks are 2-dimensional, antisymmetric and have equal diagonal terms. Using such a form, even
and odd indexes of e’s dimensions play the roles of real and imaginary numbers respectively. The
ComplEx model (Trouillon et al., 2016) and its variations (Lacroix et al., 2018) yields state of the
art performance on knowledge base completion on numerous evaluations.
4	Textual relational learning composition functions as SRL
MODELS
We call “textual relational models” relational learning models where sentence embeddings hi act
as entity embeddings ei, as shown in figure 1. In the following we focus on sentence embeddings,
although the model can be straightforwardly applied to other levels of language granularity (such as
words, clauses, or documents).
Some models (Chen et al., 2017b; Seo et al., 2016; Gong et al., 2018) do not rely on explicit sentence
encodings to perform relation prediction. They combine information of input sentences at earlier
stages, using conditional encoding or cross-attention. There is however no straightforward way to
derive transferable sentence representations in this setting, and so these models are out of the scope
3
Under review as a conference paper at ICLR 2019
Figure 1: Overview of our relational model
of this paper. They sometimes make use of composition functions, so our work could still be relevant
to them in some respect.
In this section We will make a link between sentence composition functions and SRL scoring func-
tions, and propose new scoring functions drawing inspiration from SRL.
(a) Score map of (si,Rto_the_past,S2) over possible
sentences s2 using Unstructured composition.
(b) Score map of (si,Rto_the_past,S2) over possible
sentences s2 using TransE composition.
(c) Score map of (s1 , Rentailment , s2) over possible (d) Score map of (s1 , Rentailment , s2) over possible
sentences s2 using DistMult composition.	sentences s2 using ComplEx composition.
Figure 2: Scoring function values according to different composition functions. s1 and R are fixed
and color brightness reflects likelihood of (s1, R, s2) for each position of s2 embedding. (b) and (d)
are respectively more expressive than (a) and (c).
4.1	Linking composition functions and SRL models
The composition function f from equation 2 followed by a softmax regression yields a score whose
analytical form is identical to the Distmult model score described in section 3. If θR denotes the
softmax weights for relation R, the logit score for the truth of (s1, R, s2) is f(h1, h2)θR = (h1
h2)θR which is equal to the Distmult scoring function < h1, θR, h2 > if h acts as entity embedding
and θR as the relation weight wR .
Similarly, the composition f- from equation 3 followed by a softmax regression can be seen as an
element-wise weighted score of Unstructured (both are equal if softmax weights are all unitary).
Thus, f- from 4 (with softmax regression) can be seen as a weighted ensemble of Unstructured and
Distmult. These two models are respectively outperformed by TransE and ComplEx on knowledge
base link prediction by a large margin (Trouillon et al., 2016; Bordes et al., 2013a). We therefore
4
Under review as a conference paper at ICLR 2019
propose to change the Unstructured and Distmult in f- such that they match their respective state
of the art variations in the following sections. We will also show the implications of these refine-
ments.
4.2	Casting TransE as a composition
Simply replacing |h2 - h1 | with
ft(h1,h2) = |h2 - h1 + t|, where t ∈ Rd	(6)
would make the model analogous to TransE. t is learned and is shared by all relations. A relation-
specific translation tR could be used but it would make f relation-specific and disallow the use of a
standard softmax. Instead, here, each dimension of ft(h1, h2) can be weighted according to a given
relation. Non-zero t makes ft asymmetrical and also yields features that allow for the checking
of an analogy between s1 and s2 . Sentence embeddings often rely on pre-trained word embeddings
which have demonstrated strong capabilities for analogical reasoning. Some analogies, such as part-
whole, are computable with off-the-shelf word embeddings (Chen et al., 2017a) and should be very
informative for natural language inference tasks with sentence embeddings. As an illustration, let
us consider an artificial semantic space (depicted in figures 2a and 2b) where we posit that there is a
“to the past” translation t so that h1 + t is the embedding of a sentence s1 changed to the past tense.
Unstructured is not able to leverage this semantic space to correctly score (si,Rto_the_past, s2) while
TransE is well tailored to provide highest scores for sentences near hi + t where tis an estimation
of t that could be learned from examples.
4.3	Casting ComplEx as a composition
Let us partition h dimensions into two equally sized groups R and I. For instance, they could be
even and odd dimension indices of h. We propose a new function fC as a way to fit the ComplEx
scoring function into a composition function.
fC(h1,h2) = [h1Rh2R+h1Ih2I,h1Rh2I - h1I	h2R]	(7)
fC(h1, h2) multiplied by softmax weights θr is equivalent to the ComplEx scoring function
Re <hι,θr,h2 >. The first half of θr weights corresponds to the real part of ComplEx relation
weights while the last half corresponds to the imaginary part.
fC is to the ComplEx scoring function what f is to the DistMult scoring function. Intuitively,
ComplEx is a minimal way to model interactions between distinct latent dimensions while Distmult
only allows for identical dimensions to interact.
Let us consider a new artificial semantic space (shown in figures 2c and 2d) with the first dimension
activating when a sentence means that it just rained, and the second dimension activating when
the ground is wet. Over this semantic space, Distmult is only able to recognize entailment for
paraphrases whereas ComplEx is also able to naturally model that (“it just rained”, Rentailment,
“the ground is wet”) should be high while its converse should not.
We also propose two more general versions of fC :
fCα(h1, h2) = [h1R h2R, h1I h2I, h1R h2I - h1I h2R]	(8)
fCβ (h1, h2) = [h1R h2R, h1I h2I, h1R h2I, h1I h2R]	(9)
fCα can be seen as Distmult concatenated with the asymmetrical part of ComplEx and fCβ can be
seen as RESCAL with unconstrained block diagonal relation matrices. These compositions have
higher dimensionality but this is not as problematic as in SRL (Freebase contains 35k relation types
which can make it hard to learn high dimensional relation embeddings). NLP problems tend to have
a moderate number of relations and we can afford to use slightly more relation parameters. 5
5 On the evaluation of relational models
The SentEval framework (Conneau et al., 2017) provides a general evaluation for transferable sen-
tence representations, with open source evaluation code. One only needs to specify a sentence
5
Under review as a conference paper at ICLR 2019
name	N	task	C	representation(s) used
MR	11k	sentiment (movies)	2	h1
SUBJ	10k	subjectivity/objectivity	2	h1
MPQA	11k	opinion polarity	2	h1
TREC	6k	question-type	6	h1
SICKsm	10k	NLI	3	fm,s(h1, h2)
MRPCsm	4k	paraphrase detection	2	(fm,s(h1, h2) + (fm,s(h2, h1))/2
PDTBsm	17k	discursive relation	5	fm,s(h1,h2)
STS14	4.5k	similarity	-	cos(h1,h2)
Table 2: Transfer evaluation tasks. N = number of training examples; C = number of classes if
applicable. h1, h2 are sentence representations, fm,s a composition function from section 4.
encoder function, and the framework performs classification tasks or relation prediction tasks us-
ing cross-validated logistic regression on embeddings or composed sentence embeddings. Tasks
include sentiment analysis, entailment, textual similarity, textual relatedness, and paraphrase de-
tection. These tasks are a rich way to train or evaluate sentence representations since in a triple
(si, R, s2), We can see (R, s2) as a label for si (BaUdis et al., 2016). Unfortunately, the relational
tasks hard-code the composition function from equation 4. From our previous analysis, we believe
this composition function favors the use of contextual/lexical similarity rather than high-level rea-
soning and can penalize representations based on more semantic aspects. This bias could harm
research since semantic representation is an important next step for sentence embedding. Train-
ing/evaluation datasets are also arguably flaWed With respect to relational aspects since several re-
cent studies (Dasgupta et al., 2018; Poliak et al., 2018; Levy et al., 2018; Glockner et al., 2018)
shoW that InferSent, despite being state of the art on SentEval evaluation tasks, has poor perfor-
mance When dealing With asymmetrical tasks and non-additive composition of Words. In addition
to providing neW Ways of training sentence encoders, We Will also extend the SentEval evaluation
frameWork With a more expressive composition function When dealing With relational transfer tasks,
Which improves results even When the sentence encoder Was not trained With it.
6	Experiments
Our goal is to shoW that transferable sentence representation learning and relation prediction tasks
can be improved When our expressive compositions are used instead of the composition from equa-
tion 4. We train our relational model adaptations on tWo relation prediction base tasks (T), one
supervised (T = NLI) and one unsupervised (T = Disc) described beloW, and evaluate sen-
tence/relation representations on base and transfer tasks using the SentEval frameWork in order to
quantify the generalization capabilities of our models. The code for our experiments Will be publicly
available.
6.1	Training tasks
Natural language inference (T =NLI)’s goal is to predict Whether the relation betWeen tWo sen-
tences (premise and hypothesis) is Entailment, Contradiction or Neutral. We use the SNLI dataset
(BoWman et al., 2015) Which contains 570k examples based on Flickr captions and manual annota-
tions. We also use the MNLI dataset (Williams et al., 2017) Which amounts to 433k examples across
various domains. We call the resulting dataset AllNLI. Conneau et al. (2017) claim that NLI data
alloWs universal sentence representation learning. They used the f,- composition function With
concatenated sentence representations in order to train their Infersent model.
We also train on the prediction of discourse connectives betWeen sentences/clauses (T = Disc).
Discourse connectives make discourse relations betWeen sentences explicit. In the sentence I live in
Paris but I’m often elsewhere, the Word but highlights that there is a contrast betWeen the tWo clauses
it connects. We use Malmi et al.’s (2017) dataset of selected 400k instances With 20 discourse
connectives (e.g. however, for example) With the provided train/dev/test split. This dataset has no
other supervision than the list of20 connectives. Nie et al. (2017) used f,- concatenated With the
sum of sentence representations to train their model, DisSent, on a similar task and shoWed that their
encoder Was general enough to perform Well on SentEval tasks. They use anot yet available dataset.
6
Under review as a conference paper at ICLR 2019
Models trained on natural language inference (T = NLI )
m,s	MR	SUBJ	MPQA	TREC	MRPC-	PDTB-	SICK-	STS14	T
, -	81.2	92.7	90.4	89.6	76.1	46.7	86.6	69.5	84.2
α, -	81.4	92.8	90.5	89.6	75.4	46.6	86.7	69.5	84.3
β, -	81.2	92.6	90.5	89.6	76.0	46.5	86.6	69.5	84.2
, t	81.1	92.7	90.5	89.7	76.5	46.4	86.5	70.0	84.8
α, t	81.3	92.6	90.6	89.2	76.2	47.2	86.5	70.0	84.6
β, t	81.2	92.7	90.4	88.5	75.8	47.3	86.8	69.8	84.2
Table 3: SentEval and base task evaluation results for the models trained on natural language infer-
ence (T = NLI ); AllNLI is used for training. All scores are accuracy percentages, except STS14,
which is Pearson correlation percentage.
6.2	Evaluation tasks
Table 2 provides an overview of different transfer tasks that will be used for evaluation. We added
another relation prediction task, the PDTB coarse-grained implicit discourse relation task, to Sent-
Eval. This task involves predicting a discursive link between two sentences among {Comparison,
Contingency, Entity based coherence, Expansion, Temporal}. We followed the setup of Pitler et al.
(2009), without sampling negative examples in training. MRPC, PDTB and SICK will be tested
with two composition functions: besides SentEval composition f,-, we will use fCβ,- for transfer
learning evaluation, since it has the most general multiplicative interaction and it does not penalize
models that do not learn a translation. For all tasks except STS14, a cross-validated logistic regres-
sion is used on the sentence or relation representation. The evaluation of the STS14 task relies on
Pearson or Spearman correlation between cosine similarity and the target. We force the composi-
tion function to be symmetrical on the MRPC task since paraphrase detection should be invariant to
permutation of input sentences.
6.3	Setup
We want to compare the different instances of f. We follow the setup of Infersent (Conneau et al.,
2017): we learn to encode sentences into h with a bi-directional LSTM using element-wise max
pooling over time. The dimension size of h is 4096. Word embeddings are fixed GloVe with 300
dimensions, trained on Common Crawl 840B1. Optimization is done with SGD and decreasing
learning rate until convergence.
The only difference with regard to Infersent is the composition. Sentences are composed with six
different compositions for training according to the following template:
fm,s,1,2(h1,h2) = [fm(h1,h2),fs(h1,h2),h1,h2]	(10)
fs (subtractive interaction) is in {f-, ft}, fm (multiplicative interaction) is in {f, fCα , fCβ}. We
do not consider fC since it yielded inferior results in our early experiments using NLI and SentEval
development sets.
fm,s,1,2(h1, h2) is fed directly to a softmax regression. Note that Infersent uses a multi-layer percep-
tron before the softmax, but uses only linear activations, so f,-,1,2(h1, h2) is analytically equiva-
lent to Infersent when T = NLI .
6.4	Results
Having run several experiments with different initializations, the standard deviations between them
do not seem to be negligible. We decided to take these into account when reporting scores, contrary
to previous work (Kiros et al., 2015; Conneau et al., 2017): we average the scores of 6 distinct runs
for each task and use standard deviations under normality assumption to compute significance. Table
3 shows model scores for T = NLI , while Table 4 shows scores for T = Disc. For comparison,
Table 5 shows a number of important models from previous work. Finally, in Table 6, we present
1https://nlp.stanford.edu/projects/glove/
7
Under review as a conference paper at ICLR 2019
Models trained on discourse connective prediction (T = Disc)
m,s	MR	SUBJ	MPQA	TREC	MRPC-	PDTB-	SICK-	STS14	T
, -	80.4	92.7	90.2	89.5	74.5	47.3	83.2	57.9	35.7
α, -	80.4	92.9	90.2	90.2	75	47.9	83.3	57.8	35.9
β, -	80.2	92.8	90.2	88.4	74.9	47.5	82.9	57.7	35.9
, t	80.2	92.8	90.2	90.4	74.6	48.5	83.4	58.6	36.1
α, t	80.2	92.9	90.3	90.3	75.1	47.8	83.2	58.3	36.1
β, t	80.2	92.8	90.3	89.7	74.4	47.9	83.7	58.2	35.7
Table 4: SentEval and base task evaluation results for the models trained on discourse connective
prediction (T = Disc). All scores are accuracy percentages, except STS14, which is Pearson
correlation percentage.
Comparison models
model	MR	SUBJ	MPQA	TREC	MRPC-	PDTB-	SICK-	STS14
Infersent	81.1	92.4	90.2	88.2	76.2	-	86.3	70
SkipT	76.5	93.6	87.1	92.2	73	-	82.3	29
BoW	77.2	91.2	87.9	83	72.2	43.9	78.4	54.6
Table 5: Comparison models from previous work. InferSent represents the original results from
Conneau et al. (2017), SkipT is SkipThought from Kiros et al. (2015), and BoW is our re-evaluation
of GloVe Bag of Words from Conneau et al. (2017).
results for sentence relation tasks that use an alternative composition function (fCβ,-) instead of the
standard composition function used in SentEval.
For sentence representation learning, the baseline, f - composition already performs rather well,
being on par with the InferSent scores of the original paper, as would be expected. However, macro-
averaging all accuracies, it is the second worst performing model. fCα ,t,1,2 is the best performing
model, and all three best models use the translation (s = t). On relational transfer tasks, training
with fCα,t,1,2 and using complex Cβ for transfer (Table 6) always outperform the baseline (f,-,1,2
with - composition in Tables 3 and 4). Averaging accuracies of those transfer tasks, this result
is significant for both training tasks at level p < 0.05 (using Bonferroni correction accounting for
the 5 comparisons). To our knowledge, it outperforms all previously reported scores (Conneau
et al., 2017) on SICK Entailment task with single task training. On base tasks and the average
of non-relational transfer tasks (MR, MPQA, SUBJ, TREC), our proposed compositions are on
average slightly better than f,-,1,2. Representations learned with our proposed compositions can
still be compared with simple cosine similarity: all three methods using the translational composition
(s = t) very significantly outperform the baseline (significant at level p < 0.01 with Bonferroni
correction) on STS14 for T = NLI. Thus, we believe fCα,t,1,2 has more robust results and could
be a better default choice than f,-,1,2 as composition for representation learning. 2
Additionally, using Cβ (Table 6) instead of (Tables 3 and 4) for transfer learning in relational
transfer tasks (PDTB, MRPC, SICK) yields a significant improvement on average, even when m =
was used for training (p < 0.001). Therefore, we believe fCβ,- is an interesting composition for
inference or evaluation of models regardless of how they were trained.
7	Related work
There are numerous interactions between SRL and NLP. We believe that our approach merges two
specific lines of work: relation prediction and modeling textual relational tasks.
Some previous NLP work focused on composition functions for relation prediction between text
fragments, even though they ignored SRL and only dealt with word units. Word2vec (Mikolov et al.,
2Note that our compositions are also beneficial with regard to convergence speed: on average, each of our
proposed compositions needed less epochs to converge than the baseline f,-,1,2, for both training tasks.
8
Under review as a conference paper at ICLR 2019
m,s	T = NLI			T= Disc		
	MRPCβ-	PDTB-	SICKβ-	MRPCβ-	PDTB-	SICKβ-
, -	74.8	48.2	83.6	76.2	47.2	86.9
α, -	74.9	49.3	83.8	75.9	47.1	86.9
β, -	75	48.8	83.4	75.8	47	87
, t	74.9	48.7	83.6	76.2	47.8	86.8
α, t	75.2	48.6	83.5	76.2	47.6	87.3
β, t	74.6	48.9	83.9	76.2	47.8	87
Table 6: Results for sentence relation tasks using an alternative composition function (fCβ,-) during
the evaluation step.
2013) has sparked a great interest for this task with word analogies in the latent space such as king -
q-u-e-e→n ≈ -m-a→n - w--o-m--a→n. Levy & Goldberg (2014) explored different scoring functions between
words, notably for analogies. Hypernymy relations were also studied, by Chang et al. (2017) and
Fu et al. (2014). Levy et al. (2015) proposed tailored scoring functions. Even the skipgram model
(Mikolov et al., 2013) can be formulated as finding relations between context and target words. We
did not empirically explore textual relational learning at the word level, but we believe that it would
fit in our framework, and could be tested in future studies. Numerous approaches (Chen et al.,
2017b; Seok et al., 2016; Gong et al., 2018) were proposed to predict inference relations between
sentences, but don’t explicitely use sentence embeddings. Instead, they encode sentences jointly,
possibly with the help of previously cited word compositions, therefore it would also be interesting
to try applying our techniques within their framework.
Some modeling aspects of textual relational learning have been formally investigated by BaUdis
et al. (2016). They noticed the genericity of relational problems and explored multi-task and transfer
learning on relational tasks. Their work is complementary to ours since their framework unifies tasks
while ours unifies composition functions. Subsequent approaches use relational tasks for training
and evaluation on specific datasets (Conneau et al., 2017; Nie et al., 2017).
8	Conclusion
We have demonstrated that a number of existing models used for textual relational learning rely on
composition functions that are already used in Statistical Relational Learning. By taking into ac-
count previous insights from SRL, we proposed new composition functions and demonstrated their
usefulness for tasks presented in recent work. These composition functions are all simple to im-
plement and we hope that it will become standard to try them on relational problems. Larger scale
data might leverage these more expressive compositions, as well as more compositional, asymmet-
ric, and arguably more realistic datasets (Dasgupta et al., 2018; Gururangan et al., 2018). Finally,
our compositions can also be helpful to improve interpretability of embeddings, since they can help
measure relation prediction asymmetry. Analogies through translations helped interpreting word
embeddings, and perhaps anlyzing our learned t translation could help interpreting sentence embed-
dings.
9
Under review as a conference paper at ICLR 2019
References
Isaac K E Ampomah, Seong-bae Park, and Sang-jo Lee. A Sentence-to-Sentence Relation Network
for Recognizing Textual Entailment. World Academy of Science, Engineering and Technology
International Journal ofComputer and Information Engineering, 10(12):1955-1958, 2016.
Petr BaUdis, Jan PichL Tomas VyskociL and Jan Sedivy. Sentence Palr Scoring: Towards Unified
Framework for Text Comprehension. 2016. URL http://arxiv.org/abs/1603.06127.
Antoine Bordes, Xavier Glorot, Jason Weston, and YoshUa Bengio. A Semantic Matching Energy
FUnction for Learning with MUlti-relational Data. Machine Learning, 2013a. ISSN 0885-6125.
doi: 10.1007/s10994-013-5363-6. URL http://arxiv.org/abs/1301.3485.
Antoine Bordes, Nicolas UsUnier, Jason Weston, and Oksana Yakhnenko. Translating Embeddings
for Modeling MUlti-Relational Data. Advances in NIPS, 26:2787-2795, 2013b. ISSN 10495258.
doi: 10.1007/s13398-014-0173-7.2.
SamUel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large an-
notated corpUs for learning natUral langUage inference. Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,Lisbon, Portugal, 17-21 September 2015,
(September):632-642, 2015. ISSN 9781941643327.
Haw-ShiUan Chang, ZiYUn Wang, LUke Vilnis, and Andrew McCallUm. UnsUpervised Hypernym
Detection by DistribUtional InclUsion Vector Embedding. 2017. URL http://arxiv.org/
abs/1710.00880.
Dawn Chen, JoshUa C. Peterson, and Thomas L. Griffiths. EvalUating vector-space models of anal-
ogy. Proceeding:0-5, 2017a. URL http://arxiv.org/abs/1705.04416.
Qian Chen, Xiaodan ZhU, Zhen-HUa Ling, Si Wei, HUi Jiang, and Diana Inkpen. Enhanced lstm
for natUral langUage inference. In Regina Barzilay and Min-Yen Kan (eds.), ACL (1), pp. 1657-
1668. Association for CompUtational LingUistics, 2017b. ISBN 978-1-945626-75-3. URL http:
//dblp.uni- trier.de/db/conf/acl/acl2017- 1.html#ChenZLWJI17.
Alexis ConneaU, DoUwe Kiela, Holger Schwenk, Loic BarraUlt, and Antoine Bordes. SUpervised
Learning of Universal Sentence Representations from NatUral LangUage Inference Data. Emnlp,
2017.
Ishita Dasgupta, Demi Guo, Andreas StuhlmUller, Samuel J. Gershman, and Noah D. Goodman.
EvalUating Compositionality in Sentence Embeddings. (2011), 2018. URL http://arxiv.
org/abs/1802.04302.
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. Learning Semantic
Hierarchies via Word Embeddings. Acl, pp. 1199-1209, 2014.
Lise Getoor and Ben Taskar. Introduction to Statistical Relational Learning (Adaptive Computation
and Machine Learning). The MIT Press, 2007. ISBN 0262072882.
Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking NLI Systems with Sentences that
Require Simple Lexical Inferences. Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Short Papers), (3):1-6, 2018. URL http://arxiv.org/abs/
1805.02266.
Yichen Gong, Heng Luo, and Jian Zhang. Natural language inference over interaction space. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=r1dHXnH6-.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and
Noah A. Smith. Annotation Artifacts in Natural Language Inference Data. 2018. URL http:
//arxiv.org/abs/1803.02324.
Sohail Hooda and Leila Kosseim. Argument Labeling of Explicit Discourse Relations using LSTM
Neural Networks. 2017. URL http://arxiv.org/abs/1708.03425.
10
Under review as a conference paper at ICLR 2019
Yacine Jernite, Samuel R. Bowman, and David Sontag. Discourse-Based Objectives for Fast Unsu-
pervised Sentence Representation Learning. 2017. URL http://arxiv.org/abs/1705.
00557.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing
systems,pp. 3294-3302, 2015.
Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for
knowledge base completion. In ICML, 2018.
Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representa-
tions. Proceedings of the Eighteenth Conference on Computational Natural Language Learning,
pp. 171-180, 2014. doi: 10.3115/v1/W14-1618. URL http://aclweb.org/anthology/
W14-1618.
Omer Levy, Steffen Remus, Chris Biemann, and Ido Dagan. Do Supervised Distributional Methods
Really Learn Lexical Inference Relations? Naacl-2015, pp. 970-976, 2015. URL http://
www.aclweb.org/anthology/N/N15/N15-1098.pdf.
Omer Levy, Samuel R Bowman, and Noah A Smith. Annotation Artifacts in Natural Language
Inference Data. Proceedings of NAACL-HLT 2018, pp. 107-112, 2018.
Hanxiao Liu, Yuexin Wu, and Yiming Yang. Analogical Inference for Multi-Relational Embeddings.
Icml, 2017. ISSN 1938-7228. URL http://arxiv.org/abs/1705.02426.
Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representa-
tions. pp. 1-16, 2018. URL http://arxiv.org/abs/1803.02893.
Lajanugen Logeswaran, Honglak Lee, and Dragomir Radev. Sentence Ordering using Recurrent
Neural Networks. pp. 1-15, 2016. URL http://arxiv.org/abs/1611.02654.
Eric Malmi, Daniele Pighin, Sebastian Krause, and Mikhail Kozhevnikov. Automatic Prediction of
Discourse Connectives. 2017. URL http://arxiv.org/abs/1702.00992.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words
and Phrases and their Compositionality. Nips, pp. 1-9, 2013. ISSN 10495258. doi: 10.1162/jmlr.
2003.3.4-5.951.
Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. Natural Language Inference by
Tree-Based Convolution and Heuristic Matching. pp. 130-136, 2015. URL http://arxiv.
org/abs/1512.08422.
Jonas Mueller and Aditya Thyagarajan. Siamese Recurrent Architectures for Learning Sentence
Similarity. In AAAI, pp. 2786-2792, 2016.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A Three-Way Model for Collective
Learning on Multi-Relational Data. Icml, pp. 809—-816, 2011.
Allen Nie, Erin D. Bennett, and Noah D. Goodman. DisSent: Sentence Representation Learning
from Explicit Discourse Relations. 2017. URL http://arxiv.org/abs/1710.04334.
Emily Pitler, Annie Louis, and Ani Nenkova. Automatic sense prediction for implicit discourse
relations in text. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Language Processing of the AFNLP Volume
2 ACLIJCNLP 09, 2(August):683-691, 2009. doi: 10.3115/1690219.1690241. URL http:
//www.aclweb.org/anthology/P/P09/P09-1077.
Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme.
Hypothesis Only Baselines in Natural Language Inference. Proceedings of the 7th Joint Confer-
ence on Lexical and Computational Semantics, (1):180-191, 2018.
Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention
flow for machine comprehension. CoRR, abs/1611.01603, 2016. URL http://arxiv.org/
abs/1611.01603.
11
Under review as a conference paper at ICLR 2019
Miran Seok, Hye-Jeong Song, Chan-Young Park, Jong-Dae Kim, and Yu-Seop Kim. Named Entity
Recognition using Word Embedding as a Feature 1. International Journal of Software Engineer-
ingand ItsAPPlications, 10(2):93-104, 2016. ISSN 1738-9984. doi: 10.14257∕ijseia.2016.102
08. URL http://dx.doi.org/10.14257/ijseia.2016.10.2.08.
Dinghan Shen, Yizhe Zhang, Ricardo Henao, Qinliang Su, and Lawrence Carin. Deconvolutional
Latent-Variable Model for Text Sequence Matching. 2017.
Richard Socher, Danqi Chen, Christopher Manning, Danqi Chen, and Andrew Ng. Reasoning
With Neural Tensor Networks for Knowledge Base Completion. In Neural Information Pro-
cessing Systems (2003), pp. 926-934, 2013. URL https://nlp.stanford.edu/pubs/
SocherChenManningNg{\_}NIPS2013.pdf.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved Semantic Representations
From Tree-Structured Long Short-Term Memory Networks. 2015. ISSN 9781941643723. doi:
10.1515/popets-2015-0023. URL http://arxiv.org/abs/1503.00075.
Theo Trouillon, Johannes WelbL Sebastian Riedel, Enc GaUssier, and GUillaUme Bouchard. Com-
plex Embeddings for Simple Link Prediction. In Proceedings of the 33nd International Confer-
ence on Machine Learning, volUme 48, 2016. ISBN 9781510829008. URL http://arxiv.
org/abs/1606.06357.
Adina Williams, Nikita Nangia, and SamUel R. Bowman. A Broad-Coverage Challenge CorpUs for
Sentence Understanding throUgh Inference. 2017. URL http://arxiv.org/abs/1704.
05426.
Min ChUl Yang, Do Gil Lee, So YoUng Park, and Hae Chang Rim. Knowledge-based qUestion
answering Using the semantic embedding space. ExPert Systems with APPlications, 42(23):9086-
9104, 2015. ISSN 09574174. doi: 10.1016/j.eswa.2015.07.009. URL http://dx.doi.org/
10.1016/j.eswa.2015.07.009.
12