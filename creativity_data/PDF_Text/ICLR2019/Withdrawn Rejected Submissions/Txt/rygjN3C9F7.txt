Under review as a conference paper at ICLR 2019
The Variational Deficiency B ottleneck
Anonymous authors
Paper under double-blind review
Ab stract
We introduce a bottleneck method for learning data representations based on chan-
nel deficiency, rather than the more traditional information sufficiency. A varia-
tional upper bound allows us to implement this method efficiently. The bound it-
self is bounded above by the variational information bottleneck objective, and the
two methods coincide in the regime of single-shot Monte Carlo approximations.
The notion of deficiency provides a principled way of approximating complicated
channels by relatively simpler ones. The deficiency of one channel w.r.t. another
has an operational interpretation in terms of the optimal risk gap of decision prob-
lems, capturing classification as a special case. Unsupervised generalizations are
possible, such as the deficiency autoencoder, which can also be formulated in
a variational form. Experiments demonstrate that the deficiency bottleneck can
provide advantages in terms of minimal sufficiency as measured by information
bottleneck curves, while retaining a good test performance in classification and
reconstruction tasks.
Keywords: Variational Information Bottleneck, Blackwell Sufficiency, Le Cam
Deficiency, Information Channel
1 Introduction
The information bottleneck (IB) is an approach to learning data representations based on a notion
of minimal sufficiency. The general idea is to map an input source into a representation that re-
tains as little information as possible about the input (minimality), but retains as much information
as possible in relation to a target variable of interest (sufficiency). See Figure 1. For example, in
a classification problem, the target variable could be the class label of the input data. In a recon-
struction problem, the target variable could be a denoised reconstruction of the input. Intuitively,
a representation which is minimal in relation to a given task, will discard nuisances in the inputs
that are irrelevant to the task, and hence distill more meaningful information and allow for a better
generalization.
In a typical bottleneck paradigm, an input variable X is first mapped to an intermediate represen-
tation variable Z, and then Z is mapped to an output variable of interest Y . We call the mappings,
resp., a representation model (encoder) and an inference model (decoder). The channel κ models
the true relation between the input X and the output Y . In general, the channel κ is unknown, and
only accessible through a set of examples (x(i), y(i))iN=1. We would like to obtain an approximation
of κ using a probabilistic model that comprises of the encoder-decoder pair.
The IB methods (WitsenhaUsen & Wyner, 1975; Tishby et al., 1999; Harremoes & Tishby, 2007;
Hsu et al., 2018) have found numerous applications, e.g., in representation learning, clustering,
classification, generative modeling, model selection and analysis in deep neUral networks, among
others (see, e.g., Shamir et al., 2008; Gondek & Hofmann, 2003; Higgins et al., 2017; Alemi et al.,
2018; Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017).
In the traditional IB, minimality and sUfficiency are measUred in terms of the mUtUal information.
CompUting the mUtUal information can be challenging in practice. VarioUs recent works have formU-
lated more tractable fUnctions by way of variational boUnds on the mUtUal information (Chalk et al.,
2016; Alemi et al., 2016; Kolchinsky et al., 2017), sandwiching the objective fUnction of interest.
Instead of maximizing the sUfficiency term of the IB, we formUlate a new bottleneck method that
minimizes deficiency. Deficiencies provide a principled way of approximating complex channels
1
Under review as a conference paper at ICLR 2019
X——κ—> Y
Figure 1: The bottleneck paradigm: The general idea of a bottleneck method is to first map an
input X to an intermediate representation Z , and then map Z to an output Y . We call the map-
pings, resp., an encoder (e) and a decoder (d). In general, the true channel κ is unknown, and only
accessible through a set of training examples. We would like to obtain an approximation of κ.
by relatively simpler ones. The deficiency of a decoder with respect to the true channel between
input and output variables quantifies how well any stochastic encoding at the decoder input can be
used to approximate the true channel. Deficiencies have a rich heritage in the theory of comparison
of statistical experiments (Blackwell, 1953; Le Cam, 1964; Torgersen, 1991). From this angle, the
formalism of deficiencies has been used to obtain bounds on optimal risk gaps of statistical decision
problems. As we show, the deficiency bottleneck minimizes a regularized risk gap. Moreover,
the proposed method has an immediate variational formulation that can be easily implemented as
a modification of the Variational Information Bottleneck (VIB) (Alemi et al., 2016). In fact, both
methods coincide in the limit of single-shot Monte Carlo approximations. We call our method the
Variational Deficiency Bottleneck (VDB).
Perfect maximization of the IB sufficiency corresponds to perfect minimization of the DB deficiency.
However, when working over a parametrized model and adding the bottleneck regularizer, both
methods have different preferences, with the DB being closer to the optimal risk gap. Experiments
on basic data sets show that the VDB is able to obtain more compressed representations than the
VIB while performing equally well or better in terms of test accuracy.
We describe the details of our method in Section 2. We elaborate on the theory of deficiencies in
Section 3. Experimental results with the VDB are presented in Section 4.
2 The variational deficiency bottleneck (VDB)
Let X denote an observation or input variable and Y an output variable of interest. Let p(x, y) =
π(x)κ(y∣x) be the true joint distribution, where the conditional distribution or channel κ(y∖x) de-
scribes how the output depends on the input. We consider the situation where the true channel
is unknown, but we are given a set of N independent and identically distributed (i.i.d.) sam-
ples (x(i) , y(i) )iN=1 from p. Our goal is to use this data to learn a more structured version of the
channel κ, by first “compressing” the input X to an intermediate representation variable Z and
subsequently mapping the representation back to the output Y. The presence of an intermediate
representation can be regarded as a bottleneck, a model selection problem, or as a regularization
strategy.
We define a representation model and an inference model using two parameterized families of chan-
nels e(z∖x) and d(y∖z). We will refer to e(z∖x) and d(y∖z) as an encoder and a decoder. The
encoder-decoder pair induces a model b(y∖x) = / d(y∖z)e(z∖x)dz. Equivalently, we write b = doe.
Given a representation, we want the decoder to be as powerful as the original channel κ in terms of
ability to recover the output. The deficiency of a decoder d w.r.t. κ quantifies the extent to which
any pre-processing of d (by way of randomized encodings) can be used to approximate κ (in the
KL-distance sense). Let M(X ; Y) denote the space of all channels from X to Y. We define the
deficiency of d w.r.t. κ as follows.
Definition 1. Given the channel κ ∈ M(X ; Y) from X to Y, and a decoder d ∈ M(Z; Y) from
some Z to Y, the deficiency of d w.r.t. κ is defined as
δπ (d, κ) =	min DKL(κkd ◦ e∖π).	(1)
e∈M(X;Z)
Here Dkl(∙∣∣ ∙ ∖∙) is the conditional KL divergence (Csiszar & Korner, 2011), and π is an input
distribution over X. The definition is similar in spirit to Lucien Le Cam’s notion of weighted defi-
2
Under review as a conference paper at ICLR 2019
ciencies of one channel w.r.t. another (Le Cam, 1964; Torgersen, 1991, Section 6.2) and its recent
generalization by Raginsky (2011).
We propose to train the model by minimizing the deficiency of d w.r.t. κ subject to a regular-
ization that penalizes complex representations. The regularization is achieved by limiting the
rate I(Z; X), the mutual information between the representation and the raw inputs. We call our
method the Deficiency Bottleneck (DB). The DB minimizes the following objective over all tu-
ples (e ∈ M(X;Z),d ∈ M(Z; Y)):
LDB(e,d) := δπ(d,κ) + βI(Z; X).
(2)
The parameter β ≥ 0 allows us to adjust the level of regularization.
For any distribution r(z), the rate term admits a simple variational upper bound (Csiszar & Korner,
2011, Eq. (8.7)):
I(Z; X) ≤ ∕p(χ,z)iog erzZ))
dx dz .
(3)
Let Pdata be the empirical distribution of the data (input-output pairs). By noting that δπ (d, K) ≤
DKL(Kkd◦ e∣π) for any e ∈ M(X; Z), and ignoring (unknown) data-dependent constants, we obtain
the following optimization objective which we call the Variational Deficiency Bottleneck (VDB)
objective:
LVDB(e,d) := E(χ,y)〜Pdata [-log((d◦ e)(y∣x)) + βDκL(e(Z∣x)kr(Z))].	(4)
The computation is simplified by defining r(z) to be a standard multivariate Gaussian distribu-
tion N(0, I), and using an encoder of the form e(z∣x) = N(z∣fφ(χ)), where fφ is a neural network
that outputs the parameters of a Gaussian distribution. Using the reparametrization trick (Kingma
& Welling, 2013; Rezende et al., 2014), we then write e(z|x)dz = p()d, where z = f(x, ε) is a
function of x and the realization of a standard normal distribution. This allows us to do stochastic
backpropagation through a single sample z. The KL term admits an analytic expression for a choice
of Gaussian r(z) and encoders. We train the model by minimizing the following empirical objective:
1	N Γ ι M	一
INE - log( MM：£[d(y(i)|f(x(i),e(j)))]) + βDκL(e(Z∣χ(i))kr(Z)) .	(5)
N i=1	M j=1
For training, we choose a mini-batch size ofN = 100. For Monte Carlo estimates of the expectation
inside the log, we choose M = 3, 6, 12 samples from the encoding distribution.
We note that the Variational Information Bottleneck (VIB) (Alemi et al., 2016) leads to a similar-
looking objective function, with the only difference that the sum over j is outside of the log. By
Jensen’s inequality, the VIB loss is an upper bound to our loss. If one uses a single sample from the
encoding distribution (i.e., M = 1), the VDB and the VIB objective functions coincide.
The average log-loss and the rate term in the VDB objective equation 4 are the two fundamental
quantities that govern the probability of error when the model is a classifier. For a discussion of
these relations, see Appendix A.
3	B lackwell S ufficiency and Channel Deficiency
In this section, we discuss an intuitive geometric interpretation of the deficiency in the space of
probability distributions over the output variable. We also give an operational interpretation of the
deficiency as a deviation from Blackwell sufficiency (in the KL-distance sense). Finally, we discuss
its relation to the log-loss.
3.1	Deficiency and Decision Geometry
We first formulate the learning task as a decision problem. We show that δπ(d, K) quantifies the gap
in the optimal risks of decision problems when using the channel d rather than K.
3
Under review as a conference paper at ICLR 2019
Let X , Y denote the space of possible inputs and outputs. In the following, we assume that X and
Y are finite. Let PY be the set of all distributions on Y. For every x ∈ X, define κx ∈ PY as
κχ(y) = κ(y∣x), ∀y ∈ Y. Nature draws X 〜 π and y 〜 Kχ. The learner observes X and quotes a
distribution qχ ∈ PY that expresses her uncertainty about the true value y. The quality of a quote qχ
in relation to y is measured by an extended real-valued loss function called the score ': YXPY → R.
For a background on such special kind of loss functions see, e.g., GrUnWaId et al., 2004; Gneiting
& Raftery, 2007; Parry et al., 2012. Ideally, the quote qx should to be as close as possible to the
true conditional distribution κx. This is achieved by minimizing the expected loss L(κx, qx) :=
Ey〜Kx '(y, qx), for all X ∈ X. The score is called proper if Kx ∈ argmi',∈pγι L(κχ, qχ). Define
the Bayes act against κx as the optimal quote
qX ：= arg min L(κχ,qχ).
qx∈PY
If multiple Bayes acts exist then select one arbitrarily. Define the Bayes risk for the distribu-
tion PXY(x,y) = π(x)κ(y∣x) as R(PXY, ') := Ex〜∏L(κχ, q*). A score is strictly proper if
the Bayes act is unique. An example of a strictly proper score is the log-loss function defined
as 'l(j, q) := 一 log q(y). For the log-loss, the Bayes act is qx = Kx and the Bayes risk is just the
conditional entropy
R(pxy ,'l) = Ex 〜∏ Ey 〜κx[ — log qx(y)] = Ex 〜∏ Ey 〜κx[ — log κx(y)] = H (Y X).	(6)
Given a representation z ∈ Z (output by some encoder), when using the decoder d, the learner is
constrained to quote a distribution from a subset ofPY. Let C = conv({dz : z ∈ Z}) ⊂ PY be the
convex hull of the points {dz }z∈Z ∈ PY. The Bayes act against dz is
qxZ ：= arg min Ey〜κx [ — log qx(y)].	(7)
qx ∈C
qxxZ has an interpretation as the reverse I-projection of Kx to the convex set of probability mea-
sures C ⊂ PY (Csiszar & Matus, 2003)1. We call the associated Bayes risk as the projected
Bayes risk RZ(PXY,'l) and the associated conditional entropy as the projected conditional en-
tropy HZ(Y|X),
rZ (PXY,'L)= Ex 〜π Ey 〜Kx [一 log qxZ ⑻]=HZ (Y |X )∙	(8)
The gap in the optimal risks, ∆R := RZ(PXY,'l) 一 R(PXY,'l) when making a decision based
on an intermediate representation and a decision based on the input data is just the deficiency. This
follows from noting that
∆R= HZ(Y|X) 一 H(Y|X) =	π(X)
x∈X
= min
e∈M(X;Z)
= min
e∈M(X;Z)
min DKL(Kxkqx)
qx∈C⊂PY
π(X)DKL(Kxkd ◦ ex)
x∈X
DKL(Kkd ◦ e∣π) = δπ(d, κ).
(9)
∆R vanishes if and only if the optimal quote against dz, qxx matches Kx for all X, y. This gives an
intuitive geometric interpretation of a vanishing deficiency in the space of distributions over Y.
Given a decoder channel d, since δπ(d, K) ≤ DKL(Kkd◦ e∣π) for any e ∈ M(X; Z), the loss term in
the VDB objective is a variational upper bound on the projected conditional entropy HZ(Y|X).
However, this loss is still a lower bound to the standard cross-entropy loss in the VIB objec-
tive (Alemi et al., 2016), i.e.,
^^(X,y)~pdata [一 log d ◦ e(y|x)] ≤ E(x,y)~pdata
一 e(z|X) log d(y|z)dz
(10)
This follows simply from the convexity of the negative logarithm function.
1 Such a projection exists and is not necessarily unique. If nonunique, we arbitrarily select one of the
minimizers as the Bayes act.
4
Under review as a conference paper at ICLR 2019
3.2	Deficiency as a KL-distance from Input-Blackwell Sufficiency
In a seminal paper David Blackwell (1953) asked the following question: if a learner wishes to
make an optimal decision about some target variable of interest and she can choose between two
channels with a common input alphabet, which one should she prefer? She can rank the channels
by comparing her optimal risks: she will always prefer one channel over another if her optimal
risk when using the former is at most that when using the latter for any decision problem. She can
also rank the variables purely probabilistically: she will always prefer the former if the latter is an
output-degraded version of the former, in the sense that she can simulate a single use of the latter by
randomizing at the output of the former. Blackwell showed that these two criteria are equivalent.
Very recently, Nasser (2017) asked the same question, only now the learner has to choose between
two channels with a common output alphabet. Given two channels, κ ∈ M(X; Y) and d ∈ M(Z; Y),
We say that K is input-degraded from d and write d 占Y K if K = d◦ e for some e ∈ M(X; Z). Stated
in another way, d can be reduced to κ by applying a randomization at its input. Nasser (2017) gave
a characterization of input-degradedness that is similar to Blackwell’s theorem (Blackwell, 1953).
We say, d is input-Blackwell sufficient for K if d Y K.
Input-Blackwell sufficiency induces a preorder on the set of all channels with the same output al-
phabet. In practice, most channels are uncomparable, i.e., one cannot be reduced to another by a
randomization. When such is the case, the deficiency quantifies how far the true channel K is from
being a randomization (by way of all input encodings) of the decoder d. See Appendix B for a brief
summary of Blackwell-Le Cam theory.
3.3	Deficiency and the Log-Loss
When Y - X - Z is a Markov chain, the conditional mutual information I(Y ; X|Z) is the Bayes
risk gap for the log-loss. This is apparent from noting that I(Y ; X|Z) = H(Y |Z) - H(Y |XZ) =
H(Y|Z) 一 H(Y|X) = R(PZY, 'l) 一 R(PXY, 'l). This risk gap is closely related to BlackWelrs
original notion of sufficiency. Since the log-loss is strictly proper, a vanishing I(Y; X|Z) implies
that the risk gap is zero for all loss functions. This suggests that minimizing the log-loss risk gap
under a suitable regularization constraint is a potential recipe for constructing representations Z
that are approximately sufficient for X W.r.t. Y, since in the limit When I(Y; X|Z) = 0 one Would
achieve I(Y; Z) = I(Y; X). This is indeed the basis for the IB algorithm (Tishby et al., 1999)
and its generalization, clustering With Bregman divergences (Banerjee et al., 2005; van Rooyen &
Williamson, 2015; 2014).
One can also approximate a sufficient statistic by minimizing deficiencies instead. This is motivated
from noting the folloWing proposition.
Proposition 2. When Y — X — Z is a Markov chain, δπ(d,κ)=0 ^⇒ I (Y; X |Z) = 0.
In general, for the bottleneck paradigms involving the conditional mutual information (IB) and the
deficiency (DB), We have the folloWing relationship:
min	I(X; Z) ≥ min	I(X; Z).	(11)
e(z∣x): I(Y[X∣Z)≤e	e(z∣x): δπ(d,κ)≤e
Our experiments corroborate that for achieving the same level of sufficiency, one needs to store
less information about the input X When minimizing the deficiencies than When minimizing the
conditional mutual information.
4	Experiments
We present some experiments on the MNIST dataset (LeCun & Cortes, 2010). Classification on
MNIST is a very Well studied problem. The main objective of our experiments is to evaluate the
information-theoretic properties of the representations learned by the VDB and Whether it can match
the classification accuracy provided by other bottleneck methods.
For the encoder, we use a fully connected feedforward network with 784 input UnitS-1024 ReLUs-
1024 ReLUs-512 linear output units. The deterministic output of this network is interpreted as
the vector of means and variances of a 256 dimensional Gaussian distribution. The decoder is a
5
Under review as a conference paper at ICLR 2019
1；
0.9
0.8
0.7
>
o
巴0.6
40.5
0.4
0.3
0.2
0.1
10-8
MNIST train test accura
10-6	10-4	10-2
β
MNIST VDB IZX vs Beta curve train test
—M— M = 1
-F- M = 1 train
I M = 3
—壬--M = 3 train
—M— M = 6
-F - M = 6 train
—M— M = 12
M = 12 train
100
I(Z；X)
102
101
100
10-1
10-2
:	M	=	1
---------M	=	1 train
:-------------M	=	3
---------M	=	3 train
M = 6
，	M	=	6 train
:-------------M	=	12
:-------------M	=	12 train
/
一_
10-10
10-8	10-6	10-4	10-2	100
I(Z；X)
Figure 2: Effect of the regularization parameter β. The upper left panel shows the accuracy on train
and test data after training the VDB for different values of M . Here, M is the number of encoder
output samples used in the training objective. L is the number of encoder output samples used for
evaluating the classifier. The upper right panel traces the deficiency bottleneck curve for different
values of β (see text). The curves are averages over 5 repetitions of the experiment. Each curve
corresponds to one value of M = 1, 3, 6, 12. Notice the generalization gap for small values of β
(towards the right of the plot). The lower right panel plots the corresponding information bottleneck
curve. The lower left panel plots the minimality term vs. β. Evidently, the levels of compression
vary depending on M. Higher values of M (our method) lead to a more compressed representation.
For M = 1, the VDB and the VIB models coincide.
Table 1: Comparison of test accuracy values for
different values of β and M. K is the size of the
bottleneck and L =12. We see a slight
improvement in the test accuracies for higher
values of M .
β	K	M			
		1	3	6	12
10-5	256	0.9869	0.9873	0.9885	0.9878
	2	0.9575	0.9678	0.9696	0.9687
10-3	256	0.9872	0.9879	0.9875	0.9882
	2	0.9632	0.9726	0.9790	0.9702
simple logistic regression model with a softmax layer. These are the same settings of the model
used by Alemi et al. (2016). We implement the algorithm in Tensorflow and train for 200 epochs
using the Adam optimizer.
As can be seen from the upper left panel in Figure 2, the test accuracy is stable with increasing M .
Here, M is the number of encoder output samples used in the training objective. We note that M = 1
is just the VIB model (Alemi et al., 2016). L is the number of encoder output samples used for eval-
Uating the classifier (i.e., We use L Pf=ι d(y∣z(j)) where Zj)〜e(z∣χ)). Numerical values of the
6
Under review as a conference paper at ICLR 2019
β \ M
10-1
10-5
1	3	6	12
-3-2-1 O 1	2	3
10-3
-g-2Φ02ΦQ	-g-2Φ02ΦQ	-g-2Φ02ΦQ	-g-2βO2ΦQ
Figure 3: We trained the VDB on MNIST with the basic encoder given by a fully connected network
with two hidden layers of ReLUs generating the means and variances of 2D independent Gaussian
latent representation. Ellipses represent the posterior distributions of 1000 input images in latent
space after training with β = 100, 10-1, 10-3, 10-5 and M = 1, 3, 6, 12. Color corresponds to the
class label.
1
test accuracies are provided in Table 1 for different values of β and M . We see a slight improvement
in the test accuracies for higher values of M . See Figure 5 for train and test accuracies for L = 3
and L = 12 in Appendix D. The traditional IB paradigm traces the mutual information I(Z; Y )
between representation and output (sufficiency) vs. the mutual information I(Z; X) between repre-
sentation and input (minimality), for different values of the regularization parameter β . This curve is
called the information bottleneck curve (Tishby et al., 1999). In the case of the VDB, we define the
corresponding sufficiency term as J(Z; Y) := H(Y) - E(χ,y)〜Pdata [- log(ʃ d(y∣z)e(z∣x) dz)].
Here H(Y) = log2(10) is the entropy of the output which has 10 classes. In our method, “more
informative” means “less deficient”. The upper right panel in Figure 2 shows the deficiency bottle-
neck curve which traces J(Z; Y) vs. I(Z; X) for different values of β at the end of training. For
orientation, lower values of β have higher values of I(Z; X) (towards the right of the plot). For
small values of β, when the effect of the regularization is negligible, the bottleneck allows more
information from the input through the representation. In this case, J(Z; Y) increases on the train-
ing set, but not necessarily on the test set. This is manifest in the gap between the train and test
curves indicative of a degradation in generalization. For intermediate values of β, the gap is smaller
for larger values of M (our method). The lower right panel plots the corresponding information
bottleneck curve. The lower left panel in Figure 2 plots the minimality term I(Z; X) vs. β. We see
that, for β in the range between 10-8 and 10-4, for the same level of sufficiency, setting M = 12
consistently achieves more compression of the input compared to the setting M = 1. The dynamics
of the information quantities during training are also interesting. We provide figures on these in
Appendix D.
In order to visualize the representations, we also train the VDB on MNIST with a 2 dimensional
representation. We use the same settings as before, with the only difference that the dimension of
7
Under review as a conference paper at ICLR 2019
i（z；x）
epoch
Figure 4: Learning curves for MNIST, where the encoder is a MLP of size 784-1024-1024-2K ,the
last layer being a K = 2 dimensional diagonal Gaussian. The decoder is simply a Softmax with 10
classes. The left panel plots the mutual information between the representation and the class label,
I(Z; Y), against the mutual information between the representation at the last layer of the encoder
and the input, I(Z; X), as training progresses. The former increases monotonically, while the latter
increases and then decreases. The right panel shows the test accuracy as training progresses.
the output layer of the encoder is 4, with two coordinates representing the mean, and two a diagonal
covariance matrix. The results are shown in Figure 3. For β = 10-5, the representations are well
separated, depending on the class. For related figures in the setting of unsupervised learning see
Appendix E.
The learning dynamics of the mutual information and classification accuracy are shown in Fig-
ure 4. The left panel has an interpretation in terms of a phase where the model is mainly fitting the
input-output relationship and hence increasing the mutual information I(Z; Y), followed by a com-
pression phase, where training is mainly reducing I(Z; X), leading to a better generalization. The
right panel shows the test accuracy as training progresses. Higher values of M (our method) usually
lead to better accuracy. An exception is when the number L of posterior samples for classification
is large.
5	Discussion
We have formulated a bottleneck method based on channel deficiencies. The deficiency ofa decoder
with respect to the true channel between input and output quantifies how well a randomization at the
decoder input (by way of stochastic encodings) can be used to simulate the true channel. The VDB
has a natural variational formulation which recovers the VIB in the limit of a single sample of the
encoder output. Experiments demonstrate that the VDB can learn more compressed representations
while retaining the same discriminative capacity. The method has a statistical decision-theoretic
appeal. Moreover, the resulting variational objective of the DB can be implemented as an easy
modification of the VIB, with little to no computational overhead.
Given two channels that convey information about a target variable of interest, two different notions
of deficiencies arise, depending on whether the target resides at the common input or the common
output of the given channels. When the target is at the common output of the two channels, as
is in a typical bottleneck setting (see Figure 1), our Definition 1 has a natural interpretation as a
KL-divergence from input-Blackwell sufficiency (Nasser, 2017). Here sufficiency is achieved by
applying a randomization at the input of the decoder with the goal of simulating the true chan-
nel. The notion of input-Blackwell sufficiency contrasts with Blackwell’s original notion of suffi-
ciency (Blackwell, 1953) in the sense that Blackwell’s theory compares two channels with a common
input. One can again define a notion of deficiency in this setting (see Appendix B for a discussion
on deficiencies in the classical Blackwell setup). The associated channels (one from Y to Z and the
other from Y to X ) do not however have a natural interpretation in a typical bottleneck setting. In
contrast, the input-Blackwell setup appears to be much more intuitive in this context.
8
Under review as a conference paper at ICLR 2019
The more detailed view of information emerging from this analysis explains various effects and
opens the door to multiple generalizations. In the spirit of the VDB, one can formulate a deficiency
autoencoder as well (see sketch in Appendix E). On a related note, we mention that the deficiency
is a lower bound to a quantity called the Unique information (Bertschinger et al., 2014; Banerjee
et al., 2018a) (see details in Appendix C). An alternating minimization algorithm similar in spirit
to the classical Blahut-Arimoto algorithm (Blahut, 1972) has been proposed to compute this quan-
tity (Banerjee et al., 2018b). A deep neural network implementation of such an algorithm remains
a challenge. In the limit β → 0, the VDB is a step forward towards estimating the unique informa-
tion. This might be of independent interest in improving the practicality of the theory of information
decompositions.
References
Alexander A. Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016. ICLR 2017.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A. Saurous, and Kevin Murphy.
Fixing a broken ELBO. In International Conference on Machine Learning, pp. 159-168, 2018.
Philip Bachman and Doina Precup. Training deep generative models: Variations on a theme. In
NIPS Approximate Inference Workshop, 2015.
Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with Breg-
man divergences. Journal of Machine Learning Research, 6(Oct):1705-1749, 2005.
PradeeP Kr. Banerjee, Eckehard Olbrich, Jurgen Jost, and Johannes Rauh. Unique informations and
deficiencies. arXiv preprint arXiv:1807.05103, 2018a. Allerton 2018.
Pradeep Kr. Banerjee, Johannes Rauh, and Guido Montufar. Computing the unique information. In
Proc. IEEE ISIT, pp. 141-145. IEEE, 2018b.
Nils Bertschinger and Johannes Rauh. The Blackwell relation defines no lattice. In Proc. IEEE ISIT,
pp. 2479-2483. IEEE, 2014.
Nils Bertschinger, Johannes Rauh, Eckehard Olbrich, Jurgen Jost, and Nihat Ay. Quantifying unique
information. Entropy, 16(4):2161-2183, 2014.
David Blackwell. Equivalent comparisons of experiments. The Annals of Mathematical Statistics,
24(2):265-272, 1953.
Richard Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions
on Information Theory, 18(4):460-473, 1972.
Stephane Boucheron, Olivier Bousquet, and Gabor Lugosi. Theory of classification: A survey of
some recent advances. ESAIM: Probability and Statistics, 9:323-375, 2005.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015. ICLR 2016.
Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-
jardins, and Alexander Lerchner. Understanding disentangling in β-VAE. arXiv preprint
arXiv:1804.03599, 2018.
Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational informa-
tion bottleneck. In Advances in Neural Information Processing Systems, pp. 1957-1965, 2016.
Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016. ICLR 2017.
Chris Cremer, Quaid Morris, and David Duvenaud. Reinterpreting importance-weighted autoen-
coders. arXiv preprint arXiv:1704.02916, 2017. Workshop track - ICLR 2017.
9
Under review as a conference paper at ICLR 2019
Imre Csiszar. A class of measures of informativity of observation channels. Periodica Mathematica
Hungarica, 2(1-4):191-213,1972.
Imre Csiszar and JanoS Korner. Information theory: coding theorems for discrete memoryless sys-
tems. Cambridge University Press, 2011.
Imre CSiSzar and Frantisek Matus. Information projections revisited. IEEE Transactions on Infor-
mation Theory, 49(6):1474-1490, 2003.
Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 102(477):359-378, 2007.
David Gondek and Thomas Hofmann. Conditional information bottleneck clustering. In 3rd IEEE
International Conference on Data Mining, Workshop on Clustering Large Data Sets, 2003.
Peter D. Grunwald, A. Philip Dawid, et al. Game theory, maximum entropy, minimum discrepancy
and robust bayesian decision theory. The Annals of Statistics, 32(4):1367-1433, 2004.
Malte Harder, Christoph Salge, and Daniel Polani. A bivariate measure of redundant information.
Physical Review E, 87:012130, 2013.
Peter HarremOeS and Naftali Tishby. The information bottleneck revisited or how to choose a good
distortion measure. In Proc. IEEE ISIT, pp. 566-570. IEEE, 2007.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. β-VAE: Learning basic visual concepts with a con-
strained variational framework. 2017. ICLR 2017.
Hsiang Hsu, Shahab Asoodeh, Salman Salamatian, and Flavio P. Calmon. Generalizing bottleneck
problems. In Proc. IEEE ISIT, pp. 531-535. IEEE, 2018.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013. ICLR 2013.
Artemy Kolchinsky, Brendan D. Tracey, and David H. Wolpert. Nonlinear information bottleneck.
arXiv preprint arXiv:1705.02436, 2017.
Janos Korner and Katalin Marton. Comparison of two noisy channels. In Topics in information
theory, volume 16, pp. 411-423. Colloquia Mathematica Societatis Jnos Bolyai, Keszthely (Hun-
gary), 1975.
Tuan Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and Frank Wood. Auto-encoding sequential
Monte Carlo. arXiv preprint arXiv:1705.10306, 2017. ICLR 2018.
Lucien Le Cam. Sufficiency and approximate sufficiency. The Annals of Mathematical Statistics,
pp. 1419-1455, 1964.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Friedrich Liese and Igor Vajda. On divergences and informations in statistics and information theory.
IEEE Transactions on Information Theory, 52(10):4394-4412, 2006.
Christian Naesseth, Scott Linderman, Rajesh Ranganath, and David Blei. Variational sequential
Monte Carlo. In International Conference on Artificial Intelligence and Statistics, pp. 968-977,
2018.
Rajai Nasser. On the input-degradedness and input-equivalence between channels. In Proc. IEEE
ISIT, pp. 2453-2457. IEEE, 2017.
Matthew Parry, A Philip Dawid, Steffen Lauritzen, et al. Proper local scoring rules. The Annals of
Statistics, 40(1):561-592, 2012.
Maxim Raginsky. Shannon meets Blackwell and Le Cam: Channels, codes, and statistical experi-
ments. In Proc. IEEE ISIT, pp. 1220-1224. IEEE, 2011.
10
Under review as a conference paper at ICLR 2019
Tom Rainforth, Adam R Kosiorek, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood,
and Yee Whye Teh. Tighter variational bounds are not necessarily better. arXiv preprint
arXiv:1802.04537, 2018. ICML 2018.
Danilo Jimenez Rezende and Fabio Viola. Taming VAEs. arXiv preprint arXiv:1810.00597, 2018.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing,pp.1278-1286, 2014.
Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information
bottleneck. In International Conference on Algorithmic Learning Theory, pp. 92-107. Springer,
2008.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
Information Theory Workshop (ITW), 2015 IEEE, pp. 1-5. IEEE, 2015.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In
Proceedings of the 37th Annual Allerton Conference on Communication, Control and Computing,
pp. 368-377, 1999.
Erik Torgersen. Comparison of statistical experiments, volume 36. Cambridge University Press,
1991.
Brendan van Rooyen and Robert C. Williamson. Le Cam meets LeCun: Deficiency and generic
feature learning. arXiv preprint arXiv:1402.4884, 2014.
Brendan van Rooyen and Robert C. Williamson. A theory of feature learning. arXiv preprint
arXiv:1504.00083, 2015.
Matias Vera, Pablo Piantanida, and Leonardo Rey Vega. The role of information complexity and
randomization in representation learning. arXiv preprint arXiv:1802.05355, 2018.
Hans S. Witsenhausen and Aaron D. Wyner. A conditional entropy bound for a pair of discrete
random variables. IEEE Transactions on Information Theory, 21(5):493-501, 1975.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards deeper understanding of variational
autoencoding models. arXiv preprint arXiv:1702.08658, 2017.
11
Under review as a conference paper at ICLR 2019
Appendix
A	Misclassification error and the average log-loss
In a classification task, the goal is to use the training dataset to learn a classifier b(y|x) that mini-
mizes the probability of error under the true data distribution, defined as follows.
PE(b) ：= 1 — E(x,y)〜P [K(y∣χ)].	(12)
It is well known that the optimal classifier that gives the smallest probability of error is the Bayes
classifier (Boucheron et al., 2005). Since we do not know the true data distribution we try to
learn based on the empirical error. Directly minimizing the empirical probability of error over
the training dataset is in general a NP-hard problem. In practice, one minimizes a surrogate loss
function that is a convex upper bound on PE . A natural surrogate is the average log-loss func-
tion E(x,y)〜P [- logb(y∣x)]. When the model is b = d ◦ e, the following upper bounds are immedi-
ate from using Jensen’s inequality.
Pe(b) ≤ 1 - exp ( - E(x,y)〜P [-logd◦ e(y∣x)])
≤ 1 - exp ( - E(x,y)〜PEz〜e(z∣χ) [- log d(y∣z)])	(13)
The bound using the standard cross-entropy loss is evidently weaker than the average log-loss. A
lower bound on the probability of error is controlled by a convex functional of the mutual infor-
mation between the representation and the raw inputs I(Z; X) (Vera et al., 2018, see, e.g., Lemma
4). The average log-loss and the rate term in the VDB objective equation 4 are two fundamental
quantities that govern the probability of error.
B	Classical Theory of Comparison of Channels
In this section, we discuss the classical theory of comparison of channels due to Blackwell (1953)
and its extension by Le Cam (1964); Torgersen (1991) and more recently by Raginsky (2011).
Suppose that a learner wishes to predict the value of a random variable Y that takes values in a
set Y. She has a set of actions A. Each action incurs a loss '(y, a) that depends on the true state y
of Y and the chosen action a. Let πY encode the learners’ uncertainty about the true state y. The
tuple (πY , A, `) is called a decision problem. Before choosing her action, the learner observes a
random variable X through a channel κ ∈ M(Y; X ). An ideal learner chooses a strategy ρ ∈
M(X; A) that minimizes her expected loss or risk R(∏γ, κ, ρ,') := Ey〜∏γEa〜p。/'(y, a). The
optimal risk when using the channel K is R(∏γ, κ,') := minp∈M(χ；A) R(∏γ, κ, ρ,').
Suppose now that the learner has to choose between X and another random variable Z that she
observes through a second channel μ ∈ M(Y; Z) with common input Y. She can always discard X
in favor of Z if, knowing Z, she can simulate a single use of X by randomly sampling a x0 ∈ X
after each observation z ∈ Z.
Definition 3. We say that X is output-degraded from Z w.r.t. Y, denoted Z w0Y X, if there exists a
random variable X0 such that the pairs (Y, X) and (Y, X0) are stochastically indistinguishable, and
Y - Z - X0 is a Markov chain.
She can also discard X if her optimal risk when using Z is at most that when using X for any de-
cision problem. Write Z WY X if R(∏γ, κ,') ≥ R(∏γ, μ,') for any decision problem. Blackwell
(1953) showed the equivalence of these two relations.
Theorem 4. (Blackwell,s Theorem) Z WY X ^⇒ Z WY X.
Write μ WY K if K = λ ◦ μ for some λ ∈ M(Z; X). If ∏γ has full support, then it easy to check
that μ WY κ ^⇒ Z WY X (Bertschinger & Rauh, 2014, Theorem 4).
The learner can also compare K and μ by comparing the mutual informations I(Y; X) and I(Y; Z)
between the common input Y and the channel outputs X and Z .
Definition 5. μ is said to be more capable than κ, denoted μ Wmc κ, if I(Y; Z) ≥ I(Y; X) for all
probability distribution on Y .
12
Under review as a conference paper at ICLR 2019
It follows from the data processing inequality that μ WY K =⇒ μ Wmc κ. However, the converse
implication is not true in general (Korner & Marton, 1975).
The converse to the Blackwell’s theorem states that if the relation Z W0Y X does not hold, then there
exists a set of actions A and a loss function '(y, a) ∈ Ry×a such that R(∏γ, κ,') < R(∏γ, μ,').
Le Cam introduced the concept of a deficiency of μ w.r.t. K to express this deficit in optimal
risks (Le Cam, 1964) in terms of an approximation of K from μ via Markov kernels.
Definition 6. The Le Cam deficiency of μ w.r.t. K is
δ3, k) :=、Mf v、SUp kλ ◦ 〃y- KykTV,	(14)
λ∈M(Z;X) y∈γ
where ∣∣ ∙ ∣∣tv denotes the total variation distance.
When the distribution of the common input to the channels is fixed, one can define a weighted
deficiency (Torgersen, 1991, Section 6.2).
Definition 7. Given Y 〜∏γ, the weighted Le Cam deficiency of μ w.r.t. K is
δπ(μ, k):
inf Ey 〜∏γ k λ ◦ μy - Ky 11 TV .
λ∈M(Z;X) y Y	y y
(15)
Le Cam’s randomization criterion (Le Cam, 1964) shows that deficiencies quantify the maximal gap
in the optimal risks of decision problems when using the channel μ rather than k.
Theorem 8 (Le Cam (1964)). Fix μ ∈ M(Y; Z), K ∈ M(Y; Z) and a probability distribution ∏γ
on Y and write k'k∞ = maxy,a '(y, a). For every e > 0, δπ(μ, κ) < e if and only if R(∏γ, μ,') 一
R(∏y, κ,') < e k'k∞ for any set of actions A and any bounded loss function '.
Raginsky (2011) introduced a broad class of deficiency-like quantities using the notion of a gener-
alized divergence between probability distributions that satisfies a monotonicity property w.r.t. data
processing. The family of f -divergences due to Csiszar belongs to this class (Liese & Vajda, 2006).
Definition 9. The f -deficiency of μ w.r.t. K is
δf(μ,κ):
λ∈MnZ;X) SuYDf (Ky kλ ◦ μy),
(16)
Many common divergences, such as the KL divergence, the reverse-KL divergence, and the total
variation distance are f-divergences. When the channel μ is such that its output is constant, no
matter what the input, the corresponding f -deficiency is called f -informativity (Csiszar, 1972). The
f-informativity associated with the KL divergence is just the channel capacity which has a geometric
interpretation as an “information radius” (Csiszar & Korner, 2011).
We can also define a weighted f -deficiency of μ w.r.t. κ.
Definition 10. The weighted f -deficiency of μ w.r.t. K is
δf(μ,κ):
λ∈MnZ;X)Dfg kλ ◦ μylπγ),
(17)
Specializing to the KL divergence, we have the following definition.
Definition 11. The weighted output deficiency of μ w.r.t. K is
δπ (μ,κ) := λ∈miZ;X)DKL(Kkλ ◦ μlπY),
(18)
where the subscript o in δoπ emphasizes the fact that the randomization is at the output of the chan-
nel μ.
Note that δ∏ (μ, κ) = 0 if and only if Z WY X, which captures the intuition that if δ∏ (μ, κ) is small,
then X is approximately output-degraded from Z w.r.t. Y . Using Pinsker’s inequality, we have
6π(μ, K) ≤ J竽δ∏ (μ,^κ).
(19)
13
Under review as a conference paper at ICLR 2019
C The Unique Information B ottleneck
In this section, we give a new perspective on the Information Bottleneck paradigm using nonnega-
tive mutual information decompositions. The quantity we are interested in is the notion of Unique
information proposed in (Bertschinger et al., 2014). Work in similar vein include (Harder et al.,
2013) and more recently (Banerjee et al., 2018a) which gives an operationalization of the unique
information.
Consider three jointly distributed random variables Y , X, and Z. Y is the target variable of interest.
The mutual information between Y and X can be decomposed into information that X has about Y
that is unknown to Z (we call this the unique information of X w.r.t. Z) and information that X has
about Y that is known to Z (we call this the shared information).
I(Y;X) = UfI(Y;X\Z)+SfI(Y;X,Z) .	(20)
、-----{z----} 、------V-----}
unique X wrt Z shared (redundant)
Conditioning on Z destroys the shared information but creates complementary or synergistic infor-
mation from the interaction of X and Z .
I(Y; X|Z) = UfI(Y; X\Z) +	CfI(Y;X,Z)	.	(21)
、-----{-----}	、-----{-----}
unique X wrt Z complementary (synergistic)
Using the chain rule, the total information that the pair (X, Z) conveys about the target Y can be
decomposed into four terms.
I(Y; XZ) =I(Y;X)+I(Y;Z|X)	(22)
= UfI(Y;X\Z) +SfI(Y;X,Z) +UfI(Y;Z\X) +CfI(Y;X,Z).	(23)
U I , SI , and CI are nonnegative functions that depend continuously on the joint distribution of
(Y,X,Z).
For completeness, we rewrite the information decomposition equations below.
I(Y;X) = UI(Y;X\Z)+SI(Y;X,Z),	(24a)
I(Y;Z) = UfI(Y;Z\X) +SfI(Y;X,Z),	(24b)
^ ^ ^	-~~^ . ^ ^	-~~^ , ^
I(Y; X|Z) = UfI(Y;X\Z) +CfI(Y;X,Z),	(24c)
I(Y; Z|X) = UfI(Y;Z\X) +CfI(Y;X,Z),	(24d)
The unique information can be interpreted as either the conditional mutual information without the
synergy, or as the mutual information without the redundancy.
When Y - X - Z is a Markov chain, the information decomposition is
UfI(Y;Z\X) = 0,	(25a)
-~~^ .	^ ^ ^ ^ ^ ^ ^ ^ ^
UfI(Y;X\Z) =I(Y;X|Z) =I(Y;X)-I(Y;Z),	(25b)
SfI(Y;X,Z) =I(Y;Z),	(25c)
CfI(Y;X,Z) = 0.	(25d)
The Information bottleneck (Tishby et al., 1999) minimizes the following objective
LIB (e)= I (Y; XIZ) + βI (X ； Z),
(26)
over all encoders e ∈ M(X ; Z) : Y - X - Z. Since Y - X - Z is a Markov chain, the sufficiency
term in the IB objective depends on the pairwise marginals (Y, X) and (Y, Z), while the minimality
term depends on the (X, Z)-marginal. From equation 25b, it follows that one can equivalently write
the IB objective function as
LIB (e) = UfI(Y; X\Z) + βI(X; Z).	(27)
14
Under review as a conference paper at ICLR 2019
From an information decomposition perspective, the original IB is actually minimizing just the
unique information subject to a regularization constraint. This is a simple consequence of the fact
that the synergistic information CI(Y ; X, Z) = 0 (see equation 25d) when we have the Markov
chain condition Y - X - Z . Hence, one might equivalently call the original IB as the Unique
information bottleneck.
Appealing to classical Blackwell theory, Bertschinger et al. (2014) defined a nonnegative decompo-
sition of the mutual information I(Y ; XZ) based on the idea that the unique and shared information
should depend only on the pairwise marginals (Y, X) and (Y, Z).
Definition 12. Let (Y,X,Z)〜P, Y 〜∏γ and let K ∈ M(Y; X), μ ∈ M(Y; X) be two channels
with the same input alphabet such that PYX(y,x) = ∏γ(y)κy(x) and PYZ(y,z) = ∏γ(y)μy(z).
Define
∆P = Q ∈ PY×X×Z : QYX (y, x) = πY(y)κy(x),
qyz (y,z) = πy (y)μy(Z)},	(28a)
UI(Y; X\Z) = min IQ(Y;X|Z),	(28b)
Q∈∆P
UI(Y; Z\X) = min IQ(Y; Z|X),	(28c)
Q∈∆P
SI(Y; X, Z) = I(Y; X) -UI(Y;X\Z),	(28d)
CI(Y; X, Z) = I(Y; X|Z) - UI(Y; X\Z),	(28e)
where the subscript Q in IQ denotes that joint distribution on which the quantities are computed.
The functions UI, SI, and CI are nonnegative and satisfy equation 24. Furthermore, UI and SI
depend on the marginal distributions of the pairs (Y, X) and (Y, Z). Only the function CI depends
on the full joint distribution P.
UI satisfies the following intuitive property in relation to Blackwell’s theorem 4.
Proposition 13. (Bertschinger et al., 2014, Lemma 6) UI(Y; X\Z) = 0 ^⇒ Z WY X.
Proposition 2 follows from noting that δπ (d, κ) = 0 q⇒ UI(Y; X\Z) = 0 (Bertschinger et al.,
2014, Theorem 22) and the fact that UI(Y; X\Z) = I(Y; X|Z) when Y - X - Z is a Markov
chain.
D Additional figures on VDB experiments
1
0.9
0.8
0.7
>
o
巴0.6
自0.5
0.4
0.3
0.2
0.1
MNIST train test accuracy, VDB, L=12
—	M—M = 1
-	ɪ- -M = 1 train
[ M = 3
—	壬--M = 3 train
1 M = 6
-ɪ- - M = 6 train
—M—M = 12
M = 12 train
100
10-8	10-6	10-4	10-2
Figure 5: Train and test accuracy of the VDB for L = 3 and L = 12. Similar to Figure 2.
15
Under review as a conference paper at ICLR 2019
Figure 6: Evolution of the mutual information between representation and output vs. representation
and input (values farther up and to the left are better) over 200 training epochs (dark to light color)
on MNIST. The curves are averages over 20 repetitions of the experiment. At early epochs, training
mainly effects fitting of the input-output relationship and an increase of I(Z; Y ). At later epochs,
training mainly effects a decrease of I(Z; X), which corresponds to the representation increasingly
discarding information about the input. An exception is when the regularization parameter β is very
small. In this case the representation captures more information about the input, and longer training
decreases I(Z; Y ), which is indicative of overfitting to the training data. Higher values of M lead
to the representation capturing more information about the target, while at the same time discarding
more information about the input. M = 1 corresponds to the Variational Information Bottleneck.
16
Under review as a conference paper at ICLR 2019
E Unsupervised representation learning objectives
Recent work on variational autoencoders (VAEs) has shown that optimizing the standard evidence
lower-bound (ELBO) is not sufficient in itself for learning useful representations (Chen et al., 2016;
Alemi et al., 2018; Zhao et al., 2017). Generalizing the ELBO by incorporating different bottleneck
constraints (Zhao et al., 2017; Higgins et al., 2017; Rezende & Viola, 2018) has shown promise in
learning better latent codes.
In this section, we discuss some preliminary results on an unsupervised version of the VDB objec-
tive. We discuss its relation to VAE objectives such as the β-VAE (Higgins et al., 2017) and the
importance weighted autoencoder (IWAE) (Burda et al., 2015).
E.1 β-VAE AND IWAE
A variational autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014) is a directed
probabilistic model that defines a joint density pθ(x, Z) = pθ(x∣z)p(z) between some continuous
latent variable z and observed data x. p(z) is chosen to be a simple prior distribution over the latents
(e.g., isotropic unit Gaussian) and pθ(x|z) is a decoder network that models the data generative
process. Maximum likelihood estimation of the model parameters θ is in general intractable. VAEs
instead maximize the evidence lower-bound (ELBO) by jointly training the decoder with an auxiliary
encoder network qφ(z∣χ), parameterized by φ. The ELBO objective is
LELBO (X)= Ez"(z∣Jlog pθ⅛≡ 1	(29)
L	qφ(z|x) J
=logPθ(X)- DKL(qφ(z∣x)kpθ(z∣x)) ≤ logPθ(x).	(30)
The ELBO is optimized by sampling from qφ(z∣x) using the reparameterization trick to obtain low
variance Monte Carlo estimates of the gradient.
The ELBO is tight when qφ(z∣x) matches the true posterior pθ(z|x). The tightness of the bound
is coupled to the expressiveness of the encoder distribution. When qφ(z∣χ) is chosen as a simple
diagonal Gaussian, minimizing DκL(qφ(z∣χ)kPθ(z|x)) encourages the model,s posterior to be ap-
proximately factorial which limits the capacity of the model. The importance weighted autoencoder
(IWAE) (Burda et al., 2015) addresses this issue. The key observation is that the unified ELBO
objective in equation 29 is the log of a single (unnormalized) importance weight pθ(χ,Z) with a pro-
posal density defined by qφ(z∣χ). Using more samples from the proposal can only tighten the bound.
The M -sample IWAE bound is
LM (X)= EzLM〜QM=I qφ(z(i)∣x)
Pθ (x,z(i))∖^∣
qφ(z(i)|x) J∖ .
(31)
L1 is just the ELBO and limM→∞ LM = logpθ(X). The M -sample bound can alternatively be
written as (Le et al., 2017)
LM (X) = logpθ (X) - DKL(QISkPIS) ≤ logpθ(X),
(32)
where QIS and PIS are, resp., proposal and target densities defined on an extended sample space.
1M
Qis(z1:M) = MM ∏qφ(zc⅜),
M i=1
PIS(z1:M
i=1
QIS (Z1:M )
qφ(z⑴ |x)
Pθ (z(i)∣x).
(33)
Optimizing over this extended sample space allows for more flexible decoders and gives the IWAE
additional degrees of freedom to model complex posteriors. The IWAE bound is in fact equivalent
to the ELBO in expectation with a more complex proposal density GIW defined by importance
reweighting (Bachman & Precup, 2015; Cremer et al., 2017; Naesseth et al., 2018).
LM (x) = Ez2：M 〜QM=2 qφ(z(i)∣x)Ez 〜gw (z∣x,z2M)
l (	Pθ(x, z)
,°g ∖Ww(z∣x,z2:M)
where the inner expectation is w.r.t. the unnormalized distribution qGIW defined as follows.
GIw(z∣x,z2:M):= Mwqφ(z∣x), where W
Pθ (X,Z)
qφ(ZIx)
PM Pθ (x,z j))
j=1 = Φ qφ(Zj)Ix)
(34)
(35)
17
Under review as a conference paper at ICLR 2019
For M = 1, qιw(z∣χ) = qφ(z∣χ), and the unified ELBO objective admits the following decompo-
sition.
LELBO (X)= Ll(x) = Ez 〜qφ(z∣χ) [log Pθ (x|z)] - DKL (qφ (z |x) ∣∣p(z)).	(36)
The first term can be interpreted as an expected reconstruction cost and the second term as a regu-
larizer (Kingma & Welling, 2013). For M > 1 however, the IWAE bound admits no such decom-
position. As M → ∞, qιw approaches the true posterior pθ(z|x). However, the magnitude of the
gradient w.r.t. the encoder parameters also decays to zero as more samples are used (Rainforth et al.,
2018). This potentially limits the IWAE’s ability to learn useful representations.
The β-VAE (Higgins et al., 2017; Burgess et al., 2018) augments the ELBO by incorporating a
hyperparameter β for the regularization term.
Le (X) = Ez 〜qφ(z∣x) [log Pθ (x|z)] - βDKL(qφ(zlx)kP(Z))	(37)
For β = 1, we recover the standard ELBO. Higgins et al. (2017) showed that when the priorp(z) and
the encoder qφ(z∣χ) are chosen as diagonal Gaussians, reducing the capacity of the latent bottleneck
by choosing β > 1 incentivizes the latent representations to be more disentangled. Higher values
of β also results in a more coherent latent space so that the reconstructions interpolate smoothly on
latent traversals.
E.2 Unsupervised Deficiency Bottleneck Objective
We now discuss some preliminary results on an unsupervised version of the VDB objective on the
MNIST and Fashion-MNIST datasets. We consider a standard VAE model with the prior p(z) and
the encoder qφ(z∣χ) parameterized by diagonal Gaussians. The encoder has two hidden layers with
200 units each. The decoder is a factorized Bernoulli parameterized by MLP’s with two hidden
layers with 200 units each. Using factorial decoder densities constrains the model to use the latent
code to attain a high likelihood (Chen et al., 2016; Alemi et al., 2018). This is simple way to achieve
a nonvanishing mutual information between the latent variable and the input. This is important in
our setting since we are interested in learning a useful representation.
We train the model by minimizing the following unsupervised version of the VDB objective.
1 N I"	1 M	一
NFE -log( M∙∑>θ (x(i)lf(x(i)，e(j)))]) + βDκL(qφ(Z∣χ(i))kp(Z)) .	(38)
N i=1	M j=1
We note that the β-VAE has a similar-looking training objective, with the only difference that the
averaging w.r.t. to the posterior samples is outside the log. In particular, if M = 1, this is just the
β-VAE objective. The objective in equation 38 also shares some superficial similarities with the
IWAE objective for β = 1. Note however, as discussed in Section E.1, we cannot decompose the
IWAE objective for M > 1. In particular, this implies we cannot trade-off reconstruction fidelity
for learning more meaningful representations by incorporating bottleneck constraints. We have not
explored if using more complex posteriors such as the qqIW is possible in the bottleneck formulation.
For training, we choose a mini-batch size of N = 100 and draw M = 1, 3, 6 samples from the
approximate posterior by using the reparameterization trick (Kingma & Welling, 2013). For our
choice of Gaussian prior and encoder, the KL term can be computed and differentiated without
estimation. We estimate the expected loss term using Monte Carlo sampling. Since the expectation
is inside the log, higher values of M increases the variance of gradient estimates. Also numerically
handling the log requires some care. We used the log-sum-exp trick to compute the expectation. For
values of M beyond 12, we observe some degradation in the visualizations.
18
Under review as a conference paper at ICLR 2019
E.3 Visualizations
β= .05
β=0.5
Figure 7: Sampling grids in latent space for M = 6 for different values of β for the MNIST. Higher
values of β results in a more coherent latent space.
β=1.5
β= .05, M = 1
β=1.5, M =1
β=.05, M =3
β=1.5, M =3
Figure 8: The latent space (mean values of the posterior for 5000 test examples) for the FMNIST
for different values of M and β . M = 1 corresponds to the β-VAE.
β=.05, M =6 β= 1.5, M =6
Figure 9: FMNIST reconstructions for different values of M and β . At low values of β, we have
good reconstructions. M = 1 corresponds to the β-VAE.
19