Under review as a conference paper at ICLR 2019
Riemannian Stochastic Gradient Descent for
Tensor-Train Recurrent Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The Tensor-Train factorization (TTF) is an efficient way to compress large weight
matrices of fully-connected layers and recurrent layers in recurrent neural net-
works (RNNs). However, high Tensor-Train ranks for all the core tensors of pa-
rameters need to be element-wise fixed, which results in an unnecessary redun-
dancy of model parameters. This work applies Riemannian stochastic gradient
descent (RSGD) to train core tensors of parameters in the Riemannian Manifold
before finding vectors of lower Tensor-Train ranks for parameters. The paper
first presents the RSGD algorithm with a convergence analysis and then tests it
on more advanced Tensor-Train RNNs such as bi-directional GRU/LSTM and
Encoder-Decoder RNNs with a Tensor-Train attention model. The experiments
on digit recognition and machine translation tasks suggest the effectiveness of the
RSGD algorithm for Tensor-Train RNNs.
1	Introduction
Recurrent Neural Networks (RNNs) are typically composed of large weight matrices of fully-
connected and recurrent layers, thus massive training data as well as exhaustive computational re-
sources are required. The Tensor-Train factorization (TTF) aims to reduce the redundancy of RNN
parameters by reshaping large weight matrices into high-dimensional tensors before factorizing them
in a Tensor-Train format Oseledets (2011). The notation of Tensor-Train usually suggests that TTF
is applied for the tensor representation of model parameters. Tensor-Train was initially applied to
fully-connected layers Novikov et al. (2015), and it has been recently generalized to recurrent layers
in RNNs such as LSTM and GRU Yu et al. (2017). Compared with other tensor decomposition
techniques like the CANDECOMP/PARAFAC decomposition Kolda & Bader (2009) and Tucker
decomposition Kim & Choi (2007), Tensor-Train can be easily scaled to arbitrarily high dimensions
and have the advantage of computational tractability to significantly large weight matrices.
Given a vector of Tensor-Train ranks r = (ri, τ2, ∙ ∙ ∙, rd+ι), TTF decomposes a d-dimensional
tensor W ∈ R(m1 ∙nι)×(m2∙n2)×∙∙∙×(md∙nd) into a multiplication of core tensors according to (1),
where the k-th core tensor C[k] ∈ Rrk×mk×nk×rk+1, and any index pair (ik , jk ) satisfies 1 ≤ ik ≤
mk, 1 ≤ jk ≤ nk. Additionally, the ranks r1 and rd+1 are fixed to 1.
W((i1, j1), (i2,j2), ..., (id,jd)) = C[1] (T1,i1, j1,T2)C[2] (T2,i2,j2,τ3>“c[d](Td, id, jd,τd+I)(I)
Thus, when the TTF technique is applied to the fully-connected (FC) layer with feed-forward weight
matrix W, a tensor W is firstly converted from W and is then decomposed to a multiplication of the
core tensors as shown in (2), where C[t] is the t-th core tensor, X denotes a tensor of input, B refers
n ×n ××n
to a tensor of bias, the tensor of outputs Y ∈ Rl 2	d, and σ is a sigmoid function. For clarity,
the notation TTL(W, X) is used to simplify the representation of a Tensor-Train fully-connected
layer, which is shown in (3).
ml	md d
Yji,j2,…Jd) = σ( X …X Y CItt(Tt, it, jt, τt+1)x(i1,i2,…,id) + B(ji,j2,…Jd))⑵
il=1	id=1 t=1
,ʌ ..
Y = σ(TTL(W, X)).	(3)
1
Under review as a conference paper at ICLR 2019
Likely, we use (4) to represent an RNN with a feed-forward weight matrix W and a recurrent weight
matrix U . In (4), Xt is an input matrix at time t, ht-1 and ht separately denote the hidden vectors
of time t - 1 and t, and σ refers to the sigmoid function.
ht =σ(TTL(W,Xt)+TTL(U,ht-1)).	(4)
The largest benefit from Tensor-Train models is the capability to reduce the model parameters
tremendously. For example, a Tensor-Train FC layer needs Pi mi ∙ n ∙ r ∙ r+ι parameters in
total. In comparison, the total number of parameters of an FC layer is about O(Qi mi ∙ Qi ni),
which is much larger than the associated Tensor-Train FC one.
The Tensor-Train models are found widespread. For example, Yang et al. (2017) set up Tensor-Train
Recurrent Neural Networks for video classification, and Yu et al. (2017) applied the Tensor-Train
as an End-to-End dynamic model for multi-variate forecasting environmental data. However, those
applications focused on simple deep learning architectures, and a vector of Tensor-Train ranks r
is element-wise fixed. When more complex deep models with numerous Tensor-Train layers are
involved, it is not easy to find appropriate Tensor-Train ranks for each core tensor. However, when
applying a vector of shared and high Tensor-Train ranks to all parameters of Tensor-Train layers,
each of them has a vector of lower Tensor-Train ranks, which results in an additional decrease in the
number of parameters.
This work applies Riemannian Stochastic Gradient Descent (RSGD) to the RNN Tensor-Train lay-
ers. Unlike Stochastic Gradient Descent (SGD), which conducts the update of parameters in the
Euclidean space, RSGD finds optimal core tensors of parameters associated with a vector of lower
Tensor-Train ranks in Riemannian Manifold, thereby decreasing the number of total parameters of
Tensor-Train RNNs.
Our work is inspired by Bonnabel (2013) and Lubich et al. (2015), which are two excellent in-
troductions to the theory of Stochastic Gradient Descent on Riemannian Manifolds. Additionally,
Zhang et al. (2016) was a recent work that proposed a fast stochastic optimization on Riemannian
Manifolds. Besides, Absil et al. (2009) is a useful reference of Riemannian optimization.
Contributions. We summarize the key contributions of this paper as follows:
•	This work applies RSGD to iteratively find vectors of lower Tensor-Train ranks with the up-
date of parameters in the training process. The RSGD algorithm and the related theoretical
analysis are also presented.
•	We design Bi-directional Tensor-Train GRU/LSTM Chung et al. (2014), and Encoder-
Decoder Tensor-Train RNNs with a Tensor-Train Attention mechanism Luong et al. (2015).
We apply the RSGD algorithm to the Tensor-Train RNN models on the digit recognition
and machine translation tasks.
To the best of our knowledge, this is the first work that applies RSGD to train Tensor-Train RNNs
to find the optimal core tensors of parameters with vectors of lower Tensor-Train ranks. Moreover,
this is the first work that builds Tensor-Train RNNs with complex architectures for natural language
processing (NLP) tasks.
2 Riemannian Stochastic Gradient Descent
The optimization problem of Tensor-Train RNNs can be formulated as a Riemannian optimization
problem as shown in (5), where {X, Y } is a data sequence with length T, W represents a tensor of
parameters which lies in a d-dimensional Riemannian Manifold (M, μ) with a Riemannian measure
μ , and the Tensor-Train ranks for core tensors of W must be element-wise no higher than the vector
r = (r1, r2, ..., rd+1) as shown in (5).
min
W∈M
f(W;X,Y)
s.t.,tt_rank(W) ≤ r.
(5)
Besides, the Riemannian measure μ induces an inner product structure in each tangent space TxM
associated with a tensor X ∈ M. Specifically, ∀u, V ∈ TxM, the inner product < u, V >= μx (u, V).
2
Under review as a conference paper at ICLR 2019
Algorithm 1 Riemannian Stochastic Gradient Descent
1.	Given the labeled input data (X, Y ) with sequence length T , and the learning rate η.
2.	W(0) J the randomly initialized core tensors {C[1], C[2],..., C[d]}.
3.	Fort = 1,2,...,T:
4.	For i = 1, 2, ..., d:
5.	Choose a gradient gC[i] = VC[i] f (W(t); X, Y) in the tangent space 丸团 M.
6.	Ai = C[i] - ηgC[i].
[i]
7.	C J ExPc，] (Ai).
[i]	[i]
8.	(C , ri) J rounding(C ).
9.	W⑴ J{C[1],C⑶…C[d]}.
10.	r J -1,。2,...rd).
11.	Reshape the core tensors {C* [1] , C[2] * * * * , ..., C[d] } based on the updated r.
12.	Return W(T+1) = {C[1], C[2],…，C[d]}.
Similarly, μ induces the norm of U ∈ TxM as ||u|| = 'μx(u, U) ≥ 0. In addition, the μ induced
inner product and the norm preserve the basic properties like definiteness, homogeneity and triangle
inequality.
Algorithm 1 presents the RSGD Algorithm. The algorithm mainly consists of two main procedures:
one is the update of parameters in the tangent space and conducting an exponential mapping, and
the second one is the rounding to lower Tensor-Train ranks. As illustrated in Figure 1, step 5 firstly
obtains a gradient gC[i] on a tangent space TCiM at the core tensor C[i] in Riemannian Manifold
(M, μ), and step 6 conducts a gradient descent on the tangent space to generate a new tensor Ai.
[i]
Step 7 projects Ai back to C in Riemannian Manifold (M, μ) by an exponential mapping. Finally,
[i]
as shown in Figure 2, the rounding function in step 8 transforms C in the submanifold Sr to the
[i]
core tensor C with a vector of lower Tensor-Train ranks r in a new submanifold Sr. Note that the
vectors of Tensor-Train ranks r and r span two submanifolds Sr ⊂ M and Sr ⊂ M respectively.
After that, the next iteration of the the parameter update is conducted in Sr.
Figure 1: An exponential mapping.
Figure 2: The rounding procedure.
The exponential mapping in Algorithm 1 is formulated in (6). Unfortunately, it is not easy to solve
the problem because we have to deal with the calculus of variations, or we have to know the Christof-
fel symbols Lubich et al. (2015). Therefore, a fast and straightforward retraction method is applied
as a first-order approximation to the exponential mapping as shown in Algorithm 2.
min ||W - Y||F
Y∈Sr⊂Md
s.t.,tt_rank (Y) = r.
(6)
3
Under review as a conference paper at ICLR 2019
Algorithm 2 The Retraction Algorithm
1.	Given the core tensors {C[1], C[2], ∙∙∙, C[d]} and the Tensor-Train rank r = {r1,r2, ∙∙∙, rd+ι}.
2.	For i = 1 to d:
3.	Ai J reshape C[i] by the shape of ⑺∙ ni ∙ mi) X ri+ι.
4.	For i = 1 to d:
5.	Ai, ∑i J QR_decomposition(Ai).
6.	C[i] J reshape Ai by (ri, n, mi, ri+ι).
7.	Ai+1 J multiply(Σi, Ai+1), if i < d.
8.	Return {C[1], C[2],…,C[d]}.
Algorithm 3 The Rounding Algorithm
1.	Given the core tensors {C[1], C[2],…,C[d]} and a constant maximum rank rmax.
2.	Initialize r = {1,1,..., 1}.
3.	For i = 1 to d:
4.	Ai	J reshape C[i]	by the shape of (%	∙	n%	∙ m/	×	%+1
5.	For i = 1 to d:
6.	coLnum(C[i]) = ni ∙ mi ∙ ^i+ι.
7.	row_num(C[i]) = ri∖ ri+1.
^i+ι
8.	^i+ι = min(rmaχ, col,num(C[i]), row」num(C[i])).
9.	(Ui,Si,Vi)=SVD(Ai).
..	q_	__-	rd	_ -	r ʌ.	__-	r
10.	Ui	=	Ui[:,	1 :	ri+ι],	Si	= Si[1	:	ri+ι, 1:	^i+ι], V =	Vi[1: ^i+ι,:].
Λ	ʌ ʌ ʌ
11.	Ai = UiSiVi.
12.	C[i] J reshape Ai as 债小……办.
13.	Return the updated core tensors {C[1], C[2],…，C[d]}.
Algorithm 2 presents the retraction algorithm. The main idea of the retraction algorithm is to or-
thogonalize the core tensors {C[1], C[2],…,C[d]} in a left-to-right order by the QR decomposition.
The rounding algorithm is shown in Algorithm 3. Similar to the retraction algorithm, a left-to-right
tensor matricization is firstly initialized. Then, the Tensor-Train rank is updated before conducting
an SVD computation. The returned core tensors {C[1], C[2],…,C[d]} are based on the updated
vector of lower Tensor-Train ranks. Furthermore, the rounding procedure has the property presented
in Proposition 1.
Proposition 1. The rounding procedure of Algorithm 3 does not change values of the objective
function f for the Tensor-Train RNNs. That is, for a tensor x ∈ Sr, the rounding tensor x ∈ S^, we
have f (X) = f (X).
Proof. Given the weight matrix W with core tensors {C[1], C[2], ∙ ∙ ∙, C[d]}, for an input tensor
X∈M,we obtain (7) and (8) according to (1).
m1	md d
Y(ji,j2,…,jd) = X …XY C[t] (rt, it,jt, rt+1)X(i1,i2,…，id) + B(j1,j2,…j	⑺
i1=1	id=1 t=1
m1	md d
=X …XYc[t](rt,it,jt,rt+ι)X(iι,i2,…，id) + Bj1,j2,…j)	⑻
i1=1	id=1 t=1
which suggests that a vector of Tensor-Train ranks determines a submanifold for generating core
tensors of the tensor W, but the values of the objective functions are invariant to the change of the
vector of Tensor-Train ranks, obtaining Proposition 1.	□
4
Under review as a conference paper at ICLR 2019
3 Convergence Analysis
This section analyzes the convergence of the RSGD algorithm. The necessary definitions and the-
orems are firstly introduced, and the analysis is then provided. Since the objective functions of
Tensor-Train RNNs are always geodesically non-convex Zhang et al. (2016), we only consider the
convergence of RSGD for non-convex cases.
Definition 2. A different function f : M → R is geodesically L-smooth if its first-order gradient is
geodesically L-Lipchitz continuous. Specifically, ∀x, y ∈ M we have 9.
f(y) ≤ f(χ)+ < gχ,Eχp-1(y) > + L∣∣Eχpχ1(y)∣∣2,	(9)
where gx is the sub-gradient of f(x) at x in the tangent space TxM, and Expx-1(y) is the inverse
exponential mapping which projects the curve line in Md from x to y back to the gradient x in the
tangent space Tx M .
From the RSGD algorithm (Algorithm 1), it is not hard to find the sub-gradient gχ = Vf (x) and
Eχp-1(y) = -ηVχf (x), and thus Theorem 3 can be derived.
Theorem 3. For a differentiable and geodesically L-smooth function f, the Riemannian stochastic
gradient descent algorithm ensures (10).
mtinE|||Vf(Xt)||] ≤ ηT(f(XO)- f(Xio) + Lη2k2,	(IO)
where T refers the total iterations, xo and Xi denote the initial and the optimal points respectively,
η is the learning rate, and ||Expx-1(y)||2 is bounded by η2k2.
Proof. Assume xi and x0 separately refer to the optimal and initial points. For all xt and xt+1 at
two consecutive times, we can derive (11) based on Definition 2.
E[f(Xt+1)] ≤ Ef(Xt)] + E[< Vf(Xt),ExPxt1(Xt+1) >] + 2E[||ExPx1(Xt+1)||2].	(II)
By applying Expx-t1(Xt+1) = -ηVf (Xt)) and E[||Expx-t1(Xt+1)||2] ≤ η2k2, we obtain (12),
where Xt+ι is the rounding tensor of X^t+ι. The Proposition 1 ensures that f (Xt+ι) = f (X^t+ι).
E[∣∣Vf (Xt)∣∣] ≤ 1 E[f(Xt) - f(Xt+ι)] + Lη2k2 = 1 E[f(Xt)- f(Xt+ι)] + Lη2k2.	(12)
η	2η	2
By summing the two sides of equation 12 from 0 to T - 1, we derive (13).
1 T-1	1	L
mtinE[||Vf(Xt)∣∣] ≤ T 三 E[∣∣Vf(g)∣∣] ≤ ηT(f (xo) - f (xt)) + 2η2k2.	(13)
Since f(Xt) ≤ f(XT), ∀0 ≤ t ≤ T -1, we have 14.
mtinEMVf(Xt)||] ≤ ηT(f(χo) - f(XT)) + Lη2k2 ≤ ηT(f(χo) - f(χ*)) + Lη2k2. (14)
After rounding Xi to Xi , we finally obtain the result 15 of Theorem 3.
mtinE[∣∣Vf(xt)∣∣] ≤ %(f(xo) - f(Xi)) + Lη2k2.	(15)
□
Furthermore, Theorem 3 suggests that the number of iterations T satisfies (16) before reaching the
convergence.
T ≤	f (XO) — f (Xi)
一η(minE[||Vf(xt)∣∣] - Lη2k2)
(16)
5
Under review as a conference paper at ICLR 2019
4	Advanced Tensor-Train RNNs
This section introduces the Tensor-Train RNNs with the advanced architectures used in this work.
The new Tensor-Train architecture is based on the Bi-directional Tensor-Train GSU/LSTM. The
other one is the Tensor-Train Encoder-Decoder RNNs with the Tensor-Train Attention mechanism.
4.1	Bi-directional Tensor-Train GRU/LSTM
Firstly, we introduce the Bi-directional Tensor-Train GRU/LSTM, which involves twice more pa-
rameters than the Tensor-Train GRU/LSTM. Equations from (17) to (24) present the functional
mechanisms of the Bi-directional Tensor-Train Gru, where the pairs (→,行)and (→, Zt) denote
forward and backward update gate operations, respectively, and the pair (dt, dt) represents the
forward-backward reset gate operations. In addition, the pair (ht, ht) is the memory cell holding
information of last times. oh is the output of the Bi-directional Tensor-Train GRu which combines
ht and ht by concatenating them before feeding through a Tensor-Train linear layer.
-→rt	=σ(TTL(-W→r, -X→t)+TTL(-→Ur,h--t-→1))	(17)
Zr-t	=σ(TTL(ZW-r,XZ-t)+TTL(ZU-r,hZt---1))	(18)
-→zt	=σ(TTL(-W→z, -X→t)+TTL(-→Uz,h--t-→1))	(19)
Zz-t	=σ(TTL(ZW-z,XZ-t)+TTL(ZU-z,hZt---1))	(20)
tanh(TTL(-W→d, -X→t) +TTL(-→Ud,(-→rt ◦h--t-→1)))		(21)
tanh(TTL(ZW-d,XZ-t) +TTL(ZU-d,(Zr-t ◦hZt---1)))		(22)
-→zt)	◦ h--t-→1 +	-→zt	◦	-→dt,	hZ-t	= (1	- Zz-t )	◦ hZt---1 +	Zz-t	◦	Zd-t	(23)
	Oh = σ(TTL(Ah, [→; Q)).	(24)
In addition, we also design a Bi-directional Tensor-Train LSTM which involves more operational
gates than the Bi-directional Tensor-Train GRu.
4.2	The Tensor-Train Encoder-Decoder RNNs with Attention Models
The Encoder-Decoder RNNs are commonly used in sequence-to-sequence deep learning appli-
cations. Moreover, the attention mechanism significantly improves the performance of Encoder-
Decoder RNNs Vaswani et al. (2017).
The Bi-directional Tensor-Train RNN like GRu or LSTM is used to construct the Encoder-Decoder
architecture. Moreover, we set up the Tensor-Train Attention model in addition to the Tensor-Train
Encoder-Decoder RNNs. Thus, the entire model is built on Tensor-Train layers.
To build a Tensor-Train Attention model, it is necessary to add the Tensor-Train layer to generate an
Attention vector as shown in (25), where ct is a context vector (26) with attention weights αts (27),
at is the output of the Attention model at time t, hS denotes a vector built from the outputs of the
forward and backward stages, and ht refers to the output of the hidden layers at time t.
at = tanh(TTL(Wc, [ct; ht]))	(25)
ct = Eatsh s
s
exp(score(ht, hs))
Qts = —s-----------------二---
Eso=ι exp(score(ht, hso))
(26)
(27)
5	Applications
This section first introduces the implementation of the Tensor-Tensor RNNs. Then, we present two
applications where the RSGD algorithms were tested. One application is the digit recognition task
on the sequential MNIST dataset; the other is the task of machine translation on the Multi30K dataset
Elliott et al. (2016).
6
Under review as a conference paper at ICLR 2019
5.1	Implementations
We employed PyTorch to implement our Tensor-Train RNNs. The data structures of our imple-
mentations were partly built on the free Tensor-Train toolkit implemented by the tool Tensorflow
Novikov et al. (2018). However, we employed the tool PyTorch to take the advantage of dynamic
graph generation, which is much more useful for NLP tasks.
5.2	Digit Recognition on the MNIST dataset
The first application is the digit recognition task on the MNIST dataset. The dataset consists of
60000 data With 28 * 28 pixels for each digital image. Instead of vectorizing the image pixels into
a long vector as an input for a static deep neural network, image pixels are taken as data sequence
Where the time step is set to 28, and the input dimension is set to 28. In our experiments, the training
and testing sets Were separately composed of 50000 and 10000 data. 2000 data Were selected from
the training set for building a validation set, and they Were not included in the training set.
As for the experimental setup, We applied both Bi-directional Tensor-Train GRU and Bi-directional
Tensor-Train LSTM to the task. The dimension of the Manifold (M, μ) was set to d = 3, and the
vector of Tensor-Train ranks Was initialized With high and shared values r = (1, 10, 10, 1) for the
Tensor-Train layers. The weight matrix of the input-hidden Tensor-Train layer was converted to
the tensor with the shape (2 × 7 × 2) by (6 × 6 × 6), and the weight matrix of the hidden-hidden
Tensor-Train layer was converted to the tensor with the shape (6 × 6 × 6) by (6 × 6 × 6). The RSGD
algorithm with a learning rate 0.01 was applied to both Bi-directional Tensor-Train GRU/LSTM.
The results are shown in Figure 3, where we compared the Bi-directional Tensor-Train GRU/LSTM
with the traditional Bi-directional GRU/LSTM regarding recognition error rates and number of pa-
rameters. Inspecting the recognition error rate, the Bi-directional Tensor-Train GRU obtains a result
that is close to that of the traditional Bi-directional GRU/LSTM, and the performance of the Bi-
directional Tensor-Train LSTM becomes a bit worse. Regarding comparison with the number of
parameters, both Bi-directional Tensor-Train GRU/LSTM can significantly reduce the number of
parameters by taking only 1% parameters of the Bi-directional GRU/LSTM at final. Notably, the
RSGD algorithm further reduces the number of parameters of Bi-directional GRU/LSTM by lower-
ing the Tensor-Train ranks.
Figure 3: Experimental results on the sequential MNIST dataset (‘TT’ denotes Tensor-Train,
‘params.’ means the number of parameters, ‘init.’ means the initial, and ‘final’ refers to the final
number).
7
Under review as a conference paper at ICLR 2019
Table 1: Statistics of Model Parameters
Models	Parameters	of	hidden	layers
Bi-directional Encoder-Decoder	462144
Tensor-Train Bi-directional Encoder-Decoder (initial)	15154
Tensor-Train Bi-directional Encoder-Decoder (final)	11088
5.3 Machine Translation on the Multi30k Dataset
The next application is a machine translation task from Dutch to English on the Multi30K dataset.
In the dataset, there are separately 29000, 1014, and 1000 sentence pairs for training data, validation
data, and test data, respectively.
The Bi-directional Tensor-Train GRU was used to build the Encoder-Decoder architecture, and a
Tensor-Train Attention model was added to the architecture. The RSGD algorithm with a learning
rate 0.01 was applied to the Tensor-Train Encoder-Decoder RNNs with the Tensor-Train Attention
model. An initial vector of Tensor-Train ranks was set as r = (1, 6, 6, 6, 1), and the weight matrix
of hidden layers was converted to the tensor with the shape of (4 × 4 × 4 × 4) by (4 × 4 × 4 × 4).
Besides, the baseline was based on the traditional Bi-directional GRU and LSTM-based Encoder-
Decoder architecture with the Attention model, where the Stochastic Gradient Descent algorithm
with learning rate 0.001 was used to update parameters.
The experimental results are shown in Figure 4, and the statistics of parameters of hidden layers
are shown in Table 1. The results in Figure 4 suggest that the Tensor-Train Encoder-Decoder RNN
performs closer or even better than the Encoder-Decoder one, although the convergence speed of
the Tensor-Train model is relatively slower in the first several iterations. On the other hand, Table 1
shows that RSGD leads to a further decrease in the number of parameters.
Figure 4: Experimental results of Machine Translation on the Multi30K dataset (PPL refers to the
logarithm of loss values).
6	Conclusions
This paper presents the RSGD algorithm for training Tensor-Train RNNs including the related prop-
erties, implementations, and convergence analysis. Our experiments on digit recognition and ma-
chine translation tasks suggest that RSGD can work effectively on the Tensor-Train RNNs regarding
performance and model complexity, although the convergence speed is relatively slower in the be-
ginning stages. Our future work will consider two directions: one is to apply the RSGD algorithm
to more Tensor-Train models and test it on larger datasets of other fields; and the second one is to
generalize Riemannian optimization to the variants of the SGD algorithms and study how to speed
up the convergence rate.
8
Under review as a conference paper at ICLR 2019
References
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.
Princeton University Press, 2009.
Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Trans. Automat.
Contr, 58(9):2217-2229, 2013.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. Multi30k: Multilingual english-
german image descriptions. arXiv preprint arXiv:1605.00459, 2016.
Yong-Deok Kim and Seungjin Choi. Nonnegative tucker decomposition. In IEEE Conference on
Computer Vision and Pattern Recognition., pp. 1-8. IEEE, 2007.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
Christian Lubich, Ivan V Oseledets, and Bart Vandereycken. Time integration of tensor trains. SIAM
Journal on Numerical Analysis, 53(2):917-941, 2015.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural
networks. In Advances in Neural Information Processing Systems, pp. 442-450, 2015.
Alexander Novikov, Pavel Izmailov, Valentin Khrulkov, Michael Figurnov, and Ivan Oseledets. Ten-
sor train decomposition on tensorflow. arXiv preprint arXiv:1801.01928, 2018.
Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295-
2317, 2011.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998-6008, 2017.
Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for
video classification. arXiv preprint arXiv:1707.01786, 2017.
Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue. Long-term forecasting using tensor-
train rnns. arXiv preprint arXiv:1711.00073, 2017.
Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian SVRG: Fast stochastic optimization on
riemannian manifolds. In Advances in Neural Information Processing Systems, pp. 4592-4600,
2016.
9