Under review as a conference paper at ICLR 2019
Difference-Seeking Generative Adversarial
Network
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel algorithm, Difference-Seeking Generative Adversarial
Network (DSGAN), developed from traditional GAN. DSGAN considers the sce-
nario that the training samples of target distribution, pt , are difficult to collect.
Suppose there are two distributions Pd and Pd such that the density of the target
distribution can be the differences between the densities of Ipd and Pd. We show
how to learn the target distribution pt only via samples from Pd and Ipd (relatively
easy to obtain). DSGAN has the flexibility to produce samples from various target
distributions (e.g. the out-of-distribution). Two key applications, semi-supervised
learning and adversarial training, are taken as examples to validate the effective-
ness of DSGAN. We also provide theoretical analyses about the convergence of
DSGAN.
1	Introduction
In machine learning, how to learn a probability distribution is usually conducted in a unsupervised
learning manner. Generative approaches are developed for learning data distribution from its sam-
ples and thereafter produce novel and high-dimensional samples from learned distributions, such as
image and speech synthesis (Saito et al. (2018)). The state-of-the-art approaches is so-called Gen-
erative Adversarial Networks (GAN)(Goodfellow et al. (2014)). GAN produces sharp images based
on a game-theoretic framework, but can be tricky and unstable to train due to multiple interacting
losses. Specifically, GAN consists of two functions: generator and discriminator. Both functions
are represented as parameterized neural networks. The discriminator network is trained to classify
whether or not inputs belong to real data or fake data created by the generator. The generator learns
to map a sample from a latent space to some distribution to increase the classification errors of dis-
criminator. GAN corresponds to a minimax two-player game, which ends if the generator actually
learns the real data distribution. The generator is of main interest because the discriminator will be
unable to differentiate between both distributions once the generator has been trained well.
1.1	Motivations
In reality, it is difficult to collect training samples from unseen classes, which none of their samples
involves the training phase, but their samples could be encountered in the testing phase. How to
reject or recognize unseen data as ”abnormal“ (not belonging to the training data) is an important
issue known as one-class classification (Ruff et al. (2018)). Due to the absence of unseen data, most
of algorithms are unsupervised (Scholkopf & Smola (2001)). Based on the assumption that there do
exist very few unseen examples, some approaches (Wu & Ye (2009)) focus on supervised learning
using unbalanced data. In addition to one-class classification, Dai et al. (2017) theoretically show
that complementary data, which is also considered as unseen data, can improve semi-supervised
learning. Another related issues is adversarial attack, where classifiers may be vulnerable to adver-
sarial examples, which are unseen during the training phase.
Fig. (8) in Appendix A illustrates the applications regarding unseen data. Apparently, if we can
generate unseen data via GAN, it is helpful for those applications. But, traditional GAN requires
preparing plenty of training samples of unseen classes for training, leading to the contradiction with
the prerequisite. This fact motivates us to design the proposed DSGAN, which can generate unseen
data by taking seen data as training samples shown in Fig. (1). The nuclear idea is to consider
the distribution of unseen data as the difference of two distributions, which both are relatively easy
to obtain. For example, out-of-distribution examples in the MNIST dataset, from another point of
view, are found to belong to the difference of the set of examples in MNIST and the universal set.
1
Under review as a conference paper at ICLR 2019
Examples in both sets are relatively easy to obtain. It should be noted that the target distribution
is equal to the training data distribution in traditional GANs; nevertheless, both distributions are
considered different in DSGAN.
Figure 1: The illustration of difference between traditional GAN and DSGAN.
1.2 Contributions
In this paper, we make the following contributions:
(1)	We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DS-
GAN), where the density of target distribution pt is the difference between those of any two
distributions, Pd and Pd. Nevertheless, the differences of two densities being negative are
not well-defined. Thus, instead of learning the target distribution pt directly, the genera-
tor distribution approximates Pt by minimizing the statistical distance between the mixture
distribution of Pd and the generator distribution, and pd.
(2)	Theoretical results based on traditional GAN are extended to the case of mixture distribu-
tion considered in this paper. With enough capacity of the generator and the discriminator,
we show DSGAN can learn the generator distribution Pg under mild condition where the
support set of Pg is the difference of support sets of Pd and Pd.
(3)	We show that DSGAN possesses the flexibility to learn different target distributions in
two key applications: semi-supervised learning and adversarial training. Samples from
target distribution in semi-supervised learning must satisfy two conditions: i) are linear
combination of any label data and unlabel data; ii) do not belong to neither label data nor
unlabel data. For adversarial training, samples from the target distribution are assigned as
out-of-distribution examples with bounded distortion. Experiments validate that DSGAN
can learn these two kinds of distributions well in various datasets.
The paper is structured as follows. In Sec. 2, we introduce DSGAN, including the algorithm in the
training procedure. Theoretical results are described in Sec. 3. In Sec. 4, two applications are taken
as examples to show the effectiveness of DSGAN. Finally, we present the experimental results in
Sec. 5 and conclude by pointing out some promising directions for future work in Sec. 7.
2 Proposed Method-DSGAN
2.1	The scheme of DSGAN
We denote the generator distribution as Pg and training data distribution as Pd , both in a N -
dimensional space. Let Pd be the distribution decided by user. For example, Pd can be the con-
volution of Pd and normal distribution. Let Pt be the target distribution which the user is interested
in, and it can be expressed as
(I - α)Pt(X) + αPd(X) = Pd(x),
(1)
2
Under review as a conference paper at ICLR 2019
where α ∈ [0, 1]. Our method, DSGAN, aims to learn pg such that pg = pt. Intuitively, our method
p (x) - αpd (x)
tries to learn Pg such that Pg (x)〜---------------.In other words, the generator is going to output
1-α
samples located in high-density areas of Pd and low-density areas of Pd.
At first, we formulate the generator and discriminator in GANs. The inputs z of the generator are
drawn from Pz (z) in an M -dimensional space. The generator function G(z; θg ) : RM → RN
represents a mapping to data space, where G is a differentiable function with parameters θg . The
discriminator is defined as D (x; θd) : RN → [0, 1] that outputs a single scalar. D (x) can be
considered as the probability that x belongs to a class of real data.
Similar to traditional GAN, we train D to distinguish the real data and the fake data sampled from
G. Meanwhile, G is trained to produce realistic data as possible to mislead D. But, in DSGAN,
the definitions of “real data“ and “fake data“ are different from those in traditional GAN. The sam-
ples from Ipd are considered as real but those from the mixture distribution between Pd and Pg are
considered as fake. The objective function is defined as follows:
min max V (G,D) = Ex〜「『⑺[log D(x)] + (1 - α)Ez〜Pz(Z) [log(1 - D (G (z)))]
G D	(2)
+ αEx〜pd(x) [log (1 - D(x))].
During the training procedure, an iterative approach like traditional GAN is to alternate between k
steps of training D and one step of training G. In practice, minibatch stochastic gradient descent via
back propagation is used to update θd and θg. In other words, for each of Pg, Pd and Pd, m sample
are required for computing gradients, where m is the number of samples in a minibatch. Algorithm 1
illustrates the training procedure in detail. DSGAN suffers from the same drawbacks with traditional
GAN (e.g., mode collapse, overfitting, and strong discriminator such that the generator gradient
vanishes). There are literatures (Salimans et al. (2016), Arjovsky & Bottou (2017), and Miyato et al.
(2018)) focusing on improving the above problems, and their ideas can be combined into DSGAN.
Li et al. (2017) and Reed et al. (2016) proposed the similar objective function like (2). Their goal
is to learn the conditional distribution of training data. Nevertheless, we aim to learn the target
distribution Pt in Eq. (1), not the training data distribution.
Algorithm 1 The training procedure of DSGAN using minibatch stochastic gradient descent. k is
the number of steps to apply to the discriminator. α is the ratio between Pg and Pd in the mixture
distribution. We used k = 1 and α = 0.8 in experiments.
01. for number of training iterations do
02. for k steps do
03.	Sample minibatch of m noise samples z(1), ..., z(m) fromPg(z).
04.	Sample minibatch of m samples x(d1), ..., x(dm) from Pd(x).
05.	Sample minibatch of m samples x(dd1), ..., x(ddm) from Pdd(x).
06.	Update the discriminator by ascending its stochastic gradient:
m
Vθd m X log D M) + log (1 - D (G (z(i))))
i=1
+log (1 - D (x(ddi)))i
07. end for
08. Sample minibatch of m noise samples z(1), ..., z(m) fromPg(z).
09. Update the generator by descending its stochastic gradient:
m
vθg m XhlOg(1 - D (G (四))i
i=1
10. end for
3
Under review as a conference paper at ICLR 2019
2.2 Case Study on Synthetic Data and MNIST
To get more intuitive understanding about DSGAN, we conduct several case studies on 2D synthetic
datasets and MNIST. Those results validate that DSGAN can learn the distribution we desire.
Figure 2: DSGAN generated
boundary points of a swissroll.
Figure 3: Left: A circle which the density is low in the center.
Right: Green points are generated by DSGAN.
Figure 4: The illustration about generating boundary data around training data. First, the convolution
of Pd and normal distribution makes the density on boundary data be no longer zero. Second, We
seek pg such that Eq. (1) holds, where the support set of pg is approximated by the difference of
those between Pd and of pd.
Low-density samples generation Fig. 2 illustrates that DSGAN is able to generate boundary sam-
ples on the 2D swissroll. Given the density function of the swissroll as Pd, we assign Pd as the
convolution of Pd and the normal distribution. Then, by applying DSGAN, we achieve our goal to
generate boundary samples. The intuition of our idea is also illustrated by a 1D example in Fig. 4.
In general, our idea will lead DSGAN to generate low-density samples like another example in Fig.
3. In this case, the density is low in the center of the circle, and our generator can not only create the
boundary samples but also the samples located in low-density area.
Difference-set generation We also validate DSGAN on high dimensional dataset such as MNIST.
In this example, we define Pd be the distribution of digit "1” and Pd be the distribution contains both
digits “1” and “7”. Since the density Pd(x) is high when x is digit “1”, the generator is prone to
output digit “7” with high probability.
From the above results, we can observe two properties of generator distribution Pg : i) the higher
density of Pd(x), the lower density of Pg (x); ii) Pg prefers to output samples from high-density
areas of Pd(X)-Pd(x). In those case studies, α = 0.8 in Eq. (1) is used.
In the next section, we will show that the objective function is equivalent to minimizing the Jensen-
Shannon divergence between the mixture distribution (Pd and Pg) and Pd as G and D are given
enough capacity. Furthermore, we provide a trick (see Appendix C in details) by reformulating the
objective function (2) such that it is more stable to train DSGAN.
3	Theoretical Results
There are two assumptions for subsequent proofs. First, in a nonparametric setting, we assume
both generator and discriminator have infinite capacity. Second, Pg is defined as the distribution of
the samples drawn from G(Z) under Z 〜Pz. We will first show the optimal discriminator given
G and then show that minimizing V (G, D) via G given the optimal discriminator is equivalent to
minimizing the Jensen-Shannon divergence between (1 - α)Pg + αPd and Pd.
4
Under review as a conference paper at ICLR 2019
Figure 5: Illustration of difference-set seeking in MNIST.
7 7⅜777Hr77^∙
7r-77*77l'7⅜1
777λ7t∕^t^z7t*7
777∙77t77777
£*??7777*777
7r7>H'7-n777
777t>779777
777ΓK77771 7
7t7777∕^>77
7777π7 7 777
Figure 6: DSGAN learn the
difference between two sets.
Proposition 1. For G being fixed, the optimal discriminator D is
DG (x)
pd(x)
pd(x) + (1 - α)pg(x) + αpd (x) .
Proof. See Appendix B.1 in details.
□
Moreover, D can be considered to discriminate between samples from Pd and those from
((1 - α)pg(x) + αpd(x)). By replacing the optimal discriminator into V (G, D), we obtain
C(G) = maxV(G,D)
Ex〜PXx) [log DG(x)] + (1 — α)Ez〜Pz(Z) [log (1 - DG (G(Z)))]
+ αEx〜pd(x) [log(1 - DG(X))]
Ex〜PXx) [log DG (x)]+ Ex〜(1-α)pg(x)+α〜Pd(x)—— DG (x))]	(3)
Pd(X)
x~p'x) θg Pd(X) + (1 - α)pg (x) + apd(x)_
+ Ex~(1 —α)Pg (x)+α~pd(x)
log	(1 - α)pg (x) + αpd(x)
.	Pd(X) + (1 - α)pg(x) + αpd(x)
where the third equality holds because of the linearity of expectation.
Actually, the previous results show the optimal solution of D given G being fixed in (3). Now, the
next step is to find the optimal G with DG being fixed.
Theorem 1. Suppose αPd(X) ≤ Pdd(X) for all X’s. The global minimum of the virtual training
criterion C(G) is achieved if and only if (1 - α)Pg (X) + αPd(X) = Pdd(x) for all X’s. At that point,
C(G) achieves the value - log 4.
Proof. See Appendix B.2 in details.
□
The assumption, αPd(X) ≤ Pdd(X) for all X’s, in Theorem 1 may be impractical in real applications.
We discuss that DSGAN still works well even though the assumption does not hold. There are
two facts: i) given D, V (G, D) is a convex function in Pg and ii) Due to
Pg (X)dX
x
1, the set
collecting all feasible solutions of Pg is a convex set. In other words, there always exists a global
minimum of V (G, D) given D, but it may not be - log(4). In this following, we show that the
support set ofPg is contained within the difference of support sets of Pdd and Pd while achieving the
global minimum such that we can generate the desired Pg by designing appropriate Pdd.
Proposition 2. Suppose αPd(X) ≥ Pdd for X ∈ Supp(Pd) and all density functions Pd(X), Pdd(X) and
Pg (X) are continuous. If the global minimum of the virtual training criterion C(G) is achieved, then
Supp (Pg) ⊆ Supp (Pdd) - Supp(Pd).
5
Under review as a conference paper at ICLR 2019
Proof. See Appendix B.3 in details.	□
In sum, the generator is prone to output samples located in high-density areas of Ipd and low-density
areas of pd .
Another concern is the convergence of Algorithm 1.
Proposition 3. The discriminator reaches its optimal value given G in Algorithm 1, and pg is
updated by minimizing
Ex 〜P(J(X) [log DG(X)] + Ex 〜(1 —α)Pg (x) + α 〜Pd(X) [log (I - DG (X))] .
If G and D have enough capacity, then pg converges to argmin JSD (P(J ∣∣ (1 一 α)pg + apd).
Pg
Proof. See Appendix B.4 in details.	□
4	Applications
DSGAN can be applied to two applications: semi-supervised learning and adversarial training. In
semi-supervised learning, DSGAN acts as a “bad generator”, which creates complement samples in
the feature space of real data. As for adversarial training, DSGAN generates adversarial examples
located in the low-density areas of training data.
4.1	Semi-supervised Learning
Semi-supervised learning (SSL) is a kind of learning model with the use ofa small number of labeled
data and a large amount of unlabeled data. The existing SSL works based on generative model (e.g.,
VAE (Kingma et al. (2014)) and GAN (Salimans et al. (2016))) obtain good empirical results. Dai
et al. (2017) theoretically show that good semi-supervised learning requires a bad GAN with the
objective function:
max Eχ,y〜L log PD (y | χ,y ≤ K) + Ex〜Pd(X) log PD (y ≤ K | χ)
D	(4)
+ Ex〜Pg (x) log Pg (K + 1 | x),
where (X, y) denotes a pair of data and its corresponding label, {1, 2, . . . , K} denotes the label
space for classification, and L = {(X, y)} is the label dataset. Moreover, in the semi-supervised
settings, the pd in (4) is the distribution of the unlabeled data. Note that the discriminator D in GAN
also plays the role of classifier. If the generator distribution exactly matches the real data distribution
(i.e., pg = pd), then the classifier trained by the objective function (4) with the unlabeled data cannot
have better performance than that trained by supervised learning with the objective function:
max Ex,y〜L log PD (y | χ,y ≤ K).	(5)
On the contrary, the generator is preferred to generate complement samples, which lie on low-
density area of pd. Under some mild assumptions, those complement samples help D to learn correct
decision boundaries in low-density area because the probabilities of the true classes are forced to be
low on out-of-distribution areas.
The complement samples in Dai et al. (2017) are complicate to produce. We will demonstrate that
DSGAN is easy to generate complement samples in Sec. 5.
4.2	Adversarial Training
Deep neural networks have impacted on our daily life. Neural networks, however, are vulnerable
to adversarial examples, as evidenced in recent studies (Papernot et al. (2016))(Carlini & Wagner
(2017)). Thus, there has been significant interest in how to enhance the robustness of neural net-
works. Unfortunately, if the adversary has full access to the network, namely white-box attack, a
complete defense strategy has not yet been found.
Athalye et al. (2018) surveyed the state-of-the-art defense strategies and showed that adversarial
training (Madry et al. (2018)) is more robust than other strategies. Given a trained classifier C
parameterized by θ and a loss function ` (X; y; Cθ), adversarial training solves a min-max game,
6
Under review as a conference paper at ICLR 2019
where the first step is to find adversarial examples within -ball for maximizing the loss, and the
second step is to train the model for minimizing the loss, given adversarial examples. Specifically,
the objective (Madry et al. (2018)) is
argmin E(x,y)〜L	max `( (x； y； Cθ) .	(6)
θ	,	δ∈[-, ]N
The authors used projected gradient descent (PGD) to find adversarial examples by maximizing the
inner optimization.
Instead of relying on PGD, our DSGAN generates adversarial examples directly, which are com-
bined into real training data to fine-tune Cθ . -ball in terms of `2 or `inf can be intuitively incorpo-
rated into the generation of adversarial examples.
5	Experiments
In this section, we demonstrate the empirical results about semi-supervised learning and adversarial
training in Sec. 5.1 and Sec. 5.2, respectively.
Note that, the training procedure of DSGAN can be improved by other extensions of GANs such as
WGAN (Arjovsky et al. (2017)), WGAN-GP (Gulrajani et al. (2017)), EBGAN (Zhao et al. (2017)),
LSGAN (Mao et al. (2017)) and etc. We use the idea of WGAN-GP in our method such that DSGAN
is stable in training and suffers less mode collapse.
5.1	DSGAN in Semi-supervised learning
Following the previous works, we apply the proposed DSGAN in semi-supervised learning on three
benchmark datasets, including MNIST (LeCun et al. (1998)), SVHN (Netzer et al. (2011)), and
CIFAR-10 (Krizhevsky (2009)).
We first introduce how DSGAN generates complement samples in the feature space. Specifically,
Dai et al. (2017) proved that if complement samples generated by G can satisfy the following two
assumptions in (7) and (8):
∀x 〜Pg(x), 0 > maχ WTf (x), and ∀x 〜Pd(X), 0 < maχ WTf (x)	(7)
where f is the feature extractor and wi is the linear classifier for the ith class and
∀X1 〜L,X2 〜Pd(x), ∃Xg 〜Pg(x) s.t. f(xg) = βf(xι) + (1 - β)f(X2)With β ∈ [0, 1],	(8)
then all unlabeled data will be classified correctly via the objective function (4).
The assumption in (8) implies the complement samples have to be at the space created by linear
combination of labeled and unlabeled data. Besides, they cannot fall into the real data distribution
Pd due to the assumption (7). In order to have DSGAN generate such samples, We let the samples of
K ʌ [∙	.. ,∙	f e 广 X O-	/ 、〜PN(X) — Md(X) ʌ	F
Rd be the linear combination of those from L and pd. Since Pg (x) ≈ ---------, the Pg Will
1-α
tend to match Ipd while the term -αPd ensures that samples from Pg do not belong to Pd. Thus, Pg
satisfies both assumptions in (7) and (8).
In practice, We parameterized f and all the W together as a neural netWork. The details of the
experiments, including the netWork models, can be found in Appendix D.
5.1.1	MNIST, SVHN, AND CIFAR- 1 0
For evaluating the semi-supervised learning task, We used 60000/ 73257/ 50000 samples and 10000/
26032/ 10000 samples from the MNIST/ SVHN/ CIFAR-10 dataset for training and testing, respec-
tively. Due to the semi-supervised setting, We randomly chose 100/ 1000/ 4000 samples from the
training samples as the MNIST/ SVHN/ CIFAR-10 labeled dataset, and the amount of labeled data
for all classes are equal.
Our criterion to determine the hyperparameters are in Appendix D.1. We perform testing With 10/ 5/
5 runs on MNIST/ SVHN/ CIFAR-10 based on the selected hyperparameters and randomly selected
labeled dataset. FolloWing Dai et al. (2017), the results are recorded as the mean and standard
deviation of number of errors from each run.
7
Under review as a conference paper at ICLR 2019
5.1.2	Main results
First, the hyperparameters we chose is depicted in Table 3 in Appendix D.1. Second, the results ob-
tained from our DSGAN and the state-of-the-art methods on three benchmark datasets are depicted
in Table 1.
It can be observed that our results can compete with state-of-the-art methods on the three datasets.
Moreover, in comparison with Dai et al. (2017), our methods don’t need to rely on an additional
density estimation network PixelCNN++ (Salimans et al. (2017)). Although PixelCNN++ is one
of the best density estimation network, it cannot estimate the density in the feature space, which is
dynamic during training. This drawback make the models in Dai et al. (2017) cannot completely
fulfill the assumptions in their paper.
Table 1: Comparison of semi-supervised learning between our DSGAN and other state-of-the-art
results. For fair comparison, We only consider the GAN-based methods. * indicates the use of the
same architecture of classifier. f indicates a larger architecture of classifier. ∣ indicates the use of
data augmentation.
Methods	MNIST (# errors)	SVHN (% of errors)	CIFAR-10 (% of errors)
CatGAN (SPringenberg (2016))	191(±10)	-	19.58(±0.46)
TriPIeGANt (Li et al. (2017))	91(±58)	5.77(±0.17)	16.99(±0.36)
FM* (Salimans et al. (2016))	93(±6.5)	8.11(±1.3)	18.63(±1.32)
badGAN* (Dai et al. (2017))	79.5(±9.8)	4.25(±0.03)	14.4l(±0.3θ)
CT-GAM (Wei et al.(2018))	-	-	9.98(±0.21)
OUr method*	一	82.7(±4.6)-	5.01(±0.14厂	15.08(±0.24y
Our results in Table 1 is slightly inferior to the best record of badGAN (Dai et al. (2017)) but
outperforms other approaches. In comparison With them, there is a probable reason to explain
the slightly inferior performance in the folloWing. Since the patterns of images are complicated,
the generator Without enough capacity is not able to learn our desired distribution, Which is the
distribution meets the conditions in (7) and (8). HoWever, this problem Will be attenuated With the
improvements of GAN, and our models benefit from them. In badGAN, they rely on the feature
matching in their objective function. No matter hoW they change the divergence criterion on tWo
distributions, feature matching still let them learn the distribution matching the first-order statistics,
so they cannot totally get the advantage of the progress of GANs.
5.2 DSGAN in Adversarial Training
Our proposed DSGAN is capable to be used to improve the robustness of the classifier against
adversarial examples. In the experiments, We mainly validate DSGAN on CIFAR-10, Which is
Widely used in adversarial training.
Recall that the objective function (6) requires finding adversarial examples to maximize the clas-
sification error ' (∙). Adversarial examples usually locate on the low-density area of Pd and are
generated from labeled data via gradient descent. Instead of using gradient descent, We aim to
generate adversarial examples via GAN. By assigning Ipd as the convolution of Pd and uniform dis-
tribution, samples from pg will locate on the low-density area ofpd. Furthermore, the distortion is
directly related to the range of uniform distribution. It, however, may be impractical for training the
generator for each class. Thus, we propose a novel semi-supervised adversarial learning approach
here.
Three stages are required to train our model: First, we train a baseline classifier on all the training
data. All the training data are labeled and represent samples from L in (9). Second, we train a gen-
erator to generate adversarial examples and treat these adversarial examples as additional unlabeled
training data (X 〜pg in (9)). Third, we fine-tune the classifier Cθ with all training data and the data
produced by the generator via minimizing the following objective:
argminE(x,y)~L [' (x； y； Cθ)]+ W ∙ Eχg~pg(x) H (Cθ(Xg))],	(9)
θ
where the first term is a typical supervised loss such as cross-entropy loss and the second term is
the entropy loss H of generated unlabeled samples corresponding to the classifier, meaning that
8
Under review as a conference paper at ICLR 2019
we would like the classifier to confidently classify the generated samples. In other words, if an
adversarial example xg is the closest to one of labeled data x, it should be classified into the class of
x. Thus, the additional entropy loss will prevent our model from the attack by adversarial examples.
Furthermore, in (9), one can view the weight w is the trade-off between the importance of labeled
data in high-density area and unlabeled data in low-density area. If w is 0, the model might be
prone to classify correctly only on the labeled data. When increasing w, the model will place more
emphasis on unlabeled data. Since the unlabeled data acts as adversarial examples, therefore, the
classifier is more robustness.
5.2.1	Experiments settings
We evaluate the trained models against a range of adversaries, where the distortion is evaluated by
'2-norm or 'af-norm. The adversaries include:
•	White-box attacks with Fast Gradient Sign Method (FGSM) (Goodfellow et al. (2015))
using `inf -norm.
•	White-box attacks with PGD (Kurakin et al. (2017)) using `inf -norm.
•	White-box attacks with DeePfool (Moosavi-Dezfooli et al. (2016)) using '2-norm and 'af-
norm.
According to different adversaries, we generate 10000 adversarial examPles from testing data and
calculate the accuracy of the model after attacking. The accuracy is record as the Probability that
adversarial examPles fail to attack when the distortion created by attacking algorithm cannot exceed
a maximum value. We also train our models with different ranges of uniform distribution. The
exPerimental detail can be found in APPendix E.
To validate our method, we ProPose two kinds of baseline networks. One is a baseline classifier we
train in the first stage, which is a tyPical classifier trained by all data. The other one is the model
with noisy inPuts. Adding noise to the inPut is a Prevalent strategy to train a classifier and it is also
able to Protect the neighborhood of the training data. For fare comParison to our method, uniform
noise is used in the second baseline model.
5.2.2	Main results
Fig. 7 demonstrates that our models exhibit stronger robustness among all the adversaries. w is set
to 10 in this figure, other results with different w are disPlayed in the APPendix E and we claim that
our method can outPerform other baselines in a wide range of values ofw. We notice that the model
benefits from controlling the weight w. When we increase the w from 1 to 3, and then from 3 to 10,
the robustness keePs becoming stronger.
Our second baseline models have the similar intuition with our method, they ProPagate the label
information to the neighborhood of each data Point by introducing the noise to inPuts. This strategy
can imProve the accuracy and the robustness of the model. Nevertheless, the training data distri-
bution after aPPlying noise can be viewed as a smoother version of original distribution. Most of
samPles still are locate on high-density area of original distribution. Due to this reason, the second
baseline models cannot emPhasize low-density samPles via w like the ProPosed model, leading to
the inferior robustness.
Our method relies ona generator to Produce low-density data. The generated samPles helP our model
to Put decision boundary outside low-density area. Thus, the model can resist adversarial attacks
with larger distortion theoretically. It’s worth mentioning that our method is able to combine with
the idea of second baseline to the suPervised term in (9) and the Performance might be imProved.
6 Related Works
We introduce related works about generating unseen data.
Yu et al. (2017) ProPosed a method to generate samPles of unseen classes in the unsuPervised man-
ner via an adversarial learning strategy. But, it requires solving an oPtimization Problem for each
samPle, which may lead to high comPutation cost. On the contrary, DSGAN has the caPability to
create infinite diverse unseen samPles. Hou et al. (2018) Presented a new GAN architecture that can
learn both distributions of unseen data from Part of seen data and unlabeled data. But, unlabeled
9
Under review as a conference paper at ICLR 2019
(a) Deepfool with `2 norm
(b) Deepfool with `inf norm
OooOOOoO
876%4321
累)Aue.lnυue
8070605040302010
?) Aue」nuue
(c) FGSM with `inf norm	(d) PGD with `inf norm
Figure 7: The accuracy of baseline and our models after attacks. Blue line is first baseline model.
Orange, green and red lines are second baseline models with different range of uniform noise. Pur-
ple, brown and pink lines are our methods. The float number also indicates the range of noise where
“w10” means that w in (9) is set to 10. The epsilon means that the `2 (or`inf) norm between original
image (pixel values are normalized to range [-0.5, 0.5]) and corresponding adversarial example.
data must be a mixture of seen and unseen samples; DSGAN does not require any unseen data in-
stead. Both Dai et al. (2017) aim to generate complementary samples (or out-of-distribution sample)
but assume that in-distribution can be estimated by a pretrained model such as PixelCNN++, which
might be difficult and expensive to train. Lee et al. (2018) use a simple classifier to replace the
role of PixelCNN++ in Dai et al. (2017) such that the training is much easier and more suitable.
Nevertheless, their method only focuses on generates unseen data surrounding low-density area of
seen data, but DSGAN is more flexible to generate different kinds of unseen data (e.g., the linear
combination of seen data shown in Sec.5.1). Besides, their method needs the label information of
data while ours is fully unsupervised.
7 Conclusions
In this paper, we propose DSGAN that can produce samples from the target distribution based on
the assumption that the density of target distribution can be the difference between the densities of
any two distributions. DSGAN is useful in the environment when the samples from the target dis-
tribution are more difficult to collect than those from the two known distributions. We demonstrate
that DSGAN is really applicable to, for example, semi-supervised learning and adversarial train-
ing. Empirical and theoretical results are provided to validate the effectiveness of DSGAN. Finally,
because DSGAN is developed based on traditional GAN, it is easy to extend any improvements of
traditional GAN to DSGAN.
10
Under review as a conference paper at ICLR 2019
References
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial net-
works. In ICLR. 2017.
M.	Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML,
volume 70,pp. 214-223, 2017.
A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circum-
venting defenses to adversarial examples. In ICML. 2018.
N.	Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Sympo-
sium on Security and Privacy (SP), pp. 39-57, 2017.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semi-
supervised learning that requires a bad gan. In NIPS, pp. 6510-6520. 2017.
I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In ICLR,
2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672-2680.
2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Im-
proved training of wasserstein gans. In NIPS, 2017.
M. Hou, B. Chaib-draa, C. Li, and Q. Zhao. Generative adversarial positive-unlabelled learning. In
IJCAI, pp. 2255-2261, 2018.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In NIPS, pp. 3581-3589. 2014.
A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial machine learning at scale. In ICLR, 2017.
Y. LeCun, C. Cortes, and C. J. C. Burges. The mnist database of handwritten digits. 1998.
Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers
for detecting out-of-distribution samples. In ICLR, 2018.
Chongxuan Li, Kun Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In NIPS, 2017.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. In ICLR. 2018.
X. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang, and S. P. Smolley. Least squares generative adversarial
networks. In IEEE ICCV, pp. 2813-2821, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In ICLR. 2018.
S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: A simple and accurate method to fool
deep neural networks. In IEEE CVPR, 2016.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images
with unsupervised feature learning. In NIPS Workshop, 2011.
N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep
learning in adversarial settings. In IEEE European Symposium on Security and Privacy (EuroS
P), pp. 372-387, 2016.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-
supervised learning with ladder networks. In NIPS, 2015.
11
Under review as a conference paper at ICLR 2019
Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak
Lee. Generative adversarial text to image synthesis. In ICML, 2016.
Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexan-
der Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In ICML, pp.
4393—4402, 2018.
Y. Saito, S. Takamichi, and H. Saruwatari. Statistical parametric speech synthesis incorporating
generative adversarial networks. IEEE/ACM Transactions on Audio, Speech, and Language Pro-
cessing, 26(1):84-96, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In NIPS, pp. 2234-2242. 2016.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modifications. In ICLR, 2017.
B. Scholkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization,
Optimization, and Beyond. MIT Press, Cambridge, MA, USA, 2001. ISBN 0262194759.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative
adversarial networks. In ICLR, 2016.
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training
of wasserstein gans: A consistency term and its dual effect. In ICLR, 2018.
M. Wu and J. Ye. A small sphere and large margin approach for novelty detection using training
data with outliers. IEEE Trans. on Pattern Analysis and Machine Intelligence, 31(11):2088-2092,
2009.
Y. Yu, W.-Y. Qu, N. Li, and Z. Guo. Open-category classification by adversarial sample generation.
In IJCAI, pp. 3357-3363, 2017.
J. J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. In ICLR, 2017.
Appendix
A Applications about Unseen Data
We show key applications regarding as unseen data as follow:
B Proofs
B.1 Proof of the Proposition 1
Proof. Given any generator G, the training criterion for the discriminator D is to maximize the
quantity V (G, D):
V (G, D) =	Pdd(x) log (D (x)) dx + (1 - α)	Pz(z) log (1 - D (G (z))) dz
xz
+ α	Pd (x) log (1 - D (x)) dx
x
=	Pdd(x) log (D (x)) dx + (1 - α)	Pg (x) log (1 - D (x)) dz
xx
+ α	Pd (x) log (1 - D (x)) dx
x
P Pd(X)Iog(D (x)) + ((1 - α)pg(x) + αpd(x))log(1 - D (x)) dx.
x
12
Under review as a conference paper at ICLR 2019
Figure 8: The illustration of applications related to unseen data.
For any (a, b) ∈ R2\{0, 0}, the function alog (y) + blog (1 - y) achieves its maximum in [0, 1]
at y = a+ab. The discriminator only needs to be defined within SUpp(Pa) U SUpp(Pd) U SUpp(Pg).
We complete this proof.	□
B.2 Proof of Theorem 1
Proof. We start from
⑶=-log(4) + Ex〜Pd(χ)
___________2Pd(x___________
Pd(X) + (1 - α)pg (x) + αpd (x)
+ Ex-(1-α)pg (x)+α~pd (x)
2 ((1 - a)pg(x) + αpd(x))
Pa(X) + (1 — α)pg (x) + αpd (x)
=-log(4) + KL (Pj Pa +(1 — ；)% + aP )
+ KL ((1- α)Pg(x) + αPd Pa +(1 -	+ 川)
=TOg⑷ + 2 JSD(Pa k (I- a)Pg + αPd),
where KL is the Kullback-Leibler divergence and JSD is the Jensen-Shannon divergence. The JSD
returns the minimal value, which is 0, iffboth distributions are the same, namely Pd = (1 — α)Pg +
αPd. Because Pg(X)’s are always non-negative, it should be noted both distributions are the same
only if aPa (x) ≤ Pa(X) for all x's. We complete this proof.	□
13
Under review as a conference paper at ICLR 2019
B.3 Proof of Proposition 2
Proof. Recall
C(G) =	pdJ(x) log
x
Pd(X)____________
+ (1 - α)Pg(x) + αPd(x)
+ ((1 - α)pg (x) + αpd(x)) log
-α)pg (x) + αpd(x)	λ d工
+ (1 - α)pg (x) + αpd(x) J
S(pg; x)dx
x
Z
J x∈Supp(p(J)-SUPP(Pd)
S(pg ; x)dx +	S(pg ; x)dx
x∈SUPP(pd)
S(Pg; x) is to simplify the notations inside the integral. For any x, S(Pg; x) in Pg (x) is non-
increasing and S(Pg; x) ≤ 0 alWays holds. Specifically, S(Pg; x) is decreasing along the increase of
Pg(x) if PdJ(x) > 0; S(Pg; x) attains the maximum value, zero, for any Pg(x) if PdJ(x) = 0. Since
DSGAN aims to minimize C(G) With the constraint	Pg (d)dx = 1, the solution attaining the
x
global minima must satisfy Pg (x) = 0 if PdJ(x) = 0; otherWise, there exists another solution With
smaller value of C(G). Thus, SuPP (Pg) ⊆ SuPP (PdJ).
∂S(Pg; x)	(1 - α)Pg(x) + αPd(x)
Furthermore, T(Pg; x) = 口 ：、= log ,」 八~~——----------------------------------rʌ is increasing on
∂Pg (x)	PdJ(x) + (1 - α)Pg (x) + αPd(x)
Pg(x) and converges to 0. When X ∈ SuPP(Pf) T SuPP(Pd), T(Pg; x) ≥ log 1 always holds due to
the assumption αPd(x) ≥ PdJ(x). Based on the folloWing expression,
/
x∈SUPP(pdJ)-SUPP(pd)
Pd(x)dx +
pdJ(x)dx = 1
x∈SUPP(pd)
⇒
⇒
⇒
pdJ(x)dx
x∈SUPP(pdJ)-SUPP(pd )
pdJ(x)dx
x∈SUPP(pdJ)-SUPP(pd )
pdJ(x)dx
x∈SUPP(pdJ)-SUPP(pd )
1 -	αpd(x)dx
x∈SUPP(pd)
1-α
≥	(1 - α)pg (x)dx,
x∈SUPP(pdJ)-SUPP(pd)
≥
≥
the last inequality implies that there exists a solution such that (1 - α)pg (x) ≤ pdJ(x) for x ∈
SuPP(Pf) — SuPP(Pd) With	Pg(d)dx =1 In this case, T(Pg; x) < log 2 for
x∈SUPP(pdJ)-SUPP(pd)
X ∈ SuPP(P(J) - SuPP(pd). We complete this proof.
□
B.4 Proof of Proposition 3
Proof. Consider V (G, D) = U(Pg, D) as a function of Pg. By the proof idea of Proposition 2
in Goodfellow et al. (2014), if f(x) = suPα∈A fα(x) and fα(x) is convex in x for every α, then
∂fβ(x) ∈ ∂f if β = argsuPα∈A fα(x). In other words, if suPD V (G, D) is convex in Pg, the
subderivatives of suPD V (G, D) includes the derivative of the function at the point, where the max-
imum is attained, implying the convergence with sufficiently small updates ofPg. We complete this
proof.	□
C Tricks for Stable Training
We provide a trick to stabilize the training procedure by reformulating the objective function. Specif-
ically, V (G, D) in (2) is reformulated as:
V(G,D)=
x
Pdd(x) log (D (x)) + ((1 - α)Pg(x) + αPd(x)) log (1 - D (x)) dx
(10)
Ex〜PJ(X) [log D(X)] + Ex〜(1-α)pg (x) + α〜Pd(X) [log (1 - D (X))] .
14
Under review as a conference paper at ICLR 2019
Instead of sampling a mini-batch of m samples from pz and pd in Algorithm 1, (1 - α)m and αm
samples from both distributions are required, respectively. The computation cost in training can be
reduced due to fewer samples. Furthermore, although (10) is equivalent to (2) in theory, we find that
the training using (10) achieves better performance than using (2) via empirical validation in Table
2. We conjecture that the equivalence between (10) and (2) is based on the linearity of expectation,
but mini-batch stochastic gradient descent in practical training may lead to the different outcomes.
Table 2: Comparing the semi-supervised learning results on MNIST whether to use the sampling
tricks.
Methods	MNIST (# errors)
Our method w/o tricks Our method w/ tricks	91.0(±7.0) 82.7(±4.6)
D Experimental details for semi-supervised learning
D. 1 Hyperparameters
The hyperparameters were chosen to make our generated samples consistent with the assumptions
in (7) and (8). However, in practice, if we make all the samples produced by the generator following
the assumption in (8), then the generated distribution is not close to the true distribution, even a
large margin between them existing in most of the time, which is not what we desire. So, in our
experiments, we make a concession that the percentage of generated samples, which accords with
the assumption, is around 90%. To meet this objective, we tune the hyperparameters. Table 3 shows
our setting of hyperparameters, where β is defined in (8).
Table 3: Hyperparameters in semi-supervised learning.
Hyperparameters	MNIST	SVHN	CIFAR-10
α	-O-	-O-	05
β	0.3	0.3	0.1
D.2 Architecture
In order to fairly compare with other methods, our generators and classifiers for MNIST, SVHN,
and CIFAR-10 are same as in Salimans et al. (2016) and Dai et al. (2017). However, different from
previous works that have only a generator and a discriminator, we design an additional discriminator
in the feature space, and it’s architecture is similar across all datasets with only the difference in the
input dimensions. Following Dai et al. (2017), we also define the feature space as the input space of
the output layer of discriminators.
Compared to SVHN and CIFAR-10, MNIST is a simple dataset as it is only composed of fully
connected layers. Batch normalization (BN) or weight normalization (WN) is used to every layer to
stable training. Moreover, Gaussian noise is added before each layer in the classifier, as proposed
in Rasmus et al. (2015). We find that the added Gaussian noise exhibits positive effect for semi-
supervised learning and keep to use it. The architecture is shown in Table 4.
Table 5 and Table 6 are models for SVHN and CIFAR-10, respectively, and these models are almost
the same except for some implicit differences, e.g., the number of convolutional filters and types of
dropout. In these tables, given a dropping rate, “Dropout” is a normal dropout in that the elements
of input tensor are randomly set to zero while Dropout2d is a dropout only applied on the channels
to randomly zero all the elements.
Furthermore, the training procedure alternates between k steps of optimizing D and one step of
optimizing G. We find that k in Algorithm 1 is a key role to the problem of mode collapse for
different applications. For semi-supervised learning, we set k = 1 for all datasets.
15
Under review as a conference paper at ICLR 2019
Table 4: Network architectures for semi-supervised learning on MNIST. (GN: Gaussian noise)
Generator G	Discriminator D	Classifier C
Input z ∈ R100 from unif(0, 1)	Input 28 × 28 gray image	Input 28 × 28 gray image
100 × 500 FC layer with BN Softplus 500 × 500 FC layer with BN Softplus 500 × 784 FC layer with WN Sigmoid	250 × 400 FC layer ReLU 400 × 200 FC layer ReLU 200 × 100 FC layer ReLU 100 × 1 FC layer	GN, std = 0.3 784 × 1000 FC layer with WN ,ReLU GN, std = 0.5 1000 × 500 FC layer with WN, ReLU GN, std = 0.5 500 × 250 FC layer with WN, ReLU GN, std = 0.5 250 × 250 FC layer with WN, ReLU GN, std = 0.5 250 × 250 FC layer with WN, ReLU 250 × 10 FC layer with WN
Table 5:	Architectures of generator and discriminator for semi-supervised learning on SVHN and
CIFAR-10. N was set to 128 and 192 for SVHN and CIFAR-10, respectively.
Generator G
Input z ∈ R100 from unif(0, 1)
100 X 8192 FC layer with BN, ReLU
Reshape to 4 × 4 × 512
5 × 5 conv. transpose 256 stride = 2 with BN, ReLU
5 × 5 conv. transpose 128 stride = 2 with BN, ReLU
5 × 5 conv. transpose 3 stride = 2 with WN, Tanh
Discriminator D
Input 32 × 32 RGB image
N × 400 FC layer, ReLU
400 × 200 FC layer, ReLU
200 × 100 FC layer, ReLU
100 × 1 FC layer
Table 6:	The architecture of classifiers for semi-supervised learning on SVHN and CIFAR-10. (GN:
Gaussian noise, lReLU(leak rate): LeakyReLU(leak rate))
Classifier C for SVHN
Input 32 × 32 RGB image
GN, std = 0.05
Dropout2d, dropping rate = 0.15
3 ×	3	conv.	64	stride =	1 with	WN, lReLU(0.2)
3 ×	3	conv.	64	stride =	1 with	WN, lReLU(0.2)
3 ×	3	conv.	64	stride =	2 with	WN, lReLU(0.2)
Dropout2d, dropping rate = 0.5
3 ×	3	conv.	128	stride = 1 with WN, lReLU(0.2)
3 ×	3	conv.	128	stride = 1 with WN, lReLU(0.2)
3 ×	3	conv.	128	stride = 2 with WN, lReLU(0.2)
Dropout2d, dropping rate = 0.5
3 ×	3	conv.	128	stride = 1 with WN, lReLU(0.2)
1 ×	1	conv.	128	stride = 1 with WN, lReLU(0.2)
1 ×	1	conv.	128	stride = 1 with WN, lReLU(0.2)
Global average Pooling
128 × 10 FC layer with WN
Classifier C for CIFAR-10
Input 32 × 32 RGB image
GN, std = 0.05
Dropout2d, dropping rate = 0.2
3 ×	3	conv.	96	stride =	1 with	WN, lReLU(0.2)
3 ×	3	conv.	96	stride =	1 with	WN, lReLU(0.2)
3 ×	3	conv.	96	stride =	2 with	WN, lReLU(0.2)
Dropout, dropping rate	= 0.5
3 ×	3	conv.	192	stride =	1	with WN, lReLU(0.2)
3 ×	3	conv.	192	stride =	1	with WN, lReLU(0.2)
3 ×	3	conv.	192	stride =	2	with WN, lReLU(0.2)
Dropout, dropping rate = 0.5
3 ×	3	conv.	192	stride =	1	with WN, lReLU(0.2)
1 ×	1	conv.	192	stride =	1	with WN, lReLU(0.2)
1 ×	1	conv.	192	stride =	1	with WN, lReLU(0.2)
Global average Pooling
192 × 10 FC layer with WN
E Experimental details for adversarial training
The size of labeled data for CIFAR-10 is 50000 and we balance the number of data for each class.
16
Under review as a conference paper at ICLR 2019
In our experiments, as for the second stage, we train DSGAN for 50 epochs in Algorithm 1 to
generate our adversarial examples. In the third stage, we finetune the baseline classifier for 50
epochs.
In the experiments for adversarial training on CIFAR-10, the generator and discriminator are the
same as those in semi-supervised learning. The architecture is described in Table 5 and the classifier
is modified from the one shown in Table 6. First, we get rid of all the dropouts and Gaussian noise
so that we can compare among different models with less randomness. Moreover, we decrease the
number of layers in the original model, simply intending to accelerate training. The number of layers
following the feature space is increased to 3. Because we apply our method in the feature space, the
sub-network after feature space should be non-linear so that it can correctly classify generated data.
The architecture is described in Table 7. Furthermore, k is assigned to 5 in all experiments.
We show more results in Fig. 9 with w = 1 and 10 with w = 3.
Table 7:	The architecture of classifier for adversarial training on CIFAR-10. (lReLU(leak rate):
LeakyReLU(leak rate))
Classfier C for CIFAR-10
InPUt 32 X 32 RGB image
3 × 3 conv. 96 stride = 1 with WN, lReLU(0.2)
3 × 3 conv. 96 stride = 2 with WN, lReLU(0.2)
DroPoUt, droPPing rate = 0.5
3 × 3 conv.	192 stride =	1	with WN,	lReLU(0.2)
3 × 3 conv.	192 stride =	2	with WN,	lReLU(0.2)
DroPoUt, droPPing rate = 0.5
3 × 3 conv.	192 stride =	1	with WN,	lReLU(0.2)
1 × 1 conv.	192 stride =	1	with WN,	lReLU(0.2)
Global average Pooling
192 × 192 FC layer with WN, lReLU(0.2)
192 × 192 FC layer with WN, lReLU(0.2)
192 × 192 FC layer with WN, lReLU(0.2)
192 × 10 FC layer with WN
17
Under review as a conference paper at ICLR 2019
0.00	0.05	0.10	0.15	0.20	0.25	0.30	0.35	0.40
epsilon
(a) Deepfool with `2 norm
DeepFooILinfinityAttack
(b) Deepfool with `inf norm
(送 Aυe.lnuue
FGSM
----baseline
----baseline_0.01
----baseline 0.03
----baseline_0.05
----our_0.01_wl
---- our_O.O3_wl
——ourO.O5wl
Xir 0.03 Wl
baseline
baseline/.Ol
baseline0.03
baseline_0.05
ɔurθ,θlwl
80706050403020
?) Aue」nuue
(c)	FGSM with `inf norm
(d)	PGD with `inf norm
Figure 9:	The setting is the same with Fig. 7 unless w = 1.
18
Under review as a conference paper at ICLR 2019
DeepFoolL2Attack
0.00	0.05	0.10	0.15	0.20	0.25	0.30	0.35	0.40
epsilon
(a) Deepfool with `2 norm
DeepFooILinfinityAttack
(b) Deepfool with `inf norm
(送 Aυe.lnuue
----baseline
----baseline_0.01
----baseline 0.03
----baseline_0.05
----our_0.01_w3
----our_O.O3_w3
——ourO.O5w3
80706050403020
?) Aue」nuue
(c) FGSM with `inf norm
(d) PGD with `inf norm
Figure 10:	The setting is the same with Fig. 7 unless w = 3.
19