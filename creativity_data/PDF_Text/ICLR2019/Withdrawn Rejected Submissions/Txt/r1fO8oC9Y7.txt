Under review as a conference paper at ICLR 2019
Multi-Task Learning for Semantic Parsing
with Cross-Domain Sketch
Anonymous authors
Paper under double-blind review
Ab stract
Semantic parsing, which maps a natural language sentence into a machine-
readable representation of its meaning, is highly constrained by the limited an-
notated training data. Inspired by the idea of coarse-to-fine, we propose a general-
to-detailed neural network (GDNN) by incorporating middle coarse cross-domain
sketch (CDS) among utterances and their logic forms. For utterances in different
domains, the General Network will extract CDS using an encoder-decoder model
in a multi-task learning setup. Then for some utterances in a specific domain,
the Detailed Network will generate the detailed target parts using a sequence-
to-sequence architecture with advanced attention to both utterance and generated
CDS. Our experiments show that compared to direct multi-task learning, CDS has
improved the performance in semantic parsing task which converts users’ requests
into meaning representation language (MRL). We also use experiments to illus-
trate that CDS works by adding some constraints to the target decoding process,
which further proves the effectiveness and rationality of CDS.
1 Introduction
Recently many natural language processing (NLP) tasks based on the neural network have shown
promising results and gained much attention because these studies are purely data-driven without
linguistic prior knowledge. Semantic parsing task which maps a natural language sentence into a
machine-readable representation (Fan et al. (2017)), as a particular translation task, can be treated as
a sequence-to-sequence problem (Dong & Lapata (2016)). Lately, a compositional graph-based se-
mantic meaning representation language (MRL) has been introduced (Kollar et al. (2018a)), which
converts utterance into logic form (action-object-attribute), increasing the ability to represent com-
plex requests. This work is based on MRL format for semantic parsing task.
Semantic parsing highly depends on the amount of annotated data and it is hard to annotate the data
in logic forms such as Alexa MRL. Several researchers have focused on the area of multi-task learn-
ing and transfer learning (Hakkani-Tur et al. (2016), Fan et al. (2017), Kollar et al. (2018b)) with
the observation that while these tasks differ in their domains and forms, the structure of language
composition repeats across domains (Herzig & Berant (2017)). Compared to the model trained on
a single domain only, a multi-task model that shares information across domains can improve both
performance and generalization. However, there is still a lack of interpretations why the multi-task
learning setting works (Ruder (2017)) and what the tasks have shared. Some NLP studies around
language modeling (Le & Mikolov (2014), Vaswani et al. (2017), Devlin et al. (2018)) indicate that
implicit commonalities of the sentences including syntax and morphology exist and can share among
domains, but these commonalities have not been fully discussed and quantified.
To address this problem, in this work, compared to multi-task learning mentioned above which
directly use neural networks to learn shared features in an implicit way, we try to define these
cross-domain commonalities explicitly as cross-domain sketch (CDS). E.g., Search weather in 10
days in domain Weather and Find schedule for films at night in domain ScreeningEvent both have
action SearchAction and Attribute time, so that they share a same MRL structure like SearchAc-
tion(Type(time@?)), where Type indicates domain and ? indicates attribute value which is copying
from the original utterance. We extract this domain general MRL structure as CDS. Inspired by
the research of coarse-to-fine (Dong & Lapata (2018)), we construct a two-level encoder-decoder
by using CDS as a middle coarse layer. We firstly use General Network to get the CDS for every
1
Under review as a conference paper at ICLR 2019
utterance in all domains. Then for a single specific domain, based on both utterance and extracted
CDS, we decode the final target with advanced attention while CDS can be seen as adding some
constraints to this process. The first utterance-CDS process can be regarded as a multi-task learning
setup since it is suitable for all utterances across the domains. This work mainly introducing CDS
using multi-task learning has some contributions listed below:
1)	We make an assumption that there exist cross-domain commonalities including syntactic and
phrasal similarity for utterances and extract these commonalities as cross-domain sketch (CDS)
which for our knowledge is the first time. We then define CDS on two different levels (action-level
and attribute-level) trying to seek the most appropriate definition of CDS.
2)	We propose a general-to-detailed neural network by incorporating CDS as a middle coarse layer.
CDS is not only a high-level extraction of commonalities across all the domains, but also a prior
information for fine process helping the final decoding.
3)	Since CDS is cross-domain, our first-level network General Network which encodes the utterance
and decodes CDS can be seen as a multi-task learning setup, capturing the commonalities among
utterances expressions from different domains which is exactly the goal of multi-task learning.
2	Related Work
2.1	Spoken Language Understanding
Traditional spoken language understanding (SLU) factors language understanding into domain clas-
sification, intent prediction, and slot filling, which proves to be effective in some domains (Gupta
et al. (2006)). Representations of SLU use pre-defined fixed and flat structures, which limit its ex-
pression skills like that it is hard to capture the similarity among utterances when the utterances are
from different domains (Kollar et al. (2018b)). Due to SLU’s limited representation skills, meaning
representation language (MRL) has been introduced which is a compositional graph-based seman-
tic representation, increasing the ability to represent more complex requests (Kollar et al. (2018a)).
There are several different logic forms including lambda-calculus expression (Kwiatkowski et al.
(2011)), SQL (Zhong et al. (2018)), Alexa MRL (Kollar et al. (2018a)). Compared to fixed and flat
SLU representations, MRL (Kollar et al. (2018a)) based on a large-scale ontology, is much stronger
in expression in several aspects like cross-domain and complex utterances.
2.2	Sequence-to-Sequence for Semantic Parsing
Mapping a natural language utterance into machine interpreted logic form (such as MRL) can be re-
garded as a special translation task, which is treated as a sequence-to-sequence problem (Sutskever
et al. (2014)). Then Bahdanau et al. (2015) and Luong et al. (2015) advance the sequence-to-
sequence network with attention mechanism learning the alignments between target and input words,
making great progress in the performance. Malaviya et al. (2018) explore the attention mechanism
with some improvements by replacing attention function with attention sparsity. Besides, to deal
with the rare words, Gu et al. (2016) incorporate the copy mechanism into the encoder-decoder
model by directly copying words from inputs. Lately, many researchers have been around improv-
ing sequence-to-sequence model itself, in interpreting the sentence syntax information. Eriguchi
et al. (2016) encode the input sentence recursively in a bottom-up fashion. Wu et al. (2017) gen-
erate the target sequence and syntax tree through actions simultaneously. Another aspect which
has caught much attention is constrained decoding. Krishnamurthy et al. (2017) and Post & Vilar
(2018) add some constraints into decoding process, making it more controllable. Dong & Lapata
(2016) use the recurrent network as encoder which proves effective in sequence representation, and
respectively use the recurrent network as decoder and tree-decoder. Krishnamurthy et al. (2017)
employ the grammar to constrain the decoding process. Dong & Lapata (2018), believe utterance
understanding is from high-level to low-level and by employing sketch, improve the performance.
2.3	Multi-task learning
For semantic parsing task especially in MRL format, it is expensive and time-consuming to annotate
the data, and it is challenging to train semantic parsing neural models. Multi-task learning aims to
2
Under review as a conference paper at ICLR 2019
use other related tasks to improve target task performance. Liu & Lane (2016b) deal with traditional
SLU piper-line network by jointly detecting intent and doing slot filling. Sogaard & Goldberg
(2016) share parameters among various tasks, according to the low-level and high-level difference.
Hershcovich et al. (2018) divide the representation network into task-specific and general which is
shared during multi-task learning. Fan et al. (2017) and Herzig & Berant (2017) directly share the
encoder or decoder neural layers (model params) through different semantic parsing tasks. In Kollar
et al. (2018b), multi-task learning also mainly acts sharing the params of the network.
3	Approach
3.1	Definition of Cross-Domain sketch
For human language expressions, especially task-oriented requests, there exist commonalities across
sentences including linguistic syntax and phrase similarity. They can be seen with general sentence
templates. Table 1 shows some examples.
Utterance	Sentence Template	CDS
Find restaurants nearby What is the weather in NYC Tell me what movies tonight Play last year’s music Book today’s restaurant downtown	(find What where) (what where) (tell me what when) (play when’s what) (book when,s what where)	SearchAction(Type(place@?)) SearchAction(Type(place@?)) SearchAction(Type(time@?)) PlayAction(Type(time@?)) BookAction(Type(place@?,time@?))
Table 1: Utterances’ Commonalities.
Since sentence templates are too many, we try to leverage these common regularities more abstractly.
We extract these invariant commonalities which are implicit across domains, and call them as cross-
domain sketch (CDS) in a canonical way.
We define CDS in meaning representation language (MRL) format (action-object-attribute) and on
two levels (action-level and attribute-level). Action-level CDS means to acquire the same action
for utterances from different domains while the attribute-level CDS means to extract more detailed
information. See examples in Table 1. Instead of extracting CDS from utterance directly, we try
converting from logic form into CDS reversely, because it is easier to deal with structural logic
form than utterance in natural language form. We analyze the dataset Snips and use a rule-based
method to obtain CDS. We strip logic forms from domain-specific components and preserve domain-
independent parts including general actions and attributes.
We do some statistics on the dataset Snips (Goo et al. (2018)) used in this paper. We con-
vert attributes [object_type, movie_type, restaurant_type] into {object-type}, [object_name, restau-
rant-name, movie_name] into {object_name}, [year, timeRange] into {time}, [location_name, cur-
rent-location] into {object-location}, [country, city] into {place}. All those attributes account for
55% of all attributes which indicate the existence and feasibility of CDS.
3.2	Model
Figure 1 shows our overall network, which contains a two-level encoder-decoder. The General
Network encodes utterance and decodes cross-domain sketch (CDS). Since this process is domain-
general, it can be done to all domains, which is a multi-task setup. The Detailed Network firstly
encodes the CDS and the utterance, then it decodes the target result based on both utterance and
CDS. This process is domain-dependent, so that it is a fine-tuning process in a specific domain.
3.2.1	Problem Definition
For an input utterance u = u1, u2, ...u|u|, and its middle result cross-domain sketch (CDS) c =
c1, c2, ...c|c|, and its final result logic form y = y1, y2, ...y|y|, the conditional probability is:
|y|	|c|
p(y|u, c) =	p(yt|y<t, u, c)	p(c|u) =	p(ct|c<t, u)	(1)
t=1	t=1
3
Under review as a conference paper at ICLR 2019
Utterances:
Weather in 10 days (Domain: Weather)
Find schedule for films at night (Domain: ScreeningEVent)
Figure 1: Overall Network. General Network (red dashed box below) encodes the utterance with
bi-directional LSTM and decodes cross-domain sketch (CDS) using unidirectional LSTM with at-
tention to utterance in all domains. For identical encoding, general utterance encoding and specific
utterance encoding share the same encoder while for separate encoding, they are not (see Sec-
tion 3.2.2). Then Detailed Network, in one specific domain, encodes CDS and utterance using
bi-directional LSTM, decodes the final target with advanced attention to both utterance and CDS.
where y<t = y1, y2, ...y|t-1|, and c<t = c1,c2, ...c|t-1|.
3.2.2	Utterance Encoder
The neural encoder of our model is similar to neural machine translation (NMT) model, which uses
a bi-directional recurrent neural network. Firstly each word of utterance is mapped into a vector
ut ∈ Rd via embedding layer and we get a word sequence u = (u1, ..., u|u|). Then we use a
bi-directional recurrent neural network with long short-term memory units (LSTM) (Hochreiter &
Schmidhuber (1997)) to learn the representation of word sequence. We generate forward hidden
state htu = fLS T M (htu-1 , ut) and backward state htu = fLST M (htu-1 , ut). The t-th word will be
h= [→,¾.
We construct two kinds of utterance encoders, general utterance encoder for General Network
and specific utterance encoder for Detailed Network (see in Figure 1), so as to extract different
information for different purposes. The general utterance encoder, meant to pay more attention to
cross-domain commonalities of utterances, is used by all utterances from all domains. The specific
utterance encoder, which is domain-dependent, belongs to one specific domain and is more sensi-
tive to details. We call encoder outputs htug from general utterance encoder and htus from specific
utterance encoder. When the two encoders share the same parameters that is htug = htus , we call it
identical encoding and when they are not, we call it separate encoding, inspired by (Sogaard &
Goldberg (2016);Krishnamurthy et al. (2017);Liu et al. (2017);Abdou et al. (2018)) which explore
the sharing mechanisms of multi-task learning and propose some improvements.
3.2.3	CDS Decoder & Encoder
The General Network is meant to obtain cross-domain sketch (CDS) c conditioned on utterance u,
using an encoder-decoder network. After encoding utterance by general utterance encoder for all
domains, we obtain htug(see Section 3.2.2). Then we start to decode CDS.
4
Under review as a conference paper at ICLR 2019
The decoder is based on a unidirectional recurrent neural network, and the output vector is used to
predict the word. The cd represents CDS decoder.
htcd = fLSTM(htc-d1,ct)	(2)
where ct is the previously predicted word embedding.
The LuongAttention (Luong et al. (2015)) between decoding hidden state dt and encoding sequence
ei(i = 1, 2, ...|e|) at time step t is computed as:
=	exp(dt ,ei)
“	Pkc= ι exp(dt,ek)
|e|
at =	st,iei	(4)
i=1
Based on equations (3) and (4), we compute the attention atu by setting dt = htcd and ei = hiug (i =
1, 2, ...|u|). The t-th predicted output token will be:
htcadtt = tanh(Wcdhtcd + Wcuatu)	(5)
p(ct|c<t, u) = softmax(Wcohtcadtt + bco)	(6)
where W, b are parameters. After decoding CDS words c = (c1, ..., c|c|), we use an encoder to
represent its meaning and due to words’ relation with forward and backward contexts, we choose to
use a bi-directional LSTM. We generate forward hidden state htce and backward state htce . The t-th
word will be hce = [-→, tce].
3.2.4	Target Decoder
Through specific utterance encoder and cross-domain sketch (CDS) encoder, we acquired t-th word
representation htus and htce . Finally with advanced attention to both encoded utterance u and CDS
c, we decode the final target y. The decoder is based on a unidirectional recurrent neural network,
and the output vector is used to predict the word. The y represents target decoder.
hty = fLST M (hty-1, yt)	(7)
where yt is the previously predicted word embedding. During target decoding process and at time
step t, we not only compute the attention to utterance encoding outputs hus but also compute the
attention to CDS encoding outputs hce . The attention between target hidden state and utterance is atu
by computing attention between hty and encoding sequence hius(i = 1, 2, ...|u|) based on equations
(3) and (4). The attention between target hidden state and CDS is atc by also computing attention
between hty and hice(i = 1, 2, ...|c|) in the same way. Then the t-th predicted output token will be
based on the advanced two-aspect attention:
htyatt = tanh(Wyhty + Wyuatu + Wycatc)	(8)
p(dt|d<t, u) = sof tmax(Wyohtyatt + byo)	(9)
3.3 Model Training and Inference
For training process, the objective is:
max	p(y|u, c) + p(c|u)	(10)
(u,c,y)∈T
T is the training corpus. For inference process, we firstly obtain cross-domain sketch (CDS) via
ce = argmax p(c|u) then we get the final target logic form via ye = argmax p(y|u, ec). For both
decoding processes, we use greedy search to generate words one by one.
5
Under review as a conference paper at ICLR 2019
4 Experiments
4.1 Datasets
Existed semantic parsing datasets, e.g., GEO (Zettlemoyer & Collins (2012)), ATIS (Zettlemoyer &
Collins (2007)), collect data only from one domain and have a very limited amount, which can not
fully interpret the effectiveness of cross-domain sketch (CDS) since it needs large dataset among
different domains.
In this case, we mainly consider the semantic parsing task Snips (Goo et al. (2018)) based on MRL
format (action-object-attribute). Snips collects users’ requests from a personal voice assistant. The
original dataset is annotated in spoken language understanding (SLU) format (intent-slot). It has
7 intent types and 72 slot labels, and more statistics are shown in Table 2. Based on the format
(intent-slot), we pre-process this dataset into MRL format by some pre-defined rules, then we regard
the intent as domain/task and share CDS among them. The details are shown in Table 3.
Domain	Total	AddTo Playlist	Book Restaurant	Get Weather	Play Music	Rate Book	Search CreativeWork	Search ScreeningEvent
train	13084	1818	1881	-^1896^^	1914	1876	1847	1852
dev	700	100	100	100	100	100	100	100
test	700	124	92	104	86	80	107	107
Table 2: Statistics of the dataset Snips.
Utterance	let me know the weather forcast of Stanislaus national forest far in nine months
Intent Slots	GetWeather OOOOOOO B-geographic_poi I-geographic_poi I-geographic_Poi B-SPatial_relation O B-timeRange I-timeRange
Action-level CDS Attribute-level CDS Target	SearchAction SearchAction ( Type (Poi @? , SpatiaLrelation @ ? , time @?)) SearchAction ( WeatherType ( geographic_poi @ 7 89, SpatiaLrelation @ 10 , timeRange @ 12 13))
Utterance	find the schedule for films at night at great escape theatres
Intent Slots	SearchScreeningEvent O O B-ObjeCtJyPe O B-movie_type O B-timeRange O B-location_name I-loCatiOn_name I-loCatiOn_name
Action-level CDS Attribute-level CDS Target	SearchAction SearchAction ( Type ( object_type @ ? , movie_type @ ? , time @ ?, ObjeCUocation @?)) SearchAction ( ScreeningEventType ( object_type @ 2 , movie_type @ 4 , timeRange @ 6 ,loCatiOn_name @8910))
Table 3: Several examples of Snips. Utterance is the user’s request which is a natural language
expression. Intent and slots are in formats from original dataset. Cross-domain sketch (CDS) has
two levels (action-level and attribute-level). Target is the final logic form with numbers indicating
copying words from utterance (index starting from 0).
4.2	Settings
We use Tensorflow in all our experiments, with LuongAttention (Luong et al. (2015)) and copy
mechanism. The embedding dimension is set to 100 and initialized with GloVe embeddings (Pen-
nington et al. (2014)). The encoder and decoder both use one-layer LSTM with hidden size 50.
We apply the dropout selected in {0.3,0.5}. Learning rate is initialized with 0.001 and is decaying
during training. Early stopping is applied. The mini-batch size is set to 16. We use the logic form
accuracy as the evaluation metric.
6
Under review as a conference paper at ICLR 2019
4.3	Results and Analysis
Firstly, in order to prove the role of the cross-domain sketch (CDS) in helping to guide decoding
process with multi-tasking learning setup, we do several experiments, and the results are shown
in Table 4. For joint learning, we apply several multi-task architectures from (Fan et al. (2017)),
including one-to-one, one-to-many and one-to-shareMany. One-to-one architecture applies a single
sequence-to-sequence model across all the tasks. One-to-many only shares the encoder across all
the tasks while the decoder including the attention parameters is not shared. In one-to-shareMany
model, tasks share encoder and decoder (including attention) params, but the output layer of decoder
is task-independent.
From the Table 4, in general, joint learning performs better than single task learning. In joint learn-
ing, one-to-one is the best and performs way better than one-to-many and one-to-shareMany, prob-
ably limited by the dataset’s size and similarity among tasks. By incorporating CDS, our GDNN
(general-to-detailed neural network) models have all improved the performance to different degrees.
The CDS is defined on two levels (action-level and attribute-level, see examples in Table 3) and
attribute-level CDS improves greater than action-level CDS, which is in our expectation since it of-
fers more information for tasks to share. We also experiment on different utterance encoding setups
with identical encoding and separate encoding (see Section 3.2.2). The separate encoding setup
performs better than sharing the same encoder for utterance, which integrates the fact that different
encoders pay different attention to the utterances due to different purposes which means one is more
general and the other is more specific detailed.
Method	Snips Accuracy
Single Seq2Seq (Sutskever et al. (2014))	62.3
Joint Seq2Seq (one-to-many) (Fan et al. (2017))	62.0
Joint Seq2Seq (one-to-shareMany) (Fan et al. (2017))	64.2
Joint Seq2Seq (one-to-one) (Fan et al. (2017))	71.4
GDNN with Action-level CDS (identical encoding)	74.9
GDNN with Action-level CDS (separate encoding)	75.1
GDNN with Attribute-level CDS (identical encoding)	76.7
GDNN with Attribute-level CDS (separate encoding)	78.1
Table 4: Multi-task Results. Single Seq2Seq means each task has a sequenece-to-sequence model.
Joint Seq2Seq show results with three multi-task mechanisms. Our results include GDNN (general-
to-detailed neural network) models with different levels of CDS (action-level/attribute level) and
different utterance encoding mechanisms (identical encoding/separate encoding).
We also list the full results of GDNN in Table 5 below, including CDS accuracy by General Network,
target accuracy by Detailed Network when feeding with right CDS, and the final accuracy, which
further proves the effectiveness of CDS.
GDNN	CDS Accuracy	Target Accuracy while CDS is true	Final Accuracy
Action-level CDS (identical encoding)	100.0	74.9	74.9
Action-level CDS (separate encoding)	100.0	75.1	75.1
Attribute-level CDS (identical encoding)	93.7	81.8	76.7
Attribute-level CDS (separate encoding)	91.0	83.6	78.1
Table 5: GDNN Results. Full results of general-to-detailed neural network (GDNN) with different
levels of CDS (action-level/attribute level) and different utterance encoding mechanisms (identical
encoding/separate encoding).
Moreover, we compare our experiments with traditional models which regard the task as intent
classification and slot filling (IC_SF). The results are shown in Table 6 below.
From Table 6, We can see compared to ICSF models (based on sequence labeling format), Seq2Seq
perform worse (71.4% compared to 73.2%) due to its fewer assumptions and larger decode size
as well as its difficulty of training, which is usual in comparing IC_SF models and sequence-to-
sequence models. Through using CDS, the performance has significantly improved. On the one
7
Under review as a conference paper at ICLR 2019
MethOd	Snips Accuracy
Joint Seq. (Hakkani-Tur et al. (2016))	73.2
Atten.-Based (Liu & Lane (2016a))	74.1
Slot.-Gated (Intent Atten.) (Goo et al. (2018))	74.6
Slot.-Gated (Full Atten.)(Goo et al.(2018))	75.5
Joint Seq2Seq (Fan et al. (2017))	71.4
GDNN with Action-level CDS	73.2
GDNN With AttribUte-level CDS	74.6
Table 6: Seq2Seq results Vs traditional results. The first four results show IC_SF models' Perfor-
mance. The last three results are based on the Seq2Seq architecture.
hand, CDS extract the cross-domain commonalities among tasks helPing to make the multi-task
learning more sPecific, which can be seen as an advance to multi-task learning.
On the other hand, CDS can be seen adding some constraints to the final target decoding Process
which has offered more information for the decoding Process, comPared to direct joint Seq2Seq. To
better Prove and exPlain this idea, we do some exPeriments according to constraint decoding asPect.
We try to comPare the sub-Process of converting utterance to CDS through different models, e.g.,
ICSF, Seq2Seq. From the Table 7, we can see that Seq2Seq achieve the comparable results (87.7%)
to IC-SF model (84.9%) for generating CDS from utterance, which further explains that, the fact
joint seq2seq performs worse (71.4%, see Table 6) than IC_SF model (73.2%) is owing to the lack
of guidance and constraints during the follow-up decoding process. By incorporating CDS, we add
some constraints to this decoding process thus obtaining better performance.
Method	Attribute-level Accuracy
intent	973
IC_SF	slot	87.3
final	84.9
Seq2Seq	87.7	一
Table 7: Results of CDS generation in dataset Snips by two methods. IC_SF is using intent clas-
sification and slot filling with evaluation metric (intent accuracy, slot labelling accuracy and final
accuracy). Seq2Seq generates CDS based on utterance using an encoder-decoder.
5 Conclusions and Future Work
In this paper, we propose the concept of cross-domain sketch (CDS) which extracts some shared
information across domains, trying to fully utilize the cross-domain commonalities such as syn-
tactic and phrasal similarity in human expressions. We try to define CDS on two levels and give
some examples to illustrate our idea. We also present a general-to-detailed neural network (GDNN)
for converting an utterance into a logic form based on meaning representation language (MRL)
form. The general network, which is meant to extract cross-domain commonalities, uses an encoder-
decoder model to obtain CDS in a multi-task setup. Then the detailed network generates the final
domain-specific target by exploiting utterance and CDS simultaneously via attention mechanism.
Our experiments demonstrate the effectiveness of CDS and multi-task learning.
CDS is able to generalize over a wide range of tasks since it is an extraction to language expressions.
Therefore, in the future, we would like to perfect the CDS definition and extend its’ ontology to other
domains and tasks. Besides, in this paper, we use attention mechanism to make use of CDS which
is still a indirect way. We would like to explore more effective ways such as constraint decoding to
further enhance the role of CDS.
References
Mostafa Abdou, Artur Kulmizev, Vinit Ravishankar, Lasha Abzianidze, and Johan Bos. What can
we learn from semantic tagging? arXiv preprint arXiv:1808.09716, 2018.
8
Under review as a conference paper at ICLR 2019
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. international conference on learning representations, 2015.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Li Dong and Mirella Lapata. Language to logical form with neural attention. meeting of the associ-
ationfor computational linguistics, 1:33-43, 2016.
Li Dong and Mirella Lapata. Coarse-to-fine decoding for neural semantic parsing. meeting of the
association for computational linguistics, pp. 731-742, 2018.
Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. Tree-to-sequence attentional neural
machine translation. meeting of the association for computational linguistics, 1:823-833, 2016.
Xing Fan, Emilio Monti, Lambert Mathias, and Markus Dreyer. Transfer learning for neural seman-
tic parsing. meeting of the association for computational linguistics, pp. 48-56, 2017.
Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and
Yun-Nung Chen. Slot-gated modeling for joint slot filling and intent prediction. In Proceedings of
The 16th Annual Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, 2018.
Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O K Li. Incorporating copying mechanism in
sequence-to-sequence learning. meeting of the association for computational linguistics, 1:1631-
1640, 2016.
Narendra K Gupta, Gokhan Tur, Dilek Hakkanitur, Srinivas Bangalore, Giuseppe Riccardi, and
Mazin Gilbert. The at&t spoken language understanding system. IEEE Transactions on Audio,
Speech, and Language Processing, 14(1):213-222, 2006.
Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz, YUn-NUng Chen, Jianfeng Gao, Li Deng, and
Ye-Yi Wang. Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. In Inter-
speech, pp. 715-719, 2016.
Daniel Hershcovich, Omri Abend, and Ari Rappoport. Multitask parsing across semantic represen-
tations. meeting of the association for computational linguistics, pp. 373-385, 2018.
Jonathan Herzig and Jonathan Berant. Neural semantic parsing over multiple knowledge-bases.
arXiv preprint arXiv:1702.01569, 2017.
SePP Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Thomas Kollar, Danielle Berry, Lauren Stuart, Karolina Owczarzak, Tagyoung Chung, Lambert
Mathias, Michael Kayser, Bradford Snow, and SPyros Matsoukas. The alexa meaning rePresen-
tation language. PP. 177-184, 2018a.
Thomas Kollar, Vittorio Perera, Emma Strubell, and Tagyoung Chung. Multi-task learning for
Parsing the alexa meaning rePresentation language. 2018b.
Jayant Krishnamurthy, PradeeP Dasigi, and Matt Gardner. Neural semantic Parsing with tyPe con-
straints for semi-structured tables. PP. 1516-1526, 2017.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. Lexical generaliza-
tion in ccg grammar induction for semantic Parsing. PP. 1512-1523, 2011.
Quoc Le and Tomas Mikolov. Distributed rePresentations of sentences and documents. In Interna-
tional Conference on Machine Learning, PP. 1188-1196, 2014.
Bing Liu and Ian Lane. Attention-based recurrent neural network models for joint intent detection
and slot filling. arXiv preprint arXiv:1609.01454, 2016a.
9
Under review as a conference paper at ICLR 2019
Bing Liu and Ian R Lane. Attention-based recurrent neural network models for joint intent detection
and slot filling. conference of the international speech communication association, pp. 685-689,
2016b.
Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classifica-
tion. arXiv preprint arXiv:1704.05742, 2017.
Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based
neural machine translation. empirical methods in natural language processing, pp. 1412-1421,
2015.
Chaitanya Malaviya, Pedro Ferreira, and Andre F T Martins. Sparse and constrained attention for
neural machine translation. meeting of the association for computational linguistics, pp. 370-376,
2018.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Matt Post and David Vilar. Fast lexically constrained decoding with dynamic beam allocation for
neural machine translation. north american chapter of the association for computational linguis-
tics, 1:1314-1324, 2018.
Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint
arXiv:1706.05098, 2017.
Anders Sogaard and Yoav Goldberg. Deep multi-task learning with low level tasks supervised at
lower layers. 2:231-235, 2016.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
neural information processing systems, pp. 3104-3112, 2014.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998-6008, 2017.
Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li, and Ming Zhou. Sequence-to-dependency
neural machine translation. 1:698-707, 2017.
Luke Zettlemoyer and Michael Collins. Online learning of relaxed ccg grammars for parsing to
logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 2007.
Luke S Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured
classification with probabilistic categorial grammars. arXiv preprint arXiv:1207.1420, 2012.
Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from
natural language using reinforcement learning. arXiv: Computation and Language, 2018.
10