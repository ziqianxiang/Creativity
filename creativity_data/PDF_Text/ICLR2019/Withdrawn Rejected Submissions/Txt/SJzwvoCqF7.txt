Under review as a conference paper at ICLR 2019
On Tighter Generalization Bounds for Deep
Neural Networks: CNNs, ResNets, and Beyond
Anonymous authors
Paper under double-blind review
Ab stract
We propose a generalization error bound for a general family of deep neural net-
works based on the depth and width of the networks, as well as the spectral norm
of weight matrices. Through introducing a novel characterization of the Lips-
chitz properties of neural network family, we achieve a tighter generalization er-
ror bound. We further obtain a result that is free of linear dependence on norms
for bounded losses. Besides the general deep neural networks, our results can be
applied to derive new bounds for several popular architectures, including convolu-
tional neural networks (CNNs), residual networks (ResNets), and hyperspherical
networks (SphereNets). When achieving same generalization errors with previous
arts, our bounds allow for the choice of much larger parameter spaces of weight
matrices, inducing potentially stronger expressive ability for neural networks.
1 Introduction
We aim to provide a theoretical justification for the enormous success of deep neural networks
(DNNs) in real world applications (He et al., 2016; Collobert et al., 2011; Goodfellow et al., 2016).
In particular, our paper focuses on the generalization performance of a general class of DNNs.
The generalization bound is a powerful tool to characterize the predictive performance of a class of
learning models for unseen data. Early studies investigate the generalization ability of shallow neural
networks with no more than one hidden layer (Bartlett, 1998; Anthony & Bartlett, 2009). More
recently, studies on the generalization bounds of deep neural networks have received increasing
attention (Dinh et al., 2017; Bartlett et al., 2017; Golowich et al., 2017; Neyshabur et al., 2015;
2017). There are two major questions of our interest in these analysis of the generalization bounds:
(Q1) Can we establish tighter generalization error bounds for deep neural networks in terms of
the network dimensions and structure of the weight matrices?
(Q2) Can we develop generalization bounds for neural networks with special architectures?
For (Q1), (Neyshabur et al., 2015; Bartlett et al., 2017; Neyshabur et al., 2017; Golowich et al.,
2017) have established results that characterize the generalization bounds in terms of the depth
D and width p of networks and norms of rank-r weight matrices. For example, Neyshabur et al.
(2015) provide an exponential bound on D based on the Frobenius norm kWd kF, where Wd is the
weight matrix of d-th layer; Bartlett et al. (2017); Neyshabur et al. (2017) provide a polynomial
bound on p and D based on kWd k2 (spectral norm) and kWd k2,1 (sum of the Euclidean norms
for all rows of Wd). Golowich et al. (2017) provide a nearly size independent bound based on
kWd kF. Nevertheless, the generalization bound depends on other than the spectral norms of the
weight matrices may be too loose. In specific, kWd|卜(IlWdk21) is in general √r (r) times larger
than kWdk2. Given m training data points and suppose kWdk2 = 1 for ease of discussion, Bartlett
et al. (2017) and Neyshabur et al. (2017) demonstrate generalization error bounds as O( D3pr/m),
and GoloWich et al. (2017) achieve a bound O(rD/2 min(mT/4,，D/m)), where O(∙) represents
the rate by ignoring logarithmic factors. In comparison, we show a tighter generalization error bound
as O( Dpr/m), which is significantly smaller than existing results and achieved based on a new
Lipschitz analysis for DNNs in terms of both the input and weight matrices. We notice that some
recent results characterize the generalization bound in more structured ways, e.g., by considering
specific error-resilience parameters (Arora et al., 2018), which can achieve empirically improved
1
Under review as a conference paper at ICLR 2019
Table 1: Comparison of existing results with ours on norm based generalization error bounds for
DNNs. For ease of illustration, we suppose the upper bound of input norm R and the Lipschitz
constant 1 of the class of loss functions g7 are generic constants. We use Bd,2, Bd,F, and Bd,2→1
as the upper bounds of kWdk2, kWdkF, and kWdk2,1 respectively. For notational convenience, we
suppose the width pd = p for all layers d = 1, . . . , D. We further show the results when kWdk2 = 1
for all d = 1,...,D, where IlWdkF = Θ(√r) and IlWdk2 1 = Θ(r) in generic scenarios.
Generalization Bound	Original Results	kWdk2 = 1
Neyshabur et al. (2015)	O (2D∙∏D=ιBf	O(r)一
Bartlett et al. (2017)	O ( ∏D=ιBd,2 (PD	b1→i Y"! OI-^m~ I=I FJ )	o(√Dmr)
Neyshabur et al. (2017)	O( ⅝da rD2 P PD=I b⅛)	Oj可)一
Golowich et al. (2017)	O(nD=iBd,F ∙ min n 册,qD})	Ok* ∙ min {√D, qm })
Our results	-TheOrem 1:O(屋速卢Dpr j' 	∖	√m	)	O( M)
generalization bounds than existing ones based on the norms of weight matrices. However, it is
not clear how the weight matrices explicitly control these parameters, which makes the results less
interpretable. Thus, we do not compare with these types of results. We summarize the comparison
between existing norm based generalization bounds with our results in Table 1, as well as the results
when kWd k2 = 1 for more explicit comparison in terms of the network sizes (i.e, depth and width).
For (Q2), we consider several widely used architectures to demonstrate, including convolutional
neural networks (CNNs) (Krizhevsky et al., 2012), residual networks (ResNets) (He et al., 2016),
and hyperspherical networks (SphereNets) (Liu et al., 2017b). By taking their structures of weight
matrices into consideration, we provide tight characterization of their resulting capacities. In partic-
ular, we consider orthogonal filters and normalized weight matrices, which show good performance
in both optimization and generalization (Mishkin & Matas, 2015; Xie et al., 2017). This is closely
related with normalization frameworks, e.g., batch normalization (Ioffe & Szegedy, 2015) and layer
normalization (Ba et al., 2016), which have achieved great empirical performance (Liu et al., 2017a;
He et al., 2016). Take CNNs as an example. By incorporating the orthogonal structure of convolu-
tional filters, We achieve O((S) S √Dk2∕√m), while Bartlett et al. (2017); Neyshabur et al. (2017)
achieve O((S)~^- pD3p2∕√m) and Golowich et al. (2017) achieve O(PDD min {泰,^/D})
(rank(Wd) = p in CNNs), where k is the filter size that satisfies k p and s is stride size that
is usually of the same order with k; see Section 4.1 for details. Here we achieve stronger results
in terms of both depth D and width p for CNNs, where our bound only depend on k rather than
p. Some recent result achieved results that is free of the linear dependence on the weight matrix
norms by considering networks with bounded outputs (Zhou & Feng, 2018). We can achieve similar
results using bounded loss functions as discussed in Section 3.2, but do not restrict ourselves to this
scenario in general. Analogous improvement is also attained for ResNets and SphereNets. In addi-
tion, we consider some widely used operations for width expansion and reduction, e.g., padding and
pooling, and show that they do not increase the generalization bound. Further numerical evaluation
is provided for quantitative comparison in Section 4.5.
Our tighter bounds result in potentially stronger expressive power, hence higher training/testing
accuracy for the DNNs. In particular, when achieving the same order of generalization errors, we
allow the choice of a larger parameter space with deeper/wider networks and larger matrix spectral
norms. We further show numerically that a larger parameter space can lead to better empirical
performance. Quantitative analysis for the expressive power of DNNs is of great interest on its
own, which includes (but not limited to) studying how well DNNs can approximate general class
of functions and distributions (Cybenko, 1989; Hornik et al., 1989; Funahashi, 1989; Barron, 1993;
1994; Lee et al., 2017; Petersen & Voigtlaender, 2017; Hanin & Sellke, 2017), and quantifying the
computation hardness of learning neural networks; see e.g., Shamir (2016); Eldan & Shamir (2016);
Song et al. (2017). We defer our investigation toward this to future efforts.
2
Under review as a conference paper at ICLR 2019
Notation. Given an integer n > 0, we define [n] = {1, . . . , n}. Given a matrix A ∈ Rn×m,
we denote kAk as a generic norm, kAk2 as the spectral norm, kAkF as the Frobenius norm, and
∣∣Ak21 = Pn=IIlAi*∣∣2∙ We use the standard notations O (∙), Θ (∙), and Ω (∙) to denote limiting
1 1	,∙	,	, F 二 /、 U /、	1 X Z X ,	.	,1	∙	1	∙,1	∙.,
behaviors ignoring constants, and O (∙), Θ(∙) and Ω(∙) to further ignore logarithmic factors.
2 Preliminaries
We provide a brief description of the DNNs. Given an input x ∈ Rp0, the output of a D-layer
network is defined as f°f (Wd, x) = fw0 (…fwι (x)) ∈ RpD, where fwd(y) = σd (Wd ∙ y):
Rpd-1 → Rpd with an entry-wise activation function σd(∙). We specify ◎& as the rectified linear
unit (ReLU) activation (Nair & Hinton, 2010) for ease of discussion. The extension to more general
activations, e.g., Lipschitz continuous functions, is straightforward. Then we denote DNNs with
bounded weight matrices WD = {Wd ∈ Rpd×pd-1 }dD=1 and ranks as
FDJHl = ff (Wd，x)∣∀d ∈ D],Wd ∈ Wd, ∣Wdk ≤ Bd, rank(Wd) ≤ rd},	(1)
where X ∈ Rp0 is an input, and {Bd} are real positive constants. We will specify the norm ∣∣∙∣ and
the corresponding upper bounds Bd, e.g., ∣∙∣2 and Bd,2, or |卜|卜 and Bd,F, when necessary.
Given a loss function g(∙, ∙),we denote a class of loss functions measuring the discrepancy between
a DNN’s output f (WD, x) and the corresponding observation y ∈ Ym for a given input x ∈ Xm as
G (FDjHl) = {g(f (WD,x) ,y) ∈ R | X ∈ Xm,y ∈ Ym,f(∙,，) ∈ FDjH∣},
where the sets of bounded inputs Xm and the corresponding observations Ym are
Xm = {Xi ∈ Rp0 | ∣Xi∣2 ≤ R for all i ∈ [m]} ⊂ X and Ym = {yi ∈ [pD] for all i ∈ [m]} ⊂ Y.
Then the empirical Rademacher complexity (ERC) of G (Fd,∣∙∣) given Xm and Ym is
Rm (G(FD,MD )=工
sup
f (∙,∙)∈FD,k∙k
1m
~y2f( ∙ g(f (WD,χi) ,yi)
m
i=1
(2)
where {±1}m ∈ Rm is the set of vectors only containing entries +1 and -1, and ∈ Rm is a vector
with Rademacher entries, i.e., i = +1 or -1 with equal probabilities.
Take the classification as an example. For multi-class classification, suppose pD = Nclass is the
number of classes. Consider g with bounded outputs, namely the ramp risk. Specifically, for an
input X belonging to class y ∈ [Nclass], we denote ν = (f (WD,X))y - maxi6=y (f (WD,X))i. For
a given real value γ > 0, the class of ramp
risk functions with parameter Y is GY (Fd,∣∙∣) =	j 0,	ν>γ
{gγ (f (Wd ,x) ,y) IfD ∈ Fd,k∙k}, where gγ is gY (f(WD,x) ,y) = j；-Y, V < %γ] ⑶
Y -Lipschitz continuous, defined in (3). For conve-	1 ,	,
nience, we denote gY (f (WD, X) , y) as gY (f (WD, X)) (or gY) in the rest of the paper.
Then the generalization error bound for PAC learning (Bartlett et al., 2017) (Lemma 3.1) states the
following. Given any real δ ∈ (0, 1) and gY, with probability at least 1 - δ, we have that for any
f (∙, ∙) ∈ FD,∣∙k, the generalization error is upper bounded with respect to (w.r.t.) the ERC satisfies
E [gY (f (WD, X))] — mm Pm=I gγ (f (WD, Xi)) ≤ 2Rm (GY (FD,k∙k)) + 3Q'Rm) ∙	(4)
The right hand side of (4) is viewed as a guaranteed error bound for the gap between the testing and
the empirical training performance. Since the ERC is generally the dominating term in (4), a small
Rm is desired for DNNs given the loss function gY . Analogous results hold for regression tasks; see
e.g., Kearns & Vazirani (1994); Mohri et al. (2012) for details.
3	Generalization Error B ound for DNNs
We introduce some additional notations first. Given any two layers i, j ∈ [D] and input X, we denote
Jixj as the Jacobian from layer i to layer j, i.e., fwj (…fwi (x)) = Jj x. For convenience, we
denote fwi (x) = Jixi ∙ X when i = j and denote Jixj = I when i > j. Next, we denote BJaC,X as an
upper bound of the norm of Jacobian for input X over the parameter, i.e., supWD Jix,i 2 ≤ BiJ:ajc,,2x.
3
Under review as a conference paper at ICLR 2019
3.1	A Tighter ERC Bound for DNNs
We first provide the ERC bound for the class of DNNs defined in (1) and Lipschitz loss functions in
the following theorem. The proof is provided in Appendix B.
Theorem 1. Let gγ be a Y-LiPschitz loss function and Fd,∣∣∙∣∣2 be the class of DNNs defined in
(1), pd = p, rd = r for all d ∈ [D], B瞑 = maXd∈[D],χ∈Xm 右5-劫/牌；”？ and CNet =
B'd?RdD'm//"m；；dBd,2	. Then the ERC satisfies
suPf∈FD,k∙k2 ,x∈Xm gγ (f (WD，x))	_________
Rm (GY CFD,k∙k2)) = O (RnD=I Bdγ√DprlogCet)	(5)
Remark 1. Note that CNet dePends on the norm of Jacobian, which is significantly smaller than the
Product of matrix norms that is exPonential on D in general. For examPle, when we obtain the net-
work from stochastic gradient descent using randomly initialized weights, then B\Jadc,2 Qd Bd,2.
EmPirical distributions of B\Jadc,2 and Qd Bd,2 are Provided in APPendix A.2, where B\Jadc,2 ’s are
constants that are orders of magnitude smaller than Qd Bd,2 . Further exPeriment in APPendix A.3
shows that B\Jadc,2 has a dePendence slower than some low degree Poly(dePth), rather than exPonential
on the dePth as in Qd Bd,2. Thus, log CNet can be considered as a constant almost indePendent of
D in Practice. Even in the worst case that B\Jadc,2 ≈ Qd Bd,2 (this almost never haPPens in Practice),
our bound is still tighter than existing spectral norm based bounds (Bartlett et al., 2017; Neyshabur
et al., 2017) by an order of √D. Also note that CNet is a quantity (including B∖1dc2) only depending
on the training dataset, which is due to the fact that the ERC only depends on the training dataset.
For convenience, We treat RlY as a constant here. We achieve Ο(ΠD=ιBd,2 ∙ PDprlm in The-
orem 1, which is tighter than existing results based on the network sizes and norms of weight
matrices, as shoWn in Table 1. In particular, Neyshabur et al. (2015) shoW an exponential depen-
dence on D, i.e., O(2dΠ,D=1BdF/√m), which can be significantly larger than ours. Bartlett et al.
(2017); Neyshabur et al. (2017) demonstrate polynomial dependence on sizes and the spectral norm
of weights, i.e., Ο(ΠD=IBd,2 ∙ pD3pr∕m). OUr result in (5) is tighter by an order of D, which is
significant in practice. More recently, Golowich et al. (2017) demonstrate a bound w.r.t the Frobe-
nius norm as O(πD=1Bd,F ∙ min {qt ,m-4 ∙ log3 (m) pIog(C) })，where C = SupxRmf⅛x)k2 .
This has a tighter dependence on network sizes. Nevertheless, k WdkF is generally √r times larger
than kWd k2, which results in an exponential dependence pD/2 compared with the bound based on
the spectral norm. Moreover, log(C) is linear on D except that the stable ranks kWdkF / kWdk2
across all layers are close to 1 (rather than almost independent on D as in (5) without low-rank
constraints). In addition, it has m-4 dependence rather than m- 1 except when D = O (√m). Note
that our bound is based on a novel characterization of Lipschitz properties of DNNs, which may be
of independent interest from the learning theory point of view. We refer to Appendix B for details.
We also remark that when achieving the same order of generalization errors, we allow the choices of
larger dimensions (D, p) and spectral norms of weight matrices, which lead to stronger expressive
power for DNNs. For example, when achieving the same bound with kWd k2 = 1 in spectral norm
based results (e.g. in ours) and kWdkF = 1 in Frobenius norm based results (e.g., in Golowich et al.
(2017)), they only have IlWdk2 = O(1∣√r) in Frobenius norm based results. The later results in a
much smaller space for eligible weight matrices as r is of order p in general (i.e., r = δp for some
constant δ ∈ (0, 1)), which may lead to weaker expressive ability of DNNs. We also demonstrate
numerically in Section 4.5 that when norms of weight matrices are constrained to be very small,
both training and testing performance degrade significantly. A quantitative analysis for the tradeoff
between the expressive ability and the generalization for DNNs is deferred to a future effort.
3.2	A Spectral Norm Free ERC Bound
When, in addition, the loss function is bounded, we have the ERC bound free of the linear depen-
dence on the spectral norm, as in the following corollary. The proof is provided in Appendix C.
Corollary 1. In addition to the conditions in Theorem 1, suppose we further let gY be bounded, i.e.,
|gY | ≤ b. Then the ERC satisfies
Rm (GY (FD,k∙k2)) = O (min n R nDY1 Bd ,b0 ∙ q Dpr lmg CNet) .	(6)
4
Under review as a conference paper at ICLR 2019
The boundedness of Gγ holds for certain loss functions, e.g., the ramp risk defined in (3). When b
is constant (e.g., b = 1 for the ramp risk) and R QdD=1 Bd,2 > γ, we have that the ERC reduces to
O( Dpr/m). This is close to the VC dimension of DNNs, which can be significantly tighter than
existing norm based bounds in general. Similar norm free results hold for the architectures discussed
in Section 4 using argument for Corollary 1, which we skip due to space limit. Moreover, our bound
(6) is also tighter than recent results that are free of linear dependence on QdD=1 Bd,2 (Zhou &
Feng, 2018; Arora et al., 2018). Specifically, Zhou & Feng (2018) show that the generalization
bound for CNNs is Oe(Dypillm, which results in a bound larger than (6) by O(√Dr). Arora
et al. (2018) derive a bound for a compressed network in terms of some error-resilience parameters,
which is O( √D3p2/m) since the cushion parameter therein is of the order μ = O(1∕√p). Further
discussion is provided in Appendix A.1.
4	Exploring Network S tructures
The generic result in Section 3 does not highlight explicitly the potential impacts for specific struc-
tures of the networks. In this section, we consider several popular architectures of DNNs, including
convolutional neural networks (CNNs) (Krizhevsky et al., 2012), residual networks (ResNets) (He
et al., 2016), and hyperspherical networks (SphereNets) (Liu et al., 2017b), and provide sharp char-
acterization of the corresponding generalization bounds. In particular, we consider orthogonal filters
and normalized weight matrices, which have shown good performance in both optimization and gen-
eralization (Mishkin & Matas, 2015; Huang et al., 2017). Such constraints can be enforced using
regularizations on filters and weight matrices, which is very efficient to implement in practice. This
is also closely related with normalization approaches, e.g., batch normalization (Ioffe & Szegedy,
2015) and layer normalization (Ba et al., 2016), which have achieved tremendous empirical success.
4.1	CNNs with Orthogonal Filters
CNNs are one of the most powerful architectures in deep learning, especially in tasks related to im-
ages and videos (Goodfellow et al., 2016). We consider a tight characterization of the generalization
bound for CNNs by generating the weight matrices using unit norm orthogonal filters, which has
shown great empirical performance (Huang et al., 2017; Xie et al., 2017). Specifically, we generate
the weight matrices using a circulant approach, as follows. For the convolutional operation at the d-
th layer, we have nd channels of convolution
filters, each of which is generated from a kd-
dimensional feature using a stride side sd. Sup-
pose that Sd divides both kd and pd-ι, i.e., kd-1
sd
Wd
[wd1)>... wdnd)>i>∈ Rpd Xpd-I, (7)
and pd-1 are integers, then We have Pd
nd∙pd-ι
Sd
This is equivalent to fixing the weight matrix at
the d-th layer to be generated as in (7), where
for all j ∈ [nd], each Wdj) ∈ R Sd XPd-I
is formed in a circulant-like way using a vector
w(d,j) ∈ Rkd with unit norms for allj as in (8).
w(d,j) 0.................
|------------{----
∈Rpd-1-kd
w(dj)0..................0
1---------{---------}
∈Rpd-1-kd-sd
. (8)
(d,j)
,(sd+1)ιkd 0
—{—J} w(dSd)
∈Rpd-1-kd
0
}

w
When the stride size sd = 1, Wd(j) corresponds to a standard circulant matrix (Davis, 2012). The
following lemma establishes that when w(d,j) jn=d 1 are orthogonal vectors with unit Euclidean
norms, the generalization bound only depend on sd and kd that are independent of the width pd. The
proof is provided in Appendix D.
Corollary 2. Let gγ be a Y-Lipschitz and bounded loss function, i.e., ∣gγ| ≤ b, and Fd』[％ be the
class of CNNs defined in (1). Suppose the weight matrices in CNNs are formed as in (7) and (8)
with Sd = s, kd = k, and S divides both k and Pd for all d ∈ [D], where {wldj)}；;[ satisfies
w(j)>w(i) = 0 for all i,j ∈ [nd] and i 6= j with w(d,j) 2 = 1 for all j ≤ nd. Denote CNet =
B\d,2 RJDm/s
suPf∈FD,k∙k2 ,x∈Xm gγ (f (WD，x)).
Then the ERC for CNNs satisfies
Rm (GY (FdjhJ) =O (mm { R^，b} ∙ J K P= ^o CNet) ∙
5
Under review as a conference paper at ICLR 2019
Since n ≤ k in our setting, the ERC for CNNs is proportional to √Dk2 instead of √Dpr. For the
orthogonal filtered considered in Corollary 2, We have IlWdkF = √Pd and IWdk21 = Pd, which
lead to the bounds of CNNs in existing results in Table 2. In practice, one usually has kd《Pd,
which exhibit a significant improvement over existing results, i.e., √Dk2《,D3p2. Even without
the orthogonal constraint on filters, the rank r in CNNs is usually of the same order with width P,
which also makes the existing bound unde-
sirable. On the other hand, it is widely
used in practice that kd = μsd for some
small constant μ ≥ 1 in CNNs, then we
have (kd/sd)D/2	PD/2 resulted from
Rm (GY (fDJHIf)).
Remark 2. We consider vector input in
Corollary 2. For matrix inputs, e.g., images,
similar results hold by considering vectorized
input and permuting columns of Wd. Specif-
ically, suppose √kd and √pd-1 are integers
for ease of discussion. Consider the input as a
Pd-1 dimensional vector obtained by vector-
izing a √Pd-ι X √Pd-ι input matrix. When
the 2-dimensional (matrix) convolutional fil-
ters are of size √kd × √kd, we form the rows
(j)
of each Wd by concatenating kd vectors
{w(j,i)}√=d padded with 0's, each of which is
a concatenation of one row of the filter of size
√kd with some zeros as follows:
Table 2: Comparison with existing norm based
bounds of CNNs. We suppose R and γ are generic
constants for ease of illustration. The results of
CNNs in existing works are obtained by substitut-
ing the corresponding norms of the weight matri-
ces generated by orthogonal filters, i.e., kWd k2 =
p/s, kWd∣∣F = √p, and kWdk2,1 = p.
Generalization Bound
Neyshabur et al. (2015)
Bartlett et al. (2017)
Neyshabur et al. (2017)
Golowich et al. (2017)
Our results
CNNs
〜
Oe
w(j,1) 0........0 ............ w(j，Vkd) 0.......0 0.............0.
"l{Z∙} 、	7	'	、	{	' 、	7	‘ 、 V '
∈R√kd ∈R∙rpkF -Vkd	∈R√kd ∈R∙∕pkF -Vkd ∈Rpd-L√pd-1
Correspondingly, the stride size is kd on average and we have kWdk2 ≤ kd if ∣∣w(j,i) ∣[ = 1 for all
i, j ; see Appendix F for details. This is equivalent to permuting the columns of Wd generated as in
(8) by vectorizing the matrix filters in order to validate the convolution of the filters with all patches
of the matrix input.
Remark 3. A more practical scenario for CNNs is when a network has a few fully connected layers
after the convolutional layers. Suppose we have DC convolutional layers and DF fully connected
layers. From the analysis in Corollary 2, when sd = kd for convolutional layers and kWdk2 = 1 for
fully connected layers, we have that the overall ERC satisfies O( R YDC√√2+Dp).
4.2	ResNets with Structured Weight Matrices
Residual networks (ResNets) (He et al., 2016) is one of the most powerful architectures that allows
training of tremendously deep networks. Then we denote the class of ResNets with bounded weight
matrices VD = {Vd ∈ Rpd×qd}dD=1,UD = {Ud ∈ Rqd×pd-1}dD=1 as
FDNHI = f(VD,Ud，x)∈ RpDlkVdk≤ BVd,k%k≤ Bud},	⑼
Given an input x ∈ Rp0 , the output of a D-layer ResNet is defined as f (VD, UD, x) =
fvD,Ud (…fv1,U1 (x)) ∈ RpD, where fvd,ud (X)= σ (Vd ∙ σ (Udx) + x). For any two layers i,j ∈
[D] and input x, we denote JX as the Jacobian from layer i to layer j, i.e., fvi,Uj (∙ ∙ ∙ fvi,ui (x))=
Jixj ∙ x, and BJaC,X as an upper bound of the norm of Jacobian for input X over the parameter, i.e.,
supWD ∣Jix,i∣2 ≤ BiJ:ajc,,2x. We then provide an upper bound of the ERC for ResNets in the following
corollary. The proof is provided in Appendix E.
Corollary 3. Let gγ be a Y-Lipschitz and bounded loss function, i.e., ∣gγ | ≤ b, and FDNl∙∣ be
the ResNets defined in (9) with Pd = P and qd = q for all d ∈ [D], B\Jadc,2 = maxd∈[D],x∈Xm
BJac,χ 1)2BJac,χx θ2,and CNet = Bad, maxd(BVd"+♦*，"R√mqτ. Then the ERC satisfies
1:(dT),2 (d+I):D,2,	SUPf ∈Fd,k∙k2 ,χ∈χm gγ (f(VD,VD ,X))
6
Under review as a conference paper at ICLR 2019
Rm 仅 Y (FDlkJ)= O (min { RQ=I(BVrBUd,2+1)，b} ∙ / DPqm CNet) ∙
Compared with the D-layer networks without shortcuts (1), ResNets have a stronger dependence
on the input due to the shortcuts structure, which leads to (BVd,2BUd,2 + 1) dependence for each
layer. When Bvd,2 and Bud,2 are of order 1∕√D, We still have QD=I(BVd,2B% ,2 + 1) = O(1). This
partially explains the observation in practice that ResNets have good performance when the weight
matrices have relatively small scales. Also note that to achieve the same bound for Rm (GY (FDNk ,1/),
We require Bvd,F, Bud,F ≤ c, Which leads to a much smaller parameter space than the space corre-
sponding to Bvd,2, Bud,2 ≤ c for the same c.
4.3	Hyperspherical Networks
We also consider the hyperspherical netWorks (SphereNets) (Liu et al., 2017b), Which demonstrate
improved performance than the vanilla DNNs. In specific, the SphereNets has the same architec-
ture With DNNs defined in (1), except that the Weight matrix can be vieWed as Wd = SWd Wd,
Where SWd is a diagonal matrix With the i-th diagonal entries being the Euclidean norm of the
i-th roW of Wd. Note that We do not normalize the input x as in (Liu et al., 2017b) for ease
of the discussion. A direct result of applying Theorem 1 implies that Rm (GY (FDNk) =
O(R ∙ QD=I BWd 2 ∙ √ Dpr/ (γ√m)). Such a self-normalization architecture has a benefit that Bfd 2
is small (close to 1) in general When the Weights are spread out. In addition, it has loWer com-
putational costs than the Weight normalization based on the spectral norm directly, and improved
empirical results over batch normalization have been observed (Liu et al., 2017b;a).
4.4	Extension to Width-Change Operations
Change the Width for certain layers is a Widely used operation, e.g., for CNNs and ResNets,
Which can be vieWed as a linear transformation in many cases. In specific, denote x{d} ∈ Rpd
as the output of the d-th layer. Then We can use a transformation matrix Td ∈ Rpd+1 ×pd to
denote the operation to change the dimension betWeen the output of the d-th layer and the in-
put of the (d + 1)-th layer as fWd+1 x{d} = σ Wd+1Tdx{d} . Denote the set of layers
With Width changes by IT ⊆ [D]. Combining With Theorem 1, We have that the ERC satisfies
Rm (GY (FDjhQ) = O ( Rnd=IBd,2∏√m kTtk2∙√Dpr). Next, we illustrate several popular ex-
amples to shoW that Πt∈IT kTt k2 is a size independent constant. We refer to GoodfelloW et al.
(2016) for more operations of changing the width.
Width Expansion. Two popular types of width expansion are padding and 1 × 1 convolution. For
ease of discussion, suppose pd+1 = S ∙ Pd for some positive integer S ≥ 1. Taking padding with 0 as
an example, where we plug in (s - 1) zeros before each entry of x{d}, which is equivalent to setting
Td ∈ Rspd×pd with (Td)ij = 1 ifi = jS, and (Td)ij = 0 otherwise. This implies that kTdk2 = 1.
For 1 × 1 convolution, suppose that the convolution features are {c1, . . . , cs}. Then we expand
width by performing convolution (essentially entry-wise product) using S features respectively. This
is equivalent to setting Td ∈ Rspd×pd with (Td)ij = ck ifi = j+(k- 1)S fork ∈ [S], and (Td)ij = 0
otherwise. It implies that kTd k2 = PPS=1 C. When PS=1 C ≤ 1, we have IlTdk2 ≤ 1.
Width Reduction. Two popular types of width reduction are average pooling and max pooling.
Suppose pd+1 = pd is an integer for some positive integer s. For average pooling, we pool each
pd
nonoverlapping S features into one feature. This is equivalent to setting Td ∈ R 三 ×pd with (Td)j =
1/s if j = (i - 1)s + k for k ∈ [s], and (Td)切=0 otherwise. This implies that kTdk2 = pl/s.
For max pooling, we choose the largest entry in each nonoverlapping feature segment of length S.
Denote the set IS = {(i 一 1) X S +1,...,i ∙ s}. This is equivalent to setting Td ∈ RPsd×pd with
(Td)ij = 1 if |(x{d})j | ≥ |(x{d})k| ∀ k ∈ Is, k 6= j, and (Td)ij = 0 otherwise. This implies that
kTdk2 = 1. For pooling with overlapping features, similar results hold.
7
Under review as a conference paper at ICLR 2019
4.5	Numerical Evaluation
To better illustrate the difference between our result and existing ones, We demonstrate some com-
parison results in Figure 1 using real
data. In specific, we train a simpli-
fied VGG19-net (Simonyan & Zisser-
man, 2014) using 3 X 3 convolution fil-
ters (with unit norm constraints) on the
CIFAR-10 dataset (Krizhevsky & Hinton,
2009). We first compare with the capacity
terms in Bartlett et al. (2017) (Bound1),
Neyshabur et al. (2017) (Bound2), and
Golowich et al. (2017) (Bound3) by ignor-
ing the common factor -R= as follows:
YMm
Figure 1: Panel (a) shows comparison results for the
same VGG19 network trained on CIFAR10 using unit
norm filters. The vertical axis the corresponding bounds
in the logarithmic scale. Panel (b) shows the training ac-
curacy (red diamond), testing accuracy (blue cross), and
the empirical generalization error using different scales
of the filters listed on the horizontal axes.
•	Ours: ∏D=ιBd,2 Jk PD=I nd
•	BoUndL πD=1Bd,2 (PD=I B,2→1 )
•	Bound2: ΠD=1Bd,2 ,D2p PD= 筌F
•	Bound3: ∏D=ιBd,F√D
Note that since we may have more filters nd than their dimension k, we do not assume orthogonality
here. Thus we simply use the upper bounds of norms Bd rather than the form as in Table 2. Follow-
ing the analysis of Theorem 1, we have Jk PD=I n dependence rather than √Dpr as k PD=I n
is the total number free parameter for CNNs, where nd is the number of filters at d-th layer. Also
note that we ignore the logarithms factors in all bounds for simplicity and their empirical values are
small constants compared with the the dominating terms.
For the same network and corresponding weight matrices, we see from Figure 1 (a) that our result
(104 〜105) is significantly smaller than Bartlett et al. (2017); Neyshabur et al. (2017) (108 〜109)
and Golowich et al. (2017) (1014 〜1015). As we have discussed, our bound benefits from tighter
dependence on the dimensions. Note that k PdD=1 nd is approximately of order Dk2, which is sig-
nificantly smaller than (PD=I B2∕→ι∕B2N in Bartlett et al. (2017) and D2p PD=I p^B^F/B^^ in
Neyshabur et al. (2017) (both are of order D3pr). In addition, this verifies that spectral dependence
is significantly tighter than Frobenius norm dependence in Golowich et al. (2017). Further, we show
the training accuracy, testing accuracy, and the empirical generalization error using different scales
on the norm of the filters in Figure 1 (b). We see that the generalization errors decrease when the
norm of filters decreases. However, note that when the norms are too small, the accuracies drop
significantly due to a potentially much smaller parameter space. Thus, the scales (norms) of the
weight matrices should be nether too large (induce large generalization error) nor too small (induce
low accuracy) and choosing proper scales is important in practice as existing works have shown. On
the other hand, this also support our claim that when Rm (GY (尸0,卜仙))(or other existing bound)
attains the same order with our Rm (GY (左/川3))，we have better training/testing performance.
Further experimental result that compare our bound (Corollary 1) with (Zhou & Feng, 2018; Arora
et al., 2018) in the bounded output case is provided in Appendix A.1.
We want to remark that all numerical evaluations are empirical estimation of the generalization
bounds, rather than their exact values. This is because all existing bounds requires to take uniform
bounds of some quantities on parameters or the supremum value over the entire space, which is em-
pirically not accessible. For example, in the case that when it involve the upper/lower bound of quan-
tities (norm, rank, or other parameters) depending on weight matrices, theoretically we should take
the values of their upper/lower bounds (this leads to worse empirical bounds) rather than estimating
them from the training process; or in the case that the bounds involve some quantities depending on
the supremum over the entire parameter space, numerical evaluations cannot exhaust the entire pa-
rameter space to reach the supremum (Bartlett et al., 2017; Golowich et al., 2017; Neyshabur et al.,
2015; 2017; Zhou & Feng, 2018; Arora et al., 2018). Our experiments here (including Appendix A)
cannot avoid such restrictions, but the comparison is fair across various bounds as they are obtained
from the same training process.
8
Under review as a conference paper at ICLR 2019
References
Martin Anthony and Peter L Bartlett. Neural Network Learning: Theoretical Foundations. Cam-
bridge University Press, 2009.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information Theory, 39(3):930-945, 1993.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
Learning, 14(1):115-133, 1994.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of
the weights is more important than the size of the network. IEEE Transactions on Information
Theory, 44(2):525-536, 1998.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray KavUkcUoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Re-
search, 12(Aug):2493-2537, 2011.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-
trol, Signals and Systems, 2(4):303-314, 1989.
Philip J Davis. Circulant Matrices. American Mathematical Soc., 2012.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. arXiv preprint arXiv:1703.04933, 2017.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference
on Learning Theory, pp. 907-940, 2016.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.
Neural Networks, 2(3):183-192, 1989.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence
and Statistics, pp. 249-256, 2010.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541, 2017.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press Cambridge, 2016.
Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural Networks, 2(5):359-366, 1989.
Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, Yongliang Wang, and Bo Li. Orthogonal
weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep
neural networks. arXiv preprint arXiv:1709.06079, 2017.
9
Under review as a conference paper at ICLR 2019
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Michael J Kearns and Umesh Virkumar Vazirani. An Introduction to Computational Learning The-
ory. MIT Press, 1994.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Holden Lee, Rong Ge, Andrej Risteski, Tengyu Ma, and Sanjeev Arora. On the ability of neural
nets to express distributions. arXiv preprint arXiv:1702.07028, 2017.
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. In IEEE Conference on Computer Vision and Pattern
Recognition, volume 1, 2017a.
Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep
hyperspherical learning. In Advances in Neural Information Processing Systems, pp. 3953-3963,
2017b.
Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422,
2015.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT Press, 2012.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.
In International Conference on Machine Learning, pp. 807-814, 2010.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564, 2017.
Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions
using deep relu neural networks. arXiv preprint arXiv:1709.05289, 2017.
Ohad Shamir. Distribution-specific hardness of learning neural networks. arXiv preprint
arXiv:1609.01037, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural net-
works. In Advances in Neural Information Processing Systems, pp. 5520-5528, 2017.
Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution
for training extremely deep convolutional neural networks with orthonormality and modulation.
arXiv preprint arXiv:1703.01827, 2017.
Pan Zhou and Jiashi Feng. Understanding generalization and optimization performance of deep
cnns. arXiv preprint arXiv:1805.10767, 2018.
10
Under review as a conference paper at ICLR 2019
A Extended Numerical Results
A. 1 Comparison with Existing Results with B ounded Output
We compared our norms based bound with several other norm based results in Section 4.5. In
this section, we further compare our result with bounded output in Corollary 1 with Zhou & Feng
(2018); Arora et al. (2018). Analogous to Section 4.5, we train a simplified VGG19-net (Simonyan
& Zisserman, 2014) using 3 × 3 convolution filters (with unit norm constraints) on the CIFAR-10
dataset (Krizhevsky & Hinton, 2009). We compare with the capacity terms in Zhou & Feng (2018)
(Bound 1) and Arora et al. (2018) (Bound 2) by ignoring the factor -Rm as follows:
• OUrs: b∖∕k PD=ι
nd . Note that b
1 in this case.
• Bound1:
Ddp PD=1 rank(Wd) • nd. Note that their activation functions for the interme-
diate layers are sigmoid and the last layer is softmax, with squared error, which essentially
has a bounded output. We also take the last layer as convolution layer for ease of discussion.
• Bound2: maxχ∈χm ∣∣f (Wd, x)k2 ∙ CD。、IPD=I d2//, where C = Ω(1) is the activa-
μdμd→
tion contraction, β = Ω(1) is the well-distributed Jacobian, μd = O(1∕√p) is the layer
cushion, and μd→ = O(1∕√p) is the interlayer cushion. See more details in Arora et al.
(2018).
Figure 2: Comparison results for the same VGG19 network trained on CIFAR10. The vertical axis
is the corresponding bounds in the logarithmic scale.
The resulting bounds on the trained networks are provided in Figure 2. We observe that our bound is
smaller than Zhou & Feng (2018); Arora et al. (2018) by at least an order of magnitude. Specifically,
our bound is of order ≈ 102, while Zhou & Feng (2018) result in a bound of order > 103 and Arora
et al. (2018) result in a bound of order > 104. This coincide with our discussion in Section 3.2 and
allows us to obtain non-vacuous bound (generalization bound < 1) with moderate training sample
sizes (e.g., m = Ω(104)). Also note that compared with the norm based based results in Figure 1,
the results here based on the bounded output are significantly tighter in general. We regard this as
a trade-off between the looser norm based bounds that allow more explicit interpretability in terms
of the weight matrix structures and the tighter bounds that are more obscure in terms of dependence
on the network structures. A result that have both merits are desired as a future direction.
A.2 COMPARISON BETWEEN B\JAdC,2 AND Qd Bd,2
We demonstrate the empirical difference between B\Jadc,2 and Qd Bd,2. Using the same setting of the
network and dataset as above, we provide the empirical distribution of B\Jadc,2 and Qd Bd,2 over the
training set using different random initializations of weight matrices, which is provided in Figure 3.
We can observe that the values of B\Jadc,2 are approximately 2 orders smaller than the values of
Qd Bd,2, which support our claim that B\Jadc,2 is a tighter quantification then Qd Bd,2.
11
Under review as a conference paper at ICLR 2019
Figure 3: Empirical distribution of B\a2 and Qd Bd,2 for the same VGG19 network trained on
CIFAR10.	,
A.3 DEPENDENCE OF B\JAdC,2 AND Qd Bd,2 ON DEPTH
We further provide an empirical evaluation to see how strong the quantities B\Jadc,2 and Qd Bd,2
depend on the depth. Note that we use d as the variable for depth. Using the same setting as above,
we provide the magnitude of log B\Jadc,2 and log Qd Bd,2 in Figure 4. We also provide the plots
for log d and log d2 as reference. We can observe that log Qd Bd,2 has an approximately linear
dependence on the depth, which matches with our intuition. In terms of log B\Jadc,2, we can see that it
has a significantly slower increasing rate than log Qd Bd,2. Compared with the reference plots log d
and log d2, we can observe even a slower rate than log d2. This further indicates that log B\Jadc,2 may
has a dependence slower than some low degree of poly(d).
2	4	6	8	10	12	14
Depth
Figure 4: Comparison results for the dependence of B∖ac2 and Qd Bd,2 on depth. The horizontal
axes is the depth and the vertical axes is the values of corresponding quantities in the logarithmic
scale.
B Proof of Theorem 1
We start with some definitions of notations. Given a vector x ∈ Rp , we denote xi as the i-th
entry, and xi:j as a sub-vector indexed from i-th to j-th entries of x. Given a matrix A ∈ Rn×m,
we denote Aij as the entry corresponding to i-th row and j-th column, A^ (A*i) as the i-th row
(column), AI1I2 as a submatrix of A indexed by the set of rows I1 ⊆ [n] and columns I2 ⊆ [m].
Given two real values a, b ∈ R+, we write a . (&)b if a ≤ (≥)cb for some generic constant c > 0.
Our analysis is based on the characterization of the Lipschitz property of a given function on both
input and parameters. Such an idea can potentially provide tighter bound on the model capacity in
terms of these Lipschitz constants and the number of free parameters, including other architectures
12
Under review as a conference paper at ICLR 2019
of DNNs. We first provide an upper bound for the Lipschitz constant of f (WD , x) in terms of the
input x.
Lemma 1. Given WD, for any f (Wd, ∙) ∈ Fd,k∙∣∣2 and x1,x2 ∈ Rp0, We have
D
kf(WD,xι)- f (Wd,x2)k2 ≤ kxι- x2k2 ∙ Y Bd.
d=1
Proof. We prove by induction. Specifically, We have
kf(WD,x1)-f(WD,x2)k2 = kσ (WDf (WD-1, x1)) - σ (WDf (WD-1, x2))k2
(i)
≤ kWDf (WD-1, x1) - WDf (WD-1, x2)k2
≤ kWD k2 ∙ kf (WD-1,Xl) — f (WD-l,X2)k2
≤ Bd ∙kf (Wd-i,xi) — f (WD-1,x2)k2,
where (i) comes from the entry-wise I-LiPschitz continuity of σ(∙). For the first layer, We have
kf (W1,x1) — f (W1,x2)k2 = kσ (W1x1) — σ (WDx2)k2 ≤ kW1x1 — W1x2k2
≤ Bi ∙ ∣∣xι — x2k2.
By repeating the argument above, we complete the proof.	□
Next, we Provide an uPPer bound for the LiPschitz constant off (WD, x) in terms of the Parameters
WD.
Lemma 2. Given any X ∈ Rp0 satisfying ∣∣x∣2 ≤ R, for any f (Wd,x) , f (Wd,x) ∈ Fd,k∙∣∣2
with WD = {Wd}d=1 and WD = nWdod=1, and denote B\Jadc,,2x = maxd∈[D] B1Ja:(c,dx-1),2B(Jdac+,x1):D,2,
then we have
D
-—■
f(WD,x)—f WfD,x 2 ≤ B\Jadc,,2x
∙ R√Dt EkWd- WdkF.
d=1
Proof. Given x and two sets of weight matrices {Wd}dD=1,
Wfd
D
, we have
d=1
IIfWD (fWD-1 (…fWl (X))) — ffD (ffD-i (…ffι (X))) Il2
D
= IIX	fWD	∙ ∙ ∙	fWd+1	fWfd	∙ ∙ ∙ fWf1	(X)	— fWD	∙ ∙ ∙ fWd	fWfd-1	∙ ∙ ∙	fWf1	(X)	II
Id=1	I2
D
≤ X	IIfWD	∙ ∙ ∙ fWd+1	fWfd	∙ ∙	∙ fWf1	(X)	— fWD	∙ ∙
d=1
∙ fWd (fWfd-1 (∙ ∙ ∙ fWf1 (X))))II2
(=ii)
2
≤ X B(Jdac+,x1):D ∙ IIWdfWfd-1 (∙ ∙ ∙ fWf1 (X)) — WdfWfd-1 (∙ ∙ ∙ fWf1 (X))II2
d=1
D
≤ X	B(Jadc+,x1):D	∙	IIWd	— WfdII2 ∙	IIfWfd-1	∙∙∙fWf1(X)	II2,
d=1
(10)
where (i) is derived from adding and subtracting intermediate neural network functions recurrently,
where fWD ∙ ∙ ∙ fWd+1 fWf ∙ ∙ ∙ fWf (X) share the same output of activation functions from
d + 1-th layer to D-the layer with fWD fWD-1 (∙ ∙ ∙ fW1 (X)) , (ii) is from fixing the activation
13
Under review as a conference paper at ICLR 2019
function output, and (iii) is from the entry-wise I-LiPschitz continuity of σ(∙). On the other hand,
for any d ∈ [D], we further have
kfWd (…fwι (x))k2 = k JX：d ∙ xk2 ≤ BJad,x ∙kxk2.	(11)
where (i) is from the entry-wise I-Lipschitz continuity of σ(∙) and (ii) is from recursively applying
the same argument.
In addition, we denote Wd = UdVd> and Wfd = UedVed>, where Ud, Vd, Ued, Ved ∈ Rp×r and kUk2 =
kVk2 = Ue	= Ve	= kWdk21/2. Then we have
Wd - Wfd2 = UdVd>-UedVed>2
= UdVd> - UdVed> + UdVed> - UedVed>
≤kUk2V -Ve2+Ve2U-Ue2
≤kWdk2/2 (IlV - VIL+IIU -Ut).	(12)
Applying (10) recursively and combining (11) and (12), we obtain the desired result as
IlfWD (fWD-1 (∙ ∙ ∙ fW1 (X)))- ffD (ffD-i (…ffι (X))) I2
D
≤X
B(d++1):D ∙ B 1:(d-1)，kxk2.2 (Bv - <+BU-矶)
d=1
D
≤ BJdx ∙ r√d ∙ maχ Bd/22 X (IIv - VIIF+IIU - UIIF)
d=1
一 DII	ll2 H	ll2
≤ BJdx ∙ r√2d ∙ maχ Bd/22t XIIV - VIIF+IIU - UIIF.
d=1
□
Lemma 3. Suppose g(w, X) is Lw-Lipschitz over w ∈ Rh with kwk2 ≤ K and α =
supg∈G,x∈X |g(w, X)|. Then the ERC of G = {g(w, X)} satisfies
/α√h log KL√√m ∖
Rm (G)= O 」_.
m
Proof. For any w, we ∈ Rh and Xm = {Xi}im=1, we consider the matric ∆ (g1, g2) =
maχxi∈Xm |g1(Xi) - g2(Xi)|, which satisfies
∆ (g1,g2) = maχ |g1(X) - g2(X)| = |g (w, X) - g (we, X)| ≤ Lw kw - wek2 .	(13)
x∈Xm
Since g is a parametric function with h parameters, then we have the covering number of G under
the metric ∆ in (13) satisfies
N(G,∆,δ) ≤ (3KLw)h.
Then using the standard Dudley’s entropy integral bound on the ERC (Mohri et al., 2012), we have
the ERC satisfies
1	suPg∈G δS,O)
Rm(G). β>f0β + √m /	PogNE dδ∙
(14)
Since we have
α = sup	∆ (g, 0) = sup	|g(w, X)| .
g∈G,x∈Xm
g∈G,x∈Xm
14
Under review as a conference paper at ICLR 2019
Then we have
Rm (G). f+√1m Ze ho log KLwdδ
≤ inf β + αJ^0SE (.)MI
β>0	m	m
where (i) is obtained by taking β = α,0∕m.
□
By definition, We have ɑ = SuPf∈fd n ,χ∈χm gγ (f (WD,x)). From Lemma 1 and 1 -LiPschitz
continuity of g, we also have
α ≤ LR ≤ R ∙QD=1 Bd,2 .	(15)
γγ
From Lemma 2, we have
Lw ≤ max BJac,χ ∙ R√2D ∙ max B1,/?.
w	x∈Xm	\d,2	d d,2
Moreover, when pd = p for all d ∈ [D], we have
K = JX IlWdkF ≤ pppD ∙ maxBd,2∙
d=1	d
Combining the results above with Lemma 3 and o = 2Dpr, we have
αq∕θ log KL√√m
Rm (G) .7-r- aVh
m
D D	二^^B	B\d,2∙R√Dm∕r∙maXd 豆2
V ∙ ∏d=l d,2y Pr θg SUPf ∈Fd,k∙k2 ,x∈Xm gγ(f(WD,x))
.	γ√m
C Proof of Corollary 1
The analysis follows Theorem 1, excePt that the bound for α in (15) satisfies
α ≤ min
b,
R ∙ QD=I Bd,2 }
since g satisfies |g| ≤ b and 1 -Lipschitz continuous. Then we have the desired result.
D Proof of Corollary 2
We first show that using unit norm filters for all d ∈ [D] and nd ≤ kd, we have
(16)
First note that when n = kd, due to the orthogonality of {w(d,j) }；[ 1, for all i,q ∈ [kd], i = q, we
have
kd	2	kd
X (W户,j)) = 1 and X Wqdj) ∙ w(d,j) = 0.	(17)
j =1	j=1
15
Under review as a conference paper at ICLR 2019
When nd = kd, we have for all i ∈ [pd-1], the diagonal entries of Wd>Wd satisfy
kd
kd	kd Sd	O , , 7
”>Wd)ii=x Il(Wdj))*』2=xx (WM)+(h-i)sd) = kd
j=1	j=1h=1	d
(18)
where (i) is from (17). For the off-diagonal entries of Wd>Wd, i.e., for i 6= q, i, q ∈ [pd], we have
kd
kd Sd
w(i%sd)+(h-1)sd w(q%sd)+(h-1)sd
j=1 h=1
(=i) 0,
(19)
where (i) is from (17). Combining (18) and (19), we have that Wd> Wd is a diagonal matrix with
IIw>Wd∣∣2 = kd =⇒ kWdk2 = Jkd.
sd	sd
For nd < nk , we have that Wd is a row-wise submatrix of that when nd = kd, denoted as Wfd . Let
ndkd
S ∈ R Sd Pd be a row-wise submatrix of an identity matrix corresponding to sampling the row of
Wd to form Wd. Then we have that (16) holds, and since
∣∣fd∣∣2=q∣S ∙ WdW>∙ S >∣2=rkd.
Suppose kι =…=kD = k for ease of discussion. Then following the same argument as in the
proof of Theorem 1 and Lemma 3, we have
α = sup	gγ (f(WD,x)) ≤ R L Bd，2 = R I'=1 底
f ∈FD,k∙∣∣2 ,x∈Xm	Y	Y
Lw ≤ max BJac,X ∙ Rp2Dk∕s,
x∈Xm \d,2
K
D nd	IU^
xx ∣∣w(d,j)∣∣2 = utxnd, and
d=1 j=1	d=1
D
h = k nd.
d=1
Using the fact that the number of parameters in each layer is no more than knd rather than 2pr, we
have
ɑqh log KL√√m
Rm (G).八-r- α7h
m
R ∙ QD ɑ/ɪ ∙ Sk PD Ind log	Bad WK√m/	二
口d=1 V Sd V 乙 d=1 d S)SUPf ∈Fd,k∙k2 ,x∈Xm gγ (f(WD，X))
.	γ√m	.
E Proof of Corollary 3
The analysis is analogous to the proof for Theorem 1, but with different construction of the interme-
diate results. We first provide an upper bound for the Lipschitz constant of f (VD, UD, x) in terms
of x.
16
Under review as a conference paper at ICLR 2019
Lemma 4. Given VD and UD, for any f (Vd, Ud, ∙) ∈ Fdjni? and x1,x2 ∈ Rp0, We have
kf (Vd,Ud,xi)- f (Vd,Ud,x2)k2 ≤	- x2k2 ∙∏D=ι(Bu也,2 + 1).	(20)
Proof. Consider the ResNet layer, for any x1 , x2 ∈ Rk, We have
kf(VD,UD,x1)-f(VD,UD,x2)k2
=kfVD,UD (∙ ∙ ∙ fV1,U1 (XI)) 一 fVD,UD (∙ ∙ ∙ fV1,U1 (x2))k2
=∣∣σ	(VD	∙ σ (UD	∙	fVD-1,UD-1	(∙ ∙ ∙	fV1,U1	(XI)))	+	fVD-ι ,Ud-ifVι,U	(XI)))
一 σ (VD ∙ σ (UD ∙ fVD-1,UD-1(…fV1,U1 (X2))) + fVD-1,UD-1 (∙ ∙ ∙ fV1,U1 (X2))) ∣I2
(i)
≤ ∣∣VD ∙ σ (UD ∙ fVD-1,UD-1 (…fVι,Uι (XI))) - VD ∙ σ (UD ∙ fVD-1,UD-1 (…fVι,U (X2)))l∣2
+ llfVD-1,UD-1 (∙ ∙ ∙ fV1,U1 (XI)) - fVD-1,UD-1 (∙ ∙ ∙ fV1,U1 (X2))∣∣2
(ii)
≤ (HVD l∣2 kUD ∣∣2 + 1) ∙ ∣∣fVD-ι,UD-ι (…fV1,U1 (XI)) - fVD-1,UD-1 (∙ ∙ ∙ fV1,U1 (X2))∣∣2 ,
where (i) is the fact that σ is I-LiPschitz, and (ii) is from repeating the arguments of (i) and (ii).
By recursively applying the argument above, we have the desired result.	□
Next, we provide an upper bound for the Lipschitz constant of f (VD, UD, X) in terms of VD and
UD .
Lemma 5. Given any X ∈ Rp0 satisfying kXk2 ≤ R, for any f (VD, UD, X) , f
Fd,∣h∣2 with Vd = {Vd}D=ι, UD = {Ud}D=ι ,Vd = {Vd}；fandUD = {Ud}
B\adc,,2x = maxd∈[D] B1a:(c,dx-1),2B(adc+,x1):D,2, then we have
∣∣f(VD,UD,X) -f
, UD , X

2
VeD, UeD, X ∈
D
, and denote
d=1
≤ B∖d,,X max (B^,2 + BUdG R国∙
uD	2 D	2
utX∣∣∣VD-VeD∣∣∣2F+X∣∣∣UD-UeD∣∣∣2F.
d=1	d=1
17
Under review as a conference paper at ICLR 2019
Proof. Given x and two sets of weight matrices {Wd}dD=1,
Wfd
D
, we have
d=1
fVD ,Ud (∕Vd-1,Ud-1 (∙∙∙ fVl,Ul (X))) - fVD,UD fVd_1,Ud-1 (…fVι,Uι (X))) 112
D
≤ X fD ,Ud (…fVd+1,Ud+1 fVd,Ud(…D) - /Vd,Ud (…fVd+ι ,Ud+ι (fVd,Ud (…)))
d=1
+ fVD,UD (…fVd+1,Ud+1 IfVd,Ud (…)) - - fVD ,Ud (…fVd,Ud fVd_1,Ud-1(…)
2
D
≤ X fVD
,UD (…fVd + 1,Ud+1 (fVd,Ud (…)))-fVD,UD (… fVd+1 ,Ud+1 fVd,Ud (…)))∣∣2
d=1
D
+ X fVD,UD (…fVd+ι,Ud+ι fVd,Ud (…)))-fVD,UD (…fVd,Ud (fVd-ι,Ud-ι (…)))IL
d=1
D
X Ii jχd+1):D ∙ fVd,Ud (fVd-1,Ud-1 (…))-jχd+1):D ∙ fVd,Ud (fVd-1,Ud-1 (…))(
d=1
D
+ X II Jxd+1):D ∙ fd,Ud (fVd-ι,Ud-ι (…))-Jxd+1):D，fVd,Ud (fd-ι,Ud-ι (…))]
d=1
D
≤ X Bs1):D ∙ fVd,UdfVd-I,Ud-I (…)) - fed,Ud fVd-ι,Ud-ι (•∙• )):2
d=1
D
+ X BJd+1):D ∙ fVd,Ud fVd-1,Ud-1 (…))-fVd,Ud fVd-1,Ud-1 (…))ll2
d=1
≤) X BJd+xi):D ∙ M (Ud ∙ fVd-1,Ud-1(…))- Vdσ
d=1
fVd-1,Ud-1 (••• ))：2
D
+ X B(d+i):D ∙M (Ud ∙ fVd-1,Ud-1 (…))-Vdσ
d=1
fVd-1,Ud-1 (…))I2
≤ XBJd+xi):D ∙ (IIUd- UdlI2 Mk2 + IM - VdlI2 kUdk2) fd-1,Ud-1 (…)∣∣2,
d=1
(21)
where (i) and (ii) from the entry-wise I-LiPsChitz continuity of σ(∙). In addition, for any d ∈ [D],
we further have
l∣fVd-ι,Ud-ι (…fVι,Uι (X)) I2 = k Jx:d ∙ xk2 ≤ BIaCd-1) ∙ kxk2 .
(22)
Combining (21) and (22), we obtain
fVD,Ud(…fV1,U1 (X))- fVD,UD (…fV1,U1 (X)) ll2
D
≤ X BJd+x1):D ∙ BJaCd-1) #:2 ∙ (∣∣% -同 IF ∙kUdk2+IlUd-UdUF ∙ Il 同 I2)
d=1
D
\d,2	d d,	d, R X ∙(llVd - VdllF + llUd- UdIIF)
d=1
≤
BJd,,X maX(BVd,2 + BUd,2) R√2D ∙
uD	2 D	2
utXIIIVD-VeDIII2F+XIIIUD-UeDIII2F.
d=1	d=1
□
18
Under review as a conference paper at ICLR 2019
On the other hand, for any d ∈ [D], we have
kfVd,Ud (∙ ∙ ∙ fV1,U1 (x))k2
(i)
≤ kVdk2 kUdk2 ∙ ∣∣fVd-1,Ud-1 (∙ ∙ ∙ fV1,U1 (X))Il2 + ∣∣fVd-ι,Ud-ιfV1,U1 (X))Il2
(ii)	d
≤ ∏(kKk2 kUik2 + 1) ∙kxk2 .
i=1
(23)
where (i) is from the entry-wise I-LiPschitz continuity of σ(∙) and (ii) is from recursively applying
the same argument.
Let pi = •一=PD = P and qι = •一=q° = q. Then following the same argument as in the proof
of Theorem 1 and (23), we have
α = sup	gγ (f(VD,VD, x)) ≤ R ID=1 (B3" + 1)
f ∈FD, k∙k2 ,x∈Xm	Y
Lw ≤ max B∖ac,X max (BVd2 + BUd,2) R√D,
x∈Xm \ , d
K=t
D
X ∣∣VdkF + ∣∣UdkF ≤ pppD ∙ max(Bvd,2 + B∕,2), and h = 2Dpq,
d=1
Combining Lemma 3, and Lemma 4, Lemma 5, we have
α /h log KL√√m
Rm(G). V	√m α√h
≤
R ∙ QD 1 (BVa 2Bud 2 + 1) YLPq ∙ log (暨W(BVdeBU ：『三
d=i I Vd,2 Ud,2	suPf∈FD,k∙k2 ,x∈Xm gγ (f(VD ,VD ,x))
γ√m
F SPECTRAL BOUND FOR Wd IN CNNS WITH MATRIX FILTERS
We provide further discussion on the upper bound of the spectral norm for the weight matrix Wd in
CNNs with matrix filters. In particular, by denoting Wd using submatrices as in (7), i.e.,
Wd = [wd1)> ∙∙∙ Wdnd)>i> ∈Rpd×pd-1,
we have that each block matrix Wd(j) is of the form
Wd(j) (1, 1)	Wd(j) (1, 2)
Wd(j) (2, 1)	Wd(j) (2, 2)
Wdj) (1, √pd-τ)
Wdj) (2, √Pd-τ)
W(j) I ʌ/pd-1 kd 1 )	w(j) I ʌ/pd-1 kd 2
(24)
19
Under review as a conference paper at ICLR 2019
where Wdj) (i,l) ∈ R-S-	×√pd-1 for all i ∈ VZps-Ikd and l ∈ [√pd-ι]. Particularly, off-
diagonal blocks are zero matrices, i.e., Wd(j) (i, l) = 0 for i 6= l. For diagonal blocks, we have
z
w(j,1) 0 ∙ ∙
l-{z-}×—
∈R√kd fR
∈R
Zkd
w(j,√kd) 0
|---{----
∈R√kd
0 …0 w(j,1) 0 • •
^{^} ^{^}s
∈R√kd ∈R√kd ∈R.
wj,√kI)
^{z
p Pd-I
∈RV kd
) 0......
/ 、
Zkd
w(j,1)	0
(j, )	(j, )
WheIe wʃ Sd I = wι . Sd
{ 丁 }	√kd
∈R
Sd
∈ R √kd and w
(j,1)
J
and (25), we have that the stride for Wd(j ) is
kWdk2 = 1 if JPjWj叫2 = k
ZkdJ
、至
、kd.
∈R√kd	,
∈R
Zkd—Sd=
kd
w(j,√kd) 0
|---{----
∈R√kd
w(j,1)
∈R
Sd
(j,1)
9 w{ sd}
(25)
∈ R √kd. Combining (24)
:√kd
Using the same analysis for Corollary 2. We have
0
}
0
}


z

0
}
-1 -√kd
0
}
z

z

0
}
d
苞
For image inputs, we need an even smaller matrix Wd(j) (i, i) with fewer rows than (25), denoted as
w
(j,1)
0
0
,Cj,√kd)
0
W
|
}
z
z
/、
0
}
z
∈R
∈R

Zk
d
∈R
Pd-I
0 …0 w1
Cj,1)
0
|
0
S
Zk
z
}
w
(j,√kd)
∈R
0
|
k
d

z
Zk
d
0
}
∈R
d
∈R

∈R

0
|
z
∈R
0w
}
Cj,1)
∈R
Zk
0
|
0
z
}
d
∈R
Pd-I
d

Zk
d
∈R
Zk
S
7- d
d	√k7
d
cj,√kd)
z
}
∈R
Zk
d
(26)
Then IlWdIl2 ≤ 1 still holds if ʌ/PJwCji)Il
of Wd generated using (25).
kd since Wd generated using (26) is a submatrix
20