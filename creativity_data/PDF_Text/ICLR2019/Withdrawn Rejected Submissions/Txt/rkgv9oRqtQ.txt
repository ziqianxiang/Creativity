Under review as a conference paper at ICLR 2019
Compound Density Networks
Anonymous authors
Paper under double-blind review
Ab stract
Despite the huge success of deep neural networks (NNs), finding good mecha-
nisms for quantifying their prediction uncertainty is still an open problem. It was
recently shown, that using an ensemble of NNs trained with a proper scoring rule
leads to results competitive to those of Bayesian NNs. This ensemble method can
be understood as finite mixture model with uniform mixing weights. We build
on this mixture model approach and increase its flexibility by replacing the fixed
mixing weights by an adaptive, input-dependent distribution (specifying the prob-
ability of each component) represented by an NN, and by considering uncountably
many mixture components. The resulting model can be seen as the continuous
counterpart to mixture density networks and is therefore referred to as compound
density networks. We empirically show that the proposed model results in better
uncertainty estimates and is more robust to adversarial examples than previous
approaches.
1 Introduction
Deep neural networks (NNs) have achieved state-of-the-art performance in many application areas,
such as computer vision (Krizhevsky et al., 2012) and natural language processing (Collobert et al.,
2011). However, despite achieving impressive prediction accuracy on these supervised machine
learning tasks, NNs do not provide good ways of quantifying predictive uncertainty. This is unde-
sirable for many mission critical applications, where taking wrong predictions with high confidence
could have fatal consequences (e.g. in medical diagnostics or autonomous driving).
A principled and the most explored way to quantify the uncertainty in NNs is through Bayesian
inference. In the so-called Bayesian neural networks (BNNs) (Neal, 1995), the NN parameters are
treated as random variables and the goal of learning is to infer the posterior probability distribution
of the parameters given the training data. Since exact Bayesian inference in NNs is computation-
ally intractable, different approximation techniques have been proposed (Neal, 1995; Blundell et al.,
2015; Hemandez-Lobato & Adams, 2015; Ritter et al., 2018, etc.). Given the (approximate) Pos-
terior, the final predictive distribution is obtained as the expected distribution under the posterior,
where the expectation can be seen as an ensemble of an uncountably infinite number of predictors,
where the prediction of each model is weighted by the posterior probability of the corresponding
parameters.
Based on a Bayesian interpretation of dropout (Srivastava et al., 2014), Gal & Ghahramani (2016)
proposed to apply it not only during training but also when making predictions to estimate predictive
uncertainty. Interestingly, dropout has been also interpreted as ensemble model (Srivastava et al.,
2014) where the predictions are averaged over the different NNs resulting from different dropout-
masks. Inspired by this, Lakshminarayanan et al. (2017) proposed to use an simple NN ensemble
to quantify the prediction uncertainty, i.e. to train a set of independent NNs using a proper scoring
rule and defining the final prediction as the arithmetic mean of the outputs of the individual models,
which corresponds to defining a uniformly-weighted mixture model. It is argued, that the model
is able to encode two sources of uncertainty by calibrating the prediction uncertainty in each com-
ponent and capturing the “model uncertainty” by averaging over the components. Note, that this
should hold for any kind of predictive model that is defined in terms of a mixture distribution.
In this paper, we therefore aim at further investigating the potential that lies in using mixture distribu-
tions for uncertainty quantification. The flexibility of a mixture model can be increased by learning
input-conditioned mixture weights, like it is done by mixture density networks (MDN) (Bishop,
1
Under review as a conference paper at ICLR 2019
1994). Furthermore, one can consider uncountably many component distributions instead of a finite
set, which turns the mixture distribution into a compound distribution. We combine both by deriv-
ing the continuous counterpart of MDNs, which we call compound density networks (CDNs). This
model corresponds to a compound distribution in which both, component and mixing distribution,
are parametrized based on NNs. We experimentally show that CDNs allow for better uncertainty
quantification and are more robust to adversarial examples than previous approaches.
This paper is organized as follows. In Section 2 we give a brief introduction to MDNs. We then
formally define CDNs in Section 3. We review related work in Section 5 and present a detailed
experimental analysis in Section 6. Finally we conclude our paper in Section 7.
2	Mixture Density Networks
Let D = {xn , yn}nN=1 be a i.i.d dataset and let us define the following conditional mixture model
K
p(y∣χ) = XP(y∣φk(χ))p(0®(χ)∣∏(χ)),	⑴
and an NN that maps x onto both the parameters π(x) of the mixing distribution and the parameters
{φk(χ)}kK=1 of the K mixture components. The complete system is called mixture density network
(MDN) and was proposed by Bishop (1994). That is, an MDN is an NN parametrizing a conditional
mixture distribution, where both the mixture components and the mixture coefficients depend on
input χ.1 MDNs can be trained by maximizing the log-likelihood of the parameters of the NN given
the training set D using gradient-based optimizers such as stochastic gradient descent (SGD) and its
variants.
MDNs belong to a broader class of models called mixture of experts (MoE) (Jacobs et al., 1991)
which differ from standard mixture models by assuming that the mixture coefficients depend on the
input.2 Because of its formulation as a mixture distribution, the predictive distribution of an MDN
can handle multimodality better than a standard discriminative neural network. Furthermore, the
mixture coefficients allow us to encode uncertainty about the prediction, namely by modelling the
probability from which component distribution a data point was sampled.
3	Compound density networks
We aim at generalizing the MDN from a finite mixture distribution to a mixture of an uncountable
set of components. The continuous counterpart of a conditional mixture distribution in eq. (1) is
given by the conditional compound probability distribution
p(y∣χ) = ∕p(y∣Φ(χ))p(Φ(χ)l∏(χ)) dφ(χ) ,	(2)
where φ(χ) turns from a discrete into a continuous random variable.
We now want to follow the approach of MDNs to model the parameters of the components and
the mixing distribution by NNs. To handle the continuous state space of φ(χ) in the case of a
compound distributions, the key idea is now to let φ(χ) be given by a stochastic NN f(χ; θ) =
φ(χ) with stochastic parameters θ. Since given χ, f is a deterministic map from θ to φ(χ), it
is possible to replace the mixing distribution p(φ(χ)∣π(χ)) = p(f (χ; θ)∣π(χ)) by a distribution
p(θ∣π(χ)) over θ. We further assume, that the parameters π(χ) of the mixing distribution are given
by some parametrized function g(χ; ψ) = π(χ) which can also be modelled based on NNs. In
correspondence to MDNs, the complete system is called compound density network (CDN) and it is
summarized by the following equation
p(y∣χ; ψ) = /p(y∣f(χ; θ))p(θ∣g(χ; ψ)) dθ = Ep(θ∣gχψ))[p(y∣f(χ; θ))] .	(3)
1For instance, as in the original publication, the mixture components could be K Gaussians, with φk (x)
being input-specific means and variances, and the mixture probabilities could be given by (applying the softmax
function to the unnormalized) mixing weights π(x), both computed by one NN.
2See (Bishop, 2006, ch. 5.6 and ch. 14.5.3) and (Murphy, 2012, ch. 11.2.4) for a detailed discussion of
MDNs.
2
Under review as a conference paper at ICLR 2019
Figure 1: An example of a probabilistic hypernetwork applied to a two-layer MLP.
As for MDNs, CDNs training corresponds to maximizing its likelihood function, which is given by
N
log p(D∣ψ) = ɪ^log Ep(θ∣g(χnM)[ p(yn∣f(xn ； θ)] .	(4)
n=1
As we now have to deal with an integral instead of a finite sum, this log-likelihood function is
intractable. Fortunately, applying the reparametrization trick (Kingma & Welling, 2014) to θ 〜
p(θ∣g(x; ψ)) enables Monte Carlo integration, while still being able to optimize the log-likelihood
via backpropagation in conjunction with SGD.
To avoid overfitting, We can regularize the mixing distribution p(θ∣g(x; ψ)) by penalizing its KL-
divergence w.r.t. some simple distribution p(θ)3, leading to the regularized objective function
N
L(ψ)=logP(D∣Ψ) - λX Dkl[p(θ∣g(xn；ψ))kp(θ)] ,	(5)
n=1
where λ controls the regularization strength. We summarize the training procedure of CDNs in
the pseudo-code provided in Appendix A. Note, that CDNs correspond to an abstract framework
for modelling compound distributions with NNs. In the following section, we present a concrete
example on how CDNs can be implemented.4
3.1	Probabilistic hypernetworks
Ha et al. (2017) proposed to (deterministically) generate the parameters of an NN by another NN,
which they call the hypernetwork. 5 We would like to follow this approach for modelling a CDN,
that is, we aim at modelling the mixing distribution p(θ∣g(x; ψ)) over network parameters by NNs.
Since now the hypernetworks maps x to a distribution over parameters instead ofa specific value θ,
we refer to them as probabilistic hypernetworks. In the following we will describe this idea in more
detail.
Let f be a multi-layer perceptron6 (MLP) with L-layers, parametrized by a set of layers’ weight
matrices7 θ = {Wl}lL=1, that computes the parameters φ(x) = f(x; θ) of the CDNs component
distribution in eq. (3). Let h1, . . . , hl-1 denote the states of the hidden layers, and let us define
h0 = x, and hL = f(x; θ). We now assume the weight matrices {Wl}lL=1 to be random variables
and to be independent from each other given the state of the previous hidden layer. We define a
series of probabilistic hypernetworks g = {gl}lL=1 (parametrized by ψ = {ψl}lL=1), where gl maps
hl-1 to the parameters of the distribution of Wl, and let the joint distribution over θ be given by
L
p(θ∣g(x; ψ)) = Yp(Wι∣gι(hi-i; ψι)) .	(6)
l=1
An illustration of a stochastic two-layer network f(x; θ) computing φ(x), with parameters distri-
butions given by probabilistic hypernetworks as defined in eq. (6) is given in Figure 1. To make
3Note, that another option is Lp regularization on ψ .
4An alternative implementation corresponding to a form of adaptive dropout is described in Appendix B.
5Specifically, they propose to apply a hypernetwork to compute the weight matrix of a recurrent NN at each
time-step, given the current input and the previous hidden state.
6Note, that probabilistic hypernetworks can analogously be applied to model the distribution over parame-
ters of any other kind of network.
7We assume that the bias parameters are absorbed into the weight matrix.
3
Under review as a conference paper at ICLR 2019
the definition of our model complete, we need to define the concrete statistical model we pick for
p(Wl|gl(hl-1; ψ)). This is done in the following section.
3.2	Probabilistic hypernetworks with matrix variate normals
A statistical model that was recently applied as the posterior over weight matrices in BNNs (Louizos
& Welling, 2016; Sun et al., 2017; Zhang et al., 2018; Ritter et al., 2018) is the matrix variate normal
(MVN) distribution (Gupta & Nagar, 1999). An MVN is parametrized by three matrices: a mean
matrix M and two covariance factor matrices A and B. It is connected to the multivariate Gaussian
by the following equivalence
X 〜MN(X; M, A, B) o Vec(X)〜N(Vec(X); Vec(M), A ③ B) ,	(7)
where vec(X) denotes the vectorization of X. Due to the Kronecker factorization of the covariance,
an MVN requires less parameters compared to a multiVariate Gaussian, which motiVates us to use it
as the distribution oVer weight matrices in this work. Furthermore, we assume that the coVariance
factor matrices are diagonal matrices, following Louizos & Welling (2016). That is, we choose the
mixture distribution of the CDN to be
LL
p(θ∣g(x;ψ)) = YMN(Wι∣gι(hι-ι;ψι)) = YMN(WlMι,diag(aι),diag(bι)) ,	(8)
where gl maps the state hl-1 of the preVious hidden layer onto the l-th MVN’s parameters
{Ml, al, bl}, defining the distribution oVer Wl. Suppose Wl ∈ Rr×c, then the corresponding
MVN distribution has rc + r + c parameters, which is more efficient compared to rc + rc parameters
when modeling Wl as fully-factorized Gaussian random Variable.
In practice, the procedure of sampling θ and computing f(x; θ) = hL now can be described by
hl = σ(hl-ιWl) ,where Wl 〜MN(Wl∣gl(hl-l;ψl)) ,	(9)
for l = 1, . . . , L, where σ is an arbitrary point-wise nonlinear actiVation function, which may differ
for hidden and output layers.
Note that using the mixing distribution defined in eq. (8) allows us to apply the reparametrization
trick (Appendix C.1). For the KL-diVergence-based regularization, a straight forward choice for
the simple distribution is p(θ) = QlL=1 MN(Wl |0, I, I), which corresponds to assuming that each
layer’s weight matrix is standard matrix normal distributed. In this case, the KL-term can be com-
puted in closed form, as noted by Louizos & Welling (2016) and shown in Appendix C.2. Finally,
we use a Vector-scaling parametrization similar to the one used by Ha et al. (2017) and Krueger et al.
(2017) for the mean matrices {Ml}lL=1, which we explain in detail in Appendix D.
4	Connection to Bayesian neural networks and Variational
Information B ottleneck
Interestingly, eq. (2) could also be interpreted as integrating oVer the parameters of a Bayesian
neural network, wherep(φ(x)∣π(x)) is an amortized approximate posterior. Performing variational
inference in such a setting would lead to the following objectiVe 8
N
ELBO(ψ) = XEp(θ∣g(χ3ψ))[logp(yn∣f(xn;θ)] — Dkl[p(θ∣g(xn;ψ))kp(θ)] .	(10)
n=1
Note, that this is a lower bound of eq. (5) (by Jensen’s inequality) and that stochastic approximations
of both objectiVes become equiValent when they are based on a single sample of θ and λ = 1. For
arbitrary Value of λ, the one-sample-approximation of eq. (5) is also equiValent to the approximated
Variational information bottleneck (VIB) objectiVe (Alemi et al., 2017). Following this objectiVe,
the proposed approach corresponds to performing VIB where the network parameters, instead of
the hidden units, are considered as latent Variables. We will analyze the effects of following these
different objectiVes experimentally in Section 6.4.
8Usually the approximate posterior does not depend on the input and therefore the KL term is independent
from x as well. When performing mini-batch optimization in this case, it is common practice to re-scale the
KL-term with 1/B where B is the number of mini-batches as described by GraVes (2011).
4
Under review as a conference paper at ICLR 2019
5	Related work
Various approaches for quantifying predictive uncertainty in NNs have been proposed. Applying
Bayesian inference to NNs, i.e. treating the network parameters as random variables and estimating
the posterior distribution given the training data based on Bayes’ theorem, results in Bayesian neu-
ral networks (BNNs) (MacKay, 1992; Neal, 1995; Graves, 2011; Blundell et al., 2015; Louizos &
Welling, 2016; Sun et al., 2017; Louizos & Welling, 2017; Ritter et al., 2018; Zhang et al., 2018,
etc). Since the true posterior distribution is intractable, BNNs are trained based on approximate
inference methods such as variational inference (VI) (Peterson, 1987; Hinton & Van Camp, 1993;
Graves, 2011; Blundell et al., 2015), Markov Chain Monte Carlo (MCMC) (Neal, 1995), or Laplace
approximation (MacKay, 1992; Ritter et al., 2018). The final prediction is then given by the expec-
tation of the network prediction (given the parameters) w.r.t. the approximate posterior distribution.
Louizos & Welling (2016) proposed to train BNNs based on VI with an MVN as the approximate
posterior of each weight matrix (leading to a model they refer to as variational matrix Gaussian
(VMG)). Multiplicative normalizing flow (MNF) (Louizos & Welling, 2017) models the approxi-
mate posterior as a compound distribution, where the mixing density is given by a normalizing flow.
Zhang et al. (2018) also use an MVN approximate posterior and apply approximate natural gradient
(Amari, 1998) based maximization on the VI objective, which results in an algorithm called noisy
K-FAC. Meanwhile, the Kronecker-factored Laplace approximation (KFLA) (Ritter et al., 2018)
extends the classical Laplace approximation by using an MVN approximate posterior with tractable
and efficiently computed covariance factors, based on the Fisher information matrix.
There have been several concurrent works (Krueger et al., 2017; Louizos & Welling, 2017;
Pawlowski et al., 2017; Sheikh et al., 2017) applying hypernetworks (Ha et al., 2017) to model
the posterior distribution over network parameters in BNNs. In contrast to CDNs, the hypernet-
works in these approaches are used to transform random noise drawn from a simple distribution into
a random variable with a complicated distribution. That is, they use hypernetworks to sample from
an implicit distribution of the parameters, without explicitly specifying the statistical model (like
the MVN in our case). Krueger et al. (2017) and Louizos & Welling (2017) use normalizing flows,
while Pawlowski et al. (2017) and Sheikh et al. (2017) use arbitrary NNs as their hypernetworks.
The approach by Sheikh et al. (2017) is the one most closely related to ours, since they use an objec-
tive analogous to eq. (4). Note, that the main difference between CDNs and these hypernet-BNNs
is that the approximate posterior in a Bayesian setting does not depend on the current input point,
while the mixing distribution of the CDN does.
Gal & Ghahramani (2016) developed a theoretical framework that relates dropout training in NNs to
approximate Bayesian inference and, as a result, proposed to approximate the predictive distribution
by an average over the different networks resulting from independently sampled dropout-masks, a
technique which they referred to as MC-dropout and which they applied to estimate the prediction
uncertainty in NNs. Recently, Lakshminarayanan et al. (2017), proposed to use an ensemble of
NNs in conjunction with a proper scoring rule and adversarial training to quantify the prediction
uncertainty of deep NNs, leading to a model referred to as Deep Ensemble. The Deep Ensemble
provides a non-Bayesian way to quantify prediction uncertainty, and is in this sense related to the
approaches of Guo et al. (2017) and Hendrycks & Gimpel (2017).
6	Experiments
We consider several standard tasks in our experimental analysis: 1D toy regression problems in-
spired by Hemandez-Lobato & Adams (2015)and DePeWeg et al. (2018) (Section 6.1), classifica-
tion under out-of-distribution data (Section 6.2), and detection of and defense against adversarial
examples (Szegedy et al., 2014) (Section 6.3). We consider the folloWing recent models (described
in Section 5) as the baselines for our CDN model: MNF, KFLA, noisy K-FAC, MC-dropout, and
Deep Ensemble.9
In all of the experiments, We consider λ ∈ {10-n}5n=1 When training CDNs based on eq. (5), and
pick the highest value of λ that still alloWs for high predictive poWer (e.g. an accuracy of > 0.97
on the validation set). We use this selection heuristic, based on our observation that in general, as λ
9We also investigated the VMG, an MoE, and an MDN. Due to space restrictions, results for these methods
are reported in Appendix E.
5
Under review as a conference paper at ICLR 2019
increases, the accuracy is decreasing while the uncertainty estimate is increasing (see Appendix F.1
for an investigation of this behavior). The hyperparameters for the baselines are set to the values
suggested by the respective original publications as outlined in Appendix F.
For BNNs and the CDN we estimate the predictive distribution p(y|x), based on 100 samples of
network parameters from the posterior distribution or the mixing distribution, respectively. Unless
stated otherwise, we use a single sample of θ to do Monte Carlo integration during training. For
MNIST and CIFAR-10 experiments, we use mini-batches of size 200. We use Adam (Kingma &
Ba, 2015) with default hyperparameters for optimization in all experiments and the implementations
provided by Louizos & Welling (2017)10 and Zhang et al. (2018)11 for MNF and noisy K-FAC,
respectively. The implementation of all models and experiments will be made available to the public
once the review process is concluded.
6.1	Toy regression
Following Hemandez-Lobato & Adams (2015) and DePeWeg et al. (2018), We use two toy regression
problems to demonstrate the capability of CDNs of quantifying predictive uncertainty. The datasets
of those Probems are generated as follows.
•	Cubic (Hernandez-Lobato & Adams, 2015): We sample 20 input points X 〜U[-4,4] and
their target values y = x3 + e, where e 〜N(0, 32).
•	Mixture (Depeweg et al., 2018): We sample 750 input points X 〜p(χ), where p(χ)=
1N(-4, 5) + 1N(0,0.9) + 1N(4, 2) and generate their target values by y = SinX +
3∣cos X2 |e, where e 〜N(0,1).
We use a single layer MLP with 100 hidden units as the predictive network, while the hypernetworks
(g1 and g2) are modeled with single layer MLPs with 10 hidden units each. Three samples from the
mixing distribution are used to do Monte Carlo integration to approximate eq. (4).
(a) Cubic
(b) Mixture
Figure 2: Predictive distributions of CDN. Black lines corresponds to the true noiseless function,
red dots correspond to samples, orange lines and shaded regions correspond to the empirical mean
and the ±3 standard deviation of the predictive distribution, respectively.
The results are presented in Figure 2. We observe that our model is able to accurately quantify
the heteroscedastic noise in the Mixture dataset. Comparing the results for the CDN with those of
Bayesian models on the Cubic dataset (Figure 8 in the appendix) it becomes clear that the CDN
is quantifying the noise applied in the data-generating process (aleatoric uncertainty) instead of
epistemic uncertainty. A visualization of the learned mixing distributions for different input points
is provided in Appendix G.
6.2	Out-of-distribution classification
Following Lakshminarayanan et al. (2017), we train all models on the MNIST training set and
investigate their performance on the MNIST test set and the notMNIST dataset12, which contains
10https://github.com/AMLab-Amsterdam/MNF_VBNN
11 https://github.com/gd- zhang/noisy- K- FAC
12Available at http://yaroslavvb.blogspot.com/2011/09/notmnist- dataset.html.
6
Under review as a conference paper at ICLR 2019
——CDN (0.971)
——MNF (0.98)
KFLA (0.978)
noisy K-FAC (0.974)
MC-DropOUt (0.972)
DeepEnsembIe (0.982)
(a) MNIST
Figure 3: CDFs of the empirical entropy of the predictive distribution of the models, where the
y-axis denotes the fraction of predictions having entropy less than the corresponding value on the x-
axis. Confident models should have its CDF close to the top-left corner of the figure, while uncertain
models to the bottom-right. The number next to each model name indicates its test accuracy.
——CDN (0.094)	——noisy K-FAC (0.125)
——MNF (0.146)	Deep Ensemble (0.155)
——KFLA (0.116)	——MC-Dropout (0.117)
(b) notMNIST
images (of the same size and format as MNIST) of letters from the alphabet instead of handwritten
digits. On such an out-of-distribution test set, the predictive distribution of an ideal model should
have maximum entropy, i.e. it should have a value of ln 10 ≈ 2.303 which would be achieved if all
ten classes are equally probable. The NN used for this experiment is an MLP with a 784-100-10
architecture.
We present the results in Figure 3, where we plotted the cumulative distribution function (CDF)
of the empirical entropy of the predictive distribution, following Louizos & Welling (2017). A
CDF curve close to the top-left corner of the figure implies that the model yields mostly low entropy
predictions, indicating that the model is very confident. While one wishes to observe high confidence
on data points similar to those seen during training, the model should express uncertainty when
exposed to out-of-distribution data. That is, we prefer a model to have a CDF curve closer to
the bottom-right corner on notMNIST, as this implies it makes mostly uncertain (high entropy)
predictions, and a curve closer to the upper-left corner for MNIST. As the results show, our model
has very high confidence on the test set of MNIST while having the lowest confidence on notMNIST
compared to all baseline models. It is surprising that even though CDNs are designed to capture
aleatoric uncertainty they outperforms other models in this experiment, which is rather designed for
quantifying model uncertainty (Kendall & Gal, 2017).
6.3	Adversarial examples
To investigate the robustness and detection performance of the CDN w.r.t. adversarial exam-
ples (Szegedy et al., 2014), we apply the Fast Gradient Sign Method (FGSM) (Goodfellow et al.,
2015) to a 10% fraction (i.e. 1000 samples) of the MNIST and CIFAR-10 test set. We do so, by
making use of the implementation provided by Cleverhans (Papernot et al., 2018). We use the same
MLP (with 784-100-10 architecture) as before for experiments on MNIST, and the LeNet5 convolu-
tional network (LeCun et al., 1998) for experiments on CIFAR-10. Out of implementation reasons,
we only model the parameters of the fully-connected layers of LeNet5 as random variables for CDN
and KFLA. The probabilistic hypernetworks are two-layer MLPs with 100 hidden units. Note, that
we do not use adversarial training when training the Deep Ensemble in this experiment to allow for
a fair comparison.
MNIST Figure 4 presents the accuracy and the average empirical entropy of the predictive distri-
bution w.r.t. adversarial examples for MNIST with varying levels of perturbation strength (between
0 and 1). We observe that the CDN is significantly more robust to adversarial examples than all
baseline models. The prediction entropy for adversarial examples is also higher for the CDN than
for most other models (only MC-dropout shows higher uncertainty in the beginning). Furthermore,
the prediction uncertainty of the CDN is steadily increases with increasing perturbation strength.
7
Under review as a conference paper at ICLR 2019
(a) Accuracy	(b) Entropy
(a) 1 sample
Figure 5: Accuracy and average entropy of our proposed model under FGSM attack. We use a single
sample of adversarial example in the left figure, and use the average of 10 adversarial examples to
take into account the stochasticity of our model in the right figure. Circle indicates accuracy, while
cross indicates entropy. The y-axis represents both the accuracy and the relative entropy to the
maximum entropy (i.e. ln 10).
Figure 4: Prediction accuracy and average entropy of models trained on MNIST when attacked by
FGSM-based adversarial examples (Goodfellow et al., 2015) with varying perturbation strength.
(b) 10 samples
We demonstrate how the performance of our model depends on the choice of regularization strength
λ in Figure 5. It is clearly visible that λ acts as a hyperparameter that balances the trade-off between
detection (uncertainty) and robustness (accuracy). When allowing for lower uncertainty, the CDN
gets surprisingly robust, showing a prediction accuracy of around 0.4 even for a perturbation strength
of 1. Even when we increase the strength of the adversarial examples by computing them based on
10 different samples from our model, the CDN is still significantly more robust than all baseline
models w.r.t. the weaker adversarial examples relying on a single sample.
CIFAR-10 Figure 6 shows that, leading to second best results w.r.t. accuracy as well as uncer-
tainty, our model is competitive to other state of the art models on CIFAR-10. Note, that for the
CDN we do not use probabilistic hypernetworks for the convolution layers of LeNet5, i.e. we only
treat the parameters of the fully-connected layers as random variables. Treating all parameters as
random variables, as it is done by the BNNs in this experiment (except from KFLA) could poten-
tially improve the performance of the CDN.
6.4	Comparison to training based on ELBO and VIB objective
In this section, we experimentally investigate the effects of (a) optimizing our proposed objective
(eq. (5)), (b) treating our model as a BNN and train it with the ELBO objective (eq. (10)), and
(c) following the VIB approach (which corresponds to optimizing the objective which results from
introducing a hyperparameter λ in front of the KL-term in eq. (10)). As stated before, (a) and (c)
8
Under review as a conference paper at ICLR 2019
0.0	0.2	0.4	0.6	0.8	1.0
Perturbation strength
(a) Accuracy
0.0	0.2	0.4	0.6	0.8	1.0
Perturbation strength
(b) Entropy
Figure 6: Prediction accuracy and average entropy of models trained on CIFAR-10 when attacked
by FGSM-based adversarial examples with varying perturbation strength.
——CDN-IO (0.972)	——VIB-1/CDN-1 (0.971)
ELBO-IO (0.192)	——VMG-I (0.985)
——VlB-IO (0.978)
(a) MNIST uncertainty
——CDN-IO (0.133)	——VIB-1/CDN-1 (0.117)
ELBO-IO (0.081)	——VMG-I (0.131)
——VIB-IO (0.116)
(b) notMNIST uncertainty
(c) MNIST adversarial accuracy
(d) MNIST adversarial entropy
Figure 7: Comparison between the ELBO, VIB and the CDN objective. “Objective-S” denotes that
the objective was approximated based on S samples of θ during training.
become equivalent when approximating the objective by a single sample of θ, and both become
equivalent to (b) when setting λ = 1 in addition. To better analyze the effects of following the
different objectives we therefore approximate them based on 10 samples.
Results for the OOD classification and the robustness to adversarial examples are shown in Figure 7.
Training based on the ELBO results in unsatisfactory performance as can be seen from the orange
graphs and the indicated low validation accuracy. The CDN and the VIB objective both work well
on the OOD classification task, where no performance is gained by using 10 instead of one sample
(which makes both equivalent). This suggests that the input dependency of the distribution over θ
(in addition to the weighting of the KL-term in the objective) plays a crucial role for the increased
9
Under review as a conference paper at ICLR 2019
performance observed compared to baseline models for OOD classification. This hypothesis is also
supported by the fact that the performance is increase compared to the VMG (Louizos & Welling,
2016), which is a closely related BNN using a MVN distributions as approximate (input indepen-
dent!) posterior.
While for the robustness against adversarial attacks the increased sample size improved the perfor-
mance of models trained with the VIB as well as with the CDN objective, the model trained with
the CDN objective clearly outperforms the others, reaching an surprisingly high accuracy about 0.9
even under huge perturbations.
7	Conclusion
We introduce compound density networks (CDNs), a new framework that allows for better uncer-
tainty quantification in neural network (NN), and corresponds to a compound distribution (i.e. mix-
ture with uncountable components) in which both the component distribution and the mixing distri-
bution are parametrized by NNs. CDNs are inspired by the success of recently proposed ensemble
methods and represent a continuous generalization of mixture density networks (MDNs) (Bishop,
1994). They can be implemented by using a hypernetwork to map the input to a distribution over
the parameters of the target NN, that models a predictive distribution. An extensive experimental
analysis showed that CDNs are able to produce promising results in terms of uncertainty quantifi-
cation. Especially, when facing FGSM-based adversarial attacks, the predictions of our model are
significantly more robust than those of previous models, while simultaneously providing a better
chance of detecting the attack by showing increased uncertainty.
References
Alex Alemi, Ian Fischer, Josh Dillon, and Kevin Murphy. Deep variational information bottleneck.
In ICLR, 2017. URL https://arxiv.org/abs/1612.00410.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-
276, 1998.
Jimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Advances in
Neural Information Processing Systems, pp. 3084-3092, 2013.
Christopher M Bishop. Mixture density networks. 1994.
Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. ISBN 978-
0387-31073-2.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. pp. 1613-1622, 2015.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray KavUkcUoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Re-
search, 12(Aug):2493-2537, 2011.
Stefan Depeweg, Jose-Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty in bayesian deep learning for efficient and risk-sensitive learning. In
International Conference on Machine Learning, pp. 1192-1201, 2018.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059,
2016.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Alex Graves. Practical Variational Inference for Neural Networks. In Advances in Neural Informa-
tion Processing Systems 24, pp. 2348-2356. 2011.
10
Under review as a conference paper at ICLR 2019
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70,
pp.1321-1330, 06-11 Aug 2017.
Arjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 1999.
David Ha, Andrew Dai, and Quoc V. Le. HyperNetworks. In Proceedings of the Second Interna-
tional Conference on Learning Representations (ICLR 2017), 2017.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In Proceedings of International Conference on Learning Represen-
tations, 2017.
Jose MigUel Hemandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learn-
ing of bayesian neural networks. In International Conference on Machine Learning, pp. 1861-
1869, 2015.
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Proceedings of the sixth annual conference on Computational
learning theory, pp. 5-13. ACM, 1993.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of
local experts. Neural computation, 3(1):79-87, 1991.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? In Advances in neural information processing systems, pp. 5574-5584, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the 3rd International Conference for Learning Representations, 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the
Second International Conference on Learning Representations (ICLR 2014), April 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron
Courville. Bayesian Hypernetworks. arXiv:1710.04759 [cs, stat], October 2017. arXiv:
1710.04759.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, pp. 6402-6413, 2017.
Yann LeCun, Leon Bottou, YoShua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix
gaussian posteriors. In International Conference on Machine Learning, pp. 1708-1716, 2016.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational Bayesian neural
networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 2218-
2227, 2017.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural compu-
tation, 4(3):448-472, 1992.
Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012. ISBN
0262018020, 9780262018029.
Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD thesis, University of
Toronto, 1995.
11
Under review as a conference paper at ICLR 2019
Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Ku-
rakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan,
Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg,
Jonathan Uesato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber,
and Rujun Long. Technical report on the cleverhans v2.1.0 adversarial examples library. arXiv
preprint arXiv:1610.00768, 2018.
Nick Pawlowski, Andrew Brock, Matthew C. H. Lee, Martin Rajchl, and Ben Glocker. Implicit
Weight Uncertainty in Neural Networks. arXiv:1711.01297 [cs, stat], November 2017. arXiv:
1711.01297.
Carsten Peterson. A mean field theory learning algorithm for neural networks. Complex systems, 1:
995-1019,1987.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural
networks. In International Conference on Learning Representations, 2018.
Abdul-Saboor Sheikh, Kashif Rasul, Andreas Merentitis, and Urs Bergmann. Stochastic maximum
likelihood optimization via hypernetworks. In Advances in Neural Information Processing Sys-
tems, 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning structured weight uncertainty in
bayesian neural networks. In Artificial Intelligence and Statistics, pp. 1283-1292, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. 2014.
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as
variational inference. In Proceedings of the 35th International Conference on Machine Learning,
pp. 5852-5861, 2018.
12
Under review as a conference paper at ICLR 2019
Appendix A	Pseudocode for training CDNs
Algorithm 1 The training procedure of CDN.
Require:
Mini-batch size M , number of samples S used for Monte Carlo integration (eq. (4)), regulariza-
tion strength λ, and learning rate α.
1
2
3
4
5
6
7
8
9
10
11
while the stopping criterion is not satisfied do
{Xm, ym}M=1 〜D
for m = 1, . . . , M ; s = 1, . . . , S do
θms 〜p(θ∣g(Xm; ψ))
φs (Xm)
f (Xm; θms)
end for
log P(DM = PM=IlOg S Ps = IPRm lφs(Xm))
DKL = PM=IDKL[p(Mg(χm; ψ))kp(θ))]
L(ψ) = logp(D∣ψ) - λDκL
ψ — ψ + αVL(ψ)
. Sample mini-batch from dataset
. Use reparametrization trick
. Eq. 4 with Monte Carlo integration
. Update parameters ψ
end while
Appendix B	CDNs with latent variables
In Section 3, we introduce CDNs as a compound distribution where the stochasticity of f(X; θ)
comes from treating θ as random variables. Another approach to achieve stochasticity on f is by
introducing latent variable z and letting the parameter θ to be fixed (learned with MLE). The CDN
would then be descibed by
p(y∣χ) = / p(y∣χ,z; θ)p(z∣χ)dz
：=/ p(y∣f (χ; z, θ))p(z∣g(χ; ψ))dz
=Ep(z∣ggψ))[p(y∣f(χ;z,θ))] .	(11)
The log-likelihood function is therefore
N
logp(D∣θ,ψ) = XlogEp(z∣g(xnM)[p(yn∣f(χn;z,θ))] ,	(12)
n=1
and the objective function is
L(θ,ψ)=logp(D∣θ,ψ) - λDκL[p(z∣g(χn;ψ))kp(z)] .	(13)
The training algorithm in Algorithm 1 only needs to be trivially modified to accommodate θ, i.e. in
the gradient steps. In the following section, we present an instance of this framework.
B.1 A variant of adaptive dropout
This model relies on injecting noise into the hidden activations of the neural network f, similar to
dropout (Srivastava et al., 2014). However, in this model, we condition the dropout’s Gaussian on
the activations of the previous hidden layer, similar to adaptive Bernoulli dropout proposed by Ba &
Frey (2013).
Let Zl 〜p(zι∣gι(hι-ι; ψι)) be the noise vector of layer l. We then multiplicatively apply this noise
to the activations hl, i.e. hl zl, where denotes the Hadamard product. Let z := {zl}lL=-11 and let
g(χ; Ψ) := {gl(hl-ι; Ψl)}L=ι. Letp(z∣g(χ; ψ)) =： QL=IIP(zl∣g(hl; ψl)) be the miXingdistribU-
tion. Each of the P(zl |g(hl; ψl)) can be any distribution as long as we can apply reparametrization
trick. Then, we can immediately apply latent CDNs training procedure on this model. One eXample
ofP(zl|gl(hl-1; ψl)) is N(zl |1, gl(hl-1; ψl)), which then the model resembles Gaussian dropout
(Srivastava et al., 2014) with adaptive scaling.
13
Under review as a conference paper at ICLR 2019
Appendix C	Details about the matrix variate normal (MVN)
Let X ∈ Rr×c and p(X) := MN (X|M, diag(a), diag(b)). Taken from Louizos & Welling (2016),
the procedure to sample from p(X) using reparametrization trick (Kingma & Welling, 2014) and
the closed-form expression of the KL-divergence to MN (0, I, I) are presented in the following
sections.
C.1 Reparametrization trick
Let E ∈ Rr×c. Sampling p(X) can be done by
E 〜MN (0, I, I) ^⇒ 6ij 〜N (0,1) ∀ i = 1,...,r ∀ j = 1,...,c	(14)
X = M + diag(a)1E diag(b)2	(15)
C.2 KL-divergence to standard MVN
Let Ir ∈ Rr×r, Ic ∈ Rc×c be identity matrices, the KL-divergence between p(X) and
MN (X|0, I, I) is given by
1c	r	c
DKL [p(X) kMN(0, I, I)] = 2 I ∑>i E bj + kMkF - rc - c £ log ai - <£ log bj I .
=r j=	=	j=	(16)
Appendix D	Vector scaling parametrization
The naive formulation of gl can be very expensive in term of number of parameters. Suppose
Wl ∈ Rr×c and gl is atwo layer MLP with k hidden units. Then gl would have rk + krc+ kr + kc
many parameters, which quickly becomes very large for a moderately sized NNs. The majority of
the parameters are needed to define the mean matrix Ml. Following the approach ofHaet al. (2017)
and Krueger et al. (2017), we make a trade-off between expressiveness of gl on the mean matrix with
the number of parameter by instead replacing Ml with a matrix Vl of the same size and a vector
dl ∈ Rr, which is the output of gl. Thus, now gl maps hl-1 7-→ {dl, al, bl} and we can get Ml by
Mf = dfvl2	.	(17)
...
dlfrvlr
That is, each element of dl is being used to scale the corresponding row of Vl . Note that although
Vl is a parameter matrix with the same size of Ml, it crucially is not an output of gl as in the naive
parametrization. Thus the number of parameter of gl is now rk + rc + 2kr + kc, which is more
manageable and implementable for larger weight matrices.
Appendix E	Experimental results for additional models
In this section we compare the CDN descibed in the main text with the CDN proposed in Ap-
pendix B.1 (CDN-dropout), the BNN proposed by Louizos & Welling (2016) (VMG), a mixture of
experts (MoE) and an MDN on the MNIST dataset. For both MDN and MoE, we use two-layer MLP
with 100 hidden units, with 5 mixture components. Specifically for MoE, the mixing distribution is
also given by another NN of the same architecture. We use the code provided by Louizos & Welling
(2016) to get VMG’s results. 13 The results for the out-of-distribution prediction are presented in
Figure 9. Note that the analysis presented in the main text applies here as well.
13https://github.com/AMLab-Amsterdam/SEVDL_MGP.
14
Under review as a conference paper at ICLR 2019
(b) MNF
(a) CDN
(c) KFLA
(d) Noisy K-FAC
(e) Deep Ensemble
(f) MC-dropout
Figure 8: Comparison of the predictive distributions given by the CDN and the baselines on toy
dataset introduced by Hemandez-Lobato & Adams (2015). Black lines corresponds to the true
noiseless function, red dots correspond to samples, orange lines and shaded regions correspond to
the empirical mean and the ±3 standard deviation of the predictive distribution, respectively.
Prob, hypernets	--- VMG	MoE
CDN-dropout	—— MDN
(a) MNIST
(b) notMNIST
Figure 9: CDF of the prediction entropy on MNIST and notMNIST test set of additional models.
Appendix F	Hyperparameters
For the baseline models in our experiments, we use the hyperparameter values that are suggested in
the respective publications ans summarized in the following:
•	MNF: The KL-term is weighted by 1/B, where B is the number of mini-batches used
during optimization (see (Graves, 2011) for a justification of this). Moreover it is annealed
with a hyperparameter initialized to 0 and increasing to 1 during training. We found that
this yields better results than using no re-weighting of the KL term and is necessary to
achieve the results reported by Louizos & Welling (2017).
•	noisy K-FAC: KL-term weight is set to λ = 0.5 and the prior variance to η = 0.01.
•	KFLA: we use the best prior precision τ ∈ {1, 10, 20, 30, 40, 50} we found in an hyperpa-
rameter search, which we detail in the next section.
•	Deep Ensemble: The number of mixture components is 5, the adversarial perturbation
strength is set to 1% of the input range, and the weight decay is 0.001.
•	MC-dropout: The dropout probability was set to 0.5 and the weight decay parameter to
0.001.
15
Under review as a conference paper at ICLR 2019
F.1 Hyperparameter search
We perform a hyperparameter search on λ for the training of CDNs and for τ for KFLA based on
the MNIST validation accuracy, and pick the highest λ (the lowest τ) that achieve > 0.97 validation
accuracy.
	λ	CDN			
	1	0.107	τ	KFLA
	0.1	0.189	1	0.806
	0.01	0.111	10	0.977
	0.001	0.948	20	0.978
	0.0001	0.966	30	0.978
	0.00001	0.972	40	0.978
	0.000001	0.972		
Table 1: Validation accuracy on MNIST vs λ for CDN and τ for KFLA.				
λ	MNF	n. K-FAC	Deep Ens. MC-dropout	
1	0.981	0.974	0.207	0.162
0.1	0.981	0.976	0.885	0.834
0.01	0.978	0.981	0.965	0.932
0.001	0.98	0.981	0.983	0.971
0.0001	0.98	0.979	0.986	0.976
0.00001	0.98	0.979	0.986	0.977
Table 2: Validation accuracy on MNIST vs λ.
τ=20
τ = 4O
---τ=20
τ = 40
Figure 10: Empirical CDF curve of uncertainty on MNIST (left) and notMNIST (right)


16
Under review as a conference paper at ICLR 2019
0.5
0 2	⅛i
0.0^~.-------.--------.--------.--------.-------
0.0	0.2	0.4	0.6	0.8	1.0
Perturbation strength
0.0
0.0	0.2	0.4	0.6	0.8	1.0
Perturbation strength
(a) CDN
2.0
>
Q.
i ι.5
<υ
S-
S
0.5
0.8
>0.6
I 0.4
0.2
0.0 j~I------------1 r^8 ^~早 一金 阜 金 华 •卓
0.0	0.2	0.4	0.6	0.8	1.0
Perturbation strength
τ=40
Figure 11: Accuracy (left) and uncertainty (right) w.r.t. adversarial examples.
0.0
0.0	0.2	0.4	0.6	0.8	1.0
Perturbation strength
(b) KFLA
Appendix G	Visualization of the learned mixing distribution
To further understand the effect of conditioning the distribution over θ (i.e. the mixing distribution)
We compute the mixing distribution p(θ∣g(xκ ψ)) for a set of samples xι,... Xn and a CDN trained
on a variant of the Cubic dataset, where we add random noise of different variance depending on
x (that is, e 〜N(0,152) if X < 0 and e 〜N(0,32), otherwise). The results for two randomly
selected Weights wi(l) ∈ θ are shoWn in Figure 12. We found that the mean and the variance of
the distribution (which is Gaussian due to our model) varies depending on the value of the input x
indicating that different mixture components get high probability for different inputs.
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
P(W招2∣X)
-3	-2	-1	0	1	2	3
(a)
(b)
Figure 12: Visualization of the distribution p(wi(l))|g(x; ψ))ofa randomly selected weight wi(l) ∈ θ
for different samples of input x from the modified Cubic toy dataset. wi(l) denotes the i-th weight of
the l-th layer of f(x; θ).
17
Under review as a conference paper at ICLR 2019
Furthermore, to show that CDNs are able to capture multimodality in weight space, we train a
CDN with a 5 hidden units mixture component on a toy classification dataset that is constructed as
follows: We sample an input X from a mixture of Gaussian P(X) = 2N(-3,1) + 2N(3,1), and
assign a label depending whether it comes from the first (y = 0) or the second Gaussian (y = 1).
To evaluate the resulting distribution, We marginalize the mixing distribution p(θ∣g(x; ψ)) w.r.t.
x, i.e. we evaluate p(θ) = Jp(θ∣g(x; ψ))p(x) dx. The resulting distribution for two randomly
selected weights wi(l) ∈ θ are shown in the figure below. We observe that indeed our model can
learn a multimodal weight distribution.
P(Wa = JP(W 利 X)P(X)dx
(a)
(b)
Figure 13: Visualization of the marginal distribution p(wi(l)) = R p(wi(l) |g(X; ψ))p(X) dX for two
randomly selected weights wi(l) ofa CDN trained on a toy classification dataset.
18