Under review as a conference paper at ICLR 2019
Estimating Information Flow in DNNs
Anonymous authors
Paper under double-blind review
Ab stract
We study the evolution of internal representations during deep neural network
(DNN) training, aiming to demystify the compression aspect of the information
bottleneck theory. The theory suggests that DNN training comprises a rapid fitting
phase followed by a slower compression phase, in which the mutual information
I(X; T) between the input X and internal representations T decreases. Several
papers observe compression of estimated mutual information on different DNN
models, but the true I(X; T) over these networks is provably either constant (dis-
crete X) or infinite (continuous X). This work explains the discrepancy between
theory and experiments, and clarifies what was actually measured by these past
works. To this end, we introduce an auxiliary (noisy) DNN framework for which
I(X; T) is a meaningful quantity that depends on the network’s parameters. This
noisy framework is shown to be a good proxy for the original (deterministic) DNN
both in terms of performance and the learned representations. We then develop a
rigorous estimator for I(X; T ) in noisy DNNs and observe compression in various
models. By relating I(X; T) in the noisy DNN to an information-theoretic commu-
nication problem, we show that compression is driven by the progressive clustering
of hidden representations of inputs from the same class. Several methods to directly
monitor clustering of hidden representations, both in noisy and deterministic DNNs,
are used to show that meaningful clusters form in the T space. Finally, we return
to the estimator of I(X; T ) employed in past works, and demonstrate that while it
fails to capture the true (vacuous) mutual information, it does serve as a measure
for clustering. This clarifies the past observations of compression and isolates the
geometric clustering of hidden representations as the true phenomenon of interest.
1 Introduction
Recent work by Shwartz-Ziv & Tishby (2017) uses the Information Bottleneck framework (Tishby
et al., 1999; Tishby & Zaslavsky, 2015) to study the dynamics of DNN learning. The framework
considers the mutual information pair(I(X; T}),I(Y; T)) between the input X or the label Y
and the network's hidden layers Tg. Plotting the evolution of these quantities during training,
Shwartz-Ziv & Tishby (2017) made two interesting observations: (1) while I (Y; Tg) remains mostly
constant as the layer index Q increases, I(X; Tg) decreases, suggesting that layers gradually shed
irrelevant information about X ; and (2) after an initial fitting phase, there is a long compression
phase during which I(X; Tg) slowly decreases. It was suggested that this compression is responsible
for the generalization performance of DNNs. A follow-up paper (Saxe et al., 2018) contends that
compression is not inherent to DNN training, claiming double-sided saturating nonlinearities yield
compression while single-sided/non-saturating ones do not necessarily compress.
Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018) present many plots of(I (X; Tg), I (Y; Tg))
evolution across training epochs. These plots, however, are inadvertently misleading: they show a
dynamically changing I(X; Tg) when the true mutual information is provably either infinite or a
constant independent of the DNN’s parameters (see (Amjad & Geiger, 2018) for a discussion of
further degeneracies related to to the Information Bottleneck framework). Recall that the mutual
information I(X; Tg) is a functional of the joint distribution of (X, Tg)〜PXT = PχPTeIX, and
that, in standard DNNs, Tg is a deterministic function of X. Hence, if Pχ is continuous, then so is
Tg, and thus I(X; Tg) = ∞ (cf. (Polyanskiy & Wu, 2012-2017, Theorem 2.4)). If Pχ is discrete
(e.g., when the features are discrete or if X adheres to an empirical distribution over the dataset),
then the mutual information is a finite constant that does not depend on the parameters of the DNN.
Specifically, for deterministic DNNs, the mapping from a discrete X to Tg is injective for strictly
1
Under review as a conference paper at ICLR 2019
Epoch
Figure 1: I(X; Bin(Te)) vs. epochs for different bin sizes and the model in Shwartz-Ziv & Tishby
(2017). The curves converge to ln(212) ≈ 8.3 for small bins, per the 12-bit uniformly distributed X.
monotone nonlinearities such as tanh or sigmoid, except for a measure-zero set of weights. In other
words, deterministic DNNs can encode all information about a discrete X in arbitrarily fine variations
of Tg, causing no loss of information and implying I(X; Tg) = H(X), even if deeper layers & have
fewer neurons.
The compression observed in Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018) therefore cannot be
due to changes in mutual information. This discrepancy between theory and experiments originates
from a theoretically unjustified discretization of neuron values in their approximation of I(X; Tg). To
clarify, the quantity computed and plotted in these works is I(X; Bin(Tg)), where Bin is a per-neuron
discretization of each hidden activity of Tg into a user-selected number of bins. This I (X; Bin(Tg))
is highly sensitive to the selection of bin size (as illustrated in Fig. 1) and does not track I(X; Tg) for
any choice of bin size.1 Nonetheless, compression results based on I(X; Bin(Tg)) are observed by
Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018) in many interesting cases.
To understand this curious phenomenon we first develop a rigorous framework for tracking the flow
of information in DNNs. In particular, to ensure I(X; Tg) is meaningful for studying the learned
representations, We need to make the map X → Tg a stochastic parameterized channel whose
parameters are the DNN’s weights and biases. We identify several desirable criteria that such a
stochastic DNN framework should fulfill for it to provide meaningful insights into commonly used
practical systems. (1) The stochasticity should be intrinsic to the operation of the DNN, so that the
characteristics of mutual information measures are related to the learned internal representations, and
not to an arbitrary user-defined parameter. (2) The stochasticity should relate the mutual information
to the deterministic binned version I(X; Bin(Tg)), since this is the object whose compression was
observed; this requires the injected noise to be isotropic over the domain of Tg analogously to the
per-neuron binning operation. And most importantly, (3) the network trained under this stochastic
model should be closely related to those trained in practice.
We propose a stochastic DNN framework in which independent and identically distributed (i.i.d.)
Gaussian noise is added to the output of each of the DNN’s neurons. This makes the map from
X to Tg stochastic, ensures the data processing inequality (DPI) is satisfied, and makes I(X; Tg)
reflect the true operating conditions of the DNN, following Point (1). Since the noise is centered
and isotropic, Point (2) holds. As for Point (3), Section 2 experimentally shows the DNN’s learned
representations and performance are not meaningfully affected by the addition of noise, for variances
β2 not too large. Furthermore, randomness during training has long been used to improve neural
network performance, e.g., to escape poor local optima (Hinton et al., 1984), improve generalization
performance (Srivastava et al., 2014), encourage learning of disentangled representations (Achille &
Soatto, 2018), and ensure gradient flow with hard-saturating nonlinearities (Gulcehre et al., 2016).
Under the stochastic model, I(X; Tg) has no exact analytic expression and is impossible to approx-
imate numerically. In Section 3 we therefore propose a sampling technique that decomposes the
estimation of I(X; Tg) into several instances of a simpler differential entropy estimation problem:
estimating h(S + Z) given n samples of the d-dimensional random vector S and knowing the
distribution of Z 〜N(0, β2L). We analyze this problem theoretically and show that any differential
entropy estimator over the noisy DNN requires at least exponentially many samples in the dimension
d. Leveraging the explicit modeling of S + Z, we then propose a new estimator that converges
1 Another approach taken in Saxe et al. (2018) considers I(X; Te + Z) (instead of I(X; Bin(n))), where
Z is an independent Gaussian with a user-defined variance. This approach has two issues: (i) the values as
a function of C may violate the data processing inequality, and (ii) they do not reflect the operation of the
actual DNN, which was trained without noise. We focus on I(X; Bin(Te)) because it was commonly used in
Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018), and since both methods have a similar effect of blurring Te.
2
Under review as a conference paper at ICLR 2019
as O ((log n)44∕√n), which significantly outperforms the convergence rate of general-purpose
differential entropy estimators when applied to the noisy DNN framework.
We find that I(X; TE) exhibits compression in many cases during training of small DNN classifiers.
To explain compression in an insightful yet rigorous manner, Section 4 relates I(X; T) to the
well-understood notion of data transmission over additive white Gaussian noise (AWGN) channels.
Namely, I(X; T) is the aggregate information transmitted over the channel P⅛χ with input X
drawn from a constellation defined by the data samples and the noisy DNN parameters. As training
progresses, the representations of inputs from the same class tend to cluster together and become
increasingly indistinguishable at the channel,s output, thereby decreasing I(X; Te). Furthermore,
these clusters tighten as one moves into deeper layers, providing evidence that the DNN’s layered
structure progressively improves the representation of X to increase its relevance for Y .
Finally, we examine clustering in deterministic DNNs. We identify methods for measuring clustering
that are valid for both noisy and deterministic DNNs, and show that clusters of inputs in learned
representations typically form in both cases. We complete the circle back to I(X; Bin(TE)) by
clarifying why this binned mutual information measures clustering. This explains what previous
works were actually observing: not compression of mutual information, but increased clustering by
hidden representations. The geometric clustering of hidden representations is thus the fundamental
phenomenon of interest, and we aim to test its connection to generalization performance, theoretically
and experimentally, in future work.
2 Preliminary Definitions
Noisy DNNs: For integers k ≤ 1, let [k : 4 = {i ∈ Zlk ≤
i ≤ 1} and use [1] when k = 1. Consider a noisy DNN
with L +1 layers {TE}e∈[0s], with input To = X and output
Tl. The 1th hidden layer, 1 ∈ [L - 1], is described by TE =
fE(TE-ι) + ZE, where fg : Rd-1 → Rdg is a deterministic
function of the previous layer and ZE 〜 N(0,β2Idg) ； no
noise is injected to the output, i.e., TL = fL(TL-1). We set
SE = ∕e(TE-i) and use 夕 for the probability density function
(PDF) of Ze. The functions {∕e}e∈[l] can represent any type
of layer (fully connected, convolutional, max-pooling, etc.).
Fig. 2 shows a neuron in the 1th layer of a noisy DNN.
To explore the relation between noisy and deterministic DNNs
under conditions representative of current machine learning
practices, we trained four-layer convolutional neural networks
(CNNs) on MNIST (LeCun et al., 1999). The CNNs used
different levels of internal noise, including no noise, and one
used dropout in place of additive noise. We measured their
performance on the validation set and characterized the co-
sine similarities between their internal representations. Full
details of the CNN architecture and training procedure are in
Supplement 9.3. The results in Table 1 show small amounts
of internal additive noise (β ≤ 0.1) have a minimal impact on
classification performance, while dropout strongly improves
it. The histograms in Fig. 3 show that the noisy (for small β)
and dropout models learn internal representations similar to
T-^→ σ(w"-+bι(k))	θj≡
Zι(k)〜N(0, β2)
Figure 2: kth noisy neuron in layer 1
with nonlinearity σ; WEk) and bg(k)
are the kth row/entry of the weight
matrix and the bias, respectively.
Model		# Errors
Deterministic		50 ± 4.6
Noisy (β	0.05)	50 ± 5.0
Noisy (β	0.1)	51 ± 6.9
Noisy (β	0.2)	86 ± 9.8
Noisy (β	0.5)	2200 ± 520
Dropout (p = 0.2)		39 ± 3.9
Table 1: Total MNIST validation
errors for different models, show-
ing mean ± standard deviation over
eight initial random seeds.
the representations learned by the deterministic model. In this high-dimensional space, unrelated
representations would create cosine similarity histograms with zero mean and standard deviation
between 0.02-0.3, so the observed values are quite large. As expected, dissimilarity increases as the
noise increases, and similarity is lower for the internal layers (2 and 3).
Mutual Information: Noisy DNNs induce a stochastic map from X to the rest of the network,
described by the conditional distribution PT1,...,TL|X. The corresponding PDF2 is pT1,...,TL|X=x. Its
marginals are denoted by keeping only the relevant variables in the subscript. Let X = {xi}i∈[m] be
2PT1,...,TL|X=x is absolutely continuous with respect to (w.r.t.) the Lebesgue measure for all x ∈ X.
3
Under review as a conference paper at ICLR 2019
0.0	0.5	1.0 0.0	0.5	1.0 0.0	0.5	1.0 0.0	0.5	1.0
Cosine similarity to noiseless model
Figure 3: Histograms of cosine similarities between internal representations of deterministic, noisy,
and dropout MNIST CNN models. To encourage comparable internal representations, all models
were initialized with the same random weights and accessed the training data in the same order.
Layer 4 (full)
Model
C=I noisy (β = 0.05)
匚二I noisy (β = 0.1)
匚二I noisy (β = 0.2)
I I noisy (β = 0.5)
I I dropout (p = 0.2)
the input dataset, and PX be its empirical distribution, described by the probability mass function
(PMF) PX(x) = m ∑i∈[m] 1{xi=x}, for X ∈ X. Since data sets typically contain no repetitions,
we assume PX (x) = ml, ∀χ ∈ X. The input and the hidden layers are jointly distributed according
to3 4 Px,Ti,...,Tl = PXPTi,...,Tl∣x, under which X - T1 - ... - TL-I - TL forms a Markov chain.
For each Q ∈ [L - 1], We study the mutual information (Supplement 7 explains this factorization)
I (Xs) = LRde dpXT log (dP⅜⅛) = h(pT')- mm 工h(PT"=x J	⑴
i∈[m]
where log(∙) is with respect to the natural base. Although PT and PTe∣X are readily sampled from
using the DNN’s forward pass, these distributions are too complicated (due to the composition of
Gaussian noises and nonlinearities) to analytically compute I(X; T) or even to evaluate their densities
at the sampled points. Therefore, we must estimate I(X; T心 directly from the available samples.
3 Mutual Information Estimation over Noisy DNNs
Expanding I(X; T) as in (1), our goal is to estimate h(pT) and h(pT↑X=χ), ∀χ ∈ X: a problem
that we show is hard in high dimensions. Each differential entropy term is estimated and computed
via a two-step process. First, we develop the sample propagation (SP) estimator, which exploits the
ability to propagate samples up the DNN layers and the known noise distribution. This estimator
approximates each true entropy by the differential entropy of a known Gaussian mixture (defined only
through the available resources: the samples we obtain from the DNN and the noise parameter). This
estimate is shown to converge to the true entropy when the number of samples grows. However, since
the entropy of a Gaussian mixture has no closed-form expression, in the second (computational) step
we use Monte Carlo (MC) integration to numerically evaluate it.
3.1	The Sample-Propagation Differential Entropy Estimator
In what follows, we denote the empirical PMF associated with a set A = {ai }i∈[n] ⊂ Rd by P/.
Unconditional Entropy: Since T = Sg + Z乜 where Sg and Zg are independent, we have
PTe = PSg * 2.To estimate h(pτ^), let {Xj}j∈[n] be n i.i.d. samples from PX. Feed each Xj into the
DNN and collect the outputs it produces at the ( - 1)-th layer. The function fg is then applied on
each collected output to obtain Sg = {sg,ι, sg,2,∙∙∙, sg,n }, which is a set of n i.i.d. samples from
PSg. We estimate h(Pτg) by h(Psg * 夕)，which is the differential entropy of a Gaussian mixture with
centers sgj, j ∈ [n]. The term h(Psg * 夕)is referred to as the SP estimator of h(Pτg) = h(PSg * 4).
Conditional Entropies: Fix i ∈ [m] and consider the estimation of h(Pτg∣X=χi). Note that
PTeIX=Xi = PSg∣X=χi * 夕 since Zg is independent of (X, Tg-1). To sample fromPse∖X=χi, we feed
Xi into the DnN n times, collect outputs from Tg-ι corresponding to different noise realizations,
and apply fg on each. The obtained samples Sf) = {s(i), Sgi),..., Sgini} are i.i.d. according to
Pse∣x=Xi. Each h(PTe∣X=Xi) = h(Pse∣x=Xi * 2)is estimated by the SP estimator h(Psgi) * 夕).4
Mutual Information Estimator: Combining the above described pieces, we estimate I(X; Tg) by
I(X； Tg) = h(PSe * 夕)——£,	] h (PS(i) * 夕).	Q)
m Z""zi∈[m]	Se	)
3We set X 〜Unif (X) to conform with past works (Shwartz-Ziv & Tishby, 2017; Saxe et al., 2018).
4For C = 1, we have h(Tι∣X) = h(Zι) = dɪ log(2πeβ2) because its previous layer is X (fixed).
4
Under review as a conference paper at ICLR 2019
3.2	Theoretical Guarantees and Computing the Estimator
The above sampling procedure unifies the estimation of h(pτ∕ and {h(pn∣χ=x) }χ∈χ into a single
new differential entropy estimation problem: estimate h(ps * 夕)based on i.i.d. samples Sn =
(Si)i∈[n] from PS and knowledge of 夕.The SP estimator solution approximates h(ps * 夕)by
hsp(Sn,2)=h(psη * 夕),where PSn is the empirical PMF induced by Sn. Before analyzing the
ʌ
performance of hSP, we note that this estimation problem is statistically difficult in the sense that any
good estimator of h(ps * 夕)based on Sn and 夕 requires exponentially many samples in d (Theorem 2
from Supplement 10). Nonetheless, the following theorem shows that the SP estimator absolute-error
risk converges at a satisfactory rate (Theorem 4 from Supplement 10 states this with all constants
explicit, and Theorem 5 gives the results for ReLU).
Theorem 1 Fix β > 0, d ≥ 1, and let Fd be the class of d-dimensional PDFs supported inside
[-1,1]d. Wehave: supps ∈Fd Elh(PS * ψ) - hsp (S n,ψ)∖ = O ((log n)d/4∕√n).
ʌ
Evaluating the SP estimator hsp(Sn,夕)of the true entropy h(ps * /)requires computing the
differential entropy of the (known) Gaussian mixture Psn * / since
hsp(Sn,0 = h(psn * φ).	(3)
Noting that the differential entropy h(p) = -Eχ~p[logP(X)], we rewrite the SP estimator as
hsp(Sn, 6 = h(G) = -E[log ((Psn * 0(G))],	(4)
where G 〜Psn * 6 is distributed according to the Gaussian mixture.
We numerically approximate the right-hand side of (4) via efficient Monte Carlo (MC) integration
(Robert, 2004). Specifically, we generate ∏mc i.i.d. samples from Psn * 6 and approximate the
expectation by an empirical average. This unbiased approximation achieves a mean squared error
of O((n ∙ ∏Mc)-1) (Supplement 10). This approximation thus only adds a negligible amount to the
error of the SP estimator ∖h(Ps * 6)一 hSP(Sn,6)∖ itself. There are other ways to numerically
evaluate this expectation, such as the Gaussian mixture bounds from Kolchinsky & Tracey (2017);
however, our proposed method is the fastest approach of which we are aware.
Remark 1 (Choosing Noise Parameter and Number of Samples) We describe practical guide-
lines for selecting the noise standard deviation β and the number of samples n for estimating
I(X; Tg) in an actual CIaSSifier Ideally, β should be treated as a hyperparameter tuned to optimize
the performance of the classifier on held-out data, since internal noise serves as a regularizer similar
to dropout. In practice, we find it is sometimes necessary to back off from the β value that optimizes
performance to a higher value to ensure accurate estimation of mutual information (the smaller β is,
the more samples our estimator requires), depending on factors such as the dimensionality of the
layer being analyzed and the number of data samples available for a task.
The number of samples ncan be selected using the bound in Theorem 1, but because this theorem is
a worst-case result, in practice it is quite pessimistic. Specifically, generating the estimated mutual
information curves shown in Section 5 requires running the SP estimator multiple times5, which
makes the number of samples dictated by Theorem 1 infeasible. To overcome this computational
burden while adhering to the theoretical result, we tested the value of n given by the theorem on afew
points of each curve and reduced it until the overall computation cost became reasonable. To ensure
estimation accuracy was not compromised we empirically tested that the estimate remained stable.
As a concrete example, to achieve an error bound of5% of Fig. 5 plot’s vertical scale (which amounts
to an 0.4 absolute error bound), the number of samples required by Theorem 1 is n = 4 ∙ 109. This
number is too large for our computational budget. Performing the above procedure for reducing
n, we find good accuracy is achieved for n = 4 ∙ 106 samples (Theorem 1 has the pessimistic error
bound of 3.74 for this value). Adding more samples beyond this value does not change the results.
5Each I (X; T⅛), fora given set ofDNN parameters, involves computing m +1 differential entropy estimates,
and our experiments estimate the trajectory of I(X; Te) across training epochs.
5
Under review as a conference paper at ICLR 2019
Figure 4: Single-layer tanh network: (a) the density pT (k) at epochs k = 250, 2500; (b) pT (k) and (c)
I X; T(k) as a function of k; and (d) mutual information as a function of weight w with bias -2w.
4 Compression and Clustering: A Minimal Example
Before presenting our empirical results, we connect compression to clustering using an information-
theoretic perspective. Consider a single noisy neuron with a one-dimensional input X. Let T (k) =
S(k) + Z be the neuron's output at epoch k, where S(k) = σ(WkX + bk), for a strictly monotone
nonlinearity σ, and Z 〜N(0, β2). Invariance of mutual information to invertible operations implies
I(X； T(k)) = I(σ(WkX + bk); σ(WkX + bk) + Z) = I(S(k); S(k) + Z).	(5)
From an information-theoretic perspective, I S(k); S(k) + Z is the aggregate information transmit-
ted over an AWGN channel with input constellation Sk = {σ(WkX + bk) | X ∈ X}. In other words,
I S(k); S(k) + Z is a measure of how distinguishable the symbols of Sk are when composed with
Gaussian noise (roughly equals log of the number of resolvable clusters under noise level β). Since
the distribution of T(k) = S(k) + Z is a Gaussian mixture with means s ∈ Sk, the closer two
constellation points S and s' are, the more overlapping the Gaussians around them will be. Hence
reducing point spacing in Sk (by changing Wk and bk) directly reduces I X; T(k) .
Let σ = tanh and β = 0.01, and set X = X-1 ∪ X1, with X-1 = {-3, -1, 1} and X1 = {3},
labeled -1 and 1, respectively. We train the neuron using mean squared loss and gradient descent
(GD) with a fixed learning rate of 0.01 to best illustrate the behavior of I X; T(k) . The Gaussian
mixture pT(k) is plotted across epochs k in Fig. 4(a)-(b). The learned bias is approximately -2.3W,
ensuring that the tanh transition region correctly divides the two classes. Initially W = 0, so all four
Gaussians in pT (0) are superimposed. As k increases, the Gaussians initially diverge, with the three
from X-1 eventually re-converging as they each meet the tanh boundary. This is reflected in the
mutual information trend in Fig. 4(c), with the dips in I X; T(k) around k = 103 and k = 104
corresponding to the second and third Gaussians respectively merging into the first. Thus, there is a
direct connection between clustering and compression. Fig. 4(d) shows the mutual information for
different noise levels β as a function of epoch. For small β (as above) the X-1 Gaussians are distinct
and merge in two stages as W grows. For larger β, however, the X-1 Gaussians are indistinguishable
for any W, making I(X; T) only increase as the two classes gradually separate. A similar example
for a two-neuron network with leaky-ReLU nonlinearities is provided in the Supplement 8.
5	Empirical Results
We now show the observations from our minimal examples also hold for two larger networks. Namely,
the presented experiments demonstrate the compression of mutual information in noisy networks is
driven by clustering of internal representation, and that deterministic networks cluster samples as
well (despite I(X; T) being constant over these systems). The DNNs we consider are: (1) the small,
fully connected network (FCN) studied in (Shwartz-Ziv & Tishby, 2017; Saxe et al., 2018), which we
call the SZT model; and (2) a convolutional network for MNIST classification, called MNIST CNN.
We present selected results; additional details and experiments are found in the supplement.
SZT model:
Consider the data and model of Shwartz-Ziv & Tishby (2017) for binary classification of 12-
dimensional inputs using a fully connected 12-10-7-5-4-3-2 architecture. The FCN was tested with
tanh and ReLU nonlinearities as well as a linear model. Fig. 5(a) presents results for the SZT model
with tanh nonlinearity and β = 0.005 (test classification accuracy 99%), showing the relationship
across training epochs between estimated I(X; T), train/test losses and the distribution of neuron
values in 5 layers (layers 0 (d0 = 12) and 6 (d7 = 2) are not shown). The rise and fall of mutual
information corresponds to how spread out or clustered the representation in each layer are. For
6
Under review as a conference paper at ICLR 2019
Q
Figure 5: (a) Evolution of I(X; TQ) and training/test losses across training epochs for the SZT
model with β = 0.005 and tanh nonlinearities. The scatter plots show the values of Layer 5 (d5 = 3)
at the arrow-marked epochs on the mutual information plot. The bottom plot shows H(Bin(Te))
across epochs for bin size B= 10β. (b) Same setup as in (a) but with regularization that encourages
orthonormal weight matrices. (c) SZT model with β = 0.01 and linear activations.
example, I(X; T5) grows until epoch 28, when the Gaussians move away from each other along a
curve (see scatter plots on the right). Around epoch 80 they start clustering and I(X; T5) drops. At
the end of training, the saturating tanh nonlinearities push the Gaussians to two furthest corners of
the cube, reducing I(X; T5) even more.
To confirm that clustering (via saturation) was central to the compression observed in Fig. 5(a),
we also trained the model using the regularization from (Cisse et al., 2017) (test classification
accuracy 96%), which encourages orthonormal weight matrices. The results are shown in Fig. 5(b).
Apart from minor initial fluctuations, the bulk of compression is gone. The scatter plots show that
the vast majority of neurons do not saturate and no clustering is observed at the later stages of
training. Saturation is not the only mechanism that can cause clustering and consequently reduce
I(X; To). For example, in Fig. 5(c) We illustrate the clustering behavior in a linear SZT model (test
7
Under review as a conference paper at ICLR 2019
Figure 6: (a) Histogram of within- and between-class pairwise distances for SZT model with tanh
non-linearities and additive noise β = 0.005. (b) Same as (a) but training with weight normalization.




Lcaoooo
KOODO
(b)
classification accuracy 89%). As seen from the scatter plots, due to the formation of several clusters
and projection to a lower dimensional space, I(X; T) drops even without the nonlinearities. The
results in Fig. 5(a) and (b) also show that the relationship between compression and generalization
performance is not a simple one. In Fig. 5(a), the test loss begins to increase at roughly epoch 3200
and continues to increase until training ends, while at the same time compression occurs in layers
4 and 5. In contrast, in Fig. 5(b) the test loss does not increase, and compression does not occur in
layers 4 and 5. We believe that this is a subject that deserves further examination in future work.
To provide another perspective on clustering that is sensitive to class membership, we compute
histograms of pairwise distances between representations of samples, distinguishing within-class
distances from between-class distances. Fig. 6 shows histograms for the SZT models from Figs. 5(a)
and (b). As training progresses, the formation of clusters is clearly seen (layer 3 and beyond) for
the unnormalized SZT model in Fig. 5(a). In the normalized model (Fig. 5(b)), however, no tight
clustering is apparent, supporting the connection between clustering and compression.
Once clustering is identified as the source of compression, we focus on it as the point of interest. To
measure clustering, the discrete entropy of Bin(或)is considered, where the number of equal-sized
bins, B, is a tuning parameter. Note that Bin(T^) partitions the dynamic range (e.g., [-1,1]de for
a tanh layer) into Bd cells or bins. When hidden representations are spread out, many bins will
be non-empty, each assigned with a positive probability mass. On the other hand, for clustered
representations, the distribution is concentrated on a small number of bins, each with relatively high
probability. Recalling that discrete entropy is maximized by the uniform distribution, we see why
reduction in H(Bin(T^)) measures clustering.
To illustrate this measure, We compute H(Bin(T^)) for each of the SZT models using bin size
B = 10β (bottom plots in Fig. 5(a), (b) and (c)). We can see a clear correspondence between
H(Bin(或))and I(X; T), indicating that although H(Bin(T^)) does not capture the exact value of
I(X; TQ), it follows this mutual information in measuring clustering. This is particularly important
when moving back to deterministic DNNs, where I(X; TQ) is no longer an informative measure,
being either a constant or infinity, for discrete or continuous X , respectively.
Fig. 1 shows H(Bin(TQ)) for the deterministic SZT model (β = 0). The bin size is a free parameter,
and depending on its value, H(Bin(TQ)) reveals different clustering granularities. Moreover, since in
deterministic networks TQ = fQ(X), for a deterministic map fQ, we have H(Bin(TQ) ∣X) = 0, and
therefore I(X; Bin(TQ)) = H(Bin(TQ)). Thus, the plots from (Shwartz-Ziv & Tishby, 2017), (Saxe
et al., 2018) and our Figs. 1 and 5(a), (b) and (c) all show the entropy of the binned TQ.
MNIST CNN: We now examine a model that is more representative of current machine
learning practice: the MNIST CNN trained with dropout from Section 2. Fig. 7 por-
trays the near-injective behavior of this model. Even when only two bins are used to
compute H(Bin(TQ)), it takes values that are approximately ln(10000) = 9.210, for all
layers and training epochs, even though the two convolutional layers use max-pooling.
8
Under review as a conference paper at ICLR 2019
This binning merges two samples in the validation set, so
the input has H(Bin(T})) = 9.209. While Fig. 7 does
not show compression at the level of entire layers, com-
puting H(Bin(TXk))) for individual units k in layer 3
reveals a gradual decrease over epochs 1-128. To quan-
tify this trend, we computed linear regressions predicting
H(Bins(k))) from the epoch index, for all units k in
layer 3. Then we found the mean and standard deviation
of the slope of the linear predictions. If most slopes are
negative, then compression occurs during training at the
level of individual units. For a range of bin sizes from
10-4-10-1 the least negative mean slope was -0.002
nats/epoch with a maximum standard deviation of 0.001,
showing that most units undergo compression.
O
2
((EUw
9.19
O
----Input
---- Layer 1 (conv)
Layer 2 {conv)
Layer 3 (full)
—'Layer 4 (full)
50	IOO
epoch
Figure 7: H(Bin(T^)) for the MNIST
CNN, computed using two bins: [-1,0]
and (0,1]. The tiny range of the y axis
shows the near injectivity of the model.

In Fig. 8 we show histograms of pairwise distances between MNIST validation set samples in the
input (pixel) space and in the four layers of the CNN. The histograms were computed for epochs 0, 1,
32, and 128, where epoch 0 is the initial random weights and epoch 128 is the final weights. The
histogram for the input shows that the mode of within-class pairwise distances is lower than the mode
of between-class pairwise distances, but that there is substantial overlap. Layers 1 and 2, which are
convolutional and therefore do not contain any units that receive the full input, do little to reduce this
overlap, suggesting that the features learned in these layers are somewhat generic. In contrast, even
after one epoch of training, layers 3 and 4, which are fully connected, separate the distribution of
within-class distances from the distribution of between-class distances.
Input Layer 1 (conv) Layer 2 (conv) Layer 3 (full)
ιoooooo -
Layer 4 (full)
1000000 -
o-
0 LPod 山 1LPod 山
Wωucωb3uuo #
1000000 -
O-
1000000 -
,40
I O
,
,40
I O
,
,40
Z"LPodW S LPodW
pairwise distance
Figure 8: Histograms of within-class and between-class pairwise distances from the MNIST CNN.
To summarize, we made the following observations in our experiments. (i) Compression can be
observed in a noisy network that is similar to the deterministic network in which (Shwartz-Ziv &
Tishby, 2017) reported compression (upper left plot in Fig. 5(a)). (ii) Compression is caused by
clustering of samples, with clusters most often comprising samples having the same class label, as seen
in the scatter plots on the right sides of Figs. 5(a) and (c) and the distributions of pairwise distances
between samples shown in Figs. 6 and 8. (iii) Regularization that limits the ability of a network to
drive hidden units into saturation may limit or eliminate compression (and clustering) as seen in
Fig. 5(b). Fig. 5 also demonstrated that I(X; Tg) and H(Bin(T^)) are highly correlated, establishing
the latter as an additional measure for clustering (applicable both in noisy and deterministic DNNs).
(iv) Clustering of internal representations can also be observed in a somewhat larger, convolutional
network trained on MNIST. While Fig. 7 shows that due to the dimensionality, H(Bin(Tg)) fails to
track compression in the larger CNN, strong evidence for clustering is found via estimates done at
the level of individual units (described in the text on the MNIST CNN) and the analysis of pairwise
distances between samples shown in Fig. 8.
6 Conclusions
In this work we reexamined the compression aspect of the Information Bottleneck theory (Shwartz-
Ziv & Tishby, 2017), noting that fluctuations of I(X; Tg) in deterministic networks with strictly
9
Under review as a conference paper at ICLR 2019
monotone nonlinearities are theoretically impossible. Setting out to discover the source of com-
pression observed in past works, we: (i) created a rigorous framework for studying and accurately
estimating information-theoretic quantities in DNNs whose weights are fixed; (ii) identified clustering
of the learned representations as the phenomenon underlying compression; and (iii) demonstrated
that the compression-related experiments from past works were in fact measuring this clustering
through the lens of the binned mutual information. In the end, although binning-based measures do
not accurately estimate mutual information, they are simple to compute and prove useful for tracking
changes in clustering, which is the true effect of interest in deterministic DNNs. We believe that
further study of geometric phenomena driven by DNN training is warranted to better understand the
learned representations and to potentially establish connections with generalization.
10
Under review as a conference paper at ICLR 2019
References
A. Achille and S. Soatto. Information dropout: Learning optimal representations through
noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.
10.1109/TPAMI.2017.2784440.
R. A. Amjad and B. C. Geiger. Learning representations for neural network-based classification using
the information bottleneck principle. arXiv preprint arXiv:1802.09766, 2018.
M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving
robustness to adversarial examples. In Proceedings of the International Conference on Machine
Learning (ICML), 2017.
C. Gulcehre, M. Moczulski, M. Denil, and Y. Bengio. Noisy activation functions. In Proceedings of
the International Conference on Machine Learning (ICML),pp. 3059-3068, Jun. 2016.
G. E. Hinton, T. J. Sejnowski, and D. H. Ackley. Boltzmann machines: Constraint satisfaction
networks that learn. Technical Report CMU-CS84-119, Carnegie-Mellon University, 1984.
A. Kolchinsky and B. D. Tracey. Estimating mixture entropy with pairwise distances. Entropy, 19(7):
361, Jul. 2017.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proc. IEEE, 86(11):2278-2324, Nov. 1999.
Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. 2012-2017. URL http:
//people.lids.mit.edu/yp/homepage/data/itlectures_v5.pdf.
Christian P Robert. Monte Carlo Methods. Wiley Online Library, 2004.
A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky, B. D. Tracey, and D. D. Cox. On the
information bottleneck theory of deep learning. In Proceedings of the International Conference on
Learning Representations (ICLR), 2018.
R. Shwartz-Ziv and N. Tishby. Opening the black box of deep neural networks via information.
arXiv:1703.00810 [cs.LG], 2017.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple
way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:
1929-1958, 2014.
N. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. In Proceedings
of the Information Theory Workshop (ITW), pp. 1-5, Jerusalem, Israel, Apr.-May 2015.
N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Proceedings of
the Allerton Conference on Communication, Control and Computing, pp. 368-377, Monticello,
Illinois, US, Sep. 1999.
11
Under review as a conference paper at ICLR 2019
Supplement to Estimating Information Flow in
DNNs
Anonymous authors
Paper under double-blind review
7 Mutual Information
Let (A, B) be a pair of random variables with values in the product set A × B and a joint distribution
PA,B (whose marginals are denoted by PA and PB). The mutual information between A and B is:
I(A; B) = /	dPA,B log ( ,pPA,B ) ,	(5)
A×B	dPA × PB
where dP：：% is the Radon-Nikodym derivative of Pa,b with respect to the product measure
PA × PB . We are mostly interested in the scenario where A is discrete with a probability mass
function (PMF) pA, and given A = a ∈ A, B is continuous with probability density function (PDF)
PB∣A=a = PB∣A(∙∣a). In this case, (5) simplifies to
I(A; B) = E PA(a) ɪPB∣A(b∣a)log (，；；(：：) db.	(6)
Defining the differential entropy of a continuous random variable C with PDF pC supported in C as1
h(C) = h(pC)
-C
pC (c) logpC (c)dc,
the mutual information from (6) can also be expressed as
I(A; B) = h(pB) -	pA(a)h(pB|A=a).
a∈A
(7)
(8)
The subtracted term above is the conditional differential entropy of B given A, denoted by h(B|A).
8	Two-Neuron Leaky-ReLU Network Example
To expand upon Section 4, we provide here a second example to illustrate the relation between
clustering and compression of mutual information. In particular, this example also shows that as
opposed to the claim from (Saxe et al., 2018), non-saturating nonlinearities can achieve compression.
Consider the non-saturating Leaky-ReLU nonlinearity R(X) = max(x, x/10). Let X = X。∪ X1/4,
with X0 = {1, 2, 3, 4} and X1/4 = {5, 6, 7, 8}, and labels 0 and 1/4, respectively. We train the
network via GD with learning rate 0.001 and mean squared loss. Initialization (shown in Fig. 9(a))
was chosen to best illustrate the connection between the Gaussians’ motion and mutual information.
The network converges to a solution where w1 < 0 and b1 is such that the elements in X1/4 cluster.
The output of the first layer is then negated using w2 < 0 and the bias ensures that the elements in X0
are clustered without spreading out the elements in X1/4 . Figs. 9(b) show the Gaussian motion at the
output of the first layer and the resulting clustering. For the second layer (Fig. 9(c)), the clustered
bundle X1/4 is gradually raised by growing b2, such that its elements successively split as they cross
the origin; further tightening of the bundle is due to shrinking |w2|. Fig. 9(d) shows the mutual
information of the first (blue) and second (red) layers. The merging of the elements in X1/4 after their
initial divergence is clearly reflected in the mutual information. Likewise, the spreading of the bundle,
and successive splitting and coalescing of the elements in X1/4 are visible in the spikes in the red
mutual information curve. The figure also shows how the bounds on I X; T(k) precisely track its
evolution.
1Throughout this work we interchanging use h(C) and h(pc) for the differential entropy of C 〜PC.
1
Under review as a conference paper at ICLR 2019
Figure 9: Two-layer leaky ReLU network: (a) network parameters as a function of epoch, (b,c) the
corresponding PDFs pT1(k) and pT2(k), and (d) the mutual information for both layers.
9	Experimental Details
9.1	SZT Model
In this section we provide additional experimental details and results for the SZT model discussed in
Section 5 of the main paper.
To regularize the network weights, we followed (Cisse et al., 2017) and adopted their approach for
enforcing an orthonormality constraint. Specifically, We first update the weights {W^}c∈[L] using the
standard gradient descent step, and then perform a secondary update to set
W — W - α (WCWT - IdJ W,
where the regularization parameter α controls the strength of the orthonormality constraint. The
value of α was was selected from the set {1.0 × 10-5, 2.0 × 10-5, 3.0 × 10-5, 4.0 × 10-5, 5.0 ×
10-5, 6.0 × 10-5, 7.0 × 10-5} and the optimal value was found to be equal to 5.0 × 10-5 for both
the tanh and ReLU.
In Fig. 10 we present additional experimental results that provide further insight into the clustering
and compression phenomena for both tanh and ReLU nonlinearities. Fig. 10(a) shows what happens
when the additive noise has a high variance. In this case, although saturation still occurs (see the
histograms on top of Fig. 10(a)) and the Gaussians still cluster together (see the scatter plots on the
right for the epoch 54 and epoch 8990), compression overall is very mild. The effect of increasing
the noise parameter was explained in Section 4 of the main text (see, in particular, Fig. 4(d) therein).
Comparing Fig. 10(a) to Fig. 5(a) of the main text, for which β = 0.005 was used and compression
was observed, further highlights the effect of large β . Recall that smaller β values correspond to
narrow Gaussians, while larger β values correspond to wider Gaussians. When β is small, even
Gaussians that belong to the same cluster are distinguishable so long as they are not too close. When
clusters tighten, the in-class movement brings these Gaussians closer together, effectively merging
them, and causing a reduction in mutual information (compression). One the other hand, for large
β, the in-class movement is blurred at the outset (before clusters tighten). Thus, the only effect on
mutual information is the separation between the clusters: as these blobs move away from each other,
mutual information rises.
Based on the above observation, we can conclude that while the two notions of “clustering Gaussians”
and “compression/decrease in mutual information” are strongly related in the low-beta regime, once
the noise becomes large, these phenomena decouple, i.e., the network may cluster inputs and neurons
may saturate, but this will not be reflected in a decrease of mutual information.
Finally, we present results for ReLU activation without weight normalization (Fig. 10(b)) and with
orthonormal weight regularization (Fig. 10(c)). We see that both these networks exhibit almost no
compression. For Fig. 10(c), the lack of compression is attributed to regularization of the weight
matrices, as explained in Section 5 of the main text. For Fig. 10(b), the reduction in compression can
be explained by the fact that although ReLU forces saturation of the neurons at the origin (which
promotes clustering), since the positive axes remain unconstrained, the Gaussians can move off
towards infinity without bound. This is visible from the histograms in the top row of Fig. 10(b),
where, for example, in layer 5 the neurons can take arbitrarily large positive values (note that the bin
corresponding to the value 5 accumulates all the values from 5 to infinity). Therefore, the clustering at
the origin and the potential drop in mutual information is counterbalanced by the spread of Gaussians
along the positive axes and the potential increase of mutual information it causes. Eventually, this
leads to the approximately constant profile of the mutual information plot in Fig. 10(b).
2
Under review as a conference paper at ICLR 2019
Figure 10: SZT model with (a) tanh nonlinearity and additive noise β = 0.01 without weight
normalization, (b) ReLU nonlinearity and β = 0.01 without weight normalization, (c) ReLU
nonlinearity and β = 0.01 with weight normalization. Test classification accuracy is 97%, 96%, and
97%, respectively.
The behavior of the weight-normalized ReLU in Fig. 10(c) is similar to Fig. 10(b), although now the
growth of the network weights is bounded and the saturation around origin is reduced. For example,
for layers 4 and 5 we can see an upward trend in the mutual information, which is then flattened at
the end of training. This occurs since more Gaussians are moving away from the origin, although
their motion remains bounded (see the histograms on the top and the scatter plots on the right), thus
decreasing the clustering density, leading to the rise in the mutual information profile. Once the
Gaussians are prevented from moving any further along the positive axes, a slight compression occurs
and the mutual information flattens.
3
Under review as a conference paper at ICLR 2019
9.2	Spiral Model
In this section we present results for another synthetic example. We generated data in the form of
spiral as in Fig. 11. The network architecture was similar to SZT model, except that the size of each
layer was set to 3.
Fig. 12 shows MI estimates I(X; TQ) computed using SP estimator and the discrete entropy estimates
H(Bin(TQ)) for weight un-normalized Fig. 12 (a) and normalized models Fig. 12 (b) and using
additive noise β = 0.005. Similar as in the main paper, the results in the figure illustrate a connection
between clustering and compression.
Figure 11: Generated spiral data for binary classification problem.
Finally, in Fig. 13 we also show an estimate of H(Bin(TQ)) for the case of deterministic DNN trained
on spiral data. For the particular choice of the bin size, the result of the estimated entropy reveal a
certain level of clustering granularity.
9.3 MNIST CNN
In this section, we describe in detail the architecture of the MNIST CNN models used in Sec-
tions 2 and 5 in the main paper.
The MNIST CNNs were trained using PyTorch (Paszke et al., 2017) version 0.3.0.post4. The
CNNs use the following fairly standard architecture with two convolutional layers, two fully connected
layers, and batch normalization.
1.	2-d convolutional layer with 1 input channel, 16 output channels, 5x5 kernels, and input
padding of 2 pixels
2.	Batch normalization
3.	Tanh() activation function
4.	Zero-mean additive Gaussian noise with variance β2 or dropout with a dropout probability
of 0.2
5.	2x2 max-pooling
6.	2-d convolutional layer with 16 input channels, 32 output channels, 5x5 kernels, and input
padding of 2 pixels
7.	Batch normalization
8.	Tanh() activation function
9.	Zero-mean additive Gaussian noise with variance β2 or dropout with a dropout probability
of 0.2
10.	2x2 max-pooling
11.	Fully connected layer with 1586 (32x7x7) inputs and 128 outputs
12.	Batch normalization
13.	Tanh() activation function
4
Under review as a conference paper at ICLR 2019
I—	Layer 1
----Layer 2
-Layer 3
—Layer 4
Layer 5
8 4 050
(SIEU)-W0.5
SSol
Layer 1
Layer 2
-Layer 3
—Layer 4
----Layer 5
-----Train
-----Test
Figure 12: (a) Evolution of I(X; TQ) and training/test losses across training epochs for Spiral dataset
with β = 0.005 and tanh nonlinearities. The scatter plots on the right are the values of Layer 5
(d5 = 3) at the arrow-marked epochs on the mutual information plot. The bottom plot shows the
entropy estimate H(Bin(TQ)) across epochs for bin size B = 10β. (b) Same setup as in (a) but with
a regularization that encourages orthonormal weight matrices.
Figure 13: H (Bin(TQ)) estimate for deterministic net using spiral data. Bin size was set to B = 0.001.
14.	Zero-mean additive Gaussian noise with variance β2 or dropout with a dropout probability
of 0.2
15.	Fully connected layer with 128 inputs and 10 outputs
5
Under review as a conference paper at ICLR 2019
All convolutional and fully connected layers have weights and biases, and the weights are initialized
using the default initialization, which draws weights from Unif[-1∕√m, 1∕√m], with m the fan-
in to a neuron in the layer. Training uses cross-entropy loss, and is performed using stochastic
gradient descent with no momentum, 128 training epochs, and 32-sample minibatches. The initial
learning rate is 5 × 10-3, and it is reduced following a geometric schedule such that the learning
rate in the final epoch is 5 × 10-4. To improve the test set performance of our models, we applied
data augmentation to the training set by translating, rotating, and shear-transforming each training
example each time it was selected. Translations in the x- and y-directions were drawn uniformly from
{-2, -1,0,1, 2}, rotations were drawn from Unif(-10°, 10°), and shear transforms were drawn
from Unif(-10°, 10°).
To obtain more reliable performance results, we train eight different models and report the mean
number of errors and standard deviation of the number of errors on the MNIST validation set. To
ensure that the internal representations of different models are comparable, which is necessary for
the use of the cosine similarity measure between internal representations, for each noise condition
(deterministic, noisy with β = 0.05, noisy with β = 0.1, noisy with β = 0.2, noisy with β = 0.5,
and dropout with p = 0.2), we use a common random seed (different for the eight replications, of
course) so the models have the same initial weights and access the training data in the same order
(use the same minibatches).
At test time, all models are fully deterministic: the additive noise blocks and dropout layers are
replaced by identities. Thus, in the figures and text in the main paper, “Layer 1” is the output of step
5 (2x2 max-pooling), “Layer 2” is the output of step 10 (2x2 max-pooling), “Layer 3” is the output
of step 13 (Tanh() activation function), and “Layer 4” is the output of step 15 (fully connected layer
with 10 outputs).
10 Sample Propagation Estimator - Theoretic Guarantees
Both conditional and unconditional entropy estimators reduce to the problem of estimating h(ps * 夕)
using i.i.d. samples Sn = (Si)i∈[n] from S 〜PS while knowing 夕.In this section we state
performance guarantees for the SP estimator. These results are excerpted from our work (Anonymized,
2018), where this estimation problem is thoroughly studied. The interested reader is referred to
(Anonymized, 2018) for proofs of the subsequently stated results.
10.0.1 Preliminary Definitions
Let Fd be the set of distributions P with supp(P) ⊆ [-1, 1]d.2 The minimax absolute-error risk
over Fd is
Rd(n,β) = inf sup Esn ∣h(P ”)- h(Sn,β)∣,	(9)
h P∈Fd	'	'
ʌ
where h is an estimator of h(P * 夕)based on the empirical data Sn = (Si,..., Sn) of i.i.d.
samples from P and the noise parameter β2. In (9), by P * 夕 we mean either: (i) (P * 夕)(χ)=
JRd p(u)夕(X - u)du = (P * 夕)(x), when P is continuous with density p; or (ii) (P * 夕)(x)=
Nu∙ p(u)>o P(U)夕(X - u), if P is discrete with PMF p. This convolved distribution can be defined
generally in a way that the two instances above as special cases using measure-theoretic concepts
(see (Anonymized, 2018)). Regardless of the nature of P, however, we stress that P * 夕 is always a
continuous distribution since it corresponds to the random variable S + Z , where Z is an isotropic
Gaussian. The sample complexity n与(η, β) is defined as the smallest number of samples (up to
constant factors) for which estimation within an additive gap η is possible. Namely,
nd(η,β) = min {n∣Rd(n,β) ≤ η}.	(10)
We also consider the class of distributions with subgaussian marginals; these will correspond to
mutual information estimation over noisy DNNs with ReLU nonlinearities. A subgaussian random
variable is defined as follows.
2Any support included in a compact subset of Rd would do. We focus on the case of supp(P) ⊆ [-1, 1]d
due to its correspondence to a noisy DNN with tanh nonlinearities.
6
Under review as a conference paper at ICLR 2019
Definition 1 (Subgaussian Random Variable) A random variable X is subgaussian if it satisfies
either of the following equivalent properties
1. Tail condition: ∃K1 > 0,
P(∣X∣ > t) ≤ exp(1 — kK2), forall t ≥ 0;
2.
1
Moment condition: ∃K2 > 0, (E|X∣p)p ≤ Kʌ/p, for allP ≥ 1;
3.	Super-exponential moment: ∃K3 > 0,
Eexp (K2) ≤ e
where Ki, for i = 1, 2, 3, differ by at most an absolute constant. Furthermore, the subgaussian
norm ∣∣X ∣∣ψ2 ofa Subgaussian random variable X is defined as the smallest K in property 2, i.e.,
∣∣X∣∣ψ2 ʌ supp≥ιP- 1(E∣X∣p)1.
Now, let Fd(,SKG) be the class of distributions P of a d-dimensional random variable S =
(S(1),..., S(d)) whose coordinates are SUbgaUssian with ∣∣S(i)∣∣ψ2 ≤ K, for all i ∈ [d]. The
risk and the sample complexity defined with respect to the nonparametric class Fd(,SKG) are denoted
by RdK(n,β) and nd K(δ,β), respectively. Clearly, for any S 〜P with SuPP(P) ⊆ [—1,1]d
We have ∣∣S(i)∣∣ψ2 ≤ 1, for all i ∈ [d], and therefore Fd ⊆ Fd(SG). As a consequence We obtain
Rd(n, β) ≤ RdiK(n, β) and njd(δ, β) ≤ nd K(δ, β) for all n ∈ N and δ > 0, whenever K ≥ 1. As
explained in the following remark, the considered sUbgaUssianity reqUirement is natUrally satisfied by
oUr noisy DNN framework.
Remark 1 (Generality of Subgaussian Class Fd(,SKG) and Noisy ReLU DNNs) The class Fd(S,KG) ac-
counts for distributions induced by noisy DNNs with various nonlinearities. Specifically, it captures
the following important cases:
1.	Distributions with bounded support (corresponding to noisy DNN with bounded activisions).
2.	Discrete distributions over a finite set, which is a special case of bounded support.
3.	Distributions ofthe random variable Sg = f (T^-I) in a noisy ReLU DNN, so long as the
input X to the network is itself subgaussian. To see this recall that linear combinations
of independent subgaussian random variables is also subgaussian. Furthermore, for any
(scalar) random variable A, we have that IReLU(A) ∣ = ∣ max{0, A} ∣ ≤ |A|, almost surely.
Now, since each layer in a noisy ReLU DNN is nothing but a coordinate-wise ReLU applied
to a linear transformation of the previous layer plus a Gaussian noise, one may upper bound
(E∣S (i)∣p)P ,fora dg-dimensional hidden layer Sg and i ∈ [dg], as in Item (2) of Definition
1, provided that the input X is coordinate-wise subgaussian. The constant K2 will depend
on the network’s weights and biases, the depth of the hidden layer, the subgaussian norm
ofthe input ∣∣X ∣∣ψ2 and the noise variance. This input subgaussianity assumption is, in
particular, satisfied by the distribution of X COnSidered herein, i.e., by X 〜Unif (X).
10.1 Sample Complexity is Exponential in Dimension
We start with two converse claims establishing that the sample complexity is exponential in d. The
first claim states that there exists a class of distributions P, for which the estimation of h(P * 夕)
cannot be done with fewer than exponentially many samples in d, when d is sUfficiently large.
Theorem 2 (Asymptotic (in d) Exponential Sample-Complexity) For any β > 0 there exist
γ (β) > 0 (monotonically decreasing in β) and a class of distributions in Fd, such that for any
d sufficiently large and η > 0 sufficiently small, the sample complexity of estimating h(p * 夕)within
(
)
an additive gap η > 0 over that class gr^ws as Ω
2γ(β)d
β > 0. In particular, nd (η, β)
Ω (	2Y(β)d ʌ
l(η+δβ1d)dJ
(η+δβ1d)d
in this regime.
, where limd→∞ δβ(1,d)
= 0, for all
7
Under review as a conference paper at ICLR 2019
The fact that the exponent γ(β) is monotonically decreasing in β suggests that larger values of β
are favorable for estimation. Theorem 2 shows that an exponential sample complexity is inevitable
when d is large. As a complementary result, the next theorem gives a sample complexity lower bound
valid in any dimension but only for small enough noise variances. Nonetheless, the result is valid for
orders of β considered in this work.
Theorem 3 (Asymptotic (in β) Exponential Sample-Complexity) Fix d ≥ 1. There exists a class
of distributions in Fd such that for any β, η > 0 sufficiently small, the sample complexity of
estimating h(S + Z) within an additive gap η > 0 over that class grows as Ω
(
d
)
22一2d2∏- ∖ where
η+δβ(2,)d d
limβ→o δ(2d = 0, for all d ≥ 1. In particular, nd(η, β) = Ω
(η+δβ2d)d
in this regime.
Remark 2 We state Theorem 3 asymptotically in β for the sake of simplicity, but for any d it is
possible to follow the constants through the proof to determine a value c such that Theorem 3 holds
for all β < c. For example for d = 1, a careful analysis gives that Theorem 3 holds for all β < 0.08,
which is satisfied by most of the experiments run in this paper. This threshold on β changes very
slowly with increasing d due to the rapid decay of the PDF of the normal distribution.
10.2 Minimax Risk Convergence Rate of the Sample Propagation Estimator
We next focus on analyzing the performance of the SP estimator. For any fixed Sn = sn , denote the
empirical PMF associated with Sn by Psn. The SP estimator of h(T) = h(ps * 夕)is
ʌ	,	. Λ	.
hSP(S ) = h(psn * 2).
(11)
ʌ
The estimator hSP(sn) also depends on β, but we omit this from our notation. The following theorem
shows that the expected absolute error of hSP decays like O (pol√gS)) for all dimensions d. We
provide explicit constants (in terms of β and d), which present an exponential dependence on the
dimension, in accordance to the results of Theorems 2 and 3.
Theorem 4 (SP Estimator Absolute-Error Risk for Bounded Support) Fix β > 0, d ≥ 1 and
any e > 0. The absolute-error risk ofthe SP estimator (11) over the class Fd, for all n sufficiently
large, is bounded as
sup ESnIh(P * φ) — hsp(Sn)
P∈Fd
≤ 2(4∏β2) 4 log
nn (2 + 2β√(2 + e)logn) ʌ (	_ʌ 2 1
[--E-- J (2 + 2β kE) √
+
2cβ,d d(I + β2)
β
8d(d +2β4 + dβ4八 2
+ β )n (12)
where c§,d = d log(2∏β2) + β2. In particular,
PsuF ESn ∣h(P * Ψ) - hsp(Sn)∣ = Oβ,d (Polyog(n)),	(13)
and the right-hand sides (RHSs) of (12) and (13) are, respectively, explicit and implicit upper bounds
on the minimax absolute-error risk R》(n, β).
Remark 3 (Comparison to General-Purpose Estimators) Note that one could always sample 夕
and add up these noise samples to Sn to obtain a sample set from P * 夕.These samples can be
used to get a proxy of h(P * 夕)via a kNN- or a KDE-based differential entropy estimator. However,
P * 夕 violated the boundedness awayfrom zero assumption that most ofthe convergence rate results
in the literature rely on (Levit, 1978; Hall, 1984; Joe, 1989; Hall & Morton, 1993; Tsybakov &
der MeUIen, 1996; Haje & Golubev, 2009; K et al., 2012; Singh & Poczos, 2016; Kandasamy
8
Under review as a conference paper at ICLR 2019
et al., 2015). The only result we are aware of that analyses a differential entropy estimator (namely,
the kNN-based estimator from (A. Kraskov & Grassberger, 2004)) without assuming the density is
bounded from below (Jiao et al., 2017) relies on the density being supported inside [0, 1]d, satisfying
periodic boundary conditions and having a Holder smoothness parameter S ∈ (0,2]. The Convolved
density P * φ satisfies neither ofthese three conditions. Furthermore, because the SP estimator is
constructed to exploit the particular structure of our estimation setup it achieves a fast convergence
rateof (PoL√g(ni)
The risk associated with unstructured differential entropy estimators typically
converges as the slower O
to general-purpose estimation.
(n- βs+d )
This highlights the advantage of ad-hoc estimation as opposed
Theorem 1 provides convergence rates when estimating differential entropy (or mutual information)
over DNNs with bounded activation functions, such as tanh or sigmoid. To account for networks with
unbounded nonlinearities, such as ReLU networks, the following theorem gives a more general result
of estimation over the nonparametric class Fd(,SKG) of d-dimensional distributions with subgaussian
marginals.
Theorem 5 (SP Estimator Absolute-Error Risk for Subgaussians) Fix β > 0 and d ≥ 1. The
absolute-error risk of the SP estimator (11) over the class Fd(,SKG) , for all n sufficiently large, is
bounded as
sup ESn Ih(P * φ) — hsP(Sn)
P∈Fd(S,KG)
≤ log
(8(K + β)2 )八(2(K + β)2
I (e — 1)∏β2 J J ∖ (e — 1)∏β2
log n
1
d
4
( cβ,d + 2dK 2)2 + 2 (%d + 2dK ”( β2d + 2dK K2
8(l6d2K4 + β4d(2 + d)) ∖ ed
+ β 厂
(14)
where Cβ,d = d log(2∏β2). In particular,
+
sup ESn
P∈Fd(S,KG)
|h(P ")— hsp(S")∣ = Oβ,d (P0√g(n2)，
(15)
and the RHSs of (14) and (15) are, respectively, explicit and implicit upper bounds on the minimax
absolute-error risk RdK (n, β)∙
As mentioned in Remark 1, the class Fd(,SKG) is rather general, and, in particular, includes Fd whenever
K ≥ 1. This means that Theorem 5 also provides an upper bound on the minimax risk under the
setup of Theorem 1. Nonetheless, we chose to separately state Theorem 1 since the derivation under
the bounded support assumption enables extracting slightly better constants (which is important for
our applications - see Section 5). We do highlight, however, that the expressions from (12) and (14)
with K = 1 not only have the same convergence rates, but their constants are also very close.
Remark 4 (Near Minimax Rate-Optimality) A convergence rate faster than √ cannot be at-
tained for parameter estimation under the absolute-error loss. This follows from, e.g., Proposition
1 of (Chen, 1997), which establishes this convergence rate as a lower bound for the parametric
estimation problem given n i.i.d. samples. Consequently, the convergence rate
of Oσ,d (p0√g(n))
established in Theorems 1 and 5 for the SP estimator is near minimax rate-optimal (i.e., up to
logarithmic factors).
Remark 5 (Mutual Information Estimation) Denoting the upper bound on the estimation error
from Theorem 1 or 5 by ∆n (β, d), we see that the error of the mutual information estimator from (2)
is bounded as by 2∆n (β, d), which vanishes as n → ∞.
9
Under review as a conference paper at ICLR 2019
10.2. 1 Sample Propagation Estimator Bias
The results of the previous subsection are of minimax flavor. That is, they state worst-case convergence
rates of the SP estimation over a certain nonparametric class of distributions. In practice, the true
distribution may very well not be one that attains these worst-case rates, and convergence may be
ʌ
faster. However, while variance of hSP(Sn) can be empirically evaluated using bootstrapping, there
ʌ
is no empirical test for the bias. Even if multiple estimations of h(P * 夕)via hsp(Sn) consistently
produce similar values, this does not necessarily suggest that these values are close to the true
h(P * 夕).To have a guideline to the least number of samples needed to avoid biased estimation, we
present the following lower bound on SuPP 三尸& ESn lh(P * 夕)-hsp(Sn)l.
Theorem 6 (SP Estimator Bias Lower Bound) Fix d ≥ 1 and β > 0, and let E ∈
1 - (1 - 2Q (2β))
1 , where Q is the Q-function.3 Set k* =
__________1_________
βQ-1( 1 (l-(1-e)d ))
, where
QT is the inverse of the Q-function. By the choice of e, clearly k* ≥ 2, and the bias of the SP
estimator over the class Fd is bounded as
ʌ
SuP h(P * φ) — ESn hsp(Sn)
P ∈Fd
k kd(1-e)∖
≥ log (-*∙^ )-Hb9.
(16)
Consequently, the bias cannot be less than a given δ > 0 so long as n ≤ -d(1-e) ∙ e-(δ+Hb(e)).
Since Hb(E) shrinks with G for sufficiently small E values the lower bound from (16) essentially shows
that the SP estimator will not have negligible bias unless n > -d(1-e) is satisfied. The condition
(1 - 2Q (2))
is non-restrictive in any relevant regime of β and d. For instance, for
typical β values we work with - around 0.1 - this lower bound is at most 0.0057 for all dimensions
up to at least d = 104. Setting, e.g., E = 0.01 (for which Hb(0.01) ≈ 0.056), the corresponding
-* equals 3 for d ≤ 11 and 2 for 12 ≤ d ≤ 104. Thus, with these parameters, the number of
estimation samples n should be at least 20.99d, for any conceivably relevant dimension, in order to
have negligible bias.
10.3 Computing the Sample Propagation Estimator
Evaluating the mutual information estimator from (2) requires computing the differential entropy of a
Gaussian mixture. Although it cannot be computed in closed form, this section presents a method for
approximate computation via MC integration (Robert, 2004). To simplify the presentation, we present
the method for an arbitrary Gaussian mixture without referring to the notation of the estimation setup.
Let g(t) = n £记网 夕(t 一 μi) be a d-dimensional, n-mode Gaussian mixture, with {μi}i∈[n] ⊂ Rd
and φ as the PDF of N(0, β2Id). Let C 〜Unif{μi}i∈[n] be independent of Z 〜 夕 and note that
V = C + Z 〜g.
We use Monte Carlo (MC) integration (Robert, 2004) to compute the h(g). First note that
h(g) = -Elog g(V) = 一n E E[ log g(μi + Z)IC = μj = 一n E E log g(μi + Z), (17)
i∈[n]	i∈[n]
where the last step follows by the independence of Z and C. Let {Z(i)} i∈[n] be n X nMc i.i.d.
j∈[nMC]
samples from 夕.For each i ∈ [n], we estimate the i-th summand on the RHS of (17) by
=n⅛ ,E ,log g(μi+ Zji)),
j∈[nMC]
(18a)
3The Q-function is defined as Q(X) = √2∏ f∞ e-⅛"dt.
E > 1 一
10
Under review as a conference paper at ICLR 2019
which produces
hMC ʌ IE IMC	(18b)
n i∈[n]
as our estimate of h(g). Define the mean squared error (MSE) of hMc as
MSE 々Me) ʌ E 0mc - h(g))2 .	(19)
We have the following bounds on the MSE for tanh and ReLU networks.
Theorem 7 (MSE Bounds for MC Estimator)
(i)	Assume C ∈ [-1, 1]d almost surely (i.e., tanh network), then
MSE (hMe) ≤	1	2d(2+ 沁.	(20)
V n n ∙ nMe P2
(ii)	Assume MC = EllC∣∣2 < ∞ (e.g., ReLU network with bounded2ndmoments), then
MSE (hMc) ≤
1	9dβ2 + 8(2 + P√d)Mc + 3(11Pvd + 1)√MC
(21)
n ∙ nMC	β2
The bounds on the MSE scale only linearly with the dimension d, making σ2 in the denominator
often the dominating factor experimentally.
Remark 6 (Comparison to Generic Entropy Estimation) We briefly present empirical results il-
lustrating the convergence of the SP estimator and comparing it to two current state-of-the-art
methods: the KDE-based estimator of (Kandasamy et al., 2015) and the kNN-based estimator often
known as the Kozachenko-Leonenko (KL) nearest neighbor estimator (Kozachenko & Leonenko,
1987; Jiao et al., 2017). In this example, the distribution P of S is set to be a mixture of Gaus-
sians truncated to have support in [-1, 1]d. Before truncation, the mixture consists of 2d Gaussian
components with means at the 2d corners of [—1,1]d. The entropy of P * φ, i.e., h(S + Z), where
Z 〜N(0, σ2Id), is estimated and various values of σ are examined.
Fig. 14 shows the estimation error results as a function of the number of samples n for dimensions
d = 5 and d = 10. The kernel width for the KDE estimate was chosen via cross-validation, varying
ʌ
with both d and n; the kNN estimator and hSP(Sn) require no tuning parameters. We found that
the KDE estimate is highly sensitive to the choice of kernel width, the curves shown correspond to
optimized values and are highly unstable to any change in kernel width. Note that both the kNN and
the KDE estimators converge slowly, at a rate that degrades with increased d. This rate is worse than
ʌ
that of hSP, which also lower bounds the true entropy (as according to our theory - see (Anonymized,
2018, Equation (60))).
References
H. Stogbauer A. KraSkov and P Grassberger. Estimating mutual information. Phys. rev. E, 69(6):
066138, June 2004.
Anonymized. Estimating differential entropy under Gaussian convolutions. Submitted to IEEE
Transactions on Information Theory, 2018.
J. Chen. A general lower bound of minimax risk for absolute-error loss. Canadian Journal of
Statistics, 25(4):545-558, Dec. 1997.
M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving
robustness to adversarial examples. In Proceedings of the International Conference on Machine
Learning (ICML), 2017.
H. F. El Haje and Y. Golubev. On entropy estimation by m-spacing method. Journal of Mathematical
Sciences, 163(3):290-309, Dec. 2009.
11
Under review as a conference paper at ICLR 2019
Figure 14: Estimation results for the SP estimator compared to state-of-the-art kNN-based and
KDE-based differential entropy estimators. The differential entropy of S + Z is estimated, where S
is a truncated d-dimensional mixture of 2d Gaussians and Z 〜N(0, σ2Id). Results are shown as a
function of n, for d = 5, 10 and σ = 0.1, 0.5. The SP estimator presents faster convergence rates,
improved stability and better scalability with dimension compared to the two competing methods.
P. Hall. Limit theorems for sums of general functions of m-spacings. Mathematical Proceedings of
the Cambridge Philosophical Society, 96(3):517-532, Nov. 1984.
P. Hall and S. C. Morton. On the estimation of entropy. Annals of the Institute of Statistical
Mathematics, 45(1):69-88, Mar. 1993.
J.	Jiao, W. Gao, and Y. Han. The nearest neighbor information estimator is adaptively near minimax
rate-optimal. arXiv:1711.08824 [stat.ML], 2017.
H. Joe. Estimation of entropy and other functionals of a multivariate density. Annals of the Institute
of Statistical Mathematics, 41(4):683-697, Dec. 1989.
Sricharan K, R. Raich, and A. O. Hero. Estimation of nonlinear functionals of densities with
confidence. IEEE Trans. Inf. Theory, 58(7):4135-4159, Jul. 2012.
K.	Kandasamy, A. Krishnamurthy, B. Poczos, L. Wasserman, and J. M. Robins. Nonparametric
von Mises estimators for entropies, divergences and mutual informations. In Advances in Neural
Information Processing Systems (NIPS), pp. 397-405, 2015.
L.	F. Kozachenko and N. N. Leonenko. Sample estimate of the entropy of a random vector. Problemy
Peredachi Informatsii, 23(2):9-16, 1987.
B.	Y. Levit. Asymptotically efficient estimation of nonlinear functionals. Problemy Peredachi
Informatsii, 14(3):65-72, 1978.
A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017.
Christian P Robert. Monte Carlo Methods. Wiley Online Library, 2004.
A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky, B. D. Tracey, and D. D. Cox. On the
information bottleneck theory of deep learning. In Proceedings of the International Conference on
Learning Representations (ICLR), 2018.
12
Under review as a conference paper at ICLR 2019
S. Singh and B. P6czos. Finite-sample analysis of fixed-k nearest neighbor density functional
estimators. In Advances in Neural Information Processing Systems, pp. 1217-1225, 2016.
A. B. Tsybakov and E. C. Van der Meulen. Root-n consistent estimators of entropy for densities with
unbounded support. Scandinavian Journal of Statistics, pp. 75-83, Mar. 1996.
13