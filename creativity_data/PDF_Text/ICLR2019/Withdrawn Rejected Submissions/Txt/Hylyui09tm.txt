Under review as a conference paper at ICLR 2019
EMI: Exploration with Mutual Information
Maximizing State and Action Embeddings
Anonymous authors
Paper under double-blind review
Ab stract
Policy optimization struggles when the reward feedback signal is very sparse and
essentially becomes a random search algorithm until the agent accidentally stum-
bles upon a rewarding or the goal state. Recent works utilize intrinsic motivation
to guide the exploration via generative models, predictive forward models, or more
ad-hoc measures of surprise. We propose EMI, which is an exploration method
that constructs embedding representation of states and actions that does not rely
on generative decoding of the full observation but extracts predictive signals that
can be used to guide exploration based on forward prediction in the representation
space. Our experiments show the state of the art performance on challenging lo-
comotion task with continuous control and on image-based exploration tasks with
discrete actions on Atari.
1	Introduction
The central task in reinforcement learning is to learn policies that would maximize the total reward
received from interacting with the unknown environment. Although recent methods have demon-
strated to solve a range of complex tasks (Mnih et al., 2015; Schulman et al., 2015; 2017), the
success of these methods, however, hinges on whether the agent constantly receives the intermedi-
ate reward feedback or not. In case of challenging environments with sparse reward signals, these
methods struggle to obtain meaningful policies unless the agent luckily stumbles into the rewarding
or predefined goal states.
To this end, prior works on exploration generally utilize some kind of intrinsic motivation mech-
anism to provide a measure of surprise. These measures can be based on density estimation via
generative models (Bellemare et al., 2016; Fu et al., 2017; Oh et al., 2015), predictive forward mod-
els (Stadie et al., 2015; Houthooft et al., 2016), or more ad-hoc measures that aim to approximate
surprise (Pathak et al., 2017). Methods based on predictive forward models and generative models
must model the distribution over state observations, which can make them difficult to scale to com-
plex, high-dimensional observation spaces, while models that eschew direct forward predictive or
density estimation rely on heuristic measures of surprise that may not transfer effectively to a wide
range of tasks.
Our aim in this work is to devise a method for exploration that does not require a direct generation
of high-dimensional state observations, while still retaining the benefits of being able to measure
surprise based on the forward prediction. If exploration is performed by seeking out states that
maximize surprise, the problem, in essence, is in measuring surprise, which requires a representation
where functionally similar states are close together, and functionally distinct states are far apart.
In this paper, we propose to learn compact representations for both the states (φ) and actions (ψ)
simultaneously satisfying the following criteria: First, given the representations of state and the cor-
responding next state, the uncertainty of the representation of the corresponding action should be
minimal. Second, given the representations of the state and the corresponding action, the uncer-
tainty of the representation of the corresponding next state should also be minimal. Third, the action
embedding representation (ψ) should seamlessly support both the continuous and discrete actions.
Finally, we impose the linear dynamics model in the representation space which can also explain
the rare irreducible error under the dynamics model. Given the representation, we guide the explo-
ration by measuring surprise based on forward prediction and relative increase in diversity in the
embedding representation space. Figure 1 illustrates an example visualization of our learned state
embedding representations (φ) and sample trajectories in the representation space in Montezuma’s
Revenge.
We present two main technical contributions that make this into a practical exploration method.
First, we describe how compact state and action representations can be constructed via Donsker &
Varadhan (1983) estimation of mutual information without relying on generative decoding of full
1
Under review as a conference paper at ICLR 2019
observations. Second, we show that imposing linear topology on the learned embedding representa-
tion space (such that the transitions are linear), thereby offloading most of the modeling burden onto
the embedding function itself, provides an essential informative measure of surprise when visiting
novel states.
For the experiments, we show that we can
use our representations on a range of com-
plex image-based tasks and robotic locomotion
tasks with continuous actions. We report sig-
nificantly improved results compared to recent
intrinsic motivation based exploration methods
(Fu et al., 2017; Pathak et al., 2017) on several
challenging Atari tasks and robotic locomotion
tasks with sparse rewards.
Figure 1: Visualization of sample trajectories in
our learned embedding space.
2	Related works
Our work is related to the following strands of
active research:
Unsupervised representation learning via mutual information estimation Recent literature on
unsupervised representation learning generally focus on extracting latent representation maximizing
approximate lower bound on the mutual information between the code and the data. In the context of
generative adversarial networks (Goodfellow et al., 2014), Chen et al. (2016); Belghazi et al. (2018)
aims at maximizing the approximation of mutual information between the latent code and the raw
data. Belghazi et al. (2018) estimates the mutual information with neural network via Donsker &
Varadhan (1983) estimation to learn better generative model. Hjelm et al. (2018) builds on the idea
and trains a decoder-free encoding representation maximizing the mutual information between the
input image and the representation. Furthermore, the method uses f -divergence (Nowozin et al.,
2016) estimation of Jensen-Shannon divergence rather than the KL divergence to estimate the mu-
tual information for better numerical stability. Oord et al. (2018) estimates mutual information via
autoregressive model and makes predictions on local patches in an image. Thomas et al. (2017) aims
to learn the representations that maximize the causal relationship between the distributed policies
and the representation of changes in the state.
Exploration with intrinsic motivation Prior works on exploration mostly employ intrinsic motiva-
tion to estimate the measure of novelty or surprisal to guide the exploration. Bellemare et al. (2016)
utilize density estimation via CTS (Bellemare et al., 2014) generative model and derive pseudo-
counts as the intrinsic motivation. Fu et al. (2017) avoids building explicit density models by train-
ing K-exemplar models that distinguish a state from all other observed states. Some methods train
predictive forward models (Stadie et al., 2015; Houthooft et al., 2016; Oh et al., 2015) and estimate
the prediction error as the intrinsic motivation. Oh et al. (2015) employs generative decoding of
the full observation via recursive autoencoders and thus can be challenging to scale for high dimen-
sional observations. VIME (Houthooft et al. (2016)) approximates the environment dynamics, uses
the information gain of the learned dynamics model as intrinsic rewards, and showed encouraging
results on robotic locomotion problems. However, the method needs to update the dynamics model
per each observation and is unlikely to be scalable for complex tasks with high dimensional states
such as Atari games.
Other approaches utilize more ad-hoc measures (Pathak et al., 2017; Tang et al., 2017) that aim to
approximate surprise. ICM (Pathak et al. (2017)) transforms the high dimensional states to feature
space and imposes cross entropy and Euclidean loss so the action and the feature of the next state are
predictable. However, ICM does not utilize mutual information like VIME to directly measure the
uncertainty and is limited to discrete actions. Our method (EMI) is also reminiscent of (Kohonen
& Somervuo, 1998) in a sense that we seek to construct a decoder-free latent space from the high
dimensional observation data with a topology in the latent space. In contrast to the prior works
on exploration, we seek to construct the representation under linear topology and does not require
decoding the full observation but seek to encode the essential predictive signal that can be used for
guiding the exploration.
2
Under review as a conference paper at ICLR 2019
3	Preliminaries
We consider a Markov decision process defined by the tuple (S, A, P, r, γ), where S is the set of
states, A is the set of actions, P : S × A × S → R+ is the environment transition distribution,
r : S → R is the reward function, and γ ∈ (0, 1) is the discount factor. Let π denote a stochastic
policy over actions given states. Denote P0 : S → R+ as the distribution of initial state s0 . The
discounted sum of expected rewards under the policy π is defined by
η(π) = Eτ X γtr(st) ,	(1)
t=0
where T = (so, a。，...，aτ-ι, ST) denotes the trajectory, so 〜P0(s0), at 〜 π(at | st), and st+ι 〜
P (st+1 | st, at). The objective in policy based reinforcement learning is to search over the space of
parameterized policies (i.e. neural network) πθ(a | s) in order to maximize η(πθ).
Also, denote PπSAS0 as the joint probability distribution of singleton experience tuples (s, a, s0) start-
ing from so 〜 Po (so) and following the policy ∏. Furthermore, define PA = (乂§0 dP∏Aso as the
marginal distribution of actions, PπSS0 = A dPπSAS0 as the marginal distribution of states and the
corresponding next states, Pπs0 = S ×A dPπsAs0 as the marginal distribution of the next states, and
PπsA = S 0 dPπsAs0 as the marginal distribution of states and the actions following the policy π.
4 Methods
Our goal is to construct the embedding representation of the observation and action (discrete or
continuous) for complex dynamical systems that does not rely on generative decoding of the full
observation, but still provides a useful predictive signal that can be used for exploration. This re-
quires a representation where functionally similar states are close together, and functionally distinct
states are far apart. We approach this objective from maximizing mutual information under several
criteria.
4.1	Mutual information maximizing state and action embedding
REPRESENTATIONS
We first introduce the embedding function of states φα : S → Rd and actions ψβ : A → Rd with
parameters α and β (i.e. neural networks) respectively. We seek to learn the embedding function of
states (φα) and actions (ψβ) satisfying the following two criteria:
1.	Given the embedding representation of states and the actions [φα(s); ψβ(a)], the uncer-
tainty of the embedding representation of the corresponding next states φα (s0) should be
minimal and vice versa.
2.	Given the embedding representation of states and the corresponding next states
[φα(s); φα(s0)], the uncertainty of the embedding representation of the corresponding ac-
tions ψβ (a) should also be minimal and vice versa.
Intuitively, the first criterion translates to maximizing the mutual information between
[φα (s); ψβ (a)], and φα (s0) which we define as Is(α, β) in Equation (2). And the second crite-
rion translates to maximizing the mutual information between [φα(s); φα(s0)] and ψβ (a) defined as
IA(α, β) in Equation (3).
maximize IS(α,β):= I([φɑ(s); Ψβ(a)]; φα(s0)) = DKL (PSASOk PSA 0 PS，)	(2)
α,β
maximize Ia(α, β) := I([φɑ(s); φα(s0)]; Ψβ(a)) = DKL (PSASOk PSs，0 PA)	(3)
α,β
Mutual information is not bounded from above and maximizing mutual information is notoriously
difficult to compute in high dimensional settings. Motivated by Hjelm et al. (2018); Belghazi et al.
(2018), we compute Donsker & Varadhan (1983) lower bound of mutual information. Concretely,
Donsker-Varadhan representation is a tight estimator for the mutual information of two random
variables X and Z, derived as in Equation (4).
I(	X； Z) = DKL(PXZ k PX 0 PZ) ≥ sup EPXZ‰(x,z) — logEPX那Z expTω(x,z),	(4)
ω∈Ω
where Tω : X × Z → R is a differentiable transform with parameter ω. Furthermore, for better
numerical stability, we utilize a different measure between the joint and marginals than the KL-
3
Under review as a conference paper at ICLR 2019
几sS0),W(αJ0(蜀+Lm∕G)
G 4 -ISJSD)
T3s(Φ(sjW(%),Φ(sj)
Figure 2: Computational architecture for estimating IS(JSD) and IA(JSD) for image-based observations.
divergence. In particular, we employ Jensen-Shannon divergence (JSD) (Hjelm et al., 2018) which
is bounded both from below and above by 0 and log(4) 1.
Theorem 1. I(JSD)(X； Z) ≥ suPω∈Ω Epxz [-sp (-Tω(x,z))] - EPX0Pz [sp (TL(x,z))] + log(4)
Proof. I(JSD)(X； Z)= DJSD(PXZ k PX ③ PZ)
≥ sup Epχz [Sω (x,z)] - EPXOPz [JSD" (Sω (x,z))]
ω∈Ω
= supEPXz [-sp(-Tω(x,z))] -EPXOPz [sp(Tω(x,z))] + log(4),	(5)
ω∈Ω
where the inequality in the second line holds from the definition of f -divergence (Nowozin et al.,
2016). In the third line, we substituted Sω(x, z) = log(2) - log(1 + exp(-Tω(x, z))) and Fenchel
conjugate of Jensen-Shannon divergence, JSD*(t) = - log(2 - exp(t)).	口
From Theorem 1, we have,
maximize IS(JSD)(α, β) ≥ maximize sup EPπ 0 [-sp (-TωS (φα(s), ψβ (a), φα (s0)))]	(6)
α,β	α,β	ωs ∈Ωs	SASO
-EPSA OPgo [sp (Tωs (φα(s),Ψβ (a),φα(s0)))] +lθg4,
maximize IA(JSD)(α,β) ≥ maximize sup EPπ 0 [-sp(-TωA(φα(s),ψβ(a), φα(s0)))]	(7)
α,β	α,β	la∈Ωa	SAS
-EPSSO OPA [sp (TLA (φα (S), ψβ (G), φα(SO)))]+ log 4,
where sp(z) = log(1 + exp z). The expectations in Equation (6) and Equation (7) are approximated
using the empirical samples trajectories T. Note, the samples s0 〜PS，and a 〜PA from the
marginals are obtained by dropping (S, a) and (S, S0) in samples (S, a, S0) and (S, Ga, S0) from PπSASO.
Figure 2 illustrates the computational architecture for estimating the lower bounds on IS and IA.
4.2 Embedding linear dynamics model under sparse noise
Since the embedding representation space is learned, it is natural to impose a topology on it (Ko-
honen, 1983). In EMI, we impose a simple and convenient topology where transitions are linear
since this spares us from having to also represent a complex dynamical model. This allows us to
offload most of the modeling burden onto the embedding function itself, which in turn provides us
with a useful and informative measure of surprise when visiting novel states. Once the embedding
representations are learned, this linear dynamics model allows us to measure surprise in terms of
the residual error under the model or measure diversity in terms of the similarity in the embedding
space. Section 5 discusses the intrinsic reward computation procedure in more detail.
1In Nowozin et al. (2016), the authors actually derived the lower bound of DJSD = DKL (P ||M) +
D(Q∣∣M), instead of DJSD = 1 (DKL(P||M)+ Dkl(Q∖∖M)), where M = 2(P + Q).
4
Under review as a conference paper at ICLR 2019
Concretely, we seek to learn the representation of states φ(s) and the actions ψ(a) such that the
representation of the corresponding next state φ(s0) follow linear dynamics i.e. φ(s0) = φ(s) +
ψ(a). Intuitively, we would like the nonlinear aspects of the dynamics to be offloaded to the neural
networks φ(∙),ψ(∙) so that in the Rd embedding space, the dynamics become linear. Regardless
of the expressivity of the neural networks, however, there always exists irreducible error under the
linear dynamic model. For example, the state transition which leads the agent from one room to
another in Atari environments (i.e. Venture, Montezuma’s revenge, etc.) or the transition leading
the agent in the same position under certain actions (i.e. Agent bumping into a wall when navigating
a maze environment) would be extremely challenging to explain under the linear dynamics model.
To this end, we introduce the error model Sγ : S × A → Rd, which is another neural network taking
the state and action as input, estimating the irreducible error under the linear model. Motivated by
the work of CandeS et al. (2011), We seek to minimize for the sparsity of the term so that the error
term contributes only on rare unexplainable occasions. Equation (8) shows the embedding learning
problem under linear dynamics with sparse errors.
minimize kSγ k2,0
ɑ,β,γ 、_{^}
error sparsity
subject to Φ0α = Φα + Ψβ + Sγ ,
V-------------{z--------------}
embedding linear dynamics
(8)
where we used the matrix notation for compactness. Φα, Ψβ, Sγ denotes the matrices of respective
embedding representations stacked columns wise. Relaxing the `0 norm with `1 norm, Equation (9)
shows our final learning objective.
minimize kΦ0α - (Φα+Ψβ +Sγ) k2,1 +λsparsitykSγk2,1 +λunitKLDKL(Pπψ k N (0, I))	(9)
α,β,γ
+ λinfo^Jnf JPSASOSP (-Tωs (Φɑ(s),Ψβ (a),φα(s'))) + EPSAgPSO SP ^S (Φɑ(s),Ψβ (a),Φα(S0)))
+ λinfo ω inΩA EPSASO sP (-久人(Φα(s),Ψβ (a),φα(s0))) + Epg,,就玄 SP (久人(φα(s),ψβ (°),φα(s')))
λinfo , λsparsity , λunitKL are hyper-parameters which control the relative contributions of the linear dy-
namics error and the sParsity. In Practice, we found the oPtimization Process to be more stable when
we further regularize the distribution of action embedding rePresentation to follow a Predefined Prior
distribution. Concretely, we regularize the action embedding distribution to follow a standard nor-
mal distribution via DKL(Pπψ k N(0, I)) similar to VAEs Kingma & Welling (2013). Intuitively, this
has the effect of grounding the distribution of action embedding rePresentation (and consequently
the state embedding rePresentation) across different iterations of the learning Process. 2
5	Intrinsic reward augmentation
We consider two different formulations of comPuting the intrinsic reward. First, we consider a
relative difference in the novelty of state rePresentations based on the distance in the embedding
rePresentation sPace similar to Oh et al. (2015) as shown in Equation (10). The relative difference
makes sure the intrinsic reward diminishes to zero (Ng et al., 1999) once the agent has sufficiently
exPlored the state sPace. Also, we consider a formulation based on the Prediction error under the
linear dynamics model as shown in Equation (11). This formulation incorPorates the sParse error
term and makes sure we differentiate the irreducible error that does not contribute as the novelty.
rd(st ,at ,st)= g(st) — g(st), where g(s) = 1 X exp (-kφ(S) - φ (Si)k )	(10)
n	2σ2
i=1
re(st, at, s0t) = kφ(st) + ψ(at) + S(st, at) - φ(s0t)k2	(11)
Note the relative diversity term should be comPuted after the rePresentations are uPdated based on
the samPles from the latest trajectories while the Prediction error term should be comPuted before
the uPdate. Algorithm 1 shows the comPlete learning Procedure in detail.
2Note, regularizing the distribution of state embeddings instead renders the oPtimization Process much more
unstable. This is because the distribution of states are much more likely to be skewed than the distribution of
actions, esPecially during the initial stage of oPtimization, so the Gaussian aPProximation becomes much less
accurate in contrast to the distribution of actions.
5
Under review as a conference paper at ICLR 2019
Algorithm 1 Exploration with mutual information state and action embeddings (EMI)
initialize α, β, γ, ωA , ωS
for i = 1, . . . , MAXITER do
Collect samples {(st , at , s0t)}tn=1 with policy πθ
Compute residual error intrinsic rewards {re(st, at, s0t)}tn=1 following Equation (11)
for j = 1, . . . , OPTITER do
for k = 1,..., bmnC do
Sample a minibatch {(stl ,atl,s0tl)}lm=1
b b	b b b 号 C	f /	/	、	、)b mm C
Compute {tLa Q(Stj,ψ(atJ,φ(stι, }l=1 and ∖t3a (φ(Stj,ψ (M+bmC),φ(Stl)) }l=1
to derive the lower bound on IA(JSD) (α, β) in Equation (7)
b b	b	b b mm C (	/	b	∖ ∖)b m2 C
Compute {Tωs Q(Stl)	ψ(atl ), φ(Stl))	}l=1	and {Tωs (φ(Stl),	ψ(atl ), φ 卜 tl+b m C) ) }l=1
to derive the lower bound on IS(JSD) (α, β) in Equation (6)
Update α, β, γ, ωA , ωS using the Adam (Kingma & Ba, 2015) update rule to minimize
Equation (9)
end for
end for
Compute diversity intrinsic rewards {rd(St, at, S0t)}tn=1 following Equation (10)
Augment the intrinsic rewards and update the policy network πθ using any RL method
end for
6 Experiments
We compare the experimental performance of EMI to recent prior works on both of the low-
dimensional locomotion tasks with continuous control from rllab benchmark (Duan et al., 2016)
and the complex vision-based tasks with discrete control from the Arcade Learning Environment
(Bellemare et al., 2013). For the locomotion tasks, we chose SwimmerGather and SparseHalfChee-
tah environments for direct comparison against the prior work ofFu et al. (2017). SwimmerGather is
a hierarchical task where a two-link robot needs to reach green pellets, which give positive rewards,
instead of red pellets, which give negative rewards. SparseHalfCheetah is a challenging locomotion
task where a cheetah-like robot does not receive any rewards until it moves 5 units in one direction.
For vision-based tasks, we selected Freeway, Frostbite, Venture, Montezuma’s Revenge, Gravitar,
and Solaris for comparison with recent prior works (Pathak et al., 2017; Fu et al., 2017). These
six Atari environments feature very sparse reward feedback and often contain many moving dis-
tractor objects which can be challenging for the methods that rely on explicit decoding of the full
observations (Oh et al., 2015).
6.1 Implementation Details
We use TRPO (Schulman et al., 2015) for policy optimization because of its capability to support
both the discrete and continuous actions and its robustness with respect to the hyperparameters. In
the locomotion experiments, we use a 2-layer fully connected neural network as the policy network.
In the Atari experiments, we use a 2-layer convolutional neural network followed by a single layer
fully connected neural network. We convert the 84 x 84 input RGB frames to grayscale images
and resize them to 52 x 52 images following the practice in Tang et al. (2017). The embedding
dimensionality is set to d = 2 in all of the environments except for Gravitar and Solaris where
we set d = 8 due to their complex environment dynamics. We use Adam (Kingma & Ba, 2015)
optimizer to train embedding networks. Please refer to Appendix A.1 for more details.
6.2 Locomotion tasks with continuous control
We compare EMI with TRPO (Schulman et al., 2015), EX2 (Fu et al., 2017), and ICM (Pathak
et al., 2017) on two challenging locomotion environments: SwimmerGather and SparseHalfCheetah.
Figure 4 shows that EMI outperforms the baseline methods on both tasks. Figure 3b visualizes the
scatter plot of the learned state embeddings and an example trajectory for the SparseHalfCheetah
experiment. The figure shows that the learned representation successfully preserves the similarity in
observation space. Please refer to Appendix A.3 for further experiments including ablation study.
6
Under review as a conference paper at ICLR 2019
(a) Example paths in SparseHalfCheetah
(b) Our state embeddings for SparseHalfCheetah
(c) Example paths in Montezuma’s Revenge
(d) Our state embeddings for Montezuma’s Revenge
(f) Our state embeddings for Frostbite
Figure 3: Example sample paths in our learned embedding representations. Note the embedding
dimensionality d is 2, and thus we did not use any dimensionality reduction techniques.
(e) Example paths in Frostbite
(a) SwimmerGather
(b) SparseHalfCheetah
Figure 4: Performance of EMI on locomotion tasks with sparse rewards compared to baseline meth-
ods (TRPO, EX2, ICM). The solid line is the mean reward (y-axis) of 5 different seeds at each
iteration (x-axis) and the shaded area represents one standard deviation from the mean.
6.3 Vision-based tasks with discrete control
For vision-based exploration tasks, our results in Figure 5 show that EMI achieves the state of the
art performance on Freeway, Frostbite, Venture, and Montezuma’s Revenge in comparison to the
baseline exploration methods. Figures 3c to 3f illustrate our learned state embeddings φ. Since our
embedding dimensionality is set to d = 2, we directly visualize the scatter plot of the embedding
representation in 2D. Figure 3d shows that the embedding space naturally separates state samples
into two clusters each of which corresponds to different rooms in Montezuma’s revenge. Figure 3f
shows smooth sample transitions along the embedding space in Frostbite where functionally similar
states are close together and distinct states are far apart. For information about how our error term
S(s, a) works in those vision-based tasks, please refer to Appendix A.2.
Extending our experiments in Figure 4 and Figure 5, we further compare EMI with other exploration
methods as shown in Table 1. EMI shows the outstanding performance on 6 out of 8 environments.
7
Under review as a conference paper at ICLR 2019
(b) Frostbite
(a) Freeway
(d) Gravitar
O	IOO	2∞	300	400	500
(e) Solaris
(c) Venture
(f) Montezuma’s Revenge
Figure 5: Performance of EMI on sparse reward Atari environments compared to the baseline meth-
ods (TRPO, EX2, ICM). EMI in (a), (b), (d), (e) uses relative diversity intrinsic rewards. Prediction
error intrinsic rewards are used in (c), (f). The solid line is the mean reward (y-axis) of 5 different
seeds at each iteration (x-axis) and the shaded area represents one standard deviation from the mean.
	EMI (5 seeds)	EX2 (5 seeds)	ICM (5 seeds)	SimHash	VIME	TRPO (5 seeds)
SWimmerGather	0.442	0.200	0	0.258	0.196	0
SparseHalfCheetah	194.9	153.7	-14-	0.5	98.0	0
Freeway	-340-	-271-	-33.6-	33.5	-	-267-
Frostbite	7388	-3387-	-4465-	5214	-	-2034-
Venture	-646-	-589-	-418-	616	-	-263-
Gravitar	-599-	-550-	-424-	604	-	-508-
Solaris	-2775-	-2276	-2453-	4467	-	-ɜɪθl-
MontezUma	387	0	161	238	-	0
Table 1: Mean score comparison on baseline methods. We compare EMI with EX2 (Fu et al.,
2017), ICM (Pathak et al., 2017), SimHash (Tang et al., 2017), VIME (Houthooft et al., 2016)
and TRPO (Schulman et al., 2015). EX2, ICM and TRPO columns are average of 5 seeds runs
coherent to Figure 4 and Figure 5. SimHash and VIME results are reported in previous works. All
exploration methods here are implemented based-on TRPO policy. Results of SparseHalfCheetah
and SwimmerGather are reported around 5M and 100M time steps respectively. Results of Atari
environments are reported around 50M time steps.
7 Conclusion
We presented EMI, a practical exploration method that does not rely on direct generation of high di-
mensional observations while extracting the predictive signal that can be used for exploration within
a compact representation space. Our results on challenging robotic locomotion tasks with continu-
ous actions and high dimensional image-based games with sparse rewards show that our approach
transfers to a wide range of tasks and shows state of the art results significantly outperforming recent
prior works on exploration. As future work, we would like to explore utilizing the learned linear
dynamic model for optimal planning in the embedding representation space. In particular, we would
like to investigate how an optimal trajectory from a state to a given goal in the embedding space
under the linear representation topology translates to the optimal trajectory in the observation space
under complex dynamical systems.
8
Under review as a conference paper at ICLR 2019
References
Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, and Aaron Courville. Mutual
information neural estimation. In International Conference on Machine Learning, volume 2018,
2018.
Marc Bellemare, Joel Veness, and Erik Talvitie. Skip context tree switching. In International
Conference on Machine Learning, pp. 1458-1466, 2014.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, pp. 1471-1479, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?
Journal of the ACM (JACM), 58(3):11, 2011.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process
expectations for large time. iv. Communications on Pure and Applied Mathematics, 36(2):183-
212, 1983.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning,
pp. 1329-1338, 2016.
Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2577-2587,
2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and
Yoshua Bengio. Learning deep representations by mutual information estimation and maximiza-
tion. arXiv preprint arXiv:1808.06670, 2018.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems, pp. 1109-1117, 2016.
Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Proceedings
of the 3rd International Conference on Learning Representations (ICLR), 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Teuvo Kohonen. Representation of information in spatial maps which are produced by self-
organization. In Synergetics of the Brain, pp. 264-273. Springer, 1983.
Teuvo Kohonen and Panu Somervuo. Self-organizing maps of symbol strings. Neurocomputing, 21
(1-3):19-30, 1998.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
9
Under review as a conference paper at ICLR 2019
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In ICML, volume 99, pp. 278-287, 1999.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271-279, 2016.
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional
video prediction using deep networks in atari games. In Advances in neural information process-
ing systems, pp. 2863-2871, 2015.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning, volume 2017,
2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, volume 2015, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement
learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2753-2762,
2017.
Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean
Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently controllable factors.
arXiv preprint arXiv:1708.01289, 2017.
10
Under review as a conference paper at ICLR 2019
A Appendix
A.1 Experiment Hyperparameters
In all experiments, we use Adam optimizer with a learning rate of 0.001 and a minibatch size of 512
for 3 epochs to optimize embedding networks. In each iteration, we utilized collected TRPO batch at
each iteration to train embedding networks except for SparseHalfCheetah which uses FIFO replay
buffer of size 250000. The embedding dimensionality is set to d = 2 in all of the environments
except for Gravitar and Solaris where we set d = 8. Relative diversity term is used as an intrinsic
reward with the weight of 0.1, except for Venture and Montezuma’s Revenge where the intrinsic
reward is set as a prediction error term with the weight of 0.001. The following tables give the
detailed information of the remaining hyperparameters.
Environments	SWimmerGather	∣	SParSeHalfCheetah
TRPO method	Single Path
TRPO step size	0.01
TRPO batch size	50k	I	5k
Policy network	A 2-layer FC with (64, 32) hidden units (tanh)
Baseline network	A 32 hidden UnitS FC (ReLU) ∣	Linear baseline
φ network	Same structure as policy network
ψ network	A 64 hidden UnitS FC (ReLU)
Information network	A 2-layer FC with (64, 64) hidden units (ReLU)
Error network	State input passes the same network structure as policy network. Concat layer concatenates state output and action. A 256 units FC (ReLU)
Max path length	500
Discount factor	0.995
λinfo		0.05
λsparsity		10000
λunitKL		0.1
Table 2: Hyperparameters for MuJoCo experiments.
Environments	Freeway, Frostbite, Venture, Montezuma,s Revenge, Gravitar, Solaris
TRPO method	Single Path
TRPO step size	001
TRPO batch size	100k	—
Policy network	2 convolutional layers (16 8x8 filters of stride 4, 32 4x4 filters of stride 2), followed by a 256 hidden units FC (ReLU)
Baseline network	Same structure as policy network
φ network	Same structure as policy network
ψ network	A 64 hidden UnitS FC (ReLU)	一
Information network	A 2-layer FC with (64, 64) hidden units (ReLU)
Error network	State input passes the same network structure as policy network. Concat layer concatenates state output and action. A 256 units FC (ReLU)
Max path length	4500
Discount factor	0.995
λinfo		01
λsparsity		100
λunitKL		0.5	—
Table 3: Hyperparameters for Atari experiments.
11
Under review as a conference paper at ICLR 2019
A.2 Experimental evaluation of the error model
(a)	st and s0t are from different
rooms with distant background
images.
(b)	The agent is already off the
platform in st .
(c)	The agent climbs up the lad-
der as expected.
Figure 6:	Example transitions that entail large or small instances of the error term S(s, a), in Mon-
tezuma’s Revenge.
In order to understand how the error term S(s, a) in EMI works in practice, we visualize three
representative transition samples in Figure 6 and check the residual error norm without the error
term (kφ(s0t) - (φ(st) + ψ(at)) k2) and the error term norm (kS(st, at)k2).
In the case of Figure 6a, due to the discrepancy between the two different background images,
kφ(st) - φ(s0t)k2 becomes large which makes the residual error as well as the error term larger, too.
For this specific sample, the residual error norm without the error term was 2.72 and the norm of the
error term was 0.0296. Figure 6b describes the case where the action chosen by the policy has no
effect on s0t i.e. P(s0t|st, at) = P (s0t |st). Linear models without any noise terms can easily fail in
such events. Thus, the error term in our model gets bigger to mitigate the modeling error. The norm
of the residual error without the error model for this example transition was 3.81, and its error term
had a norm of 0.0309.
On the other hand, Figure 6c represents cases that the chosen action works in the environment as
intended. The residual error norm for this sample was 0.79 without the error term and the error term
norm was 0.0082.
In conclusion, we observed the error terms generally had much larger norms in the cases such as
Figure 6a (0.0296) and Figure 6b (0.0309) compared to the case like Figure 6c (0.0082), in order to
alleviate the occasional irreducible large residual errors under the linear dynamics model.
12
Under review as a conference paper at ICLR 2019
A.3 Ablation study
Figure 7	shows the ablation study of loss terms in EMI to verify the influence of each factor. Ablat-
ing a single factor like the information term, the linear dynamics with sparse noise or the unit KL
divergence constraint degrades performance significantly. It means that each factor has a non-trivial
impact on EMI. Also, simultaneously ablating the information gain term with another factor dimin-
ishes reward into zero. It denotes that the information gain term has the most critical impact on EMI.
Figure 7: Ablation study of loss terms in EMI on SparseHalfCheetah environment. Each solid line
represents the mean reward of 5 random seeds.
In reward augmentation process, EMI agent computes intrinsic reward rd and then learns from
r = renv + αrd. Figure 8 shows the impact of α in EMI. Although α = 0.1 gives the best
performance, other choices also give comparable performance. It can be concluded that EMI is
robust to the choice of intrinsic reward coefficient.
300-
250-
----cr = 1
α = 0.1
----(X = 0.01
----a = 0.001
200-
150 -
100 -
50 -
J4>1

-50 -
____________⅛,
200
400
600
800
1000


0
Figure 8:	Study of intrinsic reward coefficient α in EMI on SparseHalfCheetah environment. Each
solid line represents the mean reward of 5 random seeds.
13