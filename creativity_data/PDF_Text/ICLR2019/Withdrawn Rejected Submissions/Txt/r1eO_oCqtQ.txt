Under review as a conference paper at ICLR 2019
Gaussian-gated LSTM: Improved convergence
BY REDUCING STATE UPDATES
Anonymous authors
Paper under double-blind review
Ab stract
Recurrent neural networks can be difficult to train on long sequence data due to the
well-known vanishing gradient problem. Some architectures incorporate methods
to reduce RNN state updates, thereby allowing the network to preserve memory
over long temporal intervals. We propose a timing-gated LSTM RNN model,
called the Gaussian-gated LSTM (g-LSTM) for reducing state updates. The time
gate controls when a neuron can be updated during training, enabling longer mem-
ory persistence and better error-gradient flow. This model captures long temporal
dependencies better than an LSTM on very long sequence tasks and the time gate
parameters can be learned even from a non-optimal initialization. Because the
time gate limits the updates of the neuron state, the number of computes needed
for the network update is also reduced. By adding a computational budget term
to the training loss, we obtain a network which further reduces the number of
computes by at least 10×. Finally, we propose a temporal curriculum learning
schedule for the g-LSTM that helps speed up the convergence time of the equiva-
lent LSTM on long sequences.
1	Introduction
Numerous methods and architectures have been proposed to mitigate the vanishing gradient problem
in RNNs, with LSTMs (Hochreiter & Schmidhuber, 1997) as one of the first prominent solutions
doing so by including gating structures in the computation. Although the LSTM has excelled at
handling many tasks (Schmidhuber, 2015; Lipton, 2015), it still has difficulties in learning complex
and long time dependencies (Neil et al., 2016; Chang et al., 2017; Trinh et al., 2018).
In the last few years, various methods which reduce the state updates of an RNN (LSTM) have been
explored to better learn long time dependencies from data. Clockwork RNNs (Koutnik et al., 2014)
group the hidden units of the RNN into “modules,” where each module is executed at pre-specified
time steps thereby skipping time steps which helps learn longer time dependencies. Recently, var-
ious other methods have been proposed which can be characterized by the use of additional “time
gates,” kn, that control the information flow from one time step to the next (Krueger et al., 2016;
Campos et al., 2017). Phased LSTM (PLSTM) (Neil et al., 2016) learns a parameterized function,
kn , from the the time input of the current state and was proven to be successful at learning over long
sequences.
The PLSTM time gate, parameterized by period, phase, and ratio parameters for each hidden unit, is
defined through a modulo function with an ill-defined gradient. Furthermore, with periodic functions
being hard to learn using gradient-based methods (Shamir, 2016) and with kn being periodic, the
PLSTM was unable to learn the time gate parameters and hence relied on careful initialization. In
order to offset these difficulties, this work proposes a new LSTM variant called the Gaussian-gated
LSTM (g-LSTM). Similar to the PLSTM, it is an LSTM model with a parameterized kn but with
only two parameters per hidden unit. Unlike the PLSTM which uses a periodic formulation for kn ,
the g-LSTM uses a Gaussian function.
We show in this work that the g-LSTM can provide a number of possible advantages over the LSTM,
in particular, on long sequence tasks that pose convergence problems during training:
1
Under review as a conference paper at ICLR 2019
•	The g-LSTM network can process very long sequences by reducing the time over which
the neurons can be updated. It converges faster than the LSTM, especially on sequences
that are over 500 steps.
•	The “openness” of the neuron for an update can be adapted according to the task during
training, even for extreme, non-optimal initializations of the time gate parameters.
•	By introducing a computational budget term into the loss function during training, the
“openness” of the neuron can be optimized for a reduced computational budget. This re-
duction can be achieved with little or no degradation to the network performance and is
useful for network pruning.
•	A “temporal curriculum” training schedule can be set up for the g-LSTM so that it helps to
speed up the convergence of a normal LSTM.
The paper is structured as follows: In section 2, we discuss briefly the related work. Then, in
section 3, we present the formulation of the g-LSTM, the datasets used in this work and details
about the experimental hyperparameters. In section 4, we present experiments demonstrating the
usefulness of the g-LSTM with respect to the four claims listed above. We provide gradient analysis
in section 5 to further explain the faster convergence results of the g-LSTM. Finally in section 6, we
conclude with a brief discussion of the results.
2	Related Work
There have been a multitude of proposed methods to improve the training of RNNs, especially for
long sequences. Apart from incorporating additional gating structures, for example the LSTM and
the GRU (Cho et al., 2014), more recently various techniques were proposed to further increase the
capabilities of recurrent networks to learn on sequences of length over 1000. Proposed initialization
techniques such as the orthogonal initialization of kernel matrices (Cooijmans et al., 2016), chrono
initialization of the biases (Tallec & Ollivier, 2018), and diagonal recurrent kernel matrices (e.g. Li
et al. (2018)) have demonstrated success. Trinh et al. (2018) propose using truncated backpropaga-
tion with an additional auxiliary loss to reconstruct previous events.
Methods that enable more efficient learning on long temporal sequences use solutions that preserve
memory over longer timescales. Such solutions were first explored by Koutnik et al. (2014) in the
Clockwork RNN (CW-RNN). This network skips state updates by allowing different neurons to
be “activated” on different, modulated clock cycles. More recently proposed models for skipping
updates include the Phased LSTM (PLSTM) (Neil et al., 2016) which uses a modulo-periodic timing
gate to limit state updates; the Zoneout network (Krueger et al., 2016) which skips state updates in
a random manner; and the Skip RNN (Campos et al., 2017) which learns a state skipping scheme
from the data to shorten the effective sequence length for the task. Additionally, the LSTM-Jump (Yu
et al., 2017) uses a reinforcement learning algorithm to learn when to skip state updates, showing a
method to more quickly process (long) sequential data with an RNN while maintaining an accuracy
comparable to a baseline LSTM.
It has been suggested but not yet demonstrated in the literature that the parameters of the CW-RNN
clock cycle and PLSTM timing gate could be learned in training. Currently, the implementation of
these networks requires a careful initialization of these parameters. With the Gaussian-gated LSTM
(g-LSTM) in this work we present a time gated RNN network that converges on long sequence tasks
and also has the ability to learn its time gate parameters even when initialized in a nonoptimal way.
3	Methods
3.1	g-LSTM
The g-LSTM is an LSTM model with an additional time gate (Fig. 1). This time gate is used to
regulate the information flow in time. Equations 1 - 3 describe the update equations for the hidden
and cell states of the LSTM. Equations 4 and 5 describe the gating mechanism of the time gate, kn .
2
Under review as a conference paper at ICLR 2019
Figure 1: Comparison of the LSTM and g-LSTM models. K is the computational block for the time
gate in (b).
in = σ(xnWxi + hn-1Whi + bi),fn = σ(xn Wxf + hn-1Whf + bf)	(1)
Wn = fn ® Cn—1 + in ® σ(xnWxg + hn-1Whg + bg )	(2)
on = σ(xn Wxo + hn—1 Who + bo), hn = on tanh(cWn )	(3)
cn = kn	cWn + (1 - kn )	cn—1	(4)
hn = kn	hn + (1 - kn)	hn—1	(5)
In a standard LSTM, the gating functions in , fn , on , represent the input, forget, and output gates
respectively at sequence index n. cn is the cell activation vector, and xn and hn represent the input
feature vector and the hidden output vector, respectively. The cell state cn is updated with a fraction
of the previous cell state that is controlled by fn, and a new input state created from the element-wise
(Hadamard) product, denoted by , of in and the candidate cell activation as in Eq. 2.
In the g-LSTM, we further control the cell state and the output hidden state through the kn gate
which is independent of the input data and hidden states, and is purely dependent on the time input
corresponding to the sequence index n. The use of the Hadamard product ensures that each hidden
unit is independently controlled by the corresponding time gate unit, thus enabling the different units
in the layer to process the input at different time scales.
The time gate kn is defined based on a Gaussian function as: kn = e-(tn-*)2/。2 where the mean
parameter, μ, defines the time when the hidden unit is “open” and the standard deviation, σ, con-
trols the openness of the time gate for each unit around its corresponding μ. The time inputs
t = {t1, t2, ..., tn, ..., tN} for the sequence x = {x1, x2, ..., xn, ..., xN} can correspond to the phys-
ical notion of time at the respective sequence input. In the absence of a standard notion of time, we
use the sequence indices as the time input, i.e. t = {1, 2, ..., n, ..., N}. In this work, we assume this
notion of time by default. The “openness” of kn for a neuron is defined by the parameterization of
its Gaussian function.
3.2	Back Propagation for g-LSTM
An important characteristic of the g-LSTM is reduced gradient flow in back propagation training
methods. By having the gating structure as in Eqs. 4 and 5 there are fewer gradient product terms,
which reduces the likelihood of vanishing or exploding gradients. In a gradient descent learning
scheme for a given loss function, L, when training the recurrent parameters, Wh (from Eqs. 1 - 3),
the gradient as in Eq. 6 is used.
∂L ∂L ∂hN
---=-----：--
∂Wh ∂hN ∂Wh
(6)
By the chain rule ddWwN expands for all time steps of the sequence, n ∈ {1,...,N}. Because each
output state is gated by the time gate, kn the gradient terms in the expansion of ddWwN are scaled by
kn . When the time gate is open less often, i.e. with a small σ value, then there are fewer influential
gradient terms. More details are given in Appendix A.
3
Under review as a conference paper at ICLR 2019
Dataset	# units	Initialization		Performance	
		μ	σ	g-LSTM	LSTM
Adding (N=1000)	110	〜U(300, 700)	40	3.8 ∙ 10-5	1.4 ∙ 10-3
Adding (N=2000)	110	〜U(500,1500)	40	1.3 ∙ 10-3	1.6 ∙ 10T
SMNIST	110	〜U(1, 784)	250	1.3%	1.8%
SCIFAR-10	128	〜U(1,1024)	650	41.1%	41.8%
Table 1: Network architectures and performance for the convergence experiments in subsection 4.1.
The performance metric is the final mean squared error (MSE) loss for the adding task, and the label
error rate for both sMNIST and sCIFAR-10.
3.3	Datasets
The experiments described in the paper are carried out on the adding task and two standard long
sequence datasets: the sequential MNIST and the sequential CIFAR-10 datasets.
Adding task: In order to test the long sequence learning capability of the g-LSTM, we use the
adding task (Hochreiter & Schmidhuber, 1997). In this task, the network is presented with two
sequences of length N, X = {xι, ...,xn}, Xn 〜 U(0,1)) and m = {mι, ...,mN},mn ∈
{0, 1}, PtN=1 mn = 2. The sequence m has exactly two values of 1 and the remaining values of
the sequence are 0. The indices of the “1” values are chosen at random. For each pair in the se-
quence (x, m), the associated label value, y, is the sum of the two values in x corresponding to
the “1” values of m. The objective of this task is to minimize the mean squared error between the
predicted sum from the network, y, and the labeled sum, y. A new training set of 5000 sequence
samples is presented in every epoch during training in order to avoid overfitting. The test set consists
ofa separate 5000 samples. For N > 1000, it is known that LSTMs have difficulty learning the task
and hence we focus on values of N > 1000 in this work.
sMNIST: The sequential MNIST dataset is widely used to analyze the performance of a recurrent
model. This dataset consists of 60,000 training samples and 10,000 test samples, each a single vector
sequence of length 784 corresponding to the 28 × 28 pixel images in the MNIST dataset (LeCun
et al., 1998). We also use permuted MNIST (pMNIST), a permuted variant of the sMNIST dataset
where the sequences are processed with a fixed random permutation, making the task harder.
sCIFAR-10: The sequential CIFAR-10 dataset is another long sequence dataset based on CIFAR-
10 (Krizhevsky et al., 2014) with 10 classes. The 32 × 32 RGB pixel images are reshaped into
sequences of length 1024 with 3 dimensional features corresponding the RGB channels at every
time step. Like in the sMNIST dataset, the dataset consists of 60,000 training samples and 10,000
test samples.
3.4	Experimental Hyperparameters
For the adding task, a mean squared error (MSE) loss was used with the Adam optimizer (Kingma
& Ba, 2014) with a learning rate of 10-3. The g-LSTM time gate parameters were trained using a
learning rate of 100. For both sMNIST and sCIFAR-10 datasets, the cross-entropy loss function was
used along with the RMSProp optimizer (Tieleman & Hinton, 2012) with a learning rate of 10-3.
Decay parameters of 0.5 and 0.9 were used for sMNIST and sCIFAR-10, respectively. The bias of
the forget gate is initialized to 1 following (Jozefowicz et al., 2015).
4	Results
Section 4.1 presents results that demonstrate the faster convergence properties of the g-LSTM on
long sequence tasks. Section 4.2 shows the trainability of the time gate parameters of the g-LSTM
even when the parameters are initialized in a non-optimal way. Section 4.3 presents a modified
loss function used during training to reduce the number of computes for the network update and
Section 4.4 presents a new “temporal curriculum” learning schedule that allows g-LSTMs to help
LSTMs converge faster.
4
Under review as a conference paper at ICLR 2019
SSOl ⅛ΦHsBJ JOJS"qBI ⅛ΦH
(c) sMNIST	(d) sCIFAR-10
Figure 2: Test loss and test label error rates across training epochs for LSTM (black) and g-LSTM
(blue) networks on different tasks.
4.1	Fast convergence properties of g-LSTM
First, we look at the convergence properties of the g-LSTM on the long-sequence adding task, the
sMNIST task and the sCIFAR-10 task. Table 1, above, details the network architectures used in the
experiments in this section. Similar to the architecture from Trinh et al. (2018), the recurrent layer
of the sCIFAR-10 network is followed by two 256 unit fully-connected (FC) layers, where Drop-
Connect (Wan et al. (2013)) (p = 0.5) is applied to the second FC layer. The kernel matrices in the
LSTM networks were initialized in an orthogonal manner as described in (Cooijmans et al., 2016).
The test performances of these networks during the course of the training on different datasets are
shown in Fig. 2, while the corresponding final performance metrics at the end of training are shown
in Table 1. From Fig. 2, it is evident that the test loss of the g-LSTM decreases faster in training
than the LSTM across all datasets. Further experiments show that this trend is maintained with
different training optimizers, LSTM initializations including the bias initialization following Tallec
& Ollivier (2018), and network sizes as shown in Appendix D.
Table 2 compares the performance of various networks including the g-LSTM and the baseline
LSTM on sMNIST and sCIFAR-10 (from Table 1). The results show that the g-LSTM consistently
performs better than the LSTM and has a similar performance to other state-of-the-art networks.
Different network sizes were also investigated for the sMNIST task, see Appendix D.
Network	sMNIST	pMNIST	sCIFAR-10
g-LSTM (ours)	-^13%^^	-^75%^^	^^41.1%
LSTM (ours)	1.8%	8.4%	41.8%
r-LSTM (Trinh et al., 2018)	1.6%	4.8%	27.8%
Zoneout (Krueger et al., 2016)	1.3%	6.9%	-
IndRNN (6 layers) (LietaL,2018)	1.0%	4.0%	-
BN-LSTM (Cooijmans et al., 2016)	1.0%	4.6%	-
Skip LSTM (Campos et al., 2017)	2.7%	-	-
Table 2: Comparison of label error rates across different networks.
5
Under review as a conference paper at ICLR 2019
4.2	Trainability of the time gate parameters of g-LSTM
To demonstrate that the g-LSTM can be trained even with non-optimal initializations, we look at the
performance of the g-LSTM on the adding task with different time gate parameter initializations.
We concern ourselves with sequences of length 1000 that are difficult for the LSTM. The time gate
parameters are initialized in a way to temporally constrain the network so that it can only process for
a short period of time. For example, a network With time gate parameters initialized With μ = 500
and σ = 40 as in Figure 3 (a) can only process a short period of time around the middle of sequence.
It folloWs that the netWork Would be unable to learn With these parameters because in the adding
task the input data is distributed equally across the sequence length (T = 1000). Therefore, in order
to learn the task from this initialization, the time gate parameters must learn a distribution such that
the gates over all hidden units are open across the entirety of the sequence.
We observe that the time gate parameters do learn, as shoWn in Figure 3 (b), thereby enabling the
netWork to solve the task. Independent of various time gate initializations, the netWork reaches an
MSE of around 3.9 × 10-5 at the end of 700 epochs; details of Which could be found in Appendix B.
The ability of the netWork to learn the time gate parameters necessary to cover the entire sequence
is especially significant because it shoWs that even With this narroW time WindoW initialization that
requires learning of the time gates, the g-LSTM learns the task, Whereas the PLSTM does not learn
the task as Well. An example of this is shoWn in Appendix C.
(a) Pre-training
(b) Post-training
Figure 3: Time gate behavior pre and post training, demonstrating the ability of the netWork to learn
from extreme initialization parameters. Here, kn is plotted as a function of time (x-axis) With black
values corresponding to a fully closed gate (value 0) and White values corresponding to a fully open
gate (value 1). Note that loWer values of σ ensure that the unit is processed only if the time input is
μ, while higher σ values lead to the unit processed like in the standard LSTM, at all times.
4.3	Reduction in computation
Although the formulation of the g-LSTM appears to require more computes, it offers substantial
speedup as a large proportion of the neurons can be skipped in a timestep at runtime. We can set a
threshold on the time gate so that we skip all corresponding computations for time steps where kn
is below this threshold. To further reduce the number of operations, it is preferred that the σ of the
kn for different neurons should be small but the network performance should not be significantly
degraded. To achieve this goal, we included a “computational budget” loss term during the opti-
mization of the gate parameters, μ and σ. The loss equation for updating the kn parameters is given
by:
L = Ldata + λLbudget.
Similar to the Skip RNN network (Campos et al., 2017), a budget loss term which minimizes the
average openness of the time gate over time is applied:
NJ
Lbudget = E[kn] ≈ XXkn(j)
n=1 j=1
for every neuron j of the g-LSTM. The study was carried out on sMNIST using a network with 110
units, σ initialized to 50, μ initialized uniformly at random between 1 to 784, and a λ value of 1.
6
Under review as a conference paper at ICLR 2019
The network’s performance of 2.2% LER was comparable to the network’s performance of 1.3%
when no additional budget constraint was imposed. The final σ range for the budgeted g-LSTM is
much smaller compared to that of the g-LSTM as shown in Fig. 4. There is only a slight increase in
LER for the budgeted g-LSTM versus the g-LSTM (see Table 2), even though there is a significant
decrease in the average time gate openness across all hidden units.
τ1.0
0.8
0.6
0.4
0.2
∣0.0
(a) g-LSTM
(b) budgeted g-LSTM
O O
• •
O O
。+-j比」」0X1。一。q比一 *S⅛L
Sdo =UaLrnOaI Jo uo-ɔ比立
1
O -
W W
Figure 4: Time gate behavior of (a) g-LSTM and (b) budgeted g-LSTM for 110 units post training.
8 7 6 5 4
• ∙ ∙ ∙ ∙
Ooooo
。比」」0X1。一。q比一s芦
20	40	60	80	100
Epoch number
Figure 5: Reduction in computes as a func- Figure 6: Speed up in convergence of LSTM us-
tion of threshold for budgeted g-LSTM. ing the temporal curriculum learning schedule.
In order to reduce the number of computes, we set a threshold, vT for kn so that the update steps
are carried out only if kn > vT , if kn < vT the previous neuron state can be copied over to the
current state. By increasing vT , the number of computes decreases as shown in Fig. 5. In the case
of vT = 0.01, only 8.2% of the time gates are open on average across all hidden units and all time
steps. Furthermore, the LER increased only slightly to 2.3% from 2.2%.
We give a quantitative estimate for the number of operations (Ops) corresponding to the number of
update equations for a g-LSTM. In the estimate, we count a multiply and an add operation as 1 Op
and non-linear functions as 5 Ops. For an LSTM, the number of operations is given by
NLSTM =TH(8D+8H+29)
where T is the number of time steps, H is the number of hidden units, and D is the dimension of
the input data. For a g-LSTM, the number of operations is given by
Ng-LSTM = NLSTM + Ngate
where Ngate = 13 T H is the total number of operations for computing the time gate. The total
number of operations for the g-LSTM network on the sMNIST dataset is around 80 MOps for
N = 110 and T = 784, after thresholding the budgeted g-LSTM this number is reduced to 7.6
MOps. Additional λ hyperparameters were also investigated for the sMNIST task, see Appendix D.
4.4	Temporal curriculum training schedule for LSTMs
We demonstrate that it is possible to train an LSTM network to converge faster on a difficult task by
using a “temporal curriculum” training schedule for the equivalent g-LSTM network. According to
this schedule, the initial σ values of the g-LSTM network are increased continuously throughout the
7
Under review as a conference paper at ICLR 2019
training period ending up with high values by the end of training. With such high values, the time
gates are essentially open, resulting in an LSTM network. At every training epoch, the lowest ρ%
of the σ values, σ in the layer are updated as: σ -→ (1 + α) ∙ σ.
We analyze the impact of this training schedule for training an LSTM network on sCIFAR-10. For
the equivalent g-LSTM network with 110 units, μ is initialized uniformly at random between 1 and
1024 and σ is initialized to 50. An α value of 1/6 and ρ value of 15% are chosen. To ensure that
the time gate is fully open by the end of training, σ is set to 5000 across all units during the last
10 epochs of training. The learning rate of the time gate parameters is set to 0, i.e. μ and σ are
no longer updated. Figure 6 shows that the temporal curriculum training schedule allows for faster
convergence of an LSTM network. The final weights of the trained g-LSTM network can then be
copied over to a LSTM network for inference.
5 Gradient flow
We present results regarding backpropagation flow through the LSTM and g-LSTM networks. Fol-
lowing the hypothesis presented in Section 3.2 on the reduced likelihood of vanishing or exploding
gradients in the g-LSTM, we investigate the average gradient norms across time steps, similar to
the work in (Krueger et al., 2016). We compute the gradient norms of the loss with respect to the
hidden activations, the exact definition is given in Appendix E. Comparing the error propagation of
the g-LSTM and LSTM networks on the SMNIST task (as in Sec. 4.1), Figure 7 shows the gradient
norms at each time step after training for two different epochs.
0.00-
08060402
■ ■ ■ ■
Oooo
日joU 2up∙ea置W
Timestep
0.00
08060402
■ ■ ■ ■
Oooo
日joU 2up∙ea置W
(a) After epoch 1	(b) After epoch 11
Figure 7: Average gradient norm through time steps for g-LSTM and LSTM.
Interpreting the gradient flow from higher to lower time steps (right to left), the gradients of the
g-LSTM shown in Fig. 7 show higher gradient values in earlier time steps than the LSTM. It is
possible that one of the reasons the g-LSTM converges more quickly (as in Fig. 2 (c)) is that this
back-propagated gradient information is more consistent across time steps and does not vanish at
early time steps.
6 Conclusion
This work proposes a novel RNN variant with a time gate which is parameterized by the input in
time. The convergence speeds of the g-LSTM and LSTM are similar for short sequence tasks but
the g-LSTM shows faster convergence and produces higher accuracies than LSTM networks on long
sequence tasks, as demonstrated for adding task sequences which are longer than 1000 timesteps;
and on the sMNIST and sCIFAR-10 datasets. We also demonstrate that the time gate parameters of
the g-LSTM (unlike those of the PLSTM) are learnable even when the time gates are initialized in
an extreme non-optimal manner for the adding task. The time gate of the g-LSTM can reduce the
number of computes that is needed for the updates of the LSTM equations and with an additional
loss term to reduce the compute budget, the σ values of the time gate are reduced leading to a 10×
decrease in the number of actual computes and with little loss in network accuracy, for the sMNIST
dataset. The observation that the budgeted g-LSTM has neurons which are closed by the timing gate
suggests that this method can be used to prune a network. We also show that our proposed temporal
curriculum training schedule for the g-LSTM can help a corresponding LSTM network to converge
during training on long sequence tasks. For future work, it will be of interest to investigate whether
these properties carry over to larger or domain-specific datasets.
8
Under review as a conference paper at ICLR 2019
References
Victor Campos, Brendan Jou, Xavier Giro i Nieto, Jordi Torres, and Shih-Fu Chang. Skip RNN:
learning to skip state updates in recurrent neural networks. CoRR, abs/1708.06834, 2017. URL
http://arxiv.org/abs/1708.06834.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael J.
Witbrock, Mark Hasegawa-Johnson, and Thomas S. Huang. Dilated recurrent neural networks.
CoRR, abs/1710.02224, 2017. URL http://arxiv.org/abs/1710.02224.
KyUnghyUn Cho, Bart van Merrienboer, Calar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using RNN Encoder-Decoder for
statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 1724-1734, Doha, Qatar, October 2014. As-
sociation for Computational Linguistics. URL http://www.aclweb.org/anthology/
D14-1179.
Tim Cooijmans, Nicolas Ballas, Cesar Laurent, and Aaron C. Courville. Recurrent batch normal-
ization. CoRR, abs/1603.09025, 2016. URL http://arxiv.org/abs/1603.09025.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth Interna-
tional Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine
Learning Research, pp. 249-256, Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010. PMLR.
URL http://proceedings.mlr.press/v9/glorot10a.html.
SePP Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735—
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In International Conference on Machine Learning, pp. 2342-2350, 2015.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Jan Koutnik, Klaus Greff, Faustino J. Gomez, and Jurgen Schmidhuber. A clockwork RNN. CoRR,
abs/1402.3511, 2014. URL http://arxiv.org/abs/1402.3511.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset. online: http://www. cs.
toronto. edu/kriz/cifar. html, 2014.
David Krueger, Tegan Maharaj,Janos Kramar, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary
Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and Chris Pal. Zoneout: Regularizing
RNNs by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305, 2016.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, Nov 1998. ISSN 0018-9219.
doi: 10.1109/5.726791.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural net-
work (IndRNN): Building a longer and deeper RNN. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5457-5466, 2018.
Zachary Chase Lipton. A critical review of recurrent neural networks for sequence learning. CoRR,
abs/1506.00019, 2015.
Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased LSTM: Accelerating recurrent network
training for long or event-based sequences. In Advances in Neural Information Processing Sys-
tems, pp. 3882-3890, 2016.
Jurgen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117,
2015. doi: 10.1016/j.neunet.2014.09.003. Published online 2014; based on TR arXiv:1404.7828
[cs.NE].
9
Under review as a conference paper at ICLR 2019
Ohad Shamir. Distribution-specific hardness of learning neural networks. CoRR, abs/1609.01037,
2016. URL http://arxiv.org/abs/1609.01037.
Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time. In Proceedings of
International Conference on Learning Representation, 2018.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Trieu H. Trinh, Andrew M. Dai, Thang Luong, and Quoc V. Le. Learning longer-term dependencies
in RNNs with auxiliary losses. CoRR, abs/1803.00144, 2018. URL http://arxiv.org/
abs/1803.00144.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using DropConnect. In International Conference on Machine Learning, pp. 1058-1066,
2013.
Adams Wei Yu, Hongrae Lee, and Quoc V. Le. Learning to skim text. CoRR, abs/1704.06877, 2017.
URL http://arxiv.org/abs/1704.06877.
10
Under review as a conference paper at ICLR 2019
A Back propagation in Gaussian-gated RNN
For ease of illustration we analyze the gradient of a plain RNN with a Gaussian time gate (Eqs. 7
and 8).
hn = kn ∙ hn + (I - kn) ∙ hn-1	⑺
~ ______ __________ 、
h n = f (WχXn + Whhn-I)	(8)
∂hN
∂Wh
加 N	N	N
∂W0 Y (knwhfn + (I Ikn)) + X (knfn hn-1) Y (ks Whfs + (I- kS))
h n=1	n=1	s=n+1
(9)
入一
where ∂W0 = 1,h0 = ho = 0.
From Eq. 9 we can deduce some information about the advantages of the Gaussian time gate in
gradient flow for two simple cases of the function kn .
In Case 1 we choose a timing gate openness which corresponds to a very small σ for the Gaussian
gate, i.e. the gate is only open for 1 time step.
k5 = 1, kn = 0 ∀ n ∈ {1, ..., N}\{5}
∂WN = f5 + f5f5h4 = f5Wh (1 + f5h4)
In Case 2 we choose a timing gate openness which corresponds to a slightly larger σ for the Gaussian
gate, i.e. it is open for 5 time steps.
k2 = 1, k3 = 1, k4 = 1, k5 = 1, k6 = 1, kn = 0 ∀ n ∈ {1, ..., N}\{2, 3, 4, 5, 6}
∂hN
∂Wh
f20Whf30Whf40Whf50Whf60Wh(1+f20h1+f30h2+f40h3+f50h4+f60h5)
These cases show that there are fewer terms in the gradient for a timing gate that is opened for only
a small fraction of the sequence.
11
Under review as a conference paper at ICLR 2019
B Comparing various g-LSTM initializations
Initialization
Experiment ID	μ	σ	Final MSE Loss
A1	〜U(300, 700)	1	4.4 ∙ 10-4
A2	〜U(0, 400)	40	2.0 ∙ 10-5
A3	〜U(600,1000)	40	4.0 ∙ 10-4
Table 3: Adding task (T=1000): 110 unit g-LSTM network initializations and performances.
]1.0
0.8
0.6
0.4
0.2
0.0
time
(b) Experiment A1: after training
(a) Experiment A1: before training
]1.0
0.8
0.6
0.4
0.2
0.0
time
(d) Experiment A2: after training
(c) Experiment A2: before training
]1.0
0.8
0.6
0.4
0.2
∣0.0
time
(f) Experiment A3: after training
Figure 8: Additional experiments: timing gate openness for non-optimal initializations
(e) Experiment A3: before training
12
Under review as a conference paper at ICLR 2019
C	Comparing time gate parameter trainability in g-LSTM and
PLSTM
Network	Initialization	Final MSE Loss
g-LSTM	μ 〜U(300, 700), σ = 40	7.7 ∙ 10-5
PLSTM T = 1000, S 〜U(250, 650), r = 0.10	2.4 ∙ 10-4
Table 4: Adding task (T=1000): Comparing 110 unit g-LSTM and PLSTM networks with similar
initializations, MSE computed after training for 500 epochs.
s-!m UOPPIq
(a) before training
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
(b) after training (500 epochs)
Figure 9: g-LSTM
(a) before training
s-!m UOPPIq
(b) after training (500 epochs)
Figure 10:	PLSTM
13
Under review as a conference paper at ICLR 2019
D Hyperparameter Investigation
We look at the network performance for different hyperparameter values, focusing on the sMNIST
task.
Network Initialization and Optimizer In Fig. 2(c) of Section 4.1, we show that the g-LSTM
network converges faster than the LSTM for the sMNIST task using the RMSProp optimizer and
with an orthogonal initialization of LSTM kernels of both networks, as in (Cooijmans et al., 2016).
In addition to using this initializer and optimizer we include results using the ADAM initializer
(learning rate of 10-3) and a random weight initialization, “Xavier” as in (Glorot & Bengio, 2010).
Across all of these different training techniques we consistently observe that the g-LSTM converges
more quickly than the LSTM.
ωsJ jojjə∙q∙>I 场I
20	40	60	80	100
Epoch number
ωsJ jojjə∙q∙>I 场I
20	40	60	80	100
Epoch number
ωsJ jojjə∙q∙>I 场I
20	40	60	80	100
Epoch number
(a) Optimizer: RMSProp. Initializa-(b) Optimizer: ADAM. Initializa-(c) Optimizer: ADAM. Initializa-
tion: Xavier	tion: Xavier	tion: Orthogonal
Figure 11:	Results for sMNIST with various optimizers and initializations.
We ran further experiments to compare the chrono initialization of the LSTM forget and input biases
from (Tallec & Ollivier, 2018). The forget and input biases are set as bf 〜Iog(U([1, Tmax - 1]))
and bi = -bf where Tmax = 784 for the sMNIST task. The use of the time gate with the g-LSTM
shortens the effective sequence length for each unit; to account for this, we also provide the results
of using a smaller Tmax value for the chrono initialization, Tmax = σ = 250. The comparison
of both g-LSTM and LSTM with the chrono initialization and with the “constant initialization”
(bf = 1) in Fig. 12 shows that the g-LSTM with the constant initialization converges the fastest.
We hypothesize that the g-LSTM can converge faster when using the constant initialization over
the chrono initialization because the time gate’s effect of sequence-length-shortening reduces the
necessity for long memory, for which chrono initialization seeks to provide. We see that when we
reduce the maximum temporal dependency for the chrono initialization (to Tmax = 250, “chrono-g-
LSTM-250”) this g-LSTM network converges more quickly, similar to the g-LSTM with a constant
bias initialization. This suggests that these two techniques, chrono initialization and a Gaussian time
gate, could be used together to improve convergence in LSTM networks.
①sI loll ① I ① q£ WLL
0.6
0.4
0.2
0.0
0	20	40	60	80	100
Epoch number
Figure 12: Comparative results on sMNIST with chrono initialization and constant initialization.
Network Size Aside from the network size of 110 hidden units, we investigated the training con-
vergence for two additional network sizes: 25 and 220 hidden units. Note that the LSTM for network
size 25 is trained for 100 additional epochs until convergence was observed. Across all different net-
work sizes the g-LSTM converges much faster than the LSTM network. With fewer hidden units,
14
Under review as a conference paper at ICLR 2019
as seen in Fig. 13 (a), an even more dramatic speed-up in convergence is seen for the g-LSTM com-
pared with the LSTM. The final LERs (g-LSTM, LSTM) for each network size are: 25 units (2.79%,
3.58%), 110 units (1.35%, 1.81%), 220 units (1.10%, 1.34%).
4 2
♦ ♦
O O
-Vy JojJHFqσ3α∞。H
150
Epoch Number
(a) 25 hidden units
g-LSTM
LSTM
8 6 4 2 0
♦ ♦ ♦ ♦ ♦
Ooooo
Vy JOJJFqQα。
Epoch Number
(b) 220 hidden units
O
Figure 13: Results for sMNIST using two different network sizes.
Budgeted g-LSTM We provide additional results of the budgeted network (subsection 4.3) for
2 additional λ values, λ = 0.1 and λ = 10, comparing with the original result, for λ = 1. The
final LERs: 2.4% (λ = 0.1), 2.2% (λ = 1), 2.8% (λ = 10). The number of computes used by
the network trained with λ = 10 is significantly lower than the network that was trained for both
λ = 0.1 and 1.
-≡n U①PPW
3 2 1
♦ ♦ ♦
Ooo
J Jo」」① Ioq0I
0
100	150
Epoch number
Figure 14: Label error rate on sMNIST for different λ values (shown in legend).
-≡n u-pw
(a)λ =0.1	(b)λ= 1	(c)λ= 10
Figure 15: Gate openness for different λ values of the budgeted g-LSTM.
15
Under review as a conference paper at ICLR 2019
E Average Gradient Norm Definition
The average gradient norm in Section 5 is defined as:
Γ ∈ R+N
where N is the number of time steps of the sequence (for SMNIST, N
784).
γ= K1 E
Σ
(k)
∂L
∂hnk)
≈ Lb X
(l,k)
∂L
∂hnk)
summing over all L samples of the training set and all K hidden units.
16