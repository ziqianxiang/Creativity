Under review as a conference paper at ICLR 2019
Neural Rendering Model: Joint Generation
and Prediction for Semi-Supervised Learning
Anonymous authors
Paper under double-blind review
Ab stract
Unsupervised and semi-supervised learning are important problems that are es-
pecially challenging with complex data like natural images. Progress on these
problems would accelerate if we had access to appropriate generative models under
which to pose the associated inference tasks. Inspired by the success of Convolu-
tional Neural Networks (CNNs) for supervised prediction in images, we design
the Neural Rendering Model (NRM), a new hierarchical probabilistic generative
model whose inference calculations correspond to those in a CNN. The NRM
introduces a small set of latent variables at each level of the model and enforces
dependencies among all the latent variables via a conjugate prior distribution. The
conjugate prior yields a new regularizer for learning based on the paths rendered in
the generative model for training CNNs-the Rendering Path Normalization (RPN).
We demonstrate that this regularizer improves generalization both in theory and
in practice. Likelihood estimation in the NRM yields the new Max-Min cross
entropy training loss, which suggests a new deep network architecture-the Max-
Min network-which exceeds or matches the state-of-art for semi-supervised and
supervised learning on SVHN, CIFAR10, and CIFAR100.
Keywords: neural nets, generative models, semi-supervised learning, cross-entropy
1	Introduction
Unsupervised and semi-supervised learning have still lagged behind compared to performance leaps
we have seen in supervised learning over the last five years. This is partly due to a lack of good
generative models that can capture all latent variations in complex domains such as natural images
and provide useful structures that help learning. When it comes to probabilistic generative models, it
is hard to design good priors for the latent variables that drive the generation.
Instead, recent approaches avoid the explicit design of image priors. For instance, the Generative Ad-
versarial Networks (GANs) use implicit feedback from an additional discriminator that distinguishes
real from fake images (Goodfellow et al., 2014). Using such feedback helps GANs to generate
visually realistic images, but it is not clear if this is the most effective form of feedback for predictive
tasks. Moreover, due to separation of generation and discrimination in GANs, there are typically
more parameters to train, and this might make it harder to obtain gains for semi-supervised learning
in the low (labeled) sample setting.
We propose an alternative approach to GANs by designing a class of probabilistic generative models,
such that inference in those models also has good performance on predictive tasks. This approach is
well-suited for semi-supervised learning since it eliminates the need for a separate prediction network.
Specifically, we answer the following question: what generative processes output Convolutional
Neural Networks (CNNs) when inference is carried out? This is natural to ask since CNNs are state-
of-the-art (SOTA) predictive models for images, and intuitively, such powerful predictive models
should capture some essence of image generation. However, standard CNNs are not directly reversible
and likely do not have all the information for generation since they are trained for predictive tasks
such as image classification. We can instead invert the irreversible operations in CNNs, e.g., the
rectified linear units (ReLUs) and spatial pooling, by assigning auxiliary latent variables to account
for uncertainty in the CNN’s inversion process due to the information loss.
Contribution 1 - Neural Rendering Model: We develop the Neural Rendering Model (NRM)
whose bottom-up inference corresponds to a CNN architecture of choice (see Figure 1a). The “reverse”
1
Under review as a conference paper at ICLR 2019
image	unpooled	pooled	rectified
feature	feature	feature
map	map	map
0.5 dog
0.2 cat
0.1 horse
One Layer
Rendering
in NRM
object
category
One Layer
Inference
in UN
CNN: InferenCe
image at level '
NRM: Generation
Render Upsample, Choose
select
∣"	location
I y ¼
render
or not
Γ('
Is_render=YeS
ILoCation=LowerRight
latent
!variables
ReLUHIf render
MaXPool H Location
f feature map
1.0 dog
Zimage at level' -1
Convolution
W (')ɑ Γ> (')
/∕/ !/feature map
z^"— J at level ' -1
rendered upsampled masked class
image template template template
image IX
Noise
(a)
(b)
Figure 1: (a) The Neural Rendering Model (NRM) captures latent variations in the data and yields
CNNs as its inference. Costs for training CNNs are derived from likelihood estimations in NRM. (b)
Graphical model depiction of NRM. Latent variables in NRM depend on each other. Object category
y decides the class template, and then new latent variables are incorporated at each layer to render
intermediate images (red) with finer details. Finally, pixel noise is added to render the image x.
top-down process of image generation is through coarse-to-fine rendering, which progressively
increases the resolution of the rendered image. This is intuitive since the reverse process of bottom-up
inference reduces the resolution (and dimension) through operations such as spatial pooling. We
also introduce structured stochasticity in the rendering process through a small set of discrete latent
variables, which capture the uncertainty in reversing the CNN feed-forward process. The rendering
in NRM follows a product of linear transformations, which can be considered as the transpose of
the inference process in CNNs. In particular, the rendering weights in NRM are proportional to
the transposes of the filters in CNNs. Furthermore, the bias terms in the ReLU units at each layer
(after the convolution operator) make the latent variables in different network layers dependent
(when the bias terms are non-zero). This design of image prior has an interesting interpretation from
a predictive-coding perspective in neuroscience: the dependency between latent variables can be
considered as a form of backward connections that captures prior knowledge from coarser levels in
NRM and helps adjust the estimation at the finer levels (Rao & Ballard, 1999; Friston, 2018).
NRM is a likelihood-based framework, where unsupervised learning can be derived by maximizing
the expected complete-data log-likelihood of the model while supervised learning is done through
optimizing the class-conditional log-likelihood. Semi-supervised learning unifies both log-likelihoods
into an objective cost for learning from both labeled and unlabeled data. The NRM prior has the
desirable property of being a conjugate prior, which makes learning in NRM computationally efficient.
Interestingly, we derive the popular cross-entropy loss used to train CNNs for supervised learning
as an upper bound of the NRM’s negative class-conditional log-likelihood. This new interpretation
of cross-entropy allow us to develop better losses for training CNNs. An example is the Max-Min
cross-entropy discussed in Contribution 2 and Section 3.
Contribution 2 - New regularization, loss function, architecture and generalization bounds:
The joint nature of generation, inference, and learning in NRM allows us to develop new training
procedures for semi-supervised and supervised learning, as well as new theoretical (statistical)
guarantees for learning. In particular, for training, we derive a new form of regularization termed as
the Rendering Path Normalization (RPN) from the NRM’s conjugate prior. A rendering path is a set
of latent variable values in NRM. Unlike the path-wise regularizer in (Neyshabur et al., 2015), RPN
uses information from a generative model to penalizes the number of the possible rendering paths,
encouraging the network to be compact in terms of representing the image. It also helps enforce the
dependency among different layers in NRM during training and improves classification performance.
We provide a new theoretical bound based on NRM. In particular, we prove that NRM is statistically
consistent and derive a generalization bound of NRM for (semi-)supervised learning tasks. Our
generalization bound is proportional to the number of active rendering paths that generate close-to-real
images. This suggests that RPN regularization may help in generalization since RPN enforces the
dependencies among latent variables in NRM and, therefore, reduces the number of active rendering
paths. We observe that RPN helps improve generalization in our experiments.
Max-Min Cross-entropy and network: We propose the new Max-Min Cross-entropy loss function for
learning, based on negative class-conditional log-likelihood in NRM. It combines the traditional
cross-entropy with another loss, which we term as the Min cross-entropy. While the traditional
(Max) cross-entropy maximizes the probability of correct labels, the Min cross-entropy minimizes
2
Under review as a conference paper at ICLR 2019
the probability of incorrect labels. We show that the Max-Min cross-entropy is also an upper bound
to the negative conditional log-likehood of NRM, just like the cross-entropy loss. The Max-Min
cross-entropy is realized through a new CNN architecture, namely the Max-Min network, which is
a CNN with an additional branch sharing weights with the original CNN but containing minimum
pooling (MinPool) operator and negative rectified linear units (NReLUs), i.e., min(∙, 0) (see Figure 4).
Although the Max-Min network is derived from NRM, it is a meta-architecture that can be applied
independently on any CNN architecture. We show empirically that Max-Min networks and cross-
entropy help improve the SOTA on object classification for supervised and semi-supervised learning.
Contribution 3 - State-of-the-art empirical results for semi-supervised and supervised learn-
ing: We show strong results for semi-supervised learning over CIFAR10, CIFAR100 and SVHN
benchmarks in comparison with SOTA methods that use and do not use consistency regularization.
Consistency regularization, such as those used in Temporal Ensembling (Laine & Aila, 2017) and
Mean Teacher (Tarvainen & Valpola, 2017), enforces the networks to learn representation invariant
to realistic perturbations of the data. NRM alone outperforms most SOTA methods which do not use
consistency regularization (Salimans et al., 2016; Dumoulin et al., 2017) in most settings. Max-Min
cross-entropy then helps improves NRM’s semi-supervised learning results significantly. When
combining the NRM, Max-Min cross-entropy, and Mean Teacher, we achieve SOTA results or very
close to those on CIFAR10, CIFAR100, and SVHN (see Table 2, 3, and 4). Interestingly, compared
to the other competitors, our method is consistently good, achieving either best or second best results
in all experiments. Furthermore, Max-Min cross-entropy also helps supervised learning. Using the
Max-Min cross-entropy, we achieve SOTA result for supervised learning on CIFAR10 (2.30% test
error). Similarly, Max-Min cross-entropy helps improve supervised training on ImageNet.
Despite good classification results, there is a caveat that NRM may not generate good looking
images since that objective is not “baked” into its training. NRM is primarily aimed at improving
semi-supervised and supervised learning through better regularization. Potentially, an adversarial
loss can be added to NRM to improve visual characteristics of the image.
Related Work: In addition to GANs, other recently developed deep generative models include
the Variational Autoencoders (VAE) (Kingma & Welling, 2013) and the Deep Generative Net-
works (Kingma et al., 2014). Unlike these models, which replace complicated or intractable inference
by CNNs, NRM derives CNNs as its inference. This advantage allows us to develop better learning
algorithms for CNNs with statistical guarantees, as being discussed in Section 2.3 and 2.4. Recent
works including the Bidirectional GANs (Donahue et al., 2017) and the Adversarially Learned Infer-
ence model (Dumoulin et al., 2017) try to make the discriminators and generators in GANs reversible
of each other, thereby providing an alternative way to invert CNNs. These approaches, nevertheless,
still employ a separate network to bypass the irreversible operators in CNNs. NRM is also close in
spirit to the Deep Rendering Model (DRM) (Patel et al., 2016) and the Multi-layered Convolutional-
Sparse-Coding Model (ML-CSC) (Papyan et al., 2018) but markedly different. Compared to NRM,
DRM and ML-CSC have several limitations. In particular, latent variables in DRM are assumed to be
independent, which is rather unrealistic. This lack of dependency causes the missing of the bias terms
in the ReLUs of the CNN derived from DRM. Furthermore, the cross-entropy loss used in training
CNNs for supervised learning tasks is not captured naturally by DRM and ML-CSC. Due to these
limitations, model consistency and generalization bounds are not derived for DRM and ML-CSC.
Notation: To facilitate the presentation, the NRM’s notations are deferred to Table 5 in Appendix A.
2	The Neural Rendering Model
2.1	Generative Model
The NRM attempts to invert the CNNs as its inference by employing the structure of its latent
variables. The dependencies among latent variables in the model are implicitly captured by their
conjugate prior distribution. More precisely, NRM can be defined as follows:
Definition 2.1. [Neural Rendering Model (NRM)] NRM is a deep generative model in which the
山tent variables z(') = {t('), s(')}L=ι at different layers ' are dependent. Let X be the input image
and y ∈ {1, . . . , K} be the target variable, e.g. object category. Generation in NRM takes the form
∏z∣y , Softmax (J η(y, z)) ； z∣y 〜Cat(∏z∣y)	(1)
h(y, z;0) , A(ZI)A(Z;2)…A(Z L)μ(y); x|z,y ~ N(h(0),σ21D(0)),	⑵
3
Under review as a conference paper at ICLR 2019
render
at the
next
location
/7^7
h(' - 1)
.
.
.,Noise
Rendering /
X
Figure 3: Rendering from level' to level' 一 1 in the NRM. At each pixel P in the intermediate image
h('), NRM renders iff the template selects latent variable s(',p) = 1. If rendering, the template
Γ(',p) is multiplied by the pixel value h(',p). Then the matrix B(',p) zero-pads the result to the
size of the intermediate image at level' 一 1. Next, the translation matrix T(',p) locally translates
the rendered image to location specified by the latent variable t(',p). The same rendering repeats at
other pixels of h('). NRM adds all rendered images to obtain the intermediate image h(' 一 1).
where η(y,z)，PL=I hb(t;'), s(') Θ h(')i and Softmax (η)，exp(η)/ Pn exp(η).
The generation process in the NRM can be summarized in the
following steps (details are
1) Sample the latent variables Z from a cat-
egorical distribution whose prior is ∏z∣y.
2) Render its coarsest image, the object
template h(L) = μ(y) of class y. 3) Incor-
porate a set of of latent variations z(') into
h(y;') at each layer ' via a linear trans-
formation Λ(z;') to render the finer im-
age h(y, z;' - 1). 4) Add Gaussian pixel
noise into h(y, z; 0) to render the final im-
age x. In Eqn.2, if Λ(z; `) is a linear trans-
formation, it will yield many parameters
to learn. As a result, we would like to in-
troduce new structures into NRM so that
CNNs can be derived as NRM’s inference.
This way, NRM will be informed by the
in Algorithm 1 in Appendix A):
Layer 4 Layer 3 Layer 2 Layer 1 Layer 0 Original
Figure 2: Reconstructed images at each layer in a 5-
layer NRM trained on MNIST with 50K labeled data.
Original images are in the rightmost column. Early
layers in the rendering such as layer 4 and 3 capture
coarse-scale features of the image while later layers
such as layer 2 and 1 capture finer-scale features. From
layer 2, we begin to see the gist of the rendered digits.
prior knowledge of natural images captured by CNNs. In our attempt to invert CNNs, we constrain
the latent variables z(`) at layer ` in NRM to a set of template selecting latent variables s(`) and
local translation latent variables t(`). As been shown later in Section 2.2, during inference of NRM,
the ReLU non- linearity at layer ` “inverts” s(`) to find if particular features are in the image or not.
Similarly, the MaxPool operator “inverts” t(`) to locate where particular features, if exist, are in the
image. Both s(') and t(') are vectors indexed by p(') ∈ P(') ≡ {pixels in level'}. The rendering
process from layer ` to layer ` 一 1 is given by:
h(' - 1)，Λ(')h(')= V 仅八 s(',PyT(t; ',p)B(',p)Γ(',p)h(',p).	(3)
p∈P (`)
The rendering process in Eqn.3 is illustrated in Figure 3.
At each pixel P in the intermediate image h(`) at layer `, NRM decides to use that pixel to render
or not according to the value of the template selecting latent variable s(P; `) at that pixel location.
If s(P; `) = 1, then NRM renders; otherwise, it does not. If rendering, the pixel value h(`, P) is
used to scale the local rendering template Γ(', p), which has the same number of feature maps as the
next rendered image h(` 一 1), but is of smaller size, e.g., 3 × 3 or 5 × 5. Next, the padding matrix
n B(', P) pads the resultant patch to the size of the image h(' 一 1) with zeros, and the translation
matrix T(t; ',p) translates the result to a local location. NRM then keeps rendering at the next pixel
location P + 1 following the same process. All rendered images are added to form the final rendered
image h(` 一 1) at layer ` 一 1. Note that we can constrain NRM by enforcing all pixels in the same
feature maps of h(`) share the same rendering template. This constraint helps to yield convolutions
in CNNs during the inference of NRM, and the rendering template in NRM now corresponds to
the convolution filters in CNNs. Reconstructed images at each layer of a 5-layered NRM trained
on MNIST are visualized in Figure 2. The network is trained using the semi-supervised learning
framework discussed in Section 2.3.
4
Under review as a conference paper at ICLR 2019
While s(`) and t(`) can be independent, we
further constrain the model by enforcing the
dependency among s(`) and t(`) at different
layers in NRM. This constraint is motivated
from realistic rendering of natural objects: dif-
ferent parts of a natural object are dependent
on each other. NRM captures such depen-
dency in natural objects by imposing more
structures into the joint prior of latent vari-
ables at all layer in the model. The form of
thejoin prior ∏z∣y in Eqn. 1 might look mys-
terious at first, but NRM parametrizes ∏z∣y in
this particular way so that ∏z∣y is the conju-
gate prior of the model likelihood as being
proven in Appendix C.11. The conjugate form
of ∏z∣y allows efficient inference in the NRM.
Parameters b(t;') of the conjugate prior ∏z∣y
will become the bias terms after the CNN’s
convolutions as will be shown in Theorem 2.2.
When training in an unsupervised setup, the
conjugate prior results in the RPN regulariza-
tion as shown in Theorem 2.3(b). This RPN
regularization helps enforce the dependencies
among latent variables and increases the like-
lihood of latent configuration presents in the
data during training.
Table 1: Correspondence between NRM and CNN.
NRM (Generation) CNN (Inference)
Rendering templates Γ(')	Transpose of weights W (`)
Class templates μ(y)	Softmax weights
Parameters b(t; `) of the conjugate prior ∏y∣χ	Bias terms b(`) after each convolution
Intermediate image h(`) = h(y, z; `) of size D(') in layer '	Feature maps in layer ` in CNNs
Latent variables z(',p) = {s(',P),t(',P)} at pixel p, layer `	States of ReLUs & MaxPools
Max over template selecting variable s(`, p)	ReLU
Max over local translation variable t(`, p)	MaxPool
Zero-padding matrices B(',ρ) in layer '	Downsampling in MaxPool
Conditional log-likelihood	Cross-entropy loss
Expected complete-data log-likelihood	Reconstruction loss
NRM with skip connections: We derive ResNet and DenseNet by adding skip connections into the
rendering matrices Λ(') of NRM (see Appendix B).
2.2 Inference
We show that calculations in the bottom-up inference in NRM corresponds to a CNN (see Figure 1b)
and, therefore, is tractable and efficient. The impact of this correspondence goes beyond a reverse-
engineering effort. First, it provides probabilistic semantics for components in CNNs, justifying
their usage, and providing an opportunity to employ probabilistic inference methods in the context
of CNNs. Second, such a correspondence offers a flexible framework to design CNNs. Instead
of directly engineering CNNs for new tasks and datasets, we can modify NRM to incorporate our
knowledge of the tasks and datasets into the model and perform JMAP inference to achieve a new
CNN architecture. The following theorem establishes the NRM-CNN correspondence:
Theorem 2.2.	The JMAP inference of latent variable z in NRM is the feedforward step in CNNs.
Particularly, we have:
max {p(z∣x, y)} = max -ɪr {(h(y,z;0), Xi + η(y,z)} + const ≥ -ɪrhμ(y), Ψ(L)i + const (4)
z	z σ2	σ2
where ψ(L) is computed recursively. In particular, ψ(0) = x and
ψ(') = MaXPool(ReLU(Conv(Γ>('), ψ(' - 1))) + b(')).	(5)
The equality holds in Eqn. 4 when the parameters θ in NRM satisfy the non-negativity assumption
that the intermediate rendered image h(') ≥ 0, ∀' = 1, 2,...,L.
There are four key results in Theorem 2.2. First, ReLU non-linearities in CNNs find the optimal
value for the template selecting latent variables s(`) at each layer ` in NRM, detecting if particular
features exist in the image or not. Second, MaxPool operators in CNNs find the optimal value for the
local translation latent variables t(`) at each layer ` in NRM, locating where features are rendered in
the image. Third, bias terms after each convolution in CNNs are from the prior knowledge of latent
variables in the model. Those bias terms update the posterior estimation of latent variables from data
using the knowledge encoded in the prior distribution of those latent variables. Fourth, convolutions
in CNNs result from reversing the local rendering operator using template Γ(') in NRM. Instead of
rendering as in NRM, convolutions in CNNs perform template matching. The convolution weights
W(') in CNNs are proportional to the transposes of the rendering templates Γ('). The proofs for
these correspondences and derivations for leaky ReLU and batch normalization are in Appendix C.
5
Under review as a conference paper at ICLR 2019
2.3 Learning
Learning in NRM can be posed as likelihood estimation problems in which we find the optimal
values for parameters in NRM to optimize the appropriate likelihood functions. The optimization can
be done by gradient-based methods (Robbins & Monro, 1985). The following theorem derives the
learning objectives for NRM, and more details and proofs is provided in Appendix B and C.
Theorem 2.3.	For any n ≥ 1, let x1, . . . , xn be i.i.d. samples from the NRM. Assume that the final
rendered template h(y, z; 0) is normalized such that its norm is constant. The following holds:
(a)	Cross-entropy loss for training CNNs with labeled data
1n	1n
*ax -ɪ^logP(yiE，Zi； θ) ≥ max —£log q(y，E)
(zi)in=1,θ n i=1	θ ni=1
mAin Hp,q(y|x)
θ∈Aγ
(6)
	
where q(y|x) is the posterior estimated by CNN, and Hp,q (y |x) is the cross-entropy between q(y|x)
and the true posterior p(y|x) given by the ground truth.
(b)	Reconstruction loss with RPN for unsupervised training of CNNs with labeled and unlabeled data
min — XE [logp(xi, Zi∣yi)]	min — X kxi——Zi ； °)k——+ RPN, when σ → 0	(7)
θn	θn	2
i=1	i=1
where the latent variable zi is estimated by the CNN as described in Theorem 2.2, h(yi, zi; 0) is the
reconstructed image, and the RPN regularization is the negative log prior defined as follows:
1n	1n
RPN = -- X logp(zi∣yi) = -- X Softmax (η(yi,zi)).	(8)
nn
i=1
i=1
Cross-Entropy Loss for Training CNNs with Labeled Data: Theorem 2.3(a) establishes the
cross-entropy loss in the context of CNNs as an upper bound of the NRM’s negative conditional
log-likelihood Lsup := - n Pn=Ilogp(y∕χi, zi； θ). In contrast to other derivations of cross-entropy
loss via logistic regression, we derives the cross-entropy loss in conjunction with the architecture
of CNNs since the estimation of the optimal latent variables zi is part of the optimization in Eqn. 6.
In other word, Theorem 2.3(a) ties feature extraction and learning for classification in CNNs into
an end-to-end conditional likelihood estimation problem in NRM. This new interpretation of the
cross-entropy loss suggests an interesting direction in which better losses for training CNNs with
labeled data for supervised classification tasks can be derived from other upper bounds of Lsup . The
Max-Min cross-entropy in Section 3 is an example.
Reconstruction Loss with RPN Regularization: Theorem 2.3(b) suggests that NRM learns without
labels by maximizing its expected complete-data log-likelihood. One term in this objective function
is the reconstruction loss between the input image xi and the reconstructed template h(yi, zii； 0).
Another term is the RPN from Eqn. 8. RPN encourages the (yi, zii) inferred in the bottom-up E-step
to have higher prior among all possible values of (yi, zi) and, thanks to its structure, enforces the
dependencies among latent variables (s(`), t(`)) at different layers in NRM.
Semi-Supervised Learning in NRM: Let x1, . . . , xn be i.i.d. samples from NRM and assume that
the labels y1 , . . . , yn1 are unknown for some 0 ≤ n1 ≤ n, NRM determines optimal parameters
employed for the semi-supervised classification task via the following model:
min
θ
kxi - h(yi,zi; O)k2
2
+ RPN
n≡7. XH Jg qθ (yilxi)｝，
i=n1 +1
(9)
	
where αRC and αCE are non-negative weights associated with the reconstruction loss with RPN
regularization and the cross-entropy loss, respectively. Again, the optimal latent variables zii is
estimated by CNN as in Theorem 2.2. For unlabeled data, yi is set to the label estimated by CNN.
2.4 Generalization B ound for Classification
Our generalization bound for classification with NRM is proportional to the ratio of the number of
active rendering paths and the total number of rendering paths in the trained NRM. A rendering path
is a configuration of all latent variables in NRM, and active optimal rendering paths are those among
optimal rendering paths (yb, zb) whose corresponding rendered image is sufficiently close to one of the
data point from the input data distribution. Let LA and LD denote the population and empirical losses
on the data population A and the training set D of NRM, respectively. Our key result is summarized
below. More details and proofs are deferred to Appendix B and C.
6
Under review as a conference paper at ICLR 2019
Theorem 2.4.	Under the margin-based loss, with high probability, the following result on the
generalization bound of the classification framework with optimal solutions from Eqn. 9 holds:
LA ≤ LD + Tn∣L∣∕√n.
Here, Tn ∈ (0,1) denotes the ratio of active optimal rendering paths among all the optimal rendering
paths, |L| is the total number of rendering paths, and n is the number of training data samples. The
dependence of generalization bound on the number of active rendering paths Tn|L| helps tojustify
our modeling assumptions. In particular, NRM helps to reduce the number of active rendering paths
thanks to the dependencies among its latent variables, thereby tightening the generalization gap.
Nevertheless, there is a limitation regarding the current generalization bound. In particular, the bound
involves the number of rendering paths |L|, which is usually large. This is mainly because our bound
has not fully taken into account the structure of CNNs, which is the limitation shared among other
latest generalization bounds for CNN. It is interesting to explore if techniques in works by Bartlett
et al. (2017) and Golowich et al. (2018) can be employed to improve the term |L| in our bound.
3	New Max-Min Cross Entropy From The Neural Rendering Model
In this section, we explore a particular way to derive an alternative to cross-entropy inspired by
the results in Theorem 2.3(a). In particular, denoting zmax , arg maxz {hh(y, z; 0), xi + η(y, z)}
and Zmm，argmi□z {〈h(y, z; 0), Xi + η(y, z)}, the new cross-entropy HM&M, which is called the
Max-Min cross-entropy, is the weighted average of the cross-entropy losses from Zmax and zmin:
HM&M , αmaxHp,q(y∣x,zmax) + αminHp,q(y|x, Zmin) = αmaxHmqx(y∣x) + αminHmin(y∣x).
Here the Max cross-entropy Hmqx and Min cross
entropy HmlqI maximizes the correct target poste-
rior and and minimizes the incorrect target pos-
terior, respectively. Similar to the cross-entropy
loss, the Max-Min cross-entropy can also be
shown to be an upper bound for the negative
conditional log-likelihood LSUP of the NRM and
has the same generalization bound derived in
Section 2.4. The Max-Min networks in Figure 4
Shared weights
Figure 4: The Max-Min network
MaX-Min
XentroPy
Min
XentroPy
realize this new loss. These networks have two CNN-like branches that share weights. The max
branch estimates Zmax using ReLU and Max-Pooling, and the min branch estimates zmin using the
Negative ReLU, i.e., min(∙, 0), and Min-Pooling. The Max-Min networks can be interpreted as
a form of knowledge distillation like the Born Again networks (Furlanello et al., 2018)and the
Mean Teacher networks. However, instead of a student network learning from a teacher network, in
Max-Min networks, two students networks, the Max and the Min networks, cooperate and learn from
each other during the training.
4	Experiments
4.1	Semi-Supervised Learning
We show NRM armed with Max-Min cross-entropy and Mean Teacher regularizer achieves SOTA
on benchmark datasets. We discuss the experimental results for CIFAR10 and CIFAR100 here. The
results for SVHN, training losses, and training details, can be found in the Appendix A & D.
CIFAR-10: Table 2 shows comparable results of NRM to SOTA methods. NRM is also better
than the best methods that do not use consistency regularization like GAN, Ladder network, and
ALI when using only Nl=2K and 4K labeled images. NRM outperforms DRM in all settings.
Also, among methods in our comparison, NRM achieves the best test accuracy when using all
available labeled data (Nl=50K). NRM has the advantage over consistency regularization methods
like Temporal Ensembling and Mean Teacher when there are enough labeled data because the
consistency regularization in those methods tries to match the activations in the network, but does not
take into account the available class labels. On the contrary, NRM employs the class labels, if they
are available, in its reconstruction loss and RPN regularization as in Eqns. 7 and 8. In all settings,
RPN regularizer improves NRM performance. Even though the improvement from RPN is small,
it is consistent across the experiments. Furthermore, using Max-Min cross-entropy significantly
reduces the test errors. When combining with Mean-Teacher, our Max-Min NRM improves upon
Mean-Teacher and consistently achieves either SOTA results or second best results in all settings.
7
Under review as a conference paper at ICLR 2019
Table 2: Error rate percentage on CIFAR-10 over 3 runs.
	1K labels 50K images	2K labels 50K images	4K labels 50K images	50K labels 50K images
Adversarial Learned Inference (Dumoulin et al., 2017)	19.98 ± 0.89	19.09 ± 0.44	17.99 ± 1.62	
Improved GAN (Salimans et al., 2016)	21.83 ± 2.01	19.61 ± 2.09	18.63 ± 2.32	
Ladder Network (Rasmus et al., 2015)			20.40 ± 0.47	
Π model (Laine & Aila, 2017)	27.36 ± 1.20	18.02 ± 0.60	13.20 ± 0.27	6.06 ± 0.11
Temporal Ensembling (Laine & Aila, 2017)			12.16 ± 0.31	5.60 ± 0.10
Mean Teacher (Tarvainen & Valpola, 2017)	21.55 ± 1.48	15.73 ± 0.31	12.31 ± 0.28	5.94 ± 0.15
VAT+EntMin (Miyato et al., 2018)			10.55	
DRM (Patel et al., 2016; 2015)	27.67 ± 1.86	20.71 ± 0.30	15.36 ± 0.34	5.75 ± 0.24
Supervised-only	46.43 ± 1.21	33.94 ± 0.73	20.66 ± 0.57	5.82 ± 0.15
NRM without RPN	24.88 ± 0.76	18.97 ± 0.80	14.41 ± 0.19	5.57 ± 0.07
NRM+RPN	24.48 ± 0.43	18.62 ± 0.70	14.18 ± 0.46	5.35 ± 0.08
NRM+RPN+Max-Min	21.55 ± 0.46	16.24 ± 0.17	12.50 ± 0.35	4.85 ± 0.10
NRM+RPN+Max-Min+Mean Teacher	19.79 ± 0.74	15.11 ± 0.51	11.81 ± 0.13	4.88 ± 0.09
This consistency in performance is only observed in our method and Mean-Teacher. Also, like with
Mean-Teacher, NRM can potentially be combined with other consistency regularization methods,
e.g., the Virtual Adversarial Training (VAT) (Miyato et al., 2018), to obtain better results.
CIFAR-100: Table 3 shows NRM’s comparable results to Π model and Temporal En-
sembling, as well as better results than DRM. Same as with CIFAR10, using the RPN
regularizer results in a slightly better test accuracy, and NRM achieves better results
than Π model and Temporal Ensembling method when using all available labeled data.
Notice that combining with
Mean-Teacher just slightly
improves NRM’s performance
when training with 10K labeled
data. This is again because con-
sistency regularization methods
like Mean-Teacher do not add
much advantage when there are
enough labeled data. However,
NRM+Max-Min still yields
Table 3: Error rate percentage on CIFAR-100 over 3 runs.
10K labels	50K labels
50K images	50K images
Π model (Laine & Aila, 2017)	39.19 ± 0.36	26.32 ± 0.04
Temporal Ensembling (Laine & Aila, 2017)	38.65 ± 0.51	26.30 ± 0.15
DRM (Patel et al., 2016; 2015)	41.09 ± 0.31	27.06 ± 0.19
Supervised-only	44.56 ± 0.30	26.42 ± 0.17
NRM without RPN	40.70 ± 1.13	26.27 ± 0.09
NRM+RPN	39.85 ± 0.46	25.84 ± 0.10
NRM+RPN+Mean Teacher	39.84 ± 0.32	25.98 ± 0.35
NRM+RPN+Max-Min	37.75 ± 0.66	24.38 ± 0.29
better test errors and achieves SOTA result in all settings.
4.2	Supervised Learning with Max-Min Cross-Entropy
The Max-Min cross-entropy can be applied not only to improve semi-supervised learning on deep
models including CNNs but also to enhance their supervised learning performance. In our ex-
periments, we indeed observe Max-Min cross-entropy reduces the test error for supervised object
classification on CIFAR10. In particular, using the Max-Min cross-entropy loss on a 29-layer ResNet
(Xie et al., 2017) trained with the Shake-Shake regularization (Gastaldi, 2017) and Cutout data aug-
mentation (DeVries & Taylor, 2017), we are able to achieve SOTA test error of 2.30% on CIFAR10,
an improvement of 0.26% over the test error of the baseline architecture trained with the traditional
cross-entropy loss. While 0.26% improvement seems small, it is a meaningful enhancement given that
our baseline architecture (ResNeXt + Shake-Shake + Cutout) is the second best model for supervised
learning on CIFAR10. Such small improvement over an already very accurate model is significant in
applications in which high accuracy is demanded such as self-driving cars or medical diagnostics.
Similarly, we observe Max-Min improves the top-5 test error of the Squeeze-and-Excitation ResNeXt-
50 network (Hu et al., 2018) on ImageNet by 0.17% compared to the baseline (7.04% vs. 7.21%). For
a fair comparison, we re-train the baseline models and report the scores in the re-implementation.
5 Conclusions
We present the NRM, a general and an effective framework for semi-supervised learning that combines
generation and prediction in an end-to-end optimization. Using NRM, we can explain operations
used in CNNs and develop new features that help learning in CNNs. For example, we derive the
new Max-Min cross-entropy loss for training CNNs, which outperforms the traditional cross-entropy.
Despite promising results in this paper, NRM can still be improved. For instance, an adversarial
loss like in GANs can be incorporated into NRM so that the model can generate realistic images.
Furthermore, more knowledge of image generation from graphics and physics can be integrated in
NRM so that the model can employ more structures to help learning and generation.
8
Under review as a conference paper at ICLR 2019
References
P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural
networks. Advances in Neural Information Processing Systems (NIPS), 2017.
D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association, 112(518):859-877, 2017.
T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout.
arXiv preprint arXiv:1708.04552, 2017.
L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Stochastic
Modelling and Applied Probability, Springer, 1996.
J. Donahue, P. Krahenbuhl, and T. Darrell. Adversarial feature learning. In International Conference
on Learning Representations, 2017.
R. Dudley. Central limit theorems for empirical measures. Annals of Probability, 6, 1978.
V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and A. Courville.
Adversarially learned inference. In International Conference on Learning Representations, 2017.
K. Friston. Does predictive coding have a future? Nature Neuroscience, 21(8):1019-1021, 2018. doi:
10.1038/s41593-018-0200-7.
T. Furlanello, Z. C. Lipton, M. Tschannen, L. Itti, and A. Anandkumar. Born-again neural networks.
Proceedings of the International Conference on Machine Learning (ICML), 2018.
X. Gastaldi. Shake-shake regularization of 3-branch residual networks. In International Conference
on Learning Representations, 2017.
N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.
Proceedings of the Conference On Learning Theory (COLT), 2018.
I.	Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27,
pp. 2672-2680. 2014.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
J.	Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional
networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
D. P. Kingma, S. Mohamed, D. Jimenez Rezende, and M. Welling. Semi-supervised learning with
deep generative models. In Advances in Neural Information Processing Systems 27, pp. 3581-3589.
2014.
V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization
error of combined classifiers. Annals of Statistics, 30, 2002.
A. Kumar, P. Sattigeri, and T. Fletcher. Semi-supervised learning with gans: Manifold invariance with
improved inference. In Advances in Neural Information Processing Systems 30, pp. 5534-5544.
2017.
S.	Laine and T. Aila. Temporal ensembling for semi-supervised learning. In International Conference
on Learning Representations, 2017.
9
Under review as a conference paper at ICLR 2019
I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International
Conference on Learning Representations, 2016.
T.	Miyato, S.-i. Maeda, S. Ishii, and M. Koyama. Virtual adversarial training: a regularization method
for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine
intelligence, 2018.
B.	Neyshabur, R. R. Salakhutdinov, and N. Srebro. Path-sgd: Path-normalized optimization in deep
neural networks. In Advances in Neural Information Processing Systems, pp. 2422-2430, 2015.
T. Nguyen, W. Liu, E. Perez, R. G. Baraniuk, and A. B. Patel. Semi-supervised learning with the
deep rendering mixture model. arXiv preprint arXiv:1612.01942, 2016.
V. Papyan, Y. Romano, J. Sulam, and M. Elad. Theoretical foundations of deep learning via
sparse representations: A multilayer sparse model and its connection to convolutional neural
networks. IEEE Signal Processing Magazine, 35(4):72-89, July 2018. ISSN 1053-5888. doi:
10.1109/MSP.2018.2820224.
A. B. Patel, T. Nguyen, and R. G. Baraniuk. A probabilistic theory of deep learning. arXiv preprint
arXiv:1504.00641, 2015.
A. B. Patel, M. T. Nguyen, and R. Baraniuk. A probabilistic framework for deep learning. In
Advances in Neural Information Processing Systems 29, pp. 2558-2566. 2016.
R. P. N. Rao and D. H. Ballard. Predictive coding in the visual cortex: a functional interpretation of
some extra-classical receptive-field effects. Nature Neuroscience, 2:79 EP -, 01 1999.
A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko. Semi-supervised learning with
ladder networks. In Advances in Neural Information Processing Systems 28, pp. 3546-3554. 2015.
H. Robbins and S. Monro. A stochastic approximation method. In Herbert Robbins Selected Papers,
pp. 102-109. Springer, 1985.
T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen, and X. Chen. Improved
techniques for training gans. In Advances in Neural Information Processing Systems 29, pp.
2234-2242. 2016.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and
A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1-9, 2015.
A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency tar-
gets improve semi-supervised deep learning results. In Advances in Neural Information Processing
Systems 30, pp. 1195-1204. 2017.
S. van de Geer. Empirical Processes in M-estimation. Cambridge University Press, 2000.
R.	Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv:1011.3027v7,
2011.
S.	Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggregated residual transformations for deep neural
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 5987-5995. IEEE, 2017.
10
Under review as a conference paper at ICLR 2019
Supplementary Material
Appendix A
This appendix contains several key tables with simulation studies and notations as well as figures for
the connection between NRM and CNN in the main text.
Semi-Supervised Learning Results on SVHN
Table 4: Error rate percentage on SVHN in comparison with other state-of-the-art methods. All
results are averaged over 2 runs (except for NRM+RPN when using all labels, 1 run)
	250 labels 73257 images	500 labels 73257 images	1000 labels 73257 images	73257 labels 73257 images
ALI Dumoulin et al. (2017)			7.42 ± 0.65	
Improved GAN (Salimans et al., 2016)		18.44 ± 4.8	8.11 ± 1.3	
+ Jacob.-reg + Tangents (Kumar et al., 2017)		4.87 ± 1.60	4.39 ± 1.20	
Π model (Laine & Aila, 2017)	9.69 ± 0.92	6.83 ± 0.66	4.95 ± 0.26	2.50 ± 0.07
Temporal Ensembling (Laine & Aila, 2017)		5.12 ± 0.13	4.42 ± 0.16	2.74 ± 0.06
Mean Teacher (Tarvainen & Valpola, 2017)	4.35 ± 0.50	4.18 ± 0.27	3.95 ± 0.19	2.50 ± 0.05
VAT+EntMin (Miyato et al., 2018)			3.86	
DRM (Nguyen et al., 2016)		9.85	6.78	
Supervised-only	27.77 ± 3.18	16.88 ± 1.30	12.32 ± 0.95	2.75 ± 0.10
NRM without RPN	9.78 ± 0.24	7.42 ± 0.61	5.64 ± 0.13	3.46 ± 0.04
NRM+RPN	9.28 ± 0.01	6.56 ± 0.88	5.47 ± 0.14	3.57
NRM+RPN+Max-Min+MeanTeacher	3.97 ± 0.21	3.84 ± 0.34	3.70 ± 0.04	2.87 ± 0.05
Rendering Process in NRM
Algorithm 1 Rendering Process in NRM
Input: Object category y .
Output: Rendered image x given the object category y.
Parameters: θ =( {μ(y)}K=ι, {Γ(')}L=ι, {b(')}L=J where μ(y) is the class template, Γ(') is
the rendering template at layer `, and b(`) is the parameters of the conjugate priorp(z|y, x) at layer
`, which turn out to be the bias terms in the ReLU after convolutions at each layer in CNNs.
1.	Use Markov chain Monte Carlo method to sample the latent variables z(`) in NRM, ` =
1, 2,...,L from ∏z∣y = Softmax (σ2η(y,z)), where η(y,z)，PL=I hb(t;'), s(') Θ h(')i.
2.	Render h(`), `	=	0, 1, . . . , L - 1 using the recursion h(` - 1)	=
Pp∈P(`) s(',p)T(t; ',p)B(',p)Γ(',p)h(',p) in Eqn. 3, in which h(L) = μ(y) and T(t; ',p)
and B(',p) are the local translation matrix and the zero-padding matrix at pixel location P in layer
` as described above.
3.	Add Gaussian pixel noise N(0, σ21D(0)) into h(0) to achieve the final rendered image x, where
D(0) is the dimension, i.e. the number of pixels, of h(0), as well as ofx.
11
Under review as a conference paper at ICLR 2019
Table 5: Table of notations for NRM
Variables
x , input image of size D(0)
y , object category
z(') = {s('), t(')}	， all latent variables of size D(') in level'
s(`, p)	,	switching latent variable at pixel location p in level `
t(`, p)	,	local translation latent variable at pixel location p in level	`
h(y, z;')	= h(')	，	intermediate rendered image of size D(') in level'
h(y, z;	0)	= h(0)	,	rendered image of size D(0) from NRM before adding noise
ψ(')	， corresponding feature maps in layer ' in CNNs.
Parameters
μ(y) = h(y; L) = h(L)
Λ(')
Γ(')
W (`) = Γ> (`, p)
B(')
T (')
b(t; `) = b(`)
σ2
η(y, z)
Softmax (η)
RPN
(y,z(L), . . . , z(1))
, the template of class y, as well as the coarsest image of size D(L)
determined by the category y at the top of NRM before adding any fine
detail. μ(y) is learned from the data.
, rendering matrix of size D(' _ 1) X D(') at layer '.
, dictionary of D(') rendering template Γ(',p) of size F(') × 1 at layer
'.Γ(') is learned from the data.
, corresponding weight at the layer ` in CNNs
, set of zero-padding matrices B(',p) ∈ RD('-1)×F(')) at layer '
, set of local translation matrices T(',p) ∈ RD('-1)×D('-1)) at layer '.
T(', P) is chosen according to value of t(', P)
, parameter of the conjugate prior p(z|x, y) at layer `. This term is of size
D(') and becomes the bias term after convolutions in CNNs. It can be
made independent of t, which is equivalent to using the same bias in
each feature map in CNNs. Here, b(t; `) is learned from data.
, probability of object category y .
, pixel noise variance
Other NOtatiOns
,PL=1 表hb(t; '),s(') © h(')i.
Δ exp(η)
一	P exp(η0).
0
η
nn
,—1 P logp(zi⅜i) = -1 P SOftmaX(η(yi,zΓ)).
i=1	i=1
, rendering configuration.
12
Under review as a conference paper at ICLR 2019
Appendix B
In this appendix, we give proofs for Theorem 2.3. We also give further connection of NRM to cross
entropy as well as additional derivation of NRM to various models under both unsupervised and (semi)-
supervised setting of data being mentioned in the main text. We also formally present our results on
consistency and generalization bounds for NRM in supervised and semi-supervised learning settings.
In addition, we explain how to extend NRM to derive ResNet and DenseNet. For the simplicity of
the presentation, We denote θ =( {μ(y)}K=ι, {Γ(')}L=ι, {∏y}^=ι, {b(')}L=ι) to represent all the
parameters that we would like to estimate from NRM where L is the set of all possible values of latent
(nuisance) variables Z = (t('), s('))L=、. Additionally, for each (y, Z) ∈ J := {1,..., K} X L, we
denote θy,z = (μ(y), {Γ(')}L=ι, {b(')}), i.e., the subset of parameters corresponding to specific
label y and latent variable Z. Furthermore, to stress the dependence of η(y, Z) on θ, we define the
following function
LL
T(θy,z) := η(y,z) = X b>(')(s(') © z(')) = X b>(')M(s; ')z(')
'=1	'=1
for each (y, Z) ∈ J where M(s; `) = diag(s(`)) is a masking matrix associated with s(`). Through-
out this supplement, we will use τ (θy,z) and η(y, Z) interchangeably as long as the context is clear.
Furthermore, we assume that Γ(') ∈ θ`, which is a subset of RF⑶×D⑶ for any 1 ≤ ' ≤ L,
μ(y) ∈ Ω, which is a subset of RD('), for 1 ≤ y ≤ K, and b(t;') ∈ Ξ('), which is a subset of RD(')
for all choices of t(`) and 1 ≤ ` ≤ L. Last but not least, we say that θ satisfies the non-negativity
assumption if the intermediate rendered images z(') satisfy z(') ≥ 0 for all 1 ≤ ' ≤ L.
Notation: In this appendix, we use A> to denote transpose of the matrix A.
.1 Connection between NRM and cross entropy
As being established in part (a) of Theorem 2.3, the cross entropy is the upper bound of the NRM’s
negative conditional log likelihood. In the following full theorem, we will show both the upper bound
and the lower bound of maximizing the conditional log likelihood in terms of the cross entropy.
Theorem .1. Given any γ > 0, we denote Aγ = {θ : kh(y, Z; 0)k = γ}. For any n ≥ 1 and σ > 0,
let x1 , . . . , xn be i.i.d. samples from the NRM. Then, the following holds
(a)	(Lower bound)
1n
max - V"logp(yi∣Xi,Zi; θ)
(zi)in=1,θ∈Aγ n i=1
1n
≥ max 一 log I Softmax
θ∈Aγ n
i=1
1n
= max —>Zlog q(yi∣χi)=
θ∈Aγ n
i=1
max
zi
h>(yi,Zi;0)xi + η(yi, Zi)
mAnr Hp,q(y1x)
σ2
+ byi
	
where by = log πy for all 1 ≤ y ≤ K, q(y|x) = Softmax max
z；0)x + η(y,z) /σ2 + by
for all (x, y), and Hp,q (y|x) is the cross-entropy between the estimated posterior q(y|x) and the true
posterior given by the ground-true labels p(y|x).
(b)	(Upper bound)
1n
max — Tlog p(yi∣χi, Zi ； θ)
(zi)in=1,θ∈Aγ n i=1
1
≤ max 一
θ∈Aγ n
log q(yi |xi ) + max max
y zi
h> (y, Zi; 0)xi + η(y, Zi)
σ2
	
h>(y,Zi；0)Xi + η(y,Zi)
σ2
+ log K
13
Under review as a conference paper at ICLR 2019
1	—	/ I	λ∖ r r /	/
where Zi = arg maxp(y∕xi, Zi； θ) for 1 ≤ i ≤ n.
zi
Remark .2. As being demonstrated in Theorem 2.2, max h> (y, Z; 0)x + η(y, Z) /σ2, approxi-
mately, has the form of a CNN. If we further have the non-negativity assumption with θ, then this is
exact. Therefore, the cross entropy Hp,q obtained in Theorem .1 has a strong connection with CNN.
Remark .3. The gap between the upper bound and the lower bound of maximizing the conditional
log likelihood in terms of cross entropy function suggests how good the estimation in Theorem .1 is.
In particular, this gap is given by:
1 XX ʃ max( max(
n i=1t、y ∖ zi ∖
h> (y,zi；0)xi + η(y,Zi)∖	h>(y,Zi；0)Xi + η(y,zi)
σ2
{^^^^™
trade-off loss
σ2
+ log K,
	
}
where Zi = arg maxp(yi∣xi, Zi； θ) for 1 ≤ i ≤ n. As long as the number of labels is not too large
zi
and the trade-off loss is sufficiently small, the gap between the upper bound and the lower bound in
Theorem .1 is small.
.2 Learning in the NRM without noise for unsupervised setting under
non-negativity assumption
To ease the presentation of the inference with NRM without noise, we first assume that rendered
images h(y, Z； 0) satisfy the non-negativity assumption (Later in Section .3, we will discuss the relax-
ation of this assumption for the inference of the NRM). With this assumption, as being demonstrated
in Theorem 2.2, we have:
max h> (y, Z； 0)x + η(y, Z) = max h>(y)ψ(L)
y,z	y
where we define
(10)
ψ(L) = maxΛ>(Z(L)) max
z(L)	z(L-1)
z; L — 1)…(maxΛ>(z; 1)x + b(1)J ...) + b(L — 1)
+b(`).
Now, we will provide careful derivation of part (b) of Theorem 2.3 in the main text. Remind that, for
the unsupervised setting, we have data x1, . . . , xn are i.i.d. samples from NRM. The complete-data
log-likelihood of the NRM is given as follows:
n
Ey,z [log p(x, (y, Z))] = Σ Σ P (y, Z|xi){log πy,z + log N (xi|h(y, Z；0))}
i=1 (y,z)∈J
where we have
P (y, Z|xi)
∏y,z N (xi|h(y,z;0))
πy0,z0N (xi|h(y0, Z0； 0))
(y0,z0)∈J
πy exp
kxi - h(y,Z；0)k2 - 2η(y, Z)
2σ2
πy0 exp
(y0,z0)∈J
kxi - h(y0,z0;0)k2 - 2η(y0,z0)∖ '
2σ2
At the zero-noise limit, i.e., σ → 0, it is clear that P(y, Z|xi) = 1 as (y, Z) = arg min kxi -
(y0,z0)∈J
h(y0, Z0； 0)k2 - 2η(y0, Z0) and P(y, Z|xi) = 0 otherwise. Therefore, we can asymptotically view
the complete log-likelihood of the NRM under the zero-noise limit as
n
X X	ry,z
i=1 (y,z)∈J
log ∏y,z - 2 l∣Xi - h(y,z; 0)k2
n1
X X - 2 3I
i=1 (y,z)∈J
X---------
n
xn - h(y, Z； 0)l2 +Σ Σ ry,z log πy,z
^~^^^^^{^^^^^^^—
Reconstruction Loss
i=1 (y,z)∈J
J 、---------------{-------------}
Path Normalization Regularizer
	
	
14
Under review as a conference paper at ICLR 2019
where
1, if (y, z) = arg min kxi - h(y0,z0;0)k2 - 2η(y0, z0)
ry,z ≡	(y0,z0)∈J
[0, otherwise
With the above formulation, we have the following objective function
1n	1	2
Un = mθin n £ E ry,z ( 2 kxi - h(y, z; O) k2 - log πy,z)	(II)
i=1 (y,z)∈J
where θ =( {μ(y)}K=ι, {Γ(')}L=ι, {∏y}y=ι, {b(')}L=1} We call the above objective function to
be unsupervised NRM without noise.
Relaxation of unsupervised NRM without noise Unfortunately, the inference with unsupervised
NRM without noise is intractable in practice due to two elements: the involvement of kh(y0, z0; 0)k2
to determine the value of ry,z	and the summation P	exp(η(y0, z0) +log πy0) in the denominator
(y0 ,z0 )∈J
of πy,z	for all (y, z) ∈ J. Therefore, we need to develop a tractable version of this objective function.
Theorem .4. (RelaxationofunsupervisedNRMwithoutnoise)Assumethat ∏y ≥ Yforall 1 ≤ y ≤ K
for some given Y ∈ (0,1/2). Denote
i ι XX X
θ n i=ι (y0,z0)∈j I (y0,z0)=argmaχ( h>(y,z；0)xi+n(y,z)
kxi - h(y0,z,0)k2
2
- log(πy0,z0 )
where py0,z0 = exp η(y0, z0) + log πy0 / P exp m∈aLx η(y, z) +logπy	for all (y0, z0) ∈ J.
For any θ, we define
®i,Zi) = argmin{ ∣∣Xi - h(y, z；O)k2 - 2η(y,z)
(y,z)∈J
and
(yei, zei) = arg max
(y,z)∈J
z; 0)xi + η(y, z)
as 1 ≤ i ≤ n. Then, the following holds
(a) Upper bound:
Un ≤
1
min 一
θn
kxi - h(yei, zei; 0)k2
2
-l°g(πei,ei)J + Cogπei - logπyJ + + log |L|
S---------------------------{----------'
prior loss
≤
Vn + log(Y - 1)+ log |L|
(b) Lower bound:
1
min 一
θn
kxi - h(yei, zei;0)k2
≥
2
-log(πei,ej) + Cog πy - log πei
S-------------------------------
}
""^^^^^^^^^^{^^^^^^^^^"""
prior loss
+ 2 (kh(yi,Zi；0)k2-kh(ei,ei；0)k2
S-------------------------------------
}
^^^{^^^~
norm loss
≥ Vn+log(ι-^) + min n X 1 (kh(yi,zi；O)k2 -kh(ei,zi；O)k2
15
Under review as a conference paper at ICLR 2019
Unlike Un , the inference with objective function of Vn is tractable. According to the upper bound
and lower bound of Un in terms of Vn, we can use Vn as a tractable approximation of Un for the
inference purpose with unsupervised setting of data when the noise is treated to be 0. Therefore, we
achieve the conclusion of part (b) of Theorem 2.3 in the main text. The algorithm for determined
(local) minima of Vn is summarized in Algorithm 2.
.3 Relaxation of non-negativity assumption with rendered images
It is clear that the inference with Vn relies on the non-negativity assumption such that equation
equation 10 holds. Now, we will argue that when the non-negativity assumption with rendered images
h(y, z; 0) does not hold, we can relax Vn to a more tractable version under that setting.
Theorem .5. (Relaxation of objective function Vn when non-negativity assumption does not hold)
Assume that ∏y ≥ Y for all 1 ≤ y ≤ K for some given Y ∈ (0,1/2). Denote
nK
Wn:= minnXX 1( 0	(	)
θ n i=1 y0=1	y0=arg max g(y,xi)
kxi - g(y0,zi)k2
-log(πy0,Zi )
2
where
g(y,x) = h>(y) MaxPoolI ReLu(Conv ( Γ('),…MaxPool I ReLu I Conv(r⑴，I)+ b(1)
...+
forall (x, y). Additionally, Zi is the maximal value of z in the CNNstructure of g(y, Xi) for 1 ≤ i ≤ n.
For any θ, we define
(yei, zei) = arg max
(y,z)∈J
z; 0)xi + η(y, z)
and yi = arg max g(y, xi) as 1 ≤ i ≤ n. Then, thefolloWing holds
y
(a)	Upper bound:
Vn	≤
1
mm 一
θn
kxi - h(yi,zi;0)k2
-log(πFi,Zi)J + Cog πy - log
、---------------
prior loss
+ 1 (kh(ei,ei;0)k2-kh(yi,Zi;0)k2
X-------------------------------
-z
norm loss
Wn +log(1 — 1)+min1 X 1 fkh(ei,Zi;0)k2 - kh(yi,Zi;0)k2
S ) θ n i=1 2 ∖
(b)	Lower bound:
Vn ≥ min1
θn
kxi - h⅛i,zκ0)k2
-l°g(πyi,Zi)J + Cogπy - logπeJ
X-------------------------------------}
prior loss
+1 (kh(ei,ziMk2 - kh(yi,zi;0)k2) + (g(yi,xi) - {h(yi,zi;0)>xi + η(yi,ei)}
X----------------------------------} X--------------------------------------
^^^{^^^≡
norm loss
^^^{^^^≡
CNN loss
≥ Wn + l°g(1-^) +min 1X 1 (kh(ei,e30)k2 - kh(yi,zi;0)k2
+ min 1 X (g(yi, xi) — {h(ei, ei; 0)>xi + η(ei, Zi)})
≤
2
2
}
}
}
16
Under review as a conference paper at ICLR 2019
Algorithm 2 Relaxation of unsupervised NRM without noise
Input: Data Xi, translation matrices T(t;'), zero padding matrices B('), number of labels K,
number of layers L.
Output: Parameters θ.
Initialize θ = ( {μ(y)}K=ι，H(')}3，{∏y}%ι，{b(')}L=ι) ∙
while θ has not converged do
1.	E-Step: Update labels (y, z) of each data
for i = 1 to n do
(ybi, zbi) = arg max h> (y, z)xi + log(πy,z) .
y,z
end for
2.	M-Step: By using Stochastic Gradient Descent (SGD), update θ that minimizes
P “xi — h(bi,bi32	] / 八
Σ I---------2---------log(πbi,bi)).
end while
The proof argument of the above theorem is similar to that of Theorem .4; therefore, it is omitted.
The upper bound and lower bound of Vn in terms of Wn in Theorem .5 implies that we can use Wn
as a relaxation of Vn when the non-negativity assumption with rendered images h(y, z; 0) does not
hold. The algorithm for achieving the (local) minima of Wn is similar to Algorithm 2.
.4 NRM with (semi)- supervised setting
In this section, we consider the application of NRM to the (semi)-supervised setting of the data.
Under that setting, only a (full) portion of labels of data x1 , . . . , xn is available. Without loss of
generality, we also assume that the rendering path h(y, z; 0) satisfies the non-negativity assumption.
For the case that h(y, z; 0) does not satisfy this assumption, we can argue in the same fashion as
that of Theorem .5. Now, we assume that only the labels (yn1+1, . . . , yn) are unknown for some
n1 ≥ 0. When n1 = 0, we have the supervised setting of data while we have the semi-supervised
setting of data when n — n1 is small. Our goal is to build a semi-supervised model based on NRM
such that the clustering information from data x1 , . . . , xn1 can be used efficiently to increase the
accuracy of classifying the labels of data xn1+1, . . . , xn. For the sake of simple inference with that
purpose, we only consider the setting of NRM when the noise goes to 0. Our idea of constructing
the semi-supervised model based on NRM is insprired by an approximation of the upper bound of
maximizing the conditional log likelihood of NRM in terms of the cross entropy and reconstruction
loss in part (b) of Theorem 2.3. In particular, we combine the tractable version of reconstruction loss
from the unsupervised setting in Theorem 2.3b and the cross entropy of approximate posterior in
Theorem 2.3a, which can be formulated as follows
n1
mjn 等 X X 1(	(
i i=1 (y0 ,z0)∈J ( (y0,z0)=arg max ( h>(y,z;O)xi+τ (θy,z)
n
+ X X1(
、i=nι + 1 z0∈L J z0=arg max ( h>(yi,z;0)Xi+T 包2)
n
αCE
n — n1
E log qθ (yi|xi)
i=n1+1
kxi - h(y0,z,0)∣∣2
2
kxi — h^i,z,o)k2
2
where αRC and αCE are non-negative weights associated with reconstruction loss and cross entropy
respectively. Additionally, the approximate posterior qθ (y|xi) is chosen as
exp m∈aLx h> (y, z; 0)xi + τ (θy,z) +logπy
qθ (y|xi) = -K	7	7	∖
P exp max h> (y0, z; 0)xi + τ(θy0,z) + log πy0
y0=1	z∈L
17
Under review as a conference paper at ICLR 2019
Note that, since the labels (yn1+1, . . . , yn) are known, the reconstruction loss for clustering data
xn1+1, . . . , xn in the above objective function indeeds incorporate these information to improve the
accuracy of estimating the parameters. We call the above objective function to be (semi)-supervised
NRM without noise.
Boosting the accuracy of (semi)-supervised NRM without noise In practice, it may happen that
the accuracy of classifying data by using the parameters from (semi)-supervised NRM without
noise is not very high. To account for that problem, we consider the following general version of
(semi)-supervised NRM without noise that includes the variational inference term and the moment
matching term
n1
mjn aRC {X X 1/	(
i i=1 (y0,z0)∈J ( (y0,z0)=arg max h h> (y,z;0)xi+T (θy,z)
n
+ X X1/
、i=nι + 1 z0∈L J z0=arg max ( h>(yi,z;O)Xi+T 包2)
kxi - h(y0,z,0)k2
2
kxi - h(yi,z0; O)k2
2
- log(πyi,z0)
αcE
n - n1
Xn
log qθ (yi |xi ) + αKLX Xqθ (y∣χi)log( qθ (ylxi)
=n1 +1	i=1 y=1	y
L
+αMM X DKL (N(Mh('), σh(')) ||N(μψ('), σψ(') D
'=1
(12)
Here, αKL and αMM are non-negative weights associated with the variational inference loss and
moment matching loss respectively. Additionally, μh('),仃嬴),μψ(`) ,σψ(') in the moment matching
loss are defined as follows:
1n
μh(') = n T h(`)i,
i=1
1n
μψ(') = n∑Sψ(')i,
i=1
1n
σ2(') = n ∑(h(')i - μh('))2
i=1
1n
σψ(') = n E(ψ(')i- μψ('))2	(13)
ni=1
where h(') is the estimated value of h(') given the optimal latent variables ^(') and t(') inferred from
the image xi for 1 ≤ i ≤ n. It is clear that when αKL = αMM = 0, we return to (semi)-supervised
NRM without noise. In Appendix c, we provide careful theoretical analyses regarding statistical
guarantees of model equation 12.
Now, we will provide heuristic explanations about the improvement in terms of performance of model
equation 12 based on the variational inference term and the moment matching term.
Regarding the variational term: The DRMM inference algorithm developed thus far ignores
uncertainty in the latent nuisance posterior p(y, z|x) due to the max-marginalization over (y, z) in
the E-step bottom-up inference. We would like to properly account for this uncertainty for two main
reasons: (i) our fundamental hypothesis is that the brain performs probabilistic inference and (ii)
uncertainty accounting is very important for good generalization in the semi-supervised setting since
we have very little labeled data.
One approach attempts to approximate the true class posterior p(y|x) for the DRMM. We employ
variational inference, a technique that enables the approximate inference of the latent posterior.
Mathematically, for the DRMM this means we would like to approximate the true class poste-
rior p(y|x) ≈ q(y|x), where the approximate posterior q is restricted to some tractable family of
distributions (e.g. Gaussian or categorical). We strategically choose the tractable family to be
q(y∣x) ≡ p(y∣z, x), where Z ≡ argmax p(y, z|x). In other words, We choose q to be restricted
z
to the DRMM family of nuisance max-marginalized class posteriors. Note that this is indeed an
approximation, since the true DRMM class posterior has nuisances that are sum-marginalized out
p(y|x) = P p(y, z|x), whereas the approximating variational family has nuisances that are max-
z
marginalized out.
18
Under review as a conference paper at ICLR 2019
Given our choice of variational family q, we derive the variational term for the loss function, starting
from the principled goal of minimizing the KL-distance DKL[q(y|x)||p(y|x)] between the true and
approximate posteriors with respect to the parameters of q. As a result, such an optimized q will
tilt towards better approximating p(y|x), which in turn means that it will account for some of the
uncertainty in p(z|x). The variational terms in the loss are defined as Blei et al. (2017):
LV I ≡ LRC + βKLLKL
≡ -Eq [ln p(x|y)] + βKLDKL [q(y|x)||p(y)].	(14)
This term is quite similar to that used in variational autoencoders (VAE) Kingma & Welling (2013),
except for two key differences: (i) here the latent variable y is discrete categorical rather than
continuous Gaussian and (ii) we have employed a slight relaxation of the VAE by allowing for a
penalty parameter βKL 6= 1. The latter is motivated by recent experimental results showing that such
freedom enables optimal disentangling of the true intrinsic latent variables from the data.
Regarding the moment matching term: Batch Normalization can potentially be derived by nor-
malizing the intermediate rendered images h(`), ` = 1, 2, . . . , L in the NRM by subtracting their
means and dividing by their standard deviations under the assumption that the means and standard
derivations of h(') are close to those of the activation ψ(') in the CNNs. From this intuition, in
Section .4 of Appendix A, we introduce the moment-matching loss to improve the performance of
the NRM/CNNs trained for semi-supervised learning tasks.
.5 Statistical guarantees for (semi)-supervised setting
For the sake of simplicity with proof argument, we only provide detail theoretical analysis for
statistical guarantee with the setting of equation 12 when the moment matching term is skipped. In
particular, we are interested in the following (semi)-supervised model
n1
Yn ：=min 等 X X 1(	(
i i=1 (y0,z0)∈J ( (y0,z0) = arg max I h> (y,z；O)xi+τ (θy,z)
×(kxi -h(y0,z0;O)k2
I 2
n
+ X X1(
∖ i=nι + 1 z0∈L < z0=arg max ( h>(yi,z;0)Xi+T 包2)
kxi - h(yi,z0; O)k2
2
αCE
n - n1
n	nK
X log qθ (yi∣χi) + 詈 XX
qθ (y∣χi)log
=n1 +1	i=1 y=1
qθ (y|xi)
πy
(15)
where the approximate posterior q&(y∣xi) is chosen as
exp	m∈aLx	h> (y, z; O)xi	+ τ (θy,z)	+	log πy
qθ(y| Xi) =	-K	7	7	ɔ
P exp max h>(y0, z; O)xi + τ(θy0,z) + log πy0
y0=1	z∈L
Here, αRC, αCE, and αKL are non-negative weights associated with reconstruction loss, cross entropy,
and variational inference respectively. As being indicated in the formulation of objection function
Yn, the only difference between Yn and equation 12 is the weight αMM regarding moment matching
loss in equation 12 is set to be 0. To ease the presentation with theoretical analyses later, we call
the objective function with Yn to be partially labeled latent dependence regularized cross entropy
(partially labeled LDCE).
Consistency of partially labeled LDCE Firstly, we demonstrate that the objective function of
partially labeled LDCE enjoys the consistency guarantee.
Theorem .6. (ConsistenCy of objective function of partially labeled LDCE) Assume that nι is a
function of n such that n`/n → λ as n → ∞. Furthermore, P(∣∣x∣∣ ≤ R) = 1 as X 〜P for some
19
Under review as a conference paper at ICLR 2019
given R > 0. We denote the population version of partially labeled LDCE as follows
l∣χ - h(y0,z0;0)k2
0) = arg max h>(y,z;0)x+「(θy,z)
(y,z)∈J
∑1∕	(
z0∈L J z0=arg max f h>(y,z*)x+τ(θy,z)
kx - h(y, z0; 0)k2
-log(∏y,z0 )^ dQ(x,c)^ - - αCE / log qθ(y∣x)dQ(x, c) + αKLZ X qθ (y∣χ)iog( qθ∏zlx)) dP(χ).
Then, we obtain that Yn → Y almost surely as n → ∞.
The detail proof of Theorem .6 is deferred to Appendix C. Now, we denote θ :=
({e(y)}K=ι, {γ(')}	, {ey }K=ι, {b(')}	) the optimal solutions of objective function equa-
tion 15. Note that, the existence of these optimal solutions is guaranteed due to the compactness
assumption of the parameter spaces θ`, Ω, and Ξι for 1 ≤ ' ≤ L. The optimal solutions {e(y)}K=ι
and {Γ(')}
lead to corresponding set of optimal rendered images Sn. Similar to the case of SPLD
'=1
regularized K-means, our goal is to guarantee the consistency of Sen as well as {πey }yK=1 , eb(`)
'=1
0
In particular, we denote F0 the set of all optimal solutions θ0 of population partially labeled LDCE
where e0 :=( {e0⑻}K=1,
πey0 }yK=1 , neb0 (`)o	. For each θe0 ∈ Fe0, we define
0
S0 the set of optimal rendered images associated with θ0 . We denote G(F0) the corresponding set of
all optimal rendered images So, optimal prior probabilities {彳0}[[, and optimal biases {eo(')}
'=1
Theorem .7. (Consistency of optimal rendering paths and optimal solutions of partially labeled
LDCE) Assume that P(∣x∣ ≤ R) = 1 as X 〜P for some given R > 0. Then, we obtain that
inf
(eo,{ey },{e°(')})∈G(Fo)
almost surely as n → ∞.
H (Sn, So) + ∑ ∣e 厂 ∏0l + E ke(') - eo(')k → 0
y=1
'=1
L

K
L
2
2
L
L
The detail proof of Theorem .7 is postponed to Appendix C.
.6 Generalization bound for classification framework with (semi)-supervised
SETTING
In this section, we provide a simple generalization bound for certain classification function with
the optimal solutions θ
{e(y)}K=ι, {e(')}e= 1, {πy}K=1, {e(')}e=J of equation 12. In
particular, we denote the following function f : RD(0) × {1, . . . , K} → R as
f(x, y) = max
z∈L
z; 0)x + τ (θey,z) + log πey
for all (x, y) ∈ RD(0) × {1, . . . , K} where



h(y,z;0) := Λ(z; 1)... Λ(z; L)μ(y),
Λ(z;') := E s(',p)T(t; ',p)B(',p)Γ(',p)
P∈P(')
for all (y, z ) and 1 ≤ ` ≤ L. To achieve the generalization bound regarding that classification
function, we rely on the study of generalization bound with margin loss. For the simplicity of
20
Under review as a conference paper at ICLR 2019
argument, we assume that the true labels of x1, . . . , xn are y1 , . . . , yn while y1 , . . . , yn1 are not
available to train. The margin of a labeled example (x, y) based on f can be defined as
ρ(f, x, y) = f(x, y) - max f (x, l).
l6=y
Therefore, the classification function f misspecifies the labeled example (x, y) as long as ρ(f, x, y) ≤
0. The empirical margin error of f at margin coefficient Γ ≥ 0 is
1n
Rn,γ(f) = ^ ΣS 1{ρ(f,xi,yi)≤γ}.
n i=1
It is clear that Rn,0 (f) is the empirical risk of 0-1 loss, i.e., we have
1n
Rne" n i=l 1(argmax f(x° ,y) = yiY
i=1	1≤y≤K
~ L , J	,	.	.1	"Cf	_,1	1	1	..	:
Similar to the argument in the case of SPLD regularized K-means, the optimal solutions θ of partially
labeled LDCE lead to a set of rendered images h(y, z; 0) for all (y, z) ∈ J. HoWever, only a small
fraction of rendering paths are indeed active in the folloWing sense. There exists a subset Ln of L
such that |Ln| ≤ Tn|L| where Tn ∈ (0,1], which is independent of data (χι,yι),..., (xn, yn), and
the folloWing holds
max
z∈L
z; 0)x + T (θey z) + log πey = max
z∈Ln
z; 0)x +T(θey,z) + log πey
for all 1 ≤ y ≤ K. The above equation implies that
Rn,r(f ) = Rn,γ(fγn )
for all Γ ≥ 0 and n ≥ 1 where fT (x, y) = max
τn	z∈Ln
z; 0)x +T(θey,z) + log πey for all (x, y).
With that connection, We denote the expected margin error of classification function fTn at margin
coefficient Γ ≥ 0 is
RY (fτ n ) = ^E1{ρ(fτn ,x,y)≤γ}.
The generalization bound that we establish in this section will base on the gap between the expected
margin error Ro(fτn) and its corresponding empirical version Rn,r(fτn), which is also Rn,r(f).
Theorem .8. (Generalization bound for margin-based classification) Assume that P (kxk ≤ R) = 1
for some given R > 0 and X 〜P. Additionally, the parameter spaces θ` and Ω are chosen such that
kh(y, z; 0)k ≤ Rfor all (y, z) ∈ J. For any δ > 0, with probability at least 1 - δ, we have
Rθ(fτ n ) ≤ f 1Rn,Y (fτ n ) +
γ∈(0,1]
8κ(2Kn- I)(2Tn|L|(R2 + 1) + | logγ∣)
loglog2(2YT)
n
)1/2+rɪogrɪ
+
where Y is the lower bound OfPrior probability ∏y for all y.
Remark .9. The result of Theorem .8 gives a simple characterization for the generalization bound of
classification setup from optimal solutions of partially labeled LDCE based on the number of active
rendering paths, which is inherent to the structure of NRM. Such dependence of generalization bound
on the number of active rendering configurations Tn |L| is rather interesting and may provide a new
perspective on understanding the generalization bound. Nevertheless, there are certain limitations
regarding the current generalization gap: (1) the active ratio Tn may change with the sample size
unless we put certain constraints on the sparsity of switching variables a to reduce the number of
active optimal rendering configurations; (2) the generalization bound is depth- dependent due to the
involvement of the number of rendering configurations |L|. This is mainly because we have not fully
taken into account all the structures of CNNs for the studying of generalization bound. Given some
current progress on depth-independent generalization bound (Bartlett et al., 2017; Golowich et al.,
2018), it is an interesting direction to explore whether the techniques in these work can be employed
to improve |L| in the generalization bound in Theorem .8.
21
Under review as a conference paper at ICLR 2019
.7 Neural Rendering Model is the Unifying Framework for Networks in the
Convnet Family
The structure of the rendering matrices Λ(') gives rise to MaxPooling, ReLU, and convolution
operators in the CNNs. By modifying the structure of Λ('), we can derive different types of networks
in the convolutional neural network family. In this section, we define and explore several other
interesting variants of NRM: the Residual NRM (ResNRM) and the Dense NRM (DenseNRM).
Inference algorithms in these NRMs yield ResNet He et al. (2016) and DenseNet Huang et al. (2017),
respectively. Proofs for these correspondence are given in Appendix C. Both ResNet and DenseNet
are among state-of-the-art neural networks for object recognition and popularly used for other visual
perceptual inference tasks. These two architectures employ skip connections (a.k.a., shortcuts) to
create short paths from early layers to later layers. During training, the short paths help avoid the
vanishing-gradient problem and allow the network to propagate and reuse features.
.7.1 Residual Neural Rendering Model Yields ResNet
In a ResNet, layers learn residual functions with reference to the layer inputs. In particular, as
illustrated in Fig. 5, layers in a ResNet are reformulated to represent the mapping F (ψ) + Wskipψ
and the layers try to fit the residual mapping F (ψ) where ψ is the input feature. The term Wskipψ
accounts for the skip connections/shortcuts He et al. (2016). In order to derive the ResNet, we rewrite
the rendering matrix Λ(') as the sum of a shortcut matrix Λskip(') and a rendering matrix, both of
which can be updated during the training. The shortcut matrices yields skip connections in He et al.
(2016). Note that Λskip(') depends on the template selecting latent variables s('). In the rest of this
section, for clarity, We will refer to Λ(') and Λskip(') as Λ(t, s;') and Λskip(s;'), respectively, to
show their dependency on latent variables in NRM. We define the Residual Neural Rendering Model
as follows:
Definition .10. The Residual Neural Rendering Model (ResNRM) is the Neural Rendering Model
whose rendering process from layer ' to layer ' - 1, for some ' ∈ {1,2,…，L}, has the residual
form as follows:
h(' - 1) := (Λ(t, s; ') + Λskip(s;')) h('),	(16)
where Λskip(t, s; `) is the shorcut matrices that results in skip connections in the corresponding
ResNet. In particular, Λskip (t, s; `) has the following form:
∙-v
Λskip(s; ')=Λskip(')M(s;'),	(17)
where M(s;') ≡ diag (s(')) ∈ RD(E)×D(E) is a diagonal matrix whose diagonal is the vector s(').
This matrix selects the templates for rendering. Furthermore, Λskip (`) is a rendering matrix that is
independent of latent variables t and s.
The following theorem show that similar to how CNNs can be derived from NRM, ResNet can be
derived as a bottom-up inference in ResNRM.
Theorem .11. Inference in ResNRM yields skip connetions. In particular, if the rendering process at
layer ` has the residual form as in Definition .10, the inference at this layer takes the following form:
ψ(') ≡
max
t(E),s(E)
{(Λ>(t,s; ') + Λ>ip(s;')) ψ(' - 1) + b(')}
MaxPool
f
ReLu
∖
COnVcr>('), ψ(' - 1)) + b(') + ΛSkiP(')ψ(' - 1)
、----------{----------}
skip connection
(
∖
MaxPOOl
ReLU Conv(W('), ψ(' - 1)) + b(') + WSkiP(')ψ(' - 1)
\	\	skip connection
(18)
■"W
Here, when ψ(' - 1) and ψ(') have the same dimensions, Λskip(') is chosen to be an constant identity
matrix in order to derive the parameter-free, identity shortcut among layers of the same size in the
∙-v
ResNet. When ψ(' - 1) and ψ(') have the different dimensions, Λskip(') is chosen to be a learnable
shortcut matrix which yields the projection shortcut WSkiP(') among layers of different sizes in
22
Under review as a conference paper at ICLR 2019
Figure 5: ResNet building block as in He et al. (2016)
x
identity
the ResNet. As mentioned above, identity shortcuts and projection shortcuts are two types of skip
connections in the ResNet. The operator =d implies that discriminative relaxation is applied.
In practice, the skip connections are usually across two layers or three layers. This is indeed
a straightforward extension from the ResNRM. In particular, the ResNRM building block that
corresponds to the ResNet building block in He et al. (2016) (see Fig. 5) takes the following form:
h(' - 2) := (Λ(t, s;' - 1)Λ(t, s;') + Askip(s; ')) h(').
In inference, this ResNRM building block yields the ResNet building block in Fig. 5:
ψ(') = ReLU(Conv(r>('), ReLU(Conv (Γ>(' - 1),ψ(' - 2)) + b(' - 1)))
+ b(') + AIip(')Ψ('- 2))
=ReLu ( Conv (W('), ReLu ( Conv (W(' - 1),ψ(' - 2)) + b(' - 1)))
+ b(') + Wskip(')Ψ('- 2)] .	(19)
.7.2 Dense Neural Rendering Model Yields DenseNet
In a DenseNet Huang et al. (2017), instead of combining features through summation, the skip
connections concatenate features. In addition, within a building block, all layers are connected to
each other (see Fig. 6). Similar to how ResNet can be derived from ResNRM, DenseNet can also be
derived from a variant of NRM, which we call the Dense Neural Rendering Model (DenseNRM).
In DenseNRM, the rendering matrix A(') is concatenated by an identity matrix. This extra identity
matrix, in inference, yields the skip connections that concatenate features at different layers in a
DenseNet. We define DenseNRM as follows.
Definition .12. The Dense Neural Rendering Model (DenseNRM) is the Neural Rendering Model
whose rendering process from layer ' to layer ' - 1, for some ' ∈ {1,2,…，L}, has the residual
form as follows:
h(' - 1) ：= [A(t,s; ')h('), lD(')h(')] .	(20)
We again denote A(') as A(t, s;') to show the dependency of A(') on the latent variables t(') and
s(`). The following theorem establishes the connections between DenseNet and DenseNRM.
Theorem .13. Inference in DenseNRM yields DenseNet building blocks. In particular, if the
rendering process at layer ` has the dense form as in Definition .12, the inference at this layer takes
23
Under review as a conference paper at ICLR 2019
the following form:
ψ(') ≡
max
t(`),s(`)
{Λ>(t,s; ')ψ(' - 1) + b(')}
ψ('-1)
MaxPool(ReLu(Conv(Γ>('), ψ(' - 1)) + b(')))
ψ('-1)
d Γ MaxPool(ReLu(Conv(W('), ψ(' - 1)) + b(')))
=	Ψ(' - 1)
(21)
In Eqn. 21, We concatenate the output MaxPoolReLu(Conv(W('), ψ(' 一 1)) + b('))) at layer '
with the input feature ψ(' 一 1) at layer ' 一 1 to generate the input to the next layer ψ('), just like in
the DenseNet. Proofs for Theorem .11 and .13 can be found in Appendix B. The approach to proving
Theorem .11 can be used to prove the result in Eqn. 19.
Figure 6: DenseNet building block as in Huang et al. (2017)
24
Under review as a conference paper at ICLR 2019
Appendix C
In this appendix, we provide the proofs for key results in the paper.
.8 Proof for Theorem 2.2: Deriving Convolutional Neural Networks from the
Neural Rendering Model
To ease the clarity of the proof presentation, We ignore the normalizing factor = and only consider
the proof for two layers, i.e., L = 2. The argument for L ≥ 3 is similar and can be derived recursively
from the proof for L = 2. Similar proof holds when the normalizing factor 表 is considered. Now,
We obtain that
max h> (y, z; 0)x + η(y, z)
h(1,p1)s(1,p1)Γ>(1,p1)B>(1,p1)T>(t; 1,p1)x
+ E h(1,pι)s(1,pι)b(t; 1,P1) + X μ(y; P2)s(2,P2 )b(t∖2,P2)∖
p1 ∈P(1)	p2∈P(2)
h(1,P1)s(1,P1) (r>(1,P1)B>(1,P1)τ>(t； 1,PI)X + b(t； 1,pI))
+ E μ(y;P2)s(2,P2)b(t；2,P2) ∖
p2∈P(2)
=maχ,( X h(1,Pι) , ,aχ	s(1,Pι) (Γ>(1,pι)B>(1,pι)T>(t; 1,pι)x + b(t; 1,pι))
t(2),s(2)	t(1,p1),s(1,p1)
p1 ∈P (1)
+ X μ(y;P2)s(2,P2)b(t；2,P2)1 = A,
p2∈P(2)
max
t(1),s(1)
t(2),s(2)
X
' * uT>(
p1 ∈P(1)
max
t(1),s(1)
t(2),s(2)
X
' * uT>(
p1 ∈P(1)
where equation in (a) is due to the non-negativity assumption that h(1, p1 ) ≥ 0, just as in max-sum
and max-product message passing. We define ψ(1,p1 ) as follows:
Ψ(1,P1)= / maχ	s(1,Pι) (Γ>(1,pι)B>(1,pι)T>(t; 1,pι)x + b(t; 1,pι)),
t(1,p1),s(1,p1)
and let ψ(1) = (ψ(1,p1 ))p1∈P(1) be the vector of ψ(1,p1 ). The following holds:
A=*maχ4h>(1)ψ⑴+ E μ(y;p2)s(2，p2)b(t；2，p2)
,	p2∈P(2)
'max/ X μ(y;p2)s(2，p2)r>(2，p2)B>(2，p2)T>(t；2，p2w⑴
t(),s() p2∈P(2)
+ E μ(y;P2)s(2,P2)b(t；2,P2)
p2∈P(2)
£ μ(y; P2) maχx s(2,p2)
P2∈⅛)	t(2),s ⑵
p2)B>(2,p2)T>(t; 2,p2)ψ(1) + b(t; 2,p2)
{z"∖^^^^
ψ(2,p2)
}
=μ>(y)ψ(2).
Here ψ(2) = (ψ (2, p2))p2∈P(2) is the vector of ψ(2, p2), and in line (b) we substitute h(1) by:
h(I) = E s(2,P2)T(t；2，P2)B(2,P2)r(2,P2)μ(y;P2).
p∈P(2)
Notice that line (a) and (c) form a recursion. Therefore, we finish proving that the feedforward step
in CNNs is the latent variable inference in NRM if ψ(1) has the structure of the building block of
25
Under review as a conference paper at ICLR 2019
CNNs, i.e. MaxPool (ReLu (Conv())). Indeed,
ψ(1) = (ψ(1, p(1)))p(1)∈P(1)
=(,maχ	s(1,pι) (y>(1,pι)B>(1,pι)T > (t; 1,pι)x + b(t; 1,pι)f)
t(1,p1),s(1,p1)	p(1)∈P(1)
=(maχ ((ReLU (r>(1,pι)B>(1,pι)τ>(t;1,pi)χ + b(t；1,PI)))t(i,p1)=0,1,2,3))p⑴∈p⑴
(22)
=MaxPool (ReLU (Conv (Γ>⑴,x) + b(t; 1)))
=d MaχPool (ReLU (Conv (W (1), x) + b(t; 1))) ,
where W (1) = Γ> (1) corresponds to the weights at layer 1 of the CNN, and =d implies that discrimi-
native relaxation is applied. In Eqn. 22, since s(1,p1) ∈ {0, 1}, max (s(1,p1) × .) is equivalent to
s(1,p1)
max( . , 0) and, therefore, yields the ReLU function. Also, in order to compute max (), we take the
t(1,p1)
brute force approach, computing the function inside the parentheses for all possible values of t(1,p1)
and pick the maximum one. This procedure is equivalent to the MaxPool operator in CNNs. Here we
can make the bias term b(t; 1) independent oft and b(t; 1,p1) are the same for all pixels p1 in the same
feature map of h(1) as in CNNs. Similarly, ψ(2) = MaxPool (ReLU (Conv (W (2), ψ(1)) + b(2))).
Thus, we obtain the conclusion of the theorem for L = 2.
.9 Proof for Theorem .11: Deriving the Residual Networks from the Residual
Neural Rendering Model
Similar to the proof in Section .8 above, when the rendering process at each layer in ResNRM is as in
Eqn. 16, the activations ψ(') is given by:
ψ⑶=Qmax`j,p) ((κp)BS)T >色⑺+入 SM`m) ψ(' - 1)+…p)))…)
max! I ReLuI Γ>(',p)B>(',p)T>(t;',p)ψ(' - 1) + b(t;',p)
+ Λ skip(',p)Ψ(' -1)
))t(',p) = 0,1,2,3/)p∈P(')
MaxPool (ReLu (Conv (Γ>(',p), ψ(' — 1)) + b(t;') + ΛSkip(')ψ(' - 1)))
MaxPool ReLu
∖
ConV(W⑶，ψ(' - 1)) + b(t;') + WSkip(')ψ(' - 1)
skip connection
(23)
Here we can again make the biaS term b(t; `) independent oft and b(t; `,p) are the Same for all pixelS
p` in the Same feature map of h(`) aS in CNNS. We obtain the concluSion of the Theorem .11.
26
Under review as a conference paper at ICLR 2019
.10 Proof for Theorem .13: Deriving the Densely Connected Networks from the
Dense Neural Rendering Model
Similar to the proof for Theorem .11 above, when the rendering process at each layer in ResNRM is
as in Eqn. 20, the activations ψ(') is given by:
ψ(') ≡
max
s(`,p),t(`,p)
(max ((ReLU (Γ>(',p)B>(',p)T>(t; ',p)ψ(' - 1) + b(t；',P)))t(',P)=0,1,2,3))p∈P⑶
ψ(' - 1)
d
MaXPool(ReLU(Conv(Γ>(',p), ψ(' — 1)) + b(t; ')))
ψ(' - 1)
MaXPool(ReLU(Conv(W⑶，ψ(' - 1)) + b(t; '))) ^
ψ(' -1)	_ .
(24)
Again we can make the bias term b(t; `) independent oft and b(t; `,p) are the same for all pixels p`
in the same feature map of h(`) as in CNNs. We obtain the conclusion of the Theorem .13.
.11 Proving that the Parametrized Joint Prior p(y,z) is a Conjugate Prior
Again, for simplicity, we only consider the proof for two layers. The argument for L ≥ 3 is similar
and can be derived recursively from the proof for L = 2. In the derivation below, h(2) is μ(y). As in
Eqn.1 in the definition of the NRM, the joint prior of y and z is given by:
p(y,z) H exp (Wb>(t； 2)(s(2) Θ h(2)) + b>(t；1)(s(1) Θ h(1)) + ln∏y)
=exp (σ12h>(2)(b(t; 2) Θ s(2)) + J^h>(1)(b(t; 1) Θ s(1)) + ln∏y)
=exp (J h>(2)(b(t; 2) Θ s(2)) + J2 h>(2)Λ>(2)(b(t; 1) Θ s(1)) + ln ∏y)
=exp (J2h>(2) [Λ>(2)(b(t; 1) Θ s(1)) + b(t;2) Θ s(2)] +lnπj	(25)
Furthermore, as explained in Section .14 of Appendix D, due to the constant norm assumption with
h(y, z; 0), the likelihood p(x|y, z) is estimated as follows:
p(x∣y,z) H exp (1-^h>(y,z;0)x)
=exp Gh>(2)Λ>(2)Λ>(1)x)
The posterior p(y, z|x) is given by:
p(y, z|x) H exp (最h>(2) [Λ>(2)(b(t; 1) Θ s(1)) + Λ>(2)Λ>(1)x + b(t; 2) Θ s(2)] + lnπj
=exp (-2h>(2) [Λ>(2)(b(t; 1) Θ s(1) +Λ>(1)x) + b(t;2) Θ s(2)] +lnπj (26)
Comparing Eqn. 25 and Eqn. 26, we see that the prior and the posterior have the same functional
form. This completes the proof.
.12 Deriving Other Components in the Convolutional Neural Networks
.12.1 Deriving the Leaky Rectified Linear Unit (Leaky ReLU)
The Leaky ReLU can be derived from the NRM in the same way as we derive the ReLU in Section .8,
but instead of s(`) ∈ {0, 1}, we now let s(`) ∈ {α, 1}, where α is a small positive constant. Then, in
27
Under review as a conference paper at ICLR 2019
Eqn. 22, max(s(',p) X .) is equivalent to [. < 0](α X .) + [. >= 0](.), which is the LeakyReLU
s(`,p)
function. Note that compared to Eqn. 22, here we replace the layer number (1) by (`) since the same
derivation can be applied at any layer.
.13 Proofs for Batch Normalization
In this section we will derive the batch normalization from a 2-layer DRM. The same derivation can
be generalized to the case of K-layer DRM.
In order to derive the Batch Normalization, we normalize the intermediate rendering image h(`).
h(y, z; 0)
= * X s(1,p1)T(t; 1,p1)B(1,p1)Γ(1,p1)
σ%(1 PI) (h(1,p1) - Eh(I,pl)) - Eh(y,z;0)
1
σh(1,Pι)
p1 ∈P(1)
X s(1,p1)T(t; 1,p1)B(1,p1)Γ(1,p1)
p1 ∈P(1)
× ( X X s(1,P2)T (t；2,P2)B(2,P2)r(2,P2)—U~h( h(y,z；0)[ (I,Pl) - Eh(1,Pl)
~2∈P(2)	σh(y,z;0)	/
- Eh(y,z;0)	(27)
During inference, we de-mean the input image and find z* = arg max h>(y, z;0)(x 一 Eh(y,z；o)). In
particular, the inference can be derived as follows:
mzax h>(y, z;0)(x 一 Eh(y,z;0))
mzax X	s(1,p1)T(t; 1,p1)B(1,p1)Γ(1,p1)
p1∈P(1)
1
σh(I,PI)
h(1,p1) 一 Eh(1,p1)	(x 一 Eh(y,z;0))
一 Eh>(y,z;0) (x 一 Eh(y,z;0))
、--------------V--------------}
const w.r.t y,z
a
≈ max
z
Σ
p1P	(1)
s(1,P1) h(1,P1) 一 Eh(1,P1)
σh(1,pι) r>(1,Pl)B>(1:zPl)T>(t;1,p1) 1x-四
scalar
row vector
column vector
scalar
}
|
dot product
+ const := A + const.
Direct computation leads to
A =max X (s(1,pι)(h(1,pι) 一 Eh(1,pι)[ σψ (1,p1) --1--
Z P/'	'	/也以父生
α(1,p1 )	normalize
X (Γ>(1,pι)B>(1,pι)T>(t; 1,pι)x -{E[Γ>(1,pι)B>(1,pι)T>(t;1,pi)x]))
de-mean
=max	max	s(1,P1) h(1,P1) 一 Eh(1,P1)
z(2)	s(1,p1),t(1,p1)
p1∈P	(1)
X BatchNorm(Γ>(1,P1)B>(1,P1)T>(t; 1, P1)x; α(1,P1))
= maxh(1) 一 Eh(1)	MaxPool(ReLu(BatchNorm(Conv(Γ(1), x); α(1), 0)))	(28)
28
Under review as a conference paper at ICLR 2019
In line (a), we approximate Eh(y,z;0) by its empirical value Ei. The de-mean and normalize operators
with the scale parameter α(1) and the shift parameter β(1) = 0 in the equations above already have
the form of batch normalization. Note that when the model is trained with Stochastic Gradient
Descent (SGD), the scale and shift parameters at each layer also account for the error in evaluating
statistics of the activations using the mini-batches. Thus, β(1) is not 0 any more, but a parameter
learned during the training. Also, in the equations above, ψ(1) is the activations at layer 1 in CNNs
and given by:
ψ(1) = MaxPool(ReLu(Normalize(Demean(Conv(Γ(1), x))))),	(29)
where σψ (1) is the standard deviation of the ψ(1). Eqn.28 can be expressed in term of ψ(1) as
follows:
maxh(1) - Eh(1)	ψ(1) + const
maxh>(1)(ψ(1) - Eψ(1)) + (h>(1)Eψ(1) - Eh>(1)ψ(1)) + const
z(2)
max
z(2)
Σ
p2∈P(2)
s(2,P2)μ(y,P2)
σψ ⑵P2)	1
σμ(y)(P2) σψ (2,P2 )
'—{z—'、—{—}
α(2,p2 )	normalize
× 1r>(2,p2)B>(2,P2)T>(t2p2)ψ(l)-E[Γ>(2,p2)B>(2,p2)T>(t;2,p2)@(1)],
de-mean
+ h(1,p1)Eψ(1,p1) - Eh(1, p1)ψ(1, p1)
、	一一_一	/
+ const
β(2,p2)
max μ>(y)
z(2)
MaxPool(ReLu(BatchNorm(Conv(Γ(2), ψ(1)); α(2), α(2) β(2))))
+ const
(30)
The batch normalization at this layer of the CNN has the scale parameter α(2) and the shift parameter
is the element-wise product of α(2) and β(2).
.14 Proofs for connection between NRM and cross entropy
PROOF OF THEOREM .1 (a) To ease the presentation of proof, we denote the following key
notation
1n
A := max —	lnp(y/xi,Zi； θ).
(zi)in=1,θ∈Aγ n i=1
From the definition ofAγ = {θ : kh(y, z; 0)k = γ}, we achieve the following equations
1n
A = max —	max logp(y∕xi,zi; θ)
θ∈Aγ n	zi
i=1
= max 1 XX max 心 P(Xi|yi,Zi； θ)p(yi∣Zi； θ)
θ∈Aγ n N√ Zi	K z l 八、/ I 八、
i=1	P p(χi∣y,zi; θ)p(y∣zi; θ)
y=1
1n
= max - Vmax^ logp(xi|yi,zi； θ) + logp(yi,zi∣θ) - log	P(Xi|y,Zi； θ)p(y, zi∣θ)
θ∈Aγ n	zi
γ	i=1	i
From the formulation of NRM, We have the following formulation of prior probabilities p(y, z∣θ)
exp
p(y,zlθ)- P	(η(y,,z')、
yp,exp( F Yn
29
Under review as a conference paper at ICLR 2019
for all (y, z). By means of the previous equations, we eventually obtain that
1n
max —
θ∈Aγ n
i=1
1n
max —
θ∈Aγ n
-log ( X 卜(Xi |y, Zi； θ) exp ( η(σ,2zi) 卜y
By defining ψi(y, zi) := logp(xi |y, zi; θ) + log
equation can be rewritten as
卜xp( ⅛⅜)
for all 1 ≤ i ≤ n, the above
1n
max — >
θ∈Aγ n
1n
max —
θ∈Aγ n
i=1
mzax log(exp(ψi(yi,zi))) - log X exp(ψi(y, zi))
max log (Softmax(ψi(yi, zi)))
zi
max —	log SSoftmax mmax ψi(yi, Zi)) ) ) = B.
θ∈Aγ n i=1	zi
By means of direct computation, the following equations hold
(i)
1n
max 一 log I Softmax
θ∈Aγ n
1n
max 一 log I Softmax
θ∈Aγ n
i=1
1n
max 一 log I Softmax
θ∈Aγ n
i=1
max log p(xi∣yi, Zi； θ) + η(yi, Zi)∕σ2 + log ∏
yi
(31)
(32)
max -
zi
max
zi
Ilxi - μyi,zi IF
η(yi, Zi)
2σ2
σ2
+ log πyi
h>(yi, Zi； 0)xi + η(yi, Zi)
σ2
+ byi
-min X - log ( Softmax ( max
θ∈Aγ n	zi
i=1
1n
-min 一 ɪ2-logq(yi∣xi)
θ∈Aγ n
h>(yi, Zi； 0)xi + η(yi, Zi)
min Hp,q(y|x)
θ∈AΓ
σ2
+ byi
A
A
≥
B
+
	
where equation (i) is due to the constant norm assumption with rendered images h(y, Z； 0). Therefore,
we achieve the conclusion of part (a) of the theorem.
30
Under review as a conference paper at ICLR 2019
(b) Regarding the upper bound, from the definition of Zi, We obtain that
max log (Softmax(ψi(yi,zi))) ― log
zi
maxψi(yi,zi))
=log(Softmax(ψi(yi,Zi))) — log Softmax maxψi(yi,Zi)
maxexp(ψi(yi,Zi))
≤ log -jK-----------------log ( Softmax ( max ψi(yi,Zi))
P exp(ψi(y,Zi))	2
y=1
log
exp(mzaXψi(y,zi)) - log
exp(Ψi(y,Zi))
≤ log K + max max ψi(y, Zi) — max ψi(y, Zi)
≤ log K + max max Ψi(y,Zi) 一 Ψi(y,Zi)
y	zi
for any 1 ≤ i ≤ n. As a consequence, We obtain the conclusion of part (b) of the theorem.
PROOF OF THEOREM .4 (a) From the definitions of ®, Zi) and (yi, Zi), we obtain that
Un = min n	2kxi - h(yi,zi；0)k2 ― logπyi,z)
min
θ
2 kxi
+ log	exp(η(y0, Z0) + log πy0)
(y0,z0)∈J
By means of direct computation, the following inequality holds
A
A ≤ min
θ
2kxi ― h(yi,zi；O)k2 ― η(yi,zi) ― log
+log	exp(η(y0, z0) + log πy0)
(y0,z0)∈J
It is clear that
exp(η(y0, z0) + log πy0) ≤ |L|	exp(m0 ax η(y0, z0) + log πy0).
z ∈L
(y0,z0)∈J
y0
Combining this inequality With the inequality of the term A in the above display, We have
Un ≤
+
≤
+ log |L|
Vn + log (Y — l) + log |L|
y
y
	
31
Under review as a conference paper at ICLR 2019
where the final inequality is due to the fact that 不可/不%石 ≤ (1 - Y)∕γ for all 1 ≤ i ≤ n. Therefore,
we achieve the conclusion of part (a) of the theorem.
(b) Similar to the proof argument with part (a), we have
min
θ
2 kxi
	
+log	exp(η(y0, z0) +logπy0)
(y0,z0)∈J
min
θ
≥
21 l∣χik2 - h(yi,Zi；0)>Xi - η(yi,Zi) + ||h(yi,Zi；0)k2/2 - log
+ log	exp(mz0∈aLx η(y0, z0) + log πy0) = B.
Direct computation with B leads to
1
mm 一
θn
lxi - h(yei,zei; 0)l2
B
2
-log(πei,ei) + log πyi - log πei
+kh(yi,Zi；0)k2-kh@,Zi；0)k2
≥
1
mm 一
θn
lxi - h(yei,zei; 0)l2
-log(πei,ei)) +min 1 X ( log TnVi - log πei
2
+ mθin H(kh(yi，Zi；0)k2-kh("0)k2
≥
Vn +log(τ-^) +mɪn 1 X(kh®i，Zi；0)k2 - kh(ei,Zi；0)k2
where the final inequality is due to the fact that ∏ei∕∏% ≥ γ∕(1 - Y) for all 1 ≤ i ≤ n. As a
consequence, we achieve the conclusion of part (b) of the theorem.
32
Under review as a conference paper at ICLR 2019
.15 PROOFS FOR STATISTICAL GUARANTEE AND GENERALIZATION BOUND FOR
(SEMI)-SUPERVISED LEARNING
PROOF OF THEOREM .6 The proof of this theorem relies on several results with uniform laws
of large numbers. In particular, we will need to demonstrate the following results
sup
θ
1 nι
一 max
nι 乙：(y,z)∈J
i=1
Z；0)Xi + T (θy,z )
—/ max
J (y,z)∈J
z; 0)x + τ(θy,z) I dP(x) T 0,
(33)
1	__	1	∖~∙' ]	kkh(y',z，;0)k2	]	、
LnI =SUP n;t	工 1f	(	2	logπy0)
θ 1 i=1 (y0,z0)∈J q (y0,zO)=argmax ( h>(y∕;O)Xi+τ (θy,z) I > ×	J
-/( X 1f	(	S,；；0)"2 - log∏y,"dP(x) → 0,(34)
J ∖(y0,z0)∈J < (y0,zz) = argmax( h> (y,z5Q)x⅛τ (θy,z) ∖ > ×	))
sup
θ
n — nι
n
X
i=nι +1
max
z∈L
,z；0)g + T (θyi,z )
	
z;0)x + τ(θy,z) dQ(x,c) → 0,
(35)
EnI) = SUP
n - n1
n
X X1
i=nι + 1 z0∈L
IIhiy ,z,0)ll2
z0=arg max I h> (yi,z;Q)xi+r (θyi,z)
-log πyi
1f
∈L < z0=arg max h>(y,z;Q)x+「(θy,z)
1	g∈J ∖
M(y,z,0)∣∣2
-log ∏y dQ(x,c)
→ 0,
En2) = sUp
n - n1
n
E log qθ(yi∖xi) - l log qθ(y∖x)dQ(x,c) → 0,
i=nι+1	J
(36)
(37)
1
1
1
2
2
EnS) = SUP
1 n K
—T∑2qθ(y∖χi )log
n
i=1y=1
qθ(y∖χi)
Zx qθ(y∖x)dQ(x,c) log (q'y∖X)) dP(x) → 0,	(38)
j y=1	∖ πy /
almost surely as n → ∞. The proof for equation 35 is similar to that of equation 33; therefore, it is
omitted.
Proof of equation 33: It is clear that
SUp
θ
1 K
一 m max
n1 2(y,z)∈J
i=1
z；0)xi + T (θy,z )
max
(y,z)∈J
z；0)x + T (θy,z) dP (x)
≤ sup
∣s0I≤∣J∣
一	max ST
n1 二 s∈s0
i=1
[xi, 1] - J
max ST [x, 1]dP (x)
where [x, 1] ∈ RD(O)+ 1 denotes the vector forms by concatenating 1 to X ∈ RD(O) and S0 in the above
supremum is the set of finite elements in RD(O)+ 1. Therefore, to achieve the result of equation 33, it
is sufficient to show that
SUp
∣s0I≤∣J∣
1 nι
ɪ X
n1匕
max st
s∈S0
[xi, 1] - /
max st [x, 1]dP (x)
→ 0.
(39)
33
Under review as a conference paper at ICLR 2019
To obtain the conclusion of equation 39, we utilize the classical result with bracketing entropy to
establish the uniform laws of large number (cf. Lemma 3.1 in van de Geer (2000)). In particular,
we denote G to be the family of function on RD(0) with the form fS0 (x) = max s> [x, 1] where
s∈S0
S0 ∈ O|J|, which contains all sets that have at most |J | elements in RD(0)+1. Due to the assumption
with distribution P, we can restrict O|J| to contain only set S0 with elements in B(R), which is a
closed ball of radius R on RD(0)+1. By means of Lemma 2.5 in (van de Geer, 2000), we can find
a finite set Eδ such that each element in B(R) is within distance δ to some element of Eδ for all
δ > 0. We denote O|J| (δ) to be the subset of O|J| such that it only contains sets with elements in
Eδ for all δ > 0. Now, for each set S0 = {s1, . . . , sk} ∈ O|J|, we can choose corresponding set
S = {s1,..., sk} ∈ O∣j∣ (δ) such that ∣∣si - Sik ≤ δ for all 1 ≤ i ≤ k. Now, We denote
f s (x) = max s> [x, 1] + δ∣∣ [x, 1] k,
s∈S
fS(x) = max S> [x, 1] - δk [x, 1] k
一S	s∈S
for any S ∈ O∣j∣ (δ). It is clear that fS(x) ≤ fs(x) ≤ fS(x) for all X ∈ RD(O). Furthermore, we
also have that
Es(X)- fS(x))dP(x) = 2δ/ k[x,1]kdP(x) ≤ 2δ(/ kxkdP(x) + fj.
For any > 0, by choosing δ <
2(r kx∣∣dP(x) + 1) then we will have that R(fS(X)-
fs(x))dP(x) < e. It implies that the e-bracketing entropy of G is finite for the L1 norm with
distribution P (for the definition of bracketing entropy, see Definition 2.2 in van de Geer (2000)).
According to Lemma 3.1 in van de Geer (2000), it implies that G satisfies the uniform law of large
numbers, i.e., equation 39 holds.
Proof of equation 34: To achieve the conclusion of this claim, we will need to rely on the control
of Rademacher complexity based on Vapnik-Chervonenkis (VC) dimension. In particular, we firstly
demonstrate that
sup
|S0 |=k
1 n1
一 E1/	]	- /1 f	'∣	dP(χ)
n1 i=1	j=arg max[xi,1]>s0l	j=arg max [x,1]> s0l
l	1≤1≤∣S0I	J	I	1≤1≤∣S0I	J
→0
(40)
almost surely as n1 → ∞ for each 1 ≤ j ≤ k and k ≥ 1 where the supremum is taken with respect
to S0 = {s01, . . . , s0k}. For each j, we denote the Rademacher complexity as follows
Rn1 = E sup
|S0 |=k
n1
一X σi1
n1 i=1
j=arg max [xi ,1]>s0l
[ ι≤ι≤∣s0∣
where σ1, . . . , σn1 are i.i.d. Rademacher random variables, i.e., P(σi = -1) = P(σi = 1) = 1/2
for 1 ≤ i ≤ n1 . Then, for any n1 ≥ 1 and δ ≥ 0, according to standard argument with Rademacher
complexity (Vershynin, 2011),
sup
|S0|=k
1 n1
一 E 1 j	'∣	- 1(	d	dP(x)
n1 i 1	j=argmax[xi,1]>s0l	j=arg max [x,1]> s0l
I	1≤1≤∣S0I	J	I	1≤1≤∣S0I	J
with probability at least 1 -
2 exp
≤ 2Rn1 + δ
According to Borel-Cantelli’s lemma, to achieve
equation 40, it is sufficient to demonstrate that Rn1 → 0 as n1 → ∞.
To achieve that result, we will utilize the study of VC dimension with partitions (cf. Section
21.5 in Devroye et al. (1996)). In particular, for each set S0 = (s01, . . . , s0k), it gives rise to
the partition Ai = X ∈ RD(0) : [X, 1]>s0i ≥ [X, 1]>s0l ∀ l ∈ {1, . . . , k} as 1 ≤ i ≤ k. For
)
34
Under review as a conference paper at ICLR 2019
our purpose with equation 40, it is sufficient to consider Pn1
Aj,iS6=jAi
, which is a par-
tition of B(R), for each set S0 with k elements. We denote F to be the collection of all Pn
for all S0 with k elements and B(Pn) the collection of all sets obtained from the unions of ele-
ments of Pn. For each data (x1, . . . , xn1), we let NF(x1, . . . , xn1) the number of different sets in
{(x1, . . . , xn1) ∩ A : A ∈ B(Pn1 ) for Pn1 ∈ F}. The shatter coefficient of F is defined as
∆n1 (F) = s(F, n1) = max NF(x1, . . . , xn1).
(x1 ,...,xn1 )
According to Lemma 21.1 in Devroye et al. (1996), ∆m (F) ≤ 4∆n∕F) where △" (F) is the
maximal number of different ways that n1 points can be partitioned by members of F. Now, for
each element Pn1 = Aj , S Ai of F, it is clear that the boundaries between Aj and S Ai are
i6=j	i6=j
subsets of hyperplanes. From the formulation of Aj , we have at most k - 1 boundaries between
Aj and S Ai. From the classical result of Dudley (1978), each n1 points in B(R) can be splitted
i6=j
by a hyperplane in at most n1D(0)+1 different ways as the VC dimension of the hyperplane is at
most D(O) + 1. Asa consequence, we would have ∆n (F) ≤ n1D()+ 1)(k-1), which leads to
△n1(F) ≤ 4n(1D(0)+1)(k-1).
Going back to our evaluation with Rademacher complexity Rn1 , by means of Massart’s lemma, we
have that
E Eσ sup
|S0 |=k n1
n1
一X σi1
n
i=1
j= arg max[xi ,1]>s0l
I	1≤1≤∣S0I	J
|x1, . . . ,xn1
n1
2 log 2NF (x1, . . . , x
2(log 8 + (D(O) + 1)(k - 1) lognι) → 0 (41)
n1
as n1 → ∞. Therefore, equation 40 is proved.
Proof of equation 36: To achieve the conclusion of this claim, we firstly demonstrate that
sup
|S0|=k
n - n1
n
1	0 1{yi=l}
i=n1+1	j=argmax[xi,1]>s0l
L	ι≤ι≤∣so∣	J
-1
j= arg max [x,1]> s0l
I	1≤1≤∣S0I	J
(42)
almost surely as n → ∞ for each 1 ≤ j ≤ k and 1 ≤ l ≤ K where k ≥ 1 and the supremum is taken
with respect to S0 = {s01, . . . , s0k}. The proof of the above result will rely on VC dimension with
Voronoi partitions being established in equation 34. In particular, according to the standard argument
with Rademacher complexity, it is sufficient to demonstrate that
R0n = E sup
|S0|=k
1
n - n1
n
σi1	1{yi=l}
i=n +1	j=argmax[xi,1]>s0l
L	ι≤ι≤∣s0∣	J
→ 0.
≤ E
1
35
Under review as a conference paper at ICLR 2019
By means of the inequality with Rademacher complexity in equation 41, we obtain that
Rn
n—nι	/
S ~XvE (E∖*
n—nJ
S E
v=0
sup
Sl = k
n — nι
1
n - nι
n
5S σi1 f	「i丁八 1{yi=l}
i=nι + 1	j=argmax[χi,l]Tsι
I	J-SlSIS I	j
nι +V
∑S σi1[	]
i=m+ι	v=asgsmax[Xi,1]Tslj
E ∈
∈ Av)
pV(1 -Pi)n-ni-v n - n1
n—nJ
S
v = 1
n — nι
2Qθg8+ (D⑼ + 1)(k - I)IOg V) PV (1 - P)n—nj—v (n - n1
∕2log8 XJ
n - n1
V=1
pV(1 -Pi)n-nj-v n - n1
2(D(0) + 1)(k — 1) log(n — nι)
n - n1
n—nj
S
v=1
v log v
(n — nι) log(n — nι)
-PV(1 -PiL…n -n1
Q 20n8^+/
2(D(0) + 1)(k — 1) log(n — nι)
n - n1
)n—nj
S PV (1 - Pi 广
v=1
nj—v n - n1
Q n⅛⅛+∖∕
2(D ⑼ + 1)(k - 1)log(n -n1 ) (1 — (1 - Pi )n—nj ) → 0
n — n1
as n → ∞ where c = (cn—nj + 1,∙∙∙,cn) and AV is the set of c such that there are exactly V values
of yi to be l for 0 ≤ V ≤ n - n> The final inequality is due to the fact that v/(n - 1) ≤ 1 and
v log v/ {(n — 1) log(n — 1)} ≤ 1 for all 1 ≤ V ≤ n — n> Therefore, we achieve the conclusion of
equation 42.
Now, coming back to equation 36, by means of triangle inequality, we achieve that
K
EnI) ≤ S sup
l=1 θ
1 S S1	1	(Ilh3i,z,0)k2
n - n1	(	(	∖ 1 {yi=l} I	2
1 i=nj + 1 z0∈L < z0 = argmaxl hτ (纭/;0)3十丁(θyi,z)) >	、
L 1(	(	γ1{y=l}
∈L < z0 = arg max I hT(y,z;0)x+r (θy,z) J >
kh(y,z';0)||2
-log ∏y	dQ(x,y)
Λ L |h(i,z0；0)k2	,
≤ £ sup £ " J" - log ∏l
l = 1 θ z0∈L
-1
1
n - n1
n
Σ 1(	(	∖11{yi=l}
i=nj + 1 I z0 = argmaxl hτ (ya,z;0)g+T 电方)J >
/	、1 1{y=l}dQ(X, y) → 0
z0 = arg max hT(y,z;0)x+「(θy,z)	>
g∈J ∖I
where the last inequality is due to the results with uniform laws of large numbers from equation 42 and
thefactthat 网1，；0)『
of equation 36.
-log ∏l is bounded for all l and z0 ∈ L. Hence, we obtain the conclusion
≤
≤
+
≤
1
v
v
1
v
v
v
1
v
2
v
-log πVi
36
Under review as a conference paper at ICLR 2019
Proof of equation 37: For this claim, we have the following inequality
En(2)
sup
{Sy0 },{πy}
1
n - n1
n exp max s>[xi, 1] + log πyi
s∈Sy0i
log og -K	7	Y
i=n1 +1	P exp max s> [xi, 1] +log	πl
'=1	∖ s∈S0	J
≤
exp m∈ax s> [x, 1] +log πy
-/ log K--------SF---------------------YdQ(χ,y)
P exp max s> [x, 1] +log πl
'=ι	s∈ s∈Sι	J
K	n exp max s>[xi,1] + log πyi
1	s∈Sy0i
≤ t r SuP	n-n:t log -K~~7------------------------------------V 1{yi=i}
'=1 {Sy},{πy}	i=nι + 1	P exp( maxs>[xi, 1] + log∏ι )
`=:	s∈Sι0
-
exp max s> [x, 1] + log πy
log -K---S^y-------------------Y 1{c=i}dQ(x, y)
P exp max s> [x, 1] + log πl
`=:	S∈Sι0
K
XFn,l
`=:
where Sy0 in the above supremum stands for the collection of sets S:0 , . . . , SK0 such that |Sy0 | ≤ |L|
and elements in Sy0 are in RD(0)+:. Therefore, to achieve the conclusion of equation 37, it is sufficient
to demonstrate that Fn,l → 0 almost surely as n→ ∞ for each 1 ≤ l ≤ K.
In fact, for each 1 ≤ l ≤ K, we denote G to be the family of function on RD(0) × {1, . . . , K} of the
form
exp m∈ax s> [x, 1] + log πy
f{sy },{∏y }(x,y) =log -K~~U----------------------Y 1{y=i}
P exp max s> [x, 1] + log πl
`=:	S∈Sι0
for all (x, y ) where S:0 , . . . , SK0 ∈ O|L| , which contains all sets that have at most |L| elements in
CD(~1	K
RD +1, and {∏y} satisfy that Eny = 1 and ∏y ≥ Y for all 1 ≤ y ≤ K. Due to the assumption
y=:
with distribution P, we can restrict O|L| to contain only set S0 with elements in the ball B(R)
of radius R on RD(0) +:. By means of Lemma 2.5 in (van de Geer, 2000), we can find a finite
set Eδ such that each element in B(R) is within distance δ to some element of Eδ for all δ > 0.
Additionally, there exists a set ∆(δ) such that for each (π:, . . . , πK), we can find a corresponding
element (n：,...,∏K) ∈ ∆(δ) such that k(∏ι,...,∏κ) - (n：,...,∏K)k ≤ δ. We denote
F(δ) ={ {Sy} , {∏y} : elements of Sy in Eδ, and(n：,..., nκ) ∈ ∆(δ)}
for all δ > 0.
For each element Sy0 }yK=： , {ny}yK=：, we can choose the corresponding element
nSy0 τ,{∏y}K=1 ∈ F(δ) such that Sy = {sy1, . .. , syky }, Sy = nsy1, * * * , s[ky }
satisfy ksyj - s0yj k ≤ δ for all 1 ≤ y ≤ K and 1 ≤ j ≤ ky . Additionally,
37
Under review as a conference paper at ICLR 2019
k(∏ι,... ,∏κ) - (∏ι,…，∏κ )k ≤ δ. With these notations, We define
exp(max s> [x, 1] + log ∏y)
f {sy },{∏y }(x,y) = log( K—s∈Sy------------------)1{y=i} + 2δk[x, 1]k + 2δ∕Y,
E exp(max s>[x, 1] + log πτ)
T =1	s∈sT
exp(max s> [x, 1] + log ∏y)
/	s∈S0	∖
f{S0},{πy}(x,y) = logyK---------------------------p{y=l} - 2δk[x, 1]k - 2δ∕γ
E exp(max s>[x, 1] + log πτ)
τ =1	s∈sT
for any ({s；} , {∏y}) ∈ F(δ). By means of CaUchy-SchWarz's inequality, we have
sy>i[x, 1]1{y=l} - (s0yi)>[x, 1]1{y=l} ≤ ksci - s0yikk[x, 1]k1{y=l}
≤ δk[x, 1]k
for all x and 1 ≤ i ≤ k . Additionally, the folloWing also holds
syi [x, 1]1{y=l} - (syi) [x, 1]1{y=l} ≥ -δk[x, 1]k.
Furthermore, | log ∏y - log ∏y | ≤ log(1 + δ∕γ) ≤ δ∕γ. Hence, we obtain that
exp(max s> [x, 1] + log πy)
K
exp(m∈ax s> [x, 1] + log πτ)
exp(max s>[x, 1] + log πy + δ∣∣[x, 1]k + δ∕γ)
≤	s∈Sy______________________________
- K
P exp(max s>[x, 1] + log πτ - δ∣∣[x, 1]∣∣- δ∕γ)
τ =1	s∈S'τ
exp(max s> [x, 1] + log ∏y)
s∈S0
≤ -K----------------------------eχp(2δk[χ, 1]k +2δ∕γ).
E exp(max s>[x, 1] + log πτ)
τ =1	s∈sT
Similarly, we also have
exp(maχ s> [x, 1] + log ∏y)	exp(ma^ s> [x, 1] + log πy)
-K~~s∈Sy------------------- ≥ -K-s∈Sy---------------------exp(-2δk[x, 1]k- 2δ∕γ).
E exp(max s>[x, 1] + log ∏)	E exp(max s>[x, 1] + log 示T)
τ =1	s∈sT	τ =1	s∈ST
As a consequence, we achieve that
f {Sy },{∏y} Ey) ≤ f{Sy},{∏y} (X，y) ≤ f {Sy },{∏y} (X，y)
for all (x, y). With the formulations of f {g, } {∏ } (x, c) and f {耳，} {∏ } (x, y), we have
/(f {sy },{∏y}Ey)- f{sy },{∏y}(X,y) 卜QEy)
=4δ/ k[x, 1]∣∣dQ(x,y)+4δ∕γ ≤ 4δ(/ ∣∣x∣∣dQ(x, y) + 1)+ 4δ∕γ.
For any e > 0, by choosing δ <	-FU “	'、--------- then we will have
_	4 / IlXkdQ(X,y) + 4 + 4∕γ
J(f {s，} {∏ }(x, y) - f {go } {∏ }(x, y))dQ(X, y) < e. It implies that the e-bracketing entropy
of G is finite for the L1 norm with distribution Q. Therefore, it implies that G satisfies the uniform
law of large numbers, i.e., Fn,l → 0 almost surely as n → ∞ for all 1 ≤ l ≤ K. As a consequence,
the uniform law of large number result equation 37 holds.
Going back to the original problem, denote θe0
({e0(y)%, {e0(')}L=1,{e0}3
{e0(')}L
the optimal solutions of population partially labeled LDCE (Note that, the existence of these optimal
38
Under review as a conference paper at ICLR 2019
solutions is guaranteed due to the compact assumptions with the parameter spaces θ`, Ω, and Ξ' for
all 1 ≤ ` ≤ L). Then, according to the formulation of partially labeled LDCE, we will have that
Yn
≤
n1
XX if
i=1 (y0,z0)∈J	(y0,z0)=arg max
I	(y,z)∈J
x>e0(y,z；O)+τ (ey,z)
kχi -e0(y0,z0;0)k2
2
~

-log(Po(y0,z0))) + X X 1f	( ~	kXi-h0(yi，Z0；0)k2
i	i=nι + 1 z0∈L Z z0=argmaxl x>e0 (y,z*)+τ@产)I > ×
- log(pe0(yi, z0))	-
^C^ X log qeo (yi∣Xi) + αKL X X qeo (y∣Xi )log( qe0⅛)
n - n1	θ	n	θ	πy
1 i=n1+1	i=1 y=1	y
Dn
for all n ≥ 1 where we have the following formulations
e0(y, z; 0) = Λo(z; 1)... Λo(z; L)e0(y),
A0(z；') = E s(',p)T(t；',P)B(',p)ro(',p),
P∈P(')
From the results with uniform laws of large numbers in equation 33, equation 34, equation 35,
equation 36, equation 37, and equation 38, we obtain that
n1
n X X 1(	( ~	〜
1 i=1(y0,z0)∈J ( (y0,z0) = argmax x>h0(y,z;0)+「(θy,z)
I	(y,z)∈J
IM -e0(y0,z0Mk2
2
- log(pe0 (y0, z0))
→ ZX 1f	( ~	~kx-h0(y"0)k2 -log(po(y0,z0)力dP(x),
J (y0,z0)∈J ( (y0,z0) = argmax( xτe0 (y,z*)+τ Gy∕ )J
n⅛ X Xiʃ (L ~ χ(kxi-e0(y"0)k2-iog(p0(yi,z0)))
1 i=nι + 1 z0∈L Z z0 = argmaxl xτh0 (y,z;0)+T (θy,z )J1\
→ ZX 1f (~	~、］( kx-h0(y,z'0)k2 - log(p0(y,z0))卜Q(x,y),
J z0∈L Z z0 = arg maxi XTe0 (y,z*)+τ (ey,z) ) } '	)
1
n - n1
n
E log qθo (yi|xi) → /
i=n1+1
log qeo (y|x)dQ(x,y),
-Xxqeo(y∣χi)log(qθ0("i	Xqeo(y|x)log(qθ0(ylx))dP(χ)
n i=1 y=1	πy	y=1	πy
almost surely as n → ∞. Combining with the fact that n`/n → λ, the above results lead to Dn → Y
almost surely as n → ∞. Therefore, we have lim Yn ≤ Y almost surely as n → ∞. On the other
n→∞
hand, with the results of uniform laws of large numbers in equation 33, equation 34, equation 35,
39
Under review as a conference paper at ICLR 2019
equation 36, equation 37, and equation 38, we also have that
n1
n X X 1(	(
1 i=1 (y0,z0)∈J	(y0,z0)=arg max x
kxi - eh(y0, z0; 0)k2
-∣-~ ,	-・ ,ʃr
:>h(y,z;0)+T (θy,z )
- log(pe(y0, z0))
→/ X 1/	(
(y0,z0)∈J	(y0,z0)=arg max x
I	(y,z)∈J
kx - eh(y0, z0;0)k2
- log(pe(y0, z0)) dP (x),
-T-~ .
：Th(y,z；0)+T (θy,z )
n - n1
n
X X1
i=n1+1 z0∈L
kxi -eh(yi, z0;0)k2
z0 = arg max ( XT h(y,z;0)+T(θy,z )
- log(pe(yi , z0))
→ X1
z0∈L
kx - eh(y, z0;0)k2
- log(pe(y, z0)) dQ(x, y),
z0=arg max xτh0(y,zQ)+τ (θy,z)
z∈L
1
-~
■~

2
■ ~

2
~

2

2
1
n - n1
n
log logqe(yi∖χi) → l logqe(y∣χ)dQ(χ,y),
i=n1 +1
n XX qg⅛∖χi)log(qe：IXi)) → Z X qe(y∖χ)log(qeyx卜P(X)
i=1 y=1	y	y=1	y
1 1 1 :
almost surely as n → ∞ Where θ
{e(y)}y=ι, {e⑶O* 1, {ey}K=1, {e⑶O*/ isthe OPtimaI
solution of partially labeled LDCE and

〜
〜
h(y, z; O) = A(ZI)…A(Z L)e(y),
Λ(z; ')= X s(',p)T(t; ',p)B(',p)Γ(',p),
P∈P(')
pe(y0, z0) = exp(τ (θey0,z0) + log πey0 /( X exp( m∈aLx τ (θey0,z0) + log πey
Hence, we eventually achieve that
Yn → αRC
E i/	(
(y0,z0)∈J	(y0,z0)=arg max x
I	(y,z)∈J ∖
>e(y,z；0)+T (ey,z)
-log(e(y0, z0DdP(X)) +(1- λ) (/ X 1
kx - eh(y0, z0; 0)k2
kx - eh(y, z0;0)k2
z0=arg max xτ h0 (y ,z;0)+t (θy,z )
z∈L
- log(pe(y, z0)) dQ(x, y)	- αCE	log qθe(y∖x)dQ(x, y)
K
+αKL	qθe(y∖x) log
y=1
qe(ylx)∖dP(x) ≥ Y
πy
almost surely as n → ∞. As a consequence, We achieve Yn → Y almost surely as n → ∞. We
reach the conclusion of the theorem.
PROOF OF THEOREM .7 The proof argument of this theorem is a direct application of the
results with uniform laws of large numbers in the proof of Theorem .6. In fact, we define
Fe0()
(S, {∏y} , {b(')}) : S = h(y, z; 0) : (y,z) ∈ J ,
and	inf
(So,{∏y },{bo(')})∈G(Feo)
K
So) + X ∖∏y - ∏0∖ + X kb(') -eo(')k
y=1	z∈L
■ ~

-~

2
2
40
Under review as a conference paper at ICLR 2019
for any > 0. Since the parameter spaces of θ are compact sets, the set F0 () is also a compact set
for all > 0. Denote
g (S,{∏y} ,{b(')})
αRC
E 1/	(
(y0,z0)∈J ( (y0,z0)= h h>(y,z;0)x+T (θy,z)
kx-h(y0,z0;0)k2
- log(πy0,z0)
+(I- λ)( ZX 1(	(
z j z0∈L z z0=arg maxi h>(y,z;0)x+T (θy,z)
kx - h(y, z0; 0)k2
- log(πy,z0) dQ(x, y)
ZK	qθ (y |x)
log qθ (y∣χ)dQ(χ,y) + ακL∕Σqθ (y∣χ)dQ(χ, c) log (-θ∏——)dP (x)
C	11∕C(^	、 r T / ∕1∖ T ∖	1 -	.1 -I r' ∙ . ∙	∕'	/	∖	1	.1
for all (S, {∏y} , {b(')}). From the definition of F⅛(e), We have that
g(S, {∏y},{b(')}) > g(So, {e0}, {eo(')})
for all (S, {∏y} , {b(')}) ∈ F0(e) and (So, {e0} , {eo(')})
We further have that
∈ G(F0). As F0() is a compact set,
—)g(S",…>g
(So,{e0}, {eo(')})
for all (So, {e0} , {e0(')}) ∈ G(Fo) and e > 0.
NoW, according to the uniform laWs of large numbers established in the proof of Theorem .6, We
have that Yn → g Sn, {πey } ,
almost surely as n → ∞. According to the result of
Theorem .6, it implies that g	→ g Seo, {πeyo} , ebo(`)	almost surely for all
Seo, {πeyo} , ebo(`)	∈ G(Feo). Therefore, for each > 0 We can find sufficiently large N such that
We have
K
,So)+X ι∏y- ∏y i + χ ke(`) - eo(`)k
y=1	z∈L
almost surely for all n ≥ N . As a consequence, We achieve the conclusion of the theorem.
PROOF OF THEOREM .8 The proof of the theorem is an application of Theorem 11 for gen-
eralization bound With margin from Koltchinskii & Panchenko (2002) based on an evaluation of
Rademacher complexity. In particular, We denote
Jn = {hTn (X, y) : RD" × {1, . . . , K} → RI
hτn(x,y) = max X h>(y, z;0)x + T(θy,z) + + log∏y ∀ (x,y) for some ∣L(τn)| ≤ τn∣L∣
Now, We denote Jn = {hτn(., y) ： y ∈ {1,..., K} , hτn ∈ Jn}. For any δ > 0, using the same
argument as that of the proof of Theorem 11 in (Koltchinskii & Panchenko, 2002), With probability at
least 1 - δ, we have
Rθ(fτ n)	≤	infιl {Rn,r(fτ n )+8K (2K - 1) <n(Jn)
Γ∈(o,1]
loglog2(2Γ-1)
log(2δ-1)
2n
(43)
+
n
2
2
41
Under review as a conference paper at ICLR 2019
where <n(Jn) is Rademacher complexity of Jn, which in our case is defined as
<n(Jn) = ESUP	SUp
θ ∣L(T n)∣≤T n|L|
1 X σi( max h h>(y,z;0)xi + T (θy,z) 1+log Ky
n M S(T "I	J
where σ1,... ,σn are i.i.d. Rademacher random variables. Since Y is the lower bound of ∏y for all
1 ≤ y ≤ K, we obtain that
<n(J)	≤ E SUp SUp
θ ∣L(T n)∣≤T n∣L∣
+ E SUp SUp
θ ∣L(T n)∣≤T n∣L∣
≤ E SUp SUp
θ ∣L(T n)∣≤T n∣L∣
1 X σi( max h hτ (y,z; 0)xi + T(θy,z)
n	∖ g∈L(Tn) I
2=1'
1 n
-fθi log πy
n y
2=1
1 X σi( max <j hτ(y, z; 0)xi + T(Oy,z)
n Y ∖g∈L(τn) I
2=1'	、
,| log Y|
+--------
Furthermore, we have the following inequalities
E SUp SUp
θ ∣L(T n)∣≤T n∣L∣
1 X σi( max h h>(y,z;0)xi + T(θy,z)
n 2=1	'geL(T "I
1 n
≤ E sup —	σ2 max sτ[x2,1]
∣S0∣≤Cn∣L∣ n =	s∈S'
1n
≤ 2Cn∣L∣E sup - V^σ2sτ[x2,1]
s∈B(R) n M
1 n
≤ 2Cn∣L∣RE - Vσ2[x2,1]
n z—z
2=1
≤ 2Cn∣L∣(R2 + 1)
一	√n
where the final inequality is due to Cauchy-Schwartz,s inequality. Combining the above results with
equation 43, we achieve the conclusion of the theorem.
42
Under review as a conference paper at ICLR 2019
Appendix D
In this appendix, we provide detail descriptions for several simulation studies in the main text.
.16 Architecture of the Network Used in Our Semi-Supervised Experiments
Table 6: The network architecture used in all of semi-supervised experiments on CIFA10, CIFAR100
and SVHN.
NAME	Description
input conv1a conv1b conv1c pool1 drop1 conv2a conv2b conv2c pool2 drop2 conv3a conv3b conv3c pool3 dense output	32 X 32 RGB image 128 filters, 3 × 3, pad = 'same', LReLU (α = 0.1) 128 filters, 3 × 3, pad ='same',LReLU (α = 0.1) 128 filters, 3 × 3, pad ='same',LReLU (α = 0.1) Maxpool 2 × 2 pixels Dropout, P = 0.5 256 filters, 3 × 3, pad ='same',LReLU (α = 0.1) 256 filters, 3 × 3, pad = 'same', LReLU (α = 0.1) 256 filters, 3 × 3, pad ='same',LReLU (α = 0.1) Maxpool 2 × 2 pixels Dropout, p = 0.5 512 filters, 3 × 3, pad = 'valid’, LReLU (α = 0.1) 256 filters, 1 × 1, LReLU (α = 0.1) 128 filters, 1 X 1, LReLU (α = 0.1) Global average pool (6 X 6 → 1×1 pixels) Fully connected 128 → 10 Softmax
.17 Training Details
.17.1 Semi-Supervised Learning Experiments on CIFAR10, CIFAR 1 00, and SVHN
The training losses are discussed in Section 2.3. In addition to the cross-entropy loss, the recontruction
loss, and the RPN regularization, in order to further improve the performance of NRM, we introduce
two new losses for training the model. Those two new losses are from our derivation of batch
normalization using the NRM framework and from applying variational inference on the NRM. More
details on these new training losses can be found in Appendix A.4. We compare NRM with state-of-
the-art methods on semi-supervised object classification tasks which use consistency regularization,
such as the Π model (Laine & Aila, 2017), the Temporal Ensembling (Laine & Aila, 2017), the
Mean Teacher (Tarvainen & Valpola, 2017), the Virtual Adversarial Training (VAT), and the Ladder
Network (Rasmus et al., 2015). We also compare NRM with methods that do not use consistency
regularization including the improved GAN (Salimans et al., 2016) and the Adversarially Learned
Inference (ALI) (Dumoulin et al., 2017).
All networks were trained using Adam with learning rate of 0.001 for the first 20 epochs. Adam
momentum parameters were set to beta1 = 0.9 and beta2 = 0.999. Then we used SGD with decayed
learning rate to train the networks for another 380 epochs. The starting learning rate for SGD is
0.15 and the end learning rate at epoch 400 is 0.0001. We use batch size 128. Let the weights for
the cross-entropy loss, the reconsruction loss, the KL divergence loss, the moment matching loss,
and the RPN regularization be αCE, αRC, αKL, αMM, and αPN, respectively. In our training,
αCE = 1.0, αRC = 0.5, αKL = 0.5, αMM = 0.5, and αPN = 1.0. For Max-Min cross-entropy,
we use αmax = αmin = 0.5.
.17.2 Supervised Learning Experiments with Max-Min Cross Entropy
Training on CIFAR10 We use the 26 2 x 96d”Shake-Shake-Image” ResNet in Gastaldi (2017) with
the Cutout data augmentation suggested in DeVries & Taylor (2017) as our baseline. We implement
the Max-Min cross-entropy on top of this baseline and turn it into a Max-Min network. In addition to
Cutout data augmentation, standard translation and flipping data augmentation is applied on the 32 x
32 x 3 input image. Training procedure are the same as in (Gastaldi, 2017). In particular, the models
were trained for 1800 epochs. The learning rate is initialized at 0.2 and is annealed using a Cosine
43
Under review as a conference paper at ICLR 2019
function without restart (see (Loshchilov & Hutter, 2016)). We train our models on 1 GPU with a
mini-batch size of 128.
Training on CIFAR10 We use the Squeeze-and-Excitation ResNeXt-50 as in Hu et al. (2018) as
our baseline. As with CIFAR10, we implement the Max-Min cross-entropy for the baseline and
turn it into a Max-Min network. During training, we follow standard practice and perform data
augmentation with random-size cropping (Szegedy et al., 2015) to 224 x 224 x 3 pixels. We train
the network with the Nesterov accelerated SGD for 125 epochs. The intial learning rate is 0.1 with
momentum 0.9. We divide the learning rate by 10 at epoch 30, 60, 90, 95, 110, 120. Our network is
trained on 8 GPUs with batch size of 32.
.17.3 Semi-Supervised Training on MNIST with 50K Labeled to Get the Trained
Model for Generating Reconstructed Image in Figure 2
The architecture of the baseline CNN we use is given in the Table 7. We use the same training
procedure as in Section .17.1
Table 7: The network architecture used in our MNIST semi-supervised training.
NAME	Description
input conv1 pool1 conv2a conv2b pool2 conv3 pool3 dense output	28 X 28 image 32 filters, 5 × 5, pad = 'full', ReLU Maxpool 2 × 2 pixels 64 filters, 3 × 3, pad = 'valid’, ReLU 64 filters, 3 × 3, pad = 'full', ReLU Maxpool 2 × 2 pixels 128 filters, 3 × 3, pad = ’valid’, ReLU Global average pool (6 × 6 → 1×1 pixels) Fully connected 128 → 10 Softmax
44