Under review as a conference paper at ICLR 2019
Adversarial Vulnerability of Neural Net-
works Increases with Input Dimension
Anonymous authors
Paper under double-blind review
Ab stract
Over the past four years, neural networks have been proven vulnerable to adversarial
images: targeted but imperceptible image perturbations lead to drastically different
predictions. We show that adversarial vulnerability increases with the gradients of
the training objective when viewed as a function of the inputs. For most current
network architectures, We prove that the 'ι-norm of these gradients grows as the
square root of the input size. These nets therefore become increasingly vulnerable
with growing image size. Our proofs rely on the network’s weight distribution
at initialization, but extensive experiments confirm that our conclusions still hold
after usual training.
1	Introduction
Following the work of Goodfellow et al. (2015), Convolutional Neural Networks (CNNs) have been
found vulnerable to adversarial examples: an adversary can drive the performance of state-of-the art
CNNs down to chance level with imperceptible changes of the inputs. A number of studies have tried
to address this issue, but only few have stressed that, because adversarial examples are essentially
small input changes that create large output variations, they are inherently caused by large gradients
of the neural network with respect to its inputs. Of course, this view, which we will focus on here,
assumes that the network and loss are differentiable. It has the advantage to yield a large body of
specific mathematical tools, but might not be easily extendable to masked gradients, non-smooth
models or the 0-1-loss. Nevertheless, our conclusions might even hold for non-smooth models, given
that the latter can often be viewed as smooth at a coarser level.
Contributions. More specifically, we provide theoretical and empirical arguments supporting the
existence of a monotonic relationship between the gradient norm of the training objective (of a
differentiable classifier) and its adversarial vulnerability. Evaluating this norm based on the weight
statistics at initialization, we show that CNNs and most feed-forward networks, by design, exhibit
increasingly large gradients with input dimension d, almost independently of their architecture. That
leaves them increasingly vulnerable to adversarial noise. We corroborate our theoretical results
by extensive experiments. Although some of those experiments involve adversarial regularization
schemes, our goal is not to advocate a new adversarial defense (these schemes are already known),
but to show how their effect can be explained by our first order analysis. We do not claim to explain
all aspects of adversarial vulnerability, but we claim that our first order argument suffices to explain
a significant part of the empirical findings on adversarial vulnerability. This calls for researching
the design of neural network architectures with inherently smaller gradients and provides useful
guidelines to practitioners and network designers.
2	From Adversarial Examples to Large Gradients
Suppose that a given classifier 夕 classifies an image X as being in category 夕(x). An adversarial
image is a small modification of x, barely noticeable to the human eye, that suffices to fool the
classifier into predicting a class different from 夕(x). It is a small perturbation of the inputs, that
creates a large variation of outputs. Adversarial examples thus seem inherently related to large
gradients of the network. A connection, that we will now clarify. Note that visible adversarial
examples sometimes appear in the literature, but we deliberately focus on imperceptible ones.
1
Under review as a conference paper at ICLR 2019
Adversarial vulnerability and adversarial damage. In practice, an adversarial image is con-
structed by adding a perturbation δ to the original image x such that kδ k ≤ for some (small)
number E and a given norm ∣∣∙k over the input space. We call the perturbed input X + δ an E-Sized
∣∣∙k-attack and say that the attack was successful when 夕(X + δ)= 夕(x). This motivates
Definition 1. Given a distribution P over the input-space, we call adversarial vulnerability of a
classifier 夕 to an E-Sized ∣∣∙∣-attack the probability that there exists a perturbation δ of X such that
∣∣δ∣ ≤ E and q(X) =2(X + δ).	(1)
We call the average increase-after-attack Ex〜P [∆L] of a loss L the (L-) adversarial damage (of the
classifier 夕 to an E-Sized ∣∣∙∣-attack).
When L is the 0-1-loss L0/1, adversarial damage is the accuracy-drop after attack. The 0-1-loss
damage is always smaller than adversarial vulnerability, because vulnerability counts all class-changes
of 夕(x), whereas some of them may be neutral to adversarial damage (e.g. a change between two
wrong classes). The L0/1-adversarial damage thus lower bounds adversarial vulnerability. Both are
even equal when the classifier is perfect (before attack), because then every change of label introduces
an error. It is hence tempting to evaluate adversarial vulnerability with L0/1-adversarial damage.
From ∆Lo∕ι to ∆L and to ∂xL. In practice however, we do not train our classifiers with the
non-differentiable 0-1-loss but use a smoother loss L, such as the cross-entropy loss. For similar
reasons, we will now investigate the adversarial damage Ex [∆L(X, c)] with loss L rather than L0/1.
Like for Goodfellow et al. (2015); Lyu et al. (2015); Sinha et al. (2018) and many others, a classifier
夕 will hence be robust if, on average over x, a small adversarial perturbation δ of X creates only a
small variation δL of the loss. Now, if ∣δ ∣ ≤ E, then a first order Taylor expansion in E shows that
δL = δ mδaχJL(X + δ,c) - L(X,c)| ≈ δ max e |dxL ∙ δl = EIIIdxLll1,	⑵
where ∂xL denotes the gradient of L with respect to X, and where the last equality stems from the
definition of the dual norm ∣∣∣∙∣∣∣ of ∣∣∙∣. Now two remarks. First: the dual norm only kicks in because
we let the input noise δ optimally adjust to the coordinates of ∂x L within its E-constraint. This is the
brand mark of adversarial noise: the different coordinates add up, instead of statistically canceling
each other out as they would with random noise. For example, if we impose that ∣δ ∣2 ≤ E, then δ
will strictly align with ∂xL. If instead ∣δ∣∞ ≤ E, then δ will align with the sign of the coordinates of
∂xL. Second remark: while the Taylor expansion in (2) becomes exact for infinitesimal perturbations,
for finite ones it may actually be dominated by higher-order terms. Our experiments (Figures 1 & 2)
however strongly suggest that in practice the first order term dominates the others. Now, remembering
that the dual norm of an 'p-norm is the corresponding 'q -norm, and summarizing, we have proven
Lemma 2. Atfirst order approximation in e , an E-sized adversarial attack generated with norm ∣∣∙∣
increases the loss L at point X by e ∣∣∣∂xL∣∣∣, where ∣∣∣∙∣∣∣ is the dual norm of ∣∣∙∣. In particular, an
e-sized 'p-attack increases the loss by E HaxLIIq where 1 ≤ P ≤ ∞ and P + ɪ = 1.
Consequently, the adversarial damage ofa classifier with loss L to E-sized attacks generated with norm
∣∣∙∣ is e Ex ∣∣∣ ∂x L ∣∣∣. This is valid only at first order, but it proves that at least this kind of first-order
vulnerability is present. We will see that the first-order predictions closely match the experiments,
and that this insight helps protecting even against iterative (non-first-order) attack methods (Figure 1).
Calibrating the threshold E to the attack-norm ∣∣∙∣. Lemma 2 shows that adversarial vulnerability
depends on three main factors: (i) ∣∣∙∣, the norm chosen for the attack (ii) e , the size of the attack,
and (iii) ExIII∂xLIII , the expected dual norm of ∂xL. We could see Point (i) as a measure of our
sensibility to image perturbations, (ii) as our sensibility threshold, and (iii) as the classifier’s expected
marginal sensibility to a unit perturbation. Ex III∂xLIII hence intuitively captures the discrepancy
between our perception (as modeled by ∣∣∙∣) and the classifier,s perception for an input-perturbation
of small size e. Of course, this viewpoint supposes that we actually found a norm ∣∣∙∣ (or more
generally a metric) that faithfully reflects human perception - a project in its own right, far beyond
the scope of this paper. However, it is clear that the threshold E that we choose should depend on the
norm ∣∣∙∣ and hence on the input-dimension d. In particular, for a given pixel-wise order of magnitude
of the perturbations δ, the `p-norm of the perturbation will scale like d1/p. This suggests to write the
threshold Ep used with `p -attacks as:
Ep = E∞ d1/p ,	(3)
2
Under review as a conference paper at ICLR 2019
where ∞ denotes a dimension-independent constant. In Appendix D we show that this scaling also
preserves the average signal-to-noise ratio kxk2 / kδk2, both across norms and dimensions, so that
p could correspond to a constant human perception-threshold. With this in mind, the impatient
reader may already jump to Section 3, which contains our main contributions: the estimation of
Ex k∂xLkq for standard feed-forward nets. Meanwhile, the rest of this section shortly discusses two
straightforward defenses that we will use later and that further illustrate the role of gradients.
A new old regularizes Lemma 2 shows that the loss of the network after an f -sized ∣∣∙k-attack is
LeJlHll (X, C) := L(X, c) +2 MxL|||.	⑷
It is thus natural to take this loss-after-attack as a new training objective. Here we introduced a factor 2
for reasons that will become clear in a moment. Incidentally, for ∣∣∙∣ = ∣∣∙∣2, this new loss reduces to
an old regularization-scheme proposed by Drucker & LeCun (1991) called double-backpropagation.
At the time, the authors argued that slightly decreasing a function’s or a classifier’s sensitivity to input
perturbations should improve generalization. In a sense, this is exactly our motivation when defending
against adversarial examples. It is thus not surprising to end up with the same regularization term.
Note that our reasoning only shows that training with one specific norm ∣∣∣∙∣∣∣ in (4) helps to protect
against adversarial examples generated from ∣∣∙∣. A priori, We do not know what will happen for
attacks generated with other norms; but our experiments suggest that training with one norm also
protects against other attacks (see Figure 2 and Section 4.1).
Link to adversarially-augmented training. In (1), designates an attack-size threshold, while in
(4), it is a regularization-strength. Rather than a notation conflict, this reflects an intrinsic duality
between two complementary interpretations of , which we now investigate further. Suppose that,
instead of using the loss-after-attack, we augment our training set with E-SiZed ∣∣∙∣-attacks X + δ,
where for each training point X, the perturbation δ is generated on the fly to locally maximize the
loss-increase. Then we are effectively training with
1
Le,k∙k(x,c) := 2(L(x,c) + L(x + Eδ,c)),	(5)
where by construction δ satisfies (2). We will refer to this technique as adversarially augmented
training. It was first introduced by Goodfellow et al. (2015) with ∣∣∙∣ = ∣∣∙∣∞ under the name of
FGSM1-augmented training. Using the first order Taylor expansion in E of (2), this ‘old-plus-post-
attack’ loss of (5) simply reduces to our loss-after-attack, which proves
Proposition 3. UP to first-order approximations in e , Le,k∙k = Le,∣∣∣∙∣∣∣ . Sald differently, for small
enough e, adversarially-augmented training with e-sized ∣∣∙∣ -attacks amounts to penalizing the dual
norm ∣∣∣∙∣∣∣ of ∂xL with weight e/2. In particular, double-backpropagation corresponds to training
with '2 -attacks, while FGSM-augmented training COrreSPOndS to an '1 -penalty on ∂xL.
This correspondence between training with perturbations and using a regularizer can be compared
to Tikhonov regularization: Tikhonov regularization amounts to training with random noise Bishop
(1995), while training with adversarial noise amounts to penalizing ∂xL. Section 4.1 verifies the
correspondence between adversarial augmentation and gradient regularization empirically, which
also strongly suggests the empirical validity of the first-order Taylor expansion in (2).
3	ESTIMATING ∣∂xL∣q TO EVALUATE ADVERSARIAL VULNERABILITY
In this section, we evaluate the size of ∣∂x L∣q for standard neural network architectures. We
start with fully-connected networks, and finish with a much more general theorem that, not only
encompasses CNNs (with or without strided convolutions), but also shows that the gradient-norms are
essentially independent of the network topology. We start our analysis by showing how changing q
affects the size of ∣∂xL∣q. Suppose for a moment that the coordinates of ∂xL have typical magnitude
∣∂xL∣. Then ∣∣∂xL∣q scales like d1/q∣∂xL∣. Consequently
Ep IIdxLkq H Ep d1/ ∣∂xL∣ H d ∣∂xL∣.	(6)
1FGSM = Fast Gradient Sign Method
3
Under review as a conference paper at ICLR 2019
This equation carries two important messages. First, we see how k∂xLkq depends on d and q. The
dependence seems highest for q = 1. But once we account for the varying perceptibility threshold
Ep a d1/p, We see that adversarial vulnerability scales like d ∙ ∣∂xL∣, whatever 'p-norm We use.
Second, (6) shows that to be robust against any type of 'p-attack at any input-dimension d, the average
absolute value of the coefficients of ∂xL must groW sloWer than 1/d. NoW, here is the catch, Which
brings us to our core insight.
3.1	Core Idea: One Neuron with Many Inputs
In order to preserve the activation variance of the neurons from layer to layer, the neural weights are
usually initialized with a variance that is inversely proportional to the number of inputs per neuron.
Imagine for a moment that the network consisted only of one output neuron o linearly connected to
all input pixels. For the purpose of this example, we assimilate o and L. Because we initialize the
weights with a variance of 1/d, their average absolute value | ∂χ o | ≡ | ∂x L| grows like 1 / √d, rather
than the required 1/d. By (6), the adversarial vulnerability E k∂xokq ≡ E k∂xLkq therefore increases
like d/ʌ/d = √d.
This toy example shows that the Standard initialization scheme, which preserves the Variancefrom
layer to layer causes the average coordinate-size ∣∂xL∣ to grow like 1∕√d instead of 1/d. When
an '∞-attack tweaks its E-sized input-perturbations to align with the coordinate-signs of ∂xL, all
coordinates of ∂xL add up in absolute value, resulting in an output-perturbation that scales like e√d
and leaves the network increasingly vulnerable with growing input-dimension.
3.2	Generalization to Deep Networks
Our next theorems generalize the previous toy example to a very wide class of feedforward nets with
ReLU activation functions. For illustration purposes, we start with fully connected nets and only then
proceed to the broader class, which includes any succession of (possibly strided) convolutional layers.
In essence, the proofs iterate our insight on one layer over a sequence of layers. They all rely on the
following set (H) of hypotheses:
H1 Non-input neurons are followed by a ReLU killing half of its inputs, independently of the
weights.
H2 Neurons are partitioned into layers, meaning groups that each path traverses at most once.
H3 All weights have 0 expectation and variance 2/(in-degree) (‘He-initialization’).
H4 The weights from different layers are independent.
H5 Two distinct weights w, w0 from a same node satisfy E [w w0] = 0.
Ifwe follow common practice and initialize our nets as proposed by He et al. (2015), then H3-H5 are
satisfied at initialization by design, while H1 is usually a very good approximation (Balduzzi et al.,
2017). Note that such i.i.d. weight assumptions have been widely used to analyze neural nets and
are at the heart of very influential and successful prior work (e.g., equivalence between neural nets
and Gaussian processes as pioneered by Neal 1996). Nevertheless, they do not hold after training.
That is why all our statements in this section are to be understood as orders of magnitudes that are
very well satisfied at initialization in theory and in practice, and that we will confirm experimentally
after training in Section 4. Said differently, while our theorems rely on the statistics of neural nets at
initialization, our experiments confirm their conclusions after training.
Theorem 4 (Vulnerability of Fully Connected Nets). Consider a succession of fully connected
layers with ReLU activations which takes inputs x of dimension d, satisfies assumptions (H), and
outputs logits fk (x) that get fed to a final cross-entropy-loss layer L. Then the coordinates of ∂xfk
grow like 1∕√d, and
∣∣∂χLkq a d1 -2	and Ep IIdxLkq a √d.	(7)
These networks are thus increasingly vulnerable to `p -attacks with growing input-dimension.
Theorem 4 is a special case of the next theorem, which will show that the previous conclusions are
essentially independent of the network-topology. We will use the following symmetry assumption
on the neural connections. For a given path p, let the path-degree dp be the multiset of encountered
in-degrees along path p. For a fully connected network, this is the unordered sequence of layer-sizes
4
Under review as a conference paper at ICLR 2019
preceding the last path-node, including the input-layer. Now consider the multiset {dp}p∈P(x,o) of
all path-degrees when p varies among all paths from input x to output o. The symmetry assumption
(relatively to o) is
(S) All input nodes x have the same multiset {dp}p∈P(x,o) of path-degrees from x to o.
Intuitively, this means that the statistics of degrees encountered along paths to the output are the same
for all input nodes. This symmetry assumption is exactly satisfied by fully connected nets, almost
satisfied by CNNs (up to boundary effects, which can be alleviated via periodic or mirror padding)
and exactly satisfied by strided layers, if the layer-size is a multiple of the stride.
Theorem 5 (Vulnerability of Feedforward Nets). Consider any feed-forward network with linear
connections and ReLU activation functions. Assume the net satisfies assumptions (H) and outputs
logits fk (x) that get fed to the cross-entropy-loss L. Then k∂xfk k2 is independent of the input
dimension d and e? ∣∣∂xLk2 H √d. Moreover, if the net satisfies the symmetry assumption (S), then
∣∂xfk | H 1∕√d and (7) still holds: ∣∂xL∣q H d 1-1 and ep ∣∣∂xL∣q H √d.
Theorems 4 and 5 are proven in Appendix B. The main proof idea is that in the gradient norm
computation, the He-initialization exactly compensates the combinatorics of the number of paths in
the network, so that this norm becomes independent of the network topology. In particular, we get
Corollary 6 (Vulnerability of CNNs). In any succession of convolution and dense layers, strided
or not, with ReLU activations, that satisfies assumptions (H) and outputs logits that getfed to the
cross-entropy-loss L, the gradient ofthe logit-coordinates scale like 1∕√d and (7) is satisfied. It is
hence increasingly vulnerable with growing input-resolution to attacks generated with any `p -norm.
Appendix A shows that the network gradient are dampened when replacing strided layers by average
poolings, essentially because average-pooling weights do not follow the He-init assumption H3.
4 Empirical Results
In Section 4.1, we empirically verify the validity of the first-order Taylor approximation made
in (2) (Fig.1), for example by checking the correspondence between loss-gradient regularization
and adversarially-augmented training (Fig.2). Section 4.2 then empirically verifies that both the
average 'ι-norm of ∂xL and the adversarial vulnerability grow like √d as predicted by Corollary 6.
For all experiments, we approximate adversarial vulnerability using various attacks of the Foolbox-
package (Rauber et al., 2017). We use an '∞ attack-threshold of size e∞ = 0.005 (and later 0.002)
which, for pixel-values ranging from 0 to 1, is completely imperceptible but suffices to fool the
classifiers on a significant proportion of examples. This e∞-threshold should not be confused with
the regularization-strengths e appearing in (4) and (5), which will be varied in some experiments.
4.1 First-Order Approximation, Gradient Penalty, Adversarial Augmentation
2∞-attacks
々-attacks
deep-fool
iterative-2∞
iterative的
. .......* *-*-I"
*'W"k
50	100	150	200
(b)	Ex IldxLl∣ι
Figure 1: Adversarial vulnerability approximated by different attack-types for 10 trained networks as
a function of (a) the `1 gradient regularization-strength e used to train the nets and (b) the average
gradient-norm. These curves confirm that the first-order expansion term in (2) is a crucial component
of adversarial vulnerability.
5
Under review as a conference paper at ICLR 2019
OoOo
0 5 0 5
2 1- 1-
7芯=/
-3.5	-3.0	-2.5	-2.0
(a)	log10 (ed1∕p)
Ooo
3 2 1
三qEJUIΠA ijesjape
-3.5 -3.0 -2.5 -2.0
(b)	logi0(ed1/p)
...................-
5 0 5 0 5
8 8 7 7 6
-3.5	-3.0 -2.5 -2.0
(C) log10 (edιzp)
Ooo
3 2 1
≡qEJUInΛ ieijbsjdape
4 2
=76一 岗
5 。 5 U 5
8 8 7 7 6
En8B

Grad Regu q = 1
Grad Regu q = 2
Adv Train p = ∞
Adv Train P = 2
-Y-∙ PGD p = ∞
Cross-Lipschitz
50	100	150	200	50	100	150	200	10	20	30
(d) Ex IldxLI∣1	(e) Ex IldxLl/	(f) adversarial vulnerability
Figure 2: Average norm ExkdxLk of the loss-gradients, adversarial vulnerability and accuracy
(before attack) of various networks trained with different adversarial regularization methods and
regularization strengths . Each point represents a trained network, and each curve a training-method.
Upper row: A priori, the regularization-strengths have different meanings for each method. The near
superposition of all upper-row curves illustrates (i) the duality between adversarial augmentation and
gradient-regularization (Prop. 3) and (ii) confirms the rescaling of proposed in (3) and (iii) supports
the validity of the first-order Taylor expansion (2). (d): near functional relation between adversarial
vulnerability and average loss-gradient norms. (e): the near-perfect linear relation between the
Ek∂xLk1 and Ek∂x Lk2 suggests that protecting against a given attack-norm also protects against
others. (f): Merging 2band 2c shows that all adversarial augmentation and gradient-regularization
methods achieve similar accuracy-vulnerability trade-offs.
We train several CNNs with same architecture to classify CIFAR-10 images (Krizhevsky, 2009).
For each net, we use a specific training method with a specific regularization value . The training
methods used were 'ι- and '2-penalization of ∂xL (Eq. 4), adversarial augmentation with '∞- and
`2 - attacks (Eq. 5), projected gradient descent (PGD) with randomized starts (7 steps per attack with
step-size = .2 ∞; see Madry et al. 2018) and the cross-Lipschitz regularizer (Eq. 17 in Appendix C).
We then test the adversarial vulnerability of each trained network using the following attack-methods:
single-step '∞- (FGSM) and '2-attacks, iterative '∞- (PGD) and '2-attacks, and DeepFool attacks
(Moosavi-Dezfooli et al., 2016). All networks have 6 ‘strided convolution → batchnorm → ReLU’
layers with strides [1, 2, 2, 2, 2, 2] respectively and 64 output-channels each, followed by a final
fully-connected linear layer. Results are summarized in Figures 1 and 2. Figure 1 fixes the training
method - gradient '1-regularization - and plots the obtained adversarial vulnerabilities for various
attacks types. Figure 2 fixes the attack type - iterative '∞-attacks - but plots the curves obtained for
various training methods. Note that our goal here is not to advocate one defense over another, but
rather to check the validity of the Taylor expansion, and empirically verify that first order terms (i.e.,
gradients) suffice to explain much of the observed adversarial vulnerability. Similarly, our goal in
testing several attacks (Figure 1) is not to present a specifically strong one, but rather to verify that
for all attacks, the trends are the same: the vulnerability grows with increasing gradients.
Validity of first order expansion. The following observations support the validity of the first order
Taylor expansion in (2) and suggest that it is a crucial component of adversarial vulnerability: (i) the
efficiency of the first-order defense against iterative (non-first-order) attacks (Fig.1a); (ii) the striking
similarity between the PGD curves (adversarial augmentation with iterative attacks) and the other
adversarial training training curves (one-step attacks/defenses); (iii) the functional-like dependence
between any approximation of adversarial vulnerability andExk∂xLk1 (Fig.1b), and its independence
on the training method (Fig.2d). (iv) the excellent correspondence between the gradient-regularization
and adversarial training curves (see next paragraph). Said differently, adversarial examples seem
indeed to be primarily caused by large gradients of the classifier as captured via the induced loss. 2
2On Figure 1, the two '∞-attacks seem more efficient than the others, because we chose an '∞ perturbation
threshold (e∞). With an '2 -threshold it is the opposite (see Figure 7, Appendix F).
6
Under review as a conference paper at ICLR 2019
Illustration of Proposition 3. The upper row of Figure 2 plots Ex k∂xL1 k, adversarial vulnerability
and accuracy as a function of d1/p. The excellent match between the adversarial augmentation
curve with p = ∞ (p = 2) and its gradient-regularization dual counterpart with q = 1 (resp.
q = 2) illustrates the duality between as a threshold for adversarially-augmented training and as
a regularization constant in the regularized loss (Proposition 3). It also supports the validity of the
first-order Taylor expansion in (2).
Confirmation of (3). Still on the upper row, the curves for p = ∞, q = 1 have no reason to match
those for p = q = 2 when plotted against , because -threshold is relative to a specific attack-norm.
However, (3) suggested that the rescaled thresholds d1/p may approximately correspond to a same
‘threshold-unit’ across `p-norms and across dimension. This is well confirmed by the upper row plots:
by rescaling the x-axis, the p = q = 2 and q = 1, p = ∞ curves get almost super-imposed.
Accuracy-vs-Vulnerability Trade-Off. Merging Figures 2b and 2c by taking out , Figure 2f
shows that all gradient regularization and adversarial training methods yield equivalent accuracy-
vulnerability trade-offs. Incidentally, for higher penalization values, these trade-offs appear to be
much better than those given by cross Lipschitz regularization.
The penalty-norm does not matter. We were surprised to see that on Figures 2d and 2f, the L,q
curves are almost identical forq = 1 and 2. This indicates that both norms can be used interchangeably
in (4) (modulo proper rescaling of via (3)), and suggests that protecting against a specific attack-
norm also protects against others. (6) may provide an explanation: if the coordinates of ∂xL behave
like centered, uncorrelated variables with equal variance -which follows from assumptions (H) -, then
the 'ι- and '2-norms of ∂xL are simply proportional. Plotting ExkdxL(x)k2 against ExkdxL(x)∣k
in Figure 2e confirms this explanation. The slope is independent of the training method. Therefore,
penalizing kdxL(x)k1 during training will not only decrease ExkdxLk1 (as shown in Figure 2a),
but also drive down ExkdxLk2 and vice-versa.
4.2 Vulnerability Grows with Input Resolution
Theorems 4-5 and Corollary 6 predict a linear growth of the average `1 -norm of dx L with the square
root of the input dimension d, and therefore also of adversarial vulnerability (Lemma 2). To test these
predictions, we upsampled the CIFAR-10 images (of size 3 x 32 x 32) by copying pixels so as to get 4
datasets with, respectively, 32, 64, 128 and 256 pixels per edge. We then trained a CNN on each dataset
and computed their adversarial vul-
nerability (with iterative '∞-attacks,
threshold ∞ = .002) and average
kdxLk1 over the last 20 epochs on the
same held-out test-dataset. This gave
us 2 x 20-values per net and image-
size, summarized in Figure 3. The
dashed-lines follow their medians and
the errorbars show their 10th and 90th
quantiles. As predicted by our the-
orems, both kdx Lk1 and adversarial
vulnerability grow approximately lin-
early with √d. We also ran a similar
experiment on downsized ImageNet
images, where we train several iden-
tical nets per image-size rather than
just one. Conclusions are unchanged.
See Appendix E.
Figure 3: ExkdxLkI (right) increase linearly with the square-
root of the image-resolution d, as predicted by Corollary 6.
Adversarial vulnerability (left) increases similarly with d, but
gets slightly dampened with increasing dimension, probably
because the first-order approximation made in (2) becomes
less and less valid.
image-width b Vzd
All networks had exactly the same amount of parameters and very similar structure across the various
input-resolutions. The CNNs were a succession of 8 ‘convolution → batchnorm → ReLU’ layers
with 64 output channels, followed by a final full-connection to the 12 logit-outputs. We used 2 × 2-
max-poolings after the convolutions of layers 2,4, 6 and 8, and a final max-pooling after layer 8 that
fed only 1 neuron per channel to the fully-connected layer. To ensure that the convolution-kernels
cover similar ranges of the images across each of the 32, 64, 128 and 256 input-resolutions, we
respectively dilated all convolutions (七 trous') by a factor 1, 2, 4 and 8.
7
Under review as a conference paper at ICLR 2019
Figure 4: Evolution over training epochs of the average '1-gradient-norms on the training (left) and
test set (right). There is a clear discrepancy: on the training set, the gradient norms decrease (after an
initialization phase) and are dimension-independent; on the test set, they increase and scale like √d.
This suggests that, outside the training points, the nets tend to recover their prior gradient-properties
(i.e. naturally large gradients).
5	Discussions
5.1	Implications: why prior vulnerability may matter
Our theoretical results show that the priors of classical neural networks yield vulnerable functions
because of naturally high gradients. And our experiments (Fig 3&6) suggest that usual training does
not escape these prior properties. But how may these insights help understanding the vulnerability
of robustly trained networks? Clearly, to be successful, robust training algorithms must escape
ill-behaved priors, which explains why most methods (e.g. FGSM, PGD) are essentially gradient
penalization techniques. But, MNIST aside, even state-of-the-art methods largely fail at protecting
current network architectures (Madry et al., 2018), and understanding why is motivation to this and
many other papers. Interestingly, Schmidt et al. (2018) recently noticed that those methods actually
do protect the nets on training examples, but fail to generalize to the test set. They hence conclude
that state-of-the-art robustification algorithms work, but need more data. Alternatively however, when
generalization fails, one can also reduce the model’s complexity. Large fully connected nets for
example typically fail to generalize to out-of-sample examples: getting similar accuracies than CNNs
would need prohibitively many training points. Similarly, Schmidt et al.’s observations may suggest
that, outside the training points, networks tend to recover their prior properties, i.e. naturally large
gradients. Figure 4 corroborates this hypothesis. It plots the evolution over training epochs of the
`1 -gradient-norms of the CNNs from Section 4.2 (Fig 3) on the training and test sets respectively.
The discrepancy is unmistakable: after a brief initialization phase, the norms decrease on the training
set, but increase on the test set. They are moreover almost input-dimension independent on the
training set, but scale as √d on the test set (as seen in Fig 3) up to respectively 2, 4, 8 and 16 times
the training set values. These observations suggest that, with the current amount of data, tackling
adversarial vulnerability may require new architectures with inherently smaller gradients. Searching
these architectures among those with well-behaved prior-gradients seems a reasonable start, where
our theoretical results may prove very useful.3
5.2	Related Literature
On network vulnerability. Goodfellow et al. (2015) already stressed that adversarial vulnerability
increases with growing dimension d. But their argument only relied on a linear ‘one-output-to-many-
inputs’-model with dimension-independent weights. They therefore concluded on a linear growth of
adversarial vulnerability with d. In contrast, our theory applies to almost any standard feed-forward
architecture (notjust linear), and shows that, once We adjust for the weight,s dimension-dependence,
adversarial vulnerability increases like √d (not d), almost independently ofthe architecture. Never-
theless, our experiments confirm Goodfellow et al.’s idea that our networks are “too linear-like”, in
the sense that a first-order Taylor expansion is indeed sufficient to explain the adversarial vulnerability
of neural networks. As suggested by the one-output-to-many-inputs model, the culprit is that growing
3Appendix A investigates such a preliminary direction by introducing average poolings, which have a
weight-size 1/in-channels rather than the typical 1∕√in-channeis of the other He-initialized weights.
8
Under review as a conference paper at ICLR 2019
dimensionality gives the adversary more and more room to ‘wriggle around’ with the noise and adjust
to the gradient of the output neuron. This wriggling, we show, is still possible when the output is
connected to all inputs only indirectly, even when no neuron is directly connected to all inputs, like in
CNNs. This explanation of adversarial vulnerability is independent of the intrinsic dimensionality or
geometry of the data (compare to Amsaleg et al. 2017; Gilmer et al. 2018). Finally, let us mention
that Fawzi et al. (2016) show a close link between the vulnerability to small worst-case perturbation
(as studied here) and larger average perturbations. Our findings on the adversarial vulnerability NNs
to small perturbation could thus be translated accordingly.
On robustification algorithms. Incidentally, Goodfellow et al. (2015) also already relate adversar-
ial vulnerability to large gradients of the loss L, an insight at the very heart of their FGSM-algorithm.
They however do not propose any explicit penalizer on the gradient of L other than indirectly
through adversarially-augmented training. Conversely, Ross & Doshi-Velez (2018) propose the old
double-backpropagation to robustify networks but make no connection to FGSM and adversarial
augmentation. Lyu et al. (2015) discuss and use the connection between gradient-penalties and
adversarial augmentation, but never actually compare both in experiments. This comparison however
is essential to test the validity of the first-order Taylor expansion in (2), as confirmed by the simi-
larity between the gradient-regularization and adversarial-augmentation curves in Figure 2. Hein &
Andriushchenko (2017) derived yet another gradient-based penalty -the Cross-LiPschitz-penalty- by
considering (and proving) formal guarantees on adversarial vulnerability itself, rather than adversarial
damage. While both penalties are similar in spirit, focusing on the adversarial damage rather than
vulnerability has two main advantages. First, it achieves better accuracy-to-vulnerability ratios,
both in theory and practice, because it ignores class-switches between misclassified examples and
penalizes only those that reduce the accuracy. Second, it allows to deal with one number only, ∆L,
whereas Hein & Andriushchenko’s cross-Lipschitz regularizer and theoretical guarantees explicitly
involve all K logit-functions (and their gradients). See Appendix C. Penalizing network-gradients is
also at the heart of contractive auto-encoders as proposed by Rifai et al. (2011), where it is used to
regularize the encoder-features. Seeing adversarial training as a generalization method, let us also
mention Hochreiter & Schmidhuber (1995), who propose to enhance generalization by searching for
parameters in a “flat minimum region” of the loss. This leads to a penalty involving the gradient of
the loss, but taken with respect to the weights, rather than the inputs. In the same vein, a gradient-
regularization of the loss of generative models also appears in Proposition 6 of Ollivier (2014), where
it stems from a code-length bound on the data (minimum description length). More generally, the
gradient regularized objective (4) is essentially the first-order approximation of the robust training
objective maxkδk≤ L(x + δ, c) which has a long history in math (Wald, 1945), machine learning
(Xu et al., 2009) and now adversarial vulnerability (Sinha et al., 2018). Finally, Cisse et al. (2017)
propose new network-architectures that have small gradients by design, rather than by special training:
an approach that makes all the more sense, considering the conclusion of Theorems 4 and 5. For
further details and references on adversarial attacks and defenses, we refer to Yuan et al. (2017).
6	Conclusion
For differentiable classifiers and losses, we showed that adversarial vulnerability increases with the
gradients ∂xL of the loss, which is confirmed by the near-perfect functional relationship between
gradient norms and vulnerability (Figures 1&2d). We then evaluated the size of k∂xLkq and showed
that, at initialization, usual feed-forward nets (convolutional or fully connected) are increasingly
vulnerable to 'p-attacks with growing input dimension d (the image-size), almost independently of
their architecture. Our experiments show that, on the tested architectures, usual training escapes
those prior gradient (and vulnerability) properties on the training, but not on the test set. Schmidt
et al. (2018) suggest that alleviating this generalization gap requires more data. But a natural
(complementary) alternative would be to search for architectures with naturally smaller gradients, and
in particular, with well-behaved priors. Despite all their limitations (being only first-order, assuming
a prior weight-distribution and a differentiable loss and architecture), our theoretical insights may
thereby still prove to be precious future allies.
9
Under review as a conference paper at ICLR 2019
References
Laurent Amsaleg, James E. Bailey, DominiqUe Barbe, Sarah Erfani, Michael E Houle, Vinh Nguyen, and Milos
Radovanovic. The vulnerability of learning to adversarial perturbation increases with intrinsic dimensionality.
In IEEE Workshop on Information Forensics and Security, 2017.
David Balduzzi, Brian McWilliams, and Tony Butler-Yeoman. Neural taylor approximations: Convergence and
exploration in rectifier networks. In ICML, 2017.
Chris M. Bishop. Training with noise is equivalent to Tikhonov regularization. Neural computation, 7(1):
108-116,1995.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks:
Improving robustness to adversarial examples. In ICML, 2017.
Harris Drucker and Yann LeCun. Double backpropagation increasing generalization performance. In Interna-
tional Joint Conference on Neural Networks, 1991.
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: From
adversarial to random noise. In NIPS, 2016.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian
Goodfellow. Adversarial spheres. In ICLR Workshop, 2018.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In
ICLR, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification. In ICCV, 2015.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against
adversarial manipulation. In NIPS, 2017.
Sepp Hochreiter and Juergen Schmidhuber. Simplifying neural nets by discovering flat minima. In NIPS, 1995.
Jinggang Huang. Statistics of Natural Images and Models. PhD thesis, Brown University, Providence, RI, 2000.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Chunchuan Lyu, Kaizhu Huang, and Hai-Ning Liang. A unified gradient regularization family for adversarial
examples. In ICDM, 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. In ICLR, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: A simple and accurate
method to fool deep neural networks. In CVPR, 2016.
Radford M. Neal. Bayesian Learning for Neural Networks, volume 118 of Lecture Notes in Statistics. Springer,
1996.
Yann Ollivier. Auto-encoders: Reconstruction versus compression. arXiv:1403.7752, 2014.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox v0.8.0: A Python toolbox to benchmark the
robustness of machine learning models. arXiv:1707.04131, 2017.
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-encoders:
Explicit invariance during feature extraction. In ICML, 2011.
Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep
neural networks by regularizing their input gradients. In AAAI, 2018.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially
robust generalization requires more data. arXiv:1804.11285, 2018.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled
adversarial training. In ICLR, 2018.
Abraham Wald. Statistical decision functions which minimize the maximum risk. Annals of Mathematics, 46(2):
265-280, 1945.
10
Under review as a conference paper at ICLR 2019
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines.
JMLR, 10:1485-1510, 2009.
Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, and Xiaolin Li. Adversarial examples: Attacks and
defenses for deep learning. arXiv:1712.07107, 2017.
11
Under review as a conference paper at ICLR 2019
A Effects of Strided and Average-Pooling Layers on Adversarial
Vulnerability
It is common practice in CNNs to use average-pooling layers or strided convolutions to progressively
decrease the number of pixels per channel. Corollary 6 shows that using strided convolutions
does not protect against adversarial examples. However, what if we replace strided convolutions
by convolutions with stride 1 plus an average-pooling layer? Theorem 5 considers only randomly
initialized weights with typical size 1 / √in-degree. Average-poolings however introduce deterministic
weights of size 1/(in-degree). These are smaller and may therefore dampen the input-to-output
gradients and protect against adversarial examples. We confirm this in our next theorem, which uses
a slightly modified version (H0) of (H) to allow average pooling layers. (H0) is (H), but where the
He-init H3 applies to all weights except the (deterministic) average pooling weights, and where H1
places a ReLU on every non-input and non-average-pooling neuron.
Theorem 7 (Effect of Average-Poolings). Consider a succession of convolution layers, dense layers
and n average-pooling layers, in any order, that satisfies (H0) and outputs logits fk (x). Assume
the n average pooling layers have a stride equal to their mask size and perform averages over aι,
…，an nodes respectively. Then ∣∣∂xfk|卜 and ∣∂xfk| scale like 1/√αι ∙∙∙ an and 1/y/dai ∙∙∙ an
respectively.
Proof in Appendix B.4. Theorem 7 suggest to try and replace any strided convolution by its
non-strided counterpart, followed by an average-pooling layer. It also shows that if we system-
atically reduce the number of pixels per channel down to 1 by using only non-strided convolu-
tions and average-pooling layers (i.e. d = Qin=1 ai), then all input-to-output gradients should
become independent of d, thereby making the network completely robust to adversarial examples.
Our following experiments (Figure 5) show that
after training, the networks get indeed robusti-
fied to adversarial examples, but remain more
vulnerable than suggested by Theorem 7.
Experimental setup. Theorem 7 shows that,
contrary to strided layers, average-poolings
should decrease adversarial vulnerability. We
tested this hypothesis on CNNs trained on
CIFAR-10, with 6 blocks of ‘convolution →
BatchNorm →ReLU' with 64 output-channels,
followed by a final average pooling feeding one
neuron per channel to the last fully-connected
linear layer. Additionally, after every second
convolution, we placed a pooling layer with
stride and mask-size (2, 2) (thus acting on 2 × 2
neurons at a time, without overlap). We tested average-pooling, strided and max-pooling layers and
trained 20 networks per architecture. Results are shown in Figure 5. All accuracies are very close,
but, as predicted, the networks with average pooling layers are more robust to adversarial images
than the others. However, they remain more vulnerable than what would follow from Theorem 7. We
also noticed that, contrary to the strided architectures, their gradients after training are an order of
magnitude higher than at initialization and than predicted. This suggests that assumptions (H) get
more violated when using average-poolings instead of strided layers. Understanding why will need
further investigations.
88.5-
30-
B Proofs
^88.0-
3
O
E 875

87.0 average Strided_max
25-
20-

average Strided max
Figure 5: As predicted by Theorem 7, average-
pooling layers make networks more robust to ad-
versarial examples, contrary to strided (and max-
pooling) ones. But the vulnerability with average-
poolings remains higher than anticipated.
B.1 Proof of Proposition 3
Proof. Let δ be an adversarial perturbation with ∣δ ∣ = 1 that locally maximizes the loss increase
at point x, meaning that δ = arg maXkδok≤ι∂xL ∙ δ0. Then, by definition of the dual norm of ∂xL
We have: ∂xL ∙ (eδ) = E ∣∣∣∂xL∣∣∣. Thus
12
Under review as a conference paper at ICLR 2019
Le,k∙k(x, c) = 2(L(x, c) + L(X + e δ,c)) = 1(2L(x, c) + e ∣∂χL ∙ δ∣ + o(kδ∣∣))=
=L(X, c) + 2 llldχLl11 +。9 = LeJlHII(X, C) + o(e). □
B.2 Proof of Theorem 4
Proof. Let x designate a generic coordinate of X. To evaluate the size of k∂x Lkq , we will evaluate
the size of the coordinates ∂xL of ∂xL by decomposing them into
∂χL = X f f =： XX ∂kL ∂xfk,
k=1	k	k=1
wherefk (X) denotes the logit-probability of X belonging to class k. We now investigate the statistical
properties of the logit gradients ∂xfk, and then see how they shape ∂xL.
Step 1: Statistical properties of ∂xfk. Let P(x, k) be the set of paths p from input neuron x to
output-logit k. Let P - 1 and P be two successive neurons on path p, and P be the same path P but
without its input neuron. Let wp designate the weight from p - 1 to p and ωp be the path-product
ωp := Qp∈ρ wp. Finally, let σp (resp. σp) be equal to 1 if the ReLU of node P (resp. if path P) is
active for input X, and 0 otherwise.
As previously noticed by Balduzzi et al. (2017) using the chain rule, we see that ∂xfk is the sum of
all ωp whose path is active, i.e. ∂xfk (X) = Pp∈P(x,k) ωpσp. Consequently:
EW,σ ∂xfk(X)2 = X YEW wp2Eσ σp2
P∈P(χ,k) p∈P
21	1	1
=IP(X,k)1Ydp-ι2 = Ydp∙Ydp-ι = d.	⑻
p∈p P	p∈p	p∈p P
The first equality uses H1 to decouple the expectations over weights and ReLUs, and then applies
Lemma 10 of Appendix B.3, which uses H3-H5 to kill all cross-terms and take the expectation over
weights inside the product. The second equality uses H3 and the fact that the resulting product is the
same for all active paths. The third equality counts the number of paths from x to k and we conclude
by noting that all terms cancel out, except dp-1 from the input layer which is d. Equation 8 shows
that ∣∂χfk∣ H 1∕√d.
Step 2: Statistical properties of ∂kL and ∂xL. Defining qk(x) := Pjefkex)(X) (the probability
of image x belonging to class k according to the network), we have, by defin=ition of the cross-entropy
loss, L(X, c) := - log qc(X), where c is the label of the target class. Thus:
∂kL(X)=1--qk(qXc()X)	iofthke6=rwcise,	and
∂xL(X) = (1 - qc) ∂xfc(X) + X qk(-∂xfk(X)).	(9)
k6=c
Using again Lemma 10, we see that the ∂xfk (X) are K centered and uncorrelated variables. So
∂xL(X) is approximately the sum of K uncorrelated variables with zero-mean, and its total variance
is given by ((1 - qj2 + Pk=C q2)/d. Hence the magnitude of ∂xL(x) is 1∕√d for all x, so the
'q-norm of the full input gradient is d1/q-1/2. (6) concludes.	□
Remark 1. Equation 9 can be rewritten as
K
∂χL(x) = X qk(x) (∂χ∕c(x) - ∂χfk(x)) .	(10)
k=1
As the term k = c disappears, the norm of the gradients ∂xL(X) appears to be controlled by the total
error probability. This suggests that, even without regularization, trying to decrease the ordinary
13
Under review as a conference paper at ICLR 2019
classification error is still a valid strategy against adversarial examples. It reflects the fact that when
increasing the classification margin, larger gradients of the classifier’s logits are needed to push
images from one side of the classification boundary to the other. This is confirmed by Theorem 2.1 of
Hein & Andriushchenko (2017). See also (16) in Appendix C.
B.3 Proof of Theorem 5
The proof of Theorem 5 is very similar to the one of Theorem 4, but we will need to first generalize
the equalities appearing in (8). To do so, we identify the computational graph of a neural network to
an abstract Directed Acyclic Graph (DAG) which we use to prove the needed algebraic equalities. We
then concentrate on the statistical weight-interactions implied by assumption (H), and finally throw
these results together to prove the theorem. In all the proof, o will designate one of the output-logits
fk(x).
Lemma 8. Let x be the vector of inputs to a given DAG, o be any leaf-node of the DAG, x a generic
coordinate of x. Let P be a path from the set ofpaths P(x, o) from X to o, P the same path without
node x, P a generic node in P, and dp be its input-degree. Then:
X X Y >	(11)
x∈xp∈P(x,o) p∈p P
Proof. We will reason on a random walk starting at o and going up the DAG by choosing any
incoming node with equal probability. The DAG being finite, this walk will end up at an input-node
x with probability 1. Each path P is taken with probability Qp∈p 六.And the probability to end
up at an input-node is the sum of all these probabilities, i.e. Px∈x Pp∈P(x,o) p∈p dp-1, which
concludes.	□
The sum over all inputs x in (11) being 1, on average it is 1/d for each x, where d is the total number
of inputs (i.e. the length of x). It becomes an equality under assumption (S):
Lemma 9. Under the symmetry assumption (S), and with the previous notations, for any input
x ∈ x:
X Y dp = d
P∈P(χ,o) p∈p
(12)
Proof. Let us denote D(x, o) := {dp }x∈P(x,o). Each path P in P(x, o) corresponds to exactly one
element dp in D(x, o) and vice-versa. And the elements dp of dp completely determine the product
Qp∈ρ d-1. By using (11) and the fact that, by (S), the multiset D(χ, o) is independent of x, We
hence conclude
X X Y d- = X X Y d-
χ三xp∈P(x,o) p∈p P χ∈χ dp∈D(x,o) dp∈dp P
=d X Y d- = ι.	□
dp∈D(x,o) dp∈dp p
Now, let us relate these considerations on graphs to gradients and use assumptions (H). We remind
that path-product ωp is the product Qp∈p wp.
Lemma 10. Under assumptions (H), the path-products ωp, ωp0 of two distinct paths P and P0
starting from a same input node x, satisfy:
EW [ωp ωp0] = 0 and EW ωp2 =	EW wp2 .
P∈p
Furthermore, if there is at least one non-average-pooling weight on path P, then EW [ωp] = 0.
14
Under review as a conference paper at ICLR 2019
Proof. Hypothesis H4 yields
EW [ωp] = EW	ɪɪ Wp
p∈p
EW wp2	.
p∈p
Now, take two different paths p and p0 that start at a same node x. Starting from x, consider the first
node after which p and p0 part and call p and p0 the next nodes on p and p0 respectively. Then the
weights wp and wp0 are two weights of a same node. Applying H4 and H5 hence gives
EW [ωp ωp0 ] = EW [ωp∖p ωρ0∖p0] EW [wp wp0 ] = 0 .
Finally, if p has at least one non-average-pooling node p, then successively applying H4 and H3
yields: EW [ωp] = EW [ωp∖p] EW [wp] = 0.	□
We now have all elements to prove Theorem 5.
Proof. (of Theorem 5) For a given neuron P in p, let P - 1 designate the previous node in p of p.
Let σp (resp. σp ) be a variable equal to 0 if neuron p gets killed by its ReLU (resp. path p is inactive),
and 1 otherwise. Then:
∂xo =	∂p-1 P = X	ωp σp
p∈P(x,o) p∈p	p∈P(x,o)
Consequently:
EW,σ (∂xo) ] =	EW [ωp ωp0] Eσ [σp σp0]
p,p0 ∈P (x,o)
= X YEW ωp2] Eσ σp2]
p∈P(χ,o) p∈p
=X Y -21 = 1
乙 Udp 2 d,
p∈P (χ,o) p∈p
(13)
where the firs line uses the independence between the ReLU killings and the weights (H1), the second
uses Lemma 10 and the last uses Lemma 9. The gradient ∂χθ thus has coordinates whose squared
expectations scale like 1/d. Thus each coordinate scales like 1∕√d and Ildxokq like d1/2-1/q.
Conclude on k∂xLkq and p k∂xLkq by using Step 2 of the proof of Theorem 4.
Finally, note that, even without the symmetry assumption (S), using Lemma 8 shows that
EW k∂xok22 = XEW (∂xo)2]
x∈x
X X Y m
x∈xp∈P(x,o) p∈p P
1.
Thus, with or without (S), k∂xo∣2 is independent of the input-dimension d.	□
B.4 Proof of Theorem 7
To prove Theorem 7, we will actually prove the following more general theorem, which generalizes
Theorem 5. Theorem 7 is a straightforward corollary of it.
Theorem 11. Consider any feed-forward network with linear connections and ReLU activation
functions that outputs logits fk (x) and satisfies assumptions (H). Suppose that there is a fixed
multiset of integers {a1, . . . , an} such that each path from input to output traverses exactly n average
pooling nodes with degrees {a1, . . . , an}. Then:
k∂χfk∣2 Y τln 1 L .	(14)
i=1 ai
Furthermore, ifthe net satisfies the symmetry assumption (S), then: ∣∂xfk | H /」	=.
x	d in=1 ai
15
Under review as a conference paper at ICLR 2019
Two remarks. First, in all this proof, “weight” encompasses both the standard random weights,
and the constant (deterministic) weights equal to 1/(in-degree) of the average-poolings. Second,
assumption H5 implies that the average-pooling nodes have disjoint input nodes: otherwise, there
would be two non-zero deterministic weights w, w0 from a same neuron that would hence satisfy:
EW [w w0] 6= 0.
Proof. As previously, let o designate any fixed output-logit fk(x). For any path p, let a be the set of
average-pooling nodes of p and let q be the set of remaining nodes. Each path-product ωp satisfies:
ωp = ωqωa, where ωa is a same fixed constant. For two distinct paths p,p0, Lemma 10 therefore
yields: EW ωp2 = ωa2 EW ωq2 and EW [ωpωp0] = 0. Combining this with Lemma 9 and under
assumption (S), we get similarly to (13):
EW,σ (∂x o)	=	ωa ωa0 EW [ωq ωq0 ] Eσ [σq σq0 ]
p,p0 ∈P (x,o)
n
E ∏J ∏ Ew [ω2] Eσ[σ2]
p∈P(x,o) i=1 i
q∈q
n
m ai p∈P(x,o) π ai∏q dq 2
ai
ai
q∈q
(15)
n
1
|
same value
for all p
I
1 n I
1 ∏上
d ai
{z
Πp∈25 dP
{^^^∖^^^^^^^
d (Lemma 9)
}
}
Therefore, ∣∂xo∣ = ∣∂xfk∣ 8 1A∕d∏n=1ɑi∙ Again, note that, even without assumption (S), using
(15) and Lemma 8 shows that
EW hk∂xok22i = X EW,σ [(∂xo)2]
x∈x
(=)Xna x ∏ɪ∏ɪl
x∈x i=1 i p∈P(x,o) i=1 i p∈p P
n1	1 n1
=∏ B X X ∏ ；=∏ ；
aa
i=1	x∈Xp∈P(x,o) p∈p P i=1
∣------------}
=1 (Lemma 8)
which proves (14).
□
C Comparison to the Cross-Lipschitz Regularizer
In their Theorem 2.1, Hein & Andriushchenko (2017) show that the minimal = kδkp perturbation
to fool the classifier must be bigger than:
fc(x) - fk (x)
min.
k6=c maxy∈B(x,) k∂xfc(y) - ∂xfk(y)kq
(16)
They argue that the training procedure typically already tries to maximize fc (x) - fk (x), thus one
only needs to additionally ensure that k∂xfc(x) - ∂xfk(x)kq is small. They then introduce what
they call a Cross-Lipschitz Regularization, which corresponds to the case p = 2 and involves the
gradient differences between all classes:
1K
RxLiP := K E Ildxfh(X)- dχfk(X)II2
(17)
k,h=1
In contrast, using (10), (the square of) our proposed regularizer k∂xLkq from (4) can be rewritten,
for p = q = 2 as:
16
Under review as a conference paper at ICLR 2019
K
Rk∙k2(f) = X qk(x)qh(x) (∂χfc(χ) - ∂χfk(x))∙
k,h=1
• (∂χfc(x)- ∂χfh(x))	(18)
Although both (17) and (18) consist in K2 terms, corresponding to the K2 cross-interaction between
the K classes, the big difference is that while in (17) all classes play exactly the same role, in (18) the
summands all refer to the target class c in at least two different ways. First, all gradient differences
are always taken with respect to ∂xfc . Second, each summand is weighted by the probabilities qk (x)
and qh(x) of the two involved classes, meaning that only the classes with a non-negligible probability
get their gradient regularized. This reflects the idea that only points near the margin need a gradient
regularization, which incidentally will make the margin sharper.
D	Perception Threshold
To keep the average pixel-wise variation constant across dimensions d, we saw in (3) that the
threshold Ep of an 'p-attack should scale like d1/p. We will now see another justification for this
scaling. Contrary to the rest of this work, where we use a fixed p for all images x, here we will let
Ep depend on the '2-norm of x. If, as usual, the dataset is normalized such that the pixels have on
average variance 1, both approaches are almost equivalent.
Suppose that given an 'p-attack norm, we want to choose Ep such that the signal-to-noise ratio (SNR)
kxk2 / kδk2 of a perturbation δ with `p-norm ≤ Ep is never greater than a given SNR threshold 1/E.
Forp = 2 this imposes E2 = E kxk2. More generally, studying the inclusion of 'p-balls in '2-balls
yields
Ep = E kxk2 d1/p-1/2.	(19)
Note that this gives again Ep = E∞d1∕p. This explains how to adjust the threshold E with varying
'p -attack norm.
Now, let us see how to adjust the threshold of a given 'p-norm when the dimension d varies. Suppose
that x is a natural image and that decreasing its dimension means either decreasing its resolution or
cropping it. Because the statistics of natural images are approximately resolution and scale invariant
(Huang, 2000), in either case the average squared value of the image pixels remains unchanged,
which implies that kx∣∣2 scales like √d. Pasting this back into (19), we again get:
E = E d1/p
Ep = E∞	.
In particular, e∞ a E is a dimension-free number, exactly like in (3) of the main part.
Now, why did we choose the SNR as our invariant reference quantity and not anything else? One
reason is that it corresponds to a physical power ratio between the image and the perturbation, which
we think the human eye is sensible to. Of course, the eye’s sensitivity also depends on the spectral
frequency of the signals involved, but we are only interested in orders of magnitude here.
Another point: any image x yields an adversarial perturbation δx, where by constraint kxk2 / kδxk ≤
1/E. For'2-attacks, this inequality is actually an equality. But what about other'p-attacks: (on average
over x,) how far is the signal-to-noise ratio from its imposed upper bound 1/E? For p 6∈ {1, 2, ∞},
the answer unfortunately depends on the pixel-statistics of the images. But when p is 1 or ∞, then
the situation is locally the same as for p = 2. Specifically:
Lemma 12. Let x be a given input and E > 0. Let Ep be the greatest threshold such that for any δ
with kδkp ≤ Ep, the SNR kxk2 / kδk2 is ≤ 1/E. Then Ep = E kxk2 d1/p-1/2.
Moreover, forp ∈ {1, 2, ∞}, if δx is the Ep-sized 'p-attack that locally maximizes the loss-increase
i.e. δχ = argmaxkδk ≤ep ∣∂xL • δ∣, then:
SNR(X) = 7k⅛ = 1 and Ex [SNR(x)] = 1 .
kδxk2	E	E
Proof. The first paragraph follows from the fact that the greatest 'p-ball included in an '2-ball of
radius E kxk2 has radius E kxk2 d1/p-1/2 .
17
Under review as a conference paper at ICLR 2019
The second paragraph is clear for P = 2. For P = ∞, it follows from the fact that δχ = e∞ sign ∂xL
which satisfies: ∣∣δχ∣∣2 = e∞√d = e ∣∣x∣∣2. For P = 1, it is because δχ = eι maxi=ι..d ∣(∂xL)/,
WhiChsatisfies: ∣∣δχk2 = e2∕√d = e ∣∣x∣∣2.	□
Intuitively, this means that for P ∈ {1, 2, ∞}, the SNR of ep-sized 'p-attacks on any input X will be
exactly equal to its fixed upper limit 1/e. And in particular, the mean SNR over samples x is the
same (1/e) in all three cases.
E Vulnerability-Dimension Dependence using Downsized
ImageNet Images
We also ran a similar experiment as in Section 4.2, but instead of using upsampled CIFAR-
10 images, we created a 12-class dataset of approximately 80, 000 3 × 256 × 256-sized RGB-
images by merging similar ImageNet-classes, resizing the smallest image-edge to 256 pixels and
center-cropping the result. We then
downsized the images to 32, 64, 128
and 256 pixels per edge, and trained,
not 1, but 10 CNNs per image-size.
We then computed their adversarial
vulnerability and average k∂xLk1.
This gave us 2 values per trained
net, i.e. 2 x 10 values per image-size,
which are shown in 6. The lines fol-
low their medians, the errorbars show
their 10th and 90th quantiles. The con-
clusions are identical to Section 4.2:
after usual training, the vulnerability
and gradient-norms still increase like
32 64	128	256	32 64	128	256
image-width a √d	image-width a √d
Figure 6: Similar experiment than Fig 3, but with downsam-
pled ImageNet images, rather than upsampled CIFAR-10
images. Same conclusion: after usual training, vulnerability
and average ∣∣∂xLkι grow like √d.
√d. Note that, as the gradients get much larger at higher dimensions, the first order approximation in
(2) becomes less and less valid, which explains the little inflection of the adversarial vulnerability
curve. For smaller e-thresholds, we verified that the inflection disappears.
F Figures with an `2 Perturbation-Threshold and Deep-Fool
Attacks
Here we plot the same curves as in the main part, but using an '2-attack threshold of size e2 =
0.005√d instead of the '∞-threshold and deep-fool attacks (Moosavi-Dezfooli et al., 2016) instead
of iterative '∞-ones in Figs. 8 and 9. Note that contrary to '∞-thresholds, '2-thresholds must be
rescaled by √d to stay consistent across dimensions (see Eq.3 and Appendix D). All curves look
essentially the same as their counterparts in the main text.
♦…∙ 2∞-attacks
※…々-attacks
d- deep-fool
iter iterative-2∞
**-- iterative的
,∙∙*
1* ...∙-∙ τ
*-
50	100	150	200
(b)	Ex IldxLl∣ι


Figure 7:	Same as Figure 1 but using an '2 threshold instead of a '∞ one. Now the '2-based methods
(deep-fool, and single-step and iterative '2-attacks) seem more effective than the '∞ ones.
18
Under review as a conference paper at ICLR 2019
OoOo
0 5 0 5
2 1- 1-
7芯=/
-3.5	-3.0	-2.5	-2.0
(a)	log10 (ed1∕p)
Ooo
3 2 1
三qEJUIΠA ijesjape
-3.5 -3.0 -2.5 -2.0
(b)	logi0(ed1/p)
...................-
5 0 5 0 5
8 8 7 7 6
-3.5	-3.0 -2.5 -2.0
(C) log10 (edιzp)
I蝮蝌皿种*处*
Ooo
3 2 1
≡qEJUInΛ ieijbsjdape
50	100	150
(d)	Ex IldxLi∣ι
200
4 2
=76一 岗
50	100	150
(e)	Ex IldxLl∣1
200
5 。 5 U 5
8 8 7 7 6
En8B
Grad Regu q = 1
Grad Regu q = 2
Adv Train p = ∞
Adv Train P = 2
-Y-∙ PGD p = ∞
Cross-Lipschitz
10	20	30
(f) adversarial vulnerability
Figure 8:	Same as Figure 2, but with an '2- perturbation-threshold (instead of '∞) and deep-fool
attacks (Moosavi-Dezfooli et al., 2016) instead of iterative '∞ ones. All curves look essentially the
same than in Fig. 2.
-IUqEJə-nA
image-width α VZd
100-
80-
60-
40-
32 64	128	256
image-width α VZd
Figure 9:	Same as Figure 6 but with an '2 perturbation-threshold (instead of an '∞ one) and using
deep-fool (instead of iterative-'∞) attacks to approximate adversarial vulnerability.
G A Variant of Adversarially-Augmented Training
In usual adversarially-augmented training, the adversarial image x + δ is generated on the fly, but
is nevertheless treated as a fixed input of the neural net, which means that the gradient does not
get backpropagated through δ. This need not be. As δ is itself a function of x, the gradients could
actually also be backpropagated through δ. As it was only a one-line change of our code, we used this
opportunity to test this variant of adversarial training (FGSM-variant in Figure 2) and thank Martin
Arjovsky for suggesting it. But except for an increased computation time, we found no significant
difference compared to usual augmented training.
19