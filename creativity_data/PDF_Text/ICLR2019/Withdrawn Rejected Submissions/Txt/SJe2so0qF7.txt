Under review as a conference paper at ICLR 2019
Learning data-derived privacy preserving rep-
RESENTATIONS FROM INFORMATION METRICS
Anonymous authors
Paper under double-blind review
Ab stract
It is clear that users should own and control their data and privacy. Utility
providers are also becoming more interested in guaranteeing data privacy. There-
fore, users and providers can and should collaborate in privacy protecting chal-
lenges, and this paper addresses this new paradigm. We propose a framework
where the user controls what characteristics of the data they want to share (utility)
and what they want to keep private (secret), without necessarily asking the util-
ity provider to change its existing machine learning algorithms. We first analyze
the space of privacy-preserving representations and derive natural information-
theoretic bounds on the utility-privacy trade-off when disclosing a sanitized ver-
sion of the data X. We present explicit learning architectures to learn privacy-
preserving representations that approach this bound in a data-driven fashion. We
describe important use-case scenarios where the utility providers are willing to
collaborate with the sanitization process. We study space-preserving transforma-
tions where the utility provider can use the same algorithm on original and san-
itized data, a critical and novel attribute to help service providers accommodate
varying privacy requirements with a single set of utility algorithms. We illustrate
this framework through the implementation of three use cases; subject-within-
subject, where we tackle the problem of having a face identity detector that works
only on a consenting subset of users, an important application, for example, for
mobile devices activated by face recognition; gender-and-subject, where we pre-
serve facial verification while hiding the gender attribute for users who choose to
do so; and emotion-and-gender, where we hide independent variables, as is the
case of hiding gender while preserving emotion detection.
1	Introduction, Challenges, and Contributions
Individuals are sharing vast amounts of data on a daily basis; at the same time, advances in machine
learning mean that service providers that receive this user data can infer increasingly more sensitive
attributes about their users. Care must be taken to provide an appropriate protection for users in order
to adhere to various privacy, legal, and ethical constraints. Critical to this is the capability of finding
effective privacy-preserving data representations in a data-driven manner that are able to accommo-
date each user’s individual needs. It is also in the interest of the utility provider to collaborate in this
endeavor, the privacy-preserving data representation should account for data processing pipelines
already in place. It would be impractical to tackle many of the privacy-preserving challenges today
using model-based approaches, a data-driven approach is essential in allowing privatization mecha-
nisms to keep up with new applications.
Each user should have the ability to define, sensitive and non-sensitive information associated with
their data; we propose that a user and the service provider collaborate towards achieving user-
specific privacy. This results in a system where the user sanitizes the data, at the sensing and/or
transmitting stage without affecting the utility they need; at the same time, the provider can use the
same processing pipeline for both sanitized and non-sanitized data. This is essential to ensure the
service provider can accommodate several privacy requests with a single set of utility algorithms.
An example addressed by the proposed framework is to block, even at the sensor level, a device
from constantly “listening.” For example, mobile devices scan images until they detect the owner,
and then they open. But all images not from the owner should be private. The proposed framework
1
Under review as a conference paper at ICLR 2019
will make such devices not to understand the data until the visual or sound trigger is detected, with
the capability to do this at the sensor level and without modifying the existing recognition system.
This new paradigm of collaborative privacy environment is critical since it has also been shown
that algorithmic or data augmentation and unpredictable correlations can break privacy Israel et al.
(2014); Narayanan & Shmatikov (2008); Oh et al. (2016); Reuben et al. (2016). The impossibility of
universal privacy protection has been studied extensively in the domain of differential privacy Dwork
(2008), where a number of authors have shown that assumptions about the data or the adversary must
be made in order to be able to provide utility Dwork & Naor (2010); Hardt et al. (2016); Kifer &
Machanavajjhala (2011; 2014). We can, however, minimize the amount of privacy we are willing
to sacrifice for a given level of utility. Other recent data-driven privacy approaches like Wu et al.
(2018) have also explored this notion, but do not integrate the additional collaborative constraints.
Therefore, it is important to design collaborative systems where each user shares a sanitized version
of their data with the service provider in such a way that user-defined non-sensitive tasks can be
performed but user-defined sensitive ones cannot, without the service provider requiring to change
any data processing pipeline otherwise.
Contributions- We consider a scenario where a user wants to share a sanitized representation of data
X in a way that a latent variable U can be inferred, but a sensitive latent variable S remains hidden.
We formalize this notion using privacy and transparency definitions. We derive an information-
theoretic bound on privacy-preserving representations. The metrics induced by this bound are used
to learn such a representation directly from data, without prior knowledge of the joint distribution
of the observed data X and the latent variables U and S. This process can accommodate for several
user-specific privacy requirements, and can be modified to incorporate constraints about the service
provider’s existing utility inference algorithms enabling several privacy constraints to be satisfied in
parallel for a given utility task.
We apply this framework to challenging use cases such as hiding gender information from a facial
image (a relatively easy task) while preserving subject verification (a much harder task), or designing
a sanitization function that preserves subject identification on a consenting subset of users, while
disallowing it on the general population. Blocking a simpler task while preserving a harder one
and blocking a device from constantly listening out-of-sample data are new applications in this
work, here addressed with theoretical foundations and respecting the provider’s existing algorithms,
which can simultaneously handle sanitized and non-sanitized data.
The problem statement is detailed in Section 2, and the information-theoretic bounds are derived
in Section 3. Section 4 defines a trainable adversarial game that directly attempts to achieve this
bound; the section also discusses how service-provider specific requirements can be incorporated.
Examples of this framework are shown in Section 5. The paper is concluded in Section 6. Comple-
mentary information and proofs are presented in the Supplementary Material .
2	Problem Statement
We describe a scenario in which we have access to possibly high-dimensional data X ∈ X, this
data depends on two special latent variables U and S. U is called the utility latent variable, and is a
variable we want to communicate, while S is called the secret, and is a variable we want to protect.
We consider two agents, a service provider that wants to estimate U from X , and an actor that wants
to infer S from X .
We define a third agent, the privatizer, that wants to learn a space-preserving stochastic mapping
Q : X → Q ⊃ X in such a way that Q(X) provides information about the latent variable U, but
provides relatively little information of S . In other words, we want to find a data representation
that is private with respect to S and transparent with respect to U.1 We first recall the definition of
privacy presented in Kifer (2009):
Definition 2.1. Privacy: Let δs be a measure of distance between probability distributions, bs ∈ R+
a positive real number, and P(S) the marginal distribution of the sensitive attribute S. The stochastic
mapping Q(X) is (δs, bs)-private with respect to S if δs(P(S), P (S |Q(X))) < bs.
1 Note that stochastic mappings are a natural extension of deterministic sanitization mappings. Furthermore,
a deterministic mapping only truly destroys information if it is non-invertible.
2
Under review as a conference paper at ICLR 2019
We can define transparency in the same fashion:
Definition 2.2. Transparency: Let δu be a measure of distance between probability distributions,
bu ∈ R+ a positive real number, and P(U|X) the posterior conditional distribution of the utility
variable U after observing X. The stochastic mapping Q(X) is (δu, bu)-transparent with respect to
U if δu(P(U|X),P(U∣Q(X))) < bu.
Both definitions depend on the learned mapping Q; in the following section, we derive an
information-theoretic bound between privacy and transparency, and show that this bound infers a
particular choice of metrics δu , δs . We then show that this inferred metric can be directly imple-
mented as a loss function to learn privatization transformations from data using standard machine
learning tools.
A similar analysis of these bounds for the special case where we directly observe the utility variable
U (X = U) was analyzed in Calmon et al. (2015) in the context of the Privacy Funnel. Here, we
extend this to the more general case where U is observed indirectly. More importantly, these bounds
are used to design a data-driven implementation for learning privacy-preserving mappings.
3	Information-theoretic bounds on privacy
Consider the utility and secret variables U and S defined over discrete alphabets U , S ,and the ob-
served data variable X, defined over X, with joint distribution PX,U,S. Figure 1a illustrates this
set-up, and shows the fundamental relationship of their entropies H(∙) and mutual information.
O /(U; XiS)
Θ /(s； Xv)
❸ /(U; SlX)
Ol(U-X)-I(U-X∖S}
/(U;S) —/(U; SIX)
/(S;X) —/(S; XlS
(a)

Figure 1: Left of figure (a) shows the dependency graph of the observed variable X and the latent utility and
secret variables U and S . Right of figure (a) shows a Venn diagram illustrating conditional mutual informations
I that provide constraints on the performance of any sanitization mapping Q(X). Left of figure (b) extends
the dependency graph to show the sanitzed data Q(X) in red. Right of figure (b) shows that the information
leakage I(S; Q(X)) and censured information I(U; X | Q(X)) shown in red and blue respectively cannot be
simultaneously set to 0, since they are partially at odds.
We analyze the properties of any mapping Q : X → Q, and measure the resulting mutual informa-
tion between the transformed variable Q(X) and our quantities of interest. Our goal is to find Q such
that the information leakage from our sanitized data I(S; Q(X)) is minimized, while maximizing
the shared information of the utility variable I (U; Q(X)). We will later relate these quantities and
the bounds here developed with the privacy/utility definitions presented in the previous section.
Maximizing I(U; Q(X)) is equivalent to minimizing I(U; X | Q(X)), since I(U; X |Q(X)) =
I(U; X) - I(U; Q(X)). The quantity I (U; X | Q(X)) is the information X contains about U that
is censured by the sanitization mapping Q.
Figure 1b illustrates I(S; Q(X)) and I(U; X |Q(X)). One can see that there exists a trade-off area,
I(U,S) - I(U,S|X),thatisalwaysincludedintheunionofI(S;Q(X)) andI(U;X|Q(X)). The
lower we make I(S; Q(X)), the higher we make the censored information I(U; X |Q(X)), and vice
versa. This induces a lower bound over the performance of the best possible mappings Q(X) that is
formalized in the following lemma.
Lemma 3.1. Let X, U, S be three discrete random variables with joint probability distribution
PX,U,S. For any stochastic mapping Q : X → Q we have
I (U; S)-1 (U; S |X) ≤ [I (S; Q(X)] + [I (U; X |Q(X))].
(1)
3
Under review as a conference paper at ICLR 2019
Proof of this lemma is shown in Supplementary Material. To show that this bound is reachable
in some instances, consider the following example. Let U and S be independent discrete random
variables, and X = (U, S). The sanitization mapping Q(X) = U satisfies this bound with equality.
We can also prove, trivially, an upper bound for these quantities.
Lemma 3.2. Let X, U, S be three discrete random variables with joint probability distribution
PX,U,S . For any stochastic mapping Q : X → Q we have:
I (S; Q(X)) +1 (U; X∣Q(X)) ≤ I (X; U S).
(2)
That simply states that the information leakage about the secret and the censured information on the
utility variable cannot exceed the total information present in the original observed variable X.
p(u | x)	(3)
P(U | q) ,
3.1	Relation With Privacy And Transparency Metrics
We relate the terms I(S; Q(X)) and I(U; X | Q(X)) in Eq.1 back to our definitions of privacy and
transparency.
I(U;X | Q) = Xp(q,x) XP(U | χ,q)log p(；U χ, ；) ,
q,x	u
=	p(q, x)	p(u | x)log
q,x	u
= EX,Q DKL(pU|X || pU|Q) .
Here we used the fact that U is conditionally independent of Q given X. We then observe that Eq.
3	induces the Kullback-Leibler divergence DKL as a natural transparency metric δu in Def.2.2.
Similarly, we can analyze I(S; Q) to get,
I(S; Q) = Xp(q) Xp(s | q)log
= EX,Q RDKL(pS || pS|Q) .
We can see from Eq.4 that the natural induced metric for measuring privacy δs in Def,2.1 is the
reverse Kullback-Leibler divergence RDKL .
We can thus rewrite our fundamental tradeoff equation as
P(S | q)
p(s)
(4)
EX,Q [RDKL (PS || PS|Q) + DKL(PU|X || PU|Q)] ≥ I(U, S) - I(U, S | X).	(5)
We show next how this bound can be used to define a trainable loss metric, allowing the privatizer
to select different points in the transparency-privacy trade-off space.
3.2 Defining a trainable loss metric
Assume that for any given stochastic transformation mapping Q 〜 Q(X), We have access to the
posterior conditional probability distributions P (S | Q), P (U | Q) , and P (U | X). Assume we
also have access to the prior distribution ofP(S). Inspired by the bounds from the previous section,
the proposed privatizer loss is
minQEχ,Q[(I- α)DKL(Pu|X || Pu|Q)2 + αRDKL(ps || PS∣Q)2],	⑹
4
Under review as a conference paper at ICLR 2019
where α ∈ [0, 1] is a tradeoff constant. A low α value implies a high degree of transparency, while a
high value of α implies a high degree of privacy. Using Eq.5 we have a lower bound on how private
or transparent the privatizer can be for any given α value, as detailed next.
Theorem 3.3. For any α ∈ [0, 1], and stochastic mapping Q : X → Q the solution to Eq.6
guarantees the following bounds,
EX,Q[DKL(pU|X || pU|Q)] ≥ α[I(U, S) - I(U, S | X)],
EX,Q[RDKL(pS ||pS|Q)] ≥ (1-α)[I(U,S)-I(U,S |X)].
(7)
The proof is shown in Supplementary Material. To recap, we proposed a privatizer loss Eq.6 with a
controllable trade-off parameter α, and showed bounds on how transparent and private our data can
be for any given value of α. Next we show how to optimize this utility-privacy formulation.
4	A Data-Driven implementation
Even if the joint distribution of P (U, S, X) is not known, the privatizer can attempt to directly
implement Eq.6 in a data-driven architecture to find the optimal Q. Assume the privatizer has access
to a dataset {(x, s, u)}, where s and u are the ground truth secret and utility values of observation
x. Under these conditions, the privatizer searches for a parametric mapping q = Qθ (x, z), where
z is an independent random variable, and attempts to predict the best possible attack by learning
Pn(S | q), an estimator of P(S | q). The Privatizer also needs Pψ (u|q) and Pφ(u∣χ), estimators of
P (u | q) and P (u | x) respectively, to measure how much information about the utility variable
is censored with the ProPosed maPPing. Under this setuP Qθ (x, z) is obtained by oPtimizing the
following adversarial game:
η = argminnEχ,s,z - - Iog(Pn(s∣Qθ(x, z))],
ψ = argminψEχ,u,z [ - log(Pψ(u∣Qθ(x, z))],
φ = argminφEχ,u [ - log(Pφ(u∣x)],
θ = argminθ(I- α)EX,U,Z [DKL(pφ(u | x) || Pψ(U | Q^(X,z)))] +
(8)
+ αEX,s,z [R^Dkl(P(s) || Pn(S I Qθ(x,z)))]∙
Here the first three terms are crossentropy loss terms to ensure our estimators Pn(s|q), Pψ (u∣q), and
Pφ(u∣χ) are a good approximation to the true posterior distributions. The final loss term attempts to
find the best possible sampling function Qθ(x, z) such that (1 - α)I2(U; X | Q) + αI2(S; Q) is
minimized. Details on the algorithmic implementation are given in Section 7.3.1. Performance on
simulated datasets is shown in Section 7.2.
4.1	Privacy Under Fixed Utility Inference
The proposed framework naturally provides a means to achieve collaboration from the utility
provider. In this scenario, the utility provider wishes to respect the user’s desired privacy, but is
unwilling to change their estimation algorithm P^(u ∣ x), and expects the privatizer to find a map-
ping that minimally affects its current performance.2 This is a more challenging scenario, with worse
tradeoff characteristics, in which Qθ(x, z) is obtained by optimizing
η = argminnEχ,s,z - - Iog(Pn(s∣Qθ(x, z))],
θ = argminθ(I- α)EX,u,z [dkl(Pφ(U | X) || Pφ(U | Qθ(X,z)))]+
(9)
+ αEX,s,z [R^Dkl(P(s) II Pn(S I Qθ(x,z)))]∙
2Recall that the utility provider wants to use the same algorithm for sanitized and non-sanitized data, a
unique aspect of the proposed framework and critical to accept its collaboration.
5
Under review as a conference paper at ICLR 2019
4.2	Privacy Under Fixed Utility and Secret Inference
A final scenario addressed by the proposed framework arises when the utility provider is the sole
agent to access the sanitized data, and it has estimation algorithms for both the utility and the privacy
variable P^(u | x), Pn(S | x), that it is unwilling to modify. The service provider wishes to reassure
the users that they are unable to infer the secret attribute from the sanitized data, if and when the
user decides so. Under these conditions, we optimize for
。=argminθ (I- α)EX ,U,Z [DKL(Pφ(u | X) || P^ (U | Qθ(X,z)))] +
+ αEX,s,z [R^Dkl(P(s) || P^^(s | Qj(x,z)))].
(10)
5	Experiments and Results
The following examples are based on the framework presented in Figure 2. Here we have the three
key agents mentioned before: (1) the utility algorithm that is used by the provider to estimate the
information of interest. This algorithm can take the raw data (X) or the mapped data (Q(X)) and
be able to infer the utility; (2) the secret algorithm that is able to operate on the mapped data to
infer the secret; (3) the privatizer that learns a space preserving mapping Q that allows the provider
to learn the utility but prevents the secret algorithm to infer the secret. The utility algorithm is
trained to perform well on raw data, the secret algorithm is adversarially trained to infer the secret
variable after sanitization. In the next examples we show how the proposed framework performs
under different scenarios, the privatizer architecture is kept unchanged across all experiments to
show that the same architecture can achieve very different objectives using the proposed framework,
the detailed architectures are shown in Section 7.3.2. Extra experiments under known conditions are
shown in 7.2.
Figure 2: Three components of the
collaborative privacy framework. Raw
data can be directly fed into the secret
and utility inferring algorithm. Since
the privatization mapping is space pre-
serving, the privatized data can also be
directly fed to both tasks without any
need for further adaptations.
5.1	Subject Within Subject
We begin by analyzing the subject-within-subject problem. Imagine a subset of users wish to unlock
their phone using facial identification, while others opt out of the feature; we wish the face iden-
tification service to work only on the consenting subset of users. We additionally assume that the
utility provider wishes to comply with the user’s wishes, so we can apply the framework described
in Section 4.2. Note that in this problem, the utility and secrecy variables are mutually exclusive.
We solve this problem by training a space-preserving stochastic mapping Q on facial image data
X , where the utility and secret variable U and S are categorical variables over consenting and non-
consenting users respectively. We test this over the FaceScrub dataset Kemelmacher-Shlizerman
et al. (2016), using VGGFace2 Cao et al. (2017) as the utility and secrecy inferring algorithm. The
stochastic mapping was implemented using a stochastic adaptation of the UNET Ronneberger et al.
(2015), architecture details are provided in Section 7.3.2.
Table 1 shows the top-5 categorical accuracy of the utility network over the sanitized data at various
α points in the privacy-utility trade-off. Figure 3 show some representantive images on how images
are sanitized. It also shows that the sanitization function is able to preserve information about the
utility variable while effectively censoring the secret variable, even for unobserved images. A phone
equipped with this filter at the sensor level would be effectively incapable of collecting information
on nonconsenting users.
6
Under review as a conference paper at ICLR 2019
(a) Filtered images of consenting
users (CU)
D□0□
(b) Filtered images of private
users (PU)
Top-5 Accuracy
α	CU	OPU	UPU
0	99.8%	99.8%	99.8%
0.3	94.7%	86.1%	87.5%
0.35	93.5%	15.3%	33.6%
0.40	93.0%	14.9%	31.8%
0.45	89.4%	12.6%	30.2%
0.50	70.7%	10.9%	27.0%
Table 1: Subject detection on
users
Figure 3: Left and center figures show images of consenting and nonconsenting (private) users respectively,
along with their sanitized counterparts. The identity of consenting users is still easily verified, while the identity
of nonconsenting users is effectively censored. Table on the right shows Top-5 accuracy performance of the
subject detector after sanitization across several sanitation levels α. Performance is shown across 3 subsets,
consenting users (CU) are users that decided to be detected by the utility algorithm, observed private users
(OPU) are those that explicitly decided to protect their privacy, while unobserved private users (UPU) are
users that decided to protect their privacy but where not available during training. Consenting users are still
recognized by the system, while nonconsenting users are not. For example, for α = 0.4, we significantly block
OPU and UPU while preserving CU
5.2	Obfuscating Emotion While Preserving Gender
Here we continue to work on facial image data X , where utility variable U is gender recognition,
and the secret variable S is emotion (smiling/non-smiling). In this scenario, variables U and S are
independent. We implement this over the CelebA dataset Liu et al. (2015), using Xception networks
Chollet (2017) as our utility and privacy estimators. Table.2 shows the distribution of the utility and
secrecy estimators over the sanitized data. Figure 4 shows example sanitized images. It is visually
possible to identify the gender of the subject but not their emotion. Most importantly, the existing
gender detection algorithm still performs correctly over the sanitized images.
Figure 4: Images before and after sanitization for Gen-
der (utility) vs Emotion (privacy).
Inference	Data	Male	Female
Gender	Raw	94.2%	94.5%
	Sanit.	91.2%	96.1%
Emotion	Raw Sanit.	93.3% 54.0%	92.0% 60.0%
Table 2: Gender and emotion detection
on users on raw and sanitized data.
5.3	Subject preservation and gender obfuscation
In this setup, we want to find a mapping Q that hides the gender attribute but allows subject verifi-
cation. The mapping Q should prevent a standard gender detection algorithm from performing its
task, while allowing a standard subject detector algorithm to still perform subject verification. This
is the only experiment in this section where the secret inference algorithm is fixed.
To realize this constrained scenario, we used a WideResNet-based Zagoruyko & Komodakis (2016)
implementation of DEXNet Rothe et al. (2018) as our gender detector adversary, and a ResNet-
50 implementation of VGGface2 Cao et al. (2017) as a subject identifier. The stochastic mapping
was implemented using two differing architectures, the first is a stochastic adaptation of the UNET
Ronneberger et al. (2015), and the second a concatenation of a FaderNet Lample et al. (2017) and a
UNET Ronneberger et al. (2015) as shown in Bertran et al. (2018).
The mapping that incorporates a pretrained FaderNet was chosen as the baseline for the stochastic
mapping function since this network is already trained to defeat a gender discriminator in its en-
coding space. This proves a suitable baseline comparison and starting point for a mapping function
that needs to fool a gender discriminator in image space while simultaneously preserving subject
verification performance. We show the performance of using only the pretrained gender FaderNet
7
Under review as a conference paper at ICLR 2019
and demonstrate how we can improve its performance by training a posterior processing mapping
(UNET) using the loss proposed in Eq.10.
We tested this framework on the FaceScrub dataset Ng & Winkler (2014). Figure 5 shows how the
output probabilities of the gender classification model approach the prior distribution of the dataset
as α increases. We see that sanitized images produce output gender probabilities close to the dataset
prior even for relatively low α values. Last column of figure 5 shows how the top-5 categorical
accuracy of the subject verification task varies across different α values. These results suggest that
under these conditions we can achieve almost perfect privacy while maintaining reasonable utility
performance.
P(P(S(X)) = M∣%(x) = F, a)
^δ?δ 0?05	0?2 æʒ oɪ
a parameter
(b) P(S(Q(X)) = M|s(x)=
M, α), Stochastic UNET model
(c) Top-5 Categorical accuracy,
Stochastic UNET model
a parameter
(a) P(S(Q(X)) = M|s(x)=
F, α), Stochastic UNET model
P(P(S(X)) = MIyD(X) = M, a)
0.000^050^200?500.80 baseline
a parameter
(d) P (S(Q(X)) = M |S(X) =
F, α), FaderNet-UNET model
P(P(S(X)) = MIyD(X) =尸,a)
0.00^^0?05^^0^200?500.80 baseline
a parameter
(e) P (S(Q(X)) = M |S(X) =
M, α), FaderNet-UNET model
(f) Top-5 Categorical accuracy,
FaderNet-UNET model
Figure 5: First two columns show gender probabilities on the sanitized data Q(X) as a function of α, results
are split according to real gender (M:Male, F:Female). Whisker plots show median and interquartile ranges,
with outliers shown as circles. Pretrained FaderNet is shown as a baseline for comparison. Third column shows
the top-5 categorical accuracy of the subject recognition task as a function of α. Each row correspond to the
Stochastic UNET and FaderNet-UNET models. Note that the concatenation the UNET with the FaderNet is
able to improve performance on the Top-5 categorical accuracy metric when compared to the baseline model.
6	Concluding Remarks
Inspired by information-theory bounds on the privacy-utility trade-off, we introduced a new
paradigm where users and entities collaborate to achieve both utility and privacy per a user’s spe-
CifiC requirements. One salient feature of this paradigm is that it Can be completely transparent -
involving only the use of a simple user-specific privacy filter applied to user data - in the sense that
it requires otherwise no modifiCations to the system infrastruCture, inCluding the serviCe provider
algorithmic capability, in order to achieve both utility and privacy.
Representative architectures and results suggest that a collaborative user-controlled privacy approach
can be achieved. While the results here presented clearly show the potential of this approach, much
has yet to be done, of particular note is extending this approach to continuous utility and privacy
variables. While the underlying framework still holds, reliably measuring information between
continuous variables is a more challenging task to perform and optimize for.
The proposed framework provides privacy metrics and bounds in expectation; we are currently in-
vestigating how the privacy tails concentrate as data is acquired and if there is a need to use informa-
tion theory metrics with worst-case scenario guarantees. Modifying the information theory metrics
to match some of the theoretical results in (local) differential privacy is also the subject of future
research.
Privacy is closely related to fairness, transparency, and explainability, both in goals and in some of
the underlying mathematics. A unified theory of these topics will be a great contribution to the ML
community.
8
Under review as a conference paper at ICLR 2019
References
Martin Bertran, Natalia Martinez, Afroditi Papadaki, Qiang Qiu, Miguel Rodrigues, and Guillermo
Sapiro. Learning to collaborate for user-controlled privacy. arXiv preprint arXiv:1805.07410,
2018.
Flavio P Calmon, Ali Makhdoumi, and Muriel Medard. Fundamental limits of perfect privacy. In
Information Theory (ISrT),2015 IEEE International Symposium on ,pp.1796-1800. IEEE, 2015.
Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. VGGFace2: A dataset
for recognising faces across pose and age. arXiv preprint arXiv:1710.08092, 2017.
Francois Chollet. XcePtion: Deep learning with depthwise separable convolutions. arXiv preprint,
pp. 1610-02357, 2017.
Cynthia Dwork. Differential privacy: A survey of results. In International Conference on Theory
and Applications of Models of Computation, pp. 1-19. Springer, 2008.
Cynthia Dwork and Moni Naor. On the difficulties of disclosure prevention in statistical databases
or the case for differential privacy. Journal of Privacy and Confidentiality, 2(1), 2010.
Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. Ad-
vances in Neural Information Processing Systems (NIPS), pp. 3315-3323, October 2016.
Salomon Israel, Avshalom Caspi, Daniel W Belsky, HonaLee Harrington, Sean Hogan, Renate
Houts, Sandhya Ramrakha, Seth Sanders, Richie Poulton, and Terrie E Moffitt. Credit scores,
cardiovascular disease risk, and human capital. Proceedings of the National Academy of Sci-
ences, 111(48):17087-17092, 2014.
Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The megaface
benchmark: 1 million faces for recognition at scale. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4873-4882, 2016.
Daniel Kifer. Attacks on privacy and definetti’s theorem. In Proceedings of the 2009 ACM SIGMOD
International Conference on Management of data, pp. 127-138. ACM, 2009.
Daniel Kifer and Ashwin Machanavajjhala. No free lunch in data privacy. ACM SIGMOD Interna-
tional Conference on Management of data, pp. 193-204, 2011.
Daniel Kifer and Ashwin Machanavajjhala. Pufferfish: A framework for mathematical privacy
definitions. ACM Transactions on Database Systems, 39(1):3:1-3:36, 2014.
G Lample, N Zeghidour, N Usunier, A Bordes, L Denoyer, andM Ranzato. Fader networks: Manip-
ulating images by sliding attributes. Advances in Neural Information Processing Systems (NIPS),
2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Arvind Narayanan and Vitaly Shmatikov. Robust de-anonymization of large sparse datasets. IEEE
Symposium on Security and Privacy, pp. 111-125, 2008.
HW Ng and S Winkler. A data-driven approach to cleaning large face datasets. IEEE International
Conference on Image Processing (ICIP), pp. 343-347, 2014.
Seong Joon Oh, Rodrigo Benenson, Mario Fritz, and Bernt Schiele. Faceless person recognition;
Privacy implications in social media. European Conference on Computer Vision (ECCV), pp.
19-35, 2016.
Aaron Reuben, Terrie E Moffitt, Avshalom Caspi, Daniel W Belsky, Honalee Harrington, Felix
Schroeder, Sean Hogan, Sandhya Ramrakha, Richie Poulton, and Andrea Danese. Lest we forget:
comparing retrospective and prospective assessments of adverse childhood experiences in the
prediction of adult health. Journal of Child Psychology and Psychiatry, 57(10):1103-1112, 2016.
9
Under review as a conference paper at ICLR 2019
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-
ical image segmentation. In International Conference on Medical Image Computing and
Computer-Assisted Intervention, pp. 234-241. Springer, 2015.
Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from
a single image without facial landmarks. International Journal of Computer Vision, 126(2-4):
144-157, 2018.
Zhenyu Wu, Zhangyang Wang, Zhaowen Wang, and Hailin Jin. Towards privacy-preserving visual
recognition via adversarial training: A pilot study. arXiv preprint arXiv:1807.08379, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
10
Under review as a conference paper at ICLR 2019
7 Supplementary Material
7.1 Proofs
Proof. Lemma 1
Consider the equality
I(U; S) - I(U; S|X) = I(S; Q(X)) - I(S; Q(X)|U)
+I(U;X|Q(X))-I(U;X|Q(X),S).
We know that
0 ≤ I (S; Q(X )|U) ≤ I (S; Q(X)), 0 ≤ I (U; x∣Q(x ),S) ≤ I (U; X IQ(x)),
(11)
(12)
so we can guarantee
I (U; S)-1 (U; S |X) ≤ I (S; Q(X)) +1 (U; X IQ(X)).	(13)
□
Proof. Theorem 3.3
Consider
AQ = EX,Q[DKL(pU|X || pU|Q)],
BQ = EX,Q[DKL(pU|X || pU|Q)],	(14)
K = I(U,S) - I(U,S | X),
and α ∈ [0, 1].
Minimizing Eq.6 respecting Eq.5 and Eq.12 is equivalent to solving:
minQ{(1 - α)A2Q + αBQ2 },
s.t.AQ, BQ ≥ 0,
AQ + BQ ≥ K,
α∈[0,1],
(15)
Consider the following relaxation of Eq.15
minA,B {(1 - α)A2 + αB2},
s.t.A, B ≥ 0,
A+B ≥ K,
(16)
α ∈ [0, 1],
where A and B are positive real values. Eq.16 is a relaxation of Eq.15 because the space of possible
tuples (AQ, BQ) is included in the space of possible values of R2+
Suppose Q* is the solution to Eq.15, with corresponding values (Aq* , Bq* ), and suppose (A*,B*)
is the solution to Eq. 16. We know
(1 - α)A*2 + αB*2 ≤ (1 - α)A2Q* + αBQ2 *,	(17)
Assume A* > AQ* , it follows that (1 - α)AQ* + αB*2 ≤ (1 - α)A*2 + αB*2.
However, AQ* > 0 and AQ* + B* ≤ A* + B* ≤ K. Therefore, (AQ* , B*) is a valid solution to
Eq.16, and is smaller than the lower bound (A*, B*).
11
Under review as a conference paper at ICLR 2019
This contradiction arises from assuming A* > Aq* , We thus conclude that
A* ≤ Aq* .
Similarly for B* and BQ* We get
B* ≤ BQ*.
Additionally, Eq.16 is easily solvable and has solutions
A* = αK;	B* = (1 - α)K	(18)
Consequently, We proved
EX,Q[DKL(pU|X || pU|Q)] ≥ α[I(U, S) - I(U, S | X)],	19
EX,Q[RDKL(pS ||pS|Q)] ≥ (1-α)[I(U,S)-I(U,S |X)].	()
□
7.2	Experiments on simulated data
The folloWing experiments attempt to shoW hoW close to the theoretical bound shoWn in Eq. 5 We
can get by folloWing Algorithm 1 under knoWn conditions.
Consider the folloWing scenario: Utility variable U and secret variable S are tWo Bernoulli variables
With the folloWing joint distribution:
P (u, s)
kρβ
(1 - kρ)β
(1 - kβ)ρ
1 - β - ρ + kρβ
if (u, s) = (1, 1)
if(u,s)=(0,1)
if(u,s)=(1,0)
if(u,s)=(1,0)
(20)
Where the marginal probabilities are P (U = 1) = ρ, P (S = 1) = β, and k is a parameter that
controls the dependence betWeen and S, k ∈ [0, min{ρ-1, β-1}].
For these experiments, We make both marginals equal to 0.5 (ρ = β = 0.5). Note that When k = 1,
U and S are independent (I(U; S) = 0) and When k = 0 or k = 2 they reach maximum mutual
information (I(U; S) = H(U) = H(S) = Hb(0.5) = ln(1) nats).
Our observations X Will be taken in the extreme case, Where X contains almost perfect information
about the values of U and S. We do this by assuming that X ∈ R2 is a Gaussian Mixture Model X
With the folloWing conditional distribution:
X|U,S 〜N( U ,σ2I2×2)
(21)
We choose a loW σ = 0.05; this makes it so that every pair (u, s) is mapped to almost entirely
disjoint regions, therefore knoWing X gives nearly perfect information about (u, s) (I(U; X) '
H(U), I(S; X) ' H(S), I(U; S|X) ' 0). For added simplicity, the privacy filter is linear:
Qθ (x, z) = θ × x + z
Z 〜N( O ,I2×2)
(22)
12
Under review as a conference paper at ICLR 2019
(a) Results on k = 1
(b) Results on k = 0.4
(c) Results on k = 0
Figure 6: Left column shows the baseline distributions across varying codependence levels I (U, S) =
{0, 0.19, 0.69}[nats] (k = {1, 0.4, 0}). Remaining columns show the linearly sanitized data for varying levels
of tradeoff α, from left to right, α = {0.1, 0.5, 0.8}. As α increases, the amount of utility the filter allows gets
gradually smaller.
Figure 6 shows how the raw and sanitized data are distributed for varying levels of codependence
k and tradeoff α for linear sanitization functions. Figure 7 shows that privacy filters optimized
using Algorithm 1 learn effective privacy-preserving mappings close to the theoretical bounds, even
for a simple filtering architecture. They do so without any explicit modelling of the underlying
data-generating distributions, and we can achieve different tradeoff points by simply modifing the
parameter α. Note that when variables U and S are perfectly independent or codependent, the linear
filter is perfectly able to reach any point in the optimal bound. For intermediate cases, the linear
filter was capable of reaching the bound in the region where no utility is compromised, but was not
capable of following the optimal tradeoff line for higher levels of privacy.
13
Under review as a conference paper at ICLR 2019
(a) Measured privacy loss
IOOO 2000 3000 4000 5000 6000
epochs
1000 2000 3000 4000 5000 6000
epochs
2000	4000	6000	8000 10000
epochs
(b) Distance to information bounds
(c) Trayectories in information plane
Figure 7: Top row shows the best privacy-utility loss so far on the validation set for different levels of code-
pendence I(U ; S) and trade-off parameter α. Middle row shows how the sum of the estimated informations
approximate the information bound for the best privacy-utility loss so far on the validation set. Finally, bottom
row illustrates the trajectory in information space of the privacy-utility filters as they are being trained.
7.3	Implementation details
Here, we explicitly elaborate on how we optimize the data-driven loss functions shown in equations
8, 9, and 10 using an adversarial approach. We first detail the exact adversarial training setup that was
used to perform the experiments in Section 5, and then provide the concrete network architectures
used for all shown results.
7.3.1	Algorithmic implementation
Minimizing the objective functions in equations 8, 9, and 10, is a challenging problem in general.
By focusing our attention on Eq. 8, we see that each of the four loss terms have a distinct purpose:
14
Under review as a conference paper at ICLR 2019
η = argminηEχ,s,z - - Iog(Pn(s∣Qθ(x,z))],
ψ = argminψEχ,u,z - - Iog(Pψ(u∣Qj(x,z))],
φ = argminφEχ,u - - log(Pφ(u∣x)],	(23)
θ = argmine (1 — 3)EX U,Z [DKL(Pφ(u | X) || pψ (U | Q^ (X,Z)))] +
+ αEX,s,z [R:Dkl(P(s) || Pn(S | Qθ(x,z)))]∙
The first three loss terms minimize a crossentropy objective for functions Pη(s | q), Pψ (u | q), and
Pφ(u | X); this ensures that these functions are good estimators of the unknown true distributions of
P(S | q), P(u | q), and P(u | X), where samples q are drawn from the learned sanitization mapping
q = Qθ(X, z). The final loss term attempts to find the best possible sampling function Qθ(X, z) such
that (1 - α)I2(U; X | Q) + αI2(S; Q) is minimized.
We can approximately solve this problem by applying iterative Stochastic Gradient Descent to each
of the individual loss terms with respect to their relevant parameters, this is similar to the procedure
used to train Generative Adversarial Networks.
The algorithm we used to solve Eq. 8 is shown in Algorithm 1, similarly, algorithms to solve Eq. 9
and Eq. 10 are shown in Algorithm 2 and Algorithm 3 respectively.
Algorithm 1 Sanitization algorithm
(θ, η, φ, ψ) J initialize network parameters
1:	repeat
2:	(X(1), u(1), S(1)),...(X(b), u(b), S(b)) 〜Pχ,u,s	. Draw b samples from joint distribution
3:	Z⑴，…z(b)〜PZ	. Draw b samples from sampling distribution
4:	Φ(φ) = b pb=ι -logPφ(ul | xi)	. Evaluate crossentropy loss on raw utility inference
5:	φ J φ — lrVφΦ(φ)	. Stochastic gradient descent step on Pφ(u | x)
6:	Ψ(ψ) = b Pb=I -logPψ(ui∣Qe(χi,zi))	. Evaluate crossentropy loss on filtered utility
inference
7
8
9
10
ψ J ψ - lrVψΨ(ψ)	. Stochastic gradient descent step on Pψ(u | q)
H(η) = b Pb=I -IogPn(si∣Qe(xi, Zi))	. Evaluate crossentropy loss on secret inference
η J η - lrVnH(η)	. Stochastic gradient descent step on Pn(S | q)
θ(θ) = (1 - α)[ b Pb=I log Pψ (pΦ∣Qi(Xi,Zi)) ]2 + α[ b Pb=I log P (si⅞ff,Zi))) ]2	.
Evaluate sanitation loss
11:	θJθ-lrVθΘ(θ)
12:	until convergence
. Stochastic gradient descent step on Qθ(x, Z)
7.3.2 Network architectures
We now describe the exact architecture used to implement the privacy filter on all experiments shown
in Section 5. Figure 8 shows the network diagram.
The architecture presented in Figure 8 is fully convolutional, so the same network definition could
be used across all three experiments by varying the input layer. To speed up convergence to a good
filtering solution, filters were initially trained to copy the image (under RMSE loss), and optionally
infer some meaningful attribute from the input (in subject-within-subject, this attribute was a simple
class label on whether the subject wished their privacy preserved). We stress that this was only done
for initialization, final training of the network was done exactly as described in Algorithm 1.
15
Under review as a conference paper at ICLR 2019
Algorithm 2 Sanitization algorithm; fixed utility
(θ, η) J initialize network parameters
φ J φ Utility inference algorithm is fixed and known
1:	repeat
2:	(x(1), u(1), s(1)),...(x(b), u(b), s(b)) 〜Pχ,u,s	. Draw b samples from joint distribution
3:	Z⑴，…z(b)〜PZ	. Draw b samples from sampling distribution
4:	H(η) = b Pb=ι -logPη(s* i 2 3 4 5 6∣Qθ(xi, Zi)) . Evaluate crossentropy loss on secret inference
5:	η J η 一 lrVηH(η)	. Stochastic gradient descent step on Pn (S | q)
6:	Θ(θ) = (1 - α)[b Pb=ι logPφ(PφQM")]2 + α[b £：曰 logP(门索厂'》]2	.
Evaluate sanitation loss
7:	θ J θ - lrVθΘ(θ)	. Stochastic gradient descent step on Qθ(x, Z)
8:	until convergence
Algorithm 3 Sanitization algorithm; fixed utility and secret
θ J initialize network parameters
φ J φ Utility inference algorithm is fixed and known
η J η Secret inference algorithm is fixed and known
1: repeat
2:	(X(I),u(1), S(I)),…(x(b),u(b), s(b))〜Pχ,u,s	. Draw b samples from joint distribution
3:	Z(I),…z(b)〜PZ	. Draw b samples from sampling distribution
4:	θ(θ) = (1 — α)[b Pb=ι logPφ(Pφ"))]2 + α[b P：=i logP(SlQ1)))]2	.
Evaluate sanitation loss
5:	θ J θ - lrVθΘ(θ)	. Stochastic gradient descent step on Qθ(x, Z)
6: until convergence
16
Under review as a conference paper at ICLR 2019
♦	Conv2D 1x1, ReLu
♦	Conv2D 3x3, BN1 LeakyReIu
♦ UpsampleConv2D
Noise Layer
MaxPooI 2x2
♦ Concatenate
) Auxiliary label Softmax (optional)
Figure 8: Architecture of privacy filter, based on UNET. There is a single noise layer (shown in yellow) where
standard Gaussian noise is injected into the network to make the resulting filtered image stochastic in nature.
The other notable component is the auxiliary label softmax, used for the subject-within-subject experiment.
This extra layer was trained only to initialize the network, but was not preserved during the final training stage.
Input image sizes are shown for the subject-within-subject experiment.
The architecture of the networks used to infer the utility and secret attribute in the emotion vs.
gender experiment are identical, and are shown in Figure 9.
Networks used for the experiments in Section 7.2 are shown in Figure 10.
All other networks used in the results section are implemented as described in their respective papers.
17
Under review as a conference paper at ICLR 2019
*	Conv2D 3x3, stride 2, BN, LeakyReIu
Conv2D 3x3, BN, LeakyReIu
*	MaxPooI 2x2
♦	Add
*	SepConv2D 3x3
*	Conv2D 1x1, stride 2, BN
♦ Conv2D 3x3, GlobalAveragePooling2D, sigmoid
Figure 9:	Architecture of utility and secret inference networks used in the emotion vs gender experiment. These
architectures closely follow the one proposed in Chollet (2017)
2	32	32	32	2
∖∖y
) FuIIyConnected + LeakyReIu
+ Dropout (0.2)
Fully Connected+ Sigmoid
(a)
Figure 10:	Architecture used for utility and secret inference in the experiments described in Section 7.2
18