Under review as a conference paper at ICLR 2019
Sample Efficient Deep Neuroevolution in Low
Dimensional Latent Space
Anonymous authors
Paper under double-blind review
Ab stract
Current deep neuroevolution models are usually trained in a large parameter
search space for complex learning tasks, e.g. playing video games, which needs
billions of samples and thousands of search steps to obtain significant perfor-
mance. This raises a question of whether we can make use of sequential data
generated during evolution, encode input samples, and evolve in low dimensional
parameter space with latent state input in a fast and efficient manner. Here we
give an affirmative answer: we train a VAE to encode input samples, then an RNN
to model environment dynamics and handle temporal information, and last evolve
our low dimensional policy network in latent space. We demonstrate that this ap-
proach is surprisingly efficient: our experiments on Atari games show that within
10M frames and 30 evolution steps of training, our algorithm could achieve com-
petitive result compared with ES, A3C, and DQN which need billions of frames.
1	Introduction
Training an agent that can handle difficult tasks in complex environment is always challenging for
artificial intelligence. Most works for solving such problems are using reinforcement learning algo-
rithms, e.g. learning a value function (Watkins & Dayan, 1992), or a policy function (Kakade, 2002),
or both (Sutton et al., 2000). Combined with deep neural networks, the so-called deep reinforcement
learning achieved great success in playing Atari games (Mnih et al., 2015), operating robots (Yahya
et al., 2017), and winning human experts in challenging competitions like Go and Dota (Silver et al.,
2016; OpenAI, 2018). Recently, a portion of works are trying to combine deep neural networks with
evolution strategies to solve similar problems. This approach, called deep neuroevolution, achieved
competitive results compared to deep reinforcement learning on Atari Games and Mujoco tasks as
well in much shorter training time due to its outstanding scalability and feasibility for parallelization
(Salimans et al., 2017; Such et al., 2017).
However, large-scale deep neuroevolution is both data and computation inefficient: it consumes
thousands of CPU cores and needs billions of frames and thousands of generations to obtain signif-
icant results. In this work, we introduce a novel way to improve its efficiency: we train a variational
autoencoder (VAE) to encode samples, and a recurrent neural network to model environment dy-
namics and handle temporal information; afterward, we train a two layer policy network with latent
state vectors using covariance matrix adaption evolution strategy (CMA-ES).
We evaluate our sample efficient deep neuroevolution algorithm, or in short SEDN, on 50 Atari
Games from OpenAI Gym (Brockman et al., 2016), experiment shows that our approach surpassed
DQN (Mnih et al., 2013), A3C (Mnih et al., 2016), and ES (Salimans et al., 2017) methods in several
Atari games within much fewer state frames, shown in Table 1. Our key findings are listed below:
•	Evolution goes faster with latent state input. SEDN takes less than 30 evolution steps with
only 32 children to surpass ES (Salimans et al., 2017) in several games: the latter needs
thousands of children and generations.
•	Training is more data efficient. Compared to ES and asynchronous RL methods which
needs billions of frames to train, SEDN takes less than 10 million frames to achieve targets.
•	Training is more computation efficient. On a workstation of one Intel(R) Xeon(R) CPU
E5-2640 v4 @ 2.40GHz and one Titan X Pascal GPU, SEDN takes less than 4 hours to
finish the evolution, compared to ES which needs a cluster to finish.
1
Under review as a conference paper at ICLR 2019
2	Method
Many real world decision making problems could be modeled as Markov Decision Process (MDP).
Since proposed by Bellman (1957), it has been applied in many disciplines, e.g. robotics, economics
and game playing. A Markov decision process is defined by: a set of states S, a set of actions
A, transition probabilities defining probability over next state given current state and action P :
S × A × S → [0, 1], a initial state distribution d : S → [0, 1], and a reward function that maps
states and actions into a scalar R : S × A → R. A finite time horizon episode τ of T steps is a
sequence of state-action-reward tuples: (so, ao, ro), (sι,aι,rι),..., (st, at, rt),... (ST, 0, 0) called
trajectory, where St ∈ S, at ∈ A, r = R(st, at), st+ι 〜 P (st, at), so 〜do(s) where ST is called
terminal state.
For a finite time horizon MDP, reinforcement learning (RL) algorithms learn a parameterized policy
that defining probability of actions to be taken given current states πθ : S × A → [0, 1], and
maximize episodic reward: PiT=-o1 ri , or formally:
T
maximize E	R(st, at)	(1)
θ	i=o
where so 〜d(s),at 〜∏ (st), st+ι 〜P (st, at)	(2)
Above objective function takes the expectation over episodic rewards, which could be approximated
by sampling N trajectories from policy π and environment dynamics P and d. We misuse notations
such that r(τ) donates episodic reward and p(τ) represents trajectory probability, then we have
below reformulation:
N
maximize L(θ) = X r(τi)p(τi)	(3)
θ	i=1
T-1	T-1
wherep(τ) = d(so)	πθ(st, at)p(st, at, st+1), r(τ) = ri	(4)
t=o	t=o
Vanilla policy gradient (PG) methods optimize L(πθ) by taking its gradient:
1N
Vθ L ≈ NEr(TiN θ log Pθ (Ti)	(5)
i=1
1 N	Ti -1
=NEr(Ti) E Vθ logπθ(st,at)	(6)
i=1	t=o
and updates policy πθ to maximize expected episodic reward. Variants of policy gradient methods
achieved great success in recent years, e.g. TRPO (Schulman et al., 2015) and PPO Schulman et al.
(2017).
Evolution strategies (ES), instead, take another approach to solve above optimization problem with-
out any differentiation: it approximates the gradient by sampling, shown in Equation 7.
1
VθL ≈ σEθ〜N(0,σ2)(L(θ + θ)θ)	(7)
Vanilla ES solves MDP problems iteratively shown in Algorithm 1: firstly, it samples Gaussian
noise from a normal distribution; secondly, it injects noise into current policy, and evaluates it in
environment, obtaining episodic rewards; thirdly, it updates current policy parameters by the product
of episode reward and noise injected. ES and PG are interpreted as two faces of Gaussian smoothing
on policy: ES is on parameter space smoothing and PG is on action space smoothing (Salimans
et al., 2017).
2
Under review as a conference paper at ICLR 2019
Algorithm 1 Evolution Strategies
Input: Learning rate η, noise standard derivation σ, initial policy parameter θ
1:	for t = 0, 1, . . . do
2:	Sample N parameter noise: θi 〜N (0, σ2I) fori = 1, . . . ,N
3:	Execute perturbated deterministic policies πθi : at = arg maxa πθi (st) to produce trajectory
τi, where θi = θ + θi for i = 1, . . . , N
4:	Update policy parameter θ  θ + α∕(Nσ2) PN=I r(τ)θi
5:	end for
ES enjoys several advantages over PG. Firstly, it is a black-box optimization method that does not
need gradients. In problems where gradients are not available, or state encoding is not feasible, or
gradient is blocked by some operations in neural networks, ES exhibits its wider range of appli-
cations over PG. Secondly, in low dimensional optimization problems like Mujoco tasks, ES has
competitive performance with PG (Salimans et al., 2017), but is less sensitive to hyperparameters
(Heidrich-Meisner & Igel, 2008). Thirdly, the exploration-exploitation trade-off needs to be bal-
anced in RL here is integrated: policy parameter perturbation ensures different behaviors during
evaluation, whereas RL has to use a stochastic policy to explore. Lastly, ES is highly scalable
therefore can be trained faster by a group of workers in parallel.
However, in n-dimensional search space where n is large, the exploration directions O(2n) increases
in orders with n, therefore random sampling for gradient approximation would be very inefficient.
For video games like Atari, high dimensional pixel state space induces more parameters of policy
to be optimized, hence less efficient for ES. Surprisingly, Salimans et al. (2017) shows that even
natural evolution strategies (NES) can beat common RL algorithms in several Atari games. Among
the class of evolution strategies, the covariance matrix adaption evolution strategy (Hansen & Oster-
meier, 2001) is the most successful one in solving low dimensional optimization problems. Would
covariance matrix adaption evolution strategy (CMA-ES) perform better or faster over NES for high
dimensional tasks? To answer this question, one problem has to be solved, that the application of
CMA-ES on high dimensional space is infeasible since it needs to compute a large covariance ma-
trix of size O(n2 ). Recently advances in deep learning show promises to encode high dimensional
spatial-temporal information into low dimensional latent vectors (Oh et al., 2015; Pathak et al., 2018;
Ha & Schmidhuber, 2018) by modeling environment dynamics. In this case, we can convert high
dimensional RL tasks into low dimensional tasks, where CMA-ES can take its advantage.
2.1 Covariance Matrix Adaption Evolution Strategy
CMA-ES improves ES in two ways. Firstly, vanilla ES samples noise from a fixed Gaussian distri-
bution N(0, σ2I) of only one degree of freedom σ, the step size; whereas CMA-ES, samples noise
from N(0, C) where covariance matrix C has n(n + 1)/2 degree of freedom. Secondly, CMA-ES
updates sampling parameters σ, C by rank based method, rather than updating policy parameter by
approximating its gradients.
We now explain rank-based ES. Rank-based ES is a selection-recombination process. Let f donates
the objective function, λ be population size, μ be parent population size, {w∕wι > w2 > ∙… >
wμ > 0, pμ=ι wμ = 1,i = 1,...,μ} be weight coefficient for recombination. Firstly, Rank-
based ES first samples λ points from Gaussian distribution Xi 〜 m + σNi(0, C) for i = 1,...,λ.
Next, it evaluates sampled points on the objective function, and sort according to the cost such that
f (xι) < f (x2) < .… < f (xλ). In the following, it recombines to get new mean and covariance
matrix m J Pμ=ι WiXi, C J Pμ=ι Wi(Xi - m)(xi - m)T. This procedure repeats for G
generations.
Similar to gradient based optimization methods, CMA-ES further uses momentum, step size control
and path accumulation for more stable optimization. Let y(g+1) 〜 N(0, C(g)), i = 1,...,λ be
sampled noise to be injected, Xi = m + σyi, suppose {y∕i = 1,...,μ} are sorted such that
f (xι) < f (x2) < .… < f (xλ), updating of covariance matrix with momentum gives:
μ
C(g+1) = (1 - Cμ)c(g) + Cμ X Wiy(g+1) y(g+1)T	⑻
i=1
3
Under review as a conference paper at ICLR 2019
where g is the number of generation and cμ is momentum constant. In favor to generate best point
from the current generation, it further adds the best point’s covariance term into Equation 8:
μ
C(g+I) = (I- Cμ - cι)C⑷ + cιy(g+1)y1g+1)T + c“ X Wiy(g+1)y(g+1)T	(9)
i=1
In practice, exponential smoothing over parents’ mean path, or evolution path, is used:
pCg+1) = (1 - Cc)Pcg) + cc(m(g+1) - m(g))∕σ(g)	(10)
μ
C(HI) = (1 - Cμ - Cl)C⑷ + C1 Pcg+I)Pcg+I)T + Cμ X Wiy(g + 1)y(g+1)T	(11)
i=1
Next, we explain how to control the step size σ. The step size is related to the evolution path: it
should be decreased if the path is short and increased if the path is long. In contrast to evolution
path Pc , here we need to correct its directions by taking conjugate evolution path Pσ :
p(g+1) = (1 — Cσ )p(g) + % C(g),-1 (m(g+1) — m(g))∕σ(g)	(12)
To update step size, we compare conjugate evolution path with the expectation of length of vectors
sampled from normal distribution N (0, I):
ln σ(g+1)
ln σ(g)
kPσ k
+ cσ (
+ dσ (EkN(0,1)k
- 1)
(13)
Hansen (2016) suggests default hyperparameters for CMA-ES, we summarize complete CMA-ES
in Algorithm 2.
Algorithm 2 CMA-ES
Input: m ∈ Rn,σ ∈ R+,λ,μ ∈ N,μ<λ, C = I, pc = 0, pσ = 0
Wi SUch that μw = 1/ Pμ=ι w2 ≈ 0.3λ, wι ≥ …≥ Wμ > 0, Pμ=i Wi = 1
cc, ≈ 4/n, Cσ ≈ 4∕n,c∖ ≈ 2∕n2,c* ≈ μw/n2,cι + Cμ ≤ 1,dσ ≈ 1 + √μw/n
1:	for each generation step do
2:	Sampling: Ni 〜 Ni(0, C), Xi = m + σy for i = 1,...,λ
3:	Evaluation: obtain f (xi), sort {x∕i = 1,...,λ} points such that f (xι) ≤ ∙∙∙ ≤ f (xλ)
4:	Mean update: m — Piμ=1 WiXi, Yw = pμ=ι Wiy________________
5:	Evolution path update: Pc J (1 - Cc)Pc + yw √1 - (1 - cc)2√μW
6:	Conjugate evolution path update: Pσ J (1 一 Cσ)Pσ + C-Iyw Vz1 - (1 - Cσ)2√μW
7:	Covariance matrix update: C J (1 — ci — Cμ)C + CIPcPcT + Cμ P=I WiyiyT
8:	Step size update: σ J σ exp(怒(EkN011)k — 1))
9:	end for
CMA-ES addresses typical problems in non-linear optimization problems like ill-conditioning and
ruggedness. The covariance matrix approximates the inverse Hessian matrix, and the updates ap-
proximate natural gradient by adapting the search metric into a sphere, hence increasing performance
by orders of magnitude. Rank based selection ensures its invariance against translation and rotation
in search space. The step size control facilitates fast convergence. However, it is not applicable to
deep neural networks used in DRL which has millions of parameters and induces O(n2) order of
the size of the covariance matrix. This leads to an intuitive approach by encoding frames into small
latent vectors to model the dynamics of the game environment, and evolve with a lower dimensional
policy parameter space.
4
Under review as a conference paper at ICLR 2019
Mean Vector
Input Image
Conv
Encoder
Network
Sampled
Latent
Vector
Standard
Derivation
Vector
(a) Variational Autoencoder
(b) LSTM Forward Model Dynamics
Previous
RNN Hidden
State
⅛-ι
Current
Latent
State
MLP
Controller
Action to take
at
(c) Policy Network
Figure 1: Our SEDN model. VAE encodes state frame into latent vector; LSTM encodes current
latent state and action, predicts next latent state; policy network takes current latent state and hidden
state from previous timestep of RNN as input, predict action to execute
2.2 Environment Dynamics Modeling for Episodic Data Encoding
How to learn a model that can encode episode data τ : s0, a0, s1, a1, . . . sT , aT ? We develop a
method by modeling environment dynamics. Perhaps the earliest work on building predictive model
for vision based RL tasks was introduced by Schmidhuber & Huber (1991), which proposed using
neural networks to predict attention region given previous frames and actions. After the success
of deep learning, Oh et al. (2015) proposed an Encoding-Transformation-Decoding framework for
action conditional video prediction, and applied on Atari games. More recently, Pathak et al. (2018)
learns forward and inverse model dynamics to imitate expert’s behavior given only image sequences.
Ha & Schmidhuber (2018) also learned forward model dynamics using random policy demonstra-
tions, and train a controller totally inside the dream.
In this work, we take similar approach to encode episodic data. Specifically, we first train a Vari-
ational Autoencoder (VAE) that encode spatial information; then train an recurrent neural network
(RNN) to encode temporal information by learning model dynamics. Let φe : S → Rn , φd :
Rn → S donates our encoder and decoder network where n is our latent space dimension,
f : Rn × A → Rn be the our RNN, and c : Rn × Rm → A be our policy network where m
is the dimension of our RNN’s hidden state. The encoder φe encodes state frame st into a latent
vector zt :
μ(st),σ(st) = φe(st)
Zt 〜N(μ(xt),σ(xt))
(14)
(15)
The decoder φd decodes latent vector back to state frame ^t. VAE is trained to minimize the recon-
struction loss between st, ^t, and KL-divergence between latent vector and N(0, I):
5
Under review as a conference paper at ICLR 2019
St = φd(zt)	(16)
Lvae = kst- Stk2 + KL[N(μ(st),σ(st))kN(0,I)]	(17)
LSTM f models the environment dynamics in latent state space, and encode temporal information:
μ(^t+1),σ(^t+1) = f(zt,at)	(18)
zt+ι 〜N(μ(st+ι), σ(st+ι))	(19)
LSTM is trained to minimize the KL-divergence between actual latent state from vae’s encoder and
predicted latent state:
μ(st+1),σ(st+1) = φe(st+ι)	(20)
Lrnn = KLW (μ(^t+ι ),σ(^t+ι)) kN (μ(st+1),σ(st+1))]	(21)
The policy network takes the latent vector of the current state, and previous timestep’s hidden state
of RNN, we use deterministic policy in this work:
at 〜c(zt, ht-i)
We illustrate our model in Figure 1.
3	Experiments
Our complete training pipeline is summarized in Algorithm 3. We use OpenMPI’s master-slave
mode (Scarabello & Clipp, 2018) for parallel training of policy network during evolution steps,
Hansen (2018) for CMA-ES optimizer, and Pytorch (Paszke et al., 2017) deep learning platform to
train VAE and RNN. We train the three modules separately. Firstly, we collect episode data during
training the policy network using CMA-ES. Secondly, we extract all video frames from collected
data and train our variational autoencoder. Thirdly, we convert all state frames into latent state
vectors using VAE trained. Lastly, we train our LSTM using the converted episode data. This
training cycle iterates till the end.
During the training, we set our training data buffer to be a FIFO queue of size 1M: this will ensure
the safety of workstation memory. The training process will stop when the maximum number of
frames is reached or the maximum evolution step is reached, whichever is earlier. Our parallel
training adopts a task-queue master-slave mode: the master node immediately distributes remaining
tasks once there is any slave node available, and slave node requests new task right after it finishes
a previous task. Unlike Salimans et al. (2017), our master-slave communication frequency is so low
that it only takes a small percentage of time compared to episode data sampling steps.
We conduct experiments on 50 Atari 2600 games in OpenAI Gym (Brockman et al., 2016). We
choose a larger encoder convolutional network compared to Vanilla DQN (Mnih et al., 2013) for
better VAE performance: our experiment shows poor decoding performance if following Vanilla
DQN’s convolutional network. Our data pre-possessing is also different from Vanilla DQN: we
resize image frames directly without frame stack, grayscale conversion or frame skip. We tried to
stack latent state vector as input for policy network but found performance drop: the reason may be
that increased dimension required for policy network slows down the evolution process for CMA-ES
optimizer.
Our encoder network consists of 4 convolutional layers with 32, 64, 128, 256 channels followed
by a flatten layer and a linear layer of 128 units. The convolutional layers use 4 × 4, 4 × 4, 3 ×
3, 3 × 3 filter size and strides of 2, 2, 1, 1. Our decoder network reverses the operations except
the last deconvolutional layer uses 6 × 6 filter size. Each convolutional and deconvolutional layer
uses rectified activation, except that last convolutional layer uses tanh activation. Image frames are
resized into (84, 84, 3) to feed into our network. We set our latent state dimension to be 128. We use
one layer LSTM of 128 hidden size to model environment dynamics, therefore our input dimension
of policy network is 256. Our policy network is a two-layer MLP of size 256 - 32 - na , where na
is the action space dimension.
6
Under review as a conference paper at ICLR 2019
The training of VAE and RNN is triggered every 8 evolution step. Each child policy is evaluated
for 16 episodes, and the average episode reward is returned for CMA-ES optimizer to update. To
visualize the training process of policy network, we make a boxplot of average episode reward dis-
tribution of Frostbite, shown in Figure 2 The maximum reward performance among policy networks
increases significantly right after the training of VAE and RNN triggered at 8th, 16th generations.
This agrees with our assumption that VAE and RNN encode spatial-temporal information from past
experience and provides good feature representation for policy to evolve. It is worth to notice that
for some game, e.g. Private Eye, policy performance increases significantly even without updating
VAE and RNN; this observation agrees with Blundell et al. (2016) where features are extracted from
random matrix projection.
Figure 2: Boxplot of the training process of Frostbite. Maximum reward performance among policy
networks increases significantly after training of VAE and RNN triggered at 8th, 16th generations.
Figure 3: Boxplot of the training process of Atlantis. Maximum reward performance among policy
networks drops significantly after training of VAE and RNN triggered at 8th generations.
However, we also notice that for some games, policy networks performance drops significantly right
after updating of VAE and RNN, e.g Atlantis shown in Figure 3. We find that the cause may be
the poor encoding performance of VAE. Shown in Figure 4, we compare VAE’s encoding-decoding
on Frostbite and Atlantis. We find that image decoded from VAE will ignore tiny moving objects,
leaving only a static background; this issue occurs especially in rare frames far from episode starts;
similar phenomenon was found in games like Kangaroo, Gravitar and Breakout.
Experiment result is summarized in Table 1. Our SEDN surpasses ES, DQN, and A3C in 7 of
50 Atari games, but with only 10M frames, whereas DQN using 200M frames, and ES and A3C
using 1B frames. Due to the limitation of time and resources, we did not conduct hyperparameter
tuning on SEDN. We believe better encoding techniques to address above issues will give a boost of
performance on SEDN. We left this as an open direction in future works.
4	Discussion
Whereas traditional reinforcement learning algorithms need to balance between exploration and ex-
ploitation (Sutton et al., 1998), and even add perturbation on parameters to ensure good exploration
(Plappert et al., 2017); evolution strategy directly drives exploration by sampling from optimizer’s
distribution parameters (Salimans et al., 2017) during evolution: it integrates exploration and ex-
ploitation. This is what makes it so efficient especially in some low dimensional control tasks like
CartPole. By encoding spatial-temporal information from high dimensional pixel space into low
dimensional latent space, a challenging RL task could hence be converted into a simpler task where
evolution strategies have advantages over deep reinforcement learning methods.
7
Under review as a conference paper at ICLR 2019
(a) Frostbite #1	(b) Frostbite #16	(c) Atlantis #1	(d) Atlantis #16
Figure 4: Comparison of VAE’s encoding-decoding on Atlantis and Frostbite. Upper: Original
image; Lower: decoded image from the latent vector. Notice for Frostbite, decoded image agrees
with original image at the 16th frame; but for Atlantis decoder does not recover the original small
objects at 16th frame
Similar to what Salimans et al. (2017) are facing: ES does not perform well on some easy games
like Enduro and Breakout. One possible reason is that ES’s optimized policy is close to where it
is randomly initialized due to evolution path control under ES, whereas optimal policy for dense
reward games like Breakout are far away from where it is randomly initialized. Another reason
is that, long episodic time horizon games like Enduro are very inefficient for ES to solve: given
limited number of frames, a longer episode will result in less number of episodes, hence less and
slower updates for ES. In addition, episode rewards ignore temporal reward information over time
steps. This issue is critical in games like Enduro. Enduro is a car racing game, in which agent
obtain rewards after passing other cars and penalty if it is passed by other cars, final ranking will be
accounted as the episodic reward after a long episode. A random policy will always obtain a zero
episodic reward under this case, hence ES learns nothing from such games. These two issues limit
the application of vanilla ES and its variants, remaining to be resolved in future works. A possible
direction is a mixture of ES and RL, e.g. using ES as a warm start of RL policy.
5	Conclusion
Our work introduces a fast and efficient evolution algorithm that trains deep neural networks to
play Atari games. Our experiment shows that encoding spatial-temporal information from high
dimensional pixel space into low dimensional latent space makes ES fast, effective and efficient:
SEDN outperforms DQN, A3C, and ES in 30 evolution steps with 10M frames experience in several
Atari games. We conclude SEDN is a competitive approach to solving challenging RL tasks.
In future works, we plan to address issues discussed in section 4by applying state of the art computer
vision techniques like self-attention maps for better information encoding; we would also like to
apply information encoding with RL to understand feature importance for RL.
8
Under review as a conference paper at ICLR 2019
Table 1: Result is obtained using SEDN on Atari games with 16 re-runs with up to 30 random initial
no-ops compared with DQN Mnih et al. (2013), A3C Mnih et al. (2016), and ES Salimans et al.
(2017); DQN, A3C, ES data are obtained from Salimans et al. (2017). Enduro’s data is missing due
to its long episode problem discussed in Section 4
	DQN	A3C,FF	ES, FF	SEDN, Ours
Frames	200 M	1B	1B	10M
Time	7-10 day	4 day	4 hour	4 hour
Amidar	133.4	283.9	112	33.4
Assault	3332.3	3746.1	1673.9	175.9
Asterix	124.5	6723	1440	387.5
Asteroids	697.1	3009.4	1562	519.4
Atlantis	76108	772392	1267410	5237.5
Bank Heist	176.3	946	225	13.1
Battle Zone	17560	11340	16600	3625
Beam Rider	8672.4	13235.9	744	332.8
Berzerk		1433.4	686	224.2
Bowling	41.2	36.2	30	97.6
Boxing	25.8	33.7	49.8	55.3
Breakout	303.9	551.6	9.5	8.8
Centipede	3773.1	3306.5	7783.9	3586.9
Chopper Command	3046	4669	3710	3718.8
Crazy Climber	50992	101624	26430	23662.5
Demon Attack	12835.2	84997.5	1166.5	1812.4
Double Dunk	21.6	0.1	0.2	-8.8
Enduro	475.6	82.2	95	
Fishing Derby	2.3	13.6	49	-75.4
Freeway	25.8	0.1	31	23.8
Frostbite	157.4	180.1	370	387.5
Gopher	2731.8	8442.8	582	242.5
Gravitar	216.5	269.5	805	131.2
Ice Hockey	3.8	4.7	4.1	-2.9
Kangaroo	2696	106	11200	262.5
Krull	3864	8066.6	8647.2	1238.2
Montezumas Revenge	50	53	0	0
Name This Game	5439.9	5614	4503	5403.1
Phoenix		28181.8	4041	1258.1
Pit Fall		123	0	0
Pong	16.2	11.4	21	21
Private Eye	298.2	194.4	100	6674.6
Q*Bert	4589.8	13752.3	147.5	160.9
River Raid	4065.3	10001.2	5009	531
Road Runner	9264	31769	16590	26430.8
Robotank	58.5	2.3	11.9	4
Seaquest	2793.9	2300.2	1390	128.8
Skiing		13700	15442.5	16948.4
Solaris		1884.8	2090	2832.5
Space Invaders	1449.7	2214.7	678.5	148.8
Star Gunner	34081	64393	1470	337.5
Tennis	2.3	10.2	4.5	-2.1
Time Pilot	5640	5825	4970	1518.8
Tutankham	32.4	26.1	130.3	12.5
Up and Down	3311.3	54525.4	67974	8031.8
Venture	54	19	760	12.5
Video Pinball	20228.1	185852.6	22834.8	23058.4
Wizard of Wor	246	5278	3480	606.2
Yars Revenge		7270.8	16401.7	2953.7
Zaxxon	831	2659 9	6380	262.5
Under review as a conference paper at ICLR 2019
Algorithm 3 Complete Training Pipeline
Input: VAE encoder φenc : xt → zt, VAE decoder φdec : zt → xt
RNN f : (zt, at) → zt+1, Policy p : (zt, ht-1) → at and CMA-ES optimizer: g
1: for each iteration do
2: training data buffer X = {}
// Train policy network and collect data
3: for each evolution step do
4:	sample λ children {c1, . . . , cλ} from CMA-ES optimizer g
5:	for child i from 1 to λ do
6:	contrust policy pi from ci
7:	for each episode j from 1 to N do
8:	sample episode data Yij = (s0, a0, . . . , sT , aT ) and episode reward Rij with VAE,
RNN and Policy: φenc, f,Pi
9:	append episode data to training data X — X ∪ {Yj }
10:	end for
11:	Calculate average return for child i: IRi = pN=1 Rij/N
12:	end for
13:	Update CMA-ES optimizer g J g(c1, R1,c2,R2,...,Cλ, Rλ)
14:	end for
// Train VAE and extract frames
15: Training image frames I = {}
16: for each Y in X do
17:	(s1 , a1 , . . . , sT , aT) — Y
18:	I — I ∪ {s1, s2, . . . , sT}
19:	end for
20:	Train and update VAE φenc , φdec with I
21: Extracted episode data S = {}
22: for each Y in X do
23:	(s1 , a1 , . . . , sT, aT) — Y
24:	S — S ∪ {φenc(s1), a1, . . . ,φenc(sT),aT}
25:	end for
// Train RNN
26: for each Y in S do
27:	(φenc (s1 ), a1 , . . . , φenc (sT ), aT ) — Y
28:	μi:T ,σi:T = φenc(si：T), zi：T 〜N(μi:T,σi:T)
29:	42：T,d* l 112∙.τ = f(zi：T-1, ai：T-1)
30:	Calculate KL-divergenceLoss L = KL[N(μ(^t+ι), σ(^t+ι))∣∣N(μ(st+ι), σ(st+ι))]
31:	Backward NLL loss and update RNN f
32:	end for
33: end for
34: Construct final policy pf from CMA-ES optimizer’s favorite solution
35: return φdec , φenc , f, pf
10
Under review as a conference paper at ICLR 2019
References
Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, pp.
679-684,1957.
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo,
Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint
arXiv:1606.04460, 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
David Ha and Jurgen Schmidhuber. World models. arxiv preprint. arXivpreprint arXiv:1803.10122,
2018.
Nikolaus Hansen. The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.
Nikolaus Hansen. pycma. https://github.com/CMA-ES/pycma, 2018.
Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution
strategies. Evolutionary computation, 9(2):159-195, 2001.
Verena Heidrich-Meisner and Christian Igel. Evolution strategies for direct policy search. In Inter-
national Conference on Parallel Problem Solving from Nature, pp. 428-437. Springer, 2008.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937, 2016.
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional
video prediction using deep networks in atari games. In Advances in neural information process-
ing systems, pp. 2863-2871, 2015.
OpenAI. Openai five, Jul 2018. URL https://blog.openai.com/openai-five/.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan
Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. In
International Conference on Learning Representations, 2018.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
arXiv preprint arXiv:1706.01905, 2017.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Luca Scarabello and Landon T. Clipp. mpi-master-slave. https://github.com/luca-s/
mpi-master-slave, 2018.
11
Under review as a conference paper at ICLR 2019
Juergen Schmidhuber and Rudolf Huber. Learning to generate artificial fovea trajectories for target
detection. International Journal of Neural Systems, 2(01n02):125-134, 1991.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and
Jeff Clune. Deep neuroevolution: genetic algorithms are a competitive alternative for training
deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press,
1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen Chebotar, and Sergey Levine. Collective robot
reinforcement learning with distributed asynchronous guided policy search. In Intelligent Robots
and Systems (IROS), 2017 IEEE/RSJ International Conference on, pp. 79-86. IEEE, 2017.
12