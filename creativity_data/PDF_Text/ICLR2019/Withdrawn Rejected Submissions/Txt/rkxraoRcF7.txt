Under review as a conference paper at ICLR 2019
Learning Disentangled Representations with
Reference-Based Variational Autoencoders
Anonymous authors
Paper under double-blind review
Ab stract
Learning disentangled representations from visual data, where different high-level
generative factors are independently encoded, is of importance for many computer
vision tasks. Supervised approaches, however, require a significant annotation
effort in order to label the factors of interest in a training set. To alleviate the
annotation cost, we introduce a learning setting which we refer to as “reference-
based disentangling”. Given a pool of unlabelled images, the goal is to learn a
representation where a set of target factors are disentangled from others. The
only supervision comes from an auxiliary “reference set” that contains images
where the factors of interest are constant. In order to address this problem, we
propose reference-based variational autoencoders, a novel deep generative model
designed to exploit the weak supervisory signal provided by the reference set.
During training, we use the variational inference framework where adversarial
learning is used to minimize the objective function. By addressing tasks such as
feature learning, conditional image generation or attribute transfer, we validate the
ability of the proposed model to learn disentangled representations from minimal
supervision.
1 Introduction
Natural images can be considered the result of a generative process involving many factors of vari-
ation. For instance, the appearance of a face is determined by the interaction between many latent
variables including the pose, the illumination, the subject’s age and expression. Given that the inter-
action between these underlying explanatory factors is usually very complex, inverting the genera-
tive process is extremely challenging. From this perspective, learning disentangled representations
where different high-level generative factors are independently encoded, can be considered one of
the most relevant problems in computer vision (Bengio et al., 2013). For instance, these represen-
tations can be applied to complex classification tasks given that features correlated with the image
labels can be identified. We can find another example in conditional image generation (van den
Oord et al., 2016; Yan et al., 2016), where disentangled representations allow to manipulate desired
attributes in synthesized images.
Motivation: Latent variable models have a long history as tools to learn abstract data representa-
tions. The main idea behind these methods is to define a probabilistic model relating the observa-
tions, e.g. images, with a set of latent variables encoding the factors of variation underlying the data.
In recent years, Variational autoencoders (VAEs) (Kingma & Welling, 2014) have emerged as a pow-
erful latent variable model coupling deep learning with variational inference. However, even though
VAEs are able to build generic data representations, they are trained in an unsupervised manner and,
thus, they lack a mechanism to impose specific high-level semantics on the latent variables. In order
to address this limitation, different semi-supervised autoencoders have been proposed (Rifai et al.,
2012; Kingma et al., 2014). In these approaches, a subset of the latent factors are assumed to be
manually labelled in a training set. These annotations provide supervision to the model, and allow
to disentangle the labelled variables from the remaining generative factors. The main drawback of
this strategy is that it require a significant effort in order to annotate the targeted factors in a dataset.
In this context, we are motivated by the following question: “Is it possible to disentangle specific
factors of variation with minimal supervision?”.
1
Under review as a conference paper at ICLR 2019
Figure 1: Illustration of different reference-based disentangling problems. (a) Disentangling style
from digits. The reference distribution is composed by numbers with a fixed style (b) Disentangling
factors of variations related with facial expressions. Reference images correspond to neutral faces.
Contributions. In this paper, we introduce “reference-based disentangling”. A learning setting in
which, given a training set of unlabelled images, the goal is to learn a representation where a specific
set of generative factors are disentangled from the rest. For that purpose, supervision comes in the
form of an auxiliary “reference set” containing images where the factors of interest are constant.
See Fig. 1 for illustrative examples. Different from a semi-supervised scenario, explicit labels are
not available during training. In contrast, reference-based disentangling is a weakly-supervised task,
where the reference set only provides implicit information about the generative factors that we aim
to disentangle. Note that a collection of reference images is generally easier to obtain compared
to explicitly annotations of target factors. For instance, if we are interested in disentangling facial
gesture information from faces, we would need to annotate images according to different expression
classes. Despite the fact that this is feasible for a reduced number of basic gestures, naturalistic
expressions depend on a combination of a large number of facial muscle activations with their cor-
responding intensities (Ekman & Rosenberg, 1997). As a consequence, annotating facial expression
datasets typically requires a very expensive process. By contrast, collecting a large collection of
neutral faces is much easier and can be carried out by non-expert annotators.
The main contributions of our paper are summarized as follows:
•	We propose reference-based variational autoencoders (Rb-VAEs). Different from standard un-
supervised VAEs, our model is able to impose high-level semantics into the latent variables by
exploiting the weak supervision provided by the reference set.
•	We identify critical limitations of the VAE objective function when used to train our model. For
this reason, we propose an alternative training procedure based on recently introduced ideas in
the context of variational inference and adversarial learning.
•	By learning disentangled representations from minimal supervision, we show how the proposed
framework is able to naturally address different tasks such as feature learning, conditional image
generation, and attribute transfer.
2	Related work
Deep Generative Models have been extensively explored to model visual and other types of data
during recent years. Variational autoencoders (Kingma & Welling, 2014) and generative adversarial
networks (GANs) (Goodfellow et al., 2014) have emerged as two of the most effective frameworks.
VAEs use variational inference in order to learn an encoder network that maps images to a pos-
terior distribution over latent variables. Similarly, a decoder network is learned that produces the
conditional distribution on images given the latent variables. GANs are also composed of two dif-
ferentiable networks. The generator network synthesizes images from latent variables, similar to the
VAE decoder. The discriminator network discriminates between real training images and generated
synthetic images. During training, GANs employ an adversarial learning procedure which allows to
simultaneously optimize the discriminator and generator parameters. Even though GANs have been
shown to generate more realistic samples than VAEs, their main limitation is the lack of an infer-
ence mechanism able to map images into their corresponding latent variables. In order to address
this drawback, there have been several attempts to combine ideas from VAEs and GANs (Larsen
et al., 2015; Dumoulin et al., 2017; Donahue et al., 2017). Interestingly, it has been shown that
adversarial learning can be used to minimize the variational objective function of VAEs (Makhzani
et al., 2016; Huszar, 2017). Inspired by this observation, various methods such as adversarial Vari-
2
Under review as a conference paper at ICLR 2019
ational Bayes (Mescheder et al., 2017), α-GAN (Rosca et al., 2017), and symmetric-VAE (sVAE)
(Pu et al., 2018) have incorporated adversarial learning into the VAE framework.
Different from this prior work, our Rb-VAE model is a deep generative model specifically designed
to solve the reference-based disentangling problem. During training, adversarial learning is used in
order to minimize a variational objective function inspired by the one employed in sVAE (Pu et al.,
2018). Although sVAE was originally motivated by the limitations of the maximum likelihood cri-
terion used in unsupervised VAE, we show how the variational formulation of sVAE offers specific
advantages in the context of our proposed model.
Learning disentangled representations is a long standing problem in machine learning and com-
puter vision (Bengio et al., 2013). In the literature, we can differentiate three main paradigms to
address it: unsupervised, supervised, and weakly-supervised. Unsupervised models are trained with-
out specific information about the generative factors of interest (Desjardins et al., 2012; Chen et al.,
2016). To address this task, the most common approach consists in imposing different constraints
on the latent representation. For instance, unsupervised VAEs typically define the prior over the
latent variables with a fully-factorized Gaussian distribution. Given that high-level generative fac-
tors are typically independent, this prior encourage their disentanglement in different dimensions of
the latent representation. Based on this observation, different approaches such as β-VAE (Higgins
et al., 2017), DIP-VAE (Kumar et al., 2018), FactorVAE Kim & Mnih (2018) or β-TCVAE (Chen
et al., 2018) have explored more sophisticated regularization mechanisms over the distribution of
inferred latent variables. Although unsupervised approaches are able to identify simple explanatory
variables of visual data, these methods do not allow latent variables to model specific high-level
factors of variation.
A straight-forward approach to overcome this limitation is to use a fully-supervised strategy. In
this scenario, models are learned by using a training set where target factors of interest are explic-
itly labelled. Following this paradigm, we can find different semi-supervised (Kingma et al., 2014;
Narayanaswamy et al., 2017), and conditional (Yan et al., 2016; Pu et al., 2016) variants of autoen-
coders. In spite of the effectiveness of supervised approaches in different applications (Ma et al.,
2017; Pu et al., 2016; Tran et al., 2017), obtaining explicit labels is not feasible in scenarios where
we aim to disentangle a large number of factors or their annotation is difficult.
An intermediate solution between unsupervised and fully-supervised methods are weakly-
supervised approaches. In this case, only implicit information about factors of variation is pro-
vided during training. Several works have explored this strategy by using different forms of weak-
supervision such as: temporal coherence in sequential data (Hsu et al., 2017; Denton et al., 2017;
Villegas et al., 2017), pairing relations between images sharing the same generative factors (Mathieu
et al., 2016; Donahue et al., 2018), grouping information for training samples (Bouchacourt et al.,
2017), or partial knowledge about the rendering process in computer graphics (Yang et al., 2015;
Kulkarni et al., 2015).
Other than previous approaches relying on other forms of weak supervision, our proposed method
addresses the reference-based disentangling problem. In this scenario, the challenge is to exploit
the implicit information provided by a training set of images where the generative factors of interest
are constant, e.g. a set of faces with neutral expression. Note that for this problem, supervised
approaches are not a directly applicable since explicit labels are not available.
3	Preliminaries: Variational autoencoders
Variational autoencoders (VAEs) (Kingma & Welling, 2014) are generative models defining a joint
distribution pθ(x, Z) = pθ(x∣z)p(z), where X is an observation, e.g. an image, and Z is a latent
variable with a simple prior p(z), e.g. a Gaussian with zero mean and identity covariance matrix.
Moreover, pθ(x|z) is typically modeled as a factored Gaussian, whose mean and diagonal covari-
ance matrix are given by a non-linear function of Z, implemented as a deep (convolutional) neural
network. The latter network is referred to as the “decoder”, or “generator”.
Given a training set of i.i.d. samples from an unknown data distribution p(X), the goal of VAEs is
two-fold: (i) To learn the optimal generator parameters θ so that is possible to synthesize samples
following P(X) from the prior p(z), (ii): To approximate the intractable posterior pθ(z|x) in order
to infer latent variables Z from a given observation X. For this purpose, VAEs define a variational
3
Under review as a conference paper at ICLR 2019
distribution qψ(x, Z) = qψ(z∣x)p(x). The approximate posterior qψ(z|x) is defined as another fac-
tored Gaussian, whose mean and diagonal covariance matrix are given as the output ofan “encoder”
or “inference” network with parameters ψ and taking x as an input. During training, the parameters
of the decoder and encoder are learned jointly by solving the following optimization problem:
minEp(X)IKL(qψ(z|x) k P(Z)) - Eqψ(z∣χ) log(pθ(x∣z)) ,	(1)
θ,ψ
which is equivalent to the minimization of the KL divergence between qψ (x, Z) and pθ (x, Z). Note
that the first term can be interpreted as a regularization mechanism encouraging the distribution
qψ (Z|x) to be similar to the prior p(Z). The second term is known as the reconstruction error,
measuring the negative log-likelihood of a generated sample x from its inferred latent variables
qψ (Z|x). The described minimization problem is usually solved by using stochastic gradient descent
(SGD) where p(x) is approximated by the training set. The “re-parametrization trick” (Rezende
et al., 2014) is used to enable gradient back-propagation across samples from qΦ(z|x).
4	Reference-based disentangled representations
Consider a training set of unlabelled images (e.g. human faces) x ∈ RW×H×3 sampled from a
given distribution pu (x). Our goal is to learn a latent variable model defining a joint distribution
over x and latent variables e ∈ RDe and z ∈ RDz . Whereas e is expected to encode information
about a set of generative factors of interest, e.g. facial expressions, z should model the remaining
factors of variation underlying the images, e.g. pose, illumination, age, etc. From now on, we will
refer to e and z as the “target” and “common factors”, respectively. In order to disentangle them,
we are provided with an additional set of reference images sampled from pr (x), representing a
distribution over x where target factors e are constant e.g. neutral faces. Given pr(x) and pu(x),
we define an auxiliary binary variable y ∈ {0, 1} indicating whether an image x has been sampled
from the unlabelled or reference distributions, i.e. p(x|y = 0) = pu(x) and p(x|y = 1) = pr(x).
In reference-based disentangling we aim to exploit the weak-supervision provided by y in order to
effectively disentangle target and common factors.
4.1	Reference-based variational autoencoders
In this section, we present our reference-based variational autoencoder (Rb-VAE): a deep latent
variable model that defines a joint distribution pθ (x, z, e, y) as
Pθ(x, z, e, y) = Pθ(x|z, e)p(z)p(e∣y)p(y),	(2)
where conditional dependencies are designed to explicitly address the reference-based disentan-
gling problem. See Figure 2 for a schematic illustration of our model. We define p(x|z, e)=
L(x∣Gθ(z, e), λ), where Gθ(z, e) is a decoder network that maps a pair of latent variables (z, e) to
an image defining the mean of a Laplace distribution L with fixed scale parameter λ. Note that we
use a Laplace distribution instead of the Gaussian usually employed in the VAEs. The reason is that
the negative log-likelihood is equivalent to the 'ι-loss which is known to encourage sharper image
reconstructions with better visual quality.
To reflect the assumption of constant target factors across reference images, we define the condi-
tional distribution over e given y = 1 as a delta peak centered on a learned vector er ∈ RDe, i.e.
p(e|y = 1) = δ(e - er). In contrast, for y = 0, the conditional distribution is set to a unit Gaussian:
p(e|y = 0) = N (e|0, I). In the following, we denote p(e|y = 0) = p(e). Contrary to the case of
target factors e, the prior over common factors z is equal for reference and unlabelled images, and
taken to be a unit Gaussian y for p(z) = N (z|0, I). Finally, we assume a uniform prior over y, i.e.
p(y = 0) = p(y = 1) = 2.
4.2	Conventional variational learning
Following the VAE framework described in Section 3, we can define a variational distribution
qψ (z, e, x,y) = qψ (z∣x)qψ (e∣x,y)p(x,y), and learn the model parameters θ by minimizing the
KL divergence between qψ and pθ. In this case, note that the conditionals qψ(e∣x,y) and qψ(z|x)
provide a factored approximation of the intractable posterior pθ(e, z|x, y), allowing to infer tar-
get and common factors e and z given an image x. Given a reference image, i.e. with y = 1,
4
Under review as a conference paper at ICLR 2019

(c)	(d)
Figure 2: Overview of the proposed Rb-VAE model. Shaded circles correspond to observed vari-
ables in each scenario. (a) Generative process where pθ(x|z, e) maps latent variables Z (common
factors) and e (target factors) to images x. Reference images are known to be generated by con-
stant er . (b) The decoder is a deep network G with inputs z,e. (c) Encoders map images x to
the corresponding common and target factors z and e respectively. (d) The encoders E and Z are
implemented as deep nets modelling two Gaussians distribution over e and z respectively.
the target factors qψ (e∣x,y = 1) are known to be equal to the reference value er. On the
other hand, given an non-reference image, i.e. with y = 0, we define the approximate posterior
qψ(e∣x,y = 0) = N(e|E*(x), Eσ(x)), where the means and diagonal covariance matrices of a
conditional Gaussian distribution are given by non-linear functions Eμ(x) and Eσ (x). Similarly, We
use an additional network to model qψ(z|x) = N(z∖Zμ(x), Zσ(x)).
Optimization with SGVB: In Appendix (A) we show that the minimization of the KL divergence
between qψ and pθ can be expressed as:
θminr Epu(X) ∣KL(qψ(z∖x)qψ(e∖x) k p(z)p(e)) — Eqψ(z∣χ)qψ(e∣x) log(pθ(x∖z, e))]
+ Epr(X)IKL(qψ(z∖x) k p(z)) — Eqψ(z∣χ) log(pθ(x∖z, er))],	(3)
where the second and fourth terms of the expression correspond to the reconstruction errors for
unlabelled and reference images respectively. Note that for reference images, no inference over
target factors e is needed. Instead, the generator reconstructs them using the optimized parameter
er. Similar to standard VAEs, the remaining terms consist ofKL divergences between approximate
posteriors and priors over the latent variables. The minimization problem defined in Eq. (3) can
be solved using SGD and the re-parametrization trick in order to back-propagate the gradient when
sampling from qψ(e∖x) and qψ(z∖x).
4.3	Variational Learning with Symmetric KL divergence
The main limitation of the variational objective defined in Eq. (3) is that it does not guarantee that
common and target factors will be effectively disentangled in z and e respectively. In order to
understand this phenomenon, it is necessary to analyze the role of the conditional prior p(e∖y) in
Rb-VAEs. By defining p(e∖y = 1) as a delta function, the model is forced to encode into z all
the generative factors of reference images, given that they must be reconstructed via pθ(x∖z, er)
with constant er. Therefore, p(e∖y) is implicitly encouraging qψ(z∖x) to encode common factors
present in reference and unlabelled samples. However, this mechanism does not avoid the scenario
where target factors are also encoded into latent variables z. More formally, the minimization of (3)
does not prevent a degenerate solution pθ (x∖z, e) = pθ(x∖z), where the inferred latent variables by
qψ (e∖x) are uninformative and the decoder ignores them.
In order to address this limitation, we propose to optimize an alternative variational expression
inspired by unsupervised Symmetric VAEs (Pu et al., 2018). Specifically, we add into the minimized
objective the reversed KL between qψ and pθ as:
min	KL(qψ (z,	e, x, y)	k pθ(x,	z, e, y)) + KL(pθ(x, z, e,	y)	k	qψ (z,	e, x,	y)),	(4)
θ,ψ
5
Under review as a conference paper at ICLR 2019
In order to understand why this additional term allows to mitigate the degenerate solution
Pθ(x|z, e) = pθ(x|z), it is necessary to observe that its minimization is equivalent to:
min Ep(z,e) IKL(pθ(x|z, e) ∣∣ Pu(X)) - Ep0(x|z,e)[log(qψ(z∣x)) + log(qψ(e|x))]]
+ Ep(Z)Pθ(x∣z,er) [kl(pθ(x|z, er) k Pr(x)) — log(qψ(z|x))],	(5)
see Appendix A for a detailed derivation. In the defined expression, the two KL divergences encour-
age images generated using P(z), P(e) and er to be similar to samples from the real distributions
Pr (x) and Pu (x). On the other hand, the remaining terms correspond to reconstruction errors over
latent variables z, e inferred from generated images drawn from Pθ. As a consequence, the mini-
mization of these errors is encouraging the decoder Pθ (x|z, e) to generate images x by taking into
account latent variables e, since the latter must be reconstructed via qψ(e|x). In conclusion, the
addition of the reversed KL in the objective avoids the degenerate solution that ignores variables e.
Optimization via Adversarial Learning. Given the introduction of the reversed KL divergence,
the learning procedure described in Section 4.2 can not be directly applied to the minimization of
Eq. (4). However, note that the objective function defined in Eq. (4) is equivalent to:
qψ (e,z∣x)pu(x)	Pθ (x|e, z)p(z, e 八
Eqψ(e,z|X)Pu(X) log (pθ(x|e, z)p(z, e)) + Epθ3e,Z)P(ZQ log《(e,z∣x)pu(x) ) +
qψ (z∣x)p(x)r	Pθ(x∣er, z)p(z))
Eqψ(ZIX)Pr(X) log (pθ(x|er, Z)P(Z)) + Epθ3er,Z)P(Z) log ( qψ (z∣x)pr(x) J ,	⑹
corresponding to the minimization of: (i) A difference of two expectations over the log-density
ratio of qψ(e, z∣x)pu(x) and p§(x|e, z)p(z)p(e). (ii) An analogous expression for distributions
qψ(z∣x)pr (x) andp§(x∣er, z)p(z)). By approximating both density ratios with two auxiliary func-
tions:
qψ(e,z∣x)pu(x)	qψ(z∣x)pr(x) '
ξ (x,z, e,=* (pθ (x∣e,z)p(z)p(ej , γ (X,z)=og (p« (x∣er ,z)p(z))-	()
we can express the minimization problem in Eq. (6) as:
min Eqψ (e,Z|X)Pu (X) dξ (x, z, e) - EPθ (X|e,Z)P(Z)P(e) dξ (x, z, e)
θ,ψ
+ Eqψ (Z|X)Pr (X) dγ (x, z) - EPθ (X|er,Z)P(Z) dγ (x, z)	(8)
which can be solved with SGD. Concretely, we can evaluate functions dξ(x, z, e), dγ (x, z) and
back-propagate the gradients w.r.t parameters ψ and θ by using the re-parametrization trick over
samples of x,e and z.
The main challenge of approximating the log-density ratio using the functions defined in Eq. (7),
is that parameters ξ and γ are not known and must be also optimized. Fortunately, the log-ratio
between two distributions can be estimated by using logistic regression (Bickel et al., 2009). For
instance, given fixed encoder and generator parameters θ and ψ, we can optimize ξ by solving:
max Epθ(x∣Z,e)P(Z,e) log(σ(dξ(x, z, e)) + Eqψ(©声反酸的 log(1 — σ(dξ(x, z, e)),	(9)
where σ(∙) refers to the sigmoid function. A similar strategy can be applied to optimize parameters
of dγ (x, z). This approach is analogous to adversarial unsupervised methods such as ALI (Du-
moulin et al., 2017), where the function dγ(∙) acts as a discriminator trying to distinguish whether
pairs of reference images x and latent variables z generated by qψ and pθ . However, in our case we
have an additional discriminator dξ operating over unlabelled images and its corresponding latent
variables z and e. Interestingly, note that dξ encourages the distribution qψ(z, e|x) to be similar to
the prior p(z, e). Therefore, it forces representations z and e to be conditionally independent. This
is especially interesting in our context since we assume that target and common factors are uncor-
related. To conclude, it is also interesting to observe that the discriminator dγ (x, z) is implicitly
encouraging latent variables z to encode only information about the common factors. The reason
is that samples generated from pθ(x|z, er)p(z) are forced to be similar to reference images. As a
consequence, z can not contain information about target factors, which must be encoded into e.
6
Under review as a conference paper at ICLR 2019
Using previous definitions, we use an adversarial procedure where model and discriminators param-
eters (θ,ψ), and (ξ,γ) are simultaneously optimized by minimizing and maximizing Eqs. (8) and
(9) respectively. The algorithm used to process one batch during SGD is shown in Appendix B. In
Rb-VAEs, the discriminators dγ(∙) and dξ(∙) are also implemented as deep convolutional networks.
Explicit log-likelihood maximization. As shown in Eqs. (3) and (5), the minimization of the sym-
metric KL divergence encourages low reconstruction errors for images and inferred latent variables.
However, by using the proposed adversarial learning procedure, the minimization of these terms be-
comes implicit. As shown in (Dumoulin et al., 2017; Donahue et al., 2017), this can cause original
samples to differ substantially from their corresponding reconstructions. In order to address this
drawback, we use a similar strategy as (Pu et al., 2018; Li et al., 2017), and add the reconstruction
terms:
Eqψ (e∣x)qψ (e∣x)pu(x) Iog(Pθ (X|z, e)) + Eqψ(z∣x)pr (x) Iog(Pθ (X|z, e )) +
Epθ(x∣z,e)p(z)p(e)[lθg(qψ (z∣x)) + log(qψ (e|x))] + Ep0(x|z,er )p(z) log(qψ (z∣x))	(1。)
into the learning objective, minimizing them together with Eq. (8). In preliminary experiments, we
found that the explicit addition of these reconstructions terms during learning is important to achieve
low reconstruction errors and increase stability during the adversarial training procedure. Figure 5
in the Appendix illustrates all the different losses minimized during learning in sRBD-VAE.
5	Experiments
5.1	Datasets
To validate our approach and to compare to existing work, we consider two different reference-based
disentangling problems.
Digit style disentangling: The goal is to model style variations from hand-written digits. Con-
cretely, we consider the digit style as a set of three different properties: scale, width and color. In
order to address this task from a reference-based perspective, we consider half of the original training
images in the MNIST dataset (LeCun et al., 1998) as our reference distribution (30k examples). The
unlabelled set is synthetically generated by applying different transformations over the remaining
half of images: (1) Simulation of stroke widths by using a dilation with a given filter size; (2) Digit
colorization by multiplying the RGB components of each pixel by a 3D RGB color vector; (3) Size
variations by down-scaling the image by a given factor and applying zero-padding to recover the
original resolution. We randomly transform each image twice to obtain a total of 60k unsuperivsed
images. More details can be found in Appendix C.
Facial expression disentangling: We consider disentangling of facial expressions by using a refer-
ence set of neutral faces. For the unlabelled dataset we use a subset of the AffectNet dataset (Mol-
lahosseini et al., 2017), which contains a large quantity of facial images. This dataset is especially
challenging since images were collected “in the wild” and exhibit a large variety of natural ex-
pressions. A subset of the images are annotated according to different basic facial expressions:
happiness, sadness, surprise, fear, disgust, anger, and contempt. We use these labels for quantitative
evaluation. Given that we found that many neutral images in the original database were not cor-
rectly annotated, we collected a separate reference set, see Appendix C for details. The unlabelled
and reference sets consist of 150k and 10k images, respectively.
5.2	Experimental setup
Baselines: We compare the two different variants of our proposed method: Rb-VAE, trained using
the standard variational objective (Section 4.2), and sRb-VAE, learned by minimizing the symmet-
ric KL divergence (Section 4.3). To demonstrate the advantages of exploiting the weak-supervision
provided by reference images, we compare both methods with various state-of-the-art unsupervised
approaches based on the VAE framework: β-VAE (Higgins et al., 2017), β-TCVAE (Chen et al.,
2018) sVAE (Pu et al., 2018), DIP-VAE-I and DIP-VAE-II (Kumar et al., 2018). Note that β-VAE
DIP-VAE and have β-TCVAE been specifically proposed for learning disentangled representations,
showing better performance than other unsupervised methods such as InfoGAN (Chen et al., 2016).
On the other hand, sVAE is trained using a similar variational objective as sRb-VAE, and can there-
fore be considered an unsupervised version of our method. We also use standard VAE (Kingma &
7
Under review as a conference paper at ICLR 2019
	AffectNet								I	MNIST					
	Happ	Sad	Sur	Fear	DiSg	Ang	Compt	Avg.	I R	G	B	Scale	Width I	Avg.
VAE	.554	.279	.383	.357	.256	.415	-.439-	.383	.099	.104	.101	[.034]	.085	.085
DIP-VAE-I	.561	.269	[.401]	367	.258	.397	-.463-	.388	I [.055]	.064	.063	.038	.100 I	.064
DIP-VAE-H	.548	.245	[.401]	[389]	.268	.391	-.463-	.386	.077	.069	.076	.035	.098	.071
β VAE	.581	.283	.373	.323	.250	.415	-.467-	.384	I .093	.099	.094	.039	.089 I	.083
SVAE	.583	.251	.389	.349	.260	.391	.469	.384	.094	.092	.084	.036	.104	.082
β-TCVAE	.563	.277	.393	.349	.256	[.427]	.467	.390	.098	.100	.099	[.034]	[.084]	.083
[Mathieu et. al,2016]	.567	.388	.312	.330	.295	.353	[.512]	395	.116	.116	.114	.039	.104	.098
RBD-VAE	.536	393	.379	.311	.320	.383	-.421-	392	I .065	.069	.062	.061	.095 I	.070
SRBD-VAE	[.587]	[.405]	.387	.327	[.344]	.425	-.483-	T.422T	.057	[.053]	[.055]	.038	.095	[.060]
Table 1: Prediction of target factors from learned representations. For Rb-VAE and sRb-VAE only
the latent variables e are used as feature vectors. All the latent variables are employed for unsuper-
vised models. We report accuracy and Mean Absolute Error as evaluation metrics in the AffectNet
and MNIST datasets respectively. Two best methods shown in bold, best result in brackets.
Welling, 2014) as an additional baseline. Finally, in order to evaluate an alternative approach able
to exploit the reference-set, we have implemented (Mathieu et al., 2016) adapting it to our context.
Note that the amount of supervision used in the cited method is larger than the available in reference-
based disentangling. Concretely, we do not have information about unlabelled samples sharing the
same target factors (e.g the same expression). In contrast, this information is only available for ref-
erence images. This fact renders the original learning algorithm proposed in (Mathieu et al., 2016)
inapplicable. However, we have modified it in order to use only pairing information from reference
images by removing the reconstruction losses assuming known pairs for unlabelled samples.
Implementation details: The different components of our method are implemented as deep neural
networks. For this purpose, we have used an architecture based on the main building blocks em-
ployed by (Karras et al., 2018). More concretely, generator networks are implemented as a sequence
of convolutions, Leaky-ReLU non-linearities, and nearest-neighbour up-sampling operations. En-
coders and discriminators follow a similar architecture, using average pooling for down-sampling.
Channel normalization is used for generators and encoders. See Appendix D for more details on the
precise architecture. Our code will be made available online upon publication. For a fair compar-
ison, we have developed our own implementation for all the evaluated methods in order to use the
same network architectures and hyper-parameters. During optimization, SGD is employed using the
Adam optimizer (Kingma & Ba, 2015) (with α = 10-4, β1 = 0.5, β2 = 0.99, = 10-8), and a
batch size of 36 images. For the MNIST and AffectNet databases, the models are learned for 30 and
20 epochs respectively. The number of latent variables for the encoders has been set to 32 for all the
experiments and models. The λ parameter in the Laplace distribution is set to 0.01.
5.3	Quantitative evaluation: Feature Learning
A common strategy to evaluate the quality of learned representations is to measure the amount
of information that they convey about the underlying generative factors (Eastwood & Williams,
2018). In the context of reference-based disentangling, we are interested in modelling the target
factors that are constant in the reference distribution. In this experiment, we use this observation
in order to quantitatively evaluate our method. For this purpose, we use the learned representations
as feature vectors and train a low-capacity model estimating the target factors involved in each
problem. Concretely, in the MNIST dataset we employ a set of linear-regressors predicting the
scale, width and color parameters for each digit. To predict the different expression classes in the
AffectNet dataset, we use a linear classifier. For evaluation, we split each dataset in three subsets.
The first is used to learn each generative model. Then, the second is used for training the regressors
or classifier. The third is used to evaluate the predictions in terms of the mean absolute error and
per-class accuracy for the MNIST and AffectNet datasets, respectively. In MNIST, the second and
third subset have been randomly generated from the original MNIST test set using the procedure
described in Section 5.1 (5k images each). For AffectNet, we randomly select 500 images for each
of the seven expressions from the original dataset, yielding 3,500 images per fold.
Table 1 shows the results obtained by the unsupervised models considered 1 and the proposed Rb-
VAE and sRb-VAE. For Rb-VAE and sRb-VAE, we used the inferred latent variables e as features
1For DIP-VAE-I, DIP-VAE-II, β-VAE and β-TCVAE we tested different regularization parameters in the
range [1, 50], and report the best results.
8
Under review as a conference paper at ICLR 2019
sRb-VAE
Rb-VAE
Figure 3: Conditional image synthesis for MNIST (top) and AffectNet (bottom) using sRb-VAE and
Rb-VAE. Within each column images are generated using the same random target factors e.
since the distribution over these variables is expected to encode the information regarding the tar-
get factors. For the unsupervised models we use all the latent variables. By analyzing the results,
we obtain the following conclusions. First, the unsupervised approach DIP-VAE-I achieves better
average results than Rb-VAE for MNIST. Moreover, for facial expression recognition, β-TCVAE
achieves comparable or better performance in several cases. This may seem counter-intuitive be-
cause, unlike Rb-VAE, DIP-VAE-I is trained without the weak-supervision provided by reference
images. However, it confirms our hypothesis that the learning objective of Rb-VAE does not explic-
itly encourage the disentanglement between target and common factors. In contrast, we can see that
sRb-VAE obtains comparable or better results than rest of the methods in most cases. Moreover,
it achieves the best average performance in both datasets. This demonstrates that the information
provided by the reference distribution is effectively exploited by the symmetric KL objective used
to train sRb-VAE. In order to further validate the ability of the proposed method to disentangle the
target factors, we have followed the same evaluation protocol for Rb-VAE and sRb-VAE but consid-
ering the latent variables z as features. The average performance obtained by Rb-VAE is .349 and
.195 for AffectNet and MNIST respectively. On the other hand, sRb-VAE achieves .335 and .189.
Note that for both methods these results are significantly worse compared to using e as a represen-
tation in Table 1. This suggests that latent variables z are mainly modelling the common factors
between reference and unlabelled images. The qualitative results presented in the next section con-
firm this. To conclude, note that sRBD-VAE also obtains better performance than (Mathieu et al.,
2016) in both data-sets. So even though this method also uses reference images during training, our
results confirms that sRBD-VAE better exploits the weak-supervision provided in reference-based
disentangling.
5.4	Qualitative evaluation
In contrast to unsupervised approaches, reference images can be used by our model in order to
explicitly encode the target and common factors into two different subsets of latent variables. This
directly enables tasks such as conditional image synthesis or attribute transfer. In this section, we
illustrate the potential applications of our proposed model in this type of applications.
Conditional image synthesis. The goal is to transform real images by modifying only the target
factors e. For instance, given a face of an individual, we aim to generate images of the same subject
exhibiting different facial expressions. For this purpose, we use our model in order to infer the
common factors z. Then, We sample a vector e 〜N(0,1) and use the generator network to obtain
a new image from e and z. In Fig. 3 we show examples of samples generated by Rb-VAE and sRb-
VAE following this procedure. As we can observe, sRb-VAE generates more convincing results
than its non-symmetric counterpart. In the AffectNet database, the amount of variability in Rb-VAE
9
Under review as a conference paper at ICLR 2019
⅛NpθN
JSi
Figure 4: Transferring target factors e from image A to an image B on the AffectNet (expression)
and MNIST (style) using sRb-VAE and Rb-VAE.
samples is quite low. In contrast, sRb-VAE is able to generate more diverse expressions related
with eyes, mouth and eyebrows movements. Looking at the MNIST samples, we can draw similar
conclusions. Whereas both methods generate transformations related with the digit color, Rb-VAE
does not model scale variations in e, while sRb-VAE does. This observation is coherent with results
reported in Table 1, where Rb-VAE offers a poor estimation of the scale.
Visual attribute transfer. Here we transfer target factors e between a pair of images A and B.
For example, given two samples from the MNIST dataset, the goal is to generate a new image with
the number in A modified with the style in B. Using our model, this can be easily achieved by
synthesizing a new image from latent variables e and z inferred from A and B respectively. Figure 4
shows images generated by sRb-VAE and Rb-VAE in this scenario. In this case, we can draw similar
conclusions than the previous experiment. Rb-VAE is not able to swap target factors related with
the digit scale in the MNIST dataset, unlike sRb-VAE which better model this variation factor. On
the AffectNet images, both methods are able to keep most of the information regarding the identity
of the subject, but again Rb-VAE leads to weaker expression changes than sRb-VAE.
These qualitative results demonstrate that the standard variational objective of VAE is sub-optimal to
train our model, and that the symmetric KL divergence objective used in sRb-VAE allows to better
disentangle the common and target factors.
6	Conclusions
In this paper we have introduced reference-based disentangling, a novel weakly supervised learn-
ing problem, and proposed reference-based variational autoencoder models to address it. We have
shown that the standard variational learning objective used to train VAE can lead to degenerate so-
lutions when it is applied in our setting, and proposed an alternative training strategy that exploits
adversarial learning. Comparing the proposed model with previous state-of-the-art unsupervised
approaches, we have demonstrated its ability to learn disentangled representations from minimal
supervision, and its potential application to tasks such as feature learning, conditional image gener-
ation, and attribute transfer.
10
Under review as a conference paper at ICLR 2019
References
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. PAMI, 2013.
Steffen Bickel, Michael Bruckner, and Tobias Scheffer. Discriminative learning for differing training
and test distributions. In ICML, 2009.
Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-level variational autoen-
coder: Learning disentangled representations from grouped observations. arXiv preprint
arXiv:1705.08841, 2017.
Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentangle-
ment in variational autoencoders. ICLR Workshops, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
NIPS, 2016.
Emily L Denton et al. Unsupervised learning of disentangled representations from video. In NIPS,
2017.
Guillaume Desjardins, Aaron Courville, and Yoshua Bengio. Disentangling factors of variation via
generative entangling. ICML, 2012.
Chris Donahue, Zachary C Lipton, Akshay Balsubramani, and Julian McAuley. Semantically de-
composing the latent spaces of generative adversarial networks. ICLR, 2018.
Jeff Donahue, PhiliPP KrahenbuhL and Trevor Darrell. Adversarial feature learning. ICLR, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Ar-
jovsky, and Aaron Courville. Adversarially learned inference. ICLR, 2017.
Cian Eastwood and ChristoPher KI Williams. A framework for the quantitative evaluation of disen-
tangled rePresentations. ICLR, 2018.
Paul Ekman and Erika L Rosenberg. What the face reveals: Basic and applied studies of sponta-
neous expression using the Facial Action Coding System (FACS). Oxford University Press, USA,
1997.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Irina Higgins, Loic Matthey, Arka Pal, ChristoPher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-vae: Learning basic visual concePts with a
constrained variational framework. ICLR, 2017.
Wei-Ning Hsu, Yu Zhang, and James Glass. UnsuPervised learning of disentangled and interPretable
rePresentations from sequential data. In NIPS, 2017.
Ferenc Huszar. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235,
2017.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
Proved quality, stability, and variation. ICLR, 2018.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. ICML, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. ICLR, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-suPervised
learning with deeP generative models. In NIPS, 2014.
11
Under review as a conference paper at ICLR 2019
Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional
inverse graphics network. In NIPS, 2015.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentan-
gled latent concepts from unlabeled observations. ICLR, 2018.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. AUtoen-
coding beyond pixels using a learned similarity metric. ICML, 2015.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 1998.
Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun Chen, Ricardo Henao, and Lawrence
Carin. Alice: Towards understanding adversarial learning for joint distribution matching. In
NIPS, 2017.
Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, and Mario Fritz. Dis-
entangled person image generation. In ICCV, 2017.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. ICLR, 2016.
Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann
LeCun. Disentangling factors of variation in deep representation using adversarial training. In
NIPS, 2016.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. ICML, 2017.
Ali Mollahosseini, Behzad Hasani, and Mohammad H Mahoor. Affectnet: A database for facial ex-
pression, valence, and arousal computing in the wild. IEEE Transactions on Affective Computing,
2017.
Siddharth Narayanaswamy, T Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, Noah
Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr. Learning disentangled representations
with semi-supervised deep generative models. In NIPS, 2017.
Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence
Carin. Variational autoencoder for deep learning of images, labels and captions. In NIPS, 2016.
Yunchen Pu, Liqun Chen, Shuyang Dai, Weiyao Wang, Chunyuan Li, and Lawrence Carin. Sym-
metric variational autoencoder and connections to adversarial learning. AISTATS, 2018.
D. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference
in deep generative models. In ICML, 2014.
Salah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent, and Mehdi Mirza. Disentangling
factors of variation for facial expression recognition. ECCV, 2012.
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational
approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987,
2017.
Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled representation learning gan for pose-invariant
face recognition. In CVPR, 2017.
Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Condi-
tional image generation with pixelcnn decoders. In NIPS, 2016.
Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing motion
and content for natural video sequence prediction. ICLR, 2017.
Xuehan Xiong and Fernando De la Torre. Supervised descent method and its applications to face
alignment. In CVPR, 2013.
12
Under review as a conference paper at ICLR 2019
Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image
generation from visual attributes. In ECCV. Springer, 2016.
Jimei Yang, Scott E Reed, Ming-Hsuan Yang, and Honglak Lee. Weakly-supervised disentangling
with recurrent transformations for 3d view synthesis. In NIPS, 2015.
13
Under review as a conference paper at ICLR 2019
Appendix A Mathematical derivations
Equivalence between KL(qψ (z, e, x, y) k pθ (x, z, e, y)) and Eq. (3):
qψ(z, e|x, y)p(x|y)p(y)
与 Jx,e,z qψ(Tx, y)qψ(ZIX)P(XIy)P⑷ log (pθ(x|e, z)p(z)p(e∣y)p(y) dxdzde	(11)
y∈[0,1]
=1 Z	qψ (e∣x)qψ (ZIX)Pu(X)log (qψ(elχ)qψ (ZX)PJX)! dxdzde
2 x,e,z	Pθ (XIe, z)P(z)P(e)
+ 1 / qψ(z∣X)pr(x) log q qψ(ZIX)P(X)) )dXdz	(12)
2 x,z	Pθ(XIer,Z)P(Z)
=1 Epu(X)Eqψ(e∣χ)qψ (z|x) log (qψ (e(X)PψeZlX)) - log(Pθ (X∣e, Z)) - H u(X)
+ 2Epr(x)Eqψ(z∣x) log (qψp((ZIX)) - log(Pθ(X∣er, z)) - Hr(x)	(13)
=2Epu(X) KL(qψ(ZIx)qψ(eIx) ∣∣ p(Z)p(e)) - Eqψ包氏”他但乂)log(pθ(x[z, e))
+ 1 Epr(X) KL(qψ(zIx) k p(z)) - Eqψ(z∣χ) log(pθ(xIz, er)) - Hr(x) - Hu(x)
We denote Hr (X) and Hu (X) as the entropy of the reference and unlabelled distributions Pr (X)
and Pu (X) respectively. Note that they can be ignored during the minimization since are constant
w.r.t. parameters θ and ψ. For the second equality, we have used the definitions P(XIy = 0) =
pu(x), p(xIy = 1) = pr (x) and assumed p(y = 0) = p(y = 1) = ɪ. Moreover, We have
exploited the fact that qψ(eIX, y = 1) and P(eIy = 1) are defined as delta functions and, therefore,
Ep(e∣y=i)log( qψ(W)) = 0. Wedenote p(e1y = 0) = p(e) and qψ (e1x,y = 0) = qψ (e1x) for the
sake of brevity.
Equivalence between KL(Pθ (x, z, e, y) ∣ qψ (z, e, x, y)) and the expression in Eq. (5)
X
y∈[0,1] x,e,z
Pθ(xIe, z)P(z)P(eIy)P(y) log
Pθ (xIe,z)p(z)p(eIy)p(y)
qψ(z, e, x,y)p(XIy)P(y)
dxdzde
(14)
2 I pθ(xIe,
2 x,e,z
z)P(z)P(e) log
Pθ (xIe,z)P(z)P(e)
qψ (eIx)qψ (ZIX)PU(X)
dxdzde
+ 1/ Pθ(xIer, z)p(z) log
Pθ (xIer ,z)p(z)
qψ (ZIX)Pu(X)
dxdzde
(15)
2 Ep(Z)p(e)Epθ (x∣e,z)
log 心(xIe，Z)
g	P(x)u
- log(qψ (eIx)qψ (zIx))
+ 2Ep(Z)Epθ(x∣er,z) log (，[([)；))- log(qψ (ZIX)) - H(z) - 2H(e)
=2Ep(z)p(e) IKL(Pθ(xIz,e) k Pu(x)) - Epθ(χ∣z,e)[log(qψ(ZIX)) +log(qψ(eIx))]i
2Ep(z) ∣KL(Pθ(xIz, er) k Pr(x)) - Epθ(χ∣z,er) log(qψ (ZIx))] - H(z) - 1 H(e)
(16)
(17)
14
Under review as a conference paper at ICLR 2019
We have used the same definitions and assumptions previously discussed. Moreover, we denote
H(z) and H(e) as the entropy of the priors p(z) and p(e). Again, we can ignore these terms when
we are optimizing w.r.t parameters ψ and θ.
Equivalence between the minimization of the symmetric KL divergence in Eq. (4) and the
expression in Eq. (6)
KL(qψ(z, e, x, y) k pθ(x, z, e, y)) + KL(pθ(x, z, e, y) k qψ(z, e, x, y)) =	(18)
Eqψ(elx,y)qψ(ZIx)P(XIyMy) log
qψ (e∣x,y)qψ (z∣x)p(x∣y)p(y)
Pθ (x∣e,z)p(z)p(e∣y)p(y)
+ Epθ (xIe,Z)p(Z)p(eIy)p(y) log
Pθ(X|e, Z)P(Z)P(eIy)P(y)
qψ (e∣x,y)qψ (z|x)P(x|y)P(y)
(19)
1
2
E	l	qψ(e,z|X)Pu(X) + E	l	qψ (ZIX)P(X)r
Eqψψ(e，z|x)Pu(X) log Pθ(x∣e, Z)P(Z)P(e) + Eqψ(ZIx)Pr(X) log 「©(x|e「, Z)P(Z)
+Epθ3e,Z)P(Z)P(C) log (Pqx⅛z⅛Pu(Xe)) + Ep"x1er,Z)P(Z) log (PqψxZex)Z?)Px))))[ (20)
Appendix B Pseudo-code for adversarial learning procedure
Alg. 1 shows pseudo-code for the adversarial learning algorithm described in Sec. 4.3 of the paper.
Fig. 5 illustrates all the different losses minimized during learning in sRBD-VAE.
Algorithm 1 SRb-VAE Advesarial Learning (Batch processing during SGD)
1:	/*** Gradient φ ***/
2:	Sample {x1 , ..., xM } from Pu (x)
3:	Sample {xr1 , ..., xrM } from Pr (x)
4:	Sample {e1, ..., eM} using qφ (eIx)
5:	Sample {Z1 , ..., ZM} using qφ(ZIx)
6:	Sample {Zr1 , ..., ZrM} using qφ(ZIxr)
7:	compute gradient of eq. (8) w.r.t ψ using the reparametrization
trick for stochastic variables Z, e and Zr:
gφ 一弋 " m h XX dξ(xm , Zm , em )
m
+ dγ (xrm , Zrm )i
8:	/*** Gradient θ ***/
9:	Sample {eι, ..., Cm} fromP(e)
10:	Sample {^ι,..., ZM} fromP(Z)
11:	Sample {z；,...,ZM } from P(Z)
12:	Sample {^ι ,…，Xm} using pθ(x∣Z, e)
13:	Sample {^1,..., xM} using pθ(x∣Z, er)
14:	compute gradient of eq. (8) w.r.t θ using the reparametrization
trick for stochastic variables x and xr:
gθ J Vθm [X dξ(xm , Zm , em )
m
+ dγ (xm, zm)i
15:	/*** Gradient ξ ***/
16:	compute gradient of discriminator function (eq. (9)) w.r.t ξ:
gξ J Vξ ɪ X h log(σ(dξ(xm, Zm, em)) +
m
lθg(1 — σ(dξ(^m, Zm, Cm))]
17:	/*** Gradient γ ***/
18:	compute gradient of discriminator function (eq. (9)) w.r.t γ:
gγJ vy 21m X h ιog(σ(dγ (xm, Zm))+
m
log(ι — σ(dγ (xm, zm)i
19:	/*** Update Parameters ***/
20:	update parameters via sgd with learning rate λ:
θ J θ + λgθ
ψ J ψ - λgψ
ξJξ+λgξ
γ J γ+ λgγ
=0
Appendix C	Datasets
Examples of reference and unlabelled images for MNIST and AffectNet are shown in Fig. 6. Fol-
lowing, we provide more information about the used datasets.
15
Under review as a conference paper at ICLR 2019
Common factors ( Z )	Target factors ( e )	Target reference factors ( er )
segamI dellebaln
Loss
-->i-iog(⅜(θ∣χ)⅛---i
(Ioλ∕v 2 9 (IɛN ?z
ι -►： - IOg(QS(ZlX) y-j
segamI ecnerefeR
(e)

Figure 5: Illustration of the different losses minimized during learning in sRBD-VAE. (a) Model en-
coders and generator are trained to minimize the divergence between distributions qψ(z, e|x)pu(x)
andpθ(x|e, z)p(z)p(e). The discriminator dξ(x, z, e) measures the log-density ratio between both
distributions. (b) A similar loss is minimized for reference images using an additional discrimina-
tor dγ (x, z) (c,d) Encoder and generator aim to reconstruct unlabelled and reference images from
their inferred latent variables. Note that for reference images, no inference over constant factors e
is needed. (e,f) For unlabelled and reference images, sRDB-VAE also minimizes the reconstruction
error over latent variables of images generated using p(z), p(e) and er
C.1 MNIST
We use slightly modified version of the MNIST images: the size is increased to 64 × 64 pixels and an
edge detection procedure is applied to keep only the boundaries of the digit. We obtain the samples
in the unlabelled dataset by applying the following transformations over the MNIST images:
1. Width: Generate a random integer in the range {1, . . . , 10} using a uniform distribution.
Apply a dilation operation over the image using a squared kernel with pixel-size equal to
the generated number.
2. Color: Generate a random 3D vector c ∈ [0, 1]1 2 3 using a uniform distribution. Normalize
the resulting vector as c = c∕∣∣c∣∣ι. Multiply the RGB components of all the pixels in the
image by c.
3. Size: Generate a random number in the range [0.5, 1] using a uniform distribution. Down-
scale the image by a factor equal to the generated number. Apply zero-padding to the
resulting image in order to recover the original resolution.
The data and code to generate it will be made available online upon publication.
C.2 AffectNet
Reference set collection. We collected a reference set of face images with neutral expression. We
applied specific queries in order to obtain a large amount of faces from image search engines. Then,
five different annotators filtered them in order to keep only images showing a neutral expression.
This was motivated because we found that many neutral images in the AffectNet dataset Mollahos-
seini et al. (2017) were not accurate. As detailed in the original paper, each image was only labelled
by a single subject. As a consequence, the agreement between annotators was shown to be low for
neutral images. In contrast, in our reference-set, each image was annotated in terms of “neutral”
/ “non-neutral” by two different annotators. In order to ensure a higher label quality compared to
16
Under review as a conference paper at ICLR 2019
⅛NP9 N
Figure 6: Examples of reference and unlabelled images used in our experiments. Extracted from
MNIST (top) and AffectNet (bottom) databases.
the AffectNet, only the images where both annotators agreed were added to the reference-set. The
collected dataset will be made available upon publication.
Pre-processing. In order to remove 2D affine transformations such as scaling or in-plane rotations,
we apply an alignment process to the face images. We localize facial landmarks using (Xiong &
De la Torre, 2013). Then, we apply Procrustes analysis in order to find an affine transformation
aligning the detected landmarks with a mean shape. Finally, we apply the transformation to the
image and crop it. The resulting image is then re-sized to a resolution of 96 × 96 pixels.
Appendix D	Network architectures
Fig. 7 illustrates the network architectures used in our experiments. CN refers to pixel-wise nor-
malization as described in (Karras et al., 2018). FC defines a fully-connected layer. For Leaky
ReLU non-linearities, we have used an slope of 0.2. Given that we normalize the images in the
range [-1, 1], we use an hyperbolic tangent function as the last layer of the generator. For the dis-
criminator dγ (x, z), we use the same architecture showed for dξ (x, z, e) but removing the input
corresponding to e. In preliminary experiments, we found that the discriminator in sRb-VAE can
start to ignore the inputs corresponding to latent variables e and z while focusing only on real and
generated images. In order to mitigate this problem during training, we found it effective to ran-
domly set to zero the inputs corresponding to latent variables and images of the last fully-connected
layer. In particular, we use a probability of 0.25.
Appendix E	Additional results
Figures 8 and 9 show additional qualitative results for conditional image generation and visual at-
tribute transfer, in the same spirit as the ones in 5.4. The additional results further support the
conclusions drawn in the main paper. In Fig. 10, we also show additional images generated by
sRB-VAE trained with the AffectNet dataset. Different from the previous cases, these images have
been generated by injecting random noise over both latent variables e and z. Note that different
target factors e generate similar expressions in images generated from different common factors z.
17
Under review as a conference paper at ICLR 2019
q,.(Z x)z⅛(e∣x)
W X H RGB Image
3 X 3 X 32 conv - CN -IeakyReLU ∣
Avg.Pool 2x2
-3 X 3 X 32 conv - CN - IeakyReLU |
: ▼ 二
-3 X 3 X 64 conv - CN - IeakyReLU I
Avg.Pool 2x2
-3 X 3 X 64 conv - CN - IeakyReLU |
: ▼
I 3 X 3 X 128 conv - CN - IeakyReLU |
: ▼	:
Avg.Pool 2x2
:	一V	二
—3 X 3 X 128 conv - CN - IeakyReLU |
I 3 X 3 X 256 conv - CN - IeakyReLU |
Avg.Pool 2x2
—3 X 3 X 256 COnV- CN - IeakyReLUl
*	⅜~
W/16 X H/16 X 32 FCl ∣ W/16 X H/16 X 32 FC |
M	σ
W(XlZ,e)
Z	32 features | |	32 features1
I *	Wl
concatenation
丁
W/16 X H/16 X 256 COnvt - CN -IeakyReLU~∣
NN UpsampIing x2
3 X 3 X 256 conv - CN - IeakyReLU
V
3 X 3 X 128 conv - CN - IeakyReLU I
NN UpsampIing x2
3 X 3 X 128 conv - CN - IeakyReLU
3 X 3 X 64 conv - CN - IeakyReLU
NN UpsampIing x2
3 x 3 x 64 conv - CN - IeakyReLU
3 x 3 x 32 conv - CN - IeakyReLU
NN UpsampIing x2
3 x 3 x 3 conv - Tanh
W x H RGB Images
Figure 7: Network architectures used in our experiments
18
Under review as a conference paper at ICLR 2019
Synthetic variations
虱
Q
久
Q
Ref. Image
W
Q
<
又


久



Q
Q
<
Q
Q
Q
Q
5咨9∙9SΛ∙gg M引讶引引弓650写 5
Figure 8: Qualitative results of sRb-VAE and Rb-VAE applied to conditional image generation. See
Sec. (5.5) of the paper for details.
⅛NPΘ N
JS-NIAl
SRb-VAE	Rb-VAE
Figure 9: Qualitative results of sRb-VAE and Rb-VAE applied to visual attribute transfer. See Sec.
(5.5) of the paper for details.
19
Under review as a conference paper at ICLR 2019
Figure 10: Images generated by sRB-VAE from random noise over latent variables e and z. Images
in the same panel share the same target factors e (expression). Images sharing the same position in
each panel are generated from the same common factors z (identity)
20