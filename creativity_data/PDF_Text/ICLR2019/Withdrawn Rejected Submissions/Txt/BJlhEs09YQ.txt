Under review as a conference paper at ICLR 2019
End-to-end Learning of a
Convolutional Neural Network via
Deep Tensor Decomposition
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we study the problem of learning the weights of a deep convolutional
neural network. We consider a network where convolutions are carried out over non-
overlapping patches with a single kernel in each layer. We develop an algorithm for
simultaneously learning all the kernels from the training data. Our approach dubbed
Deep Tensor Decomposition (DeepTD) is based on a rank-1 tensor decomposition.
We theoretically investigate DeepTD under a realizable model for the training data
where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are
generated according to planted convolutional kernels. We show that DeepTD is
data-efficient and provably works as soon as the sample size exceeds the total
number of convolutional weights in the network. Our numerical experiments
demonstrate the effectiveness of DeepTD and verify our theoretical findings.
1	Introduction
Deep neural network (DNN) architectures have led to state of the art performance in many domains
including image recognition, natural language processing, recommendation systems, and video
analysis (He et al. (2016); Krizhevsky et al. (2012); Van den Oord et al. (2013); Collobert & Weston
(2008)). Convolutional neural networks (CNNs) are a class of deep, feed-forward neural networks with
a specialized DNN architecture. CNNs are responsible for some of the most significant performance
gains of DNN architectures. In particular, CNN architectures have led to striking performance
improvements for image/object recognition tasks. Convolutional neural networks, loosely inspired
by the visual cortex of animals, construct increasingly higher level features (such as mouth and
nose) from lower level features such as pixels. An added advantage of CNNs which makes them
extremely attractive for large-scale applications is their remarkable efficiency which can be attributed
to: (1) intelligent utilization of parameters via weight-sharing, (2) their convolutional nature which
exploits the local spatial structure of images/videos effectively, and (3) highly efficient matrix/vector
multiplication involved in CNNs compared to fully-connected neural network architectures.
Despite the wide empirical success of CNNs the reasons for the effectiveness of neural networks
and CNNs in particular is still a mystery. Recently there has been a surge of interest in developing
more rigorous foundations for neural networks (Soltanolkotabi et al. (2017); Zhong et al. (2017b);
Alon Brutzkus & Globerson (2017); Soltanolkotabi (2017); Oymak (2018); Zhong et al. (2017a);
Janzamin et al. (2015); Li & Yuan (2017); Mei et al. (2018)). Most of this existing literature however
focus on learning shallow neural networks typically consisting of zero or one hidden layer. In practical
applications, depth seems to play a crucial role in constructing progressively higher-level features
from pixels. Indeed, state of the art Resnet models typically have hundreds of layers. Furthermore,
recent results suggest that increasing depth may substantially boost the expressive power of neural
networks (Raghu et al. (2016); Cohen et al. (2016)).
In this paper, we propose an algorithm for approximately learning an arbitrarily deep CNN model
with rigorous guarantees. Our goal is to provide theoretical insights towards better understanding
when training deep CNN architectures is computationally tractable and how much data is required
for successful training. We focus on a realizable model where the inputs are chosen i.i.d. from a
Gaussian distribution and the labels are generated according to planted convolutional kernels. We
use both labels and features in the training data to construct a tensor. Our first insight is that, in the
limit of infinite data this tensor converges to a population tensor which is approximately rank one and
1
Under review as a conference paper at ICLR 2019
Figure 1: Depiction of the input-output relationship of a non-overlapping Convolutional Neural Network (CNN)
model along with the various notations and symbols.
whose factors reveal the direction of the kernels. Our second insight is that even with finite data this
empirical tensor is still approximately rank one. We show that the gap between the population and
empirical tensors provably decreases with the increase in the size of the training data set and becomes
negligible as soon as the size of the training data becomes proportional to the total numbers of the
parameters in the planted CNN model. Combining these insights we provide a tensor decomposition
algorithm to learn the kernels from training data. We show that our algorithm approximately learns
the kernels (up to sign/scale ambiguities) as soon as the size of the training data is proportional to
the total number of parameters of the planted CNN model. Our results can be viewed as a first step
towards provable end-to-end learning of practical deep CNN models. Extending the connections
between neural networks and tensors (Janzamin et al. (2015); Cohen et al. (2016); Zhong et al.
(2017a)), we show how tensor decomposition can be utilized to approximately learn deep networks
despite the presence of nonlinearities and growing depth. While our focus in this work is limited to
tensors, we believe that our proposed algorithm may provide valuable insights for initializing local
search methods (such as stochastic gradient descent) to enhance the quality and/or speed of CNN
training.
2	Problem formulation and models
In this section we discuss the CNN model which is the focus of this paper. A fully connected artificial
neural network is composed of computational units called neurons. The neurons are decomposed into
layers consisting of one input layer, one output layer and a few hidden layers with the output of each
layer is fed in (as input) to the next layer. In a CNN model the output of each layer is related to the
input of the next layer by a convolution operation. In this paper we focus on a CNN model where the
stride length is equal to the length of the kernel. This is sometimes referred to as a non-overlapping
convolution operation formally defined below.
Definition 2.1 (Non-overlapping convolution) For two vectors k ∈ Rd and h ∈ Rp=dp their non-
overlapping convolution, denoted by k 因 h yields a vector U ∈ Rp= d whose entries are given by
Ui =	(k('), h[i]>	where	h[i]	：=	[h(i-ι)d+ι	h(i-1)d+2	…	hid].
In this paper it is often convenient to view convolutions as matrix/vector multiplications. This leads
us to the definition of the kernel matrix below.
Definition 2.2 (Kernel matrix) Consider a kernel k ∈ Rd and any vector h ∈ Rp=pd. Corresponding
to the non-overlapping convolution k 因 h, we associate a kernel matrix K因 ∈ RpXp defined as
K因：=Ip N kτ. Here, A N B denotes the Kronecker product between the two matrices A and B and
Ip denotes the P X P identity matrix. We note that based on this definition k 因 h = K因 h. Throughout
the paper we shall use K interchangeably with K因 to denote this kernel matrix with the dependence
on the underlying kernel and its non-overlapping form implied.
With the definition of the non-overlapping convolution and the corresponding kernel matrix in hand
we are now ready to define the CNN model which is the focus of this paper. For ease of exposition,
the CNN input-output relationship along with the corresponding notation is depicted in Figure 1.
•	Depth and numbering of the layers. We consider a network of depth D where we number the
input as layer 0 and the output as layer D and the hidden layers 1 to D - 1.
•	Layer dimensions and representations. We assume the input of the CNN, denoted by x ∈ Rp,
consists of P features and the output is a one dimensional label. We also assume the hidden layers
(numbered by ' = 1, 2,..., D — 1) consists of p` units with h(' ∈ Rp' and h(' ∈ Rp' denoting the
input and output values of the units in the `th hidden layer. For consistency of our notation we shall
also define h(0) ：= x ∈ Rp and note that the output of the CNN is h(D) ∈ R. Also, P0 = P and PD = 1.
2
Under review as a conference paper at ICLR 2019
•	Kernel dimensions and representation. For ` = 1, . . . D we assume the kernel relating the output
of layer (' - 1) to the input of layer ' is of dimension d` and is denoted by k(') ∈ Rd'.
•	Inter-layer relationship. We assume the inputs of layer ' (denoted by h(') ∈ Rp') are related to
the outputs of layer (' - 1) (denoted by h('-1) ∈ Rp'-1) via a non-overlapping convolution
h⑹=k⑹团 h('-1) = K(')h('-1) for ' = 1,...,D.
In the latter equality we have used the representation of non-overlapping convolution as a matrix/vector
product involving the kernel matrix K(') ∈ Rp'×p'-1 associated with the kernel k(') ∈ Rd' per
Definition 2.2. We note that the non-overlapping nature of the convolution implies that p` = p`-i/d`.
• Activation functions and intra-layer relationship. We assume the input of each hidden unit is
related to its output by applying an activation function φ' : R → R. More precisely, h(') ：= φ'(h⑹)
where for a vector U ∈ Rp, φ'(u) ∈ Rp is a vector obtained by applying the activation function φ'
to each of the entries of u. We allow for using distinct activation functions {Φ'}D=ι at every layer.
Throughout, we also assume all activations are 1-Lipschitz functions (i.e. ∣φ'(a) - φ'(b)∣ ≤ ∣a - b∣).
•	Final output. The input-output relation of our CNN model with an input x ∈ Rp is given by
x ↦ fCNN (x) := h(D),	(2.1)
with hidden unit relations h(') = φ'(h(')) and h⑹=K(')h('-1).
3 Algorithm: Deep Tens or Decomposition (DeepTD)
This paper introduces an approach to approximating the convolutional kernels from training data
based on tensor decompositions dubbed DeepTD, which consists of a carefully designed tensor
decomposition. To connect these two problems, we begin by stating how we intend to construct the
tensor from the training data. To this aim given any input data x ∈ Rp , we form a D-way tensor
X ∈ R0'=ιd' as follows. First, we convert x into a matrix by placing every d1 consecutive entries
of x as a row of a matrix of size p1 × d1 . From this matrix we then create a 3-way tensor of size
p2 × d2 × d1 by grouping d2 consecutive entries of each of the d1 columns and so on. We repeat this
procedure D times to arrive at the D-way tensor X ∈ R⑼=1d'. We define T ： Rp ↦ R⑼=1d' as the
corresponding tensor operation that maps X ∈ Rp to X ∈ Rθ'=1 d'.
Given a set of training data consisting of n input/output pairs (xi, yi) ∈ Rp × R we construct a tensor
Tn by tensorizing the input vectors as discussed above and calculating a weighted combination of
these tensorized inputs. More precisely,
1n	1n
Tn ：= n ∑ (yi - yavg) Xi where 小=n ∑ y and Xi = T(Xi).	(3.1)
We then perform a rank-1 tensor decomposition on this tensor to approximate the convolutional
kernels. Specifically we solve
k ,...,k = arg max(Tn, 0 v`)	subject to	IIvIIl'2 = ... = IlvD [e2 = 1.	(3.2)
V'∈Rd' T	'=1 I
In the above ㊁D=I v` denotes the tensor resulting from the outer product of vι, v2,..., vD. This
tensor rank decomposition is also known as CANDECOMP/PARAFAC (CP) decomposition Bro
(1997) and can be solved efficiently using Alternating Least Squares and a variety of other algorithms
Anandkumar et al. (2014a); Ge et al. (2015); Anandkumar et al. (2014b). 1
At this point it is completely unclear why the tensor Tn or its rank-1 decomposition can yield anything
useful. The main intuition is that as the data set grows (n → ∞) the empirical tensor Tn converges
close to a population tensor T whose rank-1 decomposition reveals useful information about the
kernels. Specifically, we will show that
D
ni→∞Tn = T ：= ex~N(0,ip)[fCNN(X)T(x)] ≈ α 0 k⑶,	(3.3)
with α a scalar whose value shall be discussed later on. Here, X is a Gaussian random vector with
i.i.d. N(0, 1) entries and represents a typical input with fCNN(X) the corresponding output and T(X)
the tensorized input. We will also utilize a concentration argument to show that when the training
1We would like to note that while finding the best rank-1 TD (3.2) is NP-hard, our theoretical guarantees continue
to hold when using an approximately optimal solution to (3.2). In fact, we can show that unfolding the tensor
along the ith kernel into a di × ∏j≠i dj matrix and using the top left singular vector yields a good approximation
to k(i). However, in our numerical simulations we instead utilize popular software packages to solve (3.2).
3
Under review as a conference paper at ICLR 2019
data set originates from an i.i.d. distribution, for a sufficiently large training data n, Tn yields a good
approximation of the population tensor T .
Another perhaps perplexing aspect of the construction of Tn in (3.1) is the subtraction by yavg in the
weights. The reason this may be a source of confusion is that based on the intuition above
E[T] =	E [1	∑∑ yiXi]	= E [ɪ ∑∑ (yi	- yavg) Xi] = ɪ E[Tn]	≈ α(DD k('),
Ln	i=ι	Ln - 1 i=1	n - 1	'=ι
so that the subtraction by the average seems completely redundant. The main purpose of this
subtraction is to ensure the weights yi - yavg are centered (have mean zero). This centering allows
for a much better concentration of the empirical tensor around its population counter part and is
crucial to the success of our approach. We would like to point out that such a centering procedure is
reminiscent of batch-normalization heuristics deployed when training deep neural networks.
Finally, we note that based on (3.3), the rank-1 tensor decomposition step can recover the convo-
Iutional kernels {k(')}j=1 UP to sign and scaling ambiguities. Unfortunately, depending on the
activation function, it may be impossible to overcome these ambiguities. For instance, if the activa-
tions are homogeneous (i.e. φ'(aχ) = aφ'(χ)), then scaling up one layer and scaling down the other
layer by the same amount does not change the overall function ∕cnn(∙). Similarly, if the activations
are odd functions, negating two of the layers at the same time preserves the overall function. In
Appendix C, we discuss some heuristics and theoretical guarantees for overcoming these sign/scale
ambiguities.
4 Main results
In this section we introduce our theoretical results for DeepTD. We will discuss these results in
three sections. In Section 4.1 we show that the empirical tensor concentrates around its population
counterpart. Then in Section 4.2 we show that the population tensor is well-approximated by a rank-1
tensor whose factors reveal the convolutional kernels. Finally, in Section 4.3 we combine these results
to show DeepTD can approximately learn the convolutional kernels up to sign/scale ambiguities.
4.1	Concentration of the empirical tensor
Our first result shows that the empirical tensor concentrations around the population tensor. We
measure the quality of this concentration via the tensor spectral norm defined below.
Definition 4.1 Let RO be the set ofrank-one tensors ㊁D=I v` satisfying IIvi E ≤ 1 for all 1 ≤ i ≤ D.
The spectral norm of a tensor X ∈ R ⑼=1d' is given by the supremum ∣∣∣T∣∣ = SuPV ∈ro (V, T).
Theorem 4.2 Consider a CNN model x ↦ fCNN(x) of the form (2.1) consisting of D ≥ 2 layers
with convolutional kernels k(1), k(2), . . . , k(D) of lengths d1, d2, . . . , dD . Let x ∈ Rp be a Gaussian
random vector distributed as N(0, Ip) with the corresponding labels y = fCNN(x) generated by
the CNN model and X ：= T(x) the corresponding tensorized input. Suppose the data set consists
of n training samples where the feature vectors xi ∈ Rp are distributed i.i.d. N (0, Ip) with the
corresponding labels yi = fCNN (xi) generated by the same CNN model and Xi ：= T(xi) the
corresponding tensorized input. Suppose n ≥ (∑D=1 d`) log D. Then the empirical tensor Tn and
population tensor T defined based on this dataset obey
∣∣Tn - T∣∣ = ∣1 ∑∑(yi- yvg)工-E[yx] ≤ CInlIkq& √(∑'=1 d√)logD +^,	(4.i)
Illn i=1	'=1	"2	√n
with probability at least 1 - 5e-min(t 托√n,n), where C > 0 is an absolute constant.
The theorem above shows that the empirical tensor approximates the population tensor with high
probability. This theorem also shows that the quality of this approximation is proportional to
∏dd=i ∣k(') ∣∣'2. This is natural as ∏D=ι ∣k(') l`^ is an upper-bound on the Lipschitz constant of the
network and shows how much the CNN output fluctuates with changes in the input. The more
fluctuations, the less concentrated the empirical tensor is, translating into a worse approximation
guarantee. Furthermore, the quality of this approximation grows with the square root of the parameters
in the model (∑D=ι d`) and is inversely proportional to the square root of the number of samples
(n) which are typical scalings in statistical learning. We would also like to note that as we will
see in the forthcoming sections, in many cases ∣∣T∣∣∣ is roughly on the order of ∏D=ι ∣k(')∣'2 so
that (4.1) guarantees that ∣∣∣Tn - TIMHT∣∣ ≤ C J(∑21d；)log D
Therefore, the relative error in the
4
Under review as a conference paper at ICLR 2019
approximation is less than as soon as the number of observations exceeds the number of parameters
in the model by a logarithmic factor in depth i.e. n N (∑D=1 d')暗.
4.2	Rank one approximation of the population tensor
Our second result shows that the population tensor can be approximated by a rank one tensor. To
explain the structure of this rank one tensor and quantify the quality of this approximation we require
a few definitions. The first quantity roughly captures the average amount by which the nonlinear
activations amplify or attenuate the size of an input feature at the output.
Definition 4.3 (CNN gain) Let X Z N(0, Ip) and define the hidden unit/output values of the CNN
based on this random input per equations (2.1). We define the CNN gain as a CNN = ∏D=ι E[φ'(hf))].
In words, this is the product of expectations of the activations evaluated at the first entry of each
layer. For non differentiable activations, φ' should be interpreted as the average of the left and right
derivatives. For instance, when φ'(z) = ReLU(z) then φ'(0) = 1/2.
This quantity is the product of the average slopes of the activations evaluated along a path connecting
the first input feature to the first hidden units across the layers all the way to the output. We note that
this quantity is the same when calculated along any path connecting an input feature to the output
passing through the hidden units. Therefore, this quantity can be thought of as the average gain
(amplification or attenuation) of a given input feature due to the nonlinear activations in the network.
To gain some intuition consider a ReLU network which is mostly inactive. Then the network is dead
and αCNN ≈ 0. On the other extreme if all ReLU units are active the network operates in the linear
regime and αCNN = 1. We would like to point out that αCNN can in many cases be bounded from
below by a constant. For instance, as proven in Appendix B, for ReLU activations as long as the
kernels obey
(ITk(')) ≥ 4 M)Il'2,	(4.2)
then acNN ≥ 1/4. Another example is the SoftPlus activation φ'(x) = log (1 + ex) for which We prove
αCNN ≥ 0.3 under similar assumptions (Also in Appendix B). We note that an assumption similar to
(4.2) is needed for the network to be active. This is because if the kernel sums are negative one can
show that with high probability, all the ReLUs after the first layer will be inactive and the network
will be dead. With this definition in hand, we are now ready to describe the form of the rank one
tensor that approximates the population tensor.
Definition 4.4 (Rank one CNN tensor) We define the rank one CNN tensor LCNN ∈ Rθd=1 d' as
L CNN = α CNN ㊁ D=I k('. That is, the product ofthe kernels {k(')}D=ι scaled by the CNN gain a cnn.
To quantify how well the rank one CNN tensor approximates the population tensor we need two
definitions. The first definition concerns the activation functions.
Definition 4.5 (Activation smoothness) We assume the activations are differentiable everywhere
and S-smooth (i.e. ∣φ'(x) 一 φ'(y)∣ ≤ S∣x 一 y∣ for all x,y ∈ R)forsome S ≥ 0.
The reason smoothness of the activations play a role in the quality of the rank one approximation is
that smoother activations translate into smoother variations in the entries of the population tensor.
Therefore, the population tensor can be better approximated by a low-rank tensor. The second
definition captures how diffused the kernels are.
Definition 4.6 (Kerneldiffuseness parameter) Given kernels {k(')}D=ι with dimensions {d'}D=ι,
the kernel diffuseness parameter μ is defined as μ = supι≤'≤° jd` ∣∣k(`) ∣∣` / ∣k(`) ∣∣` .
The less diffused (or more spiky) the kernels are, the more the population tensor fluctuates and thus
the quality of the approximation to a rank one tensor decreases. With these definitions in place, we
are now ready to state our theorem on approximating a population tensor with a rank one tensor.
Theorem 4.7 Consider the setup of Theorem 4.2. Also, assume the activations are S -smooth per
Definition 4.5 and the convolutional kernels are μ-diffused per Definition 4.6. Then, the population
tensor T ：= E[yX] can be approximated by the rank-1 tensor LCNN ：= αCNN ㊁D=I k(' asfollows
D	'
∣∣∣T - LCNN∣∣ ≤ IIT - LCNNIIF ≤ √8∏μS ∙ ∏ |仲)八 ∙ SUp ∏ |仲)∣'2
i=1	2	` i=1	2
D
∖/
min d`
`
The theorem above states that the quality of the rank one approximation deteriorates with increase
in the smoothness of the activations and the diffuseness of the convolutional kernels. As mentioned
5
Under review as a conference paper at ICLR 2019
earlier increase in these parameters leads to more fluctuations in the population tensor making it
less likely that it can be well approximated by a rank one tensor. We also note that ∣∣∣LCNN∣∣∣ =
αcNN ∏D=ι ∣∣k(') ∣∣'2 and therefore the relative error in this approximation is bounded by
IT-LCN ≤√8Π^S-sup∏∣k⑴l` -7^.
∣∣Lcnn∣∣	αcNN ` i=ι11	'2 √min d`
We would like to note that for many activations the smoothness is bounded by a constant. For instance,
for the softplus activation (φ(x) = log(1 + ex)) and one can show that S ≤ 1. As stated earlier,
under appropriate assumptions on the kernels and activations, the CNN gain αCNN is also bounded
from below by a constant. Assume the convolutional kernels have unit norm and are sufficiently
diffused so that the diffuseness parameter is bounded by a constant. We can then conclude that
IT-LCNNl ≤
∣∣∣LCNN∣∣∣
This implies that as soon as the length of the convolutional patches scale
with the square of depth of the network by a constant factor the rank one approximation is sufficiently
good. Our back-of-the-envelope calculations suggest that the correct scaling is linear in D versus
the quadratic result we have established here. Improving our result to achieve the correct scaling is
an interesting future research direction. Finally, we would like to note that while we have assumed
differentiable and smooth activations we expect our results to apply to popular non-differentiable
activations such as ReLU activations.
4.3 Learning the convolutional kernels
We demonstrated in the previous two sections that the empirical tensor concentrates around its
population counter part and that the population tensor is well-approximated by a rank one tensor. We
combine these two results along with a perturbation argument to provide guarantees for DeepTD.
Theorem 4.8 (Main theorem) Consider the setups of Theorems 4.2 and 4.7. Assume activations
are S-smooth per Definition 4.5 and the convolutional kernels are μ-diffused per Definition 4.6. The
DeepTD estimates of the convolutional kernels given by (3.2) using the empirical tensor Tn obeys
√(∑Zι d`) log D +1
∏D=ι I®'),M'"I ≥ ι-2(C
∏D=i∣∣k(')∣l'2	- αCNN Γ
'
+ ∖∕8∏μS ∙ sup ∏ ∣k(i)∣'
` i=1	`2
D
with probability at least 1 - 5e-min(t2 ,t√n,n), Where C > 0 isan absolute constant.
The above theorem is our main result on learning a non-overlapping CNN with a single kernel at each
layer. It demonstrates that estimates k(') obtained by DeePTD have significant inner product with
the ground truth kernels k(`) with high probability, using only few samples. Indeed, similar to the
discussion after Theorem 4.7 assuming the activations are sufficiently smooth and the convolutional
kernels are unit norm and sufficiently diffused, the theorem above can be simplified as follows
∏L∣(k2 ^叼 I 1	(√(∑L d`)log D D ∖
∏zjk叫'2	≥-1	√n	+ √mRJ
Thus the kernel estimates obtained via DeepTD are well aligned with the true kernels as soon as the
number of samples scales with the total number of parameters in the model and the length of the
convolutional kernels (i.e. the size of the batches) scales quadratically with the depth of the network.
5	Numerical experiments
Our goal in this section is to numerically corroborate the theoretical predictions of Section 4. To this
aim we use a CNN model of the form (2.1) with D layers and ReLU activations and set the kernel
lengths to be all equal to each other i.e. d4 = . . . = d1 = d. We use the identity activation for the last
layer (i.e. φD (z) = z) with the exception of the last experiment where we use a ReLU activation
(i.e. φD (z) = max(0, z)). We conducted our experiments in Python using the Tensorly library for the
tensor decomposition in DeepTD (Kossaifi et al. (2016)). Each curve in every figure is obtained by
averaging 100 independent realizations of the same CNN learning procedure. Similar to our theory,
we use Gaussian data points x and ground truth labels y = fCNN(x).
We conduct two sets of experiments: The first set focuses on larger values of depth D and the
second set focuses on larger values of width d. In all experiments kernels are generated with random
Gaussian entries and are normalized to have unit Euclidean norm. For the ReLU activation if one
of the kernels have all negative entries, the output is trivially zero and learning is not feasible. To
6
Under review as a conference paper at ICLR 2019
Uo4e-tut03
(a) d = 2, D = 12.
Figure 2: Correlations (COrr(k('), k(')) = ∖(k('), k('))∣) between the DeePTD estimate and the ground truth
kernels for different layers and over-sampling ratios N .
Uoqe-tuto3
0.70
0.65
8
N
N
N= IOO
-Z--ɪ---
12	3
Layer (£)
(b) d = 3, D = 8.
Figure 3: DeePTD estimate vs NaiveTD estimate when final aCtivation is ReLU. Bias of NaiveTD results in
signifiCantly worse PerformanCe.
address this, we Consider operational networks where at least 50% of the training labels are nonzero.
Here, the number 50% is arbitrarily Chosen and we verified that similar results hold for other values.
Finally, to study the effeCt of finite samPles, we let the samPle size grow ProPortional to the total
degrees of freedom ∑D=ι d`. In particular, we set an oversampling factor N = D and carry out
∑'=1 d'
the experiments for N ∈ {10,20,50,100}. While our theory requires N > log D, in our experiments,
we typically observe that improvement is marginal after N = 50.
In Figure 2, we consider two networks with d = 2, D = 12 and d = 3, D = 8 configurations. We plot
the absolute correlation between the ground truth and the estimates as a function of layer depth. For
each hidden layer 1 ≤ ` ≤ D, our correlation measure (y-axis) is
corr(^('), k⑻)=∣伍⑶,k⑶)∣.
This number is between 0 and 1 as the kernels and their estimates both have unit norm. We observe
that for both d = 2 and d = 3, DeepTD consistently achieves correlation values above 75% for N = 20.
While our theory requires d to scale quadratically with depth i.e. d > D2, we find that even small
d values work well in our experiments. The effect of sample size becomes evident by comparing
N = 20 and N = 50 for the input and output layers (` = 1, ` = D). In this case N = 50 achieves
perfect correlation. Interestingly, correlation values are smallest in the middle layers. In fact this even
holds when N is large suggesting that the rank one approximation of the population tensor provides
worst estimates for the middle layers.
In Figure 3, we use a ReLU activation in the final layer and assess the impact of the centering
procedure of the DeepTD algorithm which is a major theme throughout the paper. We define the
NaiveTD algorithm which solves (3.2) without centering in the empirical tensor i.e.
<1 n	D	∖
∑ yiXi,⑤ v`)	subject to IWIIl` = ... = IIvD [ = 1.	(5.1)
n M	'=ι	2	2
Since the activation of the final layer is ReLU, the output has a clear positive bias in expectation
which will help demonstrating the importance of centering. We find that for smaller oversampling
factors of N = 10 or N = 20, DeepTD has a visibly better performance compared with NaiveTD.
The correlation difference is persistent among different layers (we plotted only Layers 1 and 2) and
appears to grow with increase in the kernel size d.
Finally, in Figure 4, we assess the impact of activation nonlinearity by comparing the ReLU and
identity activations in the final layer. We plot the first and final layer correlations for this setup. While
the correlation performances of the first layer are essentially identical, the ReLU activation (dashed
lines) achieves significantly lower correlation at the final layer. This is not surprising as the final layer
passes through an additional nonlinearity.
7
Under review as a conference paper at ICLR 2019
Figure 4: Performance of DeepTD when the final activation is ReLU in lieu of the identity activation.
6	Related work
Our work is closely related to the recent line of papers on neural networks as well as tensor decompo-
sitions. We briefly discuss this related literature.
Neural networks: Learning neural networks is a nontrivial task involving non-linearities and non-
convexities. Consequently, existing theory works consider different algorithms, network structures
and assumptions. A series of recent work focus on learning zero or one-hidden layer fully connected
neural networks with random inputs and planted models (Goel et al. (2016); Mei et al. (2018); Zhong
et al. (2017b); Soltanolkotabi (2017); Oymak (2018); Ge et al. (2017); Fu et al. (2018)). Other
publications (Alon Brutzkus & Globerson (2017); Du et al. (2017b;a); Zhong et al. (2017a); Goel
et al. (2018)) consider the problem of learning a CNN with 1-hidden layer. In particular, first three of
these focus on learning non-overlapping CNNs as in this paper (albeit in the limit of infinite training
data). The papers above either focus on characterizing the optimization landscape, or population
landscape, or providing exact convergence guarantees for gradient descent. In comparison, in this
paper we focus on approximate convergence guarantees using tensor decompositions for arbitrary
deep networks. A few recent publications (Soltanolkotabi et al. (2017); Sagun et al. (2017); Soudry
& Carmon (2016)) consider the training problem when the network is over-parametrized and study
the over-fitting ability of such networks. Closer to our work, Malach & Shalev-Shwartz (2018) and
Arora et al. (2014) consider provable algorithms for deep networks using layer-wise algorithms. In
comparison to our work, Malach & Shalev-Shwartz (2018) applies to a very specific generative model
that assumes a discrete data distribution and studies the population loss in lieu of the empirical loss.
Arora et al. (2014) studies deep models but uses activation functions that are not commonly used and
assumes random network weights. In comparison, we work with realistic activations and arbitrary
network weights.
Tensor decomposition: Tensors are powerful tools to model a wide variety of big-data problems
(Sidiropoulos et al. (2017); Anandkumar et al. (2014a)). Recent years have witnessed a growing
interest in tensor decomposition techniques to extract useful latent information from the data (Anand-
kumar et al. (2014a); Ge et al. (2015)). The connection between tensors and neural networks have
been noticed by several papers (Mondelli & Montanari (2018); Janzamin et al. (2015); Cohen et al.
(2016); Kossaifi et al. (2017); Cohen & Shashua (2016)). Cohen & Shashua (2016); Cohen et al.
(2016) relate convolutional neural networks and tensor decompositions to provide insights on the
expressivity of CNNs. Mondelli & Montanari (2018) connects the hardness of learning shallow
networks to tensor decompositions. Closer to this paper, Janzamin et al. (2015) and Zhong et al.
(2017b) apply tensor decomposition on one-hidden layer, fully connected networks to approximately
learn the latent weight matrices.
7	Conclusion
In this paper we studied a multilayer CNN model with depth D. We assumed a non-overlapping
structure where each layer has a single convolutional kernel and has stride length equal to the
dimension of its kernel. We establish a connection between approximating the CNN kernels and
higher order tensor decompositions. Based on this, we proposed an algorithm for simultaneously
learning all kernels called the Deep Tensor Decomposition (DeepTD). This algorithm builds a
D-way tensor based on the training data and applies a rank one tensor factorization algorithm to
this tensor to simultaneously estimate all of the convolutional kernels. Assuming the input data is
distributed i.i.d. according to a Gaussian model with corresponding output generated by a planted set
of convolutional kernels, we prove DeepTD can approximately learn all kernels with a near minimal
amount of training data. A variety of numerical experiments complement our theoretical findings.
8
Under review as a conference paper at ICLR 2019
References
A. Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with Gaussian inputs.
arXiv preprint arXiv:1702.07966, 2017.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions
for learning latent variable models. The Journal ofMachine Learning Research,15(1):2773-2832, 2014a.
Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal tensor decomposition via
alternating rank-1 updates. arXiv preprint arXiv:1402.5180, 2014b.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representa-
tions. In International Conference on Machine Learning, pp. 584-592, 2014.
Rasmus Bro. Parafac. tutorial and applications. Chemometrics and intelligent laboratory systems, 38(2):149-171,
1997.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions. In
International Conference on Machine Learning, pp. 955-963, 2016.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In
Conference on Learning Theory, pp. 698-728, 2016.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine learning,
pp. 160-167. ACM, 2008.
Sjoerd Dirksen. Tail bounds via generic chaining. arXiv preprint arXiv:1309.3522, 2013.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint
arXiv:1709.06129, 2017a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017b.
Haoyu Fu, Yuejie Chi, and Yingbin Liang. Local geometry of one-hidden-layer neural networks for logistic
regression. arXiv preprint arXiv:1802.06463, 2018.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic gradient for
tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design.
arXiv preprint arXiv:1711.00501, 2017.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time.
arXiv preprint arXiv:1611.10258, 2016.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches.
arXiv preprint arXiv:1802.02547, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed
training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Jean Kossaifi, Yannis Panagakis, and Maja Pantic. Tensorly: Tensor learning in python. arXiv preprint
arXiv:1610.09555, 2016.
Jean Kossaifi, Zachary C Lipton, Aran Khanna, Tommaso Furlanello, and Anima Anandkumar. Tensor regression
networks. arXiv preprint arXiv:1707.08308, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances
in Neural Information Processing Systems, pp. 597-607, 2017.
Eran Malach and Shai Shalev-Shwartz. A provably correct algorithm for deep learning that actually works.
arXiv preprint arXiv:1803.09522, 2018.
9
Under review as a conference paper at ICLR 2019
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural
networks. arXiv preprint arXiv:1804.06561, 2018.
Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural networks and
tensor decomposition. arXiv preprint arXiv:1802.07301, 2018.
Samet Oymak. Learning compact neural networks with regularization. arXiv preprint arXiv:1802.01223, 2018.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power
of deep neural networks. arXiv preprint arXiv:1606.05336, 2016.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of
over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalexakis, and Christos
Faloutsos. Tensor decomposition for signal processing and machine learning. IEEE Transactions on Signal
Processing, 65(13):3551-3582, 2017.
Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. arXiv preprint arXiv:1705.04591, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape
of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer
neural networks. arXiv preprint arXiv:1605.08361, 2016.
Michel Talagrand. The generic chaining: upper and lower bounds of stochastic processes. Springer Science &
Business Media, 2006.
Michel Talagrand. Gaussian processes and the generic chaining. In Upper and Lower Bounds for Stochastic
Processes, pp. 13-73. Springer, 2014.
Ryota Tomioka and Taiji Suzuki. Spectral norm of random tensors. arXiv preprint arXiv:1407.1870, 2014.
Aaron Van den Oord, Sander Dieleman, and Benjamin Schrauwen. Deep content-based music recommendation.
In Advances in neural information processing systems, pp. 2643-2651, 2013.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural networks with
multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-
hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b.
10
Under review as a conference paper at ICLR 2019
A	Proofs
In this section we will prove our main results. Throughout, for a random variable X, we use zm(X) to denote
X - E[X]. Simply stated, zm(X) is the centered version of X. For a random vector/matrix/tensor X, zm(X)
denotes the vector/matrix/tensor obtained by applying the zm() operation to each entry. For a tensor T we use
∣∣T∣∣f to denote the square root of the sum of squares of the entries of the tensor. Stated differently, this is the
Euclidean norm of a vector obtained by rearranging the entries of the tensor. Throughout we use c, c1 , c2, and C
to denote fixed numerical constants whose values may change from line to line. We begin with some useful
definitions and lemmas.
A. 1 Useful concentration lemmas and definitions
In this section we gather some useful definitions and well-known lemmas that will be used frequently throughout
our concentration arguments.
Definition A.1 (Orlicz norms) For a scalar random variable Orlicz-a norm is defined as
∣∣X∣∣ψa = sup k-"(E[∣X∣k ])1/k
k≥1
Orlicz-a norm of a vector x ∈ Rp is defined as ∣x∣ψa = supv∈Bp ∣vT x∣ψa where Bp is the unit `2 ball. The
sub-exponential norm is the function ∣∣∙∣ψι and the sub-gaussian norm the function ∣∣∙ ∣ψ2.
We now state a few well-known results that we will use throughout the proofs. This results are standard and are
stated for the sake of completeness. The first lemma states that the product of sub-gaussian random variables are
sub-exponential.
Lemma A.2 Let X, Y be subgaussian random variables. Then ∣XY ∣ψ1 ≤ ∣X ∣ψ2 ∣Y ∣ψ2 .
The next lemma connects Orlicz norms of sum of random variables to the sum of the Orlicz norm of each random
variable.
LemmaA.3 Suppose X, Y are random variables with bounded ∣∣∙∣ψa norm. Then ∣∣X + Y∣∣ψa ≤
2 max{∣X ∣ψa, ∣Y∣ψa}. In particular ∣X - EX]∣ψa ≤ 2∣X∣ψa.
The lemma below can be easily obtained by combining the previous two lemmas.
Lemma A.4 Let X, Y be subgaussian random variables. Then ∣zm(XY)∣ψ1 ≤ 2∣X ∣ψ2 ∣Y ∣ψ2 .
Finally, we need a few standard chaining definitions.
Definition A.5 (Admissible sequence Talagrand (2014)) Given a set T an admissible sequence is an increas-
ing sequence {An}n∞=0 of partitions ofT such that ∣An∣ ≤ Nn where N0 = 1 and Nn = 22n for all n ≥ 1.
For the following discussion ∆d(An(t)), will be the diameter of the set S ∈ An that contains t, with respect to
the d metric.
Definition A.6 (γa functional Talagrand (2014)) Given a > 0, and a metric space (T, d) we define
Ya (T,d) = inf sup ∑ 2n∕a∆d (An(t)),
t∈T n≥0
where the infimum is taken over all admissible sequences.
The following lemma upper bounds γα functional with covering numbers of T. The reader is referred to Section
1.2 of Talagrand (2006), Equation (2.3) of Dirksen (2013), and Lemma D.17 of Oymak (2018).
Lemma A.7 (Dudley’s entropy integral) Let N(ε) be the ε covering number of the set T with respect to the
d metric. Then
γα(T,d) ≤Cα
where Cα > 0 depends only on α > 0.
/'OU
N (ε)dε,
11
Under review as a conference paper at ICLR 2019
A.2 Concentration of the empirical tensor (Proof of Theorem 4.2)
To prove this theorem, first note that given labels {yi}n=ι Z y and their empirical average y” = n-1 ∑n=1 yi, We
have E[yavg] = E[y]. Hence y - yavg = zm(y - yavg) and we can rewrite the empirical tensor as follows
1n
Tn ：=— ∑(yi - yavg)Xi
n i=1
1n
=—∑ zm(yi - yavg)Xi
n i=1
1n	1n
=—∑ zm(yi)Xi -zm(yavg)( — ∑ XiL
n i=1	n i=1
(A.1)
Recall that the population tensor is equal to T := E[yi Xi]. Furthermore, E[E[yi]Xi] = E[yi] E[Xi] = 0. Thus
the population tensor can alternatively be written as T = E[zm(yi)Xi] = n ∑n=1 E[zm(yi)Xi]. Combining
the latter with (A.1) we conclude that
1n	1n
Tn - T =— ∑ (zm(yi)Xi - E[zm(yi)Xi]) - zm(yavg) (一 ∑ Xi),
n i=1	n i=1
1n	1n
=—∑ zm(zm(yi)Xi) - zm(yavg) ( 一 ∑ Xi),
ni=1	ni=1
1n	1n	1n
=-∑ zm(zm(yi)Xi) - I-∑ zm(yi))(-∑ Xij.
n i=1	n i=1	n i=1
Now using the triangular inequality for tensor spectral norm we conclude that
∣∣Tn - T∣∣∣≤ In ∑ Zm(Zm(yi)Xi)
+∣K n ∑ zm(yi))( n ∑ Xi)I
We now state two lemmas to bound each of these terms. The proofs of these lemmas are defered to Sections
A.2.1 and A.2.2.
Lemma A.8 For i = 1, 2, . . . , n let Xi ∈ Rp be i.i.d. random Gaussian vectors distributed as N (0, Ip). Also let
Xi ∈ R®'=1 d' be the tensorized version of Xi i.e. Xi = T(Xi). Finally, assume f : Rp ↦ R is an L LiPschitz
function. Furthermore, assume n ≥ (∑D=1 d`) log D and D ≥ 2. Then
P{∣∣1 ∑zm(zm(f(Xi))Xi)∣∣∣ ≤ √n 卜
holds with c1 > 0 a fixed numerical constant.
(∑ d')log D + t∣ } ≤ e- min(t2 ,t√n)
Lemma A.9 Consider the setup of Lemma A.8. Then
P
1n
n ∑ Zmf(Xi))
holds with c2 > 0 a fixed numerical constant.
)(1 ∑ Xi)I∣≥ *(旧■
log D +12∣ } ≤ 2 (e-t1 + e-t2),
Combining Lemma A.8 with f = ∕cnn(), L = ∏D=ι ∣∣k(') ll` , and ci = c/2 together with Lemma A.9 with
ti = √n, t2 = t, and c2 = c/2 concludes the proof of Theorem 4.2. All that remains is to prove Lemmas A.8 and
A.9 which are the subject of the next two sections.
A.2.1 Proof of Lemma A.8
It is more convenient to carryout the steps of the proof on ∑in=1 Zm(Zm(f (xi))Xi) in liue of
1 ∑n=ι zm(zm(f (Xi))Xi). The lemma trivally follows by a scaling by a factor 1/n. We first write the
tensor spectral norm as a supremum
∣∣∣∑ zm(zm(f (xi))Xi)∣∣∣
sup
T∈RO
n
∑ (zm(zm(f (Xi))Xi), T).
i=1
(A.2)
Let Yi = zm(f(Xi))Xi. Define the random process g(T) = ∑n=1 (Zm(Yi),T〉. We claim that g(T) has a
mixture of subgaussian and subexponential increments (see Definition A.1 for subgaussian and subexponential
random variables). Pick two tensors T, H ∈ R0'=1 d'. Increments of g satisfy the linear relation
n
g(T) - g(H) = ∑ (zm(Yi),T - H).
i=1
By construction E[g(T) - g(H)] = 0. We next claim that Yi is a sub-exponential vector. Consider a
tensor T with unit length ∣∣TIIF = 1 i.e. the sum of squares of entries are equal to one. We have (Yi, T)=
zm(f (Xi)) (Xi, T). f (Xi) is a Lipschitz function of a Gaussian random vector. Thus, by the concentration of
12
Under review as a conference paper at ICLR 2019
Lipschitz functions of Gaussians we have
t2
P(∣zm(f(Xi))∣ ≥ t) ≤ 2exp(— -).	(A.3)
This immediately implies that ∣∣zm(f (Xi))I∣ψ2 ≤ cL for a fixed numerical constant c. Also note that {Xi, T〉Z
N(0,1) hence ∣∣(Xi,T)∣ψ2 ≤ c. These two identities combined with Lemma A.4 implies a bound on the
sub-exponential norm
∣∣zm(<%,T>)∣∣ψι ≤ cL.
Next, we observe that g(T) - g(H) is sum of n i.i.d. sub-exponentials each obeying IKzm(Yi), T - H)|M ≤
cL∣T - H ∣F . Applying a standard sub-exponential Bernstein inequality, we conclude that
P(Ig(T) - g(H )∣ ≥ t) ≤ 2exp (-c ∙ min ( L||T： h∣∣f , nL2!∣Tt-H∣∣F ))，	(A.4)
holds with γ a fixed numerical constant. This tail bound implies that g is a mixed tail process that is studied
by Talagrand and others (Talagrand (2014); Dirksen (2013)). In particular, supremum of such processes are
characterized in terms of a linear combination of Talagrand’s γ1 and γ2 functionals (see Definition A.6 as well
as Talagrand (2014; 2006) for an exposition). We pick the following distance metrics on tensors induced by the
Frobenius norm: dι(T, H) = L∣∣H - T∣∣f/c and d2(T, H) = ∣∣H 一 T∣∣fL√n∕c. We can thus rewrite (A.4)
in the form
P{∣g(T) - g(H)∣ ≥ t} ≤ 2eχp (-min(di7THy, d2(TH))) ,
which implies P{∣g(T) - g(H)∣ ≥ √d2(T, H) + tdι(T, H)} ≤ 2exp(-t). Observe that the radius of
RO with respect to ∣∣∙∣∣f norm is 1 hence radius with respect to di, d2 metrics are L/c, L∖Jn∕c respectively.
Applying Theorem 3.5 of Dirksen (2013), we obtain
Pl SuP ∣g(T)∣ ≥ C(γ2(RO,d2) + γ1(RO,d1) + L√un∕c + uL∕c) ∣ ≤ e-u.
I T∈RO	∖	f I
Observe that we can use the change of variable t = L ∙ max (√un, u) to obtain
p1 ^supɔ ∣g(T)∣ ≥ C (γ2(RO,d2) + γ1(RO,d1) + t) J ≤ exp (-min (LL2n, -t)),
(A.5)
with some updated constant C > 0. To conclude, we need to bound the γ2 and γ1 terms. To achieve this we will
upper bound the γα functional in terms of Dudley’s entropy integral which is stated in Lemma A.7. First, let us
find the ε covering number of RO. Pick 0 < δ ≤ 1 coverings C' of the unit '2 balls Bd'. These covers have size
at most (1 + 2∕δ)d'. Consider the set of rank 1 tensors C = Ci 0 …0 CD with size (1 + 2∕δ)∑D=1 d'. For any
区D=I v` ∈ RO, we can pick 0D;I u` ∈ C satisfying ∣∣ v` — u` ∣∣'2 ≤ δ for all 1 ≤ ' ≤ D. This implies
D
∣∣({v'}D=ι) - ({u'}Zι)∣∣F ≤ ∑ ∣∣(vi,…,v`, U'+1,..., UD) - (vi,..., V'-1, u`,..., UD )∣∣F	(A.6)
'=1
D
=∑ ∣∣V'∣∣'2 ∙∙∙ ∣∣V'-1∣∣'2 ∣∣V'- U'∣∣'2 ∣∣U'+1∣∣'2 ∙∙∙ ∣∣UD ∣∣'2 ≤ Dδ. (A.7)
'=1
Denoting Frobenius norm covering number of RO by N (ε), this implies that, for 0 < ε ≤ 1,
N(ε) ≤ (1 + 2D∕ε)∑D≈1 d'.
Clearly, N (ε) = 1 for ε ≥ 1 by picking the cover {0}. Consequently,
Yi(RO, ∣∙∣f ) ≤ / log N (ε)dε ≤ c(∑ d`)log D, γ2(RO,∣∣∙∣∣F) ≤ / √log N (ε)dε ≤ c、
(∑ d`) log D.
(A.8)
Thus the metrics di , d2 metrics are ∣∙∣F norm scaled by a constant. Hence, their γα functions are scaled versions
of (A.8) given by
γ1(RO,d1) ≤ CL (∑ d`) logD, γ2(RO,d2) ≤ cL、n (∑ d`)
log D.
Now, observe that if n ≥ (∑D=I d`) log D, we have γ1(RO,d1) ≤ CLyJn (∑D=I d`) log D. Substituting these
in (A.5) and using n ≥ (∑D=I d`) log D we find
卜、n (∑ d')log D + t)J
PIl sup ∣g(T)∣ ≥ C
II T∈RO
≤ e-min( Lt2n, L)
Substituting t → L√nt and recalling (A.2), concludes the proof.
13
Under review as a conference paper at ICLR 2019
A.2.2 Proof of Lemma A.9
We first rewrite,
Il( 1 ∑Σ zm(f(Xi)))( 1 ∑Σ Xi) =	1 ∑Σ zm(f(Xi))∣∙[∣1 ∑Σ XiI	(A.9)
n i=1	n i=1	n i=1 n i=1
As discussed in (A.3), ∣∣zm(f (Xi))I∣ψ2 ≤ CL for C a fixed	numerical constant. Since Xi's are i.i.d the empirical
sum favg =	1	∑n=1 zm(f (Xi)) obeys the bound ∣∣favg ∣∣ψ2	≤ cL∕√n as well. Hence,
P {IfavgI ≥ c' √n} ≤ 2e t1.	(A.1O)
Also note that √η ∑n=1 Xi is a tensor with standard normal entries, applying (Tomioka & Suzuki, 2014,
Theorem 1) we conclude that
缺 ∑χi∣∣∣≤ c OF+12)	⇒ ∣∣∣1 JH∙ c	, (a-
t2
holds with probability 1 - 2e-t2 . Combining (A.10) and (A.11) via the union bound together with (A.9)
concludes the proof.
A.3 Rank one approximation of the population tensor (Proof of Theorem 4.7)
We begin the proof of this theorem by a few definitions regarding non-overlapping CNN models that simplify
our exposition. For these definitions it is convenient to view non-overlapping CNNs as a tree with the root of the
tree corresponding to the output of the CNN and the leaves corresponding to input features. In this visualization
D - `th layer of the tree corresponds to the `th layer. Figure 5 depicts such a tree visualization along with the
definitions discussed below.
Definition A.10 (Path vector) A vector i ∈ RD+1 is a path vector if its zeroth coordinate satisfies 1 ≤ i0 ≤ p
and for all D - 1 ≥ j ≥ 0, ij+1 obeys ij+1 = [ ij /dj∖ This implies 1 ≤ ij ≤ Pj and i° = 1. We note that in the
tree visualization a path vector corresponds to a path connecting a leaf (input feature) to the root of the tree
(output). We use I to denote the set of all path vectors and note that III = p. We also define i(i) ∈ I be the
vector whose zeroth entry is i0 = i. Stated differently i(i) is the path connecting the input feature i to the output.
Given a path i and a P dimensional vector V, we define Vi ：= Vi0. A sample path vector is depicted in bold in
Figure 5 which corresponds to i = (d1 , 1, 1, 1).
Definition A.11 (Kernel and activation path gains) Consider a CNN model of the form (2.1) with input X
and inputs Ofhidden units given by {h (`) }D=ι. To any path vector i connecting an input feature to the output we
associate two path gains: a kernel path gain denoted by ki and activation path gain denoted by φi(x) defined
as
DD
ki = ∏ kmod(i'-1,d') and φi(x) = ∏ φ'(h(')),
where mod(a, b) denotes the remainder of dividing integer a by b. In words the kernel path gain is multiplication
of the kernel weights along the path and the activation path gain is the multiplication of the derivatives of the
activations evaluated at the hidden units along the path. For the path depicted in Figure 5 in bold the kernel path
gain is equal ki = kd1)k(2)k(3) and the activation path gain is equal to φi(x) = φ1(hf) φ2 (h『)φ3(hf)).
Definition A.12 (CNN offsprings) Consider a CNN model of the form (2.1) with input X and inputs of hidden
units given by {h (`) }Da. We will associate a set set ` (i) ⊂ {1,...,p} to the i th hidden unit of layer ' defined as
set'(i) = {(i - l)r` + 1, (i - l)r` + 2,...,ir'} where r` = p/p`. By construction, this corresponds to the set of
entries Ofthe input data X that hi(') (x) is dependent on. In the tree analogy set`(i) are the leaves Ofthe tree
connected to hidden unit i in the `th layer i.e. the set of offsprings of this hidden node. We depict set2 (P2 ) which
are the offsprings of the last hidden node in layer two in Figure 5.
We now will rewrite the population tensor in a form such that it is easier to see why it can be well approximated
by a rank one tensor. Note that since the tensorization operation is linear the population tensor is equal to
T = E[fCNN(X)X] = E[fCNN(X)T (X)] = T (E[fCNN(X)X]) .	(A.12)
Define the vector gCNN to be the population gradient vector i.e. gCNN = E[V∕cnn(x)] and note that Stein's
lemma combined with (A.12) implies that
T = T (E[fcNN(X)X]) = T (E[VfcNN(X)]) = T (gcNN) .	(A.13)
Also note that
∂∕cnn(x)
∂Xi
Thus we have
gCNN = E AfCNN(X)= 帆霖⑴(X)].	(A.14)
∂Xi
14
Under review as a conference paper at ICLR 2019
X = h(O) ∈ Rp ：= Rp0
k⑴ ∈ Rd1	h⑴ ∈ Rp
k⑵ ∈ Rd2
h⑵ ∈ Rp2
k⑶ ∈ Rd3
y = fcNN(x) = h(D)= h(3
k（2）
Set2(P2)
Figure 5: Tree visualization of a non-overlapping cNN model. The path in bold corresponds to
the path vector i = (d1, 1, 1, 1) from Definition A.10. For this path the kernel path gain is equal
ki = kd1)k(2)k13) and the activation path gain is equal to φi(x) = Φι(h11))Φ2(h 12))Φ3(h13)). The
set set2(p2) (offsprings of the last hidden node in layer two) is outlined.
15
Under review as a conference paper at ICLR 2019
Define a vector k ∈ Rp such that ki = ki(i). Since ki(i) consists of the product of the kernel values across the
path i(i) it is easy to see that the tensor K ：= T(k) is equal to
K = (§) k(').
(A.15)
'=1
Similarly define the vector V ∈ Rp such that Vi = E[φi(i) (x)] and define the corresponding tensor V = T(v).
Therefore, (A.14) can be rewritten in the vector form gCNN = k Θ V where a Θ b denotes entry-wise (Hadamard)
product between two vectors/matrices/tensors a and b of the same size. Thus using (A.13) the population tensor
can alternatively be written as
T = T(gCNN) = KΘ V = ((D)k(')) Θ V.
Therefore, the population tensor T is the outer product of the convolutional kernels whose entries are masked
with another tensor V. If the entries of the tensor V were all the same the population tensor would be exactly
rank one with the factors revealing the convolutional kernel. One natural choice for approximating the population
tensor with a rank one matrix is to replace the masking tensor V with a scalar. That is, use the approximation
T ≈ C 0D=ι k('). Recall that LCNN ：= Qcnn ΘD=i k(') is exactly such an approximation with C set to Ocnn given
by
αCNN
'=1
To characterize the quality of this approximation note that
(a)
IIIT - LcNNIII ≤ lT- LcNNlF ,
(=b) lK Θ (V - αcNN)lF ,
(=) llk θ (v - αCNN)l'2 ,
(d)
≤ Ukll'2 ||v - αCNNU'∞ ,
(=) ∏ k(')∣∣'2 ιιv-αcNN k.
Here, (a) follows from the fact for a tensor, its spectral norm is smaller than its Frobenius norm, (b) from
the definitions of T and LcNN, (c) from the fact that K = T(k) and V = T(V), (d) from the fact that
la Θ V l` ≤ lal` lbl` , and (e) from the fact that the Euclidean norm of the kronecker product of of vectors
is equal to the product of the Euclidean norm of the indivisual vectors. As a result of the latter inequality to
prove Theorem 4.7 it suffices to show that
` ll ll
∣∣v - acNN ll` ≤ √8πμS • SuP ∏ ∣∣k(i) Ii
` i=1	`2
D
ʌ/min d`
(A.16)
•
In the next lemma we prove a stronger statement.
Lemma A.13 Assume the activations are S-smooth. Also consider a vector V ∈ Rp with entries Vi
Eχ~N(o,i)[Φi(i)(x)] and αCNN = ∏D=ι Eχ~N(0,1)[Φ'(h('))] we have
D	'-1
Ivi - α CNN I ≤ Ki ：= 8πS∑ Σ Ik N (il-1,dl) I Π Ilk'" ll'2 .
Here, i is the vector path that starts at input feature i.
Before proving this lemma let us explain how (A.16) follows from this lemma. To show this we use the kernel
diffuseness assumption introduced in Definition 4.6. This definition implies that Ikmod(设 Idg)∣ ≤ ∣∣k(') ^` ≤
泉 ∣∣k(')∣∣'2. ThuSWehaVe
D	'-1
Ki =√8πS ς Ikm)d(i` 1 ,d`)1 ∏ llk(i) ll'2 ,
'=1	i=1
≤√8π"S ∑∑ ɪ ∣∣k(')∣∣,2∏llk(i)ll,2,
D 1	'
=√8πμS ∑ — ∏ Uk(i) He2,
'=1 V d' i=1
≤√8∏μDS • SuP ∏'=1 Hki)U'2 ,
`	d`
'
≤√8πμS • sup ∏ ∣k(i)∣ • r^-r
` i=1	`2	min d`
16
Under review as a conference paper at ICLR 2019
This completes the proof of Theorem 4.7. All that remains is to prove Lemma A.13 which is the subject of the
next section.
A.3.1 Proof of Lemma A.13
To bound the difference between vi and αCNN consider the path i = i(i) and define the variables {ai}iD=0 as
iD
ai = Ex~N(0,I)[∏ φ'(h(')] ∏ Ex~N(0,I)[φ'(h('))].
'=1	'=i+1
Note that aD = vi and a0 = αCNN. To bound the difference vi - αCNN = aD - a0 we use a telescopic sum
D-1
∣aD - ao∣ ≤ ∑ ∣a'+ι - a'∣.	(A.17)
'=0
We thus focus on bounding each of the summands ∣a' - a'-ι∣. Setting γ' = ∏D' Ex 〜N (o,i)[φ'(h (i))], this can
be written as
`-1
a` - a`-i = Ex~N(o,i)[(φ'(h(')) - E[φ'(h('))]) ∏ φ'(h('))]Y'+ι.
i=1
Using ∣γ'∣ ≤ 1 (which follows from the assumption that the activations are 1-Lipschitz), it suffices to bound
`
Ex~N(0,I)[(φ'(h(')) - E[φ'(h('))])θ`-i] where θ` = ∏ Φi(h(?).	(A.18)
i=1
To this aim we state two useful lemmas whose proofs are deferred to Sections A.3.1.1 and A.3.1.2.
Lemma A.14 Let X, Y, Z be random variables where X is independent of Z. Let f be an L-Lipschitz function.
Then
∣E[f(X+Y)Z]-E[f(X+Y)]E[Z]∣≤LE[∣zm(Y)∣(∣Z∣+∣E[Z]∣)].	(A.19)
Furthermore if ∣Z∣ ≤ 1, then
∣E[f(X+Y)Z]-E[f(X+Y)]E[Z]∣≤2LE[∣zm(Y)∣].	(A.20)
Lemma A.15 h (') (x) (and h(') (x)) is a deterministic function of the entries of X indexed by set '(i). In other
words, there exists a function f such that h (') (x) = f (x set 2⑴).
With these lemmas in-hand We return to bounding (A.18). To this aim We decompose h(') as follows
d`
h(') = ∑ k(')h('—1)	= k(')	h('—1)+ r
hi` = ∑ ki hd` (i` -1)+i = kmod(i`-1 ,d`)hi`-1 + r,
i=1	-
where the r term is the contribution of the entries of h('T) other than i`-i. By the non-overlapping assumption,
r is independent of θ`-i as well as h(H) (see Lemma A.15). In particular, h('-1) and θ`-i is a function of
Xset'_1(i'_1) whereas r is a function of the entries over the complement set`(i`) - set`-i(i`-i). With these
observations, applying Lemma A.14 with f = φ',X = r, Y = km)d(i£ ɪ d(L)h('-I), Z = θ'-ι and using the fact
that ∣θ'-ι∣ ≤ 1 which holds due to I-Lipschitzness of σ∕s, we conclude that
∣Ex~N (0,i)[(φ'(h?) - E[φ'(h (^)])θ'-ι ]∣≤ 2S EIZm(Y )∣.
Here, S is the smoothness of σ' and Lipschitz constant of φ'. Ib conclude, we need to assess the E ∣zm(Y)∣
term. Now note that starting from x, each entry of h('T) is obtained by applying a sequence of inner
products with {k(i)}'-1 and activations σ'(∙), which implies h(：-I) is a ∏'-1 ∣∣k("∣∣'2-Lipschitz function
of set`-i(i'-ι). This implies Y is a Lipschitz function of a Gaussian vector with Lipschitz constant LY =
Ikm)d(i` I dl)∖ ∏'-1 Uk(i)忆.Hence，Zm(Y) ObeyS the tail bound
t2
P(IZm(Y)I ≥ t) ≤ 2eχp(-2L2).
Using a standard integration by parts argument the latter implies that
E Izm(Y)I ≤ √2πLγ.
Thus,
`-1
la`- a'-ι∣ ≤ Ikmo)d(i`-i,d`)l ∏ M)忆.
i=1
concluding the upper-bound on each summand of (A.17). Combining such upper bounds (A.17) implies
D	'-1
IaD - ao∣ ≤ √8πS ∑ Ikmo)d(i'_1,d')∣ ∏ Uk(i) E ：= κi.
This concludes the proof of Lemma A.13.
17
Under review as a conference paper at ICLR 2019
A.3.1.1 Proof of Lemma A.14 Using the independence of X, Z, we can write
E[f(X+Y)Z]=E[f(X+E[Y])Z]+E[(f(X+Y)-f(X+E[Y]))Z]
=E[f(X+E[Y])]E[Z]+E[(f(X+Y)-f(X+E[Y]))Z]
=E[f(X+Y)]E[Z]+E[f(X+E[Y])-f(X+Y)]E[Z]+E[(f(X+Y)-f(X+E[Y]))Z].
This implies
E[f(X+Y)Z]-E[f(X+Y)] E[Z] = E[f(X+E[Y])-f(X+Y)] E[Z]+E[(f(X+Y)-f(X+E[Y]))Z].
Now, using Lipschitzness of f, we deterministically have that ∣f(X + E[Y ]) - f(X + Y )∣ ≤ L∣Y - E[Y ]∣ =
L∣zm(Y )∣. Similarly, ∣(f(X + Y ) - f(X + E[Y ]))Z ∣ ≤ L∣zm(Y )Z∣. Taking absolute values we arrive at
∣E[f(X+E[Y])-f(X+Y)]E[Z]+E[(f(X+Y)-f(X+E[Y]))Z]∣≤LE[∣zm(Y)∣(∣Z∣+∣E[Z]∣)].
This immediately implies (A.19). If ∣Z∣ ≤ 1 almost surely, we have E[∣zm(Y )Z∣] ≤ E[∣zm(Y )∣] and ∣ E[Z]∣ ≤ 1
which yields the 2L E[∣zm(Y )∣] upper bound.
A.3.1.2 Proof of Lemma A.15 Informally, this lemma is obvious via the tree visualization. To formally
prove this lemma We use an induction argument. For h(I) the result is trivial because h(1) =(k(I), x(i)) which
is a weighted sum of entries corresponding to set1 (i). Suppose the claim holds for all layers less than or equal
to ' - 1 and h(('-1) = f'-ι(xset`-ι(i)). For layer ', We can use the fact that set`(i) = U；% set'-1((i - l)d` + j)
to conclude that
d`
h = ∑ kj')σ'-1(h('-1)d'+j)
j=1
d`
=∑ kj σ'-1 (f'-1 (xset'-ι ((i-1)d'+j) )) ：= f'(xset'(i)).
j=1
The latter is clearly a deterministic function of Xset'(i). AlSo it is independent of entry i because it simply chunks
the vector Xset'(i) into d` sub-vectors and returns a sum of weighted functions of these sub-vectors. Here, the
weights arethe entries of k(') and the functions are given by σ'-1(f'-1(∙)) (also note that the activation output
is simply h(') =。'(立6现⑴))).
A.4 Proofs for learning the convolutional kernels (Proof of Theorem 4.8)
The first part of the theorem follows trivially by combining Theorems 4.2 and 4.7. To translate a bound on the
tensor spectral norm of Tn - LCNN to a bound on learning the kernels, requires a perturbation argument for
tensor decompositions. This is the subject of the next lemma.
Lemma A.16 Let L = Y 0D=1 v` be a rank one tensor with
perturbation tensor obeying ∣∣∣E ∣∣∣ ≤ δ. Set
D
u1, u2,..., UD = arg max (L,㊁ u` J	subject to
U 1,u2,∙∙∙,uD ∖	'=1	/
{vi }iD=1 vectors of unit norm. Also
∣∣u1 Hg = IlU 2 Il'2 = ... = IlU D Il'2
assume E is a
= 1.	(A.21)
Then we have
D
∏ ∣UiVi∣ ≥ 1 - 2δ∕γ.
i=1
The proof of Theorem 4.8 is complete by applying Lemma A.16 above with v` = k(`), u` = k('), Y = Ocnn and
E = Tn - LCNN. All that remains is to prove Lemma A.16 which is the subject of the next section.
A.4. 1 Proof of Lemma A.16
To prove this lemma first note that for any two rank one tensors we have
DD	D
(N U', N v`) = ∏ (Ui, Vi).
'=1	'=1	i=1
Using this equality together with the fact that the vectors {U'}'D=1 are a maximizer for (A.21) we conclude that
(L + E, N v` ) ≤ (L + E, N u`
'=1	'=1
≤ ∣(L,NU')
+
),
∣(E,ND U')∣,
D
≤ Y ∏ ∣(Ui, Vi] ∣ + δ.
i=1
(A.22)
18
Under review as a conference paper at ICLR 2019
Furthermore, note that
(L + E, 0 V)= Y +(E, 0 v) ≥ Y - δ.
Combining (A.22) and (A.23) we conclude that
D
Y ∏ ∣ (ui, Vi) ∣ + δ ≥ Y - δ,
i=1
(A.23)
concluding the proof.
B Lower bounds on CNN gain
Lemma B.1 Consider a CNN model per SeCtiOn 2 with fCNN () the corresponding CNN function per (2.1). We
have the following lower bound on the nonlinearity parameter a CNN = ∏D=ι E[Φ'(h 1'))] from Definition 4.3.
•	ReLU model: φ(x) = max(0, x) with the added assumption that the kernels have mean larger than
zero and are modestly diffused. Specifically, assume
(IT k )
UkrU'2
Then
α CNN ≥
≥ 4.
1
.
4
(B.1)
•	softplus model: φ(x) = log (1 + ex) with the added assumption that the kernels have mean larger
than zero, are modestly diffused and have a sufficiently large Euclidean norm. Specifically, assume
≥ 10 and Uk'Hg ≥ L	(B.2)
Then
αCNN ≥ 0.3.
Proof ReLU: In this case note that
αCNN = ∏ nh(`))]
=∏ "八}]
=∏P{M? ≥ 0}
'=1
Thus using t = E [h：')] we arrive at
“	r	[一"划)2
p{hi? < 0[≤ e 2πr=1"kr"22 .	(B.3)
Since the entries of h(')and h(')are i.i.d. we use H(')and H(')to denote the corresponding distributions. We
note that
E [hi?]= E [”, h(')[i']>]
=(ITk(')) E[H⑶]
=(1tk(')) E [“H(')]
≥ (1tk(')) φ (E[H⑶])
=(1t4('))• E[H(')].
where in the last inequality we used the fact that (1tk(')) ≥ 0 and applied Jenson,s inequality for a convex φ.
Applying this inequality recursively we arrive at
'	'
E [h??] ≥ ∏ (1tk(r)) E[φ(g)] = √2π ∏ (1tk⑺).	(B.4)
Using the latter in (B.3) we arrive at
(	ʌ ɪ M (IT W))2	(	ʌ	ɪ M (ITE2
p[hi') < 0] ≤e-4π	⇒	p(h(') ≥ 0] ≥ 1-e-4π ∏τ	.
19
Under review as a conference paper at ICLR 2019
Thus using the diffusion assumption (B.2) in the latter inequality we arrive at
°cnn ≥
∏ (1 - e 看 j")
'=1 ∖	'
Thus using the fact that for 0 ≤ x1,x2,...,Xn ≤ 1 we have
n
n
∏(1-Xi) ≥ 1 - ∑Xi,
we conclude that
i=1
i=1
D 1 ..22
αcNN ≥1 - ∑ e 4π
2=1
≥1 -
≥1 -

D _ ɪ *x
e 4π	dx
∞	_ ɪ ^2^
e 4π dx
=1 -
=1 -
E1(套)
log V 2
E1(套)
log 16
1
To	≥ 4.
In the above E1(x) = fj et-dt and we used the fact that E1 (4∏) ≤ 1.016 and V ≥ 4. Thus,
αcNN ≥ -.
4
Softplus: For the softplus activation we have
αCNN=∏ EM (h ：，)]
D
∏ E
2=1
1
L1 +e-h i?
∏ E (1 -
'=1	∖
1
K
∏ (1 - E
'=1 ∖
1
L 1 + J %
Thus using the fact that for 0 ≤ X1 , X2 , ..., Xn ≤ 1 we have
n
n
∏(1 - Xi) ≥ 1 - ∑Xi,
we conclude that
i=1
i=1
D
acNN ≥1 - ∑ E
2=1
D
≥1 - ∑ E
2=1
1
LI+eh n
L + e ` .J
-* * 1{hi?<E[hi?]/2} + "{戏汨戏]/2} 一
1 + eh (`
≥1 - S E t1{h i")<E[h 曲/2}]-S E
-1{h(7≥E[hi?同一
D
=1 - SP{
D
≥1 - SP{
D
hi?< E[h?]/2}- ∑ E
D
h?< E[hR]∕2}-∑
L 1 + eh i")	」
-1{h aE[h */2} 一
1 + eh (?
1
1 + eE[hi')]/2 .
(B.5)
We bound the expected value of the hidden unites similar to the argument for ReLU activations. The only
difference is that in the identity (B.4) we need to use the softplus activation in lieu of the ReLU activation for φ.
20
Under review as a conference paper at ICLR 2019
Therefore, (B.4) changes to
``
E [hi')] ≥ ∏ (ITk(r)) E[φ(g)] = 0.806059 ∏ (ITk(r)) .	(B.6)
r=1	r=1
Similar to the ReLU argument, We note that hi') is a LiPschitz function of a GaUssian random vector (g) with
LiPsChitz constant equal to ∏r=ι ∣∣k(r) 卜.Using Lipschitz concentration of Gaussians we thus have
(	a___________t2. .___

p] hi') - E [h(')] < -t I ≤ e 2∏r=Jk(R'2
(B.7)
Thus using t = E [h(')]∕2 We arrive at

Pdh i') < E[h (')]∕2∣≤ e 8 ∏
Ok⅛
(B.8)
Thus using the diffusion assumption (B.2) with ν ≥ 4 we have
D-1	D-1
∑ P(h(') < E[h(')]∕2}≤ ∑
e-0.1ν2'
≤
≤
∫0
∫0
D-1e-0.1ν2xdx
+∞ e-0.1ν2x dx
E1(0.1)
Also using (B.6) and assuming ∣∣k(') ^` ≥ 1 we have
log ν2
D-1
∑
'=1
1 + eE [h (') ]/2
D-1
≤'∑=11+e
,0.4030295 ∏r=Jlτk(r) )
D-1
≤∑
e-0.4030295∏r=1(1τ k(r))
'=1
1
1
D-1 -0.4030295∙ν'∙∏r=Jk(r)∣∣
≤ ∑ e	r=	`2
'=1
D-1	`
Σ-0.4030295∙ν'
e
'=1
≤
≤
∫0
∫0
DT -0.4030295∙νx
e
+ ∞ -0.4030295∙νx
e
E1(0.4030295)
log ν
Plugging the latter two inequalities in (B.5), allows to conclude that for ν ≥ 10
αCNN =1 -
E1(0.1) E1(0.4030295)
-----------------------
log ν2
log ν
≥0.3.
C Centered ERM and Resolving sign & scaling ambiguities
We note that DeepTD operates by accurately approximating the rank one tensor RD=I k(') from data. Therefore,
DeePTD can only recover the convolutional kernels uP to Sign/Scale Ambiguities (SSA). In general, it may not
be possible to recover the ground truth kernels from the training data. For instance, when activations are ReLU,
the norms of the kernels cannot be estimated from data as multiplying a kernel and dividing another by the same
positive scalar leads to the same training data. However, we can try to learn a good approximation fCNN() of the
network fcnn() to minimize the risk E[(∕cnn(x) - /Cnn(x))2].
To this aim, we introduce Centered Empirical Risk Minimization (CERM) which is a slight modification of
Empirical Risk Minimization (ERM). Let us first describe how finding a good fCNN() can be formulated with
CERM. Given n i.i.d. data points {(xi,yi)}n=ι Z (x,y), and a function class F, CERM applies ERM after
21
Under review as a conference paper at ICLR 2019
centering the residuals. Given f ∈ F , define the average residual function ravg(f) = 1 ∑n=ι yi-f(xi). We
define the Centered Empirical Risk Minimizer as
1n
f = min - ∑(yi - f (xi ) - ravg(f ))2,
f∈F n i=1
1n	1 n	2
=minn ∑ (yi-f(xi) - E[(yi- f (xi))]) - n (∑yi-f(xi) - E[(yi- f (xi))]) .	(C.1)
The remarkable benefit of CERM over ERM is the fact that, the learning rate doesn’t suffer from the label or
function bias. This is in similar nature to the DeepTD algorithm that applies label centering. In the proofs (in
particular Section C.2, Theorem C.2) we provide a generalization bound on the CERM solution (C.1) in terms
of the Lipschitz covering number of the function space. While (C.1) can be used to learn all kernels, it does
not provide an efficient algorithm. Instead, we will use CERM to resolve SSA after estimating the kernels via
DeepTD. Interestingly, this approach only requires a few (O(D)) extra training samples. Inspired from CERM,
in Section C.1, we propose a greedy algorithm to address SSA. We will apply CERM to the following function
class with bounded kernels,
FkB ：= {f : Rp ↦ R ∣ fis a CNN function of the form (2.1) With kernels {β'k(') }：ɪ With ∣β'∣ ≤ BDD }.
(C.2)
In Words this is the function class of all CNN functions With kernels the same as those obtained by DeepTD up
to sign/sCale ambiguities {β'}D=ι where the maximum scale ambiguity is B.
Theorem C.1 Let fcnn() be defined via (2.1) with convolutional kernels {k(') }D=1 obeying IIk⑶忆≤ b1/d
for some B > 0 and consider the function class Fk B above with the same choice of B. Assume we have n i.i.d.
samples (xi, yi) ~ (x,y) where x ~ N(0, Ip) and y = fCNN(x). Supposefor some ε ≤ B,
n ≥ cB2D log ( CDBp ) max (1, -22) ,
holds for fixed numerical constants c, C > 0. Then the solution f to the CERM problem (C.1) obeys
E [ (f(x) - fCNN(x))2 ] ≤ min E[(f(x) - fCNN(x))2] + ε.
L	」f∈F^,B
(C.3)
on a new sample x ∈ N(0, Ip) with probability at least 1 - e-γn - 4n exp(-p) with γ > 0 an absolute constant.
The above theorem states that CERM finds the sign/scale ambiguity that accurate estimates the labels on neW
data as long as the number of samples Which are used in CERM exceeds the depth of the netWork by constant/log
factors. In the next section We present a greedy heuristic for finding the CERM estimate.
C.1	Greedy algorithm for resolving sign and scale ambiguities
In order to resolve SSA, inspired from CERM, We propose Algorithm 1 Which operates over the function class,
F^ := {γf : Rp ↦ R ∣ fisa CNN of the form (2.1) with kernels {β'k(')}: 1 with β' ∈ {1, -1}, Y ≥ 0}.
(C.4)
It first determines the signs g` by locally optimizing the kernels and then finds a global scaling Y > 0. In the first
phase, the algorithm attempts to maximize the correlation between the centered labels yc,i = yi -n-1 ∑in=1 yi and
1n
the ∕cnn() predictions given by yc,i = yi - n ∑i=ι yi. It goes over all kernels one by one and it flips a kernel
(k(') → -k(')) if flipping increases the correlation. This process goes on as long as there is an improvement.
Afterwards, we use a simple linear regression to get the best scaling Y by minimizing the centered empirical loss
∑n=ι(yc,i - γyc,i)2. While our approach is applicable to arbitrary activations, itis tailored towards homogeneous
activations (φ(cx) = cφ(x)). The reason is that for homogeneous activations, function classes (C.2) and (C.4)
coincide and a single global scaling Y is sufficient. Note that ReLU and the identity activation (i.e. no activation)
are both homegeneous, in fact they are elements of a larger homogeneous activation family named Leaky ReLU.
Leaky ReLU is parametrized by some scalar 0 ≤ β ≤ 1 and defined as follows
LReLU(x) ={xifx≥0,
βx if x < 0.
C.2 Generalization bounds for CERM
In this section we prove a generalization bound for Centered Empirical Risk Minimization (CERM) (C.1). The
following theorem shows that using a finite sample size n, CERM is guaranteed to choose a function close to
population’s minimizer. For the sake of this section If IL∞ will be the Lipschitz constant of a function.
Theorem C.2 Let F be a class of functions f : Rp → R. Suppose SuPf ∈f ∣∣f ∣∣l∞ ≤ R. Let {(xi,yi )}in=ι Z
(x, y) be i.i.d. data points where x ~ N(0, Ip) and y is so that y - E[y] is K subgaussian. Suppose F has
∣∣ ∙ ∣∣ l∞ , δ-covering bound obeying log N ≤ S log C for some constants S ≥ 1, C ≥ R. Given ε ≤ K = K + R,
22
Under review as a conference paper at ICLR 2019
Algorithm 1 Greedily algorithm for resolving sign/scale ambiguities for Leaky ReLU activations.
1:	procedure MAXCORR
2:	Inputs: Data (yi, Xi)n=ι, estimates {M')}D=r
3:	Pmax 一 ∣Corr({Me) }乙,0)∣, FLIP-TRUE.
4:	while FLIP do
5:	FLIP—FALSE.
6:	for 1 ≤ ` ≤ D do
7:	p — ∣Corr(很 ⑴，...，一k⑻,..., MD)}, 0)∣.
8:	if ρ > ρmax then
9:	ρmax — ρ
io：	k ⑶—-^ ⑹
11:	FLIP—TRUE
12： Y ― Corr({^ ⑹}D=ι,1).
13: return kernels {M')}D=ι, scaling γ.
Algorithm 2 Return the correlation between centered labels.
1: procedure CORR({M')}D=1,opt)
2：	yi — fCNN({M')}D=i； Xi).
3：	yc,i	—	yi- n ∑n=ι yi	and	yc,i	— yi- 1 ∑n=ι	yi
4： ρ ― ∑n=ι yc,iyc,i.
5: return P if opt = 0, ρ∕(∑=ι y2,i) if opt = 1.
suppose n ≥ O(max{ε-1,ε-2}K2slog C PK) forsome C' > 0. Then the CERM output (C.1) obeys
E[(f(x) - y - E[(f(x)-y)])2] ≤ min E[(f (x) - y - E[(f(x) - y)])2] + ε.
(C.5)
with probability 1 - exp(-O(n)) - 4n exp(-p).
Proof Consider the centered empirical loss that can alternatively be written in the form
E(f) = * 1 ∑ zm(yi - f (xi))2 -与(∑ zm(yi - f (xi)))2 - E[zm(f (x) - y)2].
n i=1	n i=1
1n	1 n
=—∑zm(zm(yi - f(xi)) ) — - (∑zm(yi - f (xi)))
n i=1	n i=1
：=Tl + T2	(C.6)
To prove the theorem, we will simply bound the supremum supf ∈F ∣E(f)∣ ≤ supf ∈F ∣T1 + T2∣. Pick a δ
covering Fδ of F With size S log C where δ will be determined later in this proof. We first bound Ef) for
all f ∈ Fδ. Given a fixed f, observe that ∣∣zm(zm(yi - f (xi))2)^ψι ≤ O(K)2 = O(K + R)2 which follows
from ∣∣zm(yi - f (Xi))I∣ψ2 ≤ O(K) = O(K + R). Applying subexponential concentration, since Ti is sum of
i.i.d. subexponentials, we have
P(∣T1∣ ≥ ε) ≤ exp(-O(nmin{ε2∕K2,ε∕K})).	(C.7)
Next, since ∣∣zm(yi- f(xi))∣∣ψ2 ≤ O(K), we can conclude that ∣ ɪ ∑i=ι zm(yi - f(xi))∣∣ψ2 ≤
O(K)∕√n =⇒ ∣∣T2 ∣∣ψι ≤ O(K)2∕n. Using (C.7) for Ti and the subexponential tail bound for T2 holds
when ε ≤ K, and assuming the number of samples n obeys n ≥ O(max{ε-1,ε-2}Kt2s log C), then for all
cover elements
∣Ti∣+∣T2∣ ≤2ε.
holds with probability at least 1 - exp(-O(n)). To conclude the proof, we need to move from the cover Fδ
to F. Pick f ∈ F and its δ neighbor fδ ∈ Fδ. Utilizing the deterministic relation ∣zm(X)∣ ≤ ∣X∣ + ∣ E[X]∣ and
using the fact that fδ is in a δ neighborhood of f, we arrive at the following bounds
∣zm(f (x) - fδ(x))∣ ≤ δ(∣∣x∣∣'2 + E[∣∣x∣∣g]).	(C.8)
∣zm(f(x) +fδ(x)-2y)∣ ≤ 2R(∣∣x∣∣g + E[∣∣x∣∣g]) + 2K∣zm(y)∣.	(C.9)
Next observe that, with probability at least 1 - 4nexp(-p), all Xi,yi obey ∣∣Xi∣∣3 ≤ O(√p), ∣zm(yi)∣ ≤
O(K√p). Combining this with (C.8), we conclude that for all 1 ≤ i ≤ n
∣zm(f (xi) - yi)2 - zm(fδ(xi) - yi)2∣ ≤ O(Kδp).	(C.10)
23
Under review as a conference paper at ICLR 2019
Expanding the square differences in the same way, an identical argument shows the following two deviation
bounds,
∣ E[zm(f(xi) -yi)2 -zm(fδ(Xi) -yi)2]∣ ≤ O(Kδp),
nn
—1(∑ zm(yi - f (xi)))2 - (∑ zm(yi - fδ(Xi)))2∣ ≤ O(Kδp).
n i=1	i=1
(C.11)
Combining these three inequalities ((C.10) and (C.11)) and substituting them in (C.6), we conclude that for all
neighbors fδ , f,
_	∣E(f) - E(fδ)∣ ≤ O(Kδp).
Next we set δ = cε∕(ρK) fora sufficiently small constant C > 0, to find that with probability at least 1 - exp(-n),
SuPf∈f ∣E(f )∣ ≤ ε holds as long as the number of samples n obeys n ≥ O(max{ε-1,ε-2}K2slog CcK). We
define Lerm(f) = 1 ∑i=ι(yi - f (xi) - rvg(f ))2 and Lpop(f) = E[zm(f (x) - y)2]. We also denote the
CERM minimizer ferm = arg minf∈F L(f) and population minimizer fpop = minf ∈F Lpop (f). Inequality
(C.5) follows from the facts that we simultaneously have ∣E (ferm )∣ ≤ O(ε) and ∣E(fpop)∣ ≤ O(ε) which
implies that
Lpop (ferm ) ≤ Lerm (ferm ) + O(ε) ≤ Lerm (fpop) + O(ε) ≤ Lpop (fpop) + O(ε),
concluding the proof.	■
C.3 Proof of Theorem C.1
In this section we will show how Theorem C.1 follows from Theorem C.2. To this aim we need to show that
Fk B has a small LiPscshitz covering number. We construct the following cover F' for the set FkB ∙ Let
B' = B1/D. Pick a δ ≤ B' '2 cover C of the interval [-B', B'] which has size 2B'∕δ. Let Ci be identical copies
of C . We set
F' = {fCNN(β'k(')) ∣ β' ∈ C', 1 ≤ ' ≤ D}.
In words, we construct CNNS by picking numbers from the cartesian product Ci X …CD and scaling {k(') }Da
with them. We now argue that F' provides a cover of F. Given f ∈ F with scalings e`, there exists f ' ∈ F'
which uses scalings β' such that ∣β' 一 β'∣ ≤ δ. Now, let f` be the function with scalings βi until i = ` and βi for
i > `. Note that f0 = f, fD = f'. With this, we write
D
Ilf -f 1∣L∞ ≤ ∑ ∣∣fi+i-fi∣∣L∞.
i=1
Observe that fi-1 and fi have equal layers except the ith layer. Let g1 be the function of the first i-1 layers and g2
be the function of layers i+ 1 to D. We have that fi+1(X) -fi(X) = g2(φ(Ki(g1(X)))) - g2 (φ(Ki' (g1 (X))))
where Ki, Ki differ in the ith layer kernels of f and f′ created from βik(i) and β'k(i) respectively. Also,
observe thatgi is B'i-i Lipschitz and g2(φ(∙)) is B'D-i Lipschitz functions. Hence,
∣g2(φ(Ki(g1(X)))) - g2(φ(Ki'(g1(X))))∣ ≤ B'D-i∣Ki(g1(X)) - Ki'(g1(X))∣ ≤ B'D-iδB'i-1 ≤ δB'D-1.
(C.12)
Summing over all i, this implies that If - f' IL∞ ≤ DδB'D-1 . Recalling ∣F'∣ ≤ (2B'∕δ)D and setting
δ = ε∕(DB'D-i), the ε covering number of F^ b is N ≤ (2DB'D∕ε)D = (2DB∕ε)D which implies
log N = D log(2DεB). Now, since all kernels have Euclidean norm bounded by B', We have ∣∣fCNN()I∣L∞ ≤ B
and If IL∞ ≤ B for all f ∈ F. This also implies Izm(fCNN(X))Iψ2 = O(B). Hence, we can apply Theorem
C.2 to conclude the proof of Theorem C.1.
24