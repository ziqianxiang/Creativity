Under review as a conference paper at ICLR 2019
Perception-Aware Point-Based Value
Iteration for Partially Observable
Markov Decision Processes
Anonymous authors
Paper under double-blind review
Ab stract
Partially observable Markov decision processes (POMDPs) are a widely-used
framework to model decision-making with uncertainty about the environment and
under stochastic outcome. In conventional POMDP models, the observations that
the agent receives originate from fixed known distribution. However, in a variety
of real-world scenarios the agent has an active role in its perception by selecting
which observations to receive. Due to combinatorial nature of such selection pro-
cess, it is computationally intractable to integrate the perception decision with the
planning decision. To prevent such expansion of the action space, we propose a
greedy strategy for observation selection that aims to minimize the uncertainty in
state. We develop a novel point-based value iteration algorithm that incorporates
the greedy strategy to achieve near-optimal uncertainty reduction for sampled be-
lief points. This in turn enables the solver to efficiently approximate the reach-
able subspace of belief simplex by essentially separating computations related to
perception from planning. Lastly, we implement the proposed solver and demon-
strate its performance and computational advantage in a range of robotic scenarios
where the robot simultaneously performs active perception and planning.
1	Introduction
In the era of information explosion it is crucial to develop decision-making platforms that are able to
judiciously extract useful information to accomplish a defined task. The importance of mining useful
data appears in many applications including artificial intelligence, robotics, networked systems and
Internet of things. Generally in these applications, a decision-maker, called an agent, must exploit
the available information to compute an optimal strategy toward a given objective.
Partially observable Markov decision processes (POMDPs) provide a framework to model sequen-
tial decision-making with partial perception of the environment and under stochastic outcomes. The
flexibility of POMDPs and their variants in modeling real-world problems has led to extensive re-
search on developing efficient algorithms for finding near-optimal policies. Nevertheless, the major-
ity of previous work on POMDPs either deal with sole perception or sole planning.
While independent treatment of perception and planning deteriorates performance, an integrated
approach usually becomes computationally intractable. Thereupon, one must establish a trade-off
between optimality and tractability when determining how much perception and planning rely on
each other. We show that by restricting the perception to the class of subset selection problems
and exploiting submodular optimization techniques, it is possible to partially decouple computing
perception and planning policies while considering their mutual effect on the overall policy value.
In this work, we consider joint perception and planning in POMDPs. More specifically, we consider
an agent that decides about two sets of actions; perception actions and planning actions. The per-
ception actions, such as employing a sensor, only affect the belief of the agent regarding the state
of the environment. The planning actions, such as choosing navigation direction, are the ones that
affect the transition of the environment from one state to another. In subset selection problems, at
each time step, due to power, processing capability, and cost constraints, the agent can pick a sub-
set of available information sources along a planning action. The subset selection problem arise in
various applications in control systems and signal processing, in wireless sensor networks, as well
1
Under review as a conference paper at ICLR 2019
as machine learning (Hashemi et al., 2018) and have been widely-studied (Shamaiah et al., 2010;
Krause & Guestrin, 2007). However, the previous work on sensor selection problems assume that
the planning strategy is known, while in this work, we simultaneously learn a selection strategy and
a planning strategy.
1.1	Related Work
Exact POMDP solvers optimize the value function over all reachable belief points. However, finding
exact solution to POMDPs is PSPACE-complete (Papadimitriou & Tsitsiklis, 1987) which deems
solving even small POMDPs computationally intractable. This has led to extensive search for near-
optimal algorithms. A common technique is to sample a finite set of belief points that approximate
the reachable subspace of belief and apply value iteration over this set, e.g., (Sondik, 1978; Cheng,
1988; Lovejoy, 1991; Zhang & Zhang, 2001; Spaan & Vlassis, 2005; Pineau et al., 2006). Pineau
et al. (2006) proved that the errors due to belief sampling is bounded where the bound depends on the
density of the belief set. A well-established offline POMDP solver is SARSOP (Kurniawati et al.,
2008). SARSOP, similar to HSVI (Smith & Simmons, 2012), aims to minimize the gap between
the lower and upper bounds on the value function by guiding the sampling toward the belief points
that are reachable under union of optimal policies. In this paper, we show that the proposed greedy
observation selection scheme leads to belief points that are on expectation close to the ones from the
optimal (with respect to uncertainty reduction) set of observations, and hence value loss is small.
An instance of active perception is dynamic sensor selection. Kreucher et al. (2005) proposes a rein-
forcement learning approach that uses Renyi divergence to compute utility of sensing actions. Joshi
& Boyd (2009) formulated a single step sensor selection problem as semi-definite programming,
however, it lacks theoretical guarantee. In Kalman filtering setting, Shamaiah et al. (2010) devel-
oped a greedy selection scheme with near-optimal guarantee to minimize log-determinant of the
error covariance matrix of estimated state. Some prior work such as (Spaan, 2008; Spaan & Lima,
2009; Natarajan et al., 2015) model active perception as a POMDP. However, the most relevant work
to ours are that of (Araya et al., 2010; Spaan et al., 2015; Satsangi et al., 2018). Araya et al. (2010)
proposed ρPOMDP framework where the reward depends on entropy of the belief. Spaan et al.
(2015) introduced POMDP-IR where the reward depends on accurate prediction about the state.
Satsangi et al. (2018) established an equivalence property between ρPOMDP and POMDP-IR. Fur-
thermore, they employed the submodularity of value function, under some conditions, to use greedy
scheme for sensor selection. The main difference of our work is that we consider active perception
as a means to accomplishing the original task while in these work, the active perception is the task
itself and hence the POMDP rewards are metrics to capture perception quality.
The problem of selecting an optimal set of sensors from a ground set under cardinality constraint is
NP-hard (Williams & Young, 2007). This hardness result has motivated design of greedy algorithms
since they make polynomial oracle calls to the objective function. Additionally, if the objective
function is monotone non-decreasing and submodular, Nemhauser et al. (1978) showed that a greedy
selection achieves (1 - 1/e) approximation factor. Mirzasoleiman et al. (2015) and Hashemi et al.
(2018) developed randomized greedy schemes that accelerate the selection process for monotone
submodular and weak-submodular objective functions, respectively. Krause & Guestrin (2007);
Krause & Golovin (2014) have introduced different submodular information-theoretic objectives for
greedy selection and have studied the theoretical guarantees of their maximization under different
constraints. Here, we use the entropy of belief to capture the level of uncertainty in state and aim
to select a subset of sensors that leads to maximum expected reduction in entropy. We employ the
monotonicity and submodularity of the proposed objective to establish near-optimal approximation
factor for entropy minimization.
1.2	Contributions
A summary of our contributions are as follows:
•	Formulating the active perception problem for POMDPs: We introduce a new mathemat-
ical definition of POMDPs, called AP2-POMDP, that captures active perception as well as
planning. The objective is to find deterministic belief-based policies for perception and
planning such that the expected discounted cumulative reward is maximized.
2
Under review as a conference paper at ICLR 2019
•	Developing a perception-aware point-based value iteration algorithm: To solve AP2-
POMDP, we develop a novel point-based method that approximates the value function
using a finite set of belief points. Each belief point is associated with a perception action
and a planning action. We use the near-optimal guarantees for greedy maximization of
monotone submodular functions to compute the perception action while the planning ac-
tion is the result of Bellman optimality equation. We further prove that greedy perception
action leads to an expected reward that is close to that of optimal perception action.
2	Problem Formulation
This section starts by giving an overview of the related concepts and then stating the problem.
2.1	Preliminaries
The standard POMDP definition models does not capture the actions related to perception. We
present a different definition which we call AP2-POMDP as it models active perception actions as
well as original planning actions. The active perception actions determine which subset of sensors
(observations) the agent should receive. We restrict the set of states, actions, and observations to be
discrete and finite.
2.1.1	POMDP with Perception Action
We formally define an AP2-POMDP below.
Definition 1. An AP2-POMDP is a tuple P = (S, A, k,T,Ω, O, R, Y), where
•	S is the finite set of states.
•	A = Apl × Apr is the finite set of actions with Apl being the set of planning actions and
Apr being the set of perception actions. Apr = {δ ∈ {0,1}n∣ ∣δ∣o ≤ k} constructs
an n-dimensional lattice. Each component of an action δ ∈ Apr determines whether the
corresponding sensor is selected.
•	k is the maximum number of sensor to be selected.
•	T : S × Apl × S → [0, 1] is the probabilistic transition function.
•	Ω = Ω1 X Ω2 X ... X Ωn is the partitioned set OfobServations, where each Ω% corresponds
to the set of measurements observable by sensor i.
•	O : S x A x Ω → [0,1] is the probabilistic observation function.
•	R : S X Apl → R is the reward function, and
•	γ ∈ [0, 1] is the discount factor.
At each time step, the environment is in some state s ∈ S. The agent takes an action β ∈ Apl
that causes a transition to a state s0 ∈ S with probability P r(s0|s, β) = T(s, β, s0). At the same
time step, the agent also picks k sensors by δ ∈ Apr. Then it receives an observation ω ∈ Ω with
probability Pr(ω∣s0, β, δ) = O(s0, β, δ, ω), and a scalar reward R(s, β).
Assumption 1. We assume that the observations from sensors are mutually independent given
the current state and the previous action, i.e., ∀I1,I2 ⊆ {1,2,...,n},Iι ∩ I2 = 0 :
Pr(Si1∈I1 ωi1, Si2∈l2 ωi2 |s, β) = Pr(Si1∈I1 ωi1 |s, e)Pr(Si2∈I2 ωi2 |s, β).
Let Z(δ) = {i∣δ(i) = 1} to denote the subset of sensors that are selected by δ. If Assumption 1
holds, then:
Pr3s0,β,6 = Pr ( U ωils0,β,δ j = Y Oi(S0,β,ωi工	⑴
i∈ζ(δ)	i∈ζ(δ)
where Pr(ωi∣s0,β) = Oi(s0,β,ωi).
3
Under review as a conference paper at ICLR 2019
The belief of the agent at each time step, denoted by bt is the posterior probability distribution of
states given the history of previous actions and observations, i.e., ht = (a0, ω1 , a1 , . . . , at-1, ωt).
A well-known fact is that due to Markovian property, a sufficient statistics to represent history of
actions and observations is belief (Astrom, 1965; SmanWood & Sondik, 1973). Given the initial
belief b0, the following update equation holds between previous belief b and the belief bba,ω after
taking action a = (β, δ) and receiving observation ω:
b0a,ω ( 0) __ Pr(WS0, B, S) Es Pr(Sls, Ie)b(S)
b (S ) =	Pr (ω∣β,δ)
=Qi∈ζ(δ) Oi(s0,β,ωi) Ps T(s,β,s0)b(s)
=Pso Qi∈ζ(δ) Oi(s0,β,ωi) Ps T(s,β,s0)b(s).
(2)
The goal is to learn a deterministic policy to maximize E[Pt∞=0 γtR(St, βt)∣b0]. A deterministic
policy is a mapping from belief to actions π : B → A, Where B is the set of belief states. Note that
B constructs a (∣S∣ - 1)-dimensional probability simplex.
The POMDP solvers apply value iteration (Sondik, 1978), a dynamic programming technique, to
find the optimal policy. Let V be a value function that maps beliefs to values in R. The folloWing
recursive expression holds for V :
Vt(b) = max X b(S)R(S, a) + γ X P r(ω∣b, a)Vt-1(b0ba,ω) .	(3)
∖s∈S	ω∈Ω	)
The value iteration converges to the optimal value function V * which satisfies the Bellman,s opti-
mality equation (Bellman, 1957). Once the optimal value function is learned, an optimal policy can
be derived. An important outcome of (3) is that at any horizon, the value function is piecewise-linear
and convex (PWLC) (Smallwood & Sondik, 1973) and hence, can be represented by a finite set of
hyperplanes. Each hyperplane is associated with an action. Let a's to denote the corresponding
vectors of the hyperplanes and let Γt to be the set of α vectors at horizon t. Then,
Vt(b) = max α.b.	(4)
α∈Γt
This fact has motivated approximate point based solvers that try to approximate the value function
by updating the hyperplanes over a finite set of belief points.
2.1.2 Submodularity
Since the proposed algorithm is founded upon the theoretical results from the field of submodular
optimization, here, we overview the necessary definitions. Let X to denote a ground set and f a set
function that maps an input set to a real number.
Definition 2. Set function f : 2X → R is monotone nondecreasing if f(T1) ≤ f(T2) for all
T1 ⊆ T2 ⊆ X.
Definition 3. Set function f : 2X → R is submodular if
f(T1∪{i})-f(T1) ≥ f(T2 ∪ {i}) - f(T2)
for all subsets T1 ⊆ T2 ⊂ X andi ∈ X\T2. The term fi(T1) = f(T1 ∪ {i}) - f(T1) is the marginal
value of adding element i to set T1 .
Monotonicity states that adding elements to a set increases the function value while submodularity
refers to diminishing returns property.
2.2 Problem Definition
Having stated the required background, next, we state the problem.
Problem 1. Consider a AP2-POMDP P = (S, A, k,T,Ω, O, R, γ) and an initial belief bo. We aim
to learn a policy π(b) = (β, δ) such that the expected discounted cumulative reward is maximized,
i.e,
∞
π* = argmax E[X γtR(St, π(bt))∣b0].	(5)
π	t=0
4
Under review as a conference paper at ICLR 2019
1
(a) Entropy
0.5
0.5
÷ι)	1 0
0.5
6(⅛)
(b) Level sets of entropy
Figure 1: Entropy of belief for a 3-state POMDP.
It is worth noting that the perception actions affect the belief and subsequently the received reward
in the objective function.
3 Active Perception with Greedy Scheme
For variety of performance metrics, finding an optimal subset of sensors poses a computationally
challenging combinatorial optimization problem that is NP-hard. Augmenting POMDP planning
actions with nk active perception actions results in a combinatorial expansion of the action space.
Thereupon, it is infeasible to directly apply existing POMDP solvers to Problem 1. Instead of con-
catenating both sets of actions and treating them similarly, we propose a greedy strategy for selecting
perception actions that aims to pick the sensors that result in minimal uncertainty about the state.
The key enabling factor is that the perception actions does not affect the transition, consequently,
we can decompose the single-step belief update in (2) into two steps:
bβ (SO) = ET (S,a,SO)b(S),
s0
(6a)
b0δ,ω (OO) =	Qi∈Z(δ) Oi(S ,β,ω)b(S)
b	PsθQi∈Z(δ) Ol(S0,β,ωi)b(S0)
(6b)
This in turn implies that after a transition is made, the agent should pick a subset of observations
that lead to minimal uncertainty in bbδ,ω.
To quantify uncertainty in state, we use Shannon entropy of the belief. Fora discrete random variable
x, the entropy is defined as H(x) = -Pip(xi) logp(xi). An important property of entropy is its
strict concavity on the simplex of belief points, denoted by ∆B (Cover & Thomas, 2012). Further,
the entropy is zero at the vertices of ∆B and achieves its maximum, log |S |, at the center of ∆B
that corresponds to uniform distribution, i.e., when the uncertainty about the state is the highest.
Figure 1 demonstrates the entropy and its level sets for |S | = 3. Since the observation values are
unknown before selecting the sensors, we optimize conditional entropy that yields the expected value
of entropy over all possible observations. For discrete random variables x and y, conditional entropy
is defined as H(x|y) = Ey[H(x|y)] =Pip(yi)H(x|yi). Subsequently, with some algebraic
manipulation, it can be shown that the conditional entropy of state given current belief with respect
to δ is:
H(s|b, δ) = -	...	b(S)	Oij(S,β,ωij)
ωiι ∈Ωiι	ωik ∈Ωik s∈S ∖	ij ∈ζ(δ)
log
b(S) ∏ij ∈ζ(δ) Oij (S,β,ωij)
Σs0∈s b(S0) ∏ij ∈ζ(δ) Oij (S0,β,ωij)
))
(7)
0
0
where ζ(δ) = {i1, i2, . . . , ik}. It is worth mentioning that b is the current distribution of s and is
explicitly written only for the purpose of better clarity, otherwise, H(s∣b, δ) = H(s∣δ).
To minimize entropy, we define the objective function as the following set function:
f(Z)= H(s∣bβ)-H(s∣bβ,U ωi)
i∈ζ
(8)
5
Under review as a conference paper at ICLR 2019
and the optimization problem as:
δ* = argδmaχ, f(Z ⑹.
(9)
We propose a greedy algorithm, outlined in Algorithm 1 to find a near-optimal, yet efficient solution
to (9). The algorithm takes as input the agent’s belief and planning action. Then it iteratively adds
elements from the ground set (set of all sensors) whose marginal gain with respect to f is maximal
and terminates when k observations are selected.
Algorithm 1 Greedy policy for perception action
1:	Input: AP2-POMDP P = (S, A, k, T,Ω, O, R, γ), Current belief b, Planning action β.
2:	Output: Perception action δ.
3:	Initialize X = {1, 2,..., n}, Z = 0.
4:	for l = 1, . . . , k do
5:	j * = argmaχj∈χ∖ζ-H(SIbe, Ui∈ζ∪{j} ωi)
6:	Z 一 Z ∪{j*}
7:	end for
8:	return δ corresponding to Z .
Next, we derive a theoretical guarantee for the performance of the proposed greedy algorithm. The
following lemma states the required properties to prove the theorem. The proof of the lemma follows
from monotonicity and submodularity of conditional entropy (Ko et al., 1995). See the appendix for
the complete proof.
Lemma 1. Let Ω = {ω* 1, ω2,..., ωn} to represent a set of observations of the state S that Con-
ditioned on the state, are mutually independent (Assumption 1 holds). Then, f(Z), defined in (8),
realizes the following properties:
1. f(0) = 0,
2. f is monotone nondecreasing, and
3. f is submodular.
The above lemma enables us to establish the approximation factor using the classical analysis in
(Nemhauser et al., 1978).
Theorem 1. Let Z* to denote the optimal subset of observations with regard to objective function
f(Z), and Zg to denote the output of the greedy algorithm in Algorithm 1. Then, the following
performance guarantee holds:
H(S帆,U ωi) ≤ eH(s∣bβ) +(1 - e) H(s∣bβ, U ωi).	(10)
i∈Qg	、	/	i∈Z*
Remark 1. Intuitively, one can interpret the minimization of conditional entropy as pushing the
agent’s belief toward the boundary of the probability simplex ∆B. Due to convexity of POMDP
value function on ∆B (Sondik, 1978), this in turn implies that the agent is moving toward regions of
belief space that have higher value.
Although Theorem 1 proves that the entropy of the belief point achieved by the greedy algorithm
is close to the entropy of the belief point from the optimal solution, the key question is whether the
value of these points are close. We assess this question in the following and show that at each time
step, on expectation, the value from greedy scheme is close to the value from optimal observation
selection with regard to (9). To that end, we first show that the distance between the two belief points
is upper-bounded. Thereafter, using a similar analysis as that of Pineau et al. (2006), we conclude
that the difference between value function at these two points is upper-bounded.
Theorem 2. Let the agent’s current belief to be b and its planning action to be β. Consider the
optimization problem in (9), and let δ* and δg to denote the optimal perception action and the
perception action obtained by the greedy algorithm, respectively. It holds that:
E[kbg - b*k1] ≤C1,
where b* and bg are the updated beliefs according to (6) and C1 is a constant value.
6
Under review as a conference paper at ICLR 2019
Figure 2: The belief reachabil-
ity tree. The circles are be-
lief points while squares de-
pict branchings based on ac-
tions. Addition of perception
actions leads to combinatorial
expansion of number of belief
points in each layer.
Proof. We outline the sketch of the proof and bring the complete proof in the appendix. First, we
show that minimizing conditional entropy of posterior belief is equivalent to maximizing Kullback-
Leibler (KL-) divergence between current belief and the posterior belief, i.e., DKL (bbδ,ω kb). Next,
we exploit Pythagorean theorem for KL-divergence alongside its convexity to find a relation between
DKL(bbδg,ω kb) and DKL(bbδg,ω kbbδ ,ω) . Afterwards, using Pinkster’s inequality, we prove that the
total variation distance between b：9 ,ω and bbδ ,ω is bounded. This in turn yields the desired result
on boundedness of ∣∣bg - b* ∣∣1.
Theorem 3. Instate the notation and hypothesis of Theorem 2. Additionally, let V to be the true
value function for AP2-POMDP. The following statement holds:
E[V(bg)-V(b*)] ≤C2.
Proof. The proof is omitted for brevity. See the appendix for the proof.
4 Perception-aware Point-based Value Iteration
In this section, we propose a novel point-based value iteration algorithm to approximate the value
function for AP2-POMDPs. The algorithm relies on the performance guarantee of the proposed
greedy observation selection in previous section. Before describing the new point-based solver, we
first overview how point-based solvers operate. Algorithm 2 outlines the general procedure for a
point-based solver. It starts with an initial set of belief points B0 and their corresponding α vectors.
Then it performs a Bellman backup for each point to update α vectors. Next, it prunes α vectors to
remove dominated ones. Afterwards, it samples a new set of belief points and repeats these steps
until convergence or other termination criteria is met. The difference between solvers is in how they
apply sampling and pruning. The sampling step usually depends on the reachability tree of belief
space, see Figure 2. The state-of-the-art point-based methods do not traverse the whole reachability
tree, but they try to have enough sample points to provide a good coverage of the reachable space.
Algorithm 2 Generic algorithm for point-based solvers (Araya et al., 2010)
1:	Input: POMDP.
2:	Output: Approximate value function V .
3:	Initialize B = B0 and Γ0 .
4:	while 〜(termination condition) do
5:	B — Sample (B)
6:	Γ . BaCkUP(B, Γ)
7:	Γ . PrUne(B, Γ)
8:	end while
9:	return V (b) = maxα∈Γ α.b.
Note that the combinatorial number of actions due to observation selection highly expand the size
of the reachability tree. To avoid dealing with perception actions in the reachability tree, we apply
the greedy scheme to make the choice of δ deterministically dependent on β and previous belief. To
that end, we modify the BackUp step of point-based value iteration. The proposed BackUp step can
be combined with any sampling and pruning method in other solvers, such as the ones developed by
Spaan & Vlassis (2005); Kurniawati et al. (2008); Smith & Simmons (2012).
7
Under review as a conference paper at ICLR 2019
4.1 Proposed Point-based S olver
In point-based solver each witness belief point is associated with an α vector and an action. Never-
theless, for AP2-POMDPs, each witness point is associated with two actions, β and δ. We compute
δ based on greedy maximization of (9) so that given b and β, δ is uniquely determined. Henceforth,
we can rewrite (3) using (4) to obtain:
Vt(b) = max ( ^X b(s)R(s,β)+ Y ^X Pr(ω∣b, β, δ) max a.bfeβ,δ,ω
(β,δ) ∖s∈S	ω∈Ω	α∈rt-1
max
β
sX∈S
b(s)R(s, β)+
max
ω∈Ωiι ×...×Ωik
ij∈ζ(δS)
α∈Γt-1
s0∈S
ij eZ(S)
s∈S
Σ
max	X b(s)R(s, β)+
ω
Σ
max
∈ωWi x...xaik
ij ∈ζ(S)
α∈Γt-1
s∈S s0∈S
α(s0) ×	Oi(s0,β,ωij)T(s,β,s0)b(s) .
ij eZ(S)
where S = argmaxδ∈Apr f (Z(δ)) and f is computed at bβ. This way, We can partially decouple the
computation of perception action from the computation necessary for learning the planning policy.
Inspired by the results in the previous section, we propose the BackUp step detailed in Algorithm 3
to compute the new set of α vectors from the previous ones using Bellman backup operation. What
distinguishes this algorithm from conventional Bellman backup step is the inclusion of perception
actions. Basically, we need to compute the greedy perception action for each belief point and each
action (Line 7). This in turn affects computation of Γtb,β,ω as it represents a different set for each
belief point (Lines 9-13). However, notice that this added complexity is significantly lower than
concatenating the combinatorial perception actions with the planning actions and using conventional
point-based solvers. See the appendix for detailed complexity analysis.
5	Simulation Results
To evaluate the proposed algorithm for active perception and planning, we developed a point-based
value iteration solver for AP2-POMDPs. We initialized the belief set by uniform sampling from
∆B (Devroye, 1986). To focus on the effect of perception, we did not apply a sampling step, i.e,
the belief set is fixed throughout the iterations. However, one can integrate any sampling method
such as the ones proposed by Smith & Simmons (2012); Kurniawati et al. (2008). The α vectors
are initialized by ι-1γmin§,aR(s, a).Ones(∣S∣) (Shani et al., 2013). Furthermore, to speedup the
solver, one can employ a randomized backup step, as suggested by Spaan & Vlassis (2005). The
solver terminates once the difference between value functions in two consecutive iterations falls
below a predefined threshold. We also implemented a random perception policy that selects a subset
of information sources, uniformly at random, at each backup step. We implemented the solver in
Python 2.7 and ran the simulations on a laptop with 2.0 GHz Intel Core i7-4510U CPU and with
8.00 GB RAM.
5.1 Robotic Navigation in 1-D Grid
The first scenario models a robot that is moving in a 1-D discrete environment. The robot can only
move to adjacent cells and its navigation actions are Apl = {lef t, right, stop}. The robot’s tran-
sitions are probabilistic due to possible actuation errors. The robot does not have any sensor and it
relies on a set of cameras for localization. There is one camera at each cell that outputs a probability
8
Under review as a conference paper at ICLR 2019
Algorithm 3 BackUp step for AP2-POMDP
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
Input: AP2-POMDP P = (S, A, k, T,Ω, O, R, Y), Current set of belief points Bt, Current set
of α vectors Γt-1.
Output: Next set of α vectors Γt .
Initialize Γt = 0, Γ,β = 0 for all b ∈ Bt and β ∈ Apl.
for β ∈ Apl do
Γβ,* J αβ* (S) = R(SM
for b ∈ Bt do
δ = Greedy-argmaXδ∈Apr f (Z(δ))
Γtb,β,ω = 0
二、
for ω ∈ Ωi1 X ... X Ωik, j ∈ Z(δ) do
for α ∈ Γt-1 do
αb,β,ω(S) = Y PsO∈S Qij ∈ζ(S) Oi(S0, β,ωij )T(S, β, s)α(s0)
Γtb,β,ω J Γtb,β,ω ∪ αb,β,ω
end for
16:
17:
18:
19:
20:
21:
22:
23:
end for
αb,β = αβ,* +
Eω∈Ωi1 ×...×Ωik argmaxα∈rb,β,ω a.b
ij ∈ζ(δS)	t
Γtb,β J Γtb,β ∪ αb,β
end for
end for
for b ∈ Bt do
αb = argmaxα∈Γb,β,β∈Apl α.b
Γt = Γt ∪ αb
end for
return Γt .
(a) 1-D grid
(b) 2-D grid
Figure 3:	The robot moves in a grid while communicating with the cameras to localize itself. There
is a camera at each state on the perimeter. The accuracy of measurements made by each camera
depends on the distance of the camera from that state. The robot’s objective is to reach the goal
state, labeled by star, while avoiding the obstacles.
distribution over the position of the robot. The camera’s certainty is higher when the robot’s position
is close to it. To model the effect of robot’s position on the accuracy of measurements, we use a bi-
nomial distribution with its mean at the cell that camera is on. The binomial distribution represents
the state-dependent accuracy. The robot’s objective is to reach an specific cell in the map. For that
purpose, at each time step, the robot picks a navigation action and selects k camera from the set of
n cameras.
After the solver terminates, we evaluate the computed policy. To that end, we run 1000 iterations of
Monte Carlo simulations. The initial state of the robot is the origin of the map and its initial belief is
uniform over the map. Figure 4-(a) demonstrates the discounted cumulative reward, averaged over
1000 Monte Carlo runs, for random selection of 1 and 2 information sources, and greedy selection
of 1 and 2 information sources. It can be seen that the greedy perception policy significantly out-
performs the random perception. Figure 4-(b) depicts the belief entropy over the time. The lowest
9
Under review as a conference paper at ICLR 2019
1 rand. sei. 2 rand. sei. 1 greedy sei. 2 greedy sei.
(a) Average discounted cu-
mulative reward
(b) Average entropy over
time
Figure 4:	Results of 1-D simulation for a map of size 12, averaged over 1000 runs for each perception
policy. Left: The average discounted cumulative reward. The solid lines depict the corresponding
standard deviations. Right: The average belief entropy over the time horizon of 25.
(a) Random percep-
tion algorithm
(b) Greedy percep-
tion algorithm
∣h-

Figure 5:	The frequency of visiting states when using different perception methods for a 2-D map
of size 5*5 according to Figure 3-(b).
entropy of greedy perception, compared to random perception, shows less uncertainty of the robot
when taking planning actions. See the appendix for further results.
5.2 Robotic Navigation in 2-D Grid
The second setting is a variant of first scenario where the map is 2-D. Therefore the navigation
actions of robot are Apl = {up, right, down, lef t, stop}. The rest of the setting is similar to 1-
D case, except the cameras’ positions, as they are now placed around the perimeter of the map.
Additionally, the robot has to now avoid the obstacles in the map. The reward is 10 at the goal state.
-4 at the obstacles, and -1 in other states.
We applied the proposed point-based solver with both random perception and greedy perception
on the 2-D example. Next, we let the robot to run for a horizon of 25 steps and we terminated
the simulations once the robot reached the goal. Figure 5 illustrates the normalized frequency of
visiting each state for each perception algorithm. It can be seen that the policy learned by greedy
active perception leads to better obstacle avoidance. See the appendix for further results.
6 Conclusion
In this paper, we studied joint active perception and planning in POMDP models. To capture the
structure of the problem, we introduced AP2-POMDPs that have to pick a cardinality-constrained
subset of observations, in addition to original planning action. To tackle the computational chal-
lenge of adding combinatorial actions, we proposed a greedy scheme for observation selection. The
greedy scheme aims to minimize the conditional entropy of belief which is a metric of uncertainty
about the state. We provided a theoretical analysis for the greedy algorithm that led to bounded-
ness of value function difference between optimal entropy reduction and its greedy counterpart.
Furthermore, founded upon the theoretical guarantee of greedy active perception, we developed a
point-based value iteration solver for AP2-POMDPs. The idea introduced in the solver to address
active perception is general and can be applied on state-of-the-art point-based solvers. Lastly, we
implemented and evaluated the proposed solver on a variety of robotic navigation scenarios.
10
Under review as a conference paper at ICLR 2019
References
Mauricio Araya, Olivier Buffet, Vincent Thomas, and Franccois Charpillet. A PomdP extension
with belief-dependent rewards. In Advances in neural information processing systems, pp. 64-72,
2010.
Karl J Astrom. Optimal control of markov processes with incomplete state information. Journal of
Mathematical Analysis and Applications, 10(1):174-205, 1965.
Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, pp.
679-684, 1957.
Hsien-Te Cheng. Algorithms for partially observable Markov decision processes. PhD thesis, Uni-
versity of British Columbia, 1988.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Imre Csiszar. I-divergence geometry of probability distributions and minimization problems. The
Annals of Probability, pp. 146-158, 1975.
Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th
conference on Winter simulation, pp. 260-265. ACM, 1986.
Abolfazl Hashemi, Mahsa Ghasemi, Haris Vikalo, and Ufuk Topcu. A randomized greedy algorithm
for near-optimal sensor scheduling in large-scale sensor networks. In 2018 Annual American
Control Conference (ACC), pp. 1027-1032. IEEE, 2018.
Siddharth Joshi and Stephen Boyd. Sensor selection via convex optimization. IEEE Transactions
on Signal Processing, 57(2):451-462, 2009.
Chun-Wa Ko, Jon Lee, and Maurice Queyranne. An exact algorithm for maximum entropy sampling.
Operations Research, 43(4):684-691, 1995.
Andreas Krause and Daniel Golovin. Submodular function maximization. In Tractability: Practical
Approaches to Hard Problems, pp. 71-104. Cambridge University Press, 2014.
Andreas Krause and Carlos Guestrin. Near-optimal observation selection using submodular func-
tions. In AAAI, volume 7, pp. 1650-1654, 2007.
Chris Kreucher, Keith Kastella, and Alfred O Hero Iii. Sensor management using an active sensing
approach. Signal Processing, 85(3):607-624, 2005.
Hanna Kurniawati, David Hsu, and Wee Sun Lee. Sarsop: Efficient point-based pomdp planning
by approximating optimally reachable belief spaces. In Robotics: Science and systems, volume
2008. Zurich, Switzerland., 2008.
William S Lovejoy. A survey of algorithmic methods for partially observed markov decision pro-
cesses. Annals of Operations Research, 28(1):47-65, 1991.
Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrak, and Andreas
Krause. Lazier than lazy greedy. In AAAI, pp. 1812-1818, 2015.
Prabhu Natarajan, Pradeep K Atrey, and Mohan Kankanhalli. Multi-camera coordination and control
in surveillance systems: A survey. ACM Transactions on Multimedia Computing, Communica-
tions, and Applications (TOMM), 11(4):57, 2015.
George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations
for maximizing submodular set functionsi. Mathematical programming, 14(1):265-294, 1978.
Christos H Papadimitriou and John N Tsitsiklis. The complexity of markov decision processes.
Mathematics of operations research, 12(3):441-450, 1987.
Joelle Pineau, Geoffrey Gordon, and Sebastian Thrun. Anytime point-based approximations for
large pomdps. Journal of Artificial Intelligence Research, 27:335-380, 2006.
11
Under review as a conference paper at ICLR 2019
Yash Satsangi, Shimon Whiteson, Frans A Oliehoek, and Matthijs TJ Spaan. Exploiting submodular
value functions for scaling UP active perception. Autonomous Robots, 42(2):209-233, 2018.
Manohar Shamaiah, Siddhartha Banerjee, and Haris Vikalo. Greedy sensor selection: Leveraging
submodularity. In Decision and Control (CDC), 2010 49th IEEE Conference on, pp. 2572-2577.
IEEE, 2010.
Guy Shani, Joelle Pineau, and Robert Kaplow. A survey of point-based pomdp solvers. Autonomous
Agents and Multi-Agent Systems, 27(1):1-51, 2013.
Richard D Smallwood and Edward J Sondik. The optimal control of partially observable markov
processes over a finite horizon. Operations research, 21(5):1071-1088, 1973.
Trey Smith and Reid Simmons. Point-based pomdp algorithms: Improved analysis and implemen-
tation. arXiv preprint arXiv:1207.1412, 2012.
Edward J Sondik. The optimal control of partially observable markov processes over the infinite
horizon: Discounted costs. Operations research, 26(2):282-304, 1978.
Matthijs TJ Spaan. Cooperative active perception using pomdps. In AAAI 2008 workshop on ad-
vancements in POMDP solvers, 2008.
Matthijs TJ Spaan and Pedro U Lima. A decision-theoretic approach to dynamic sensor selection in
camera networks. In ICAPS, 2009.
Matthijs TJ Spaan and Nikos Vlassis. Perseus: Randomized point-based value iteration for pomdps.
Journal of artificial intelligence research, 24:195-220, 2005.
Matthijs TJ Spaan, Tiago S Veiga, and Pedro U Lima. Decision-theoretic planning under uncertainty
with information rewards for active cooperative perception. Autonomous Agents and Multi-Agent
Systems, 29(6):1157-1185, 2015.
Jason D Williams and Steve Young. Partially observable markov decision processes for spoken
dialog systems. Computer Speech & Language, 21(2):393-422, 2007.
Nevin Lianwen Zhang and Weihong Zhang. Speeding up the convergence of value iteration in
partially observable markov decision processes. Journal of Artificial Intelligence Research, 14:
29-51, 2001.
Appendix
In this section, we provide the proofs to the lemmas and theorems stated in the paper.
Proof of Theorem 1
First, in the next lemma, we show that the objective function defined for uncertainty reduction
has the required properties for the analysis by Nemhauser et al. (1978), namely being normalized,
monotone, and submodular.
Lemma 1. Let Ω = {ω1, ω2,..., ωn} to represent a set of observations of the state S that Con-
ditioned on the state, are mutually independent (Assumption 1 holds). Then, f (ζ), defined in (8),
realizes the following properties:
1.	f (0) = 0,
2.	f is monotone nondecreasing, and
3.	f is submodular.
12
Under review as a conference paper at ICLR 2019
Proof. Notice that bβ is explicitly present to determine the current distribution of S and it is not
a random variable. Therefore, for simplicity, we omit that in the following proof. It is clear that
f (0) = H(SM )-H(s∣bβ )=0.
Let [n] = {1, 2,..., n}. To prove the monotonicity, consider Zi ⊂ [n] and j ∈ [n]∖Zι. Then,
H(S|	[	ωi)
i∈ζ1∪{j}
=H( U	ωi∣s)+ H(S)-H( U	ωi)
i∈ζ1∪{j}	i∈ζ1∪{j}
=H( U ωi∣s) + H(ωj∣s) + H(S)- H( U ωi)
i∈ζ1	i∈ζ1
-H(ωj | U ωi)
i∈ζ1
=H(SI U ωi) + H(ωj |s) - H(ωj | U ωi)
i∈ζ1	i∈ζ1
(=)H(s∣ U ωi) + H(ωj∣s, U ωi) -H(ωj∣ U ωi)
i∈ζ1	i∈ζ1	i∈ζ1
(e)
≤H(s∣ U ωi) + H(ωj∣ U ωi)-H(ωj∣ U ωi)
i∈ζ1	i∈ζ1	i∈ζ1
=H(s| U ωi),
i∈ζ1
where (a) and (c) are due to Bayes’ rule for entropy, (b) follows from the conditional independence
assumption and joint entropy definition, (d) is due to the conditional independence assumption, and
(e) stems from the fact that conditioning does not increase entropy. The monotonicity of the objective
function means that if the number of obtained observations are higher, the conditional entropy will
be lower, and hence, on expectation, the uncertainty in the state will be lower.
Furthermore, from the third line of above proof, we can derive the marginal gain, i.e., the value of
adding one sensor, as:
fj(Zi)= H(s| U ωi)-H(s∣	U	ωi)
i∈ζ1	i∈ζ1∪{j}
=H(ωj∣ U ωi) -H(ωj∣s)
i∈ζ1
To prove submodularity, let Zi ⊆ Z2 ⊂ [n] andj ∈ [n]\Z2. Then,
fj (Zi) = H(ωj∣ U ωi)-H(ωj ∣s)
i∈ζ1
(a)
≥H(ωj∙∣	U	ωi)-H(ωj∙∣s)
i∈ζ1∪(ζ2∖ζ1)
=) H(3jI U ωi)-H(ωj∣s) = f&),
i∈ζ2
where (a) is based on the fact that conditioning does not increase entropy, and (b) results from
Zi ⊆ Z2 . The submodularity (diminishing returns property) of objective function indicates that as
the number of obtained observations increases, the value of adding a new observation will decrease.
In the next theorem, we exploit the properties of the proposed objective function to analyze the
performance of the greedy scheme.
13
Under review as a conference paper at ICLR 2019
Theorem 1. Let Z* to denote the optimal subset of observations with regard to objective function
f (ζ), and ζg to denote the output of the greedy algorithm in Algorithm 1. Then, the following
performance guarantee holds:
H(s∣bβ, U ωi) ≤ 1 H(s∣bβ) + (l - 1) H(SZe, U ωi).	(11)
i∈ζg	e ej	i∈ζ*
Proof. The properties of f stated in Lemma 1 along the theoretical analysis of greedy algorithm by
Nemhauser et al. (1978) yields
f(ζ g)≥(i—e)f(z *).
Using the definition off(ζ) and rearranging the terms, we obtain the desired result.
Proof of Theorem 2
Before stating the proof to Theorem 2, that bounds the distance of belief points from the greedy and
optimal entropy minimization algorithms, we need to present a series of propositions and lemmas.
Mutual information between two random variables is a positive and symmetric measure of their
dependence and is defined as:
Z(X; y) = XPχ,y(X,y)Iog Ppxy(x,y∣.
px(x)py(y)
x,y
Mutual information, due to its monotonicity and submodularity, has inspired many subset selection
algorithms (Krause & Golovin, 2014). In the following proposition, we express the relation between
conditional entropy and mutual information.
Proposition 1. Minimizing conditional entropy of the state with respect to a set of observations is
equivalent to maximizing the mutual information of state and the set of observations. This equiva-
lency is due to the definition of mutual information, i.e.,
I(S; U ωi) =H(S)-H(S∣Uωi),	(12)
i∈ζ	i∈ζ
and the fact that H(S) is computed at Be which amounts to a constant value that does not affect se-
lection procedure. Additionally, notice that (12) is the same as the definition of normalized objective
function of greedy algorithm in (8).
Another closely-related information-theoretic concept is Kullback-Leibler (KL-) divergence. The
KL-divergence, also known as relative entropy, is a non-negative and non-symmetric measure of
difference between two distributions. The KL-divergence from q(x) to P(x) is:
DKL(Pxkqp) = XPx(χ)log (Px(X)).
x	qx (x)
The following relation between mutual information and KL-divergence exists:
I (χ; y)=X pχ,y (X,y)log PP≡⅛
x,y
=XPy(y) XPχ∣y(x∣y)log Ox：*
= Ey[DKL(Px|ykPx)],
which allows us to state the next proposition.
Proposition 2. The mutual information of state and a set of observations is the expected value of
the KL-divergence from prior belief to posterior belief over all realizations of observations, i.e.,
I(s; U ωi)= ESi∈ζωi [DκL(bbδ,ωkbβ)].
i∈ζ
(13)
be is the prior belief before selecting observations and bj,ω is the posterior belief after selecting
perception action δ and receiving the observations.
14
Under review as a conference paper at ICLR 2019
Figure 6: The probability simplex, ∆B , for a 3-state POMDP. Gray area illustrates the projection of
hypograph of entropy, corresponding to posterior belief after greedy perception action, onto ∆B .
Let p0 := bβ, pg := b∕'ω,ω 〜Qi∈ζg Ωi, and p* := b^δ,ω,ω 〜Qi∈ζ* Ωi to denote prior belief
(after taking planning action), posterior belief after greedy perception action, and posterior belief
after optimal perception action, respectively. So far, we have established a relation between mini-
mizing the conditional entropy of posterior belief and maximizing the expected KL-divergence from
prior belief to posterior belief, i.e., DKL (pg kp0) (See Proposition 1 and Proposition 2). To relate
DKL(pgkp0) and DKL(pgkp*), we state the next lemma. But first, we bring information-geometric
definitions necessary for proving the lemma.
Definition 4. Let p to be a probability distribution over a finite alphabet. An I-sphere with center p
and radius ρ is defined as:
S(p,ρ) d=ef {q | DKL(qkp) < ρ}.
Definition 5. Let A to denote a convex set of probability distributions that intersect S (p, ∞). A q
satisfying
DKL(qqkp)d=efmq∈iΛnDKL(qkp),
is called the I-projection ofp on A.
Lemma 2. Instate the definition of p0, pg, and p*. The following inequality holds on expectation:
DKL (p0 kp*) ≥ DKL (p0 kpg) + DKL (Pg kp*).
Proof. Consider the set Ag = {p ∈ △b IH(P) ≥ H(pg) that contain probability distributions whose
entropy is lower-bounded by entropy of pg. Since entropy is concave over △B, its hypographs are
convex. Consequently Ag, the projection ofa hypograph onto △B, is a convex set. Furthermore, due
to monotonicity of conditional entropy, i.e., expected value of entropy over observations, we know
that p0 ∈ Ag. Besides, Due to optimality ofζ*, it holds that
H(s∣bβ, U ωi) ≤H(s∣bβ, U ωi)
i∈Z *
i∈ζg
which in turn yields p* ∈ ∆B ∖Λg. Figure 6 demonstrates these facts for an alphabet of size 3. Pg
is the I-ProjeCtion of p* on Λg. Therefore, by exploiting the analogue of Pythagoras' theorem for
KL-divergence (Csiszar, 1975), we conclude:
DKL (p0 kp*) ≥ DKL (p0 kpg) + DKL (Pg kp*).
A direct result of the above lemma, after taking the expectation over i∈[n] ωi, is:
ESi∈Z* 3, [DKL(bβ kb/)i ≥ E5∈ζgωi [DKL嘘 kby)] +
ESi∈wωi hDKL(bbδ,ωζg kbbδ,ωζ* )].
(14)
In the following theorem, we use the stated lemma to bound the expected KL-divergence distance
between greedy and optimal selection strategies.
Theorem 4. The KL-divergence between pg and p* is upper-bounded, i.e.,
ESi∈[n]ωi[DKL(pgkp*)] ≤C3,
where C3 is a constant.
15
Under review as a conference paper at ICLR 2019
Proof. Notice that while KL-divergence is not symmetric, the following fact still holds:
argmaχ ESi∈ζ ωi hDKL(bbδ,ω kbβ )i = argmaχ ESi∈ζ ωi hDKL(bβ kbb'" )],
whenever the distributions consist of only non-zero elements. Once the algorithm is initialized with
belief points that are not on the boundary of the probability simplex, this condition will hold. Also,
notice that ESW4* ωi [°兀£@β∣∣b^δ,ω)] is Constant. Now, Lemma 2 along the near-optimality result
of greedy algorithm for entropy minimization (See Theorem 1) yield the desired result.
Next, We bound the total variation distance between Pg andp*.
Definition 6. The total variation distance between two probability distributions p and q, over a
countable state space S, is defined as:
TVD(p, q) = 1 X Ip(S) -q(S) |.
2 s∈S
Pinsker’s inequality bounds the total variation distance in terms of the KL-divergence (Cover &
Thomas, 2012). Using this inequality, we can prove Theorem 2.
Theorem 2.	Let the agent’s current belief to be b and its planning action to be β. Consider the
optimization problem in (9), and let δ* and δg to denote the optimal perception action and the
perception action obtained by the greedy algorithm, respectively. It holds that:
E[∣bg — b*ki] ≤ Ci,
where b* and bg are the updated beliefs according to (6).
Proof. Note that the total variation distance is half of the li-norm. Hence,
E[∣bg — b*kι]=2 E[TVD(p, q)]
(a)	1
≤ 2E[^-DκL(bgkb*)]
(b)-
≤ 2^-E[Dkl(bgkb*)]
(C),—
≤ √2C3,
where (a) is the result of applying Pinsker’s inequality, b is obtained by applying Jansen’s inequality
to concave square-root function, and (c) follows from Theorem 4. The proof completes by taking
Ci = √2C3.
Proof of Theorem 3
In the next theorem, we use the bounded distance of belief points, obtained from the greedy and op-
timal entropy minimization algorithms to find an upper bound on the gap between the corresponding
value functions.
Theorem 3.	Instate the notation and hypothesis of Theorem 2. Additionally, let V to be the true
value function for AP2-POMDP. The following statement holds:
E[V(bg) — V(b*)] ≤ C2.
Proof. We use the PWLC property of value function. Let αg and α* to represent the gradient of
value function at bg and b*, respectively. Note that for every a vector, ∣∣α∣∞ ≤ max{|Rmi—；,|Rmin |}
where Rmax = maχs,β R(S, β) and Rmin = mins,β R(S, β). Therefore, we can show that
E [V (bg) — V (b*)] = E[αg.bg — α*.b*] = E[αg.bg — αg.b* + αg.b* — α*.b*]
(a)
≤ E[ag.bg — αg.b* + α*.b* — α*.b*] = E[αg.(bg — b*)]
(b)
≤ E[kαgk∞kbg —b*ki]
(c)	maχ {|Rmax |, |Rmin |}
≤ Ci----------;-----------,
I- Y
16
Under review as a conference paper at ICLR 2019
Figure 7: Comparing random and greedy selection for 2-D robotic navigation in terms of the average
belief entropy over the time horizon of 25.
u∙s⅛ucsΦnl03>
Belief points
(a) Effect of varying number of selected
cameras on the value function
Belief points
(b) Effect of selection algorithm on the value
function
Figure 8: The value function of a subset of size 10 from sampled belief points for 1-D navigation.
The results show that the diminishing returns property of uncertainty reduction is propagated into
value function.
where (a) follows from the fact that α* is the gradient of the optimal value function, (b) is due to
Holder's inequality, and (C) is the result of Theorem 2. Taking C2 = C1 max{|RmI-Y, |RminI} yields
the desired result.
Complexity Analysis of Algorithm 3
In this section, we compare the computational complexity of a point-based value iteration method
that works with the concatenated action space, with the computational complexity of the proposed
point-based method that picks the perception actions based on a greedy approach.
First, we compute the computations required for a single backup step in the point-based method with
concatenated action space. To that end, consider a fixed set of sampled belief points B . Let Γ to
denote the current set of α vectors. Further, for the simplicity of analysis, assume that the number
of possible observations from each information source is ∣Ωi | = Ω, ∀i ∈ [n]. The cardinality of a
concatenated action space is |A| = |Apr ||Apl | = nk |Apl|. Therefore, the complexity of a single
backup step would be OG)∣Apl | X Ωn X ∣Γ∣ X |S|2 + |B| X 晨)∣Apl | X |S| X Ωn) (Shani et al.,
2013).
On the other hand, applying greedy algorithm to pick a perception action requires O(n X k) calls
to an oracle that computes the objective function (or equivalently, the marginal gain). Here the
objective function is the conditional entropy whose complexity with a naive approach in the kth
iteration is O(Ωk X |S|2). Therefore, applying Algorithm 3 as the backup step leads to O(∣Aplj| X
|B| X n X k X Ωk X |S|2 + ∣Apl| X |B| X Ωk X ∣Γ∣ X |S|2 + |B| X ∣Apl| X |S| X Ωk) operations.
Hence, the proposed approach, as a result of exploiting the structure of action space, would lead to
significant computational gain, especially for large n.
17
Under review as a conference paper at ICLR 2019
Additional Numerical Results
Figure 7 depicts the history of the belief entropy for the 2-D navigation when applying the proposed
point-based method with random selection step and the proposed greedy selection step. As ex-
pected, the greedy selection leads to smaller entropy and hence, less uncertainty about the state. The
corresponding average discounted cumulative reward after running 1000 Monte Carlo simulations
is -18.8 for point-based value iteration with random selection step and -14.5 for point-based value
iteration with greedy selection step, which demonstrates the superiority of the proposed method.
We further analyzed the effect of number of selected cameras on the agent’s performance in the 1-D
navigation scenario. Figure 8 illustrates the value function for a subset of sampled belief points after
the algorithm has been terminated. It can be seen that the diminishing returns property of entropy
with respect to number of selected observations is propagated through the value function as well.
18