Under review as a conference paper at ICLR 2019
On the Margin Theory of Feedforward Neural
Networks
Anonymous authors
Paper under double-blind review
Ab stract
Past works have shown that, somewhat surprisingly, over-parametrization can help
generalization in neural networks. Towards explaining this phenomenon, we adopt
a margin-based perspective. We establish: 1) for multi-layer feedforward relu
networks, the global minimizer of a weakly-regularized cross-entropy loss has
the maximum normalized margin among all networks, 2) as a result, increasing
the over-parametrization improves the normalized margin and generalization error
bounds for deep networks. In the case of two-layer networks, an infinite-width
neural network enjoys the best generalization guarantees. The typical infinite
feature methods are kernel methods; we compare the neural net margin with that of
kernel methods and construct natural instances where kernel methods have much
weaker generalization guarantees. We validate this gap between the two approaches
empirically. Finally, this infinite-neuron viewpoint is also fruitful for analyzing
optimization. We show that a perturbed gradient flow on infinite-size networks
finds a global optimizer in polynomial time.
1	Introduction
In deep learning, over-parametrization refers to the widely-adopted technique of using more parame-
ters than necessary (Krizhevsky et al., 2012; Livni et al., 2014). Both computationally and statistically,
over-parametrization is crucial for learning neural nets. Controlled experiments demonstrate that
over-parametrization eases optimization by smoothing the non-convex loss surface (Livni et al., 2014;
Sagun et al., 2017). Statistically, increasing model size without any regularization still improves
generalization even after the model interpolates the data perfectly (Neyshabur et al., 2017b). This is
surprising given the conventional wisdom on the trade-off between model capacity and generalization.
In the absence of an explicit regularizer, algorithmic regularization is likely the key contributor
to good generalization. Recent works have shown that gradient descent finds the minimum norm
solution fitting the data for problems including logistic regression, linearized neural networks, and
matrix factorization (Soudry et al., 2018; Gunasekar et al., 2018b; Li et al., 2018; Gunasekar et al.,
2018a; Ji & Telgarsky, 2018). Many of these proofs require a delicate analysis of the algorithm’s
dynamics, and some are not fully rigorous due to assumptions on the iterates. To the best of our
knowledge, it is an open question to prove analogous results for even two-layer relu networks. (For
example, the technique of Li et al. (2018) on two-layer neural nets with quadratic activations still
falls within the realm of linear algebraic tools, which apparently do not suffice for other activations.)
We propose a different route towards understanding generalization: making the regularization explicit.
The motivations are: 1) with an explicit regularizer, we can analyze generalization without fully
understanding optimization; 2) it is unknown whether gradient descent provides additional implicit
regularization beyond what `2 regularization already offers; 3) on the other hand, with a sufficiently
weak `2 regularizer, we can prove stronger results that apply to multi-layer relu networks. Additionally,
explicit regularization is perhaps more relevant because `2 regularization is typically used in practice.
Concretely, we add a norm-based regularizer to the cross entropy loss of a multi-layer feedforward
neural network with relu activations. We show that the global minimizer of the regularized objective
achieves the maximum normalized margin among all the models with the same architecture, if the
regularizer is sufficiently weak (Theorem 2.1). Informally, for models with norm 1 that perfectly
classify the data, the margin is the smallest difference across all datapoints between the classifier
score for the true label and the next best score. We are interested in normalized margin because its
1
Under review as a conference paper at ICLR 2019
inverse bounds the generalization error (see recent work (Bartlett et al., 2017; Neyshabur et al., 2017a;
2018; Golowich et al., 2017) or Proposition 3.1). Our work explains why optimizing the training loss
can lead to parameters with a large margin and thus, better generalization error (see Corollary 3.2).
We further note that the maximum possible margin is non-decreasing in the width of the architecture,
and therefore the generalization bound of Corollary 3.2 can only improve as the size of the network
grows (see Theorem 3.3). Thus, even if the dataset is already separable, it could still be useful to
increase the width to achieve larger margin and better generalization.
At a first glance, it might seem counterintuitive that decreasing the regularizer is the right approach.
At a high level, we show that the regularizer only serves as a tiebreaker to steer the model towards
choosing the largest normalized margin. Our proofs are simple, oblivious to the optimization
procedure, and apply to any norm-based regularizer. We also show that an exact global minimum is
unnecessary: if we approximate the minimum loss within a constant factor, we obtain the max-margin
within a constant factor (Theorem 2.2).
To better understand the neural network max-margin, in Section 4 we compare the max-margin
two-layer network obtained by optimizing both layers jointly to kernel methods corresponding to
fixing random weights for the hidden layer and solving a 2-norm max-margin on the top layer.
We design a simple data distribution (Figure 1) where neural net margin is large but the kernel
margin is small. This translates to an Ω(√d) factor gap between the generalization error bounds for
the two approaches and demonstrates the power of neural nets compared to kernel methods. We
experimentally confirm that a gap does indeed exist.
In the setting of two-layer networks, we also study how over-parametrization helps optimization.
Prior works (Mei et al., 2018; Chizat & Bach, 2018; Sirignano & Spiliopoulos, 2018; Rotskoff
& Vanden-Eijnden, 2018) show that gradient descent on two-layer networks becomes Wasserstein
gradient flow over parameter distributions in the limit of infinite neurons. For this setting, we prove
that perturbed Wasserstein gradient flow finds a global optimizer in polynomial time.
Finally, we empirically validate several claims made in this paper. First, we confirm that neural
networks do generalize better than kernel methods. Second, we show that for two-layer networks, the
test error decreases and margin increases as the hidden layer grows, as predicted by our theory.
1.1	Additional Related Work
Zhang et al. (2016) and Neyshabur et al. (2017b) show that neural network generalization defies
conventional explanations and requires new ones. Neyshabur et al. (2014) initiate the search for the
“inductive bias” of neural networks towards solutions with good generalization. Recent papers (Hardt
et al., 2015; Brutzkus et al., 2017; Chaudhari et al., 2016) study inductive bias through training time
and sharpness of local minima. Neyshabur et al. (2015a) propose a new steepest descent algorithm in
a geometry invariant to weight rescaling and show that this improves generalization. Morcos et al.
(2018) relate generalization in deep nets to the number of “directions” in the neurons. Other papers
(Gunasekar et al., 2017; Soudry et al., 2018; Nacson et al., 2018; Gunasekar et al., 2018b; Li et al.,
2018; Gunasekar et al., 2018a) study implicit regularization towards a specific solution. Ma et al.
(2017) show that implicit regularization can help gradient descent avoid overshooting optima. Rosset
et al. (2004a;b) study logistic regression with a weak regularization and show convergence to the max
margin solution. We adopt their techniques and extend their results.
A line of work initiated by Neyshabur et al. (2015b) has focused on deriving tighter norm-based
Rademacher complexity bounds for deep neural networks (Bartlett et al., 2017; Neyshabur et al.,
2017a; Golowich et al., 2017) and new compression based generalization properties (Arora et al.,
2018b). Dziugaite & Roy (2017) manage to compute non-vacuous generalization bounds from
PAC-Bayes bounds. Neyshabur et al. (2018) investigate the Rademacher complexity of two-layer
networks and propose a bound that is decreasing with the distance to initialization. Liang & Rakhlin
(2018) and Belkin et al. (2018) study the generalization of kernel methods.
On the optimization side, Soudry & Carmon (2016) explain why over-parametrization can remove
bad local minima. Safran & Shamir (2016) show that over-parametrization can improve the quality of
the random initialization. Haeffele & Vidal (2015), Nguyen & Hein (2017), and Venturi et al. (2018)
show that for sufficiently overparametrized networks, all local minima are global, but do not show
how to find these minima via gradient descent. Du & Lee (2018) show that for two-layer networks
2
Under review as a conference paper at ICLR 2019
with quadratic activations, all second-order stationary points are global minimizers. Arora et al.
(2018a) interpret over-parametrization as a means of implicit acceleration during optimization. Mei
et al. (2018), Chizat & Bach (2018), and Sirignano & Spiliopoulos (2018) take a distributional view
of over-parametrized networks. Chizat & Bach (2018) show that Wasserstein gradient flow converges
to global optimizers under structural assumptions. We extend this to a polynomial-time result.
1.2	Notation
Let R denote the set ofreal numbers. WeWill use ∣∣∙k to indicate a general norm, with ∣∣∙∣∣ι, ∣∣∙∣∣2, ∣∣∙∣∣∞
denoting the '1, '2,'∞ norms on finite dimensional vectors, respectively, and ∣∣ ∙ ∣∣f denoting the
Frobenius norm on a matrix. In general, we use — on top of a symbol to denote a unit vector:
when applicable, U，u∕∣∣u∣∣, where the norm ∣∙∣ will be clear from context. Let SdT，{u ∈
Rd : ∣∣U∣∣2 = 1} be the unit sphere in d dimensions. Let Lp(SdT) be the space of functions on
Sd-1 for which the p-th power of the absolute value is Lebesgue integrable. For α ∈ Lp(Sd-1),
we overload notation and write ∣∣α∣p，(RSd-1 ∣α(u)∣pdu)"p. Additionally, for αι ∈ LI(SdT)
and α2 ∈ L∞(Sd-1) or αι, α? ∈ L2(Sd-1), we can define hα-ι, a£，JSd-I α1(u)α2(u)du < ∞.
Furthermore, we will use Vol(Sd-1)，JSd-I 1du. Throughout this paper, we reserve the symbol
X = [x1, . . . , xn] to denote the collection of datapoints (as a matrix), and Y = [y1, . . . , yn] to denote
labels. We use d to denote the dimension of our data. We often use Θ to denote the parameters of
a prediction function f, and f(Θ; x) to denote the prediction of f on datapoint x. We will use the
notation ., & to mean less than or greater than up to a universal constant, respectively. Unless stated
otherwise, O(∙), Ω(∙) denote some universal constant in upper and lower bounds, respectively. The
notation poly denotes a universal constant-degree polynomial in the arguments.
2 Weak Regularizer Guarantees Max Margin S olutions
In this section, we will show that when we add a weak regularizer to cross-entropy loss with a
positive-homogeneous prediction function, the normalized margin of the optimum converges to some
max-margin solution. As a concrete example, feedforward relu networks are positive-homogeneous.
Let l be the number of labels, so the i-th example has label yi ∈ [l]. We work with a family F of
prediction functions f (Θ; ∙) : Rd → Rl that are a-positive-homogeneous in their parameters for
some a > 0: f (cΘ; x) = caf(Θ; x), ∀c > 0. We additionally require that f is continuous in Θ. For
some general norm ∣∣ ∙ ∣, we study the λ-regularized cross-entropy loss Lλ, defined as
n
Lλ(Θ) , X-log
i=1
exp(fyi (Θ; Xi))
Pj=I eχp(fj(θ; Xi))
+ λ∣Θ∣r
(2.1)
for fixed r > 0. Let Θλ ∈ arg min Lλ(Θ).1 We define the normalized margin of Θλ as:
γλ
min
i
max fj (Θi； Xi)
j6=yi
(2.2)
Define the ∣∣ ∙ ∣∣-max normalized margin as
γ? , max
kΘk≤1
min ( fyi(Θ; Xi) — maxf(Θ; Xi)
i	j 6=yi
and let Θ? be a parameter achieving this maximum. We show that with sufficiently small regulariza-
tion level λ, the normalized margin γλ approaches the maximum margin γ? . Our theorem and proof
are inspired by the result of Rosset et al. (2004a;b), who analyze the special case when f is a linear
predictor. In contrast, our result can be applied to non-linear f as long as f is homogeneous.
Theorem 2.1.	Assume the training data is separable by a network f (Θ?; ∙) ∈ F with an optimal
normalized margin γ? > 0. Then, the normalized margin of the global optimum of the weakly-
regularized objective (equation 2.1) converges to γ? as the strength of the regularizer goes to zero.
Mathematically, let γλ be defined in equation 2.2. Then
γλ → γ? as λ → 0
1We formally show that Lλ has a minimizer in Claim A.1 of Section A.
3
Under review as a conference paper at ICLR 2019
An intuitive explanation for our result is as follows: because of the homogeneity, the loss L(Θλ)
roughly satisfies the following (for small λ, and ignoring problem parameters such as n):
Lλ(Θλ) ≈exp(-kΘλkaγλ)+λkΘλkr
Thus, the loss selects parameters with larger margin, while the regularization favors parameters with
a smaller norm. The full proof of the theorem is deferred to Section A.1.
Theorem 2.1 applies to feedforward relu networks and states that global minimizers of the weakly-
regularized loss will obtain a maximum margin among all networks of the given architecture. By
considering global minimizers, Theorem 2.1 provides a framework for directly analyzing generaliza-
tion properties of the solution without considering details of the optimization algorithm. In Section 3
we leverage this framework and existing generalization bounds (Golowich et al., 2017) to provide a
clean argument that over-parameterization can improve generalization.
We can also provide an analogue of Theorem 2.1 for the binary classification setting. For this
setting, our prediction is now a single real output and we train using logistic loss. We provide formal
definitions and results in Section A.2. Our study of the generalization properties of the max-margin
(see Section 3 and Section 4) is based in this setting.
2.1 Optimization Accuracy
Since Lλ is typically hard to optimize exactly for neural nets, we study how accurately we need to
optimize Lλ to obtain a margin that approximates γ? up to a constant. The following theorem shows
that it suffices to find Θ0 achieving a constant factor multiplicative approximation of Lλ (Θλ), where
λ is some sufficiently small polynomial in n, l, γ? . Though our theorem is stated for the general
multi-class setting, it also applies for binary classification. We provide the proof in Section A.3.
Theorem 2.2.	In the setting of Theorem 2.1, suppose that we choose
λ=eχp(T*/a -I)-a/r) nγ¾
for sufficiently large c (that only depends on r/a). For β ≤ 2, let Θ0 denote a β-approximate
minimizer of Lλ, so Lλ(Θ0) ≤ βLλ(Θλ). Denote the normalized margin of Θ0 byγ0. Then
γ≥	.
7 - 10 ∙ βa/r
3 Generalization properties of a maximum margin neural network
In Section 2 we showed that optimizing a weakly-regularized logistic loss leads to the maximum
normalized margin. We now study the direct implications of this result on the generalization properties
of the solution. Specifically, we use existing Rademacher complexity bounds of Golowich et al.
(2017) to present a generalization bound that depends on the network architecture only through the
inverse '2-normalized margin and depth of the network (See Proposition 3.1). Next, We combine
this bound with Theorem 2.1 to conclude that parameters obtained by optimizing logistic loss with
weak '2-regularization will have a generalization bound that scales with the inverse of the maximum
possible margin and depth. Finally, we note that the maximum possible margin can only increase as
the size of the network grows, which suggests that increasing the size of the network improves the
generalization of the solution (see Theorem 3.3).
We consider depth-K neural networks with 1-Lipschitz, 1-positive-homogeneous activation φ for
K ≥ 2. Suppose that the collection of parameters Θ is given by matrices W1, . . . , WK. The K-layer
network will compute a real-valued score
f(Θ; x) , WKφ(Wκ-ιφ(∙∙∙ φ(W1x) …))	(3.1)
where we overload notation to let φ(∙) denote the element-wise application of the activation φ. Let
mi denote the size of the i-th hidden layer, so Wi ∈ Rm1 ×d, W2 ∈ Rm2 ×m1, ∙∙∙ , WK ∈ R1×mK-1.
We will let M , (m1, . . . , mK-1) denote the sequence of hidden layer sizes. We will focus on
4
Under review as a conference paper at ICLR 2019
'2-regularized loss. The weakly-regularized logistic loss of the depth-K architecture with hidden
layer sizes M is therefore
1n
Lλ,M(Θ) , - Elog(I+exp(-yif(Θ; x∕) + λ∣∣Θ∣∣F	(3.2)
n i=1
We note that f is K-homogeneous in Θ, so the results of Section 2 apply to Lλ,M .2 3 Following our
conventions from Section 2, we denote the optimizer of Lλ,M by Θλ,M , the normalized margin
of Θλ,M by γλ,M, the max-margin solution by Θ*,m, and the max-margin by γ*,M. OUr notation
emphasizes the architecture of the network. Since the classifier f now predicts a single real value, we
need to redefine
Yλ,M，minyif(Θλ,M; Xi)
Y?,M , maχ miny,f (Θ; Xi)
kΘk2≤1 i
When the data is not separable by a neural network with architecture M, we define γ?M to be zero.
Recall that X = [X1 , . . . , Xn] denotes the matrix with all the data points as columns, and Y =
[y1, . . . , yn] denotes the labels. We sample X and Y i.i.d. from the data generating distribution pdata,
which is supported on X × {-1, +1}. We can define the population 0-1 loss and training 0-1 loss of
the network parametrized by Θ by
L(Θ)=	Pr	[yf (Θ; x) ≤ 0]
(X,y)- Pdata
Let C , supx∈X kXk2 be an upper bound on the norm of a single datapoint. Proposition 3.1 shows
that the generalization error only depends on the parameters through the inverse of the margin on the
training data. We obtain Proposition 3.1 by applying Theorem 1 of Golowich et al. (2017) with the
standard technique of using margin loss to bound classification error. There exist other generalization
bounds which depend on the margin and some normalization (Neyshabur et al., 2015b; 2017a; Bartlett
et al., 2017; Neyshabur et al., 2018); we choose the bounds of Golowich et al. (2017) because they
fit well with `2 normalization. In the two-layer case K = 2, the bound below also follows from
Neyshabur et al. (2015b).
Proposition 3.1. [Straightforward consequence of Golowich et al. (2017, Theorem 1)] Suppose φ is
I-Lipschitz and 1 -positive-homogeneous. For any depth-K network f (Θ; ∙) separating the data with
normalized margin Y，mini yif (Θ; Xi) > 0, with probability at least 1 一 δ over the draw of X, Y,
C
L(⑼〜YK(K-1)/2√n + e(γ)	(3.3)
where (Y)
Jloglon2 Y + Jlog(1∕δ)-. Note that E(Y) is typically small,
and thus the above bound
mainly scales with
C	3
YK(KT)/27n.
For completeness, we state the proof in Section C.1. By combining this bound with our Theorem 2.1
we can conclude that optimizing weakly-regularized logistic loss gives us generalization error bounds
that depend on the maximum possible margin of a network with the given architecture.
Corollary 3.2. In the setting of Proposition 3.1, with probability 1 一 δ,
limSUP L(θλ,M) . γ?,MK(K-1)∕2√n + E(Y?,M)	(3.4)
where E(Y) is defined as in Proposition 3.1. Above we implicitly assume Y?,M > 0, since otherwise
the right hand side of the bound is vacuous.
2Although Theorem 2.1 is written in the language of multi-class prediction where the classifier outputs l ≥ 2
scores, the results translate to single-output binary classification. See Section A.2.
3Although the K(K-02 factor of equation 3.3 decreases with depth K, the margin Y will also tend to
decrease as the constraint kΘΘ∣∣f ≤ 1 becomes more stringent.
5
Under review as a conference paper at ICLR 2019
By applying Theorem 2.2 with Proposition 3.1, we can also conclude that optimizing Lλ,M within a
constant factor gives a margin, and therefore generalization bound, approximating the best possible.
One consequence of Corollary 3.2 is that optimizing weakly-regularized logistic loss results in the
best possible generalization bound out of all models with the given architecture. This indicates
that the widely used algorithm of optimizing deep networks with '2-regularized logistic loss has an
implicit bias towards solutions with good generalization.
Next, we observe that the maximum normalized margin is non-decreasing with the size of the architec-
ture. Formally, for two depth-K architectures M = (m1, . . . , mK-1) and M0 = (m01, . . . , m0K-1),
we say M ≤ M0 if mi ≤ m0i ∀i = 1, . . . K - 1. Theorem 3.3 states that if M ≤ M0, then the
max-margin over networks with architecture M0 is at least the max-margin over networks with
architecture M.
Theorem 3.3. Recall that γ*,M denotes the maximum normalized margin ofa network with archi-
tecture M. If M ≤ M0, we have γ*,M ≤ Y?,M . As a important consequence,
the generalization error bound of Corollary 3.2 for M0 is at least as good as that for M.
This theorem is simple to prove and follows because we can directly implement any network of
architecture M using one of architecture M0, if M ≤ M0 . This can explain why additional over-
parameterization has been empirically observed to improve generalization in two-layer networks
(Neyshabur et al., 2017b): the margin does not decrease with a larger network size, and therefore
Corollary 3.2 gives a better generalization bound. In Section 6, we provide empirical evidence that
the test error decreases with larger network size while the margin is non-decreasing.
The phenomenon in Theorem 3.3 contrasts with standard '2-normalized linear prediction. In this
setting, adding more features increases the norm of the data, and therefore the generalization error
bounds could also increase. On the other hand, Theorem 3.3 shows that adding more neurons (which
can be viewed as learned features) can only improve the generalization of the max-margin solution.
4	Neural net max-margin vs. kernel methods
We will continue our study of the max-margin neural network via comparison against kernel methods,
a context in which margins have already been extensively studied. We show that two-layer networks
can obtain a larger margin, and therefore better generalization guarantees, than kernel methods. Our
comparison between the two methods is motivated by an equivalence between the '2 max-margin of
an infinite-width two-layer network and the '1-SVM (Zhu et al., 2004) over the lifted feature space
defined by the activation function applied to all possible hidden units (Neyshabur et al., 2014; Rosset
et al., 2007; Bengio et al., 2006). The kernel method corresponds to the '2-SVM in this same feature
space, and is equivalent to fixing random hidden layer weights and solving an '2-SVM over the top
layer. In Theorem 4.3, we construct a distribution for which the generalization upper bounds for the
'1-SVM on this feature space are smaller than those for the '2-SVM by a Ω(√d) factor. Our work
provides evidence that optimizing all layers of a network can be beneficial for generalization.
There have been works that compare '1 and '2-regularized solutions in the context of feature selection
and construct a feature space for which a generalization gap exists (e.g., see Ng (2004)). In contrast,
we work in the fixed feature space of relu activations, which makes our construction particularly
challenging.
We will use m to denote the width of the single hidden layer of the network. Following the convention
from Section 3, we will use γ*,m to denote the maximum possible normalized margin of a two-layer
network with hidden layer size m (note the emphasis on the size of the single hidden layer). The
depth K = 2 case of Corollary 3.2 immediately implies that optimizing weakly-regularized '2 loss
over width-m two-layer networks gives parameters whose generalization upper bounds depend on the
hidden layer size only through 1∕γ*,m. Furthermore, from Theorem 3.3 it immediately follows that
Y?，1 ≤ Y?,2 ≤∙∙∙≤ γ?,∞
The work of Neyshabur et al. (2014) links γ*,m to the '1 SVM over a lifted space. Formally, we
define a lifting function 夕：Rd → L∞(Sd-1) mapping data to an infinite feature vector:
X ∈ Rd → 夕(x) ∈ L∞(Sd-1) satisfying 夕(x)[u] = φ(u>x)	(4.1)
6
Under review as a conference paper at ICLR 2019
where φ is the activation of Section 3. We look at the margin of linear functionals corresponding
to α ∈ LI(SdT). The 1-norm SVM (ZhU et al., 2004) over the lifted feature φ(x) solves for the
maximum margin:
γ'ι, max min yi h”(xi)i
subject to kαk1 ≤ 1
(4.2)
where we rely on the inner product and 1-norm defined in Section 1.2. This formulation is equivalent
to a hard-margin optimization on “convex neural networks” (Bengio et al., 2006). Bach (2017) also
study optimization and generalization of convex neural networks. Using results from Rosset et al.
(2007); Neyshabur et al. (2014); Bengio et al. (2006), our Theorem 2.1 implies that optimizing weakly-
regularized logistic loss over two-layer networks is equivalent to solving equation 4.2 when the size
of the hidden layer is at least n + 1, where n is the number of training examples. Proposition 4.1
essentially restates this with the minor improvement that this equivalence4 also holds when the size
of the hidden layer is n.
Proposition 4.1. Let γ'1 be defined in equation 4.2. Then 等=Y?,n = •…=Y*,∞.
For completeness, we prove Proposition 4.1 in Section B, relying on the work of Tibshirani (2013)
and Rosset et al. (2004a).
Importantly, the `1 -max margin on the lifted feature space is obtainable
by optimizing a finite neural network. We compare this to the `2 margin
attainable via kernel methods. Following the setup of equation 4.2, we
define the kernel problem over α ∈ L2 (SdT1):
Y'2，max min yi hα,φ(Xi )i
α i∈[n]	(4.3)
subject to √K∣α∣2 ≤ 1
Figure 1: A visualiza-
tion of 60 sampled points
from D in 3 dimensions.
Red points denote nega-
tive examples and blue
points denote positive ex-
amples.
where κ , Vol(SdT1). (We scale kα∣∣2 by √κ to make the lemma
statement below cleaner.) First, γ'2 can be used to obtain a standard
upper bound on the generalization error of the kernel SVM. Following
the notation of Section 3, We will let L'2-svm denote the 0-1 population
classification error for the optimizer of equation 4.3.
Lemma 4.2. In the setting of Proposition 3.1, with probability at least
1-δ, the generalization error of the standard kernel SVM with relu feature
(defined in equation 4.3) is bounded by
n
lθgmaχ1lθg2 √⅜2，2
(4.4)
where `2 ,
+ Jlog(I/6 is typically a lower-order term.
The bound above follows from standard techniques (Bartlett & Mendelson, 2002), and we provide a
full proof in Section C.2. We construct a data distribution for which this lemma does not give a good
bound for kernel methods, but Corollary 3.2 does imply good generalization for two-layer networks.
Theorem 4.3. There exists a data distribution pdata such that the `1 SVM with relu features has a
good margin: γ'1 & 1 and with probability 1 - δ over the choice ofi.i.d. samples from Pdata, obtains
generalization error
dlogn
l'i -svm . V -n----+ e'ι
where `1
Jlogn/" is typically a lower order term. Meanwhile, with high probability the '2 SVM
has a small margin: γ'2 . max
and therefore the generalization upper bound from
4The factor of 1 is due the the relation that every unit-norm parameter Θ corresponds to an α in the lifted
space with kαk = 2.
7
Under review as a conference paper at ICLR 2019
Lemma 4.2 is at least
In particular, the '2 bound is larger than the '1 bound by a Ω(√d) factor.
Although Theorem 4.3 compares upper bounds, our construction highlights properties of distributions
which result in better neural network generalization than kernel method generalization. Furthermore,
in Section 6 we empirically validate the gap in generalization between the two methods.
We briefly overview the construction of pdata here. The full proof is in Section D.1.
Proof sketch for Theorem 4.3. We base pdata on the distribution D of examples (x, y) described
below. Here ei is the i-th standard basis vector and we use x>ei to represent the i-coordinate of x
(since the subscript is reserved to index training examples).
I	y=	+1,	x> e1	+1,
〜N(0,Id-2), and ∖ II	y= y=	+1, -1,	x> e1 x> e1	-1, +1,
	[y =	-1,	x> e1	-1,
x>e2	+1
x>e2	-1
x>e2	-1
x>e2	+1
w/ prob. 1/4
w/ prob. 1/4
w/ prob. 1/4
w/ prob. 1/4
Figure 1 shows samples from D when there are 3 dimensions. From the visualization, it is clear that
there is no linear separator for D. As Lemma D.1 shows, a relu network with four neurons can fit this
relatively complicated decision boundary. On the other hand, for kernel methods, we prove that the
symmetries in D induce cancellation in feature space. As a result, the features are less predictive of
the true label and the margin will therefore be small. We formalize this argument in Section D.1.
Gap in regression setting: We are able to prove an even larger Ω(，n/d) gap between neural
networks and kernel methods in the regression setting where we wish to interpolate continuous
labels. Analogously to the classification setting, optimizing a regularized squared error loss on
neural networks is equivalent to solving a minimum 1-norm regression problem (see Theorem D.5).
Furthermore, kernel methods correspond to a minimum 2-norm problem. We construct distributions
Pdata where the 1-norm solution will have a generalization error bound of O(ʌ/d/n), whereas the 2-
norm solution will have a generalization error bound that is Ω(1) and thus vacuous. In Section D.2, We
define the 1-norm and 2-norm regression problems. In Theorem D.10 we formalize our construction.
5 Perturbed Was serstein gradient flow finds global optimizers in
POLYNOMIAL TIME
In the prior section, we studied the limiting behavior of the generalization of a two-layer network as
its width goes to infinity. In this section, we will now study the limiting behavior of the optimization
algorithm, gradient descent. Prior work (Mei et al., 2018; Chizat & Bach, 2018) has shown that as
the hidden layer size grows to infinity, gradient descent for a finite neural network approaches the
Wasserstein gradient flow over distributions of hidden units (defined in equation 5.1). Chizat & Bach
(2018) assume the gradient flow converges, a non-trivial assumption since the space of distributions
is infinite-dimensional, and given the assumption prove that Wasserstein gradient flow converges to a
global optimizer in this setting, but do not specify a convergence rate. Mei et al. (2018) show global
convergence for the infinite-neuron limit of stochastic Langevin dynamics, but also do not provide a
convergence rate.
We show that a perturbed version of Wasserstein gradient flow converges in polynomial time. The
informal take-away of this section is that a perturbed version of gradient descent converges in
polynomial time on infinite-size neural networks (for the right notion of infinite-size.)
Formally, we optimize the following functional over distributions ρ on Rd+1 :
L[ρ] , R
Φdρ + V dρ
8
Under review as a conference paper at ICLR 2019
where Φ : Rd+1 → Rk, R : Rk → R, and V : Rd+1 → R. In this work, we consider 2-homogeneous
Φ and V . We will additionally require that R is convex and nonnegative and V is positive on the unit
sphere. Finally, we need standard regularity assumptions on R, Φ, and V :
Assumption 5.1 (Regularity conditions on Φ, R, V ). Φ and V are differentiable as well as upper
bounded and Lipschitz on the unit sphere. R is Lipschitz and its Hessian has bounded operator norm.
We provide more details on the specific parameters (for boundedness, Lipschitzness, etc.) in Sec-
tion E.1. We note that relu networks satisfy every condition but differentiability of Φ.5 We can fit a
neural network under our framework as follows:
Example 5.2 (Logistic loss for neural networks). We interpret ρ as a distribution over the parameters
of the network. Let k , n and Φi (θ) , wφ(u>xi) for θ = (w, u). In this case, Φdρ is a
distributional neural network that computes an output for each of the n training examples (like a
standard neural network, it also computes a weighted sum over hidden units). We can compute the
distributional version of the regularized logistic loss in equation 3.2 by setting V (θ) , λkθk22 and
R(a1, . . . ,an) , Pin=1 log(1 + exp(-yiai)).
We will define	L0[ρ]	:	Rd+1	→ R with	L0 [ρ](θ)	,	hR0(	Φdρ), Φ(θ)i	+ V(θ)	and	v [ρ](θ) ,
-VθL0[ρ](θ). Informally, L0[ρ] is the gradient of L with respect to ρ, and V is the induced velocity
field. For the standard Wasserstein gradient flow dynamics, ρt evolves according to
-dρt = -v ∙ (v[Pt]Pt)	(5.1)
dt
where V∙ denotes the divergence of a vector field. For neural networks, these dynamics formally
define continuous-time gradient descent when the hidden layer has infinite size (see Theorem 2.6
of Chizat & Bach (2018), for instance).
We propose the following modification of the Wasserstein gradient flow dynamics:
dtρt = -σρt + σUd 一 V ∙ (v[ρt]ρt)	(5.2)
where Ud is the uniform distribution on Sd. In our perturbed dynamics, we add very small uniform
noise over Ud, which ensures that at all time-steps, there is sufficient mass in a descent direction for
the algorithm to decrease the objective. For infinite-size neural networks, one can informally interpret
this as re-initializing a very small fraction of the neurons at every step of gradient descent. We prove
convergence to a global optimizer in time polynomial in 1/, d, and the regularity parameters.
Theorem 5.3 (Theorem E.4 with regularity parameters omitted). Suppose that Φ and V are 2-
homogeneous and the regularity conditions of Assumption 5.1 are satisfied. Also assume that from
starting distribution ρ0, a solution to the dynamics in equation 5.2 exists. Define L? , infρ L[ρ].
Let e > 0 be a desired error threshold and choose σ，exp(-dlog(1∕e)poly(k, L[ρo] — L?)) and
te，d4 poly(log(1∕e), k, L[ρo] — L?), where the regularity parameters for Φ, V, and R are hidden
in the poly(∙). Then, perturbed Wasserstein gradient flow converges to an E-approximate global
minimum in t time:
min L[ρt] - L? ≤ E.
0≤t≤t	t
We provide a theorem statement that includes regularity parameters in Section E.1. We prove the
theorem in Section E.2.
As a technical detail, Theorem 5.3 requires that a solution to the dynamics exists. We can remove this
assumption by analyzing a discrete-time version of equation 5.2:
Pt+ι , Pt + η(-σρt + σUd - V ∙ (v[ρt]ρt))
and additionally assuming Φ and V have Lipschitz gradients. In this setting, a polynomial time
convergence result also holds. We state the result in Section E.3.
An implication of our Theorem 5.3 is that for infinite networks, we can optimize the weakly-
regularized logistic loss in time polynomial in the problem parameters and λ-1. By Theorem 2.2, we
only require λ-1 = poly(n) to approximate the maximum margin within a constant factor. Thus, for
infinite networks, we can approximate the max margin within a constant factor in polynomial time.
5The relu activation is non-differentiable at 0 and hence the gradient flow is not well-defined. Chizat & Bach
(2018) acknowledge this same difficulty with relu.
9
Under review as a conference paper at ICLR 2019
Figure 2: Comparing neural networks and kernel methods. Left: Classification. Right: Regression.
Figure 3: Dependence of margin and test error on hidden layer size. Left: Synthetic. Right: MNIST.
6	Simulations
We first compare the generalization of neural networks and kernel methods for classification and
regression. In Figure 2 we plot the generalization error and predicted generalization upper bounds6 of
a trained neural network against a `2 kernel method with relu features as we vary n. Our data comes
from a synthetic distribution generated by a neural network with 6 hidden units; we provide a detailed
setup in Section F.1. For classification we plot 0-1 error, whereas for regression we plot squared error.
The variance in the neural network generalization bound for classification likely occured because
we did not tune learning rate and training time, so the optimization failed to find the best margin.
The plots show that two-layer networks clearly outperform kernel methods in test error as n grows.
However, there seems to be looseness in the bounds: the kernel generalization bound appears to stay
constant with n (as predicted by our theory for regression), but the test error decreases.
We also plot the dependence of the test error and margin on the hidden layer size in Figure 3 for
synthetic data generated from a ground truth network with 10 hidden units and also MNIST. The plots
indicate that test error is decreasing in hidden layer size while margin is increasing, as Theorem 3.3
predicts. We provide more details on the experimental setup in Section F.2.
In Section F.3, we verify the convergence of a simple neural network to the max-margin solution as
regularization decreases. In Section F.4, we train modified WideResNet architectures on CIFAR10
and CIFAR100. Although ResNet is not homogeneous, we still report improvements in generalization
from annealing the weight decay during training, versus staying at a fixed decay rate.
7	Conclusion
We have made the case that maximizing margin is one of the inductive biases of relu networks
obtained from optimizing weakly-regularized cross-entropy loss. Our framework allows us to
directly analyze generalization properties of the network without considering the optimization
algorithm used to obtain it. Using this perspective, we provide a simple explanation for why
over-parametrization can improve generalization. It is a fascinating question for future work to
characterize other generalization properties of the max-margin solution. On the optimization side, we
make progress towards understanding over-parametrized gradient descent by analyzing infinite-size
neural networks. A natural direction for future work is to apply our theory to optimize the margin of
finite-sized neural networks.
6We compute the leading term that is linear in the norm or inverse margin from the bounds in Proposition 3.1
and Lemmas 4.2, D.8, and D.9.
10
Under review as a conference paper at ICLR 2019
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018a.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018b.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
Learning Research,18(19):1-53, 2017.
Keith Ball. An elementary introduction to modern convex geometry. Flavors ofgeometry, 31:1-58,
1997.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand
kernel learning. arXiv preprint arXiv:1802.01396, 2018.
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Advances in neural information processing systems, pp. 123-130, 2006.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. arXiv preprint arXiv:1611.01838, 2016.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic
activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779,
2017.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541, 2017.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent on
linear convolutional networks. arXiv preprint arXiv:1806.00468, 2018b.
Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. arXiv preprint arXiv:1506.07540, 2015.
11
Under review as a conference paper at ICLR 2019
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300, 2018.
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk
bounds, margin bounds, and regularization. In Advances in neural information processing systems,
pp. 793-800, 2009.
Vladimir Koltchinskii, Dmitry Panchenko, et al. Empirical margin distributions and bounding the
generalization error of combined classifiers. The Annals of Statistics, 30(1):1-50, 2002.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory,
pp. 2-47, 2018.
T. Liang and A. Rakhlin. Just Interpolate: Kernel “Ridgeless” Regression Can Generalize. ArXiv
e-prints, August 2018.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural
networks. In Advances in Neural Information Processing Systems, pp. 855-863, 2014.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex
statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion
and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layers neural networks. arXiv preprint arXiv:1804.06561, 2018.
Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. On the importance of
single directions for generalization. arXiv preprint arXiv:1803.06959, 2018.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Convergence
of gradient descent on separable data. arXiv preprint arXiv:1803.01905, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422-2430, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017b.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks. arXiv preprint
arXiv:1805.12076, 2018.
Andrew Y Ng. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings
of the twenty-first international conference on Machine learning, pp. 78. ACM, 2004.
12
Under review as a conference paper at ICLR 2019
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint
arXiv:1704.08045, 2017.
Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin
classifier. Journal ofMachine Learning Research, 5(Aug):941-973, 2004a.
Saharon Rosset, Ji Zhu, and Trevor J Hastie. Margin maximizing loss functions. In Advances in
neural information processing systems, pp.1237-1244, 2004b.
Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. l1 regularization in infinite di-
mensional feature spaces. In International Conference on Computational Learning Theory, pp.
544-558. Springer, 2007.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error. arXiv
preprint arXiv:1805.00915, 2018.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In
International Conference on Machine Learning, pp. 774-782, 2016.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the
hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv
preprint arXiv:1805.01053, 2018.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data.
In International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=r1q7n9gAb.
Ryan J Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics, 7:1456-1490,
2013.
Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint
arXiv:1706.05350, 2017.
Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension
have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Ji Zhu, Saharon Rosset, Robert Tibshirani, and Trevor J Hastie. 1-norm support vector machines. In
Advances in neural information processing systems, pp. 49-56, 2004.
A Missing Proofs in Section 2
We first show that Lλ does indeed have a global minimizer.
Claim A.1. In the setting of Theorems 2.1 and A.3, arg minΘ Lλ(Θ) exists.
Proof. We will argue in the setting of Theorem 2.1 where Lλ is the multi-class cross entropy
loss, because the logistic loss case is analogous. We first note that Lλ is continuous in Θ be-
cause f is continuous in Θ and the term inside the logarithm is always positive. Next, define
b，infθ Lλ(Θ) > 0. Then We note that for ∣∣Θk > (b∕λ)1"，M, We must have Lλ(Θ) > b.
It follows that infkΘk≤M Lλ(Θ) = infΘ Lλ(Θ). However, there must be a value Θλ which at-
tains inf kΘk≤M Lλ (Θ), because {Θ : ∣Θ∣ ≤ M} is a compact set and Lλ is continuous. Thus,
infθ Lλ(θ) is attained by some Θλ.	□
13
Under review as a conference paper at ICLR 2019
A. 1 Missing Proofs for Multi-class Setting
Towards proving Theorem 2.1, we first show as we decrease λ, the norm of the solution kΘλk grows.
Lemma A.2. In the setting of Theorem 2.1, as λ → 0, we have kΘλk → ∞.
To prove Theorem 2.1, we rely on the exponential scaling of the cross entropy: Lλ can be lower
bounded roughly by exp(-kΘλkγλ), but also has an upper bound that scales with exp(-kΘλ kγ?).
By Lemma A.2, we can take large kΘλk so the gap γ? -γλ vanishes. This proof technique is inspired
by that of Rosset et al. (2004a).
Proof of Theorem 2.1. For any M > 0 and Θ with yθ，mini f(Θ; Xi) - maxj=yi f (Θ; Xi)),
Lλ(M⑼=1 XX-log PexpeM:Ma：(?x.))+λMrkθkr	Sythehomogeneityoff)
i=1	j=1 exp(M fj (Θ; Xi))
1n	1
=吟-log1 + Pj=yieXp(M α(fj (Θ; Xi)-fyi(Θ; Xi))) +λMrkθkr	(A.1)
≤log(1+(l-1)exp(-MaγΘ))+λMrkΘkr	(A.2)
We can also apply Pj 6=yi exp(M a (fj (Θ; Xi) - fyi (Θ; Xi))) ≥ max exp(M a (fj (Θ; Xi) -
fyi (Θ; Xi))) = exp γΘ in order to lower bound equation A.1 and obtain
Lλ(M⑼ ≥ 1log(1 + exp(-Mαγθ)) + λMr∣∣Θkr	(A.3)
n
Applying equation A.2 with M = kΘλ k and Θ = Θ?, noting that kΘ? k ≤ 1, we have:
Lλ(Θ*∣∣Θλk) ≤ log(1 + (l - 1) exp(-kΘλkaγ?)) + λkΘλIIr	(A.4)
Next we lower bound Lλ(Θλ) by applying equation A.3,
Lλ(Θλ) ≥ 1log(1+exp(-kΘλkaγλ))+ λ∣Θλkr	(A.5)
n
Combining equation A.4 and equation A.5 with the fact that Lλ(Θλ) ≤ Lλ(Θ?∣∣Θλ∣∣) (by the global
optimality of Θλ), we have
∀λ > 0, n log(1 + (l - 1)exp(-IΘλIaγ?)) ≥ log(1 + exp(-IΘλIaγλ))
Recall that by Lemma A.2, as λ → 0, we have IΘλ I → ∞. There-
fore, exp(-IΘλ Iaγ?), exp(-IΘλ Iaγλ) → 0. Thus, we can apply Taylor expan-
sion to the equation above with respect to exp(-IΘλ Iaγ?) and exp(-IΘλ Iaγλ). If
max{exp(-IΘλIaγ?), exp(-IΘλIaγλ)} < 1, then we obtain
n(l - 1)exp(-∣Θλkαγ?) ≥ exp(-∣Θλkaγλ) - O(max{exp(-kΘλkaγ?)2,exp(-kΘλkaγλ)2})
We claim this implies that γ? ≤ lim infλ→0 γλ. If not, we have lim infλ→0 γλ < γ? , which implies
that the equation above is violated with sufficiently large ∣∣Θλk (∣∣Θλ∣∣》log(2(' - 1)n)1/a would
suffice). By Lemma A.2, IΘλ I → ∞ as λ → 0 and therefore we get a contradiction.
Finally, we have γλ ≤ γ? by definition of γ?. Hence, limλ→o Yλ exists and equals γ?.	□
Now we fill in the proof of Lemma A.2.
Proof of Lemma A.2. For the sake of contradiction, we assume that ∃C > 0 such that for any λ0 > 0,
there exists 0 < λ < λ0 with ∣Θλ∣ ≤ C. We will determine the choice ofλ0 later and pick λ such that
∣Θλ ∣ ≤ C. Then the logits (the prediction fj (Θ, Xi) before softmax) are bounded in absolute value
by some constant (that depends on C), and therefore the loss function - log expfyi'?;?)、、for
J	Ej=I exPfj 3Xi))
every example is bounded from below by some constant D > 0 (depending on C but not λ.)
14
Under review as a conference paper at ICLR 2019
Let M = X-1/(r+1),we have that
0 < D ≤ Lλ(Θλ) ≤ Lλ (MΘ?)	(by the optimality of Θλ)
≤ - log 1 + (l- 1)eXp(-Maγ?) + λMr	(by equation AZ
=log(1 + (l - 1) exp(-X-a/(r+1)Y?)) + λ1"r+1)
≤ log(1 + (l - 1) exp(-X-a/(r+1)Y?)) +》1/(r+1)
Taking a sufficiently small λ0, We obtain a contradiction and complete the proof.	□
A.2 Full Binary Classification Setting
For completeness, we state and prove our max-margin results for the setting where we fit binary labels
yi ∈ {-1, +1} (as opposed to indices in [l]) and redefining f (Θ; ∙) to assign a single real-valued
score (as opposed to a score for each label). This lets us work with the simpler λ-regularized logistic
loss:
1n
Lλ(Θ)，- Elog(I+ exp(-yif (Θ; x∕) + λ∣∣Θ∣∣
n i=1
As before, let Θλ ∈ arg min Lλ (Θ), and define the normalized margin γχ by Yλ，mini yif (Θ λ; xi).
Define the maximum possible normalized margin
γ?，max minyif (Θ; Xi)	(A.6)
Theorem A.3. Assume γ? > 0 in the binary classification setting with logistic loss. Then as λ → 0,
γλ → γ?.
The proof follows via simple reduction to the multi-class case.
Proof of Theorem A.3. We prove this theorem via reduction to the multi-class case with l = 2.
Construct f : Rd → R2 with fι(Θ; Xi) = 一2f(Θ; Xi) and f2(Θ; Xi) = 2f(Θ; Xi). Define new
labels yi = 1 ifyi = -1 andyi = 2 ify% = 1. Nownotethatfy<Θ; Xi)-fj=y<Θ; Xi) = yif(Θ; x，,
so the multi-class margin for Θ under f is the same as binary margin for Θ under f . Furthermore,
defining
1n
Lλ(θ)，n X - log
i=1
exp(fy"Θ; Xi))
P2=1 exP(fj (θ; Xi))
+ λkΘkr
we get that Lλ(Θ) = Lλ(Θ), and in particular, Lλ and Lλ have the same set of minimizers. Therefore
we can apply Theorem 2.1 for the multi-class setting and conclude γλ → γ? in the binary classification
setting.	□
A.3 Missing Proof for Optimization Accuracy
ProofofTheorem 2.2. Choose B，(γ? log (IT)(Y ' r ) / . We can upper bound Lλ(Θ0) by
computing
Lλ(Θ0) ≤ βLλ(Θλ) ≤ βLλ(BΘ?)
≤ β log(1 + (l - 1) exp(-Baγ?)) + βλBr	(by equation A.2)
≤ β(l - 1) exp(-Baγ?) + βλBr	(using log(1 + X) ≤ X)
≤ β (Y⅛ + βλ
≤ β (Y4Ta (1 +
a
， L(UB)
15
Under review as a conference paper at ICLR 2019
Furthermore, it holds that ∣Θ0kr ≤ L(UB). Now we note that
Lλ(Θ0) ≤ L(UB) ≤ 2β —λ?-
(γ大)r∕a
(l - 1)(Y?)r/a
r/a 1
≤ 2n
λ
for sufficiently large C depending only on a/r. Now using the fact that log(x) ≥ ιxχ ∀x ≥ -1,
we additionally have the lower bound Lλ(Θ0) ≥ 1 log(1 + exp(-γ0kΘ0ka)) ≥ 1 1++Xxp--Y0730：^)∙
Since L(U B) ≤ 1, we can rearrange to get
γ0
≥ - log I-LLθ'Θo) ≥ - log I-LLBB) ≥ -log(2nL(UB))
kΘ0ka
kΘ0ka
kΘ0ka
The middle inequality followed because ι-xχ is increasing in X for 0 ≤ x < 1, and the last because
L(UB) ≤ 2-. Since - log 2nL(UB) > 0 we can also apply the bound ∣∣Θ0kr ≤ LUB) to get
γ0 ≥
-λa"log 2nL(UB)
(L(UB))a/r
We will first bound *.First note that
log((YeLr ) = log (Y?λ"a - log 2βn〉log
log (I-I)(Y*)r/a = log (^ +log(i - 1) ≥ 一
(Y λ /--log 2βn(l — 1)
≥ C-^	(A.7)
C
log 沼士
where the last inequality follows from the fact that (Y λ /
fact that log (Y λ /
≥ nc(l - 1)c and β ≤ 2. Next, using the
1+
≥ (2r∕a-1)a∕r
, we note that
(l - 1)(γ?)r/a
-r/a a/r
a/r
≤ 2	(A.8)
-r/a -a/r	C-3
)	≥ 2c
Combining equation A.7 and equation A.8, we can conclude that
log( (Y*)r/：)(
.=log( 2β-λ )	/ I +
* =log (JI)(Y*)r/： l +
(l - 1)(γ?)r/a
λ
1
λ
Finally, we note that if 1 +
(i-i)(γ? )r/a
λ
r/a
is a sufficiently large constant that depends only on
a/r (which can be achieved by choosing C sufficiently large), it will follow that B ≤ 吉.Thus, if
C ≥ 5, we can combine our bounds on 帛 and B to get that
γ0 ≥ 1⅛a
□
16
Under review as a conference paper at ICLR 2019
B Missing Proof of Proposition 4.1
Proposition 4.1 follows simply from applying Corollary 1 of Neyshabur et al. (2014) to a hard-margin
SVM problem. For completeness, we provide another proof here. The proof of Proposition 4.1 will
consist of two steps: first, show that equation 4.2 has an optimal solution with sparsity n, and second,
show that sparse solutions to equation 4.2 can be mapped to a neural network with the same margin,
and vice versa. The following lemma and proof are based on Lemma 14 of Tibshirani (2013).
Lemma B.1. Let Supp(α)，{u : ∣α(u)∣ > 0}. There exists an optimal solution a? to equation 4.2
with |SUpp(a?)| ≤ n.
For the proof of this lemma, we find it convenient to work with a minimum norm formulation which
we show is equivalent to equation 4.2:
min kαk1
α	(B.1)
subject to yihα, φ(xi)) ≥ 1 ∀i
Claim B.2. Let S ⊂ L1(Sd-1) be the set of optimizers for equation 4.2, and let S0 ⊂ L1(Sd-1) be
the set of optimizers for equation B.1. Ifequation B.1 isfeasible, for any α ∈ S, Ya ∈ S0, and for
any α0 ∈ SO, |岛卜 ∈ S.
Proof. Let opt0 denote the optimal objective for equation B.1. We note that 高卜 is
feasible for equation 4.2 with objective , and therefore γ'ι ≥ 0^. Furthermore,
2^-yi Ju∈sd-ι ɑ(u)φ(u>xi)du ≥ 1 ∀i, and so / is feasible for equation B.1 with objective
十.Therefore, opt0 ≤ 十.As a result, it must hold that opt0 =十,which means that 信心 is
optimal for equation 4.2, and γa- is optimal for equation B.1, as desired.	□
First, note that if equation B.1 is not feasible, then γ'ι = 0 and equation 4.2 has a trivial sparse
solution, the all zeros function. Thus, it suffices to show that an optimal solution to equation B.1
exists that is n-sparse, since by Lemma B.2 equation B.1 and equation 4.2 have equivalent solutions
up to a scaling. We begin by taking the dual of equation B.1.
Claim B.3. The dual of equation B.1 has form
max λ>~1
λ∈Rn
n
subject to ɪ2 λiyiφ(u>xi) ≤ 1 ∀u ∈ Sd-1
i=1
λi ≥ 0
For any primal optimal solution α? and dual optimal solution λ?, it must hold that
n
^X λ:yiφ(u>xi) = sign(α?(U)) ^⇒ a?(u) = 0	(B.2)
i=1
Proof. The dual form can be solved for by computation. By strong duality, equation B.2 must follow
from the KKT conditions.	□
Now define the mapping V : SdT → Rn with Vi(U)，yiφ(u>xi). We will show a general result
about linearly dependent V(U) for U ∈ supp(α?), after which We can reduce directly to the proof of
Tibshirani (2013).
Claim B.4. Let α? be any optimal solution. Suppose that there exists S ⊆ supp(α?) such that
{v(u) : U ∈ S} forms a linearly dependent set, i.e.
X CuV(U)=~	(B.3)
u∈S
for COeffiCientS c. Then Eu∈s Cu sign(α?(U)) = 0.
17
Under review as a conference paper at ICLR 2019
Proof. Let λ? be any dual optimal solution, then λ*>v(u) = sign(α?(U)) ∀u ∈ supp(α?) by Claim
B.3. Thus, We apply λ?> to both sides of equation B.3 to get the desired statement.	□
Proof of Lemma B.1. The rest of the proof follows Lemma 14 in Tibshirani (2013). The lemma
argues that if the conclusion of Claim B.4 holds and an optimal solution α? has S ⊆ supp(α?)
with {v(u) : U ∈ S} linearly dependent, we can construct a new a0 with ∣∣α]∣ι = ∣∣α*kι and
supp(α0) ⊂ supp(α?) (Where the inclusion is strict). Thus, if We consider an optimal α? With
minimal support, it must follow that {v(u) : U ∈ supp(α?)} is a linearly independent set, and
therefore |supp(a?)| ≤ n.	□
We can now complete the proof of Proposition 4.1.
Proof of Proposition 4.1. For ease of notation, we will parametrize a two-layer network with m units
by top layer weights w1, . . . , wm ∈ R and bottom layer weights U1, . . . , Um ∈ Rd. As before, we
use Θ to refer to the collection of parameters, so the network computes the real-valued function
m
j=1
Note that we simply renamed the variables from the parametrization of equation 3.1.
We first apply Lemma B.1 to conclude that equation 4.2 admits a n-sparse optimal solution α?.
Because of sparsity, we can now abuse notation and treat α? as a real-valued function such that
Pu∈supp(α?) ∣α?(u)∣ ≤ 1. We construct Θ corresponding to a two-layer network with m ≥ n hidden
units and normalized margin at least γ'1. For clarity, we let W correspond to the top layer weights
and U correspond to the bottom layer weights. For every U ∈ supp(α), we let Θ have a corresponding
hidden unit j with (wj,Uj) = (sign(α?(U)) J |a*2u)|, J |a?Ju)| U), and set the remaining hidden
units to ~0. This is possible because m ≥ n. Now
m1
f(Θ; x) = ɪ2 WjΦ(u>x) = -	0?(	α*(U)φ(U>x)
j = 1	u∈supp(α?)
Furthermore,
kθk2 = xxWj + kUjk2 = X	+ WU4研2 = X	|a?(U)| ≤ 1
j=1	u∈supp(α)	u∈supp(α)
Thus it follows that Θ has normalized margin at least γ'1 /2, so Y?m ≥ Y'ι∕2.
To conclude, we show that Y?,m ≤ γ'ι/2. Let Θ*,m denote the parameters obtaining optimal m-unit
margin Y?,m with hidden units (w?,m, U?,m) for j ∈ [m]. We can construct a to put a scaled delta
mass of 2w?,mkU?,mk2 on U?,m for j ∈ [m]. It follows that
mm
IlallI = X2|w?,m|||U?,mk2 ≤ XWrm + kU?,mk2 = kΘ*,mk2 ≤ 1
j=1	j=1
Furthermore,
m
J	a(U)φ(U>x) = 2 X w?,mkU?,mk20((U?,m)>x)
m
=2 X w?，m0(U?，m>x) = 2f (Θ*,m; x)
j=1
Thus, a is a feasible solution to equation 4.2 with objective value at least 2γ*,m. Therefore, γg∖ ≥
2γ?，m, so γ?，m = γ'1∕2.	□
18
Under review as a conference paper at ICLR 2019
C Rademacher Complexity and Generalization Error
We prove the generalization error bounds stated in Proposition 3.1 and Lemma 4.2 via Rademacher
complexity and margin theory.
Assume that our data X, Y are drawn i.i.d. from ground truth distribution pdata supported on X × Y.
For some hypothesis class F of real-valued functions, we define the empirical Rademacher complexity
ʌ , 一、 一一一
R(F) as follows:
R(F) , 1 Eei
n
n
sup	if(xi)
f∈F i=1
where i are independent Rademacher random variables.
For a classifier f, following the notation of Section 3 We will use L(f)，Pr(χ,y)〜Pdata (yf (x) ≤ 0)
to denote the population 0-1 loss of the classifier f . The following classical theorem (Koltchinskii
et al., 2002), (Kakade et al., 2009) bounds generalization error in terms of the Rademacher complexity
and margin loss.
Theorem C.1 (Theorem 2 of Kakade et al. (2009)). Let (xi, yi)in=1 be drawn iid from pdata. We
work in the binary classification setting, so Y = {-1, 1}. Assume that for all f ∈ F, we have
supx∈X f(x) ≤ C. Then with probability at least 1 - δ over the random draws of the data, for every
γ > 0 and f ∈ F,
…、1	,, 4	4	4R(F)	∕loglog2 4C	/log(1∕δ)
L(f) ≤— E i(yif (Xi) < γ) +	+ ∖ -----------L + ∖	； / '
n	γ	n	2n
C.1 Proof of Proposition 3.1
We will prove Proposition 3.1 by applying the Rademacher complexity bounds of Golowich et al.
(2017) with Theorem C.1.
First, we show the following lemma bounding the generalization of neural networks whose weight
matrices have bounded Frobenius norms.
Lemma C.2. Define the hypothesis class FK over depth-K neural networks by
FK = f((Θ; ∙) ： kWjkF ≤ √K ∀j}
Let C，supχ∈χ kx∣∣2. Recall that L(Θ) denotes the 0-1 population loss L(f (Θ; ∙)). Then
for any f (Θ; ∙) ∈ FK classifying the training data CorrectIy with unnormalized margin yθ ，
mini yi f(Θ; xi ) > 0, with probability at least 1 - δ,
L (θ).	C	+ ʌ ∕iogiog2 4θ + r iog(i∕δ)	(CI)
L⑻.γθK(KT)/2√n + V n + V n	(CI)
Note the dependence on the unnormalized margin rather than the normalized margin.
Proof. We first claim that sup”®；,)*® supχ∈χ f (Θ; x) ≤ C. To see this, for any f (Θ; ∙) ∈ FK,
f (Θ; X) = WKφ(…φ(Wιx)…)
≤ IWkkF∣∣Φ(Wk-iΦ(…φ(Wιx)…)k2
≤ kWK kF kWK-lΦ(…Φ(W1X)…)k2
(since φ is 1-Lipschitz and φ(0) = 0, so φ performs a contraction)
<kXk2≤C	(repeatedly applying this argument and using kWj kF < 1)
Furthermore, by Theorem 1 of Golowich et al. (2017), R(FK ) has upper bound
C
19
Under review as a conference paper at ICLR 2019
Thus, We can apply Theorem C.1 to conclude that for all f (Θ; ∙) ∈ FK and all γ > 0, with probability
1 - δ,
rc∖∕ 1 G” £“、 S 、	C	Ilog log2 4C	八Og(1∕δ)
L(θ). n X 1(yif (θ;Xi) <γ)+ YK(KT)/2 √n + X -n- + V -n-
In particular, by definition choosing γ = γΘ makes the first term on the LHS vanish and gives the
statement of the lemma.	口
Proof of Proposition 3.1. Given parameters Θ = (W1, . . . , WK), we first construct parameters
Θ = (Wι,..., WK) such that f (Θ; ∙) and f (Θ; ∙) compute the same function, and ∣∣Wι∣∣F =
∣W2kF =…=IIWWKkF ≤ KK. To do this, we set
(QK=1 kWkkF)1/k
kWj kF ∣Θ∣f
By construction
kWjkF
(QK=IkWk kF )1/k
kθkF
(QK=1 kWk kF )1/k
PK=I kWk kF
1
≤ —
_ k
(by the AM-GM inequality)
Furthermore, we also have
,~ ~ , , ~ .
f (Θ; x) = WKφ(…φ(Wιx)…)
= Y (QIk=I k∣WkkF) / WKφ(…Φ(Wιx)…) (by the homogeneity of φ)
j=1	kWj kF kΘkF
=Wf®x)
Θ
=f ( HOH ; xJ	(since f is K-homogeneous in Θ)
=f (Θ; x)
-V T	,,1, F	.	. ∙	7- / rʌ ∖	7- / ʌ ∖ -VT	ι∙ / 入 、	. 1	1	∙ Γ∙ .t .	1
Now we note that by construction, L(Θ) = L(Θ). Now f (Θ; ∙) must also classify the training data
perfectly, has unnormalized margin γ, and furthermore f (Θ; ∙) ∈ FK. As a result, Lemma C.2 allows
us to conclude the desired statement.	口
To conclude Corollary 3.2, we apply the above on Θλ,M and use Theorem A.3.
C.2 Proof of Kernel Generalization Bounds
Let FB,φ denote the class of '2-bounded linear functionals in lifted feature space: F?，{x →
ha,夕(x)i : α ∈ L2(Sd-1), ka∣∣2 ≤ B}. We abuse notation and write α ∈ FB,φ to indicate a linear
functional from FB2,φ. As before, we will use L(α) to indicate the 0-1 population loss of the classifier
x → hα,夕(x)i and let C，suPχ∈χ ∣∣x∣2 be an upper bound on the norm of the data. We focus on
analyzing the Rademacher complexity R(FB,φ), mirroring derivations done in the past (Bartlett &
Mendelson, 2002). We include our derivations here for completeness.
Lemma C.3. R(FB(φ) ≤ 1 BpPn=Ik夕(Xi)k2∙
20
Under review as a conference paper at ICLR 2019
Proof. We write
R(FB,φ)=
1 Ee.
n ei
n
SuP haA2ei^(xi)i
α∈FB2,φ	i=1
1 Eei
sup
α∈FB2,φ
IlaIl2 EG夕(Xi)
i=1
Eei	Eei 中(Xi)
i=1
Eei H∣Σ ")|
(via Jensen’s inequality)
≤
≤
≤
n
2
1B •
n
1B
n
1B
n
n
n
2
2
nn
≤
Eei	Gejh夕(Xi),夕(xi)i
i=1 j=1
1BA
n
≤
n
X Il^(xi)k2
i=1
(terms where i 6= j cancel out)
□
As an example, we can apply this bound to relu features:
Corollary C.4. Suppose that φ is the relu activation. Let K，Vol(SdT). Then R(FB,φ)
BkXkF√K V BC√K
n√d	- √dn ∙
Proof. WefirstshoWthatk夕(xi)k2 = Θ (dIlXiII2). We can compute
Ik(Xi)k2 = Vol(SdT)Eu 〜sd-ι [relu(u>xi)2]
K Eu 〜sd-ι [relu(√du>Xi)2]
d
dM Eu 〜N (0,id×d)[relU(UTXi)2]
θ (d 11/2)
(M2 is the second moment of N (0, 1))
(C.2)
Where the last line uses the computation provided in Lemma A.1 by Du et al. (2017). NoW We plug
this into Lemma C.3 to get the desired bound.	□
We Will noW prove Lemma 4.2.
ProofofLemma 4.2. From equation C.2, we first obtain suPχ∈χ ∣∣夕(x)∣∣2 . Cʌ/d. Denote the
optimizer for equation 4.3 by a`?. Note that √Ka£2 ∈ F2,φ, and furthermore l(a`2) = L(√κa'2).
Since √Ka^2 has unnormalized margin √κγ'2, we apply Theorem C.1 on margin √κγ'2 and
hypothesis class F12,φ to get with probability 1 - δ,
L'2-svm = L(√κα'2 ) ≤
I ∕lθglθg24su⅞≡
n
C
Y'2 √dn
+
八οg(1∕δ)
V	2n
lοgmaχ{lοg2 √C2, 2}	rιοg(1∕δy
+ ∖-----------n----------+ V
(applying Corollary C.4)
□
21
Under review as a conference paper at ICLR 2019
D Missing Proofs for Comparis on to Kernel Methods
D. 1 Classification
In this section we will complete a proof of Theorem 4.3. Recall the construction of the distribution D
provided in Section 4. We first provide a classifier of this data with small `1 norm.
Lemma D.1. In the setting of Theorem 4.3, we have that
γ'ι ≥ ɪ.
Proof. Consider the network f (x) = 4 ((x>(eι + e2)∕√2)+ + (x>(-eι - e2)∕√2)+ - (x>(-eι +
e2)∕√2)+ - (x>(eι - e2)∕√2)+). The attained margin Y = √42, so r`, ≥ √42.	□
Now we will upper bound the margin attainable by the `2 SVM.
Lemma D.2 (Margin upper bound tool). In the setting of Theorem 4.3, we have
1
Y'2 ≤√K ∙
1n
n£。(Xi)yi
i=1	2
Proof. By the definition of γg, we have that for any α with √K∣∣αk2 ≤ 1, we have
1n
Y'2 ≤ max	— V^hα,yi夕(xi)i
√Kkαk2≤1 n i=1
Setting α = √^n1 Pn=I φ(χi)yi∕k n Pn=I 2(χi)yi∣∣2 completes the proof. (Attentive readers may
realize that this is equivalent to setting the dual variable of the convex program 4.3 to all 1’s
function.)	□
Lemma D.3. In the setting of Theorem 4.3, let (xi, yi)in=1 be n i.i.d samples and corresponding
labels from D. Let 夕 be defined in equation 4.1 with φ = relu. With high probability (at least
1 - dn-10), we have
1	n	____
一工中(Xiyyi	. K/nlo log n + √κ"
n
i=1	2
Proof. Let Wi =夕(Xi)yi. We will bound several quantities regarding Wi,s. In the rest of the proof,
we will condition on the event E that ∀i, kXik22 . dlogn. Note that E is a high probability event
and conditioned on E, Xi ’s are still independent. We omit the condition on E in the rest of the proof
for simplicity.
We first show that assuming the following three inequalities that the conclusion of the Lemma follows.
1.	∀i, kWik22 . κlogn .
2.	σ2 , Var[PiWi] , Pin=1 E[kWi - EWik22] . nκlogn
3.	k EP Wi] k 2 . √Kn∕d.
By bullets 1, 2, and Bernstein inequality, we have that with probability at least 1 - dn-10 over the
randomness of the data (X, Y ),
n
X Wi - E
i=1
X WiU . √κ log1.5 n + QnK log2 n .
nκ log2 n
22
Under review as a conference paper at ICLR 2019
By bullet 3 and equation above, we complete the proof with triangle inequality:
n
X Wi	≤
i=1	2
X WiJ +qnκ log2 n .
E
Jnκ log2 n + √κn∕d
Therefore, it suffices to prove bullets 1, 2 and 3. Note that 2 isa direct corollary of 1 so we will only
prove 1 and 3. We start with 3:
By the definition of the '2 norm in L2 (Sd-I) and the independence of (xi, yjs, we can rewrite
X Wt
K ∙ n2	E
U 〜Sd-1
E
2
E	夕(χ)[u]∙ y
_(x,y)〜D
(D.1)
Let U
(x>e3,
=(U1,...,Ud) and U-2 = (U3,...,Ud) ∈ Rd-2, and define T，∣∣U-2∣∣2. Let χ-2 =
...,x>ed). Note that 夕(x)[u]y = y[U1 ∙ x>eι + U ∙ x>e2 + U[2x-2] and uZ2X-2 has
distribution ∣∣U-2∣∣2 ∙ N(0,1) = T ∙ N(0,1). Let Z = U>2x-2∕τ, and therefore Z has standard
normal distribution. With this change of the variables, by the definition of the distribution D, we have
E 夕(x)[u] ∙ y = ɪ E [(Uι + u2 + τz)+] + ɪ E [(-uι - U2 + τz)+]
(x,y)〜D	4 Z〜N(0,1)	4 Z〜N(0,1)
—
E E [(U1 — U2 + Tz) + ] — — E	[(+U1 — U2 + Tz)+]
4 Z〜N(0,1)	4 Z〜N(0,1)
By claim D.4, and the 1-homogeneity of relu, we can simplify the above equation to
E	夕(χ)[u]∙ y
(χ,y)〜D
=4T .(2。1 + O(min{∣U1 + u2∣∕τ, ∣U1 + U2∣2∕τ2})
-4(2。1 - Olminnu1 - ^2^7,^1 - uz12"2})))
.min{∣U1∣ + ∣U2∣, (∣U1∣ + ∣U2∣)2∕τ}
It follows that
E
U 〜SdT
E	夕(χ)[u]∙ y
_(x,y)〜D	一
2
.Eu [min{(∣U1∣ + |还|)2, (∣U11 + ∣U2∣)4∕∣∣U-2∣∣2}]
.Eu [(∣U1∣ + ∣U2∣)2 ∙ 1[∣∣U-2∣∣2 ≤ 1∕2]] + Eu [(∣U1∣ + ∣U2∣)4∕∣∣u-2∣∣2 ∙ l[ku-2k2 ≥ 1∕2]]
.exp(-√d) + Eu [(∣U1∣ + ∣U2∣)4] . 1∕d2	(D.2)
Combining equation D.1 and equation D.2 we complete the proof of bullet 3. Next we prove
bullet 1. Note that 夕(χ)[u]y is bounded by ∣U1∣ + ∣U2∣ + ∣∣U>2x-2∣∣2. Therefore, conditioned on
IIXik2 . dlogn
kWik2 ≤	E	[(∣U1∣ + ∣U2∣ + ku>2χ-2k2)2]
u 〜SdT
.E	[∣U1∣2] + E	[∣U2∣2] + E	[∣∣U>2X-2k2]
u 〜SdT	u 〜SdT	u 〜SdT
.1∕d+kχ-2k2∕d. logn
Hence we complete the proof.	□
Claim D.4. Let Z 〜N(0,1) and a ∈ R. Then, there exists a universal constant c1 and c2 such that
∣E [(a + Z)+ + (-a + Z)+] — 2c11 ≤ c2 min{∣a∣, a2}.
Proof. Without loss of generality we can assume a ≥ 0. Then,
E [(a + Z)+ + (-a + Z)+] = E [(a + Z)1[Z ≥ -a]] + E [(Z — a)1[Z ≥ a]]
=E [a ∙ 1[Z ≥ -a]] + E [Z ∙ 1[Z	≥ -a]] - E [a ∙ 1[Z	≥ a]] + E [Z ∙ 1[Z ≥ a]]
=E [a1[-a ≤ Z ≤ a]] +2 E [Z ∙	1[Z ≥ a]]	(by E [Z ∙ 1[-a ≤ Z	≤ a]] = 0)
=E [a1[-a ≤ Z ≤ a]] +2 E [Z ∙	1[Z ≥ 0]] - 2 E [Z ∙	1[a ≥ Z ≥ 0]]
=2c1 + O(min{a, a2})
23
Under review as a conference paper at ICLR 2019
where the last equality uses the fact that ci ，E [Z ∙ 1[Z ≥ 0]] and E [a1[-a ≤ Z ≤ a]] ≤
aE [1[-a ≤ Z ≤ a]] . amin{1,a}.	□
Now we will prove Theorem 4.3.
Proof of Theorem 4.3. To circumvent the technical issue of bounded support in Proposition 3.1 and
Lemma 4.2, we construct pdata to be a slightly modified version of D: perform rejection sampling
of (x,y)〜D until we obtain a sample with ∣∣xk2 . dlogn. Since this occurs with very high
probability, the high probability result of Lemma D.3 still translates to pdata. Now apply Lemma D.2
to conclude that γg . Iognn + d. Furthermore, LemmaD.1 allows US to conclude that γ'ι & 1.
We can therefore apply Proposition 3.1, and conclude that with probability 1 - δ,
Le	. /d log n + ∕loglog(d log n) + /log(1∕δ)
`1 -svm	n	n	n
Furthermore, plugging 7尬 into the bound of Lemma 4.2 gives US
min
log log(dn)	/log(1∕δ)
—n — + v
□
D.2 Regression
We will first define the 1-norm and 2-norm regression problems. The regression equivalent of
equation 4.2 for α ∈ L1(Sd-1) is as follows:
ɑ`i ∈ arg min ∣∣α∣ i
α	(D.3)
subject to hα,夕(xi)i = yi
Next we define the regression version of equation 4.3:
α'2 ∈ arg min ∣∣α∣2
α	(D.4)
subject to hα,夕(xi)i = yi
where α ∈ L2 (Sd-1 ).
We will briefly motivate our study of the regression setting by connecting the minimum 1-norm solu-
tion to neural networks. To compare, in the classification setting, optimizing the weakly regularized
loss over neural networks is equivalent to solving the `1 SVM. In the regression setting, solving the
weakly regularized squared error loss is equivalent is equivalent to finding the minimum 1-norm
solution that fits the datapoints exactly.
Theorem D.5. Let f (Θ; ∙) be some two-layer neural network with m ≥ n hidden units parametrized
by Θ, as in Section 4. Define the λ-regularized squared error loss
1n
Lλ,m(Θ)，-]T(f (Θ; Xi)-yi)) + λ∣Θ∣2
n i=1
with Θλ,m ∈ argmi□θ Lλ,m(Θ). Suppose that equation D.3 is feasible with optimal solution ɑ`i.
Then as λ → 0, Lλ,m(θλ,m) → 0 and ∣∣θλ,mk2 → 2kα'ι k 1.
Proof. We can see that equation D.3 will have a n-sparse solution α? using the same reasoning
as the proof of Lemma B.1. Furthermore, following the proof of Proposition 4.1, the function
x →〈a?,夕(x)〉is implementable by a neural network Θ*,m with ∣∣Θ*,m∣2 = 2∣α*kι = 2∣α'ιkι.
Following the same reasoning as before, We can also conclude that Θ*,m is an optimal solution for:
min ∣Θ∣22
Θ	(D.5)
subject to f(Θ; xi) = yi
24
Under review as a conference paper at ICLR 2019
NoWWenotethat λ∣∣Θλ,mk2 ≤ Lλ,m(Θλ,m) ≤ Lλ,m(Θ*,m) = λ∣∣Θ*,mk2, so as λ → 0, and also
∣∣Θλ,m∣∣2 ≤ ∣∣Θ*,mk2. Now assume for the sake of contradiction that ∃B with ∣∣Θλ,m∣∣2 ≤ B <
kθ*,m∣∣2 for arbitrarily small λ. We define
1n
r?，mΘn- £(f (Θ; Xi) - yi)2
n i=1
subject to kΘk2 ≤ B
Note that r? > 0 since Θ*,m is optimal for equation D.5. However, Lλ,m ≥ r? for arbitrarily small
λ, a contradiction. Thus, limλ→o ∣∣Θ*,mk2 = ∣∣Θ*,mk2.	□
We proceed to provide similar generalization bounds as the classification setting. This time, our
bounds depend on the norms of the solution rather than the margin. Let f φ(α; ∙)，x → (α,夕(x)i
(for 夕(x) defined in equation 4.1). Following the convention in Section C.2, define hypothesis class
FB,φ，{fφ(α; ∙) : α ∈ LI(SdT), kα∣∣ι ≤ b} of linear functionals bounded by B in 1-norm. As
before, define FBcφ and let R(F) denote the empirical Rademacher complexity of hypothesis class
F.
We will first derive a Rademacher complexity bound for FB1,φ.
ClaimD.6. R(FB,φ) ≤ *B% [kP乙 ^iψ(xi)k∞]∙
Proof∙ We write
1n
R(FB,φ) = -Eei	sup (a.”*
n	α∈FB1,φ	i=
n
Eemxi)
i=i
≤ - B ∙
n
≤ -Eei	SUp kaki
n	α∈FB1,φ
n
Eemxi)
i=i	∞
□
We will now complete the bound on R(FB,φ) for Lipschitz activations φ with φ(0) = 0.
Claim D.7. Suppose that our activation φ is M -Lipschitz and φ(0) = 0∙Then
R(FB,φ) ≤
3BMVZP乙 kxik2
n
Proof∙ We will show that
Eei
n
Eemxi)
i=i	∞
≤ 3Mt
n
X kxik22
i=i
from which the statement of the lemma follows via Claim D.6. FiX any U0 ∈ Sd-i. Then we get the
decomposition
Ee
ei
sUp
u∈sd-1
n
i=i
U>xi)
n
≤ Eei	y^eiφ(u0>xi)	+
i=i
Eei	sUp
u∈sd-1
n
∑>φ(U>xi)
i=i
n
-inf	Teiφ(U>xi)
u∈sd-1 々I
i=i
(D.6)
25
Under review as a conference paper at ICLR 2019
We can bound the first term as
n
Eei	∑>Φ(U'>Xi)
i=1
≤
∖
n
X φ(U0>Xi)2
i=1
≤M
≤Mt
n
X(u0>Xi)2
i=1
(since φ is Lipschitz and φ(0) = 0)
un
≤ M uX kxik22
i=1
(D.7)
We note that the second term of equation D.6 can be bounded by
Eei	sup
2 u∈Sd-1
n
EEiφ(u> Xi)
i=1
- inf
u∈sd-1
n
EEiφ(U>Xi )
i=1
nn
≤ Eei	sup TEiφ(U>Xi)- inf TEiφ(U>Xi)
[u∈sd-1 i=1	u∈Sd-1 i=1
This follows from the general fact that the difference between the supremum and infimum of the
absolute value of a quantity is bounded by the difference between the supremum and the infimum.
Furthermore, by symmetry of the Rademacher random variables,
nn
sup	Eiφ(u>Xi) — inf	Eiφ(u> Xi)
u∈sd-1 i=1	u∈Sd-1 i=1
≤ 2Eei
n
sup Vj Eiφ(u>Xi)
u∈sd-1 i=1
(D.8)
This simply gives an empirical Rademacher complexity of the hypothesis class F，{x → φ(u>x):
U ∈ Sd-1} scaled by n. By the Lipschitz contraction property of Rademacher complexity, using the
fact that φ is M -Lipschitz, we can therefore bound equation D.8 by
n
sup ∑Eiφ(U>Xi)
u∈sd-1 i=1
un
≤ 2M uX kXik22
i=1
(D.9)
Plugging equation D.7 and equation D.9 back into equation D.6 gives the desired bound. □
The following is a generalization bound based on the 1-norm:
Lemma D.8. Let l(∙; y) : R → [—c, c] be a bounded M-Lipschitz lossfunction. Assume that φ is a
1-Lipschitz activation with φ(0) = 0. Let (Xi, yi)in=1 be drawn i.i.d from pdata. Then with probability
at least 1 — δ over the dataset, every α ∈ L1(Sd-1) satisfies
E(X,y )~Pdata [l(f φ(α; X); y)] ≤
1 X l(fφ(a; Xi); yi) + 12Mma^1,k。"1 kXkF}
nn
i=1
+c
八og(1∕δ) + log(max{1, 2kαkιkXkF})
V	2n
Proof. Our starting point is Theorem 1 of Kakade et al. (2009), which states that with probability
1 — δ, for any fixed hypothesis class F and f ∈ F,
E(χ,y)〜Pdata[l(f(x);y)] ≤ 1 Xl(f(xi);yi)+2MR(F)+ c∖∕l°g^	。⑼
nn
i=1
26
Under review as a conference paper at ICLR 2019
We define Bj，∣∣Xj山 for j ≥ 0. We note that by Claim D.7, R(FBB,φ) ≤ 32j. and apply the above
on FB,φ using δj，ji. Then using a union bound, with probability 1 - P∞=o δj = 1 - δ, for all
j ≥ 0 a∏d f φ(α; ∙) ∈FB,jφ
E(χ,y)~Pdata[l(fφ(α; x)； y)] ≤ 1 XX l(fφ(α; Xi)； yi) + 2M R(FBjφ)+ c∖ log(lj
nn
i=1
≤ 1 XX l(f Φ(α; Xi); yi) + 6M Z + Cr b如⑷十^经仪
n	nn
i=1
Now for every α with ∣∣αk1 <	, we use the inequality for FBf, and for every other α, we
apply the inequality corresponding to FB1,φ , where 2j ≤ kαk1kXkF ≤ 2j+1. This gives the desired
statement.	口
We can also provide the same generalization error bound for the 2-norm and relu features:
Lemma D.9. In the setting of Lemma D.8, choose φ to be the relu activation. Then with probability
1 - δ, every α ∈ L2 (Sd-1) satisfies
E(x,y )~pdata [l(f φ(α; X); y)]
1n
-∑l(fφ(α; Xi); yi) + M
n i=
√Kmax{1, kα∣∣2∣∣XIIF}
n√d
+c
"g(1∕δ) + log(max{1, ∣∣ɑ∣∣2∣∣XIIFi)
2	2n
Proof. We proceed the same way as in the proof of Lemma D.8. We define Bj as before, and this
time have R(Fl：) . n√j from Corollary C.4. Thus, again union bounding over all j, equation
equation D.10 gives with probability 1 - δ, for all j ≥ 0 and f φ(α; ∙) ∈ F∣,φ
E(χ,y)~Pdata[l(fφ(α; x); y)] . 1 XX l(fφ(α; Xi); yi) + M% + Cr log(1∕δ) + log(I+I
n i=1	n d	n
Now we assign the α to different j as before to obtain the statement in the lemma.	口
Note that if l is some bounded loss such that l(y; y) = 0 (for example, truncated squared error), for
ɑ`i and αg the loss terms over the datapoints (in the bounds of Lemmas D.8 and D.9) vanish. For
loss l, define
L'l-reg , Eχ,y~Pdata [l(f °(a'i; x); y)]
L'2-reg , Eχ,y~pdata [l(f °(a'2 ; x); y)]
Next, we will define the kernel matrix K with Kij =<夕(xi),夕(Xj)). Now we are ready to state and
prove the formal theorem describing the gap between the 1-norm solution and 2-norm solution.
Theorem D.10. Recall the definitions of a`i and a`? in equation D.3 and equation D.4. For any
activation φ with the property that K is full rank for any X with no repeated datapoints, there exists
a distribution pdata such that with probability 1,
kα'ι∣1 ≤ 1
On the other hand,
E(Xi,yi)i=1^iidPdata [Iα'2 k2]
n
κ
For i.i.d samples from this choice of Pdata, if l is bounded (l(∙; y) : R → [—1,1]), 0 on correct
predictions (l(y; y) = 0), and 1-Lipschitz, then with probability 1 - δ,
∣~d	/log(1∕δ) + log n
L'ι-reg. Vn + V-----------n-------
Meanwhile, in the case that kɑ`2 ∣2 ≥ K, the upper bound on L'2-reg from Lemma D.9 is Ω(1) and in
particular does not decrease with n.
27
Under review as a conference paper at ICLR 2019
We will first show that for any dataset X, there is a distribution over Y such that the expectation of
∣∣α'2 k2 is large. When it is clear from context, y will denote the vector corresponding to Y.
Lemma D.11. There is a distribution A over L1 (Sd-1) such that for any dataset X with yi ,
hφ(xi),β,i for β 〜A,
Ee[kɑ'212] ≥ n
κ
and with probability 1,
kα'ι∣1 ≤ 1
We note the order of the quantifiers in Lemma D.11: the distribution A must not depend on the
dataset X. We first provide a simple closed-form expression for kɑ`2 ∣2.
Claim D.12. If K isfull rank, then kɑ`2 ∣∣2 = y>K Ty.
Proof. This follows by taking the dual of equation D.4.	□
ProofofLemma D.11. We sample β 〜A as follows: first sample U 〜SdT uniformly. Then set β to
have a delta mass of 1 at U and be 0 everywhere else. Define the vector Vu，[φ(u>xι)…φ(u>xn)];
then it follows that we set our labels y to Vu. It is immediately clear that kɑ`i ∣1 ≤ ∣β∣1 ≤ 1.
To lower bound Ee[ka`2 ∣2], from Claim D.12 we get
Ee 〜/[kα'21∣2] = Eu 〜SdT [v> K TVu]
=Eu〜sd-ι [traceKT(Vuv>)]
=trace(K T Eu 〜sd-ι [vuv> ]
= Ltrace(KTK)	(by definition of K)
κ
n
=一
κ
□
Proof of Theorem D.10. We note that since the distribution A of Lemma D.11 does not depend on
the dataset X, it must follow that
E(χi)n=1 〜mN(0,id×d) [Ee~A[kα'2II2]]
Ee〜A [E(χi)n=1 〜mN(o,id×d)[kα'2II2]]
n
κ
n
κ
Thus, there exists β? such that if we sample X i.i.d. from the standard normal and set yi =
hφ(χi), β?i, the expectation of ka`2 ∣2 is at least K∙ We choosePdata corresponding to this β?, with
x sampled from the standard normal. Now it is clear that pdata will satisfy the norm conditions of
Theorem D.10.
For the generalization bounds, with high probability ∣∣X∣∣f = Θ(√nd) as X is sampled from the
standard normal distribution. Thus, Lemma D.8 immediately gives the desired generalization error
bounds for L'「reg. On the other hand, if kɑ`2 ∣∣2 ≥ n∕^^h, then the bound ofLemmaD.9 is at least
√K∣∣α'2 I^XIIf
n√d
≥ Ω(1)
□
E Missing Proofs in Section 5
E.1 Detailed Setup
We first write our regularity assumptions on Φ, R, and V in more detail:
28
Under review as a conference paper at ICLR 2019
Assumption E.1 (Regularity conditions on Φ, R, V ). R is convex, nonnegative, Lipschitz, and
smooth: ∃Mr, Cr such that ∣∣V2Rkop ≤ Cr, and ∣∣VRk2 ≤ Mr.
Assumption E.2. Φ is differentiable, bounded and Lipschitz on the sphere: ∃BΦ , MΦ such that
∣∣Φ((9)k ≤ Bφ ∀θ ∈ Sd, and — Φi(θ0)| ≤ Mφ∣θ —少∣∣2 ∀θ. θ0 ∈ Sd.
Assumption E.3. V is Lipschitz and upper and lower bounded on the sphere: ∃bV , BV , MV such
that0 <bv ≤ V(θ) ≤ BV ∀θ ∈ Sd, and ∣∣VV(8)∣∣2 ≤ MV ∀θ ∈ Sd.
We state the version of Theorem 5.3 that collects these parameters:
Theorem E.4 (Theorem 5.3 with problem parameters). Suppose that Φ and V are 2-homogeneous
and Assumptions E.1, E.2, and E.3 hold. Fix a desired error threshold > 0. Suppose that from a
starting distribution ρ0, a solution to the dynamics in equation 5.2 exists. Choose
σ , exp(-d log(1/)poly(k, MV, MR, MΦ, bV, BV, CR, BΦ, L[ρ0] - L?))
d2
t，在poly(log(1∕e), k, MV, Mr, Mφ, bv, BV, Cr, Bφ, L[ρ0] — L?)
Then it must hold that min0≤t≤t L[ρt] - infρ L[ρ] ≤ 2.
E.2 Proof of Theorem E.4
Throughout the proof, it will be useful to keep track of Wt，E‰rt^pt [∣θ∣2], the second moment of
ρt. We first introduce a general lemma on integrals over vector field divergences.
Lemma E.5. For any h1 : Rd+1 → R, h2 : Rd+1 → Rd+1 and distribution ρ with ρ(θ) → 0 as
∣θ∣ → ∞,
/ hι(θ)V∙ (h2(θ)ρ(θ))dθ = -Eθ^p[hVhι(θ), h2(θ)i]
Proof. The proof follows from integration by parts.	□
We note that ρt will satisfy the boundedness condition of Lemma E.5 during the course of our
algorithm - ρ0 starts with this property, and Lemma E.9 proves that ρt will continue to have this
property. We therefore freely apply Lemma E.5 in the remaining proofs. We first bound the absolute
value of L0[ρt] over the sphere by BL , MRBΦ + BV.
LemmaE.6. Forany θ ∈ Sd-1,t ≥ 0, ∣L0[ρt](θ)∣ ≤, BL.
Proof. We compute
∣L0[Pt](θ)l =
≤
,ΦW)) + V ⑻
VR
∣Φ(θ)∣2 + V(θ) ≤ MrBφ + BV
2
□
Now we analyze the decrease in L[ρt].
Lemma E.7. Under the perturbed Wasserstein gradient flow
ddtL[ρt] = -σEθ〜pt[L0[ρt](θ)]+ σEθ•〜Ud[L0[ρt](θ)] - Eθ〜pt[∣v[Pt](θ)k2]
29
Under review as a conference paper at ICLR 2019
Proof. Applying the chain rule, we can compute
ddtL[ρt]
(VR U(MPl，d J(MG + dt JVdpt
d Eθ 〜Pt[L0[pt](θ)]
L0[ρt](θ)ρ0t(θ)dθ
-Q / L0[Pt]dPt + σ J L0[Pt]dUd - / L0[pt](θ)v ∙ (v[pt](θ)pt(θ))dθ
-σEθ〜ρt[L0[pt](θ)] + σE几Ud[L0[pt](θ)] - Eθ〜ρt [∣∣v[pt](θ)k2],
where we use Lemma E.5 with h1 = L0 [ρt] and h2 = v[ρt].
□
Now we show that the decrease in objective value is approximately the average velocity of all
parameters under ρt plus some additional noise on the scale of σ. At the end, we choose σ small
enough so that the noise terms essentially do not matter.
Corollary E.8. We Can bound 今 L[pt] by
dtL[pt] ≤ qBl(Wt + 1) - Eθ〜ρt[∣lv[pt](θ)ll2]	(E∙D
Proof. By homogeneity, and LemmaE.6, Eθ〜。古[L0[pt](θ)] = Eθ〜。/刀0肪]@|网|2] ≤ BLWt2. We
also get Ea〜Ud [L0[pt](4)]≤ BL since Ud is only supported on Sd. Combining these with Lemma
E.7 gives the desired statement.	□
Now we show that if we run the dynamics for a short time, the second moment of ρt will grow slowly,
again at a rate that is roughly the scale of the noise σ .
Lemma E.9. For all 0 ≤ t0 ≤ t, W2 ≤ 工/0]+臂。.
,	t0	bV -tσBL
Proof. Let t*，arg maxt0∈[0,t] Wt. Integrating both sides of equation E.1, and rearranging, we get
0 ≤ Zt Eθ〜ρs[∣∣v[ps](θ)∣∣2]ds ≤ L[po] - L[pt] + Qbl Zt (WS2 + 1)ds
00
≤ L[Po] - L[pt*]+ ~t*σBL(Wt* + 1)
Now since R is nonnegative, we apply L[pt*] ≥ Eθ〜。=* [V(θ)] ≥ Eθ〜。”[V(8)∣∣θ∣∣2] ≥ bvWt2*. We
now plug this in and rearrange to get Wt ≤ Wt2* ≤ LbPO-：t；BBL ≤ LbP-；冷 ∀0 ≤ t0 ≤ t.	□
Now let Wt，LbP-+1BBL . By Lemma E.9, ∀0 ≤ t ≤ te, Wt ≤ Wt.
The next statement allows us to argue that our dynamics will never increase the objective by too
much.
Lemma E.10. For any t1, tt with 0 ≤ t1 ≤ tt ≤ t, L[ρt2] - L[ρt1] ≤ Q(tt - t1)BL(Wt + 1).
Proof. From Corollary E.8, ∀t ∈ [t1, tt] we have
ddtL[Pt] ≤ Qbl(W^2 + 1)
Integrating from tι to tt gives the desired result.	□
The following lemma bounds the change in expectation of a 2-homogeneous function over ρt. At a
high level, we lower bound the decrease in our loss as a function of the change in this expectation.
30
Under review as a conference paper at ICLR 2019
Lemma E.11. Let h : Rd+1 → R that is 2-homogeneous, with ∣∣Vh(^)k ≤ M ∀θ ∈ Sd and
∣h(θ)∣ ≤ B ∀4 ∈ Sd. Then ∀0 ≤ t ≤ t& we have
以 Z hdρ ≤ σB(Wf2 + 1) + MW (-ddtL[ρt] + σBz(Wf2 + 1))	(E.2)
Proof. Let Q(t) , hdρt. We can compute:
Q0(t) = Z h(θ) dρt (θ)dθ
=/ h(θ)(-σρt(θ) -V∙ (v[ρt](θ)ρt(θ)))dθ + σ / hdUd
=-σ / h⑻∣θ∣2ρt(θ)dθ + σ / hdUd - / h(θ)V ∙ (v[ρt](θ)ρt(θ))dθ
(E.3)
Note that the first two terms are bounded by σB(W2 + 1) by the assumptions for the lemma. For the
third term, we have from Lemma E.5:
I / h(θ)V∙ (v[ρt](θ)ρt(θ))dθ∣ = ∣Eθ 〜PtKVh(θ),v[ρt](θ)i]∣
≤ ,Eθ〜Pt[∣Vh(θ)k2]Eθ〜ρt[∣v[ρt](θ)k2]	(by Cauchy-SchWarz)
≤，&〜Pt[kVh⑻∣2 kθk2]Eθ〜ρJkv[Pt]⑻k2] (by homogeneity of Vh)
≤ MWe/Eθ〜Pt[∣v[ρt](θ)k2]
(since h is Lipschitz on the sphere)
≤ MWe
--dL[ρt] + σBL(Wf2 + 1)
dt	e
(by Corollary E.8)
Plugging this into equation E.3, We get that
|Q0 (t)| ≤ σB (We2 + 1) + MWe
d	1/2
--dtL[Pt] + σBL(W2 + 1))
□
We apply this result to bound the change in L0 [ρt] over time in terms of the change of the objective
value. For clarity, We Write the bound in terms of c1 that is some polynomial in the problem constants.
Lemma E.12. Define Q(t) ，R Φdpt. For every θ ∈ Sd and 0 ≤ t ≤ t + l ≤ te, ∃cι ，
poly(k, CR, BΦ , MΦ , BL ) such that
∣L0[ρt]W)- L0[ρt+ι](%≤ CrBφ
Zt+l∣Q0(t)∣1
≤ σlcι(Wf2 + 1) + cιWe√l(L[ρt] - L[ρt+ι] + σlq(Wf2 + 1))1/2
(E.4)
(E.5)
Proof. Recall that L0[ρt](^) = WR(R Φdρt), Φ(8)) + V@. Differentiating with respect to t,
dtL0[ρt]⑻Y d vr (∕φdρt), φ⑻)
=Φ⑻ >V2R(Q(t))Q0(t)
≤ CRBΦ ∣Q0(t)∣2
≤ CRBΦ ∣Q0(t)∣1
(E.6)
31
Under review as a conference paper at ICLR 2019
Integrating and applying the same reasoning to -L0 [ρt] gives us equation E.4. Now we apply Lemma
E.11 to get
kQ0(t)kι = Xldt Z ΦidPt
i=1
k
≤ X	σBΦ (We2
i=1
d	1/2
+ 1) + MφWe (-dtL[ρt] + σBL(Wf2 + 1))
d	1/2
≤ kσBφ(Wf + I) + kMφWe —- dtL[Pt] + σBL(Wf + I))
We plug this into equation E.6 and then integrate both sides to obtain
CRBΦ Zt+l kQ0(t)k1
t+l	d	1/2
≤ kσlCR Bφ (W2 + 1) + kCRBφMφWe /	(--L[ρt ] + σBz(Wf2 + 1)\
≤ kσlCRBφ(Wf2 + 1) + kCRBφMφWe√l(L[ρt] - L[ρt+ι] + σlBz(Wf2 + 1))1/2
Using c1，max{kCRBφ, kCRBφMφ, Bl} gives the statement in the lemma.	□
Now We also show that L0 is LiPschitz on the unit ball. For clarity, We let c2，λ∕⅛Mr Mφ + MV.
Lemma E.13. For all θ, θ0 ∈ Sd,
∣L0[ρ]W)- L0[ρ]C)∣≤ c2M-叫2	(E.7)
Proof. Using the definition of L0 and triangle inequality,
∣L0[ρ](θ) - L0[ρ](θ0)∣ ≤ VR U Φdρ) JΦ(θ) - Φ(θ0)k2 + |V⑻-V(铲)|
≤ (VkMRMφ + MV)||4-80∣∣2	(by definition of Mφ, MR, MV)
□
Now the remainder of the Proof will Proceed as follows: we show that if ρt is far from oPtimality,
either the exPected velocity of θ under ρt will be large in which case the loss decreases from
Corollary E.8, or there will exist θ such that L0[ρt](4)《0. We will first show that in the latter
case, the σUd noise term will grow mass exPonentially fast in a descent direction until we make
progress. Define K-T，{θ ∈ Sd : L0[ρt](^) ≤ -T}, the -τ-sublevel set of L0[ρt], and let
m(S)，Eθ〜Ud [1(θ ∈ S)] be the normalized spherical area of the set S.
LemmaE.14. If K-T is nonempty, for 0 ≤ δ ≤ T, log m(K-τ+δ) ≥ -2d log c2.
Proof. Let θ ∈ K-τ. From Lemma E.13, L0[ρ](少)≤ -τ + δ for all W with 忸-0∣2 ≤ 卷∙ Thus,
we have
m(K-T+δ) ≥ kud [1[kθ0 - θk2 ≤ f
Now the statement follows by Lemma 2.3 of (Ball, 1997).
□
Now we show that if a descent direction exists, the added noise will find it and our function value will
decrease. We start with a general lemma about the magnitude of the gradient of a 2-homogeneous
function in the radial direction.
Lemma E.15. Let h : Rd+1 → R be a 2-homogeneous function. Then for any θ ∈ Rd+1,
θW>Vh(θ) = 2kθk2h(θW).
32
Under review as a conference paper at ICLR 2019
Proof. We have h(θ + αθ) = (∣∣θ∣∣2 ÷ α)2h(θ). Differentiating both sides with respect to a and
evaluating the derivative at 0, we get ^>Vh(θ) = 2|忸口2月@, as desired.	□
We state the lemma claiming that our objective will decrease if L0[ρt](^)《 0 for some S ∈ Sd.
Lemma E.16. Choose
l ≥ log(W(2∕σ) + 2d log 竽 ÷ 1
τ-σ
If K-T is nonemPtyfor some t* satisfying t* + l ≤ t& then after l steps, we will have
L[ρt* +ι] ≤ L[ρt*] -(T/4 — σlc1W2 + 1))2 + σlcι(W∣ ÷ 1)	(E.8)
lc1 W
We will first show that a descent direction in L0 [ρt] will remain for the next l time steps. In the
rt*+s
notation ofLemmaE.12, define z(s)，CrBφ J∖*	∣∣Q0(t)kιdt. Note that from LemmaE.12, for
all θ ∈ Sd we have ∣L0[ρt*+s](s) — L0[ρt*](s)∣ ≤ z(s). Thus, the following holds:
Claim E.17. For all s ≤ l, Kt-*τ++sz(s) is nonempty.
Proof. By assumption, ∃θS with θS ∈ Kt-*τ. Then L0[ρt*+s](θS) ≤ L0[ρt*](θS) + z(s) ≤ -τ+ z(s), so
K-++z(s) is nonempty.	□
Let Ts，K-+2+z(s) for 0 ≤ S ≤ l. We now argue that this set Ts does not shrink as t increases.
Claim E.18. For all s0 > s, Ts0 ⊇ Ts.
Proof. From equation E.6 and the definition of z(s), ∣L0[ρt+so](s) - L0[ρt+s](s)∣ ≤ z(s0) - z(s). It
follows that for θS ∈ Ts
L0 [ρt+s0](θS) ≤ L0 [ρt+s](θS) + z(s0) - z(s)
≤ -τ∕2÷ z(s) — z(s) + z(s0)	(by definition of Ts)
≤ -τ∕2÷ z(s0)
which means that θ ∈ Tso.	□
Now we show that the weight of the particles in Ts grows very fast if z(k) is small.
Claim E.19. Suppose that z(l) ≤ τ∕4. Let Ts = {θ ∈ Rd+1 : θ ∈ Ts}. Define N(s)
RT kθ∣2dρt*+s and β，exp(-2d log 2c2). Then N0(s) ≥ (τ 一 σ)N (s) + σβ.
Proof. From the assumption z(l) ≤ 4, it holds that Ts ⊆ K-+4 ∀s ≤ k. Since Ts is defined as a
sublevel set, v[ρt*+s](θS) points inwards on the boundary of Ts for all θS ∈ Ts, and by 1-homogeneity
of the gradient, the same must hold for all U ∈ Ts.
Now consider any particle θ ∈ Ts. We have that θ flows to θ + v[ρt* +s](θ)ds at time t + S +
ds. Furthermore, since the gradient points inwards from the boundary, it also follows that u +
v[ρt*+s](θ)ds ∈ Ts. Now we compute
∣θ∣2 dρt* +s+ds = (1 - σdS)	∣θ + v[ρt*+s](θ)dS∣22dρt*+s + σdS	1dUd
JTs	Jts	Jts
≥ (1 - σds) / (∣θ∣2 + 2θ>v[ρt*+s](θ)ds)dρt*+s + σm(K-+s2+z(s))ds
JTS
(E.9)
33
Under review as a conference paper at ICLR 2019
Now We apply Lemma E.15, using the 2-homogeneity of F0 and the fact that L0[ρt*+s](4)≤
. ~
-τ∕4 ∀θ ∈ Ts
kθk2 + 2θ>v[ρt*+s](θ)ds = kθk2 - 4kθk2L0[ρt*+s](θ)ds
≥ kθk22(1 + τ ds)	(E.10)
Furthermore, since Kt-*τ++sz(s) is nonempty by Claim E.17, we can apply Lemma E.14 and obtain
m(K-+/s2+z(s)) ≥ β
Plugging equation E.10 and equation E.11 back into equation E.9, we get
kuk22dρt*+s+ds ≥ (1 - σds)(1 + 2τ ds)N (s) + σβds
JTS
Since we also have that Ts+ds ⊇ Ts , it follows that
N(s + ds) =	kuk22dρt*+s+ds ≥ (1 - σds)(1 + τ ds)N (s) + σβds
J TS+ ds
and so N0(s) ≥ (τ - σ)N (s) + σβ.
(E.11)
□
Now we are ready to prove Lemma E.16.
ProofofLemma E.16. If z(l) = CrBφ Rt+l kQ0(t)kι ≥ 4, then by rearranging the conclusion of
Lemma E.12 we immediately get equation E.8.
Suppose for the sake of contradiction that z(l) ≤ τ∕4. From Claim E.19, it follows that N(1) ≥ σβ,
and N (l) ≥ exp((τ - σ)(l- 1))N ⑴.Thus, in log"，2/：-^ log 等 +1 time, Wt*+ι ≥ N (l) ≥ Wf2,
a contradiction. Therefore, it must be true that z(l) ≥ τ∕4.
□
The following lemma will be useful in showing that the objective will decrease fast when ρt is very
suboptimal.
Lemma E.20. For any time t with 0 ≤ t ≤ t, we have
-dL[Pt] ≤ σBL(Wf2 + 1) - Eθ~pt[L[Pt](θ)]2	(E.12)
dt	W2
Proof. We can first compute
Eθ 〜Pt[L0[Pt](θ)] =Eθ 〜Pt [L0[ρt]⑼kθk2]
=1 Eθ 〜ρt[∣∣θ∣∣2 尹 v[ρt](θ)]	(ViaLemmaE.15)
≤ 2 JEΘ〜ρt[∣∣θ∣∣2]Eθ〜Pt[kv[ρt]⑹Il2]	(by CaUchy-SchWarz)
≤ 2 WeqE2 Pt [kv[ρt](θ)k2]
Rearranging gives Eθ〜ρt [∣∣v[ρt](θ)∣∣2] ≥ Eθ~ρt[W2ρt](θ)] , and plugging this into equation E.1 gives
the desired result.	□
Proof of Theorem E.4. Let L? denote the infimum infP L[ρ], and let ρ? be an -approximate global
minimizer of L: L[ρ?] ≤ L? + . (We define ρ? because a true minimizer of L might not exist.) Let
W? , Eθ2P? [IθI22]. We first note that since bV W?2 ≤ L[ρ?] ≤ L[ρ0], W?2 ≤ L[ρ0]∕bV ≤ We2.
NoW We bound the suboptimality of ρt : since L is convex in ρ,
L[ρ?] ≥ L[Pt] + Eθ〜ρ*[L0[ρt](θ)] - Eθ〜ρt[L0[ρt](θ)]
34
Under review as a conference paper at ICLR 2019
Rearranging gives
L[ρt] - L[ρ?] ≤ Eθ〜ρt[L0[ρt](θ)] - Eθ〜ρ? [L0[ρt](θ)]
≤ Eθ〜ρt[L0[ρt](θ)] - W?2 min ( min L0[ρt]⑹,θ∖	(E.13)
U∈sd-1	J
Now let l，—器26 (2log 萼 + 2d log 4Wc.), which satisfies Lemma E.16 with the value of T
later specified. Suppose that there is a t with 0 ≤ t ≤ t - 2l and ∀t0 ∈ [t, t + 2l], L[ρt0] - L? ≥ 2.
Then L[ρt0] - L[ρ?] ≥ . We will argue that the objective decreases when we are suboptimal:
L[ρt] - L[ρt+2l] ≥	(E.14)
min { ('"W2 -C"?(Wi2 + 1))2 — 3σlcι(W2 + 1), 14W - 2σlBz(Wf2 + 1)}	(E.15)
Using equation E.13 and Wi ≥ W?, we first note that
C ≤ Eθ〜ρt0 [L0[ρto](θ)] - We2 min Lmin L'[ρt>]@, 0 1 ∀t0 ∈ [t,t + 1]
U∈sd-1	J
Thus, either min^∈sd L∖ptj]@ ≤ -2W⅛ ≤ -而,or Eθ〜ρt, [L%](θ)] ≥ W. If ∃t0 ∈ [t,t + 1]
such that the former holds, then the T，^Wς sub-level set K-T is non-empty. Applying Lemma
E.16 gives
('/8Wi2 - 1σc1 (Wi2 + 1))2	2
L[ρt0] - L[Pt0 + l] ≥ --------C2W2∣--------------σ1cι (We + I)
Furthermore, from Lemma E.10, L[ρt+2l] - L[ρt0+l] ≤ σ1c1(Wi2 + 1) and L[ρt0] - L[ρt] ≤
σ1BL(We2 + 1), and so combining gives
L[ρt] - L[ρt+2k ] ≥ MW2- MT + 1”2 - 3σ1cι(Wf2 + 1)	(E.16)
c1 We 1
In the second case Eθ〜p” [L0[ρto](θ)] ≥ N, ∀t0 ∈ [t,t +1]. Therefore, we can integrate equation E.12
from t to t + 1 in order to get
'2
L[ρt] - L[Pt+l] ≥ 1 元运-σ1BL(W2 + I)
4We2
Therefore, applying Lemma E.10 again gives
'2
L[ρt] - L[ρt+2l] ≥ 14W2 - 2σ1BL(We + I)	(E.17)
Thus equation E.15 follows.
Now recall that we choose
σ , exp(-d log(1/')poly(k, MV, MR, MΦ, bV, BV, CR, BΦ, L[ρ0] - L[ρ?]))
For the simplicity, in the remaining computation, We will use O(∙) notation to hide polynomials in
the problem parameters besides d, '. We simply write σ = exp(-c3dlog(1/')). Recall our choice
te，O( d2 log2 (1/6)). It suffices to show that our objective would have sufficiently decreased in
te steps. We first note that with c3 sufficiently large, W2 = O(L[ρo]∕bv) = O(1). Simplifying our
expression for 1, we get that 1 = O(d log ɪ), so long as σWl = o(c), which holds for sufficiently
large c3 . Now let
,('/8W2 - 1σcι(W2 + 1))2	∕TJ∕2 I 1 ʌ
δ1 , ------C∣W2∕----------3σ1c1(We +1)
'2
δ2 , 14W2 - 2双瓦(叽2 + 1)
23
Again, for sufficiently large c3, the terms with σ become negligible, and δ1 = O(ɪ) = O( dio£i/e))∙
Likewise, δ2 = O(d' log(1/')).
Thus, if by time t we have not encountered 2'-optimal ρt, then we will decrease the objective by
O( dio；；i/e)) in O(d log I) time. Therefore, a total of O(d2 log2 (1/')) time is sufficient to obtain '
accuracy.	□
35
Under review as a conference paper at ICLR 2019
E.3 Discrete-Time Optimization
To circumvent the technical issue of existence of a solution to the continuous-time dynamics, we also
note that polynomial time convergence holds for discrete-time updates.
Theorem E.21. Along with Assumptions E.1, E.2, E.3 additionally assume that VΦi and VV are
CΦ and CV -Lipschitz, respectively. Let ρt evolve according to the following discrete-time update:
Pt+1，Pt + η(-σpt + σUd — V ∙ (v[ρt]ρt))
There exists a choice of
σ , exp(—d log(1/)poly(k, MV , MR, bV , BV , CR, BΦ, CΦ, CV , L[ρ0] — L[ρ?]))
η , poly(k, MV , MR, bV , BV , CR, BΦ, CΦ, CV , L[ρ0] — L[ρ?])
d2
t，ɪ4poly(k, MV, Mr, bv, BV, Cr, Bφ, Cφ, Cv, L[ρo] — L[ρ?])
such that min0≤t≤t L[ρt] — L? ≤ .
The proof follows from a standard conversion of the continuous-time proof of Theorem E.4 to discrete
time, and we omit it here for simplicity.
F	Additional Material on Experiments
F.1 Detailed Setup for Figure 2 Experiment
Our ground truth comes from a random neural network with 6 hidden units, and during training we
use a network with as many hidden units as examples. For classification, we used rejection sampling
to obtain datapoints with unnormalized margin of at least 0.1 on the ground truth network. We use a
fixed dimension of d = 20. For all experiments, we train the network for 20000 steps with λ = 10-8
and average over 100 trials for each plot point.
F.2 Detailed Setup for Figure 3 Experiment
The left side of Figure 3 shows the experimental results for synthetic data generated from a ground
truth network with 10 hidden units, input dimension d = 20, and a ground truth unnormalized margin
of at least 0.01. We train for 80000 steps with learning rate 0.1 and λ = 10-5, using two-layer
networks with 2i hidden units for i ranging from 4 to 10. We perform 20 trials per hidden layer size
and plot the average over trials where the training error hit 0. (At a hidden layer size of 27 or greater,
all trials fit the training data perfectly.) The right side of Figure 3 demonstrates the same experiment,
but performed on MNIST with hidden layer sizes of 2i for i ranging from 6 to 15. We train for 600
epochs using a learning rate of 0.01 and λ = 10-6 and use a single trial per plot point. For MNIST,
all trials fit the training data perfectly. The MNIST experiments are more noisy because we run one
trial per plot point for MNIST, but the same trend of decreasing test error and increasing margin still
holds.
F.3 Verifying Convergence to the Max-Margin
We verify the normalized margin convergence on a two-layer networks with one-dimensional input.
A single hidden unit computes the following: X → ajrelu(wjX + bj). We add ∣∣ ∙ k2-regularization to
a, w, and b and compare the resulting normalized margin to that of an approximate solution of the `1
SVM problem with features relu(wXi + b) for w2 + b2 = 1. Writing this feature vector is intractable,
so we solve an approximate version by choosing 1000 evenly spaced values of (w, b). Our theory
predicts that with decreasing regularization, the margin of the neural network converges to the `1
SVM objective. In Figure 4, we plot this margin convergence and visualize the final networks and
ground truth labels. The network margin approaches the ideal one as λ → 0, and the visualization
shows that the network and `1 SVM functions are extremely similar.
36
Under review as a conference paper at ICLR 2019
U-≡,ew pəz--elɪu ON
Figure 4: Neural network with input dimension 1. Left: Normalized margin as we decrease λ. Right:
Visualization of the normalized functions computed by the neural network and `1 SVM solution for
λ ≈ 10-14.
Method	CIFAR10	CIFAR100
Weight decay annealing	5.86	-26.22-
Fixed weight decay	6.01	27.00
Table 1: Test error on CIFAR10 and CIFAR100 for initial λ = 0.0005.
F.4 Experiments on CIFAR 1 0 and CIFAR 1 00
We train a modified WideResNet architecture (Zagoruyko & Komodakis, 2016) on CIFAR10 and
CIFAR100. Our theory does not entirely apply because the identity mapping prevents ResNet
architectures from being homogeneous, but our experiments show that reducing weight decay can still
help generalization error in this setting. Because batchnorm can cause the regularizer to have different
effects (van Laarhoven, 2017), we remove batchnorm layers and train a 16 layer deep WideResNet.
We again compare a network trained with weight decayed annealing to one trained without annealing.
We used a fixed learning rate schedule that starts at 0.1 and decreases by a factor of 0.2 at epochs 60,
120, and 160. For CIFAR10, we use an initial weight decay of 0.0002 and decrease the weight decay
by 0.2 at epoch 60, and then by 0.5 at epochs 90, 120, 140, 160. For CIFAR100, we initialize weight
decay at 0.0005 and decrease it by 0.2 at epochs 60, 120, and 160. We tried different parameters for
the initial weight decay and chose the ones that worked best for the model without annealing. We
also tried using small weight decays at initialization, but these models failed to generalize well - We
believe this is due to an optimization issue where the algorithm fails to find a true global minimum of
the regularized loss. We believe that annealing the weight decay directs the optimization algorithm
closer towards the global minima for small λ.
Table 1 shows the test error achieved by models with and without annealing. We see that the simple
change of annealing weight decay can decrease the test error for this architecture.
37