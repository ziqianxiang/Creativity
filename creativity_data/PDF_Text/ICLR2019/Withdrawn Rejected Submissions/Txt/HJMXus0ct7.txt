Under review as a conference paper at ICLR 2019
iRDA Method
for Sparse Convolutional Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new approach, known as the iterative regularized dual averaging
(iRDA), to improve the efficiency of convolutional neural networks (CNN) by
significantly reducing the redundancy of the model without reducing its accuracy.
The method has been tested for various data sets, and proven to be significantly
more efficient than most existing compressing techniques in the deep learning
literature. For many popular data sets such as MNIST and CIFAR-10, more than
95% of the weights can be zeroed out without losing accuracy. In particular, we are
able to make ResNet18 with 95% sparsity to have an accuracy that is comparable
to that of a much larger model ResNet50 with the best 60% sparsity as reported in
the literature.
1	Introduction
In recent decades, deep neural network models have achieved unprecedented success and state-of-
the-art performance in various tasks of machine learning or artificial intelligence, such as computer
vision, natural language processing and reinforcement learning Lecun et al. (2015). Deep learning
models usually involve a huge number of parameters to fit variant kinds of datasets, and the number
of data may be much less than the amount of parameters He et al. (2016). This may implicate that
deep learning models have too much redundancy. This can be validated by the literatures from the
general pruning methods Pratt (1988) to the compressing models Han et al. (2015a).
While compressed sensing techniques have been successfully applied in many other problems, few
reports could be found in the literature for their application in deep learning. The idea of sparsifying
machine learning models has attracted much attention in the last ten years in machine learning
Donoho (2006); Xiao (2010). When considering the memory and computing cost for some certain
applications such as Apps in mobile, the sparsity of parameters plays a very important role in model
compression Han et al. (2015a); Cheng et al. (2017). The topic of computing sparse neural networks
can be included in the bigger topic on the compression of neural networks, which usually further
involves the speedup of computing the compressed models.
There are many sparse methods in machine learning models such as FOBOS method Duchi and
Singer (2009), also known as proximal stochastic gradient descent (prox-SGD) methods Mine and
Fukushima (1981), proposed for general regularized convex optimization problem, where `1 is a
common regularization term. One drawback of prox-SGD is that the thresholding parameters will
decay in the training process, which results in unsatisfactory sparsity Xiao (2010). Apart from that,
the regularized dual averaging (RDA) method Xiao (2010), proposed to obtain better sparsity, has
been proven to be convergent with specific parameters in convex optimization problem, but has not
been applied in deep learning fields.
In this paper, we analyze the relation between simple dual averaging (SDA) method Nesterov (2009)
and the stochastic gradient descent (SGD) method Robbins and Monro (1951), as well as the relation
between SDA and RDA. It is well-known that SGD and its variants work quite well in deep learning
problems. However, there are few literatures in applying pure training algorithms to deep CNNs
for model sparsification. We propose an iterative RDA (iRDA) method for training sparse CNN
models, and prove the convergence under convex conditions. Numerically, we compare prox-SGD
with iRDA, where the latter can achieve better sparsity results while keeping satisfactory accuracy
on MNIST, CIFAR-10 and CIFAR-100. We also show iRDA works for different CNN models such
1
Under review as a conference paper at ICLR 2019
as VGG Simonyan and Zisserman (2014) and He et al. (2016). Finally, we compare the performance
of iRDA with some other state-of-the-art compression methods.
2	Related Works
Cheng et al. (2017) reviews the work on compressing neural network models, and categorizes the
related methods into four schemes: parameter pruning and sharing, low-rank factorization, trans-
fered/compact convolutional filters and knowledge distillation. Among them, Liu et al. (2015) uses
sparse decomposition on the convolutional filters to get sparse neural networks, which could be
classified to the second scheme. Apart from that, Han et al. (2015b) prunes redundant connections
by learning only the important parts. Louizos et al. (2017) starts from a Bayesian point of view,
and removes large parts of the network through sparsity inducing priors. Yin et al. (2018) He et al.
(2018) combines reinforcement learning methods to compression. Li and Hao (2018) considers deep
learning as a discrete-time optimal control problem, and obtains sparse weights on ternary networks.
Recently, Feng (2018) applies RDA to fully-connected neural network models on MNIST.
3	Algorithms
Let z = (x, y) be an input-output pair of data, such as a picture and its corresponding label in a
classification problem, and f (w, z) be the loss function of neural networks, i.e. a scalar function
that is differentiable w.r.t. weights w. We are interested in the expected risk minimization problem
min {Ezf(w, z)}.
w
The empirical risk minimization
mwn {T Xf(w,zt)∣
(1)
(2)
is an approximation of (1) based on some finite given samples {z1, z2, . . . , zT} , where Tis the size
of the sample set.
Regularization is a useful technique in deep learning. In general, the regularized expected risk
minimization has the form
min {φ(w) = Ezf(w, z) + Ψ(w)},	(3)
w
where Ψ(w) is a regularization term with certain effect. For example, Ψ(w) = kwk22 may im-
prove the generalization ability, and an `1 -norm of w can give sparse solutions. The corresponding
regularized empirical risk minimization we concern takes the form
mwn 卜(W) = T X f(w,zt) + Ψ(w)j> .
(4)
SDA method is a special case of primal-dual subgradient method first proposed in Nesterov (2009).
Xiao (2010) proposes RDA for online convex and stochastic optimization. RDA not only keeps the
same convergence rate as Prox-SGD, but also achieves more sparsity in practice.
In next sections, we will discuss the connections between SDA and SGD, as well as RDA and
Prox-SGD. We then propose iRDA for `1 regularized problem of deep neural networks.
3.1	Simple Dual Averaging Method
As a solution of (2), SDA takes the form
wt+1
arg min 1 1 X hgτ(wτ),w〉+ βt h(w) ?.
w	t τ=1	t
(5)
The first term Ptτ=1 hgτ, wi is a linear function obtained by averaging all previous stochastic gra-
dient. gt is the subgradient of ft. The second term h(w) is a strongly convex function, and
2
Under review as a conference paper at ICLR 2019
{βt } is a nonnegative and nondecreasing sequence which determines the convergence rate. As
gτ (wτ), τ = 1, . . . , t - 1 is constant in current iteration, we use gτ instead for simplicity in the
following. Since subproblem equation 5 is strongly convex, it has a unique optimal solution wt+1.
Let wo be the initial point, and h(w) = 2 ∣∣w - w0k2, the iteration scheme of SDA can be written as
wt+1
1t	t
w0- Jt NgT = w0- βgt,
(6)
where Ut = 1 PT =1 hgτ, Wi. Let Jt = γtα, SDA can be rewritten recursively as
t
wt+1
where(1 -(1 -
perturbation of SGD.
3.2	Proximal Stochastic Gradient Descent and Regularized Dual Averaging
Methods
For the regularized problem (4), we recall the well-known Prox-SGD and RDA method first. At
each iteration, Prox-SGD solves the subproblem
Wt+1 = arg min h h∂t, wi + ɪ ∣∣w - Wtk2 +Ψ(w)J .
w	2αt
(8)
Specifically, at = γ√t obtains the best convergence rate. The first two terms are an approxima-
tion of the original objective function. Note that without the regularization term Ψ, equation 8 is
equivalent to SGD. It can be written in forward-backward splitting (FOBOS) scheme
Wt+1 = Wt - atgt,
(9)
wt+1
arg min 1 1 l∣w - Wt+ 1k2 + at Ψ(w)
w2	2
(10)
where the forward step is equivalent to SGD, and the backward step is a soft-thresholding operator
working on Wt+1 with the soft-thresholding parameter at.
Different from Prox-SGD, each iteration of RDA takes the form
wt+1 = arg min 1 1 X hgτ ,ww + Ψ(w) + J h(w) ∖ ∙
w t T=1	t
Similarly, taking h(w) = 1 ∣∣w - wo∣2, RDA can be written as
wt+1 = arg min h ®,w) + J ∣∣w — wo k2 + Ψ(w)}
w	2t
=argmin 11 kw - (WO + "gt)k2 + Jψ(w)},
(11)
(12)
(13)
3
Under review as a conference paper at ICLR 2019
or equivalently,
t
wt+1 = wo - βgt,
wt+1 = argmin∣2kw - wt+1 k2 + βψ(W)},
(14)
(15)
where βt = γ√t to obtain the best convergence rate. From equation 14, one can see that the forward
step is actually SDA and the backward step is the soft-thresholding operator, with the parameter t∕βt.
3.3	`1 REGULARIZATION AND THE SPARSITY
Set Ψ(w) = λkwk1. The problem (4) then becomes
T
min X ft(w) + λkwk1,	(16)
w
t=1
where λ is a hyper-parameter that determines sparsity.
In this case, from Xiao,s analysis of RDA Xiao (2010), the expected cost Eφ(wt) - φ? associated
with the random variable wt converges with rate O(√^) when βt = γ√7. This convergence rate is
consistent with FOBOS Duchi and Singer (2009). However, both results assume f to be a convex
function, which can not be guaranteed in deep learning. Nevertheless, we can still verify that RDA
is a powerful sparse optimization method for deep neural networks.
We conclude the closed form solutions of Prox-SGD and RDA for equation 16 as follows.
	1.	The subproblem of Prox-SGD wt+1 = arg min {gT w + j kw - wtk2 + λ∣∣wkι}	(17) has the closed form solution (w(i - αt(g(i + λ), w(i) - αtg(i > αλ wt+1 = 0,	|wt(i) -αtgt(i) | ≤ αtλ,	(18) lw(i) - αt(g((Ii - λ), w(i) - αtgt(i < -αtλ 2.	The subproblem of RDA wt+1 = argmin 卜Tw + βtkw - wok2 + λ∣∣wkι}	(19) has the closed form solution (w0ii - βtt(UT) + λ), woii - βtt9ti') > βttλ, wt+1 =]0,	lw0ii - βg(i)1 ≤ βttλ,	(20) lw0ii - βtt(u0ii - λ), woii - βtt9ti'i < -βtt入. 3.	The √t-proximal stochastic gradient method has the form wt+1 = wt - αtgt, 1	2 t	(21) wt+1 = argmin 2 2kw - wt+1 k2 + βψ(w) j .
The difference between √t-Prox-SGD and Prox-SGD is the soft-thresholding parameter chosen to
be √t. It has the closed form solution
	(w(ii - atg(i) - βttλ, w(ii - αtg(ii > βttλ, w(+1 = < 0,	lw(ii - αt9ii∖ ≤ βttλ,	(22) lw(ii - atg(i) + βttλ, w(ii - atg(i) < -βttλ.
4
Under review as a conference paper at ICLR 2019
It is equivalent to
wt+1
arg min
w
wtk2+Oet kwk1
(23)
where the objective function is actually an approximation of
T
X
i=1
fi(w) + αt⅛ kwk1.
(24)
We can easily conclude that this iteration will converge to W = 0 if a = γ√t and βt = γ√7.
Now compare the threshold λpG = αtλ of PG and the threshold Xrda =第λ of RDA. With
at = γ√t and βt = γ√t, We have Xpg → 0 and Xrda → ∞ as t → 0. It is clear that RDA uses a
much more aggressive threshold, which guarantees to generate significantly more sparse solutions.
3.4	Iterative RDA Method for Deep Neural Networks
Note that when Ψ = Xkwk1 , RDA requires w1 = w0 = 0. However, this will make deep neural
network a constant function, with which the parameters can be very hard to update. Thus, in Algo-
rithm 1, we modify the RDA method as Step 1, where w1 can be chosen not equal to 0, and add an
extra Step 2 to improve the performance. We also prove the convergence rate of Step 1 for convex
problem is O(√) when βt = O(√t).
Theorem 3.1 Assume there exists an optimal solution w? to the problem (3) with Ψ(w) = Xkwk1
that satisfies h(w?) ≤ D2 for some D > 0, and let φ? = φ(w?). Let the sequences {wt}t≥1 be
generated by Step 1 in iRDA, and assume ∣∣gtk* ≤ G for some constant G. Then the expected cost
Eφ(Wt) converges to φ? with rate O( √1t)
Eφ(Wt)- φ? = O( √t),
with Wt = 1 PT=1 Wτ . See Appendix A for the proof.
3.5	Initialization
To apply iRDA, the weights of a neural network should be initialized differently from that in a
normal optimization method such as SGD or its variants. Our initialization is based on LeCun et al.
(2012), Glorot and Bengio (2010) and He et al. (2015), with an additional re-scaling. Let s be a
scalar, the mean and the standard deviation of the uniform distribution for iRDA is zero and
rs2
σiRDA = ∖ 一:
n
n = k2c
(25)
respectively, where c is the number of channels, and k is the spatial filter size of the layer (see He
et al. (2015)).
Choosing a suitable s is important when applying iRDA. As shown in Table 5 and Table 6 in Ap-
pendix B, ifs is too small or too large, the training process could be slowed down and the gener-
alization ability may be affected. Moreover, a small s usually requires much better initial weights,
which results in too many samplings in initialization process. In our experiments, a good S for iRDA
is usually much larger than √2, and unsuitable for SGD algorithms.
3.6	Iterative Retraining
Iterative retraining is a method that only updates the non-zero parameters at each iteration. A trained
model can be further updated with retraining, thus both the accuracies and sparsity can be improved.
See Table 4 for comparisons on CIFAR-10.
5
Under review as a conference paper at ICLR 2019
Algorithm 1 The iterative RDA method for `1 regularized DNNs
Input:
•	A strongly convex function h(w) = kwk22.
•	A nonnegative and nondeScreasing sequence βt = γ√7.
Step 1: RDA with proper initialization
Initialize: set w0 = 0, go = 0 and randomly choose wι with methods explained in section 3.5.
for t=1,2, ..., T do
Given the sample zit and corresponding loss function fit .
Compute the stochastic gradient
gt = Vfit (Wt).	(26)
Update the average gradient:
t - 1—	1
gt = -1-gt-1 + tgt.	(27)
Compute the next weight vector:
wt+1 = arg min {h@,w〉+ λ∣∣w∣∣ι + βt ∣∣wk2 j .	(28)
Step 2: iterative retraining
for t=T+1,T+2,T+3, ... do
Given the sample zit and corresponding loss function fit .
Compute the stochastic gradient
gt = Vfit (wt).	(29)
Set (gt)j = 0 if (wt)j = 0 for every j.
Update the average gradient:
t-1	1
gt = -1-gt-1 + tgt∙	(30)
Compute the next weight vector:
wt+1 = arg min
w
{hUt,wi + λkwkι + 2t I∣wk2∣.	(31)
4	Experiments
In this section, σ denotes the sparsity of a model, i.e.
quantity of zero parameters
σ =----------------------------
quantity of all parameters
All neural networks are trained with mini-batch size 128.
(32)
4.1	Parameters Test
We provide a test on different hyper-parameters, so as to give an overview of their effects on perfor-
mance, as shown in Table 7. We also show that the sparsity and the accuracy can be balanced with
iRDA by adjusting the parameters λ and γ, as shown in Table 8. Both tables are put in Appendix C.
4.2	Main Results
We compare iRDA with several methods including Prox-SGD, √t-SGD and normal SGD, on dif-
ferent datasets including MNIST, CIFAR-10, CIFAR-100 and ImageNet(ILSVRC2012). The main
results are shown in Table 1. Table 2 shows the performance of iRDA on different architectures
including ResNet18, VGG16 and VGG19. Table 3 shows the performance of iRDA on different
6
Under review as a conference paper at ICLR 2019
Figure 1: The first 120 epochs of loss curves corresponding to Table 1, and the sparsity curve for
another result, where the top-1 validation accuracy is 91.34%, and σ = 0.87.
datasets including MNIST, CIFAR-10, CIFAR-100 and ImageNet(ILSVRC2012). In all tables, SGD
denotes stochastic gradient methods with momentum Ruder (2016).
4.3	Comparison
Currently, many compression methods include human experts involvement. Some methods try to
combine other structures in training process to automatize the compression process. For example,
He et al. (2018) combines reinforcement learning. iRDA, as an algorithm, requires no extra structure.
As shown above, iRDA can achieve good sparsity while keeping accuracy automatically, with care-
fully chosen parameters. For CIFAR-10, we compare the performance of iRDA with some other
state-of-art compression methods in Table 4. Due to different standards, σ is referred to directly or
computed from the original papers approximately.
5	Conclusion
In comparison with many existing rule-based heuristic approaches, the new approach is based on
a careful and iterative combination of `1 regularization and some specialized training algorithms.
We find that the commonly used training algorithms such as SGD methods are not effective. We
thus develop iRDA method that can be used to achieve much better sparsity. iRDA is a variant of
RDA methods that have been used for some special types of online convex optimization problems
in the literature. New elements in the iRDA mainly consist of judicious initialization and iterative
retraining. In addition, iRDA method is carefully analyzed on its convergence for convex objective
functions.
Many deep neural networks trained by iRDA can achieve good sparsity while keeping the same
validation accuracy as those trained by SGD with momentum on many popular datasets. This result
shows iRDA is a powerful sparse optimization method for image classification problems in deep
learning fields.
7
Under review as a conference paper at ICLR 2019
Table 1: The main results of different methods. The architecture is ResNet18, and the dataset is
CIFAR-10. This table shows the top-1 and top-5 accuracies on the validation dataset. iRDA achieves
the highest top-1 accuracy and sparsity. See figure 1 for the corresponding loss curves.
Method	TOP 1 Acc.	TOP 5 Acc.	σ	λ	γ
SGD	92.69	99.72	0.00	N/A	N/A
Prox-SGD	89.80	99.40	0.03	10-5	0.8
√t-prox-SGD	82.47	99.07	0.72	10-8	1.0
iRDA	93.47	99.69	0.95	10-6	1.0
Table 2: iRDA on different Architectures. This table shows the top-1 and top-5 accuracies on the
validation dataset. The results from SGD with momentum are in the brackets. iRDA works well on
different CNN architectures.
ARCHITECTURE	TOP 1 Acc.	TOP 5 Acc.	σ	λ	γ
ResNet18	93.47 (92.69)	99.69 (99.72)	0.95	10-6	1.0
VGG16	93.24 (93.42)	99.52 (99.79)	0.94	10-6	1.0
VGG19	91.87 (91.70)	99.37 (99.40)	0.98	10-5	1.0
Table 3: iRDA on different datasets. The architecture is ResNet18. This table shows the top-1
and top-5 accuracies on the validation dataset. The results from SGD with momentum are in the
brackets. iRDA works well on different datasets.
DATASET	TOP 1 Acc.	TOP 5 Acc.	σ	λ	γ
MNIST	99.63 (99.65)	100.00	0.95	10-6	0.1
CIFAR-10	93.47 (92.69)	99.69 (99.72)	0.95	10-6	1.0
CIFAR-100	72.29 (73.69)	89.94 (92.43)	0.56	10-8	0.09
ILSVRC2012	64.93 (70.58)	84.92 (89.64)	0.36	10-8	0.1
Table 4: iRDA and different state-of-the-art compression methods on CIFAR-10. This table shows
the top-1 accuracies on the validation dataset. Due to different standards, σ is referred to directly or
computed approximately. iRDA achieves almost the same accuracy and sparsity on VGG16, while
the sparsity on ResNet18 is much better.
ARCHITECTURE	METHOD	TOP 1 Acc.	σ	λ	γ
ResNet-50	AMC(RParam) He et al. (2018)	93.64	0.60	N/A	N/A
ResNet-18	iRDA	93.47	0.95	10-6	1.0
VGG-16	VIBNet Dai et al. (2018)	93.80	0.94	N/A	N/A
VGG-16	iRDA	93.24	0.94	10-6	1.0
8
Under review as a conference paper at ICLR 2019
References
Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration
for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.
Bin Dai, Chen Zhu, Baining Guo, and David Wipf. Compressing neural networks using the varia-
tional information bottleneck. In Proceedings of the 35th International Conference on Machine
Learning (ICML 2018), 2018.
D. L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289-1306,
2006.
John Duchi and Yoram Singer. Efficient online and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10(Dec):2899-2934, 2009.
Jing Feng. Sparsification methods in convolutional neural networks. Master’s thesis, Beijing Uni-
versity of Technology, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pages 249-256, 2010.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in neural information processing systems, pages 1135-
1143, 2015b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pages 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model
compression and acceleration on mobile devices. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 784-800, 2018.
Y Lecun, Y Bengio, and G Hinton. Deep learning. Nature, 521(7553):436, 2015.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pages 9-48. Springer, 2012.
Qianxiao Li and Shuji Hao. An optimal control approach to deep learning and applications to
discrete-weight neural networks. arXiv preprint arXiv:1803.01299, 2018.
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolu-
tional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 806-814, 2015.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In
Advances in Neural Information Processing Systems, pages 3288-3298, 2017.
Hisashi Mine and Masao Fukushima. A minimization method for the sum ofa convex function and
a continuously differentiable function. Journal of Optimization Theory and Applications, 33(1):
9-23, 1981.
Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming,
120(1):221-259, 2009.
Lorien Y. Pratt. Comparing biases for minimal network construction with back-propagation. In
International Conference on Neural Information Processing Systems, pages 177-185, 1988.
9
Under review as a conference paper at ICLR 2019
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, 22(3):400-407,1951.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. Computer Science, 2014.
Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11(Oct):2543-2596, 2010.
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Bina-
ryrelax: A relaxation approach for training deep neural networks with quantized weights. arXiv
preprint arXiv:1801.06313, 2018.
10
Under review as a conference paper at ICLR 2019
A Proof of Theorem 3.1
One of the differences between RDA Xiao (2010) and iRDA is that the former one takes w1 =
arg min h(w) whereas the latter one chooses w1 randomly. In the following, we will prove the
w
convergence of iRDA Step 1 for convex problem. The proofs use Lemma 9, Lemma 10, Lemma 11
directly and modify Theorem 1 and Theorem 2 in Xiao (2010). For clarity, we have some general
assumptions:
•	The regularization term Ψ(w) is a closed convex function with convexity parameter σ and
domΨ is closed.
•	For each t ≥ 1, ft (w) is convex and subdifferentiable on domΨ.
•	h(w) is strongly convex on domΨ and subdifferentiable on rint(domΨ) and also satisfies
w0 = arg min h(w) ∈ Arg min Ψ(w).	(33)
Without loss of generality, assume h(w) has convexity parameter 1 and minw h(w) = 0.
•	There exist a constant G such that
kgtk* ≤ G, ∀t ≥ 1.	(34)
•	Require {β}t≥1 be a nonnegative and nondecreasing sequence and
β0 = max{σ, β1} > 0.	(35)
Moreover, we could always choose β1 ≥ σ such that β0 = β1.
•	For a random choosing w1, we assume
Ψ(w1) ≤ Q.	(36)
First of all, we define two functions:
Ut(s) = max {hs, w - w0i - tΨ(w)},	(37)
w∈FD
Vt(s) = max{hs, w - w0i - tΨ(w) - βth(w)}.	(38)
w
The maximum in (37) is always achieved because FD = {w ∈ domΨ∣h(w) ≤ D2} is a nonempty
compact set. Because of (35), we have σt+βt ≥ β0 > 0 for allt ≥ 0, which means tΨ(w) +βth(w)
are all strongly convex, therefore the maximum in (38) is always achieved and unique. As a result,
we have domUt = domVt = E* for all t ≥ 0. Moreover, by the assumption (33), both of the
functions are nonnegative.
Let st denote the sum of the subgradients obtained up to time t in iRDA Step 1, that is
t
st = EgT = tgt,
τ=1
and πt(s) denotes the unique maximizer in the definition of Vt(s)
which then gives
πt(s) = arg max{hs, w - w0i - tΨ(w) - βth(w)}
w
= arg min{h-s, wi + tΨ(w) + βth(w)},
w
wt+1 = πt(-st).
(39)
(40)
(41)
Lemma A.1 For any s ∈ E* and t ≥ 0, we have
Ut(s) < Vt(s) + βtD2.	(42)
For a proof, see Lemma 9 in Xiao (2010).
11
Under review as a conference paper at ICLR 2019
Lemma A.2 The function Vt is convex and differentiable. Its gradient is given by
▽Vt(S)= ∏t(s) - W0	(43)
and the gradient Lipschitz continuous with constant 1∕(σt + βt), that is
IVVt(SI) - ▽%(Sz)Il ≤	:& ks1 - s2∣∣*, ∀s1,s2 ∈ E*.	(44)
σt + βt
Moreover, the following inequality holds:
Vt(S + g) ≤ Vt(s) + hg, VVt(S))+一二、kg∣信 ∀s,g ∈ E*.	(45)
2(σt + βt)
The results are from Lemma 10 in Xiao (2010).
Lemma A.3 For each t ≥ 1, we have
Vt(-St) + Ψ(wt+1) ≤ Vt-1(-St) + (βt-1 - βt)h(wt+1).	(46)
Since h(wt+1) ≥ 0 and the sequence {βt}t≥1 is nondecreasing, we have
Vt(-St) + Ψ(wt+1) ≤ Vt-1(-St),	∀t ≥ 2,	(47)
V1(-S1) + Ψ(w2) ≤ V0(-S1) + (β0 -β1)h(w2),	t= 1.	(48)
To prove this lemma, we refer to the Lemma 11 in Xiao (2010). What’s more, from the assumption
35, we could always choose β1 ≥ σ such that β1 = β0 and
V1(-S1) + Ψ(w2) ≤ V0(-S1), t= 1.	(49)
The learner’s regret of online learning is the difference between his cumulative loss and the cumu-
lative loss of the optimal fixed hypothesis, which is defined by
tt
Rt(w) = X(fτ (wτ) + Ψ(wτ)) - X(fτ (w) + Ψ(w)),	(50)
τ=1	τ=1
and bounded by
∆t = Q + βtD2 +
G2 X	1
H T=0 στ + βτ .
(51)
Lemma A.4 Let the sequence {wt}t≥1 and {gt}t≥1 be generated by iRDA Step 1, and assume (34)
and (35) hold. Thenforany t ≥ 1 and any W ∈ FD = {w ∈ dom Ψ∣h(w) ≤ D2} ,the regret defined
in (50) is bounded by ∆t
Rt(w) ≤ ∆t	(52)
Proof First, we define the following gap sequence which measures the quality of the solutions
w1 , .., wt :
δt = max
w∈FD
[t (hgτ
τ=1
, wτ - wi + Ψ(wτ) - tΨ(w)
t = 1, 2, 3, ......
(53)
)
and δt is an upper bound on the regret Rt (w) for all w ∈ FD, to see this, we use the convexity of
ft (w) in the following:
t
δt ≥ X (fτ (wτ) - fτ (w) + Ψ(wτ)) - tΨ(w) = Rt(w).	(54)
τ=1
Then, We are going to derive an upper bound on δt. For this purpose, we subtract Ptτ =1 hgτ, w0i in
(53), which leads to
t
δt =	(hgτ, wτ - w0i + Ψ(wτ)) + max {hSt, w0 - wi - tΨ(w)} ,	(55)
w∈FD
τ=1
12
Under review as a conference paper at ICLR 2019
the maximization term in (55) is in fact Ut(-st), therefore, by applying Lemma A.1, we have
t
δt ≤ ^X (hgτ, WT - w0i + ψ(WT)) + Vt(-St) + BtD2.
τ=1
(56)
Next, we show that ∆t is an upper bound for the right-hand side of inequality (56). We consider
τ ≥ 2 and τ = 1 respectively.
For any τ ≥ 2, we have
VT(-ST) + ψ(wt +1) ≤ VT-1(-ST-1) + h-gτ, WT - w0i + 7TΓ~j------VTJ； ------C,
2(σ(τ - 1) + βT-1)
where (47),(39),(45) and (43) are used. Therefore, we have
hgT, WT	-	w0i	+ ψ(WT +1) ≤	VT-1(-ST-1) -	VT (-ST ) + ^Γ~(~~":："；R------7,	∀τ	≥	2.
2(σ(τ - 1) + βT-1)
For τ = 1, we have a similar inequality by using (49)
h∂1,W1 - Wθi + Ψ(W2) ≤ VO(-S0) - V1(-S1) + kg1k* .
2β0
Summing the above inequalities for τ = 1, ..., t and noting that V0(-S0) = V0 = 0, we arrive at
t
XWT
T=1
WT
t
-W0i + Ψ(WT+1)) + Vt(-St) ≤ X
T=1
kgTk2
2(σ(τ - 1) + 8丁-1)
Since Ψ(Wt+1) ≥ 0, we subtract it from the left hand side and add Ψ(W1) to both sides of the above
inequality yields
X (hgT,wt - W0i + ψ(wt)) + Vt(-St) ≤ W(W1) + 2 X 2( { IgTJ： R V
T=1	2 T=1 2(σ(τ - 1) + βT-1)
(57)
Combing (54), (56), (57) and using assumption (34) and (36)we conclude
Rt(W) ≤ δt ≤ ∆t =Q+βtD2+
G2 X	1
F T=0 στ + Bt
Lemma A.5 Assume there exists an optimal solution W? to the problem (3) that satisfies h(W?) ≤
D2 for some D > 0, and let φ? = φ(W?). Let the sequences {Wt}t≥1 be generated by iRDA Step 1,
and assume kgtk： ≤ G for some constant G. Then for any t ≥ 1, the expected cost associated with
the random variable Wt is bounded as
Eφ(Wt) - φ? ≤ t∆t.
Proof First, from the definition (50), we have the regret at W?
tt
Rt(W?) = X(f(WT,zT) + Ψ(WT)) - X(f (W?, zT) + Ψ(W?)),
T =1	T =1
Let z[t] denote the collection of i.i.d. random variables (z,..., zt). We note that the random variable
WT, where 1 ≤ W ≥ t, is a function of (z1, ..., zT-1) and is independent of (zT, ..., zt). Therefore
Ez[t] (f(WT, zT) + Ψ(WT)) = Ez[T-1] (ETf(WT, zT) + Ψ(WT)) = Ez[T -1]φ(WT) = Ez[t]φ(WT),
and
Ez[t] (f(W?, zT) + Ψ(W?)) = ETf(W?, zT) + Ψ(W?) = φ(W?) = φ?.
Since φ? = φ(W?) = min φ(W), we have the expected regret
w
t
Ez[t]Rt(W?) =XEz[t]φ(WT) -tφ? ≥ 0.	(58)
T=1
13
Under review as a conference paper at ICLR 2019
Then, by convexity of φ, we have
1t	1t
φ(Wt) = θ(tɪ^wTJ ≤ t∑^φ (WT ) .
(59)
Finally, from (59) and (58), we have
Ez[t]Φ(Wt) - φ? ≤ 1
Ez[t] φ(WT) - tφ?
t Ez[t]Rt(w?).
Then the desired follows from that of Lemma A.4.
Proof of Theorem 3.1 From Lemma A.5, the expected cost associated with the random variable Wt
is bounded as
Eφ(Wt) - φ? ≤ 1
2 t-1
Q + βtD2 + τ X L
(60)
Here, we consider `1 regularization function Ψ(W) = λkWk1 and it is a convex but not strongly
convex function, which means σ = 0. Now, we consider how to choose βt fort ≥ 1 and β0 = β1.
First if βt = Yt We have 1 ∙ γtD2 = γD2, which means the expected cost does not converge. Then
assume βt = γtα, α > 0 and α 6= 1, the right hand side of the inequality (60) becomes
t (Q + γD2tα + CG X Ta) ≤ ∣[Q + γD2tα + Gl(2 + X Ta)
≤ 1 [Q + γD2ta + G2(2+ Zt 1 ɪ)1 〜O(ta-1 + t-a).
t	2γ 1	Ta
From above, we see that if 0 < α < 1, the expected cost converges and the optimal convergence
rate O(t- 1) achieves when α = 2. Then we proved the Theorem 3.1.
B	Initialization
Table 6: Different initialization scalars on
CIFAR-100 with iRDA. The architecture is
ResNet18. λ = 10-8 and γ = 0.1. This
table shows the top-1 accuracies on the vali-
dation dataset. All models are trained for 120
epochs.
Table 5: Different initialization scalars on
CIFAR-10 with iRDA. The architecture is
ResNet18. λ = 10-6 and γ = 1.0. This
table shows the top-1 accuracies on the vali-
dation dataset. All models are trained for 120
epochs.
s	TOP 1 Acc.	TOP 5 Acc.	σ	s	TOP 1 Acc.	TOP 5 Acc.	σ
1,2	10.00	50.00	N/A	1	63.67	87.85	0.91
3	85.52	99.24	0.98	2	66.90	88.53	0.60
4	86.72	99.45	0.97	5	65.47	88.09	0.60
5	90.03	99.44	0.95	10	65.54	88.21	0.42
10	90.67	89.50	0.94	15	64.22	87.53	0.43
100	91.41	99.58	0.84	25	63.06	88.10	0.50
1000	90.36	99.62	0.63	30	62.75	86.80	0.42
10000	71.80	97.94	0.34	50	64.48	87.14	0.38
20000	68.06	97.39	0.99	100	60.00	86.14	0.36
14
Under review as a conference paper at ICLR 2019
C Parameters Test
Table 7: Fix γ = 1.0 and test different λ with different methods on CIFAR-10. The architecture
is ResNet18. All models are trained for 120 epochs. This table shows the top-1 accuracies on the
validation dataset. We have shown why Prox-SGD will give poor sparsity, and although √t-prox-
SGD may introduce greater sparsity, it is not convergent. Finally, iRDA gives the best result, on
both the top-1 accuracy and the sparsity.
λ	10-2	10-3	10-4	10-5	10-6	10-7	10-8
iRDA(without Retraining)	10.00	38.73	74.04	86.39	90.67	90.14	88.38
σ	1.00	1.00	1.00	0.99	0.94	0.61	0.16
√t-prox-SGD	10.00	10.00	10.00	10.00	8.43	49.20	82.47
σ	1.00	1.00	1.00	1.00	1.00	0.99	0.72
prox-SGD	17.90	75.11	88.74	88.20	89.02	86.92	87.75
σ	1.00	0.97	0.52	0.01	0.00	0.00	0.00
Table 8: Different λ and γ with iRDA on CIFAR-10. The architecture is ResNet18. All models are
trained for 120 epochs. This table shows the top-1 accuracies on the validation dataset. The highest
accuracy is 92.07% with sparsity 0.84, and with a lower accuracy 90.67% we get a sparsity of 0.94.
The result shows that we can balance the accuracy and the sparsity with iRDA, by adjusting the
parameters γ and λ.
γ	0.1		0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1.0
λ=	10-6	90.73	91.68	91.88	92.07	91.68	90.33	90.88	89.40	89.72	90.67
σ		0.78	0.81	0.82	0.84	0.89	0.83	0.92	0.94	0.94	0.94
λ=	10-7	90.87	91.39	91.88	90.77	91.80	90.46	90.04	N/A	90.19	90.14
σ		0.41	0.45	0.49	0.55	0.53	0.55	0.60	N/A	0.61	0.61
λ=	10-8	91.42	91.00	91.62	92.25	91.06	89.71	87.11	90.45	86.31	88.38
σ		0.09	0.12	0.14	0.12	0.13	0.14	0.13	0.15	0.16	0.16
15