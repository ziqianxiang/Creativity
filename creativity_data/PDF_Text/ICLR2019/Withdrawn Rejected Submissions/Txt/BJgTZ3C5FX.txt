Under review as a conference paper at ICLR 2019
Generative model based on minimizing exact
empirical Wasserstein distance
Anonymous authors
Paper under double-blind review
Ab stract
Generative Adversarial Networks (GANs) are a very powerful framework for gen-
erative modeling. However, they are often hard to train, and learning of GANs
often becomes unstable. Wasserstein GAN (WGAN) is a promising framework
to deal with the instability problem as it has a good convergence property. One
drawback of the WGAN is that it evaluates the Wasserstein distance in the dual
domain, which requires some approximation, so that it may fail to optimize the
true Wasserstein distance. In this paper, we propose evaluating the exact empiri-
cal optimal transport cost efficiently in the primal domain and performing gradient
descent with respect to its derivative to train the generator network. Experiments
on the MNIST dataset show that our method is significantly stable to converge,
and achieves the lowest Wasserstein distance among the WGAN variants at the
cost of some sharpness of generated images. Experiments on the 8-Gaussian toy
dataset show that better gradients for the generator are obtained in our method.
In addition, the proposed method enables more flexible generative modeling than
WGAN.
1	Introduction
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are a powerful framework of
generative modeling which is formulated as a minimax game between two networks: A gener-
ator network generates fake-data from some noise source and a discriminator network discrimi-
nates between fake-data and real-data. GANs can generate much more realistic images than other
generative models like variational autoencoder (Kingma & Welling, 2014) or autoregressive mod-
els (van den Oord et al., 2016), and have been widely used in high-resolution image generation
(Karras et al., 2018), image inpainting (Yu et al., 2018), image-to-image translation (Isola et al.,
2017), to mention a few. However, GANs are often hard to train, and various ways to stabilize
training have been proposed by many recent works. Nonetheless, consistently stable training of
GANs remains an open problem.
GANs employ the Jensen-Shannon (JS) divergence to measure the distance between the distributions
of real-data and fake-data (Goodfellow et al., 2014). Arjovsky et al. (2017) provided an analysis of
various distances and divergence measures between two probability distributions in view of their
use as loss functions of GANs, and proposed Wasserstein GAN (WGAN) which has better theoret-
ical properties than the original GANs. WGAN requires that the discriminator (called the critic in
Arjovsky et al. (2017)) must lie within the space of 1-Lipschitz functions to evaluate the Wasserstein
distance via the Kantorovich-Rubinstein dual formulation. Arjovsky et al. (2017) further proposed
implementing the critic with a deep neural network and applied weight clipping in order to ensure
that the critic satisfies the Lipschitz condition. However, weight clipping limits the critic’s func-
tion space and can cause gradients in the critic to explode or vanish if the clipping parameters are
not carefully chosen (Arjovsky et al., 2017; Gulrajani et al., 2017). WGAN-GP (Gulrajani et al.,
2017) and Spectral Normalization (SN) (Miyato et al., 2018) apply regularization and normaliza-
tion, respectively, on the critic trying to make the critic 1-Lipschitz, but they fail to optimize the true
Wasserstein distance.
In the latest work, Liu et al. (2018) proposed a new WGAN variant to evaluate the exact empiri-
cal Wasserstein distance. They evaluate the empirical Wasserstein distance between the empirical
distributions of real-data and fake-data in the discrete case of the Kantorovich-Rubinstein dual for-
1
Under review as a conference paper at ICLR 2019
mulation, which can be solved efficiently because the dual problem becomes a finite-dimensional
linear-programming problem. The generator network is trained using the critic network learnt to
approximate the solution of the dual problem. However, the problem of approximation error by the
critic network remains. In this paper, we propose a new generative model without the critic, which
learns by directly evaluating gradient of the exact empirical optimal transport cost in the primal
domain. The proposed method corresponds to stochastic gradient descent of the optimal transport
cost.
2	Wasserstein GAN
Arjovsky et al. (2017) argued that JS divergences are potentially not continuous with respect to
the generator’s parameters, leading to GANs training difficulty. They proposed instead using the
Wasserstein-1 distance W1 (q, p), which is defined as the minimum cost of transporting mass in
order to transform the distribution q into the distribution p. Under mild assumptions, W1 (q, p) is
continuous everywhere and differentiable almost everywhere.
The WGAN objective function is constructed using the Kantorovich-Rubinstein duality (Villani,
2009, Chapter 5) as
Wi(Pr, Pg) = DmaD {Eχ~Pr [D(x)] - Ey~Pg [D(y)]},
(1)
to obtain
mGnDmaD {Eχ~Pr [D(x)] - Ey~Pg [D(y)]},
(2)
where D is the set of all 1-Lipschitz functions, where Pr is the real-data distribution, and where
Pg is the generator distribution implicitly defined by y = G(z), Z 〜 p(z). Minimization of this
objective function with respect to G with optimal D is equivalent to minimizing W1 (Pr, Pg).
Arjovsky et al. (2017) further proposed implementing the critic D in terms ofa deep neural network
with weight clipping. Weight clipping keeps the weight parameter of the network lying in a compact
space, thereby ensuring the desired Lipschitz condition. For a fixed network architecture, however,
weight clipping may significantly limit the function space to a quite small fraction of all possible
1-Lipschitz functions representable by networks with the prescribed architecture.
3	Related works
Gulrajani et al. (2017) proposed introduction of gradient penalty (GP) to the WGAN objective func-
tion in place of the 1-Lipschitz condition in the Kantorovich-Rubinstein dual formulation, in order
to explicitly encourage the critic to have gradients with magnitude equal to 1. Since enforcing the
constraint of unit-norm gradient everywhere is intractable, they proposed enforcing the constraint
only along straight line segments, each connecting a real-data point and a fake-data point. The re-
sulting learning scheme, which is called the WGAN-GP, was shown to perform well experimentally.
It was pointed out, however (Miyato et al., 2018), that WGAN-GP is susceptible to destabilization
due to gradual changes of the support of the generator distribution as learning progresses. Further-
more, the critic can easily violate the Lipschitz condition in practice, so that there is no guarantee
that WGAN-GP optimizes the true Wasserstein distance.
SN, proposed by Miyato et al. (2018), is based on the observation that the Lipschitz norm ofa critic
represented by a multilayer neural network is bounded from above by the product, across all layers,
of the Lipschitz norms of the activation functions and the spectral norms of the weight matrices,
and normalizes each of the weight matrices with its spectral norm to ensure the resulting critic to
satisfy the desired Lipschitz condition. It is well known that, for any m × n matrix W = (wij), the
max norm ∣∣WIlmaX = max{∣Wj∣} and the spectral norm σ(W) satisfy the inequality IlWIlmaX ≤
σ(W) ≤ √mn∣ W∣maχ. This implies that the bound of the Lipschitz constant provided via weight
clipping can be loose compared with that via SN. In other words, SN is expected to provide a much
tighter bound for the Lipschitz condition than weight clipping, and accordingly, the function space
for the critic under SN is larger than that under weight clipping. The function space under SN is,
however, still a subset of the set of all functions satisfying the Lipschitz condition, and consequently,
the resulting estimate for the Wasserstein distance is a lower bound of the true Wasserstein distance.
Furthermore, one cannot tell within the framework of SN how good the estimate is.
2
Under review as a conference paper at ICLR 2019
Liu et al. (2018) proposed a new formulation to evaluate the Wasserstein distance, which is equiv-
alent to the discrete case of the Kantorovich-Rubinstein dual formulation under a mild assumption
and is more tractable due to obviating the need for the Lipschitz condition. This problem is solved
in a two-step fashion, and thus the method proposed in Liu et al. (2018) is called the WGAN-TS.
First, one estimates the Wasserstein distance on the basis of finite real- and fake-data points. The
empirical Wasserstein distance is evaluated exactly via solving the linear-programming version of
the Kantorovich-Rubinstein dual to obtain the optimizer. Second, one approximates the optimizer
obtained in the first step via regression using a deep neural network to parameterize the critic and
obtains its gradient. WGAN-TS can evaluate the Wasserstein distance more accurately than WGAN,
WGAN-GP and WGAN-SN (WGAN with SN), but there are not only approximation errors from us-
ing finite samples to evaluate the empirical Wasserstein distance but also those from deep regression
in the second step, resulting in not being able to minimize the Wasserstein distance directly.
4	Proposed method
The proposed method in this paper is based on the fact that the optimal transport cost between two
probability distributions can be evaluated efficiently when the distributions are uniform over finite
sets of the same cardinality. Our proposal is to evaluate empirical optimal transport costs on the
basis of equal-size sample datasets of real- and fake-data points.
The optimal transport cost between the real-data distribution Pr and the generator distribution Pg is
defined as
C(Pr,Pg)
inf ∖ E(x,y)〜Y [c(X, y儿
γ∈Π(Pr,Pg )	,
(3)
where c(x, y) is the cost of transporting one unit mass from x to y, assumed differentiable with
respect to its arguments almost everywhere, and where Π(Pr, Pg) denotes the set of all couplings
between Pr and Pg , that is, all joint probability distributions that have marginals Pr and Pg .
Let D = {xj |xj 〜 Pr(χ)} be a dataset consisting of independent and identically-distributed (iid)
real-data points, and F = {y∕yi 〜Pg(y)} be a dataset consisting of iid fake-data points sampled
from the generator. Let PD and PF be the empirical distributions defined by the datasets D and
F, respectively. We further assume in the following that |D| = |F | = N holds. The empirical
optimal transport cost C(D, F) = C(PD, PF) between the two datasets D and F is formulated as a
linear-programming problem, as
NN
C(D,F ) = C (Pd , Pf ) = NNΣΣMi,j c(xj, yi)	(4)
i=1 j=1
N
s.t. E Mij = I,∀i ∈ {1, ...,N},	(5)
j=1
N
Mi,j = 1,∀j∈ {1,...,N},	(6)
i=1
Mi,j ≥ 0,∀i ∈ {1,...,N},∀j ∈ {1,...,N}.	(7)
It is known (Villani, 2003) that the linear-programming problem (4)-(7) admits solutions which are
permutation matrices. One can then replace the constraints Mi,j ≥ 0 in (7) with Mi,j ∈ {0, 1}
without affecting the optimality. The resulting optimization problem is what is called the linear sum
assignment problem, which can be solved more efficiently than the original linear-programming
problem. As far as the authors’ knowledge, the most efficient algorithm to date for solving a linear
sum assignment problem has time complexity of O(N 2.5 log(N C)), where C = maxi,j c(xj, yi)
when one scales up the costs {c(xj, yi)|xj ∈ D, yi ∈ F} to integers (Burkard et al., 2012, Chapter
4).
This is a problem to find the optimal transport plan, where Mi,j = 1 is corresponding to transporting
fake-data point yi ∈ F to real-data point xj ∈ D, and where the objective is to minimize the average
transport cost NT EN=I EN=I MijC(Xj,期心)Figure 1 shows a two-dimensional example of this
problem and its solution.
3
Under review as a conference paper at ICLR 2019
Figure 1: Two-dimensional example of optimal transport problem (4)-(7) with N = 8. Circles •
represent real-data points in D and triangles ▲ represent fake-data points in F. Arrows between
circles • and filled triangles ▲ show the optimal transport plan M* with c(χ, y) = ||x - y∣∣2, which
is an identity matrix. Arrows between open △ and filled ▲ triangles show small perturbations of F,
which do not change M*.
One requires evaluations not only of the optimal transport cost C(Pr, Pg) but also of its derivative
in order to perform learning of the generator with backpropagation. Let θ denote the parameter of
the generator, and let ∂θC denote the derivative of the optimal transport cost C with respect to θ.
Conditional on z, the generator output G(z) is a function of θ. Hence, in order to estimate ∂θC,
ʌ
in our framework one has to evaluate ∂θC. In general, it is difficult to differentiate (4) with respect
to generator output yi , as the optimal transport plan M* can be highly dependent on yi . Under the
assumption |D| = |F | = N which we adopt here, however, the feasible set for M is the set of
all permutation matrices and is a finite set. It then follows that, as a generic property, the optimal
transport plan M * is unchanged under small enough perturbations of F (see Figure 1). We take
advantage of this fact and regard M* as independent of yi . Now that differentiation of (4) becomes
tractable, we use (4) as the loss function of the generator and update the generator with the direct
gradient of the empirical optimal transport cost, as ∂θC = NT EN』M*j∂yic(xj, yi)∂eG(zi).
Although the framework described so far is applicable to any optimal transport cost, several desirable
properties can be stated if one specializes in the Wasserstein distance. Assume, for a given p ≥ 1,
that the real-data distribution Pr and the generator distribution Pg have finite moments of order p.
The Wasserstein-p distance between Pr and Pg is defined in terms of the optimal transport cost with
c(x,y) = ∣∣x - y∣p as
Wp(Pr,Pg) =C (Pr,Pg)1/p.	(8)
Due to the law of large numbers, the empirical distributions PD and PF converge weakly to Pr and
Pg, respectively, as N → ∞. It is also known (Villani, 2009, Theorem 6.9) that the Wasserstein-p
distance Wp metrizes the space of probability measures with finite moments of order p. Conse-
quently, the empirical Wasserstein distance Wp(D, F) is a consistent estimator of the true Wasser-
stein distance Wp(Pr, Pg). Furthermore, with the upper bound of the error of the estimator
ʌ
|%(D,F) - Wp(Pr,Pg)| ≤ Wp(Pd, Pr)+ Wp(PF,Pg),	(9)
which is derived on the basis of the triangle inequality, as well as with the upper bounds available
for expectations of Wp(PD, Pr) and Wp(PF, Pg) under mild conditions (Weed & Bach, 2017), one
ʌ
can see that Wp(D, F) is an asymptotically unbiased estimator of Wp(Pr, Pg).
Note that our method can directly evaluate the empirical Wasserstein distance without recourse to
the Kantorovich-Rubinstein dual. Hence, our method does not use a critic and is therefore no longer
a GAN. It is also applicable to any optimal transport cost. We summarize the proposed method in
Algorithm 1.
5	Experiments
5.1	Results on MNIST with convolutional neural network
We first show experimental results on the MNIST dataset of handwritten digits. In this experiment,
we resized the images to resolution 64 × 64 so that we can use the convolutional neural networks
4
Under review as a conference paper at ICLR 2019
Algorithm 1 The proposed method.
1:	Input: Real-data samples Xreal, batch size N, Adam parameters α, β1, β2
2:	Output: Gθ
3:	Initialize θ.
4:	while θ has not converged do
5:	Sample {χi}i∈{i,...,N}〜Xreal from real-data.
6:	Sample {zj}j∈{i,...,N}〜P(Z) from random noises.
7:	Let yj = Gθ(zj), ∀j ∈ {1, . . . ,N}.
8:	Solve (4)-7 to obtain M*.
9：	gθ  ∂θC = NT EN=I Mi：j∂yiC(Xj,yi)∂θG§(Zi)
10:	θ — Adam(gθ ,θ,α,β1,β2)
11:	end while
Table 1: Comparison on MNIST. Empirical Wasserstein distance (EWD), and computation time
per generator update for each method. Each metric represents the average and standard deviation
over 3 out of 5 trials, excluding the maximum and minimum. Lower is better for EWD. WGAN-
TS* represents WGAN-TS without weight scaling. In WGAN-GP, WGAN-SN and WGAN-TS*,
training can suddenly deteriorate. Thus, we used early stopping based on EWD.
Method	EWD	Time [ms/iter]
WGAN	744.8 ± 5.2	1547 ± 18.2
WGAN-GP	811.9 ± 31.7	2867 ± 43.6
WGAN-SN	888.5 土 30.1	1839 ± 44.9
WGAN-TS*	714.5 ± 17.7	886.0 土 10.0
WGAN-TS	761.2 ± 4.5	896.8 ± 9.44
Proposed	600.5 ± 10.8	148.4 ± 3.58
described in Appendix A.1 as the critic and the generator. In all methods, the batch size was set
to 64 and the prior noise distribution was the 100-dimensional standard normal distribution. The
maximum number of iterations in training of the generator was set to 30,000. The Wasserstein-1
distance with c(χ, y) = ||x - y∣∣ ι was used. More detailed settings are described in Appendix B.1.
Although several performance metrics have been proposed and are commonly used to evaluate vari-
ants of WGAN, we have decided to use the empirical Wasserstein distance (EWD) to compare
performance of all methods. It is because all the methods adopt objective functions that are based on
the Wasserstein distance, and because EWD is a consistent and asymptotically unbiased estimator of
the Wasserstein distance and can efficiently be evaluated, as discussed in Section 4. Table 1 shows
EWD evaluated with 256 samples and computation time per generator update for each method. For
reference, performance comparison with the Frechet Inception Distance (Heusel et al., 2017) and
the Inception Score (Salimans et al., 2016), which are commonly used as performance measures to
evaluate GANs using feature space embedding with an inception model, is shown in Appendix C.
The proposed method achieved a remarkably small EWD and computational cost compared with the
variants of WGAN. Our method required the lowest computational cost in this experimental setting
mainly because it does not use the critic. Although we think that the batch size used in the experi-
ment of the proposed method was appropriate since the proposed method achieved lower EWD, if a
larger batch size would be required in training, it will take much longer time to solve the linear sum
assignment problem (4)-(7).
We further investigated behaviors of the methods compared in more detail, on the basis of EWD.
WGAN-SN failed to learn. The loss function of the critic showed divergent movement toward -∞,
and the behaviors of EWD in different trials were different even though the behaviors of the critic
loss were the same (Figure 2 (a) and (b)). WGAN training never failed in 5 trials, and EWD im-
proved stably without sudden deterioration. Although training with WGAN-GP proceeded favorably
in initial stages, at certain points the gradient penalty term started to increase, causing EWD to de-
teriorate (Figure 2 (c)). This happened in all 5 trials. Since gradient penalty is a weaker restriction
than weight clipping, the critic may be more likely to cause extreme behaviors. We examined both
WGAN-TS with and without weight scaling. Whereas WGAN-TS with weight scaling did not fail
5
Under review as a conference paper at ICLR 2019
×106
Number of generator iterations ×104
654321098
■ ■■■■■■■I
Iiiiiiioo
8u9p u-8SSeM -edE 山
0	12	3
Number of generator iterations ×104
4 2 0 8 6
■ ■ ■ ■ ■
Illoo
8u9p u-8SSeM -edE 山
Number of generator iterations ×104
(a)	(b)	(c)
Figure 2:	Training behaviors of the methods experimented. (a) Critic loss versus the number of
generator iterations, and (b) EWD versus the number of generator iterations in four trials of WGAN-
SN. (c) EWD versus the number of generator iterations, averaged over 5 trials, in WGAN, WGAN-
GP, WGAN-TS, and the proposed method.
×103
4 3 2 1 0 9 8
■ ■ ■ ■ ■ ■ ■
1 1 1 1 1 O O
8us9p U-WSJSSSeM -ey-dE 山
Number of generator iterations ×104
0.0
5 0 5 0 5
■ ■ ■ ■ ■
2 2 110
-Oσc--rau∞
0	12	3
Number of generator iterations ×104
5
1
X
5 4 3 2 1
。-。-OJ-© UO-SSdJ 6*
0	12	3
Number of generator iterations ×104
(a) EWD	(b) Scaling factor	(c) Regression error
Figure 3:	Typical behavior of WGAN-TS in training on the MNIST dataset with and without weight
scaling. (a) EWD. (b) Scaling factor, defined as β = sup{ 1, sup{ "(7：)-D(G(Zj)), ||yi - G(Zj)|| >
||yi -G(zj )||
0}} in Liu et al. (2018, Section 4.1). If β ≤ 0, the critic satisfies the 1-Lipschitz constraint. (c)
Regression error of the critic.
in training but achieved higher EWD than WGAN, WGAN-TS without weight scaling achieved
lower EWD than WGAN at the cost of the stability of training (Figure 3). The proposed method
was trained stably and never failed in 5 trials.
As mentioned in Section 3, the critic in WGAN-TS simply regresses the optimizer of the empirical
version of the Kantorovich-Rubinstein dual. Thus, there is no guarantee that the critic will satisfy
the 1-Lipschitz condition. Liu et al. (2018) pointed out that it is indeed practically problematic with
WGAN-TS, and proposed weight scaling to ensure that the critic satisfies the desired condition. We
have empirically found, however, that weight scaling exhibited the following trade-off (Figure 3).
Without weight scaling, training of WGAN-TS suddenly deteriorated in some trials because the
critic came to not satisfy the Lipschitz condition. With weight scaling, on the other hand, the re-
gression error of the critic with respect to the solution increased and the EWD became worse. The
proposed method directly solves the empirical version of the optimal transport problem in the primal
domain, so that it is free from such trade-off.
Figure 4 shows fake-data images generated by the generators trained with WGAN, WGAN-GP,
WGAN-TS, and the proposed method. Although one can identify the digits for the generated images
with the proposed method most easily, these images are less sharp. Among the generated images
with the other methods, one can notice several images which have almost the same appearance as
real-data images, whereas in the proposed method, such fake-data images are not seen and images
that seem averaged real-data images belonging to the same class often appear. This might imply that
6
Under review as a conference paper at ICLR 2019
5
Iqq3。。2,
677/6 √ 7/
q
7
3
5
√
ʃ
σ□ QJ 7 J
4» — a Qa715
B ¥ 夕 5a7qo
7。5 α l∙g/。
30。9£6 — 夕
ɔ / 6 ʃ /7 Z <σ √b
70Γ夕 与oxq
Uf ∕√^r∕6 Q-O
1 7 Θ α/ β / ZA 3
? 7 1 9
JqJU
9 ¢15
a。。a
ʃQq6
乙 9 IS 8
CC 7 9
ɑΓo!
43 3,
$Λ3 4
or ð Λ/ Trc
3，3夕
§ α∙l7
Z774
$ 7、SE
7<6 3
563 4
Wqet
ʃNc
7 4 4 K
f7 3
S 3 7 4
a4 rʌʃ
3夕6 0
3D
Q 7彳。
4斗73
7。O 9
，89？
3 6 ɑɔ
ʌz C/ Λ∕Lɛ-
q
o
7
q
I
7
(a) WGAN	(b) WGAN-GP	(c) WGAN-TS	(d) Proposed
Figure 4: Examples of fake-data images generated by the generators trained on the MNIST dataset
with the four methods compared.
(a) Initial stage	(b) Intermediate stage	(c) Late stage
Figure 5: Generator distribution in successful trial with the proposed method on the 8-Gaussian toy
dataset. Eight large circles Q represent positions of GauSSian components of real-data distribution,
and filled triangles ▲ are fake-data points. Snapshots in (a) initial stage, (b) intermediate stage, and
(c) late stage of training are shown.
merely minimizing the Wasserstein distance between the real-data distribution and the generator
distribution in the raw-image space may not necessarily produce realistic images.
5.2 Gradient optimality of generator
We next observed how the generator distribution is updated in order to compare the proposed method
with variants of WGAN in terms of the gradients provided. Figure 5 shows typical behavior of the
generator distribution trained with the proposed method on the 8-Gaussian toy dataset. The 8-
Gaussian toy dataset and experimental settings are described in Appendix B.2. One can observe
that, as training progresses, the generator distribution comes closer to the real-data distribution.
Figure 6 shows comparison of the behaviors of the proposed method, WGAN-GP, and WGAN-
TS. We excluded WGAN and WGAN-SN from this comparison: WGAN tended to yield generator
distributions that concentrated around a single Gaussian component, and hence training did not
progress well. WGAN-SN could not correctly evaluate the Wasserstein distance as in the experiment
on the MNIST dataset.
One can observe in Figure 6 that directions of sample updates are diverse in the proposed method,
especially in later stages of training, and that the sample update directions tend to be aligned with
the optimal gradient directions. These behaviors will be helpful for the generator to learn the real-
data distribution efficiently. In WGAN-GP and WGAN-TS, on the other hand, the sample update
directions exhibit less diversity and less alignment with the optimal gradient directions, which would
make the generator distribution difficult to spread and would slow training. One would be able to
ascribe such behaviors to poor quality of the critic: Those behaviors would arise when the gener-
ator learns on the basis of unreliable gradient information provided by the critic without learning
sufficiently to accurately evaluate the Wasserstein distance. If one would increase the number nc of
critic iterations per generator iteration in order to train the critic better, the total computational cost
of training would increase. In fact, nc = 5 is recommended in practice and has commonly been used
7
Under review as a conference paper at ICLR 2019
(a) Proposed
(b) WGAN-GP
(c) WGAN-TS
Figure 6: Behaviors of updating in training on the 8-Gaussian toy dataset with (a) the proposed
method, (b) WGAN-GP, and (c) WGAN-TS. Eight large circles Q represent positions of Gaussian
components of real-data distribution. Filled circles • are real-data points, filled triangles ▲ are pre-
updated fake-data points, and open triangles △ are post-updated fake-data points. Solid and dotted
arrows indicate optimal gradient directions of fake-data points and directions of their actual updates,
respectively. Left, center, and right panels are at iterations 10, 20, and 30, respectively.
in WGAN (Arjovsky et al., 2017) and its variants because the improvement in learning of the critic
is thought to be small relative to increase in computational cost. In reality, however, 5 iterations
would not be sufficient for the critic to learn, and this might be a principal reason for the critic to
provide poor gradient information to the generator in the variants of WGAN.
6 Conclusion
We have proposed a new generative model that learns by directly minimizing exact empirical
Wasserstein distance between the real-data distribution and the generator distribution. Since the pro-
posed method does not suffer from the constraints on the transport cost and the 1-Lipschitz constraint
imposed on WGAN by solving the optimal transport problem in the primal domain instead of the
dual domain, one can construct more flexible generative modeling. The proposed method provides
the generator with better gradient information to minimize the Wasserstein distance (Section 5.2)
and achieved smaller empirical Wasserstein distance with lower computational cost (Section 5.1)
than any other compared variants of WGAN. In the future work, we would like to investigate the
behavior of the proposed method when transport cost is defined in the feature space embedded by
an appropriate inception model.
Acknowledgments
Support of anonymous funding agencies is acknowledged.
8
Under review as a conference paper at ICLR 2019
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial net-
works. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Con-
ference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
214-223, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/arjovsky17a.html.
R. Burkard, M. Dell’Amico, and S. Martello. Assignment Problems.	Society for In-
dustrial and Applied Mathematics, 2012. doi: 10.1137/1.9781611972238. URL
https://epubs.siam.org/doi/abs/10.1137/1.9781611972238.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neu-
ral Information Processing Systems 27, pp. 2672-2680. Curran Associates, Inc., 2014. URL
http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein GANs. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural In-
formation Processing Systems 30, pp. 5767-5777. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
Hochreiter.	Gans trained by a two time-scale update rule converge to a local
nash equilibrium. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 30, pp. 6626-6637. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-conv
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6e—Rmsprop: Divide the gradi-
ent by a running average of its recent magnitude. COURSERA: Neural Networks for Machine
Learning, 2012.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network train-
ing by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceed-
ings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings
of Machine Learning Research, pp. 448-456, Lille, France, 07-09 Jul 2015. PMLR. URL
http://proceedings.mlr.press/v37/ioffe15.html.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with
conditional adversarial networks. In 2017 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pp. 5967-5976, July 2017. doi: 10.1109/CVPR.2017.632. URL
doi.ieeecomputersociety.org/10.1109/CVPR.2017.632.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for
improved quality, stability, and variation. International Conference on Learning Representations,
2018. URL https://arxiv.org/abs/1710.10196.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In
Proceedings of the International Conference on Learning Representations, 2015. URL
https://arxiv.org/abs/1412.6980.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Pro-
ceedings of the International Conference on Learning Representations, 2014. URL
https://arxiv.org/abs/1312.6114.
Huidong Liu, Xianfeng GU, and Dimitris Samaras. A two-step computation of the exact GAN
Wasserstein distance. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re-
search, pp. 3159-3168, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/liu18d.html.
9
Under review as a conference paper at ICLR 2019
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. International Conference on Learning Representations, 2018.
URL https://arxiv.org/abs/1802.05957.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen,
and Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Informa-
tion Processing Systems 29, pp. 2234-2242. Curran Associates, Inc., 2016. URL
http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf.
Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals,
and Alex Graves. Conditional image generation with pixelcnn decoders. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural In-
formation Processing Systems 29, pp. 4790-4798. Curran Associates, Inc., 2016. URL
http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decode
Cedric Villani. Topics in Optimal Transportation, volume 58 of Graduate Studies in Mathematics.
American Mathematical Society, 2003.
Cedric Villani. Optimal Transport: Old and New. Springer, 2009.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of em-
pirical measures in Wasserstein distance. to appear in Bernoulli, arXiv preprint arXiv:1707.00087
[math.PR], 2017.
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Free-form image
inpainting with gated convolution. arXiv preprint arXiv:1806.03589 [cs.CV], 2018.
10
Under review as a conference paper at ICLR 2019
Appendix
A	Network architectures
A.1 Convolutional Neural Networks
We show in Table 2 the network architecture used in the experiment on the MNIST dataset in Sec-
tion 5.1. The generator network receives a 100-dimensional noise vector generated from the standard
normal distribution as an input. The noise vector is passed through the fully-connected layer and
reshaped to 4 × 4 feature maps. Then they are passed through four transposed convolution layers
with 5 × 5 kernels, stride 2 and no biases (since performance was empirically almost the same with
or without biases, we took the simpler option of not considering biases), where the resolution of
feature maps is doubled and the number of them is halved except for the last layer.
The critic network is basically the reverse of the generator network. A convolution layer is used
instead of a transposed convolution layer in the critic. After the last convolution layer, the feature
maps are flattened into a vector and passed through the fully-connected layer.
We employed batch normalization (Ioffe & Szegedy, 2015) in all intermediate layers in both of the
generator and the critic. Rectified linear unit (ReLU) was used as the activation function in all but
the last layers. As the activation function in the last layer, the hyperbolic tangent function and the
identity function were used for the generator and for the critic, respectively.
A.2 Fully-connected Neural Networks
We show in Table 3 the network architecture used in the experiment on the 8-Gaussian toy dataset
in Section 5.2. The generator network architecture receives a 100-dimensional noise vector as in
the experiment on the MNIST dataset. The noise vector is passed through the four fully-connected
layers with biases and mapped to a two-dimensional space. The critic network is likewise the reverse
of the generator network.
B Detailed experimental setting
B.1	MNIST dataset
The MNIST dataset of handwritten digits used in the experiment in Section 5.1 contains 60,000
two-dimensional images of handwritten digits with resolution 28 × 28.
We used default parameter settings decided by the proposers of the respective methods. We used
RMSProp (Hinton et al., 2012) with learning rate 5e-5 for the critic and the generator in WGAN.
The weight clipping parameter c was set to 0.01. We used Adam (Kingma & Ba, 2015) with learning
rate 1e-4, β1 = 0.5, β2 = 0.999 in the other methods. λgp in WGAN-GP was set to 10. In the
methods with the critic, the number nc of critic iterations per generator iteration was set to 5.
B.2	8-Gauusian toy dataset
The 8-Gaussian toy dataset used in the experiment in Section 5.2 is a two-dimensional synthetic
dataset, which contains real-data sampled from the Gaussian mixture distribution with 8 centers
equally distant from the origin and unit variance as the real-data distribution. The centers of the 8
Gaussian component distributions are (±10,0), (0, ±10), and (±10∕√2, ±10∕√2). 30,000 sam-
ples were generated in advance before training and were used as the real-data samples.
In all methods, the batch size was set to 64 and the maximum number of iterations in training
the generator was set to 1,000. WGAN and WGAN-SN could not learn well with this dataset,
even though we considered several parameter sets. We used Adam with learning rate 1e-3, β1 =
0.5, β2 = 0.999 for WGAN-GP, WGAN-TS and the proposed method. λgp in WGAN-GP was set
to 10. In the methods with the critic, the number nc of critic iterations was set to 5.
11
Under review as a conference paper at ICLR 2019
Table 2: Convolutional Neural Network
Generator	Output shape	Parameters	Activation
Noise vector	(≡T	-	-
Fully-connected	(8096)	809.6k	ReLU
Reshape	(4, 4,1024)	-	-
BatchNorm.	-	-	-
TransPosedConv. 5 X 5	(8, 8, 512)	13.1M	ReLU
BatchNorm.	-	-	-
TransposedConv. 5 × 5	(16,16, 256)	3.3M	ReLU
BatchNorm.	-	-	-
TransposedConv. 5 × 5	(32, 32,128)	819.2k	ReLU
BatchNorm.	-	-	-
TransposedConv. 5 × 5	(64, 64, 1)	3.2k	tanh
Total trainable parameters	-	18.0M	-
Critic	Output shape	Parameters	Activation
Input image	(64, 64,1)	-	-
Conv. 5 × 5	(32, 32, 128)	3.2k	ReLU
BatchNorm.	-	-	-
Conv. 5 × 5	(16, 16, 256)	819.2k	ReLU
BatchNorm.	-	-	-
Conv. 5 × 5	(8, 8, 512)	3.3M	ReLU
BatchNorm.	-	-	-
Conv. 5 × 5	(4, 4, 1024)	13.1M	ReLU
BatchNorm.	-	-	-
Flatten	(8096)	-	-
Fully-connected		(1L	8.1k	-
Total trainable parameters	-	17.2M	-
Table 3: Fully-connected Neural Network
Generator	Output shape	Parameters	Activation
Noise vector	(100Γ	-	-
Fully-connected	(512)	51.7k	ReLU
Fully-connected	(512)	262.7k	ReLU
Fully-connected	(512)	262.7k	ReLU
Fully-connected	(2))	1.0k	-
Total trainable parameters	-	5.781M	-
Critic	Output shape	Parameters	Activation
Input points	(2Γ	-	-
Fully-connected	(512)	1.54k	ReLU
Fully-connected	(512)	262.7k	ReLU
Fully-connected	(512)	262.7k	ReLU
Fully-connected		(1L	1.02k	-
Total trainable parameters	-	5.371M	-
12
Under review as a conference paper at ICLR 2019
Table 4: Comparison on the MNIST dataset. Frechet Inception Distance (FID), Inception Score
(IS) Each metric represents the average and standard deviation over 3 out of 5 trials, excluding the
maximum and minimum. Lower is better for FID. Higher is better for IS. FID and IS were calculated
with 50, 000 samples. WGAN-TS* represents WGAN-TS without weight scaling.
Method	FID	IS
WGAN	0.41 ± 0.02	2.15 ± 0.02
WGAN-GP	3.97 ± 2.49	2.15 ± 0.09
WGAN-SN	4.01 ± 1.36	2.16 ± 0.19
WGAN-TS*	0.74 ± 0.23	2.07 ± 0.05
WGAN-TS	0.68 ± 0.06	2.16 ± 0.01
Proposed	5.57 ± 0.14	2.33 ± 0.02
B.3	Execution environment
All the numerical experiments in this paper were executed on a computer with an Intel Core i7-
6850K CPU (3.60 GHz, 6 cores) and 32 GB RAM, and with four GeForce GTX 1080 graphics
cards installed. Linear sum assignment problems were solved using the Hungarian algorithm, which
has time complexity of O(N 3). Codes used in the experiments were written in tensorflow 1.10.1 on
python 3.6.0, with eager execution enabled.
C Evaluation with FID and IS on MNIST dataset
We show the result of evaluation of the experimented methods with FID and IS in Table 4. Both FID
and IS are commonly used to evaluate GANs.
FID calculates the distance between the set of real-data points and that of fake-data points. The
smaller the distance is, the better the fake-data points are judged. Assuming that the vector ob-
tained from a fake- or real-data point through the inception model follows a multivariate Gaussian
distribution, FID is defined by the following equation:
FID2 = ∣∣μι - μ2∣∣2 + tr (∑1 + ∑2 - 2(∑ι∑2户),	(10)
where (μi, ∑i) is the mean vector and the covariance matrix for dataset i, evaluated in the feature
space embedded with inception scores. It is nothing but the square of the Wasserstein-2 distance be-
tween two multivariate Gaussian distributions with parameters (μι, ∑ι) and (μ2, ∑2), respectively.
IS is a metric to evaluate only the set of fake-data points. Let xi be a data point, y be the label of xi
in the data identification task for which the inception model was trained, p(y|xi) be the probability
of label y obtained by inputting xi to the inception model. Letting X be the set of all data points
used for calculating the score, the marginal probability of label y is p(y)=击 Exa∈χ p(y∣Xi). IS
is defined by the following equation:
IS = exp
(高工 KL(p(y∣g)∣∣p(y)))
(11)
where KL is Kullback-Leibler divergence. IS is designed to be high as the data points are easy to
identify by the inception model and variation of labels identified from the data points is abundant.
In WGAN-GP, WGAN-SN and WGAN-TS*, we observed that training suddenly deteriorated in
some trials. We thus used early stopping on the basis of EWD, and the results of these methods
shown in Table 4 are with early stopping.
The proposed method marked the worst in FID and the best in IS among all the methods compared.
Certainly, the fake-data generated by the proposed method are non-sharp and do not resemble real-
data points, but it seems that it is easy to distinguish them and they have diversity as digit images.
If one wishes to produce higher FID results using the proposed method, transport cost should be
considered in the desired space corresponding to FID.
13