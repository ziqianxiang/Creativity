Under review as a conference paper at ICLR 2019
DIRECT OPTIMIZATION THROUGH arg max FOR
Discrete Variational Auto-Encoder
Anonymous authors
Paper under double-blind review
Ab stract
Reparameterization of variational auto-encoders is an effective method for reduc-
ing the variance of their gradient estimates. However, when the latent variables are
discrete, a reparameterization is problematic due to discontinuities in the discrete
space. In this work, we extend the direct loss minimization technique to discrete
variational auto-encoders. We first reparameterize a discrete random variable us-
ing the arg max function of the Gumbel-Max perturbation model. We then use
direct optimization to propagate gradients through the non-differentiable arg max
using two perturbed arg max operations.
1	Introduction
Models with discrete latent variables drive extensive research in machine learning applications, such
as language classification and generation (Yogatama et al., 2016; Hu et al., 2017; Shen et al., 2018),
molecular synthesis (Kusner et al., 2017), or game solving (Mena et al., 2018). Compared to their
continuous counterparts, discrete latent variable models can decrease the computational complexity
of inference calculations, for instance, by discarding alternatives in hard attention models (Lawson
et al., 2017), they can improve interpretability by illustrating which terms contributed to the solution
(Mordatch & Abbeel, 2017; Yogatama et al., 2016), and they can facilitate the encoding of inductive
biases in the learning process, such as images consisting of a small number of objects (Eslami et al.,
2016) or tasks requiring intermediate alignments (Mena et al., 2018). Finally, in some cases, discrete
latent variables are natural choices, including when modeling datasets with discrete classes (Rolfe,
2016; Jang et al., 2016; Maddison et al., 2016).
Models involving discrete latent variables are hard to train, with the key issue being to estimate
the gradients of the resulting non-differentiable objectives. While one can use unbiased estimators,
such as REINFORCE (Williams, 1992), their variance is typically high (Paisley et al., 2012; Mnih
& Gregor, 2014; Titsias, 2015; Gu et al., 2015; Mnih & Rezende, 2016; Tucker et al., 2017). In
variational auto-encoders (VAEs) with continuous latent variables, the reparameterization trick pro-
vides a successful alternative (Kingma & Welling, 2013; Rezende et al., 2014). However, it cannot
be directly applied to non-differentiable objectives.
Recent work (Maddison et al., 2016; Jang et al., 2016) uses a relaxation of the discrete VAE objec-
tive, where latent variables follow a Gumbel-Softmax distribution. The Gumbel-Softmax approach
is motivated by the connection of the Gumbel-Max perturbation model and the Gibbs distribution
(Papandreou & Yuille, 2011; Tarlow et al., 2012; Hazan et al., 2013; Maddison et al., 2014). The
Gumbel-Softmax distribution relaxes the non-differentiable Gumbel-Max trick for sampling from
the categorical distribution by replacing the arg max operation with a softmax operation. The refor-
mulation results in a continuous objective function, which allows the use of the reparameterization
trick. (Mena et al., 2018) provide an extension of this approach for discrete latent structures, namely
distributions over latent matchings.
Our work proposes minimizing the non-differentiable objective, by extending the direct loss mini-
mization technique to generative models (McAllester et al., 2010; Song et al., 2016). Since categor-
ical variables are represented by the Gibbs distribution, we start from the arg max formulation of
the Gibbs distribution. We subsequently derive an optimization method that can propagate (biased)
gradients through reparameterized arg max. The gradient of the resulting expectation is estimated
by the difference of gradients of two arg max-perturbations.
1
Under review as a conference paper at ICLR 2019
We begin by introducing the notation, the VAE formulation and the equivalence between the Gibbs
distribution and the Gumbel-Max perturbation model in Section 3. In Section 4.1, we use these com-
ponents to reformulate the discrete VAE objective, with respect to the arg max prediction operation.
We subsequently state and prove the main result that allows us to differentiate through the arg max
function in Section 4.2. In Section 5, we extend this result to mixed discrete-continuous VAEs and
to semi-supervised VAE objectives. Finally, we demonstrate the effectiveness of our approach on
image generation.
2	Related work
Variational inference has been extensively studied in machine learning, see Blei et al. (2017) for a
review paper. In our work, we consider variational Bayes bounds with a discrete latent space. Many
approaches to optimizing the variational Bayes objective, that are based on samples from the distri-
bution, can be seen as applications of the REINFORCE gradient estimator (Williams, 1992). These
estimators are unbiased, but without a carefully chosen baseline, their variance tends to be too high
for the estimator to be useful and considerable work has gone into finding effective baselines (Pais-
ley et al., 2012). Other methods use various techniques to reduce the estimator variance (Ranganath
et al., 2014; Mnih & Gregor, 2014; Gu et al., 2015).
Reparameterization is an effective method to reduce the gradient estimate variance in generative
learning. Kingma & Welling (2013) have shown its effectiveness in auto-encoding variational Bayes
(also called variational auto-encoders, VAEs) for continuous latent spaces. Rezende et al. (2014)
demonstrated its effectiveness in deep latent models. The success of these works led to reparame-
terization approaches in discrete latent spaces. Rolfe (2016) and Vahdat et al. (2018) represent the
marginal distribution of each binary latent variable as a continuous variable in the unit interval. This
reparameterization allows the backpropogation of gradients through the continuous representation.
These works are restricted to binary random variables, and as a by-product, it encourages high-
dimensional representations for which inference is exponential in the dimension size. In contrast,
our work reparameterizes the discrete Gibbs latent model, using a Gumbel-Max perturbation model
and directly propagates gradients through the reparameterized objective.
Maddison et al. (2016) and Jang et al. (2016) recently introduced a novel distribution, the Concrete
distribution or the Gumbel-Softmax, that continuously relaxes discrete random variables. Replacing
every discrete random variable in a model with a Concrete random variable, results in a continu-
ous model, where the reparameterization trick is applicable. These works are close to ours, with
a few notable differences. They use the Gumbel-Softmax function and their model is smooth and
reparameterization may use the chain rule to propagate gradients. Similar to our setting, the Gumbel-
Softmax operation results in a biased estimate of the gradient. Different from our setting, the softmax
operation relaxes the variational Bayes objective and results in a non-tight representation. Our work
uses the Gumbel-Max perturbation model, which is an equivalent representation of the Gibbs dis-
tribution. With that, we do not relax the variational Bayes objective, while our arg max prediction
remains non-differentiable and we cannot naively use the chain rule to propagate gradients. Instead,
we develop a direct optimization method to propagate gradients through arg max operation using
the difference of gradients of two max-perturbations. Our gradient estimate is biased, except for its
limit.
Differentiating through arg max prediction was previously done in discriminative learning, in the
context of direct loss minimization (McAllester et al., 2010; Song et al., 2016). Unfortunately, di-
rect loss minimization cannot be applied to generative learning, since it does not have a posterior
distribution around its arg max prediction. We apply the Gumbel-Max perturbation model to trans-
form the arg max prediction to the Gibbs distribution. This also allows us to overcome the “general
position” assumption in (McAllester et al., 2010; Song et al., 2016) using our “prediction generating
function”.
3	Background
To model the data generating distribution, we consider samples S = {x1, ..., xm} originating from
some unknown underlying distribution. We explain the generation process of a parameterized model
pθ(x), by minimizing its log-loss when marginalizing over its hidden variables z. Using variational
2
Under review as a conference paper at ICLR 2019
Bayes, we upper bound the log-loss of an observed data point
E-IOg Pθ (x) ≤f-Ez 〜qφ log Pθ (x|z) + ^KL(qφ(z∖x)∖∖pθ (Z))	(1)
x∈S	x∈S	x∈S
Typically, the model distribution pθ(x∖z) follows the Gibbs distribution law pθ(x∖z) = e-θ(x,z).
When considering a discrete latent space, i.e., z ∈ {1, ..., k}, the approximated posterior distribution
follows the Gibbs distribution law qφ(z∖x) 8 eφ(x,z). The challenge in generative learning is to
reparameterize and optimize
k
-Ez〜qφ logpθ (XIz) = E
z=1
eφ(x,z)
Pz eΦ(χ,z) θ(x,z)
(2)
In our work, we reparameterize the variational bound using the equivalence between Gibbs models
and Gumbel-Max perturbation models.
Gumbel-Max perturbation models allow an alternative representation of Gibbs distributions
qφ(z∖χ) 8 eφ(x,Z) that is based on the extreme value statistics of Gumbel-distributed random vari-
ables. Let γ be a random function that associates random variables γ(z) for each z = 1, ..., k.
When the random perturbations follow the zero mean Gumbel distribution law, whose probability
density function is g(γ) = Qzk=1 e-(γ(z)+c+e-(γ(z)+c)) for the Euler
constant c ≈ 0.57, we obtain
the following identity between Gibbs models and Gumbel-Max perturbation models1 (cf. Kotz &
Nadarajah (2000))
eφ(x,z)
Pz eΦ(χ,z)
PY〜g[zφ+γ = z], where zφ+γ = arg max {φ(x,z)+ γ(z)}
/ V	z=1,…,k
(3)
For completeness, a proof for this statement appears in Appendix A.
4	Reparameterization and direct optimization
In the following section, we reformulate the discrete VAE objective using the Gumbel-Max pertur-
bation model. We then derive an optimization method that directly propagates gradients through the
reparameterized arg max function.
4.1	VAEs with Gumbel-Max perturbation models
Perturbation models allow an alternative representation of Gibbs distributions qφ(z∖χ) a eφ(x,z).
Using the Gumbel-Max perturbation model in Equation (3), the negative log likelihood in Equation
(2) takes the form
kk
-Ez〜qφ logPθ(x∖z) = X PY〜g [zφ+γ = z]θ(x,z) = X PY〜g[zφ+γ = z]θ(x, zφ+γ) (4)
z=1	z=1
k
=X EY 〜g [1ζΦ+γ=zθ(x,zφ+γ )]= EY 〜g [θ(x,zφ+γ)]	(5)
z=1
These quantities result from applying the law of total expectation, while realizing the prob-
ability events for zφ+Y : The last equality in Equation (4) holds since we restrict to the
space zφ+Y = z. The last equality in Equation (5) holds by the linearity of the expecta-
tion, i.e., Pk=ι EY〜g[1zΦ+y=zΘ(x,zφ+y)] = EY〜g[Pk=ι be+γ =zθ(χ, zφ+y)] and the fact that
Pzk=1 1zφ+γ=z = 1.
The gradient of the decoder log probability θ(x, zφ+Y) with respect to its parameters is derived by
the chain rule. The main challenge is to evaluate the gradient of EY〜g [θ(χ, zφ+y)] with respect to
the encoder parameters, since the chain rule does not propagate through the arg max function zφ+Y .
1The set argmax^=ι,…,k{φ(x, z) + γ(Z)} is the set of all maximal arguments, and does not always con-
sist of a single element. However, since the Gumbel distribution is continuous, the γ for which their set
argmaxz=ι,…,k{φ(x, Z) + γ(z)} consists more than a single element has a measure of zero. For notational
convenience, when we consider integrals (or probability distributions), we ignore measure zero sets.
3
Under review as a conference paper at ICLR 2019
4.2	DIRECT OPTIMIZATION THROUGH arg max
Our main result is presented in Theorem 1 and shows how to compute the gradient of the repa-
rameterized discrete VAE, i.e., EY〜g [θ(χ, zφ+γ)] with respect to the encoder parameters v. In the
following We omit Y 〜g for brevity. To make the encoder parameters V explicit in our notation, We
denote the encoder function by φv(x, z).
Instrumental to our approach, is a novel “prediction generating function”.
G(v, E) = EY[max{eθ(x, Z) + φv (x, z) + γ(Z)}]	(6)
Z
The proof of Theorem 1 is composed from three steps:
1.	We prove that G(v, E) is a smooth function ofv, E. Therefore, the Hessian of G(v, E) exists
and it is symmetric, namely
∂v∂G(v, E) = ∂∂vG(v, E).	(7)
2.	We show that encoder gradient is apparent in the Hessian:
∂v∂eG(v, 0) = VvEY[θ(x,zφv+γ)].	(8)
3.	We derive our update rule as the complement representation of the Hessian:
∂e∂vG(v, 0) = lim ɪ (EY[Vvφv(x, zeθ+φv+γ) - Vvφv(x, zφv+γ)])	(9)
The reparameterized gradient computation appears in Equation (10) and it is formally derived in
Theorem 1.
Theorem 1. Assume φv (x, z) is a smooth function of v. Then
VvEY[θ(x, zφv+γ)] = limɪ (EY[Vvφv(x, zeθ+φv+γ) - Vvφv(x, zφv+γ)])	(10)
Proof. First, we prove that G(v, E) is a smooth function. Recall, g(γ) is the zero mean Gumbel
probability density function. Applying a change of variable γ(z) = eθ(x, Z) + φv (x, Z) + γ(z), we
obtain
Z∞∞
g(γ) max{eθ(x, z) + φv(x, z) + γ(z)}dγ = /	g(γ — eθ — φv) max{^(z)}dγ.
-∞ Z	J-∞	Z
Since g(γ — eθ — φv) is a smooth function of E and φv (x, z) and φv (x, z) is a smooth function of v,
we conclude that G(v, E) is a smooth function of v, E. Therefore, the Hessian of G(v, E) exists and
symmetric, i.e., ∂v∂eG(v, E) = ∂e∂vG(v, E). We thus proved Equation (7).
To prove Equations (8) and (9) we differentiate under the integral, both with respect to E and with
respect to v. We are able to differentiate under the integral, since g(^ — eΘ — φv) is a smooth function
of E and v and its gradient is bounded by an integrable function (cf. Folland (1999), Theorem 2.27).
We turn to prove Equation (8). We begin by noting that maxz{eθ(x, Z) + φv(x, Z) + Y(z)} is a
maximum over linear function of E, thus by Danskin Theorem (cf. Bertsekas et al. (2003), Propo-
sition 4.5.1) holds ∂e(maxz{eθ(x, Z) + φv(x, Z) + γ(Z)}) = θ(x, zeθ+φv +y). By differentiating
under the integral, ∂eG(v, E) = EY [∂eE(θ(x, zeθ+φv +Y) + φv (x, zeθ+φv+Y) + Y(zeθ+φv+Y))] =
EY [θ(x, zeθ+φv+Y)]. We obtain Equation (8) by differentiating under the integral, now with respect
to v, and setting E = 0.
Finally, we turn to prove Equation (9). By differentiating under the integral ∂vG(v, E) =
EY [Vv φv (x, zeθ+φv+Y)]. Equation (9) is attained by taking the derivative with respect to E = 0
on both sides.
The theorem follows by combining Equation (7) when E = 0, i.e., ∂v∂eG(v, 0) = ∂e∂vG(v, 0) with
the equalities in Equations (8) and (9).	□
4
Under review as a conference paper at ICLR 2019
Figure 1: The bias/variance tradeoff of our gradient estimate as a function of , comparing to
Gumbel-Softmax gradient estimate as a function of its temperature τ . The parameters were learned
using REINFORCE and from its optimal parameters we estimate the gradient randomly for 500
times. Left: the average difference from REINFORCE gradient. Right: the average standard devia-
tion of the gradient estimate.
The gradient estimate in Theorem 1 is unbiased in the limit → 0. However, for small epsilon
the gradient is either zero, when zθ+φ+γ = zφ+γ , or very large, since the gradients difference is
multiplied by 1/. For large we often obtain a moderate non-zero gradient. In practice we use
≥ 0.1 which means that the gradient estimate is biased. This bias-variance tradeoff is evaluated in
Figure 1, on an encoder with three layers of sizes (784, 300, 10), a discrete latent structure of size
10 and a matching decoder. In this evaluation we sampled 200 gradients for a fixed to evaluate
the bias and standard deviation for each coordinate of the gradient. We report in the graph these
quantities.
The above theorem closely relates to the direct loss minimization technique (cf. McAllester et al.
(2010); Song et al. (2016)), which, in our setting, can be used to compute the gradient of Exθ(x, zφ).
The direct loss minimization predicts a single zφ for a given x and, therefore, cannot generate a
posterior distribution on all z = 1, ..., k, i.e., it lacks a generative model that exists in Gumbel-Max
perturbation models.
5	Extensions
5.1	Semi supervised generative models
The main advantage in our framework is that learning a discrete VAE using Gumbel-Max repa-
rameterization is intimately related to predicting a discrete latent label through the arg max op-
eration zφ+γ . Therefore, semi-supervised VAEs are naturally integrated into our reparameteriza-
tion framework using any loss function. Formally, assume that a subset of the data is labeled, i.e.,
S1 = {(x1, z1), ..., (xm1 , zm1)}. In semi-supervised learning, we may add to the learning objective
the loss function `(z, zφ+γ), for any (x, z) ∈ S1, to better control the prediction of the latent space.
The semi-supervised discrete VAEs objective function is
X EY[θ(x,zφ+γ)]+ X EY['(z,zφ+γ)] + X KL(qφ(z∣x)∣∣pθ(Z))	(11)
x∈S	(x,z)∈S1	x∈S
The supervised component is explicitly handled by Theorem 1 and optimizing a semi-supervised
discrete VAEs is straight forward in our framework. Our supervised component is intimately re-
lated to direct loss minimization (McAllester et al., 2010; Song et al., 2016), which, in our setting,
minimizes P(x,z)∈S `(z, zφ). Compared to direct loss minimization, our work adds random per-
turbation γ to the encoder and thus overcomes the “general position” assumption of direct loss min-
imization. This addition allows us to introduce the “prediction generating function”, which greatly
simplifies our proof. In addition, the added random perturbation γ allows us to use a generative
model to prediction, namely, we can randomly generate different explanations zφ+Y while the direct
loss minimization allows a single explanation in the form of zφ .
5
Under review as a conference paper at ICLR 2019
n×k	direct	GSM	REBAR	RELAX
20 × 10	103.62	105.36	-	-
20 × 2	126.37	128.92	127.07	126.66
1 × 40	145.64	145.67	-	-
Figure 2: Right: Test log-loss bound of VAEs with different categorial variables z = (z1 , ..., zn)
with z ∈ {1, ..., k}. Left: Test log-loss bound of discrete VAE with 20 binary units as a function of
training epochs.
Semi-supervised generative models require the discrete space to have a semantic meaning. For
example, in generating images of digits, the image may be represented both by the discrete digit
class (e.g., 0, 1, ...) and the continuous style (e.g., bold, tilted,...). Following (Kingma et al., 2014;
Jang et al., 2016) we use a mixture of discrete and continuous latent variables in our semi-supervised
setting, to capture both the discrete class information and the continuous class style. The network
architecture consists of an initial encoder φ to process the input, which is fed into two separate
encoders φd and φc that encode the discrete and continuous latent spaces respectively. The mixed
discrete-continuous latent space consists of the matrix diag(zφd+7) ∙ zo i.e, if zφd +γ = i then
this matrix is all zero, except for the i-th row, which consists of the independent Gaussian random
variables zc. An illustration of our mixed discrete-continuous VAE architecture appears in Figure 1.
The gradient of the continuous component of the encoder is computed by reparameterization trick.
The gradient of the discrete component is computed according to Theorem 1.
5.2 Discrete product spaces
Discrete VAEs are being applied also to discrete product spaces (Maddison et al., 2016; Jang et al.,
2016). A single discrete random variable z ∈ {1, .., k} may not represent the variability of the
generative process. Therefore, one may represent the latent space with a discrete product space z =
(z1, ..., zn), where zi ∈ {1, ..., k}. We note that Theorem 1 holds without any change, ifwe match
the structures of θ(x, z), φ(x, z) and γ(z), i.e., we have an independent Gumbel random variable
for each z = (z1, ..., zn). However, this may be computationally inefficient, since the number of
Gumbel random variables in this case is exponential in n. The gradient in Theorem 1 requires
two arg max-perturbations: zφ+γ that requires the encoder output and zθ+φ+γ that requires both
the encoder and the decoder log probability. Similarly to Maddison et al. (2016) and Jang et al.
(2016) we use a conditionally independent encoder, i.e., φ(x, z) = in=1 φi(x, zi) for which we can
compute zφ+γ efficiently. In contrast to their approach, we use an approximation for the decoder
log probability in order approximate z θ+φ+γ efficiently.
In discrete product spaces we use low-dimensional random perturbation γ(z) = in=1 γi (zi).
Thus instead of exponential number of random variables we use a linear number of random vari-
ables (linear in k and n). To compute zφ+γ efficiently, we first note that the φ(x, z) decom-
poses according to its dimensions, i.e., φ(x, z) = in=1 φi (x, zi). Thus the perturb-max argument
zφ+γ = (zφ+γ,…，zφ+γ) also decomposes, i.e., zφ+γ = argmaxz=ι,...,k{φi(x, z)+ γi(Z)}. There-
fore, our approach is exact for the arg max-perturbation zφ+γ
Our approximation is not necessarily exact for the arg max-perturbation zθ+φ+γ since the decoder
does not decompose according to its dimensions. To be able to compute zθ+φ+γ efficiently, we
use the fact we can compute zφ+γ efficiently and apply a low dimensional approximation for the
decoder log probability θ(x, Z) = Pi=1 θi(zi), where θi(Zi) = θ(zγ1+φ1,…,Zi,…,z7n+φn). With
this in mind, we approximate zθ+φ+γ using zθ+φ+γ , and its coordinates are computed efficiently
by zJθ+φ+7 = argmax^=ι,...,k{eθ√x,Zi) + φi(x,Z) + Yi(Z)}.
6
Under review as a conference paper at ICLR 2019
	accuracy		bound	
#labels	direct	GSM	direct	GSM
-50-	95.51%	91.78%	104.15	104.75
100	95.71%	94.79%	99.88	100.17
300	95.94%	95.04%	99.89	100.08
600	96.31%	95.53%	100.21	100.38
Figure 3: semi-supervised VAE on MNIST with 50/100/300/600 labeled examples out of the
50, 000 training examples. Direct loss minimization combined with VAE improves the performance
even with weak supervision, e.g., with only 50 examples direct loss minimization with VAE achieves
better accuracy than semi-supervised Gumbel-Softmax VAE.
Lastly, the KL-divergence in Equation (1) can utilize the decomposed encoder φ(x, z) =
pn=ι φi(x,z,. In this case q(z∣x) H eφ(x,z) and q(z∕x) H eφi(xjzi), therefore q(z∣x) =
Qn=1 q(z∣x) and KL(qφ(z∣x)∣∣pθ(Z)) = Pn=I KL(qφi(zi∣x)∣∣Pθ(Zi)), where Pθ(Zi) is the
marginal probability ofpθ(z) with respect to its i-th entry.
6	Experiments
We start by comparing our method to the state-of-the-art (Maddison et al., 2016; Jang et al., 2016;
Tucker et al., 2017; Grathwohl et al., 2017). We also investigate our approximation for discrete
product spaces. We conclude with a set of experiments that demonstrate the effectiveness of our
approach in semi-supervised learning of VAEs and the importance of weak supervision in image
generation.
6.1	Discrete VAEs
We begin our experiments by comparing the test loss bound of our direct optimization biased gradi-
ent estimator with the biased gradient estimator Gumbel-Softmax (GSM) of Maddison et al. (2016)
and Jang et al. (2016), and the unbiased estimators REBAR (Tucker et al., 2017) and RELAX (Grath-
wohl et al., 2017). We performed these experiments using the binarized MNIST dataset (Salakhut-
dinov & Murray, 2008), and the standard 50,000/10,000 split into training/testing sets. Following
Jang et al. (2016) we set our learning rate to 1e- 3 and the annealing rate to 1e- 5 and we used
their annealing schedule every 1000 step, setting the minimal to be 0.1. When considering a latent
space with n random variables Z = (Z1, ..., Zn ) where Zi ∈ {1, ..., k}, our encoder has a (784, n × k)
linear layer and the decoder has a (n × k, 784) linear layer. We applied REBAR and RELAX only
to discrete binary random variables2. The results appear in Figure 2. For the 20 × 2 and 20 × 10
architectures, we applied our approximation that is described in Section 5.2.
Next, we explore our discrete product space approximation. We compare a latent space with n
random variables Z = (Z1, ..., Zn ) with Zi ∈ {1, ..., k} with a latent space that has a single random
variable Z ∈ {1, ..., kn }. When n = 3, k = 2, a discrete VAE with a single categorial random
variable that has 23 possible values achieved a test log-loss bound of 168.57, while 3 binary variables
achieved test log-loss bound of 177.9. When n = 6, k = 2, a single categorial random variable
achieved a test log-loss bound of 147.59, while 6 binary variables achieved test log-loss bound of
161.09. One can see that there is a gap when using our approximation that increases with n.
The main advantage of our framework is that it seamlessly integrates semi-supervised learning. For
these experiments, we used a mixed continuous discrete architecture, where the architecture of the
encoder consists of a network φ which has a (784, 400) linear layer, followed by a ReLU, and a
(400, 200) linear layer. The output of this later is fed both to a discrete encoder φd and a continuous
encoder φc. The discrete latent space consists of Zd ∈ {1, ..., 10} and its encoder φd consists of a
(200, 100) linear layer, a ReLU, and a (100, 10) linear layer. The continuous latent space considers
k = 10, c = 20, and its encoder φc consists of a (200, 100) linear layer, a (100, 66) linear layer
followed by ReLU and dropout and a (66, 40) linear layer to estimate the mean and variance of
c-dimensional Gaussian random variables Z1, ..., Zk.
2For REBAR and RELAX we used the code in https://github.com/duvenaud/relax.
7
Under review as a conference paper at ICLR 2019
woman
w/o glasses
glasses
man
woman
man
w/o
smile
smile
w/o
smile
smile
w/o
smile
smile w/o smile
smile
Figure 4: Learning attribute representation in CelebA, using our semi-supervised setting, by cali-
brating our arg max prediction using a loss function. These images here are generated while setting
their attributes to get the desired image. The i-th row consists the generation of the same continuous
latent variable for all the attributes
Following Kingma et al. (2014), we conducted a quantitive experiment with weak supervision on
MNIST with 50/100/300/600 labeled examples out of the 50, 000 training examples. For labeled
examples, We set the perturbed label zeθ+φ+γ+' to be the true label. This is equivalent to using
the indicator function over the space of correct predictions. A comparison of our method with
Gumbel-Softmax appears in Figure 3. We can see that direct loss minimization combined with
VAE improves the performance even with weak supervision, e.g., with only 50 examples direct loss
minimization with VAE achieves better accuracy than semi-supervised Gumbel-Softmax VAE. We
note that we cannot compare the objective function of both methods, as our objective considers
direct loss minimization, however, we are able to compare the test log-loss bound, see Figure 3.
Supervision in generative models also greatly helps to control discrete semantics within images.
We learn to generate images using k = 8 discrete attributes of the CelebA dataset (cf. Liu et al.
(2015)) while using our semi-supervised VAE. For this task, we use convolutional layers for both
the encoder and the decoder, except the last two layers of the continuous latent model which are
linear layers that share parameters over the 8 possible representations of the image. In Figure 4, we
show generated images with discrete semantics turned on/off (with/without glasses, with/without
smile, woman/man).
7	Discussion and future work
In this work, we use the Gumbel-Max trick to reparameterize discrete VAEs using the arg max
prediction operator and show how to propagate gradients through the non-differentiable arg max
function. We show that this approach compares favorably to state-of-the-art methods, and extend it
to semi-supervised learning and image attribute generation.
These results can be taken in a number of different directions. Our gradient estimation is prac-
tically biased, while REINFORCE is an unbiased estimator. Our methods may benefit from the
REBAR/RELAX framework, which directs biased gradients towards the unbiased gradient (Tucker
et al., 2017). There are also open problems when fitting this approach to structured latent spaces
(Mena et al., 2018; Jin et al., 2018), as well as estimating its KL-divergence (Roeder et al., 2017).
There are also optimization-related questions that arise from our work: the interplay of and the
learning rate is unexplored and might be correlated. The number of stochastic gradient steps, inter-
leaving Gumbel perturbation with batch samples, might also benefit from a rigorous investigation.
The direct optimization approach we present is general and may be applied beyond VAEs, including
reinforcement learning and attention models. Further investigation in this direction is required.
8
Under review as a conference paper at ICLR 2019
References
D. P. Bertsekas, A. NediC, and A. E. Ozdaglar. ConvexAnalysis and Optimization. Athena Scientific,
2003.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal OftheAmerican Statistical Association, 112(518):859-877, 2017.
SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hin-
ton, et al. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in
Neural Information Processing Systems, pp. 3225-3233, 2016.
G.B. Folland. Real analysis: Modern techniques and their applications, john wiley & sons. New
York, 1999.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint
arXiv:1711.00123, 2017.
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation
for stochastic neural networks. arXiv preprint arXiv:1511.05176, 2015.
E.J. Gumbel and J. Lieblein. Statistical theory of extreme values and some practical applications: a
series of lectures, volume 33. US Govt. Print. Office, 1954.
T. Hazan, S. Maji, and T. Jaakkola. On sampling from the gibbs distribution with random maximum
a-posteriori perturbations. NIPS, 2013.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward controlled
generation of text. In International Conference on Machine Learning, pp. 1587-1596, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. arXiv preprint arXiv:1802.04364, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems,
pp. 3581-3589, 2014.
S. Kotz and S. Nadarajah. Extreme value distributions: theory and applications. World Scientific
Publishing Company, 2000.
Matt J Kusner, Brooks Paige, and Jose MigUel Hernandez-Lobato. Grammar variational autoen-
coder. arXiv preprint arXiv:1703.01925, 2017.
Dieterich Lawson, George Tucker, Chung-Cheng Chiu, Colin Raffel, Kevin Swersky, and Navdeep
Jaitly. Learning hard alignments with variational inference. arXiv preprint arXiv:1705.05524,
2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 3730-3738, 2015.
R.D. Luce. Individual choice behavior. 1959.
Chris Maddison, Danny Tarlow, and Tom Minka. A* sampling. In Advances in Neural Information
Processing Systems, pp. 2085-2093, 2014.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
9
Under review as a conference paper at ICLR 2019
D. McAllester, T. Hazan, and J. Keshet. Direct loss minimization for structured prediction. Advances
in Neural Information Processing Systems, 23:1594-1602, 2010.
D. McFadden. Conditional logit analysis of qualitative choice behavior. 1973.
Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permutations
with gumbel-sinkhorn networks. arXiv preprint arXiv:1802.08665, 2018.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv
preprint arXiv:1402.0030, 2014.
Andriy Mnih and Danilo J Rezende. Variational inference for monte carlo objectives. arXiv preprint
arXiv:1602.06725, 2016.
Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. arXiv preprint arXiv:1703.04908, 2017.
John Paisley, David Blei, and Michael Jordan. Variational bayesian inference with stochastic search.
arXiv preprint arXiv:1206.6430, 2012.
G. Papandreou and A. Yuille. Perturb-and-map random fields: Using discrete optimization to learn
and sample from energy models. In ICCV, Barcelona, Spain, November 2011. doi: 10.1109/
ICCV.2011.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial
Intelligence and Statistics, pp. 814-822, 2014.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Geoffrey Roeder, Yuhuai Wu, and David K Duvenaud. Sticking the landing: Simple, lower-variance
gradient estimators for variational inference. In Advances in Neural Information Processing Sys-
tems, pp. 6925-6934, 2017.
Jason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200, 2016.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In
Proceedings of the 25th international conference on Machine learning, pp. 872-879. ACM, 2008.
Dinghan Shen, Qinliang Su, Paidamoyo Chapfuwa, Wenlin Wang, Guoyin Wang, Lawrence Carin,
and Ricardo Henao. Nash: Toward end-to-end neural architecture for generative semantic hash-
ing. arXiv preprint arXiv:1805.05361, 2018.
Y. Song, A. G. Schwing, R. Zemel, and R. Urtasun. Training Deep Neural Networks via Direct Loss
Minimization. In Proc. ICML, 2016.
D. Tarlow, R.P. Adams, and R.S. Zemel. Randomized optimum models for structured prediction. In
AISTATS, pp. 21-23, 2012.
Michalis K Titsias. Local expectation gradients for doubly stochastic variational inference. arXiv
preprint arXiv:1503.01494, 2015.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar:
Low-variance, unbiased gradient estimates for discrete latent variable models. In Advances in
Neural Information Processing Systems, pp. 2624-2633, 2017.
Arash Vahdat, William G Macready, Zhengbing Bian, and Amir Khoshaman. Dvae++: Discrete
variational autoencoders with overlapping transformations. arXiv preprint arXiv:1802.04920,
2018.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. In Reinforcement Learning, pp. 5-32. Springer, 1992.
Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. Learning to
compose words into sentences with reinforcement learning. arXiv preprint arXiv:1611.09100,
2016.
10
Under review as a conference paper at ICLR 2019
20 × 2
20 × 10
120
no
IOO
Figure 5: Test log-loss bound of discrete VAE with n × k binary units as a function of training
epochs.
160
150
140
130
Figure 6: comparing direct and GSM for deep decoder and encoder over discrete product spaces
z = (z1, ..., zn) where each zi ∈ {1, ..., k} and nk = 60. The encoder is built over a 28 × 28
image and has four layers (including input) of (784, 400, 200, 60) units respectively. The decoder
architecture has four layers with (60, 200, 400, 784) units respectively, including the 28 × 28 output
layer. Between each layer we have a ReLU function and the output layer used tanh to calibrate the
pixel value. The experiment is performed over non-binarized MNIST with the MSE loss.
A Gumbel-Max perturbation model and the Gibbs distribution
Theorem 2. Gumbel & Lieblein (1954); Luce (1959); McFadden (1973) Letγ be a random function
that associates random variable γ(z) for each z = 1, ..., k whose distribution follows the zero mean
Gumbel distribution law, i.e., its probability density function is g(t) = e-(t+c+e-(t+c)) for the Euler
constant c ≈ 0.57. Then
eφ(x,z)
P eφ(χ,z)
PY〜g[z = zφ+γ], where zφ+γ = arg max {φ(x,Z)+ Y(Z)}
,"	z=1,...,k
(12)
11
Under review as a conference paper at ICLR 2019
S 5 5 3 8 ʃ 5
7 Λ/ 7 7 7 7 7
■ O，C。6 6
c∖σ- g 2 q 夕 q
“¼y ：TVyq
ζ^'∙∖>FOO /<p5^
33 3 333z
QJaacΛn2
l> H // l∙ I— ∕∙
J 4 6 G 4 b b
q 9 q 夕 q 9 4
COgCo ? g g q
7∙77Γ-77r-
6 ∖0G4 6 4，c
55，5 5 57
H √ H y ¥ / V
33>a 3 3 3 3
2 nə/ NJ7
/ I √z I Il ——
4 0 0 0 0 0 0
Figure 7: Comparing unsupervised to semi-supervised VAE on MNIST, for which the discrete la-
tent variable has 10 values, i.e., z ∈ {1, ..., 10}. One can see that semi-supervised helps the VAE
to capture the class information and consequently improve the image generation process. Left: gen-
erated images from an unsupervised VAE. Right: generated images from semi-supervised mixture
model. The j-th column consists of images generated for the j-th discrete class. The i-th row
consists the generation of the same continuous latent variable for all the 10 classes. One can see that
the discrete latent space is able to learn the class representation. The continuous latent space learns
the variability within the class. Comparing to unsupervised image generation, we observe that some
supervision improves the image generation per class.
Proof. Let G(t) = e-e-(t+c) be the Gumbel cumulative distribution function. Then
PY 〜g [z = zφ+γ]
PY〜g[z ∈ arg max {φ(x,Z)+ γ(z)}]
z z	z=1,..,k
J g(t — φ(x,z)) Y G(t — φ(x, Z))dt
z=z
Since g(t) = e-(t+c)G(t) it holds that
g(t — φ(z)) Y G(t — φ(z')')dt
Z=Z
/
Z=z
eφ(x,z)
Z
(13)
(14)
(15)
where 十=R e-(t+c) Qk=I G(t — φ(z))dt is independent of z. Since PY〜g [z = zφ+γ] is a distri-
bution then Z must equal to Pk=I eφ(x,z).	□
12
Under review as a conference paper at ICLR 2019
Figure 8: Extending Figure 4. The i-th row consists the generation of the same continuous latent
variable for all the attributes.
13