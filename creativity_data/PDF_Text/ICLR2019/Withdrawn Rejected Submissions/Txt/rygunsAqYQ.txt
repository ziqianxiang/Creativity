Under review as a conference paper at ICLR 2019
Implicit Maximum Likelihood Estimation
Anonymous authors
Paper under double-blind review
Ab stract
Implicit probabilistic models are models defined naturally in terms of a sampling
procedure and often induces a likelihood function that cannot be expressed explic-
itly. We develop a simple method for estimating parameters in implicit models that
does not require knowledge of the form of the likelihood function or any derived
quantities, but can be shown to be equivalent to maximizing likelihood under some
conditions. Our result holds in the non-asymptotic parametric setting, where both
the capacity of the model and the number of data examples are finite. We also
demonstrate encouraging experimental results.
1 Introduction
Generative modelling is a cornerstone of machine learning and has received increasing attention.
Recent models like variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al.,
2014) and generative adversarial nets (GANs) (Goodfellow et al., 2014; Gutmann et al., 2014), have
delivered impressive advances in performance and generated a lot of excitement.
Generative models can be classified into two categories: prescribed models and implicit mod-
els (Diggle & Gratton, 1984; Mohamed & Lakshminarayanan, 2016). Prescribed models are de-
fined by an explicit specification of the density, and so their unnormalized complete likelihood can
be usually expressed in closed form. Examples include models whose complete likelihoods lie in
the exponential family, such as mixture of Gaussians (Everitt, 1985), hidden Markov models (Baum
& Petrie, 1966), Boltzmann machines (Hinton & Sejnowski, 1986). Because computing the normal-
ization constant, also known as the partition function, is generally intractable, sampling from these
models is challenging.
On the other hand, implicit models are defined most naturally in terms of a (simple) sampling pro-
cedure. Most models take the form of a deterministic parameterized transformation Tθ(∙) of an
analytic distribution, like an isotropic Gaussian. This can be naturally viewed as the distribution
induced by the following sampling procedure:
1.	Sample Z 〜N(0,I)
2.	Return x := Tθ (z)
The transformation T⅛(∙) often takes the form of a highly expressive function approximator, like a
neural net. Examples include generative adversarial nets (GANs) (Goodfellow et al., 2014; Gutmann
et al., 2014) and generative moment matching nets (GMMNs) (Li et al., 2015; Dziugaite et al., 2015).
The marginal likelihood of such models can be characterized as follows:
pθ(x)
∂Xι …∂Xd Li (Tθ(z))i≤xi} φ(z)dz
where φ(∙) denotes the probability density function (PDF) of N(0, I).
In general, attempting to reduce this to a closed-form expression is hopeless. Evaluating it numeri-
cally is also challenging, since the domain of integration could consist of an exponential number of
disjoint regions and numerical differentiation is ill-conditioned.
These two categories of generative models are not mutually exclusive. Some models admit both
an explicit specification of the density and a simple sampling procedure and so can be considered
as both prescribed and implicit. Examples include variational autoencoders (Kingma & Welling,
1
Under review as a conference paper at ICLR 2019
2013; Rezende et al., 2014), their predecessors (MacKay, 1995; Bishop et al., 1998) and exten-
sions (Burda et al., 2015), and directed/autoregressive models, e.g., (Neal, 1992; Bengio & Bengio,
2000; Larochelle & Murray, 2011; van den Oord et al., 2016).
1.1	Challenges in Parameter Estimation
Maximum likelihood (Fisher, 1912; Edgeworth, 1908) is perhaps the standard method for estimating
the parameters of a probabilistic model from observations. The maximum likelihood estimator
(MLE) has a number of appealing properties: under mild regularity conditions, it is asymptotically
consistent, efficient and normal. A long-standing challenge of training probabilistic models is the
computational roadblocks of maximizing the log-likelihood function directly.
For prescribed models, maximizing likelihood directly requires computing the partition function,
which is intractable for all but the simplest models. Many powerful techniques have been devel-
oped to attack this problem, including variational methods (Jordan et al., 1999), contrastive diver-
gence (Hinton, 2002; Welling & Hinton, 2002), score matching (Hyvarinen, 2005) and PseUdolike-
lihood maximization (Besag, 1975), among others.
For imPlicit models, the sitUation is even worse, as there is no term in the log-likelihood fUnction
that is in closed form; evalUating any term reqUires comPUting an intractable integral. As a resUlt,
maximizing likelihood in this setting seems hoPelessly difficUlt. A variety of likelihood-free solU-
tions have been ProPosed that in effect minimize a divergence measUre between the data distribUtion
and the model distribUtion. They come in two forms: those that minimize an f -divergence, and those
that minimize an integral probability metric (Muller, 1997). In the former category are GANs, which
are based on the idea of minimizing the distingUishability between data and samPles (TU, 2007; GUt-
mann & Hyvarinen, 2010). It has been shown that when given access to an infinitely powerful
discriminator, the original GAN objective minimizes the Jensen-Shannon divergence, the - log D
variant of the objective minimizes the reverse KL-divergence minus a bounded quantity (Arjovsky &
Bottou, 2017), and later extensions (Nowozin et al., 2016) minimize arbitrary f -divergences. In the
latter category are GMMNs which use maximum mean discrepancy (MMD) (Gretton et al., 2007)
as the witness function.
In the case of GANs, despite the theoretical results, there are a number of challenges that arise in
practice, such as mode dropping/collapse (Goodfellow et al., 2014; Arora & Zhang, 2017), vanishing
gradients (Arjovsky & Bottou, 2017; Sinn & Rawat, 2017) and training instability (Goodfellow et al.,
2014; Arora et al., 2017). A number of explanations have been proposed to explain these phenomena
and point out that many theoretical results rely on three assumptions: the discriminator must have
infinite modelling capacity (Goodfellow et al., 2014; Arora et al., 2017), the number of samples
from the true data distribution must be infinite (Arora et al., 2017; Sinn & Rawat, 2017) and the
gradient ascent-descent procedure (Arrow et al., 1958; Schmidhuber, 1992) can converge to a global
pure-strategy Nash equilibrium (Goodfellow et al., 2014; Arora et al., 2017). When some of these
assumptions do not hold, the theoretical guarantees do not necessarily apply. A number of ways
have been proposed that alleviate some of these issues, e.g., (Zhao et al., 2016; Salimans et al.,
2016; Donahue et al., 2016; Dumoulin et al., 2016; Arjovsky et al., 2017; Hjelm et al., 2017; Li
et al., 2017; Zhu et al., 2017), but a way of solving all three issues simultaneously remains elusive.
1.2	Our Contribution
In this paper, we present an alternative method for estimating parameters in implicit models. Like
the methods above, our method is likelihood-free, but can be shown to be equivalent to maximizing
likelihood under some conditions. Our result holds when the capacity of the model is finite and
the number of data examples is finite. The idea behind the method is simple: it finds the nearest
sample to each data example and optimizes the model parameters to pull the sample towards it. The
direction in which nearest neighbour search is performed is important: the proposed method ensures
each data example has a similar sample, which contrasts with an alternative approach of pushing
each sample to the nearest data example, which would ensure that each sample has a similar data
example. The latter approach would permit all samples being similar to one data example. Such a
scenario would be heavily penalized by the former approach.
2
Under review as a conference paper at ICLR 2019
The proposed method could sidestep the three issues mentioned above: mode collapse, vanishing
gradients and training instability. Modes are not dropped because the loss ensures each data example
has a sample nearby at optimality; gradients do not vanish because the gradient of the distance
between a data example and its nearest sample does not become zero unless they coincide; training
is stable because the estimator is the solution to a simple minimization problem. By leveraging
recent advances in fast nearest neighbour search algorithms (Li & Malik, 2016; 2017), this approach
is able to scale to large, high-dimensional datasets.
2 Implicit Maximum Likelihood Estimator
2.1 Definition
We are given a set of n data examples x1 , . . . , xn and some unknown parameterized probability
distribution Pθ with density pθ. We also have access to an oracle that allows us to draw independent
and identically distributed (i.i.d.) samples from Pθ .
Let χi,..., Xm be i.i.d. samples from Pq, where m ≥ n. For each data example x^ We define a
random variable Riθ to be the distance between xi and the nearest sample. More precisely,
Rθ=jmmjxθ-χiR
j ∈[m]
where [m] denotes {1, . . . , m}.
The implicit maximum likelihood estimator θIMLE is defined as:
n
θIMLE := argmin ERθ ,...,Rn ER
i=1
n
=argminEχθ,…,χm Xjm∈in] ι∣χθ -χi∣ι2
2.2 Algorithm
We outline the proposed parameter estimation procedure in Algorithm 1. In each outer iteration, we
draw m i.i.d. samples from the current model Pθ. We then randomly select a batch of examples from
the dataset and find the nearest sample from each data example. We then run a standard iterative
optimization algorithm, like stochastic gradient descent (SGD), to minimize a sample-based version
of the Implicit Maximum Likelihood Estimator (IMLE) objective.
Algorithm 1 Implicit maximum likelihood estimation (IMLE) procedure
Require: The dataset D = {χi}in=1 and a sampling mechanism for the implicit model Pθ
Initialize θ to a random vector
for k = 1 to K do
Draw i.i.d. samples X，，...，Xm from Pq
Pick a random batch S ⊆ {1, . . . , n}
σ(i) - argminj ∣∣Xi - XQ∣∣2 ∀i ∈ S
for l = 1 to L do
Pick a random mini-batch S ⊆ S
θ J θ - ηNθ (,S| Pi∈S ∣∣Xi-君⑴ ∣∣2)
end for
end for
return θ
Because our algorithm needs to solve a nearest neighbour search problem in each outer iteration, the
scalability of our method depends on our ability to find the nearest neighbours quickly. This was
traditionally considered to be a hard problem, especially in high dimensions. However, this is no
3
Under review as a conference paper at ICLR 2019
longer the case, due to recent advances in nearest neighbour search algorithms (Li & Malik, 2016;
2017).
Note that the use of Euclidean distance is not a major limitation of the proposed approach. A variety
of distance metrics are either exactly or approximately equivalent to Euclidean distance in some
non-linear embedding space, in which case the theoretical guarantees are inherited from the Eu-
clidean case. This encompasses popular distance metrics used in the literature, like the Euclidean
distance between the activations of a neural net, which is often referred to as a perceptual similarity
metric (Salimans et al., 2016; Dosovitskiy & Brox, 2016). The approach can be easily extended to
use these metrics, though because this is the initial paper on this method, we focus on the vanilla
setting of Euclidean distance in the natural representation of the data, e.g.: pixels, both for sim-
plicity/clarity and for comparability to vanilla versions of other methods that do not use auxiliary
sources of labelled data or leverage domain-specific prior knowledge. For distance metrics that can-
not be embedded in Euclidean space, the analysis can be easily adapted with minor modifications as
long as the volume of a ball under the metric has a simple dependence on its radius.
3	Why Maximum Likelihood
There has been debate (Huszar, 2015) over whether maximizing likelihood of the data is the appro-
priate objective for the purposes of learning generative models. Recall that maximizing likelihood
is equivalent to minimizing DKL (pdata kpθ ), where pdata denotes the empirical data distribution
and pθ denotes the model distribution. One proposed alternative is to minimize the reverse KL-
divergence, DKL (pθ Ilpdata), which is suggested (HUszar, 2015) to be better because it severely
penalizes the model for generating an implausible sample, whereas the standard KL-divergence,
DKL (pdata Ipθ ), severely penalizes the model for assigning low density to a data example. As a
result, when the model is underspecified, i.e. has less capacity than what’s necessary to fit all the
modes of the data distribution, minimizing DKL (pθ Ipdata ) leads to a narrow model distribution
that concentrates around a few modes, whereas minimizing DKL (pdata Ipθ ) leads to a broad model
distribution that hedges between modes. The success of GANs in generating good samples is often
attributed to the former phenomenon (Arjovsky & Bottou, 2017).
This argument, however, relies on the assumption that we have access to an infinite number of
samples from the true data distribution. In practice, however, this assumption rarely holds: if we
had access to the true data distribution, then there is usually no need to fit a generative model, since
we can simply draw samples from the true data distribution. What happens when we only have
the empirical data distribution? Recall that DKL (p Iq) is defined and finite only ifp is absolutely
continuous w.r.t. q, i.e.: q(x) = 0 implies p(x) = 0 for all x. In other words, DKL (p Iq) is defined
and finite only if the support of p is contained in the support of q . Now, consider the difference
between DKL (pdata Ipθ ) and DKL (pθ Ipdata ): minimizing the former, which is equivalent to
maximizing likelihood, ensures that the support of the model distribution contains all data examples,
whereas minimizing the latter ensures that the support of the model distribution is contained in the
support of the empirical data distribution, which is just the set of data examples. In other words,
maximum likelihood disallows mode dropping, whereas minimizing reverse KL-divergence forces
the model to assign zero density to unseen data examples and effectively prohibits generalization.
Furthermore, maximum likelihood discourages the model from assigning low density to any data
example, since doing so would make the likelihood, which is the product of the densities at each of
the data examples, small.
From the modelling perspective, because maximum likelihood is guaranteed to preserve all modes,
it can make use of all available training data and can therefore be used to train high-capacity models
that have a large number of parameters. In contrast, using an objective that permits mode dropping
allows the model to pick and choose which data examples it wants to model. As a result, if the goal
is to train a high-capacity model that can learn the underlying data distribution, we would not be able
to do so using such an objective because we have no control over which modes the model chooses
to drop. Put another way, we can think about the model’s performance along two axes: its ability
to generate plausible samples (precision) and its ability to generate all modes of the data distribu-
tion (recall). A model that successfully learns the underlying distribution should score high along
both axes. If mode dropping is allowed, then an improvement in precision may be achieved at the
expense of lower recall and could represent a move to a different point on the same precision-recall
4
Under review as a conference paper at ICLR 2019
curve. As a result, since sample quality is an indicator of precision, improvement in sample quality
in this setting may not mean an improvement in density estimation performance. On the other hand,
if mode dropping is disallowed, since full recall is always guaranteed, an improvement in precision
is achieved without sacrificing recall and so implies an upwards shift in the precision-recall curve.
In this case, an improvement in sample quality does signify an improvement in density estimation
performance, which may explain sample quality historically was an important way to evaluate the
performance of generative models, most of which maximized likelihood. With the advent of gener-
ative models that permit mode dropping, however, sample quality is no longer a reliable indicator of
density estimation performance, since good sample quality can be trivially achieved by dropping all
but a few modes. In this setting, sample quality can be misleading, since a model with low recall on a
lower precision-recall curve can achieve a better precision than a model with high recall on a higher
precision-recall curve. Since it is hard to distinguish whether an improvement in sample quality is
due to a move along the same precision-recall curve or a real shift in the curve, an objective that
disallows mode dropping is critical tool that researchers can use to develop better models, since they
can be sure that an apparent improvement in sample quality is due to a shift in the precision-recall
curve.
4	Analysis
Before formally stating the theoretical results, we first illustrate the intuition behind why the pro-
posed estimator is equivalent to maximum likelihood estimator under some conditions. For sim-
plicity, we will consider the special case where we only have a single data example X1 and a single
sample X,. Consider the total density of Pj inside a ball of radius of t centred at xι as a function of
t, a function that will be denoted as Fe(t). If the density in the neighbourhood of xι is high, then
Fe (t) would grow rapidly as t increases. If, on the other hand, the density in the neighbourhood
of xι is low, then Fθ(t) would grow slowly. So, maximizing likelihood is equivalent to making
Fe(t) grow as fast as possible. To this end, we can maximize the area under the function Fθ(t),
or equivalently, minimize the area under the function 1 一 Fe(t). Observe that Fθ(t) can be inter-
preted as the cumulative distribution function (CDF) of the Euclidean distance between x1 and Xe,
which is a random variable because X, is random and will be denoted as R. Because R is non-
negative, recall that E [Re] = R∞ Pr (Re > t) dt = R∞(1 一 Fe (t)) dt, which is exactly the
area under the function 1 一 Fe(t). Therefore, we can maximize likelihood of a data example xi by
minimizing E [Re], or in other words, minimizing the expected distance between the data example
and a random sample. To extend this analysis to the case with multiple data examples, we show
in the supplementary material that if the objective function is a summation, applying a monotonic
transformation to each term and then reweighting appropriately preserves the optimizer under some
conditions.
We now state the key theoretical result formally. Please refer to the supplementary material for the
proof.
Theorem 1. Consider a set of observations x1, . . . , xn, a parameterized family of distributions
Pθ with probability density function (PDF) pθ(∙) and a unique maximum likelihood solution θ*.
For any m ≥ 1, let X,，...，Xm 〜Pθ be i.i.d. random variables and define rθ := |区1八2, R :=
minj∈[m] 11 Xj 112 and R := min7-∈[m] ||xj 一 Xi∣∣2. Let Fθ (∙) be the cumulative distributionfunction
(CDF)Of rθ and Ψ(z) := minj {E [Rθ] |pj (0) = Z }.
If Pθ satisfies the following:
•	pe (X) is differentiable w.r.t. θ and continuous w.r.t. X everywhere.
•	∀θ, V, there exists θ0 such that Pe (x) = pe,(X + V) ∀x.
•	For any θι,θ2, there exists θŋ such that Fe0 (t) ≥ max {Fe1(t),Fe2(t)} ∀t ≥ 0 and
pe0 (0) = max {pe1 (0)， pe2 (0)}.
5
Under review as a conference paper at ICLR 2019
Method	MNIST	TFD
DBN (Bengio et al., 2013)	138±2	1909 ± 66
SCAE (Bengio et al., 2013)	121 ± 1.6	2110 ± 50
DGSN (Bengio et al., 2014)	214 ± 1.1	1890 ± 29
GAN (Goodfellow et al., 2014)	225 ± 2	2057 ± 26
GMMN (Li et al., 2015)	147±2	2085 ± 25
IMLE (Proposed)	257±6	2139 ± 27
Table 1: Log-likelihood of the test data under the Gaussian Parzen window density estimated from
samples generated by different methods.
•	∃τ > 0 such that ∀i ∈ [n] ∀θ ∈ Bθ* (τ), pθ (Xi) < pθ*(xj, where Bθ* (τ) denotes the ball
centred at θ* ofradius τ.
•	Ψ(z) is differentiable everywhere.
∈
• For
all	θ,	if θ
ψ0(Pθ (XI))Pθ (XI)	∖
Ψ0(Pθ* (xι))Pθ* (xι)
.
.
.
Ψ0(pθ (Xn))Pθ (Xn)
Ψ0(Pθ* (Xn))Pθ* (Xn ) /
= θ*, there exists j
(Vθ (logPθ(xι))j ∖∖
.))=0.
∖ Vθ (logPθ(Xn))j ) /
[d]	such that
Then,
. XX	E [Rθ]	X]	/、
argmin ⅛ ψ0(pθ* (Xi))pθ* (Xi) = argmax 工log Pp(Xi)
Furthermore, if pp* (xi) = .…=pp* (Xn), then,
nn
arg min E Riθ = arg max	logPθ(Xi)
θ i=1	θ i=1
Now, we examine the restrictiveness of each condition. The first condition is satisfied by nearly
all analytic distributions. The second condition is satisfied by nearly all distributions that have an
unrestricted location parameter, since one can simply shift the location parameter by v. The third
condition is satisfied by most distributions that have location and scale parameters, like a Gaussian
distribution, since the scale can be made arbitrarily low and the location can be shifted so that the
constraint on pp(∙) is satisfied. The fourth condition is satisfied by nearly all distributions, whose
density eventually tends to zero as the distance from the optimal parameter setting tends to infinity.
The fifth condition requires minp {E [Rθ] |pp (0) = z } to change smoothly as Z changes. The final
condition requires the two n-dimensional vectors, one of which can be chosen from a set of d vectors,
to be not exactly orthogonal. As a result, this condition is usually satisfied when d is large, i.e. when
the model is richly parameterized.
There is one remaining difficulty in applying this theorem, which is that the quantity
1∕Ψ0(pθ* (Xi))pθ* (Xi), which appears as an coefficient on each term in the proposed objective, is
typically not known. If we consider a new objective that ignores the coefficients, i.e. Pin=1 E Riθ ,
then minimizing this objective is equivalent to minimizing an upper bound on the ideal objective,
Pin=1 E Riθ ∕Ψ0 (Pθ* (Xi))Pθ* (Xi). The tightness of this bound depends on the difference between
the highest and lowest likelihood assigned to individual data points at the optimum, i.e. the max-
imum likelihood estimate of the parameters. Such a model should not assign high likelihoods to
some points and low likelihoods to others as long as it has reasonable capacity, since doing so would
make the overall likelihood, which is the product of the likelihoods of individual data points, low.
Therefore, the upper bound is usually reasonably tight.
6
Under review as a conference paper at ICLR 2019
M审尊于寓留H皂画W营百
isiIB
需B勰勰繇Ii焉费暴懦
Bilii
Ili
IiKIi
舞器瑞智修器l≡≡l^5sl
帛MIh第»内而可害韦∙∙∙y≡
iilis
更亨也可由“

(a) MNIST	(b) TFD	(c) CIFAR-10
Figure 1: Representative random samples from the model trained on (a) MNIST, (b) Toronto Faces
Dataset and (c) CIFAR-10.
5 Experiments
We trained generative models using the proposed method on three standard benchmark datasets,
MNIST, the Toronto Faces Dataset (TFD) and CIFAR-10. All models take the form of feedforward
neural nets with isotropic Gaussian noise as input.
For MNIST, the architecture consists of two fully connected hidden layers with 1200 units each
followed by a fully connected output layer with 784 units. ReLU activations were used for hidden
layers and sigmoids were used for the output layer. For TFD, the architecture is wider and consists
of two fully connected hidden layers with 8000 units each followed by a fully connected output layer
with 2304 units. For both MNIST and TFD, the dimensionality of the noise vector is 100.
For CIFAR-10, we used a simple convolutional
architecture with 1000-dimensional Gaussian
noise as input. The architecture consists of five
convolutional layers with 512 output channels
and a kernel size of 5 that all produce 4 × 4
feature maps, followed by a bilinear upsam-
pling layer that doubles the width and height
of the feature maps. There is a batch normal-
ization layer followed by leaky ReLU activa-
tions with slope -0.2 after each convolutional
layer. This design is then repeated for each
subsequent level of resolution, namely 8 × 8,
16× 16 and 32× 32, so that we have 20 convo-
lutional layers, each with output 512 channels.
We then add a final output layer with three out-
put channels on top, followed by sigmoid acti-
vations. We note that this architecture has more
capacity than typical architectures used in other
methods, like (Radford et al., 2015). This is be-
cause our method aims to capture all modes of
the data distribution and therefore needs more
modelling capacity than methods that are per-
mitted to drop modes.
Figure 2: Samples corresponding to the same
latent variable values at different points in time
while training the model on CIFAR-10. Each row
corresponds to a sample, and each column corre-
sponds to a particular point in time.
Evaluation for implicit generative models in
general remains an open problem. Various in-
trinsic and extrinsic evaluation metrics have been proposed, all of which have limitations. Extrinsic
evaluation metrics measure performance indirectly via a downstream task (Salimans et al., 2016).
Unfortunately, dependence on the downstream task could introduce bias and may not capture de-
7
Under review as a conference paper at ICLR 2019
444。446333333333
333333333333333888

G44444夕夕夕夕夕夕夕夕
夕夕夕夕夕夕夕夕夕夕夕777777777
777777777J3333333333
3 3 3 33555555S6GOOOOOO
qqqqqqqq4峪qggg夕夕夕夕夕夕
ffffF777777777777777
77777777777799SS5555
…一
OOOOOOOO
(b) TFD
IIi
≡ilii
G 3 Z需∙⅛EM
Iilli
工总富有等$百,内患❸⅜■百
E咨看西串得百？百电0声q.卜/M⅛
E官蟹百百？ Z寓£$师有
E君画琴翁？33丙修&M百
N富≡MB星帛百寓而卜$♦寺
I
ʧt⅞H
g9≡
E
I
一 i
Ss
Q :≡,J
口； 一IA
;』□
y⅛τl□
FE二 Q
二£三
HiB
三i=∙皿
;苜
(c) CIFAR-10
(a) MNIST
9 g q q
5 5 5 5 5 3
F f f f ʃ ʃ
3 3




L
≡ 1 Z


Figure 3: Linear interpolation between samples in code space. The first image in every row is
an independent sample; all other images are interpolated between the previous and the subsequent
sample. Images along the path of interpolation are shown in the figure arranged from from left to
right, top to bottom. They also wrap around, so that images in the last row are interpolations between
the last and first samples.
sirable properties of the generative model that do not affect performance on the task. Intrinsic
evaluation metrics measure performance without relying on external models or data. Popular exam-
ples include estimated log-likelihood (Bengio et al., 2014; Wu et al., 2016) and visual assessment
of sample quality. While recent literature has focused more on the latter and less on the former,
it should be noted that they evaluate different properties - sample quality reflects precision, i.e.:
how accurate the model samples are compared to the ground truth, whereas estimated log-likelihood
focuses on recall, i.e.: how much of the diversity in the data distribution the model captures. Conse-
quently, both are important metrics; one is not a replacement for the other. As pointed out by (Theis
et al., 2015), “qualitative as well as quantitative analyses based on model samples can be misleading
about a model’s density estimation performance, as well as the probabilistic model’s performance
in applications other than image synthesis.” Two models that achieve different levels of precision
may simply be at different points on the same precision-recall curve, and therefore may not be di-
rectly comparable. Models that achieve the same level of recall, on the other hand, may be directly
compared. So, for methods that maximize likelihood, which are guaranteed to preserve all modes
and achieve full recall, both sample quality and estimated log-likelihood capture precision. Because
most generative models traditionally maximized likelihood or a lower bound on the likelihood, the
only property that differed across models was precision, which may explain why sample quality
has historically been seen as an important indicator of performance. However, in heterogenous ex-
perimental settings with different models optimized for various objectives, sample quality does not
necessarily reflect how well a model learns the underlying data distribution. Therefore, under these
settings, both precision and recall need to be measured. While there is not yet a reliable way to
measure recall (given the known issues of estimated log-likelihoods in high dimensions), this does
not mean that sample quality can be a valid substitute for estimated log-likelihoods, as it cannot
detect the lack of diversity of samples. A secondary issue that is more easily solvable is that samples
presented in papers are sometimes cherry-picked; as a result, they capture the maximum sample
quality, but not necessarily the mean sample quality.
To mitigate these problems to some extent, we avoid cherry-picking and visualize randomly chosen
samples, which are shown in Figure 1. We also report the estimated log-likelihood in Table 1. As
mentioned above, both evaluation criteria have biases/deficiencies, so performing well on either
of these metrics does not necessarily indicate good density estimation performance. However, not
performing badly on either metric can provide some comfort that the model is simultaneously able
to achieve reasonable precision and recall.
As shown in Figure 1, despite its simplicity, the proposed method is able to generate reasonably good
samples for MNIST, TFD and CIFAR-10. While it is commonly believed that minimizing reverse
KL-divergence is necessary to produce good samples and maximizing likelihood necessarily leads
to poor samples (Grover et al., 2017), the results suggest that this is not necessarily the case. Even
8
Under review as a conference paper at ICLR 2019
though Euclidean distance was used in the objective, the samples do not appear to be desaturated or
overly blurry. Samples also seem fairly diverse. This is supported by the estimated log-likelihood
results in Table 1. Because the model achieved a high score on that metric on both MNIST and TFD,
this suggests that the model did not suffer from significant mode dropping.
In Figure 4 in the supplementary material, we show samples and their nearest neighbours in the
training set. Each sample is quite different from its nearest neighbour in the training set, suggesting
that the model has not overfitted to examples in the training set.
Next, we visualize the learned manifold by walking along a geodesic on the manifold between pairs
of samples. More concretely, we generate five samples, arrange them in arbitrary order, perform lin-
ear interpolation in latent variable space between adjacent pairs of samples, and generate an image
from the interpolated latent variable. As shown in Figure 3, the images along the path of interpo-
lation appear visually plausible and do not have noisy artifacts. In addition, the transition from one
image to the next appears smooth, including for CIFAR-10, which contrasts with findings in the
literature that suggest the transition between two natural images tends to be abrupt. This indicates
that the support of the model distribution has not collapsed to a set of isolated points and that the
proposed method is able to learn the geometry of the data manifold, even though it does not learn a
distance metric explicitly.
Finally, we illustrate the evolution of samples as training progresses in Figure 2. As shown, the
samples are initially blurry and become sharper over time. Importantly, sample quality consistently
improves over time, which demonstrates the stability of training.
While our sample quality may not be state-of-the-art, it is important to remember that these re-
sults are obtained under the setting of full recall. So, this does not necessarily mean that our method
models the underlying data distribution less accurately than other methods that achieve better sample
quality, as some of them may drop modes and therefore achieve less than full recall. As previously
mentioned, this does not suggest a fundamental tradeoff between precision and recall that cannot be
overcome - on the contrary, our method provides researchers with a way of designing models that
can improve the precision-recall curve without needing to worry that the observed improvements
are due to a movement along the curve. With refinements to the model, it is possible to move the
curve upwards and obtain better sample quality at any level of recall as a consequence. This is left
for future work; as this is the initial paper on this approach, its value stems from the foundation it
lays for a new research direction upon which subsequent work can be built, as opposed to the current
results themselves. For this paper, we made a deliberate decision to keep the model simple, since
non-essential practically motivated enhancements are less grounded in theory, may obfuscate the
key underlying idea and could impart the impression that they are critical to making the approach
work in practice. The fact that our method is able to generate more plausible samples on CIFAR-10
than other methods at similar stages of development, such as the initial versions of GAN (Goodfel-
low et al., 2014) and PixelRNN (van den Oord et al., 2016), despite the minimal sophistication of
our method and architecture, shows the promise of the approach. Later iterations of other methods
incorporate additional supervision in the form of pretrained weights and/or make task-specific mod-
ifications to the architecture and training procedure, which were critical to achieving state-of-the-art
sample quality. We do believe the question of how the architecture should be refined in the context
of our method to take advantage of task-specific insights is an important one, and is an area ripe for
future exploration.
6	Discussion
In this section, we consider and address some possible concerns about our method.
6.1	Does Maximizing Likelihood Necessarily Lead to Poor Sample Quality?
It has been suggested (HUSzar, 2015) that maximizing likelihood leads to poor sample quality be-
cause when the model is underspecified, it will try to cover all modes of the empirical data distri-
bution and therefore assign high density to regions with few data examples. There is also empirical
evidence (Grover et al., 2017) for a negative correlation between sample quality and log likelihood,
suggesting an inherent trade-off between maximizing likelihood and achieving good sample quality.
A popular solution is to minimize reverse KL-divergence instead, which trades off recall for pre-
9
Under review as a conference paper at ICLR 2019
cision. This is an imperfect solution, as the ultimate goal is to model all the modes and generate
high-quality samples.
Note that this apparent trade-off exists that the model capacity is assumed to be fixed. We argue
that a more promising approach would be to increase the capacity of the model, so that it is less
underspecified. As the model capacity increases, avoiding mode dropping becomes more important,
because otherwise there will not be enough training data to fit the larger number of parameters to.
This is precisely a setting appropriate for maximum likelihood. As a result, it is possible that a
combination of increasing the model capacity and maximum likelihood training can achieve good
precision and recall simultaneously.
6.2	Would Minimizing Distance to the Nearest Samples Cause Overfitting?
When the model has infinite capacity, minimizing distance from data examples to their nearest sam-
ples will lead to a model distribution that memorizes data examples. The same is true ifwe maximize
likelihood. Likewise, minimizing any divergence measure will lead to memorization of data exam-
ples, since the minimum divergence is zero and by definition, this can only happen if the model
distribution is the same as the empirical data distribution, whose support is confined to the set of
data examples. This implies that whenever we have a finite number of data examples, any method
that learns a model with infinite capacity will memorize the data examples and will hence overfit.
To get around this, most methods learn a parametric model with finite capacity. In the parametric
setting, the minimum divergence is not necessarily zero; the same is true for the minimum distance
from data examples to their nearest samples. Therefore, the optimum of these objective functions
is not necessarily a model distribution that memorizes data examples, and so overfitting will not
necessarily occur.
6.3	Does Disjoint Support Break Maximum Likelihood?
Arjovsky et al. (2017) observes that the data distribution and the model distribution are supported
on low-dimensional manifolds and so they are unlikely to have a non-negligible intersection. They
point out DKL (pdata kpθ ) would be infinite in this case, or equivalently, the likelihood would be
zero. While this does not invalidate the theoretical soundness of maximum likelihood, since the
maximum of a non-negative function that is zero almost everywhere is still well-defined, it does
cause a lot of practical issues for gradient-based learning, as the gradient is zero almost everywhere.
This is believed to be one reason that models like variational autoencoders (Kingma & Welling,
2013; Rezende et al., 2014) use a Gaussian distribution with high variance for the conditional likeli-
hood/observation model rather than a distribution close to the Dirac delta, so that the support of the
model distribution is broadened to cover all the data examples (Arjovsky et al., 2017).
This issue does not affect our method, as our loss function is different from the log-likelihood func-
tion, even though their optima are the same (under some conditions). As the result, the gradients of
our loss function are different from those of log-likelihood. When the supports of the data distribu-
tion and the model distribution do not overlap, each data example is likely far away from its nearest
sample and so the gradient is large. Moreover, the farther the data examples are from the samples,
the larger the gradient gets. Therefore, even when the gradient of log-likelihood can be tractably
computed, there may be situations when the proposed method would work better than maximizing
likelihood directly.
7	Conclusion
We presented a simple and versatile method for parameter estimation when the form of the likelihood
is unknown. The method works by drawing samples from the model, finding the nearest sample
to every data example and adjusting the parameters of the model so that it is closer to the data
example. We showed that performing this procedure is equivalent to maximizing likelihood under
some conditions. The proposed method can capture the full diversity of the data and avoids common
issues like mode collapse, vanishing gradients and training instability. The method combined with
vanilla model architectures is able to achieve encouraging results on MNIST, TFD and CIFAR-10.
10
Under review as a conference paper at ICLR 2019
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial networks.
arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In Inter-
national Conference on Machine Learning, pp. 214-223, 2017.
Sanjeev Arora and Yi Zhang. Do GANs actually learn the distribution? an empirical study. arXiv preprint
arXiv:1706.08224, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in genera-
tive adversarial nets (GANs). arXiv preprint arXiv:1703.00573, 2017.
Kenneth Joseph Arrow, Leonid Hurwicz, Hirofumi Uzawa, and Hollis Burnley Chenery. Studies in linear and
non-linear programming. 1958.
Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state markov chains.
The annals of mathematical statistics, 37(6):1554-1563, 1966.
Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer neural networks.
In Advances in Neural Information Processing Systems, pp. 400-406, 2000.
Yoshua Bengio, GregOire Mesnil, Yann Dauphin, and Salah Rifai. Better mixing via deep representations. In
ICML (1), pp. 552-560, 2013.
Yoshua Bengio, Eric Laufer, Guillaume Alain, and Jason Yosinski. Deep generative stochastic networks train-
able by backprop. In International Conference on Machine Learning, pp. 226-234, 2014.
Julian Besag. Statistical analysis of non-lattice data. The statistician, pp. 179-195, 1975.
Christopher M Bishop, Markus Svensen, and Christopher KI Williams. GTM: The generative topographic
mapping. Neural computation, 10(1):215-234, 1998.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint
arXiv:1509.00519, 2015.
Peter J Diggle and Richard J Gratton. Monte carlo methods of inference for implicit statistical models. Journal
of the Royal Statistical Society. Series B (Methodological), pp. 193-227, 1984.
Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep
networks. In Advances in Neural Information Processing Systems, pp. 658-666, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and
Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via
maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.
Francis Ysidro Edgeworth. On the probable errors of frequency-constants. Journal of the Royal Statistical
Society, 71(2):381-397, 1908.
Brian S Everitt. Mixture DistributionsI. Wiley Online Library, 1985.
Ronald A Fisher. On an absolute criterion for fitting frequency curves. Messenger of Mathematics, 41:155-160,
1912.
Itzhak Gilboa and Eitan Zemel. Nash and correlated equilibria: Some complexity considerations. Games and
Economic Behavior, 1(1):80-93, 1989.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information process-
ing systems, pp. 2672-2680, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alex J Smola. A kernel method
for the two-sample-problem. In Advances in neural information processing systems, pp. 513-520, 2007.
11
Under review as a conference paper at ICLR 2019
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-GAN: Bridging implicit and prescribed learning in
generative models. arXiv preprint arXiv:1705.08868, 2017.
Michael Gutmann and AaPo Hyvarinen. Noise-contrastive estimation: A new estimation principle for Unnor-
malized statistical models. In AISTATS, volume 1, pp. 6, 2010.
Michael U Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Corander. Likelihood-free inference via clas-
sification. arXiv preprint arXiv:1407.4981, 2014.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation,
14(8):1771-1800, 2002.
Geoffrey E Hinton and Terrence J Sejnowski. Learning and releaming in boltzmann machines. Parallel dis-
tributed processing: Explorations in the microstructure of cognition, 1(282-317):2, 1986.
R Devon Hjelm, Athul Paul Jacob, Tong Che, Kyunghyun Cho, and Yoshua Bengio. Boundary-seeking gener-
ative adversarial networks. arXiv preprint arXiv:1702.08431, 2017.
Ferenc Huszar. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv
preprint arXiv:1511.05101, 2015.
Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. Journal of Machine
Learning Research, 6(Apr):695-709, 2005.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational
methods for graphical models. Machine learning, 37(2):183-233, 1999.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the
Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29-37, 2011.
Ke Li and Jitendra Malik. Fast k-nearest neighbour search via Dynamic Continuous Indexing. In International
Conference on Machine Learning, pp. 671-679, 2016.
Ke Li and Jitendra Malik. Fast k-nearest neighbour search via Prioritized DCI. In International Conference on
Machine Learning, pp. 2081-2090, 2017.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings of the 32nd
International Conference on Machine Learning (ICML-15), pp. 1718-1727, 2015.
Yujia Li, Alexander Schwing, Kuan-Chieh Wang, and Richard Zemel. Dualing GANs. In Advances in Neural
Information Processing Systems, pp. 5611-5621, 2017.
David JC MacKay. Bayesian neural networks and density networks. Nuclear Instruments and Methods in
Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 354(1):
73-80, 1995.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Advances in Neural
Information Processing Systems, pp. 1823-1833, 2017.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv preprint
arXiv:1610.03483, 2016.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in Applied
Probability, 29(2):429-443, 1997.
Radford M Neal. Connectionist learning of belief networks. Artificial intelligence, 56(1):71-113, 1992.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using
variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271-279,
2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and variational
inference in deep latent gaussian models. In International Conference on Machine Learning, 2014.
12
Under review as a conference paper at ICLR 2019
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training GANs. In Advances in Neural Information Processing Systems, pp. 2234-2242,
2016.
Jurgen SChmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):
863-879, 1992.
Mathieu Sinn and Ambrish Rawat. Non-parametric estimation of Jensen-Shannon divergence in generative
adversarial network training. arXiv preprint arXiv:1705.09199, 2017.
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv
preprint arXiv:1511.01844, 2015.
Zhuowen Tu. Learning generative models via discriminative approaches. In Computer Vision and Pattern
Recognition, 2007. CVPR’07. IEEE Conference on, pp. 1-8. IEEE, 2007.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv
preprint arXiv:1601.06759, 2016.
Max Welling and Geoffrey Hinton. A new learning algorithm for mean field boltzmann machines. Artificial
Neural NetworksICANN 2002, pp. 82-82, 2002.
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of decoder-
based generative models. arXiv preprint arXiv:1611.04273, 2016.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv preprint
arXiv:1609.03126, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.
13
Under review as a conference paper at ICLR 2019
ΛΛ ⅞g≡<¾.⅞g^>⅝⅞≡CT:
L
5 δ 6
23 八。09 ya
33339，S54
222β7f655
58b6g"g 夕
33QQ2Zg50
SS)。WS//6
SgGJazg"
，96,55。OQ
g8gy∖3i6
644699643
QRqgqqoGB
£夕1 I 3 3 M H a
CDgY夕夕777
a g 8 3。9 8 8 ?
夕夕g3∕∕ΘOO
Rqggssqq?
d
5
7
0
6
6
0
B
夕, J6551ΛN.:TgVV-7 , 777 夕5。
6 5gga qg8 7r< 77/夕 5。
,ggoo∕SA 夕夕 q∕∕3 0 37
q q
θ o
q q
θð
3 3
q oι
3 3
6 6
3 3
3 3
S，
G 0
a ,^
q q
6 6
t g
iiii
Siili
(c) CIFAR-10
(a) MNIST
(b) TFD
7
7
SggOoys4q 7，，夕 2037
¥401050/6 夕 q 夕夕 JS33
3 S
2 2




西博西息/甯幽国寤≡i笛


思思金寓召厚国画黑黑≡

饼i
襄遇i霹翳期雕



Figure 4: Comparison of samples and their nearest neighbours in the training set. Images in odd-
numbered columns are samples; to the right of each sample is its nearest neighbour in the training
set.
8 Supplementary Material
Before proving the main result, we first prove the following intermediate results:
Lemma 1. Let Q ⊆ Rd and V ⊆ R. For i ∈ [N], let fi : Ω → V be differentiable on Q
and Φ : V → R be differentiable on V and Strictly increasing. Assume argmin%。PN=I fi (θ)
exists and is unique. Let θ * := argminθgo PN=I fi (θ) and Wi := 1∕Φ0 (fi (θ*)). If the following
conditions hold:
•	There is a bounded set S ⊆ Q such that bd(S) ⊆ Q, θ* ∈ S and ∀fi, ∀θ ∈ Q \ S, fi (θ) >
fi(θ*), where bd(S) denotes the boundary ofS.
•	For all θ ∈	Q,	if θ =	θ*,	there
M W1Φ0(fl(θ)) ∖	( ∂fι∕∂θj(θ))∖
∖[ WnΦ0 (fn (θ)) J [ ∂fn∕∂θj (θ))'
∈
exists j
[d] such that
Then	argminx∈Q PN=I WiΦ(fi(θ))	exists and is unique.	Furthermore,
arg minθ∈Ω PN=I WiΦ(fi(θ)) = argminθ∈Ω PN=I fi(θ).
Proof. Let S ⊆ Q be the bounded set SUCh that bd(S) ⊆ Ω, θ* ∈ S and ∀fi, ∀θ ∈ Q \ S, fi(θ) >
fi(θ*). Consider the closure of S := S ∪ bd(S), denoted as S. Because S ⊆ Q and bd(S) ⊆ Ω,
S ⊆ Ω. Since S is bounded, S is bounded. Because S ⊆ Ω ⊆ Rd and is closed and bounded, it is
compact.
Consider the function PN=I wiΦ(fi(∙)). By the differentiability of fi’s and Φ, PN=I wiΦ(fi(∙))
is differentiable on Ω and hence continuous on Ω. By the compactness of S and the
continuity of PN=I WiΦ(fi(∙)) on S ⊆ Ω, Extreme Value Theorem applies, which im-
plies that mi□θ∈S PN=I WiΦ(fi(θ)) exists. Let θ ∈ S be such that PN=I WiΦ(fi(θ))=
minθ∈S PN=I Wiφ(fi(θ)).
By definition of S, Bfi, ∀θ ∈ B \ S, f(θ) > f(θ*), implying that Φ(f(θ)) > Φ(f(θ*))
since Φ is strictly	increasing. Because Φ0(∙)	> 0, Wi >	0 and so PiN=1 WiΦ(fi (θ))	>
PN=I WiΦ(fi(θ*))	Bθ ∈ Q \	S. At the same	time, since θ*	∈ S ⊂ S, by definition of	θ,
PN=I WiΦ(fi⑼)≤ PN=I WiΦ(fi(θ*)). Combining these two facts yields PN=I WiΦ(fi⑼)≤
PN=I WiΦ(fi(θ*)) < PN=I WiΦ(fi(θ)) Bθ ∈ Q \ S. Since the inequality is strict, this implies that
θ ∈ Q \ S, and so θ ∈ S \ (Ω \ S) ⊆ Q \ (Ω \ S) = S.
14
Under review as a conference paper at ICLR 2019
In addition, because θ is the minimizer of PN=I WiΦ(fi(∙)) on S, PN=1 WiΦ(fi (θ)) ≤
pi=ι WiΦ(fi(θ)) ∀θ ∈ S. So, PN=I WiΦ(fi(θ))	≤ PN=i WiΦ(fi(θ)) ∀θ ∈ S ∪
(Ω \ S) ⊇ S ∪ (Ω \ S) = Ω. Hence, θ is a minimizer of PN=I WiΦ(fi(∙)) on Ω, and so
minθ∈Ω PN=I WiΦ(fi(θ)) exists. Because PN=I WiΦ(fi(∙)) is differentiable on Ω, θ must be a
critical point of PN=I WiΦ(fi(∙)) on Ω.
On the other hand, since Φ is differentiable on V and fi(θ) ∈ V for all θ ∈ Ω, Φ0(fi(θ)) exists for
all θ ∈ Ω. So,
NN
V XWiΦ(fi(θ))	= XWiV (Φ(fi(θ)))
i=1	i=1
N
X wiΦ0(fi(θ))Vfi(θ)
i=1
At θ = θ*,
N
X
i=1
Φ0(fi(θ))
Φ0(fi(θ*))
Vfi(θ)
P(X (J)/A*、、' X φ0(fi(θ*))v7 f/A*、
V (工 Wiφ(fi(θ ))J=工 φ7wπvfi(θ)
N
= X Vfi(θ*)
i=1
Since each f is differentiable on Ω, PN=I f is differentiable on Ω. Combining this with the fact
that θ* is the minimizer of PN=Ifi on Ω, it follows that V (PN=Ifi(θ*)) = PN=I Vfi(θ*) = 0.
Hence, V (PN=I WiΦ(fi(θ*))) = 0 and so θ* is a critical point of PN=I WiΦ(fi(∙)).
Because ∀θ ∈ Ω, if θ = θ*, ∃j ∈ [d] such that (
W1Φ0(f1(θ))
WnΦ0(fn(θ))
∂ ∂fι∕∂θj(θ)
.
.
.
∂fn∕∂θj(θ)
PN=1 WiΦ0(fi(θ))Vfi(θ) = V (PN=I WiΦ(fi(θ))) = 0 for any θ = θ* ∈ Ω. Therefore, θ*
is the only critical point of PN=I WiΦ(fi(∙)) on Ω. Since θ is a critical point on Ω, We can con-
clude that θ* = 仄 and so θ* is a minimizer of PN=I WiΦ(fi(∙)) on Ω. Since any other mini-
mizer must be a critical point and θ* is the only critical point, θ* is the unique minimizer. So,
argminθ∈Ω PN=I fi(θ) = θ* =arg minθ∈Ω PN=I WiΦ(fi(θ)).	□
Lemma 2. Let P be a distribution on Rd whose density p(∙) is continuous at a point x° ∈ Rd and
X 〜 P be a random variable. Let r := ∣∣x 一 x0∣∣2, K := π"2∕Γ (d + 1), where Γ(∙) denotes the
gamma function 1, and r := κrd. Let G(∙) denote the cumulative distribution function (CDF) of r
and ∂+G(∙) denote the one-sided derivative of G from the right. Then, ∂+G(0) = p(x°).
Proof. By definition of ∂+ G( ∙),
+G(0)
lim
h→0+
G(h) - G(0) _ ɪ,ɪɪɪ G(h)
h	h→0+	h
lim
h→0+
Pr (r ≤ h)
h
Pr (r ≤ PhTK)
lim ------------
h→0+	h
1The constant K is the the ratio of the volume of a d-dimensional ball of radius r to a d-dimensional cube
of side length r.
15
Under review as a conference paper at ICLR 2019
If We define h := dh∕κ, the above can be re-written as:
1∙ Pr (r ≤ h)	1∙	RBxo (h)P(U)du
∂+G(0) = lim ----二----- = lim -----0——H-----
h→0+	Khd	h→0+	Khd
We want to show that limhʃ→0+ (JB
_ . ~
∀e > 0 ∃δ > 0 such that ∀h ∈ (0, δ),
(h)-
RB
P(U)du) /κhd = p(xo). In other words, we want to show
%㈤ P(U)du
Khd
- p(x0) < .
Let > 0 be arbitrary.
Since p(∙) is continuous at x°, by definition, ∀e > 0 ∃δ > 0 such that ∀u ∈ Bχ0(δ),
|p(u) — p(xo)∣ < e. Let δ > 0 be such that ∀u ∈ Bχ0(δ), p(xo) — e < P(U) < p(xo) + e.
We choose δ = δ.
Let0 < h < δbe arbitrary. Since P(x0) —	< P(U)	< P(x0)+	∀U	∈ Bx0 (δ)	=	Bx0 (δ)	⊃	Bx0 (h),
P(U)dU <	(P(x0 ) + ) dU
JBxo (h)	JBxo (h)
= (P(x0 ) + )	dU
Bxo (h)
Observe that JB 出 du is the volume of a d-dimensional ball of radius h, so JB 出 du = κhd.
Thus, RBx (h p(u)du < Khd (p(xo) + e), implying that (RB 向 p(u)du) /κhd < p(xo) + e. By
similar reasoning, we conclude that (RBX ⑺ p(u)du) /κhd > p(x0) — e.
Hence,
h0 (h)P(U)du
κhd
—p(xo) < e ∀h ∈ (0, δ)
〜
Therefore,
∂+ G(0) = lim
h→0+
∕bx0 (h) P(u)du
κh d
= P(x0 )
□
Lemma 3. Let Pθ be a parameterized family of distributions on Rd with parameter θ and proba-
bility density function (PDF) pθ(∙) that is continuous at a point Xi. Consider a random variable
χi 〜Pθ and define rθ := ∣∣χi — Xi^2, whose cumulative distribution function (CDF) is de-
noted by Fi(∙). Assume Pθ has the following property: for any θι, θ2, there exists θo such that
Fiθo (t) ≥ max
let χi,..., Xm
Fiθ1 (t), Fiθ2 (t) ∀t ≥ 0 and Pθo (Xi) = max{Pθ1(Xi),Pθ2(Xi)}. For any m ≥ 1,
〜 Pθ be i.i.d. random variables and define R := min7-∈[m] ∣∣X j — Xi∣∣j. Then the
function Ψi : z → minj {E [Rj] |pj (Xi) = z } is strictly decreasing.
16
Under review as a conference paper at ICLR 2019
Proof. Let rθ := K (rθ)d/2 = κ ∣∣χi - Xihd be a random variable and let Gθ(∙) be the CDF of rθ.
Since R is nonnegative,
c∞
典]=/
PT(R > t) dt
E
/	(Pr (∣∣χi-χi∣∣2 >t)) dt
f∞
Jo
f
Jo
/
o
∙∞
∙∞
(Pr (κ ∣∣Xθ - Xi∣∣d > Ktd/2)) dt
(Pr (rθ > Ktd/2)) dt
(1 - Gf (Ktd,,Y dt
Also, by Lemma 2, Pθ (Xi)	=	∂+Gf (0).
Using these facts, we can rewrite
mi∏θ {E [Rθ] |pf (Xi) = Z } as minf {∕c∞ (1 - Gθ (κtd∕2))m dt ∣∂+Gθ(0) = Z }. By definition
of Ψi, minθ {R∞ (1 - Gθ (κtd∕2))m dt ∣∂+Gθ(0) = z } exists for all z.
Let φi(z) be a value of
θ that attains the minimum. Define G*(y, z) := Gφi(z)(y). By definition, IyG*(0, z) = z, where
IyGi(y, z) denotes the one-sided partial derivative from the right w.r.t. y. Also, since Gi(∙, z) is
the CDF of a distribution of a non-negative random variable, Gi (0, z) = 0.
By definition of IyGi(0, z), ∀e > 0 ∃δ > 0 such that ∀h ∈ (0, δ), ∣ G“h⑶-GU - z∣ < e.
Let z0 > z. Let δ > 0 be such that ∀h ∈ (0, δ), ∣ Gi'""W' - z∣ < z--z and δ0 > 0 be such
that ∀h ∈ (0,δ0), ∣ Gi(h'z')-Gi(0'z') - z0∣ < 勺.
Consider h ∈	(0, min(δ, δ0)).	Then,	Gi (h")-Gi'0/'	=	Gi IhbZ)	< z +	z--z	=	z+2z-	and
Gi (h,z-)-Gi (0,z-) — Gi (h,z-)	/ Z--Z — z+z- S
h	= -h- > z-----------2~ = "ɪ. so，
G^(h,z) < Z + z0 < G^(h,z0)
h	2	h
Multiplying by h on both sides, we conclude that Gi (h, z) < Gi (h, z0) ∀h ∈ (0, min(δ, δ0)).
Let α := dJmin(δ, δ0)∕κ. We can break R∞ (1 - Gi(Ktd/2, z))m dt into two terms:
Z； (1 - G* (Ktd/2,z))mdt
=£ (1 - G* (Ktd/2,z))m dt + J∞ (1 - G* (Ktd/2, Z))
We can also do the same for R∞ (1 - Gi(Ktd/2, z0))m dt.
m
dt
Because Gi(h, z) < Gi(h, z0) ∀h ∈ (0, min(δ, δ0)), Gi(Ktd∕2,z) < Gi(Ktd∕2,z0) ∀t ∈ (0, α).
It follows that 1 - Gi(Ktd/2, z) > 1 - Gi(Ktd/2, z0) and (1 - Gi(Ktd/2, z))m >
(1 — Gi(Ktd∕2,z0))m	∀t	∈	(0,	α).	So,	/；	(1 —	Gi(Ktd/2,	z))m dt	>
R0α (1 - Gi(Ktd/2, z0))m dt.
We now consider the second term. First, observe that Fiθ(t) = Pr ^∣∣Xθ - Xi∣∣2 ≤ t) =
Pr (k ∣∣X1 - Xi∣∣d ≤ Ktd/2) = Gθ (Ktd/2) for all t ≥ 0. So, by the property of Pθ, for
any θ1,θ2, there exists θo such that Gθo(Ktd/2) = Fiθo (t) ≥ max{Fiθ1 (t),Fθ2 (t)}=
max ∣Gθ1 (Ktd/2),Gθ2(Ktd/2)} ∀t ≥ 0 and ∂+Gθo(0) = pθ°(Xi) = max {pθι (Xi),Pθ2 (Xi)}=
max{∂+Gθ1 (0),∂+Gθ2 (0)}.
17
Under review as a conference paper at ICLR 2019
Take θι = φi(Z) and θ2 = φi(z0).	Let θo be such that Gθ(κtd/2) ≥
max∣Gθ1 (κtd/2),Gθ2(Ktd/2)} Vt ≥ 0 and ∂+ Gθ0(0) =max{∂+Gf1 (0),∂+Gf2 (0)}. By
definition of φi(∙), ∂+Gθ1 (0) = Z and ∂+Gθ2(0) = Z. So, ∂+Gθ0(0) = max{z,z0} = ZL
Since Gf0(κtd∕2) ≥ Gf2(κtd∕2) Vt ≥ 0, 1 - Gf0 (κtd∕2) ≤ 1 - Gf2 (κtd∕2) Vt ≥ 0
and so R∞ (1 - Gf0 (κtd∕2)) dt ≤ R∞ (1 - Gf2 (κtd∕2)) dt. On the other hand, because
θ2 = φi(z0) minimizes R∞ (1 - Gf (κtd∕2))m dt among all θ,s such that ∂+Gf (0) = z0 and
∂+Gf0 (0) = Z, R∞ (1 - Gf2 (κtd∕2)) dt ≤ R∞ (1 - Gf0 (κtd∕2)) dt. We can therefore
conclude that R∞ (1 - Gf0 (κtd∕2)) dt = R∞ (1 - Gf2 (κtd∕2)) dt. Since 1-Gf0 (κtd∕2) ≤
1 - Gf2 (Ktd/2)vt ≥ 0, the only situation where this can happen is when Gf0 (κtd∕2)=
Gf2 (κtd∕2) Vt ≥ 0.
By definition of	G*,	GMKtd/2,z)	=	Gφi(z)(κtd/2)	=	GfI(Ktd/2)	and	GMKtd∕2,z0)=
Gφi(z)(Ktd/2) = Gf2(Ktd/2) = Gf0 (Ktd/2). By definition of θ0, Gf0 (Ktd/?) ≥ GfI(Ktd/2) Vt ≥
0.	So, Gi(Ktd∕2, z0)	=	Gf2(Ktdd2)≥	GfI(Ktd∕2)	=	Gi(Ktd∕2, z)	Vt	≥ 0. Hence,
R∞ (1 - Gi(Ktd/2, z0))m dt ≤ R∞ (1 - Gi(Ktd/2, z))m dt.
Combining with the previous result that	Ra(I — Gi (Ktd/2, z0))m dt
JOa (1 - Gi(Ktd/2, z))m dt, it follows that:
<
j； (1 - G* (Ktd∕2,z)y dt
dt +
dt +
dt +
Γ (1 - g*
J α
Γ (1 - g*
J α
Γ (1 - g*
J α
By definition,
L (1 - GF(Z)(Ktd∕2))mdt
mn {/ (1 - Gf(Ktd/2)) d"d+Gf (0) = Z
min {E [Rf ] |pf (Xi) = Z }
f
=ψi (z)
Similarly, R∞ (1 - G* (Ktd/2, z0))m dt = Ψi(z0). We can therefore conclude that Ψi(z0) < Ψi(z)
whenever z0 > z.	□
We now prove the main result.
Theorem 1. Consider a set of observations xι,..., Xn, a parameterized family of distributions
Pf with probability density function (PDF) Pf (∙) and a unique maximum likelihood solution θ*.
For any m ≥ 1, let X?,..., Xm 〜Pf be i.i.d. random variables and define rθ := ∣∣X,∣∣2, Rf ：=
min7∙∈[m] 11 Xf 112 and Rf := min7-∈[m] ∣ ∣ Xf — Xjj. Let Ff (∙) be the cumulative distributionfunction
(CDF) of rf and Ψ(z) := minf {e [Rθ] ∣pθ (0) = Z }.
If Pf satisfies thefollowing:
18
Under review as a conference paper at ICLR 2019
•	pθ (x) is differentiable w.r.t. θ and continuous w.r.t. x everywhere.
•	∀θ, v, there exists θ0 such that pθ (x) = pθ0 (x + v) ∀x.
•	For any θ1,θ2, there exists θo such that Fθ0 (t) ≥ max {Fθ1(t),Fθ2(t)} ∀t ≥ 0 and
pθ0 (0) = max {pθ1 (0), pθ2 (0)}.
•	∃τ > 0 such that ∀i ∈ [n] ∀θ ∈ Bθ* (τ), pθ (Xi) < pθ*(xj, where Bθ* (τ) denotes the ball
centred at θ* ofradius τ.
•	Ψ(z) is differentiable everywhere.
Then,
• For
/
all	θ,	if θ
Ψ0(pθ (xι))pθ(Xi)
ψ0(Pθ* (XI))Pθ* (XI)
Ψ0(pθ (Xn))Pθ (Xn)
Ψ0(Pθ* (Xn))Pθ* (Xn )
n
arg min
θ i=1
Furthermore, if pθ* (xi)=.…=
6=
/
θ*,	there exists j ∈	[d]	such that
Vθ (logPθ(xι))j
Vθ (logPθ (Xn))j
[+W
E Riθ
Ψ0(pθ* (xi))pθ* (xi)
pθ* (xn), then,
n
arg max	log pθ (xi)
θ
i=1
(
∖
∖

∖
nn
arg min E Riθ = arg max	logpθ(xi)
θ i=1	θ i=1
Proof. Pick an arbitrary i ∈ [n]. We first prove a few basic facts.
By the second property of Pθ, ∀θ ∃θ0 such that pθ(u) = pθ0 (u - xi) ∀u. In particular, pθ(xi)
pθ0 (xi - xi) = pθ0 (0). Let Fiθ be as defined in Lemma 3.
Fi⑴=Pr(rθ ≤t) = Pr (IIXI-χi∣∣2 ≤ √t)
Pθ (u)du
JBxi(√)
Pθ0 (U — Xi)du
JBxi(Vi)
=/	pio (u)du = Pr (rθ' ≤ t) = Fθ0(t)
J Bo(√t)	' J
Similarly, ∀θ0 ∃θ such that pθ0 (u) = pθ(u + Xi) ∀u. In particular, pθ0 (0) = pθ(0 + Xi) = pθ(Xi).
Fθ (t) = Pr (rθ ≤ t) = [	pio (u)du
' B	J Bo(√)
=	Pθ (u + Xi)du =	Pθ (u)du
BBo(√t)	BBxi (Vt)
=Pr (∣∣x1 -Xi||2 ≤ √t) = Pr(rθ ≤t) = Fiθ⑴
Let θ1 , θ2 be arbitrary. The facts above imply that there exist θ10 and θ20 such that Fiθ1(t) = Fθ10 (t),
Fiθ2 (t) = Fθ20 (t), pθ1 (Xi) = pθ10 (0) and pθ2 (Xi) =pθ02(0).
By the third property of Pθ, let	θ00	be such that	Fθ00 (t)	≥ max Fθ10 (t), Fθ20 (t)	∀t	≥ 0 and
pθ0 (0) = max pθ0 (0), pθ0 (0)	.	By the facts	above,	it follows that there exists	θ0	such that
F θ00 (t) = Fiθ0(t) and pθ00 (0) =pθ0(Xi).
So, we can conclude that for any θ1, θ2, there exists θ0 such that Fiθ0 (t)	≥
max nFiθ1 (t), Fiθ2 (t)o ∀t ≥ 0 and pθ0 (Xi) = max{pθ1(Xi),pθ2(Xi)}.
19
Under review as a conference paper at ICLR 2019
By Lemma 3, Ψi(z) = minθ {E [Rθ] ∣pθ(Xi) = z } is strictly decreasing.
Consider any θ. By the facts above, there exists θ0 such that pθ (xi) = pθ0 (0) and Fiθ(t) = Fθ0 (t) ∀t.
Therefore,
E [Rf ] = ∞ Pr(R > t) dt
0
=ZO	(Pr (llx1 - xill2 >t))mdt
=I∞ (1 - F(t))m dt
0
=Z∞	(1 -Fθ0(t)mdt
=	Pr (Rθ0 > t dt
=E hRθ0i
So, ∀z
Ψi(z) =min {E [Rθ] ∣Pθ(xi) = Z }
θ
= min nE Rθ0 ∣pθ0(0) =z
=Ψ(z)
Because Ψi(∙) is strictly decreasing, Ψ(∙) is also strictly decreasing.
We would like to apply Lemma 1, with fi(θ) = - logpθ(xi) ∀i ∈ [n] and Φ(y) = Ψ(exp(-y)).
By the first property of Pf, pθ(∙) is differentiable w.r.t. θ and so fi(θ) is differentiable for all i. By
the fifth property of Pf, Ψ(∙) is differentiable and so Φ(∙) is differentiable. Since y → exp(-y)
is strictly decreasing and ψ(∙) is strictly decreasing, Φ(∙) is strictly increasing. Since there is a
unique maximum likelihood solution θ*, minf pn=1 fi(θ) = maxf Pn=IlogPθ(xi) exists and has
a unique minimizer. By the fourth property ofPf, the first condition of Lemma 1 is satisfied. By the
sixth property ofPf, the second condition of Lemma 1 is satisfied. Since all conditions are satisfied,
we apply Lemma 1 and conclude that
nn
min	wiΦ(fi(θ)) =min	wiΨ(pf(xi))
i=1	i=1
n
min	wiΨi (pf (xi))
i=1
n
min
f
i=1
E [Rf]
Ψ0(pθ* (Xi))Pθ* (Xi)
exists and has a unique minimizer. Furthermore,
• X E [Rθ ]	∙ X 1	/、
argmin 之 ψo(Pθ* (Xi))pθ* (Xi) =argmin 之-log pθ(Xi)
n
= arg max	log pf (Xi)
i=1
If Pθ(Xi) = …Pθ(Xn), then w1 = … = wn, and so argminf Pn=1 WiE [Rθ] =
argminθ Pn=I E [Rθ] = argmaxf Pn=IlogPf (Xi).	□
20