Under review as a conference paper at ICLR 2019
Learning Joint Wasserstein Auto-Encoders
for Joint Distribution Matching
Anonymous authors
Paper under double-blind review
Ab stract
We study the joint distribution matching problem which aims at learning bidirec-
tional mappings to match the joint distribution of two domains. This problem
occurs in unsupervised image-to-image translation and video-to-video synthesis
tasks, which, however, has two critical challenges: (i) it is difficult to exploit suf-
ficient information from the joint distribution; (ii) how to theoretically and exper-
imentally evaluate the generalization performance remains an open question. To
address the above challenges, we propose a new optimization problem and design
a novel Joint Wasserstein Auto-Encoders (JWAE) to minimize the Wasserstein
distance of the joint distributions in two domains. We theoretically prove that the
generalization ability of the proposed method can be guaranteed by minimizing
the Wasserstein distance of joint distributions. To verify the generalization abil-
ity, we apply our method to unsupervised video-to-video synthesis by performing
video frame interpolation and producing visually smooth videos in two domains,
simultaneously. Both qualitative and quantitative comparisons demonstrate the
superiority of our method over several state-of-the-arts.
1	Introduction
The joint distribution matching problem has attracted extensive attention in computer vision, such
as unsupervised image-to-image translation (Zhu et al., 2017; Liu et al., 2017) and video-to-video
synthesis (Bashkirova et al., 2018). The goal of this problem is to learn the bidirectional mappings
between unpaired data in two different domains. Unlike the marginal distribution in each domain,
learning a joint distribution is often ignored and has the following two critical challenges.
The first key challenge, from a probabilistic modeling perspective, is how to exploit the joint distri-
bution of unpaired data by learning the bidirectional mappings between two different domains. In
the unsupervised learning setting, there are two sets of samples drawn separately from two marginal
distributions in two domains. Based on the coupling theory (Lindvall, 2002), there exist an infinite
set of joint distributions given two marginal distributions, and hence infinite bidirectional mappings
between two different domains. Therefore, directly learning the joint distribution without additional
information between the marginal distributions is a highly ill-posed problem. Recently, many stud-
ies (Zhu et al., 2017; Yi et al., 2017; Kim et al., 2017) have been proposed to learn the mappings in
two domains separately, which may incur the joint distribution mismatching issue. Therefore, how
to exploit sufficient information from the joint distribution still remains an open question.
Another important challenge is that the generalization ability w.r.t. the learned joint distribution
of two different domains is still unknown. Existing theoretical results (Pan et al., 2018; Galanti
et al., 2018) ignore the joint distribution of different data and cannot guarantee the generalization
ability of such joint distribution. Moreover, it is also very hard to evaluate the generalization ability
practically. Regarding this issue, according to (Bojanowski et al., 2018), the generalization ability
can be evaluated by the interpolation performance in the target domain. In this sense, we can extend
image-to-image translation to video space by performing video interpolation in one domain and
investigating the performance of the translated video in another domain. To achieve this, one may
directly apply existing unsupervised image-to-image translation methods (Zhu et al., 2017; Kim
et al., 2017; Yi et al., 2017). However, these methods may result in significantly incoherent videos
with low visual quality. Therefore, it is important to design an effective joint distribution learning
method and provide necessary theoretical analysis.
1
Under review as a conference paper at ICLR 2019
Regarding the above two challenges, in this paper, we propose a Joint Wasserstein Auto-Encoders
(JWAE) to learn the bidirectional mappings between two domains by minimizing Wasserstein dis-
tance of joint distributions. Relying on the optimal transport theory, we are able to exploit sufficient
information by matching latent distributions of images in two domains.
The contributions of this paper are summarized as follows:
•	We propose a novel JWAE to solve the joint distribution matching problem. Based on Theo-
rem 1, an intractable primal problem of optimal transport can be reduced to a simple optimization
problem. More critically, our method is a generalization of CycleGAN (Liu et al., 2017) and
UNIT (Liu et al., 2017).
•	We provide a generalization bound of JWAE (see Theorem 4). In particular, we theoretically
prove that the generalization ability of our method w.r.t. the learned joint distribution can be
guaranteed by minimizing Wasserstein distance of joint distributions.
•	To practically verify the generalization ability, we apply our method to unsupervised video-to-
video synthesis and obtain two visually smooth videos in two different domains. Experiments on
real-world datasets show the superiority of the proposed method over several state-of-the-arts.
2	Related Work
In this paper, we consider the joint distribution matching problem. Recently, this problem has at-
tracted extensive attention in image-to-image translation and video-to-video synthesis.
Image-to-image translation. Recently, Generative adversarial networks (GAN) (Goodfellow et al.,
2014; Cao et al., 2018; Salimans et al., 2018), Variational Auto-Encoders (VAE) (Kingma & Welling,
2014) and Wasserstein Auto-Encoders (WAE) (Tolstikhin et al., 2017) have emerged as popular
techniques for the image-to-image translation (I2IT) problem. For the unsupervised I2IT problem,
CycleGAN (Zhu et al., 2017), DiscoGAN (Kim et al., 2017) and DualGAN (Yi et al., 2017) aim at
minimizing the adversarial loss and the cycle-consistent loss in different domains, which may induce
a joint distribution mismatching issue. To address this, CoGAN (Liu & Tuzel, 2016) learns a joint
distribution by enforcing a weight-sharing constraint. Moreover, UNIT (Liu et al., 2017) builds
upon CoGAN by using a shared-latent space assumption and the same weight-sharing constraint.
However, these methods are not well-supported by any theoretical justifications.
Video-to-video synthesis. In this paper, we consider unsupervised video-to-video synthesis (V2VS)
problem. Existing image-to-image methods (Zhu et al., 2017; Kim et al., 2017; Yi et al., 2017) can-
not be directly used in the video-to-video synthesis problem, we further combine some video frame
interpolation methods (Zhou et al., 2016; Ji et al., 2017; Niklaus et al., 2017; Liu et al., 2018) to syn-
thesize video. Although UNIT (Liu et al., 2017) can be applied to video synthesis by interpolating
in the latent space, it often results in temporally incoherent videos of low visual quality. Recently,
a video-to-video translation method (Bashkirova et al., 2018) is proposed to translate a video in
one domain to a video in another domain, but this method can not conduct video frame interpola-
tion. Moreover, Wang et al. (2018) propose a video-to-video synthesis method and synthesize video
results, but it cannot work for the unsupervised learning setting.
3	Problem Definition
Notations. We use calligraphic letters (e.g., X) for space, capital letters (e.g., X) for random vari-
ables, and bold lower case letter (e.g., x) for their corresponding values. We denote probability dis-
tributions with capital letters (i.e., P (X)) and corresponding densities with bold lower case letters
(i.e., p(x)). Let (X, PX) be the domain, and P(X) be the set of all the probability measures over X,
and PX be the marginal distribution over X. Sx={xi}iM=1 and Sy={yi}iN=1 are two sets of unpaired
training data. For a set S and two functions F: S→R and G: S→R, we denote F (s).G(s), ∀s∈S
if and only if ∃ C1,C2>0 (independent of S) such that F(s)≤C1∙G(s)+C2.
2
Under review as a conference paper at ICLR 2019
_____c Cross-domain mapping -------m Mapping -------> Reconstruction	*---> Distribution divergence	*—► Distance
Figure 1: Demonstrations of (a) the JWAE scheme and (b) the interpolation based V2VS method.
(b) Interpolation based video-to-video synthesis
3.1	Joint Distribution Matching Problem
This paper considers the Joint Distribution Matching Problem for the unsupervised image-to-image
translation task. Given two domains (X, PX) and (Y, PY ), where PX and PY are distributions
over X and Y, our goal is to learn cross-domain mappings, i.e., f : X→Y and g : Y→X. Then we
construct joint distributions in two domains, i.e., PA(X, f(X)), X∈X and PB(g(Y ), Y ), Y ∈Y, and
we minimize the following distribution divergence d(PA, PB) between these two joint distributions,
min d(PA, PB),	(1)
f ∈F,g∈G
where F and G are the sets of cross-domain mappings. Note that d(∙, ∙) is an arbitrary distribu-
tion divergence, e.g., Wasserstein distance. In this paper, we study the joint distribution matching
problem from the optimal transport (OT) point of view, which would bring some interesting results.
4	Proposed Method
4.1	Wasserstein Distance of Joint Distribution
To address the joint distribution matching problem, one can learn a shared latent space Z for two
different domains (Liu et al., 2017). In this sense, any pair of images in different domains can be
mapped to the same latent representation. Inversely, there exist generative models PG1 (X0|Z) and
PG2 (Y 0|Z) that map a shared latent code Z to X0=G1 (Z) and Y 0=G2(Z), respectively. Then
PA(X, Y 0) and PB(X0, Y ) are two joint distributions between real and generated images, we can
minimize Wasserstein distance Wc(PA, PB) between joint distributions PA and PB, i.e.,
Wc(PA,PB)
min
P∈P(PA,PB)
E(X,Y0;X0,Y)~P [c(X, Y0； X0, Y儿
(2)
where P(PA, PB) is the set of couplings which is composed of joint probability distributions with
the probability distributions (PA, PB), and c is any measurable cost function.
In practice, there are two important challenges on Wasserstein distance and the cost function. First,
directly optimizing Problem (2) raises intractable computational difficulties (Genevay et al., 2018).
Second, how to choose a appropriate cost function is very challenging. In this paper, we set
c(X, Y0; X0, Y)=c1(X, X0)+c2 (Y0, Y) (Bhushan Damodaran et al., 2018), where c1 and c2 can
be any metric to measure the distance in X ×X and Y×Y, respectively. This helps to derive the fol-
lowing theorem so that the intractable Problem (2) can be reduced to a simple optimization problem.
Theorem 1 Given two deterministic models PG1 (X0|Z) and PG2 (Y0|Z) as Dirac measures, i.e.,
PG1 (X0|Z = z) = δG1(z) and PG2 (Y0|Z = z) = δG2(z) for all z ∈ Z, we have
Wc(PA,PB) = inf EPX EQ(Z1|X)[c1(X, G1(Z1))] + inf EPYEQ(Z2|Y)[c2(G2(Z2),Y)], (3)
Q∈Q1	Q∈Q2
whereQ1={Q(Z1|X)|QZ1=PZ=QZ2,PY=PG2}andQ2={Q(Z2|Y)|QZ1=PZ=QZ2,PX=PG1}
are the set of all probabilistic encoders, where QZ1 and QZ2 are the marginal distributions of
Zι~Q(Zι∣X) and Z2~Q(Z2∣Y), where X~Pχ and Y~Pγ, respectively. 1
As previously mentioned, finding an optimal couplings between joint distributions PA and PB is
very challenging. Fortunately, according to Theorem 1, we can instead optimize problem (3) for
joint distribution matching. The details of objective functions and optimizations are given below.
1See supplementary materials for the proof.
3
Under review as a conference paper at ICLR 2019
4.2	Joint Wasserstein Auto-Encoders
As shown in Figure 1, We aim to learn the cross-domain mappings (i.e., G?oEi and GiqE?) from
real data X and Y to samples Y 0 and X0 such that the generated distributions are close to the real
distribution, i.e., PX =PG1, PY =PG2. Moreover, the latent distributions generated by two Auto-
Encoders (i.e., G∖qEi and G?qE2) should be close to each other, i.e., Qzι =Qz?. To this end, we
optimize Problem (3) by relaxing the constraints PX =PG1, PY =PG2 and QZ1 =PZ =QZ2. Then
we minimize the regularized optimization problem:
Wcc(PA,PB) = infQ EPXEQ(Z1|X)[c1(X, G1(Z1))] + infQ EPY EQ(Z2|Y)[c2(Y, G2(Z2))]	(4)
+ αDX (PX, PG1) + βDY (PY , PG2) + ρDZ (QZ1, QZ2),
where α, β , ρ are positive hyper-parameters, the last three terms can be arbitrary distribution diver-
gences between two distributions. The above problems involves two kinds of functions, namely the
reconstruction loss and distribution divergence.
(i)	Reconstruction loss. We denote by Rx(E1,E2,G1,G2) and Ry(E1,E2,G1,G2) the empirical
loss functions of the first two terms in Problem (4). Taking the case for the set Q1 as an example,
the empirical reconstruction loss Rx can be rewritten as follows:
1M
Rx (E1,E2,G1,G2)=	V	c1(xi,G1(E1^Xi))) + c1(xi,G1(E2(G2(E1(xi))))),	(5)
M ʌ^i=1 ×--------{z------} ×------------V-----------}
Auto-Encoder loss
Cycle consistency loss
Here, the first term represents the loss on the Auto-Encoders reconstruction and the second term
inherently enforce the cycle consistency that widely studied in image-to-image translation tasks (Zhu
et al., 2017; Liu et al., 2017; Kim et al., 2017; Yi et al., 2017). However, unlike existing methods, in
our paper, the cycle consistency loss is directly derived from the joint distribution matching problem.
The loss Ry can be constructed similarly. Last, let R=Rx +Ry be the final reconstruction loss.
(ii)	Distribution divergence. The distribution divergences in Problem (4) can be measured by Ad-
versarial loss (e.g., original GAN, WGAN), Wasserstein distance, Maximum Mean Discrepancy
(MMD) and Kullback-Leibler (KL) divergence, etc. Here, we use GAN to measure these diver-
gences, denoted by GAN(PX, PG1), GAN(PY , PG2) and GAN(QZ1 , QZ2), respectively. Taking
GAN(PX, PG1) as example, the loss function Lx can be formulated as
Lx(E1,E2,G1,G2,Dχ')=Mm XMl 2iog(Dχ(xi))+iog(i-Dχ(G1(E2(G2(E1X)))))
1	N	(6)
+ 而 ΣS∙	Iog(I-Dx(GI(E2(Yi)))).
N	i=1
Note that it is analogous to the form of Triple GAN (LI et al., 2017). We refer to the second term in
the right-hand side of (6) as a cycle adversarial (CA) term 2. Similarly, the losses Ly and Lz w.r.t.
GAN(PY ,PG2) and GAN(QZ1 ,QZ2) can be constructed. Please find the details in supplementary
materials.
4.3	A concrete example: Interpolation based video-to-video synthesis
Since the generalization ability can be evaluated by the performance of interpolation (Bojanowski
et al., 2018), we apply our method on the interpolation based video-to-video synthesis (V2VS)
problem. The training and inference methods are shown in Algorithms 1 and 2, respectively.
In the training, given two sets of images {xi}iM=1 and {yj}jN=1 in two different domains, we seek to
learn cross-domain mappings (see Algorithm 1). In the inference, given two input video frames
xbegin and xend, we perform linear interpolation based video-to-video synthesis to produce two
videos in two different domains. Specifically, we perform a linear interpolation between two em-
beddings extracted from the first domain and then decode it to a corresponding frame in the second
domain (see Figure 1 (b) and Algorithm 2). In this sense, we can directly measure the quality of
the synthesized video in the second domain to evaluate the generalization ability of the learned joint
distribution mapping.
2Different from existing image-to-image translation methods, the cycle adversarial term is important for the
performance of the proposed method. We will verify the effectiveness of this term in the experiments.
4
Under review as a conference paper at ICLR 2019
Algorithm 1 Training details for JWAE.	Algorithm 2 Inference for unsupervised V2VS.
Input: Training data in two different domains: {xi}iM=1 and {yj}jN=1. Initialization: Models: E1 , E2, G1 , G2; Discriminators: Dw , w∈{x, y, z}. repeat Update Dx , Dy , Dz by ascending: Pw Lw (E1 ,E2,G1 ,G2,Dw), w∈{x,y,z} Update E1, E2, G1, G2 by descending: Pw Lw(E1,E2,G1,G2,Dw)+R(E1,E2,G1,G2) until models converged	Input: Testing data pair in the first domain: {xbegin , xend }. Step 1: Video frame interpolation zbegin = E1 (xbegin), zend = E1 (xend) zmid = αzbegin +(1-α)zend , α∈(0, 1) xmid = G1 (zmid ) Synthesized video: {xbegin, xmid, xend} Step 2: Video translation ybegin = G2 (zbegin), ymid = G2 (zmid), yend = G2 (zend ) Synthesized video: {ybegin, ymid, yend}
5 Generalization Analysis
In this section, we analyze generalization performance of JWAE. To begin with, we provide the
definitions of the generalization error and probabilistic cross-domain Lipschitzness.
Definition 1 (Generalization Error) Define cross-domain functions as f =G2 ◦E1 and f =G1 ◦E2
when QZ1 =QZ2, and cost functions c1 and c2 which are bounded, symmetric, Lc-Lipschitz and
satisfies the triangle inequality, and given two joint distributions PA(X, Y ) and PB (X, Y ), where
Y in PA(X, Y ) and X in PB (X, Y ) are unknown, then the generalization error E(f, g) becomes:
E(f, g) = E(X,Y)〜Pa(X,Y) [c2(Y,f (X))] + E(X,Y)〜Pb(X,Y) [c1 (X, g(Y))].	⑺
Note that in image-to-image translation problem, itis common to assume that two close samples will
have close outputs with high probability, i.e., it satisfies a probabilistic Lipschitzness assumption
(Courty et al., 2017; Urner et al., 2011). For convenience, we extend the definition as follows.
Definition 2 (φ-Probabilistic Cross-domain Lipschitzness) Given real and the generated
marginal distribution PX and PX0, and let φ : R+ → [0, 1], we say that a function f : X → Y w.r.t.
a joint distribution set P(PX, PX0) over PX and PX0 is φ-Lipschitz if for all α > 0,
P(X,XO)~P(PX ,PX0 )[kf(X)-f(X0)k <αc1(X,X0)] ≥1-φ(α).	(8)
Intuitively, given a joint distribution set P(PX, PX0), a function f satisfying the α-Lipschitz prop-
erty holds with some probability. Then, we have the following results on the generalization error.
Theorem 2 Let P*=argmi□p∈p(Pf Pg) E(χ,γf ∙Xg,γ)〜P [c(X,Yf; Xg, Y)] with Lipschitz cost
functions ci and c2. Let functions f * ∈F and g*∈G be probabilistic cross-domain Lipschitzness
w.r.t. P * that minimizes the joint error Ea,b (f * ,g*). Given M and N instances drawn form PX
andPY, respectively, with Lc1 α=Lc2 β =1, the following bound holds with probability at least 1-δ:
E(f,g).W(PbAf,PbBg)
+ EA,B(f *, g*) + φe(α, β),
(9)
where EA,B(f *, gnuEAlfn+EB(f *)+EB(g*)+EA(g*), EA(f *)=Ε(x,y)^pa [c2(Y,f *(X))],
EA (f *)=Ε(χ,γ)〜Pa [c2(f(X ),f *(X))], and likewise for the definitions of EB (f *) and EB (g*).
Here, φe(α,β)=Lc1M1φ(α)+Lc2M2φ(β), where kf*(X1)-f*(X2)k≤M1,∀X1,X2∈X and
kg*(Y1)-g*(Y2)k≤M2, ∀Y1, Y2∈Y. 3
Remark 1 Theorem 4 provides an upper bound on the generalization error. The first term in right
hand of (9) corresponds to the empirical version of (2); While the second term means that we should
minimize it with sufficient unpaired data from two different domains. The third term E(f*, g*)
correspond to the joint error. When the last term φ(α, β) is sufficiently small (i.e., f and g satisfy
Lipschitz property with high probability), the cross-domain mappings would be well learned.
3See supplementary materials for the detailed proof.
5
Under review as a conference paper at ICLR 2019
6	Experiments
We apply our method to interpolation based video-to-video synthesis (V2VS) in the unsupervised
setting. To be specific, we firstly conduct video interpolation between two input frames in one
domain and then translate it to produce a corresponding video in another domain. The qualitative
and quantitative results are shown in the following subsections 4 .
Datasets. We conduct experiments on two widely used benchmark datasets, namely
Cityscapes (Cordts et al., 2016) and SYNTHIA (Ros et al., 2016). (i) Cityscapes contains
2048×1024 street scene video of several German cities and a portion of ground truth semantic
segmentation in the video. To obtain more semantic segmentation masks, following (Wang et al.,
2018), we use a pre-trained DeepLab V3 network (Chen et al., 2017) to extract extra segmentation
videos. (ii) We also study the generality of our algorithm on the unpaired dataset SYNTHIA (Ros
et al., 2016), which contains a large collection of synthetic videos in different scenes and seasons.
We perform unsupervised V2VS on four splits of SYNTHIA with different seasons, i.e., spring,
summer, fall and winter. In this paper, we adopt the winter split as the common domain and train
models to translate videos from winter to the other three seasons.
Evaluation metrics. For quantitative comparisons, We adopt Frechet Inception Distance (FID)
(Heusel et al., 2017) to evaluate the quality of the frames in the synthesized videos. FID captures
the similarity of the generated samples to real ones and correlates Well With human judgement.
Moreover, We also use a variant of FID (Wang et al., 2018) (FID4Video) to evaluate the quality of
video. FID4Video measures the distribution similarity based on the extracted feature of videos. In
general, for both FID and FID4Video, a loWer score means the better performance.
6.1	Implementation details
We implement our method based on PyTorch5. We folloW the experimental settings in Cycle-
GAN (Zhu et al., 2017). For the optimization, We use Adam solver (Kingma & Ba, 2015) With
a mini-batch size of 1 to train the models, and use a learning rate of 0.0002 for the first 100 epochs
and gradually decrease it to zero for the next 100 epochs. FolloWing (Zhu et al., 2017), We set
α=β=0.1 in Eqn. (4). By default, We set ρ=0.1 in our experiments.
6.2	Baseline methods
We adopt several state-of-the-art baselines, including UNIT (Liu et al., 2017) With the latent space
interpolation and several constructed variants of CycleGAN (Zhu et al., 2017) using different vieW
synthesis algorithms. For those constructed baselines, We first conduct video interpolation and then
perform image-to-image translation. Moreover, We also construct a variant of our method to conduct
ablation study on the Triple GAN loss. All considered baselines are summarized as folloWs.
•	UNIT (Liu et al., 2017). UNIT is a state-of-the-art unsupervised I2IT method Which can perform
supervised video-to-video synthesis by interpolating in the latent space.
•	DVF-Cycle. This method combines the vieW synthesis method DVF (Liu et al., 2018) With Cy-
cleGAN (Zhu et al., 2017). To be specific, DVF produces videos by video interpolation in one
domain. Then, We use CycleGAN to translate the generated video to another domain.
•	DVM-Cycle. We use a geometrical vieW synthesis DVM (Ji et al., 2017) for video synthesis, and
We replace DVF in DVF-Cycle With DVM and construct a neW baseline called DVM-Cycle.
•	AdaConv-Cycle. We also compare against a state-of-the-art video interpolation method Ada-
Conv (Niklaus et al., 2017). For cross-domain video synthesis, We combine this method With a
pre-trained CycleGAN model and term it AdaConv-CycleGAN in the folloWing experiments.
•	W/O-CA. To investigate the effect of the cycle adversarial (CA) terms in the loss functions (6)
and (32), We construct a baseline method by removing it from the loss functions Lx and Ly . We
refer to this baseline as W/O-CA.
4Due to the page limit, more visual results are shoWn in the supplementary materials.
5PyTorch is from http://pytorch.org/.
6
Under review as a conference paper at ICLR 2019
Table 1: Performance comparisons with state-ot-the-art baselines on Cityscapes and SYNTHIA.
Method	Cityscapes				SYNTHIA					
	scene2segmentation		segmentation2scene		winter2spring		winter2summer		winter2fall	
	FID	FID4Video	FID	FID4Video	FID	FID4Video	FID	FID4Video	FID	FID4Video
-DVF-CyCIe-	110.59	-2395	151.27	-40.61	152.44	-42.22	160.69	-42.43	163.13	-41.04-
DVM-Cycle	50.51	17.33	116.62	40.83	129.80	38.19	140.86	36.66	129.02	36.64
AdaConv-Cycle	33.50	14.96	99.67	30.24	117.40	23.83	126.01	20.62	110.52	16.77
UNIT	31.27	10.12	76.72	29.21	96.40	23.12	108.01	24.70	97.73	20.39
W/O-CA	24.32	8.34	47.41	-27.37	92.77	-21.83	84.83	-19.54	91.37	-15.87-
Ours	22.74	6.80	43.48	25.87	88.24	21.37	77.12	17.99	87.50	14.14
Figure 2: Comparisons of different methods for photo-segmentation translation on Cityscapes
dataset. We first synthesize a video of street scene and then translate it to the segmentation domain
(Top), and vice versa for the mapping from segmentation to street scene (Bottom).
6.3	Quantitative Comparisons
We compare the performance on Cityscapes and SYNTHIA and show the results in Table 1. We
can draw the following observations. First, our method consistently outperforms the baselines in
terms of both FID and FID4Video scores. It means that our method produces frames and videos of
promising quality and exhibits strong generalization ability. Second, with the help of Triple GAN
loss, our method achieves better results than W/O-CA for both FID and FID4Video. This indicates
that the reconstructed images after the cycle translation helps to learn a better joint distribution. The
above observations demonstrate the superiority of our method over the competitive methods.
6.4	Visual Comparisons
Visual results on Cityscapes. We first interpolate videos in the cityscape domain and then translate
them to the segmentation domain. In Figure 2, we compare the visual quality of both the interpolated
and the translated images. From Figure 2 (top), our method produces sharper cityscape images and
yields more accurate results in the semantic segmentation domain, which significantly outperforms
the baseline methods, and vice versa in Figure 2 (bottom).
Visual results on SYNTHIA. We further evaluate the performance of our method on SYNTHIA.
We synthesize videos among the domains of four seasons shown in Figure 3. First, our method is
able to produce sharper images when interpolating the missing in-between frames (see top row of
Figure 3). Second, the translated frames produced by our method in the other three seasons look
7
Under review as a conference paper at ICLR 2019
Figure 3: Comparison of different methods for season translation on SYNTHIA dataset. Top row:
The synthesized video in the winter domain. Rows 2-4: The corresponding translated video in the
domains of the other three seasons, i.e., spring, summer and fall.
Table 2: Influence of ρ for the adversarial loss on Z with different values. We compare the results
of Winter-summer on SYNTHIA dataset in terms of FID and FID4Video scores.
P	Winter2summer		SUmmer2winter	
	FID	FID4Video	FID	FID4Video
0.01	94.91	-20.29	107.65	-18.90-
0.1	77.12	17.99	89.03	17.36
1	89.07	21.04	102.18	18.63
10	101.07	23.66	108.47	20.50
more photo-realistic than those produced by the other baseline methods (see the shape of cars in
roWs 2-4 of Figure 3). These results demonstrate that our method is able to produce more promising
videos and consistently outperforms other methods in different domains.
6.5	INFLUENCE OF ρ FOR THE ADVERSARIAL LOSS ON Z
We study the effect of the trade-off parameter ρ over the adversarial loss on Z in Eqn. (4). The
results are shoWn in Table 2. Given a very small Weight ρ=0.01, the model obtains larger FID and
FID4Video scores compared to that With ρ=0.1. When We increase it to ρ = 1 and ρ = 10, We also
observe large performance degrades. Therefore, We suggest setting ρ = 0.1 in our method.
7	Conclusion
In this paper, We have proposed a novel joint Wasserstein Auto-Encoders method for the joint dis-
tribution matching problem. Instead of directly optimizing the primal problem of Wasserstein dis-
tance, We turn to propose a simple but effective optimization problem. In this Way, We are able
to conduct analysis on the generalization ability of JWAE and theoretically prove that minimizing
the Wasserstein distance can guarantee the generalization ability. Extensive experiments on unsu-
pervised V2VS task over several benchmark datasets demonstrate the superiority of the proposed
method over the state-of-the-art methods.
8
Under review as a conference paper at ICLR 2019
References
Dina Bashkirova, Ben Usman, and Kate Saenko. Unsupervised video-to-video translation. arXiv
preprint arXiv:1806.03698, 2018.
Bharath Bhushan Damodaran, Benjamin Kellenberger, Remi Flamary, Devis Tuia, and Nicolas
Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation.
In European Conference on Computer Vision, 2018.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space
of generative networks. In International Conference on Machine Learning, 2018.
Francois Bolley, Arnaud Guillin, and Cedric Villani. Quantitative concentration inequalities for
empirical measures on non-compact spaces. Probability Theory and Related Fields, 2007.
Jiezhang Cao, Yong Guo, Qingyao Wu, Chunhua Shen, Junzhou Huang, and Mingkui Tan. Adver-
sarial learning with local coordinate coding. In International Conference on Machine Learning,
2018.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous
convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic
urban scene understanding. In Computer Vision and Pattern Recognition, 2016.
Nicolas Courty, Remi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. In Advances in Neural Information Processing
Systems, 2017.
Tomer Galanti, Sagie Benaim, and Lior Wolf. Generalization bounds for unsupervised cross-domain
mapping with wgans. arXiv preprint arXiv:1807.08501, 2018.
AUde Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn diver-
gences. In Artificial Intelligence and Statistics, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, 2017.
Dinghuang Ji, Junghyun Kwon, Max McFarland, and Silvio Savarese. Deep view morphing. In
Computer Vision and Pattern Recognition, 2017.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover
cross-domain relations with generative adversarial networks. In International Conference on Ma-
chine Learning, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference for Learning Representations, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations, 2014.
Chongxuan LI, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In Advances
in Neural Information Processing Systems, 2017.
Torgny Lindvall. Lectures on the Coupling Method. Courier Corporation, 2002.
Ming-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In Advances in Neural
Information Processing Systems, 2016.
9
Under review as a conference paper at ICLR 2019
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks.
In Advances in Neural Information Processing Systems, 2017.
Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala. Video frame synthesis
using deep voxel flow. In International Conference on Computer Vision, 2018.
Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convo-
lution. In International Conference on Computer Vision, 2017.
Xudong Pan, Mi Zhang, and Daizong Ding. Theoretical analysis of image-to-image translation with
adversarial learning. In International Conference on Machine Learning, 2018.
Svetlozar Todorov Rachev et al. Duality theorems for kantorovich-rubinstein and wasserstein func-
tionals. Instytut Matematyczny Polskiej Akademi Nauk (Warszawa), 1990.
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M. Lopez. The
synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes.
In Computer Vision and Pattern Recognition, 2016.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal
transport. In International Conference on Learning Representations, 2018.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. In International Conference on Learning Representations, 2017.
Ruth Urner, Shai Shalev-Shwartz, and Shai Ben-David. Access to unlabeled data can speed up
prediction time. In International Conference on Machine Learning, 2011.
Cedric Villani. Optimal Transport: Old and New. Springer Science & Business Media, 2008.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catan-
zaro. Video-to-video synthesis. In Advances in Neural Information Processing Systems, 2018.
Zili Yi, Hao (Richard) Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning
for image-to-image translation. In International Conference on Computer Vision, 2017.
Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A Efros. View synthesis
by appearance flow. In European conference on computer vision, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In International Conference on Computer Vision,
2017.
10
Under review as a conference paper at ICLR 2019
Appendix: “Joint Wasserstein Auto-Encoders”
A Proof of Theorem 1
Proof We denote by P(X〜PX,XAPGj and P(Y〜PY,Y0〜Pg?) the set of all joint distri-
butions of (X, X0) and (Y, Y0) with marginals PX, PG1 and PY, PG2 , respectively, and denote by
P(PA , PB ) the set of all joint distribution of PA and PB . Recall the definition of Wasserstein dis-
tance Wc(PA, PB), we have
Wc(Pa,Pb )=	inf	E(x,y，；x，,y g∏ [c(X,Y 0 ； X 0,Y)]	(10)
π∈P(PA,PB)
=π∈pinAPB) E(X,YXY卜π [C1(X，X0)] + π∈PiPA,Pb) E(X,YK，Y卜π [c2(Y0, Y)]
= pc⅛f	E(X,XO)〜P [cι(X,X0)]+Jnf	E(Y 0,y )〜P [C2(Y0,Y)]	(11)
P ∈PX,X0	P ∈PY,Y 0
= "(infp )e(X,X0)~P [c1(X,x 0)] +	rpinf	)E(Y0,Y )~P [c2(Y0,Y)] (12)
P ∈P (PX,PG1 )	P∈P(PY ,PG2 )
=Wc1(PX,PG1)+Wc2(PG2,PY).
Line (10) holds by the definition of Wc(PA, PB). Line (11) uses the fact that the variable pair
(X, X0) is independent of the variable pair (Y, Y0), and PX,X0 and PY,Y0 are the marginals on
(X, X0) and Y, Y0 induced by joint distributions in PX,Y 0,X0,Y. In Line (12), if PG1 (X0|Z) and
PG2 (Y0|Z) are Dirac measures (i.e., X0=G1(Z) and Y0=G2 (Z)), we have
PX,X0 =P (PX, PG1),	PY,Y0=P(PY, PG2).
We consider certain sets of joint probability distributions PX,X0,Z1 and PY,Y0,Z2 of three ran-
dom variables (X, X0, Z1 ) ∈ X ×X ×Z and (Y0, Y, Z2 ) ∈ Y ×Y ×Z, respectively. We de-
note by P (X 〜PX ,Zι 〜PzJ and P (Y 〜PY, Z2 〜Pz? ) the set of all joint distributions of (X, Zι)
and (Y, Z2) with marginals PX, PZ1 and PY, PZ2, respectively. The set of all joint distributions
Pχ,χ0,Z1 SUch that X〜PX, (X0, Zι)〜Pgi,Zi and (X0⊥X)|Zi, and likewise for Py,y，％. We de-
note by PX,X0 and PX,z1 the sets of marginals on (X, X0) and (X, Z1) induced by distributions in
PX,X0,z1, respectively, and likewise for PY,Y0 and PY,z? . For the further analyses, we have
Wc1(PX,PG1)+Wc?(PY,PG?)
:P∈pinf , e(X,X0,Zi)〜P[c1 (X,X0)]+ P∈pinf E(Y0,Y,Z2)〜P [c2 (Y0,Y)]	(13)
:pe3nf EPZIEX〜P(X∣Zi)ex0〜P(X0∣Zι)[c1(X, X0)]	(14)
+ d Jnf	EPZ2EY〜P(Y|Z2)EY0〜P(Y0|Z2)[c2(Y0,Y)]
P∈PY0,Y,Z?
P ∈pinf ,	EPZIEX 〜P (X|Zi)[c1(X, GI(ZI))]+ P ∈pinf	EPZ2 EY 〜P (Y ∣Z2) [c2 (G2 (Z2 ),Y )]
inf	E(X,Zi)〜P[c1 (X,G1(ZI))]+	inf	E(YZhP[c2 (G2 (Z2),Y)]
P ∈PX,Z1	P ∈PY,Z2
PfTinf 7 1e(X,Zi)~P [c1(X, GI(ZI))]+ pj>nff 7 )e(Y,Z2)~P [c2(G2(Z2),Y)]
P ∈P (X,z1 )	P∈P(Y,z2 )
(15)
(16)
inf EPX EQ(z1|X)[c1(X, G1(Z1))]+ inf EPYEQ(z2|Y)[c2(G2(Z2),Y)],	(17)
Q∈Q1	Q∈Q2
whereQ1={Q(Z1|X)|Qz1=Pz=Qz2,PY=PG2}andQ2={Q(Z2|Y)|Qz1=Pz=Qz2,PX=PG1}
are the set of all probabilistic encoders, where Qz1 and Qz2 are the marginal distributions of
Zi〜Q(Zι∣X) and Z2〜Q(Z2∣Y), where X〜PX and Y〜PY, respectively.
Line (13) uses the tower rule of expectation and Line (14) holds by the conditional indepen-
dence property of PX,X0,z. In line (15), we take the expectation w.r.t. X0 and Y0, respectively,
and use the total probability. Line (16) follows the fact that Pχ,Zι =P(X〜PX ,Zi〜PzJ and
Py,Z2 =P(Y〜PY, Z2〜Pz2) since P(PX,Pgi ), Pχ,χ0,Z1 and PχY depend on the choice of con-
ditional distributions PG1 (X0|Zi), while PX,z1 does not, and likewise for distributions w.r.t. Y and
G2. In line (17), the generative model Q(Zi|X) can be derived from two cases where Zi can be
sampled from Ei(X) and E2(G2(Ei(X))) when Qz1 =Qz2 and PY=PG2, and likewise for the
generative model Q(Z2 |Y).
11
Under review as a conference paper at ICLR 2019
B Proof of Theorem 4
Lemma 1 Given an Lc2 -Lipschitz cost function c2, a probabilistic cross-domain Lipschitz func-
tion f*, and two joint distributions PAf and PBg w.r.t. f and g, respectively, then for any coupling
P(X,Yf;Xg,Y) ∈P(PAf,PBg),we have
where PA=(X, Yf )x〜PX
(X×Y)2
and PB=(X g ,Y )γ /Y, and Y f=f (X) and X g=g(Y).
Proof Given joint distributions PA =(X,f (X))χ〜PX and PB=(g(Y),Y)γ〜PY, and an Lc?-
Lipschitz cost function c2 on Y × Y, and based on the duality form of the Kantorovitch-Rubinstein
theorem (Rachev et al., 1990; Villani, 2008), we have
SupjX J2(Y,f *(X ))d(PA - PB)=inf( yJf*(X,Y f ； Xg ,Y )dP(X,Y f; Xg ,Y)
where df* (X, Yf; Xg, Y) is a cost function of f * w.r.t. (X, Yf; Xg ,Y), and the cost function c2
satisfies the condition, i.e., c2(Yf, f *(X)) - c2(Y, f *(Xg)) ≤ df*(X,Yf; Xg, Y). For simplicity,
we choose its equality, then we have
(X×Y)2

Lemma 2 Given an Lc1 -Lipschitz cost function c1, two joint distributions PAf and PBg, and a φ(α)-
probabilistic cross-domain Lipschitz function f*, we assume the input space X is bounded such that
kf*(X1) - f* (X2)k ≤ M1, then we have
EAf (f *) - EBg (f *) ≤W(PAf,PBg)+Lc1M1φ(α).
Proof Based on the definition of EAf (f*) and EBg (f*), we discuss the absolute value of the dif-
ference between them as follows:
IEA (f *) - EB (f *)∣
=IE(X,Y)〜Pf (X,Y) [c2 (Y, f * (X ))] - E(X,Y )~Pg (X,Y) [c2 (Y,f *(X ))]|
TZL yC2(γ,f *(X Rd(PA - PB )|	(18)
≤
(X ×Y)2
IIc2(Yf,f*(X))-c2(Y,f*(Xg))IIdP*(X,Yf;Xg,Y)
(19)
/
(X ×Y)2
IIc2(Yf,f*(X))-c2(Yf,f*(Xg))+c2(Yf,f*(Xg))-c2(Y,f*(Xg))IIdP*(X,Yf;Xg,Y)
≤
(X ×Y)2
IIc2(Yf,f*(X))-c2(Yf,f*(Xg))II+IIc2(Yf,f*(Xg))-c2(Y,f*(Xg))IIdP*(X,Yf;Xg,Y)
≤	hLc2 kf*(X)-f*(Xg)k+c2(Yf,Y)idP*(X,Yf;Xg,Y)
(X ×Y)2
≤
(X ×Y)2
hL1αc1 (X, Xg)+c2 (Yf, Y)i dP *(X, Y f; Xg, Y)+L1M1φ(α)
≤	hc1 (Xg, X)+c2 (Y, Yf)i dP*(X,Yf;Xg,Y) + L1M1φ(α)
=W(PAf,PBg)+L1M1φ(α).
(20)
(21)
(22)
(23)
12
Under review as a conference paper at ICLR 2019
Line (18) follows by the definition of EA(f *) and EB(f*) and the definition of the expectation
w.r.t. PA and PB. Line (19) holds by Lemma 1 and We choose the optimal coupling P*, i.e.,
P * = argminp ∈p(pf ,pg) E(x,γo χ,γ)〜P [c(X,Y 0； X 0,Y)].
Line (20) holds by the Lc2 -Lipschitz cost function c2 w.r.t. the second argument, i.e.,
∣C2(Yf,f*(X)) -c2(Yf,f*(Xg))∣ ≤ Lc2 If*(X) -f*(Xg)1,
and the triangle inequality of the cost c2, i.e., |c2(Yf, f*(Xg )) - c2(Y, f*(Xg ))| ≤ c2(Yf, Y).
Based on Definition 2, line (21) contains two cases: (1) f* and P* satisfies the φ-probabilistic
cross-domain Lipschitzness with probability 1 - φ(α); (2) the difference of f* between any two
instances is bounded by M1, and we have the additional term Lc1M1φ(α) that covers the regions
where the φ-probabilistic cross-task Lipschitzness does not hold, i.e.
Lc1
(X×Y)2
|f*(X) - f*(Xg)|dP*(X,Yf;Xg,Y)
≤
(X×Y)2
Lc1 αc2 (X, X g )dP * (X, Y f; X g ,Y) + Lc1 Mιφ(α).
Line (22) holds by the Lipschitz cost function c1, and line (23) uses the definition ofW(PAf, PBg).
Lemma 3 Given Lipschitz cost function c1 and c2, and the input instances are bounded for f* and
g*, i.e., kf*(x1)-f*(x2)k ≤ M1 and kg*(y1)-g*(y2)k ≤ M2, if Lc1 α=Lc2β=1, we have
EA(f) ≤Wc(PAf,PBg)+Lc1M1φ(α)+EA(f*)+EBg(f*),
EB(g) ≤ Wc(PAf, PBg) + Lc2M2φ(β) + EB(g*) + EAf (g*),
with probability at least 1 - δ.
Proof	Given Ea(f) = E(χ,γ)〜p4[c2(Y, f (X))] and Eβ(g) = E(χ,γ)〜PB [ci(X,g(Y))], with-
out loss of generality, we analyze the former as follows, and the latter is similar for the results.
EA(f) =E(X,Y)〜PA[c2(Y,f(X))]
≤E(X,Y)〜Pa [c2(Y,f*(X)) + c2(f*(X),f (X))]	(24)
=E(X,Y)〜Pa [c2(f (X ),f * (X))] + EA(f *)	(25)
=E(X,Y)〜Pf [c2(f(X),f*(X)] + EA(f*)	(26)
=EAf(f*)+EA(f*)
=EAf(f*) -EBg(f*)+EBg(f*)+EA(f*)
≤∣∣∣EAf(f*) -EBg(f*)∣∣∣+EBg(f*)+EA(f*)	(27)
≤Wc(PAf,PBg)+L1M1φ(α)+EBg(f*)+EA(f*).	(28)
Line (24) holds by an assumption that the loss function c1 satisfies triangle inequality. Line
(25) holds by a symmetric loss function c1 and the definition of EA(f*), i.e., EA(f*) =
E(x,y)〜PA [c2(Y, f *(X)]. Line (26) uses a fact that
E(X,Y)〜PA [c2(f(X),f*(X))] = E(X,f(X))〜Pf [c2(f(X),f*(X))] = EA(f*).
In order to prove Theorem 4, we introduce an important theorem that shows the concentration in-
equality for Wasserstein distance of the empirical measure and its true measure.
Theorem 3 (Empirical Concentration (Bolley et al., 2007), Theorem 1.1) Let μ be a probability
measure on Ω and μ = N PN=I δwi be the associated empirical measure defined on a sample of
independent variables {wi}N=ι drawnfrom μ. Then,forany d0 > dim(Ω) and C0 < C, there exists
13
Under review as a conference paper at ICLR 2019
some constant No depending on d0 and some square exponential monments of μ such that for any
> 0 and N ≥ N0 max{-(d0+2) , 1},
C0	2
P [W(μ,b) >e] ≤ e-F Nl
where C0 can be calculated explicitly, and C is such that μ VerfieSfor any measure V the Talagrand
(transportation) inequality TI(C) : W(μ,ν)≤(CH(ν∣μ)) 1 Withtherelativeentropy H, andTι(C)
holds when for some τ > 0 and R eτd(W,wO)2 dμ(w)<+∞,∀ w0∈Ω.
This theorem allows us to propose generalization bounds based on the Wasserstein distance. Now
we can use this theorem and the previous Lemma to prove the following theorem.
Theorem 4 Let P*=argmi□p∈p(Pf Pg)E(χ,γf ；xg,Y)~p[c(X, Yf; Xg, Y)] with Lipschitz cost
functions ci and c2. Let functions f * ∈F and g*∈G be probabilistic CrOSS-domain Lipschitzness
w.r.t. P * that minimizes the joint error Ea,b (f *, g*). Given M and N instances drawn form PX
andPY, respectively, with Lc1 α=Lc2 β =1, the following bound holds with probability at least 1-δ:
E(f,g) ≤
— ,.,,. ~,
+ eA,B(f , g ) + φ(α, β),
where Ea,b(f*,g*)=E∕(f*)+EB(f*)+Eb(g*)+EA(g*), E∕(f*)=E(χ,γ)~「人[。2(工/*(X))],
Ea (f *)=E(x,y )~Pa [c2(f(X ),f *(X))], and likewise for the definitions of EB (f *) and EB (g*).
Here, φe(α,β)=Lc1M1φ(α)+Lc2M2φ(β), where kf*(X1)-f*(X2)k≤M1,∀X1,X2∈X and
kg*(Y1)-g*(Y2)k≤M2, ∀Y1, Y2∈Y. Here, C0 is some constant satisfying Theorem 3.
Proof Based on the definition of E(f, g), we have
E(f, g) =E(X,Y)~Pa [c2(Y, f (X))] + E(X,Y)~Pb [c1(X, g(Y))]
≤2W (PA ,PB) + E (f *,g*)+ φ(α,β)
.∙^∙f∙^∙c.	, ^ -f	f .	.∙^∙c	C.	..	..	~ .	.
≤2W (PA, PB) + W (PA, PA )+w (PB, PBHE (f *,g* )+φ(α, β)
(29)
(30)
(31)
. . ~ , .
+ E(f *,g*) + Φ(α,β).
Line (29) holds by the conclusion in Lemma 3, and line (30) uses the triangle inequality for 1-
Wasserstein distance. In line (31), we apply the union bound of W(PbAf, PAf) and W(PbBg, PBg) (with
probability 2 for each) in Theorem 3.	口
C DETAILS OF OPTIMIZATION FOR GANY (PY , PG2) AND GANZ (QZ1, QZ2)
In this section, we discuss some details of GAN(PY, PG2) and GAN(QZ1, QZ2 ) as follows:
(1)	For GAN(PY, PG2), the loss function Ly can be written as the following optimization problem:
1N
Ly (E1,E2,G1,G2,Dy )=N7]Γ, 2log(Dy (yi))+log(1-Dy (G2(E1(G1(E2(%)))))
i=1	1 M	(32)
+ ~m Σ- 1 log(1-Dy(G2(EI(Xi)))).
M i=1
(2)	For GAN(QZ1, QZ2 ), we decompose it into GANZ (PZ, QZ1) and GANZ (PZ, QZ2), denoted
by Lz1 and Lz2 . Then we simultaneously optimize these two loss functions Lz1 and Lz2 , and they
can be formulated as the following optimization problems:
Lzi (E1,E2,G2,Dz)=M XM=I 2log(Dz(zi))+log(1-Dz(Eι(xi))),	(33)
1N
Lz2(E1,E2,G1,Dz)=方	2iog(Dz(zi))+log(l-Dz(E2(%))),	(34)
N i=1
where z is drawn from the prior distribution PZ, e.g., Gaussian distribution. Last, we let
Lz (E1,E2,G1,G2,Dz)=Lz1 (E1,E2,G2,Dz)+Lz2 (E1,E2,G1,Dz)
be the final loss for GAN(QZ1, QZ2 ).
14
Under review as a conference paper at ICLR 2019
Figure 4: Comparisons of different methods for season winter→spring translation on SYNTHIA
dataset. The figure shows all frames of a video synthesized and translated by these mehtods.
15
Under review as a conference paper at ICLR 2019
UNIT
Ours
AdaConv-Cycle
DVM-Cycle
DVF-Cycle
Figure 5: Comparisons of different methods for season winter→summer translation on SYNTHIA
dataset. The figure shows all frames of a video synthesized and translated by these mehtods.
16
Under review as a conference paper at ICLR 2019
Figure 6: Comparisons of different methods for season winter→fall translation on SYNTHIA
dataset. The figure shows all frames of a video synthesized and translated by these mehtods.
17
Under review as a conference paper at ICLR 2019
Figure 7: Comparisons of different methods for photo→segmentation translation on Cityscapes
dataset. The figure shows all frames of a video synthesized and translated by these mehtods.
18
Under review as a conference paper at ICLR 2019
Figure 8: Comparisons of different methods for segmentation→photo translation on Cityscapes
dataset. The figure shows all frames of a video synthesized and translated by these mehtods.
19
Under review as a conference paper at ICLR 2019
Figure 9: Comparisons of different methods for photo→segmentation translation on Cityscapes
dataset. The figure shows all frames of another video synthesized and translated by these mehtods.
20
Under review as a conference paper at ICLR 2019
Ours
UNIT
AdaConv-Cycle
DVM-Cycle
DVF-Cycle
Figure 10:	Comparisons of different methods for segmentation→photo translation on Cityscapes
dataset. The figure shows all frames of another video synthesized and translated by these mehtods.
21
Under review as a conference paper at ICLR 2019
E To AR1: Clear revision has been updated
Thanks for your helpful review. We have uploaded a revised version of the paper.
1)	We have revised the paper and provided a formal problem definition in Section 3.
2)	The definitions of EA(f*), EB(f*) were given in the supplementary material. We have also
defined them in Theorem 2 of the revised paper.
3)	We have revised the paper and unified notations as follows:
(a)	G2 ◦ E1 and G1 ◦ E2 denote the cross-domain mappings;
(b)	G1 ◦ E 1 and G2 ◦ E2 represent two Auto-Encoders;
(c)	M and N denote the number of samples in the X and Y domain, respectively;
(d)	F(s) . G(s) indicates that there exists C1, C2 > 0 such that F(s) ≤ C1G(s) + C2.
4)	Concerns on Q1, Q2 and Eqn. (17)
(a)	The definitions of Q1 , Q2 in Theorem 1 are reasonable. Taking Q1 as example, the encoder
Q(Z1 |X) must satisfy the distribution constraint QZ1 = PZ = QZ2 , PY = PG2. Similarly, we can
also obtain the constraint for Q2.
(b)	The Equality (17) does hold. Firstly, we decompose P(X, Z1) into P (X)P (Z1 |X). Then, we
use Q(Z1 |X) to replace the P(Z1 |X), and enforce its marginal distribution QZ1 = EX [Q(Z1 |X)]
to be identical to the prior distribution PZ, i.e., QZ1 = PZ. Meanwhile, the encoder Q(Z1 |X) can
be obtained from E1(X) and E2(G2(E1(X))) when QZ1 = QZ2 and PY = PG2.
5)	Concerns on Lemma 1
According to the duality theorem of Kantorovich-Rubinstein, we can choose any value greater or
equal than the considered cost function to satisfy the condition. For simplicity, we choose the
equality in the proof of Lemma 1.
6)	Formula (24) in Lemma 3 is an inequality.
7)	The sets Q1 and Q2 in Problem (4) are different from the definitions in Theorem 1, because the
constraints Q1 and Q2 are regularized as penalties. We have made them clearer in the revision.
22
Under review as a conference paper at ICLR 2019
F	To AR3: First work derived from a theoretical perspective
The reviewer undervalued the significance and novelty of the proposed method. We have highlighted
them in the revision.
1)	Novelty
Most image-to-image translation (I2IT) methods (e.g., Cycle GAN) are often designed without a
theoretical analysis, which, however, may limit the understanding and the learning performance.
Unlike existing methods, to our knowledge, our method is the first work to solve the I2IT problem
from a theoretical perspective. Essentially, our method can be regarded as a generalization of Cy-
cleGAN. More critically, we believe that our theoretical results would be helpful for understanding
the I2IT problem.
2)	Results of image translation
We have shown the results of image-to-image translation in Table 1 and Figures 2 and 3. Further-
more, we also conducted interpolation-based video-to-video synthesis to evaluate the quality of the
learned joint distribution, which is a better method to evaluate the generalization ability of JWAE.
3)	Generative power of the proposed method
On Cityscapes and SYNTHIA, we compare FID scores of the models trained with different distribu-
tion divergences and show the results in Table A. In general, lower FID indicates better performance.
From Table A and Figure 11, GAN-JWAE consistently outperforms other distribution divergences.
It is worth mentioning that our method is not restricted to the choice of distribution divergences.
Table A: FID results for different distribution divergences
Method	Cityscapes		SYNTHIA		
	SCene2segmentation	Segmentation2scene	winter2spring	winter2summer	winter2fall
-WGAN-JWAE-	134.40	84:00	12227	97:86	-86.52-
SN-WGANJWAE	128.01	87.60	117.05	97.72	85.57
GAN-JWAE	21.89	42.13	88.26	84.37	83.26
Methods
SN-WGAN-JWAE WGAN-JWAE	GAN-JWAE
Street scene
(Ground Truth)
Segmentation
Segmentation
(Ground Truth)
Street scene
Figure 11:	Comparisons of different methods on Cityscapes (left) and SYNTHIA (right).
23
Under review as a conference paper at ICLR 2019
G To AR2: Misunderstanding for experiments; Difficult
extension for I2IT and V2VS problem
The reviewer misunderstood the results in Table 1. We have described them clearly in the revision.
1)	& 2) Issues on baselines
Our method (JWAE) focus on different problem and different experiment settings from Bicycle-
GAN, Triple GAN and Triangle GAN. Specifically, JWAE focus on the unsupervised learning set-
ting for image-to-image translation and video-to-video synthesis tasks. In the training, JWAE is
trained on unpaired training data. However,
(a)	Bicycle-GAN requires paired data and is trained in a supervised setting;
(b)	The regularizations in Triple GAN are designed for the classification task and the semi-
supervised learning;
(c)	Triangle GAN requires the semi-supervised learning. Thus, the comparison between JWAE and
these methods is not fair.
2)	Concerns on MMD
On Cityscapes and SYNTHIA, we show the results using MMD and GAN, respectively. Note that
MMD is to measure the distribution divergence of embeddings. Here, we use FID and FID4Video
to evaluate the quality of images and videos, respectively. In general, lower score indicates better
performance. From Table B, MMD-JWAE can achieve the comparable performance with GAN-
JWAE.
Table B: FID results for different distribution divergences
Method	Cityscapes				SYNTHIA					
	Scene2segmentation		Segmentation2scene		Winter2spring		Winter2summer		Winter2fall	
	FID	FID4Video	FID	FID4Video	FID	FID4Video	FID	FID4Video	FID	FID4Video
MMD-JWAE	41.60	6.62	42.65	-23.48	89.72	-2T7T8	83.10	-19.88	86.56	15:99-
GAN-JWAE	22.74	6.80	43.48	25.87	88.24	21.37	77.12	17.99	87.50	14.14
3)	Concerns on W/O-Triple
There is a misunderstanding of the results in Table 1. The baseline method W/O-Triple indicates
that we remove the second term (i.e., cycle adversarial (CA) term) in the right-hand side of Loss (6),
instead of removing the whole Triple-GAN loss. We refer to this baseline as W/O-CA and we have
clarified this in the revision.
4)	Generalization ability of JWAE
We have shown the generalization ability of JWAE in the paper. According to (Bojanowski et al.,
2018), the generalization ability can be evaluated by the interpolation performance in the target
domain. From the results in Table 1 and Figures 2 & 3, our method consistently outperforms the
considered baseline methods both quantitatively and qualitatively.
5)	Difficulty of applying WAE to I2IT and V2VS tasks
It is very difficult to extent WAE to image-to-image translation (I2IT) and video-to-video synthesis
(V2VS) tasks, because there exists an intractable joint distribution matching problem. Moreover,
the original WAE aims at learning a generative model from noise to images, and thus it cannot be
directly applied in I2IT and V2VS problem.
24