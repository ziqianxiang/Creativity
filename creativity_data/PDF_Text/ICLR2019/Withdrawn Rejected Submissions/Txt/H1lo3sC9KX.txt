Under review as a conference paper at ICLR 2019
Asynchronous SGD without Gradient Delay
for Efficient Distributed Training
Anonymous authors
Paper under double-blind review
Ab stract
Asynchronous distributed gradient descent algorithms for training of deep neural
networks are usually considered as inefficient, mainly because of the Gradient de-
lay problem. In this paper, we propose a novel asynchronous distributed algorithm
that tackles this limitation by well-thought-out averaging of model updates, com-
puted by workers. The algorithm allows computing gradients along the process
of gradient merge, thus, reducing or even completely eliminating worker idle time
due to communication overhead, which is a pitfall of existing asynchronous meth-
ods. We provide theoretical analysis of the proposed asynchronous algorithm,
and show its regret bounds. According to our analysis, the crucial parameter for
keeping high convergence rate is the maximal discrepancy between local param-
eter vectors of any pair of workers. As long as it is kept relatively small, the
convergence rate of the algorithm is shown to be the same as the one of a sequen-
tial online learning. Furthermore, in our algorithm, this discrepancy is bounded
by an expression that involves the staleness parameter of the algorithm, and is
independent on the number of workers. This is the main differentiator between
our approach and other solutions, such as Elastic Asynchronous SGD or Down-
pour SGD, in which that maximal discrepancy is bounded by an expression that
depends on the number of workers, due to gradient delay problem. To demon-
strate effectiveness of our approach, we conduct a series of experiments on image
classification task on a cluster with 4 machines, equipped with a commodity com-
munication switch and with a single GPU card per machine. Our experiments
show a linear scaling on 4-machine cluster without sacrificing the test accuracy,
while eliminating almost completely worker idle time. Since our method allows
using commodity communication switch, it paves a way for large scale distributed
training performed on commodity clusters.
1	Introduction
Distributed training of deep learning models is devised to reduce training time of the models. Syn-
chronous distributed SGD methods, such as Chen et al. (2016) and You et al. (2017), perform train-
ing using mini-batch size of several dozens of thousands of images. However, they either require
expensive communication switch for fast gradient sharing between workers or, otherwise, introduce
a high communication overhead during gradient merge, where workers are idle waiting for commu-
nicating gradients over communication switch.
Distributed asynchronous SGD methods reduce the communication overhead on one hand, but usu-
ally introduce gradient delay problem on the other hand, as described in Chen et al. (2016). Indeed,
usually in an asynchronous distributed approach, a worker w obtains a copy of the central model,
computes a gradient on this model and merges this gradient back into the central model. Note, how-
ever, that since the worker obtained the copy of the central model till it merges its gradient back into
the central model, other workers could have merged their gradients into the central model. Thus,
when the worker w merges its gradient into a central model, that model may have been updated and,
thus, the gradient of the worker w is delayed, leading to gradient delay problem. We will refer to
algorithms that suffer from gradient delay problem as gradient delay algorithms, e.g. Downpour
SGD Dean et al. (2012).
1
Under review as a conference paper at ICLR 2019
As our analysis reveals in Section 3, the quantity that controls the convergence rate of an asyn-
ChronoUs distributed algorithm, is maximal pairwise distance - the maximal distance between local
models of any pair of workers at any iteration. Usually gradient delay algorithms do not limit this
distance and it may depend on the number of asynchronous workers, which may be large in large
clusters. This may explain their poor scalability, convergence rate and struggle to reach as high test
accuracy as in synchronous SGD algorithms, as experimentally shown in Chen et al. (2016).
While Elastic Averaging SGD Zhang et al. (2015) is also a gradient delay algorithm, it introduces
a penalty for workers, whose models diverge too far from the central model. This, in turn, helps
to reduce the maximal pairwise distance between local models of workers and, thus, leads to bet-
ter scalability and convergence rate. In contrast, our analysis introduces staleness parameter that
directly controls the maximal pair distance of the asynchronous workers.
Our analysis builds on the work of Zinkevich et al. (2009b), who studied convergence rate of gra-
dient delay algorithms, when the maximum delay is bounded. They provided analysis for Lipschitz
continuous losses, strongly convex and smooth losses. While they show that bounding staleness can
improve convergence rate of a gradient delay algorithm, in their algorithm each worker computes
exactly one gradient and is idle, waiting to merge the gradient with PS model and download the
updated PS model back to the worker.
The main contributions of this paper are the following. We present and analyze a new asynchronous
distributed SGD method that both reduces idle time and eliminates gradient delay problem. Our
main theoretic result shows that an asynchronous distributed SGD algorithm can achieve conver-
gence rate as good as in sequential online learning. We support this theoretic result by conducting
experiments that show that on a cluster with up to 4 machines, with a single GPU per machine and
a commodity communication switch, our asynchronous method achieves linear scalability without
degradation of the test accuracy.
2	Algorithm description
Below we describe two algorithms. We will use the terms model and parameter vector to refer to the
collection of trainable parameters of the model. Each worker in these algorithms starts computing
gradients from a copy of a PS model and, prior to merging with the central model at PS, the worker
can compute either a single gradient or several gradients, advancing the local model using all these
gradients. In the sequel we will use term model update of a worker - the difference between the
last local model prior to the merge and the latest copy of PS model, from which the worker started
computing gradients.
2.1	Asynchronous SGD, without gradient delay
The key idea of our approach is to reduce gradient delay, as described in Algorithm 1. To achieve
this goal, the merge process waits till each worker has at least one gradient computed locally. Then,
model updates from all the workers are collected and averaged and the average model update is
used to update the model at PS. This is the synchronous part of our hybrid algorithm. In this way
we replace gradient delay by averaging gradients, which is widely used as a technique to increase
mini-batch size.
Workers are allowed to compute gradients asynchronously to each other and to the merge process to
reduce wait times, when workers are idle. This is the asynchronous part of our hybrid algorithm.
Furthermore, the idea of computing several gradients to form a model update is used to hide com-
munication overhead of gradient merge with useful computation of gradients at the workers. Master
starts with computing the initial version of the model in line 27 and provides it for transferring to
workers. Then Master waits till each worker has computed at least one gradient and used it to ad-
vance the worker’s local model. When this happens, Master instructs on transferring a model update
w.∆x from each worker to PS, where model updates from all workers are averaged and the average
is used to advance the PS model. Finally the updated PS model is provided to all workers.
A worker starts with assigning value 0 to each variable, except variable staleness s, which is assigned
the maximal staleness τ . In line 5 the worker checks if the staleness s has already achieved its
maximal value and in this case it waits in line 6 till an initial version of the PS model is transferred to
2
Under review as a conference paper at ICLR 2019
the worker. When the PS model is transferred to the worker, in lines 9 and 10, the worker initializes
the two model variables x and xinit with the PS model, since at this stage model update w.∆x is
still 0. Next, the worker sets staleness to 0. Lines 13-15 comprise a usual update of the local model.
In line 16, the worker notifies Master on availability of a non-zero model update w.∆x. Now in line
17, the worker advances the local iteration counter i+ = 1. It also advances staleness s, since with
advancing the local model with one gradient, the local model gets far away from the latest version
of PS model, stored in line 9 in xinit , by one more gradient. In this way, the worker can perform
several iterations, computing a gradient and advancing the local model in lines 13-15, as long as the
current staleness s does not hit the maximal staleness τ .
Assume now that at some point in time, Master requests to transfer model update w.∆x to PS in
line 30. Each worker receives this request in line 21 of Thread 2. In this case, the worker releases
the model update in line 22 to transfer to PS to merge with the PS model and sets the local model x
to xinit , to indicate that the model update is re-initialized to 0. The value iinit is re-initialized to i
to indicate that the number of gradients, accumulated in model update x - xinit is 0.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Algorithm 1: Asynchronous SGD without Gradient Delay.
Worker Procedure: THREAD 1
Input: τ -maximal staleness, αi-learning schedule.
initialization: i = iinit = 0, s = τ, x = xinit = w.∆x = 0;
while true do
if s = τ then
I wait to receive model from PS: PS.x;
end
if P S.x is available then
P S.x;
xinit
x = xinit + w.∆x;
s = i - iinit;
end
compute gradient g(x);
x = x - αig(x);
w.∆x = x - xinit
notify Master on w.∆x;
i = i+ 1,s = s+ 1;
end
Worker procedure: THREAD 2
while true do
wait for a Master request to transfer w.∆x
provide w.∆x to transfer to PS.
xinit = x , iinit = i
end
Master Procedure.
initialization: W 一 the set of workers, PS.x = initial model;
provide PS.x to workers;
while not stop condition do
wait for each worker w to provide w.∆x.
request each worker w to transfer its model update ∆x.
for each worker w do
I PS.x = PS.x + ∣wW^ w.∆x
end
provide PS.x to workers;
end
When the worker discovers in line 8 of THREAD 1, that there is anew version ofPS model available
locally, it stores this model in xinit , advances the local model x from xinit using the model update
w.∆x and sets staleness to the number of gradients i - iinit, used to compute the local update.
3
Under review as a conference paper at ICLR 2019
Setting staleness to this value indicates that the local model diverges from the latest, locally available
PS model, by this number of locally computed gradients.
2.2	Simplified asynchronous SGD without gradient delay
Algorithm 1 is rather hard to analyze. To simplify the analysis, we formulate Algorithm 2 that is a
simplified version of Algorithm 1. Algorithm 2 incorporates the most of asynchrony of Algorithm 1:
each worker computes locally multi-gradient model update, workers compute their model updates
in parallel to each other and to synchronous merge of previous model updates from all the workers.
This means that if either the communication between workers is fast enough or staleness is large
enough, worker idle time can be eliminated completely. The only difference between the algorithms
is that Algorithm 2 synchronizes the time, when workers receive a new model from PS and provide
their model updates to PS to merge with the PS model.
More specifically, Algorithm 2 works in cycles of 2 iterations, where T is the maximal staleness.
To simplify notation, We denote β the cycle length, i.e. β = 2. At the end of each cycle, e.g. at
iteration jβ , all workers synchronize with PS: each worker acquires the copy of a new PS model
and provides its accumulated model update to PS. If PS does not have enough time to merge model
updates from the previous cycle and transfer the new model to the workers, workers wait for the new
PS model in line 6. When a worker receives the new PS model, it computes the model update w.∆x
in line 7 and provides it to PS in line 8. Note that due to the cycle length of β iterations, model
update w.∆x is computed using β locally computed gradients. Now, in line 9, the worker advances
the new PS model using the model update and sets the resulting model to xinit and x. Note that at
this point in time, at each worker, the parameter vector x is sum of the new PS model, which is the
same in all the workers, and the model update w.∆x that is computed using β gradients, computed
locally in the worker. This means that the worker can perform β additional iterations before hitting
the staleness boundary ofτ.
Now, after the synchronization with PS, each worker computes β gradients locally and uses them
to advance the local model in lines 11-13. At this point in time, each worker hits the staleness
boundary of τ . In parallel with this computation of model update in each worker, in lines 20-24,
Master transfers model updates that workers provided to PS in iteration jβ, merges them with PS
model and provides the new model to the workers. When each worker finishes iteration (j + 1)β,
the current cycle completes and, again, synchronization starts between workers and PS.
Note that workers in Algorithm 2 may be idle, waiting for the new PS model in line 6, only if the
communication between workers and PS is slow and does not allow transferring model updates from
workers to PS, merge them with the PS model and transfer the new PS model back to the workers
before workers complete computing β gradients. Also note that further increasing staleness, reduces
or completely eliminates worker idle time.
3	Analysis
In this section, we analyze Algorithm 2. In the analysis we adopt the notation of Zinkevich et al.
(2009b). In the proof of Lemma 3.4 and of Theorem 3.5, we use derivation similar to Zinkevich
et al. (2009b), while adding terms, which are relevant for our specific algorithms. The most of the
analysis is our own contribution. We will point out to parts that we borrow from prior research work.
Our main result is Theorem 3.9, where we show that the convergence rate of Algorithm 2 is O(T2 +
Tt). The algorithm has two phases. In the first phase, during the early iterations, the asynchronous
training leads to a slow-down, expressed in the term T2. However, in the second phase, during
later iterations, the asynchronous training appears to be harmless. We show that, ifT T, the
convergence rate of Algorithm 2 is as good as in sequential online learning.
Note that the convergence rate, stated in Theorem 3.9, assumes that the standard deviation of gradient
computation is inversely proportional to the maximal staleness T . In practice, this means that for
each given value of maximal staleness T, one needs to set the size of mini-batch at each worker so
that the standard deviation of gradient computation gets below 1.
4
Under review as a conference paper at ICLR 2019
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
Algorithm 2: Simplified Asynchronous SGD without Gradient Delay.
Worker Procedure.
Input: T-even maximal staleness, β = 2 and ɑi -learning schedule.
initialization: i = iinit = 0, x = xinit = w.∆x = 0;
while true do
if i ≡ 0 (mod β) then
wait to receive model from PS: PS.x ;
w.∆x = x - xinit;
provide w.∆x to transfer to PS;
x = xinit = PS.x + w.∆x;
end
compute gradient g(x);
x = x- αig(x);
i=i+1;
end
Master Procedure.
initialization: W 一 the set of workers, PS.x = initial model;
provide PS.x to workers;
while not stop condition do
wait for each worker w to provide w.∆x.
request each worker w to transfer its model update ∆x.
for each worker w do
I PS.x = PS.x + ∣wW^ w.∆x
end
provide PS.x to workers;
end
Now We compare our main result, stated in Theorem 3.9 with the main result of Zinkevich et al.
(2009a), stated in Theorem 8, where the convergence bound is O(T2 logT + √T). While the be-
havior in the second phase is the same O(√T), in the first training phase, our result O(T2) is better
than in Zinkevich et al. (2009a) O(T2logT), since our result does not have the dependency on T.
Denote convex (cost) functions by fi : X → R, and their parameter vector by x. Our goal is to
find a sequence of xi such that the cumulative loss Pi fi(xi) is minimized. With some abuse of
notation, we identify the average empirical and expected loss both by f *. This is possible, simply
by redefining p(f) to be the uniform distribution over F . Denote by
f* = 1 Xfi(x) or f*(x) = Ef〜p(f)[f(x)] and x* = argmin f*(x).
F i	x∈X
the average risk. We assume that x* exists (convexity does not guarantee a bounded minimizer) and
that it satisfies ||x* || ≤ R (this is always achievable, simply by intersecting X with the unit-ball of
radius R).
We remind that in Algorithm 2, in each cycle each worker computes an update to its local parameter
vector, based on β locally computed gradients and PS averages these per-worker updates from all
the workers to update its own parameter vector.
In Algorithm 2 at time t, each worker w computes the gradient of the same function f on its own
parameter vector xt,w and its own mini-batch. We denote this function at worker w and time t
ft,w and denote the gradient of this function, computed at the local parameter vector xt,w , gt,w =
▽ft,W (Xt,w ).
For the analysis, we define the global parameter vector at time t (as opposite to per-worker parameter
vector xt,w) as average of per-worker parameter vectors
xt
IW X
w∈W
xt,w .
(1)
5
Under review as a conference paper at ICLR 2019
Also we denote
ft(Xt) = ∣ww∣ X ft,w (Xt).
(2)
We assume that each ft,w, and thus ft, is convex, and subdifferentials of ft are bounded ∣∣Vft(x)∣∣ ≤
L by some L > 0. Denote by x* the minimizer of f* (x). We want to find a bound on the regret R,
associated with a sequence X = X1, ..., XT of parameter vectors
T
R[X] =Xft(xt)-ft(x*) .	(3)
t=1
Such bounds can be converted into bounds on the expected loss, as in Shalev-Shwartz & Singer
(2007), for an example. Note that with the definitions (1) and (2), the regret in (3) is well-defined.
We denote
gt = Vft(Xt) = ∣W∣ X vft,w (Xt).
w∈W
(4)
This is how a gradient is computed in synchronous distributed SGD algorithms. Since all ft are
convex, we can upper bound R[X] via
TT
RX] ≤ X < Vft(xt), Xt- X* >= X < gt, Xt- X* > .	(5)
Let US define a distance function between X and x0: D(x∣∣x0) = 1 ∣∣x 一 x0∣∣2. In Algorithm 2, each
worker at the beginning of cycle i, i.e. at time iβ, receives the copy ofa new PS parameter vector. In
Lemmas 3.1 — 3.3 We study properties of Algorithm 2 at this point in time. In Lemma 3.1, We show
that the copy of PS model at time iβ, equals to the average of local parameter vectors of workers
from iteration (i - 1)β, which, according to (1), equals to x(i-1)β.
Lemma 3.1. The copy ofa new PS parameter vector that each worker receives at iteration (i+ 1)β,
equals to the average of local parameter vectors of workers from iteration iβ
PS.x(i+1)β
W∣∖ X
w∈W
xw,(i+1)β
x(i+1)β .
(6)
The proof may be found in Appendix A. Next, when a worker receives at iteration iβ the copy
of a new PS parameter vector, which, according to Lemma 3.1, equals to x(i-1)β, it adds to this
parameter vector its latest model update Xw,iβ J x(i-i)β + ∆w.x. Note that this operation resets
all the local parameter vectors of the worker, computed in the last cycle: the operation moves all the
local parameter vectors {xw,(i-1)β+t ∣t = 0, . . . , β} along the vector xw,(i-1)β - x(i-1)β towards
the average parameter vector x(i-1)β :
xw,(i-1)β+t J xw,(i-1)β+t + (xw,(i-1)β - x(i-1)β), t = 0, . . . , β .	(7)
Lemma 3.2 shows that computing average parameter vector (1) is invariant under rest operation (7).
Lemma 3.2. Computing average parameter vector, according to (1), is invariant to the reset oper-
ation (7), i.e. for each i ∈ N
xiβ+t
西 X (Xw
w∈W
,iβ+t + (xw,iβ - xiβ)) , t
0, . . . , β .
(8)
The proof may be found in Appendix A. Now, Lemma 3.3 bounds the distance between parameter
vectors for any pair of workers after the reset operation at iteration iβ .
Lemma 3.3. Suppose gradients of cost functions ft are bounded ∣∣Vft(x)∣∣ ≤ L by some L. Let
w, w0 ∈ W be any two workers. Then after the reset operation (7), at iteration iβ,
∣∣xiβ,w - xiβ,w0 ∣∣ ≤ Lτ η(i-1)β .	(9)
6
Under review as a conference paper at ICLR 2019
The proof may be found in Appendix A. After the reset operation at iteration iβ, in the next β
iterations, each worker computes β gradients locally to advance its local parameter vector
xiβ+t,w = xiβ+t-1,w - ηiβ+t-1giβ+t-1,w , 1 ≤ t ≤ β .
Using this expression, we can re-write (1)
xiβ+t
χiβ+t-i - ηiβ+t-i|W| X giβ+t-i,w .
w∈W
(10)
We abuse notation and denote an average over gradients giτ +t,w as
giτ+t = giτ+t(XiT+t)
|W| X giτ+t,w .
w∈W
(11)
Note the difference between (4) and (11). We use expression (4) as one way to define the average
gradient, with which we start to bound regret in (5). In this expression, each gradient uses the
same parameter vector, defined in (1), to compute its gradient. Expression (11), is another way to
compute an average gradient, where each worker uses its own local parameter vector to compute
gradient. With notation (11) we rewrite (10)
xiβ+t = xiβ+t-1 - ηiβ+t-1giτ+t-1 , t = 1 ...,β .
(12)
Next, to prove our regret bounds, we adapt Lemma 1 from Zinkevich et al. (2009b). In Zinkevich
et al. (2009b), Lemma 1 is proved for an asynchronous algorithm, in which each worker computes
exactly one gradient and transfers it to PS to merge with its parameter vector. We adapt this lemma
to Algorithm 2, in which each worker computes β gradients locally and then all the workers transfer
their updates of their local parameter vectors to PS, which averages them and uses this average to
update its own parameter vector.
Lemma 3.4. For all x* ,for all i and 0 ≤ t < β, if X = Rn ,thefollowing expansion holds:
< Xiβ+t -X*,giβ+t >=1 ηiβ+t∣∣giβ+t∣∣2 + D(X*llxiβ+t)- D(X*llxiβ+t+1)
2	ηiβ+t
+ < Xiβ+t - x* ,giβ+t - giβ+t > .	(13)
The proof may be found in Appendix A. The decomposition (13) is very similar to standard regret
decomposition bounds, such as Zinkevich et al. (2009a). We add to this decomposition a new term
< Xiβ+t - x*,giβ+t - giβ+t > to adapt the analysis to peculiarities of Algorithm 2. This term
characterizes the difference between two ways to compute an average gradient at a specific time.
In Algorithm 2, at iteration iβ, for each i, each worker starts computing gradients after it adds its
local update of its local parameter vector to the copy of a common PS parameter vector and computes
β gradients locally. As workers compute more gradients in iterations iβ+t, t = 1, . . . , β, their local
parameter vectors get more far apart from each other. However, for sufficiently small step size, the
distance between local parameter vectors in different workers remains small. The key to proving our
bounds is to impose further smoothness constraints on ft . The rationale is quite simple: we want
to ensure that small changes in X do not lead to large changes in the gradient. More specifically we
assume that the gradient of ft is a Lipschitz-continuous function. That is,
∣∣Vft(x) -Vft(X0)Il ≤ H∣∣x - x0∣∣ .	(14)
for some constant H. Following Theorem 2 from Zinkevich et al. (2009b), we use Lemma 3.4 to
prove Theorem 3.5. The key difference between Theorem 3.5 and Theorem 2 from Zinkevich et al.
(2009b), is that we introduce a new expression and bound it to adapt the analysis to Algorithm 2.
Theorem 3.5. Suppose gradients of cost functions ft are bounded ∣∣Vft(X)∣∣ ≤ L by some
L and that H also upper-bounds the change in the gradients, as in (14). Also suppose that
maxχ,χo∈x D(x∣∣x0) < F2. Given η = √t, for some constant σ > 0, the regret of Algorithm 2 is
bounded by
一一	CLF 2L	I------
R[X] ≤ σL2 VT + —T+ + 2τFLH(2τ + 2σ√T - 2τ) .	(15)
σ
7
Under review as a conference paper at ICLR 2019
Consequently, for σ2 = 2Fj, we obtain the bound
RX] ≤ 4FLHτ2 + F(3L + 4FH)√ΓT .
(16)
The proof may be found in Appendix A. According to Theorem 3.5, the convergence rate of Algo-
rithm 2 is O(T2 + √τT). As in Zinkevich et al. (2009a), We note that this result is expected, because
an adversary can rearrange training instances so that each worker receives training instances that
are very similar to training instances in other Workers during τ asynchronous iterations. In this case
multiple Workers do not perform better than a single Worker and, thus, parallel algorithm is not better
than a sequential one.
We Will use the results of Theorem 3.5 to prove our main result in Theorem 3.9
3.1 Decorrelated gradient analysis
In this section We assume that training samples at different Workers are draWn independently from
the same underlying distribution. Also, to improve the convergence rate, We should limit the diver-
gence of different Workers from each other, as they asynchronously advance their local parameter
vectors. Namely, We introduce an assumption on variance of gradients, computed in the same point
at different workers. We denote g* = Vf * and assume that variation of gradients at different work-
ers, is modeled by an additive Gaussian noise Vft,w(x) = g*(x) + et,w, for et,w 〜 N (0, C), where
C is a covariance matrix.
We start with the following lemma that bounds the distance between parameter vectors at any two
workers w, w0 ∈ W after each one of them makes j asynchronous steps, starting from parameter
vectors xiβ,w and xiβ,w0 .
Lemma 3.6. Assume that variation of the gradient of the cost function is governed by an additive
Gaussian noise
Vft,w (x) = g* (x) + et,w ,	(17)
for et,w 〜N(0, C) for covariance matrix C. Thenfor any two worker w, w0 ∈ W,
j-1
llxiβ+j,w - xiβ+j,w0 || ≤ ||xie,w - xiβ,w0 || ɪ ɪ (∏iβ+kH + I)
k=0
j-1	j-1
+〉: ηiβ+k ||eie+k,w - eiβ+k,w0 || ∙ ɪ ɪ (ηiβ+mH + I) .	(18)
k=0	m=k+1
The proof may be found in Appendix A. Lemma 3.6 bounds the distance between local parameter
vectors in two different workers, as they proceed with asynchronous iterations. Lemma 3.7 uses this
bound to prove a bound on the expected value of this difference.
Lemma 3.7. Denote var(∣et,w |) = S Also, in addition to the conditions OfLemma 3.6, assume that
the cost functions ft are i.i.d. Then, for
t ≥ t0 = (10στ H)2 ,	(19)
the expectation of the difference ∣∣Xiτ+j,w - XiT+j,w0|| is bounded by
Ellxiβ+j,w - xiβ+j,w0 || ≤ 1.2 ∙ E||xie,w - xiβ,w0 || + 1.2 ∙ ηiβτs .	(20)
The proof may be found in Appendix A. Using (20) for j = β, before reset operation (7),
Ellx(i+1)β,w - x(i+1)β,w0 || ≤ 1.2 ∙ EllXiβ,w - xiβ,w0 || + 1∙2 ∙ ηiβτs ∙	(21)
After reset operation, updates of local parameter vectors of w and w0, computed in iterations t =
iβ + 1, . . . , (i + 1)β, are added to the common copy of a PS model. This results in reduction of
RHS of (21) by llXiβ,w - Xiβ,w0 ll, resulting in
Ellx(i+1)β,w - x(i+1)β,w0 || ≤ 0.2 ∙ E llxiβ,w - xiβ,w0 || + 1∙2 ∙ ηiβτs ∙	(22)
In (22), the bound on expected distance between local parameter vectors of workers w and w0 at
iteration (i + 1)β, depends on the distance at the previous cycle, at iteration iβ. Lemma 3.8 develops
a similar bound without the dependence on the distance from the previous cycle.
8
Under review as a conference paper at ICLR 2019
Lemma 3.8. Let i0 be the smallest index, s.t. i0β ≥ t0, for t0 defined in (19). Then, for
, S
jo = Iog 斤，	(23)
L
and any i ≥ i0 + j0,
Ellx(i+1)β,w - x(i+1)β,w0 || ≤ 2 ∙ ηiβτs .
(24)
The proof may be found in Appendix A. Now, when we bounded the difference between parameter
vectors in different workers, we can use this bound along with the assumption (14) to bound the
difference between two ways to compute an average gradient: (4) and (11). This leads to our main
result - bound on the expected regret of Algorithm 2.
Theorem 3.9. In addition to the conditions of Theorem 3.5, assume that the cost functions ft are
i.i.d. and that variation of gradients of the cost functions is governed by an additive Gaussian law,
i.e. Vft,w (x) = g*(x) + et,w ,for et,w 〜N (0,C), St
T ∙ var(et,w) ≤ 1 .	(25)
Given η = √ for some constant σ > 0, the expected regret ofAlgorithm 2 is bounded asfollows
ER[X] ≤ 4FLH(1 + 2σ2H)τ2 + (σL2 + F2 + 4FHσ) √T .	(26)
Consequently, for σ
F
L,
ER[X] ≤ 4FLH
2FL + 4 F2 H) √T.
(27)
The proof may be found in Appendix A.
4 Experimental results
In this section we provide an initial experimental support for the effectiveness of our algorithm
and show discrepancy in local parameter vectors of the asynchronous workers in two approaches:
averaging models updates and gradient delay. We start with the discrepancy. For this experiment we
trained ResNet50 model (Szegedy et al. (2017)) on CIFAR-10 data (Krizhevsky et al.). We show the
results in Figure 1, where different gradient average plots correspond to different values of staleness.
To produce a point in a gradient average plot for a given values of staleness and the number of
workers, each worker starts computing an update to its local parameter vector from a parameter
vector that is common to all the workers. Workers proceed computing the gradients, updating their
local parameter vectors asynchronously until each worker computes the number of gradients, as the
chosen value of staleness. At this point we compute the average over the last parameter vectors in the
workers. Next we compute the average distance between the last parameter vectors of the workers
and the average parameter vector. This average distance is plotted in Figure 1 for the corresponding
values of staleness and the number of workers.
To produce a point in the gradient delay plot, we simulate the worst case in gradient delay: workers
read the central parameter vector PS.x in a sequence. The first worker reads the initial value of
P S.x, the second after PS.x is advanced with one gradient, the third after PS.x is advanced with
two gradients, etc., until the last worker read PS.x after it is advanced the number of times as the
number workers minus 1. Each worker computes only one gradient and uses it to advance its local
parameter vector. Since the last value of PS.x represents the most up-to-date model in the system,
the average distance is now computed between this model and all the local models of the workers.
This average distance is shown in the gradient delay plot.
As we see, from Figure 1, staleness is an effective tool to keep low discrepancy of local parameter
vectors of asynchronous workers - the discrepancy is roughly linear in the value of the maximal
staleness parameter. In contrast, in gradient delay, this discrepancy is linear in the number of workers
and, thus, is limited only by the number of workers, which may be large in very large clusters.
9
Under review as a conference paper at ICLR 2019
Figure 1: Average distance for various values of staleness and number of workers.
Next, we provide an initial experimental support for scalability of our algorithm. We trained
GoogleNet model (Szegedy et al. (2016)) on ImageNet ILSVRC12 dataset (Russakovsky et al.
(2015)) on a cluster with 4 machines. Each machine was equipped with a single Nvidia GeForce
GTX TITAN X GPU card. The machines were interconnected using 1 giga bit communication
switch. We used mini-batches of size 32 images and set the maximal staleness parameter to the
value of 16. Also we implemented a linear scaling rule, where training with n workers, we increase
the learning rate n times and reduce the number of iterations in each worker n times. We added a
linear warm-up of 50K iterations to gradually increase the base learning rate from 0.01 to n ∙ 0.01.
After training, we test the resulting model using ImageNets own 50,000 images validation set. From
Figure 2 and Figure 3, we see that our method achieves linear scalability without degradation of
the test accuracy. We measured average idle time of workers and found it to be practically 0 for
staleness value of at least 8.
number of workers
88.15
A。区 n8els 名 s—dol
IJ
Figure 2: Speedup vs the number of workers.
Figure 3: Top-5 test accuracy.
5	Conclusions
We presented a new asynchronous distributed SGD method. We show empirically that it reduces
both idle time and gradient delay. We analyze the synchronous part of the algorithm, and show
theoretical regret bounds.
The proposed method shows promising results on distributed training of deep neural networks. We
show that our method eliminates waiting times, which allows significant improvements in run time,
compared to fully synchronous setup. The very fact of efficient hiding of communication overhead
opens opportunity for distributed training over commodity clusters. Furthermore, the experiments
show linear scaling of training time from 1 to 4 GPU’s without compromising the final test accuracy.
10
Under review as a conference paper at ICLR 2019
References
Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting dis-
tributed synchronous sgd. 2016.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and Andrew Y. Ng. Large scale
distributed deep networks. pp. 1223-1231, 2012. URL http://papers.nips.cc/paper/
4687-large-scale-distributed-deep-networks.pdf.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). URL http://www.cs.toronto.edu/~kriz/cifar.html.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Shai Shalev-Shwartz and Yoram Singer. Online learning: Theory, algorithms, and applications.
2007.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818-2826, 2016.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4,
inception-resnet and the impact of residual connections on learning. In AAAI, pp. 4278-4284.
AAAI Press, 2017.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training.
CoRR, abs/1708.03888, 2017. URL http://arxiv.org/abs/1708.03888.
Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In
Advances in Neural Information Processing Systems, pp. 685-693, 2015.
Martin Zinkevich, John Langford, and Alex J Smola. Slow learners are fast. In Advances in neural
information processing systems, pp. 2331-2339, 2009a.
Martin Zinkevich, John Langford, and Alex J Smola. Slow learners are fast. In Advances in neural
information processing systems, pp. 2331-2339, 2009b.
Appendix A Proofs
Proof of Lemma 3.1.
Proof. We prove this statement by induction on i. At i = 0, the statement holds, since in the begin-
ning all workers start from the common random model. Now assume that the statement holds for i.
At iteration iβ, PS collects update to local parameter vector from each worker xw,iβ - xw,(i-1)β,
averages them and uses the average to update its own parameter vector
PSxie = PS.x(i-1)β + |W| X (Xw,iβ - xw,(i-1)β)
w∈W
=PS.χ(i-1)β + |W| X xw,iβ - |W| X xw,(i-1)e
w∈W	w∈W
(28)
According to the inductive assumption, the last summand in (28) equals PS.x(i-i)e.	□
Proof of Lemma 3.2.
11
Under review as a conference paper at ICLR 2019
Proof. We start with RHS of (8)
i⅛ X(XW
1	1 w∈W
,iβ+t + (XW,iβ - xiβ))
1	1
西 2-^xw<iβ+t + 西
E Xw,iβ
w∈W
-xiβ .
(29)
By the definition of average parameter vector (1), the first summand in RHS of (29) equals Xiβ+t,
while the last two summands cancel each other.	□
Proof of Lemma 3.3.
Proof. At iteration (i — 1)β after reset operation (7), workers w and w0 start updating their local
parameter vectors for β iterations, so that at the end of the cycle before the reset operation (7),
β-i
xiβ,w - xiβ,w0 = x(i-1)β,w -〉: η(i-1)β+kg(i-1)β+k,w
k=0
，	β-i
X(i-1)β,w0 -	η(i-1)β+kg(i-1)β+k,w0
、	k=0
β-1
(x(i-1)β,w - x(i-1)β,w0) -	η(i-1)β+k(g(i-1)β+k,w - g(i-1)β+k,w0) ∙
k=0
(30)
The reset operation (7) at iteration iβ adds the updates of workers W and w0, computed at iterations
(i - 1)β + 1,...,iβ, to the common copy of PS model, so that X(i-i)β,w 一 X(i-i)β,wz = 0. Thus,
after the reset operation
β-i
xiβ,w - xiβ,w0 = -〉: η(i-1)β+k (g(i-1)β+k,w - g(i-1)β+k,w0 ) ∙
k=0
(31)
Finally, from (31), our assumption on the size of the gradient Nft(X) and from decreasing learning
rate, during this cycle that started at iteration (i - 1)β, the distance between workers W and w0 grows
at most by
llxiβ,w - xiβ,w0 ll ≤ L2βη(i-1)β = Lτη(i-1)β .
Proof of Lemma 3.4.
Proof. We decompose our progress as follows
D(X"Xiβ+t+1) - D(X*llxiβ+t)
=2 ||x* - xiβ+t + xiβ+t - xiβ+t+1ll2 - $ ||x* - xiβ+tll2
=2 ||x* - xiβ+t + ηiβ+tgiβ+tll2 - 2 ||x* - xiβ+tll2
(32)
(33)
2 ηiβ+t llgiβ+t Il
2 ηiβ+t llgiβ+tll
-ηiβ+t < xiβ+t - x*, giβ+t >
-ηiβ+t < xiβ+t - x*, giβ+t - giβ+t + giβ+t >
2 ηiβ+t llgiβ+tll
-ηiβ+t < xiβ+t - x*,giβ+t > +ηiβ+t < xiβ+t - X*,0iβ+t - giβ+t
To prove (33) from (32), we used (12) Dividing both sides by 巾丁+t and moving < XiT+t -
X*,%τ+t > to the LHS completes the proof.
Proof of Theorem 3.5.
—
□
2
2
> .
□
12
Under review as a conference paper at ICLR 2019
Proof. First we state a useful inequality: For n vectors ai, i = 1,...,n (by induction on n):
n
X a,
i=1
Also we will use the following sum bounds:
n
Xi
i=1
and
1
^2√i ≤
i=a
√b — √a — 1 ≤ √b — a +1 .
n
≤ X |a,|
i=1
n(n + 1)
-2
(34)
(35)
(36)
We start with summing (13) along iterations:
T
X < xt — χ*,gt >
t=1
TT
X 2 ηt∣∣gtll2 + X
D(x* llχt) — D(x*llχt+1)
ηt
T
+	< xt — x* ,gt — gt >
t=1
=X [ 1 ηtlkll2l + DXJXi - D(X*"XT+1) + X ∣D(x*llxt)(--
t=1 L2	」 ηι	ητ	t⅛ L	H
T
+ X < Xt- x*,gt - gt > .	(37)
t=1
Next, we borrow the derivation of expressions (38) and (39) from the proof of Theorem 2, Zinkevich
et al. (2009a). Note, however, that we added a new term to (37) - the last term that is specific to
Algorithm 2. This term does not appear in the proof of Theorem 2, Zinkevich et al. (2009a). By
the Lipschitz property of gradients and the definition of ηt, we can bound the first summand of the
above regret expression via
(38)
TT
X2ηtllgtH2 ≤ X 2ηt-W X Hgt,wH2
t=1	t=1	w∈W
TT
≤ X 1 ηtL2 = X 1 :L2 ≤ σL2√T.
≤ t=12 ηt	t=1 2 √	≤
Also
D™ + XD(x*llxt) [ɪ - ɪ 1 ≤ 严√T
η1	M	Lηt	ηt-d	σ
(39)
We omit the negative factor — D(X )jτ+1). Now we start the analysis of the last term in (37), which
is new and specific to Algorithm 2. Using (34), we bound the last summand of (37)
T	T
< xt - x*,gt - Gt >	≤ X llxt - x*lHIgt-Gtll.	(40)
t=1	t=1
Substituting (38), (39) and (40) into (37), we get
F 2	、T、
R[χ ] ≤σL2√T+—√T+E Hxt- x*ll∙ngt- gtH,	(41)
t=1
13
Under review as a conference paper at ICLR 2019
Next, we proceed with bounding the last multiplicative factor in the RHS of (40)
∖∖gt - gt∖∖ =
焉 X gt,w - ɪ X VftW(Xt)
I I w∈w	I I w∈w
1
西
£ (gt,w - Vft,w(χt))
w∈W
1	L	一	—一 ,… H	L	-
≤ ∖w	E	∖∖gt,w- vft,w(Xt)Il	≤ ∖w	E	∖∖χt,w-χt∖∖ •
(42)
In (42) we used inequality (34) and (14). Also note that if t is the last iteration in a cycle, gt,w is
computed on the parameter vector before the reset operation (7) is applied. Now we bound each
summand in (42). For t = iβ + j
j-1	1(	j-1
xiβ+j,w - xiβ+j = xiβ,w + ɪ2 -%β+kgiβ+k,w - 7W7 ɪ2 I xiβ,w0 + ɪ2 -%β+kgiβ+k,w0
k=0	∖	∖ w0∈W ∖	k=0
1	jT	1
而 f(x,β,w-X,β,wA- Eηiβ+k ιw Σ (giβ+k,w - giβ+k,w0 )
∖	∖ w0∈W	k=0	∖	∖ w0∈W
(43)
Using Lemma 3.3, the assumption that gradients of cost functions are bounded by L, and learning
rate decreases as iterations increase,
j-1	1
∖∖xiβ+j,w - xiβ+j ∖∖ ≤ Lτη(i-1)β + y^ηiβ ^W £ 2L ≤ 2LTn(I-1)β .
k=0 w w w0∈W
Substituting (44) into (42), gives
∖∖giβ+j
-giβ+j W ≤ 备
E 2LTn(i-i)β
w∈W
≤ 2τLHn(i-i)β .
(44)
(45)
Now, using the fact that learning rate is a decreasing function and the assumption that distance
between any pair of points is bounded by F2, we substitute (45) into (40) to get
T
V xt - x^,gt - gt >
t=1
TT
≤ X 2FτLHnt-2τ = 2FτLH X nt-2τ .
(46)
Note that for t < 1, we use nt = 1. Next, we separate the iteration t into two ranges: t = 1,..., 2τ
and t > 2τ
2τ	T-2τ	T-2τ 1
X nt-2τ + X nt = 2τ + 2σ X -√ ≤ 2τ + 2σ√T - 2τ .	(47)
t=1	t=1	t=1 2 t
Substituting (47) into (46)
T
〉：< Xt- x*, gt - gt > ≤ 2τFLH(2τ + 2σ√T - 2τ).
t=1
(48)
Substituting (38), (39) and (48) into (37), we prove (15). Using σ2 = 2F^ in (15), we prove
(16)	.	□
Proof of Lemma 3.6.
Proof. We prove the lemma by induction on j. For j = 0, the statement of lemma holds. Now
assume that the lemma holds for j - 1. Then
∖∖xiβ+j,w - xiβ+j,w0 ∖∖ = ∖∖xiβ+j-1,w - xiβ+j-1,w0
-niβ+j-1(g* (Xiβ+j-1,w ) - g*(xiβ+j-1,wz))
-niβ+j-1(eiβ+j-1,w - eiβ+j-1,w0 )∖∖
≤ ∖∖xiβ+j-1,w - xiβ+j-1,w0 ∖∖ + niβ+j-1 ∖∖g (xiβ+j-1,w ) - g (xiβ+j-1,w0 ) ∖∖
+ niβ+j-1∖∖eiβ+j-1,w - eiβ+j-1,w0 ∖∖ .
14
Under review as a conference paper at ICLR 2019
We use (14) in the above expression
∖∖xiβ+j,w - xiβ+j,w0 Il
≤ (ηiβ+j-1H + 1)∖∖xiβ+j-1,w - xiβ+j-1,w0 ∖∖ + ηiβ+j-1∖∖eiβ+j-1,w - eiβ+j-1,w0 ∖∖
Now, applying the inductive assumption
j-2
∖∖xiβ+j,w - xiβ+j,w0 ∖∖ ≤ (niβ+j-1H + 1) ∖∖χiβ,w - xiβ,w0 ∖∖ , H(ηiβ+k H + I)
k=0
j-2	j-2
+ (ηiβ+j-1H + I) Eniβ+k ∖∖eiβ+k,w - eiβ+k,w0 ∖∖ ∙ ɪɪ (ηiβ+mH + I)
k=0	m=k+1
+ ηiβ+j-1∖∖eiβ+j-1,w — eiβ+j-1,w0 ∖∖	(49)
Inserting (力丁+j-1H + 1) into product in the first summand in (49) amd into the sum in the second
summand,
j-1
∖∖xiβ+j,w — xiβ+j,w0 ∖∖ ≤ ∖∖xiβ,w — χiβw∖∖ ∏(ηiβ+kH + 1)
k=0
j-2	j-1
+	ηiβ+k ∖∖eiβ+k,w — eiβ+k,w0 ∖∖∙ ɪɪ (ηiβ+mH +1) + 6β+j-1 ∖∖eiβ+j-1,w — eiβ+j-1,w0 ∖∖
k=0	m=k+1
(50)
Now, note that the last summand in (50) corresponds to the summand in (18) for k = j — 1. This
completes the proof.	□
Proof of Lemma 3.7.
Proof. First note that from (19) it follows that
ηtβH ≤ 0.1 .	(51)
Since ηi shrinks down, as i grows up, and ηtH + 1 > 1, (18) yields
∖∖xiβ+j,w — xiβ+j,w0 ∖∖ ≤ (ηiβH + 1)”
β	β-1	ʌ
• I ∖∖xiβ,w — xiβ,w0 ∖∖ + ηiβ〉: ∖∖eiβ+k,w — eiβ+k,w0 ∖∖ ) .	(52)
k=0
After the expansion of (ηiβ H + 1)β in (52) into Taylor sequence and applying a simple algebra
(ηiβH + 1)β ≤ 1 + βηiβH + (βηiβH)2 (1 — βηiβH)-1	(53)
From (51) we can bound
(ηiβH +1)β ≤ 1.2 .	(54)
Assigning (54) into (52),
β-1
∖∖xiβ+j,w — xiβ+j,w0 ∖∖ ≤ 1.2 • ∖∖xiβ,w — xiβ,w0 ∖∖ + 1 ∙2 ' ηiβ〉: ∖∖eiβ+k,w — eiβ+k,w0 ∖∖ .	(55)
k=0
Since e^ +k,w and e^ +k,wz are i.i.d. with mean 0,
E∖∖eiβ+k,w — eiβ+k,w0 ∖∖ = var(eiβ+k,w — eiβ+k,w0) = 2var(eiβ+k,w ) .	(56)
Taking expectation of the both sides of (55), substituting (56) into the resulting inequality and using
the assumption on variance of random variables et,w, we complete the proof.	□
Proof of Lemma 3.8.
15
Under review as a conference paper at ICLR 2019
Proof. From Lemma 3.3,
llxioβ,w - xi0β,w0 || ≤ 2Lτη(io-1)β .
(57)
From (22), each cycle j after iteration i0β, reduces the distance between w and w0 by factor of 0.2,
while adding to the distance the value of 1.2 ∙ ηiβ+jτs. This means that after the number of cycles
j0
.	1	1.2 ∙ ηioβ Ts
j0 = log 卞-------------
2Lτ η(i0 -1)β
1 S
= log 7 ,
L
for any i ≥ io + jo, the first summand in (22) gets bounded by 0.8 ∙ ηφτs.
□
Proof of Theorem 3.9.
Proof. To prove this theorem, we follow the proof of Theorem 3.5 and develop an alternative bound
on ||xt,w - xt|| in (42).
We will split the sum in (40) into two ranges oft: t < to + joβ and t ≥ to + joβ for to,jo, defined
in (19) and (23) respectively. To simplify notation, from (23), we can assume that jo is a constant
and we can assume that to + jo ≤ 2to. For t < 2to, we use (48) to show
2to — 1
X ||xt - x* ||，||gt -曲| ≤ 2τFLH(2τ + 2σ√2t0 - 2τ).
t=1
(58)
For t≥ 2to, we first observe
||xt,w - xt|| = "W
(xt,w - xt,w0)
w0∈W
≤ JWi X llxt,w - xt,w0 || .
w0∈W
(59)
Next we use (24) to bound the above expression.
E∣∣χt,w - xt|| ≤ 2ηt-ττs.	(60)
Substituting this bound into (42), we get
E||gt -曲 | ≤ 2Hηt-ττs .	(61)
Using (61) in (40) for t≥ 2to
T
E ^X < Xt - χ* ,gt - Gt >
t=2t0
TT
≤ X E(∣∣χt-χ*∣H∣gt-gt∣l) ≤ X FEl∣gt -gt∣l>
t=2t0	t=2t0
T
≤ F ^X 2Hηt-ττs ≤ 4FHσ√Tτs .	(62)
t=t0
Combining (58) and (62) in (40) and using the definition (19) of to
T
E^X < Xt — x*,gt — gt > ≤ 4τ2FLH(1 + 2σ2H) + 4FHσ√Tτs	(63)
t=1
Using the assumption (25), and substituting into (41) We prove (26). Finally We use value σ = F to
prove (27).	□
16