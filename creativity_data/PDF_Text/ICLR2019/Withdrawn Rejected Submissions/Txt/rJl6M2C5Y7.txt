Under review as a conference paper at ICLR 2019
Online Hyperparameter Adaptation via
Amortized Proximal Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Effective performance of neural networks depends critically on effective tuning of
optimization hyperparameters, especially learning rates (and schedules thereof).
We present Amortized Proximal Optimization (APO), which takes the perspec-
tive that each optimization step should approximately minimize a proximal objec-
tive (similar to the ones used to motivate natural gradient and trust region policy
optimization). Optimization hyperparameters are adapted to best minimize the
proximal objective after one weight update. We show that an idealized version of
APO (where an oracle minimizes the proximal objective exactly) achieves global
convergence to stationary point and locally second-order convergence to global
optimum for neural networks. APO incurs minimal computational overhead. We
experiment with using APO to adapt a variety of optimization hyperparameters
online during training, including (possibly layer-specific) learning rates, damping
coefficients, and gradient variance exponents. For a variety of network architec-
tures and optimization algorithms (including SGD, RMSprop, and K-FAC), we
show that with minimal tuning, APO performs competitively with carefully tuned
optimizers.
1	Introduction
Tuning optimization hyperparameters can be crucial for effective performance of a deep learning
system. Most famously, carefully selected learning rate schedules have been instrumental in achiev-
ing state-of-the-art performance on challenging datasets such as ImageNet (Goyal et al., 2017) and
WMT (Vaswani et al., 2017). Even algorithms such as RMSprop (Tieleman & Hinton, 2012) and
Adam (Kingma & Ba, 2015), which are often interpreted in terms of coordinatewise adaptive learn-
ing rates, still have a global learning rate parameter which is important to tune. A wide variety of
learning rate schedules have been proposed (Schraudolph, 1999; Li & Malik, 2016; Baydin et al.,
2017). Seemingly unrelated phenomena have been explained in terms of effective learning rate
schedules (van Laarhoven, 2017). Besides learning rates, other hyperparameters have been identified
as important, such as the momentum decay factor (Sutskever et al., 2013), the batch size (Smith et al.,
2017), and the damping coefficient in second-order methods (Martens & Grosse, 2015; Martens,
2010).
There have been many attempts to adapt optimization hyperparameters to minimize the training
error after a small number of updates (Schraudolph, 1999; Andrychowicz et al., 2016; Baydin et al.,
2017). This approach faces two fundamental obstacles: first, learning rates and batch sizes have
been shown to affect generalization performance because stochastic updates have a regularizing
effect (Dinh et al., 2017; Li et al., 2017; Mandt et al., 2017; Smith & Le, 2018; van Laarhoven, 2017).
Second, minimizing the short-horizon expected loss encourages taking very small steps to reduce
fluctuations at the expense of long-term progress (Wu et al., 2018). While these effects are specific
to learning rates, they present fundamental obstacles to tuning any optimization hyperparameter,
since basically any optimization hyperparameter somehow influences the size of the updates.
In this paper, we take the perspective that the optimizer’s job in each iteration is to approxi-
mately minimize a proximal objective which trades off the loss on the current batch with the
average change in the predictions. Specifically, we consider proximal objectives of the form
J(φ) = h(f (g(θ, φ))) + λD(f (θ), f (g(θ, φ))), where f is a model with parameters θ, h is an ap-
proximation to the objective function, g is the base optimizer update with hyperparameters φ, and D
1
Under review as a conference paper at ICLR 2019
is a distance metric. Indeed, approximately solving such a proximal objective motivated the natural
gradient algorithm (Amari, 1998), as well as proximal reinforcement learning algorithms (Schul-
man et al., 2017; 2015). We introduce Amortized Proximal Optimization (APO), an approach which
adapts optimization hyperparameters to minimize the proximal objective in each iteration. We use
APO to tune hyperparameters of SGD, RMSprop, and K-FAC; the hyperparameters we consider
include (possibly layer-specific) learning rates, damping coefficients, and the power applied to the
gradient covariances.
Notice that APO has a hyperparameter λ which controls the aggressiveness of the updates. We
believe such a hyperparameter is necessary until the aforementioned issues surrounding stochastic
regularization and short-horizon bias are better understood. However, in practice we find that by per-
forming a simple grid search over λ, we can obtain automatically-tuned learning rate schedules that
are competitive with manual learning rate decay schedules. Furthermore, APO can automatically
adapt several optimization hyperparameters with only a single hand-tuned hyperparameter.
We provide theoretical justification for APO by proving strong convergence results for an oracle
which solves the proximal objective exactly in each iteration. In particular, we show global linear
convergence and locally quadratic convergence under mild assumptions. These results motivate the
proximal objective as a useful target for meta-optimization.
We evaluate APO on real-world tasks including image classification on MNIST, CIFAR-10, CIFAR-
100, and SVHN. We show that adapting learning rates online via APO yields faster training conver-
gence than the best fixed learning rates for each task, and is competitive with manual learning rate
decay schedules. Although we focus on fast optimization of the training objective, we also find that
the solutions found by APO generalize at least as well as those found by fixed hyperparameters or
fixed schedules.
2	Amortized Proximal Optimization
We view a neural network as a parameterized function z = f (x, θ), where x is the input, θ are the
weights and biases of the network, and z can be interpreted as the output of a regression model or the
un-normalized log-probabilities of a classification model. Let the training dataset be {(xi, ti)}iN=1,
where input xi is associated with target ti. Our goal is to minimize the loss function:
NN
L(Z, T) = X '(zi,ti) = X '(f (xi,θ),ti),	(1)
i=1	i=1
where Z is the matrix of network outputs on all training examples x1 , . . . , xN, and T is the vector
of labels. We design an iterative optimization algorithm to minimize Eq. 1 under the following
framework: in the kth iteration, one aims to update θ to minimize the following proximal objective:
hprox(θ) = h(f (x,θ)) + λEx〜P[D(f (X,θ),f (x, θk))],	(2)
where x is the data used in the current iteration, P is the distribution of data, θk is the parameters
of the neural network at the current iteration, h(∙) is some approximation of the loss function, and
D(∙, ∙) represents the distance between network outputs under some metric (for notational conve-
nience, we use mini-batch size of 1 to describe the algorithm). We first provide the motivation for
this proximal objective in Section 2.1; then in Section 2.2, we propose an algorithm to optimize it in
an online manner.
2.1	Motivation for the proximal objective
In this section, we show that by approximately minimizing simple instances of Eq. 2 in each iteration
(similar to Schulman et al. (2015)), one can recover the classic Gauss-Newton algorithm and Natural
Gradient Descent (Amari, 1998). In general, updating θ so as to minimize the proximal objective is
impractical due to the complicated nonlinear relationship between θ and z. However, one can find
an approximate solution by linearizing the network function:
f(x,θ+∆θ)≈f(x,θ)+J∆θ,	(3)
where J = Vθ f (x, θ) is the Jacobian matrix. We consider the following instance ofEq. 2:
hprox(θ) = ∆z>Vz'(f (x,θk), t) + λ E"P [D(f (x, θ),f(x, θk))],	(4)
2
Under review as a conference paper at ICLR 2019
where ∆z , f(x, θ) - f(x, θk) is the change of network output, t is the label of current data x.
Here h(∙) is defined as the first-order Taylor approximation of the loss function. Using the linear
approximation (Eq. 3), and a local second-order approximation ofD, this proximal objective can be
written as:
hprox(θ) ≈ ∆θτVθ '(f(x,θk ),t) + λ∆θ> Eχ~p [J>V2 D Ji ∆θ,	(5)
where J = Vθf (x, θk) is the Jacobian matrix on data x, V2D，VZD(z, f (x, θk)) is the HeSSian
matrix of the dissimilarity measured at Z = f (X,θk).
Solving Eq. 5 yields:
∆θ = - 1 G-1Vθ '(f(x,θ),t),	(6)
λ
where G，Eχ~p JTV2D j] is the pre-conditioning matrix. Different settings for the dissimilarity
term D yield different algorithms. When
D(zj,zjk)= kjz-jzkk22	(7)
is defined as the squared Euclidean distance, Eq. 6 recovers the classic Gauss-Newton algorithm.
When
D(Z, Zk) = '(Z) - '(Zk) - hV'(Zk), Z — Zki	(8)
is defined as the Bregman divergence, Eq. 6 yields the Generalized Gauss-Newton (GGN) method.
When the output of neural network parameterizes an exponential-family distribution, the dissimilar-
ity term can be defined as Kullback-Leibler divergence:
D(Z，jk ) = XX p(yiZ)log 3,
(9)
in which case Eq. 6 yields Natural Gradient Descent (Amari, 1998). Since different versions of our
proximal objective lead to various efficient optimization algorithms, we believe it is a useful target
for meta-optimization.
2.2	Amortized optimization
Although optimizers including the Gauss-Newton algorithm and Natural Gradient Descent can be
seen as ways to approximately solve Eq. 2, they rely on a local linearization of the neural network
and usually require more memory and more careful tuning in practice. We propose to instead directly
minimize Eq. 2 in an online manner.
Finding good hyperparameters (e.g., the learning rate for SGD) is a challenging problem in practice.
We propose to adapt these hyperparameters online in order to best optimize the proximal objective.
Consider any optimization algorithm (base-optimizer) of the following form:
θ J g(x, t,θ,ξ,φ).	(10)
Here, θ is the set of model parameters, x is the data used in this iteration, t is the corresponding
label, ξ is a vector of statistics computed online during optimization, and φ is a vector of optimiza-
tion hyperparameters to be tuned. For example, ξ contains the exponential moving averages of the
squared gradients of the parameters in RMSprop. φ usually contains the learning rate (global or
layer-specific), and possibly other hyperparameters dependent on the algorithm.
For each step, we formulate the meta-objective from Eq. 2 as follows (for notational convenience
we omit variables other than θ and φ of g):
J (φ) = h(f (x, g(θ, φ))) + λEχ~p[D(f (x, g(θ, φ)),f (x, θ))].	(11)
Here, xj is a random mini-batch sampled from the data distribution P. We compute the approxima-
tion to the loss, h, using the same mini-batch as the gradient of the base optimizer, to avoid the short
horizon bias problem (Wu et al., 2018); we measure D on a different mini-batch to avoid instability
that would result if we took a large step in a direction that is unimportant for the current batch, but
important for other batches. The hyperparameters φ are optimized using a stochastic gradient-based
algorithm (the meta-optimizer) using the gradient VφJ(φ) (similar in spirit to (Schraudolph, 1999;
Maclaurin et al., 2015)). We refer to our framework as Amortized Proximal Optimization (APO).
The simplest version of APO, which uses SGD as the meta-optimizer, is shown in Algorithm 1. One
can choose any meta-optimizer; we found that RMSprop was the most stable and best-performing
meta-optimizer in practice, and we used it for all our experiments.
3
Under review as a conference paper at ICLR 2019
Algorithm 1: Amortized Proximal Optimization (SGD as meta-optimizer)
Input : η, θo, φo, M, T
Output: θ
θ J θo
φ 一 Φo
for i J 1, . . . , M do
sample data (x, t)
for t J 1, . . . , T do
sample X ~ P
J(Φ) = h(f (x, g(x, t,θ, ξ, Φ))) + λD(f(x, g(x, t, θ, ξ, φ)), f (X,θ))
φ 一 φ 一 ηVφJ(φ)
end
θ J g(x,t,θ,ξ,φ)
end
return θ
3	Analysis of an Idealized Version
When considering optimization meta-objectives, it is useful to analyze idealized versions where the
meta-objective is optimized exactly (even when doing so is prohibitively expensive in practice). For
instance, Wu et al. (2018) analyzed an idealized SMD algorithm, showing that even the idealized
version suffered from short-horizon bias. In this section, we analyze two idealized versions of APO
where an oracle is assumed to minimize the proximal objective exactly in each iteration. In both
cases, we obtain strong convergence results, suggesting that our proximal objective is a useful target
for meta-optimization.
We view the problem in output space (i.e., explicitly designing an update schedule for zi). Consider
the space of outputs on all training examples; when we train a neural network, we are optimizing
over a manifold in this space:
M = {(f (xι,θ),f (X2,θ),...,f (XN ,θ)) | θ ∈ RD }	(12)
We assume that f is continuous, so that M is a continuous manifold. Given an oracle that for each
iteration exactly minimizes the expectation of proximal objective Eq. 2 over the dataset, we can
write one iteration of APO in output space as:
N
Z J argmin	[h(zi) + λD(zi, zk,i)],	(13)
where zi is ith column of Z, corresponding to the network output on data xi after update, zk,i is the
current network output on data xi .
3.1	Projected Gradient Descent
We first define the proximal objective as Eq. 4, using the Euclidean distance as the dissimilarity
measure, which corresponds to Gauss-Newton algorithm under the linearization of network. With
an oracle, this proximal objective leads to projected gradient descent:
Zk + 1 J argmin Z 一 (Zk - 2λVZL(Zk, T)) ] .	(14)
Consider a loss function on one data point `(z) : Rd → R, where d is the dimension of neural
network’s output. 1 We say the gradient is L-Lipschitz if:
kVz'(zι) - Vz'(z2)k ≤ L ∣∣zι - Z2k.	(A1)
When the manifold M is smooth, a curve in M is called geodesic if it is the shortest curve connect-
ing the starting and the ending point. We say M have a C-bounded curvature if for each trajectory
1For convenience of notation, we omit the dependence of loss on the fixed label.
4
Under review as a conference paper at ICLR 2019
v(t) : [0,1] → Rd going along some geodesic and ∣∣V(t)∣∣2 = 1, there is ∣∣v(t)k ≤ C with spectral
norm. For each point Z ∈ M, consider the tangent space at point Z as TZM. We call the projection
of V L(Z) onto the hyperplane TZM as the effective gradient of L at Z ∈ M. It is worth noting
that zero effective gradient corresponds to stationary point of the neural network.
We have the following theorem stating the global convergence of Eq. 14 to stationary point:
Theorem 1. Assume the loss satisfies A1. Furthermore, assume L is lower bounded by L* and has
gradient norm upper bound G. Let gT* be the effective gradient in the first T iterations with minimal
norm. When the manifold is smooth with C-bounded curvature, with λ ≥ {CG, L }, the norm of gT
converges with rate O (T) as:
16λ
kgTk2 ≤ 亍[L(Zo) -L*].
(15)
This convergence result differs from usual neural network convergence results, because here the
Lipschitz constants are defined for the output space, so they are known and generally nice. For
instance, L = 1 when we use a quadratic loss. In contrast, the gradient is in general not Lipschitz
continuous in weight space for deep networks.
3.2	Proximal Newton method
We further replace the dissimilarity term with:
D(zi, zk,i) = (zi - Zk,i)>V2'(zk,i)(zi - zk,i),	(16)
which is the second-order approximation of Eq. 8. With a proximal oracle, this variant of APO turns
out to be Proximal Newton Method in the output space, if we set λ = 1:
Zk + 1 - argmin hVZ L(Zk), Z - Zk i + - IlZ - ZkllH ,	(17)
Z∈M	2
where ∣Z - Zk ∣2H is the norm with local Hessian as metric. In general, Newton’s method can’t
be applied directly to neural nets in weight space, because it is nonconvex (Dauphin et al., 2014).
However, Proximal Newton Method in output space can be efficient given a strongly convex loss
function.
Consider a loss '(z) with μ-strongly convex:
'(z)-'(z*) ≥ 2 kz-z*k2 ,	(A2)
where z* is the unique minimizer and μ is some positive real number, and LH-smooth Hessian: for
any vector v ∈ Rd such that ∣v∣ = 1, there is:
∣∣V2'(z1)v — V2'(z2)v∣∣ ≤ LH ∣∣z1 - Z2k .	(A3)
The following theorem suggests the locally fast convergence rate of iteration Eq. 17:
Theorem 2. Under assumptions A2 and A3, if the unique minimum Z* ∈ M, then whenever itera-
tion (17) converges to Z*, it converges locally quadratically2:
lim kZk+τ - Z*2k ≤ LH
k→∞ ∣∣Zk - Z*k2 一 μ
Hence, the proximal oracle achieves second-order convergence for neural network training under
fairly reasonable assumptions. Of course, we don’t expect practical implementations of APO (or
any other practical optimization method for neural nets) to achieve the second-order convergence
rates, but we believe the second-order convergence result still motivates our proximal objective as a
useful target for meta-optimization.
2It is worth noting that here we don’t need assumptions of manifold M being even differentiable. Therefore,
the result holds for neural networks where non-smooth activation functions like ReLU are used.
5
Under review as a conference paper at ICLR 2019
4	Related Work
Finding good optimization hyperparameters is a longstanding problem (Bengio, 2012). Classic
methods for hyperparameter optimization, such as grid search, random search, and Bayesian opti-
mization (Snoek et al., 2012; 2015; Swersky et al., 2014), are expensive, as they require performing
many complete training runs, and can only find fixed hyperparameter values (e.g., a constant learning
rate). Hyperband (Li et al., 2016) can reduce the cost by terminating poorly-performing runs early,
but is still limited to finding fixed hyperparameters. Population Based Training (PBT) (Jaderberg
et al., 2017) trains a population of networks simultaneously, and throughout training it terminates
poorly-performing networks, replaces their weights by a copy of the weights of a better-performing
network, perturbs the hyperparameters, and continues training from that point. PBT can find a
coarse-grained learning rate schedule, but because it relies on random search, it is far less efficient
than gradient-based meta-optimization.
There have been a number of approaches to gradient-based adaptation of learning rates. Gradient-
based optimization algorithms can be unrolled as computation graphs, allowing the gradients of
hyperparameters such as learning rates to be computed via automatic differentiation. Maclaurin
et al. (2015) propagate gradients through the full unrolled training procedure to find optimal learning
rate schedules offline. Stochastic meta-descent (SMD) (Schraudolph, 1999) adapts hyperparameters
online. Hypergradient descent (HD) (Baydin et al., 2017) takes the gradient of the learning rate with
respect to the optimizer update in each iteration, to minimize the expected loss in the next iteration.
In particular, HD suffers from short horizon bias (Wu et al., 2018), while in Appendix F we show
that APO does not.
Some authors have proposed learning entire optimization algorithms (Li & Malik, 2016; 2017;
Andrychowicz et al., 2016). Li & Malik (2016) view this problem from a reinforcement learning
perspective, where the state consists of the objective function L and the sequence of prior iterates
{θt} and gradients {Vθ L(θt)}, and the action is the step ∆θ. In this setting, the update rule φ is a
policy, which can be found via policy gradient methods (Sutton et al., 2000). Approaches that learn
optimizers must be trained on a set of objective functions {f1, . . . , fn} drawn from a distribution F;
this setup can be restrictive if we only have one instance of an objective function. In addition, the
initial phase of training the optimizer on a distribution of functions can be expensive. APO requires
only the objective function of interest and finds learning rate schedules in a single training run.
In principle, APO could be used to learn a full optimization algorithm; however, learning such an
algorithm would be just as hard as the original optimization problem, so one would not expect an
out-of-the-box meta-optimizer (such as RMSprop with learning rate 0.001) to work as well as it does
for adapting few hyperparameters.
5	Experiments
In this section, we evaluate APO empirically on a variety of learning tasks; Table 1 gives an overview
of the datasets, model architectures, and base optimizers we consider.
In our proximal objective, J (φ) = h (f(x, g(θ, φ))) + λ Ex〜P [D (f (X, g(θ, φ)) ,f (X,θ))], h can
be any approximation to the loss function (e.g., a linearization); in our experiments, we directly used
the loss value h = `, as we found this to work well in many settings. As the dissimilarity term D,
we used the squared Euclidean norm.
We used APO to tune the optimization hyperparameters of four base-optimizers: SGD, SGD with
Nesterov momentum (denoted SGDm), RMSprop, and K-FAC. For SGD, the only hyperparameter
is the learning rate; we consider both a single, global learning rate, as well as per-layer learning
rates. For SGDm, the update rule is given by:
Vt - μvt-i + gt
θt - θt-i - η(gt + μVt)
(18)
(19)
where g = V'. Since adapting μ requires considering long-term performance (Sutskever et al.,
2013), it is not appropriate to adapt it with a one-step objective like APO. Instead, we just adapt the
learning rate with APO as if there,s no momentum, but then apply momentum with μ = 0.9 on top
of the updates.
6
Under review as a conference paper at ICLR 2019
Dataset	Architecture	Optimizer	Batch Size
Rosenbrock & 2-Layer Linear	-	RMSprop	-
MNIST	MLP	SGD, RMSprop	100
FashionMNIST	2-Layer CNN	RMSprop, K-FAC	100
CIFAR-10	ResNet-34	SGD, RMSprop, K-FAC	128
CIFAR-100	ResNet-34	SGD	128
SVHN	ResNet-18	RMSprop	128
Table 1: Summary of the datasets, model architectures, and optimizers we investigate.
北北二10-°-
SSol 6UWJ1
B-H 6U-Ee3
RMSProPm
RMSpropIr-Ie-S
RMSprop lr^le-*
RMSprop-APO
■J ∙J
B-H 6U-Ee3
(a)	(b)	(c)	(d)
Figure 1: Experiments on toy examples. (a) Rosenbrock objective values during training; (b) Learning rates
for RMSprop compared to the adaptive learning rate of RMSprop-APO on Rosenbrock; (c) Loss on the badly-
conditioned regression problem; (d) Learning rate adaptation on the badly-conditioned regression problem.
For RMSprop, the optimizer step is given by:
st J γst-i + (I - Y)g2	(20)
θt J θt-1-----J gt	(21)
st +
We note that, in addition to the learning rate η , we can also consider adapting and the power to
which s is raised in the denominator of Eq. 21—we denote this parameter ρ, where in standard
RMSProP We have P = ɪ. Both e and P can be interpreted as having a damping effect on the update.
K-FAC is an approximate natural gradient method (Amari, 1998) based on preconditioning the gra-
dient by an approximation to the Fisher matrix, θ J θ — F-1V'. For K-FAC, we tune the global
learning rate and the damping factor.
Meta-Optimization Setup. Throughout this section, we use the following setup for meta-
optimization: we use RMSprop as the meta-optimizer, with learning rate 0.1, and perform 1 meta-
optimization update for every 10 steps of the base optimization. We show in Appendix E that with
this default configuration, APO is robust to the initial learning rate of the base optimizer. Each
meta-optimization step takes approximately the same amount of computation as a base optimization
step; by performing meta-updates once per 10 base optimization steps, the computational overhead
of using APO is just a small fraction more than the original training procedure.
5.1	Toy Optimization Problems
Rosenbrock. We first validated APO on the two-dimensional Rosenbrock function, f (x, y) =
(1 - x)2 + 100(y - x2)2, with initialization (x, y) = (1, -1.5). We used APO to tune the learning
rate of RMSprop, and compared to standard RMSprop with several fixed learning rates. Because
this problem is deterministic, we set λ = 0 for APO. Figure 1(a) shows that RMSprop-APO was
able to achieve a substantially lower objective value than the baseline RMSprop. The learning rates
for each method are shown in Figure 1(b); we found that APO first increases the learning rate to
make rapid progress at the start of optimization, and then gradually decreases it as it approaches the
local optimum. In Appendix D we show that APO converges quickly from many different locations
on the Rosenbrock surface, and in Appendix E we show that APO is robust to the initial learning
rate of the base optimizer.
Badly-Conditioned Regression. Next, we evaluated APO on a badly-conditioned regression prob-
lem (Recht & Rahimi, 2017), which is intended to be a difficult test problem for optimization
algorithms. In this problem, we consider a dataset of input/output pairs {(x, y)}, where the out-
puts are given by y = Ax, where A is an ill-conditioned matrix with κ(A) = 1010. The task
7
Under review as a conference paper at ICLR 2019
Epoch
SSon 6u-u-eJ1-
25 50 75 IOO 125 150 175 200
Epoch
•,TT T
Iwww
SSon 6u-u-eJ
ejnɔɔv-sə.L
——SGD Ir=O.I
SGwSGDm lr=0.01
SGQ-APO ^=0.∞l
——SGDm-APOA=O-I
—RMSpmp lr"le>3
--RMSnrC口 Ir∙le-4 I
——RMSprop schedule
——RMSprap-APO λ-½-3
IDlZ
,°,°M,°^
BH 6U-Eeaη
'jV∖ΛJ'
IOOOO 2008 3∞∞ Λ0∞0 50000 6∞∞
Iteration
"J "J
BaH 6U-Eeaη
se>j 6u-lue2
30000	50000	70000
Iteration
se>j 6u-lue01
SGD/SGDm MNIST
(a)
RMSprop MNIST
(b)
SGD/SGDm CIFAR-10
(c)
RMSprop CIFAR-10
(d)
Figure 2: Experiments on MNIST and CIFAR-10, with and without APO. Upper row: mean loss over the
training set. Middle row: accuracy on the test set. Bottom row: learning rate per iteration. We use ResNet34
for CIFAR-10, and train for 200 epochs with learning rate decayed by a factor of 10 every 60 epochs.
is to fit a two-layer linear model f(x) = W2W1x to this data; the loss to be minimized is
L = Ex〜N(0,i) IjIAx - 卬2卬1久|图.Figure 1(c) compares the performance of RMSProP with a
hand-tuned fixed learning rate to the performance of RMSprop-APO, with learning rates shown in
Figure 1(d). Again, the adaptive learning rate enabled RMSprop-APO to achieve a loss value orders
of magnitude smaller than that achieved by RMSprop with a fixed learning rate.
5.2	Real-World Datasets
For each of the real-world datasets we consider—MNIST, CIFAR-10, CIFAR-100, SVHN, and
FashionMNIST—we chose the learning rates for the baseline optimizers via grid searches: for SGD
and SGDm, we performed a grid search over learning rates {0.1, 0.01, 0.001}, while for RMSprop,
we performed a grid search over learning rates {0.01, 0.001, 0.0001}. For SGD-APO and SGDm-
APO, we set the initial learning rate to 0.1, while for RMSprop-APO, we set the initial learning rate
to 0.0001. These initial learning rates are used for convenience; we show in Appendix E that APO
is robust to the choice of initial learning rate. The only hyperparameter we consider for APO is the
value of λ: for SGD-APO and SGDm-APO, we select the best λ from a grid search over {0.1, 0.01,
1e-3}; for RMSprop, we choose λ from a grid search over {0.1, 0.01, 1e-3, 1e-4, 1e-5, 0}. Note that
because each value of λ yields a learning rate schedule, performing a search over λ is much more
effective than searching over fixed learning rates. In particular, we show that the adaptive learning
rate schedules discovered by APO are competitive with manual learning rate schedules.
5.2.1	Multi-Layer Perceptron on MNIST
First, we compare SGD and RMSprop with their APO-tuned variants on MNIST, and show that
APO outperforms fixed learning rates. As the classification network for MNIST, we used a two-
layer MLP with 1000 hidden units per layer and ReLU nonlinearities. We trained on mini-batches
of size 100 for 100 epochs.
SGD with APO. We used APO to tune the global learning rate of SGD and SGD with Nesterov
momentum (denoted SGDm) on MNIST, where the momentum is fixed to 0.9. For baseline SGDm,
we used learning rate 0.01, while for baseline SGD, we used both learning rates 0.1 and 0.01. The
training curve of SGD with learning rate 0.1 almost coincides with that of SGDm with learning rate
0.01. For SGD-APO, the best λ was 1e-3, while for SGDm-APO, the best λ was 0.1. A comparison
of the algorithms is shown in Figure 2(a). APO substantially improved the training loss for both
SGD and SGDm.
8
Under review as a conference paper at ICLR 2019
---K-FAC schedule
K-FAC Ue-3
——K-FAC-APO flr}
---K-FAC∙AP0 {lrfd∙mplπg}
Iteration
■J
6~dEeα
Figure 3: K-FAC results on CIFAR-10. We compare K-FAC with a fixed learning rate and a manual learning
rate schedule to APO, used to tune 1) the learning rate; and 2) both the learning rate and damping coefficient.
RMSprop with APO. Next, we used APO to tune the global learning rate of RMSprop. For baseline
RMSprop, the best fixed learning rate was 1e-4, while for RMSprop-APO, the best λ was 1e-5.
Figure 2(b) compares RMSprop and its APO-tuned variant on MNIST. RMSprop-APO achieved a
training loss about three orders of magnitude smaller than the baseline.
5.2.2	CIFAR- 1 0 Experiments
We trained a 34-layer residual network (ResNet34) (He et al., 2016) on CIFAR-10 (Krizhevsky &
Hinton, 2009), using mini-batches of size 128, for 200 epochs. We used batch normalization and
standard data augmentation (horizontal flipping and cropping). For each optimizer, we compare
APO to 1) fixed learning rates; and 2) manual learning rate decay schedules.
SGD with APO. For SGD, we used both learning rates 0.1 and 0.01 since both work well. For SGD
with momentum, we used learning rate 0.01. We also consider a manual schedule for both SGD
and SGDm: starting from learning rate 0.1, and we decay it by a factor of 5 every 60 epochs. For
the APO variants, we found that λ=1e-3 was best for SGD, while λ = 0.1 was best for SGDm. As
shown in Figure 2(c), APO not only accelerates training, but also achieves higher accuracy on the
test set at the end of training.
RMSprop with APO. For RMSprop, we use fixed learning rates 1e-3 and 1e-4, and we consider a
manual learning rate schedule in which we initialize the learning rate to 1e-3 and decay by a factor of
5 every 60 epochs. For RMSprop-APO, we used λ = 1e-3. The training curves, test accuracies, and
learning rates for RMSprop and RMSprop-APO on CIFAR-10 are shown in Figure 2(d). We found
that APO achieved substantially lower training loss than fixed learning rates, and was competitive
with the manual decay schedule. In particular, both the final training loss and final test accuracy
achieved by APO are similar to those achieved by the manual schedule.
K-FAC with APO. We also used APO to tune the learning rate and damping coefficient of K-FAC.
Similarly to the previous experiments, we use K-FAC to optimize a ResNet34 on CIFAR-10. We
used mini-batches of size 128 and trained for 100 epochs. For the baseline, we used a fixed learning
rate of 1e-3 as well as a decay schedule with initial learning rate 1e-3, decayed by a factor of 10 at
epochs 40 and 80. For APO, we used λ = 1e-2. In experiments where the damping is not tuned, it
is fixed at 1e-3. The results are shown in Figure 3. We see that K-FAC-APO performs competitively
with the manual schedule when tuning just the global learning rate, and that both training loss and
test accuracy improve when we tune both the learning rate and damping coefficient simultaneously.
5.2.3	CIFAR- 1 00 Experiments
Next, we evaluated APO on the CIFAR-100 dataset. Similarly to our experiments on CIFAR-10, we
used a ResNet34 network with batch-normalization and data augmentation, and we trained on mini-
batches of size 128, for 200 epochs. We compared SGD-APO/SGDm-APO to standard SGD/SGDm
using (1) a fixed learning rate found by grid search; (2) a custom learning rate schedule in which the
learning rate is decayed by a factor of 5 at epochs 60, 120, and 180. We set λ = 1e-3 for SGD-APO
and λ = 0.1 for SGDm-APO. Figure 4 shows the training loss, test accuracy, and the tuned learning
rate. It can be seen that APO generally achieves smaller training loss and higher test accuracy.
5.2.4	SVHN Experiments
We also used APO to train an 18-layer residual network (ResNet18) with batch normalization on
the SVHN dataset (Netzer et al., 2011). Here, we used the standard train and test sets, without
additional training data. We used mini-batches of size 128 and trained our networks for 160 epochs.
We compared APO to 1) fixed learning rates, and 2) a manual schedule in which we initialize the
learning rate to 1e-3 and decay by a factor of 10 at epochs 80 and 120. We show the training loss,
test accuracy, and learning rates for each method in Figure 5. Here, RMSprop-APO achieves similar
9
Under review as a conference paper at ICLR 2019
sso-jπc-c-EH
(a) Mean CIFAR-100 training loss.
Epoch
(b) Accuracy on the test set.
aeα6u 三，Jeφ~l
1L
10000
30000	50000	70000
Iteration
(c) Learning rate per iteration.
I I II II
sso*∣ 6U_U@H
20 40 60 80 IOO 120 140 160
Epoch
Figure 4: Comparison of SGD and SGD-APO on CIFAR-100.
0.97
---RMSprop lr=le-3
RMSprop lr=le-4
RMSprop lr=le-5
RMSprop schedule
RMSprop-APO Λ=le-2
O 20000	40000	60000	80000
Iteration
Figure 5:	Comparison of RMSprop and RMSprop-APO used to optimize a ResNet on SVHN.
training loss to the manual schedule, and obtains higher test accuracy than the schedule. We also
see that the learning rate adapted by APO spans two orders of magnitude, similar to the manual
schedule.
Epoch
e 1 2 3
O - - -
loon
111
sso*∣ 6u-uu
0.96
0.94-
>0.92
S 0.90
W 0.88
⅛J 0.86
小
0.82
0.80
0°TT-j
loon
Iln
6u-UJeθη
Figure 6:	SGD with weight decay compared to SGD-APO without weight decay, on CIFAR-10.
5.3 Batch Normalization and Weight Decay
Batch normalization (BN) (Ioffe & Szegedy, 2015) is a widely used technique to speed up neural
net training. Networks with BN are commonly trained with weight decay. It was shown that the
effectiveness of weight decay for networks with BN is not due to the regularization, but due to the
fact that weight decay affects the scale of the network weights, which changes the effective learning
rate (Zhang et al., 2018; Hoffer et al., 2018; van Laarhoven, 2017). In particular, weight decay
decreases the scale of the weights, which increases the effective learning rate; if one uses BN without
regularizing the norm of the weights, then the weights can grow without bound, pushing the effective
learning rate to 0. Here, we show that using APO to tune learning rates allows for effective training
of BN networks without using weight decay. In particular, we compared SGD-APO without weight
decay and SGD with weight decay 5e-4. Figure 6 shows that SGD-APO behaved better than SGD
with a fixed learning rate, and achieved comparable performance as SGD with a manual schedule.
6 Conclusions
We introduced amortized proximal optimization (APO), a method for online adaptation of opti-
mization hyperparameters, including global and per-layer learning rates, and damping parameters
for approximate second-order methods. We evaluated our approach on real-world neural network
optimization tasks—training MLP and CNN models—and showed that it converges faster and gen-
eralizes better than optimal fixed learning rates. Empirically, we showed that our method overcomes
short horizon bias and performs well with sensible default values for the meta-optimization param-
eters.
10
Under review as a conference paper at ICLR 2019
References
ShUn-Ichi Amari. Natural gradient works efficiently in learning. Neural COmputatiOn, 10(2):251-
276, 1998.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in Neural InfOrmatiOn PrOcessing Systems (NIPS), pp. 3981-3989, 2016.
Atilim G Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. Online
learning rate adaptation with hypergradient descent. arXiv preprint arXiv:1703.04782, 2017.
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In
Neural NetwOrks: Tricks Of the Trade, pp. 437-478. Springer, 2012.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in Neural InfOrmatiOn PrOcessing Systems (NIPS), pp. 2933-2941, 2014.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. arXiv preprint arXiv:1703.04933, 2017.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training Imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image recog-
nition. In COnference On COmputer VisiOn and Pattern RecOgnitiOn (CVPR), PP. 770-778, 2016.
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: Efficient and accurate
normalization schemes in deeP networks. arXiv preprint arXiv:1803.01814, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deeP network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali
Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. PoPulation-based train-
ing of neural networks. arXiv preprint arXiv:1711.09846, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In InternatiOnal
COnference On Learning RepresentatiOns (ICLR), 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiPle layers of features from tiny images. Tech-
nical rePort, University of Toronto, 2009.
Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscaPe of neural nets.
arXiv preprint arXiv:1712.09913, 2017.
Ke Li and Jitendra Malik. Learning to oPtimize. arXiv preprint arXiv:1606.01885, 2016.
KeLi and Jitendra Malik. Learning to oPtimize neural nets. arXiv preprint arXiv:1703.00441, 2017.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. HyPer-
band: Bandit-based configuration evaluation for hyPerParameter oPtimization. arXiv preprint
arXiv:1603.06560, 2016.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyPerParameter oPtimiza-
tion through reversible learning. In InternatiOnal COnference On Machine Learning (ICML), PP.
2113-2122, 2015.
StePhan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as aPProximate
Bayesian inference. The JOurnal Of Machine Learning Research, 18(1):4873-4907, 2017.
James Martens. DeeP learning via Hessian-free oPtimization. In InternatiOnal COnference On Ma-
chine Learning (ICML), 2010.
11
Under review as a conference paper at ICLR 2019
James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approxi-
mate curvature. In International Conference on Machine Learning (ICML), pp. 2408-2417, 2015.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
Benjamin Recht and Ali Rahimi. Reflections on random kitchen sinks. 2017. URL http://www.
argmin.net/2017/12/05/kitchen-sinks/.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International Con-
ference on Machine Learning (ICML), pp. 343-351, 2013.
Nicol N Schraudolph. Local gain adaptation in stochastic gradient descent. In Ninth International
Conference on Artificial Neural Networks (ICANN), volume 2, pp. 569-574, 1999.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), pp. 1889-1897,
2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Samuel L Smith and Quoc V Le. Understanding generalization and stochastic gradient descent.
International Conference on Learning Representations (ICLR), 2018.
Samuel L Smith, Pieter-Jan Kindermans, and Quoc V Le. Don’t decay the learning rate, increase
the batch size. In International Conference on Learning Representations (ICLR), 2017.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine
learning algorithms. In Advances in Neural Information Processing Systems (NIPS), pp. 2951-
2959, 2012.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Prabhat Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep
neural networks. In International Conference on Machine Learning (ICML), pp. 2171-2180,
2015.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initializa-
tion and momentum in deep learning. In International Conference on Machine Learning (ICML),
pp. 1139-1147, 2013.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Infor-
mation Processing Systems (NIPS), pp. 1057-1063, 2000.
Kevin Swersky, Jasper Snoek, and Ryan P Adams. Freeze-thaw Bayesian optimization. arXiv
preprint arXiv:1406.3896, 2014.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5—RMSprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint
arXiv:1706.05350, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems (NIPS), pp. 5998-6008, 2017.
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in
stochastic meta-optimization. In International Conference on Learning Representations (ICLR),
2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
12
Under review as a conference paper at ICLR 2019
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay
regularization. arXiv preprint arXiv:1810.12281, 2018.
A Proof of Theorem 1
We first introduce the following lemma:
Lemma 1. Assume the manifold is smooth with C-bounded curvature, the gradient norm of loss
function L is upper bounded by G. If the effective gradient at point Zk ∈ M is gk, then for any
0 < γ ≤ 4c1g there exist Z ∈ M such that
"z L(Zk), Z - Zki + ʒ- IlZ - Zk IlF ≤ -孑 Ilgk 112 .
2γ	4
Proof. We construct the Z satisfying the above inequality. Consider the following point in Rd :
Y , Zk - Ygk .
Also, consider a geodesic
v : [0,γkgkk] → M
such that
kvk = 1,
V(O)=备
Define point
Z , v(Ykgkk) ∈ M.
We show that Z is a point satisfying the inequality in the lemma. Firstly, we notice that
∣Y - Zk ≤ 2Y2kgtk2.
This is because when We introduce the extra curve v : [0,γ ∣∣gkk] → Rd going directly from Zk to
Y with unit speed, there is
∣Y - Zk = kv(γ kgtk) - V(Y kgtk)k
γkgt k
I t=0
γkgt k
≤
t=0
γkgt k
t=0
γkgt k
t=0
γkgt k
V(t)dt - /
t=0
IlMt)-讥以I dt
v(t) dt
Z	v(t0)dt0 - Z	V(t0)dt0 dt
t0=0	t0=0	I
Zt
t0=0
v(t0)dt0 dt

γkgt k	t
≤ / I	kv(t0)k dt0dt
t=0	t0=0
≤ Z γkgtk Z t Cdt0dt
t=0	t0=0
=2 γ2 kgtk2.
13
Under review as a conference paper at ICLR 2019
Here We use the fact that v = 0 and ∣∣vk ≤ C. Therefore We have
"z L(Zk), Z - Zki + ʒ- IlZ - Zk IIF
2γ	F
="z L(Zk), (Z - Y) +(Y - Zk)i + ʒ- Il(Z - Y) +(Y — Zk)IlF
2γ
1	21	2
≤ "z L(Zk ), Z - Yi + "z L(Zk ), Y - Zki + ʒ- IlZ - YIlF + 7Γ kY - Zk IlF
2γ	F	2γ	F
=-Y IgkI2 + hVz L(Zk), Z - Yi + ɪ IZ - YkF
2	2γ
≤ -Y llgk 112 + -I^Y2 Ilgk 112 + -χ-γ3 Ilgk∣∣4
22	8
≤ — Y IlgkIl2 + cGy2 IlgkIl2 + C^-γ3 IlgkIl2.
22	8
Here the first equality is by introducing the extra Y , the first inequality is by triangle inequality,
the second equality is by the definition of gk being YZ L(Zk) projecting onto a plane, the second
inequality is due to the above bound of IY - ZI, the last inequality is due to Igk I ≤ IYZ L(Zk)I.
Since Y ≤ ^^, there is therefore
〈▽z L(Zk), Z - Zki + ʒ- ||Z - Zk ∣∣F ≤ - 7 Igk 112 ,
2Y	F 4
which completes the proof.
□
Now we return to the proof of Theorem 1:
Proof. For the ease of notation, We denote the effective gradient at iteration k as gk. For one itera-
tion, there is
L
L(Zk+ 1)≤ L(Zk) + "z L(Zk) , Zk+1 - Zk i + ~ ∣∣Zk+1 - Zk ∣∣
≤ L(Zk) + RZ L(Zk), Zk+ι — Zk)+ 2λ ∣Zk+ι - Zkk
≤ L(Zk) - -τχτ ∣∣gk 112 .
16λ
2
2
Here the first inequality is due to the Lipschitz continuity and the fact that total loss equals to the
sum of all loss functions, and the second inequality is due to λ ≥ LL, the third inequality is due to
Lemma 1 with Y = 4λ. (When λ ≥ CG, there is naturally Y ≤ 41^, satisfying the assumptions in
Lemma 1.)
So We have
L(Zk+1) ≤ L(Zk) - 16λ kgk1 .
Telescoping, there is
1T-1	16λ
Ilgt i ≤ τ X IlgM ≤ ~r~[L(ZO)-L ].
T t=0	T
□
B	Proof of Theorem 2
Proof. For notational convenience, We think of Z as a vector rather than a matrix in this proof.
The Hessian ▽2 L(Z) is therefore a block diagonal matrix, Where each block is the Hessian of loss
on a single data.
14
Under review as a conference paper at ICLR 2019
First, we notice the following equation:
arg min
Z∈M
"z L(Zk), Z - Zk〉+ 1 ∣∣Z - ZkkH
argmin ["z L(Zk), Z - Zk〉+ ∣(Z - Zk)tV2 L(Zk)(Z - Zk)
Z∈M
argmin 1 [∣∣Z -(Zk- V2 L(Zk)-1VL(Zk)) ∣∣^72 L(Zk)-IIVL(Zk)的 L(Zk)L]
argmin ∣∣Z -(Zk- V2 L(Zk )-1V L(Zk)) R? L(Zk).
Here
IivkA =VT Av
is the norm of vector v defined by the positive definite matrix A. [V2 L(Zk)] 1 is the inverse of
positive definite matrix V2 L(Zk), therefore also positive definite.
As a result of the above equivalence, one step of Proximal Newton Method can be written as:
Zk+1 = argmjn∣∣Z -(Zk -V2 L(Zk)-1VL(Zk))∣∣772 L(ZtI) ∙
Since Z* ∈ M by assumption, there is:
∣ ∣ Zk+1 - (Zk - V2 L(Zk)-1V L(Zk)) ∣ ∣ 7 L(zk) ≤ ∣∣Z* - (Zk - V2 L(Zk)-1VL(Zk)) ∣ ∣ 7 L(Zk)
Now we have the following inequality for one iteration:
llZk+1 - Z* k72 L(Zk)
≤ ∣∣Zk+ι - (Zk -V2 L(Zk)-1VL(Zk))∣∣72 L(Zk) + ∣∣Z* - (Zk -V2 L(Zk)-1 VL(Zk))∣K L(Zk)
≤ 2∣∣Z* - (Zk -V2 L(Zk)-1VL(Zk))∣K L(Zk)
=2 ∣∣Z* - Zk + V2 L(Zk)-1(VL(Zk) - VL(Z*)∣∣72 L(Zk)
2
≤ √μ ∣ ∣ V2L(Zk)(Zk -Z*) -VL(Zk) + VL(Z*) ∣ ∣.
Here the first inequality is because of triangle inequality, the second inequality is due to the previous
result, the equality is because V L(Z*) = 0, the last inequality is because of the strong convexity.
By the Lipschitz continuity of the Hessian, we have:
∣∣V2 L(Zk)(Zk - Z*) -VL(Zk) + VL(Z*)∣∣
N
≤ X ∣∣V24(zk,i)(zk,i - z；) -VL(zk,i) + VL(z*)∣∣
i=1
L N
≤~2~ X llzk,i - z"『
i=1
=l2h I%- z*∣∣2 .
15
Under review as a conference paper at ICLR 2019
Therefore, we have:
kZk + 1 - Z*k ≤ √μ kZk + 1 - Z*kV2 L(Zk) ≤ ~H^ kZk - Z*『.
□
C Multiple Optimization Hyperparameters & Per-Layer Tuning
"J
seH 6u-UJeaη
∙∙τ∙,TYTY
Ioooooo
SSon 6U-UJ
20	40	60 BO 100
Epoch
1 1 1
SSon 6U-U∙J1-
α α α
X3ejn33v-s8j.
g. 0.910
C 0.905
n
^0.900
308m
I- 0.890
0	20	40	60 8a 100
Epoch
——RMSprop Ir=OXIOl
---RMSPK>p∙ΛPOA=le∙3
——KFAC Ir=O.OOl
——KfAC-APOJI=O.1 {lr, damping)
Io0Io-1
eH 6u-UJeaη
10-≈;
IOOOO
Iteration
6000。
——KFlC-APO Ir
----KFiC-APOdainptng
MO8 20000 30∞0 40000 SOooo 60000
Iteration
(a)	(b)	(c)	(d)
Figure 7: (a) Tuning multiple RMSprop parameters from {η, ρ, } on MNIST. (b) Tuning per-layer learning
rates for SGD on MNIST. (c) Tuning per-layer learning rates for RMSprop on MNIST. (d) K-FAC and RMSprop
on FashionMNIST.
0	10000 20000 30000 40000 50000 60000
Iteration
(a) RMSprop-APO tuning ρ.
UO--Sd 山
0	10000 20000 30000 40000 50000 60000
Iteration
(b) RMSprop-APO tuning .
Figure 8: Adaptation of ρ and using RMSprop-APO on MNIST.
Here we highlight the ability of APO to tune several optimization hyperparameters simultaneously.
We used APO to adapt all of the RMSprop hyperparameters {η, ρ, }. As shown in Figure 7(a),
tuning ρ and in addition to the learning rate η can stabilize training. We also used APO to adapt
per-layer learning rates. Figure 7(b) shows the per-layer learning rates tuned by APO, when using
SGD on MNIST. Figure 7(c) uses APO to tune per-layer learning rate of RMSprop on MNIST.
Figure 8 shows the adaptation of the additional ρ and hyperparameters of RMSprop, for training
an MLP on MNIST. Tuning per-layer learning rates is a difficult optimization problem, and we
found that it was useful to use a smaller meta learning rate of 0.001 and perform meta-updates more
frequently.
16
Under review as a conference paper at ICLR 2019
K-FAC. We also used APO to train a convolutional network on the FashionMNIST dataset (Xiao
et al., 2017). The network we use consists of two convolutional layers with 16 and 32 filters re-
spectively, both with kernel size 5, followed by a fully-connected layer. The results are shown in
Figure 7(d), where we also compare K-FAC to hand-tuned RMSprop and RMSprop-APO on the
same problem. We find that K-FAC with a fixed learning rate outperforms RMSprop-APO, while
K-FAC-APO substantially outperforms K-FAC. The results are shown in Figure 7(d). We also show
the adaptation of both the learning rate and damping coefficient for K-FAC-APO in Figure 7(d).
D Additional Experiments on Rosenbrock
In this section, we present additional experiments on the Rosenbrock problem. We show in Figure 9
that APO converges quickly from different starting points on the Rosenbrock surface.
SSoI 6u≡s,Jl
(a) RMSprop-APO training convergence from dif-
ferent initial (x,y) positions on the Rosenbrock
surface.
ω%α 6u-u-eω~l
——Init (x,y) = (-1,1.5)
Init (x,y) = (-2, 2)
-∣nit (x,y) = (O, O)
——lnit(x,y) = (0.5, 0.5)
-Init (x,y) = (1, -1)
O IOOOO 20000	30000	40000
Iteration
(b)	RMSprop-APO learning rate adaptation from
different initial (x,y) positions on the Rosenbrock
surface.
Figure 9: RMSprop-APO convergence from different initializations on the Rosenbrock surface. We
find that RMSprop-APO achieves very low training loss starting from any of the points (x, y) ∈
{(-1, 1.5), (-2, 2), (0, 0), (0.5, 0.5), (1, -1)}.
E Robustness to Initial Learning Rate
SSoI σc-c-SH
ilr=le-2
ilr=le-3
ilr=le-4
ilr=le-5
ilr=le-6
ilr=le-7
ioɪ
lθ-ɪ
10^3
10~5
10^7
10^9
lθ-ɪɪ
IOT 3
IO-15
O IOOOO 20000	30000
Iteration
ω%α 6u-u-eω~l
Io-7
O IOOOO 20000	30000	40000
Iteration
(b) RMSprop-APO learning rate adaptation with
different initial learning rates.
starting with different initial learning rates for the base
40000
(a) RMSprop-APO training convergence with dif-
ferent initial learning rates.
Figure 10: RMSprop-APO performance on Rosenbrock,
optimizer.
In this section we show that APO is robust to the choice of initial learning rate of the base optimizer.
With a suitable meta learning rate, APO quickly adapts many different initial learning rates to the
same range, after which the learning rate adaptation follows a similar trajectory. Thus, APO helps
to alleviate the difficulty involved in selecting an initial learning rate.
17
Under review as a conference paper at ICLR 2019
ssoη 6u 一 uδ∙ll
0 12 3
O - - -
Ioon
111
ssoη 6u 一 u∙ll
25	50	75 IOO 125 150 175 200
Epoch
(a)
Epoch
ι∣r=ie-2
llr=le-3
Ilr=Ifr4
∣∣r=le-5
llr=le-6
ι∣r=ie-7
ι∣r=ie-2
llr=le-3
llr=le-4
llr=le-5
llr=le-6
llr=le-7
0.9875
>0.9850
S 0.9825
u 0.9800
<
¾5 0.9775-
——ilr*le-2
—ilr"le-3 _
—ilr"le4
---ilr*le-5 —
—ilr"le-6
---ilr≡le-7
φsκ6u-u∙Jeφ*l
0	20	40	60	80	100
Epoch
0.9725
ɪ Jl Jl J
ω*jeα6EEe,u~l
10000	30000	50000	70000
Iteration
(c)
Figure 11: Robustness to initial learning rates on MNIST and CIFAR-10. Top row: RMSprop-APO tuning
an MLP on MNIST. Bottom row: RMSprop-APO tuning ResNet34 on CIFAR-10. (a) Training loss; (b) test
accuracy; (c) learning rate adaptation.
First, we used RMSprop-APO to optimize Rosenbrock, starting with a wide range of initial learning
rates; we see in Figure 10 that the training losses and learning rates are nearly identical between all
these experiments.
Next, we trained an MLP on MNIST and ResNet34 on CIFAR-10 using RMSprop-APO, with the
learning rate of the base optimizer initialized to 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, and 1e-7. We used
the default meta learning rate 0.1. As shown in Figure 11, the training loss, test accuracy, and
learning rate adaptation are nearly identical using these initial learning rates, which span 5 orders of
magnitude.
F The Noisy Quadratic Problem
In this section we apply APO to the noisy quadratic problem investigated in (Wu et al., 2018; Schaul
et al., 2013), and demonstrate that APO overcomes the short horizon bias problem. We optimize a
quadratic function
f(x) = xTHx,
where X ∈ R1000, H isa diagonal matrix H = diag{hι, h2, ∙∙∙ , h1000}, With eigenvalues h evenly
distributed in interval [0.01, 1]. Initially, we setx with each dimension being 100. For each iteration,
we can access the noisy version of the function, i.e., the gradient and function value of function
f(x) = (X — C)T H (X — c).
Here c is the vector of noise: each dimension of c is independently randomly sampled from a normal
distribution at each iteration, and the variance of dimension i is set to be -1. For SGD, we consider
the following four learning rate schedules: optimal schedule, exponential schedule, linear schedule
anda fixed learning rate. For SGD with APO, we directly use function f as the loss approximation h,
use Euclidean distance norm square as the dissimilarity term D, and consider the following schedules
for λ: optimal schedule(with λ ≥ 0), exponential schedule, linear schedule and a fixed λ. We
calculate the optimal parameter for each schedule of both algorithms so as to achieve a minimal
function value at the end of 300 iterations. We optimize the schedules with 10000 steps of Adam
and learning rate 0.001 after unrolling the entire 300 iterations.
The function values at the end of 300 iterations with each schedule are shown in Table 2.
Figure 12 plots the training loss and learning rate of SGD during the 300 iterations under optimal
schedule, figure 13 plots the training loss and λ under optimal schedule for SGD with APO. It can be
18
Under review as a conference paper at ICLR 2019
	Optimal	Exponential	Linear	Fix
SGD	3.3	4.6	12.5	81.9
SGD + APO	4.69	6.73	15.6	80.8
Table 2: Loss at the end of 300 iterations with optimized schedules.
(a) Loss of different optimal schedules.
(b) Learning rate of different optimal schedules.
(a) Loss of different optimal schedules.
Figure 13: Loss and learning rate for APO.
Figure 12: Loss and learning rate for SGD.
(b) Learning rate of different optimal schedules.
seen that SGD with APO achieves almost the same training loss as optimal SGD for noisy quadratics
task. This indicates that APO doesn’t suffer from the short-horizon bias mentioned in (Wu et al.,
2018).
G Adam Experiments
Adam (Kingma & Ba, 2015) is an adaptive optimization algorithm widely used for training neural
networks, which can be seen as RMSProp with momentum. The update rule is given by:
mt — β1mt-1 + (I - βI)gt	(22)
Vt 一 β2Vt-i + (1 — β2)gl	(23)
mt 一 mt∕(1 — βt)	(24)
Vt 一 vt∕(1- β2)	(25)
Ot - θt-i — ηmt / (Pt + e)	(26)
where gt = R θ 't(θt-ι).
Similar to SGD with Nesterov momentum, we fixed the β1 and β2 for Adam, and used APO to
tune the global learning rate η. We tested Adam-APO with a ResNet34 network on CIFAR-10
dataset, and compared it with both Adam with fixed learning rate and Adam with a learning rate
schedule where the learning rate is initialized to 1e-3 and is decayed by a factor of 5 every 60
epochs. Similarly to SGD with momentum, we found that Adam generally benefits from larger
values of λ. Thus, we recommend performing a grid search over λ values from 1e-2 to 1. As shown
in Figure 14, APO improved both the training loss and test accuracy compared to the fixed learning
rate, and achieved comparable performance as the manual learning rate schedule.
19
Under review as a conference paper at ICLR 2019
0.95-
0.94-
AO.93-
Φ 0.92
5 0.91
< 0.90
⅛ 0.89-
l- 0.88-
0.87
0.86-
25	50 75 100 125 150 175 200
Epoch
ɪ I I
aeκ6u-UJeθη
10000	30000	50000	70000
Iteration
Figure 14: Adam results on CIFAR-10. We compare Adam with a fixed learning rate and a manual learning
rate schedule to Adam-APO.
H Comparison with Population-Based Training
Population-based training (PBT) (Jaderberg et al., 2017) is an approach to hyperparameter optimiza-
tion that trains a population of N neural networks simultaneously: each network periodically eval-
uates its performance on a target measure (e.g., the training loss); poorly-performing networks can
exploit better-performing members of the population by cloning the weights of the better network,
copying and perturbing the hyperparameters used by the better network, and resuming training. In
this way, a single model can essentially experience multiple hyperparameter settings during training;
in particular, we are interested in evaluating the learning rate schedule found using PBT.
Here, we used PBT to tune the learning rate for RMSprop, to optimize a ResNet34 model on CIFAR-
10. For PBT, we used a population of size 4 (which we found to perform better than a population of
size 10), and used a perturbation strategy that consists of randomly multiplying the learning rate by
either 0.8 or 1.2. In PBT, one can specify the probability with which to re-sample a hyperparameter
value from an underlying distribution. We found that it was critical to set this to 0; otherwise, the
learning rate could jump from small to large values and cause instability in training. Figure 15
compares PBT with APO; we show the best training loss achieved by any of the models in the PBT
population, as a function of wall-clock time. For a fair comparison between these methods, we ran
both PBT and APO using 1 GPU. We see that APO outperforms PBT, achieving a training loss an
order of magnitude smaller than PBT, and achieves the same test accuracy, much more quickly.
SSoIU-S1-
(a) Train loss
Figure 15: Training loss and test accuracy of APO and PBT, as a function of wall-clock time.
Time (s)
(b) Test accuracy
20