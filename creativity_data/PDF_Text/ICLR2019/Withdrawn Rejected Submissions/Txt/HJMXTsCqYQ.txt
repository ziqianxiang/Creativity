Under review as a conference paper at ICLR 2019
Constrained Bayesian Optimization for
Automatic Chemical Design
Anonymous authors
Paper under double-blind review
Ab stract
Automatic Chemical Design provides a framework for generating novel molecules
with optimized molecular properties. The current model suffers from the pathol-
ogy that it tends to produce invalid molecular structures. By reformulating the
search procedure as a constrained Bayesian optimization problem, we showcase
improvements in both the validity and quality of the generated molecules. We
demonstrate that the model consistently produces novel molecules ranking above
the 90th percentile of the distribution over training set scores across a range of
objective functions. Importantly, our method suffers no degradation in the com-
plexity or the diversity of the generated molecules.
1	Introduction
There are two fundamental ways in which Machine Learning can be leveraged in chemical design:
1.	To evaluate a molecule for a given application.
2.	To find a promising molecule for a given application.
There has been much progress in the first use-case through the development of quantitative structure
activity relationship (QSAR) models using deep learning (Ma et al., 2015). These models have
achieved state-of-the-art results in predicting properties of known molecules.
The second use-case, finding new molecules that are useful for a given application, is arguably more
important however. One existing approach for finding molecules that maximise an application-
specific metric involves searching a large library of compounds, either physically or virtually Pyzer-
Knapp et al. (2015). This has the disadvantage that the search is not open-ended; if the molecule is
not in the library you specify, the search won’t find it.
A second method involves the use of genetic algorithms. In this approach, a known molecule acts as
a seed and a local search is performed over a discrete space of molecules. Although these methods
have enjoyed success in producing biologically active compounds, an approach featuring a search
over an open-ended, continuous space would be beneficial. The use of geometrical cues such as
gradients to guide the search in continuous space could accelerate both drug (Pyzer-Knapp et al.,
2015; Gomez-Bombarelli et al., 2016a) and material (Hachmann et al., 2011; 2014) discovery by
functioning as a high-throughput virtual screen of unpromising candidates.
Recently, Gomez-Bombarelli et al. (Gomez-Bombarelli et al., 2016b) presented Automatic Chem-
ical Design, a VAE architecture capable of encoding continuous representations of molecules. In
continuous latent space, gradient-based optimization is leveraged to find molecules that maximize a
design metric.
Although a strong proof of concept, Automatic Chemical Design possesses a deficiency in so far
as it fails to generate a high proportion of valid molecular structures. Gomez-Bombarelli et al.
(2016b) hypothesise that molecules selected by Bayesian Optimization lie in “dead regions” of the
latent space far away from any data that the VAE has seen in training, yielding invalid structures
when decoded. Although there have been many attempts to address the issue of generating valid
molecules, they all rely on model assumptions that can negatively impact either the complexity
(Jaques et al., 2017; Janz et al., 2017; Kusner et al., 2017) or the diversity (Jin et al., 2018) of the
generated molecules.
1
Under review as a conference paper at ICLR 2019
Figure 1: The SMILES and one-hot encoding for benzene. For simplicity only the characters present
in Benzene are shown in the one-hot encoding. In reality there would be a column for each character
in the SMILES alphabet.
Figure 2: The SMILES Variational Autoencoder with the learned constraint function illustrated by
a circular feasible region in the latent space.
The principle contribution of this paper will be to present an approach based on constrained Bayesian
optimization that generates a high proportion of valid sequences without relying on model assump-
tions that affect the complexity or the diversity of the generated molecules.
2	Methods
2.1	SMILES Representation
SMILES strings Weininger (1988) are a means of representing molecules as a character sequence.
This text-based format facilitates the use of tools from natural language processing for applications
such as organic chemistry reaction prediction Schwaller et al. (2017); Jin et al. (2017). For use with
the variational autoencoder, the SMILES strings in turn are converted into one-hot vectors indicating
the presence or absence of a particular character within a sequence as illustrated in Figure 1.
2.2	Variational Autoencoder
Variational autoencoders Kingma & Welling (2013); Kingma et al. (2014) allow us to map molecules
x to and from continuous values z in a latent space. The encoding z is interpreted as a latent variable
in a probabilistic generative model over which there is a prior distribution p(z). The probabilistic de-
coder is defined by the likelihood function pθ (x|z). The posterior distribution pθ (z|x) is interpreted
as the probabilistic encoder. The parameters of the likelihood pθ (x|z) as well as the parameters of
the approximate posterior distribution q© (z |x) are learned by maximizing the evidence lower bound
(ELBO)
L(φ,θ; x) = Eqφ(z∣χ) [log Pθ (x,z) - log qφ(z∣x)].
Variational autoencoders have been coupled with recurrent neural networks by Bowman et al. (2015)
to encode sentences into a continuous latent space. This approach is followed for the SMILES format
both by GOmez-BOmbarelli et al. (2016b) and here. The SMILES variational autoencoder, together
with our constraint function, is shown in Figure 2.
2
Under review as a conference paper at ICLR 2019
2.3	Objective Functions for Bayesian Optimization of Molecules
Bayesian Optimization is performed in the latent space of the variational autoencoder in order to
find molecules that score highly under a specified objective function. We assess molecular quality
on the following objectives:
JcloogmPp(m) = logP(m) - SA(m) - ring-penalty(m),
JcQoEmDp (m) = QED(m) - SA(m) - ring-penalty(m),
JQED(m) = QED(m).
m denotes a molecule, logP(m) is the water-octanol partition coefficient, QED(m) is the quantita-
tive estimate of drug-likeness (Bickerton et al., 2012) and SA(m) is the synthetic accessibility (Ertl
& Schuffenhauer, 2009). The ring penalty term is as featured in (Gomez-Bombarelli et al., 2016b).
The “comp” subscript is designed to indicate that the objective function is a composite of standalone
metrics.
It is important to note, that the first objective, a common metric of comparison in this area Gomez-
Bombarelli et al. (2016b); Kusner et al. (2017); Jin et al. (2018), is mis-specified. From a chemical
standpoint it is undesirable to maximize the logP score as is being done here. Rather it is preferable
to optimize logP to be in a range that is in accordance with the Lipinski Rule of Five Lipinski et al.
(1997). We use the penalized logP objective here because regardless of its relevance for chemistry,
it serves as a point of comparison against other methods.
2.4	Constrained Bayesian Optimization of Molecules
We now describe our extension to the Bayesian Optimization procedure followed by Gomez-
Bombarelli et al. (2016b). Expressed formally, the constrained optimization problem is
max f(m) s.t. Pr(C(m)) ≥ 1 - δ
m
where f (m) is a black-box objective function, Pr(C(m)) denotes the probability that a boolean
constraint C(m) is satisfied and 1 - δ is some user-specified minimum confidence that the constraint
is satisfied (Gelbart et al., 2014). The constraint is that a latent point must decode successfully a
large fraction of the times decoding is attempted. The black-box objective function is noisy because
a single latent point may decode to multiple molecules when the model makes a mistake, obtaining
different values under the objective. In practice, f(m) is one of the objectives described in section
2.3
2.5	Expected Improvement with Constraints (EIC)
EIC may be thought of as expected improvement (EI),
EI(m) = Ef(m) max(0, η - f (m)),
that offers improvement only when a set of constraints are satisfied (Schonlau et al., 1998):
EIC(m) = EI(m) Pr(C(m)).
The incumbent solution η in EI(m), may be set in an analogous way to vanilla expected improvement
(Gelbart, 2015) as either:
1.	The best observation in which all constraints are observed to be satisfied.
2.	The minimum of the posterior mean such that all constraints are satisfied.
3
Under review as a conference paper at ICLR 2019
The latter approach is adopted for the experiments performed in this paper. If at the stage in the
Bayesian optimization procedure where a feasible point has yet to be located, the form of acquisition
function used is that defined by (Gelbart, 2015)
EIC(m)	Pr(C(m)EI(m),	if ∃m, Pr(C(m)) ≥ 1 - δ
Pr(C(m)),	otherwise
with the intuition being that if the probabilistic constraint is violated everywhere, the acquisition
function selects the point having the highest probability of lying within the feasible region. The al-
gorithm ignores the objective until it has located the feasible region. It would also have been possible
to adopt the methodologies of Rainforth et al. (2016); Mueller et al. (2017) under the assumption
that the constraint is cheap to evaluate.
3	Related Work
Gomez-Bombarelli et al.(2016b); Segler et al. (2017) constructed character-level generative models
of SMILES strings using recurrent decoders. These models both suffered from the problem that
they produced a high proportion of invalid SMILES strings. Blaschke et al. (2017) compared a
range of autoencoder architectures in terms of their ability to produce valid molecules. Kusner et al.
(2017); Dai et al. (2018) addressed the issue of generating valid molecules explicitly by imposing
syntactic and semantic constraints using context free and attribute grammars while Janz et al. (2017);
Guimaraes et al. (2017) leveraged additional training signal to enforce molecular validity.
A potential drawback of these methods is that the constraints may favour the generation of simple
molecules. Simonovsky & Komodakis (2018); Li et al. (2018) build generative models of molecular
graphs but do not impose any constraints for molecular validity. Jin et al. (2018) build a genera-
tive model of molecular graphs with constraints in order to achieve 100% validity. One possible
disadvantage of this approach is that each molecule is assumed to be constructed from a fixed vo-
cabulary of subgraphs and so it will be impossible to generate a molecule comprised of subgraphs
outside this vocabulary. De Cao & Kipf (2018) use a graph-based generative model which enforces
a quality constraint in the latent space. Direct comparison with the aforementioned methods is com-
plicated due to the orthogonality of the assumptions made for each model. As such, we compare out
method against Gomez-Bombarelli et al. (2016b) only.
Our principle contribution is in showing that valid molecules can be generated without encoding
assumptions into the design model that limit the complexity and diversity of the generated molecules.
4	Experiment I: Drug Design
In this section We conduct an empirical test of the hypothesis from (Gomez-Bombarelli et al., 2016b)
that the decoder’s lack of efficiency is due to data point collection in “dead regions” of the latent
space far from the data on Which the VAE Was trained. We use this information to construct a binary
classification Bayesian Neural NetWork (BNN) to serve as a constraint function that outputs the
probability of a latent point being valid. Secondly, We compare the performance of our constrained
Bayesian optimization implementation against the original model (baseline) in terms of the num-
bers of valid and drug-like molecules generated. Thirdly, We compare the quality of the molecules
produced by constrained Bayesian optimization With those of the baseline model.
4.1	Implementation
The implementation details of the encoder-decoder netWork as Well as the sparse GP for mod-
elling the objective remain unchanged from (Gomez-Bombarelli et al., 2016b). For the constrained
Bayesian optimization algorithm, the BNN is constructed With 2 hidden layers each 100 units Wide
With ReLU activation functions and a logistic output. Minibatch size is set to 1000 and the netWork
is trained for 5 epochs With a learning rate of 0.0005. 20 iterations of parallel Bayesian optimization
are performed using the Kriging-Believer algorithm in all cases. Data is collected in batch sizes of
50. The same training set as (Gomez-Bombarelli et al., 2016b) is used, namely 249,456 drug-like
molecules draWn at random from the ZINC database (IrWin et al., 2012).
4
Under review as a conference paper at ICLR 2019
(a) % Valid Molecules
(b) % Methane Molecules
(c) % Drug-like Molecules
Figure 3: Experiments on 5 disjoint sets comprising 50 latent points each. Very small (VS) Noise are
training data latent points with approximately 1% noise added to their values, Small (S) Noise have
10% noise added to their values and Big (B) Noise have 50% noise added to their values. All latent
points underwent 500 decode attempts and the results are averaged over the 50 points in each set.
The percentage of decodings to: a) valid molecules b) methane molecules. c) drug-like molecules.
4.2	Diagnostic Experiments and Labelling Criteria
These experiments were designed to test the hypothesis that points collected by Bayesian optimiza-
tion lie far away from the training data in latent space. In doing so, they also serve as labelling
criteria for the data collected to train the BNN acting as the constraint function. The resulting ob-
servations are summarized in Figure 3.
There is a noticeable decrease in the percentage of valid molecules decoded as one moves further
away from the training data in latent space. Points collected by Bayesian optimization do the worst
in terms of the percentage of valid decodings. This would suggest that these points lie farthest from
the training data. The decoder over-generates methane molecules when far away from the data. One
hypothesis for why this is the case is that methane is represented as ’C’ in the SMILES syntax and
is by far the most common character. Hence far away from the training data, combinations such as
’C’ followed by a stop character may have high probability under the distribution over sequences
learned by the decoder.
Given that methane has far too low a molecular weight to be a suitable drug candidate, a third plot
in Figure 3(c), shows the percentage of decoded molecules such that the molecules are both valid
and have a tangible molecular weight. The definition of a tangible molecular weight was interpreted
somewhat arbitrarily as a SMILES length of 5 or greater. Henceforth, molecules that are both valid
and have a SMILES length greater than 5 will be loosely referred to as drug-like. This is not to
imply that a molecule comprising five SMILES characters is likely to be drug-like, but rather this
SMILES length serves the purpose of determining whether the decoder has been successful or not.
As a result of these diagnostic experiments, it was decided that the criteria for labelling latent points
to initialize the binary classification neural network for the constraint would be the following: if the
latent point decodes into drug-like molecules in more than 20% of decode attempts, it should be
classified as drug-like and non drug-like otherwise.
4.3	Molecular Validity
The BNN for the constraint was initialized with 117, 440 positive class points and 117, 440 negative
class points. The positive points were obtained by running the training data through the decoder
assigning them positive labels if they satisfied the criteria outlined in the previous section. The
negative class points were collected by decoding points sampled uniformly at random across the
56 latent dimensions of the design space. Each latent point undergoes 100 decode attempts and
the most probable SMILES string is retained. JcloogmPp(m) is the choice of objective function. The
relative performance of constrained Bayesian optimization and unconstrained Bayesian optimization
(baseline) (Gomez-Bombareni et al., 2016b) is compared in 4(a).
The results show that greater than 80% of the latent points decoded by constrained Bayesian opti-
mization produce drug-like molecules compared to less than 5% for unconstrained Bayesian opti-
mization. One must account however, for the fact that the constrained approach may be decoding
5
Under review as a conference paper at ICLR 2019
(a) Drug-like Molecules
(b) New Drug-like Molecules
Figure 4: a) The percentage of latent points decoded to drug-like molecules. The results are from 20
iterations of Bayesian optimization with batches of 50 data points collected at each iteration (1000
latent points decoded in total). The standard error is given for 5 separate train/test set splits of 90/10.
Figure 5: The best scores for new molecules generated from the baseline model (blue) and the model
with constrained Bayesian optimization (red). The vertical lines show the best scores averaged over
5 separate train/test splits of 90/10. For reference, the histograms are presented against the backdrop
of the top 10% of the training data in the case of Composite LogP and QED, and the top 20% of the
training data in the case of Composite QED.
multiple instances of the same novel molecules. Constrained and unconstrained Bayesian optimiza-
tion are compared on the metric of the percentage of unique novel molecules produced in 4(b).
One may observe that constrained Bayesian optimization outperforms unconstrained Bayesian op-
timization in terms of the generation of unique molecules, but not by a large margin. A manual
inspection of the SMILES strings collected by the unconstrained optimization approach showed that
there were many strings with lengths marginally larger than the cutoff point, which is suggestive
of partially decoded molecules. As such, a fairer metric for comparison should be the quality of
the new molecules produced as judged by the scores from the black-box objective function. This is
examined next.
6
Under review as a conference paper at ICLR 2019
Table 1: Percentile of the averaged new molecule score relative to the training data. The results of 5
separate train/test set splits of 90/10 are provided.
Objective	Baseline	Constrained
LogP Composite	36± 14	92± 4
QED Composite	14± 3	72± 10
QED	11±2	79± 4
Figure 6: The best scores for novel molecules generated by the constrained Bayesian optimization
model optimizing for PCE. The results are averaged over 3 separate runs with train/test splits of
90/10.
4.4	Molecular Quality
The results of Figure 5 indicate that constrained Bayesian optimization is able to generate higher
quality molecules relative to unconstrained Bayesian optimization across the three drug-likeness
metrics introduced in section 2.3. Over the 5 independent runs, the constrained optimization proce-
dure in every run produced new drug-like molecules ranked in the 100th percentile of the distribution
over training set scores for the JcloogmPp (m) objective and over the 90th percentile for the remaining
objectives. Table 1 gives the percentile that the averaged score of the new molecules found by each
process occupies in the distribution over training set scores.
5	Experiment II: Material Design
In order to show that the constrained Bayesian optimization approach is extensible beyond the realm
of drug design, we trained the model on data from the Harvard Clean Energy Project (Hachmann
et al., 2011; 2014) to generate molecules optimized for power conversion efficiency (PCE).
5.1	Implementation
A neural network was trained to predict the PCE of 200, 000 molecules drawn at random from the
Harvard Clean Energy Project dataset using 512-bit Morgan circular fingerprints Rogers & Hahn
(2010) as input features with bond radius of 2 using RDKit. If unmentioned the details of the
implementation remain the same as section 4.
5.2	Results
The results are given in Figure 6. The averaged score of the new molecules generated lies above the
90th percentile in the distribution over training set scores. Given that the objective function in this
instance was learned using a neural network, advances in predicting chemical properties from data
Duvenaud et al. (2015); Ramsundar et al. (2015) are liable to yield concomitant improvements in
the optimized molecules generated through this approach.
7
Under review as a conference paper at ICLR 2019
6	Conclusion and Future Work
6.1	Contributions
The reformulation of the search procedure in the Automatic Chemical Design model as a constrained
Bayesian optimization problem has led to concrete improvements on two fronts:
1.	Validity - The number of valid molecules produced by the constrained optimization pro-
cedure offers a marked improvement over the original model. Notably, by applying con-
straints solely to the latent space of the variational autoencoder, the method does not require
model assumptions that compromise either the complexity or the diversity of the generated
molecules.
2.	Quality - For five independent train/test splits, the scores of the best molecules generated
by the constrained optimization procedure consistently ranked above the 90th percentile of
the distribution over training set scores for all objectives considered. The ability to find
new molecules that are competitive with the very best of a training set of already drug-like
molecules is a powerful demonstration of the model’s capabilities. As a further point, the
generality of the approach should be emphasised. This approach is liable to work for a
large range of objectives encoding countless desirable molecular properties.
6.2	Future Work
Future work could investigate whether performance gains can be achieved through the implementa-
tion of a more accurate constraint model. Recent work by Blaschke et al. (2017); Polykovskiy et al.
(2018); Tabor et al. (2018); Aumentado-Armstrong (2018); Sanchez-Lengeling & Aspuru-Guzik
(2018) has featured a more targeted search for novel compounds. This represents a move towards
more industrially-relevant objective functions for Bayesian Optimization which should ultimately
replace the chemically mis-specified objectives, such as the penalized logP score, identified here.
8
Under review as a conference paper at ICLR 2019
References
Tristan Aumentado-Armstrong. Latent molecular optimization for targeted therapeutic design. arXiv
preprint arXiv:1809.02032, 2018.
R.	G. Bickerton, G. V. Paolini, J. Besnard, S. Muresan, and A. L. Hopkins. Quantifying the chemical
beauty of drugs. Nature chemistry, 4(2):90-98, 2012.
Thomas Blaschke, Marcus Olivecrona, OlaEngkvist, Jurgen Bajorath, and Hongming Chen. APPli-
cation of generative autoencoder in de novo molecular design. Molecular informatics, 2017.
S.	R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio. Generating sentences
from a continuous sPace. arXiv preprint arXiv:1511.06349, 2015.
T.	D. Bui, J. Yan, and R. E. Turner. A unifying framework for sParse gaussian Process aPProximation
using Power exPectation ProPagation. arXiv preprint arXiv:1605.07066, 2016.
Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoen-
coder for structured data. arXiv preprint arXiv:1802.08786, 2018.
Nicola De Cao and Thomas KiPf. Molgan: An imPlicit generative model for small molecular graPhs.
arXiv preprint arXiv:1805.11973, 2018.
D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Gomez-Bombarelli, T. Hirzel, A. Aspuru-
Guzik, and R. P. Adams. Convolutional networks on graPhs for learning molecular fingerPrints.
In Proceedings of the 28th International Conference on Neural Information Processing Systems -
Volume 2, NIPS’15, pp. 2224-2232, Cambridge, MA, USA, 2015. MIT Press.
P. Ertl and A. Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules
based on molecular complexity and fragment contributions. Journal of cheminformatics, 1(1):8,
2009.
M. A. Gelbart. Constrained Bayesian Optimization and Applications. PhD thesis, Harvard Univer-
sity, 2015.
M. A. Gelbart, J. Snoek, and R. P. Adams. Bayesian optimization with unknown constraints. arXiv
preprint arXiv:1403.5607, 2014.
David Ginsbourger, Rodolphe Le Riche, and Laurent Carraro. Kriging is well-suited to parallelize
optimization. In Computational intelligence in expensive optimization problems, pp. 131-162.
Springer, 2010.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,
pp. 249-256, Chia Laguna Resort, Sardinia, Italy, 2010. PMLR.
R. Gomez-Bombarelli, J. Aguilera-Iparraguirre, T. D. Hirzel, D. Duvenaud, D. Maclaurin, M. A.
Blood-Forsythe, H. S. Chae, M. Einzinger, D-G. Ha, T. Wu, and G. Markopoulos. Design of
efficient molecular organic light-emitting diodes by a high-throughput virtual screening and ex-
perimental approach. Nature materials, 15(10):1120-1127, 2016a.
R. Gomez-Bombarelli, D. Duvenaud, J. M. Hernandez-Lobato, J. Aguilera-Iparraguirre, T. D.
Hirzel, R. P. Adams, and A. Aspuru-Guzik. Automatic chemical design using a data-driven con-
tinuous representation of molecules. arXiv preprint arXiv:1610.02415, 2016b.
Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Pedro Luis Cunha Farias, and Alan Aspuru-
Guzik. Objective-reinforced generative adversarial networks (organ) for sequence generation
models. arXiv preprint arXiv:1705.10843, 2017.
J. Hachmann, R. Olivares-Amaya, S. Atahan-Evrenk, C. Amador-Bedolla, R. S. Snchez-Carrera.,
A. Gold-Parker, L. Vogt, A. M. Brockway, and A. Aspuru-Guzik. The harvard clean energy
project: Large-scale computational screening and design of organic photovoltaics on the world
community grid. The Journal of Physical Chemistry Letters, 2(17):2241-2251, 2011. doi: 10.
1021/jz200866s.
9
Under review as a conference paper at ICLR 2019
J. Hachmann, R. Olivares-Amaya, A. Jinich, A. L. Appleton, M. A. Blood-Forsythe, L. R. Seress,
C. Roman-Salgado, K. Trepte, S. Atahan-Evrenk, S. Er, S. Shrestha, R. Mondal, A. Sokolov,
Z. Bao, and A. Aspuru-Guzik. Lead candidates for high-performance organic photovoltaics from
high-throughput quantum chemistry - the harvard clean energy project. Energy Environ. Sci., 7:
698-704, 2014.
J. M. Hernandez-Lobato, Y. Li, M. Rowland, T. Bui, D. Hernandez-Lobato, and R. E. Turner. Black-
box alpha divergence minimization. In Maria Florina Balcan and Kilian Q. Weinberger (eds.),
Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceed-
ings of Machine Learning Research, pp. 1511-1520, New York, New York, USA, 20-22 Jun
2016. PMLR.
J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G. Coleman. Zinc: a free tool to
discover chemistry for biology. Journal of chemical information and modeling, 52(7):1757-1768,
2012.
David Janz, Jos van der Westhuizen, Brooks Paige, Matt J Kusner, and Jose Miguel Hernandez-
Labato. Learning a generative model for validity in complex discrete structures. arXiv preprint
arXiv:1712.01664, 2017.
N. Jaques, S. Gu, D. Bahdanau, J. M. Hernandez-Lobato, R. E. Turner, and D. Eck. Sequence
tutor: Conservative fine-tuning of sequence generation models with KL-control. In Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pp. 1645-1654, International Convention Centre, Sydney, Australia, 06-11
Aug 2017. PMLR.
Wengong Jin, Connor Coley, Regina Barzilay, and Tommi Jaakkola. Predicting organic reaction out-
comes with weisfeiler-lehman network. In Advances in Neural Information Processing Systems,
pp. 2604-2613, 2017.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. arXiv preprint arXiv:1802.04364, 2018.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems,
pp. 3581-3589, 2014.
Matt J Kusner, Brooks Paige, and Jose Miguel Hernandez-Lobato. Grammar variational autoen-
coder. In International Conference on Machine Learning, pp. 1945-1954, 2017.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative
models of graphs. arXiv preprint arXiv:1803.03324, 2018.
C.	A. Lipinski, F. Lombardo, B. W. Dominy, and P. J. Feeney. Experimental and computational
approaches to estimate solubility and permeability in drug discovery and development settings.
Advanced drug delivery reviews, 23(1-3):3-25, 1997.
D.	C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Mathe-
matical programming, 45(1):503-528, 1989.
Junshui Ma, Robert P Sheridan, Andy Liaw, George E Dahl, and Vladimir Svetnik. Deep neural
nets as a method for quantitative structure-activity relationships. Journal of chemical information
and modeling, 55(2):263-274, 2015.
Jonas Mueller, David Gifford, and Tommi Jaakkola. Sequence to better sequence: continuous re-
vision of combinatorial structures. In International Conference on Machine Learning, pp. 2536-
2544, 2017.
10
Under review as a conference paper at ICLR 2019
Daniil Polykovskiy, Alexander Zhebrak, Dmitry Vetrov, Yan Ivanenkov, Vladimir Aladinskiy, Ma-
rine Bozdaganyan, Polina Mamoshina, Alex Aliper, Alex Zhavoronkov, and Artur Kadurin. En-
tangled conditional adversarial autoencoder for de-novo drug discovery. Molecular pharmaceu-
tics, 2018.
E.	O. Pyzer-Knapp, C. Suh, R. GOmez-Bombarelli, J. Aguilera-Iparraguirre, and A. Aspuru-Guzik.
What is high-throughput virtual screening? a perspective from organic materials discovery. An-
nual Review ofMaterials Research, 45:195-216, 2015.
Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A Osborne, and Frank Wood.
Bayesian optimization for probabilistic programs. In Advances in Neural Information Processing
Systems, pp. 280-288, 2016.
B. Ramsundar, S. Kearnes, P. Riley, D. Webster, D. Konerding, and V. Pande. Massively Multitask
Networks for Drug Discovery. ArXiv e-prints, February 2015.
RDKit. Rdkit: Open-source cheminformatics. URL http://www.rdkit.org.
D. Rogers and M. Hahn. Extended-connectivity fingerprints. Journal of chemical information and
modeling, 50(5):742-754, 2010.
Benjamin Sanchez-Lengeling and Alan Aspuru-Guzik. Inverse molecular design using machine
learning: Generative models for matter engineering. Science, 361(6400):360-365, 2018.
M. Schonlau, W. J. Welch, and D. R. Jones. Global versus local search in constrained optimization
of computer models. Lecture Notes-Monograph Series, pp. 11-25, 1998.
Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, and Teodoro Laino. ” found in
translation”: Predicting outcome of complex organic chemistry reactions using neural sequence-
to-sequence models. arXiv preprint arXiv:1711.04810, 2017.
Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused
molecule libraries for drug discovery with recurrent neural networks. ACS Central Science, 2017.
Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using
variational autoencoders. arXiv preprint arXiv:1802.03480, 2018.
Daniel P Tabor, Loic M Roch, Semion K Saikin, Christoph Kreisbeck, Dennis Sheberla, Joseph H
Montoya, Shyam Dwaraknath, Muratahan Aykol, Carlos Ortiz, Hermann Tribukait, et al. Ac-
celerating the discovery of materials for clean energy in the era of smart automation. Nat. Rev.
Mater., 3:5-20, 2018.
Theano Development Team. Theano: A Python framework for fast computation of mathematical
expressions. arXiv e-prints, abs/1605.02688, 2016.
David Weininger. Smiles, a chemical language and information system. 1. introduction to method-
ology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36,
1988.
11
Under review as a conference paper at ICLR 2019
(a) Minima Locations
(b) Disk Constraint
Figure 7: Constrained Bayesian optimization of the 2D Branin-Hoo Function.
A	Toy Experiment: The Branin-Hoo Function
The Branin-Hoo function will act as a toy problem on which to test the functionality of the al-
gorithmic implementation for constrained Bayesian optimization. The particular variant of the
Branin-Hoo optimization of interest here is the constrained formulation of the problem as fea-
tured in Gelbart et al. (2014). This Branin-Hoo function has three global minima at the coordinates
(-π, 12.275), (π, 2.275) and (9.42478, 2.475). In order to formulate the problem as a constrained
optimization problem, a disk constraint on the region of feasible solutions is introduced. In con-
trast to the formulation of the problem in Gelbart et al. (2014), the disk constraint is coupled in this
scenario in the sense that the objective and the constraint will be evaluated jointly at each iteration
of Bayesian optimization. In addition, the observations of the black-box objective function will be
assumed to be noise-free. The minima of the Branin-Hoo function as well as the disk constraint are
illustrated in Figure 7.
The disk constraint eliminates the upper-left and lower-right solutions, leaving a unique global min-
imum at (π, 2.275). Given that our implementation of constrained Bayesian optimization relies on
the use of a sparse GP as the underlying statistical model of the black-box objective and as such is
designed for scale as opposed to performance, the results will not be compared directly against those
of Gelbart et al. (2014) who use a full GP to model the objective. It will be sufficient to compare the
performance of the algorithm against random sampling. Both the sequential Bayesian optimization
algorithm and the parallel implementation using the Kriging-Believer algorithm Ginsbourger et al.
(2010) are tested.
A.1 Implementation
A Sparse GP featuring the FITC approximation, based on the implementation of Bui et al. (2016)
is used to model the black-box objective function. The kernel choice is exponentiated quadratic
with ARD lengthscales. The number of inducing points M was chosen to be 20 in the case of
sequential Bayesian optimization, and 5 in the case of parallel Bayesian optimization using the
Kriging-Believer algorithm. The sparse GP is trained for 400 epochs using Adam Kingma & Ba
(2014) with the default parameters and a learning rate of 0.005. The minibatch size is chosen to be
5. The extent of jitter is chosen to be 0.00001. A Bayesian Neural Network (BNN), adapted from
the MNIST digit classification network of Hernandez-Lobato et al. (2016) is trained using black-box
alpha divergence minimization to model the constraint.
The network has a single hidden layer with 50 hidden units, Gaussian activation functions and lo-
gistic output units. The mean parameters of q, the approximation to the true posterior, are initialized
by sampling from a zero-mean Gaussian with variance “；& according to the method of Glorot
& Bengio (2010), where din is the dimension of the previous layer in the network and dout is the
dimension of the next layer in the network. The value of α is taken to be 0.5, minibatch sizes are
taken to be 10 and 50 Monte Carlo samples are used to approximate the expectations with respect to
q in each minibatch. The BNN adapted from Hernandez-Lobato et al. (2016) was implemented in
the Theano library Theano Development Team (2016). The LBFGs method Liu & Nocedal (1989)
was used to optimize the EIC acquisition function in all experiments.
12
Under review as a conference paper at ICLR 2019
(a) Collected Data Points
(b) Objective Predictive Mean
11
L Ji
-4	-2 O 2	4	6	8
(C) Pr(C(X) ≥ 0)
Figure 8: a) Data points collected over 40 iterations of sequential Bayesian optimization. b) Con-
tour plot of the predictive mean of the sparse GP used to model the objective function. Lighter
colours indicate lower values of the objective. c) The contour learned by the BNN giving the prob-
ability of constraint satisfaction.
9nΛ> uo-tunLL∙92t⅛qo g-sseəu.ttəg
Figure 9: Performance of Parallel Bayesian Optimization with EIC against Random Sampling.
A.2 Results
The results of the sequential constrained Bayesian optimization algorithm with EIC are shown in
Figure 8. The algorithm was initialized with 50 labeled data points drawn uniformly at random from
the grid depicted. 40 iterations of Bayesian optimization were carried out.
The figures show that the algorithm is correctly managing to collect data in the vicinity of the single
feasible minimum. Figure 9 compares the performance of parallel Bayesian optimization using the
Kriging-Believer algorithm against the results of random sampling. Both algorithms were initialized
using 10 data points drawn uniformly at random from the grid on which the Branin-Hoo function is
defined and were run for 10 iterations of Bayesian optimization. At each iteration a batch of 5 data
points was collected for evaluation.
After 10 iterations, the minimum feasible value of the objective function was 0.42 for parallel
Bayesian optimization with EIC using the Kriging-Believer algorithm and 2.63 for random sam-
pling. The true minimum feasible value is 0.40.
13
Under review as a conference paper at ICLR 2019
Itis very likely that the parallel constrained Bayesian optimization algorithm experienced a serendip-
itous initialisation on the single run shown.
A.3 Discussion
The Branin-Hoo experiment is designed to yield some visual intuition for the constrained Bayesian
Optimization implementation in two dimensions before moving to higher dimensional molecular
space. The results demonstrate that the implementation of constrained Bayesian optimization is
behaving as expected in so far as the constraint in the problem is recognized and the search procedure
outperforms random sampling.
It could be worth performing some investigation into how much worse the sparse GP performs
relative to the full GP in the constrained setting. Another aspect that could be explored is the impact
of the initialization.
14