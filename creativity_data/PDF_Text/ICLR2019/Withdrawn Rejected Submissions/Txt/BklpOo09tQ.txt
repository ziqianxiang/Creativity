Under review as a conference paper at ICLR 2019
Efficient TWO-STEP Adversarial Defense for
Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
in recent years, deep neural networks have demonstrated outstanding performance
in many machine learning tasks. However, researchers have discovered that these
state-of-the-art models are vulnerable to adversarial examples: legitimate exam-
ples added by small perturbations which are unnoticeable to human eyes. Adver-
sarial training, which augments the training data with adversarial examples during
the training process, is a well known defense to improve the robustness of the
model against adversarial attacks. However, this robustness is only effective to
the same attack method used for adversarial training. Madry et al. (2017) sug-
gest that effectiveness of iterative multi-step adversarial attacks and particularly
that projected gradient descent (PGD) may be considered the universal first or-
der adversary and applying the adversarial training with PGD implies resistance
against many other first order attacks. However, the computational cost of the
adversarial training with PGD and other multi-step adversarial examples is much
higher than that of the adversarial training with other simpler attack techniques.
in this paper, we show how strong adversarial examples can be generated only at
a cost similar to that of two runs of the fast gradient sign method (FGSM), allow-
ing defense against adversarial attacks with a robustness level comparable to that
of the adversarial training with multi-step adversarial examples. We empirically
demonstrate the effectiveness of the proposed two-step defense approach against
different attack methods and its improvements over existing defense strategies.
1	Introduction
Despite the fact that deep neural networks demonstrate outstanding performance for many machine
learning tasks, researchers have found that they are susceptible to attacks by adversarial examples
(Szegedy et al. (2014); Goodfellow et al. (2015)). Adversarial examples which are generated by
adding crafted perturbations to legitimate input samples are indistinguishable to human eyes. For
classification tasks, these perturbations may cause the legitimate samples to be misclassified by
the model at the inference time. While there exists no widely agreed conclusion, several studies
attempted to explain the underlying causes of the susceptibility of deep neural networks toward ad-
versarial examples. The vulnerability is ascribed to the linearity of the model (Goodfellow et al.
(2015)), low flexibility (Fawzi et al. (2018)), or the flatness/curvedness of the decision boundaries
(Moosavi-Dezfooli et al. (2017)), but a more general cause is still under research. The recent litera-
ture considered two types of threat models: black-box and white-box attacks. in black-box attacks,
the attacker is assumed to have no access to the architecture and parameters of the model, whereas
in white-box attacks, the attacker has complete access to such information. Several white-box attack
methods were proposed (Goodfellow et al. (2015), Papernot et al. (2016a), Su et al. (2017), Carlini
& Wagner (2016), Moosavi-Dezfooli et al. (2016)). in response, several defenses have been pro-
posed to mitigate the effect of adversarial attacks. These defenses were developed along three main
directions: (1) expanding the training data to make the classifier more robustly learn the underlying
function, e.g., by adversarial training which augments the training data set with adversarial examples
generated by certain attack methods (Szegedy et al. (2014), Goodfellow et al. (2015), Kurakin et al.
(2016)); (2) modifying the training procedure to reduce the gradients of the model w.r.t. the input
such that the classifier becomes more robust to input perturbations, e.g., via input gradient regular-
ization (Ross & Doshi-Velez, 2017), or defensive distillation (Papernot et al., 2016c); and (3) using
1
Under review as a conference paper at ICLR 2019
external models as network add-ons when classifying unseen examples (feature squeezing (Wang
et al., 2018), MagNet (Meng & Chen, 2017), and Defense-GAN) (Samangouei et al., 2018)).
Adversarial training, a simple but effective method to improve the robustness of a deep neural net-
work against white-box adversarial attacks, uses the same white-box attack mechanism to generate
adversarial examples for augmenting the training data set. However, if the attacker applies a differ-
ent attack strategy, adversarial training does not work well due to gradient masking (Papernot et al.,
2016b). Madry et al. (2017) have suggested the effectiveness of iterative multi-step adversarial at-
tacks. In particular, it was suggested that projected gradient descent (PGD) PGD may be considered
the strongest first-order attack so that the adversarial training with PGD can boost the resistance
against many other first-order attacks. However, in the literature a large number (e.g. 40) of steps
of back propagation are typically used in the iterative attack method of PGD or its closely related
variant iterative fast gradient (IFGSM) (Kurakin et al., 2016) to find strong adversarial examples to
be used in each adversarial training step, incurring a prohibitively high computational complexity
particularly for large DNNs or training datasets.
In this paper, we propose an efficient two-step adversarial defense technique, called e2SAD, to fa-
cilitate defense against multiple types of whitebox and blackbox attacks with a quality on a par with
the expensive adversarial training using the well-known multi-step attack the iterative fast gradient
method (IFGSM) (Kurakin et al., 2016). The first step of e2SAD is similar to the basic adversarial
training, where an adversarial example is generated by applying a simple one-step attack method
such as the fast gradient sign method (FGSM). Then in the second step, e2SAD attemps to generate
a second adversarial example at which the vulnerability of the current model is maximally revealed
such that the resulting defense is at the same quality level of the much more expensive IFGSM-
based adversarial training. Finally, the two adversarial examples are taken into consideration in the
proposed loss function according to which a more robust model is trained, resulting strong defense
to both one-step and multi-step iterative attacks with a training time much less less than that of the
adversarial training using IFGSM. The main contributions of this paper are as follows:
•	We propose a computationally efficient method to generate two adversarial examples per
input example while effectively revealing the vulnerability of the learned classifier in the
neighborhood of each clean data point;
•	We show that by considering the generated adversarial examples as part of a well-designed
final loss function, the resulting model is robust to both one-step and iterative white box
attacks;
•	We further demonstrate that by adopting other techniques in our two-step approach like the
use of soft labels and hyper parameter tuning, robust defense against black box attacks can
be achieved.
2	Background
We provide a brief overview of related existing attacks and defense methods, part of which will be
also used to compare with the proposed e2SAD approach.
Attack Models and Algorithms. The goal of all attack models is to find a perturbation δ to be
added to a clean input x ∈ Rd , resulting in an adversarial example xadv = x + δ which may
potentially lead to misclassification of the classifier. Typically, the noise level of the perturbation
is constrained by the '∞ ball denoted by ε to make sure that the perturbation is sufficiently small.
Based on the amount of information the attacker knows, there are two threat levels as follows:
1.	White box: the attacker has full information about the model including its architecture
and parameters such that it is possible craft adversarial examples using techniques such as
gradient based attacks to specifically target the model;
2.	Black box: the attacker has no knowledge about the architecture and parameters of the
model. Neither is the attacker able to query the model. Adversarial examples can be
generated using a substitute model which is a white-box to the attacker.
2
Under review as a conference paper at ICLR 2019
2.1	White Box Attacks
The Fast Gradient Sign Method (FGSM). Given a clean input x and its corresponding true label
y, FGSM perturbs x by (Goodfellow et al., 2015):
Xadv = X + ε ∙ Sign(NxJ(θ,x,y))	(1)
where J(θ, x, y) is the loss function and ε ∈ [0, 1] is a constant value used to constrain the noise
level of the perturbation.
Iterative Fast Gradient Sign Method (IFGSM) and PGD IFGSM attack generates adversarial
examples by iteratively applying FGSM attack multiple, say N, times to a clean input X with a small
constant a (Kurakin et al., 2016)
Xadv = Clampx,ε (Xa-I + a ∙ Sign(NxkdvI J(θ,xk--v「y))) , Xadv = x, k = [1,…，N].⑵
In our implementation, we set a = N. Typically, each component of the input vector, e.g. a pixel,
is normalized to be within [0, 1]. The function clampx,ε is an elementwise clipping function which
clips each element Xi of input X into the range of [maX(0, Xi - ε), min(1, Xi + ε)].
Projected gradient descent (PGD) is a closely related variant of IFGSM. Typically, PGD first ran-
domly picks a point within a confined small ball around each clean input and then applies the multi-
step IFGSM to generate adversarial examples for that clean input.
2.2	Representative Existing Defense Methods
Adversarial Training. This is a popular defense approach which augments the training dataset
with adversarial examples (Goodfellow et al. (2015), Kurakin et al. (2016)). In our implementation,
we adopt the adversarial training equation proposed in (Goodfellow et al., 2015) as the loss function
Jadv(θ,x,y) = α ∙ J(θ, x, y) + (I - α) ∙ J(θ, X	, y),	(3)
where α is a constant specifying the relative importance of the adversarial examples. In our latter
comparison, we choose two methods, FGSM and IFGSM, for generating the adversarial examples.
Minmax. There exist defense methods (Huang et al. (2015); Madry et al. (2017)) which view the
process of training a robust model as solving a minmax optimization problem
argmin E(x,y)〜D
max J(θ, X + δ, y ) ,
δ
(4)
where D is the underlying training data distribution, J(θ, X, y) is the loss function, and θ is the
parameters of the model. In (Huang et al. (2015)) and Madry et al. (2017)), the maximization with
respect to δ is approximated by a specific attack method, for example, by PGD Madry et al. (2017).
Hamm (2018) proposes anew approach which can instead of targeting the saddle points like previous
methods, find the true optimal solution of the minmax optimization problem. Hamm (2018) chooses
FGSM to approximate the inner maximization step and for the outer minimization step, instead of
plugging the adversarial version of the clean data directly and solving the optimization problem,
changes the minimization objective function to
adv εN ∂J(θ, Xadv, y)
argmin J(θ,X	,y) + ^------∂Xadv-卜	⑸
where ε is a constant restricting the level of input perturbation, and N is the number of the training
examples in each minibatch.
3
Under review as a conference paper at ICLR 2019
3	Methods
3.1	Adversarial Training
Adversarial training, which augments the training dataset with adversarial examples during the train-
ing process, has been shown to increase the robustness of the model against white box attacks when
the attack method used to generate the augmented training set is the same as the method used by
the attacker. However, if the attacker uses a different attack strategy to apply the white box at-
tack, adversarial training does not perform well. For example, adversarial training using one-step
FGSM can not improve the robustness of the model against multi-step attacks such as IFGSM and
PGD. However, compared to adversarial training using IFGSM or PGD, adversarial training using
one-step FGSM takes much less time for the training process since it takes only one step of back
propagation to generate adversarial example during each training iteration. Madry et al. (2017) sug-
gest that PGD, one particular type of multi-step iterative adversarial attack, is the strongest universal
first-order adversary. It is also suggested that the model trained by the adversarial training with PGD
is robust against both PGD and one-step FGSM, however at the expenses of multiple steps of back
propagation per a clean training data point.
3.2	Proposed Efficient Two-Step Adversarial Defense (e2SAD)
Our ojective is to develop a defense method with a cost similar to that of FGSM adversarial training
while being robust to both FGSM and multiple-step attacks such as IFGSM.
Iversarial Input Generation
Clean input
First adversarial input
Second adversarial input
FGSM based oɪ
the loss value
Loss of first
adv input
FGSM based on
the dissimilarity teι
For the robustness against iterative adv attacks
For the robustness against one-step adv attacks
J Total Loss Function
CNN
Γ
The dissimilarity term between
Ist adv input and 2nd adv input
Loss of clean
input
Figure 1: e2SAD: the proposed efficient two-step adversarial defense method.
A shown in Figure 1, the proposed efficient two-step adversarial defense (e2SAD) approach takes
only two steps of back propagation to find adversarial examples. First, for a given input we define
the categorical distribution of the model as the vector of probabilities the model outputs, where
each component of the vector representing the probability for the input to be in the corresponding
class. At the first step of e2SAD, a one-step attack method such as FGSM is applied to find the
first adversarial example per each clean input. At the second step, within the neighborhood of the
first adversarial example, the input point whose categorical distribution is most different from that
of the first adversarial point is selected as the second adversarial example. For each clean input,
these two generated adversarial examples are considered in the final loss function for training. The
loss function consists of three terms: the loss of the original clean inputs, the loss of the adversarial
examples generated at the first step, and the dissimilarity in categorical distribution of all pairs of
the corresponding first and second adversarial examples. It is worth noting that the two-step e2SAD
approach is structured in a particular way such that it may provide strong defense against both one-
step and multiple-step attacks, as detailed below.
4
Under review as a conference paper at ICLR 2019
3.2.1	Robustness against one-step adversarial attacks
The main objective of the first step of e2SAD is to find a highly vulnerable neighborhood immedi-
ately around each clean training data point such that the trained model can be made robust to one-step
gradient-based attacks. In so doing, we simply apply a one-step attack method such as FGSM to
maximize the loss around around each clean input xi to generate the first adversarial example xiadv
Xadv = Xi + ει ∙ sign(∆χi J(θ, xi,y)),	(6)
where ε1 is a constant chosen step size. We include the loss of this adversarial example in the final
loss function (8), discussed in detail in the next subsection. Essentially, by doing so, the first term
of (8) guides the training process to reduce the losses of both Xi and Xiadv , acting as a mechanism
for defending one-step adversarial attacks.
3.2.2	Robustness against iterative adversarial attacks
As discussed earlier, compared to one-step adversarial attacks iterative multi-step attacks can be
much stronger as they search the neighborhood of a clean data point more exhaustively, which in
turns makes the adversarial training using iterative adversarial attacks a stronger defense. At the
second step of e2SAD, our goal is to efficiently defend against multi-step attacks by using only one
extra step of computation. As such, the key challenge here is to find a second adversarial example
Xadv which is close to Xadv and can effectively reveal the vulnerability of the model in a way similar
to expensive multi-step attacks.
In a multi-step attack method such as IFGSM or PGD, each adversarial example in the iterative
process is typically found by perturbing the preceding adversarial example to maximize its loss,
where the loss, for example, may be described using the cross entropy based on either the hard or
soft label. Despite this common practice, we argue that a more appropriate approach is to instead
constrain the training process such that a level of similarity (or uniformity) in the prediction of the
trained model is maintained in the neighborhood of each clean input Xi. It is important to note that
maintaining similarity of prediction and minimizing the loss may be correlated but are not necessarily
identical objectives; the latter attempts to ensure that predictions made in some neighborhood of the
input individually have low loss without specifically constraining these predictions to be similar to
each other. Nevertheless, we believe that the objective of maintaining similarity of prediction is
more relevant as far as adversarial defense is concerned as it may lead to a well-regularized decision
boundary around each Xi.
With the above understanding, at the second step of e2SAD, we attempt to find the second adversarial
example Xadv whose categorical distribution is maximally different from that of the first adversarial
example Xiadv in the neighborhood of Xiadv . The dissimilarity in categorical distribution between
these two points is measured by cross entropy (CE). To locate Xadv, FGSM is used as a one-step
optimization method to maximize the CE-based dissimilarity measure
Xadv = Xadv+ε2 ∙ sign (νχadν cE(f (Xadv ,θ),f (Xadv "))孱，。=,项强),	⑺
where ε2 is the step size, and the gradient of the CE-based dissimilarity is evaluated at Xiadv .
The reason for using categorical distribution as the measure of dissimilarity to find the second ad-
versarial point is as follows. First, note that the value of loss for a model prediction does not fully
indicate whether the prediction is a misclassification or not. To see this, consider a simple classi-
fication task with three classes. Assume that the true class labels for two different inputs are both
the first class, and the corresponding categorical distributions are [0.45, 0.55, 0] and [0.4, 0.3, 0.3],
respectively. Let us further assume that one-hot encoding is conventionally used for the labels. In
this case, the model misclassifies the first input while correctly classifies the second. However, this
happens even when the loss of the first input is lower than that of the second input.
Figure 2 shows how the choice of the optimization objective may influence the generation of the
second adversarial example for an illustrative three-class classification problem. The probabilities
of three classes predicted by a trained model for a set of inputs are illustrated using the green, red,
and purple curves, respectively. Accordingly, the one-hot encoding loss as a function of the input
is shown by the blue curve. The cross entropy of categorical distribution between each input and
5
Under review as a conference paper at ICLR 2019
Figure 2: Using the loss and cross-entropy dissimilarity at the second step of e2SAD for a three-class
classification problem. Both the clean input and first adversarial example belong to class 1. Green
cross: the 1st adversarial point; Red cross: the 2nd adversarial point found by applying FGSM on
the loss value; Yellow cross: the 2nd adversarial point found by applying FGSM on the cross entropy
dissimilarity with the 1st adversarial point. Only the yellow cross corresponds to a misclassification.
the first adversarial example (green cross) is shown by the orange curve. Note that both the clean
input and first adversarial example belong to class 1 in this setup. Starting from the green cross,
maximizing the loss using FGSM produces the red cross as the second adversarial example. In
comparison, using the CE dissimilarity measure as the objective function leads to the yellow cross.
While having the highest loss, the red cross is correctly classified by the model. On the other hand,
misclassification happens at the yellow cross which is found based on the CE dissimilarity measure,
suggesting its effectiveness in finding stronger adversarial points.
3.2.3	The final loss function
Based the two adversarial examples generated at the two steps of e2SAD, we design the loss function
used for training the final model as follows. For a mini batch X of m clean examples {xι,…，Xm}
and the corresponding mini batch of the first set of adversarial examples Xadv = {xfdv, ∙∙∙ , XmdV}
generated at the first step of e2SAD, the total loss function is given by
arg min α ∙ J(θ, X, Y),+(1 - α) ∙ J(θ, X
θ
adv,Y) + λ∖ XX D(f (xadv,θ),f (Xadv,θ)) , (8)
where θ is the parameters of the model, each Xadv is the second adversarial example which has the
maximally different categorical distribution from the corresponding first adversarial example xiadv ,
f(X, θ) indicates the categorical distribution output function of the model for input X, and D is the
cross-entropy dissimilarity measure.
3.3	The training process
We adopt label smoothing (Szegedy et al., 2014) for the training process. Here, instead of using hard
labels (one-hot labels) for each cross-entropy loss, we employ the so-called soft labels which assign
the correct class a target probability of 1 - δ and divide the remaining δ probability mass uniformly
among the incorrect classes. We have found that the use of label smoothing in e2SAD leads to better
performance.
The overall training algorithm of the proposed e2SAD approach is summarized in Algorithm 1. The
hyperparameters α and λ specify the weights for the losses of the clean and first set of adversarial
6
Under review as a conference paper at ICLR 2019
inputs and those for the dissimilarity between each pair of the first and second adversarial inputs,
respectively. While the first two terms in the final loss function target the defense against one-step
adversarial attacks, the last term mainly plays the role of defending multi-step attacks. In practice,
α and λ shall be properly chosen to balance between these two different defense needs.
Algorithm 1 The proposed two-step adversarial defense: e2SAD
Input: training dataset (X, Y ); Initial model parameter θ; batch size: m; hyperparameters
α, λ, ε1 , ε2
Output: Trained model parameter θ
1:	for each minibatch t do
2:	for each (xi , yi ) in the current batch do
3：	Xadv — Xi + ει ∙ SigngxiCE(yi, f (xi, θ)))
4:	Xiadv 一 Xadv + ε2 ∙ Sign ZXiadVCE(f(x即,θ),f (Xiadv/))|2。=靖”)
5： end for
6： Ladv = α ∙ J (θ,Xt,Yt ) + (1 - α) ∙ J (θ,Xfd ,匕)
7： Ld = ml λ PLD(f (Xadv ,θ),f (Xiadv ,θ))
8： Ltotal = Ladv + Ld
9： Update θ using backpropogation based on Ltotal
10： end for
11： Return θ
We visually show the two-step e2SAD adversarial example generation process and the loss surfaces
of four different models for a minibatch of 128 clean images from the MNIST handwritten digits
dataset (LeCun et al., 1998) in Figure 3 and Figure 4 of the Appendix, respectively, to demonstrate
the effectiveness of e2SAD.
4 Experimental Results
We compare the proposed e2SAD method with two widely adopted techniques in the literature：
adversarial training using single-step FGSM (Goodfellow et al., 2015) and the adversarial training
using multi-step IFGSM. We also report our experience on the minimax adversarial defense method
proposed in (Hamm, 2018). We adopt the widely used the MNIST handwritten digits dataset (Le-
Cun et al., 1998) and the Street View House Numbers (SVHN) Dataset (Netzer et al., 2011) as
benchmarks.
4.1	Results on the MNIST Handwritten digits dataset
MNIST consists 60,000 training images and 10,000 testing images, where each pixel value is nor-
malized to be within [0, 1]. The adversarial attacks considered are：
•	White-box attacks with FGSM under different noise levels： ε = 0.3, 0.4.
•	White-box attacks with IFGSM under the fixed noise level of ε = 0.3 with different num-
bers of steps： k = 10, 30.
•	Black-box attacks from three substitute models： the naturally trained model (i.e. the one
trained using only the clean inputs without any additional defense strategy), one trained
with FGSM adversaries under the noise level of ε = 0.3, and one trained with IFGSM
adversaries under the total noise level of ε = 0.3 and step size of 0.01 (k = 30). With
respect to these substitute models, IFGSM with ε = 0.3 and k = 30 is used to generate
adversarial examples, which are then employed to attack each of the targeted models.
All CNNs we use consist of two convolutional layers with 32 and 64 filters respectively, each of
which is followed by a 2 × 2 max-pooling layer and ReLU activation function, and a fully connected
layer of 1, 024 neurons. The configuration of the CNNs is summarized in Table 4 in the Appendix.
7
Under review as a conference paper at ICLR 2019
4.1.1	Results on White-Box Attacks
For our proposed e2SAD method, we set the hyperparameters in the training Algorithm 1 as: α =
0.6, λ = 0.1, ε1 = 0.3, and ε2 = 0.1. To increase the searching ability of the second step of e2SAD,
we do not clamp the second adversarial point to be within a norm ball around the clean data point.
All models are trained on MNIST for 30,000 iterations with the batch size of 256.
Comparison with adversarial training The performances of different models under various
white-box attacks are shown in Table 1. It can be seen that each model reaches the accuracy of
over 99% on the clean dataset. The baseline model trained naturally shows no defense ability to-
wards both FGSM and IFGSM adversaries while other three models demonstrate different levels
of defense. The model obtained via FGSM adversarial training maintains a very high accuracy
under FGSM attacks with different noise levels. However, FGSM adversarial training can only
defend FGSM attacks while shows no defense ability against IFGSM attacks of any step number.
IFGSM adversarial training performs well under IFGSM adversaries and also shows robustness
against FGSM attacks. However, the defense ability drops fast when the noise level ε increases in
the case of FGSM attacks. Specifically, the accuracy can drop by almost 14% under the FGSM
attacks when the noise level increases to ε = 0.4. Note that it makes 30 steps to generate IFGSM
adversarial examples in each training iteration, leading to the high cost of the considered IFGSM
adversarial training.
Table 1: Accuracy of different models under white-box adversarial attacks evaluated using MNIST.
Rows are the attacks where “Clean Data“ in the first row means no attack. Columns report the accu-
racy of different defense solutions where ”Natural” means the baseline model without any additional
defense strategy, and “FGSM Adv. Train” and “IFGSM Adv. Train” mean the adversarial training
with FGSM and IFGSM, respectively. “H”(“S”) stands for use of one-hot hard (soft) labels when
training the defense model.
Attack		Label	FGSM Adv. Train Natural	ττ^η	zʃ ε = 0.3 I ε = 0.4	IFGSM Adv. Train ε =0.3,k= 30	e2SAD
Clean Data		H S	0.9942	0.9938	0.9921 0.9952	0.9943	0.9941	0.9913 0.9913	0.9932
FGSM	ε = 0.3	H S	0.1741	0.9768	0.9658 0.4256	0.9846	0.9919	0.9519 0.9595	0.9641
	ε = 0.4	H S	0.1027	0.9136	0.9732 0.2146	0.9374	0.9914	0.8152 0.9012	0.9499
IFGSM	k=10	H S	0.0001	0.0856	0.2108 0.1019	0.1166	0.0101	0.9336 0.9422	0.8687
	k=30	H S	0	0.082	0.1968 0.093	0.0966	0.0065	0.9325 0.9412	0.8633
Among all models considered, the proposed e2SAD method produces the highest accuracy for both
the clean data and FGSM attacks at different noise levels. Under IFGSM attacks e2SAD significantly
outperforms the FGSM adversarial training, demonstrating the effectiveness of the proposed two-
step approach’s generalization capability with respect to defense against strong multi-step attacks.
Compared with the adversarial training using IFGSM, e2SAD offers stronger defense against FGSM
attacks while maintaining a good robustness against IFGSM attacks. Note that these are achieved
using only two steps of gradient calculation in each training iteration, presenting a significant reduc-
tion of computational cost compared with the IFGSM adversarial training, which performs 30 steps
of gradient computation.
Label smoothing is adopted in e2SAD and it is shown to be effective in helping the trained model
generalize well. In our experiments, we set the probability for the correct label to 0.75 and the
one for all other incorrect labels to 0.25. Table 1 shows that label smoothing also improves the
performance of the traditional adversarial training under some circumstances, but not significantly.
8
Under review as a conference paper at ICLR 2019
Table 2: Accuracy of different models under black-box IFGSM attacks evaluated using MNIST.
Substitude Model
Label
Natural
FGSM Adv. Train
ε = 0.3 I ε = 0.4
IFGSM Adv.Train
ε = 0.3, k = 30
e2SAD
Natural Model (H)	H S	0 0.1376	0.9083 0.7887	0.9158 0.8963	0.9668 0.9641	0.868
	H	0.9127	0.082	0.8945	0.967	
FGSM (H) ε = 0.3	S	0.9083	0.7581	0.8214	0.9671	0.9422
	H	0.9163	0.9319	0.9148	0.9324	
IFGSM (H) ε = 0.3,k = 30	S	0.885	0.768	0.8482	0.9574	0.8886
	H	0.9024	0.9742	0.9674	0.9838	
Natural Model (S)	S	0.0929	0.8485	0.8963	0.9847	0.9719
	H	0.9777	0.9708	0.9543	0.9825	
FGSM (S) ε = 0.3	S	0.9745	0.0966	0.7658	0.9832	0.9698
	H	0.939	0.952	0.9464	0.9578	
IFGSM (S) ε = 0.3,k = 30	S	0.9435	0.9476	0.9481	0.9412	0.9525
Comparison with the minimax adversarial defense We also implemented the minimax adver-
sarial defense method proposed in (Hamm, 2018) with a minor modification that the model is
trained using a mixture of clean and adversarial examples to achieve better performance. Our results
show that the trained model is very robust against FGSM attacks, however, shows no defense against
IFGSM attacks.
4.1.2	Results on Black-Box Attacks
In Table 2, we consider how adversarial examples generated by applying IFGSM to a substitute
model may attack a different model. The rows of the table are the considered substitute models:
“Natural model” is again the baseline model without any additional defense strategy; “FGSM ε =
0.3” is the model obtained via FGSM adversarial training with the setting ε = 0.3; “IFGSM ε =
0.3, k = 30” is the model obtained via IFGSM adversarial training with the setting ε = 0.3, k = 30.
The substitution models are trained using hard labels (“H”) and label smoothing (“S”), then attacked
by IFGSM with the setting (ε = 0.3, k = 30) for generating adversarial examples. The adversarial
examples generated from the substitute models are used to attack the four models shown in the
columns of the table: “Natural” is the baseline model; “FGSM Adv. Train” and “IFGSM Adv.
Train” are models trained by the FGSM and IFGSM adversarial training using the settings specified
in the table, respectively; “e2SAD” is the proposed model. The models under attack are trained
using both hard and label smoothing except for e2SAD which is based on label smoothing only.
Note that in Table 2, white-box attacks are resulted when the substitute model and the one under
attack are identical, and all other combinations correspond to black-box attacks.
Table 2 demonstrates that the proposed e2SAD approach delivers a well-balanced defense against
black-box IFGSM attacks from all three substitute models with an accuracy of nearly 90% or higher.
There are several cases under which the natural training (baseline) and FGSM adversarial training
have a poor performance. In all cases, e2SAD either noticeably outperforms both the natural training
and FGSM adversarial training or produce a fairly close performance. Compared with the models
trained with the 30-steps IFGSM adversarial training, e2SAD is still very competitive particularly
given the fact that only two-steps of gradient computation are performed at each training iteration.
4.2 Results on the Street View House Numbers (SVHN) Dataset
The Street View House Numbers (SVHN) dataset ( (Netzer et al., 2011)) consists of a training set of
73,257 digits and a testing set of 26,032 digits obtained from house numbers in Google Street View
images, representing a significantly harder real-world dataset compared to MNIST. We process the
SVHN dataset by removing the mean and normalizing the pixel values with the standard deviation
of all pixels in each image so that the normalized pixel values are within [-1, 1].
9
Under review as a conference paper at ICLR 2019
We train three different models with the CNN configuration summarized in Table 5 in the Appendix
and compare their performances under the scenario of white box attacks. All models are trained for
20 epochs with the following setup
•	FGSM-based adversarial training: {Batch size = 256, optimizer=AdamOptimizer with
learning rate 0.001, α = 0.6, ε = 24/255}
•	IFGSM-based adversarial training: {Batch size = 256, optimizer=AdamOptimizer with
learning rate 0.001, α = 0.6, ε = 24/255, attack steps=10}
•	e2SAD: {Batch size = 256, optimizer=AdamOptimizer with learning rate 0.001, α = 0.6,
λ = 0.3, ε1 = 24/255, ε2 = 8/255, label smoothing with correct class probability of
0.75}
The performances of the various models on this much harder SVHN dataset are summarized in Ta-
ble 3. It turns out that e2SAD outperforms all other models in this case. More specifically, the
baseline (natural) model shows no defense to any attack. e2SAD attains a significantly stronger ro-
bustness against the iterative IFGSM white-box attacks compared with the FGSM adversarial train-
ing, which shows no defense to such attacks. Furthermore, compared with the expensive IFGSM
adversarial training, e2SAD offers a much stronger defense against the one-step FGSM attacks.
This fact may be attributed to the particular two-step structure of e2SAD, which is geared towards
defending both one-step and multi-step adversarial attacks.
Table 3: Test accuracy of different models under white-box adversarial attacks evaluated using
SVHN. The e2SAD models are trained with soft labels while all other models are trained with
hard labels. The definitions of all variables are identical to those used in Table 1.
Attack		Natural	FGSM Adv. Train ε = 24∕255	IFGSM Adv. Train ε = 24∕255, k = 10	e2SAD
Clean data ∣∣	I	0.9006 I	0.9119	I	0.9001	I	0.9236
FGSM Il	ε = 24/255	∣	0.1962 I	0.7842	I	0.4289	I	0.7881
Il ε	=24∕255,k=10 ∣	0.0628 I	0.0455	I	0.3228	I	0.3328
IFGSM Il ε	= 24∕255,k=20 ∣	0.0602 I	0.0402	I	0.3165	I	0.4020
Il ε	= 24∕255,k=30 ∣	0.0593 I	0.0385	I	0.3146	I	0.3868
5 Conclusion
We have aimed to improve the robustness of deep neural networks by presenting an efficient two-
step adversarial defense technique e2SAD, particularly w.r.t to strong iterative multi-step attacks.
This objective is achieved by finding a combination of two adversarial points to best reveal the vul-
nerability of the model around each clean input. In particular, we have demonstrated that using a
dissimilarity measure between the first and second adversarial examples we are able to appropri-
ately locate the second adversary in a way such that including both types of adversaries in the final
training loss function leads to improved robustness against multi-step adversarial attacks. We have
demonstrated the effectiveness of e2SAD in terms of defense against while-box one-step FGSM and
multi-step IFGSM attacks and black-box IFGSM attacks under various settings.
e2SAD provides a general mechanism for defending both one-step and multiple attacks and for
balancing between these two defense needs, the latter of which can be achieved by properly tuning
the corresponding weight hyperparameters in the training loss function. In the future work, we
will explore hyperparameter tuning and other new techniques to provide a more balanced or further
improved defense quality for a wider range of white and black box attacks.
References
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. arXiv
preprint arXiv:1608.04644, 2016.
10
Under review as a conference paper at ICLR 2019
Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classifiers robustness to adversarial
perturbations. Machine Learning, 107(3):481-508, 2018.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Jihun Hamm. Machine vs machine: Minimax-optimal defense against adversarial examples. 2018.
RUitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvari. Learning with a strong adver-
sary. arXiv preprint arXiv:1511.03034, 2015.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Yann LeCun, Leon Bottou, YoShUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,
pp. 135-147. ACM, 2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2574-2582, 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard, and Stefano
Soatto. Analysis of universal adversarial perturbations. arXiv preprint arXiv:1705.09554, 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (Eu-
roS&P), 2016 IEEE European Symposium on, pp. 372-387. IEEE, 2016a.
Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of
security and privacy in machine learning. arXiv preprint arXiv:1611.03814, 2016b.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016c.
Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and in-
terpretability of deep neural networks by regularizing their input gradients. arXiv preprint
arXiv:1711.09404, 2017.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against
adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi. One pixel attack for fooling deep
neural networks. arXiv preprint arXiv:1710.08864, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. 2014.
Jingyi Wang, Jun Sun, Peixin Zhang, and Xinyu Wang. Detecting adversarial samples for deep
neural networks through mutation testing. arXiv preprint arXiv:1805.05010, 2018.
11
Under review as a conference paper at ICLR 2019
A Appendix
A. 1 CNN model configurations
Layer Type	Details
ReLU Convolutional	32 filters (5*5, stride 1, padding 2)
Max Pooling	2*2
ReLU Convolutional	64 filters (5*5, stride 1, padding 2)
Max Pooling	2*2
ReLU Fully Connect	1024 UnitS
Fully Connect	10 units
Softmax	10 units
Table 4: CNN model configuration for the MNIST dataset.
Layer Type	Details
ReLU Convolutional	32 filters (5*5, stride 1, padding 0)
ReLU Convolutional	64 filters (5*5, stride 1, padding 0)
Max Pooling	2*2
ReLU Convolutional	128 filters (3*3, stride 1, padding 0)
Max Pooling	2*2
ReLU Fully COnnect	512 units
Fully Connect	10 units
Softmax	10 units
Table 5: Model configuration for the SVHN dataset.
A.2 Visualization of e2SAD generated adversarial example
To demonstrate the two-step adversarial generation process of e2SAD, we consider a minibatch
of 128 clean images from the MNIST handwritten digits dataset (LeCun et al., 1998). We apply
e2SAD to find the first and second adversarial examples for each clean image xi in the batch. To
help visualize the loss surface of the model around this minibatch, which may be explored by IFGSM
attacks in a two-dimensional input space, we identify a search direction g1 = sign(xiadv,IF - xi),
where xiadv,IF is the adversary for xi found by IFGSM. We define a second search direction g2 to
be orthogonal to g1. Then around each xi, we generate a set of perturbed images along g1 and g2:
Xp = {xi + tι ∙ gι + t2 ∙ g2}, t1,t2 ∈ [0,0.4]. tι and t? are chosen to be the two lateral axes in
Figure 3. Here the loss is defined as the cross entropy loss based on hard target labels. The mesh loss
surface shows the loss of the model summed over the perturbed images for the entire minibatch as a
function oft1 andt2. The blue dot at (0, 0) location is the loss of the minibatch of clean images. The
red line starting from this blue point illustrates the two-step e2SAD adversarial searching direction.
The second and third blue points on the red line show the losses summed over the first and second
sets of adversarial examples, respectively, generated by e2SAD for this minibatch of clean images.
The locations of these two points are projected on the t1 and t2 coordinates for visualization. In
this case, at the second step e2SAD is able to identify an effective set adversarial examples with a
cost further increased from the first set, suggesting its effectiveness in defending both one-step and
multi-step adversarial attacks.
A.3 Visualization of the Loss surfaces of four different models
We visualize the loss surfaces of different models to shed light on the potential defense capabilities of
these models with respect to both one-step FGSM attacks and multi-step IFGSM attacks in Figure 4a
and Figure 4b, respectively. Here, the baseline model again is only trained with the clean data and
with no additional defense strategy; “FGSM Adv. Train” is the model is trained by adversarial
training with adversaries generated from FGSM (ε = 0.3); “IFGSM Adv. Train” is the model
trained by adversarial training with adversaries generated from IFGSM (ε = 0.3, k = 30); And
12
Under review as a conference paper at ICLR 2019
Figure 3: The two-step adversarial example generation process of e2SAD for a minibatch of 128
MNIST images.
e2SAD is the proposed approach with the setting (θ = 0.2, λ = 0.1, ε = 0.3, ε = 0.1). All models
are trained using a total number of 30,000 mini-batches of 256 images each over the MNIST dataset.
Figure 4a and Figure 4b illustrate the loss surface of each model in the input space, which may
be viewed by FGSM and IFGSM attacks, respectively, when they generate adversarial exam-
ples. To make visualizations possible in a reduced 2-dimensional input space, we take the ap-
proach adopted in Figure 3. For example, in the case of Figure 4a, we identify a search direction
adv,F GSM	adv,FGSM
g1 = sign(xi ,	- xi), where xi , is the adversary for each clean image xi found
by the FGSM attack. We define a second search direction g2 to be orthogonal to g1. Then around
each Xi, We generate a set of perturbed images along gι and g2： Xp = {x% + tι ∙ gι + t2 ∙ g2},
t1 , t2 ∈ [0, 0.4]. t1 and t2 are again chosen to be the two lateral axes in Figure 4a as in Figure 3.
The mesh loss surface of a model shoWs the loss summed over the perturbed images for the en-
tire MNIST dataset as a function of t1 and t2 . Again, the value at (0, 0) location is the loss of all
(MNIST) clean images. The same visualization approach is taken in Figure 4b With the difference
that the tWo search directions are defined by the adversary found by the IFGSM attack for each clean
image.
In both figures, it can be observed that the loss surface of the e2SAD model is the flattest one With the
loWest average value Within the large 2-dimensional adversarial searching space. This is consistent
With the empirically observed effectiveness of e2SAD’s defense against both FGSM and IFGSM
attacks.
(a)
Figure 4： Loss surfaces in a tWo-dimensional input space of different models： (a) surfaces seen by
the FGSM attacks, (b) surfaces seen by the IFGSM attacks.
(b)
13