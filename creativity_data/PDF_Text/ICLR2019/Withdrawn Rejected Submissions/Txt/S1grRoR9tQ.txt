Under review as a conference paper at ICLR 2019
Bayesian Deep Learning via Stochastic Gradi-
ent MCMC with a Stochastic Approximation
Adaptation
Anonymous authors
Paper under double-blind review
Ab stract
We propose a robust Bayesian deep learning algorithm to infer complex posteriors
with latent variables. Inspired by dropout, a popular tool for regularization and
model ensemble, we assign sparse priors to the weights in deep neural networks
(DNN) in order to achieve automatic “dropout” and avoid over-fitting. By alter-
natively sampling from posterior distribution through stochastic gradient Markov
Chain Monte Carlo (SG-MCMC) and optimizing latent variables via stochastic
approximation (SA), the trajectory of the target weights is proved to converge
to the true posterior distribution conditioned on optimal latent variables. This
ensures a stronger regularization on the over-fitted parameter space and more
accurate uncertainty quantification on the decisive variables. Simulations from
large-p-small-n regressions showcase the robustness of this method when applied to
models with latent variables. Additionally, its application on convolutional neural
networks (CNN) leads to state-of-the-art performance on MNIST and Fashion
MNIST datasets and improved resistance to adversarial attacks.
1	Introduction
Bayesian deep learning, which evolved from Bayesian neural networks (Neal, 1996; Denker and
leCun, 1990), provides an alternative to point estimation due to its close connection to both Bayesian
probability theory and cutting-edge deep learning models. It has been shown of the merit to quantify
uncertainty (Gal and Ghahramani, 2016b), which not only increases the predictive power of DNN,
but also further provides a more robust estimation to enhance AI safety. Particularly, Gal and Ghahra-
mani (2016a;b) described dropout (Srivastava et al., 2014) as a variational Bayesian approximation.
Through enabling dropout in the testing period, the randomly dropped neurons generate some amount
of uncertainty with almost no added cost. However, the dropout Bayesian approximation is variational
inference (VI) based thus it is vulnerable to underestimating uncertainty.
MCMC, known for its asymptotically accurate posterior inference, has not been fully investigated
in DNN due to its unscalability in dealing with big data and large models. Stochastic gradient
Langevin dynamics (SGLD) (Welling and Teh, 2011), the first SG-MCMC algorithm, tackled this
issue by adding noise to a standard stochastic gradient optimization, smoothing the transition between
optimization and sampling. Considering the pathological curvature that causes the SGLD methods
inefficient in DNN models, Li et al. (2016) proposed combining adaptive preconditioners with SGLD
(pSGLD) to adapt to the local geometry and obtained state-of-the-art performance on MNIST dataset.
To avoid SGLD’s random-walk behavior, Chen et al. (2014) proposed using stochastic gradient
Hamiltonian Monte Carlo (SGHMC), a second-order Langevin dynamics with a large friction term,
which was shown to have lower autocorrelation time and faster convergence (Chen et al., 2015).
Saatci and Wilson (2017) used SGHMC with GANs (Goodfellow et al., 2014a) to achieve a fully
probabilistic inference and showed the Bayesian GAN model with only 100 labeled images was able
to achieve 99.3% testing accuracy in MNIST dataset. Raginsky et al. (2017); Zhang et al. (2017);
Xu et al. (2018) provided theoretical interpretations of SGLD from the perspective of non-convex
optimization, echoing the empirical fact that SGLD works well in practice.
When the number of predictors exceeds the number of observations, applying the spike-and-slab
priors is particularly powerful and efficient to avoid over-fitting by assigning less probability mass on
1
Under review as a conference paper at ICLR 2019
the over-fitted parameter space (Song and Liang, 2017). However, the inclusion of latent variables
from the introduction of the spike-and-slab priors on Bayesian DNN is not handled appropriately
by the existing methods. It is useful to devise a class of SG-MCMC algorithms that can explore the
Bayesian posterior as well as optimize the hidden structures efficiently.
In this paper, we propose a robust Bayesian learning algorithm: SG-MCMC with a stochastic
approximation adaptation (SG-MCMC-SA) to infer complex posteriors with latent variables and
apply this method to supervised deep learning. This algorithm has four main contributions: 1. It
enables SG-MCMC method to efficiently sample from complex DNN posteriors with latent variables
and is proved to converge; 2. By automatically searching the over-fitted parameter space to add
more penalties, the proposed method quantifies the posterior distributions of the decisive variables
more accurately; 3. The proposed algorithm is robust to various hyperparameters; 4. This method
demonstrates more resistance to over-fitting in simulations, leads to state-of-the-art performance and
shows more robustness over the traditional SG-MCMC methods on the real data.
2	Stochastic Gradient MCMC
We denote the decaying learning rate at time k by (k), the entire data by D = {di}iN=1, where
di = (xi, yi),the log of posterior by L(β), V as the gradient of any function in terms of β. The mini-
batch of data B is of size n with indices S = {s1, s2, ..., sn}, where si ∈ {1, 2, ..., N}. Stochastic
gradient VL(β) from a mini-batch of data B randomly sampled from D is used to approximate the
true gradient VL(β):
N
VL(β) = V logP(β) + — EV logP(di∣β).
n i∈S
The stochastic gradient Langevin dynamics (no momentum) is formed as follows:
β(k+1) = β(k) + e(k)GVL(β(k)) + N(0, 2e(k)Gτ-1),	(1)
where τ > 0 denotes the temperature, Gis a positive definite matrix to precondition the dynamics. It
has been shown that SGLD asymptotically converges to a stationary distribution π(β∣D) 8 eτL(β)
(Teh et al., 2015; Zhang et al., 2017). As τ increases, the algorithm tends towards optimization with
underestimated uncertainty. Another variant of SG-MCMC, SGHMC (Chen et al., 2014; Ma et al.,
2015), proposes to use second-order Langevin dynamics to generate samples:
(β(k+1) = β(k) + Kk)MTr(k)
<	⑵
(r(k+I) = r(k) + Kk)VL(e(k)) — Kk)CMTr + N(0, e(k)(2C — Kk)B(k))τ-1)
where r is the momentum item, M is the mass, BB is an estimate of the error variance from the
stochastic gradient, C is a user-specified friction term to counteracts the noisy gradient.
3 Stochastic Gradient MCMC with a Stochastic Approximation
Adaptation
Dropout has been proven successful, as it alleviates over-fitting and provides an efficient way of
making bagging practical for ensembles of countless sub-networks. Dropout can be interpreted
as assigning the Gaussian mixture priors on the neurons (Gal and Ghahramani, 2016c). To mimic
Dropout in our Bayesian CNN models, we assign the spike-and-slab priors on the most fat-tailed
weights in FC1 (Fig. 1). From the Bayesian perspective, the proposed robust algorithm distin-
guishes itself from the dropout approach in treating the priors: our algorithm keeps updating
the priors during posterior inference, rather than fix it. The inclusion of scaled mixture priors in
deep learning models were also studied in Blundell et al. (2015); Li et al. (2016) with encouraging
results. However, to the best of our knowledge, none of the existing SG-MCMC methods could
deal with complex posterior with latent variables. Intuitively, the Bayesian formulation with model
averaging and the spike-and-slab priors is expected to obtain better predictive performance through
averages from a "selective" group of "good" submodels, rather than averaging exponentially many
posterior probabilities roughly. For the weight priors of the rest layers (dimension u), we just assume
they follow the standard Gaussian distribution, while the biases follow improper uniform priors.
2
Under review as a conference paper at ICLR 2019
(2∏σ2)-n/2 exp {-白
Similarly to the hierarchical prior in the EM approach to variable selection (EMVS) (Rorkova and
George, 2014), We assume the weights β ∈ Rp in FC1 follow the SPike-and-slab mixture prior
π(β lσ2, Y) = NP (0, Vσ, Y),	(3)
where Y ∈ {0,1}p, σ ∈ R, Vσ,γ = σ2 diag(a1,..., ap) with aj∙ = (1 - Yj)v0 + Yjv1 for each j and
0 < v0 < v1. By introducing the latent variable Yj = 0 or 1, the mixture prior is represented as
βj∣Yj ~ (1 - Yj)N(0,σ2V0) + YjN(0,σ2vι).	(4)
The interpretation is: if Yj = 0, then βj is close to 0; if Yj = 1, the effect of βj on the model is
intuitively large. The likelihood of this model given a mini-batch of data {(xi , yi)}i∈S is
pi∈s(y - ψ(Xi； e))2}	(regression)
π(B∣β,σ 2 )=∖	(5)
exp{ψyi (Xi； β)}	/ ] ∙q +∙、
∏ ——F-------------------- (classification),
、i∈S Pt=I exP{ψt(Xi； β)}
where ψ(Xi; β) can be a mapping for logistic regression or linear regression, or a mapping based on
a series of nonlinearities and affine transformations in the deep neural network. In the classification
formulation, yi ∈ {1, . . . , K} is the response value of the i-th example.
In addition, the variance σ2 follows an inverse gamma prior
∏(σ2∣γ) = IGWl2, νλ∕2).	(6)
The i.i.d. Bernoulli prior is used since there is no structural information in the same layer.
π(γ∣δ) = δlγ| (1 - δ)p-lγ|, where π(δ) X δa-1(1 - δ)b-1 and δ ∈ R.	(7)
Finally, our posterior density follows
π(β, σ2, δ, γ|B) x ∏(B∣β, σ2)nπ(β∣σ2, γ)∏(σ2∣γ)π(γ∣δ)π(δ).	(8)
The EMVS approach is efficient in identifying potential sparse high posterior probability submodels
on high-dimensional regression (Rorkova and George, 2014) and classification problem (McDermott
et al., 2016). These characteristics are helpful for large neural network computation, thus we refer the
stochastic version of the EMVS algorithm as Expectation Stochastic-Maximization (ESM).
3.2	Expectation Stochastic-Maximization
Due to the existence of latent variables, optimizing π(β, σ2, δ|B) directly is difficult. We instead
iteratively optimize the “complete-data” posterior logπ(β, σ2, δ, Y|B), where the latent indicator Y
is treated as “missing data”.
More precisely, the ESM algorithm is implemented by iteratively increasing the objective function
Q(β,σ,δ∣βIk,σ(k),δIk)= EY∣. [log∏(β,σ,δ,Y∣B)∣β(k),σ(k),δ(k),b] ,	(9)
3
Under review as a conference paper at ICLR 2019
where Eγ∣∙ denotes the conditional expectation Eγ∣β(k),σ(k),δ(k),B(∙). Given (β(k),σ(k),δ(k)) at the
k-th iteration, we first compute the expectation of Q, then alter (β, σ, δ) to optimize it.
For the conjugate spike-slab hierarchical prior formulation, the objective function Q is of the form
Q(β,σ,δ∣β(k),σ(k),δ(k)) = C + Qι(β,σ∣β(k),σ(k),δ(k)) + Q2(δ∣β(Rσ(RS(k)),
(10)
where
spike-and-slab priors in the "sparse" layer, e.g. FC1 	ʌ		
z log likelihood N~}∙	-	Pp=I β2Eγ∣∙ Qι(β,σ∣β(k),σ(k),δ(k)) = — log π(B∣β,σ2)	 n	{ -	1	- _vo(1 - Yj)+ ViYj一 2σ2
Pjp=+pu+1 βj2	p+ν+2
-----------------------------------
2	2
'------{z------}
Gaussian priors in other layers
IOg(M) - yλ2
2σ2
(11)
and
pδ
Q2(δ∣β(R σ(R δ(k)) =ElOg (口) Eγ∣∙Yj + (a - 1) log(δ) + (p + b - 1) log(1 - δ).
j=1	(12)
3.2.1	The E-step
The physical meaning of Eγ∣∙ Yj in Q2 is the probability Pj, where P ∈ Rp, of βj having a large effect
on the model. Formally, we have
Pj= Eγ∣∙Yj = P(Yj = 1∣β(k),σ⑻,δ(k)) = aj,	(13)
where aj- = ∏(βjk)∣γj- = 1)P(Yj = 1∣δ(k)) and bj = ∏(βjk)∣Yj = 0)P(Yj = 0∣δ(k)). The choice of
Bernoulli prior enables US to use P(Yj = 1∣δ(k)) = δ(k).
The other conditional expectation comes from a weighted average κj , where κ ∈ Rp.
Kj = Eγ∣∙
________1_________] = Eγ∣∙(I - Yj) + Eγ∣∙Yj = 1 - Pj + ρj
vo(1 - Yj) + vιYj∖	vo	vι	vo vι
(14)
3.2.2	The Stochastic M-step
Since there is no closed-form optimal solution for β here, to optimize Q1 with respect to β, we use
Adam (Kingma and Ba, 2014), a popular algorithm with adaptive learning rates, to train the model.
In order to optimize Q1 with respect to σ, by denoting diag{κi }ip=1 as V, following the formulation
in McDermott et al. (2016) and ROfkOVg and George (2014) we have:

σ(k+1) =
3 - ψ(xi;e(k+1)))2 + ||V 1/2e(k+1)||2 + νλ
N + P + V
(regression),
J(V 1∕2β(k+1w2+νλ
、∖ p + V + 2
(classification).
(15)
N J
To optimize Q2, a closed-form solution can be derived from Eq.(12) and Eq.(13).
δ(k+1) = arg max Q2 (δ∣β(k),σ(k),δ(k)) = Pj=1,Pj + a - 1
δ∈R	a + b + p - 2
(16)
4
Under review as a conference paper at ICLR 2019
Algorithm 1 SGLD-SA with spike and slab priors
Inputs： {e(k)}T=ι, {ω(k)}T=ι, τ, a, b, ν, λ, vo, vι
Outputs: {β(k)}kT=1
Initialize: β(1), ρ(1), κ(1), σ(1) and δ(1) from scratch
for k - 1 : T do
Sample mini-batch B(k)
Sampling
β(k+1) - β(k) + e(k)VQ(β,σ,δ∣β(k),σ(k),δ(k)) + N(0, 2e(k)τ-1)
Optimization
Compute ρ(k+1) inEq.(13), perform SA: ρ(k+1) . (1 — ω(k+1))ρ(k) + ω(k+1)ρ(k+1)
Compute κ(k+1) in Eq.(14), perform SA: κ(k+1) - (1 — ω(k+1))κ(k) + ω(k+1)κ(k+1)
Compute σ(k+1) in Eq.(15), perform SA: σ(k+1) J (1 — ω(k+1))σ(k) + ω(k+1)σ(k+1)
Compute δ(k+1) in Eq.(16), perform SA: δ(k+1) ― (1 — ω(k+1))δ(k) + ω(k+1)δ(k+1)
end for
3.3	SG-MCMC-SA
The EMVS algorithm is designed for linear regression models, although the idea can be extended to
nonlinear models. However, when extending to nonlinear models, such as DNNs, the M-step will
not have a closed-form update anymore. A trivial implementation of the M-step will likely cause a
local-trap problem. To tackle this issue, we replace the E-step and the M-step by SG-MCMC with
the prior hyperparameters tuned via stochastic approximation (Benveniste et al., 1990):
β(k+1) = β(k) + e(k)VL(β(k), θ(k)) + N(0, 2e(k)τ-1),
θ(k+1) = θ(k) + ω(k+1) gθ(k) (β(k+1)) — θ(k) ,
(17)
where gθ(k) (β(k+1)) — θ(k) is the gradient-like function in stochastic approximation (see details in
Appendix A.2), gθ(∙) is the mapping detailed in Eq.(13), Eq.(14), Eq.(15) and Eq.(16) to derive the
optimal θ based on the current β, the step size ω(k) can be set as A(k + Z)-α with α ∈ (0.5, 1]. The
interpretation of this algorithm is that We sample β(k+1) from L(β(k), θ(k)) and adaptively optimize
θ(k+1) from the mapping gθ(k) . We expect to obtain an augmented sequence as follows:
β(1), θ(1); β(2), θ(2); β(3), θ(3);...
(18)
We shoW the (local) L2 convergence rate of SGLD-SA beloW and present the details in Appendix B.
Theorem 1 (L2 convergence rate). For any α ∈ (0,1] and any compact subset Ω ∈ R2p+2, under
assumptions in Appendix B.1, the algorithm satisfies: there exists a constant λ such that
E [kθ(k) — θ*k2I(k ≤ t(Ω))] ≤ λω(k),
where t(Ω) = inf{k : θ(k) ∈ Ω} and ω(k) = A(k + Z)-α 〜O(k-α).
Corollary 1. Forany α ∈ (0,1] and any compact subset Ω ∈ R2p+2, under assumptions in Appendix
B.2, the distribution of β(k) in (18) converges weakly to the invariant distribution eL(β,θ*) as E → 0.
The key to guaranteeing the consistency of the latent variable estimators is from stochastic approx-
imation and the fact of Pi∞=1 ω(i) = ∞. The non-convex optimization of SGLD (Raginsky et al.,
2017; Xu et al., 2018), in particular the ability to escape shallow local optima (Zhang et al., 2017),
ensures the robust optimization of the latent variables. Furthermore, the mappings of Eq.(13), Eq.(14),
Eq.(15) and Eq.(16) all satisfy the assumptions on g in a compact subset Ω (Appendix B), which
enable us to apply theorem 1 to SGLD-SA. Because SGHMC proposed by Chen et al. (2014) is
essentially a second-order Langevin dynamics and yields a stationary distribution given an accurate
estimation of the error variance from the stochastic gradient, the property of SGLD-SA also applies
to SGHMC-SA.
Corollary 2. Forany α ∈ (0,1] and any compact subset Ω ∈ R2p+2, under assumptions in Appendix
B.2, the distribution of β(k) from SG-MCMC-SA converges weakly to the invariant distribution
π(β, ρ*, κ*, σ*, δ* |D) as E → 0.
5
Under review as a conference paper at ICLR 2019
Table 1: Predictive errors in linear regression based on a test set considering different v0 and σ
MAE / MSE	v0=10-3, σ=0.4	vo=10-3, σ=0.5	v0=10-2, σ=0.4	v0 =10-2 , σ=0.5
SGLD-SA	1.32 / 2.85	1.34 / 2.90	1.34 / 2.92	1.37 / 2.93
EMVS	1.43 / 3.19	3.04 / 13.6	3.40 / 18.8	1.33 / 2.95
SGLD	4.98 / 42.6	4.98 / 42.6	4.98 / 42.6	4.98 / 42.6
3.4	Posterior Approximation
The posterior average given decreasing learning rates can be approximated through the weighted
sample average E[ψ(β)] = PTPT)：(?)) (Welling and Teh, 2011) to avoid over-emphasizing the
tail end of the sequence and reduce=the variance of the estimator. Teh et al. (2015); Chen et al. (2015)
showed a theoretical optimal learning rate c(k)H k-1/3 for SGLD and k-1/5 for SGHMC to achieve
faster convergence for posterior average, which are used in Sec. 4.1 and Sec. 4.2 respectively.
4	Experiments
4.1	Simulation of Large-p-Small-n Regression
SGLD-SA can be applied to the (logistic) linear regression cases, as long as u = 0 in Eq.(11).
We conduct the linear regression experiments with a dataset containing n = 100 observations
and p = 1000 predictors. Np(0, Σ) is chosen to simulate the predictor values X (training set)
where Σ = (Σ)ip,j=1 with Σi,j = 0.6|i-j|. Response values y are generated from Xβ + η, where
β = (β1,β2,β3,0,0, ..., 0)0 andη 〜N,(0, 3In). To make the simulation in Rorkovd and George
(2014)more challenging, we assume βι ~ N(3, σ2), β2 ~ N(2, σ2), β3 ~ N(1, σ2), σc = 0.2.
We introduce some hyperparameters, but most of them are uninformative, e.g. ν ∈ {10-3, 1, 103}
makes little difference in the test set performance. Sensitivity analysis shows that three hyperparame-
ters are important: v0 , a and σ, which are used to identify and regularize the over-fitted space. We fix
τ = 1, λ = 1, ν = 1, v1 = 100, δ = 0.5, b = p and set a = 1. The learning rates for SGLD-SA and
SGLD are set to e(k) = 0.001 X k-3, and the step size ω(k) = 0.1 X (k + 1000)-4. We vary v° and
σ to show the robustness of SGLD-SA with respect to different initializations.
To implement SGLD-SA, we perform stochastic gradient optimization by randomly selecting 50
observations and calculating the corresponding gradient in each iteration. We simulate 500, 000
samples from the posterior distribution and at the same time keep optimizing the latent variables.
EMVS is implemented with β directly optimized each time. We also simulate a group of the test
set with 1000 observations (display 50 in Fig. 2(e)) in the same way as generating the training set
to evaluate the generalizability of our algorithm. Tab.1 shows that EMVS frequently fails given
bad initializations, while SGLD-SA is fairly robust to the hyperparameters. In addition, from
Fig. 2(a), Fig. 2(b) and Fig.2(c), we can see SGLD-SA is the only algorithm among the three that
quantifies the uncertainties of β1 , β2 and β3 reasonably and always gives more accurate posterior
average (Fig.2(f)); by contrast, the estimated response values y from SGLD is close to the true values
in the training set (Fig.2(d)), but are far away from them in the testing set (2(e)), indicating the
over-fitting problem of SGLD without proper regularization.
For the simulation of SGLD-SA in logistic regression to demonstrate the advantage of SGLD-SA
over SGLD and ESM, we leave the results in Appendix C.
4.2	Classification Problem
We implement all the algorithms in Pytorch (Paszke et al., 2017) and run the experiments on GeForce
GTX 1080 GPUs. ThefirstDNNweuseisa standard 2-Conv-2-FC CNN model (Fig.1) of 670K
parameters (see details in Appendix D.1). The first set of experiments is to compare methods on the
same model without using other complication, such as data augmentation (DA) or batch normalization
(BN) (Ioffe and Szegedy, 2015). We refer to the general CNN without dropout as Vanilla, with 50%
dropout rate applied to the green neurons (Fig.1) as Dropout. Vanilla and Dropout models are trained
6
Under review as a conference paper at ICLR 2019
(a) Posterior estimation of βι.	(b) Posterior estimation of β2.	(c) Posterior estimation of β3.
(d) Performance on training set.
• SGLD-SA
•	SGLD ,
•	EMVS
(
■	True value ∙
(f) Posterior means vs true values.
(e) Performance on test set.
Figure 2: Linear regression simulation when v0 = 10-3 and σ = 0.5.
with Adam (Kingma and Ba, 2014) with Pytorch default parameters (with learning rate 0.001). We
use SGHMC as a benchmark method as it is also sampling-based and has a close relationship with
the popular momentum based optimization approaches in DNN. SGHMC-SA differs from SGHMC
in that SGHMC-SA applies the spike-and-slab priors to the FC1 layer while SGHMC just uses the
standard normal priors. The hyperparameters v1 = 1,vo = 1 × 10-3 and σ = 0.1 in SGHMC-SA
are used to regularize the over-fitted space, and a, b are set to p to obtain a moderate "sparsity" to
resemble dropout, the step size is ω(k) = 0.1 × (k + 1000)-3. We use training batch size 1000 and
a thinning factor 500 to avoid a cumbersome system, and the posterior average is applied to each
Bayesian model. Temperatures are tuned to achieve better results (Appendix D.2).
The four CNN models are tested on the classical MNIST and the newly introduced Fashion MNIST
(FMNIST) (Xiao et al., 2017) dataset. Performance of these models is shown in Tab.2. Compared
with SGHMC, our SGHMC-SA outperforms SGHMC on both datasets. We notice the posterior
averages from SGHMC-SA and SGHMC obtain much better performance than Vanilla and Dropout.
Without using either DA or BN, SGHMC-SA achieves 99.60% which even outperforms some
state-of-the-art models, such as stochastic pooling (99.53%) (Zeiler and Fergus, 2013), Maxout
Network (99.55%) (Goodfellow et al., 2013) and pSGLD (99.55%) (Li et al., 2016). In F-MNIST,
SGHMC-SA obtains 93.01% accuracy, outperforming all other competing models.
To further test the maximum performance of SGHMC-SA, we apply DA and BN to the following
experiments (see details in Appendix D.3) and refer the datasets with DA as aMNIST and aFMNIST.
All the experiments are conducted using a 2-Conv-BN-3-FC CNN of 490K parameters. Using this
model, we obtain 99.75% on aMNIST (300 epochs) and 94.38% on aFMNIST (1000 epochs). The
results are noticeable because posterior model averaging is essentially conducted on a single Bayesian
neural network. We also conduct the experiments based on the ensemble of five networks and refer
them as aMNIST-5 and aFMNIST-5 in Tab. 2. We achieve 99.79% on aMNIST-5 using 5 small
Bayesian neural networks each with 2 thinning samples (4 thinning samples in aFMNIST-5), which
is comparable with the state-of-the-art performance (Wan et al., 2013).
4.3 Defenses against Adversarial Attacks
Continuing with the setup in Sec. 4.2, the third set of experiments focus on evaluating model
robustness. We expect less robust models perform considerably well on a certain dataset due to
7
Under review as a conference paper at ICLR 2019
Table 2: Classification accuracy on MNIST and Fashion MNIST using small networks
Dataset	MNIST	aMNIST	aMNIST-5	FMNIST	aFMNIST	aFMNIST-5
Vanilla	99.31	99.54	99.75	92.73	93.14	94.48
Dropout	99.38	99.56	99.74	92.81	93.35	94.53
SGHMC	99.55	99.71	99.77	92.93	94.29	94.64
SGHMC-SA	99.60	99.75	99.79	93.01	94.38	94.78
(a) ζ = ..., 0.3,  	(b) MNIST
Figure 3: Adversarial test accuracies based on adversarial images of different levels
over-tuning; however, as the degree of adversarial attacks increases, the performance decreases
sharply. In contrast, more robust models should be less affected by these adversarial attacks.
We apply the Fast Gradient Sign method (Goodfellow et al., 2014b) to generate the adversarial
examples with one single gradient step as in Papernot et al. (2016)’s study:
Xadv — X — Z ∙ sign{δχ maxlogP(y |x)},
y
where ζ ranges from 0.1, 0.2, . . . , 0.5 to control the different levels of adversarial attacks.
Similar to the setup in the adversarial experiments by Li and Gal (2017), we normalize the adversarial
images by clipping to the range [0, 1]. As shown in Fig. 3(b) and Fig.3(d), there is no significant
difference among all the four models in the early phase. As the degree of adversarial attacks arises,
the images become vaguer as shown in Fig.3(a) and Fig.3(c). In this scenario the performance of
Vanilla decreases rapidly, reflecting its poor defense against adversarial attacks, while Dropout
performs better than Vanilla. But Dropout is still significantly worse than the sampling based
methods SGHMC-SA and SGHMC. The advantage of SGHMC-SA over SGHMC becomes more
significant when ζ > 0.25.
In the case of ζ = 0.5 in MNIST where the images are hardly recognizable, both Vanilla and
Dropout models fail to identify the right images and their predictions are as worse as random guesses.
However, SGHMC-SA model achieves roughly 11% higher than these two models and 1% higher
than SGHMC, which demonstrates the strong robustness of our proposed SGHMC-SA. Overall,
SGHMC-SA always yields the most robust performance.
5 Conclusion and Future Research
We propose a mixed sampling-optimization method called SG-MCMC-SA to efficiently sample from
complex DNN posteriors with latent variables and prove its convergence. By adaptively searching and
penalizing the over-fitted parameter space, the proposed method improves the generalizability of deep
neural networks. This method is less affected by the hyperparameters, achieves higher prediction
accuracy over the traditional SG-MCMC methods in both simulated examples and real applications
and shows more robustness towards adversarial attacks.
Interesting future directions include applying SG-MCMC-SA towards popular large deep learning
models such as the residual network (He et al., 2015) on CIFAR-10 and CIFAR-100, combining active
learning and uncertainty quantification to learn from datasets of smaller size and proving posterior
consistency and the consistency of variable selection under various shrinkage priors concretely.
8
Under review as a conference paper at ICLR 2019
References
Albert Benveniste, Michael Metivier, and Pierre Priouret. Adaptive Algorithms and Stochastic
Approximations. Berlin: Springer, 1990.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. In Proc. of the International Conference on Machine Learning (ICML), pages
1613-1622, 2015.
Changyou Chen, Nan Ding, and Lawrence Carin. On the convergence of stochastic gradient MCMC
algorithms with high-order integrators. In Proc. of the Conference on Advances in Neural Informa-
tion Processing Systems (NIPS), pages 2278-2286, 2015.
Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In
Proc. of the International Conference on Machine Learning (ICML), 2014.
John S. Denker and Yann leCun. Transforming neural-net output levels to probability distributions.
In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS), pages
853-859, 1990.
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with Bernoulli approxi-
mate variational inference. In ICLR workshop, 2016a.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In Proc. of the International Conference on Machine Learning
(ICML), 2016b.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Proc. of the Conference on Advances in Neural Information Processing Systems
(NIPS), page 1019-1027, December 2016c.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. of the Conference on
Advances in Neural Information Processing Systems (NIPS), pages 2672-2680, 2014a.
Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. In Proc. of the International Conference on Machine Learning (ICML), pages III-1319-
III-1327, 2013.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. ArXiv e-prints, December 2014b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. ArXiv e-prints, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proc. of the International Conference on Machine Learning
(ICML), pages 448-456, 2015.
K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for
object recognition? In Proc. of the International Conference on Computer Vision (ICCV), pages
2146-2153, September 2009.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. of the
International Conference on Learning Representation (ICLR), 2014.
Chunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned stochastic
gradient Langevin dynamics for deep neural networks. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence, Proc. of the National Conference on Artificial Intelligence
(AAAI), pages 1788-1794, 2016.
Yingzhen Li and Yarin Gal. Dropout inference in Bayesian neural networks with alpha-divergences.
In Proc. of the International Conference on Machine Learning (ICML), 2017.
9
Under review as a conference paper at ICLR 2019
Faming Liang. Trajectory averaging for stochastic approximation MCMC algorithms. The Annals of
Statistics, 38:2823-2856, 2010.
Yi-An Ma, Tianqi Chen, and Emily B. Fox. A complete recipe for stochastic gradient mcmc. In NIPS
Autodiff Workshop, 2015.
Patrick McDermott, John Snyder, and Rebecca Willison. Methods for Bayesian variable selection
with binary response data using the em algorithm. ArXiv e-prints, May 2016.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, 1996.
Nicolas Papernot, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Fartash Faghri, Alexander
Matyasko, Karen Hambardzumyan, Yi-Lin Juang, Alexey Kurakin, Ryan Sheatsley, Abhibhav
Garg, and Yen-Chen Lin. cleverhans v2.0.0: an adversarial machine learning library. ArXiv
e-prints, October 2016.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS Autodiff Workshop, 2017.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic
gradient Langevin dynamics: a nonasymptotic analysis. ArXiv e-prints, June 2017.
Veronika Rorkovd and Edward I. George. Emvs: The EM approach to Bayesian variable selection.
Journal of the American Statistical Association, 109(506):828-846, 2014.
Yunus Saatci and Andrew G Wilson. Bayesian gan. In Proc. of the Conference on Advances in Neural
Information Processing Systems (NIPS), pages 3622-3631, 2017.
Issei Sato and Hiroshi Nakagawa. Approximation analysis of stochastic gradient Langevin dynamics
by using fokker-planck equation and ito process. In Proc. of the International Conference on
Machine Learning (ICML), pages 982-990, 2014.
Qifan Song and Faming Liang. Nearly optimal Bayesian shrinkage for high dimensional regression.
ArXiv e-prints, December 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15:1929-1958, 2014.
Yee Whye Teh, Alexandre Thiery, and Sebastian Vollmer. Consistency and fluctuations for stochastic
gradient Langevin dynamics. ArXiv e-prints, September 2015.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. Regularization of neural
networks using dropconnect. In Proc. of the International Conference on Machine Learning
(ICML), 2013.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proc. of the International Conference on Machine Learning (ICML), pages 681-688, 2011.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. ArXiv e-prints, August 2017.
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of Langevin dynamics
based algorithms for nonconvex optimization. ArXiv e-prints, February 2018.
Matthew Zeiler and Robert Fergus. Stochastic pooling for regularization of deep convolutional neural
networks. In Proc. of the International Conference on Learning Representation (ICLR), 2013.
Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient
Langevin dynamics. In Proc. of Conference on Learning Theory (COLT), pages 1980-2022, 2017.
10
Under review as a conference paper at ICLR 2019
A Review
A. 1 Fokker-Planck Equation
The Fokker-Planck equation (FPE) can be formulated from the time evolution of the conditional
distribution for a stationary random process. Denoting the probability density function of the random
process at time t by q(t, β), where β is the parameter, the stochastic dynamics is given by
∂tq(t, β) = ∂β(∂β(-L(β))q(t,β)) + ∂β2q(t,β).	(19)
Let q(β) = limt→∞ q(t, β). If limt→∞ ∂tq(t, β) = 0, then
lim ∂β(∂β(-L(β))q(t, β)) + ∂β2q(t,β) = 0
t→∞
⇒ ∂β (∂β (-L(β)q(β))) + ∂β2q(β) = 0
⇔ ∂β (∂β(-L(β))q(β) + ∂βq(β)) = 0
⇔ ∂β (q(β) (∂β(-L(β) + ∂β log q(β))) =0.
(20)
Therefore, ∂β(-L(β)) + ∂β log(q(β)) = 0, which implies q(β) H eL⑶.In other words,
limt→∞ q(t, β) H eL⑶,i.e. q(t, β) gradually converges to the Bayesian posterior eL(β).
A.2 Stochastic Approximation
A.2.1 Special Case： ROBBINS—Monro Algorithm
Robbins-Monro algorithm is the first stochastic approximation algorithm to deal with the root finding
problem. Given the random output of H(θ, β) with respect to β, our goal is to find θ* such that
h(θ*)= Eθ*[H(θ*, β)]=0,	(21)
where Eθ* denotes the expectation with respect to the distribution of β given parameter θ*. To
implement the Robbins-Monro Algorithm, we can generate iterates of the form as follows :
θ (k+1) = θ(k) + ω(k+1)H (θ(k), β(k+1)).
Note that in this algorithm, H(θ, β) is the unbiased estimator of h(θ), that is
Eθ(k)[H(θ(k),β(k+1)) - h(θ⑹)∣Fk]=0.	(22)
If there exists an antiderivative Q(θ, β) that satisfies H (θ, β) = dQ∂θ,β) and Eθ [Q(θ, β)] is concave,
it is equivalent to solving the stochastic optimization problem maxθ∈Θ Eθ [Q(θ, β)].
A.2.2 General Stochastic Approximation
In contrast to Eq.(21) and Eq.(22), the general stochastic approximation algorithm is intent on solving
the root of the integration equation
h(θ) = lim Eθ[H(θ,β (k+1))|Fk]
k→∞
= lim	H(θ, β)Πθ(β(k), dβ)
k→∞
(23)
=	H(θ, β)fθ(dβ) = 0,
where θ ∈ Θ, β ∈ B, for a subset B ∈ B, the transition kernel Πθ(β(k), B), which converges to
the invariant distribution fθ (β), satisfies that P[β (k+1) ∈ B|Fk] = Πθ(β (k), B). The stochastic
approximation algorithm is an iterative recursive algorithm consisting of two steps:
(1) Generate β(k+1) from the transition kernel ∏θ(k)(β(k), ∙), which admits fθ(k) (β) as the
invariant distribution,
11
Under review as a conference paper at ICLR 2019
(2)	Update θ(k+1) = θ(k) + ω(k+1)H(θ(k), β(k+1)).
Then we have E®(k)[H(θ(k), β(k+1))∣Fk] = R H(θ(k), β)∏θ(β(k), dβ). Compared with Eq.(22),
h(θ(k)) - Eθ(k) [H(θ(k), β(k+1))∣Fk] is not equal to 0 but decays to 0 as k → ∞.
To summarise, H(θ, β) is a biased estimator of h(θ) in finite steps, but as k → ∞, the
bias decreases to 0. In the SG-MCMC-SA algorithm (17), H(θ(k), β(k+1)) is defined by
gθ(k)(β(k+1))-θ(k).
B Convergence Analysis
B.1	Convergence of Hidden Variables
The stochastic gradient Langevin Dynamics with a stochastic approximation adaptation (SGLD-SA)
is a mixed half-optimization-half-sampling algorithm to handle complex Bayesian posterior with
latent variables, e.g. the conjugate spike-slab hierarchical prior formulation. Each iteration of the
algorithm consists of the following steps:
(1)	Sample β(k+1) using SGLD based on the current θ(k), i.e.
∂
β(k+1) = β(k) + e—L(β(k), θ(k)) + N(0, 2eτ-1);	(24)
∂β
(2)	Optimize θ(k+1) from the following recursion
θ(k+1) = θ(k) +ω(k+1) gθ(k)(β(k+1)) - θ(k)
= (1 - ω(k+1))θ(k) + ω(k+1)gθ(k) (β(k+1)),
where g®(k)(∙) is some mapping to derive the optimal θ based on the current β.
Remark: Define H(θ(k), β(k+1)) = gθ(k) (β(k+1)) - θ(k). In this formulation, our target is to find
θ* such that h(θ*) = 0, where h(θ) := R H(θ, β)fθ(dβ). E8(k)[H(θ(k), β(k+1))∣Fk] converges to
h(θ(k)) as k → ∞ , this algorithm falls to the category of the general stochastic approximation.
General Assumptions
To provide the local L2 upper bound for SGLD-SA, we first lay out the following assumptions:
Assumption 1 (Step size and Convexity). {ω(k)}k∈N is a positive decreasing sequence of real
numbers such that
∞
ω(k) → 0, X ω(k) = +∞.
k=1
There exist constant δ > 0 and θ* such thatfor all θ ∈ Θ
hθ - θ*,h(θ)i≤-δ∣∣θ - θ*∣∣2,
with additionally
ω(k)	ω(k+1) - ω(k)
lim inf 2δ n l 1, +------δ——> 0.
k→∞	ω(k+1)	ω(k+1)2
Then for any α ∈ (0, 1] and suitable A and B, a practical ω(k) can be set as
ω(k) =A(k+B)-α
(26)
(27)
(28)
(29)
Assumption 2 (Existence of Markov transition kernel). For any θ ∈ Θ, in the mini-batch sampling,
there exists a family of noisy Kolmogorov operators {Πθ } approximating the operator (infinitesimal)
of the Ito diffusion, such that every Kolmogorov operator Πθ corresponds to a single stationary
distribution fθ, and for any Borel subset A of β, we have
P[β(k+1) ∈ A|Fk] =Πθ(k)(β(k),A).
12
Under review as a conference paper at ICLR 2019
Assumption 3 (Compactness). For any compact subset Ω of Θ, we only consider θ ∈ Ω such that
kθk ≤ Co(Ω)	(30)
Note that the compactness assumption of the latent variable θ is not essential, the assumption that
the variable is in the compact domain is not only reasonable, but also simplifies our proof.
In addition, there exists constants Cι(Ω) and C2(Ω) so that
Eθ(k) [kH(θ(k), β(k+1))k2∣Fk] ≤ Cι(Ω)(1 + kθ(k)k2)
≤ C2(Ω)(I + kθ(k) - θ* + θ*k2)	(31)
≤ C2(Ω)(I + kθ(k) - θ*k2)
Assumption 4 (Solution of Poisson equation). For all θ ∈ Θ, there exists a function μθ on β that
solves the Poisson equation μθ(β) 一 ∏θμθ(β) = H(θ, β) — h(θ), whichfollows that
H (θ(k), β(k+1)) = h(θ(k)) + μθ(k) (β(k+1)) — ∏θ(k) μθ(k) (β(k+1)).	(32)
For any compact subset Ω of Θ, there exist constants C3(β, Ω) and C4(β, Ω) such that for all
θ, θ0 ∈ Ω,
⑴ k∏θμθk ≤ C3(Ω),
⑵ k∏θμθ — ∏Θ0μθ0k ≤ C4(Ω)kθ — θ0k,	()
Remark: For notation simplicity, We write C1(Ω) as Ci, C2(Ω) as C2,... in the following context.
Lemma 1 is a restatement of Lemma 25 (page 447) from Benveniste et al. (1990).
Lemma 1. Suppose k0 is an integer which satisfies with
ω(k+1) — ω(k)
inf  ———； ：—
k≥ko ω(k) ω(k+1)
+ 2δ — ω(k+1)C2 > 0.
Then for any k > k0, the sequence {ΛkK}k=k0 ,...,K defined below is increasing and bounded by
2ω(K)
f 2ω(k) QK=J(I- 2ωj + 1δ + ωj + iC2) if k < K,
ΛkK =	(34)
[ 2ω(K)	if k = K.
Lemma 2 is an extension of Lemma 23 (page 245) from Benveniste et al. (1990)
Lemma 2. There exist λ0 and k0 such that for all λ ≥ λ0 and k ≥ k0, the sequence u(k) = λω(k)
satisfies
u(k+i) ≥ (1 - 2ω(k+1)δ + ω(k+1)2C2)u(k) + ω(k+1)2C2 + ω(k+1)Ci.	(35)
Proof. Replace u(k) = λω(k) in (35), we have
λω(k+1) ≥ (1 - 2ω(k+1)δ + ω(k+1)2C2)λω(k) + ω(k+1)2C2 + ω(k+1)C 1.	(36)
From (28), we can denote a positive constant ∆+ as limk→∞ inf 2δω(k+1)ω(k) + ω(k+1) - ω(k).
Then (36) can be simplified as
λ(∆+ - ω(k+1)2ω(k)C2) ≥ ω(k+1)2C2 + ω(k+1)Ci.	(37)
There^xist λ0 and k° such that for all λ > λ0 and k > k0, (37) holds. Note that in practical case
when Ci is small, finding a suitable λo will not be a problem.	口
13
Under review as a conference paper at ICLR 2019
Theorem 1 (L2 convergence rate). Suppose that Assumptions 1-4 hold, for any compact subset
Ω ∈ Θ, the algorithm satisfies: there exists a constant λ such that
E hkθ(k) - θ*k2I(k ≤ t(Ω))i ≤ λω(k),
where t(Ω) = inf{k : θ(k) ∈ Ω}.
Proof. Denote T(k) = θ(k) - θ*, with the help of (25) and PoiSSon equation (32), we deduce that
kT(k+1)k2 = kT(k)k2 + ω(k+1)2kH(θ(k), β(k+1))k2 + 2ω(k+1)hT (k), H(θ(k), β(k+1))i
= kT (k)k2 + ω(k+1)2kH(θ(k), β(k+1))k2
+ 2ω(k+1)hT(k),h(θ(k))i +2ω(k+1)hT(k),μθ(k) (β(k+1)) - ∏θ(k)μθ(k) (β(k+1)))
= kT(k)k2 +D1+D2+D3.
First of all, according to (31) and (27), we have
ω(k+1)2kH(θ(k), β(k+1))k2 ≤ ω(k+1)2C2(1 + kT (k)k2),	(D1)
2ω(k+1)hT (k), h(θ(k))i ≤ -2ω(k+1)δkT (k)k2,	(D2)
Conduct the decomposition ofD3 similar to Theorem 24 (p.g. 246) from Benveniste et al. (1990) and
Lemma A.5 (Liang, 2010).
μ	θ(k) (β(k+1)) - ∏θ(k)μθ(k) (β(k+1))
=	(μθ(k) (β(k+1)) - ∏θ(k)μθ(k) (θ(k)))
+	(πθ(k) μθ(k) (θ(k)) - πθ(k-1) μθ(k-1) (e(k)))
+	(πe(k-i)μθ(k-i)(θ(k)) - πθ(k)Me3(e(k+1))),
= D3-1 + D3-2 + D3-3.
(i)	μe(k) (β(k+1)) - ∏θ(k) μe(k) (θ(k)) forms a martingale difference sequence since
E hμe(k) (β(k+1)) - ∏θ(k)μe(k) (θ(k))∣Fk] =0.	(D3-1)
(ii)	From (33) and (30) respectively, we deduce that
k∏θ(k)μe(k)(θ(k)) - ∏θ(k-i)μe(k-i) (θ(k))k ≤ C4kθ(k) - θ(k-1)k ≤ 2。4。0
Thus there exists C4 = 2C4C0 such that
2ω(k+1)hT(k), Πθ(k)μe(k) (θ(k)) - ∏e(k-i)μe(k-i) (θ(k))i ≤ ω(k+1)C4.	(D3-2)
(iii)	D3-3 can be further decomposed to D3-3a and D3-3b
hT(k), ∏θ(k-i)μe(k-i) (θ(k)) - ∏θ(k)μe(k) (β(k+1))i
=(hT(k), ∏θ(k-ι)μe(k-ι) (θ(k))i-hT(k+1), ∏θ(k)μe(k) (β(k+1))i)
+ (hT(k+1),∏θ(k)μe(k) (β(k+1))i -hTk,∏θ(k)μe(k) (β(k+1))i)
=(z(k) - z(k+1)) + hT(k+1)- T(k), Πθ(k)μθ(k) (β(k+1))i
= D3-3a + D3-3b,
14
Under review as a conference paper at ICLR 2019
where z(k) =〈T(k), ∏θ(k-i)μθ(k-i) (θ(k))i with a constant C3 = 4C0C3 which satisfies that
2ω(k+1)hT(k+1)- T(k), Πθ(k)μθ(k) (β(k+1))i
=2ω(k+1)hθ(k+1) — θ(k), Πθ(k)μθ(k) (β(k+1))i	(D3-3b)
≤ 2ω(k+1)kθ(k+1) - θ(k)k ∙ k∏θ(k)μθ(k) (β(k+1))k
≤ 4ω(k+I)COC3 = C3ω(k+1)
Finally, add all the items D1, D2 and D3 together, for some C 1 = C3 + C4, We have
E hkT (k+1)k2i ≤ (1 - 2ω(k+1)δ + ω(k+1)2C2)E kTkk2
+ ω(k+1)2C2 + ω(k+1)C 1 + 2ω(k+1)E[z(k) - z(k+1)].
Moreover, from (33) and (30), there exists a constant C5 such that
E[|z(k)|] ≤C5.	(38)
Lemma 3 is an extension of Lemma 26 (page 248) from Benveniste et al. (1990).
Lemma 3. Let {u(k)}k≥k0 as a sequence of real numbers such that for all k ≥ k0, some suitable
constants C 1 and C2
u(k+1) ≥ u(k)(1 — 2ω(k+1)δ + ω(k+1)2C2) + ω(k+1)2C2 + ω(k+1)C 1,	(39)
and assume there exists such k0 that
E hkT(k0) k2i ≤ u(k0).	(40)
Then for all k > k0, we have
k
E hkT(k) k2i ≤ u(k) + X Λjk(z(j-1) - z(j)).
j=k0+1
Proof of Theorem 1 (Continued). From Lemma 2, we can choose λ0 and k0 which satisfy the
conditions (39) and (40)
E[kT(k0) k2] ≤ u(k0) = λ0ω(k0).
From Lemma 3, it follows that for all k > k0
k
E kTkk2 ≤ u(k) +E X Λjk z(j-1) - z(j)) .	(41)
j=k0+1
From (38) and the increasing property of Λjk in Lemma 1, we have
k
E X Λjk z(j-1) - z(j))
j=k0+1
k-1
E X (Λjk+1 - Λjk)z(j) - 2ω(k)z(k) +Λkk +1z(k0)
j=k0+1
k-1
≤ X (Λk+1- Λj)C5 + E[∣2ω(k)z(k)∣]+ΛkC5
j=k0+1
≤ (Λkk -Λkk0)C5+ΛkkC5+ΛkkC5
≤ 3ΛkkC5 = 6C5ω(k) .
(42)
Therefore, given the sequence u(k) = λ0ω(k) that satisfies conditions (39), (40) and Lemma 3, for
any k > k0, from (41) and (42), we have
E[kT (k)k2] ≤ u(k) +3C5Λkk = (λ0 + 6C5) ω(k) = λω(k),
where λ = λo + 6C5.	□
15
Under review as a conference paper at ICLR 2019
B.2 Convergence of Samples
In addition to the previous assumptions, we make one more assumption on the stochastic gradients to
guarantee that the samples converge to the posterior conditioned on the optimal latent variables:
Assumption 5 (Gradient Unbiasedness and Smoothness). For all β ∈ B and θ ∈ Θ, the mini-batch
of data B, the stochastic noise ξ, Which comes from VL(β, θ) — VL(β, θ), is a white noise and
independent with each other. In addition, there exists a constant l ≥ 2 such that the following
conditions hold:
EB [ξ] = 0 and EB[ξl] < ∞.	(43)
For all θ, θ0 ∈ Θ, there exists a constant M > 0 such that the gradient is M-smooth:
VL(β, θ) — VL(β,θ0) ≤ Mkθ — θ0k	(44)
Corollary 1. For all α ∈ (0, 1], under assumptions 1-5, the distribution of β(k) converges weakly to
the invariant distribution eL(β,θ*) as E → 0.
Proof. The proof framework follows from section 4 of Sato and Nakagawa (2014). In the context of
stochastic noise ξ(k), we ignore the subscript of E and only consider the case of τ = 1. Since θ(k)
converges to θ* in SGLD-SA and the gradient is M-smooth (44), We transform the stochastic gradient
from VL(β(k), θ(k)) to VL(β(k), θ*) + ξ(k) + O(eα), therefore Eq.(24) can be written as
β(k+1) =	β(k)	+ E (VL(e(k), θ*) +	ξ(k)	+ O(eα))	+ √2Eη(k),	where	η(k)	〜N(0, I).	(45)
Using Eq.(43), the characteristic function of Eξ(k) + O(Eα+1) is
∞1	l
E 卜Xp θs(eξ(k) + O(eα+1))] = E X - θs(eξ(k) + O(eɑ+1))	= 1 + O(eα+1).	(46)
l=0
Then the characteristic function of √26η(k) is exp(—es2).
Rewrite β(k+1) as β(k+), the characteristic function of β(t+) is the characteristic function of
β⑹ + E (∂βL(β(k), θ*) + ξ(k) + O(Ea)) + √2Eη(k), which is
φt+(s)
/ exp(isβ + isEVL(β, θ*) — Es2)(1 + O(ea+1)) q(k, β)dβ
/ exp(isβ + isEVL(β, θ*) — Es2)q(k, β)dβ + O(Ea+1).
With the fact exp(x) = 1 + x + O(x2), we can get
φ(k+) (s) — φ(k) (s)
J exp(isβ) (exp(iscVL(β, θ*) — es2) — 1)q(k, β)dβ + O(Ea+1)
/ exp(isβ) (iseVL(β, θ*) — Es + O(e2)) q(k, β)dβ + O(ea+1)
(47)
(48)
/ exp(isβ) (isEVL(β, θ*) — Es2) q(k, β)dβ + O(Ea+1).
Therefore,
— is	exp(is
β)V(—L(β, θ*))q(k, β)dβ
+ (—is)2
exp(isβ)q(k, β)dβ + O(Eα).
(49)
16
Under review as a conference paper at ICLR 2019
Table 3: Predictive errors in logistic regression based on a test set considering different v0 and σ
MAE / MSE	v0 =10-2 , σ=0.6	v0=10-1, σ=0.6	v0 =10-2 , σ=2	v0 =10-3 , σ=2
SGLD-SA	0.182 / 0.0949	0.195 / 0.1039	0.146 / 0.1049	0.165 / 0.0890
SGLD	0.311 / 0.2786	0.304 / 0.2645	0.333 / 0.2977	0.331 / 0.3037
ESM	0.240 / 0.0982	0.454 / 0.2080	0.182 / 0.0882	0.172 / 0.1102
For any integrable function f , set F as the Fourier transform defined by
Fɪ=√2∏∕
f(x) exp(isx)dx.
(50)
The inverse Fourier transform of F[f (x)] and the l-th order derivatives off(l) (x) is
FTf(X)](S) = f (X) = √= / Ff(X)](S) eχp(-isχ)dχ,
2π	(51)
F(f(l))(S) = (-iS)l(F(f))(S).
Combine Eq.(49), Eq.(50) and Eq.(51), we arrive at the following simplified equation:
φ(t+e)(s) - φ(k)(s)
√2πe
=— isF∂β(-L(β, θ*))q(k, β) + (-is)2Fq(k, β) + O(eα)
=Fde (∂β(-L(β, θ*))q(k, β)) + Fdeq(k，β) + Od).
(52)
Since F-1[φ(k)(s)] = √2πq(k, β) and α ∈ (0,1],
lim q(k + e, β) - q(k, β)
e→0	C
1 Γφ(k+e)(s) - φ(k)(s)^
=l→oF	[	√π	J	(53)
=l→0 de (de (-L(β, θ*))q(k, β)) + dβ q(k, β) + O(eα)
=de(de(-L(β, θ*))q(k, β)) + dβq(k, β).
Finally, we have proved that the distribution of β(k) converges weakly to the invariant distribution
eL(e,θ*) as c → 0.	□
C S imulation of Logistic Regression
Now we conduct the experiments on binary logistic regression. The setup is similar as before, except
n is set to 500, ∑i,j = 0.3li-jl and η 〜N(0, I/2). We set the learning rates in SGLD-SA and SGLD
to 0.01 X k-1 and step size ω(k) to 0.1 X (k + 1000)-3. The binary response values are simulated
from Bernoulli(p) where p = 1/(1 + e-Xe-η). Fig.4(a), Fig.4(b) and Fig.4(c) demonstrate the
posterior distribution of SGLD-SA is significantly better than that of SGLD. As shown in Fig.4(f),
SGLD-SA is the best method to regulate the over-fitting space and provides the most reasonable
posterior mean. Table.3 illustrates the predictive power of SGLD-SA is overall better than the other
methods and robust to different initializations. Fig.4(d) and Fig.4(e) show that the over-fitting problem
of SGLD when p > n in logistic regression and the algorithm fails to regulate the over-fitting space;
We observe SGLD-SA is able to resist over-fitting and always yields reproducible results.
17
Under review as a conference paper at ICLR 2019
■ SGLD-SA
■ SGLD
(a) Posterior estimation of βι.
■ SGLD-SA
(b) Posterior estimation of β2.
♦ ♦♦ ♦ ♦••♦♦•••••••••••
・ SGLD-SA :	：♦	♦
♦ ♦ ♦ ♦
■	SGLD .
■	ESM .'
■	True value	.
■ SGLD-SA
■ SGLD
■ True value
(c) Posterior estimation of β3.
■ SGLD-SA-'	二
■	SGLD
■	ESM
4
■	True value
0	20	40	60	80	100
(d)	Performance on training set.
(e)	Performance on test set.
^
• SGLD-SA
• SGLD
0.0	0.5	1.0	1.5	2.0	2.5	3.0
(f) Posterior means vs true values.
Figure 4: Logistic regression simulation when v0 = 0.1 and σ = 0.6
D	Experimental S ETUP
D.1 Network Architecture
The first DNN we use is a standard 2-Conv-2-FC CNN: it has two convolutional layers with a2 ×
2 max pooling after each layer and two fully-connected layers. The filter size in the convolutional
layers is 5 × 5 and the feature maps are set to be 32 and 64, respectively (Jarrett et al., 2009). The
fully-connected layers (FC) have 200 hidden nodes and 10 outputs. We use the rectified linear unit
(ReLU) as activation function between layers and employ a cross-entropy loss.
The second DNN is a 2-Conv-BN-3-FC CNN: it has two convolutional layers with a 2 × 2 max
pooling after each layer and three fully-connected layers with batch normalization applied to the first
FC layer. The filter size in the convolutional layers is 4 × 4 and the feature maps are both set to 64.
We use 256 × 64 × 10 fully-connected layers.
D.2 Temperature Strategy
In practice, we observe a suitable temperature setting is helpful to improve the classification accuracy.
For example, by setting T = 100 in the second DNN (see Appendix D.1) we obtain 99.70% on
aMNIST. To account for the scale difference of weights in different layers, we apply different
temperatures to different layers based on different standard deviations of the gradients in each layer
and obtain the results in Tab. 2.
D.3 Data Augmentation
The MNIST dataset is augmented by (1) randomCrop: randomly crop each image with size 28
and padding 4, (2) random rotation: randomly rotate each image by a degree in [-15o, +15。], (3)
normalization: normalize each image with empirical mean 0.1307 and standard deviation 0.3081.
The FMNIST dataset is augmented by (1) randomCrop: same as MNIST, (2) randomHorizontalFlip:
randomly flip each image horizontally, (3) normalization: same as MNIST.
18