Under review as a conference paper at ICLR 2019
The Anisotropic Noise in Stochastic Gradient
Descent: Its Behavior of Escaping from Min-
ima and Regularization Effects
Anonymous authors
Paper under double-blind review
Ab stract
Understanding the behavior of stochastic gradient descent (SGD) in the context
of deep neural networks has raised lots of concerns recently. Along this line, we
theoretically study a general form of gradient based optimization dynamics with
unbiased noise, which unifies SGD and standard Langevin dynamics. Through in-
vestigating this general optimization dynamics, we analyze the behavior of SGD
on escaping from minima and its regularization effects. A novel indicator is de-
rived to characterize the efficiency of escaping from minima through measuring
the alignment of noise covariance and the curvature of loss function. Based on this
indicator, two conditions are established to show which type of noise structure is
superior to isotropic noise in term of escaping efficiency. We further show that
the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape
from sharp and poor minima effectively, towards more stable and flat minima
that typically generalize well. We verify our understanding through comparing
this anisotropic diffusion with full gradient descent plus isotropic diffusion (i.e.
Langevin dynamics) and other types of position-dependent noise.
1	Introduction
As a successful learning algorithm, stochastic gradient descent (SGD) was originally adopted for
dealing with the computational bottleneck of training neural networks with large-scale datasets (Bot-
tou, 1991). Its empirical efficiency and effectiveness have attracted lots of attention. And thus, SGD
and its variants have become standard workhorse for learning deep models. Besides the aspect of
empirical efficiency, recently, researchers started to analyze the optimization behaviors of SGD and
its impacts on generalization.
The optimization properties of SGD have been studied from various perspectives. The convergence
behaviors of SGD for simple one hidden layer neural networks were investigated in (Li & Yuan,
2017; Brutzkus et al., 2017). In non-convex settings, the characterization of how SGD escapes from
stationary points, including saddle points and local minima, was analyzed in (Daneshmand et al.,
2018; Jin et al., 2017; Hu et al., 2017).
On the other hand, in the context of deep learning, researchers realized that the noise introduced by
SGD impacts the generalization, thanks to the research on the phenomenon that training with a large
batch could cause a significant drop of test accuracy (Keskar et al., 2017). Particularly, several works
attempted to investigate how the magnitude of the noise influences the generalization during the
process of SGD optimization, including the batch size and learning rate (Hoffer et al., 2017; Goyal
et al., 2017; Chaudhari & Soatto, 2017; Jastrzebski et al., 2017). Another line of research interpreted
SGD from a Bayesian perspective. In (Mandt et al., 2017; Chaudhari & Soatto, 2017), SGD was
interpreted as performing variational inference, where certain entropic regularization involves to
prevent overfitting. And the work (Smith & Le, 2018) tried to provide an understanding based on
model evidence. These explanations are compatible with the flat/sharp minima argument (Hochreiter
& Schmidhuber, 1997; Keskar et al., 2017), since Bayesian inference tends to targeting the region
with large probability mass, corresponding to the flat minima.
However, when analyzing the optimization behavior and regularization effects of SGD, most of
existing works only assume the noise covariance of SGD is constant or upper bounded by some
1
Under review as a conference paper at ICLR 2019
constant, and what role the noise structure of stochastic gradient plays in optimization and general-
ization was rarely discussed in literature.
In this work, we theoretically study a general form of gradient-based optimization dynamics with
unbiased noise, which unifies SGD and standard Langevin dynamics. By investigating this general
dynamics, we analyze how the noise structure of SGD influences the escaping behavior from minima
and its regularization effects. Several novel theoretical results and empirical justifications are made.
1.	We derive a key indicator to characterize the efficiency of escaping from minima through
measuring the alignment of noise covariance and the curvature of loss function. Based
on this indicator, two conditions are established to show which type of noise structure is
superior to isotropic noise in term of escaping efficiency;
2.	We further justify that SGD in the context of deep neural networks satisfies these two con-
ditions, and thus provide a plausible explanation why SGD can escape from sharp minima
more efficiently, converging to flat minima with a higher probability. Moreover, these flat
minima typically generalize well according to various works (Hochreiter & Schmidhuber,
1997; Keskar et al., 2017; Neyshabur et al., 2017; Wu et al., 2017). We also show that
Langevin dynamics with well tuned isotropic noise cannot beat SGD, which further con-
firms the importance of noise structure of SGD;
3.	A large number of experiments are designed systematically to justify our understanding
on the behavior of the anisotropic diffusion of SGD. We compare SGD with full gra-
dient descent with different types of diffusion noise, including isotropic and position-
dependent/independent noise. All these comparisons demonstrate the effectiveness of
anisotropic diffusion for good generalization in training deep networks.
The remaining of the paper is organized as follows. In Section 2, we introduce the background
of SGD and a general form of optimization dynamics of interest. We then theoretically study the
behaviors of escaping from minima in Ornstein-Uhlenbeck process in Section 3, and establish two
conditions for characterizing the noise structure that affects the escaping efficiency. In Section 4,
we show that the noise of SGD in the context of deep learning meets the two conditions, and thus
explains its superior efficiency of escaping from sharp minima over other dynamics with isotropic
noise. Various experiments are conducted for verifying our understanding in Section 5, and we
conclude the paper in Section 6.
2	Background
In general, supervised learning usually involves an optimization process of minimizing an empirical
loss over training data, L(θ) := 1/N PiN=1 `(f (xi; θ), yi), where {(xi, yi)}iN=1 denotes the training
set with N i.i.d. samples, the prediction function f is often parameterized by θ ∈ RD, such as
deep neural networks. And '(∙, ∙) is the loss function, such as mean squared error and cross entropy,
typically corresponding to certain negative log likelihood. Due to the over parameterization and
non-convexity of the loss function in deep networks, there exist multiple global minima, exhibiting
diverse generalization performance. We call those solutions generalizing well good solutions or
minima, and vice versa.
Gradient descent and its stochastic variants A typical approach to minimize the loss function is
gradient descent (GD), the dynamics of which in each iteration t is, θt+1 = θt - ηtg0 (θt), where
go(θt) = VθL(θt) denotes the full gradient and η denotes the learning rate. In non-convex op-
timization, a more useful kind of gradient based optimizers act like GD with an unbiased noise,
including gradient Langevin dynamics (GLD), θt+ι = θt 一 ηtgo(θt) + σtj,j 〜 N (0, I), and
stochastic gradient descent (SGD), during each iteration t of which, a minibatch of training samples
with size m are randomly selected, with index set Bt ⊂ {1, 2, . . . , N}, and a stochastic gradient is
evaluated based on the chosen minibatch, g(θt) = Pi∈B= V'(f (xi； θt), yi)∕m, which is an unbi-
ased estimator of the full gradient g0(θt). Then, the parameters are updated with some learning rate
ηt as θt+ι = θt 一 ηtg(θt). Denote g(θ) = Vθ'((f (x; θ), y), the gradient for loss with a single data
point (x, y), and assume that the size of minibatch is large enough for the central limit theorem to
2
Under review as a conference paper at ICLR 2019
hold, and thus g(θt) follows a Gaussian distribution (Mandt et al., 2017; Li et al., 2017),
g(θt)〜N (go(θt), m1∑(θt)) , where ∑(θt) ≈ N X (g(θt;, Xi)- go(θt)) (g(θt; Xi)- go(θt))T ∙
i=1	(1)
Note that the covariance matrix Σ depends on the model architecture, dataset and the current param-
eter θt. Now we can rewrite the update of SGD as,
θt+ι = θt - ηtgo(θt) +—√= j, Et 〜N (0, ςGA ∙
(2)
Inspired by GLD and SGD, we may consider a general kind of optimization dynamics, namely,
gradient descent with unbiased noise,
θt+ι = θt - ηtgo(θt) + σtj, Et 〜N (O, ς/ ∙
(3)
For small enough constant learning rate ηt = η, the above iteration in Eq. (3) can be treated as the
numerical discretization of the following stochastic differential equation (Li et al., 2017; JaStrzebSki
et al., 2017; Chaudhari & Soatto, 2017),
dθt = -VθL(θt) dt + Jησ2∑t dWt∙	(4)
Considering y∕ησ2Σt as the coefficient of noise term, existing works (Hoffer et al., 2017; JaStrzebSki
et al., 2017)studied the influence of noise magnitude of SGD on generalization, i.e. ησt = η∕m.
In this work, we focus on studying the benefits of anisotropic structure of Σt in SGD helping escape
from minima by bridging the covariance matrix with the Hessian of the loss surface, and its implicit
regularization effects on generalization, especially in deep learning context. For the purpose of
eliminating the influence of the noise magnitude, we constrain it to be a constant when studying
different structures of noise covariance. The noise magnitude could be evaluated as the expectation
of the squared norm of the noise vector,
E[(√ησtEt)T(√ησt6t)] = ησt2E[eTe] = ησt TrE[eet] = ησt Tr∑t∙	(5)
Thus, we introduce the following constraint,
given time t, ησt2 Tr (Σt ) is constant∙	(6)
From the statistical physics point of view, Tr(ησt2Σt) characterizes the kinetic energy (Gardiner),
thus it is natural to force the energy to be unchanging, otherwise it is trivial that the higher the energy
is, the less stable the system is.
For simplicity, we absorb ησt2 into Σt, denoting ησt2Σt as Σt. If not pointed out, the subscript t of
matrix Σt is omitted to emphasize that we are fixing t and discussing the varying structure of Σ.
3	The behaviors of escaping from minima in Ornstein-Uhlenbeck
PROCESS
For a general loss function L(θ) = EX'χ (θ) (the expectation could be either population or em-
pirical), where X denotes data example and θ denoted parameters to be optimized, under suitable
smoothness assumptions, the SDE associated with the gradient variant optimizer as shown in Eq. (4)
can be written as follows (Li et al., 2017; Jastrzebski et al., 2017; Chaudhari & Soatto, 2017; Hu
et al., 2017), with little abuse of notation,
1
dθt = -VθL(θt)dt + ∑2 dWt∙	(7)
Let L0 = L(θ0) be one of the minimal values of L(θ), then for a fixed t small enough (such that
Lt - L0 ≥ 0), Eθt [Lt - L0] characterizes the efficiency ofθ escaping from the minimum θ0 of L(θ).
It is natural to measure the escaping efficiency using E[Lt - L0] since it characterizes the increase
of the potential, i.e., the increase of the loss L. And also note that Lt - L0 ≥ 0, for any δ > 0,
the escaping probability P(Lt - L0 ≥ δ) can be controlled by the expectation E[Lt - L0] since by
Markov,s inequality, we have P(Lt — Lo ≥ δ) ≤ ELt-L0].
3
Under review as a conference paper at ICLR 2019
Proposition 1 (Escaping efficiency for general process). For the process (7), provided mild smooth-
ness assumptions, the escaping efficiency from the minimum θ0 is,
E[Lt - Lo] = - [ E [VLT VLi + [gE Tr(Ht∑t) dt,
(8)
where Ht denotes the Hessian of L(θt) at θt.
We provide the proof in Appendix, and the same for the other propositions.
The escaping efficiency for general processes is hard to analyze due to the intractableness of the
integral in Eq. (8). However, we may consider the second-order approximation locally near the
minima θo, where L(θ) ≈ Lo + 1 (θ 一 θ0)TH(θ 一 θ0). Without losing generality, We suppose
θ0 = 0. Further, suppose that H is a positive definite matrix and the diffusion covariance Σt = Σ is
constant for t. Then the SDE (7) becomes an Ornstein-Uhlenbeck process,
dθt = -Hθt dt + Σ 1 dWt,	θo = 0.	(9)
Proposition 2 (Escaping efficiency of Ornstein-Uhlenbeck process). For Ornstein-Uhlenbeck pro-
cess (9), with t small enough, the escaping efficiency from minimum θ0 = 0 is,
E[Lt - L0]
≈ 2 Tr(H ∑).
(10)
Inspired by Proposition 1 and Proposition 2, we propose Tr (HΣ) as an empirical indicator mea-
suring the efficiency for a stochastic process escaping from minima. Now we turn to analysis which
kind of noise covariance structure Σ will benefit escaping sharp minima, under the constraint Eq. (6).
Firstly, for the isotropic loss surface, i.e., H = λI, the escaping efficiency is E[Lt 一 Lo] = λ2t Tr Σ,
which is invariant under the constraint that Tr Σ is constant (Eq. (6)). Thus it is only nontrivial to
study the impact of noise structure when the Hessian of loss surface is anisotropic.
Secondly, H and Σ being semi-positive definite, to achieve the maximum of Tr(H Σ) under con-
straint (6), Σ should be Σ* = (TrΣ) ∙ λιuιuτ, where λι,uι are the maximal eigenvalue and
corresponding unit eigenvector of H. Note that the rank-1 matrix Σ* is highly anisotropic. More
generally, the following Proposition 3 characterizes one kind of anisotropic noise significantly out-
performing isotropic noise in order of number of parameters D, given H is ill-conditioned.
Proposition 3 (The benefits of anisotropic noise). With semi-positive definite H and Σ, assume
(1) H is ill-conditioned. Let λ1 ≥ λ2 ≥ . . . , ≥ λD ≥ 0 be the eigenvalues of H in descent order,
and for some Constant k《D and d > 2,
λ1 > 0,	λk+1, λk+2, . . . , λD < λ1D-d,	(11)
(2) Σ is “aligned” with H. Let ui be the corresponding unit eigenvector of eigenvalue λi, for some
projection coefficient a > 0,
u1T Σu1 ≥ aλ1
TrΣ
Tr H,
(12)
then we have the benefit of the anisotropic noise over the isotropic one in term of escaping efficiency,
which can be characterized by the follow ratio,
Tr(H Σ)
Tr(H Σ)
O aD(2d-1) ,
(13)
where Σ = TD∑ I denotes the covariance ofisotropic noise, to meet the constraint Eq. (6).
To give some geometric intuitions on the left hand side of Eq. (12), let the maximal eigenvalue and
its corresponding unit eigenvector of Σ be γ1 , v1, then the right hand side has a lower bound as
u1T Σu1 ≥ u1Tv1γ1v1Tu1 = γ1 hu1, v1i2. Thus if the maximal eigenvalues of H and Σ are aligned in
proportion, γ1/ Tr Σ ≥ a1λ1/ Tr H, and the angle of their corresponding unit eigenvectors is close
to zero, hu1, v1i ≥ a2, the second condition Eq. (12) in Proposition 3 holds for a = a1a2.
4
Under review as a conference paper at ICLR 2019
Typically, in the scenario of modern deep neural networks, due to the over-parameterization, Hessian
and the gradient covariance are usually ill-conditioned and anistropic near minima, as shown by
(Sagun et al., 2017) and (Chaudhari & Soatto, 2017). Thus the first condition in Eq. (11) usually
holds for deep neural networks, and we further justify it by experiments in Section 5.3. Therefore,
in the following section, we turn to focus on how the gradient covariance, i.e. the covariance of SGD
noise meets the second condition of Proposition 3 in the context of deep neural networks.
4	The anisotropic noise of SGD in deep networks
In this section, we mainly investigate the anisotropic structure of gradient covariance in SGD, and
explore its connection with the Hessian of loss surface.
Around the true parameter According to the classic statistical theory (Pawitan, 2001, Chap. 8),
for population loss L(θ) = EX `(θ), with ` being the negative log likelihood, when evaluating at the
true parameter θ*, there is the exact equivalence between the Hessian H of the population loss and
Fisher information matrix F,
F(θ*) ：= EX[Vθ'(θ*)Vθ'(θ*)T] = EX[V2'(θ*)] = V2L(θ*) =: H(θ*).	(14)
In practice, with the assumptions that the sample size N is large enough (i.e. indicating asymptotic
behavior) and suitable smoothness conditions, when the current parameter θt is not far from the
ground truth, Fisher is close to Hessian. Thus we can obtain the following approximate equality
between gradient covariance and Hessian,
Σ(θt) = F(θt) - VθLT(θt)VθL(θt) ≈ F(θt) ≈ H(θt).
The first approximation is due to the dominance of noise over the mean of gradient in the later stage
of SGD optimization, which has been shown in (Shwartz-Ziv & Tishby, 2017). A similar experiment
as (Shwartz-Ziv & Tishby, 2017) has been conducted to demonstrate this observation, which is left
in Appendix due to the limit of space.
In the following, we theoretically characterize the closeness between Σ and H in the context of one
hidden layer neural networks; and show that the gradient covariance introduced by SGD indeed has
more benefits than isotropic one in term of escaping from minima, provided some assumptions.
One hidden layer neural network with fixed output layer parameters For binary classification
neural network with one hidden layer in classic setups (with softmax and cross-entropy loss), we
have following results to globally bound Fisher and Hessian with each other.
Proposition 4 (The relationship between Fisher and Hessian in one hidden layer neural network).
Consider the binary classification problem with data {(xi, yi)}i∈I, y ∈ {0, 1}, and typical (either
population or empirical) loss as L(θ) = E[φ ◦ f (x; θ)], where f denotes the output of neural
network, and φ denotes the cross-entropy loss with softmax,
ef(x)	1
φ(f(X)，y) = - ∣ylog 1 + ef(x)+(I - y)log1 + ^⑺,y ∈{0,1}.
If: (1) the neural network f is with one hidden layer and piece-wise linear activation. And the
parameters of output layer are fixed during training; (2) the optimization happens on a set U such
that, f(x; θ) ∈ (-C, C), ∀θ ∈ U, ∀x, i.e., the output of the classifier is bounded during optimization.
Then, we have the following relationship between (either population or empirical) Fisher F and
Hessian H almost everywhere:
e-C F (θ)	H(θ)	eCF(θ).
A B means that (B - A) is semi-positive definite.
There are a few remarks on Proposition 4. Firstly, as shown in (Brutzkus et al., 2017), the considered
neural networks in Proposition 4 are non-convex and have multiple minima, and thus it is still non-
trivial to consider the escaping from minima. Secondly, the Proposition 4 holds in both population
and empirical sense, since the proof does not distinguish the two circumstances. Thirdly, the bound
5
Under review as a conference paper at ICLR 2019
between F and H holds "globally" in the set U where the output f is bounded, rather than merely
around the true global minima as discussed previously.
By Proposition 4, the following relationship between gradient covariance and Hessian could be
derived.
Proposition 5 (The relationship between gradient covariance and Hessian in one hidden layer neural
network). Assume the conditions in Proposition 4 hold, then for some small δ > 0 and for θ close
enough to minima θ* (local or global),
UT Σu ≥ e-2(C+δ)λ TrH
holds for any positive eigenvalue λ and its corresponding unit eigenvector u of Hessian H.
(15)
As a direct corollary of Proposition 5, for such neural networks, the second condition Eq. (12) in
Proposition 3 holds in a very loose sense.
Therefore, based on the discussion on population loss around the true parameters and one hidden
layer neural network with fixed output layer parameters, given the ill-conditioning of H due to the
over-parameterization of modern deep networks, according to Proposition 3, we can conclude the
noise structure of SGD helps to escape from sharp minima much faster than the dynamics with
isotropic noise, and converge to flatter solutions with a high probability. These flat minima typically
generalize well (Hochreiter & Schmidhuber, 1997; Keskar et al., 2017; Neyshabur et al., 2017; Wu
et al., 2017). Thus, we attribute such properties of SGD on its better generalization performance
comparing to GD, GLD and other dynamics with isotropic noise (Hoffer et al., 2017; Goyal et al.,
2017; Keskar et al., 2017).
In the following, we conduct a series of experiments systematically to verify our understanding
on the behavior of escaping from minima and its regularization effects for different optimization
dynamics.
5 Experiments
To better understanding the behavior of anisotropic noise different from isotropic ones, we introduce
dynamics with different kinds of noise structure to empirical study with, as shown on Table 1.
Table 1: Compared dynamics defined in Eq. (3). For GLD dynamic, GLD diagonal, GLD Hessian and GLD
1st eigvec(H), σt are adjusted to make σtt share the same expected norm as that of SGD. For GLD leading,
σt is same as in SGD. Note that GLD 1st eigvec(H) achieves the best escaping efficiency as our indicator
suggested.
	Noise €t	Remarks
SGD	以〜N(0, ∑tgd厂	Σtgd is defined as in Eq.(1), and σt = -√m
GLD constant	et 〜N(0,I)	σt is a tunable constant
GLD dynamic	U 〜N (0,I)	σt is adjusted to make σ±ct share the same expected norm as that of SGD
GLD diagonal	Ct 〜N(0, diag(∑t))	The covariance diag(∑t) is the diagonal of the covari- ance of SGD noise.
GLD leading	Ct 〜N (0,Σt)	Σt = pk=ι YivivT. γi,vi are the first k leading eigen- values and corresponding eigenvalues of the covariance of SGD noise, respectively. (A low rank approximation of Σtgd)	
GLD Hessian	Ct 〜N(0, Ht)	Ht is a low rank approximation of the Hessian matrix of loss L(θ) by its the first k leading eigenvalues and corresponding eigenvalues.
GLD 1st eigven(H)	Ct 〜N(0, λ1U1 UT)	λ1 , u1 are the maximal eigenvalue and its correspond- ing unit eigenvector of the Hessian matrix of loss L(θt).
5.1 Two-dimensional toy example
We design a 2-D toy example L(w1, w2) with two basins, a small one and a large one, corresponding
to a sharp and flat minima, (1, 1) and (-1, -1), respectively, both of which are global minima.
6
Under review as a conference paper at ICLR 2019
Please refer to Appendix for the detailed constructions. We initialize the dynamics of interest with
the sharp minimum (w1 , w2) = (1, 1), and run them to study their behaviors escaping from this
sharp minimum.
To explicitly control the noise magnitude, we only conduct experiments on GD, GLD const, GLD
diag, GLD leading (with k = 2 = D in Table 1, or in other words, the exactly covariance of
SGD noise), GLD Hessian (k = 2) and GLD 1st eigven(H). And we adjust σt in each dynamics
to force their noise to share the same expected squared norm as defined in Eq. (6). Figure 1(a)
shows the trajectories of the dynamics escaping from the sharp minimum (1, 1) towards the flat
one (-1, -1), while Figure 1(b) presents the success rate of escaping for each dynamic during 100
repeated experiments.
As shown in Figure 1, GLD 1st eigvec(H) achieves the highest success rate, indicating the fastest
escaping speed from the sharp minimum. The dynamics with anisotropic noise aligned with Hessian
well, including GLD 1st eigvec(H), GLD Hessian and GLD leading, greatly outperform GD, GLD
const with isotropic noise, and GLD diag with noise poorly aligned with Hessian. These experiments
are consistent with our theoretical analysis on Ornstein-Uhlenbeck process shown Proposition 2
and 3, demonstrating the benefits of anisotropic noise for escaping from sharp minima.
(a)
Jl
GLD GLD GLD GLD GLD 1st
const dlag leading Hesslanelgven(H)
2.00
1.75-
1.50-
1.25-
1.00-
0.75-
0.50
0.25
0.00
----GLD const
——GLDdIag
----GLD leading
GLD Hesslan
— GLD 1st elgven(H)
0	50	100	150	2∞	250	300	350	400
iteration
(b)
(c)
Figure 1: 2-D toy example. Compared dynamics are defined in Table 1, k = 2, σt2 is tuned to keep noise of
all dynamics sharing same expected squared norm, 0.01. All dynamics are run by 500 iterations with learning
rate 0.005. (a) The trajectory of each compared dynamics for escaping from the sharp minimum in one run.
(b) Success rate of arriving the flat solution in 100 repeated runs. (c) Tr(HtΣt) of compared dynamics in one
run.
5.2	One hidden layer neural network with fixed output layer parameters
We empirically show that in one hidden layer neural net-
work with fixed output layer parameters, the anisotropic
noise induced by SGD indeed helps escape from sharp
minima more efficiently than isotropic noise. Three net-
works are trained to binary classify 1, 000 linearly separa-
ble two-dimensional points. The number of hidden nodes
for each network varies in {20, 200, 2000}. We plot the
empirical indicator Tr (HΣ) in Figure 2. We can eas-
ily observe that as the increase of the number of hidden
nodes, the ratio Tr(H∑) is enlarged significantly, which is
consistent with the Eq. (13) described in Proposition 3.
Figure 2: One hidden layer neural net-
works with fixed output layer parameters.
The solid and the dotted lines represent
the value of Tr(HΣ) and Tr(HΣ), respec-
tively. The number of hidden nodes varies
in {20, 200, 2000}, the results of which are
denoted in different colors.
5.3	Practical datasets
In this part, we conduct a series of experiments in real
deep learning scenarios to demonstrate the behavior of
SGD noise and its implicit regularization effects. We con-
struct a noisy training set based on FashionMNIST dataset1. Concretely, the training set consist of
1000 images with correct labels, and another 200 images with random labels. All the test data are
with clean labels. A small LeNet-like network is utilized such that the spectrum decomposition over
1https://github.com/zalandoresearch/fashion-mnist
7
Under review as a conference paper at ICLR 2019
gradient covariance matrix and Hessian matrix are computationally feasible. The network consists
of two convolutional layers and two fully-connected layers, with 11, 330 parameters in total.
We firstly run the standard gradient decent for 3000 iterations to arrive at the parameters θGD near
the global minima with near zero training loss and 100% training accuracy, which are typically sharp
minima that generalize poorly (Neyshabur et al., 2017). And then all other compared methods are
initialized with θGD and run for optimization with the same learning rate η = 0.07 and same batch
size m = 20 (if needed) for fair comparison2 *.
Verification of SGD noise satisfying the conditions in Proposition 3 To see whether the noise of
SGD in real deep learning circumstance satisfies the two conditions in Proposition 3, we run SGD
optimizer initialized from θGD, i.e. the sharp minima found by GD.
Figure 3(a) shows the first 400 eigenvalues of Hessian at θGd，from which We see that the
140th eigenvalue has already decayed to about 1% of the first eigenvalue. Note that Hessian
H ∈ Rd×d, D = 11330, thus H around θ"D approximately meets the ill-conditioning require-
ment in Proposition 3. Figure 3(b) shows the projection coefficient estimated by ^
UT Σuι Tr H
-λι TrΣ~
along the trajectory of SGD. The plot indicates that the projection coefficient is in a descent scale
comparing to D2d-1, thus satisfying the second condition in Proposition 3. Therefore, Proposition 3
ensures that SGD would escape from minima θGD faster than GLD in order of O(D2d-1), as shown
in Figure 3(c). An interesting observation is that in the later stage of SGD optimization, Tr(H Σ) be-
comes significantly (107 times) smaller than in the beginning stage, implying that SGD has already
converged to minima being almost impossible to escape from. This phenomenon demonstrates the
reasonability to employ Tr(H Σ) as an empirical indicator for escaping efficiency.
order Ofeigenvalues
(a)
(b)
(c)
Figure 3:	FashionMNIST experiments. (a) The first 400 eigenvalues of Hessian at θGD, the sharp minima
found by GD after 3000 iterations. (b) The projection coefficient estimation a = u1；：； H, as shown in
Proposition 3. (C)Tr(Ht∑t) versus Tr(Ht∑t) during SGD optimization initialized from θGd，Σt = τD∑t I
denotes the isotropic noise with same expected squared norm as SGD noise.
Behaviors of different dynamics escaping from minima and its generalization effects To com-
pare the different dynamics on escaping behaviors and generalization performance, we run dynamics
initialized from the sharp minima θGD found by GD. The settings for each compared method are as
follows. The hyperparameter σ2 for GLD const has already been tuned as optimal (σ = 0.001) by
grid search. For GLD leading, we set k = 20 for comprising the computational cost and approxi-
mation accuracy. As for GLD Hessian, to reduce the expensive evaluation of such a huge Hessian in
each iteration, we set k = 20 and update the Hessian every 10 iterations. We adjust σt in GLD dy-
namic, GLD Hessian and GLD 1st eigvec(H) to guarantee that they share the same expected squred
noise norm defined in Eq. (6) as that of SGD. And we measure the expected sharpness of different
minima as EV〜N(。炉1)[L(θ + ν)] - L(θ), as defined in ((Neyshabur et al., 2017), Eq.(7)). The
results are shown in Figure 4.
As shown in Figure 4, SGD, GLD 1st eigvec(H), GLD leading and GLD Hessian successfully
escape from the sharp minima found by GD, while GLD, GLD dynamic and GLD diag are trapped in
the minima. This demonstrates that the methods with anisotropic noise “aligned” with loss curvature
can help to find flatter minima that generalize well.
We also provide experiments on standard CIFAR-10 with VGG11 in Appendix.
2In fact, in our experiment, we test the equally spacing learning rates in the range [0.01, 0.1], and the final
results are consistent with each other.
8
Under review as a conference paper at ICLR 2019
(*) AUeJnUue c_£
o 28。 4ooα cααα ∞αα ιαααα 12ααα umo
iteration
"8∙∙8
(％) AUEnUUe≡9-
0	20M 4oαα cααα ∞βα IaMa 12ααα uoαα
iteration
SSSUdJe5P£3edxs
—GD
GLD const
—GLD dynamic
―■— GLD d∣ag
GLD leading
GLD Hesslan
τ- GLD 1st elgven(H)
—SG□
048、
O
sea。 wɪoɑ β∞α a∞α ιwαα 12ααα 14ααα ιβaaa
iteration
(a)	(b)	(c)
Figure 4:	FashionMNIST experiments. Compared dynamics are initialized at θGD found by GD, marked by
the vertical dashed line in iteration 3000. The learning rate is same for all the compared methods, ηt = 0.07,
and batch size m = 20. (a) Training accuracy versus iteration. (b) Test accuracy versus iteration. (c) Expected
sharpness versus iteration. Expected sharpness is measured as EV〜N(0户1)[L(θ + ν)] — L(θ), and δ = 0.01,
the expectation is computed by average on 1000 times sampling.
6 Conclusion
We theoretically investigate a general optimization dynamics with unbiased noise, which unifies var-
ious existing optimization methods, including SGD. We provide some novel results on the behaviors
of escaping from minima and its regularization effects. A novel indicator is derived for characteriz-
ing the escaping efficiency. Based on this indicator, two conditions are constructed for showing what
type of noise structure is superior to isotropic noise in term of escaping. We then analyze the noise
structure of SGD in deep learning and find that it indeed satisfies the two conditions, thus explaining
the widely know observation that SGD can escape from sharp minima efficiently toward flat minina
that generalize well. Various experimental evidence supports our arguments on the behavior of SGD
and its effects on generalization. Our study also shows that isotropic noise helps little for escaping
from sharp minima, due to the highly anisotropic nature of landscape. This indicates that it is not
sufficient to analyze SGD by treating it as an isotropic diffusion over landscape (Zhang et al., 2017;
Mou et al., 2017). A better understanding of this out-of-equilibrium behavior (Chaudhari & Soatto,
2017) is on demand.
References
Leon Bottou. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nimes, 91(8),
1991.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. arXiv preprint arXiv:1710.11029, 2017.
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with
stochastic gradients. arXiv preprint arXiv:1803.05999, 2018.
C Gardiner. Stochastic methods: a handbook for the natural and social sciences 4th ed.(2009).
Priya Goyal, Piotr Doll狂 Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1-42, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems 30, pp. 1729-1739. 2017.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of noncon-
vex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.
9
Under review as a conference paper at ICLR 2019
StanisIaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint
arXiv:1711.04623, 2017.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. HoW to escape
saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training
for deep learning: Generalization gap and sharp minima. In In International Conference on
Learning Representations (ICLR), 2017.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pp. 2101-2110, 2017.
Yuanzhi Li and Yang Yuan. Convergence analysis of tWo-layer neural netWorks With relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
Stephan Mandt, MattheW D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. arXiv preprint arXiv:1704.04289, 2017.
Wenlong Mou, LiWei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for non-
convex learning: TWo theoretical vieWpoints. arXiv preprint arXiv:1707.05947, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring generaliza-
tion in deep learning. In Advances in Neural Information Processing Systems 30, pp. 5949-5958.
2017.
Bernt 0ksendal. Stochastic differential equations. In Stochastic differential equations, pp. 65-84.
Springer, 2003.
Yudi PaWitan. In all likelihood: statistical modelling and inference using likelihood. Oxford Uni-
versity Press, 2001.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of
the hessian of over-parametrized neural netWorks. arXiv preprint arXiv:1706.04454, 2017.
Ravid ShWartz-Ziv and Naftali Tishby. Opening the black box of deep neural netWorks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gra-
dient descent. International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=BJij4yg0Z.
Lei Wu, Zhanxing Zhu, et al. ToWards understanding generalization of deep learning: Perspective
of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.
Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient
langevin dynamics. arXiv preprint arXiv:1702.05575, 2017.
A Proofs of propositions in main paper
A. 1 Proof of Proposition 1
Proof. The "mild smoothness assumptions" refers that Lt = L(θt) ∈ C2. Then the Ito’s lemma
holds (0ksendal, 2003).
And by Ito’s lemma, the SDE of Lt is
dLt = (-VLTVL + 1 Tr (∑lHt∑t1) ) dt + VLTΣ2 dWt
=(-VLTVL + J Tr (Ht∑t)) dt + VLTΣ2 dWt.
10
Under review as a conference paper at ICLR 2019
Taking expectation with respect to the distribution of θt ,
dELt
E (-VLTVL + J Tr(HtΣt)) dt,
(16)
for the expectation of Brownian motion is zero. Thus the solution of EYt is,
ELt
L0
-Z0tEVLTVL+Z0t
2 E Tr(HtΣt)dt.
□
A.2 Proof of Proposition 2
Proof. Without losing generality, we assume that L0 = 0.
For multivariate Ornstein-Uhlenbeck process, when θ0 = 0 is an constant, θt follows a multivariate
Gaussian distribution (0ksendal, 20θ3).
Consider change of variables θ → φ(θ, t) = eHtθt. Here, for symmetric matrix A,
eA := Udiag(eλ1,...,eλn)U,
where λ1, . . . , λn and U are the eigenvalues and eigenvector matrix of A. Note that with this nota-
tion,
deHt
dt
HeHt.
Applying Ito’s lemma, we have
dφ(θt,t) = eHtΣ1 dWt,
which we can integrate form 0 to t to get
θt =0+ t eH(s-t')Σ1 dWs
0
The expectation of θt is zero. And by Ito's isometry (0ksendal, 2003), the covariance of θt is,
EθtθtT = E
Z0 eH(ST) ∑1 dWs (∕t eH(Lt)∑2 dWr!
E
E
Z eH(ST)∑1 ∑1 eH(ST) ds
0
Zt eH(S-t)ΣeH(S-t) ds
0
Z eH(s-t)ΣeH(s-t) ds. (for H and Σ are both constant.)
0
11
Under review as a conference paper at ICLR 2019
Thus,
EL(θt) = 2E Tr (θTHθt)
=1 Tr (HEθtθT)
=11： Tr (HeH(ST)ΣeH(ST)) ~§
=2 J： Tr 卜H(I)HΣeH(ST)) ds
=2 1O Tr (e2H(s-t)H∑) ds
= Ilr ( 2 HT (IH)Hς)
= 1Tr ((I - e-2Ht) ς).
(for H is symmetric.)
The last approximation is by Taylor’s expansion.
□
A.3 Proof of Proposition 3
Proof. Firstly, Tr(HΣ) has the decomposition as Tr(HΣ) = PiD=1 λiuiTΣui.
Secondly, compute Tr(HΣ), Tr(HΣ) respectively,
and bound their quotient,
Tr(H Σ) ≥ u1T Σu1 ≥ aλ1
Tr(H Σ)
与Ir H
TrΣ
Tr H
Tr(H∑) ≥ aλιD ≥	aλιD	( D2d-i
Tr(H∑) ≥ (TrH)2 ≥ (kλ1 + (D - k)D-dλJ2 =1
The proof is finished.
(17)
□
A.4 Proof of Proposition 4
Proof. Firstly compute the gradients and Hessian of φ,
∂φ	ef
∂f = ιτ^f- y
∂2φ __	ef
df2 = (1 + ef )2 .
∕ι+e7 > 0
I-ι+ef < 0
y = 0,
y = 1.
And note the Gauss-Newton decomposition for functions with the form of L = φ ◦ f,
H e e.	∖ d'((x，y'y、e)
==心(χ,y)	∂θ2
=E(χ,y) f ∂ffT +E(χ,y)舞薇.
Since the output layer parameters for f is fixed and the activation functions are piece-wise linear,
f (x; θ) is a piece-wise linear function on its parameters θ. Therefore d∂f = 0, a.e., and H =
E	∂2φ∂f∂fT
E(χ,y) ∂f2 ∂θ ∂θ .
12
Under review as a conference paper at ICLR 2019
It is easy to check that e-C (df) ≤ ∂fφφ ≤ ec (∂φ) . Thus,
H — E .. ∂2φ∂f∂fT Y E C (∂φ)2 ∂f ∂fT __ E C (∂φ∂f)(∂φ∂f )T — eC F
==(x,y)y) ∂f2 ∂θ ∂θ Y (x,y,y)e Idfl ∂θ ∂θ =心(X,y)e ∖∂f ∂θ ) ∖∂f ∂θ ) = e r∙
H = E(χ,y)dfφ∂fdfΓ 占 E(χ,y)e-C (dφ) If 务=E(χ,y)e-C (dφdf) (dφdf) = e-CF∙
□
A.5 Proof of Proposition 5
Proof. For simplicity, We define g := ▽', go := VL = EV'.
The gradient covariance and Fisher has the following relationship,
F = Eg ∙ gT = E(go + E)(go + E)T = gogT + EEET = gogT + ς∙
Applying Taylor’s expansion to g0 (θ),
go (θ) = go(θ*) + H (θ*)(θ — θ*) + o(θ — θ*) = H (θ*)(θ — θ*) + o(θ — θ*).
Hence,
I∣go(θ)∣∣2 ≤ kHk2kθ -叫2 +。(kθ - θ*k2) = kHk2kθ - θ*k2 +。(kθ -叫2
Therefore, with the condition∣∣θ — θ*k2 ≤ √δHTFu, We have
I∣go(θ)∣∣2 ≤ δuTFu +。(∣δ∣).
Thus,
UTΣU UTFU - UTgogoTU UTFU -kgo k22 UTFU - kgo k22
τr∑ = TrF - Tr(gogT) ≥ TrF-∣∣gok2 ≥ ~TrF~
UTFU	kgok22	UTFU	UTFU -2δ
=TrF C-UTF2U) ≥ TrF (1-δ-。町2 TrFe，
for δ small enough.
On the other hand, Proposition 4 indicates that e-CF Y H Y eCF, Which means,
∀U, UT (eCF - H)U ≥ 0
and Tr(H - e-CF) ≥ 0.
Thlla UTFU -> UT(e CH)U
ThuS Tr F ≥ Tr(eC H) .
Therefore, for λ, U being a positive eigenvalue and the corresponding unit eigenvector of H, We
have
UTFU
Tr F
UTΣU
TrΣ
≥ e-2Cɪ
≥ Tr H
≥ UTFUe-2δ ≥ e-2(C+δ) λ
≥ TrF e ≥ e TrH-
□
B	Additional experiments
B.1 Dominance of noise over gradient
Figure 5 shows the comparison of gradient mean and the expected norm of noise during training
using SGD. The dataset and model are same as the experiments of FashionMNIST in main paper, or
as in Section C.2. From Figure 5, we see that in the later stage of SGD optimization, noise indeed
dominates gradient.
These experiments are implemented by TensorFlow 1.5.0.
13
Under review as a conference paper at ICLR 2019
Figure 5: L? norm of gradient mean, kVL(θt) k, and the expected norm of noise √ηtE[eTet]∕m dur-
ing the training using SGD. The dataset and model are same as the experiments of FashionMNIST
in main paper, or as in Section C.2
B.2 THE FIRST 50 ITERATIONS OF FASHIONMNIST EXPERIMENTS IN MAIN PAPER
Figure 6 shows the first 50 iterations of FashionMNIST experiments in main paper. We observe
that SGD, GLD 1st eigvec(H), GLD Hessian and GLD leading successfully escape from the sharp
minima found by GD, while GLD diag, GLD dynamic, GLD const and GD do not.
(求)>UE3uun ush
20	30
iteration
——GD
----GLD COΠSt
----GLD dynamic
——GLDdiag
----GL□ leading
GLD Hessian
——GLD 1st eigveπ(H)
SGD
-1 J IR Id
(求)>UE3uunaa
20	30
iteration
(a)	(b)
Figure 6:	The fisrt 50 iterations of FashionMNIST experiments in main paper. Compared dynamics are
initialized at θGD found by GD. The learning rate is same for all the compared methods, η = 0.07, and batch
size m = 20. (a) Training accuracy versus iteration. (b) Test accuracy versus iteration.
These experiments are implemented by TensorFlow 1.5.0.
B.3 Additional experiments on standard CIFAR- 1 0 and VGG11
Dataset Standard CIFAR-10 dataset without data augmentation.
Model Standard VGG11 network without any regularizations including dropout, batch normaliza-
tion, weight decay, etc. The total number of parameters of this network is 9, 750, 922.
Training details Learning rates ηt = 0.05 are fixed for all optimizers, which is tuned for the best
generalization performance of GD. The batch size of SGD is m = 100. The noise std of GLD
constant is σ = 10-3, which is tuned to best. Due to computational limitation, we only conduct
experiments on GD, GLD const, GLD dynamic, GLD diag and SGD.
Estimation of Sharpness The sharpness are estimated by
1M
M	L(θ + Vj) - L⑹，Vj ~N(0, δ2I),
with M = 100 and δ = 0.01.
Experiments Similar experiments are conducted as in main paper for CIFAR-10 and VGG11, as
shown in Figure 7. The observations and conclusions consist with main paper.
These experiments are implemented by PyTorch 0.3.0.
14
Under review as a conference paper at ICLR 2019
100
∞ ∞ æ ∞ TC
(求)us匕
0	2500 5000 7500 lβ∞0 12500 15000 17500
iteration
(a)
R=855
(求)ls£
	,ajΛA≠r∕A**VVrlZVYVrV-
	——GD 	GLD const 	GLD dynamic 一GLD diag ——SGD
O 2500 5000 7500 IOOOO 12500 15000 17500
iteration
(b)
0.0	.	.	.	.	.	.
O 2500	5000	7500	lβ∞O 12500	15000	17500
iteration
(c)
Figure 7:	CIFAR-10 experiments. Compared dynamics are initialized at ΘgD found by GD, marked by the
vertical dashed line in iteration 3000. The learning rate is same for all the compared methods, ηt = 0.05, and
batch size m = 100. (a) Training accuracy versus iteration. (b) Test accuracy versus iteration. (c) Expected
sharpness versus iteration. Expected sharpness is measured as EV〜N(o,δ2i) [L(θ + ν)] — L(θ), and δ = 0.01,
the expectation is computed by average on 100 times sampling. All observations consist with main paper.
C Detailed setups for experiments in main paper
C.1 Two-dimensional toy example
Loss Surface The loss surface L(w1, w2) is constructed by,
s1 = w1 - 1 - x1 ,
s2 = w2 - 1 - x2,
`(w1, w2; x1, x2) = min{10(s1 cosθ - s2 sin θ)2
+ 100(s1 cos θ + s2 sin θ)2, (w1 - x1 + 1)2
1N
L(W1,w2) = N £ '(WLw2； Xk, Xk),
+ (w2 - x2 + 1)2},
k=1
where
θ
N
1
4 π,
100,
xk
cos θ
〜N(O，刀，ς= -Sin θ
sin θ
cos θ
Note that Σ is the inverse of the Hessian of the quadric form generalizeing the sharp minima. And
the 3-dimensional plot of the loss surface is shown in Figure 8.
Hyperparameters All learning rates are equal to 0.005. All dynamics concerned are tuned to
share the same expected square norm, 0.01. The number of iteration during one run is 500.
These experiments are implemented by PyTorch 0.3.0.
C.2 FashionMNIST with corrupted labels
Dataset Our training set consists of 1200 examples randomly sampled from original FashionM-
NIST training set, and we further specify 200 of them with randomly wrong labels. The test set is
same as the original FashionMNIST test set.
Model Network architecture:
input ⇒ conv1 ⇒ max_pool ⇒ ReLU ⇒ conv2 ⇒ max_pool
⇒ ReLU ⇒ fc1 ⇒ ReLU ⇒ fc2 ⇒ output.
Both two convolutional layers use 5 × 5 kernels with 10 channels and no padding. The number of
hidden units between fully connected layers are 50. The total number of parameters of this network
are 11, 330.
15
Under review as a conference paper at ICLR 2019
Figure 8: Constructed 2-dimensional surface in main paper.
Training details
•	GD: Learning rate η = 0.1. We tuned the learning rate (in diffusion stage) in a wide range
of {0.5, 0.2, 0.15, 0.1, 0.09, 0.08, . . . , 0.01} and no improvement on generalization.
•	GLD constant: Learning rate η = 0.07, noise std σ = 10-3. We tuned the noise std in
range of {10-1, 10-2, 10-3, 10-4, 10-5} and no improvement on generalization.
•	GLD dynamic: Learning rate η = 0.07.
•	GLD diagnoal: Learning rate η = 0.07.
•	GLD leading: Learning rate η = 0.07, number of leading eigenvalues k = 20, batchsize
m = 20. We first randomly divide the training set into 60 mini batches containing 20
examples, and then use those minibatches to estimate covariance matrix.
•	GLD Hessian: Learning rate η = 0.07, number of leading eigenvalues = 20, update
frequence f = 10. Do to the limit of computational resources, we only update Hessian
matrix every 10 iterations. But add Hessian generated noise every iteration. And to the
same reason, We SimPlily set the coefficent of Hessian noise to ∙∖∕Tr H/m TrΣ, to avoid
extensively tuning of hyperparameter.
•	GLD 1st eigvec(H): Learning rate η = 0.07, as for GLD Hessian, and we set the coeffi-
cient of noise to ,λ]∕m TrΣ, where λι is the first eigenvalue of H.
•	SGD: Learning rate η = 0.07, batchsize m = 20.
Estimation of Sharpness The sharPness are estimated by
1M
M y^L(θ+Vj) - L(O),	Vj ~N(0 δI),
with M = 1, 000 and δ= 0.01.
These exPeriments are imPlemented by TensorFlow 1.5.0.
16