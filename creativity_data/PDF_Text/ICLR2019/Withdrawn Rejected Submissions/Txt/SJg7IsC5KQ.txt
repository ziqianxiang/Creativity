Under review as a conference paper at ICLR 2019
On the Convergence and Robustness of Batch
Normalization
Anonymous authors
Paper under double-blind review
Ab stract
Despite its empirical success, the theoretical underpinnings of the stability, con-
vergence and acceleration properties of batch normalization (BN) remain elusive.
In this paper, we attack this problem from a modeling approach, where we perform
a thorough theoretical analysis on BN applied to a simplified model: ordinary least
squares (OLS). We discover that gradient descent on OLS with BN has interest-
ing properties, including a scaling law, convergence for arbitrary learning rates for
the weights, acceleration effects, as well as insensitivity to the choice of learn-
ing rates. We then demonstrate numerically that these findings are not specific
to the OLS problem and hold qualitatively for more complex supervised learning
problems. This points to a new direction towards uncovering the mathematical
principles that underlies batch normalization.
1	Introduction
Batch normalization (Ioffe & Szegedy, 2015) (BN) is one of the most important techniques for
training deep neural networks and has proven extremely effective in avoiding gradient blowups dur-
ing back-propagation and speeding up convergence. In its original introduction (Ioffe & Szegedy,
2015), the desirable effects ofBN are attributed to the so-called “reduction of covariate shift”. How-
ever, it is unclear what this statement means in precise mathematical terms. To date, there lacks a
comprehensive theoretical analysis of the effect of batch normalization.
In this paper, we study the convergence and stability of gradient descent with batch normalization
(BNGD) via a modeling approach. More concretely, we consider a simplified supervised learning
problem: ordinary least squares regression, and analyze precisely the effect of BNGD when applied
to this problem. Much akin to the mathematical modeling of physical processes, the least-squares
problem serves as an idealized “model” of the effect of BN for general supervised learning tasks. A
key reason for this choice is that the dynamics ofGD without BN (hereafter called GD for simplicity)
in least-squares regression is completely understood, thus allowing us to isolate and contrast the
additional effects of batch normalization.
The modeling approach proceeds in the following steps. First, we derive precise mathematical re-
sults on the convergence and stability of BNGD applied to the least-squares problem. In particular,
we show that BNGD converges for any constant learning rate ε ∈ (0, 1], regardless of the condition-
ing of the regression problem. This is in stark contrast with GD, where the condition number of the
problem adversely affect stability and convergence. Many insights can be distilled from the analysis
of the OLS model. For instance, we may attribute the stability of BNGD to an interesting scaling law
governing ε and the initial condition; This scaling law is not present in GD. The preceding analysis
also implies that if we are allowed to use different learning rates for the BN rescaling variables (εa)
and the remaining trainable variables (ε), we may conclude that BNGD on our model converges for
any ε > 0 as long as εa ∈ (0, 1]. Furthermore, we discover an acceleration effect of BNGD and
moreover, there exist regions of ε such that the performance of BNGD is insensitive to changes in
ε, which help to explain the robustness of BNGD to the choice of learning rates. We reiterate that
contrary to many previous works, all the preceding statements are precise mathematical results that
we derive for our simplified model.
The last step in our modeling approach is also the most important: we need to demonstrate that these
insights are not specific features of our idealized model. Indeed, they should be true characteristics,
at least in an approximate sense, of BNGD for general supervised learning problems. We do this
1
Under review as a conference paper at ICLR 2019
by numerically investigating the convergence, stability and scaling behaviors of BNGD on various
datasets and model architectures. We find that the key insights derived from our idealized analysis
indeed correspond to practical scenarios.
1.1	Related work
Batch normalization was originally introduced in (Ioffe & Szegedy, 2015) and subsequently studied
in further detail in (Ioffe, 2017). Since its introduction, it has become an important practical tool
to improve stability and efficiency of training deep neural networks (He et al., 2016; Bottou et al.,
2018). Initial heuristic arguments attribute the desirable features of BN to concepts such as “co-
variate shift”, which lacks a concrete mathematical interpretation and alternative explanations have
been given (Santurkar et al., 2018). Recent theoretical studies ofBN includes (Ma & Klabjan, 2017),
where the authors proposed a variant of BN, the diminishing batch normalization (DBN) algorithm
and analyzed the convergence of the DBN algorithm, showing that it converges to a stationary point
of the loss function. More recently, (Bjorck et al., 2018) demonstrated that the higher learning rates
of batch normalization induce a regularizing effect. Another related work is (Kohler et al., 2018),
where the authors also considered the convergence properties of BNGD on linear networks (similar
to the least-squares problem), as well as other special problems, such as learning halfspaces and
extensions. In the OLS case, the authors showed that for a particularly adaptive choice of dynamic
learning rate schedule, which can be seen as a fixed effective step size in our terminology (see equa-
tion (11) and the discussion that immediately follows), BNGD converges linearly if λmax is known.
Moreover, the analysis also requires setting the rescaling parameter a every step to satisfy a sta-
tionarity condition, instead of simply performing gradient descent on a, as is done in the original
BNGD.
The present research differs from these previous analysis in an important way - we study the BNGD
algorithm itself, and not a special variant. More specifically, we consider constant learning rates
(without knowledge of properties of the OLS loss function) and we perform gradient descent on
rescaling parameters. We prove that the convergence occurs for even in this case (and in fact, for
arbitrarily large learning rates for ε, as long as 0 < εa ≤ 1). This poses more challenges in the
analysis and contrasts our work with previous analysis on modified versions of BNGD. This is an
important distinction; While a decaying or dynamic learning rate is sometimes used in practice, in
the case of BN it is critical to analyze the non-asymptotic, constant learning rate case, precisely
because one of the key practical advantages of BN is that a bigger learning rate can be used than that
in GD. Hence, it is desirable, as in the results presented in this work, to perform our analysis in this
regime.
Finally, through the lens of the least-squares example, BN can be viewed as a type of over-
parameterization, where additional parameters, which do not increase model expressivity, are in-
troduced to improve algorithm convergence and stability. In this sense, this is related in effect
to the recent analysis of the implicit acceleration effects of over-parameterization on gradient de-
scent (Arora et al., 2018).
1.2	Organization
Our paper is organized as follows. In Section 2, we outline the ordinary least squares (OLS) problem
and present GD and BNGD as alternative means to solve this problem. In Section 3, we demonstrate
and analyze the convergence of the BNGD for the OLS model, and in particular contrast the results
with the behavior of GD, which is completely known for this model. We also discuss the important
insights to BNGD that these results provide us with. We then validate these findings on more general
supervised learning problems in Section 4. Finally, we conclude in Section 5.
2	Background
Consider the simple linear regression model where x ∈ Rd is a random input column vector and y is
the corresponding output variable. Since batch normalization is applied for each feature separately,
in order to gain key insights it is sufficient to the case of y ∈ R. A noisy linear relationship is
assumed between the dependent variable y and the independent variables x, i.e. y = xTw + noise
2
Under review as a conference paper at ICLR 2019
where w ∈ Rd is the parameters. Denote the following moments:
H := E[xxT], g := E[xy], c := E[y2].	(1)
To simplify the analysis, we assume the covariance matrix H of x is positive definite and the mean
E[x] of x is zero. The eigenvalues of H are denoted as λi(H), i = 1, 2, ...d,. Particularly, the
maximum and minimum eigenvalue of H is denoted by λmax and λmin respectively. The condition
number of H is defined as K := λmax. Note that the positive definiteness of H allows Us to define
λmin
the vector norms k.kH and k.kH-1 by kxk2H = xTHx and kxk2H-1 = xTH-1x respectively.
2.1	Ordinary least squares
The ordinary least squares (OLS) method for estimating the unknown parameters w leads to the
following optimization problem
min Jo(w) := 2Eχ,y [(y — xτw)2] = C — gτW + 1WTHw.	(2)
w∈Rd
The gradient of J with respect to W is Nw Jo(w) = Hw - g, and the unique minimizer is W =
u := H-1g. The gradient descent (GD) method (with step size or learning rate ε) for solving the
optimization problem (2) is given by the iterating sequence,
wk+1 = wk - εNwJ0(wk) = (I - εH)wk + εg,	(3)
which converges if 0 < ε < λ2~ =： εmaχ, and the convergence rate is determined by the spectral
radius ρε := P(I - εH) = maxi{∣1 - ελi(H)|} with
ku - wk+1k ≤ ρεku - wkk.	(4)
It is well known (for example see Chapter 4 of (Saad, 2003)) that the optimal learning rate is εopt =
λ----2λ——,where the convergence estimate is related to the condition number K(H):
λmax+λ
min
Ilu - wk + 1k ≤ K++1 ku - wk Il ∙	(S)
2.2	Batch normalization
Batch normalization is a feature-wise normalization procedure typically applied to the output, which
in this case is simply z = xTw. The normalization transform is defined as follows:
NBN(Z) ：= √⅛ = xTw,	(6)
Var[z]	σ
where σ := WTHHw. After this rescaling, NBN(Z) will be order 1, and hence in order to reintro-
duce the scale (Ioffe & Szegedy, 2015), we multiply NBN (z) with a rescaling parameter a (Note
that the shift parameter can be set zero since E[wT x|w] = 0). Hence, we get the BN version of the
OLS problem (2):
min J(a,w) : = 2Eχ,y [(y — &Nbn(XTw))2] = C — wσga + 2a2∙	(7)
The objective function J(a, w) is no longer convex. In fact, it has trivial critical points,
{(a*,w*)∣a* = 0, w*τg = θ}, which are saddle points of J(a, w).
We are interested in the nontrivial critical points which satisfy the relations,
a* = Sign(S) VuT Hu, w = su, for some s ∈ R \ {0}.	(8)
It is easy to check that the nontrivial critical points are global minimizers, and the Hessian matrix at
each critical point is degenerate. Nevertheless, the saddle points are strict (Details can be found in
Appendix), which typically simplifies the analysis of gradient descent on non-convex objectives (Lee
et al., 2016; Panageas & Piliouras, 2017).
Consider the gradient descent method to solve the problem (7), which we hereafter call batch nor-
malization gradient descent (BNGD). We set the learning rates for a and w to be εa and ε respec-
tively. These may be different, for reasons which will become clear in the subsequent analysis. We
thus have the following discrete-time dynamical system
ak+1 = ak + εa (wkkg - ak) ,	(9)
wk+1 = wk + ε σak (g - Wk2g HWk) ∙	(10)
We now begin a concrete mathematical analysis of the above iteration sequence.
3
Under review as a conference paper at ICLR 2019
3	Mathematical analysis of BNGD on OLS
In this section, we discuss several mathematical results one can derive concretely for BNGD on
the OLS problem (7). First, we establish a simple but useful scaling property, which an important
ingredient in allowing us to prove a linear convergence result for arbitrary constant learning rates.
We also derive the asymptotic properties of the “effective” learning rate of BNGD (to be precisely
defined subsequently), which shows some interesting sensitivity behavior of BNGD on the chosen
learning rates. Detailed proofs of all results presented here can be found in the Appendix.
3.1	Scaling property
In this section, we discuss a straightforward, but useful scaling property that the BNGD iterations
possess. Note that the dynamical properties of the BNGD iteration are governed by a set of numbers,
or a configuration {H, u, a0 , w0 , εa , ε}.
Definition 3.1 (Equivalent configuration). Two configurations, {H, u, a0 , w0 , εa , ε} and
{H0 , u0 , a00 , w00 , ε0a , ε0}, are said to be equivalent if for iterates {wk }, {wk0 } following these con-
figurations respectively, there is an invertible linear transformation T and a nonzero constant t such
that wk0 = Twk , a0k = tak for all k.
The scaling property ensures that equivalent configurations must converge or diverge together, with
the same rate up to a constant multiple. Now, it is easy to check the system has the following scaling
law.
Proposition 3.2 (Scaling property). Suppose μ = 0,γ = 0,r = 0, QT Q = I,then
(1)	The configurations {μQTHQ, √μQu, γao, γQwo, εa,ε} and {H, u, ao, wo, εa, ε} are
equivalent.
(2)	The configurations {H, u, a0, w0 , εa , ε} and {H, u, a0 , rw0 , εa , r2ε} are equivalent.
It is worth noting that the scaling property (2) in Proposition 3.2 originates from the batch-
normalization procedure and is independent of the specific structure of the loss function. Hence,
it is valid for general problems where BN is used (Lemma A.9). Despite being a simple result, the
scaling property is important in determining the dynamics of BNGD, and is useful in our subsequent
analysis of its convergence and stability properties (see the sketch of the proof of Theorem 3.3).
3.2	Batch normalization converges for arbitrary step size
We have the following convergence result.
Theorem 3.3 (Convergence for BNGD). The iteration sequence (ak, wk) in equation (9)-(10) con-
verges to a stationary point for any initial value (a0, w0) and any ε > 0, as long as εa ∈ (0, 1].
Particularly, we have the following sufficient conditions of converging to global minimizers.
(1)	If a0w0Tg > 0 (or a0 = 0, w0Tg 6= 0), εa ∈ (0, 1] and ε is sufficiently small (the smallness
is quantified by Lemma A.13), then (ak, wk) converges to a global minimizer.
(2) If εa = 1 and ε > 0, then (ak, wk) converges to global minimizers for almost all initial
values (a0, w0).
Sketch of Proof.
We first prove that the algorithm converges for any εa ∈ (0, 1] and small enough ε, with any initial
value (a0, w0) such that kw0k ≥ 1 (Lemma A.13). Next, we observe that the sequence {kwkk} is
monotone increasing, and thus either converges to a finite limit or diverges. The scaling property is
then used to exclude the divergent case if {kwk k} diverges, then at some k the norm kwk k should
be large enough, and by the scaling property, it is equivalent to a case where kwkk=1 and ε is small,
which we have proved converges. This shows that kwk k converges to a finite limit, from which the
convergence of wk and the loss function value can be established, after some work. The proof is
fully presented in Theorem A.17 and preceding Lemmas.
4
Under review as a conference paper at ICLR 2019
In addition, using the ’strict saddle point’ arguments in (Lee et al., 2016; Panageas & Piliouras,
2017), we can prove the set of initial value for which (ak , wk) converges to saddle points has
Lebesgue measure 0, provided some conditions, such as when εa = 1, ε > 0 (Lemma A.20). It
is important to note that BNGD converges for all step size ε > 0 of wk, independent of the spectral
properties of H . This is a significant advantage and is in stark contrast with GD, where the step
size is limited by λmax(H), and the condition number of H intimately controls the stability and
convergence rate. Although we only prove the almost sure convergence to global minimizer for the
case of εa = 1, we have not encountered convergence to saddles in the OLS experiments even for
εa ∈ (0, 2) with initial values (a0, w0) drawn from typical distributions.
3.3	Convergence rate, acceleration and asymptotic sensitivity
Now, let us consider the convergence rate of BNGD when it converges to a minimizer. Compared
with GD, the update coefficient before Hwk in equation (10) changed from ε to a complicated term
which We named as the effective step size or learning rate εk
εk := ε
T
ak Wk g
σk	σk
and the recurrence relation in place of u - wk is
U - Wσ⅜gwk + 1 = (I - εkH) (U - Wkgwk).
σk	σk
(11)
(12)
Consider the dynamics of the residual ek := U - (wTg∕σ2 )wk, which equals 0 if and only if wk is
a global minimizer. Using the property of H -norm (see section A.1), we observe that the effective
learning rate εk determines the convergence rate of ek via
∣∣ek+1kΗ ≤ Hu - Wk2~wk+1 L ≤ P(I - εkH)kek l∣Η,
(13)
where P(I-εk H) is spectral radius of the matrix I-εkH. The inequality (13) shows that the conver-
gence of ek (and hence the loss function, see Lemma A.23) is linear provided ɛk ∈ (δ, 2∕λmaχ - δ)
for some positive number δ. In fact, if we enforce εk = 1∕λmaχ for each k, which is done in the
analysis in Kohler et al. (2018), then one immediately obtains the same linear convergence rate. But
this requires knowledge of λmax (problem-dependent) and a modification the BNGD algorithm. We
instead focus our analysis on the original BNGD algorithm.
Next, let us discuss below an acceleration effect of BNGD over GD. When (ak, wk) is close to a
minimizer, we can approximate the iteration (9)-(10) by a linearized system. The Hessian matrix for
BNGD at a minimizer (a*,w*) is diag(1, H *∕∣w* ∣2), where the matrix H * is
H * = H - HUuTH.	(14)
uT H u
The matrix H* is positive semi-definite (H*U = 0) and has better spectral properties than H, such
λt
as a lower pseudo-condition number κ* = λmax ≤ κ, where λ*naχ and λζ^n are the maximal
min
and minimal nonzero eigenvalues of H* respectively. Particularly, κ* < κ for almost all U (see
section A.1 ). This property leads to acceleration effects of BNGD: When ∣ek ∣H is small, the
contraction coefficient P in (13) can be improved to a lower coefficient. More precisely, we have the
following result:
Proposition 3.4. For any positive number δ ∈ (0, 1), if (ak, wk) is close to a minimizer, such that
λmaσεlakl ∣∣ek kH ≤ δ, then we have
∣ek+ι∣Η ≤ min{Py-I-H*)+δ,ρ(I - εkH川际∣H,	(15)
Where ρ* (I- εkH) = max{|1 - εkλ*rtin∖, |1 - εkλ*naχ |}.
Generally, we have ρ*(I - εkH*) ≤ P(I - εkH) provided εk > 0, and the optimal rate is
*
Popt ：= K*-1 ≤ K~+1 =： Popt, where the inequality is strict for almost all u. Hence, the esti-
mate (15) indicates that the optimal BNGD could have a faster convergence rate than the optimal
GD, especially when κ* is much smaller than κ.
Finally, we discuss the dependence of the effective learning rate εk (and by extension, the effective
convergence rate (13) or (15)) on ε. This is in essence a sensitivity analysis on the performance
of BNGD with respect to the choice of learning rate. The explicit dependence of ɛk on ε is quite
complex, but we can nevertheless give the following asymptotic estimates.
5
Under review as a conference paper at ICLR 2019
Proposition 3.5. Suppose εa ∈ (0, 1], a0w0Tg >
0, and ∣∣g∣∣2 ≥ 篝gTHw0,
then
(1)	When ε is small enough, ε 1, the effective step size has a same order with ε, i.e. there
are two positive constants, Ci, C2, independent on ε and k, such that Ci ≤ εk∕ε ≤ C2.
(2)	When ε is large enough, ε	1, the effective step size has order O(ε-1), i.e. there are two
positive constants, C1,C2, independent on ε and k, such that Ci ≤ ε^k ε ≤ C2.
Observe that for finite k, ε% is a differentiable function of ε. Therefore, the above result implies, via
the mean value theorem, the existence of some ε0 > 0 such that d^/dε∖ε=ε0 = 0. Consequently,
there is at least some small interval of the choice of learning rates ε where the performance of BNGD
is insensitive to this choice. In fact, empirically this is one commonly observed advantage of BNGD
over GD, where the former typically allows for a variety of (large) learning rates to be used without
adversely affecting performance. The same is not true for GD, where the convergence rate depends
sensitively on the choice of learning rate. We will see later in Section 4 that although we only have
a local insensitivity result above, the interval of this insensitivity is actually quite large in practice.
4	Experiments
Let us first summarize our key findings and insights from the analysis of BNGD on the OLS problem.
1.	A scaling law governs BNGD, where certain configurations can be deemed equivalent
2.	BNGD converges for any learning rate ε > 0, provided that εa ∈ (0, 1]. In particular,
different learning rates can be used for the BN variables (a) compared with the remaining
trainable variables (w)
3.	There exists intervals ofε for which the performance of BNGD is not sensitive to the choice
of ε
In the subsequent sections, we first validate numerically these claims on the OLS model, and then
show that these insights go beyond the simple OLS model we considered in the theoretical frame-
work. In fact, much of the uncovered properties are observed in general applications of BNGD in
deep learning.
4.1	Experiments on OLS
Here we test the convergence and stability of BNGD for the OLS model. Consider a diagonal matrix
H = diag(h) where h = (1, ..., κ) is a increasing sequence. The scaling property (Proposition 3.2)
allows us to set the initial value w0 having same 2-norm with u, kw0k = kuk = 1. Of course, one
can verify that the scaling property holds strictly in this case.
Figure 1 gives examples of H with different condition numbers κ. We tested the loss function of
BNGD, compared with the optimal GD (i.e. GD with the optimal step size εopt), in a large range
of step sizes εa and ε, and with different initial values of a0 . Another quantity we observe is the
effective step size ɛk of BN. The results are encoded by four different colors: whether ɛk is close
to the optimal step size εopt , and whether loss of BNGD is less than the optimal GD. The results
indicate that the optimal convergence rate of BNGD can be better than GD in some configurations.
This acceleration phenomenon is ascribed to the pseudo-condition number of H * (discard the only
zero eigenvalue) being less than κ(H). This advantage of BNGD is significant when the (pseudo)-
condition number discrepancy between H and H* is large. However, if this difference is small, the
acceleration is imperceptible. This is consistent with our analysis in section 3.3.
Another important observation is a region such that ε is close to ε°pt, in other words, BNGD signifi-
cantly extends the range of ‘optimal’ step sizes. Consequently, we can choose step sizes in BNGD at
greater liberty to obtain almost the same or better convergence rate than the optimal GD. However,
the size of this region is inversely dependent on the initial condition a0 . Hence, this suggests that
small a0 at first steps may improve robustness. On the other hand, small εa will weaken the perfor-
mance of BN. The phenomenon suggests that improper initialization of the BN parameters weakens
the power of BN. This experience is encountered in practice, such as (Cooijmans et al., 2016), where
higher initial values of BN parameter are detrimental to the optimization of RNN models.
6
Under review as a conference paper at ICLR 2019
a0 = 10
a0 = 1
a0 = 0.01
a0 = 0.0001
a0 = 1e - 8
yes
no
close to
optimal
step size
yes
no
< loss_GD
1OSS_BN<gOSS	^^^^^BN^^^^^	—loss_BN < loss_C
E!≡≡tf
≡l
□ □αaκ
”- -
£ J GD(Opt)
∖ 1叱腔、
`0¼--
BNGD

犷1,
Figure 1: Compare of BNGD and GD on OLS model. The results are encoded by four different
colors: whether εk is close to the optimal step size ε°pt of GD, characterized by the inequality
0.8εopt < εk < εopt/0.8, and whether loss OfBNGD is less than the optimal GD. Parameters: H =
diag(logspace(0,log10(κ),100)), u is randomly chosen uniformly from the unit sphere in R100, w0
is set to H u/kH uk. The GD and BNGD iterations are executed for k = 2000 steps with the same
w0. In each image, the range ofεa (x-axis) is 1.99 * logspace(-10,0,41), and the range ofε (y-axis)
is logspace(-5,16,43).
4.2	Experiments on practical deep learning problems
We conduct experiments on deep learning applied to standard classification datasets: MNIST (Le-
Cun et al., 1998), Fashion MNIST (Xiao et al., 2017) and CIFAR-10 (Krizhevsky & Hinton, 2009).
The goal is to explore if the key findings outlined at the beginning of this section continue to hold for
more general settings. For the MNIST and Fashion MNIST dataset, we use two different networks:
(1) a one-layer fully connected network (784 × 10) with softmax mean-square loss; (2) a four-
layer convolution network (Conv-MaxPool-Conv-MaxPool-FC-FC) with ReLU activation function
and cross-entropy loss. For the CIFAR-10 dataset, we use a five-layer convolution network (Conv-
MaxPool-Conv-MaxPool-FC-FC-FC). All the trainable parameters are randomly initialized by the
Glorot scheme (Glorot & Bengio, 2010) before training. For all three datasets, we use a minibatch
size of 100 for computing stochastic gradients. In the BNGD experiments, batch normalization is
performed on all layers, the BN parameters are initialized to transform the input to zero mean/unit
variance distributions, and a small regularization parameter e =1e-3 is added to variance √σ2 + e
to avoid division by zero.
Scaling property Theoretically, the scaling property 3.2 holds for any layer using BN. However,
it may be slightly biased by the regularization parameter . Here, we test the scaling property in
practical settings. Figure 2 gives the loss of network-(2) (2CNN+2FC) at epoch=1 with different
learning rate. The norm of all weights and biases are rescaled by a common factor η. We observe
that the scaling property remains true for relatively large η. However, when η is small, the norm
of weights are small. Therefore, the effect of the e-regUIariZatiOn in √σ2 + e becomes significant,
causing the curves to be shifted.
Stability for large learning rates We use the loss value at the end of the first epoch to characterize
the performance of BNGD and GD methods. Although the training of models have generally not
converged at this point, it is enough to extract some relative rate information. Figure 3 shows the loss
value of the networks on the three datasets. It is observed that GD and BNGD with identical learning
rates for weights and BN parameters exhibit a maximum allowed learning rate, beyond which the
iterations becomes unstable. On the other hand, BNGD with separate learning rates exhibits a much
larger range of stability over learning rate for non-BN parameters, consistent with our theoretical
results in Theorem 3.3.
Insensitivity of performance to learning rates Observe that BN accelerates convergence more sig-
nificantly for deep networks, whereas for one-layer networks, the best performance of BNGD and
7
Under review as a conference paper at ICLR 2019
Figure 2: Tests of scaling property of the 2CNN+2FC network on MNIST dataset. BN is performed
on all layers, and e=1e-3 is added to variance √σ2 + e. All the trainable parameters (except the BN
parameters) are randomly initialized by the Glorot scheme, and then multiplied by a same parameter
η.
GD are similar. Furthermore, in most cases, the range of optimal learning rates in BNGD is quite
large, which is in agreement with the OLS analysis (Proposition 3.5). This phenomenon is poten-
tially crucial for understanding the acceleration of BNGD in deep neural networks. Heuristically, the
“optimal” learning rates of GD in distinct layers (depending on some effective notion of “condition
number”) may be vastly different. Hence, GD with a shared learning rate across all layers may not
achieve the best convergence rates for all layers at the same time. In this case, it is plausible that the
acceleration of BNGD is a result of the decreased sensitivity of its convergence rate on the learning
rate parameter over a large range of its choice.
learning rate	learning rate
Figure 3: Performance of BNGD and GD method on MNIST (network-(1), 1FC), Fashion MNIST
(network-(2), 2CNN+2FC) and CIFAR-10 (2CNN+3FC) datasets. The performance is characterized
by the loss value at ephoch=1. In the BNGD method, both the shared learning rate schemes and
separated learning rate scheme (learning rate lr_a for BN parameters) are given. The values are
averaged over 5 independent runs.
5 Conclusion and Outlook
In this paper, we adopted a modeling approach to investigate the dynamical properties of batch
normalization. The OLS problem is chosen as a point of reference, because of its simplicity and
the availability of convergence results for gradient descent. Even in such a simple setting, we saw
that BNGD exhibits interesting non-trivial behavior, including scaling laws, robust convergence
properties, acceleration, as well as the insensitivity of performance to the choice of learning rates.
Although these results are derived only for the OLS model, we show via experiments that these are
qualitatively valid for general scenarios encountered in deep learning, and points to a concrete way
in uncovering the reasons behind the effectiveness of batch normalization.
Interesting future directions include the extension of the results for the OLS model to more general
settings of BNGD, where we believe the scaling law (Proposition 3.2) should play a significant role.
In addition, we have not touched upon another empirically observed advantage of batch normaliza-
tion, which is better generalization errors. It will be interesting to see how far the current approach
takes us in investigating such probabilistic aspects of BNGD.
8
Under review as a conference paper at ICLR 2019
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research,pp. 244-253, Stockholmsmssan, Stockholm Sweden, 10-15 JUl 2018. PMLR.
J. Bjorck, C. Gomes, and B. Selman. Understanding Batch Normalization. ArXiv e-prints, May
2018.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Tim Cooijmans, Nicolas Ballas, Cesar Laurent, and Aaron C. Courville. Recurrent batch normal-
ization. CoRR, abs/1603.09025, 2016. URL http://arxiv.org/abs/1603.09025.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized
models. CoRR, abs/1702.03275, 2017. URL http://arxiv.org/abs/1702.03275.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
J. Kohler, H. Daneshmand, A. Lucchi, M. Zhou, K. Neymeyr, and T. Hofmann. Towards a Theoret-
ical Understanding of Batch Normalization. ArXiv e-prints, May 2018.
Steven G Krantz and Harold R Parks. A primer of real analytic functions. Springer Science &
Business Media, 2002.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient Descent Converges to Minimizers.
ArXiv e-prints, February 2016.
Yintai Ma and Diego Klabjan. Convergence analysis of batch normalization for deep neural nets.
CoRR, 1705.08011, 2017. URL http://arxiv.org/abs/1705.08011.
Ioannis Panageas and Georgios Piliouras. Gradient Descent Only Converges to Minimizers: Non-
Isolated Critical Points and Invariant Regions. In Christos H. Papadimitriou (ed.), 8th Innovations
in Theoretical Computer Science Conference (ITCS 2017), volume 67 of Leibniz International
Proceedings in Informatics (LIPIcs), pp. 2:1-2:12, Dagstuhl, Germany, 2017. Schloss Dagstuhl-
Leibniz-Zentrum fuer Informatik. ISBN 978-3-95977-029-3. doi: 10.4230/LIPIcs.ITCS.2017.2.
S. P. Ponomarev. Submersions and preimages of sets of measure zero. Siberian Mathematical
Journal, 28(1):153-163, Jan 1987. ISSN 1573-9260. doi: 10.1007/BF00970225. URL https:
//doi.org/10.1007/BF00970225.
Yousef Saad. Iterative methods for sparse linear systems, volume 82. siam, 2003.
S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry. How Does Batch Normalization Help Optimiza-
tion? (No, It Is Not About Internal Covariate Shift). ArXiv e-prints, May 2018.
Michael Shub. Global stability of dynamical systems. Springer Science & Business Media, 2013.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
9
Under review as a conference paper at ICLR 2019
A Proof of Theorems
A. 1 Gradients and Hessian matrix
(16)
The objective function in problem (7) has an equivalent form:
J(a, w) = 2(U — aW)TH(U — aw) = 2∣∣u∣∣H — wσga + 1 a2,
where u = H-1g.
The gradients are:
j = - 1 (WT Hu - a wT HW) = _ 1 wT g + a,
-a g + 亲 (wT g)Hw∙
where
A22 = σ(WTg)hH + WTg ((HW)gT + g(Hw)T) - σ32(HW)(HW)Ti,
A21 = -1 (g - σ⅛ (WTg)Hw).
(17)
(18)
(19)
(20)
(21)
The objective function J(a,w) has trivial critical points, {(a*,w*)∣a* = 0,w*tg = 0}. It is
obvious that a* is the minimizer of J(a, w*), but (a*,w*) is not a local minimizer of J(a, w) unless
g = 0, hence (a*,w*) are saddle points of J(a, w). The Hessian matrix at those saddle points has
at least a negative eigenvalue, i.e. the saddle points are strict. In fact, the eigenvalues at the saddle
point (a*,w*) are {2(1 ± ,1 + 4Wk⅛WZ), 0,…,0}
and a negative eigenvalue.
which contains d- 2 repeated zero, a positive
On the other hand, the nontrivial critical points satisfies the relations,
a* = ±√UTHu, w* 〃u,
(22)
where the sign of a* depends on the direction of U, w*, i.e. sign(a*) = sign(UTw*). It is easy to
check that the nontrivial critical points are global minimizers. The Hessian matrix at those minimiz-
ers is diag(1, H*/kw* k2) where the matrix H* is
H * = H - HUUTH	(23)
uT H u
which is positive semi-definite and has a zero eigenvalue corresponding to the eigenvector U,
i.e. H*U = 0. The following lemma, similar to the well known Cauchy interlacing theorem, gives
an estimate of eigenvalues of H*.
Lemma A.1. If H is positive definite and H * is defined as H * = H 一 HTHH ,then the eigenvalues
of H and H* satisfy the following inequalities:
0=λ1(H*)	<λ1(H)	≤	λ2(H*)	≤λ2(H)	≤ ... ≤λd(H*) ≤λd(H).	(24)
Here λi (H) means the i-th smallest eigenvalue ofH.
Proof. (1) According to the definition, we have H* U = 0, and for any x ∈ Rd,
xτH*x = xτHx - (XUTHU ∈ [0,xτHx],	(25)
which implies H* is semi-positive definite, and λi(H*) ≥ λ1(H*) = 0. Furthermore, we have the
following equality:
xTH* x = min kx - tUk2H .	(26)
10
Under review as a conference paper at ICLR 2019
(2)	We will prove λ%(H*) ≤ λi(H) for all i, 1 ≤ i ≤ d. In fact, using the Min-Max Theorem, We
have
λi(H*)
min max
dimV =i x∈V
XT H *x
min max
dimV =i x∈V
XT Hx
λi(H).
≤
(3)	We will prove λ%(H*) ≥ λ%-ι(H) for all i, 2 ≤ i ≤ d. In fact, using the Max-Min Theorem, we
have
ʌ ( τr*∖	___ 一一 ♦一 xTH* x	___ _一 ♦ 一 一一 ♦ 一 Il X - tu k 2-
λi(H ) = max min x∣∣ %X = max	mm mm ∣∣ ∣∣2H
dimV =n-i+1 x∈V IxI	dimV =n-i+1,u⊥V x∈V t∈R	IxI
≥
max min min
dimV =n-i+1,u⊥V X∈V t∈R
kχ-tukH
∣∣x-tu∣2
max min
dimV =n-i+1 y∈span{V,u}
∣⅛ υ-X - tu
∣∣y∣2 ,y = X tu
≥ max miny Hy
dimV =n-(i-1)+1 y∈V ∣y∣
λi-1(H),
where we have used the fact that X ⊥ u, kX - tuk2 = kXk2 + t2 kuk2 ≥ kXk2 .
□
There are several corollaries related to the spectral property of H *. We first give some definitions.
Since H * is positive semi-definite, we can define the H *-seminorm.
Definition A.2. The H * -seminorm ofa vector X is defined as kXkH* := XTH*X. kXkH* = 0 if and
only if X is parallel to u.
Definition A.3. The pseudo-condition number of H* is defined as κ*(H*) :
λd(H *)
λ2(H*).
Definition A.4. For any real number ε, the pseudo-spectral radius of the matrix I - εH* is defined
as ρ*(I — εH*):= max |1 — ελi(H*) |.
2≤i≤d
The following corollaries are direct consequences of Lemma A.1, hence we omit the proofs.
Corollary A.5. The pseudo-condition number of H* is less than or equal to the condition number
ofH:
κ*( H * ) .一	λd(H*)	≤	λd(H)	一.	K(H)	(27)
K (H ):=	λ2(H*)	≤	λι(H)	=:	K(H)，	(2/)
where the equality holds up if and only if u ⊥ span{v1, vd}, vi is the eigenvector of H correspond-
ing to eigenvalue λi (H).
Corollary A.6. For any vector X ∈ Rd and any real number ε, we have k(I - εH*)XkH* ≤
ρ* (I - εH* )kXkH* .
Corollary A.7. For any positive number ε > 0, we have
ρ*(I-εH*) ≤ρ(I-εH),	(28)
where the inequality is strict if uTvi 6= 0 for i = 1, d.
It is obvious that the inequality in (27) and (28) is strict for almost all u.
A.2 Scaling property
The dynamical system defined in equation (9)-(10) is completely determined by a set of configura-
tions {H, u, a0, w0, εa, ε}. It is easy to check the system has the following scaling property:
Lemma A.8 (Scaling property). Suppose μ = 0,γ = 0,r = 0, QT Q = I ,then
(1)	The configurations {μQτHQ, √γμQu, γao, γQwo, εa,ε} and {H,u,a0,w0,εa,ε} are
equivalent.
(2)	The configurations {H, u, a0, w0 , εa , ε} and {H, u, a0 , rw0 , εa , r2ε} are equivalent.
11
Under review as a conference paper at ICLR 2019
The scaling property is valid for general loss functions provided batch normalization is used. Con-
sider a general problem
min J0(w) := Ex,y[f(y, xT w)],	(29)
w∈Rd
and its BN version
min	J(a, w) : = Ex,y f y, aNBN (xT w) .	(30)
w∈Rd,a∈R
Then the gradient descent method gives the following iteration,
0,,T N
ak+1 = ak + εa~σkΓ~,	(31)
σk
wk+1 = wk + ε σk (h - Wk2^ Hwk) ,	(32)
σk	σk
∙-v
where h = h®wk/σk), and h is the gradient of original problem:
h(w) := Ex,y[xf20 (y, xT w)].	(33)
It is easy to check the general BNGD has the following property:
Lemma A.9 (General scaling property). Suppose r = 0, then the configurations {wo,ε, *} and
{rwo, r2ε, *} are equivalent. Here the sign * means other parameters.
A.3 Proof of Theorem 3.3
Recall the BNGD iterations
wkT g
ak+1 = ak + εa I ^k----ak卜
wk+1 = wk + ε σk (g - Wk^ Hwk).
The scaling property simplify our analysis by allowing us to set, for example, kuk = 1 and kw0 k =
1. In the rest of this section, we only set kuk = 1.
For the step size of a, it is easy to check that ak tends to infinity with εa > 2 and initial value
a0 = 1, w0 = u. Hence we only consider 0 < εa < 2, which make the iteration of ak bounded by
some constant Ca .
Lemma A.10 (Boundedness of ak). If the step size 0 < εa < 2, then the sequence ak is bounded
for any ε > 0 and any initial value (a0, w0).
TC
Proof. Define ak := ^kg, which is bounded by ∣αk | ≤ UtHHu =: C, then
ak+1 = (1 - εa)ak + εaαk
= (1 - εa)k+1a0 + (1 - εa)kεaα0 + ... + (1 - εa)εaαk-1 + εaαk.
Since |1 - ε°∣ < 1, we have |a®+i| ≤ |ao| + 2CPk=0 |1 - ε°∣i ≤ |ao| + 2C 1-∣1-εa∣.	□
According to the iterations (34), we have
U - 祟 wk+1 =(I — ε ^ 耍 H )(u — 耍 wk)	(34)
Define
ek := U — W^wk,	(35)
qk := uT HU — (wkkg) = kek IlH ≥ 0,	(36)
T
εk := εσk W⅛g,	(37)
12
Under review as a conference paper at ICLR 2019
and using the property w^ = argmin ku - tw∣∣H, and the property of H-norm, We have
qk+1 ≤ I |u - wσ2g wk+1 ||h = k (I - εkH )ek kH ≤ P(I - εkH )2 qk ∙	(38)
Therefore We have the folloWing lemma to make sure the iteration converge:
Lemma A.11. Let 0 < ε0 < 2. Ifthere are two positive numbers ε- and ε+, and the effective SteP
size εk satisfies
0 < k⅛ ≤ ε ≤ ε+< λm2aχ
(39)
for all k large enough, then the iterations (34) converge to a minimizer.
Proof. Without loss of generality, we assume ∣∣ ɛ ∣∣2 < V^ and the inequality (39) is satisfied for
kwk k	λmax
all k ≥ 0. We will prove kwkk converges and the direction ofwk converges to the direction ofu.
(1)	Since kwkk is always increasing, we only need to prove it is bounded. We have,
2
kwk+ιk2 = kwk k2 + ε2 σ⅛ kHek k2	(40)
k
2
=kwok2 + ε2E σkHeik2	(41)
i=0
k
≤ IIwOk2 + ε2λmax ^X σ2qi	(42)
i=0 i
k
≤kw0k2+ε2 λmmCa X k⅛.	(43)
i=0
The inequality in last lines are based on the fact that kHeik2 ≤ λmax kei k2H, and |ak| are bounded
by a constant Ca . Next, we will prove Pi∞=0 * < ∞, which implies kwk k are bounded.
According to the estimate (38), we have
qk+1 ≤ max{∣1 - ε+λi∣2, |1 - f-ɪ∣2}qk	(44)
≤ max{1 - Y +,1 - e∣⅛2n }qk,	(45)
where 1 - γ+ = maxi{∣1 - ε+λi∣2} ∈ (0,1). Using the definition of qk, we have
CT CT -、min{Y+kw0k2,ε-λmin}c,	Cqk	为G
qk - qk+1 ≥ --------百口---------qk =： ∣Wk∣2 ≥ 0.	(46)
Since qk is bounded in [0, uTHu], summing both side of the inequality, we get the bound of the
infinite series P rq⅛ ≤ UTHu < ∞.
∣wk ∣2	C
k
(2)	Since kwkk is bounded, we denote ε- := ∣^	, and define P := max{∣1 - ε土λ∕} ∈ (0,1),
∣w∞∣	i
then the inequality (38) implies qk+1 ≤ P2qk. As a consequence, qk tends to zero, which implies
the direction of wk converges to the direction of u.
(3)	The convergence ofak is a consequence ofwk converging.
□
Since a. is bounded, we assume |ak| < GaVuTHu, Ca ≥ 1, and define ε0 := 尸 1----------. The
2Ca κλmax
following lemma gives the convergence for small step size.
Lemma A.12. If the initial values (a0, w0) satisfies a0w0Tg > 0, and step size satisfies εa ∈
(0,1],ε∕kwok2 < εo, then the Sequence (ak, wk) converges to a global minimizer.
13
Under review as a conference paper at ICLR 2019
T
Remark 1: If We set ao = 0, then We have wι = wo, aι = £a^g, hence aιwTg > 0 provided
w0Tg 6= 0.
Remark 2: For the case of εa ∈ (1, 2), if the initial value satisfies an additional condition 0 <
|ao| ≤ εa gg1, then we have (ak, Wk) converges to a global minimizer as well.
Proof. Without loss of generality, We only consider the case of a0 > 0, w0Tg > 0, kw0 k ≥ 1.
(1) We will prove ak > 0, WTg > 0 for all k. Denote yk := WTg, δ = k4gk.
On the one hand, if ak > 0, 0 < yk < 2δ, then
yk+ι ≥ y + ε σk 呼 ≥ y.	(47)
On the other hand, when ak > 0, yk > 0, ε < ε0, we have
yk+1 ≥ ε akσgk + yk (1 - εσ2 PgTHg) ≥ 1 yk ,	(48)
ak+1 ≥ min{ak,yk∕σk}.	(49)
As a consequence, we have ak > 0, yk ≥ δy := min{y0, δ} for all k by induction.
(2) We will prove the effective step size ε^k satisfying the condition in Lemma A.11.
Since ak is bounded, ε < ε0, we have
Λ, ♦一	L ak	Wk g ‹- εCaλmax UC fcτ 一♦	F+	/	1	KG
εk :=	ε σk	σ2	≤	λmin∣∣Wk∣∣2 ≤ εCaκ =:	ε	< 2λmaχ ,	(50)
and
qk+1 ≤ (I - εkλmin)2qk ≤ (I - εkλmin)qk < qk ∙	(51)
T	TT	T
which implies k+1 ≥ -ɪg ≥ —0-g. Furthermore, we have ak ≥ min{a0, —0-g}, and there is a
σk+1	σk	σ0	0 σ0
positive constant ε- > 0 such that
εk ≥ ε Im。：IkWkk 2
T
Wk g
σk
(52)
≥
ε
^kF.
(3) Employing the Lemma A.11, we conclude that (ak, Wk) converges to a global minimizer. 口
Lemma A.13. If step size satisfies εa ∈ (0, 1], ε∕kW0k2 < ε0, then the sequence (ak, Wk) con-
verges.
Proof. Thanks to Lemma A.12, we only need to consider the case of ak WkTg ≤ 0 for all k, and we
will prove the iteration converges to a saddle point in this case. Since the case ofak = 0 or WkTg = 0
is trivial, we assume akWkTg < 0 below. More specifically , we will prove |ak+1| < r|ak | for some
constant r ∈ (0, 1), which implies convergence to a saddle point.
(1)	If ak and ak+1 have same sign, hence different sign with WkT g, then we have |ak+1| = |1 -
εa∣∣ak|- εa∣wTg∖∕σk ≤ |1 - εa∣∣ak∣.
(2)	If ak and ak+1 have different signs, then we have
IWTg|	W ι 1	(∣∣z1∣∣2 WTg ,Tτj.nι∖	W	nuC	/ 1
∣∑kσ∏	≤ εσ2	(kgk	- ~2i g HWk)≤	2εκλmaχ	< L
(53)
Consequently, we get
lak+1| —
Iakl
εa ∣∣awkσk∣∣- (1 - εa) ≤ 2εεaκλmax - (1—εa) < εa ≤ 1.
(54)
(3)	Setting r := max(∖1 - εa∖, 2εεaκλmaχ - (1 - εa)), we finish the proof.	口
To simplify our proofs for Theorem 3.3, we give two lemmas which are obvious but useful.
14
Under review as a conference paper at ICLR 2019
Lemma A.14. If positive series fk, hk satisfy fk+1 ≤ rfk + hk, r ∈ (0, 1) and lim hk = 0, then
k→∞
lim fk = 0.
k→∞
Proof. It is obvious, because the series bk defined by bk+ι = rbk + hk, bo > 0, tends to zeros. □
Lemma A.15 (Separation property). For δ0 small enough, the set S := {w|y2q < δ0, kwk ≥ 1}
is composed by two separated parts: S1 and S2, dist(S1, S2) > 0, where in the set S1 one has
y2 < δ1, q > δ2, and in S2 one has q < δ2, y2 > δ1 for some δ1 > 0, δ2 > 0. Here y := wTg, q :=
uTHu-
(WT Hu)2
WT Hw
uTHu-
y2
WT Hw
Proof. The proof is based on H being positive. The geometric meaning is illustrated in Figure 4.
， □
Figure 4: The geometric meaning of the separation property
Corollary A.16. If lim kwk+1 - wk k = 0, and lim (wkT g)2qk = 0, then either lim (wkT g)2 = 0
k→∞	k→∞	k→∞
or lim qk = 0.
k→∞
Proof. Denote yk := wkT g. According to the separation property (Lemma A.15), we can chose a
δ0 > 0 small enough such that the separated parts of the set S := {w|y2q < δ0, kwk ≥ 1}, S1 and
S2, have dist(S1, S2) > 0.
Because yk2 qk tends to zero, we have wk belongs to S for k large enough, for instance k > k1. On
the other hand, because kwk+1 - wkk tends to zero, we have kwk+1 - wkk < dist(S1, S2) for k
large enough, for instance k > k2. Then consider k > k3 := max(k1, k2), we have all wk belongs
to the same part S1 or S2 .
If wk ∈ S1, (qk > δ2), for all k > k3, then we have lim (wkTg)2 = 0.
k→∞
On the other hand, if wk ∈ S2, (yk2 > δ1), for all k > k3, then we have lim qk = 0.
k→∞
□
Theorem A.17. Let εa ∈ (0, 1] and ε > 0. The sequence (ak, wk) converges for any initial value
(a0, w0).
Proof. We will prove kwk k converges, then prove (ak, wk) converges as well.
(1) We will prove that kwkk is bounded and hence converges.
In fact, according to the Lemma A.13, once ∣∣wk ∣∣2 ≥ ε∕ε0 for some k, the rest of the iteration will
converge, hence kwk k is bounded.
(2) We will prove lim ∣wk+1 - wk ∣ = 0, and lim (wkTg)2qk = 0.
k→∞	k→∞
15
Under review as a conference paper at ICLR 2019
The convergence of kwk k implies Pk a2kqk is summable. As a consequence,
lim a2kpk = 0, lim akek = 0,	(55)
k→∞	k→∞
and lim kwk+1 - wk k = 0. In fact, we have
k→∞
Ilwk+ι - wk 112 = ε2aσ2IlHekII2 ≤ λmaxε aqk → 0.	(56)
' ⅛*k*i ■» ∙y->
Consider the iteration of series |ak — Wg∕σk |,
ak+1 -
wT+lg∣ / I 力 1 wT+lg∣ I I wT+lg
~k+τ^| ≤ ∣αk+1 - ~σr ∣+1~σr-
T
wk + lg
σk+ι
≤	(1	一 εa )∣∣ak 一		WTg σk	I JakgTHek |	I + ε -σ- +	lwk+1gl ∣σk+ι 一 σk| (σk σk+1) k+1	k
≤	(1	一 εa )	∣ak 一	T Wk g σk	I kgg^aɑkekh^ + ε	σ	+ Jwk+1gl ε λmax ∣∣αk ek ∣∣h (σk σk+1)	σk	k k H
≤	(1	一 εa )	∣ak 一	T Wk g σk	∣∣ + 2CIakek IH.	
(57)
The constant C in (57) can be chosen as C = ελmaχkukH. Since IIakekIIH tends to zero, We can
λmin kw0 k2	k k H	,
use Lemma A.14 to get lim |ak - wkTg∕σk | = 0. Combine the equation (55), then We have
k→∞
lim (wkTg)2pk = 0.
k→∞
(3) According to the Corollary A.16, We have either lim yk2 = 0, or lim qk = 0. In the former
k→∞	k→∞
case, the iteration of (ak, wk) converges to a saddle point. HoWever, in the latter case, (ak, wk)
converges to a global minimizer. In both cases We have (ak, wk) converges.
□
To finish the proof of Theorem 3.3, We have to demonstrate the special case of εa = 1 Where the
set of initial values such that BN iteration converges to saddle points is Lebeguse measure zero. We
leave this demonstration in next section Where We consider the case of εa ≥ 1.
A.4 Impossibility of converging to strict saddle points
In this section, We Will prove the set of initial values such that BN iteration converges to saddle points
is (Lebeguse) measure zero, as long as εa ≥ 1. The tools in our proof is similar to the analysis
of gradient descent on non-convex objectives (Lee et al., 2016; Panageas & Piliouras, 2017). In
addition, We used the real analytic property of the BN loss function (16).
For brevity, here We denote x := (a, w) and let εa = ε, then the BN iteration can be reWrote as
xn+1 = T(Xn) := Xn - £▽ J(Xn).
Lemma A.18. If A ⊂ T(Rd∕{0}) is a measure zero set, then the preimage T-1(A) is of measure
zero as well.
Proof. Since T is smooth enough, according to Theorem 3 of (Ponomarev, 1987), We only need
to prove the Jacobian of T(X) is nonzero for almost all X ∈ Rd. In other Words, the set {X :
det(I — εV2J(x)) = 0} is of measure zero. This is true because the function det(I — εV2J(x))
is a real analytic function of X ∈ Rd∕{0}. (Details of properties of real analytic functions can be
found in (Krantz & Parks, 2002) for instance).
□
Lemma A.19. Let f : X → R be twice continuously differentiable in an open set X ⊂ Rd and
x* ∈ X be a stationary point of f. If ε > 0, det(I 一 εV2f (x*)) = 0 and the matrix V2 f (x*) has
at least a negative eigenvalue, then there exist a neighborhood U of x* such that the following set
B has measure zero,
B := {X0 ∈ U : Xn+1 = Xn 一 εVf (Xn) ∈ U, ∀n ≥ 0}.	(58)
16
Under review as a conference paper at ICLR 2019
Proof. The detailed proof is similar to (Lee et al., 2016; Panageas & Piliouras, 2017).
Define the transform function as F(x) := X - εVf (x). Since det(I - εV2f (x*)) = 0, accorded
to the inverse function theorem, there exist a neighborhood U of x* such that T has differentiable
inverse. Hence T is a local C 1 diffeomorphism, which allow us to use the central-stable manifold
theorem (Shub, 2013). The negative eigenvalues of V2f(x*) indicates λmax(I - εV2f(x*)) > 1
and the dimension of the unstable manifold is at least one, which implies the set B is on a lower
dimension manifold hence B is of measure zero.
□
Lemma A.20. If εa = ε ≥ 1, then the set of initial values such that BN iteration converges to
saddle points is of Lebeguse measure zero.
Proof. We will prove this argument using Lemma A.18 and Lemma A.19. Denote the saddle points
set as W := {(a*,w*) : a* =0,w*Tg = 0}. The basic point is that the saddle point x* := (a*,w*)
of the BN loss function (16) has eigenvalues { ɪ(1 ± 1 + 4 4w*THw*), 0,…，θ} of the Hessian
matrix.
(1)	For each saddle point X* := (a*, w*) of BN loss function, ε ≥ 1 is enough to allow us to use
Lemma A.19. Hence there exist a neighborhood Ux* of X* such that the following set Bx* is of
measure zero,
Bx* := {xo ∈ Uχ* : Xn ∈ Uχ*, ∀n ≥ 1}.
(59)
(2)	The neighborhoods Uχ* of all x* ∈ W forms a cover of W, hence, accorded to Lindelof's open
cover lemma, there are countable neighborhoods {Ui : i = 1, 2, ...} cover W, i.e. U := ∪iUi ⊇ W.
As a consequence, the following set A0 is of measure zero,
A0 := ∪iBi = ∪i{X0 ∈ Ui : Xn ∈ Ui, ∀n ≥ 1}.
(60)
(3)	Define Am+1 := T-1(Am) = {X ∈ Rd : T(X) ∈ Am}, m ≥ 0. According to Lemma A.18, we
have all Am and ∪mAm are of measure zero.
(4)	Since each initial value X0 such that the iteration converges to a saddle point must be contained
in some set Am, we finish the proof.
□
Combine the results of Lemma A.20, scaling property 3.2 and the convergence theorem A.17, we
have the following theorem directly.
Theorem A.21. If εa = 1, ε ≥ 0, then the BN iteration (9)-(10) converges to global minimizers for
almost all initial values.
A.5 Convergence rate
In the last section, We encountered the following estimate for ek = U 一 wkgWk
I∣ek+1 ∣∣H ≤ P(I - εkH)kek∣∣h.
(61)
We can improve the convergence rate of the above if H * has better spectral property. This is the
content of Proposition 3.4 and the following lemma is enough to prove it.
Lemma A.22. The following inequality holds,
(1 一 δk)∣ek+1∣H ≤ (p* (I — εk H *) + δk) ∣∣ek ∣∣H,
(62)
λmax £| ak |
where δk :
σ2
∣ek∣H.
17
Under review as a conference paper at ICLR 2019
Proof. The case of wkTg = 0 is trivial, hence we assume wkTg 6= 0 in the following proof. Rewrite
the iteration on wk as the following equality,
U - w^kg wk+1 =(I - εk H )ek = (I - εk H *)ek - εk(1 - UWH {2 )Hu.	(63)
σk	u uσk
Then We will use the properties of H * -seminorm to prove our argument.
(1)	Estimate the H * -seminorm on the right hand of equation (63).
kright∣∣Η* ≤ Il(I - εkH*)ek ∣∣Η* + lεk | (1 - UWHU；2 ) kHukH*	(64)
≤ P*(I - εkH*)kek∣H* + λma⅛lkekkH	(65)
=P*(I - εkH*)√U⅛kekkH + λm√U⅛≡TgikekkH	(66)
=√U⅛ ("(I - εkH*)+ δk 1际kH.	(67)
(2)	Estimate the H*-seminorm on the left hand of equation (63). Using the H -norm on the iteration
of wk , we have
σk+1 = kwk + εσkHek l∣H ≥ σk - ε λmσχlakl kek ∣∣H∙	(68)
σk	σk
Consequently, we have
kleftkH* = √⅛⅛σ≡kek+ιkH ≥ √⅛⅛(1 - δk)kek+ιkH∙	(69)
(3)	Combining (1) and (2), we finish the proof.	□
Now, we turn to the convergence of the loss function which can be rewritten as Jk = 2 ∣∣≡k ∣∣H with
≡k = u 一 σkWk. There is an useful equality between k(⅛kH and ∣∣ek ∣∣H:
kekIlH = IlekIlH + (ak - wkkg) .	(70)
Recalling the inequality (57) and the boundedness of ak, we have a constant C0 such that
lak+1 — wk+1g I ≤ |1 — εα∣∣ak — wTg I + C0∣∣ek Ih ,	(71)
k+1	σk+1	a k	σk	0 k H,
which indicates that we can use the convergence of ek to estimate the convergence of the loss value
Jk . In fact we have the following lemma.
Lemma A.23. If kek kH ≤ Cρk for some constant C and ρ ∈ (0, 1), εa ∈ (0, 1], then we have
BkH ≤ C2ρ2k + (Cι(1-εa)k + C2kγk)2,	(72)
where Y = max(ρ, 1 — εα), Ci = |ao — WTg∕σ0∣ and C2 = CCo.
Proof. According to the inequality (71), we have
k-1
ak -	wσkg I	≤	CI(I -	εa)k	+ C2	^X(I -	εJpk	i ≤	CI(I -	εɑ)k	+	C2kY k ∙	(73)
i=0
Put it in the equality (70), then we finish the proof.
□
18
Under review as a conference paper at ICLR 2019
A.6 Estimating the effective step size
Accorded to Lemma A.12, the effective step size εk has same order with 口仅：产 provided a°WTg >
0,ε∕∣∣wo∣∣ < εo. In fact, we have
T T-
:=	aow0-g W_ε_尸	≤ εk	≤ Ju HU、 Caε υ2	=:产后.	(74)
kwk k2	σ0 λmax kwk k2 k	λminkwkk2 kwk k2
Hence, to prove the Proposition 3.5, we only need to estimate the norm ofwk.
proof of Proposition 3.5. According to the BNGD iteration, we have (see the proof of Lemma A.11)
k
∣∣wk+1k2 ≤ IIwOk2 + ε2λmax ^X W 9i .	(75)
i=0 i
(1)	When jw^2 < εo (εo is defined in Lemma A.12), the sequence qk satisfies qk+ι ≤ (1 一
εkλmin)qk. Hence the norm of wk is bounded by
∞
Ilwk k2 ≤ IIwOk2 + εκCɑWσ0g Xgi - qi+1) ≤ llw0k2 + Cε,	(76)
0 i=0
for some constant C. As a consequence,
Clε := T一uCιεr,、≤ εk ≤ 旨2 =: C2ε.	(77)
1 kw0k2(1+Cε0) k kw0 k2	2 .
(2)	When ε is large enough, the increment of the norm Iwk I at the first step is large as well. In fact,
we have
2
kw1 k - kwOk = ε σ02 kHe0k = C3ε .	(78)
T
Since ∣∣g∣∣2 ≥ —⅛ggτHwo, we have aιwTg > aιwTg > 0. Choose ε to be larger than some value
σ0
ει such that 口仅：口2 < εo, then We can use the argument in (1) on (a1,w1). More precisely, there are
two constants, C1 , C2, such that
IIe1jz ≤ εk ≤ U°2：2.	(79)
kw1k2	k	kw1 k2
Plugging the equation (78) into it, we have
____C1ε2_k ≤ _____Cιε2____ ≤	ει ε	≤ ___C2ε2___ ≤	C2	(80)
∣∣W0 k2 + C3ε2 ≤ kw0k2 + C3ε2 ≤ εkε ≤ kw0k2 + C3ε2 ≤ C3 .	(80)
□
19
Under review as a conference paper at ICLR 2019
B Modified BNGD
Through our analysis, we discovered that a modification of the BNGD, which we call MBNGD,
becomes much easier to analyze and possesses better convergence properties. Note that the results
in the main paper do not depend on the results in this section.
The modification is simply to enforce ak = wσk-g at every iteration, which yields the modified
iterations:
wk+1
wk + ε
—
T
wkT g
k
Hwk
(81)
In a sense, one can view the above as a limiting version of BNGD where the BN rescaling variable a
is adjusted every step to the optimal value based on the current value of the weights w. The MBNGD
iterations is governed by the variables: H, u, a0 , w0 , ε, where the scaling properties (Lemma 3.2,
omit the parameter εa now) remains. More importantly, we find the iteration will converge to a
saddle point if and only if it exactly meets the saddle point at a finite step. More precisely, we have
the convergence theorem:
Theorem B.1 (Convergence for modified BNGD). The iteration sequence wk in equation (81) con-
verges for any initial value w0 and any step size ε > 0. It converges to a global minimizer almost
sure, in the sense that the set of initial values such that wk converges to a saddle point is of Lebesgue
measure zero. Furthermore, It converges to a saddle point if and only if wkTg = 0 for some k.
Particularly, if ε < 、 2kw0∣k ∣∣2, WTg = 0, then Wk converges to a global minimizer
λmax κkuk
In the following, we assume kuk = 1.
B.1 Proof
Lemma B.2. If wTg = 0 and 口仅；产 < ε0 := κλ2-, then the sequence Wk converges to a global
minimizer.
Proof. Similar to the proof of Lemma A.12, but here the effective step size is always nonnegative
which is defined as
ɛk
εκ
≤ ', εκ,c =： ε+ <	2
—∣∣W0 k2	λmax
The inequality (38) immediately gives qk+1 ≤ qk, which implies (wk+1g)
σk+1
a consequence, the effective step size has a lower bound
(82)
ɛk ≥ ε(W°g)λmaχlwkk2 =： f⅛ ∙	(83)
Employing the Lemma A.11,we conclude that Wk converges to a global minimizer.	□
Lemma B.3. IfWkTg 6= 0for all k, then Pk∞=0 |WkT g| = Pk∞=0 (WkTg)2 = ∞.
Proof. Without loss of generality, we assume ∣∣w0k ≥ 1, denote yk := wT g and set δ = Igk. From
the iteration of Wk , we have
yk+1 = yk + 喷(kgk2 - σy∣gτHWk)∙	(84)
If 0 < |WkT g | < 2δ, then we have the inequality:
ι∣gk2 - σy∣ gτ HWk ≥ ι∣gk2 - KigI |yk| ≥1 ι∣gk2,	(85)
then
lyk+1l ≥ (l + λmax∣∣wfck2 吟)|yk | > |yk | > 0∙	(86)
As a consequence, limk→∞ WkTg = 0 is not possible unless WkTg = 0 for some k, which implies the
results we want.
□
20
Under review as a conference paper at ICLR 2019
Theorem B.4. The iteration sequence wk in equation (81) converges for any initial value w0 and
any step size ε > 0. Furthermore, wk will converge to a global minimizer unless wkTg = 0 for some
k.
Proof. Obviously, if wkTg = 0 for some k = k0, then wk = wk0 for all k ≥ k0, hence wk converges
to wk0. Without losing generality, we consider wkTg 6= 0 for all k and kw0k ≥ 1 below.
(1)	Firstly, we will prove that kwk k is bounded and hence converges.
In fact, according to the Lemma B.2, once ∣∣wk ∣∣2 ≥ ε∕ε0 for some k, the rest of the iteration will
converge, hence kwk k is bounded.
(2)	Secondly, we will prove wk converges to a vector parallel to u.
T
Denote yk := WTg, Zk := Wkg. The convergence of ∣∣wk ∣∣ indicates that Ek=O Zkqk is summable,
and then Pk∞=0 yk2qk is summable as well. Therefore we have
Ilwk+1 - wkk2 = ε2 (wσ 4 ) I |g - wk2g Hwk I I ≤ λmaxε2 z2qk,	(87)
and the above tends to zero, i.e. limk→∞ ∣wk+1 - wk ∣ = 0.
According to the separation property (Lemma A.15), we can chose a δ0 > 0 small enough such that
the separated parts of the set S := {w|y2q < δ0, ∣w∣ ≥ 1}, S1 and S2, have dist(S1, S2) > 0.
Because yk2qk tends to zero, we have wk belongs to S for k large enough, for instance k > k1. On
the other hand, because ∣wk+1 - wk∣ tends to zero, we have ∣wk+1 - wk∣ < dist(S1, S2) for k
large enough, for instance k > k2. Then consider k > k3 := max(k1, k2), we have all wk belongs
to the same part S1 or S2 .
However, Lemma B.3 says Pk∞=0 yk2 = ∞, hence wk ∈ S1 (qk > δ2) for all k > k3 is not true.
Therefore wk ∈ S2 (yk2 > δ1) for all k > k3. Consequently, we can claim that Pk∞=0 qk is summable
and wk converges to a vector parallel to u.
□
B.2 Experiment
Here we test the convergence and stability of MBNGD for OLS model. Consider the diagonal matrix
H = diag(h), where h = (1, ..., κ) is an increasing sequence. The scaling property allows us to set
the initial value w0 having same 2-norm with u, ∣w0 ∣ = ∣u∣ = 1.
Figure 5 gives an example of a 5-dimensional H with condition number κ = 2000. The GD and
MBNGD iteration are executed k = 5000 times where u and w0 are randomly chosen from the unit
sphere. The values of effective step size, loss ∣ek ∣2H and error ∣ek ∣ are plotted. Furthermore, to
explore the performance of GD and MBNGD, the mean values over 300 random tests are given. It
is worth to note that, the geometric mean (G-mean) is more reliable than the arithmetic mean (A-
mean), where the geometric mean of x can be defined as exp(E(ln x)). Here the reliability means
that the G-mean converges quickly when the number of tests increase, however the A-mean does not
converge as quickly. In this example, the optimal convergence rate of MBNGD is observably better
than GD. This acceleration phenomenon is ascribed to the pseudo-condition number of κ*(H*)
being less than K(H). However, if the difference between (PSeUdo-)condition number of H and H *
is small, the acceleration is imperceptible.
Another important observation is that the BN significantly extends the range of ‘optimal’ step size,
which is embodied by the effective step size ε^k having a large constant C in ε = O(Cε-1). This
means we can chose step size in BN at a large interval to get almost same or better convergence rate
than that of the best choice for GD.
Figure 6 gives an example of 100-dimension H with condition number κ = 2000. Similar results
as those in the 5-dimensional case are obtained. However, the best optimal convergence rate of
MBNGD here has not noticeably improved compared with GD with the optimal learning rate, which
is due to the fact that large d decrease the difference between eigenvalues of H and H*.
Additional tests indicate that:
21
Under review as a conference paper at ICLR 2019
Figure 5: Plot of 300 random initial tests. H = diag(logspace(0,log10(2000),5)).
Figure 6: Plot of 500 random initial tests. H = diag(linspace(1,2000,100)).
(1)	larger dimensions leads to larger intervals of ‘optimal’ step size, (Figure 7)
(2)	the effect of condition number on the ‘optimal’ interval is small (Figure 8).
Figure 7: H = diag(linspace(1,2000,d)).
Figure 8: H = diag(linspace(1,cond,100)).
22