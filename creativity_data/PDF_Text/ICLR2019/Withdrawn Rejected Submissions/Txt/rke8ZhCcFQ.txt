Under review as a conference paper at ICLR 2019
Attack Graph Convolutional Networks by
Adding Fake Nodes
Anonymous authors
Paper under double-blind review
Ab stract
Graph convolutional networks (GCNs) have been widely used for classifying
graph nodes in the semi-supervised setting. Previous work have shown that GCNs
are vulnerable to the perturbation on adjacency and feature matrices of existing
nodes. However, it is unrealistic to change existing nodes in many applications,
such as existing users in social networks. In this paper, we design algorithms to
attack GCNs by adding fake nodes. A greedy algorithm is proposed to generate
adjacency and feature matrices of fake nodes, aiming to minimize the classifica-
tion accuracy on the existing nodes. In additional, we introduce a discriminator
to classify fake nodes from real nodes, and propose a Greedy-GAN attack to si-
multaneously update the discriminator and the attacker, to make fake nodes indis-
tinguishable to the real ones. Our non-targeted attack decreases the accuracy of
GCN down to 0.10, and our targeted attack reaches a success rate of 99% on the
whole datasets, and 94% on average for attacking a single target node.
1	Introduction
Graphs play a very important role in many real world applications, such as social networks (Face-
book and Twitter), biological networks (protein-protein interaction networks and gene interaction
networks), as well as attribute graphs (PubMed and Arxiv) (Grover & Leskovec, 2016; Ying et al.,
2018; Rhee et al., 2017). Node classification is one of the most important tasks on graphs—given
a graph with labels associated with a subset of nodes, predict the labels for rest of the nodes. For
this node classification task, deep learning models on graphs, such as Graph Convoltional Networks
(GCNs), have achieved state of the art performance (Kipf & Welling, 2016). Moreover, GCNs
have wide applications in cyber security, where they can learn a close-to-correct node labeling semi-
autonomously. This reduces the load on security experts and helps to manage networks that add or
remove nodes dynamically, such as, WiFi networks in universities and web services in companies.
The wide applicability of GCNs motivates recent studies about their robustness. Zugner et al. (2018)
Dai et al. (2018) developed algorithms to attack GCNs, showing that by altering a small amount of
edges and features, the classification accuracy of GCNs can be reduced to chance-level. However,
changing edges or features associated with existing nodes is impractical in many cases. For example,
in social network applications, an attacker has to login to the users’ accounts to change existing
connections and features, and gaining login accesses is almost impossible. In comparison, adding
fake nodes that correspond to fake accounts or users, can be much easier in practice. But the key
question is can we interfere the classification results of existing nodes by adding fake nodes to the
network? We answer this question affirmative by introducing novel algorithms to design fake nodes
that successfully reduce GCN’s performance on existing nodes.
To design the adjacency and feature matrices associated with fake nodes, we have to address two
challenges. First, the edges and features are usually discrete 0/1 variables. Although there have been
many algorithms proposed for attacking image classifiers, such as FGSM, C&W and PGD attacks
(Goodfellow et al., 2014; Carlini & Wagner, 2017; Madry et al., 2017), they all assume continuous
input space and cannot be directly applied to problems with discrete input space. Second, it’s not
easy to make the fake nodes “looked” like the real ones? For example, if we add a fake node that
connects to all existing nodes, the system can easily detect and disable such fake node. In this paper,
we propose two algorithms, Greedy attack and Greedy-GAN attack, to address these two challenges.
Our contributions can be summarized below:
1
Under review as a conference paper at ICLR 2019
•	To the best of our knowledge, this is the first paper studying how to add fake nodes to attack
GCNs. We do not need to manipulate existing nodes’ adjacency and feature matrices.
•	We propose a Greedy attack algorithm to address the discrete input space problem in de-
signing fake nodes’ adjacency and feature matrices.
•	We introduce a discriminator to classify fake nodes from real nodes, and propose a Greedy-
GAN algorithm to simultaneous optimize the discriminator and the attacker. Despite a
lower successful rate, this approach can make fake nodes harder to detect.
•	We conduct experiments on several real datasets. For non-targeted attack, we get accuracy
down to 0.10 for the Cora dataset, and 0.14 for the Citeseer dataset. For targeted attack on
whole datasets, Greedy attack have up to 99% success rate on Cora and 90% on Citeseer.
For targeted attack on a single node, it could reach 94% success rate on Cora and 0.80%
success rate on Citeseer.
2	Related Work
Adversarial Attacks Adversarial examples for computer vision tasks have been studied exten-
sively. Goodfellow et al. (2014) discovered that deep neural networks are vulnerable to adversarial
attacks—a carefully designed small perturbation can easily fool a neural network. Several algo-
rithms have been proposed to generate adversarial examples for image classification tasks, including
FGSM (Goodfellow et al., 2014), IFGSM (Kurakin et al., 2016), C&W attack (Carlini & Wagner,
2017) and PGD attack (Madry et al., 2017). In the black-box setting, it has also been reported that an
attack algorithm can have high success rate using finite difference techniques (Chen et al., 2017b),
and several algorithms are recently proposed to reduce query numbers (Ilyas et al., 2018; Suya et al.,
2017). Transfer attack PaPemot et al. (2016a) and ensemble attack Tramer et al. (2017) can also be
applied to black-box setting, with lower successful rate but less number of queries. Besides attack-
ing image classification, CNN related attacks have also been exPlored. A tyPical usage is attacking
semantic segmentation and object detection (Metzen et al., 2017; Arnab et al., 2017; Xie et al., 2017;
Lu et al., 2017; Eykholt et al., 2017). Image caPtioning (Chen et al., 2017a) and visual QA (Xu et al.,
2017) could also be attacked.
Most of the above-mentioned work are focusing on Problems with continuous inPut sPace (such
as images). When the inPut sPace is discrete, attacks become discrete oPtimization Problems and
are much harder to solve. This haPPens in most natural language Processing (NLP) aPPlications.
For text classification Problem, Fast Gradient Sign Method (FGSM) is firstly aPPlied by (PaPernot
et al., 2016b). Deleting imPortant words (Li et al., 2016), rePlacing or inserting words with tyPos
and synonyms (Samanta & Mehta, 2017; Liang et al., 2017). For black box setting, Gao et al.
(2018) develoPs score functions to find out the works to modify. Jia & Liang (2017) adds misleading
sentences to fool the reading comPrehension system. Zhao et al. (2017) use GAN to generate natural
adversarial examPles. Ebrahimi et al. (2017) and Cheng et al. (2018) attacks machine translation
system Seq2Seq by changing words in text.
Graph Convolutional Neural Networks (GCN). Classification of graPh nodes has become an
imPortant Problem in cyber security aPPlications and recommender systems. GraPh convolutional
networks (GCNs) solve the Problem in an end-to-end manner and can avoid the ”oPtimize twice”
Problem of Previous graPh embedding methods. The main idea of GCNs is to aggregate associated
information from a node and its neighbors using some aggregation functions. They train the aggre-
gation stePs and the final Prediction layer end-to-end to achieve better Performance than traditional
aPProaches. There are several variations of GCNs ProPosed recently (KiPf & Welling, 2016; Pham
et al., 2016; Defferrard et al., 2016; Hamilton et al., 2017; Chen et al., 2018; Ying et al., 2018), and
we will focus on the commonly used structure ProPosed in (KiPf & Welling, 2016).
There have been several Work on attacking GCNs. Recently, Zugner et al. (2018) published an
attack on GCNs by changing current nodes’ links and features. They Present a FGSM-like method
and optimize a surrogate model named Nettack to choose the edges and features that should be
manipulated. Dai et al. (2018) shoWed that by only manipulating graph structure, the Graph Neural
NetWorks are quite vulnerable to the attacks of challenging feW edges. They proposed reinforcement
learning-based attack method to attack. They also employed a gradient ascent method to change the
2
Under review as a conference paper at ICLR 2019
graph structures in the white-box setting. Both of these two papers perform non-targeted attacks
only.
Instead of altering edges or features of existing nodes, we develop novel algorithms to add fake nodes
to interfere the performance of GCNs. This has not been done in previous work. Furthermore, we
test our algorithm in both targeted and non-targeted attacks in the experiments.
3	Preliminary
GCN is a semi-supervised learning method to classify nodes in attribute graphs. Given an adjacency
matrix A ∈ Rn×n, feature matrix X ∈ Rn×d, and a subset of labeled nodes, the goal is to predict
the labels of all the nodes in the graph. There are several variations of GCNs, but we consider one of
the most common approaches introduced in (Kipf & Welling, 2016). Starting from H0 = X, GCN
uses the following rule to iteratively aggregate features from neighborhoods:
H (l+1) = σ(D - 1 AD) -1H (I)W(l))	(1)
where A = A + IN is the adjacency matrix of the undirected graph with added self connections, IN
is the identity matrix, D is a diagonal matrix with Di,i = j Aij, and σ is the activation function.
We set σ(x) = ReLU(x) = max(0, x), which is the most common choice in practice. For a GCN
with L layers, after getting the top-layer feature HL, a fully connected layer with soft-max loss is
used for classification. A commonly used application is to apply two-layer GCN for semi-supervised
learning node classification on graph (Kipf & Welling, 2016). The model could be simplified as:
Z = f (X, A) = Softmax(Aσ(AxW(O))W⑴)，	(2)
1	1Λ 1	1A
where A = D 2 AD 2. Another choice for forming A is to normalize the adjacency matrix by
rows, leading to A = D-1A. We will experience with both choices in the experimental results.
For simplicity, we will assume our target network is structured as equation 2, but in general our
algorithm can be used to attack GCNs with more layers.
4 Attack Algorithms
In this section, we will describe our “fake nodes” attack on GCNs. We will describe both a non-
targeted attack, which tries to lower the accuracy of all the existing nodes uniformly, and a targeted
attack, which attempts to force the GCN to give a desired label to nodes. Instead of manipulating
the feature and adjacency matrices of existing nodes, we insert m fake nodes with corresponding
fake features into the graph. After that, the adjacency matrix is A0
BT
C
A
B
and the feature
X
Xf ake
matrix becomes : X 0
. Note that A is the original adjacency matrix and X is the original
feature matrix. Starting from B = 0, C = I, our goal is to design B, C, Xf ake to achieve the desired
objective (e.g., lower the classification accuracy on existing nodes).
4.1	non-targeted attack
The goal of non-targeted attack is to lower the classification accuracy on all the existing nodes by
designing features and links of fake nodes. We use the accuracy of GCN to measure the effectiveness
of attacks. We will present two different algorithms to attack GCNs: Greedy attack that updates links
and features one by one, and Greedy-GAN attack that uses a discriminator to generate unnoticeable
features of fake nodes.
4.1.1	Greedy Attack
In our attack, we define the objective function as
J (A0,X 0)= X (max ([A0σ(A0XW ⑼)W ⑴]J - [A0σ(A0XW(O))W ⑴]2),	⑶
i=1,...,n
3
Under review as a conference paper at ICLR 2019
Algorithm 1 Greedy Attack
Input: Adjacency matrix A; feature matrix X; A classifier f with loss function J; number of
iterations T .
Output: Modified graph and features G0 = (A0, X0) after adding fake nodes.
for: t = 0 to T - 1 do
Let e* = (u*, v*) J arg max Vb,c J(A0, X0)
G(t+C J G噩 + e*
B,C	B,C
Letf* = (u*,i*) J arg max VXfakeJ(A0, X0)
G(Xt+1) J G(Xt)	+ f*
Xf ake	Xf ake
return: G(t)
where yi is the correct label of node i. In this objective function, if the largest logit of node i is
not the correct label yi , it will encounter a positive score; otherwise the score will be zero. We then
solve the following optimization problem to form the fake nodes:
arg max J(A0, X0) s.t. kBk0 + kCk0 + kXf akek0 ≤ T,	(4)
B,C,Xfake
where ∣∣ ∙ ∣∣o denotes number of nonzero elements in the matrix. Also, We assume B,C,Xfake
can only be 0/1 matrices. Unlike images, graphs have discrete values in the adjacency matrix, and
in many applications the feature matrix comes from indicator of different categories. Therefore,
gradient-based techniques such as FGSM and PGD cannot be directly applied.
Instead, we propose a greedy approach—starting from B, C, Xf ake all being zeros, we add
one feature and one edge at each step. To add a feature, we find the the maximum element
in VXfake J(A0, X0) and turn it into nonzero. Similarly, we find the maximum element in
VB,CJ(A0, X0) and add the entry to the adjacency matrix. The Greedy attack is presented in Algo-
rithm 1.
In the algorithm, when adding links and features, we make sure that there is no such a link or feature
before adding. In practice, we can adjust the frequency of feature and weight updates. For example,
if the original adjacency matrix has twice nonzero elements than the feature matrix, we can update
two elements in the adjacency matrix and one element in the feature matrix at every two iterations.
4.1.2	Greedy-GAN Attack
Next we will present the attack based on the idea of Generative Adversarial Network (GAN). The
main idea is to add a discriminator to generate fake features that are similar to the original ones. In
order to do this, we first design a neural network with two fully connected layers plus a softmax
layer as the discriminator, which can be written as
D(X0) = softmax(σ(X0W(0))W(1)),	(5)
where softmax works on each row of the output matrix. Each element in D(X0) indicates whether
the discriminator classifies the node as real or fake.
We want to generate fake nodes with features similar to the real ones to fool the discriminator.
Since the output of discriminator is binary, we use binary cross entropy loss defined by L(p, y) =
-(y log(p)+(1-y) log(1-p)), where y is binary indicator of the ground-truth (real or fake images),
and p is the predicted probability by our discriminator. Then we solve the following optimization
problem :
arg max min( J(A0, X0) — C * L(D(X0), Y))	(6)
B,C,Xfake D
where Y is the ground-truth (real/fake) indicator for nodes and c is the parameter determine with the
weight of discriminator and the GCN performance. For example, if c is set with a very large value,
the objective function is dominated by the discriminator, so the node features generated will be very
close to real but with lower attack successful rate.
We adopt the GAN-like framework to train both features/adjacency matrices and discriminator
parameters iteratively. In experiments, at each epoch we conduct 10, 000 greedy updates for
4
Under review as a conference paper at ICLR 2019
Algorithm 2 Greedy-GAN Attack
Input: Adjacency matrix A; feature matrix X; A classifier f with loss function J; Discriminator
D with loss function L; number of iterations T .
Output: Modified graph G0 = (A0, X0) after adding fake nodes.
for: t = 0 to T - 1 do
if t%10000 == 0:
retrain discriminator D 10 times.
Let eadd = Hddadd) J argmaχ Vb,c [J(A0,XO)-C * L(D(XO))]
edrop = (udrop,vdrop ) J arg min NBQ [J (A ,X 0) - c * L(D(X 0))]
if ∣Vb,c [J (A0,X 0) - C * L(D(X 0))]eaJ > Vb,c [J (A,X 0) — c * L(D(X 0))]吟。。1 :
G(t+1) j G(t) + 尸*
GB,C J GB,C + eadd
else:
G(t+1) J G(t)	_ 尸*
GB,C J GB,C - edrop
Let fdd =(Uadd,iadd) J argmax VXfake [J(A0,X 0) — c * L(D(X 0))
fdrop = (Udrop，%rop) J arg min VXfake [J(A, X0) — C * L(D(X0))]
if ∣Vχfake[J (A0,X 0) — C * L(D(X 0))]fαj > ∣Vχfake[J (A0,X 0) — C * L(D((X 0))]fdrop | ：
「(t+I) j_「It)	_i_ f*
GXfake J GXfake + fadd
else:
G(t+1) J G(t)	f*
Xf ake	Xf ake	drop
return: G(t)
B, C, Xf ake and then 10 iterations of D updates. The Greedy-GAN algorithm is given as Algo-
rithm 2. Greedy-GAN supports both adding and dropping links and features. In the algorithm, we
add or drop elements according to the absolute gradient of elements, and the one with larger absolute
value will be chosen.
4.2 Targeted attack
Next we extend the proposed algorithms to conduct targeted attack on GCNs. Given an adjacency
matrix and a feature matrix, the goal is to make nodes to be classified as a desired class by manip-
ulating links and features of fake nodes. We present methods of attacking the whole dataset and
attacking only a single node for different situations.
In our method, when attacking the whole dataset, the fake nodes labels are given by a uniform
distribution, which is the same as in the non-targeted attack setting. In targeted attack, we define the
objection function as:
J(A0,X0)= X ([A0σ(A0XW(O))W⑴]i,y厂 max ([A0σ(A0XW(O))W(I)]i,：)),	⑺
i=1,...,n
where yi* is the target label for adversarial attack of node i. In this objective function, if largest logit
of node i is the target label yi* , the objective value is 0 ; otherwise the value is negative. Similar to
the non-targeted attack method, we would like to find B, C and Xfake using Greedy attack to solve
the optimization problem (4) and Greedy-GAN attack to solve the optimization problem (6).
For attacking a single node, we add three fake nodes with target labels, and the objective function is
J(A0, X0) = [A0σ(A0XW(O))W⑴]i,第 — max ([A0σ(A0XW(O))W⑴]，,)，	(8)
where i is the node to attack, then update their edges and features by Greedy attack. We do not
perform Greedy-GAN attack due to the number of fake nodes is too small, the sample of real nodes
and fake nodes are extremely unbalanced, which leads to over-fitting and an inaccurate discriminator.
5	Experiments
We use Cora and Citeseer attribute graphs as benchmarks, with a 20% / 80% labeled / unlabeled
split of the data. We add some fake nodes with corresponding fake feature matrix, and the rate of
5
Under review as a conference paper at ICLR 2019
Table 1: Accuracy of GCN before and after non-targeted attacks. Note that the final two rows are the
F1 score of the discriminator-lower ValUes indicate that added fake nodes are harder to be detected.
Dataset	Cora		Citeseer	
Normalization	row-wise	symmetric	row-wise	symmetric
Clean	-0.84	0.81	0.76	0.73
Random	0.29	0.29	0.33	0.31
Greedy	0.10	0.16	0.15	0.14
Greedy-GAN	0.12	0.34	0.25	0.36
F1 score for Greedy	^^065	0.75	0.83	0.72
F1 score for Greedy-GAN	0.40	0.46	0.53	0.24
Table 2: Success rate for targeted attacks on the whole graph.
Dataset	Cora		Citeseer	
Normalization	row-wise	symmetric	row-wise	symmetric
Greedy	0.99	-097	0.90	0.85
Greedy-GAN	0.64	0.44	0.45	0.22
fake labels, to investigate the inflUences on the resUlts. The defaUlt nUmber of fake nodes we add is
20% of real nodes, and 25% percent of fake nodes (i.e. 5% of number of real nodes) have labels.
In the initial state, the fake nodes have random features only, and no connection to any other node.
The results are shown in GCN classification accuracy for non-targeted attacks, and success rate
for targeted attacks. Lower classification accuracy or higher success rate indicates more effective
attacks.
5.1	Non-targeted Attack
We compare the effectiveness of random, Greedy algorithm, and Greedy-GAN algorithm in attack-
ing a given GCN. Table 1 shows the accuracy before and after non-targeted attacks. We can see that
both Greedy and Greedy-GAN methods work with row-wise and symmetric normalization on the
adjacency matrix. Greedy reduces the classification accuracy further than Greedy-GAN, because
Greedy-GAN generates features of nodes closer to the real nodes, and leads to lost in attacking
performance. We train a discriminator using randomly generated nodes, then feed it with fake fea-
tures generated by Greedy and Greedy-GAN algorithms. The f1 scores of the discriminators shows
that it is much harder to differentiate the features of a node between real and fake under attacks by
Greedy-GAN, as compared to Greedy.
5.2	targeted attack
A targeted attack tries to force the GCN to classify nodes to a certain class. We perform two different
experiments for targeted attacks: (1) attack the whole dataset; (2) attack only a single node.
When attacking the whole dataset, we randomly select a class, and other settings are the same as
non-targeted attacks. Table 2 shows that targeted attacks work for both Greedy and Greedy-GAN
algorithms. In particular for Greedy attack, we can achieve effective attacks, while Greedy-GAN
will make fake nodes indistinguishable from real ones.
When attacking only a single node, we add three fake nodes with target labels. Table 3 shows the
average success rate of attacking one node is very high. Due to the tiny number of nodes added,
the change in distributions of labels is unnoticeable. In our experiments, we add 13 edges and 10
Table 3: Success rate for targeted attacks on a single node.
Dataset	Cora		Citeseer	
Normalization	row-wise	symmetric	row-wise	symmetric
Greedy	0.94	0.83	0.80	0.62
6
Under review as a conference paper at ICLR 2019
Table 4: Accuracy under Greedy attack, for nodes with different degrees; Cora Dataset with sym-
metric normalization. _____________________________________________________
nodes degree	(0,5]	(5,10]	(10,20]	(20, ∞)
clean	0.82	^085^^	0.87	0.75
Greedy	0.13	0.25	0.37	0.5
Figure 1: Accuracy of GCN by attacking fea-
tures, edges or both, with Cora dataset. Green:
attacking edges; blue: attacking features; red:
attacking both.
Figure 2: Accuracy of GCN using row-wise or
symmetrical normalization, with Cora dataset.
Red: Row-wise normalization, Green : Sym-
metric normalization
features per fake nodes on average. We did not perform Greedy-GAN attack, because the numbers
of real and fake nodes are so unbalanced that over fitting will make the discriminator inaccurate.
We notice that attacking nodes in certain classes are more difficult than others. For example, at-
tacking nodes in class 0 (249 nodes) from Citeseer dataset (3312 nodes in 6 classes) is hard. The
percentage of class 0 nodes are lower that others, thus the attacked node may only have weak link-
age to class 0, and the classification is more influenced by nodes in other classes. Only linking the
attacked node with three fake nodes is not powerful enough to make it classified as class 0.
5.3	Parameter Sensitivity
Degree of Nodes As shown in Table 4, nodes with smaller degrees are easier to attack, i.e. produce
a different classification from the original one. When adding extra links between fake nodes and
real nodes, higher degree nodes are more resistant to the impact of fake nodes.
Features vs. Edges We want to know which is more important in influencing the output of the GCN:
modifying features or modifying edges. In this experiment, we randomly initialize some edges and
features for the fake nodes, then update either features or edges, while keeping the other one fixed.
Figure 1 shows that modifying features have a minor effect on the classification accuracy, and the
attack becomes less effective under too many feature modifications. Edges play a more significant
role for attacking. This insight inspires us to use a GAN to generate fake nodes and features.
Row-wise vs. Symmetrical Normalization We found that the implementations of GCNs may dif-
fer by frameworks in how they normalize the adjacency matrix. The Pytorch code normalizes the
adjacency matrix by row A = D-1 (A + I), while the TensorFloW code normalizes it symmetri-
cally A = D-1/2(A + I)D-1/2. They are both correct as claimed by the authors, and this is an
ambiguous point in the definition of GCNs in general.
In this experiment, a fixed amount of fake data are added per iteration, i.e. more iterations means
more fake data. Figure 2 shows that when training a ”clean” GCN, with no attack, there is only
a slight difference in performance, and row-wise normalized GCN is better than the symmetric
normalized one. While under attacks by a small amount of fake data, row-wise normalized GCN is
more robust than the symmetric normalized one. However, when under attacks by a large amount of
fake data, the symmetric scaling version is more robust. The reason might be that, when adding a
fake link, the change of each element in adjacency by the symmetric normalization is smaller than
the row-wise one.
7
Under review as a conference paper at ICLR 2019
Figure 3: Accuracy of GCN under evasion or
poisoning attacks, with Citeseer dataset.
Figure 4: Accuracy of GCN using different
percentage of fake nodes, with Cora dataset.
Table 5: Accuracy of GCN under non-attacks by Greedy and Greedy-GAN, with different label rates
and Cora dataset. _________________________________________________________
label rate	0%	5%	10%	15%	20 %
Greedy	0.08	0.10	0.18	0.21	0.23
Greedy-GAN	0.33	0.12	0.13	0.14	0.16
Evasion vs Poisoning In industry, GCN is normally used with longitudinal data. Training a new
network is time consuming, and changes between updates are not significant, thus people just retrain
the network after certain amount of time. Figure 3 shows the difference in classification accuracy
without (evasion) and with (poisoning) retraining including the fake data in the process. The GCN
is retained with learning rate 0.01, 50 epochs after the data has been modified. We run GCN with
different random seeds. The results show that both evasion and poisoning can effectively reduce the
accuracy of GCN, and the symmetric normalization is more robust under poisoning attacks.
Number of Fake Nodes Figure 4 shows how the number of fake nodes influences the classification
accuracy. We use 2.5%, 5%, 10 % and 20% of fake nodes, and assign random labels to the fake
nodes. As expected, more fake nodes yields more effective attacks, for both evasion and poisoning.
We notice with 20 % of fake nodes and all nodes labeled, the attack has lower effectiveness than the
default setting with 20 % fake nodes and 5 % labeled. We will talk about this in the next section.
Label Rate We will discuss the effect of different label rates of fake nodes. In Table 5 we keep
the percentage of fake nodes to 20%, and change the percentage of labeled nodes from 0 % to
20%. Intuitively we thought that a higher label rate will lead to a larger impact in GCN’s accuracy.
However, it turns out that when the label rate is 0%, non-targeted attack (Greedy) has the largest
effect. We suspect the reason may be that when the label rate is 0, i.e. not considering the features
matrix, multiple edges might be viewed as adding edges for the existing nodes in the graph. It is
also true for targeted attack; for example, if we give all the fake nodes random labels for the Citeseer
dataset and use a symmetric normalized adjacency matrix, the targeted attack success rate is 0.81,
compared to 0.85 when with label as shown in Table 2. For attack by Greedy-GAN, a different
pattern is observed. Using 0 label rate yields the least effect, and from 5% to 20%, the result is
similar to what we find using Greedy.
6	Conclusion
We present two algorithms, Greedy and Greedy-GAN, on adversarial attacks of GCNs by adding
fake nodes, without changing any existing edges or features, for both non-targeted and targeted at-
tacks. We successfully attacked existing GCN implementations, and explored parameter sensitives,
such as number of fake nodes and different label rates of fake nodes. To make the attack unnotice-
able, we added a discriminator using the Greedy-GAN algorithm to generate features of fake nodes.
We noticed that data cleaning before training is crucial, and adding a discriminator makes the impact
of attacks weaker. There is a trade-off between the efficiency of attack and realness of fake nodes’
features.
8
Under review as a conference paper at ICLR 2019
References
Anurag Arnab, Ondrej Miksik, and Philip H. S. Torr. On the robustness of semantic segmentation
models to adversarial attacks. CoRR, abs/1711.09856, 2017.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017,
pp. 39-57, 2017.
Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. Show-and-fool: Crafting
adversarial examples for neural image captioning. CoRR, abs/1712.02051, 2017a.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via
importance sampling. CoRR, abs/1801.10247, 2018.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: zeroth order op-
timization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS
2017, Dallas, TX, USA, November 3, 2017, pp. 15-26, 2017b.
Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. Seq2sick: Evaluating the
robustness of sequence-to-sequence models with adversarial examples. CoRR, abs/1803.01128,
2018.
Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack
on graph structured data. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 1123-1132, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-
10, 2016, Barcelona, Spain, pp. 3837-3845, 2016.
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples
for NLP. CoRR, abs/1712.06751, 2017.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Dawn Song, Tadayoshi Kohno, Amir
Rahmati, Atul Prakash, and Florian Tramer. Note on attacking object detectors with adversarial
stickers. CoRR, abs/1712.08062, 2017.
Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text
sequences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops, SP
Workshops 2018, San Francisco, CA, USA, May 24, 2018, pp. 50-56, 2018.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. CoRR, abs/1412.6572, 2014.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
San Francisco, CA, USA, August 13-17, 2016, pp. 855-864, 2016.
W. L. Hamilton, R. Ying, and J. Leskovec. Inductive Representation Learning on Large Graphs.
ArXiv e-prints, June 2017.
A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Black-box Adversarial Attacks with Limited Queries
and Information. ArXiv e-prints, April 2018.
Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pp. 2021-2031, 2017.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. CoRR, abs/1609.02907, 2016.
9
Under review as a conference paper at ICLR 2019
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial Machine Learning at Scale. ArXiv e-prints,
November 2016.
Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation
erasure. CoRR, abs/1612.08220, 2016.
Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text
classification can be fooled. CoRR, abs/1704.08006, 2017.
Jiajun Lu, Hussein Sibai, and Evan Fabry. Adversarial examples that fool detectors. CoRR,
abs/1712.02494, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. CoRR, abs/1706.06083, 2017.
Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, and Volker Fischer. Universal
adversarial perturbations against semantic image segmentation. In IEEE International Conference
on Computer Vision, ICCV2017, Venice, Italy, October 22-29, 2017, pp. 2774-2783, 2017.
Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning:
from phenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016a.
Nicolas Papernot, Patrick D. McDaniel, Ananthram Swami, and Richard E. Harang. Crafting ad-
versarial input sequences for recurrent neural networks. In 2016 IEEE Military Communications
Conference, MILCOM 2016, Baltimore, MD, USA, November 1-3, 2016, pp. 49-54, 2016b.
T. Pham, T. Tran, D. Phung, and S. Venkatesh. Column Networks for Collective Classification.
ArXiv e-prints, September 2016.
S. Rhee, S. Seo, and S. Kim. Hybrid Approach of Relation Network and Localized Graph Convolu-
tional Filtering for Breast Cancer Subtype Classification. ArXiv e-prints, November 2017.
Suranjana Samanta and Sameep Mehta. Towards crafting text adversarial samples. CoRR,
abs/1707.02812, 2017.
Fnu Suya, Yuan Tian, David Evans, and Paolo Papotti. Query-limited black-box attacks to classifiers.
CoRR, abs/1712.08713, 2017.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick D. McDaniel. Ensemble
adversarial training: Attacks and defenses. CoRR, abs/1705.07204, 2017.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan L. Yuille. Adversarial
examples for semantic segmentation and object detection. In IEEE International Conference on
Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pp. 1378-1387, 2017.
Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darell, and Dawn Song. Can you
fool AI with adversarial examples on a visual turing test? CoRR, abs/1709.08693, 2017.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Pro-
ceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, KDD 2018, London, UK, August 19-23, 2018, pp. 974-983, 2018.
Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. CoRR,
abs/1710.11342, 2017.
D. Zugner, A. Akbarnejad, and S. Gunnemann. Adversarial Attacks on Neural Networks for Graph
Data. ArXiv e-prints, May 2018.
10