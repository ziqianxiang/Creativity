Under review as a conference paper at ICLR 2019
Understanding	GANs via Generalization
Analysis for Disconnected Support
Anonymous authors
Paper under double-blind review
Ab stract
This paper provides theoretical analysis of generative adversarial networks (GANs)
to explain its advantages over other standard methods of learning probability
measures. GANs learn a probability through observations, using the objective
function with a generator and a discriminator. While many empirical results
indicate that GANs can generate realistic samples, the reason for such successful
performance remains unelucidated. This paper focuses the situation where the
target probability measure satisfies the disconnected support property, which means
a separate support of a probability, and relates it with the advantage of GANs. It is
theoretically shown that, unlike other popular models, GANs do not suffer from
the decrease of generalization performance caused by the disconnected support
property. We rigorously quantify the generalization performance of GANs of a
given architecture, and compare it with the performance of the other models. Based
on the theory, we also provide a guideline for selecting deep network architecture
for GANs. We demonstrate some numerical examples which support our results.
1	Introduction
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) attract much attention as tech-
nology for learning a distribution and generating data. The purpose of GANs is to learn a probability
measure from a given dataset and generate samples from the learned measure. It is often seen that
samples generated by GANs can extract effectively features in the real world; it is difficult, for
instance, to distinguish real images and generated images. By practical successes, a countless number
of variations of GANs have been developed (Dziugaite et al., 2015; Arjovsky et al., 2017; Li et al.,
2015; Nowozin et al., 2016; Gulrajani et al., 2017; Zhao et al., 2016), and applied to a wide range of
tasks (Reed et al., 2016; Zhu et al., 2017; Gauthier, 2014).
Understanding the remarkable performance of GANs is, however, still a challenging problem. There
are active discussions on the role of generators in its learning scheme (Goodfellow, 2016; Arjovsky &
Bottou, 2017; Arora et al., 2018; Creswell et al., 2018), and adversarial structures with discriminators
are also a target of interest (Lotter et al., 2015; Zhang et al., 2018). A gaming structure between gen-
erators and discriminators is also regarded as a useful factor in the mechanism of GANs (Mescheder
et al., 2017; Arora et al., 2017; Heusel et al., 2017). Generalization performance of GANs has been
investigated in several studies (Liang, 2017; Liu et al., 2017; Tolstikhin et al., 2017). In spite of these
studies, it is not yet clear why GANs can generate well-extracted data better than other standard
methods.
This paper introduces the disconnected support property, and explains an advantage of GANs in
connection with this notion. The disconnected support property refers to a probability measures of
which the support is divided into several disjoint sets, allowing non-differentiable density on the
boundary. The property makes a probability measure be complex, hence it can be an obstacle for
standard methods to learn the measure effectively. This property, however, is popularly seen in many
real data, especially data with cluster structure, as demonstrated in Section 3.
We investigate in detail the approximation and estimation ability of GANs and some other methods,
and provide novel generalization analysis of probability measures with disconnected support. Firstly,
we show that the other methods suffer worse generalization performance due to complex structures
of disconnected supports (Proposition 1 and Lemma 2). Secondly, our generalization analysis reveals
that GANs can learn the probability measure without loss of efficiency under the the disconnected
1
Under review as a conference paper at ICLR 2019
supports (Theorem 1, Corollary 1 and ??). Additionally, we derive a guideline for choosing the
number of layers or connections of the generator and discriminator from the generalization analysis.
Numerical results support our theoretical findings.
We remark that the disconnected support property is different from the low-dimensional supports
studied in Arjovsky & Bottou (2017), where the support of a measure generated by neural networks is
disjoint to the measure of observations. In contrast, this paper considers the case in which the support
of the observation measure is divided into disjoint subsets. The problem of disconnected supports
is complement to the low-dimensionality, hence these two problems can be investigated separately.
In this paper, to simplify the discussion, we assume that the support of a probability measure is not
low-dimensional.
The contributions of this paper are summarized as follows:
1.	We show that GANs perform better than other standard methods of estimating probability
measures when the measure satisfies the disconnected support property.
2.	We provide a new generalization error bound under a general formulation of GANs by
analyzing an approximation error. The result is thus applicable to a wide range of variations
of GANs.
3.	Based on the generalization bound, we provide a theoretical guideline for selecting architec-
tures of generators and discriminators.
All the proofs are given in Supplementary materials.
2	Preliminaries
2.1	Notation
We use notations I:“ [0,1]. A j-th element of a vector b is denoted by bj, and }b}q :“(Xj bj)1{q
is the q-norm (q P [0,8]). vec(∙) is a vectorization operator for matrices. For Z P N, [z]:“
t1, 2, . . . , zu is a set of positive integers no more than z. For α P R, tαu denotes the largest integer
which is not larger than a. For a domain Ω in a Euclidean space and a function f : Ω → R,
}f∣∣Lp ：= (∖ω lf(t)lpdtq1{p denotes the Lp norm for P P [0, 8]. For f : Ω → RD with a multi-
dimensional output, fd denotes a d-th coordinate of f (x) = (fι(x),..., fD(χ))J. Let Hβ(Ω) be the
Holder space for β > 0 such as a set of β-smooth functions f : Ω → R, namely, f is Ctβu-class and
its tβU-th derivative is β — tβU-Holder continuous. b denotes a tensor product, and o a composition
of functions, namely, for functions f and f, f o f = f (f(∙)). A Borel σ-algebra of Ω is denoted
as σ(Ω). For a measurable mapping f : Ω → Ω1 and B1 U Ω1, a pre-image of f is defined as
f T(BI) ：= {t P Ω | B1 Q f(t)}. Let 1 ω : x → {0,1} be an indicator function such that 1 ω(x) = 1
if X P Ω, and 1 ω(x) = 0 otherwise.
2.2	General Framework of GANs
We provide a general formulation of a learning problem with generative adversarial networks (GANs)
following Liu et al. (2017). In this paper, we consider a probability measure P * on a measurable
space (ID, Σ) with a dimensionality D P N and Σ := σ(ID). Here, we set D23. Suppose we have
a set of n observations D := tX1, ..., Xn} which is independently and identically generated from
P *. Let Pn := * XiPrns δχi be an empirical measure where δχ is the Dirac measure at x.
The goal of generative networks is to estimate P * from D. To this end, we construct a probability
measure by generators. Let PZ be the uniform distribution on (ID, Σ). For a measurable mapping
g : ID → ID, we define Pg as the pushforward measure: i.e., Pg(B) = PZ(g´1(B)) for B P Σ. We
call g as a generator and use G for a set of generators.
GANs employ a learning scheme with a metric with discriminators (Goodfellow et al., 2014). Let
F = tf : ID → R} be a a set of discriminators. This paper considers a general metric for GANs
(Liu et al., 2017),
dF(P,P1) :=supEX„P[f(X)]—EX„P1[f(X)],	(1)
2
Under review as a conference paper at ICLR 2019
between probability measures P and P1.
For the learning process, we generate m
noise samples Z1, ..., Zm from PZ and obtain generated
1
samples as Xj :“ g(Zj) With g P G for j P [m]. Let Pg,m := * ∑jp[m] δχ^ denote the sampling
measure. GANS construct an estimator Pg for P * by learning g with the following optimization
problem
gp P argmin dF pPn , Pg,m) .
gPG
(2)
The metric (1) covers a wide variety of GANs by selecting F. Among others, the original GAN
(Goodfellow et al., 2014) is realized if F contains a logarithm of density ratio; Wasserstein-GAN
(Arjovsky et al., 2017), MMD-GAN (Dziugaite et al., 2015; Li et al., 2017) and Energy-Based GAN
(Zhao et al., 2016) are given if F is the set of 1-Lipschitz functions, a reproducing kernel Hilbert
space, and the bounded continuous functions, respectively. The f -GAN (Nowozin et al., 2016) also
belongs to this class.
We assume that F is large enough to contain functions which can work as a discriminator, namely,
we assume that the following holds:
dFpp,p 1) “ 0 O P = P1.	(3)
A sufficient condition for (3) is investigated in Zhang et al. (2018).
2.3	Deep Neural Networks for Generators and Discriminators
In the schemes of GANs, F and G are realized by deep neural networks (DNNs). For further
discussion, we formulate the function class given by DNNs.
Let L P N be a number of layers in DNNs, and D' P N be a dimensionality of variables in an '-th
layer for ` P rL ` 1s. Here, we set DL1 `1 “ D for generators and DL1 `1 “ 1 for discriminators. We
introduce a` P RD''1 X d' and b` P RD' as matrix and vector parametersof the '-th layer. An architec-
ture Θ of DNNs is defined as a set of L pairs of (Ag, b`) as Θ :“((A1,b1),…，(AL, bLq). We define
notations for Θ as follow: ∣Θ∣ :“ L as the number of layers, }Θ}o :“ 方德5〕} vec(Ag)}o ' }bg}o as
the number of non-zero elements in Θ, and }Θ}g :“ max{max'p[L] } vec(A')}g, max'p[L] }b'∣8}
be the scale of parameters in Θ. We employ the ReLU activation function η : RD1 → RD1 for each
D1 P N such as η(x) “ (maxtxd, 0u)dPrD1s .
We define functions of DNNs with an architecture Θ as ξ rΘs : RD1 → RD2 by
ξ[Θ](x) “ xpL'1q, xpιq :“ x, xp''ιq :“ η(Agxp'q ' b`), for ' P [L].
The function class of DNNs is thus given by
Ξ(S, B, L) = {ξ[Θ] : ID → R | }Θ}o ≤ S, }Θ}8 ≤ B, ∣Θ∣ ≤ L),
where S P N, B > 0, and L P N are hyper-parameters. Here, S bounds the number of non-zero
parameters of DNNs, namely, it controls the sparseness of DNNs. B is a bound for scales of
parameters.
3	Disconnected Support Property
3.1	Introduction and Example
It is often observed that data in real world data the support of its probability measure may not be
connected but a union of disjoint subsets. This is typical if the data has cluster structures, as seen in
many data sets for classification tasks. Moreover, the density function of the probability measure
may not be smooth at a boundary of the support. Figures 1 (MNIST, (LeCun et al., 1998)) and
2 (Shelter Animal, Center) illustrate such examples in the real world. They are projected onto a
2-dimensional Euclidean space by t-SNE (Maaten & Hinton, 2008) so that they preserve the original
distance structure among points. We can see that both of the data are concentrated on several disjoint
3
Under review as a conference paper at ICLR 2019
subsets and there are a clear gap or empty regions between some of the subsets. This observation
suggests that the disconnected property of probability measures should be addressed in discussing
estimation of probability measures, while standard analysis does not consider this phenomenon. In
fact, this paper will show that the disconnected supports property has an important role in showing an
advantage of GANs over standard estimation methods.
Figure 2: Plot of the animal data.
Figure 1: Plot of the MNIST data.
3.2	Mathematical Formulation of disconnected supports
Here we make a rigorous definition of disconnected supports. The property of smoothness (i.e.
differentiability) is involved, which is a key factor to analyze generalization performance in the fields
of the statistics (Stone, 1982; Tsybakov, 2009); Stone (1982) shows, for instance, that smoothness
and a dimension of data are sufficient to characterize an optimal convergence of generalization errors.
We first prepare a family of subsets as a component in the disconnected supports:
Sα,J :“ {S U ID | A boundary of S is J combination of α-smooth hyper surfaces}.
The supplementary material will provide a more rigorous definition.
Now, we define the disconnected support property of probability measures as well as a probability
measure with global support, i.e., with no the disconnected support property. Let SupppP q be
the support of P., i.e, SUpp(P) :“ {x P ID | P(Vx) > 0 for all open neighborhood Vx of x}.
Hereafter, M ≥ 2 is the number of disjoint components of a support.
Definition 1. (Disconnected Supports / Global Support)
Let M ≥ 2. A probability measure P on (ID, Σ) has M disconnected supports, if there exist
nonempty disjoint sets S1, ..., SM P Sα,J such that
SUpp(P)“ U Sm.
mPrM s
A probability measure P on (ID, Σ) has a global support, if SUpp(P ) “ ID.
Figure 3 illustrates the disconnected support property.
We next formulate a notion of smoothness for P with disconnected supports. Let β ≥ 1 be a
parameter for a degree of smoothness of P .
Definition 2. (Local Smoothness)
A probability measure P with M disconnected support is locally β-smooth, if there exist M pairs
(Sm, Sm) P S2β,J ^ S2β,J and β ' 1-smooth bijective measurable maps Ym : Sm → Sm as
P (B)“ PZ (Ym1 (B)), VB P σ(Sm),
for m P rMs.
This definition of local smoothness says that a probability measure P with disconnected supports
can be generated by sufficiently smooth mappings Ym . It is used for considering a smooth density
function of P restricted on Sm .
4
Under review as a conference paper at ICLR 2019
Lemma 1. (Locally Smooth Density Functions)
If a probability measure P with M disconnected support is locally β-smooth, then there exists a
function Pm : Sm → r` such that
PpBq “
B
pm pxqdλ, B P σpSmq,
where λ is the Lebesgue measure, and pm is β-smooth for all m P rMs.
We call pm as a local density function.
Note that, since P with disconnected supports is not absolutely continuous to the Lebesgue measure
on IDZ UmPrMS Sm, an ordinary density function cannot be defined. Instead, a localized version of
density functions for each Sm is introduced, which is guaranteed by the local smoothness.
Figure 3: Illustration of a probability measure Figure 4: Illustration of a generator g. To
P with a disconnected support. SupppP q is a represent discontinuous S1 and S2, g should
union of two disjoint sets S1 and S2.	be discontinuous.
3.3 Difficulty with Disconnected Supports
As shown in this subsection, the generalization performance of many standard estimation methods
is worsened with disconnected supports. We consider popular nonparametric methods, for which
the generalization performance is well-studied in the asymptotics of the observation size n. The
considered methods are the kernel density estimator (KDE) (Nadaraya, 1964), the nonparametric
Bayes (NB) by the Dirichlet mixtures of normal distributions (Ferguson, 1973), the series density
estimator (SDE) (Efromovich et al., 2008; Efromovich, 2010) and the density estimator with Gaussian
process (GP) (Leonard, 1978). Bounds of the generalization errors for these methods are already
known (Tsybakov, 2009; Ghosal et al., 2007; van der Vaart & van Zanten, 2008), and they are optimal
in the minimax sense. Here, their performance is evaluated with respect to a root of an expected
squared loss with respect to the L2-norm, namely, d2 pP, P1q :“ Er}p ´ p1}2L2s1{2 where p and p1 are
densities for P and P1.
We show the deterioration of their performance by the disconnected support property. Let P be an
estimator for P * by KDE, NB, SDE, or GP. If P * has a global support and a β-smooth density, the
existing studies (Tsybakov, 2009; Ghosal et al., 2007; van der Vaart & van Zanten, 2008) show that
d2(P*,P) “ O (n-β/(2β'D)).
These bounds are sufficiently tight, since these bounds corresponds to an optimal rate (Stone, 1982),
and the performance of the methods can be improved as the density for P* is smoother (larger β).
On the other hand, we consider a case in which P* has the disconnected support property.
Proposition 1. (Deterioration of other standard methods)
There exists P * of the disconnected support property and locally β-smooth such that
d2PP*,Pq = O (nT∕(2'D)).
5
Under review as a conference paper at ICLR 2019
When P * has disconnected supports, the errors are worse than those for the global support, indepen-
dent of β . This worse generalization error can be understood by the non-smoothness or discontinuity
of the density functions on the boundaries of the disconnected sets (see Figure 3).
We next discuss other generative models for estimating a probability measure. To the best of our
knowledge, other probabilistic generative methods (Koller et al., 2009) and the variational auto-
encoder (Kingma & Welling, 2013), their statistical generalization property is not well investigated.
Here we provide a property of generators for probability measures with disconnected supports.
Lemma 2. (Discontinuous Generators for Disconnected Supports)
If P * has disconnected supports and P * “ Pg* with a generator g*, then g* is not uniformly
continuous.
Lemma 2 states that a generator must be discontinuous to construct a probability measure with
disconnected support sets. Intuitively, to make Pg* (B) “ 0 for B P ID with λ(B) > 0, the slope of
g* at Z P ID should be close to infinite for Z P g*,—1 (B), hence g* cannot be uniformly continuous
(see Figure 4). Because of the discontinuity, generative models with smooth functions, such as an
adversarial generative model with kernel generators (Sinn & Rawat, 2018), cannot work well with
disconnected supports.
4 Generalization by GANs
We provide generalization analysis for GANs for probability measures with and without disconnected
supports. For the purpose, we employ a metric dF with properly selected discriminators F and
evaluate the generalization error dF(P*, Pgp) with respect to an observation size n and a sampling
♦	ττ τ	-r~ ♦	1 ∙ ιι i ʌ ʌ τ-⅛ τ	-r-	I / r> I > ι ∖	-τ^∙ ∙ ,ι	,	ri τ-ι τ
size m. We assume F is realized by DNNs as F “ Ξ(Sf, Bf, Lf) X F with parameters Sf, Bf, Lf,
where F is a specified functional class; for an example, F is 1-Lipschitz functions for Wasserstein-
GAN. Here, we consider settings that all f P F are LI-LiPSChitz continuous and }f }l8 ≤ BF
with constants Li, BF > 0.Generators are also constructed by DNNs as G “ Ξ(Sg, Bg, Lg) with
parameters Sg , Bg , Lg .
A standard line of discussing generalization, we should consider statistical errors and approximation
errors. We define a measure of the complexity of F
Yn(F) :“ inf 4η + 12nT/2「logN((Lι + l)´1 e, F,} ∙ }n)1{2de,
η>0	Jn
*

where C > 0 is a constant depends on F and N(e, F, }∙}) is a covering number of F with respect to
an empirical norm }∙}. We note that Yn(F) bounds an expectation of the Rademacher complexity as
Yn(F) > E sup1 £ Tif (Xi)
fPF n iP⅛
where τi is the i.i.d. Rademacher random variables; Pr(τi “ 1) “ Pr(τi “ 1) “ 1{2, and the
expectation is about Xi and τi . Using the statistics and learning theory van der Vaart & Wellner
(1996); Bartlett et al. (2005), we can apply a bound for Yn (F) as
Yn(F) ≤ CF…,
with some constants CF > 0 and K22.
Regarding approximation errors, we need to consider approximation of a discontinuous function;
since Lemma 2 shows that a discontinuous generator is necessary to represent disconnected supports.
To approximate such generators, DNNs in GANs has an advantage.
Lemma 3. (Approximation for Discontinuous g by DNNs)
Suppose P* has M disconnected supports and locally β-smooth, and also P * “ Pg* holds with
some g*. Then,, for any Sg, there exist G, g P G, and a COnStant Cg “ Cg (Bg, Lg) > 0 such that
}gd ´ g*∣L2 ≤ CgMSgβD, Nd P [D].	(4)
Furthermore, ifP* “ Pg* has a global support and it is β-smooth, (4) holds with M “ 1.
6
Under review as a conference paper at ICLR 2019
Lemma 3 shows that G for GANs can approximate g* for disconnected supports with the rate (一β/D)
by Sg , and the rate is same in the case of global support. This implies an advantage of GANs in
comparison with the other standard methods (Proposition 1).
Based on Lemma 3, we obtain the main theorem for generalization analysis.
Theorem 1. (Generalization of GANs)
Suppose that P * has M disconnected supports and locally β-smooth, and we have n observations
and m samplings. Then, with F, an existing G, an estimator Pgp by (2), and finite constants c1 “
c1pLf,Bf,Lg,Bgq,c2,c3 “ c3(Lf, Bf q > 0, the following inequality holds with high probability,
dF(P*,Ppq ≤ Ym(F) + ci FS pZZSf
+ IMD S-oi∣θ + Yn(F) + C3< Snf
(5)
"V
"V
:I
Furthermore, ifP* has a global support and it is β-smooth, (5) holds with M “ 1.
Each of the terms I , II and III has the following role: I bounds an error by the m samplings, II
bounds an error from approximation by G, and III bounds an error by n observations.
Proof Outline: By the definition of Pgp in (2) and standard calculation, we obtain the inequality
dF(P*,Pg)W 2supsup ∣Epg,mrf (X)S ´ EPgrf(X)S∣ + inf &F(Pg, P*) + 2dF(Pn, Po).
gPG	loooooooooooon
gPG fPF
"V
"V
"V
:iii
:i
:ii
To obtain i ≤ I and iii ≤ III, We apply an empirical process technique (van der Vaart & Wellner,
1996), especially convergence of integral probability measures (Sriperumbudur et al., 2012) and the
entropy control technique (Lemma 4 and 5 in the supplementary material). To show ii ≤ II, we
employ recent results on approximation ability of DNNs (Yarotsky, 2017; Petersen & Voigtlaender,
2017; Imaizumi & Fukumizu, 2018) and obtain an approximation bound for dF(Pg, P*) (Lemma 3).
Combining these results, we obtain the statement of Theorem 1.	□
Theorem 1 provides two trade-off relations with respect to Sg and Sf. The generator class G controls
uncertainty by sampling and the approximation error, while Sf controls uncertainty of observations
and discrimination. For balancing the trade-offs, we select the number of parameters (connections of
DNNs) with some constants Cg ,cf > 0 as
Sg “ CgmD/(2e'D), and Sf “。"(”2)，
(6)
for optimizing the bound (5). We then obtain the following corollary.
Corollary 1. (Convergence Rate of GANs)
Make the same assumptions as Theorem 1, and set Sf and Sg as in (6). Then, with high probability
converging to 1, we obtain
dF(P*,Pg) “ O (n-1/κ) + O (m-1/κ + m-β{p2β'Dq).
(7)
A selection of F determines the first term in (7), since κ depends on F. For an example, when F is a
set of 1-Lipschitz functions, the first term is O (n-1∕(2+2D)) (Sriperumbudur et al., 2012).
Remark 1. (Heterogeneous Smoothness)
Corollary 1 can be extended when P* has different smoothness for each m P rMs, i.e., P* is locally
βm -smooth on a set Sm . In this case, we can easily extend our analysis in Theorem 1 and Corollary
1, and obtain the following convergence rate.
dF(P*,Pp) “ O (n-1/κ) + O (m-1/κ + m-r{(2r'D)),
where β :“ minmPrMs βm.
5	Discussion
We emphasize the theoretical results show that GANs do not suffer from the effect of disconnected
supports. A larger β improves performance of GANs even with disconnected supports, as shown
7
Under review as a conference paper at ICLR 2019
in Theorem 1 and Corollary 1. This phenomenon is different from the result of the other methods
discussed in Proposition 1. Hence, we can state that GANs have advantages over the other methods
which are deteriorated by the disconnected property (Section 3.3). In other words, when data are
generated from a probability measure with disconnected supports and sufficiently smooth in each
of the sets, only GANs can estimate the measure effectively and the other methdos cannot. This
advantage of GANs comes from the approximation power for discontinuous generators shown in
Lemma 3.
The results (5) and (7) provide interpretation about performance of GANs. About convergence with n,
the complexity of F and Sf control a trade-off between convergence and a power of discrimination.
While smaller Sf reduce the errors in terms of dF , too small Sf can lose the power of discrimination
to satisfy (1). Hence, setting Sf as in (6) can keep the discrimination power and does not worsen
the overall rate of convergence O(n—1{K). About convergence with m, Sg controls the trade-off
between the bias and variance of the estimator. An optimal way to select Sg is provided in (6) which
depends on β and D, and it is more important when κ is small (e.g. κ “ 2 as MMD-GAN). Based
on the interpretation and the selection rule (6), our study can provide a guideline for a design of the
architecture of DNNs.
5.1	Related Works
Compared with studies for understanding GANs (Dziugaite et al., 2015; Liang, 2017; Liu et al., 2017;
Zhang et al., 2018; Biau et al., 2018; Arora et al., 2017), we show that GANs can avoid influences
of disconnected supports, unlike the other methods, hence it is a source of the advantage of GANs.
This paper is the first work to focus on the disconnected support property, while several discussions
(Goodfellow, 2016; Liang, 2017; Creswell et al., 2018) focus on models and metrics of the learning
scheme of GANs.
It is important to compare our result with other studies for generalization analysis. Although some
existing studies (Dziugaite et al., 2015; Liang, 2017; Liu et al., 2017; Zhang et al., 2018; Biau et al.,
2018; Arora et al., 2017) provide generalization analysis, they do not analyze an approximation effect,
namely, they evaluate dF(P*,Pg) — infgp0(P*,Pg). Since We analyze the term infgpG(P*,Pg),
we can provide a more general bound and discuss the effect of disconnected support.
As we mentioned in the introduction, this paper is not along with studies for low-dimensional supports
(Arjovsky & Bottou, 2017). We also note that an optimization aspect and gaming aspect of GANs are
out of concerns of this paper. We focus on the statistical aspects of GANs such as sample complexity.
6	Numerical Experiments
We compare the numerical performance of GANs and the other methods with toy data with. We
generate synthetic data from the following two settings: (A) Gaussian distribution restricted on a
compact set (global support), and (B) a probability measure with two disconnected supports (density
function is the black solid line in Figure 6). We generate n “ 500, 1000, ..., 5000 observations and
estimate the true probability measures with Wasserstein GAN, MMD-GAN, KDE (the Gaussian
kernel and the Epanechnikov kernel), SDE (Fourier basis), and NB (Dirichlet process prior). Hyper-
parameters for these methods are selected by cross-validation. For GANs, we set m “ n. We use dF
to evaluate errors by GANs, and a root of the expected squared errors with the L2-norm for the other
methods. The plots are the mean of 30 replications.
Figure 5 shows generalization errors by the methods. With the disconnected support case (B), we
plot the estimated density in Figure 6. The black line shows the true density, the dashed line is
by estimated densities of the other methods, and bars are histograms by GANs. The results by
Wasserstein-GAN and MMD-GAN are almost same, so we omit the result by Wasserstein-GAN.
In Figure 5, we see that in the case of global support (A), the error of GANs and the other methods are
comparable. In contrast, in the case of disconnected supports (B), the other standard methods show
worse generalization and only GANs keep the high performance. From Figure 6, we can see that
GANs can reveal the disconnected supports, while some of the other methods fail to fit. KDE with
the Gaussian kernel represents the disconnected support by employing a small bandwidth. However,
the small bandwidth yields a too sharp density, tending to worsen the generalization performance.
8
Under review as a conference paper at ICLR 2019
Figure 5: Generalization errors.
Figure 6: Estimated density func-
tions with the case (B).
7	Conclusion
We investigate a generalization performance of GANs with a situation such that a support of real
probability measures is divided into several sets. We find that GANs do not suffer from the division
of supports, while some of the other nonparametric methods loss their efficiency by the division.
Since real data are often distributed on such divided supports, the finding in this paper is related to
the question of why GANs perform well with real datasets.
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and
empirics. 2018.
Peter L Bartlett, Olivier Bousquet, Shahar Mendelson, et al. Local rademacher complexities. The
Annals ofStatistics, 33(4):1497-1537, 2005.
G Biau, B Cadre, M Sangnier, and U Tanielian. Some theoretical properties of gans. arXiv preprint
arXiv:1803.07819, 2018.
Austin Animal Center. Shelter animal outcomes. http://www.austintexas.gov/
department/aac.
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A
Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1):
53-65, 2018.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural net-
works via maximum mean discrepancy optimization. In Proceedings of the Thirty-First Conference
on Uncertainty in Artificial Intelligence, pp. 258-267. AUAI Press, 2015.
Sam Efromovich. Orthogonal series density estimation. Wiley Interdisciplinary Reviews: Computa-
tional Statistics, 2(4):467-476, 2010.
9
Under review as a conference paper at ICLR 2019
Sam Efromovich et al. Adaptive estimation of and oracle inequalities for probability densities and
characteristic functions. TheAnnalsofStatistics, 36(3):1127-1155, 2008.
Thomas S Ferguson. A bayesian analysis of some nonparametric problems. The annals of statistics,
pp. 209-230, 1973.
Jon Gauthier. Conditional generative adversarial nets for convolutional face generation. Class Project
for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester,
2014(5):2, 2014.
Subhashis Ghosal, Aad Van Der Vaart, et al. Posterior convergence rates of dirichlet mixtures at
smooth densities. The Annals of Statistics, 35(2):697-723, 2007.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint
arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5769-5779, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pp. 6629-6640, 2017.
Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effectively.
arXiv preprint arXiv:1802.04474, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Daphne Koller, Nir Friedman, and Francis Bach. Probabilistic graphical models: principles and
techniques. MIT press, 2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Tom Leonard. Density estimation, stochastic processes and prior information. Journal of the Royal
Statistical Society. Series B (Methodological), pp. 113-146, 1978.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabds P6czos. Mmd gan:
Towards deeper understanding of moment matching network. In Advances in Neural Information
Processing Systems, pp. 2200-2210, 2017.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International
Conference on Machine Learning, pp. 1718-1727, 2015.
Tengyuan Liang. How well can generative adversarial networks (gan) learn densities: A nonparametric
view. arXiv preprint arXiv:1712.08244, 2017.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties
of generative adversarial learning. In Advances in Neural Information Processing Systems, pp.
5551-5559, 2017.
William Lotter, Gabriel Kreiman, and David Cox. Unsupervised learning of visual structure using
predictive generative networks. arXiv preprint arXiv:1511.06380, 2015.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Pascal Massart. Some applications of concentration inequalities to statistics. In Annales-Faculte des
Sciences Toulouse Mathematiques, volume 9, pp. 245-303. Universite Paul Sabatier, 2000.
10
Under review as a conference paper at ICLR 2019
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in
Neural Information Processing Systems ,pp.1823-1833, 2017.
James R Munkres. Topology. Prentice Hall, 2000.
Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1):
141-142, 1964.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Systems,
pp. 271-279, 2016.
Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions using
deep relu neural networks. arXiv preprint arXiv:1709.05289, 2017.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. arXiv preprint arXiv:1708.06633, 2017.
Mathieu Sinn and Ambrish Rawat. Non-parametric estimation of jensen-shannon divergence in
generative adversarial network training. In International Conference on Artificial Intelligence and
Statistics, pp. 642-651, 2018.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, Gert RG Lanckriet,
et al. On the empirical estimation of integral probability metrics. Electronic Journal of Statistics,
6:1550-1599, 2012.
Elias M Stein. Singular integrals and differentiability properties of functions (PMS-30), volume 30.
Princeton university press, 2016.
Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business
Media, 2008.
CJ Stone. Optimal global rates of convergence for nonparametric regression. The Annals of Statistics,
10:1040-1053, 1982.
Ilya O Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard
Scholkopf. Adagan: Boosting generative models. In Advances in Neural Information Processing
Systems, pp. 5430-5439, 2017.
Alexandre B Tsybakov. Introduction to nonparametric estimation, 2009.
AW van der Vaart and JH van Zanten. Rates of contraction of posterior distributions based on gaussian
process priors. The Annals of Statistics, 36(3):1435-1463, 2008.
AW van der Vaart and Jon Wellner. Weak Convergence and Empirical Processes: With Applications
to Statistics. Springer Science & Business Media, 1996.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-
generalization tradeoff in gans. Proceedings of International Conference on Learning Representa-
tions, 2018.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv
preprint arXiv:1609.03126, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.
11
Under review as a conference paper at ICLR 2019
Supplementary Materials for
“Understanding GANs via disconnected Support Detection”
We introduce a new notation Pf :“ EX„P rfpXqs with a probability measure P and a function
f. For a set Ω with equipped distance d, let N(g Ω, d) be a covering number which is a minimum
number of e-balls to cover Ω.
A Some Additional Information
A Rigorous Definition of Sα,J
We consider a set represented by a combination of multiple horizon functions, which has been used
in Petersen & Voigtlaender (2017). Given ɑ-smooth function h P Hα(IDT) with a21, a horizon
function Ψh : ID → t0, 1u is defined for some d P rDs as
Ψh “ Ψ(xi, . . . ,Xd—1,Xd + h(xι, . . . ,Xd—i,Xd'i, . . . ,XD),Xd'i,…，XD),
where Ψ is the Heaviside function; Ψ(x) “ ItxPID |xd?。}. We define a set by the intersection of J
horizon functions Ψh1 , ..., ΨhJ; namely the family of sets is defined by
Sα,J := { S U [0, 1]D I 1S “ Ψhι b∙∙∙b Ψhj(.
Intuitively, h is regarded as an α-smooth curved surface in ID, and Ψh describes a set which is one
side of the surface. Also, S P Sα,J is a set which is a intersection of J sets by Ψh1 , ..., ΨhJ.
A Support of Probability Measures
Let Nx denote an open neighborhood of x P ID, and For a probability measure P, a support of P is
defined as
SUpp(Pq :“ {x P IDI P(Nx) > 0, @Nx P ∑).
B Proofs
B.1 Proof of Lemma 1
τ-,∙	「71 C	1	1∙	Py Γ1	Λ	1 -	I >	/∕∙>∖,Ile	C	∙ 1 -I
Fix m P rMs and a corresponding Sm, Sm and gm. For any B P σ(Sm), the definition of γm yields
PX ⑻=PZ (Ym1(B)) = f	PZ (Zqdz,
JYmIpB)
where PZ is a density function of a uniform measure PZ . By changing variables x = γm (z), we have
LBqPZ (Zqdz = JB PZ (γm1(χ)) JYm (Xqdx,
where JYm (Xq = ∣ det Vgm1 (x)∣. Using PZ(z) = 1 for all Z P ID, we obtain the following form of
Pχ (B) using a function Pm : Sm → Sm as
PX(Bq “	Jgm (xqdx “:	pm(xqdx,
and pm is β-smooth since γm is β ` 1-smooth and bijective.
(8)
□
B.2 Proof of Lemma 2
Firstly, we show the first points. Suppose that g is a continuous mapping. By the generalized
intermediate value theorem (Theorem 24.3 in Munkres (2000)), we know that gpIDq is connected
since ID is a connected set. Thus, a support of Pg is connected. However, Pg has a disconnected
support, thus there is a contradiction.
□
12
Under review as a conference paper at ICLR 2019
B.3 Proof of Lemma 3
In this proof, a < b denotes that b is larger than a up to a finite constant. a - b denotes that a < b
and a > b hold.
By the definition of tgmumPrMs for the measure with local smoothness, we consider an explicit form
of gm. By Stein (2016), we can extend gm : Sm → Sm to rm : ID → Sm since boundaries of Sm
are Lipschitz continuous. Then, we provide the following formulation
grmpxq “ pγm,1pxq, ...,γm,DpxqqJ,
where γm,d P Hβ PID). Then, we obtain the form of g* as
g*“ ∑ rm b1 Sm.
mPrM s
Also, by the definition of S2β,J which contains Sm, we obtain the form
1 r “ (^) ψh .,
Sm	hm,j ,
jPrJs
with existing ψhm,j . Then, we have
g* “ Σ rm 0 ψhm,j.
mPrMs	jPrJs
Preliminarily, we apply sub-neural networks from Yarotsky (2017) and Petersen & Voigtlaender
(2017). Let Z[θ`s be a network for summation such that Z[θ'](xι,…，xDι) “ XdPrDIs xd, and
Z[Θ^S be a network for approximate multiplication such as [Z[Θ^](xι,…，xDι) 一 ∏dp[D∕s xd| < E
with some E > 0 for all x, x1 P I (Proposition 3 in Yarotsky (2017) and Lemma 1 in Imaizumi &
Fukumizu (2018)).
We consider approximation χmp[Ms Ym,d BjPrJs Ψhm,j for each d P [D]. Let Z[Θγ,d,m] and
Z rΘh,m,j s for d P rDs, m P rMs and j P rJs, and we will specify the networks later. Also, let
ZrθS,ms = Z[θ^s(zrθh,m,1sp∙q, ∙∙∙,zrθh,m,1s(∙)).
We consider a neural network
Zrθds = Z2+s(Z23(2Y,d,1s(・),ZrθS,1sp∙qq, ...,Z[㊀川(rθγ,d,MMXrθS,M](•))).
Then, an approximation error is evaluated as
γm,d	ψhm,j ´ ZrΘds
›mPrMs	jPrJs	›L2
≤ X	Ym,d 0 ψhm,j ´ Z[θ^s(z2Y,d,ms(・),ZrθS,ms(∙))
mPrMs ›	jPrJs	›L2
& X	γm,d 0 ψhm,j ´ Zrθγ,d,ms b ZrθS,ms
mPrM s ›	jPrJs	›L2
+ X	Z [Θγ,d,mS b Z [Θs,m s´ Z [Θ^](Z [Θγ,d,mS(∙), Z [θs,m](∙))
mPrM s	L2
≤ X >Ym,d b 1 Sm ´ Z [Θγ,d,m Sb Z [θs,m]> J Me^
mPrM s
≤ X	>Ym,d b	p 1 Sm	´	ZrθS,mS)	+	(Ym,d	´	Zrθγ,d,msq b	ZrθS,ms ||	2 + ME^
mPrM s
W X }γm,d} L8 >1 Sm ´ Z rθS,ms>	+ }γm,d ´ Z rθγ,d,ms} 工2 }Z rθS,ms} 工8 + ME ^
mPrM s
“：X T1,m + T2,m + ME^,
mPrM s
13
Under review as a conference paper at ICLR 2019
where the last inequality follows the Holder,s inequality.
About T1,m, there exists a corresponding ζrΘγ,d,ms such that
}Ym,d ´ Z[Θγ,d,m]}L2 W }Θγ,d,m∣Lβ'WD
by following Theorem A.8 in Petersen & Voigtlaender (2017). Also, since γm,d is bounded by its
smoothness and compact support, we have }γm,d∣∣L8 V 8 hence
Tl,m < }Θγ,d,m }]pβ'iq/D
For T2,m, we specify ζrΘh,m,js as Theorem 3.1 in Petersen & Voigtlaender (2017). Then, we evaluate
the following as
}γm,d ´ ζ rΘγ,d,m s}L2
≤	(×) ψhm,j ´ Zrθ^spζrθh,m,1s(∙), …,Zrθh,m,1s(∙))
›jPrJs
≤	(×) ψhm,j ´ (×) Z[Θh,m,j]	+ (X) ψhm,j ´ Z[Θ^](Z[Θh,m,ιS(∙), ..., Z[Θh,m,ιS(∙))
›jPrJs	jPrJs	›L2	›jPrJs
< X 口	Qψhm,ji>L2 _ }Zrθh,m,j1 }L2) >ψhm,j ´ Zrθh,m,js>L2 + EX
jPrJs j1PrJsztju	L
L2
& X >ψhm,j ´ zrθh,m,js>L2 + e^ .
jPrJs
Here, the last inequality follows the boundedness of ψhm,j1 and Z rΘh,m,j s by Theorem 3.1 in Petersen
& Voigtlaender (2017). Also, Theorem 3.1 in Petersen & Voigtlaender (2017) provides an existence
of rΘh,m,j such that
}ψhm,j ´ Zrθh,m,jS}L2 ≤ }Θh,m,j }Iβ/(DT).
We apply the boundedness of Z rΘh,m,j s, we have
T2,m < X }Θh,m,j }Iβ/(DT)+ E^ .
jPrJs
Combining the bounds for T1,m and T2,m, we bound
γm,d	ψhm,j ´ ZrΘds
›mPrMs	jPrJs	›L2
& X }Θγ,d,mLβ'1)/D+ X X }Θh,m,j }*DTq + (M + 1)e^ .
mPrMs	mPrMs jPrJs
Here, we consider a parameter S “ XdPrDs }Θd}ι such that S - }Θγ,d,m}ι — }Θh,m,j }ι — }Θ^}1.
Also, following Yarotsky (2017) and Petersen & Voigtlaender (2017), a proper selection of L “ ∣Θ^ |
and B “ }Θ^ ∣∣8 provides e^ < }Θ^ }0^β/D. Then, we have
X Ym,d 区 ψhm,j ´ Z [Θd]	< MJSf/D
›mPrMs	jPrJs	›L2
Since J is finite, we obtain the result.	□
B.4 Proof of Theorem 1
By the definition of gp in (2), the following inequality holds
dF(Pn,Pg,m)= SUp(Pnf - Pg,mfq ≤ dF(Pn,Pg,m)= SUp(Pnf - Pg,mfq,	(9)
fPF	fPF
14
Under review as a conference paper at ICLR 2019
for arbitrary g P G .
We consider a bound for "f(P*, Pgqas
dF(p*,Pg`q = sup(p*f ´ Pgf q
fPF
“ sup(P* f ´ Pnf ` Pnf ´ Pgp,mf ` Pgp,mf ´ Pgpfq
fPF
≤ SuP(P*f ´ Pnf ' Pg,mf ´ Pgf q ' SuP(Pnf ´ Pg,mf q,
fPF	fPF
where the inequality follows (9) with an existing g9 P G . We will provide a detailed construction of
g* . We continue the bound as
dF (P *, Pgpq
≤ sup(P*f ´ Pnf ' Pg,mf ´ Pgf)+ SuP(Pnf ´ P* f ' P*f ´ Pgf ' Pg f ´ Pg,mf q
fPF	fPF
≤ 2 sup sup ∣Pg,mf — Pgf | + SuP(P*f — Pgf) + 2 sup |Pnf — P*f |
gPG fPF	fPF	fPF
“: i + ii + iii.
Here, i denotes an effect from m samplings, ii denotes an approximation error, and iii denotes an
uncertainty with the n observations.
To evaluate i and iii, we provide the following lemma. This result follows a standard technique of
the empirical process theory and we provide its outline for a sake of completeness.
Lemma 4. Let H be a some set of measurable functions andX1, ..., Xn „ P be i.i.d. n observations.
Suppose that EP [h2(X )S ≤ σ2 and }h}L8 V Ch hold with finite parameters σ2 > 0 and Ch > 0.
Then, there exists a constant Cθ and we obtain
SuP 1 y h(Xi) ´ EP[h(X)s
hPH n 品
W ɪnf ∣4η+12 IhCo Ng H,}∙}L8q de +cτ≡4
with probability at least 1 — 2 exP(一τq for all T > 0.
Proof. At the beginning, we bound an expectation of the empirical process (Steinwart & Christmann,
2008; Bartlett et al., 2005; Massart, 2000; Sriperumbudur et al., 2012). Afterward, we evaluate a
concentration of the empirical process around the expectation.
By applying the symmetrization and concentration techniques (Proposition 7.10 in Steinwart &
Christmann (2008)), we obtain
EP bn SuP — £ h(Xiq — Erh(Xqs
hPH n 品
W 2EPbnbνbn SuP
hPH
1MSuih(Xiq
where ui „ ν is the Rademacher variable which takes 0 or1 with probability 0.5. Combining this
bound with the Taralgand’s inequality (Theorem A.9.1 in Steinwart & Christmann (2008)), we obtain
the following inequality
SuP
hPH
—£ h(Xiq ´ EP rh(XqS
n iPrns
W (1 + θqEP bnbνbn SuP
hPH
—£ Uih(Xiq
n iPrns
+产+
n
2 + θ),	(10)
3θ
with probability at least 1 — exP(—τq for all T > 0 and θ > 0.
15
Under review as a conference paper at ICLR 2019
About the term with the Rademacher variable, we also apply a similar strategy (Lemma A.4 in Bartlett
et al. (2005)), then obtain
sup 1 Y Uih(Xi)
hpH n 扁
≤ 1	百 EVbn
sup 1 Y Uih(Xi)
hpH n 品
T 1Ch
nθ1(1 ´ θ1),
(11)
with probability at least 1 — exp(一τ1) for all T1 > 0 and θ1 > 0.
Let } ∙ }n be an empirical norm as }f }^	“ n´1 XiPlns f (Xi)2.	About the term
EVbn SuPhPH In XiPrns Uih(Xi)I , We apply the chaining technique and obtain
1Y Uih(Xi)	W㈣[4θ2`ι2[hClog","∙"nqdi
iPlns	I
Eνbn sup
hPH
W5〃+12「ClogNgH,}∙}L8qd	(12)
where the last inequality follows a bound for an empirical norm and the boundedness ofH.
Combining (10), (11) and (12) and changing variables, we obtain the result.	□
To bound I with X1, ..., Xm
„ Pg , we consider the following value
SuP SuP — Y f (Xi) ´ EPg [h(X)S = SuP SuP - Y f。g(Zi) ´ EPZrf。g(X)S
gPGfPFIImiPlms	II	gPGfPFIImiPlms
=: SuP
hPH
Y h(Zi) ´ EPZrh(X)s
iPlms
where we define H = th = f 。 g | f P F, g P Gu. To apply Lemma 4, we investigate a covering
numberN(e, H,} ∙匠8).
Lemma 5. Assume that f P F is L1 -Lipschitz continuous. We obtain
log N(e, H, } , }L8) W log N((I ' L1 厂% G, } Tl L8) ' log N((I ' L1 )´j,F, } ∙ }L8)
Proof. Fix E > 0. Let G U G and F U F be covering sets as a set of centers of e-balls for the
covering G and F. Obviously, |G| = N(e, G,} ∙ }l8) and |F| = N(e, F,} ∙ }l8). We define a subset
H := {h = f。g | g P G,h P f} U H,
and we known |H| = |G| X |F|.
For any h P H, there exist g P G and f P F, then f = f 。 g holds. Also, by the definition of covering
sets, there exist f1 P F and g1 P G such that }f — f1 }L8 W e and }g — g1 }L8 W e. Let h1 = f1 。 g1,
and we measure the distance
}h — h1 }L8 W }f 。 g — f1 。 g1 }L8
“}f。g — f。g1 ' f。g1 — f1。g1}L8
W }f 。 g — f 。 g1}L8 ` }f 。 g1 — f1 。 g1}L8
W L1 }g — g1 }L8 ` }f — f1 }L8
W (L1 ` 1)e.
Here, the third inequality follows the Lipschitz property of f P F. Here, we know that H is covered
by (Li ' 1)e-balls with the center H. Since |H| = |G| X |F|, the result holds.
16
Under review as a conference paper at ICLR 2019
Now, we have the following entropy bound
logNPg H, } ∙ }L8q
≤ log N pp1 ' LIq 1e, G, } , }L8 q ' log N pp1 ' LIq 1e, F, } , }L8 q
≤ PSg + ιq log(2(Lι + 1q^´1PLg + DDg Bg q
+ min∣PSf + 1q log(2PLI + !ɔe´1(Lf + 1qDf Bfq, Cκ(1 + LIq1")
with Dg ：= Π'P[Lg'i](D' + iq and Df= 0俎力,`i] (D' + D Let Γf :“ 2(Lf + 1)Df Bf ,and
NePF) := logN(e, F, }∙}n) for brevity. Here，We apply Lemma 8 In Schmidt-Hieber (2017) for the
entropy bound for G and F. Using the entropy bound and Lemma 4, we obtain
2 …´ P*
+
(2τσ2 + Cθ q1{2 + τCh (2/3 + Cθ q
n1{2	n
≤ 4η + n122 JCF Ne(Fq 1/2de + n122 (Sf + iq1{2(log1{2 Γf + CF log1{2 CF ´ η log1{2 哈
ι (2τσ2 + Cθ q1{2 + τCh (2/3 + Cθq
n1{2	n
≤ Yn(Fq + n'{2 ((Sf + 1q1{2A1 + A2) + ~n,	(13)
with some η > 0, Ai = 12log1{2 Γf, A2 = CF log1{2 CF + (2τσ2 + Cθq1{2, and A3 = τC% (2/3 +
Cθq. Also, we set YnPFq = 4η + 吊 * NpLi'i)-ie(Fq1/2du Then, we have
iii ≤ Yn(Fq + n]/? ((Sf + iq1{2AI + A2)+ -n.	(14)
About I, we define Γg := 2(Lf + 1qDgBg and obtain a similar bound as
i & Ym(Fq + mi/? ((Sg + 1q1/2Aι + A2) + m3,,
where A11 = 12(log1{2 Γf + log1{2 Γg q, A2 = CFr log1{2 CFr + CG log1{2 CG + 2(2τσ2 + Cθq1{2,
and A3 = 2τCh (2/3 + Cθq.
About ii, we evaluate the error from approximation by constructing a specific deep neural network
for generators. We apply Lemma 3 and let g9 = (g9 1, ..., g9Dq be a generator specified in Lemma 3.
ii = Pq* f ´ P9 f
gg
=JPf。g* ´f。g qdPZ
≤ Li J }g* — g}2dPZ
W Li ( ∑ / |g}(xq - gd(xq∣2dPZ(Xq)
dPrDs
=Li ( X llg√ ´ gd}L2]
dPrDs
≤ CgLiDMS丁/D,
17
Under review as a conference paper at ICLR 2019
which follows L1-Lipschitz continuity of f, the Jensen’s inequality, the Cauchy-Schwartz inequality,
compactness of the support ID, and uniformity of PZ. Then, we have
ii ≤ c2MDSjIeD.
Combining the result, We obtain the result of Theorem 1.	□
B.5 Proof of Proposition 1
When P are globally smooth, We obtain a β-smooth density function on I D by its definition. Due
to the smoothness, the studies for nonparametric statistics (Nadaraya, 1964; Ghosal et al., 2007;
Efromovich, 2010; van der Vaart & van Zanten, 2008; Tsybakov, 2009) guarantees that the methods
(KDE,NB,SDE, and GP) obtain the rate θPn~β{p2β'Dqq with respect to the roof of L2 norm.
When P have disconnected supports and locally smooth, We consider a folloWing specific P . Fix
M = 2. Let Us define supports as Si “ Si = {x P ID | xi ≤ 0.5} U ID and S2 “ Sr2 “ {x P ID |
xi ≤ 0.5} U Id. Also, gi(z) “ Z and g2 : Sr2 → S? as
g2pzq “ pg2,ipziq, ..., g2,DpzDqqJ,
where g2,ipziq “ 0.6 ` cpzi ´ 0.5qi{3 and g2,dpzdq “ zd for d P rDsz{1} with a constant c. Then, by
the proof of Lemma 1, p2pxq on S2 is a quadratic function with respect to zi is 1-times differentiable
but not twice-differentiable at the boundary {x P ID | xi “ 0.6}. Hence, the studies (Nadaraya,
1964; Ghosal et al., 2007; Efromovich, 2010; van der Vaart & van Zanten, 2008; Tsybakov, 2009)
provides that the generalization error of the methods is bounded by opn^β{p2β'Dqq with β = 1. □
18