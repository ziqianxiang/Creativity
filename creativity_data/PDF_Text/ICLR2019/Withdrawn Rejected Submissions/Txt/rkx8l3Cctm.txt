Under review as a conference paper at ICLR 2019
Safe Policy Learning from Observations
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we consider the problem of learning a policy by observing numer-
ous non-expert agents. Our goal is to extract a policy that, with high-confidence,
acts better than the agents’ average performance. Such a setting is important for
real-world problems where expert data is scarce but non-expert data can easily be
obtained, e.g. by crowdsourcing. Our approach is to pose this problem as safe
policy improvement in reinforcement learning. First, we evaluate an average be-
havior policy and approximate its value function. Then, we develop a stochastic
policy improvement algorithm that safely improves the average behavior. The pri-
mary advantages of our approach, termed Rerouted Behavior Improvement (RBI),
over other safe learning methods are its stability in the presence of value estima-
tion errors and the elimination of a policy search process. We demonstrate these
advantages in the Taxi grid-world domain and in four games from the Atari learn-
ing environment.
1	Introduction
Recent progress in Reinforcement Learning (RL) has shown a remarkable success in learning to play
games such as Atari from raw sensory input (Mnih et al., 2015; Hessel et al., 2017). Still, tabula
rasa, RL typically requires a significant amount of interaction with the environment in order to
learn. In real-world environments, particularly when a risk factor is involved, an inept policy might
be hazardous (Shalev-Shwartz et al., 2016). Thus, an appealing approach is to record a dataset of
other agents in order to learn a safe initial policy which may later be improved via RL techniques
(Taylor et al., 2011).
Learning from a dataset of experts has been extensively researched in the literature. These learning
methods and algorithms are commonly referred to as Learning from Demonstrations (LfD) (Argall
et al., 2009). In this paper, we consider a sibling, less explored problem of learning form a dataset
of observations (LfO). We define LfO as a relaxation of LfD, where: (1) we do no assume a single
policy generating the data and; (2) the policies are not assumed to be optimal, nor do they cover the
entire state space. Practically, LfO is of interest since it is often easier to collect observations than
expert demonstrations, for example, using crowdsourcing (Kurin et al., 2017). Technically, LfO is
fundamentally different from LfD, where the typical task is to clone a single expert policy. The
data under the LfO setting is expected to be more diverse than LfD data, which in general can be
beneficial for learning. However, it also brings in the challenge of learning from multiple, possibly
contradicting, policy trajectories.
In this work, we propose to solve the LfO problem with a three phases approach: (1) imitation (2)
annotation and; (3) safe improvement. The imitation phase seeks to learn the average behavior in
the dataset. In the annotation part we approximate the value function of the average behavior and in
the final safe improvement step (section 5), we craft a novel algorithm that takes the learned average
behavior and its approximated value function and yields an improved policy without generating
new trajectories. The improvement step is designed to increase the policy performance with a high
confidence in the presence of the value estimation errors that exists in a LfO setup.
Our three phases approach which we term Rerouted Behavior Improvement (RBI), provides a ro-
bust policy (without any interaction with the environment), that both eliminates the risks in random
policy initialization and in addition, can boost the performance of a succeeding RL process. We
demonstrate our algorithm both in a Taxi grid-world (Dietterich, 2000) as well as in the Atari do-
main (section 6). In the latter, we tackle the challenge of learning from non-experts human players
1
Under review as a conference paper at ICLR 2019
(Kurin et al., 2017). We show that our algorithm provides a robust policy, on par with deep RL poli-
cies, using only the demonstrations and without any additional interaction with the environment. As
a baseline, we compare our approach to two state-of-the-art algorithms: (1) learning from demon-
strations, DQfD (Hester et al., 2018) and; (2) robust policy improvement, PPO (Schulman et al.,
2017).
2	Related Work
Learning from demonstrations in the context of deep RL in the Atari environment have been studied
in Cruz Jr et al. (2017), DQfD (Hester et al., 2018) and recently in Pohlen et al. (2018). However,
all these methods focus on expert demonstrations and they benchmark their scores after additional
trajectory collection with an iterative RL process. Therefore, essentially these methods can be cat-
egorized as a RL augmented with expert’s supervised data. In contrast, we take a deeper look into
the first part of best utilizing the observed data to provide the highest initial performance.
Previously, LfO has often been solved solely with the imitation step, e.g. in AlphaGo (Silver et al.,
2016), where the system learned a policy which mimics the average behavior in a multiple policies
dataset. While this provides sound empirical results, we found that one can do better by applying a
safe improvement step to boost the policy performance. A greedy improvement method with respect
to multiple policies has already been suggested in Barreto et al. (2017), yet we found that practi-
cally, estimating the value function of each different policy in the dataset is both computationally
prohibitive and may also produce large errors since the generated data by each different policy is
typically small. In section 4 we suggest a feasible alternative. Instead of learning the value of each
different policy, estimate the value of the average behavior. While such estimation is not exact, we
show both theoretically and experimentally that it provides a surrogate value function that may be
used for policy improvement purposes.
There is also a significant research in the field of safe RL (Garcia & Fernandez, 2015), yet, here
there may be multiple accepted definitions of this term ranging from worst-case criterion (Tamar
et al., 2013) to baseline benchmark approach (Ghavamzadeh et al., 2016). We continue the line
of research of safe RL investigated by Kakade & Langford (2002); Pirotta et al. (2013); Thomas
et al. (2015) but we focus on a dataset composed of unknown policies. Finally, there are two recent
well-known works in the context of non-decreasing policy improvement (also can be categorized
as safe improvement) TRPO and PPO (Schulman et al., 2015; 2017). We compare our work to
these algorithms and show two important advantages: First, our approach can be applied without
an additional Neural Network (NN) policy optimization step and second we provide theoretical and
experimental arguments why both approaches may be deemed unsafe when applied in the context
of a LfO setup.
3	Problem Formulation
We are dealing with a Markov Decision Process (MDP) (Puterman, 2014) where an agent interacts
with an environment and tries to maximize a reward. A MDP is defined by the tuple (S, A, P, R),
where S is a set of states and A is a set of actions. P : S × A → S is the set of probabilities of
switching from a state s to s0 when executing action a, i.e. P(s0|s, a) and R is a reward function
S × A → R which defines the reward r that the agent gets when applying action a in state s.
An agent acts according to a policy π, and its goal is to find a policy that maximizes the expected
cumulative discounted reward, also known as the objective function J(π) = E Pk∞=0 γkrk s0, π
where γ < 1 is a discount factor, k is a time index and s0 is an initial state.
We assume that all policies belong to the Markovian randomized set ΠMR s.t. π ∈ ΠMR is a
probability distribution over A given a state s, i.e. π (a|s).1 For convenient and when appropriate,
We may simply write ∏ to denote ∏(a∕s) (omitting the state,s dependency). In the paper We will
discuss two important distance measures between policies. The first is the Total Variation (TV)
δ(∏, ∏0) = 1 PaJni - ∏i∣ and the second is the KL divergence Dkl(∏∣∣∏0) = - Paa ∏i log ∏i.
1 Note that humans’ policies can generally be considered as part of the history randomized set ΠHR where
π ∈ ΠHR is a probability function over A given the states and actions history. In the appendix we explain how
we circumvented this hurdle in the Atari dataset.
2
Under review as a conference paper at ICLR 2019
These measures are often used to constrain the updates of a learned policy in an iterative policy
improvement RL algorithm (Schulman et al., 2015; 2017).
For a given policy, the state’s value is the expected cumulative reward starting at this state, Vπ(s) =
E Pk∞=0 γkrks, π . Similarly, the Q-value function Qπ(s, a) is the value of taking action a in state
s and then immediately following with policy π. The advantage Aπ(s, a) = Qπ (s, a) - V π(s) is
the gain of taking action a in state S over the average value (note that Pa∈∕ ∏(a∣s)Aπ (s, a) = 0).
We denote by P (so → s∣∏) the probability of switching from state so to state S in k steps with a
policy π .
We define the LfO problem as learning a policy π solely by observing a finite set of trajectories of
other behavior policies without interacting with the environment. Formally, we are given a dataset
D of trajectories executed by N different players each with a presumably different policy. Players
are denoted by pi, i = 1, ..., N, and their corresponding policies are βi, with value and Q-value
functions V i , Qi respectively. D is indexed as {xj }|jD=|o, where the cardinality of the dataset is
denoted by |D| and each record is the tuple xj = (sj , aj , rj, tj, ij) s.t. tj is a termination signal
and ij is the player’s index. D may also be partitioned to D = {Di|i = 1, .., N}, representing the
different players’ records.
The paper is accompanied with a running example, based on the Taxi
grid-world domain (Dietterich, 2000), In the Taxi world, the driver’s task
is to pickup and drop a passenger in predefined locations with minimal
number of steps (See figure 1). For this example, we synthetically gen-
erated policies of the form βi = ai(s)πTand +(1 — αi(s))π*, where π*
is the optimal policy and αi(s) is a different mixing parameter for each
different policy. Generally we divide the state space into two comple-
mentary and equal-size sets S； US； = S, |S*|= |S". Where αi(s) = 0
for s ∈ Si； and αi(s) = 0.75 for s ∈ Si；. In the next sections, we will
use different selections of Si； to generate different types of datasets. For
example, randomly pick half of the states to form Si； is termed in the
paper as a random selection (see also appendix).
Figure 1: Taxi world
4	Average Behavior Policy and its Value Function
We begin with the first phase of the RBI approach which is learning a behavior policy from the
dataset. Learning a behavior policy is a sensible approach both for generating a sound initial perfor-
mance, as well as to avoid unknown states and actions. Yet, contrary to LfD where a single expert
policy is observed, in multiple policies dataset the definition of a behavioral policy is ambiguous.
To that end, we define the average behavior of the dataset as a natural generalization of the single
policy imitation problem.
Definition 4.1 (Average Behavior). The average behavior of a dataset D is
j∈ .}= j Is,。⑺=X Σj∈Di Is,。。・) Σj∈Di ɪs(j )
β(a|S)= Pj∈D ɪs(j)=亍 Pj∈D ɪs(j) Pj∈D ɪs(j) ,
(1)
where ls,a(j) = 1 if (sj,aj) = (s, a), and 0 otherwise. The first form in Eq. equation 4.1 is simply
the fraction of each action taken in each state in D, which for a single policy dataset is identical to
behavioral cloning. Typically, when β is expressed with a NN (as in section 6) we apply a standard
classification learning process with a Cross Entropy loss. Otherwise, for a tabular representation (as
in the Taxi example) we directly enumerate to calculate this expression. The second form in Eq.
equation 4.1 is a weighted sum over all players in the dataset, which may also be expressed with
conditional probability terminology as
β (a|s) = Xβi(a|s)P (pi|s).	⑵
pi
Here P (pi|s) is the probability of visiting an ith player,s record given that a uniform sample X 〜
U (D) picked s. While other definitions of average behavior are possible, the ease of learning such
3
Under review as a conference paper at ICLR 2019
formulation with a NN makes it a natural candidate for RBI. Yet, for the second phase of RBI, one
must evaluate its Q-value function, i.e. Qβ. To that end, we study two alternatives: (1) Temporal
Difference (TD) learning and (2) Monte-Carlo (MC) approximation.
4.1	LEARNING Qβ WITH TD METHODS
A natural approach to evaluate Qβ, would be with off-policy TD learning (Sutton & Barto, 2017).
However, directly learning Qβ requires the knowledge of β which can only be approximated in LfO.
To skirt this requirement, we can first evaluate V β over the distribution of states in D with one-step
TD. This is attained by minimizing the loss Es 〜U (D) IjVβ (S) - r(s,a) - YVβ (s0 )|2] where s,a, s0
are tuples of state-action and next state sampled from D. Notice that since the distribution of actions
in each state in the dataset equals to β, there is no requirement for Importance Sampling (IS) factor.
With Vβ at hand, Qβ can be learn by minimizing the loss
Qe = argmp Es,a,so〜U(D) [∣Qβ(s,a) - r(s,a) - YVβ(s0)∣2] .	(3)
Qβ
While this TD method avoids IS calculation and doe not require any β knowledge, it may still be
problematic in LfO due to the bootstrapping operation. Bootstrapping is the practice of estimating
the value of s from the estimations of the next state s0. If our evaluation of s0 is incorrect, this
error also propagates to s. Unlike, iterative RL this cannot be compensated by visiting again in
s0 and updating the s0 value. In this sense, deep NN parametric forms are much more susceptible
to bootstrapping since the evaluation error is larger comparing to a tabular representation (see also
Sutton’s Deadly Triad discussion in (Sutton & Barto, 2017)).
4.2	APPROXIMATING Qβ WITH MC LEARNING
As an alternative, to avoid bootstrapping we suggest to evaluate a surrogate function
QD(s, a) = X Qi(S, a)P(pi|s,a)	(4)
pi
which we term the Q-value of the dataset. QD may be interpreted as the weighted average over the
players’ Q-values, where the weights P pi|S, a are the probability of visiting an ith player’s record
given that a uniform sample X 〜U(D), picked s, a. In the following two propositions, We show
that such a function has two appealing characteristics. First, it may be evaluated with a L1-norm
loss and Monte-Carlo (MC) learning (Sutton & Barto, 2017) from the dataset trajectories, without
off-policy corrections and without the burden of evaluating each Qi independently. Secondly, it is a
Q-value function of a time dependent policy with a very similar structure to the average behavior.
Taken together, they provide an efficient alternative to approximate the value function of β .
Proposition 4.1 (Consistency of MC upper bound). For an approximation QD and a loss
Lli (Q D)= 击 Pj∈D | QD (Sj, aj) — Q D (sj,aj )| ,an upper bound for the loss when ∣D∣→ ∞
is
Lli (QD) ≤Lmc(QD) = Exj〜U(D) [∣QD(Sj,aj) - Rj ∣] ,	(5)
where Rj =	k≥0 Ykrj+k is the sampled Monte-Carlo return (Proof in the appendix).
The attractive implication of Proposition 4.1 is that we can learn QD without any policy information
and specifically without plugging Importance Sampling (IS) corrections such as 呆 which are known
as a source of high variance (Munos et al., 2016). In addition, as suggested, this method does not
use bootstrapping. The next proposition defines a policy with a Q-value equal to QD .
Proposition 4.2. Given a state-action pair S, α, then QD (S, a) is the Q-value of a time dependent
policy βDi (a|s, k), where k is a time index and s, α is a fixed initial state-action pair,
βD(a∣s,k) = Xβi(a∣s)P(pi∣S,a → s).	⑹
pi
Here the conditional probability is over a uniform sample X 〜U(Dg,a,k) where Dg,a,k is a subset
ofD that contains all the entries in the dataset with distance k from an entry with a state-action pair
Sα, αa (Proof in the appendix).
4
Under review as a conference paper at ICLR 2019
Proposition 4.2 indicates that when P(pi|s, a → s) can be approximated as P (pi|s) at least for a
finite horizon k (X 1-1γ, then QD ' Qβ . This happens when the distribution of players in states
near S, a equals to the distribution of players in those states in the entire dataset. Practically, while
both policies are not equal, they have a relatively low TV distance and therefore their Q-values are
close. To further increase the robustness of our method we add an interesting consideration: our
improvement step will rely only on the action ranking of the Q-value in each state, i.e. the order of
{Qβ (s, ai)}ai∈A (see next section). This, as we show hereafter, significantly increases the effective
similarity between QD and Qβ .
We demonstrates the action ranking similarity between Qβ and QD in the Taxi grid-world example.
To that end, We generated trajectories with N players according to three selection types of S*: (1)
row selection (2) column selection and; (3) random selection. Each selection type provides a dif-
ferent class of policies and therefore form a different dataset (see exact definitions in the appendix).
In the first experiment (Figure 2a), we plot the average TV distance (for various initial states sa),
between β and βS?-, as a function of the time-step for N = 2. Generally it is low, but it may be ques-
tionable whether relying on the true value of QD for the improvement step will provide adequate
results. However, when we consider only the action ranking similarity (evaluated with the Pearson’s
rank correlation), we find even more favorable pattern.
First, in Figure 2b we plot the average rank correlation between Qβ and QD (for γ = 0.9) as
a function of the number of different policies used to generate the dataset. It is evident that the
rank correlation is very high and stable for any number of policies. In the second experiment, we
generated N = 2 (Figure 2c) and N = 10 (Figure 2d) policies and examined the impact of different
discount factors. Also here, for the majority of practical scenarios we observe sufficiently high rank
correlation. Only for a very large discount factor (close to 1) the rank correlation reduces. This
happens since the long horizon accumulates more error from the difference (in high k steps) in the
TV distance. In conclusion, while Proposition 4.2 state bounds on the similarity between Qβ and
QD , evaluating the Pearson’s rank correlation confirms our statement that in practice the action
ranking of QD is an acceptable surrogate for the action ranking of Qβ .
2 Rank Correlation vs number of policies
1.1 -
1.0-
0.9-
0.8-
0.7-
0.6-
0	10	20	30	40	50
(a) k time step
constant y= 0.9
(b) # policies
----row selection
----column selection
random selection
RankCorreIation vs discount (N=2)
1.2 -
1.1-
1.0-
0.9
0.8
0.7-
0.6
Figure 2: Taxi: comparison between Qβ and QD
(d) discount γ

5	Safe Policy Improvement
Our next step, is to harness the approximated Q-function QD ' Qβ (which here will be termed Qβ),
in order to improve the average behavior β, i.e. generate a policy π such that J(π) - J(β) ≥ 0.
However, one must recall that Qβ is learned from a fixed, sometimes even small dataset. Therefore,
in order to guarantee improvement, we analyze the statistics of the value function’s error. This
leads to an interesting observation: the Q-value has a higher error for actions that were taken less
frequently, thus, to avoid improvement penalty, we must restrict the ratio of the change in probability
Ci = ∏i. We will use this observation to craft our third phase of RBL i.e. the safe improvement
βi
step, and show that other well-known monotonic improvement methods (such as PPO (Schulman
et al., 2017) and TRPO (Schulman et al., 2015)) overlooked this consideration and therefore their
improvement method may be unsafe for a LfO setup.
5
Under review as a conference paper at ICLR 2019
5.1	Soft Policy Improvement
Before analyzing the error’s statistics, we begin by considering a subset of policies in ΠMR which
are proven to improve β if our estimation of Qβ is exact. Out of this subset we will later pick
our improved policy. Recall that the most naive and also common improvement method is taking a
greedy step, i.e. deterministically acting with the highest Q-value action in each state. This is known
by the policy improvement theorem (Sutton & Barto, 2017), to improve the policy performance. The
policy improvement theorem may be generalized to include a larger family of soft steps.
Lemma 5.1 (Soft Policy Improvement). Given a policy β, with value and advantage V β , Aβ, a
policy π ∈ ΠMR improves β, i.e. Vπ ≥ Vβ ∀s, if it satisfies Pa π(a∣s)Aβ (s, a) ≥ 0 ∀s with at
least one state with strict inequality. The term Ea π(a∣s)Aβ(s, a) is called the improvement step.2
Essentially, every policy that increases the probability of taking positive advantage actions over
the probability of taking negative advantage actions achieves improvement. We will use the next
Corollary to prove that our specific improvement step guarantees a positive improvement step.
Corollary 5.1.1 (Rank-Based Policy Improvement). Let (Ai)|iA=|1 be an ordered list of the β advan-
tages in a state s, s.t. Ai+ι ≥ Ai, and let Ci = ∏i∕βi. Iffor all states (Ci)iA[ is a monotonic
non-decreasing sequence s.t. ci+1 ≥ ci, then π improves β (Proof in the appendix).
5.2	Standard Error of the Value Estimation
To provide a statistical argument for the expected error of the Q-function, consider learning Qβ with
a tabular representation. The Q-function is the expected value of the random variable zπ (s, a) =
Pk≥o γkIrk|s, a, π. Therefore, the Standard Error (SE) of an approximation Qβ(s, a) for the Q-
value with N MC trajectories is
σz(s,a)
σε(s,a) — PNsβ(a∣s),
(7)
where Ns
σε(s,a) H
Ej∈d ls(j) is the number of visitations in state s, s.t. N = β(a∣s)Ns. Therefore,
and specifically for low frequency actions such estimation may suffer large SE.3
1
√β(0N
5.3	Policy Improvement in the Presence of Value Estimation Errors
We now turn to the crucial question of what happens when one applies an improvement step with
respect to an inaccurate estimation of the Q-function, i.e. Qβ.
Lemma 5.2 (Improvement Penalty). Let Qβ = Vβ + Ae be an estimator of Qβ with an error
ε(s, a) = (Qβ 一 Qβ)(s, a) and let π be a policy that satisfies lemma 5.1 with respect to A^β. Then
the following holds
Vπ (S)- Vβ (S) ≥-X X γk P(S → s0∣∏) X ε(s0,a) (β(a∣s0) - π(a∣s0)),	(8)
s0∈S k≥0	a∈A
where the difference, denoted E (S), is called the improvement penalty (proof in the appendix).
For simplicity we may write E(s) = - £§, ɑ ρπ(s0∣s)ε(s0, a)(β(a∣s0) - π(a∣s0)), where ρπ(s0∣s) is
sometimes referred to as the undiscounted state distribution of policy π given an initial state S. Since
ε(s0, a) is a random variable, it is worth to consider the variance of E (s). Assuming that the errors
ε(s, a) are positively correlated (since neighboring state-action pairs share trajectories of rewards)
and under the error model introduced above, it follows that
σ2(s) ≥ X(Pn(SlS))Zb")(β(a∣s0)-π(a∣sθ))2 = X (Pn(SlNF⑷肉可⑷*0))2 .
2We post a proof in the appendix for completeness, though it may have been proven elsewhere.
3 Note that even for deterministic environments, a stochastic policy inevitably provides σz(s,a) > 0.
6
Under review as a conference paper at ICLR 2019
Hence, it is evident that the improvement penalty can be extremely large when the term lβ-π is
unregulated. Moreover, a single mistake along the trajectory, caused by an unregulated element,
might wreck the performance of the entire policy. Therefore, it is essential to regulate each one of
these elements to minimize the potential improvement penalty.
5.4	The Reroute Constraint
In order to regulate the ratio lβ-πl , We suggest limiting the improvement step to a subset of ΠMR
based on the following constraint.
Definition 5.1 (Reroute Constraint). Given a policy β, a policy π is a reroute(cmin, cmax) of β, if
π(a∣s) = c(s,a)β(a∣s) where c(s,a) ∈ [cmin, Cmax]. Further, note that reroute is a subset of the TV
constraint with δ = min(1 — Cmin, max( CmaxT, 1-2min)) (proof in the appendix).
Now, it is evident that with the reroute constraint, each element in the sum of (8) is regulated and
proportional to ,β(a∣s)∣1 — c(s,a)∣ where c(s,a) ∈ [cmin,Cmaχ]. Analyzing other well-known
trust regions such as the TV constraint δ ≥ ɪ Pa β(a|s) — π(a∣s)∣, the average KL-divergence
constraint DκL(β∣∣∏) = -Es*[P° β(a∣s) log 聆∙∣)], used in the TRPO algorithm, and the PPO
objective function (Schulman et al., 2017), surprisingly reveals that non of them properly controls
the improvement penalty (see an example and an analysis of the PPO objective in the appendix, we
also show in the appendix that the solution of the PPO objective is not unique).
5.5	Maximizing the Improvement Step under the Reroute Constraint
We now consider the last part of our improvement approach, i.e. maximizing the objective func-
tion under the reroute constraint and whether such maximization provides a positive improvement
step. It is well-known that maximizing the objective function without generating new trajecto-
ries of π is a hard task since the distribution of states induced by the policy π is unknown. Pre-
vious works have suggested to maximize a surrogate off-policy objective function JOP (π) =
Es〜β[Pa ∏(a∣s)Aβ(s, a)]. These works have also suggested to solve the constrained maximiza-
tion with a NN policy representation and a gradient ascent approach (Schulman et al., 2015). Here
we suggest a refreshing alternative, instead of precalculating the policy π that maximizes JOP one
may ad hoc compute the policy that maximizes the improvement step Pa π(a∣s)Aβ(s,a) (which is
the argument of the JOP objective) for each different state. Such an approach maximizes also the
JOP objective since the improvement step is independent between states. For the reroute constraint,
this essentially sums up to solving the following simple linear program for each state
Maximize: (Aβ)Tπ
Subject to: Cminβ ≤ π ≤ Cmaxβ
(9)
And:	πi = 1.
Where π, β and An are vector representations of (∏(ai∣s))iA[, (β(ai∣s))iA1 and (Ae(s,a))iA]
respectively. We term the algorithm that solves this maximization problem as Max-Reroute (see
Algorithm 1). In the appendix we also provide an analogous algorithm that maximizes the improve-
ment step under the TV constraint (termed Max-TV). We will use Max-TV as a baseline for the
performance of the reroute constraint. With an ad hoc maximization approach, we avoid the hassle
of additional learning task after the policy imitation step, and in addition, our solution guarantees
maximization without the common caveats in NN learning such as converging to local minima or
overfitting etc.
Further analyzing Max-Reroute and Max-TV quickly reveals that they both rely only on the action
ranking at each state (as stated in the previous section). This is also in contrast with the aforemen-
tioned methods (TRPO and PPO) where by their definition as policy gradient methods (Sutton et al.,
2000), they optimize the policy according to the magnitude of the advantage function. Finally, notice
that both Max-Reroute and Max-TV satisfy the conditions of Corollary 5.1.1, therefore they always
provide a positive improvement step and hence for a perfect approximation of the value function
they are guaranteed to improve the performance.
7
Under review as a conference paper at ICLR 2019
Algorithm 1: Max-Reroute
Data: s, β, Aβ, (cm
in, cmax)
Result: {π(a∣s), a ∈ A}
begin
A —A
∆ 4-- 1 — Cmin
π(a∣s) 4— Cminβ(a∣s) ∀a ∈ A
while ∆ > 0 do
a = argmaXa∈A Ae (s,a)
∆a = min{∆, (Cmax — Cmin)β(α∣s)}
A 4- A/a
∆ 4- ∆ - ∆a
_ π(a∣s) 4— π(a∣s) + ∆a
Let us now return to our Taxi example and examine differ-
ent types of improvement steps with respect to a behavioral
cloning baseline:: (1) a greedy step4 (2) a TV constrained and;
(3) a reroute constrained steps. The dataset is generated by two
policies with a discount factor γ = 0.9. We consider two types
of policies: row selection (3a) and random selection (3b). The
behavior policy was calculated by enumerating actions in each
state. We examine the two alternatives of evaluating Qβ : MC
(without off-policy corrections) and TD learning (as described
in section 4).
First, it is clear that taking a greedy step is a precarious ap-
proach. In these experiments, the generated greedy policy con-
tained recurrent states (which is a repetitive series of states).
This prevented the completion of the task. Comparing the TV
step to the average behavior baseline, reveals that only in rela-
tively large datasets (more than 103 episodes in this example)
TV step is better than behavioral cloning. This demonstrates
the problematic approach of taking actions that were insuffi-
ciently evaluated. In real-world MDP with larger number of
states, it is extremely difficult to sufficiently sample the entire
state-space, hence, we project that TV should be poorer than
behavioral cloning even for large datasets. In the next section,
this premise is verified in the Atari domain.
Contrary to TV, reroute provided almost always better perfor-
mance than the average behavior. This pattern repeats both
with TD and MC learning. An important observation is that
MC outperform TD for larger datasets. Since MC should con-
verge to QD and TD to Qβ, we cannot link any performance
degradation to the evaluation of QD instead of Qβ . On the
other hand, we can still hypothesize that the slow improve-
-20-
-40 -
-60-
-80-
-100-
IO1	IO2	IO3
(a)
LfO - 2 random selection policies
-20-
-40 -
-60-
-80-
-100-
Figure 3: Taxi:
Improvement steps comparison
ment in TD (in large datasets) is due to bootstrapping. In the next section in the Atari example, we
show that when we utilize a NN parametric form where bootstrapping errors are larger, this pattern
intensifies.
6 Learning to play Atari by ob serving inexpert human players
In the previous section, we analyzed the expected error which motivated the reroute constraint for a
tabular representation. In this section, we experimentally show that the same ideas holds for Deep
NN parametric form. We conducted an experiment with a crowdsourced data of 4 Atari games
4An unconstrained step, equivalent to reroute parameters (Cmin, Cmax.) = (0, ∞)
8
Under review as a conference paper at ICLR 2019
(Spaceinvaders, MsPacman, Qbert and Montezuma’s Revenge) (Kurin et al., 2017). Each game had
roughly 1000 recorded episodes. We employed two networks, one for policy cloning β and one
for Q-value estimation with architecture inspired by the Dueling DQN (Mnih et al., 2015; Wang
et al., 2015) and a few modifications (see appendix). We evaluated two types of local maximization
steps: Max-Reroute, with different parameters (cmin, cmax); and Max-TV. We implemented two
baselines: (1) DQfD algorithm with hyperparameters as in Hester et al. (2018) and (2) a single PPO
policy search step based on the learned behavior policy and the estimated advantage. The following
discussion refers to the results presented in Figure 4.
dqfd
mspacman
reroute reroute	reroute tv
humans behavioral (0.5,1.5) (0.25,1.75) (0,2)	(0.25)
25.0K-
20.0K-
15.0K-
10.0K-
5.0K-
o.o-
ɪ
T^T
THi .
XHT.
revenge
8.0K-
6.0K-
4.0K-
2.0K-
0.0-
spaceinvaders
1.2K-
1.0K-
0.8K-
0.5K-
0.2K-
0.0-
o
humans behavioral
10.0K-
5.0K-
o.o-
3.0K-
2.0K-
ι.oκ-
o.o-
0.8K-
0.6K-
0.4K-
0.2 K-
0.0-
Minibatches (# of backward passes)
(b) Learning Curves
(a) Box Plot of final policy performance
,ɪ


Figure 4: Atari experiment results
Average behavior performance: When comparing the average behavior performance to the av-
erage human score, we get very similar performance in 2 out of 4 games. However, in Qbert, we
obtained substantially better performance than the average human score. Generally, we expect that in
games where longer trajectories lead to significantly more reward, the average policy obtains above
average reward, since the effect of good players in the data has a heavier weight on the results. On
the other hand, we assume that the lower score in MsPacman is due to the more complex and less
clear frame in this game with respect to the other games. We take the average behavior performance
as a baseline for comparing the safety level of the subsequent improvement steps.
MC and TD estimators of the Q-value function: We evaluated the performance of
reroute(0.5, 1.5) during the training process with two different estimators, both concurrently
learned from the same batches. MC-reroute(0.5, 1.5) learned the surrogate QD with full trajec-
9
Under review as a conference paper at ICLR 2019
tories and provided better score than the behavioral cloning baseline in 3 out of 4 games and tied in
one. On the other hand TD-reroute(0.5, 1.5) learned the true Qβ with 1-step TD learning but did
not show a significant improvement in any game. We hypothesize that larger evaluation errors due
to bootstrapping may be the reason for the low performance of the TD estimator. The rest of the
discussion focuses on the MC evaluation.
Local maximization steps: We chose to evaluate Max-TV with δ = 0.25 since it encapsulates
the reroute(0.5, 1.5) region. It is clear that Max-TV is always dominated by Max-Reroute and it
did not secure safe improvement in any of the games. A comparison of different reroute parame-
ters reveals that there is an fundamental trade-off between safety and improvement. Smaller steps
like reroute(0.5, 1.5) support higher safety at the expense of improvement potential. In all games
reroute(0.5, 1.5) provided safe improvement, but in Qbert its results were inferior to reroute(0, 2).
On the other hand, reroute(0, 2) reduced the Revenge score which indicates a too greedy step. Our
results indicates that it is important to set cmin > 0 since avoiding so may discard some important
actions altogether due to errors in the value evaluation, resulting in a poor performance.
Comparison with PPO: For the PPO baseline, we executed a policy search with the learned advan-
tage Aβ according to the PPO objective
J PPO (∏) = Es 〜β
β(a∣s) min
a∈A
Ae(s,a)泮,Aβ(s,a)cliP (讲,1-,，］十 :
We chose ε = 0.5, motivated by the similarity to the reroute(0.5, 1.5) region (see appendix).
Contrary to the PPO paper, we plugged our advantage estimator and did not use the Generalized
Advantage Estimation (GAE). While P P O(0.5) scored (see box plot in Figure 4a) better than
reroute(0.5, 1.5) in Qbert, in all other games it reduced the behavioral cloning score. The over-
all results indicate the similarity between P P O(0.5) and reroute(0, 2), probably since negative
advantages actions tend to settle at zero-probability to avoid negative penalty. This emphasizes the
importance of the cmin parameter of reroute which is missing from PPO.
Comparison with DQfD: DQfD scored below the average behavior in all games. The significantly
low scores in MsPacman and Qbert raise the question whether DQfD and more generally, Off-
Policy greedy RL can effectively learn from multiple non-exploratory fixed policies. The most
conspicuous issue is the greedy policy improvement approach taken by DQfD: we have shown that
an unconstrained greedy improvement step leads to a poor performance (recall the Taxi example).
In addition, as discussed in section 4, Off-Policy RL with TD learning also suffers from the second
RL ingredient, i.e. policy evaluation . As our results show, our proposed safe policy improvement
scheme with MC learning mitigates these issues, leading to significantly better results
7 Conclusions
In this paper, we studied both theoretically and experimentally the problem of LfO. We analyzed
factors that impede classical methods, such as TRPO/PPO and Off-Policy greedy RL, and proposed
a novel alternative, Rerouted Behavior Improvement (RBI), that incorporates behavioral cloning
and a safe policy improvement step. RBI is designed to learn from multiple agents and to mitigate
value evaluation errors. It does not use importance sampling corrections or bootstrapping to estimate
values, hence it is less sensitive to deep network function approximation errors. In addition, it does
not require a policy search process. Our experimental results in the Atari domain demonstrate the
strength of RBI compared to current state-of-the-art algorithms. We project that these attributes of
RBI would also benefit an iterative RL process. Therefore, in the future, we plan to study RBI as an
online RL policy improvement method.
10
Under review as a conference paper at ICLR 2019
References
Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning
from demonstration. Robotics and autonomous systems, 57(5):469-483, 2009.
Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom SchaUL Hado P van Hasselt, and
David Silver. Successor features for transfer in reinforcement learning. In Advances in neural
information processing systems, pp. 4055-4065, 2017.
Gabriel V Cruz Jr, Yunshu Du, and Matthew E Taylor. Pre-training neural networks with human
demonstrations for deep reinforcement learning. arXiv preprint arXiv:1709.04083, 2017.
Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decompo-
sition. Journal of Artificial Intelligence Research, 13:227-303, 2000.
Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(1):1437-1480, 2015.
Mohammad Ghavamzadeh, Marek Petrik, and Yinlam Chow. Safe policy improvement by minimiz-
ing robust baseline regret. In Advances in Neural Information Processing Systems, pp. 2298-2306,
2016.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan,
John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. 2018.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
ICML, volume 2, pp. 267-274, 2002.
Vitaly Kurin, Sebastian Nowozin, Katja Hofmann, Lucas Beyer, and Bastian Leibe. The atari grand
challenge dataset. arXiv preprint arXiv:1705.10998, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054-1062,
2016.
Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Adaptive step-size for policy gradient meth-
ods. In Advances in Neural Information Processing Systems, pp. 1394-1402, 2013.
Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden,
Gabriel Barth-Maron, Hado van Hasselt, John Quan, Mel Vecerk et al. Observe and look further:
Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Jeff Racine. Consistent cross-validatory model-selection for dependent data: hv-block cross-
validation. Journal of econometrics, 99(1):39-61, 2000.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
11
Under review as a conference paper at ICLR 2019
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine
LearningResearch, 15(1):1929-1958, 2014.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
RS Sutton and AG Barto. Reinforcement learning: An introduction, (complete draft), 2017.
Aviv Tamar, Huan Xu, and Shie Mannor. Scaling up robust mdps by reinforcement learning. arXiv
preprint arXiv:1306.6189, 2013.
Matthew E Taylor, Halit Bener Suay, and Sonia Chernova. Integrating reinforcement learning with
human demonstrations of varying ability. In The 10th International Conference on Autonomous
Agents and Multiagent Systems-Volume 2, pp. 617-624. International Foundation for Autonomous
Agents and Multiagent Systems, 2011.
Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy
improvement. In International Conference on Machine Learning, pp. 2380-2388, 2015.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas.
Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581,
2015.
12
Under review as a conference paper at ICLR 2019
A Operator Notation
In part of the appendix, we use vector and operator notation, where vπ and qπ represent the value
and Q-value functions as vectors in V ≡ R1S1 and Q ≡ RlS×Al. We denote the partial order
X ≥ y ^⇒ χ(s,a) ≥ y(s,a) ∀s, a ∈ S ×A, and the norm ∣∣xk= maxs,α∣x∣. Letus define two
mappings between V and Q:
1.	Q → V mapping Ππ: (Ππqπ)(s) = Ea∈∕∏(a∣s)qπ(s, a) = vπ(s).
This mapping is used to backup state-action-values to state value.
2.	V → Q mapping P: (Pvπ) (s,a) = Ps0∈S P(s0∣s,a)vπ(s0).
This mapping is used to backup state-action-values based on future values. Note that this
mapping is not dependent on a specific policy.
Let us further define two probability operators:
1.	V → V state to state transition probability Pvπ :
(Pvvπ)(s) = Pa∈A ∏(a∣s) Ps0∈S P(S0|s, a)vπ(s0).
2.	Q → Q state-action to state-action transition probability Pqπ :
(Pqrqπ)(S, a) = Ps0∈S P(Sls, a) Pao∈AπWs0)qπ(S0, a0).
Note that the probability operators are bounded linear transformations with spectral radios σ(Pvπ) ≤
1 and σ(Pqπ) ≤ 1 (Puterman, 2014). Prominent operators in the MDP theory are the recursive value
operator Tvπv = Ππr + γPvπv and recursive Q-value operator Tqπ q = r + γPqπ q. Both have a
unique solution for the singular value equations v = Tvπv and q = Tqπq. These solutions are vπ and
qπ respectively (Sutton & Barto, 2017; Puterman, 2014). In addition, in the proofs we will use the
following easy to prove properties:
___ ~_____
1.	Pq = Pnn
2.	Pv = Ππ P
3.	x ≥ y, x, y ∈ V, ⇒ Px ≥ Py
4.	The probability of transforming from S to s0 in k steps is P(s → s0∣π) = (Pv)k(s, s0).
5.	[(Pq)kx](S)= Esk~∏∣s [x(Sk)]
B Average B ehavior Proofs
B.1	UPPER BOUND FOR MONTE-CARLO EVALUATION OF QD WITH L1 LOSS
Proof. We denote by Ll、(QD) the true Li least absolute error loss of the value function estimator,
and We show that LMC(QD) ≥ Ll、(QD) for ∣D∣→ ∞ such that minimization of LMC leads
to a bounded error of LL1 . Remember that the dataset D is a concatenation of the sets Di =
{(S1, a1), (S2, a2),...}, i = 1,..,N.
LLI (QD ) = |d| X IQD(Sj ,aj ) - QD (Sj, aj )1 =
|D| j∈D
ɪ X
1D1 j∈D∩Ω<
)：Q (Sj, aj )P (PISj, aj ) - QD (Sj, aj ) +
pi
13
Under review as a conference paper at ICLR 2019
where Ω< and Ω> denote the complementary subspaces of S × A where QD underestimates and
overestimates the target (respectively).
ɪ X
IDI j∈D∩Ω<
EQi(Sj ,aj )P(p|Sj ,aj ) — QD (Sj ,aj)=
pi
IDI X X P(PlSj,αj) I Qi(Sj,aj) - QD(Sj, aj)] ≤
I I j∈D∩Ω< PW
1
D
X XP(PiISj,aj) IQi(Sj,aj) — QD(Sj,aj)| ∙
j∈D∩Ω< pi
Joining the equivalent term for the Ω> region, we obtain the inequality
Ll1 (QD) ≤
ɪ X (X is,。(小 XP⅛∣ ∣Qi(Sj,aj) - QD(Sj,a)∣ =
I I UniqUe(S,a)∈D ∖j∈D	) Pi 乙j∈D s,a ⑺
ɪ X X(X is,a(j )1 I Qi(Sj ,a) - QD (Sj ,a )∣ =
unique(s,a)∈D Pi ∖j∈Di	)
IDI X 1 Qij (Sj ,aj ) - QD (Sj，aj )| = Idi X 1 E [Rj 岛，aj ,pi] - Q D (Sj ,aj ) 1
”i j∈D	1D1 j∈D
Plugging in the Jensen inequality for all Sj, aj- ,pij
∣E [Rj |Sj ,aj ,pij]—QD (Sj ,aj)∣ ≤ EbRj- Q D (Sj ,aj)1 i Sj ,aj ,pij]
we obtain
LLI (QD ) ≤ IDI X E h|Rj - QD (Sj , aj )1 I Sj , aj ,pij] ∙
IDI j∈D
In the limit when ∣D∣→ ∞ the sample average converges to the expectation, i.e. ∣D∣ Pj∈D [∙] →
E [∙], therefore
LLI(QD) ≤ IDT XE ||Rj - QD(Sj,aj)1 ∣ Sj,aj,pij]
IDI j∈D
=E [E [|Rj- QD (Sj ,aj )ι | Sj ,aj ,pij]]
=E[ιRj - QD(Sj, aj)ι]
=IDT XIRj - Q D (Sj, aj )l= LMC (QD )∙
IDI j∈D
□
B.2	QD IS A VALUE OF A TIME DEPENDENT POLICY βDa
Given a state-action pair s, a, then QD(s, a) is the Q-value of a time dependent policy βDa(a∣S, k),
where k is a time index and s, a is a fixed initial state-action pair,
β∙DG(a∣S,k) = Xβi(a∣S)P(Pi∣a,a → S)∙	(10)
Pi
Here the conditional probability is over a uniform sample x 〜U(D,s,d,k) where Ds,d,k is a subset
of D that contains all the entries in the dataset with distance k from an entry with a state-action pair
a, aa.
14
Under review as a conference paper at ICLR 2019
Proof. For the proof, let us construct an extended MDP, with an additional initial state, termed s0
with N different actions. Following an action selection in state so, an action a is executed in state S
(in the original MDP) and then the original MDP continues as usual.
Let us define a History Randomized (HR) policy βh ∈ ΠHR according to the following rule:
1.	In state s0, select an action i with probability P(pi|sa, aa).
2.	In all consecutive states act with the Markovian policy βi .
Notice that it is clearly a HR policy since the second step is determined by the history (that is the
first step). It is straightforward to calculate the value of the s0 state with the law of total probability
s.t. Vβh(s0) = Pπ Qi(sa, aa)P (pi|sa, aa) =QD(sa,aa).
Puterman (2014) proved (Theorems 5.5.1-3) that for every HR policy, there exists a time dependent
Markovian policy with the same value, let it be denoted as βDd. The Markovian policy satisfies
βDa(a∣s,k) = P(a∣s,k,βh),
that is the probability of executing action a in state s and time step k when following βh . Again,
with the law of total probability this may be written as
βDa(a|s, k) = Xβi(a|s)p(a(so) = i|so → s),
where a(s0 ) is the chosen action in state s0 . Notice that sa, aa deterministically follow state s0 so by
applying Bayes’ theorem, we get that
P(a(S0) = i∖S0 -→k S)
P(a(s0) = i)P(S, a → s∣a(so) = i)	P(Pi|S, a)P(s, a → s∣βi)
p(s, a → S)	Ppj P(Pj∣s, s)p(s, a → s∣βj).
Let US define by lg,a,k,s an indicator function which equals 1 for the entries in the dataset which
are in state s in a distance k from an entry with state-action pair sa, aa. We may represent the above
expression with the indicator summation notation
.	k	.	Pj∈Di ɪs,a Pj∈Di ls,a,k,s	_
P(Pls, a)P(a, a → s∖βi,)	=	Pj∈D 1 喜,& Pj∈Di1 喜,&	= Σj∈Di ɪs,ɑ,k,s
PPj P(Pjis, a)P(S,a → s∖βj)	P “ ∕P∈D≤⅛2 三…1⅛a,k,s ʌ	∑j∈D Ig,a,k,s
乙Pi k Ej∈D ɪs,ɑ	∑2j∈Di0 ɪs,ɑ I
Finally, the last expression can be transformed back to conditional probability notation of the form
工j∈Di ɪs,ɑ,k,s m i「〜k \
Pj~------------ = P(Pi ∖s, a → S)
乙j∈D ɪs,a,k,s
□
C Safe Policy Improvement
C.1 Reroute is a subset of TV
The set of reroute policies with [cmin, cmax] is a subset of the set of δ-TV policies, where δ
min(1 — Cmin, max( Cmax-1, 1-2min)).
Proof. For a reroute policy, the TV distance is ɪ Pa β(ai∖s)∖1 — c∕≤ Pa β(a∕s)δ = δ. For the
second upper bound 1 — cmin, notice that a rerouted policy {πi} may be written as πi = cminβi +δi,
where δi ≥ 0. Therefore, the TV distance is
TV =1 X∖∏i — βi∖=2 X∖Cminβi + δi — βi∖≤ 2 X ((1 — Cmin)βi + &),
15
Under review as a conference paper at ICLR 2019
but 1 = iπi	=	i(cminβi	+ δi)	=	cmin +	i δi.	Thus, i	δi	= 1 - cmin and we may write
TV ≤ 2 ((I - Cmin) + (I - Cmin)) = 1 - cmin ∙
To show that the TV region is not in reroute, take any policy with a zero probability action and
another action with a probability of β(a/s) ≤ δ. The policy which switches probabilities between
these two actions is in TV(δ) but it is not in reroute for any finite Cmaχ.	□
C.2 Soft Policy Improvement
The soft policy improvement rule states that every policy π that satisfies
^X π(a∣s)Aβ(s,a) ≥ 0 ∀s ∈ S
a
improves the policy β s.t. Vπ ≥ Vβ ∀s. To avoid stagnation we demand that the inequality be strict
for at least a single state. In operator notation, the last equation can be written as
Ππaβ ≥ 0
Proof. Plugging in aβ = qβ - vβ and using vβ = Ππvβ, we get that
vβ ≤ Ππqβ = Ππ(r + YPve) = rπ + YPvvβ.	(11)
Then, by applying (11) recursively we get
vβ ≤ rπ + YPvπvβ = rπ + YPvπ (rπ + YPvπvβ) ≤ rπ + YPvπrπ + Y2 (Pvπ)2vβ ≤ rπ+
YPvπrπ + Y2(Pvπ)2rπ + Y3(Pvπ)3vβ ≤ ... ≤ X Yk(Pvπ)krπ = (I - YPvπ)-1rπ = vπ.
k≥0
□
(Schulman et al., 2015) have shown that
J(∏) - J(β) = ET5 X YkAe(sk,ak) .
k≥0
This equation can also be written as
J(π) - J(β) = X YkEsk〜∏∣S0 X π(alsk)Ae(Sk,ak) =
k≥0	a∈A
=X Yk XP(SO → s|n) X n(a|s)Ae(s, a) =
k≥0	s∈S	a∈A
X (X YkP(SO → s|n) ) X π(als)Aβ(Sk, a).
s∈S k≥0	a∈A
Recall the definition of the discounted distribution of states
Pπ(S) = X YkP(SO → s∣∏),
k≥O
we conclude that
J(π) - J(β) = Xρπ(S) X n(a|S)Ae(s, a)
s∈S	a∈A
16
Under review as a conference paper at ICLR 2019
C.3 Rank-Based Policy Improvement
Proof. Denote by i the index of the advantage ordered list {Ai}. Since Pa βiAiβ = 0, we can
write it as an equation of the positive and negative advantage components
N	in
XβiAiβ=Xβi(-Aiβ),
a=ip	a=0
where ip is the minimal index of positive advantages and in is the maximal index of negative advan-
tages. Since all probability ratios are non-negative, it is sufficient to show that
N	in
ciβiAiβ ≥	ciβi(-Aiβ).
a=ip	a=0
But clearly
N	N	in	in
X ciβiAiβ ≥ cip XβiAiβ≥cinXβi(-Aiβ)≥Xciβi(-Aiβ).
a=ip	a=ip	a=0	a=0
□
C.4 Policy Improvement Penalty
Let Qβ be an approximation of Qβ with an error ε(s, a) = (Qβ - Qe)(s, a) and let π be a policy
that satisfies the soft policy improvement theorem with respect to Qe. Then the following holds:
Vπ (S)- Vβ (S) ≥- X (X Yk P(S → SIn)) X ε(s0, a)(β (a∣s0) - π(a∣s0)) .	(12)
s0∈S k≥0	a∈A
The proof resembles the Generalized Policy improvement theorem (Barreto et al., 2017).
Proof. We will use the equivalent operator notation. Define the vector ε = qβ - ^β
Tn 铲=r + YPn 铲 ≥ r + γp∏β 铲
=r + YPne qβ - γP∏βε
= Tqβqβ - YPqβε
= qβ - YPqβε
=铲 + (1 -YPe )ε
Note that the inequality is valid, since Ππ qe ≥ Πeqe (by the theorem assumptions), and if V ≥ U
then Pv ≥ Pu. Set y = (1 - γPβ)ε and notice that Tq (qe + y) = Tqqe + YPqy. By induction,
we show that
n
(Tn)nqe ≥ qe + XYk(P∏)ky.
k=0
We showed it for n = 1, assume it holds for n, then for n + 1 we obtain
(TqYiqe = TKTq)nqe ≥Tqr (qe + XYk(Pq)ky)=
k=0
n	n	n+1
=t∏ 铲 + X Yk+1(pq )k+1y ≥ 铲+y + X Yk+1(pq )k+1 =铲 + X Yk(pq )k y
k=0	k=0	k=0
17
Under review as a conference paper at ICLR 2019
Using the contraction properties of Tqπ , s.t. limk→∞ (Tqπ)kx = qπ, ∀x ∈ Q and plugging back
(1 - γPqβ )ε = y , we obtain
qπ = nlimo(Tq∏)n(qβ) ≥ qβ + X Yk(Pn)k(1 - γPβ)ε
k≥0
=铲 + ε + X Y k+1(P∏ )k (Pfn - Pq )ε.
k≥0
Applying Πn we transform back into V space
vπ ≥ Ππ铲 +Ππε + X γk+1Ππ(Pq)k(Pq - Pq)ε
k≥0
≥ Πβ铲 +Ππε + X γk+1Ππ(Pq)k(Pqn - Pe)ε
k≥0
=vq + (Πn -Πq)ε+XYk+1Πn(Pqn)k(Pqn -Pqq)ε
k≥0
.
Notice that Ππ(Pf )kPn = Ππ(P∏π)kPnn = (ΠπP)k+1∏π = (Pv)k+1∏π, and in the same
manner, Πn (Pqn)k Pqq = (Pvn)k+1Πq. Therefore, we can write
vn ≥ vq - XYk(Pvn)k(Πq -Πn)ε,
k≥0
which may also be written as (12).	□
C.5 Unbounded Probability Ratios in the TV and KL constraints
To verify that TV and KL do not regulate the probability ratios, let’s consider a tiny example of
maximizing the objective function JPPO for a single state MDP with two actions {a0, a1}. Assume
a behavior policy β = {1, 0} and an estimated advantage Aq = {0, 1}. We search for a policy
π = {1 - α, α} that Maximize improvement step under the TV or KL constraints.
For a δ-TV constraint, ɪ (1 一 (1 一 ɑ) + α) = ɑ ≤ δ. The improvement step in this case is
Pa Ae∏i = α. Hence the solution is α = δ and the probability ratio ∏(aι)∕β(aι) is unconstrained
(and undefined). Similarly for a δ-KL constraint we get - log α ≤ δ and the improvement step is
identical. Hence α = e-δ and again, no constraint is posed on the probability ratio.
C.6 MAX-TV
Algorithm 2: Max-TV
Data: s, β, AD, δ
Result: {π(a∣s), a ∈ A}
begin
π(a∣s)《——β(a∣s), ∀a ∈ A
a = argmaXa∈A AD(s, a)
∆ = min{δ, 1 一 β(a∣s)}
π(a∣s)《——π(a∣s) + ∆
A — A
while ∆ > 0 do
a = argmina∈A AD(s, a)
△a = min{∆,β (a∣s)}
A《—A/a
△4---△ 一 △a
π(a∣s)《——π(a∣s) 一 △&
18
Under review as a conference paper at ICLR 2019
C.7 The PPO Objective Function
Recently, (Schulman et al., 2017) have suggested a new surrogate objective function, termed Proxi-
mal Policy Optimization (PPO), that heuristically should provide a reliable performance as in TRPO
without the complexity of a TRPO implementation:
J PPO (∏)= Es 〜β
X e(a|s)min (Ae Ga) β7⅛ ,Aβ Ga) clip fβ7⅛, 1 - ε,1 + ε
a∈A	β (a|s)	β (a|s)
where ε is a hyperparameter. PPO tries to ground the probability ratio by penalizing negative advan-
tage actions with probability ratios above 1-ε. In addition, it clips the objective for probability ratios
above 1 + ε so there is no incentive to move the probability ratio outside the interval [1 - ε, 1 + ε].
However, we show in the following that the solution of PPO is not unique and is dependent on the
initial conditions, parametric form and the specific optimization implementation. This was also ex-
perimentally found in (Henderson et al., 2017). The effect of all of these factors on the search result
is hard to design or predict. Moreover, some solutions may have unbounded probability ratios, in
this sense, JPPO is not safe.
First, notice that PPO maximization can be achieved by ad hoc maximizing each state since for each
state the objective argument is independent and there are no additional constraints. Now, for state
s, let’s divide A into two sets: the set of positive advantage actions, denoted A+, and the set of
negative advantage actions, A-. For convenience, denote Ci = 累弋)and βi = β(a∕s). Then, We
can write the PPO objective of state s as
JPPO (π, s) = X βiAiβ min (ci, 1 + ε) - X βi(-Aiβ) max(ci, 1 - ε) .
ai ∈A+	ai∈A-
Clearly maximization is possible (yet, still not unique) When setting all ci = 0 for ai ∈ A-, namely,
discarding negative advantage actions. This translates into a reroute maximization With parameters
(cmin , cmax )
(0,1+ε)
argmax[JPPO(π, s)] = arg max	βiAiβ min (ci, 1 + ε)
ci	ci
ai∈A+
arg max	ciβiAi
ci
ai∈A+
for ci ≤ 1 + ε. The only difference is that the sum i ciβi = 1 - △ may be less then 1. In this
case, let us take the unsafe approach and dispense △ to the highest ranked advantage. It is clear that
partition of △ is not unique, and even negative advantage actions may receive part of it as long as
their total probability is less than (1 -ε)βi. We summarize this procedure in the folloWing algorithm.
Algorithm 3: Ad hoc PPO Maximization
Data: s, β, Aβ, ε
Result: {π(a∣s), a ∈ A}
begin
A = {A+,A-}
A <— A+
∆ 4— 1
π (a | s)《—0 ∀a ∈ A.
while ∆ > 0 and |A| > 0 do
a = argmaXa∈A Ae(s, a)
△a = min{∆, (1 + ε)β(a∣s)}
A V— A/a
△ V----- △ - ∆a
π(a∣s) V- n(a|s) + ∆a
a = arg maxa∈A Aβ (s, a)
π(a∣s) V——π(a∣s) + △
19
Under review as a conference paper at ICLR 2019
D Taxi Experiment: Technical Details
In the paper, We presented three selection types of the subset of the optimal policy, i.e. S*: (1)
row selection (2) column selection and; (3) random selection. In the Taxi environment the states are
enumerated {sj}j5=001. For N players, the definitions for our the selection types are
Row Selection:
Column Selection:
sj
(mod500) < 250
Sj ∈ Si*
(j + i) (modN) <

Random Selection: Randomly (and uniformly) choose 250 states out of the 500 state for each
different Si* .
For the improvement step experiment, We evaluated the behavior policy β With a tabular representa-
tion, i.e., β(a∣s) = NNa (see the first item in Definition 4.1). For Monte-Carlo learning, we applies
Ns
the following update rule
Qe(s, a)4—Qe(s, a) + α (R — Qe(s, a)),
(13)
where α = 0.1 is a learning rate and Rs,a(j) is a single instance of the total discounted return from
a state-action pair S, a, up to the terminal state. For Temporal Difference learning, we applied
Qe(s, a){—Qe(s, a) + α [r + γ^β(a0∣s0)Qβ(s0, a0) - Qe(s, a)J ,	(14)
with the same learning rate. Notice that in this experiment we did not evaluated Qe from a V e
evaluation since for the tabular case, β is perfectly calculated with β(a∣s) = NNa.
Ns
E Atari Experiment: Technical Details
E.1 Dataset Preprocessing
The dataset (Kurin et al., 2017) (Version 2) does not contain an end-of-life signal. Therefore, we
tracked the changes of the life icons in order to reconstruct the end-of-life signal. The only problem
with this approach was in Qbert where the last life period has no apparent icon but we could not
match a blank screen since, during the episode, life icons are flashed on and off. Thus, the last end-
of-life signal in Qbert is missing. Further, the dataset contained some defective episodes in which
the screen frames did not match the trajectories. We found them by comparing the last score in the
trajectory file to the score that appears in the last or near last frame.
We also found some discrepancies between the Javatari simulator used to collect the dataset and the
Stella simulator used by the OpenAI gym package where we evaluated the agents’ scores:
• The Javatari reward signal has an offset of -2 frames. We corrected this shift in the pre-
processing phase.
• The Javatari actions signal has an offset of 〜一2 frames (depending on the action type),
where it is sometimes recorded non-deterministically s.t. the character executed an action
in a frame before the action appeared in the trajectory file. We corrected this shift in the
preprocessing phase.
• The Javatari simulator is not deterministic, while Stella can be defined as deterministic.
This has the effect that icons and characters move in different mod4 order, which is crucial
when learning with frame skipping of 4. Thus, we evaluated the best offset (in terms of
score) for each game and sampled frames according to this offset.
• There is a minor difference in colors/hues and objects location between the two simulators.
20
Under review as a conference paper at ICLR 2019
E.2 Learning from Human Players
The Atari dataset introduced a further challenge of learning from human players. Contrary to RL
agents, while much of the developed theory in previous sections assumes observation of Markovian
policies, humans do not play with MR policies. We found that two significant sources of Non-
Markovian behavior are: (1) delayed response and (2) action repetition. The first happens due to
an unavoidable delay between eye and hand movement. In simple words, humans respond to a
past state. The second implies that actions depend on action history, which violates the Markovian
assumption. Practically, when using a NN classifier, such Non-Markovian behavior hurts the classi-
fication since the network learns to predict past actions. We found that training the network to predict
an action in the near future (6 frames away, i.e. 0.1 seconds) can mitigate such a non-Markovian
nature. This way there is less correlation between the present state and the executed action, and the
human delayed response is mitigated.
E.3 Network Architecture
A finite dataset introduces overfitting issues. To avoid overfitting, DQN constantly updates a replay
buffer with new samples. In a finite dataset this is not possible, but, contrary to many Deep Learning
tasks, partitioning into training and validation sets is also problematic: random partitions introduce
a high correlation between training and testing, and blocking partitioning (Racine, 2000) might miss
capturing parts of the action-states space. Moreover, the ultimate learning goal, i.e. the playing
score, is not necessarily captured via the loss function score. In LfO, evaluating the agent’s score to
avoid overfitting violates the assumption of learning only from the dataset. Fortunately, we found
that Dropout (Srivastava et al., 2014), as a source of regularization, improves the network’s resiliency
to overfitting. In addition, it has the benefit of better generalization to new unseen states since the
network learns to classify based on a partial set of features. We added two sources of dropout: (1)
in the input layer (25%) and (2) on the latent features i.e. before the last layer (50%).
We also found that for a finite dataset with a constant statistics, Batch Normalization (BN) can
increase the learning rate. Therefore, we used BN layer before the ReLu nonlinearities. Note that
for an online RL algorithm, BN may sometimes impede learning because the statistics may change
during the learning process.
To estimate the advantage for the PPO objective, we used a trick inspired by the Duelling DQN
architecture (Wang et al., 2015). We added a single additional output to the last layer of the Q net-
work which represents the value V β . The other |A| outputs represents the unnormalized advantages
{Ai }i and the Q-function outputs are therefore expressed as
Qe = Vβ + Ai- X Ajej,	(15)
j
where {βj }j are the outputs of the policy network. Note that Pi Qiβ = Vβ . The normalized
advantage is therefore Ae = Ai - Pj Ajej.
Finally, we also shaped a reward for an end-of-life signal (for all our experiments including DQfD).
Usually, DQN variants set an end-of-life signal only as a termination signal so that the agent learns
that near end-of-life states have a 0 value and, as a consequence, states that dodge termination are
preferred. Since RBI policy is stochastic and does not just choose the best one, it is helpful to
differentiate between near zero reward and termination. Thus, we added a negative reward (-1) for a
terminal state.
To summarize, we used two DQN style networks: one for e and the second for Qe . We used the
same network architecture as in (Mnih et al., 2015) except for the following add-ons:
•	A batch normalization layer before nonlinearities.
•	A 25% Dropout in the input layer.
•	A 50% Dropout in the latent features layer. To compensate for the dropped features we
expanded the latent layer size to 1024, instead of 512 as in the original DQN architecture.
•	An additional output that represents the state’s value.
21
Under review as a conference paper at ICLR 2019
The overall architecture is depicted in figure 5. For the behavioral cloning, we used the Cross-
Entropy Loss. For the value network, though the theoretical bound in the paper is calculated for the
L1 loss, we found that slightly better results are obtained with the M SE loss. This may be due to
better regression of outliers. All results reported in the paper used the MSE loss for value.
E.4 Learning Process
A single iteration of our learning process is depicted with the following PyTorch style pseudo-code.
Value and Behavioral networks are trained simultaneously on the same sample. The state is passed
through the beta net once in training mode (with dropout) and once in evaluation mode (without
dropout) for the advantage calculation.
22
Under review as a conference paper at ICLR 2019
beta_net. train ()
value_net. train ()
for sample in train_loader :
s , a, r_mc = sample
beta = beta_net ( s )
beta_net. eval ()
beta_eval = beta_net(s)
beta_net. train ()
q, adv, V = value_net(s, beta_eval)
q-a = q[a]
loss_v = MSELoSS(q_a, r_mc )
loss_beta = CroSSEntroPyLoSS ( beta , a)
#	execute gradient descent with Adam
#	optimization over loss-beta and loss _v
Figure 5: Network architecture
E.5 Evaluation
The execution of a RBI policy at evaluation time is depicted with the following pseudo-code.
beta_net. eval ()
value_net. eval ()
t=0
score = 0
s = env. reset ()
while not t :
beta = beta_net ( s )
q, adv , V = value_net ( s , beta )
pi = max_reroute ( beta , adv , c_min , c_max)
a = choice ( pi)
s , r , t = env. step (a)
s c ore += r
23
Under review as a conference paper at ICLR 2019
E.6 Hyperparameters tables
Table 1: Policy and Q-value networks Hyperparameters
Name	Value
Last linear layer size	1024
Optimizer	Adam
Learning Rate	0.00025
Dropout [first layer]	0.25
Dropout [last layer]	0.5
Minibatch	32
Iterations	1562500
Frame skip	4
Reward clip	-1,1	
Table 2: DQfD Hyperparameters
Name	Value
Optimizer	Adam
Learning Rate	0.0000625
n-steps	10
Target update period	16384
laE	0.8
Priority Replay exponent	0.4
Priority Replay IS exponent	0.6
Priority Replay constant	0.001
Other parameters	As in policy/value networks
	(except Dropout)
F Final Policy Performance Table
Table 3: Final scores table
Method	MsPacman	Qbert	Revenge	SpaceInvaders
Humans	3024	3401	1436	634
Behavioral cloning	1519	8562	1761	678
Reroute-(0.5, 1.5)	1550	12123	2379	709
Reroute-(0.25, 1.75)	1638	13254	2431	682
Reroute-(0, 2)	1565	14494	473	675
TV(0.25)	1352	5089	390	573
PPO(0.5)	1528	14089	388	547
DQfD	83	1404	1315	402
24