Under review as a conference paper at ICLR 2019
Generating Text through Adversarial
Training using Skip-Thought Vectors
Anonymous authors
Paper under double-blind review
Ab stract
In the past few years, various advancements have been made in generative mod-
els owing to the formulation of Generative Adversarial Networks (GANs). GANs
have been shown to perform exceedingly well on a wide variety of tasks pertaining
to image generation and style transfer. In the field of Natural Language Process-
ing, word embeddings such as word2vec and GLoVe are state-of-the-art methods
for applying neural network models on textual data. Attempts have been made for
utilizing GANs with word embeddings for text generation. This work presents an
approach to text generation using Skip-Thought sentence embeddings in conjunc-
tion with GANs based on gradient penalty functions and f-measures. The results
of using sentence embeddings with GANs for generating text conditioned on input
information are comparable to the approaches where word embeddings are used.
1	Introduction
Numerous efforts have been made in the field of natural language text generation for tasks such
as sentiment analysis (Zhang et al. (2018)) and machine translation (Gangi & Federico; Qian et al.
(2018)). Early techniques for generating text conditioned on some input information were template
or rule-based engines, or probabilistic models such as n-gram. In recent times, state-of-the-art results
on these tasks have been achieved by recurrent (Press et al.; Mikolov et al. (2010)) and convolutional
neural network models trained for likelihood maximization. This work proposes an approach for text
generation using Generative Adversarial Networks with Skip-Thought vectors.
GANs (Goodfellow et al. (2014)) are a class of neural networks that explicitly train a generator to
produce high-quality samples by pitting against an adversarial discriminative model. GANs output
differentiable values and hence the task of discrete text generation has to use vectors as differentiable
inputs. This is achieved by training the GAN with sentence embedding vectors produced by Skip-
Thought (Kiros et al. (2015)), a neural network model for learning fixed length representations of
sentences.
2	Related Works
Deep neural network architectures have demonstrated strong results on natural language generation
tasks (Xie (2017)). Recurrent neural networks using combinations of shared parameter matrices
across time-steps (Sutskever et al. (2014); Mikolov et al. (2010); Cho et al. (2014)) with different
gating mechanisms for easing optimization (Hochreiter & Schmidhuber (1997); Cho et al. (2014))
have found success in modeling natural language. Another approach is to use convolutional neu-
ral networks that reuse kernels across time-steps with attention mechanism to perform language
generation tasks (Kalchbrenner et al. (2016; 2014)).
Supervised learning with deep neural networks in the framework of encoder-decoder models has be-
come the state-of-the-art methods for approaching NLP problems (Young et al.). Stacked denoising
autoencoder have been used for domain adaptation in classifying sentiments (Glorot et al. (2011))
and combinatory categorical autoencoders demonstrate learning the compositionality of sentences
(Hermann & Blunsom (2013)). Recent text generation models use a wide variety of GANs such as
gradient policy based sequence generation framework (Yu et al. (2016)) and an actor-critic condi-
tional GAN to fill missing text conditioned on surrounding text (Fedus et al. (2018)) for performing
1
Under review as a conference paper at ICLR 2019
natural language generation tasks. Other architectures such as those proposed in Wang et al. (2017)
with RNN and variational auto-encoder generator with CNN discriminator and in Guo et al. (2017)
with leaky discriminator to guide generator through high-level extracted features have also shown
great results.
3	Skip-Thought Generative Adversarial Network (STGAN)
This section introduces Skip-Thought Generative Adversarial Network with a background on mod-
els that it is based on. The Skip-Thought model (Kiros et al. (2015)) induces embedding vectors for
sentences present in training corpus. These vectors constitute the real distribution for the discrimi-
nator network. The generator network produces sentence vectors similar to those from the encoded
real distribution. The generated vectors are sampled over training and decoded to produce sentences
using a Skip-Thought decoder conditioned on the same text corpus.
3.1	Skip-Thought Vectors
Skip-Thought is an encoder-decoder framework with an unsupervised approach to train a generic,
distributed sentence encoder. The encoder maps sentences sharing semantic and syntactic proper-
ties to similar vector representations and the decoder reconstructs the surrounding sentences of an
encoded passage. The sentence encoding approach draws inspiration from the skip-gram model in
producing vector representations using previous and next sentences.
The Skip-Thought model uses an RNN encoder with GRU activations (Chung et al. (2014)) and an
RNN decoder with conditional GRU, the combination being identical to the RNN encoder-decoder
of Cho et al. (2014) used in neural machine translation.
3.1.1	Skip-Thought Architecture
For a given sentence tuple (si-1, si, si+1), let wit denote the t-th word for sentence si, and let xit
denote its word embedding. The model has three components:
Encoder. Encoded vectors for a sentence si with N words wi, wi+1,...,wn are computed by iterating
over the following sequence of equations:
rt = σ(Wrxt + Urht-1)
zt = σ(Wzxt + Uzht-1)
~t = tanh(Wxt + U(rt	ht-1))
ht = (1 -zt) ht-1 +zt	~t
where hit is a hidden state at each time step and interpreted as a sequence of words wi1,...,win,~t is
the proposed state update at time t, zt is the update gate and rt is the reset gate. Both update gates
take values between zero and one.
Decoder. A neural language model conditioned on the encoder output hi serves as the decoder.
Bias matrices Cz , Cr , C are introduced for the update gate, reset gate and hidden state computation
by the encoder. Two decoders are used in parallel, one each for sentences si + 1 and si - 1. The
following equations are iterated over for decoding:
rt = σ(Wrdxt-1 + Urdht-1 + Crhi)
zt = σ(Wzdxt-1 +Uzdht-1 + Czhi)
~t = tanh(Wdxt-1 +Ud(rt	ht-1) +Chi)
hit+1 = (1 - zt)	ht-1 + zt	~t
Objective. For the same tuple of sentences, objective function is the sum of log-probabilities for the
forward and backward sentences conditioned on the encoder representation:
XlogP(wit+1|wi<+t1,hi) + X logP (wit-1|wi<-t1, hi)
tt
2
Under review as a conference paper at ICLR 2019
Figure 1: Model architecture: Skip-Thought Generative Adversarial Network
3.2	Generative Adversarial Networks
Generative Adversarial Networks (Goodfellow et al. (2014)) are deep neural net architectures com-
prised of two networks, contesting with each other in a zero-sum game framework. For a given data,
GANs can mimic learning the underlying distribution and generate artificial data samples similar
to those from the real distribution. Generative Adversarial Networks consists of two players - a
Generator and a Discriminator. The generator G tries to produce data close to the real distribution
P(x) from some stochastic distribution P(z) termed as noise. The discriminator D’s objective is to
differentiate between real and generated data G(z).
The two networks - generator and discriminator compete against each other in a zero-sum game.
The minimax strategy dictates that each network plays optimally with the assumption that the other
network is optimal. This leads to Nash equilibrium which is the point of convergence for GAN
model.
Objective. Goodfellow et al. (2014) have formulated the minimax game for a generator G, discrim-
inator D adversarial network with value function V (G, D) as:
min max V (D, G) =Ex〜Pdata(X)[logD(x)] + Ez〜pz(z)[log (1 - D(G(Z)))]
3.3	Model Architecture
The STGAN model uses a deep convolutional generative adversarial network, similar to the one used
in Radford et al.. The generator network is updated twice for each discriminator network update to
prevent fast convergence of the discriminator network.
The Skip-Thought encoder for the model encodes sentences with length less than 30 words using
2400 GRU units (Chung et al. (2014)) with word vector dimensionality of 620 to produce 4800-
dimensional combine-skip vectors. (?. The combine-skip vectors, with the first 2400 dimensions
being uni-skip model and the last 2400 bi-skip model, are used as they have been found to be the
best performing in the experiments1. The decoder uses greedy decoding taking argmax over soft-
max output distribution for given time-step which acts as input for next time-step. It reconstructs
sentences conditioned on a sentence vector by randomly sampling from the predicted distributions
with or without a preset beam width. Unknown tokens are not included in the vocabulary. A 620
dimensional RNN word embeddings is used with 1600 hidden GRU decoding units. Gradient clip-
ping with Adam optimizer (Kingma & Ba (2014)) is used, with a batch size of 16 and maximum
sentence length of 100 words for decoder.
3.4	Improving Training and Loss
The training process of a GAN is notably difficult as demonstrated by Salimans et al. (2016) and
several improvement techniques such as batch normalization, feature matching, historical averaging
(Salimans et al. (2016)) and unrolling GAN (Metz et al.) have been suggested for making the
training more stable. Training the Skip-Thought GAN often results in mode dropping (Arjovsky &
1 https://github.com/ryankiros/skip- thoughts/
3
Under review as a conference paper at ICLR 2019
Table 1: BLEU-2, BLEU-3 and BLEU-4 metric scores for different models with (a) test set as
reference, and (b) entire corpus as reference. ST: Skip-Thought, GAN: Generative Adversarial
Network, W: Wasserstein GP: Gradient Penalty
Model	Test set BLEU-3	Complete corpus reference		
		BLEU-2	BLEU-3	BLEU-4
STGAN	0.521	^^0.709	0.564	0.525
STGAN(minibatch)	0.526	0.745	0.607	0.531
STGAN-GP	0.558	0.791	0.621	0.547
STWGAN	0.582	0.833	0.669	0.580
STWGAN-GP	0.617	0.836	0.682	0.594
Bottou; Srivastava et al.) with a parameter setting where it outputs a very narrow distribution of
points. To overcome this, it uses minibatch discrimination by looking at an entire batch of samples
and modeling the distance between a given sample and all the other samples present in that batch.
The minimax formulation for an optimal discriminator in a vanilla GAN is Jensen-Shannon Distance
between the generated distribution and the real distribution. Arjovsky et al. (2017) used Wasserstein
distance or earth mover’s distance to demonstrate how replacing distance measures can improve
training loss for GAN. Gulrajani et al. (2017) have incorporated a gradient penalty regularizer in
WGAN objective for discriminator’s loss function. The experiments in this work use the above
f-measures to improve performance of Skip-Thought GAN on text generation.
4 Results and Discussion
4.1	Conditional Generation of Sentences.
GANs can be conditioned on data attributes to generate samples (Mirza & Osindero (2014); Radford
et al.). In this experiment, both the generator and discriminator are conditioned on Skip-Thought
encoded vectors (Kiros et al. (2015)). The encoder converts 70000 sentences from the BookCorpus
dataset collected in Zhu et al. (2015) with a training/test/validation split of 5/1/1 into vectors used
as real samples for discriminator. The decoded sentences are used to evaluate model performance
under corpus level BLEU-2, BLEU-3 and BLEU-4 metrics (Papineni et al.), once using only test
set as reference and then entire corpus as reference. Table 1 compares these results for different
architectures that have been experimented with in this paper.
4.2	Language Generation.
Language generation is done on a dataset comprising simple English sentences referred to as CMU-
SE2 in Rajeswar et al. (2017). The CMU-SE dataset consists of 44,016 sentences with a vocabulary
of 3,122 words. For encoding, the vectors are extracted in batches of sentences having the same
length. The samples represent how mode collapse is manifested when using least-squares distance
(Mao et al. (2016)) f-measure without minibatch discrimination. Table 2(a) contains sentences gen-
erated from STGAN using least-squares distance (Mao et al. (2016)) in which there was no mode
collapse observed, while 2(b) contains examples wherein it is observed. Table 2(c) shows gener-
ated sentences using gradient penalty regularizer(GAN-GP). Table 2(d) has samples generated from
STGAN when using Wasserstein distance f-measure as WGAN in Arjovsky et al. (2017)) and 2(e)
contains samples when using a gradient penalty regularizer term as WGAN-GP from (Gulrajani
et al. (2017)).
2https://github.com/clab/sp2016.11-731/tree/master/hw4/data
4
Under review as a conference paper at ICLR 2019
Table 2: Sample sentences generated from training on CMU-SE Dataset; mode collapse is overcome
by using minibatch discrimination. Formation of sentences further improved by changing f-measure
to Wasserstein distance along with gradient penalty regularizer.
Mode collapse	1. 2. 3. 4. 5.	it? it? it? it ? how would it ? it ? how would it ?
With minibatch discrimination	丁 2. 3. 4. 5.	it a bottle ? a glass bottle ? a glass bottle it ? it my hand a bottle ? the phone my hand it
With gradient penalty	丁 2. 3. 4. 5.	battery is eighteen percent Um ? what fine are cash please ? youre gonna go over the t- house . do you have a nice store around here? open this flight number six zero one.
Wasserstein STGAN	丁 2. 3. 4. 5.	we have new year S holidays, always. here you can nt see your suitcase , please show me how much is a transfer? i had a police take watch out of my wallet. here i collect my telephone card and telephone number
Wasserstein STGAN with gradient penalty	丁 2. 3. 4. 5.	my passport and a letter card with my card, please here on my telephone, mr. kimuras registration cards address. i can nt see some shopping happened . get him my camera found a person S my watch . delta airlines flight six zero two from six p.m. to miami, please?
4.3	Further Work
Another performance metric that can be computed for this setup has been described in Rajeswar et al.
(2017) which is a parallel work to this. Simple CFG3 and more complex ones like Penn Treebank
CFG generate samples (Eisner & Smith (2008)) which are used as input to GAN and the model is
evaluated by computing the diversity and accuracy of generated samples conforming to the given
CFG.
Skip-Thought sentence embeddings can be used to generate images with GANs conditioned on
text vectors for text-to-image conversion tasks like those achieved in Reed et al.; Bodnar (2018).
These embeddings have also been used to Models like neural-storyteller4 which use these sentence
embeddings can be experimented with generative adversarial networks to generate unique samples.
Acknowledgments
Daniel Ricks for Penseur: An interface for the Sent2Vec encoder and training code from the ”Skip-
Thought Vectors” paper.
Taehoon Kim for DCGAN-tensorflow: A tensorflow implementation of ”Deep Convolutional Gen-
erative Adversarial Networks”.
Dept of CSIS, BITS Hyderabad for providing Nvidia Titan XP GPUs for experimentation with code.
3http://www.cs.jhu.edu/~ jason/4 65/hw-grammar/extra-grammars/holygrail
4https://github.com/ryankiros/neural-storyteller
5
Under review as a conference paper at ICLR 2019
References
M. Arjovsky and L. Bottou. Towards Principled Methods for Training Generative Adversarial Net-
works. ArXiv e-prints.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. ArXiv e-prints, January 2017.
Cristian Bodnar. Text to image synthesis using generative adversarial networks.	CoRR,
abs/1805.00676, 2018.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation. Association for Compu-
tational Linguistics, 2014.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Jason Eisner and Noah A Smith. Competitive grammar writing. In Proceedings of the Third Work-
shop on Issues in Teaching Computational Linguistics, pp. 97-105. Association for Computa-
tional Linguistics, 2008.
W. Fedus, I. Goodfellow, and A. M. Dai. MaskGAN: Better Text Generation via Filling in the
ArXiv e-prints, January 2018.
Mattia Antonino Di Gangi and Marcello Federico. Deep neural machine translation with weakly-
recurrent units. CoRR, abs/1805.04185.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proceedings of the 28th international conference on
machine learning (ICML-11), pp. 513-520, 2011.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems 27, pp. 2672-2680. Curran Associates, Inc., 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems 30,
pp. 5767-5777. 2017.
Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. Long text generation via
adversarial training with leaked information. CoRR, abs/1709.08624, 2017.
Karl Moritz Hermann and Phil Blunsom. The role of syntax in vector space models of compositional
semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1, pp. 894-904, 2013.
SePP Hochreiter and Jurgen SChmidhuber. Long short-term memory. Neural Comput., 9(8), Novem-
ber 1997.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), 2014.
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray
Kavukcuoglu. Neural machine translation in linear time. CoRR, abs/1610.10099, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.	CoRR,
abs/1412.6980, 2014.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urta-
sun, and Sanja Fidler. Skip-thought vectors. arXiv preprint arXiv:1506.06726, 2015.
Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, and Zhen Wang. Multi-class generative
adversarial networks with the L2 loss function. CoRR, 2016.
6
Under review as a conference paper at ICLR 2019
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. CoRR, abs/1611.02163.
Tomas Mikolov, Martin Karafiat, LUkas BUrgeL Jan Cernocky, and Sanjeev KhUdanpur. Recurrent
neural network based language model. In INTERSPEECH, 2010.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784,
2014.
Kishore Papineni, Salim RoUkos, Todd Ward, and Wei-Jing ZhU. BleU: A method for aUtomatic
evalUation of machine translation. In Proceedings of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ’02.
Ofir Press, Amir Bar, Ben Bogin, Jonathan Berant, and Lior Wolf. LangUage generation with recUr-
rent generative adversarial networks withoUt pre-training. CoRR, abs/1706.01399.
Xin Qian, Ziyi Zhong, and Jieli ZhoU. MUltimodal machine translation with reinforcement learning.
CoRR, abs/1805.02356, 2018.
Alec Radford, LUke Metz, and SoUmith Chintala. UnsUpervised representation learning with deep
convolUtional generative adversarial networks. CoRR, abs/1511.06434.
Sai Rajeswar, Sandeep SUbramanian, Francis DUtil, Christopher Joseph Pal, and Aaron C. CoUrville.
Adversarial generation of natUral langUage. CoRR, abs/1705.10929, 2017.
Scott E. Reed, Zeynep Akata, Xinchen Yan, LajanUgen Logeswaran, Bernt Schiele, and Honglak
Lee. Generative adversarial text to image synthesis. CoRR, abs/1605.05396.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki CheUng, Alec Radford, and Xi Chen.
Improved techniqUes for training gans. In Proceedings of the 30th International Conference on
Neural Information Processing Systems, NIPS’16, 2016.
A. Srivastava, L. Valkov, C. RUssell, M. U. GUtmann, and C. SUtton. VEEGAN: RedUcing Mode
Collapse in GANs Using Implicit Variational Learning. ArXiv e-prints.
Ilya SUtskever, Oriol Vinyals, and QUoc V. Le. SeqUence to seqUence learning with neUral networks.
CoRR, abs/1409.3215, 2014.
Heng Wang, Zengchang Qin, and Tao Wan. Text generation based on generative adversarial nets
with latent variable. CoRR, abs/1712.00170, 2017.
Ziang Xie. NeUral text generation: A practical gUide. arXiv preprint arXiv:1711.09534, 2017.
Tom YoUng, DevamanyU Hazarika, SoUjanya Poria, and Erik Cambria. Recent trends in deep learn-
ing based natUral langUage processing. CoRR, abs/1708.02709.
Lantao YU, Weinan Zhang, JUn Wang, and Yong YU. Seqgan: SeqUence generative adversarial nets
with policy gradient. CoRR, abs/1609.05473, 2016.
Lei Zhang, ShUai Wang, and Bing LiU. Deep learning for sentiment analysis : A sUrvey. CoRR,
abs/1801.07883, 2018.
YUkUn ZhU, Ryan Kiros, Richard S. Zemel, RUslan SalakhUtdinov, RaqUel UrtasUn, Antonio Tor-
ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visUal explanations by
watching movies and reading books. 2015.
7