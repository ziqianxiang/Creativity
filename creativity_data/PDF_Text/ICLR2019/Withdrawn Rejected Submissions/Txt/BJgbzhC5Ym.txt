Under review as a conference paper at ICLR 2019
NECST: Neural Joint Source-Channel Coding
Anonymous authors
Paper under double-blind review
Ab stract
For reliable transmission across a noisy communication channel, classical results
from information theory show that it is asymptotically optimal to separate out the
source and channel coding processes. However, this decomposition can fall short
in the finite bit-length regime, as it requires non-trivial tuning of hand-crafted
codes and assumes infinite computational power for decoding. In this work, we
propose Neural Error Correcting and Source Trimming (NECST) codes to jointly
learn the encoding and decoding processes in an end-to-end fashion. By adding
noise into the latent codes to simulate the channel during training, we learn to both
compress and error-correct given a fixed bit-length and computational budget. We
obtain codes that are not only competitive against several capacity-approaching
channel codes, but also learn useful robust representations of the data for down-
stream tasks such as classification. Finally, we learn an extremely fast neural
decoder, yielding almost an order of magnitude in speedup compared to standard
decoding methods based on iterative belief propagation.
1	Introduction
We consider the problem of encoding images as bit-strings so that they can be reliably transmitted
across a noisy communication channel. Classical results from (Shannon (1948)) show that for a
memoryless communication channel, as the image size goes to infinity, it is optimal to separately:
1) compress the images as much as possible to remove redundant information (source coding) and
2) use an error-correcting code to re-introduce redundancy, which allows for reconstruction in the
presence of noise (channel coding). This separation theorem has been studied in a wide variety of
contexts and has also enjoyed great success in the development of new coding algorithms.
However, this elegant decomposition suffers from two critical limitations in the finite bit-length
regime. First, without an infinite number of bits for transmission, the overall distortion (reconstruc-
tion quality) is a function of both source and channel coding errors. Thus optimizing for both: (1) the
number of bits to allocate to each process and (2) the code designs themselves is an extremely dif-
ficult task. Second, maximum-likelihood decoding is in general NP-hard (Berlekamp et al. (1978)).
Thus obtaining the accuracy that is possible in theory is contingent on the availability of infinite
computational power for decoding, which is impractical for real-world systems. Though several
lines of work have explored various relaxations of this problem to achieve tractability (Koetter &
Vontobel (2003), Feldman et al. (2005), Vontobel & Koetter (2007)), many decoding systems rely on
heuristics and early stopping of iterative approaches for scalability, which can be highly suboptimal.
To address these challenges we propose Neural Error Correcting and Source Trimming (NECST)
codes, a deep learning framework for jointly learning to compress and error-correct an input image
given a fixed bit-length budget. Three key steps are required. First, we use neural networks to encode
each image into a suitable bit-string representation, sidestepping the need to rely on hand-designed
coding schemes that require additional tuning for good performance. Second, we simulate a dis-
crete channel within the model and inject noise directly into the latent codes to enforce robustness.
Third, we amortize the decoding process such that after training, we can scale our model to mas-
sively large datasets. These components pose significant optimization challenges due to the inherent
non-differentiability of discrete latent random variables. We overcome this issue by leveraging re-
cent advances in unbiased low-variance gradient estimation for variational learning of discrete latent
variable models, and train the model using a variational lower bound on the mutual information be-
tween the images and their binary representations to obtain good, robust codes. At its core, NECST
1
Under review as a conference paper at ICLR 2019
can also be seen as an implicit deep generative model of the data derived from the perspective of the
joint-source channel coding problem.
In experiments, we test NECST on several grayscale and RGB image datasets, obtaining improve-
ments over industry-standard compression (e.g, JPEG) and error-correcting codes (e.g., low density
parity check codes). We also learn an extremely fast neural decoder, yielding almost an order of
magnitude (two orders for magnitude for GPU) in speedup compared to standard decoding methods
based on iterative belief propagation. Additionally, we show that in the process we learn discrete
representations of the input images that are useful for downstream tasks such as classification.
2	Source and channel coding
2.1	Preliminaries
We consider the problem of reliably communicating data across a noisy channel using a system that
can detect and correct errors. Let X be a space of possible inputs (e.g., images), and pdata(x) be a
source distribution defined over x ∈ X. The goal of the communication system is to encode samples
X 〜Pdata(X) as messages in a codeword space Y = {0,1}m. The messages are transmitted over a
noisy communication channel, where they are corrupted to become noisy codewords in Y. Finally,
a decoder produces a reconstruction X ∈ X of the original input X from a received noisy code y.
The goal is to minimize the overall distortion (reconstruction error) kx 一 Xk in 'ι or '2 norm while
keeping the message length m as small as possible (rate). In the absence of channel noise, low
reconstruction errors can be achieved using a compression scheme to encode inputs X 〜Pdata(X) as
succinctly as possible (e.g., JPEG). In the presence of channel noise, longer messages are typically
needed to redundantly encode the information and recover from errors (e.g., parity check bits).
2.2	The Joint Source-Channel Coding Problem
Based on fundamental results due to Shannon (Shannon, 1948), existing systems adhere to the fol-
lowing pipeline. A source encoder compresses the source image into a bit-string with the minimum
number of bits possible. A channel encoder re-introduces redundancies, part of which were lost
during source coding to prepare the codeword y for transmission. The decoder then leverages the
channel coding scheme to infer the original signal y, producing an approximate reconstruction X.
In the separation theorem, Shannon proved that the above approach is optimal in the limit of in-
finitely long messages. That is, we can minimize distortion by optimizing the source and channel
coding processes independently. However, given a finite bit-length budget, the relationship between
rate and distortion becomes more complex. The more bits that we reserve for compression, the fewer
bits we have remaining to construct the best error-correcting code and vice versa. Balancing the two
sources of distortion through the optimal bit allocation, in addition to designing the best source and
channel codes, makes this joint source-channel coding (JSCC) problem challenging.
In addition to these design challenges, real-world communication systems face computational and
memory constraints that hinder the straightforward application of Shannon’s results. Specifically,
practical decoding algorithms rely on approximations that yield suboptimal reconstruction accura-
cies. The information theory community has studied this problem extensively, and proposed dif-
ferent bounds for finite bit-length JSCC in a wide variety of contexts (Pilc (1967), Csiszar (1982),
Kostina & VerdU (2013)). Rather than relying on hand-crafted coding schemes that may require
additional tuning in practice, we propose to learn the appropriate bit allocations and coding mecha-
nisms using a flexible deep learning framework.
3	Neural Source and Channel codes
Given the above considerations, we explore a learning-based approach to the JSCC problem. To do
so, we consider a flexible class of codes parameterized by a neural network and jointly optimize
for the encoding and decoding procedure. This approach is inspired by recent successes in training
(discrete) latent variable generative models, such as VAEs (Neal (1992), Rolfe (2016), van den Oord
et al. (2017)). We explore the connection to different types of autoencoders in section 4.1.
2
Under review as a conference paper at ICLR 2019
3.1	Coding Process
T .	W ，广F	1	∙ 11	1	. ∙	. 1	1	1	♦	1	1	1
Let X, Y , Y, X be random variables denoting the inputs, codewords, noisy codewords, and recon-
StrUctions respectively. We model their joint distribution p(x, y, y, X) using the following graphical
ʌ ʌ
Y → Y → X as:
model X →
p(x, y, y, X) = Pdata(X)qencoder(b∣x; Φ)Pchannel(y |b； €)Pdecoder(X|y； θ)
(1)
In equation 1, pdata(X) denotes the distribution over inputs X. It is not known explicitly in practice,
and only accessible through samples. pchannel (y|yb; ) is the channel model, where we specifically
focus on the binary symmetric channel (BSC) case. The BSC independently flips each bit in the
codeword with probability (e.g., 0 → 1). Therefore, Y takes values in Y = Y = {0, 1}m and
m
Pchannel(y|b； e) = Y 杂唬(1 - e)y出b®
i=1
where ㊉ denotes addition modulo 2 (i.e., an eXclusive OR).
A StochaStic encoder qencoder(y∣x; Φ) generates a codeword y given an input x. Specifically, We
model each bit yi in the code with an independent Bernoulli random vector. We model the parame-
ters of this Bernoulli with a neural network fφ(∙) (an MLP or a CNN) parameterized by φ:
m
qencoder(y∣X, Φ) = Y σ(fφ(xi))yi (1 - σ(fφ(xi )))(1-yi)
i=1
where σ(z) denotes the sigmoid function.
Similarly, We posit a probabilistic decoder PdeCOder(X|y； θ) parameterized by an MLP/CNN that,
given y, generates a decoded image X. We model each pixel as a factorized Gaussian with a fixed,
isotropic covariance to yield: X|y 〜N(fθ(y), σ21), where fθ(∙) denotes the decoder network.
4	End-to-End Variational Learning of Neural Codes
In order to learn an effective coding scheme, we maximize the mutual information between the
input X and the corresponding noiSy codeword Y (Barber & Agakov (2006)). That is, the code
y should be robust to partial corruption; even its noisy instantiation y should preserve as much
information about the original input X as possible (MacKay (2003)).
First, we note that we can analytically compute the encoding distribution qnoisy_enc(y|x； 3 Φ) after it
has been perturbed by the channel by marginalizing over y:
qnoisy_enc(y|x; 3 φ) =): qencoder(y|x; φ)pchannel(y|y; E)
y∈Y
which yields:
m
qnoisy.enc(y|x； φ,6 = Y (σ(fφ(xi)) - 2σ(fφ(xi))e + e)yi (1 - σ(fφ(xi)) + 2σ(fφ(xi))e - e)(1-yi)
i=1
It is relatively straightforward to handle other types of channel noise; we provide additional exam-
ples of the binary erasure channel (BEC) and deletion channel in the Appendix.
We get the following optimization problem:
max I (X,Y; φ,e) = H (X) - H (X |Y; φ,e)
φ
=Ex〜Pdata(X)Ey〜qnoisy_enc(y|x；3。)[logP(XE, 0)] + const.	⑵
≥ Ex~Pdata(x)Ey~qnoisy.enc(y∣X{,Φ) [logPdecoder(XIy； θ)] + const.	(3)
where P(X|y; E, φ) is the true (and intractable) posterior from Eq. 1 andPdecoder(X|y; θ) is an amor-
tized variational approximation. The true posterior P(XIy; E, φ) — the posterior probability over
3
Under review as a conference paper at ICLR 2019
possible inputs x given the received noisy codeword y — is the best possible decoder. However, it
is also often intractable to evaluate and optimize. We therefore use a tractable variational approx-
imation Pdecoder(X∣y; θ). Crucially, this variational approximation is amortized (Kingma & Welling
(2013)), and is the inference distribution that will actually be used for decoding at test time. Because
of amortization, decoding is guaranteed to be efficient, in contrast with existing error correcting
codes, which typically involve NP-hard MPE inference queries.
Given any encoder (φ), we can find the best amortized variational approximation by maximizing the
lower bound (3) as a function of θ. Therefore, the NECST training objective is given by:
mm ax Ex ^Pdata ( x ) Ey ^ qnoisy-enc ( y | x ； e,φ ) [log PdeCoder(XIy； θ)]
θ,φ
In practice, we approximate the expectation of the data distribution Pdata(x) with a finite dataset D:
mm ax〉: Ey~qnoisy_enc(y|x；30) [log PdeCoder(X |y ； θ)] ≡ L(φ, θ; x, E)	(4)
,φ x∈D
Thus we can jointly learn the encoding and decoding scheme by optimizing the parameters φ and
θ . In this way, the encoder (φ) is “aware” of the computational limitations of the decoder (θ), and is
optimized accordingly. However, a main learning challenge is that we are not able to backpropagate
directly through the discrete latent variable y; we elaborate upon this point further in Section 5.1.
The solution will depend on the number of available bits m, the noise level E, and the structure of the
input data X. For intuition, consider the case m = 0. In this case, no information can be transmitted,
and the best decoder will fit a single Gaussian to the data. When m = 1 and E = 0 (noiseless
case), the model will learn a mixture of 2m = 2 Gaussians. However, if m = 1 and E = 0.5,
again no information can be transmitted, and the best decoder will fit a single Gaussian to the data.
Adding noise forces the model to decide how it should effectively partition the data such that (1)
similar items are grouped together in the same cluster; and (2) the clusters are ”well-separated”
(even when a few bits are flipped, they do not ”cross over” to a different cluster that will be decoded
incorrectly). Thus, from the perspective of unsupervised learning, NECST attempts to learn robust
binary representations of the data.
4.1	NECST as a Generative Model
The objective function in equation (4) closely resembles those commonly used in generative mod-
eling frameworks; in fact, NECST can also be viewed as a generative model. In its simplest form,
our model with a noiseless channel (i.e., E = 0), deterministic encoder, and deterministic decoder is
identical to a traditional autoencoder (Bengio et al. (2007)). Once channel noise is present (E > 0)
and the encoder/decoder become probabilistic, NECST begins to more closely resemble other varia-
tional autoencoding frameworks. Specifically, NECST is similar to Denoising Autoencoders (DAE)
(Vincent et al. (2008)), except that it is explicitly trained for robustness to partial destruction of the
latent space, as opposed to the input space.
NECST is also a variant of the VAE, with two nuanced distinctions. While both models learn a
joint distribution over the observations and latent codes, the VAE: (1) optimizes a variational lower
bound to the marginal log-likelihood P(X) as opposed to the mutual information I(X, Y ), and
(2) explicitly posits a prior distribution P(Y ) over the latent variables. Their close relationship is
evidenced by a line of work on rate-distortion optimization in the context of VAEs (Balle et al.
(2016), Alemi et al. (2017)), as well as other information-theoretic interpretations of the VAE’s
information preference (Hinton &Van Camp (1993), Honkela & Valpola (2004), Chen et al. (2016)).
We also note that existing autoencoders are aimed at compression, while for sufficiently large m
NECST will attempt to learn lossless compression with added redundancy for error correction.
Finally, NECST can be seen as a discrete version of the Uncertainty Autoencoder (UAE) (Grover
& Ermon (2018)). The two models share identical objective functions with two notable differences:
For NECST, (1) the latent codes Y are discrete random variables, and (2) the noise model is no
longer continuous. The special properties of the continuous UAE carry over directly to its discrete
counterpart. Grover & Ermon (2018) proved that under certain conditions, the UAE specifies an
implicit generative model (Diggle & Gratton (1984), Mohamed & Lakshminarayanan (2016)). We
restate their major theorem and extend their results here.
4
Under review as a conference paper at ICLR 2019
Starting from any data point x(0) 〜 X , we define a Markov chain over X × Y with the following
transitions:
y(右)〜qnoisy_enc(y|x"); φ, E) , x("“〜Pdecoder(X|y"); θ)	(5)
Theorem 1. For any fixed value of φ and > 0, suppose that the NECST objective is globally
maximizedfor some choice of θ* so that inequality (3) is tight. Then the Markov Chain (5) with pa-
rameters φ and θ* is ergodic and its stationary distribution is given by Pdata(x)qnoiyenc(y | x; e, φ).
Proof. See Appendix A.1.	□
Hence NECST has an intractable likelihood but can generate samples from Pdata by running the chain
above. Samples initialized from both random noise and test data can be found in Appendix D.5.
5 Experimental Results
We first review the optimization challenges of training discrete latent variable models and elaborate
on our training procedure. Then to validate our work, we first assess NECST’s compression and
error correction capabilities against a combination of two widely-used compression (JPEG, VAE)
and channel coding (ideal code, LDPC (Gallager (1962))) algorithms. We experiment on randomly
generated length-100 bitstrings, MNIST (LeCun (1998)), Omniglot (Lake et al. (2015)), Street View
Housing Numbers (SVHN) (Netzer et al. (2011)), and CelebA (Liu et al. (2015)) datasets to account
for different image sizes and colors. Next, we test the decoding speed of NECST’s neural decoder
and find that it performs upwards of an order of magnitude faster than standard decoding algorithms
based on iterative belief propagation (and two orders of magnitude on GPU). Finally, we assess the
quality of the latent codes after training, and examine interpolations in latent space as well as how
well the learned features perform for downstream classification tasks.
5.1	Optimization Procedure
Recall the NECST objective in equation 4. While obtaining Monte Carlo-based gradient estimates
with respect to θ is easy, gradient estimation with respect to φ is challenging because these param-
eters specify the Bernoulli random variable qnoisy_enc(y|x； e, φ). The commonly used reparameteri-
zation trick cannot be applied in this setting, as the discrete stochastic unit in the computation graph
renders the overall network non-differentiable (Schulman et al. (2015)).
A simple alternative is to use the score function estimator in place of the gradient, as defined in
the REINFORCE algorithm (Williams (1992)). However, this estimator suffers from high variance,
and several others have explored different formulations and control variates to mitigate this issue
(Wang et al. (2013), Gu et al. (2015), Ruiz et al. (2016), Tucker et al. (2017), Grathwohl et al.
(2017)). Others have proposed a continuous relaxation of the discrete random variables, as in the
Gumbel-softmax (Jang et al. (2016)) and Concrete (Maddison et al. (2016)) distributions.
To preserve the hard “discreteness” of the latent codes, we used VIMCO (Mnih & Rezende (2016)),
a multi-sample variational lower bound objective for obtaining low-variance gradients. VIMCO
constructs leave-one-out control variates using its samples, as opposed to the single-sample objective
NVIL (Mnih & Gregor (2014)) which requires learning additional baselines during training. Thus,
we used the 5-sample VIMCO objective in subsequent experiments for the optimization procedure,
leading us to our final multi-sample (K = 5) objective:
1K
L (φ, θ; x, E) = mJaχ): EyLK〜qnoisy_enc(y|x;eM log ~r7 ) ;PdeCoder(XIy-; θ)	(6)
,φ x∈D	L	i=1	_
5.2	Fixed distortion: JPEG + Ideal channel code
In this experiment, we compare the performances of: (1) NECST and (2) JPEG + ideal channel
code in terms of compression. Specifically, we fix the number of bits m used by NECST to source
and channel code and obtain the corresponding distortion levels (reconstruction errors) at various
noise levels E. For fixed m, distortion will increase with E. Then, for each noise level we estimate
the number of bits an alternate system using JPEG and an ideal channel code — the best that is
5
Under review as a conference paper at ICLR 2019
theoretically possible — would have to use to match NECST’s distortion levels. Assuming an ideal
channel code implies that all messages will be transmitted perfectly across the noisy channel; thus
the resulting distortion will only be a function of the compression mechanism. We note that some
approximations commonly used in the information theory community are used to obtain the analytic
expressions used for this estimate. Full details and additional results may be found in Appendix D.1.
We find that with the exception of random data (Appendix D.1), NECST excels at compression; Fig-
ure 1 shows that in order to achieve the same level of distortion as NECST, the JPEG-ideal channel
code system requires a much larger number bits across all noise levels. In particular, we note that
NECST is slightly more effective than JPEG at pure compression ( = 0), and becomes significantly
better at higher noise levels (e.g., for = 0.4, NECST requires 20× less bits). The negative result
for random bits is to be expected because the data has no exploitable statistical dependencies.
Oooo
Oooo
0 5 0 5
2 11
S+Jm5U-P8ωuIlPio⅞」9qlunN
(a) Binary Omniglot	(b) celebA
Figure 1: Theoretical m required for JPEG + ideal channel code to match NECST’s distortion.
5.3	Fixed Rate: source coding + LDPC
Next, we compare the performances of: (1) NECST and (2) discrete VAE + LDPC in terms of
distortion. Specifically, we fix the bit-length budget for both systems and evaluate whether learning
compression and error-correction jointly (NECST) improves reconstruction errors (distortion) for
the same rate. We do not report a comparison against JPEG + LDPC as we were often unable to
obtain valid JPEG files to compute distortions after imperfect LDPC decoding. However, as we have
seen in the JPEG + ideal channel code experiment, we can reasonably assume that the JPEG-LDPC
system would have displayed worse performance.
We experiment with a discrete VAE with a uniform prior over the latent codes for encoding. To fix
the total number of bits that are transmitted through the channel, we double the length of the VAE
encodings with an LDPC channel code to match NECST’s latent code dimension m. We note from
Figure 2a that although the resulting distortion is similar, NECST outperforms the VAE across all
noise levels for image datasets. These results suggest that NECST’s learned coding scheme performs
at least as well as LDPCs, an industrial-strength, widely deployed class of error correcting codes.
5.4	Decoding Time
Another advantage of NECST over traditional channel codes is that after training, the amortized
decoder can very efficiently map the transmitted code into its best reconstruction at test time. Other
sparse graph-based coding schemes such as LDPC, however, are more time-consuming because
decoding involves an NP-hard optimization problem typically approximated with multiple iterations
of belief propagation.
To compare the speed of our neural decoder to LDPC’s belief propagation, we fixed the number
of bits that were transmitted across the channel and averaged the total amount of time taken for
decoding across ten runs. The results are shown below in Figure 2b for the binarized MNIST and
Omniglot datasets, and refer the reader to Appendix D.7 for the simulated random data. On CPU,
NECST’s decoder averages an order of magnitude faster than the LDPC decoder running for 50
iterations, which is based on an optimized C implementation. On GPU, NECST displays two orders
of magnitude in speedup.
6
Under review as a conference paper at ICLR 2019
BinaryMNIST	0.0	0.1	0.2	0.3	0.4	().5
50-bit VAE+100-bit LDPC	^TOΓ	1O6Γ	IW	Tl20^	^TOΓ	-ra^
100-bit NECST	0.039	0.052	0.066	0.091	0.125	0.131
Binary Omniglot	0.0	0.1	0.2	0.3	0.4	0.5
100-bit VAE+ 200-bit LDPC	^0W	"W	^w	1O9Γ	"αior	^αioo^
200-bit NECST	0.035	0.045	0.057	0.070	0.080	0.080
Random bits	0.0	0.1	0.2	0.3	0.4	().5
25-bit VAE +50-bit LDPC 50-bit NECST	^0352^ 0.291	^037Γ 0.357	"04TΓ 0.407	-044Γ 0.456	-017Γ 0.500	"W 0.498
(a) NECST vs. VAE-LDPC distortion
(b) Average decoding times
Figure 2: (Left) Reconstruction error of NECST vs. VAE + rate-1/2 LDPC codes. Lower is better.
(Right) Average decoding times for NECST vs. 50 iterations of LDPC decoding.
6 NECST for Robust Representation Learning
In addition to its ability to source and channel code effectively, NECST also serves as an im-
plicit generative model that yields robust and interpretable latent representations and realistic sam-
ples from the underlying data distribution. Specifically, in light of Theorem 1, we can think of
qencoder(yb|x; φ) as mapping images x to latent representations yb.
6.1	Interpolation in latent space
To assess whether the model has: (1) injected redundancies into the learned codes and (2) learned
interesting features, we interpolate between different data points in latent space and qualitatively
observe whether the model captures semantically meaningful variations in the data. We select two
test points to be the start and end, sequentially flip one bit at a time in the latent code, and pass the
altered code through the decoder to observe how the reconstruction changes. In Figure 3, we show
two illustrative examples from the MNIST digits. From the starting digit, each bit-flip slowly alters
characteristic features such as rotation, thickness, and stroke style until the digit is reconstructed to
something else entirely. We note that because NECST is trained to encode redundancies, we do not
observe a drastic change per flip. Rather, it takes a significant level of corruption for the decoder
to interpret the original digit as another. Also, due to the i.i.d. nature of the channel noise model,
we observe that the sequence at which the bits are flipped do not have a significant effect on the
reconstructions. Additional interpolations may be found in Appedix D.4.
gyyy&ssssssssss
y 5- y y y s 5 5 5 5 3 3 ʒ › 5
3 5 5 5 3 3
(a)	MNIST: 6 → 3 leads to an intermediate 5
^^^S33333332222
NN2NN；?7NN7N44qq
qqqqqqqqqqqqqqq
q q q 4
(b)	MNIST: 8 → 4 leads to an intermediate 2 and 9
Figure 3: Latent space interpolation, where each image is obtained by flipping one additional latent
bit and passing through the decoder.
6.2	Downstream classification
To demonstrate that the latent representations learned by NECST are useful for downstream clas-
sification tasks, we extract the binary features and train eight different classification algorithms: k-
nearest neighbors (KNN), decision trees (DT), random forests (RF), multilayer perceptron (MLP),
AdaBoost (AdaB), Quadratic Discriminant Analysis (QDA), and support vector machines (SVM).
As shown on the leftmost numbers in Table 1, the simple classifiers perform reasonably well in the
digit classification task for MNIST ( = 0). We observe that with the exception of KNN, all other
7
Under review as a conference paper at ICLR 2019
classifiers achieve higher levels of accuracy when using features that were trained with simulated
channel noise ( = 0.1, 0.2).
Table 1: Classification accuracy on MNIST/noisy MNIST using 100-bit features from NECST.
Noise	KNN	DT	RF	MLP	AdaB	NB	QDA	SVM
0	0.95/0.86	0.65/0.54	0.71/0.59	0.93/0.87	0.72/0.65	0.75/0.65	0.56/0.28	0.88/0.81
0.1	0.95/0.86	0.65/0.59	0.74/0.65	0.934/0.88	0.74/0.72	0.83/0.77	0.94/0.90	0.92/0.84
0.2	0.94/0.86	0.78/0.69	0.81/0.76	0.93/0.89	0.78/0.80	0.87/0.81	0.93/0.90	0.93/0.86
To further test the hypothesis that NECST trained with the noisy channel learns more robust la-
tent features, we freeze the pre-trained model and evaluate classification accuracy using a ”noisy”
MNIST dataset. We synthesized noisy MNIST by adding E 〜N(0,0.5) noise to all pixel values in
MNIST, and ran the same experiment as above. As shown in the rightmost numbers of Table 1, we
again observe that most algorithms show improved performance with added channel noise.
7	Related work
There has been a recent surge of work applying deep learning and generative modeling techniques
to lossy image compression, many of which compare favorably against industry standards such as
JPEG,JPEG2000, and WebP (Toderici et al. (2015), Balle et al. (2016), Toderici et al. (2017)). Theis
et al. (2017) use compressive autoencoders that learn the optimal number of bits to represent images
based on their pixel frequencies. Balle et al. (2018) use a variational autoencoder (VAE) (Kingma
& Welling (2013) Rezende et al. (2014)) with a learnable scale hyperprior to capture the image’s
partition structure as side information for more efficient compression. Santurkar et al. (2017) use
adversarial training (Goodfellow et al. (2014)) to learn neural codecs for compressing images and
videos using DCGAN-style ConvNets (Radford et al. (2015)). Yet these methods focus on source
coding only, and do not consider the setting where the compression must be robust to channel noise.
In a similar vein, there has been growing interest on leveraging these deep learning systems to
sidestep the use of hand-designed codes. Several lines of work train neural decoders based on known
coding schemes, sometimes learning more general decoding algorithms than before (Nachmani et al.
(2016), Gruber et al. (2017), Cammerer et al. (2017), Dorner et al. (2017)). (Kim et al. (2018a), Kim
et al. (2018b)) parameterize sequential codes with recurrent neural network (RNN) architectures
that achieve comparable performance to capacity-approaching codes over the additive white noise
Gaussian (AWGN) and bursty noise channels. However, source coding is out of the scope for these
methods that focus on learning good channel codes.
The problem of end-to-end transmission of structured data, on the other hand, is less well-studied.
Farsad et al. (2018) use RNNs to communicate text over a discrete, binary erasure channel (BEC)
and are able to preserve the words’ semantics. The most similar to our work is that of Bourtsoulatze
et al. (2018), who use autoencoders for transmitting images over the AWGN and slow Rayleigh
fading channels, which are continuous. We provide a holistic treatment of various discrete noise
models and show how NECST can also be used for unsupervised learning.
8	Discussion
We described how NECST can be used to learn an efficient joint source-channel coding scheme by
simulating a noisy channel during training. We showed that the model: (1) is competitive against a
combination of industry standard compression and error-correcting codes, (2) learns an extremely
fast neural decoder through amortized inference, and (3) learns a latent code that is not only robust
to corruption, but also useful for general downstream tasks such as classification.
One limitation of NECST is that we need to train the model separately for different code-lengths
and datasets; in its current form, it can only handle fixed-length codes. Extending the model to
streaming or adaptive learning scenarios that allow for learning variable-length codes is an exciting
direction for future work. Another direction would be to analyze the characteristics and properties
of the learned latent codes under different discrete channel models, such as erasure channels.
8
Under review as a conference paper at ICLR 2019
References
Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, RifA Saurous, and Kevin Murphy. An
information-theoretic analysis of deep latent-variable models. arXiv preprint arXiv:1711.00464,
2017.
Johannes Balle, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.
arXiv preprint arXiv:1611.01704, 2016.
Johannes Balle, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational
image compression with a scale hyperprior. arXiv preprint arXiv:1802.01436, 2018.
David Barber and Felix V Agakov. Kernelized infomax clustering. In Advances in neural informa-
tion processing systems, pp. 17-24, 2006.
Yoshua Bengio, Yann LeCun, et al. Scaling learning algorithms towards ai. 2007.
Elwyn Berlekamp, Robert McEliece, and Henk Van Tilborg. On the inherent intractability of certain
coding problems (corresp.). IEEE Transactions on Information Theory, 24(3):384-386, 1978.
Eirina Bourtsoulatze, David Burth Kurka, and Deniz Gunduz. Deep joint source-channel coding for
wireless image transmission. arXiv preprint arXiv:1809.01733, 2018.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
Sebastian Cammerer, Tobias Gruber, Jakob Hoydis, and Stephan ten Brink. Scaling deep learning-
based decoding of polar codes via partitioning. In GLOBECOM 2017-2017 IEEE Global Com-
munications Conference, pp. 1-6. IEEE, 2017.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016.
Imre Csiszar. Linear codes for sources and source networks: Error exponents, universal coding.
IEEE Transactions on Information Theory, 28(4):585-592, 1982.
Peter J Diggle and Richard J Gratton. Monte carlo methods of inference for implicit statistical
models. Journal of the Royal Statistical Society. Series B (Methodological), pp. 193-227, 1984.
Sebastian Dorner, Sebastian Cammerer, Jakob Hoydis, and Stephan ten Brink. On deep learning-
based communication over the air. In Signals, Systems, and Computers, 2017 51st Asilomar
Conference on, pp. 1791-1795. IEEE, 2017.
Nariman Farsad, Milind Rao, and Andrea Goldsmith. Deep learning for joint source-channel coding
of text. arXiv preprint arXiv:1802.06832, 2018.
Jon Feldman, Martin J Wainwright, and David R Karger. Using linear programming to decode
binary linear codes. IEEE Transactions on Information Theory, 51(3):954-972, 2005.
Robert Gallager. Low-density parity-check codes. IRE Transactions on information theory, 8(1):
21-28, 1962.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint
arXiv:1711.00123, 2017.
Aditya Grover and Stefano Ermon. Variational compressive sensing using uncertainty autoencoders.
In 2018 UAI Workshop on Uncertainty in Deep Learning, pp. 1-10. UAI, 2018.
9
Under review as a conference paper at ICLR 2019
Tobias Gruber, Sebastian Cammerer, Jakob Hoydis, and Stephan ten Brink. On deep learning-based
channel decoding. In Information Sciences and Systems (CISS), 2017 51st Annual Conference on,
pp. 1-6. IEEE, 2017.
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation
for stochastic neural networks. arXiv preprint arXiv:1511.05176, 2015.
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Proceedings of the sixth annual conference on Computational
learning theory, pp. 5-13. ACM, 1993.
Antti Honkela and Harri Valpola. Variational learning and bits-back coding: an information-
theoretic view to bayesian learning. IEEE Transactions on Neural Networks, 15(4):800-810,
2004.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Hyeji Kim, Yihan Jiang, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath. Deepcode: Feed-
back codes via deep learning. arXiv preprint arXiv:1807.00801, 2018a.
Hyeji Kim, Yihan Jiang, Ranvir Rana, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath.
Communication algorithms via deep learning. arXiv preprint arXiv:1805.09317, 2018b.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Ralf Koetter and Pascal O Vontobel. Graph-covers and iterative decoding of finite length codes.
Citeseer, 2003.
Victoria Kostina and Sergio VerdU. Lossy joint source-channel coding in the finite blocklength
regime. IEEE Transactions on Information Theory, 59(5):2545-2575, 2013.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), 2015.
David JC MacKay. Information theory, inference and learning algorithms. Cambridge university
press, 2003.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv
preprint arXiv:1402.0030, 2014.
Andriy Mnih and Danilo J Rezende. Variational inference for monte carlo objectives. arXiv preprint
arXiv:1602.06725, 2016.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv
preprint arXiv:1610.03483, 2016.
Eliya Nachmani, Yair Be’ery, and David Burshtein. Learning to decode linear codes using deep
learning. In Communication, Control, and Computing (Allerton), 2016 54th Annual Allerton
Conference on, pp. 341-346. IEEE, 2016.
Radford M Neal. Connectionist learning of belief networks. Artificial intelligence, 56(1):71-113,
1992.
10
Under review as a conference paper at ICLR 2019
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning. NIPS, 2011.
Randolph John Pilc. Coding theorems for discrete source-channel pairs. PhD thesis, Massachusetts
Institute of Technology, 1967.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Jason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200, 2016.
Francisco R Ruiz, Michalis Titsias RC AUEB, and David Blei. The generalized reparameterization
gradient. In Advances in neural information processing Systems, pp. 460-468, 2016.
Shibani Santurkar, David Budden, and Nir Shavit. Generative compression. arXiv preprint
arXiv:1703.01467, 2017.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using
stochastic computation graphs. In Advances in Neural Information Processing Systems, pp. 3528-
3536, 2015.
Claude Elwood Shannon. A mathematical theory of communication. Bell Syst. Tech. J., 27:623-656,
1948.
Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszar. Lossy image compression with
compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017.
George Toderici, Sean M O’Malley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet
Baluja, Michele Covell, and Rahul Sukthankar. Variable rate image compression with recurrent
neural networks. arXiv preprint arXiv:1511.06085, 2015.
George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and
Michele Covell. Full resolution image compression with recurrent neural networks. In CVPR, pp.
5435-5443, 2017.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar:
Low-variance, unbiased gradient estimates for discrete latent variable models. In Advances in
Neural Information Processing Systems, pp. 2627-2636, 2017.
Aaron van den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in
Neural Information Processing Systems, pp. 6306-6315, 2017.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096-1103. ACM, 2008.
Pascal O Vontobel and Ralf Koetter. On low-complexity linear-programming decoding of ldpc
codes. European transactions on telecommunications, 18(5):509-517, 2007.
Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
gradient optimization. In Advances in Neural Information Processing Systems, pp. 181-189,
2013.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
11
Under review as a conference paper at ICLR 2019
Appendix
A	Proofs of theoretical results
A.1 Stationary Distribution of Markov chain
We follow the proof structure in (Grover & Ermon (2018)) with minor adaptations. First, we note
that the Markov chain defined in Eq. 5 is ergodic due to the BSC noise model. This is because
the BSC defines a distribution over all possible (finite) configurations of the latent code Y, and the
Gaussian decoder posits a non-negative probability of transitioning to all possible reconstructions
X. Thus given any (X, Y ) and (X0, Y0) such that the density p(X, Y) > 0 and p(X0, Y0) > 0, the
probability density of transitioning p(X0, Y0|X, Y) > 0.
Next, we can rewrite the objective in Eq. equation 4 as the following:
Ep(x,yH,φ) [logPdecoder(X|y； θ)] = Eq(y;0) /P(X|y； φ) logPdecoder(X|y； θ)
=-H(X|Y; φ) - EqM,φ)[KL(p(X∣y; φ)∣∣p(X|y; θ))]
We note that for any value of φ, the optimal value of θ = θ* eliminates the KL-divergence term, as
it is non-negative and minimized only when its argument distributions are identical. Then, for any
φ, we note that the following Gibbs chain converges to P(X, y) if the chain is ergodic:
y(t) ~ q∏oisy-e∏c(y|x(t); Φ,e) , χ(t+1) ~ p(χ∣y(t); θ*)	⑺
Substituting p(x∣y; θ*) for PdeCoder(x|y； θ) finishes the proof.
B Additional channel models
B.1	B inary Erasure Channel (BEC)
The BEC erases each bit at position i in the latent code y with some i.i.d. probability E into a
new symbol. Thus if the input to the channel is y|x 〜Bern(σ(fφ(x))), the BEC induces a 3-way
Categorical distribution where the third category refers to the bit-erasure. We denote the erased bit
as ?, which can be modeled as category 2 in the new alphabet of Y = {0, 1, 2}m. This yields:
y|x ~ Cat(1 - C - σ(fφ(x)) + σ(fφ(x)) ∙ e, σ(fφ(x)) - σ(fφ(x)) ∙ e, E)
where:
p(y = 0∣y = 0, x) = 1 -E- σ(fφ(x)) + σ(fφ(x)) ∙ E
p(y = 1|y = 1,X) = σfφ(X)) - σfφ(X)) ∙E
p(y = ?|y = i,x) + p(y = ?|y = 0,x) = E
B.2	Deletion Channel
The deletion channel is similar to the BEC, except that with some independent probability E, the bit
at position i is deleted. Thus our codewords y may not necessarily be of length m as in y ∈ {0,1}m,
and the space ofy may have variable lengths.
C NECST architecture and hyperparameter configurations
C.1 MNIST
For MNIST, we used the static binarized version as provided in (Burda et al. (2015)) with
train/validation/test splits of 50K/10K/10K respectively.
•	encoder: MLP with 1 hidden layer (500 hidden units), ReLU activations
12
Under review as a conference paper at ICLR 2019
•	decoder: 2-layer MLP with 500 hidden units each, ReLU activations. The final output layer
has a sigmoid activation for learning the parameters of Pnoisy_enc(y |x； Φ, E)
•	n_bits: 100
•	n_epochs: 200
•	batch size: 100
•	L2 regularization penalty of encoder weights: 0.001
•	Adam optimizer with lr=0.001
C.2 Omniglot
We statically binarize the Omniglot dataset by rounding values above 0.5 to 1, and those below to 0.
•	encoder: MLP with 1 hidden layer (500 hidden units), ReLU activations
•	decoder: 2-layer MLP with 500 hidden units each, ReLU activations. The final output layer
has a sigmoid activation for learning the parameters of Pnoisy_enc(y|x； Φ, E)
•	n_bits: 200
•	n_epochs: 500
•	batch size: 100
•	L2 regularization penalty of encoder weights: 0.001
•	Adam optimizer with lr=0.001
C.3 Random bits
We randomly generated length-100 bitstrings by drawing from a Bern(0.5) distribution for each
entry in the bitstring. The train/validation/test splits are: 5K/1K/1K.
•	encoder: MLP with 1 hidden layer (500 hidden units), ReLU activations
•	decoder: 2-layer MLP with 500 hidden units each, ReLU activations. The final output layer
has a sigmoid activation for learning the parameters of p∏oisy-e∏c(y∣x; Φ, E)
•	n_bits: 50
•	n_epochs: 200
•	batch size: 100
•	L2 regularization penalty of encoder weights: 0.001
•	Adam optimizer with lr=0.001
C.4 SVHN
For SVHN, we collapse the ”easier” additional examples with the more difficult training set, and
randomly partition 10K of the roughly 600K dataset into a validation set.
•	encoder: CNN with 3 convolutional layers + fc layer, ReLU activations
•	decoder: CNN with 4 deconvolutional layers, ReLU activations.
•	n_bits: 500
•	n_epochs: 500
•	batch size: 100
•	L2 regularization penalty of encoder weights: 0.001
•	Adam optimizer with lr=0.001
The CNN architecture for the encoder is as follows:
1.	conv1 = nfilters=128, kerneLSize=2, strides=2, Padding="VALID”
13
Under review as a conference paper at ICLR 2019
2.	conv2 = nfilters=256, kerneLsize=2, Strides=2, Padding="VALID”
3.	conv3 = nfilters=512, kerneLsize=2, strides=2, padding="VALID”
4.	fc = 4*4*512 → n_bits, no activation
The decoder architecture follows the reverse, but without a final deconvolution layer as: nfilters=3,
kernel-size=1, strides=1, Padding="VALID”，activation=ReLU.
C.5 celebA
We use the celebA dataset with standard train/validation/test splits with minor preprocessing. First,
we align and crop each image to focus on the face, resizing the image to be (64, 64, 3).
•	encoder: CNN with 5 convolutional layers + fc layer, ELU activations
•	decoder: CNN with 5 deconvolutional layers, ELU activations.
•	n_bits: 1000
•	n_epochs: 500
•	batch size: 100
•	L2 regularization penalty of encoder weights: 0.001
•	Adam optimizer with lr=0.0001
The CNN architecture for the encoder is as follows:
1.	conv1 = nfilters=32, kerneLsize=4, strides=2, Padding="SAME”
2.	conv2 = nfilters=32, kerneLsize=4, strides=2, padding="SAME"
3.	conv3 = nfilters=64, kerneLsize=4, strides=2, padding="SAME"
4.	conv4 = nfilters=64, kerneLsize=4, strides=2, padding="SAME"
5.	conv5 = nfilters=256, kerneLsize=4, strides=2, padding="VALID"
6.	fc = 256 → n_bits, no activation
The decoder architecture follows the reverse, but without the final fully connected layer and
the last deconvolutional layer as: nfilters=3, kerneLsize=4, strides=2, padding="SAME", activa-
tion=sigmoid.
D Additional experimental details and results
D.1 Fixed Distortion: JPEG-Ideal Channel Code System
For the BSC channel, we can compute the theoretical channel capacity with the formula C = 1 -
Hb(), where denotes the bit-flip probability of the channel and Hb denotes the binary entropy
function. Note that the communication rate of C is achievable in the asymptotic scenario of infinitely
long messages; in the finite bit-length regime, particularly in the case of short blocklengths, the
highest achievable rate will be much lower.
For each image, we first obtain the target distortion d per channel noise level by using a fixed bit-
length budget with NECST. Next we use the JPEG compressor to encode the image at the distortion
level d. The resulting size of the compressed image f (d) is used to get an estimate f (d)/C for
the number of bits used for the image representation in the ideal channel code scenario. While
measuring the compressed image size f (d), we ignore the header size of the JPEG image, as the
header is similar for images from the same dataset.
The plots compare f (d)/C with m, the fixed bit-length budget for NECST.
14
Under review as a conference paper at ICLR 2019
MNIST: Required Number of Bits for Fixed Distortion
Random: Comparison of Number of Bits
(a) Binary MNIST
(b) random
Figure 4: Theoretical m required for JPEG + ideal channel code to match NECST’s distortion.
D.2 Fixed Rate: JPEG-LDPC system
We first elaborate on the usage of the LDPC software. We use an optimized C implementation of
LDPC codes to run our decoding time experiments:
(http://radfordneal.github.io/LDPC-codes/).
The procedure is as follows:
•	make-pchk to create a parity check matrix for a regular LDPC code with three 1’s per
column, eliminating cycles of length 4 (default setting). The number of parity check bits
is: total number of bits allowed - length of codeword.
•	make-gen to create the generator matrix from the parity check matrix. We use the default
dense setting to create a dense representation.
•	encode to encode the source bits into the LDPC encoding
•	transmit to transmit the LDPC code through a bsc channel, with the appropriate level
of channel noise specified (e.g. 0.1)
•	extract to obtain the actual decoded bits, or decode to directly obtain the bit errors
from the source to the decoding.
LDPC-based channel codes require larger blocklengths to be effective. To perform an end-to-end
experiment with the JPEG compression and LDPC channel codes, we form the input by concatenat-
ing multiple blocks of images together into a grid-like image. In the first step, the fixed rate of m is
scaled by the total number of images combined, and this quantity is used to estimate the target f (d)
to which we compress the concatenated images. In the second step, the compressed concatenated
image is coded together by the LDPC code into a bit-string, so as to correct for any errors due to
channel noise.
Finally, we decode the corrupted bit-string using the LDPC decoder. The plots compare the re-
sulting distortion of the compressed concatenated block of images with the average distortion on
compressing the images individually using NECST. Note that, the experiment gives a slight dis-
advantage to NECST as it compresses every image individually, while JPEG compresses multiple
images together.
We report the average distortions for sampled images from the test set.
Unfortunately, we were unable to run the end-to-end for some scenarios and samples due to errors
in the decoding (LDPC decoding, invalid JPEGs etc.).
D.3 VAE-LDPC SYSTEM
For the VAE-LDPC system, we place a uniform prior over all the possible latent codes and com-
Pute the KL penalty term between this prior p(y) and the random variable qnoisy_enc(y|x； φ, 6). The
learning rate, batch size, and choice of architecture are data-dependent and fixed to match those of
NECST as outlined in Section C. However, we use half the number of bits as allotted for NECST so
15
Under review as a conference paper at ICLR 2019
that during LDPC channel coding, we can double the codeword length in order to match the rate of
our model.
D.4 Interpolation in Latent Space
We show results from latent space interpolation for two additional datasets: SVHN and celebA. We
used 500 bits for SVHN and 1000 bits for celebA across channel noise levels of [0.0, 0.1, 0.2, 0.3,
0.4, 0.5].
Figure 5: Latent space interpolation of 5 → 2 for SVHN, 500 bits at noise=0.1

Figure 6: Latent space interpolation for celebA, 1000 bits at noise=0.1
D.5 Markov chain image generation
We observe that we can generate diverse samples from the data distribution after initializing the
chain with both: (1) examples from the test set and (2) random Gaussian noise x0 〜N(0,0.5).
D.6 Downstream classification
Following the setup of (Grover & Ermon (2018)), we used standard implementations in sklearn with
default parameters for all 8 classifiers with the following exceptions:
1.	KNN: n_neighbors=3
2.	DF: max_depth=5
3.	RF: max_depth=5, n_estimators=10, maxfeatures=1
4.	MLP: alpha=1
5.	SVC: kernel=linear, C=0.025
16
Under review as a conference paper at ICLR 2019
∕6G303qq∕G
/42384 夕 6 / g
∕Gqq6G∕G63
“£。3”/夕埠3
3d43GqZ3 夕，
90466r324g
Qq 335z√33OH
gqaqz/37/O
633J8G3834
卷HD。■堂H汽ElE
ιππ up .灯，行。
OnQn Eg tin 白
ππm∏ππnππ
E1E1ΠE3 0E!ΠΠ∩Π
ππππeifip FrIrI
πππ∏flnι πππ
∏E]∏n jir,ππrι
目口。。£1*白口。口 G
ΠΠΠΠΠΠHb1Π 2
(a) MNIST, initialized from ran- (b) celebA, initialized from data (c) SVHN, initialized from data
dom noise
Figure 7: Markov chain image generation after 9000 timesteps, sampled per 1000 steps
D.7 Decoding time
We show the average decoding times for the randomly generated bitstring data, averaged over 10
runs:
Figure 8: Average decoding times for NECST vs. 50 iterations of BP (LDPC decoding)
17