Under review as a conference paper at ICLR 2019
Convergent Reinforcement Learning with
Function Approximation: A Bilevel Optimiza-
tion Perspective
Anonymous authors
Paper under double-blind review
Ab stract
We study reinforcement learning algorithms with nonlinear function approxima-
tion in the online setting. By formulating both the problems of value function
estimation and policy learning as bilevel optimization problems, we propose on-
line Q-learning and actor-critic algorithms for these two problems respectively.
Our algorithms are gradient-based methods and thus are computationally efficient.
Moreover, by approximating the iterates using differential equations, we establish
convergence guarantees for the proposed algorithms. Thorough numerical exper-
iments are conducted to back up our theory.
1	Introduction
In reinforcement learning (Sutton & Barto, 1998), the agent aims to make optimal decisions by inter-
acting with the environment and learning from the experiences, where the environment is modeled
as a Markov Decision Process (MDP). With the recent advancement of deep learning, reinforce-
ment learning has achieved extraordinary empirical success in solving complicated decision making
problems, such as the game of Go (Silver et al., 2016; 2017), navigation (Banino et al., 2018), and
dialogue systems (Li et al., 2016).
Despite its great empirical success, there exists a gap between the theory and practice of reinforce-
ment learning. Specifically, in terms of theory, most existing works focus on either the tabular case or
the case with linear function approximation. In the former case, both the state and action spaces are
finite, while in the latter the value function is assumed to be linear in a given feature mapping. Using
tools for convex optimization and linear regression, the statistical and computational properties of
the reinforcement learning algorithms are well-understood under these restrictive settings. More-
over, when it comes to nonlinear function approximation, theoretical analysis becomes intractable
as it involves solving a highly nonconvex statistical optimization problems. Moreover, it is shown
in Tsitsiklis & Van Roy (1997) that simple method such as TD(0) might fail to converge when non-
linear value functions are applied. However, using deep learning techniques, methods such as deep
Q-network (DQN) (Mnih et al., 2015) and asynchronous advantage actor-critic (A3C) (Mnih et al.,
2016) become baseline algorithms for artificial intelligence for practical applications.
To bridge such a gap in DRL, we make the first attempt to study the convergence of online rein-
forcement learning algorithms with nonlinear function approximation in general. In particular, we
propose online Q-learning (Watkins & Dayan, 1992) and actor-critic algorithms with two-timescale
updates Konda & Tsitsiklis (2000), i.e., the iterative algorithm consists of two parameters that up-
dates with different learning rates. Although the statistical and computational properties of these
methods with linear function approximation are well-studied, convergence of reinforcement learn-
ing methods with nonlinear function approximation is fundamentally more challenging due to the
lack of convexity. In this case, we can only expect convergence to local minima or stationary points.
Furthermore, we bring both the problems of value function and policy learning under the unified
framework of bilevel optimization, which motivates two-timescale updating rules in our algorithms.
1
Under review as a conference paper at ICLR 2019
Specifically, bilevel optimization problems consist of two subproblems that are intertwined with
each other. Such phenomenon appears in both Q-learning and actor-critic algorithms. For Q-learning
with function approximation, the updates of the target function (also known as target network for
DQN) and the value function are coupled together; the same is the updates of the policy and value
function in the actor-critic algorithm. Since the two variables in bilevel optimization have different
roles, one servers as the constraint of the other, two-timescale updates appears naturally for these
problems. In particular, the parameter of the lower optimization problem needs to be updated using a
larger stepsize. In addition, as a byproduct, from the view of bilevel optimization, the target network
in DQN appears to be the parameter of the upper level optimization, which justifies the trick of using
a target network in DQN.
The analysis of our algorithms utilizes stochastic approximation (Borkar, 2008; Kushner & Yin,
2003), which approximates the asymptotic behavior of the iterates using ordinary differential equa-
tions. Specifically, under certain assumptions, we show that the online Q-learning algorithm con-
verges almost surely to a local minimizer of the mean squared Bellman error. In addition, for the
actor-critic algorithm, we show that the limit points of the actor and critic updates can be character-
ized by a set of equations.
Our contributions are three-fold. First, we unify the problems of value function estimation and
policy learning using the framework of bilevel optimization, which leads to the online Q-learning
and actor-critic algorithms using two-timescale updates. Second, using stochastic approximation,
the convergence of the proposed algorithms are established with nonlinear function approximation.
Third, as a byproduct, the formulation of Q-learning via bilevel optimization justifies the techniques
of target network used in DQN, which can be viewed as the parameter of the upper level optimization
subproblem.
Related Work. Our work is closely related to the literature on the convergence of online reinforce-
ment learning algorithms with function approximation. For policy evaluation, most existing works
focus on algorithms with linear function approximation. Tsitsiklis & Van Roy (1997) study the
convergence of the on-policy TD(λ) algorithm based on temporal-difference (TD) error. To handle
off-policy sampling, Maei et al. (2010); Sutton et al. (2009; 2016); Yu (2015); Hallak & Mannor
(2017) propose various TD-learning methods with convergence guarantees. Utilizing two-timescale
stochastic approximation in Borkar (2008) and strong convexity, they establish global convergence
results for the proposed methods. The finite-sample analysis of these methods are recently estab-
lished in Dalal et al. (2017b;a). Moreover, using a duality formulation, Liu et al. (2015); Du et al.
(2017) study the finite-sample performance of TD-learning algorithms with primal-dual updates. As
for nonlinear function approximation, to the best of our knowledge, the only convergent algorithm is
the nonlinear-GTD algorithm proposed in Bhatnagar et al. (2009a), which is more pertinent to our re-
sults. Their analysis also depends on two-timescale stochastic approximation, where the variable in
the faster timescale essentially solves a linear equation, thus converges to the global optimum. How-
ever, in our problem, both the two variables solves nonconvex problems, which adds more challenge
in the analysis. In addition, we mainly consider the problem of Q-learning and actor-critic, whereas
their results only focus on policy evaluation. Moreover, their algorithm involves the Hessian of the
value function and their theory requires the value function to be three times differentiable, whereas
our algorithm only requires the gradient of the value function with less stringent assumptions. Thus,
our results are not directly comparable with this work.
Furthermore, for Q-learning and the actor-critic algorithm, existing results all consider linear func-
tion approximation. For Q-learning, Melo et al. (2008); Prashanth et al. (2014) study the conver-
gence using stochastic approximation. In addition, the actor-critic algorithm is introduced in Sutton
et al. (2000); Konda & Tsitsiklis (2000), which also study its convergence with linear function
approximation. Later, Kakade (2002); Peters & Schaal (2008) propose the natural actor-critic algo-
rithm which updates the policy function using natural gradient descent (Amari, 1998). Convergence
of actor-critic and natural actor-critic algorithms with linear function approximation are studied in
Bhatnagar et al. (2009b; 2008); Castro & Meir (2010); Bhatnagar (2010); Maei (2018).
2
Under review as a conference paper at ICLR 2019
2	Notation and Background
In this section, we first lay out the notation that will be followed throughout this paper and then
introduce some background knowledge on reinforcement learning.
We stick to the following notation. For a measurable space with domain S, we denote by B(S)
the set of bounded measurable functions on S. Let Lip(S, L) be the set of Lipschitz continuous
functions on S with Lipschitz constant L. Let P(S) be the set of all probability measures over S.
For any ν ∈ M(S) and any measurable function f : S → R, we denote by kf kν,p the `p-norm of f
with respect to measure ν. For an integer k, we use ∆k to denote the probability simplex in Rk, and
let [k] = {1, 2,..., k}.
In reinforcement learning, the problem is modeled by a Markov decision process. A discounted
MDP is defined by a tuple (S, A, P, R,p0, γ), where S is the set of states, A is the set of all possible
actions, P : S × A → P(S) is the Markov transition kernel, R: S × A → R is the reward function,
p0 ∈ P (S) is the distribution of the initial state s0, and γ ∈ (0, 1) is the discount factor. At any
state S ∈ S, suppose action a ∈ A is executed, We use P(∙ |s, a) ∈ P(S) to denote the probability
distribution of the next state, and let R(s, a) be the the immediate reward. For regularity, we assume
that the reWards are uniformly bounded by Rmax. By taking action at at state st for each time step
t ≥ 0, the agent receives the discounted cumulative reward R = Pt≥0 Yt ∙ R(st, at).
Moreover, a policy π : S → P(A) is a action selection rule. Specifically, π(a | s) is the probability
of selecting action a at state s. We define the state value function and the action value function of
policy π as Vπ(s) = Eπ(R | s0 = s, π) and Qπ(s, a) = Eπ(R | s0 = s, a0 = a), respectively.
Here we use Eπ to indicate that the actions {at }t≥1 follow policy π. We define Bellman operator
of Tn : B(S) → B(S) by TnV(S) = E[r(st, at) + Y ∙ V(st+ι) | St = s], where at 〜π(∙ | st). By
definition, we have Vπ = Tπ Vπ for any policy ∏. Since T* is Y-ContraCtive, Vπ is the unique fixed
point of Tπ. The problem of estimating Vπ for a fixed policy π is called policy evaluation.
Furthermore, in reinforcement learning, the goal is to obtain the policy that yields the maximal
cumulative reward in expectation. To this end, we define the optimal value function Q* : S × A → R
by Q* (S, a) = supπ Qπ (S, a), ∀(S, a) ∈ S × A, where the supremum is taken over all possible
policies. Then it is known that the greedy policy with respect to Q* is the optimal policy. To
estimate Q* with accuracy, most existing methods utilize the fact that Q* is the unique fixed point
of the Bellman optimality operator T*, which is defined by
(T*Q)(s, a) =r(s, a) + Y ∙ E[max Q(st+ι,a) ∣ St = s, at = a].	(2.1)
The most well-known method for estimating Q* is the Q-learning algorithm (Watkins & Dayan,
1992), based upon a large family of algorithms are derived.
In addition to methods that focus on estimating the value functions, another class of algorithms in
reinforcement learning directly search for a good policy over a parametrized policy class {πω : ω ∈
Rp} directly, where ω is the parameter. Specifically, the goal is to maximize J(ω) = Eπω (R) =
Eso〜po [Vπω (so)]. The basis of these methods are provided by the policy gradient theorem (Baxter
& Bartlett, 2000), which states that
(1 - γ) Nω J(ω) = Es〜ρω,a〜∏ω(∙∣ s)[Vω log ∏ω (a | S) ∙ Q"ω (s, a)]
=Es 〜ρθ ,a 〜∏ω (∙∣ s){Vω log ∏ω (a | S) ∙ [R(s, a) + Y ∙ V "ω (s0) - Vπω (s)]},	(2.2)
where s0 〜P(∙ | s, a) is the next state of the MDP given (s, a) ∈ S×A, and ρω ∈ P(S) is the state
occupancy measure of ∏ω, i.e., ρω (s) = (1 一 Y) ∙ Pt≥0 Yt ∙ P∏ω (St = s). We note that (2.2) brings
the value function and the policy together. By updating θ using (2.2) with the value function Qπω
or Vπω estimated using temporal difference learning methods, we obtain the actor-critic algorithms
(Konda & Tsitsiklis, 2000).
3
Under review as a conference paper at ICLR 2019
3 Reinforcement Learning as Bilevel Optimization
As we introduced in the previous section, estimation of the value functions and policy learning are
crucial problems of reinforcement learning. In this section, we show that both these two problems
can be cast into bilevel optimization. From this view, we establish the Q-learning and actor-critic
algorithms with two-timescale updates.
In the unconstrained form, bilevel optimization is formulated as an optimization problem that contain
another optimization problem as a constraint. Specifically, it can be written as
x* = argmin F[x,y*(x)],	where y*(x) = argmin G(x,y),	(3.1)
x∈X	y∈Y
where F, G: X × Y → R are two differentiable functions. Here the challenges in (3.1) are threefold.
First, to evaluate the objective function F[x, y* (x)], one needs to achieve the global minimizer of
G(x, y) for each x, which might not be feasible when G(x, y) is not a convex function of y. Second,
even if we are able to obtain y*(x) and evaluate F[x, y* (x)] for any x ∈ X, since it is impossible to
compute the gradient of y*(∙), minimize F[x, y*(x)] via gradient-based methods is impossible. In
practice, we can only hope for obtaining (x\ , y \ ) ∈ X × Y satisfying
y\ = y*(x\) and x] = argminF(x,y) | y=y\.	(3.2)
x∈X
Third, in machine learning applications, usually both F and G in (3.1) are computed using large-
scale datasets. In this situation, it can be highly costly to solve the lower level optimization problem
to the exact minimizer. Thus, efficient algorithms have to be robust to the error incurred in solving
for y*(∙).
An efficient algorithm that achieves the condition in (3.2) is two-timescale gradient descent, where
we simultaneously perform gradient update with respect to both x and y. Specifically, we have
xt+1 -	Xt	-	αt	∙ PxF(Xt, yt),	yt+1 -	yt	- βt	∙	RyG(xt, yt),	(3.3)
where the stepsizes {αt, βt}t≥0 satisfy the following two-timescale assumption.
Assumption 3.1. We assume that the stepsizes {αt, βt }t≥0 satisfy
X αt	= X βt	= ∞,	X αt2	+	βt2	<	∞,	lim	αt /βt = 0.	(3.4)
t→∞
t≥0	t≥0	t≥0
That is, in addition to the standard Robbins-Monro condition (Robbins et al., 1951), we have
αt∕βt → 0, which indicates that {yt}t≥o updates in a faster pace than {χt}t≥o. ThiS condition
gives the name of two-timescale update, and plays a pivotal role in the theoretical analysis.
More rigorously, measured in the faster timescale using {βt}t≥0, the updates in (3.3) converge to the
asymptotically stable equilibria of the ODE system {X = 0,y = -VyG(x, y)}. Thus, {xt, yt}t≥o
converges to some (χ∖, y\) where y\ is a local minimizer of G(χ∖, ∙). When G(χ∖, ∙) has a unique
minimizer, we have y\ = y*(x\). Thus, under the faster timescale, the updates in (3.3) essentially
fix the x variable and let {yt}t≥0 converge to y*(x) using gradient descent.
Furthermore, to determine the convergence of {xt}t≥0, we need to look into the slower timescale.
Since {yt}t≥0 updates in a faster speed under Assumption 3.1, we could safely assume that {yt}t≥0
already converge to y*(x) for some x ∈ X. In this scenario, the asymptotic behavior of {xt}t≥0 is
captured by the ODE X = -VxF(x, y) | y=y*(境).Suppose minx F(x, y) has a unique minimizer,
denoted by x* (y), then {xt, yt}t≥0 converges to some (x\, y\) satisfying x\ = x* (y\), y\ = y*(x\).
This implies that the two-timescale gradient update in (3.3) achieves the condition in (3.2) asymptot-
ically. Moreover, when the minimizers of F(∙, y) and G(x, ∙) are not unique, using similar analysis,
it can be shown that X\ is a local minimizer of minx F(X, y\), and simultaneously y\ is a local
minimizer of miny G(X\, y).
In the rest of this section, we apply the above argument to problems in reinforcement learning.
In particular, we formulate both value function estimation and policy learning as bilevel optimiza-
tion problems, which naturally motivates us to propose two-timescale algorithms with convergence
guarantees.
4
Under review as a conference paper at ICLR 2019
3.1 Value Function Estimation
In value function estimation, the goal is to estimate either Vπ or Q* defined in §2 using function
approximation. Note that Vπ and Q* are the unique fixed points of Bellman operators Tπ and T*,
respectively. Here we mainly focus on Q*, the results for Vπ can be similarly obtained by replacing
T* in(2.1)byTπ.
Let F = {Qθ : S × A → Rd, θ ∈ Rd} be a parametrized function class, where θ is the parameter.
Our goal is to find Qθ such that the mean-squared Bellman error (MSBE)
'(θ) = kQθ - T*Qθkp = E(s,a)〜ρ({Qθ(s,a) - R(s,a) - Y ∙ E[m01axQθ(s0,a0) ∣ s,a]}2) (3.5)
is minimized, where P ∈ P (S X A) is probability distribution, and s0 〜 P (∙ | s, a) is the next state
of the MDP. Note that Q-learning is an off-policy method. To learn Q*, it is common to obtain
sample from the MDP using a behavioral policy πb, which induces a Markov chain on S × A. Then
ρ can be viewed as the stationary distribution of this Markov chain. Furthermore, for simplicity, we
assume that drawing i.i.d. samples from ρ is possible, which approximately holds when the Markov
chain enjoys rapid mixing properties.
Moreover, notice that in (3.5), the conditional expectation is inside the quadratic function, which is
known to cause the issue of “double sampling” (Baird et al., 1995). That is, to obtain an unbiased
estimate of the MSBE, given (s, a)〜 ρ, We need two conditionally independent samples from
P(∙ | s,a), which cannot be satisfied in practice. To avoid this issue, we write MSBE minimization
as a bilevel optimization problem
ω = min ∣∣ω 一 H(ω)∣∣2 subject to H(ω) = argmin{L(θ,ω) = ∣∣Qθ - T*Qω IlP}.	(3.6)
We note that if there exists a θ* such that Qθ* = Q*, then we have H(θ*) = θ*, which implies that
(ω, θ) = (θ*, θ*) is the solution of (3.6). Intuitively, with nonlinear function approximation, the
lower level optimization problem minθ L(θ, ω) solves a nonlinear least square regression problem
using function class F, where T*Qω can be viewed as the response variable. After obtaining H(ω),
then for the upper level optimization problem, we directly replace by H(ω). Thus, in the batch
setting, solving the bilevel optimization in (3.6) yields the fitted Q-iteration algorithm (Riedmiller,
2005; Munos & Szepesvari, 2008). Specifically, When deep neural networks are used for function
approximation, we recover the well-known deep Q-network (DQN) (Mnih et al., 2015), where Qω
in (3.6) is the target network. We note that the usage of the target network is a critical trick in
DQN, whose mechanism is not fully understood. Therefore, the target network in DQN can be
viewed as the parameter of the upper level optimization of the bilevel formulation in (3.6). From
the perspective of bilevel optimization, it is natural that the target network needs to be updated in a
slower rate, which justifies the empirical practice of DQN that the target network is updated every
Ttarget iterations while the Q-network is updated in each iteration.
To obtain an online algorithm, we solve (3.6) via two-timescale gradient update given in (3.3). The
update rules are given by ωt+ι J (1 一 αt) ∙ ωt + a- θt, and θt+ι J θt 一 βt ∙ Vθ L(θt, ωj In
addition, we note that VθL(Θ, ω) has an unbiased estimate
[Qθ(s,a) - R(s,a) - γ ∙ maxQω(s0,a0)] ∙ VθQθ(s,a).
a0∈A
Thus, replacing VθL(θ, ω) by its estimate, we obtain our online Q-learning algorithm, which is
stated in Algorithm 1. Here in line 6 we project the iterate to a compact set X = {x ∈ Rd : ∣x∣2 ≤
RX } for stability, where RX is a sufficiently large constant. It is clear that our algorithm is free
from the double sampling issue. Moreover, as we see in §4, this algorithm converges almost surely
to ω*,H(ω*), where H(ω*) is a local minimizer of min& L(θ, ω*), and ω* is close to H(ω*) up to
some error caused by projection.
Finally, we note that Algorithm 1 can be similarly applied to policy evaluation, where we estimate
Vπ using function class {Vθ : S → R, ω ∈ Rd}. In this case, by the definition of Tπ, we only need
to replace δt in line 5 by δt = Vet (St) 一 R(st,at) 一 Y ∙ Vωt (st). Moreover, here P is set to be the
stationary distribution induced by policy π. We defer the algorithm for policy evaluation to §A.
5
Under review as a conference paper at ICLR 2019
Algorithm 1 Online Q-learning with nonlinear function approximation.
1:	Input: Initialization θ0 = ω0 ∈ Θ, stepsizes {αt, βt}t≥0.
2:	Output: The sequences {θt}t≥0 and {ωt}t≥0 .
3:	for t = 0, 1, 2, . . . do
4:	Sample (st, at)〜ρ, observe reward R(st, at) and next state s；.
5:	Compute the TD-error δ; = Qθt (s；,a；) - R(s；,a；) - Y ∙ max。*/ Qωt (s；,a).
6:	Update the parameters by
ωt+ι =	πx[(I -	at)	∙ ωt	+ αt ∙	θt],	θt+ι = θt	- Bt	∙	δt ∙ VθQθt(St, at).	(3.7)
7:	end for
3.2 The Actor-Critic Algorithm
Now we formulate policy learning as bilevel optimization and obtain the actor-critic algorithm. Let
{πω : ω ∈ Rp} be the family of policies and let {Vθ : θ ∈ Rd} be a parametrized family of value
functions. Note that the objective in policy learning can be written as J(ω) = Es0〜p0 [Vπω (so)].
We define F(θ, ω) = Es0〜p0 [Vθ(so)] and consider the bilevel optimization problem
max F[H(ω), ω]	subject to H(ω) = argmin{G(θ, ω) = ∣∣Vθ — TπωVθ∣∣2ω },	(3.8)
ω	θρ
where ρω is the state-occupancy measure of policy πω. To see the correctness of such a formulation,
notice that if Vπω ∈ {Vθ, θ ∈ Rd}, then the solution of the lower level optimization problem H(ω)
satisfies that /q)= Vπθ, which leads to F[H(ω), ω] = J(ω) for all ω ∈ Rp. Thus, We recover the
original policy learning problem.
Note that minθ G(θ, ω) is an policy evaluation problem. Directly solving this problem suffers the
issue of double sampling. To resolve this issue, as stated above, one need to formulate itself as a
bilevel optimization problem. This will result in a three-timescale algorithm for policy learning.
However, due to its complexity, instead we consider the TD(0) method, which is a semi-gradient
method for policy evaluation (Sutton & Barto, 1998). Specifically, we define
g(θ, ω) = Es 〜ρω ,a 〜∏ω (∙ | s){[R(s, a) + Y ∙ Vθ )(SO)- Vθ (S)] ∙ Vθ Vθ (S)},	(3.9)
which can be viewed as a biased estimate of -VθG(θ, ω). Similar to (3.3), We approximately solve
(3.8) via ωt+ι  ωt + αVωF(θt, ωt), and θt+ι  θt + βt ∙ g(θt, ωt). Moreover, by policy gradient
theorem, we have
vω F(θ,ω) = Es〜ρω ,a〜∏ω (∙∣s){[R(s,a) + Y ∙ Vθ )(S)- Vθ (s)] ∙Vω log ∏ω (a | s)}.	(3.10)
Thus, replacing (3.9) and (3.10) by their sample-based counterparts, we recover the actor-critic
algorithm with TD(0) updates,whose details are deferred to §A. Moreover, for algorithmic stability,
we project {ωt}t≥o to Ω = {y ∈ Rd: ∣∣y∣∣2 ≤ Rω} with Rω sufficiently large.
Furthermore, we note that TD(0) with nonlinear function approximation can diverge in some sit-
uations (Tsitsiklis & Van Roy, 1997). In the next section, we given sufficient conditions for the
convergence of actor-critic with nonlinear function approximation.
4 Main Results
In this section, we establish the convergence results for the algorithm presented in §3. We first
consider the online Q-learning algorithm with nonlinear function approximation. Theoretical results
for policy evaluation can be similarly obtained, which are omitted for simplicity. We make the
following assumption for the online Q-learning.
Assumption 4.1. For the online Q-learning algorithm, we assume the following conditions holds.
(i)	. For any (s, a) ∈ S × A and any θ ∈ Rd, we have ∣Qθ(s, a)| ≤ Qmax, ∣∣VθQθ(s,a)k2 ≤
Gmax, and VθQθ(S, a) is Lipschitz continuous in θ. Here Qmax ≥ Rmax/(1 - Y) and
Gmax are two positive constants.
6
Under review as a conference paper at ICLR 2019
(ii)	. Let L(θ, ω) be defined in (3.6). For any ω ∈ X, we assume that any local minimizer θ* of
minθ L(θ,ω) satisfies that V2L(θ*,ω) > 0.
Here condition (i) in Assumption 4.1 requires that Qθ (s, a), as a function of θ, satisfies certain
smoothness condition. Condition (ii) postulates that any local minimizer of Lω(θ) = L(θ, ω) can
be determined using second order optimality condition. This assumption is strictly weaker than the
strict saddle property (Ge et al., 2015), which is known to hold for a family of nonconvex functions.
Theorem 4.2. Under Assumptions 3.1 and 4.1, if supt≥0 kθt k2 < ∞, then {θt, ωt}t≥0 converges
almost surely to some (θ*,ω*), where ω* ∈ X and θ* is a local minimizer of the function L(∙,ω*).
In addition, if ∣∣ω*∣∣2 ≤ RX, we have θ* = ω*. Whereas if ∣∣ω*k2 = RX, we have θ* = λ ∙ ω* for
some λ ≥ 1.
To see the intuition of Theorem 4.2, consider the two-timescale gradient updates for the population
problem given in (3.6). In the faster timescale, we could fix {ωt}t≥0 at ω*. Then {θt}t≥0 converges
to a local minimizer of L(∙,ω), which is denoted by H(ω*). For the slower timescale, due to the
projection Πχ in (3.7), the dynamics of {ωt}t≥0 is characterized by a projected ODE ω = H(ω) 一
ω + ξω, where ξω (t) is the additional term introduced by projection. Consider any asymptotically
stable equilibrium ω* of this projected ODE. If ω* is in the interior of X, then projection is not
activated, and we have H(ω*) = ω*. If ω* is on the boundary of X, since X is a Euclidean ball,
H(ω*) — ω* must be in the same direction as ω*, which implies that H(ω*) = λ ∙ ω* with λ ≥ 1.
In the following, we lay out the assumption for the actor-critic algorithm.
Assumption 4.3. For the actor-critic algorithm, we assume that the following conditions holds.
(i)	. For any s ∈ S and any θ ∈ Rd, we assume that ∣Vθ(s)| ≤ Qmax, ∣∣VθVθ(s)k2 ≤ Gmax,
and VθVθ (s) is Lipschitz continuous. In addition, there exists a constant πmax > 0 such
that kVω log∏ω(a | s)k2 ≤ ∏max for all (s, a) ∈ S × A and ω ∈ Ω.
(ii)	. For function g(θ,ω) defined in (3.9), we assume that for each ω ∈ Ω, the ODE θ(t)=
g[θ(t), ω] has a local asymptotically stable equilibrium H(ω). Moreover, K) is Lipschitz
continuous in a neighborhood of ω .
The first condition in Assumption 4.3 is parallel to that in Assumption 4.1. This condition ensures
that both the value function and the policy function are regular. More importantly, as shown in §C.2,
under the faster timescale, we could fix {ω=}t≥o at some ω* ∈ Ω, and {θt}t≥0 converges to a stable
equilibrium of ODE θ = g(θ, ω*). Thus, condition (ii) ensures that this equilibrium, as a function
of ω, is locally Lipschitz. We note that condition (ii) is more restrictive compared with the second
condition in the previous assumption. The reason is that TD(0) update for the critic is not a gradient
step, i.e., g(θ, ω) is not equal to VθG(θ, ω). Thus, {θt}t≥0 cannot converge to a local minimizer of
G(∙, ω). Now we present the convergence result for the actor-critic algorithm.
Theorem 4.4. Under Assumptions 3.1 and 4.3, if supt≥0 kθtk2 < ∞, {θt, ωt} converges to some
{H(ω*), ω*}, where H(ω*) is an asymptotically stable equilibrium of ODE θ = g(θ, ω*). Moreover,
if ∣∣ω*k2 < Rω, we have VωF[H(ω*),ω*] = 0. Whereas if ∣∣ω*∣∣2 = Rω, then there exists λ ≥ 0
such that VωF[H(ω*),ω*] = λ ∙ ω*.
Similar to Theorem 4.2, to determine the convergence of {ωt}t≥0, we consider the slower timescale,
under which we have the projected ODE ω = VωF["(ω), ω] + ξω. By studying the equilibrium of
this ODE, we obtain the above theorem. An interesting case is that when ∣∣ω*∣2 < Rω, we have
g(θ*,ω*) = 0 and VωF(θ*,ω*) = 0, where θ* = H(ω*). Intuitively, this implies that actor-critic
converges to a local Nash equilibrium.
5	Numerical Experiments
In this section, we evaluate the performance of our online Q-learning and actor-critic algorithms
using OpenAI Gym (Brockman et al., 2016), which is a standard testbed for reinforcement learning.
7
Under review as a conference paper at ICLR 2019
Online Q-learning. First, we present the results for Algorithm 1 on Acrobot and MountainCar,
which are two classical control tasks. The value functions for Acrobot and MountainCar are neural
networks with one and two hidden layers, respectively, where all hidden layers have 64 neurons. To
corroborate with our theory, in the experiments, we fix βt = 10-3 for all t ≥ 0 and let αt be various
constants. We train the neural networks using minibatch stochastic gradient descent, where the batch
size is 32. The results are plotted in Figure 1, where three independent experiments are performed
for each setting. Here each curve is the mean value μ of the results in the same setting, together with
the shadow representing the interval [μ 一 σ∕2, μ + σ∕2], where σ is the standard deviation. Also,
in order to have a better visualization, we use a uniform moving average method with window size
20000 frames to smooth out the curves.
As shown in Figure 1, for both Acrobot and MountainCar, when αt < βt (green and blue lines), the
reward grows gradually and approaches the maximum reward. Whereas if αt > βt (dark yellow and
black lines), the reward finally approaches the minimum. This sharp transition justifies the validity
of using two-timescale updates with βt αt in the online Q-learning.
Figure 1: Training Acrobot and MountainCar using online Q-learning. Here we set βt = 10-3 for
both problems. In addition, for Acrobot, We let at be one of {10-2,8 ∙ 10-3,3 ∙ 10-4,10-4}. For
or MountainCar We set at to be one of {0.7,0.5, 5 ∙ 10-4,10-4}.
Two-timescale actor-critic algorithm. We also test the actor-critic algorithm on two Atari 2600
games: Pong and Breakout. Here we implement our algorithm based on the A3C framework (Mnih
et al., 2016) with 32 asynchronous agents in parallel. In these experiments, we fix stepsize of the
actor to at = 10-4 and let the stepsize of the actor βt be various constants. Similar to the previous
experiment, we plot the results in Figure 2 based on three independent trials, which is deferred to
the appendix.
As shown in Figure 2, for those settings with a larger critic stepsize βt (green and blue lines), the
reward gradually converges to its maximum, while for others with a smaller critic stepsize (dark
yellow and black lines), the rewards stay at a very low level. This agrees with our claim in Theorem
4.4 that the critic proceeds in a faster speed than the actor.
6	Conclusion
We study online reinforcement learning with nonlinear function approximation in general. Using
the unified framework of bilevel optimization, we propose online first-order algorithms for both
value function estimation and policy learning. Moreover, using stochastic approximation results, we
show that the asymptotic behavior of the algorithms are captured by ordinary differential equations.
Finally, we perform thorough empirical studies in support of our theory.
8
Under review as a conference paper at ICLR 2019
References
ShUn-Ichi Amari. Natural gradient works efficiently in learning. Neural COmputatiOn, 10(2):251-
276, 1998.
Leemon Baird et al. Residual algorithms: Reinforcement learning with function approximation. In
InternatiOnal COnference On Machine Learning, pp. 30-37, 1995.
Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski,
Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based
navigation using grid-like representations in artificial agents. Nature, pp. 1, 2018.
Jonathan Baxter and Peter L Bartlett. Direct gradient-based reinforcement learning. In InternatiOnal
SympOsium On Circuits and Systems, pp. 271-274, 2000.
Shalabh Bhatnagar. An actor-critic algorithm with function approximation for discounted cost con-
strained Markov Decision Processes. Systems & COntrOl Letters, 59(12):760-766, 2010.
Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton. Incremental natu-
ral actor-critic algorithms. In Advances in Neural InfOrmatiOn PrOcessing Systems, pp. 105-112,
2008.
Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba
Szepesvari. Convergent temporal-difference learning with arbitrary smooth function approxi-
mation. In Advances in Neural InfOrmatiOn PrOcessing Systems, pp. 1204-1212, 2009a.
Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic
algorithms. AutOmatica, 45(11):2471-2482, 2009b.
Vivek S Borkar. StOchastic ApprOximatiOn: A Dynamical Systems ViewpOint. Cambridge University
Press, 2008.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Dotan Di Castro and Ron Meir. A convergent online single-time-scale actor-critic algorithm. JOurnal
Of Machine Learning Research, 11(Jan):367-410, 2010.
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Concentration bounds for two
timescale stochastic approximation with applications to reinforcement learning. arXiv preprint
arXiv:1703.05376, 2017a.
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis for td (0) with
linear function approximation. arXiv preprint arXiv:1704.01161, 2017b.
Asen L Dontchev and William W Hager. Implicit functions, lipschitz maps, and stability in opti-
mization. Mathematics Of OperatiOns Research, 19(3):753-768, 1994.
Joseph L Doob. StOchastic prOcesses, volume 7. Wiley New York, 1953.
Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction
methods for policy evaluation. In InternatiOnal COnference On Machine Learning, pp. 1049-1058,
2017.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In COnference On Learning TheOry, pp. 797-842, 2015.
Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. arXiv preprint
arXiv:1702.07121, 2017.
9
Under review as a conference paper at ICLR 2019
Sham M Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems,
pp.1531-1538,2002.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems, pp. 1008-1014, 2000.
Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and
Applications. Springer, New York, NY, 2003.
Harold Joseph Kushner and Dean S Clark. Stochastic Approximation Methods for Constrained and
Unconstrained Systems. Springer Science & Business Media, 1978.
Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforce-
ment learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient td algorithms. In Conference on Uncertainty in Artificial Intelli-
gence, pp. 504-513. AUAI Press, 2015.
Hamid Reza Maei. Convergent actor-critic algorithms under off-policy training and function ap-
proximation. arXiv preprint arxiv:1802.07842, 2018.
Hamid Reza Maei, Csaba Szepesvari, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy
learning control with function approximation. In nternational Conference on International Con-
ference on Machine Learning, pp. 719-726, 2010.
Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning with
function approximation. In International Conference on Machine Learning, pp. 664-671. ACM,
2008.
Michel Metivier and Pierre Priouret. Applications of a Kushner and Clark lemma to general classes
of stochastic algorithms. IEEE Transactions on Information Theory, 30(2):140-151, 1984.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal ofMachine
Learning Research, 9(May):815-857, 2008.
Jacques Neveu. Discrete-parameter martingales. Elsevier, 1975.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7):1180-1190, 2008.
HL Prasad, LA Prashanth, and Shalabh Bhatnagar. Actor-critic algorithms for learning Nash equi-
libria in n-player general-sum games. arXiv preprint arXiv:1401.2086, 2014.
LA Prashanth, Abhranil Chatterjee, and Shalabh Bhatnagar. Two timescale convergent q-learning
for sleep-scheduling in wireless sensor networks. Wireless networks, 20(8):2589-2604, 2014.
Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforce-
ment learning method. In European Conference on Machine Learning, pp. 317-328. Springer,
2005.
Herbert Robbins, Sutton Monro, et al. A stochastic approximation method. The Annals of Mathe-
matical Statistics, 22(3):400-407, 1951.
10
Under review as a conference paper at ICLR 2019
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of Go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go
without human knowledge. Nature, 550(7676):354-359, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. Cambridge: MIT
press, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Infor-
mation Processing Systems, pp. 1057-1063, 2000.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesvari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learn-
ing with linear function approximation. In International Conference on Machine Learning, pp.
993-1000. ACM, 2009.
Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem
of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1):
2603-2631, 2016.
John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. In Advances in Neural Information Processing Systems, pp. 1075-1081, 1997.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Huizhen Yu. On convergence of emphatic temporal-difference learning. In Conference on Learning
Theory, pp. 1724-1751, 2015.
11
Under review as a conference paper at ICLR 2019
A The Policy Evaluation and Actor-Critic Algorithms
Algorithm 2 Online policy evaluation with nonlinear function approximation.
1:	Input: Initialization θ0 = ω0 ∈ Θ, stepsizes {αk}, {βk}, and sampling distribution ρ ∈
∆(S × A).
2:	Output: The sequences {θt}t≥0 and {ωt}t≥0 .
3:	for t = 0, 1, 2, . . . do
4:	Sample (st, at)〜ρ, observe reward R(st, at) and next state s；.
5:	Compute the TD error δ; = Qθt - Rt — Y ∙ max。*/ Qωt (St, a).
6:	Update the function estimates by
θt+ι =πω[(I - at) ∙ ωt +	αt	∙ θt],	θt+ι	=	θt	-	Bt	∙	δt	∙	VθQ§七(St, At).	(A.1)
7:	end for
Algorithm 3 The actor-critic algorithm with nonlinear function approximation.
1:	Input: Initialization ω0 ∈ Rd and θ0 ∈ Rp, stepsizes {αt, βt}t≥0.
2:	Output: The sequences {θt}t≥0 and {ωt}t≥0 .
3:	for t = 0, 1, 2, . . . do
4:	Sample St 〜ρωt, take action at 〜∏ωt (∙ | st), observe reward R(st, at) and the next state s；.
5:	Compute the TD-error δt = R(st, at) + Y ∙ Vot (St) 一 ^¾ (st).
6:	Update the parameters by
ωt+1 =	πΩ[ωt	+ αt	∙	Vω log πωt (at	| st) ∙	δt],	θt+1	=	θt	+ Bt ∙ δt	∙ ▽&Vθt (st)∙ (A.2)
7:	end for
B Additional Figures of Numerical Experimetns
Ponq
20
Oooo
1 1 2
- -
P-IBMB」
400
350
Breakout
Ooooooo
5
p-IBM.!
01234567	0.0	0.2	0.4	0.6	0.8
frame	le7	frame	le8
Figure 2: Training Pong and Breakout using two timescale actor-critic algorithm, where we use 32
agents, minibatch of size 20, with a fixed actor network stepsize αt ≡ 10-4, but a varied critic
network stepsize βt ∈ {10-7, 2 ∙ 10-7, 5 ∙ 10-4,10-3}. Left: training Pong. Right: training
Breakout.
C	Proofs of the Main Results
In this section, we lay out the proofs of Theorems 4.2 and 4.4.
12
Under review as a conference paper at ICLR 2019
C.1 Proof of Theorem 4.2
Proof. Our proof consists of two steps. The first step consists of the analysis under the faster
timescale, where We show that in this case We could fix {ωt}t≥o at some ω* ∈ X and only fo-
cus on {θt}t≥0. In the second step, we look into the slower timescale, which yields the convergence
result for {ωt}t≥0 and completes the proof.
Step 1. Faster timescale. We first consider the convergence of {θt, ωt}t≥1 in the faster timescale.
Using stochastic approximation, we will show that the sequence {θt }t≥0 defined in (3.7) tracks a
local minimizer of the lower level optimization.
To begin with, for notational simplicity, we define
δt = Qθt (St ,at) - r(st ,at) - Y ∙ Qωt (st, at), Φt = R θ Qθt (st,at),
for all t ≥ 0, where a0t = argmaxa∈A Qθt (s0t, a). Then the updating rules in Algorithms 1 reduce
to
ωt+ι J	πx [ωt	+ αt	∙ (θt -	ωt)],	θt+ι J θt	- βt	∙	δt	∙ φt,	(CI)
where {αt, βt}t≥0 are the stepsizes, and ΠX is the projection operator onto set X, which is assumed
to be a closed Euclidean ball with radius Rω. In addition, let F(I) = σ({θ',ωg}'≤t) bethe σ-algebra
generated by the parameter iterates until time t. By definition, θt and ωt are Ft(1)-measurable for
any t ≥ 1. Furthermore, for any θ ∈ Rd and ω ∈ Rp , we define
h(θ,ω) = E(s,a)〜ρ{[Qθ(s, a) - (T*Qω)(s, a)] ∙RθQθ(s,a)},	(C.2)
Thus, by the definition of the Bellman optimality operator in , for any t ≥ 0, we have
E[δtφt IF(I)] = h(θt,ωt). This implies that {Zt}t≥o defined by Zt = h(θt,ωt) - δt ∙ φt is a
martingale difference sequence with respect to the filtration {Ft(1)}t≥0.
Moreover, for any compact set X ⊆ Rd, we denote by CX (x) the outer normal cone of X at x ∈ X.
That is, CX (x) = {u ∈ Rd : u>(y - x) ≤ 0, ∀y ∈ X}. In particular, if x is in the interior of X,
then CX (x) is empty. Moreover, since we assume X is an Euclidean ball with radius RX, for any x
in the boundary of X, denoted by ∂X, we have CX(x) = {λ ∙ X: λ ≥ 0}. Using the notion of the
outer normal cone, the first equation in (C.1) can be written as
ωt+ι = ωt + at ∙ (θt - ωt + ξt),	(C.3)
where ξt ∈ -CX(ωt+ι) is the correction term caused by projection onto X. Since (1 - at) ∙ ωt ∈ X
and ωt+ι is the projection of ωt + at ∙ (θt - ωt), we have
∣∣ξt∣∣2 = 1/at ∙ inf IM + at ∙ (θt - ωt) - y∣∣2 ≤ ∣∣θtk2
y∈X
for all t ≥ 0. Then, writing the two equations in (C.1) together, we have
θt+1	θt + β	-h(θt, ωt) + ζt
Vωt+ι)	∖ωJ	t 1%/仇∙ (θt- ωt + ξt)√.
(C.4)
(C.5)
In the following, we apply the result on ODE approximation to (C.5). First note that under Assump-
tion 4.1, ∣Qθ(s, a)|, ∣VθQθ(s, a)∣2 are bounded by QmaX and Gmax, respectively. This implies
that implies that ∣δt ∙ φt∣∣ is bounded by 2Qmaχ ∙ GmaX for all t ≥ 1. Thus, {Zt}t≥0 is a bounded
martingale sequence. Then there exist an absolute constant C > 0 such that
E[kZtk2 IFt] ≤ C forall t ≥ 1.
Moreover, let MT = PT=O βt ∙ Zt for any T ≥ 0. Since Pt≥0 β2 < ∞, by the Martingale
convergence theorem (Proposition VII-2-3(c) on page 149 of Neveu (1975)), {MT}T ≥0 converges
almost surely. Additionally, MT is square integrable with EkMT∣∣2 ≤ C ∙ Pt≥0 β2. Thus, for any
> 0, by Doob’s martingale inequality (Doob, 1953), we have
π√ IIS a ʃ Il . λ , SUPN ≥T E[kMN - MT ∣∣2]	2C2 Pt≥T β2
P SUP ∣∣Mn - MTk≥ € ≤ -----------=------2--------- ≤ -------宁------,
N≥T	2	2
13
Under review as a conference paper at ICLR 2019
which converges to zero as T goes to infinity. Furthermore, under Assumption 3.1 and the assump-
tion that supt≥o ∣∣θt ∣∣2 < ∞, it holds that ɑt∕βt ∙ (θt — ωt + ξt) = 0 as t goes to infinity. Besides,
since both VθQθ(s, a) and Qθ(s, a) are bounded and LiPSchitz continuous under Assumption 4.1,
h(θ, ω) defined in (C.2) is Lipschitz in both θ and ω.
Now we apply Theorem D.2 to sequence {(θt, ωt)}t≥0, which implies that the asymptotic behavior
of {(θt, ωt)}t≥0 converges almost surely to the set of asymptotically stable equilibria of the ODE
θ = -h(θ,ω),	ω = 0.	(C.6)
Specifically, the set of asymptotically stable equilibria of the ODE in (C.6), denoted by K1, is
Ki = {(θ*,ω*): ω* ∈ X,h(θ*,ω*)=0}.	(C.7)
For any ω ∈ Rd, we denote Lω(θ) = L(θ, ω) = ∣∣Qθ 一 T*Qω IlP. Since h(θ, ω) = VθLω* (θ), the
asymptotically stable equilibrium of ODE θ = -VθLω*(θ) is the local minima of Lω* (θ).
Therefore, for the analysis under the faster timescale, essentially We can fix {ωt}t≥0 at some ω* and
consider solely the asymptotic behavior of {θt}t≥0. In this case, {θt}t≥0 converges almost surely
to a local minimizer θ* of Lω* (∙). We note that Lω* (∙) may have multiple local minimizers. In
this case, the local minimizer that {θt}t≥0 converges to is determined by the basin of attraction that
{θt}t≥o enters. Moreover, note that under Assumption 4.1, VjLω(∙) is positive definite at all its
local minima, which implies that the set of all local minima of Lω* (∙) is a disjoint set.
Consider the equation VθLω* (θ) = 0. Note that θ = θ* is a solution. Besides, by the continuity of
V^Lω (∙), there exists a open neighborhood U of θ* and a positive number CL such that
V2Lω*(θ) A Cl ∙ Id	∀θ ∈Uθ.
By Lipschitz implicit function theorem (Dontchev & Hager, 1994), there exist a neighborhood U of
θ*, a neighborhood V of ω*, and a mapping H: V → U such that H(ω*) = θ*. In addition, H is
Lipschitz continuous and satisfies VθL(θ, ω)∣θ=^ω) = 0 for all ω ∈ V.
Thus, we show that, in the faster timescale, {θt,ωt}t≥0 converges almost surely to [H(ω*),ω*],
where ω* ∈ X and H is Lipschitz continuous.
Step 2. Slower timescale.
Note that (C.7) cannot characterize the convergence of {ωt}t≥0. Now we look into the dynamics in
the slower timescale for a finer characterization. Recall that the update rule of the {ωt}t≥0 is given
in (C.3). For ease of presentation, we define σ-field F(2) = σ({ωg}g≤t) for all t ≥ 0. b In addition,
we define
ψt = E(θt	一	ωt)	|	Ft(2)	一 [(H(ωt)	一 ωt]	= Eθt	| Ft(2)	一 H(ωt).	(C.8)
Then (C.3) becomes ωt+ι = ωt + αt ∙ [(H(ω∕ — ωt] + at ∙ (ψt + ξt), where ξt ∈ -Cχ(ωt+ι).
As we have shown in the first step of the proof, H(ωt) 一 θt converges to zero as t goes to infinity,
where the mapping H : Rd → Rd is Lipschitz continuous. In addition, since supt≥0 ∣θt ∣2 is finite,
ψt defined in (C.8) is uniformly bounded for all t ≥ 0.
Now we apply the Kushner-Clark lemma (Kushner & Clark, 1978, see also Theorem E.2 in §E for
details.) to sequence {ωt}t≥0, it holds that {θt}t≥1 converges almost surely to the set of asymptoti-
cally stable equilibria of the ODE
ω = H(ω) — ω + ξω,	ξω(t) ∈ -Cχ(ω(t)),	(C.9)
where the function ξω appears due to the projection in (C.11). Recall that X is a Euclidean ball with
radius RX. For any asymptotically stable equilibrium ω* of (C.9), if ω* is in the interior of X, we
have CX(ω*) = 0, which implies that ω* = H(ω*). In this case, sequence {(θt, ωt)}t≥ι converges
almost surely to (ω*,ω*), which satisfies that
E(s,a)〜ρ{[Qω* (s, a) -(T*Qω* )(s, a)] ∙ vθ Qω* (s, a) } = 0.
14
Under review as a conference paper at ICLR 2019
Meanwhile, if ω* ∈ X, We have H(ω*) - ω* ∈ CX(ω*), which implies that there exists λ > 0 such
that H(ω*) = λ ∙ θ*. Thus, {(θt, ωt)}t≥ι converges almost surely to (λ ∙ ω*,ω*). Therefore, we
conclude the proof of Theorem 4.2.	□
C.2 Proof of Theorem 4.4
Proof. The proof is similar to that of Theorem 4.2 in §C.1. We prove the theorem in two steps,
considering the faster and slower timescales seperately.
Step 1. Faster timescale. We first consider the convergence of {θt, ωt}t≥1 in the faster timescale.
Using stochastic approximation, we will show that the critic sequence {θt}t≥0 converges to an
asymptotically stable equilibrium of ODE θ = g(θ.ω*) for some ω* ∈ Ω.
First recall that, for any t ≥ 0, we have St 〜ρωt, at 〜∏ωt (∙ | st), and St 〜 P(∙ | st, at). To simplify
the notation, we define
δt =	Vθt (st) - r(st, at)	- Y ∙呃(st),	φt = VθV⅛(st),	At	=	Vω	log∏ωt (at	|	st).	(C.10)
Using these terms, the updating rules in Algorithm 3 become
ωt+ι  ∏ω [ωt	+ at	∙	δt	∙	At],	θt+ι	-	θt	+ βt ∙ δt	∙ φt,	(C.11)
where {at, βt}t≥o are the stepsizes, and ∏ω is the projection operator onto Ω. In addition, under
Assumption 4.3, we have ∣Vθ(s)| ≤ VmaX and ∣∣Vω log∏ω(a | s)∣∣2 ≤ ∏maχ. Thus δt and At defined
in (C.10) satisfy ∣δ∕ ≤ 2Vm&x and ∣∣At∣ ≤ ∏max for all t ≥ 1. Combining (C.11) and Assumption
3.1, this implies that αt∕βt ∙ δt ∙ At converges to zero as t goes to infinity.
Moreover, let Ft'1 = σ({θ', ωg}'≤t). Recall that we define function g(θ, ω) in (3.9). To simplify
the notation, let h(θ, ω) = VωF(θ, ω, which is defined in (3.10). Then by (C.10), we have
E[δt ∙	φt	|F(1)] =	g(θt,ωt),	E[δt	∙	At	|F⑴]=h(θt,ωj	(C.12)
which implies that {Zt}t≥0 is a martingale difference sequence, where we define Zt = δt ∙ φt -
g(θt, ωt) for all t ≥ 0. Moreover, under Assumption 4.3, we have ∣∣δt ∙ φt∣∣2 ≤ 2Vm&x ∙ GmaX, which
implies that there exists an absolute constant C such that E[∣ζt ∣22] < C. Therefore, similar to the
proof of Theorem 4.2 in §C.1, by applying Theorem D.2, we obtain that, in the faster timescale,
sequence {θt, ωt}t≥0 converges almost surely to the asymptotically stable equilibria of ODE sys-
tem {θ = g(θ,ω),ω = 0}. Under Assumption 4.3, this implies that {ωt}t≥0 converges to some
ω*, while {θt}t≥0 converges to a local asymptotically stable equilibrium θ* = H(ω*). Moreover,
H: Rp → Rp is Lipschitz continuous in a open neighborhood of ω*.
Step 2. Slower timescale. In the sequel, we characterize the convergence of {θt, ωt}t≥1 under the
slower timescale. To begin with, we define F(2) = σ({ωg}g≤t) for all t ≥ 0. In addition, we define
ψ(11 = -δt ∙ At + E[δt ∙ At | Ft⑵], ψ(21 = -E[δt ∙ At | F2] + h[θt, ω(θt)],	(C.13)
where function g isdefinedin(3.9), At and φt are defined in (C.10). Since ∣∣δt∙At∣2 ≤ 2Vmaχ∙Gmax,
{ψt(1)}t≥1 is a bounded martingale difference sequence. Moreover, since Ft(2) ⊆ Ft(1), by the tower
property and (C.12), we have
E[δt ∙ At | F(21] = E[E(δt ∙ At | F(I)) ∣ F(21] = E[h(θt, ωt) ∣ F(2)].
Using the notation in (C.13), the primal update in (C.11) can be written as
ωt+ι = πω [ωt + at ∙ h[ωt, "(ωt)] + at ∙ ψ(1) + at ∙ ψ(21],	(C.14)
where H(ω*) is a local asymptotically stable attractor of ODE θ = h(θ,ω*). Moreover, under
Assumption 4.3, VθVθ(s) and Vω log πω(a | s) are bounded, which implies that
∣h(θ(1),ω) - h(θ(2),ω)k ≤ 2Gmaχ ∙ ∏max
15
Under review as a conference paper at ICLR 2019
for any θ(1), θ(2) ∈ Rd and any ω ∈ Ω. Then, by CaUchy-SchWarz inequality, We have
kψ(2)k2 ≤ E{∣∣h(θt,ωt) - h[⅜ωt),ωt]∣∣2∣F(2)} ≤ 2Gmaχ ∙ ∏maχ ∙ E[kθt -必ωt)k2 ∣Ft⑵],
which tends to zero as t goes to infinity. As shown in the first step of the proof, ι0(ωt) - ωt converges
to zero as t goes to infinity.
Furthermore, we define WT = PT=I αt ∙ ψ(1) ∈ Rd for any T ≥ 1. Since Pt≥ι 02 < ∞ and ψ(1)
is bounded, {WT}T ≥1 is a square-integrable martingale sequence, which converges almost surely
by the Martingale convergence theorem (Neveu, 1975). Moreover, Doob’s martingale inequality
(Doob, 1953) implies that
lim P sup kWN - WT k ≥
T→∞	N≥T
supN ≥T E[kWN - WT k2]
≤ lim ----=----5--------
T→∞	2
lim
T→∞
≥T αt2
目
0.
≤
Finally, applying by the Kushner-Clark lemma (Kushner & Clark, 1978) to sequence {ωt }t≥0, it
holds that {ωt}t≥0 converges almost surely to the set of asymptotically stable equilibria of the ODE
ω = h[*ω),ω]+ ξω,	ξω(t) ∈ -Cn(ω(t)),	(C.15)
where Cq3 is the outer normal cone of Ω at ω. Recall that we assume that Ω is a Euclidean ball
with radius Rω. Thus, for any asymptotically stable equilibrium ω* of (C.9), if ω* is in the interior
of Ω, i.e., kω*k < Rω, we have h[H(ω*), ω*] = 0. Additionally, if ∣∣ω*k2 = Rω, then there exists
λ ≥ 0 such that h[^(ω*),ω*] = λ ∙ ω*. Therefore, we conclude the proof of Theorem 4.4.	□
D Background on Stochastic Approximation
We first present a fundamental result for stochastic approximation which is obtained from Borkar
(2008). Consider a sequence of iterations in Rd
xt+i = Xt + γt ∙ [G(χt) + Mt+ι], t ≥ 0, xo ∈ Rd.	(D.1)
Here G : Rd → Rd is a deterministic mapping, Mt+1 ∈ Rd is a random vector, and γt > 0 is the
stepsize.
Assumption D.1. We make the following assumption on the iteration (D.1).
•	Function G : Rd → Rd is Lipschitz continuous.
•	The stepsizes {γt}t≥0 satisfies Pt≥0 γt = ∞ and Pt≥0 γt2 < ∞;
•	Random vectors {Mt }t≥0 is a martingale difference sequence. That is, M0 = 0,
E[Mt+1 | xτ , Mτ , τ ≤ t] = 0. Moreover, we assume that there exists some K > 0 such
that
E(∣Mt+ιk2 |xτ,Mτ,τ ≤ t) ≤ K ∙ (1 + ∣xtk2).
Then the asymptotic behavior of the {xt}t≥0 in (D.1) is characterized by ODE
X = G(X)	(D.2)
Suppose Eq. (D.2) has a unique globally asymptotically stable equilibrium x*, we then have the
following two theorems.
Theorem D.2. Under Assumption D.1, if supt ∣Xt∣2 < ∞ almost surely, we have Xt → X* almost
surely.
Theorem D.3. Under Assumption D.1, suppose
lim Gcx) = G∞(x)
c→∞ c
exists uniformly on compact sets for some G∞ ∈ C(Rn). If the ODE y = G∞(y) has origin as the
unique globally asymptotically stable equilibrium, then we have
sup ∣Xt∣ < ∞ almost surely.
t
16
Under review as a conference paper at ICLR 2019
E Kushner-Clark Lemma
We state here the well-known Kushner-Clark Lemma (Kushner & Clark, 1978; Metivier & Priouret,
1984; Prasad et al., 2014) in the sequel.
Let Γ be an operator that projects a vector onto a compact set X ⊆ RN. Define a vector Γ(∙) as
b[h(x)] = lim (「[x + ηh(X)I -X ),
0<η→0	η
for any X ∈ X and with h : X → RN continuous. Consider the following recursion in N dimensions
xt+1 =Γ{xt + γt[h(xt) + ξt + βt]}.	(E.1)
The ODE associated with (E.1) is given by
公--	，	、r
X = Γ[h(x)].	(E.2)
Assumption E.1. We make the following assumptions:
•	h(∙) is a continuous RN-valued function.
•	The sequence {βt}, t ≥ 0 is a bounded random sequence with βt → 0 almost surely as
t → ∞.
•	The stepsizes γt , t ≥ 0 satisfy γt → 0 as t → ∞ and Pt γt = ∞.
•	The sequence ξt, t ≥ 0 satisfies for any > 0
lim P sup X γτξτ	≥	= 0.
t	n≥t τ=t	2
Then the Kushner-Clark Lemma says the following.
Theorem E.2. Under Assumption E.1, suppose that the ODE (E.2) has a compact set K as its set
of asymptotically stable equilibria. Then Xt in (E.1) converges almost surely to K as t → ∞.
17