Under review as a conference paper at ICLR 2019
Effective and Efficient Batch Normalization
Using Few Uncorrelated Data for Statistics’
Estimation
Anonymous authors
Paper under double-blind review
Ab stract
Deep Neural Networks (DNNs) thrive in recent years in which Batch Normaliza-
tion (BN) plays an indispensable role. However, it has been observed that BN is
costly due to the reduction operations. In this paper, we propose alleviating the
BN’s cost by using only a small fraction of data for mean & variance estimation at
each iteration. The key challenge to reach this goal is how to achieve a satisfactory
balance between normalization effectiveness and execution efficiency. We iden-
tify that the effectiveness expects less data correlation while the efficiency expects
regular execution pattern. To this end, we propose two categories of approach:
sampling or creating few uncorrelated data for statistics’ estimation with certain
strategy constraints. The former includes “Batch Sampling (BS)” that randomly
selects few samples from each batch and “Feature Sampling (FS)” that randomly
selects a small patch from each feature map of all samples, and the latter is “Vir-
tual Dataset Normalization (VDN)” that generates few synthetic random samples.
Accordingly, multi-way strategies are designed to reduce the data correlation for
accurate estimation and optimize the execution pattern for running acceleration in
the meantime. All the proposed methods are comprehensively evaluated on vari-
ous DNN models, where an overall training speedup by up to 21.7% on modern
GPUs can be practically achieved without the support of any specialized libraries,
and the loss of model accuracy and convergence rate are negligible. Furthermore,
our methods demonstrate powerful performance when solving the well-known
“micro-batch normalization” problem in the case of tiny batch size.
1	Introduction
Recent years, Deep Neural Networks (DNNs) have achieved remarkable success in a wide spectrum
of domains such as computer vision (Krizhevsky et al., 2012) and language modeling (Collobert &
Weston, 2008). The success of DNNs largely relies on the capability of presentation benefit from
the deep structure (Delalleau & Bengio, 2011). However, training a deep network is so difficult to
converge that batch normalization (BN) has been proposed to solve it (Ioffe & Szegedy, 2015). BN
leverages the statistics (mean & variance) of mini-batches to standardize the activations. It allows
the network to go deeper without significant gradient explosion or vanishing (Santurkar et al., 2018;
Ioffe & Szegedy, 2015). Moreover, previous work has demonstrated that BN enables the use of
higher learning rate and less awareness on the initialization (Ioffe & Szegedy, 2015), as well as
produces mutual information across samples (Morcos et al., 2018) or introduces estimation noises
(Bjorck et al., 2018) for better generalization.
Despite BN’s effectiveness, it is observed that BN introduces considerable training overhead due to
the costly reduction operations. The use of BN can lower the overall training speed (mini second
per image) by >45% (Wu et al., 2018), especially in deep models. To alleviate this problem, several
methods were reported. Range Batch Normalization (RBN) (Banner et al., 2018) accelerated the for-
ward pass by estimating the variance according to the data range of activations within each batch. A
similar approach, L1-norm BN (L1BN) (Wu et al., 2018), simplified both the forward and backward
passes by replacing the L2-norm variance with its L1-norm version and re-derived the gradients for
backpropagation (BP) training. Different from the above two methods, Self-normalization (Klam-
bauer et al., 2017) provided another solution which totally eliminates the need ofBN operation with
1
Under review as a conference paper at ICLR 2019
an elaborate activation function called “scaled exponential linear unit” (SELU). SELU can automat-
ically force the activation towards zero mean and unit variance for better convergence. Nevertheless,
all of these methods are not sufficiently effective. The strengths of L1BN & RBN are very limited
since GPU has sufficient resources to optimize the execution speed of complex arithmetic operations
such as root for the vanilla calculation of L2-norm variance. Since the derivation of SELU is based
on the plain convolutional network, currently it cannot handle other modern structures with skip
paths like ResNet and DenseNet.
In this paper, we propose mitigating BN’s computational cost by just using few data to estimate
the mean and variance at each iteration. Whereas, the key challenge of this way lies at how to
preserve the normalization effectiveness of the vanilla BN and improve the execution efficiency in
the meantime, i.e. balance the effectiveness-efficiency trade-off. We identify that the effectiveness
preservation expects less data correlation and the efficiency improvement expects regular execution
pattern. This observation motivates us to propose two categories of approach to achieve the goal of
effective and efficient BN: sampling or creating few uncorrelated data for statistics’ estimation with
certain strategy constraints.
Sampling data includes “Batch Sampling (BS)” that randomly selects few samples from each batch
and “Feature Sampling (FS)” that randomly selects a small patch from each feature map (FM) of all
samples; creating data means “Virtual Dataset Normalization (VDN)” that generates few synthetic
random samples, inspired by Salimans et al. (2016). Consequently, multi-way strategies including
intra-layer regularity, inter-layer randomness, and static execution graph during each epoch, are de-
signed to reduce the data correlation for accurate estimation and optimize the execution pattern for
running acceleration in the meantime. All the proposed approaches with single-use or joint-use are
comprehensively evaluated on various DNN models, where the loss of model accuracy and con-
vergence rate is negligible. We practically achieve an overall training speedup by up to 21.7% on
modern GPUs. Note that any support of specialized libraries is not needed in our work, which is
not like the network pruning (Zhu et al., 2018) or quantization (Hubara et al., 2017) requiring extra
library for sparse or low-precision computation, respectively. Most previous acceleration works tar-
geted inference which remained the training inefficient (Wen et al., 2016; Molchanov et al., 2016;
Luo et al., 2017; Zhang et al., 2018b; Hu et al., 2018), and the rest works for training acceleration
were orthogonal to our approach (Lin et al., 2017; Goyal et al., 2017; You et al., 2017). Addition-
ally, our methods further shows powerful performance when solving the well-known “micro-batch
normalization” problem in the case of tiny batch sizes.
In summary, the major contributions of this work are summarized as follows.
•	We propose a new way to alleviate BN’s computational cost by using few data to estimate
the mean and variance, in which we identify that the key challenge is to balance the normal-
ization effectiveness via less data correlation and execution efficiency via regular execution
pattern.
•	We propose two categories of approach to achieve the above goal: sampling (BS/FS) or
creating (VDN) few uncorrelated data for statistics’ estimation, in which multi-way strate-
gies are designed to reduce the data correlation for accurate estimation and optimize the
execution pattern for running acceleration in the meantime. The approaches can be used
alone or jointly.
•	Various benchmarks are evaluated, on which up to 21.7% practical acceleration is achieved
for overall training on modern GPUs with negligible accuracy loss and without specialized
library support.
•	Our methods are also extended to the micro-BN problem and achieve advanced perfor-
mance1.
In order to make this paper easier for understanding, we present the organization of the whole paper
in Fig. 14, Appendix G.
1Due to the limitation of pages, this part is represented in Appendix F.
2
Under review as a conference paper at ICLR 2019
2	Motivation, Opportunity, and Challenge
2.1	Motivation: Costly Batch Normalization & the Bottleneck Analysis
The activations in one layer for normalization can be described by a d-dimensional activation feature
X = (x(1), .., x(d)), where for each feature we have x(k) = (x(1k), .., x(mk)). Note that in convolu-
tional (Conv) layer, d is the number of FMs and m equals to the number of points in each FM across
all the samples in one batch; while in fully-connected (FC) layer, d and m are the neuron number
and batch size, respectively. BN uses the statistics (mean E[x(k)] & variance V ar[x(k)]) of the
intra-batch data for each feature to normalize activation by
b(k) = Xxkk - E[x(k)L, y(k) = γ(k) b(k) + β(k)
V ar[x(k)] +
(1)
where γ(k) & β(k) are trainable parameters introduced to recover the representation capability, is
a small constant to avoid numerical error, and E[X(k)] & V ar[X(k)] can be calculated by
mm
E[x(k)] = — XXjk), Var[χ(k)] = — X(Xjk)- E[x(k)])2.	(2)
mj	m j
The detailed operations of a BN layer in the backward pass can be found in Appendix C.
Iter. per second.
Model w/o BN w/ BN
-C	267~17.7(-33.7%)
D	7.7	5.2(-32.5%)
E	5.4	3.1(-42.6%)
(a)	(b)
Figure 1: Illustration of BN’s cost, operations, bottleneck, and opportunity: (a) iterations per second
for training various models with or without BN, Model C,D,E are defined in Table 2; (b) usual
optimization of the reduction operation using adder tree; (c) the computational graph of BN in the
forward pass (upper) and backward pass (lower); (d) the computation graph ofBN using few data for
statistics’ estimation in forward pass (upper) and backward pass (lower). x is neuronal activations,
μ and σ denote the mean and standard deviation of X within one batch, respectively, and P is the
summation operation.
From Fig. 1(a), we can see that adding BN will significantly slow down the training speed (iterations
per second) by 32%-43% on ImageNet. The reason why BN is costly is that it contains several
”reduction operations”, i.e. Pjm=1. We offer more thorough data analysis in Appendix E. If the
reduction operations are not optimized, it’s computational complexity should be O(m). With the
optimized parallel algorithm proposed in (Che et al., 2008), the reduction operation is transformed
to cascaded adders of depth of log(m) as shown in Fig. 1(b). However, the computational cost is
still high since we usually have m larger than one million. As shown in Fig. 1(c), the red ”P”s
represent operations that contain summations, which cause the BN inefficiency.
2.2	Opportunity: Using Few Data for Statistics’ Estimation
Motivated by the above analysis, decreasing the effective value of m at each time for statistics
estimation seems a promising way to reduce the BN cost for achieving acceleration. To this end,
we propose using few data to estimate the mean and variance at each iteration. For example, if m
changes to a much smaller value of s, equation (2) can be modified as
1s	1s
E[x(k)] ≈ E[χSkk] = - EXjk), Var[x(k)] ≈ Var[χSk)] =	£(xjk)- E[xSkk])2	(3)
j=1
j=1
3
Under review as a conference paper at ICLR 2019
where x(sk) denotes the small fraction of data, s is the actual number of data points, and we usually
have s	m. Here we denote s/m as Sampling Ratio (it includes both the cases of sampling
and creating few data in Section 3.1 and 3.2, respectively). Since the reduction operations in the
backward pass can be parallelized whereas in the forward pass, the variance can not be calculated
until mean is provided (which makes it nearly twice as slow as backward pass), we just use few data
in the forward pass. The computational graph of BN using few data is illustrated in Fig. 1(d). The
key is how to estimate E[x(k)] & V ar[x(k)] for each neuron or FM within one batch with much
fewer data. The influence on the backward pass is discussed in Appendix C.
2.3	Challenge: Effectiveness-Efficiency Balance
Although using few data can reduce the BN’s cost dramatically, it will meet an intractable challenge:
how to simultaneously preserve normalization effectiveness and improve the execution efficiency.
On one side, using few data to estimate the statistics will increase the estimation error. By regarding
the layers with high estimation error as unnormalized ones, we did contrast test in Fig. 2. The mean
& variance will be scaled up exponentially as network deepens, which causes the degradation of
BN’s effectiveness. This degradation can be recovered from two aspects.
•	Intra-layer. For the reason that the estimation error is not only determined by the amount
of data but also the correlation between them, we can sample less correlated data within
each layer to improve the estimation accuracy.
•	Inter-layer. As depicted in by Fig. 2, the intermittent BN configuration (i.e. discon-
tinuously adding BN in different layers) can also prevent the statistics scaling up across
layers. This motivates us that as long as layers with high estimation error are discontinu-
ous, the statistics shift can still be constrained to a smaller range. Therefore, to reduce the
correlation between estimation errors in different layers can also be beneficial to improve
the accuracy of the entire model, which can be achieved by sampling less correlated data
between layers.
On the other side, less data correlation indicates more randomness which usually causes irregular
memory access irregular degrading the running efficiency. In this paper, we recognize that the
overhead of sampling can be well reduced by using regular and static execution patterns, which is
demonstrated with ablation study at Fig. 9 (c), Sec. 4.2.
Figure 2: The influence of BN configuration on deep models: (a) & (b) average mean & variance
of each layer during the first training epoch; (c) loss curve during the first training epoch. Here we
train a 30-layer CNN on CIFAR-10. For “without BN” curve, BN is only applied to the last Conv
layer; For “BN” curve, BN is applied to all Conv layers; For “intermittent BN” curve, BN is applied
to layer 5k + 4, where k ∈ {0, 1, 2, 3, 4, 5}.
In a nutshell, careful designs are needed to balance the normalization effectiveness via less data
correlation and the execution efficiency via regular execution pattern. Only in this way, it is possible
to achieve practical acceleration with little accuracy loss, which is our major target in this work.
Based on the above analysis, we summarize the design considerations as follows.
•	Using few data for statistics’ estimation can effectively reduce the computational cost of
BN operations. Whereas, the effectiveness-efficiency trade-off should be well balanced.
•	Less data correlation is promising to reduce the estimation error and then guarantees the
normalization effectiveness.
•	More regular execution pattern is expected for efficient running on practical platforms.
4
Under review as a conference paper at ICLR 2019
3	Approaches
To reach the aforementioned goal, We propose two categories of approach in this section: sampling
or creating few uncorrelated data for statistics, estimation. Furthermore, multi-way strategies to
Figure 3: Illustration of approaches: (a) Naive Sampling; (b) Batch Sampling; (c) Feature Sampling;
(d) Virtual Dataset Normalization. ‘H' and 'W' are the height and width of FMs, respectively, ‘C' is
the number of FMs for current layer, and 'N' denotes the number of samples in current batch. The
orange and yellow rectangles in (a)-(c) represent the sampled data and those in (d) are the created
virtual sample(s). The yellow data are used to estimate the statistics for current FM.
3.1	Sampling Uncorrelated Data： Batch Sampling and Feature Sampling
Uncorrelated Sampling. Here “sampling” means to sample a small fraction of data from the acti-
vations at each layer for statistics' estimation. As discussed in the previous section, mining uncor-
related data within and between layers is critical to the success of sampling-based BN. However,
the correlation property of activations in deep networks is complex and may vary across network
structures and datasets. Instead, we apply a hypothesis-testing approach in this work.
We first make two empirical assumptions:
•	Hypothesis 1. Within each layer, data belonging to the different samples are more likely
to be uncorrelated than those within the same sample.
•	Hypothesis 2. Between layers, data belonging to different locations and different sam-
ples are less likely to be correlated. Here “location” means coordinate within FMs.
These two assumptions are based on the basic nature of real-world data and the networks, thus they
are likely to hold in most situations. They are further evaluated through experiments in Section 4.1.
Based on above hypotheses, we propose two uncorrelated-sampling strategies: Batch Sampling
(BS) and Feature Sampling (FS). The detailed Algorithms can be found in Alg. 1 and 2, respec-
tively, Appendix A.
•	BS (Fig. 3(b)) randomly selects few samples from each batch for statistics' estimation. To
reduce the inter-layer data correlation, it selects different samples across layers following
Hypothesis 2.
•	FS (Fig. 3(c)) randomly selects a small patch from each FM of all samples for statistics'
estimation. Since the sampled data come from all the samples thus it has lower correlation
within each layer following Hypothesis 1. Furthermore, it samples different patch locations
across layers to reduce the inter-layer data correlation following Hypothesis 2.
A Naive Sampling (NS) is additionally proposed as a comparison baseline, as shown in Fig. 3(a).
NS is similar to BS while the sampling index is fixed across layers, i.e. consistently samples first
few samples within each batch.
Regular and Static Sampling. in order to achieve practical acceleration on GpU, we expect more
regular sampling pattern and more static sampling index. Therefore, to balance the estimation ef-
fectiveness and execution efficiency, we carefully design the following sampling rules: (1) in BS,
the selected samples are continuous and the sample index for different channels are shared, while
they are independent across layers; (2) in FS, the patch shape is rectangular and the patch location
is shared by different channels and samples within each layer but variable as layer changes. Fur-
thermore, all the random indexes are updated only once for each epoch, which guarantees a static
computational graph during the entire epoch.
5
Under review as a conference paper at ICLR 2019
3.2	Creating Uncorrelated Data: Virtual Dataset Normalization
Instead of sampling uncorrelated data, another plausible solution is to directly create uncorrelated
data for statistics’ estimation. We propose Virtual Dataset Normalization (VDN)2 to implement it, as
illustrated in Fig. 3(d). VDN is realized with the following three steps: (1) calculating the statistics
of the whole training dataset offline; (2) generating s virtual samples3 4 at each iteration to concatenate
with the original real inputs as the final network inputs. (3) using data from only virtual samples at
each layer for statistics’ estimation. Due to the independent property of the synthesized data, they
are more uncorrelated than real samples thus VDN can produce much more accurate estimation.
The detailed implementation algorithm can be found in Alg. 3, Appendix A.
3.3	Single Use or Joint Use
The sampling approach and the creating approach can be used either in a single way or in a joint
way. Besides the single use, a joint use can be described as
E[x(k)] = βE[x(sk)] + (1 - β)E[x(vk)]	(4)
V ar[x(k)] = βV ar[x(sk)] + (1 - β)V ar[x(vk)] +β(1 - β)(E[x(sk)] - E[x(vk)])2
where xs denotes the sampled real data while xv represents the created virtual data. β is a controlling
variable, which indicates how large the sampled data occupy the whole data for statistics’ estimation
within each batch: (1) when β = 0 or 1, the statistics come from single approach (VDN or any
sampling strategy); (2) when β ∈ (0, 1), the final statistics are a joint value as shown in equation 4.
3.4	Comparison between Different Using Ways
A comparison between different using ways is presented in Table 1, where “d.s.” denotes “different
samples”; “d.l.” stands for “different locations”, and “g.i.” indicates “generated independent data”.
Compared to NS, BS reduces the inter-layer correlation via selecting different samples across layers;
FS reduces both the intra-layer and inter-layer correlation via using data from all samples within
each layer and selecting different locations across layers, respectively. Though VDN has a similar
inter-layer correlation with NS, it slims the intra-layer correlation with strong data independence. A
combination of BS/FS and VDN can inherit the strength of different approaches, thus achieve much
lower accuracy loss.
Table 1: Comparison between different using ways.
	NS	BS	FS	VDN	BS/FS + VDN
Inter-layer Correlation	X	d.s.	~dτ~	X	d.l./d.c.
Intra-layer Correlation	X	X	d.s.	g.i.	x/d.s. + g.i.
4	Experiments
Experimental Setup. All of our proposed approaches are validated		Table 2: Model configuration.			
	Model	Dataset	Network	Samples/GPU	GPU
on image classification task using	A	CIFAR-10	ReSNet-20	128	Titan Xp X1
CIFAR-10, CIFAR-100 and Ima-	B C	CIFAR-10/100 ImageNet	ResNet-56 ResNet-18	128 32	Titan Xp X1 Tesla V100 X1
geNet datasets from two perspec-	D	ImageNet	ResNet-18	128	Tesla V100 X2
tives: (1) effectiveness evaluation and (2) efficiency execution. To demon-	E F	ImageNet ImageNet	ResNet-50 DenseNet-121	64 64	Tesla V100 X4 Tesla V100 X3
					
strate the scalability and generality of our approaches on deep networks, we select ResNet-56
on CIFAR-10 & CIFAR-100 and select ResNet-18 and DenseNet-121 on ImageNet4. The model
2Virtual: synthesized samples rather than real ones are used for statistics’ estimation; Dataset: the virtual
samples are generated following the dataset’s distribution.
3Virtual samples: random tensors wherein each element simply follows N(μ,σ) where μ and σ are the
statistics of the whole training dataset calculated offline in step (1).
4On ImageNet, most experiments are conducted on ResNet-18 due to the faster training time, and only joint
approaches are demonstrated on on DenseNet-121. The accuracy and speedup results are on DenseNet-121 are
enough to evidence the effectiveness and efficiency of our approaches for deep models.
6
Under review as a conference paper at ICLR 2019
configuration can be found in Table 2. The means and variances for BN are locally calculated in
each GPU without inter-GPU synchronization as usual. We denote our approaches as the format of
“Approach-Sampled_size/Original_size-sampling_ratio(%)”. For instance, if We assume batch size
is 128, “BS-4/128-3.1%” denotes only 4 samples are sampled in BS and the sampling ratio equals
to 148 = 3.1%. Similarly, “FS-1/32-3.1%” implies a = 3.1% patch is sampled from each FM,
and “VDN-1/128-0.8%” indicates only one virtual sample is added. The traditional BN is denoted
as “BN-128/128-100.0%”. Other experimental configurations can be found in Appendix B.
4.1	Effectiveness Evaluation
Convergence Analysis. Fig. 4 shoWs the top-1 validation accuracy and confidential interval of
ResNet-56 on CIFAR-10 and CIFAR-100. On one side, all of our approaches can Well approxi-
mate the accuracy of normal BN When the sampling ratio is larger than 2%, Which evidence their
effectiveness. On the other side, all the proposed approaches perform better than the NS baseline.
In particular, FS performs best, Which is robust to the sampling ratio With negligible accuracy loss
(e.g. at sampling ratio=1.6%, the accuracy degradation is -0.087% on CIFAR-10 and +0.396% on
CIFAR-100).
0'号。u< UOQrap=ra>'do1-
1	2	4	8	16
Sampling Ratio(%)
2 0 8 6 4 2
7 7 6 6 6 6
a6a
eJ3u4 UOee>dol
1	2	4	8	16
Sampling Ratio(%)

Figure 4: Top-1 validation accuracy of ResNet-56 on CIFAR-10 (left) & CIFAR-100 (right).
VDN outperforms BS and NS with a large margin in extremely small sampling ratio (e.g. 0.8%),
whereas the increase of virtual batch size leads to little improvement on accuracy. BS is con-
stantly better than NS. Furthermore, an interesting observation is that the BN sampling could even
achieve better accuracy sometimes, such as NS-8/128(72.6±1.5%), BS-8/128(72.3±1.5%), and
FS-1/64(71.2±0.96%) against the baseline (70.8±1%) on CIFAR-100. Fig.5 further shows the
training curves of ResNet-56 on CIFAR-10 under different approaches. It reveals that FS and VDN
would not harm the convergence rate, while BS and NS begin to degrade the convergence when the
sampling ratio is smaller than 1.6% and 3.1%, respectively.
>UΞ3UU< uoπep=e> I4ol
50	100	0
Epochs
50	100	0
Epochs
50	100	0
Epochs
50	100
Epochs
Figure 5:	Training curves of ResNet56 on CIFAR-10.
Table 3 shoWs the top-1 validation error on ImageNet under different approaches. With the same
sampling ratio, all the proposed approaches significantly outperform NS, and FS surpasses VDN and
BS. Under the extreme sampling ratio of 0.78%, NS and BS don’t converge. Due to the limitation of
FM size, the smallest sampling ratio of FS is 1.6%, Which has only -0.5% accuracy loss. VDN can
still achieve relatively loW accuracy loss (1.4%) even if the sampling ratio decreases to 0.78%. This
implies that VDN is effective for normalization. Moreover, by combining FS-1/64 and VDN-2/128,
7
Under review as a conference paper at ICLR 2019
Table 3: Top-1 validation error on ImageNet.
Model	Approach	Sampling Ratio	Top-1 Error(%)	Accuracy Loss(%)
ResNet-18 (256, 128)	BN	128/128(100%)-	29.8	baseline
	NS	1/128(078%) 4/128(3.1%)	NA 35.2	NA -5.42
	BS	1/128(0.78%) 4/128(3.1%)	NA 31.7	NA -1.9
	VDN	1/128(0.78%) 2/128(1.6%)	31.2 30.8	-1.4 -1.0
	FS	-1/64(1.6%)- 1/32(3.1%)	30.3 30.4	-0.5 -0.6
	FS+VDN	-4/128(3.1%)-	30.0	-0.2
DenseNet-121 (192, 64)	BN	64/64(100%)	26.1	baseline
	NS	-3/64(4.7%)-	NA	NA
	BS+VDN	(1+2)/64(4.7%)	285	-2.4
	FS+VDN	(1+2)/64(4.7%)-	26.7	-0.6
we get the lowest accuracy loss (-0.2%). This further indicates that VDN can be combined with
other sampling strategies to achieve better results. Since training DenseNet-121 is time-consuming,
we just report the results with FS/BS-VDN joint use. Although DenseNet-121 is more challenging
than ResNet-18 due to the much deeper structure, the “FS-1/64 + VDN-2/64” can still achieve very
low accuracy loss (-0.6%). “BS-1/64 + VDN-2/64” has a little higher accuracy loss, whereas it
still achieves better result than NS. In fact, we observed gradient explosion if we just use VDN on
very deep network (i.e. DenseNet-121), which can be conquered through jointly applying VDN and
other proposed sampling approach (e.g. FS+VDN). Fig. 6 illustrates the training curves for better
visualization of the convergence. Except for the BS with extremely small sampling ratio (0.8%)
and NS, other approaches and configurations can achieve satisfactory convergence. Here, we further
evaluate the fully random sampling (FRS) strategy, which samples completely random points in
both the batch and FM dimensions. We can see that FRS is less stable compared with our proposed
approaches (except the NS baseline) and achieves much lower accuracy. One possible reason is that
under low sampling ratio, the sampled data may occasionally fall into the worse points, which lead
to inaccurate estimation of the statistics.
5 4 3
θs0
UOp=>dol
6
S
山
0	100	200	300	400	500	0	200	400	600
lterations(k)	Iterations (k)
Figure 6:	Training curves of ResNet18 (left) and DenseNet121 (right) on ImageNet.
θuus,le> jo山 UoqeEQS 山
IO1
IO0
BS-4∕128-3.1%
BS-4∕128-3.1%-centrold
FS-1∕32-3.1%
FS-l∕32-3.1%-centroid
VDN-1/12 8-0.8%
VDN-l∕128-0.8%-centroid
NS-4∕128-3.1%
NS-4/128-3.1%-centroid
Correlation Analysis. In this section, We bring
more empirical analysis to the data correlation that
affects the error of statistical estimation. Here we
denote the estimation errors at lth layer as Es)=
∣∣μSl) -μ(l)∣∣2 andEy) = ∣∣σSl) -σ(I)∣∣2, whereμSl)
& σ(l) are the estimated mean & variance from the
sampled data (including the created data in VDN)
while /1l & σ(l) are the ground truth from the
vanilla BN for the whole batch.
ιo-1	100
Estimation Error of Mean
Figure 7: Estimation error distribution.
The analysis is conducted on ResNet-56 over
CIFAR-10. The estimation errors of all layers are
recorded throughout the first training epoch. Fig. 7
and Fig. 8 present the distribution of estimation errors for all layers and the inter-layer correlation
between estimation errors, respectively. In Fig. 7, FS demonstrates the least estimation error within
each layer which implies its better convergence. The estimation error of VDN seems similar to BS
and NS here, but we should note that it uses much lower sampling ratio of 0.8% compared to others
8
Under review as a conference paper at ICLR 2019
NS-4∕128-3.1% mean
o	一
20
40
O 20	40
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
1.0
0.8
0.6
0.4
0.2
0.0
τ 1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
06 .
0.4 '
0.2
0∙0 z
-0.2
-0.4
FS-1∕32-3.1% var
20
40
产 1/32-3.1% mean
O
20
40
O
20
40
Figure 8: Inter-layer correlation between estimation errors. Both x &y axes denote layer index.
VDN-l∕128-0.8% mean
20
)40
VDN-l∕128-0.8% var
O	一
O
20
40
T1-0
F 0.8
0.6
0.4
0.2
0.0
-0.2
of 3.1%. From Fig. 8, it can be seen that BS presents obviously less inter-layer correlation than NS,
which is consistent with previous experimental results that BS can converge better than NS even
though they have similar estimation error as shown in Fig. 7. For FS and VDN, although it looks
like they present averagely higher correlations, there exist negative corrections which effectively im-
prove the model accuracy. Moreover, FS produces better accuracy than NS and BS since its selected
data come from all the samples with less correlation.
4.2 Efficiency Evaluation
After the normalization effectiveness evaluation, we will evaluate the execution efficiency which is
the primary motivation. Fig. 9 shows the BN speedup during training and overall training improve-
ment.
2.2
(a)
2.0------------
--baseline
1.8------------
1.6
1.4
■ FS-1∕32-3.1%
■ VDN-l∕128-0.8%
■ BS-4/12 8-3.1%
1.0
JUllI ■■
ABCDE
Models
Models
(c)
Influence of sampling regularity
	FS		
Regular Patch	√	x	√
Static- Index	x	√	√
Speedup (%)	+5.0	+3.1	+10.7
Figure 9: Efficiency evaluation: (a) BN speedup; (b) overall training speedup; (C) influence of
sampling regularity (ImageNet, ReSNet-18, FS-1/32-3.1%) on overall training speed.
In general, BS can gain higher acceleration ratio because
it doesn,t incur the fine-grained sampling within FMs like
in FS and it doesn,t require the additional calculation and
concatenation of the virtual samples like in VDN. As for
FS, it fails to achieve speedup on CIFAR-10 due to the
small image size that makes the reduction of operations
unable to cover the sampling overhead. The proposed ap-
proaches can obtain up to 2x BN acceleration and 21.8%
overall training acceleration.
Table 4 gives more results on ResNet-18 using single
approach and DenseNet-121 using joint approach. On
ResNet-18, We perform much faster training compared
Figure 10: Sampling ratio v.s. speedup.
with two recent methods for BN simplification (Wu et al., 2018; Banner et al., 2018). On ResNet-
18 We can achieve up to 16.5% overall training speedup under BS; on very deep networks with
more BN layers, such as DenseNet-121, the speedup is more significant that reaches 23.8% under
“BS+VDN” joint approach. “FS+VDN” is a little bit slower than “BS+VDN” since the latter one
has a more regular execution pattern as aforementioned. Nonetheless, on a very deep model, we still
recommend the “FS+VDN" version because it can preserve the accuracy better. The relationship be-
9
Under review as a conference paper at ICLR 2019
tween sampling ratio and overall training speedup is represented in Fig. 10 which illustrates that 1)
BS & FS can still achieve considerable speedup with a moderate sampling ratio; 2) BS can achieve
more significant acceleration than FS, for its more regular execution pattern. It’s worth noting that,
our training speedup is practically obtained on modern GPUs without the support of specialized li-
brary that makes it easy-to-use. Fig. 9 (c) reveals that the regular execution pattern can significantly
help us achieve practical speedup.
Table 4: Lateral comparison of overall training speed.
Approach	Model	Sampling Ratio	Iter. per Second	Speedup(%)
ResNet-18 (256, 128)	BN	=	128/128	5.21	baseline
	LIBN(Wuetal., 2018)	128/128	523	+0.38
	RBN (Banner et al., 2018)	128/128	530	+173
	FS	1/4(25%)	559	+73
		-1/8(12.5%)-	567	+88
		-1/16(6.25%)-	574	+τ02
		-1/32(3.1%)-	577	+10.7
	VDN	一	1/128(0.78%)-	593	+1378
	BS	-32/128(25%)-	579	+TΓΠ
		16/128(12.5%)	592	+136
		8/128(6.25%)-	6.04	+15.9
		-4/128(3.1%)-	6.07	+16.5 —
DenseNet-121 (192, 64)	BN	二	64/64	2.44	baseline
	BS+VDN	(1+2)/64(4.7%)	302	+2378
	FS+VDN	一	(1+2)/64(4.7%~	2.97	+21.7 —
5	Related Work
BN has been applied in most state-of-art DNN models (He et al., 2016; Szegedy et al., 2017) since
it was proposed. As aforementioned, BN standardizes the activation distribution to reduce the in-
ternal covariate shift. Models with BN have been demonstrated to converge faster and generalize
better (Ioffe & Szegedy, 2015; Morcos et al., 2018). Recently, a model called Decorrelated Batch
Normalization (DBN) was introduced which not only standardizes but also whitens the activations
with ZCA whitening (Huang et al., 2018). Although DBN further improves the normalization per-
formance, it introduces significant extra computational cost.
Simplifying BN has been proposed to reduce BN’s computational complexity. For example, L1BN
(Wu et al., 2018) and RBN (Banner et al., 2018) replace the original L2-norm variance with an
L1 -norm version and the range of activation values, respectively. From another perspective, Self-
normalization uses the customized activation function (SELU) to automatically shift activation’s
distribution (Klambauer et al., 2017). However, as mentioned in Introduction, all of these methods
fail to obtain a satisfactory balance between the effective normalization and computational cost,
especially on large-scale modern models and datasets. Our work attempts to address this issue.
6	Conclusion
Motivated by the importance but high cost of BN layer, we propose using few data to estimate the
mean and variance for training acceleration. The key challenge towards this goal is how to balance
the normalization effectiveness with much less data for statistics’ estimation and the execution effi-
ciency with irregular memory access. To this end, we propose two categories of approach: sampling
(BS/FS) or creating (VDN) few uncorrelated data, which can be used alone or jointly. Specifically,
BS randomly selects few samples from each batch, FS randomly selects a small patch from each
FM of all samples, and VDN generates few synthetic random samples. Then, multi-way strategies
including intra-layer regularity, inter-layer randomness, and static execution graph are designed to
reduce the data correlation and optimize the execution pattern in the meantime. Comprehensive
experiments evidence that the proposed approaches can achieve up to 21.7% overall training ac-
celeration with negligible accuracy loss. In addition, VDN can also be applied to the micro-BN
scenario with advanced performance. This paper preliminary proves the effectiveness and efficiency
of BN using few data for statistics’ estimation. We emphasize that the training speedup is practi-
cally achieved on modern GPUs, and we do not need any support of specialized libraries making
it easy-to-use. Developing specialized kernel optimization deserves further investigation for more
aggressive execution benefits.
10
Under review as a conference paper at ICLR 2019
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of
neural networks. arXiv preprint arXiv:1805.11046, 2018.
Johan Bjorck, Carla Gomes, and Bart Selman. Understanding batch normalization. arXiv preprint
arXiv:1806.02375, 2018.
Shuai Che, Jie Li, Jeremy W Sheaffer, Kevin Skadron, and John Lach. Accelerating compute-
intensive applications with gpus and fpgas. In Application Specific Processors, 2008. SASP 2008.
Symposium on, pp. 101-107. IEEE, 2008.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th international conference on
Machine learning, pp. 160-167. ACM, 2008.
Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in
Neural Information Processing Systems, pp. 666-674, 2011.
Sanjay Surendranath Girija. Tensorflow: Large-scale machine learning on heterogeneous distributed
systems. 2016.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, PP.
770-778, 2016.
Yiming Hu, Siyang Sun, Jianquan Li, Xingang Wang, and Qingyi Gu. A novel channel Pruning
method for deeP neural network comPression. arXiv preprint arXiv:1805.11394, 2018.
Gao Huang, Zhuang Liu, Van Der Maaten Laurens, and Kilian Q Weinberger. Densely connected
convolutional networks. PP. 2261-2269, 2016.
Lei Huang, Dawei Yang, Bo Lang, and Jia Deng. Decorrelated Batch Normalization. ArXiv e-prints,
APril 2018.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low Precision weights and activations. Journal
of Machine Learning Research, 18:187-1, 2017.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dePendence in batch-normalized
models. In Advances in Neural Information Processing Systems, PP. 1945-1953, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deeP network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
GUnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In Advances in Neural Information Processing Systems, PP. 971-980, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4013-4021, 2016.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Re-
ducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887,
2017.
11
Under review as a conference paper at ICLR 2019
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural
network compression. arXiv preprint arXiv:1707.06342, 2017.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. On the importance of
single directions for generalization. arXiv preprint arXiv:1803.06959, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch
normalization help optimization?(no, it is not about internal covariate shift). arXiv preprint
arXiv:1805.11604, 2018.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4,
inception-resnet and the impact of residual connections on learning. In AAAI, volume 4, pp.
12, 2017.
Guangrun Wang, Jiefeng Peng, Ping Luo, Xinjiang Wang, and Liang Lin. Batch kalman
normalization: Towards training deep neural networks with micro-batches. arXiv preprint
arXiv:1802.03133, 2018.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074-2082,
2016.
Shuang Wu, Guoqi Li, Lei Deng, Liu Liu, Dong Wu, Yuan Xie, and Luping Shi. L1-norm batch nor-
malization for efficient training of deep neural networks. IEEE Transactions on Neural Networks
and Learning Systems, 2018.
Yuxin Wu and Kaiming He. Group normalization. arXiv preprint arXiv:1803.08494, 2018.
Yang You, Zhao Zhang, C Hsieh, James Demmel, and Kurt Keutzer. Imagenet training in minutes.
CoRR, abs/1709.05011, 2017.
Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and
Amit Agrawal. Context encoding for semantic segmentation. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018a.
Tianyun Zhang, Kaiqi Zhang, Shaokai Ye, Jiayu Li, Jian Tang, Wujie Wen, Xue Lin, Makan Fardad,
and Yanzhi Wang. Adam-admm: A unified, systematic framework of structured weight pruning
for dnns. arXiv preprint arXiv:1807.11091, 2018b.
Maohua Zhu, Jason Clemons, Jeff Pool, Minsoo Rhu, Stephen W Keckler, and Yuan Xie. Struc-
turally sparsified backward propagation for faster long short-term memory training. arXiv preprint
arXiv:1806.00512, 2018.
12
Under review as a conference paper at ICLR 2019
Appendix A	Implementation Algorithms
Notations. We use the Conv layer for illustration, which occupies the major part of most modern
networks (He et al., 2016; Huang et al., 2016). The batched features can be viewed as a 4D tensor.
We use “E0,1,2” and “V ar0,1,2” to represent the operations that calculate the means and variances,
respectively, where “0, 1, 2” denotes the dimensions for reduction.
Algorithm 1: NS/BS Algorithm
Data: input batch at layer l: Bl ∈ RN×Hl×Wl ×Cl, l ∈ all layers; sampling size: n ∈ (0, N]
Result: estimation of E[Bl] & V ar[Bl], l ∈ all layers
begin
for ep ∈ all epochs do
for l ∈ all layers do
L if BS: beginl = randint(0, N — n§); else NS: beginl = 0
for it ∈ all iterations do
for l ∈ all layers do
Bs = Bl [beginl : beginl + n — 1,:,:,:]
L E[Bl] = E0,1,2[Bs], Var[Bl] = Var0,1,2[Bs]
Algorithm 2: FS Algorithm
Data: input batch at layer l: Bl ∈ RN×Hl×Wl×Cl, l ∈ all layers; sampling size: h(sl) ∈ (0, Hl] &
ws(l) ∈ (0,Wl]
Result: estimation of E[Bl] & V ar[Bl], l ∈ all layers
begin
for ep ∈ all epochs do
for l ∈ all layers do
L begin? = randint(0, Hl — h(sl)), begin(wl) = randint(0, Wl — ws(l))
for it ∈ all iterations do
for l ∈ all layers do
Bs = B[:, begin? : begin? + hsl) — 1, beginW : beginW + Ws) — 1,:]
_ E[Bl] = E0,1,2[Bs], Var[Bl] = Var0,1,2[Bs]
Algorithm 3: VDN Algorithm
Data: Dataset: D ∈ RND ×H0×w0×C0; input batch: Bl ∈ RN×Hl×wι×Cl; number of virtual
samples: nv
Result: estimation of E[Bl] & V ar[Bl] at layer l, l ∈ all layers
begin
Calculate μ = E0,1,2[D] & σ2 = Var0,1,2[D] offline
for ep ∈ all epochs do
for it ∈ all iterations do
Create virtual samples V ∈ Rnv ×H0 ×W0 × c0, V 〜N(μ,σ)
for l ∈ all layers do
if l = 0: Bl = [V, Bl], then feed Bl into the network
Bs=Bl[0:nv—1,:,:,:]
L E[Bl] = E0,1,2[Bs], Var[Bl] = Var0,1,2[Bs]
Appendix B	Experimental Configuration
All the experiments on CIFAR-10 & CIFAR-100 are conducted on a single Nvidia Titan Xp GPU.
We use a weight decay of 0.0002 for all weight layers and all models are trained by 130 epochs. The
13
Under review as a conference paper at ICLR 2019
initial learning rate is set to 0.1 and it is decreased by 10x at 50, 80, 110 epoch. During training, we
adopt the “random flip left & right” and all the input images are randomly cropped to 32 × 32. Each
model is trained from scratch for 5 times in order to reduce random variation.
For ImageNet, We use 2 Nvidia Tesla V100 GPUs on DGX station for ResNet-18 and 3 for
DenseNet-121. We use a weight decay of 0.0001 for all weight layers and all models are trained by
100 epochs. The initial learning rate is set to 0.1/256 × “Gradient batch size” and we decrease the
learning rate by 10x at 30, 60, 80, 90 epoch. During training, all the input images are augmented by
random flipping and cropped to 224 × 224. We evaluate the top-1 validation error on the validation
set using the centered crop of each image. To reduce random variation, we use the average of last
5 epochs to represent its final error rate. Besides, Winograd (Lavin & Gray, 2016) is applied in all
models to speedup training.
Appendix C	Analysis of Speedup
Compute Cycle and Memory Access. Our proposed approaches can effectively speedup forward
pass. Under the condition that we use s m data to estimation the statistics for each feature, the
total accumulation operations are significantly reduced from m - 1 to s - 1. If using the adder
tree optimization illustrated in Section 2.1, the tree’s depth can be reduced from log(m) to log(s).
Thus, the theoretical compute speedup for the forward pass can reach logs (m) times. For instance,
if the FM size is 56 × 56 with batch size of 128 (m = 56 × 56 × 128), under sampling ratio of
1/32, the compute speedup will be 36.7%. The total memory access is reduced by m/s times. For
example, when the sampling ratio is 1/32, only 3.1% data need to be visited. This also contributes a
considerable part in the overall speedup.
Speedup in the Backward Pass. The BN operations in the forward pass have been shown in
equation (1)-(3). Based on the derivative chain rule, we can get the corresponding operations in the
backward pass as follows
士=工∙ γ(k) d	=(X 与 L -1	)
∂χ(k)	⅜i㈤ 7	,	dE [x(k)] j=1 ∂bjk)	PVar[x(k)]	+ e
∂V⅛)j= X ” ∙(Xj- E*]) TV"2 + i
∂l
∂x(k
∂l
∂γ(k)
∂l
∂x(k
，Var[x(k)] + E
X JL ʌ(k) JL
i=1 讲∙ i , N
∂l 1 ∂l
∂ ∂E[x(k)] m + ∂Var[x(k)]
m
=X JL
乙 ∂2 Ak)
i=1 ∂yi
2(x(k) - E[x(k)])
m
(5)
1
Take the BN sampling as an example, the calculation of —¾) can be modified as
∂xi
+ 8石队)]* S + ∂Va‰)]
, otherwise
2(Xik)-SEi)]), if Xi ∈ S
(6)
1
while the others remain the same format. Here S is the location set for sampled neurons. For neurons
outside S, they didn’t participate in the estimation of the mean and variance in the forward pass, so
dE[χkJ and dVar[χ()] equal to zeros. We should note that although We sampled only S data in the
∂x(i )	∂x(i )
forward pass, the reduction operations in the backward pass still have m length since the neurons
outside S also participate in the activation normalization using the estimated statistics from the
sampled data. Therefore, there is no theoretical speedup for the backward pass. In our experiments,
to assemble a dense and efficient addition operation with -^dk ∙ √======= for all neurons, the
mentioned zeros outside S are pre-padded. However, it has great potential for memory space and
access reduction since the dE[χkJ and dVar[x(()] outside S are zeros, which can be leveraged by
∂x(i )	∂x(i )
specialized devices.
14
Under review as a conference paper at ICLR 2019
Reason for only Speeding up Forward Pass. As shown in equation (1) & (5), although the forward
pass only takes 1 reduction operations in BN, it requires nearly 2 time. The underlying reason is
that the reduction operations in the backward pass can be parallelized while the second reduction
operation in the forward pass used to calculate variance cannot operate until the mean is provided.
Regular Execution Pattern. To approach the theoretical speedup, we suggest using more regular
sampling pattern such as the continuous samples in BS and intra-FM rectangular shape and inter-FM
shared location in FS. A regular sampling pattern can improve the cache usage by avoiding random
access. Moreover, the operation in the backward pass corresponding to the sampling operation in
the forward pass is “padding” shown in Fig. 1(d). Under a regular sampling pattern, the padding is
block-wise that becomes much easier. On the other side, intuitively, a static computational graph can
be calculated faster and easier deployed on various platforms for (1) the static graph’s pipeline can
be optimized once for all before execution, (2) popular deep learning frameworks like TensorFlow
(Girija, 2016) are developed as a static computational graph. For these reasons, we expect the
random sampling is achieved through a static computational graph, which is obtained by updating
the sampling indexes only per training epoch.
Appendix D Influence of Decay Rate for Moving Average
During each validation step after certain training iterations, E[x(k)] & V ar[x(k)] are replaced with
the recorded moving average, which is governed by
X	[x(k)]	=	X[x(k)]it, it = 1
ma 沆 α αX [x(k)]it + (I- α)Xma[x(k)]it-1, it ∈ [2, B ] ，
(7)
where X stands for “E” or“V ar”, Xma[x(k)] denotes the moving average, it is the iteration number,
and α is the decay rate. Based on equation (7), wherein the variance of Xma [x(k)]it equals to
α2Var[X[x(k)]it] + (1 - α)2V ar[Xma [x(k)]it-1], ifwe assume that (1) each estimated value shares
the same variance V ar[X[x(k)]it], (2) they are independent with each other, and (3) the iteration
number goes sufficiently large, we will get
Var[Xma[x(k)]it] ≈ T^Var[X[x(Mit].
2 — α
(8)
0.916
0.914
0.912
0.910
0.908
18
9
AUSnbXZ uop -->dol
					
					
					
					
LΞ	3				
				ɪj	
Figure 11: Influence of decay rate
for moving average.
The above equation reveals that an appropriately smaller ɑ
might scale down the estimation error, thus produces better
validation accuracy. To verify this prediction, the experiments
are conducted on ReSNet-56 over CIFAR-10 and using BS-
1/128(0.78%) sampling. As shown in Fig. 11, it,s obvious that
there exists a best decay rate setting (here is 0.7) whereas the
popular decay rate is 0.9. The performance also decays when
decay rate is smaller than 0.7, which is because a too small ɑ
may lose the capability to record the moving estimation, thus
degrade the validation accuracy. This is interesting because
the decay rate is usually ignored by researchers, but the de-
fault value is probably not the best setting in our context.
Appendix E	Reduction Operations in BN
To further demonstrate that reduction operations are the major bottleneck
of BN, we build a pure BN network for profiling as shown in Fig. 12. The
network’s input is a variable with shape of [128, 112, 112, 64] which is ini-
tialized following a standard normal distribution. The network is trained
for 100 iterations and the training time is recorded. We overwrite the orig-
inal BN’s codes and remove the reduction operations in both forward pass
and backward pass for contrast test. We use three different GPUs: K80,
Titan Xp, and Tesla V100 to improve the reliability. The results are shown
in Table 5. We can see that on all the three GPUs, reduction operations
take up to >60% of the entire operation time in BN. As a result, it’s solid
to argue that the reduction operations are the bottleneck of BN.
Figure 12: BN net-
work for profiling.
15
Under review as a conference paper at ICLR 2019
Table 5: The execution time in different ablation study of reduction operations in BN.
GPU	BN	w/o Reduction	w/oBN	Reduction Percentage
K80	107.32s	49.49s =	9.96s	59.4%
Titan XP	64.27s	29.04s	9.80s	647%
Tesla V100	21.82s	13.14s —	7.86s	62.2%
Appendix F	Micro-BN Extension
Micro-BN aims to alleviate the diminishing of BN’s effectiveness when the amount of data in each
GPU node is too small to provide a reliable estimation of activation statistics. Previous work can
be classified to two categories: (1) Sync-BN (Zhang et al., 2018a) and (2) Local-BN (Ba et al.,
2016; Wu & He, 2018; Ioffe, 2017; Wang et al., 2018). The former addresses this problem by
synchronizing the estimations from different GPUs at each layer, which induces significant inter-
GPU data dependency and slows down training process. The latter solves this problem by either
avoiding the use of batch dimension in the batched activation tensor for statistics or using additional
information beyond current layer to calibrate the statistics.
In terms of BN’s efficiency preservation, we are facing a similar problem with micro-BN, thus our
framework can be further extended to the micro-BN scenario. In Sync-BN: (1) With FS, each GPU
node executes the same patch sampling as normal FS; (2) With BS, we can randomly select the
statistics from a fraction of GPUs rather than all nodes; (3) With VDN, the virtual samples can be
fed into a single or few GPUs. The first one can just simplify the computational cost within each
GPU, while the last two further optimize the inter-GPU data dependency. In Local-BN, since the
available data for each GPU is already tiny, the BN sampling strategy will be invalid. Fortunately,
the VDN can still be effective by feeding virtue samples into each node.
Experiments. The normalization in Sync-BN is based on the statis-
tics from multiple nodes through synchronization, which is equiva-
lent to that in Fig. 6 with large batch size for each node. Therefore, to
avoid repetition, here we just show the results on Local-BN with VDN
optimization. We let the overall batch size of 256 breaks down to 64
workers (each one has only 4 samples for local normalization). We
use “(gradient batch size, statistics batch size)” of (256, 4) to denote
the configuration (Wang et al., 2018). A baseline of (256, 32) with BN
and one previous work Group Normalization (GN) (Wu & He, 2018)
are used for comparison. As shown in Fig. 13, although the reduction
of batch size will degrade the model accuracy, our VDN can achieve
slightly better result (top-1 validation error rate: 30.88%) than GN
(top-1 validation error rate: 30.96%), an advanced technique for this
scenario with tiny batch size. This promises the correct training of
very large model so that each single GPU node can only accommo-
date several samples.
O
6 5 4 3
JOXI 山 uop=e>dol
200	400
Iterations (k)
Figure 13: Training curves
in micro-BN scenario. (Im-
ageNet, ResNet 18)
16
Under review as a conference paper at ICLR 2019
Appendix G Organization of the whole paper
Figure 14: Illustration of the paper,s organization. The Green and Purple markers (round circle with
a star in the center) represent whether the effectiveness is preserved by reducing inter-layer or intra-
layer correlation (Green: inter-layer; Purple: intra-layer). Moreover, the consideration of regular &
static execution pattern is applied to all approaches.
17