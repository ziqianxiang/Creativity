Under review as a conference paper at ICLR 2019
Cross-Entropy Loss Leads To Poor Margins
Anonymous authors
Paper under double-blind review
Ab stract
Neural networks could misclassify inputs that are slightly different from their
training data, which indicates a small margin between their decision boundaries
and the training dataset. In this work, we study the binary classification of lin-
early separable datasets and show that linear classifiers could also have decision
boundaries that lie close to their training dataset if cross-entropy loss is used for
training. In particular, we show that if the features of the training dataset lie in a
low-dimensional affine subspace and the cross-entropy loss is minimized by using
a gradient method, the margin between the training points and the decision bound-
ary could be much smaller than the optimal value. This result is contrary to the
conclusions of recent related works such as (Soudry et al., 2018), and we identify
the reason for this contradiction. In order to improve the margin, we introduce
differential training, which is a training paradigm that uses a loss function defined
on pairs of points from each class. We show that the decision boundary of a linear
classifier trained with differential training indeed achieves the maximum margin.
The results reveal the use of cross-entropy loss as one of the hidden culprits of ad-
versarial examples and introduces a new direction to make neural networks robust
against them.
1 Introduction
Training neural networks is challenging and involves making several design choices. Among these
are the architecture of the network, the training loss function, the optimization algorithm used for
training, and their hyperparameters, such as the learning rate and the batch size. Most of these
design choices influence the solution obtained by the training procedure and have been studied in
detail (Kingma & Ba, 2014; Hardt et al., 2015; He et al., 2016; Wilson et al., 2017; Nar & Sastry,
2018; Smith et al., 2018). Nevertheless, one choice has been mostly taken for granted when the
network is trained for a classification task: the training loss function.
Cross-entropy loss function is almost the sole choice for classification tasks in practice. Its preva-
lent use is backed theoretically by its association with the minimization of the Kullback-Leibler
divergence between the empirical distribution of a dataset and the confidence of the classifier for
that dataset. Given the particular success of neural networks for classification tasks (Krizhevsky
et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016), there seems to be little motivation to
search for alternatives for this loss function, and most of the software developed for neural networks
incorporates an efficient implementation for it, thereby facilitating its use.
Recently there has been a line of work analyzing the dynamics of training a linear classifier with
the cross-entropy loss function (Soudry et al., 2018; Nacson et al., 2018a;b; Ji & Telgarsky, 2018).
They specified the decision boundary that the gradient descent algorithm yields on linearly separable
datasets and claimed that this solution achieves the maximum margin.1 However, these claims were
observed not to hold in the simple experiments we ran. For example, Figure 1 displays a case where
the cross-entropy minimization for a linear classifier leads to a decision boundary which attains an
extremely poor margin and is nearly orthogonal to the solution given by the hard-margin support
vector machine (SVM).
We set out to understand this discrepancy between the claims of the previous works and our obser-
vations on the simple experiments. We can summarize our contributions as follows.
1The term “maximum margin” is used for `2 norm throughout the paper.
1
Under review as a conference paper at ICLR 2019
80-
70 -
>60-
50-
40 -
Cross-entropy min.
decision boundary
(poor margin)
10	20	30	40
SVM decision boundary
(largest margin)
x
Figure 1: Orange and blue points represent the data from two different classes in R2 . Cross-entropy
minimization for a linear classifier on the given training points leads to the decision boundary shown
with the solid line, which attains a very poor margin and is almost orthogonal to the solution given
by the SVM.
1.	We analyze the minimization of the cross-entropy loss for a linear classifier by using only
two training points, i.e., only one point from each of the two classes, and we show that the
dynamics of the gradient descent algorithm could yield a poor decision boundary, which
could be almost orthogonal to the boundary with the maximum margin.
2.	We identify the source of discrepancy between our observations and the claims of the recent
works as the misleading abbreviation of notation in the previous works. We clarify why the
solution obtained with cross-entropy minimization is different from the SVM solution.
3.	We show that for linearly separable datasets, if the features of the training points lie in an
affine subspace, and if the cross-entropy loss is minimized by a gradient method with no
regularization to train a linear classifier, the margin between the decision boundary of the
classifier and the training points could be much smaller than the optimal value. We verify
that when a neural network is trained with the cross-entropy loss to classify two classes
from the CIFAR-10 dataset, the output of the penultimate layer of the network indeed
produces points that lie on an affine subspace.
4.	We show that if there is no explicit and effective regularization, the weights of the last layer
of a neural network could grow to infinity during training with a gradient method. Even
though this has been observed in recent works as well, we are the first to point out that this
divergence drives the confidence of the neural network to 100% at almost every point in
the input space if the network is trained for long. In other words, the confidence depends
heavily on the training duration, and its exact value might be of little significance as long
as it is above 50%.
5.	We introduce differential training, which is a training paradigm that uses a loss function
defined on pairs of points from each class - instead of only one point from any class.
We show that the decision boundary of a linear classifier trained with differential training
indeed produces the SVM solution with the maximum hard margin.
2 Classification of Two Points Manifests Poor Margin
We start with a simple binary classification problem. Given two points x ∈ Rd and -y ∈ Rd from
two different classes, we can find a linear classifier by minimizing the cross-entropy loss function
w∈mn∈R {- log (e-w>χ1-b+ι)- log
ew>y-b
ew>y-b + 1
2
Under review as a conference paper at ICLR 2019
or equivalently, by solving
min [log(e-w^ + 1) + log(e-w>y + 1)] ,	(1)
W∈Rd+1 I	J
where X = [χ> 1]>, -y = [-y> 1]> and W = [w> b]>. Unless the two points X and -y are equal,
the function (1) does not attain its minimum at a finite value of W. Consequently, if the gradient
descent algorithm is used to minimize (1), the iterate at time k, W[k], diverges as k increases. The
following theorem characterizes the growth rate of W[k] and its direction in the limit by using a
continuous-time approximation to the gradient descent algorithm.
Theorem 1. Given two points X ∈ Rd and —y ∈ Rd, let X and —y denote [x> 1]> and [-y> 1],
respectively. Without loss of generality, assume kXk ≤ kyk. If the two points are in different classes
and we minimize the cross-entropy loss
min log(1 + e-w>x) + log(1 + e-w>y)
W∈Rd+1
by using the continuous-time approximation to the gradient descent algorithm
dW _ δ δe-w>x	〜δe-w>y
^dt = X1 + e-w>x + y 1 + e-w>y
with the initialization W(0) = 0 and the learning rate δ, then
lim
t→∞
W(t)	σ σy-σx2 X+ σx-σx2 y
w( ) = W σχσy-σ2y	σχσy-σ2y y
log⑴	1 ɪ X
σx
if σxy < σx,
if σxy ≥ σx,
(2)
where σχ = ∣∣X∣∣2,σχy = X>y and σy = ∣∣yk2.
Note that first d coordinates of (2) represent the normal vector of the decision boundary obtained by
minimizing the cross-entropy loss (1). This vector is different from X + y, which is the direction
of the maximum-margin solution given by the SVM. In fact, the direction in (2) could be almost
orthogonal to the SVM solution in certain cases, which implies that the margin between the points
and the decision boundary could be much smaller than the optimal value. Corollary 1 describes a
subset of these cases.
Corollary 1. Given two points X and -y in Rd, let ψ denote the angle between the solution given
by (2) and the solution given by the SVM, i.e., (X + y). If X>y = 1, then
cos2 ψ ≤
4
where σχ = ∣∣x∣2+1 andσy = ∣∣y∣2+1. Consequently, as ∣∣x∣∣/Ilyk approaches 0 while maintaining
the condition X>y = 1, the angle ψ converges to π∕2.
Remark 1. Corollary 1 shows that if X and -y have disparate norms, the minimization of the cross-
entropy loss with gradient descent algorithm could lead to a direction which is almost orthogonal to
the maximum-margin solution. It may seem like this problem could be avoided with preprocessing
the data so as to normalize the data points. However, this approach will not be effective for neural
networks: if we consider an L-layer neural network, W>φL-1(X), and regard the first L - 1 layers,
Φl-i(∙), as a feature mapping, preprocessing a dataset {Xi}i∈ι will not produce a normalized set of
features {φL-1(Xi)}i∈I. Note that we could not normalize {φL-1(Xi)}i∈I directly either, since the
mapping Φl-ι(∙) evolves during training.
max
Remark 2. Theorem 1 shows that the norm of W keeps growing unboundedly as the training con-
tinues. The same behavior will be observed for larger datasets in the next sections as well. Since the
“confidence” of the classifier for its prediction at a point X is given by
1	e-w> x-b
e-w>x-b + 1 , e-w>x-b + 1
this unbounded growth of ∣W∣ drives the confidence of the classifier to 100% at every point in the
input space, except at the points on the decision boundary, if the algorithm is run for long. Given
the lack of effective regularization for neural networks, a similar unbounded growth is expected to
be observed in neural network training as well, which is mentioned in (Bartlett et al., 2017). As a
result, the confidence of a neural network might be highly correlated with the training duration, and
whether a neural network gives 99% or 51% confidence fora prediction might be of little importance
as long as itis above 50%. In other words, regarding this confidence value as a measure of similarity
between an input and the training dataset from the most-likely class should be reconsidered.
3
Under review as a conference paper at ICLR 2019
3 Data with Low-Dimensional Features Attain Poor Margin
In this section, we examine the binary classification of a linearly separable dataset by minimizing
the cross-entropy loss function. Recently, this problem has also been studied in (Soudry et al., 2018;
Nacson et al., 2018b;a; Ji & Telgarsky, 2018). We restate an edited version of the main theorem of
(Soudry et al., 2018), followed by the reason of the edition.
Theorem 2 (Adapted from Theorem 3 of (Soudry et al., 2018)). Given two sets of points {xi}i∈I
and {-yj}j∈j that are linearly separable in Rd, let Xi and -yj denote [x> 1]> and [—y> 1]>,
respectively, for all i ∈ I, j ∈ J. Then the iterate of the gradient descent algorithm, W(t), on the
cross-entropy loss function
min X log(1 + e-w>遏)+ X log(1 + e-w>yj)	(3)
W∈Rd+ι j∈ι *λ	J	乙"j∈J	、	J
with a sufficiently small step size will converge in direction:
W(t) _ w
t→→∞ PW =丽,
where w is the solution to
w = argmin ∣∣uk2 s.t.	hu,Xi)	≥ 1,	hu,yji	≥ 1	∀i	∈ I,∀j	∈ J.	(4)
u∈Rd+1
The solution (4) given in Theorem 2 was referred in (Soudry et al., 2018), and consequently in
the other works, as the maximum-margin solution. However, due to the misleading absence of the
bias term in the notation, this is incorrect. Given the linearly separable sets of points {xi}i∈I and
{-yj}j∈J, the maximum-margin solution given by the SVM solves
minimize ∣w∣22
w,b
subject to hw, xii + b ≥ 1	∀i ∈ I,	(P1)
hw, -yj i + b ≤ -1 ∀j ∈ J.
On the other hand, the solution given by Theorem 2 corresponds to
minimize ∣w∣22 + b2
w,b
subject to hw, Xii + b =(W, Xii ≥ 1	∀i ∈ I,	(P2)
hw, -y i +b = hw, -yji ≤ -1 ∀j ∈ J,
where We define W = [w> b]>, Xi = [x> 1]> and yj = [y> - 1]> for all i ∈ I,j ∈ J. Even
though the sets of constraints for both problems are identical, their objective functions are different,
and consequently, the solutions are different. As a result, the decision boundary obtained by cross-
entropy minimization does not necessarily attain the maximum hard margin. In fact, as the following
theorem shows, its margin could be arbitrarily worse than the maximum margin.
Theorem 3.	Assume that the points {Xi}i∈I and {-yj}j ∈J are linearly separable and lie in an
affine subspace; that is, there exist a set of orthonormal vectors {rk}k∈K and a set of scalars
{∆k }k∈K such that
hrk,Xii = hrk,-yji =∆k ∀i ∈I,∀j ∈ J,∀k ∈ K.
Let (W, ∙i + B = 0 denote the decision boundary obtained by minimizing the cross-entropy loss, i.e.
the pair (W, B) solves
min ∣w∣2 + b2 s.t. (w, Xii + b ≥ 1, (w, -yji + b ≤ -1 ∀i ∈ I, ∀j ∈ J.
w,b
Then the minimization of the cross-entropy loss (3) yields a margin smaller than or equal to
1
√⅜ + B2 Pk∈κ ∆
where γ denotes the optimal hard margin given by the SVM solution.
4
Under review as a conference paper at ICLR 2019
Remark 3. Theorem 3 shows that if the training points lie in an affine subspace, the margin obtained
by the cross-entropy minimization will be smaller than the optimal margin value. As the dimension
of this affine subspace decreases, the cardinality of the set K increases and the term Pk∈K ∆2k could
become much larger than 1 /γ2. Therefore, as the dimension of the subspace containing the training
points gets smaller compared to the dimension of the input space, cross-entropy minimization with
a gradient method becomes more likely to yield a poor margin. Note that this argument also holds
for classifiers of the form w>φ(x) with the fixed feature mapping φ(∙).
The next theorem relaxes the condition of Theorem 3 and allows the training points to be near an
affine subspace instead of being exactly on it. Note that the ability to compare the margin obtained
by cross-entropy minimization with the optimal value is lost. Nevertheless, it highlights the fact that
same set of points could be assigned a different margin by cross-entropy minimization if all of them
are shifted away from the origin by the same amount in the same direction.
Theorem 4.	Assume that the points {xi}i∈I and {-yj}j∈J in Rd are linearly separable and there
exist a set of orthonormal vectors {rk}k∈K and a set of scalars {∆k}k∈K such that
hrk , xi i ≥ ∆k ,	hrk ,	-yj i	≤	∆k	∀i	∈ I,	∀j	∈ J,	∀k	∈ K.
Let hw, •)+ B = 0 denote the decision boundary obtained by minimizing the cross-entropy loss, i.e.
the pair (w, B) solves
min	kwk2 + b2 s.t.	hw, xii + b ≥	1,	hw,	-yji	+ b ≤	-1	∀i ∈ I,	∀j	∈ J.
w,b
Then the minimization of the cross-entropy loss (3) yields a margin smaller than or equal to
1
q 2 Pk∈κ ∆
Remark 4. Both Theorem 3 and Theorem 4 consider linearly separable datasets. If the dataset is not
linearly separable, (Ji & Telgarsky, 2018) predicts that the normal vector of the decision boundary,
w, will have two components, one of which converges to a finite vector and the other diverges.
The diverging component still has the potential to drive the decision boundary to a direction with a
poor margin. In fact, the margin is expected to be small especially if the points intruding into the
opposite class lie in the same subspace as the optimal normal vector for the decision boundary. In
this work, we focus on the case of separable datasets as this case provides critical insight into the
issues of state-of-the-art neural networks, given they can easily attain zero training error even on
randomly generated datasets, which indicates the linear separability of the features obtained at their
penultimate layers (Zhang et al., 2017).
4	Differential Training Improves Margin
In previous sections, we saw that the cross-entropy minimization could lead to poor margins, and the
main reason for this was the appearance of the bias term in the objective function of (P2). In order
to remove the effect of the bias term, consider the SVM problem (P1) and note that this problem
could be equivalently written as
minimize kwk22
w	(P3)
subject to hw, xi + yj i ≥ 2 ∀i ∈ I, ∀j ∈ J
ifwe only care about the weight parameter w. This gives the hint that ifwe use the set of differences
{xi + yj : i ∈ I, j ∈ J} instead of the individual sets {xi}i∈I and {-yj}j∈J, the bias term could
be excluded from the problem. This was also noted in (Keerthi et al., 2000; Ishibashi et al., 2008)
previously. Indeed, this approach allows obtaining the SVM solution with a loss function similar to
the cross-entropy loss, as the following theorem shows.
Theorem 5.	Given two sets of points {xi}i∈I and {-yj}j ∈J that are linearly separable in Rd, if
min X X log(1 + e-w>(xi+yj))	(5)
w∈Rd	i∈I	j∈J
5
Under review as a conference paper at ICLR 2019
by using the gradient descent algorithm with a sufficiently small learning rate, the direction of w
converges to the direction of maximum-margin solution, i.e.
lim㈡
t→∞ kw⑴k
W SVM
kwSVM k
(6)
where wSVM is the solution of (P3).
Proof. Apply Theorem 2 by replacing the sets {xi}i∈I and {-yj}j∈J with {xi + yj }i∈I,j∈J and
the empty set, respectively. Then the minimization of the loss function (5) with the gradient descent
algorithm leads to
「 Ww
li^∩ -----------
t-∞ kwk	kwk
where w satisfies
w = arg min	∣∣wk2 such that(w,	Xi	+ yj	≥ 1	∀i	∈ I,	∀j	∈ J.
w
Since WSVM is the solution of (P3), We obtain w = 2 wsvm, and the claim of the theorem holds. ■
Remark 5. Theorem 5 is stated for the gradient descent algorithm, but the identical statement could
be made for the stochastic gradient method as well by invoking the main theorem of (Nacson et al.,
2018).
Minimization of the cost function (5) yields the weight parameter W of the decision boundary. The
bias parameter, b, could be chosen by plotting the histogram of the inner products {(W, Xi)}i∈∕ and
{(w, 一yj}j∈j and fixing a value for b such that
(w, Xii + b ≥ 0 ∀i ∈ I,	(7a)
hw, -yj + b ≤ 0 ∀j ∈ J.	(7b)
The largest hard margin is achieved by
11
b = --min(w, Xii - KmaXhw, — j	(8)
2 i∈I	2 j∈J
However, by choosing a larger or smaller value for b, it is possible to make a tradeoff between the
Type-I and Type-II errors.
The cost function (5) includes a loss defined on every pair of data points from the two classes. This
cost function can be considered as the cross-entropy loss on a new dataset which contains |I| × |J|
points. There are two aspects of this fact:
1.	When standard loss functions are used for classification tasks, we need to oversample or
undersample either of the classes if the training dataset contains different number of points
from different classes. This problem does not arise when we use the cost function (5).
2.	Number of pairs in the new dataset, |I | × |J|, will usually be much larger than the original
dataset, which contains |I | + |J | points. Therefore, the minimization of (5) might appear
more expensive than the minimization of the standard cross-entropy loss computationally.
However, if the points in different classes are well separated and the stochastic gradient
method is used to minimize (5), the algorithm achieves zero training error after using only
a few pairs, which is formalized in Theorem 6. Further computation is needed only to im-
prove the margin of the classifier. In addition, in our experiments to train a neural network
to classify two classes from the CIFAR-10 dataset, only a few percent of |I| × |J| points
were observed to be sufficient to reach a high accuracy on the training dataset.
Theorem 6. Given two sets of points {Xi}i∈I and {-yj}j ∈J that are linearly separable in Rd,
assume the cost function (5) is minimized with the stochastic gradient method. Define
Rx = maX{∣Xi - Xi0 ∣ : i, i0 ∈ I},	Ry = maX{∣yj - yj0 ∣ : j, j0 ∈ J}
and let γ denote the hard margin that would be obtained with the SVM:
2γ = maXu∈Rd mini三ι,j三j (xi + y,u∕∣∣u∣∣).
If 2γ ≥ 5max(Rχ, Ry), then the stochastic gradient algorithm produces a weight parameter W,
only in one iteration which satisfies the inequalities (7a)-(7b) along with the bias, b, given by (8).
6
Under review as a conference paper at ICLR 2019
5	Numerical Experiments
In this section, we present numerical experiments supporting our claims.
Differential training. In Figure 2, we show the decision boundaries of two linear classifiers, where
one of them is trained by minimizing the cross-entropy loss, and the other through differential train-
ing. Unlike the example shown in Figure 1, here the data do not exactly lie in an affine subspace.
In particular, one of the classes is composed of 10 samples from a normal distribution with mean
(2, 12) and variance 25, and the other class is composed of 10 samples from a normal distribution
with mean (40, 50) and variance 25. As can be seen from the figure, the cross-entropy minimization
yields a margin that is smaller than differential training, even though when the training dataset is not
low-dimensional, which is predicted by Theorem 4.
Figure 2: Classification boundaries obtained using differential training and cross-entropy minimiza-
tion. The margin recovered by cross-entropy minimization is worse than differential training even
when the training dataset is not low-dimensional.
Low-dimensionality. We empirically evaluated if the features obtained at the penultimate layer of
a neural network indeed lie in a low-dimensional affine subspace. For this purpose, we trained
a convolutional neural network architecture to classify horses and planes from the CIFAR-10
dataset (Krizhevsky & Hinton, 2009). Figure 3 shows the cumulative variance explained for the
features that feed into the soft-max layer as a function of the number of principle components used.
We observe that the features, which are the outputs of the penultimate layer of the network, lie in a
low-dimensional affine subspace, and this holds for a variety of training modalities for the network.
This observation is relevant to Remark 3. The dimension of the subspace containing the training
points is at most 20, which is much smaller than the dimension of the feature space, 84. Conse-
quently, cross-entropy minimization with a gradient method is expected to yield a poor margin on
these features.
6	Discussion
We compare our results with related works and discuss their implications for the following subjects.
Adversarial examples. State-of-the-art neural networks have been observed to misclassify inputs
that are slightly different from their training data, which indicates a small margin between their de-
cision boundaries and the training dataset (Szegedy et al., 2013; Goodfellow et al., 2015; Moosavi-
Dezfooli et al., 2017; Fawzi et al., 2017). Our results reveal that the combination of gradient meth-
ods, cross-entropy loss function and the low-dimensionality of the training dataset (at least in some
domain) has a responsibility for this problem. Note that SVM with the radial basis function was
shown to be robust against adversarial examples, and this was attributed to the high nonlinearity of
the radial basis function in (Goodfellow et al., 2015). Given that the SVM uses neither the cross
entropy loss function nor the gradient descent algorithm for training, we argue that the robustness
of SVM is no surprise - independent of its nonlinearity. Lastly, effectiveness of differential training
for neural networks against adversarial examples is our ongoing work.
7
Under review as a conference paper at ICLR 2019
number of principal components used
Figure 3: The activations feeding into the soft-max layer could be considered as the features for a
linear classifier. Plot shows the cumulative variance explained for these features as a function of
the number of principle components used. Almost all the variance in the features is captured by the
first 20 principle components out of 84, which shows that the input to the soft-max layer resides
predominantly in a low-dimensional subspace.
Low-dimensionality of the training dataset. As stated in Remark 3, as the dimension of the affine
subspace containing the training dataset gets very small compared to the dimension of the input
space, the training algorithm will become more likely to yield a small margin for the classifier. This
observation confirms the results of (Marzi et al., 2018), which showed that if the set of training
data is projected onto a low-dimensional subspace before feeding into a neural network, the perfor-
mance of the network against adversarial examples is improved - since projecting the inputs onto a
low-dimensional domain corresponds to decreasing the dimension of the input space. Even though
this method is effective, it requires the knowledge of the domain in which the training points are
low-dimensional. Because this knowledge will not always be available, finding alternative training
algorithms and loss functions that are suited for low-dimensional data is still an important direction
for future research.
Robust optimization. Using robust optimization techniques to train neural networks has been
shown to be effective against adversarial examples (Madry et al., 2018; Athalye et al., 2018). Note
that these techniques could be considered as inflating the training points by a presumed amount and
training the classifier with these inflated points. Consequently, as long as the cross-entropy loss is
involved, the decision boundaries of the neural network will still be in the vicinity of the inflated
points. Therefore, even though the classifier is robust against the disturbances of the presumed
magnitude, the margin of the classifier could still be much smaller than what it could potentially be.
Differential training. We introduced differential training, which allows the feature mapping to re-
main trainable while ensuring a large margin between different classes of points. Therefore, this
method combines the benefits of neural networks with those of support vector machines. Even
though moving from 2N training points to N2 seems prohibitive, it points out that a true classifica-
tion should in fact be able to differentiate between the pairs that are hardest to differentiate, and this
search will necessarily require an N2 term. Some heuristic methods are likely to be effective, such
as considering only a smaller subset of points closer to the boundary and updating this set of points
as needed during training. If a neural network is trained with this procedure, the network will be
forced to find features that are able to tell apart between the hardest pairs.
Nonseparable data. What happens when the training data is not linearly separable is an open
direction for future work. However, as stated in Remark 4, this case is not expected to arise for
the state-of-the-art networks, since they have been shown to achieve zero training error even on
randomly generated datasets (Zhang et al., 2017), which implies that the features represented by the
output of their penultimate layer eventually become linearly separable.
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, 2018.
8
Under review as a conference paper at ICLR 2019
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
A. Fawzi, S. Moosavi-Dezfooli, and P. Frossard. The robustness of deep networks: A geometrical
perspective. IEEE Signal Processing Magazine, 34(6):50-62, Nov 2017.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
Kosuke Ishibashi, Kohei Hatano, and Masayuki Takeda. Online learning of maximum p-norm mar-
gin classifiers with bias. In Conference on Learning Theory, 2008.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. CoRR,
abs/1803.07300, 2018.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. A fast iterative nearest point
algorithm for support vector machine classifier design. IEEE Transactions on Neural Networks,
11(1):124-136, Jan 2000.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Z. Marzi, S. Gopalakrishnan, U. Madhow, and R. Pedarsani. Sparsity-based Defense against Adver-
sarial Attacks on Linear Classifiers. ArXiv e-prints, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In IEEE Conference on Computer Vision and Pattern Recognition, pp.
86-94, 2017.
M. Shpigel Nacson, J. Lee, S. Gunasekar, P. H. P. Savarese, N. Srebro, and D. Soudry. Convergence
of Gradient Descent on Separable Data. ArXiv e-prints, 2018a.
M. Shpigel Nacson, N. Srebro, and D. Soudry. Stochastic Gradient Descent on Separable Data:
Exact Convergence with a Fixed Learning Rate. ArXiv e-prints, 2018b.
Kamil Nar and S. Shankar Sastry. Step size matters in deep learning. CoRR, abs/1805.08890, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase
the batch size. In International Conference on Learning Representations, 2018.
D. Soudry, E. Hoffer, M. Shpigel Nacson, S. Gunasekar, and N. Srebro. The Implicit Bias of
Gradient Descent on Separable Data. ArXiv e-prints, 2018.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. In International Conference on Learning Representations, 2018.
9
Under review as a conference paper at ICLR 2019
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
10
Under review as a conference paper at ICLR 2019
A Proof of Theorem 1
Theorem 1 could be proved by using Theorem 2, but we provide an independent proof here. Gradient
descent algorithm with learning rate δ on the cross-entropy loss (1) yields
dw	e-w>≡	e-w>y
——=δX--------r>7 + δy------干?.
dt	1 + e-W Ix 1 + e-W 1 y
If W(0) = 0, then W(t) = p(t)x + q(t)y for all t ≥ 0, where
e-pk 训2—qhx,yi	e-qkyk2-phx,yi
P = δ 1 + e—pkxk2—qhx,yi,	q = δ 1 + e—qk训2—phx,y).
Define
α=PkXll2 + q(x,办	β = qι∣y∣ι2 + p〈x,办
a = δ∣∣X∣∣2 = δσx, C = δ∣y∣2 = δσy,	b = δ(X, y) = δσxy.
Then we can write
e-α	j	e-β
Q = a---------+ b----石,
1 + e-a 1 + e-β
e e—β	e—α
β = c------+ + b------
尸	1 + e—β 1 + e—a
Finally, define Z = eα and v = eβ so that
Z = ɪfɑ + b±
Z + 1 V V + 1
V=’T(C + bV+1
v + 1 ∖ z + 1
Without loss of generality, assume C ≥ a. Before we proceed, note that
d
dt
c — b z
Z+1v
a—b Z+1
--------
c — b v + 1
d ( z + 1 ʌ dt I v + 1J	=-ɪ- ^v- + 1 a—vb― fvz- + 1 c ― bY (v + 1)2 [z + 1	z	∖zv + 1	)
Let U and w denote Z and 公1, respectively. Then,
	a—b U < 0 if w > 	 c—b a—b U > 0 if w < 		 c—b W < 0 if ( — + bj u < cw + b w W > 0 if ( — + b) u > cw + b w
Lemma 1. If b = 0, then	1. w(t) — 1	1 ~ lιm  ；-- = ———x + ———y. t→∞ log(t) I∣x∣	kyky
Proof. Note that
d(z + log(z))
------;-----=a
dt
d(v + log(v))
dt
=⇒ z(t) — Z0 + log(z(t)∕z0) = at,
=⇒ v(t) — V0 + log(v(t)∕vo) = ct.
11
Under review as a conference paper at ICLR 2019
Then,
α(t)	1.	log(z(t))	1	1.	log(v(t))	β(t)
lιm -—	= lιm	—■~~--	= 1	= lιm	—■~~--	= lιm -~~—
t→∞ log(t) t→∞ log(t)	t→∞ log(t)	t→∞ log(t)
and
lim业=上+ ɪ
t→∞ log(t)	IIxk2	∣∣y∣∣2
Lemma 2. If b < 0, then there exists to ∈ (0, ∞) such that
—b Z + 1 a
一 ≤ --- ≤ --
c — v + 1 - -b
∀t ≥ t0.
Proof- Note that - ≤ a≡b ≤ W because b <0. First assume z0⅛ ≥ W.Then, Z ≤0 and
V ( v v + 1 ∖ V (	b2 ∖	Vo ac — b2
V = --- C + b-- ≥ ----- C ——≥ —0-----------
v + 1 ∖ Z + 1 y V + 1 ∖ a )	Vo + 1 a
which implies that
as long as 貂 ≥
Then, V ≤ 0 and
z + 1	/	- ( Vo ac — b2	τ、1
—τ ≤ (zo + 1) -ɪ----------------------1 + 1
V + 1	∖Vo + 1 a	)
-⅞, and this can be satisfied only for a finite time. Now assume z0⅛ ≤ ≡.
≡b'	<	V0 +1 - C
Z	Z + 1	Z0 ac - b2
Z = ------- a + b-------- ≥-------------------
Z + 1 V + 1	Z0 + 1 c
which implies
z + 1 Z Z0 ac — b2 τ、/ τ、1
—τ ≥ TT--------------------1 + 1 (Vo + 1)≡1
V + 1 Z0 + 1 c
as long as V+1 ≤ ≡b, and this can be satisfied only for a finite time as well.
Lemma 3. If b < 0, then
ac — b2	ac — b2
0 ≤ Z ≤ -, 0 ≤ V ≤ - ∀t ≥ to,
ca
where to is given by Lemma 2.
Proof.
Z
Z + 1
Z + 1 Z ac — b2
a + bV∏J ≤ R
ac — b2
≤--------
c
V
V =----
V+1
V + 1 ʌ V ac — b2
C + Z + 1 ) ≤ V + 1 -a-
ac — b2
≤ ------
a
Lemma 4. If b < 0, then
lim W=Iim ㈣Vl = 1
t→∞ log t	t→∞ log t
Proof. From Lemma 3,
V+1
V ≤ 0 ^⇒ C + b-- ≥ 0,
Z+1
and
V≤
Combining these two inequalities, we have
C +— [(ac — b2)t + V + 1] —+ɪ
≥ 0 ^⇒ Z + 1 ≥ --[(ac — b2)t + vo + 1]
ac	o
12
Under review as a conference paper at ICLR 2019
As a result,
(-b)(ac - b2)t + Z1 ≤ z(t) ≤ 竺二艺
ac
c
log(z)
t + Zz ∀t ≥ t0 =⇒ tlim∞ Iog6=1.
By using Lemma 2,
log(-b/c) ≤ log(z +1) - log(v + 1) ≤ log(-a/b) =⇒ lim IOg(V) = 1.
t→∞ log(t)
Lemma 5. If b < 0, then
W⑴	_ XC - b
Jim 1 〜小=δ------G
t→∞ log(t) ac - b2
a - b σy - σxy
X + δ 五y =	2-
ac - b2	σxσy - σx2
y xy
σx - σxy
χ +---------2- y
σxσy - σx2y
Proof. Solving the set of equations
we obtain
1
1
lim
t→∞
Lemma 6. If b > 0, then
= lim	α	二 a lim	P	+ - lim	q
	log(t) =	δ	log(t)	+ δ	log(t),
= lim	β =	二 C lim	q	+ - lim	P
	log(t)	δ	log(t)	+ δ	log(t),
L P	=δc ac	—b	lim t→∞	q =	a a — b δac - b2
ɔ log(t)		-b2 ,		log(t)一	
z	= lim t→∞	z + 1	0	if a ≤ b	
lim 一 t→∞ V		V + 1	=t a—b	ifa>b		
Proof. Note that Z ≥ a/2 and V ≥ c/2; therefore,
z+1	z
lim ------ = lim — =⇒ lim
t→∞ V + 1	t→∞ V	t→∞
if either side exists. Remember that
u
lim w
t→∞
cW2 + bW
W < 0 ^⇒ u < ------------ =: f (w).
a + bW
We can compute
0	2acW + bcW2 + ab
W	b2w2 + 2abw + a2 .
The function f is strictly increasing and convex for W > 0. We have
f(0)=0,
a-b a-b
f (C-I) = C-b.
Therefore, when b ≥ a, the only fixed point of f over [0, ∞) is the origin, and when a > b, 0 and
(a - b)/(c - b) are the only fixed points of f over [0, ∞).
Figure 4 shows the curves over which U = 0 and W = 0. Since limt→∞ U = limt→∞ w, the only
points (u, w) can converge to are the fixed points of f. Remember that
c -b	a -b
U = ---u ------——W ,
Z+1	c-b
so when a > b, the origin (0, 0) is unstable in the sense of Lyapunov, and (u, w) cannot converge to
it. Otherwise, (0, 0) is the only fixed point, and it is stable. As a result,
lim Z = Iim 山
t→∞ v t→∞ v + 1
0
a-b
c—b
if a ≤ b
if a > b
13
Under review as a conference paper at ICLR 2019
c-b
Figure 4: Stationary points of function f.
Lemma 7. If a > b > 0, then
lim
t→∞ log(t)
δ
c — b
ac — b* 2
a—b
x+δac~i2y
σy — σxy ~ +
σχσy — σ2y
σx — σxy ~
---------2 y.
σxσy — σx2y
Proof. From Lemma 6,
lim Z
t→∞ t
lim Z
t-∞
lim
t-∞
Z
Z+1
a + b
Z +1
v + 1
a+b
a—b
ac — b2
Consequently,
c—b
c—b ,
lim V
t→∞ t
ac — b2
a—b
lim叩
t→∞ log(t)
lim9
t→∞ log(t)
1
which gives the same solution as Lemma 5:
ιim r^pπ
t→∞ log(t)
c—b
ac — b2 ,
.lim rΛττ
t→∞ log(t)
a—b
ac — b2 .
δ
δ
Lemma 8. If b ≥ a, then
lim ≡ = ɪ ≡
t→∞ log(t)	∣3∣
Proof.
lim Z
t→∞ t
a,
lim*
t→∞ log(t)
1,
lim
t-∞
lim V
t-∞
lim≥⅛)
t→∞ log(t)
lim Vt
t→∞ v
lim
t-∞
v+1
lim
t-∞
ct
v+1
lim
t-∞
bt
Z+1
lim
t-∞
P
log(t)
lim
t-∞
q
log(t)
n 一 「	pχ + qy
0 = lim -；~~k
t→∞ log(t)
1
-----X
kXk2
v
t
∞
1
k训2,
1
C+⅛∙)t
+
b
a
Proof of Theorem 1. Lemma 1, Lemma 5, Lemma7 and Lemma 8 prove all cases of Theorem 1. ■
14
Under review as a conference paper at ICLR 2019
B Proof of Corollary 1
Since x>y = 1, we have σχy = x>y = x>y 一 1 = 0 < a. Then the normal vector of the decision
boundary is proportional to -1X + -1 y. For the angle between this vector and the solution of the
σx	σy
SVM, we can write
h σ1χ X + σ1 y,x + yi
xy
2
cos ψ
σ1xX + σ1y y∣ 川 X + 别 Il σ1χX + σ1y yII √σχ + σ
We can obtain a lower bound for square of the denominator as
土X+σyyII	(σx+σy) ≥ (2+σχ	一	σ2)	+ (g + W + σx	一	σ2)	≥2+σ;	(1一
As a result,
4
cos2 ψ ≤
2+σ
C Proof of Theorem 3
Let hwSVM, •)+ bSVM = 0 denote the hyperplane obtained as the solution of SVM, i.e., (WSVM, 6svm)
is the solution to the problem
min ∣∣w∣∣2 s.t. hw, Xii + b ≥ 1,〈w, —yj)+ b ≤ —1 ∀i ∈ I, ∀j ∈ J.
w,b
Assume that w = U + Pm=I akrk, where U ∈ Rd andhu, Irk = 0 for all k ∈ K.
The Lagrangian of the problem (4) can be written as
2 kwk2 + 2 b2 + XiCI μi(I — hW,Xi i 一 b) + XjJ Vj (I 一 hw,yj i + b),
where μ% ≥ 0 for all i ∈ I and Vj ≥ 0 for all j ∈ J. KKT conditions for the optimality of W and B
requires that
W = XiCI μtχt + XjCJVj yj,
B = EiCI 〃'一 EjJVj,
and consequently, for each k ∈ K,
hw, rk i = EiCI μihXi,rk i - fjCJVjh-yj ,rk i = EiCI δ μi 一 £jcJdk Vj = BAk.
Then, we can write W as
W = u
+ kCKB∆krk.
Remember, by definition,
WSVM = arg min kWk2	s.t.	hW,	Xi	+ yji	≥ 2	∀i	∈ I,	∀j	∈ J.
Since the vector U also satisfies hU, Xi + yji = hW, Xi + yji ≥ 2 for all i ∈ I, j ∈ J, we have
IlUIl ≥ IlWSVMIl = 1. As a result, the margin obtained by minimizing the cross-entropy loss is
11	1
-——-=—,	≤ -,	二
kwk	PkUk2 + P kB∆”kk2 - qγ12 + B2 P A
15
Under review as a conference paper at ICLR 2019
D Proof of Theorem 4
If B < 0, We could consider the hyperplanehw, •)一 B = 0 for the points {-Xi}i∈ι and {y7-}j∈j,
which would have the identical margin due to symmetry. Therefore, without loss of generality,
assume B ≥ 0. As in the proof of Theorem 3, KKT conditions for the optimality of W and B
requires
W = ∑i∈ι μiχi + fj∈jν yj, B = ∑i∈ι μi - ∑j∈j νj
where μ% ≥ 0 and Vj ≥ 0 for all i ∈ I, j ∈ J. Note that for each k ∈ K,
hw,rki	= Ei∈iμihxi,rki - ∑Sj∈j Vjh-yj,rki
=B∆k + y^j∈ι μi(hxi,rki - δQ - y^j∈j Vj (h-yj, rk i - ∆k ) ≥ B∆k ∙
Since {rk}k∈K is an orthonormal set of vectors,
kwk2 ≥ ∑k∈κ hw,rki2 ≥ ∑k∈κ BN.
The result follows from the fact that kw∣∣ T is an upper bound on the margin.	■
E Proof of Theorem 6
In order to achieve zero training error in one iteration of the stochastic gradient algorithm, it is
sufficient to have
mini0 ∈I hxi0, xi + yji > maxj0∈jh-yj0,xi + yji ∀i ∈ I, ∀j ∈ J,
or equivalently,
hxi0 + yj0,xi + yji > 0 ∀i, i0 ∈ I, ∀j,j0 ∈ J.
(9)
By definition of the margin, there exists a vector WSVM ∈ Rd with unit norm which satisfies
2γ = mini∈I,j∈j hxi + yj, WSVMi.
Note that WSVM is orthogonal to the decision boundary given by the SVM. Then we can write every
xi + yj as
xi + yj = 2γWSVM + δix + δjy ,
where δix , δjy ∈ Rd and kδix k ≤ Rx and kδjy k ≤ Ry . Then, condition (9) is satisfied if
h2γWSVM+δix+δjy,2γWSVM+δix0 +δjy0i >0 ∀i, i0 ∈ I, ∀j, j0 ∈ J,
or equivalently if
4γ2+2γhWSVM,δix+δjy+δix0+δjy0i+hδix+δjy,δix0+δjy0i	>0	∀i,i0∈I,	∀j,j0	∈ J. (10)
If we choose γ > 5 max(Rx, Ry), we have
4γ2 -2γ(2Rx+2Ry) -(Rx+Ry)2 >0,
which guarantees (10) and completes the proof.	■
16