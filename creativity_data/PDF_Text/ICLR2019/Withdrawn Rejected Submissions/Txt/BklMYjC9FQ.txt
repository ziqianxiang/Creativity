Under review as a conference paper at ICLR 2019
microGAN: Promoting Variety through Micro-
batch Discrimination
Anonymous authors
Paper under double-blind review
Ab stract
We propose to tackle the mode collapse problem in generative adversarial net-
works (GANs) by using multiple discriminators and assigning a different portion
of each minibatch, called microbatch, to each discriminator. We gradually change
each discriminator’s task from distinguishing between real and fake samples to
discriminating samples coming from inside or outside its assigned microbatch by
using a diversity parameter α. The generator is then forced to promote variety in
each minibatch to make the microbatch discrimination harder to achieve by each
discriminator. Thus, all models in our framework benefit from having variety in
the generated set to reduce their respective losses. We show evidence that our so-
lution promotes sample diversity since early training stages on multiple datasets.
1	Introduction
Generative adversarial networks (Goodfellow et al. (2014)), or GANs, consist of a framework de-
scribing the interaction between two different models - one generator (G) and one discriminator (D)
- that are trained together. While G tries to learn the real data distribution by generating realistic
looking samples that are able to fool D, D tries to do a better job at distinguishing between real
and the fake samples produced by G. Although showing very promising results across various do-
mains (Edwards and Storkey (2015); Ho and Ermon (2016); Yu et al. (2017); Kim et al. (2017a);
Yang et al. (2017); Donahue et al. (2018)) , GANs have also been continually associated with insta-
bility in training, more specifically mode collapse (Kim et al. (2017b); Arjovsky and Bottou (2017);
Mescheder et al. (2017); Che et al. (2016); Arjovsky et al. (2017)). This behavior is observed when
G is able to fool D by only generating samples from the same data mode, leading to very similar
looking generated samples. This suggests that G did not succeed in learning the full data distribution
but, instead, only a small part of it. This is the main problem we are trying to solve with this work.
The proposed solution is to use multiple discriminators and assign each D a different portion of the
real and fake minibatches, i.e., microbatch. Then, we update each D’s task to discriminate between
samples coming from its assigned fake microbatch and samples from the microbatches assigned to
the other discriminators, together with the real samples. We call this microbatch discrimination.
Throughout training, we gradually change from the originally proposed real and fake discrimination
by Goodfellow et al. (2014) to the introduced microbatch discrimination by the use of an additional
diversity parameter α that ultimately controls the diversity in the overall minibatch.
The main idea of this work is to force G to reduce its loss by inducing variety in the generated set,
complicating each D ’s task on separating the samples in its microbatch from the rest. Even though
only producing very similar images would also complicate the desired discrimination, it would not
benefit any of the models. This is due to the attribution of distinct probabilities by each D to samples
from and outside its microbatch being required to minimize G and D’s losses. Hence, all models in
the proposed framework, called microGAN, benefit directly from diversity in the generated set.
Our main contributions can be stated as follows: (i) proposal of a novel multi-adversarial GANs
framework1 (Section 3) that mitigates the inherent mode collapse problem in GANs; (ii) empirical
evidence on multiple datasets showing the success of our approach in promoting sample variety
since early stages of training (Section 4) (iii) Competitiveness against other previously proposed
methods on multiple datasets and evaluation metrics (Section 5).
1Code will be available upon publication.
1
Under review as a conference paper at ICLR 2019
1.1	Related Work
Previous works have optimized GANs training by changing the overall models’ objectives, either by
using discrepancy measurements (Li et al. (2017); Sutherland et al. (2016)) or different divergence
functions (Nowozin et al. (2016); Uehara et al. (2016)) to approximate the real data distribution.
Moreover, Zhao et al. (2016); Berthelot et al. (2017); Unterthiner et al. (2017) proposed to use
energy-driven objective functions to encourage sample variety, Mroueh et al. (2017) tried to match
the mean and covariance of the real data, and Metz et al. (2016) used an unrolled optimization of D
to train G. Che et al. (2016); Warde-Farley and Bengio (2016); Wang et al. (2017); Berthelot et al.
(2017) penalized missing modes by using an extra autoenconder in the framework. Salimans et al.
(2016) performed minibatch discrimination by forcing D to condition its output on the similarity
between the samples in the minibatch. Springenberg (2015) increased D’s robustness by maximizing
the mutual information between inputs and corresponding labels, while Lin et al. (2017) forced D
to make decisions over multiple samples of the same class, instead of independently.
Regarding using multiple discriminators, Neyshabur et al. (2017) extended the framework to several
discriminators with each focusing in a low-dimensional projection of the data, set a priori. Durugkar
et al. (2016) proposed GMAN, consisting of an ensemble of discriminators that could be accessed
by the single generator according to different levels of difficulty. Nguyen et al. (2017) introduced
D2GAN, introducing a single generator dual discriminator architecture where one discriminator
rewards samples coming from the true data distribution whilst the other rewards samples coming
from the generator, forcing the generator to continuously change its output. Mordido et al. (2018)
applied adversarial dropout by omitting the feedback of a given D at the end of each batch.
2	Generative Adversarial Networks
The original GANs framework (Goodfellow et al. (2014)) consists of two models: a generator (G)
and a discriminator (D). Both models are assigned different tasks: whilst G tries to capture the real
data distribution pr , D learns how to distinguish real from fake samples. G maps a noise vector z,
retrieved from a noise distribution pz, to a realistic looking sample belonging to the data space. D
maps a sample to a probability p, representing the likeliness of that given sample coming from pr
rather than from pg . The two models are trained together and play the following minimax game:
min max V (D,G) = Ex 〜p,(x)[log D(x)] + Ez 〜Pz(Z) [log(1 - D(G(Z)))],	(1)
where D maximizes the probability of assigning samples to the correct distribution and G minimizes
the probability of its samples being considered from the fake data distribution.
Alternatively, one can also train G to maximize the probability of its output being considered from
the real data distribution, i.e., log D(G(z)). Even though this changes the type of the game, by being
no longer minimax, it avoids the saturation of the gradient signals at the beginning of training (Good-
fellow et al. (2014)), where G only receives continuously negative feedback, making training more
stable in practice. However, since we employ multiple discriminators in the proposed framework,
it is less likely that G does not receive any positive feedback from the whole adversarial ensem-
ble (Durugkar et al. (2016)). Therefore, we make use of the original value function in this work.
3	microGAN
In this work, we propose a novel generative multi-adversarial framework named microGAN, where
we start by splitting each minibatch into several microbatches and assigning a unique one to each D .
The key aspect of this work is the usage of microbatch discrimination, where we change the original
discrimination task of distinguishing between real and fake samples, as proposed in Goodfellow
et al. (2014), to each D distinguishing between samples coming or not from its fake microbatch.
This change is performed in a gradual fashion, using an additional diversity parameter α. Thus, each
D ’s output gradually changes from the probability of a given sample being real to the probability
of a given sample not belonging to its fake microbatch. Moreover, since each D is trained with
different fake and real samples, we encourage them to focus on different data properties. Figure 1
illustrates the proposed framework.
2
Under review as a conference paper at ICLR 2019
The proposed microbatch-level discrimination
task leads to G making such discrimination
harder for each D to lower its loss. Hence, G
is forced to induce variety on the overall mini-
batch, making it a substantially harder task for
each D to be able to separate its subset of fake
samples in the diverse minibatch. Note that pro-
ducing very similar samples across the whole
minibatch would also make such discrimina-
tion difficult by making the whole minibatch
the same. However, G also benefits from each
D assigning distinct probabilities to samples
from inside and outside its designed microbatch
to lower its loss, making the generation of dif-
ferent samples in the minibatch a necessary re-
quirement to obtain different outputs from D.
Hence, all models in our framework benefit di-
rectly from sample variety in the generated set.
In the microGAN scenario with a positive di-
versity parameter α, each D assigns low proba-
bilities to fake samples from its microbatch and
high probabilities to fake samples from the rest
of the microbatches as well as samples from
the real data distribution. Hence, fake sam-
ples in the rest of the minibatch, i.e., not com-
ing from its assigned microbatch, shall be given
distinct output probabilities by each D. On the
other hand, G minimizes the probability given
by each D to the samples outside its microbatch
Figure 1: microGAN framework assuming a posi-
tive diversity parameter α. Each discriminator Dk
is assigned a different microbatch xGD , where it
discriminates between samples coming from in-
side its microbatch and samples coming from the
microbatches assigned to the rest of the discrimi-
nators (xG\xGD ) together the real samples xrD .
and maximizes the probability given to the fake samples assigned to that specific D. The value func-
tion of our minimax game is as follows:
K
K
min maχ EV(Dk,G)=工四方〜石2 (χ)[logDk(x)]+ Ez~pzG	(z)[log(1 - Dk(G(z)}}]
G	Dk k=1	k=1	k	Dk	(2)
+α × Ez0~Pzr. \{z「)(z0)[logDk(G(z,))],
zGD \{zGDk }
where K represents the number of total discriminators in the set. prD represents real samples from
Dk’s real microbatch, pzG indicates fake samples from Dk’s fake microbatch, and pzG \{zG }
relates to the rest of the fake samples in the minibatch but not in pzG . α represents the diver-
sity parameter responsible for penalizing the incorrect discrimination of fake samples coming from
pzG \{zG } by each Dk. Note that α = 0 would represent the original GANs objective for each D
in the set. Appropriate α values and its effects on the whole minibatch diversity are discussed next.
3.1	DIVERSITY PARAMETER α
We control the weight of the microbatch discrimination in the models’ losses by introducing an addi-
tional diversity parameter α. Lower α values lead to G significantly lowering its loss by generating
realistic looking samples on each microbatch without taking much consideration on the variety of
the overall minibatch. On the other hand, higher α values induce a stronger effect on G’s loss if each
D is able to discriminate between samples inside and outside its microbatch. However, high values
of α might compromise the realistic properties of the produced samples, since too much weight is
given to the last part of Eq. 2, being sufficient to effectively minimize G’s loss. Thus, using α > 0
represents an additional way of ensuring data variety within the minibatch produced by G at each
iteration. An overview of different possible α settings follows below.
3
Under review as a conference paper at ICLR 2019
3.1.1	STATIC α
First, we statically set α to values between 0 and 1 throughout the whole training. For the evaluation
of the effects of each α value, we used a toy experiment of a 2D mixture of 8 Gaussian distributions
(representing 8 data modes) firstly presented by Metz et al. (2016), and further adopted by Nguyen
et al. (2017). We used 8 discriminators for all the experiments. Results are shown in Figure 2.
When setting α = 0, G mode collapses
on a specific mode, showing the impor-
tance of using positive α values to miti-
gate mode collapse. When setting 0.1 ≤
α ≤ 0.5, G is able to capture all data
modes during training. However, learn-
ing problems in the early stages are ob-
served, with G only focusing on promot-
ing variety in the generated samples. For
higher α values (α ≥ 0.6), G was un-
able to produce any realistic looking sam-
ples throughout the whole training, focus-
ing solely on sample diversity to lower its
loss, suggesting the dominance of the last
part of Eq. 2. Hence, a mild, dynamic,
manipulation of α values seems to be nec-
essary for a successful training of G, ulti-
mately meaning both realistic and diverse
samples from an early training stage.
Figure 2: Toy experiment using static α values. Real
data is presented in red while generated data is in blue.
3.1.2	SELF-LEARNED α
We dynamically set α over time by adding it as a parameter of G and letting it self-learn its values
to lower its loss. However, we observed that G takes advantage of being able to reduce its loss by
increasing α at a large rate, focusing simply on promoting diversity in the generated samples without
much realism, similarly to what was observed when using α = 0.6 in the toy experiment (Figure 2).
Hence, we suggest several properties that α should have so that diversity does not compromise the
veracity of the generated samples.
First, α should be upper bounded so that the last part of Eq. 2 (responsible for sample diversity)
does not overpower the first part (responsible for sample realism), ultimately not compromising
the feedback given to G to also be able to generate realistic samples. Second, a's growth should
saturate over time, meaning that continuously increasing at large rates α is no longer an option to
substantially decrease G’s loss over time. Lastly, to tackle the problem in learning of early to mid
stages, we suggest that α should grow in a controlled fashion, so focus can also be given in the
realistic aspect of the samples since the beginning of training.
Thus, we propose to make α a function of β, where α(β) ∈ [0, 1[, and let G regulate β instead of
directly learning α. We evaluated regulating α over three different functions that have the desired
properties:
'αsigm(β) = Sigmoid(β),β ≥ βsigm
α(β) = αsoft(β) = Softsign(β),β ≥βsof t
,αtanh(β) = Tanh(β),β ≥ βtanh
(3)
with βsigm , βsoft , and βtanh representing the initial values ofβ when training begins for the respec-
tive functions. For all the experiments of this paper, we set βtanh = βsof t = 0, to obtain a positive
codomain, and βsigm = -1.8, since we achieved better empirical results by starting β with this
value (for further discussion about the effects of using different βsigm on αsigm(β)s growth please
see the Appendix). Note that learning α without any constraints can be characterized as using the
identity function (α(β) = αident (β) = β). Thus, each used function promotes a different α growth
over time. To ease presentation, we neglect to write β's dependence for the rest of the manuscript
and use only the function names to described each α setting: αsigm , αsoft , αtanh, and αident .
4
Under review as a conference paper at ICLR 2019
Results on the toy dataset using the different
proposed α functions are shown in Figure 3.
The benefits of increasing α in a milder fash-
ion, as performed when using αsigm , are ob-
served especially early on training, with G be-
ing concerned with the realism of the gener-
ated samples. On the other hand, when using
αtanh and αsoft , the network takes longer to
focus on the data realism (10K steps) since it
is able to reduce its loss significantly by simply
promoting variety due to the steeper growth of
α in the earlier stages on both functions. Nev-
ertheless, as the functions gradually saturate,
all α settings manage to eventually capture the
real data distribution while still keeping the di-
versity in the generated samples.
In conclusion, one can summarize micro-
GAN’s training using these variations of self-
learned α as the following: in the first itera-
tions, G increases α to reduce its loss, expand-
ing its output. As α starts to saturate and each
D learns how to distinguish between real and
fake samples, G is forced to lower its loss by
creating both realistic and diverse samples.
(a) Generated samples.
(b) α evolution.
Figure 3: Analysis of using different α functions
on the toy dataset. The generated samples are
shown in (a). The evolution of α on each function
is presented in (b).
4	Experimental Results
We validated the effects of using different α functions on MNIST (LeCun and Cortes (2010)),
CIFAR-10 (Krizhevsky (2009)), and cropped CelebA (Liu et al. (2015)). To quantitatively eval-
Uate such effects, We used the Frechet Inception Distance (HeUsel et al. (2017)), or FID, since it
has been shown to be sensitive to image quality as well as mode collapse (Lucic et al. (2017)), with
the returned distance increasing notably When modes are missing from the generated data. We used
several variations of the standard FID for a thorough study of a's effects in training, as well as the
influence of using a different number of discriminators in our frameWork.
4.1	Intra FID
To measure the variety of samples of the generated set, we propose to calculate the FID between
two subsets of 10K randomly picked fake samples generated at the end of every thousand iterations.
We call this metric Intra FID. Important to note that Intra FID only measures the diversity in the
generated set, not its realism. Hence, higher values indicate more diversity within the generated
samples while lower values might indicate mode collapse in the generated set. The relation between
Intra FID and progressive values of α is shown in Figure 4.
Figure 4: Intra FID as α progresses. Higher values represent higher variety in the generated set.
We observe a strong correlation between a's growth and variety in the set, especially in beginning
to mid-training. Later on, as α saturates, the variety is kept (represented by the stability of the Intra
5
Under review as a conference paper at ICLR 2019
FID). It is further visible that αsigm , αsoft, and αtanh converge to similar Intra FID on all datasets.
Important to note, that, to ease the visualization, the graphs only represent 0 ≤ α ≤ 1, with αide"s
values naturally surpassing 1 as time progresses.
4.2	Cumulative Intra FID
To analyze the sample variety over time, we summed the Intra FID values obtained from every
thousand iterations. Hence, higher values indicate that the model was able to promote more variety in
the set across time. Results are shown in Figure 5, where we observe that using more discriminators
leads to more variety across all datasets and α functions. Moreover, using α = 0 leads to lower
variety compared to using positive α values, with αsigm, αsoft, and αtanh obtaining similar values
throughout the different datasets. Even though αident promotes the highest variety, the generated
samples lack realism, as previously witnessed in the toy experiment and further discussed next.
Figure 5: Cumulative Intra FID using a different number of discriminators and α functions on the
different datasets. Higher values correlate to higher variety in the produced samples across time.
Values obtained using standard GANs are represented by the grey plane as a baseline.
4.3	Mean and Minimum FID
To analyze both the realism and variety of the generated samples, we used the standard FID cal-
culated between 10K fake samples and the real training data. Lower values should indicate both
diversity and high-quality samples. The Mean FID and Minimum FID across 50K iterations are
presented in Table 1 for each dataset. We observe that the best values, both in terms of mean and
minimum, are obtained when using a higher number of discriminators, i.e., 5 or 10, and αtanh,
αsoft, and αsigm. Moreover, the high distances obtained when using αident confirm the lack of
realism of the generated samples, highlighting the importance of constraining α by the properties
previously stated in Section 3.
Table 1: Mean and Minimum FID over 50K iterations on the different datasets.
MICROGAN I	MNIST	CIFAR-10	CELEBA
K	α	Mean FID	MIN FID	MEAN FID	MIN FID	MEAN FID	MIN FID
1	-	50.9 ± 9.7	22.7 ± 0.7	125.5 ± 1.5	84.8 ± 1.6	77.3 ± 1.7	38.5 ± 1.1
2	αsigm	37.6 ± 1.1	23.5 ± 3.0	111.9 ± 0.1	90.8 ± 0.6	76.3 ± 0.6	53.0 ± 2.6
2	αsof t	41.9 ± 1.2	24.6 ± 0.0	110.2 ± 0.9	90.6 ± 1.2	74.7 ± 2.9	49.5 ± 0.1
2	αtanh	43.9 ± 0.8	27.2 ± 0.5	115.3 ± 0.5	91.3 ± 0.4	87.1 ± 2.4	54.7 ± 0.8
2	αident	89.1 ± 2.2	53.6 ± 2.9	168.1 ±2.0	113.2 ±2.2	206. 1 ± 3.5	113.6 ± 5.2
5	αsigm	34.7 ± 0.3	20.1 ± 0.1	103.9 ± 1.8	81.4 ± 1.1	66.5 ± 0.6	40.4 ± 3.1
5	αsof t	37.2 ± 0.3	19.4 ± 0.1	106.4 ± 0.8	82.5 ± 1.2	69.1 ± 0.3	42.0 ± 2.0
5	αtanh	39.4 ± 1.1	20.0 ± 0.1	107.2 ±0.8	80.8 ± 0.6	70.3 ± 1.3	42.8 ± 0.5
5	αident	61.2 ± 0.3	37.3 ± 0.2	127.9 ±0.4	97.5 ± 2.8	135.9 ± 1.1	77.5 ± 2.0
10	αsigm	38.9 ± 3.0	18.0 ± 0.1	110.2 ± 1.7	79.0 ± 0.7	68.4 ± 0.1	34.8 ± 1.2
10	αsof t	36.2 ± 0.9	17.1 ± 0.2	110.8 ± 0.4	79.2 ± 0.5	67.8 ± 2.6	34.5 ± 0.2
10	αtanh	37.4 ± 1.2	17.4 ± 0.2	112.8 ± 1.7	77.7 ± 0.6	71.0 ± 1.4	34.5 ± 0.3
10	αident	48.7 ± 0.9	28.7 ± 0.1	117.0 ± 0.2	87.1 ± 1.0	91.4 ± 0.2	45.4 ± 0.1
6
Under review as a conference paper at ICLR 2019
4.4	Generated samples
The generated samples on each dataset using 1 and 10 discriminators with different α are presented
in Figure 6. For an objective assessment of the variety by the end of each iteration, the Intra FID is
also provided. We observe the superiority of the generated samples, both in terms of realism and va-
riety, when using αsigm, αsoft, and αtanh on all datasets. However, αtanh seems to show a delayed
ability in generating realistic samples, possibly due to the increase of α at a steeper fashion. The
inability of generating realistic samples when using αident is also clearly detected on all datasets,
as previously discussed. More importantly, the high variety on the generated set, observed by the
high Intra FID, is witnessed since very early iterations when using αsigm, αsoft, and αtanh. The
observed mitigation of mode collapse is carried out throughout the whole training.
O


Q

O

Standard GANs
(K≡ 1; α ≡ 0)
microGAN
(K = 10；a = as.)
microGAN
(K≡10ja≡a5oft)
microGAN
(K = Io"%G
microGAN
(K=I0；a = %em)
microGAN
(K = 10; a = 0)
lw)i
目BBΞ
06142285
0.6 630
QDQQ
□ΞH□
0.25
1.20
Q□□□
BBSS
QQ□□
□Q□Q
0.01
0.3S
0.90
1Λ2
0.31
0.33
ttt⅞*⅞aMwχj
BlHffiflS
0.38
1.68
2.36
□E!即届
HlJIH
在臼ISE
0.44
1.87
2.20
■■网■
厘” ,，
…财
0.50
1.41
2.48
0.44
1.92
2.57
0.49
0.93
0.99
∑99
0.53
1.45
8⅛
hiEEm
两肉后二
F二；
,18
lκ3κ5κ50κ
≡
另

F - P
I■如口
・座传E"
M翡U
■FG
Figure 6: Generated samples from 1K, 2K, 5K and 50K iteration with the respective Intra FID.
When using standard GANs, we notice severe mode collapse, especially early on training. When
using 10 discriminators and α set to 0, we notice a slight variation in the generated set, yet, this is
only detected after a decent number of iterations, when each D has seen enough samples to guide
its judgment to a specific data mode due to the usage of different microbatch for each D, delaying
sample variety substantially. Thus, using positive α values is shown to be a necessary measure to
stimulate variety since the beginning and until the end of training.
5	Method Comparis on
We proceeded to compare different settings of microGAN to other existing methods on 3 different
datasets: CIFAR-10, STL-10 (Coates et al. (2011)), and ImageNet (Deng et al. (2009)). We down-
sampled the images of the last two datasets down to 32x32 pixels. We used Inception Score (Sali-
mans et al. (2016)) or IS (higher is better) as the first quantitative metric. Even though IS has been
shown to be less correlated with human judgment than FID, most previous works only report results
on this metric, making it a useful measure for model comparisons. Out of fairness to the single
discriminator methods that we compare our method against, we used only 2 discriminators in our
experiments. The architectures and training settings used for all the experiments can be found in the
Appendix.
The comparison results are shown in Table 2. We point special attention to the underlined method
representing standard GANs, since it was the only method executed with our own implementation
and identical training settings as microGAN. Thus, this represents the only method directly compa-
rable to ours. We notice a fair improvement of IS on all the tested datasets, observing an increase up
to around 15% for CIFAR-10, 7% for STL-10, and 5% for ImageNet. This indicates the success of
our approach on improving the standard GANs framework on multiple datasets with different sizes
and challenges.
7
Under review as a conference paper at ICLR 2019
On CIFAR-10, microGAN achieves com-
petitive results, significantly outperform-
ing GMAN with 5 discriminators while
using a similar architecture. We argue that
the use of more powerful architectures in
the higher ranked methods plays a big role
in their end score, especially for DCGAN.
Nonetheless, we acknowledge that using
different objectives for each D (as pro-
posed in D2GAN) seems to be beneficial
in a multi-discriminator setting, represent-
ing a good path to follow in the future.
Moreover, we observe that using extra au-
toencoders (DFM) or classifiers (MGAN)
in the framework can help to achieve a bet-
ter performance in the end. However, we
note that MGAN makes use of a 10 gen-
Table 2: Inception scores. For a fair comparison, only
unsupervised methods are compared.
	CIFAR-10	STL-10	ImageNet
Real data	11.24	26.08	25.78
WGAN (ARJOVSKY ET al. (2017))	3.82	-	-
MIX+WGAN (ARORA ET AL. (2017))	4.04	-	-
ALI (Dumoulin et al. (2016))	5.34	-	-
BEGAN (Berthelot et al. (20 17))	5.62	-	-
MAGAN (WANG ET AL. (20 17))	5.67	-	-
GMAN (K = 2) (Durugkar et al. (2016))	5.87	-	-
GANs* (Goodfellow et al. (2014))	5.92	6.78	7.04
GMAN (K = 5) (DURUGKAR ET al. (2016))	6.00	-	-
DCGAN (RADFORD ET AL. (2015))	6.40	7.54	7.89
Improved-GAN (Salimans et al. (2016))	6.86	-	-
D2GAN (NGUYEN ET AL. (2017))	7.15	7.98	8.25
DFM (Warde-Farley and Bengio (2016))	7.72	8.51	9.18
MGAN (HoANG ET AL. (201 7))	8.33	9.22	9.32
MICRoGAN (K = 2; α = αsigm)	6.77	7.23	7.32
MICRoGAN (K = 2; α = αsoft)	6.66	7.19	7.40
MICRoGAN (K = 2; α = αtanh)	6.61	7.07	7.40
erator framework, on top of an extra classifier, to achieve the presented results. Furthermore, the
generated samples presented in their paper (Hoang et al. (2017)) indicate signs of partial mode col-
lapse, which is not reflected in its high IS.
We further compared our best FID with a subset of
the reported methods in Lucic et al. (2017), namely
GANs, both with the original and modified objec-
tive, LSGAN, and DRAGAN on CIFAR-10 (Ta-
ble 3). These methods were chosen since they
represent interesting variants of standard GANs.
We extended each method to an ensemble of dis-
criminators, for a fair comparison to our multiple
discriminator approach. We used the same architec-
ture of the last experiment for all methods. We ob-
serve that all variants of microGAN outperform the
rest of the compared methods under controlled and
equal experiments.
Table 3: Minimum FID comparison.
CIFAR-1 0	
GANs (Goodfellow et al. (2014))	70.73
mod-GANs (Goodfellow et al. (2014))	79.58
LSGAN (MAo ET AL. (2017))	83.66
DRAGAN (KoDALI ET AL. (2017))	80.57
GANs (K = 2)	74.07
MoD-GANS (K = 2)	71.96
LSGAN (K = 2)	73.33
DRAGAN (K = 2)	75.83
MICRoGAN (K = 2; α = αsigm)	66.93
MICRoGAN (K = 2; α = αsoft)	65.54
MICROGAN (K = 2; α = αtanh)	65.84
A subset of the generated samples produced by the different
variations of microGAN reported in Table 2 are shown in
Figure 7, where we observe high variety and realism across
all generated sets. Extended results are provided in the Ap-
pendix
6	Conclusions
In this work, we present a novel framework, named micro-
GAN, where each D performs microbatch discrimination,
differentiating between samples within and outside its fake
microbatch. This behavior is enforced by the diversity pa-
rameter α, that is indirectly self-learned by G. In the first
iterations, G increases α to lower its loss, expanding its out-
put. Then, as α gradually saturates and each D learns how to
better distinguish between real and fake samples, G is forced
to fool each D by promoting realism in its output, while
keeping the diversity in the generated set. We show evi-
dence that our solution produces realistic and diverse sam-
ples on multiple datasets of different sizes and nature, ulti-
mately mitigating mode collapse.
Figure 7: CIFAR-10, STL-10, and
ImageNet results.
N«
A =
JGɑ
/□5
N瞿H一
-⅛i∙
Γ≡Ha
回”⅛
WHBH
■BE
.(K=
icr2:
m =
(κ
8
Under review as a conference paper at ICLR 2019
References
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial net-
works. In International Conference on Learning Representations (ICLR 2017), 2017.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In D. Precup
and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning,
volume 70 of Proceedings ofMachine Learning Research, pages 214-223, International Conven-
tion Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
S.	Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generalization and equilibrium in generative ad-
versarial nets (GANs). In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pages 224-232, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
D. Berthelot, T. Schumm, and L. Metz. Began: Boundary equilibrium generative adversarial net-
works. arXiv preprint arXiv:1703.10717, 2017.
T.	Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode regularized generative adversarial networks.
CoRR, abs/1612.02136, 2016.
A. Coates, A. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning.
In Proceedings of the fourteenth international conference on artificial intelligence and statistics,
pages 215-223, 2011.
J. Deng, W. Dong, R. Socher, L. jia Li, K. Li, and L. Fei-fei. Imagenet: A large-scale hierarchical
image database. In In CVPR, 2009.
C. Donahue, J. McAuley, and M. Puckette. Synthesizing audio with generative adversarial networks.
arXiv preprint arXiv:1802.04208, 2018.
V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and A. Courville.
Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
I. P. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-adversarial networks. CoRR,
abs/1611.01673, 2016.
H.	Edwards and A. Storkey. Censoring representations with an adversary. arXiv preprint
arXiv:1511.05897, 2015.
I.	Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages
2672-2680. Curran Associates, Inc., 2014.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-
scale update rule converge to a local nash equilibrium. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information
Processing Systems 30, pages 6629-6640. Curran Associates, Inc., 2017.
J.	Ho and S. Ermon. Generative adversarial imitation learning. In Advances in Neural Information
Processing Systems, pages 4565-4573, 2016.
Q. Hoang, T. D. Nguyen, T. Le, and D. Q. Phung. Multi-generator generative adversarial nets.
CoRR, abs/1708.02556, 2017. URL http://arxiv.org/abs/1708.02556.
T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to discover cross-domain relations with
generative adversarial networks. arXiv preprint arXiv:1703.05192, 2017a.
T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to discover cross-domain relations with
generative adversarial networks. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learn-
ing Research, pages 1857-1865, International Convention Centre, Sydney, Australia, 06-11 Aug
2017b. PMLR.
9
Under review as a conference paper at ICLR 2019
N. Kodali, J. Abernethy, J. Hays, and Z. Kira. On convergence and stability of gans. arXiv preprint
arXiv:1705.07215, 2017.
A.	Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Y. LeCun and C. Cortes. MNIST handwritten digit database. 2010.
C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Poczos. Mmd gan: Towards deeper understanding
of moment matching network. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems
30, pages 2200-2210. Curran Associates, Inc., 2017.
Z. Lin, A. Khetan, G. Fanti, and S. Oh. Pacgan: The power of two samples in generative adversarial
networks. arXiv preprint arXiv:1712.04086, 2017.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), 2015.
M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet. Are gans created equal? a large-scale
study. arXiv preprint arXiv:1711.10337, 2017.
X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smolley. Least squares generative adversarial
networks. In Computer Vision (ICCV), 2017 IEEE International Conference on, pages 2813-
2821. IEEE, 2017.
L. Mescheder, S. Nowozin, and A. Geiger. The numerics of gans. In Proceedings Neural Information
Processing Systems, 2017.
L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks. CoRR,
abs/1611.02163, 2016.
G. Mordido, H. Yang, and C. Meinel. Dropout-gan: Learning from a dynamic ensemble of discrim-
inators. arXiv preprint arXiv:1807.11346, 2018.
Y. Mroueh, T. Sercu, and V. Goel. Mcgan: Mean and covariance feature matching gan. arXiv
preprint arXiv:1702.08398, 2017.
B.	Neyshabur, S. Bhojanapalli, and A. Chakrabarti. Stabilizing GAN training with multiple random
projections. CoRR, abs/1705.07831, 2017.
T. D. Nguyen, T. Le, H. Vu, and D. Q. Phung. Dual discriminator generative adversarial nets. CoRR,
abs/1709.03831, 2017.
S.	Nowozin, B. Cseke, and R. Tomioka. f-gan: Training generative neural samplers using variational
divergence minimization. In Advances in Neural Information Processing Systems, pages 271-279,
2016.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
T.	Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques
for training gans. In Advances in Neural Information Processing Systems, pages 2234-2242,
2016.
J. T. Springenberg. Unsupervised and semi-supervised learning with categorical generative adver-
sarial networks. arXiv preprint arXiv:1511.06390, 2015.
D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton. Gen-
erative models and model criticism via optimized maximum mean discrepancy. arXiv preprint
arXiv:1611.04488, 2016.
M. Uehara, I. Sato, M. Suzuki, K. Nakayama, and Y. Matsuo. Generative adversarial nets from a
density ratio estimation perspective. arXiv preprint arXiv:1610.02920, 2016.
10
Under review as a conference paper at ICLR 2019
T. Unterthiner, B. Nessler, C. Seward, G. Klambauer, M. Heusel, H. Ramsauer, and S. Hochre-
iter. Coulomb gans: Provably optimal nash equilibria via potential fields. arXiv preprint
arXiv:1708.08819, 2017.
R. Wang, A. Cully, H. J. Chang, and Y. Demiris. Magan: Margin adaptation for generative adver-
sarial networks. arXiv preprint arXiv:1704.03817, 2017.
D. Warde-Farley and Y. Bengio. Improving generative adversarial networks with denoising feature
matching. 2016.
Z. Yang, W. Chen, F. Wang, and B. Xu. Improving neural machine translation with conditional
sequence generative adversarial nets. arXiv preprint arXiv:1703.04887, 2017.
L. Yu, W. Zhang, J. Wang, and Y. Yu. Seqgan: Sequence generative adversarial nets with policy
gradient. In AAAI, pages 2852-2858, 2017.
J. J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. CoRR,
abs/1609.03126, 2016.
11
Under review as a conference paper at ICLR 2019
A Algorithm
The training procedure of microGAN is presented in Algorithm 1.
Algorithm 1 microGAN.
Input: K number of discriminators, α diversity parameter, B minibatch size
Initialize: m — B
for number of training iterations do
•	Sample minibatch z%, i = 1 ...B, Zi 〜Pg(Z)
•	Sample minibatch xi, i = 1 ...B, Xi 〜Pr (x)
for k = 1 to k = K do
•	Sample microbatch Zkj, j = 1 ...m, Zkj = z(k-i)×m+ι,k×m
•	Sample microbatch Xkj, j = 1...m, Xkj = x(k-i)×m+±k×m
•	Sample microbatch Zk0 , j = 1 . . . m, Zk0 ⊂ Zi \ {Zkj }
•	Update Dk by ascending its stochastic gradient:
1m
VθDk- X[logDk(Xkj) + log(1 - Dk(G(Zkjy)) + α × logDk(G(Zk/)]
m j=1
end for
• Update G by descending its stochastic gradient:
Km
VθG X [m X[log(1 - Dk(G(Zkj))) + α × log Dk(G(Zkj))]]
k=1 j=1
end for
B	Theoretical Discussion
To better understand how our approach differs from the original GANs in promoting variety in the
generated set, we used a simplified version of the minimax game where we freeze each Dk and train
G until convergence. In the most extreme case, we say that we have mode collapse when:
For all Z 〜Pg(z), G(z0) = X	(4)
Theorem 1.	In original GANs, mode collapse fully minimizes G’s loss when we train G exhaustively
without updating D.
Proof. The optimal x* is the one that maximizes D's output, where:
X* = argmaXD(X)
x
Thus, assuming G would eventually learn how to produce X*, mode collapse on X* would fully
minimize its loss, making x* independent of z.	□
Theorem 2.	In microGAN, assuming α > 0, X 〜Pg must be dependent of Z for G tofully minimize
its loss, eliminating mode collapse when we train G exhaustively without updating any Dk.
Proof. The value function between G and each Dk can be expressed as follows:
V(Dk,G) = Ex〜pr [log Dk(x)] + Ex，〜Pg [log(1 - Dk(x0))] + α X Ex〃〜Pg [log Dk(X00)]	(5)
To fully minimize its loss in relation to Dk, G must find:
12
Under review as a conference paper at ICLR 2019
x0 = argmaxDk (x) and x00 = argminDk (x)
xx
which implies
Dk (x0) 6= Dk (x00)	=⇒ x0 6= x00
Thus, generating different outputs for different z is a requirement to fully minimize G’s loss regard-
ing each Dk. Since we sum all V (Dk, G) to calculate G’s final loss, this also applies to overall
adversarial set, concluding the proof.
□
C Training settings
The architectural and training settings used in Sections 3, 4, and 5 are presented in Tables 4, 5,
and 6, respectively. For the FID comparison on CIFAR-10 and CelebA in Section 5, we used the
same architectures as Table 6 but with a batch size of 64 on both datasets, and ran for 78K iterations
on CIFAR-10 and 125K iterations on CelebA.
D Sigmoid initial value
In Figure 8, we show and discuss the effects of using different βsigm on αsigm on the toy dataset,
giving more insights regarding the choice of βsigm = -1.8 mentioned in Section 3.
Figure 8: Analysis of self-learning αsigm with different initial values of β. The generated samples
in (a) show that using lower βsigm values lead the model to mode collapse, since only low α values
are used throughout the whole training. On the other hand, using higher values, e.g., βsigm = 0.0,
leads to a steeper increase of α values, inducing the model to only generate varied, but not realistic,
samples. We empirically found that using -2.0 ≤ βsigm ≤ -1.8 led to diverse plus realistic looking
samples from early iterations due to the mild, yet meaningful, increase ofα throughout training. The
evolution of α's values are presented in (b).
(b) α evolution.
13
Under review as a conference paper at ICLR 2019
E	Toy dataset comparis ons
Figure 9 shows how different methods compare using the above mentioned toy dataset. We com-
pared microGAN’s results (K = 8, αsigm) to the standard GAN (Goodfellow et al. (2014)), Unrol-
lledGAN (Metz et al. (2016)), D2GAN (Nguyen et al. (2017)), and MGAN (Hoang et al. (2017)).
We observe bigger sample diversity with our method, while still approximating the real data distri-
bution.
Figure 9: Method comparisons on the toy dataset.
F Extended Results
Additional results for CIFAR-10, STL-10, and ImageNet are presented bellow.
Figure 10: CIFAR-10 extended results using K = 2 and αsigm .
14
Under review as a conference paper at ICLR 2019
Figure 11: CIFAR-10 extended results using K = 2 and αsoft .
Figure 12: CIFAR-10 extended results using K = 2 and αtanh .
—图
Table 4: Training settings for the toy dataset.
	Feature maps	Nonlinearity
G(Z) : Z 〜Normal(0, I)	256	
Fully connected	128	ReLu
Fully connected	128	ReLu
Fully connected	2	Linear
D(x)	2	
Fully connected	128	ReLu
Fully connected	1	S oftplus
Number of discriminators	8	
α (STATIC)	{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}	
α (SELF-LEARNED)	{αsigm , αsof t , αtanh , αident}	
Batch size	512	
Iterations	25K	
Optimizer	ADAM (lr = 0.0002, β1 = 0.5)	
15
Under review as a conference paper at ICLR 2019
Table 5: Training settings for MNIST, CIFAR-10, and CelebA.
	Kernel	Strides	Feature maps	Batch Norm.	Nonlinearity
G(Z) : Z 〜Uniform[-1,1]	-	-	100	-	-
Transposed convolution	3×3	4×4	128	Yes	ReLu
Transposed convolution	5×5	2×2	64	Yes	ReLu
Transposed convolution	5×5	2×2	32	Yes	ReLu
Transposed convolution	5×5	2×2	1/3	NO	Tanh
D(x)	-	-	32 × 32 × 1/3	-	-
Convolution	3×3	2×2	32	Yes	Leaky ReLu (0.2)
Convolution	3×3	2×2	64	Yes	Leaky ReLu (0.2)
Convolution	3×3	2×2	128	Yes	Leaky ReLu (0.2)
Fully connected	-	-	1	NO	Sigmoid
Number of discriminators	{1, 2, 5, 10}				
α (STATIC)	{0}				
α (SELF-LEARNED)	{αsigm, αsoft, αtanh, αident}				
Batch size	100				
Iterations	50K				
Optimizer	ADAM (lr = 0.0002, β1	0.5)			
Table 6: Training settings for CIFAR-10, STL-10, and ImageNet.
	Kernel	Strides	Feature maps	Batch Norm.	Nonlinearity
G(Z) : z 〜Uniform[-1,1]	-	-	100	-	-
Transposed convolution	3×3	4×4	256	Yes	ReLu
Transposed convolution	5×5	2×2	128	Yes	ReLu
Transposed convolution	5×5	2×2	64	Yes	ReLu
Transposed convolution	5×5	2×2	1/3	NO	Tanh
D(x)	-	-	32 × 32 × 1	-	-
Convolution	3×3	2×2	64	Yes	Leaky ReLu (0.2)
Convolution	3×3	2×2	128	Yes	Leaky ReLu (0.2)
Convolution	3×3	2×2	256	Yes	Leaky ReLu (0.2)
Fully connected	-	-	1	NO	Sigmoid
Number of discriminators	{2}				
α (SELF-LEARNED)	{αsigm, αsoft, αtanh}				
Batch size	100				
Iterations	200K, 400K, 1M				
Optimizer	ADAM (lr = 0.0002, β1 = 0.5)				
Figure 13: STL-10 extended results using K = 2 and αsigm.
16
Under review as a conference paper at ICLR 2019
Figure 14: STL-10 extended results using K = 2 and αsoft.
Figure 15: STL-10 extended results using K = 2 and αtanh .
Figure 16: ImageNet extended results using K = 2 and αsigm .
17
Under review as a conference paper at ICLR 2019
Figure 17: ImageNet extended results using K = 2 and αsigm .

Figure 18: ImageNet extended results using K = 2 and αsigm .
18