Under review as a conference paper at ICLR 2019
Generalized Adaptive Moment Estimation
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive gradient methods have experienced great success in training deep neural
networks (DNNs). The basic idea of the methods is to track and properly make use
of the first and/or second moments of the gradient for model-parameter updates
over iterations for the purpose of removing the need for manual interference. In this
work, we propose a new adaptive gradient method, referred to as generalized adap-
tive moment estimation (Game). From a high level perspective, the new method
introduces two more parameters w.r.t. AMSGrad (S. J. Reddi & Kumar (2018)) and
one more parameter w.r.t. PAdam (Chen & Gu (2018)) to enlarge the parameter-
selection space for performance enhancement while reducing the memory cost per
iteration compared to AMSGrad and PAdam. The saved memory space amounts to
the number of model parameters, which is significant for large-scale DNNs. Our
motivation for introducing additional parameters in Game is to provide algorithmic
flexibility to facilitate a reduction of the performance gap between training and
validation datasets when training a DNN. Convergence analysis is provided for
applying Game to solve both convex optimization and smooth nonconvex opti-
mization. Empirical studies for training four convolutional neural networks over
MNIST and CIFAR10 show that under proper parameter selection, Game produces
promising validation performance as compared to AMSGrad and PAdam.
1	Introduction
Stochastic gradient descent (SGD) and its variants have become the mainstream training methods
in machine learning (ML) due to its simplicity and effectiveness. In general, SGD is known to
work reasonably well regardless of their problem structure if the learning rate is set properly in a
dynamical manner over training iterations. Intuitively speaking, if optimization problems admit
certain structural properties (e.g., gradients magnitudes not balanced across the parameter set),
advanced gradient descent methods exploiting the structural properties would be likely to boost
optimization performance. In 2011, Duchi et al. firstly proposed to track the second moment of
gradients and then scale each gradient coordinate using the tracked information before updating the
model parameters, which is referred to as AdaGrad (Duchi et al. (2011)). From a conceptual point of
view, AdaGrad adaptively adjusts “individual learning rates” for all the model parameters, allowing
for the parameters to be updated on a roughly equal scale. It is found that AdaGrad converges
significantly faster than SGD when the gradients are sparse. Its performance deteriorates when the
gradients are dense due to a rapid decay of the learning rates.
Since the pioneering work of AdaGrad, various adaptive gradient methods have been proposed on
tracking and using the first and/or second moment of the gradients for effective parameter updates.
The methods include, for example, RMSProp (Tieleman & Hinton (2012)), Adam (Kingma & Ba
(2017)), and NAdam (Dozat (2016)). The main difference of the above methods from AdaGrad is
that the first and/or second moment of the gradients are tracked via exponential moving averaging to
enhance the importance of the most recent gradients.
Despite the wide usage of Adam in ML community, Reddit et al. have recently shown that the
method does not even converge for a class of specially constructed convex functions (S. J. Reddi &
Kumar (2018)) due to the non-monotonicity of the “individual learning rates” being multiplied to the
gradients. To fix the convergence issue of Adam, the authors proposed a so-called AMSGrad method
by additionally tracking the maximum value of the second moment of gradients over iterations (i.e.,
vector V of Alg. 1 in Table 1). Later on, Chen and Guo generalized AMSGrad by introducing one
free parameter (i.e., p of Alg. 2 in Table 1), referred to as PAdam (Chen & Gu (2018)). The authors’
1
Under review as a conference paper at ICLR 2019
Table 1: Comparison of three adaptive gradient methods
Alg.1: AMSGrad		Alg. 2: PAdam	Alg. 3: Game
Input: xι, {αt}, {βιt}, β2 Init.: mo — 0, Vo — 0, vo — 0 for t = 1 to T do gt = Vxf (xt; ξt) mt = β1tmt-1 + (1 — βιt)gt Vt = β2vt-1 + (1 — β2)g2 Vt = max(vt, Vt-ι) -1/2 xt+ι — Xt — atVt / mt end for	Input: xι, {αt}, {βιt}, β2 Init.: mo — 0, Vo — 0, Vo — 0 for t = 1 to T do gt = Vxf (xt； ξt) mt = β1tmt-1 + (1 — βιt)gt Vt = β2 Vt-1 + (1 — β2)gt Vt = max(Vt, Vt-1) -p xt+1 J Xt — at Vt Pmt end for	Input: xι, {αt}, {βιt}, β2 Init.: mo — 0, Vo — 0 for t = 1 to T do gt = Vxf (xt； ξt) mt = β1tmt-1 + (1 — βιt)gt Vt = β2Vt-1 + (1 — β2)∣gt∣q Vt = max(Vt, Vt-1) -p xt+1 J xt — at Vt Pmt end for
Comparison of analysis results for smooth nonconvex optimization		
	(Zhou etal. (2018)): O (T + √t ) ； β < β2 ,p = 1	
	new results : O (T + √T) When P =1/4	new results : O (T + √T) When Pq = 1/2
Notations: Vt = diag(Vt); GT = E (Pd=II∣gLT,i∣∣2); d : dimension of X		
primary motivation is to improve the generalization performance by tuning the parameter. Both
AMSGrad and PAdam require additional memory in comparison to Adam for storing V.
While machine learning has seen rapid advances in algorithm development, theoretical convergence
analysis has also made remarkable progress recently. The work of (Zhou et al. (2018)) extends the
original analysis for PAdam in (Chen & Gu (2018)) from convex optimization to smooth nonconvex
optimization. In another recent article (Chen et al. (2018)), the authors also considered smooth
nonconvex optimization and provided convergence analysis for a class of Adam-related algorithms
including AMSGrad and Adam. The convergence bounds developed in the above two articles differ
in their analysis approach and step-size selection. From a high level point of view, analysis on
nonconvex optimization is highly valuable in practice as training deep neural networks (DNNs) is
well known to be a nonconvex optimization problem. An improved convergence analysis on existing
algorithms would provide insights on designing more advanced adaptive gradient methods.
In this paper, we make two main contributions. Firstly, we propose a new adaptive gradient method
named generalized adaptive moment estimation (Game). As shown in Table 1, Game tracks only
two variables (m, V) 1 over iterations as compared to AMSGrad and PAdam, which track three
variables (m, v, V). We emphasize that since the dimension of v is the same as the number of model
parameters, the memory saved by Game can be remarkable for modern large-scale neural networks,
which usually have millions of parameters. Furthermore, Game introduces two parameters (i.e., (p, q)
of Alg. 3 in Table 1) in comparison to PAdam which introduces one parameter (i.e., p of Alg. 2 in
Table 1) to further enhance its flexibility. The parameter q enables Game to track information of the
qth moment of gradient magnitude rather than only the second moment of the gradients. By doing so,
the parameter provides one more degree of freedom for the method to balance the tradeoff between
convergence speed on training data and generalization ability on validation data.
Secondly, in our theoretical convergence analysis for Game, we manage to remove the condition on
the relationship between β1 and β2 while β1 ≤ β2 and β1 ≤ β2p are required in (S. J. Reddi & Kumar
(2018)) for AMSGrad and (Zhou et al. (2018)) for PAdam, respectively. We provide convergence
analysis for both convex optimization and smooth nonconvex optimization. The results for nonconvex
case are briefly summarized in Table 1 with the analysis results from (Zhou et al. (2018)) as a
reference. Finally, our experimental results on training four convolutional neural networks (CNNs)
for MNIST and CIFAR10 suggest that with a proper setup of (p, q), Game produces promising
validation performance in comparison to AMSGrad and PAdam.
2	Problem Setup
Before introducing the problem, we describe the notation used in the paper, which are basically in
line with that of (S. J. Reddi & Kumar (2018)) and (Chen & Gu (2018)) for consistency. We denote
1 v is not tracked but just computed and used at each iteration.
2
Under review as a conference paper at ICLR 2019
scalars by lower-case letters, vectors by bold lower-case letters, and matrices by bold upper-case
letters. Following convention, we denote the lp (p ≥ 1) norm of a vector x ∈ Rd by kxkp =
(Pid=1 |xi|p)1/p. As p → ∞, the l∞ norm takes a special form kxk∞ = maxid=1 |xi|. We let
gi:t,i = [gι,i, g2,i,…，gt,i], where gt,i is the ith element of a vector gt ∈ Rd. We use E[∙] to denote
the expectation operation. Given two sequences {at} and {bt}, the notation at = O(bt) indicates
that the magnitude of at is proportional to that of bt for t ≥ 0.
Formally, we reconsider solving the stochastic smooth nonconvex optimization studied in (Ghadimi
& Lan (2013; 2016); Zhou et al. (2018))
mRd f (X)=Eξ f(X； ξ)],
(1)
where X represents the parameters of a model in a vector form and ξ is a random variable. Due to
randomness of ξ, one can only obtain an unbiased noisy gradient Vf (x; ξ), satisfying Vf (x)=
Eξ[Vf (x; ξ)]. In practice, the variable ξ often represents the random mini-batch state. For the above
situation, we denote the function realization at iteration t as ft(X; ξt), where ξt is a realization of ξ.
Solving (1) is asymptotically equivalent to addressing the following optimization
T
lim min X ft (x; ξt).	(2)
T →∞ x∈Rd
We will consider both the two formulations (1) and (2) in the reminder of the paper.
3	Generalized Adaptive moment estimation (Game)
In this section, we present our new adaptive gradient method for solving (1). Our algorithm Game is
motivated by making two observations about AMSGrad and PAdam in Table 1. Firstly, both the two
methods have to track the parameter V in addition to v. In particular, the parameter Vt at iteration t is
computed as
Vt = β2Vt-1 + (1 - β2)gt2	(3)
Vt = max(vt, Vt-ι).	(4)
It is seen from (3)-(4) that Vt is a function of the squared gradients {gj2}1j=1 while Vt tracks the
maximum values of {Vj}tj=1 up to iteration t. From a high level perspective, it feels redundant to
keep both Vt and Vt in AMSGrad and PAdam as both parameters are related to the second moment
of the gradients. One natural question is if it is sufficient to keep only one parameter about the
second moment of the gradients without sacrificing convergence speed. Less memory usage is always
desirable in designing an algorithm for simplicity and applicability.
ʌ -1
Secondly, it is clear from Table 1 that PAdam is designed by replacing the diagonal matrix Vt 2 with
Vt-P at iteration t in AMSGrad, where P ∈ [0,11 ]. One can easily show that P = 0 corresponds to
SGD while P = 1 leads to AMSGrad. The parameter P establishes a smooth connection between
SGD and AMSGrad, allowing the resulting method PAdam to carry the advantages of both methods.
The empirical study in (Chen & Gu (2018)) suggests that when P is properly chosen in the range
[0,1 ], PAdam shows better generalization performance than AMSGrad, which is as expected due to
the fact SGD usually produces good generalization performance. To summarize, the above technique
of introducing an additional parameter to an existing algorithm adds one more degree of freedom
to enhance its applicability. One can find many examples in applied mathematics that use similar
techniques for knowledge expansion. For instance, the extension from l2 norm to lp norm and
generalization from Cauchy-Schwarz inequality to Holder,s inequality.
Based on the above two observations, we propose a new adaptive gradient method named Game as
summarized in Table 1. Basically, the new method only tracks two parameters (m, V), where Vt at
iteration t is computed as
Vt = β1Vt-ι + (1 — β1)∣gt∣q	(5)
Vt = max(Vt, Vt-ι),	(6)
where q > 0 is our newly introduced parameter and |gt|q denotes element-wise qth power of |gt|. It is
clear from (5)-(6) that Vt is only a temporary parameter at iteration t and is not required to be stored
3
Under review as a conference paper at ICLR 2019
in the memory for next iteration. Therefore, it is safe to say that Game saves roughly 33 percent
memory per iteration as compared to AMSGrad and PAdam by removing the need for tracking {vt}.
We now study the impact of saved memory by Game when training large-scale neural networks. Note
that the dimension of v is actually the number of model parameters. That is, Game manages to save a
memory space which is equivalent to the size of the neural network to be trained. It is known that
state-of-the-art neural networks for challenging tasks (e.g., ImageNet competition (Russakovsky et al.
(2015))) tend to be extremely deep and consist of millions of parameters (Simonyan & Zisserman
(2016)). When applying Game to train those large-scale networks, the memory saved by the new
training method becomes remarkable with AMSGrad as a reference.
Next We let q = 2 and P = 2 for Game to make a fair comparison w.r.t. AMSGrad . In this case,
one can easily show that Vt in (6) is either greater than or equal to the one in (4). As a result, Game
ʌ -1
would always have either equal or smaller effective learning rate at V 2 than AMSGrad. The above
property implies that Game is more conservative than AMSGrad and PAdam due to the update
reformulation (5)-(6). If needed, one can adjust the parameters {αt} to larger values to make Game
more aggressive.
Finally we consider the new scalar parameter q introduced in (5). From an algebraic point of view,
introduction of parameter q allows V in (5)-(6) to track information of the qth moment of the gradient
magnitude over iterations. As q decreases, small gradients would be amplified while large gradients
would be suppressed, leading to a decreasing dynamic range of V. When q → 0, it is not difficult to
show that Game approaches SGD. Therefore, the parameter q has a similar effect as the parameter p
of PAdam. We point out that when β2 > 0, we cannot merge p and q into one parameter via certain
reformulation of the updating expressions of Game. That is, p and q play different roles in Game.
Our purpose of introducing q in addition to p is to enlarge the parameter-selection space of Game and
improve generalization performance with proper parameter setup.
4	Convergence Analysis for Game
In this section, we analyze the convergence of Game for both convex optimization and smooth
nonconvex optimization. The cost regret will be studied for the convex case while gradient expectation
will be considered for the nonconvex case, which represents two different analysis approaches.
4.1	Analysis for convex optimization
Formally, we define the cost regret up to iteration T as
T
RT = X [ft(χt; ξt) - ft(χT; ξt)],	⑺
t=1
where XT = arg min PT=I ft(x) denotes the optimal solution within the iteration range [1,T], and
Xt is a causal estimate of XT at iteration t as obtained by following Game in Table 1. Our objective
is to derive an upper bound of RT and then quantify its convergence behaviour as T increases.
Next we present our convergence analysis:
Lemma 1. Let (βι, β2) ∈ [0,1), βιt ≤ βι, pq < 2 andp, q > 0, and at = α∕√7. Then the quantity
PT=I a/|H-p/2mtk2]
of Game is upper bounded by
T	d——I——F	d / T	∖ 1
Xat hkVt-p12mtk2i ≤ (1 a√1+^2)p X j>∣2(2-pq))	.	(8)
Proof. See Appendix A for proof.
□
Remark 1. We emphasize that there is a major difference between the upper bound expression in (8)
and those derived in (S. J. Reddi & Kumar (2018)) and (Chen & Gu (2018)) for analyzing AMSGrad
and PAdam. That is the new upper bound does not put any restriction on the relationship between β1
and β2 for (8) to hold while in the above two articles, it is required that β1 ≤ β22p, where p is the
parameter of PAdam.
4
Under review as a conference paper at ICLR 2019
Theorem 1 (convex). Suppose {ft} are close, proper and convex functions (Sawaragi et al. (1985)).
Let β1,β2 ∈ [0,1), βιt ≤ βι, pq < 2 and p,q > 0, and ɑt = ɑ/ʌ/t. Assume ∣∣Vft(x; ξt)k∞ ≤
G∞ for all t ≤ T and distance between any Xt generated by Game and XT is bounded, i.e.,
Ilxm — XT k∞ ≤ D∞. Then we have the following bound on the regret
R < D∞ √T XX 货 + D∞ X XX β1tvp,i
RT ≤ g-而 ⅛ vT,i + 2(1—11)t=ι N F
α √1 + log T
+ (1 — βι)3(1 — β2)p
d / T	γ∕[
∑(∑ ∣gj产-Pq))
i=1	j=1
(9)
Based on Lemma 1, one can easily derive the upper bound expression (9) by following the derivation
steps for Theorem 4 in (S. J. Reddi & Kumar (2018)). When pq = 1 and p = 1/2, (9) can be
simplified as
≤ D∞ √T XX V1/2 +	D∞ XX XX β1tvP,i + α√1 + log T
≤ α(1- βι) = vT,i + 2(1— βι) = = Fr+(1— βι)3(√1-β
d
X kgLT,i∣2 .(10)
i=1
One can see that no constraint is imposed between the relationship of β1 and β2 for the upper bound
to hold. We have conducted empirical studies and found that both AMSGrad and Game converge
even when β2 < β1 .
4.2	Analysis for smooth nonconvex optimization
Differently from the analysis for convex optimization, we will consider gradient expectation for
nonconvex case. To simplify study later on, we choose the output Xout from {Xt}tT=2 with probability
αt-ι/(PT-11 aj), which is in line with the definition in (Zhou et al. (2018)). In practice, it is natural
to take the most recent estimate XT at last iteration T as output.
We now introduce the L-smooth assumption needed for analysis:
Assumption 1 (L-smooth). f(X) = Eξf(X; ξ) is L-smooth: for any X, y ∈ Rd, we have
If(x) — f(y) + hVf(y), X — yil ≤ LL∣χ - y∣2.	(11)
Furthermore, f (x) is lower bounded, i.e., infX f (x) > —∞.
The above smooth assumption is standard for nonconvex optimization. It essentially requires the
object function f changes smoothly in the parameter space. See (Zhou et al. (2018); Chen et al.
(2018)) for employing the assumption in their analysis.
We now present our convergence analysis in two steps. Firstly, we provide upper bounds for the two
quantities PT=I α2E IjH-Pmtk2] and PT=I α2E IjH-Pgtk2]
in a lemma below. We then show
the main result in a theorem, which are derived based on the lemma.
Lemma 2. Let β1, β2 ∈ [0, 1), pq ≤ 1 and p, q > 0, the step sizes αj ≤ αj-1 for all j > 1, and
r ∈ [2pq — 1, 1]. Assuming kVft(X; ξt)k∞ ≤ G∞ for all t ≤ T, we then have
T
Xɑ2E [kVζ-pmtk2]
t=1
α1G∞+r-'22j,q')T (1+r)/2
(1 — β2 )2p
d
X (kg1:T,ik2)(1-r)
i=1
T
Xɑ2E [kVζ-pgtk2]
t=1
≤ (IMq2pE X (k∣gι"2)2(1-pq)
(12)
(13)
≤
Proof. See Appendix B for proof.
□
5
Under review as a conference paper at ICLR 2019
We note that a scalar parameter r ∈ [2pq - 1, 1] is introduced in the upper bound expression of
Lemma 2. As will be discussed in Corollary 1 later on, the parameter r provides a freedom to tune
the expression (12) and merge with other expressions for simplicity.
Remark 2. Again the two upper bounds (12)-(13) do not require any constraint on the relationship
of β1 and β2, which is consistent with the results in Lemma 1.
Theorem 2 (nonconvex). Let β1, β2 ∈ [0, 1), pq < 1 and p, q > 0, the step sizes αt = α1 for all
t > 1, and r ∈ [2pq — 1,1]. Assume ∣∣Vft(x; ξt)k∞ ≤ G∞ for all t ≤ T and Assumption 1 holds.
The output xout of Game satisfies
M4T(I+r)/2
-T — 1 ^^
E [∣Vf (Xout)k2] ≤TM-T + TM—d + MM3TpqE (XX (kgrτ,ik2)2(1-pq)
E (XX (kgi：T,ik2)j)	T ≥ 2,	(14)
where
Mi = G∞ ∆f∕αι
G∞+pq ∣V-pkι +	G∞
d(1 — βι)	+(1 — β2)P
M3
2LG∞ αι
(1 —户2产
M4
4LG∞+r-pqαι ( βι Y
(1 — β2)2p	[1-βι )
where ∆f = f(x1) — infx f (x).
Proof. See Appendix C for proof.
(15)
(16)
(17)
(18)
□
It is clear from Theorem 2 that the two parameters p and q have to satisfy certain conditions. That is
p, q > 0 and pq < 1. The conditions suggest if one parameter is chosen large, the other one should
be set small to avoid divergence. In practice, it is found that Game also converges when pq = 1. This
leaves us an open question on how to elaborate the convergence analysis to cover the special case of
pq = 1 for Game.
We now study the upper bound on the right hand side of (14). The expression includes four quantities,
where the first two quantities are independent of the gradients {g1:T,i|i = 1, . . . , d} while the last
two are contributed by the inequalities derived in Lemma 2. All the four scalar parameters Mi ,
i = 1, . . . , 4, are independent of iteration number T. The convergence of Game can be established if
one can show that the upper bound expression approaches to zero as T increases.
In the following, we simplify the upper bound expression in (14) by specifying pq and r with
particular values and then study its convergence behaviour.
Corollary 1. Let Pq = 1 and r = 2pq — 1 in Theorem 2, the upper bound of E [kVf(Xout)∣2] then
takes the form of
E [∣Vf (xout)k2]	≤	MI	+ = +(M3	+M4)α1 1√E	(XX kgi：T,ik2)	T ≥2.	(19)
2	(T— 1)α1 T— 1	T— 1	i=1
As summarized in Table 1, It is immediate that the upper bound in (19) is proportional to O(d∕T +
GT∕√T), where GT = E (Pd=I ||gi：T,i||2). As reflected in Table 1, the upper bound also holds
for PAdam even though the iteration procedure (3)-(4) of PAdam is different from (5)-(6) of Game.
We are now in a position to study under what conditions Game would converge. It is not difficult
to conclude that when GT is of order GT = O((dT)s) where s < 1, (19) tends to converge with
6
Under review as a conference paper at ICLR 2019
ssol gniniar
0	20	40	60	80
epochs
epochs
(a)	Performance over MNIST using SERLU
epochs	epochs
(b)	Performance over CIFAR10 using SERLU
epochs	epochs
(c)	Performance over CIFAR10 using ELU
epochs
epochs
(d)	Performance over CIFAR10 using ReLU
Figure 1: Performance comparison of PAdam, AMSGrad, and Game for training four CNNs (see
Table 2-5 for the detailed CNN architectures in Appendix D).
the speed O(d∕T + ds∕T1/2-s). We note that in DUchi et al. (2011)), the assumption GT《√dT
was used for analyzing AdaGrad, which was later on employed for convergence analysis of other
adaptive gradient methods (see (Zhou et al. (2018)) and (S. J. Reddi & Kumar (2018)) for example).
The assumption GT = O((dT)s) is slightly stronger than GT《√dT as in practice, the dimension
d could be larger than T when training large-scale neural networks.
7
Under review as a conference paper at ICLR 2019
5	Experimental results
In the experiment, we evaluated the effectiveness of AMSGrad, PAdam and Game for training
convolutional neural networks (CNNs). The classification problems on the two datasets MNIST and
CIFAR10 were considered. To alleviate overfitting, each of the two datasets was augmented with
additional training data (e.g., shifting images vertically and/or horizontally, and image flipping for
CIFAR10). In brief, we tested four CNNs: the 1st one consists of four layers for MNIST using the
activation function SERLU (Zhang & Li (2018)), the 2nd consists of 8 layers for CIFAR10 using
SERLU, and the 3rd and 4th have the same structure as the 2nd one but using ELU (Clevert et al.
(2016)) and ReLU (Nair & Hinton (2010)), respectively. More detailed information of the four CNNs
can be found in Table 2-5 of Appendix D.
We first consider the CNN training for MNIST using SERLU. The shared parameters among the
three methods include (αt, β1, β2) = (0.001, 0.9, 0.999), which was recommended in (Kingma &
Ba (2017)) for Adam. On the other hand, the special parameters include p = 0.125 for PAdam2 and
(p, q) = (0.5, 1) for Game. The setup (p, q) = (0.5, 1) was found empirically to be more effective
than other tested values. It is worth noting that q = 1 indicates that Game tracks information of the
gradient magnitude rather than the second moment of gradients.
Next we study the CNN training for CIFAR10 using SERLU, ELU and ReLU. As the three neural
networks (see Table 3-5) are deeper than that for MNIST, we employed shift-dropout for SERLU
and dropout for ELU and ReLU to combat overfitting, respectively. The parameter setup is slightly
different from the one for MNIST. In particular, the shared parameters among the three methods
include (β1, β2) = (0.9, 0.999) and dropout rate of 0.2. Special parameters include αt = 0.0001 for
AMSGrad, 3 (αt,p) = (0.001, 0.125) for PAdam, and (αt, p, q) = (0.001, 0.5, 1) for Game.
The convergence results for training the four CNNs are demonstrated in Figure 1. It is seen that Game
produces better generalization performance (on validation datasets) than AMSGrad in subplot (a)
and (c) - (d) while in the same three subplots, AMSGrad exhibits the fastest convergence speed
over training data. The two methods have roughly the same convergence performance in subplot
(b), suggesting that selection of activation functions also affects the convergence behaviours. It is
observed that SERLU performs slightly better than ELU and ReLU in terms of validation loss. In all
the four subplots, PAdam converges slightly slower due to the fact the a small value p = 0.125 is
selected as compared to p = 0.5 of AMSGrad.
To briefly summarize, the above convergence results suggest that Game produces promising gen-
eralization performance in comparison to AMSGrad and PAdam on validation datasets. The nice
convergence behaviour of Game might be because the method tracks the 1st moment of gradient
magnitude by setting q = 1. As a result, Game achieves better validation performance by slightly
sacrificing convergence speed on training dataset. If one also takes into account of memory usage (see
Table 1), it is clear that Game is simpler to implement and requires less memory resource, rendering
Game an advantage over AMSGrad and PAdam.
6	Conclusions and Future works
In this paper, we have proposed a new adaptive gradient method termed as Game. The new method
only needs to track two parameters (m, V) in comparison to AMSGrad and PAdam, which track three
parameters (m, v, V),thus requiring only two-thirds of memory w.r.t. the two methods. The saved
memory scales along with the number of model parameters, which becomes significant when training
large-scale neural networks. Furthermore, Game introduces one additional parameter q, allowing
the method to track information of the qth moment of the gradient magnitude, while AMSGrad and
PAdam only consider tracking the 2nd moment of gradients. The freedom of tuning parameter q
makes Game more flexible. Theoretical convergence analysis is provided for applying Game to solve
both convex and smooth nonconvex optimization. Experimental results on MNIST and CIFAR10
demonstrate that Game produces promising generalization performance in comparison to AMSGrad
and PAdam. Given that Game demands only two-thirds of memory in implementation, we conclude
that the new method is a promising candidate for training large-scale neural networks.
2This parameter value p = 0.125 was suggested by the authors of (Chen & Gu (2018)).
3It is found that αt = 0.001 is not stable when applying AMSGrad to train the CNNs in this case.
8
Under review as a conference paper at ICLR 2019
References
J. Chen and Q. Gu. Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural
Netwoks. arXiv preprint arXiv:1806.0676v1, June 2018.
X. Chen, S. Liu, R. Sun, and M. Hong. On the Convergence of A Class of Adam-Type Algorithms for
Non-Convex Optimization. arXiv:1808.02941v1 [cs. LG], 2018.
D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and Accurate Deep Network Learning by Exponential
Linear Units (ELUs). arXiv:1511.07289v5 [cs.LG], 2016.
T. Dozat. Incorporating Nesterov Momentum into Adam. In International conference on Learning Representa-
tions (ICLR), 2016.
J. Duchi, E. Hazan, and Y. Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimiza-
tion. Journal of Machine Learning Research, 12:2121-2159, 2011.
S. Ghadimi and G. Lan. Stochastic First-and Zeroth-Order Methods for Nonconvex Stochastic Programming.
SIAM Journal on Optimization, 23:2341-2368, 2013.
S. Ghadimi and G. Lan. Accelerated Gradients Methods for Nonxonvex Nonlinear and Stochastic Programming.
Mathematical Programming, 156:59-99, 2016.
D. P. Kingma and J. L. Ba. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980v9,
2017.
V. Nair and G. E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In Proceedings of
the 27th International Conference on Machine Learning,, 2010.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and F.-F. Li. Unified Convergence Analysis of Stochastic Momentum Methods for Convex and
Non-Convex Optimization. arXiv:1409.0575v3, 2015.
S.	Kale S. J. Reddi and S. Kumar. On The Convergence of Adam and Beyond. In International conference on
Learning Representations (ICLR), 2018.
Y. Sawaragi, H. Nakayama, and T. Tanino. Theory of Multiobjective Optimization. Elsevier Science, 1985.
K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In
International conference on Learning Representations (ICLR), 2016.
T.	Tieleman and G. Hinton. Lecture 6.5-RMSProp: Divide The Gradient by a Running Average of Its Recent
Magnitude. COURSERA: Neural networks for machine learning, 2012.
T.	Yang, Q. Lin, and Z. Li. Unified Convergence Analysis of Stochastic Momentum Methods for Convex and
Non-Convex Optimization. arXiv:1604.03257, 2016.
G. Zhang and H. Li. Effectiveness of Scaled Exponentially-Regularized Linear Units (SERLUs).
arXiv:1807.10117 [cs.LG], July 2018.
D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. On the Convergence of Adaptive Gradient Methods for Nonconvex
Optimization. arXiv:1808.05671v1 [cs. LG], 2018.
9
Under review as a conference paper at ICLR 2019
APPENDIX
A Proof of Lemma 1
Before formally presenting the proof, we first introduce a lemma below:
Lemma 3. Let Vt be defined as in Game and q > 0. Then each component Vt,i is lower bounded by
Vt,i ≥ (1 - β2)∣gt,i∣q, i = 1,...,d.
Proof. The proof follows directly from the computation Vt,i = max(Vt-ι,i, β2Vt-1,i + (l-β2)∣gt,i∣q)
as summarized in Table 1 for Game.	□
With Lemma 3, we are ready to describe the proof for Lemma 1. Firstly, the quantity
αtE [kV⅛-p/2mtk22 can be upper bounded by
at [∣H-P/2mtk2]
≤ at 归 + (Xj="J
(S at "XX VP； (W/) (Xj="gj,i∣2)
d
≤) τ⅛ [∑ v⅛ (Xj=MjMi2I
≤) !¾" X j N"
i=1 j=1 j,i
(e)	d t
≤ (i-β1ai-β2)P [∑Xengj,i”]	(20)
where step (a) uses β1t ≤ β1 < 1, step (b) uses Cauchy-Schwarz inequality, step (c) uses
Pj=ι βt-j ≤ 1/(1 - βι), step (d) uses Vpi ≥ Vpi for all j ≤ t, and step (e) uses the results
in Lemma 3.
Next taking the summation of (20) over t = 1 to t = T produces
T
X αth∣M-p∕2mtk2i
t=1
1
一(I- βI)(I- β2)p
(a)	a
一(I- βI)(I- β
dT	T
≤∏→1⅛ XX jj,i"Xβt-j
Tdt
XXX atβt-j∣gj,i∣"
t=1 i=1 j=1
-d T	T H-j
F XX|gj，i1"X看
10
Under review as a conference paper at ICLR 2019
(b	ɑ
≤ (1- βι)2(1- β2)p
(C)	α
≤ (1- βι)2(1- β2)p
Id	α√1 + log T
≤ (1- βι)2(1- β2)p
(2-Pq)
d T
XX M |2(2-Pq)
i=1 ∖j=1
dT
XX Ml2(2-Pq)
i=1 ∖j=1
1/2 / T Λ1/2]
(Xj H
1/2
(21)
where step (a) uses αt = α∕√t, step (b) uses PT=j∙ βt-j < 1/(1 - β1), step (c) follows from
Cauchy-Schwarz inequality, and step (d) uses PT=I 1/t ≤ (1 + log T). The proof is complete.
B Proof of Lemma 2
We first consider the summation PT=I α2E IjH-Pmtk2]. Each quantity α2E IjH-Pmtk2] can
be upper bounded by
α2E [陀-Pmtk2]
)(Xjj-jM∣(1-r+2Pq))
(c)	,
≤ α2(1- β1)G∞+r-2Pq)E
j=“1”1(I-r+2Pq)
[1βt-jG∞+r-2Pq))(Xj=1βt-jl”i∣(1-r+2Pq))
(d)
≤ α2(1 - β1)G∞+r-2Pq)E
(≤) α2(1-β1)G∞+r-2Pq)
(1 - β2)2p
dt
XX i βt-j∣ j I (I-r+2Pq)
i=1 j=1 vj,i
dt
XX βt-jl j I (If)
i=1j=1
(22)
E
where step (a) uses Cauchy-Schwarz inequality and the property that 2pq - 1 ≤ r ≤ 1 and pq ≤ 1,
step (b) uses Igj,i∣ ≤ ∣∣gj∙∣∣∞ ≤ G∞, step (c) uses Pj=I βt-j ≤ 1/(1 -β1), step (d) uses 错 ≥ 号P
for all j ≤ t, and step (e) uses the results in Lemma 3.
Taking the summation of (22) over t = 1 to t = T produces
T
Xα2E [kVζ-Pmtk2]
t=1
≤
(1 - β1)G∞+r-2Pq)
(1 - β2)2P
T d t
XXX ɑ2βt-jMl(If)
t=1i=1j=1
(23)
E
11
Under review as a conference paper at ICLR 2019
≤) α2(1- βι)G∞+r-2Pq) E
一 (1- β2)2p
dT	T
X X |gj,i|(1-r) X β1t-j
dT
XX|gj,i|(1-r)
i=1 j=1
dT
X(X Ml2
i=1	j=1
(1-r)/2
T (1+r)/2
(b α2G∞+r-2Pq) E
―	(1 — β2)2p
(C) α2G∞+r-2Pq) E
一(1 — β2)2P E
d
X (kg1:T,ik2)(1-r)
i=1
where step (a) uses the property that aj ≤ aj-1 for all j > 1, step (b) uses PtT=j β1t-j < 1/(1 — β1),
and step (c) follows from Holder’s inequality. The proof is complete for (12).
The quantity PT=I α2E IjH-Pgt k22 in (13) can be upper bounded as
T
Xɑ2Ε [kV>t-pgtk2]
t=1
T	d2
=X ɑ2Ε X T
t=1	i=1 vt,i
g2,i
(T-WpCTpq
dT
XXIgj,i∣2-2pq
≤) (τ-‰ EMaTpq
=(TS⅛ ε "X (ku? )2(IT
where step (a) uses the property that aj ≤ aj-1 for all j > 1 and the results in Lemma 3, step (b)
uses pq < 1, p, q > 0, and step (c) — (d) follow from Holder’s inequality. The proof is complete for
(13).
(a)
≤
(b)
t=1
α21E
i=1
α12
(1 — β2 )2P
T
d
E
C Proof of Theorem 2
In brief, we use the technique of parameter transformation proposed in (Yang et al. (2016)) to study
Game for solving stochastic nonconvex optimization. In particular, we let x0 = x1 and for each
t≥1
~、 βι	、	1	βι
Zt = xt + T-H(Xt- XtT) =『Xt-『xt-1,
(24)
where z1 = X1 for the special case t = 1. We note that the technique has also been employed in
(Zhou et al. (2018)) and (Chen et al. (2018)) for analyzing PAdam and a class of Adam-type methods.
Instead of considering {Xj}jT=0 directly, we tackle {zj}jT=1 as in the literature.
In (Zhou et al. (2018)) and (Chen et al. (2018)), the authors provided a general upper bound for
f (zt+1) - f(zt), which also holds for Game. We now summarize their result in a lemma below:
12
Under review as a conference paper at ICLR 2019
Lemma 4. Suppose the sequence {zj}T=1 is as defined in (24). Then under Assumption 1, the
quantity f (zt+1) — f (Zt) is upper bounded by
f(Z2)- f(z1) ≤ — Vf(XI)TαιV-pgι + 2Lgι%-Pg1k∣	(25)
f (Zt+1) — f (Zt) ≤ - Vf(Xt)Tat-NtUgt + r⅛G∞(kαt-iV-p1k1 -kαtV-pkι)
1 - βι
+ 2LgtZ-Pgtk2+4L () E- Xt-Ik2 t ≥ 2.	(26)
∖ 1 - P1)
With Lemma 4, we are ready to present the proof for Theorem 2. Firstly, taking expectation on both
sides of (25) produces
E[f(z2) - f(z1)] ≤ E [-Vf(XI)TαιV-pgι + 2L∣∣aι%-pg1k2]
≤ E [da1kVf(x1)k∞ ∙kV-Pgιk∞ +2LkaM-Pgιk2]
≤ E [「二、P kVf(xι)k∞ ∙k∣g∣1-pqk∞ + 2L∣∣aM-pgιk2
L(I - β2)p	」
≤ E [T1": + 2L∣∣aM-pgιk2] ,	(27)
L(1 - P2)p	」
where step (a) uses ¼ = (1 - P2)∣g1∣q andpq < 1, and step (b) uses ∣∣Vft(x; ξt)k∞ ≤ G∞.
Next by rearranging terms and taking expectation in (26), we have
「	G2	. G	G2	.	、1
E f(zt+1) +	kɑtV-pkι - f(zt) +	kat-iV1k1
1 - P1	1 - P1
≤ E -Vf (xt)tat-1 学-Pgt + 2Lkatz-Pgtl∣2 +4L ( PI ) ∣∣Xt-1 - Xtk2
1 - P1
=-Vf(Xt)tat-1 学-PVf(xt)+ E 2L∣∣atZ-PgtI∣2+4L (T^Ir) gt-1 学-Pmt-Ik2
1	- P1
2
≤ -at-1kVf(Xt)k2(GPq)-1 + E 2LkatVt-Pgtk2+4L (T-Pkat-1^-Pmt-1k2 , (28)
where step (a) uses the property that E[gt] = Vf (Xt), and step (b) uses the inequality ∣∣vt-1k∞ ≤
G∞ which can be derived from ∣∣Vft(X; ξt)k∞ ≤ G∞, t ≥ 1.
Taking the summation of (27)-(28) over t = 1 to T produces
T
(GS )-1 X ɑt-1EkVf (Xt)k2
t=2
≤ E f (z1) + G∞ka1vJk1 + 加1%：^ - (f (zT +1) + G∞- ||aT V-PkI
一	" 1'T 1-β1	(1-P2)P	+1；	1 - p111 J T 111
T	/ P 、2 T
+ 2LXE UatVt-Pgtk2] +4L	T-IF	XE Uat-IZ-Pmt-1 间
t=1	∖ P" t=2
(PE Af + G∞ka1V-P k1 + da1GNq
≤ E N + -1-P — + (1- P2)P
T
+ 2LXE Uat%-Pgt间
t=1
+ 4L (ɪ-p-)2 XE [kat%-Pmtk2],
(29)
where step (a) uses the inequality Af = f (x1) - infX f (x) ≥ f (x1) - f (ZT +1) and z1 = X1.
13
Under review as a conference paper at ICLR 2019
Finally, substituting (12)-(13) into (29) produces
Ek▽/(Xout)k2
1T
—X
2 αt-1 t=2
αt-1EkVf(Xt)k2
< G∞ E Af , G∞kα1V-pk1 , dαιG∞Pq
≤ PT=2 at-1 E Γ +	—	+「2F
+
2LG∞	α2T Pq E
P=^O∑7 (1-12F
Xd (kg1:T,ik2)2(1-pq)
,4LG∞
P= αt-i
βι Y
1 - β1√
021G∞++r-r2pclqT(I+r)/2
(1 - β2)2P
Xd (kg1:T,ik2)1-r
(=G∞Af + ɪ GG∞+pqkv-pkι + dG∞
(T - 1)αι + T - 11	1 - βι	+(1- β2)P
,
TPq
T — 1
(2LG∞⅛E (X(kg1"2)2(I-Pq)
T (1+r)/2
T — 1
∖ (ɪ了 E (X (k…
Mi	M2d	M3Tpq E
T - 1 + T - 1 + T -1 E
(kg1:T,ik2)2(1-Pq)
,
+ MT^E (X (kgi：T,ik2)1-r
where step (a) uses the property αt = α1 for all t ≥ 2, and {Ml}l4=1 are given by (15)-(18). The
proof is complete.
D CNN Architectures in experiments
Remark 3. The full name of SERLU in Table 3 is so called scaled exponentially-regularized linear
unit. Furthermore, the shift-dropout in Table 3 is designed specifically for SERLU, which includes
dropout as a special case.
Table 2: CNN for MNIST using SERLU
Layer 1	conv.: 3 X 3@32 (SERLU)
Layer 2	-conv.: 3 × 3@64 (SERLU)- max-pooling
Layer 3	dense: 512 neurons (SERLU)
Layer 4	dense + Softmax
14
Under review as a conference paper at ICLR 2019
Table 3: CNN for CIFAR10 using SERLU
Layer 1	conv.: 3 X 3@32 (SERLU)
Layer 2	conv.: 3 × 3@32 (SERLU)- max-pooling shift-dropout
Layer 3	-conv.: 3 × 3@64 (SERLU)-
Layer 4	conv.: 3 × 3@64 (SERLU)- max-pooling shift-dropout
Layer 5	-conv.: 3 × 3@128 (SERLU)-
Layer 6	-conv.: 3 × 3@128 (SERLU)- max-pooling shift-dropout
Layer 7	dense: 512neurons (SERLU) shift-dropout
Layer 8	dense + Softmax
Table 4: CNN for CIFAR10 using ELU
Layer 1	conv.: 3 × 3@32 (ELU)
Layer 2	conv.: 3 × 3@32 (ELU)- max-pooling dropout
Layer 3	-conv.: 3 × 3@64 (ELU)-
Layer 4	conv.: 3 × 3@64 (ELU)- max-pooling dropout
Layer 5	-conv.: 3 × 3@128 (ELU)-
Layer 6	-conv.: 3 × 3@128 (ELU)- max-pooling dropout
Layer 7	dense: 512 neurons (ELU) dropout
Layer 8	dense + softmax
Table 5: CNN for CIFAR10 using ReLU
Layer 1	conv.: 3 × 3@32 (ReLU)
Layer 2	conv.: 3 × 3@32 (ReLU) max-pooling dropout
Layer 3	conv.: 3 × 3@64 (ReLU)~~
Layer 4	conv.: 3 × 3@64 (ReLU) max-pooling dropout
Layer 5	-conv.: 3 × 3@128 (ReLU)-
Layer 6	-conv.: 3 × 3@128 (ReLU)- max-pooling dropout
Layer 7	dense: 512neurons (ReLU) dropout
Layer 8	dense + softmax
15