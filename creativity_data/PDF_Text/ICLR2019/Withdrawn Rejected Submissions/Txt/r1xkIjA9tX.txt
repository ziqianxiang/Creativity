Under review as a conference paper at ICLR 2019
q-NEURONS:	NEURON ACTIVATIONS BASED ON
Stochastic Jackson’s Derivative Operators
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new generic type of stochastic neurons, called q-neurons, that con-
siders activation functions based on Jackson’s q-derivatives, with stochastic pa-
rameters q. Our generalization of neural network architectures with q-neurons
is shown to be both scalable and very easy to implement. We demonstrate ex-
perimentally consistently improved performances over state-of-the-art standard
activation functions, both on training and testing loss functions.
1 Introduction
The vanilla method to train a Deep Neural Network (DNN) is to use the Stochastic Gradient Descent
(SGD) method (a first-order local optimization technique). The gradient of the DNN loss function,
represented as a directed computational graph, is calculated using the efficient backpropagation
algorithm relying on the chain rule of derivatives (a particular case of automatic differentiation).
The ordinary derivative calculus can be encompassed into a more general q-calculus Jackson (1909);
Kac & Cheung (2001) by defining the Jackson’s q-derivative (and gradient) as follows:
Dqf (X) := f (XI - q：Xx , q=1,x = 0.
(1)
The q-calculus generalizes the ordinary Leibniz gradient (obtained as a limit case when q → 1 or
when x → 0) but does not enjoy a generic chain rule property. It can further be extended to the
(p, q)-derivative Sadjang (2013); Khan et al. (2018) defined as follows:
Dp,qf(x) := f (PX)- f Xqx) , P = q,x = 0.
which encompasses the q-gradient as D1,q f (X) = Dq,1f (X) = Dq f (X).
The two main advantages of q-calculus are
(2)
1.	To bypass the calculations of limits, and
2.	To consider qas a stochastic parameter.
We refer to the textbook Kac & Cheung (2001) for an in-depth explanation of q-calculus. Ap-
pendix A recalls the basic rules and properties of the generic (P, q)-calculus Khan et al. (2018) that
further generalizes the q-calculus.
To the best of our knowledge, the q-derivative operators have seldom been considered in the ma-
chine learning community XU & Nielsen (2018). We refer to GoUvea et al. (2016) for some encour-
aging preliminary experimental optimization results on global optimization tasks.
In this paper, we introdUce a meta-family of neUron activation fUnctions based on standard activation
fUnctions (e.g., sigmoid, softplUs, ReLU, ELU). We refer to them as q-activations. The q-activation
is a stochastic activation function bUilt on top of any given activation fUnction f. q-Activation
is very easy to implement based on state-of-the-art AUto-Differentiation (AD) frameworks while
consistently prodUcing better performance. Based on oUr experiments, one shoUld almost always
Use q-activation instead of its deterministic coUnterpart. In the remainder, we define q-neurons as
stochastic neUrons eqUipped with q-activations.
OUr main contribUtions are sUmmarized as follows:
1
Under review as a conference paper at ICLR 2019
•	The generic q-activation and an analysis of its basic properties.
•	An empirical study that demonstrates that the q-activation can reduce both training and
testing errors.
•	A novel connection and sound application of (stochastic) q-calculus in machine learning.
2 NEURONS WITH q-ACTIVATION FUNCTIONS
Given any activation function f : R → R, we construct its corresponding “quantum” version, also
called q-activation function, as
gq (X) := f(x)~f(qx) = (Dqf (X))(X),
1-q
(3)
where q is a real-valued random variable. To see the relationship between gq(X) and f (X), let us
observe that we have the following asymptotic properties:
Proposition 1. Assume f (X) is smooth and the expectation ofq is E(q) = 1. Then ∀X, we have
lim gq(X) = f0(X)X.
Var(q)→0
lim gq0 (X) = f 0(X) + f 00(X)X,
Var(q)→0
where E(∙) denotes the expectation, and Var(∙) denotes the variance.
Proof.
lim gq(X) = lim	f (X)- f (qx) x = lim f (x) - f (qx) X = f0(χ)χ
Var(q)→0	Var(q)→0 X - qX	qx→x X - qX
lim	gq(x) = lim	f0(X)-qfO(qX) = lim 广⑺ - "⑺ + qf0(X) - qf0(qX)
Var(q)→0 q	Var(q)→0	1 - q	Var(q)→0	1 - q
f 0(x) +	lim qf0(X)-qfO(qX) = f 0(x) + lim f0(X)-f皿)qX = f 0(x) + f3
Var(q)→0	1 - q	q→1	X -
□
Notice that as Var(q) → 0, the limit of gq(X) is not f(X) but f0(X)X. Thus informally speaking, the
gradient ofgq(X) carries second-order information of f (X). We further have the following property:
Proposition 2. We have:
Dp(gq (x)) = 1-p Dq f (x) — I-PDp,pq f (x).	(4)
Proof.
Dp (gq (X)) =	_ gq (x) - gq (PX) = (Dq f3) X - (Dq f (PX)) PX	
	(1 - p)X	(1 - p)X 1P =1-ρDqf (x) - 1-ρDqf (PX)	(5)
Since Dqf (px) = f (Ppx-f(XqX) = f(Pp)-Pf(IPqx) = Dp,pqf (x), eq. (4) is straightforward.	□
By proposition 2, the p-derivative of the q-activation gq(X) agrees with the original activation func-
tion f.
See table 1 for a list of activation functions with their corresponding functions f0(X)X, where
sigm(X) = 1/(1 + exp(-X)) is the sigmoid function, softplus(X) = log(1 + exp(X)) is the softplus
function,
relu(X)
if X ≥ 0
otherwise
2
Under review as a conference paper at ICLR 2019
Table 1: Common activation functions f (x) with their corresponding limit cases
limVar(q)→0 gq (x) = f0(x)x.
f(x)	Sigm(X)	tanh(x)	relu(x)	softplus(x)	elu(x)
f0(x)x	sigm(x)(1 - sigm(x))x	sech2(x)x	relu(x)	sigm(x)x	X x	x ≥ 0 [a exp(x)x x < 0
Figure 1: The probability density function of stochastic variable q used when calculating q-
derivatives.
is the Rectified Linear Unit (ReLU) Maas et al. (2013), and
elu(x) =	α(exp(x) - 1)
if x ≥ 0
otherwise
denotes the Exponential Linear Unit (ELU) Clevert et al. (2016).
A common choice for the random variable q that is used in our experiments is
q = 1 + (2[e ≥ 0] - 1) (λl∈l + φ),
(6)
where e 〜 N(0, 1) follows the standard Gaussian distribution, [∙] denotes the Iverson bracket (mean-
ing 1 if the proposition is satisfied, and 0 otherwise), λ > 0 is a scale parameter of q, and φ = 10-3
is the smallest absolute value of q so as to avoid division by zero. See fig. 1 for the density function
plots of q defined on (-∞, -φ] ∪ [φ, ∞]
To implement q-neurons, one only need to tune the hyper-parameter λ. It can either be fixed to a
small value, e.g. 0.02 or 0.05 during learning, or be annealed from an initial value λ0 . Such an
annealing scheme can be set to
λ
λ0
1 + Y(T — 1),
(7)
where T = 1,2,… is the index of the current epoch, and Y is a decaying rate parameter. This
parameter γ can be empirically fixed based on the total number of epochs: For example, in our
experiments, we train 100 epochs and apply Y = 0.5, so that in the final epochs λ is a small value
(around 0.02λ0). We will investigate both of those two cases in our experiments.
Let us stress out that deep learning architectures based on stochastic q-neurons are scalable and
easy to implement. There is no additional free parameter imposed. The computational overhead
of gq(x) as compared to f(x) involves sampling one Gaussian random variable, and then calling
f(x) two times and computing gq(x) according to eq. (3). In our Python implementation, the core
implementation of q-neuron is only in three lines of codes (see A.3).
Alternative approaches to inject stochasticity into neural network training include dropout Srivastava
et al. (2014), gradient noise Neelakantan et al. (2016), etc. Both q-neuron and dropout modify the
forward pass of the neural network. In the experimental section, we will investigate the effect of
q-neurons with or without dropout. q-neuron contracts from the broad array of heristic-based DNN
ingredients in that it is based on q-calculus, which combines stochasticity and some second-order
information in an easy-to-compute way.
3
Under review as a conference paper at ICLR 2019
sulptfosq
hnatq
∙N
ulerq
∙N
uleq 2- 2
N'N'N'N
∙N
∙
N∙N
-3	x
3	-3	x
3	-3	x
Figure 2:	The density function of q-neurons with q sampled according to eq. (6) for different values
of λ. The activation is roughly a deterministic function f0(x)x for small λ as shown in table 1. The
activation is random for large λ. Darker color indicates higher probability density.
3	Experiments
We carried experiments on classifying MNIST digits1 and CIFAR10 images2 using Convolutional
Neural Networks (CNNs) and Multi-Layer Perceptrons (MLPs). Our purpose is not to beat state-of-
the-art records but to investigate the effect of applying q-neuron and its hyper-parameter sensitivity.
1http://yann.lecun.com/exdb/mnist/
2https://www.cs.toronto.edu/~kriz/cifar.html
4
Under review as a conference paper at ICLR 2019
The MNIST-CNN architecture is given as follows: 2D convolution with 3 × 3 kernel and 32 features;
(q-)activation; batch normalization; 2D convolution with 3 × 3 kernel and 32 features; (q-)activation;
2 × 2 max-pooling; batch normalization; 2D convolution with 3 × 3 kernel and 64 features; (q-
)activation; batch normalization; 2D convolution with 3 × 3 kernel and 64 features; (q-)activation;
2 × 2 max-pooling; flatten into 1D vector; batch normalization; dense layer of output size 512; (q-
)activation; batch normalization; (optional) dropout layer with drop probability 0.2; dense layer of
output size 10; soft-max activation.
The MNIST-MLP architecture is: dense layer of output size 256; (q-) activation; batch normaliza-
tion; (optional) dropout layer with drop probability 0.2; dense layer of output size 256; (q-) activa-
tion; batch normalization; (optional) dropout layer with drop probability 0.2; dense layer of output
size 10; soft-max activation.
The CIFAR-CNN architecture is: 2D convolution with 3 × 3 kernel and 32 features; (q-)activation;
2D convolution with 3 × 3 kernel and 32 features; (q-)activation; 2 × 2 max-pooling; (optional)
dropout layer with drop probability 0.2; 2D convolution with 3 × 3 kernel and 64 features; (q-
)activation; 2D convolution with 3 × 3 kernel and 64 features; (q-)activation; 2 × 2 max-pooling;
(optional) dropout layer with drop probability 0.2; flattern into 1D vector; dense layer of output size
512; (q-) activation; (optional) dropout layer with drop probability 0.1; dense layer of output size
10; soft-max activation.
We use the cross-entropy as the loss function. The model is trained for 100 epochs based on a
stochastic gradient descent optimizer with a mini-batch size of 64 (MNIST) or 32 (CIFAR) and a
learning rate of 0.05 (MNIST) or 0.01 (CIFAR) without momentum. The learning rate is multiplied
by (1 - 10-6) after each mini-batch update. We compare tanh, relu, elu, softplus activations with
their q-counterparts. We either fix λ0 = 0.02 or 0.1, or anneal from λ0 ∈ {1, 5, 9} with γ = 0.5. The
learning curves are shown in figs. 3 to 5, where the training curves show the sample-average cross-
entropy values evaluated on the training set after each epoch, and the testing curves are classification
accuracy. In all figures, each training or testing curve is an average over 10 independent runs.
For q-activation, c means the λ parameter is fixed; a means the λ is annealed based on eq. (7).
For example, “c0.02” means λ = 0.02 throughout the training process, while “a1” means that λ is
annealed from λ0 = 1.
We see that in almost all cases, q-activation can consistently improve learning, in the sense that both
training and testing errors are reduced. This implies that q-neurons can get to a better local optimum
as compared to the corresponding deterministic neurons. The exception worth noting is q-relu,
which cannot improve over relu activation. This is because gq(x) is very similar to the original f(x)
for (piece-wisely) linear functions. By proposition 1, f00 (x) = 0 implies that the gradient of gq (x)
and f(x) are similar for small Var(q). One is advised to use q-neurons only with curved activation
functions such as elu, tanh, etc.
We also observe that the benefits of q-neurons are not sensitive to hyper-parameter selection. In al-
most all cases, q-neuron with λ simply fixed to 0.02/0.1 can bring better generalization performance,
while an annealing scheme can further improve the score. Setting λ too large may lead to under-fit.
One can benefit from q-neurons either with or without dropout.
On the MNIST dataset, the best performance with error rate 0.35% (99.65% accuracy) is achieved
by the CNN architecture with q-elu and q-tanh. On the CIFAR10 dataset, the best performance of
the CNN with accuracy 82.9% is achieved by q-elu.
4 Conclusion
We proposed the stochastic q-neurons based on converting activation functions into correspond-
ing stochastic q-activation functions using Jackson’s q-calculus. We found experimentally that q-
neurons can consistently (although slightly) improve the generalization performance, and can goes
deeper on the error surface.
5
Under review as a conference paper at ICLR 2019
SSo-U-∙J~
0	20	40	60 SO 100
SSo-U-£
epoch
0.07
0.06
0.05
i
f 0.04
0.03
0.02
0.07
0.06
0.05
I
E 0.04
0.03
0.02
tanh dropout
e∣u dropout
Sso- U-eh
SSo-U-∙J~
s6su93J9≡JCUJeas
s6elu∞J9d) JCUJφbs
e∣ u
SSo-U-eh
(θβu∞JθdαJ,Jα
(Θ6au∞jθdrαj,jα≈s
O 20	40	60	80 IOO
epoch
SOftplUS
0	20	40	60 SO IOO
epoch
0	20	40	60	80	100
epoch
SOftPlUS_dropout
0.4
0.5
o-e⅞
s
0.7
I
0.8 Q
⅛i
S
0.9

Figure 3:	Training loss (descending curves) and testing accuracy (ascending curves) of a CNN on
the MNIST dataset, using different activation functions (from top to bottom), with (left) or without
(right) dropout.
6
Under review as a conference paper at ICLR 2019
0.225
0.200
0.175
g 0.150
£ 0.125
0.100
0.075
0.050
0.225
0.200
0.175
g 0-150
f 0-125
0.100
0.075
0.050
tanh
0	20	40	60	80	100
epoch
tanh (99.06%)
qtanh (c0.02) (99.10%)
qtanh (cθ.l) (99.09%)
qtanh (al.0) (99.13%)
qtanh (a5.0) (99.14%)
qtanh (a9.0) (99.15%)
SSO-U-E
tanhdropout
0.30
s6su8J9≡JcUje-səl
Sso- U-eh
SSO-U-BJ-
0.200
0.175
0.150
0.125
0.100
0.075
0.050
0.250
0.225
0.200
0.175
0.150
0.125
0.100
0.075
0.050
SSo-U-£
(θβsu∞JθdαJ,Jα
(Θ6elusgr2α≈s
SSo-U-∙J~
SOftplUS
0.40
1-5	0.35
2 i 0-30
h
S £ 0.25
2∙5∣ S
3 i 0-2°
0.15
3.5
0.10
0	20	40	60 BO 100
epoch
eiu_dropout
SOftPlUS_dropout
s6su8J9≡Joile-səl
s6su93J9≡Joileas
s6suee9d) JCUJeas
Figure 4:	Training loss (descending curves) and testing accuracy (ascending curves) of a MLP on
MNIST.
7
Under review as a conference paper at ICLR 2019
1.2
1.1
i1^0
S 0.9
0.8
0.7
relu dropout
SSO-U-BJ-
O 20	40	60 BO IOO
SSO-U-BJ-
6u3a) JaJJ。sa
0 5 0 5 0 5
2 2 3 3 4 4
20
40
0 5 0 5 0 5
2 2 3 3 4 4
s6su3a) JaJJ。~sa~
epoch
0
60
BO
100
SOftpIus
epoch
elu-dropout
s6sueeed) J0JJ9as
epoch
Figure 5: Training loss (descending curves) and testing accuracy (ascending curves) of a CNN on
CIFAR10.
8
Under review as a conference paper at ICLR 2019
References
Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with Breg-
man divergences. Journal of machine learning research, 6(Oct):1705-1749, 2005.
Djork-Ame Clevert, Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (ELUs). In ICLR, 2016. arXiv 1511.07289.
Thomas Ernst. A comprehensive treatment of q-calculus. SPringer Science & Business Media, 2012.
Erica JC Gouvea, Rommel G Regis, Alme C Soterroni, Marluce C Scarabello, and Fernando M
Ramos. Global oPtimization using q-gradients. European Journal of Operational Research, 251
(3):727-738, 2016.
Frank Hilton Jackson. On q-functions and a certain difference oPerator. Earth and Environmental
Science Transactions of The Royal Society of Edinburgh, 46(2):253-281, 1909.
Victor Kac and Pokman Cheung. Quantum Calculus. Universitext. SPringer New York, 2001. ISBN
9780387953410.
Shujaat Khan, Alishba Sadiq, Imran Naseem, Roberto Togneri, and Mohammed Bennamoun. En-
hanced q-least mean square. 2018. arXiv:1801.00410 [math.OC].
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier non-linearities imProve neural
network acoustic models. In ICML, 2013.
Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and
James Martens. Adding gradient noise imProves learning for very deeP networks. In ICLR, 2016.
P Njionou Sadjang. On the fundamental theorem of (p, q)-calculus and some (p, q)-Taylor formulas.
arXiv preprint arXiv:1309.3934, 2013.
Aline C Soterroni, Roberto L Galski, and Fernando M Ramos. The q-gradient method for continuous
global oPtimization. In AIP Conference Proceedings, volume 1558, PP. 2389-2393. AIP, 2013.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
DroPout: A simPle way to Prevent neural networks from overfitting. Journal of machine learning
research, 15:1929-1958, 2014.
Zhen Xu and Frank Nielsen. Beyond ordinary stochastic gradient descent, March 2018. PrePrint
INF517.
A B RIEF OVERVIEW OF THE (p, q) -DIFFERENTIAL CALCULUS
For p, q ∈ R, p 6= q, define the (p, q)-differential:
dp,q f (x) := f (qx) - f (px).
In Particular, dp,qx = (q - p)x.
The (p, q)-derivative is then obtained as:
Dp,q f (x) :
dp,q f (x)
dp,qx
f(qx) - f (Px)
(q - P)X
We have Dp,q f (x) = Dq,pf (x).
Consider a real-valued scalar function f (x). The differential oPerator D consists in taking the
derivative: Df(X = dχ = f(x).
The (P, q)-differential operator Dp,q for two distinct scalars P and q is defined by taking the follow-
ing finite difference ratio:
Dp,q f (x) :=
f(pχ)-f(qχ)
(P-q)x
f0(0),
x 6= 0,
x = 0.
(8)
9
Under review as a conference paper at ICLR 2019
We have Dp,q f (x) = Dq,pf (x).
The (P, q)-derivative is an extension of Jackson’s q-derivative Jackson (1909); Kac & Cheung
(2001); Ernst (2012); Sadjang (2013) historically introduced in 1909. Notice that this finite dif-
ference differential operator that does not require to compute limits (a useful property for derivative-
free optimization), and moreover can be applied even to nondifferentiable or discontinuous func-
tions.
An important property of the (P, q)-derivative is that it generalizes the ordinary derivative:
Lemma 3. For a twice continuously differentiable function f, we have limp→q Dp,q f (x) =
1 Df(qx) = 1 f0(qx) and limχ→o Dp,qf(x) = Df(0).
Proof. Let us write the first-order Taylor expansion of f with exact Lagrange remainder for a twice
continuously differentiable function f :
f(qχ) = f(pχ) + (qχ - pχ)f 0(pχ) + 1(qχ - Pxyf 00(ε),
for ε ∈ (min{px, qx}, max{px, qx}).
It follows that
Dp,q f (x)
f (px) - f (qx)	f (qx) - f (px)
x(p - q)
x(q - p)
(9)
f0(px) + 2x(q -p)f00(ε).
(10)
Thus, whenever P = q We have Dp,q f (x) = f0(ρχ) = PDf (px), and whenever X = 0, we have
Dp,q f (0) = f0(0). In particular, when P = 1, we have Dqf(x) = f0(x) when q = 1 or when
x = 0.
Let us denote Dq the q-differential operator Dq := D1,q = Dq,1.
The following (P, q)-Leibniz rules hold:
Since BF(qx : px) := f (qx) — f (px) — (qx — px)f 0(px) = 1 (qx — px)2f00(ε), we can further
express the (P, q)-differential operator using Bregman divergences Banerjee et al. (2005) as follows:
Corollary 4. We have:
Dp,qf(x)
f (px) - f (qχ)
x(p - q)
f(qx) - f (Px
x(q - P)
f0 (px) +
f0(qx) +
BF (qx : Px)
X(P - q) ,
BF (Px : qx)
x(q - P).
A.1 LEIBNIZ (p, q)-RULES OF DIFFERENTIATION
•	Sum rule (linear operator):
Dp,q (f (x) + λg(x)) = Dp,q f (x) + λDp,q g(x)
•	Product rule:
Dp,q (f (X)g (X)) = f (PX) + Dp,qg(X) + g(qX)Dp,qf(X),
= f (qX) + Dp,qg(X) + g(PX)Dp,qf(X).
•	Ratio rule:
Dp,q(f(x)/g(x))
g(qx)Dp,qf(x) - f(qx)Dp,qg(x)
g(px)g(qx)
g(Px)Dp,qf (X) - f (Px)Dp,qg(X)
g(px)g(qx)
General Leibniz rule for (P, q)-calculus.
10
Under review as a conference paper at ICLR 2019
A.2 THE (p, q)-GRADIENT OPERATOR
For a multivariate function F (x) = F (x1 , . . . , xd) with x = (x1, . . . , xd), let us define the first-
order partial derivative and i ∈ [d] and pi 6= qi,
{	F (X 1 ,...,pi xi,...,Xn) - F (X1,...,qi xi ,...,Xn)
…、	(pi-qi)xi
∂F (x)
∂Xi
xi 6= 0,
xi
0
F F (x+(pi-1)ei)-F (x+(qi-1)ei)
∂	∂F (x)
〔∂xi
(Pi-qi)xi
xi 6= 0,
xi
0
where ei is a one-hot vector with the i-th coordinate at one, and all other coordinates at zero.
The generalization of the (p, q)-gradient Soterroni et al. (2013) follows by taking d-dimensional
vectors forp = (p1, . . . ,pd) and q = (q1, . . . ,qd):
Dp1,q1,X1 F (x)
Vp,qF(x):=	.
Dpd,qd,XdF(x)
The (p, q)-gradient is a linear operator: Vp,q (aF(x) + bG(x)) = aVp,qF(x) + bVp,qG(X) for any
constants a and b. When p,q → 1, Vp,q → V: That is, the (p, q)-gradient operator extends the
ordinary gradient operator.
A.3 Pseudo-Codes
We can easily implement q-neurons based on the following reference code, which is based on a
given activation function activate. Note, q has the same shape as x. One can fix eps= 10-3
and only has to tune the hyper-parameter lambda.
def qactivate( x, lambda, eps ):
q = random_normal( shape=shape(x) )
q = ( 2*( q>=0 )-1 )
return ( activate( x
*	( lambda * abs(q) + eps )
*	(1+q) ) - activate( x ) ) / q
11