Under review as a conference paper at ICLR 2019
Deconfounding Reinforcement Learning
in Observational Settings
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a general formulation to cope with a family of reinforce-
ment learning tasks in observational settings, that is, learning good policies solely
from the historical data produced by real environments with confounders (i.e.,
the factors affecting both actions and rewards). Based on the proposed approach,
we extend one representative of reinforcement learning algorithms: the Actor-
Critic method, to its deconfounding variant, which is also straightforward to be
applied to other algorithms. In addition, due to lack of datasets in this direction, a
benchmark is developed for deconfounding reinforcement learning algorithms by
revising OpenAI Gym and MNIST. We demonstrate that the proposed algorithms
are superior to traditional reinforcement learning algorithms in confounded envi-
ronments. To the best of our knowledge, this is the first time that confounders are
taken into consideration for addressing full reinforcement learning problems.
1	Introduction
In recent years, reinforcement learning (RL) has made great progress, spawning a large number of
successful applications especially in terms of games (Silver et al., 2016; Mnih et al., 2013; Ope-
nAI, 2018). Within this background, much attention has been devoted to the development of RL
algorithms with the goal of improving treatment policies in healthcare (Gottesman et al., 2018). In
fact, various RL algorithms have been proposed to infer better decision-making strategies for me-
chanical ventilation (Prasad et al., 2017), sepsis management (Raghu et al., 2017a;b), and treatment
of schizophrenia (Shortreed et al., 2011). In healthcare, a common practice is to focus on the ob-
servational setting, because ones do not wish to experiment with patients’ lives without evidence
that the proposed treatment strategy is better than the current practice (Gottesman et al., 2018) 1. As
pointed out in (Raghu et al., 2017a), even ifin the observational setting, RL also has advantages over
other machine learning algorithms especially in two situations: when the ground truth of a “good”
treatment strategy is unclear in medical literature (Marik, 2015), and when training examples do not
represent optimal behavior. On the other hand, although causal inference (Pearl, 2009) has been ex-
tensively explored and used in healthcare and medicine (Liu et al.; Soleimani et al., 2017; Schulam
& Saria, 2017; Alaa et al., 2017; Alaa & van der Schaar, 2018; Atan et al., 2016), the efficient ap-
proach to dealing with time-varying data is still unclear (Peters et al., 2017; Hernn & Robins, 2018).
On the basis of the discussion above, in this paper we attempt to combine advantages on both sides
to cope with an important family of RL problems in the observational setting, that is, learning good
policies solely from the historical data produced by real environments with confounding bias.
This type of problems are inevitable in the future RL research with the burgeoning development in
healthcare and medicine. To the best of our knowledge, however, little work has been done in the
direction. Confounding is a causal concept that is described in the language of causality instead of
probability and statistics (Pearl, 2009). Confounding bias occurs when a variable influences both
who is selected for the treatment and the outcome of the experiment (Pearl & Mackenzie, 2018),
which naturally corresponds to the action and the reward in RL, respectively. As a matter of fact,
confounders have been extensively studied in epidemiology, sociology, and economics. Take for
example the widespread kidney stones in which the size of the kidney stone is a confounding factor
affecting both the treatment and the recovery (Peters et al., 2017; Pearl, 2009), whether decon-
founding the size of the kidney stone or not entirely determines how to choose a more effective
treatment. Similarly, in RL, if unobserved potential confounders exist, they would affect both ac-
1Another straightforward example is that, in economics, considering the risky cost in terms of time and
money, it is not practical to study the optimal strategy by actually buying and selling stocks in the market.
1
Under review as a conference paper at ICLR 2019
tions and rewards when an agent interacts with environments and eventually influence the policy to
be optimized.
It is widely acknowledged that one should draw a causal graph before one can achieve any causal
conclusion (Pearl, 2009; Pearl & Mackenzie, 2018). Throughout the paper, we assume that, given
causal assumptions, we first estimate a model from the observational data we collected from real
environments or simulators, and then optimize a policy on the basis of the learned model. As men-
tioned previously, this assumption is quite useful in real-world RL applications (e.g., healthcare,
medicine, economics, etc), because in most circumstances, except the data we observed, we nei-
ther know anything about real environments nor are allowed to do anything in real environments
probably for the sake of ethics, laws or cost.
In order to adjust for confounders, we present a general formulation for addressing this class of
RL problems, namely deconfounding reinforcement learning (DRL). More specifically, given sev-
eral common confounding assumptions, under some conditions for identification we first estimate a
latent-variable model from observational data in which we simultaneously discover the latent con-
founders and infer how they affect action and reward, then deconfound the confounders using the
causal language developed by (Pearl, 2009), and finally optimize the policy based on the decon-
founding model we calculated. On the basis of the proposed formulation, we extend one popular RL
algorithm, the Actor-Critic method, to its corresponding deconfounding variant, which is straight-
forward to be applied to other RL algorithms. Due to lack of datasets in this respect, we revise the
classic control toolkit in OpenAI Gym (Brockman et al., 2016), making it a benchmark for compar-
ison of DRL algorithms. In addition, we also devise a confounding version of the MNIST dataset
(LeCun et al., 1998) to verify the performance of our causal model. Finally, we conduct extensive ex-
periments to demonstrate the superiority of the proposed formulation in confounded environments,
in comparison to traditional RL algorithms.
To sum up, our contributions in this paper are as follows:
1.	We propose a general formulation to address a family of RL problems in confounded envi-
ronments, namely deconfounding reinforcement learning (DRL);
2.	We present the deconfounding variant of Actor-Critic methods, which is obvious to be
applied to other RL methods;
3.	We develop a benchmark for DRL by revising the toolkit for classic control in OpenAI
Gym (Brockman et al., 2016) and by devising a confounding version of the MNIST dataset
(LeCun et al., 1998);
4.	We perform a comprehensive comparison of DRL algorithms with their vanilla versions,
showing that the proposed approach has an advantage in confounded environments.
5.	To the best of our knowledge, this is the first attempt to build a bridge between confounding
and the full RL problem. This is one of few research papers aiming at understanding the
connections between causal inference and the full RL.
2	Background
In this section, we briefly review confounding in causal inference. We recommend Pearls excellent
monograph for further reading (Pearl, 2009; Pearl & Mackenzie, 2018).
2.1	Simpson’ s Paradox
Let us begin with one of the most famous paradoxes in statistics: Simpson’s Paradox. Consider the
previously mentioned kidney stones, a classic example of Simpson’s paradox (Peters et al., 2017).
We collect electronic patient records to investigate the effectiveness of two treatments against kidney
stones, where although the overall probability of recovery is higher for patients who took treatment
b, treatment a performs better than treatment b on both patients with small kidney stones and with
large kidney stones. More precisely, we have
p(R	=	1|T	=	b)	> p(R =	1|T	= a);	but
p(R = 1|T	= b, Z	=	0)	< p(R =	1|T	= a, Z =	0),
p(R = 1|T	= b, Z	=	1)	< p(R =	1|T	= a, Z =	1);	(1)
where Z is the size of the stone,	T	the	treatment, and	R the recovery (all binary). How do we
cope with this inversion of conclusion? Which treatment do you prefer if you had kidney stones?
2
Under review as a conference paper at ICLR 2019
Does treatment b cause recovery? The answers to these questions depend on the causal relationship
between treatment, recovery, and the size of the kidney stone.
2.2	Confounding
An intuitive explanation for this kidney stone example of Simpson’s paradox is that larger stones are
more severe than small stones and are much more likely to be treated with treatment a, resulting in
that treatment a looks worse than treatment b. Therefore, it is straightforward to assume that the true
underlying causal diagram of the kidney stone example is shown in Figure 1(a), where confounding
occurs because the size of kidney stones influences both treatment and recovery. Here, the size of
kidney stones is called confounder. The term “confounding” originally meant “mixing” in English,
which describes that the true causal effect T → R is “mixed” with the spurious correlation between
T and R induced by the fork T J Z → R (Pearl & Mackenzie, 2018). In other words, We will not
be able to disentangle the true effect ofT on R from the spurious effect ifwe do not have data on Z.
Conversely, if we have measurements of Z, it is easy to deconfound the true and spurious effects by
adjusting for Z that averages the effect of T on R in each subgroup of Z (i.e., different size groups
in the case of kidney stones).
2.3	Do-Operator and Adjustment Criterion
From the viewpoint of causal inference, we can also use the language of intervention, namely do-
operator, to formulate confounding. In fact, in the example of kidney stones, what we are interested
in is how these two treatments compare when we force all patients to take treatment a or b, rather than
which treatment has a higher recovery rate given only the observational patient records. Mathemati-
cally, we focus on the true effect p(R = 1|do(T = a)) (i.e., intervention distribution where patients
are forced to take treatment a) instead of the spurious effect p(R = 1|T = a) (i.e., observational
distribution where patients are observed to take treatment a). Therefore, as described previously,
confounding can be naturally formulated by the discrepancy between p(R|T) andp(R|do(T)).
Generally speaking, do-operator can be executed in two common ways: by Randomized Controlled
Trials (RCTs) (Fisher, 1935) and by adjustment formulas (i.e., Back-door criterion and Front-door
criterion) (Pearl, 2009). RCTs is the so-called golden standard but rather limited due to many im-
practical factors (e.g., safety, laws, ethics, physically infeasibility, etc.). Back-door and Front-door
criterions require a known causal diagram in which causal assumptions are provided in advance.
According to the Back-door criterion, in the kidney stone example, we can immediately attain
p(R = 1|do(T = a)) = X	p(R = 1|T = a, Z = z)p(Z = z).	(2)
z=0
In fact, apart from the two adjustment formulas, we are provided with a syntactic method of deriving
claims about interventions, namely do-calculus, developed by Pearl (Pearl, 2009). The do-calculus
consists of three rules which can be repeatedly applied to simplify the expression for an interven-
tional distribution. It offers a powerful tool to convert intervention probabilities to observation prob-
abilities so that we can estimate the former from observational data alone.
2.4	Proxy Variables for Confounding
If confounders can be measured, then they can be adjusted for through the methods we discussed
in Section 2.3. However, in most cases where confounders are hidden or unmeasured, without
further assumptions, it is impossible to estimate the effect of the intervention on the outcome. A
common practice is to leverage observed proxy variables of the confounders (Angrist & Pischke,
2008; Maddala & Lahiri, 1992; Montgomery et al., 2000), which is one of the promising directions
of exploiting big-data to conduct causal inference in the presence of multiple proxy variables for
unmeasured confounders (Louizos et al., 2017). However, using proxy variables to correctly recover
causal effects should meet strict mathematical assumptions (Louizos et al., 2017; Edwards et al.,
2015; Kuroki & Pearl, 2014; Miao et al., 2018; Pearl, 2012; Wooldridge, 2009). Unfortunately,
in practice, we do not know whether or not the complicated data from the real world meet those
assumptions. Hence, following the same spirit from (Louizos et al., 2017), with multiple proxy
variables available, under relaxed assumptions we estimate a latent-variable model in which we
simultaneously discover the latent confounders and infer how they affect treatment and outcome.
3	Deconfounding Reinforcement Learning
In this section, we will formally introduce deconfounding reinforcement learning (DRL). Generally
speaking, DRL consists of two steps: learning a deconfounding model shown in Figure 1(b) and
optimizing a policy based on the learned deconfounding model. The main idea in step 1 is to si-
multaneously discover the hidden confounder and infer the causal effect through an estimation of
3
Under review as a conference paper at ICLR 2019
Figure 1: (a) Causal diagram for kidney stones. (b) The model for deconfounding reinforcement
learning. Solid nodes denote observed variables and open nodes represent unobserved variables.
Red and blue arrows emphasize the observed variables affected by u and by zt, respectively. The
causal effects of interest are colored in green.
a latent-variable model. More specifically, we first discuss the time-independent confounding as-
sumption in Section 3.1, and then, based on the assumption, formalize the deconfounding model in
Section 3.2. Section 3.3 talks about the problem of identification in our model, which is a central
issue in causal inference. After that, we present the details about how to learn the model via varia-
tional inference in Section 3.4. Step 2 is provided in Section 3.5 where we describe how to design
the deconfounding actor-critic method. It is straightforward to apply this to other RL algorithms in
the same manner.
3.1	Causal Assumptions
Without loss of generality, as shown in Figure 1(b) 2, we assume there exists a common confounder
in the sequential model, which is time-independent for each individual or for each procedure. This
assumption is so general that it would apply to various RL tasks across domains. For example, in
personalized medicine or precision medicine, socio-economic status can affect both the medication
strategy a patient has access to, and the patients general health (Louizos et al., 2017). Therefore
socio-economic status acts as confounder between the medication and health outcomes, in which
case socio-economic status is time-independent for each patient during the course of treatment. In
agriculture, soil fertility may serve as one of confounders affecting both the application of fertilizer
and the yield of each plot (Pearl & Mackenzie, 2018). In this circumstance, soil fertility is sta-
ble and thought of as a time-independent factor within a period of time (e.g., several months, the
growth circle of crops, etc.). In the example of stock markets, apart from socio-economic status as
mentioned above, government policy may also act as one of confounders, all of which can be seen
time-independent during a reasonable period of time.
3.2	The Model
Given the causal assumption, we first fit a generative model to a sequence of observational data:
observations, actions, and rewards, where actions and rewards are confounded by one or several
unknown factors. Formally, Let ~x = (x1, . . . , xT), ~a = (a1 , . . . , aT-1), ~r = (r2, . . . , rT+1),
~z = (z1 , . . . , zT ) be the sequence of observations, actions, rewards, and corresponding latent states,
respectively. The confounder is denoted by u, and it is worth noting that here u may stand for
more than one confounder in which multiple confounders are seen as a whole represented by u. We
assume that xt ∈ RDx , at ∈ RDa , rt ∈ RDr, zt ∈ RDz , and u ∈ RDu , where Dz Dx. The
generative model for DRL is then given by:
p(zt) = YjD=z1N(ztj|0, 1);	p(u) = YjD=u1N(uj|0, 1);
P(Xt∣zt,u)= N (Xt∣μX,σχ2) ； μχ = fι(Zt,u) σ^χ2 = f2(Zt,u);	(3)
P(at∣zt,u) = N (αt∖at ,σf 2) ; μca = f3(Zt,u) σt2 = f4(Zt,u);	(4)
2Note that, in our case where a policy depending only on the confounder is applied to generating the data
(Section 4.2), the arrow from zt to at is not necessary when learning the model so that zt can be viewed as not
a confounder of at and rt+1. This also provides another reason why we do not need to adjust for zt. However,
in some other cases such as when applied to medical data, the strategies of treatment from physicians definitely
contain valuable information about zt and at , and therefore the arrow between them is necessary when learning
the model.
4
Under review as a conference paper at ICLR 2019
p(rt+1 lzt,at,u) = N rrt+1 lμιr,σr2) ； μr = f5(zt,at,u)
P (zt∣zt-1 ,at-1) = N [t∣t∖zt ,σ∕∖ ； μ⅛ = f7( Zt-1 ,at-1)
标『=f6( zt,at ,u )；
σz = f8( Zt-1 ,at-1) ∙
(5)
(6)
Note that we parametrize each probability distribution as a Gaussian with its mean and variance
modeled by nonlinear functions fk and each fk is parametrized by a neural network with its own
parameters θk for k = 1, . . . , 8. Note that Equation (4) is not necessary in our model, but it is
potentially useful acting as a prior policy because it is learned from the observational data containing,
for example, the real treatment strategies by doctors.
3.3	Identification of Causal Effect
In our deconfounding model shown in Figure 1(b), the key difference from traditional RL lies in
the reward function 3. To be more precise, assuming that an agent standing at state Zt performs an
action at = a, unlike the traditional reward p(rt∖Zt, at = a), our deconfounding version based on
do-operator as depicted in Section 2.3 is given by
p(rt∖Zt, do(at = a)) =	p(rt∖Zt, do(at = a), u)p(u∖Zt, do(at = a))du,
u
=	p(rt ∖Zt, at = a, u)p(u)du,
u
(7)
(8)
where Equation 8 is by the rules of do-calculus applied to the causal graph in Figure 1(b) (Pearl,
2009). In fact, we can utilize the back-door criterion to directly obtain Equation (8). Furthermore,
through Equation (8) we proved thatp(rt ∖Zt, do(at = a)) can be identified from the joint distribution
p(u, Z~, ~x, ~a, ~r). Then, motivated by the idea that one is supposed to use the knowledge inferred from
the joint distribution between proxy variables and confounders to adjust for the hidden confounders
(Louizos et al., 2017; Edwards et al., 2015; Kuroki & Pearl, 2014; Miao et al., 2018; Pearl, 2012;
Wooldridge, 2009), in this paper we also assume that the joint distribution p(u, Z~, ~x, ~a, ~r) can be
approximately recovered solely from the observations (~x, ~a, ~r). The details about identification can
be found in Appendix A.
3.4	Learning
Since the nonlinear functions parametrized by neural networks make inference intractable, we will
learn the parameters of the model θk by employing variational inference along with an inference
model, a neural network which approximates the intractable posterior (Rezende et al., 2014; Kingma
& Welling, 2013; Krishnan et al., 2015). More specifically, using the variational principle, we posit
an approximate posterior distribution qφ(Z∖x) to obtain the following lower bound on the marginal
likelihood:
logpθ(x) ≥	E	[logpθ(x∖Z)] - KL(qφ(Z∖x)∖∖pθ(Z)) ,	(9)
qφ( zlx)
where the inequality is by Jensen’s inequality and φ is the parameter of the inference model q(Z∖x).
Note that in this general case x stands for observational variables and Z for latent variables.
3.4.1	Variational Lower Bound
Directly applying the lower bound in Inequality (9) to our model, we obtain
logpθ(~x, ~a, ~r) ≥ E [logpθ(~x, ~a, ~r∖~Z, u)] - KL (qφ(~Z, u∖~x, ~a, ~r)∖∖pθ(~Z, u))
qφ (~,u∣~,~,~)
= L(~x, ~a, ~r; θ, φ).	(10)
Using the Markov property of our model, the full distribution can be factorized in the following way:
pθ(~x, ~a, ~r, ~Z, u) = p(u)p(Z1) Yt=1p(xt∖Zt,u)p(at∖Zt, u)p(rt+1∖Zt, at, u)	Yt=2p(Zt∖Zt-1, at-1)
(11)
In addition, for simplicity’s sake, we also have the factorization assumption for the posterior approx-
imation:
T
qφ(~Z, u∖~x, ~a, ~r) = q(u∖~x, ~a, ~r)q(Z1∖~x, ~a, ~r)	t=2 q(Zt∖Zt-1, ~x, ~a, ~r).	(12)
3An intuition can be found in Appendix F.2.
5
Under review as a conference paper at ICLR 2019
Combining Equation (10), (11) and (12) yields:
logpθ(~x, ~a, ~r) ≥L(~x, ~a, ~r; θ, φ)
= X	E	[logp(xt|zt,u) + logp(at|zt,u) + logp(rt+1 |zt, at, u)]
t==1 Zt〜q(Zt ∣zt-1 ,x,~,~)
U〜q (u∣x,~,r)
- KL (q(u|~x, ~a, ~r)||p(u)) - KL (q(z1|~x, ~a, ~r)||p(z1))
-X ：=2	zt-1 〜	[KL(q(ztlzt- 1,χ,~,~) iip (ztlzt- 1 ,at- 1))],	(13)
q(Zt-1 |Zt-2,~x,~a,~~)
where we omit subscripts θ and φ, and a more detailed derivation can be found in Appendix B.
Obviously, Equation (13) is differentiable with respect to the parameters of the model θ and φ.
Using the reparametrization trick (Kingma & Welling, 2013), we can directly apply backpropagation
to update the parameters.
3.4.2 Inference Model
From the factorization form in Equation (12), we can see that there are two types of inference mod-
els: q(u|~x, ~a, ~r) and q(~z|~x, ~a, ~r). Similar to the generative model in Section 3.2, we also parametrize
both of them as Gaussian:
q(ul~, ~, r)) = N (uiμu,σu2) ;	μu = f9(~,~,~) σtu2 = f 10(x,~,~);	(14)
22
q(z∖x,~,ιr)= N ~ls∖μt,σt ; ；	μt = f 11(χ,~,~) σt = f 12(χ,~,~).	(15)
In fact, as shown in Equation (12), q(~z|~x, ~a, ~r) can be further factorized as the product of
q(zt∖zt-1, rχ,	r~, r~) for t = 1, . . . , T. Taking a closer look at this term, based on the Markov	property
of our model, we have zt χ1, . . . , χt-1, ~1,	. . . , ~t-2, ~2, . . . , ~t∖zt-1, and then the	term	can be
simplified as follows,
q(zt∖zt-1, rχ, r~, r~) = q(zt∖ zt-1, ~t-1, χt, ~t, ~t+1,χt+1, ~t+1,~t+2, . . . , χT, ~T, ~T+1).	(16)
|} |} |}
past	current	future
Equation (16) tells us that zt depends on zt-1 and all the current and future observed data (rχ, r~, r~).
Meanwhile, the conditional independence above means that zt-1 contains all the historical data.
Therefore, it is natural to calculate zt based on the whole sequence of data, which is exactly what
recurrent neural networks (RNNs) do. Inspired by (Krishnan et al., 2015; 2017), we similarly choose
a bi-directional LSTM (Zaremba & Sutskever, 2014) to parameterize f11 and f12 in Equation (15).
Considering Equation (14) has the same structure as Equation (15), f9 and f10 are parameterized by
a bi-directional LSTM as well. More details about the architecture can be found in Appendix D.
Note that for the task of sample predictions, that is, at any time step t, given a new χt, we require to
know ~t and ~t+1 before inferring the distribution over zt. Hence, we need to introduce two auxiliary
distributions, denoted by red and blue dashed lines in Figure 1(b), to help conduct counterfactual
reasoning (i.e., sample prediction on unseen χt). To be more precise, we have
22
μ = μa,σ = σa )	μa = f 13(Xt) σa = f 14(Xt);	(17)
q(~t +1 ∖xt, ~t) = N (μ = μt, σ = σt ) μt = f 15 (xt,~t) σt = f 16(xt, ~t),	(18)
where f13, f14, f15, and f16 are also parameterized by neural networks. To estimate the parameters,
we will add these two extra terms in the variational lower bound (Equation (13)):
LDRL = L + XtT=1 (log q(~t∖xt) + log q(~t+1∖xt, ~t)) .	(19)
3.5 Deconfounding RL Algorithms
Now we have all the building blocks for DRL algorithms. Once our model is learned from the
observational data, it can be directly used as a dynamic environment like those in OpenAI Gym
(Brockman et al., 2016). We can exploit the learned model to generate rollouts for policy optimiza-
tion. In practice, Equation (8) is approximated using the Monte Carlo method as follows:
1N
P(~t∖zt = Z, do(~t = a)) =而 V, ∣p(~t∖zt = Z,~t = a,u) Ui 〜P(U),	(20)
N	i=1
6
Under review as a conference paper at ICLR 2019
where N is the number of samples from the prior p(u). In the presence of observational data, a
better estimate could be given on the basis of the samples drawn from the approximate posterior
q(u|~x, ~a, ~r) which we compute through the inference network presented in Section 3.4.2.
On the basis of our deconfounding reward function, it is straightforward to extend traditional RL
algorithms to their corresponding deconfounding version. In this paper, we select one representative
of them: the Actor-Critic method (Sutton et al., 1998), but it is straightforward to be applied to other
algorithms.
Deconfounding Actor-Critic Methods The actor-critic method is a policy-based method directly
parameterizing the policy ∏(a∣z; θ), which aims to reduce the variance of the estimate of the policy
gradient by subtracting a learned function of the state b(z), known as a baseline, from the return.
The learned value function V (z; φ) is commonly used as the baseline. Taking into consideration
that the return is the estimate of Q(z, a; φQ) and b(z) is the estimate of V (z; φV ), the gradient of
the actor-critic loss function at step time t is given by
RJ(θ)= Eπ [(Q(zt,at； Φq) — V(Zt； Φv)) Rθ ln∏(atZt； θ)],	(21)
where Q(zt, at; φQ) -V(zt; φV) used to be seen as an estimate of the advantage of action at in state
zt. In practice, Q(zt, at; φQ) is usually replaced with one-step return, that is, rt+1 + V(zt+1; φV).
Importantly, in deconfounding actor-critic methods, We use r+ι 〜 P(r +ι ∣zt, do(at)) (Equation
(20)), as opposed to r +ι 〜P(r+ι ∣zt, at) in vanilla actor-critic methods. The details about training
can be found in Appendix E.
4 Experimental Results
It is widely acknowledged that evaluating approaches dealing with confounding is always challeng-
ing due to lack of groundtruth and benchmark datasets. Besides, little work has been done before in
DRL, which renders evaluating such algorithms in this respect much harder. Therefore, we first de-
velop benchmark datasets for evaluation of our algorithms, primarily by revising the MNIST dataset
(LeCun et al., 1998) and two environments in OpenAI Gym (Brockman et al., 2016): CartPole and
Pendulum. We then evaluate our model as well as compare two proposed deconfounding algorithms
to their corresponding vanilla versions on the benchmark datasets we developed.
4.1	Implementation details
We used Tensorflow (Abadi et al., 2016) for the implementation of our model and DRL algorithms.
Optimization was done with Adam (Kingma & Ba, 2014). Unless stated otherwise, the setting of all
the hyperparameters and architectures of the neural networks we adopted in this paper can be found
in Appendix J.
To verify how good the learned model is, we performed two types of tasks: reconstruction and
counterfactual reasoning. The reconstructions were performed by feeding the input sequence into
the learned inference network, and then sampling from the resulting posterior distribution according
to Equation (15), and finally feeding those samples into the generative network described in Equation
(3). The counterfactual reasoning, that is, predicting xt+1 given xt we have not seen in the training
set and a model trained in the training set, were executed through four steps: 1) Given an xt unseen
in the training set, we estimate at and rt+1 based on Equation (17) and (18); 2) Once we have xt,
at, and rt+1, it is easy to estimate zt from Equation (15); 3) Using the estimated zt and at, we can
directly compute zt+1 from Equation (6); (4) The final step is to reconstruct xt+1 from zt+1 and u
according to Equation (3). Repeating the four steps, we can counterfactually reason out a sequence
of data.
To evaluate the confounder u, there are also two scenarios. The easy one is that, given a sequence
of observational data (~x, ~a, ~r), it is obvious to estimate u from Equation (14). The more challenging
one is to calculate u given only xt at any time step. Following the same steps used in the task of
counterfactual reasoning, we first compute at and rt+1 based on Equation (17) and (18), and then
estimate u through Equation (14).
4.2	Confounding Datasets
Due to the space limit, we only introduce the Confounding Pendulum dataset in this section. Con-
founding CartPole and Confounding MNIST are devised in the same way and can be found in Ap-
pendix H.1 and Appendix H.2, respectively.
7
Under review as a conference paper at ICLR 2019
Confounding Pendulum Motivated by (Krishnan et al., 2015) where the authors synthesized a
dataset mimicking the healthcare data under harsh conditions (e.g., noisy laboratory measurements,
surgeries and drugs affected by patient age and gender, etc.), as well as considering our focus on
RL tasks, we revised the original Pendulum in OpenAI Gym (Brockman et al., 2016) to develop a
confounding Pendulum dataset in which a binary confounder is introduced. More specifically, we
select 100 different screen images of Pendulum to create a synthetic dataset where actions are joint
effort with the value of between -2 and 2 4. First, 20% bit-flip noise are added to each image , and
then, based on a random policy but confounded by a binary factor (will be described soon), actions
are performed on each noisy image for five time steps, which produces a large number of 5-step
sequences of noisy images. To each generated sequence, exactly one sequence of three consecutive
squares (2 × 2 in pixel) is superimposed with the top-left corner of the images in a random starting
location. The squares within the sequences are intended to be analogous to seasonal flu or other
ailments that a patient could exhibit that are independent of the actions and which last several time
steps (Krishnan et al., 2015). We aim to show that our model can learn long-range patterns, which
plays an important role in medical applications. We treat such generated sequences of images as
the observations ~x. The training/validation/test set respectively comprises 140000/28000/28000
sequences of length five.
Now we are arriving at the key stage: how to define a reasonable relationship between the con-
founder, action, and reward. For simplicity of notation, we denote the confounder by u, action by
a, and reward by r. u is a binary variable mimicking the socio-economic status (i.e., the rich and
the poor) . The range of actions a ∈ [-2, 2] is grouped into two categories T1 (i.e., 1 ≤ |T1 | ≤ 2)
and T2 (i.e., 0 ≤ |T2| ≤ 1) representing different treatments (i.e., T1 is more valid and T2). How to
choose the treatment depends on u. The reward is defined as follows,
r = ro +rc,	(22)
where ro is the original reward in Pendulum, which is a function of a 5, and rc is the extra reward
caused by both u and a, where rc comes from a two Gaussian mixture
屋 ~ X r。= {R 1 ,R 2 } πrc N (μrc ,σ 2) , X rc= {R 1 R } "r。= 1," R1 , " R ",	(23)
with σ and μrc fixed and ∏rc determined by both U and a (See more in Appendix H.3). Obviously，
in the definition above, ris a function of a and u. It is worth noting that u has an influence on xt
through at-1 and zt， meaning that xt contains some piece of information about u and therefore can
be viewed as its proxy variable. In our case， for the sake of generality and practice， we assume the
influence between the confounder， action and reward is stochastic. Take the case of kidney stones
for example， even though treatment a is more valid than treatment b， there are still some patients
choosing treatment b in each category， but with different probabilities. All the details can be found
in Appendix H.3， where a straightforward analogy is provided as well.
4.3	Performance analysis of the deconfounding model
In this section， to demonstrate the validity of our deconfounding model， denoted by Mdecon， we
compare with the original model (i.e.， the model similar to that shown in Figure 1(b) but without
the confounder u)， denoted by Morin. We train Mdecon by optimizing Equation (19) but train Morin
by a little different loss function excluding the confounder u whose full derivation can be found in
Appendix C. Both models are separately trained in a batch manner on the training set (i.e.， 140K
sequences of length five of images) of the confounding dataset. Afterwards， following the steps
depicted in Section 4.1， we use each trained model to perform the reconstruction task on the train-
ing set， and both reconstruction and counterfactual reasoning tasks on the testing set (i.e.， 28K
sequences of length five of images).
Figure 2 presents a comparison of Mdecon and Morin， in terms of reconstruction and counterfactual
reasoning on the confounding Pendulum dataset. The second row is based on Mdecon (Figure 1(b))，
whilst the top row comes from Morin. It is evident that the results generated by the deconfounding
model is superior to those produced by the model not taking into account the confounder. To be
more specific， as shown in the zoom of samples on the bottom row， Morin generates more blurry
images than Mdecon， because， without modelling the confounder u， Morin is forced to average over its
4More details can be found at https://github.com/openai/gym/wiki/Pendulum-v0.
5In fact， here ro is a function of both a and a state， and we mention only a to emphasize that the confounder
affects the action.
8
Under review as a conference paper at ICLR 2019
Figure 2: Reconstruction and counterfactual reasoning on the confounding Pendulum dataset. Top
row: results from the model without the confounder u (Morin); Second row: results from the model
with the confounder u (Mdecon). The last two rows are the zoom of samples selected in the same
positions from the top row (dashed boxes for Morin) and the second row (solid boxes for Mdecon),
respectively.
multiple latent states resulting in more blurry samples. Likewise, we can attain the same conclusion
from the samples respectively produced by Morin and Mdecon on the confounding MNIST dataset, as
shown in Figure 7 of Appendix I.
In addition, looking closely at the squares on the generated digit samples (colored in yellow box),
it is obvious to observe that Morin will generate non-consecutive squares in the task of counterfac-
tual reasoning, which does not really make sense because only consecutive patterns appear in the
training set. In contrast, this case does not take place on the samples from Mdecon, showing that our
deconfounding model is able to cope with long-range patterns.
It is worth noting that, as shown in Figure 8 of Appendix I, by visualizing the 2-dimensional con-
founder u, we can discern that although the prior distribution of the confounder is assumed to be a
unit Gaussian distribution, the model still can learn two obvious clusters from the data because it is
originally a binary variable. It demonstrates that our model has an advantage in learning confounders
even if the assumed prior over them were not that accurate.
4.4	Comparison of RL algorithms
In this section, we will evaluate the proposed deconfounding actor-critic (AC) method by comparing
with its vanilla version on the confounding Pendulum dataset. In the vanilla AC method, given a
learned Morin, we optimize the policy by calculating the gradient presented in Equation (21) on
the basis of the trajectories/rollouts generated through Morin. Equation (21) involves two functions:
V(Zt; φv) and π(ag; θ), whose parameters can be found in Appendix J. It is worth noting that, in
this vanilla case, each reward rt+1 is produced from the conditional distribution p(rt+1 |zt, at). In
contrast, the proposed deconfounding AC method is built on Mdecon . Although the same gradient
method (Equation (21)) is utilized to optimize the policy, we base the deconfounding AC approach
on the different trajectories/rollouts generated by Mdecon in which each reward rt+1 relies on the
interventional distribution p(rt+1 |zt, do(at)) computed using Equation (20).
In the training phase, for both vanilla AC and deconfounding AC, we run a respective experiment
over 1500 episodes with 200 time steps each. In order to reduce non-stationarity and to decorrelate
updates, the generated data is stored in an experience replay memory and then randomly sampled
in a batch manner (Mnih et al., 2013; Riedmiller, 2005; Schulman et al., 2015; Van Hasselt et al.,
2016). In each episode, we summarize all the rewards and further average the sums over a window of
100 episodes to obtain a more smooth curve. As shown in Figure 3(a), obviously our deconfound-
ing AC algorithm performs significantly better than the vanilla AC algorithm in the confounded
environment.
9
Under review as a conference paper at ICLR 2019
In the testing phase, we first randomly select 100 samples from the testing set, each starting a new
episode, and then use the learned policies to perform reasoning over 200 time steps as we did during
the training time. From the resulting 100 episodes, we plot the total reward for each, shown in Figure
3(b), and compute the percentage of the optimal action T1 in each episode, presented in Figure 3(c).
It is worth noting that Figure 3(c) tells us that in each episode our deconfounding AC almost chooses
the optimal action at each time step, whilst the vanilla AC makes a wrong decision for more than
half time.
----Decon-AC
----Orin-AC
----Decon_AC
----Orin-AC
Il IN J ∣. ʌ A
---Decon_AC
——Orin-AC
O	500 IOOO 1500	O 20	40	60	80 IOO	O 20	40	60	80 IOO
episode	episode	episode
(a)	(b)	(c)
Figure 3: Comparison of vanilla (Orin-AC) and deconfounding (Decon_AC) Actor-Critic methods
on the confounding Pendulum dataset. (a) total reward over 1500 episodes in the training phase;
(b) total reward over 100 episodes in the testing phase; (c) Probability of optimal action over 100
episodes, corresponding to (b).
5	Related Work
Krishnan et al. (2015; 2017) used deep neural networks to model nonlinear state space models
and leveraged a structured variational approximation parameterized by recurrent neural networks to
mimic the posterior distribution. Levine (2018) reformulated RL and control problems to probabilis-
tic inference, which allows us to bear a large pool of approximate inference methods, and flexibly
extend the model. Raghu et al. (2017a;b) exploited continuous state-space models and deep RL to
deduce treatment policies for septic patients from observational data. Gottesman et al. (2018) dis-
cussed some issues of evaluating RL algorithms in observational health setting. However, all the
work mentioned above did not take into account confounders in their models.
Louizos et al. (2017) attempted to learn individual-level causal effects from observational data using
variational auto-encoder to estimate the unknown confounder given a causal graph in then non-
temporal setting. Paxton et al. (2013) developed predictive models based on electronic medical
records without using causal inference. Saria et al. (2010) proposed a nonparametric Bayesian
method to analyze clinical temporal data. Soleimani et al. (2017) represented the treatment response
curves using linear time-invariant dynamical systems which provides a flexible approach to model-
ing response over time. Although the latter two work modeled the sequential data, they both do not
exploit RL or causal inference.
Bareinboim et al. (2015) considered the problem of bandits with unobserved confounders, which is
one quite simple RL setting without state transitions. Sen et al. (2016) and Ramoly et al. (2017)
further studied contextual bandits with latent confounders. Forney et al. (2017) circumvented
some problems caused by unobserved confounders in Multi-Armed Bandit by counterfactual-based
decision-making. Zhang & Bareinboim (2017) leveraged causal inference to tackle the problem of
transferring knowledge across bandit agents. However, all these methods are based on the bandit
problem, a simplified version of RL, instead of the full RL problem.
In fact, as far as we are concerned, this is the first attempt to build a bridge between confounding
and the full RL problem, and this is also one of few research papers aiming at understanding the
connections between causal inference and full RL.
6	Conclusion and Future Work
To address the confounding issue in RL, we introduced a general formulation, namely deconfound-
ing reinforcement learning. On the basis of the proposed formulation, we presented deconfounding
variants of actor-critic methods and showed their superior performance on confounding datasets that
we created by revising OpenAI Gym and MNIST. In the future, we will collaborate with hospitals
and apply our approach to real-world medical datasets. We also hope that our work will stimulate
further investigation of connections between causal inference and RL.
10
Under review as a conference paper at ICLR 2019
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-
scale machine learning. In OSDI, volume 16, pp. 265-283, 2016.
Ahmed M Alaa and Mihaela van der Schaar. Bayesian nonparametric causal inference: Information
rates and learning algorithms. IEEE Journal of Selected Topics in Signal Processing, 12(5):1031-
1046, 2018.
Ahmed M Alaa, Michael Weisz, and Mihaela van der Schaar. Deep counterfactual networks with
propensity-dropout. arXiv preprint arXiv:1706.05966, 2017.
Elizabeth S Allman, Catherine Matias, John A Rhodes, et al. Identifiability of parameters in latent
structure models with many observed variables. The Annals of Statistics, 37(6A):3099-3132,
2009.
Joshua D Angrist and Jorn-Steffen Pischke. Mostly harmless econometrics: An empiricist's Com-
panion. Princeton university press, 2008.
Onur Atan, William R Zame, Qiaojun Feng, and Mihaela van der Schaar. Constructing effective
personalized policies using counterfactual inference from biased data sets with many features.
arXiv preprint arXiv:1612.08082, 2016.
Elias Bareinboim, Andrew Forney, and Judea Pearl. Bandits with unobserved confounders: A causal
approach. In Advances in Neural Information Processing Systems, pp. 1342-1350, 2015.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Alexander D’Amour.	(non-)identification in latent confounder mod-
els.	https://www.alexdamour.com/blog/public/2018/05/18/
non-identification-in-latent-confounder-models/, June 9, 2018.
Jessie K Edwards, Stephen R Cole, and Daniel Westreich. All your data are always missing: in-
corporating bias due to measurement error into the potential outcomes framework. International
journal of epidemiology, 44(4):1452-1459, 2015.
Ronald Aylmer Fisher. The design of experiments. 1935.
Andrew Forney, Judea Pearl, and Elias Bareinboim. Counterfactual data-fusion for online reinforce-
ment learners. In International Conference on Machine Learning, pp. 1156-1164, 2017.
Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan,
Linying Zhang, Yi Ding, David Wihl, Xuefeng Peng, et al. Evaluating reinforcement learning
algorithms in observational health settings. arXiv preprint arXiv:1805.12298, 2018.
MA Hernn and JM Robins. Causal Inference. Boca Raton: Chapman and Hall/CRC, forthcoming,
2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint
arXiv:1511.05121, 2015.
Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state
space models. In AAAI, pp. 2101-2109, 2017.
Manabu Kuroki and Judea Pearl. Measurement bias and effect restoration in causal inference.
Biometrika, 101(2):423-437, 2014.
11
Under review as a conference paper at ICLR 2019
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909, 2018.
Qing Liu, Katharine Henry, Yanbo Xu, and Suchi Saria. Using causal inference to estimate what-if
outcomes for targeting treatments.
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling.
Causal effect inference with deep latent-variable models. In Advances in Neural Information
Processing Systems, pp. 6446-6456, 2017.
Gangadharrao Soundaryarao Maddala and Kajal Lahiri. Introduction to econometrics, volume 2.
Macmillan New York, 1992.
PE Marik. The demise of early goal-directed therapy for severe sepsis and septic shock. Acta
Anaesthesiologica Scandinavica, 59(5):561-567, 2015.
Wang Miao, Zhi Geng, and Eric J Tchetgen Tchetgen. Identifying causal effects with proxy variables
ofan unmeasured confounder. Biometrika, 105(4):987-993, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Mark R Montgomery, Michele Gragnolati, Kathleen A Burke, and Edmundo Paredes. Measuring
living standards with proxy variables. Demography, 37(2):155-174, 2000.
OpenAI. Openai five. https://blog.openai.com/openai-five/, 2018.
Chris Paxton, Alexandru Niculescu-Mizil, and Suchi Saria. Developing predictive models using
electronic medical records: challenges and pitfalls. In AMIA Annual Symposium Proceedings,
volume 2013, pp. 1109. American Medical Informatics Association, 2013.
Judea Pearl. Causality. Cambridge university press, 2009.
Judea Pearl. On measurement bias in causal inference. arXiv preprint arXiv:1203.3504, 2012.
Judea Pearl and Dana Mackenzie. The Book of Why. Allen Lane, 2018.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements ofcausal inference: foundations
and learning algorithms. MIT press, 2017.
Niranjani Prasad, Li-Fang Cheng, Corey Chivers, Michael Draugelis, and Barbara E Engelhardt.
A reinforcement learning approach to weaning of mechanical ventilation in intensive care units.
arXiv preprint arXiv:1704.06300, 2017.
Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh
Ghassemi. Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602,
2017a.
Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits, and Marzyeh Ghas-
semi. Continuous state-space models for optimal sepsis treatment-a deep reinforcement learning
approach. arXiv preprint arXiv:1705.08422, 2017b.
Nathan Ramoly, Amel Bouzeghoub, and Beatrice Finance. A causal multi-armed bandit approach
for domestic robots failure avoidance. In International Conference on Neural Information Pro-
cessing, pp. 90-99. Springer, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
12
Under review as a conference paper at ICLR 2019
Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforce-
ment learning method. In European Conference on Machine Learning, pp. 317-328. Springer,
2005.
Suchi Saria, Daphne Koller, and Anna Penn. Learning individual and population level traits from
clinical temporal data. In Proceedings of Neural Information Processing Systems, pp. 1-9. Cite-
seer, 2010.
Peter Schulam and Suchi Saria. Reliable decision support using counterfactual models. In Advances
in Neural Information Processing Systems, pp. 1697-1708, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
Rajat Sen, Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G Dimakis, and Sanjay
Shakkottai. Contextual bandits with latent confounders: An nmf approach. arXiv preprint
arXiv:1606.00119, 2016.
Susan M Shortreed, Eric Laber, Daniel J Lizotte, T Scott Stroup, Joelle Pineau, and Susan A Mur-
phy. Informing sequential clinical decision-making through reinforcement learning: an empirical
study. Machine learning, 84(1-2):109-136, 2011.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Hossein Soleimani, Adarsh Subbaswamy, and Suchi Saria. Treatment-response models for coun-
terfactual reasoning with continuous-time, continuous-valued interventions. arXiv preprint
arXiv:1704.02038, 2017.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press,
1998.
Dustin Tran, Rajesh Ranganath, and David M Blei. The variational gaussian process. arXiv preprint
arXiv:1511.06499, 2015.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In AAAI, volume 2, pp. 5. Phoenix, AZ, 2016.
Jeffrey M Wooldridge. On estimating firm-level production functions using proxy variables to con-
trol for unobservables. Economics Letters, 104(3):112-114, 2009.
Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.
Junzhe Zhang and Elias Bareinboim. Transfer learning in multi-armed bandit: a causal approach. In
Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pp. 1778-
1780. International Foundation for Autonomous Agents and Multiagent Systems, 2017.
KUn Zhang, Biwei Huang, Jiji Zhang, Bernhard Scholkopf, and Clark Glymour. Discovery and
visualization of nonstationary causal models. arXiv preprint arXiv:1509.08056, 2015.
Kun Zhang, Biwei Huang, Jiji Zhang, Clark Glymour, and Bernhard Scholkopf. Causal discovery
from nonstationary/heterogeneous data: Skeleton estimation and orientation determination. In
IJCAI: proceedings of the conference, volume 2017, pp. 1347. NIH Public Access, 2017.
13
Under review as a conference paper at ICLR 2019
Appendices
A Identification of Causal Effect
As pointed out in (Louizos et al., 2017; Edwards et al., 2015; Kuroki & Pearl, 2014; Miao et al.,
2018; Pearl, 2012; Wooldridge, 2009), it is impossible to recover the joint distribution from the ob-
servational data if the hidden confounder has no connection to the observed variables. Fortunately,
there still exist a large number of possible circumstances, and we refer readers to (Louizos et al.,
2017; Kuroki & Pearl, 2014; Miao et al., 2018; Pearl, 2012; Allman et al., 2009) for more details.
Here, we only focus on one possible case presented in Figure 1 of (Louizos et al., 2017) (see Ap-
pendix F.1), because the result can be directly used to identify the causal effect in our model. More
precisely, in our model, at each time step, the 4-tuple (zt, xt, at, rt+1) is exactly the same as that
case in Figure 1 of (Louizos et al., 2017), and therefore p(zt, xt, at, rt+1) can be approximately
recovered only from the observations (xt, at, rt+1). Likewise, the joint distribution over the other
4-tuple (u, xt, at, rt+1) can be recovered from (xt, at, rt+1) in the same manner (see Appendix G).
Applying this rule repeatedly to the sequential data, we are finally able to approximately recover
p(u, ~z, ~x, ~a, ~r) solely from the observations (~x, ~a, ~r).
We recover the joint distribution using Variational Auto-Encoders (VAEs) (Kingma & Welling,
2013), which is a powerful tool to recover a very large class of latent-variable models as a minimizer
of an optimization problem (Tran et al., 2015; Louizos et al., 2017). Generally speaking, however,
VAEs currently have little theory available to guarantee that the true model can be identified through
learning (Louizos et al., 2017). The main reason behind is that, in such latent variable models, con-
founders and causal parameters, both of which are unknown and might influence each other, will
inevitably result in the case that different causal parameters could generate the same observable data
(D’Amour, June 9, 2018). Fortunately, identification is possible when the latent confounder and
their relationship to the outcome are themselves identified. For instance, in the presence of proxy
variables of the latent confounder, as described in the previous paragraph, both our model and the
one in (Louizos et al., 2017) are identified under some conditions outlined in (Louizos et al., 2017;
Kuroki & Pearl, 2014; Miao et al., 2018; Pearl, 2012; Allman et al., 2009). In addition, even if only
considering our model, because of the existence of many proxy variables of the confounders (e.g.,
each of zt and u has at least T proxy variables {xt}t=1,...,T, where zt can be viewed as hidden states
in HMMs (Allman et al., 2009)), identification can be also justified if the latent confounders zt and u
are categorical (Allman et al., 2009). In practice, however, we are not able to know the exact nature
of the confounders, e.g., what kinds of distributions they follow, if categorical how many categories
they have, etc. Hence, we take advantage of VAEs, which allows us to make substantially weaker
assumptions about the data generating process and the structure of the hidden confounders (Louizos
et al., 2017). Nevertheless, we empirically show that our approach is more beneficial to learning a
better policy in the existence of confounders.
Note that, in our model we have to differentiate the two types of confounders: the time-independent
confounder u and time-dependent confounders {zt}t=1,...,T , each playing a respective role in the
model. The former, as a global confounder, will affect the whole course of treatment, and therefore
should be adjusted for. In the example of kidney stones, the existence of the confounder (i.e., the
size of stones) will lead to a wrong treatment if not adjusting for it. In contrast, the time-varying
confounders {zt} act as states in RL, which, in principle, should not be adjusted for, because the
goal in RL is to learn a good policy in which any action is indeed supposed to be conditional on
a specific state. On the other hand, in terms of rewards, what an agent expects at each time step
is exactly the immediate reward when taking a specific action at a specific state, without the need
of adjusting for states. It is worth noting that the case with time-varying confounders {zt} can be
thought of to meet a pseudo or weak causal sufficiency assumption under which the causal effects
of actions on rewards will not be influenced by states at each time step (Zhang et al., 2017; 2015).
This key difference motivates us to only adjust for the time-independent confounder u in this paper.
In addition, as shown in Figure 1(b), in our case where a policy depending only on the confounder
is applied to generating the data (Section 4.2), the arrow from zt to at is not necessary so that zt
can be viewed as not a confounder of at and rt+1 . This also provides another reason why we do not
need to adjust for zt .
14
Under review as a conference paper at ICLR 2019
B Variational Lower Bound for Mdecon
= log
I I pθ(x)a)r)z)u)d~dU
JuJz
≥ [ qqφ(z,u∖x,a,r))logtpθ(々a：rzu)dzdU
JUJz	qφ(zu∖x,a,rr)
[[qψ(u∖X, a, r)qφ(z∖X, a, r) log — P(χ, a, ,r ZU) T	dZdU	(factorization assumption)
JU JZ	qψ(u∖χ, a, r) qφ (z∖χ, a, r)
/ / qψ (u∖x, a, r) qφ (z∖x, a, r)
log
P(u)P(Z1) [q乙1 p(Xt∖zt,u)p(at∖zt,u)p(r+1 ∖zt, at,u)] [口T=2 p(zt∖zt-i,aɪ)]
qψ (u∖χ, a, r)qφ (z∖x, a, r)
dzdu
I q q(u∖x, a, r)q(Z1 ∖X, a, r)…q(ZT∖zτ — ι,X,a,r))
JUJz
log
P(U)p(Z1) Q21 p(Xt∖zt,u)p(at∖zt,u)p(r +1 ∖zt,at,u) Q晨 p(zt∖z- 1,at-1) d宓U
q(u∖X, a, r)q(Z1 ∖X, a, r)…q(ZT∖zτ — 1, X, a, r)
q(u∖X, a, r)q(Z1 ∖X, a, r)…q(ZT∖zτ — 1, X, a, r)
log (p(Xt∖zt, u)p(at∖zt, u)p(r+1 ∖zt, at, U)) dzUU
+	…	q (u∖X, a,r)) q (z 1 ∖x, a,r))…q (ZT ∖zτ -1 ,x, a,r))log
JUJZ1	J ZT
+	…	q (u∖x, a,r：) q (Z1 ∖x, a,r：)…q (ZT ∖zt -1 ,x, a,r))log
JUJZ1	J ZT
P(U)
q(u∖ X, a, r)
P(z I)
d zdu
q(z1 ∖X, a, r)
d zdu
logpθ(X ~,~)
τ	八
XLLι L
T
+ £ / / ... / q(u∖x,a,r))q(Z1 ∖x,a,r^∖…q(ZT∖zτ-1 ,x,a,r))log P ： Zj1,>-1-ʌuzuU
M Uu JZ1	JZT	q (zt∖zt-1,x, a, r)
T
X /
t =1 JU JZt
q (u∖x, a,r) q (Zt∖zt-1 ,X, a,r)log (P (Xt ∖zt,u) P (at∖zt,u) P (rt +1 ∖zt,at,u)) d ZtdU
+ q q(u∖x,a,r.)log / P(U) τ∖dU
JU	q(u∖x, a,r)
+ [ q(z 1 ∖x, a, r) log	P(:I)	uZ1
Z1	q(z1∖ X, a, r)
T
+ £ /	/ q(zt∖zt-1 ,X, a,r))log P Zt "-1,7-1τdztdzt-1
M JZt-I JZt	q(zt∖zt-1,x,a,r)
T
E2	E	[log p (xt∖zt,u) + log p (at∖zt,u ) + log p (r+1 ∖zt,at,u)]
T^^ Zt〜q(Zt ∖Zt-1 ,x,a,r)
t =1	U 〜q( U∖x,a,r)
—	KL (q (u∖x, a, r) ∖∖p (u))
—	kl (q(Z1 ∖x, a, r,) ∖∖p(ZI))
T
-	T	E	[KL (q(Zt∖zt-1 ,X, a,r)∖∖p(Zt∖zt-1, at-1))] ∙
t=2 Zt-1 〜q(Zt-1 \Zt-2 ,x,a,r)
15
Under review as a conference paper at ICLR 2019
C Variational Lower Bound for MORIN
logpθ(X ~,~)
= log
/ pθ(X a,r, Z)
dz
≥ / q@ (z∖χ, a,r)iog
pθ (x, a,r,Z) d Z
qφ(z∖x, a,r,) Z
/ qφ (z∖χ, a,r)iog
p(zι) [∏T=Ip(xt∖zt)p(at∖zt)p(r +ι∖zt,at)] [∏二2p(zt∖zt-1,at-1)]
qφ (z∖χ, a,r)
/ q (Z ι ∖χ, a,r)
• ∙ q (ZT ∖zt - ι,x, a,r)
l P(ZI) ∏T=IP(χt∖zt)P(at∖zt)P(Irt+ι∖zt,at) ∏T=2P(zt∖zt-ι,at-1)dZ
g	， I TT f	( I	TT f
q(Zι ∖x, a, r) ••• q(zτ ∖zτ-1 ,x, a,r)
• / q(zι ∖x, a,V))
q(ZT∖zτ-1, x, a, r) log (P(xt∖zt)P(at∖zt)p(Irt+1 ∖zt, at)) dz
+/ ,
JZ1
I
JZT
q(z1 ∖x, a, r)
••• q (zτ ∖zτ - 1,x, a,r)iog
P( Z I)	d Z
q (Z1 ∖x, a,r)
τ
+ X /1 Lt
q (Z1 ∖x, a,r) • • • q (ZT ∖zτ -1, x, a, r) log
P(Zt∖zt-1, at-1) d-
-7 i T T ~→Td Z
q(zt∖zt-1, x, a, r)
T r
X L1
d z
T
X
t =1 JZt
q(Zt∖zt-1, x, a, r) log (P(xt∖zt)P(at∖zt)p(Irt+1 ∖zt, at)) dZt
+ [ q(Z1 ∖x, a, r) log	P(ZI)	dZ1
Z1	q(z1 ∖ x, a, r)
T
+ X /	/ q(zt∖zt- 1,x,a,r)log P 2t Zj1,二1LdZtdZt-1
M JZt-IJZt	q ( zt∖zt-1,x,a,r )
T
E2	E	[log P (xt∖zt) + log P (at∖zt) + log P (r +1 ∖zt,at)]
t=1 Zt 〜q( Zt∣Zt-1 ,x,a,r)
—KL (q(Z1 ∖x,a,r：)∖∖p(Z1))
T
-72	, e	JKL (q(zt∖zt-1 ,x,a,r))∖∖p(zt∖zt-1 ,at-1))] ∙
t=2 Zt-1 〜q(Zt-1 Zt^-2 ,x,a,r)
D Bi-directional LSTM
In our inference model, we use a similar architecture of bi-directional LSTM to that in (Krishnan
et al., 2017). Apart from different inputs, the main difference is to introduce at-1 to computing the
16
Under review as a conference paper at ICLR 2019
combined hidden feature as shown in the following formula.
hcombined = 4 ^tanh(Wz ∙ Zt-1 + bz ) + tanh(Wa ∙ at-1 + ba ) + 淖" + h，飘
μt = Wμ ∙ hcombined + bμ
σt = softplus (W^σ2 ∙ hcombined + bσ2 )
E Training Details
As mentioned in the main body, DRL consists of two steps: learning a deconfounding model and
optimizing a policy based on the learned deconfounding model. To be more specific, in step 1,
given the observational data (~x, ~a, ~r), we learn the deconfounding model by optimizing the varia-
tional lower bound as presented in Equation (19). Once the deconfounding model is learned, we
know the state transition function p(zt|zt-1, at-1) and can also calculate the deconfounding reward
function p(rt|zt, do(at)) according to Equation (20). In step 2, we treat the learned deconfounding
model as a RL environment like CartPole in OpenAI Gym, and directly exploit it to generate trajec-
tories/rollouts through the state transition function and the deconfounding reward function. On the
basis of the generated trajectories/rollouts, we can train the policy network using Equation (21).
F	Example of Kidney Stones
F.1 Example of a proxy variable
For convenience, we directly move Figure 1 of (Louizos et al., 2017) here, as shown in Figure 4.
F.2 Intuition
Take for example the case of kidney stones, in the existence of the confounder (i.e., the size of
stones), one prefers treatment b when considering the overall probability of recovery, whilst one
chooses with more probabilities treatment a when investigating the recovery rate in each size. The
correct answer is treatment a in this example. The do-operator in Equation (2) of Section 2.3 pro-
vides a formal approach to seeking the correct solution to such confounded problems, which can not
be addressed within the traditional RL framework, because the traditional framework only involves
the conditional probability (e.g., the overall probability of recovery) instead of the interventional
probability as do-operator does.
17
Under review as a conference paper at ICLR 2019
Figure 4: Example of a proxy variable. t is a treatment, e.g., medication; y is an outcome, e.g.,
mortality; Z is an unobserved confounder, e.g., socio-economic status; and X is noisy views on the
hidden confounder Z, say income in the last year and place of residence.
G Analysis of the Deconfounding Model
At each time step t, from the deconfounding model presented in Figure 1(b), we can extract two 4-
tuple components, centering at zt (Figure 5) and u (Figure 6), respectively. It is apparent to observe
that both Figure 5 and Figure 6 share the exact same structure with Figure 4.
Figure 5: The component of 4-tuple (zt, xt, at, rt+1).
Figure 6: The component of 4-tuple (u, xt, at, rt+1).
18
Under review as a conference paper at ICLR 2019
H Confounding Datasets
H. 1 Confounding CartPole
The confounding CartPole dataset can be implemented in the exact same manner as Confounding
Pendulum, except for the action which is originally binary and can be naturally divided into two
categories.
H.2 Confounding MNIST
We follow the same setting to develop a confounding MNIST dataset, but the definitions of action
and the original reward term are different. Similar to the Healing MNIST dataset (Krishnan et al.,
2015), rotations of digit images are encoded as the actions ~a (-45 ≤ a ≤ 45) which, according to
the binary confounder u, is divided into two categories (22.5 ≤ |a| ≤ 45) and (0 ≤ |a| < 22.5).
The original reward term ro is defined as the minus degree between the upright position and the
position the digit rotates to. For example, if the digit rotates to the position of 3 o’clock or 9 o’clock,
then both rewards are -90.
H.3 Details about Confounding S imulations
The probabilities between the confounder, action and reward are presented as follows,
p(u = 0) = 0.8,	p(u = 1) = 0.2;				(24)
p(a = T1 |u = 0) = 0.24, p(a = T2 |u = 0) = 0.76;				(25)
p(a = T1 |u = 1) = 0.77, p(a = T2 |u = 1) = 0.23;				(26)
p(rc = R1 |a = T1 , u = 0) = 0.93,	p(rc =	R2 |a = T1 , u = 0) = 0.07;		(27)
p(rc = R1 |a = T2 , u = 0) = 0.87,	p(rc =	R2 |a = T2, u = 0) = 0.13;		(28)
p(rc = R1 |a = T1 , u = 1) = 0.73,	p(rc =	R2 |a = T1 , u = 1)	0.27;	(29)
p(rc = R1 |a = T2 , u = 1) = 0.69,	p(rc =	R2|a = T2, u = 1)	0.31;	(30)
μrc = R1 = -1, μrc = R 2 = -200,	σ = 2.			(31)
Note that, in our model rc is a function of u, at, and zt . We omit the notation of the state in
the conditional probability of rc because zt is fixed at each time step, which does not affect the
probability. To help readers understand this setting, we can make a straightforward analogy. More
specifically, the confounder u stands for the socio-economic status where u = 0 represents the poor
and u = 1 the rich. Treatment T1 is more expensive but more valid than Treatment T2 . R1 means
a healthier feedback/recovery than R2 . Equation (24) says that the poor are much more than the
rich (it is indeed in reality). Equation (25) and Equation (26) tell us the fact that the poor tend to
choose the cheaper but less valid treatment T2 whilst the rich prefer the more expensive but more
valid treatment T1 , which also makes sense in the real world. Finally, Equations (27)-(30) reveal
that T1 is a better treatment than T2 within both poor and rich population.
I Additional Experimental Results
As shown in Figure 7 and Figure 8.
J Experimental Settings
As shown in Table 1 and Table 2.
19
Under review as a conference paper at ICLR 2019
O 50 IOO	O 50 IOO
50 IOO
train data
50 ιoo
test reconstruction
50 ιoo
Counterfactual
reasoning
50	100	O 50 ιoo
train reconstruction	test data
Figure 7: Reconstruction and counterfactual reasoning on the confounding MNIST dataset. Top
row: results from the model without the confounder u (Morin); Bottom row: results from the model
with the confounder u (Mdecon). The sequences boxed in yellow have non-consecutive squares.
(a)
-0.2	0.0	0.2
Training
-0.2 0.0	0.2	0.4
Testing
(b)
Training	Testing
Figure 8: (a) Plot of 128 data points sampling from the posterior approximate of u on the confound-
ing MNIST dataset; (b) Plot of 128 data points sampling from the posterior approximate of u on the
confounding CartPole dataset;
Hyperparameter	Value
Deconfounding Model	
learning rate	0.0001
dimension of zt	50
dimension of xt	784
dimension of at	1
dimension of rt	1
dimension of u	2
dimension of LSTM unit	100
batch size	128
number of steps	5
number of epoch	400
DecOnfOunding AC	
number of episodes	^^1500-
number of time steps in each episode	200
capacity of the replay memory	100,000
batch size	128
sample size of U	200
Table 1: Hyperparameters for deconfounding reinforcement learning. Note that, we set the dimen-
sion u to 1 in Section 4.4 for simplicity.
20
21
Function
Architecture
3 2 4 6
九λzλλλλλ
Deconfounding Model
FC512 → Conv7 32 → Conv1416 → Conv28,ι → FC784 → {sigmoid, softplus)
{FCioo7 FC100} T FC200 T FC200 T FCl T {tanh, softplus}
{FC100, FC100, FC100} → FCIoo → FCIoo → FCl → (sigmoid, softplus}
{FCιoθ, FC100} T FCioo T FCioo T FC50 T {None, softplus}
{[Conv5,i6 → Conv5,32 → ConV5,32 → FC100],FC100,FC100} → FCIoo → FCIoo → LSTMIo0,5 → FCl → {None, softplus}
{[C0nv546 T COnV5,32 T COnV5,32 T FCιoo]7FCioo7FC100} T FCioo T FCioo T LSTMIc)C),5 T FC50 T {None, softplus}
Conv5jl6 → Conv5 32 → Conv5 32 → FC1 → (tanh, softplus}
{[C0nv5,i6 → ConV5,32 → ConV5,32 → FeIO0]了ClO0} → FClOO → FCl → {sigmoid, SoftPIus}
Deconfounding AC
UnderreVieW as a ConferenCe PaPersICLR 2019
V（々；。V）
开（血1々;夕）
FCsoo T FC300 T {None, softplus}
FC300 T FC300 T {tanh, softplus}
Table 2: Architectures for deconfounding reinforcement learning. Here FCk stands for a fully-connected layer with k units, ConVkm for a convolution layer with
n filters of size k × k, LSTMn t for a LSTM layer rolling out for t steps with latent size of n, {∙} for the parallel operators, and [∙] for the sequential operators. In
default, ECk and ConVkm are followed by a softplus activation layer and a batch-norm layer, which are omitted here for simplicity. Note that, in our setting, the
two functions in each pair {f2i-1, f2i}i=1... 8 share the same parameters except for the last layer.