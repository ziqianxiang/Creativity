Under review as a conference paper at ICLR 2019
Provab le Guarantees on Learning Hierarchi-
cal Generative Models with Deep CNNs
Anonymous authors
Paper under double-blind review
Ab stract
Learning deep networks is computationally hard in the general case. To show any
positive theoretical results, one must make assumptions on the data distribution.
Current theoretical works often make assumptions that are very far from describ-
ing real data, like sampling from Gaussian distribution or linear separability of the
data. We describe an algorithm that learns convolutional neural network, assum-
ing the data is sampled from a deep generative model that generates images level
by level, where lower resolution images correspond to latent semantic classes. We
analyze the convergence rate of our algorithm assuming the data is indeed gen-
erated according to this model (as well as additional assumptions). While we do
not pretend to claim that the assumptions are realistic for natural images, we do
believe that they capture some true properties of real data. Furthermore, we show
that on CIFAR-10, the algorithm we analyze achieves results in the same ballpark
with vanilla convolutional neural networks that are trained with SGD.
1	Introduction
The success of deep convolutional neural networks (CNN) has sparked many works trying to un-
derstand their behavior. As various theoretical studies have shown, learning deep networks is com-
putationally hard in the general case, when no assumptions on the distribution of the data are taken
(see for example Livni et al. (2014)). In practice, learning CNNs is done successfully using simple
gradient-based optimization algorithms like SGD. Hence, to provide a theoretical analysis that will
explain the practical success of deep learning, one must make assumptions on the distribution of
the learned data. Currently, theoretical works in the literature of deep learning make rather strong
assumptions, that clearly do not capture the properties of natural data. For example, many works
assume that the examples are sampled from a Gaussian distribution, an assumption that is very far
from describing distributions on natural images. Other works assume linear separability of the data,
which clearly does not hold for any rich enough dataset.
In this work, we assume the data is generated from a deep generative model. According to this
model, the examples are generated in a hierarchical manner: each example (image) is generated by
first drawing a high-level semantic image, and iteratively refining the image, each time generating a
lower-level image based on the higher-level semantics from the previous step. Similar models were
suggested in other works as good descriptions of natural images encountered in real world data.
While we do not claim that natural images actually come from such distribution, we believe that it
captures some key properties of real world data. Importantly, the problem we study is not trivially
learned by simple “shallow” learning algorithms.
Our work analyzes the training of a CNN on data from this generative distribution. In a shallow case,
where the generative model has only two levels of hierarchy, we analyze the behavior of standard
gradient-descent. In the deep case, where our model can have many levels of hierarchy, we analyze
a layerwise optimization algorithm, proving its convergence under the assumed generative model
(as well as additional, admittedly strong, assumptions). The algorithm we analyze is somewhat
different than optimization algorithms that are commonly used in practice, and may seem “tailored”
to solve the problem of learning our generative heirarchical model. We show that implementing this
algorithm to learn real-world data (CIFAR-10 dataset) achieves performance that are in the same
ballpark as a vanilla CNN trained with SGD-based optimizer. This result hints that our model and
algorithm indeed capture properties of distributions and algorithms that are common in practice.
1
Under review as a conference paper at ICLR 2019
2	Related Work
Any theoretical work that aims to give positive results on learning deep networks must make as-
sumptions on the data distribution and on the learning algorithm. We can roughly divide such works
into three categories: (1) works that study practical algorithms (SGD) solving “simple” problems
that can be otherwise learned with “shallow” algorithms. (2) works that study problems with less
restrictive assumptions, but using algorithms that are not applicable in practice. (3) works that study
a generative model similar to ours, but either give no theoretical guarantees, or otherwise analyze an
algorithms that are not applicable for learning CNNs on image data.
Trying to study a practically useful algorithm, Daniely (2017) proves that SGD learns a function that
approximates the best function in the conjugate kernel space derived from the network architecture.
Although this work provides guarantees for a wide range of deep architectures, there is no empirical
evidence that the best function in the conjugate kernel space performs at the same ballpark as CNNs.
The work of Andoni et al. (2014) shows guarantees on learning low-degree polynomials, which
is learnable via SVM or direct feature mapping. Other works study shallow (one-hidden-layer)
networks under some significant assumptions. The works of Gori & Tesi (1992); Brutzkus et al.
(2017) study the convergence of SGD trained on linearly separable data, which could be learned
with the Perceptron algorithm, and the works of Brutzkus & Globerson (2017); Tian (2017); Li &
Yuan (2017); Zhong et al. (2017) assume that the data is generated from Gaussian distribution, an
assumption that clearly does not hold in real-world data. The work of Du et al. (2017) extends
the results in Brutzkus & Globerson (2017), showing recovery of convolutional kernels without
assuming Gaussian distribution, but is still limited to the regime of shallow two-layer network.
Another line of work aims to analyze the learning of deep architectures, in cases that exceed the
capacity of shallow learning. The works of Livni et al. (2014); Zhang et al. (2015; 2016a) show
polynomial-time algorithms aimed at learning deep models, but that seem far from performing well
in practice. The work of Zhang et al. (2016b) analyses a method of learning a model similar to CNN
which can be applied to learn multi-layer networks, but the analysis is limited to shallow two-layer
settings, when the formulated problem is convex.
Finally, there have been a few works suggesting distributional assumptions on the data that are
similar in spirit to the generative model that we analyze in this paper. Again, these works can be
largely categorized into two classes: works that provide algorithms that have theoretical guarantees
but are not applicable for learning CNNs, and works that show practical results without theoretical
guarantees. The work of Arora et al. (2014) shows a provably efficient algorithm for learning a
deep representation, but this algorithm seems far from capturing the behavior of algorithms used
in practice. Our approach can be seen as an extension of the work of Mossel (2016), who studies
Hierarchal Generative Models, focusing on algorithms and models that are applicable to biological
data. Mossel (2016) suggests that similar models may be used to define image refinement processes,
and our work shows that this is indeed the case, while providing both theoretical proofs and empirical
evidence to this claim. Finally, the works of Tang et al. (2012); Patel et al. (2016); Van den Oord &
Schrauwen (2014) study generative models similar to ours, with promising empirical results when
implementing EM inspired algorithms, but giving no theoretical foundations whatsoever.
3	Distributional Assumptions
In this paper, we are concerned with the problem of learning binary classification of images. Assume
we are given a sample S = {(X1, y1), . . . , (Xn, yn)} from some distribution D on Rm×m × Y,
where Y = {±1} is our labels. We wish to learn a CNN model that will classify the images
correctly. As noted, if we hope to give any guarantees on the success of learning such model, we
must make strong assumptions on the distribution D. One naive assumption on D is that it is linearly
separable: there exists W * ∈ Rm×m such that y〈W *, X)≥ 1 for (X, y)〜D, where We denote
hA, Bi = tr(A>B). Clearly, such assumption does not hold for any rich enough distribution of
natural images (as linear classifiers give poor results on natural image datasets).
To provide a more realistic assumption, we will instead assume the following:
Assumption 1 The images in D are generated from a latent distribution G0, such that G0 is linearly
separable.
2
Under review as a conference paper at ICLR 2019
*×→ U {
X∈R640×512
CO = {
sky
,...}
■,幽,0,国…},C1 ={^^1, cloud , sun J, . . . }
Figure 1:	Generative model schematic description
Thus, sampling an image (X, y)〜D is equivalent to sampling a latent representation (X(0), y)〜
G0, and then sampling (X, y) 〜 GX(0), where GX(0)is a distribution dependent on X(0). Note
that in this case, the distribution D can be very complex, although the images sampled from it
have a latent representation that is relatively simple. To give a concrete description of the latent
representation, we will assume that G0 is a distribution over “semantic images”: images where each
“pixel” represents a semantic class. Formally, we will assume that X(0) ∈ C0m0 ×m0 , where C0 is
some finite set of “semantic classes”.
Now, we want to describe the generative process of generating the image X from the semantic
representation X(0) . This process is done in a hierarchical manner: Given the high-level semantic
representation X(0), where each “pixel” represents a semantic class (for example, background, sky,
grass etc.), we generate a lower level image X(1) , where each patch comes from a distribution de-
pending on each “pixel” of the high-level representation, generating a larger semantic image (lower
level semantic classes for natural images could be: edges, corners, texture etc.). We can repeat this
process iteratively any number of times, each time creating a larger image of lower level semantic
classes, thus generating a sequence of increasingly large semantic images X(0) , X(1) , . . . , X(d) .
Since D is a distribution over greyscale images, we will assume that the last iteration of this process
samples patches over R, i.e X = X(d) ∈ Rm×m. This model is described schematically in figure 1,
with a formal description given in section 3.1. In section 3.2 we describe a synthetic example of digit
images generated according to this model.
3.1	Formal Description
Here, we will give a detailed description of the generative process described above. To generate an
example, We start by sampling the label y 〜U(Y), where U(Y) is the uniform distribution over the
set of labels. Given y , we generate a small image with m0 × m0 pixels, where each pixel belongs to
a set C0. Elements ofC0 corresponds to semantic entities (e.g. “sky”, “grass”, etc.). The generated
image, denoted X(0) ∈ C0m0 ×m0 , is sampled according to some simple distribution Dy. Next, we
generate a new image X(1) ∈ C1m0s×m0s as follows. Pixel i in X(0) corresponds to some c ∈ C0.
For every such c, let Dc be the uniform distribution over a finite set of patches Sc(0) ⊂ C1s×s , where
we refer to s as a “patch size”. We assume the following:
Assumption 2 The sets {Sc(0)}c∈C0 are disjoint, and each one is of size k.
So,	pixel i in X (0) whose value is c ∈ C0 generates a patch of size s in X (1) by sampling the
patch according to Dc. This process continues, yielding images X(2) , . . . , X(d) whose sizes are
m0s2 × m0s2 , . . . , m0 sd × m0 sd. Each pixel in level i comes from Ci , and each patch comes from
Sc(i-1) for some c ∈ Ci-1. We assume that Cd = R, so the final image is over the reals. The observed
example is the pair (X(d), y). We denote the distribution generating the image of level i by Gi.
3
Under review as a conference paper at ICLR 2019
As noted, we assume that G0 , the distribution that generates the high-level semantic images, is
linearly separable. As G0 is a distribution over semantic images, i.e over C0m×m × Y, we need to
define what “linear separability” means in this case. To do this, we assign to the i-th class in C0 the
unit vector e% ∈ R1C0|. Then We represent X ∈ Cm×m With an equivalent tensor X ∈ RlC0l×m×m,
and define linear separability with respect to the tensor representation. Namely, there exists W* such
that for (X, y)〜G0 it holds that y〈X, W*i ≥ 1, where〈X, W* := Pij 卜 Xj ∙ Wij k.
,,	,,
Notice that our semantic classes partition the patches in the image into disjoint sets. There are
various ways to partition the patches, some of which might in fact generate the same distribution.
For the analysis, we add an assumption that ensures that the semantic classes defined in the model are
different enough from each other. We identify each class with a matrix that captures the frequency
of its appearance in the image, with respect to the label of the image. For this, we define the “labeled
frequency matrix” of class c ∈ Ci to be the matrix Fc ∈ Rm0 si ×m0 si defined as follows: for every c
we denote by Ic the operator that takes a matrix as its input and replaces every element of the input
by the boolean that indicates whether it equals to c. Then, we define: Fc := E(z,y)^Gi [yIc(Z)].
Notice that Fc is the “mean” image over the distribution for every given semantic class c. For
example, semantic classes that tend to appear in the upper-left corner of the image for positive
images will have positive values in the upper-left entries of Fc . We now assume:
Assumption 3 The vectors {Fc}c∈Ci are linearly independent in pairs.
For each c1, c2 ∈ Ci we denote the angle between Fc1 and Fc2 by: ∠(Fc1 , Fc2) :=
arccos (卜.彳；FcikF)∙ Denote θ := mini,cι,c2∈c ∠(Fc1, Fc2) and λ := mini,c∈q ||久||尸.From
the linear independence assumption it follows that both θ and λ are strictly positive. The runtime of
the algorithms described in the next sections depend on 1∕θ and 1∕λ.
3.2	Synthetic Digits Example
To demonstrate our generative model, we use a small synthetic example to generate images of digits.
In this case, we use a three levels model, where semantic classes represent lines, corners etc. In the
notations above, we use:
Co = { 口,厂「，厂「，「」，厂「} , Ci = { □,「〕口不，「}, C = R
We define Deven , Dodd to be the uniform distributions over the even/odd digital representations:
Deven
U{
}
Now, in the second level of the generative model, each pixel in Co can generate one of four possible
manifestations. For example, for the pixel 口 , we sample over: 十 , 1 , 一 , 十 .
Similarly, in the final level we sample for each C ∈ Ci from a distribution Dc supported over 4
elements. For example, for the pixel Q , we sample over: ∣ ∣ ,	∣ , ∣	, ∣ ∣ .
Notice that though this example is extremely simplistic, it can generate 49 examples per digit in the
first level, and 490 examples for each digit in the final layer, amounting to 9 ∙ 490 ≈ 1.38 ∙ 1055
different examples. Figure 2 shows the process output.
3.3	Linear Separability
While the generative model described above seems complex, one might wonder whether this model,
along with all the distributional assumptions given so far, is in fact an overly complicated fashion
to describe a rather simple problem. While generally speaking, it is not clear how to measure the
“complexity” of the suggested distribution, we can at least address the question of whether this
distribution is linearly separable or not.
It is immediate to show that when all the generated patches are orthogonal to each other, this distri-
bution becomes linearly separable. We analyze this case in the next section, and prove some results
in this simplistic case. On the other hand, when such assumption is not taken, we argue that the
model is typically far from being linearly separable. To show this, we generate examples using this
4
Under review as a conference paper at ICLR 2019
H1HBBMHΠHIHH
HII≡≡I≡I≡I≡H≡ISI
SIBIBaiBISSHBa
BΠ≡≡I9≡≡HBS
HliaBIHBnHIIIIH
Figure 2:	Left: Image generation process example. Right: Synthetic examples generated.
Experiment	θ	λ	Linear	CNN	OUrS
k = 30	-0:96-	-0ΠT	^065-	1.0-	1.0-
k = 40	1.64	0.16	0.58	1.0	1.0
k = 50	1.65	0.18	0.58	0.99	1.0
Figure 3: Linear and CNN classifiers on generated data
generative distribution, where the generated patches are chosen randomly. Recall that learning a
linear separator is a convex problem. Therefore, if the generated data was linearly separable, finding
such a separator would be trivial, using the SGD algorithm. Figure 3 shows the accuracy of the
linear classifier, compared to the performance of a simple CNN and our algorithm. As is clear from
these results, a linear separator achieves very poor results on this data. Furthermore, to show that
the generated distribution does not break Assumption 3, we show the values of θ and λ, as measured
on the generated data.
The exact details of the experiment are given in appendix D.
4	Two-Level Model
As a warm-up, we first limit ourselves to observing distributions with only two levels in the hi-
erarchy. Hence, the process of generating examples is simply: G0 (X (0) , y) G1 (X(1),y).
Furthermore, in this section we add an important assumption on the data distribution:
Assumption 4 The set of patches that compose the images generated by G1 is orthonormal. Recall
that we denoted Sc(0) ⊂ C0s×s the set of patches that are sampled for each pixel of class c. Our
assumption means that ∀P1, P2 ∈ {Sc(0)}c∈C0 we have hP1, P2i = 1P1 =P2.
Notice that this assumption implies the following lemma:
Lemma 1 GiVen assumptions 1, 2, 4, Gi is linearly separable with margin Λ∕k∣∣W*k, where W*
is the linear separator of G0 (see section 3.1). Consequently, running linear SVM finds a classifier
with zero classification loss on this distribution in O(kkW* k2) iterations.
While usually in classification problems our goal is to minimize the training loss, in this case we
would like to find an algorithm that instead recovers the latent representation X (0) given the raw
image X(1) . The reasons for this requirement will become clear in the next section. We will
prove that training a two layer linear CNN with gradient-descent (GD) on distribution G1 implicitly
recovers the latent representation, even if G0 is not linearly separable. Note that this is a rather
surprising result, as we are only observing (X(1), y), and have no access to the latent representation
X(0). The following section will give the details of this result: in section 4.1 we describe the details
of the GD algorithm, and in section 4.2 we describe the theoretical analysis of this algorithm.
5
Under review as a conference paper at ICLR 2019
4.1	Algorithm: GD on Two-Layer CNN
Recall that for (X, y)〜Gι, X is an image of size sm° X sm0, that is composed of S X S patches
generated from a high-level semantic image of size m0 × m0 . To simplify notation, we consider
X to be “reshaped” such that each column is a patch in the original image (the so called “im2col”
operation), so X ∈ Rs2 ×m02. Given K ∈ Rs2×n, W ∈ Rm20×n, we define a convolutional subnet
to be a function NK,W : Rs2×m02 → R such that: NK,W (X) = hW>, K>Xi.
This is equivalent to a convolution operation on an image, followed by a linear weighted sum:
multiplying X by K > is equivalent to performing a non-overlapping convolution operation with
kernels k1, . . . , kn on the original image (where ki is the i-th vector of matrix K). Flattening the
resulting matrix and multiplying by the weights in W yields the second linear layer.
The top linear layer of the network outputs a prediction for the label y ∈ Y . The network is trained
with respect to the loss LSK,W on a given set of examples S, defined as the expected value of some
label dependent loss function 'y : R → R
LKW = E(X,y)~S ['y INKW(X))]
For the analysis, We use the loss 'y (y) = -yy. This loss simplifies the analysis, and seems to
capture a similar behavior to other loss types used in practice.
Although in practice We perform a variant of SGD on a sample of the data to train the netWork,
we perform the analysis with respect to the population loss: Lk,w = E(X,y)~g ['y(Nk,w(X))].
We denote Kt the Weights of the first layer of the netWork in iteration t, and denote W0 the initial
weights of the second layer. For simplicity of the analysis, we assume that only the first layer of the
network is trained, while the weights of the second layer are fixed. Thus, we perform the following
update step at each iteration of the gradient descent: Kt = Kt-ι 一 η∂KLκt-1,W0.
We initialize W0 〜N(0,1) and each n-dimensional column of Ko is sampled from the uniform
distribution on the sphere of radius 2√n, where σ is a parameter of the algorithm.
4.2	Theoretical Analysis
In this section, we will consider a more general case, where we do not assume that G0 is linearly
separable (Assumption 1). As noted, our main claim in this section is that training the two-layer
Conv net implicitly recovers the latent semantic representation of the image. Specifically, we show
that gradient-descent finds an embedding of the observed patches into a space such that patches from
the same semantic class are close to each other, while patches from different classes are far. Note
that this property is indeed enough to recover the latent representation, as running a trivial clustering
algorithm on the embedded patches would reconstruct the latent image. Recall that we do not
have access to the latent distribution, and thus cannot possibly learn such embedding directly. This
surprising property of gradient descent is the key feature that allows our main algorithm (described in
the next section) to learn the high-level semantics of the images. Our claim is given in the following
theorem:
Theorem 1 Suppose that assumptions 2, 3, 4 hold. Let θ, λ as described in section 3.1. Assume
we train a two-layer network of size n > 音储 log(ICɪ) With respect to the population loss on
distribution Gi, with learning rate η, for T > S+；" iterations, for some γ > 0. Assume that the
training is as described in section 4.1, where the parameter σ of the initialization is also described
there. Then with probability of at least 1 一 δ:
1.	for each C ∈ Co ,for every x1,x2 ∈ S(O) we get ∣∣K> ∙ xi — K> ∙ x2k < σ
2.	for ci, c2 ∈ Co, if ci = c2 ,forevery xi ∈ S(O),x2 ∈ S(O), we get ∣K> ∙ xi — K> ∙ x2k > Y
We give a similar analysis for the SGD algorithm in appendix B.
6
Under review as a conference paper at ICLR 2019
5	Deep Model
Now, assume we are given data from a deep generative distribution as described in section 3 (with
d ≥ 2), and our goal is to learn a classifier that predicts the label for each image. A reasonable ap-
proach, given the properties of the above distribution, would be to try to infer from the raw image the
higher-level semantic representations. If given an example (X, y) we succeed to infer the sequence
that generated it, X(0) , X(1) , . . . , X(d), we could then use SVM on the high-level representation,
and learn to infer its label.
Given the results of the previous section, we can recover these semantic representations by training
a two-layer Conv net and applying a trivial clustering on the resulting patches. Note that in general
we do not assume that all the patches are orthogonal (Assumption 4), as such a property will make
the whole model linearly separable. Thus, to apply the results of section 4.2, we would use the
clustering algorithm both for standard clustering and also as an “orthogonalization” step, that will
map different clusters to orthonormal vectors.
We next describe in details the full algorithm (section 5.1), and show that it finds a network that
achieves zero loss in polynomial time (section 5.2).
5.1	Algorithm Description
The algorithm we suggest is built from three building-blocks composed together to construct the full
algorithm: (1) clustering algorithm, (2) gradient-based optimization of two-layer Conv net and (3)
a simple classification algorithm. In order to expose the latent representation of each layer in the
generative model, we perform the following iteratively:
(1)	Run a k-means algorithm on patches of size s × s from the input images defined by the previous
step (or the original images in the first step), w.r.t. the cosine distance, to get ki cluster centers.
(2)	Run a convolution operation with the cluster centroids as kernels, followed by ReLU with a
fixed bias and a pooling operation. This will result in mapping the patches in the input images to
(approximately) orthogonal vectors in an intermediate space Rki .
(3)	Initialize a 1x1 convolution operation, that maps from ki channels into n channels, followed
by a linear layer that predicts Y . We train this two-layer subnet using gradient-descent. As an
immediate corollary of the analysis in the previous section, this step implicitly learns an embedding
of the patches into a space where patches from the same semantic class are close to each other, while
patches from different classes are far away. This lays the groundwork for the clustering step of the
next iteration.
(4)	“Throw” the last linear layer, thus leaving a trained block of Conv(s×s)-ReLU-Pool-Conv(1 ×1)
which finds a “good” embedding of the patches of the input image, and repeat the process again,
where the output of this block is the input to step 1.
Finally, after we perform this process for d times, we get a network of depth d composed from
Conv(s × s)-ReLU-Pool-Conv(1 × 1) blocks. Then, we feed the output of this (already trained)
network to an SVM algorithm that learns a linear classifier, training it to infer the label y from the
semantic representation that the convolution network outputs. We now describe the building blocks
for the algorithm, followed by the definition of the complete algorithm.
Clustering. The first block of the algorithm is the clustering step. We denote KMEANS(S, k) to be
the output of the k-means++ algorithm (Arthur & Vassilvitskii (2007)) running on sample S to find k
clusters, w.r.t the cosine similarity. We assume the algorithm returns a mapping φS : Rs → Rk, such
that if xi, xj ∈ S are in the same cluster then φS(xi) = φS (xj), and otherwise φS(xi) ⊥ φS (xj).
For the consistency with common CNN architecture, we can use the centroids of each cluster as
kernels for a convolution operation. Combining this with ReLU with a fixed bias and a pooling
operation gives an operation that maps each patch to a single vector, where vectors of different
patches are approximately orthogonal.
Two-Layer Network Algorithm We denote TLGD(S, T, η, n, σ) the GD algorithm, training a two-
layer Conv net of width n, on sample S, with learning rate η forT interations. This algorithm returns
the weights of the first layer KT. The details of this algorithm are described in section 4.1.
7
Under review as a conference paper at ICLR 2019
Classification Algorithm Finally, the last building block of the algorithm is a classification stage,
that is used on top of the deep convolution architecture learned in the previous steps. As we show, at
this stage the examples are linearly separated, so we can use the SVM algorithm to find large margin
linear separator. We denote SVM(S) the output of running SVM on sample S.
Complete Algorithm Utilizing the building blocks described previously, our algorithm learns a
deep CNN layer after layer. This network is used to infer the label for each image. The algorithm
is described formally in algorithm 1. In the description, We use the notation φ * A to denote the
operation of applying a map φ : K0m0 → K1m1 on a tensor A, replacing patches of size m0 by
vectors in Km1. Formally: φ * A :=［。㈠:①Μ。….+：!)^。儿
Algorithm 1 Deep LayerWise Clustering
input:
numbers η, T, n, σ, k1 , . . . , kd
sample S = {(X1,y1),...(XN,yN)} ⊆Rm0sd×m0sd ×Y
hd — id
for i = d . . . 1 do
set Si — {(hi(Xι),yι),...,(hi(XN ),yN)} // construct sample using the current network
set Pi — {patches of size S × S from Si} // generate patches from the current sample
set φi — KMEANS(Pi, ki) // cluster the sampled patches
set Si J {(φi * hi(Xι),yι),..., (φi * hi (Xn ),yN)} // map patches to orthogonal vectors using φi
if i > 1 then
set Ki-ι J TLGD(Si,T, η,n, σ) // train a two-layer net to find a “good” embedding for the patches
set hi-1 J Ki>-1 (φi * hi) // add the current block to the network
end if
end for
_ , ^..
set h J SVM(So)
return h ◦ h1
5.2	Theoretical Analysis
In this section we show that, algorithm 1 learns in polynomial time (with high probability) a network
model that correctly classifies the examples according to their labels.
Recall that we have shown that training a two-layer network implicitly learns an embedding of the
patches into a space where patches from the same semantic class are close to each other, while
patches from different classes are far. Using this, we show that performing clustering + two-layer
network training iteratively, layer by layer, leads to revealing the underlying model, and hence the
algorithm’s convergence. We claim that algorithm 1 successfully learns a model that correctly classi-
fies the examples sampled from the observed distribution Gd. This is stated in the following theorem:
Theorem 2 Suppose that assumptions 1, 2, 3 hold. Fix δ > 0, and let δ0 = 2d. Denote
C := maxi<d |Ci| the maximal number of semantic classes in any level i. Choose σ = S,
n >	02∏3θ	log( C0),	T > (C∕√η+2σ)k,	kι	=	∣C0∣,...,kd-i	=	∣Cd-2∣,kd	= k∣Cd-i∣.	Thenw.p
≥ 1 - δ, running algorithm 1 with parameters γ, η, T, n, σ, k1 , . . . , kd on distribution Gd returns
hypothesis h s.t P(x,y)〜Gd (h(X) = y) = 0, and runs in time O (d(C2k2 + T) + k∣∣W*k2).
6	Experiments
A common criticism of theoretical results on deep learning is that they fail to account for the em-
pirical success of deep networks. Indeed, negative results show that learning deep networks is
computationally hard, while in practice efficient algorithms like SGD achieve remarkably good per-
formance. Positive results, on the other hand, often make very strong assumptions that clearly do not
hold in practice, like assuming the inputs are sampled from Gaussian distribution, or that they are
linearly separable. Other positive results make less restrictive assumptions, but analyze algorithms
that are very far from algorithms that are used in practice. At first glance, our work may seem to
suffer from the same common drawbacks of positive theoretical results: we describe a distribution
that is admittedly synthetic, and for deep models we analyze an algorithm that seems “tailored” to
8
Under review as a conference paper at ICLR 2019
Classifier	Accuracy(FC)	Accuracy(Linear)
-CNN	-0.759	0.735
CNN(Random)	0.645	0.616
CluStering+JL	0.586	0.588
Ours	0.734	—	0.689
Figure 4: Results of various configurations on the CIFAR-10 dataset
learn this distribution. To show that our model and algorithm do capture properties of distributions
and algorithms used in practice, we implemented our algorithm to learn a CNN on the CIFAR-10
dataset, comparing it to a vanilla CNN trained with a common SGD-based optimization algorithm.
As our aim is to show that our algorithm achieves comparable result to a vanilla SGD-based opti-
mization, and not to achieve state-of-the-art results on CIFAR-10, we do not use any of the common
“tricks” that are widely used when training deep networks (such as data augmentation, dropout,
batch normalization, scheduled learning rate, averaging of weights across iterations etc.). We im-
plemented our algorithm by repeating the following steps twice: (1) Sample N patches of size 3x3
uniformly from the dataset. (2) For some `, run the K-means algorithm to find ` cluster centers
ci... e`. (3) At this step, We need to associate each cluster with a vector in R', such that the image
of this mapping is a set of orthonormal vectors, and then map every patch in every image to the
vector corresponding to the cluster it belongs to. We do so by performing Conv3x3 operation with
the ' kernels ci... e`, and then perform ReLU operation with a fixed bias b. This roughly maps each
patch to the vector ei, where i is the cluster the patch belongs to. (4) While our analysis corresponds
to performing the convolution from the previous step with a stride of 3, to make the architecture
closer to the commonly used CNNs (specifically the one suggested in the Tensorflow implementa-
tion Google-Brain (2016)), we used a stride of 1 followed by a 2x2 max-pooling. (5) Randomly
initialize a two layered linear network, where the first layer is Conv1x1 with `0 output channels,
and the second layer is a fully-connected Affine layer that outputs 10 channels to predict the 10
classes of CIFAR-10. (6) Train the two-layers with Adam optimization (Kingma & Ba (2014)) on
the cross-entropy loss, and remove the top layer. The output of this block is the output of these steps.
Repeating the above steps twice yields a network with two blocks of Conv3x3-ReLU-Pool-Conv1x1.
We feed the output of these steps to a final classifier that is trained with Adam on the cross entropy
loss for 100k iterations, to output the final classification of this model. We test two choices for this
classifier: a linear classifier and a three-layers fully-connected neural network. Note that in both
cases, the output of our algorithm is a vanilla CNN. The only difference is in the training algorithm.
To calibrate the various parameters that define the model, we first perform random parameter search,
where we use 10k examples from the train set as validation set (and the rest 40k as train set). After
we found the optimal parameters for all the setups we compare, we then train the model again with
the calibrated parameters on all the train data, and plot the accuracy on the test data every 10k
iterations. The parameters found in the parameter search are listed in appendix E.
We compared our algorithm to several alternatives. First, the standard CNN configuration in the
Tensorflow implementation with two variants: CNN+(FC+ReLU)3 is the Tensorflow architecture
and CNN+Linear is the Tensorflow architecture where the last three fully connected layers were
replaced by a single fully connected layer. The goal of this comparison is to show that the perfor-
mance of our algorithm is in the same ballpark as that of vanilla CNNs. Second, we use the same two
architectures mentioned before, but while using random weights for the CNN and training only the
FC layers. Some previous analyses of the success of CNN claimed that the power of the algorithm
comes from the random initialization (see Daniely (2017)), and only the training of the last layer
matters. As is clearly seen, random weights are far from the performance of vanilla CNNs. Our last
experiment aims at showing the power of the two layer training in our algorithm (step 6). To do so,
we compare our algorithm to a variant of it, in which step 6 is replaced by random projections (based
on Johnson-Lindenstrauss lemma). We denote this variant by Clustering+JL. As can be seen, this
variant gives drastically inferior results, showing that the training step of Conv1x1 is crucial, and
finds a “good” embedding for the process that follows, as is suggested by our theoretical analysis.
A summary of all the results is given in figure 4.
9
Under review as a conference paper at ICLR 2019
References
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with
neural networks. In International Conference on Machine Learning, pp. 1908-1916, 2014.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some
deep representations. In International Conference on Machine Learning, pp. 584-592, 2014.
David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pp. 1027-1035.
Society for Industrial and Applied Mathematics, 2007.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pp. 2419-2427, 2017.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv
preprint arXiv:1709.06129, 2017.
G. Google-Brain. Tensorflow. https://www.tensorflow.org/tutorials/deep_cnn,
2016.
Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 14(1):76-86, 1992.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training
neural networks. In Advances in Neural Information Processing Systems, pp. 855-863, 2014.
Elchanan Mossel. Deep learning and hierarchal generative models. arXiv preprint
arXiv:1612.09057, 2016.
Ankit B Patel, Minh Tan Nguyen, and Richard Baraniuk. A probabilistic framework for deep learn-
ing. In Advances in Neural Information Processing Systems, pp. 2558-2566, 2016.
Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey Hinton. Deep mixtures of factor analysers.
arXiv preprint arXiv:1206.4635, 2012.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Aaron Van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep
gaussian mixture models. In Advances in Neural Information Processing Systems, pp. 3518-3526,
2014.
Yuchen Zhang, Jason D Lee, Martin J Wainwright, and Michael I Jordan. Learning halfspaces and
neural networks with random initialization. arXiv preprint arXiv:1511.07948, 2015.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly
learnable in polynomial time. In International Conference on Machine Learning, pp. 993-1001,
2016a.
Yuchen Zhang, Percy Liang, and Martin J Wainwright. Convexified convolutional neural networks.
arXiv preprint arXiv:1609.01000, 2016b.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
10
Under review as a conference paper at ICLR 2019
A Proof of Lemma 1 and Theorem 1
Denote m = m0s the size of the images sampled from G1 (these images are in Rm×m). As in
section 4, we will consider the “reshaped” version of images sampled from G1, so we will have
X ∈ Rs2×m20. Equivalently, patches of the sampled images, that belong to sets {Sc(0)}c∈C, will be
vectors in Rs . Finally, We denote (Z, y)〜 Go the latent images in the generative process, where
2
Z ∈ C0m0×m0. Here we will instead consider these images as vectors in C0 0. Similarly, the labeled
frequency matrix of class c is Fc ∈ Rm0×m0, and we will consider it to be a vector Fc ∈ Rm20. In
the proof we use the notations Fc to denote the negative labeled frequency matrix:
Fc := E(z,y)〜Gi [-yIc(Z)]
We will start by proving Lemma 1:
Proof of Lemma 1: For simplicity, we can assume C0 = [C], for some natural C. Therefore,
the linear separator of Go is a matrix W * ∈ RC×m0. For (Z,y)〜Go define〈W *, Zi =
Pc∈[c] hW*,ZC(Z)i, where ZC is the operator that replaces each entry of Z with the boolean 1
if it equals c, or 0 otherwise. Notice that the definition of linear separability in section 3 is equiva-
lent to y〈W*, Zi ≥ 1 for every (Z, y)〜Go.
Denote P1,..., PC ∈ Rs2 such that Pi =
for every P ∈ S(O) we have P ∙ Pjj = L=j.
P ∈Si(0)
P. From the orthonormality of the patches,
22
Now, denote T : Rs ×m0 → R such that:
T (X )= X hW*,P>X i
j∈[C]
For (X, y)〜Gι, denote Z the latent representation of X, then by the definition of the generative
distribution it is easy to verify that Zj (Z) = P>X, and therefore:
T(X) = X hWj*,Zj(Z)i
j∈[C]
Since T is a linear function such that kT k2F = kkW *k2F, and using standard results on the SVM
algorithm, the conclusion of the lemma follows.
In the rest of this section, we will prove Theorem 2. We use the notations wi(t), ki(t) to denote the
i-th columns of Wt , Kt respectively.
For some class c ∈ Co and for some patch x0 ∈ SC(o), denote fx0 a function that takes a matrix X
and returns a vector fx0(X) such that the i’th element of fx0 (X) is the 1 if the i’th column of X,
denoted xi , equals to x0 and 0 otherwise. That is,
1x1 =x0
fχ0 (X):=	.
1xm=x0
Notice that from the orthonormality of the observed columns ofX it follows that: fx0(X) = X>x0.
We begin with proving the following technical lemma.
Lemma 2 For each c ∈ Co and for each x0 ∈ SC(o) we have:
E(X,y)〜Gi [-yfx0 (X)] = 1 Fc
k
Proof Denote DZ the distribution of (X⑴，y)〜Gi conditioned on X(O) = Z (the distribution of
the images generated from the latent image Z). Observe that
E(X,y)〜Gi [-yfχ0 (X)] = 2 X -yEZ〜Dy [EX〜DZ [fx0 (X)]]
2 y=±1
11
Under review as a conference paper at ICLR 2019
Recall that for each c ∈ C0, we have |Sc(0) | = k. Therefore, for each j ∈ [m20] we have:
2 X -yEz~Dy [EX ~Dz [fχ0 (X )j ]] = 2 X -yEZ~Dy [EX~Dz [1Xj=x0 ]]
2 y=±1	2 y=±1
=2 X -yEZ~Dy [PX~Dz (Xj = XO)]
2 y=±1
11
=2 工-yEZ~Dy k 1zj = c
y=±1
=k ∙ 2 X -yEZ~Dy [Ic(Z)j] = £ [Fc]j
y=±1
The next lemma reveals a surprising connection between the gradient and the vectors Fc .
Lemma 3 for every c ∈ C0 and for every X0 ∈ Sc(0):
x0^7^~LK,W = Twi ∙ Fc
∂ki	k
Proof For a fixed X and W, denote y(K) = Nk,w(X). Note that:
∂
∂kiy(K) = Xwi
So for X0 ∈ Sc(0) we have:
∂
X ∙∂kiy=(x) Xwi=wi∙fx0(X)
Combining the above With the definition of the loss function, 'y (y) = -yy, and With Lemma 2 We
get:
x0∂k LKt,Wo = E(X,y)~Gι x0∂k'y(y)
E
--0 d /
’(x,y)~Gι -y X ∂k y
E(x,y)~gJ-y W(O) ∙ fx0 (X)]
wi ∙ E(X,y)~Gι [-yfx0 (X)]
k wi0) ∙ Fc
As an immediate corollary We obtain that a gradient step does not change the projection of the kernel
on tWo vectors that correspond to the same class (both are in the same Sc(0)).
Corollary 1 For every t ≥ 0, i ∈ [n], for every semantic class c ∈ C and for every X1, X2 ∈ Sc(0) it
holdsthat: ∣k(t+1) ∙ xι — k(t+1) ∙ X2∣ = |k(t) ∙ xι — k(t) ∙ X2∣.
Proof From Lemma 3 We can conclude that for a given c ∈ C, for every X1, X2 ∈ Sc(0) We get:
∂∂
x1 ∂k LKt,Wo = x2 ∂k LKt,Wo
12
Under review as a conference paper at ICLR 2019
From the gradient descent update rule:
ki(t+1) = ki(t)
” d L
-η∂ki LKt,W0
And therefore:
lk(t+1) ∙ x1 - k(t+1) ∙ x2| = |(k(i)- η∂k LKt,Wo ) ∙ x1 - (k(t) - η∂k LKt,Wo ) ∙ x2|
∂ki	∂ki
=|k(t)∙ x1 - k(t)∙ x2 - (η∂k LKt,Wo x1 - η∂k LKt,Wo x2)|
=|k(t) ∙ X1 - k(t ∙ X2∣
Next we turn to show that a gradient step improves the separation of vectors coming from different
semantic classes.
Lemma 4 Fix c1, c2 ∈ C0. Recall that we denote ∠(Fc1 , Fc2) to be the angle between the vectors
Fci, Fc2. Then, with probability ∠(Fcι, F⅛ )∕∏ on the initialization of W(O) we get:
Sign(W(0) ∙ Fci) = Sign(W(O) ∙ %)
Proof Observe the projection of wi(O) on the plane spanned by Fc1 , Fc2 . Then, the result is
immediate from the symmetry of the initialization of W(O).	■
Lemma 5 Fix ci = c2 ∈ Co. Then, with probability of at least 0.23 ∠(Fc∏,Fc2) we get for ^ve^y
x1 ∈ Sc(iO), x2 ∈ Sc(2O):
Ik(T) ∙ xi - k(T) ∙ X2∣ > 1 ηTkFcik + kFc2k - 2σ
k2
Proof Notice that since w(0) 〜N(0,1), We get that w(0) ∙ FCj 〜N(0, kFcj k2) for j ∈ {1,2}.
Therefore, the probability that w(0) ∙ FCj deviates by at most 1 -std from the mean is erf( 2√2). Thus,
We get that:
P(∣w(0) ∙ Fcj ∣≤kFcj k)=erf(2-√=)
And using the union bound:
P(∣w(0) ∙ Fci I ≤ kFcik ∨ ∣w(0) ∙ Fc21 ≤ ∣∣Fc2k) ≤ 2erf(2√=) < 0.77
Thus, using Lemma 4, we get that the following holds with probability of at least 0.23 ∠(Fc∏,Fc2):
•	∣w(0) ∙ FciI > kFcik
•	∣w(0) ∙ Fc2∣> kFc2k
•	sign(w(0) ∙ Fci) = sign(w(0) ∙ %)
Assume w.l.o.g that w(0) ∙ Fci < 0 < w(0) ∙ F⅛, then using Lemma 3 we get:
T
k(T)xi = k(0)χ1 - η X ∂k(i) x1LKt,Wo
t=i
T
= k(0) - η X kw(0) ∙ Fci
t=i
=k(0) - 1 ηTw(0) ∙ Fci > 1 ηTʃ - σ
k	k2
13
Under review as a conference paper at ICLR 2019
In a similar fashion we can get:
k(T)X2 < — 1 ηTkFC2k- + σ
k2
And thus the conclusion follows:
k(T)xι - k(T)x2 > 1 ηT kFc1k + kFc2 k - 2σ
k2
■
Finally, we are ready to prove the main theorem.
Proof of Theorem 1.
We show two things:
1.	Fix c ∈ C0. By the initialization, we get that for every x1, x2 ∈ Sc(0) and for every i ∈ [n]:
|xi ∙ k(0) — χ2 ∙ k(0)| < √σn
Using Corollary 1, we get that:
|xi ∙ k(T) — χ2 ∙ k(T) | < √n
And thus:
kKT x1 — KTx2k < σ
2.	Let ci = c2 ∈ Co. Assume T > 3+；；". For i ∈ [n], from Lemma 5 We get that with
probability of at least 0.23 ∠(Fc∏,Fc2) > 0.23θ for every xi ∈ S(0), x2 ∈ sC0):
Ik(T)X1 — k(T)X21 > 1 ηT kFc1k + kFc2 k — 2σ > 1 ηTλ — 2σ > γ
k2	k
For a given ci 6= c2 ∈ C0, denote the event:
AC1,C2 = {∀i ∈ [n] : |k(T) ∙ x1 — k(T) ∙ x2| < γ, x1 ∈ S(0), x2 ∈ S(0)}
Then, from what we have showed, it holds that:
θθ
P (AC1,C2 ) < (I — 0.23 ∏ ) ≤ exp(-0.23nn )
Using the union bound, we get that:
θ
P(∃cι = c2 ∈ Co s.t Aci,c2 ) < exp(—0.23n—)|Co|
Choosing n > 02∏^ log(苧) We get P(∃cι = c2 ∈ Co s.t Aci,c2) < δ. Now, if for every
ci 6= c2 ∈ Co the event AC1,C2 doesn’t hold, then clearly for every Xi ∈ SC(1o), X2 ∈ SC(2o)
we would get kKt ∙ Xi 一 KT ∙ X2k > γ, and this is what we wanted to show.
B	Analysis of SGD
We show a theorem equivalent to Theorem 1, when using the SGD algorithm on samples from
distribution Gi, instead of updating on the population gradient with the GD algorithm. For this
analysis, we fix mo = 3 and take the columns of W to be initialized on the unit sphere.
14
Under review as a conference paper at ICLR 2019
Theorem 3 Assume that assumptions 2, 3, 4 hold. Assume we train a two-layer network of
size n > o2% log(亨)with SGD with batch size of 1 on samples from distribution Gi, for
T > max{ (γ+4λ))4k , 8nσm0 log2( 2咤 kn)} iterations, with learning rate η = T-3/4, for some
γ > 0. Then with probability of at least 1 - 2δ:
1.	for each C ∈ Cq ,for every x1,x2 ∈ S(O) we get ∣∣K> ∙ xi — K> ∙ x2k < 2σ
2.	for ci, c2 ∈ Cq , if ci = c2 ,forevery xi ∈ S(Q),x2 ∈ S(Q), we get ∣K> ∙ xi - K> ∙ x2k > Y
Assume we are given a sample (Xi, yi), . . . , (XT, yT) sampled i.i.d from distribution Gi, and we
run SGD with batch size of 1 (with respect to the same problem analyzed in the previous section).
Denote by yt the prediction of the network on iteration t. Denote g(t) the gradient with respect to
the kernel ki on iteration t.
Similar to Lemma 3, we get for every c ∈ CQ and x0 ∈ Sc(Q):
Xg(Itt = x0∂k'yt (yt) = -Iytx∂kw(Q) ∙ fχ0(X)
∂ki	∂ki
Using Lemma 3 along with the fact that (Xt, yt)〜i.i.d Gi We get that:
E(Xt,yt)〜GIhx0g(t)i = 1 W(Q) ^ Fc
Since we assume ∣wQ(i) ∣ ≤ 1, we get that:
∣ηx0gf)∣ ≤ ηkw(Q)kkfx0(Xt)Il ≤ ηmo
Therefore, fixing α > 0, and using Hoeffding’s bound we get:
T
Pr(| Xηx0g(t) — ɪW(Q) ∙ Fcl ≥ α) ≤ 2exp(-2α2∕(Tη2mQ))
t=i
And taking η = T -3/4 we get:
Pr(∣ ^Xηx0g(t) — η-^~W(Q) ∙ Fc∣ ≥ α) ≤ 2exp(-2√Tα2∕m2)
t=i
Denote by A the event where:
TT
∃i ∈ [n], ∃x0 ∈ ∪c∈CoS(Q) : ∣ EnxOgf)—工W(Q) ∙ FC| ≥ α
t=i
Taking T ≥ 2m⅛ log2(2lCTkn) and using union bound over all kernels ki, i ∈ [n] and patches in
∪c∈C0 Sc(Q) we get:
Pr(A) ≤ δ
Using the above, we get the following:
Corollary 2 With probability at least 1 — δ, we get:
Ik(T) ∙ xi — k(T) ∙ x2∣≤ √σn + 2α
15
Under review as a conference paper at ICLR 2019
Proof With probability at least 1 - δ, for every c ∈ C0 and for every x1 , x2 ∈ Sc(0) we get:
TT
Ik(T) ∙ xι - k(T) ∙ X21 = ∣k(0) ∙ xι - k(0) ∙ x2 - η X x0 ∙ g(t) + η X x0 ∙ g(t) |
t=1	t=1
T
≤	|k(0) ∙ χι - k(0) ∙ x2| + | X ηχιg(t) - kw(0) ∙ FcI
t=1
T
+	| X ηχ2g(t) - kw(0) ∙ FcI
t=1
σ
≤	-j= + 2α
n
We repeat the proof of Lemma 5, now assuming the updates are of SGD.
Lemma 6 Assume event A does not occur. Fix c1 6= c2 ∈ C0. Then, with probability of at least
0.23∠(Fc∏,Fc2) we getfor every xi ∈ S(0), x2 ∈ S(0)：
Ik(T) ∙ Xi - k(T) ∙ X2∣ > ηT(1 kFcιk + kFc2k - 2α) - 2σ
k2
Proof Denote V := 2(hw(0), Fcji/kFCj k + 1). The vector w(0) is distributed uniformly on the
sphere, and from spherical symmetry we can see that the distribution of v is independent of the
value of Fcj /kFcj k. Therefore, we can see that the distribution of v is simply the distribution
of the first coordinate of wi(0), normalized to [0, 1]. Since w0(0) is of dimension m20, we get that
v 〜Beta((m0 - 1)/2, (m2 - 1)/2)) = Beta(4,4). Therefore, we get:
P (0.4 ≤ v ≤ 0.6) < 0.44
And from this we get:
P(∣w(0) ∙ FcjI ≤ 0.2kFcj k) < 0.44
And using the union bound:
P(Iw(0) ∙ Fc11 ≤ 0.2kFcι k ∨ Iw(0) ∙ Fc21 ≤ 0.2k% k) < 0.88
Thus, using Lemma 4, We get that the following holds with probability of at least 0.12 ∠(Fc∏,Fc2):
•	Iw(0) ∙ Fci I > 0.2kFcι k
•	Iw(0) ∙ Fc21 > 0.2kFc2 k
•	Sign(W(0) ∙ Fci) = Sign(W(0) ∙ %)
Assume w.l.o.g that W(CI) ∙ Fc1 < 0 < W(CI) ∙ Fc2, then using what we have shown:
T
ki(T)Xi = ki(C)Xi - X ηXigi(t)
t=i
T
=k(0)xi - X ηχig(t) + ɪw(0) ∙ Ki -
t=i
T
≥ k(0)χι- ηT w(0) ∙ Fci-I T X
t=i
0.2T i/4
> -k-IIfCi k - σ - α
k
ηT w(0) ∙ Fci
rυ
(t) ηT (C)
Xigi	———w；	∙
FciI
k

16
Under review as a conference paper at ICLR 2019
In a similar fashion we can get:
(T)	0.2k 1/4
ki	x2 <------k--∣∣K2 k + σ + α
k
And thus the conclusion follows:
T	T 0 2T 1/4
k(T)xι — k(T)X2 > —k— (kFcιk + kFc2k) — 2σ - 2α
k
We can now repeat the main theorem for the SGD case:
Proof of Theorem 3.
Assume event A does not occur, and fix α = 2√n. We show two things:
1.	Using Corollary 2, we get that:
|xi ∙ k(T) - χ2 ∙ k(T) | < √n
And thus:
kKT x1 — KTx2 k < 2σ
2.	Let ci = c2 ∈ Co. Assume T > (ɔ(θ^j . For i ∈ [n], from Lemma 6 we get that with
ProbabilityofatleaSt 0.12∠(FC∏,Fc2) > 0.12∏ for every Xi ∈ S(0), X2 ∈ SC0):
T	T	0.2T 1/4	0.4T 1/4
Ik(T)xi - k(T)x2∣ > —— (kFcι k + kFc2 k) - 2σ - 2α > -ʒ-λ - 4σ > γ
kk
For a given ci 6= c2 ∈ C0, denote the event:
Acι,c2 = {∀i ∈ [n] : |k(T) ∙ x1 - k(T) ∙ x2| < Y, x1 ∈ S(O), x2 ∈ S(O)}
Then, from what we have showed, it holds that:
θθ
P(Ac1,c2) < (1 — 0.12-)n ≤ exp(-0.12n-)
Using the union bound, we get that:
θ
P(∃cι = c2 ∈ Co s.t Ac1,c2) < exp(-0.12n∏)∣Co∣
Choosing n > 02^ log(苧)we get P(∃cι = c? ∈ Co s.t 4内心)< δ. Now, if for every
ci 6= c2 ∈ Co the event Ac1,c2 doesn’t hold, then clearly for every Xi ∈ Sc(1o), X2 ∈ Sc(2o)
we would get kKt ∙ xi 一 KT ∙ x2k > γ, and this is what we wanted to show.
Since event A happens with probability at most δ, the conclusion follows.	■
C Proof of Theorem 2
Our algorithm uses the k-means++ algorithm, which is a variant of Lloyd’s algorithm where the
initial cluster centers are chosen with probability proportional to their distance to the closest cluster
that was already chosen. The algorithm first chooses a cluster center ci uniformly on all examples.
Any new cluster is chosen in the following way: assume we already chose cluster centers ci, . . . , cn,
denote D(X) := minj∈[n] kX - cj k, the minimal distance from example X ∈ S to the closest cluster
center. Then, the algorithm chooses an example X ∈ S with probability
cluster centers are chosen, we run the standard Lloyd’s algorithm for k-means.
D(x)2
Pχ0∈s D(x0)2
After the
The following lemma shows that k-means++ finds an optimal solution with high probability on
highly-clustered data:
17
Under review as a conference paper at ICLR 2019
Lemma 7 Fix δ > 0. Let S be a finite set that is “highly-clustered”: S is partitioned such that
S = ∪j∈[C] Bj, where the partition satisfies:
1.	For every x, y ∈ Bj it holds that kx - yk < 1.
2.	For every x ∈ Bi , y ∈ Bj such that i 6= j it holds that kx - yk > c∕√δ.
3.	All Bj-s are sets of fixed size: ∀i,j |Bi| = |Bj |.
Assume we run the k-means++ algorithm on set S to find C clusters. Then with probability at
least 1 - δ, the algorithm returns an “optimal clustering”: the centers returned by the algorithm
Xi,..., XC satisfy thatfor every j ∈ [C] there exist i ∈ [C] with Xi ∈ Bj.
Proof We will prove by induction that at the i-th step of choosing the centers, with probability
at least 1 - Cδ, the chosen centers Xi,..., Xi satisfy that there are no ii = i2 ∈ [i] such that
xil , Xi2 ∈ Bj :
•	for i = 1 this is immediate.
•	assume We chose Xi,..., Xi that satisfy the above condition, and w.l.o.g We can assume
that for j ≤ i we have Xj ∈ Bj. Then, for every X ∈ Bj with j ≤ i we have D(X) < 1,
and for every X ∈ Bj with j > i we have D(X) > C∕√7. Therefore, the probability of
choosing X ∈ Bj with j ≤ i in the next step is at most:
i	C -1 δ
i	+(C - i)C 2∕δ ≤ C 2∕δ ≤ C
Now, using the union bound we get the required.
Taking i = C proves the initialized center are already optimal. Clearly, any step of Lloyd’s
algorithm will maintain this property. In fact, the algorithm will converge after one step, returning
Xi,..., XC with Xj = CC Px∈Bj X.	■
Before we complete the proof of the theorem, we remind a few notations that were used in the
algorithm’s description. We use φi to denote the clustering of patches learned in the i-th iteration
of the algorithm, and Ki the weights of the kernels learned BEFORE the i-th step (thus, the patches
mapped by Ki are the input to the clustering algorithm that outputs φi). Note that we do not learn a
mapping in the last step, as from Lemma 1 as the distribution Gi is linearly separable, so we could
simply use SVM on the output of the network at that stage. Finally, we use the notations φ * A to
indicate that we operate φ on every patch of the tensor A. When we use operations on distributions,
for example h ◦ G or φ * G, we refer to the new distribution generated by applying these operation
to every examples sampled from G . The essence of the proof is the following lemma:
Lemma 8 Let G := Gd be the distribution over pairs (X, y), where X is the observed image over
the reals, and recall that for i < d, the distribution Gi is over pairs (X (i) , y) where X(i) is in a
space of latent semantic images over Ci. For every i ∈ [d], with probability at least 1 - 2(d - i)δ0,
there exists an orthonormal patch mapping 夕i : Ciy× → Rki such that φi * (h ◦ G)二夕i * Gi,
where φi and hi are as defined in algorithm 1.
The lemma tells us that the neural network at step i of the algorithm reveals (in some sense) the
latent semantic structure.
We will again use a “reshaped” notation: in every level i (the i-th step of the induction), we treat
2	22
patches P ∈ Cis×s as vectors P ∈ Cis . For a sub-image in the next level, denoted Z ∈Cis+×i s ,we
will denote Zj ∈ Cis+2 i to be the j -th patch of size s × s in the sub-image (generated by the class at
the j -th coordinate of higher level patch P ∈ Cis2 ).
We will prove by induction the following claim:
Lemma 9 Assume that for some i < d the condition of the lemma holds for i + 1, namely that
there exists an orthonormal patch mapping ψi+ι : Ciyi → Rki+1 such that φi+ι * (hi+i ◦ G)=
夕i+i * Gi+i. Then, with probability at least 1 — 2δ0 this condition holdsfor i.
18
Under review as a conference paper at ICLR 2019
Proof Let 夕i+ι be the mapping satisfying the condition of the claim for i + 1. Notice that the data
that is fed to the two-layer training step comes from the distribution 夕i+ι * Gi+ι, and satisfies the
conditions for the analysis of Theorem 1.
First, we will show that the set of patches Si , the set of all patches in the distribution hi ◦ G is
“highly-clustered” with probability at least 1 - δ0 .
For every C ∈ Ci, denote Bc = {K>ψi+ι(P) : P ∈ S(i)}. Recall that from our assumption,
hi ◦ G = K>(夕i+ι * Gi+ι). Therefore, We have that Si = ∪c∈CiBc. Now, from Theorem 1,
since we have T > k。^+2。) and σ = S, with probability at least 1 一 δ0, conditions 1 and 2 in
the “highly-clustered” definition hold. Since we assume that Sc(i) are of the same size, condition 3
follows.
From the above, using Lemma 7, with probability at least 1 一 2δ0 the set Si is “highly-clustered”
and k-means++ returns an optimal clustering. We will now limit ourselves to the event that both of
these hold.
Now, define the map ψi : Ci×× → Rk in the following way: first, for every patch P ∈ Cs we take
22
an arbitrary manifestation of the patch P in the next level, denoted Z ∈ Cis+×1 s . In other words, Z
could be any s2 × s2 sub-image in the next level that could be generated from the patch P . Now,
take 夕i(P) := φi(Ki ∙ 3i+ι * Z)). Then, the following holds:
1	.夕i(P) does not depend on the choice of Z: if Z, Z0 are two different manifestations
of P, then, from the definition of the generative model, for every j ∈ [s] it holds that
Zj, Zj0 ∈ SPj . Thus from what we have shown:
IlKi ∙2 i+1(Zj ) — Ki ∙2 i+1(Zj)Il < σ = S
Therefore:
s2
kKi ∙沔+1 * Z - Ki ∙沔+1 * Z0k2 = X kKi ∙ ψi+ι(Zj) — Ki ∙ Ψi+ι(Z,j)k2
j=1
<1
and since we get an optimal clustering, we have:
φi(Ki∙3i+ι* Z)) = φi(Ki ∙ Qi+1 * Z0))
2	. for every two patches P = P0 we get 夕K(P) ⊥ 夕K(P0): let Z, Z0 be the manifestations
of p, p0 respectively. Since P 6= P0 there exists j ∈ [s2] such that Pj 6= Pj0. From the
generative model it follows that Zj ∈ SP(i) and Zj0 ∈ SP(i)0, and therefore from the behavior
of the algorithm: ∣K> ∙ ψi+ι(Zj) 一 K> ∙夕i+ι(Zj) ∣ > C/√δ0 > k/√δ0. Therefore, we
get that ∣Ki ∙夕i+ι * Z 一 Ki ∙夕i+ι * Z01 > ki/ √7, and thus from the clustering optimality
we get ψi(P) ⊥ 夕i(P0).
Now, recall that in the algorithm definition: h = Ki ∙ (φi+ι * hi+ι). Using the assumption for
i + 1, we get:
hi ◦ G = Ki(ψi+ι * Gi+ι)
and from the definition of ψi and what we have shown we get:
φi * (hi ◦ G) = ψi * Gi
Proof [of Lemma 8] We prove by induction:
•	Note that an immediate property of the k-means++ algorithm is that it doesn’t choose the
same example twice. Therefore, ifwe run k-means++ on sample S to find |S| clusters, we
19
Under review as a conference paper at ICLR 2019
get a single cluster center for each element of S . From the definition of our model, there are
|Cd-1 |k patches in the observed image, and since we run k-means++ to find kd = |Cd-1|k
clusters, we get a cluster center for each patch. Hence, k-means++ returns an orthogonal
patch mapping φd , and since hd = id we get the required.
•	Assume the claim holds for i + 1, then with probability at least 1 - 2(d - i - 1)δ0 the
condition holds for i + 1. In such event, from Lemma 9 with probability at least 1 - 2δ0, it
holds for i. Therefore, with probability (1 - 2(d - i - 1)δ 0)(1 - 2δ0) ≥ 1 - 2(d - i)δ0 it
holds for i.
Proof [ of Theorem 2] From Lemma 8, after performing d iterations of the algorithm, we observe
that with probability at least 1 - 2dδ0 = 1 - δ distribution 夕 1 *Gι, where 夕 1 is an orthonormal patch
mapping. Therefore, using Lemma 1, it is linearly separable with margin kkW"2. Therefore, the
SVM algorithm finds an optimal separator in O(k k W* k2). To count the overall amount of iterations,
notice that the when running k-means++ on sample S to find C clusters, the initialization step at most
|S |C iterations and the algorithm itself converges after one iteration, so in our case its runtime is
bounded by C2k2. The runtime of the TLGD algorithm is T. We perform d iterations of clustering
and the TLGD algorithm, and finally run the SVM algorithm that converges in O(kkW* k2). So the
runtime of the whole algorithm is O (d(C2k2 + T) + k∣∣W*k2).
20
Under review as a conference paper at ICLR 2019
D	Synthetic Data Experiment
To confirm that our model is typically not linearly separable, we generated synthetic examples using
our model in the following way:
•	We chose Y = {±1}.
•	We fixed C0 = {0, 1, 2, 3}, and chose 4 vectors for each class, uniformly over C03×3. Notice
that overall there are 8 vectors labeled with ±1, and they are linearly independent with high
probability (we also validated that they are indeed independent). This process defines the
distribution G0, and it is therefore indeed linearly separable.
•	We fix C1 = {0, 1, 2, 3}, and for each c ∈ C0 we choose the set Sc(0) be drawing k vectors
uniformly over C13×3. This defines the distribution G1.
•	Finally, we choose C2 = R3, and for each c ∈ C1 we choose Sc(1) by drawing k vectors from
a normal distribution over C23×3. This defines the distribution G2 , which is the observed
distribution.
We run this model with different values of k (30, 40, 50). We then measure the values of θ and λ, to
show that Assumption 3 holds for this distribution.
We generate a sample of 50, 000 examples from this distribution, taking 40, 000 as train examples,
and 10, 000 as test. We train a linear classifier on this distribution, and display its performance on the
test data. For comparison, we also train a CNN with non-overlapping convolutions (convolutions of
3x3 and stride 3) with the ReLU activation. We use an architecture of two layers of convolution, and
a final readout layer, and train with respect to the logistic loss. This training is done with the Adam
optimizer in a standard end-to-end fashion. We also train the same architecture with our algorithm,
where we the training is done layer-by-layer.
E Parameters for CIFAR-10 Experiments
figure 5 below lists the parameters that were learned in the random parameter search for the different
configurations of the algorithm, as described in 4. The table lists the parameters used in each layer:
`1, `2 are the number of clusters for the first and second layer, and `01, `02 are the output channels of
the Conv1x1 operation for each layer. These parameters could be used to reproduce the results of
our experiments.
Classifier	N	kι		k2	k2	b	Accuracy
Ours+FC	47509	1377	155	3534	216	-0.14	0.734
Ours+Linear	32124	1384	97	2576	211	-0.63	0.689
Clustering+JL+FC	12369	^39-	^39-	184	T84-	0.84	0.586
CluStering+JL+Lineαr	57893	5004	345	6813	407	-0.01	0.588
Figure 5: Parameters used in our experiment
21