Under review as a conference paper at ICLR 2019
Predictive Local Smoothness for Stochastic
Gradient Methods
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic gradient methods are dominant in nonconvex optimization especially for
deep models but have low asymptotical convergence due to the fixed smoothness.
To address this problem, we propose a simple yet effective method for improving
stochastic gradient methods named predictive local smoothness (PLS). First, we
create a convergence condition to build a learning rate which varies adaptively
with local smoothness. Second, the local smoothness can be predicted by the latest
gradients. Third, we use the adaptive learning rate to update the stochastic gradients
for exploring linear convergence rates. By applying the PLS method, we implement
new variants of three popular algorithms: PLS-stochastic gradient descent (PLS-
SGD), PLS-accelerated SGD (PLS-AccSGD), and PLS-AMSGrad. Moreover, we
provide much simpler proofs to ensure their linear convergence. Empirical results
show that the variants have better performance gains than the popular algorithms,
such as, faster convergence and alleviating explosion and vanish of gradients.
1	Introduction
In this paper, we consider the following nonconvex optimization:
minχ∈Rd f (x) := 1 Pn=I fi(x),
(1)
where x is the model parameter, and neither f nor the individual fi (i ∈ [n]) are convex, such as, deep
models. Stochastic gradient descent (SGD) is one of the most popular algorithms for minimizing the
loss function in equation 1. It iteratively updates the parameter by using the product of a learning rate
and the negative gradient of the loss, which is computed on a minibatch drawn randomly from training
set. Unfortunately, small learning rate makes SGD painfully slow to converge, and high learning rate
causes SGD to diverge. Therefore, choosing a proper learning rate (or step size) becomes a challenge.
A popular adaptive method adjusts automatically the learning rate by using some forms of the past
gradients to scale coordinates of the gradient. For example, AdaGrad Duchi et al. (2011) is the first
adaptive algorithm to update the sparse gradients by dividing positive square root of averaging the
squared past gradients, and its several variants (e.g., Adadelta Zeiler (2012), RMSProp Tieleman &
Hinton (2012), Adam Kingma & Ba (2015), Nadam Dozat (2016), and AMSGrad Reddi & Kumar
(2018)) have been widely and successfully applied into train deep models. Specifically, they use the
exponential moving averages of squared past gradients to manage the rapidly decayed learning rate.
Most of these algorithms are to establish their convergence guarantees based on a maximum of the
Lipschitz constant L, which governs smoothness of the loss function in whole parameter space, often
called L-smoothness Bottou et al. (2016); Reddi et al. (2016).
However, the maximum of the L-smoothness results in a small learning rate as it is inversely
proportional to L1 Bottou et al. (2016). Since the L-smoothness changes with the parameter and
the given data, it leads to a local smoothness of the loss function near the optimum for selecting the
learning rate. Specifically, the local smoothness is often computed by the the Hessian matrix V2f (x),
such as data dependent local smoothness (e.g., local SVRG Vainsencher et al. (2015)) and parameter
dependent local smoothness (e.g., Newton method Nocedal & Wright. (2006)). Unfortunately, the
Hessian matrix results in a high computational cost and a high memory requirement. Recently,
1A large L results in a low learning rate to slowly move the loss function from a point f (x0) to local
minimum loss f (x*) with an equilibrium parameter x*, and vice versa.
1
Under review as a conference paper at ICLR 2019
Stochastic Quasi-Newton (SQN) Byrd et al. (2016) and Adaptive Quasi-Newton (adaQN) Keskar
& Berahas (2016a) are to partly reduce the expensive cost by using the Hessian-vector product
Nocedal & Wright. (2006). However, these algorithms are still difficult to train a large number of the
parameters of deep models due to the loop computation of the Hessian-vector product.
To address the above problem, we exploit a simple yet effective method to predict the parameter
dependent local smoothness for the adaptive learning rate. To ensure the convergence of the stochastic
algorithms, an ideal local smoothness L(xt) of the loss function is based on an ideal neighborhood
Nrxt(X*1 of an equilibrium x* With a radius ∣∣χt 一 x*k by using the current parameter Xt. However,
it is difficult to apply the Nrxt(X* into the stochastic algorithms due to the unknown equilibrium
X* . Since the past parameter Xt-1 has been known, we consider a local smoothness L(Xt) on a
neighborhood Nrx (Xt) of Xt with a radius ∣Xt 一 Xt-1 ∣. Because Nrx (X*) and Nrx (Xt) have
a common point Xt or region, L(Xt) is naturally used instead of L(Xt). Based on the definition of the
L-smoothness (or Lipschitz continuous gradients) Bottou et al. (2016), L(Xt) on this neighborhood
is easily predicted by using the past gradient Vf (Xt-I) and the current gradient Vf (xt), that is,
L(Xt) = gf(χχ)-Kxj-1)k2. Obviously, it only needs to storage the past parameter xt-ι and
gradient Vf(Xt-1), and perform twice subtractions and norm computations.
This paper, therefore, provides a local smoothness strategy to study the adaptive learning rate. In this
strategy there are two important problems that how to easily build a direct functional relationship
between the learning rate and the local smoothness, and how to calculate the local smoothness. To
address these problems, the stochastic gradient algorithms are transformed into a linear dynamical
system Lessard et al. (2016); Hu et al. (2017) by using the local smoothness to linearize the gradient
function. The functional relationship is obtained by constructing the convergence condition for the
linear dynamical system. Based on the above discussion, the local smoothness is simply predicted by
using the current gradient and the latest gradient. Overall, our main contributions are summarized as
•	We propose a predictive local smoothness (PLS) method to adjust automatically learning
rate for stochastic gradient algorithms. Our PLS method will lead these algorithms to drive
a loss function to fast converge to a local minimum.
•	We apply PLS into SGD, SQN Byrd et al. (2016), AMSGrad Reddi & Kumar (2018)
and AccSGD Kidambi et al. (2018). Correspondingly, we establish four PLS-SGD, PSL-
SQN, PLS-AMSGrad and PLS-AccSGD algorithms, and provide corresponding theoretical
conditions to ensure their linear convergence.
•	We also provide an important empirical result that PLS can alleviate the exploding and
vanishing gradients in the classical algorithms (e.g., SGD, AMSGrad Reddi & Kumar (2018)
and AccSGD Kidambi et al. (2018)) for training deep model with least squares regression.
Note that due to the limited space PLS-AccSGD is provided in subsection 6.1 in Supplementary
Material. We do not provide PSL-SQN since it is similar to PLS-SGD. But, we empirically verify
that PSL-SQN is faster than SQN in the subsection 4.1.
2	Preliminaries
In this section, we introduce some notations, local smoothness assumption, and three popular
stochastic gradient algorithms: SGD, SQN Byrd et al. (2016), AMSGrad Reddi & Kumar (2018),
and AccSGD Kidambi et al. (2018).
Notation. Vf (X) denotes exact gradient of f at X, while Vfit (X) denotes a stochastic gradient
of f, where it is sampled uniformly at random from [1, ∙ ∙ ∙ ,n] and n is the number of samples.
Since it is sampled in an independent and identically distributed (IID) manner from {1, ∙ ∙ ∙ ,n},
the expectation of smoothness Lit (Xt) is denoted as L(Xt) = E [Lit (Xt)] = n PZi Li(Xt). A '1
or '2-norm of a vector X is denoted as ∣∣x∣, and its square is ∣∣x∣∣2. A positive-definite matrix A is
denoted as A * 0. The Kronecker product of matrices A and B is denoted as A 0 B. We denote
a d × d identity matrix by Id. A neighborhood of a point X1 ∈ Rd with radius rx2 is denoted as
Nrx2 (xi) = {y ∈ RdIIlXI- y∣ < S = IM- X2k}.
2
Under review as a conference paper at ICLR 2019
Assumption 1. We say f is local smoothness on a set C ⊂ Rd if there is a constant L such that
l∣Vf (X)-Vf (y)k ≤ Lkx -yk, for ∀ x,y ∈ C.
(2)
Assumption 1 is an essential foundation for convergence guarantees of most stochastic gradient
methods as the gradient of f is controlled by L with respect to the parameter vector.
SGD. Stochastic Gradient Descent (SGD) simply computes the gradient of the parameters by uni-
formly randomly choosing a single or a few training examples. Its update is given by
xt+1 = xt - ηtVfit (xt),
(3)
where η is the learning rate. η is usually set to a decay form η0∕√t in practice. This setting leads to
slower convergence. Moreover, the gradient of the loss of a deep model with rectified linear units
(ReLU) Nair & Hinton (2010) often explodes when large initialization η0 .
SQN. Stochastic Quasi-Newton (SQN) Byrd et al. (2016) employs iterations of the form
xt+1 = xt - ηtHtVfit (xt),
(4)
where Ht+1 = (I - ρkskykT)Ht(I - ρkykskT) + ρkskskT with sk = xt - xt-1, yk = Vfit (xt) -
Vfit-1 (xt-1) and ρk = ykTsk. In practice, the quasi-Newton matrix Ht+1 is not formed explicitly,
and is computed by the Hessian-vector product Nocedal & Wright. (2006).
AMSGrad. AMSGrad Reddi & Kumar (2018) is an exponential moving average variant of the
popular Adam algorithm Kingma & Ba (2015) in the scale gradient method. AMSGrad uses the
factors β1t = β1∕t and β2 to exponentially move the momentums of the gradient and the squared
gradient, respectively. β1t = β1 = 0.9 and β2 = 0.999 are typically recommended in practice. The
key update is described as follows:
mt+1 = β1tmt + (1 - β1t)Vfit (xt),
vt+1 = β2vt + (1 - β2) (Vfit(xt))2 ,	m
vt+1 = max{vt+l, vt}, xt+1 = xt - ηt √t+1-.
(5)
vbt+1
AccSGD. Accelerated SGD (AccSGD) proposed in Jain et al. (2017) is much better than SGD, HB
Polyak (1964) and NAG Nesterov (1983) in the momentum method. An intuitive version of AccSGD
is presented in Kidambi et al. (2018). Particularly, AccSGD takes three parameters: learning rate ηt,
long learning rate parameter K ≥ 1, statistical advantage parameter ξ ≤ √κ, and α =1 一 0.72ξ∕κ.
This update can alternatively be stated by:
mt+1 = αmt + (1 ― α) (xt ―器 Vfit (xt)),
xt+1 = 0.7+0(i-α) (Xt- ηtvfit(xt))+ 0.7+-lα-α)mt+1.
(6)
3	Predictive Local Smoothness
The popular adaptive learning rate methods are based on using gradient updates scaled by square roots
of exponential moving averages of squared past gradients Reddi & Kumar (2018). These methods
indirectly adjust the learning rate as they can be essentially viewed as gradient normalization. In this
section, we study the local smoothness to directly and adaptively adjust the learning rate, propose
a predictive local smoothness (PLS) method, and apply this method into SGD, AMSGrad Reddi &
Kumar (2018), and AccSGD Kidambi et al. (2018). Before showing our PLS method, we first give
two local smoothness sequences definition.
Definition 1. Let x* be an equilibrium of the local minimum f (x*) and {xt}t≥0 be a updating
parameter procedure, where xo is an initial point. A corresponding neighborhood sequence of x* is
denoted by {Nrx (x*)}t≥0, where rxt = kx* - xtk. An ideal local smoothness sequence ofx* is
defined as {L(xt)}t≥0 which satisfies that,
kVf(x*) - Vf(y)k ≤ L(xt)kx* -yk, for ∀y∈Nrxt(x*).
(7)
A backward neighborhood sequence on {xt}t≥1 is denoted by {Nrx (xt)}t≥1, where rxt-1 =
kxt - xt-1 k. A backward local smoothness sequence is defined as {L(xt)}t≥1 which satisfies that
kVf(xt) - Vf(y)k ≤ L(xt)kxt - yk, for ∀y ∈ Nrxt-1(xt).
(8)
This definition reveals that L(xt) governs the ideal local smoothness between the equilibrium x* and
xt to strictly ensure the convergence of the updating parameter procedure. However, L(xt) cannot be
computed due to the unknown x*. Clearly, L(xt) is easily calculated by using xt and xt-1.
3
Under review as a conference paper at ICLR 2019
Procedure PLS
•	Build the learning rate η = η(L(xt)) by linearizing Vf (xt).
•	Predict the local smoothness L(Xt)
gf(Xt)-Vf(xt-ι)k
∣∣xt-xt-ι ∣∣+eι
•	Apply η(L(χt)) 8 L(X，+_ to update xt+i by any stochastic gradient algorithms.
Figure 1: Predictive Local Smoothness
3.1 PLS method
PLS is an adaptive learning rate method based on the local smoothness. The local smoothness
varies based on the updating parameters {xt}t≥0 in stochastic gradient algorithms. According to the
definition 1, the local smoothness sequence {L(xt)}t≥0 are ideally used to adjust the learning rate in
the neighborhood sequence {Nrxt (x*)}t≥o∙ However, it is difficult to compute {L(χt)}t≥o because
of the unknown x*. Since there is a common point sequence on {χt}t≥ι in both {Nrxt_ 1 (xt)}t≥ι
and {Nrxt (x*)}t≥o, {L(xt)}t≥ι can be predicted by {L(χ∕}t≥ι, which is easily computed by using
the current gradient and the latest gradient. In this paper, our key idea is to use the predictive local
smoothness sequence {L(xt)}t≥1 instead of the unknown {L(xt)}t≥0 to adjust automatically the
learning rate ηt . PLS is described as the following three steps.
Building adaptive learning rate with local smoothness. We firstly create a functional relationship
between ηt and L(xt) by using the convergence conditions of stochastic gradient algorithms. Fol-
lowing the local smoothness in equation 7, since Vf(x*) = 0, Vf(xt) is linearized by Vf(xt) =
(L(Xt) 0 Id) (Xt — χ*). Using L(Xt) instead of L(Xt), we consider the following linearization
Vf(Xt) = (L(Xt) 0 Id)(Xt- x*),	(9)
where L(Xt) is computed by equation 10 in the neighborhood Nrx (Xt), and L(Xt) = kV2f(Xt)k
if f is twice continuously differentiable. Stochastic gradient algorithms can use the linearization in
equation 9 to transform it into a simple time-varying linear system. The convergence of the algorithms
is achieved by studying the stability of this linear system Lessard et al. (2016); Hu et al. (2017).
Therefore, the stability condition is naturally used to construct the functional relationship between ηt
and L(Xt), ηt = η(L(Xt)). This shows that the learning rate is adaptively tuned by L(Xt).
Predicting the local smoothness. We secondly predict the local Lipschitz constant L(Xt) by using
the current gradient Vf(Xt) and the latest gradient Vf(Xt-1). By using the local smoothness in
equation 7, L(Xt) on Nrx (Xt) is predicted by
L(Xt)
IIVf(Xt)-Vf(Xt-I)II
IlXt - Xt-Ill + ei
(10)
where e1 is a parameter to prevent IXt - Xt-1 I going to zero. This predictive Lipschitz constant
L(Xt) is utilized to adjust automatically the learning rate ηt for computing the parameter Xt+1. In the
next subsections, we prove that η is inversely proportional to L(Xt), ηt a (l/(L(Xt) + e2)), where
e2 is another parameter to avoid the learning rate to be over large in the later updating process.
Applying the adaptive learning rate into any stochastic gradient algorithms. We thirdly use
the adaptive learning rate ηt = η(L(Xt)) to update the parameter Xt+1 in the stochastic gradient
algorithm. Overall, Fig. 1 summarizes the proposed adaptive local smoothness method. This method
can be applied into the adaptive method based on exponential moving averages (e.g., AdaGrad Duchi
et al. (2011), Adadelta Zeiler (2012), RMSProp Tieleman & Hinton (2012), Adam Kingma & Ba
(2015), Nadam Dozat (2016) and AMSGrad Reddi & Kumar (2018)), the momentum methods (e.g.,
HB Polyak (1964), NAG Nesterov (1983) and AccSGD Jain et al. (2017); Kidambi et al. (2018)) and
the Quasi-Newton methods (e.g., SQN Byrd et al. (2016)). Next, we will apply PLS into SGD, and
AMSGrad Reddi & Kumar (2018) to show its effectiveness. Moreover, due to the limited space, PLS
is also used to improve AccSGD Kidambi et al. (2018) in subsection 6.1 in Supplementary Material.
Remark 1. In essence, both PSL and SQN Byrd et al. (2016) have a similarity and a difference.
The similarity is that they calculate the Quasi-quadratic differential to accelerate the convergence
of stochastic gradient methods. The difference is that SQN employs the Quasi-Newton matrix by
4
Under review as a conference paper at ICLR 2019
equation 4 and PSL uses the predictive local smoothness by equation 10. Compared to SQN, PSL has
two advantages. First, PSL requires low computational time and low memory since it only needs to
storage the past parameter and gradient, and perform twice subtractions and norm computations.
Second, SQN can still be improved by using our PLS, and we empirically verify that PLS-SQN is
better than SQN in subsection 4.1.
Remark 2. Compared to the related local smoothness methods Kpotufe & Garg (2013); Johnson &
Zhang (2013); Vainsencher et al. (2015), which require that the loss function is twice differentiable,
PLS only requires the differentiable loss function. In addition, the unknown global smoothness is
estimated along with the optimization Malherbe & Vayatis (2017), while we provide an effective
method to predict unknown local smoothness using equation 10.
3.2 PLS-SGD
In this subsection, we introduce a PLS-SGD algo-
rithm. By using the linearization in equation 9 and
computing the expectation, the updating rule of SGD
is converted into the linear system:
χt+ι - χ* = ((1 - ηtLit(χt)) Z Id)(Xt - χ*), (11)
where it is sampled in an IID manner from Ω =
{1, ∙∙∙ , n}. Then, the convergence condition of S-
GD is obtained by employing the stability condition
of the linear system in equation 11, which shows that
xt converges to x? at a given linear rate ρ. Now we
Algorithm 1 PLS-SGD.
1 2	input： no, eι, S. : initialize: X0.
3	for t = 1,…，T — 1 do
4	Randomly pick it from {1, ∙∙∙ , n};
5	:	gt = Vfit (Xt);
6	Lit (xt) = Ukgt-gt-1k	; it	t	kxt-xt-1k+1
7	= =no nt = (Lit(Xt) + C2);
8	:	Xt+1 = Xt 一 ntVfit (Xt);
9	: end for
present a linear convergence condition for the SGD as follows.
Theorem 12. Consider the linear system in equation 11. Assume that it is sampled in an IID manner
from a uniform distribution, the assumption 1 holds and there exists an equilibrium x* ∈ Rd such
that Vf (x*) = 0. Let μ = 1 En=I Li(Xt), and V = 1 En=I Li(Xt)∙ Forafixed linear convergence
rate 0 < ρ < 1 ,if μ2 > (1 一 ρ2)ν holds, then we have μ―√μ -V(I-P ) < 小 < μ+√μ -V(I-P ),
and the linear system is exponentially stable, that is, kXt - X* k2 ≤ ρtkX0 - X* k2.
Theorem 1 provides a simple condition for the linear convergence of SGD, which benefits from our
PLS method. The condition reveals that the functional relationship between ηt, μ and V. In the
updating process, since it is sampled uniformly at random from Ω, it only uses μ = Lit (Xt) and
v = L2t (xt). Hence, we have (1 一 ρ)∕Lit (Xt) < ηt < (1 + ρ)∕Lit (Xt) and set η = ηo∕Lit (xt),
where η0 is an initialized learning rate and 1 一 ρ ≤ η0 ≤ 1 +ρ, and Lit (Xt) can be predicted by using
the equation 10. Similar to 1, 2 is another parameter to stop Lit(Xt) going to zero, and avoid the
learning rate to be over large in the latter updating process. In our PLS-SGD algorithm, therefore, the
learning rate η is set to no/(Lit (Xt) + e2)∙ The adaptive learning rate results in that PLS-SGD has a
faster (linear) convergence rate than the traditional SGD. PLS-SGD is summarized in Algorithm 1.
3.3 PLS-AMSGRAD
Similar to PLS-SGD, we integrate the proposed PLS method into the classical adaptive method
based on exponential moving averages, AMSGrad Reddi & Kumar (2018), and propose a PLS-
AMSGrad algorithm. The Lipschitz linearization in equation 9 is used to linearize the updating rules
in equation 5 of AMSGrad as:
mt+1
Xt+1 一 X*
(Ait ㊈ Id) (XmtX*)
Xt 一 X
where Ait
(1 一 β1t)Lit (Xt)
1 一 (i-βιt)ηtLit(Xt)	, (12)
vbt+1
bt+1 = max{vt+ι,bt}, vt+ι = (β2 0 Id) Vt + ((1 — βz)L2t (Xt) Z) Id) (Xt — x*)2. In fact, mt is
the momentum method to manage the velocity of the gradient. Using this linearization, we provide a
much simpler convergence analysis of AMSGrad by studying the linear system in equation 12. The
linear convergence condition of AMSGrad is described as
2All the proofs for the Theorems and the linear systems are provided in Supplementary Material.
5
Under review as a conference paper at ICLR 2019
Theorem 2. Consider the linear system in equation 12. Assume that it is sampled in an IID manner
from a uniform distribution, the assumption 1 holds and there exists an equilibrium x* ∈ Rd such
that Vf (x*) = 0. For a fixed linear convergence rate P = maxt{ √β1t}, if there exists a 2 X 2
positive definite matrix P 0 such that
1n
n χ AT PAi- PP Y 0,
i=1
or thefollowing condition holds, for any it ∈ Ω = {1, ∙ ∙ ∙ , n},
(13)
(1 - √βιt) pvt+1	1
ʒ-ʃʒ <ηt <
Lit(xt)
1 + √ Bit
(1 + √β∖t) Pvt+1	1
1 - √β1t	Lit (Xt)，
(14)
then the linear system is exponentially stable, that is,
mt+1
xt+1 - x*
≤ Jcond(P)Pt
2
m0
x0 - x*
where cond(P) is the Condition number of P and Cond(P) = σι(P)∕σp(P), where σι(P) and
σp (P) denote the largest and smallest singular values of the matrix P.
Compared to the convergence analysis of AMSGrad
in Reddi & Kumar (2018), Theorem 2 establishes
simpler conditions in equation 13 and equation 14
for its linear convergence. The 2 × 2 linear matrix
inequality (LMI) condition in equation 13 is built
by using the control theory (e.g., integral quadratic
constraint Lessard et al. (2016); Hu et al. (2017)) to
Algorithm 2 PLS-AMSGrad.
1:	input: η0 > 0, {βit > 0}tT=-0i, β2 , i , 2 .
2:	initialize: x0 = 0, u0 = v0 = vb0 = 0.
3:	for t = 1,…，T 一 1 do
4:	Randomly pick it from {1, ∙ ∙ ∙ , n};
study the stability of the linear system equation 12.
It is easily solved by LMI toolbox Boyd et al. (1994).
Although the condition in equation 13 is not very
clear to the relationship between ηt and Lit (xt), the
condition in equation 13 directly reveals its func-
tional relationship, that is, ηt = ηt，3t+i/L^(xt),
Where 1+√g <ηt <
i+√β1t
i-√β1t.
Based on the equa-
tion 5, vbt+i tends to zero as it is a linear system,
5:
6:
7:
8:
9:
10:
11:
12:
gt = Vfit (xt);
Lit (xt)
kgt-gt-1 k .
∣∣Xt-Xt-1 k+e1 ;
ηt = (Lit(Xt) + €2)；
mt+i = βitmt + (1 - βit)gt .
vt+i = β2vt + (1 - β2 )gt2 .
^
vbt+i
xt+i
end for
max{vt+i, vbt}
Xt- ηt H ；
0 < β2 < 1, and the gradient goes to zero. For sim-
plification, ηt = ηo∕Lit (xt), where ηo is an initialized learning rate and i+√β∣ < ηo < 1-√β1, since
βit = βi. Similar to PLS-SGD, Lit (Xt) is also sampled uniformly at random from [1,…，n], that is,
Lit (xt) is computed by equation 10, and the learning rate ηt is set to ①.(鲁什^)or ；；…)
for avoiding the over-large learning rate in the latter updating process. Thus, PLS-AMSGrad is
summarized in Algorithm 2.
2
Remark 2. Based on the Lemma, the 2 × 2 LMI in equation 13 is equivalent to the condition in
equation 14. The former is obtained by constructing the Lyapunov function in the control theory,
while the latter is built by calculating the spectral radius of the weight matrix in the linear system in
equation 12, which is defined as the magnitude of the largest eigenvalue of the weight matrix.
4 Experiments
In this section, we present empirical results to confirm the effectiveness of the PLS method on
linear predictors (convex) and neural networks (nonconvex). We compare PLS-SGD, PLS-SQN,
PLS-AMSGrad and PLS-AccSGD with SGD, SQN Byrd et al. (2016), adaQN Keskar & Berahas
(2016a), AMSGrad Reddi & Kumar (2018) and AccSGD Kidambi et al. (2018).
4.1	PSL for convex optimization
In this subsection, we compare PLS-SGD and PLS-SQN with SGD, SQN and adaQN by using convex
linear predictor problems (e.g., logistic regression and least squares regression). The code of SGD,
SQN and adaQN is downloaded in the github3 Keskar & Berahas (2016b). Based on this code, we
3https://github.com/keskarnitish/minSQN
6
Under review as a conference paper at ICLR 2019
Figure 2: Logistic regression (convex) and least squares regression (convex) on mushroom using
SGD, SQN, adaQN, PLS-SGD and PLS-SQN.
6 4 2
■ ■ ■
Ooo
SSO- 6u-uejl
sso-s91
SGD
PLS-SGD
Oooo
6 5 4 3
SSO- 6uejl
AMSGrad
-----PLS-AMSGrad
Oooo
6 5 4 3
sso-91
AMSGrad
-----PLS-AMSGrad
50
00
___
50	100
#grad / n
-------'---------- 3
AMSGrad
S-----PLS-AMSGrad -	2 2
匹
-	1	0 L
50	100	0
#grad / n
0	50	100
#grad / n
0.2
0.1
0	50	100
#grad / n
Classification
0 8 6
3 2 2
Sso- 6uejl
0 8 6
3 2 2
sso-91
-----AccSGD
-----PLS-AccSGD
0	50	100	0	50	100
#grad / n	#grad / n
Reconstruction
Figure 3: Performance comparison of SGD, AMSGrad, AccSGD, PLS-SGD, PLS-AMSGrad and
PLS-AccSGD on MNIST using neural networks with two fully-connected hidden layers. The left
two columns show the training loss and test loss for classification, while right two columns show the
training loss and test loss for reconstruction.
implement PLS-SGD and PLS-SQN. We do experiments on mushroom dataset4, which contains
8124 data points with 112 dimensions. We use mini-batches of size 256 and tune the learning rate
for the best SGD, SQN and adaQN in all experiments. We report the training loss with respect to
iterations in Figure 2, and have the following observations. Figure 2 shows the effectiveness of PLS.
Specifically, PLS-SGD has faster convergence and lower loss than SGD, and PLS-SQN is also faster
convergent than SQN. Moreover, PLS-SGD and PLS-SQN are much better than adaQN.
4.2	PSL FOR NONCONVEX OPTIMIZATION
In this subsection, We compare PLS-SGD, PLS-AMSGrad and PLS-AccSGD with SGD, AMSGrad
and AccSGD by studying nonconvex multiclass classification and image reconstruction using neural
network with least squares regression (LSR) loss and '2-regularization. The weight parameters
of the neural network are initialized by using the normalized strategy choosing uniformly from
[-Vz6∕(nin + Iut), ∙√6∕(nin + n.ut)], where nin and n°ut are the numbers of input and output
layers of the neural network, respectively. We use mini-batches of size 100 in all experiments.
Datasets. MNIST5 contains 60,000 training samples and 10,000 test samples with 784 dimensional
image vector and 10 classes, and CIFAR106 includes 50,000 training samples and 10,000 test samples
with 1024 dimensional image vector and 10 classes, and 512 dimensional features are extracted by
deep residual networks He et al. (2016) for verifying the effectiveness of our methods.
4https://archive.ics.uci.edu/ml/datasets/mushroom
5http://yann.lecun.com/exdb/mnist/
6https://www.cs.toronto.edu/~kriz/cifar.html
7
Under review as a conference paper at ICLR 2019
0
0
1 5
00
O
Sa」ou-ebə- θ≥BP4
20	40	60	80	100
#grad / n
Sa®」ou-ebə- θ≥sBP4
0.015
0.01
0.005
20	40	60
#grad / n
Sa®」ou-ebə- θ≥iiBP4
------PLS-AccSGD-1th layer
------PLS-AccSGD-2th layer
------PLS-AccSGD3th layer
80	100	0	20	40	60	80	100
#grad / n
Figure 4: Adaptive learning rates of three layers of neural networks for classification task on MNIST
dataset using PLS-SGD, PLS-AMSGrad and PLS-ACCSGD.
Classification. We train the neural networks with two fully-connected hidden layers of 500 ReLU
units and 10 output linear units to investigate the performance of all algorithms on MNIST and
CIFA10 datasets. The '2-regularization is 1e-4 (MNIST) and 1e-2 (CIFAR10). A grid search
is used to determine the learning rate that provides the best performance for SGD, AMSGrad and
AccSGD. We set the adaptive learning rate η = ηo/(Lit (Xt) + e2) for PLS-SGD, PLS-AMSGrad
and PLS-AccSGD, where no is chosen as 0.001 or 0.002. To enable fair comparison, We set typical
parameters βι =0.9 and β2 =0.999 for AMSGrad and PLS-AMSGrad, and set κ=1000 and ξ =10
for AccSGD and PLS-AccSGD Reddi & Kumar (2018); Kidambi et al. (2018). For convenience, both
parameters eι and e2 are same, e = eι = e2, and e chosen as 0.01 for PLS-SGD and PLS-AMSGrad,
and 0.001 for PLS-AccSGD.
We report the training loss and test loss with respect to iterations on MNIST in the left two columns
of Figure 3. We can see that PLS-SGD, PLS-AMSGrad and PLS-AccSGD preform much better than
SGD, AMSGrad and AccSGD. The important reason is that our PLS method can directly adjust the
learning rate from a small initialization value to a suitable value. In practice, a large fixed learning rate
results in the explosion of the loss of neural network with ReLU using SGD, AMSGrad and AccSGD
since the loss will go to infinity when the learning rate is larger than 0.011 in our experiments. We
observe that the learning rate fast increases in the initial stage and slowly varies in the late stage in
Figure 4. Moreover, there are similar observations on CIFAR10. Due to the limited space, the losses
and learning rate are plotted in Figure 5 in Supplementary Material.
Reconstruction. We train a deep fully connected neural network to reconstruct the images in
comparison with all algorithms on MNIST dataset. Its structure is represented as 784-1000-500-
200-500-1000-784 with the first and last 784 nodes representing the input and output respectively.
In this experiment, we provides the best performance for SGD, AMSGrad and AccSGD by searching
from a grid learning rates. The initial learning rate no is set to 5e-7,1e-2 and 1e-7 for PLS-SGD, PLS-
AMSGrad and PLS-AccSGD, respectively. In addition, we also set to n = no/(Vi(Lit (Xt) + e2))
for PLS-AMSGrad, βι =0.9, β2=0.999, κ=1000 and ξ =10. Moreover, e = eι = e2, and e chosen
as 0.01 for PLS-SGD and PLS-AccSGD, and 0.1 for PLS-AMSGrad.
The training loss and test loss with respect to iterations are reported in the right two columns of
Figure 3. We can still see that PLS-SGD, PLS-AMSGrad and PLS-AccSGD have significant better
performance than SGD, AMSGrad and AccSGD since our PLS method adaptively adjusts the learning
rate to prevent the explosion of the LSR loss with large learning rate. The adaptive learning rate is
shown in Figure 6 in Supplementary Material. We also observe that the learning rate is initialized a
small value, fast increases in the earlier stage and slowly varies in the latter stage.
5 Conclusions
This paper introduced an adaptive local smoothness method for stochastic gradient descent algorithms.
This method adjusted automatically learning rate by using the latest gradients to predict the local
smoothness. We proposed PLS-SGD, PLS-AMSGrad and PLS-AccSGD algorithms by applying our
adaptive local smoothness method into the popular SGD, AMSGrad and AccSGD algorithms. We
proved that our proposed algorithms enjoyed the linear convergence rate by studying the stability of
their transformed linear systems. Moreover, our proof was significantly simpler than the convergence
analyses of SGD, AMSGrad and AccSGD. Experimental results verified that the proposed algorithms
provided better performance gain than SGD, AMSGrad and AccSGD.
8
Under review as a conference paper at ICLR 2019
References
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning.
arXiv:1606.04838v1 ,pp.1-93, 2016.
S.	Boyd, L.E. Ghaoui, E. Feron, and V. Balakrishnan. Linear Matrix Inequalities in System and
Control Theory. SIAM, 1994.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008-1031, 2016.
T.	Dozat. Incorporating nesterov momentum into adam. In ICLR workshop, 2016.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. J. Mach. Learn. Res., 12:2121-2159, July 2011.
K.M. He, X.Y. Zhang, S.Q. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV,
2016.
B. Hu, P. Seiler, and A. Rantzer. A unified analysis of stochastic optimization methods using jump
system theory and quadratic constraints. In COLT, pp. 1157-1189, 2017.
P. Jain, S.M. Kakade, R. Kidambi, P. Netrapalli, and A. Sidford. Accelerating stochastic gradient
descent. arXiv:1704.08227, pp. 1-55, 2017.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction.
In NIPS, pp. 315-323, 2013.
N.S. Keskar and A.S. Berahas. adaqn: An adaptive quasi-newton algorithm for training rnns. In
G. Manco J. Vreeken P. Frasconi, N. Landwehr (ed.), ECML-PKDD, volume 9851, pp. 1-16.
Springer, Cham, 2016a.
N.S. Keskar and A.S. Berahas. minSQN: Stochastic Quasi-Newton Optimization in MATLAB, 2016b.
URL https://github.com/keskarnitish/minSQN/. [Online].
R.	Kidambi, P. Netrapalli, P. Jain, and S.M. Kakade. On the insufficiency of existing momentum
schemes for stochastic optimization. In ICLR, 2018.
D.P. Kingma and J.L. Ba. Adam: a method for stochastic optimization. In ICLR, 2015.
S.	Kpotufe and V.K. Garg. Adaptivity to local smoothness and dimension in kernel regression. In
NIPS, 2013.
L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms via integral
quadratic constraints. SIAM Journal on Optimization, 26(1):57-95, 2016.
C. Malherbe and N. Vayatis. Global optimization of lipschitz functions. In ICML, pp. 2314-2323,
2017.
V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, pp.
807-814, 2010.
Y. Nesterov. A method of solving a convex programming problem with convergence rate. Soviet
Mathematics Doklady, 27(2):372-376, 1983.
J. Nocedal and S.J. Wright. Numerical Optimization. Springer Science+Business Media, 2006.
B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 4(5):1-17, 1964.
S.J. Reddi and S. Kumar. On the convergence of adam and beyond. In ICLR, 2018.
S.J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex
optimization. In ICML, pp. 314-323, 2016.
9
Under review as a conference paper at ICLR 2019
T. Tieleman and G. Hinton. Rmsprop: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
D. Vainsencher, H. Liu, and T. Zhang. Local smoothness in variance reduced optimization. In NIPS,
pp. 2179-2187, 2015.
Matthew D. Zeiler. Adadelta: An adaptive learning rate method. CoRR, abs/1212.5701, 2012.
10
Under review as a conference paper at ICLR 2019
6 Supplementary Material
6.1 PLS-ACCSGD
In this subsection, we present a PLS-AccSGD algorithm. Similar to PLS-AMSGrad, our PLS method
is integrated into the classical momentum method, AccSGD Jain et al. (2017); Kidambi et al. (2018).
Using the Lipschitz linearization in the equation 9, the updating rules in the equation 6 of AccSGD is
simply linearized as:
where Bt
mt+1 -
xt+1 -
(1 -
x*
x*
(Bit ㊈ Id) (m;- x:
α)(1 - aηtLit (xt))
(1 - b)(1 - ηtLit (xt)) + b(1 - α)(1 - aηtLit (xt))
,α = 1 - o12ξ, a
,	κ,
(15)
b = 0 7+-l-α), ξ and K are defined in the equation 6. This linearization leads Us to provide a much
simpler proof for linear convergence analysis of AccSGD by studying the stability of the linear
system in the equation 15. We have following Theorem 3 for its convergence condition.
Theorem 3. Consider the linear system in the equation 15. Assume that it is sampled in
an IID manner from a uniform distribution, the assumption 1 holds and there exists an equi-
librium x* ∈ Rd such that Vf (x*) = 0. For a fixed linear convergence rate 0 < ρ <
max 1 -
such that
07ξ, maχt { κ(1-+L.7Ixt)) }}, if there
exists a 2 × 2 positive definite matrix P 0
1n
-X BTPBi- PP Y 0,
n
i=1
or thefollowing condition holds, for any it ∈ Ω = {1, ∙ ∙ ∙ , n},
0 < 1 - 072ξ<ρand (l-ρ31)	ɪ
κ	κ	Lit(xt)
1
<ηt <Lit(xt),
(16)
(17)
then the linear system is exponentially stable, that is,
where cond(P) is the condition number ofP.
―	*
mt+ι - X
*
xt+1 - x*
≤ cocond(P)Pt
2
*
m0 - x*
*
x0 - x*
K
077,
Theorem 3 shows that the linear convergence con-
ditions in the equation 16 and the equation 17 are
simpler than the convergence analysis of AccSGD
Kidambi et al. (2018). Similar to PLS-AMSGrad,
the 2 × 2 LMI condition equation 16 is built by us-
ing the control theory and is easily solved by LMI
toolbox Boyd et al. (1994). The condition in the
equation 17 directly opens the learning rate ηt is a
functional relationship with the local smoothness
Lit(xt), ηt = L.η0χ), where no is an initialized
learning rate, 1 - P κ+0.7ξ < no < 1. This revels
PLS η0 can be set to a negative value. The rea-
son is that the eigenvalues of the weight matrix in
the system in the equation 15 are 1 - 07ξ and
Algorithm 3 PLS-AccSGD.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
input： no, κ, ξ ≤ √κ, €1, €2.
initialize: xo, uo = 0, α = 1 — 0.7 ξ.
for t = 1,…，T - 1 do
Randomly pick it from {1, ∙ ∙ ∙ , n};
gt = Vfit (xt);
Lit (Xt) = Ukgt-gY ;
it t	kxt-xt-1k+1
nt = no/(Lit (xt) +€2);
mt+i = αmt + (1 - α) (xt -岩gt);
xt+1 = o.7+(7-α) (Xt - ntgt)
+ o.7+-1-α) mt+i;
end for
κ(i-ηtLit(xt))
κ+0.7ξ
. The stability of the system in the
equation 15 needs to satisfy the condition in the equation 17. Similar to PLS-SGD, Lit (xt) is also
sampled uniformly at random from [1, ∙ ∙ ∙ ,n], that is, Lit (Xt) is computed by the equation 10. To
prevent the over-large learning rate, the learning rate nt is set to no/(Lit (xt) + €2) in our PLS-
AccSGD, which is outlined in Algorithm 3. Following Remark 2, the 2 × 2 LMI in the equation 16
is equivalent to the condition in the equation 17.
6.2 Proofs
Proof of Theorem 1: First, we prove that SGD in equation 3 is converted into the stochastic linear
system in equation 11. By putting the Lipschitz linearization in equation 9 into the equation 3, we
11
Under review as a conference paper at ICLR 2019
have
χt+ι = Xt — ηt (Lit (Xt) 0 Id )(χt — χ*).
(18)
By adding —x* into the both sides of the equation 18 and combining like terms, it holds xt+ι — x* =
((1 — ηtLit (Xt)) 0 Id)(Xt — x*). We thus have the equation 11.
Second, we construct the Lyapunov function V (xt) = (xt — x*)T (p 0 Id) (xt — x*), where p > 0,
to prove the stability of the system in the equation 11. Defining
E[∆V (Xt)] = E[V (Xt+1) — ρ2V (Xt)]
= E[(Xt+1 — X*)T (p0 Id) (Xt+1 — X*) — ρ2(Xt — X*)T (p 0 Id) (Xt — X*)]
= (Xt — X*)T E[(1 — ηtLit(Xt))2] — ρ2 (p0 Id) (Xt — X*).	(19)
Then for any Xt 6= X*, E[∆V (Xt)] < 0 if E[(1 — ηtLit (Xt))2] — ρ2 < 0, which implies
E[L2t(xt)]η2 — 2E[Lit(xt)]ηt + 1 — ρ2 < 0	(20)
Since it is sampled in an IID manner from {1,…，n}, We let μ = E [L〃 (xt)] = § PZi Li(Xt),
and V = E [L2t (Xt)] = ɪ P§=i L2(xt). So, the equation 20 is satisfied if we have the condition in
Theorem 1 as
μ2 > (1 — P2)ν.	(21)
This implies
μ — pμ — V(I —落 < { < μ + pμ — V(I 一沁	(22)
ν	tν
By using the nonnegativity of the equation 19, we have
(xl+1 — x*)T	(p 0 Id) (xl+1	—	x*)	≤	ρ2(xl — x*)T (p 0	Id)	(xl	— x*).	(23)
Inducting from l = 1 to t, we see that for all t
(xt — x*)T	(p	0 Id)	(xt	— x*) ≤	ρ2t(x0 —	x*)T	(p	0 Id)	(x0	— x*),	(24)
which implies kxt — x* k2 ≤ ρtkx0 — x* k2, where p is a positive number. The proof is complete.
Proof of Theorem 2: First, we prove that AMSGrad is converted into the stochastic linear system in
the equation 12. By putting the Lipschitz linearization in the equation 9 into the equation 5, we have
mt+1 = (β1t 0 Id) mt + (1 — β1t) (Lit(xt) 0 Id) (xt — x*),	(25a)
vt+1 = (β2 0 Id) vt + (1 — β2) (Lit (xt) 0 Id)2 (xt — x*)2.	(25b)
By adding —x* into the both sides of xt+i = Xt — ηt mt+1_ in the equation 5 and substituting the
vbt+1
equation 25a and the equation 25b into the equation 5, it holds
* _	*	β1tmt + (1 — Bit) (Lit (Xt) 0 Id)(Xt — x*)
xt+1—x =Xt-X — ηt	√vt+τ
=(» 0 Id! mt + ( (l -「β1 ㈣”(Xt)! 0 Id! (xt - x*),	(26)
vbt+i	vbt+i
where vbt+i = max{vt+i,vbt}. So, we have the equation 12 by combining the equation 25a with the
equation 26.
Second, we construct the Lyapunov function V (ζt) = ζtT (P 0 Id) ζt, where ζt = x m—tx* and
P 0 is a 2 × 2 positive matrix, to prove the stability of the system in the equation 12. Defining
E[∆V (Zt)] = E[V (Zt+ι) — P2V (Zt)]
=E[ZT+ι (At 0 Id) Zt+i — P2ZT (At 0 Id) Zt]
=ZT ((E[ATtPAit] — P2P) 0 Id) Zt.	(27)
12
Under review as a conference paper at ICLR 2019
Then if the condition in the equation 13 is satisfied, then E[∆V(xt)] < 0 for any Xt = x*. By using
the nonnegativity of the equation 19, we have
ζl+ι (P 乳 Id) Zι+1 ≤ P2ZT (P 乳 Id) Zι.	(28)
Inducting from l = 1 to t, we see that for all t
ZT (P 乳 Id) Zt ≤ P2tZT (P 乳 Id) Zo,	(29)
which implies
mt+1
xt+1 - x*
≤ ,cond(P) Pt
2
m0
x0- x*
, where cond(P) is the condition number of
2
P and Cond(P) = σι(P)∕σp(P), where σι(P) and σp(P) denote the largest and smallest singular
values of the matrix P .
Third, we prove the another condition in the equation 14. If it holds ATPAit - P2P Y 0 for any
it ∈ Ω = {1, ∙∙∙ ,n}, then we have the equation 13. Moreover, ATPAit - P2P Y 0 is equivalence
to a simple condition that the eigenvalues of Ait is less than P. Hence, we consider the eigenvalues of
Ait calculated by
λtI - At
λt - β1t
ηpt
,√b+1
-(1 - β1t)Lit (xt )
λt - 1 -
(1-βιt)ηtLit (Xt)
vbt+1
0,
(λt - β1t )	λt -	1 -
(1 - β1t)ηtLit (xt)
K----
vbt+1
)) + (1-βIt)Lit (Xt) ppv+τ = 0,
-	1 + β1t -
(1 - β1t)ηtLit (xt)
K----
vbt+1
λt + β1t = 0,
1 + β1t -
(I - ∕β1t)ηt Lit(Xt)
λt
√bt + 1
2
± √τ
(30)
(31)
(32)
(33)
where Γ = 1 + β1t -
(1-βιt)ηtLit (Xt)
vbt+1
2
- 4β1t.
Similar to the Proof of Proposition 1 Lessard et al. (2016), if Γ < 0, then the magnitudes of the roots
satisfy ∣λt∣ < √β1t < ρ = maxt{√βt}. Then Γ < 0 implies that
1 +β1t -
2
(1- βιt)ηtLit(Xt) ʌ
Pbt+1	)
< 4β1t,
-2√β!t < 1+ βιt-
(I - 8lt)ηtLit (Xt)
Pbt+1
< 2 √β!t,
(1 - √βιt) Pbt+ι	1	(1 + √βιt) Pbt+ι	1
1 - β1t	Lit (Xt)	"t	1 - β1t	Lit (Xt)
(34)
(35)
(36)
The proof is complete.
Proof of Theorem 3: First, we prove that AccSGD is converted into the stochastic linear system in the
equation 15. By putting the Lipschitz linearization in the equation 9 into the equation 6, we have
mt+1 = (α 0 Id) mt +(1 - α) (Xt - 0^7 (Lit (Xt) 0 Id)(Xt - x*)) ,	(37a)
0.7	*	1 - α
Xt+1 = 0.7+(1-α) (Xt- ηt (Lit(Xt) 0 Id)(Xt-X))+6+(…a) 0 Id) mt+1.
(37b)
Let a = 07 and b = 0 7∣-0La), We have
mt+1 = (α 0 Id) mt + (1 - α) (Xt - aηt (Lit(Xt) 0 Id) (Xt - X*)) ,	(38a)
Xt+1 = (1 - b) (Xt - ηt (Lit(Xt) 0 Id) (Xt - X*)) + (b 0 Id) mt+1.	(38b)
13
Under review as a conference paper at ICLR 2019
SSo- 6u-uejl
20	40	60	80	100
#grad / n
0.5
0.4
S 0.3
g0.2
0.1
00
1
∞
O
4 3 2 1 0
■ ■ ■ ■
Oooo
se,! 6U-U」& 3>dep<
------PLS-SGD-1th layer
------PLS-SGD-2th layer
------PLS-SGD-3th layer
3 2
■ ■
O O
SSo- 6u-ue∙ll
AMSGrad
PLS-AMSGrad
20	40	60	80	100
#grad / n
#grad / n
s2bj 6u-UJe3- 3>=dep<
6 4 2
Ooo
■ ■ ■
Ooo
20
40
60
80
00
1 8 6 4 2
Ioooo
0L
Oooo
SSO- 6u-uejl
AccSGD
PLS-AccSGD
20	40	60	80	100
#grad / n
5 15
La
se,! 6U-U」& 3>=dep<
#grad / n
------PLS-ACCSGD-1th layer
------PLS-AccSGD-2th layer
------PLS-ACCSGD-3th layer
0	20	40	60	80	100
#grad / n
Figure 5: Performance comparison of SGD, AMSGrad, AccSGD, PLS-SGD, PLS-AMSGrad and
PLS-AccSGD on CIFAR10 using neural network with two fully-connected hidden layers. The left,
middle, and right columns show the training loss, test loss, and adaptive learning rate, respectively.
80
00
1
，
5 4 3 2 1 0
Ooooo
■ ■ ■ ■ ■
Ooooo
sBJ 6U-Ee9
------PLS-AMSGrad-4th layer
------PLS-AMSGrad-5th layer
------PLS-AMSGrad-6th layer
X 10
80
00
1
PLS-SGDWth layer
PLS-S(
PLS-S(
iD-5th layer
!D-6th layer
，
5 2 5 1 5 0
20J00
Ooo
sBJ 6u-u∙!e9
50
1
50
1
1
ssBJ 6u'≡e3
------PLS-AccSGD-4th layer
------PLS-AccSGD-5th layer
------PLS-AccSGD-6th layer
20
40
60
80
00
1
ssBJ 6u'≡e3
50
00°
50
1



Figure 6: Adaptive learning rates of different layers of neural network for reconstruction using
PLS-SGD, PLS-AMSGrad and PLS-ACCSGD on MNIST. The down row shows the adaptive learning
rate in the first 200 iterations.
By adding -x* into the both sides of the equation 38a and the equation 38b, and substituting the
equation 38a into the equation 38b, it holds
mt+1 — x* = (α 0 Id) (mt - x*) + ((1 - α)(1 - a/Lit (Xt)) 0 Id)(Xt - x*),	(39a)
Xt+1 - x* = ((1 - b)(1 - ηtLit (xt)) 0 Id)(Xt - x*) + (b 0 Id) (mt+1 一 x*)
=(αb 0 Id) (mt - x*) + (((1 - b)(1 - ηtLit (Xt)) + b(1 - α)(1 - aηtL豌 (Xt))) 0 Id)(Xt - x*).
(39b)
Thus, We have the equation 15 by combining the equation 39a with the equation 39b.
14
Under review as a conference paper at ICLR 2019
Second, we construct the Lyapunov function V (ξt) = ξT (P 0 Id)ξt, where ξt = gt- ：), P A 0
is a 2 × 2 positive matrix, to prove the stability of the system in the equation 15. Defining
E[∆V(ξt)] = E[V (ξt+1) - ρ2V(ξt)]
= E[ξtT+1 (P 0 Id) ξt+1 -ρ2ξtT(P0Id)ξt]
=ξT ((E[BTPBit]-PP) 0 Id) ξt.	(40)
Then if the equation 16 is satisfied, then E[∆V(ξt)] < 0 for any ξt 6= 0. By using the nonnegativity
of the equation 40, we have
ξlT+1(P0Id)ξl+1 ≤ρ2ξlT(P0Id)ξl.	(41)
Inducting from l = 1 to t, we see that for all t
ξtT(P0Id)ξt ≤ρ2tξ0T(P0Id)ξ0,	(42)
which implies
of P.
―	*
mt+ι - X
*
xt+1 - x*
≤ Vzcond(P)Pt
2
*
m0 - x*
*
x0 - x*
, where cond(P) is the condition number
2
Third, We certify the another condition in the equation 17. If it holds BTPBit 一 P2P Y 0 for any
it ∈ Ω = {1, ∙ ∙ ∙ ,n}, then we have the equation 16. Moreover, BTPBit 一 P2P Y 0 is equivalence
to a simple condition that the eigenvalues of Bit is less than P. Hence, we consider the eigenvalues of
Bit , which is calculated as follows.
By adding a product of -b and the first row of Bt into the second row of Bt, Bt is rewritten as Bt:
λtI 一 Bbt
The two eigenvalues of Bt is
b α (1 一 α)(1 一 aηtLit (xt))
Bt = 0	(1 一 b)(1 一 ηtLit (xt))	,
λt 一 α 一(1 一 α)(1 一 aηtLit (xt))
0	λt 一 (1 一 b)(1 一 ηtLit (xt))	.
λ1t = α,
Since λ1t > 0 and λ2t > 0, we have
0 < λ1t = α < P,
λ2t = (1 一 b)(1 一 ηtLit (xt)).
0 < λ2t = (1 一 b)(1 一 ηtLit (xt)) < P.
(43)
(44)
(45)
(46)
By substituting α = 1 一 0.7κ2ξ and b =
equation 17. The proof is complete.
0 7+-l-α) into the equation 46, it holds the condition in the
6.3 Figures
In the classification experiment, we select the learning rate from
{0.011, 0.0090.008, 0.007, 0.006, 0.05, 0.004} for providing the best performance of the S-
GD, AMSGrad and AccSGD algorithms. In the reconstruction experiment, the learning rate is
chosen from {6, 5,4,3, }e-7 for SGD and AccSGD and {10, 7,5,3}e-2 for AMSGrad due to the
explosion of the LSR loss with large learning rate, and select the learning rate from these sets for
providing the best performance of the algorithms. To prevent the over-fitting, the learning rate η is
set to ηo∕√t for AMSGrad (except MNIST).
Figure 5 reports the adaptive learning rate, the training loss and test loss with respect to iterations on
CIFAR10 for classification, while Figure 6 shows the adaptive learning rate with respect to iterations
on MNIST for reconstruction.
15