Under review as a conference paper at ICLR 2019
Non-Syn Variational Autoencoders
Anonymous authors
Paper under double-blind review
Ab stract
Learning disentangling representations of the independent factors of variations
that explain the data in an unsupervised setting is still a major challenge. In the fol-
lowing paper we address the task of disentanglement and introduce a new state-of-
the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE).
Our model draws inspiration from population coding, where the notion of syn-
ergy arises when we describe the encoded information by neurons in the form of
responses from the stimuli. If those responses convey more information together
than separate as independent sources of encoding information,they are acting syn-
ergetically. By penalizing the synergistic mutual information within the latents we
encourage information independence and by doing that disentangle the latent fac-
tors. Notably, our approach could be added to the VAE framework easily, where
the new ELBO function is still a lower bound on the log likelihood px . In addi-
tion, we qualitatively compare our model with Factor VAE and show that this one
implicitly minimises the synergy of the latents.
1	Introduction
Our world is hierarchical and compositional, humans can generalise better since we use primitive
concepts that allow us to create complex representations (Higgins et al. (2016)). Towards the cre-
ation of truly intelligent systems, they should learn in a similar way resulting in an increase of their
performance since they would capture the underlying factors of variation of the data ( Bengio et al.
(2013); Hassabis et al. (2017); Botvinick et al. (2017)). In addition, good representations improve
the performance for tasks involving transfer learning and multi-task learning; since it will capture
the explanatory factors.
According to Lake et al. (2016), a compositional representation should create new elements from
the combination of primitive concepts resulting in a infinite number of new representations. For
example if our model is trained with images of white wall and then is presented a boy with a white
shirt, it should identify the color white as a primitive element. Intuitively, our model will be able to
construct different and multiple representations from the primitives.
Furthermore, a disentangled representation has been interpreted in different ways, for instance Ben-
gio et al. (2013) define it as one where single latent variables are sensitive to changes in generative
factors, while being invariant to changes in other factors. In addition, we agree with Higgins et al.
(2017a), which mentions that a disentangle representation should be factorised and interpretable. In-
tuitevely, the model could learn generative factors such as position, scale or colour; if it is disentan-
gle it should be able to traverse along the position variable without changing the scale or the colour.
It’s worth noting that disentangled representations have been useful for a variety of downstream
tasks such as domain adaptation by training a Reinforcement Learning agent that uses a disentan-
gled representation of its environment Higgins et al. (2017b); or for learning disentangled primitives
grounded in the visual domain discovered in an unsupervised manner Higgins et al. (2017c).
2	Related work
The original Variational autoencoder (VAE) framework (Kingma & Welling (2013); Rezende et al.
(2014)) has been used extensively for the task of disentanglement by modifying the original ELBO
formulation; for instance β-VAE is presented in Higgins et al. (2017a) which increases the latent
capacity by penalising the KL divergence term with a β hyperparameter. In addition, Kim & Mnih
1
Under review as a conference paper at ICLR 2019
(2018) achieved a more robust disentangled representation by using the model called Factor VAE
which penalises the total correlation of the latent variables encouraging the independence of the
latents; a similar approach is shown in Chen et al. (2018), where they present a clever ELBO de-
composition based on Hoffman & Johnson (2016). Other approaches rely on information bottleneck
presented in Tishby et al. (1999) to model frameworks for this task such as Alemi et al. (2016);
Achille & Soatto (2016). Furthermore, Chen et al. (2016) describe a model based on Generative
Adversarial Networks (Goodfellow et al. (2014)) by encouraging the mutual information between
the latents and the output of the generator. Notably, in Higgins et al. (2016), they describe compre-
hensively the β-VAE model using a neuroscience and information theory approaches; they suggest
that by encouraging redundancy reduction the model achieves statistical independence within the
latents. This model inspired us to look into different fields for new ways to enforce disentanglement
of the latents.
3	S ynergy
To understand our model, we need first to describe Synergy (Gat & Tishby (1998); Schneidman
et al. (2003)) being a popular notion of it as how much the whole is greater than the sum of its parts.
It’s common to describe it with the XOR gate, since we need two independent variables to fully
specified the value of the output. Following, we describe the synergy from two related fields.
3.1	Information theory approach
Computing the multivariate synergistic information is an ongoing topic of research Schneidman
et al. (2003); Williams & Beer (2010); Bertschinger et al. (2012); Griffith & Koch (2012). Most
of the current research in this topic uses the Partial information diagram described by Williams &
Beer (2010). In order to understand the importance of the Synergy information in our framework
it’s essential to describe the relations with the Unique and Redundant Information. Introducing the
notation from Williams & Beer (2010), let’s consider the random variable S and a random vector
R = {R1, R2, .., Rn}, being our goal to decompose the information that the variable R provides
about S; the contribution of these partial information could come from one element from R1 or from
subsets of R (ie. R1, R2). Considering the case with two variables, {R1, R2}, we could separate the
partial mutual information in unique (U nq(S; R1) and Unq(S; R2) ), the information that only R1
or R2 provides about S is redundant (Rdn(S; R1 , R2)), which it could be provided by R1 or R2;
and synergistic (Syn(S; R1, R2)), which is only provided by the combination of R1 and R2. The
figure 1 depicts the decomposition; this diagram is also called PI-diagram (Partial information) .
Additional notation is used for a better visualisation in the case of more variables: Unique {1},{2};
Redundant {1}{2} and Synergistic {12}.
Figure 1: Structure of total information of two variables about S
2
Under review as a conference paper at ICLR 2019
For the case of 2 variables (X1 , X2), we expect four contributions to the mutual information as
described in Bertschinger et al. (2012); Olbrich et al. (2015):
I(S;R1,R2) =SI(S;R1,R2)+Unq(S;R1\R2)+Unq(S;R2\R1)+Syn(S;R1,R2)
'--------{--------} |
Redundant
{z'^^^^
Unique
}|
^"^\^^^
Unique
J 1-------------{----------}
Synergistic
(1)
It’s easy to see that the number of terms increases exponentially as the number of sources increases.
The best measure for synergy is an ongoing topic of research. In the subsection we are going to talk
about the synergy metrics.
3.2	Population coding approach
For neural codes there are three types of independence when it comes to the relation between stimuli
and responses; which are the activity independence, the conditional independence and the informa-
tion independence. One of the first measures of synergy for sets of sources of information came
from this notion of independence. In Williams & Beer (2010) it is stated that if the responses come
from different features of the stimulus, the information encoded in those responses should be added
to estimate the mutual information they provide about the stimulus. Formally:
I(S;R1,R2)=I(S;R1)+I(S;R2)
(2)
However, we just saw in the previous sections that the I(S; R1) and I(S; R2) could be decomposed
in their unique and redundant and synergistic terms. Intuitively, this formulation only holds if there
is no redundant or synergistic information present; which means in the context of population coding
that the responses encoded different parts of the stimulus. If the responses R1 and R2 convey more
information together than separate, we can say we have synergistic information; if the information
is less, we have redundant information. That’s the reason why in Gat & Tishby (1998), the synergy
is considered as measure of information independence:
Syn(R1,R2) =I(S;R1,R2)-I(S;R1)-I(S;R2)
(3)
3.3	Synergy metric
First we need to state the notation (the same as Griffith & Koch (2012)):
•	n: Number of individual predictors Xi
•	Ai : subset of individual predictors (ie. Ai = {X1 , X3})
•	X: Joint random variable of all individual predictors X1X2..Xn
•	{X1 , X2 , ..., Xn}: Set of all the individual predictors
•	Y: Random variable to be predicted
•	y: A particular outcome of Y.
The intuition behind this metric is that synergy should be defined as the ”whole beyond the maximum
of its parts”. The whole is described as the mutual information between the joint X and the outcome
Y; whereas the maximum of all the possible subsets is interpreted as the maximum information that
any of the sources Ai provided about each outcome. Formally, this is stated as:
Smax({X1,X2,...,Xn};Y) =I(X;Y) -Imax({A1,A2..An};Y)	(4)
= I(X;Y) - Xp(Y = y) max I(Ai; Y = y)	(5)
y∈Y
This metric derives from Williams & Beer (2010) and Griffith & Koch (2012), however one of the
differences with this metric with the one presented in Griffith & Koch (2012) is that in this one we
3
Under review as a conference paper at ICLR 2019
are considering the specific mutual information Imax in a group of latents Ai , whereas in the paper
mentioned it considers only an individual latent. Notably the Imax can be expressed in terms of the
KL divergence.
I(Ai;Y =y)=
P(ai | y) log
ai ∈Ai
P (ai,y)
Ρ(ai)Ρ(y)
DKL P(Ai|y) kP(Ai)
(6)
(7)
Putting together the equation 7 and 5, we have the following:
Smax({X1,X2,...,Xn};Y) =I(X;Y)-	p(Y = y) miaxDKL P (Ai | y) k P(Ai)	(8)
y∈Y
In the following section we are going to use the intuition provided by the above equation in the VAE
framework for the task of disentanglement.
4	Model Derivation
The motivation of our contribution is inspired in this concept and driven by the belief that synergy
is not desirable for the task of disentanglement, since we want the latents to be independently infor-
mative as possible about the data, instead of needing many latents to specify the data. Therefore, we
argue that by penalising the synergistic information within the latents and the data, we would en-
courage the disentanglement of the underlying factors of variation. This hypothesis is also inspired
in the information presented in Griffith & Koch (2012); Gat & Tishby (1998), where it is stated that
the synergy is a measure of independence information in the responses of the stimuli.
First, we need to change the notation to match the VAE framework notation (Z are the latents and
X is the observations). Also, Ai is a subset of the latents, such that Ai ∈ {Z1, Z2, ..., Zn} and
Z is the joint of the latents. Formally: Z = Qid Zi , where d is the number of dimensions of the
latent variables. Besides, from the VAE standard framework, we know that the distribution p(Ai|x)
is intractable which is Why We need to use an approximate distribution qφ(A∕χ) parametrised by
φ parameters. It’s important to notice that this KL divergence could be computed in the same way
as in the VAE frameWork; the only difference is the number of dimensions used for the random
variable z. In the original VAE frameWork, We compute the KL divergence considering the joint
Z = Qid Zi; Whereas for the Synergy metric We don’t use the joint but a subset of the latents. For
instance, if Ai = Z2Z5Z8, We have the folloWing expression:
DKL qφ(z2z5z8 | x) k p(z2z5z8)	(9)
Taking in account these considerations, We express the equation 8 as folloWs:
Smax({Z1, Z2, ..., Zd};X)	=	I(Z;	X) - X p(X	= x)	maxDKL	qφ(Ai	| x)	k	p(Ai)	(10)
x∈X
We start With the original ELBO formulation (Rezende et al. (2014); Kingma & Welling (2013)) and
add the penalised term corresponding to the synergy, Where α is a hyperparameter:
Lelbo(θ,φ,x) = Eqφ(z∣x) [ logPθ (X | z)] - DKL [φφ{z | X)∣∣ p(z)	(11)
Lnew(θ, φ,x) = Lelbo(θ, φ, x) - αSmax({Z1, Z2, ..., Zd};X)	(12)
4
Under review as a conference paper at ICLR 2019
Expanding the Smax term, we have the llowing:
Lnew(θ, φ,x) = Lelbo(θ, φ,x) - α(I(z; x) -	p(X = x) max DKL qφ(Ai |x) k p(Ai)) (13)
x∈X	i
From Hoffman & Johnson (2016), we know that the KL term in the ELBO loss is decomposed in
DKL qφ(zn) k p(zn) + I(xn; z) when we use the aggregate posterior and define the loss over the
empirical distribution of the data pdata(x). Taking in account that, we can express the equation 12
as follows:
Lnew(θ, φ, x) =
1
Nf Eqφ(z∣x)[ log Pθ (X(i)। Z)]
i=1
- DKL qφ(zn) k p(zn) - I(xn; z)
-αI (xn; z) +α p(X = x)
、	-γ- J	-*,
Penalise	x∈X
max DKL qφ(Ai | x) k p(Ai) )
(14)
If we penalise the synergy (see Eq. 12), we will be penalising the mutual information term which is
not desirable for this task Kim & Mnih (2018); we can see this effect explicitly in Eq. 14. Therefore,
we use only the second term to perform the optimisation which means maximising the subset of
latents with the most amount of MI per outcome.
Lnew(θ, φ, x) = Lelbo (θ, φ, x) + α	p(X = x) max DKL qφ(Ai | x) k p(Ai) )	(15)
x∈X
It’s easy to see in Eq. 15 that it’s not a guaranteed lower bound on the log likelihood px anymore,
which is why we decided to penalise the subset of latents with the minimum specific mutual infor-
mation (ie. Aw). In practice we found that computing the maximum subset Ai for each outcome
of x is too computational intensive, which is why we decided to use a mini-batch approximation,
which is the version we show in the pseudo-code using a two step optimisation in the next section.
The final version we are going to use is the one below, where Imax is the KL term of the synergy
term.
Lnew
(θ,φ,x) = Eqφ(z∣χ)[ logPθ(x∣z)] - DκL(qφ(z∣x)kp(z)) - αDκL [qφ(Aw∣x) k p(Aw)]
J
J I
I
z
z
Lelbo
α*Imax
(16)
Algorithm 1 Non Syn VAE
Input: Observations (x(i))iN=1, batch size m, latent dimension d, weight of synergy loss α, discount
factor ω, optimiser optim, reparametrisation function gφ.
θ,φ J Initialise VAE parameters
repeat
3:
x(i) J Random minibatch B of size m, i ∈ B
Zi J gφ(e, x(i))	. Sample Zi 〜qφ(z∣x)
6:	φ,θ J optim(Vθ,φLeibo(θ, φ; x))	. : Gradients of ELBO minibatch
x0(i) J
Random minibatch B’ of size m, i ∈ B0
mu, logvar J Encoder(x0(i), φ)
9:	Worstjindex J get_index_greedy(mu, logvar, ω)
Lsyn J α * Imax(mu, logvar, worst-index)	. See Eq.16 for Imax function
φ J optim(VφLsyn (φ; x0(i)))	. : Gradients of Syn loss minibatch
12: until convergence of objective
In the algorithm shown above we see in practice that we get better results when we sample the
values of mu and logvar from the encoder for the step 2 of the optimisation. We use a greedy
approximation of the best latents by following a greedy policy (See Appendix).
5
Under review as a conference paper at ICLR 2019
5	Experiments
5.1	Latent traversals
For disentanglement, the dataset most commonly used is the dsprites dataset Matthey et al. (2017),
which consists on 2D shapes generated from independent latent factors. We used the same archi-
tecture and optimizer as Factor VAE Kim & Mnih (2018) for training our model. In order to test
qualitatively the Non-Syn VAE model, we decided to tranverse the latents and plot the mean activa-
tions of the latents.
In Figure 2 (left), we see clearly that our model disentangles the factors of variation. Likewise,
on the right we see the mean activation of each active latent averaged across shapes, rotations and
scales.
Figure 2: Left: Traverse of latents (110k steps). Right: Mean activations (110k steps)
After looking at the figure above we can state that our model achieves state-of-the-art results using
a qualitatively benchmark. Interestingly, both models perform quite similar in this test.
5.2	Synergy in Factor VAE
Also, we decided to compute the same synergy term from the Non-Syn VAE in Factor VAE (just
compute it, we didn’t use it for training). The hypothesis was that if Factor VAE achieves disentan-
glement, it should minimise the synergy as well. We train Factor VAE using the same parameters
and architecture described in Kim & Mnih (2018). We show the first 4000 steps for the Synergy
term (i.e. α * Imax), since most of the interaction happens in the first steps.
Figure 3: Synergy loss for Factor VAE - 4k steps
As a comparison, we also show the synergy for the Non-Syn VAE for the same number of steps in
Fig 4. Surprisingly, Factor VAE minimises the Synergy implicitly by penalising the Total correlation
term.
6
Under review as a conference paper at ICLR 2019
Figure 4: Synergy loss for Non-Syn VAE - 4k steps
6	Conclusions
In this paper we presented the intuition and derivation of the lower bound of a model that uses a novel
approach inspired by the information theory and Neuroscience fields to achieve the disentanglement
of the underlying factor of variations in the data. After looking at the results,we can state that our
model achieved state-of-the-art results, with a performance close to FactorVAE Kim & Mnih (2018).
This is not the first time that a model draws ideas from information theory. Many models Tishby
et al. (1999); Alemi et al. (2016); Achille & Soatto (2016) used the information bottleneck presented
in Tishby et al. (1999) using the VAE framework. Therefore, we truly believe that we should keep
looking at the neuroscience and information theory fields for inspiration. In general, we don’t need
to replicate or simulate biological models; however we should analyse the intuition about the known
main mechanisms of our brain and adapt those to our models.
7
Under review as a conference paper at ICLR 2019
References
Alessandro Achille and Stefano Soatto. Information dropout: learning optimal representations
through noise. CoRR, abs/1611.01353, 2016. URL http://arxiv.org/abs/1611.
01353.
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. CoRR, abs/1612.00410, 2016. URL http://arxiv.org/abs/1612.00410.
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798-1828, 2013. doi: 10.
1109/TPAMI.2013.50. URL https://doi.org/10.1109/TPAMI.2013.50.
Nils Bertschinger, Johannes Rauh, Eckehard Olbrich, and Jurgen Jost. Shared information - new
insights and problems in decomposing information in complex systems. CoRR, abs/1210.5902,
2012. URL http://arxiv.org/abs/1210.5902.
Matthew Botvinick, David G. T. Barrett, Peter Battaglia, Nando de Freitas, Dharshan Kumaran,
Joel Z. Leibo, Tim Lillicrap, Joseph Modayil, S. Mohamed, Neil C. Rabinowitz, Danilo Jimenez
Rezende, Adam Santoro, Tom Schaul, Christopher Summerfield, Greg Wayne, T. Weber, Daan
Wierstra, Shane Legg, and Demis Hassabis. Building machines that learn and think for them-
selves: Commentary on lake et al., behavioral and brain sciences, 2017. CoRR, abs/1711.08378,
2017. URL http://arxiv.org/abs/1711.08378.
Tian Qi Chen, Xuechen Li, Roger B. Grosse, and David K. Duvenaud. Isolating sources of
disentanglement in variational autoencoders. CoRR, abs/1802.04942, 2018. URL http:
//arxiv.org/abs/1802.04942.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems 29: Annual Conference on Neural Informa-
tion Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2172-2180, 2016.
URL https://arxiv.org/abs/1606.03657.
Itay Gat and Naftali Tishby. Synergy and redundancy among brain cells of be-
having monkeys. In Advances in Neural Information Processing Systems
11,	[NIPS Conference, Denver,	Colorado, USA, November 30 - December
5,	1998],	pp. 111-117,	1998. URL http://papers.nips.cc/paper/
1611- synergy- and- redundancy- among- brain- cells- of- behaving- monkeys.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neu-
ral Information Processing Systems 27: Annual Conference on Neural Information Processing
Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672-2680, 2014. URL
http://papers.nips.cc/paper/5423- generative- adversarial- nets.
Virgil Griffith and Christof Koch. Quantifying synergistic mutual information.	CoRR,
abs/1205.4265, 2012. URL http://arxiv.org/abs/1205.4265.
Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick.
Neuroscience-inspired artificial intelligence. Neuron, 95(2):245 - 258, 2017. ISSN 0896-6273.
doi: https://doi.org/10.1016/j.neuron.2017.06.011. URL http://www.sciencedirect.
com/science/article/pii/S0896627317305093.
Irina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles BlundelL Shakir Mo-
hamed, and Alexander Lerchner. Early visual concept learning with unsupervised deep learning.
CoRR, abs/1606.05579, 2016. URL http://arxiv.org/abs/1606.05579.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017a.
8
Under review as a conference paper at ICLR 2019
Irina Higgins, Arka Pal, Andrei A. Rusu, Lolc Matthey, Christopher Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: improving zero-shot
transfer in reinforcement learning. In Proceedings of the 34th International Conference on Ma-
chine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1480-1490, 2017b.
URL http://proceedings.mlr.press/v70/higgins17a.html.
Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher Burgess, Matthew Botvinick,
Demis Hassabis, and Alexander Lerchner. SCAN: learning abstract hierarchical compositional
visual concepts. CoRR, abs/1707.03389, 2017c. URL http://arxiv.org/abs/1707.
03389.
Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the varia-
tional evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS,
2016.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Proceedings of the 35th Inter-
national Conference on Machine Learning, ICML2018, Stockholmsmassan, Stockholm, Sweden,
July 10-15, 2018, pp. 2654-2663, 2018. URL http://proceedings.mlr.press/v80/
kim18b.html.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114,
2013. URL http://arxiv.org/abs/1312.6114.
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
machines that learn and think like people. CoRR, abs/1604.00289, 2016. URL http://arxiv.
org/abs/1604.00289.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
Eckehard Olbrich, Nils Bertschinger, and Johannes Rauh. Information decomposition and synergy.
Entropy, 17(5):3501-3517, 2015. doi: 10.3390/e17053501. URL https://doi.org/10.
3390/e17053501.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31th International Con-
ference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 1278-1286,
2014. URL http://jmlr.org/proceedings/papers/v32/rezende14.html.
Elad Schneidman, William Bialek, and Michael J. Berry. Synergy, redundancy, and indepen-
dence in population codes. Journal of Neuroscience, 23(37):11539-11553, 2003. doi: 10.
1523/JNEUROSCI.23-37-11539.2003. URL http://www.jneurosci.org/content/
23/37/11539.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. pp.
368-377, 1999.
Paul L. Williams and Randall D. Beer. Nonnegative decomposition of multivariate information.
CoRR, abs/1004.2515, 2010. URL http://arxiv.org/abs/1004.2515.
9
Under review as a conference paper at ICLR 2019
7 Appendix
7.1	Details of training
We trained the Non-Syn VAE model for 150,000 steps using the d-sprites data set (unsupervised
setting) obtaining the best result using the following parameters:
•	Optimiser: Adam
•	Learning rate: 1e-4
•	Beta 1 (Adam): 0.9
•	Beta 2 (Adam): 0.999
•	Batch size: 64
•	Discount ω of greedy policy: 0.9
•	Weight of the Synergy loss α: 5.0
7.2	Architecture of Non-syn VAE - dsprites
Encoder	Decoder
Input 64 X 64 binary image	Input
4x4 conv.32 ReLU, Stride 2	FC. 128 ReLU
4x4 conv.32 ReLU, Stride 2	FC.4x4x64 ReLU.
4x4conv.64 ReLU, stride2	4x4UPconv.64ReLU, stride2
4x4 conv.64 ReLU, stride 2	4x4 UPconv.32 ReLU, Stride 2
FC. 128	4x4 UPconv.32 ReLU, Stride 2
FC. 2x10	4x4 UPconv.1, stride 2
7.3	Details of algorithm
Algorithm 2 Imax
Input: Compute specific mutual information
Output:
1:	function IMAX(mu, logvar, index)
2:	mu_syn J mu[:, index]
3:	Iogvarsyn J logvar[:, index]
4:
5:	Imax J compute_KL(mu_syn, Iogvarsyn)
6:
7:	return Imax
10
Under review as a conference paper at ICLR 2019
Algorithm 3 generate_candidate
Input: dimension of Z, index
Output: List of indices
1:	function GENERATE_CANDIDATE(d, index)
2:	if len(d) = 0 then
3:	candidates - [1 : 10]
4:	else
5:	candidates - [1 : 10] not in index
6:	return candidates
Algorithm 4 get_index_greedy
Input: mu μ, log std. deviation σ, discount factor ω
Output: list of indices of the latents with the lowest Specific Mutual Information
function GET_INDEX_GREEDY(mu, logvar, ω)
2:	for i — 1 to d do
candidates J generate_candidate(d, index)	.㊉：bitwise exclusive-or
4:
for c J 1 in candidates do
6:	index — best_index + [c]
Imax_new — Imax(μ, σ, index)
8:
if Imax_new * ω > IjmaxJbest then
10:	ImaXjJest — Imax _new
best.index — index
12:
worst.index - [1 : 10] not in best.index
14:	return worst-index
11