Under review as a conference paper at ICLR 2019
Optimal margin Distribution Network
Anonymous authors
Paper under double-blind review
Ab stract
Recent research about margin theory has proved that maximizing the minimum
margin like support vector machines does not necessarily lead to better perfor-
mance, and instead, it is crucial to optimize the margin distribution. In the mean-
time, margin theory has been used to explain the empirical success of deep net-
work in recent studies. In this paper, we present ODN (the Optimal margin Distri-
bution Network), a network which embeds a loss function in regard to the optimal
margin distribution. We give a theoretical analysis for our method using the PAC-
Bayesian framework, which confirms the significance of the margin distribution
for classification within the framework of deep networks. In addition, empirical
results show that the ODN model always outperforms the baseline cross-entropy
loss model consistently across different regularization situations. And our ODN
model also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge
loss model in generalization task through limited training data.
1 Introduction
In the history of machine learning research, the large margin principle has played an important role in
theoretical analysis of generalization ability, meanwhile, it also achieves remarkable practical results
for classification (Cortes & Vapnik, 1995) and regression problems (Drucker et al., 1997). More than
that, this powerful principle has been used to explain the empirical success of deep neural network.
Bartlett et al. (2017) and Neyshabur et al. (2018) present a margin-based multi-class generalization
bound for neural networks that scales with their margin-normalized spectral complexity using two
different proving tools. Moreover, Arora et al. (2018) proposes a stronger generalization bounds for
deep networks via a compression approach, which are orders of magnitude better in practice.
As for margin theory, Schapire et al. (1997) first introduces it to explain the phenomenon that Ad-
aBoost seems resistant to overfitting problem. Two years later, Breiman (1999) indicates that the
minimum margin is important to achieve a good performance. However, Reyzin & Schapire (2006)
conjectures that the margin distribution, rather than the minimum margin, plays a key role in being
empirically resistant to overfitting problem; this has been finally proved by Gao & Zhou (2013). In
order to restrict the complexity of hypothesis space suitably, a possible way is to design a classifier
to obtain optimal margin distribution.
Gao & Zhou (2013) proves that, to attain the optimal margin distribution, it is crucial to consider
not only the margin mean but also the margin variance. Inspired by this idea, Zhang & Zhou (2016)
proposes the optimal margin distribution machine (ODM) for binary classification, which optimizes
the margin distribution through the first- and second-order statistics, i.e., maximizing the margin
mean and minimizing the margin variance simultaneously. To expand this method to the multi-class
classification problem, Zhang & Zhou (2017) presents a multi-class version of ODM.
Based on these recent works, we consider the expansion of the optimal margin distribution principle
on deep neural networks. In this paper, we propose an optimal margin distribution loss for con-
volution neural networks, which is not only maximizing the margin mean but also minimizing the
margin variance as ODM does. Moreover, we use the PAC-Bayesian framework to derive a novel
generalization bound based on margin distribution. Comparing to the spectrally-normalized margin
bounds of Bartlett et al. (2017) and Neyshabur et al. (2018), our generalization bound shows that we
can restrict the capacity of the model by setting an appropriate ratio between the first-order statistic
and the second-order statistic rather than trying to control the whole product of the spectral norms of
each layer. And we empirically evaluate our loss function on deep network across different datasets
1
Under review as a conference paper at ICLR 2019
ODN Loss Function
O r-θ
(a)	(b)
Figure 1: The optimal margin distribution loss function (a) and the structure of fully connected
network with optimal margin distribution loss (b).
and model structures. Specifically, we consider the performance of these models in generalization
task through limited training data.
Recently, many researchers try to explain the experimental success of deep neural network. One of
the research direction is to explain why the deep learning does not have serious overfitting problem.
Although several common techniques, such as dropout (Srivastava et al., 2014), batch normalization
(Ioffe & Szegedy, 2015), and weight decay (Krogh & Hertz, 1992), do improve the generalization
performance of the over-parameterized deep models, these techniques do not have a solid theoretical
foundation to explain the corresponding effects. As for our optimal margin distribution loss, it has
a generalization bound to prove that we can restrict the complexity of hypothesis space reasonably
through searching appropriate statistics dependent on data distribution. In experimental section, we
compare our optimal margin distribution loss with the baseline cross-entropy loss under different
regularization methods.
2 Optimal Margin Distribution Loss
Consider the classification problem with input domain X = {x| x ∈ Rn } and output domain
Y = {1, . . . , k}, we denote a labeled sample as z ∈ (X, Y). Suppose we use a network generating
a prediction score for the input vector x ∈ X to class i, through a function fi : X → R, for i =
1, . . . , k. The predicted label is chosen by the class with maximal score, i.e. h(x) = arg maxi fi(x).
Define the decision boundary of each class pair {i, j} as:
Di,j := {x|fi(x) = fj (x)}
Constructed on this definition, the margin distance of a sample point x to the decision boundary Di,j
is defined by the smallest translation of the sample point to establish the equation as:
∣γ(x,i,j)| = min ∣∣δ∣∣2 s.t. fi(x + δ) = f (X + δ).
δ
In order to approximate the margin distance in the nonlinear situation, Elsayed et al. (2018) has
offered a linearizing definition:
|b(x,i,j)|
Ifi(X) - fj(X)I
l∣Vχfi(χ) - Vxfj(χ)k2
Naturally, this pairwise margin distance leads us to the following definition of the margin for a
labeled sample z = (X, y):
(X ) =	fy(X)- maxi=y fi(X)
γh( ,y)= kVxfy(X)-Vxmaxi=y fi(X)∣2.
Therefore, the defined classifier h misclassifies (X, y) if and only if the margin is negative. Given a
hypothesis space HS of functions mapping X to Y, which can be learned by the fixed deep neural
2
Under review as a conference paper at ICLR 2019
network through the training set S, our purpose is to find a way to learn a decision function h ∈ HS
such that the generalization error R(h) = E(x,y)~(x,γ) [1h(χ)=y] is small.
In this work, we introduce a type of margin loss, and connect it to deep neural networks. The origin
loss function has been specially adapted for the difference between deep learning models and linear
models by us as following definition:
{r-θ-Yh
r-θ
0
μ(Yh -r-θ)2
(r+θ)2
γh ≤ r - θ
r - θ < γh ≤ r + θ
γh > r + θ,
(1)
where r is the margin mean, θ is the margin variance and μ is a parameter to trade off two different
kinds of deviation (keeping the balance on both sides of the margin mean). In the Appendix A,
we explain the reason why we use these three hyper-parameters to construct such a optimal margin
distribution loss function.
Fig. 1 shows, equation 1 will produce a linear loss decreasing progressively when the margins of
sample points satisfy γh ≤ r-θ and a square loss increasing progressively when the margins satisfy
γh ≥ r + θ. Therefore, our margin loss function will enforce the tie which has zero loss to contain
the sample points as many as possible. So the parameters of the classifier will be determined not only
by the samples that are close to the decision boundary but also by the samples that are away from
the decision boundary. In other words, our loss function is aimed at finding a decision boundary
which is determined by the whole sample margin distribution, instead of the minority samples that
have minimum margins. To verify superiority of the optimal margin distribution network, our paper
verifies it both theoretically and empirically.
3 Analysis
To present a new margin bound for our optimal margin distribution loss, some notations are needed.
Consider that the convolution neural networks can be regarded as a special structure of the fully
connected neural networks, we simplify the definition of the deep networks. Let fw (x) : X → Rk
be the function learned by a L-layer feed-forward network for the classification task with parameters
w = (W1, . . . , WL), thus fw(x) = WLφ(WL-1φ(. . . φ(W1x))), here φ is the ReLU activation
function. Let fwi denote the output of layer i before activation and ρ be an upper bound on the
number of output units in each layer. Recursively, we can redefine the deep network: fw1 (x) = W1x
and fwi (x) = Wiφ(fwi-1(x)). Let k.k F, ∣∣∙∣∣ ι and ∣∣∙∣∣2 denote the Frobenius norm, the element-wise
`1 norm and the spectral norm respectively.
In order to facilitate the theoretical derivation of our formula, we simplify the definition of the loss
function:
Lr,θ (fw )
Pr
(x,y)~(X,Y)
fw,y(x) — maxi=y fw,i(x)	≤ r - 9
▽xfw,y (X) - Nx maxi=y fw,i (X)	_
+ Pr
(x,y)~(X,Y)
fw,y(X) ― maxj=y fw,j(X)
▽xfw,y (X) - Vx maχj=y fw,j (X)
≥r+θ
Specially, define the L0 as r = θ and θ → ∞, actually equal to the 0-1 loss. And let Lbr,θ (fw) be
the empirical estimate of the optimal margin distribution loss. So we will denote the expected risk
and the empirical risk as L0(fw) and L0(fw), which are bounded between 0 and 1.
In the PAC-Bayesian framework, one expresses the prior knowledge by defining a prior distribution
over the hypothesis class. Following the Bayesian reasoning approach, the output of the learning
algorithm is not necessarily a single hypothesis. Instead, the learning process defines a posterior
probability over H, which we denote by Q. In the context of a supervised learning problem, where
H contains functions from X to Y , one can think of Q as defining a randomized prediction rule.
We consider the distribution Q which is learned from the training data of form fw+u, where u is a
random variable whose distribution may also depend on the training data. Let P be a prior distri-
bution over H that is independent of the training data, the PAC-Bayesian theorem states that with
possibility at least 1 - δ over the choice ofan i.i.d. training set S = {z1, ..., zm} sampled according
to (X, Y), for all distributions Q over H (even such that depend on S), we have (McAllester, 2003):
3
Under review as a conference paper at ICLR 2019
Eu[Lo(fw+u)] ≤ Eu[Lo(fw+u)] + {(以式之谓-；+ 1"m.	(2)
Note that the left side of the inequality is based on fw+u. To derive an expected risk bound L0(fw)
for a single predictor fw , we have to relate this PAC-Bayesian bound to the expected perturbed loss
just like Neyshabur et al. (2018) derive the Lemma 1 in their paper. Based on the inequality 2, we
introduce a perturbed restriction which is related to the margin distribution (the margin mean r and
margin variance θ):
Lemma 1. Let fw (x) : X → Rk be any predictor with parameters w, and P be any distribution
on the parameters that is independent of the training data. Then, for any r > θ > 0, δ > 0, with
probability at least 1 - δ over the training set of size m, for any w, and any random perturbation u
St Pru [maXχ∈χ ∣fw+u(x) - fw(x)∣∞ < r-θ≥ 2, we have：
L0(fw ) ≤ Lr,θ (fw) + S DKL(W +u-P)+ln 3m.
The margin variance information does not change the conclusion of the perturbed restriction, the
proof of this lemma is similar to Lemma 1 in Neyshabur et al. (2018).
In order to bound the change caused by perturbation, we have to bring in three definitions that are
used to formalize error-resilience in Arora et al. (2018) as follows:
Definition 1. (Layer Cushion). The layer cushion oflayer i is defined to be largest number μi such
that for any x ∈ S:
MWikFMfwrI(X川2 ≤ kfw(x)k2.
Intuitively, cushion considers how much smaller the output Wiφ(fwi-1(x)) is compared to the upper
bound kWi k2 kφfwi-1(x)k2. However, for nonlinear operators the definition of error resilience is
less clean. Let’s denote Mi,j : Rhi → Rhj the operator corresponding to the portion of the deep
network from layer i to layer j, and by Ji,j its Jacobian. If infinitesimal noise is injected before
level i then Mi,j passes it like Ji,j , a linear operator. When the noise is small but not infinitesimal
then one hopes that we can still capture the local linear approximation of the nonlinear operator M
by define Interlayer Cushion:
Definition 2. (Interlayer Cushion). For any two layers i < j, we define the interlayer cushion μ%,j,
as the largest number such that for any x ∈ S:
〃i,j Jfw(x)kFMfwrI(X川2 ≤kfW(χ)k2.
Furthermore, for any layer i we define the minimal interlayer cushion as μi→ = mini≤j≤L μi,j =
min{ √ρ, mini≤j≤L μi,j }.
The next condition qualifies a common appearance: if the input to the activations is well-distributed
and the activations do not correlate with the magnitude of the input, then one would expect that on
average, the effect of applying activations at any layer is to decrease the norm of the pre-activation
vector by at most some small constant factor.
Definition 3. (Activation Contraction). The activation contraction c is defined as the smallest num-
ber such that for any layer i and any X ∈ X:
Mfw(X川2 ≥ kfw(X)k2
c
To guarantee that the perturbation of the random variable u will not cause a large change on the
output with high possibility, we need a perturbation bound to relate the change of output to the
structure of the network and the prior distribution P over H. Fortunately, Neyshabur et al. (2018)
proved a restriction on the change of the output by norms of the parameter weights. In the follow-
ing lemma, we preset our hyper-parameters r and θ, s.t. the parameter weights w ∈ H satisfying
kfw (X)k2 ≤ r + θ, when fixing kWL k2 = 1. Thus, we can bound this change in terms of the
spectral norm of the layer and the presetting hyper-parameters:
4
Under review as a conference paper at ICLR 2019
Lemma 2. For any L > 0, let fw : X → Rk be a L-layer network. Then for any w ∈ H satisfying
kfw(x)k2 ≤ r + θ, and x ∈ X, and any perturbation u = vec({Ui}L=ι) s.t. ∣∣Ui∣∣2 ≤ LIIWiIl2,
the change of the output of the network can be bounded as follows:
|fw+u(x) - fw(x)|2 ≤ O
c(r + θ) X	∣Ui∣2	1
J + ) i=1 kWik2〃i〃i—J.
The proof of this lemma is given in Appendix B. Eventually, we utilize all above bounding lemmas
and the error-resilience definitions to derive the following new margin based generalization bound
for our Optimal margin Distribution Network.
Theorem 1. (Generalization Bound). For any L, ρ > 0, let fw : X → Rk be a L-layer feed-forward
network with ReLU activations. Then, for any δ > 0,r > θ > 0, with probability ≥ 1 - δ over a
training set of size m, for any w, we have:
(
t
∖
L0(fw) ≤ Lbr,θ(fw) +O
c2L2ρIn(LP)(r + θ)2 Pi= kw¾W⅛→ + ln 噜
m(r - θ)2
The proof of this theorem is given in Appendix B.
Remark. Comparing with the spectral complexity in Bartlett et al. (2017):
Rw := YL IWi I2
i=1
kWi- © 2/13 !3/2
kWik2/3	)
(3)
which is dominated by the product of spectral norms across all, our margin bound is relevant to
r, θ dependent on the margin distribution and μi and μi→ dependent on the network structure.
The product value in equation 3 is extremely large and is hard to control it, but the parameter in
our generalization bound is easy to restrict. Explicitly, the factor consisted of hyper-parameters
(r⅛θ) = (1⅛) = (1⅛ — 1) is a monotonicity increasing function with regard to the ratio
θ ∈ [0,1). Under the assumption of separability, we can come to the conclusion that maller θ and
larger r make the complexity smaller. Searching a suitable value of r and θ for the specific data
distribution will lead us to a better generalization performance.
4 Experiment
In this section, we empirically evaluate the effectiveness of our optimal margin distribution loss on
generalization tasks, comparing it with three other loss functions: cross-entropy loss (Xent), hinge
loss, and soft hinge loss. We first compare them under limited training data situation, using only
part of the MNIST dataset (LeCun et al., 1998) to train and evaluate the models deploying the four
different losses, with the used data ratio ranging from 0.125% to 100%. Similar experiments are
also performed on the legend CIFAR-10 dataset (Krizhevsky & Hinton, 2009). Then we compare
them under different regularization situations, investigating the combination of optimal margin dis-
tribution loss with dropout and batch normalization. Finally, we visualize and compare the features
learned by the deep learning model with the four lose functions as well as the margin distribution
from those models.
Here we introduce three commonly used loss functions in deep learning for comparison in the ex-
perimental section:
Cross-entropy Loss (Xent):
LXent (fw )
- ln f	eχPfw,y (X))
∖Pk=1 eχpfw,i(X))
Hinge Loss:
(	) =	fw,y (X) - maχi=y fw,i (X)
Yh ",y	∣∣Vχfw,y(x) - Vχ maxi=y fw,i(x)∣∣2
LHinge (fw ) = maχ [0, γh (X, y) - γ0] ,
5
Under review as a conference paper at ICLR 2019
where γ0 is a hyper-parameter to control the minimum margin as support vector machine does.
Soft Hinge Loss:
LSoftHinge(fw) = --1 ∙ ln(1 + exp(-fw,y(x))) - X ln(1+exp(-fw,i(x))),
i6=y
where - is the number of classes.
4.1	Experimental Setup
Regarding the deep models, we use the following combination of datasets and models: a simple
deep convolutional network for MNIST, original Alexnet (Krizhevsky et al., 2012) for CIFAR-10.
In terms of the implementation of optimal margin distribution loss, as shown in Section 2, there
is a gradient term in the loss itself, which can make the computation expensive. To reduce com-
putational cost as Elsayed et al. (2018) do, in the backpropagation step we considered the gra-
dient term ∣∣Vχfy (x) - Rx maxi=y fi(x)∣∣2 as a constant, so that We recomputed the value of
∣∣Vχfy(x) -Vx maxi=y fi(x)∣∣2 at every forward propagation step. Furthermore, since the de-
nominator item could be too small, Which Would cause numerical problem, We added an With
small value to the denominator so that clip the loss at some threshold.
For special hyperparameters, including the margin mean parameter and margin variance parame-
ter for the ODN model, and margin parameter for hinge loss model, we performed hyperparameter
searching. We held out 5000 samples of the training set as a validation set, and used the remaining
samples to train models with different special hyperparameters values, on both the MNIST dataset
and the CIFAR-10 dataset. As for the common hyperparameters, such as, learning rate and momen-
tum, we set them as the default commonly used values in Pytorch for all the models. We chose
batch stochastic gradient descent as the optimizer. Evaluated on the testing dataset, the baseline
cross-entropy model achieves a test accuracy of 99.09%; the hinge loss model achieves 98.95% on
MNIST dataset; the soft-hinge loss model achieves 99.14% and the ODN model achieves 99.16%.
On the CIFAR-10 dataset, the baseline cross-entropy model trained on the remaining training sam-
ples achieves a test accuracy of 83.51%; the hinge loss model achieves 82.15%; the soft-hinge loss
model achieves 81.96% and the ODN model achieves 84.61%.
4.2	Limited Small Sample Learning
It is well-known that deep learning method is very data-hungry, which means that if the training
data size decreases, the model’s performance can decrease significantly. In reality, this disadvantage
of deep learning method can restrict its application seriously since sufficient amount of data is not
always available. On the other hand, one of the desirable property of optimal margin distribution
loss based models is that it can generalize well even when the training data is insufficient because
the optimal margin distribution loss can restrict the complexity of the hypothesis space suitably.
To evaluate the performance of optimal margin distribution loss based models under insufficient
training data setting, we randomly chose some fraction of the training set, in particular, from 100%
of the training samples to 0.125% on the MNIST dataset, and from 100% of the training samples to
0.5% on the CIFAR-10 dataset, and train the models accordingly.
In Fig. 2, we show the test accuracies of cross-entropy, hinge, soft hinge, and optimal margin dis-
tribution loss based models trained on different fractions of the MNIST and CIFAR-10 dataset. As
shown in the figure, the test accuracies of all these four models increase as the fraction of training
samples increases. Obviously, the ODN models proposed by our paper outperform all the other mod-
els constantly across different datasets and different fractions. Furthermore, the less training data
there are, the larger performance gain the ODN model can have. On the MNIST dataset, the optimal
margin distribution loss based model outperforms cross-entropy loss model by around 4.95%, hinge
loss model by around 6.84% and soft-hinge loss model by around 3.03% on the smallest training set
which contains only 0.125% of the whole training samples. Similarly, The ODN model outperforms
cross-entropy loss model by around 9.9%, hinge loss model by around 10.1%, and soft hinge loss
model by 13.4% on the smallest CIFAR-10 dataset which contains only 0.5% of the whole training
samples.
6
Under review as a conference paper at ICLR 2019
(a)
Figure 2: Performance of selected MNIST (a) and CIFAR10 (b) models on generalization tasks.
(b)
4.3	Regularization Methods
Table 1: Test accuracy of Alexnet on CIFAR-10 with different regularization methods and different
fraction of training set.
Accuracy (%)	Batch Normalization		Non Batch Normalization	
	Xent	ODN	Xent	ODN
ALL-DROPOUT	85.782 ± 0.198	87.644 ± 0.151	83.517 ± 0.322	84.643 ± 0.255
ALL-NON-DROPOUT	81.491 ± 0.143	86.233 ± 0.244	72.223 ± 1.284	76.793 ± 1.279
5%_DROPOUT	61.955 ± 1.945	67.636 ± 1.633	50.747 ± 3.735	58.739 ± 1.348
5%_NON_DROPOUT	57.753 ± 2.228	64.173 ± 1.982	36.293 ± 4.872	47.056 ± 3.927
Hyper-param (r / θ / μ)	-	30/0.7/0.1	-	1.2/0.7/0.1
We also compared our optimal margin distribution loss with the baseline cross-entropy loss under
different regularization methods and different amounts of training data, whose results are shown
in Table 1. As suggested by Table 1, our loss can outperform the baseline loss consistently across
different situations, no matter whether dropout, batch normalization or all the CIFAR-10 dataset
are used or not. Specifically, when the size scale of training samples is small (5% fraction of the
CIFAR-10 training set), the advantage of our optimal margin distribution loss is more significant.
Moreover, our optimal margin distribution loss can cooperate with batch normalization and dropout,
achieving the best performance in Table.1, which is shown in bold red text. Unlike dropout and
batch normalization which are lack of solid theory ground, our optimal margin distribution loss has
the margin bound, which guides Us to find the suitable ratio θ to restrict the capacity of models and
alleviate the overfitting problem efficiently.
4.4	Data Visualization
Since the performance of the ODN models is excellent, we hope to see that the distributions of data
in the learned feature space (the last hidden layer) are consistent with the generalization results. In
this experiment, we use t-SNE method to visualize the data distribution on the last hidden layer
for training samples and test samples. Fig. 3 and Fig. 4 plots the 2-dimension embedding image
on limited MNIST and CIFAR-10 dataset, which is only 1% of the whole training samples. t-
SNE (Maaten & Hinton, 2008) is a tool to visualize high-dimensional data. It converts similarities
between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence
between the joint probabilities of the low-dimensional embedding and the high-dimensional data.
7
Under review as a conference paper at ICLR 2019
(a)
15
10
5
0
-5
-10
-15
(b)
Figure 3: Learned features visualization of selected MNIST models on training set (a); Learned
features visualization of selected CIFAR-10 models on training set (b).
20
15
10
-15	-10	-5	0	5	10	15
-15	-10	-5	0	5	10
8
Under review as a conference paper at ICLR 2019
(a)
Cross-entropy Loss
Hinge Loss
-15
-10
0
5
10
15
-15
-10
0
5
10
(b)
Figure 4: Learned features visualization of selected MNIST models on testing set (a); Learned
features visualization of selected CIFAR-10 models on testing set (b).
9
Under review as a conference paper at ICLR 2019
Table 2: Variance decomposition of selected MNIST models on embedding space.
Models	Training data				Test data			
	Xent	Hinge	Soft Hinge	ODN	Xent	Hinge	Soft Hinge	ODN
I nter C lass V ar	522	529	466	190	831	811	854	649
I ntra C lass V ar	11200	11092	14128	16469	13007	12986	11362	13955
ratio	21.45	20.96	30.32	86.68	15.65	16.01	13.3	21.5
Table 3: Variance decomposition of selected CIFAR-10 models on embedding space.
Models	Training data				Test data			
	Xent	Hinge	Soft Hinge	ODN	Xent	Hinge	Soft Hinge	ODN
I nter C lass V ar	804	713	637	193	1993	1429	1917	1279
I ntra C lass V ar	15692	9466	17546	13273	7260	4780	5810	5645
ratio	19.52	13.28	27.55	68.77	3.64	3.34	3.03	4.41
Consistently, we can find that the result of our ODN model is better than all the others, the distribu-
tion of the samples which has the same label is more compact. To quantify the degree of compactness
of the distribution, we perform a variance decomposition on the data in the embedding space. By
comparing the ratio of the intra-class variance to the inter-class variance in Table 2 and Table 3, we
can know that our optimal margin distribution loss alway attain the most compact distribution in
these four loss functions.
Figure 5: Margin distribution of selected MNIST models.
Moreover, the visualization result is consistent with the margin distribution of these four models in
Fig. 5, which means getting an optimal margin distribution is helpful to deriving a good learned
features space. And that representation features space can further alleviate the overfitting problem
of deep learning. Hence, the optimal margin distribution loss function can significantly outperforms
the other loss functions in generalization task through limited training data.
4.5 Margin Distributions
Fig. 5 plots the kernel density estimates of margin distribution producted by cross-entropy loss,
hinge loss, soft hinge loss and ODN models on dataset MNIST. As can be seen, our ODN model
derives a large margin mean with a smallest margin variance in all these four models. By calculating
the value of ratio between the margin mean and the margin standard deviation, we know that the
ratio in our ODN model is 3.20 which is significantly larger than 2.38 in the cross-entropy loss,
10
Under review as a conference paper at ICLR 2019
2.35 in the hinge loss and 2.63 in the soft hinge loss. The distribution of our model becomes more
”sharper”, which prevents the instance with small margin, so our method can still perform well as
the training data is limited, which is also consistent with the result in Fig. 2.
5 Conclusions
Recent studies disclose that maximizing the minimum margin for decision boundary does not nec-
essarily lead to better generalization performance, and instead, it is crucial to optimize the margin
distribution. However, the influence of margin distribution for deep networks still remains undis-
cussed. We propose ODN model trying to design a loss function which aims to control the ratio
between the margin mean and the margin variance. Moreover, we present a theoretical analysis for
our method, which confirms the significance of margin distribution in generalization performance.
As for experiments, the results validate the superiority of our method in limited data problem. And
our optimal margin distribution loss function can cooperate with batch normalization and dropout,
achieving a better generalization performance.
Acknowledgments
We are grateful to Yu Li and Kangle Zhao for discussions and helpful feedback on the manuscript.
11
Under review as a conference paper at ICLR 2019
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In Proceedings of the 35th International Conference on
Machine Learning,pp. 254-263, 2018.
Peter L. Bartlett, Dylan J. Foster, and Matus J. Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Leo Breiman. Prediction games and Arcing algorithms. Neural Computation, 11(7):1493-1517,
1999.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273-297,
1995.
Harris Drucker, Christopher J. C. Burges, Linda Kaufman, Alex J. Smola, and Vladimir Vapnik.
Support vector regression machines. In Advances in Neural Information Processing Systems, pp.
155-161. MIT Press, 1997.
Gamaleldin F Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classification. CoRR, abs/1803.05598, 2018.
Wei Gao and Zhi-Hua Zhou. On the doubt about margin explanation of boosting. Artificial Intelli-
gence, 203:1-18, 2013.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning, pp. 448-456, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical Report 4, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105.
2012.
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances
in neural information processing systems, pp. 950-957, 1992.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Lester Mackey, Michael I Jordan, Richard Y Chen, Brendan Farrell, Joel A Tropp, et al. Matrix
concentration inequalities via the method of exchangeable pairs. The Annals of Probability, 42
(3):906-945, 2014.
David McAllester. Simplified PAC-Bayesian margin bounds. In Learning Theory and Kernel Ma-
chines, pp. 203-215. 2003.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018.
Lev Reyzin and Robert E. Schapire. How boosting the margin can also boost classifier complexity.
In Proceedings of the 23rd International Conference on Machine Learning, pp. 753-760. ACM,
2006.
Robert E. Schapire, Yoav Freund, Peter Barlett, and Wee Sun Lee. Boosting the margin: A new ex-
planation for the effectiveness of voting methods. In Proceedings of the Fourteenth International
Conference on Machine Learning, pp. 322-330, 1997.
12
Under review as a conference paper at ICLR 2019
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
LearningResearch,15(1):1929-1958, 2014.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012.
Teng Zhang and Zhi-Hua Zhou. Optimal margin distribution machine. CoRR, abs/1604.03348,
2016.
Teng Zhang and Zhi-Hua Zhou. Multi-class optimal margin distribution machine. In Proceedings
of the 34th International Conference on Machine Learning, volume 70, pp. 4063-4071, 2017.
13
Under review as a conference paper at ICLR 2019
A Explaination for Optimal margin Distribution Loss
Inspired by the optimal margin distribution principle, Zhang & Zhou (2017) propose the multi-class
optimal margin distribution machine, which characterizes the margin distribution according to the
first- and second-order statistics. Specially, let Y denote the margin mean, and the optimal margin
distribution machine can be formulated as:
λm
min ω(W) - ηγ + — X(ξ2 + e2)
w,羽ξi,ei	m
i=1
s.t. γh (xi , yi ) ≥ γY - ξi
γh(xi,yi) ≤ γY + i,∀i,
where Ω(w) is the regularization term to penalize the norm of the weights, η and λ are trading-off
parameters, ξi and are the deviation of the margin γh(xi, yi) to the margin mean. It is evident that
Pim=1(ξi2 + i2)/m is exactly the margin mean.
In the linear situation, scaling w does not affect the final classification results such as SVM, the
margin mean can be normalized as 1, then the deviation of the margin of (xi , yi ) to the margin mean
is ∣γh(xi, yi) - 1|, and the formula can be reconstruct as:
min Ω(w) + A X ξ2±μ∣
w,ξi,∈i	m i=1 (1 — θ)2
s.t. γh (xi , yi ) ≥ 1 - θ - ξi
γh (xi, yi) ≤ 1 + θ + i, ∀i,
where μ ∈ (0,1] is parameter to trade off two different kinds of deviation (keeping the balance on
both sides of the margin mean). θ ∈ [0, 1) is a parameter of the zero loss band, which can control
the number of support vectors. In other words, θ is a parameter to control the margin variance, while
the data which is out of this zero loss band will be used to update the weights to minimize the loss.
For this reason, we simply regard it as the margin variance.
However, under the non-linear setting in our paper, we can not directly linearly normalize the margin
mean to the value 1. So we assume that the normalized margin mean is r, then the optimization target
can be reformulated as:
min Ω(w) + λ X ξ2+μ2
w,ξi,i	m i=1 (r - θ)2
s.t. γh(xi,yi) ≥ r - θ - ξi
γh(xi,yi) ≤ r + θ + i, ∀i.
In our paper, we use the linear approximation (Elsayed et al., 2018) to normalize the magnitude of
the norm of weights, so we can just transform this optimization target to a loss function as:
{(r-θ-γh)2
(r-θ)2
0
μ(Yh -r-θ)2
-(r+θ)2-
γh ≤ r - θ
r - θ < γh ≤ r + θ
γh > r + θ,
There is always some noise in the actual data, when deep network try to fit these data, the perfor-
mance of model get worse with a larger margin variance. So we hope the larger side of the margin
mean has a larger loss, which can effectively control the noise-fitting ability of models. That is why
we adapt the smaller side of the margin mean to hinge form as:
{r-θ-Yh
r-θ
0
μ(Yh -r-θ)2
(r+θ)2
γh ≤ r - θ
r - θ < γh ≤ r + θ
γh > r + θ
14
Under review as a conference paper at ICLR 2019
B Proof of Lemma 2 and Theorem 1
Proof of Lemma 2.
Let fW-i(∙) = WlΦ(Wl-iΦ... Wi+ιφ(∙)) and fW-1(∙) = Wi-1φ(Wi-2φ(∙∙∙ W1 (∙))), we will
write the network as fw = fbwL-i(Wiφ(fwi-1(x))). If we just give ith layer parameter weights Wi a
perturbation Ui , we can have following:
k∆ik2 = k(Wi+Ui)φ(fwi-1(x))-Wiφ(fwi-1(x))k2
= kUiφ(fwi-1(x))k2
≤kUik2 ∙kΦfW-1(χ)k2
’kfw(X)k2 ∙∣Wkl
(4)
In the last Approximate equation in Equation 4, we assume that the perturbation Ui is in the linear
space span by Wi, therefore, the part of φ(fwi-1(x)) that is orthogonal to the space of Wi will
not affect the output of perturbation. In other word, we equal the projection on the linear space of
Wi + Ui with the one on the linear space of Wi, ie. kΦfW-1(x)k2 = fWX)k2.
w	kWi k2
kMi,i+1(fwi (x)+∆i)-Jfi,wii+(x1)(fwi (x) + ∆i)k2
≤ kWi+1k2k∆ik2 ≤
k∆ik2 kWi+1 ΦfW (x)k2
kfW (X)k2
kWi+1k2kfW (x)k2
kWi+1 ΦfW (χ)k2
k∆ik2kWi+1ΦfW (x)k2
kfW (X)k2
k∆ik2kWi+1ΦfW (x)k2
kfW (X)k2
k∆ik2kWi+1ΦfW (x)k2
kfW (X)k2
kWi+1k2kfW (x)k2
μi+1kwi+1kF kφfW (X)k2
ckWi+1k2
μi+ιk wi+ιkF
c
μi+1ri+1
Layer Cushion
Activation Contraction
≤≡2 kfw+1(X)k2
c
μi+ιr
where ri+ι is the stable rank of layer i + 1, i.e. kW+jkF . Therefore by induction method We have:
j
kMi,j (fW (X) + A)- Jfw (x)(fW (χ) + ∆i)k2 ≤w2 kfW (X)k2 ∙ Y μk+⅛k+1
cj-i
=O( ^--^)
μj i
15
Under review as a conference paper at ICLR 2019
d
Obviously, We can know that kMi,j f (x) + △，)- Jfw(x)(fW(x) + ∆i)∣∣2 ≤ O(μd), When
d=j-i≥2
-ʌr -	-ʌr -	-	..
fW-i(fW (χ) + ∆i) - fW-i(fW (X))
= kMi,L(fwi (x) + △i) - M i,L(fwi (x))k2
=kMi,L(fW(x) + ∆i) - Mi,L(fW(x)) + JfW(X)(∆i) - JfW(X)(∆i)k2
≤ k jfW(χ)(∆i)k2 + kM i,L(fW (X) + ∆i) - M i,L(fWw (X))- JfW(X)(∆i)k2
≤ k JfW(χ)(∆i)k2 + k(Mi,L - JfW(X))(fW(x) + ∆i)k2 + k(M j- JfW(X))(fW(x))k2
cL-i
≤k JfW(X)(A)k2 +O( L)
μ
cL-i
≤ kJfft(X)kF kWikF kfW-1(x)k2 + O( F )
w μ
cL-i
≤ CkJfiL(X)kFl∣Wi∣∣FkΦfW-l(x)k2 + O(-ɪ-)	Activation Contraction
JS	μ ”
c	cL--
≤ μ k Jf,L(X)kF l∣WiΦfW-l(x)k2 + O( μL--)	Layer Cushion
c	cL--
=μ kJfW(X)kF kfW (χ)k2 + o(μL-i )
c	cL--
≤-------|fw (x) 12 + O(	)	Interlayer Cushion
μ-μ-→	μL-
≤ O ((r + θ) kUik2 —c—)
≤ V + ) kWik2 μiμi→)
Suppose that all the perturbations U- are independent from each other, so we can just add the influ-
ence linearly for union bound:
∣fw+u(x) - fw(x)∣2 ≤ O (c(r + θ) XT UJUik2	).
∖	= IIWik2μiμi→)
Proof. of Theorem 1.
The proof involves chiefly two steps. In the first step we bound the maximum value of perturbation
of parameters to satisfied the condition that the change of output restricted by hyper-parameter of
margin r, using Lemma 2. In the second step we proof the final margin generalization bound through
Lemma 1 with the value of KL term calculated based on the bound in the first step.
1
Let β = (q3 k Wi ∣∣2) L and consider a network structured by normalized weights Wi =
kWβk2 Wi. Due to the homogeneity of the ReLU, we have that for feedforward networks with ReLU
activations fwe = fw , so the empirical and expected loss is the same for we and w. Furthermore, we
can also get that Qi=1 kWik2 = Qi=1 kWi l∣2 and IWf = Ifkjf ∙ Hence, we can just assume
that the spectral norm is equal across the layers, i.e. for any layer i, kWik2 = β.
When we choose the distribution of the prior P to be N(0, σI), i.e. U 〜N(0, σI), the problem is
that we will set the parameter σ according to β, which can not depend on the learned predictor w
or its norm. Neyshabur et al. (2018) proposed a method that can avoid this block: they set σ based
on an approximation β on a pre-determined grid. By formalizing this method, we can establish the
generalization bound for all W for which ∣coβ 一 β∣ ≤ Lβ, while given a constant c, and ensuring
that each relevant value of cβ is covered by some βe on the grid, i.e. c1 ⅜ ≤ PL= βμμ→ ≤ C2 β,
μiμi→ can be considered as a constant.
16
Under review as a conference paper at ICLR 2019
Since U 〜N(0, σ21), We get the following bound for the spectral norm of Ui according to the
matrix extension of Hoeffding’s inequalities (Tropp, 2012; Mackey et al., 2014):
t2
[∣∣Ui∣∣2 ≥ t] ≤ 2ρe 2ρσ2 .
(5)
Pr
Ui~N (0,σ2I)
Taking the union bound over layers, with probability ≥ 2, the spectral norm of each layer perturba-
tion Ui is bounded by σ,2ρln(4Lρ). Plugging this into Lemma 2 we have that with probability
≥ 1:
≥ 2 :
maχ ∣fw+u(x) — fw (x)∣2 ≤ C(T + θ) X JUik2
x∈X	i=1 βμiμi→
≤ eL(r + θ)β-1σp2ρ ln(4Lρ) ≤ rɪ^
We can derive σ =-----------r-θ∕	from the above inequality. Naturally, we can calculate the
cL(r+θ)β-1 ʌ/p ln(4ρL)	FJ	J
KL-diversity in Lemma 1 with the chosen distributions for P ~ N(0,σ2I).
DKL(W + UllP) ≤ L 2 ≤ O c c2 L2 P In(LP)
2σ2
"β-2 X kWikF)
(T + θ)2 X kWikF !
(r — θ)2 i=1 β2μiμi→y
(r + θ)2 X	kWikF
(r -θ)2 i=1 kWik2μiμi→
≤ O c2 L2 P ln(LP)
≤ O c2 L2 P ln(LP)
T T	Γ∙	/-ɪ	∙ . ∖	F F ∙ 1 ∙ .	¢-	1 Γ∙	11	1 .1 . ∖ ^Γ>	Cl / IC	1
Hence, for any β, with probability ≥ 1 一 δ and for all W such that, ∣β 一 β∣ ≤ Lβ, we have:
L0(fw) ≤ Lbr,θ(fw) +O
t
∖
C2L2PIn(LP)(r + θ)2 P= kWiWμi2Li→ +ln 华
(r 一 θ)2m
This proof method based on PAC-Bayesian framework has been raised by Neyshabur et al. (2018),
we use this convenient tool for proofing generalization bound with our loss function which can
obtain the optimal margin distribution.
17