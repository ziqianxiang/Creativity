Under review as a conference paper at ICLR 2019
Dense Morphological Network: An Universal
Function Approximator
Anonymous authors
Paper under double-blind review
Ab stract
Artificial neural networks are built on the basic operation of linear combination
and non-linear activation function. Theoretically this structure can approximate
any continuous function with three layer architecture. But in practice learning the
parameters of such network can be hard. Also the choice of activation function
can greatly impact the performance of the network. In this paper we are proposing
to replace the basic linear combination operation with non-linear operations that
do away with the need of additional non-linear activation function. To this end
we are proposing the use of elementary morphological operations (dilation and
erosion) as the basic operation in neurons. We show that these networks (Denoted
as DenMo-Net) with morphological operations can approximate any smooth func-
tion requiring less number of parameters than what is necessary for normal neural
networks. The results show that our network perform favorably when compared
with similar structured network.
1	Introduction
In artificial neural networks, the basic building block is an artificial neuron or perceptron that sim-
ply computes the linear combination of the input (Rosenblatt, 1958). It is usually followed by a
non-linear activation function to model the non-linearity of the output. Although the neurons are
simple in nature, when connected together they can approximate any continuous function of the
input (Hornik, 1991). This has been successfully utilized in solving different real world problems
like image classification (Krizhevsky et al., 2012), semantic segmentation (Long et al., 2015) and
image generation (Isola et al., 2017). While these models are quite powerful in nature, their efficient
training can be hard in general (LeCun et al., 2012) and they need support of specials techniques,
such as batch normalization (Ioffe & Szegedy, 2015) and dropout (Srivastava et al., 2014), in or-
der to achieve better generalization capabilities. Their training time also depends on the choice of
activation function (Mishkin et al., 2017).
In this paper we are proposing new building blocks for building networks similar to neural network.
Here, instead of the linear combination operation of the artificial neurons, we use a non-linear op-
eration that eliminates the need of additional activation function while requiring a small number of
neurons to attain same performance or better. More specifically, We use morphological operations
(i.e. dilation and erosion) as the elementary operation of the neurons in the network. Our contribu-
tion in this paper is building a network with these operations that has the following properties.
1.	Networks built with with dilation-erosion neurons followed by linear combination can ap-
proximate any continuous function given enough dilation/erosion neurons.
2.	As dilation and erosion operation are non-linear by themselves, requirement of separate
non-linear activation function is eliminated.
3.	The use of dilation-erosion operation greatly increases number of possible decision bound-
aries. As a result, complex decision boundaries can be learned using small number of
parameters.
The rest of the paper is organized as follows. Section 2 describes the prior work on morphological
neural network. In Section 3, we introduce our proposed network and prove its capabilities theoreti-
cally. We further demonstrate its capabilities empirically on a few benchmark datasets in Section 4.
Lastly Section 6 concludes the paper.
1
Under review as a conference paper at ICLR 2019
2	Related Work
Morphological neuron was first introduced by Davidson & Hummer (1993) in their effort to learn
the structuring element of dilation operation in images. Similar effort has been made to learn the
structuring elements in a more recent work by Masci et al. (2013). Use of morphological neurons in
a more general setting was first proposed by Ritter & Sussner (1996). They restricted the network to
a single layer architecture and focused only on binary classification task. To classify the data, these
networks use two axis parallel hyperplanes as the decision boundary. This single layer architecture
of Ritter & Sussner (1996) has been extended to two layer architecture by Sussner (1998). This two
layer architecture is able to learn multiple axis parallel hyperplanes, and therefore is able to solve
arbitrary binary classification task. But, in general the decision boundaries may not be axis parallel,
as a result this two layer network may need to learn a large number of hyperplanes to achieve good
results. So, one natural extension is to incorporate the option to rotate the hyperplanes. Taking a
cue from this idea, Barmpoutis & Ritter (2006) proposed to learn a rotational matrix that rotates
the input before trying to classify the data using axis parallel hyperplanes. In a separate work by
Ritter et al. (2014) the use of L1 and L∞ norm has been proposed as a replacement of the max/min
operation of dilation and erosion in order to smooth the decision boundaries.
Ritter & Urcid (2003) first introduced the dendritic structure of biological neurons to the morpholog-
ical neurons. This new structure creates hyperbox based decision boundaries instead of hyperplanes.
The authors have proved that with hyperboxes any compact region can be estimated, therefore any
two class classification problems can be solved. A generalization of this structure to the multiclass
case has also been done by (Ritter & Urcid, 2007). Sussner & Esmi (2011) had proposed a new
type of structure called morphological perceptrons with competitive neurons, where the output is
computed in winner-take-all strategy. This is modelled using the argmax operator and this allows
the network to learn more complex decision boundaries. Later Sossa & Guevara (2014) proposed a
new training strategy to train this model with competitive neurons.
The non-differentiability of the max-min operations has forced the researchers to propose special-
ized training procedures for their models. So, a separate line of research has attempted to modify
these networks so that gradient descent based optimizer can be used for training. Pessoa & Maragos
(2000) have combined the classical perceptron with the morphological perceptron. The output of
each node is taken as the convex combination of the classical and the morphological perceptron. Al-
though max/min operation is not differentiable, they have proposed methodology to circumvent this
problem. They have shown that this network can perform complex classification tasks. Morpholog-
ical neurons have also been employed for regression task. de A. Arajo (2012) has utilized network
architecture similar to morphological perceptrons with competitive learning to forecast stock mar-
kets. The argmax operator is replaced with a linear function so that the network is able to regress
forecasts. The use of linear activation function enables the use of gradient descent for training
which is not possible with the argmax operator. For morphological neurons with dendritic structure
Zamora & Sossa (2017) had proposed to replace the argmax operator with a softmax function. This
overcomes the problem of gradient computation and therefore gradient descent is employed to train
the network. So, this retains the hyperbox based boundaries of the dendritic networks, but facilitates
easy training with gradient descent.
3	Dense Morphological Network
In this section we introduce the basic components and structure of our network and establish its
approximation power.
3.1	Dilation and Erosion neurons
Dilation and Erosion are two basic operations of our proposed network. Given an input x ∈ Rd and
some structuring element S ∈ Rd+1, dilation (㊉)and erosion (㊀)neurons computes the following
two functions respectively
X ㊉ s = max(xk + Sk),	(1)
k
x ㊀ s = min(xk — Sk).	(2)
k
2
Under review as a conference paper at ICLR 2019
∂L
∂ S
x->
皿〜㊉
∂z+ ∂ S
z
∂L
∂z+
Figure 1: Forward and backward pass through a node computing dilation
Where x0 = [x, 0] and x0k denotes the kth component of vector x0. The 0 is appended to the input
x to take care of the ‘bias’. Here we try to learn the structuring element (s). Note that erosion
operation can also be written in the following form.
X ㊀ S = — max(sk — Xk)
(3)
3.2	Gradient of Dilation and Erosion
Artificial neural networks are trained using back-propagation. To be able to use the dilation/erosion
neurons as a drop-in replacement of the artificial neurons, we must be able to compute the gradient
of this operation. Here we show the gradient of the dilation operation. For erosion the gradient
computation will be similar. Figure 1 shows the computational graph model of dilation operation
and its gradient flow. Lets assume L is the loss, We are trying to optimize. We need to compute ∂∂S
to be able to update the structuring element. This can be computed using the chain rule as follows.
∂L ∂L ∂z+
∂s	∂z+ ∂s
(4)
Here ∂L will be the input gradient to this node which will be available from the node following
this node. We need to compute ∂∂+. Now as S ∈ Rd+1 we can write the following.
∂z+	∂z+ ∂z+	∂z+ T
------— ---------.∙ ∙ ∙.----
∂S	[ ∂s1,∂s2 ,	,∂Sd+ι ]
∂+
Now for each dZ- the gradient is computed as follows,
∂ si
∂z+	11 if x ㊉ s = Xi + Si,
∂si [0 otherwise.
(5)
(6)
3.3	Network Structure
The Dense Morphological Net or ‘DenMo-Net’, in short, that we propose here is a simple feed
forward network with some dilation and erosion neurons followed by classical artificial neurons
(Figure 2). We call the layer of dilation and erosion neurons as the dilation-erosion layer and the
following layer as the linear combination layer. Let’s assume the dilation-erosion layer contains
n dilation neurons and m erosion neurons, followed by c neurons in the linear combination layer.
Let x ∈ Rd is the input to the network. Let zi+ and zj- be the output of ith dilation neuron and j th
erosion node, respectively. Then we can write,
z+	= X ㊉ s+,	(7)
z-	=	X ㊀ S-	(8)
where, Si+	and	Sj-	are	the	structuring elements of the ith	dilation neuron	and jth	erosion	neuron
respectively.	Note that	i	∈	{1, 2,	.	.	.	,	n} and j ∈ {1, 2, . .	. , m}. The final output from a node of the
linear combination layer is computed in the following way.
nm
g(X) = Xzi+ωi+ +Xzj-ωj-	(9)
i=1	j=1
where ωi+ and ωj- are the weights of the artificial neuron in the linear combination layer. In follow-
ing subsection we show that g(X) can approximate any continuous function f : Rd → R.
3
Under review as a conference paper at ICLR 2019
Figure 2: Single Layer DenMo-Net with n dilation and m erosion neuron and c output neurons
3.4 Function Approximation
Here we show that with the linear combination of dilation and erosion, any function can be ap-
proximated, and the approximation error decreases with increase in the number of neurons in the
dilation-erosion layer. Before that we need to describe some concepts.
Definition 1 (k-order Hinge Function (Wang & Sun, 2005)) A k-order hinge function consists of
(k + 1) hyperplanes continuously joined together. it is defined by the following equation,
h(k) (x) = ± max{w1Tx + b1 , w2Tx + b2, . . . , wkT+1x + bk+1}.	(10)
Definition 2 (d-order hinging hyperplanes (d-HH) (Wang & Sun, 2005)) A d-order hinging hy-
perplanes (d-HH) is defined as the sum of multi-order hinge function as follows,
Xαih(ki)(x)	(11)
i
with αi ∈ {-1, 1}, ki ≤ d.
From Wang & Sun (2005) the following can be said about hinging hyperplanes.
Proposition 1 For any given positive integer d and arbitrary continuous piece-wise linear function
f : Rd → R, there exists finite, say N, positive integers η(k) ≤ d+1, 1 ≤ k < N and corresponding
αi ∈ {-1, 1} such that
N
f(x) = X αih(η(k))(x),	∀x ∈ Rd.	(12)
k=1
This says that any continuous piece-wise linear function of d variables can be written as an d-HH,
i.e. the sum of multi-order hinge functions. Now to show that our network can approximate any
continuous functions, we show the following.
Lemma 1 g(x) is sum of multi-order hinge functions.
The proof of this lemma is given in Appendix A. Basically we show that g(x) can written as the
sum of l hinge functions in the following form.
l
g(x) = X αiφi(x)	(13)
i=1
4
Under review as a conference paper at ICLR 2019
where l = m + n (number of neurons in the dilation-erosion layer), a ∈ {1, -1} and φi (Xysare
d-order hinge function.
Proposition 2 (Stone-Weierstrass approximation theorem) Let C be a compact domain (C ⊂
Rd) and f : C → R a continuous function. Then there exists a continuous piece wise linear
function g such that for all x ∈ C, |f (x) - g(x)| < for some > 0.
Theorem 1 (Universal approximation) Only a single dilation-erosion layer followed by a linear
combination layer can approximate any continuous smooth function provided there are enough
nodes in dilation erosion-layer.
Sketch of Proof From lemma 1 we know that our DenMo-Net with of n dilation and m erosion
neurons followed by a linear combination layer computes g(x), which is a sum of multi-order hinge
functions. Now from proposition 1 we get that any continuous piecewise linear function can be
written by a finite sum of multi-order hinge function. Now from Proposition 2 we can say that any
continuous function can be well approximated by a piecewise linear function. In general if l → ∞
then → 0. If we increase the number of neurons in the dilation-erosion layer the approximation
error decreases. Therefore, we can say that a DenMo-Net with enough dilation and erosion neurons
can approximate any continuous function.
3.5 Learned Decision B oundary
The DenMo-Net we have defined above learns the following function,
l
g(x) = Xαiφi(x).	(14)
i=1
Where each φ(x) is collection of multiple hyperplanes joined together. Therefore the number of
hyperplanes learned by the network with l neurons in the dilation-erosion layer is much more than l.
Each morphological neuron allows only one of the inputs to pass through because ofmax/ min op-
eration after addition with the structuring element. So, effectively each neuron in the dilation-erosion
layer chooses one component of the d-dimensional input vector. Depending on which component
is being chosen, the final linear combination layer computes the hyperplane by taking either all the
components of the input or only some of them (when a subset of input components is chosen more
than once in the dilation-erosion layer). Note that this choice depends on the input and the struc-
turing element together. For a network with d dimensional input data and l neurons (l ≥ d) in the
dilation-erosion layer, theoretically (d + 1)l - 1 hyperplanes can be formed in d dimension. Out of
the all possible planes only lPd × (d + 1)l-d planes can span anywhere in the d dimensional space.
Therefore, increasing the number of neurons in the dilation-erosion layer exponentially increases
the possible number of hyperplanes, i.e., the decision boundaries. This implies that, using only a
small number of neurons, complex decision boundaries can be learned.
4 Results
Here we empirically validate the power of our DenMo-Net and demonstrate its advantages in com-
parison with other networks like artificial neural networks with different activation functions i.e.
tanh (NN-tanh) and ReLU (NN-ReLU) and Maxout network (Goodfellow et al., 2013). As our
network is defined with all possible connections between two consecutive layers, we have compared
with only similar structured networks. We have chosen the maxout network for comparison, be-
cause it uses the max function as a replacement of the activation function but with added nodes to
compute the maximum. The experiments have been carried out on a toy dataset with two concentric
circles for visualizing the decision boundaries and also on benchmark datasets like MNIST (LeCun
et al., 1998), Fashion-MNIST (Xiao et al., 2017), CIFAR-10 and CIFAR-100 (Krizhevsky & Hin-
ton, 2009). For all the tasks we have used categorical cross entropy as the loss and in the last layer
softmax function is used. In the training phase, all the networks have been optimized using Adam
(learning rate= 0.001, β1 = 0.9, β2 = 0.999) optimizer (Kingma & Ba, 2014) with mini batches
of size 32. In all the experiments, we have used same number of dilation and erosion neurons in
dilation-erosion layer unless otherwise stated.
5
Under review as a conference paper at ICLR 2019
I Legend:	Class 1 Class 2 ∣
(a) NN-ReLU
(b) Maxout Network
Figure 3: Decision boundaries of different networks
(c) DenMo-Net
Table 1: Training accuracy achieved on the circle dataset by different networks
Methods	Hidden nodes	Parameters	Training accuracy
NN-ReLU	2	12	68.87
NN-tanh	2	12	69.10
Maxout Network (h=2)	2	18	87.17
DenMo-Net	2	12	91.6
4.1	Visualization with a toy dataset
For visualizing the decision boundaries learned by the classifiers, we have generated data on two
concentric circles belonging to two different classes with center at the origin. We compare the
results when only two neurons are taken in the hidden layer in all the networks. It is observed that
classical neural network fails to classify this data with two hidden neurons as it learns one hyperplane
per one hidden neuron. The boundaries learned by the network with ReLU activation function (NN-
ReLU) is shown in figure 3a. The result of maxout network is better (87.17% training accuracy)
as it introduces extra parameters with max function to achieve non-linearity. In the maxout layer
we have taken maximum among h = 2 features. As we see in the figure 3b the network learns
(2 * h =)4 straight lines when trying to classify these data. For the same data and two neurons in
dilation-erosion layer, our DenMo-Net has learned 6 lines to form the decision boundary (figure 3c).
Although from equation 14 we can say that we get at most 8 lines, only two of them can be placed
anywhere in the 2D space while others are parallel to the axes. For this reason, we are getting two
slanted lines and the remaining lines are parallel to the axes. The classification accuracy achieved
by the networks along with their number of parameters is reported in table 1. The difference in the
accuracy clearly shows the power of DenMo-Net.
4.2	MNIST DATASET
MNIST dataset (LeCun et al., 1998) contains gray scale images of hand written numbers (0-9) of size
28 × 28. It has 60,000 training images and 10,000 test images. Since our network does not support
two dimensional input, we have converted each image to a column vector (in row major order) before
giving it as input. The network we use follows the structure we have previously defined: input layer,
dilation-erosion layer and linear combination layer computing the output. As in this dataset we had
to distinguish between 10 classes of images, 10 neurons are taken in the output layer. In table 2 we
have shown the accuracy on test data after training the network for 150 epochs with different number
of nodes (l) in the dilation-erosion layer. The change of test accuracy over the epochs is shown in
figure 4a. It is seen that increasing number of nodes in the dilation-erosion layer helps to increase
non-linearity, and thus it results in better accuracy on test data. We get test average accuracy of
98.43% after training 3 times with the DenMo-Net of 200 dilation and 200 erosion neurons (Table
3) up to 400 epochs. We have also experimented when only dilation, only erosion and both type of
neurons were used in the dilation-erosion layer (Figure 4b). We see that using both erosion only and
both dilation and erosion neurons giving better accuracy.
6
Under review as a conference paper at ICLR 2019
Figure 4: Test accuracy achieved over epochs in the MNIST dataset
Table 2: Accuracy on MNIST dataset with different architectures
Neurons in dilation-erosion layer (l)
Test Accuracy
10	50	100	200
76.35	93.38	95.51	96.85
4.3	Fashion-MNIST Dataset
The Fashion-MNIST dataset (Xiao et al., 2017) has been proposed with the aim of replacing the
popular MNIST dataset. Similar to the MNIST dataset this also contains 28 × 28 images of 10
classes and 60,000 training and 10,000 testing samples. While MNIST is still a popular choice
for benchmarking classifiers, the authors’ claim that MNIST is too easy and does not represent the
modern computer vision tasks. This dataset aims to provide the accessibility of the MNIST dataset
while posing a more challenging classification task.
For the experiment, we have converted the images to a column vector similar to what we have done
for the MNIST dataset. We have taken 400 dilation and 400 erosion nodes in the dilation-erosion
layer for this experiment. We have trained the network separately 3 times up to 300 epochs. The
reported test accuracy (Table 3) is the average of the 3 runs. We see that our method gives better
results.
4.4	CIFAR- 1 0 DATASET
CIFAR-10 (Krizhevsky & Hinton, 2009) is natural image dataset with 10 classes. It has 50,000
training and 10,000 test images. Each of them is a color image of size 32 × 32. The images are
converted to column vector before they are fed to the DenMo-Net. For all the networks we compare
with, the experiments have been conducted with keeping the number of neurons same in the hidden
layer. For maxout network each hidden neuron have two extra nodes over which the maximum is
computed. In table 4 we have reported the average test accuracy obtained over three run of 150
epochs. The change of accuracy over epochs is also shown in figure 5a when number of hidden
neurons is 600. As it can be seen from both the table and the figure that DenMo-Net achieves the best
accuracy in all the cases. Maxout network lags behind even with more number of parameters. This
Table 3: Achieved accuracy in the test set
Dataset	Test accuracy	
	DenMo-Net	State of the art
MNIST	98.43 (l = 400)	99.79 (Wan et al., 2013)
Fashion-MNIST	89.87 (l = 800)	89.70 (Xiao et al., 2017)
7
Under review as a conference paper at ICLR 2019
nodes)
Figure 5: Test accuracy over epochs on CIFAR-10 dataset
Table 4: Test accuracy achieved on CIFAR-10 dataset by different networks
Architecture		l=200				l=400				l=600		
	parameters	accuracy	parameters accuracy		parameters a	ccuracy
NN-tanh	616,610	^^48.88	1,233,210	49.39	1,849,810	51.24
NN-ReLU	616,610	49.28	1,233,210	50.43	1,849,810	52.25
Maxout-Network	1,231,210	49.51	2,462,410	50.10	3,693,610	51.51
DenMo-Net	616,610	51.84	1,233,210	53.41	1,849,810	54.49
happens because our network is able to learn more hyperplanes with number of parameters similar
to normal artificial neural networks. When using only a single type of neurons in our network, we
see a different result for this dataset (Figure 5b). The network takes time to learn with only erosion
neurons. The situation improves a little when using only dilation neurons. When using both type of
morphological neurons, the network is able to perform better by leveraging the power of both the
operations.
4.5	CIFAR- 1 00 DATASET
CIFAR-100 (Krizhevsky & Hinton, 2009) is a image dataset similar to CIFAR-10 but with 100
classes with 600 images in each. There are 500 training and 100 testing images for each class. The
training has been done similar to what is done for CIFAR-10. Network has been trained with batch
size 100. We have reported the average test accuracy of 3 run with 100 epochs each in table 5. The
change of test accuracy over the epochs is plotted in figure 6a. The results show trend similar to
what is observed in other dataset. DenMo-Net is giving better result with comparable number of
trainable parameters and trains much faster. When the type of neurons is restricted in our network,
the results are similar to what we have seen for CIFAR-10 dataset (Figure 6b). Using only erosion
neurons, the networks lags behind in terms of accuracy. But interestingly, using erosion only the
network is able to perform better than using both type of morphological neurons.
5 Discussions
5.1	Gradient Propagation
In our network we are learning the structuring element and the weights of the linear combination
layer using gradient descent method. While learning the structuring elements we may encounter two
kinds of problem. Dilation/erosion operation involves max/min operation. As shown in Section 3.2,
the gradient of the loss with respect to the structuring element contains a single non-zero element.
8
Under review as a conference paper at ICLR 2019
5 0 5 0
2 2 11
Cicicici
>02⊃00<
O 20	40	60	80 IOO
Epoch
(a) On different networks (100 epochs, 500 hidden
nodes)
0.00 ɑ
05
0.
5 0 5 0
2 2 11
20	40	60	80 IOO
Epoch
(b) Using only dilation, only erosion and both
Figure 6: Test accuracy over epochs on CIFAR-100 dataset
Table 5: Comparison with Baseline CIFAR100
Architecture	l=200		l=400			l=600		
	parameters accuracy		parameters accuracy		parameters a	ccuracy
NN-tanh	634,700	19.50	1,269,300	19.62	1,903,900	20.46
NN-ReLU	634,700	17.83	1,269,300	19.63	1,903,900	20.77
Maxout-Network	1,249,300	21.58	2,498,500	21.49	3,747,700	21.69
DenMo-Net	634,700	23.65	1,269,300	25.89	1,903,900	26.93
Therefore, only a single element of the structuring element is updated. So, the learning can be slow.
On the other hand, some values of the structuring element may not get updated at all.
5.2 Stacking Multiple Layers
We have defined the network and have shown its properties when only three layers are employed
in the network. Although it may seem stacking multiple of these layers would translate to better
results, the results are showing the opposite. Straight-forward stacking of the layers we have used in
our network can give rise to two kinds of network.
Type-I Multiple dilation-erosion layer, followed by a single linear combination layer at the end.
Type-II Dilation-Erosion layer followed by a linear combination layer. Then another dilation-
erosion layer followed by one more linear combination layer and so on.
For the network of Type-I, it can be argued that the network is performing some combination of
opening and closing operation, and their linear combination. As there are dilation-erosion (DE)
layers back to back, the problem of gradient propagation is amplified. As a result it takes much
more time to train than single layer architecture (Table 6).
Similar explanation doesn’t work for Type-II networks. From Figure 7 we see that the network has
tendency to overfit. We believe its understanding requires further exploration.
6 Conclusion
In this paper we have proposed a new class of networks that uses both normal and morphological
neurons. These network consists of three layers only: input layer, dilation-erosion layer with dilation
and erosion neurons followed by linear combination layer giving the output of the network with nor-
mal artificial neurons. We have done our analysis using this three layer network only, but its deeper
version can also be explored. We have shown that this three layer architecture can approximate any
9
Under review as a conference paper at ICLR 2019
Table 6: Test accuracy achieved on CIFAR-10 and CIFAR-100 dataset by different networks
Network
NN-tanh (400D-200D-100D)
NN-ReLU (400D-200D-100D)
DenMo-Net (400DE-100FC)
DenMo-Net (400DE-200DE-100FC)
DenMo-Net (400DE-200FC-100DE-100FC)
Accuracy
CIFAR-10 CIFAR-100
51.6
52.79
53.41
51.2
39.2
24.37
24.37
25.89
23.31
16.32
Figure 7: Train and Test accuracy achieved over epochs in the CIFAR-10 dataset
sufficiently smooth function without requiring any non-linear activation function. These networks
are able to learn a large number of hyperplanes with very few neurons in the dilation-erosion layer
thereby providing superior results compared to other networks with three layer architecture. The im-
proved results could also be the result of ‘feature selection’ by the max/min operator in the dilation
erosion layer. In this work we have only worked with fully connected layers, i.e. a node in a layer
is connected to all the nodes in the previous layer. This type of connectivity is not very efficient for
image data where architectures with convolution layers perform better. So, extending this work to
the case where a structuring element operates by sliding over the whole image, should be the next
logical step.
References
A. Barmpoutis and G. X. Ritter. Orthonormal Basis Lattice Neural Networks. In 2006 IEEE In-
ternational Conference on Fuzzy Systems, pp. 331-336, July 2006. doi: 10.1109/FUZZY.2006.
1681733.
Jennifer L. Davidson and Frank Hummer. Morphology neural networks: An introduction with
applications. Circuits, Systems and Signal Processing, 12(2):177-210, June 1993. ISSN 1531-
5878. doi: 10.1007/BF01189873. URL https://doi.org/10.1007/BF01189873.
Ricardo de A. Arajo. A morphological perceptron with gradient-based learning for Brazilian stock
market forecasting. Neural Networks, 28:61-81, April 2012. ISSN 0893-6080. doi: 10.1016/
j.neunet.2011.12.004. URL http://www.sciencedirect.com/science/article/
pii/S0893608011003200.
Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
Networks. In Proceedings of the 30th International Conference on International Conference
on Machine Learning - Volume 28, ICML’13, pp. III-1319-III-1327, Atlanta, GA, USA, 2013.
JMLR.org. URL http://dl.acm.org/citation.cfm?id=3042817.3043084.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Net-
works, 4(2):251-257, January 1991. ISSN 0893-6080. doi: 10.1016/0893-6080(91)
10
Under review as a conference paper at ICLR 2019
90009-T. URL http://www.sciencedirect.com/science/article/pii/
089360809190009T.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training
by Reducing Internal Covariate Shift. In International Conference on Machine Learning, pp.
448-456, June 2015. URL http://Proceedings.mlr.press∕v37∕ioffe15.html.
P. Isola, J. Zhu, T. Zhou, and A. A. Efros. Image-to-Image Translation with Conditional Adversarial
Networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
5967-5976, July 2017. doi: 10.1109/CVPR.2017.632.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980
[cs], December 2014. URL http://arxiv.org/abs/1412.6980. arXiv: 1412.6980.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, University of Toronto, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with
Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp.
1097-1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.
pdf.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann A. LeCun, Lon Bottou, Genevieve B. Orr, and Klaus-Robert Mller. Efficient BackProp. In
Grgoire Montavon, Genevive B. Orr, and Klaus-Robert Mller (eds.), Neural Networks: Tricks of
the Trade: Second Edition, Lecture Notes in Computer Science, pp. 9-48. Springer Berlin Heidel-
berg, Berlin, Heidelberg, 2012. ISBN 978-3-642-35289-8. doi: 10.1007/978-3-642-35289-8_3.
URL https://doi.org/10.1007/978-3-642-35289-8_3.
J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation.
In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3431-3440,
June 2015. doi: 10.1109/CVPR.2015.7298965.
Jonathan Masci, JesUs Angulo, and JUrgen Schmidhuber. A learning framework for morphological
operators using counter-harmonic mean. In International Symposium on Mathematical Morphol-
ogy and Its Applications to Signal and Image Processing, pp. 329-340. Springer, 2013.
Dmytro Mishkin, Nikolay Sergievskiy, and Jiri Matas. Systematic evaluation of convolution neu-
ral network advances on the Imagenet. Computer Vision and Image Understanding, 161:11-
19, August 2017. ISSN 1077-3142. doi: 10.1016/j.cviu.2017.05.007. URL http://www.
sciencedirect.com/science/article/pii/S1077314217300814.
Lcio F. C. Pessoa and Petros Maragos. Neural networks with hybrid morphologi-
cal/rank/linear nodes: a unifying framework with applications to handwritten character recog-
nition. Pattern Recognition, 33(6):945-960, June 2000. ISSN 0031-3203. doi: 10.
1016/S0031-3203(99)00157-0. URL http://www.sciencedirect.com/science/
article/pii/S0031320399001570.
G. X. Ritter and P. Sussner. An introduction to morphological neural networks. In Proceedings
of 13th International Conference on Pattern Recognition, volume 4, pp. 709-717 vol.4, August
1996. doi: 10.1109/ICPR.1996.547657.
G. X. Ritter and G. Urcid. Lattice algebra approach to single-neuron computation. IEEE Transac-
tions on Neural Networks, 14(2):282-295, March 2003. ISSN 1045-9227. doi: 10.1109/TNN.
2003.809427.
G. X. Ritter, G. Urcid, and V. Juan-Carlos. Two lattice metrics dendritic computing for pattern
recognition. In 2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), pp. 45-52,
July 2014. doi: 10.1109/FUZZ-IEEE.2014.6891551.
11
Under review as a conference paper at ICLR 2019
Gerhard X. Ritter and Gonzalo Urcid. Learning in Lattice Neural Networks that Employ Dendritic
Computing. In Vassilis G. Kaburlasos and Gerhard X. Ritter (eds.), Computational Intelligence
Based on Lattice Theory, Studies in Computational Intelligence, pp. 25-44. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 2007. ISBN 978-3-540-72687-6. doi:10.1007/978-3-540-72687-6_
2. URL https://doi.org/10.1007/978-3-540-72687-6_2.
F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization
in the brain. Psychological Review, 65(6):386-408, 1958. ISSN 1939-1471(Electronic),0033-
295X(Print). doi: 10.1037/h0042519.
Humberto Sossa and Elizabeth Guevara. Efficient training for dendrite morphological neural
networks. Neurocomputing, 131:132-142, May 2014. ISSN 0925-2312. doi: 10.1016/j.
neucom.2013.10.031. URL http://www.sciencedirect.com/science/article/
pii/S0925231213010916.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
P. Sussner. Morphological perceptron learning. In Proceedings of the 1998 IEEE International Sym-
posium on Intelligent Control (ISIC) held jointly with IEEE International Symposium on Compu-
tational Intelligence in Robotics and Automation (CIRA) Intell, pp. 477-482, September 1998.
doi: 10.1109/ISIC.1998.713708.
Peter Sussner and Estevo Laureano Esmi. Morphological perceptrons with competitive learning:
Lattice-theoretical framework and constructive learning algorithm. Information Sciences, 181
(10):1929-1950, May 2011. ISSN 0020-0255. doi: 10.1016/j.ins.2010.03.016. URL http:
//www.sciencedirect.com/science/article/pii/S0020025510001283.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International Conference on Machine Learning, pp. 1058-1066,
2013.
Shuning Wang. General constructive representations for continuous piecewise-linear functions.
IEEE Transactions on Circuits and Systems I: Regular Papers, 51(9):1889-1896, 2004.
Shuning Wang and Xusheng Sun. Generalization of hinging hyperplanes. IEEE Transactions on
Information Theory, 51(12):4425-4431, 2005.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Erik Zamora and Humberto Sossa. Dendrite morphological neurons trained by stochastic gradi-
ent descent. Neurocomputing, 260:420-431, October 2017. ISSN 0925-2312. doi: 10.1016/
j.neucom.2017.04.044. URL http://www.sciencedirect.com/science/article/
pii/S0925231217307956.
12
Under review as a conference paper at ICLR 2019
Appendix A Proof of Lemma 1
From equation 9 we have
nm
g(x) =	ωi+zi+ +	ωj-zj-.
i=1	j=1
Now this equation can be rewritten as follows
(15)
nm
g(x) =	ωi+ max(xk + si+k) +	-ωi- max(si-k - xk),
kk
i=1	i=1
(16)
where si+k and si-k denote the kth component of the ith structuring element of dilation and erosion
neurons, respectively. The above equation can be further expressed in the following form,
nm
g(x) =	αi+ mkax(βi+xk + di+k) +	αi- mkax(βi-xk + di-k).	(17)
Where βi+, βi-, di+k and di-k are define in the following way
+ = ωi+	if ωi+ ≥0
i = -ωi+ if ωi+ <0
=	si+kωi+	if ωi+ ≥0
-si+kωi+	if ωi+ <0
+	1	if ωi+ ≥ 0
αi+ =	i+
i	-1	if ωi+ < 0
if ωi- ≥ 0
if ωi- < 0
if ωi- ≥ 0
if ωi- < 0
if ωi- ≥ 0
if ωi- < 0
Now, without any loss of generality we can write equation 17 as follows
m+n
g (x) =	αi max(βi xk + dik )
k
i=1
(18)
where
αi
if i ≤ n
if n < i ≤ m + n
if i ≤ n
if n < i ≤ m + n
dik=	ddi+-k
d(i-n)k
if i ≤ n
if n < i ≤ m + n
Finally, we can rewrite equation 18 as
l
g(x) =	αiφi(x),
i=1
where l = m + n, ai ∈ {1, -1} and φi(x)'s are of the following form
φi(x) = max(viTkx + dik),
k
with
βi if t = k
vikt = 0	ift 6= k
(19)
(20)
(21)
In equation 20, viTkx + dik is affine and αiφi(x) is a d-order hinge function. Hence Pli=1 αiφi(x)
i.e., g(x) represents sum of multi-oder hinge function.
13
Under review as a conference paper at ICLR 2019
However, it may be noted that taking l ≥ d results hinge hyper planes which can span any where in
d dimensional input space. We can assume there are l1 and l2 number of terms where α = 1 and
α = -1 respectively, then
l1	l2
g(x) =	φi(x)- φi (x),
i=1	i=1
where l1 + l2 = l and φ0i(x), φ0i0 (x) is of same form as equation 20. Threfore can write,
l1	l1
Σ φ0i (x) = Σ max(viTkx + dik),
i=1	i=1 k
l1	l1	l1
φi(x) =	max ((	vik1)T x +	diki)
k1,k2,k3,..,kl1
i=1	1 i=1	i=1
(22)
(23)
(24)
where ki ∈ {1, 2, .., d + 1}∀i. In equation 24 we are taking maximum of (d + 1)l1 terms. Similarly
we can derive same expression for Pli2=1 φ0i0 (x) . Hence out of all l number of sum we can select
d number of coefficient of x. So, l ≥ d results in hinge hyperplanes which can span any where in
d-dimensional space.
Appendix B Alternative theoretical justification
Here give an alternate proof of the claim that g(x) can approximate any continuous function. For
that take the help of following two proposition.
Proposition 3 (From Wang (2004)) Any continuous piece-wise linear function (PWL) can be ex-
pressed by as difference of two convex PWL function.
Then we show the following,
Lemma 2 g(x) is a continuous piece-wise linear function.
Proof From equation 19, without any loss of generality we can assume there are t1 and t2 number
of terms where α = 1 and α = -1 respectively, then
t1	t2
g(x) = Xφi(x) -Xφi (x),	(25)
i=1	i=1
where t1 + t2 = l and φ0i(x), φ0i0 (x) are of same form as equation 20.
As sum of PWL functions is also a PWL function, hence each Pit=1 1 φ0i (x) and Pit=2 1 φ0i0 (x) and
PWL. Now, if t1 > 0, from Proposition 3 we can conclude that g(x) is PWL linear function since
difference of two continuous PWL function is PWL function . If t1 = 0 then g(x) becomes PWL
concave function.Hence, can say g(x) is PWL function.
It may be noted that if l < d then PWL hyperplane will be in parallel to at least one of the axis.
Taking l ≥ d results PWL hyperplane which may span anywhere in d dimensional space.
Theorem 2 (Universal approximation) Using only a single dilation-erosion layer followed by a
linear combination layer any continuous function can be approximated.
Sketch of Proof From proposition 2 we get that any continuous function can be well approximated
by a PWL function with an error bound of . Now from lemma 2 we know that our DenMo-Net
with of n dilation and m erosion neurons followed by a linear combination layer computes a PWL
function. Hence we can say our network can approximate any continuous function. In general if we
increase the neurons in the dilation-erosion layer, number of affine function in g(x) (equation 18)
increases and the error bound → 0 as the number of nodes in dilation-erosion layer increases.
14