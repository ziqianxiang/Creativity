Under review as a conference paper at ICLR 2019
BIGSAGE: unsupervised inductive representa-
tion learning of graph via bi-attended sam-
pling and global-biased aggregating
Anonymous authors
Paper under double-blind review
Ab stract
Different kinds of representation learning techniques on graph have shown sig-
nificant effect in downstream machine learning tasks. Recently, in order to in-
ductively learn representations for graph structures that is unobservable during
training, a general framework with sampling and aggregating (GraphSAGE) was
proposed by Hamilton and Ying and had been proved more efficient than trans-
ductive methods on fileds like transfer learning or evolving dataset. However,
GraphSAGE is uncapable of selective neighbor sampling and lack of memory of
known nodes that’ve been trained. To address these problems, we present an unsu-
pervised method that samples neighborhood information attended by co-occurring
structures and optimizes a trainable global bias as a representation expectation for
each node in the given graph. Experiments show that our approach outperforms
the state-of-the-art inductive and unsupervised methods for representation learn-
ing on graphs.
1	Introduction
Graphs and networks, e.g., social network analysis Hamilton et al. (2017a), molecule screening
Duvenaud et al. (2015), knowledge base reasoning Trivedi et al. (2017), and biological protein-
protein networks (Zitnik & Leskovec (2017)), emerge in many real-world applications. Learning
low-dimensional vector embeddings of nodes in large graphs has been proved effective for a wide
variety of prediction and graph analysis tasks (Grover & Leskovec (2016); Tang et al. (2015)). The
high-level idea of node embedding is to explore high-dimensional information about the neighbor-
hood of a node with a dense vector embedding, which can be fed to off-the-shelf machine learning
approaches to tasks such as node classification and link prediction (Perozzi et al. (2014)).
Whereas previous approaches (Perozzi et al. (2014); Grover & Leskovec (2016); Tang et al. (2015))
can transductively learn embeddings on graphs, without re-training they cannot generalize to new
nodes that are newly added to graphs. It is ubiquitous in real-world evolving networks, e.g., new
users joining in a social friendship circle such as facebook. To address the problem, Hamilton et al.
(2017b) propose an approach, namely GraphSAGE, to leverage node feature information (e.g., text
attributes) to efficiently generate node embeddings for previously unseen nodes. Despite the success
of GraphSAGE, it randomly and uniformly samples neighbors of nodes, which suggests it is difficult
to explore the most useful neighbor nodes. It could be helpful if we can take advantage of the most
relevant neighbors and ignore irrelevant neighbors of the target node. Besides, GraphSAGE only
focuses on training parameters of the hierarchical aggregator functions, but lose sight of preserving
the memory of the training nodes, which means when training is finished, those nodes that have been
trained over and over again would still be treated like unseen nodes, which causes a huge waste.
To address the first issue, inspired by GAT (Velickovic et al. (2017)), a supervised approach that
assigns different weights to all neighbors of each node in each aggregating layer, we introduce a
bi-attention architecture (Seo et al. (2016)) to perform selective neighbor sampling in unsupervised
learning scenarios. In unsupervised representation learning, when encoding embeddings of a pos-
itive1 node pair before calculating their proximity loss (Hamilton et al. (2017a)), we assume that
1In random-walk based approaches, nodes that tend to cooccur in short random walks over the graph are
usually referred as positive to each other.
1
Under review as a conference paper at ICLR 2019
Figure 1: Visual illustration of the co-occurrence bi-attented sampling of nodes’ neighborhood. The
left one figure gives a vivid example of why we can use co-occurrence node to filter out useful
information; and the two figures in the right show how attention from co-occurrence nodes affects
sampling and aggregating over the neighborhood.
neighbor nodes positive to both of the pair should have larger chance to be selected, since they are
statistically more relevant to the current positive pair than other neighbors. For example, when em-
bedding words like ”mouse”, in Figure 1, it’s more reasonable to choose ”keyboard” rather than
”cat” as sampled neighbor while maximizing co-occrrence probability between ”mouse” and ”PC”,
because ”keyboard” also tends to co-occurr with ”PC”, which means its imformation should be more
relevant. We thus stack a bi-attention architecture (Seo et al. (2016)) on representations aggregated
from both side in a positive node pair. In this way, we learn the most relevant representations for
each positive node pair corresponding to their most relevant neighbors, and simply use a fixed-size
uniform sampling which allows us to efficiently generate node embeddings in batches.
To address the second issue, we combine the idea behind transductive approaches and inductive
approaches, by intuitively applying an additive global embedding bias to each node’s aggregated
embedding. The global embedding biases are trainable as well as parameters of aggregator functions
and can be considered as a memorable global identification of each node in training sets. When
the training is completed, we generate the embedding for each node by calculating an average of
multiple embedding outputs corresponding to different sampled neighbors with respect to different
positive nodes. In this way, nodes that tend to co-occur in short random-walks will have more similar
embeddings based on our bi-attention mechanism.
Based on the above-mentioned two techniques, we propose a novel approach, called BIGSAGE
(which stands for the BI-attention architeture, global BIas and the original framework GraphSAGE,)
to explore most relevant neighbors and preserve previously learnt knowledge of nodes by utilizing
bi-attention architecture and introducing global bias, respectively.
2	Related Work
2.1	Network embedding
In this paper, we focus on unsupervised and inductive node embedding learning for large and evolv-
ing network data. For unsupervised learning, many different approachs have been proposed. Deep-
Walk (Perozzi et al. (2014)) and node2vec (Grover & Leskovec (2016))are two classic approaches
that learn node embeddings based on random-walks using or extending the Skip-Gram model. Sim-
ilarly, LINE (Tang et al. (2015))seeks to preserve first- and second-order proximity and trains the
embedding via negative smpling. SDNE(Wang et al. (2016)) jointly uses unsupervised components
to preserve second-order proximity and expolit first-order proximity in its supervised components.
Unlike the methods mentioned above, some approaches were proposed to takes use of not only
network structure but also node attributes and potentially node labels. Such as TRIDNR(Pan
et al. (2016)), CENE(Sun et al. (2016)), TADW(Yang et al. (2015)). GraphSAGE(Hamilton et al.
(2017b)), which this paper is motivated from, also requires rich attributes of nodes for sampling and
aggregating into embeddings that preserve rich local neighborhood strutural infromation.
Recently, in order to address the problem in large and dense networks of consequently encounted
newly jointed nodes/edges, approaches such as Hamilton et al. (2017b) Velickovic et al. (2017)
2
Under review as a conference paper at ICLR 2019
Bojchevski & Gunnemann (2017) were proposed as inductive Ways of graph embedding learning
and had produced impressive performance across several large-scale inductive benchmarks. In fact,
we find that the key of inductive learning is to learn an embedding encoder that relies on only
information from a single node itself and/or its local observable neighborhood, instead of the entire
graph as in transductive setting.
2.2	Attention
Attention mechanism in neural processes have been largely studied in Neuroscience and Compu-
tational Neuroscience (Itti et al. (1998); Desimone & Duncan (1995)) and since these few years
frequently applied in Deep Learning for speech recognition (Chorowski et al. (2015)), transla-
tion(Luong et al. (2015)), question answering (Seo et al. (2016)) and visual identification of ob-
jects Xu et al. (2015) . The principle inside attention mechanism is that focusing on most pertinent
parts of the input, rather than using all available information, a large part of which being irrelevant
to compute the desirable output. In this paper, we are inspired by Seo et al. (2016) to construct a
bi-attention layer upon aggregators in order to capture the useful part of the neighborhood.
3	Model
In this section, we propose a hierarchical bi-attended sampling and global-biased aggregating frame-
work (BIGSAGE). We start by presenting an overview of our framework: the training in Algorithm
1 and the embedding generation in Algorithm 2. Followingly, section 3.2 gives the detailed imple-
mention of our bi-attention architeture, section 3.3 demonstrates how we combine global bias within
the framework. Given an undirected network G = {V, E, X}, in which a set of nodes V are con-
nected by a set of edges E, and X ∈ R* 1 2 3 4 5 6 7 8 9 10 11 Vl×f is the attribute matrix of nodes. We denote the global
embedding bias matrix as B ∈ R1 Vl×d, where each row of B represents the d-dimensional global
embedding bias of each node. The hierarchical layer number is set as K, the embedding output of
k-th layer is represented by hk, and the final output embedding z.
3.1	Overview
To learn representations in unsupervised setting, we apply the same graph-based loss function used
in the origin GraphSAGE:
JG (Zv ) = -log(σ(zT Zvp )) - Q ∙ EVn^Pn(v)lθg(σ(-zT Zvn X
where node vp co-occurs with v on fixed-length random walk (Perozzi et al. (2014)), sigma is the
sigmoid function, Pn is a negative sampling distribution, and Q defines the number of negative
samples.
Algorithm 1 shows the training of our framework.
Algorithm 1 BIGSAGE: training
input: Training graph Gtrain (Vtrain, Etrain); node attributes X; global embedding bias matrix B;
sampling times T
1: zero_initialize(B)
2: hv V — Xv ,∀v ∈ Vtrain
3: run random-walk in Gtrain and do negative sampling to gain a set of triplets: {(v, vp, vn)}
4: for (v, vp , vn ) do
5:	Zi V— 0, Z2 V— 0
6:	for t ∈ {1, ..., T} do
7:	Z1 V Z1 ∪ {SAGB(v)}
8:	Z2 V Z2 ∪ {SAGB(vp)}
9:	Zv , Zvp V BIATT(Z1, Z2 )
10:	Zvn V SAGB(vn)
11:	calculate the graph-based loss JG and update model parameters with SGD
3
Under review as a conference paper at ICLR 2019
When generating embeddings with the learned parameters after optimization is done, GraphSAGE
encode only one single random-partial-sampled neighborhoods for each node, likely leaving out
information of those unselected part.
To fully preseve the structural information around, we first rerun random walk on a full graph that
inlcudes the seen and unseen nodes; Then, through our bi-attention mechanism built upon the ag-
gregating layers, we generate the most relevant embeddings of each node w.r.t its positive nodes;
Eventually, we take average of these generated embeddings as final embeddings of each node and
use them in downstream machine learning tasks. The generation process is shown in Algorithm 2:
Algorithm 2 BIGSAGE: generation of embedding
input: Testing graph Gtest(Vtest, Etest); node attributes Xtest; learned model BIGSAGE
output: Vector representations zv for all v ∈ Vtest
1:	run random-walks for each node in Gtest and gain a set of positive nodes pair:{(v, vp)}
2:	Zi — 0, ∀i ∈ Vtest
3:	for i, j ∈ {(v, vp)} do
4:	use step 5-9 in Algorithm 1 to generate zi , zj
5:	Zi — Zi ∪ {zi}
6:	Zj — Zj ∪ {zj }
return zv — MEAN(Zv), ∀v ∈ Vtest
3.2	co-occurrence bi-attended ”sampling”
We now describe our bi-attention achitecture. Given node n, and node m as a positive node pair,
after sampling T times using a uniform fixed-size sampler, we have
hnKt = Agg(hnK-1, {hj ∈-N t (n) })
hKmt = Agg(hKm-1, {hj ∈-N t (m) })
t = 1, ..., T
With T different representations corresponding to T different sampled neighborhoods, their similar-
ity matrix can be calculated by
Si,j = α(hni, hmj), i, j = 1, ..., T
where α represents an dot-product operation.
Figure 2: Bi-attention layer between the final aggregating layer and loss layer.
4
Under review as a conference paper at ICLR 2019
our goal is to find the most similar or relevant neighborhood match between n and m within T ×
T possibilities, so we need to apply softmax on the flattened similarity matrix, and sum up by
collumn(row) to gain attention over T neighborhoods of n(m):
Qttn = reduce_sum(softmax(S), 0),
attm = reduce_sum(softmax(S), 1),
and apply attention to T Kth layer representations for the final encoded embeddings,
T
hn =	Qttnt hnKt
t=1
T
hm =	Qttmt hKmt
t=1
The aggregation process with bi-attention architecture is illustrated by Figure 2.
3.3	global bias
At first, we consider simply adding a trainable bias upon the encoder’s final ouput for each node in
training set,
zi = hiK + bi
bi = one_hot(i)T B
By training global bias, Our framework will be able to learn parameters of aggregator functions
for inductive learning meanwhile preserve sufficient embedding informations for known nodes. On
one hand, these informations can be reused to produce embeddings for the known nodes or the
unknown connected with the known, as supplement to the aggregator. On the other hand, they
can patially offset the uncertainty of the generation braught by the random sampling. But through
further research, we find it more efficient when appling global bias to not only the last layer but also
all hidden layers, as follows:
hik+1 =Wk+1(hik,{hjk∈N(i)})+bkag+g1+bi
A^v <Γ <Γ ∙∙∙ Λr ∙∙∙
Figure 3: Applying global bias through aggregating .
Applying globla bias vectors through all layers improves not only the expressivity of representations
on hidden layers, but also the training efficiency. In fact, the hidden layers’ output embeddings and
the global biases are now belonging to one single d-dimensional vector space, as a result of which,
the parameters from lower layers can be updated more directly by the involving global bias vectors
that are adjusted for all different layers including the last layer, which means the loss funtion can
now have more instant impact on the lower layers instead of back-propagating from top to bottom.
We propose our aggregating process with global bias in Algorithm 3.
5
Under review as a conference paper at ICLR 2019
Algorithm 3 SAGB: sampling and aggregating with globa_bias
input: node u; hierarchical depth K; weight matrices W k ; non-linearity σ; differentiable neighbor
aggregator Agkg ; fixed-size uniform sampler S : v → 2V
output: embedding zu ;
1:	N0u = {u};
2:	for l = 1...K do
3:	Nu 一 {S(i), ∀i ∈ N-1};
4:	Nu = N0uSN1uS...SNuK;
5:	for k=1...K do
6:	forj∈N0uSN1uS...SNuK-kdo
7:	hj 一 σ(Agg(hj-1, {hk-1, ∀m ∈ S* *(j)}, Wk))+ one-hot(j)τ B
return Zu — hK
4 Experiments
In this section, we evaluate BIGSAGE against strong baselines on three challenging benchmark
tasks: (i) classifying Reddit posts as belonging to different community; (ii) predicting different
classes of papers in Pubmed (Sen et al. (2008)); (iii) classifying protein functions across varieties of
protein-protein interaction (PPI) graphs (Zitnik & Leskovec (2017)). We start by summarizing the
overall settings in our comparison experiments, and then present the experiment results of each task.
We also study the seperate effect of our bi-attention architecture and global bias in section 4.4.
4.1 Experimental settings
We compare BIGSAGE against the following approaches for graph representations learning under a
fully unsupervised and inductive setting:
• GraphSAGE: Our proposed method originates from the unsupervised variant of Graph-
SAGE, a hierarchical sampling and aggregating framwork for inductive learning. We com-
pare our method against GraphSAGE using three different aggregator: (1) Mean aggrega-
tor, which simply takes the elementwise mean of the vectors in hku-∈N1 (v); (2) LSTM aggre-
gator, which adapts LSTMs to encode a random permutation of a node’s neighbors’ hk-1;
(3) Maxpool aggregator, which apply an elementwise maxpooling operation to aggregate
information across the neighbor nodes.
• GraPh2Gauss(Bojchevski & Gunnemann (2017)): Unlike GraPhSAGE and my method,
G2G only uses the attributes of nodes to learn their representations, with no need for link
information. Here we comPare against G2G to Prove that certain trade-off between sam-
Pling granularity control and embedding effectiveness does exists in inductive learning
scenario.
• SPINE, Guo et al. (2018): Instead of hierarchical neighbor samPling, SPINE uses Rooted-
PageRankLiben-Nowell & Kleinberg (2007) to rePresent the high-order structural Proxim-
ities of neighborhood. The k largest Proximities are then emPloyed to aggregate attributes
of the corresPonding k neighbor nodes.
For our ProPosed aPProach and the origin framework, we set the number of hierarchical layers as
K = 2 with neighbor samPling sizes S1 = 20 and S2 = 10, the number of random-walks for each
node as 100 and the walk length as 5. The samPling time of our bi-attention layer is set as T = 10.
For all methods, the dimensionality of embeddings is set to 256. Our aPProach is imPemented in
Tensorflow (Abadi et al. (2016)) and trained with the Adam oPtimizer (Kingma & Ba (2014)) at an
initial learning rate of 0.0001. We rePort the comParison results in Table 1.
4.2 Inductive learning in evolving graphs
In real-world large and evolving graPhs, inductive node embedding learning techniques would re-
quire high efficiency of the information extraction strategy as well as stability and robustness of the
6
Under review as a conference paper at ICLR 2019
Table 1: Prediction results for the three datasets (micro-averaged F1 scores).
	Reddit	Pubmed	PPI
Graph2Gauss	-	0.789	0.430
SPINE	-	-	0.505
GraphSAGE-mean	0.897	0.820	0.502
BIGSAGE-mean	0.927	0.834	0.573
GraphSAGE-seq	0.907	0.811	0.505
BIGSAGE-seq	0.903	0.828	0.563
GraphSAGE-pool	0.892	0.830	0.510
BIGSAGE-pool	0.915	0.843	0.566
embedding encoder. We here perform comparison against other approaches to show our method’s
effective use in these challenging datasets.
Reddit is an large internet forum where users can post and comment on any content they are in-
terested in. The task is to predict the community, that a post belongs to. We use the exact dataset
conducted by Hamilton et al. (2017b). In this dataset, each node represents a post and are connected
with one another if the same user comments on both of them. The node attribute is constructed
by word2vec embeddings of post contents. The first 20 days is for training, and the rest for test-
ing/validation. In all, this dataset contains 232,965 nodes(posts) with an average degree of 492. The
first collumn summarizes the comparison results against GraphSAGE. For LSTM aggregator, our
model shows slightly poorer performance, which is reasonable, because LSTM aggregator causes
more differences between multiple sampled neighborhoods, not only the components but also the
order, making it harder for our bi-attention layer to capture the proximities between neighborhoods.
One can observe that BIGSAGE outperforms GraphSAGE in both mean and pool aggregator.
Another representative of evolving graphs we evaluate on is Pubmed, one of the commonly used
citation network data . This dataset contains 19717 nodes and 44324 edges. We remove 20 percengt
of the nodes as unseen, the rest for training. From the second column of Table 2 we find our model
have better prediction results in all three aggregators.
4.3	Generalizing across graphs
Generalizing accross graphs requires inductive methods capable of learning a transferable encoder
function rather than the present community structure.
The protein-protein-interaction(PPI) networks dataset consists of 24 graphs corresponding to differ-
ent human tissues(Zitnik & Leskovec (2017)). We use the preprocessed data provided by Hamilton
et al. (2017b), where 20 graphs for training, 2 for validation and 2 for testing. For each node, there
are 50 features representing their positional gene sets, motif gene sets and immunological signatures,
and 121 labels set from gene ontology( collected from the Molecular Signatures Database (?)). This
dataset contains 56944 nodes and 818716 edges.
The final collumn of Table 1 shows us that our method outperforms GraphSAGE by 14% at most on
the PPI data. The results on three different aggregators indicates that the Mean-aggregator beats the
other two in our method. And we also quote the micro-averaged F1 score of SPINE which is based
on the exact same PPI dataset as ours.
4.4	Model study
In this section we adjust BIGSAGE for tests on PPI to further study the seperate effect of bi-attention
layer and global bias:
•	BIGSAGE-ba: only with bi-attention layer, no global bias;
•	BIGSAGE-sg: with bi-attention layer, global bias only applied to embeddings of the last
layer;
•	BIGSAGE-cb: with bi-attention layer, global bias applied to all layer during training but
forgotten (reset to zero matrix) while generating embeddings.
7
Under review as a conference paper at ICLR 2019
Table 2: Model Study
PPr
GraphSAGE	0.502
BIGSAGE-ba	0.521
BIGSAGE-sg	0.518
BIGSAGE-cb	0.523
BIGSAGE	0.572
From Table 2, we observe the three variant of BIGSAGE still show certain advance to the origin
GraphSAGE, but reasonably less accurate than the origin BIGSAGE. The result of using only bi-
attention layer proves the effect of bi-attended sampling. And the comparison between BIGSAGE-sg
and BIGSAGE shows the high efficiency of applying global bias through all layers. From the result
of BIGSAGE-cb, we find that even after training with global-bias, it’s critical to have the memory
of the known nodes, which is stored in the learnt global embedding biases.
4.5	Trade-off
We compare Graph2Gauss against our model as well as GraphSAGE on Pubmed and PPI. Note that
Graph2Gauss only needs node attributes for embedding learning. The comparison results in Table
1 shows that our model and GraphSAGE beat g2g whether in evolving network data or generalizing
across different graphs, which proves the significance of neighbor sampling for inductive learning
and that certain trade-off exists between encoding granularity and embedding effect.
5 Conclusions
In this paper, we proposed BIGSAGE, an unsupervised and inductive network embedding approach
which is able to preserve local proximity wisely as well as learn and memorize global identities for
seen nodes while generalizing to unseen nodes or networks. We apply a bi-attention architeture upon
hierarchical aggregating layers to directly capture the most relevant representations of co-occurring
nodes. We also present an efficient way of combining inductive and transductive approaches by
allowing trainable global embedding bias to be retrieved in all layers within the hierarchical ag-
gregating framework. Experiments demenstrate the superiority of BIGSAGE over the state-of-art
baselines on unsupervised and inductive tasks.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Gre-
gory S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian J. Good-
fellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal JOzefowicz, LUkasz
Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Gor-
don Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, FernandaB. Viegas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow:
Large-scale machine learning on heterogeneous distributed systems. CoRR, abs/1603.04467,
2016. URL http://arxiv.org/abs/1603.04467.
Aleksandar Bcjchevski and Stephan Gunnemann. Deep gaussian embedding of attributed graphs:
Unsupervised inductive learning via ranking. CoRR, abs/1707.03815, 2017.
Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio.
Attention-based models for speech recognition. In Advances in neural information processing
systems,pp. 577-585, 2015.
Robert Desimone and John Duncan. Neural mechanisms of selective visual attention. Annual review
of neuroscience, 18(1):193-222, 1995.
8
Under review as a conference paper at ICLR 2019
David K. Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, RafaeI Gomez-Bombarelli,
Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for
learning molecular fingerprints. In NIPS, pp. 2224-2232, 2015.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In KDD, pp.
855-864, 2016.
Junliang Guo, Linli Xu, and Enhong Chen. SPINE: structural identity preserved inductive network
embedding. CoRR, abs/1802.03984, 2018. URL http://arxiv.org/abs/1802.03984.
William L. Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. IEEE Data Eng. Bull., 40(3):52-74, 2017a.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In NIPS, 2017b.
Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid
scene analysis. IEEE Transactions on pattern analysis and machine intelligence, 20(11):1254-
1259, 1998.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
David Liben-Nowell and Jon M. Kleinberg. The link-prediction problem for social networks. JA-
SIST, 58(7):1019-1031, 2007.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
Shirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, and Yang Wang. Tri-party deep network repre-
sentation. In IJCAI, pp. 1895-1901, 2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representa-
tions. In Sofus A. Macskassy, Claudia Perlich, Jure Leskovec, Wei Wang, and Rayid Ghani (eds.),
KDD, pp. 701-710. ACM, 2014.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93, 2008.
Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention
flow for machine comprehension. CoRR, abs/1611.01603, 2016. URL http://arxiv.org/
abs/1611.01603.
Xiaofei Sun, Jiang Guo, Xiao Ding, and Ting Liu. A general framework for content-enhanced
network representation learning. CoRR, abs/1610.02906, 2016.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale
information network embedding. In WWW, pp. 1067-1077, 2015.
Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-evolve: Deep temporal reasoning
for dynamic knowledge graphs. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 3462-3471, 2017. URL
http://proceedings.mlr.press/v70/trivedi17a.html.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. CoRR, abs/1710.10903, 2017.
Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In KDD, pp. 1225-
1234, 2016.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
with visual attention. In ICML, pp. 2048-2057, 2015.
9
Under review as a conference paper at ICLR 2019
Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y. Chang. Network representation
learning with rich text information. In IJCAI, pp. 2111-2117, 2015.
Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue
networks. Bioinformatics, 33(14):i190-i198, 2017.
10