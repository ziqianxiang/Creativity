Under review as a conference paper at ICLR 2019
ON THE EFFECT OF THE ACTIVATION FUNCTION
ON THE DISTRIBUTION OF HIDDEN NODES
IN A DEEP NETWORK
Anonymous authors
Paper under double-blind review
ABSTRACT
We analyze the joint probability distribution on the lengths of the vectors of hidden
variables in different layers of a fully connected deep network, when the weights
and biases are chosen randomly according to Gaussian distributions, and the input
is in {−1, 1}N . We show that, if the activation function φ satisfies a minimal set
of assumptions, satisfied by all activation functions that we know that are used
in practice, then, as the width of the network gets large, the “length process”
converges in probability to a length map that is determined as a simple function of
the variances of the random weights and biases, and the activation function φ.
We also show that this convergence may fail for φ that violate our assumptions.
1 INTRODUCTION
The size of the weights of a deep network must be managed delicately. If they are too large, signals
blow up as they travel through the network, leading to numerical problems, and if they are too small,
the signals fade away. The practical state of the art in deep learning made a significant step forward
due to schemes for initializing the weights that aimed in different ways at maintaining roughly
the same scale for the hidden variables before and after a layer [9, 4]. Later work [7, 14, 2] took
into account the effect of the non-linearities on the length dynamics of a deep network, informing
initialization policies in a more refined way.
In this paper, we continue this line of work, theoretically analyzing what might be called the “length
process”. That is, for a given input, chosen for simplicity from {−1, 1}N , we study the probability
distribution over the lengths of the vectors of hidden variables, when the parameters of a deep
network are chosen randomly. We analyze the case of fully connected networks, with the same
activation function φ at each hidden node and N hidden variables in each layer. As in [14], we
consider the case where weights between nodes are chosen from a zero-mean Gaussian with variance
σ2w/N, and where the biases are chosen from a zero-mean distribution with variance σb2.
Our first result holds for activation functions φ that satisfy the following properties: (a) the restriction
of φ to any finite interval is bounded; (b) as z gets large, |φ(z)| = exp(o(z2
)); (c) φ is measurable.
We refer to such φ as permissible. Note that conditions (a) and (c) both hold for any non-decreasing
φ.
We show that, for all permissible φ and all σw and σb, as N gets large, the length process converges
in probability to a length map that is a simple function of φ, σw and σb. This length map was first
discovered in [14], where it was claimed that it holds for all φ; it has since been used in a number of
other papers [15, 17, 12, 10, 16, 1, 13, 5].
In Section 4, to motivate our new analysis, we provide examples of φ that are not permissible that
lead the length processes with arguably surprising properties. For example, we show that, for arbi￾trarily small positive σw, even if σb = 0, for φ(z) = 1/z, the distribution of values of each of the
hidden nodes in the second layer diverges as N gets large. For finite N, each node has a Cauchy
distribution, which already has infinite variance, and as N gets large, the scale parameter of the
Cauchy distribution gets larger, leading to divergence. We also show that the hidden variables in the
1
Under review as a conference paper at ICLR 2019
second layer may not be independent, even for some permissible φ like the ReLU. The results of
this section contradict claims made in [14, 10].
Section 5 describes some simulation experiments verifying some of the findings of the paper, and
illustrating the dependence among the values of the hidden nodes.
Our analysis of the convergence of the length map borrows ideas from Daniely, et al. [2], who
studied the properties of the mapping from inputs to hidden representations resulting from random
Gaussian initialization. Their theory applies in the case of activation functions with certain smooth￾ness properties, and to a wide variety of architectures. Our analysis treats a wider variety of values
of σw and σb, and uses weaker assumptions on φ.
2 PRELIMINARIES
2.1 NOTATION
For n ∈ N, we use [n] to denote the set {1, 2, . . . , n}. If T is a n × m × p tensor, then, for i ∈ [n],
let Ti,:,: = h Ti,j,ki jk, and define Ti,j,:
, etc., analogously.
2.2 THE FINITE CASE
Consider a deep fully connected width-N network with D layers. Let W ∈ RD×N×N . An activa￾tion function φ maps R to R; we will also use φ to denote the function from RN to RN obtained
by applying φ componentwise. Computation of the neural activity vectors x0,:
, ..., xD,: ∈ RN and
preactivations h1,:
, ..., hD,: ∈ RN proceeds in the standard way as follows:
h`,: = W`,:,:x` −1,: + b`,: x`,: = φ(h`,:), for ` = 1, . . . , D.
We will study the process arising from fixing an arbitrary input x0,: ∈ {−1, 1}N and choosing the
parameters independently at random: the entries of W are sampled from Gauss  0, σ2wN 
, and the
entries of b from Gauss ￾ 0, σb2
. For each ` ∈ [D], define q` = 1N P Ni=1 h2
`,i.
Note that for all ` ≥ 1, all the components of h`,: and x`,: are identically distributed.
2.3 THE WIDE-NETWORK LIMIT
For the purpose of defining a limit, assume that, for a fixed, arbitrary function χ : N → {−1, 1}, for
finite N, we have x0,: = (χ(1), ..., χ(N)). For ` > 0, if the limit exists (in the sense of “convergence
in distribution”), let x` be a random variable whose distribution is the limit of the distribution of x`,1
as N goes to infinity. Define h` and q` similarly.
2.4 TOTAL VARIATION DISTANCE
If P and Q are probability distributions, then dT V (P, Q) = supE P(E) − Q(E), and if p and q are
their densities, dT V (P, Q) = 12 R |p(x) − q(x)| dx.
3 CONVERGENCE IN PROBABILITY
In this section we characterize the length map of the hidden nodes of a deep network, for all activa￾tion functions satisfying the following assumptions.
Definition 1 An activation function φ is permissible if, (a) the restriction of φ to any finite interval
is bounded; (b) |φ(x)| = exp(o(x2
)) as |x| gets large.1
; and (c) φ is measurable.
Conditions (b) and (c) ensure that a key integral can be computed. The proof of Lemma 1 is in
Appendix A.
1 This condition may be expanded as follows, limsupx→∞
log |φ(x)| x2 = 0 and limsupx→−∞
log |φ(x)| x2 = 0. 2
Under review as a conference paper at ICLR 2019
Lemma 1 If φ is permissible, then, for all positive constants c, the function g defined by g(x) =
φ(cx)2
exp(−x2/2) is integrable.
Now, we recall the definition of a length map from [14]; we will prove that the the length process
converges to this length map. Define q˜0, ..., q˜D and r˜0, ..., r˜D recursively as follows. First q˜0 = r˜0 = 1. Then, for ` > 0, q˜` = σ2wr˜` −1 + σb2
and
r˜` = Ez∈Gauss(0,1)[φ(p q˜` z)2].
If φ is permissible, then, since φ(cz)2
exp(−z2/2) is integrable for all c, we have that
q˜0, ..., q˜D, r˜0, ..., r˜D are well-defined finite real numbers.
The following theorem shows that the length map q0, ..., qD converges in probability to q˜0, ..., q˜D.
Theorem 2 For any permissible φ, σw, σb ≥ 0, any depth D, and any , δ > 0, there is an N0 such
that, for all N ≥ N0, with probability 1 − δ, for all ` ∈ {0, ..., D}, we have |q` − q˜` | ≤ .
The rest of this section is devoted to proving Theorem 2. Our proof will use the weak law of large
numbers.
Lemma 3 ([3]) For any random variable X with a finite expectation, and any , δ > 0, there is an
N0 such that, for all N ≥ N0, if X1, ..., XN are i.i.d. with the same distribution as X, then
Pr 

 

 E[X] − 1N NXi=1
Xi > ! ≤ δ.
In order to divide our analysis into cases, we need the following lemma, whose proof is in Ap￾pendix B.
Lemma 4 If φ is permissible and not zero a.e., for all σw > 0, for all ` ∈ {0, ..., D}, q˜` > 0 and
r˜` > 0.
We will also need a lemma that shows that small changes in σ lead to small changes in Gauss(0, σ2).
Lemma 5 (see [8]) There is an absolute constant C such that, for all σ1, σ2 > 0, dT V (Gauss(0, σ21), Gauss(0, σ22
)) ≤ C |σ1−σ2| σ1 .
The following technical lemma is proved in Appendix C.
Lemma 6 If φ is permissible, for all 0 < r ≤ s, for all β > 0, there is an a ≥ 0 such that, for all
q ∈ [r, s], R a∞ φ(√qz)2
exp(−z2/2) dz ≤ β and R −a
−∞ φ(√qz)2
exp(−z2/2) dz ≤ β.
Armed with these lemmas, we are ready to prove Theorem 2.
First, if φ is zero a.e., or if σw = 0, Theorem 2 follows directly from Lemma 3, together with a
union bound over the layers. Assume for the rest of the proof that φ(x) is not zero a.e., and that
σw > 0, so that q˜` > 0 and r˜` > 0 for all ` .
For each ` ∈ [D], define r` = 1N P Ni=1 x2
`,i.
Our proof of Theorem 2 is by induction. The inductive hypothesis is that, for any , δ > 0 there
is an N0 such that, if N ≥ N0, then, with probability 1 − δ, for all ` 0 ≤ ` , |q` 0 − q˜` 0 | ≤  and
|r` 0 − r˜` 0 | ≤  .
The base case holds because q0 = ˜q0 = r0 = ˜r0 = 1, no matter what the value of N is.
Now for the induction step; choose ` > 0, 0 <  < min{q˜` /4, r˜` } and 0 < δ ≤ 1/2. (Note that
these choices are without loss of generality.) Let  0 ∈ (0, ) take a value that will be described later,
using quantities from the analysis. By the inductive hypothesis, whatever the value of  0 , there is an
N00
such that, if N ≥ N00
, then, with probability 1 − δ/2, for all ` 0 ≤ ` − 1, we have |q` 0 − q˜` 0 | ≤  0
and |r` 0 − r˜` 0 | ≤  0 . Thus, to establish the inductive step, it suffices to show that, after conditioning
3
Under review as a conference paper at ICLR 2019
on the random choices before the ` th layer, if |q` −1 − q˜` −1| ≤  0 , and |r` −1 − r˜` −1| ≤  0 , there is
an N` such that, if N ≥ N` , then with probability at least 1 − δ/2 with respect only to the random
choices of W`,:,: and b`,:
, that |q` − q˜` | ≤  and |r` − r˜` | ≤  . Given such an N` , the inductive step
can be satisfied by letting N0 be the maximum of N00
and N` .
Let us do that. For the rest of the proof of the inductive step, let us condition on outcomes of the
layers before layer ` , and reason about the randomness only in the ` th layer. Let us further assume
that |q` −1 − q˜` −1| ≤  0 and |r` −1 − r˜` −1| ≤  0 .
Recall that q` = 1N P Ni=1 h2
`,i. Since we have conditioned on the values of h` −1,1, ..., h` −1,N ,
each component of h`,i is obtained by taking the dot-product of x` −1,: = φ(h` −1,:) with W`,i,:
and adding an independent b`,i. Thus, conditioned on h` −1,1, ..., h` −1,N , we have that h`,1, ..., h`,N
are independent. Also, since x` −1,:
is fixed by conditioning, each h`,i has an identical Gaussian
distribution.
Since each component of W and b has zero mean, each h`,i has zero mean.
Choose an arbitrary i ∈ [N]. Since x` −1,:
is fixed by conditioning and W`,i,1, ..., W`,i,N and b`,i are
independent,
E[q` ] = E[h2
`,i] = σb2 + σ2wN X j x2`−1,j = σb2 + σ2wr` −1
def = q` . (1)
We wish to emphasize the q` is determined as a function of random outcomes before the ` th layer,
and thus a fixed, nonrandom quantity, regarding the randomization of the ` th layer. By the inductive
hypothesis, we have
|E[q` ] − q˜` | = |E[h2
`,i] − q˜` | = |q` − q˜` | = σ2w|r` −1 − r˜` −1| ≤  0 σ2w. (2)
The key consequence of this might be paraphrased by saying that, to establish the portion of the in￾ductive step regarding q` , it suffices for q` to be close to its mean. Now, we want to prove something
similar for r` . We have
E[r` ] = 1N NXi=1
E[x2
`,i] = 1N NXi=1
E[φ(h`,i)2
] = E[φ(h`,1)2],
since h`,1, ..., h`,N are i.i.d. Recall that, earlier, we showed that h`,i ∼ Gauss(0, q` ). Thus
E[r` ] = Ez∼Gauss(0,q` )[φ(z)2
] = Ez∼Gauss(0,1)[φ(p q` z)2
] = r
21π Z φ(p q` z)2
exp(−z2/2) dz.
which gives
|E[r` ] − r˜` | ≤
  Ez∼Gauss(0,q` )[φ(z)2] − Ez∼Gauss(0,q˜` )[φ(z)2] .
Since |q` − q˜` | ≤  0 σ2w and we may choose  0 to ensure  0 ≤ ˜q`2σ2w
, we have q˜` /2 ≤ q` ≤ 2˜q` .
For β > 0 and κ ∈ (0, 1/2) to be named later, by Lemma 6, we can choose a such that, for all
q ∈ [˜q` /2, 2˜q` ], Z −a
−∞
φ(√
qz)2
exp(−z2/2) dz ≤ β/2 and Z
a∞ φ(√
qz)2
exp(−z2/2) dz ≤ β/2
and √21
πq
R
a−a
exp  −z22q 
dz ≥ 1 − κ. Choose such an a. 4
Under review as a conference paper at ICLR 2019
We claim that
 
 R a−a φ(√qz)2
exp(−z2/2) dz − R φ(√qz)2
exp(−z2/2) dz

 ≤ β for all q˜` /2 < q ≤ 2˜q` . Choose such a q. We have




Z
a−a φ(√
qz)2
exp(−z2/2) dz − Z φ(√
qz)2
exp(−z2/2) dz



= Z −a
−∞
φ(√
qz)2
exp(−z2/2) dz + Z a∞ φ(√
qz)2
exp(−z2/2) dz
≤ 2 max  Z
−a
−∞
φ(√
qz)2
exp(−z2/2) dz, Z
∞a φ(√
qz)2
exp(−z2/2) dz
≤ β.
So now we are trying to bound
   R a−a φ(√q` z)2
exp(−z2/2) dz − R a−a φ(√q˜` z)2
exp(−z2/2) dz


using q˜` /2 ≤ q` ≤ 2˜q` .
Using changes of variables, we have




Z
a−a φ(p q` z)2
exp(−z2/2) dz − Z −aa φ(p q˜` z)2
exp(−z2/2) dz



=      1√q` Z a√q` −a√q` φ(z)2
exp  − z2 2q` 
dz − √1q˜` Z a√q˜` −a√q˜` φ(z)2
exp  − z2
2˜q` 
dz



 .
Since φ is permissible, φ2
is bounded on [−a√
2˜q` , a√
2˜q` ]. If P is the distribution obtained by con￾ditioning Gauss(0, q` ) on [−a√q` , a√q` ], and ˜P by conditioning Gauss(0, q˜` ) on [−a√q˜` , a√q˜` ],
then if M = √2π supz∈[−a√
2˜q` ,a√
2˜q` ] φ(z)2
, since q` ≤ 2˜q` ,      1√q` Z a√q` −a√q` φ(z)2
exp(− z2 2q` ) dz − 1√q˜` Z a√q˜` −a√q˜` φ(z)2
exp(− z2
2˜q` ) dz



 ≤ M dT V (P, P˜).
But since, for κ < 1/2, conditioning on an event of probability at least 1 − κ only changes a
distribution by total variation distance at most 2κ, and therefore, applying Lemma 5 along with the
fact that |q` − q˜` | ≤  0 σ2w, for the constant C from Lemma 5, we get
dT V (P, P˜) ≤ 4κ + dT V (Gauss(0, q` ), Gauss(0, q˜` ))
≤ 4κ + C|√q` − √q˜` | √q˜`
= 4κ + C|q` − ˜q` | |√q` + √q˜` |√q˜` ≤ 4κ +
C0 σ2 q˜` w .
Tracing back, we have




Z
−aa φ(p q` z)2
exp(−z2/2) dz − Z −aa φ(p q˜` z)2
exp(−z2/2) dz


 ≤ M  4κ +
C0 σ2w q˜` 
which implies
|E[r` ] − r˜` | ≤

 
 Z φ(p q` z)2
exp(−z2/2) dz − Z φ(p q˜` z)2
exp(−z2/2) dz



≤ M  4κ +
C0 σ2w q˜` 
+ 2β.
If κ = min{ 24
M , 13 }, β =  12 , and  0 = min n  2 ,  2σ2w , ˜q`2σ2w , q˜`  6CMσ2w o
this implies |E[r` ] − r˜` | ≤
/2. 5
Under review as a conference paper at ICLR 2019
Recall that q` is an average of N identically distributed random variables with a mean between 0
and 2˜q` (which is therefore finite) and r` is an average of N identically distributed random variables,
each with mean between 0 and r˜` +/2 ≤ 2˜r` . Applying the weak law of large numbers (Lemma 3),
there is an N` such that, if N ≥ N` , with probability at least 1 − δ/2, both |q` − E[q` ]| ≤ /2 and
|r` − E[r` ]| ≤ /2 hold, which in turn implies |q` − q˜` | ≤  and |r` − r˜` | ≤  , completing the proof
of the inductive step, and therefore the proof of Theorem 2.
4 DIVERSITY OF BEHAVIOR IN THE DISTRIBUTION OF HIDDEN NODES
In this section, we show that, for some activation functions, the probability distribution of hidden
nodes can have some surprising properties.
4.1 NON-GAUSSIAN
In this subsection, we will show that the hidden variables are sometimes not Gaussian. Our proof
will refer to the Cauchy distribution.
Definition 2 A distribution over the reals that, for x0 ∈ R and γ > 0, has a density f given by
f(x) = 1
πγh 1+( x−γx0 )2i is a Cauchy distribution, denoted by Cauchy(x0, γ). Cauchy(0, 1) is the
standard Cauchy distribution.
Lemma 7 ([6]) If X1, ..., Xn are i.i.d. random variables with a Cauchy distribution, then
1n P ni=1 Xi has the same distribution.
Lemma 8 ([11]) If U and V are zero-mean normally distributed random variables with the same
variance, then U/V has the standard Cauchy distribution.
The following shows that there is a φ such that the limiting h2
is not defined. It contradicts claims
made on line 7 of Section A.1 of [14] and line 7 of Section 2.2 of [10].
Proposition 9 There is a φ such that, for every σw > 0, if σb = 0, then (a) for finite N, h2,1 does
not have a Gaussian distribution, and (b) h2,1 diverges as N goes to infinity.
Proof: Consider φ defined by φ(y) =  1/y if y 6 = 0
0 if y = 0.
Fix a value of N and σw > 0, and take σb = 0. Each component of h1,:
is a sum of zero-mean
Gaussians with variance σ2w/N; thus, for all i, h1,i ∼ Gauss(0, σ2w). Now, almost surely, h2,1 = P Nj=1 W2,1,jφ(h1,j ) = P Nj=1 W2,1,j/h1,j . By Lemma 8, for each j, W2,1,j/h1,j has a Cauchy
distribution, and since (NW2,1,1), ...,(NW2,1,N ) ∼ Gauss(0, Nσ2w), recalling that h1,1, ..., h1,N ∼
Gauss(0, σ2w), we have that NW2,1,1/h1,1, ..., NW2,1,N /h1N are i.i.d. Cauchy(0, √N). Applying
Lemma 7, h2,1 = P Nj=1 W2,1,jφ(h2,j ) = 1N P Nj=1 NW2,1,jφ(h1,j ) is also Cauchy(0, √N).
So, for all N, h2,1 is Cauchy(0, √N). Suppose that h2,1 converged in distribution to some dis￾tribution P. Since the cdf of P can have at most countably many discontinuities, we can cover
the real line by a countable set of finite-length intervals [a1, b1], [a2, b2], ... whose endpoints are
points of continuity for P. Since Cauchy(0, √N) converges to P in distribution, for any i, P([ai
, bi
]) ≤ limN→∞
|bi−ai| π√N
= 0. Thus, the probability assigned by P to the entire real line
is 0, a contradiction.
4.2 INDEPENDENCE
The following contradicts a claim made on line 8 of Section A.1 of [14].
Theorem 10 If φ is either the ReLU or the Heaviside function, then, for every σw > 0, σb ≥ 0, and
N ≥ 2, (h2,1, ..., h2,N ) are not independent.
6
Under review as a conference paper at ICLR 2019
Proof: We will show that E[h22,1h22,2] 6 = E[h22,1]E[h22,2], which will imply that h2,1 and h2,2 are not
independent.
As mentioned earlier, because each component of h1,:
is the dot product of x0,: with an independent
row of W1,:,: plus an independent component of b1,:
, the components of h1,: are independent, and
since x1,: = φ(h1,:), this implies that the components of x1,: are independent. Since each row of
W1,:,: and each component of the bias vector has the same distribution, x1,:
is i.i.d.
We have
E[h22,1
] = E iX∈[N] W2,1,ix1,i
 + b2,12 = X (i,j)∈[N]2 E [W2,1,iW2,1,jx1,ix1,j ] + X
i∈[N] E [W2,1,ix1,ib2,1] + E  b22,1 .
The components of W2,:,: and x1,:
, along with b2,1, are mutually independent, so terms in the double
sum with i 6 = j have zero expectation, and E[h22,1
] =  P i∈[N] E  W22,1,i E  x21,i  + E[b22,1]. For
a random variable x with the same distribution as the components of x1,:
, this implies
E[h22,1
] = σ2wE  x2 + σb2. (3)
Similarly,
E[h22,1h22,2] = E iX∈[N] W2,1,ix1,i + b2,12 iX∈[N] W2,2,ix1,i + b2,22 = X (i,j,r,s)∈[N]4 E[W2,1,iW2,1,jW2,2,rW2,2,sx1,ix1,jx1,rx1,s]
+ 2 X
(i,j,r)∈[N]3 E[W2,1,iW2,1,jW2,2,rx1,ix1,jx1,rb2,2]+2
(i,r,s
X)∈[N]3 E[W2,1,iW2,2,rW2,2,sx1,ix1,rx1,sb2,1]
+ 4 X
(i,r)∈[N]2 E[W2,1,iW2,2,rx1,ix1,rb2,1b2,2] + X (i,j)∈[N]2 E[W2,1,iW2,1,jx1,ix1,j b22,2
] +
(r,s
X)∈[N]2 E[W2,2,rW2,2,sx1,rx1,sb22,1]
+ 2 X
i∈[N] E[W2,1,ix1,ib2,1b22,2
] + 2
rX∈[N] E[W2,2,rx1,rb22,1b2,2] + E[b22,1b22,2] = X (i,r)∈[N]2,i6=r E[W22,1,iW22,2,r]E[x21,i]E[x21,r] +
iX∈[N] E[W22,1,iW22,2,i]E[x41,i] + X i∈[N] E[W22,1,i]E[x21,i]E[b22,2
] +
rX∈[N] E[W22,2,r]E[x21,r]E[b22,1] + E[b21,2b22,2] = (N2 − N)σ4wE[x2]2 N2 +
Nσ4wE[x4] N2 + 2Nσ2wE[x2]σb2 N + σb4 = σ4wE[x2]2 + σ4w(E[x4] − E[x2]2) N
+ 2σ2wσb2E[x2
] + σb4.
Putting this together with (3), we have
E[h22,1h22,2] − E[h22,1]E[h22,2
] = σ4w(E[x4] − E[x2]2) N . (4)
7
Under review as a conference paper at ICLR 2019
(a) N = 10 (b) N = 100 (c) N = 1000
Figure 1: Histograms of h[2, :], averaged over 100 random initializations, for N ∈ {10, 100, 1000},
along with Cauchy(0, √N) (shown in green) and Gauss(0, σ2) for σ estimated from the data
(shown in red).
Now, we calculate the difference using (4) for the Heaviside and ReLU functions.
Heaviside. Suppose φ is Heaviside function, i.e. φ(z) is the indicator function for z > 0. In this
case, since the components of h1,: are symmetric about 0, the distribution of x1,:
is uniform over
{0, 1}N . Thus E[x4
] = E[x2
] = 1/2, and so (4) gives E[h22,1h22,2] − E[h22,1]E[h22,2
] = 3σ4w 4N 6 = 0.
ReLU. Next, we consider the case that φ is the ReLU. Recalling that, for all i, h1,i ∼ Gauss(0, σ2w),
we have E[x2
] = √21
πσ2w R 0∞ z2
exp  −z2 2σ2w 
dz. By symmetry this is 12Ez∼Gauss(0,σ2w)[z2
] = σ2w/2.
Similarly, E[x4
] = 12Ez∼Gauss(0,σ2w)[z4
] = 3σ4 2
. Plugging these into (4) we get that, in the case the
φ is the ReLU, that
E[h22,1h22,2] − E[h22,1]E[h22,2
] = σ4w ￾ (3/2)σ4w − σ4w/4 N = 5σ8w 4N > 0,
completing the proof.
4.3 UNDEFINED LENGTH MAP
Here, we show, informally, that for φ at the boundary of the second condition in the definition
of permissibility, the recursive formula defining the length map q˜` breaks down. Roughly, this
condition cannot be relaxed.
Proposition 11 For any α > 0, if φ is defined by φ(x) = exp(αx2), there exists a σw, σb s.t. q˜` , r˜`
is undefined for all ` ≥ 2.
Proof: Suppose σ2w + σb2 = 14α2 . Then q˜1 = 14α2 , so that
r˜1 = 1 √2π Z ∞
−∞
φ(p q˜1z) exp  −z22 
dz = √12π Z ∞
−∞
exp(αp q˜1z2
) exp  −z22 
dz
= 1 √2π Z ∞
−∞
exp(z2/2) exp  −z22 
dz = ∞,
and downsteam values of q˜` and r˜` are undefined.
5 EXPERIMENTS
Our first experiment fixed x[0, :] = (1, ..., 1), σw = 1, σb = 0, φ(z) = 1/z.
For each N ∈ {10, 100, 1000}, we (a) initialized the weights 100 times, (b) plotted the his￾tograms of all of the values of h[2, :], along with the Cauchy(0, √N) distribution from the proof
of Proposition 9, and Gauss(0, σ2) for σ estimated from the data. Consistent with the theory, the
Cauchy(0, √N) distribution fits the data well.
To illustrate the fact that the values in the second hidden layer are not independent, for N = 1000
and the parameters otherwise as in the other experiment, we plotted histograms of the values seen
8
Under review as a conference paper at ICLR 2019
Figure 2: Histograms of h[2, :] for nine random weight initializations.
in the second layer for nine random initializations of the weights in Figure 2. When some of the
values in the first hidden layer have unusually small magnitude, then the values in the second hidden
layer coordinately tend to be large. This is in contrast with the claim made at the end of Section
2.2 of [10]. Note that this is consistent with Theorem 2 establishing convergence in probability for
permissible φ, since the φ used in this experiment is not permissible.
REFERENCES
[1] M. Chen, J. Pennington, and S. S. Schoenholz. Dynamical isometry and a mean field theory
of RNNs: Gating enables signal propagation in recurrent neural networks. arXiv preprint
arXiv:1806.05394, 2018.
[2] A. Daniely, R. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The
power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pages 2253–2261, 2016.
[3] W. Feller. An introduction to probability theory and its applications. John Wiley & Sons, 2008.
[4] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence
and statistics, pages 249–256, 2010.
[5] S. Hayou, A. Doucet, and J. Rousseau. On the selection of initialization and activation function
for deep neural networks. arXiv preprint arXiv:1805.08266, 2018.
[6] M. Hazewinkel. Cauchy distribution. In Encyclopaedia of Mathematics: Volume 6. Springer
Science & Business Media, 2013.
[7] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification. In Proceedings of the IEEE international conference
on computer vision, pages 1026–1034, 2015.
[8] B. Klartag. A central limit theorem for convex sets. Inventiones mathematicae, 168(1):91–131,
2007.
[9] Y. A. LeCun, L. Bottou, G. B. Orr, and K. M¨uller. Efficient backprop. In Neural networks:
Tricks of the trade. Springer, 1998.
9
Under review as a conference paper at ICLR 2019
[10] J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural
networks as gaussian processes. ICLR, 2018.
[11] R. Lupton. Statistics in theory and practice. Princeton University Press, 1993.
[12] J. Pennington, S. Schoenholz, and S. Ganguli. Resurrecting the sigmoid in deep learning
through dynamical isometry: theory and practice. In Advances in neural information process￾ing systems, pages 4785–4795, 2017.
[13] J. Pennington, S. S. Schoenholz, and S. Ganguli. The emergence of spectral universality in
deep networks. arXiv preprint arXiv:1802.09979, 2018.
[14] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in
deep neural networks through transient chaos. In Advances in neural information processing
systems, pages 3360–3368, 2016.
[15] S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation.
arXiv preprint arXiv:1611.01232, 2016.
[16] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington. Dynamical isometry
and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural
networks. arXiv preprint arXiv:1806.05393, 2018.
[17] G. Yang and S. Schoenholz. Mean field residual networks: On the edge of chaos. In Advances
in neural information processing systems, pages 7103–7114, 2017.
A PROOF OF LEMMA 1
Choose c > 0. Since limsupx→∞
log |φ(x)| x2 = 0 and limsupx→−∞
log |φ(x)| x2 = 0, we also have
limsupx→∞
log |φ(cx)| x2 = 0 and limsupx→−∞
log |φ(cx)| x2 = 0. Thus, there is an a such that, for all
x 6∈ [−a, a], log |φ(cx)| ≤ x28
, which implies φ(cx)2 ≤ exp  x24 
. Since φ is permissible, it is
bounded on [−a, a]. Thus, we have
Z
φ(cx)2
exp(−x2/2) dx
= Z −a
−∞
φ(cx)2
exp(−x2/2)dx + Z −aa φ(cx)2
exp(−x2/2)dx + Z a∞ φ(cx)2
exp(−x2/2)dx
≤ Z −a
−∞
exp(−x2/4)dx + sup
x∈[−a,a] φ(cx)2! Z −aa
exp(−x2/2)dx + Z a∞
exp(−x2/4)dx
< ∞
completing the proof.
B PROOF OF LEMMA 4
The proof is by induction. The base case holds since q˜0 = ˜r0 = 1.
To prove the inductive step, we need the following lemma.
Lemma 12 If φ is not zero a.e., then, for all c > 0, Ez∈Gauss(0,1)(φ(cz)2) > 0.
Proof: If µ is the Lebesgue measure, since
µ({x ∈ R : φ2(cx) > 0}) = limn→∞
µ({x : φ2(cx) > 1/n} ∩ [−n, n]) > 0,
there exists n such that µ({x : φ2(cx) > 1/n} ∩ [−n, n]) > 0. For such an n, we have
Ez∈Gauss(0,1)(φ(cz)2) ≥ 1ne−n2/2µ({x : φ2(cx) > 1/n} ∩ [−n, n]) > 0.
Returning to the proof of Lemma 4, by the inductive hypothesis, r˜` −1 > 0, which, since σw > 0,
implies q˜` > 0. Applying Lemma 12 yields r˜` > 0.
10
Under review as a conference paper at ICLR 2019
C PROOF OF LEMMA 6
Since limsupx→∞
log |φ(x)| x2 = 0 there is an b such that, for all x ≥ b, log |φ(x)| ≤ x28s
, which implies
φ(x)2 ≤ exp  x24s 
. Now, choose q ∈ [r, s]. For a = b/√r, we then have
Z
∞a φ(√
qx)2
exp(−x2/2) dx
= 1√q Z a∞√q φ(z)2
exp  −z2 2q 
dz
≤ 1√q Z a∞√q
exp 
z2 4s
exp  −z2 2q 
dz
≤ 1√q Z a∞√q
exp  −z2 4q 
dz
≤ 1√q Z b∞
exp  −z2 4q 
dz.
By increasing b if necessary, we can ensure √1q R b∞ exp  −z24q 
dz ≤ β which
then gives R ∞a φ(√qx)2
exp(−x2/2) dx ≤ β. A symmetric argument yields
R
a
−∞ φ(√qx)2
exp(−x2/2) dx ≤ β, completing the proof.
11
