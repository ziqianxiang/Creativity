Under review as a conference paper at ICLR 2019
Convergence Guarantees for RMSProp and
ADAM in Non-Convex Optimization and an Em-
pirical Comparison to Nesterov Acceleration
Anonymous authors
Paper under double-blind review
Ab stract
RMSProp and ADAM continue to be extremely popular algorithms for training
neural nets but their theoretical convergence properties have remained unclear.
Further, recent work has seemed to suggest that these algorithms have worse
generalization properties when compared to carefully tuned stochastic gradient
descent or its momentum variants. In this work, we make progress towards a
deeper understanding of ADAM and RMSProp in two ways. First, we provide
proofs that these adaptive gradient algorithms are guaranteed to reach criticality
for smooth non-convex objectives, and we give bounds on the running time.
Next we design experiments to empirically study the convergence and gen-
eralization properties of RMSProp and ADAM against Nesterov’s Accelerated
Gradient method on a variety of common autoencoder setups and on VGG-9 with
CIFAR-10. Through these experiments we demonstrate the interesting sensitivity
that ADAM has to its momentum parameter β1 . We show that at very high values
of the momentum parameter (β1 = 0.99) ADAM outperforms a carefully tuned
NAG on most of our experiments, in terms of getting lower training and test
losses. On the other hand, NAG can sometimes do better when ADAM’s β1
is set to the most commonly used value: β1 = 0.9, indicating the importance
of tuning the hyperparameters of ADAM to get better generalization performance.
We also report experiments on different autoencoders to demonstrate that
NAG has better abilities in terms of reducing the gradient norms, and it also
produces iterates which exhibit an increasing trend for the minimum eigenvalue
of the Hessian of the loss function at the iterates.
1	Introduction
Many optimization questions arising in machine learning can be cast as a finite sum optimization
problem of the form: minχ f (x) where f (x) = 1 Pk=I fi(x). Most neural network problems
also fall under a similar structure where each function fi is typically non-convex. A well-studied
algorithm to solve such problems is Stochastic Gradient Descent (SGD), which uses updates of the
form: xt+ι := Xt - αVfit (xt), where α is a step size, and 力t is a function chosen randomly from
{f1, f2 , . . . , fk} at time t. Often in neural networks, “momentum” is added to the SGD update to
yield a two-step update process given as: vt+ι = μvt - αVfit (Xt) followed by Xt+ι = Xt + vt+ι.
This algorithm is typically called the Heavy-Ball (HB) method (or sometimes classical momentum),
with μ > 0 called the momentum parameter (Polyak, 1987). In the context of neural nets, another
variant of SGD that is popular is Nesterov’s Accelerated Gradient (NAG), which can also be thought
of as a momentum method (Sutskever et al., 2013), and has updates of the form vt+ι = μvt -
aVfit (Xt + μvt) followed by xt+ι = Xt + vt+ι (see Algorithm 1 for more details).
Momentum methods like HB and NAG have been shown to have superior convergence properties
compared to gradient descent in the deterministic setting both for convex and non-convex functions
(Nesterov, 1983; Polyak, 1987; Zavriev & Kostyuk, 1993; Ochs, 2016; O’Neill & Wright, 2017;
Jin et al., 2017). While (to the best of our knowledge) there is no clear theoretical justification in
1
Under review as a conference paper at ICLR 2019
the stochastic case of the benefits of NAG and HB over regular SGD in general (Yuan et al., 2016;
Kidambi et al., 2018; Wiegerinck et al., 1994; Orr & Leen, 1994; Yang et al., 2016; Gadat et al.,
2018), unless considering specialized function classes (LoizoU & Richtarik, 2017); in practice, these
momentum methods, and in particular NAG, have been repeatedly shown to have good convergence
and generalization on a range of neural net problems (Sutskever et al., 2013; Lucas et al., 2018;
Kidambi et al., 2018).
The performance of NAG (as well as HB and SGD), however, are typically quite sensitive to the
selection of its hyper-parameters: step size, momentum and batch size (Sutskever et al., 2013).
Thus, “adaptive gradient” algorithms such as RMSProp (Algorithm 2) (Tieleman & Hinton, 2012)
and ADAM (Algorithm 3) (Kingma & Ba, 2014) have become very popular for optimizing deep
neural networks (Melis et al., 2017; Xu et al., 2015; Denkowski & Neubig, 2017; Gregor et al.,
2015; Radford et al., 2015; Bahar et al., 2017; Kiros et al., 2015). The reason for their widespread
popularity seems to be the fact that they are believed to be easier to tune than SGD, NAG or HB.
Adaptive gradient methods use as their update direction a vector which is the image under a linear
transformation (often called the “diagonal pre-conditioner”) constructed out of the history of the
gradients, of a linear combination of all the gradients seen till now. It is generally believed that
this “pre-conditioning” makes these algorithms much less sensitive to the selection of its hyper-
parameters. A precursor to these RMSProp and ADAM was outlined in Duchi et al. (2011).
Despite their widespread use in neural net problems, adaptive gradients methods like RMSProp
and ADAM lack theoretical justifications in the non-convex setting - even with exact/deterministic
gradients (Bernstein et al., 2018). Further, there are also important motivations to study the behavior
of these algorithms in the deterministic setting because of usecases where the amount of noise is
controlled during optimization, either by using larger batches (Martens & Grosse, 2015; De et al.,
2017; Babanezhad et al., 2015) or by employing variance-reducing techniques (Johnson & Zhang,
2013; Defazio et al., 2014).
Further, works like Wilson et al. (2017) and Keskar & Socher (2017) have shown cases where SGD
(no momentum) and HB (classical momentum) generalize much better than RMSProp and ADAM
with stochastic gradients. Wilson et al. (2017) also show that ADAM generalizes poorly for large
enough nets and that RMSProp generalizes better than ADAM on a couple of neural network tasks
(most notably in the character-level language modeling task). But in general it’s not clear and no
heuristics are known to the best of our knowledge to decide whether these insights about relative
performances (generalization or training) between algorithms hold for other models or carry over to
the full-batch setting.
A summary of our contributions In this work we try to shed some light on the above described
open questions about adaptive gradient methods in the following two ways.
•	To the best of our knowledge, this work gives the first convergence guarantees for adap-
tive gradient based standard neural-net training heuristics. Specifically we show run-time
bounds for deterministic RMSProp and ADAM to reach approximate criticality on smooth
non-convex functions, as well as for stochastic RMSProp under an additional assumption.
Recently, Reddi et al. (2018) have shown in the setting of online convex optimization that
there are certain sequences of convex functions where ADAM and RMSprop fail to con-
verge to asymptotically zero average regret. We contrast our findings with Theorem 3 in
Reddi et al. (2018). Their counterexample for ADAM is constructed in the stochastic op-
timization framework and is incomparable to our result about deterministic ADAM. Our
proof of convergence to approximate critical points establishes a key conceptual point that
for adaptive gradient algorithms one cannot transfer intuitions about convergence from on-
line setups to their more common use case in offline setups.
•	Our second contribution is empirical investigation into adaptive gradient methods, where
our goals are different from what our theoretical results are probing. We test the conver-
gence and generalization properties of RMSProp and ADAM and we compare their perfor-
mance against NAG on a variety of autoencoder experiments on MNIST data, in both full
and mini-batch settings. In the full-batch setting, we demonstrate that ADAM with very
high values of the momentum parameter (β1 = 0.99) matches or outperforms carefully
tuned NAG and RMSProp, in terms of getting lower training and test losses. We show
that as the autoencoder size keeps increasing, RMSProp fails to generalize pretty soon. In
2
Under review as a conference paper at ICLR 2019
the mini-batch experiments we see exactly the same behaviour for large enough nets. We
further validate this behavior on an image classification task on CIFAR-10 using a VGG-9
convolutional neural network, the results to which we present in the Appendix E.
We note that recently it has been shown by Lucas et al. (2018), that there are problems
where NAG generalizes better than ADAM even after tuning β1 . In contrast our experi-
ments reveal controlled setups where tuning ADAM’s β1 closer to 1 than usual practice
helps close the generalization gap with NAG and HB which exists at standard values of β1.
Remark. Much after this work was completed we came to know of a related paper (Li & Orabona,
2018) which analyzes convergence rates of a modification of AdaGrad (not RMSPRop or ADAM).
After the initial version of our work was made public, a few other analysis of adaptive gradient
methods have also appeared like Chen et al. (2018), Zhou et al. (2018) and Zaheer et al. (2018).
2	Notations and Pseudocodes
Firstly we define the smoothness property that we assume in our proofs for all our non-convex
objectives. This is a standard assumption used in the optimization literature.
Definition 1. L-smoothness If f : Rd → R is at least once differentiable then we call it L-smooth
for some L > 0 if for all x, y ∈ Rd the following inequality holds,
f(y) ≤ f (x) + hvf (x), y - Xi + L ky - χ∣∣2.
We need one more definition that of square-root of diagonal matrices,
Definition 2. Square root of the Penrose inverse If v ∈ Rd and V = diag(v) then we define,
V-2 := Pi∈Support(v) √vieieT, where {ei}{i=1,...,d} is the Standardbasisof Rd
Now we list out the pseudocodes used for NAG, RMSProp and ADAM in theory and experiments,
Nesterov’s Accelerated Gradient (NAG) Algorithm
Algorithm 1 NAG
1:	Input : A step size α, momentum μ ∈ [0,1), and an initial starting
point x1	∈	Rd, and we are given query access to a (possibly
noisy) oracle for gradients of f : Rd → R.
2:	function NAG(Xι,α,μ)
3:	Initialize : v1 = 0
4:	for t = 1, 2, . . . do
5:	vt+ι = μvt + Pf(Xt)
6:	Xt+ι = Xt — α(Vf (xt) + μvt+ι)
7:	end for
8:	end function
RMSProp Algorithm
Algorithm 2 RMSProp
1:	Input : A constant vector Rd 3 ξ1d ≥ 0, parameter β2 ∈ [0, 1), step size α, initial starting point X1 ∈ Rd, and we are given query access to a (possibly noisy) oracle for gradients of f : Rd → R.
2:	function RMSPROP(X1 , β2, α, ξ)
3:	Initialize :	v0 = 0
4:	for t = 1, 2, . . . do
5:	gt = Vf(Xt)
6:	vt = β2vt-1 + (1 - β2)(gt2 + ξ1d)
7:	Vt = diag(vt) 1
8:	Xt+1 = Xt — αVΓ 2 gt
9:	end for
10: end function
3
Under review as a conference paper at ICLR 2019
ADAM Algorithm
Algorithm 3 ADAM
1:	Input : A constant vector Rd 3 ξ1d > 0, parameters β1 , β2 ∈ [0, 1), a
sequence of step sizes {αt}t=1,2.. , initial starting point x1 ∈ Rd,
and we are given oracle access to the gradients of f : Rd → R.
2:	function ADAM(x1 , β1, β2, α, ξ)
3:	Initialize : m0 = 0, v0 = 0
4:	for t = 1, 2, . . . do
5：	gt = Nf(Xt)
6:	mt = β1mt-1 + (1 - β1)gt
7:	vt = β2vt-1 + (1 - β2)gt2
8:	Vt = diag(vt)
9：	Xt+1 = Xt — at (VtI + diag(ξ1d)) mt
10:	end for
11： end function
3 Convergence Guarantees for ADAM and RMSProp
Previously it has been shown in Rangamani et al. (2017) that mini-batch RMSProp can off-the-shelf
do autoencoding on depth 2 autoencoders trained on MNIST data while similar results using non-
adaptive gradient descent methods requires much tuning of the step-size schedule. Here we give the
first result about convergence to criticality for stochastic RMSProp albeit under a certain technical
assumption about the training set (and hence on the first order oracle). Towards that we need the
following definition,
Definition 3. The sign function
We define the function sign : Rd → {—1, 1}d s.t it maps v 7→ (1 if vi ≥ 0 else — 1)i=1,...,d.
Theorem 3.1. Stochastic RMSProp converges to criticality (Proof in subsection A.1) Let
f : Rd → R be L-smooth and be of the form f = ɪ Pk=I f s.t. (a) each f is at least once
differentiable, (b) the gradients are s.t ∀X ∈ Rd, ∀p, q ∈ {1, . . . , k}, sign(Nfp(X)) = sign(Nfq(X))
, (c) σf < ∞ is an upperbound on the norm of the gradients of fi and (d) f has a minimizer,
i.e., there exists x* such that f (x*) = minχ∈Rd f (x). Let the gradient oracle be s.t when invoked
at some Xt ∈ Rd it uniformly at random picks it 〜 {1, 2, ..,k} and returns, ▽/〃(Xt) = g，
Then corresponding to any , ξ > 0 and a starting point x1 for Algorithm 2, we can define,
T≤
1 2 2Lσf(σf +0f (XI)-"x*D
衽(	(1-β2)ξ
s.t. we are guaranteed that the iterates of Algorithm 2 using
a constant step-length of, a =力 j2ξ(1-β2)f (LI)-f(x*)) will find an e-critical point in at most T
steps in the sense that, mint=1,2...,τE[kVf(Xt)II2] ≤ e2.	□
Remark. We note that the theorem above continues to hold even if the constraint (b) that we have
about the signs of the gradients of the {fi}i=1,...,k only holds on the points in Rd that the stochastic
RMSProp visits and its not necessary for the constraint to be true everywhere in the domain. Further
we can say in otherwords that this constraint ensures all the options for the gradient that this stochas-
tic oracle has at any point, to lie in the same orthant ofRd though this orthant itself can change from
one iterate of the next. A related result was concurrently shown by Zaheer et al. (2018).
Next we see that such sign conditions are not necessary to guarantee convergence of the deterministic
RMSProp which corresponds to the full-batch RMSProp experiments in Section 5.3.
Theorem 3.2.	Convergence of deterministic RMSProp - the version with standard speeds
(Proof in subsection A.2) Let f : Rd → R be L-smooth and let σ < ∞ be an upperbound
on the norm of the gradient of f. Assume also that f has a minimizer, i.e., there exists X* such that
f(X*) = minx∈Rd f (X). Then the following holds for Algorithm 2 with a deterministic gradient
oracle:
4
Under review as a conference paper at ICLR 2019
For any E, ξ > 0, using a constant step length of at = α = (1-β2)ξ for t = 1, 2,..., guarantees
L σ2+ξ
that ∣∣Vf (xt)k ≤ E for some t ≤ £ X
algorithm.
2L(σ2+ξ)f (xι)-f(x*))
(1-β2)ξ
, where x1 is the first iterate of the
□
One might wonder if the ξ parameter introduced in the algorithms above is necessary to get con-
vergence guarantees for RMSProp. Towards that in the following theorem we show convergence of
another variant of deterministic RMSProp which does not use the ξ parameter and instead uses other
assumptions on the objective function and step size modulation. But these tweaks to eliminate the
need of ξ come at the cost of the convergence rates getting weaker.
Theorem 3.3.	Convergence of deterministic RMSProp - the version with no ξ shift (Proof
in subsection A.3) Let f : Rd → R be L-smooth and let σ < ∞ be an upperbound on the
norm of the gradient of f. Assume also that f has a minimizer, i.e., there exists x* such that
f (x*) = minχ∈Rd f (x), and the function f be bounded from above and below by constants b` and
Bu as Bl ≤ f (x) ≤ Bu for all X ∈ Rd. Then for any e > 0, ∃T = O(2) s.t. the Algorithm 2 with
a deterministic gradient oracle and ξ = 0 is guaranteed to reach a t-th iterate s.t. 1 ≤ t ≤ T and
□
kVf(xt)k ≤E.
In Section 5.3 we show results of our experiments with full-batch ADAM. Towards that, we ana-
lyze deterministic ADAM albeit in the small β1 regime. We note that a small β1 does not cut-off
contributions to the update direction from gradients in the arbitrarily far past (which are typically
significantly large), and neither does it affect the non-triviality of the pre-conditioner which does not
depend on β1 at all.
Theorem 3.4.	Deterministic ADAM converges to criticality (Proof in subsection A.4) Let
f : Rd → R be L-smooth and let σ < ∞ be an upperbound on the norm of the gradient of f.
Assume also that f has a minimizer, i.e., there exists x* such that f (x*) = minχ∈Rd f (x). Then
the following holds for Algorithm 3:
For any e > 0, βι < ~+ and ξ >
σ2β1
-βισ+e(1-βι)
, there exist step sizes αt > 0, t = 1, 2, . . . and a
natural number T (depending on βι, ξ) such that kVf (xt)k ≤ E for some t ≤ T.
In particular if one sets βι = ~+σ, ξ = 2σ, and at
kgt k2	4ι
L(1-βt )2 3(e+2σ)2
where gt is the gradient
of the objective at the tth iterate, then T can be taken to be 9∣6σ2 [f (x2) — f (x*)], where x2 is the
second iterate of the algorithm.
□
Our motivations towards the above theorem were primarily rooted in trying to understand the situ-
ations where ADAM can converge at all (given the negative results about ADAM as in Reddi et al.
(2018)). But we point out that it remains open to tighten the analysis of deterministic ADAM and
obtain faster rates than what we have shown in the theorem above.
Remark. It is sometimes believed that ADAM gains over RMSProp because of its “bias correction
term” which refers to the step length of ADAM having an iteration dependence of the following
form,，1 - β2/(1 一 βt). In the above theorem, We note that the 1/(1 一 βt) term of this “bias
correction term” naturally comes out from theory!
4 Experimental setup
For testing the empirical performance of ADAM and RMSProp, we perform experiments on fully
connected autoencoders using ReLU activations and shared weights and on CIFAR-10 using VGG-
9, a convolutional neural network. Let z ∈ Rd be the input vector to the autoencoder, {Wi}i=1,..,`
denote the weight matrices of the net and {bi}i=1,..,2' be the bias vectors. Then the output z ∈ Rd
of the autoencoder is defined as Z = W>σ(... σ(W-]σ(W>a + b'+ι) + b'+2)...) + b2' where
a = σ(W'σ(... σ(W2σ(W1z + bi) + b2)...) + b`). This defines an autoencoder with 2' _ 1
hidden layers using ' weight matrices and 2' bias vectors. Thus, the parameters of this model are
given by X = [vec(W1)> ... vec(W^)τ b> ... b>e]τ (where we imagine all vectors to be column
vectors by default). The loss function, for an input Z is then given by: f (z; x) = ∣∣z 一 Z∣2.
Such autoencoders are a fairly standard setup that have been used in previous work (Arpit et al.,
2015; Baldi, 2012; Kuchaiev & Ginsburg, 2017; Vincent et al., 2010). There have been relatively
5
Under review as a conference paper at ICLR 2019
fewer comparisons of ADAM and RMSProp with other methods on a regression setting. We were
motivated by Rangamani et al. (2017) who had undertaken a theoretical analysis of autoencoders
and in their experiments had found RMSProp to have good reconstruction error for MNIST when
used on even just 2 layer ReLU autoencoders.
To keep our experiments as controlled as possible, we make all layers in a network have the same
width (which we denote as h). Thus, given a size d for the input image, the weight matrices (as
defined above) are given by: W1 ∈ Rh×d, Wi ∈ Rh×h , i = 2, . . . , `. This allowed us to study the
effect of increasing depth ` or width h without having to deal with added confounding factors. For
all experiments, we use the standard “Glorot initialization” for the weights (Glorot & Bengio, 2010),
where each element in the weight matrix is initialized by sampling from a uniform distribution with
[-limit, limit], limit = ,6∕(fanin + fan°ut), where fanm denotes the number of input units in the
weight matrix, and fanout denotes the number of output units in the weight matrix. All bias vectors
were initialized to zero. No regularization was used.
We performed autoencoder experiments on the MNIST dataset for various network sizes (i.e., dif-
ferent values of ` and h). We implemented all experiments using TensorFlow (Abadi et al., 2016)
using an NVIDIA GeForce GTX 1080 Ti graphics card. We compared the performance of ADAM
and RMSProp with Nesterov’s Accelerated Gradient (NAG). All experiments were run for 105 it-
erations. We tune over the hyper-parameters for each optimization algorithm using a grid search
as described in the Appendix (Section B). To pick the best set of hyper-parameters, we choose the
ones corresponding to the lowest loss on the training set at the end of 105 iterations. Further, to cut
down on the computation time so that we can test a number of different neural net architectures, we
crop the MNIST image from 28 × 28 down to a 22 × 22 image by removing 3 pixels from each side
(almost all of which is whitespace).
Full-batch experiments We are interested in first comparing these algorithms in the full-batch
setting. To do this in a computationally feasible way, we consider a subset of the MNIST dataset
(we call this: mini-MNIST), which we build by extracting the first 5500 images in the training
set and first 1000 images in the test set in MNIST. Thus, the training and testing datasets in mini-
MNIST is 10% of the size of the MNIST dataset. Thus the training set in mini-MNIST contains
5500 images, while the test set contains 1000 images. This subset of the dataset is a fairly reasonable
approximation of the full MNIST dataset (i.e., contains roughly the same distribution of labels as in
the full MNIST dataset), and thus a legitimate dataset to optimize on.
Mini-batch experiments To test if our conclusions on the full-batch case extend to the mini-batch
case, we then perform the same experiments in a mini-batch setup where we fix the mini-batch size
at 100. For the mini-batch experiment, we consider the full training set of MNIST, instead of the
mini-MNIST dataset considered for the full-batch experiments and we also test on CIFAR-10 using
VGG-9, a convolutional neural network.
5 Experimental Results
5.1 RMSPROP AND ADAM ARE SENSITIVE TO CHOICE OF ξ
The ξ parameter is a feature of the default implementations of RMSProp and ADAM such as in
TensorFlow. Most interestingly this strictly positive parameter is crucial for our proofs. In this
section we present experimental evidence that attempts to clarify that this isn’t merely a theoretical
artefact but its value indeed has visible effect on the behaviours of these algorithms. We see in Figure
1 that on increasing the value of this fixed shift parameter ξ, ADAM in particular, is strongly helped
towards getting lower gradient norms and lower test losses though it can hurt its ability to get lower
training losses. The plots are shown for optimally tuned values for the other hyper-parameters.
5.2 TRACKING λmin (HESSIAN) OF THE LOSS FUNCTION
To check whether NAG, ADAM or RMSProp is capable of consistently moving from a “bad” sad-
dle point to a “good” saddle point region, we track the most negative eigenvalue of the Hessian
λmin(Hessian). Even for a very small neural network with around 105 parameters, it is still in-
tractable to store the full Hessian matrix in memory to compute the eigenvalues. Instead, we use the
6
Under review as a conference paper at ICLR 2019
Figure 1: Optimally tuned parameters for different ξ values. 1 hidden layer network of 1000 nodes;
Left: Loss on training set; Middle: Loss on test set; Right: Gradient norm on training set
Scipy library function scipy.sparse.linalg.eigsh that can use a function that computes
the matrix-vector products to compute the eigenvalues of the matrix (Lehoucq et al., 1998). Thus,
for finding the eigenvalues of the Hessian, it is sufficient to be able to do Hessian-vector products.
This can be done exactly in a fairly efficient way (Townsend, 2008).
We display a representative plot in Figure 2 which shows that NAG in particular has a distinct
ability to gradually, but consistently, keep increasing the minimum eigenvalue of the Hessian while
continuing to decrease the gradient norm. However unlike as in deeper autoencoders in this case the
gradient norms are consistently bigger for NAG, compared to RMSProp and ADAM. In contrast,
RSMProp and ADAM quickly get to a high value of the minimum eigenvalue and a small gradient
norm, but somewhat stagnate there. In short, the trend looks better for NAG, but in actual numbers
RMSProp and ADAM do better.
Figure 2: Tracking the smallest eigenvalue of the Hessian on a 1 hidden layer network of size 300.
Left: Minimum Hessian eigenvalue. Right: Gradient norm on training set.
5.3	Comparing performance in the full-batch setting
In Figure 3, we show how the training loss, test loss and gradient norms vary through the iterations
for RMSProp, ADAM (at βι = 0.9 and 0.99) and NAG (at μ = 0.9 and 0.99) on a 3 hidden layer
autoencoder with 1000 nodes in each hidden layer trained on mini-MNIST. Appendix D.1 and D.2
have more such comparisons for various neural net architectures with varying depth and width and
input image sizes, where the following qualitative results also extend.
Conclusions from the full-batch experiments of training autoencoders on mini-MNIST:
•	Pushing β1 closer to 1 significantly helps ADAM in getting lower training and test losses
and at these values of β1, it has better performance on these metrics than all the other
algorithms. One sees cases like the one displayed in Figure 3 where ADAM at β1 = 0.9 was
getting comparable or slightly worse test and training errors than NAG. But once β1 gets
closer to 1, ADAM’s performance sharply improves and gets better than other algorithms.
•	Increasing momentum helps NAG get lower gradient norms though on larger nets it might
hurt its training or test performance. NAG does seem to get the lowest gradient norms
compared to the other algorithms, except for single hidden layer networks like in Figure 2.
7
Under review as a conference paper at ICLR 2019
Figure 3: Full-batch experiments on a 3 hidden layer network with 1000 nodes in each layer; Left:
Loss on training set; Middle: Loss on test set; Right: Gradient norm on training set
Figure 4: Mini-batch experiments on a network with 5 hidden layers of 1000 nodes each; Left: Loss
on training set; Middle: Loss on test set; Right: Gradient norm on training set
5.4	Corroborating the full-batch behaviors in the mini-batch setting
In Figure 4, we show how training loss, test loss and gradient norms vary when using mini-batches
of size 100, on a 5 hidden layer autoencoder with 1000 nodes in each hidden layer trained on the
full MNIST dataset. The same phenomenon as here has been demonstrated in more such mini-batch
comparisons on autoencoder architectures with varying depths and widths in Appendix D.3 and on
VGG-9 with CIFAR-10 in Appendix E.
Conclusions from the mini-batch experiments of training autoencoders on full MNIST dataset:
•	Mini-batching does seem to help NAG do better than ADAM on small nets. However, for
larger nets, the full-batch behavior continues, i.e., when ADAM’s momentum parameter β1
is pushed closer to 1, it gets better generalization (significantly lower test losses) than NAG
at any momentum tested.
•	In general, for all metrics (test loss, training loss and gradient norm reduction) both ADAM
as Wen as NAG seem to improve in performance When their momentum parameter (μ for
NAG and β1 for ADAM) is pushed closer to 1. This effect, which was present in the
full-batch setting, seems to get more pronounced here.
•	As in the full-batch experiments, NAG continues to have the best ability to reduce gradient
norms While for larger enough nets, ADAM at large momentum continues to have the best
training error.
6 Conclusion
To the best of our knoWledge, We present the first theoretical guarantees of convergence to criticality
for the immensely popular algorithms RMSProp and ADAM in their most commonly used setting
of optimizing a non-convex objective.
By our experiments, We have sought to shed light on the important topic of the interplay betWeen
adaptivity and momentum in training nets. By choosing to study textbook autoencoder architectures
8
Under review as a conference paper at ICLR 2019
where various parameters of the net can be changed controllably we highlight the following two
aspects that (a) the value of the gradient shifting hyperparameter ξ has a significant influence on
the performance of ADAM and RMSProp and (b) ADAM seems to perform particularly well (often
supersedes Nesterov accelerated gradient method) when its momentum parameter β1 is very close to
1. On VGG-9 with CIFAR-10 and for the task of training autoencoders on MNIST we have verified
these conclusions across different widths and depths of nets as well as in the full-batch and the
mini-batch setting (with large nets) and under compression of the input/out image size. Curiously
enough, this regime of β1 being close to 1 is currently not within the reach of our proof techniques
of showing convergence for ADAM. Our experiments give strong reasons to try to advance theory
in this direction in future work.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In OSDI, volume 16, pp. 265-283, 2016.
Devansh Arpit, Yingbo Zhou, Hung Ngo, and Venu Govindaraju. Why regularized auto-encoders
learn sparse representation? arXiv preprint arXiv:1505.05561, 2015.
Reza Babanezhad, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub Konecny, and Scott
Sallinen. Stop wasting my gradients: Practical svrg. arXiv preprint arXiv:1511.01942, 2015.
Parnia Bahar, Tamer Alkhouli, Jan-Thorsten Peter, Christopher Jan-Steffen Brix, and Hermann Ney.
Empirical investigation of optimization algorithms in neural machine translation. The Prague
Bulletin of Mathematical Linguistics, 108(1):13-25, 2017.
Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In Proceedings of ICML
Workshop on Unsupervised and Transfer Learning, pp. 37-49, 2012.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd:
compressed optimisation for non-convex problems. arXiv preprint arXiv:1802.04434, 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of adam-type
algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018.
Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein. Automated inference with adaptive
batches. In Artificial Intelligence and Statistics, pp. 1504-1513, 2017.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646-1654, 2014.
Michael Denkowski and Graham Neubig. Stronger baselines for trustable results in neural machine
translation. arXiv preprint arXiv:1706.09733, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Sebastien Gadat, Fabien Panloup, Sofiane Saadane, et al. Stochastic heavy ball. Electronic Journal
of Statistics, 12(1):461-529, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A
recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017.
9
Under review as a conference paper at ICLR 2019
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing Systems, pp. 315-323, 2013.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of ex-
isting momentum schemes for stochastic optimization. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=rJTutzbA-.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arxiv. org, 2014.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing
systems, pp. 3294-3302, 2015.
Oleksii Kuchaiev and Boris Ginsburg. Training deep autoencoders for collaborative filtering. arXiv
preprint arXiv:1708.01715, 2017.
Richard B Lehoucq, Danny C Sorensen, and Chao Yang. ARPACK users’ guide: solution of large-
scale eigenvalue problems with implicitly restarted Arnoldi methods, volume 6. Siam, 1998.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. arXiv preprint arXiv:1805.08114, 2018.
Nicolas Loizou and Peter Richtarik. Momentum and stochastic momentum for stochastic gradient,
newton, proximal point and subspace descent methods. arXiv preprint arXiv:1712.09677, 2017.
James Lucas, Richard Zemel, and Roger Grosse. Aggregated momentum: Stability through passive
damping. arXiv preprint arXiv:1804.00325, 2018.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Gabor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language
models. arXiv preprint arXiv:1707.05589, 2017.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2).
In Soviet Mathematics Doklady, volume 27, pp. 372-376, 1983.
Peter Ochs. Local convergence of the heavy-ball method and ipiano for non-convex optimization.
arXiv preprint arXiv:1606.09070, 2016.
Michael O’Neill and Stephen J Wright. Behavior of accelerated gradient methods near critical points
of nonconvex problems. arXiv preprint arXiv:1706.07993, 2017.
Genevieve B Orr and Todd K Leen. Momentum and optimal stochastic search. In Proceedings of
the 1993 Connectionist Models Summer School, pp. 351-357. Psychology Press, 1994.
Boris T Polyak. Introduction to optimization. translations series in mathematics and engineering.
Optimization Software, 1987.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Akshay Rangamani, Anirbit Mukherjee, Ashish Arora, Tejaswini Ganapathy, Amitabh Basu, Sang
Chin, and Trac D Tran. Critical points of an autoencoder can provably recover sparsely used
overcomplete dictionaries. arXiv preprint arXiv:1708.03735, 2017.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
10
Under review as a conference paper at ICLR 2019
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139-1147, 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine
learning. University of Toronto, Technical Report, 2012.
Jamie Townsend. A new trick for calculating Jacobian vector products. https://j-towns.
github.io/2017/06/12/A-new-trick.html, 2008. [Online; accessed 17-May-2018].
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. Journal of Machine Learning Research, 11(Dec):3371-3408, 2010.
Wim Wiegerinck, Andrzej Komoda, and Tom Heskes. Stochastic dynamics of learning with mo-
mentum in neural networks. Journal of Physics A: Mathematical and General, 27(13):4425,
1994.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4151-4161, 2017.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International Conference on Machine Learning, pp. 2048-2057, 2015.
Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016.
Kun Yuan, Bicheng Ying, and Ali H Sayed. On the influence of momentum acceleration on online
learning. Journal of Machine Learning Research, 17(192):1-66, 2016.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive meth-
ods for nonconvex optimization. In Advances in Neural Information Processing Systems, 2018.
SK Zavriev and FV Kostyuk. Heavy-ball method in nonconvex optimization problems. Computa-
tional Mathematics and Modeling, 4(4):336-341, 1993.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of
adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.
11
Under review as a conference paper at ICLR 2019
Appendix
A Proofs of convergence of (stochastic) RMSProp and ADAM
A.1 Proving stochastic RMSProp (Proof of Theorem 3.1)
Now we give the proof of Theorem 3.1.
Proof. We define σt := maxk=ι,..,t ∣∣Vfik(Xk)k and We solve the recursion for Vt as, Vt = (1 -
β2) Ptk=1 β2t-k(g2k + ξ). This lets us write the following bounds,
λmin(Vt- 2 ) ≥
≥
11
≥
maxi=1,..,d(Vt)i	maxi=1,..,d((1 - β2) Ptk=1 β2t-k(g2k + ξ1d)i)
1
P1 - β2 pσ2 + ξ
NoW We define, t := mink=1,..,t,i=1,..,d(Vfik (Xk))i2 and this lets us get the folloWing bounds,
1
mini=ι,..,d(Vz(VtK)
NoW We invoke the bounded gradient assumption about the fi functions and replace in the above
equation the eigenvalue bounds of the Pre-Conditioner by worst-case estimates μmaχ and μmin de-
fined as,
λmin (Vt 2 ) ≥
Using the L-smoothness of f between consecutive iterates Xt and Xt+1 we have,
f(xt+l) ≤ f(xt) + hVf (Xt), Xt+1 - Xti + L2∣∣Xt+1 - Xtk2
We note that the update step of stochastic RMSProP is xt+i = Xt - α(Vt)-2gt where gt is the
stochastic gradient at iterate xt. Let Ht = {X1, X2, .., Xt} be the set of random variables corre-
sponding to the first t iterates. The assumptions we have about the stochastic oracle give us the
following relations, E[gt] = Vf(xt) and E[∣gt∣2] ≤ σf2 . Now we can invoke these stochastic ora-
cle’s properties and take a conditional (on Ht) expectation over gt of the L-smoothness in equation
to get,
E[f(Xt+ι) | Ht] ≤ f(Xt) - αE [(Vf (Xt), (Vt)-2臣〉| Hti + aLE ["(% - 2臣『∣ Hti
≤ f(Xt)-aE [hVf(Xt),(%-1 gti | Hti + “maxa2LE	∣ 同
≤ f (Xt)- aE [hVf (Xt), (Vt) -2 gti | Hti + μ2naχ ° f	⑴
We now separately analyze the middle term in the RHS above in Lemma A.1 below and we get,
12
Under review as a conference paper at ICLR 2019
E[hVf(xt), (Vt)-2gti | Ht] ≥ μminkVf(xt)k2
We substitute the above into equation 1 and take expectations over Ht to get,
α2 σf2 L
E[f (xt+1) - f (Xt)] ≤ -αμminkvf (xt)k + μmaχ —%—
=⇒ E[kVf(xt)k2] ≤ -ɪE[f(xt) - f(xt+ι)] + ασfLμmax	(2)
αμmin	2	μmin
Doing the above replacements to upperbound the RHS of equation 2 and summing the inequation
over t = 1 to t = T and taking the average and replacing the LHS by a lowerbound of it, we get,
min E[kVf(xt)k2] ≤
t=1...T
T1— E[f (Xi) - f (XT +1)] + ασfLμmax
αT μmin	2 μmin
1
≤ -----
αT μmin
(f (Xi) - f(x*)) + ασfL -
2	μmin
Replacing into the RHS above the optimal choice of,
α
1	∕2(f(xι)- f(x*)) = 1	∕2ξ(1- β2)(f (Xi)- f(x*))
/TV	σf Lμmχ = √T∖∕	σfL
we get,
.	MHV7〃 *⑵/ Ds^^1 ^^、^^L	L^^Lσf μmax	1	S2Lσ2(σ2 + ξ)(f(Xi) — f (x*))
min,E[∣∣Vf(xt)k2] ≤ 2\ 下,—(f(Xi) - f(x*)) × Tμmax = √=∖ —f f*-----------------------------------
t=i …T	V T μmin	2 μmin T T V	(1	β2)ξ
Thus stochastic RMSProp with the above step-length is guaranteed is reach -criticality in number
of iterations given by, T ≤ 十 (2Lσf (σf：：,；(χξ)-"x*)))	□
Lemma A.1. At any time t, the following holds,
EKVf(Xt),匕-i/2gti | Ht] ≥ μmin kVf(xt)k2
Proof.
d
E hVf(xt), Vt-i/2gti | Ht =E XVif(xt)(Vt-i/2)ii(gt)i | Ht
i=i
d
=XVif(xt)E (Vt-i/2)ii(gt)i | Ht	(3)
i=i
Now we introduce some new variables to make the analysis easier to present. Let api := [Vfp(xt)]i
where p indexes the training data set, p ∈ {1, . . . , k}. (conditioned on Ht, apis are constants) This
implies, Vif (Xt) = ɪ Pp=i ap We recall that E [(g)] = Vif (Xt) where the expectation is taken
over the oracle call at the tth update step. Further our instantiation of the oracle is equivalent to
doing the uniformly at random sampling, (gj 〜{api}p=i,…,k.
Given that we have, Vt = diag(vt) with vt = (1 - β2 ) Ptk=i β2t-k(g2k + ξ1d) this im-
plies, (Vt-i/2 )ii
/	i, ,° where We have defined di
√(i-β2)(gt)2+di	i
(1 - β2 )ξ + (1 -
β2 ) Ptk-=ii β2t-k((gk)i2 + ξ). (conditioned on Ht, di is a constant) This leads to an explicit form
of the needed expectation over the tth-oracle call as,
13
Under review as a conference paper at ICLR 2019
E (Vt-1/2)ii (gt)i | Ht = E (Vt-1/2)ii (gt)i | Ht
E(gt)i^{api}p = 1,...,k
________(gt)_________ । H
p∕(l - β2)(gt)2 + di
k
1	api
k P=1 J(I - β2)api + di
Substituting the above (and the definition of the constants api) back into equation 3 we have,
dk	k
E hhVf (xt), VtT/2gti | Hti= X1 X api	1 X /	ap∖	:
L	」匕 kp P=I	八k P=1 J(l-β2)aPi + di
We define two vectors a” hi ∈ Rk s.t (ai)p = api and (hi)p = √. .1)心 十^
Substituting this, the above expression can be written as,
dd
E [hvf(Xt),Vtτ∕2gti | Hl= k2 X (a>1k) (h>ai) = k2 Xa> (1kh>) ai	⑷
k i=1	k i=1
Note that with this substitution, the RHS of the claimed lemma becomes,
d lk	2
μmin kVf (Xt)k2 = μmin^ (j ^Vpf (xt^|
d
μmin	>τ 1 '2
=-kτ Ua 1k)
i=1
d
=得 X a>1k 4i
i=1
Therefore our claim is proved if we show that for all i, 表a> (1kh>) ai -锣a>1k1>ai ≥ 0. This
can be simplified as,
kaJ (1kh>) ai - μmin启aJ1k 1>ai =启aJ ∣	(1k (hi - μmin1k ) ) ai
To further simplify, we define qi ∈ Rk, (qi)p = (hi)p -仙面口 = need to show,	二八、R1 2 工T -μmin. We therefore (1-β2)a2pi+di	mn
k2a> (1kq>) ai ≥ 0
We first bound di by recalling the definition of σf (from which it follows that ap2i ≤ σf2),
14
Under review as a conference paper at ICLR 2019
t-1
di	≤ (1 -	β2)	ξ + ^~^β2	k(σ2 +	ξ)	= (1 -	β2)	ξ +
_	k=1	_	-
β2 - β2-1 ( 2 + ξ)
1-β2	( f+ ξ1
≤ (1 - β2)ξ + (β2 - β2-1)ξ + (β2 - β2-1)σf = (1 - β2-1)ξ + (β2 - β2-1)σf
ʌ/(I - β2)αpi+ di ≤ ʌ/(I - β2)σ2+ (I - β2-1 )ξ + (β2- β2-1)σ2 = J(1 -
1	1	1
--⇒ - μmin +	/	== ≥ - μmin +	/
J(I-β2)αpi + di	J(I- β2 1)(σ∕ + ξ)
1
=⇒ -Mmin +---/	≥ 0
J(I- β2)α% + di
(5)
The inequality follows since β2 ∈ (0,1]
Putting this all together, we get,
(a>lk)(q>ai)
1
Σ apiaqi -μmin T /
p,q=1	_	VZ(I- β2)αpi + di.
—
Now our assumption that for all x, sign(V∕p(x)) = sign(V∕q(x)) for all p,q ∈ {1,..., k}
leads to the conclusion that the term apiaqi ≥ 0. And we had already shown in equation 5 that
-μmin + √(1-ft⅛i+di
proof.
≥ 0. Thus we have shown that (a> 1k )(q>a/ ≥ 0 and this finishes the
□
15
Under review as a conference paper at ICLR 2019
A.2 Proving deterministic RMSProp - the version with standard speed (Proof
of Theorem 3.2)
Proof. By the L-smoothness condition and the update rule in Algorithm 2 we have,
f(Xt+1) ≤ f (Xt)- αt"f (Xt), Vt-1 Vf(Xt)i + α2L2kVi-1 Vf(Xt)k2
=⇒ f (Xt+1)— f (Xt) ≤ αt (Lat IIVt-2 Vf(Xt)k2-hvf (Xt), Vt-2 Vf(Xt)i)	(6)
For 0 < δt2 <
1
we now show a strictly positive lowerbound on the following function,
2 h hVf (Xt), Vt-2 Vf (Xt))- δ IVf(Xt)I2!	⑺
l ∖	IVt-1 Vf(Xt)k2	)
We define σt := maxi=1,..,t kVf(Xi)k and we solve the recursion for vt as, vt = (1 -
β2 ) Ptk=1 β2t-k (g2k + ξ). This lets us write the following bounds,
hVf (Xt), Vt- 2 Vf(Xt )i ≥ λmin(V-1 )∣Vf (Xt)I2 ≥
IVf(Xt)I2
VZmaXi=ι,..,d(vt)i
IVf (Xt)I2
JmaXi=ι,..,d((1 - β2) Pk=I β2-k(gk + ξid)i)
≥	IVf(Xt)I2
P1 - βt Pσ +ξ
(8)
Now we define, t := mink=1,..,t,i=1,..,d(Vf (Xk))i2 and this lets us get the following sequence of
inequalities,
IVf 1 Vf(Xt)『≤ λ∖(Vt- 2 3Vf(Xt)B ≤ "iU ))2 ≤ (I-fMl)
(9)
So combining equations 9 and 8 into equation 7 and from the exit line in the loop we are assured
that IVf(Xt)I2 6= 0 and combining these we have,
2 - —δ2 IVf(Xt)I2 +JVf(Xt),Vt-2Vf(Xt))
L I	IVt-1 Vf(Xt)I2
- δt2 and rewrite
Now our definition of δt2 allows us to define a parameter 0 < βt
the above equation as,
2 --δ2 IVf(Xt)I2 +JVf(Xt), Vt-1 Vf(Xt))! ≥ 2(1- β2)(ξ + q)βt
L I	IVT2Vf(Xt)I2	) l	L
(10)
We can as well satisfy the conditions needed on the variables, βt and δt by choosing,
δ2 = L min
2 t=1,..
p-⅛w=1 PbI =:δ2
16
Under review as a conference paper at ICLR 2019
and
1	21	1
βt = min —,	,	— δ =-----,
t=1,.. P - β2 Pσ2 + ξ	2 Pσ + ξ
Then the worst-case lowerbound in equation 10 becomes,
2 — —δ2 kVf(xt)k2 +hVf(xt),Vt-1 Nf(Xty)! ≥ 2(1— β2)ξ × 1	1
LI	kVt-2Vf(xt)k2	L L L 2 pσ2 +ξ
This now allows us to see that a constant step length αt = α > 0 can be defined as,
α = j√β2)ξ and this is such that the above equation can be written as, LakV 2 Vf (Xt)II2 一
hVf (xt),V1~ 2 Vf(Xt)) ≤ -δ2 ∣∣Vf (xt) ∣2 . This when substituted back into equation 6 we have,
f(xt+ι) — f (xt) ≤ —δ2α ∣Vf (xt)k2 = -δ2α ∣Vf(xt)∣2
This gives us,
IVf (xt)k2 ≤ δ2α [f(xt)-f(xt+ι)]
T1
F EkVf(Xt)k2 ≤ δ2α[f(xι)- f(x*)i	(id
t=1
1
=⇒ tminτ kVf (xt)k ≤ Tδ2α[f(xi) — f(x*)]	(12)
Thus for any given e > 0, T satisfying, τδ2α [f (xι) 一 f (x*)] ≤ 心 is a sufficient condition to ensure
that the algorithm finds a point xresult := arg mint=1,,.T kVf(xt)k2 with kVf (xresult)k2 ≤ 2.
Thus we have shown that using a constant step length of
(1-β2)ξ
deterministic RMSProp can
find an e—critical point in T =* X "x1δ-f (x*)=* X
2Lg2+ξ)(f(xι)-f(x*)) ςtpnς
(i-β2)ξ	steps∙
α
□
17
Under review as a conference paper at ICLR 2019
A.3 Proving deterministic RMSProp - the version with no added shift (Proof
of Theorem 3.3)
Proof. From the L-smoothness condition on f we have between consecutive iterates of the above
algorithm,
f(xt+ι) ≤ f(xt) - αthVf(xt),V-2Vf(xt)i + 2α2∣M-2Vf(xt)∣∣2
- 1	1	Lα+	_ 1	C
=⇒ EfgtY 2Vf(Xt)i≤a(f(Xt)-f(xt+ι))+-TkVt2Vf(Xt)k2
(13)
(14)
Now the recursion
Then ∣∣Vt2∣∣	≥
1
for vt can be solved to get, vt
1
maxi∈Support(vt ) σ√(1-β2) Pk=ι β2-k
LHS of equation 13 we get,
maχi∈Support(vt ) ∙∖J(vt )i
1
maxi∈Support(vt) √(1-β2) Pk = I β2	(gk )i
Substituting this in a lowerbound on
the
σVZ(I - βt)
kVf(xt)k2 ≤ hVf(xt),
VJ2 Vf(Xt)i≤ at f (Xt)-f(xt+ι))+LF kVt- 2 Vf (χt)k
1
_1
_1
2
Summing the above we get,
T
X
t=1
1
kVf(xt)k2 ≤ XXɪ (f(xt)- f(xt+ι))+XX Lαtkv-1 Vf(Xt)k2	(15)
t=1 αt	t=1 2
Now We substitute at = √t and invoke the definition of b` and BU to write the first term on the
RHS of equation 15 as,
X 工 [f(xt) - f (xt+ι)] = f"1)+X (
t=1 αt	α t=1
f(xt+1)	f (xt+1)	f(xT+1)
αt+1
αt
αT+1
—
—
T
fx) - f(xT+1) + -Xf (xt+ι)(√+T - √t)
α	αT+1 α
αT+1
t=1
≤ B - BeL + B (√T+1 - 1)
α
α
α
Now we bound the second term in the RHS of equation 15 as follows. Lets first define a function
P(T) as follows, P(T) = PT=I atkVt-2 Vf(Xt)k2 and that gives us,
d g2	d
P (T) - P(T - 1) = ατJ2 ITi = ατ^2
d2
,i
(i-β2) PT=I βT-k gk“
αT
,i
(T-沟 i=1 PT=I βT-kgk
dα
:≤ (1 - β2)√T
TT
XX [P(t) - PJ1)] = P(T)- P(I) ≤ (Γ⅛ XX 不 ≤ 2τ⅛0- 2)
=⇒ P(T) ≤ P(1) + ^daTn(√T - 2)
2(1 - β2 )
18
Under review as a conference paper at ICLR 2019
So substituting the above two bounds back into the RHS of the above inequality 15and removing
the factor of，1 - βT < 1 from the numerator, we can define a point Xresult as follows,
1T
l∣Vf (Xresult)k2 := arg min ∣∣Vf (xt)k2 ≤ T EkVf(Xt)∣∣2
≤ σ BB - BlL + B(√T÷i-1)
Tα	α	α
L	da
+ 2 卜⑴+ 2T-西(√T - 2)∖)
Thus it follows that for T = O(2) the algorithm 2 is guaranteed to have found at least one point
Xresult SUchthat, ∣Vf (Xresult) k 2 ≤ a	□
A.4 Proving ADAM (Proof of Theorem 3.4)
Proof.
Let us assume to the contrary that kgt k > for all t = 1,2, 3  We will show that this
assumption will lead to a contradiction. By L-smoothness of the objective we have the following
relationship between the values at consecutive updates,
L2
f(Xt+ι) ≤ f(Xt) + Ef(Xt) Xt+ι — Xti + 2 ∣∣Xt+ι — Xtk
Substituting the update rule using a dummy step length ηt > 0 we have,
f (Xt+1) ≤ f (Xt) - ηthVf (Xt),(Vti + diag(ξ1d))	m" + Lil || (vi2 + diag(ξ1d))	mt
=⇒	f(χt+ι)- f (Xt)	≤	ηt	(-hgt,	(V2 + diag(ξ1d))	mti	+ Lntk匕2 + diag(ξ1d))	mt )
(16)
2
The RHS in equation 16 above is a quadratic in ηt with two roots: 0 and
hgt ,(Vt2 +diagKId))	mti
^2 .
L
2
+diag(ξ1d)	mt
So the quadratic’s minimum value is at the midpoint of this interval, which gives us a candidate
1	hgt ,(Vt2 +diagKId)) mti
tth-step length i.e ɑJ= := 1 ∙——ɪ-------------ɪ1------2 and the value of the quadratic at this point
(hgt,(Vt2 +diag(ξ1d)) mti)2
is 一 4 ∙----(―^ ---------J——h. That is with step lengths being this ɑ= we have the following
guarantee of decrease of function value between consecutive steps,
f(xt+ι) - f(Xt) ≤ -彳
2L
(hgt, (VI + diag(ξ1d)) Imti)2
+ diag(ξ1d)- mt
(17)
2
19
Under review as a conference paper at ICLR 2019
Now we separately lower bound the numerator and upper bound the denominator of the RHS above.
Upperbound on
-1
+ diag(ξ1d)	mt
We have, λmaχ ((匕2 + diag(ξ1d) I I ≤---------:--1——√== Further We note that the recursion of
ξ+mini=1..d
vt can be solved as, vt = (1 - β2) Ptk=1 β2t-kg2k. NoW We define, t := mink=1,..,t,i=1,..,d(g2k)i
and this gives us,
1
λmax((vi2 + diag(ξ1d))- )
ξ + Vz(I - β2 )Et
(18)
≤
We solve the recursion for mt to get, mt = (1 - β1) Ptk=1 β1t-kgk. Then by triangle inequality and
defining σt := maxi=ι,..,t ∣∣Vf(Xi)k Wehave, ∣∣mtk ≤ (1 - β1 )σ> Thus combining this estimate
of kmtk With equation 18 We have,
-1
+ diag(ξ1d)	mt
≤ (I - βt)σt	≤ (I - βt)σt
ξ + p∕et(1- β2)	ξ
(19)
Lowerbound on hgt,(Vti + diag(ξ1d))	m/
To analyze this We define the folloWing sequence of functions for each i = 0, 1, 2.., t
-1
Qi = hgt, (Vt2 + diag(ξ1d)	mi)
This gives us the folloWing on substituting the update rule for mt,
-1
Qi - β1Qi-1 = hgt, Vt2 + diag(ξ1d)	(mi — βιmi-ι))
-1
(I- βι)hgt, Vt2 + diag(ξ1d)	gii
At i = t We have,
Qt - β1Qt-1 ≥ (1 - βι) kgtk2 λmin((Vt1 + diag(ξ1d))一 )
Lets define, σt-1 := maxi=1,..,t-1 ∣Vf (Xi)∣ and this gives us for i ∈ {1, .., t - 1},
Qi- β1Qi-1 ≥ -(I- β1) Ilgtll σt-1λmax ((Vt2 + diag(ξ1d)))
We note the folloWing identity,
Qt -β1tQ0 = (Qt - β1Qt-1) + β1 (Qt-1 -β1Qt-2) +β12 (Qt-2 -β1Qt-3) + .. + β1t-1 (Q1 -β1Q0)
NoW We use the loWerbounds proven on Qi - β1Qi-1 for i ∈ {1, .., t - 1} and Qt - β1Qt-1 to
loWerbound the above sum as,
20
Under review as a conference paper at ICLR 2019
Qt- β1 Qo ≥ (1- βι) kgtk2 λmin((vt1 + diag(ξ1d))T)
-1 t-1
-(I- βI) l∣gtk σt-1λmax( (Vt2 + diag(ξId))	X X βj
j=1
≥ (1 - βl) kgtk2 λmin((Vt1 + diag(ξ1d))T)
-(βι - β1) kgtk σt-iλmaχ((Vt2 + diag(ξ1d))T)	(20)
We can evaluate the following Iowerbound, λmin ((Vt2 + diag(ξ1d)) [ ≥ ----------------1	1	==.
ξ+ maxi=1,..,d(vt)i
Next we remember that the recursion of vt can be solved as, vt = (1 - β2) Ptk=1 β2t-kg2k and we
define, σt := maxi=ι,..,t ∣∣Vf (xi)k to get,
λmin((Vt2 + diag(A))T)≥ ξ + ,(Let腌
(21)
Now we combine the above and equation 18 and the known value of Q0 = 0 (from definition and
initial conditions) to get from the equation 20,
Qt ≥ -(βι - βt) kgtk σt-ι7^^^/门	Qt、
ξ+	(1 -β2t)t
21
+ (	βl)kgtk ξ + p(1-β2 )σ2
≥ k ∣2 ( (I - βI)_(β - βt)σ
≥kgtk [ξ + σp1-β) - ξ kgtk
(22)
In the above inequalities we have set t = 0 and we have set, σt = σt-1 = σ. Now we examine the
following part of the lowerbound proven above,
(I- βI)	(β1 - βt)σ _ ξ kgtk (I- βI)- σ(β1 - βt)(ξ + σa/(1 - β2))
-----.	—...~r:---——---------------------.	---------
ξ + PQ-β2 )σ2	ξ kgtk	ξ kgtk(ξ + σP0-βU)
σ(β	βt)ξ (⅞⅛ff - 1)-σpT-^
@	1 ξ kgtk(ξ + σP(T-1∏)
(R	Rt、*gtk(1-βι)	1
“β) ( σ(βι-βt) - 1
ξ-
-1+
(1-βl)kgtk
(βl-β1 )σ
ξ kgtk(ξ + σ
Now we remember the assumption that we are working under i.e kgt k > . Also by definition
0 < βι < 1 and hence we have 0 < βι - βt < βι. This implies, ((—-；*' ›。-：16 > 1 where
the last inequality follows because of our choice of as stated in the theorem statement. This allows
us to define a constant, '(『"-1 := θι > 0 s.t (1-βl)1gtk - 1 〉 θι
β1 σ	1	(β1 -β11)σ	1
Similarly our definition ofξ allows us to define a constant θ2 > 0 to get,
-1 +
(1-βι)kgtk
(βι-β1 )σ
<卷=ξ-θ2
21
Under review as a conference paper at ICLR 2019
Putting the above back into the lowerbound for Qt in equation 22 we have,
Qt ≥kgtk2 (σ(βσ(ξ+F
(23)
Now we substitute the above and equation 19 into equation 17 to get,
f (xt+1) f(xt) ≤
1	(hgt, (VtI + diag(gId))	mti)2
-τ^ . -----------------------2—
∣∣(vt2 + diag(ξ1d))	mt
J____________QI____________
2L U (v1 + diag(ξ1d))	mt
ɪ kgtk4 ((β⅞⅜θ2 J
2L	((i-∕t)σ )2
kgtk4 ( (βι-β2)%2θ2	ʌ
2L l(ξ + σ)2(1 - β1 )2σ2J
(24)
((βι- β2)%2θ	∖
12L(ξ + σ)2(1 - β1 )2σ2)
∣∣Vf(xt)k4 ≤ [f(Xt)- f(xt+ι)]
=⇒ XXX ((2L(ξ+*σθ2) kVf(χt)k4 ≤ [f(X2) - f(XT+1)]
t=2	2L(ξ + σ) σ
=⇒ t=m,inτkVf (xt)k4 ≤ T 2Lι(-+⅞⅞⅞[f (χ2)- f (χ*)i
Observe that if,
T≥
2Lσ2(ξ + σ)2
2c4(βι- β2)2 θ2θ2
[f (Xz)- f (X*)]
≤
≤
≤
then the RHS of the inequality above is less than or equal to 4 and this would contradict the as-
sumption that kVf(Xt)k > for all t = 1, 2, .....
As a consequence we have proven the first part of the theorem which guarantees the existence of
positive step lengths, αt s.t ADAM finds an approximately critical point in finite time.
Now choose θ1 = 1 i.e f = 1-σ i.e β1 = ;+⅝σ =⇒ β1(1 - βI) = ⅛2σ(I - ∑⅛σ) = (e+222σ)2 .
This also gives a easier-to-read condition on ξ in terms of these parameters i.e ξ > σ. Now choose
ξ = 2σ i.e θ2 = σ and making these substitutions gives us,
T ≥ /18J	[f (X2) - f (X*)] ≥ 18L^ [f (X2) - f (X*)] ≥ 9LF [f (x2) - f (X*)]
2e4 (⅛)) σ2	8e6 f	≡
We substitute these choices in the step length found earlier to get,
22
Under review as a conference paper at ICLR 2019
hgt,(匕2 + diag^Id))	mti	1
q=L
+ diag(ξ1d)-1mt
Qt
+ diag(ξ1d))-1mt
2
〉1 kgtk2 (") kgtk2	4e
≥ L-((i-yσ)2	= L(1- βt)2 3(e + 2σ)2 :
In the theorem statement we choose to call as the final αt the lowerbound proven above. We check
below that this smaller value of αt still guarantees a decrease in the function value that is sufficient
for the statement of the theorem to hold.
A consistency check! Let us substitute the above final value of the step length αt =
kg ∣∣2 / σ2(βι-β2 A	2
L ~/(1-βt )σ∖ 2- = L(IFet)2 kgtk2 (嗅普) , the bound in equation 19 (with σt replaced by
) 1
σ), and the bound in equation 23 (at the chosen values of θ1 = 1 and θ2 = σ) in the original equation
17 to measure the decrease in the function value between consecutive steps,
f (xt+ι) -	f (xt)	≤	at	(-(gt,	(Vt2	+ diag(ξ1d))	m/	+ Lat	(Vti	+ diag(ξ1d))	mt
≤ αt (-Qt +-2^t (V? + diag(ξ1d)) mt )
< ξ	IlClI2 ((β1 - β2)MIl2 ∣∣2 (σ(β1- β2)θlθ2 ʌʌ
≤ LT-W kgtk (^ξ+πJLkgtk (	ξσ(ξ + σ)))
l L ( ξ	IlrIl2 ( (β1 - β2) A(1 - β)σ V
+ 2 UT-W kgtk b≡π)
The RHS above can be simplified to be shown to be equal to the RHS in equation 24 at the same
values of θ1 and θ2 as used above. And we remember that the bound on the running time was derived
from this equation 24.	□
23
Under review as a conference paper at ICLR 2019
B	Hyperparameter Tuning
Here we describe how we tune the hyper-parameters of each optimization algorithm. NAG has two
hyper-parameters, the step size α and the momentum μ. The main hyper-parameters for RMSProP
are the step size α, the decay parameter β2 and the perturbation ξ . ADAM, in addition to the ones in
RMSProp, also has a momentum parameter βι. We vary the step-sizes of ADAM in the conventional
way of at = αp1 - β2/(1 — βt).
For tuning the step size, we follow the same method used in Wilson et al. (2017). We start out with
a logarithmically-spaced grid of five step sizes. If the best performing parameter was at one of the
extremes of the grid, we tried new grid points so that the best performing parameters were at one of
the middle points in the grid. While it is computationally infeasible even with substantial resources
to follow a similarly rigorous tuning process for all other hyper-parameters, we do tune over them
somewhat as described below.
NAG The initial set of step sizes used for NAG were: {3e-3, 1e-3, 3e-4, 1e-4, 3e-5}. We tune
the momentum parameter over values μ ∈ {0.9,0.99}.
RMSProp The initial set of step sizes used were: {3e-4, 1e-4, 3e-5, 1e-5, 3e-6}. We tune
over β2 ∈ {0.9, 0.99}. We set the perturbation value ξ = 10-10, following the default values
in TensorFlow, except for the experiments in Section 5.1. In Section 5.1, we show the effect on
convergence and generalization properties of ADAM and RMSProp when changing this parameter
ξ.
Note that ADAM and RMSProp uses an accumulator for keeping track of decayed squared gradi-
ent vt . For ADAM this is recommended to be initialized at v0 = 0. However, we found in the
TensorFlow implementation of RMSProp that it sets v0 = 1d. Instead of using this version of the
algorithm, we used a modified version where we set v0 = 0. We typically found setting v0 = 0 to
lead to faster convergence in our experiments.
ADAM The initial set of step sizes used were: {3e-4, 1e-4, 3e-5, 1e-5, 3e-6}. For ADAM,
we tune over β1 values of {0.9, 0.99}. For ADAM, We set β2 = 0.999 for all our experiments as
is set as the default in TensorFlow. Unless otherwise specified we use for the perturbation value
ξ = 10-8 for ADAM, following the default values in TensorFlow.
Contrary to what is the often used values of β1 for ADAM (usually set to 0.9), we found that we
often got better results on the autoencoder problem when setting β1 = 0.99.
C EFFECT OF THE ξ PARAMETER ON ADAPTIVE GRADIENT ALGORITHMS
In Figure 5, we show the same effect of changing ξ as in Section 5.1 on a 1 hidden layer network
of 1000 nodes, while keeping all other hyper-parameters fixed (such as learning rate, β1 , β2). These
other hyper-parameter values were fixed at the best values of these parameters for the default values
of ξ, i.e., ξ = 10-10 for RMSProp and ξ = 10-8 for ADAM.
(a) Loss on training set
(b) Loss on test set
Figure 5: Fixed parameters with changing ξ values. 1 hidden layer network of 1000 nodes
(c) Gradient norm on training set
24
Under review as a conference paper at ICLR 2019
D Additional Experiments
D.1 ADDITIONAL FULL-BATCH EXPERIMENTS ON 22 × 22 SIZED IMAGES
In Figures 6, 7 and 8, we show training loss, test loss and gradient norm results for a variety of
additional network architectures. Across almost all network architectures, our main results remain
consistent. ADAM with β1 = 0.99 consistently reaches lower training loss values as well as better
generalization than NAG.
(a) 1 hidden layer; 1000 nodes
each	each
(b) 3 hidden layers; 1000 nodes (c) 5 hidden layers; 1000 nodes
(d) 3 hidden layers; 300 nodes
(e) 3 hidden layer; 3000 nodes
(f) 5 hidden layer; 300 nodes
Figure 6: Loss on training set; Input image size 22 × 22
(b) 3
(a) 1 hidden layer; 1000 nodes
hidden layers; 1000 nodes (c) 5 hidden layers; 1000 nodes
(d) 3 hidden layers; 300 nodes
(e) 3 hidden layer; 3000 nodes
(f) 5 hidden layer; 300 nodes
Figure 7: Loss on test set; Input image size 22 × 22
25
Under review as a conference paper at ICLR 2019
(a) 1 hidden layer; 1000 nodes
(b) 3 hidden layers; 1000 nodes (c) 5 hidden layers; 1000 nodes
each	each
(d) 3 hidden layers; 300 nodes
(e) 3 hidden layer; 3000 nodes
Figure 8: Norm of gradient on training set; Input image size 22 × 22
(f) 5 hidden layer; 300 nodes
26
Under review as a conference paper at ICLR 2019
D.2 Are the full-batch results consistent across different input dimensions?
To test whether our conclusions are consistent across different input dimensions, we do two experi-
ments where we resize the 22 × 22 MNIST image to 17 × 17 and to 12 × 12. Resizing is done using
TensorFlow's tf.image.resize_images method, which uses bilinear interpolation.
D.2. 1 INPUT IMAGES OF SIZE 17 × 17
Figure 9 shows results on input images of size 17 × 17 on a 3 layer network with 1000 hidden nodes
in each layer. Our main results extend to this input dimension, where we see ADAM with β1 = 0.99
both converging the fastest as well as generalizing the best, while NAG does better than ADAM with
β1 = 0.9.
(a) Training loss
(b) Test loss	(c) Gradient norm
Figure 9:	Full-batch experiments with input image size 17 × 17
D.2.2 INPUT IMAGES OF SIZE 12 × 12
Figure 10 shows results on input images of size 12 × 12 on a 3 layer network with 1000 hidden
nodes in each layer. Our main results extend to this input dimension as well. ADAM with β1 =
0.99 converges the fastest as well as generalizes the best, while NAG does better than ADAM with
β1 = 0.9.
(a) Training loss
(b) Test loss	(c) Gradient norm
Figure 10:	Full-batch experiments with input image size 12 × 12
27
Under review as a conference paper at ICLR 2019
D.3 ADDITIONAL MINI-BATCH EXPERIMENTS ON 22 × 22 SIZED IMAGES
In Figure 11, we present results on additional neural net architectures on mini-batches of size 100
with an input dimension of 22 × 22. We see that most of our full-batch results extend to the mini-
batch case.
(a) 1 hidden layer; 1000 nodes
(b) 3 hidden layers; 1000 nodes (c) 9 hidden layers; 1000 nodes
each	each
(d) 1 hidden layer; 1000 nodes
(e) 3 hidden layers; 1000 nodes (f) 9 hidden layers; 1000 nodes
(g) 1 hidden layer; 1000 nodes
each
each
(h) 3 hidden layers; 1000 nodes (i) 9 hidden layers; 1000 nodes
each	each
Figure 11:	Experiments on various networks with mini-batch size 100 on full MNIST dataset with
input image size 22 × 22. First row shows the loss on the full training set, middle row shows the
loss on the test set, and bottom row shows the norm of the gradient on the training set.
28
Under review as a conference paper at ICLR 2019
E Image Classification on Convolutional Neural Nets
(a) Training loss
Figure 12: Mini-batch image classification experiments with CIFAR-10 using VGG-9
(b) Test set accuracy
To test whether these results might qualitatively hold for other datasets and models, we train an
image classifier on CIFAR-10 (containing 10 classes) using VGG-like convolutional neural networks
(Simonyan & Zisserman, 2014). In particular, we train VGG-9 on CIFAR-10, which contains 7
convolutional layers and 2 fully connected layers, a total of 9 layers. The convolutional layers
contain 64, 64, 128, 128, 256, 256, 256 filters each of size 3 × 3, respectively. We use batch
normalization (Ioffe & Szegedy, 2015) and ReLU activations after each convolutional layer, and
the first fully connected layer. Table 1 contains more details of the VGG-9 architecture. We use
minibatches of size 100, and weight decay of 10-5. We use fixed step sizes, and all hyperparameters
were tuned as indicated in Section B.
We present results in Figure 12. As before, we see that this task is another example where tuning
the momentum parameter (β1 ) of ADAM helps. While attaining approximately the same loss value,
ADAM with β1 = 0.99 generalizes as good as NAG and better than when β1 = 0.9. Thus tuning
β1 of ADAM helped in closing the generalization gap with NAG.
Table 1: VGG-9 on CIFAR-10.
layer type	kernel size	input size	output size
Conv_1	-3×3^^	-^3 × 32 × 32	64 × 32 × 32
ConvN	3 × 3	64 × 32 × 32	64 × 32 × 32
Max Pooling	2 × 2	64 × 32 × 32	64 × 16 × 16
Conv_3	-3×3^^	64 × 16 × 16	128× 16× 16
Conv_4	3 × 3	128 × 16× 16	128× 16× 16
Max Pooling	2 × 2	128 × 16 × 16	128 × 8 × 8
Conv_5	-3×3^^	128 × 8 × 8	256 × 8 × 8
Conv_6	3 × 3	256 × 8 × 8	256 × 8 × 8
Conv_7	3 × 3	256 × 8 × 8	256 × 8 × 8
Max Pooling	2 × 2	256 × 8 × 8	256 × 4 × 4
Linear	-1×1	1 × 4096	1 × 256
Linear	1 × 1	1 × 256	1 × 10
29