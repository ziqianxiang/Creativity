Under review as a conference paper at ICLR 2019
Accelerating first order optimization
ALGORITHMS
Anonymous authors
Paper under double-blind review
Ab stract
There exist several stochastic optimization algorithms. However in most cases,
it is difficult to tell for a particular problem which will be the best optimizer to
choose as each of them are good. Thus, we present a simple and intuitive tech-
nique, when applied to first order optimization algorithms, is able to improve the
speed of convergence and reaches a better minimum for the loss function compa-
red to the original algorithms. The proposed solution modifies the update rule, ba-
sed on the variation of the direction of the gradient during training. We conducted
several tests with Adam and AMSGrad on two different datasets. The preliminary
results show that the proposed technique improves the performance of existing
optimization algorithms and works well in practice.
1	Introduction
There are several ways to improve learning in neural networks such as improving the architecture,
finding the optimal parameters, playing with the data representation, choosing the best optimization
algorithm etc. In this paper, we are interested in the gradient-descent-based method for optimizing
the learning process in neural network architectures. Optimization algorithms in machine learning
(especially in neural networks) aim at minimizing an objective function (generally called loss or cost
function), which is intuitively the difference between the predicted data and the expected values.
The minimization consists of finding the set of parameters (weights) of the architecture that give
best results in the targeted tasks such as classification, prediction or clustering. The problem of
finding a set of weights to minimize residuals in a feedfoward neural network is not a trivial one.
It is nonlinear and dynamic in that any change of one weight requires adjustment of many others
(Eberhart & Kennedy, 1995).
Several optimization algorithms exist in the literature. The majority of them are first order methods
(as they use first derivatives of the function to minimize), and are based on the gradient descent.
Gradient descent techniques, e.g. back-propagation of error, are used to find a matrix of weights
that meets the error criterion. Adam (Adaptive Moment estimation) is probably the most popular
optimization algorithm in the literature (Goodfellow et al., 2016; Isola et al., 2017; Xu et al., 2015).
However, it has been recently proved that Adam (even discounting the fact that its convergence proof
has some issues), is unable to converge to the optimal solution for a simple convex optimization
setting (Reddi et al., 2018). A more recent algorithm (AMSGrad) fixes the problems of Adam and
others stochastic optimization methods by endowing them with a long-term memory. Unfortunately,
Adam can still outperforms AMSGrad in some cases 1 as we will see in our experiments. This
suggests that, there is no a better optimizer than others as each can be good than others or vice
versa depending on the problem. Thus, we investigate a new technique that improve thee empirical
performance of any first order optimization algorithm, while preserving their property. We will refer
to the solution when applied to an existing algorithm A as AA for (Accelerated Algorithm). For
example, for AMSGrad and Adam, the modified versions will be AAMSGrad and AAdam. The
proposed solution improves the convergence of the original algorithm and finds a better minimum
faster. Our solution is based on the variation of the direction of the gradient with respect to the
previous. We conducted several tests on problems where the shape of the loss is simple (has a convex
form) like Logistic regression, but also with non trivial neural network architectures such as deep
1. https ://fdlm.github.io/post/amsgrad/
1
Under review as a conference paper at ICLR 2019
Convolutional Neural Networks. We used MNIST 2 and Movie Review Data 3 datasets to validate our
solution. Note that we limited our experiments to SGD, Adam and AMSGrad, but the solution can
be applied to others algorithms as well. The preliminary results suggests that adding the proposed
solution to the update rule of a given optimizer makes it performs better.
The rest of the paper is organized as follows : Section 2 presents some related work ; Section 3 des-
cribes the proposed solution and provides its theoretical analysis ; Section 4 presents the experiments
setup, the results and discussions about the performance assessment of the proposed solution and its
possible integration in other optimizers. The paper ends with some concluding remarks.
2	Related Work
Learning in neural networks is done by minimizing an error function. This function therefore mea-
sures the difference between the expected and the computed outputs on the complete sample. An
error close to 0 implies that the network correctly predicts the expected outputs of the data on which
it has learned. Minimization of the loss involves finding the values of a set of parameters (weights)
that minimizes the function (this minimum could be local or global). What makes the problem more
complex is that, the general shape of the loss function is very poorly understood. Logistic regression
(LR) and softmax (multiclass generalization of LR) do not belong to that class as they have well-
studied convex objectives (Rennie, 2005). However that is not the case for neural networks with
more than one layer where the shape of the loss is generally neither convex nor concave and thus
can admit several minima (Choromanska et al., 2015). The loss function is usually minimized using
some form of stochastic gradient descent (SGD) (Bottou, 2010), in which the gradient is evaluated
using the back-propagation procedure (LeCun et al., 1989). Gradient Descent is one of the most
popular algorithms to perform such a task and is the much more common way to optimize neural
networks (Ruder, 2016). To be used in back-propagation, the loss function must satisfy some pro-
perties such as being able to be written as an average, and not be dependent on any activation values
of a neural network besides the output values. There exist many loss functions but the quadratic or
mean squared error and the cross-entropy are the most commonly used in the domain.
2.1	How Gradient Descent Method Works ?
Let J(θ) be a function parameterized by a model’s parameters θ ∈ Rn, sufficiently differentiable
of which one seeks a minimum. The gradient method builds a sequence that should in principle
approach the minimum. For this, we start from any value x0 (a random value for example) and we
construct the recurrent sequence by :
θn+1 = θn — η ∙ Vθn J(θ)	(1)
where η is the learning rate. For adaptive method like Adam, the learning rate is variable for each
parameters. This method is ensured to converge, even if the input sample is not linearly separable,
to a minimum of the error function for a well-chosen learning rate. There exist several variants of
this method : there are first-order and second-order methods. While first order methods use first
derivatives of the function to minimize, second order methods make use of the estimation of the
Hessian matrix (second derivative matrix of the loss function with respect to its parameters) (Schaul
et al., 2013; LeCun et al., 1993). The latter determines the optimal learning rates (or step size)
to take for quadratic problems. While such approach provides additional information useful for
optimization, computing accurate second order derivatives is too computationally expensive for large
models and the value computed is usually a bad approximation of the Hessian. In this paper, we will
only focus on first order techniques which are the most popular in machine learning domain and are
most suited for large scale models.
Adam (Kingma & Ba, 2014) is a first-order-gradient based algorithm of stochastic objective func-
tions, based on adaptive estimates of lower-order moments. The first moment normalized by the
second moment gives the direction of the update. Adam updates are directly estimated using a run-
ning average of first and second moment of the gradient. It computes adaptive learning rates for
each parameter. In addition to storing an exponentially decaying average of past squared gradients
2. http ://yann.lecun.com/exdb/mnist/
3. http ://www.cs.cornell.edu/people/pabo/movie-review-data/
2
Under review as a conference paper at ICLR 2019
vt, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum.
It has two main components : a momentum component and an adaptive learning rate component.
It can be viewed as a combination of RMSprop (Hinton et al.)) and momentum techniques NAG
(Nesterov, 1983). Adam has a bias-correction feature which helps it slightly outperforms previous
adaptive learning rate methods towards the end of optimization as gradients become sparser (Ru-
der, 2016). To our knowledge, Adam is one of the latest state of the art optimization algorithms
being used by many practitioners of machine learning. There exist several techniques to improve
Adam such as fixing the weight decay (Loshchilov & Hutter, 2017), using the sign of the gradient
in distributing learning cases (Bernstein et al., 2018), switching from Adam to SGD (Keskar & So-
cher, 2017). However, it has been recently proved that Adam, beyond the fact that the convergence
proof has some mistakes, is unable to converge to the optimal solution for a simple convex opti-
mization setting (Reddi et al., 2018). A more recent algorithm (AMSGrad) fixes the problems of
Adam and others stochastic optimization methods by endowing them with a long-term memory. The
main difference between AMSGrad and ADAM is that, AMSGrad maintains the maximum of all
vt (exponentially decaying average of past squared gradients) until the present time step and uses
this maximum value for normalizing the running average of the gradient. AMSGrad either performs
similarly, or better, than Adam on some commonly used problems in machine learning. The update
rule is as follows :
θn+1
mn
vn
mn
Vn
β1 ∙ mn-1 + (I- βI) ∙ VθnJ(θ)
β2 ∙ Vn-1 + (1 - β2) ∙ Vθn J(θ)2
mn
1	- βn
max(Vn-L J -nβn ).
Recall that the aim of an optimizer is to finds parameters that minimize a function, knowing that we
don’t really have any knowledge on how the function looks like. If one knew the shape of the function
to minimize, it would be easy to take accurate steps that lead to the minimum. We don’t know how
the shape is but, each time we take a step (using any of the optimizers) we can know if we passed a
minimum by computing the product of the current and the past gradient, and check if it is negative.
This informs us on the curvature of the loss surface and as far as we know, it is the only accurate
information we have on the curvature of our loss function in real time. The proposed method exploits
this knowledge to improve the convergence of an optimizer. There exist several ways to improves
each optimizers such as using momentum techniques etc. However, none of them used the variation
of the direction of the gradient as an information to compute the next step. The proposing solution
is not specific to a particular optimizer as many solutions that have been proposed. It can be applied
even on an already proposed accelerated version of the algorithm which makes it easily adaptable
on every optimizers.
3 A method to accelerate first order optimization algorithms
3.1	Notation.
kxik2 denotes the l2-norm of ith row of x. The projection operation QF,A(x) for A ∈ S+d (the set
of all positive definite d * d matrices) is defined as arg minχ∈F IlA1/2(x - y) ∣∣ for y ∈ Rd. F has
bounded diameter D∞ if kx - yk∞ ≤ D∞ forall x, y ∈ F . All operations applied on vectors or
matrices are element-wise.
3.2	Intuition and Pseudo-code.
Here we present the intuition behind the proposed solution. See algorithm 1 for pseudo-code of our
proposed method applied to the generic adaptive method setup proposed by Reddi et al. (2018). For
Adam, φt(gι,…,gt) = (I- βι) Pi=I βt-igi and ψt(gι,…,gt) = (I- Ihjdiag Si=I βt-ig2)
3
Under review as a conference paper at ICLR 2019
Algorithm 1 Accelerated - Generic Adaptive Method Setup
Input: xι ∈ F, step size (αt > 0)T=ι, sequence of functions (φt, ψt)T=ι
t — 0
pgo J 0 (Initialize previous gradient).
repeat
gt = Nft(Xt)
Vt = ψt(gt)
if gt * Pgt < 0 then mt =夕t(gt)
else : mt =夕t(max(gt,pgt))
Xt+1 = Xt — αtmtVt-1/2
xt+1 = Qp,√Vt(Xt+1)
pgt+1 = gt
tJt+1
until t > T
To give an odd entrance to our method, lets take the famous example of the ball rolling down a hill.
If we consider that our objective is to bring that ball (parameters of our model) to a lowest elevation
of a road (cost function), what we do is to push the ball with a force equal to the max of the past (t-1)
and the current computed force. This is done by taking the maximum between the computed gradient
(taken by any optimizer) and the previous gradient. The ball will gain more speed as it continues to
go in the same direction and looses its current speed as soon as it passes over a minimum. Note that,
the previous gradient can be replace by tan ( arctan(pgt )+arctan(gt)) which is the angle between the
two gradients (sum of vectors). This will reduce the step taken if we consider the maximum and
will still converge faster but slower. Another solution could be to stop accelerating the ball and
let the original optimizer takes the full control of the rest, once the direction change for the first
time. This solution also works. We will only focus on the case where the gradient is replaced by
the maximum between its current value and the past value. Note that Vt do not change. The change
we make to the optimizer do not alterate its convergence since the quantity R which essentially
measures the change in the inverse of learning rate of the adaptive method with respect to time keep
its value (vtremainunchanged and the current gradient will not be more than the previous one).
We provided in the appendix, a theoretical proof that the method we are proposing do not change
the bound of the regret.
3.3	Convergence Analysis
We assume that ifwe are able to prove that modifying one optimizer with the method do not alter its
convergence, then the same applies for other optimizers as well. Thus, we analyze the convergence
of AAMSGrad using the online learning framework.
Theorem 1 Assume that F has bounded diameter D∞ and
k5ft(X)k∞ ≤ G∞
for all t ∈ [T] and X ∈ F. With αt = Ja, AAMSGrad achieves the following gurantee, for all T ≥
1:
RT ≤
D∞ √ XX (^-ι∕2
2α(1-βι) =(VT，i
+
-1/2
^ i 、
+	)
α2
D∞
2(1 — βι)
Td
Xt=1Xi=1
(2)
+
α Vz1 + Iog(T)
(I — βI)2(1 - Y)p(1 - β2)
d
X kg1:T,i k2
i=1
4
Under review as a conference paper at ICLR 2019
The proof of this bound is given in the appendix. The following result falls as an immediate corollary
of the above result :
Corollary 1.1 Let β1t = β1λt-1, λ ∈ (0, 1) in Theorem 1, then we have :
RT ≤
D∞ √T
2α(I - βI)
d
X(v-,i/2
i=1
VT/2
v2 i
+ j2~~)
α2
+	βιD∞ G∞
+ 2(1 - βι)(1- λ)2
α Vz1 + Iog(T)
(I - βι)2(1 - Y)p(1 - β2)
(3)
d
X kg1:T,i k2
i=1
The regret of AAMSGRAD can be bounded by O(G∞ √T). The term Pid=i ∣∣g±τ,i k can also be
bound by O(G∞√T) since Pd=IIlg±τ,i∣∣2 << dG∞√T (Kingma & Ba, 2014).
4	Experiments
In order to empirically evaluate the proposed solution, we investigated different models : Logistic
Regression and Convolutional Neural Networks (CNN). We used the same parameter initialization.
β1 was set to 0.9 and β2 was set to 0.999 as recommended for Adam (Kingma & Ba, 2014) and
the learning rate was set to 0.001 and 0.01. The algorithms were coded using Googles TensorFlow 4
API and the experiments were done using the built-in TensorFlow models, making only small edits
to the default settings.
4.1	Image Recognition
No pre-processing was applied on MNIST images. All optimizers were trained with a mini-batch
size of 100. All weights were initialized from values truncated using normal distribution with stan-
dard deviation 0.1. The biases values were initialized to 0.1. We used the softmax cross entropy as
the loss function.
Logistic regression has a well-studied convex objective, making it suitable for comparison of dif-
ferent optimizers without worrying about local minimum issues (Kingma & Ba, 2014). We imple-
mented the same model used for comparing Adam with other optimizers in the original paper on
Adam. In Figure 1 , we see that the accelerated versions outperform the original algorithms (SGD
and ADAM) from the beginning to the end of the training even when we change the learning rate.
Our intuition that the method is able to improve the convergence and finds a better minimum is
thus consistent. CNNs (LeCun et al., 1995) are neural networks with layers representing convolving
filters (Krizhevsky et al., 2012) applied to local features. CNNs are now used in almost every task
in machine learning (i.e. text classification (Tato et al., 2017), speech analysis (Abdel-Hamid et al.,
2014), etc.). Figures 2 shows the results of running a neural architecture with one convolutional
layer and one fully connected layer on MNIST data with the learning rate set to 1E-2. Again, the
accelerated versions outperform the originals ( Adam and AMSGrad). We can also note that, in
this example, Adam outperforms AAMSGrad which suggests that the performance of an optimizer
depends on the data (shape of the loss).
4.2	Movie Reviews Data
We experimentally evaluated our method on movie reviews data. We ran a convolutional model on
sentence polarity dataset v1.0 which consists of 5331 positive and 5331 negative processed sen-
tences / snippets. We used a slightly simplified version 5 of the CNN proposed by (Kim, 2014) on
the dataset. All the parameter setting of the models was left unchanged. The dimensionality of cha-
racter embedding was set to 128, the filter sizes was set to 3,4,5, the number of filters per filter size
4. https ://www.tensorflow.org/
5. https ://github.com/dennybritz/cnn-text-classification-tf
5
Under review as a conference paper at ICLR 2019
FIGURE 1 - Logistic regression training negative log likelihood on MNIST With learning rate =
1E-2 (first plot on the left) and learning rate = 1E-3 (second plot).
Figure 2 - Convolutional Neural Networl with on conv and one fully connected layer on MNIST.
Learning rate = 1E-2 on training step (first plot on the left) and testing step (second plot).
Figure 3 - Convolutional Neural Network on movie review data. Learning rate = 1E-2 (plot on
the left) and Learning rate = 1E-3 (seconf plot) on training step.
6
Under review as a conference paper at ICLR 2019
was set to 128, the dropout keep probability set to 0.5, the batch size was set to 64, and the number
of training epochs was set to 50 ( Results are presented in Figure 3. As we expected, AAdam consis-
tently outperforms Adam on both cases (lr=1E-2 and 1E-3) and thus converges faster. AAMSGrad
outperforms AMSGrad and finds a better minimum also on both cases.
5	Conclusion and Final Remarks
In this paper, we have presented a simple and intuitive method that modifies any optimizer update
rule to improve its convergence. There is no optimizer that works for all problems. Each optimizer
has its advantages and disadvantages. No one can tell in advance which will be the best choice. The
solution we are proposing is to speed up the convergence of any optimizer (as each is important)
based on the variation of the sign of the gradient. Instead of using the gradient as it is for computing
the next step, we used the maximum of the past and the current gradient to compute the step if both
has the same sign (the direction did not change) else we keep the current gradient. We conducted a
successful preliminary experiment with three well known optimizers (SGD, Adam and AMSGrad)
on different models using two datasets (MNIST and movie review data). In all our experiments, the
accelerated versions outperformed the originals. In worst cases, both have the same convergence
which suggests that the accelerated versions are at least as good as the originals. This work takes
those optimizers one step further, and improves their convergence without noticeably increasing
complexity. The only drawback of the proposed solution is that it takes a little bit more computation
than the standard approach (as it has to cumpute the if else instruction). The new update rule depends
only on the variation of the direction of the gradient, which means that it can be used in any other
optimizer for the same goal.
Acknowledgments
The authors would like to thank the anonymous referees for their valuable comments and helpful
suggestions. The work is supported by the GS501100001809Natural Sciences and Engineering Re-
search Council of Canadahttp ://www.nserc-crsng.gc.ca/indexeng.aspDiscoveryGrantP rogram.
References
Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and Dong Yu.
Convolutional neural networks for speech recognition. IEEE/ACM Transactions on audio, speech,
and language processing, 22(10) :1533-1545, 2014.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd :
compressed optimisation for non-convex problems. arXiv preprint arXiv :1802.04434, 2018.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192-204, 2015.
Russell Eberhart and James Kennedy. A new optimizer using particle swarm theory. In Micro
Machine and Human Science, 1995. MHS’95., Proceedings of the Sixth International Symposium
on, pp. 39-43. IEEE, 1995.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6a overview of mini-batch gradient
descent (2012). Coursera Lecture slides https ://class. coursera. org/neuralnets-2012-001/lecture.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. arXiv preprint, 2017.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv :1712.07628, 2017.
7
Under review as a conference paper at ICLR 2019
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint
arXiv :1408.5882, 2014.
Diederik P Kingma and Jimmy Ba. Adam : A method for stochastic optimization. arXiv preprint
arXiv :1412.6980, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
Iutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-
bard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4) :541-551, 1989.
Yann LeCun, Patrice Y Simard, and Barak Pearlmutter. Automatic learning rate maximization by
on-line estimation of the hessian’s eigenvectors. In Advances in neural information processing
systems, pp. 156-163, 1993.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series.
The handbook of brain theory and neural networks, 3361(10) :1995, 1995.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint
arXiv :1711.05101, 2017.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2).
In Soviet Mathematics Doklady, volume 27, pp. 372-376, 1983.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. 2018.
Jason DM Rennie. Regularized logistic regression is strictly convex. Unpublished manuscript. URL
people. csail. mit. edu/jrennie/writing/convexLR. pdf, 745, 2005.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv :1609.04747, 2016.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International Confe-
rence on Machine Learning, pp. 343-351, 2013.
Ange Tato, Roger Nkambou, Aude Dufresne, and Miriam H Beauchamp. Convolutional neural
network for automatic detection of sociomoral reasoning level. pp. 284-289, 2017.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell : Neural image caption generation with visual
attention. In International Conference on Machine Learning, pp. 2048-2057, 2015.
8
Under review as a conference paper at ICLR 2019
6	Appendix : Proof of the convergence of AAMs Grad
The proof presented below is along the lines of the Theorem 4 in Sashank J et al (ICLR-2018), which
provides a proof of convergence for AMSGrad.
The goal is to prove that, the regret of AAMSGrad is bounded by :
RT ≤
D∞	x(
2(1- βι) = (
-1/2	-1/2
VTL +也」)+
αT
α2
D∞ XX X
2(1-βι) t=1 ⅛
1/2
β1tvt,i
αt
α，1 + Iog(T)
(I -尸I)2(1 - Y)p(1 - β2)
d
X kg1:T,ik2
i=1
(4)
+
For each optimization method, we have :
xt+1 = xt- αU
(5)
Where α is the step size. Note that, the value of the update U is the gradient (or slope) of a line. For
example, this line is the slope of the tangent of the loss at xtin SGD.
AAMSGrad has 2 rules for the update which differ depending on whether the direction of the update
changes or not. The current step taken by AAMSGrad is :
(1)	xt+ι = ∏f √^(xt - atmt/v¼t) if sign(d) has changed.
(2)	xt+ι = ∏f √^(xt - atmχ/Wt otherwise	俗)
≤ ∏f √^(xt - max(mt/v¼t, mt-i/VZvt-I))
mx = βι ∙ mt-1 + (1 - βι) ∙ max(PθJ(θ), 5- J(θ))
All operations are element wise. When the direction of the current update changes from the past
one, the current update is the same as in AMSGrad (rule (1)). When the direction stays the same,
the update is the maximum between the update that AMSGrad would have taken and the previous
update it took (rule (2)). Thus the regret bound of AAMSGrad is :
RT ≤ max(RT (1), RT (2) )	(7)
Where RT (1) is the regret when we consider only the first update rule of AAMSGrad. Please note
that, the bound of RT (1) is similar to that of AMSGrad. RT (2) is the regret if we consider only
the second rule. The proof will consist of finding the bound for RT (2) . The second update rule of
AAMSGrad can be rewritten as follows :
xt+ι ≤ ∏F√-ξ (Xt — U) where U = max(αtVr1-1/2mt, αt-ι^Vt--1/2mt-i)	(8)
We will only focus on the case where U is at-ι Vt-1/2mt-i as for the case where U is atV-1/2mt,
it is AMSGrad.
We begin with the following observation :
Xt+2 ≤ ∏f√^(atV-1/2mt) = min IVtT/4(x -(Xt- atVt "2mt))卜	⑼
Using Lemma 4 of Sashank J et Al (2018) and along the lines of their proof of Theorem 4, we have
the following :
匕-"(xt+2 - x*)『≤ IwT/4(Xt- at匕-1/2mt - x*)『
≤	∣∣^Vt1/4(xt-	χ*)|| + a2 ∣∣Vt1/4mt|| -	2athβι,tmt-ι + (1- βι,t)gt),xt-	x*i
(10)
9
Under review as a conference paper at ICLR 2019
Re-arranging the equation 10, and applying Cauchy-Schwarz and Young’s inequality, we have :
hgt ,xt-x*i≤ 2αt(1 -β1t) JlVt I"'-*" T∣Vt ι∕4(xt+2 -x*"]
+
at
2(I- βIt)
∣∣VtT4 mt∣∣2
B∖tat
2(I- βIt)
^ -1/4	∣∣2 l β1t
匕 mt-1ll +2αt(1-βιt)
Vt1∕4(xt-x*)∣∣2
(11)
Knowing that a convex and differentiable function ft (the loss) can be lower bounded by a hyper-
plane at its tangent (Lemma 10.2 in Kingma & Ba, 2015), we have :
T
T
RT (2) =	ft(xt) - ft(x*) ≤	hgt,xt -x*i
T
≤X
t=1
t=1	t=1
∣∣VtT/4 (χt-χ* )∣∣2-∣∣VtT∕4(χt+2 -χ*)∣∣2
2at (I - ιβ1,t)	十
αt
2(1 - β1t)
VtT4mt∣∣2
+ J ∣VtT4mt-i∣∣2 +
β1t
2αt(1 - β1t)
VtT∕4(xt-x*)∣∣2
T
≤X
t=1
VtT/4 (Xt- χ* )∣∣ - ∣∣ VtT/4 (Xt+2 - x*)∣∣
2at(1 - β1,t)
+
β1t
2αt(1 - β1t)
VtT∕4(xt-x*)∣∣2
+
a √1 + log(T)
(I - βI)2(1 - Y)VZ(I - β2)
d
X	kg1:T,ik2
i=1
<	1 X
≤ 2(1 - βι) ⅛
KT∕4(xt-x*)∣∣2
αt
ι 1
+ 2(1 - βι)
T
X
t=3
VtT∕4(xt-x*)∣∣2
Vt-r(xt-χ*)∣∣2
—
αt
αt-2
T
+X
t=1
β1t
2αt (1 - β1t)
VtT∕4(χt-χ*)∣∣2 +
a √1 + log(T)
(I -eI)2 (I - Y)VZ(I -尸2)
/	1
≤ 2(1 - βι)
X
,2(x1,i - x*)2 + ¾1∕2(x2,i - x*)2) +
α1
ι 1
+ 2(1 - βι)
≤	D∞
≤ 2(1 - βι)
+
D∞2
2(1 - β1)
i=1
XX XX βιt(Xt,i - χ*)2v1，/2 +
t=1 i=1	at
d ^-1/2	^-1/2
X v1,i	+ v2,i	+
i=1 a1 a2
T d	1∕2
XX β1tvt,i +
t=1 i=1
D∞2
α2
1
2(1 - βι)
a √1 + log(T)
(I - βι)2(1 - Y)VZ(I -尸2)
d
X	kg1:T,ik2
i=1
Td
X X(xt,i -
t=3 i=1
d
X	kg1:T,i k2
i=1
*、2
xi )
-1/2	-1/2
H	vt-2,i
—
at at-2
αt
2(1 - β1)
Td
Xt=3Xi=1
a √1 + log(T)
-1/2	-1/2
vt,i	J,i
—
at	at-2
(I - βι)2(1 - Y)VZ(I -伪)
d
X	kg1:T,ik2
i=1
(12)
The third inequality is due to Lemma 2 in Sashank J et al. (2018). The fourth inequality uses the
v-1/2	v-1/2
fact that βι,t ≤ βι. The fifth inequality uses the property : ；-2； ≤ -t,i- and the last inequality
above uses the following definitions that simplify the notation : F has bounded diameter D∞ and gt
= 5ft(θt), kgtk2 ≤ G, kgtk∞ ≤ kGk∞forallt∈ [T]andX∈F.
Using the telescopic sum, we have the following regret bound :
10
Under review as a conference paper at ICLR 2019
RT (2) ≤
D∞
2(1- βι)
d
X(
i=1
V-1/
αT
VT/2	D2	Td
+	^K) it X
βιtV1,/2
αt
+
a，1 + Iog(T)
(I - βI)2(I - Y)P(I -尸2)
d
X kg1:T,ik2
i=1
(13)
VT/2
We can easily see that RT(-)≤ RT(2), because of the term -jOi- ≥ 0.
Thus, the regret bound of AAMSGrad is :
RT ≤ max(RT (1), RT(2)) = RT (2)
Which complete the proof.
(14)
11