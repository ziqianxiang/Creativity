Under review as a conference paper at ICLR 2019
DL2: Training and Querying Neural Networks
with Logic
Anonymous authors
Paper under double-blind review
Ab stract
We present DL2, a system for training and querying neural networks with logical
constraints. DL2 is more expressive than prior work and can capture a richer class
of constraints on inputs, outputs and internals of models. Using DL2, one can
declaratively specify domain knowledge to be enforced during training or pose
queries on the model with the goal of finding inputs that satisfy a given constraint.
DL2 works by translating logical constraints into a differentiable loss with desir-
able mathematical properties, then minimized with standard gradient-based meth-
ods. Our evaluation demonstrates that DL2 can express interesting constraints
beyond the reach of prior work, leading to improved prediction accuracy.
1	Introduction
With the success of neural networks across a wide range of important application domains, a key
challenge that has emerged is that of making neural networks more reliable. Promising directions
to address this challenge are incorporating constraints during training (Madry et al., 2017; Min-
ervini et al., 2017) and inspecting already trained networks by posing specific queries (Goodfellow
et al., 2014b; Pei et al., 2017; Xu et al., 2018)). While useful, these approaches are described and
hardcoded to particular kinds of constraints, making their application to other settings difficult.
Inspired by prior work (e.g., Cohen et al. (2017); Fu & Su (2016); Hu et al. (2016); Bach et al.
(2017)), we introduce a new method and system, called DL2 (acronym for Deep Learning with
Differentiable Logic), which can be used to: (i) query networks for inputs meeting constraints,
and (ii) train networks to meet logical specifications, all in a declarative fashion. Our constraint
language can express rich combinations of arithmetic comparisons over inputs, neurons and outputs
of neural networks using negations, conjunctions, and disjunctions. Thanks to its expressiveness,
DL2 enables users to enforce domain knowledge during training or interact with the network in
order to learn about its behavior via querying.
DL2 works by translating logical constraints into non-negative loss functions with two key proper-
ties: (P1) a value where the loss is zero is guaranteed to satisfy the constraints, and (P2) the resulting
loss is differentiable almost everywhere. Combined, these properties enable us to solve the problem
of querying or training with constraints by minimizing a loss with off-the-shelf optimizers.
Training with DL2 To make optimization tractable, we exclude constraints on inputs that capture
convex sets and include them as constraints to the optimization goal. We then optimize with pro-
jected gradient descent (PGD), shown successful for training with robustness constraints (Madry
et al., 2017). The expressiveness of DL2 along with tractable optimization through PGD enables us
to train with new, interesting constraints. For example, we can express constraints over probabilities
which are not explicitly computed by the network. Consider the following:
∀x.ppθeople(x) < ∨ ppθeople(x) > 1 -
This constraint, in the context of CIFAR-100, says that for any network input x (network is pa-
rameterized by θ), the probability of people (ppeople) is either very small or very large. However,
CIFAR-100 does not have the class people, and thus we define it as a function of other probabil-
ities, in particular: ppeople = pbaby + pboy + pgirl + pman + pwoman . We show that with a similar
constraint (but with 20 classes), DL2 increases the prediction accuracy of CIFAR-100 networks in
the semi-supervised setting, outperforming prior work whose expressiveness is more restricted.
1
Under review as a conference paper at ICLR 2019
DL2 can capture constraints arising in both, classification and regression tasks. For example,
GalaxyGAN (Schawinski et al., 2017), a generator of galaxy images, requires the network to respect
constraints imposed by the underlying physical systems, e.g., flux: the sum of input pixels should
equal the sum of output pixels. Instead of hardcoding such a constraint into the network in an ad
hoc way, with DL2, this can now be expressed declaratively: sum(x) = sum(GalaxyGAN(x)).
Global training A prominent feature of DL2 is its ability to train with constraints that place re-
strictions on inputs outside the training set. Prior work on training with constraints (e.g., Xu et al.
(2018)) focus on the given training set to locally train the network to meet the constraints. With
DL2, we can, for the first time, query for inputs which are outside the training set, and use them to
globally train the network. Previous methods that trained on examples outside the training set were
either tailored to a specific task (Madry et al., 2017) or types of networks (Minervini et al., 2017).
Our approach splits the task of global training between: (i) the optimizer, which trains the network to
meet the constraints for the given inputs, and (ii) the oracle, which provides the optimizer with new
inputs that aim to violate the constraints. To illustrate, consider the following Lipshcitz condition:
∀z1 ∈ L∞(x1,e), z2 ∈ L∞(x2,e).∣∣pθ(z1) -pθ(z2)∣∣2 < L||z1 - z2∣∣2
Here, for two inputs from the training set (x1, x2), any point in their -neighborhood (z1, z2) must
satisfy the condition. This constraint is inspired by recent works (e.g., Gouk et al. (2018); Balan et al.
(2017)) which showed that neural networks are more stable if satisfying the Lipschitz condition.
Querying with DL2 We also designed an SQL-like language which enables users to interact
with the model by posing declarative queries. For example, consider the scenarios studied by a
recent work (Song et al., 2018) where authors show how to generate adversarial examples with AC-
GANs (Odena et al., 2016). The generator is used to create images from a certain class (e.g., 1)
which fools a classifier (to classify as, e.g., 7). With DL2, this can be phrased as:
find n[100]
where n in [-1, 1],
class (M_NN1(M_ACGAN_G(n, 1))) = 7
return M_ACGAN_G(n, 1)
This query aims to find an input n ∈ R100 to the generator satisfying two constraints: its entries are
between -1 and 1 (enforcing a domain constraint) and it results in the generator producing an image,
which it believes to be classified as 1 (enforced by M_ACGAN_G(n, 1)) but is classified by the
network (M_NN1) as 7. DL2 automatically translates this query to a DL2 loss and optimizes it with
an off-the-shelf optimizer (L-BFGS-B) to find solutions, in this case, the image to the right. Our
language can naturally capture many prior works at the declarative level, including finding neurons
responsible for a given prediction (Olah et al., 2018), inputs that differentiate two networks (Pei
et al., 2017), and adversarial example generation (e.g., Szegedy et al. (2013)).
Main Contributions The DL2 system is based on the following contributions:
•	An approach for training and querying neural networks with logical constraints based on
translating these into a differentiable loss with desirable properties ((P1) and (P2)).
•	A training procedure which extracts constraints on inputs that capture convex sets and
includes them as PGD constraints, making optimization tractable.
•	A declarative language for posing queries over neural network’s inputs, outputs, and inter-
nal neurons. Queries are compiled into a differentiable loss and optimized with L-BFGS-B.
•	An extensive evaluation demonstrating the effectiveness of DL2 in querying and training
neural networks. Among other experimental results, we show for the first time, the ability
to successfully train networks with constraints on inputs not in the training set.
2 Related Work
Adversarial example generation (Pei et al., 2017; Goodfellow et al., 2014b) can be seen as a fixed
query to the network, while adversarial training (Madry et al., 2017) aims to enforce a specific con-
straint. Most works aiming to train networks with logic impose soft constraints, often through an
additional loss (Pathak et al., 2015; XU et al., 2018); (Marquez-Neila et al., 2017) shows that hard
constraints have no empirical advantage over soft constraints. Probabilistic Soft Logic (PSL) (Kim-
mig et al., 2012) translates logic into continuous functions over [0, 1]. As we show, PSL is not
2
Under review as a conference paper at ICLR 2019
amenable to gradient-based optimization as gradients may easily become zero. Hu et al. (2016)
builds on PSL and presents a teacher-student framework which distills rules into the training phase.
The idea is to formulate rule satisfaction as a convex problem with a closed-form solution. However,
this formulation is restricted to rules over random variables and cannot express rules over probabil-
ity distributions. In contrast, DL2 can express such constraints, e.g., p1 > p2 , which requires the
network probability for class 1 is greater than for 2. Also, the convexity and the closed-form solution
stem from the linearity of the rules in the network’s output, meaning that non-linear constraints (e.g.,
Lipschitz condition, expressible with DL2) are fundamentally beyond the reach of this method. The
work of Xu et al. (2018) is also restricted to constraints over random variables and is intractable for
complicated constraints. Fu & Su (2016) reduces the satisfiability of floating-point formulas into
numerical optimization, however, their loss is not differentiable and they do not support constraints
on distributions. Finally, unlike DL2, no prior work supports constraints for regression tasks.
3	From Logic to a Differentiable Loss
We now present our constraint language and show how to translate constraints into a differentiable
loss. To simplify presentation, we treat all tensors as vectors with matching dimensions.
Logical Language Our language consists of quantifier-free constraints which can be formed with
conjunction (∧), disjunction (∨) and negation (). Atomic constraints (literals) are comparisons ./
of terms (here ./ ∈ {=, 6=, ≤, <, ≥, >}). Comparisons are defined for scalars and applied element-
wise on vectors. A term t is: (i) A variable z or a constant c , representing real-valued vectors;
constants can be samples from the dataset. (ii) An expression over terms, including arithmetic
expressions or function applications f : Rm → Rn, for m, n ∈ Z+ . Functions can be defined
overvariables, constants, and network parameters θ1 , . . . , θl. Functions can be the application of
a network with parameters θ, the application of a specific neuron, or a computation over multiple
networks. The only assumption on functions is that they are differentiable (almost everywhere) in
the variables and network parameters. We write t(z1, . . . , zk, c1, . . . , cj, θ1, . . . θl) to emphasize
the variables, constants, and network parameters that t can be defined over (that is, t may refer to
only a subset of these symbols). We sometimes omit the constants and network parameters (which
are also constant) and abbreviate variables by W, i.e., We write t(z). Similarly, We write 夕(Z) to
denote a constraint defined over variables Z When variables are not important, we write 夕.
Translation into loss Given a formula 夕,we define the corresponding loss L(夕)recursively on the
structure of 夕.The obtained loss is non-negative: for any assignment X to the variables Z, we have
≥0
l(夕八X)∈ r— . Fuiihei, the IIanSIatiOn has two pιopeιtιes. (pi)any X for which the loss is zero
(L(夕)(x) = 0)isa satisfying assignment to 夕(denoted by Xi=夕)and (P2) the loss is differentiable
almost everywhere. This construction avoids pitfalls of other approaches (see Appendix B). We
next formally define the translation rules. Formula 夕 is parametrized by ξ > 0 which denotes
tolerance for strict inequality constraints. Since comparisons are applied element-wise (i.e., on
scalars), atomic constraints are transformed into a conjunction of scalar comparisons:
L(t1 ./ t2)	:= L V ti1 ./ ti2
i=1
The comparisons = and ≤ are translated based on a function d: R × R → R which is a continuous,
differentiable almost everywhere, distance function with d(xι, X2)≥0 and d(χ1,χ2)=0 什 xi =X2.
L(t1 = t2) := d(t1,t2);	L(t1 ≤ t2) := 1t,2 ∙ d(t1,t2)
Here, 1t1>t2 is an indicator function: it is 1 ift1 > t2, and0 otherwise. The function dis a parameter
of the translation and in our implementation we use the absolute distance it1 - t2 i. For the other
comparisons, we define the loss as follows: L(t1 < t2 ) = L(t1 + ξ ≤ t2 ), L(t1 6= t2) = L(t1 <
t2 ∨ t2 < t1), and L(t1 > t2) and L(t1 ≥ t2) are defined analogously.
Conjunctions and disjunctions of formulas 夕 and ψ are translated into loss as follows.
L(夕 ∧ ψ) := LQ) + L(ψ);	L(夕 ∨ ψ) := L(2)∙L(ψ)
Note that L(夕 ∧ ψ) = 0 if and only if L(夕)=0 and L(ψ) = 0, which by construction is true if 夕
and ψ are satisfied, and similarly L(夕 ∨ ψ) = 0 if and only if L(夕)=0 or L(ψ) = 0.
3
Under review as a conference paper at ICLR 2019
Translating negations Negations are handled by first eliminating them from the constraint through
rewrite rules, and then computing the loss of their equivalent, negation-free constraint. Negations of
atomic constraints are rewritten to an equivalent atomic constraint that has no negation (note that 6=
is not a negation). For example, the constraint -(t1 ≤ t2) is rewritten to t2 < t1, while negations
of conjunctions and disjunctions are rewritten by repeatedly applying De Morgan’s laws: (ψ ∧ ψ)
is equivalent to -夕 ∨ -ψ and -(夕 ∨ ψ) is equivalent to -夕 ∧ -ψ.
With our construction, we get the following theorem:
Theorem 1 For all X, if L(夕)(x) = 0, then X satisfies 夕(Xl=夕).Conversely, for any 夕,there is a
δ(ξ) ≥ 0 with limξ→o δ(ξ) = 0 such that for all X, L(夕)(x) > δ(ξ) implies that X does not satisfy
夕(X=2).
Essentially, as we make ξ smaller (and δ approaches 0), we get closer to an if and only if theorem:
if X makes the loss 0, then We have a satisfying assignment; otherwise, if X makes the loss > δ, then
X is not a satisfying assignment. We provide the proof of the theorem in Appendix A.
4	Constrained Neural Networks
In this section, we present our method for training neural networks with constraints. We first define
the problem, then provide our min-max formulation, and finally, discuss how we solve the problem.
We write [φ] to denote the indicator function that is 1 if the predicate holds and 0 otherwise.
Training with constraints To train with a single constraint, we consider the following maximiza-
tion problem over neural network weights θ:
arg max Esi	Sm~d ∀Zz. φ(z, S), c, θ)].
θ	,,
Here, S1,..., Sm (abbreviated by S) are independently drawn from an underlying distribution D
and 夕 is a constraint over variables z, constants S and c, and network weights θ. This objective
is bounded between 0 and 1, and attains 1 if and only if the probability the network satisfies the
constraint 夕 is 1. We extend this definition to multiple constraints, by forming a convex combination
of their respective objectives: for w with Pit=1 wi = 1 and wi > 0 for all i, we consider
t
arg max	Wi ∙ Esi,...,Sm~D [∀z.Pi(z, S, c, θ) .	(1)
θ	i=1
As standard, we train with the empirical objective where instead of the (unknown) distribution D,
we use the training set T to draw samples.
To use our system, the user specifies the constraints φι,...,φt along with their weights wι,...,wt.
In the following, to simplify the presentation, we assume that there is only one constraint.
Formulation as min-max optimization We can rephrase training networks with constraints as
minimizing the expectation of the maximal violation. The maximal violation is an assignment to
the variables zc which violates the constraint (if it exists). That is, it suffices to solve the problem
argmi□θ Esι..Sm^ψ (maxzi,…/七-φ(z, S, c, θ)). Assume that one can compute, for a given S
and θ, an optimal solution XS g for the inner maximization problem:
XS θ = argmax[—2(Z,S,C,θ)].	(2)
, z1 ,...,zk
Then, we can rephrase the optimization problem in terms of XS 请
argminEsι,...sm~τ[w(XS,θ, S, c, θ)].	(3)
The advantage of this formulation is that it splits the problem into two sub-problems and the overall
optimization can be seen as a game between an oracle (solving (2)) and an optimizer (solving (3)).
Solving the optimization problems We solve (2) and (3) by translating the logical constraints
into differentiable loss (as shown in Sec. 3). Inspired by Theorem 1, for the oracle (Eq. (2)), we
approximate the inner maximization by a minimization of the translated loss L(「夕)：
XS θ = arg min L(—夕)(Z,S,C,θ).	(4)
, z1 ,...,zk
4
Under review as a conference paper at ICLR 2019
Given Xs,θ from the oracle, We optimize the following loss using Adam (Kingma & Ba, 2014):
Esι,...,sm〜T (L(0(xs,θ,S",θ))∙	(5)
Constrained optimization In general, the loss in (4) can sometimes be difficult to optimize. To
illustrate, assume that the random samples are input-label pairs (x, y) and consider the constraint:
夕(Z, (x, y),θ) = ||x - zM∞ ≤ E----⇒ logit (Z)y > δ∙
Our translation of this constraint to a differentiable loss produces
L(-φ)(z, (x,y), θ) = max(0, ||x - z∣∣∞ - e) + max(0, logitθ(z)y - δ).
This function is difficult to minimize because the magnitude of the two terms is different. This
causes first-order methods to optimize only a single term in an overly greedy manner, as reported
by Carlini & Wagner (2017). However, some constraints have a closed-form analytical solution,
e.g., the minimization of max(0, ||x - z∣∣∞ - E) can be solved by projecting into the L∞ ball. To
leverage this, we identify logical constraints which restrict the variables Z to convex sets that have
an efficient algorithm for projection, e.g., line segments, L2, L∞ or L1 balls (Duchi et al., 2008).
Note that in general, projection to a convex set is a difficult problem. We exclude such constraints
from 夕 and add them as constraints of the optimization. We thus rewrite (4) as:
XS θ = arg min	L(-φ)(z, iS, C, θ),	(6)
,	z1∈Dι(S),...,zk ∈ Dk(S)
where the Di denote functions which map random samples to a convex set. To solve (6), we employ
Projected Gradient Descent (PGD) which was shown to have strong performance in the case of
adversarial training with L∞ balls (Madry et al., 2017).
Training procedure Algorithm 1 shows
our training procedure. We first form a
mini-batch of random samples from the
training set T. Then, the oracle finds a
solution for (4) using the formulation in
(6). This solution is given to the optimizer,
which solves (5). Note that if 夕 has no vari-
ables (k = 0), the oracle becomes trivial
and the loss is computed directly.
Algorithm 1: Training with constraints.
input : Training set T, network parameters θ,
and a constraint φ(z, S, c, θ)
for epoch = 1 to nepochs do
Sample mini-batch of m-tuples
S = S1 Sm 〜T
,...,	.
Using PGD, compute
X ≈ argminzi∈Dι(S),…,zk∈Dk(S) L(-φ)(z,s,c, θ),
Perform Adam update with VθL(φ)(x, S, c, θ).
5	Querying Networks
We build on DL2 and design a declarative language for querying networks. Interestingly, the hard-
coded questions investigated by prior work can now be phrased as DL2 queries: neurons responsible
for a prediction (Olah et al., 2018), inputs that differentiate networks (Pei et al., 2017), and adver-
sarial examples (e.g., Szegedy et al. (2013)). We support the following class of queries:
find z1(m1),..., zk(mk) where g(C) [init z1 = c1,..., Zk = ck] [return t(c)]
Here, find defines the variables and their shape in parentheses, where defines the constraint (over the
fragment described in Sec. 3), init defines initial values for (part or all of) the variables, and return
defines a target term to compute at the end of search; if missing, Z1, ∙ ∙ ∙ , Zk are returned. Networks
(loaded so to be used in the queries) and constants are defined outside the queries. We note that
the user can specify tensors in our language (we do not assume these are simplified to vectors). In
queries, we write comma (,) for conjunction (∧); in for box-constraints and class for constraining
the target label, which is interpreted asconstraints over the labels’ probabilities.
Examples Fig. 1 shows few interesting queries. The first two are defined over networks trained for
CIFAR-10, while the last is for MNIST. The goal of the first query is to find an adversarial example
i of shape (32, 32, 3), classified as a truck (class 9) where the distance of i to a given deer image
(deer) is between 6 and 24, with respect to the infinity norm. Fig. 1b is similar, but the goal is to
find i classified as a deer where a specific neuron is deactivated. The last query’s goal is to find i
classified differently by two networks where part of i is fixed to pixels of the image nine.
5
Under review as a conference paper at ICLR 2019
find i[32, 32, 3]
where i in [0, 255],
class (NN(i)) = 9,
ki - deerk∞ < 25,
ki - deerk∞ > 5
find i[32, 32, 3]
where i in [0, 255],
ki - deerk∞ < 25,
NN(i).l1 [0, 1, 1, 31]
class (NN(i)) = 4
find i[28, 28]
where i in [0, 1],
i[0:9,:] = nine[0:9,:],
= 0, class (NN1(i)) = 8,
class (NN2(i)) = 9
(a)	Adversarial example.
(b)	Neuron deactivated.
(c)	Diffing networks.
Figure 1: DL2 queries enable to declaratively search for inputs satisfying constraints over networks.
Solving queries As with training, we compile the constraints to a loss, but unlike training, we
optimize with L-BFGS-B. While training requires batches of inputs in PGD optimization, querying
looks for one assignment, and thus there is more time to employ the more sophisticated, but slower,
L-BFGS-B. We discuss further optimizations in Appendix C.
6	Experimental Evaluation
We now present a thorough experimental evaluation on the effectiveness of DL2 for querying and
training neural networks with logical constraints. Our system is implemented in PyTorch (Paszke
et al., 2017) and evaluated on an Nvidia GTX 1080 Ti and Intel Core i7-7700K with 4.20 GHz.
6.1	Training with DL2
We evaluated DL2 on various tasks (supervised, semi-supervised and unsupervised learning) across
four datasets: MNIST, FASHION (Xiao et al., 2017), CIFAR-10, and CIFAR-100 (Krizhevsky &
Hinton, 2009). In all experiments, one of the constraints was cross-entropy (see Sec. 4), to optimize
for high prediction accuracy. For each experiment, we describe additional logical constraints.
Supervised learning
We consider two types
of constraints for super-
vised learning: global
constraints, which have
z-s, and training set con-
straints, where the only
variables are from the
training set (no z-s). Note
that none of prior work ap-
plies to global constraints
in general. Furthermore,
because of limitations of
their encoding explained
in Sec. 2, they are not able
to handle complex training
MNIST	FASHION	CIFAR-10
Baseline DL2 Baseline DL2 Baseline DL2
RobustnessT	P	99.39	98.61
	C	96.10	97.30
RobustnessG	P	99.38	99.45
	C	35.02	92.18
LipschitzT	P C	99.48 07.20	97.95 99.20
LipschitzG	P C	99.44 00.00	99.24 99.90
C-similarityT	P C	- -	- -
C-similarityG	P C	- -	- -
SegmentG	P C	98.16 18.97	97.44 37.70
90.56	89.70	90.98	87.08
95.20	96.30	91.10	93.08
91.45	89.41	90.59	78.86
00.00	80.20	07.16	21.00
92.10	87.49	90.59	90.24
06.40	99.53	07.16	99.60
92.22	82.27	90.51	87.32
00.00	89.38	00.00	99.55
-	-	91.52	90.70
-	-	89.10	99.60
-	-	91.06	90.30
-	-	49.74	57.31
88.13	87.18	-	-
21.80	48.18	-	-
Figure 2: Supervised learning, P/C is prediction/constraint accuracy.
set constraints considered in our experiments (e.g., constraints between probability distributions).
To ease notation, we write random samples (the S-s) as xi and yi for inputs from the training set
(xi) and their corresponding label (yi).
For local robustness (Szegedy et al., 2013), the training set constraint says that if two inputs from
the dataset are close (their distance is less than a given 1, with respect to L2 norm), then the KL
divergence of their output probabilities is smaller than 2 :
||x1- X2∣∣2 < €1 =⇒ KL(pθ(x1)∣∣pθ(x2)) < €2
(RobustnessT)
Second, the global constraint requires that for any input x, whose classification is y, inputs in its
€ neighborhood which are valid images (pixels are between 0 and 1), have a high probability for
y . For numerical stability, instead of the probability we check that the corresponding logit is larger
than a given threshold δ :
∀z ∈ L∞ (x, €) ∩ [0, 1]d. logitθ (z)y > δ	(RobustnessG)
6
Under review as a conference paper at ICLR 2019
Similarly, we have two definitions for the Lipschitz condition. The training set constraint requires
that for every two inputs from the training set, the distance between their output probabilities is less
than the Lipschitz constant (L) times the distance between the inputs:
∣∣pθ(x1) - pθ(x2)∣∣2 < L||x1 - x2∣∣2	(LiPschitzT)
The global constraint poses the same constraint for valid images in the neighborhood x1 and x2 :
∀z1 ∈ L∞(x1 ,e) ∩ [0,1]d, z2 ∈ L∞(x2,e) ∩ [0,1]d∙∣∣pθ (z1) - pθ (z2)∣∣2 < L||z1 - Z ∣∣2
(LiPschitzG)
We also consider a training set constraint called C-similarity, which imPoses domain knowledge
constraints for CIFAR-10 networks. The constraint requires that inPuts classified as a car have a
higher Probability for the label truck than the Probability for dog:
y = car =⇒ pθ(x)truck > pθ(x)dog + δ	(C-similarityT)
The global constraint is similar but aPPlied for valid images in the -neighborhood of x:
∀z ∈ L∞(x, ) ∩ [0, 1]d.y = car =⇒ pθ(z)truck > pθ(z)dog + δ	(C-similarityG)
Finally, we consider a Segment constraint which requires that if an inPut z is on the line between two
inPuts x1 and x2 in Position λ, then its outPut Probabilities are on Position λ on the line between
the outPut Probabilities:
∀z. Z = λ ∙ x1 + (1 - λ) ∙ x2 =⇒ -λ ∙ logitθ(z)yi - (1 - λ) ∙ logitθ(z)y2 < δ (SegmentG)
Fig. 2 shows the Prediction accuracy (P) and the constraint accuracy (C) when training with
(i) crossed-entroPy only (CE) and (ii) CE and the constraint. Results indicate that DL2 can signifi-
cantly imProve constraint accuracy (0% to 99% for LiPschitzG), while Prediction accuracy slightly
decreases. The decrease is exPected in light of a recent work (TsiPras et al. (2018)), which shows
that adversarial robustness comes with decrease of Prediction accuracy. Since adversarial robustness
is a tyPe of DL2 constraint, we susPect that we observe a similar Phenomenon here.
Semi-supervised learning For semi-suPervised learning,
we focus on the CIFAR-100 dataset, and sPlit the training set
into labeled, unlabeled and validation set in ratio of 20/60/20.
In the sPirit of the exPeriments of Xu et al. (2018), we con-
sider the constraint which requires that the Probabilities of
groups of classes have either very high Probability or very
low Probability. A grouP consists of classes of a similar tyPe
(e.g., the classes baby, boy, girl, man, and woman are Part of
the people grouP), and the grouP’s Probability is the sum of
Method	Accuracy (%)
Baseline	47.03
Semantic loss	51.82
Rule distillation	45.56
DL2	53.48
Figure 3: Semi-suPervised training.
its classes’ Probabilities. Formally, our constraint consists of 20 grouPs and its structure is:
(ppeople < ∨ ppeople > 1 - ) ∧ ... ∧
(pinsects <	∨ pinsects > 1 - )
for a small . We use this constraint to comPare the Performance of several aPProaches. For all
aPProaches, we use the Wide Residual Network (Zagoruyko & Komodakis (2016)) as the network
architecture. As a baseline, we train in a Purely-suPervised fashion, without using the unlabeled data.
We also comPare to semantic loss (Xu et al., 2018) and rule distillation (Hu et al., 2016). Note that
this constraint is restricting the Probability distribution and not samPles drawn from it which makes
other methods inaPPlicable (as shown in Sec. 2). As these methods cannot encode our constraint,
we rePlace them with a closest aPProximation (e.g., the exactly-one constraint from Xu et al. (2018)
for semantic loss). Details are shown in APPendix D. Fig. 3 shows the Prediction accuracy on the
test set for all aPProaches. Results indicate that our aPProach outPerforms all existing works.
Unsupervised learning We also consider a regression task
in an unsuPervised setting, namely training MLP (Multilayer
PercePtron) to Predict the minimum distance from a source to
every node in an unweighted graPh, G = (V, E). One can no-
tice that minimum distance is a function with certain ProPer-
ties (e.g., triangle inequality) which form a logical constraint
listed below. Source is denoted as 0.
APProach	MSE
Supervised (regression)	0.0516
UnsuPervised (baseline)	0.4938
UnsuPervised (with DL2)	0.0998
Figure 4: UnsuPervised training.
∀v∈G,d(v) ≥ 0 ∧ (∨(v,v0)∈E(d(v) = d(v0) + 1)) ∧ (∧(v,v0)∈E(d(v) 6 d(v0) + 1))
7
Under review as a conference paper at ICLR 2019
Nr.	MNIST			FASHION			CIFAR-10			GTSRB			ImageNet		
	#✓		©✓	#✓		©✓	#✓		©✓	#✓		©✓	#✓		怎/
1	10	0.4	0.4	10	0.4	0.4	10	0.4	0.4	10	0.6	0.6	10	1.6	1.6
2	10	1.0	1.0	10	1.00	1.00	10	1.2	1.2	10	3.0	3.0	10	80.7	80.7
3	10	1.0	1.0	10	0.9	0.9	10	3.4	3.4	10	3.1	3.1	10	81.0	81.0
4	10	1.5	1.5	10	1.6	1.6	9	9.8	1.9	0	120.0	0.0	10	94.4	94.4
5	10	1.0	1.0	10	1.0	1.0	8	16.5	1.1	10	3.2	3.2	10	80.2	80.2
6	9	15.7	4.1	8	25.5	1.9	9	5.7	1.7	8	27.4	4.2	9	81.4	77.2
7	10	1.0	1.0	10	1.0	1.0	10	1.1	1.1	10	3.0	3.0	10	74.0	74.0
8	6	48.6	1.0	6	51.7	6.2	9	5.9	1.0	9	15.8	4.2	10	78.4	78.4
9	10	1.4	1.4	10	1.5	1.5	8	10.5	1.7	0	120.0	0.0	10	86.6	86.6
10	10	1.8	1.8	10	1.7	1.7	10	2.6	2.6	0	120.0	0.0	10	92.2	92.2
11	6	50.0	3.3	7	42.0	8.7	7	35.0	17.1	0	120.0	0.0	8	96.6	90.7
12	10	2.0	2.0	10	2.0	2.0	9	28.8	18.7	10	6.2	6.2	0	120.0	0.0
13	5	63.5	7.1	7	39.3	4.7	7	30.2	7.6	9	21.9	11.0	0	120.0	0.0
14	0	120.0	0.0	0	120.0	0.0	7	68.21	46.01	-	-	-	-	-	-
15	3	79.08	71.27	9	35.02	25.58	7	66.02	42.88	-	-	-	-	-	-
16	1	108.2	2.0	1	108.2	2.0	8	34.4	13.1	4	75.6	9.0	0	120.0	0.0
17	10	2.9	2.9	10	3.2	3.2	5	61.6	4.0	0	120.0	0.0	0	120.0	0.0
18	10	4.0	4.0	10	4.0	4.0	7	50.6	24.9	0	120.0	0.0	0	120.0	0.0
Table 1: Results for queries: (#✓) number of completed instances (out of 10), @ is the average
running time in seconds, and ©✓the average running time of successful runs (in seconds).
Additionally, we constrain d(0) = 0. Next, we train the model in an unsupervised fashion with the
DL2 loss. In each experiment, we generate random graphs with 15 vertices and split the graphs into
training (300), validation (150) and test set (150). As an unsupervised baseline, we consider a model
which always predicts d(v) = 1. We also train a supervised model with the mean squared error
(MSE) loss. Remarkably, our approach was able to obtain an error very close to supervised model,
without using any labels at all. This confirms that loss generated by DL2 can be used to guide the
network to satisfy even very complex constraints with many nested conjunctions and disjunctions.
6.2	Querying with DL2
We evaluated DL2 on the task of querying with constraints, implemented in TensorFlow. We con-
sidered five image datasets, and for each, we considered at least two classifiers; for some we also
considered a generator and a discriminator (trained using GAN (Goodfellow et al., 2014a)). Table 3
(Appendix E) provides statistics on the networks. Our benchmark consists of 18 template queries
(Appendix E), which are instantiated with the different networks, classes, and images. Table 1 shows
the results (- denotes an inapplicable query). Queries ran with a timeout of 2 minutes. Results in-
dicate that our system often finds solutions. It is unknown whether queries for which it did not find
a solution even have a solution. We observe that the success of a query depends on the dataset.
For example, queries 9-11 are successful for all datasets but GTSBR. This may be attributed to
the robustness of GTSBR networks against the adversarial examples that these queries aim to find.
Query 14, which leverages a discriminator to find adversarial examples, is only successful for the
CIFAR dataset. A possible explanation can be that discriminators were trained against real images
or images created by a generator, and thus the discriminator performs poorly in classifying arbi-
trary images. Query 15, which leverages the generators, succeeds in all tested datasets, but has only
few successes in each. As for overall solving time, our results indicate that, successful executions
terminate relatively quickly and that our system scales well to large networks (e.g., for ImageNet).
7	Conclusion
We presented DL2, a system for training and querying neural networks. DL2 supports an expres-
sive logical fragment and provides translation rules into a differentiable (almost everywhere) loss,
which is zero only for inputs satisfying the constraints. To make training tractable, we handle input
constraints which capture convex sets through PGD. We also introduce a declarative language for
querying networks which uses the logic and the translated loss. Experimental results indicate that
DL2 is effective in both, training and querying neural networks.
8
Under review as a conference paper at ICLR 2019
References
Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. Hinge-loss markov random
fields and probabilistic soft logic. Journal of Machine Learning Research, 18:109:1-109:67,
2017. URL http://jmlr.org/papers/v18/15- 631.html.
Radu Balan, Maneesh Singh, and Dongmian Zou. Lipschitz properties for deep convolutional net-
works. CoRR, abs/1701.05217, 2017.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
William W. Cohen, Fan Yang, and Kathryn Mazaitis. Tensorlog: Deep learning meets probabilistic
dbs. CoRR, abs/1707.05390, 2017.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto
the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference
on Machine learning, pp. 272-279. ACM, 2008.
Zhoulai Fu and Zhendong Su. Xsat: a fast floating-point satisfiability solver. In International
Conference on Computer Aided Verification, pp. 187-209. Springer, 2016.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural In-
formation Processing Systems 27: Annual Conference on Neural Information Processing Systems
(NIPS), pp. 2672-2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014b.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J. Cree. Regularisation of neural net-
works by enforcing lipschitz continuity. CoRR, abs/1804.04368, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. pp. 770-778, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.
1109/CVPR.2016.90.
Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural
networks with logic rules. arXiv preprint arXiv:1603.06318, 2016.
Angelika Kimmig, Stephen Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. A short
introduction to probabilistic soft logic. In Proceedings of the NIPS Workshop on Probabilistic
Programming: Foundations and Applications, pp. 1-4, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. CoRR, abs/1706.06083, 2017.
Pablo Marquez-Neila, MathieU Salzmann, and Pascal Fua. Imposing hard constraints on deep net-
works: Promises and limitations. arXiv preprint arXiv:1706.02025, 2017.
Pasquale Minervini, Thomas Demeester, Tim Rocktaschel, and Sebastian Riedel. Adversarial sets
for regularising neural link predictors. CoRR, abs/1707.07596, 2017. URL http://arxiv.
org/abs/1707.07596.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-
iary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and
Alexander Mordvintsev. The building blocks of interpretability. Distill, 2018. doi: 10.23915/
distill.00010. https://distill.pub/2018/building-blocks.
9
Under review as a conference paper at ICLR 2019
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural networks
for weakly supervised segmentation. In Proceedings of the IEEE international conference on
computer vision, pp. 1796-1804, 2015.
Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore: Automated whitebox testing of
deep learning systems. In Proceedings of the 26th Symposium on Operating Systems Principles,
Shanghai, China, October 28-31, 2017, pp. 1-18, 2017. doi: 10.1145/3132747.3132785. URL
http://doi.acm.org/10.1145/3132747.3132785.
Kevin Schawinski, Ce Zhang, Hantian Zhang, Lucas Fowler, and Gokula Krishnan Santhanam.
Galaxygan: Generative adversarial networks for recovery of galaxy features. Astrophysics Source
Code Library, 2017.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Generative adversarial examples. CoRR,
abs/1805.07894, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
There is no free lunch in adversarial robustness (but there are unexpected benefits). arXiv preprint
arXiv:1805.12152, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss func-
tion for deep learning with symbolic knowledge. In Jennifer Dy and Andreas Krause (eds.), Pro-
ceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pp. 5502-5511, Stockholmsmssan, Stockholm Sweden, 10-15
Jul 2018. PMLR.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
10
Under review as a conference paper at ICLR 2019
A	Proof of Theorem 1
A.1 L(夕)(X) = 0 IMPLIES SATISFACTION
We start by giving a proof for the if direction of the Theorem 1, i.e. if L(夕)(x) = 0, then X satisfies
夕.The proof is by induction on the formula structure (We assume 夕 is negation-free as negations
can be eliminated as described in the text).
As a base case, We consider formulas consisting ofa single atomic constraint.
•	φ = (t1(X) = t2(x))
If d(t1(x),t2(x)) = 0, then by definition t1(x) = t2(x), and 夕 is satisfied.
•	夕=(t1(x) ≤ t2(x))
If 1tγχ)>t2⑶∙ d(t1(x), t2(x)) = 0, then by definition t1(x) -12 (x) ≤ 0, and 夕 is satisfied.
•	夕=(t1(x) < t2(χ))
If Iti⑶+ξ>t2⑶∙ d(t1(x) + ξ,t2(x)) = 0, then t1(x) + ξ - t2(x) ≤ 0, and since ξ > 0,
we get t1(x) < t2(x). Thus,夕 is satisfied.
As an induction step, We consider combination of formulas using single logical and or logical or
operation.
•	φ ∨ ψ
If L(6 ∙ L(Ψ) = 0, then either L(夕)=0 or L(ψ) = 0. By the induction hypothesis, either
夕 is satisfied or ψ is satisfied, implying that 夕 ∨ ψ is satisfied.
•	φ ∧ ψ
If L(夕)+ L(ψ) = 0, then (because L is non-negative) both L(夕)=0 and L(ψ) = 0. By
the induction hypothesis,夕 and ψ are satisfied, implying that 夕 ∧ ψ is satisfied.
A.2 L(夕)(X) ≥ δ(ξ) IMPLIES NON-SATISFACTION
As all variables come from a bounded set, it is easy to see that for every formula 夕 there exists
bound T(夕)such that L(夕)(x) ≤ T(夕).In other words, loss can not be arbitrarily large for a fixed
formula 夕.Given formula 夕，we will define N(夕)such that the following statement holds:
Lemma 1 If L(q)(x) > N(夕)∙ ξ then X does not satisfy 夕(X = 夕).
We prove Lemma 1 by induction on the formula structure. Base case of the induction is logical
formula consisting of one atomic expression. In this case it is easy to see that if L(夕)(x) > ξ then
X does not satisfy the formula. This means we can set N (夕)=1 for such formulas and statement
of the theorem holds.
We distinguish between two cases:
•	夕=夕1 ∧22
In this case we define:
N Q) = N (n) + N 32
Let X be an assignment which satisfies the formula 夕.This implies that X satisfies both
夕ι and 夕2. From the assumption of induction we know that L(夕i)(x) < N(夕ι)ξ and
LM)(X) < N(^2)ξ.
Adding these inequalities (and using definitions of L and N) we get:
LQ)(X) = LQI)(X) + £(22)(X) < NQI)ξ + N(22优=N(P)ξ
•夕=夕1 ∨22
In this case we define:
N(P) = max{N (P1)T (P2), N(P2)T(P1)}
Let X be an assignment which satisfies the formula 夕.This implies that X satisfies one
of 夕ι and 夕2. We can assume (without loss of generality) that X satisfies 夕1. From the
11
Under review as a conference paper at ICLR 2019
assumption of induction we know that L(夕ι)(x) < N(夕ι)ξ and also L(夕2)(x) < T(夕2).
Multiplying these inequalities (and using definitions of L and N) we get:
Lw)(X) = LWI)(X) ∙ L(ψ2)(x) < Nwl)ξ ∙ T(Ψ2) < Ns)ξ
Thus, one can choose δ(ξ) = N(夕)ξ. Then, limξ→0 δ(ξ) = 0 and for every assignment X,
L(夕)(x) > δ(ξ) implies that X does not satisfy 夕(X =夕)，thus proving the Theorem 1.
To illustrate this construction We provide an example formula 夕 = x1 < 1 ∧ x2 < 2. The loss
encoding for this formula is L(φ) = max{X1 + ξ 一 1,0} + max{X2 + ξ 一 2,0}, where ξ is the
precision used for strong inequalities. For the given example our inductive proof gives δ(ξ) = 2ξ. It
is not difficult to show that assignments with loss greater than this value do not satisfy the formula.
For example, consider x1 = 1+ ξ and x2 = 2 + 3ξ. In this case L(W)(X~) = 6ξ > δ(ξ) = 2ξ and
the assignment obviously does not satisfy 夕.But also consider the assignment x1 = 1 一 0.5ξ and
x2 = 2 一 0.5ξ. In this case L(W)(X) > 0 and L(W)(X) = ξ ‹ δ(ξ) = 2ξ and the assignment is
indeed satisfying.
B Comparison of DL2 with Prior Approaches
XSAT (Fu & Su, 2016) also translates logical constraints into numerical loss, but its atomic con-
straints are translated into non-differentiable loss, making the whole loss non-differentiable. Proba-
bilistic soft logic (e.g., Cohen et al. (2017); Hu et al. (2016)) translates logical constraints into differ-
entiable loss, which ranges between [0, 1]. However, using their loss to find satisfying assignments
with gradient methods can be futile, as the gradient may be zero. To illustrate, consider the toy exam-
ple of W(z) := (z = ( 11 )). PSL translates this formula into the loss LPSL(W) = max{z0 +z1 一 1, 0}
(it assumes z0, z1 ∈ [0, 1]). Assuming optimization starts from x = ( 00..22 ) (or any pair of numbers
such that zo + zι 一 1 ≤ 0), the gradient is RzLPSL(W) (x) = (0), which means that the optimization
cannot continue from this point, even though xis not a satisfying assignment to W. In contrast, with
our translation, we obtain L(W)(z) = |z0 一 1| + |z1 一 1|, for which the gradient for the same xis
▽zL(W)(X) = ( -1 ).
C Optimization for Querying Networks
Here we discuss how the loss compilation can be optimized for L-BFGS-B. While our translation is
defined for arbitrary large constraints, in general, it is hard to optimize for a loss with many terms.
Thus, we mitigate the size of the loss by extracting box constraints out of the expression. The loss
is then compiled from remaining constraints. Extracted box constraints are passed to the L-BFGS-B
solver which is then used to find the minimum of the loss. This “shifting” enables us to exclude a
dominant part of W from the loss, thereby making our loss amenable to optimization. To illustrate
the benefit, consider the query in Fig. 1a. Its box constraint, i in [0,255], is a syntactic sugar
to a conjunction with 2 ∙ 32 ∙ 32 ∙ 3 = 6,144 atomic constraints (two for each variables, i.e., for
every index j, we have ij ≥ 0 and ij ≤ 255). In contrast, the second constraint consists of 9 atomic
constraints (one for each possible class different from 9, as we shortly explain), and the third and
fourth constraints are already atomic. If we consider 6, 155 atomic constraints in the loss, finding a
solution (with gradient descent) would be slow. For larger inputs (e.g., inputs for ImageNet, whose
size is 224 ∙ 224 ∙ 3 > 150,000), it may not terminate in a reasonable time. By excluding the
box constraints from the loss, the obtained loss consists of only 11 terms, making it amenable for
gradient optimization. We note that while a solution is not found (and given enough timeout), we
restart L-BFGS-B and initialize the variables using MCMC sampling.
12
Under review as a conference paper at ICLR 2019
D Experiments details
	MNIST, FASHION			CIFAR-10		
	λ	PGD Iterations	Params	λ	PGD Iterations	Params
RobustnessT	0.2	-	∈ι = 7.8, €2 = 2.9	0.04	-	1 =	13.8, 2 = 0.9
RobustnessG	0.2	50	1 = 0.3, δ = 0.52	0.1	7	1 =	0.03, δ = 0.52
LipschitzT	0.1	-	L = 0.1	0.1	-	L = 1.0
LipschitzG	0.2	50	L = 0.1	0.1	5	L = 1.0
ClassesT	-	-	-	0.2	-	δ=0.01
ClassesG	-	-	-	0.2	10	δ=0.01
SegmentG	0.01	5	= 100	-	-	-
Table 2: Hyperparameters used for supervised learning experiment
Here we describe implementation details (including hyperaparameters) used during our experiments.
Supervised learning For our experiments with supervised learning we used batch size 128, Adam
optimizer with learning rate 0.0001. All other parameters are listed in 2. Additionally, for CIFAR-
10 experiments we use data augmentation with random cropping and random horizontal flipping.
Experiments with Segment constraints are done by first embedding images in 40-dimensional space
using PCA. In lower dimensional space it is sensible to consider linear interpolation between images
which is not the case otherwise. Note that this experiment is not performed for CIFAR-10 because
we do not observe good prediction accuracy with baseline model using lower dimensional embed-
dings. This is likely because dimensionality of CIFAR-10 images is much higher than MNIST or
FASHION.
We used ResNet-18 (He et al., 2016) for experiments on CIFAR-10 and convolutional neural network
(CNN) with 6 convolutional and 2 linear layers for MNIST and FASHION (trained with batchnorm
after each convolutional layer). The layer dimensions of CNN are (1, 32, 5x5) - (32, 32, 5x5) - (32,
64, 3x3) - (64, 64, 3x3) - (64, 128, 3x3) - (128, 128, 1x1) - 100 - 10 where (in, out, kernel-size)
denotes a convolutional layer and a number denotes a linear layer with corresponding number of
neurons.
Semi-supervised learning All methods use the same Wide Residual Network model. We use depth
28 and widening factor 10. Neural network is optimized using Adam with learning rate 0.001. We
use λ = 0.6 as weighting factor for DL2 loss.
For semantic loss experiment we follow the encoding from Xu et al. (2018). Please consult the
original work to see how exactly-one constraint is encoded into semantic loss. Since rule distillation
does not support our constraint, we use the following approximation (following notation from Hu
et al. (2016)):
rl(X,Y)= X	σθ(Y0)	(7)
Y 0∈G(Y)
We denote G(Y ) as set of labels sharing the same group as Y . Note that rule is meant to encourage
putting more probability mass into the groups which already have high probability mass. This
should result in the entire probability mass collapsed in one group in the end, as we want. We use
πt = max(0, 1.0 - 0.97t) as mixing factor. Other constants used are C = 1 and λ = 1.
In this experiment, we used Wide Residual Networks (Zagoruyko & Komodakis, 2016) with n=28
and k=10 (i.e. 28 layers).
Unsupervised learning
Our model is the multilayer perceptron with N*N input neurons, three hidden layers with 1000
neurons each and an output layer of N neurons. N is the number of vertices in the graph, in our
case 15. The input takes all vertices in the graph and the output is the distance for each node. The
network uses ReLU activations and dropout of 0.3 after at each hidden layer. Network is optimized
using Adam with learning rate 0.0001.
13
Under review as a conference paper at ICLR 2019
Dataset	Type	Network	Architecture	Accuracy
	C	M^N1: [0,1]28×28 → [0,1]10	Tensorflow Tutorial	0.992t
MNIST	C	M^N2: [0, l]28×28 → [θ,l]10	M_NN1 with an additional layer	0.990t
	G	M_G: [-1, 1]100→ [0,1]28×28	DC-GAN	-
	D	md[o,i]28×28 → [0,1]	DC-GAN	-
	G	M4CGAN_G:[-1,1]100 ×{0,..., 9} → [0, 1]28×28	AC-GAN	-
	D	MqCGAND:[θ, 1]28×28 → [0, 1] × [0,1]10	AC-GAN	-
	C	FM^N1 : [0,1]28×28 → [0,1]10	Tensorflow Tutorial	0.917t
Fashion	C	FM^N2 : [0,1]28×28 → [0,1]10	FM-NNI with an additional layer	0.910t
MNIST	G	FM_G: [-1, 1]100 → [0,1]28×28	DC-GAN	-
	D	FM工:[0,1]28×28 → [0,1]	DC-GAN	-
	C	C^N1 : [0, 255]32×32×3 → [0,1]10	4-layer-model	0.712#
	C	C加N2 : [0, 255]32×32×3 → [0,1]10	6-layer-model	0.756#
CIFAR	C	JVGG : [0, 255]32×32×3 → [0,1]10	VGG-16-based	0.935#
	G	JG : [-1, 1]100→ [0, 255]32×32×3	DC-GAN	-
	D	C-D : [0, 255]32×32×3 → [0,1]	DC-GAN	-
「TCDIJ	C	G-LeNet : [0, 1]32×32 → [0,1]43	based on LeNet	0.914t
GTSRB	C	G_VGG : [0,1]32×32 → [0,1]43	based on VGG	0.973t
	C	I_V16 : [0, 255]224×224×3 → [0,1]1000	VGG-16 from Keras	0.715*
ImageNet	C	I_V19 : [0, 255]224×224×3 → [0,1]1000	VGG-19 from Keras	0.727*
	C	I_R50 : [0, 255]224×224×3 → [0,1]1000	ResNet-50 from Keras	0.759*
Table 3: The datasets and networks used to evaluate DL2. The reported accuracy is top-1 accuracy
and it was either computed by the authors (*), users that implemented the work (#), or by Us (*).
Note that for GTSRB the images have dimensions 32 × 32 × 3, but the Cs take inputs of 32 × 32(×1),
which are pre-processed grayscale versions.
E	Additional Details for Section 6.2
Here we provide statistics on the networks used in the experiments of Sec. 6.2, as well as the query
templates.
Dataset and networks Our benchmark consists of five image datasets, each with different well-
established neural networks’ architectures. For each dataset, we consider at least two classifiers, and
for some we also consider a generator and a discriminator (trained using GAN Goodfellow et al.
(2014a)). We trained most networks ourselves, except for the C_VGG and the ImageNet classifiers,
for which the weights were available to download. Table 3 summarizes the networks that we used,
their architecture, and accuracy. Each row shows the dataset, the type of the network (classifier,
generator, or discriminator), the network signature, and the architecture of the network. For example,
the first row describes a classifier that takes as input images of size 28 × 28 pixels, each ranging
between 0-1, and returns a probability distribution over ten classes.
14
Under review as a conference paper at ICLR 2019
Query
1 eval N(var)
2	find i[shape]
where c (N(i))=c
3	find i[ shape ]
where c (N(i))=c,
pix_con
4	find i[ shape ]
where c (N(i))=c,
N(i).p[c] > 0.8,
pix_con
5	find i[ shape ]
where c (N(i))=c
i n i t i=var
6	find i[ shape ]
where c (N(i))=c,
pix_con,
ki - vark∞ < dist
i n i t i=var
7	find i[ shape ]
where c (N(i))=c,
pix_con
i	n i t i=var
8	find i[ shape ]
where c (N(i))=c,
i[mask] in range,
i[nm]=var[nm]
i n i t i=var
9	find i[ shape ]
where c (N(i))=c,
pix_con,
N(i).p[c] > 0.8
i n i t i=var
10	find i[ shape ]
where c (N(i))=c,
pix_con,
N(i).p[c] > 0.8,
N(i).p[cv] < 0.1
i n i t i=var
11	find i[ shape ]
where c (N(i))=c,
pix_con,
N(i).p[c] > 0.8,
N(i).p[cv] < 0.1,
ki-vark∞ < dist
i n i t i=var
Query
12	find i[ shape ]
where pix_con,
c (N2 (i))=c2 ,
C (Ni (i))=cι
13	find i[ shape ]
where pix_con,
c (N2 (i))=c,
ki - vark∞ < dist,
c (N1 (i))=cv,
i n i t i=var
14	find i[ shape ]
where c (N1 (i))=c1,
c (N2 (i))=c2 ,
N1 (i).p[c1] > 0.5,
N2 (i).p[c1] < 0.1,
N2 (i).p[c2] > 0.5,
N1 (i).p[c2] < 0.1,
pix_con,D (i) < 0.1
15	find i[100]
where i in [-1,1],
c (N1 (G(i)))=c1,
N1 (G(i)).p[c1]> 0.3,
c (N2 (G(i)))=c2,
N2 (G(i)).p[c2]> 0.3
16	find i[ shape ]
where c (N1 (i))=cv,
c (N2 (i))=c,
i[mask] in range,
i[nm]=var[nm]
i n i t i=var
17	find i[ shape ]
where c (N1 (i))=c1,
c (N2 (i))=c2 ,
N1 (i).p[c1] > 0.5,
N1 (i).p[c2] < 0.1,
pix_con
18	find i[ shape ]
where c (N1 (i))=c1,
c (N2 (i))=c2 ,
N1 (i).p[c1] > 0.6,
N1 (i).p[c2] < 0.1,
N2 (i).p[c2] > 0.6,
N2 (i).p[c1] < 0.1,
pix_con
Figure 5:	The template queries used to evaluate DL2.
F Additional Experiments
Here we provide further experiments to investigate scalability and run-time behavior of DL. For all
experiments we use the same hyperparameters as in Section 6.2, but ran experiments I and II on a
laptop CPU and experiment III on the same GPU setup as in Section 6.2 and increased the timeout
to 300 s.
15
Under review as a conference paper at ICLR 2019
(a) Run-time for experiment 1. Runs up to 213 vari-
ables are between 0.1 - 0.2 s and don’t show in-
crease with number of variables. Afterwards growth
is linear.
Figure 6:	Experimental results for Experiments
different random seed. All runs succeed.
Time [«]
(b) Run-time for experiment 2. All runs up to c
5000 take around 0.15 s.
1 and 2. Results are average over 10 runs with
Experiment I: Number of variables To study the run-time behavior in the number of variables we
consider a simple toy query
find i[c]
where 1000 < sum(i), sum(i) < 1001
return i
for different integers c. We execute this query for a wide range of c values, 10 times each and report
the average run-time in Figure 6a. All runs succeeded and found a correct solution. We observe
constant run-time behavior for up to 213 variables and linear run-time in the number of variables
afterwards.
Experiment II: Opposing constrains To study the impact of (almost) opposing constraints we
again consider a simple toy query
find i[1]
where i[0] < -c ∨ c < i[0]
return i
for an integer c. This query requires optimizing two opposing terms until one of them is fulfilled.
The larger c the more opposed the two objectives are and indeed for c → ∞ we would obtain an
unsatisfiable objective. Again all runs succeeded and found a correct solution. In Figure 6b we
present the average run-time over 10 runs for different c. Up to c = 5000 the run-time is constant
with roughly 0.15 s.
Experiment III: Scaling in the number of constraints To study the scaling of DL2 in the number
of constraints consider the following query for an adversarial example:
For this experiment we consider the query:
FIND p[28, 28]
WHERE c l a s s (M_NN1(clamp(p + M_nine, 0, 1))) = c
RETURN i, clamp(p + M_nine, 0, 1)
The query looks for an adversarial perturbation p to a given image of a nine (M_nine) such that the
resulting image gets classifies as class c. The query returns the found perturbation and the resulting
image. The clamp(I, a, b) operation takes an input I and cuts off all it’s values such that they
are between a and b.
Additionally we impose constraints the rows and columns of the image. For a row i we want to
enforce that the values of the perturbation vector are increasing from left to right:
16
Under review as a conference paper at ICLR 2019
Row constraints
k	0	1	3	5	10	20	28
怎✓ [s]	1.01	2.63	5.77	9.63	19.77	44.61	84.49
#✓	9	9	9	9	9	9	9
		Column constraints					
k	0	1	3	5	10	20	28
©✓ [s]	0.89	2.68	5.73	9.30	18.46	42.03	125.92
#✓	9	9	9	9	9	9	9
		Row & Column constraints					
k	0	1	3	5	10	20	28
怎✓ [s]	0.87	4.19	11.04	17.74	44.04	163.57	243.72
#✓	9	9	9	9	9	9	4
Table 4: RUn times for additional constraints on adversarial perturbation. #/is the number of suc-
cessful runs out of 9 and ©✓is the average run time over the successful runs in seconds. k row or
column constraints corresponds to 27 individual constraints in DL2 each. So the right most column
adds 756 constraints for the first two settings and 1512 for the last.
p[i, 0] < p[i, 1], p[i, 1] < p[i, 2], p[i, 2] < p[i, 3], ...
For one row this yields 27 constraints. Further we consider a similar constraint for a column j ,
where we want the values to increase from to to bottom:
p[0, j] < p[1, j], p[1, j] < p[2, j], p[2, j] < p[3, j] . . .
We apply these constraints on the first k rows and columns of the image independently and jointly.
For different k we execute the query over all possible target classes c ∈ {0, . . . , 8} and report the
average time in Table 4. The run-time is mostly linear for k up to 20 but then jumps as we increase
it to 28. The reason for this is likely that with many more constraints the solution spare grows sparer
and sparser and it becomes hard to find an assignment. We observe that all queries, but for 5 in
the case with k = 28 row and column constraints, could be solved. These 5 queries hit the 300 s
timeout. Figure 7 shows a resulting image.
(a) The found perturbation p, scaled such that -0.3
corresponds to black and 0.3 to white.
(b) The resulting image.
Figure 7: Found results for the full 28 row & column constraints and target class 6.
17