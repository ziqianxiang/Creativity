Under review as a conference paper at ICLR 2019
Exploration in Policy Mirror Descent
Anonymous authors
Paper under double-blind review
Ab stract
Policy optimization is a core problem in reinforcement learning. In this paper, we
investigate Reversed Entropy Policy Mirror Descent (REPMD), an on-line policy
optimization strategy that improves exploration behavior while assuring monotonic
progress in a principled objective. REPMD conducts a form of maximum entropy
exploration within a mirror descent framework, but uses an alternative policy update
with a reversed KL projection. This modified formulation bypasses undesirable
mode seeking behavior and avoids premature convergence to sub-optimal poli-
cies, while still supporting strong theoretical properties such as guaranteed policy
improvement. An experimental evaluation demonstrates that this approach signifi-
cantly improves practical exploration and surpasses the empirical performance of
state-of-the art policy optimization methods in a set of benchmark tasks.
1	Introduction
Model-free deep reinforcement learning (RL) has recently been shown to be remarkably effective
in solving challenging sequential decision making problems (Schulman et al., 2015; Mnih et al.,
2015; Silver et al., 2016). A central method of deep RL is policy optimization (aka policy gradient),
which is based on formulating the problem as the optimization of a stochastic objective (expected
return) with respect to the underlying policy parameters (Williams & Peng, 1991; Williams, 1992;
Sutton et al., 1998). Unlike standard optimization, stochastic optimization requires the objective
and gradient to be estimated from data, typically gathered from a process depending on current
parameters, simultaneously with parameter updates. Such an interaction between estimation and
updating complicates the optimization process, and often necessitates the introduction of variance
reduction methods, leading to algorithms with subtle hyperparameter sensitivity. Joint estimation and
updating can also create poor local optima whenever sampling neglect of some region can lead to
further entrenchment of a current solution. For example, a non-exploring policy might fail to sample
from high reward trajectories, preventing any further improvement since no useful signal is observed.
In practice, it is well known that successful application of deep RL techniques requires a combination
of extensive hyperparameter tuning, and a large, if not impractical, number of sampled trajectories. It
remains a major challenge to develop methods that can reliably perform policy improvement while
maintaining sufficient exploration and avoiding poor local optima, yet do so quickly.
Several ideas for improving policy optimization have been proposed in the literature, generally
focusing on the goals of improving stability and data efficiency (Peters et al., 2010; Van Hoof et al.,
2015; Fox et al., 2015; Schulman et al., 2015; Montgomery & Levine, 2016; Nachum et al., 2017b;c;
Tangkaratt et al., 2017; Abdolmaleki et al., 2018; Haarnoja et al., 2018). Unfortunately, a notable gap
remains between empirically successful methods and their underlying theoretical support. Current
analyses typically assume a simplified setting that either ignores the policy parametrization or only
considers linear models. These assumptions are hard to justify when current practice relies on
complex function approximators, such as deep neural networks, that are highly nonlinear in their
underlying parameters. This gulf between theory and practice is a barrier to wider adoption of
model-free policy gradient methods.
In this paper, we focus on a setting where the policy can be a non-convex function of its parameters.
We begin by considering the entropy-regularized expected reward objective, which has recently
re-emerged as a foundation for state-of-the-art RL methods (Williams & Peng, 1991; Fox et al., 2015;
Schulman et al., 2017a; Nachum et al., 2017b; Haarnoja et al., 2017). Our first contribution is to
reformulate the maximization of this objective as a lift-and-project procedure, following the lines of
Mirror Descent (Nemirovskii et al., 1983; Beck & Teboulle, 2003). Such a reformulation achieves
1
Under review as a conference paper at ICLR 2019
two things. First, it makes it easier to analyze policy optimization in the parameter space: using this
reformulation we can establish a monotonic improvement guarantee with a fairly simple proof, even
in a non-convex setting. We also provide a study of the fixed point properties of this setup. Second,
the proposed reformulation has practical algorithmic consequences, suggesting, for example, that
multiple gradient descent updates should be performed in the projection step. These considerations
lead to our first practical algorithm, Policy Mirror Descent (PMD), which first lifts the policy to the
entire policy-simplex, ignoring the constraint induced by its parametrization, then approximately
solves the projection step by multiple gradient descent updates to the policy in the parameter space.
We then investigate additional modifications to mitigate the potential deficiencies of PMD. The main
algorithm we propose, Reversed Entropy PMD (REPMD), incorporates both an entropy and relative
entropy regularizer, and uses the mean seeking direction of KL divergence for projection. The benefit
of this approach is twofold. First, using just the mean seeking direction of KL divergence for the
projection step helps avoids poor local optima; second, this specific problem can now be efficiently
solved to global optimality in certain non-trivial non-convex scenarios, such as one-layer-softmax
networks. Similar guarantees can be proved for REPMD, which additionally incorporates entropy
regularization, with respect to a surrogate objective SR(π). We further study the properties of
SR(π) and provide theoretical and empirical evidence that SR can effectively guide the search for
good policies. Finally, we also show how this algorithm can be extended with a value function
approximator, and develop an actor-critic version that is effective in practice.
1.1	Notation and Problem Setting
For simplicity, we only consider finite horizon settings with finite state and action spaces. The
behavior of an agent is modelled by a policy ∏(a∣s) that specifies a probability distribution over a
finite set of actions given an observed state. At each time step t, the agent takes an action at by
sampling from π(a∕st). The environment then returns a reward r = r(st, a/ and the next state
st+1 = f(st, at), where f is the transition function not revealed to the agent. Given a trajectory, a
sequence of states and actions ρ = (s1, a1, . . . , aT-1, sT), the policy probability and the total reward
of P are defined as π(ρ) = QT=-II ∏(at∣st) and r(ρ) = PT=II r(st, at). Given a set of parametrized
policy functions ∏θ ∈ Π, policy optimization aims to find the optimal policy ∏θ by maximizing the
expected reward,
∏θ ∈ arg max E r(ρ),
∏θ ∈π P 〜πθ
(1)
We use ∆，{∏∣ EP ∏(ρ) = 1, ∏(ρ) ≥ 0, ∀ρ} to refer to the probability simplex over all possible
trajectories. Without loss of generality, we also assume that the state transition function is determinis-
tic, and the discount factor γ = 1. This same simplification is also assumed in Nachum et al. (2017a).
Results for the case of a stochastic state transition function are presented in Appendix D.
2	Policy Mirror Descent
We begin with the development of the basic Policy Mirror Descent (PMD) strategy, which will form
the basis for our subsequent algorithms and their analyses. As mentioned in the introduction, our
analysis of this and subsequent methods focuses on the non-convex setting.
Consider the following local optimization problem: given a reference policy π (usually the current
policy), maximize the proximal regularized expected reward, using relative entropy as the regularizer:
∏θ = argmax E r(ρ) — tDkl(∏θ∣∣π).
∏θ ∈π P 〜πθ
(2)
Relative entropy regularization has been widely investigated in online learning and constrained
optimization (Nemirovskii et al., 1983; Beck & Teboulle, 2003), primarily as a component of the
mirror descent algorithm. Observe that when ∏ is the uniform distribution, Eq. (2) reduces to entropy
regularized expected reward. It is important to note that, since we are interested in the non-convex
setting and only assume that π is parametrized as a smooth function of θ ∈ Rd , Π is generally a
non-convex subset of the simplex. Therefore, Eq. (2) is a difficult constrained optimization problem.
2
Under review as a conference paper at ICLR 2019
A useful way to decompose this constrained optimization is to consider an alternating lift-and-project
procedure that isolates the different computational challenges.
(Project Step) arg min Dkl (∏θ k∏T),
πθ ∈Π
(Lift Step) where ∏T = argmax E r(ρ) — tDkl(π∣∣∏).
π∈∆ P〜π
(3)
Crucially, the reformulation Eq. (3) remains equivalent to the original problem Eq. (2), in that it
preserves the same set of solutions, as established in Proposition 1.
Proposition 1. Given a reference policy ∏,
arg max E r(ρ) — tDkl(∏θ ∣∣π) = argmin Dkl(∏θ ∣∣∏T).
∏Θ ∈∏ P 〜πθ	∏θ ∈∏
Note that this result holds even for the non-convex setting. The reformulation Eq. (3) immediately
leads to our PMD algorithm: Lift the current policy ∏θt to ∏T, then perform multiple steps of gradient
descent in the Project Step to update πθt+1 .1
When Π is a convex set, one can show that PMD asymptotically converges to the optimal policy
(Nemirovskii et al., 1983; Beck & Teboulle, 2003). The next proposition shows that despite the
non-convexity of Π, PMD still enjoys desirable properties.
Proposition 2. Let πθt denote the policy produced at step t of the update sequence. Then PMD
satisfies the following properties for an arbitrary parametrization of π.
1.	(Monotonic Improvement Guarantee) If the Project Step min Dkl(∏θ∣∣∏T) Can be
πθ∈Π	τ
solved globally optimally, then
EP〜∏θt + 1 r(P)- EP〜∏θt r(P) ≥ 0.
2.	(Global Optimum Guarantee) Ifthe Project SteP min Dkl(∏θ k∏T) Can be SolVed glob-
πθ∈Π	τ
ally optimally, then ∏θt converges to the global optimum π*, i.e.
lim EP〜ng, r(ρ) = EP〜∏* r(ρ) ≥ EP〜∏ r(ρ),	∀∏ ∈ ∏.
t→∞	t
3.	(Fixed Points) Assume that the Project Step is optimized by gradient descent, then the fixed
points ofPMD are the stationary points of the expected reward EP 〜∏g r(ρ).
Despite these desirable properties, Proposition 2 relies on the condition that the Project Step in PMD
is solved to global optimality. It is usually not practical to achieve such a stringent requirement when
πθ is not convex in θ, limiting the applicability of Proposition 2.
Another shortcoming with this naive strategy is that PMD typically gets trapped in poor local optima.
Indeed, while the relative entropy regularizer help prevent a large policy update, it also tends to limit
exploration. Moreover, minimizing the KL divergence Dkl (∏θ k∏T) is known to be mode seeking
(Murphy, 2012), which can lead to mode collapse during the learning process. Once a policy ∏T has
lost important modes, learning can easily become trapped at a sub-optimal policy. Unfortunately, at
such points, the relative entropy regularizer does not encourage further exploration.
3 Reversed Entropy Policy Mirror Descent
We now introduce two modifications to PMD that overcome its aforementioned deficiencies. These
two modifications lead to our main proposed algorithm, Reversed Entropy Policy Mirror Descent
(REPMD), which retains desirable theoretical properties while achieving vastly superior performance
to PMD in practice. (We consider some additional refinements to REPMD in the next section.)
1 To estimate this gradient one would need to use self-normalized importance sampling Owen (2013). We
omit the details here since PMD is not our main algorithm; similar techniques can be found in the implementation
of REPMD.
3
Under review as a conference paper at ICLR 2019
The first modification is to add an additional entropy regularizer to the Lift Step, to improve the
exploration behavior of the algorithm. The second modification is to use a reversed, mean seeking
direction of the KL divergence in the Project Step. In particular, the REPMD algorithm solves the
following alternating optimization problems to update the policy πθt+1 at each iteration:
(Project Step) arg min Dkl (∏T T，k∏θ),
πθ ∈Π	,
(Lift Step) where ∏T T ‘ =argmax E r(ρ) 一 tDkl(π∣∣∏θJ + T 0H(π).
,	π∈∆ P〜π
(4)
The idea of optimizing the reverse direction of KL divergence has proved to be effective in previous
work, such as reward augmented maximum likelihood (Norouzi et al., 2016) and UREX (Nachum
et al., 2017a). Its mean seeking behavior further encourages exploration by the algorithm. As we will
see in Section 5, REPMD outperforms PMD significantly in experimental evaluations.
Theorem 1 shows that REPMD still enjoys similar desirable properties to PMD in the non-convex
setting, but with respect to a surrogate reward SR(πθ), which we analyze further below.
Theorem 1. Let πθt denote the policy produced at step t of the update sequence. REPMD satisfies
the following properties for an arbitrary parametrization of π.
1.	(Monotonic Improvement Guarantee) Ifthe Project Step Dkl(∏T,t0 k∏θ) Can be Solved
globally optimally, then
SR(∏θt+ι) - SR(∏θt) ≥0,
where
SR(∏θ) , (τ + T0) log X exp {():10； πθ(P) }.	⑸
ρ
2.	(Global Optimum Guarantee) If the Project Step can be solved globally optimally, then
∏θt converges to the global optimum π*, i.e.
lim SR(∏θj = SR(π*) ≥ SR(π),	∀π ∈ Π.
t→∞
3.	(Fixed points) Assume that the Project Step is optimized by gradient descent, then the fixed
points of REPMD are the stationary points of the expected reward SR(πθ).
A key question is the feasibility of solving the Project Step to global optimality. It is obvious that
when π(θ) = θ ∈ ∆ the Project Step is a convex optimization problem, hence can be solved optimally.
Furthermore, as shown in Proposition 3, for a one-layer-softmax neural network π, the Project Step
Dkl(∏T τo k∏θ) Can also still be solved to global optimality, affording a significant computational
advantage over PMD.
Proposition 3. Assume ∏θ (s) = softmax(φ>θ). Given a reference policy π, the Projection Step
minθ∈Rd DκL(∏k∏θ) is a convex optimization problem in θ.
3.1	Learning Algorithms
We now provide some of the learning algorithms for REPMD. Despite its non-convexity, the Lift
Step has an analytic solution,
∏3eχp n U Iog n(p)o
πτ,τ0 (P) , P r 0、	n r(p，)-T，log Π(ρ0) o .	⑹
Σρ0 π(P0) exP ∖ (P) τ+τg (P) ʃ
The Project Step in Eq. (4), min∏θ∈∏ Dkl (∏T T‘ k∏θ), Can be optimized via stochastic gradient
descent, given that one can sample trajectories from ∏T,τ，to estimate its gradient. To see this, note
that argmin∏θ∈∏ Dkl(∏T,tok∏θ) = argmin∏θ∈∏ EP〜∏* ， ―log∏θ(ρ). The next lemma shows
that sampling from ∏T T，can be done using self-normalized importance sampling (Owen, 2013) when
it is possible to draw multiply samples, following the idea of UREX (Nachum et al., 2017a).
4
Under review as a conference paper at ICLR 2019
Algorithm 1 The REPMD algorithm
Input: temperature parameters τ and τ0, number of samples for computing gradient K
1:	Random initialized πθ
2:	For t = 1,2,. . . do
3:	Set ∏ = ∏θ
4:	Repeat
5:	Sample a mini-batch of K trajectories from ∏
6:	Compute the gradient according to Eq. (7)
7:	Update πθ by gradient descent
8:	Until converged or reach maximum of training steps
Lemma 1. Let ωk = NPk)-T+^^π(ρk). Given K i.i.d. samples {ρι,..., PK} from the reference
policy π, we have the following unbiased gradient estimator,
K
vθDKL(πT,τo kπθ) ≈ - X
k=1
exp {ωk}
PjK=1 exp {ωj}
Vθ log ∏θ (Pk),
(7)
Derivation for the analytic solution of the Lift Step and above Lemma as well as other implementation
details can be found in Appendix B.
3.2 BEHAVIOR OF SR(π)
Theorem 1 only establishes desirable properties for REPMD with respect to SR(π), but not necessarily
r(ρ). In this section we present theoretical and empirical evidence that SR(πθ) is in fact a reasonable
surrogate that can provide good guidance for learning, even when targeting desirable behavior with
respect to r(ρ). In fact, by properly adjusting the two temperature parameters τ and τ0 in REPMD,
the resulting surrogate objective SR(πθ) recovers existing performance measures.
Proposition 4. SR(πθ ) satisfies the following properties:
(i)	SR(πθ) → maxρ r(ρ), as τ → 0, τ0 → 0.
(ii)	SR(∏θ) → EP〜∏θ r(ρ), as T → ∞,τ0 → 0.
Remark 1. Note that SR(πθ) also resembles the “softmax value function” that has become popular
in value based RL (Nachum et al., 2017b; Haarnoja et al., 2018; Ding & Soricut, 2017). The standard
softmax value can be recovered by SR(πθ) as a special case when T = 0 or T0 = 0.
According to Proposition 4, one should gradually decrease τ0 to reduce the level of exploration as
sufficient reward landscape information is collected during the learning process. Different choices can
be made for τ , depending on the policy constraint set Π. Given τ0 → 0 and a sufficiently explored
reward landscape, the resulting unconstrained policy must satisfy ∏T To → ∏* as T → 0, where ∏* is
the globally optimal deterministic policy. Therefore, in the Project Step, πθ is obtained by directly
projecting ∏* into Π. When the policy constraint Π has nice properties that support good behavior
of KL projection, such as convexity, πθ will achieve good performance. However, in practice, Π is
typically non-convex, and setting T → 0 might not work very well, since directly projecting π* into
Π will not always lead to a πθ with large expected reward.
On the other hand, as T → ∞, the stationary point of SR(πθ) will approach the stationary point
of Pρ πθ(ρ)r(ρ). Fig. 1 shows the a simulation investigating the behavior of SR(πθ) for different
T . Note that there is a poor local maximum for θ < 0, where naive gradient method will converge
to if initialization is about θ < -10. When T = 0.05, the reward landscape of SR(πθ) guides θ to
converge to the neighbourhood of 0, thus helping avoid the poor local maximum. Later in training,
when T increases to 5, SR(πθ) recovers the true expected reward landscape, ensuring that θ will
converge to a good local (if not the global) maximum. While it is hard for a simulation study to be
comprehensive, these results demonstrate how SR(πθ) can offer reasonable guidance for maximizing
the true expected reward. Further investigation on the behavior of SR(πθ) is left for future work.
5
Under review as a conference paper at ICLR 2019
2 1
seMΘ.! θs60tns
O Q
PH」θa60ns
PH」θa60tns
-1 1-'----'----'----'----'----'	0.5 I-'----'----'----'----'----'
-30	-20	-10	0	10	20	30	-30	-20	-10	0	10	20	30
True reward	T = 0.05
-130	-20	-10	0	10	20	30	-130	-20	-10	0	10	20	30
τ = 0,5	τ = 5
Figure 1: Simulation results for the true reward landscape and SR(πθ) with different values of
τ in a bandit setting with 10,000 actions. Each action is represented by a feature ω ∈ R. Let
Ω = (ωι,..., ω10,000) be the feature vector. The policy is parameterized by a weight scalar θ ∈ R.
The policy is defined by SOftmax(Ω>θ). True Reward landscape shows the expected reward as a
function of θ. The rest figures show SR(πθ) as a function of θ with different value of τ.
4 An Actor-Critic Extension
Finally, we develop a natural extension of REPMD to an actor-critic formulation by incorporating a
value function approximator. We refer to this final algorithm as Policy Mirror Actor-Critic (PMAC).
It is well known that data efficiency of policy-based methods can be generally improved by adding a
value-based critic. For a reference policy ∏ and an initial state s, recall that the objective in the Lift
Step of REPMD is
OREPMD(∏,s)= E r(ρ) - TDKL (∏k∏)+ T0H(π),
P〜π
where ρ = (s1 = s, a1, s2, a2, . . .). To incorporate value function approximation, we need to derive
the temporal consistency structure for this objective, which can be done as follows. First, write
OREPMD(∏, S) = Ea〜∏(∙∣s) [r(s,a) + OREPMD(∏, s0) + Tlog∏(a∣s) - (τ + T0)logπ(a∣s)].
Let ∏T,τo(∙∣s) = argmax∏ OREPMD(∏, S) denote the optimal policy on state s. Further denote the
soft optimal state-value function OREPMD(n； T，(∙∣s), S) by VrT，(s), and let QTτ，(s,a) = r(s, a) +
YWTO (s0) be the soft-Q function. It can then be verified that
%，(s) = (τ + T 0) log X exp ( Q "(S,aT + T∣og n(a|s))；
a	(8)
_* / I、 JQ T,τ O (s,a) + T log n(a|s) - V,T，(S)I
πτ,τ 0(a|s) = exP <-------------TTP------------------卜
We propose to train a soft state-value function Vφ parameterized by φ, a soft Q-function Qψ param-
eterized by ψ , and a policy πθ parameterized by θ, based on Eq. (4). The update rules for these
parameters can be derived as follows.
The soft state-value function approximates the soft optimal state-value PTr，∙ Note that We can
re-express PTT，by
-	,	(Q二，(s, a) — T0 log ∏(a∣s)]]
V,T，(s) =(T + T0) log Ea〜∏ exp 彳 We，T + T0	∖ .
This suggests a Monte-Carlo estimate for P；T，(s): by sampling one single action a according to the
reference policy π, We have VTTO (S) ≈ QT T，(s, a) - t0 logπ(a∣s). Then, given a replay buffer D,
the soft state-value function can be trained to minimize the mean squared error,
L(φ) = Es〜D 1 (Vφ(s) - [Qψ(s, a) - t0logπ(a∣s)])2 .	(9)
6
Under review as a conference paper at ICLR 2019
Figure 2: Results of MENT (red), UREX (green), and REPMD (blue) on synthetic bandit problem
and algorithmic tasks. Plots show average reward with standard error during training. Synthetic
bandit results averaged over 5 runs. Algorithmic task results averaged over 25 random training runs
(5 runs × 5 random seeds for neural network initialization). X-axis is number of sampled trajectories.
One might note that, in principle, there is no need to include a separate state-value approximation,
since it can be directly computed from a soft-Q function and reference policy, using Eq. (8). However,
including a separate function approximator for the state-value can help stabilize the training (Haarnoja
et al., 2018). The soft Q-function parameters ψ is then trained to minimize the soft Bellman error
using the state-value network,
L(ψ) = E(s,a,s0)〜D I(Qψ (s,a) - [r(s, a) + YVφ(SO)D2	(IO)
The policy parameters are updated by performing the Project Step in Eq. (4) with stochastic gradient
descent,
Qψ( κ Q Q	QQψ(s,•)+ Tlogπ(∙∣s) - Vφ(s)l Il ., Λl
L(θ) = Es〜D DKL I exp j----------------T + T0------------∖ ll∏θ('⑼ )	(11)
where We approximate ∏T,τ，by the Soft-Q and state-value function approximations.
Finally, we also use a target state-value network (Lillicrap et al., 2015) and the trick of maintaining
two soft-Q functions (Haarnoja et al., 2018; Fujimoto et al., 2018). Implementation details for PMAC
are given in Appendix C.
5	Experiments
We evaluate the main proposed methods, REPMD and PMAC, on a number of benchmark tasks
against strong baseline methods. Additional implementation details are provided in Appendix E.2.
5.1	Settings
We first investigate the performance of REPMD on a synthetic bandit problem and the algorithmic
task suite from the OpenAI gym (Brockman et al., 2016). The synthetic multi-armed bandit problem
has 10,000 distinct actions, where the reward of each action i is initialized by ri = Si8 such that Si
is randomly sampled from a uniform [0, 1) distribution. Each action i is represented by a randomly
sampled feature vector ωi ∈ R20 from standard normal distribution. Note that these features are fixed
during training. We further test our method on five algorithmic tasks from the OpenAI gym library,
in rough order of difficulty: Copy, DuplicatedInput, RepeatCopy, Reverse, and ReversedAddition
(Brockman et al., 2016). Second, we test the second PMAC approach on continuous-control bench-
marks from OpenAI Gym, utilizing the Mujoco environment (Brockman et al., 2016; Todorov et al.,
2012); including Hopper, Walker2d, HalfCheetah, Ant and Humanoid. The details of the algorithmic
and Mujoco tasks are provided in Appendix E.1.
7
Under review as a conference paper at ICLR 2019
Figure 3: Learning curves of DDPG (red), TD3 (yellow), SAC (green) and PMAC (blue) on Mujoco
tasks (with SAC+R (gray) added on Humanoid). Plots show mean reward with standard error during
training, averaged over five different instances with different random seeds. X-axis is millions of
environment steps.
Note that only cumulative rewards are available in both the synthetic bandit and algorithmic tasks.
Therefore, value-based RL algorithms can not be applied in this setting. This restriction compels us to
compare REPMD against REINFORCE with entropy regularization (MENT) (Williams, 1992), and
under-appreciated reward exploration (UREX) (Nachum et al., 2017a). To the best of our knowledge,
these are the state-of-the-art policy-based algorithms for these algorithmic tasks. For the continuous
control tasks, we compare the second PMAC approach to deep deterministic policy gradient (DDPG)
(Lillicrap et al., 2015), an efficient off-policy deep RL methods; twin delayed deep deterministic
policy gradient algorithm (TD3) (Fujimoto et al., 2018), a recent extension to DDPG by applying
the double Q-learning trick to address over-estimation problem when function approximations are
adopted; and Soft-Actor-Critic (SAC) (Haarnoja et al., 2018), a recent state-of-the-art off-policy
algorithm on a number of benchmarks. All of these algorithms are implemented in rlkit.2 We do not
include TRPO and PPO in these experiments, as their performance is dominated by SAC and TD3, as
shown in (Haarnoja et al., 2018; Fujimoto et al., 2018).
5.2	Comparative Evaluation
The results on synthetic bandit problem and algorithmic tasks are reported in Fig. 2. It is clear
that REPMD substantially outperforms the baseline methods on these tasks. REPMD appears able
to consistently achieve a higher score and learn substantially faster than UREX. We also find the
performance of UREX is unstable. On the difficult tasks, including RepeatCopy, Reverse and
ReversedAddition, UREX only finds solutions a few times out of 5 runs for each random seed, which
brings the overall scores down. This observation explains the gap between the results we find here and
those reported in Nachum et al. (2017a).3 Note that the performance of REPMD is sill significantly
better than UREX even compared to the results reported in Nachum et al. (2017a).
Fig. 3 presents the results on the continuous-control benchmarks, reporting the total mean returns on
evaluation rollouts obtained by the algorithms during training. These results are averaged over five
different instances with different random seeds. The solid curves corresponds to the mean and the
shaded region to the standard errors over the five trials. To ensure a fair comparison with PMAC, we
implemented the double-Q but not the reparameterization trick for SAC (see equation (11)-(13) in
(Haarnoja et al., 2018)). This difference explains the discrepancy between the results we see here
and those reported in (Haarnoja et al., 2018). We observe that without the reparameterization trick
SAC cannot make any progress in the hardest problem Humanoid. Therefore, to gain further clarity,
2 https://github.com/vitchyr/rlkit
3 The results reported in (Nachum et al., 2017a) are averaged over 5 runs of random restarting, while our
results are averaged over 25 random training runs (5 runs × 5 random seed for neural network initialization).
8
Under review as a conference paper at ICLR 2019
Figure 4: Ablation Study of REPMD and PMAC on ReversedAddition and Ant.
we also report the result of SAC with the reparameterization trick, denoted SAC+R, on Humanoid
using the author’s GitHub implementation. The results show that PMAC matches or, in many cases,
surpasses all other baseline algorithms in both final performance and sample efficiency across tasks,
except compared to SAC+R in Humanoid. In Humanoid, although SAC+R outperforms PMAC, its
final performance is still comparable with SAC+R. Note that the reparameterization trick that benefits
SAC could also be applied in PMAC; we have yet to add this modification and will add this to PMAC
in future work.
5.3	Ablation Study
The comparative evaluations provided in the previous sections suggest that our proposed algorithms
outperform conventional RL methods on a number of challenging benchmarks. In this section,
we further investigate how each novel component of Eq. (4) improves learning performance, by
performing an ablation study on ReversedAddition and Ant.
Importance of entropy regularizer. The main difference between the objective in Eq. (4) and the
PMD objective Eq. (3) is the entropy regularizer. We demonstrate the importance of this choice by
presenting the results of REPMD and PMAC without the extra entropy regularizer, i.e. τ0 = 0.
Importance of KL divergence projection. Another important difference between Eq. (4) with other
RL methods is to use a Project Step to optimize the policy, rather than direct SGD of the objective
function. To show the importance of the Project Step, we test REPMD and PMAC without projection,
which only performs one step of gradient update at each iteration of training.
Importance of direction of KL divergence. We choose PMD Eq. (3) as another baseline to prove
the effectiveness of using the mean seeking direction of KL divergence in the project step. Similar to
REPMD, we add a separate temperature parameter τ0 > 0 to the original objective function in Eq. (3)
to encourage policy exploration, which gives arg max∏0∈∏ EP〜∏θ r(ρ) - TKL(∏θ∣∣π) + τ0H(∏θ).
We name this algorithm PMD+entropy. The corresponding algorithms in the actor-critic setting,
named PMD-AC and PMD-AC+entropy, are also implemented for comparison.
The results are presented in Fig. 4, which clearly indicate all of the three major components of Eq. (4)
are helpful for achieving better performance.
6	Related Work
The lift-and-project approach is distinct from the previous literature on policy search, with the
exception of a few recent works: Mirror Descent Guided Policy Search (MDGPS) (Montgomery &
Levine, 2016), Guide Actor-Critic (GAC) (Tangkaratt et al., 2017), Maxmimum aposteriori (MPO)
(Abdolmaleki et al., 2018), and Soft Actor-Critic (SAC) (Haarnoja et al., 2018). These approaches
also adopt a mirror descent framework, but differ from the proposed approach in key aspects. MDGPS
(Montgomery & Levine, 2016) follows a different learning principle, using the Lift Step to learn
multiple local policies (rather than a single policy) then aligning these with a global policy in the
Project Step. MDGPS also does not include the entropy term in the Lift objective, which we have
found to be essential for exploration. MPO (Abdolmaleki et al., 2018) also neglects to add the
additional entropy term; Section 5.3 shows that entropy regularization with an appropriate annealing
of τ0 significantly improves learning efficiency. Both GAC and SAC use the mode seeking direction
of KL divergence in the Project Step, in opposition to the mean seeking direction we consider here
9
Under review as a conference paper at ICLR 2019
(Tangkaratt et al., 2017; Haarnoja et al., 2018). Additionally, SAC only uses entropy regularization
in the Lift Step, neglecting the proximal relative entropy. The benefits of regularizing with relative
entropy has been discussed in TRPO (Schulman et al., 2015) and MPO (Abdolmaleki et al., 2018),
where it is noted that proximal regularization significantly improves learning stability. GAC seeks to
match the mean of Gaussian policies under second order approximation in the Project Step, instead
of directly minimizing the KL divergence with gradient descent. Although one might also attempt to
interpret “one-step” methods in terms of lift-and-project, these approaches would obliviously still
differ from REPMD, given that we use different directions of the KL divergence for the Lift and
Project steps respectively.
Regarding the optimization objective, several existing methods have considered related approaches,
either by considering (relative) entropy regularization during policy search, or directly using KL
divergence as the target objective. As noted in Section 2, REPMD resembles policy gradient methods
that maximize expected reward with an additional entropy regularizer (Williams & Peng, 1991; Fox
et al., 2015; Nachum et al., 2017b). Using KL divergence (or Bregman divergences more generally)
as regularizers has also been explored in Liu et al. (2015); Thomas et al. (2013); Mahadevan & Liu
(2012). However, these approaches differ from the proposed method in important ways. In particular,
they apply regularization to the parameters of the linear approximated value functions, whereas here
KL regularization is applied directly to the policy space. The literature on relative entropy policy
search also uses a similar KL divergence regularization scheme (Peters et al., 2010; Van Hoof et al.,
2015), but on joint state-action distributions. Instead of KL divergence, Reward-Weighted Regression
(RWR) uses a log of the correlation between ∏T and ∏θ , which is then approximated similar to a cross
entropy loss (Peters & Schaal, 2007; Wierstra et al., 2008).
TRPO and PPO also have a similar formulation to Eq. (2), using a constrained version with a mean
seeking KL divergence Schulman et al. (2015; 2017b). Our proposed method includes additional
modifications that, as shown in Section 5, significantly improve performance. UREX also uses mean
seeking KL for regularization, which encourages exploration but also complicates the optimization;
as shown in Section 5, UREX is significantly less efficient than the method proposed here.
Trust PCL adopts the same objective defined in Eq. (4), including both entropy and relative entropy
regularization (Nachum et al., 2017c). However, the policy update strategy is substantially different:
while REPMD uses KL projection, Trust PCL minimizes a path inconsistency error (inherited from
PCL) between the value and policy along observed trajectories (Nachum et al., 2017b). Although
policy optimization by minimizing path inconsistency error can efficiently utilize off-policy data, this
approach loses the desirable monotonic improvement guarantee.
In terms of existing theoretical analyses, similar monotonic improvement guarantee exists for TRPO,
but only for an impractical formulation.4 Here, by contrast, we use the lift-and-project reformulation to
establish a monotonic improvement guarantee in a simple and direct way. MPO provides a guarantee
on the regularized reward of a non-parametric policy, but this depends on the assumption that a
non-convex optimization problem can be solved globally. SAC obtains a similar result with respect
to the optimal achievable Q-values, again relying on an assumption that a non-convex optimization
problem can be solved globally. We have shown that similar results hold in the case of PMD/REPMD,
but here we have also established something stronger: when the projection step cannot be efficiently
solved to global optimality, we have shown that PMD/REPMD still preserve stationary points in their
respective, principled objectives. MPO and SAC have no such guarantee, as far as we are aware.
7	Conclusion and Future Work
We have proposed reversed entropy policy mirror descent (REPMD) as an effective new approach
for policy based reinforcement learning that also guarantees monotonic improvement in a well
motivated objective. We show that the resulting method achieves better exploration than both a
directed exploration strategy (UREX) and undirected maximum entropy exploration (MENT). It will
be interesting to further extend the follow-on PMAC actor-critic framework with further development
of the value function learning approach.
4 For the monotonic improvement guarantee, TRPO must use DKmLax rather than the stanard KL divergence.
10
Under review as a conference paper at ICLR 2019
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin
Riedmiller. Maximum a posteriori policy optimisation. In ICLR, 2018.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Nan Ding and Radu Soricut. Cold-start reinforcement learning with softmax policy gradient. In
Advances in Neural Information Processing Systems, pp. 2817-2826, 2017.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft
updates. arXiv preprint arXiv:1512.08562, 2015.
Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient td algorithms. In UAI, pp. 504-513. Citeseer, 2015.
Sridhar Mahadevan and Bo Liu. Sparse q-learning with mirror descent. arXiv preprint
arXiv:1210.4893, 2012.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.
William H Montgomery and Sergey Levine. Guided policy search via approximate mirror descent.
In Advances in Neural Information Processing Systems, pp. 4008-4016, 2016.
Kevin P Murphy. Machine learning: a probabilistic perspective. Cambridge, MA, 2012.
Ofir Nachum, Mohammad Norouzi, and Dale Schuurmans. Improving policy gradient by exploring
under-appreciated rewards. In ICLR, 2017a.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems, pp. 2772-2782, 2017b.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy trust
region method for continuous control. In ICLR, 2017c.
Arkadii Nemirovskii, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity and
method efficiency in optimization. 1983.
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans,
et al. Reward augmented maximum likelihood for neural structured prediction. In Advances In
Neural Information Processing Systems, pp. 1723-1731, 2016.
11
Under review as a conference paper at ICLR 2019
Art B. Owen. Monte Carlo theory, methods and examples. 2013.
Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational
space control. In Proceedings of the 24th international conference on Machine learning, pp.
745-750. ACM, 2007.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In AAAI, pp.
1607-1612. Atlanta, 2010.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning.
arXiv preprint arXiv:1704.06440, 2017a.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998.
Voot Tangkaratt, Abbas Abdolmaleki, and Masashi Sugiyama. Guide actor-critic for continuous
control. arXiv preprint arXiv:1705.07606, 2017.
Philip S Thomas, William C Dabney, Stephen Giguere, and Sridhar Mahadevan. Projected natural
actor-critic. In Advances in neural information processing systems, pp. 2337-2345, 2013.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
Herke Van Hoof, Jan Peters, and Gerhard Neumann. Learning of non-parametric control policies
with high-dimensional state features. In Artificial Intelligence and Statistics, pp. 995-1003, 2015.
Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Episodic reinforcement learning
by logistic reward-weighted regression. In International Conference on Artificial Neural Networks,
pp. 407-416. Springer, 2008.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229-256, 1992.
Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning
algorithms. Connection Science, 3(3):241-268, 1991.
12
Under review as a conference paper at ICLR 2019
A Proofs
A.1 Proof of Proposition 1
Proof. Notethat -tDkl(∏θ∣∣∏T) = -T EP ∏(ρ) log∏(P) + T EP ∏(ρ)(log∏(ρ)+ r(ρ)∕τ)-
Zn = EP 〜∏θ r(ρ) — TDKL (∏θ IIn) 一 Zn. Note the fact that Zn，T log PP ∏(ρ)exp {r(ρ)∕τ} is
indenpendent of ∏θ given the reference policy n.	□
A.2 Proof of Proposition 2
Proof. (Monotonic Improvement Guarantee) By the definition of πθt+1 , note that
DκL(∏θt+ιk∏T) = min∏θ∈π Dkl(∏θk∏T) ≤ DκL(∏θtk∏T). By expanding the KL divergence
and rearranging terms, wehaveTDKL(πθt+1∣πθt) - P πθt+1 (ρ)r(ρ) ≤ - P πθt (ρ)r(ρ), which
gives EP〜∏θt+1 r(ρ)-
EP 〜∏θtr(ρ) ≥ TDκL (πθt+1 ∣πθt) ≥ 0.
(Global Optimum Guarantee) By the Monotonic Improvement Guarantee proved above, the
sequence EP〜∏θ= r(ρ) is monotonically increased. Let n* be the converged fixed policy, We
show n* is the optimal policy. According to the algorithm, at convergence it must be that
∀n ∈ Π,n = π*,Dkl(∏*k∏T) ≤ Dkl(∏∣∣∏T)∙ Using the same argument above we have
∀n ∈ Π,n = n*, EP〜∏* r(ρ) ≥ EP〜∏ r(ρ). Hence n* is optimal in Π.
(Fixed Points) The stationary point of EP〜∏θ r(ρ) is the ∏θ which satisfies,
dnθ (P) = o
dθ =
P
^⇒ X log ∏θ (ρ) - log ∏θ (ρ) - r(ρ)卜 dd(ρ) = 0	(τ > 0)
^⇒ X [lognθ(P) 一 log {nθ(ρ)eχp {r(ρ)∕τ}}] ∙ dnθZ(P) = 0.
dθ
P
On the other hand, the fixed point of PMD indicates at some iteration t,
nθt = nθt+1 ,
Lift Step * Project Step
where ∏θt --→ n* -----→ ∏θt+ι in Eq. (3),
which means nθt is the solution of the Project Step,
dDκL(nθkn*)	= ο
dθ	θ=θt
^⇒ X [lognθ(P)—logn*(ρ) + 1] ∙ dd(ρ)	= 0
0 Xbg nθ (P)-log(pn% (P)OeXp %)/：} ] I + 11 ∙ T	=0
„	I ∑nθt(p0) eχp {r(P0)∕τ} I	dθ
L	IPO」	θ=θt
(by Lift Step in Eq. (13))
^⇒ X [log nθ(P)	一 log {nθt (P)	eχp	{r(P)/T}}	+	c]	∙	ddθP)	=0,
(12)
(13)
(14)
where we denote c = 1 + log	nθt (P0) eχp {r(P0)∕T}. Note that for c independent ofP, we have,
∑c ∙
P
dnθ (P)
dθ
=C ∙
θ=θt
d ∑P nθ (P)
dθ
d1
=C ∙--
θ=θt	dθ
= 0.
θ=θt
13
Under review as a conference paper at ICLR 2019
Therefore, Eq. (14) is equivalent with,
^⇒
X [log πθ(P) - log {πθt (P) eχp {r(P)/T}}] ∙ TP)	=0.
ρ	θ=θt
(15)
Comparing Eq. (15) with Eq. (12), we have the fixed point condition of PMD is the same as the
definition of the stationary point of EP〜∏θ r(ρ).	□
A.3 Proof of Theorem 1
Proof. (Monotonic Improvement Guarantee) Using	DKL (∏T,τ，k∏θt+ι)
min∏θ∈∏ Dkl(∏T,t，k∏θ) ≤ Dkl(∏T,t，k∏θt) and Jensen,s inequality,
°vn r r(P)+τ logF+ι (P) I
exp I	_i_ o	[
SR(πθt+ι) -SR(πθt) = (τ + T0)logXPeχp nr(ρ)+τ詈πt7ρyo
P	τ+τ
=τ0) log X exp{ ；+T…：! ∙ eχp r T log πθt+ι(P)- T log πθt ⑺
‘	乙 P pvn n r(P)+「log 穴。t(P) o	I	τ+τ 0
P 乙 exp 1	T+T0	f
P
=(T + T0) log X ∏T,τ0 (P) ∙ exp { T log πθt+1 (ρ+-τT log πθt(P) )
P
≥(t + T 0) X ∏J (P) logexp f T log πθt+ι1?二 log 顶(P) }
P
=T X πT,τ 0 (P)log πθt+1(P) = T [DKL(πT,τ 0 kπθt) - DKL(πT,τ 0 kπθt+J] ≥ 0.
P	πθt (P)
(Global Optimum Guarantee) By the Monotonic Improvement Guarantee proved above, the
sequence SR(∏θj is monotonically increased. Let ∏* be the converged fixed policy, We show ∏*
is the optimal policy. According to the algorithm, at convergence it must be that ∀π ∈ Π, π 6=
∏*, Dkl(∏T,t0 k∏*) ≤ Dkl(∏T,t0 k∏). Using the same argument above we have SR(∏*) ≥ SR(∏).
Hence π* is optimal in Π.
(Fixed Points) The stationary point of SR(πθ) is the πθ which satisfies,
dSR(∏θ) _ o
-dθ-=
^⇒
(t + T0) ∙
P QY门 r r (P)+τ log πθ (P) I . T .	1	. dπθ (P)
2j P ɪ	τ+τ0	T τ+τ0 ∏θ (p)	dθ
Pp，exp{此车普3 }
r	∏θ(P)exp{ r(P)-T+Tgn"P)} ɪ d∏θ(P)
3 Pp, ∏θ(P0)exp{ r(p0)-T+Tgπθ(p0) } ^P	dθ
0.
(16)
(T > 0)
0
On the other hand, the fixed point of REPMD indicates at some iteration t,
πθt = πθt+1 ,
L	Lift Step _* Project Step	. τ-, /八
where ∏θt ---------→ 亓「,「，-----------→ ∏θt+ι in Eq. (4),
(17)
14
Under review as a conference paper at ICLR 2019
which means πθt is the solution of the Project Step,
dDκL(∏T,τ 0 k∏θ)
dθ
=0
θ=θt
0 - X πτ,τ0 (P) ∙ ∏Θ1P)
L	∏θt (ρ)exp{ r(ρ)-τ⅛πθt (P) O	1
< ⇒ —	•
T Pρ0 ∏θt (P0)exp{ r(P Y…} πθ(P)
d∏θ (P)
dθ
θ=θt
d∏θ (P)
dθ
θ=θt
(18)
(by Lift Step in Eq. (17))
Comparing Eq. (18) with Eq. (16), we have the fixed point condition of REPMD is the same as the
definition of the stationary point of SR(∏θ ).	□
A.4 Proof of Proposition 3
exp{Φ> θ}
Proof. Note that ∏ = ι> eχP{φ>J9}, where Φ is the feature matrix and θ is the policy parameter.
Simply compute the Hessian matrix of the objective,
d2DKL (πkπθ)	>	T^T v ∩
-----dθ2----- = Φ(∆(∏θ) 一 ∏θ∏θ )Φ 占 0.
Thus DκL(∏k∏θ) is convex in θ.	□
A.5 Proof of Proposition 4
Proof. To prove (i), note that as τ → 0, SR(πθ) → τ0 log ρ exp
{守}
the standard softmax
value. Taking limit on τ0 gives the hardmax value maxρ r(P) as τ0 → 0.
To prove (ii), we have
τ→∞ (τ + T 0)log X exp{ r(P)：T(P)
ρ
].	PP πθ(ρ) eχp{ r(P)-τ'+τg πθ(P) }(r(ρ)-τ0 log πθ (P))
im	L r r r r(ρ)-τ0 log ∏θ (P)-I
T→∞	Eρ ∏θ(P) exp{ w' τ+τo θ'*' }
πθ(P) [r(P) - τ0 log πθ (P)] = E r(P) + τ0H(πθ)
P p	P 〜∏θ
P
As τ0 → 0, SR(∏θ) → EP〜∏θ r(P).
□
B Details of REPMD Learning
This section provides some of the details of learning algorithms for REPMD. We first show the
derivation of the analytic solution of the Lift Step.
Lemma 2. The lift step of Eq. (4) has the following closed form expression:
∏(P)exp{ N"l0og π(P) I
∏ * JP)，----------J-------------L.
,	P ∏(P0)exp n r"；+%MP O
(19)
Proof. Rewrite the objective function defined in Eq. (4),
E Ir(P) 一 τDκL(∏k∏) + τ0H(π) = E 卜(夕)+ Tlog∏(p)] + (τ + T0)H(π),	(20)
P〜∏	P〜π
which is an entropy regularized reshaped reward objective. The optimal policy of this objective can
be obtained by directly applying Lemma 4 of Nachum et al. (2017b), i.e.
r / 、	r γ(p) + τ log ∏(p)〕	/ 、	(r(P) 一 T0 log ∏(p)〕	〜、
πT,τ0 (P) H exp { WT + T WJ j = π (P) exp { WT + 2	..	(21)
15
Under review as a conference paper at ICLR 2019
The next lemma provides the derivation of the gradient estimation of REPMD (Lemma 1).
Proof. Note that
DKL(πT,τ0 kπθ) =	E,	[log πT,τ0 (P) - log πθ (P)]
,	P 〜π* ,	,
τ,τ0
PE∏]π∏UΓ (log πτ,τ0 (P)- log πθ(P))
Therefore, taking gradient on both sides,
1K
VθDKL(πT,τ0 kπθ) ≈ -KE
k=1
πT,τ 0 (Pk)
π(Pk)
Vθ log πθ(Pk)
1 自	∏(Pk)exp{ r(pk)-τ+ lτgn(Pk) }
-F 2,	(~Γλ―Γ∣—— 0' ] vθ log πθ (Pk )
K k=1 礼 Pk) Pρ0 ∏(P0) exp { MP)” MP) }
by 19
1 XX	exp{"k}
K k=ι -K PK=I exP{ωj}
Vθlogπθ(Pk)
K
-X
k=1
exp {ωk }
PjK=1 exp {ωj}
Vθ logπθ(Pk).

□
Recall that in Algorithm 1 the project step is performed by SGD. In our implementation, the end
condition of SGD is controlled by two parameters: E > 0 and F-STEP ∈ {0,1}. First, SGD halts if the
change of the KL divergence is below or equal to e. Second, F_STEP decides the maximum number
of SGD steps. If F_STEP = 1, the maximum number is √t at iteration t; while if F_STEP = 0, there
is no restriction on the maximum number of gradient steps, and stopping condition only depends on E.
C Details of PMAC Learning
To increase the stability of the training, we include a target state value network Vφ, where φ is an
exponentially moving average of the value network weights φ. Different from Eq. (10), the soft
Q-function parameters ψ is then trained to minimize the soft Bellman error using the target state
value network,
L(ψ) = E(S,a,s0)〜D
2 (Qψ (S, a) — [r(S, a) + γVφ(s0)])2
(22)
Our approach also use two soft-Q functions in order to mitigate the overestimation problem caused
by value function approximation (Haarnoja et al., 2018; Fujimoto et al., 2018). Specifically, we apply
two soft-Q function approximations, Qψ1 (S, a) and Qψ2 (S, a), and train them independently. The
minimum of the two Q-functions will be used whenever the soft-Q value is needed.
The next lemma shows that the gradient of Eq. (11) can be computed by importance sampling using
the reference policy,
Lemma 3. The gradient of Eq. (11) is,
v7 Qψ(	Q π7 「甯 「 Q Qψ (S, a) - τ 0 log n(a|s) - Vφ(S) 1]	/ I Jl zɔɔʌ
VθL(θ) = VθEs〜D Ea〜∏ expj」-----------------T + To--------^- l log∏(a∣s)	∙	(23)
16
Under review as a conference paper at ICLR 2019
Algorithm 2 The PMAC algorithm
Input: temperature parameters τ and τ0, lag on target value network α, number of training steps M
1:	Initialize ∏, Vφ, Vφ, Qψι, Qψ? and replay buffer D
2:	For t = 1,2, . . . do
3:	For each environment step do
4:	a 〜∏θ(∙∣s)
5:	Observe s0 and r from environment
6:	DjD∪ {(s,a,r,s0)}
7:	Set ∏ = ∏θ
8:	For i = 1, . . . , M do
9:	Sample a mini-batch of data {(sj, aj, rj, s0j)}jB=1 from D
10:	Compute gradient VθL(θ), VφL(φ), VψιL(ψι), Vψ2L(ψ2)according to Eqs.(9) to (11)
11:	Update parameters θ, φ, ψ1, ψ2 by gradient descent
12:	Update φ by αφ +(1 — α)φ
Proof. Let π(a∣s) = exp {(Q (s,a)+τ ,； ∏0Sls)-Vφ(S) } ,then We have,
VθL(θ) =VθEs〜D y^π(a∣s) logπ(a∣s) — π(a∣s) log∏θ(a|s)
a
=VθEs〜D "- X exp { Qψ(S，a)+ τTog；(a|s)- Vφ(s) } logΠθ(a⑸
a
=VθEs〜D — X∏(a∣s)exp { Qψ(s, CaTJog；(a国_Vφ(s) } log∏(a|s)
a
v7 π-1 L Γ Q Qψ(S,a)— T 0logπ(aIs)- Vφ(S) 1]	/ I Jl
=VθEs〜D Ea〜∏ — exp < ------------T + T0--------- l log∏θ(a∣s)
□
Pseudocode for PMAC is presented in Algorithm 2. The major difference betWeen PMAC and
REPMD in the lift step is that instead of sampling K actions as described in Algorithm 1, PMAC
only samples one action to construct ∏T T‘, due to the fact both the soft-Q and state value function
approximations are adopted. The value function approximations also make PMAC capable of using
off-policy data from a reply buffer. Furthermore, in the project step of PMAC, We use a fixed number
of iteration for SGD, Which is given by an input parameter of the algorithm.
D Stochastic Transition Setting
In Section 1.1, We assume that the state transition function is deterministic for simplicity. For
completeness, We consider the general stochastic transition setting here.
D. 1 Notations and Settings
Recall in Section 1.1, the policy probability of trajectory ρ = (S1, a1, . . . , aT-1, ST) is denoted as
π(ρ) = QtT=-11 π(atISt). We define transition probability of ρ as f(ρ) , QtT=-11 f(Ss+1ISt,at).
The total probability of ρ under policy π and transition f is then pπ,f (ρ) , π(ρ)f (ρ) =
QtT=-11π(atISt)f(Ss+1ISt,at). We use ∆f , {πI Pρpπ,f(ρ) = Pρ π(ρ)f (ρ) = 1, π(ρ) ≥
0, f(ρ) > 0, ∀ρ} to refer to the probabilistic simplex over all possible trajectories. It is obvious that
pπ,f (ρ) = π(ρ) and ∆f = ∆ under deterministic transition setting, i.e., f(ρ) = 1, ∀ρ.
17
Under review as a conference paper at ICLR 2019
D.2 REPMD Optimization Problem
The proposed REPMD algorithm solves Eq. (4) in the deterministic transition setting. In the stochastic
setting, the corresponding problem is,
(Project Step)	arg mm DKL (p∏* 0 ,f ∣∣p∏θ,f),
πθ∈Π	τ,τ0
θ 一	r z x 0	z	(24)
(Lift Step) where n丁丁0 = arg max E	[r(ρ) — T log π(ρ)] — TDKL (p∏,f ∣∣P∏θt ,f).
,	π∈∆f ρ~p∏,f	t
which also recovers Eq. (4) as a special case when f(ρ) = 1, ∀ρ.
Like Eq.(4), ∏T,τ0 in Eq. (24) also has a closed form expression,
Lemma 4. The unconstrained optimal policy of Eq. (24) has the following closed form expression:
πT,τ0 (P)
∏(ρ)exp{ I+% π(P) }
P ∏(P0)f (P0)exp{ r(PO)-T+Tg π(PO) O
Proof. Rewrite the maximization problem in Eq. (24) as (take ∏θt as the reference policy ∏),
maximize〉"^ ∏(ρ)f (ρ)[r(ρ) 一 (T + T0) log π(ρ) + T log π(ρ)]
π
P
subject to	π(ρ)f (ρ) = 1.
P
The KKT condition of the above problem is,
f (ρ) [r(ρ) — (t + T0) log π(ρ) + t log π(ρ) + λ — (T + T0)] = 0, ∀ρ
X π(ρ)f(ρ) = 1.
P
Using f (ρ) > 0, ∀ρ and solving the KKT condition, We obtain the expression of ∏T 丁0.	□
Lemma 4 recovers Lemma 2 as a special case when f(ρ) = 1, ∀ρ.
D.3 Theoretical Analysis
In stochastic transition setting, we define the follow softmax approximated expected reward of πθ
SRf (πθ) , (t + T0) log X f (P) eχp { r(P) +ττ+og Tn(P) ),
P
which recovers SR(Tθ) when f(P) = 1, ∀P. The monotonic improvement property is for SRf(Tθ).
Theorem 2.	Assume that Tθt is the update sequence of the REPMD algorithm in Eq. (24), then
SRf(∏θt+ι) — SRf(∏θt) ≥ o.
18
Under review as a conference paper at ICLR 2019
Proof. Using DKL(P∏τ,τ 0,f l∣p∏θt+1 ,f ) = miη∏θ ∈Π DKL(P∏τ,τ 0,f l∣p∏θ ,f ) ≤ DKL(Pn ;T 0,f ∣∣p∏θt ,f )
and Jensen’s inequality,
SRf(πθt+1)-SRf(πθt)
f (ρ)exp n r(p)+τTZ>1 (P) O
P f (ρ)exp n r(ρ)+⅛(ρ) O
Z oυ X f(ρ)exp { (P)+τ+T0 θt(P) }	J T log πθt+ι (P) - T log πθt (P)
=(T + τ)log ? P f(ρ)exp{ Ja o ∙ exp 1-------------------^T-------------
/ ,	0M	*	τ	T τ log πθt+ι (P) - T log πθt (P)
=(T + τ )log T πτ,τ0 (P)f(P) ∙ exp，------------+ τ + T0------------
ρ
0	*	Tlogπθt+1(P) -Tlogπθt(P)
≥(t + T ) Tnτ,τ0(P)f(P) ∙ logexp <------+ t + T0----------
ρ
τΣπT,τ0 (P)f(P) ∙ log
ρ
πθt+ι (P)
πθt (P)
7∑SπT,τ0 (P)f (P) ∙ log
ρ
πθt+ι (P)f (P)
πθt (P)f (P)
t [DKL(p∏T,τ0 ,f kp∏θt ,f ) - DKL (PnT,τ0,f kp∏θt+ι ,f )] ≥ 0.
□
SRf (πθ) also recovers corresponding performance measures in the stochastic transition setting.
Proposition 5. SRf (πθ) satisfies the following properties:
(i)	SRf (πθ) → maxρ r(P), as T → 0, T0 → 0.
(ii)	SRf (πθ) → E	r(P), as T → ∞, T0 → 0.
P 〜p∏θ,f
Proof. To prove (i), note that as T → 0, SRf (πθ) →
gives the hardmax value maxP r(P) as T0 → 0.
T0 log PPf(P) exp { rTp) }. Taking limit
on T0
To prove (ii), we have
τl→∞ (t + t0) log X f (P) exp J "P) ++og "θ (P) }
P
=lim PP πθ(P)f (P)exp{r(P)-T+τg nθ(P) } (r(P) - T0 log πθ(P))
=T→∞	PP ∏θ(P)f(P)exp{r(P)-T+Tg nθ(P) }
=X∏θ(P)f (P)[r(P) — τ0 log∏θ(P)] = E	r(P) - T0 ∙ E	log∏(P)
J	P 〜p∏θ,f	P 〜p∏θ,f
As T0 → 0, SRf (∏θ) → EP〜pπg,f r(P).
□
D.4 Learning
The REPMD learning process is intact under the stochastic transition setting. Similar with Appendix B,
we can estimate the KL divergence in the projection step of Eq. (24) by drawing K i.i.d. samples
{pi, ..., PK} fromp∏f, i.e., the mixture of ∏ and f, which is exactly the process of sampling from
∏ and interacting with the environment,
DKL(Pjf kpnθ f ) = PjE 0 Jog π*,τ0 (P) - log πθ (P)]
τ,τ0
P 〜E∏,f ⅛T [log πτ,τ0 (P) - log πθ (P)].
(25)
19
Under review as a conference paper at ICLR 2019
We can then approximate the gradient of DKL (p∏*	f ∣∣p∏θ f) by averaging these K samples accord-
τ,τ
ing to Eq. (25).
Theorem 3.	Let ωk = NPk)-T+ T，gπ(ρk). Given K i.i.d. samples {ρι,..., PK} from the reference
policy ∏, we have the following unbiased gradient estimator,
vθ DKL(Pnτ,τ0,fkpπθ,f) ≈-X jPexpL } 口 log πθ(Pk),
(26)
Proof. See the proof of Lemma 1.
Similar argument could be applied for PMAC learning objectives.
E Experiments Details
We describe the algorithmic and mujoco tasks we experimented on as well as details of experimental
setup in this section.
E.1 Algorithmic and Mujoco Tasks
In each algorithmic task, the agent operates on a tape of characters or digits. At each time step,
the agent read one character or digit, and then decide to either move the read pointer one step in
any direction of the tape, or write a character or digit to output. The total reward of each sampled
trajectory is only observed at the end. The goal of each task is:
•	Copy: Copy a sequence of characters to output.
•	DuplicatedInput: Duplicate a sequence of characters.
•	RepeatCopy: Copy a sequence of characters, reverse it, then forward the sequence again.
•	Reverse: Reverse a sequence of characters.
•	ReversedAddition: Observe two numbers in base 3 in little-endian order on a 2 × n grid
tape. The agent should add the two numbers together.
The Mujoco library contains various of continuous control tasks (Todorov et al., 2012). The specific
action dimensions of each problem is summarized in Table 1.
Table 1: Action Dimensions of Mujoco Tasks
Task
Hopper
Walker2d
HalfCheetah
Ant
Humanoid
Action Dimensions
3
6
6
8
17
E.2 Implementation Details
For the synthetic bandit problem, we parameterize the policy by a weight vector θ ∈ R20 . Let
Ω = (ωι,..., ωιo,00o) be the feature matrix. The policy is defined by SOftmax(Ω>θ). The REPMD
parameters used in Fig. 2 are summarized in Table 2.
For the algorithmic tasks, policy is parameterized by a recurrent neural network with LSTM cells of
hidden dimension 256 (Hochreiter & Schmidhuber, 1997). In all algorithms, N distinct environments
are used to generate samples. On each environment, K random trajectories are sampled using the
agent’s policy to estimate gradient according to (7), which gives the batch size N × K in total.
We apply the same batch training setting as in UREX (Nachum et al., 2017a), where N = 40 and
20
Under review as a conference paper at ICLR 2019
Table 2: REPMD Hyperparameters in Synthetic Bandit
Parameter	Values
T	0.1
τ0	0.0
learning rate	0.01
	5∙10-4
F_STEP	0
Table 3: REPMD Hyperparameters in Algorithmic Tasks
	Copy	DuplicatedInput	RepeatCopy	Reverse	ReversedAddition
τ	0.5	0.5	"^0	0.2	0.5
τ0	0.01	0.01	0.01	0.02	0.01
learning rate	0.01	0.01	0.01	0.001	0.001
clip norm	20	20	20	20	20
	0.01	0.01	0.005	0.005	0.005
K = 10. F_STEP of REMPD is set to 1 in all tasks (See Appendix B). The REPMD parameters used
in Fig. 2 are summarized in Table 3.
We use standard gaussian policy for all experimented algorithms in the mujoco tasks. Two layer
fully-connected feed-forward neural networks with hidden dimension 300 and ReLU nonlinearity are
applied to parameterize policy, soft state value, and soft-Q value. We batch size 256 for all algorithms
on all tasks. The lag parameter α of PMAC for target value network update is 0.01, and the number
of training steps is set as M = 100 in all tasks. The other domain-dependent PMAC parameters are
summarized in Table 4.
Table 4: REPMD Hyperparameters in Mujoco Tasks
	Walker2d	Hopper	HalfCheetah	Ant	Humanoid
τ	1.5	-05	0.5	1.0	2.0
τ0	0.2	0.05	0.2	0.1	0.05
ψ learning rate	5∙10-4	5∙10-4	3∙10-4	3∙10-4	3∙10-4
φ learning rate	5∙10-4	5∙10-4	3∙10-4	3∙10-4	3∙10-4
θ learning rate	5∙10-4	5∙10-4	3∙10-4	3∙10-4	3∙10-4
21