Under review as a conference paper at ICLR 2019
Learning to progressively plan
Anonymous authors
Paper under double-blind review
Ab stract
For problem solving, making reactive decisions based on problem description
is fast but inaccurate, while search-based planning using heuristics gives better
solutions but could be exponentially slow. In this paper, we propose a new approach
that improves an existing solution by iteratively picking and rewriting its local
components until convergence. The rewriting policy employs a neural network
trained with reinforcement learning. We evaluate our approach in two domains:
job scheduling and expression simplification. Compared to common effective
heuristics, baseline deep models and search algorithms, our approach efficiently
gives solutions with higher quality.
1	Introduction
In recent years, deep reinforcement learning has achieved strong success in scenarios that are well-
defined and can be precisely and efficiently simulated (e.g., games like Atari (Mnih et al., 2013)).
One typical scenario is model-free control: given the current state (e.g., a ball is falling), a decision is
made reactively to yield maximal rewards (e.g., move the pedal to bounce the ball back).
In more challenging scenarios, a one-off reactive decision is usually insufficient. It is necessary to
plan before a strong decision. As illustrated by many works, especially competitive games like Chess
and Go (Silver et al., 2017), most of the times, search-based policy is stronger than reactive ones.
However, the time consumption can also be exponential, if planning is done from scratch. To address
this issue, previous works use neural networks to predict (and execute) an entire plan from scratch,
given a complete specification of the problem description (Vinyals et al., 2015; Mao et al., 2016;
Graves et al., 2014). While this avoids search, a direct prediction could be very difficult and even
impossible in complicated scenarios.
An alternative approach is to start from an existing plan and iteratively improve. When both state
and action spaces are continuous, many trajectory optimization techniques have been proposed in
the context of robotics and control (MAYNE, 1973; Tassa et al., 2012; Levine & Abbeel, 2014),
which use local gradient information to gradually improve the existing plan. On the other hand, with
discrete state and action spaces, such approaches become inapplicable due to the indifferentiablity.
In this paper, we propose a novel approach of progressive planning in the discrete space, by iteratively
picking a local part of an existing plan and rewriting it. The rewriting decision is made by a neural
network, trained end-to-end with reinforcement learning. During the training, the neural network
learns to find common local patterns in the current solution and replace them with a better choice,
until there is no more improvement.
We apply our approach to two different domains: job scheduling and expression simplification. We
show that our rewriting approach is better than strong heuristics of both domains using multiple met-
rics. For job scheduling, under a controlled setting, we also demonstrate that our model outperforms
DeepRM (Mao et al., 2016), a neural network predicting a holistic scheduling plan, by large margins
especially when more heterogeneous resources lead to more complicated scheduling.
2	Related Work
Trajectory optimization problems in the continuous space have been widely studied in the domain of
robotics and control, and many effective techniques have been proposed (MAYNE, 1973; Bradtke
et al., 1994; Vrabie et al., 2009; Tassa et al., 2012; Levine & Koltun, 2013; Levine & Abbeel,
2014). Typically, local gradient information is leveraged to gradually optimize the existing trajectory.
However, these techniques are not directly applicable to domains with discrete state and action spaces.
In this work, we study approaches of iterative refinement for discrete optimization problems.
1
Under review as a conference paper at ICLR 2019
Job scheduling and resource management problems are ubiquitous and fundamental in computer sys-
tems. Various work have studied these problems from both theoretical and empirical sides (BlazeWicz
et al., 1996; Grandl et al., 2015; Armbrust et al., 2010; Scully et al., 2017; Terekhov et al., 2014;
Mao et al., 2016; Chen et al., 2017). In particular, recent work use deep reinforcement learning
for job scheduling with a simplified setup (Mao et al., 2016; Chen et al., 2017). Existing work
focus on proposing algorithms to construct a schedule from scratch. However, with more complex
configurations, it becomes challenging to make an effective arrangement in this way, as indicated in
our evaluation. Thus, we make an initial step towards tackling the job scheduling problem through
rewriting, and we consider extending our approach to real-world settings as future work.
A recent line of work studies using deep neural networks to discover equivalent expressions (Cai
et al., 2018; Allamanis et al., 2017; Zaremba et al., 2014). In particular, (Cai et al., 2018) trains a
deep neural network to rewrite algebraic expressions. However, they use supervised learning to train
the model, which requires a collection of ground truth rewriting paths, and lacks the capability to
find novel rewriting routines. On the contrary, we use reinforcement learning to train our rewriter for
expression simplification, which mitigates these limitations.
Our rewriting formulation is closely related to other applications such as code optimization (Schkufza
et al., 2013; Chen et al., 2018), theorem proving (Huang et al., 2018; Lederman et al., 2018), and
text simplification (Cohn & Lapata, 2009; Paetzold & Specia, 2013; Feblowitz & Kauchak, 2013).
In addition, our rewriting approach could also be extended to classical combinatorial optimization
problems (Khalil et al., 2017; Bello et al., 2016; Vinyals et al., 2015; Karp, 1972), e.g., Traveling
Salesman Problem (Applegate et al., 2006) and Vertex Cover Problem (Bar-Yehuda & Even, 1981).
3	Problem Setup
In this work, we propose to formulate optimization as a rewriting problem, and find the optimal
solution through an iterative rewriting process. Before we motivate our formulation, we start with
the description of optimization problems of our interest. Typically, an optimization problem can be
defined as follows:
Definition 1 (Optimization problem.) Let S be the space of all valid states in the problem domain,
and c : S → R is the cost function. The goal is to find arg mins∈S c(s).
When S is continuous, gradient-based algorithms are effective ways of finding the optimal solutions.
For example, trajectory optimization techniques such as iLQR leverage the gradient information
to gradually improve the current trajectory towards the optimum (MAYNE, 1973). In contrast, for
discrete S , it is not straightforward to apply similar approaches. A long line of works propose
algorithms to construct from scratch the optimal solution for combinatorial optimization problems
and job scheduling problems (Karp, 1972; Grandl et al., 2015). However, when dealing with more
complicated problems, it is challenging to output an effective solution from the ground up.
In comparison, for many optimization problems, a better alternative approach is to first construct a
feasible solution, then make incremental improvement. This is because (1) a feasible solution is often
easy to find; (2) an existing solution provides a full context for the improvement, which is not the
case if a solution is generated from scratch, (3) many problems, as well as their solutions, have strong
local structures that can be utilized when improving incrementally, and (4) different solutions might
share a common routine towards the optimal.
For example, in job scheduling, it is usually difficult to decide whether a job with a large resource
requirement needs to be postponed; however, from an existing schedule, it is easy to identify what jobs
cause a long waiting time for later ones. In this case, re-scheduling them improves the efficiency.
Therefore, we propose to solve optimization problems through rewriting, as defined formally below.
Definition 2 (Optimization as a rewriting problem.) Let S be the space of all valid states, A be
the rewriting ruleset, c : S → R is the cost function. Suppose st is the state at rewriting iteration t,
we can apply a rewriting rule at ∈ A to gt, where gt ⊂ St, and the rewriting step results in the next
state st+ι = f (st, gt, at). Given an initial state so, our goal is to find a sequence ofrewriting steps
(s0, (g0, a0)), (s1, (g1, a1)), ..., (sT-1, (gT-1, aT -1)), sT that minimizes c(sT).
To tackle a rewriting problem, rule-based rewriters with manually-designed rewriting routines have
been proposed (Halide, 2018). However, manually designing such routines is not a trivial task. An
incomplete set of routines often leads to an inefficient exhaustive search, while a set of kaleidoscopic
routines is often cumbersome to design, hard to maintain and lacks flexibility.
2
Under review as a conference paper at ICLR 2019
In this paper, we propose to train a neural network instead, using reinforcement learning. Recent
advance in deep reinforcement learning suggests the potential of well-trained models to discover
novel effective policies, such as demonstrated in Computer Go (Silver et al., 2017) and video
games (OpenAI, 2018). In our evaluation, we demonstrate that our approach not only mitigates
laborious human efforts, but also enables the model to discover novel rewriting paths from its own
exploration.
In the following sections, we discuss the application of our rewriting approach to two different
domains: job scheduling (as mentioned above) and expression simplification, in which we minimize
the expression length using a well-defined semantics-preserving rewriting ruleset.
3.1	Job Scheduling Problem
We first study the job scheduling problem, and in particular, we consider a simplified problem setup
studied in (Mao et al., 2016) as follows.
Suppose we have a machine with D types of resources. Each job j is specified as gj =
(rj1, rj2, ..., rjD, Sj, Tj), where rj1, rj2, ..., rjD denotes the required portion (between 0 and 1)
of resources for each type, Sj is the arrival timestep, and Tj is the duration. We assume that the
resource requirement is fixed during the entire job execution, each job must run continuously until
finishing, and no preemption is allowed. We adopt an online setting: there is a pending job queue Q
which can hold at most LQ jobs. When a new job arrives, it can either be allocated immediately, or
be added to Q. If Q is already full, to make space for the new job, at least one job in the Q needs
to be scheduled immediately. The goal is to find a time schedule for every job, so that the average
waiting time is as short as possible.
For job scheduling, the only type of rewriting is to re-schedule a job gj and allocate it after another
job gj0 finishes or at its arrival time Sj . Details of a rewriting step is presented in Appendix B.1.
Thus, the size of the rewriting ruleset is |A |= 2LQ, since each job could only switch its scheduling
order with at most LQ of its former and latter jobs respectively.
3.2	Expression Simplification
We also apply our approach to expression simplification domain. In particular, we consider expres-
sions in Halide, a domain-specific language for high-performance image processing (Ragan-Kelley
et al., 2013), which is widely used at scale in multiple products of Google (e.g., YouTube) and Adobe
Photoshop. Simplifying Halide expressions is an important step towards the optimization of the entire
code. To this end, a rule-based rewriter is implemented for the expressions, which is carefully tuned
with manually-designed heuristics 1. The grammar of the expressions considered in the rewriter is
specified in Appendix A. Notice that the grammar includes a more comprehensive operator set than
previous works on finding equivalent expressions, which consider only boolean expressions (Alla-
manis et al., 2017; Evans et al., 2018) or a subset of algorithmic operations (Allamanis et al., 2017).
The rewriter includes hundreds of manually-designed rewriting templates. Given an expression, the
rewriter checks the templates in a pre-designed order, and applies those rewriting templates that
match any sub-expression of the input.
After investigating into the rewriting templates in the rule-based rewriter, we find that a large number
of rewriting templates enumerate specific cases for an uphill rule, which lengthens the expression first
and shortens it later (e.g., “min/max” expansion). Similar to momentum terms in gradient descent for
continuous optimization, such rules are used to escape a local optimum. However, they should only
be applied when the initial expression satisfies certain pre-conditions, which is traditionally specified
by manual design, a cumbersome process that is hard to generalize.
Observing these limitations, we hypothesize that a neural network model has the potential of doing a
better job than the rule-based rewriter. In particular, we propose to only keep the core rewriting
rules in the ruleset, remove all unnecessary pre-conditions, and let the neural network decide which
and when to apply each rewriting rule. In this way, the neural rewriter has a better flexibility than the
rule-based rewriter, because it can learn such rewriting decisions from data, and has the ability of
discovering novel rewriting patterns that are not included in the rule-based rewriter.
4	Neural Rewriter Model
1The code is released in their public repository here: https://github.com/halide/Halide.
3
Under review as a conference paper at ICLR 2019
Figure 1: The framework of the neural rewriter architecture. Here, the score predictor computes
SP(gt), which is the rewriting score; and the rule selector predicts RS(^t), which is the probability
distribution of applying each rewriting rule a ∈ A.
In the following, we present the design of our rewriting model, i.e., Neural Rewriter. We first provide
an overview of our model framework, then present the design details for different applications.
4.1	Model Overview
Figure 1 illustrates the overall framework of our neural rewriter, and we describe the two key
components for rewriting as follows. More details can be found in Appendix C.
Score predictor. Given the state st, the score predictor computes a score SP(gt) for every gt ⊂ st,
which measures the benefit of rewriting gt. A high score indicates that rewriting gt could be desirable.
Rule selector. Given gt ⊂ St to be rewritten, the rule selector predicts a probability distribution
RS(gt) over the entire ruleset A, and selects a rule at ∈ A to apply accordingly.
4.2	Model Details for Job Scheduling Problem
Figure 2a demonstrates the model architecture for job scheduling, and we discuss the details below.
Input embedding. As described in Section 3, each job is specified by gj =
(rj1, rj2, ..., rjD, Sj, Tj). In addition, we define Aj as the schedule time, and Cj = Aj + Tj
as the completion time. We embed each job into a (D × (Tmax + 1) + 1)-dimensional vector ej,
where Tmax is the maximal duration of a job. This vector encodes the information of its attributes
and the machine status during its execution, and we defer the embedding details to Appendix C.1.
We represent each schedule as a directed acyclic graph (DAG), which describes the dependency
among the schedule time of different jobs. Specifically, we denote each job gj as a node in the graph,
and we add an additional node g0 to represent the machine, which has a zero embedding vector
e0. Ifa job gj is scheduled at its arrival time Sj, then we add a directed edge hg0, gji in the graph.
Otherwise, there must exist at least one job gj0 such that Cj0 = Aj. We add an edge hgj0 , gji for
every such job gj 0 to the graph. Figure 3 illustrates an example of the graph construction.
To encode the graphs, we extend the Child-Sum Tree-LSTM architecture in (Tai et al., 2015), which
is similar to the DAG-structured LSTM in (Zhu et al., 2016). Specifically, for a job gj, suppose
(h1, c1), (h2, c2), ..., (hp, cp) are the LSTM states of all parents ofgj, then its LSTM state is
pp
(h, c) = LSTM((	hi,	ci), ej)	(1)
For each node, the d-dimensional hidden state h is used as the embedding for other two components.
Score predictor. This component is a LP -layer fully connected neural network with a hidden size
of Np, and the input to the predictor of job gj is hj .
Rule selector. The rule selector is a LS -layer fully connected neural network with a hidden size of
NS. The rewriting options described in Section 3 is equivalent to moving the current job gj to be a
child of another job gj0 or g0 in the graph, which means allocating job gj after job gj0 finishes or at
its arrival time Sj . Thus, the input to the rule selector not only includes hj , but also hj0 of all other
4
Under review as a conference paper at ICLR 2019
Input Encoder	Score Predictor	Rule Selector
(a)
InputEncoder	ScorePredictor	Rule Selector a】。
St+1
(b)
Figure 2: The instantiation of neural rewriter architectures for different domains: (a) job scheduling;
(b) expression simplification. In (a), St is the dependency graph representation of the job schedule.
Each circle with index greater than 0 represents ajob node, and node 0 is an additional one repre-
senting the machine. Edges in the graph reflect job dependencies. The score predictor selects a job
gt 〜SP(gt) from all job nodes to re-schedule. The rule selector chooses a moving action at ∈ A
for gt, then modifies St to get a new dependency graph st+ι. In (b), St is the expression parse tree,
where each square represents a node in the tree. The set of gt ⊂ st includes every sub-tree rooted at a
non-terminal node, from which the score predictor selects gt to rewrite. Afterwards, the rule selector
predicts a rewriting rule at, then rewrites the sub-tree gt to get the new tree st+ι.
gj0 that could be used for rewriting. The output layer is an |A|-dimensional softmax layer, where
|A |= 2LQ as discussed in Section 3. More details can be found in Appendix C.1.
4.3 Model Details for Expression S implification
We present the instantiation of our neural rewriter framework for expression simplification in Fig-
ure 2b. We mainly discuss the design choices different from the model for job scheduling below.
Input embedding. We use expression parse trees as the input, and employ the N-ary Tree-LSTM
designed in (Tai et al., 2015) as the input encoder to compute the embedding for each node in the tree.
Notice that in this problem, each non-terminal has at most 3 children. Thus, let x be the embedding of
a non-terminal, (hL, cL), (hM, cM), (hR, cR) be the LSTM states maintained by its children nodes,
the LSTM state of the non-terminal node is computed as
(h, c) = LSTM(([hL; hM; hR], [cL; cM; cR]), x)	(2)
Where [a; b] denotes the concatenation of vectors a and b. For non-terminals with less than 3 children,
the corresponding LSTM states are set to be zero.
Score predictor. The score predictor is a LP -layer fully connected neural network with a hidden
size of NP . For each sub-tree gi , its input to the score predictor is represented as a 2d-dimensional
vector [h0 ; hi], where h0 embeds the entire tree. More details can be found in Appendix C.2.
5
Under review as a conference paper at ICLR 2019
Figure 3: An example to illustrate the graph construction approach for the job scheduling problem.
Node 0 is an additional node representing the machine.
Rule selector. The rule selector is a LS -layer fully connected neural network with a hidden size of
NS, and its input format is the same as the score predictor.
4.4 Training Details
Let (so, (go, a。))，(si, (^ι, aι)),…，(ST-ι, (gτ-ι, aτ-ι)), ST be the rewriting sequence in the for-
ward pass, We define the reward function r(st, (gt, at)) as follows. For job scheduling prob-
lem, r(st, (gt,at)) = C(St) — c(st+ι). For expression simplification problem, r(st, (gt,at)) =
maxt+ι≤k≤τ(γk-t(c(st) - C(Sk)))∕c(gt), where Y is a discount factor. This reward function is
designed to credit uphill rules that could lead to a simplified expression in the end. More discussion
about the forward pass algorithm and reward design can be found in Appendix D.
The reward function r(st,(gt, at)) is also used as the training target of SP(gt). Specifically, let θ be
the model parameters, then the loss function of the score predictor is
1 T-1
LSP (θ) = T E(r(st, (gt, at)) - Sp(gt))2
T t=o
(3)
To train the rule selector, we employ the Advantage Actor-Critic algorithm (Sutton et al., 1998), and
weuse SP(gt) as the critic. In particular, let the advantage function A(st, (gt, at)) = r(st, (gt, at)) 一
SP(gt), then the loss function of the rule selector is
T-1
LRS(θ) = - X A(st, (gt, at)) log RS(gt)
t=o
(4)
The overall loss function is L(θ) = LRS(θ) + αLSP (θ), where α is a hyper-parameter.
5	Experiments
In this section, we present the evaluation results of both job scheduling and expression simplification
problems. All neural network models in our evaluation are implemented in PyTorch (Paszke et al.,
2017). To calculate the inference time during testing, we run all algorithms on the same server
equipped with 2 Quadro GP100 GPUs and 80 CPU cores. Only 1 GPU is used when evaluating
neural network models, and 4 CPU cores are used for search algorithms . For both tasks, we set the
timeout of search algorithms to be 10 times as long as the timeout of our neural rewriter.
6
Under review as a conference paper at ICLR 2019
D	2	5	10	20
Shortest Job First	4.80	5.83	5.58	5.00
Shortest First Search	4.25	5.05	5.54	4.98
DeepRM (Mao et al., 2016)	2.81	6.52	9.20	10.18
Neural Rewriter (Ours)	2.80	3.36	4.50	4.63
OPtim (LoWer bound)	2.57	3.02	4.08	4.26
Earliest Job First (Upper bound)	11.11	13.62	22.13	24.23
Table 1: Experimental results of the job scheduling problem on the test set. For each approach, we
report the average slowdown of the jobs with different number of resource types D.
5.1	Job Scheduling Problem
5.1.1	Evaluation Setup
We randomly generate 100K job sequences, and use 80K for training, 10K for validation, and
10K for testing. We use an online setting where jobs arrive on the fly with a pending job queue of
length LQ = 10. When the number of resource types D = 2, we follow the same setup as in (Mao
et al., 2016). The maximal job duration Tmax = 15, and the latest job arrival time is Smax = 50.
With larger D, except changing the resource requirement of each job to include more resource types,
other configurations stay the same. We use average job slowdown as our evaluation metric, which is
computed by (Cj - Sj)/Tj. This metric is also used in (Mao et al., 2016).
5.1.2	Model Configuration
For our neural rewriter model, we provide an initial schedule that allocates jobs in a first-come-first-
serve manner for every job sequence, then feed it to the neural rewriter for refinement. Such an
initial schedule is intuitive to design, easy to compute with a negligible overhead, while is much less
effective than the optimal solution, as we will demonstrate in Table 1.
We compare our neural rewriter model with two kinds of baselines. The first kind of baselines use
manually designed heuristics: Shortest Job First (SJF) always allocates the shortest job in the pending
job queue at each timestep, also used as a baseline in (Mao et al., 2016). Shortest First Search
searches over the shortest k jobs to schedule at each timestep, and returns the optimal one. We find
that other heuristic-based baselines used in (Mao et al., 2016) generally perform worse than SJF,
especially with large D. Thus, we omit the comparison.
The second kind of baselines use a deep neural network to construct the job schedule from scratch.
We re-implement DeepRM (Mao et al., 2016), a neural network trained with reinforcement learning,
and test it on larger D. For a fair comparison, we tune the hyper-parameters of DeepRM for best
performance.
To measure the optimality of these algorithms, we also compute the following empirical bounds.
Earliest Job First (EJF) schedules each job by their arrival time. This provides an upper bound of the
average job slowdown. We also use this algorithm to generate initial schedules for the neural rewriter.
Optim assumes an offline setting: the entire job sequence is available before scheduling. This leads to
a better algorithm that sorts jobs’ duration at the earliest time runnable on the machine. It gives a
lower bound of the average job slowdown in an idealized setting.
5.1.3	Results
Table 1 presents the results for the job scheduling problem. Our neural rewriter model outperforms
both heuristic algorithms and the baseline neural network DeepRM. In particular, we observe that
while the performance of DeepRM and our neural rewriter are similar when D = 2, with larger
D, DeepRM starts to perform worse than heuristic-based algorithms, which is consistent with our
hypothesis that it becomes challenging to design a schedule from scratch when the environment
becomes more complex. On the other hand, our neural rewriter could capture the bottleneck of
an existing schedule that limits its efficiency, then progressively refine it to obtain a better one.
Meanwhile, our results are also closer to the empirical lower bound computed by the Optim algorithm,
which further demonstrates the effectiveness of our rewriting approach. More results and discussion
can be found in Appendix E.
7
Under review as a conference paper at ICLR 2019
NUmber of expressions in the dataset	Length of expressions	Size of expression parse trees
Total: 1.36M Training/Val/TeSt: 1.09M/136K/136K	-Average: 106.84- Min/Max:10/579	Average: 27.39 Min/Max:3/100
Table 2: Statistics of the dataset for expression simplification.
	Average expression length reduction	Average tree size reduction
HaIide Rule-based ReWriter	36?T3	9.68
Heuristic Search	43.27	12.09
Neural Rewriter (Ours)	46.98	13.53
Z3 Solver	50.81	15.82	—
Table 3: Experimental results of the Halide expression simplification task on the test set. Note that
the Z3 solver can perform rewriting steps that are not included in the Halide ruleset.
5.2	Expression Simplification
5.2.1	Evaluation Setup
To construct the dataset, we first generate random pipelines using the generator in the Halide
repository 2, then extract expressions from them. We filter out those expressions that can not
be further rewritten, then split the rest into 8/1/1 for training/validation/test set respectively. We
summarize the statistics of the dataset in Table 2, and more details can be found in Appendix A.
We measure the following metrics in our evaluation: (1) Average expression length reduction, which
is the length reduced from the initial expression to the rewritten one, and the length is defined as the
number of characters in the expression; (2) Average tree size reduction, which is the number of nodes
decreased from the initial expression parse tree to the rewritten one.
5.2.2	Model Configuration
We discuss the ruleset design of our neural rewriter as follows. We look into the Halide rewriting
ruleset with hundreds of templates, and for those templates that can not be further simplified, e.g.,
reducing v - v into 0 as in Figure 2b, we simply include them in our ruleset. As discussed in Section 3,
a large number of templates are enumerating pre-conditions to apply uphill rules. For these templates,
we remove the manually-designed pre-conditions, and only include the uphill rules themselves in the
ruleset. In this way, we manually build a ruleset with |A |= 19 categories of rewriting rules. More
details about the ruleset and rewriting process can be found in Appendix B.2.
We examine the effectiveness of our neural rewriter against two kinds of baselines. The first kind
of baselines are heuristic-based rewriting approaches, including the rule-based Halide rewriter
(Section 3) and a heuristic search algorithm using our ruleset only. In each rewriting iteration, the
search algorithm first enumerates all combinations of rewriting node and rule, then selects the top k
shortest resulted expressions to be rewritten in the next iteration.
In addition, We evaluate Z3 solver on our dataset (De MoUra & Bj0rner, 2008). Z3 solver is a high-
performance theorem prover developed by Microsoft Research. Its simplifier works by traversing
each sub-formula in the input expression and invoking the solver to find a simpler equivalent one
to replace it. Therefore, the simplification steps performed by this solver may not be included in
the Halide ruleset, Which makes it a strong baseline to compare With. We set the timeout to be 10
seconds for each input expression, and We find that the results are not significantly better With a
longer timeout.
5.2.3	Results
Table 3 presents the main results of our expression simplification problem. We can observe that
our neural reWriter model outperforms both the rule-based reWriter and the heuristic search by
a large margin. In particular, our neural reWriter could reduce the expression length and parse
tree size by around 45% on average; meanWhile, compared to the rule-based reWriter, our model
further reduces the average expression length and tree size by 30% and 40% respectively. We
observe that the main performance gain comes from learning to apply uphill rules appropriately
in Ways that are not included in the manually-designed templates. For example, consider the
2https://github.com/halide/Halide/tree/new_autoschedule_with_new_
simplifier/apps/random_pipeline.
8
Under review as a conference paper at ICLR 2019
expression 5 ≤ max(max(v0, 3) + 3, max(v1, v2)), which could be reduced to True by expanding
max(max(v0, 3) + 3, max(v1, v2)) and max(v0, 3). Using a rule-based rewriter would require the
need of specifying the pre-conditions recursively, which becomes prohibitive when the expressions
become more complex. On the other hand, heuristic search may not be able to find the correct order
of expanding the right hand size of the expression when more “min/max” are included, which would
make the search less efficient. Meanwhile, the performance of our neural rewriter is also much closer
to the performance of Z3 solver, which could perform rewriting steps that are not included in the
Halide ruleset. More results can be found in Appendix F.
6	Conclusion
In this work, we propose to formulate optimization as a rewriting problem, and solve the problem
by iteratively rewriting an existing solution towards the optimum. We utilize deep reinforcement
learning to train our neural rewriter. In our evaluation, we demonstrate the effectiveness of our neural
rewriter on job scheduling and expression simplification problems, where our model outperforms
heuristic-based algorithms and baseline deep neural networks that generate an entire solution directly.
Meanwhile, we observe that since our approach is based on local rewriting, it could become time-
consuming when large changes are needed. In extreme cases where each rewriting step needs to
change the global structure, starting from scratch becomes preferrable. We consider improving the
efficiency of our rewriting approach and extending it to more complicated scenarios as future work.
References
Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli, and Charles Sutton. Learning
continuous semantic representations of symbolic expressions. In International Conference on
Machine Learning, pp. 80-88, 2017.
David L Applegate, Robert E Bixby, Vasek Chvatal, and William J Cook. The traveling salesman
problem: a computational study. Princeton university press, 2006.
Michael Armbrust, Armando Fox, Rean Griffith, Anthony D Joseph, Randy Katz, Andy Konwin-
ski, Gunho Lee, David Patterson, Ariel Rabkin, Ion Stoica, et al. A view of cloud computing.
Communications of the ACM, 53(4):50-58, 2010.
Reuven Bar-Yehuda and Shimon Even. A linear-time approximation algorithm for the weighted
vertex cover problem. Journal of Algorithms, 2(2):198-203, 1981.
Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial
optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.
Jacek Blazewicz, Wolfgang Domschke, and ErWin Pesch. Thejob shop scheduling problem: Con-
ventional and new solution techniques. European journal of operational research, 93(1):1-33,
1996.
Steven J Bradtke, B Erik Ydstie, and Andrew G Barto. Adaptive linear quadratic control using policy
iteration. In Proceedings of the American control conference, volume 3, pp. 3475-3475. Citeseer,
1994.
Cheng-Hao Cai, Yanyan Xu, Dengfeng Ke, and Kaile Su. Learning of human-like algebraic reasoning
using deep feedforward neural networks. Biologically Inspired Cognitive Architectures, 25:43-50,
2018.
Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos Guestrin,
and Arvind Krishnamurthy. Learning to optimize tensor programs. NIPS, 2018.
Weijia Chen, Yuedong Xu, and Xiaofeng Wu. Deep reinforcement learning for multi-resource
multi-machine job scheduling. arXiv preprint arXiv:1711.07440, 2017.
Trevor Anthony Cohn and Mirella Lapata. Sentence compression as tree transduction. Journal of
Artificial Intelligence Research, 34:637-674, 2009.
Leonardo De Moura and Nikolaj Bj0rner. Z3: An efficient smt solver. In International conference on
Tools and Algorithms for the Construction and Analysis of Systems, pp. 337-340. Springer, 2008.
9
Under review as a conference paper at ICLR 2019
Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. Can neural
networks understand logical entailment? ICLR, 2018.
Dan Feblowitz and David Kauchak. Sentence simplification as tree transduction. In Proceedings of
the Second Workshop on Predicting and Improving Text Readability for Target Reader Populations,
pp.1-10, 2013.
Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula, Sriram Rao, and Aditya Akella. Multi-
resource packing for cluster schedulers. ACM SIGCOMM Computer Communication Review, 44
(4):455-466, 2015.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
Halide. Halide simplifier. https://github.com/halide/Halide, 2018.
Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever. Gamepad: A learning environment
for theorem proving. arXiv preprint arXiv:1806.00608, 2018.
Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer computa-
tions, pp. 85-103. Springer, 1972.
Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6348-6358, 2017.
Gil Lederman, Markus N Rabe, and Sanjit A Seshia. Learning heuristics for automated reasoning
through deep reinforcement learning. arXiv preprint arXiv:1807.08058, 2018.
Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under
unknown dynamics. In Advances in Neural Information Processing Systems, pp. 1071-1079, 2014.
Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine
Learning, pp. 1-9, 2013.
Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Resource management
with deep reinforcement learning. In Proceedings of the 15th ACM Workshop on Hot Topics in
Networks, pp. 50-56. ACM, 2016.
DAVID Q MAYNE. Differential dynamic programming-a unified approach to the optimization of
dynamic systems. In Control and Dynamic Systems, volume 10, pp. 179-254. Elsevier, 1973.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
OpenAI. Openai dota 2 bot. https://openai.com/the-international/, 2018.
Gustavo H Paetzold and Lucia Specia. Text simplification as tree transduction. In Proceedings of the
9th Brazilian Symposium in Information and Human Language Technology, 2013.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.
Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Fredo Durand, and Saman
Amarasinghe. Halide: a language and compiler for optimizing parallelism, locality, and recompu-
tation in image processing pipelines. ACM SIGPLAN Notices, 48(6):519-530, 2013.
Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. In ACM SIGARCH
Computer Architecture News, volume 41, pp. 305-316. ACM, 2013.
Ziv Scully, Guy Blelloch, Mor Harchol-Balter, and Alan Scheller-Wolf. Optimally scheduling jobs
with multiple tasks. ACM SIGMETRICS Performance Evaluation Review, 45(2):36-38, 2017.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354, 2017.
10
Under review as a conference paper at ICLR 2019
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. 1998.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations
from tree-structured long short-term memory networks. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, 2015.
Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behaviors
through online trajectory optimization. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
International Conference on, pp. 4906-4913. IEEE, 2012.
Daria Terekhov, Douglas G Down, and J Christopher Beck. Queueing-theoretic approaches for
dynamic scheduling: a survey. Surveys in Operations Research and Management Science, 19(2):
105-129, 2014.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural
Information Processing Systems, pp. 2692-2700, 2015.
Draguna Vrabie, O Pastravanu, Murad Abu-Khalaf, and Frank L Lewis. Adaptive optimal control for
continuous-time linear systems based on policy iteration. Automatica, 45(2):477-484, 2009.
Wojciech Zaremba, Karol Kurach, and Rob Fergus. Learning to discover efficient mathematical
identities. In Advances in Neural Information Processing Systems, pp. 1278-1286, 2014.
Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. Dag-structured long short-term memory for
semantic compositionality. In Proceedings of the 2016 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, pp. 917-926,
2016.
11
Under review as a conference paper at ICLR 2019
<Expr> ::= <AlgExpr> | <BoolExpr>
<BoolExpr> ::= <AlgExpr> <<AlgExpr>
| <AlgExpr> <= <AlgExpr>
| <AlgExpr> == <AlgExpr>
|	(!<BoolExpr>)
|	(<BoolExpr> && <BoolExpr>)
|	(<BoolExpr> || <BoolExpr>)
<AlgExpr> ::=	<Term>
|	(<AlgExpr> + <Term>)
|	(<AlgExpr> - <Term>)
|	(<AlgExpr> * <Term>)
|	(<AlgExpr> / <Term>)
|	(<AlgExpr> % <Term>)
<Term> ::= <Var> | <Const>
|	max(<AlgExpr>, <AlgExpr>)
|	min(<AlgExpr>, <AlgExpr>)
|	select(<BoolExpr>, <AlgExpr>, <AlgExpr>)
Figure 4: Grammar of the Halide expressions in our evaluation. “select (c, e1, e2)” means that
when the condition c is satisfied, this term is equal to e1, otherwise is equal to e2. In our dataset, all
constants are integers ranging in [-1024, 1024], and variables are from the set {v0, v1, ..., v12}.
Algorithm 1 Algorithm of a Single Rewriting Step for Job Scheduling Problem
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
function REWRITE(gj , gj0 , st)
if Cj0 < Sj or Cj0 == Aj then
return S
end if
if j0 6= 0 then A0j = Cj 0 else A0j = Sj fi
Cj0 = A0j +Tj
//Resolve potential resource occupation overflow within [A0j , Cj0]
J = all jobs in st except gj that are scheduled within [A0j , Cj0]
Sort J in the topological order
for gi ∈ J do
A0i = the earliest time that job gi can be scheduled
Ci0 = A0i + Ti
end for
For gi 6∈ J,A0i=Ai,Ci0=Ci
st+1 = {(A0i,Ci0)}
return st+1
end function
A Grammar of the Halide Expressions
Figure 4 presents the grammar of Halide expressions in our evaluation.
B	More Details on the Rewriting Ruleset
B.1	More Details for Job Scheduling Problem
Algorithm 1 describes a single rewriting step for job scheduling problem.
B.2	More Details for Expression Simplification Problem
More discussions about the uphill rules. A commonly used type of uphill rules is “min/max”
expansion, e.g., min(a, b) < c → a < c||b < c. Dozens of templates in the ruleset of the Halide
rewriter are describing conditions when a “min/max” expression could be simplified. Notice that
although applying this rewriting rule has no benefit in most cases, since it will increase the expression
12
Under review as a conference paper at ICLR 2019
max
Min/Max
Distribution
max
Min/Max Expansion
Figure 5: An example of the rewriting process for Halide expressions. The initial expression is
5 ≤ max(v0,3) + 3, which could be reduced to 1, i.e., True.
Job schedule
Job embedding
r11,r12 = [0.1, 0.7], S1 = 1,T1 = 2,A1 = 1
Job 1	1-----------------------1
r21,r22 = [0.2, 0.5], S2 = 2,T2 = 3,A2=3
Job 2	I__________
r31,r32 = [0.6, 0.1], S3 = 3,T3 = 1,A3=3
Job 3	I__________
el = [0.1, 0.7, 0.1, 0.7, 0.1, 0.7, 0.0,…，0.0, 1.0]
e2 = [0.2, 0.5, 0.8, 0.6, 0.2, 0.5, 0.2, 0.5, 0.0,…,0.0, 4/3]
_______________I
e3 = [0.6, 0.1, 0.8, 0.6, 0.0,…，0.0,1.0]
1
2	3
4	5
6
Time
Figure 6: An example to illustrate the job embedding approach for the job scheduling problem.
length, it is necessary to include it in the ruleset, because when either a < c or b < c is always
true, expanding the “min” term could reduce the entire expression to a tautology, which ends up
simplifying the entire expression. Figure 5 shows an example of the rewriting process using uphill
rules properly.
C More Details on Model Architectures
C.1 More Details for Job Scheduling Problem
Job embedding. We describe the details of job embedding as follows. Consider a job gj =
(rj1, rj2, ..., rjD, Sj, Tj). We denote the amount of resources occupied by all jobs at each timestep t
as rt0 = (rt01, rt02, ..., rt0D). Each job gj is represented as a (D × (Tmax + 1) + 1)-dimensional vector,
where the first D dimensions of the vector are (rj1, rj2, ..., rjD), representing its resource requirement.
The following D × Tj dimensions of the vector are the concatenation of rA0 , rA0 +1 , ..., rA0 +T -1,
which describes the machine usage during the execution of the job gj . When Tj < Tmax , the
following D × (Tmax - Tj ) dimensions are zero. The last dimension of the embedding vector is
the slowdown of the job in current schedule. The definition of the slowdown is the same as in (Mao
et al., 2016), which is computed by (Cj - Sj)/Tj, where Cj = Aj + Tj is the completion time.
We denote the embedding of each job gj as ej . Figure 6 shows an example of our job embedding
approach.
13
Under review as a conference paper at ICLR 2019
Algorithm 2 Forward Pass Algorithm for the Neural Rewriter during Training
Require: initial states0, hyper-parameters β, E, pc, Titer, Tsp, Trs
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
for t = 0 → Titer - 1 do
for i = 1 → Tsp do
Sample gt 〜exp(β ∙ SP(gt)), where gt ⊂ St
if SP(gt) < E then
Re-sample g0 〜exp(β ∙ SP (gt)) with a probability of 1 - Pc
if Re-sampling is not performed then break fi
else
break
end if
end for
for i = 1 → Trs do
Sample at 〜RS(gt)
if at can be applied to (st, gt) then break fi
end for
if at does not applied to (st, gt) then break fi
st+1 = f(st,gt,at)
end for
Rule selector. The rule selector has two modules. The first module is a LS -layer fully connected
neural network with a hidden size of NS . For each job gj , let Nj be the number of jobs that could
be the parent of gj, and {gj0 } denotes the set of such jobs. For each gj0 , the input is [hj; hj0 ], and
this module computes a d-dimensional vector h0k to encode such a pair of jobs. The second module
of the rule selector is another LS -layer fully connected neural network with a hidden size of NS .
For this module, the input is an (|A| × d)-dimensional vector [h01; h02; ...; h0|A|], where |A |= 2LQ.
When Nj < |A|, h0N +1, h0N +2, ..., h|0A| are set to be zero. The output layer of this module is an
|A|-dimensional softmax layer, which predicts the probability of each different move ofgj.
C.2 Model Details for Expression S implification
Input representation. As discussed in Section 4, for each sub-tree gi , its input to both the score
predictor and the rule selector is represented as a 2d-dimensional vector [h0 ; hi], where h0 is the
embedding of the root node encoding the entire tree. The reason why we include h0 in the input is
that looking at the sub-tree itself is sometimes insufficient to determine whether it is beneficial to
perform the rewriting. For example, consider the expression max(a, b) + 2 < a + 2, by looking at
the sub-expression max(a, b) + 2 itself, it does not seem necessary to rewrite it as max(a + 2, b + 2).
However, given the entire expression, we can observe that this rewriting is an important step towards
the simplification, since the resulted expression max(a + 2, b + 2) < a + 2 could be reduced to
f alse. We have tried other approaches of combining the parent information into the input, but we
find that including the embedding of the entire tree is the most efficient way.
C.3 Model hyper-parameters
For both tasks, LS = LP = 1, NS = NP = 256, d = 512.
D	More Details on Training
Algorithm 2 presents the details of the forward pass during training. The forward pass during
evaluation is similar, except that we compute ^ and at as gt = argmaxgt SP(gt) and at =
argmaXa(RS(^t)), and the inference immediately terminates when SP(gt) < E or at does not
apply.
Reward design. For job scheduling problem, we simply define the reward function as
r(st, (gt, at)) = c(st) — c(st+ι). The same reward function does not apply to the expression
simplification problem, since it would always give a negative reward for uphill rewriting rules such as
expanding a “min/max”. Thus, for expression simplification problem, we modify the reward function
to r(st,(gt, at)) = maxt+ι≤k≤τ(γk-t(c(st) - C(Sk)))∕c(gt), where Y is a discount factor. This
14
Under review as a conference paper at ICLR 2019
Initial average slow down	≤ 10	10 - 25	> 25
Final average slow down	4.49	4.61	4.78
Results from Table 1			
Shortest Job First Shortest First Search DeepRM (Mao et al., 2016) Neural Rewriter (Ours)	5.00 4.98 10.18 4.63		
Optim (Lower bound) Earliest Job First (Upper bound)	4.26 	24.23			
Table 4: Experimental results of the job scheduling problem using initial schedules with different
average slow down. The number of resource types D = 20. We also include the results from Table 1
for reference.
design of reward function would assign a positive value to an uphill rule if it is included in a path that
results in a simplified expression in the end. We normalize the reward by c(gt), so that the reward
function is bounded. In our evaluation, we set γ = 0.9.
Hyper-parameters. In Algorithm 2, β = 10.0, = 0.0, Tsp = 10, Trs = 10, Titer = 50. pc is
initialized with 0.5, and is decayed by 0.8 for every 1000 timesteps until pc = 0.01, where it is not
decayed anymore. In the training loss function,α = 10.0. The initial learning rate is 1e - 4, and is
decayed by 0.9 for every 1000 timesteps. Batch size is 128. Gradients with L2 norm larger than 5.0
are scaled down to have the norm of 5.0. The model is trained using Adam optimizer. All weights are
initialized uniformly randomly in [-0.1, 0.1].
E More Results for Job Scheduling Problem
To examine how the initial schedules affect the final results, besides earliest-job-first schedules dis-
cussed in Section 5, we also evaluate initial schedules with different average slow down. Specifically,
for each job sequence, we generate different initial schedules by randomly allocating one job at a
time.
In Table 4, we present the results with D = 20 types of resources. For each job sequence, we
randomly generate 10 different initial schedules. We can observe that although the effectiveness of
initial schedules affect the final schedules, the performance is consistently better than other baseline
approaches, which demonstrates that our neural rewriter is able to substantially improve the initial
solution regardless of its quality.
F	More Results for Expression S implification
In Figures 7 and 8, we present some success cases of expression simplification, where we can simplify
better than the Halide rule-based rewriter.
15
Under review as a conference paper at ICLR 2019
(d) Step 3.
(c) Step 2.
(e) Step 4.
Figure 7: The rewriting process that simplifies the expression ((v0 - v1 + 18)/35 * 35 + 35) ≤
v0 -v1 + 119 to 34 ≤ (v0 - v1 + 13)%35.
16
Under review as a conference paper at ICLR 2019
(b) Step 1.
(a) Step 0.
(e) Step 4.
(h) Step 7.
(g) Step 6.
(j) Step 9.
(k) Step 10.
Figure 8: The rewriting process that simplifies the expression ((v0 - v1 + 12)/137 * 137 + 137) ≤
min((v0 - v1 + 149)/137 * 137, v0 - v1 + 13) to 136 ≤ (v0 - v1 + 12)%137.
(c) Step 2.
(d) Step 3.
(f) Step 5.
(i) Step 8.
17