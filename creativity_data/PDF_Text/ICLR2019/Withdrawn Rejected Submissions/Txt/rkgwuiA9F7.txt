Under review as a conference paper at ICLR 2019
Cramer-Wold AutoEncoder
Anonymous authors
Paper under double-blind review
Ab stract
Assessing distance between the true and the sample distribution is a key compo-
nent of many state of the art generative models, such as Wasserstein Autoencoder
(WAE). Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and
WAE-MMD (WAE using maximum mean discrepancy based distance function) we
construct a new generative model - Cramer-Wold AUtoEncoder (CWAE). CWAE
cost function is based upon a characteristic kernel we introduce, called Cramer-
Wold kernel, and has a a simple closed-form in the case of normal prior. As
a consequence, while simplifying the optimization procedure (no need of sam-
pling necessary to evaluate the distance function in the training loop), CWAE
performance matches quantitatively and qualitatively that of WAE-MMD and often
improves upon SWAE.
1 Introduction
Figure 1: Cramer-Wold distance of two sets
obtained as the mean squared L2 -distance
of their smoothed projections on all one-
dimensional lines.
One of the crucial aspects in construction of generative models is devising effective method for
computing and minimizing distance between the true and the model distribution. Originally in
Variational Autencoder (VAE) (Kingma & Welling, 2014) this computation was carried out using
variational methods. An important improvement was brought by the introduction of Wasserstein
metric (Tolstikhin et al., 2017) and the construction of WAE-GAN and WAE-MMD models, which
relax the need for variational methods. WAE-GAN requires a separate optimization problem to be
solved to approximate the used divergence measure, while in WAE-MMD the discriminator has the
closed-form obtained from a characteristic kernel, i.e. one that is injective on distributions Muandet
et al. (2017). A recent contribution to this trend of simplifying the construction of generative models
is Sliced-Wasserstein Autoencoder (SWAE, Kolouri et al. (2018)), where a significantly simpler
AutoEncoder based model based on Wasserstein distance is proposed.
The main innovation of SWAE was the introduction of
the sliced-Wasserstein distance - a fast to estimate met-
ric for comparing two distributions, based on the mean
Wasserstein distance of one-dimensional projections.
However, even in SWAE there is no close analytic for-
mula that would enable computing the distance of the
sample from the standard normal distribution. Conse-
quently in SWAE two types of sampling are needed: (i)
sampling from the prior distribution and (ii) sampling
over one-dimensional projections.
Our main contribution is introduction of the Cramer-
Wold distance between distributions, which has a
closed-form for the distance of a sample from standard
multivariate normal distribution. Its important feature
is that it is given by a characteristic kernel which has
a closed-form given by equation 7 for the product of ra-
dial Gaussians1. We use it to construct an AutoEncoder
based generative model, called Cramer-Wold AutoEn-
coder (CWAE), in which the cost function, for a normal
prior distribution, has a closed analytic formula. Thus
1For more information we refer the reader to Appendix, Sections A and B.
1
Under review as a conference paper at ICLR 2019
CWAE can be seen as a borderline model between SWAE and WAE-MMD, as we use a characteristic
kernel to discriminate distributions, but its formula comes from the application of the sliced approach.
An introduction of sliced approach is given in the following section, the comparison of CWAE and
WAE-MMD is given in Appendix, Section B. We benchmark CWAE against generative models using
a non-parametrized distance functions: WAE-MMD (WAE variant from Tolstikhin et al. (2017) based
on the classical MMD distance) and SWAE. Our results show that, while simplifying the optimization
procedure by forgoing the need to sample data projections or from the prior distribution we obtain an
AutoEncoder based generative model which obtains quantitatively and qualitatively similar results to
WAE-MMD and SWAE models, see Section 5.
Let us now briefly describe the outline of the paper. In the following two sections we introduce
and theoretically investigate the Cramer-Wold distance. However, the reader interested mainly in
the construction of our generative AutoEncoder model can proceed directly to Section 4. Section 5
contains experiments. Conclusions are given in Section 6.
2 Cramer-Wold distance: construction
Motivated by the prevalent use of normal distribution as prior in modern generative models, we
investigate whether it is possible to simplify optimization of such models. As the first step towards
this, in this section we introduce Cramer-Wold distance, which has a simple analytical formula for
computing normality of a high-dimensional sample. On a high level our approach uses the traditional
L2 distance of kernel-based density estimation, computed across multiple single-dimensional pro-
jections of the true data and the output distribution of the model. We base our construction on the
following two popular tricks of the trade:
Sliced-based decomposition of a distribution: Following the footsteps of Kolouri et al. (2018);
DeshPande et al. (2018), the first idea is to leverage the Cramer-Wold Theorem (Cramer & Wold,
1936) and Radon Transform (Deans, 1983) to reduce computing distance between two distributions to
one dimensional calculations. For v in the unit sPhere SD ⊂ RD, the Projection of the set X ⊂ RD
onto the sPace sPanned by v is given by vTX and the Projection of N (m, αI) is N(vTm, α).
Cramer-Wold theorem states that two multivariate distributions can be uniquely identified by their all
one-dimensional Projections. For examPle, to obtain the key comPonent of SWAE model, i.e. the
sliced-Wasserstein distance between two samPles X, Y ∈ RD, we comPute the mean Wasserstein
distance between all one-dimensional Projections:
dW(X,Y)
dW(vTX,vTY) dσD (v),
SD
(1)
where SD denotes the unit sPhere in RD and σD is the normalized surface measure on SD . This
aPProach is effective since the one-dimensional Wasserstein distance between samPles has the closed
form, and therefore to estimate (1) one has to samPle only over the Projections.
Smoothing distributions: Using the sliced-based decomPosition requires us to define distance
between two sets of samPles, in a single dimensional sPace. To this end we will use a trick-of-trade
aPPlied commonly in statistics in order to comPare samPles or distributions which is to first smoothen
(samPle) distribution with a Gaussian kernel. For the samPle R = (ri)i=1..n ⊂ R by its smoothing
with Gaussian kernel N (0, γ) we understand
SmY (R) = n χ N (ri,γ),
i
where by N(m, S) we denote the one-dimensional normal density with mean m and variance S.
This Produces a distribution with regular density, and is commonly used in kernel density estimation.
If R comes from the normal distribution with standard deviation close to one, the asymPtotically
optimal choice of Y is given by the Silverman,s rule of thumb Y =(言)2/5, see Silverman (1986).
For continuous density f, its smoothing smγ(f) is given by the convolution with N(0, γ), and in
the special case of Gaussians we have smγ (N (m, S)) = N(m, S + Y). While in general kernel
density estimations works well only in low-dimensional spaces, this fits the bill for us, as we will
only compute distances on single dimensional projections of the data.
2
Under review as a conference paper at ICLR 2019
Cramer-Wold distance. We are now ready to introduce the Cramer-Wold distance. In a nutshell,
we propose to compute the squared distance between two samples by considering the mean squared
L2 distance between their smoothed projections over all single dimensional subspaces. By the squared
L2 distance between functions f, g : R → R we refer to kf - gk22 = |f (x) - g(x)|2dx. A key
feature of this distance is that it permits a closed-form in the case of normal distribution.
More precisely, the following algorithm fully defines the Cramer-Wold distance between two samples
X = (xi)i=1..n, Y = (yj)j=1..k ⊂ RD (for illustration of Steps 1 and 2 see Figure 1):
1.	given v in the unit sphere S(0, 1) ⊂ RD consider the projections vTX = (vT xi)i=1..n and
vTY = (vTyj)j=1..k,
2.	compute the squared L2 distance of the densities smγ(vTX) and smγ(vTX):
ksmγ(vTX) - smγ(vTY)k22,
3.	to obtain squared Cramer-Wold distance average (integrate) the above formula over all
possible v ∈ SD.
3 Cramer-Wold distance: theory
The key theoretical outcome of this paper is that the result of the computation of the Cramer-Wold
distance from the previous section can be simplified to a closed form solution. Consequently, to
compute the distance of two samples there is no need of finding the optimal transport like in WAE or
the necessity to sample over the projections like in SWAE. For the case of simplicity we provide in
this section the formulas for the distance between two samples and the distance of a sample from the
standard normal density. The general definition of Cramer-Wold metric is presented in Appendix,
Section A.
Theorem 3.1. Let X = (xi)i=1..n, Y = (yj)j =1..n ⊂ RD be given2. We formally define the squared
Cramer-Wold distance by the formula
dc2w(X,Y) :=
SD
ksmγ(vT X) - smγ (vT Y )k22 dσD (v).
Then
d2w (X,Y ) = 2n2√∏γ (X Φd( ⅛k2) + X Φd (kyj-yj0k2) -2 X Φd (kxi-j2)),
ii0	jj0	ij
(2)
where Φd (S) = 1F1 (1; D; -S) and 1F1 is the Kummer's confluent hypergeometric function (see,
e.g., Barnard et al. (1998)). Moreover, φD (s) has the following asymptotic formula valid for D ≥ 20:
Φd(S) ≈ (1+ 2D-3)-1/2.	(3)
To prove the Theorem 3.1 we will need the following crucial technical proposition.
Proposition 3.1. Let z ∈ RD and γ > 0 be given. Then
R N(VTz, Y)(O) dσD(V) = √2∏γφD (kzk2) .	(4)
SD
Proof. By applying orthonormal change of coordinates without loss of generality we may assume
that z = (z1, 0, . . . , 0), and then VTz = z1V1 for V = (V1, . . . , VD). Consequently we get
RSD N(VTz,γ)(0)dσD(V) = RSD N (z1V1, γ)(0) dσD(V).
Making use of the formula for slice integration of functions on spheres (Axler et al., 1992, Corol-
lary A.6) we get:
RSD f dσD = VD-1 R-1(1 -x2)(D-3)/2 RSD-If (x, √1 -x2 Z) dσD-1(Z) dx,
2For clarity of presentation we provide here the formula for the case of samples of equal size.
3
Under review as a conference paper at ICLR 2019
where VK denotes the surface volume of a sphere SK ⊂ RK . Applying the above equality for the
function f (vi,...,vd ) = N (zιvι,γ)(0) and S = z2/(2γ) = ∣∣zk2∕(2γ) We consequently get that
the LHS of (4) simplifies to
VD-I√2∏γ R-I(I - x2)(D-3)/2 exp(-sx2) dχ,
K	[
which completes the proof since VK = 2涔)and f1 exp(-sx2)(1 — x2)(D-3)/2 dx
√π Er(Dy) 1F1 (2; DD; -s).
□
Proof of Theorem 3.1. Directly from the definition of smoothing we obtain that
dCw (χ,γ)= ∩ι n X N(VTxi,γ)-
SD	i
n X N(VTyj ,γ)∣∣2 dσD (v).
j
(5)
Now applying the one-dimensional formula the the L2-scalar product of two Gaussians:
hN(r1, γ1), N(r2, γ2)i2 = N(r1 - r2,γ1 + γ2)(0)
and the equality ∣f - g∣22 = hf, fi2 + hg,gi2 - 2hf,gi2 (where hf,gi2 = R f(x)g(x)dx), we
simplify the squared-L2 norm in the integral of RHS of (5) to
k n P N(VTxi,Y)- 1 P N(VT y ,Y)∣∣2
=n12 hP N(VTxi,γ), P N(VTxi,γ)i2 + n2 hP N(VTyj, γ), P N(VTyj,γ)i2
--n hP N(VTxi, Y), P N(VT yj, Y)i2
ij
=n⅛ P N(vt(xi - xiθ), 2γ)(0) + n2 P N(vt(yj-yjo), 2γ)(0) - W P N(vt(xi - yj), 2γ)(0).
ii0	jj0	ij
Applying directly Proposition 3.1 we obtain formula (2). Proof of the formula for the asymptotics of
the function φ° is provided in the Appendix.	□
Thus to estimate the distance of a given sample X to some prior distribution f, one can follow the
common approach and take the distance between X and a sample from f. As the main theoretical
result of the paper we view the following theorem, which says that in the case of standard Gaussian
multivariate prior, we can completely reduce the need for sampling (we omit the proof since it is
similar to that of Theorem 3.1).
Theorem 3.2. Let X = (xi )i=1..n ⊂ RD be a given sample. We formally define
dc2w(X, N (0, I)) :=
SD
∣smγ(VTX) - smγ (N (0, 1))∣22dσD(V).
Then
d2w(X,N(0,I)) = 2n√π(√ XφD(kxi-xjk ) + √+γ - √Y+τ XφD(2x+⅛7)).
i,j	2 i
(6)
One can easily obtain the general formula for the distance between mixtures of radial distributions.
This follows from the fact that the Cramer-Wold distance is given by a scalar product(•，∙)cw which
has a closed-form for the product of two radial Gaussians:
hN(x, αI),N(y, βI)icw = √2∏(α+β+2γ) φD (⅛y‰).	⑺
The above formula means that Cramer-Wold distance is defined by Cramer-Wold kernel, for more
details see Appendix, Section A.
4	Cramer-Wold AutoEncoder model (CWAE)
This section is devoted to the construction of CWAE. Since we base our construction on the AutoEn-
coder, to establish notation let us formalize it here.
4
Under review as a conference paper at ICLR 2019
AutoEncoder. Let X = (xi)i=1..n ⊂ RN be a given data set. The basic aim of AE is to transport
the data to a typically, but not necessarily, less dimensional latent space Z = RD with reconstruction
error as small as possible. Thus, we search for an encoder E : Rn → Z and decoder D : Z → Rn
functions, which minimize the reconstruction error on the data set X :
1n
MSE(X； E, D) = -EIIxi-D(Exi)∣∣2.
n i=1
AutoEncoder based generative model. CWAE, similarly to WAE, is a classical AutoEncoder
model with modified cost function which forces the model to be generative, i.e. ensures that the
data transported to the latent space comes from the (typically Gaussian) prior f . This statement is
formalized by the following important remark, see also (Tolstikhin et al., 2017).
Remark 4.1. Let X be an N -dimensional random vector from which our data set was drawn, and
let Y be a random vector with a density f on latent Z.
Suppose that we have constructed functions E : RN → Z and D : Z → RN (representing the
encoder and the decoder) such that3
1.	D(Ex) = x for x ∈ image(X),
2.	random vector EX has the distribution f.
Then by the point 1 we obtain that D(E X) = X, and therefore
DY has the same distribution as D(E X) = X.
This means that to produce samples from X we can instead produce samples from Y and map them
by the decoder D.
Since an estimator of the image of the random vector X is given by its sample X, we conclude that a
generative model is correct if it has small reconstruction error and resembles the prior distribution in
the latent. Thus, to construct a generative AutoEncoder model (with Gaussian prior), we add to its
cost function a measure of distance of a given sample from normal distribution.
CWAE cost function. Once the crucial ingredient of CWAE is ready, we can describe its cost
function. To ensure that the data transported to the latent space Z are distributed according to the
standard normal density, we add to the cost function logarithm4 of the Cramer-Wold distance from
standard multivariate normal density dc2w(X, N(0, I)):
cost(X;E,D) = logdc2w(EX,N(0,I)) +MSE(X;E,D).	(8)
Since the use of special functions involved in the formula for Cramer-Wold distance might be
cumbersome, we apply in all experiments (except for the illustrative 2D case) the asymptotic form
(12) of function φD :
2√∏d2w(X) ≈ nl2 X(Yn + ⅛32 )-1/2 + (1+ Yn)T∕2 - 2 X(Yn + 2 + 厂”，
ij	i
where Yn = (34n )2/5 is chosen by the Silverman's rule of thumb (Silverman, 1986).
Comparison with WAE and SWAE models. Finally, let us briefly recapitulate differences be-
tween the introduced CWAE, WAE variants of (Tolstikhin et al., 2017) and SWAE (Kolouri et al.,
2018). In contrast to WAE-MMD and SWAE, CWAE model does not require sampling from normal
distribution (as in WAE-MMD) or over slices (as in SWAE) to evaluate its cost function, and in this
sense uses a closed formula cost function. In contrast to WAE-GAN, our objective does not require a
separately trained neural network to approximate the optimal transport function, thus avoiding pitfalls
of adversarial training. In this paper we are interested in WAE-MMD and SWAE models, which do
not use parameterized distance functions, e.g. trained adversarially like in WAE-GAN. However, in
future work we plan to introduce an adversarial version of CWAE and compare it with WAE-GAN.
3We recall that for function (or in particular random vector) X : Ω → RD by image(X) We denote the set
consisting of all possible values X can attain, i.e. {X(ω) : ω ∈ Ω}.
4We take the logarithm of the Cramer-Wold distance to improve balance between the two terms in the
objective function.
5
Under review as a conference paper at ICLR 2019
5 Experiments
In this section we empirically validate the proposed CWAE model on standard benchmarks for
generative models: CelebA, Cifar-10 and MNIST. We will compare CWAE model with WAE-
MMD (Tolstikhin et al., 2017) and SWAE (Kolouri et al., 2018). As we will see, our results match
those of WAE-MMD, and in some cases improve upon SWAE, while using a simpler to optimize
cost function (see the previous section for a more detailed discussion). The rest of this section is
structured as follows. In Section 5.2 we report results on standard qualitative tests, as well as a visual
investigations of the latent space. In Section 5.3 we will turn our attention to quantitative tests using
Frechet Inception Distance and other metrics.
5.1	Experimentation setup
In the experiment we have used two basic architecture types. Experiments on MNIST were performed
using a feedforward network for both encoder and decoder, and a 20 neuron latent layer, all with
ReLU activations. In case of CIFAR-10 and CelebA data sets we used convolution-deconvolution
architectures. Please refer to Appendix E for full details.
5.2	Qualitative tests
The quality of a generative model is typically evaluated by
examining samples or interpolations. We present such a com-
parison between CWAE with WAE-MMD in Figure 2. We
follow the same procedure as in (Tolstikhin et al., 2017). In
particular, we use the same base neural architecture for both
CWAE and WAE-MMD. We consider for each model (i) in-
terpolation between two random examples from the test set
(leftmost in Figure 2), (ii) reconstruction of a random exam-
ple from the test set (middle column in Figure 2), and finally
a sample reconstructed from a random point sampled from
the prior distribution (right column in Figure 2). The exper-
iment shows that there are no perceptual differences between
CWAE and WAE-MMD generative distribution.
In the next experiment we qualitatively assess normality of
the latent space. This will allow us to ensure that CWAE does
not compromise on the normality of its latent distribution,
Table 1: CWAE achieves similar
FID (lower is better) and sharpness
(higher is better) to WAE-MMD on
the original WAE architecture (see
Appendix E for details).
Algorithm	FID	Sharpness
SWAE	72	0.008
VAE	63	0.003
WAE-MMD	55	0.006
CWAE	54	0.006
WAE-GAN	42	0.006
True data	2	0.020
which recall is part of the cost function for all the models
except AE. We compare CWAE5 with AE, VAE, WAE and SWAE on the MNIST data with using
2-dimensional latent space and a two dimensional Gaussian prior distribution. Results are reported in
Figure 3. As is readily visible, the latent distribution of CWAE is as close, or perhaps even closer, to
the normal distribution than that of the other models. Furthermore, the AutoEncoder presented in the
second figure is noticeably different from a Gaussian distribution, which is to be expected because it
does not optimize for normality in contrast to the other models.
To summarize, both in terms of perceptual quality and satisfying normality objective, CWAE matches
WAE-MMD. The next section will provide more quantitative studies.
5.3	Quantitative tests
In order to quantitatively compare CWAE with other models, in the first experiment we follow
the common methodology and use the Frechet Inception Distance (FID) introduced by Heusel
et al. (2017). Further, we evaluate the sharpness of generated samples using the Laplace filter
following Tolstikhin et al. (2017). Results for CWAE and WAE are summarized in Tab. 1. In
agreement with the qualitative studies, we observe FID and sharpness scores of CWAE to be similar
to WAE-MMD.
5Since (3) is valid for dimensions D ≥ 20, to implement CWAE in 2-dimensional latent space we apply
equality ιF1(1∕2,1, -S) = e-SIo (S) jointly with the approximate formula (Abramowitz & Stegun, 1964,
page 378) for the Bessel function of the first kind I0, for more details see Appendix C.
6
Under review as a conference paper at ICLR 2019
Test interpolation
Test reconstruction
Random sample
Clnn 由 VMwvMɔ
Figure 2: CWAE achieves perceptually similar results to WAE-MMD. Results of WAE-MMD
(first row) and CWAE (second row) on CelebA data set of original WAE-MMD architecture. Left:
Interpolations between two examples from the test distribution (left to right, in each row). Middle:
Reconstruction of examples from the test distribution; odd rows correspond to the real test points.
Right: Reconstructed examples from a random samples from the prior distribution.
Figure 3: Latent distribution of CWAE is close to the normal distribution. Each subfigure presents
points sampled from two-dimensional latent spaces, AE, VAE, WAE, SWAE, and CWAE (left to
right). All trained on the MNIST data set.
Next, by comparing training time between CWAE and other models, we found that for batch-sizes up
to 1024, which covers the range of batch-sizes used typically for training autoencoders, CWAE is
faster (in terms of time spent per batch) than other models. More precisely, CWAE is approximately
2× faster up to 256 batch-size. Details are relegated to the Appendix D.
Finally, motivated by Remark 4.1 we propose a novel method for quantitative assessment of the
models based on their comparison to standard normal distribution in the latent. To achieve this we
have decided to use one of the most popular statistical normality tests, i.e. Mardia tests (Henze, 2002).
Mardia's normality tests are based on verifying whether the skewness b1,D (∙) and kurtosis b2,D (∙) of
a sample X = (xi)i=1..n ⊂ RD:
b1,D (X) = n12 X(XTXk)3, and b2,D (X) = 1 X Ilxj k4,
j,k	j
are close to that of standard normal density. The expected Mardia’s skewness and kurtosis for standard
multivariate normal distribution is 0 and D(D + 2), respectively. To enable easier comparison in
experiments we consider also the value of the normalized Mardia’s kurtosis given by b2,D (X) -
D(D + 2), which equals zero for the standard normal density.
7
Under review as a conference paper at ICLR 2019
Table 2: Comparison between different models output distributions and the normal distribution,
together with reconstruction error. All models outputs except AE are similarly close to the normal
distribution. Normality is assessed by comparing Mardia’s skewness, kurtosis, and the reconstruction
error. For reference FID scores are provided as well (except for MNIST, where it is not defined).
Data set	Method	AE	VAE	WAE	SWAE	CWAE
MNIST	Skewness	659.67	0.49	82.12	55.59	52.83
	Kurtosis (normalized)	749.58	-410.69	35.61	-37.43	95.77
	Reconstruction error	2.10	4.12	2.11	2.11	2.13
CIFAR10	Skewness	11444.35	3.07	1893.10	996.01	171.67
	Kurtosis (normalized)	-2219.50	-4158.33	2346.49	193.15	1943.35
	Reconstruction error	49.67	82.82	45.80	44.84	45.52
	FID score error	400.14	218.43	146.34	145.04	121.16
CelebA	Skewness	59770025.50	22.07	301.71	196.64	59.22
	Kurtosis (normalized)	1363931.65	53.09	942.68	507.39	307.29
	Reconstruction error	139.30	142.46	139.10	138.23	138.54
	FID score error	307.70	95.35	96.30	100.56	97.22
Figure 4: Metrics assessing normality of the model output distributions, during training: FID score,
Mardia’s skewness and kurtosis of models WAE, SWAE and CWAE, on the CelebA test set. The
values given are mean from 5 experiments of differently initialised models together with standard
deviation confidence intervals. Low standard deviation of CWAE indicates its stability during learning.
Optimal value of kurtosis, (i.e. for normal distribution) is given by a dash line.
Results are presented in Figure 4 and Table 2. In Figure 4 we report for CelebA data set the value
of FID score, Mardia’s skewness and kurtosis during learning process of AE, VAE, WAE, SWAE
and CWAE (measured on the validation data set). WAE, SWAE and CWAE models obtain the best
reconstruction error, comparable to AE. VAE model exhibits a sightly worse reconstruction error,
but values of kurtosis and skewness indicating their output is closer to normal distribution. As
expected, the output of AE is far from normal distribution; its kurtosis and skewness grow during
learning. This arguably less standard evaluation, which we hope will find adoption in the community,
serves as yet another evidence that CWAE has strong generative capabilities which at least match
performance of WAE-MMD. Moreover we observe that VAE model’s output distribution is closest to
the normal distribution, at the expense of the reconstruction error, which is reflected by the blurred
reconstructions typically associated with VAE model.
Moreover, motivated by the above approach based on normality tests6 we have verified how the
Cramer-Wold metric works as a Gaussian goodness of fit, however, the results were not satisfactory.
The tests based on Cramer-Wold metric were, in general, in the middle of compared tests (Mardia,
Henze-Zirkler and Royston tests).
On the whole, WAE-MMD and CWAE achieve, practically speaking, the same level of performance
in terms of FID score, sharpness, and our newly introduced normality test. Additionally, CWAE fares
better in many of these metrics than SWAE.
6For more information on the statistical tests based on the kernel approach we refer the reader to (Muandet
et al., 2017, Subsection 3.5).
8
Under review as a conference paper at ICLR 2019
6 Conclusions
In the paper we have presented a new autoencoder based generative model CWAE, which matches
results of WAE-MMD, while using a cost function given by a simple closed analytic formula. We
hope this result will encourage future work in developing simpler to optimize analogs of strong neural
models.
Crucial in the construction of CWAE is the use of the developed Cramer-Wold metric between samples
and distributions, which can be effectively computed for Gaussian mixtures. As a consequence we
obtain a reliable measure of the divergence from normality. Future work could explore use of the
Cramer-Wold distance in other settings, in particular in adversarial models.
References
M. Abramowitz and I.A. Stegun. Handbook of mathematical functions with formulas, graphs, and
mathematical tables, volume 55 of National Bureau of Standards Applied Mathematics Series.
U.S. Government Printing Office, Washington, D.C., 1964.
S. Axler, P. Bourdon, and R. Wade. Harmonic function theory, volume 137 of Graduate Texts in
Mathematics. Springer, New York, 1992.
R.W. Barnard, G. Dahlquist, K. Pearce, L. Reichel, and K.C. Richards. Gram polynomials and the
kummer function. J. Approx. Theory, 94(1):128-143,1998.
H.	Cramer and H. Wold. Some theorems on distribution functions. J. London Math. Soc., 11(4):
290-294, 1936.
S.R. Deans. The Radon Transform and Some of Its Applications. Wiley, New York, 1983.
I.	Deshpande, Z. Zhang, and A. Schwing. Generative modeling using the sliced wasserstein distance.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3483-
3491, 2018.
I.S.	Gradshteyn and I.M. Ryzhik. Table of integrals, series, and products. Elsevier/Academic Press,
Amsterdam, 2015.
N. Henze. Invariant tests for multivariate normality: a critical review. Statist. Papers, 43(4):467-506,
2002.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. Gans trained
by a two time-scale update rule converge to a nash equilibrium. arXiv:1706.08500, 2017.
D.P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.
D.P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2014.
S. Kolouri, Ch.E. Martin, and G.K. Rohde. Sliced-wasserstein autoencoder: an embarrassingly simple
generative model. arXiv:1804.01947, 2018.
K. Muandet, K. Fukumizu, B. Sriperumbudur, B. Scholkopf, et al. Kernel mean embedding of
distributions: A review and beyond. Foundations and TrendsR in Machine Learning, 10(1-2):
1-141, 2017.
B.W. Silverman. Density estimation for statistics and data analysis. Monographs on Statistics and
Applied Probability. Chapman & Hall, London, 1986.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
arXiv preprint arXiv:1711.01558, 2017.
F. Tricomi and A. Erdelyi. The asymptotic expansion of a ratio of gamma functions. Pacific J. Math.,
1:133-142, 1951.
9
Under review as a conference paper at ICLR 2019
Appendices
A Cramer-Wold kernel
In this section we first formally define the Cramer-Wold metric, and later show that it is given by
a characteristic kernel which has closed-form for spherical Gaussians. For more information on
the kernels, and in general kernel embedding of distributions we refer the reader to (Muandet et al.,
2017).
Let us first introduce the general definition of the cw-metric. To do so we generalize the notion of
smoothing for arbitrary measures μ by the formula:
smγ(μ) = μ * N(0, YI),
where * denotes the convolution operator for two measures, and We identify the normal density
N(0, γI) with the measure it introduces. It is well-known that the resulting measure has the density
given by
X → /N(χ,γl)(y)dμ(y).
Clearly
smγ (N (0, αI)) = N(0, (α + γ)I)).
Moreover, by applying the characteristic function one obtains that if the smoothing of two measures
coincide, then the measures also coincide:
smγ (μ) = smγ (μ) =⇒ μ = ν.
(9)
We also need to define the transport of the density by the projection x → vTx, where v is chosen
from the unit sphere SD. The definition is formulated so that if X is a random vector with density f,
then fv is the density of the random vector Xv := vTX. Then we have
fv(r) =	f (z)dD-1(z),
y y:y-rv⊥v
where dD-ι denotes the D 一 1-dimensional Lebesgue measure. In general, if μ is a measure on RD,
then μv is the measure defined on R by the formula
μv(A) = μ({x : vTX ∈ A}).
Since, if a random vector X has the density N(a, γI), then the random variable Xv has the density
N(vTa, α), we directly conclude that
N(a, γI)v = N(vTa, γ).
It is also worth noticing, that due to the fact that the projection of a Gaussian is a Gaussian, the
smoothing and projection operators commute, i.e.:
smγ (μv) = (smγ μ)v.
Given fixed γ > 0, the two above notions allow us to formally define the cw-distance of two measures
μ and V by the formula
d2w(μ, v) = / ∣∣smγ(μv) - smγ(Vv)kL2dσ0(v).
First observe that this implies that cw-distance is given by the kernel function
(10)
k(μ, ν) = h hsmγ (μv ) , smγ (Vv )i L/2 dσ D (V).
SD
Let us now prove that the function dcw defined by equation 10 is a metric (which, in the kernel
function literature means that the kernel is characteristic).
Theorem A.1. The function dcw is a metric.
10
Under review as a conference paper at ICLR 2019
Proof. Since dcw comes from a scalar product, we only need to show that if a distance of two
measures is zero, the measures coincide.
So let μ, V be given measures such that dcw(μ, V) = 0. This implies that
smγ (μv) = smγ (Vv).
By equation 9 this implies that μv = Vv. Since this holds for all V ∈ SD, by the Cramer-Wold
Theorem We obtain that μ = V.	□
Thus we can summarize the above by saying that the Cramer-Wold kernel is a characteristic kernel
Which has the closed-form the scalar product of tWo radial Gaussians given by equation 7:
hN(X,αI),N(y,βI)icw = √2π(α+β+2γ) φD (⅛⅛!).
Remark A.1. Observe, that except for the Gaussian kernel it is the only kernel which has the closed
form for the spherical Gaussians, which as we discuss in the next section is important, as the RBF
(Gaussian) kernels cannot be successfully applied in AutoEncoder based generative models. The
reason is that the derivative of Gaussian decreases to fast, and therefore it does not enforce the
proper learning of the model, see also the comments in (Tolstikhin et al., 2017, Section 4, WAE-GAN
and WAE-MMD specifics).
B CWAE VS WAE-MMD
In this section We are going to compare CWAE model to WAE-MMD. In particular We shoW that
CWAE can be seen as the intersection of the sliced-approach together With MMD-based models.
Since both WAE and CWAE use kernels to discriminate betWeen sample and normal density, to
compare the models We first describe the WAE model.
WAE cost function for a given characteristic kernel k and sample X = (xi)i=1..n ⊂ RD (in the
D-dimensional latent) is given by
WAE cost = MSE + λ ∙ dk(X, Y),
Where Y = (yi)i=1..n is a sample from the standard normal density N(0, I), and d2k(X, Y) denotes
the kernel-based distance between the probability distributions representing X and Y, that is n Pi δχi
and § Pi δyi, where δz denotes the atom Dirac measure at Z ∈ RD. The inverse multiquadratic
kernel k is chosen as default
k-、_ C
(M = C + kχ-yk2,
where in experiments in (Tolstikhin et al., 2017) a value C = 2Dσ2 was used, where σ is the
hyper-parameter denoting the size of the normal density. Thus the model has hyper-parameters λ and
σ, which were chosen to be λ = 10, σ2 = 1 in MNIST, λ = 100, σ2 = 2 in CelebA. Observe that
the hyper-parameters do not depend on the sample size and that, in general, the WAE-MMD model
hyper-parameters have to be chosen by hand.
Now let us describe the CWAE model. CWAE cost function for a sample X = (xi )i=1..n ⊂ RD (in
the D-dimensional latent) is given by
CWAE cost = MSE + log dc2w (X, N(0, I)),
where distance between the sample and standard normal distribution is taken with respect to the
Cramer-Wold kernel with a regularizing hyperparameter γ given by the Silverman’s rule of thumb
(the motivation for such a choice of hyper-parameters is explained in Section 2).
Thus, we have the following differences:
•	Due to the properties of Cramer-Wold kernel, in the distance we are able to substitute
the sample estimation of d2k (X, N(0, I)) given in WAE-MMD by dc2w(X, Y) by its exact
formula.
•	CWAE, as compared to WAE, has no hyper-parameters:
11
Under review as a conference paper at ICLR 2019
1.	In our preliminary experiments we have observed that in many situations (like in the
case of log-likelihood), taking the logarithm of the nonnegative factors of the cost
function, which we aim to minimize to zero, improves the learning process. Motivated
by this, instead of taking the additional weighting hyper-parameter λ (as in WAE-
MMD), whose aim is to balance the MSE and divergence terms, we take the logarithm
of the divergence. Automatically (independently of dimension) balance those terms in
the learning process.
2.	The choice of regularization hyper-parameter is given by the Silverman’s rule of thumb,
and depends on the sample size (contrary to WAE-MMD, where the hyper-parameters
are chosen by hand, and in general do not depend on the sample size).
Summarizing, in CWAE model, contrary to WAE-MMD, we do not have to choose hyper-parameters.
Moreover, since we do not have the noise in the learning process given by the random choice of the
sample Y from N (0, I), the learning should be more stable. As a consequence, see Figure 7, CWAE
in generally learns faster then WAE-MMD, and has smaller standard deviation of the cost-function
during the learning process.
C COMPUTATION OF φD
In this section we consider the estimation of values of the function
φD (S) = 1F1( 2; D; -S) for S ≥ 0,
which is crucial in the formulation for the Cramer-Wold distance. First we will provide its approximate
asymptotic formula valid for dimensions D ≥ 20, and then we shall consider the special case of
D = 2 (see Figure 5)
To do so, let us first recall (Abramowitz & Stegun, 1964, Chapter 13) that the Kummer’s confluent
hypergeometric function 1F1 (denoted also by M) has the following integral representation
ιF1 (a, b, Z) =	rb~~V Z 1 ezuua-1(1 — u)b-a- du,
Γ(a)Γ(b - a) 0
valid for a, b > 0 such that b > a. Since we consider that latent is at least of dimension D ≥ 2, it
follows that
Φd(S)= .ιΓZ)- ι)[ e-suu-1/2(1 -U)D/2-3/2 du.
By making a substitution u = x2, du = 2xdx, we consequently get
φn(s) = 2 ∙_____γ(d∕2)_____ / e-sx2(1 - x2)(D-3)/2 dχ
φD(S) = 2 Γ(1∕2)Γ(D∕2-1∕2) JUe (Ix )	dx
=______r(D/2)_____ /1 e-sx2 (1 - x2)(D-3)/2 dx
=r(1/2)r(D/2-1/2) J J (Ix )	αx.
(11)
Figure 5: Comparison of φD value (red line) with the approximation given by equation 12 (green
line) in the case of dimensions D = 2, 5, 20. Observe that for D = 20, the functions practically
coincide.
12
Under review as a conference paper at ICLR 2019
Proposition C.1. For large7 D we have
Φd(S) ≈ (1 + 2D4-3 )-1/2 forall S ≥ 0.	(12)
Proof. By (11) we have to estimate asymptotics of
φD(S)
Γ(1¾O 11 e-sx2(1-x2)(D-3)/2 dx.
Since for large D, for all x ∈ [-1, 1] we have
(1 - x2)(D-3"2e-sx2 ≈ (1 - x2)(D-3)/2 ∙ (1 - x2)s = (1- x2)s+(D-3)/2,
we get
ΦD(S) ≈ ɪ J (1 - x2)s + (D-3)/2 dx = ɪ ∙ √∏γ⅛⅛ .
r( D-) √π J-I	r( D-) √π	r(s + D2)
To simplify the above We apply the formula (1) from (Tricomi & Erdelyi, 1951):
Γ(z + a)
Γ(z + β)
α-β(1 + (a - B； β - 1) + O(|z|-2)),
With α, β fixed so that α + β = 1 (so only the error term of order O(|z|-2) remains), and get
γ(D =	γ((d∙	-1) +	3)	≈	(D	- 3ʌ1 and γ(s + ID -1) ≈	(S + D - 3「2
Γ(≡) =	Γ((D	- 1) +1)	≈	U	- 4) and Γ(s + D)	≈	(S + T - 4；
Summarizing,
(D - 3 )1/2	,
φD(S) ≈( j D 4 3)i/2 =(1+ 2D-3) 1 2.
(S + τ - 1) /
(13)
□
In general one can obtain the iterative direct formulas for function φD With the use of erf and modified
Bessel functions of the first kind I0 and I1 , but for large D they are of little numerical value. We
consider here only the special case D = 2 since it is used in the paper for illustrative reasons in the
latent for the MNIST data set. Since We have the equality (Gradshteyn & Ryzhik, 2015, (8.406.3)
and (9.215.3)):
02(s) = 1F1(2, 1, -s) = e 2I0 (2),
to practically implement φ2 We apply the approximation of I0 from (AbramoWitz & Stegun, 1964,
page 378) given in the folloWing remark.
Remark C.1. Let S ≥ 0 be arbitrary and lett = S/7.5. Then
φ2(S) ≈
S
e-2 ∙ (1+3.5156229t2 +3.0899424t4 + 1.2067492t6 + .2659732t8 + .0360768t10 + .0045813t12)
for S ∈ [0, 7.5],
< y∣ ∙ (.39894228+.01328592t-1 + .00225319t-2-.00157565t-3 + .0091628t-4-.02057706t-5
十.02635537t-6-.01647633t-7 + .00392377t-8)
for S ≥ 7.5.
D Comparison of learning times
Figure 6 gives comparison of mean learning time for different most frequently used batch-sizes.
Time spent on processing a batch is actually smaller for CWAE for a practical range of batch-sizes
[32, 512]. For batch-sizes larger than 1024, CWAE is sloWer due to its quadratic complexity With
respect to the batch-size. HoWever, We note that batch-sizes larger even than 512 are relatively rarely
used in practice for training autoencoders.
7In practice We can take D ≥ 20.
13
Under review as a conference paper at ICLR 2019
Figure 6: Comparison of mean batch learning time (times are in log-scale) for different algorithms in
seconds, all for the same architecture like the one in Tolstikhin et al. (2017) and all requiring similar
number of epochs to train the full model. This times may differ for computer architectures with
more/less memory on a GPU card.
lθ-ɪ ：
10~2 ：
©Eq 6ussφu OJd uheq
32	64	128	256	512	1024	2048
Batch size
E Architecture details
MNIST (28 × 28 images) an encoder-decoder feedforward architecture:
encoder three feed-forward ReLU layers, 200 neurons each
latent 20-dimensional,
decoder three feed-forward ReLU layers, 200 neurons each.
CelebA (with images centered and cropped to 64×64 with 3 color layers) a convolution-deconvolution
network:
encoder four convolution layers with 4 × 4 filters and 2 × 2 strides (consecutively 32, 32, 64,
and 64 output channels), all ReLU activations,
two dense layers (1024 and 256 ReLU neurons)
latent 64-dimensional,
decoder first two dense layers (256 and 1024 ReLU neurons),
three transposed-convolution layers with 4 × 4 filters with 2 × 2 strides (consecutively
64, 32, 32 channels) with ReLU activation,
transposed-convolution 4 × 4 with 2 × 2 stride, 3 channels, and sigmoid activation.
CIFAR-10 dataset (32× images with three color layers): a convolution-deconvolution network
encoder four convolution layers with 2 × 2 filters, the second one with 2 × 2 strides, other
non-strided (3, 32, 32, and 32 channels) with ReLU activation,
128 ReLU neurons dense layer,
latent with 64 neurons,
decoder two dense ReLU layers with 128 and 8192 neurons,
two transposed-convolution layers with 2 × 2 filters (32 and 32 channels) and ReLU
activation,
a transposed convolution layer with 3 × 3 filter and 2 × 2 strides (32 channels) and
ReLU activation,
a transposed convolution layer with 2 × 2 filter (3 channels) and sigmoid activation.
The last layer returns the reconstructed image. The results for all above architectures are given in
Table 2. All networks were trained with the Adam optimizer (Kingma & Ba, 2014). The hyper-
parameters used were learning rate = 0.001, β1 = 0.9, β2 = 0.999, = 1e - 8. MNIST models
were trained for 500 epochs, both CIFAR-10 and CelebA for 200.
14
Under review as a conference paper at ICLR 2019
Additionally, to have a direct comparison to WAE-MMD model on CelebA, an identical architecture
was used as that in Tolstikhin et al. (2017) utilized for the WAE-MMD model (WAE-GAN architecture
is, naturally, different):
encoder four convolution layers with 5 × 5 filters, each layer followed by a batch normalization
(consecutively 128, 256, 512, and 1024 channels) and ReLU activation,
latent	64-dimensional,
decoder	dense 1024 neuron layer,
three transposed-convolution layers with 5 × 5 filters, and each layer followed by a
batch normalization with ReLU activation (consecutively 512, 256, and 128 channels),
transposed-convolution layer with 5 × 5 filter and 3 channels, clipped output value.
The results for this architecture for CWAE compared to VAE and WAE-MMD models are given in
Table 1.
Similarly to Tolstikhin et al. (2017), models were trained using Adam with for 55 epochs, with the
same optimizer parameters.
15
Under review as a conference paper at ICLR 2019
F Training results for the CelebA dataset
0.100
ω
I 0-075
S。。5。
<
M 0.025
2000
1800
I 1600
⅞
G 1400
1200
5
.0.1.0.0
O O O O O
80Ueωp 山 4MS
00007550
.06,04,02
Ooo
80Ueωp M。
Io»0
。 5
2 1
ajoos QE
IoIo
O 。
1 O
Ooo
5 g!5
7 5 2
SS8UM8*S
Figure 7: CelebA trained CWAE, WAE, and SWAE models with FID score, kurtosis and skewness,
as well as CW-, WAE-, and SWAE-distances. Results are given both related to epochs and learning
time. All values are mean from 5 models trained for each architecture. Confidence intervals represent
the standard deviation. Optimum kurtosis is marked by a dash line.
0.000
100	200	300	400	500	2000 4000 6000 8000 10000 12000 14000
Epochs	Execution time
16
Under review as a conference paper at ICLR 2019
G Training results for the CIFAR 1 0 data s et

200
S?
§175
Q
「150
125
10000
7500
5000
2500
0
7000
ω
'g 6000
t
n
“ 5000
4000
0.100
o 0.075
a
ω
:6 0.050
M
° 0.025
是 0.05
0.00
0.125
8 0.100
c
a
ω 0.075
⅛
< 0.050
S 0.025
100	200	300	400	500 0
Epochs
1000	2000	3000	4000
Execution time
Figure 8: CIFAR10 trained CWAE, WAE, and SWAE models with FID score, kurtosis and skewness,
as well as CW-, WAE-, and SWAE-distances. Results are given both related to epochs and learning
time. All values are mean from 5 models trained for each architecture. Confidence intervals represent
the standard deviation. Optimum kurtosis is marked by a dash line.
17
Under review as a conference paper at ICLR 2019
H Additional figures

Test interpolation
WVMSWVMU
Clnn 由VM
Figure 9: Results of VAE, WAE-MMD, SWAE, and CWAE models trained on
the WAE architecture from Tolstikhin et al. (2017). In “test reconstructions” o
Test reconstruction
Random sample
rows correspond to
the real test points.
18