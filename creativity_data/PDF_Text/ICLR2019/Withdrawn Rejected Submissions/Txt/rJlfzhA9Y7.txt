Under review as a conference paper at ICLR 2019
Distributed Deep Policy Gradient for Compet-
itive Adversarial Environment
Anonymous authors
Paper under double-blind revieW
Ab stract
This Work considers the problem of cooperative learners in partially observable,
stochastic environment, receiving feedback in the form of joint reWard. The paper
presents a flexible multi-agent competitive environment for online training and
direct policy performance comparison. This forms a formal problem of a multi-
agent Reinforcement Learning (RL) under partial observability, Where the goal is
to maximize the score performance measured in a direct confrontation. To address
the complexity of the problem We propose a distributed deep stochastic policy
gradient With individual observations, experience replay, policy transfer, and self-
play.
1	Introduction
The recent development of Artificial Intelligence (AI) and Reinforcement Learning (RL) in particu-
lar demonstrate that AI agents are capable of learning to complete a variety of tasks using a reWard
feedback. It Was shoWn that RL agents utilizing deep Q-learning outperform humans in many tasks
such as Atari Games (Mnih et al., 2013; 2015; Guo et al., 2014; He et al., 2016). Furthermore, ad-
vanced layered solutions involving Alpha-Beta search, Reinforcement Learning, Monte-Carlo Tree
Search, and Learning from Demonstration have been successfully applied to very challenging prob-
lems that previously Were claimed intractable in the nearest future. This includes defeating the
human World champion in the game of Go (Silver et al., 2017) and the Worlds best players in DotA
2 (Bansal et al., 2017). HoWever, real-World problems remain to be far more challenging than RL is
currently capable of.
0
Environment
Figure 1: Mutli-agent Reinforcement Learning Framework
Many real-life tasks involve partial observability and multi-agent planning. Traditional RL ap-
proaches such as Q-Learning and policy-based methods are poorly suited to multi-agent problems.
Actions performed by third agents are usually observed as transition noise that makes the learning
very unstable (NoWe et al., 2012; Omidshafiei et al., 2017). Policy gradient methods, usually ex-
hibit very high variance when coordination of multiple agents is required (Lowe et al., 2017). The
complex interaction betWeen the agents makes learning difficult due to the agent’s perception of
1
Under review as a conference paper at ICLR 2019
the environment as non-stationary and partially observable. Nevertheless, multi-agent systems are
finding applications in high demand problems including resource allocation, robotic swarms, dis-
tributed control, collaborative decision making, real-time strategy (RTS) and real robots. But they
are a substantially more complex task for online learning algorithms and often require multi-layer
solutions.
This work considers the problem of cooperative learners in partially observable, stochastic envi-
ronment, receiving feedback in the form of joint reward. The paper presents a flexible multi-agent
competitive environment for online learning and policy performance comparison. We also introduce
the multi-agent RL solution under partial observability problem, where the goal is to maximize the
score performance measured in a direct confrontation.
2	Capture the Flag Environment
To throw together different control algorithms, we developed a complex dynamic environment we
called Capture the Flag (CtF) that models a real combat scenario and mimics multi-player computer
games. We made an effort to simplify the environment so it runs fast and requires minimum graphics,
computation, and the number of dependencies. The simulation is open-source and works under
OpenAI GYM software package (Brockman et al., 2016) that is a well-respected framework for
training AI algorithms. In addition to the mentioned above, the CtF introduces the ability to run
multiple AI policies playing against each other. This allows comparing not only the performance
of the algorithms but also serves as a gladiator pit where the resulted policies can be evaluated in
a direct confrontation. All these arguments make the CtF the perfect environment to train and test
various competitive AI algorithms.
Figure 2: The proposed simulation is a gladiator pit for algorithms. Python-based environment
Capture the Flag (CtF) is designed to throw together various AI algorithms or a human. Two teams
confront each other and the goal is to capture the other team’s flag or destroy the enemy units. Two
types of units exist in each team and they have different abilities. The observation is limited by the
fog of war shown on the left and provides the information available to the team. The true state of
the environment shown on the right and reflects all the teams.
The package includes sample solutions that may challenge other project investigators. These solu-
tions carry simple script-based heuristic behaviors that would help the researchers to train and test
their adaptive AI algorithms. These algorithms may serve as building bricks for advanced adaptive
solutions. For instance, we utilized the heuristic behaviors during the training process of the pro-
posed RL solution. The policy-based RL algorithm described in this paper is also provided to the
general audience. This algorithm may serve as a separate distributed solution or may be extended to
2
Under review as a conference paper at ICLR 2019
TyPe	Speed	Obs. Range	Attack Range	Map Code	Qty
UGV	-1-	3	2	-2or4-	4
UAV	3	5	一	0	—	3 or 5	~^T~
Table 1: Type of units available to the team.
Hidden Space	-1
BlueTeamField-	ɪ
Red Team Field	
BlueTeamUGV	丁
BlueTeamUAV	丁
Red Team UGV	ɪ
Red Team UAV-	丁
Blue Team Flag	ɪ
Red Team Flag-	~Γ~
Obstacle	ɪ
Table 2: Codes of the objects available in the observation.
form a hierarchical solution with a centralized planner. The layered solution currently is the topic of
ongoing research and currently is under testing.
2.1	Environment Description
The CtF simulates a grid world of a finite size where two teams compete against each other trying
to capture the other team’s flag in conditions of partial information and visibility. The goal of each
team is to survive, destroy the competitor or capture their flag.
Each team is represented by a number of units of a different type - unmanned ground vehicles
(UGVs) and unmanned aerial vehicles (UAVs). Every unit carries certain properties such as an
observation range, attack range, speed and different abilities as demonstrated in Table 1.
UGVs serve as a main actor of the game, they are allowed to defend their flag, capture the enemy’s
flag, fight the enemy UGVs, but they are limited in movements to the space that is free from obstacles
and other units. UAVs serve as an observer of the system. They are allowed to move everywhere,
they have a larger observation range and speed. The UAVs are invulnerable, but cannot attack or
capture the flag.
All the objects in the environment are projected on the 2D map that is separated into two pieces:
the blue team territory and the red team territory. These areas are equivalent and serve as parameter
modifiers affecting the interaction of the units. For instance, the initial location of each unit is a
uniform random draw limited by its team territory. Additionally, the territory affects the outcome of
the local confrontation of the units by favoring those that belong to the same color.
The original map forms a complete state of the dynamical system, while the teams are provided
with a partially observable state. The observation of each team is limited to the areas surrounding
the friendly units while the rest of the map remains hidden. This is often called a fog of war and
demonstrated in Figure 2. All visible regions are superposed to a single observation provided as a
matrix of numbers representing the objects in accordance with the Table 2.
2.2	Problem Taxonomy
The selection of hyperparameters of environment allows to gradually increase the complexity of the
problem. In this paper, we will be talking about the most sophisticated case as the most ambitious
goal. With that in mind, the formal taxonomy of the problem becomes a partially observable multi-
agent non-homomorphic planning Markov decision process (MDP) with simultaneous stochastic
transitions and a stochastic reward function.
3
Under review as a conference paper at ICLR 2019
Stateenv ∈	S2Tr0u×e 20, where STrue = {0, 1..8}	(1)
Obsenv ∈	Z20×20, where Z = {-1, 0, 1..8}	(2)
Actionenv ∈	AN, where A = {0, 1, 2, 3, 4}	(3)
Rewardenv =	f(Nblue,Nred,Nflag),∈ R,whereR= {-100.. + 100}	(4)
By default, the map is limited to 20 × 20 grid world, where each team controls 4 UGVs and 2 UAVs.
These two types of units have different abilities what adds a non-homomorphism to the problem.
The fully observable map shown in Eq. 1 is available to spectators for demonstration purpose only
and represents the complete state space of the environment. The observation space available to the
control algorithm is represented by a 2D vector of integers as shown in Eq. 2. As shown in Eq. 3,
action vector is limited to the choice of five discrete actions per each unit and applied simultaneously
to all the units including the enemy units. This adds a transition noise to the problem in contrast to
such games as Chess and Go.
The reward system shown in Eq. 4 represents the current score of the game that is a step function
of a number of units alive and flags available. For instance, once the friendly unit is lost, the score
of the game changes to -10 and remains unchanged till the next score change happens. Once the
enemy flag captured, it terminates the game and returns the score +100. Therefore, the reward is
clamped in range [-100..100].
3	Multi-Agent Deep Reinforcement Learning
The solution that we proposed in this paper is based on the Policy Gradient method (Williams, 1992;
Sutton et al., 1998). It has been demonstrated that Policy-based methods outperform Q-Learning
due to optimizing the expected reward directly and learning the explicit policy (Karpathy, 2016;
Schulman et al., 2015). It follows the idea of a score function estimator shown in Eq. 9 that we use
to update the model.
To address the complexity of the problem we created a distributed deep stochastic policy gradi-
ent. To make the training converging, we also utilized the following techniques: first-person view
observation, experience replay, policy transfer and self-play.
Vθ Es^∏(s；θ)[R(s)]
Vθ	π(s)R(s) s	(5)
X Vθπ(s)R(s) s	(6)
X π(s) V^ f (s) π(s) s	(7)
π(s)Vθ log π(s)R(x)	(8)
s Es[R(s)Vθ logπ(s)]	(9)
Distributed policy gradient proposed in this paper utilizes a stochastic policy gradient algorithm
REINFORCE (Williams, 1992) modified to fit multi-agent needs. The detailed algorithm is shown
in Alg. 1. Each agent carries its own policy that defines the action choice with respect to the
agent’s current observation. Simulation of all the agents generates a game trajectory τ from initial
global state S0 to termination state ST . This trajectory is decoupled into agent-centered trajectories
4
Under review as a conference paper at ICLR 2019
Parameter	Value
	Y		0.98
α	0.0001
Episode length	-100-
Update freq	-10-
Experience buffer	50000
Batch size	2000
Table 3: Training hyperparameters
Figure 3: First-person observation. Global observation is split into individual agent observations
and shifted to put the controlled agent into the center.
τ1, τ2...τN and then to their states s1, s2...sN. The individual trajectories populate the experience
replay buffer. The parameters of the algorithm are shown in Table 3.
Data: CtF simulation
Result: Optimal policy model θ*
Initialize θ ∈ RD ;
while k < Nepisodes do
Generate trajectory τ = CtF(π(θ));
Split s1 , s2 ...sagents = τ (t);
Populate buffer B with {s, R(s1 , s2...sagents)};
if k = Kupdate then
Sample buffer s,R,t 〜B(U(0, NbUffe));
G J PT=t+ι YiTRk；
θ — θ + αγtGV log(π(at∣st; θ));
end
end
Algorithm 1: Multi-agent SPG with experience replay
The experience replay is a fairly common technique in deep RL that allows uniform parameter up-
dates and, as result, a quicker convergence (Mnih et al., 2013). The replay buffer has been modified
to accommodate the experience from multiple units at a time. We utilized a global reward as a met-
ric of success for the policy that would allow the agents to learn to sacrifice themselves for a bigger
reward. This final reward at the end of each experiment has been discounted by time and populated
the experience buffer that was later uniformly sampled to update the policy model.
The first-person observations, in contrast to global observations, are rarely used in RL but make the
total sense in the case of the CtF problem. The observations provided by the game don’t draw a
difference in the units. All the units of similar type look the same that makes it difficult to recognize
which unit corresponds to which action input. The other reason to use first-person observations that
we want to train self-sufficient agents that work in any number from 1 to N units and can be easily
5
Under review as a conference paper at ICLR 2019
Experiment	N Enemies	Observation	Pre-training
1	0	Full	No
2	4	Full 一	1
3	4	Part.	2
4	一	4	Part. + UM	2	一
Table 4: Training procedure, experiments description
scalable. The first-person learning puts the controlled agent into the center of observation by shifting
the map as shown in Figure 3. Moreover, it allows projecting observations of other friendly units
onto the decoupled observations. This allows a convergence to a mutual (team-based) policy even
with no the coordination between the units.
In order to gradually approach to the optimal policy, this work proposed to use a policy transfer
approach where the policy was moved from a simple to a complex task. We split the training into
several steps with respect to complexity and run the training separately one by one. The optimal
policy has been transferred from task to task and adapted to the changes in the environment. As
demonstrated in Table 4, we started with the case where the map is fully seen, no enemies. This
trained the agents to approach and capture the flag. Then, we rerun the training with the enemy
team. This time the agents learned to carefully approach the flag and defend their flag. Next, we
trained on the partially observable scenario.
The initial training (in experiments 1 and 2) does not require a sophisticated opponent and may
converge to optimal policies which are different from our target policy. Anticipating the effect, we
introduced a Self Play technique that proved its efficiency in the game of Go (Silver et al., 2017).
After the policy update, we upgraded the policy of the opponent team with the current best model.
This constantly challenges the algorithm and pushes the best policy further.
In this research, the policy model has been created using TensorFlow package (Abadi et al., 2016).
The design of the neural network model is demonstrated in Table 5 and represented by a deep
convolutional network with a dense output layer operating a softmax activation function. Our model
takes a 2D state vector size of 20 × 20 and returns a probability vector size of 5. These probabilities
correspond to the actions that the agent prefers at the given input state.
Layer	Size
Input	-20 X 20-
CNN	[5 X 5] X 32
CNN	[3 × 3] × 32
Dense	400
Output	5	.
Table 5: Model structure
We understand that POMDP nature of the problem requires from us to use the temporal information
from the environment. However, we deliberately chose to not utilize Deep Recurrent Q-Networks
(DRQNs) Hausknecht & Stone (2015); Omidshafiei et al. (2017) due to the complexity of the task.
The temporal component would significantly increase the complexity of the model and resulted
policies. On the other side, we wanted to test an approximate solution using an observer design. As
mentioned above, the UAVs are designed to provide a safe exploration and increase the observability
of the problem. The assumption is that with an accurate design of the policy for the observer we can
turn the problem into an approximation of a fully observable problem. The limitation in the number
of UAVs is making the task challenging, especially when learning the policies of UAVs together
with other units.
4 Results
We first evaluated the performance in the fully observable case with no enemies. As demonstrated in
Figure 4, the policy converged to the optimal policy. With no enemies, the agents learned to navigate
6
Under review as a conference paper at ICLR 2019
RecordsZmearLreward
0.0CO IOQO 2∞∙0	300.。	400.0	500.0	600.0	700.0 BOOQ 900.0	1.000k	1.10Ok 1.200k
Episode
Figure 4: Performance of distributed policy learning with no competitors and fully observable envi-
ronment.
through the world, avoid obstacles and capture the other team’s flag. This navigation problem was
an essential step for training the further steps.
(Sφpos-α8 0 二 SB= PJEMSB UEəw
Figure 5: Performance of distributed policy learning with default number of competitors and fully
observable environment.
With introduction of a competitor team, the performance of the policy drastically dropped to the
negative score as shown in Figure 5. This was caused by the optimistic policy from step one that
was trying to explore and capture the flag. The policy eventually learned the presence of the enemy
units and adapted itself to perform a safer exploration and more conservative planning.
With added partial-observability, the policy demonstrated a very short peak in the performance that
is shown in Figure 6. We explain this peak with that the agents stopped seeing the enemies and
turned to use the policy previously learned in Step 1. This policy was over-optimistic and allowed
reaching the higher score at the beginning due to less fear of enemies. However, on average, it
quickly returned to the average of fully observable case with enemies that was more conservative.
The next improvement of the performance was related to the fact that policy learned to use UAVs
which previously have been unused. In partially observable case, the designing the observer helps
to improve the performance of the system. The UAVs in this simulation serve as observers, giving
some information that agents previously did not have access to. However, the lack of observers and
temporal logic in the model caused the agents to forget the previously seen enemies.
7
Under review as a conference paper at ICLR 2019
Records/mea n_rewa rd
a,θθɔk 3.5∞k	4.0∞k	4.5∞k	5.0∞k	5.5∞k	6,0∞k	6.5∞k	7.003k	7.60Ok 8.003k	S,6∞k	9.0∞k
Episode
Figure 6: Performance of distributed policy learning with default number of competitors and par-
tially observable environment.
5 Conclusion
In this project, we presented the new environment specifically designed for deep multi-agent learn-
ers. This environment allows to directly compare the performance of the policies and online train-
ing. The complex environment allows to design multi-layered solutions and throw them against each
other.
We also demonstrated that the complex multi-agent POMDP problem can be efficiently solved using
the distributed policy gradient method. The deep learning agent can learn a conservative policy in
case of the limited information and even design the policy for its own observers to address the partial
observability problem. The work also demonstrated, that the limitations in observers don’t allow to
turn the POMDP problem into fully observable and work only as an approximation. Addressing
the temporal component by utilizing RNN networks or introducing a belief state into the input state
would allow to further improve the performance.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-
scale machine learning. In OSDI, volume 16, pp. 265-283, 2016.
Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent com-
plexity via multi-agent competition. arXiv preprint arXiv:1710.03748, 2017.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard L Lewis, and Xiaoshi Wang. Deep learning
for real-time atari game play using offline monte-carlo tree search planning. In Advances in neural
information processing systems, pp. 3338-3346, 2014.
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps.
CoRR, abs/1507.06527, 7(1), 2015.
Frank S He, Yang Liu, Alexander G Schwing, and Jian Peng. Learning to play in a day: Faster deep
reinforcement learning by optimality tightening. arXiv preprint arXiv:1611.01606, 2016.
Andrej Karpathy. Deep reinforcement learning: Pong from pixels. url: http://karpathy. github.
io/2016/05/31/rl, 2016.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379-6390, 2017.
8
Under review as a conference paper at ICLR 2019
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518:529-533, 2015.
A Nowe, P Vrancx, YM De Hauwere, M Wiering, and M van Otterlo. Reinforcement learning:
state-of-the-art. Game Theory and Multi-agent Reinforcement Learning, pp. 441-470, 2012.
Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P How, and John Vian. Deep
decentralized multi-task multi-agent reinforcement learning under partial observability. arXiv
preprint arXiv:1703.06182, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press,
1998.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
9