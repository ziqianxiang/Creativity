Under review as a conference paper at ICLR 2019
A Main/Subsidiary Network Framework for
Simplifying B inary Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
To reduce memory footprint and run-time latency, techniques such as neural net-
work pruning and binarization have been explored separately. However, it is un-
clear how to combine the best of the two worlds to get extremely small and effi-
cient models. In this paper, we, for the first time, define the filter-level pruning
problem for binary neural networks, which cannot be solved by simply migrating
existing structural pruning methods for full-precision models. A novel learning-
based approach is proposed to prune filters in our main/subsidiary network frame-
work, where the main network is responsible for learning representative features
to optimize the prediction performance, and the subsidiary component works as a
filter selector on the main network. To avoid gradient mismatch when training the
subsidiary component, we propose a layer-wise and bottom-up scheme. We also
provide the theoretical and experimental comparison between our learning-based
and greedy rule-based methods. Finally, we empirically demonstrate the effec-
tiveness of our approach applied on several binary models, including binarized
NIN, VGG-11, and ResNet-18, on various image classification datasets. For bi-
nary ResNet-18 on ImageNet, we use 78.6% filters but can achieve slightly better
test error 49.87% (50.02%-0.15%) than the original model.
1	Introduction
Deep neural networks (DNN), especially deep convolution neural networks (DCNN), have made
remarkable strides during the last decade. From the first ImageNet Challenge winner network,
AlexNet, to the more recent state-of-the-art, ResNet, we observe that DNNs are growing substan-
tially deeper and more complex. These modern deep neural networks have millions of weights,
rendering them both memory intensive and computationally expensive. To reduce computational
cost, the research into network acceleration and compression emerges as an active field.
A family of popular compression methods are the DNN pruning algorithms, which are not only effi-
cient in both memory and speed, but also enjoy relatively simple procedure and intuition. This line of
research is motivated by the theoretical analysis and empirical discovery that redundancy does exist
in both human brains and several deep models (De Vivo et al., 2017; Denil et al., 2013). According to
the objects to prune, we can categorize existing research according to the level of the object, such as
connection (weights)-level pruning, unit/channel/filter-level pruning, and layer-level pruning (Wen
et al., 2016). Connection-level pruning is the most widely studied approach, which produces sparse
networks whose weights are stored as sparse tensors. Although both the footprint memory and the
I/O consumption are reduced (Han, 2015), Such methods are often not helpful towards the goal of
computation acceleration unless specifically-designed hardware is leveraged. This is because the
dimensions of the weight tensor remain unchanged, though many entries are zeroed-out. As a well-
known fact, the MAC operations on random structured sparse matrices are generally not too much
faster than the dense ones of the same dimension. In contrast, structural pruning techniques (Wen
et al., 2016), such as unit/channel/filter-level pruning, are more hardware friendly, since they aim
to produce tensors of reduced dimensions or having specific structures. Using these techniques, it
is possible to achieve both computation acceleration and memory compression on general hardware
and is common for deep learning frameworks.
We consider the structural network pruning problem for a specific family of neural networks - binary
neural networks. A binary neural network is a compressed network ofa general deep neural network
through the quantization strategy. Convolution operations in DCNN1 inherently involve matrix mul-
tiplication and accumulation (MAC). MAC operations become much more energy efficient if we use
1Fully connected layers can be implemented as convolution. Therefore, in the rest of the paper, we mainly
focus on convolutional layers.
1
Under review as a conference paper at ICLR 2019
low-precision (1 bit or more) fixed-point number to approximate weights and activation functions
(i.e., to quantify neurons) (Biswas & Chandrakasan, 2018). To the extreme extent, the MAC oper-
ation can even be degenerated to Boolean operations, if both weights and activation are binarized.
Such binary networks have been reported to achieve 〜58x computation saving and 〜32x memory
saving in practice. However, the binarization operation often introduces noises into DNNs (Li et al.,
2017), thus the representation capacity of DNNs will be impacted significantly, especially if we
also binarize the activation function. Consequently, binary neural networks inevitably require larger
model size (more parameters) to compensate for the loss of representation capacity.
Although Boolean operation in binary neural networks is already quite cheap, even smaller models
are still highly desired for low-power embedded systems, like smart-phones and wearable devices in
virtual reality applications. Even though quantization (e.g., binarization) has significantly reduced
the redundancy of each weight/neuron representation, our experiment shows that there is still heavy
redundancy in binary neural networks, in terms of network topology. In fact, quantization and
pruning are orthogonal strategies to compress neural networks: Quantization reduces the precision of
parameters such as weights and activations, while pruning trims the connections in neural networks
so as to attain the tightest network topology. However, previous studies on network pruning are all
designed for full-precision models and cannot be directly applied for binary neural networks whose
both weights and activations are 1-bit numbers. For example, it no longer makes any sense to prune
filters by comparing the magnitude or L1 norm of binary weights, and it is nonsensical to minimize
the distance between two binary output tensors.
We, for the first time, define the problem of simplifying binary neural networks and try to learn ex-
tremely efficient deep learning models by combining pruning and quantization strategies. Our exper-
imental results demonstrate that filters in binary neural networks are redundant and learning-based
pruning filter selection is constantly better than those existing rule-based greedy pruning criteria
(like by weight magnitude or L1 norm).
We propose a learning-based method to simplify binary neural network with a main-subsidiary
framework, where the main network is responsible for learning representative features to optimize
the prediction performance, whereas the subsidiary component works as a filter selector on the main
network to optimize the efficiency. The contributions of this paper are summarized as follows:
•	We propose a learning-based structural pruning method for binary neural networks to sig-
nificantly reduce the number of filters/channels but still preserve the prediction perfor-
mance on large-scale problems like the ImageNet Challenge.
•	We show that our non-greedy learning-based method is superior to the classical rule-based
methods in selecting which objects to prune. We design a main-subsidiary framework
to iteratively learn and prune feature maps. Limitations of the rule-based methods and
advantages of the learning-based methods are demonstrated by theoretical and experimental
results. In addition, we also provide a mathematical analysis for L1 -norm based methods.
•	To avoid gradient mismatch of the subsidiary component, we train this network in a layer-
wise and bottom-up scheme. Experimentally, the iterative training scheme helps the main
network to adopt the pruning of previous layers and find a better local optimal point.
2	Related Work
2.1	Pruning
Deep Neural Network pruning has been explored in many different ways for a long time. Hassibi
et al. (1993) proposed Optimal Brain Surgeon (OBS) to measure the weight importance using the
second-order derivative information of loss function by Taylor expansion. Dong et al. (2017) further
adapts OBS for deep neural networks and has reduced the retraining time. Deep Compression (Han,
2015) prunes connections based on weight magnitude and achieved great compression ratio. The
idea of dynamic masks (Guo et al., 2016) is also used for pruning. Other approaches used Bayesian
methods and exploited the diversity of neurons to remove weights (Molchanov et al., 2017; Mariet &
Sra, 2016). However, these methods focus on pruning independent connection without considering
group information. Even though they harvest sparse connections, it is still hard to attain the desired
speedup on hardware.
To address the issues in connection-level pruning, researchers proposed to increase the group-
sparsity by applying sparse constraints to the channels, filters, and even layers (Wen et al., 2016;
Alvarez & Salzmann, 2016; Polyak & Wolf, 2015; Anwar et al., 2017). He et al. (2017) used LASSO
constraints and reconstruction loss to guide network channel selection. Li et al. (2016) introduced
L1 -Norm rank to prune filters, which reduces redundancy and preserves the relatively important
filters using a greedy policy. Liu et al. (2017) leverages a scaling factor from batch normalization
2
Under review as a conference paper at ICLR 2019
to prune channels. To encourage the scaling factor to be sparse, a regularization term is added to
the loss function. On one hand, methods mentioned above are all designed for full-precision mod-
els and cannot be trivially transferred to binary networks. For example, to avoid introducing any
non-Boolean operations, batch normalization in binary neural networks (like XNOR-Net) typically
doesn’t have scaling (γ) and shifting (β) parameters (Biswas & Chandrakasan, 2018). Since all
weights and activation only have two possible values {1, -1}, it is also invalid to apply classical
tricks such as ranking filters by their L1 -Norms, adding a LASSO constraint, or minimizing the
reconstruction error between two binary vectors. On the other hand, greedy policies that ignore the
correlations between filters cannot preserve all important filters.
2.2	Quantization
Recent work shows that full precision computation is not necessary for the training and inference
of DNNs (Gupta et al., 2015). Weights quantization is thus widely investigated, e.g., to explore
16-bit (Gupta et al., 2015) and 8-bit (Dettmers, 2015) fixed-point numbers. To achieve higher com-
pression and acceleration ratio, extremely low-bit models like binary weights (Courbariaux et al.,
2015; Hu et al., 2018) and ternary weights(Zhu et al., 2016; Zhou et al., 2017; Wang & Cheng, 2017)
have been studied, which can remove all the multiplication operations during computation. Weight
quantization has relatively milder gradient mismatch issue as analyzed in Section 3.1.2, and lots of
methods can achieve comparable accuracy with full-precision counterparts on even large-scale tasks.
However, the ultimate goal for quantization networks is to replace all MAC operations by Boolean
operations, which naturally desires that both activation and weights are quantized, even binarized.
The activation function of quantized network has the form of a step function, which is discontinuous
and non-differentiable. Gradient cannot flow through a quantized activation function during back-
propagation. The straight-through estimator (STE) is widely adopted to circumvents this problem,
approximating the gradient of step function as 1 in a certain range (Hinton et al., 2012; Bengio et al.,
2013). Cai et al. (2017) proposed the Half-wave Gaussian Quantization (HWGQ) to further reduce
the mismatch between the forward quantized activation function and the backward ReLU (Nair &
Hinton, 2010). Binary Neural Networks (BNN) proposed in Courbariaux et al. (2016) and Biswas
& Chandrakasan (2018) use only 1 bit for both activation functions and weights, which ends up with
an extremely small and faster network. BNNs inherit the drawback of acceleration via quantization
strategy and their accuracy also need to be further improved.
3	Approach
Let Fbi ∈ RNi×Hi×Wi denote binary input feature maps of the i-th layer in an I-layer binary neural
network, where Ni, Hi, and Wi are the number of the input feature maps, height, and width of
the activation map, respectively. Kernel weights Wbi ∈ RNi+1×Ni×Ki+1 ×Ki+1 in this layer are
convolved with the input feature map Fbi into output feature map Fbi+1. Because both weights and
activations are binary, we remove the subscripts of Fb and Wb for clarity. The goal of pruning is to
remove certain filters Wni,,：,：,：, n ∈ Ω, where Ω is the indices of pruned filters. If a filter is removed,
the corresponding output feature map of this layer (which is also the input feature map of next layer)
will be removed, too. Furthermore, the input channels of all filters in the next layer would become
unnecessary. If all filters in one layer can be removed, the filter-level pruning will upgrade to layer-
level pruning naturally. The goal of our method is to remove as many filters as possible for binary
neural networks which are already compact and have inferior numerical properties, thus this task is
more challenging compared with pruning a full-precision model.
3.1	Subs idiary Component And main Networks
We borrow the ideas from binary network optimization to simplify binary networks. While it sounds
tautological, note that the optimization techniques were originally invented to solve the quantization
problem, but we will show that it can be crafted to solve the pruning problem for binary networks. A
new binary network, called subsidiary component, acts as learnable masks to screen out redundant
features in the main network, which is the network to complete classification tasks. Each update of
the subsidiary component can be viewed as the exploration in the mask search space. We try to find
a (local) optimal mask in that space with the help of the subsidiary component.
The process of training subsidiary and main networks is as follows:
3
Under review as a conference paper at ICLR 2019
-----/ Trainable Layer
-----/ Fixed Layer
Main Network
Subsidiary Component
、讼
时
√
-1；
'J-1
-e,
1
'-1
1,
1
-1
)；1 下、ISignP11〉. Fgn广.1,下、
'<1 - Jz Y1 ' ʊ '∙;-1 ,
殴收⑻
.泌
1
法尿辽尽
_κ	NIbI Ni-IN Kr↑'∖	MIN _r. 11*	1-1、	肃1，	JN
Lmges>-1泛 ④1炽 ④1汉…④1代[⅛s> -η∖1 QR1 (Sg>N ....Q1 沐1
'q1	- O '<1 ∙	×~∙-'	7-1	■ O '<1 ∙	IZ 、、； 1 ：'■	、/1	厂，<2>	",!-1 ；'■	<L∕、/1	；,'
!.，-，	K，-，	!. L1	K，-，	. -	ι.1	、： .
IG,.
Figure 1: Pipline of our method. The main network in this figure is already pre-trained. From left
to right: our subsidiary component training for i-th layer, main network retraining, and subsidiary
component training for (i+1)-th layer.








3.1.1	Feature Learning - THE Main Network
For layer i, the weights of subsidiary component Mi ∈ RNi+1×Ni×Ki+1 ×Ki+1 are initialized by
the uniform distribution: Mi = U(-σ, σ). in practice, σ is chosen to be less than 10-5. To
achieve the goal of pruning filters, all elements whose first index is the same share the same value.
Mno = Mno , ∀o, p, q. Filter mask Oi ∈ RNi+1 ×Ni×Ki+1 ×Ki+1 is an output tensor
n,o1 ,p1 ,q1	n,o2 ,p2 ,q2
from the subsidiary component. In the first stage, We use the Iden(∙) function (identity transforma-
tion) to get Oi .
Oi = Iden(Mi)
We apply the filter mask Oi to screen main netWork’s Weights Wi,
ʌ .∙ .∙ .∙
Wi = Oi 乳 Wi
,where 0 is element-wise product. Wi denotes the weights of the main network after transfor-
mation, Which is used to be convolved With the input feature maps, Fi, to produce the output
feature maps Fi+1. Then, weights of the main network, Wj, j ∈ [1, I], are set to be trainable
while weights of the subsidiary component, Mj , j ∈ [1, I], are fixed. Because subsidiary weights
are fixed and initialized to be near-zero, it will not function in the Feature Learning stage, thus
Wj ≈ Wj, j ∈ [1, I]. The whole main binary neural network will be trained from scratch.
3.1.2	Feature Selection — the subsidiary component
Training Subsidiary Component within a Single Layer i: After training the whole main network
from scratch, we use a binary operator to select features in a layer-wise manner. in opposite to the
previous Feature Learning stage, the weights of all layers Wj , j ∈ [1, I] of the main network and
the weights except layer i of the subsidiary component Mj , j ∈ [1, I]/[i] are set to be fixed, while
the subsidiary component’s weights at the current layer Mi are trainable when selecting features for
Layer i. The transformation function for the filter mask Oi is changed from Iden(∙) to Bin(∙) (sign
transformation + linear affine),
Oi = Bin(M i) = Sign(M i) + 1
By doing this, we project the float-point Mi to binarized numbers ranging from 0 to 1. Elements in
Oi which are equal to 0 indicate that the corresponding filters are removed and the elements of value
1 imply to keep this filter.
Since Bin(∙) is not differentiable, we use the following function instead of the sign function in back
propagation when training the subsidiary component Mi (Hinton et al., 2012; Bengio et al., 2013),
{x	—1 < x < 1
1	x≥1	(1)
—1 x ≤ —1
4
Under review as a conference paper at ICLR 2019
Apart from the transformation, we also need to add regularization terms to prevent all Oi from
degenerating to zero, which is a trivial solution. So the loss function of training Layer i in the
subsidiary component is,
arg min L cross .entropy + α ∙ Lreg + β ∙ Ldistill
(2)
Lreg = kOi k1
where Lcross-entropy is the loss on data and Ldistm is the distillation loss defined in (7).
Finally, we fix the layers Mj, j ∈ [1, I] in the subsidiary component and layers before i in the main
network (i.e., Wj, j ∈ [1, i - 1]), and retrain the main layers after Layer i (i.e., Wj, j ∈ [i, I]).
Bottom-up Layer-wise Training for Multiple Layers: We showed how to train a layer in the
subsidiary component above. To alleviate the gradient mismatch and keep away from the trivial
solution during Features Selection, next, we propose a layer-wise and bottom-up training scheme
for the subsidiary component: Layers closer to the input in the subsidiary component will be trained
with priority. As Layer i is under training, all previous layers (which should have already been
trained) will be fixed and subsequent layers will constantly be the initial near-zero value during
training. There are three advantages of this training scheme.
First, as in (1), we use STE as in (Hinton et al., 2012; Bengio et al., 2013) to approximate the gradient
of the sign function. By chain rule, for each activation node j in Layer i, we would like to compute
an “error term” δj =第 which measures how much that node is responsible for any errors in the
output. For binary neural networks, activation is also binarized by a sign function which need STE
for back-propagation. The “Error term” for binary neural networks is given by,
			∂L	∂L	∂Oi	(5)
δj = Sign,(aij) ∙	X wji+,q1δqi+1	(3)	-	7 = ∂Mi	=		 ∂Oi	• -	7 ∂Mi	
∂Sign(aij )	q q =1∣ai∣≤ι	(4)	∂Oi	1		
~∂αjjj-			-	T = ∂Mi	=—∙ 1 2	∣Mi∣≤1	(6)
where (3) and (5) can be obtained by the chain rule, and (4) and (6) are estimated from STE, which
will introduce gradient mismatch into back-propagation. We refer (6) as weight gradient mismatch
issue and (4) as activation gradient mismatch issue. They are two open problems in the optimization
ofbinary neural networks, both caused by the quantization transform functions like SignG. Starting
from bottom layers, we can train and fix layers who are harder to train as early as possible for the
subsidiary component. In addition, because of the retraining part in Features Selection, bottom-up
training scheme allows bottom layers to be fixed earlier, as well. In practice, this scheme results in
more stable training curves and can find a better local optimal point.
Second, the bottom-up layer-wise training scheme helps the main network to better accommodate
the feature distribution shift caused by the pruning of previous layers. As mentioned before, the
main difference in the motivation between our pruning method and rule-based methods is that we
have more learnable parameters to fit the data by focusing on the final network output. With the
bottom-up and layer-wise scheme, even if the output of Layer i changes, subsequent layers in the
main network can accommodate this change by modifying their features.
Lastly and most importantly, we achieve higher pruning ratio by this scheme. According to our
experiments, a straight-forward global training scheme leads to limited pruning ratio. Some layers
are pruned excessively and hence damaged the accuracy, while some layers are barely pruned, which
hurts the pruning ratio. The layer-wise scheme would enforce all layer to be out of the comfort zone
and allow balancing between accuracy and pruning ratio.
3.1.3 Pipeline
The pipeline of our method is as follows:
1.	Initialize weights of subsidiary component Mj, j ∈ [1, I] with near-zero σ's.
2.	Set Mj, j ∈ [1, I] to be fixed, and train the whole main network from scratch.
3.	Train starting from the first binary kernel. Each layer is the same as in the algorithm shown
below:
5
Under review as a conference paper at ICLR 2019
•	Change the activation function for Mi from Iden(∙) to Bin(∙). And all other Param-
eters apart from Mi are fixed. Train subsidiary component according to (2).
•	Fix the subsidiary layers Mj, j ∈ [1, I] and main layers before i-th layer Wj , j ∈
[1, i - 1], and retrain main layers after i-th layer Wj, j ∈ [i, I].
3.1.4 Distillation loss
Though Pruning network filters is not an exPlicit transfer learning task, the aim is to guide the thin
network to learn more similar outPut distributions with the original network. The model is suPPosed
to learn a soft distribution but not a hard one as ProPosed in Previous traditional classifier networks.
Hence, we add a distillation loss to guide the training subsidiary comPonent to be more stable, as
shown in Figure 2.
Ldistill = DKL(pkq) = H(p,q) - H(p)	(7)
We set p to be the original binary neural network distribution. Because the distribution is fixed, the
H(p) is a constant and can be removed from Ldistill. It means that the distillation loss can be written
as
M	exp(zi /T)	exp(ti /T)
distill--⅛i °g j exp(Zj /T) X PM exp(t/T)
where zi and ti rePresent the final outPut of the Pruned and original networks before the softmax
layer. T is a temPerature Parameter for the distillation loss defined in Hinton et al. (2015). We set T
as 1 in Practice. M is the number of classes.
3.2	Comparison with rule-based methods
Previous methods use rules to rank the imPortance of each filter and then remove the toP k least
imPortant filters. The rules can be weight magnitude, e.g., measured by the L1 norm, or some other
well-designed criteria.
Studies in this line share the same motivation that individual filters have their own imPortance indi-
cation, and filters with less imPortance can be removed relatively safely. This assumPtion ignores
interactions among filters. As mentioned before, rule-based Pruning algorithms use a greedy way to
Prune filters, i.e., they assume that individual filters behave indePendently and their own imPortance
(or function) for rePresentation learning. We give a theoretical analysis in Section 3.3 about this
Point. In fact, Pruning filters indePendently may cause Problems when filter are strongly correlated.
For examPle, if two filters have learned the same features (or concePts), these two filters may be
Pruned out together by rule-based methods, because their rankings are very close. Clearly, Pruning
one of them is a better choice.
However, almost all these criteria are based on value statistics and are comPletely unsuitable for
the binary scenario with only two discrete values. One Possible Pruning method is to exhaustively
search the oPtimal Pruning set, but this is NP-Hard and Prohibitive for modern DNNs that have
thousands of filters. Our method uses the subsidiary comPonent to “search” the oPtimal solution.
Our soft “search” strategy is gradient-based and batch-based comPared to exhaustive search, and it
is much more efficient.
3.3	RELATION TO L1-NORM PRUNING
If our main network is full-Precision, the L1-Norm based Pruning technique would be strongly
relevant to our method, excePt that we target at oPtimizing the final outPut of the network, whereas
the L1-Norm based method greedily controls the Perturbation of the feature maP in the next layer.
SuPPose that W = [w1; . . . ; wn] is the original filter blocked by rows, W0 = [w10 ; . . . ; wn0 ] is the
Pruned filter, and xis the inPut feature maP. Let ∆wi ≡ wi - wi0 . Then, the L1-Norm aPProach
minimizes the uPPer bound of the following Problem: maxkxk∞<τ kWx- W0xk. To see this, note
kW x-W 0xk = k
0
w1 - w10
• ∙ ∙
wn - wn0
xk = k	∆wixk ≤	k∆wixk ≤	k∆wk1kxk∞ ≤	k∆wk1τ
To minimize i k∆wk1 by zeroing-out a single row wi , obviously, the solution is to select the one
with the smallest L1-Norm.
However, note that this strategy cannot be trivially aPPlied for binary networks, because the L1-
Norm for any filter that is a {-1, +1} tensor of the same shaPe is always identical.
6
Under review as a conference paper at ICLR 2019
3.4	Relation to LASSO regression based least reconstruction error pruning
Previous work (He et al.) uses the
LASSO regression to minimize the
reconstruction error of each layer:
min kY — PL=i βiXiWikF, kβko ≤ C;
Solving this Lo minimization problem is NP-
hard, so the Lo regularization is usually relaxed
to Li. In the binary/quantization scenario,
activations only have two/several values and
the least reconstruction error is not applicable.
Instead of minimizing the reconstruction error
of a layer, our method pays attention on the
final network output with the help of the
learnable subsidiary component. We directly
optimize the discrete variables of masks (a.k.a
subsidiary component) without the relaxation.
Figure 2: The learning curve for the Subsidiary
Component. The red line refers to the learning
curve without the distillation loss, and the red
4 Experiments
To evaluate our method, we conduct sev-
eral pruning experiments for VGG-11, Net-In-
Net (NIN), and ResNet-18 on CIFAR-10 and
ImageNet. Since our goal is to simplify binary
background represents the learning curve variance
of every epoch. The green line and background
represent the subsidiary component learning curve
and the variance of each epoch. Clearly, the distil-
lation loss makes the training more stable.
neural networks, whose activation and weights are both 1-bit, all main models and training settings
in our experiments inherit from XNOR-Net (Rastegari et al., 2016). Since we are, to the best of
our knowledge, the first work to define filter-level pruning for binary neural networks, we proposed
a rule-based method by ourselves as the baseline. Instead of ranking filters according to the L1-
Norm (Li et al., 2016), we use the magnitude of each filter’s scaling factor (MSF) as our pruning
criterion. Inspired by (Li et al., 2016), We test both the “prune once and retrain” scheme2 and the
“prune and retrain iteratively” scheme3.
As pointed out in (Rastegari et al., 2016) we
set weights of the first layer and last layer as
full-precision, which also means that we only
do pruning for the intermediate binary layers.
We measure effectiveness of pruning methods
in terms of PFR, the ratio of the number of
pruned filters to original filter number, and error
rate before and after retraining. For error ratio,
smaller is better. For PFR, larger is better. For
CIFAR-10, when training the main network,
learning rate starts from 10-4, and learning-
rate-decay is equal to 0.1 for every 20 epochs.
Learning rate is fixed with 10-3 when training
the subsidiary component. For ImageNet, we
set a constant learning rate of 10-3 for the sub-
sidiary component and main work.
Figure 3: Learning curve for subsidiary compo-
nent. We train the subsidiary component with dif-
ferent learning rate. These curves are smoothed
for the directly seeing the trend of the learning
Subsidiary Component. All the dotted lines rep-
resent the learning curve of the large learning
rate 10-3, the normal lines represent the learning
curves of the small learning rate 10-4.
For fair comparison, we control PFR for each
layer of these methods to be the same to ob-
serve the final Retrain-Error. In Figure 4, MSF-Layerwise refers to the “prune once and retrain”
scheme, and the MSF-Cascade refers the “prune and retrain iteratively” scheme. The first three
figures of experiments were done on the CIFAR-10 dataset. The last figure refers to results on
Imagenet.
4.1	NIN AND VGG-11 ON CIFAR-10
NIN is a fully convolutional network, using two 1 × 1 convolution layers instead of fully connected
layer, and has quite compact architecture. VGG-11 is a high-capacity network for classification.
VGG-11 on CIFAR-10 consists of 8 convolutional layers(including 7 binary layers) and 1 fully
connected layers. Batch normalization is used between every binary convolution and activation
2Prune filters of multiple layers at once and retrain them until the original accuracy is restored
3Prune filters layer by layer and then retrain iteratively. The model is retrained before pruning the next layer
for the weights to adapt to the changes from the pruning process.
7
Under review as a conference paper at ICLR 2019
sJRujU-EaH
W3θ-ll 6 CIFAR-IO
%JRuj-EaH
Kcsnet-Ig on CIFAR-IO
%JRuj-EaH
(a) NIN, CIFAR-10	(b) VGG-11, CIFAR-10 (C) ReSNet-18, CIFAR-10 (d) ReSNet-18,ImageNet
Figure 4: Results. We control all methods to have the same PFR and compare their error after
retraining. Compared with the baseline designed by US (in orange and green), performance of our
learning-based method (in pink) is better with the same PFR. From left to right, the PFR is 33.05%,
39.70%, 39.89%, and 21.40%. For ResNet-18 on ImageNet, We pruned 21.4% of the filters while
the retrained error decreased from 50.02% to 49.87%
layer, which makes the training process more stable and converge with high performance. For both
MSF-Layerwise and MSF-Cascade, with the same PCR, the performance is worse than us. With
30% 〜40% of pruning filter ratio, the pruned network error rate only increased 1% 〜2%.
4.1.1	LEARNING RATE IS IMPORTANT
An interesting phenomenon is observed when training subsidiary components for different models.
We try different learning rates in our experiments and observe it impacts final convergent point a lot
as shown in Figure 3. The relatively smaller learning rate (10-4) will converge with lower accuracy
and higher pruning number; however, the larger learning rate (10-3) leads to the opposite result.
One possible explanation is that the solution space of the high-dimensional manifold for binary neu-
ral networks is more discrete compared to full-precision networks, so it is difficult for a subsidiary
component to jump out of a locally optimal point to a better one. Moreover, in the binary scenario,
larger learning rate will increase the frequency of value changing for weights. Our motivation is
to use a learnable subsidiary components to approximate exhaustive search, so using a larger learn-
ing rate will enable the subsidiary component to “search” more aggressively. A large learning rate
may be unsuitable for normal binary neural networks like the main network in this paper, but it is
preferred by the subsidiary component.
4.1.2	INITIALIZATION OF SUBSIDIARY COMPONENT IS NOT SENSITIVE
As mentioned in section 3.1.1, we use the uniform distribution to initialize the mask. According to
the expectation of the uniform distribution, E(SP) = 0.5, where SP is the ratio of the number of
positive elements in subsidiary weights to size of weights. However, since we use SignG), different
SP may impact the result a lot. We conduct six experiments on different models across different
layers and show that initialization with 0.4, 0.6, 1.0 SP will all converge to the same state. However,
when SP is 0.2, final performance will be very poor. A possible reason is that the number of filters
thrown out by the initialization is too large, and due to the existence of the regularization term,
the network,s self-adjustment ability is limited and cannot converge to a good state. Hence we
recommend the SP to be intialized to greater than 0.4.
4.2	ResNet on CIFAR-10 and ImageNet
Compared with NIN and VGG-11, ResNet has identity connections within residual block and much
more layers. As the depth of network increases, the capacity of network also increases, which then
leads to more redundancy. From experimental results, we find that when the identification mapping
network has a downsampling layer, the overall sensitivity of the residual block will increase. Overall
result for ResNet on CIFAR-10 is shown in table (1), and statistics for each layer can be found in
Appendix.
We further verify our method with ResNet-18 on ImageNet. a can be set from 10-7 to 10-9 de-
pending on the expected PFR, the accuracy and pruning ratio are balanced before retraining. After
20 epoches retraining for each layer, the final PFR is 21.4%, with the retrained error has decreased
from 50.02% to 49.87%.
8
Under review as a conference paper at ICLR 2019
References
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. In
Advances in Neural Information Processing Systems, pp. 2270-2278, 2016.
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural
networks. ACM Journal on Emerging Technologies in Computing Systems (JETC), 13(3):32,
2017.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Avishek Biswas and Anantha P. Chandrakasan. Conv-RAM: An energy-efficient SRAM with em-
bedded convolution computation for low-power CNN-based machine learning applications. Di-
gest of Technical Papers - IEEE International Solid-State Circuits Conference, 61:488-490, 2018.
ISSN 01936530. doi: 10.1109/ISSCC.2018.8310397.
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep Learning with Low Precision
by Half-wave Gaussian Quantization. 2017. ISSN 1063-6919. doi: 10.1109/CVPR.2017.574.
URL http://arxiv.org/abs/1702.00953.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems, pp. 3123-3131, 2015.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to
+1 or -1. 2016. ISSN 1664-1078.
Luisa De Vivo, Michele Bellesi, William Marshall, Eric A Bushong, Mark H Ellisman, Giulio
Tononi, and Chiara Cirelli. Ultrastructural evidence for synaptic scaling across the wake/sleep
cycle. Science, 355(6324):507-510, 2017.
Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep
learning. In Advances in neural information processing systems, pp. 2148-2156, 2013.
Tim Dettmers. 8-bit approximations for parallelism in deep learning. arXiv preprint
arXiv:1511.04561, 2015.
Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. In Advances in Neural Information Processing Systems, pp. 4857-4867,
2017.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic Network Surgery for Efficient DNNs. Neural
Information Processing Systems, 2016.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In International Conference on Machine Learning, pp. 1737-1746,
2015.
Song Han. Learning Both Weights and Connections for Efficient Neural Networks. NIPS, 346
(8988):1500-1501, 2015.
Babak Hassibi, David G. Stork, Gregory Wolff, and Takahiro Watanabe. Optimal brain surgeon:
Extensions and performance comparisons. In Proceedings of the 6th International Conference
on Neural Information Processing Systems, NIPS’93, pp. 263-270, San Francisco, CA, USA,
1993. Morgan Kaufmann Publishers Inc. URL http://dl.acm.org/citation.cfm?
id=2987189.2987223.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel Pruning for Accelerating Very Deep Neural
Networks.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural net-
works. In International Conference on Computer Vision (ICCV), volume 2, 2017.
Geoffrey Hinton, Nitsh Srivastava, and Kevin Swersky. Neural networks for machine learning.
Coursera, video lectures, 264, 2012.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
9
Under review as a conference paper at ICLR 2019
Qinghao Hu, Peisong Wang, and Jian Cheng. From hashing to cnns: Training binaryweight networks
via hashing. arXiv preprint arXiv:1802.02733, 2018.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. International Conference on Learning Representations, 2016.
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training
Quantized Nets: A Deeper Understanding. Advances in Neural Information Processing Systems,
2017. ISSN 10495258.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In Computer Vision (ICCV),
2017 IEEE International Conference on,pp. 2755-2763. IEEE, 2017.
Zelda Mariet and Suvrit Sra. Diversity networks. Proceedings of ICLR, 2016.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational Dropout Sparsifies Deep
Neural Networks. International Conference on Machine Learning, 2017.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Adam Polyak and Lior Wolf. Channel-level acceleration of deep face representations. IEEE Access,
3:2163-2175, 2015.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European Conference on Computer
Vision, pp. 525-542. Springer, 2016.
Peisong Wang and Jian Cheng. Fixed-point factorized networks. In Computer Vision and Pattern
Recognition (CVPR), 2017 IEEE Conference on, pp. 3966-3974. IEEE, 2017.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning Structured Sparsity in
Deep Neural Networks. Advances in Neural Information Processing Systems, pp. 10, 2016.
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantiza-
tion: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
10
Under review as a conference paper at ICLR 2019
Appendix
4.3	The gradient flow of binary neural network
Figure 5: Gradient flow of binary neural networks during back-propagation. Rectangles represent
the weight tensor and ellipses represent functional operation. In this paper, we use binary operation
as a special quantization function. MAC is short for multiplication and accumulate operations, or
the equivalent substitution like XNOR (Biswas & Chandrakasan, 2018) in BNN.
4.4	The overall results in detail
For fair comparison, we control PFR for each layer of these methods to be the same to observe
the final Retrain-Error. In Table 1, MSF-Layerwise refers to the “prune once and retrain” scheme,
and the MSF-Cascade refers the “prune and retrain iteratively” scheme. The first three groups of
experiments were done on the CIFAR-10 dataset. The last group refers to results on Imagenet.
Table 1: Overall results
Method	Model	Original Error(%)	Retrain Error(%)	PFR(%)
MSF-Layerwise	NIN	15.79%	19.28%	33.05%
MSF-Cascade	NIN	15.79%	17.39%	33.05%
Our Method	NIN	15.79%	16.89%	33.05%
MSF-Layerwise	VGG-11	16.13%	19.59%	39.70%
MSF-Cascade	VGG-11	16.13%	18.79%	39.70%
Our Method	VGG-11	16.13%	18.03%	39.70%
MSF-Layerwise	ResNet-18	12.11%	16.44%	39.89%
MSF-Cascade	ResNet-18	12.11%	14.63%	39.89%
Our Method	ResNet-18	12.11%	13.61%	39.89%
MSF-Layerwise	ResNet-18	50.02%	51.33 %	21.40%
MSF-Cascade	ResNet-18	50.02%	50.56%	21.40%
Our Method	ResNet-18	50.02%	49.87%	21.40%
11
Under review as a conference paper at ICLR 2019
4.4.	1 Compared with MSE-Layerwise for VGG-11 on CIFAR- 1 0(Pruned
Filters/Original Filters)
Figure 6: Comparison Our method with MSF-Layerwise. It is for every binary convolution filter in
VGG-11. The x-axis is the pruning filter rate. The y-axis is the validation accuracy. As shown in
figure, our method prunes more filters with better validation accuracy.
4.4.2	Comparaed with MSE-Layerwise For VGG-11 on CIFAR- 1 0(Pruned
FIlters/Original Filters)
Table 2: Compared with MSE-Cascade for NIN Network on CIFAR-10 Dataset
PFR	Original Error
Pruned Error
Re-Train Error
		MSF-Cascade	Ours	MSF-CasCade	Ours	MSF-CasCade	Ours
Conv1	60/192	15.39%	15.39%	25.02%	18.22%	16.21%	16.02%
Conv2	16/96	16.21%	16.02%	23.71%	19.79%	16.57%	16.31%
Conv3	69/192	16.57%	16.31%	38.99%	19.77%	16.89%	16.49%
Conv4	65/192	16.89%	16.49%	396%	19.89%	17.11%	16.57%
Conv5	46/192	17.11%	16.5%	19.93%	19.70%	17.29%	17.40%
Conv6	82/192	17.29%	17.40%	38.57%	19.44%	17.47%	17.01%
Conv7	64/192	17.47%	17.01%	27.03% —	19.56%	17.39%	16.89%
4.4.3	The Pruned Filter Number in Details (Pruned Filters/Original Filters)
Table 3: Every Layer PFR for Resnet-18 on CIFAR-10
conv1	Conv2	Conv3	conv4	conv5	conv6	conv7
16/64-	21/64	22/64	17/64	46128	61/128	49/128
conv8	-conv9-	Conv10	conv11	conv12	conv13	conv14
49/128	60/128	122/256	62/256	48/256	107/256	101/256
conv15-	conv16	Conv17	conv18	conv19		
224/512	214/512	178/512	190/512	98/512		
12
Under review as a conference paper at ICLR 2019
Table 4: Every Layer PFR for Resnet-18 on ImageNet
conv1	conv2	conv3	conv4	conv5	conv6	conv7
13/64	17/64	19/64	21/64	41/128	34/128	23/128
conv8-	conv9	conv10	conv11	conv12	conv13	conv14
21/128	40/128	65/256	59/256	48/256	34/256	56/256
conv15	conv16	conv17	conv18	conv19		
70/512	67/512	98/512	101/512	77/512		
Table 5: Every Layer PFR for VGG-11 on CIFAR-10
conv1	conv2	conv3	conv4	conv5	conv6	conv7
63/128	107/256	86/256	170/512	176/512	255/512	210/512
13