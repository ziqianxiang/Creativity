Under review as a conference paper at ICLR 2019
ENCODER DISCRIMINATOR NETWORKS FOR
UNSUPERVISED REPRESENTATION LEARNING
Anonymous authors
Paper under double-blind review
ABSTRACT
Learning representations of data samples in an unsupervised way is needed when￾ever computers have to reason about unlabeled data. Applications range from
compressing and denoising data to super-resolution, generating new samples from
a given sample distribution and much more. In this work, we use information en￾tropy and a little game to motivate a new encoder discriminator architecture in
order to learn unsupervised latent representations. Inspired by the game ”Taboo”,
we train an encoder network to generate a meaningful representation of one par￾ticular sample of a dataset. Using this description, a discriminator network then
has to retrieve the same sample from the whole dataset. We show that learning
in this manner on many different samples repeatedly minimizes the information
entropy given the latent description and, thus, forces the encoder network to make
precise descriptions that can be interpreted by the discriminator. We provide first
results of this method on the MNIST and the Fashion MNIST dataset.
1 INTRODUCTION AND RELATED WORK
While in supervised machine learning the goal is to find a mapping of samples onto predefined labels,
in unsupervised learning algorithms not only have to find a mapping but also — and in particular —
to find meaningful representations themselves. The reasons to do this are manifold:
Hinton & Salakhutdinov (2006) show how to generate compressed representations of data using
deep autoencoders. Vincent et al. (2008) show how autoencoders can be used to denoise images.
This requires finding the hidden structure of the data behind the noise and thus representations
that are independent of noise added to the input. Dong et al. (2016) present a way to increase the
resolution of lower resolution input images. Most of these encoder-decoder methods optimize their
output to match the training data with respect to some metric (usually L2). These metrics however
do not capture the statistics of images well and hence often lead to blurry results in the case of model
uncertainty.
Another trend in unsupervised learning are generative adversarial networks (GANs) as introduced by
Goodfellow et al. (2014). By training a discriminative loss function to distinguish between samples
that originate from the dataset and samples coming from a generator network, new samples can be
generated from a latent noise input. These samples capture the statistics of the dataset and often
look surprisingly realistic. Radford et al. (2015) showed how the latent noise input can be related
to the generator output and was able to perform vector arithmetic for visual concepts in that latent
space. However in these generator-discriminator methods, finding a latent representation for a visual
concept is difficult — especially in the presence of mode collapse. Furthermore, retrieving a latent
representation of a particular sample is impossible since no encoder is given.
In this work, we present a new architecture that combines the benefits of the encoder part of autoen￾coders with the discriminator part of GANs in order to generate latent sample descriptions that are in
good accordance with the statistics of the dataset. We motivate the network using the game ”Taboo”
— a word guessing game where words have to be described without using forbidden phrases — and
show how this leads to optimal representations by means of minimizing information entropy.
1
Under review as a conference paper at ICLR 2019
Figure 1: Visualization of the Encoder Discriminator concept
2 ENCODER DISCRIMINATOR NETWORKS
In this section, we introduce encoder discriminator networks as a new way to learn latent represen￾tations in an unsupervised way. First, the architecture is motivated by a cooperative game similar
to ”Taboo”. Then, we want to reveal how the game can be implemented and our underlying net￾work architecture. In the end of this section, we want to give a more thorough explanation of the
theoretical aspects and derive the loss function from an information entropy argument.
2.1 ”TABOO” - A COOPERATIVE GAME
Taboo is a famous game in which players in a team try to find words collaboratively: First, one
player picks a random card from a pile and keeps it secret. This card contains a word which the
player has to describe as precisely as possible so that it can be guessed by the teammates. To make
the game harder, the player is constrained by several words which must not appear in the description.
The team which makes the best descriptions and finds the words the fastest wins the game.
The encoder discriminator architecture follows this analogy: First, the encoder picks a random
sample from a dataset which contains, for example, an image that needs to be found by the discrim￾inator. Then, the encoder has to make an as precise as possible description of this sample but is
constrained by the dimensionality and boundedness of the latent description space. The loss func￾tion is minimized when the descriptions of the encoder are precise enough and interpretable by the
discriminator network such that the discriminator can retrieve the sample from the whole dataset
with high confidence. So in contrast to GANs, where the discriminator is trained adversely, in this
setting the discriminator works collaboratively with the encoder and thus the network can be trained
end-to-end.
2.2 ENCODER DISCRIMINATOR ARCHITECTURE
Figure 1 shows the outline of an Encoder Discriminator network. First, a random sample of the
dataset is picked and fed into an encoder network. This encoder network generates a latent descrip￾tion of the picked sample and passes it to a discriminator network. The discriminator network takes
the sample description and compares it with the original as well as other samples from the dataset.
For each sample, the discriminator returns a confidence value of how certain the network is, that the
latent description indeed comes from that sample. These confidence values are then passed to a soft￾max function which computes the predicted probabilities of the samples to be the original randomly
picked sample. Finally, the aim of this network is to minimize the cross entropy between the picked
samples and the computed probability distribution. A more detailed overview of the architectures
used in the encoder and discriminator networks is given in Figure 2.
2
Under review as a conference paper at ICLR 2019
Figure 2: Architecture of the Encoder and Discriminator networks. The convolutional layers use
relu activation functions, the fully connected layers use sigmoid activations.
2.3 DISCRIMINATOR LOSS
In this section, we want to motivate our method from an information entropy stand point. So let
us first assume, we have an encoder function, latent(xj ), which maps xj from sample space onto a
latent description space. Then we can introduce the probability distribution p(xi = xj |latent(xj ))
over xi which describes the probability that sample xi
is equal to xj given the latent representation
of xj . The latent description is most precise if latent(xj ) is chosen such that the entropy of p(xi = xj |latent(xj )) is minimized:
H(p(xi = xj |latent(xj ))) =
−X xi p(xi = xj |latent(xj )) log(p(xi = xj |latent(xj )))
This entropy value is minimized when there is no xi other than xj which is mapped on the same
latent(xj ). In other words, this means latent(xj ) provides a unique description of xj . Now, we can
sum the entropy of p(xi = xj |latent(xj )) over all xj in order to enforce latent(xj ) to provide good
descriptions for all samples:
ˆH = X xj H(p(xi = xj |latent(xj )))
ˆH is minimized by latent(xj ), if latent(xj ) provides a unique description for every xj .
This condition alone, however, is not sufficient to also learn meaningful representations since a
simple hashing function which returns a unique hash value for every sample would give perfect
results with respect to ˆH. In the end, we want to make sense of the latent description and this is
where the discriminator network q(latent(xj ), xi) comes in. By training q on p, we can ensure
that the descriptions of the encoder are interpretable by the discriminator. By minimizing the cross
entropy between p and q, we can train q to become closer and closer to p. Now the loss function
3
Under review as a conference paper at ICLR 2019
becomes:
L(latent, q) = X
xj ,xi −p(xi = xj |latent(xj )) log(q(latent(xj ), xi))
= X xj ,xi −δi,j log(q(latent(xj ), xi))
Here δi,j is 1 for i=j and 0 otherwise. This is a valid assumption since the probability that latent(xj )
maps two different samples of the dataset onto exact the same position in the continuous latent
space is basically zero. Thus, p(xi = xj |latent(xj )) becomes δi,j . As q approaches p, the loss
function approximates the entropy ˆH as defined above. Hence, training on this loss end-to-end not
only optimizes the discriminator q with respect to the cross-entropy but also the encoder to make
as precise as possible descriptions. Note, however, that q can not be calculated with one separate
sample alone but always needs other samples to be properly normalized in the softmax layer.
2.4 LATENT DISTANCE
With the results obtained above, we can introduce a probabilistic measure of distance, which might
be helpful to find related samples in a dataset:
d(xi
, xj ) = log  q(latent(xi), xi) q(latent(xi), xj )
Note, that this definition of a distance is neither commutative nor positive definite and should not
be confused with the L2 distance in latent space. It rather is a measure of how likely it is that the
discriminator confuses xj with xi given the latent description of xi
. The smaller the distance, the
more likely is a confusion and the more similar are xi and xj . If xi = xj , the distance is 0.
3 EXPERIMENTS
We performed two experiments using the proposed encoder discriminator architecture. The first one
was performed on the MNIST (Modified National Institute of Standards and Technology) database
(LeCun et al. (2010)). The second was done on the Fashion MNIST dataset (Xiao et al. (2017)).
3.1 TRAINING DETAILS
The network was trained on batches of 100 samples and the latent space was chosen to be two
dimensional and bounded between zero and one. We furthermore added Gaussian noise and random
affine transformations to the samples before they were passed to the encoder and discriminator
networks. This greatly improves the performance since it enforces the encoder to learn more robust
representations.
3.2 MNIST
The MNIST dataset contains 60.000 training and 10.000 test images of handwritten digits. Figure 3
shows how the encoder network maps samples of the 10 different digit classes onto the latent space.
The encoder clearly separates the different digit classes and thus learns useful abstraction features.
However there are still problems in particular in between the digits classes 8 and 5 as well as 4
and 9. This matches our intuition that these classes should be the hardest to discriminate. Another
observation is that the encoder fills the available latent space ”densely”, because the encoder has to
use the available latent space as efficiently as possible. This is in contrast to other common clustering
algorithms that usually try to cover semantic differences in the form of larger L2 distances in latent
space. Figure 4 shows some MNIST images plotted in the latent space. As one can clearly see
in the class of ones, the encoder also learns additional features (like the digit angle) that help the
discriminator to also distinguish between samples of the same class.
4
Under review as a conference paper at ICLR 2019
Figure 3: Test labels of the MNIST set plotted in the two dimensional latent space. A plot of the
samples themselves in latent space can be found in Figure 4
Figure 4: Test samples of the MNIST set plotted in the two dimensional latent space. Best viewed
in electronic form
3.3 FASHION MNIST
The Fashion MNIST dataset is a dataset containing 60.000 training and 10.000 test images of 10
different product categories available at Zalando. These categories comprise T-shirts/tops, trousers,
pullovers, dresses, coats, sandals and more. The idea behind this dataset is that different classes
of product categories contain samples with higher variability than different digits of the original
MNIST dataset and thus are harder to cluster. Figure 5 shows how the encoder network maps
samples of the different product categories onto the latent space. As for the MNIST dataset, the
encoder fills the latent space densely and is capable of separating several categories. Problems arise
in particular in between the categories ”Pullover”, ”Coat”, ”Shirt” — categories that are sometimes
even for the human eye hard to differentiate. Figure 6 shows some Fashion MNIST images plotted in
the latent space. In this plot one can clearly observe that on top of sorting for categories, the encoder
5
Under review as a conference paper at ICLR 2019
Figure 5: Test labels of the Fashion MNIST set plotted in the two dimensional latent space. A plot
of the samples themselves in latent space can be found in Figure 6
also sorts the images after brightness (brighter samples are in the center whereas darker samples are
located farther outside).
Figure 6: Test samples of the Fashion MNIST set plotted in the two dimensional latent space. Best
viewed in electronic form
4 CONCLUSION
In this work, we motivated a new encoder discriminator architecture by taking inspiration from the
game ”Taboo” and using the concept of information entropy. First promising experiments on the
MNIST and Fashion MNIST dataset show that this approach is capable of clustering samples of
the same class. We thus conclude that this encoder discriminator networks can learn unsupervised
meaningful abstractions that might be helpful in tasks like image classification or scene understand￾ing.
6
Under review as a conference paper at ICLR 2019
In the future this architecture could be applied to larger datasets such as ImageNet and a higher
dimensional latent space in order to perform for example unsupervised transfer learning (Pan et al.
(2010)) for classification tasks. Another application could be to use the output of the encoder net￾work as the source of latent noise for generator networks in GANs. By passing also this latent
description to the discriminator, the problem of mode collapse could be addressed.
REFERENCES
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep
convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):
295–307, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor￾mation processing systems, pp. 2672–2680, 2014.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504–507, 2006.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [On￾line]. Available: http://yann. lecun. com/exdb/mnist, 2, 2010.
Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345–1359, 2010.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096–1103. ACM, 2008.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark￾ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
7
