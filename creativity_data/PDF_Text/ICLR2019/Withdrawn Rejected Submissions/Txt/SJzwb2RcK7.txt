Under review as a conference paper at ICLR 2019
Adversarial Decomposition of Text Represen-
TATION
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we present a method for adversarial decomposition of text repre-
sentation. This method can be used to decompose a representation of an input
sentence into several independent vectors, where each vector is responsible for a
specific aspect of the input sentence. We evaluate the proposed method on two
case studies: the conversion between different social registers and diachronic lan-
guage change. We show that the proposed method is capable of fine-grained con-
trolled change of these aspects of the input sentence. For example, our model
is capable of learning a continuous (rather than categorical) representation of the
style of the sentence, in line with the reality of language use. The model uses
adversarial-motivational training and includes a special motivational loss, which
acts opposite to the discriminator and encourages a better decomposition. Finally,
we evaluate the obtained meaning embeddings on a downstream task of para-
phrase detection and show that they are significantly better than embeddings of a
regular autoencoder.
1	Introduction
Despite the recent successes in using neural models for representation learning for natural language
text, learning a meaningful representation of input sentences remains an open research problem. A
variety of approaches, from sequence-to-sequence models that followed the work of Sutskever et al.
(2014) to the more recent proposals (Arora et al., 2017; Nangia et al., 2017; Conneau et al., 2017;
Logeswaran & Lee, 2018; Subramanian et al., 2018; Cer et al., 2018) share one common drawback.
Namely, all of them encode the input sentence into just one single vector of a fixed size. One way to
bypass the limitations of a single vector representation is to use an attention mechanism (Bahdanau
et al., 2014; Vaswani et al., 2017). We propose to approach this problem differently and design a
method for adversarial decomposition of the learned input representation into multiple components.
Our method encodes the input sentence into several vectors, where each vector is responsible for a
specific aspect of the sentence.
In terms of learning different separable components of input representation, our work most closely
relates to the style transfer work, which has been applied to a variety of different aspects of language,
from diachronic language differences (Xu et al., 2012) to authors’ personalities (Lipton et al., 2015)
and even sentiment (Hu et al., 2017; Fu et al., 2017). The style transfer work effectively relies on
the more classical distinction between meaning and form (de Saussure, 1959), which accounts for
the fact that multiple surface realizations are possible for the same meaning. For simplicity, we will
use this terminology throughout the rest of the paper.
Consider the case when we encode an input sentence into a meaning vector and a form vector. We are
then able to perform a controllable change of meaning or form by a simple change applied to these
vectors. For example, we can encode two sentences written in two different styles, then swap the
form vectors while leaving the meaning vectors intact. We can then generate new unique sentences
with the original meaning, but written in a different style.
In the present work, we propose a novel model for this type of decomposition based on adversarial-
motivational training and design an architecture inspired by the GANs (Goodfellow et al., 2014)
and adversarial autoencoders (Makhzani et al., 2015). In addition to the adversarial loss, we use a
special motivator (Albanie et al., 2017), which, in contrast to the discriminator, is used to provide a
1
Under review as a conference paper at ICLR 2019
motivational loss to encourage the model to better decomposition of the meaning and the form, as
well as specific aspects of meaning. We make all the code publicly available on GitHub 1.
We evaluate the proposed methods for learning separate aspects of input representation on the fol-
lowing case studies:
1.	Learning to separate out a representation of the specific diachronic slice of language. One
may express the same meaning using the Early Modern English (e.g. What would she
have?) and the contemporary English ( What does she want?)
2.	Learning a representation for a social register (Halliday et al., 1968) - that is, subsets oflan-
guage appropriate in a given context or characteristic ofa certain group of speakers. These
include formal and informal language, the language used in different genres (e.g., fiction
vs. newspapers vs. academic texts), different dialects, and even literary idiostyles. We
experiment with the registers corresponding to the titles of scientific papers vs. newspaper
articles.
2	Related work
As mentioned above, the most relevant previous work comes from the style transfer research, and it
can be divided into two groups:
1.	Approaches that aim to generate text in a given form. For example, the task may be to
produce just any verse as long as it is in the “style” of the target poet.
2.	Approaches that aim to induce a change in either the “form” or the “meaning” of an existing
utterance. For example, “Good bye, Mr. Anderson.” can be transformed to “Fare you well,
good Master Anderson” (Xu et al., 2012)).
An example of the first group is the work by Potash et al. (2015), who trained several separate
networks on verses by different hip-hip artists. An LSTM network successfully generated verses
that were stylistically similar to the verses of the target artist (as measured by cosine distance on Tf-
Idf vectors). More complicated approaches use language models that are conditioned in some way.
For example, Lipton et al. (2015) produced product reviews with a target rating by passing the rating
as an additional input at each timestep of an LSTM model. Tang et al. (2016) generated reviews not
only with a given rating but also for a specific product. At each timestep a special context vector
was provided as input, gated so as to enable the model to decide how much attention to pay to that
vector and the current hidden state. Li et al. (2016) used “speaker” vectors as an additional input to a
conversational model, improving consistency of dialog responses. Finally, Ficler & Goldberg (2017)
performed an extensive evaluation of conditioned language models based on “content” (theme and
sentiment) and “style” (professional, personal, length, descriptiveness). Importantly, they showed
that it is possible to control both “content” and “style” simultaneously.
Work from the second group can further be divided into two clusters by the nature of the training
data: parallel aligned corpora, or non-aligned datasets. The aligned corpora enable approaching
the problem of form shift as a paraphrasing or machine translation problem. Xu et al. (2012) used
statistical and dictionary-based systems on a dataset of original plays by Shakespeare and their
contemporary translations. Carlson et al. (2017) trained an LSTM network on 33 versions of the
Bible. Jhamtani et al. (2017) used a Pointer Network (Vinyals et al., 2015), an architecture that was
successfully applied to a wide variety of tasks (Merity et al., 2016; Gulcehre et al., 2016; Potash
et al., 2017), to enable direct copying of the input tokens to the output. Note that these works use
BLEU (Papineni et al., 2002) as the main, or even the only evaluation measure. This is only possible
in cases where a parallel corpus is available.
Recently, new approaches that do not require a parallel corpora were developed in both CV (Zhu
et al., 2017) and NLP. Hu et al. (2017) succeeded in changing tense and sentiment of sentences
with a two steps procedure based on a variational auto-encoder (VAE) (Kingma & Welling, 2013).
After training a VAE, a discriminator and a generator are trained in an alternate manner, where
the discriminator tries to correctly classify the target sentence attributes. A special loss component
1http://github.com/placeholder
2
Under review as a conference paper at ICLR 2019
forces the hidden representation of the encoded sentence to not have any information about the
target sentence attributes. Mueller et al. (2017) used a VAE to produce a hidden representation of
a sentence, and then modify it to match the desired form. Unlike Hu et al. (2017), they do not
separate the form and meaning embeddings. Shen et al. (2017) applied a GAN to align the hidden
representation of sentences from two corpora and force them to do not have any information about
the form via adversarial loss. During the decoding, similarly the work by Lipton et al. (2015), special
“style” vectors are passed to the decoder at every timestep to produce a sentence with the desired
properties. The model is trained using the Professor-Forcing algorithm (Lamb et al., 2016). Kim
et al. (2017) worked directly on hidden space vectors that are constrained with the same adversarial
loss instead of outputs of the generator, and use two different generators for two different “styles”.
Finally, Fu et al. (2017) proposed two models for generating sentences with the target properties
using an adversarial loss, similarly to Shen et al. (2017) and Kim et al. (2017).
Comparison with previous work In contrast to the proposals of Xu et al. (2012), Carlson et al.
(2017), Jhamtani et al. (2017), our solution does not require a parallel corpus. Furthermore, unlike
the model by Shen et al. (2017), our model works directly on representation of sentences in the
hidden space.
Most importantly, in contrast to the proposals by Mueller et al. (2017), Hu et al. (2017), Kim et al.
(2017), Fu et al. (2017), our model produces a representation for both meaning and form and does
not treat the form as a categorical (in the vast majority of works, binary) variable. Although the
form was represented as dense vectors in previous work, it is still just a binary feature, as they use a
single pre-defined vector for each form, with all sentences of the same form assigned the same form
vector. In contrast, our work treats form as a truly continuous variable, where each sentence has its
own, unique, form vector.
Treating meaning and form not as binary/categorical, but as continuous is more consistent with the
reality of language use, since there are different degrees of overlap between the language used by
different registers or in different diachronic slices. Indeed, language change is gradual, and the
acceptability of expressions in a given register also forms a continuum, so one expects a substantial
overlap between the grammar and vocabulary used, for example, on Twitter and by New York Times.
To the best of our knowledge, this is the first model that considers linguistic form in the task of text
generation as a continuous variable.
One significant consequence of learning a continuous representation for form is that it allows the
model to work with a large, and potentially infinite, number of forms. Note that in this case the
locations of areas of specific forms in the vector style space would reflect the similarity between
these forms. For example, the proposed model could be directly applied to the authorship attribution
problem. In this case, each author would have their own area in the form space, and the more similar
the authors are in terms of writing style, the closer these areas would be to each other. We performed
preliminary experiments on this and report the results in Appendix A.
3	Formulation
Let us formulate the problem of decomposition of text representation on an example of controlled
change of linguistic form and conversion of Shakespeare plays in the original Early Modern to
contemporary English. Let Xa be a corpus of texts xia ∈ Xa in Early Middle English fa ∈ F,
and Xb be a corpus of texts xib ∈ Xb in modern English fb ∈ F. We assume that the texts in both
Xa and Xb has the same distribution of meaning m ∈ M. The form f , however, is different and
generated from a mixture of two distributions:
fi = αiap(fa) + αibp(fb)
where fa and fb are two different languages (Early Modern and contemporary English). Intuitively,
we say that a sample xi has the form fa if αia > αib, and it has the form f b if αib > αia .
The goal of dissociation meaning and form is to learn two encoders Em : X → M and Ef : X → F
for the meaning and form correspondingly, and the generator G : M, F → X such that
∀j ∈ {a, b}, ∀k ∈ {a, b} : G(Em(xk), Ef (xj )) → Xj
3
Under review as a conference paper at ICLR 2019
That is, the form of a generated sample depends exclusively on the provided fj and can be the in the
same domain for two different mu and mv from two samples from different domains Xa and Xb.
Note that, in contrast to the previously proposals, the form f is not a categorical variable but a
continuous vector. This enables fine-grained controllable change of form: the original form fi is
changed to reflect the form of the specific target sentence fj with its own unique αa and αb while
preserving the original meaning mi .
An important caveat concerns the core assumption of the similar meaning distribution in the two
corpora, which is also made in all other works reviewed in Section 2. It limits the possible use of
this approach to cases where the distributions are in fact similar (i.e. parallel or at least comparable
corpora are available). It does not apply to many cases that could be analyzed in terms of meaning
and form. For example, books for children and scholarly papers are both registers, they have their
own form (i.e. specific subsets of linguistic means and structure conventions) - but there is little
overlap in the content. This would make it hard even for a professional writer to turn a research
paper into a fairy tale.
4	Method description
Inspired by Makhzani et al. (2015), Kim et al. (2017), and Albanie et al. (2017), we propose ADNet,
a new model for adversarial decomposition of text representation (Figure 1).
Figure 1: Overview of ADNet. Encoder encodes
the inputs sentences into two latent vectors m and
f . The Generator takes them as the input and
produces the output sentence. During the train-
ing, the Discriminator is used for an adversar-
ial loss that forces m to do not carry any informa-
tion about the form, and the M otivator is used
for a motivational loss that encourages f to carry
the needed information about the form.
Our solution is based on a widely used
sequence-to-sequence framework (Sutskever
et al., 2014) and consists of four main parts.
The encoder E encodes the inputs sequence x
into two latent vectors m and f which capture
the meaning and the form of the sentence cor-
respondingly. The generator G then takes these
two vectors as the input and produces a recon-
struction of the original input sequence X.
The encoder and generator by themselves will
likely not achieve the dissociation of the mean-
ing and form. We encourage this behavior in
a way similar to Generative Adversarial Net-
works (GANs) (Goodfellow et al., 2014), which
had an overwhelming success the past few
years and have been proven to be a good way
of enforcing a specific distribution and charac-
teristics on the output of a model.
Inspired by the work of Albanie et al. (2017)
and the principle of ”carrot and stick” (Safire,
1995), in contrast to the majority of work that
promotes pure adversarial approach (Goodfel-
low et al., 2014; Shen et al., 2017; Fu et al., 2017; Zhu et al., 2017), we propose two additional
components, the discriminator D and the motivator M to force and motivate the model to learn the
dissociation of the meaning and the form. Similarly to a regular GAN model, the adversarial dis-
criminator D tries to classify the form f based on the latent meaning vector m, and the encoder E
is penalized to make this task as hard as possible.
Opposed to such vicious behaviour, the motivator M tries to classify the form based on the latent
form vector f , as it should be done, and encourages the encoder E to make this task as simple as
possible. We could apply the adversarial approach here as well and force the distribution of the
form vectors to fit a mixture of Gaussians (in this particular case, a mixture of two Guassians) with
another discriminator, as it is done by Makhzani et al. (2015), but we opted for the “dualistic” path
of two complimentary forces.
4
Under review as a conference paper at ICLR 2019
4.1	Encoder-Decoder
Both the encoder E and the generator G are modeled with a neural network. Gated Recurrent
Unit (GRU) (Chung et al., 2014) is used for E to encode the input sentence x into a hidden vector
h = GRU(x).
The vector h is then passed through two different fully connected layers to produce the latent vectors
of the form and the meaning of the input sentence:
m = tanh(Wmh + bm)	f = tanh(Wf h + bf)
We use θE to denote the parameters of the encoder E: Wm, bm, Wf, bf, and the parameters of the
GRU unit.
The generator G is also modelled with a GRU unit. The generator takes as input the meaning vector
m and the form vector f, concatenates them, and passes trough a fully-connected layer to obtain a
hidden vector z that represents both meaning and form of the original input sentence:
z = tanh(Wz [m; f] + bm)
After that, we use a GRU unit to generate the output sentence as a probability distribution over the
vocabulary tokens:
T
p(x) = Y p(Xt∣z, Xi,..., Xt-ι)
t=1
We use θG to denote the parameters of the generator G: Wz, bm, and the parameters of the used
GRU. The encoder and generator are trained using the standard reconstruction loss:
Lrec (Θe , Θg) = Ex 〜Xa [- log p(X∣x)]+ Ex 〜χb[-log p(X∣x)]
4.2	Discriminator
The representation of the meaning m produced by the encoder E should not contain any information
about the form f . We achieve this by using an adversarial approach. First, we train a discriminator
D, consisting of several fully connected layers with ELU activation function (Clevert et al., 2015)
between them, to predict the form f of a sentence by its meaning vector: fD = D(m), where f is
the score (logit) reflecting the probability of the sentence x to belong to one of the form domains.
Motivated by the Wasserstein GAN (Arjovsky et al., 2017), we use the following loss function
instead of the standard cross-entropy:
LD (Θd )= Ex 〜Xa D(Em(X))] - Ex 〜X b[D(Em(x))]
Thus, a successful discriminator will produce negative scores f for sentences from Xa and positive
scores for sentences from Xb . This discriminator is then used in an adversarial manner to provide a
learning signal for the encoder and force dissociation of the meaning and form by maximizing LD
: Ladv (θE) = -λadvLD, where λadv is a hyperparameter reflecting the strength of the adversarial
loss. Note that this loss applies to the parameters of the encoder.
4.3	Motivator
Our experiments showed that it is enough to have just the discriminator D and the adversarial loss
Ladv to force the model to dissociate the form and the meaning. However, in order to achieve a
better dissociation, we propose to use a motivator M (Albanie et al., 2017) and the corresponding
motivational loss. Conceptually, this is the opposite of the adversarial loss, hence the name. As the
discriminator D, the motivator M learns to classify the form f of the input sentence. However, its
input is not not the meaning vector but the form vector: fM = M(f).
The motivator has the same architecture as the discriminator, and the same loss function. While the
adversarial loss forces the encoder E to produce a meaning vector m with no information about
the form f, the motivational loss encourages E to encode this information in the form vector by
minimizing LM: Lmotiv (θE) = λmotivLM.
5
Under review as a conference paper at ICLR 2019
4.4	Training procedure
The overall training procedure follows the methods for training GANs (Goodfellow et al., 2014;
Arjovsky et al., 2017) and consists of two stages: training the discriminator D and the motivator M,
and training the encoder E and the generator G.
In contrast to Arjovsky et al. (2017), we do not train the D and M more than the E and the G. In
our experiments we found that simple training in two stages is enough to achieve dissociation of
the meaning and the form. Encoder and generator are trained with the following loss function that
combines reconstruction loss with the losses from the discriminator and the motivator:
Ltotal (θE, θG)
Lrec + Ladv + Lmotiv
5	Experimental setup
5.1	Evaluation
Similarly to the evaluation of style transfer in CV (Isola et al., 2017), evaluation of this task is
difficult. We follow the approach of Isola et al. (2017); Shen et al. (2017) and recently proposed
by Fu et al. (2017) methods of evaluation of “transfer strength” and “content preservation”. The
authors showed the proposed automatic metrics to a large degree correlate with human judgment
and can serve as a proxy. Below we give an overview of these metrics.
Transfer Strength. The goal of this metric is to capture whether the form has been changed suc-
cessfully. To do that, a classifier C is trained on the two corpora, Xa and Xb to recognize the
linguistic “form” typical of each of them. After that, a sentence the form/meaning of which was
changed is passed to the classifier. The overall accuracy reflects the degree of success of changing
the form/meaning. This approach is widely used in CV (Isola et al., 2017), and was applied in NLP
as well (Shen et al., 2017).
In our experiments we used a GRU unit followed by four fully-connected layers with ELU activation
functions between them as the classifier.
Content preservation Note that transfer strength by itself does not capture the overall quality of
a changed sentence. A extremely overfitted model that produces the same, the most characteristic
sentence of one corpus all the time would have a high score according to this metric. Thus, we need
to measure how much of the meaning was preserved while changing the form. To do that, Fu et al.
(2017) proposed to use a cosine similarity based metric using pretrained word embeddings. First,
a sentence embedding is computed by concatenation of max, mean, and average pooling over the
timesteps:
v = [max(v1, . . . , vT); min(v1, . . . , vT); mean(v1, . . . , vT)]
Next, the cosine similarity score si between the embedding vis of the original source sentence and
the target sentence with the changed form vit is computed, and the scores across the dataset are
averaged to obtain the total score:
1
S = 2
ι x1 ɪ
∣Xal i=ι si + |Xbl
|Xb|
Xsi
i=1
5.1.1	Continuous form
The metrics described above treat the form as a categorical (in most cases, even binary) variable.
This was not a problem in previous work since the change of form could be done by just inverting
the form vector. Our work, in contrast, treats the form as a continuous variable, and, therefore, we
cannot just use the proposed metrics directly. To enable a fair comparison, we propose the following
procedure.
For each sentence ssa in the test set from the corpus Xa we sample k = 10 random sentence from
the corpus Xb of the opposite form. After that, we encode them into the meaning mi and form fi
6
Under review as a conference paper at ICLR 2019
vectors, and average the form vectors to obtain a single form vector favg = 1 Pk=1 f We then
generate a new sentence with its original meaning vector ms and the resulting form vector favg, and
use it for evalation. This process enables a fair comparison with the previous works that treat form
as a binary variable.
5.2	Datasets
We performed an extensive evaluation of the proposed method on several dataset that reflect different
changes of meaning, form, or specific aspects of meaning, such as sentiment polarity.
Changing form: register This experiment is conducted with a dataset of titles of scientific papers
and news articles published by Fu et al. (2017). This dataset (referred to as “Headlines”) contains
titles of scientific articles crawled from online digital libraries, such as “ACM Digital Library” and
“arXiv”. The titles of the news articles are taken from the “News Aggregator Data Set” from UCI
Machine Learning Repository (Dheeru & Karra Taniskidou, 2017)
Changing form: language diachrony Diachronic language change is explored with the dataset
composed by Xu et al. (2012). It includes the texts of 17 plays by William Shakespeare in the
original Early Modern English, and their translations into contemporary English. We randomly
permuted all sentences from all plays and sampled the training, validation, and test sets. Note that
this is the smallest dataset in our experiments.
Previous work on style transfer for text also included the experiments with changing sentiment
polarity (Shen et al., 2017; Fu et al., 2017). We do not report the experiments with sentiment data,
since the change in sentiment polarity corresponds to a change in a specific aspect of meaning, rather
than form. We therefore believe the comparison with these data would not be instructive.
6	Results and discussion
0.950
§ 0.925
≥ 0.900
tυ
S
tu
⅛. 0-875
S 0.850
U
O
υ 0.825
0.800
256. 64
* *256,256
J54, 256
¾56, 64
⅛56, 64
Fu et al
ADNet
ADNet + Motivator
.256, 256
*256, 256
64, 256. 64, 256=
0.88
.9 0.86
ro
≥
g 0.84
§0-32
U
3 0.80
0.78
256. 256
*	*256, 64
?56, 64
¾56, 64
Fu etal
ADNet
ADNet + Motivator
64, 25⅞
64, 256
i256, 256
256, 256
J54, 256
0.20	0.25	0.30	0.35	0.40	0.45
Transfer strength
0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90
Transfer strength
(a)	Shakespeare dataset
(b)	Headlines dataset
Figure 2: Transfer strength vs Content preservation (see subsection 5.1) for different combination of
the size of the meaning and form vectors. Each point is labeled with ”[meaning vector size3 1form
vector size%”.
Probably, the most recent and similar to our work is the model proposed by Fu et al. (2017), in
particular the “style-embedding” model. We implemented this model to provide a baseline for com-
parison.
The classifier used in the transfer strength metric achieves very high accuracy (0.832 and 0.99 for the
Shakespeare and Headlines datasets correspondingly). These results concur with the results of Shen
et al. (2017) and Fu et al. (2017), and show that the two forms in the corpora are significantly
different.
Following Fu et al. (2017), we show the result of different configuration of the size of the form and
meaning vectors on Figure 2. Namely, we report combinations of 64 and 256-dimensional vectors.
Note that the sizes of the form vector are important. The larger is the form vector, the higher is the
7
Under review as a conference paper at ICLR 2019
transfer strength, but smaller is content preservation. This is consistent with Fu et al. (2017), where
they observed a similar behaviour.
It is clear that the proposed method achieves significantly better transfer strength then the previously
proposed model. It also has a lower content preservation score, which means that it repeats fewer
exact words from the source sentence. Note that a low transfer strength and very high (0.9) content
preservation score means that the model was not able to successfully learn to transfer the form
and the target sentence is almost identical to the source sentence. The Shakespeare dataset is the
hardest for the model in terms of transfer strength, probably because it is the smallest dataset, but the
proposed method performs consistently well in transfer of both form and meaning and, in contrast
to the baseline.
Fluency of generated sentences Note that there is no guarantee that the generated sentences
would be coherent after switching the form vector. In order to estimate how this switch affects
the fluency of generated sentences, we trained a language model on the Shakespeare dataset and
calculated the perplexity of the generated sentences using the original form vector and the average
of form vectors of k random sentences from the opposite style (see subsubsection 5.1.1). While the
perplexity of such sentences does go up, this change is not big (6.89 vs 9.74).
6.1	Impact of the motivational training
To investigate the impact of the motivator, we visualized form and meaning embeddings of 1000
random samples from the Headlines dataset using t-SNE algorithm (Van Der Maaten, 2014) with
the Multicore-TSNE library (Ulyanov, 2016). The result is presented in Figure 3.
There are three important observations. First, there is no clear separation in the meaning embed-
dings, which means that any accurate form transfer is due to the form embeddings, and the dissoci-
ation of form and meaning was successful.
Second, even without the motivator the model is able to produce the form embeddings that are
clustered into two group. Recall from section 4 that without the motivational loss there are no forces
that influence the form embeddings, but nevertheless the model learns to separate them.
However, the separation effect is much more pronounced in the presence of motivator. This explains
why the motivator consistently improved transfer strength of ADNet, as shown in Figure 2.
(a) Without motivator
Figure 3: t-SNE visualization of the form and meaning embeddings of 1000 random sentences.
Green point represent sentences form news headlines, and red points represent titles of scientific
articles.
(b) With motivator
6.2	Qualitative evaluation
Table 1 and Table 2 show several examples of the successful form/meaning transfer achieved by
ADNet. Table 1 presentes the results of an experiment that to some extent replicates the approach
taken by the authors who treat linguistic form as a binary variable (Shen et al., 2017; Fu et al., 2017).
The sentences the original Shakespeare plays were averaged to get the “typical” Early Modern En-
glish form vector. This averaged vector was used to decode a sentence from the modern English
translation back into the original. The same was done in the opposite direction.
8
Under review as a conference paper at ICLR 2019
Aye, sir. (EME)	→	Yes, sir. (CE)
Fare thee well, my lord (EME)	→	Fare you well, my lord (CE)
This guy will tell us everything. (CE)	→	This man will tell us everything.	(EME)
I’ve done no more to caesar than you will do to me. (CE) →	I have done no more to caesar than,	you shall do to me. (EME)
Table 1:	Decoding of the source sentence from Early Modern English (EME) into contemporary
English (CE), and vice versa.
Table 2 illustrates the possibilities of ADNet on fine-grained transfer applied to the change of regis-
ter. We encoded two sentences in different registers from the Headlines dataset to produce form and
meaning embeddings, and then we decoded the first sentence with the meaning embedding of the
second, and vice versa. As can be seen from Table 2, the model correctly captures the meaning of
sentences and decodes them using the form of the source sentences. Note how the model preserves
specific words and the structure of the source sentence. In particular, note how in the first example,
the model decided to put the colon after the “crisis management”, as the source form sentence has
this syntactic structure (“A review:”). This is not possible in the previously proposed models, as they
treat form as just a binary variable.
A review: detection techniques for LTE system	CriSiS management: media practices in telecommunication management
Situation management knowledge from social media -A review study against intelligence internet
Security flaw could not affect digital devices, experts say	Semantic approach approach: current multimedia networks as modeling processes
Semantic approach to event processing	-*-"ʌ`Security flaw to verify leaks
Table 2:	Flipping the meaning and the form embeddings of two sentence from different registers.
Note the use of the colon in the first example, and the use of the “to”-constructions in the second
example, consistent with the form of the source sentences.
6.3 Performance of meaning embeddings on downstream tasks
We conducted some experiments to test the assumption that the derived meaning embeddings should
improve performance on downstream tasks that require understanding of the meaning of the sen-
tences regardless of their form. We evaluated embeddings produced by the ADNet, trained in the
Headlines dataset, on a task of paraphrase detection. We used the SentEval toolkit (Conneau et al.,
2017) and the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). The F1 scores on this
task for different models are presented in Table 3. Note that all models, except InferSent, are unsu-
pervised. The InferSent model was trained on a big SNLI dataset, consisting of more than 500,000
manually annotated pairs. ADNet achieves the the highest score among the unsupervised systems
and outperforms the regular sequence-to-sequence autoencoder with a large gap.
BoW	Seq2Seq	InferSent	Fu et al. (2017)	ADNet
80.82	74.68	83.17	78.88	81.38
Table 3: F1 scores on the task of paraphrase detection using the SentEval toolkit (Conneau et al.,
2017)
7 Conclusion
In this paper, we presented ADNet, a new model that performs adversarial decomposition of text
representation. In contrast to previous work, it does not require a parallel training corpus and works
directly on hidden representations of sentences. Most importantly, is does not treat the form as a
binary variable (as done in most previously proposed models), enabling a fine-grained change of
the form of sentences or specific aspects of meaning. We evaluate ADNet on two tasks: the shift of
language register and diachronic language change. Our solution achieves superior results, and t-SNE
visualizations of the learned meaning and style embeddings illustrate that the proposed motivational
loss leads to significantly better separation of the form embeddings.
9
Under review as a conference paper at ICLR 2019
References
Samuel Albanie, Sebastien Ehrhardt, and Joao F Henriques. Stopping gan violence: Generative
unadversarial networks. arXiv preprint arXiv:1703.02528, 2017.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In International Conference on Learning Representation, 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Keith Carlson, Allen Riddell, and Daniel Rockmore. Zero-shot style transfer in text using recurrent
neural networks. arXiv preprint arXiv:1711.04731, 2017.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. Universal sentence encoder. arXiv
preprint arXiv:1803.11175, 2018.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Alexis Conneau, Douwe Kiela, Holger Schwenk, LOiC Barrault, and Antoine Bordes. Supervised
learning of universal sentence representations from natural language inference data. In Proceed-
ings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 670-680,
2017.
Ferdinand de Saussure. Course in General Linguistics. New York : Philosophical Library, 1959.
URL http://archive.org/details/courseingenerall00saus.
Dua Dheeru and Efi Karra Taniskidou. UCI machine learning repository, 2017. URL http:
//archive.ics.uci.edu/ml.
Bill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Proceedings of the 20th international conference
on Computational Linguistics, pp. 350. Association for Computational Linguistics, 2004.
Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation.
In Proceedings of the Workshop on Stylistic Variation, pp. 94-104, 2017.
Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. Style transfer in text: Explo-
ration and evaluation. arXiv preprint arXiv:1711.06861, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. Pointing the
unknown words. In Proceedings of the 54th Annual Meeting ofthe Association for Computational
Linguistics (Volume 1: Long Papers), volume 1, pp. 140-149, 2016.
M. A. K. Halliday, A. McIntosh, and P. Stevens. The Linguistic Sciences and Language Teaching.
Longmans, Green and Co., London, 1968.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward controlled
generation of text. In International Conference on Machine Learning, pp. 1587-1596, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1125-1134, 2017.
10
Under review as a conference paper at ICLR 2019
Harsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric Nyberg. Shakespearizing modern language
using copy-enriched sequence-to-sequence models. arXiv preprint arXiv:1707.01161, 2017.
Yoon Kim, Kelly Zhang, Alexander M Rush, Yann LeCun, et al. Adversarially regularized autoen-
coders for generating discrete structures. arXiv preprint arXiv:1706.04223, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang, Saizheng Zhang, Aaron C
Courville, and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent net-
works. In Advances In Neural Information Processing Systems,pp. 4601-4609, 2016.
Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis, Jianfeng Gao, and Bill Dolan. A
persona-based neural conversation model. In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 994-1003,
2016.
Zachary C Lipton, Sharad Vikram, and Julian McAuley. Generative concatenative nets jointly learn
to write and classify reviews. arXiv preprint arXiv:1511.03683, 2015.
Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representa-
tions. arXiv preprint arXiv:1803.02893, 2018.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
Jonas Mueller, David Gifford, and Tommi Jaakkola. Sequence to better sequence: continuous re-
vision of combinatorial structures. In International Conference on Machine Learning, pp. 2536-
2544, 2017.
Nikita Nangia, Adina Williams, Angeliki Lazaridou, and Samuel R Bowman. The repeval 2017
shared task: Multi-genre natural language inference with sentence representations. arXiv preprint
arXiv:1707.08172, 2017.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
Peter Potash, Alexey Romanov, and Anna Rumshisky. Ghostwriter: using an lstm for automatic
rap lyric generation. In Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, pp. 1919-1924, 2015.
Peter Potash, Alexey Romanov, and Anna Rumshisky. Here’s my point: Joint pointer architecture
for argument mining. In Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing, pp. 1364-1373, 2017.
William Safire.	On Language - Gotcha!	Gang Strikes Again,
1995.	URL https://www.nytimes.com/1995/12/31/magazine/
on- language- gotcha- gang- strikes- again.html.
Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from non-parallel
text by cross-alignment. In Advances in Neural Information Processing Systems, pp. 6833-6844,
2017.
Efstathios Stamatatos. Authorship attribution using text distortion. In Proceedings of the 15th
Conference of the European Chapter of the Association for Computational Linguistics: Volume
1, Long Papers, volume 1, pp. 1138-1149, 2017.
Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J Pal. Learning general
purpose distributed sentence representations via large scale multi-task learning. arXiv preprint
arXiv:1804.00079, 2018.
11
Under review as a conference paper at ICLR 2019
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Jian Tang, Yifan Yang, Sam Carton, Ming Zhang, and Qiaozhu Mei. Context-aware natural language
generation with recurrent neural networks. arXiv preprint arXiv:1611.09900, 2016.
Dmitry Ulyanov. Multicore-tsne. https://github.com/DmitryUlyanov/
Multicore-TSNE, 2016.
Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. Journal of machine
learning research, 15(1):3221-3245, 2014.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
匕Ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 6000-6010, 2017.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural
Information Processing Systems, pp. 2692-2700, 2015.
Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. Paraphrasing for style. Pro-
ceedings of COLING 2012, pp. 2899-2914, 2012.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2223-2232, 2017.
A Multiple forms and stylistic similarities
(a) Meaning embeddings
(b) Form embeddings
Figure 4: t-SNE visualization of the form and meaning embeddings. Each color corresponds to a
different author.
In order to go beyond just two different forms, we experimented with training the model on a set of
literature novels from six different authors from Project Gutenberg2 written in two different time pe-
riods. A t-SNE visualization of the resulting meaning and form embeddings is presented in Figure 4.
Note how form embeddings create a six-pointed star. After further examination, we observed that
common phrases (for example, “Good morning” or “Hello!”) were embedded into the center of the
star, whereas the most specific sentences from a given author were placed into the rays of the star.
In particular, some sentences included character names, thus further research is required to mitigate
this problem. Stamatatos (2017) provides a promising direction for solving this.
2http://www.gutenberg.org/
12