Under review as a conference paper at ICLR 2019
Exploiting Cross-Lingual Subword Similari-
ties in Low-Resource Document Classification
Anonymous authors
Paper under double-blind review
Ab stract
Text classification must sometimes be applied in situations with no training data in a
target language. However, training data may be available in a related language. We
introduce a cross-lingual document classification framework (caco) between re-
lated language pairs. To best use limited training data, our transfer learning scheme
exploits cross-lingual subword similarity by jointly training a character-based em-
bedder and a word-based classifier. The embedder derives vector representations
for input words from their written forms, and the classifier makes predictions
based on the word vectors. We use a joint character representation for both the
source language and the target language, which allows the embedder to generalize
knowledge about source language words to target language words with similar
forms. We propose a multi-task objective that can further improve the model if
additional cross-lingual or monolingual resources are available. caco models
trained under low-resource settings rival cross-lingual word embedding models
trained under high-resource settings on related language pairs.
1	Introduction: Classifiers acros s Languages
Building an accurate document classifier with traditional supervised learning techniques requires a
large labeled corpus. Unfortunately, annotated data is often unavailable (or unreliable) in low-resource
languages. Sometimes the need for a low-resource classifier is acute: an earthquake in Haiti,1 unrest
in Ukraine, or food shortages in East Africa. Cross-lingual document classification (Klementiev
et al., 2012, CLDC) attacks this problem by using annotated dataset from a source language to build
classifiers for a target language.
The key challenge of cldc is to find a shared representation for documents from both languages. Once
we have a cross-lingual feature space, we can train a classifier on source language documents and apply
it on target language documents. Previous work uses a bilingual lexicon (Wu et al., 2008; Shi et al.,
2010; Andrade et al., 2015), machine translation (Banea et al., 2008; Wan, 2009; Zhou et al., 2016,
mt), topic models (Mimno et al., 2009), or pre-trained cross-lingual word embeddings (Klementiev
et al., 2012; Chen et al., 2016, clwe) to extract cross-lingual features. However, this may be
impossible in low-resource languages, as these methods require some combination of large parallel
or comparable text, high-coverage dictionaries, and monolingual corpora.
We propose a new cldc method for a truly low-resource setting, where unlabeled or parallel data
in target language is also limited or unavailable. Our system, Classification Aided by Convergent
Orthography (CACO) capitalizes on subword similarities between related language pairs. When the
source language and the target language come from the same family, they share cognate words, many
of which have similar forms and semantics. For example, “religious” in English, “religieux” in
French, and “religioso” in Italian and Spanish share the same meaning.
Previous cldc methods treat words as atomic symbols and ignore subword patterns. We instead use
a bi-level model with two components: a character-based embedder and a word-based classifier. The
embedder creates vectors for input words from their character sequences. Using the word embeddings,
the classifier then labels the document. We hope the embedder can learn morpho-semantic regularities,
while the classifier connects lexical semantics to labels. To allow cross-lingual transfer, we use a
single model for both languages, and we share character embeddings between source and target
1https://www.ushahidi.com/categories/haiti
1
Under review as a conference paper at ICLR 2019
languages. We jointly train the embedder and the classifier on annotated source language documents.
The embedder transfers knowledge about source language words to target language words with
similar orthographic features.
If we have additional unlabeled data such as a dictionary, pre-trained word embeddings, or parallel
text, we can fine-tune the model with multi-task learning. Specifically, we encourage the embedder
to produce similar word embeddings for dictionary word pairs, which captures patterns between
cognate pairs. We also teach the embedder to mimick pre-trained word embeddings in the source
language (Pinter et al., 2017), which exposes the model to more word types. When we have a good
reference model in a high resource language, we can train the model to match the output of the
reference model on parallel text.
We experiment on cldc between nine related language pairs on two datasets. caco can match
the accuracy of clwe-based models without using any target language data, and fine-tuning the
embedder with a small amount of additional resources further improves the accuracy of caco.
2	Classification Aided by Convergent Orthography
This section introduces our model, caco, which trains a multilingual document classifier using
labeled datasets in a source language S and applies the classifier to a low-resource target language T.
2.1	Model Architecture
Our model has a two-level architecture (Figure 1) that includes a character-level embedder e and a
word-level classifier f . The embedder e takes characters as inputs and produces a word embedding
vector. The classifier then computes a label distribution from the embeddings of all input words.
Formally, let W be an input document with a sequence of tokens W =(wi, w2,…，Wn). Our model
maps the document w to a distribution over possible labels y in two steps. First, we generate a word
embedding vi for each input word wi using the embedder e:
vi = e(wi).	(1)
We then feed the word embeddings to the classifier f to compute the distribution over labels y :
p(y | w) = f(hvi, V2 ,…，Vni).	(2)
We can use any sequence model for the embedder e and the classifier f. For our experiments, we
use a bidirectional lstm (Graves & Schmidhuber, 2005, bi-lstm) embedder and a deep averaging
network (Iyyer et al., 2015, dan) classifier.
bi-lstm Embedder: BI-LSTM is a powerful sequence model that captures complex non-local
dependencies. Character-level bi-lstm embedders are successfully used in many natural language
processing tasks (Ling et al., 2015a;b; Ballesteros et al., 2015; Lample et al., 2016). To embed a word
w, we pass its character sequence c to a left-to-right LSTM and the reversed character sequence c0
to a right-to-left lstm. We concatenate the final hidden states of the two lstm and apply a linear
transformation:
e(w) = We ∙ [LS-M(c); LSTM(C0)] + be,	(3)
where the functions LSTM and LSTM compute the final hidden states of the two LSTMs.
dan Classifier: A DAN is an unordered model that passes the arithmetic mean of the input word
embeddings through a multilayer perceptron and feed the final layer’s representation to a softmax
layer. We choose dan because it ignores cross-lingual variations in word orders (i.e., syntax) and
thus generalizes well in cldc. Despite its simplicity, dan achieves state-of-the-art accuracies on
both monolingual and cross-lingual document classification (Iyyer et al., 2015; Chen et al., 2016).
Let vι, V2,…，Vn be the input word embeddings. A DAN uses the average of the word embeddings
as the document representation z0 :
Z0 = mean(vι, V2,…，rn),	(4)
and z0 is passed through k layers of non-linearity:
Zi = g(Wi ∙ Zi-1 + bi),	(5)
2
Under review as a conference paper at ICLR 2019
DAN
Classifier
bi-lstm
Embedder
p(y | W) = Softmax(Wk+ιzk + bk+ι)
1
□ zk
1
• k layers of non-linearity
□ zο = 1 (vι + V2 + v3)
___________φ_______
□ V1	□v2
↑ ↑
Figure 1: In CACO, each input word Wi is mapped to a word vector vi by passing its characters
through a BI-LSTM embedder. The average of the word vectors z0 is then passed through k layers of
non-linearity and a final softmax layer to predict the label y .
□ V3
↑
where i ranges from 1 to k, and g is a non-linear activation function. The final representation zk is
passed to a softmax layer to obtain a distribution over the label y,
p(y |W) = softmax(Wk+1zk + bk+1).	(6)
2.2	Character-Level Cross-Lingual Transfer
To transfer character-level information across languages, the embedder uses the same character
embeddings for both languages. The character-level bi-lstm vocabulary is the union of the alphabets
for the two languages, and the embedder does not differentiate identical characters from different lan-
guages. For example, a Spanish “a” has the same character embedding as a French “a”. Consequently,
the embedder maps words with similar forms from both languages to similar vectors.
If the source language and the target language are lexically similar, the embedder can generalize
knowledge learned about source language words to target language words through shared orthographic
features. As an example, if the model learns that the Spanish word “religioso” is predictive of label y,
the model automatically infers that “religioso” in Italian is also predictive of y, even though the model
never sees any Italian text.
2.3	Objective
Our main training objective is supervised document classification. We jointly train the classifier and
the embedder to minimize average negative log-likelihood on labeled source language documents S :
Ls(θ)
—
lSSl	X logp(y | w; θ),
hw,yi∈S
(7)
where θ is model parameter, and S contains source language examples with words W and label y .
Sometimes we have additional resources for the source or target language. We use them to improve
our model with multi-task learning (Collobert et al., 2011) via three auxiliary tasks.
Word Translation (dict): There are many patterns when translating cognate words between related
languages. For example, Italian “e” often becomes “ie” when translating into Spanish. “Tempo”
(time) in Italian becomes “tiempo” in Spanish, and “concerto” (concert) in Italian becomes “concierto”
in Spanish. The embedder can learn these word translation patterns from a bilingual dictionary.
Let D be a bilingual dictionary with a set of word pairs hw, w0i, where w and w0 are translations of
each other. We add a term to our objective to minimize average squared Euclidean distances between
3
Under review as a conference paper at ICLR 2019
the embeddings of translation pairs (Mikolov et al., 2013):
Ld⑻=∣dD∣	X ke(w) - e(w0)k2.
hw,w0i∈D
(8)
A bilingual dictionary can also reveal false friends—words from different languages that have similar
forms but different meanings. For example, the Italian word “burro” (butter) is “mantequilla” in
Spanish, while the Spanish word “burro” (donkey) is “asino” in Italian. However, our current model’s
joint character embedding space treats “burro” (Spanish) and “burro” (Italian) identically. Therefore,
we consider a variant of CACO with language identifiers (in § 4 these variants are denoted by +).
Specifically, the vocabulary of the embedder now includes two special language identifier characters
for the two languages. We prepend and append the identifier to each input word. This allows the
embedder to differentiate words from different languages, but the added expressiveness might induce
overfitting to the dictionary and prevent generalization through orthographic features.
Mimicking Word Embeddings (mim): Monolingual text classifiers often benefit from initializing
embeddings with word vectors pre-trained on large unlabeled corpus (Collobert et al., 2011). This
semi-supervised learning strategy helps the model generalize to word types outside labeled training
data. Similarly, our embedder can MIMICK (Pinter et al., 2017; Stratos, 2017) an existing source
language word embeddings to learn and transfer useful representations.
Suppose we have a pre-trained source language word embedding matrix X with V rows. The i-th
row xi is a vector for the i-th word type wi . We add an objective to minimize the average squared
Euclidean distances between the output of the embedder and X:
1V
Le(θ) = V ɪ^ke(Wi)- xik2.	⑼
Parallel Projection: Sometimes we have a reliable reference classifier in another high-resource
language S0 (for example, English). If we have parallel text between S and S0, we can use parallel
projection (Yarowsky et al., 2001) to add additional training signal. Let P be a set of parallel
documents hw, w0i, where w is from source language S, and w0 is the translation of w in S0. We add
another objective term to minimize the average Kullback-Leibler divergence between the predictions
of our model predictions and the reference model:
Lp(θ) = IPPi	X	DklM(∙ I w') k p(∙ I w)],
hw,w0i∈P
(10)
where p0 is the output of the reference classifier (in language S0), andp is the output of CACO. In § 4,
we mark models that use parallel projection with a superscript “p”.
We train our model on the four tasks jointly. Our final objective is:
L(θ) = Ls(θ) +λd Ld(θ) +λeLe(θ)+λpLp(θ),	(11)
l~{z} l^z}	|^^}	||"{z"/
classifier	dictionary	embed	parallel
where the hyperparameters λd, λe, and λp trade off between the four tasks.
3 Related Work
Previous cldc methods are typically word-based and rely on one of the following cross-lingual
signals to transfer knowledge: large bilingual lexicons (Wu et al., 2008; Shi et al., 2010; Andrade
et al., 2015), mt systems (Banea et al., 2008; Wan, 2009; Zhou et al., 2016), or cross-lingual word
representations (Klementiev et al., 2012; Chen et al., 2016). Unfortunately, these resources are not
available in every language. Recent work proposes unsupervised methods for building clwe (Artetxe
et al., 2018a; Conneau et al., 2018; Zhang et al., 2017a;b) and mt systems (Artetxe et al., 2018b;
Lample et al., 2018a;b) without any cross-lingual signal. However, these methods still require large
monolingual corpora in the target language, and they might fail when the monolingual corpora for the
two languages Come from different domains (S0gaard et al., 2018). In contrast, CACO is much more
4
Under review as a conference paper at ICLR 2019
Table 1:	cldc experiments between eight related European language pairs on rcv2 topic identi-
fication. The CACO models are competitive with DAN models that use far more resources. The
combined model (COM) achieves the highest average test accuracy. We boldface the best result for
each row, UnderIine CACO results that outperform at least one DAN model.
DAN	CACO
,--------A---------{	,----------------A----------------{
source target mCCA mClu sup src dict dict+	mim all all+	com
DA	SV	一9.一	一一.-	59.7
SV	I Ida	5-.一	一-.一	5一.7
FR	Mes	一一.9	71.8	5一.一
Oit	Mes	一一.一	55.一	5一.一
Mes	FR	63.1	一-.-	一一.9
IT	FR	一一.7	66.5	一一.9
Ofr	IT	一一.一	59.一	一一.9
Mes	I Iit	5-.一	5-.一	一一.9
	average	5-.一	5一.一	5-.9
∙9一92 一042∙5一99 一
216-O-4-7 7-8-
656444445
9一2一 4一一用 2-2-5-5-
2一2一7一4一8 8一1一2一4一
665544555
4一4一3 4一2一6一3一4一5
0-8-8 1-9-6-4-3-1
654544455
一 4一用一一 2 2一4一6一。_
2一3一9一8一3 2一4一7一4一
665一一55一5
一2一3一6一一2一一一
2- O- 9-4- 9- 2- 3一 3一 5一
6655一5555
-76一59一7-
6 6一--R-
55一5一一一一5
.7.5.8.5.3.8.2.7.5
970312094
667666656
Table 2:	cldc experiments between Amharic and Tigrinya on lorelei disaster response dataset.
caco models (bottom) outperform mClu models (top) without using any target language data. For
am-ti, parallel projection (srcP and mimP) further improves caco models. We do not experiment
with parallel projection on Tigrinya because we cannot find enough unlabeled parallel text in the
language pack. We boldface the best result for each row and UnderIine CACO results that outperform
one dan model.
source target
DAN
z----A----{
mCCA mClu
______CACO______
SRC MIM SRCP MIMP
I I am I Iti
Oti Γ^-l am
591 ^^558^^555
58.1	50.0	56.8
56.3	57.0	57.6
55.1
data-efficient. By exploiting character-level similarities between related languages, caco trained
with few or no target language data is competitive with clwe-based models.
Our work builds on the success of character-level bi-lstm embedders in monolingual NLP tasks,
including language modeling and part-of-speech tagging (Ling et al., 2015a), named entity recog-
nition (Lample et al., 2016), and dependency parsing (Ballesteros et al., 2015). Character-level
embedders can generate useful representations for rare and unseen words, especially for morpho-
logically rich languages. We extend this to cldc. Recent work also uses character bi-lstms to
directly reconstruct pre-trained monolingual word embeddings (Pinter et al., 2017; Stratos, 2017),
which motivates our mimick objective.
Cross-lingual transfer at the character-level is successfully used in low-resource paradigm comple-
tion (Kann et al., 2017), morphological tagging (Cotterell & Heigold, 2017), POS tagging (Kim et al.,
2017), and named entity recognition (Cotterell & Duh, 2017; Lin et al., 2018), where the authors
train a character-level model jointly on a small labeled corpus in target language and a large labeled
corpus in source language. Our method is similar in spirit, but we focus on cldc, where it is less
obvious if orthographic features are helpful. Moreover, we introduce a novel multi-task objective to
use different types of monolingual and cross-lingual resources; we can match clwe-based models
using little or no target language data.
4	Experiments
We evaluate our method on two cldc datasets: Reuters multilingual corpora (Lewis et al., 2004,
rcv2) and lorelei language packs (Strassel & Tracey, 2016). To show the effectiveness of our
method, we train caco models in a low-resource setting and compare two clwe-based model
trained in a high-resource setting and a supervised monolingual model. Our goal is not to beat the
5
Under review as a conference paper at ICLR 2019
Table 3: Results of cldc experiments using two source languages. Models trained on two source
languages are generally better than models trained on only one source language (Table 1). We
boldface the best result for each row, UnderIine CACO results that outperform at least one DAN model.
		DAN z	A	{		Z		CACO			{
source	target	mCCA mClu		SRC	DiCT	DiCT+	MiM	ALL	ALL+
I Il Ifr+IT	IJt IES	74.2	77.0	58.8	67.0	61.1	55.8	65.3	60.4
E□Oes+it	I Ifr	55.2	65.3	51.8	55.8	50.3	50.3	56.0	52.5
HOes+fr I I it		45.4	61.0	53.2	56.1	55.6	55.9	56.5	53.2
	average	58.3	67.8	54.6	59.6	55.7	54.0	59.3	55.4
Table 4: Word translation accuracies (P@1) for different embeddings. The caco embeddings are
generated by the embedder of a src model trained on the source language. Without any cross-lingual
signal, the caco embedder has similar word translation accuracy as mCCA and mCluster, which are
supervisedly trained on large dictionaries.
source	target	mCCA	mClu	CACO
Ia Ies	LJFR	36.8	40.8	31.1
Mes	Oit	44.0	35.7	33.1
FR	Mes	34.0	40.2	30.9
FR	Oit	33.5	40.3	29.6
IT	Mes	42.1	37.8	37.5
I Iit	Ofr	35.6	39.8	36.4
average		37.7	39.1	33.1
baselines, but to show that caco is competitive with the baselines despite training on much
less target language data.
4.1	Classification Dataset
Our first dataset is rcv2, a multilingual collection of news stories labeled with topics (Lewis et al.,
2004).2 Following Klementiev et al. (2012), we remove documents with multiple topic labels. For
each language, we sample 1,500 training documents and 200 test documents with balanced labels. We
conduct cldc experiments between two North Germanic languages, Danish (da) and Swedish (sv),
and three Romance languages, French (fr), Italian (it), and Spanish (es).
To test caco on truly low-resource languages, we build a second cldc dataset with famine-related
documents sampled from Tigrinya (ti) and Amharic (am) lorelei language packs (Strassel &
Tracey, 2016).3 We train models to classify whether the document describes widespread crime or
not.4 The Amharic language pack does not have annotations, so we manually label Amharic sentences
based on English reference translations.5 Our final dataset contains 394 Tigrinya and 370 Amharic
documents with balanced labels.
4.2	Models
We experiment with several variants of caco that use different resources. The SRC model is only
trained on labeled source language documents and do not use any unlabeled data. The DICT model
is trained on both labeled documents and parallel dictionary with the supervised objective and the
word translation objective. The MIM model is jointly trained to label source language documents and
2Corporate/Industrial (ccat), Economics (ecat), Government/Social (gcat), and Markets (mcat).
3Amharic: LDC2016E87. Tigrinya: LDC2017E57.
4For Tigrinya documents, the labels are extracted from the situation frame annotation in the language pack.
We mark all documents with a “crimeviolence” situation frame as positive.
5Annotations available after blind review.
6
Under review as a conference paper at ICLR 2019
mimick the source language part of multiCCA embeddings. The ALL model is trained using both the
word translation and the mimick tasks.
All of the above models do not use language identifier characters and therefore cannot distinguish
words from different languages with identical forms. For dict and all, we also experiment with
their variants that prepend and append a language identifier character to each word. We name these
models DICT+ and ALL+ . We also use parallel projection to provide more classification signals for
some models. We mark these models with a superscript “p”.
For comparison, we train two word-based dans with pre-trained clwe features. We use the
multiCCA (mCCA) and multiCluster (mClu) embeddings (Ammar et al., 2016), which are trained
on large corpora with millions of tokens and high-coverage dictionaries with hundreds of thousands
of word types. The clwe-based dans are strong high-resource models that require large corpora
and dictionary for the target language. In contrast, caco models are trained with few or no target
language data in a simulated low-resource setting. Therefore, it is unfair to directly compare the
results of caco and clwe-based models. However, CACO models are often competitive with
clwe-based dans in our experiment, demonstrating the effectiveness of our proposed method. We
also compare caco with a lightly-supervised monolingual model SUP,6 a dan trained on fifty labeled
target language documents. Without using any target language supervision, caco models achieves
similar (and sometimes higher) test accuracies as sup.
Finally, we experiment with a model that combines caco and clwe. Specifically, we use pre-trained
multiCluster clwe as additional features for the classifier of a src model. The combined model on
average significantly outperforms both caco models and clwe-based dan, which shows that our
method is useful even when we have a high-quality cross-lingual word embedding.
4.3	Dictionary and Parallel Text
Some of the caco models use a dictionary to learn word translation patterns. We train them on the
same training dictionary used for learning multiCCA and multiCluster. To simulate the low-resource
setting, we sample only 100 translation pairs from the original dictionary for CACO.
The original dictionary contains many word types with identical forms. We remove these pairs when
sampling the dictionary for dict and all, because without the language identifiers, the embedder
always maps words with the same form to the same vector. However, our pilot experiment shows that
removing these pairs could hurt the test accuracy for dict+ and all+. The embedder likely overfits
the translation pairs by aggressively tuning the character embedder for the language identifiers. As a
result, the embedder behaves very differently for the two languages, which prevents cross-lingual
knowledge transfer. Therefore, we keep translation pairs with same surface forms when sampling
dictionaries for dict+ and all+ .
The Amharic labeled dataset is very small compared to other languages,7 so we experiment with the
parallel projection technique for the Amharic to Tigrinya cldc experiment using English-Amharic
parallel text. We first train a reference English dan on a large collection of labeled English documents
compiled from other lorelei language packs. We then use the parallel projection objective to train
the caco models to match the output of the English model on 1,200 English-Amharic parallel
documents sampled from the Amharic language pack.8 We do not use parallel projections on other
language pairs, because we have enough labeled examples for the rcv2 languages, and we do not
have enough unlabeled parallel text in the Tigrinya language pack.
4.4	Results and Analysis
We train each model using ten different random seeds and report their average test accuracy.9 We
describe training details and hyperparameters in the appendix. Table 1 and Table 2 show average test
accuracies of different models on rcv2 and lorelei on nine related language pairs.
6We only apply this baseline to rcv2, because the test sets in lorelei are too small to split further.
7Each Amharic document only contains one sentence.
8To avoid introducing extra bias, we sample the parallel documents such that the English model output
approximately follows a uniform distribution.
9For models that use dictionaries, we also re-sample the training dictionary for each run.
7
Under review as a conference paper at ICLR 2019
caco vs. dan: The low-resource CACO models have similar average test accurcy as the high-
resource dan models. The src variant does not use any target language data, and yet it outperforms
mCCA on three language pairs. When we already have a good clwe, we can get the best of both
world by combining them (com), which achieves the highest average test accuracy.
Multi-Task Learning: Training CACO with multi-task learning further improves the accuracy.
For almost all language pairs, the multi-task caco variants have higher test accuracies than src.
On rcv2, word translation (dict) is particularly effective, even with only 100 translation pairs.
Interestingly, word translation and mimick tasks together (all) do not consistently increase the
accuracy over only using the dictionary (dict). On the lorelei dataset where labeled document is
limited, parallel projection task (srcP and mimP) also significantly increases the accuracies.
Language Identifier: The variants that use language identifiers (DICT+ and ALL+) have lower
average test accuracies than their counterparts (DICT and ALL). As discussed in §2.3, language
identifiers could cause the embedders to overfit the training dictionary. The embedder might learn to
behave very differently for the two languages, which prevents generalization across languages.
Language Relatedness: We expect CACO to be less effective when the source language and the target
language are less close to each other. For comparison, we experiment on rcv2 with transferring
between more distantly related language pairs: a North Germanic language and a Romance language.
We include the results in the appendix (Table 5). Indeed, caco models score consistently lower than
the dan models when transferring from a North Germanic source language to a Romance target
language. However, caco models are surprisingly competitive with dan models when transferring
from a Romance language to a North Germanic language, which shows that our method is useful
even for loosely related languages. This asymmetry is likely due to morphology differences between
the two families.
Multi-Source Transfer: We experiment with multi-source transfer learning on RCV2 by training
models on two Romance languages and testing on another Romance language. Languages can be
similar along different dimensions, and therefore adding more source languages may be beneficial.
Moreover, using multiple source languages has a regularization effect and prevents the model from
overfitting to a single language. For fair comparison, we sample 750 training documents from each
source language, so that the multi-source models are still trained on 1,500 training documents (same
as the single-source models). We use a similar strategy to sample the training dictionaries and
pre-trained word embeddings. Multi-source models (Table 3) consistently outperform single-source
models (Table 1) for both caco and dan.
Learned Word Representation: We evaluate the word representations learned by CACO with a word
translation task between French, Italian, and Spanish. Specifically, we use the src embedder to
generate embeddings for all French, Italian, and Spanish words that appear in both multiCCA and
multiCluster vocabulary.10 Table 4 shows the word translation accuracy (P@1) on the test dictionaries
from the muse library (Conneau et al., 2018). Although the src embedder is not exposed to any
cross-lingual signal, it still rivals clwe on the word translation task by exploiting subword similarities
between languages.
Qualitative Analysis: To understand how cross-lingual subword similarity can help document
classification, we manually compare the output of a dan model and a caco model. Specifically,
we use the mCCA model and the dict model from the Spanish to Italian cldc experiment, and
we inspect their predictions on an Italian dev set. Sometimes caco can avoid the mistakes of
dan by correctly aligning word pairs that are misaligned in the multiCCA embedding space. For
example, in multiCCA, “relevancia” (relevance) is the closest Spanish word for the Italian word
“interesse” (interest), while the caco embedder maps both the Italian word “interesse” (interest) and
the Spanish word “interesse” (interest) to the same point. Consequently, caco correctly classifies
an Italian document about the interest rate with gcat (government), while the dan model predicts
mcat (market). As another example, in multiCCA, the closest Spanish word for the Italian word
“anticipazioni” (advance) is “anuncios” (advertisement), and dan assigns a wrong label to an Italian
document about wage advance. The caco embedder aligns the Italian word “anticipazioni” (advance)
with the Spanish word “aprovisionada” (provision) and therefore correctly labels the document.
10ES: 220,063 words, fr: 219,635 words, it: 234,366 words.
8
Under review as a conference paper at ICLR 2019
5	Conclusion and Future Work
We introduce a cldc framework, caco, that trains a document classifier with labeled training
documents from a related language and optionally a small dictionary, pre-trained embeddings, or
parallel text. Our method exploits subword similarities between related languages to generalize
from source language data. This technique is particularly useful in low-resource settings where
unlabeled monolingual or parallel texts are limited. We provide empirical evaluation of caco on
multiple related language pairs, and caco matches high resource clwe-based methods without
using monolingual or parallel text from the target language. In the future, we plan to extend our
method to related languages with different scripts by combining with a transliteration tool or a
grapheme-to-phoneme transducer (Mortensen et al., 2018). We also want to investigate alternative
architectures for the embedder, including morpheme-based models (Luong et al., 2013).
References
Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A.
Smith. Massively multilingual word embeddings. arXiv preprint arXiv:1602.01925, 2016.
Daniel Andrade, Kunihiko Sadamasa, Akihiro Tamura, and Masaaki Tsuchida. Cross-lingual text
classification using topic-dependent word probabilities. In Conference of the North American
Chapter of the Association for Computational Linguistics, 2015.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. A robust self-learning method for fully unsupervised
cross-lingual mappings of word embeddings. In Proceedings of the Association for Computational
Linguistics, 2018a.
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine
translation. In Proceedings of the International Conference on Learning Representations, 2018b.
Miguel Ballesteros, Chris Dyer, and Noah A. Smith. Improved transition-based parsing by modeling
characters instead of words with LSTMs. In Empirical Methods in Natural Language Processing,
2015.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan. Multilingual subjectivity analysis
using machine translation. In Proceedings of Empirical Methods in Natural Language Processing,
2008.
Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger. Adversarial deep
averaging networks for cross-lingual sentiment classification. arXiv preprint arXiv:1606.01614,
2016.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray KavUkcUoglu, and Pavel P
Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research,
12:2493-2537, 2011.
Alexis Conneau, Guillaume Lample, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou.
Word translation without parallel data. In Proceedings of the International Conference on Learning
Representations, 2018.
Ryan Cotterell and Kevin Duh. Low-resource named entity recognition with cross-lingual, character-
level neural conditional random fields. In International Joint Conference on Natural Language
Processing, 2017.
Ryan Cotterell and Georg Heigold. Cross-lingual character-level neural morphological tagging. In
Empirical Methods in Natural Language Processing, 2017.
Alex Graves and Jurgen Schmidhuber. Framewise phoneme classification with bidirectional LSTM
and other neural network architectures. Neural Networks, 18(5-6):602-610, 2005.
Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal DaUme III. Deep unordered com-
position rivals syntactic methods for text classification. In Proceedings of the Association for
Computational Linguistics, 2015.
9
Under review as a conference paper at ICLR 2019
Katharina Kann, Ryan CotterelL and Hinrich Schutze. One-shot neural cross-lingual transfer for
paradigm completion. In Proceedings of the Association for Computational Linguistics, 2017.
Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and Eric Fosler-Lussier. Cross-lingual transfer
learning for POS tagging without cross-lingual resources. In Proceedings of Empirical Methods in
Natural Language Processing, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. Inducing crosslingual distributed repre-
sentations of words. In Proceedings of International Conference on Computational Linguistics,
December 2012.
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer.
Neural architectures for named entity recognition. In Conference of the North American Chapter
of the Association for Computational Linguistics, 2016.
Guillaume Lample, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised machine translation
using monolingual corpora only. In Proceedings of the International Conference on Learning
Representations, 2018a.
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Phrase-
based & neural unsupervised machine translation, 2018b.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1: A new benchmark collection for
text categorization research. Journal ofMachine Learning Research, 5(APr):361-397, 2004.
Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng Ji. A multi-lingual multi-task architecture for
low-resource sequence labeling. In Proceedings of the Association for Computational Linguistics,
2018.
Wang Ling, Chris Dyer, Alan W. Black, Isabel Trancoso, Ramon Fermandez, Silvio Amir, Luls
Marujo, and Tiago Luis. Finding function in form: Compositional character models for open
vocabulary word rePresentation. In Proceedings of Empirical Methods in Natural Language
Processing, 2015a.
Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W. Black. Character-based neural machine
translation. In Proceedings of the International Conference on Learning Representations, 2015b.
Thang Luong, Richard Socher, and Christopher Manning. Better word representations with recursive
neural networks for morphology. In Conference on Computational Natural Language Learning,
2013.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168, 2013.
David Mimno, Hanna Wallach, Jason Naradowsky, David Smith, and Andrew McCallum. Polylingual
topic models. In Proceedings of Empirical Methods in Natural Language Processing, 2009.
David R. Mortensen, Siddharth Dalmia, and Patrick Littell. Epitran: Precision g2p for many languages.
In Proceedings of the Language Resources and Evaluation Conference, 2018.
Yuval Pinter, Robert Guthrie, and Jacob Eisenstein. Mimicking word embeddings using subword
rnns. In Proceedings of Empirical Methods in Natural Language Processing, 2017.
Lei Shi, Rada Mihalcea, and Mingjun Tian. Cross language text classification by model translation and
semi-supervised learning. In Proceedings of Empirical Methods in Natural Language Processing,
2010.
Anders S0gaard, Sebastian Ruder, and Ivan Vulic. On the limitations of unsupervised bilingual
dictionary induction. In Proceedings of the Association for Computational Linguistics, 2018.
10
Under review as a conference paper at ICLR 2019
Stephanie Strassel and Jennifer Tracey. LORELEI language packs: Data, tools, and resources for
technology development in low resource languages. In Proceedings of the Language Resources
and Evaluation Conference, 2016.
Karl Stratos. Reconstruction of word embeddings from sub-word parameters. In Proceedings of the
First EMNLP Workshop on Subword and Character Level Models in NLP, 2017.
Xiaojun Wan. Co-training for cross-lingual sentiment classification. In Proceedings of the Association
for Computational Linguistics, 2009.
Ke Wu, Xiaolin Wang, and Bao-Liang Lu. Cross language text categorization using a bilingual
lexicon. In International Joint Conference on Natural Language Processing, 2008.
David Yarowsky, Grace Ngai, and Richard Wicentowski. Inducing multilingual text analysis tools
via robust projection across aligned corpora. In Proceedings of the Human Language Technology
Conference, 2001.
Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Adversarial training for unsupervised
bilingual lexicon induction. In Proceedings of the Association for Computational Linguistics,
2017a.
Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Earth mover’s distance minimization
for unsupervised bilingual lexicon induction. In Proceedings of Empirical Methods in Natural
Language Processing, 2017b.
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. Cross-lingual sentiment classification with bilingual
document representation learning. In Proceedings of the Association for Computational Linguistics,
2016.
11
Under review as a conference paper at ICLR 2019
Table 5: cldc experiments between languages from different families on rcv2. When transferring
from a North Germanic language to a Romance language, caco models score much lower than dan
models (top). Surprisingly, caco models are on par with dan when transferring from a Romance
language to a North Germanic language (bottom). We boldface the best result for each row, UnderIine
CACO results that outperform one DAN model, and double UnderIine CACO results that outperform
both DAN models.
source	target	DAN			CACO					
		mCCA		、 mClu	Z SRC	DICT	DICT+	MIM	ALL	{ ALL+
I I DA	M IES	65.7	45.3	32.5	34.8	36.7	30.6	38.2	36.8
DA	FR	45.9	57.2	34.1	41.8	39.9	35.5	43.3	41.3
DA	Oit	47.4	52.1	36.8	43.7	36.3	37.2	41.5	37.8
Dsv	Mes	48.5	44.2	35.2	42.5	35.2	34.6	36.8	35.2
SV	FR	49.0	27.3	27.4	29.9	29.0	29.1	28.3	29.3
□ sv	Oit	40.4	49.3	34.6	36.4	35.1	33.3	35.2	35.0
	average	49.5	45.9	33.4	38.2	35.4	33.4	37.2	35.9
Mes	I IDA	56.7	53.4	47.7	48.3	42.9	46.1	52.0	48.0
Mes	SV	52.4	53.0	50.6	53.7	39.5	48.5	51.4	42.1
FR	DA	45.3	29.4	46.7	442	47.6	44.7	48.6	46.6
FR	SV	57.2	35.4	529	53.2	528	53.6	52.8	496
IT	DA	48.2	45.3	36.6	43.6	40.1	34.8	43.0	43.6
Oit	□ SV	31.1	53.8	37.8	45.3	39.2	30.7	43.9	38.9
	average	48.5	45.1	45.4	48.1	43.7	43.1	48.6	44.8
6	Appendix
Training details: For the CLWE-based models, we use forty dimensional multiCCA and multiCluster
word embeddings. We use three ReLU layers with 100 hidden units and 0.1 dropout for the clwe-
based dan models and the dan classifier of the caco models. The bi-lstm embedder uses ten
dimensional character embeddings and forty hidden states with no dropout. The outputs of the
embedder are forty dimensional word embeddings. We set λd to 1, λe to 0.001, and λp to 1 in the
multi-task objective. The hyperparameters are tuned in a pilot Italian-Spanish cldc experiment using
held-out datasets.
All models are trained with adam (Kingma & Ba, 2015) with default settings. We run the optimizer
for a hundred epochs with mini-batches of sixteen documents. For models that use additional
resources, we also sample sixteen examples from each type of training data (translation pairs, pre-
trained embeddings, or parallel text) to estimate the gradients of the auxiliary task objectives Ld, Le,
and Lp (defined in §2.3) at each iteration.
More rcv2 results: Table 5 shows the CLDC results on RCV2 between languages from different
families, which are discussed in §4.4.
12