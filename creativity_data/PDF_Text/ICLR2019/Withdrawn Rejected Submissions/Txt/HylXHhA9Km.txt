Under review as a conference paper at ICLR 2019
Statistical Characterization of Deep Neural
Networks and their Sensitivity
Anonymous authors
Paper under double-blind review
Ab stract
Despite their ubiquity, it remains an active area of research to fully understand
deep neural networks (DNNs) and the reasons of their empirical success. We con-
tribute to this effort by introducing a principled approach to statistically character-
ize DNNs and their sensitivity. By distinguishing between randomness from input
data and from model parameters, we study how central and non-central moments
of network activation and sensitivity evolve during propagation. Thereby, we pro-
vide novel statistical insights on the hypothesis space of input-output mappings
encoded by different architectures. Our approach applies both to fully-connected
and convolutional networks and incorporates most ingredients of modern DNNs:
rectified linear unit (ReLU) activation, batch normalization, skip connections.1
1	Introduction
While the empirical success of deep neural networks is not disputed anymore, a full understanding
of these models is not yet achieved (Zhang et al., 2016; Dinh et al., 2017; Neyshabur et al., 2017).
As no exception to this rule, advances in the design of neural network architectures have more often
come from the relentless race of practical applications rather than by principled approaches. Con-
sequently many common practices and rules of thumb are still awaiting for theoretical validation.
An important obstacle in the characterization of neural networks is the complex interplay of
different sources of randomness. In that respect, despite winning successes both theoretically
(Poole et al., 2016; Schoenholz et al., 2016; Yang & Schoenholz, 2017; Pennington et al., 2018)
and empirically (Pennington et al., 2017; Xiao et al., 2018), the mean-field theory of neural
networks fails to distinguish between the randomness from input data and model parameters. As a
result, input data is only modeled in the rudimentary case of two correlated signals with Gaussian
pre-activation distribution. In Balduzzi et al. (2017) input data is similarly modeled using two
correlated signals with typical activation patterns. Another path of research considers input data as
a 1-dimensional manifold of evolving length and curvature (Poole et al., 2016; Raghu et al., 2017).
All cases are limited in their scope or simplifying assumptions.
In this paper, we introduce a novel approach to statistically characterize deep neural networks and
their sensitivity. Only mild assumptions are required and the usual simplifications of infinite width,
gaussianity or typical activation patterns are not made. Both fully-connected and convolutional net-
works are encompassed and the commonly used techniques of batch normalization and skip connec-
tions are incorporated. The key of our methodology is to consider statistical moments with respect to
input data as random variables which depend on model parameters. By studying how different archi-
tecture choices influence the evolution with depth of these moments, we provide statistical insights
on the corresponding hypothesis spaces of input-output mappings. Our findings span the topics of
pseudo-linearity, exploding sensitivity, exponential and power-law evolution with depth.
2	Propagation
We start by formulating the propagation for neural networks with neither batch normalization
nor skip connections, that we refer as vanilla networks. The formulation will be slightly adapted
in section 6 with batch-normalized feedforward nets, and in section 7 with batch-normalized resnets.
1Code to reproduce all results will be made available upon publication.
1
Under review as a conference paper at ICLR 2019
Clean propagation. Suppose that We are given a random tensorial input X ∈ Rn× ×n×N0 which
is spatially d-dimensional with spatial extent of n in each direction and N0 channels. We further
suppose that this input is not trivially zero such that Ex,α,c [X2α,c] > 0,2 where α denotes the spatial
position and c the channel. This input is fed into a d-dimensional convolutional neural network
with periodic boundary conditions and constant spatial extent n.3 For each layer, we denote Nl the
number of channels or width, Kl the convolutional spatial extent, ωl ∈ rki× ×ki×ni-i×ni the
weight tensors, bl ∈ RNl the biases, and xl, yl ∈ Rn× ×n×Nl the tensors of post-activations and
pre-activations. We further denote φ the activation function and we adopt the convention X0 ≡ X.
The propagation at each layer l is given by Xl = φ(ωl * XlT + βl), where βl ∈ Rn× ×n×Nl is the
tensor with repeated version ofbl at each spatial position. From now on, we refer to the propagated
tensor Xl as the signal.
Noisy propagation. Next we suppose that the input signal X is corrupted by a small white noise ten-
sor e ∈ Rn× ×n×N0 with independent and identically distributed components such that Ee 怕i67∙]=
σ2δij (σ 1), with δij the Kronecker delta. The noisy signal is propagated in the same neural
network and we keep track of the noise corruption with the tensor l ≡ Φl (X + ) - Φl (X), where
Φl is the input-output mapping Xl = Φl (X). Again with the convention 0 ≡ , the simultaneous
propagation of the clean signal Xl and the noise l is given by
yl = ωl * Xl-1 + βl,	Xl = φ(yl)
l = ωl * l-1	φ0(yl),
(1)
(2)
where denotes the element-wise tensor multiplication and Eq. (2) is obtained by taking the
derivative in Eq. (1). As shown by Eq. (2), for given X the mapping from to l is linear. The noise
l thus stays centered with respect to during propagation with ∀α, c: Ee lα,c = 0.
To get rid of the dependence on σe, we introduce the random sensitivity tensor as the rescaling of
the noise with unit initial variance: s0 ≡ s ≡ / σe and sl ≡ l / σe. Due to the linearity of Eq. (2),
the sensitivity tensor sl is the result of the simultaneous propagation of Xl and sl in Eq. (1) and (2).
We also have Es [sisj] = δij and ∀α, c: Es slα,c = 0. The sensitivity tensor encodes derivative
information while avoiding the burden of increased dimensionality (see computation in Appendix
D.1 for an illustration). This will prove very useful.
Further scope. We restrict our analysis to the ReLU activation function: φ(yl) = max(yl, 0).
This is partly due to lack of space and partly because ReLU networks are the most widely used in
practice. Note however that the formulation with convolutional neural networks does not exclude
fully-connected networks, obtained simply as a subcase with n = 1.
3	Input DATA randomness — moments, normalized sensitivity
3.1	Input data randomness
To understand the importance of the input data distributions Px(Xl) and Px,s(sl), let us adopt a
geometrical perspective on Eq. (1) and Eq. (2) in the context of classification. The neural network
is fed a noisy point cloud with different classes spread throughout space. its goal is layer after layer
to modify this point cloud in Rn× ×n×Nl in order to better group points from the same class and
separate points from different classes. This continues until the point cloud reaches the final linear
separation. The evolution of the point cloud and its derivative across layers, Px(Xl) and Px,s(sl), is
of crucial importance since it characterizes the internal neural network machinery.
Note the distinction between Px(Xl), Px,s(sl) on one side, and Px,ω,β(Xl), Px,s,ω,β (sl) on the
other side. As an illustration, consider a neural network which has shrunk its input X to a point
mass distribution Px(Xl-1) = δpl-1 . For given ωl and βl, the propagation of Eq. (1) maps this
distribution to another point mass Px(Xl) = δpl . on the other side, Px,ω,β(Xl) has density spreading
in the whole ambient space Rn × ∙ × n × Nl and misses the distributional pathology.
2Whenever α and c are considered as random variables they are supposed uniformly sampled among all
spatial positions {1, . . . , n}d and all channels {1, . . . , Nl}.
3The assumptions of periodic boundary conditions and constant spatial extent n are made for simplicity of
the analysis. Possible relaxations are discussed in section C.2.
2
Under review as a conference paper at ICLR 2019
3.2	Moments
In order to keep track of Px(xl), Px,s(sl), our next challenge is the tensorial structure of xl and
sl. In a similar way as batch normalization, we consider feature maps at different spatial positions
as interwoven sub-signals of the tensorial meta-signal. Statistically, we work at the granularity
of sub-signals and we treat equally the randomness of the input tensors x, s and the spatial po-
sition α. This brings the definition of the feature map vector and centered feature map vector of xl as
fm(Xl, α) ≡ xα,:,	fm(xl, α) ≡ xα,: - Ex,α ∣⅛,:],
where α is uniformly sampled in {1, . . . , n}d and the denotation fm(xl, α) reminds the randomness
of both Xl and α. For any order p, the non-central moment and central moment of Xl per-channel
and averaged over channels are defined as
Vp,c(xl) ≡ Eχ,α[fm(xl, α)p],	μp,c(xl) ≡ Eχ,α[fm(xl, α)p],
Vp(Xl) ≡ Eχ,α,c[fm(xl, α)p],	μp(xl) ≡ Eχ,α,c [fm(xl, α)p].
The previous definitions are further extended to any random tensor.
3.3	Normalized Sensitivity
Normalized sensitivity. Using the moments of xl and sl we finally define our key metric for the
statistical characterization of neural networks that we refer as the normalized sensitivity ζl :
Zι ≡ μμ2(S)μ2(x°)! 1/2
ζ ≡	μ2 (xl)
(3)
where μ2(sl) measures sensitivity and μ2(xl), μ2(x0) measure signal informativeness. Again in
the classification task where the goal is to set apart different signals, informativeness is measured
by μ2(xl) since a constant shift applied to all signals is uninformative. This is summarized by the
property of ζl to measure an expected sensitivity when neural network input and output signals are
rescaled to have unit variances (proof and visual illustration in Appendix D.2).
Normalized Sensitivity and Signal-to-Noise. Now let us push further the view of noisy propaga-
tion with the terminology of signal-to-noise ratio SNRl and noise factor Fl :
SNRl ≡ σ 如1!=,
σn oise	〃2(el)
Fl = SNR =时2 (S) 〃2(x0) = (Ul )2
≡ SNR =	μ2(xl)	=(Z ) ,
where We used the definitions of μ2(xl), μ2(e1) and μ2(el) = σ2μz(sl) and μz(w0) = σ2. In
logarithmic decibel scale, SNRldB = SNRd0B - 20 log10 ζl. The normalized sensitivity ζl then
directly measures how the neural network degrades (ζl > 1) or enhances (ζl < 1) the input
signal-to-noise ratio. The factor of noise equivalence means that neural networks with high ζl are
essentially noise amplifiers.
Normalized Sensitivity and Generalization. The relevance of ζl is further supported by several
studies relating sensitivity and generalization for fully connected networks (Sokolic et al., 2017;
Novak et al., 2018; Philipp & Carbonell, 2018). Notably the coefficient defined in Philipp & Car-
bonell (2018) is equivalent to the normalized sensitivity ζl in the fully-connected case (details on
equivalence and reasons for our change of terminology in Appendix D.1).
4	Model parameters randomness
We now introduce model parameters as the second source of randomness. We only consider
untrained networks at initialization. Due to the randomness of the initialization point and its
independence from input data distribution, this can be seen as characterizing the hypothesis space
of input-output mappings encoded by the architecture. We assume that initialization is standard:
(i) weights and biases are initialized following He et al. (2015), (ii) when pre-activations are
3
Under review as a conference paper at ICLR 2019
batch-normalized, scale and shift batch normalization parameters are initialized with ones and zeros
respectively.
Our methodology from now on is to consider all moment-related quantities such as μp(xl),
μp(sl), Vp(Xl), Vp(Sl), Zl as random variables depending on (W 1, b1,..., Wl, bl). We in-
troduce the notation Θl = (ω1, β1, . . . , ωl, βl) for the full set of parameters and the notation
θl = Θl ∣Θl-1 for the conditional set of parameters, when (ωl, βl) are considered as random and
(ω1, β1, ωl-1, βl-1) as given. Furthermore we denote δμ2(x) the geometric increments such that
log δμ2(xl) = log μ2(xl) — log μ2 (xl- 1).
Evolution with Depth. We are now able to write the evolution with depth of μ2 (xl) as
logμ2(xl) - logμ2(x0)= XklOg Eθk [δμ2(xk)] + Xk Eθk [log δμ2(xk)] - logEθk [δμ2(xk)]
+ X，logδμ2(xk) - Eθk [logδμ2(xk)],	⑷
k
where Eq. (4) is obtained using logμ2(xl) — logμ2(x0) = Pklogδμ2(xk) and by de-
composing each term in the sum with telescoping terms. Let us define δrμ2(xk) such that
δμ2(xk) = δrμ2(xk) Eθk [δμ2(xk)], and denote m, m, S the three different terms in Eq. (4):
m[μ2(xk)] ≡ logEθk [δμ2(xk)],	(5)
m[μ2(xk)] ≡ Eθk [logδμ2(xk)] - logEθk [δμ2(xk)] = Eθk [log δrμ2(xk)],	(6)
s[μ2(xk)] ≡ logδμ2(xk) - Eθk [logδμ2(xk)] = logδrμ2(xk) - Eθk [logδrμ2(xk)],	⑺
where Eq. (6) is obtained by entering log E&k [δμ2(xk)] into the conditional expectation and the
logarithm, and Eq. (7) is obtained using E&k [E&k [log δμ2 (xk)]] = E6k [log δμ2(xk)].
Discussion. First we note that m[μ2(xk)] and m[μ2(xk)] are random variables which depend on
Θk-1 while s[μ2(xk)] is a random variable which depends on Θk. We also note that m[μ2(xk)] < 0
by log-concavity and that s[μ2(xk)] is centered: Egk [s[μ2(xk)]] = 0.
Under standard initialization, each channel provides an independent contribution to
〃2 (Xk) = Eχ,α,cfm(xk, ɑ)2]. As a consequence, for large Nk the relative increment δrμ2(xk)
has low expected deviation to 1, meaning with high probability that |logδrμ2(xk)| 《 1,
∣m[μ2(xk)]| 《 1, ∣s[μ2(xk)]∣ 《 1. In addition, s[μ2(xk)] is centered and non-correlated at
different k so its sum scales as √7, whereas the sums of m[μ2(xk)] and m[μ2(xk)] scale as l (see
Lemma 10 in Appendix E.1). The term s[μ2(xk)] is thus doubly negligible. In summary, the
evolution with depth is dominated by m[μ2 (xk)] when this term is non-vanishing and by m[μ2 (xk)]
otherwise. The exact same analysis can be applied to V2 (xl) and to sensitivity moments V2 (sl),
μ2 (Sl). It can also be applied to the ratio μ2 (Sl) / μ2 (xl) and thus to Zl.
Further notation. From now on, the geometric increment of any quantity is denoted with δ. The
definitions of m, m and S in Eq. (5), (6) and (7) are extended to other central and non-central
moments of signal and sensitivity as well as Zl with m[Zl] = 2(m[μ2(sl)] 一 m[μ2(xl)]),
m[ζ l] =1 (m[μ2(sl)] - m[μ2(xl)]), s[ζ l] = 2 (s[μ2(sl)] -s[μ2(xl)]).
The approximation of dominating terms such as m[μ2 (xk)] in Eq. (4) is denoted with `. To make it
precise, we write a ` b when a(1 + 6。)= b(1 + δb) with 瓦|《1, M《1 with high probability.
We write a . b when a(1 + 6。) ≤ b(1 + δb) with |6。|《1, ∣δb∣《1 with high probability. From
now on we further assume that the width is large. We stress that this assumption is milder than
mean-field since dominating terms such as m[μ2 (xk)] in Eq. (4) remain random variables and since
we do not require the signal xl to be Gaussian.
5	Vanilla Networks
We are now fully equipped to statistically characterize neural network architectures. We start by
analyzing vanilla networks corresponding to the equations of propagation introduced in section 2.
Theorem 1.	Moments of vanilla networks. (proof in Appendix E.2) Denote Al the event
V2(xl) > 0 and A0l the complementary event {ν2(x^) = 0} = {Px,α,c [xα,c = 0] = 1}. Then:
4
Under review as a conference paper at ICLR 2019
(i)	Qk=ι 0 - 2-Nk) ≤ P[Aι] ≤ Qk=ι(1 - 2-KdNk-lNk)
(ii)	There exist positive constants mmin, mmax, vmin, vmax > 0 and sequences of random variables
(ml), (m0l), (sl), (s0l) such that under Al, sl, s0l are centered and
log ν2 (X ) = -lml	+	lsl∙+ + log ν2 (X ),	mmin ≤	ml	≤	mmax,	vmin ≤	VarΘl | Ai [sl]	≤	vmax,
log μ2 (S ) = -Iml	+	vɪsi,	mmin ≤	ml	≤	mmax,	vmin ≤	VarΘl | Ai [sl]	≤	vmaχ.
Discussion. (i) is related to the collapse of ReLU networks, which is studied in Lu et al. (2018). Any
vanilla ReLU network almost surely collapses to 0 but the number of required layer is exponential
in the width Nl . In practice, it is not a real problem.
(ii)	implies that moments of xl and sl can be written ν2(xl) = ν2(x0) exp(-lmι + √7sι) and
μ2(sl) = exp(-lml + √7sJ). This behaviour comes from the particularity of He et al. (2015)
to keep stable E&iEχ,α,c[(xα,c)2] = Eθi [ν2(x1)] and EdiEχ,α,c[(sα,c)2] = Eθi [μ2(s1)] during
propagation, which results in vanishing m[ν2(xl)] and m[μ2 (sl)]. The dominating terms in Eq. (4)
are then m[ν2(xk)] < 0 and m[μ2(sk)] < 0 (see Appendix E.3 for details). The small negative
drift and the increasing variance of log ν2(xl) and log μ2(sl) is clear in Fig. 1d and Fig. 1e. It is
also clear that the distribution of log ν2(xl) and log μ2(sl) is nearly Gaussian and therefore the
distribution of ν2(xl) and μ2(sl) is nearly lognormal. Since a lognormal distribution exp(μ + σX)
with X 〜N(0,1) has expectation equal to exp(μ + σ2∕2), the increasing variance of log ν2(xl)
and log μ2(sl) must be compensated by small negative drift in order for their exponential ν2(xl)
and μ2 (sl) to have stable expectations Egi [ν2 (xl)] and Egi [μ2 (sl)]. Note that the diffusion happens
in log-space since layer composition amounts to multiplicative random effect in real space.
As a consequence of (ii) and Chebyshev’s inequality, conditionally on Al the variables ν2 (Xl ) and
μ2(sl) still converge in probability to 0 (proof in Appendix E.4). He et al. (2015) thus manages
to stabilize expectations with respect to all realizations Θl . However in practice we only see a
single realization Θl and for large l this leads with high probability to vanishing network signal
and sensitivity (i.e. activations and gradients). Note that this is a finite-width effect and the terms
m[ν2 (xl)], m[μ2 (sl)], s[ν2 (xl)], s[μ2 (sl)] also vanish in the limit of infinite width.
Theorem 2.	Normalized Sensitivity increments of vanilla networks. (proof in Appendix F.1)
Under Al-1, the dominating term in the evolution of the normalized sensitivity is:
-1/2
νι,c (yl,+) νι,c (yl,-)
〃2(xlT)
where yl,+ = max(yl, 0) and yl,- = max(-yl, 0).
))
(8)
Discussion. An immediate consequence of Theorem 2 is that δζl & 1, meaning that normalized
sensitivity always increases with depth for ReLU vanilla networks. To further understand the
behaviour of ζl we proceed by contradiction. Suppose that there is an event D with probability
P[D] > 0 under which log Zl has a drift larger than the diffusion. Under D the ratio μ2 (xl) / ν2 (xl)
then converges in probability to 0 and the variance μ2(xl) becomes arbritrary smaller than the
average magnitude ν2(xl) = Eχ,α[∣∣xl∣∣2] / Nl (proof in Appendix F.2). All inputs X are then
mapped to a very localized region and the distribution of xl resembles that of a single point. In
turn, this implies that with high probability a given pre-activation channel will have all its values
concentrated around either a single positive or a single negative value. This means that with
high probability either y:l,,c+ = 0 for nearly all x, α, or y:l,,c- = 0 for nearly all x, α, and thus
ν1,c(yl,+)νι,c(yl,-) 02(xl-1)	1 and δζl ' 1. This contradicts the presence of the drift larger
than the diffusion in log ζl .
The previous argument shows that δζl ' 1 for large l and small diffusion, i.e. large width.
This is further supported by the results of our experiments in Fig. 1a and Fig. 1b. A direct
consequence of δζl ` 1 is that the ratio νι,c(yl,+)νι,c(yl,-) / μ2(xl-1) in Eq. (8) is constrained to
small values, which leaves two possibilities for the signal distribution that we illustrate qualitatively:
(i) If max (νι,c(yl,+), νι,c(yl,-))2 ∕μ2(xl-1) → 0, then νι,° (|y|)2 ∕μ2(xl-1) → 0 and first-
order standardized moments become ill-defined.
5
Under review as a conference paper at ICLR 2019
(ii)	If min (νι,c(yl,+), νι,c(yl,-))2 / μ2(xl-1) → 0, then fm(yl, α) becomes concentrated on
the semi-line generated by its average vector (ν1,c(yl))1≤c≤Nl (proof in Appendix F.3). In
this case, the same pattern of activation is seen with probability one with respect to input
data and the neural network becomes linear.
The situation (ii) is clearly visible in Fig. 1c. Our analysis provides a novel insight in this previously
observed phenomenon of coactivation (Balduzzi et al., 2017). Note that the distributional pathology
is severe since a 1-dimensional distribution at layer l implies a 1-dimensional input when taking the
perspective of layers l0 > l.
Figure 1: Illustration of the distributional pathologies of vanilla networks with Nl = 128 and L =
200 layers. (a) Geometric increments of the normalized sensitivity δζl with rapid evolution towards
δζl ' 1. (b) The normalized sensitivity ζl has sub-exponential evolution since it is limited by neural
network pseudo-linearity. (c) 2-dimensional cuts of preactivation feature maps fm(y200, α) using
the direction of average vector (ν1,c(y200)) and a random orthogonal direction (ν1,c(y200))⊥. (d-e)
Evolution of the distributions of μ2(xl) and μ2(sl) with depth showing clear lognormality.
6 Batch-Normalized Feedforward Nets
Next we incorporate batch normalization (Ioffe & Szegedy, 2015) which has the effect of subtract-
ing νι,c(yl) and normalizing by μ2,c(yl)"2 for each channel C in yl. The equations of propagation
are given by
yl =	ωl	* Xj + βl,	zl	= BN(yl),	xl	= φ(zl),	(9)
tl =	ωl	* sl-1,	ul	= BN0(yl)	Θ tl,	sl	= φ0(zl) Θ ul,	(10)
where BN denotes batch normalization and where we introduced the tensors zl , tl and ul . Note
that Eq. (9) and Eq. (10) explicitly formulate a finer-grained subdivision of three different steps
between layers l - 1 and l in the simultaneous propagation of (Xl , sl ).
Theorem 3.	Normalized Sensitivity increments of batch-normalized feedforward nets. (proof
in Appendix G.1) The dominating term in the evolution of ζl can be decomposed as the sum of a
term mBN [Zl] due to batch normalization and a term mφ[Zl] due to the nonlinearity φ:
exp
(mBN K 1D=(μ2(xi-i))	Ec,θι
〃2,c(t1)
μ2,c (y1)
exp (m(φ[ζ 1]) =(1 - 2Ec,θi hνι,c(z1,+)νι,c(z1,-)i)，,
δζ1 ' exp (mBN/尸尸[ζ']) = exp ^bn [Z 1] + mφ [Z']).
(11)
(12)
(13)
6
Under review as a conference paper at ICLR 2019
Figure 2: Illustration of the distributional pathology of batch-normalized feedforward nets with
Nl = 384 and L = 200 layers. (a) Geometric increments δζl and their decomposition as the
product of two terms: a batch normalization term corresponding to the evolution from (xl-1 , sl-1)
to (zl, ul) and a nonlinearity term corresponding to the evolution from (zl, ul) to (xl, sl). (b)
The normalized sensitivity ζl has exploding behaviour. (c) Effective ranks of signal and sensitivity
confirm that sensitivity is much better conditioned than signal. There is a clear inverse correlation
between reff(xl) and the batch normalization term in δZl. (d) Zl becomes ill-behaved as νι(∣zl∣)
vanishes and μ4(zl) explodes. This explains the decay of the nonlinearity term in δZl.
Effect of batch normalization. The term of Eq. (11) corresponds to the evolution of ζl from
(xl-1, sl-1) at layer l - 1 to (zl, ul) just after BN. To understand this term qualitatively, the
pre-activation tensor yl can be seen as Nl random projections of xl-1, and batch normalization
can be seen as an alteration of the magnitude for each projection. Given that batch normalization
uses μ2,c(yl )1/2 as normalization factor, directions of high signal variance are dampened while
directions of low signal variance are amplified. This preferential exploration of low signal directions
naturally deteriorates the signal-to-noise ratio and amplifies ζl due to the factor of noise equivalence.
Now let us look directly at the quantity inside the expectation in Eq. (11). By spherical symmetry
under standard initialization, geometric increments from xl-1 to yl for the signal and sl-1 to tl for
the sensitivity have the same expectation Ec,θl [μ2,c(tl)] / μ2,c(SlT) = Ec,θi [μ2,c (yl)] / μ2,c(XlT).
On the other hand, the fluctuation of these geometric increments depends on the fluctuation of
the signal and sensitivity in the Nl random projections, i.e. on whether directions of signal and
sensitivity variances are rare in the ambient space. To measure this effect of conditioning, we adopt
the metric of effective rank reff from Vershynin (2012):
xl) ≡ Tr C fm(Xl, α)]	sl) ≡ Tr C fm(Sl, α)]
reff(X ) ≡ l∣C[fm(xl,α)]∣∣,	reff(S ) ≡ ||C[fm(sl,α)]∣∣,
(14)
whith C fm(xl , α) , C (fm(Sl , α) the covariance matrices of signal and sensitivity feature map
vectors and || ∙ || the spectral norm. We further extend this definition to any random tensor. If We
assume that sl has very good conditioning with rɛff(sl-1) ' Nl-ι, then μ2,c(tl) has small relative
deviation to its expectation Ec,θi [μ2,c(tl)] and this term can be treated as a constant. In turn, this
implies by convexity of X → 1/x that exp EBN [Zl]) & 1. The worse the conditioning of xl-1,
i.e. the smaller rɛff(XlT), the larger the variance of μ2,c(yl) / Ec,θi [μ2,c(yl)] at the denominator
and the impact of the convexity. Thus the smaller reff(XlT) and the larger exp EBN [Zl]). This
argument is strictly valid for the first step of the propagation where the sensitivity has perfect
conditioning, which results in exp (沆6N [Z 1]) ≥ 1 (proof in Appendix G.2). In Fig. 2c we confirm
experimentally that reff(Sl) ' Nl reff(Xl). Together with Fig. 2a we also confirm that reff(Xl) is
highly predictive of the batch normalization effect on δZl .
7
Under review as a conference paper at ICLR 2019
Effect of the nonlinearity φ. The term of Eq. (12) corresponds to the evolution of ζl from
(zl , ul) after BN to (xl , sl) after φ. The quantity inside the expectation can also be expressed
as νι,c(zl,+)νι,c(zl,-) /μ2,c(zl) since μ2,c(zl) = 1 after batch normalization. We then find a
very similar expression as Eq. (8) for vanilla networks. The difference is that first-order moments
are now normalized using μ2,c(zl) instead of μ2(xl-1). This implies that each random projec-
tion in yl is given the same importance in the sense of similar contribution to δζl . On the con-
trary, Eq. (8) for vanilla networks gives more importance to random directions with high signal
since νι,c(yl,+)νι,c(yl,-) /μ2(xl-1) is small for low signal directions. Note that νι,c(|zl|) =
ν1,c zl,+ + ν1,c zl,- implies 2 ν1,c zl,+ ν1,c zl,- ≤ ν1,c |zl |)2 when taking the square. The
term mΦ [Z l] is thus limited by Ec [νι c(∣zl∣)2] as shown by the joint examination of Fig. 2a and
Fig. 2d.	,
7 Batch-Normalized Resnets
We finish our exploration of DNN architectures with the incorporation of skip connections. We
now suppose that the width is constant, i.e. Nl = N ,4 and following He et al. (2016) we adopt the
perspective of pre-activation units. The propagation inside residual units is given by
yl,h = ωl,h * φ(BN(yl,hτ)) + βl，h,	(15)
tl,h = ωl,h * 仅,h-1 Θ BN0(yl,hT) Θ φ0(BN(yl，h-1))]	(16)
where Eq. (15) and Eq. (16) hold for 1 ≤ h ≤ H , with H the number of layers in each residual
unit. Denoting (yl,h, tl,h) = Φl,h yl,h-1, tl,h-1 , the propagation between successive residual
units is given by
(yl, tl) = (yl-1,tlτ) + (yl，H, tl，H) = (yl-1, tl-1) + Φl,H ... Φl,1(yl-1, tl-1).	(17)
For consistency reasons, we rename the inputs of the propagation as y0 ≡ y, t0 ≡ t. We further
adopt the convention that y0,H ≡ y0, t0,H ≡ t0 such that Eq. (17) can be rewritten as
l
(yl,tl)=X(yk,H,tk,H).	(18)
Theorem 4.	Normalized Sensitivity increments of batch-normalized resnets. (proof
in Appendix H.3) Suppose that for all depth l we can bound the effective ranks rmin .
reff(yl),reff(yl,H ),reff(tl),reff(tl,H) ,theseCond-Ordercentralmoment μ2,mi∏ . μ2(yl,H) . μ2,max
and the feedforward increments inside residual units δmin . δζ l,h . δmax. Denote
ρmin	=	((δmin)	μ2,min ―	μ2,max) / μ2,max and	ρmax	=	((δmax)	μ2,max ―	μ2,min) / μ2,min,
and further consider τmin, τmax such that τmin < ρmin / 2 and τmax > ρmax / 2.5 Then:
∀l	N rmin :
∀l	1 :
1+户广2. δζl . (1 +方产2
2Pmin log l . log Zl . 2 PmaXlog l,
(19)
(20)
∀l	:	lτmin . ζl	. lτmax.	(21)
Discussion. The evolution in Eq. (19) remains exponential inside residual units since Pmin and Pmax
have an exponential dependence in H. However it is slowed down by the factor 2 / (l + 2) between
successive residual units. This comes from the dilution of the residual path (yl,H, tl,H) in the skip
connection path (yl-1,tl-1) with ratio of signal variances μ2(yl,H) / μ2(yl-1) scaling as 1 /1. If
we remove the dilution effect in Eq. (19) by replacing l + 1 by 1 and if we set μ2,min = μ2,max,
4Again this assumption is only made for simplicity of the analysis. In practice, it holds at least approxi-
mately since Nl is only modified by very few units.
5 Note that Theorem 4 has very mild assumptions. The assumption on effective ranks is not required in
Eq. (20) and Eq. (21). Furthermore the assumption on μ2(yl,H) is very reasonable since batch normalization
controls signal variance at the beginning of layer H .
8
Under review as a conference paper at ICLR 2019
then we recover the feedforward evolution with (δmin)H . ζl . (δmax)H. The dilution is clearly
visible as a side effect of the layer aggregation in Eq. (18): each residual unit l adds a new term of
increased sensitivity, but its relative contribution to the aggregation becomes smaller and smaller
with l, so it gets harder and harder for the model to grow ζl.
Since δζl becomes closer and closer to 1, its fluctuation eventually becomes dominant relatively
to its expected deviation to 1. This explains why Eq. (19) only holds for small l. It continues
however to hold statistically so that the bounds on log ζl = Pk log δζk in Eq. (20) correspond
to the integration of the bounds in Eq. (19). A direct consequence of the dilution is thus the
power-law evolution of ζl in Eq. (21) instead of the exponential evolution for feedforward nets.
Equivalently, when Eq. (21) is written as exp τmin log l . ζl . exp τmax log l , the evolution
of ζl for resnets is the same as the evolution of ζlog l for feedforward nets. In words, the evolution
with depth of resnets is the logarithmic version of the evolution with depth of feedforward nets.
Up to some factor, an evolution from 100, to 1 000, and to 10 000 layers for resnets is equivalent
to an evolution from 20, to 30, and to 40 layers for feedforward nets. Despite differences in the
underlying assumptions, our results are reminiscent of the results in Philipp et al. (2017) on the
role of the dilution to alleviate the exploding gradient problem, as well as the results in Yang &
Schoenholz (2017) on the power-law evolution of mean-field resnet gradients.
A shown in Fig. 3, the slow power-law evolution ensures that all statistical quantities remain well-
behaved. The exponent in the power-law fit in Fig. 3d is set to τ = ρ / 2 = (δa2vH - 1) / 2, with δav
the feedforward increment averaged over the whole evolution. This shows that the bound in Eq. (21)
is tight.
Figure 3: Illustration of the well-behaved evolution of batch-normalized resnets with Nl = 384 and
L = 500 residual units ofH = 2 layers. (a) Decomposing δζl,h as the product of two terms: a batch
normalization term and a nonlinearity term. (b) The normalized sensitivity ζl has power law evolu-
tion. (c) Effective ranks of signal and sensitivity indicate that many directions of signal variance are
preserved. (d) Moments νι(∣zl |) and μ4(zl) indicate that Zl has nearly Gaussian distribution.
8 Conclusion
This paper introduced a novel approach for the statistical characterization of deep neural networks
and their sensitivity. Only very mild assumptions were required and most ingredients of modern
DNNs were incorporated. The main scope restriction comes from our focus on the rectifier activation
function. We expect however qualitatively similar results to hold for other activation functions.
Below is a summary of our key results:
一 For vanilla networks, He et al. (2015) only stabilizes second-order moments of activation and
sensitivity in expectation. Depth propagation still induces an additive random walk with small
negative drift in log-space. This results in slowly vanishing activations and gradients and the
9
Under review as a conference paper at ICLR 2019
inevitable convergence to a distributional pathology where the neural network becomes linear
and its signal shrunk to a single dimension.
-For batch-normalized feedforward nets, the exponential growth of sensitivity has two origins:
on the one hand batch normalization which upweights low-signal pre-activation directions, on
the other hand the nonlinear activation function φ.
- Finally for resnets the sensitivity only grows as a power-law. Equivalently the evolution with
depth of resnets is the logarithmic version of the evolution with depth of feedforward nets.
The underlying phenomenon is the dilution of the residual path in the skip connection path
with ratio of signal variances decaying as 1 / k . This ingenious mechanism is responsible for
breaking the circle of depth multiplicativity which causes distributional pathology for vanilla
networks and batch-normalized feedforward nets.
We hope that our methodology will open new perspectives in the statistical understanding of deep
neural network architectures. We believe that it could also provide novel insights regarding model
trainability and generalization.
References
David Balduzzi, Marcus Frean, Lennox Leary, J. P. Lewis, Kurt Wan-Duo Ma, and Brian
McWilliams. The shattered gradients problem: If resnets are the answer, then what is the ques-
tion? In International Conference on Machine Learning, 2017.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. ArXiv e-prints, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the 2015 IEEE Interna-
tional Conference on Computer Vision (ICCV), pp. 1026-1034. IEEE Computer Society, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In Computer Vision - ECCV 2016, pp. 630-645. Springer International Publishing,
2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Inter-
national Conference on Machine Learning - Volume 37, ICML’15, pp. 448-456, 2015.
L. Lu, Y. Su, and G. E. Karniadakis. Collapse of Deep and Narrow Neural Nets. ArXiv e-prints,
2018.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring general-
ization in deep learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
5947-5956. 2017.
R. Novak, Y. Bahri, D. A. Abolafia, J. Pennington, and J. Sohl-Dickstein. Sensitivity and Gen-
eralization in Neural Networks: an Empirical Study. In International Conference on Learning
Representations, 2018.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. CoRR, abs/1711.04735, 2017.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The emergence of spectral univer-
sality in deep networks. In AISTATS, volume 84 of Proceedings of Machine Learning Research,
pp. 1924-1932. PMLR, 2018.
G. Philipp and J. G. Carbonell. The Nonlinearity Coefficient - Predicting Overfitting in Deep Neural
Networks. ArXiv e-prints, 2018.
George Philipp, Dawn Song, and Jaime G. Carbonell. Gradients explode - deep networks are shallow
- resnet explained. ArXiv e-prints, 2017.
10
Under review as a conference paper at ICLR 2019
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems 29,pp. 3360-3368. Curran Associates, Inc., 2016.
Maithra Raghu, Ben Poole, Jon M. Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
expressive power of deep neural networks. In ICML, volume 70 of Proceedings of Machine
Learning Research, pp. 2847-2854. PMLR, 2017.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. CoRR, abs/1611.01232, 2016.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust large margin
deep neural networks. IEEE Trans. Signal Processing, 65(16):4265-4280, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed
Sensing: Theory and Applications, pp. 210-268. Cambridge University Press, 2012.
L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington. Dynamical Isometry
and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural
Networks. ArXiv e-prints, 2018.
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, pp. 7103-7114. Curran Associates, Inc.,
2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. ArXiv e-prints, 2016.
A Details of the experiments in Fig. 1, Fig. 2 and Fig. 3
All three experiments of Fig. 1, Fig. 2 and Fig. 3 were made on CIFAR10 with a random initial
convolution of stride 2 reducing the spatial dimension to n = 16. In each case we considered the
convolutional extent Kl = 3 and periodic boundary conditions. A few experiments with approxi-
mately statistics-preserving boundary conditions such as symmetric mirroring indicated qualitatively
equivalent behaviour.
Fig. 1 was obtained by considering width Nl = 128 and total depth L = 200. For 10 000 random
initializations we randomly sampled 1 024 images and computed the evolution with depth of ν2 (xl)
and μ2 (sl). The distributions of ν (xl) and μ2(sl) were estimated with the empirical distributions
on the 10 000 computations, which are shown in Fig. 1d and Fig. 1e. As for Fig. 1c, it was ob-
tained with 2-dimensional cuts of preactivation feature maps fm (y200, α) for 1 randomly sampled
point every 5 computations. Each time the cut was performed using the direction of average vector
(ν1,c(y200)) and a random orthogonal direction (ν1,c(y200))⊥.
Fig. 2 was obtained by considering 1 000 batches of 64 randomly sampled images. Due to less
demanding computations, we increased the width to a more realistic value Nl = 384. For simplicity,
we always considered batch normalization in train mode. Solid curves correspond to the expectation
over the 1 000 batches and Fig. 2a, Fig. 2c, Fig. 2d additionally show 1σ intervals.
Finally Fig. 3 was obtained by considering again 1 000 batches of64 randomly sampled images with
Nl = 384 and L = 500 residual units ofH = 2 layers. Geometric increments δζl,h were computed
in the feedforward propagation from (yl,h-1, tl,h-1) to (yl,h, tl,h) at layer h = 2 in residual unit l.
Solid curves correspond again to the expectation over the 1 000 batches and Fig. 3a, Fig. 3c, Fig. 3d
additionally show 1σ intervals.
We plan to release Jupyter notebooks to enable replication of our results upon publication.
11
Under review as a conference paper at ICLR 2019
B Complementary Definitions and Notations
Receptive field mapping. Here we temporarily need to handle the mechanics of convolution.
Let Us consider the convolution at layer l of an input V ∈ Rn× ×n×Nι-1 from layer l _ 1. The
output feature map of the convolution (Wl * v)a,∙, at position α ∈ {1,..., n}d is obtained by the
application of the convolution kernel wl over a local input region of size (KldNl-1), with Kld the
spatial extent and Nl-1 the extent in the channel dimension. The local input region is called the
receptive field of (wl * v) at spatial position α.
The receptive field mapping associates an input v from layer l - 1 to RF (v). RF (v) is the tensor
of Rn×∙∙∙×n×κdNι-1 such that RF(v)α,: is the reshaped vectorial form of the receptive field of
(wl * v) at spatial position α. We denote Rl = KldNl-1 the dimensionality of RF (v)α,: and Icl the
set of indices in RF (v)α,: corresponding to elements in channel c in the input v. Strictly speaking,
RF depend on l, but this is implied by the argument so we write RF for simplicity.
Receptive field vectors. The receptive field vector rf and centered receptive field vector rf
associated with an input v from layer l are random vectors which depend on v, α such that
rf(v, α) ≡ RF(v)α,: and R(v, α) ≡ rf(v, α) — Ev°[rf(v, α)],
where we kept the same denotation for the variable in the expectation, as a slight abuse of notation.
Again rf and rf are strictly speaking dependent on l, but this is implied by the argument.
Statistics-preserving property. RF is statistics-preserving with respect to v if for any channel c
and any index ic ∈ Icl, the random variables RF (v)α,ic and vα,c which depend on v, α have the
same distribution RF(v)α,ic Va Vα,c.
Equation of Propagation. Using the definition of RF, the affine transformation from the receptive
field RF (xl-1)α,: to the feature map in the next layer ylα,: is written as
ylα,: = WlRF(xl-1)α,: +bl,	(22)
where W l ∈ RNl ×Rl is the suitably reshaped matricial form of ωl . To lighten notation, we write
yl = WlRF(xl-1) + bl as a short for the affine transformation of Eq. (22) occuring at all spatial
positions α. We have the following equivalence between the notations with receptive field and with
convolutions:
WlRF(xl-1) +bl =ωl * xl-1 +βl.
For vanilla networks, the simultaneous propagation ofxl and sl is then written as
yl =WlRF(xl-1)+bl, xl = φ(yl),	(23)
tl = WlRF(sl-1),	sl = φ0(yl) tl.	(24)
For batch-normalized feedforward nets, the simultaneous propagation of xl and sl is written as
yl = W lRF (xl-1) + bl,	zl	=BN(yl), xl	=	φ(zl),
tl = W lRF (sl-1),	ul	=BN0(yl)	tl, sl	=	φ0(zl)	ul.
Covariance and Gramian. The Gramian operator G and covariance operator C associated with
a random vector v ∈ RN are defined as
G[v] ≡	Ev	vvT	and	C[v]	≡ Ev	vvT	— Ev	vEv	vT.
Note that the gramian and covariance of feature map and receptive field vectors are related by
ʌ ʌ
G[fm(v, α)] = C[fm(v, α)] = Cfm(v, α)] and G[rf(v, α)] = C[R(v, α)] = C[rf(v, α)].
Symmetric propagation for vanilla networks. We define additional tensors obtained by symmetric
propagation at each layer l. In the case of vanilla networks they are given by:
12
Under review as a conference paper at ICLR 2019
yl = -WIRF(XlT)- bl, Xl = φ(yl),
tl = -WlRF(sl-1),	sl = φ0(yl) Θ tl.
By spherical symmetry, tensor moments have the same distribution with respect to θl for both
propagations. FUrthermOre ∀a, c, xα,c + Xa,c = |丫匕』and (α)2 + (Xa,c)2 = (丫匕,，2 since
Xl Xt l = 0. We deduce
a,c a,c
∀c : ν2,c(Xl) + ν2,c(Xtl) = ν2,c(yl),	(25)
Now consider the second-order moments of the sensitivity tensor and sUppose first that
X, s, α, c, Θl-1 are fixed. We have the following identity:
(sla,c)2 + (tsla,c)2 = (tla,c)2φ0(ya,c)2 + (ttla,c)2φ0(yta,c)2 = (tla,c)2[φ0(ya,c)2 +φ0(yta,c)2].
There are two possible cases depending on yal ,c = Wcl,:RF(Xl-1)a,: + bl:
-If ||RF(XlT)a,：||2 = 0, then under standard initialization, Pgi [y%,c = °] = 1, and thus
Pθl φ0(yl)2 +φ0(ytl)2 = 1 = 1 and Pθl (sla,c)2 + (tsla,c)2 = (tla,c)2 = 1.
- If ||RF (Xl-1)a,: ||22 = 0, then the element-wise tensor multiplication of Eq. (24) at layer
l - 1 implies ||RF (sl-1)a,: ||22 = 0, and thus tla,c = 0, sla,c = 0, tsla,c = 0.
In all cases, Pθl (sla,c)2 + (tsla,c)2 = (tla,c)2 = 1. For given c, we now consider X, s, α as
variables again and we get by Fubini’s theorem:
Eθl Ex,s,α [1{(sα,c)2+(sα,c)2=(tα,c)2}] =1,
Eθl Ex,s,a h1{(sα,c)2+(sα,c)2=(tα,c)2}i = °,	(26)
Pθl hPx,s,a (sla,c)2 + (tsla,c)2 6=(tla,c)2 =0i =1.	(27)
where Eq. (27) is obtained by contradiction, since Pθl Px,s,a (sla,c)2 + (tsla,c)2 6= (tla,c)2 > ° >
° would imply that Eq. (26) does not hold. We can relate the moments ofsl, tsl and tl by
(ν2,c(sl) + ν2,c(sl) - ν2,c(tl)) = Ex,s,a
= Ex,s,a
≤ ν2,c
(sla,c)2 + (tsla,c)2 - (tla,c)2
h ((Sa，c)2 +(Sa，c)2- (Qc)2) 1{(sα,c)2+(sα,c)2=(tα,c)2}i
l)2 + (tsl)2 - (tl)2 Px,s,a (sla,c)2 + (tsla,c)2 6=(tla,c)2,
where we used Cauchy-Schwarz inequality and the implicit assumption that the moment quantity is
well-defined. Combined with Eq. (27), we then get
Pθl ν2,c(sl) + ν2,c(tsl) = ν2,c(tl) = 1.
Since sl, Sl and tl are centered, ν2,c(sl) + ν2,c(Sl) = μ2,c(sl) + μ2,c(Sl) and ν2,c(tl) = μ2,c(tl).
So We finally get for the event intersection {∀c : μ2,c(sl) + μ2,c(Sl) = μ2,c(tl)} =
TN=1 {μ2,c(sl) + μ2,c(sl) = μ2,c(tl)}:
Pθl [{∀c ： μ2,c(sl) + μ2,c(sl) = μ2,c(tl)}] = 1.	(28)
Symmetric propagation for batch-normalized feedforward nets. For batch-normalized feedfor-
Ward nets, the symmetric propagation at each layer l is given by
ytl =	-W lRF (Xl-1) - bl,	tzl	= BN(ytl),	Xtl	=	φ(tzl),	(29)
ttl =	-W lRF (sl-1),	utl	= BN0(ytl) Θttl,	tsl	=	φ0(ztl) Θ	utl.	(30)
13
Under review as a conference paper at ICLR 2019
BN in Eq. (29) and Eq. (30) use the statistics of yl, so that tensor moments have the same distribu-
tion with respect to θl for both propagations. We then simply have
zl = -Z,	Ul = -ul.	(31)
The exact same analysis as before gives:
∀c : V2,c(xl) + V2,c(Xl) = V2,c(zl),	(32)
Pθi [{∀c : μ2,c(sl) + μ2,c(sl) = μ2,c(ul)}] = 1.	(33)
C	Statistics-Preserving Property
C.1 CASE OF PERIODIC B OUNDARY CONDITIONS AND CONSTANT S PATIAL EXTENT n
Lemma 1. If convolutions have periodic boundary conditions and the global spatial extent n is
constant, then RF is statistics-preserving with respect to any input v.
Proof. Fix a channel c in v, an index ic ∈ Icl, and consider the tensors v:,c and RF (v):,ic ∈
Rn× ×n. The index ic corresponds to a given convolution kernel position κ ∈ {1, . . . , Kl }d.
Furthermore under periodic boundary conditions, this fixed kernel position implies that each position
α in RF (v)α,ic originates from a different position α0 in the tensor vα0,c. Therefore the index
mapping f : α → α0 from {1, . . . , n}d to {1, . . . , n}d is bijective. We then have RF (v)α,ic =
Vf (α),c ɑ Vα,c When RF(v)α,ic and Va,c are seen as random variables which depend on α and V is
given. In turn, this implies that RF(V)α,ic Va Vα,c, when they are seen as random variables which
depend on V, α.
□
Proposition 2. If convolutions have periodic boundary conditions and the global spatial extent n
is constant, then RF is statistics-preserving with respect to xl-1 and sl-1.
Proof. This follows immediately from Lemma 1.
□
Corollary 3. For	any C and ic ∈	ZC,	we have	rf(xl-1, a)ic Xa	fm(xl-1, a)c and
rf(sl-1,α)ic X以a	fm(sl-1, α)c. Since	the	cardinality	|£ | = Kd	is the	same for all chan-
nels c, it follows that
ν2(xl-1) = N^Tr G[fm(xl-1, α)] = ITr G[rf(xl-1, α)],
μ2(xl-1) = .Tr C[fm(xl-1, α)] = (Tr C[rf(xl-1, α)].
Nl-1	Rl
C.2 Relaxing the assumptions on b oundary conditions and constant s patial
EXTENT
In this section, we detail possible relaxations of the assumptions on boundary conditions and con-
stant spatial extent n. The global spatial extent is denoted nl when it is not constant.
C.2. 1 Case of stationary inputs
Periodic extension. The periodic extension V of a random tensor V ∈ Rn××n×N is defined as
va1 + k1n,...,ad + kdn,c = Vai,…,ad,c,
with (k1, .	. . , kd)	∈	Zd, and where	α1, . . . , αd	are the d components of the spatial position
α = (α1, .	. . , αd)	∈	{1, . . . ,n}d.
Stationarity. The distribution of a random vector V is defined as stationary if, for any k and any
configuration of spatial positions (α1, . . . , αk) and channels (c1, . . . , ck), the joint distribution of
Vα+a1,c1,..., Va+ak,ck is the Same for all α.
14
Under review as a conference paper at ICLR 2019
Lemma 4. If convolutions have periodic boundary conditions and the inputs x and s have
stationary periodic extensions x and s, then their periodic extension xl and sl remain StatiOnary
during propagation.
Proof. First note that convolutions with periodic boundary conditions followed by periodic
extensions on V are equivalent to convolutions on v. This is also the case for componentwise
operations such as batch normalization, nonlinear activation and their derivatives. So we can restrict
our attention to periodic extensions.
Consider a given spatial shift α and define the translation operator Tα, such that Ta(U)= V
with ∀α0, C : Va0,c = Ua+a0,c. It is easy to see that Ta commutes with convolutions as well as
componentwise operations such as batch normalization, nonlinear activation and their derivatives.
It follows that Ta commutes with the input-output mapping Φl defined as (xl, sl) = Φl(x, s), and
thus we have
Ta (xl, sl ) = Φl(Tα(X, s)),	(34)
where we adopted the notation Ta(U, V) = (Ta(U),Tα(V)). Now consider a given k and
configuration of spatial positions (αι,..., αk) and channels (ci,..., Ck) in xl and sl. Due to
limited convolutional spatial extent and due to Eq. (34), there exist a function Φ and a configuration
of spatial positions (a；,..., α^) and channels (c；,..., ck,) in X and s, such that we can write
Xl	,..., Xɑ, c, = Φ(Xa0 c0 ,..., Xa0 c0 ),	(35)
a1 ,c1	ak,ck	a1 ,c1	ak0 ,ck0
X a+aι,cι ,... , xa+ak ,ck = φ(Xa+a"c1 ,∙∙∙, xa+a、, ,c、，)，	(36)
SL 「，...，SL…=Φ(Xa0 co, sa0 co,..., Xa0 co , sa0 co ),	(37)
a1,c1 ,	, a、,c、	a1,c1 , a1,c1 ,	, a、0,c、0 , a、0,c、0 ,
sa+aι ,cι,..., sa+ak ,c、= φ(Xa+a： ,c： , sa+aJ ,cj ,∙∙∙, Xa+ak，,c、，, sa+a、, ,c、，) .	(38)
By stationarity of X and s, the terms on the right-hand sides of Eq. (35), Eq. (36) have the same
distribution, and the terms on the right-hand sides of Eq. (37), Eq. (38) have the same distribution.
It follows that the terms on the left-hand sides of both pairs of equations have the same distribution,
meaning that Xl and Sl are stationary.
□
Lemma 5. If convolutions have periodic boundary conditions, then RF is statistics-preserving
with respect to any input V which has stationary periodic extension V for any global spatial extent nl.
Proof. If V has stationary distribution, it means in particular that for any channel c, the distribution of
Va,c is the same for all α ∈ {1, . . . , nl-；}d. Fix a channel c and an index ic ∈ Icl corresponding to a
given convolution kernel position κ ∈ {1, . . . , Kl}d. A given position α in the receptive field tensor
RF (V)a,ic then corresponds to a given position α0 in the original tensor, such that RF (V)a,ic =
v a，
Va，,c. Since the distribution of Va，,c does not depend on a0, it follows that RF(V)a,ic 〜 Va，,c
for given a and random α0, and thus that RF(V)a ic Va VaC for random a.
口
Proposition 6. If convolutions have periodic boundary conditions and the input X has stationary
periodic extension X, then RF is statistics-preserving with respect to Xl and Sl, for any global
spatial extent nl.
Proof. This follows from Lemmas 4 and 5, and from the fact that the input sensitivity tensor has
stationary periodic extension S due to its definition as a white noise tensor with independent and
identically distributed components.
□
C.2.2 CASE nl	Kl
Proposition 7. If the convolution stride is one in most layers (i.e. nl-； = nl in most layers) and the
global spatial extent is much larger than the convolutional spatial extent nl	Kl in most layers,
then RF is approximately statistics-preserving with respect to Xl-； and sl-；, for any boundary
conditions.
15
Under review as a conference paper at ICLR 2019
Proof. Fix a layer l- 1 such that nl-1 = nl and nl Kl. Denote RF (p) the receptive field mapping
at layer l associated with periodic boundary conditions. Since nl-1 = nl Kl the receptive fields
RF(xl-1)α,:, RF (sl-1)α,: and RF (p)(xl-1)α,:, RF(p)(sl-1)α,: do not intersect boundary regions
for most α, implying that RF (xl-1)α,: = RF (p) (xl-1)α,:, RF (sl-1)α,: = RF (p) (sl-1)α,:
for most α. If we denote with F the cumulative distribution functions of any random vari-
able, this implies for any index ic that Fx,α RF (xl-1)α,ic ' Fx,α RF(p)(xl-1)α,ic and
Fx,s,αRF(sl-1)α,ic 'Fx,s,αRF(p)(sl-1)α,ic.
Since RF (p) is statistics-preserving with respect to xl-1 and sl-1 by Lemma 1, it follows
that for any channel c and index ic ∈ Icl, we have Fx,α RF (p) (xl-1)α,ic	= Fx,α xlα-,c1
and Fx,s,α RF (p)(sl-1)α,ic	= Fx,s,α slα-,c1. We then deduce that Fx,α RF (xl-1)α,ic '
Fx,α xlα-,c1 and Fx,s,α RF (sl-1)α,ic	' Fx,s,α slα-,c1, meaning that RF is approximately
statistics-preserving for xl-1 and sl-1.
□
D Definition and Properties of the Normalized Sensitivity
D. 1 Equivalence with previous definitions
In the fully-connected case n = 1, Philipp & Carbonell (2018) recently introduced the following
coefficient:
EEX 川JlIIF] Ec[Varχ[x0]]! 1/2	(39)
Let us prove the equivalence between the definitions of Eq. (3) and Eq. (39). In the fully-
connected case, the spatial position α can be ignored and tensors and feature map vectors
coincide as Xl = fm(xl), Sl = fm(sl) = fm(sl). When X is fixed, the input-output Jacobian
Jl = ∂∂χχo ∈ RNl × N0 directly summarizes the propagation of the noise el = JIa and thus the
sensitivity sl =Jls. Due to the white noise property Es [sisj] = δij,
Eχ,s[X(SC)2i = Ex [IIJl IIF ],	Ex,s,c[(sC)2] = N Eχ[∣∣Jl∣∣F ].
Here we clearly see the advantage of the sensitivity tensor to encode information on the Jacobian
while avoiding increased dimensionality. Going back to our calculation, the definitions
μ2(sl) = Eχ,s,cfm(sl)2] = Eχ,s,c[fm(sl)2] = Eχ,s,c [晨)2],
μ2(xl) = Eχ,c[fm(xl)2] = Ec [Varχ[xc ]],
finally give the equivalence between the two definitions:
Z l =	( μ2(sl) μ2(x0) Y/2	= E Eχ	[IIJl iiF ]	EC [Varχ [x0]]
∖ μ2(xl)	) y Nl	Ec[Varχ[xc]]
Philipp & Carbonell (2018) chose the terminology of nonlinearity coefficient for this metric. While
our analysis unveils a strong relationship between ζl and the nonlinearity φ, it also reveals a strong
relationship with batch normalization which is still a linear operation. So we chose instead the
terminology of normalized sensitivity.
D.2 Property of Normalized Sensitivity
Proposition 8. The SenSitivity tensor and the tensor Vχx),C containing for given α, C the deriva-
tives of xlα,c with respect to X are related by Es [(s.,'2] = ∣VχxL,cII2∙
16
Under review as a conference paper at ICLR 2019
Proof of Proposition 8. Due to the definition of l as a small corruption to xl , lα,c can be written
as a function of VχX%,C and the input noise e,
^α,c = hVχxα,c, Ci,
where h . i denotes the standard dot product in input space. It follows from the definition of sl =
el / σe that s%,c = Exxa,c, si∙ DUe to the white noise property Es [sisj = δj, We then get
Es [(Sa,C)2] = lVxxa,c"2∙
□
Proposition 9. Define v the rescaling by a constant factor of x with unit variance
Ex,a,cfm(v, α)2] = 1, and vl the rescaling by a constant factor of xl with unit variance
Ex,a,cfm(vl, α)2] = L When vl is considered as a function of V, Zl measures an expected
sensitivity of fm(vl, α) as Zl = Ev,a,c [∣∣Vvfm(vl, α)c|[2]1/2.
Proof of Proposition 9. First we work with the non-normalized x and xl . We can express the
second-order central moment of s as
μ2(sl) = Ex,a,cEs [fm(sl, α)2] = Ex,a,cEs [(sL,c)2]	(40)
= Ex,a,c [||Vxxa,c||2]	(41)
= Ex,a,c [||Vxfm(x , α)c ||2] ,
where Eq. (40) follows from sl being centered and Eq. (41) follows from Proposition 8. Now for
given C and α, the difference fm(xl, α)c 一 加(xl, α)c is constant with respect to xl, and thus with
respect to x. The derivatives of fm(xl, α)c and fm(xl, α) with respect to X are then equal for all x,
α and c. Together with the definition of μ2(xl), we deduce the following:
μ2(S) = Ex,a,c [∣∣Vxfm(X', a)c||2],
μ2(xl) = Eχ,a,c[fm(xl, α)2],
Z l = μ μ2(SI) μ2(x0 ) !	= E Ex,a,c [IIVxfm(Xl, a)c||2] Ex,a,c [fm(x, a)2] ∖	口?)
一1μ2(XI)	)	= [	Ex,a,c[fm(xl, α)2]	).
Defining vl = xl / Ex,a,c fm(xl, α)2]1/2, V = X / Ex,a,cfm(x, α)2]1/2, we get Vxfm(vl, α)=
Vxfm(Xl, α) / Ex,a,c[fm(xl, α)2] 1/2 and Vvfm(Vl, α) = Vxfm(Vl, α) Ex,a,c[fm(x, α)2]1/2. It
follows that
Ev,a,c[fm(v, α)2]
Ev,a,c [IIVvfm(v , α)c II2]
1,
Ex,a,c [∣∣Vxfm(xl, α)c∣∣2] Ex,a,c [fm(x, α)2]
Ex,a,c[fm(xl, α)2
(43)
Finally combining Eq. (42) and Eq. (43), we get Zl = Ev,a,c [∣∣Vv^(vl, α)c∣∣2] 1/2
□
Illustration. Let us consider the case of fully-connected networks with L layers, and with 1-
dimensional input N0 = 1 and 1-dimensional output NL = 1. Denote xL = Φ(x) the original
input-output mapping and VL = Φ(V) the rescaled input-output mapping. Proposition 9 then sim-
ply becomes ZL = Ev [Φ0(v)2] 1/2. In Fig. 4 we show the resut of the propagation vl = Φ(v) of
an input V having a mixture of Gaussians distribution in three different cases:
一 In Fig. 4a the input V is propagated through L = 1 layer with sigmoid activation. Af-
ter propagation, the expected derivative is low since each mode of the Gaussian mixture
appears in a relatively flat region of the sigmoid activation. This is clearly shown by the
input-output data points and by the histogram of the outputs.
17
Under review as a conference paper at ICLR 2019
-In Fig. 4b the input V is propagated through L = 1 layer with linear activation.
-In Fig. 4c the input V is propagated through L = 25 randomly initialized layers, with ReLU
activation in the L - 1 first layers and linear activation in the final layer. After propagation,
the expected derivative is high due to the erratic behavior of the input-output mapping.
2.0
1.5
1.0
0.5
0.0
Figure 4: Illustration of the normalized sensitivity for fully-connected networks of L layers, with
1-dimensional input N0 = 1 and 1-dimensional output NL = 1. The distribution of the input V is a
mixture of two Gaussians. We show the result of the propagation in three different cases: (a) L = 1
layer with sigmoid activation, (b) L = 1 layer with linear activation, (c) L = 25 randomly initialized
layers, with Nl = 100 channels and ReLU activation for 1 ≤ l < L, and linear activation in the final
layer l = L. Top: full input-output mapping (blue curve) and randomly sampled input-output data
points (red circles). Bottom: histograms of inputs and outputs.
E Moments of Vanilla Networks
E.1 Lemma on the sum of increments
Lemma 10. Let Xk be a sequence of random variables which depend on Θk, and denote
Yk = Eθk [Xk] and Zk = Xk - Eθk [Xk]. If there exist constants mmin, mmax, vmin, vmax with
∀k ≤ l: mmin ≤ Yk ≤ mmax and vmin ≤ VarΘk [Zk] ≤ vmax, then:
(i)	The increments Zk are centered and non-correlated:
∀k ≤ l : EΘk[Zk] = 0,	∀k 6= k0 ≤ l : EΘmax(k,k0)[ZkZk0] =0.
(ii)	There exist random variables ml and sl such that sl is centered and
l
EXk = Iml + lSlsι, mmin ≤ mi ≤ mmax,	Vmin ≤ Vagi [si] ≤ Vmax.
k=1
Proof of (i). First we show that Zk is centered:
Eθk [Zk] = Eθk [Xk] - Eθk [Xk] = 0,	(44)
EΘk [Zk] = EΘk-1 [Eθk [Zk]] = 0.
Now consider k < k0, which implies k ≤ k0 - 1 and thus that Zk is a random variable which only
depends on Θk0-1. Then we can write
18
Under review as a conference paper at ICLR 2019
EΘk0 [Zk Zk0] = EΘk0-1Eθk0 [ZkZk0]
= EΘk0-1 Zk Eθk0 [Zk0]
= 0,	(45)
where Eq. (45) follows from Eq. (44).
□
Proof of (ii). Denote Ml = Plk=1 Yk and Sl = Plk=1 Zk . We then have
EΘlSl =X EΘl[Zk] =0,
EΘlSl2 =X 0EΘlZkZk0,
k,k
VarΘlSl = Xk EΘk Zk2 =XkVarΘkZk,	(46)
where Eq. (46) follows from (i). The hypothesis then gives lmmin ≤ Ml ≤ lmmax and
lvmin ≤ Va31 [Sl] ≤ lvmax. If We define ml = Ml /l and Sl = Sl / √7, then Sl is centered and the
telescoping sum Plk=1 Xk = Plk=1 Yk + Plk=1 Zk can be written as required:
l
〉：Xk	= Ml	+	SI	= lml +	lslsl,	mmin ≤ ml ≤ mmax,	vmin ≤ VarΘl	[sl] ≤	vmax∙
k=1
□
E.2 Proof of Theorem 1
Theorem 1. Moments of vanilla networks. Denote Al the event ν2 (xl) > 0 and A0l the
complementary event ν2 (xl) = 0 = Px,α,c xlα,c = 0 = 1 . Then:
(i)	Qk=ι 0 - 2-Nk) ≤ P[Al] ≤ Qk=ι(1 - 2-KdNk-lNk)
(ii)	There exist positive constants mmin, mmax, vmin, vmax > 0 and sequences of random variables
(ml), (m0l), (Sl), (S0l) such that under Al, Sl, S0l are centered and
log ν2 (X ) =	-lml +	ʌ/lsl + log ν2 (X0),	mmin ≤ ml ≤ mmax,	vmin ≤	VarΘl | Ai [sl] ≤	vmax,
log μ2(s') =	-lml +	ʌ/lsl,	mmin ≤ ml ≤ mmax,	vmin ≤	VarΘl | Ai [sl] ≤	vmax∙
Proof of (i). We use the definitions and notations from section B. We further denote (e1, . . ., eRi )
and (λ1, . . ., λRi ) respectively the orthogonal eigenvectors and eigenvalues of G[rf(Xl-1, α)], and
Wl = Wl(eι,..., eRi). We then get
∀c : ν2,c(yl) = Ex,α [(yα,c)2] = Ex,α [(Wcl,Jf(Xl-1, α))[
=Xi(WCl,i)2λi.	(47)
Given the assumption of standard initialization, biases are initialized with zeros and weights are
initialized as Wl 九 Wl 九 N(0, p2 / RlI). Under Al-1, it follows from Corollary 3 that
Tr G[rf(Xl-1, α)] = Rlν2(Xl-1 ) > 0 and thus Tr G[rf(Xl-1, α)] = Pi λi > 0. Combined with
Eq. (47), we get
∀c : Pθi∣Ai-ι [ν2,c(yl)=0] =0.	(48)
Now we introduce the symmetric propagation from section B and we denote Al,c = ν2,c(Xl ) 6= 0 ,
Al,c =	{ν2,c(Xl)	=	0}	and	Al=	{ν2,c(xl)	= 0},	A[c	=	{ν2,c(Xl)	= 0} the complementary
events. It follows from Eq. (48) and Eq. (25) that Pθi |Ai-1 [A0l,c ∩ A0l,c] = 0. Furthermore by spher-
θi
ical symmetry of the propagation, ν2,c(xl)〜 ν2,c(xl) and thus Pθi∖a1-1 [Al J = Pθi∣A1-1 [Al J We
19
Under review as a conference paper at ICLR 2019
then get
∀c : Pθi∣Ai-ι [Al,c ∪ Al+1,c] = Pθi∣Ai-ι [Al,c] + Pθi∣Ai-i[A1+1,c] ≤ 1，
∀c : Pθl∣Ai-ι [Al,c] ≤ 2.
Since A0l = Tc A0l,c and since the events A0l,c are independent conditionally on Θl-1 and Al-1, we
conclude that Pθi∣Aι-ι [Al] = Qc Pθi∣Aι-ι [Al,c] ≤ 2-Nl.
The other side of the	inequality is easier. If ∀c, ic: Wcl,ic ≤ 0, it follows that	∀x, α, c: yαl ,c	< 0,
xlα,c = (yαl ,c)+	=	0, and thus ν2(xl) = 0. Therefore Pθl |Al-1 [A0l]	≥	2-KldNl-1Nl ,	since
KldNl-1Nl = RlNl is the number of elements in Wl. We finally get
1 - 2-Nl ≤ Pθi∣Al-1 [Al] ≤ 1 - 2-KdNlTNl.	(49)
Since P[A0] =	1,	due to the assumption ν2(x) = Ex,α,c[x2α,c]	>	0, it follows	that
1 - 2-N1 ≤ P[A1] ≤ 1 - 2-K1dN0N1. This proves (i) for l = 1.
Now we proceed by induction and suppose that (i) is true for given l - 1. Using Eq. (49), we get
ll
Y (1 -	2-Nk)	≤ Pθl	[Al]	=	Pθl-1 [Aι-ι] Pθl∣Αl-ι	[Al]	≤ Y(1	-	2-KANkTNk),
k=1	k=1
meaning that (i) is true for l. (i) is thus true for all l.
□
Proof of (ii) for ν2(xl). Again we denote (e1, . . . , eRl ) and (λ1, . . . , λRl ) the eigenvectors and
eigenvalues of G[rf(xl-1, α)], and Wl = Wl (eι,..., eRl). We further define
uc =
ν2,c(Xl)
V2,c(xl) + V2,c(Xl)
0
if V2,c(xl) + V2,c(Xl) > 0
otherwise.
Combining the definition of ulc with Eq. (25) and Eq. (47), we get
∀c : ν2,c(xl) = ulc ν2,c(yl),
VC : V2,c(xl) = UC Xi(WCl,i)2λi = Ri ν2(xl-1) UC Xi(WCl,i)2λi,	(50)
where we defined λi = λi / Pj λj and used Pj λj = Tr G[rf(xl-1, α)] = Riν2(xl-1) due to
Corollary 3. The symmetric propagation gives
∀c ： ν2,c(Xl) = Ri ν2(xl-1)(1 - uC) Xi ( - W,i)2λi,
Vc : V2,c(xi) + V2,c(xi) = Ri ν2(xi-1) Xi(Wl,i)2λi.	(51)
θl
By symmetry of the propagation ν2,c(xi)〜 ν2,c(xi). Combined with Eq. (51) and the assumption
of standard initialization, we deduce
2Eθl∣Αl-ι [ν2,c(xi)] = Eθl∣Al-ι [ν2,c(xi) + 沙2,旧)]
=Eθl∣Al-ι [Ri V2(xi-1) Xi(Wci,户i]
2
=Ri ν2(xi 1)- £ λi = 2ν2(xi 1).
Ri i
We then obtain Vc : Eθl∣Al-ι [ν2,c(xi)] = ν2(xi-1), and thus
Eθl∣Al-ι[ν2(Xi)] = ν2(XiT),	Eθl∣Al-ι[δν2(Xi)] = 1.	(52)
20
Under review as a conference paper at ICLR 2019
Both logx and (log x)2 are integrable at zero since logx dx = x logx - x and
(log x)2dx = x(log x)2 - 2x logx + 2x. Combined with Eq. (50), we deduce that log δν2 (xl)
has well-defined conditional expectation and variance under Al . Furthermore by Eq. (49), Al has
probability exponentially low in Nl, which gives
∣Eθi∣Ai [δν2(xl+1)] — 1| ≪ 1,	Eθi∣Ai [logδν2(xl+1)] < 0,	(53)
where Eq. (53) is obtained by log-concavity. We now write the evolution of ν2 (xl ) as
l
logν2(xl) - ν2(x0) = X log δν2(xk).
k=1
Let Us define Xk = logδν2(xk), Yk = Ejk.[logδν2(xk)] and Zk = logδν2(xk)一
Eθk |A1 [log δν2 (xk)] and apply Lemma 10 for each l conditionally on Ai. Suppose there exist mmin,
mmax, vmin, Vmax, such that for each l We have conditionally on Al that ∀k ≤ l, mmn ≤ -Yk ≤ mmax
and Vmin ≤ Varjk|Ak[Zk] ≤ VmaXWhiChimPIies Vmin ≤ Va3k∣A1[Zk] ≤ Vmax. Then we have
∀k ≤ l, -mmax ≤ Yk ≤ -mmin and by Lemma 10 there exist sequences of random variables (ml)
and (sl) such that ∀l under Al, sl is centered and
log ν2(xl) - log V2(x0) = Iml + √7sl,	-mmax ≤ ml ≤ -mmin,	Vmin ≤ Varθi∣Aι [sl] ≤ Vmax.
By simply changing the variable ml to -ml, We get
log ν2(xl) - log ν2(x0) = -Iml + √7sl,	mmin ≤ ml ≤ mmax,	Vmin ≤ Var8i|Ai [sl] ≤ VmaX.
To obtain the bounds mmi∩, mmax, Vmin, Vmax, we consider extreme cases for Uc and Pi(Wl,i)2λi
in Eq. (50). Denoting χ2 (N) the chi-square distribution With N degree of freedom, We ob-
tain minimum bounds by considering Uc 九 1/2 and Pi(Wcl,i)2λi 九 χ2 (Rl ). This leads to
jl
log δν2 (xl)〜χ2(NlRl). Denoting Bern(I/2) the Bernouilli distribution with P = 1 /2, we obtain
maximum bounds by considering Uc Z Bern(I/2) and X (Wel,123 Z χ2(1). The conditionality
on Al has highly negligible impact in practice.
Let us give an example in the fully-connected case with constant width Nl = 100. We then find
numerically mmin ' 9.7×10-5 and Vmin ' 2.0×10-4 as minimum bounds and mmax ' 2.5×10-2
and Vmax ' 5.2×10-2 as maximum bounds. The length scale is experimentally close to Lmax =
1/mmax ' 40 for ν2 (xl).
□
Proof of (ii) for μ2(sl). Let us denote Bl = {∀k ≤ l, ∀c : μ2,c(sk) + μ2,c(sk) = μ2,c(tl)} and
Bl = {∃k ≤ l, ∃c : μ2,c(Sk) + μ2,c(Sk) = μ2,c(tl)} = uk=1 {∃c : μ2,c(Sk) + μ2,c(Sk) =
μ2,c(tk)} the complementary event. Eq. (28) gives ∀k: Pjk [{∃c : μ2,c(sk) + μ2,c(Sk) =
μ2,c(tk)}] = 0. Since B0 is the union of probability zero events, it follows that Pjl (Bl0) = 0,
Pjl (Bl ) = 1, and thus PΘl (Bl0) = 0, PΘl (Bl ) = 1.
Since Bl has probability 1, the conditionality on Bl leaves moments of random variables unchanged.
To see this, consider the moment of order p ofa random variable x:
(EΘ11Al [xp] - EΘ1∣Aι,Bι[xp]2 =
12
Eθ1A [x"] - P"(Bl)E”A1[lB1xP])
=EΘ1∣A1 [1B0 xp]2
≤ Eθi∣Ai [x2p] Pθi∣Aι [BO] = 0,
(54)
where Eq. (54) is obtained with Cauchy-Schwarz inequality and the implicit assumption that x
has well-defined moment of order 2p. It follows that all arguments used in the proof of (i) remain
valid, in particular regarding the distribution of Wl. Conditionality on Bl, the proof then proceeds
identically by simply replacing ν2(xl) by μ2(Sl), yl by tl, G by C, and using the identity with μ2
instead of ν2 in Corollary 3. In particular, we have
21
Under review as a conference paper at ICLR 2019
Eθl |Al-1,Bl [〃2(Sl)] = 〃2(SlT),	EΘ1∣A1-1,B1 [S〃2(Sl)] = L	(55)
Furthermore under Al ∩Bl, for the same positive constants mmin, mmax, vmin, vmax > 0 as previously
defined, there exist sequences of random variables (m0l) and (s0l) such that ∀l under Al ∩ Bl, s0l is
centered and
log μ2(s ) = ―lml + ls,sl, mmin ≤ ml ≤ mmax, Vmin ≤ VarΘl | Ai ,Bi [sl] ≤ VmaX,
where We used the fact that μ2 (s0) = 1 by the definition of s. Now We extend ml and Sl to Bl by set-
ting ml to any value between mmin and mmax and sl such that log μ2(sl) = -lml+√7sl. The reason-
ing of Eq. (54) then ensures that Eθi∣Ai,Bi [s,l] = Eθi∣Ai [sl] and Eθi∣Ai,Bi [(sl)2] = E©，|A」(sl)2],
and Var©i|Ai,Bi [sl] = Var©i|Ai [sl]. It means that ∀l under Al, sl is centered and
log μ2(s) = -lml + lsl,sl,	mmin ≤ ml ≤ mmax,	vmin ≤ VarΘl | Ai [sl] ≤ vmax∙
□
E.3 RELATION TO THE TERMS m, m, o DEFINED IN SECTION 4
Here we relate Theorem 1 to the terms m, m, S defined in section 4. By Eq. (52),
∣Eθk∣Ak [δν2(xk)] - 1|《1, andthus:
∣m[ν2(xk)]| = | logEθk|Ak [δν2(xk)]| ` ∣Eθk|Ak [δν2(xk)] - 1| ≪ 1.
As in the proof of Theorem 1, we denote Bl = {∀k ≤ l, ∀c : μ2,c(sk) + μ2,c(Sk) = μ2,c(tl)} and
Bll the complementary event. Then conditionally on Ak and Bk:
| log Eθk∣Ak,Bk [δν2(xk)]| ' lEθk∣Ak,Bk [δν2(xk)] - 1| ≪ L
The reasoning of Eq. (54) can be applied to δν2(xk), which results in Egk∣Ajδν2(xk)] =
Eθk |Ak,Bk [δν2 (xk)]. Therefore we also get
|m[〃2(Sk)]| = | log Eθk∣Ak [δμ2(sk )]| ≪ L
The terms m[ν2 (xk)] and m[μ2 (Sk)] are thus vanishing and the evolution is dominated by the terms
m[ν2(xk)] < 0, m[μ2(Sk)] < 0. These terms correspond to Yk in the proof of Theorem 1 (ii).
E.4 Convergence in probability to zero
Corollary 11. Conditionally on Al the variables ν2(xl) and μ2(Sl) still converge in probability to
zero:
Ve :	Pθl∣Aι[∣ν2(Xl)I >	e]	→	0,	Ve ：	P©1. [3(Sl)|	> e] →	0.
Proof. Consider a given and the evolution of ν2 (xl ). From theorem 1 (ii), we can write
under Al: log ν2(xl) = -lml + √7sl + log ν2(x0), with sl centered and mmi∏ ≤ ml ≤ mmax,
Vmin ≤ Var©i|Ai [sl] ≤ Vmax. Therefore:
PΘ1∣A1 [∣V2(xl)∣ > e] = PΘ1∣A1 [log ν2(xl) > loge] = P©i|Al [√⅛ > lml + log e - log ν2(x0)]
≤ Pθi∣A1 [sl > √ (lml + log e - log ν2(x0))i
≤ PΘ1∣A1 [∣sl∣ > √ (lmmin + log e - log V2(x0))]∙
(56)
Chebyshev’s inequality on the centered random variable sl then gives:
22
Under review as a conference paper at ICLR 2019
l
2 VarΘl∣Aι [sl]
≤ -------------------------
lmmin + log - logν2(x0)
l
1 vmax
'2 Vmax 〜ιm2m-
≤ -------------------------
lmmin + log - logν2(x0)
where 〜denotes the equivalence for large l. It follows that Pθi∣Ai [∣ν2(xl)∣ > e] → 0. The same
analysis applied to μ2(sl) gives Pgi. [∣μ2(s1 )| > e] → 0.
□
F Normalized Sensitivity increments of Vanilla Networks
F.1 Proof of Theorem 2
Theorem 2. Normalized Sensitivity increments of vanilla networks. Under Al-1, the dominat-
ing term in the evolution of the normalized sensitivity is:
νι,c (yl,+) νι,c (yl-
μ2(XlT)
where yl,+ = max(yl, 0) and yl,- = max(-yl , 0).
))
-1/2
(57)
Proof. The dominating term in the evolution of Zl is ɪ (m[μ2(sl)] - m[μ2(xl)). The terms
m[μ2(sl)] and m[μ2(xl) are simply obtained by considering Eji [δμ2(sl)] and Eji [δμ2(xl)].
By Eq. (53) in the proof of Theorem 1: Eji∣Ai-i,Bi [δμ2(sl)] = 1. By replicating the reasoning of
Eq. (54), this further gives Eθi∣a-i [δμ2(sl)] = 1, andthus m[μ2(sl)] = logEθi∣a-i [δμ2(sl)] = 0.
Next We turn to the term m[μ2(xl)]. Again We use the definitions and notations from section B.
We further denote (e1, . . . , eRi ) and (λ1, . . . , λRi ) respectively the orthogonal eigenvectors and
eigenvalues of C[rf(xl-1, α)] and W l = W l(eι,..., eRi). Using these notations, we get
∀c : μ2,c(yl) = Eχ,α[fm(yl, α)2] = Eχ,α [(Wcl,：£(xl-1, α))[
=Xi(WCl,i)2λi.	(58)
Then due to Wl 幺 Wl 幺 N(0, √2∕R∣I);
22
Eθl∣Aι-ι [μ2,c(y )]=瓦 £% =瓦 Tr C[rf(x	, a)]
i-1	Rl i	Rl
= 2μ2(xlT).	(59)
where Eq. (59) follows from Corollary 3. Furthermore the symmetric propagation gives:
μ2,c(xl) + μ2,c(Xl) = Ex,α[(yα+)2] - %氏]2 + %[匕)2] - %°%]2.
= ν2,c(yl,+) - ν1,c(yl,+)2 + ν2,c(yl,-) - ν1,c(yl,-)2
= ν2,c(yl) - ν1,c(yl,+)2 + ν1,c(yl,-)2	(60)
We have ν1,c(yl) = ν1,c(yl,+) - ν1,c(yl,-) and thus ν1,c(yl)2 = ν1,c(yl,+)2 + ν1,c(yl,-)2 -
2ν1,c(yl,+)ν1,c(yl,-). We can then rewrite Eq. (60) as
μ2,c(xl) + μ2,c(xl) = V2,c(yl) - νι,c(yl)2 - 2νι,c(yl,+)νι,c(yl,-)
=μ2,c(yl) - 2νι,c(yl,+ )νι,c(yl,-)	(61)
23
Under review as a conference paper at ICLR 2019
Combining Eq. (59) and Eq. (61):
Eθi∣Ai-i [μ2,c(xl) + μ2,c(X1)] = 2μ2(XlT)- 2Eθi∣Ai-i [νι,c(yl,+)νι,c(yl,-)],
2EΘ1∣A1-1 [〃2,c(Xl)] = 2μ2 (XlT)- 2EΘ1∣A1-1 [ν1,c(yl, + )ν1,c(yl,-)],	(62)
EΘ1∣A1-1 [μ2,c(xl)] = μ2(XlT)
where Eq. (62) is obtained by spherical symmetry of the propagation. We finally get
(yl,+)vi,c(yl,-)])
μ2(xl-1)	.
Eθl | Al-1 [μ2 (X')] = Eθl∣Al-1 [Ec[μ2,c(X')]] = Ec [Eθl ∣Al-1 [μ2,c(X')]]
=μ2(χl-1)(1--1"入W #!,
-1
νι,c(yl,+)νι,c(yl,-)
〃2(XlT)
Combined with Eθι∣Al-1 [δμ2(sl)] = 1,
δZl ' exp (mvanilla [Zl]
1 - Ec,θl |Al-1
Eθi∣ A1-1 M〃2 (Sl)] !1/2
Eei|Ai-iM〃2(Xl)])
ν1,c(yl,+)ν1,c (yl,-) #! 1/2
μ2(Xl-1)
□
F.2 IF Zl HAS DRIFT LARGER THAN DIFFUSION, THEN μ2(Xl) /ν2(Xl) CONVERGES IN
PROBABILITY TO ZERO
We only need to slightly adapt the reasoning of section E.4. From theorem 1, We can write
under Al: log ν2 (x1 ) = -Iml + √7sl + log ν2(X0), with sl centered and mm⅛ ≤ ml ≤ mmax,
Vmin ≤ Var&i|A』sl] ≤ vmax. We Can also write logμ2(sl) = -Iml + √7s∣, with Sl centered and
mmin ≤ m'l ≤ mmax and Vmin ≤ Var8i| a』s。≤ Vmax. We further suppose that there is an event D
with P(D) > 0 under which Zl has drift larger than diffusion. To make it precise, this means that
∃m > 1 (mmax - mmin) SUCh that for l large enough: log δZl ≥ m, and thus ∃c ∈ R such that for
∀l: log Zl ≥ c + lm.
The ratio μ2 (xl) / ν2 (xl) can be expressed as
μ2(xl) _ μ2(xl) μ2(sl)μ2(x0) _ 1 μ2(sl)μ2(x0)
----：———-----：------：--------：	———：—：------： 一
V (xl) μ2 (sl)μ2 (x0) ν2(xl)---------------------(Z l)2 V (xl)
which gives with logarithms,
log μ2(xl) - log ν2(xl) = -2log ζl + log μ2(sl) - log ν2(xl )+log μ2(x0)
≤ -2c - 2lm - lmmin + Visl + lmmax - Vlsi - log ν2(x0) + log μ2(x0)
≤ C - IM + √lsl,
where we denoted C = -2c - logν2(x0) + logμ2(x0), M = 2m + mmin - mmax > 0 and
sl = sll - sl. The variance of sl is bounded as
EΘ1∣A1 [s2] = VarΘl∣Aι [sl] + VarΘl∣Aι [sl] - 2EΘl∣Aι [slsl]
≤ VarΘl |Al [sl] + VarΘl |Al [sll] + 2VarΘl |Al [sl]1/2 VarΘl |Al [sll]1/2 ≤ 4Vmax
VarΘl∣Aι,D [sl] = EΘ1∣A1,D [sl] = P(D) EΘ1∣A1 [1D Si] ≤ P(D) 4vmax∙
24
Under review as a conference paper at ICLR 2019
Now for given :
Pθi∣Aι,D μj(χι) >e = Pθi∣Aι,D [ log μ2(xl) - log μ2(xl) > log e]
≤ PΘl∣Aι,D hsl > √ (lM + log e - C)]
Chebyshev’s inequality on the centered random variable sl further gives
g	M2(Xl)	l	、,—	「】
P©l|Al,D [k >e] ≤ (1M + loge - C)2 Var©l|Al,D[sl]
l	1 4	1 4vmax
≤ (lM + loge - C)2 P(D) Vmax 〜7M2PD),
proving that under D the ratio μ2 (xl) / ν (xl) converges in probability to zero.
F.3 Limits of signal distribution
Proposition 12. Suppose that
Pc,θ({ min (νι,c(yl,+), νι,c(yl,-)) =0} ∩ { max M,c(yl,+ ), νι,c(yl,-)) > 0}] =1.
Then fm(yl, α) concentrates on the semi-line generated by its average vector (ν1,c(yl))1≤c≤Nl.
Proof. Let us consider the feature map vectors fm(xl-1, α) and receptive field vectors rf(xl-1, α).
Due to the statistics-preserving property of Corollary 3, for each channel c and index ic ∈ Icl,
Xo-I xα RF(XlT)α,ic and thus Eχ,α[fm(xl-1, α)c] = Eχ,α[rf(xl-1, a)/
Now let us reason by contradiction and suppose that there exists a direction e which is orthogonal
to Eχ,α [rf(xl-1, α)ic] 1≤ i ≤R and with non-zero variance v > 0. Consider the weight matrix
W l which projects on direction e for given channel c. For this direction, we have ν1,c(yl ) = 0,
ν2,c (yl) > 0. In turn, this implies ν1,c(yl,+) = ν1,c(yl,-) and ν1,c(yl,+) + ν1,c(yl,-) > 0, which
further gives min ν1,c(yl,+), ν1,c(yl,-) > 0. By continuity, min ν1,c(yl,+), ν1,c(yl,-) > 0
in a small neighborhood for the sampling of the weights Wl, which contradicts the hypothesis. It
follows that rf(xl, α) concentrates on the direction generated by Eχ,α [rf(xl-1, α)ic] 1≤ i ≤R .
Now consider the weight matrix W l which projects on the direction generated
by	Eχ,α [rf(xl-1, α)ic] 1≤ i ≤R for given channel c.	A similar argument gives
min ν1,c(yl,+), ν1,c(yl,-)	= 0, and thus that fm(yl, α)c either concentrates in R+ or
concentrates in R-. It follows that rf(xl-1, α) concentrates on the semi-line generated by
Eχ,α [rf(xl-1, α)ic] 1≤ i ≤R . Under standard initialization, the image fm(yl, α) of rf(xl-1, α)
by the affine transform fm(yl, α) = W lrf(xl-1, α) + bl = W lrf(xl-1, α) thus concentrates on
the semi-line generated by its average vector (ν1,c(yl))1≤c≤N1.	□
G	Normalized Sensitivity increments of batch-normalized
FEEDFORWARD NETS
G.1 Proof of Theorem 3
Theorem 3. Normalized Sensitivity increments of batch-normalized feedforward nets. The
dominating term in the evolution of Zl can be decomposed as the sum of a term m^BN [Zl] due to
batch normalization and a term mφ[Zl ] due to the nonlinearity φ:
25
Under review as a conference paper at ICLR 2019
exP 皿N [Zl] ][Z
〃2,c(tl)
μ2,c (yl)
1/2
-1/2
exp (mφ[ζl]) =(1 - 2Ec,θi [νι,c(zl,+)νι,c(zl，-)D
δZl ' exp GBN/ff [Zl]) = exp (mBN [Zl] + mφ [Zl]).
Proof. First let us decompose the dominating term as the product of two terms:
exp mBBN [Zl])=
Eθi [μ2(ul)]! 1/2 EEθi [μ2(zl)]
μ2(Sl-1)	μ2(xl-1)
-1/2
exp (mφ[Zl])=
EΘ1 [〃2 (Sl)] ! 1/2 E Eθi [〃2 (Xl)
Eθi [μ2(ul)]	Eθi [μ2(zl)]
-1/2
exp (mBN/FF [ζl])=
甄[〃2 (Sl)! 1/2 E Eθi [〃2 (Xl)]
μ√s-1J	μ2(χl-1)
-1/2
exp EBN [Zl]) exp (mφ [Zl]).
mBN [Zl] is a dominating term in the evolution of Zl from (Xl-1, SlT) to (zl, ul), while m$ [Zl] is a
dominating term in the evolution ofZl from (zl, ul) to (Xl, Sl). These terms can be seen as the contri-
bution to m，Bn/ff [Zl] of respectively batch normalization and φ. Now let Us explicitate both terms.
Term exp (mBN [Zl]). First We note that batch normalization directly gives μ2(zl) = 1 and thus
Eθi [μ2(zl)] = 1. Now let US explicitate Eji [μ2(ul)]:
∀c : ul
:,c
tl,c
μ2,c(yl)1/2,
∀c: "2,c(Ul) = μ7⅛),
Eθl [〃2(ul)] = Ec,θl [〃2,c(ul)] = Ec,θl
μ2,c(tl)
μ2,c(yl)
All together, we get
exp
(mBN
)=EEθi [〃2(ul)] ! 1/2 EEθi [〃2(zl)]
= =	μ√s-1)	μ2(χl-1)
-1/2
μ2(sl-1) !-1/2E
μ√xl-1)	Ec,θl
M2,c(tl)
μ2,c(Yl)
Term exp (mφ [Zl]). We consider the symmetric propagation for batch-normalized feedforward
nets and again we denote Bl = {∀k ≤ l, ∀c : μ2,c(sk) + μ2,c(Sk) = μ2,c(ul)} = Tk=I {∀c :
μ2,c(sk) + μ2,c(Sk) = μ2,c(uk)} and B0 the complementary event. By Eq. (33): Peι (Bl) = 1,
PΘl (Bl0) = 0, and replicating the reasoning of Eq. (54) we deduce
Eθl [μ2(Sl)] + Eθl — (Sl)] = Eθl∣Bι [μ2(sl)] + Eθl∣Bι 加(Sl)] = Eθl∣Bι [μ2(ul)] = Eθl [μ2(Ul)],
2Eθl [μ2(sl)] = Eθl [μ2(ul)],	(63)
where Eq. (63) follows from spherical symmetry of the propagation. Now we turn to the symmetric
propagation of the signal:
42,c(xl) + M2,c(Xl ) = Ex,α[(ZaA)2] - Ex,α[zα+]2 + Eχ,α[(Zal)2] - Ex,α⅛c]2 ∙	(64)
= ν2,c(zl,+) - ν1,c(zl,+)2 + ν2,c(zl,-) - ν1,c(zl,-)2
= ν2,c(zl) - ν1,c(zl,+)2 + ν1,c(zl,-)2,
26
Under review as a conference paper at ICLR 2019
where Eq. (64) follows from Eq. (31). Due to the constraints imposed by batch normalization,
ν1,c(zl) = 0 and ν2,c(zl) = 1, it follows that
μ2,c(xl) + μ2,c(X1) = 1 - (ν1,c(zl,+)2 + νι,c(zl,-产)	(65)
ν1,c (z ) = ν1,c (z , ) - ν1,c (z , ) = 0,
(νι,c(zl,+ ) - νι,c(zl,-))2 = ν1,c(zl,+ )2 + νι,c(zl,-)2 - 2ν.(zl,+)v/(zl,-) = 0.	(66)
Using Eq. (65), Eq. (66) and the symmetry of the propagation,
μ2,c(xl) + μ2,c(xl) = 1 - 2vι,c(zl,+ )vι,c(zl,-),
2Eθi [μ2(xl)] = 1 - 2Ec,θi hv1,c(zl,+)v1,c(zl,-)i.	(67)
We finally combine Eq. (63) and Eq. (67):
exp(m°F])=E eθ≠⅛ !i Eθl—!-1/2,
Eθi [μ2(ul)]	Eθi [μ2(z1)])
=(1- 2Ec,θi hv1,c(zl,+ )v1,c(zl,-)i)	/ .
□
G.2 exp EBN [ζ 1]) > 1 IN THE FIRST STEP OF THE PROPAGATION
Let us explicitate the second-order moment in channel c of t1:
μ2,c(t1) = Eχ,s,α [fm(t1, α)2i = Eχ,s,α [fm(t1, α)2] = Eχ,s,α [(W]rf(s, α))[	(68)
=Xij WIiWIjEs,α[rf(s, α)irf(s, α)j] = Xi(WcIi)2 = ||哩』|2.	(69)
where Eq. (68) follows from t1 being centered and Eq. (69) follows from the white noise property
Es[sisj] = δij, which implies for any α that Es[rf(s, α)irf(s, α)j] = δij under periodic boundary
conditions.
Now we turn to the second-order moment in channel c of y1. Denoting (e1, . . . , eR1 ) and
(λ1, . . . , λR1 ) respectively the orthogonal eigenvectors and eigenvalues of C[rf(x, α)], and
W1 = W 1(e1,..., eR1),weget
μ2,c(y1) = Ex,α Ifm(y1, α)2i = Ex,α [(W:rf(X, α))[ = Xi(WcIi)2λi
=iiWci：ii2 Xi(WcI/2% = μ2,c(t1) Xi(WcIi)2λi,
where we defined W1 SUCh that ∀c: W1,： = Wc； / || W：』| and we used Eq. (69). Under standard
initialization, the distribution of W1 is spherically symmetric, implying that for all c the distribution
of W1： is uniform on the sphere of RR1. In turn, this implies
∀i : Eθi [(W1i)2i = A,
VC： Eθi [Xi (Wcii)2λii = R1 Xe,	%ι[Xi (W=ii)2λii =:Xiλi.	(70)
μ2,c(t1)
μ2,c(y1)
1	1/2
Pi (WcIi)2% _
≥(: X %)1/2(吼,。[Xi (Wdi)2 λi i-1!1/2=1.
(71)
(72)
27
Under review as a conference paper at ICLR 2019
where Eq. (71) is obtained using μ2(s0) = 1 and μ2(x0) = -R1^ Tr C[rf(x, α)] = R∙ Pi λ% by
Corollary 3 while Eq. (72) is obtained using the convexity of x → 1/x and Eq. (70).
H	Normalized Sensitivity increments of batch-normalized
RESNETS
H. 1 Adaptation of the previous setup to resnets
Before proceeding to the analysis, slight adaptations and forewords are necessary. Let us denote
Θl,h = (ω1,1, β1,1, . . . , ω1,H, β1,H, . . . , ωl,1, βl,1, . . . , ωl,h, βl,h) for the full set of parameters
UP to layer h in residual unit l and θl,h = Θl,h∣Θl,h-1 for the conditional set of parameters of layer
h in residual unit l. We further denote Θl = Θl,H and θl = Θl,H ∣Θl-1,H respectively the full and
conditional sets of parameters at the granularity of the residual unit.
We now clarify to what extent Theorem 3 on batch-normalized feedforward nets still apply. First
let us rewrite the propagation at layer 1 ≤ h ≤ H inside residual unit l with the pre-activation
perspective:
Zlh = BN(yl,h-1),	xl，h	= φ(zl,h),	yl，h	=	ωl,h	* xl，h + βl,h,	(73)
U，h = BN0(yl,hT)	Θ tl,h-1,	sllh	= φ0(zllh)	Θ ullh,	tllh	=	ωllh	* sllh,	(74)
In the pre-activation perspective, each layer starts with (yl,h-1, tl,h-1) after the convolution and
ends at (yl,h, tl,h) again after the convolution. The concrete effect is that in the first layer h = 1 of
each residual unit l, batch normalization and φ are completely deterministic conditionally on Θl-1.
This occurs again for h ≥ 2 since batch normalization and φ are random conditionally on Θl-1 but
completely deterministic conditionally on Θl,h-1. At even larger granularity, due to the aggregation
(yl, tl) = Plk=0(yk,H, tk,H), the input signal (yl-1, tl-1) of each residual unit becomes more
and more correlated between successive l and less and less dependent on the parameters θl-k of
individual previous units.
Since batch normalization and the nonlinearity φ are the drivers of the evolution of ζl, this shift
can be thought as attributing the parameters and thus the stochasticity of layer h to the layer h - 1.
A possible strategy is thus to consider the evolution from (xl,h-1, sl,h-1) to (xl,h, sl,h) for layers
2 ≤ h ≤ H. This strategy however does not work for the first layer h = 1, since the input signal
(yl-1, tl-1) depends on the whole sequence of parameters Θl-1 and not only on θl-1,H . Another
strategy consists in treating all moment-related quantities as deterministic instead of random. Sym-
metric propagation does not occur strictly in this case, but it still occurs in a mean-field sense when
averaging over channel. So we expect Theorem 3 to remain valid.
H.2 Lemma on dot-product
Lemma 13. Forany random tensor U of Rn×…×n×N:
Eθi [Ey,α,c fm(u, α)cfm(yl,H, α)c]] = 0,
Eθi ∣Ey,α,cfm(u, α)cfm(yl,H, α)c]2i ≤ NrIf(U)μ2(u)Eθi [μ2(yl,H)],
Eθi [Ey,t,α,c fm(u, α)cfm(tl,H, α)c]] = 0,
Eθi 回t,α,cfm(u, α)cfm(tl,H, α)c]2] ≤ NrIf(U)μ2(u)Eθi [μ2(tl,H)].
Proof. By spherical symmetry, moments of fm(yl,H, α)c and -fm(yl,H, α)c = fm(-yl,H, α)c
have the same distribution with respect to θl . It follows that
Eθi [Ey,α,c[fm(u, α)cfm(yl,H, α)c]] = Eθi [Ey,α,c[fm(u, α)c( — fm(yl,H, α)J]],
Eθi [Ey,α,c[fm(u, α)cfm(yl,H, α)c]] = 0.
Next we note that
28
Under review as a conference paper at ICLR 2019
Ey,α,cfm(u, α)cfm(yl,H, α)J = N X Ey,α [fm(u, α)cfm(yl,H, α)J ,
=NEy,α h〈fm(u, α)fm(yl,H, a)〉],	(75)
where〈•〉denotes the standard dot product in RN. Let Us denote (eι,..., eN) and (λι,..., λχ)
respectively the orthogonal eigenvectors and eigenvalues of C[fm(u, α)]. We further denote ui
the unit-variance components of fm(u, α) in the basis (e1, . . . , eN), and yi the components of
fm(yl,H, a) in the basis (eι,..., eN). This gives
fm(u, α) = £ λuiei ei, Ey ,α [ui] = 0,	Ey,α [ui ] = 1,
i
fm(yl,H, a) = Eyiei.
i
Now we decompose each component yi of yl,H as:
∀j : αi,j = Ey,α [yiuj],	yi =	αi,juj + zi,
j
From this definition, we get
∀j :	Ey,α	ziuj	= 0,	Ey,α	yiui	=	αi,i ,	Ey,α	yi	=	αi,j	+ Ey,α	zi	,
j
μ2(yl，H) = NEy,α[hyl,H,yl，Hi] = NXEy,α[y2] = XX。窘 + XEy,α[z2]). (76)
i	i,j	i
The dot product can be computed in any orthogonal basis, so we use the basis (e1, . . . , eN):
Ey,α [〈fm(u, a)fm(yl,H, a)“= Xpλ-Ey,α[yiUi] = X pλiαi,i.
ii
Spherical symmetry implies that moments of yιeι + •一+ yiei + •一+ yNeN and yιeι + •••-
yiei +---+ yNeN have the same distribution with respect to θl. It follows that
θl
Vj = i ： Ey,α [yiui] Ey,α [yjuj]〜Ey,α [ - yiui] Ey,α [yjuj],
Vj = i : αi,iaj,j 九 (一αi,i)αj,j,
∀j 6= i : Eθl [αi,i αj,j ] = 0.
We deduce that
Eθi Ey,α[<fm(u, a)fm(yl,H, a)〉]	= X λiEθi [α2,i].
i
Spherical symmetry also implies that the distribution of αi,j with respect to θl is the same for all i.
Denoting (βj) such that Vi,j: βj = Eθl αi2,j , we get combined with Eq. (76):
Eθi [μ2(yl,H)] ≥ N(Xβj)= Xβi,
i,j	i
EΘ1	Ey,α[<fm(u, a)fm(yl,H, a)〉]	= X	λi βi	≤	λmax (X βi)	≤	λmaxEθi	[μ2 (yl,H)].
29
Under review as a conference paper at ICLR 2019
Finally combining with Eq. (75):
Eθi ∣Ey,α,cfm(u, α)cfm(yl,H, α)J 2] = N12Eθi Ey,α [fm(u, αfm(yl,H, a)〉i
≤ NλmaxEθl [μ2(yl,H)]
≤ JrI	μ2(U)Eθi[μ23,H)],
N reff (u)
_ ʌ
where We used λmaχ∙eff(u) = Ei λi = Nμ2(u). The same analysis can be applied to fm(u, α) and
fm(tl,H, α).
□
Corollary 14. Let us denote the dot products for k, l ≥ 0 as
Yk,ι = Ey,α,c fm(yk,H, α)cfm(yl,H, α)c], Tk,ι = Ey,α,c [fm(tk,H, α)cf1m(tl,H, α)c],
which combined with Eq. (18) implies
l-1
X Yk,l = Ey,α,c [fm(yl-1, α)cfm(yl,H, α)c],
k=0
l-1
X Tk,l = Ey,α,c [fm(tl-1, α)cfm(tl,H, α)c].
k=0
Then by spherical symmetry ∀k, l, Yk,l and Tk,l are centered, and ∀{k, l}	6= {k0, l0}:
EΘmax(k,l,k0,l0) [Yk,lYk0,l0] = 0, EΘmax(k,l,k0,l0) [Tk,lTk0,l0] = 0. Furthermore:
∀k<l: Eθl	≤ Eθl-1 NrfyHμ2(yk,H)Eθl[μ2(yl,H)]
∀k < l : EΘl [Tk2,l ] ≤ EΘl-1
l-1	2
∀l : EΘl	X Yk,l
k=0
l-1	2
∀l : EΘl	X Tk,l
k=0
≤ EΘl-1
≤ EΘl-1
1
Nreff(tk,H)
μ2(tk,H)Eθi [μ2(tl,H)]
NreffIyl-1) μ2(yl-1)Eθl [μ2(yl,H)]
1
Nreff(tl-1)
μ2(tl-1)Eθi [μ2(tl,H)]
(77)
(78)
(79)
(80)
H.3 Proof of Theorem 4
Theorem 4. Normalized Sensitivity increments of batch-normalized resnets. Suppose that for
all depth l we can bound the effective ranks rmin . reff(yl), reff (yl,H), reff(tl ), reff (tl,H), the
second-order central moment μ2,min . μ2 (yl,H) . μ2,max and the feedforward increments inside
residuαl Units δmin . δζ'," . δmax∙ Denote Pmin = ((δmin)?”μ2,min ― μ2,max) / μ2,max and
ρmax = ((δmax)2 "μ2,max ― μ2,min) / μ2,min, and further ConSider Tmin, τmax SUch that τmin < ρmin / 2
and τmax > Pmax / 2. Then:
∀l	N rmin :	(1 + 管1 )1/2. δζl	.(1+帘)1/2,	(81)
∀l 1 :	2Pmin log l . log Zl . 2 PmaXlog ',		(82)
∀l 1 :	lτmin . ζl	. lτmax .	(83)
30
Under review as a conference paper at ICLR 2019
Proof. First we introduce the additional constants γmin = (δmin)2H and γmax = (δmax)2H, so that we
can write Pmin = (YminM2,min - M2,max) / μ2,max and P2,max = (Ymax仙2,max - M2,min) / M2,min∙
We also remind that the symbols . in Theorem 4 denote inequalities up to small non-dominating
terms. We write a . b when a(1 + δa) ≤ b(1 + δb) with 瓦|《1, ∣δb∣《1 with high probability.
We write a ` b when a(1 + δa) = b(1 + δb) with 瓦|《1, ∣δb∣《1 with high probability.
Denoting ∧ for the logical and, the following rules are easily verified:
(a . b) ∧ (a & b) Q⇒ (a ' b),
(a . b) =⇒ (-a & -b),
(a . b) =⇒ (1/a & 1/b),
(a . b) ∧ (c . d) =⇒ (ac . bd),
(a . b) ∧ (b . c) =⇒ a . c.
Finally (a . b) ∧ (c . d) =⇒ (a + c . b + d) under the condition that |a + c|	|a| + |c| and
|b + d|	|b| + |d| are very small probability events. We keep these rules in mind in the course
of this proof.
Proof of Eq. (81). Adopting the same notations as Corollary 14 and using yl = Plk=0 yk,H by
Eq. (18), we get
μ2(yl) = Ey,α,c[Xk,k' fm(yk,H，α)cfm(yk ,H，α)ci = Xk,ko Yk,k0.
μ2(tl) = Ey,α,c hXk,Jm(tk,H a)cfm(tk0，H ɑ)c] = Xk,k0 Tk,k0.
Now using the hypotheses μ2,min . μ2(yl,H) . μ2,max and rmin . reff(yl'H), combined with
Eq. (77) from Corollary 14,
(I + 1)μ2,min + Ek=k0 YkM . μ2(Yl) . (I + 1)μ2,max +	k6=k0 Yk,k0 ,
Eθl (Xk=k0Yk，k)	.l(l+1)Nrmn"2,max,
Eθl h∣Xk=ko yk,k°∣i. Q+1) √⅛ μ2,max，	(84)
where Eq. (84) is obtained using Cauchy-Schwarz inequality. It follows that for large width N 1,
with high probability ∣ Pk=k, Yk,k'∣ ≪ (l + 1)μ2,min and ∣ Pk=k> Yk,k'∣ ≪ (l + 1)μ2,max, which
then gives
(I + 1)μ2,min .μ2(yl) . (I + 1)μ2,max .	(85)
We can write (ζl)2 as
2 = 〃2(y0)〃2(tl) =	( 0) μ2(tl-1) + Tl,l + 2 Pk=^0 Tk,l
一	〃2(yl)	μ2(yl-1) + 切 + 2 Pk-O Yk，l,
2 = (ζl-1)2 μ2(yl-1) + μ2(-l)2Tl,l +2μ2-y)2 Pk=0Tk,l
=	μ2(yl-1)+γl,l + 2 Pk=O γk,l	.
(86)
Let us denote Vk ≤ l: Tk，l = μ2(y1)2 Tk,l and Yl = Pk-O Yk,l and Tl = Pk=O Tk，l. Eq. (86) then
gives
δ Cl )2 =	(Z I)2	=	〃2(ylT) + Tl，l	+ 2Tl
(Z) =	(ZW	=	μ2(yl-1) + Yl，l	+ 2匕.
(87)
Furthermore we can bound Tl l as
Tl，l
〃2(y0)
(ζl-1)2
〃2(tl，H)
〃2(y0)
(Z-Ψ
(Z l-1)2Yh(δζ l，h)2 μμ⅛0y
YminM2，min . Tl,l . YmaxM2，max.
(88)
31
Under review as a conference paper at ICLR 2019
Combining Eq. (79) and Eq. (80) from Corollary 14 with Eq. (85), we get for the variance of the
terms Tl and Yl :
Eθi [匕1 . N1-lμ2,max,	(89)
W ∖μμ2(y0) X τ、2] <	1w	]〃2(y0)〃2(tlT)W ]〃2(y0)μ2(tl,H)B
Eθ[(Ly工k<τk,l)..NrmnEθl-1 [—WIP—Eθl[—WlP-]]，
Eθi [Tl2] . N1-Eθi-1 [〃2 (yl-1 )Eθi [Tl,l]] . YmaxN1-lμ2,maχ.	(90)
It follows that ∣Yl∣《1 and 忸∣《1 With high probability when l《Nrmin Combined With
Eq. (87) and Eq. (88),
μ2(yl-1) + Ymin 祖2,min
μ2(yl-1) + μ2,max
. (δζl )2 .
μ2(yl-1) + Ymaxμ2,max
μ2(yl-1) + μ2,min
1 + Yminμ2,min - μ2,max . (`[l)2 . 1 + Ymaxμ2,max - μ2,min
(I + I) μ2,max	(	~	(l + 1)μ2,min
Finally for l	Nrmin, we find that δζl is bounded as
(1 + 用T . δζl . (1 + 吊)1/2
□
Proof of Eq. (82). Using Eq. (89) and Eq. (90), we apply Cauchy-Schwarz inequality on Yl and Tl :
Eθl h∣wi. √Nrmn √lμ2,max,
EΘl h∣T∣ ∣i . √γmax √Nr , √lμ2,max.
Combined with Eq. (85), We deduce that for large width ∣匕 ∣《μ2(yl) and ∣Tl ∣《μ2(yl) always
hold with high probability. Due to μ2,m⅛ . Y1,1 . μ2,max and to Eq. (88), we also have with high
probability that Yll《μ2 (yl) and Tl《μ2(yl) when l》1. Now let Us take the logarithm in
Eq. (87):
2log δζl = log (1 +----/ l 1、TI,l +-----/ l 1、Tl) - log (1 +-----/ l 1、Yl,l +----/ l 1、
μ2(yl-1)	μ2(yl-1)	μ2(yl-1)	μ2(yl-1)
When l 1, all the terms added to 1 in the logarithm are 1 with high probability. Therefore for
l > l0	1, we can write
2log δζl ' —L^^1√Tl,l H--L^^1√Tl------L^^1√γll-----L^^1√γl.
μ2(yl-1) ,	μ2(yl-1)	μ2(yl-1)	,	μ2(yl-1)
ll
2 X logδζk' X
k=l0+1	k=l0+1
1
μ2(yk-1)
(Tk,k
- Yk,k
l
+kX+1 μ^ (Tk- Yk).
Let us bound the first term:
l
X
k=l0 +1
Yminμ2,min - μ2,max
kμ2,max
l
.X
k=l0 +1
μ2(yk-1)
l
(T	_ Y ) (	Ymaxμ2,max - μ2,min
( k,k - k,k .乙	kμ2 min
k=l0 +1	μ2,min
1
PmmZl3 dx . k=X+1	μ√yk-)	(Tk,k	-	Yk，k)	. Pmax /0 1 dX，
Pminlog (ll0) . X	μ2(y1k-1)	(Tk，k-	Yk，k)	. Pmax Iog (ll0).
k=l0 +1
(91)
32
Under review as a conference paper at ICLR 2019
Now We consider the second term. By spherical symmetry, Yk /μ2(yk-1) and Yk，/μ2(yk0-1) are
non-correlated for k 6= k0. So we get combined with Eq. (89) that for l > l0	1,
EΘl	l (X k=l0+1	μ2(yk-1) Yk)	l =	EΘk k=l0+1	'(1	Yk)2	
				)μ2(yk-1)叼	,
			.	μ2,max ~ Nrminμ2,m⅛	k <	μ2,max	]	( l k2 ~ Nrminμ2,m⅛	' 5 k=l0+1	
A similar calculation for Tk gives
(k X 1 μ^yk-1)Tk).YmaX Nrm¾nlog( 10).
k=l0 +1
For large width and l	1, these terms are very small with high probability compared to the term
of Eq. (91). It follows that for l > l0	1,
Pmin log (:) . 2 log . Pmax log (:),
l0	l0
1 Pmin log l . log Zl . 1 Pmax log l.
□
Proof of Eq. (83). As a consequence Eq. (82), for l》1 there exist δ, δ0 with ∣δ∣《1, ∣δ0∣《1
with high probability and
(1 + δ)1 Pmin log l ≤ log Zl ≤ (1 + δ0 )1 PmaX log l
exp (2(1 + δ)Pmin log l) ≤ Zl ≤ exp (2(1 + δ0)Pmax log l).
NoW ConSider τmin, τmax SUCh that τmin < 2 Pmin and τmax > 2 Pmax∙ Then
2(1 + 6)PminIOg l = (2Pmin + 2ρminδ ― Tmin) log l + τmin log l,
2(1 + δOPmaX log l = (2PmaX + ]Pmaxδ' ― τmax) log l + τmax log l∙
The terms 2Pmin + 2Pmi∏δ — Tmin and 2PmaX + 1 Pmaxδ0 — Tmax are respectively positive with high
probability and negative with high probability. Therefore with high probability, exp τmin log l ≤
Zl ≤ exp TmaX log l and thus for l 1:
exp τmin log l . ζl . exp τmax log l ,
lτmin . ζl . lτmax .
□
33