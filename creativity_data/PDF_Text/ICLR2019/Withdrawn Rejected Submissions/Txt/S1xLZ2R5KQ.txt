Under review as a conference paper at ICLR 2019
Maximum a Posteriori on a Submanifold: a
General Image Restoration Method with GAN
Anonymous authors
Paper under double-blind review
Ab stract
We propose a general method for various image restoration problems, such as de-
noising, deblurring, super-resolution and inpainting. The problem is formulated
as a constrained optimization problem. Its objective is to maximize a posteriori
probability of latent variables, and its constraint is that the image generated by
these latent variables must be the same as the degraded image. We use a Gener-
ative Adversarial Network (GAN) as our density estimation model. Convincing
results are obtained on MNIST dataset.
1 Introduction
Image restoration has been researched for many years, but in a case-by-case way (Park et al., 2003;
Mairal et al., 2008; Guillemot & Le Meur, 2014). Almost all image restoration algorithms are
only designed for certain type of images or degradation. This research paradigm has some obvious
disadvantages. It is exhausting to invent new algorithms or train new models for slightly different
situations. Even if we can, those specialized solutions are not so elegant, because they are very
unlike one another even though the problems they focus on are fundamentally so similar.
It is worth noting that any image degradation process can be abstracted as a many-to-one function.
More specifically, for any given degradation process, one degraded image could be degraded from
many possible original images. From that point of view, we propose a general method for various
image restoration problems, such as denoising, deblurring, super-resolution and inpainting. Our al-
gorithm chooses the most probable original image from all those possible original images, and uses
it as the restoration of the given degraded image. To be more precise, the general image restoration
is formulated as a constrained optimization problem. Its objective is to maximize a posteriori proba-
bility of latent variables, and its constraint is that the image generated by these latent variables must
be the same as the degraded image.
Recent progress of density estimation techniques makes our algorithm possible. In the field of image
generation, Generative Adversarial Networks (GANs) make a huge success in recent years (Goodfel-
low et al., 2014; Radford et al., 2015). As research continues, images generated by GANs become
more and more realistic and clear, and training procedure of GANs become more and more sta-
ble (Salimans et al., 2016; Arjovsky et al., 2017). Besides being an image generation technique,
GANs can also be used for density estimation. The generator part of a GAN is an implicit probabil-
ity distribution model, and it will converge to a good estimator of the data distribution after training.
In this work, we solve the inference problem with the probability density estimated by a GAN.
Figure 1 provides an illustration of how our image restoration method works. There are four dashed
boxes from left to right in Figure 1, corresponding to four different phases of image capture and
restoration process. Images in the first dashed box are original images, which are clear and un-
degraded. These images undergo a series of degradation in the second dashed box, and then are
captured by our camera. In the image restoration process, we hope to estimate the original images
with the degraded images we captured. As we pointed out before, every degraded image could be
degraded from many possible original images. To be more precise, there is a particular subset of the
original image manifold for any degraded image, and all image samples on the submanifold could
be degraded to the given degraded image. Images in the third dashed box are those samples on the
submanifold, and they are arranged in ascending order of log-likelihood from left to right. Images
marked by yellow boxes are samples with the highest log-likelihood in their group, and they are
placed in the last dashed box as restoration outputs.
1
Under review as a conference paper at ICLR 2019
original image
degradation
samples on a submanifold
image restoration:
MAP on a submanifold
5□π□π
目口口□D
□软 iε□E≡
回 Sbbqbbbqb q
喇 E3ΞΞΞQQQ□□□□Q
目	O
Ξ E1□□QQEIEIEI ŋ
BQB□S
motion down- add damaged --------1------1-----1------∙	∣------1-----1~~A Iog-
blur sample noise sensor	-30	-20	-10	0	10	20	30 J⅛eJ⅛ood
Figure 1: An illustration of our image restoration method.
Overall, the contributions of this work are mainly in two aspects:
1.	We propose a general method for various image restoration problems. In the method, we explic-
itly use density information estimated by a GAN, an implicit model; and we directly solve the
image restoration problem, an inference problem, with a GAN, a generative model. To the best
of our knowledge, our work is the first to do those two things.
2.	We propose a new algorithm to solve the optimization problem in our method. The new optimiza-
tion algorithm is a first-order iterative algorithm for constrained problems, and it works well even
for problems with highly nonlinear objective and constraints. These features make it especially
suited to neural network related constrained optimization problems.
2 Related Work
The most similar work to ours is proposed in Yeh et al. (2017). They propose a semantic image
inpainting method, which can generate missing content with a trained GAN. They search in the
latent space of the trained GAN for the image which is closest to the corrupted image, and use the
discriminator loss of the trained GAN as an indicator of how realistic their restoration is. Their
motivation is similar to our work, but unfortunately, there is a major theoretical flaw in their method.
Goodfellow et al. (2014) already prove that the discriminator is unable to identify how realistic
an input is after several steps of training, if the GAN has enough capacity. During the training,
the information of the data distribution gradually transfer from the discriminator to the generator.
Ideally, the generator will have all the information of the data distribution while the discriminator
will have none. That is why we use the generator of a trained GAN as an implicit probability density
model in our method. Another difference between their work and ours is that they only focus on
image inpainting problem, while our method applies to various image restoration problems.
The maximum a posteriori (MAP) has existed for a long time as a classic estimation method (Camp-
isi & Egiazarian, 2016). But before GANs, people do not have a probability density model which
is good enough to describe the distribution of images. After GANs make a huge success in image
generation, researchers start to use them in image restoration tasks to get more realistic results (Isola
et al., 2017; Bousmalis et al., 2017). Ledig et al. (2017) and S0nderby et al. (2016) try to use the
MAP estimation on GANs to solve image super-resolution problem. However, they only use the
MAP estimation implicitly and indirectly, while our method use it explicitly and directly. We sus-
pect that all methods do implicit MAP estimation on GANs would require redesigning or retraining
when the image restoration task changes, and this makes implicit methods not as general as our
explicit method.
Ulyanov et al. (2017) is another work which is seemingly similar to ours, but they are actually quite
different. Their work uses a randomly-initialized neural network as an image prior to solve various
image restoration problems. The prior in their method is elaborate, neural network related but still
handcrafted, while in our method the prior is learned from data. So our data-driven prior has better
adaptability to specific image distribution.
2
Under review as a conference paper at ICLR 2019
3 Maximum a Posteriori on a Submanifold
3.1 Formulation
Consider a general image degradation model as follows,
X = F(x, Ω)
(1)
where x, X, and Ω represent the original image, the degraded image, and the parameters of the
degradation model, respectively. The image degradation function F is a deterministic function.
That means, given an original image X and a particular set of parameters Ω, the image degradation
model will always produce the same degraded image X.
Our goal is to get a reasonable estimate of X with given X and F. In this paper, We use the maxi-
mum a posteriori probability (MAP) estimate of X as the restoration of X. Compared to MSE-based
method, MAP estimate of X is perceptually more convincing. We can perform inference by maxi-
mizing the posterior p(X, Ω∣X):
{X, Ω} = argmaxp(X, Ω∣X)
x,Ω
argmax
x,Ω
p(X|x, Ω)p(x∣Ω)p(Ω)
p(x)
(2)
where X and Ω represent MAP estimate of X and Ω. Note that P(X) is always positive and does not
depend on X and Ω, and typically we assume that X and Ω are independent. Therefore,
{X, Ω} = argmaxp(X|x, Ω)p(x)p(Ω)
x,Ω
(3)
Note that X = F(x, Ω) is a deterministic function, i.e., p(X|x, Ω) = δ(X - F(x, Ω)). Therefore,
the estimation is equivalent to
{X, Ω} =argmax p(x)p(Ω)
x,ω	(4)
s.t. IIX - F (x, Ω)k =0
Here we write p(X) more specifically as pr(X), which stand for the probability density of real
data distribution. We can estimate pr (X) with the generator part of a trained GAN, which is an
implicit probability distribution model with distribution pG(X). The trained generator G represents
a mapping from latent space of z to data distribution of original image X, i.e., pr(X) = pG(X),
and pG(X) is a probability density function implicitly defined by X = G(z), where z is typically
sampled from some simple distribution, such as the uniform distribution or the normal distribution.
Assuming G : Rn → Rm is an injective function, the estimation is equivalent to
{z, Ω} = argmax PG(G(Z))p(Ω)
z,Ω
s.t. IIX - F(G(z), Ω)k =0
and X = G(Z)
(5)
(6)
Generally the dimension of vector space of Z is far lower than the dimension of vector space of
X. Note that PG (X) is nonnegative if and only if X is on the low dimensional manifold M defined
by X = G(Z), we can replace the probability density on the original space PG(G(Z)) in Eq. (5)
by the probability density on the manifold pm(z), and end up with the same estimation result ^.
According to Pennec (2004), the probability density on the manifold can be calculated by
PM(Z)
P(z)
qdet GTam( ∂G,…,∂Gn
(7)
where Gram represents the Gram matrix, and
Jdet Grαm( ∂∂G,..., ∂∂G-) is the volume of the
parallelotope spanned by the vectors (∂G,..., ∂G), so the square root of the Gram determinant
can serve as a local scale factor. It has an effect similar to the Jacobian determinant, but we can only
3
Under review as a conference paper at ICLR 2019
use the Gram determinant here because G is a function from Rn to Rm , and generally n is much
less than m.
The Gram matrix can be simply calculated by Gram( ∂∂G,..., ∂∂ZGG) = VT V, where V is an m X n
matrix, whose entries are given by Vij∙ = ∂χi. Therefore, Eq. (5) is equivalent to
L Γ	p(z)p(Ω)
{z, Ω} = argmax -,	二
Z,ω	√det VT V	(8)
s.t. ∣∣x — F (G(Z), Ω )k =0
To solve the estimation problem efficiently, we represent probabilities in Eq. (8) in logarithmic
space, i.e.,
{Z, Ω} = argmax — ɪ log det VT V + logp(z) + logp(Ω)
z,Ω	2	(9)
s.t. Ilx — F (G(Z), Ω )k =0
Matrix VT V is a positive-definite matrix, so we can use Cholesky decomposition to calculate
log det VTV efficiently, i.e.,
logdetVTV =2tr(log(chol(VTV)))	(10)
Finally We deduce a set of expressions which can be calculated directly, and their final outcome x is
the restored image we want, i.e.,
{z, Ω} =argmax — tr(log(chol(V T V))) + log p(z) + log p(Ω)
z,ω	(11)
s.t. kx — F (G(Z), Ω )∣ =0
and x = G(^)	(12)
Note that (G(Z), Ω) form a low dimensional manifold which is embedded in the space of (x, Ω),
and the feasible solutions of Eq. (11) is on a subset of the manifold, which is defined by ∣∣x —
F(G(Z), Ω)k = 0. So our method basically makes a MAP estimate on a submanifold.
Figure 2: A toy example to show how our image restoration method works.
Figure 2 is a toy example to show how our method works in a very visible way. Suppose there is a
grayscale original image x, which has only three pixels. Then it is downsampled to only one pixel
during the image capture process, and our task is to estimate x with the one pixel image we captured.
Suppose we have trained a GAN as an implicit model of data distribution of x. More specifically,
the generator of the trained GAN represents a mapping from its input noise Z to data distribution of
x. The left part of Figure 2 describes the two dimensional latent space of Z . We use the saturation
of orange color to represent probability density level, i.e., a thicker orange color means a higher
4
Under review as a conference paper at ICLR 2019
probability density. So the uniform orange color in the latent space means that the input noise z is
sampled from a uniform distribution.
Then the two dimensional vector z is mapped to three dimensional space of image x by the generator
of the trained GAN, and the big orange square in the latent space of z is transformed into a twisted
torus in the three dimensional data space of x, which is described in the right part of Figure 2.
Some areas in space of z expand during the transformation, while other areas shrink. We can find
this out by comparing the red and blue quadrilateral between the latent and data space. Therefore,
the probability density on the torus is no longer uniform. The orange colors of the expanded areas
become lighter, and the colors of the shrunken areas become thicker. Quantitatively speaking, the
square root of the Gram determinant in Eq. (7) is the local area scale factor of the mapping, and its
inverse, of course, is the local density scale factor.
The pale yellow plane in the data space represents the constraint in the toy example. All points on
the plane would exactly be downsampled to the one pixel image we captured. So the intersection
curve of the plane and the torus is the submanifold we are looking for, and that white curve is the
feasible set of the toy problem. In this problem, p(z) is a constant in the domain, and degradation
parameters Ω does not exist at all. According to Eq. (8), What We need to do is to maximize the
inverse of the square root of the Gram determinant on the submanifold. In other words, the point
with the thickest orange color on the intersection curve is the restored image X, the MAP estimate on
the submanifold. We can find out that the method is both intuitive and rational for this toy example.
3.2 Optimization Algorithm
We propose a neW optimization algorithm to solve Eq. (11). Note that the objective function and the
equality constraint in Eq. (11) are both highly nonlinear, so gradient-based method seems a natural
choice for the problem. Our algorithm is inspired by Projected Gradient Descent Method.
To solve a unconstrained problem With ordinary Gradient Descent Method, We take small steps in
the direction of the negative gradient. To solve an constrained problem, We can try to use Projected
Gradient Descent Method, take small step as usual and then project variables back onto the feasible
set. But unfortunately, Projected Gradient Descent Method is only valid for problems With very
simple feasible set, such as solution set of linear equations, some simple polyhedra and simple cone,
etc. If constraints of a problem is too complex, like the constraint in Eq. (11), it is very hard to
project variables back onto the feasible set.
To overcome this shortage, We propose a neW optimization algorithm called Quasi Projected Gra-
dient Descent Method. In our algorithm, the gradient information is not only used to improve the
objective function, but helps to satisfy the constraints as Well. Consider the standard form of contin-
uous optimization problem,
minimize f(u)
u
s.t. hi(u) = 0, i = 1, . . . , m	(13)
hj (u) ≤ 0, j = m + 1, . . . , m + p
Where f, hi , hj : Rn → R, and they are all highly nonlinear. Algorithm 1 is the proposed algorithm
for the problem.
To solve Eq. (11) using the proposed algorithm, we only need to set U = {z, Ω}, objective function
f (U) = -(-tr(log(Chol(VTV)))+logp(z)+logp(Ω)), and the only equality constraint function
hι(u) = IIx - F(G(Z) Ω)k.
In the proposed algorithm, we first define an overall constraint function h(U) : Rn → R≥0, and
the feasible set of the optimization problem is the region where h(U) = 0. In each iteration of
the algorithm, we calculate the gradients of f(U) and h(U) at Ui-1. If we take a small step in
the direction of the negative gf, the value of f(U) will decrease a little bit, but it may have a
unwanted impact on the value of h(U). In order to avoid this problem, we calculate g k, the tangential
component of gf on the isocontour of h(Ui-1), which can be calculated by vector rejection of gf
on gh. In each iteration, we actually take a small step in the direction of the negative gk, the value
of f(U) will still decrease, while it has almost no impact on the value of h(U). We also take a small
step in the direction of the negative g⊥, i.e., gh itself, which is perpendicular to the isocontour of
5
Under review as a conference paper at ICLR 2019
Algorithm 1 Quasi Projected Gradient Descent Method
Require: step size ηk and η⊥, positive factors ci and cj , number of iterations n, small positive
constant for numerical stability, initial guess u0
m	m+p
Define h(u)	= Eci	∙	∣∣hi(u)k2 + E	cj	∙	H(hj(u))	∙	∣∣hj(u)k2, where H represents the
i=1	j=m+1
Heaviside step function
for i = 1 to n do
g f = Vf(Ui-1)
g h = Vh(Ui-1)
gk = gf - gh ∙gh+e ∙ gh
g⊥ = gh
Ui = Ui-I — η∣∣ ∙ gk (or do with an advanced gradient descent optimizer)
Ui = Ui — η⊥ ∙ g⊥ (or do with another advanced gradient descent optimizer)
end for
return Un and f(Un)
h(Ui-1). Repeat these steps, and the sequence U will hopefully converge to the desired optimal
solution.
Behaviors of our Quasi Projected Gradient Descent Method is similar to behaviors of the original
Projected Gradient Descent Method. Consider a point U which is very close to the feasible re-
gion. The summation of two moves against g k and g⊥ is actually an inaccurate Projected Gradient
Descent. That is why we name our method as “Quasi Projected Gradient Descent Method”.
isocontours of constraint
gradient step of constraint
projected step of objective
gradient step of objective
combined step in an iteration
isocontours of objective
Figure 3: A toy example to show how our Quasi Projected Gradient Descent Method works.
Here we use the same toy example we used in Section 3.1, to show how our Quasi Projected Gradient
Descent Method works. In Figure 3, solid curves in black and white are isocontour of constraint
function h. The whiter the curve, the lower value of h it corresponds; Dashed lines in color are
isocontour of objective function f. The redder the line, the lower value of f it corresponds. Note
that the white solid curve is the feasible set of the toy problem, so intersection points of the white
solid curve and the red dashed line in the latent space is Z in Eq. (11), while the intersection points
in the data space is X in Eq. (12).
Our iterative optimization algorithm starts from the bottom left corner of the latent space. The
red vector is a gradient step of h. It is pointing towards the direction of the negative gh , and is
perpendicular to the black solid curve, an isocontour of h. The green vector is a gradient step of
f. It is pointing towards the direction of the negative gf , and is perpendicular to the yellow dashed
6
Under review as a conference paper at ICLR 2019
line, an isocontour of f. The blue vector is a projected gradient step. It is pointing towards the
direction of the negative gk , and is the tangential component of the green vector on the black solid
curve, which can be calculated by vector rejection of the green vector on the red vector. We only plot
green, red and blue vector for the first iteration to keep Figure 3 clean and easy to understand. Black
vectors are combined gradient steps, which are vector sums of red and blue vectors. We move along
these black vectors and we can find out that our optimization algorithm reaches a desired solution
quickly.
4	Experiments
In this Section, we use MNIST dataset (LeCun et al., 1998) to test our image restoration method.
The dataset is divided in 50k for the training set, 10k for each of the validation and test set. We use a
WGAN-GP (Gulrajani et al., 2017) trained on the training set as the density estimation model. The
architecture of the WGAN-GP we used is shown in Table 1 and Table 2, and we add a L2 weight
decay term with decay parameter of 0.001 on the generator loss to prevent over-fitting. The network
we used is very simple, but it is enough to prove the effectiveness of our method.
Table 1: Architecture of the generator
	Kernel size	Output shape
z	-	16
Linear, tanh	-	64 × 4 × 4
Deconv, tanh	5×5	32 × 7 × 7
Deconv, tanh	5×5	16×14×14
Deconv, sigmoid	5×5	1 ×28× 28
Table 2: ArchitectUre of the discriminator
	Kernel size	Output shape
G(z)	-	1 × 28 × 28
Conv, LeakyReLU	5×5	16× 14× 14
Conv, LeakyReLU	5×5	32 × 7 × 7
Conv, LeakyReLU	5×5	64 × 4 × 4
Linear	-	1
We use four different kinds of degradation to test the generality of our method. The first three kinds
of degradation are relatively simple. They are 7× downsampling, making a 14×14 square hole in the
center of the image, and adding Gaussian white noise with a standard deviation of 1.0, respectively.
The last kind of degradation is a composition ofa series of degradation in order, which are (a) adding
linear motion blur by at most 14 pixels in any direction, (b) 4× downsampling, (c) adding uniform
noise between -0.05 and 0.05, (d) randomly removing 10% of the pixels.
We use two independent ADAM optimizer (Kingma & Ba, 2014) with g k and g⊥ respectively in the
Quasi Projected Gradient Descent Method. For all four kinds of degradation, we run the algorithm
with the same settings. Settings for both ADAM optimizer are learning rate α = 0.01 (decayed
linearly to 0), β1 = 0.9, β2 = 0.99, and number of iterations n = 500.
To the best of our knowledge, there is only one other general image restoration algorithm which can
cope with various kinds of degradation like ours does. And that is the nearest neighbor algorithm.
It searches in the training set for an image, whose degradation is the nearest to the given degraded
image X. More specifically, if there are m points x(1),..., x(m) in the training set, then
{XNN, ΩNN} = argmin ∣∣X - F(x(i), Ω)k	(14)
χ(i),Ω
where XNN represents the restored image by the nearest neighbor algorithm. In case of multiple
occurrences of the minimum objective values, We choose the one with the largest p(ΩNN). Note
that the empirical distribution of the training set is
m
Pe(X) = - X δ(x - X(i) )	(15)
m
i=1
If we replace p(X) with pe(X) rather than pG(X) in Eq. (4), or in other words, if the generative
model in our method is extremely over-fitting, our method will degenerate to the nearest neighbor
algorithm. So our method can be treated as a generalized method of the nearest neighbor algorithm.
In the experiments, we use the nearest neighbor algorithm as a baseline, and compare its results with
the proposed method.
7
Under review as a conference paper at ICLR 2019
Table 3: Visual results
Downsample
Hole
Original image
Degraded image
Nearest neighbor
Our restoration
Noise
Composition
Original image
Degraded image
Nearest neighbor
Our restoration
Table 4: QuantitatiVe results
		Downsample	Hole	Noise	Composition
， ， ^ 、、 MSE(X,F(X, Ω))	NN	1.0e-3	0.0061	0	0.0025
	Ours	7.2e-5	0.0014	0	6.4e-5
MSE (x, X)	NN	0.043	0.037	0.039	0.056
	Ours	0.026	0.034	0.034	0.043
					
MSSSIM (x, X)	NN	0.73	0.76	0.77	0.69
	Ours	0.84	0.77	0.80	0.76
L	∙	/F	F/	1	∙ mil C	1 mil Λ	∙Λ∕ΓC7~1∕~T~J / 人 x∖∖ ∖ ∙
Experimental results are shown in Table 3 and Table 4. MSE(X, F(X, Ω)) is a measure
of how accurately a restored image can be degraded back to its input, and MSE(x, X) and
MSSSIM(x, X) (Wang et al., 2003) are measures of the difference between the restoration and
the ground truth. We can find out that our general image restoration method is better than the base-
line method. The nearest neighbor algorithm cannot use the information of the probability density
of images well, and that is the major disadVantage compared to our method.
5 Conclusions and future work
We propose a general image restoration method in this work. Compared with traditional image
restoration algorithms, our method is much more powerful. Image restoration is an inherently ill-
posed problem, so additional prior knowledge is needed. In our method, we use all prior knowledge
of original images, i.e., the probability distribution of original images; and we use all prior knowl-
edge of degradation, i.e., the degradation model itself. Traditional image restoration, by contrast,
just uses a small part of the prior, typically some statistical properties. Besides, unlike our method,
there is usually no guarantee that an output restoration from a traditional method can be degraded
back accurately to its input. This makes restorations from a traditional method less plausible than
restorations from our method.
For future work, We think our method can be straightforwardly extended to other domains which
GANs are gifted in, such as Video, audio and language. We will try to solVe restoration problems and
other inference problems in these domains with our paradigm. The conVergence and other properties
of the Quasi Projected Gradient Descent Method would be interesting as well.
8
Under review as a conference paper at ICLR 2019
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint arX-
iv:1701.07875, 2017.
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 7, 2017.
Patrizio Campisi and Karen Egiazarian. Blind image deconvolution: theory and applications. CRC
press, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems,pp. 2672-2680, 2014.
Christine Guillemot and Olivier Le Meur. Image inpainting: Overview and recent advances. IEEE
signal processing magazine, 31(1):127-144, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. arXiv preprint, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro A-
costa, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single
image super-resolution using a generative adversarial network. In CVPR, volume 2, pp. 4, 2017.
Julien Mairal, Michael Elad, and Guillermo Sapiro. Sparse representation for color image restora-
tion. IEEE Transactions on image processing, 17(1):53-69, 2008.
Sung Cheol Park, Min Kyu Park, and Moon Gi Kang. Super-resolution image reconstruction: a
technical overview. IEEE signal processing magazine, 20(3):21-36, 2003.
Xavier Pennec. Probabilities and statistics on riemannian manifolds: A geometric approach. PhD
thesis, INRIA, 2004.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Casper Kaae S0nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszar. Amortised
map inference for image super-resolution. arXiv preprint arXiv:1610.04490, 2016.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. arXiv preprint arX-
iv:1711.10925, 2017.
Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003,
volume 2, pp. 1398-1402. Ieee, 2003.
Raymond A Yeh, Chen Chen, Teck-Yian Lim, Alexander G Schwing, Mark Hasegawa-Johnson, and
Minh N Do. Semantic image inpainting with deep generative models. In CVPR, volume 2, pp. 4,
2017.
9