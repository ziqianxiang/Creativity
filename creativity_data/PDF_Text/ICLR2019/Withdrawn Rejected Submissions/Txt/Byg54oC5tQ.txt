Under review as a conference paper at ICLR 2019
GENERATIVE MODEL FOR MATERIAL IRRADIATION
EXPERIMENTS BASED ON PRIOR KNOWLEDGE AND
ATTENTION MECHANISM
Anonymous authors
Paper under double-blind review
ABSTRACT
Material irradiation experiment is dangerous and complex, which requires large
number of high-level expertise in the manual processing of experimental images
and data. In this paper, we propose a generative adversarial model based on prior
knowledge and attention mechanism to achieve the generation of irradiated ma￾terial images (data-to-image model), and a prediction model for corresponding
industrial performance (image-to-data model). With the proposed models, re￾searchers can skip the dangerous and complex irradiation experiments and obtain
the irradiation images and industrial performance parameters directly by input￾ing some experimental parameters only. We also introduce a new dataset ISMD
which contains 22000 irradiated images with 22,143 sets of corresponding param￾eters. Our model achieved high quality results by compared with several baseline
models. The evaluation and detailed analysis are also performed.
1 INTRODUCTION
In recent years, significant progress has been made in the development of deep learning and genera￾tion models Kingma & Welling (2013) Goodfellow et al. (2014) Mirza & Osindero (2014). However,
they are far less used in natural sciences than in services or the arts, experimental images from natu￾ral sciences have rich scientific connotations that can be analyzed using deep learning and generating
models (e.g. medical images, fluid experiments, materials experiments etc. Kisilev et al. (2015) Jing
et al. (2017) Alom et al. (2018) Li et al. (2018)).
Most of the researchs on analyzing experimental images of natural sciences are based on feature ex￾traction and end-to-end mapping to obtain valuable experimental target parameters Li et al. (2018)
Cirean et al. (2013) Jing et al. (2017), which can be called as image-to-data task. And most of
these researchs are based on convolutional neural networks(CNN) and semantic segmentation Ron￾neberger et al. (2015) Menze et al. (2010). Although impressive results have been achieved, none of
these researchs have built a data-to-image model.
We propose a generation model based on prior knowledge and attention mechanism to generate im￾ages of the experimental results. The overall architecture of the model is shown in Fig.3 .First, we
propose an embedding model f c(m) = P hm for the molecular composition of materials, so that we
can get the feature vector Cim for each material composition i, which is called molecule2vec.Then
we can embed the feature vector Cim of the material composition as a priori knowledge into the
latent variable sampling of the generation model(see Fig.3 for details).For the irradiated material
images, the swelling cavities distribution Hv is the most important feature Ehrlich (1981), so an
attention mechanism is introduced to make the model focus on the section with swelling cavities in
the material images(see Fig.1).The image generated by the feedforward propagation of the model
X0img = G(Dd, Dc, z) can be encoded by the encoder network to be transformed into the corre￾sponding swelling cavities distribution H0v
, we construct the loss term LHv = ||H0v − Hv||2
to
measure the distance between the real distribution of the swelling cavities and the feature of the
generated image , and then the attention mechanism is optimized by gradient descent.In the end, we
can obtain high-quality images of irradiated materials. In addition, we pre-trained the image-to-data
network P red(Ximg) = Dr to predict the performance parameters Dr of the material through the
1
Under review as a conference paper at ICLR 2019
(a) Overview (b) Representation Vector
Figure 1: (a) :data-to-image model and image-to-data model. In the data-to-image task, model
generate images based on prior knowledge and attention mechanism. In the image-to-data task, im￾ages generated are used to predict the performance parameters. (b): For different alloy materials, the
proportion of the elements in their molecular compositions is calculated, which is used by element
representation vector m.
corresponding experimental image Ximg, which makes it possible to predict the material’s perfor￾mance parameters D0r by generating image X0img by P red(X0img) = D0r.
our contributes:
• data-to-image material images generation model: a generation model
P(Ximg|Dd, Dc, z) is built based on prior knowledge and attention mechanism.
• image-to-data material performance prediction model: using the images Ximg and the
context association method of CNN+BiLSTM, the network P(Dr|Ximg, Cm) is estab￾lished to predict the performance parameters Dr by the image Ximg. • The experimental dataset ISMD for irradiated materials: the images with correspond￾ing data from large number of irradiation experiments and manual annotations.
2 RELATED WORK
Our research is related to deep generation models for image generation, image information mining
in the natural sciences, and irradiated material science.
Deep generation model for image generation. The caption-to-image generation model Gregor
et al. (2015) Zhang et al. (2016) Mansimov et al. (2015) is the majority, most of these models
will first introduce a semantic capture module to extract semantic features, then generate images
through RNN Mansimov et al. (2015), stackGAN Zhang et al. (2016), etc. In addition, enhancing
the connection between semantics and images by attention mechanism is also popular Zhang et al.
(2017).
Image information mining in the natural sciences. Medicine, biology, materials and other fields
have recently introduced deep learning models to mine the connotation information of experimental
images Jung et al. (2017) Ronneberger et al. (2015) Li et al. (2018). Some researchs are inspired by
natural language processing(NLP) using the context information association trick Ssm et al. (2017),
which can be seen as an image-to-data process.
Irradiated material science. In recent years, deep learning methods have been introduced into
material images processing Li et al. (2018) Rovinelli (2018), which has replaced the traditional
artificial technology analysis and achieved good results.
3 MODEL
Recall that our aim is to generate the corresponding experimental image X0img based on the me￾chanics and thermodynamic parameters Dd of the material under the irradiation condition Dc by
the prior knowledge, the molecular composition feature vector Cm, and the attention mechanism. In
2
Under review as a conference paper at ICLR 2019
addition, after generating experimental image X0img, CNN+BiLSTM network P(D0r|X0img) is used
to predict the performance parameters D0r
for image X0img.
3.1 EMBEDDING MODEL FOR THE MOLECULAR COMPOSITION OF MATERIALS
Our dataset contains 14 kinds of alloy materials:
Cm ∈ {Inconel718, InconeX750, Zr1, Zr2, Zr4, Zr1Nb, Zr2.5Nb, 1Cr13, 2Cr13, 00Cr13Ni5Mo4, Au304, Au317, Cr17T i, Cr25}
The molecular composition of these alloy materials is an important prior knowledge for images
generation task, so embedding molecular composition features into the generation model is needed.
Let E = {Al, Mg, Si, Cu, F e, O...} be the set of all the elements appeared. Let mi ∈ R|E| be
the element percentage representation vector of the alloy material i. Inspired by the word2vec
method Mikolov et al. (2013) for word embedding, a method for molecular composition of alloy
materials feature extracting called moleculars2vec is proposed. P hm = f c(m) is used to learn the
thermodynamic properties P hm from the alloy material by the element representation vector m :
P hm = f c(m) = Relu(Wmf c0 (m))
where f c(·) represents a fully connected network, f c0 (·) represents the network part of f c(·) without
the last layer, Wm ∈ R|E|×dP h is the weight matrix of the last layer, dP h is the dimension of the
thermodynamic property P hm. Since the weight matrix Wm has the ability to represent both the
molecular composition features and the properties of the alloy material, we take Cim = Wi,: m as the
feature vector of the material i.
3.2 GENERATIVE MODEL BASED ON PRIOR KNOWLEDGE AND ATTENTION MECHANISM
The prior distribution. In the previous section we extracted the prior knowledge as vectors, that is,
the features of the molecular composition of the alloy materials. To introduce it into the model, we
define the distribution of the latent variable P(z) as follow(prior distribution):
z ∼ N(µ(Cm), δ(Cm))
µ(Cm) = tanh(WµCm) δ(Cm) = exp(tanh(WδCm))
where Wµ ∈ RD×dim(Cm),Wδ ∈ RD×dim(Cm)
are the learnable parameters. Other similar methods
introduce the distribution as latent variable dependencies are Mansimov et al. (2015) Bachman &
Precup (2015), and our model is optimized in performance after introducing a prior distribution.
The attention module. The swelling cavities distribution Hv of the irradiated material image is a
statistics count vector for the distribution of different sizes, which is an important feature for material
performance prediction Porollo et al. (2000). Therefore the model needs to allocate more attention
to the section contains cavities, which requires attention mechanism to achieve. With this design
goal in mind, we can set the attention module as an estimate of the corresponding degree of the
cavities distribution Hv and the image Ximg, which is the soft attention mechanism Attn(·): ˆX = Att(Xconv, ˆHv) = Relu(CNN(Ximg), WvHv)
where Xconv ∈ RL×L×C is a feature calculated by the forward propagation of convolutional layers,
which with C feature maps and with dimension of L × L. Wv ∈ R|Hv|×dim(Xconv)
is a learnable
parameter matrix that maps the swelling cavities distribution Hv to the visual space. Finally, ˆX =
Att(Xconv, ˆHv) is inputed into the discriminator D(·) to complete feedforward propagation.
3
Under review as a conference paper at ICLR 2019
Figure 2: The data-to-image model based on prior knowledge and attention mechanism achieves the
generation of irradiated material images. Prior knowledge distribution is employed to embed the
molecular composition of materials into the generative model. The attention mechanism is utilized
to generate images with physical connotations(swelling cavities distribution). In the image-to-data
task, CNN+BiLSTM is used to extract the features and learn the dependency of the performance
variables.
3.3 PREDICTION MODEL FOR THE PERFORMANCE OF THE MATERIALS
After generating the image of the irradiated material, it is necessary to evaluate the various properties
of the material under the condition Dc. A network is constructed to achieve the image-to-data task:
Fp : V → R
dim(Dr)
, where V represents the visual space.
Before we design the structure of this performance evaluate network, let’s revisit the two challenges
in implementing this map: (1) Irradiated material image Ximg and performance parameter Dr are
both highly abstract data forms. It is very difficult to establish mapping between two highly ab￾stract data forms Jung et al. (2017). (2) The variables in the performance parameters Dr are not
independent to each other, which have mutual influence and physical connection.
To address the challenge 1, we introduce CNN to extract the visual features for the image Ximg,
and the molecular composition feature vector Cm is introduced to change the model P(Dr|Ximg)
to P(Dr|Ximg, Cm); For the challenge 2, BiLSTM Schuster (1996) is used to learn the dependency
of the variables in the performance parameters Dr. The prediction model is defined as:
Dir = Relu(Wh[
−→hi ,
←−
hi
] + bh)
−→hi = δ(Wxf ¯X + Wfh1:i−1 + bf )
←−
hi = δ(Wxb ¯X + Wfh>i + bb) ¯X = Relu(Wx ˆXimg + WmCm + b)
where ˆXimg = CNN(Ximg) is the visual feature matrix extracted by convolutional neural net￾works(CNN),
−→hi ,
←−
hi are the forward and backward propagation hidden state vectors, which are
concatenated together to predict the i-th variable in the performance parameters Dir.
3.4 LEARNING
In order to generate real images with physical connotations, the learning loss consists of two parts:
GAN loss LGAN (prompting the model to generate real images) and swelling cavities feature loss
4
Under review as a conference paper at ICLR 2019
Algorithm 1 The training algorithm using minibatch SGD with learning rate η
Input: minibatch images Ximg; minibatch data Dd, Dc, Cm, Hv; minibatch size S;
1: for n = 1 to S do do
2: z ∼ N(µ(Cm), δ(Cm));
3: X0img ← G(z, Dd, Dc);
4: Lr ← D(Attn(Ximg, Hv));
5: Lf ← D(Attn(X0img, Hv));
6: LHv ← ||En(X0img) − Hv||2;
7: LD ← −log(Lr) − log(1 − Lf ) + λ2LHv ;
8: θD ← θD − η 5 θD LD;
9: LG ← −log(Lf ) + λ2LHv ;
10: θG ← θG − η 5 θG LG;
11: end for
LHv
(prompting the model to generate images with physical connotations, adjusting the parameters
of attention mechanism by backpropagation). In order to calculate swelling cavities feature loss
LHv
, We need to measure the corresponding degree of the image Ximg and the swelling cavities
feature Hv, Which requires encoding the image.
The image encoder: image encoder maps the image Ximg to the space of the swelling cavities
feature space by the convolutional neural network(CNN). The middle layer of the CNN can learn
local features of the region in the image Ximg. Finally, we map the CNN features of Ximg to
the swelling cavities distribution features through the fully connected layers: ¯Hv = En(Ximg) =
W f c(CNN(Ximg)).
The loss and learning steps: the loss function consists of the loss of GAN LGAN and the swelling
cavities distribution features loss LHv
, and the final loss based on prior knowledge and attention
mechanism is defined as L = LGAN + λLHv .
where λ is a hyperparameter that balances two losses. The first loss represents the unconditional
loss of GAN Xu et al. (2017), and the second loss represents the conditional loss constrainted by the
swelling cavities features. The loss LHv minimizes the difference between the cavities feature H0v
of the generated image X0img and the true cavities feature Hv, defined as:LHv = ||H0v − Hv||2 =
||En(X0img) − Hv||2.
The loss of GAN consists of the loss of the generator and the loss of the discriminator. We assign the
swelling cavities feature loss LHv
to the generator and the discriminator. The generator is trained
to generate real images X0img with swelling cavities features(physical connotations), whose loss is
defined as:
LG = −EX0img∼PG
logD(X0img, Hv) + λ2LHv
The discriminator is trained to distinguish whether the input material image X0img is real or fake,
whose loss is defined as:
LD = −EXimg∼Pdata logD(Ximg, Hv) − EX0img∼PG
log(1 − D(X0img, Hv)) + λ2LHv
Finally, we can train the generator and discriminator alternately.
3.5 TRAINING DETAILS
The training algorithm is implemented with the deep learning lib Keras. The networks are randomly
intialized without any pre-training and is trained with decayed Adagrad and RMSprop. We train for
a total of 2000 epochs and use a batch size of 10, a learning rate of 1×10−5
, and a weight decay rate
of 1.6×10−6
for the image generation task. The generated image size is 64×64. The molecule2vec
embedding vectors are intialized as random vectors.
5
Under review as a conference paper at ICLR 2019
4 EXPERIMENTS
Extensive experiments is carried out to evaluate the proposed model by comparing with multiple
baseline models. First, we performed an experiment on image generation task and evaluate the
ability of the model to generate experimental material images based on condition parameters(data￾to-image task). Then the network P(D0r|X0img) predicting the performance parameters D0r by the
image X0img is tested(image-to-data task), which is also compared with the theoretical models from
computational materials science Ehrlich (1981) Boltax et al. (1978) Rest & Hofman (1999).
4.1 DATASET
We introduce a dataset called ISMD(Irradiation Swelling Material Dataset) into this task. This
dataset includes more than 22,000 images with size of 64 × 64 and with shooting scale of 100nm
from irradiation experiments, corresponding the molecular compositions set m of 14 alloys, mate￾rial thermodynamics and mechanical properties set Dd, experimental condition parameters set Dc,
swelling cavities distribution set Hv, and the performance parameters set Dr. More details about
dataset ISMD production Details and data formats can be found in Appendix.A;
4.2 BASELINES
Baseline models for image generation.
• Pure GAN: the prior distribution and the attention mechanism removed from the proposed
model.
• The prior distribution dropped: only the prior distribution of the proposed model re￾moved and is used to evaluate the value of prior distribution.
• The attention mechanism dropped: only the attention mechanism of the proposed model
removed and is used to evaluate the value of the attention mechanism.
• Variational Auto-Encoder: To verify whether GAN is a better generation model for this
task.
Baseline model for materials performance prediction: The theoretical model from computational
material science is used to evaluate the prediction model in image-to-data task.
Figure 3: Above:The images generated by the proposed model and the baseline models.Below:
The materials performance prediction model compared to theoretical models and the experimental
results (Partial results).
4.3 EVALUATION METRICS
Evaluation metrics for image generation: Inception score Salimans et al. (2016) Springenberg
(2015)is the evaluation method for generating images. The core idea of the method is: im-
6
Under review as a conference paper at ICLR 2019
Figure 4: The data is collected by instruments
records, organized and annotated by researchers.
Figure 5: We put the trained models on the server
and developed a Web App.
ages contain meaningful objects should have a conditional label distribution p(y|x) with low en￾tropy, moreover, the model is expected to generate varied images, so the marginal R p(y|x0 = G(z))dz should have high entropy.Combining these two requirements, the evaluation is defined
as exp(EX0imgKL(p(m|X0img)||p(m|Ximg))).
Evaluation metrics for materials performance prediction: the material performance parameters
from the performance evaluation experiments is Dr∗
, the parameters calculated by the theoretical
models Ehrlich (1981) Boltax et al. (1978) Rest & Hofman (1999) is ˆDr, and which predicted by
the image Ximg in the proposed model P(Dr|Ximg, Cm) is D0r
. Finally, the accuracy score is
defined as AccuracyScore = 103/||D0r − Dr∗
||2.
4.4 RESULTS
For the data-to-image task, the proposed model performs better on the ISMD dataset than other
baseline models, achieving higher scores.For the image-to-data task, our model performs better
on the prediction of some material properties compared to the theoretical model. In summary, the
combination of the image-to-data model and computational material science models should be better.
Table 1: The data-to-image task.
models Inception score
VAE 1.89 ± 0.05
gan 2.30 ± 0.07
pr+gan 2.40 ± 0.02
att+gan 2.84 ± 0.07
Our model 3.87 ± 0.08
Table 2: The accuracy score for materials
performance prediction.
prediction tasks img-to-data theoretical models
deltas 2.59 3.07
deltab 2.71 3.76
deltae 2.80 2.75
deltaL 2.56 4.17
HB 2.08 3.27
HRC 2.01 4.08
HV 2.05 2.00
K 2.59 2.54
5 CONCLUSION
In this work, we propose the data-to-image model based on prior knowledge and attention mecha￾nism to achieve the generation of irradiated material images. In the image-to-data task, CNN and
BiLSTM are used to extract the features and learn the dependency of the variables. In the future
work, we plan to generate more controllable and diverse material images with physical connotations
and predict the performance data to replace the experiments.
APPENDIX
.1 MATERIAL THERMODYNAMICS AND MECHANICAL PROPERTIES SET Dd
mechanical properties:
7
Under review as a conference paper at ICLR 2019
Figure 6: We visualize the distribution of the
molecular composition feature vector Cm in two￾dimensional space.
Figure 7: The swelling cavities distribution Hv. E: modulus of elasticity (unit: MP a); ν: Poisson’s ratio;
δ¯: Equivalent stress (unit: MP a); ε¯: Equivalent plastic strain (unit: MP a);
n: strain hardening index; m: strain rate sensitivity index;
K: intensity factor; Lame constant λ;
Lame constant G;
thermodynamics:
Ct: crystal type; Cd: lattice parameter (unit: nm);
Tm: melting point (unit: K); ρ: theoretical density (unit: g/cm3);
Vˆ : Thermal expansion coefficient (unit: 10−6 · K−1); Ch: Thermal conductivity (unit: W · m−1 · C−1);
Hc: heat capacity (unit: KJ/mol · K); Eh: hot (unit: KJ/mol);
Ce: Seebeck temperature difference electromotive force factor (unit: uV /K); CR: resistivity (unit: 103 · Ω · cm);
.2 EXPERIMENTAL CONDITION PARAMETERS SET Dd
Fast neutron injection volume φf (unit: 1019n/cm2); Thermal neutron injection amount φt(unit: 1019n/cm2);
Irradiation flux φi(unit: 1019n/cm2); Irradiation temperature Ti(unit: K);
Experimental temperature Te(unit: K);
.3 PERFORMANCE PARAMETERS SET Dr
Yield limit δs(unit: MP a); Stretch limit δb(unit: MP a);
Elastic limit δe(unit: MP a); The total extension rate is δL(unit: %);
Brinell hardness HB(unit: kg/mm2); Rockwell hardness HRC (unit: mm) (dimensionless: HR*T);
Vickers hardness HV (unit: kg/mm2); Volume expansion rate Kv(unit: %);
Irradiation growth rate KL(unit: %); Fracture toughness KIc(unit: MP a/m1/2);
Creep performance δt(unit: MP a); Crisp feature CHe ∈ {0, 1};
Figure 8: Images generation recording of the baseline models and proposed model from different
epochs.
8
Under review as a conference paper at ICLR 2019
REFERENCES
Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M. Taha, and Vijayan K. Asari. Re￾current residual convolutional neural network based on u-net (r2u-net) for medical image segmen￾tation. IEEE Conf, 2018.
Philip Bachman and Doina Precup. Data generation as sequential decision making. Computer
Science, pp. 3249–3257, 2015.
A. Boltax, J. P. Foster, J. E. Kalinowski, and D. C. Swenson. Design applications of irradiation creep
and swelling data. Trans. Am. Nucl. Soc.; (United States), 28, 1978.
D. C. Cirean, A Giusti, L. M. Gambardella, and J Schmidhuber. Mitosis detection in breast cancer
histology images with deep neural networks. In International Conference on Medical Image
Computing and Computer-Assisted Intervention, pp. 411–8, 2013.
Karl Ehrlich. Irradiation creep and interrelation with swelling in austenitic stainless steels. Journal
of Nuclear Materials, 100(1):149–166, 1981.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In International Conference
on Neural Information Processing Systems, pp. 2672–2680, 2014.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: a
recurrent neural network for image generation. Computer Science, pp. 1462–1471, 2015.
Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports.
ICML, 2017.
Kim Uk Jung, Kim Gu Hak, and Ro Man Yong. Iterative deep convolutional encoder-decoder
network for medical image segmentation. IEEE Conf, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. Arxiv, 2013.
P. Kisilev, E. Walach, E. Barkan, B. Ophir, S. Alpert, and S. Y. Hashoul. From medical image to
automatic medical report generation. Ibm Journal of Research and Development, 59(2/3):2:1–2:7,
2015.
Wei Li, Kevin G. Field, and Dane Morgan. Automated defect analysis in electron microscopic
images. Arxiv, 2018.
Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Generating images
from captions with attention. Computer Science, 2015.
B. H. Menze, Leemput K Van, D Lashkari, M. A. Weber, N Ayache, and P Golland. A genera￾tive model for brain tumor segmentation in multi-modal images. Ibm Journal of Research and
Development, 13(2):151–9, 2010.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen￾tations in vector space. Computer Science, 2013.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. Computer Science, pp.
2672–2680, 2014.
S. I. Porollo, A. N. Vorobjev, Yu. V. Konobeev, N. I. Budylkin, E. G. Mironova, F. A. Garner, S. I.
Porollo, A. N. Vorobjev, Yu. V. Konobeev, and N. I. Budylkin. Irradiation creep and stress-affected
swelling in austenitic stainless steel 16cr-15ni-3mo-nb-b irradiated in the bn-350 reactor. Aaps
Pharmscitech, 13(4):1386–1395, 2000.
J. Rest and G. L. Hofman. Dart model for irradiation-induced swelling of uranium silicide dispersion
fuel elements. Nuclear Technology, 126(1):88–101, 1999.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomed￾ical Image Segmentation. Springer International Publishing, 2015.
9
Under review as a conference paper at ICLR 2019
A Rovinelli. Using machine learning and a data-driven approach to identify the small fatigue crack
driving force in polycrystalline materials. Arxiv, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. Computer Science, 2016.
Mike Schuster. Bi-directional recurrent neural networks for speech recognition. Ieice Technical
Report Speech, 96:7–12, 1996.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative
adversarial networks. Computer Science, 2015.
Salehi Ssm, D Erdogmus, and A Gholipour. Auto-context convolutional neural network (auto-net)
for brain extraction in magnetic resonance imaging. IEEE Transactions on Medical Imaging, PP
(99):1–1, 2017.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong
He. Attngan: Fine-grained text to image generation with attentional generative adversarial net￾works. Arxiv, 2017.
Han Zhang, Tao Xu, and Hongsheng Li. Stackgan: Text to photo-realistic image synthesis with
stacked generative adversarial networks. Computer Science, pp. 5908–5916, 2016.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim￾itris N. Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial
networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP(99), 2017.
10
