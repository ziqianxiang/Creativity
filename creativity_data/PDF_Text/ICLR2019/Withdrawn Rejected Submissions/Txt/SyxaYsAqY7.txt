Under review as a conference paper at ICLR 2019
Second-Order Adversarial Attack and Certi-
fiable Robustness
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial training has been recognized as a strong defense against adversarial
attacks. In this paper, we propose a powerful second-order attack method that
reduces the accuracy of the defense model by Madry et al. (2017). We demonstrate
that adversarial training overfits to the choice of the norm in the sense that it is
only robust to the attack used for adversarial training, thus suggesting it has not
achieved universal robustness. The effectiveness of our attack method motivates an
investigation of provable robustness of a defense model. To this end, we introduce
a framework that allows one to obtain a certifiable lower bound on the prediction
accuracy against adversarial examples. We conduct experiments to show the
effectiveness of our attack method. At the same time, our defense model achieves
significant improvements compared to previous works under our proposed attack.
1	Introduction
Deep neural networks (DNNs) have achieved significant success when applied to a variety of
challenging machine learning tasks. For example, DNNs have obtained state-of-the-art accuracy
on large-scale image classification (Krizhevsky et al., 2012; He et al., 2016b). At the same time,
vulnerability to adversarial examples, an undesired property of DNNs, has drawn attention in the
deep-learning community (Szegedy et al., 2013; Goodfellow et al., 2014). Generally speaking,
adversarial examples are perturbed versions of the original data that successfully fool a classifier. For
example, in the image domain, adversarial examples are images transformed from natural images
with visually negligible changes but lead to different classification results (Goodfellow et al., 2014).
The existence of adversarial examples has raised many concerns, especially in scenarios with a high
risk of misclassification, such as autonomous driving.
To tackle adversarial examples, many works have been proposed to improve the robustness of DNNs
(Papernot et al., 2016; Meng & Chen, 2017), referred to as defense models. However, most of these
defense methods have been later attacked successfully by new attack methods (Carlini & Wagner,
2017b;a). For example, Athalye et al. (2018) conducted a case study that successfully attacked seven
defense methods submitted to ICLR2018.
One model that demonstrated good performance against strong attacks, and has thus far not been
successfully attacked, is based on adversarial training (Goodfellow et al., 2014; Madry et al., 2017).
Adversarial training constructs a defense model by augmenting the training set with adversarial
examples. Though successful in adversarial defensing, the underlying mechanism is still unclear. In
this paper, we explain the good performance is due to "degenerate global minimum", a phenomenon
firstly discussed in (Trambr et al., 2017), and show that the Madry's defense model (Madry et al.,
2017), adversarial training with '∞ attack is not robust against an '2 attack even on MNIST. In
particular, we develop a new attack method based on the approximated second-order derivative that
forces its accuracy worse than naturally trained baseline models. To our knowledge, this is the
first `2 attack method that significantly reduces the accuracy of Madry’s model. In addition, the
'∞ version of our method also breaks the '2 based adversarially trained model. Considering that
Sharma & Chen (2017) has proposed `1 -based adversarial examples that break the Madry’s model,
we believe the robustness of adversarial training overfits to the choice of norms and is not universally
robust. Concurrent work by Schott et al. (2018) also proposes similar conclusions via comprehensive
experiments.
1
Under review as a conference paper at ICLR 2019
Our findings lead to a concern that most of the existing defense methods are heuristically driven, thus
any defense method that cannot provide theoretically-provable robustness guarantee is potentially
vulnerable to future attacks.
Several works on provable or certifiable adversarial defense methods have been proposed recently
(Raghunathan et al., 2018; Kolter & Wong, 2017; Sinha et al., 2017). However, most of them either
have to make strong assumptions, such as on the structure of the model or on the smoothness of the
loss function or are difficult to extend to large-scale datasets, the most common application scenario
for DNNs.
Recently, Mathias et al. (2018) developed theoretical insight of certifiable robust prediction, by
building a connection between differential privacy and model robustness. It is shown that adding
properly chosen noise to the classifier will lead to certifiable robust prediction.
Built on the idea in (Mathias et al., 2018), as our second contribution, we conduct an analysis based
on R6nyi divergence (Van Erven & Harremos, 2014) and show a higher upper bound on the tolerable
size of attacks compared with Mathias et al. (2018). In addition, we suggest there exists a connection
between adversarial defense and robustness to random noise. Based on this, we introduce a more
comprehensive framework, incorporating stability training to improve classification accuracy. One
advantage of our framework is that it has no requirement on the model structure and is applicable to
all classification models. Considering MNIST and CIFAR-10, our experiments demonstrate that the
proposed defense yields stronger robustness to adversarial attacks, compared to other models.
2	Preliminary
2.1	Notation
We consider the task of image classification. Natural images are represented by x ∈ X , [0, 1]h×w×c,
where X represents the image space, with h, w, c the height, width, and channels of an image,
respectively. An image classifier over k classes is considered as a function f : X → {1, . . . , k}. In
this paper, we only consider classifiers constructed by DNNs. To better present our framework, we
define a stochastic classifier, a function f over x with output f (x) being a multinomial distribution
over {1, . . . , k}, i.e., P (f (x) = i) = pi for Pi pi = 1. One can classify x by picking argmaxi pi.
Note this distribution is different from the one generated from softmax.
2.2	RENYI DIVERGENCE
Our theoretical result depends on the Renyi divergence, defined as follows (Van Erven & Harremos,
2014):
Definition 1 (Renyi Divergence) FOr two probability distributions P and Q over R, the Renyi
divergence of order α > 1 is
1	Pα
Da(PkQ) = α-ιlog Exy (Q)
(1)
2.3	Adversarial Examples
Given a classifier f : X → {1, . . . , k} for an image x ∈ X, an adversarial example x0 satisfies
D(x, x0) < E for some small e > 0, and f (x) = f (x0), where D(∙, ∙) is some distance metric, i.e., x0
is close to x but yields a different classification result. The distance is often described in terms of an
'p metric, and in most of the literature '2 and '∞ metrics are considered. In this paper, we focus on
the '2 metric, but our method is easy to extend to the '∞ scenario.
Adversarial examples are often constructed by iterative optimization methods. Previous work has
proposed a number of adversarial attack methods, such as the Fast Gradient Sign Method (FGSM)
(Kurakin et al., 2016), along with its multi-step variant FGSMk, which is equivalent to exploring
adversarial examples that increase the classification loss using projected gradient descent (PGD)
(Madry et al., 2017):
xt+1 =∏x+s (xt + ɑ(Vχ L(θ, x,y))	⑵
where Πx+S is the projection operation that ensures adversarial examples stay in the 'p ball S
around x. In (Madry et al., 2017), it has also been shown that a PGD attack is a universal adversary
2
Under review as a conference paper at ICLR 2019
among all first-order attack methods. Their analysis and evidence from experiments all suggest any
adversarial attack method that only incorporates gradients of the loss function w.r.t. the input cannot
do significantly better than PGD.
2.4	Adversarial Training
Adversarial training constructs adversarial examples and includes them into a training set to train
a new and more robust classifier. This method is intuitive and has gained great success in defense
(Goodfellow et al., 2014; Madry et al., 2017). The motivation behind adversarial training is that
finding a robust model against adversarial examples is equivalent to solving the saddle-point problem
minθ maxx0:D(x,x0)< L(θ, x0, y). The inner maximization is equivalent to constructing adversarial
examples, while the outer minimization is the standard training procedure for loss minimization.
3	Second-Order Adversarial Attack
In this section, we propose an efficient second-order adversarial attack method. As one motivation,
note most current attack methods construct adversarial examples based on the gradient of a loss
function. However, a first-order derivative is not effective for attacks if the defense model is trained
adversarially.
To see this, first note that adversarial training is equivalent to solving the optimization problem:
(θ, x) = argminθ argmaXχo：d(χ,χo)<e L(θ, x0, y). The solution is a saddle point, i.e., it not only
converges to a local minimum θ in the parameter space, but also converges to a local maximum X in
the sample space, in WhiCh the gradient ideally vanishes at X as VχL(θ, x, y)∣χ = 0. In practice, an
adversarial training often finds θ that makes the loss function flat in the neighborhood of a natural
example x, Which leads to inefficient exploration for adversarial examples in the attack methods. This
phenomenon is referred to as "degenerate global minimum" in (Tramer et al., 2017) and analyzed for
adversarial training via single-step attacks. Here We argue using multiple-step attacks also suffers
from this issue. This motivates utilization of the second-order derivative of the loss function to
construct adversarial examples.
Specifically, assume the loss function is tWice differentiable With respect to x. Using Taylor expansion
on the difference betWeen the losses on the original and perturbed samples, and assuming the gradient
vanishes, we have L(θ, X + r,y) 一 L(θ, x, y) ≈ 1 rτH(θ, x, y)r with r being the perturbation, and
H(θ, x, y) is the Hessian matrix of the loss function. Our goal is to find a small perturbation r that
maximizes the difference L(θ, x + r, y) 一 L(θ, x, y). Our idea is based on the observation that
the optimal perturbation direction should be in the same direction as the first dominant eigenvector,
e(θ, x,y), of H(θ, x,y), that is r = E1看片几 for some constant e > 0. However, computing
the eigenvectors of the Hessian matrix requires O(I3) runtime with I the dimension of the data.
To tackle this issue, we adopt the fast approximation method from Miyato et al. (2017), which is
essentially a combination of the power-iteration method and the finite-difference method, to efficiently
find the direction of the eigenvector. Based on this method, the optimal direction, denoted radv , is
approximated* by
radv = μ~厂,	with g = vxL(θ, x, y^x+ξd	(3)
kgk2
where d is a randomly sampled unit vector and ξ > 0 is a manually chosen step size. In practice, d is
drawn from a centered Gaussian distribution and normalized such that its `2 norm is 1.
This procedure is essentially a stochastic approximation to the optimal direction, where the random-
ness comes from d. To reduce the variance of the approximation, we further take the expectation
over the Gaussian noise, yielding g = Ed~N(0,σ2i) [VχL(θ, x,y)∣χ+d]. Note that choosing σ is
equivalent to choosing the step size ξ in (3). Finally, we construct adversarial examples by an iterative
update via PGD:
xt+1
Πx+S (x + αradv) = Πx+S (x +
〜gt )
α E),
where gt = Ed~N(。⑹)VxL(θ, x,y)∣χ+d]
(4)
* Detailed derivations are provided in the Appendix.
3
Under review as a conference paper at ICLR 2019
Intuitively, this method perturbs the example at each iteration and tries to move out of the local
maximum in the sample space, due to the introduction of random Gaussian noise.
Connection to Expectation of Transformation (EOT) Attack We can think of (4) as gra-
dient descent for maximizing an objective function of Ed〜N(。卢工)[VχL(θ,x,y)∣χ+d]. Note
Ed〜N(0,σ2i) [VχL(θ, x,y)∣χ+d] = VxEd〜N(0,σ2i)[VχL(θ, x,y)∣χ+d]. Consequently, (4) is equiv-
alent to constructing adversarial examples with the EOT method proposed by Athalye & Sutskever
(2017), when a defense model contains added Gaussian noise d.
In general, EOT tries to construct adversarial samples against a randomized defense model by solving
the optimization problem: x0 = argmaxx Et〜T [L(θ, x, y)], where t 〜T is the randomization in
the defense model. Intuitively, EOT aims to estimate the correct gradient by excluding the effect of
randomization via expectation. If a defense model tries to add Gaussian noise to its inputs, the corre-
sponding optimization problem for EOT becomes x0 = argmaxx Ed〜N(o,σ2i) [VxL(θ, x, y) |x+d],
which is the same as the approximate solution to our second-order attack method.
Note a significant difference between our method and EOT is that the random noise added in EOT
depends on its corresponding defense model, whereas randomness in our method is independent of a
defense model. In the experiments, we show our attack method dramatically reduces the accuracy of
a defense model proposed by Madry et al. (2017).
4 Certifiable Robustness
The effectiveness of the proposed attack method, as we will show in the experiments, suggests
defending from adversarial attack is extremely difficult, and any defense model without certifiable
robustness are always possible to be bypassed by stronger attacks.
Inspired by the PixelDP method (Mathias et al., 2018), we propose a framework that enables
certifiable robustness on any classifier. Intuitively, our approach adds random noise to pixels of
adversarial examples before classification, to eliminate the effects of adversarial perturbations. The
most important feature of this framework is that it allows calculation of an upper bound on the
tolerable size of attacks.
Algorithm 1 Certifiable Robust Classifier
Require: An input image x; A standard deviation σ > 0; A classifier f over {1, . . . , k}; Number of
1:
2:
3:
iterations n (n = 1 is sufficient if only the robust classification c is desired).
Set i = 1.
for i ∈ [n] do
Add i.i.d. Gaussian noise N(0, σ2) to each pixel of x and apply the classifier f on it. Let the
4:
5:
6:
output be ci = f(x + N(0, σ2I)).
end for
Estimate the distribution of the output as pj
Calculate the upper bound
#{ci=j ：i=1 ,...,n}
L = supα>1
log ( 1 - p(ι) - p(2) + 2 (2 (p1-α + p1-)α
n
where p(1) and p(2) are the first and the second largest values in p1, . . . ,pk.
7: Return classification result c = argmaxi pi and the tolerable size of the attack L.
Our approach is summarized in Algorithm 1. In the following, we develop theory to prove the
certifiable robustness of the proposed algorithm. Our goal is to show that if the classification of x in
Algorithm 1 is c, then for any examples x0 such that kx - x0 k2 ≤ L, the classification of x0 is also c.
To prove our claim, first recall that a stochastic classifier f over {1, . . . , k} is a classifier whose
output f(x) has a multinomial distribution over {1, . . . , k} with probabilities as (p1, . . . ,pk). In this
context, robustness to an adversarial example x0 generated from x means argmaxi pi = argmaxj p0j
with P (f (x) = i) = Pi and P (f (x0) = j) = pj, where P(∙) denotes the probability of a specific
output value. In the remainder of this section, we show Algorithm 1 achieves such robustness based
on the Renyi divergence, starting with the following lemma.
4
Under review as a conference paper at ICLR 2019
Lemma 1 Let P = (p1, . . . ,pk) and Q = (q1, . . . , qk) be two multinomial distributions over the
same index set {1, . . . , k}. If the indexes of the largest probabilities do not match on P and Q, that is
argmaxi pi 6= argmaxj qj, then
Dα(QkP) ≥-log
- p(2) + 2
(5)
where p(1) and p(2) are the largest and the second largest probabilities in pi’s.
To simplify notation, We define Mp(χι,..., xn) = (1 PZi xf) 1/p as the generalized mean. The
RHS in condition (5) becomes - log 1 - 2M1 p(1) , p(2) + 2M1-α p(1) , p(2) .
Lemma 1 proposes a lower bound of the Renyi divergence for changing the index of the maximum of
P, i.e., for any distribution Q, if Dα(QkP) < Tog(1 - 2Mi (p(i),P(2)) + 2Mι-α (P⑴,P⑵)),
the index of maximum of P and Q must be the same. Based on Lemma 1, we obtain our main
theorem on certifiable robustness, validating our claim:
Theorem 2 Suppose we have x ∈ X, and a potential adversarial example x0 ∈ X such that
∣∣x — x0∣∣2 ≤ L. Given a k-classifier f : X 一 {1,...,k} ,let f(x + N (0, σ2I))〜(pι,...,Pk) and
f(x0 + N(0,σ2I))〜(p'ι,...,p'k).
If the following condition is satisfied, with p(1) and p(2) being the first and second largest probabilities
in pi ’s:
SUp (- 2σ- log (1 — 2Mi (P(1),P(2)) + 2Mi-α(P(1),P(2)))) ≥ L2 ,
α>1	α
(6)
then argmaxi pi = argmaxj p0j
With Theorem 2, we can enable certifiable robustness on any classifier f by adding i.i.d. Gaussian
noise to pixels of inputs during testing, as done in Algorithm 1. It provides an upper bound for the
tolerable size of attacks for a classifier. Since the upper bound only depends on the output distribution
(p1 , . . . , pk ), one can evaluate the bound based only on natural examples. Note the evaluation requires
adjustment and computing confidence intervals for p(1) andp(2), but we omit the details as it is a
standard statistical procedure. In the experiments, we use the end points of the 95% confidence
intervals for p(1) and p(2).
Based on the property of generalized mean, one can show that the upper bound is larger when
the difference between p(1) and p(2) becomes larger. This is consistent with the intuition that a
larger difference between p(1) and p(2) indicates more confident classification. In other words, more
confident classification, in the sense that more probability concentrated on one class, is beneficial for
robustness.
It is worth mentioning that, as pointed out in (Mathias et al., 2018), the noise is not necessarily added
directly to the inputs but also to the first layer of a DNN. Given the Lipschitz constant of the first layer,
one can still calculate an upper bound using our analysis. We omit the details here for simplicity.
Compared to the upper bound in PixelDP, our upper bound is strictly higher. We show the improve-
ment using a simulation detailed in the Appendix. In the next section, we propose a simple strategy
to improve the empirical performance of this framework.
5	Improved Certifiable Robustness
In Algorithm 1, for a classifier f with a standard DNN, the added Gaussian noise is harmful to the
classification accuracy on the original data. As discussed above, inaccurate prediction, in the sense
that p(1) and p(2) are close, leads to weak robustness. Fortunately, one strength of Algorithm 1 is it
requires nothing particular on the classifier f, which yields flexibility to modify f to make it more
robust against Gaussian noise.
Note robustness to Gaussian noise is much easier to achieve than robustness to carefully crafted
adversarial examples. In PixelDP, the authors incorporated noise by directly adding the same noise
during the training procedure. However, we note that there have been notable efforts at developing
5
Under review as a conference paper at ICLR 2019
robust DNNs to natural perturbations (Xie et al., 2012; Zhang et al., 2017); yet, these methods
failed to defend models from adversarial attacks, as they are not particularly designed for that task.
Our framework allows us to adapt these methods to improve the accuracy of classification when
Gaussian noise is present, improving the robustness of our model. We emphasize that a connection
between robustness to adversarial examples and robustness to natural perturbation (Xie et al., 2012;
Zhang et al., 2017) has been established, which introduces a much wider scope of literature into the
adversarial defense community.
Stability Training The idea of introducing perturbations during training to improve model ro-
bustness has been studied in many works. In (Bachman et al., 2014) the authors regard perturbing
models as a construction of pseudo-ensembles, to improve semi-supervised learning. More recently,
Zheng et al. (2016) used a similar training strategy, named stability training, to improve classification
robustness on noisy images.
For any natural image x, stability training encourages its perturbed version x0 to yield a similar
classification result under a classifier f, i.e., D(f (x), f(x0)) is small for some distance measure D.
Specifically, given a loss function L0 for the original classification task, stability training introduces a
regularization term L(x, x0) = L0 + γLstability (x, x0) = L0 +γD(f(x), f(x0)), where γ controls the
strength of the stability term. As we are interested in a classification task, we use cross-entropy as the
distance D between f(x) and f(x0), yielding the stability loss Lstability = - Pj P(yj |x) log P(yj|x0)
where P(yj |x) and P(yj |x0) are the probabilities generated after softmax. In this paper, we add i.i.d.
Gaussian noise to each pixel of x to construct x0, as suggested in (Zheng et al., 2016). Note that this
is in the same spirit as adversarial training, but is only designed to improve the classification accuracy
under a Gaussian perturbation.
6	Experiments
We conduct experiments to evaluate the performance of our proposed methods in terms of attack
effectiveness and defense robustness. Our methods are tested on the MNIST and CIFAR-10 data sets.
The architecture of our model follows the ones used in (Madry et al., 2017). Specifically, for the
MNIST data set, the model contains two convolutional layers with 32 and 64 filters, each followed
by 2 × 2 max-pooling, and a fully connected layer of size 1024. For the CIFAR-10 dataset, we use
a wide ResNet model (Zagoruyko & Komodakis, 2016; He et al., 2016a). The network contains
5 residual unites with (16, 160, 320, 640) filters. The implementation details are provided in the
Appendix. In both datasets, image intensities are scaled to [0, 1], and the size of attacks are also
rescaled accordingly. For reference, a distortion of 0.031 in the [0, 1] scale corresponds to 8 in [0, 255]
scale.
6.1	Theoretical Bound
We first evaluate the theoretical bounds of the size of attacks. With Algorithm 1, we are able to
classify a natural image x and calculate an upper bound for the size of attacks L for this particular
image. Thus, with a given size of the attack L0, we know the classification result is robust if L0 < L.
Further, if the classification for a natural example is correct and robust for L0 simultaneously, we
know any adversarial examples x0 such that kx - x0 k2 < L0 will be classified correctly. Therefore,
we can calculate the proportion of such examples in the test set to determine a lower bound of
accuracy under attack of size L0 . We plot different lower bounds for various L0 for both MNIST and
CIFAR-10 in Figure 1. To interpret the result, for example on MNIST, when σ = 0.4, Algorithm 1
achieves at least 50% accuracy under attacks whose '2-norm sizes, kx - x0∣∣2, are smaller than 0.8.
A clear trade-off between the tolerable attack sizes and the accuracy lower bound is present in
Figure 1. This is anticipated, as our lower bound 6 indicates a higher standard deviation σ results in a
higher proportion of robust classification, but in practice, a larger amount of noise would also lead to
a worse classification accuracy.
6.2	Empirical Results
We next perform classification and measure the accuracy on real adversarial examples to evaluate the
performance of our attack and defense methods. We first apply our attack method to the state-of-the-
6
Under review as a conference paper at ICLR 2019
4
Figure 1: Accuracy lower bounds for MNIST data set (left) and CIFAR-10 data set (right) against
tolerable sizes of attacks with various choices of σ in Algorithm 1.
art defense model proposed in (Madry et al., 2017) based on adversarial training. In all experiments,
we focus on five settings, summarized in Table 1. The adversarially trained model is trained against
'∞ PGD attack while all the attacks are '2 attack.
Table 1: Five settings of attack methods and defense models.		
Number	Defense Model	Attack Method
1	Naturally Trained Model	PGD
2	Adversarial Training (Madry's)	PGD
3	Naturally Trained Model	second-order (S-O) attack
4	Adversarial Training (Madry's)	second-order (S-O) attack
5	Stability trained model with Gaussian Noise (STN)	second-order (S-O) attack
MNIST In the first plot in Figure 2, we monitor the average `2 norm of gradients of
the loss function during the construction of adversarial examples. Specifically, we compute
Nb D∈i kVχL(θ, x,y)∣χ(t) k2 for each t, where I is the index set of a batch. We monitor this
quantity under the setting 1, 2 and 4 in Table 1. The result shows the `2 norms of the gradients of the
adversarially trained model are much smaller than the ones of the naturally trained model, validating
our explanation in Section 3, that an adversarially trained model tends to make the loss function
flat in the neighborhood of natural examples. It also shows our attack method can find adversarial
examples with large loss more efficiently, by incorporating second-order derivative information.
In the second plot, we show the classification accuracy on MNIST under the setting 1, 2 and 4. The
plot suggests that S-O attack is able to dramatically reduce the accuracy of Madry’s model compared
to PGD attack. The resulting accuracy is even worse than the one from the naturally trained baseline
model. Note we do not include other attack methods such as C&W attack (Carlini & Wagner, 2017b)
as no first-order attack is significantly stronger than PGD (Madry et al., 2017). We include the results
of other attack methods in the Appendix.
In the third plot, we show the accuracy of different defense models under S-O attack, under the
setting 3, 4 and 5. The plot suggests our model achieves better accuracy than both the baseline and
the Madry’s model.
Note in all experiments for evaluating our defense model, we use S-O attack as the adversary.
The reason is if we use other attacks such as PGD, our defense model may cause the problem of
obfuscating true gradients via randomization (Athalye et al., 2018), thus achieving robustness unfairly.
On the other hand, as we discussed in Section 3, S-O attack is equivalent to EOT attack with respect to
Gaussian noise (the noise we used in our defense model), thus it avoids obfuscating the true gradients
via randomization. As a result, our results are reliable.
CIFAR-10 In Figure 3, we show the classification accuracy on CIFAR-10 under the setting 3, 4
and 5. In addition, we include results for PixelDP from (Mathias et al., 2018) to show how stability
training helps improve classification accuracy. Here the attack method we use is the S-O attack. As
a comparison, our defense model obtains higher accuracies than both Madry’s model and PixelDP
against '2 and '∞ attacks.
Overall, stability training combined with Gaussian noise shows a promising level of robustness and
performs better than other models even under attacks that incorporate randomization.
7
Under review as a conference paper at ICLR 2019
5 0 5 0 5
2 2 11
upeJEJOU
-→- Natural
-→- PGD
O
5 IO 15
Number of iterations
0 8 6 4 2 0
■ ■ ■ ■ ■ ■
Iooooo
Auguue 1s3
0 8 6 4 2 0
■ ■■■■■
Iooooo
AUeJnSe 1s3O
1
8
6
-8.6
L∙^F L∙^F
Figure 2: MNIST data set Left: Average '2 norm of the gradients of the loss function for a batch in
each iteration during adversarial attack. Middle: Classification accuracy for Madry’s model under
PGD attack and S-O attack. Right: Accuracy for different models under the same S-O attack.
-8.6
L∙^F L∙^F
-6
L∙^F
■05
L∙^F
■06
L∙^F
07
■
■08
L∙^F

AUeJnbe»9J_
O
■
O
4
L∙^F
∞
■
6
L∙^F
Figure 3: CIFAR-10 data set. We compare the accuracy on adversarial examples with various attack
sizes for both '2 (left) and '∞ (right).
Another strength of our method is that stability training only requires twice the computational
time, whereas adversarial training is extremely time-consuming due to the iterative construction of
adversarial examples.
7	Discussion
Adversarial Training Overfit to the Choice of Norms In (Madry et al., 2017), the authors propose
an adversarially trained model and argue it is robust against `2 attacks even though it is trained against
'∞ attacks. Our proposed '2 attack significantly reduces their classification accuracy. On the other
hand, the '∞ version of S-O attack also successfully attacks the adversarially trained model based on
'2 attacks. We include this experiment result in the Appendix. In general, we find adversarial training
overfit to the choice of norms.
On the Gap between Empirical and Theoretical Results A noticeable gap exists between the
theoretical bound shown in Figure 1 and the empirical accuracy. There are three possible explanations
for this gap, each pointing to a direction for future works.
The first explanation for the gap is that the proposed upper bound is not tight. Another possible
reason is that the empirical results should be worse for stronger attacks that have not been proposed,
as it has happened to many defense models. The third one is due to the limitation of the '2 distance.
Although our framework is proposed in the context of adversarial attacks, where potential attacks are
limited to be non-perceptible by humans, our theoretical analysis does not distinguish the types of
perturbations up to their '2 norms. In practice, one can perturb a few pixels with large values, such
that the change is perceptible by humans and indeed leads to a change in classification, but it only
yields small '2 distance. The existence of such perturbations enforces the small upper bound. Future
work might explore similar guarantees for the stronger '∞ distance.
8	Conclusion
We propose S-O attack, a new attack method based on an approximated second-order derivative
of the loss function. S-O attack can effectively attack adversarial training, a training strategy that
demonstrated significant robustness in previous works. We also propose an analysis on constructing
defense models with certifiable robustness, combined with a strategy based on stability training for
improving the robustness of the defense models. The experimental results show our defense model
achieve better robustness compared to previous works.
8
Under review as a conference paper at ICLR 2019
References
Anish Athalye and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint
arXiv:1707.07397, 2017.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In Advances
in Neural Information Processing Systems,pp. 3365-3373, 2014.
Nicholas Carlini and David Wagner. Magnet and" efficient defenses against adversarial attacks" are
not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017a.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39-57. IEEE, 2017b.
Gene H Golub and Henk A Van der Vorst. Eigenvalue computation in the 20th century. In Numerical
analysis: Historical developments in the 20th century, pp. 209-239. Elsevier, 2001.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.
J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Lecuyer Mathias, Atlidakis Vaggelis, Geambasu Roxana, Hsu Daniel, and Jana Suman. On the
connection between differential privacy and adversarial robustness in machine learning, 2018.
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,
pp. 135-147. ACM, 2017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a reg-
ularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976,
2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2574-2582, 2016.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP),
2016 IEEE Symposium on, pp. 582-597. IEEE, 2016.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018.
9
Under review as a conference paper at ICLR 2019
Lukas Schott, Jonas Rauber, Wieland Brendel, and Matthias Bethge. Robust perception through
analysis by synthesis. arXiv preprint arXiv:1805.09190, 2018.
Yash Sharma and Pin-Yu Chen. Breaking the madry defense model with l_1-based adversarial
examples. arXiv preprint arXiv:1710.10733, 2017.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian TramE, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017.
Tim Van Erven and Peter Harremos. Renyi divergence and kullback-leibler divergence. IEEE
Transactions on Information Theory, 60(7):3797-3820, 2014.
Junyuan Xie, Linli Xu, and Enhong Chen. Image denoising and inpainting with deep neural networks.
In Advances in neural information processing systems, pp. 341-349, 2012.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser:
Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing, 26
(7):3142-3155, 2017.
Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep
neural networks via stability training. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 4480-4488, 2016.
10
Under review as a conference paper at ICLR 2019
A Fast Approximate Method Miyato et al. (2017)
Power iteration method (Golub & Van der Vorst, 2001) allows one to compute the dominant eigen-
vector r of a matrix H. Let d0 be a randomly sampled unit vector which is not perpendicular to r,
the iterative calculation of
dt+1=kH⅛
leads to dt → r. Given H is the Hessian matrix of L(θ, x, y), we further use finite difference method
to reduce the computational complexity:
口X 〜Vχ+ξdL(θ, X + ξd, y) - VχL(θ, x, y)
^ld ≈------------------------------
ξ
Vχ+ξdL(θ, X + ξd,y)
ξ
where ξ > 0 is the step size. If we only take one iteration, it gives an approximation that only requires
the first-order derivative:
Hd ≈ Vχ+ξdL(θ, X + ξd,y)
∣∣Hdt∣∣2 ~ ∣∣Vχ+ξdL(θ,X + ξd,y)∣∣
which gives equation 3.
B Proof of Lemma 1
Lemma 1 Let P = (p1, . . . ,pk) and Q = (q1, . . . , qk) be two multinomial distributions over the
same index set {1, . . . , k}. If the indexes of the largest probabilities do not match on P and Q, that is
argmaxi pi 6= argmaxj qj , then
Da(QIlP) ≥ - log ( 1 - p(1) - p(2) +2 (2 (p1-。+ p12)α
(7)
where p(1) and p(2) are the largest and the second largest probabilities in pi’s.
Proof Think of this problem as finding Q that minimizes Dα (QIP) such that argmaxpi 6= argmaxqi
for fixed P = (pi,... ,pk). Without loss of generality, assume pi ≥ ρ ≥ ∙∙∙ ≥ pk.
It is equivalent to solving the following problem:
α
1
min -------------log
P qi=i,argmaxqi 6=i 1 - α
As the logrithm is a monotonically increasing function, we only focus on the quantity s(QIP) =
Pk=I pi (P) Part for fixed α.
We first show for the Q that minimizes S(QIlP), it must have qi = q2 ≥ q3 ≥ ∙∙∙ ≥ qk. Note here
we allow a tie, because we can always let qi = qi - and q2 = q2 + for some small to satisfy
argmaxqi 6= 1 while not changing the Renyi-divergence too much by the continuity of s.
11
Under review as a conference paper at ICLR 2019
If qj > qi for some j ≥ i, we can define Q0 by mutating qi and qj , that is Q0 =
(q1, . . . ,qi-1,qj,qi+1 . . . , qj-1, qi, qj+1, . . . , qk), then
S(QkP) - s(Q0kP) = Pi (qαpαqα) + Pj (qαpθqα) = (p1-α -p1-α)(qα - qa) > 0
which conflicts with the assumption that Q minimizes s(QkP). Thus qi ≥ qj for j ≥ i. Since q1
cannot be the largest, We have qι = q2 ≥ q3 ≥ ∙∙∙ ≥ qk.
Then we are able to assume Q = (q0, q0, q3, . . . , qk), and the problem can be formulated as
qqoY∣	(qo∖α ,XX	AiV
min Pi —	+ P2 —	+ X Pi —
q0,q2,...,qk	P1	P2	i=3	Pi
subject to 2qo + q3 +----+ qk = 1
subject to qi - q0 ≤ 0 i ≥ 1
subject to	- qi ≤ 0 i ≥ 0
Which forms a set of KKT conditions. Using Lagrange multipliers, one can obtain the solution
1
qo = (P~++p2-) ɪ a and qi = J-；-^Pi for i ≥ 3. PlUg in these quantities, the minimized
Renyi-divergence is
-log (1 - Pi - P2 +2 ɑ (P1-α + p2
Thus, We obtain the loWer bound of Dα (QkP) for argmaxPi 6= argmaxqi.
C Proof of Theorem 2
A simple result from imformation theory:
Lemma 3 Given two real-valued vectors xi and x2, the Renyi divergence of N(x1,σ2I) and
N(x2, σ2I) is
Dα(N (x1,σ2I )kN (x2,σ2I)) = Mx； -2x2k2
2σ2
(8)
Theorem 2 Suppose We have x ∈ X, and a potential adversarial example x0 ∈ X such that
∣∣x - x0k2 ≤ L. Given a k-classifier f : X → {1,...,k}, let f(x + N(0, σ2I))〜(P1,...,Pk) and
f (x0 + N(0, σ2I))〜(P<…,P).
If the folloWing condition is satisfied, With P(i) and P(2) being the first and second largest probabilities
in Pi’s:
log (1 - 2M1 (P⑴,p(2)) + 2M1-α (P⑴,p(2)))) ≥ L2	⑼
then argmaxi Pi = argmaxj P0j
Proof From lemma 3, We knoW for x and x0 such that kx - x0 k2 ≤ L, With a k-class classification
function f : X → {1, . . . , k}:
Dα(f (x0 + N(0, σ2))kf (x + N(0, σ2))) ≤ Dα(x0 + N(0, σ2)∣x + N(0, σ2)) ≤ 0L2	(10)
2σ2
if N(0, σ2) is a standard Gaussian noise. The first inequality comes from the fact that Dα(QkP) ≥
Dα(g(Q)kg(P)) for any function g.
12
Under review as a conference paper at ICLR 2019
Therefore, if we have
-log (1 - 2M1 (p(1),P(2)) + 2M1-α (p(1),P(2))) ≥ IL2
(11)
It implies
Dα(f (x0 + N(0, σ2))kf (x + N(0,σ2))) ≤ - log(1 - 2Mι (p(i),p(2)) + 2Mi-α(P(1),P(2)))
(12)
Then from Lemma 1 we know that the index of the maximums of f(x + N(0, σ2 )) and
f(x0 + N(0, σ2 )) must be the same, which means they have the same prediction, thus implies
robustness.	■
D Comparison between B ounds
We use simulation to show our proposed bound is higher than the one from PixelDP (Mathias et al.,
2018).
In PixelDP, the upper bound for the size of attacks is indirectly defined: if p(1)≥ e2p(2) + (1 + e)
and the added noise has the distribution N(0, σ2I), then the classifier is robust against attacks whose
'2 size is less than / σ ==.
2log(1.25∕δ)
As both bounds are determined by the models and data only through p(1) andp(2), it is sufficient to
compare them with simulation for different p(1) and p(2) as long as p(1) ≥ p(2) ≥ 0, p(1) + p(2) ≤ 1
and p(1) + p(2) ≥ 0.2 are satisfied.
For fixed σ, and δ are two tuning parameters that affect the result. For a fair comparison, we use a
grid search to find and δ that maximizes their bound.
Figure 4: The upper bounds under different p(1) and p(2). In both settings, we let the variance of
Gaussian noise σ2 = 1. Our bound (red) is strictly higher than the one from PixedDP(blue).
The simulation result shows our bound is strictly higher than the one from PixelDP.
E Comparis on to other Attack Methods
We first compare our attack method to well-known C&W attack (Carlini & Wagner, 2017b) on
MNIST in FIgure 5.
We then present experimental results from (Schott et al., 2018) for various existing attack methods
with the size of attacks `2 = 1.5, and compare them to our S-O attack in Table 2. All previous
13
Under review as a conference paper at ICLR 2019
q-8-6-42
LS0CiS
Ae-Jn□□4sφl
o.o-
6	2
4	6	8	10
Figure 5: S-O attack compared to C&W attack on MNIST under various `2 sizes.
methods, including FGSM (Kurakin et al., 2016), DeepFool (Moosavi-Dezfooli et al., 2016), and `2
version PGD (Madry et al., 2017) cannot significantly reduce the accuracy of Madry’s model.
The only attack method that achieves similar performance is Boundary Attack proposed by Schott
et al. (2018), and we recognize it as concurrent work. Boundary attack iteratively flips pixels between
the perturbed value and its original value to explore the minimum perturbation for altering the
classification results. Although it produces a small number of perturbations, one major issue of the
Boundary Attack is its computational inefficiency. It is hard to scale up to even CIFAR10.
Table 2: Different attack methods on Madry’s model.
	FGSM	DeePFool	PGD	Boundary Attack	S-O (ours)
Madry's Model	96%	91%	~88%^	37.0%	39.5%
F L∞ S-O ATTACK B REAKS ADVERSARIAL TRAINING VIA L2 ATTACKS
We apply L∞ S-O attack to Madry’s models based on L∞ and L2 attacks respectively. The accuracy
is plotted in Figure 6. It is clear that only the model trained against L∞ attack is robust to the L∞
attack. This result supports our claim that adversarial training overfits to the choice of norm.
L
Figure 6: Accuracy of adversarial training via L∞ and L2 attacks against L∞ S-O attack. The model
based on L2 is clearly vulnerable to L∞ attacks.
14
Under review as a conference paper at ICLR 2019
G	Adversarial Examples Illustration
We illustrate some randomly selected perturbed adversarial examples with size `2 = 2.0 in figure
7. One can observe that there are a limited amount of perturbations in the adversarial examples, yet
adversarially trained model gives false predictions for all examples.
5 9 7 5 T 7 6
5 9 7 rb 4 7 G
Figure 7: Above: Natural examples from MNIST. The correct labels are 5,9,7,3,4,9,6. Below:
Adversarial examples with perturbation size `2 = 2.0. The adversarially trained model predictions
are 3,4,2,8,9,4,5.
H Implementation Details
In this section, we specify the hyperparameters used in our experiments and other details. For all ex-
periments, the baseline models are implemented using the codes from <https://github.com/
MadryLab/mnist_challenge> and <https://github.com/MadryLab/cifar10_
challenge>.
Our defense model only requires the following modification: 1) For stability training, we remove the
adversarial training part and include the stability training regularizer. 2) At test time, we add i.i.d.
Gaussian noise to pixels before feeding the images into the model.
For MNIST, we use stability training with STD of the Gaussian noise being σtrain = 0.2. The STD of
Gaussian noise during testing is σtest = 0.2. For CIFAR-10, the STD of Gaussian noise in stability
training σtrai∏ =翳.The STD of Gaussian noise during testing is σtest =嘉. Weight of the
regularizor γ = 1 for both data sets.
15