Under review as a conference paper at ICLR 2019
ERGODIC MEASURE PRESERVING FLOWS
Anonymous authors
Paper under double-blind review
ABSTRACT
Training probabilistic models with neural network components is intractable in
most cases and requires to use approximations such as Markov chain Monte Carlo
(MCMC), which is not scalable and requires significant hyper-parameter tuning,
or mean-field variational inference (VI), which is biased. While there has been atï¿¾tempts at combining both approaches, the resulting methods have some important
limitations in theory and in practice. As an alternative, we propose a novel method
which is scalable, like mean-field VI, and, due to its theoretical foundation in erï¿¾godic theory, is also asymptotically accurate, like MCMC. We test our method on
popular benchmark problems with deep generative models and Bayesian neural
networks. Our results show that we can outperform existing approximate inferï¿¾ence methods.
1 INTRODUCTION
Approximate statistical inference with unnormalised density functions is fundamentally important
problem both Bayesian and frequentist inference. In particular, the successes of many sophistiï¿¾cated generative models in machine learning rely on power inference algorithms. Markov chain
Monte Carlo (MCMC) methods and variational inference (VI), originally developed in statistics and
physics, are two most important approximate inference methods in machine learning, which has
been widely used in all kinds of probabilistic models, like latent topic models (Blei et al., 2003),
Boltzmann machines (Hinton, 2002; Salakhutdinov & Larochelle, 2010), Bayesian non-parametric
models (Neal, 2000; Kurihara et al., 2007). However, they are facing great challenges in the recent
research on probabilistic modelling with deep neural networks (NNs). In particular, Bayesian deep
neural networks become popular in recent works, because it exploits the Bayesian framework to
overcome the overfitting and data demanding problems in deep learning (Neal, 2012). Another inï¿¾teresting research direction is to use deep neural networks in latent variable models to transform the
simple latent random variables into complex distribution, which is also known as to Deep Generative
Models (DGM). DGMs have been proved to be very powerful generative models.
Inspired by DGMs, many recent works on variational inference adopted NN to construct flexible
approximate distributions. In particular, variational autoencoders (Kingma & Welling, 2014) and
normalising flows (Rezende & Mohamed, 2015) are two most influential works in this direction.
However, due to lack of understanding of the convergence of specific NNs, the research of NNï¿¾based inference is focused on engineering the architecture of inference NNs based on experiments
and heuristics.
In this work, we propose a novel approximate inference method based on the classic inference theï¿¾ory of MCMC. Our method is inspired by the idea of parallel simulations of MCMC and the recent
advances in variational inference with NNs. Like these variational methods, it is straightforward to
accelerate the computation of our method using parallelised simulations on Graphical Processing
Units. More importantly, with solid theoretical foundations in the theory of MCMC, the proposed
method guarantees asymptotic convergence to arbitrary distributions of interest. It is a great adï¿¾vantage over variational inference, because of the approximation bias in variational methods. Our
method is also attractive to a wide range of probabilistic models without NNs and Bayesian NNs.
1
Under review as a conference paper at ICLR 2019
2 BACKGROUND
2.1 BAYESIAN NEURAL NETWORKS
Given data D = {xn, yn}Nn=1 formed by feature vectors xn and corresponding scalar targets yn, we
can assume that each yn is obtained as yn = f(xn; Î¸) +  n, where f(Â·; Î¸) is the output of a deep
neural network with weights Î¸ and the  n are independent noise variables with  n âˆ¼ N (0, Ïƒ2). This
model specifies a likelihood function p(y1, . . . , yn|x1, . . . , xn, Î¸) which can be combined with a
Gaussian prior on Î¸ to obtain a posterior distribution p(Î¸|D). Predictions for the y? corresponding
to a new feature vector x? are then obtained by using the predictive distribution p(y? |x? , D) =
R
p(y? |x? , Î¸)p(Î¸|D) dÎ¸. However, integrating with respect to the posterior distribution p(Î¸|D) is
intractable and approximations have to be performed in practice, with the most popular methods for
this being VI and MCMC.
2.2 DEEP GENERATIVE MODELS
Generative models extract intrinsic structure from data by making use of latent variables. Let D be a
dataset with n data points {xn}Nn=1. Given a latent representation z, the data point x is assumed to
be sampled from the conditional distribution pÎ¸(x|z), which is specified in terms of some parameters
Î¸. This conditional distribution is often refered to as the decoder. Given a prior distribution p(z)
over the latent variables, the joint distribution of data and latent variables is pÎ¸(x, z) = p(z)pÎ¸(x|z).
The marginal probability of data x under the model is then pÎ¸(x) = R p(z)pÎ¸(x|z)dz.
Until recenlty, pÎ¸(x|z) was typically specified using a simple distributional family, e.g. generalized
linear models (Murphy, 2012; Bishop, 2006). However, more recently, deep generative models
(DGMs) use deep neural networks with weights Î¸ to specify the decoder (Kingma & Welling, 2014;
Goodfellow et al., 2014).
Maximum likelihood is a straightforward way to train probabilistic models with latent variables.
However, the marginal likelihood pÎ¸(x) is intractable to compute in DGMs and approximations are
needed in practice. As before, the most popular methods for this are based on VI and MCMC. We
briefly describe these methods in the following sections.
2.3 VARIATIONAL INFERENCE AND NORMALIZING FLOWS
VI approximates a complex posterior distribution with another simpler parametric distribution which
is found by optimizing a lower bound on the marginal likelihood. Let the complex posterior be
pÎ¸(z|x), with associated marginal likelihood p(x), and let qÏ†(z), parameterized by Ï†, be a simpler
tractable distribution. The lower bound of the marginal likelihood is then defined as
log pÎ¸(x) â‰¥ EqÏ†
[log pÎ¸(x, z) âˆ’ log qÏ†(z)] , (1)
which is often known as the evidence lower bound (ELBO). The more flexible the parametric family
qÏ†, the better the approximation quality to the true posterior and the tighter the value of the ELBO.
Mean-field VI uses a form for qÏ†(z) which assumes independence between random variables. This
reduces the computational cost of the optimization problem but often leads to poor performance with
complex posterior distributions, such as the ones arising in DGMs or in Bayesian neural networks.
Amortization can be used to accelerate convergence and reduce computational cost when multiple
inference problems have to be solved simultaneously. The optimization of (1) can be amortized by
making qÏ† depend explicitly on the data x. In this case, qÏ†(z) is replaced with qÏ†(z|x), where Ï†
are now the weights of a neural network which computes the parameteres of a tractable parametric
distribuion on z from x. In this manner, for any new value of x, we can readily obtain a correspoding
variational approximation given by qÏ†(z|x).
Variational auto-encoders (VAEs) (Kingma & Welling, 2014) are DGMs trained by using mean-field
VI with a Gaussian parametric distribuion and amortization. Rezende & Mohamed (2015) improve
over this method by using a more flexible variational family called normalizing flows (NFs). The
NF family is obtained by applying L invertible non-linear transformations f1, . . . , fL to a random
variable z0 with tractable density q0(z0) and exact simulation. The resulting output is a random
2
Under review as a conference paper at ICLR 2019
variable zL = fL â—¦ Â· Â· Â· â—¦ f1(z0) with density qÏ†(zL|x) = q0(z0|x) Q Ll=1 |âˆ‚fl(zlâˆ’1)/âˆ‚zlâˆ’1|âˆ’1
and
with Ï† being the parameters of f1, . . . , fL.
Stochastic gradient descent (SGD), in combination with the reparameterisation trick (Kingma &
Welling, 2014), can be used for the scalable optimization of the ELBO in VAEs and NFs. However,
the main limitation of VAEs and NFs is the bias present in their variational approximations. This
bias can be quite high, even in the case of NFs, since the transformations f1, . . . , fL have to be
rather simple to ensure invertibility and to reduce computational costs.
2.4 MARKOV CHAIN MONTE CARLO
Markov chain Monte Carlo (MCMC) is an approximate inference method which does not have the
aforementioned bias problem. MCMC works by simulating a stationary Markov chain that generates
asymptotically unbiased samples from the distributions of interest. Formally, a Markov chain is a
sequence of random variables z0, z1, . . . in which the transition from state zl
to zl+1 is defined by
the conditional probability distribution of zl+1 given zl
, denoted by K(zl, zl+1). Markov chains in
MCMC methods have the following strong stationary property: if one state zl of the chain follows
the stationary distribution Ï€, so does the next state zl+1. In particular,
Ï€(zl+1) = Z Ï€(zl)K(zl, zl+1) dzl . (2)
If zl follows a distribution Ï€l
that is different from the stationary one, then the distribution of zl+1
Ï€l+1(zl+1) = Z Ï€l(zl)K(zl, zl+1) dzl , (3)
is guaranteed to be closer to Ï€ than Ï€l
. This property implies that, with sufficiently many transitions,
the distribution Ï€l of zl converges to Ï€ irrespectively of the distribution the initial state z0.
Despite beign asymptotically unbiased, MCMC methods are less popular than VI for two reasons.
First, they are computationally more expensive and second, they typically include hyper-parameters
in the Markov kernel K which are highly problem dependent and are hard to tune in practice.
3 ERGODIC MEASURE PRESERVING FLOWS
In this section, we describe an inference method that combines the strengths of MCMC and VI and
avoids their drawbacks. The idea is to use the output distribution of a MCMC chain, given by (3), as
the variational distribution and optimize a simple to evaluate objective function for tuning MCMC
parameters. Since MCMC converges to the target asymptotically, our variational approximation can
be arbitrarily accurate. The result is a computationally efficient method which avoids the bias of
parametric approximations and which can do automatic tuning of hyper-parameters.
3.1 DEFINITIONS
Given the target distribution Ï€ with unnormalised density function Ï€âˆ—
, we define an approximate
distribution q by a mixture of sequential deterministic transformations that preserves the measure
Ï€âˆ—
. We call such approximate distributions measure preserving flows (MPFs). The transformations
preserving a given measure Ï€âˆ—
are formally defined as follows (Billingsley, 1986).
Definition 3.1. Measure Preserving Transformation (MPT). Let (â„¦, F, P) be a probability space
and Âµ be a consistent measure with P. A mapping T : â„¦ â†’ â„¦ is a measure preserving transformaï¿¾tion if T is measurable in both the input filed F and the output field F and Âµ(A) = Âµ(T âˆ’1(A)) for
all A âˆˆ F. If T is a one-to-one mapping onto â„¦, then T preserves Âµ: Âµ(A) = Âµ(T âˆ’1T A) = Âµ(T A).
An example of MPT is any transformation with Jacobian determinant equal to 1, which preserves
the Lebesgue measure. In practice, it is straightforward to verify if a transformation T preserves the
measure with density function Ï€âˆ— with the following conditions:
(i) Bijection: T is invertible,
(ii) Preservation of density function: Ï€âˆ—(z) = Ï€âˆ—(T(z)) for all z. 3
Under review as a conference paper at ICLR 2019
(iii) Preservation of base measure: the Jacobian determinant is one if the Lebesgue measure is
the base measure.
In probability theory, MPTs are often used within the area of ergodic stochastic processes since many
of these processes can be reformulated as a composition of MPTs. In particular, MCMC kernels are
MPTs and stationary Markov chains in MCMC are ergodic processes (Robert & Casella, 2005).
The joint probability of states in a MCMC chain is q(z0, z1, . . . , zL) = q0(z0) Q Ll=1 K(zlâˆ’1, zl),
where q0 is the distribution of the initial state z0. The density of the last state zL is then obtained by
integrating out all the previous states:
qL(zL) = Z q0(z0) LYl=1
K(zlâˆ’1, zl) dz0 dz1 . . . dzLâˆ’1 . (4)
If the Markov chain is ergodic, qL converges to the stationary distribution Ï€ in total variation distance
as the length L of the chain increases (Robert & Casella, 2005).
We define a measure preserving flow (MPF) as a representation of (4) in which the kernel K becomes
a deterministic transformation Tr : Z â†’ Z with stochastic auxiliary input r following distribution
Âµ. We can then define zL as the result of applying these deterministic transformations to z0, that is,
zL = TrL
â—¦ Â· Â· Â· â—¦ Tr1 (z0). By following the rule of changing variables, it is then straightforward to
derive the density of zL as
qL(zL) = Z q(zL, r1:L)dr1:L = Z q(z0)Âµ(r1:L)Î´(zL âˆ’ TrL
â—¦ Â· Â· Â· â—¦ Tr1 (z0)) dz0 dr1:L , (5)
where Î´ denotes the Dirac delta function. Note that there is no Jacobian term in (5) because of the
preservation of the Lebesgue measure. Because MPFs are equivalent to ergodic Markov chains, the
density obtained at the output of an MPF, that is, qL, will converge to the stationary distribution Ï€
as L increases.
Hamiltonian Monte Carlo (HMC) is one of the most successful MCMC methods, which also can be
interpreted as an MPF. Given the target random variable z âˆˆ Rd with unnormalised density function
Ï€âˆ—
, the HMC kernel is essentially applying a deterministic transformation H to the previous state zi
with an auxiliary random variable r âˆˆ Rn with the density function Âµ(Â·). The transformation H is
given by the solution of Hamiltonian dynamics
Ë™z(t) = âˆ‚rK(r), Ë™r(t) = âˆ’âˆ‚zU(z), (6)
where Ë™z denotes the derivative of z w.r.t. the time of the dynamics t, U(z) = âˆ’ log Ï€âˆ—(z) and
K(r) = âˆ’ log Âµ(r). The preservation of total Hamiltonian energy, H(z, r) = U(z) + K(r), is the
characteristic property of Hamiltonian dynamics, which can be easily verified by Ë™H(z, r) = 0 using
(6). It is straightforward to see that H preserves the joint measure Ï€(z)Âµ(r). In particular, H is a
bijective transformation because the dynamics are deterministic and time reversible (Neal, 2010) and
the preservation of the Hamiltonian energy implies the preservation of density. Finally, the volume
preservation of H in the space of (z, r) is a well known property of Hamiltonian dynamics, which
can be proved by Liouvilleâ€™s theorem (Leimkuhler & Reich, 2004; Neal, 2010).
We can write the marginal distribution of the last sample generated by HMC as an MPF:
q(zL) = Z q(zL, r1:L)dr1:L = Z q(z0)Âµ(r)Î´(zL âˆ’ HrL
â—¦ Â· Â· Â· â—¦ Hr1 (z0)) dz0 dr1:L . (7)
We call the MPF generated by Hamiltonian dynamics a Hamiltonian MPF (HMPF).
3.2 UNDERSTANDING MEASURE PRESERVING CONDITIONS
We would like to address a common misunderstanding on the preservation of volume condition
stated by (iii) in Section 3.1. Note that we are interested in sampling the random variable z, but the
Hamiltonian dynamics preserve the joint measure Ï€(z, r) rather than Ï€(z). Following the conditions
of MPTs in Section 3.1, it seems necessary to show that, for any specific value of r, any Tr used
within an MPF should preserve volume in z space. However, this is not the case since the measure
preservation conditions in the augmented space (z, r) are enough to guarantee the preservation of
the marginal distribution in z space. Formally, we have the following proposition:
4
Under review as a conference paper at ICLR 2019
Proposition 1. Let T : Z Ã— E â†’ Z Ã— E preserve the distribution Ï€(z, r). Then, if r is sampled from
Ï€(r) = R Ï€(z, r) dr, the marginal distribution
Ï€(z) = Z Ï€(z, r) dr
is also preserved by the projection of T in the space of z, that is, by Tr : Z â†’ Z.
Proposition 1 gives us some insights on the difference between MPFs and normalising flows (NFs).
As mentioned earlier, NFs also use a sequence of transformations Tr : Z â†’ Z. However, these
do not preserve the distribution of z and, consequently, they require the computation of Jacobian
determinants by the rule of changing variables. By contrast, Proposition 1 implies that, in MPFs, Tr
preserves the marginal Ï€(z) if T preserves the joint, which means that there is no need to include
any Jacobian computations. For this reason, the transformations used in MPFs can be much more
complicated than those used in NFs. For example, in Hamiltonian MPFs, for a given r, the Jacobian
of Hr can be very complicated.
3.3 VARIATIONAL INFERENCE WITH MPFS
Given an unormalized posterior distribution pÎ¸(x, z) for z, we can construct an MPF that preserves
pÎ¸(x, z)Âµ(r), where Âµ(r) is a simple distribution with tractable density and sampling algorithm.
Let T Ï†l be the l-th transformation in the flow, where Ï†l are hyper-parameters. This transformation
maps the state of the flow from (zlâˆ’1, rl) to (zl, r0l
) = T Ï†l (zlâˆ’1, rl). Similarly, the composition
T Ï†L â—¦ Â· Â· Â· â—¦ T Ï†1 (z0, r1:L), which we denote by T Ï†, transforms (z0, r1:L) to (zL, r01:L). By the
preservation of density and the preservation of Lebesgue measure of T Ï†, as given by conditions (ii)
and (iii) in Section 3.1), we have the following equalities
pÎ¸(x, z0) LYl=1
Âµ(rl) = pÎ¸(x, z1)Âµ(r01) LYl=2
Âµ(rl) = Â· Â· Â· = pÎ¸(x, zL) LYl=1
Âµ(r0l), (8)
q0(z0) LYl=1
Âµ(rl) = qL(z1, r01; Ï†1) LYl=2
Âµ(rl) = Â· Â· Â· = qL(zL, r01, r02
. . . , r0L; Ï†), (9)
where q0(z0) is an initial proposal distribution and Ï† = (Ï†1, . . . , Ï†L) are the transformation hyperï¿¾parameters. It is important to clarify that, according to (9), the joint density of zL, r01, r02
, . . . , r0L
is
known, but the marginal density for these variables is intractable to compute in general.
Following (1), we can obtain the ELBO for the initial proposal distribution q0(z0) as
L(x; Î¸) = Z log pÎ¸(x, z0) q0(z0) q0(z0) dz0 . (10)
We call this expression the simple ELBO. We can then multiplying by the density of the auxiliary
variables Âµ(r1:L) = Q Ll=1 Âµ(rl) to obtain
L(x; Î¸) = Z log pÎ¸(x, z0) Q Ll=1 Âµ(rl) q0(z0) Q Ll=1 Âµ(rl) q0(z0) LYl=1
Âµ(rl) dz0 dr1:L . (11)
We can then replace (z0, r1:L) with (zL, r01:L) in (11) by making use of using transformation T Ï†,
(8) and (9). The result is
L(x; Î¸, Ï†) = Z log pÎ¸(x, zL) Q Ll=1 Âµ(r0l) qL(zL, r01:L; Ï†) qL(zL, r01:L; Ï†) dzL dr01:L , (12)
where we have omitted the dependence of (zL, r01:L) on Ï†, since these variables are determined by
the hyper-parameters of the MPTs. We call (12) the reparameterised ELBO.
3.4 ERGODIC LOWER BOUND AND ERGODIC INFERENCE
The reparameterised ELBO is of limited use, because it can only be as tight as the ELBO with initial
proposal distribution q0. This seems to erase the benefits of using an ergodic MPF, which we know
5
Under review as a conference paper at ICLR 2019
will converge to the target posterior distribution given a sufficiently long flow. To overcome the
drawback of the reparameterised ELBO, we propose another ELBO tailored to the MPF framework,
which becomes arbitrarily tight as the length of the flow grows. We call such an ELBO ergodic
lower bound (ERLBO).
To derive ERLBO, we first rewrite (12) as
L(x; Î¸, Ï†) = Z log pÎ¸(x, zL) qL(zL; Ï†) qL(zL; Ï†) dzL + Z log
Q
Ll=1 Âµ(r0l) qL(r01:L|zL; Ï†) qL(zL, r01:L; Ï†) dzL dr01:L = Z log pÎ¸(x, zL) qL(zL; Ï†) qL(zL; Ï†) dzL âˆ’ DL
KL . (13)
where DL
KL is the Kullback-Liebler divergence between qL(zL, r01:L; Ï†) and qL(zL; Ï†) Q Ll=1 Âµ(r0l).
It is straightforward to show that the first term on the RHS in (13) is a lower bound of the marginal
likelihood by Jensenâ€™s inequality. This leads to the ERLBO given by
LËœ(x; Î¸, Ï†) = Z log pÎ¸(x, zL) qL(zL; Ï†) qL(zL; Ï†) dzL = L(x; Î¸, Ï†) + DL
KL . (14)
This is a tighter lower bound than the simple ELBO because the difference between LËœ(x; Î¸, Ï†) and
L(x; Î¸) in (10) is DL
KL â‰¥ 0. Moreover, the ERLBO can be shown to monotonically increase w.r.t. L.
Proposition 2. The lower bound LËœ(x; Î¸, Ï†) in (14) becomes tighter and tighter as L increases, that
is, LËœ(x; Î¸, Ï†1:L) â‰¥ LËœ(x; Î¸, Ï†1:Lâˆ’1) and the equality holds if and only if DL
KL = 0.
The complete proof is included in appendix.
Recall that qL(zL; Ï†) is obtained by a sequence of transformations that preserve the probability
measure p(z|x). It is well-known that MCMC chains have a unique invariant distribution and so do
the MCMC-equivalent MPFs. Therefore, we know that if LËœ(x; Î¸, Ï†1:L) stops growing, qL(zL; Ï†)
must converge to p(z|x). This is formally described by the following theorem.
Theorem 1. Given an ergodic measure preserving flow with invariant measure Ï€, the ergodic lower
bound LËœ(x; Î¸, Ï†) increases in the length of the flow L and becomes an unbiased estimator of the
marginal log p(x) as L increases to infinity.
We could tune Ï† by optimizing the ERLBO. To better understand the values of Ï† that would be
favored by this optimisation process, we can rewrite the ERLBO by making explicit its dependence
on the entropy of qL(zL; Ï†), which we denote by H[qL(zL; Ï†)] = âˆ’ R log qL(zL; Ï†)qL(zL; Ï†) dzL.
In particular,
LËœ(x; Î¸, Ï†) = EqL(zL;Ï†)
[log pÎ¸(x, zL)] + H[qL(zL; Ï†)] . (15)
When optimizing this quantity w.r.t. Ï†, the first term in the RHS will encourage qL(zL; Ï†) to have
high density in regions where pÎ¸(x, zL) is high, while the second term will favor high entropy
solutions and will prevent qL(zL; Ï†) from converging to a Dirac delta centered at the maximizer
of log pÎ¸(x, zL). Note that the first term in the RHS of (15) can be easily approximated by Monte
Carlo, while the second term is intractable because qL(zL; Ï†) is not available. However, since
qL(zL; Ï†) converges to pÎ¸(z|x) as L increases, we expect the effect of H[qL(zL; Ï†)] on Ï† to be
small and that most of the similarity of qL(zL; Ï†) to pÎ¸(z|x) will be captured by the first term in
the RHS of (15). Therefore, we propose to tune Ï† by optimizing the tractable objective given by the
first term, that is,
FËœ(x; Î¸, Ï†) = EqL(zL;Ï†)
[log pÎ¸(x, zL)] . (16)
If with the initial flow parameter Ï†0, the objective FËœ < EpÎ¸(x,z)
[log pÎ¸(x, z)], we expect optimising
FËœ produces faster convergence of qL(zL; Ï†) towards pÎ¸(x, z). Importantly, the model parameters Î¸
can also be adjusted by optimizing FËœ because the omitted H[qL(zL; Ï†)] does not depend on Î¸ and,
consequently, optimizing FËœ(x; Î¸, Ï†) and LËœ(x; Î¸, Ï†) w.r.t. Î¸ are equivalent operations.
3.5 IMPLEMENTATION OF HMPFS
The hyper-parameters to be tuned in a HMPF include the parameters of q0 and the transformation
parameters for the Hamiltonian simulation, that is, Ï† = (Ï†1, . . . , Ï†L). A natural choice for q0 6
Under review as a conference paper at ICLR 2019
is multivariate Gaussian with mean Âµ = (Âµ1 . . . , Âµd) and diagonal covariance matrix with entries
Ïƒ2 = (Ïƒ21
, . . . , Ïƒ2d), where d is the dimensionality of the sample space. The most popular algorithm
for simulating Hamiltonian dynamics is the vanilla Leapfrog integrator. We refer to the tutorial of
Neal (2010) for more detailed description of the implementation of this algorithm. Leapfrog is a
numeric integrator that approximates the Hamiltonian dynamics (6) by an iterative procedure with
discretized time âˆ†t, that is,
x(t + âˆ†t) = x(t) + âˆ†tâˆ‚rK(r(t)), r(t + âˆ†t) = r(t) âˆ’ âˆ†tâˆ‚xU(x(t)). (17)
For the flow parameters, we consider the total simulation time T. Given a fixed number of Leapfrog
iterations m, the simulation time T can be reparameterized as the time step size âˆ†t = T /m. Neal
(2010) shows that it is possible to use different âˆ†t for each dimension of the sample space to improve
the quality of Leapfrog. Therefore, we consider the parameters of the l-th Hamiltonian simulation
in the flow to be Ï†l = (âˆ†tl,1, . . . , âˆ†tl,d). The pseudo code for ergodic inference with HMPFs is
shown in Algorithm 1.
Algorithm 1: Ergodic Inference on Hamiltonian Measure Preserving Flow.
input : potential function U(z; x, Î¸), dataset D and large L
output: optimal decoder and flow parameters Î¸âˆ—
and Ï†âˆ—
initialize Î¸ and Ï†;
while not converged do
x â†âˆ’ sample one data point from D; z0 âˆ¼ N (Âµ, diag(Ïƒ2
)); /* Start of simulation of HMPF */
for l = 1, . . . , L do
r âˆ¼ N (0, 1); zl â†âˆ’ H(zlâˆ’1, r;U(z; x, Î¸), Ï†l); /* Leapfrog simulation */
end
/* End of simulation of HMPF */
obj â†âˆ’ U(zL; x, Î¸); /* one sample Monte Carlo Approx. of FËœ(x; Î¸, Ï†) */ Î¸ â†âˆ’ AdamUpdate(Î¸, âˆ‚Î¸obj); Ï† â†âˆ’ AdamUpdate(Ï†, âˆ‚Ï†obj);
end
We do not include any Metropolis-Hastings (MH) correction steps in our method since this is not
necessary. The MH steps are included in MCMC methods to ensure asymptotic convergence to the
correct target with an unlimited number of transitions. By contrast, MPFs are in the finite-length
regime and the main concern is to accelerate the convergence of MPFs to be as close as possible to
the target measure. In this setting, it is not immediately clear that MH steps would be helpful. In
particular, in Section 4.1 we provide empirical evidence of how HMPFs can converge to the correct
target without MH steps.
Due to the composition of the MPTs, computing the gradient can be expensive when the flow is long.
To speed up training, we stop the gradient computations when evaluating âˆ’âˆ‚zU(z) in the Leapfrog
steps. This trick leads to incorrect gradients. However, we noticed that the optimization was not
significantlly affected by this and still worked very well in practice. Finally, note that working
with incorrect gradients does not affect the convergence of the flow to the correct target distribution
because that is guaranteed by the convergence of the ERLBO as we mentioned in previous section.
4 EXPERIMENTS
We provide empirical evidence of HMPFs in three inference tasks. Our goal is to show that HMPFs
can provide better approximations than other approximate inference methods.
4.1 DEMONSTRATION OF CONVERGENCE
To verify the theoretical results on the convergence of MPFs in Section 3.4, we test HMPFs on
8 bivariate distributions. The full list of benchmark distributions and results are included in the
7
Under review as a conference paper at ICLR 2019
appendix. Here, we focus on two multimodal benchmarks. The first testing target distribution is a
bimodal moon shaped distribution as shown in Figure 2a. We call this target dual moon. Dual moon
is one of the benchmarks in normalising flows (Rezende et al., 2014). The second testing target is
a mixture of 6 Gaussian distributions placed in a circle. We use 15 Hamiltonian transformations
with 5 Leapfrog steps each. The architecture detail of HMPFs 1
can be found in Section D.1. The
initial state of MPFs is sampled from a standard Gaussian distribution. The gradient of the objective
function is estimated using 1000 samples from HMPFs.
To illustrate the convergence of HMPFs to the target distribution, figures 1c and 1d show histograms
of samples as a function of the flow length and the training iterations. To confirm the convergence
numerically, we compute the ERLBO using a numeric method for the estimation of the entropy.
Plots for the ERLBO and the ground truth log normalization constants are show in figures 1a and 1b
4.2 DEEP GENERATIVE MODELS
MNIST is a standard benchmark for testing approximate inference algorithms for training deep
generative models. This dataset contains 60,000 grey level 28Ã—28 images of handwritten digits. For
fair comparison with previous work, we use the 10,000 prebinarised MNIST test images from (Burda
et al., 2015)2
. Our benchmark deep generative model is based on the deconvolutional network
used by Salimans et al. (2015) for testing Hamiltonian variational inference (HVI). In particular,
the decoder pÎ¸(x, z) consists of 32 dimensional latent variables z with isotropic Gaussian prior
p(z) = N (0, I) and a deconvolutional network with the architecture from top to bottom including
a single fully-connected layer with 500 RELU hidden units, then three deconvolutional layers with
5 Ã— 5 filters, [16, 32, 32] feature maps and RELU activations and the final output layer is simply
element-wise logistic activation functions. In the convolutional VAE, the encoder network mirrors
the architecture of the decoder.
The code for HVI (Salimans et al., 2015) is not publicly available. Nevertheless, we reimplemented
their convolutional VAE and were able to reproduce the marginal likelihood reported by Salimans
et al. (2015), as shown in Table 1. This verifies that our implementation of the convolutional VAE is
correct and that our results are comparable to the ones reported originally by Salimans et al. (2015).
We also implemented HVI following Salimans et al. (2015). We used a single hidden layer network
with 640 hidden units and RELU activations as the reverse model for the HMC transitions. We
also implemented another VI method similar to HVI and called the Hamiltonian variational encoder
(HVAE) (Caterini et al., 2018). Unlike HVI, HVAE does not use a reverse model. This method
optimizes instead a bound derived from the stationary distribution of reverse momenta. Futhermore,
HVAE uses tempering Hamiltonian dynamics that requires additional Jacobian corrections. In our
implementation of HVAE, we simply ignore the temperature for computational efficiency.
Encoders Training hours Training Epochs Test log(x) ESS
Conv VAE(nh=300) (Salimans et al., 2015) - - -83.20 -
HVI(1HMPF-16LF, nh=800) (Salimans et al., 2015) - - -81.94 -
HVAE(1HMPF-20LF, nh = 300)(Caterini et al., 2018) - - -84.78 -
Conv VAE(nh=500) (Baseline) 6.00 3000 -83.57 50
HVI(1HMPF-16LF, nh=800) 6.00 360 -83.68 48
HVAE(1HMPF-16LF, nh=500) 6.00 360 -84.22 48
HMPF(30HMPT-5LF, nh=500, no encoder network) 1.65 54 -83.17 48
HMPF(30HMPT-5LF, nh=500, no encoder network) 3.00 100 -82.76 46
HMPF(30HMPT-5LF, nh=500, no encoder network) 6.00 200 -82.65 45
HMPF(30HMPT-5LF, nh=500, no encoder network) 12.00 400 -81.43 38
Table 1: Comparison in terms of compuational efficiency and approximate test log-likelihood. For
fair comparison, we implemented the deconvolutional decoder network in (Salimans et al., 2015)
to test HVI. In (Salimans et al., 2015), the test likelihood is estimated using importence-weighted
samples from the encoder network. In our experiment, we use a more reliable estimation method
based on Hamiltonian annealled importance sampling and report the effective sample size (ESS).
1The code of HMPFs for all three experiments will be available at https://github.com/
firstauthor/hmpfs
2
https://github.com/yburda/iwae
8
Under review as a conference paper at ICLR 2019
(a) Dual Moon (b) Circular Gaussian Mixture
Flow direction (the order of HMPTs)
0
10
20
30
40
50
60
70
80
T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T14 T1 T2
(c) Dual moon Mixture
Flow direction (the order of HMPTs)
0
10
20
30
40
50
60
70
80
T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T14 T1 T2
(d) Circular Gaussian Mixture
Figure 1: The demonstration of the convergence of measure preserving flows. Figure (a) and (b)
show ergodic lower bounds to the true log normalising constant of ergodic measure preserving flows
with 14 transformations. The lower bound is estimated after each transformation as indicated by
the axis â€™flow lengthâ€™. The legend â€™ERLBO-0â€™ indicates the ERLBO of the flow with the initial
randomized flow parameters and the legend â€™ERLBO-80â€™ indicates the ERLBO of the flow after 80
training iterations of the flow parameters. Figure (c) and (d) show how the histograms of the 50000
samples from the flow evolve after each transformation (flow direction axis) and every 10 training
iterations (training iterations axis).
9
Training iterations
Under review as a conference paper at ICLR 2019
For HMPF encoder, we use 30 HMPTs with 5 Leapfrog steps per HMPT. The initial distribution q0
is 32 dimensional independent Gaussian. More detailed description of the architecture of HMPFs
is in the appendix Section D.1. We optimise the HMPF encoder and the decoder jointly using
Adam. The the initial state of the flow is sampled from independent Gaussian. The mean and
variance of the initial Gaussian is also optimised jointly with flow and model parameters and their
gradients are computed by back propagation given the value of momenta. However, we noticed
that with sufficient number of transformations, the effect of optimising initial Gaussian distribution
is not significant. Table 1 shows the marginal likelihood of HMPFs and other methods estimated
using 100 Hamiltonian annealled importance samples (HAIS) (Wu et al., 2017). We also report the
effective sample size (ESS). Overall, HMPFs produce better results and are faster than the baselines.
We also tested the same decoder with HMPFs and convolutional encoder on dynamically binarised
Fashion-MNIST (Xiao et al., 2017). The results of test marginal likelihood can be found Table 2.
Encoders Training hours Training Epochs Test log(x) ESS
Conv VAE(nh=500) 6.00 3000 -104.90 26.3
HMPF(30HMPT-5LF, nh=500, no encoder) 6.00 200 -103.087 16.2
Table 2: The comparison of log marginal likelihood on fashion MNIST between convolutional VAE
and HMPFs. We also evaluate HMPFs with different setting of HAIS that gives higher effective
sample size (ESS), but the result of test log likelihood is roughly the same.
4.3 BAYESIAN NEURAL NETWORKS
In our final experiment we approximate the posterior distribution of Bayesian neural networks. We
use four UCI datasets and compare HMPFs with relevant stochastic gradient Hamilton Monte Carlo
(SGHMC) methods from (Springenberg et al., 2016). The networks used in these experiments have
50 hidden layers and 1 real valued output unit, as stated in (Springenberg et al., 2016). The HMPFs
contain 50 HMC transformation with 3 Leapfrog steps each. The distribution the initial state of the
flow is independent Gaussian with mean and variance parameters obtained by fitting a variational
Gaussian proposal q0 with Adam optimiser for 200 iterations. To reduce the cost of the Leapfrog
iterations, we split training data into 19 mini-batches and only use one random sampled mini-batch
for computing the gradient of the potential energy. We train our HMPFs for 10 epochs and the
stationary distribution of the flow is chosen as approximate posterior on a random sampled miniï¿¾batch. The resulting test log-likelihoods are shown in Table 3. Overall, HMPFs produce significantly
better results than SGHMC.
Method/Dataset Boston Yacht Concrete Wine
SGHMC (best average) (Springenberg et al., 2016) -3.47Â±0.51 -13.58Â±0.98 -4.87Â±0.05 -1.82Â±0.75
SGHMC (tuned per dataset) (Springenberg et al., 2016) -2.49Â±0.15 -1.75Â±0.19 -4.16Â±0.72 -1.29Â±0.28
SGHMC (scale-adapted) (Springenberg et al., 2016) -2.54Â±0.04 -1.11Â±0.08 -3.38Â±0.24 -1.04Â±0.17
HMPFs -2.17Â±0.07 -0.47Â±0.06 -2.71Â±0.03 -0.71Â±0.03
Table 3: The test log-likelihood of Bayesian neural networks on UCI datasets averaged over 20 splits
with 100 sampled network parameters from HMPFs.
5 SUMMARY
We have proposed a novel method for approximate inference that combines advantages of variational
inference and MCMC methods. We call this method ergodic measure preserving flows (EMPFs).
Different from most previous works combining HMC and variational inference, EMPFs enjoy the
same asymptotic convergence as HMC and can tune sampling hyper-parameters by optimizing a
tractable objective function at a low computational cost. We have shown that EMPFs achieve better
results than existing baselines on standard benchmarks. For future work, it will be interesting to
study the convergence rate of EMPFs to the target distribution with increasing flow length. Finally,
the proposed method can be easily extended to consider recent Riemannian-manifold HMC methods
(Zhang & Sutton, 2014; Girolami & Calderhead, 2011) for the construction of the flow.
10
Under review as a conference paper at ICLR 2019
REFERENCES
Patrick Billingsley. Probability and Measure. John Wiley and Sons, third edition, 1986.
Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics).
Springer-Verlag, Berlin, Heidelberg, 2006. ISBN 0387310738.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning
research, 3(Jan):993â€“1022, 2003.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. arXiv preprint
arXiv:1509.00519, 2015.
A. L. Caterini, A. Doucet, and D. Sejdinovic. Hamiltonian Variational Auto-Encoder. ArXiv e-prints, May
2018.
Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123â€“214, 2011. ISSN
1467-9868. doi: 10.1111/j.1467-9868.2010.00765.x. URL http://dx.doi.org/10.1111/j.
1467-9868.2010.00765.x.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in neural information processing
systems, pp. 2672â€“2680, 2014.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation,
14(8):1771â€“1800, 2002.
Diederik Kingma and Max Welling. Auto-Encoding Variational Bayes. In The International Conference on
Learning Representations (ICLR), 2014.
Kenichi Kurihara, Max Welling, and Yee Whye Teh. Collapsed variational dirichlet process mixture models.
In IJCAI, volume 7, pp. 2796â€“2801, 2007.
Benedict Leimkuhler and Sebastian Reich. Simulating hamiltonian dynamics, volume 14. Cambridge university
press, 2004.
Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012. ISBN 0262018020,
9780262018029.
Radford M. Neal. Markov chain sampling methods for dirichlet process mixture models. Journal of Computaï¿¾tional and Graphical Statistics, 9(2):249â€“265, 2000.
Radford M. Neal. MCMC using Hamiltonian Dynamics. 2010.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media,
2012.
Danilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. In Proceedï¿¾ings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37,
ICMLâ€™15, pp. 1530â€“1538, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Christian P. Robert and George Casella. Monte Carlo Statistical Methods (Springer Texts in Statistics).
Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2005. ISBN 0387212396.
Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann machines. In Proceedings of
the thirteenth international conference on artificial intelligence and statistics, pp. 693â€“700, 2010.
Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and Variational Inference:
Bridging the Gap. In International Conference on Machine Learning, pp. 1218â€“1226, 2015.
Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian Optimizaï¿¾tion with Robust Bayesian Neural Networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29,
pp. 4134â€“4142. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6117-bayesian-optimization-with-robust-bayesian-neural-networks.pdf.
11
Under review as a conference paper at ICLR 2019
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Grosse Roger. On the Quantitative Analysis of Deep Belief
Networksnalysis of Decoder-based Generative Models. In ICLR. 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms, 2017.
Yichuan Zhang and Charles Sutton. Semi-separable hamiltonian monte carlo for inference in bayesian hierï¿¾archical models. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 27. Curran Associates, Inc., 2014.
12
Under review as a conference paper at ICLR 2019
A THE PROOF OF PROPOSITION 1
Proof. Because T preserves the probability measure Ï€(x, ), then for any measurable set A in Borel
set Bx Ã— B , we have
Ï€(A) = Ï€(T âˆ’1A).
Given a set B âˆˆ Bx, the set Ax generated by B Ã— E, is measurable under Ï€ with the measure
Ï€(Ax) = Z Ï€(B, )d,
that is essentially a probability measure in the space of x, also known as the marginal probability
Ï€(B) = Ï€(Ax).
Because T preserves the joint measure, applying T on Ax gives
T Ax = T(B, E) = T B Ã— E,
where T B denotes the projection of T Ax in the space of x. Follow the definition of measure
preserving transformations, we have
Ï€(Ax) = Z Ï€(B, )d = Z Ï€(T âˆ’1B, )d = Ï€(T âˆ’1Ax),
where T âˆ’1 denotes the preimage of B under T . Because Ï€(Ax) is essentially the marginal probaï¿¾bility Ï€(B), B âˆˆ Bx, we know the marginal distribution Ï€(x) is preserved by T because
Ï€(B) = Ï€(Ax) = Ï€(T âˆ’1Ax) = Ï€(T âˆ’1B).
This implies that the marginal distribution Ï€(x) is preserved by the stochastic mapping
T : X â†’ X,  âˆ¼ Ï€( ),
where Ï€( ) = R Ï€(x, )dx. If T is invertible for any  , we have
Ï€(B) = Z Ï€(B, )d = Z Ï€(T B, )d = Ï€(T B),
where T B denotes the projection of T Ax in the space of x. Therefore, we know that if we sample

from Ï€(x, ) and apply T on x, then the marginal distribution Ï€(x) is preserved.
B THE PROOF OF PROPOSITION 2
Proof. Let Âµ be the probability measure of auxiliary variable R. Following equation 14, the differï¿¾ence of ERLBO in each measure preserving transformation TL is given by
dLz(x; Î¸, Ï†1:L) = Lz(x; Î¸, Ï†1:L) âˆ’ Lz(x; Î¸, Ï†1:Lâˆ’1) = DKL  QL    Ë†QL âˆ’ DKL  QLâˆ’1    Ë†QLâˆ’1 , (18)
where QL denotes QL(ZL, R01:L) and Ë†QL denotes QL(ZL) Q Ll=1 Âµ(R0l). The KL divergence can
be written as the integral
DKL  QL    Ë†QL = Z log qL(zL, r01:L; Ï†) qL(zL; Ï†) Q Ll=1 Âµ(r0l(Ï†))
qL(zL, r01:L; Ï†)dzLdr01:L. (19)
Because (zL, r0L)is generated from (zLâˆ’1, rL) by the deterministic transformation TL that preserves
Lebesgue measure in the phase space, we have the equality
qL(zL, r01:L) = qLâˆ’1(zLâˆ’1, r01:Lâˆ’1)Âµ(rL).
Using the reparameterisation
(zL, r0L) = T(zLâˆ’1, rL),
13
Under review as a conference paper at ICLR 2019
we can rewrite the second KL term in equation 18 as
DKL  QLâˆ’1    Ë†QLâˆ’1 = Z log
qLâˆ’1(zLâˆ’1, r01:Lâˆ’1; Ï†) qLâˆ’1(zLâˆ’1; Ï†) Q Lâˆ’1 l=1 Âµ(r0l(Ï†))
qLâˆ’1(zLâˆ’1, r01:Lâˆ’1; Ï†)dzLâˆ’1dr01:Lâˆ’1 = Z log
qLâˆ’1(zLâˆ’1, r01:Lâˆ’1; Ï†)Âµ(rL) qLâˆ’1(zLâˆ’1; Ï†)Âµ(rL) Q Lâˆ’1 l=1 Âµ(r0l(Ï†))
qLâˆ’1(zLâˆ’1, r01:Lâˆ’1; Ï†)Âµ(rL)dzLâˆ’1dr01:Lâˆ’1drL = Z log qL(zL, r01:L; Ï†) qL(zL, r0L; Ï†) Q Lâˆ’1 l=1 Âµ(r0l(Ï†))
qL(zL, r01:L; Ï†)dzLdr01:L, (20)
where qL(zL, r0L; Ï†) comes from qLâˆ’1(zLâˆ’1; Ï†)Âµ(rL) by the rule of changing variables
qLâˆ’1(zLâˆ’1; Ï†)Âµ(rL) = Z qLâˆ’1(zLâˆ’1, r01:Lâˆ’1; Ï†)Âµ(rL)dr01:Lâˆ’1 = Z qL(zL, r01:L; Ï†)dr01:Lâˆ’1 = qL(zL, r0L; Ï†).
Subtract equation 20 from equation 19, then we have the difference in KL as
dLz(x; Î¸, Ï†1:L) = Z log
â¤â¤â¤â¤â¤â¤â¤ qL(zL, r01:L; Ï†) qL(zL; Ï†) Q Ll=1 Âµ(r0l(Ï†))
qL(zL, r01:L; Ï†)dzLdr01:L âˆ’ Z log
â¤â¤â¤â¤â¤â¤â¤ qL(zL, r01:L; Ï†) qL(zL, r0L; Ï†) Q Lâˆ’1 l=1 Âµ(r0l(Ï†))
qL(zL, r01:L; Ï†)dzLdr01:L = Z log qL(zL, r0L; Ï†)â³â³â³â³â³â³â³ Q Lâˆ’1 l=1 Âµ(r0l(Ï†))
qL(zL; Ï†)Âµ(r0L(Ï†))
â³â³â³â³â³â³â³ Q Lâˆ’1 l=1 Âµ(r0l(Ï†))
qL(zL, r01:L; Ï†)dzLdr01:L = Z log qL(zL, r0L; Ï†) qL(zL; Ï†)Âµ(r0L(Ï†)) qL(zL, r01:L; Ï†)dzLdr01:L = Z log qL(zL, r0L; Ï†) qL(zL; Ï†)Âµ(r0L(Ï†)) qL(zL, r0L; Ï†)dzLdr0L. (21)
From equation 21, it is easy to see that dLz(x; Î¸, Ï†1:L) is essentially KL divergence between
qL(zL, r0L; Ï†) and qL(zL; Ï†)Âµ(r0L(Ï†)). This verifies that
dLz(x; Î¸, Ï†1:L) = Lz(x; Î¸, Ï†1:L) âˆ’ Lz(x; Î¸, Ï†1:Lâˆ’1) â‰¥ 0.
This implies that the ergodic lower bound stops increasing if and only if the KL divergence between
qL(zL, r0L; Ï†) and qL(zL; Ï†)Âµ(r0L(Ï†)) is equal to 0.
C PROOF OF THEOREM 1
Proof. Because the flow is ergodic with invariant distribution p(z|x), with sufficient many transforï¿¾mations, qL(z) can converge to p(z|x) in total variation distance, that is
lim
Lâ†’âˆž
||qL(z) âˆ’ p(z|x)||TV = 0.
This is implied by the monotonic convergence of flow marginal qL(x) to Ï€(x) in total variation
simply follows monotonic convergence of MCMC chains in total variation distance by Theorem
6.51 and Proposition 6.52 in (Robert & Casella, 2005).
Then, we have
lim
Lâ†’âˆž
Lz(x; Î¸, Ï†L) = lim
Lâ†’âˆž Z
log pÎ¸(x, z) qL(z; Ï†) qL(z; Ï†)dz = Z log pÎ¸(x, z) qâˆž(zâˆž; Ï†) qâˆž(z; Ï†)dz = Z log pÎ¸(x, z) pÎ¸(z|x) pÎ¸(z|x)dz
= log p(x).
14
Under review as a conference paper at ICLR 2019
D EXPERIMENTAL RESULTS
D.1 CONFIGURATION OF HMPFS
The HMPFs in all the experiments share the following common configuration. The initial distribuï¿¾tion of HMPF q0(x) is given by independent Gaussian N (Âµ,Ïƒ2), where Ïƒ2 = (Ïƒ21
, . . . , Ïƒ2n) is the
vector of variance. We implement Hamiltonian measure preserving transformation (HMPT) fr usï¿¾ing vanilla Leapfrog integrator for simulating Hamiltonian dynamics H and independent Gaussian
momentum variable r. The implementation of Leapfrog algorithm follows the tutorial of Neal (Neal,
2010). The momentum variables in each HMPT are independent and the each momentum variable
has different variance. We consider separate step size  = ( 1, . . . , n) for each dimension of x.
Neal (Neal, 2010) shows that tuning leapfrog step size per dimension in HMC is equivalent to tunï¿¾ing the variance vector of momentum variables. So, we generate momentum variables from standard
normal and assign an independent Leapfrog step size  l for each HMPT fl
. The number of iterations
in Leapfrog integrator is a fixed parameters based manual tuning. We found that 5 to 10 Leapfrog
iterations are often good enough. More Leapfrog steps than that do not give better results. This is
consistent with the practice of tuning HMC (Neal, 2010). The intuitive explanation to this is that
because Hamiltonian dynamics often have strong oscillation, the longer simulation does not lead to
further exploration in sample space.
D.2 THE TRUE HISTOGRAM OF 2D BENCHMARKS
(a) Dual Moon (b) Circular Gaussian Mixture
Figure 2: The histogram of perfect samples from the targets using rejection sampling.
D.3 PLOTS OF MNIST AND FASHION MNIST
15
Under review as a conference paper at ICLR 2019
gen.pdf
(a) Generation mean image (VAE)
gen.pdf
(b) Generation mean image (HMPF)
Figure 3: Random generated images on fashion MNIST. There is no significant visaul difference in
the generated images.
gen.pdf
(a) VAE
gen.pdf
(b) HMPF
Figure 4: Random generated images on fashion MNIST. It is clear that VAE generates many fashion
articles that can almost fill up the whole image, like tops, bags and shirts. In contrast, the generation
from the generative model trained using HMPF can generate much diverse products in different size
and shapes, like shoes, pants and skirts.
16
Under review as a conference paper at ICLR 2019
test_origin
(a) Binarised test image
recon
(b) VAE
recon
(c) HMPF
Figure 5: The reconstructions on MNIST. On the left is dynamically binarised test image. Both conï¿¾volutional VAE and HMPF reconstruct from the same prebinariesd test image (a) and the generated
real valued images are shown as (b) and (c).
test_origin
(a) Binarised test image
recon
(b) VAE
recon
(c) HMPF
Figure 6: The reconstructions on fashion MNIST. On the left is dynamically binarised test image.
Both convolutional VAE and HMPF reconstruct from the same prebinariesd test image (a) and the
generated real valued images are shown as (b) and (c).
17
