Under review as a conference paper at ICLR 2019
Siamese Capsule Networks
Anonymous authors
Paper under double-blind review
Ab stract
Capsule Networks have shown encouraging results on defacto benchmark com-
puter vision datasets such as MNIST, CIFAR and smallNORB. Although, they are
yet to be tested on tasks where (1) the entities detected inherently have more com-
plex internal representations and (2) there are very few instances per class to learn
from and (3) where point-wise classification is not suitable. Hence, this paper
carries out experiments on face verification in both controlled and uncontrolled
settings that together address these points. In doing so we introduce Siamese Cap-
sule Networks, a new variant that can be used for pairwise learning tasks. The
model is trained using contrastive loss with '2-normalized capsule encoded Pose
features. We find that Siamese Capsule Networks perform well against strong
baselines on both pairwise learning datasets, yielding best results in the few-shot
learning setting where image pairs in the test set contain unseen subjects.
1 Introduction
Convolutional Neural networks (CNNs) have been a mainstay model for a wide variety of tasks
in computer vision. CNNs are effective at detecting local features in the receptive field, although
the spatial relationship between features is lost when crude routing operations are performed to
achieve translation invariance, as is the case with max and average pooling. Essentially, pooling
results in viewpoint invariance so that small perturbations in the input do not effect the output.
This leads to a significant loss of information about the internal properties of present entities (e.g
location, orientation, shape and pose) in an image and relationships between them. The issue is
usually combated by having large amounts of annotated data from a wide variety of viewpoints,
albeit redundant and less efficient in many cases. As noted by hinton1985shape, from a psychology
perspective of human shape perception, pooling does not account for the coordinate frames imposed
on objects when performing mental rotation to identify handedness Rock (1973); McGee (1979);
Humphreys (1983). Hence, the scalar output activities from local kernel regions that summarize sets
of local inputs are not sufficient for preserving reference frames that are used in human perception,
since viewpoint information is discarded. Spatial Transformer Networks (STN) Jaderberg et al.
(2015) have acknowledged the issue by using dynamic spatial transformations on feature mappings
to enhance the geometric invariance of the model, although this approach addresses changes in
viewpoint by learning to remove rotational and scale variance, as opposed to viewpoint variance
being reflected in the model activations. Instead of addressing translation invariance using pooling
operations, Hinton et al. (2011) have worked on achieving translation equivariance.The recently
proposed Capsule Networks Sabour et al. (2017); Hinton et al. (2018) have shown encouraging
results to address these challenges. Thus far, Capsule Networks have only been tested on datasets
that have (1) a relatively sufficient number of instances per class to learn from and (2) utilized on
tasks in the standard classification setup. This paper extends Capsule Networks to the pairwise
learning setting to learn relationships between whole entity encodings, while also demonstrating
their ability to learn from little data that can perform few-shot learning where instances from new
classes arise during testing (i.e zero-shot prediction). The Siamese Capsule Network is trained using
a contrastive loss with '2-normalized encoded features and demonstrated on two face verification
tasks.
1
Under review as a conference paper at ICLR 2019
2 Capsule Networks
Hinton et al. (2011) first introduced the idea of using whole vectors to represent internal properties
(referred to as instantiation parameters that include pose) of an entity with an associated activation
probability where each capsule represents a single instance of an entity within in an image. This
differs from the single scalar outputs in conventional neural networks where pooling is used as a
crude routing operation over filters. Pooling performs sub-sampling so that neurons are invariant to
viewpoint change, instead capsules look to preserve the information to achieve equivariance, akin
to perceptual systems. Hence, pooling is replaced with a dynamic routing scheme to send lower-
level capsule (e.g nose, mouth, ears etc.) outputs as input to parent capsule (e.g face) that represent
part-whole relationships to achieve translation equivariance and untangles the coordinate frame of
an entity through linear transformations. The idea has its roots in computer graphics where images
are rendered given an internal hierarchical representation, for this reason the brain is hypothesized
to solve an inverse graphics problem where given an image the cortex deconstructs it to its latent
hierarchical properties. The original paper by Sabour et al. (2017) describes a dynamic routing
scheme that represent these internal representations as vectors given a group of designated neurons
called capsules, which consist of a pose vector u ∈ Rd and activation α ∈ [0, 1]. The architecture
consists of two convolutional layers that are used as the initial input representations for the first
capsule layer that are then routed to a final class capsule layer. The initial convolutional layers
allow learned knowledge from local feature representations to be reused and replicated in other
parts of the receptive field. The capsule inputs are determined using a Iterative Dynamic Routing
scheme. A transformation Wij is made to output vector ui of capsule CiL. The length of the vector
ui represents the probability that this lower-level capsule detected a given object and the direction
corresponds to the state of the object (e.g orientation, position or relationship to upper capsule).
The output vector Ui is transformed into a prediction vector Uj∣i, where Uj∣i = Wijui. Then, Uj∣i
is weighted by a coupling coefficient Cij to obtain Sj = Pi Cijuj∣i, where coupling coefficients
for each capsule Pj cij = 1 and cij is got by log prior probabilities bij from a sigmoid function,
followed by the Softmax, Cij = ebij/ Pk ebik. If 叫分 has high scalar magnitude when multiplied
by ujL+1 then the coupling coefficient Cij is increased and the remaining potential parent capsules
coupling coefficients are decreased. Routing By Agreement is then performed using coincidence
filtering to find tight clusters of nearby predictions. The entities output vector length is represented
as the probability ofan entity being present by using the nonlinear normalization shown in Equation
1 where vote vj is the output from total input sj, which is then used to compute the agreement
aij = VjUj∣i that is added to the log prior bj.
V =	||sj ||2	Sj
j = 1 + Ilsjll2 Ilsjll
(1)
The capsule is assigned a high log-likelihood if densely connected clusters of predictions are found
from a subset of s. The centroid of the dense cluster is output as the entities generalized pose.
This coincidence filtering step can also be achieved by traditional outlier detection methods such as
Random sample consensus (RANSAC) Fischler & Bolles (1987) and classical Hough Transforms
Ballard (1987) for finding subsets of the feature space with high agreement. Although, the moti-
vation for using the vector normalization of the instantiation parameters is to force the network to
preserve orientation. Lastly, a reconstruction loss on the images was used for regularization which
constrains th capsules to learn properties that can better encode the entities. In this paper, we do not
use such regularization scheme by autoencoding pairs of input images, instead we use a variant of
dropout.
Extensions Of Capsule Networks Hinton et al. (2018) recently describe matrix capsules that
perform routing by agreement using the expectation maximization (EM) algorithm, motivated by
computer graphics where pose matrices are used to define rotations and translations of objects to
account for viewpoint changes. Each parent capsule is considered a Gaussian and the pose matrix
of each child capsule are considered data samples of the Gaussian. A given layer L contains a
set of capsules	CL	such that	∀C'∃	{M', α'}	∈	CL	where pose matrix Mg	∈	Rn×n	(n	=	4)
and activation α' ∈ [0,1] are the outputs. A vote is made Vij = MiWij for the pose matrix
of CjL+1 where Wij ∈ Rn×n is a learned viewpoint invariant transformation matrix from capsule
2
Under review as a conference paper at ICLR 2019
CL → CjL+1. EM determines the activation of CjL+1 as aj = σ(λ(βa - βu Pirij - Ph Costh))
where the costjh is the negative log-probability density weighted by the assignment probabilities rij,
-βu is the negative log probability density per pose matrix computed to describe CjL+1 . If CjL+1 is
activated -βa is the cost for describing (μj, σj from lower-level Pose data samples along with rj
and λ is the inverse temperature so as the assignment probability becomes higher the slope of the
sigmoid curve becomes steeper (represents the presence of an entity instead of the nonlinear vector
normalization seen in Equation 1). The network uses 1 standard convolutional layer, a primary
capsule layer, 2 intermediate capsule convolutional layer, followed by the final class capsule layer.
The matrix capsule network significantly outperformed CNNs on the SmallNORB dataset.
LaLonde & Bagci (2018) introduce SegCaps which uses a locally connected dynamic routing
scheme to reduce the number of parameters while using deconvolutional capsules to compensate
for the loss of global information, showing best performance for segmenting pathological lungs
from low dose CT scans. The model obtained a 39% and 95% reduction in parameters over baseline
architectures while outperforming both.
Bahadori (2018) introduced Spectral Capsule Networks demonstrated on medical diagnosis. The
method shows faster convergence over the EM algorithm used with pose vectors. Spatial coinci-
dence filters align extracted features on a 1-d linear subspace. The architecture consists of a 1d
convolution followed by 3 residual layers with dilation. Residual blocks R are used as nonlinear
transformations for the pose and activation of the first primary capsule instead of the linear trans-
formation that accounts for rotations in CV, since deformations made in healthcare imaging are not
fully understood. The weighted votes are obtained as sj,i = αiRj(ui) ∀i where Sj is a matrix of
concatenated votes that are then decomposed using SVD, where the first singular value dimension 瓦
is used to capture most of the variance between votes, thus the activation aj activation is computed
as σ η(s21/ Pk s2k - b) where s12/ Pk s2k is the ratio of all variance explained for all right singular
vectors in V , b is optimized and η is decreased during training. The model is trained by maximizing
the log-likelihood showing better performance than the spread loss used with matrix capsules and
mitigates the problem of capsules becoming dormant.
Wang & Liu (2018) formalize the capsule routing strategy as an optimization of a cluster-
ing loss and a KL regularization term between the coupling coefficient distribution and its past
states. The proposed objective function follows as minC,S {L(C, S) := - Pi Pj cijhoj|i, sji +
a Pi Pj CijlogCij} where oj∣i = Tijμi/1|Tj||f and ||Tij||f is the Frobenious norm of Tij. This
routing scheme shows significant benefit over the original routing scheme by Sabour et al. (2017)
as the number of routing iterations increase. Evidently, there has been a surge of interest within the
research community.
In contrast, the novelty presented in this paper is the pairwise learning capsule network scheme that
proposes a different loss function, a change in architecture that compares images, aligns entities
across images and describes a method for measuring similarity between final layer capsules such
that inter-class variations are maximized and intra-class variations are minimized. Before describing
these points in detail, we briefly describe the current state of the art work (SoTA) in face verification
that have utilized Siamese Networks.
3	Siamese Networks For Face Verification
Siamese Networks (SNs) are neural networks that learn relationships between encoded representa-
tions of instance pairs that lie on low dimensional manifold, where a chosen distance function dω is
used to find the similarity in output space. Below we briefly describe state of the art convolutional
SN’s that have been used for face verification and face recognition.
Sun et al. (2014) presented a joint identification-verification approach for learning face verification
with a contrastive loss and face recognition using cross-entropy loss. To balance loss signals for
both identification and verification, they investigate the effects of varying weights controlled by λ
on the intra-personal and inter-personal variations, where λ = 0 leaves only the face recognition
loss and λ → ∞ leaves the face verification loss. Optimal results are found when λ = 0.05 intra
personal variation is maximized while both class are distinguished.
3
Under review as a conference paper at ICLR 2019
Wen et al. (2016) propose a center loss function to improve discriminative feature learning in face
recognition. The center loss function proposed aims to improve the discriminability between feature
representations by minimizing the intra-class variation while keeping features from different classes
separable. The center loss is given as L = - Pim=1 log(ez)/(Pjn=1 ez) + λ2 Pmi = 1||xi - cyi ||22
where z = WjT xi + bj. The cyi is the centroid of feature representations pertaining to the ith class.
This penalizes the distance between class centers and minimizes the intra-class variation while the
softmax keeps the inter-class features separable. The centroids are computed during stochastic
gradient descent as full batch updates would not be feasible for large networks.
Liu et al. (2017) proposed Sphereface, a hypersphere embedding that uses an angular softmax loss
that constrains disrimination on a hypersphere manifold, motivated by the prior that faces lie on a
manifold. The model achieves 99.22 % on the LFW dataset, and competitive results on Youtube
Face (YTF) and MegaFace. Sankaranarayanan et al. (2016) proposed a triplet similarity embedding
for face verification using a triple loss arg minW = Pα,p,n∈T max(0, α+αTWTW(n-p)) where
for T triplet sets lies an anchor class α, positive class p and negative class n, a projection matrix
W, (performed PCA to obtain W0) is minimized with the constraint that WaT Wp > WaTWn. The
update rule is given as Wt+1 = Wt - ηWt(α(n - p)T + (n - p)αT). Hu et al. (2014) use deep
metric learning for face verification with loss arg minf J = 2 Pij g(1 - 'i,j (T - df (xi, Xj)))) +
λ (PM=ι (∣θ(m)lF I) where g(z) = log(1 + eβz)∕β, β controls the slope steepness of the logistic
function, ||A||F is the frobenius norm of A and λ is a regularization parameter. Hence, the loss
function is made up of a logistic loss and regularization on parameters θ = [W, b]. Best results
are obtained using a combination of SIFT descriptors, dense SIFT and local binary patterns (LBP),
obtaining 90.68% (+/- 1.41) accuracy on the LFW dataset.
Ranjan et al. (2017) used an '2-constraint on the softmax loss for face verification so that the en-
coded face features lie on the ambit of a hypersphere, showing good improvements in performance.
This work too uses an '2-constraint on capsule encoded face embeddings.FaceNet Schroff et al.
(2015) too uses a triplet network that combines the Inception network Szegedy et al. (2015) and
a 8-layer convolutional model Zeiler & Fergus (2014) which learns to align face patches during
training to perform face verification, recognition and clustering. The method trains the network on
triplets of increasing difficulty using a negative example mining technique. Similarly, we consider a
Siamese Inception Network for the tasks as one of a few comparisons to SCNs.
The most relevant and notable use of Siamese Networks for face verification is the DeepFace net-
work, introduced by Taigman et al. (2014). The performance obtained was on par with human level
performance on the Faces in the Wild (LFW) dataset and significantly outperformed previous meth-
ods. However, it is worth noting this model is trained on a large dataset from Facebook (SFC),
therefore the model can be considered to be performing transfer learning before evaluation. The
model also carries out some manual steps for detecting, aligning and cropping faces from the im-
ages. For detecting and aligning the face a 3D model is used. The images are normalized to avoid
any differences in illumination values, before creating a 3D model which is created by first identi-
fying 6 fiducial points in the image using a Support Vector Regressor from a LBP histogram image
descriptor. Once the faces are cropped based on these points, a further 67 fiducial point are identified
for 3D mesh model, followed by a piecewise affine transformation for each section of the image.
The cropped image is then passed to 3 CNN layers with an initial max-pooling layer followed two
fully-connected layers. Similar to Capsule Networks, the authors refrain from using max pooling
at each layer due to information loss. In contrast to this work, the only preprocessing steps for the
proposed SCNs consist of pixel normalization and a reszing of the image.
The above work all achieve comparable state of the art results for face verification using either
a single CNN or a combination of various CNNs, some of which are pretrained on large related
datasets. In contrast, this work looks to usea smaller Capsule Network that is more efficient, requires
little preprocessing steps (i.e only a resizing of the image and normalization of input features, no
aligning, cropping etc.) and can learn from relatively less data.
4	Siamese Capsule Network
The Capsule Network for face verification is intended to identify enocded part-whole relationships
of facial features and their pose that in turn leads to an improved similarity measure by aligning
4
Under review as a conference paper at ICLR 2019
capsule features across paired images. The architecture consists of a 5-hidden layer (includes 2
capsule layers) network with tied weights (since both inputs are from the same domain). The 1st
layer is a convolutional filter with a stride of 3 and 256 channels with kernels κi1 ∈ R9×9 ∀i
over the image pairs hx1, x2i ∈ R100×100 , resulting in 20, 992 parameters. The 2nd layer is the
primary capsule layer that takes κ(1) and outputs κ(2) ∈ R31×31 matrix for 32 capsules, leading to
5.309 × 106 parameters (663, 552 weights and 32 biases for each of 8 capsules). The 3rd layer is
the face capsule layer, representing the routing of various properties of facial features, consisting of
5.90 × 106 parameters. This layer is then passed to a single fully connected layer by concatenating
|CL |
the pose vectors ML∩ = ∩i=1 as input, while the sigmoid functions control the dropout rate for
each capsule during training. The nonlinear vector normalization shown in Equation 1 is replaced
with a tanh function tanh(.) which we found in initial testing to produce better results. Euclidean
distance, Manhattan distance and cosine similarity are considered as measures between the capsule
image encodings. The aforementioned SCN architecture describes the setup for the AT&T dataset.
For the LFW dataset, 6 routing iterations are used and 4 for AT&T.
Capsule Encoded Representations To encode paired images hx1, x2i into vector pairs hh1, h2i
the pose vector of each capsule is vectorized and passed as input to a fully connected layer containing
20 activation units. Hence, for each input there is a lower 20-dimensional representation of 32
capsule pose vectors resulting in 512 input features. To ensure all capsules stay active the dropout
probability rate is learned for each capsule. The sigmoid function learns the dropout rate of the final
capsule layer using Concrete Dropout Gal et al. (2017), which builds on prior work Kingma et al.
(2015); Molchanov et al. (2017) by using a continuous relaxation that approximates the discrete
Bernoulli distribution used for dropout, referred to as a concrete distribution. Equation 2 shows the
objective function for updating the concrete distribution. For a given capsule probability pc in the
last capsule layer, the sigmoid computes the relaxation Z on the Bernoulli variable z, where U is
drawn uniformly between [0,1] where t denotes the temperature values (t = 0.1 in our experiments)
which forces probabilities at the extremum when small. The pathwise derivative estimator is used to
find a continuous estimation of the dropout mask.
Zt = σ(t (logPc - log(1 - Pc)) + logUc - log(1 - Uc)
(2)
Loss Functions The original capsule paper with dynamic routing Sabour et al. (2017) used a
margin loss Lc = Tc max(0, m+ - ||vc ||)2 + λ(1 -Tc) max(0, ||vc|| -m-)2 where the class capsule
vc has margin m+ = 0.9 positives and m- = 1 -m+ negatives. The weight λ is used to prevent the
activity vector lengths from deteriorating early in training if a class capsule is absent. The overall
loss is then simply the sum of the capsule losses Pc Lc. A spread loss Hinton et al. (2018) has also
been used to maximize the inter-class distance between the target class and the remaining classes
for classifying on the smallNORB dataset. This is given as Li = (max(0, m - (at - ai))2 , L =
Pi6=t Li where the margin m is increased linearly during training to ensure lower-level capsule stay
active throughout training. This work instead uses a contrastive margin loss Chopra et al. (2005)
where the aforementioned capsule encoding similarity function dω outputs a predicted similarity
score. The contrastive loss Lc ensures similar vectorized pose encodings are drawn together and
dissimilar poses repulse. Equation 3 shows a a pair of images that are passed to the SCN model
where Dw = ||fω(xi) - fω(χ2)∣∣2 computes the Euclidean distance between encodings and m is
the margin. When using Manhattan distance Dw =exp ( - ||fω(xi) - fω(x2)∣∣1) in which case
m ∈ [0, 1). is used where y ∈ [-1, 1].
Lc(ω) = XX (1(1 - y(i))D(i) + ；y(i)max(0, m -。£)))
(3)
A double margin loss that has been used in prior work by Lin et al. (2015) is also considered to affect
matching pairs such that to account for positive pairs that can also have high variance in the distance
measure. It is worth noting this double margin is similar to the aforementioned margin loss used
on class capsules, without the use of λ. Equation 4 shows the double-margin contrastive loss where
positive margin mp and negative margin mn are used to find better separation between matching
5
Under review as a conference paper at ICLR 2019
Figure 1: Siamese Capsule Network Architecture
and non-matching pairs. This loss is only used for LFW, given the limited number of instances in
AT&T we find the amount of overlap between pairs to be less severe in experimentation.
m
Lc(ω) = X S — y(i))max(0, D(i) — mn)2 + y(i)max(mp - Dd 0)2)	(4)
The original reconstruction loss Lr(θ) = P∣=1 Pm=ι(y(j) — y(j))2 used as regularization is not
used in the pairwise learning setting, instead we rely on the dropout for regularization with exception
of the SCN model that uses concrete dropout on the final layer.
Optimization Convergence can often be relatively slow for face verification tasks, where few
informative batch updates (e.g a sample with significantly different pose for a given class) get large
updates but soon after the effect is diminished through gradient exponential averaging (originally
introduced to prevent α → 0). Motivated by recent findings that improve adaptive learning rates we
use AMSGrad Reddi et al. (2018). AMSGrad improves over ADAM in some cases by replacing
the exponential average of squared gradients with a maximum that mitigates the issue by keeping
long-term memory of past gradients. Thus, AMSGrad does not increase or decrease the learning
rate based on gradient changes, avoiding divergent or vanishing step sizes over time. Equation 5
presents the update rule, where diagonal of gradient gt is given as vt = Θ2vt-1 + (1 — Θ2 )gt2,
mt = Θ1mt-1 + (1 — Θι)gt, at = l/ʌ/t, Vt = max(Vt-ι, vt), ensuring a is monotonic.
mt
ωt+1 = ωt — a λ^---
√vt + e
(5)
5	Experiments on Face Verification
A.	AT&T dataset The AT&T face recognition and verification dataset consists of 40 different
subjects with only 10 gray-pixel images per subject in a controlled setting. This smaller dataset
allows us to test how SCNs perform with little data. For testing, we hold out 5 subjects so that we
are testing on unseen subjects, as opposed to training on a given viewpoint of a subject and testing
on another viewpoint of the same subject. Hence, zero-shot pairwise prediction is performed during
testing.
B.	Labeled Faces In The Wild (LFW) dataset The LFW consists of 13,000 colored pho-
tographed faces from the web. This dataset is significantly more complex not only because there
1680 subjects, with some subjects only consisting of two images, but also because of varied amount
of aging, pose, gender, lighting and other such natural characteristics. Each image is 250 × 250, in
this work the image is resized to 100 × 100 and normalized. From the original LFW dataset there has
been 2 different versions of the dataset that align the images using funneling Huang et al. (2007)
and deep funneling Huang et al. (2012). The latter learns to align the images using Restricted
Boltzmann Machines with a group sparsity penalty, showing performance improvements for face
6
Under review as a conference paper at ICLR 2019
AT&T			LFW		LFW+Double-M	
Models	Train	Test	Train	Test	Train	Test
Standard	0.013	0.042	0.0021	0.012	0.0049	0.014
ResNet-34	0.015	0.057	0.0018	0.012	0.0026	0.013
AlexNet	0.032	0.085	0.0019	0.009	0.0021	0.010
SCNet	0.008	0.019	0.0020	0.013	0.0019	0.011
SDropCapNet	0.010	0.032	0.0023	0.010	0.0028	0.012
Table 1: 5-fold CV Train & Test Contrastive Loss w/ Malahaobonis distance
verification tasks. The penalty leads to an arrangement of the filters that improved the alignment
results. This overcomes the problems previous CNNs and models alike had in accounting for pose,
orientation and problems Capsule Networks look to address. In contrast, we use the original raw
image dataset.
Both allow for a suitable variety as the former only contains grey-pixel images, a smaller dataset
with very few instances per class and images taken in a constrained setting allowing for a more
refined analysis, while the LFW data samples are colored images, relatively large with unbalanced
classes and taken in an unconstrained setting.
Baselines SCNs are compared against well-established architectures for image recognition and
verification tasks, namely AlexNet, ResNet-34 and InceptionV3 with 6 inception layers instead of
the original network that uses 8 layers which are used many of the aforementioned papers in Section
3.
5.1 Results
Table 1 shows best test results obtained when using contrastive loss with Euclidean distance between
encodings (i.e Mahalanobis distance) for both AT&T and LFW over 100 epochs. The former uses
m = 2.0 and the latter uses m = 0.2, while for the double margin contrastive loss mn = 0.2 match-
ing margin and mp = 0.5 negative matching margin is selected. These settings were chosen during
5-fold cross validation, grid searching over possible margin settings. SCN outperforms baselines
on the AT&T dataset after training for 100 epochs. We find that because AT&T contains far fewer
instances an adapted dropout rate leads to a slight increase in contrastive loss. Additionally, adding
a reconstruction loss with λr = 1e-4 for both paired images led to a decrease in performance when
compared to using dropout with a rate p = 0.2 on all layers except the final layer that encodes the
pose vectors. We find for the LFW dataset that the SCN and AlexNet have obtained the best results
while SCN has 25% less parameters. Additionally, the use ofa double margin results in better results
for the standard SCN but a slight drop in performance when used with concrete dropout on the final
layer (i.e SDropCapNet).
Figure 2 illustrates the contrastive loss during training '2-normalized features for each model tested
with various distance measures on AT&T and LFW. We find that SCN yields faster convergence on
AT&T, particularly when using Manhattan distance. However for Euclidean distance, we observe
a loss variance reduction during training and the best overall performance. Through experiments
We find that batch normalized convolutional layers improves performance of the SCN. In batch
normalization, x(k) = (x(k) - E[xk])//Var[x(I)] provides a unit Gaussian batch that is shifted by
Y(k) and scaled with β(k) so that a(k) = γ(k)X(k) + β(k). This allows the network to learn whether
the input range should be more or less diffuse. Batch normalization on the initial convolutional
layers reduced variance in loss during training on both the AT &T and LF W datasets. LFW test
results show that the SCN model takes longer to converge particularly in the early stages of training,
in comparison to AlexNet.
Figure 3 shows the probability density of the positive pair predictions for each model for all distances
between encodings with contrastive loss for the LFW dataset. We find the variance of predictions is
lower in comparison to the remaining models, showing a higher precision in the predictions, partic-
ularly for Manhattan distance. Additionally, varying distances for these matching images were close
7
Under review as a conference paper at ICLR 2019
ssoleʌ---se-l--uoo
ATT Dataset: Test Contrastive Loss
0	20	40	60	8	100
Epochs (Batch Size = 32)
Figure 2: Contastive Loss for AT&T (left) and LFW (right) datasets
Euclidean	Cosine	Manhattan
Figure 3: Probability Density of LFW Positive Pair Test Predictions
in variance to non-matching images. This motivated the use of the double margin loss considered
for the LFW dataset.
Finally, the SCN model has between 104-116 % less parameters than Alexnet, 24-27 % Resnet-34
and 127-135% less than the best standard baseline for both datasets. However, even considering tied
weights between models in the SCN, Capsule Networks are primarily limited in speed even with a
reduction in parameters due to the routing iterations that are necessary during training.
6 Conclusion
This paper has introduced the Siamese Capsule Network, a novel architecture that extends Capsule
Networks to the pairwise learning setting with a feature '2-normalized contrastive loss that maxi-
mizes inter-class variance and minimizes intra-class variance. The results indicate Capsule Networks
perform better at learning from only few examples and converge faster when a contrastive loss is
used that takes face embeddings in the form of encoded capsule pose vectors. We find Siamese
Capsule Networks to perform particularly well on the AT&T dataset in the few-shot learning setting,
which is tested on unseen classes (i.e subjects) during testing, while competitive against baselines
for the larger Labeled Faces In The Wild dataset.
References
Mohammad Taha Bahadori. Spectral capsule networks. 2018.
Dana H Ballard. Generalizing the hough transform to detect arbitrary shapes. In Readings in com-
Puter vision,pp. 714-725. ElSeVier, 1987.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with
application to face verification. In ComPuter Vision and Pattern Recognition, 2005. CVPR 2005.
IEEE ComPuter Society Conference on, volume 1, pp. 539-546. IEEE, 2005.
8
Under review as a conference paper at ICLR 2019
Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting
with applications to image analysis and automated cartography. In Readings in computer vision,
pp. 726-740. Elsevier,1987.
Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In Advances in Neural Information
Processing Systems, pp. 3584-3593, 2017.
Geoffrey Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. 2018.
Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In Interna-
tional Conference on Artificial Neural Networks, pp. 44-51. Springer, 2011.
Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Discriminative deep metric learning for face verification in
the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1875-1882, 2014.
Gary B Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: A
database for studying face recognition in unconstrained environments. Technical report, Technical
Report 07-49, University of Massachusetts, Amherst, 2007.
Gary B. Huang, Marwan Mattar, Honglak Lee, and Erik Learned-Miller. Learning to align from
scratch. In NIPS, 2012.
Glyn W Humphreys. Reference frames and shape perception. Cognitive Psychology, 15(2):151-
196, 1983.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Ad-
vances in neural information processing systems, pp. 2017-2025, 2015.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparame-
terization trick. In Advances in Neural Information Processing Systems, pp. 2575-2583, 2015.
Rodney LaLonde and Ulas Bagci. Capsules for object segmentation. arXiv preprint
arXiv:1804.04241, 2018.
Jie Lin, Olivier Morere, Vijay Chandrasekhar, Antoine Veillard, and Hanlin Goh. Deephash: Getting
regularization, depth and fine-tuning right. arXiv preprint arXiv:1501.04711, 2015.
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), volume 1, 2017.
Mark G McGee. Human spatial abilities: Psychometric studies and environmental, genetic, hor-
monal, and neurological influences. Psychological bulletin, 86(5):889, 1979.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural
networks. arXiv preprint arXiv:1701.05369, 2017.
Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-constrained softmax loss for discrimi-
native face verification. arXiv preprint arXiv:1703.09507, 2017.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Irvin Rock. Orientation and form. Academic Press, 1973.
Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In
Advances in Neural Information Processing Systems, pp. 3859-3869, 2017.
Swami Sankaranarayanan, Azadeh Alavi, and Rama Chellappa. Triplet similarity embedding for
face verification. arXiv preprint arXiv:1602.03418, 2016.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 815-823, 2015.
9
Under review as a conference paper at ICLR 2019
Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation by joint
identification-verification. In Advances in neural information processing Systems, pp. 1988-1996,
2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions.
Cvpr, 2015.
Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to
human-level performance in face verification. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1701-1708, 2014.
Dilin Wang and Qiang Liu. An optimization view on dynamic routing between capsules. 2018.
Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach
for deep face recognition. In European Conference on Computer Vision, pp. 499-515. Springer,
2016.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
10