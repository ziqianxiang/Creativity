Under review as a conference paper at ICLR 2019
Accelerated Sparse Recovery Under Struc-
tured Measurements
Anonymous authors
Paper under double-blind review
Ab stract
Extensive work on compressed sensing has yielded a rich collection of sparse re-
covery algorithms, each making different tradeoffs between recovery condition
and computational efficiency. In this paper, we propose a unified framework for
accelerating various existing sparse recovery algorithms without sacrificing recov-
ery guarantees by exploiting structure in the measurement matrix. Unlike fast al-
gorithms that are specific to particular choices of measurement matrices where the
columns are Fourier or wavelet filters for example, the proposed approach works
on a broad range of measurement matrices that satisfy a particular property. We
precisely characterize this property, which quantifies how easy it is to accelerate
sparse recovery for the measurement matrix in question. We also derive the time
complexity of the accelerated algorithm, which is sublinear in the signal length in
each iteration. Moreover, we present experimental results on real world data that
demonstrate the effectiveness of the proposed approach in practice.
1	Introduction
Natural data in its original form often contains much redundant information. In many domains, data
consists of multiple different (possibly noisy) linear measurements of the same latent signal, and so
redundancy implies that the amount of information encoded by the signal is often small relative to
its length. Because taking measurements in the real world is expensive, one of the goals of signal
processing is to reconstruct the latent signal with as few measurements as possible. Compressed
sensing (Donoho, 2006; Candes et al., 2006) refers to the idea of exploiting the redundancy to
reconstruct the signal from highly incomplete measurements, where the number of measurements is
much smaller than the length of the signal.
Mathematically, (discrete-time) signals that give rise to redundant data take the form of vectors
whose entries decay quickly when sorted by magnitude in decreasing order. As such, they can be
approximated by sparse vectors. Compressed sensing works by encouraging sparsity in the recov-
ered signal, while ensuring it is consistent with the observed measurements. Formally, its goal is to
solve the following optimization problem, which is known as the sparse recovery problem:
min kxk0 s.t. Ax = y
x∈Rn	0
where A ∈ Rm×n is the measurement matrix, y ∈ Rm is the vector of observed measurements and
X ∈ Rn is a vector representing a possible recovered signal. ∣∣∙ko denotes the '0 “norm” 1, which
returns the number of non-zero entries of a vector. Each row of A represents a particular way of
measuring the signal X. The number of measurements, m, is usually much smaller than the length
of the signal, n.
This problem is known to be NP hard in general (Natarajan, 1995), and so work in sparse recovery
focuses on developing algorithms that can efficiently recover any possible sparse latent signal for
particular classes of measurement matrices A. One way of characterizing the hardness of A is via
its restricted isometry constant (Candes & Tao, 2005). More specifically, a matrix has a k-restricted
isometry constant δk if (1 - δk ) ∣X∣22 ≤ ∣AX∣22 ≤ (1 + δk) ∣X∣22 for all k-sparse vectors X, that
is, vectors with at most k non-zero entries. Intuitively, a small k-restricted isometry constant means
1Quotation marks are duetOthefaCtthatkaxk ° = ∣α∣kx∣∣o in general.
1
Under review as a conference paper at ICLR 2019
that any set of k columns of the matrix is nearly orthonormal, since restricted isometry implies
that IlAJA∙j -	I∣∣2	≤	δk	for any J ⊆ {1,...,n}	such that	| J|	≤	k,	where	A. j	denotes the
submatrix of A consisting of columns indexed by J. (Candes & Tao, 2005) showed that as long as
δk + δ2k + δ3k < 0.25, any k-sparse vector can be recovered efficiently.
Various algorithms have been developed for sparse recovery, each of which trades off recovery guar-
antees, that is, the conditions under which the algorithm succeeds, for computational efficiency.
(See the next section for details.) In this paper, we explore an orthogonal approach that leverages
structure in measurement matrices to improve the computational efficiency of various existing al-
gorithms. While there have been prior methods along this direction, they either require carefully
chosen measurement matrices (like columns consisting of Fourier or wavelet filters (Gilbert et al.,
2002; 2005), or other specially designed constructions (Gilbert et al., 2006; 2007)) or compromise
on recovery guarantees (Jain et al., 2011). In contrast, we systematically characterize all measure-
ment matrices in terms of their ability to support efficient sparse recovery and quantify it using a
single number, known as the intrinsic dimensionality. We show that acceleration is possible as long
as intrinsic dimensionality is relatively small and precisely delineate the dependence of running time
on this number.
Many sparse recovery algorithms multiply an tall matrix with a low-dimensional vector, which re-
sults in a high-dimensional vector, only to discard most of the elements in the vector later. In the
case of greedy pursuit algorithms, given a sparse vector x, AT is multiplied with the residual vector
Ax - y to find the next coordinates to add to the support. Because the added coordinates cor-
respond to the elements of the resulting vector with the largest magnitudes, most of the elements
with small magnitudes have no effect on the algorithm and their values are discarded. Similarly, in
the case of iterative shrinkage algorithms, AT is multiplied with the residual when computing the
gradient, which is added to the current iterate and transformed by a shrinkage operator. Because
many shrinkage operators zero out elements with small magnitudes, the values of those elements are
effectively discarded. In both cases, because each element is derived from the result of an expen-
sive inner product computation that takes O(mn) time, and there are many elements whose values
are discarded, there is a lot of wasted computation, which could potentially be saved if we know
a priori which elements will be discarded. Of course, the challenge is that we don’t know a priori
which elements will be discarded, since that depends on their values, which we don’t know unless
we compute them. Is there any way around this? Somewhat surprisingly, the answer is yes. By
leveraging the fact that the matrix residuals are multiplied with is always AT over all iterations, it is
possible to preprocess AT ahead of time so that we can quickly identify the elements in ATv that
would have the largest magnitudes for any v without multiplying all rows of AT with v. Hence, by
using this technique, the time complexity per iteration becomes sublinear in the length of the latent
signal n. While one tempting way of accelerating the identification of the support is to reduce the
problem to hashing; however, as shown by (Jain et al., 2011), such an approach would result in a
significant degradation in recovery guarantees. In contrast, our technique is guaranteed to preserve
recovery guarantees. Empirically, we observe various algorithms accelerated using this technique
achieve significant speedups compared to the vanilla versions.
2	Background
There are four major types of methods for sparse recovery: basis pursuit, greedy pursuit, iterative
shrinkage and iterative reweighting. Basis pursuit (Chen et al., 2001) replaces the `0 “norm” with
`1 norm, which is convex and makes the problem much easier to solve. The `1 problem can be then
rewritten as a linear program and solved using simplex or interior-point methods. Greedy pursuit
algorithms maintain an estimated support set of the latent signal, which consists of the coordinates
of the latent signal the algorithm hypothesizes to be non-zero. In each iteration, they perform least
squares on the submatrix of A consisting of columns in the support and incrementally add or re-
move coordinates from the support based on the current residual. Examples of algorithms in this
category include orthogonal matching pursuit (OMP) (Pati et al., 1993; Tropp & Gilbert, 2007),
stagewise OMP (StOMP) (Donoho et al., 2012), regularized OMP (ROMP) (Needell & Vershynin,
2009), compressive sensing matching pursuit (CoSaMP) (Needell & Tropp, 2009), subspace pursuit
(SP) (Dai & Milenkovic, 2009), hard thresholding pursuit (Foucart, 2011) and OMP with replace-
ment (OMPR) (Jain et al., 2011). Iterative shrinkage algorithms solve a noisy version of the original
2
Under review as a conference paper at ICLR 2019
problem and possibly relax it by considering q > 0: 2
min kxkqq s.t. kAx - yk22 ≤ τ
x∈Rn
They first convert the problem into the following related unconstrained problem, which is equivalent
to the above for some unknown value of τ :
min
x∈Rn
kAx-yk22+λkxkqq
They then iteratively descend along the gradient of kAx - yk22 w.r.t. x (known as a Landweber
iteration) and then applies a shrinkage operator to each coordinate of the iterate independently.
The shrinkage operator varies for different algorithms and depends on the value of q that the algo-
rithm optimizes for, and either reduces or maintains the magnitude of each coordinate. Examples
of algorithms in this category include iterative soft thresholding (Donoho, 1995; Daubechies et al.,
2004; Maleki & Donoho, 2010), iterative hard thresholding (Blumensath & Davies, 2009), itera-
tive half thresholding (Xu et al., 2012), adaptive iterative soft and hard thresholding (Wang et al.,
2015), accelerated hard thresholding (Cevher, 2011). Like iterative shrinkage algorithms, iterative
reweighting algorithms convert the original problem into the unconstrained version; unlike iterative
shrinkage, they convert the non-convex 'q norm into convex '2 or '1 norms by setting a weight on
each coordinate and solve a sequence of weighted `2 or `1 regression problems.
Each of these methods make different tradeoffs between recovery condition and computational ef-
ficiency. Basis pursuit has excellent recovery guarantees and is able to recover all k-sparse vectors
if δ2k < 0.707 (Cai & Zhang, 2014). Unfortunately, it becomes computationally expensive in high
dimensions, where the length of the signal could be on the order of the millions or more. In terms of
computational complexity, no known linear programming algorithms achieve a strongly polynomial
running time, that is polynomial only in the number of variables, which correspond to m + n in the
original sparse recovery problem, and number of constraints without any dependence on condition
numbers of the program. It is currently not known if small restricted isometry constants imply the
conditional numbers are polynomial in m or n.
On the other hand, OMP is very computationally efficient both in theory and in practice. To recover
a k-sparse vector, it only needs to perform k iterations, where in each iteration it performs a matrix
multiplication with AT and solves a least squares problem on a m × k submatrix of A. Hence, the
algorithm is guaranteed to finish in strongly polynomial time, regardless of what A is. Unfortunately,
OMP has much weaker recovery guarantees and is only known to be able to recover k-sparse vectors
correctly if δ13k < 1/6 (Foucart & Rauhut, 2013), which is a much more stringent condition that
that is required by basis pursuit. Other greedy pursuit algorithms achieve better recovery guarantees:
ROMP requires δ3k < 0.2 (Needell & Vershynin, 2009), CoSaMP requires δ4k < 0.384 (Foucart,
2012), SP requires δ3k < 0.35 (Jain et al., 2011) and OMPR requires δ2k < 0.499 (Jain et al., 2011).
However, to date, none have been able to achieve comparable recovery guarantees as basis pursuit.
Similarly, iterative shrinkage algorithms are also more computationally efficient than basis pursuit,
but have weaker recovery guarantees. For example, the best known guarantees for iterative hard
thresholding are δ3k < 0.577 (Foucart & Rauhut, 2013) and δ3k+1 < 0.618 (Wang et al., 2015).
3	Method
We consider two examples of greedy pursuit and iterative shrinkage algorithms, compressive sensing
matching pursuit (CoSaMP) and adaptive iterative hard thresholding (AIHT).
We first make use of the fact that in sparse recovery problems, the columns of A can be normalized
without loss of generality. More formally,
Fact 1. For any invertible diagonal matrix D ∈ Rn×n, the following optimization problems are
equivalent:
min kxk0 s.t. kAx - yk22 ≤ τ	(1)
x∈Rn
min kxk0 s.t. kADx - yk22 ≤ τ	(2)
n
2where we define kxk00 to be kxk0 for notational convenience
3
Under review as a conference paper at ICLR 2019
Proof. Define x := D-1x, and so X = DX. We substitute this into problem (1), and obtain:
DmminnkD刘0 s.t. kADx - yk2 ≤T
Because D is diagonal and invertible, an element of DXx is zero if and only if the corresponding
element of Xx is zero. So, the number of zeros in DxX is the same number of zeros in xX. Hence,
kDXxk0 = kxXk0. The set the optimization is performed over is D-1(Rn), which is the same as Rn
because D is invertible.	□
If We choose Djjtobe 1/ ∣∣A∙j |卜 and define A0 := AD, then each column of A0 would have unit
norm. Therefore, we will henceforth assume that A is column-normalized.
Our goal is to speed up the computation of AT AX(t-1) - y by taking advantage of the fact that
most of the elements in the result will be discarded when the new support is computed or when
the shrinkage operator is applied. We first consider CoSaMP, which is delineated in Algorithm 1.
In CoSaMP, the result of AT AX(t-1) - y is stored in z(t), which is then used to find the set U,
consisting of new coordinates to add the support. To compute U, we need to know which elements
of z(t) are the largest in magnitude; other than this step, z(t) is not used anywhere else. So, it suffices
to identify which elements of z(t) are largest without necessarily computing all elements of z(t)
explicitly.
Algorithm 1 Compressive Sensing Matching Pursuit (CoSaMP)
Require: Measurement matrix A, observed measurements vector y and sparsity level k
x(0) 一 0
S JQ
for t = 1 to T do
z(t) J AT Ax(t-1) - y
U J indices of the 2k largest elements of z(t) in magnitude
-~
S — U ∪ S	_
x(t)S - ((ALS)T ALS)T A禺y
x(t)	— 0
Sc
S J indices of the k largest elements of x(t) in magnitude
X⑶ J x(t)|
x(t)	J 0
end for
return x(T)
Consider the jth element of z(t), which we will denote as zj(t). It can be written as follows:
Zjt) = hA∙j ,Ax(t-1) - yi
Therefore, to find the largest elements of |z(t) ∣, we need to find the Aj,s that have the highest inner
products with AX(t-1) - y in absolute value. We make use of the following fact to do so:
Fact 2. Let D be a set of vectors and S ⊆ D be the subset of vectors that attain the k highest inner
products with w in absolute value. Let S+ and S- be subsets of vectors that attain the k highest
inner products with w and -w respectively (not in absolute value). Then S ⊆ (S+ ∪ S-).
Proof. Consider a partitioning ofD into two disjoint subsets, D+ and D-, which consist vectors in
D that have positive and non-positive inner products with w respectively.
For any v ∈ S, either v, w > 0 or v, w ≤ 0. If v, w > 0, then only the other vectors in
S can have larger inner products with w than v. There are at most k - 1 of these vectors, and so
v ∈ S+ . Similarly, if on the other hand v, w ≤ 0, then only the other vectors in S can have
smaller inner products with w than v. There are at most k - 1 of these vectors, which implies that
v ∈ S- . This completes the proof.
□
4
Under review as a conference paper at ICLR 2019
Therefore, in order to find the Aj 's that have the highest inner products with Ax(t-1) -y in absolute
value, We simply combine the set of Aj's whose inner products with Ax(t-1) - y are the highest
and the set of Aj's whose inner products with y - Ax(t-1) are the highest and take the top half.
We now focus on how we can find the Aj's whose inner products with Ax(t-1) - y are the highest,
since the procedure for finding those with y - Ax(t-1) is the same.
Consider the Euclidean distance between Aj and Ax(t-1) - y, which can be rewritten as:
IlAj- (Aχ(tτ) - y) ∣∣2 =q(Aj- (AX(tT) - y))T (Aj- (AXOT) - y))
=qAτA^j-^2Aτ^(Aχ(t-ι)-^yy+^(Aχ(t-ι)-^yjτ^(Aχ(t-ι)-^yj
=q∣∣A∙j k2 - 2hA∙j,Ax(I)-yi + ∣∣AX(T)-y∣∣2
Because A is column-normalized, k Ajk? = 1 for all j. So,
∣∣A∙j -(Ax(t-1) - y) ∣∣2=qτ-2∑jAx(t-I)-^+∣Ax(t-l)-y∣2
Define φ(v) = hv,Ax(t-1) - y〉and ψ(u) = ʌ/l - 2u + ∣∣Ax(t-1) - y∣∣2. Since ψ is strictly
decreasing in u, if φ(vι) Z φ(v2), ψ(φ(vι)) < ψ(φ(v2)). In other words, a vector that achieves
a higher inner product with Ax(t-1) - y must be closer to Ax(t-1) - y in Euclidean distance.
Therefore, finding the Aj's that attain the highest inner products with Ax(t-1) - y is equivalent to
finding the Aj's that are closest to Ax(t-1) - y in Euclidean distance.
The resulting algorithm is concretely stated in Algorithm 2.
Algorithm 2 Accelerated Compressive Sensing Matching Pursuit (Accelerated COSaMP)
Require: Column-normalized measurement matrix A, observed measurements vector y and sparsity level k
x(0) 一 0
S — 0
Construct nearest neighbour search database D consisting of the vectors {A∙j }；=、
for t = 1 to T do
U+ — indices of 2k closest vectors in D to Ax(t-1) - y
V+ — absolute values of inner products between vectors indexed by U+ and Ax(t-1) - y
U- — indices of 2k closest vectors in D to y - Ax(t-1)
V- — absolute values of inner products between vectors indexed by U- and y - Ax(t-1)
U — indices of the vectors that the 2k largest elements in V+ ∪ V- correspond to
-~
S — U ∪ S	_
x(t)S — ((ALS)T ALS)T A∣⅞ y
x(t)	— 0
Sc
S — indices of the k largest elements of X(t) in magnitude
x(t) — x(t)∣
x(t)	— 0
end for
return x(T)
Similar techniques can be applied to AIHT. The most significant difference is that the shrinkage op-
erator is applied after a gradient descent step. Hence, both x(t-1) and AT Ax(t-1) - y determine
which elements will be zeroed out by the thresholding. Therefore, the coordinates corresponding
to the columns of A that achieve high inner products will not necessarily survive the thresholding,
and the coordinates associated with low inner products may not be non-zero after thresholding. To
apply the proposed technique in this case, we observe that x(t-1) is sparse and simply evaluate the
gradient descent step on the support of x(t-1) and the nearest neighbours of ± Ax(t-1) - y . The
precise algorithm is stated in Algorithm 4 in the supplementary material.
5
Under review as a conference paper at ICLR 2019
4	Sufficient Condition for Fast Recovery
Since our approach uses nearest neighbour search to accelerate sparse recovery, the amount of
speedup we can obtain depends on the fundamental difficulty of the underlying nearest neighbour
search problem. For the problem of exact nearest neighbour search, one standard way of charac-
terizing of the difficulty is the intrinsic dimensionality (Karger & Ruhl, 2002; Dasgupta & Sinha,
2015), which is defined as follows:
Definition 1. Given a dataset D ⊆ Rd, let Bp(r) be the set of points in D that are within a closed
Euclidean ball of radius r around a point p ∈ Rd. A dataset D has expansion dimension (τ, d0) if
forall r > 0,a > 1 andP such that ∣Bp(r)∣ ≥ T, |Bp(αr)∣ ≤ αd0 ∣Bp(r)∣. The quantity d0 is known
as the intrinsic dimensionality.
Intuitively, the intrinsic dimensionality characterizes how many close calls there could be when one
tries to find the nearest neighbours. If we consider a ball around a query that contains a single
data point (which is its nearest neighbour) and double the radius of the ball, then there could be as
many as 2d0 data points inside the ball that are all at most a factor of 2 more distant from the query
than the true nearest neighbour. Surprisingly, even though the number of points inside a ball grows
exponentially in d0, there is a randomized algorithm that can find the nearest neighbours in time
sublinear in d0 (Li & Malik, 2017).
We now derive a sufficient condition we would need to perform fast recovery. In our setting, nearest
neighbour search is used to quickly the find the columns of A that have the highest absolute inner
products with Ax(t-1) - y . So, it would be ideal if we can find a sufficient condition in terms
of inner products. As mentioned above, we will assume the columns of A are normalized, which
means all data points are unit vectors. We can also assume without loss of generality that query
is normalized, since we can divide it by any constant without changing the rankings of the data
points in terms of their inner products. For convenience, we will use q to denote the query and p(i)
to denote the data point with the ith highest inner product with q, or equivalently, the ith shortest
Euclidean distance to q, since ∣∣p(i) - qj^ =，2 - 2(p⑴,q.
First, we write down an equivalent statement to the definition in terms of p(i)’s and q:
∀j ≥ τ	∀i	≥	bαd0jc	+ 1	∣∣∣p(i)	- q∣∣∣	≥ α	∣∣∣p(j+1) - q∣∣∣
Then if We substitute，2 - 2(p⑴,q for ∣p(i) - q∣∣? and simplify, We get:
∀j ≥ τ ∀i ≥ bαd0jc + 1 hp(i), qi ≤ (1 - α) + αhp(j+1), qi
We now simplify this expression condition by eliminating the variable j ; we do so by finding the
value of j the results in the tightest inequality. Since j appears on the right-hand side, we’d like to
find the value of j that results in the smallest hp(j+1), qi, which happens when j is large by definition
of p(j+1). At the same time, we need to make sure that j is small enough so that i ≥ bαd0jc + 1.
This implies that j < α-d i; because j must be an integer, the inequality is the tightest when
j = bα-d0 ic. Hence, the condition simplifies to:
∀i ≥ [αd τC + 1 hp(i), qi ≤ (1 — α) + ahp(b("/ )+1c), q)
We now use this condition to derive the running time of Accelerated AIHT. For any v, ifwe now let
A∙ (i) denote the ith column of A with the highest inner product with v, then define do to be the small-
est number such that ∀v ∀α > 1 ∀i ≥ [2αd0k + 11(A.(i), v) ≤ (1 - α) + &〈/.([(〃ɑ壮0)+ij), v),
where k is the target sparsity level. Then the running time of each iteration of Accelerated AIHT is:
•	Finding 2k-nearest neighbours using Prioritized DCI (Li & Malik, 2017):
O(mk max(log(n∕2k), (n/2k)1-H/d0) + Hk log H (max(log(n∕2k), (n/2k)1-1/d0))),
where H ≥ 1 ∈ Z is a free parameter chosen by the user
•	Computing union of support and nearest neighbours: O(k)
•	Taking gradient descent step: O(mk)
6
Under review as a conference paper at ICLR 2019
(a)
⅛00 IOOOO 15000 20000 25000 388 35000 40000
DIEnSunaUty Of UtvntSIgnaI
(b)
(c)
Figure 1:	Performance of vanilla AIHT and accelerated AIHT on the image recovery task; (a)
shows a comparison of the execution time at various sizes of images (lower is better), (b) shows a
comparison of the magnitude of residuals (lower is better), and (c) visualizes the recovered images
and the ground truth images.

α∙βr
一CcSaMP
一ACTt⅛S<MP∣
(a)
ιααaα uaaa zαααa 25αaα aaaaa æaaa <αaαa
DIEnSunaUty Of SMntSignal
—CoSaMP
—AccCaSaMP
Os
(b)
ιαaαa 15aαa	20Ma	25αaα aaaaa æaɑa <βαaα
Dtmfinslcnallty Ot 3Mr* 9gnal
(c)
____ _____ ____
"金£ B ⅞2τn>a E3E-Ba
Figure 2:	Performance of vanilla CoSaMP and accelerated CoSaMP on the image recovery task; (a)
shows a comparison of the execution time at various sizes of images (lower is better), (b) shows a
comparison of the magnitude of residuals (lower is better), and (c) visualizes the recovered images
and the ground truth images.
• Taking the k largest elements of z(t) : O(k)
So, the overall running time per iteration is O(mk max(log(n/2k), (n/2k)1-H/d0 ) +
Hk log H max(log(n/2k), (n/2k)1-1/d0 ) ). Notice that it is sublinear in the signal length n.
Note that as long as d0 is relatively small, then this acceleration scheme could provide a significant
reduction in the dependence of time complexity on n relative to the vanilla version. The recovery
guarantees is inherited from AIHT, since Prioritized DCI is guaranteed to return the correct set of
nearest neighbours with high probability.
5 Convergence Analysis
Theorem 1. Denote x(t) are the iterates generatedfrom the original AIHT Suppose x(t) converges
linearly to a k sparse signal x* = 0 with rate c. And for any vector V, the probability that the
largest k elements in ATv did not match the largest k elements using DCI is at most . Then with
log ∣x"] |—log ∣∣x(O)-X* k
probability at least 1 一 e--U-∣ogc--------, the iterates x(t) generatedfrom accelerated AIHT
will be the same as X(t), i.e., x(t) = x(t) for any i ∈ N.
Proof. Because X(t) converges linearly to x*, We have
kx⑴一 X*k2 ≤ctkx⑴一 X*k2.
Because x* is k sparse, the k-th largest element in x*, denoted as x[*k], must be nonzero, i.e. |x[*k] | >
0. Combining these two facts, we know that after finite steps, the support of X(t) will not change. To
be more specific, if t > ∣oglxfk]1 ∣∣ogkx--, then ∣∣X(t) 一 x*∣∣2 ≤ CtkX(O) — x*∣∣2 ≤ ∣x,]∣. If the
7
Under review as a conference paper at ICLR 2019

'3B* B ⅞2τn>a E3E-Ba
0.050
Oais
β.Mβ
ex»5
β.βaa
t13
β.αια
IOOO 1500	2000
ot Latw* Sgnal
E3J- Bs
ErqB-Bs
(c)
(a)	(b)
E3J- Bs
Figure 3:	Performance of vanilla AIHT and accelerated AIHT on the trend detection task; (a) shows
a comparison of the execution time at various resolutions of the time series (lower is better), (b)
shows a comparison of the magnitude of residuals (lower is better), and (c) visualizes the extracted
trends.

->⅞-BK B ⅞2τn>a CKE-Ba
(a)	(b)
(c)


Figure 4:	Performance of vanilla CoSaMP and accelerated CoSaMP on the trend detection task;
(a) shows a comparison of the execution time at various resolutions of the time series (lower is
better), (b) shows a comparison of the magnitude of residuals (lower is better), and (c) visualizes the
extracted trends.
support of x(t) does not contain the support of x*,their distance will be atleast ∣x% |, which would
contradict the fact that their distance is smaller than |x；] |. Thus, the support of X(t) must contain
*	log |x[k] |—log ∣∣χ(0) —x* k
the support of x* for any t ›-----k-∣ogc----------.
As for accelerated AIHT, the probability that x(t) = X(t) for all t ≤
least 1 -
∣og ∣χ*k]∣-∣og kχ(0)-χ*k
log C
∣og∣χ*k]∣-∣ogkχ(0)-χ*k ∙,
iogc	is at
Now we will prove if that happens, iterates in accelerated AIHT
will be the same as the iterates in AIHT for all t. We will prove by induction: if x(t) = X(t) for all
log |x[*k] |—log ∣x(0) —x* ∣
t ≤ to where to >-------U-∣ogc----------, then for t = to + 1, as We discussed before, the support of
X(t0+1) is the same as the support of X(t0). On the other hand, the support of X(t0+1) is the set of k
largest elements in AT (y - Ax(t0)). In the accelerated AIHT, the support of x(t0+1) is the same as
the set of k largest elements in ATS(y - Ax(t0)) where S is the union of the support of x(t0) and the
set returned by DCI. Because that set always includes the support ofx(t0), we know that the support
of x(t0+1) will be the same as the support of x(t0). Therefore, we know that x(t0+1) = X(t0+1).
Thus, by induction, we know the conclusion holds.
□
6	Experiments
We conduct experiments on real data using CoSaMP and AIHT and compare their performance to
the accelerated versions developed in this paper. We consider two tasks, image recovery and trend
detection in time series. In image recovery, we represent a sparse image using a randomly generated
coding scheme, which each element of the code is a linear combination of all pixel values, where the
8
Under review as a conference paper at ICLR 2019
coefficients are randomly drawn i.i.d. from a standard Gaussian. The length of the code is much less
than the number of pixels in the image. The goal is to reconstruct the original sparse image from
the code. In trend detection, we take naturally occurring time series data and try to represent them
parsimoniously as piecewise linear functions. The goal is to use as few linear pieces as possible,
while approximating the original time series well.
We now present the concrete formulations of these tasks as sparse recovery problems. In image
recovery, A is an m × n matrix, where each entry is drawn randomly from a Gaussian. (Once
generated, this matrix is fixed for all images of the same size.) Each row of A represents a particular
way of computing an element in the code, and each column corresponds to a pixel in the image. x
is the image and y is the code. The image x is assumed to be sparse; in our experiments, we took
MNIST digits (which generally have few white pixels) and padded them along the sides to obtain
high-resolution images. The value of m we used was 1800 and the value of n ranged from 7000 to
40000.
In trend detection, A is an m × n matrix, where each row is a discretization of the function f (x) =
max(x - a, 0) at uniformly spaced values of x. Different rows have different horizontal shifts a.
m is always n - 2 in our case, since each subsequent row shifts the preceding row by one element.
x is the coefficients on each of the linear pieces and y is the time series we would like to explain.
We used the daily log closing prices of stocks traded on U.S. stock exchanges from 2007 to 2017
as our source of time series data. To get different resolutions of the data, we performed bilinear
downsampling.
As shown in Figures 1 and 2, the accelerated versions of both AIHT and CoSaMP are much faster
than the vanilla versions, while achieving comparable levels of accuracy. The speedup is more
significant for AIHT because it only needs to perform a gradient step after thresholding, which is
computationally inexpensive, whereas CoSaMP needs to perform least squares on the new support,
which requires a comparable computational cost as finding the support.
As shown in Figures 3 and 4, the accelerated versions of both AIHT and CoSaMP are faster than the
vanilla versions except in the very low-dimensional regime. Surprisingly, the accelerated version of
CoSaMP actually achieves better accuracy than the vanilla version. We conjecture this is because
the measurement matrix A in the case of trend detection is much more structured and so the problem
is more ill-conditioned, thereby making the randomness in the nearest neighbours search beneficial.
7	Conclusion
We presented a generic way of accelerating various sparse recovery algorithms, including CoSaMP
and AIHT, and showed a sufficient condition under which acceleration is possible practically for free
without sacrificing recovery guarantees. We also presented experiments on real world data, which
shows that our algorithms achieve significant speedups over the vanilla versions.
References
Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing. Ap-
plied and computational harmonic analysis, 27(3):265-274, 2009.
T Tony Cai and Anru Zhang. Sparse representation of a polytope and recovery of sparse signals and
low-rank matrices. IEEE transactions on information theory, 60(1):122-132, 2014.
Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on
information theory, 51(12):4203-4215, 2005.
Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete
and inaccurate measurements. Communications on pure and applied mathematics, 59(8):1207-
1223, 2006.
Volkan Cevher. On accelerated hard thresholding methods for sparse approximation. In Wavelets
and Sparsity XIV, volume 8138, pp. 813811. International Society for Optics and Photonics, 2011.
Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis
pursuit. SIAM review, 43(1):129-159, 2001.
9
Under review as a conference paper at ICLR 2019
Wei Dai and Olgica Milenkovic. Subspace pursuit for compressive sensing signal reconstruction.
IEEE transactions on Information Theory, 55(5):2230-2249, 2009.
Sanjoy Dasgupta and Kaushik Sinha. Randomized partition trees for nearest neighbor search. Algo-
rithmica, 72(1):237-263, 2015.
Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresholding algorithm
for linear inverse problems with a sparsity constraint. Communications on pure and applied
mathematics, 57(11):1413-1457, 2004.
David L Donoho. De-noising by soft-thresholding. IEEE transactions on information theory, 41(3):
613-627, 1995.
David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-
1306, 2006.
David L Donoho, Yaakov Tsaig, Iddo Drori, and Jean-Luc Starck. Sparse solution of underdeter-
mined systems of linear equations by stagewise orthogonal matching pursuit. IEEE transactions
on Information Theory, 58(2):1094-1121, 2012.
Simon Foucart. Hard thresholding pursuit: an algorithm for compressive sensing. SIAM Journal on
Numerical Analysis, 49(6):2543-2563, 2011.
Simon Foucart. Sparse recovery algorithms: sufficient conditions in terms of restricted isometry
constants. In Approximation Theory XIII: San Antonio 2010, pp. 65-77. Springer, 2012.
Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing, volume 1.
Birkhauser Basel, 2013.
Anna C Gilbert, Sudipto Guha, Piotr Indyk, S Muthukrishnan, and Martin Strauss. Near-optimal
sparse fourier representations via sampling. In Proceedings of the thiry-fourth annual ACM sym-
posium on Theory of computing, pp. 152-161. ACM, 2002.
Anna C Gilbert, S Muthukrishnan, and Martin Strauss. Improved time bounds for near-optimal
sparse fourier representations. In Wavelets XI, volume 5914, pp. 59141A. International Society
for Optics and Photonics, 2005.
Anna C Gilbert, Martin J Strauss, Joel A Tropp, and Roman Vershynin. Algorithmic linear dimen-
Sion reduction in the l_1 norm for sparse vectors. arXiv preprint cs/0608079, 2006.
Anna C Gilbert, Martin J Strauss, Joel A Tropp, and Roman Vershynin. One sketch for all: fast
algorithms for compressed sensing. In Proceedings of the thirty-ninth annual ACM symposium
on Theory of computing, pp. 237-246. ACM, 2007.
Prateek Jain, Ambuj Tewari, and Inderjit S Dhillon. Orthogonal matching pursuit with replacement.
In Advances in Neural Information Processing Systems, pp. 1215-1223, 2011.
David R Karger and Matthias Ruhl. Finding nearest neighbors in growth-restricted metrics. In
Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pp. 741-750.
ACM, 2002.
Ke Li and Jitendra Malik. Fast k-nearest neighbour search via Prioritized DCI. In Proceedings of
the 34th International Conference on Machine Learning, pp. 2081-2090, 2017.
Arian Maleki and David L Donoho. Optimally tuned iterative reconstruction algorithms for com-
pressed sensing. IEEE Journal of Selected Topics in Signal Processing, 4(2):330-341, 2010.
Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on comput-
ing, 24(2):227-234, 1995.
Deanna Needell and Joel A Tropp. Cosamp: Iterative signal recovery from incomplete and inaccu-
rate samples. Applied and computational harmonic analysis, 26(3):301-321, 2009.
10
Under review as a conference paper at ICLR 2019
Deanna Needell and Roman Vershynin. Uniform uncertainty principle and signal recovery via reg-
Ularized orthogonal matching pursuit. Foundations Ofcomputational mathematics, 9(3):317-334,
2009.
Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Orthogonal
matching pursuit: Recursive function approximation with applications to wavelet decomposition.
In Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilo-
mar Conference on, pp. 40-44. IEEE, 1993.
Joel A Tropp and Anna C Gilbert. Signal recovery from random measurements via orthogonal
matching pursuit. IEEE Transactions on information theory, 53(12):4655-4666, 2007.
Yu Wang, Jinshan Zeng, Zhimin Peng, Xiangyu Chang, and Zongben Xu. Linear convergence of
adaptively iterative thresholding algorithms for compressed sensing. IEEE Transactions on Signal
Processing, 63(11):2957-2971, 2015.
Zongben Xu, XiangyU Chang, Fengmin Xu, and Hai Zhang. l_{1/2} regularization: A thresholding
representation theory and a fast solver. IEEE Transactions on neural networks and learning
systems, 23(7):1013-1027, 2012.
11
Under review as a conference paper at ICLR 2019
8 Supplementary Material
Algorithm 3 Adaptive Iterative Hard Thresholding (AIHT)
Require: Measurement matrix A, observed measurements vector y, sparsity level k and step size η
x(0) 一 0
for t = 1 to T do
z(t) - x(t-1) - ηAT (Ax(t-1) - y)
S J indices of the k largest elements of z(t) in magnitude
x(t)	J z(t)∣
x(t)	J 0
end for
return x(T)
Algorithm 4 Accelerated Adaptive Iterative Hard Thresholding (Accelerated AIHT)
Require: Column-normalized measurement matrix A, observed measurements vector y, sparsity level k and
step size η
x(0) J 0
Construct nearest neighbour search database D consisting of the vectors {A∙j }；=、
for t = 1 to T do
S+ J indices of SuPP(X(t-1)) + k closest vectors in D to Ax(t-1) — y
S- J indices of SuPP(X(t-1)) + k closest vectors in D to y — Ax(t-1)
S J S+ ∪ S- ∪ SuPP(X(t-1))
z(t) S J x(t-1)∣S - η (ALS)T (Aχ(t-1) - y)
z(t) ∣∣	J 0
Sc
S J indices of the k largest elements of z(t) in magnitude
X(t) ∣∣∣ J z(t) ∣∣∣
X(t) ∣∣	J 0
end for
return X(T)
12