Under review as a conference paper at ICLR 2019
Dynamic Graph Representation Learning via
Self-Attention Networks
Anonymous authors
Paper under double-blind review
Ab stract
Learning latent representations of nodes in graphs is an important and ubiquitous
task with widespread applications such as link prediction, node classification, and
graph visualization. Previous methods on graph representation learning mainly
focus on static graphs, however, many real-world graphs are dynamic and evolve
over time. In this paper, we present Dynamic Self-Attention Network (DySAT),
a novel neural architecture that operates on dynamic graphs and learns node rep-
resentations that capture both structural properties and temporal evolutionary pat-
terns. Specifically, DySAT computes node representations by jointly employing
self-attention layers along two dimensions: structural neighborhood and tempo-
ral dynamics. We conduct link prediction experiments on two classes of graphs:
communication networks and bipartite rating networks. Our experimental results
show that DySAT has a significant performance gain over several different state-
of-the-art graph embedding baselines.
1	Introduction
Learning latent representations (or embeddings) of nodes in graphs has been recognized as a funda-
mental learning problem due to its widespread use in various domains such as social media (Perozzi
et al., 2014), biology (Grover & Leskovec, 2016), and knowledge bases (Wang et al., 2014). The
basic idea is to learn a low-dimensional vector for each node, which encodes the structural properties
of a node and its neighborhood (and possibly attributes). Such low-dimensional representations can
benefit a plethora of graph analytical tasks such as node classification, link prediction, and graph
visualization (Perozzi et al., 2014; Tang et al., 2015; Grover & Leskovec, 2016; Wang et al., 2016).
Previous work on graph representation learning mainly focuses on static graphs, which contain
a fixed set of nodes and edges. However, many graphs in real-world applications are intrinsically
dynamic, in which graph structures can evolve over time. They are usually represented as a sequence
of graph snapshots from different time steps (Leskovec et al., 2007). Examples include academic
co-authorship networks where authors may periodically switch their collaboration behaviors and
email communication networks whose structures may change dramatically due to sudden events. In
such scenarios, modeling temporal evolutionary patterns is important in accurately predicting node
properties and future links.
Learning dynamic node representations is challenging, compared to static settings, due to the com-
plex time-varying graph structures: nodes can emerge and leave, links can appear and disappear, and
communities can merge and split. This requires the learned embeddings not only to preserve struc-
tural proximity of nodes, but also to jointly capture the temporal dependencies over time. Though
some recent work attempts to learn node representations in dynamic graphs, they mainly impose
a temporal regularizer to enforce smoothness of the node representations from adjacent snapshots
(Zhu et al., 2016; Li et al., 2017; Zhou et al., 2018). However, these approaches may fail when nodes
exhibit significantly distinct evolutionary behaviors. Trivedi et al. (2017) employ a recurrent neural
architecture for temporal reasoning in multi-relational knowledge graphs. However, their temporal
node representations are limited to modeling first-order proximity, while ignoring the structure of
higher-order graph neighborhoods.
Attention mechanisms have recently achieved great success in many sequential learning tasks such
as machine translation (Bahdanau et al., 2015) and reading comprehension (Yu et al., 2018). The
1
Under review as a conference paper at ICLR 2019
key underlying principle is to learn a function that aggregates a variable-sized input, while focus-
ing on the parts most relevant to a certain context. When the attention mechanism uses a single
sequence as both the inputs and the context, it is often called self-attention. Though attention mech-
anisms were initially designed to facilitate Recurrent Neural Networks (RNNs) to capture long-term
dependencies, recent work by Vaswani et al. (2017) demonstrates that a fully self-attentional net-
work itself can achieve state-of-the-art performance in machine translation tasks. Velickovic et al.
(2018) extend self-attention to graphs by enabling each node to attend over its neighbors, achieving
state-of-the-art results for semi-supervised node classification tasks in static graphs.
As dynamic graphs usually include periodical patterns such as recurrent links or communities, at-
tention mechanisms are capable of utilizing information about most relevant historical context, to
facilitate future prediction. Inspired by recent work on attention techniques, we present a novel neu-
ral architecture named Dynamic Self-Attention Network (DySAT) to learn node representations on
dynamic graphs. Specifically, we employ self-attention along two dimensions: structural neighbor-
hoods and temporal dynamics, i.e., DySAT generates a dynamic representation for a node by consid-
ering both its neighbors and historical representations, following a self-attentional strategy. Unlike
static graph embedding methods that focus entirely on preserving structural proximity, we learn
dynamic node representations that reflect the temporal evolution of graph structure over a varying
number of historical snapshots. In contrast to temporal smoothness-based methods, DySAT learns
attention weights that capture temporal dependencies at a fine-grained node-level granularity.
We evaluate our framework on the dynamic link prediction task using four benchmarks of differ-
ent sizes including two email communication networks (Klimt & Yang, 2004; Panzarasa et al.,
2009) and two bipartite rating networks (Harper & Konstan, 2016). Our evaluation results show
that DySAT achieves significant improvements (3.6% macro-AUC on average) over several state-
of-the-art baselines and maintains a more stable performance over different time steps.
2	Related Work
Our framework is related to previous representation learning techniques on static graphs, dynamic
graphs, and recent developments in self-attention mechanisms.
Static graph embeddings. Early work on unsupervised graph representation learning exploits the
spectral properties of various graph matrix representations, such as Laplacian, etc. to perform di-
mensionality reduction (Tenenbaum et al., 2000; Belkin & Niyogi, 2001). To improve scalability,
some work (Perozzi et al., 2014; Grover & Leskovec, 2016) utilizes Skip-gram methods, inspired by
their success in Natural Language Processing (NLP). Recently, several graph neural network archi-
tectures based on generalizations of convolutions have achieved tremendous success, among which
many methods are designed for supervised or semi-supervised learning tasks (Niepert et al., 2016;
Defferrard et al., 2016; Kipf & Welling, 2017; Sankar et al., 2017; Velickovic et al., 2018). Hamil-
ton et al. (2017b) extend graph convolutional methods through trainable neighborhood aggregation
functions, to propose a general framework applicable to unsupervised representation learning. How-
ever, these methods are not designed to model temporal evolutionary patterns in dynamic graphs.
Dynamic graph embeddings. Most techniques employ temporal smoothness regularization to en-
sure embedding stability across consecutive time-steps (Zhu et al., 2016). Zhou et al. (2018) addi-
tionally use triadic closure (Kossinets & Watts, 2006) as guidance, leading to significant improve-
ments. Neural methods were recently explored in the knowledge graph domain by Trivedi et al.
(2017), who employ a recurrent neural architecture for temporal reasoning. However, their model is
limited to tracing link evolution, thus limited to capturing first-order proximity. Goyal et al. (2017)
learn incremental node embeddings through initialization from the previous time steps, however, this
may not guarantee the model to capture long-term graph similarity. A few recent works (Nguyen
et al., 2018; Zuo et al., 2018) examine a related setting of temporal graphs with continuous time-
stamped links for representation learning, which is however orthogonal to the established problem
setup of using dynamic graph snapshots. Li et al. (2017) learn node embeddings in dynamic at-
tributed graphs by initially training an offline model, followed by incremental updates over time.
However, their key focus is online learning to improve efficiency over re-training static models,
while our goal is to improve representation quality by exploiting the temporal evolutionary patterns
in graph structure. Unlike previous approaches, our framework captures the most relevant historical
contexts through a self-attentional architecture, to learn dynamic node representations.
2
Under review as a conference paper at ICLR 2019
Self-attention mechanisms. Recent advancements in many NLP tasks have demonstrated the su-
periority of self-attention in achieving state-of-the-art performance (Vaswani et al., 2017; Lin et al.,
2017; Tan et al., 2018; Shen et al., 2018; Shaw et al., 2018). In DySAT, we employ self-attention
mechanisms to compute a dynamic node representation by attending over its neighbors and previous
historical representations. Our approach of using self-attention over neighbors is closely related to
the Graph Attention Network (GAT) (Velickovic et al., 2018), which employs neighborhood atten-
tion for semi-supervised node classification in a static graph. As dynamic graphs usually contain
periodical patterns, we extend the self-attention mechanisms over the historical representations of a
particular node to capture its temporal evolution behaviors.
3	Problem Definition
In this work, we address the problem of dynamic graph representation learning. A dynamic graph
is defined as a series of observed snapshots, G = {G1, . . . , GT} where T is the number of time
steps. Each snapshot Gt = (V, Et) is a weighted undirected graph with a shared node set V, a link
set Et , and weighted adjacency matrix At at time t. Unlike some previous work that assumes links
can only be added over time in dynamic works, we also allow to remove links. Dynamic graph
representation learning aims to learn latent representations etv ∈ Rd for each node v ∈ V at time
steps t = 1, 2, . . . , T, such that etv preserves both the local graph structures centered at v and its
evolutionary behaviors prior to time t.
4	Dynamic S elf-Attention Network
In this section, we first describe the high-level structure of our model. DySAT consists of two
major novel components: structural and temporal self-attention layers, which can be utilized to
construct arbitrary graph neural architectures through stacking of layers. Similar to existing studies
on attention mechanisms, we employ multi-head attentions to improve model capacity and stability.
DySAT consists ofa structural block followed by a temporal block, as illustrated in Figure 1, where
each block may contain multiple stacked layers of the corresponding layer type. The structural
block extracts features from the local neighborhood through self-attentional aggregation, to compute
intermediate node representations for each snapshot. These representations feed as input to the
temporal block, which attends over multiple time steps, capturing temporal variations in the graph.
4.1	Structural self-attention
The input of this layer is a graph snapshot G ∈ G and a set of input node representations {xv ∈
RD , ∀v ∈ V} where D is the input embedding dimension. The input to the initial layer can be set
as 1-hot encoded vectors for each node (or attributes if available). The output is a new set of node
representations {zv ∈ RF, ∀v ∈ V} with F dimensions that capture local structural properties.
Specifically, the structural self-attention layer attends over the immediate neighbors of a node v (in
snapshot G), by computing attention weights as a function of their input node embeddings. The
structural attention layer is a variant of GAT (Velickovic et al., 2018), applied on a single snapshot:
e	e	exp (σ(Auv ∙ aτ[WSxU||WsXv]))
Zv =	U∈Nv ɑuvw "J,	αuv = P exp (σ(Awv ∙ aT[WSxw||WSxv]))	⑴
w∈Nv
where Nv = {u ∈ V : (u, v) ∈ E} is the set of immediate neighbors of node v in snapshot G;
WS ∈ RD×F is a shared weight transformation applied to each node in the graph; a ∈ R2D is
a weight vector parameterizing the attention function implemented as feed-forward layer; || is the
concatenation operation and σ(∙) is a non-linear activation function. Note that AUv is the weight
of link (u, v) in the current snapshot G. The set of learned coefficients αUv , obtained by a softmax
over the neighbors of each node, indicate the importance or contribution of node u to node v at the
current snapshot. We use a LeakyRELU non-linearity to compute the attention weights, followed by
ELU for the output representations. In our experiments, we employ sparse matrices to implement
the masked self-attention over neighbors.
3
Under review as a conference paper at ICLR 2019
t1	t2	tτ	TimeIine
Figure 1: Neural architecture of DySAT: we employ structural attention layers followed by temporal
attention layers. Dashed black arrows indicate new links and dashed blue arrows refer to neighbor-
based structural-attention.
4.2	Temporal self-attention
To further capture temporal evolutionary patterns in a dynamic network, we design a temporal self-
attention layer. The input of this layer is a sequence of representations of a particular node v at
different time steps. Specifically, for each node v, we define the input as {xv1 , xv2, . . . , xvT}, xtv ∈
RD0 where T is the number of time steps and D0 is the dimensionality of the input represen-
tations. The layer output is a new representation sequence for v at each time step, i.e., zv =
{zv1 , zv2 , . . . , zvT }, zvt ∈ RF0 with dimensionality F0. We denote the input and output represen-
tations of v, packed together across time, by matrices Xv ∈ RT ×D0 and Zv ∈ RT ×F0 respectively.
The key objective of the temporal self-attentional layer is to capture the temporal variations in graph
structure over multiple time steps. The input representation of node v at time-step t, xtv , constitutes
an encoding of the current local structure around v . We use xtv as the query to attend over its
historical representations (< t), tracing the evolution of the local neighborhood around v. Thus,
temporal self-attention facilitates learning of dependencies between various representations of a
node across different time steps.
To compute the output representation of node v at t, we use the scaled dot-product form of at-
tention (Vaswani et al., 2017) where the queries, keys, and values are set as the input node repre-
sentations. The queries, keys, and values are first transformed to a different space by using linear
projections matrices Wq ∈ RD0×F 0, Wk ∈ RD0×F 0 and Wv ∈ RD0×F 0 respectively. Here, we
allow each time-step t to attend over all time-steps up to and including t, to prevent leftward in-
formation flow and preserve the auto-regressive property. The temporal self-attention is defined as:
Zv = βv (Xv Wv), βvj = Texp(ej)	,斓=(((Xv Wq qq√X Wk )T ))j + Mj)	⑵
P exp(eivk)	F
k=1
where βv ∈ RT ×T is the attention weight matrix obtained by the multiplicative attention function
and M ∈ RT×T is a mask matrix with each entry Mij ∈ {-∞, 0}. When Mij = -∞, the
softmax function results in a zero attention weight, i.e., βvij = 0, which switches off the attention
from time-step i to j . To encode the temporal order, we define M as:
Mij
0,
-∞,
i≤j
otherwise
4
Under review as a conference paper at ICLR 2019
4.3	Multi-Head Attention
We additionally employ multi-head attention (Vaswani et al., 2017) to jointly attend to different
subspaces at each input, leading to a leap in model capacity. We use multiple attention heads,
followed by concatenation, in both structural and temporal self-attention layers:
Structural multi-head self-attention:	hv = Concat(zv1 , zv2 , . . . , zvH)	∀v ∈ V	(3)
Temporal multi-head self-attention:	Hv = Concat(Zv1, Zv2, . . . , ZvH)	∀v ∈ V	(4)
where H is the number of attention heads, hv ∈ RF and Hv ∈ RT×F0 are the outputs of structural
and temporal multi-head attentions respectively. Note that while structural attention is applied on a
single snapshot, temporal attention operates over multiple time-steps.
4.4	DySAT Architecture
In this section, we present our neural architecture DySAT for Dynamic Graph Representation Learn-
ing, that uses the above defined structural and temporal self-attention layers as fundamental mod-
ules. As shown in Figure 1, DySAT has three modules from its top to bottom, (1) structural attention
block, (2) temporal attention block, and (3) graph context prediction. The model takes as input a
collection of T graph snapshots, and generates outputs latent node representations at each time step.
Structural attention block. This module is composed of multiple stacked structural self-attention
layers to extract features from nodes at different distances. We apply each layer independently at
different snapshots with shared parameters, as illustrated in Figure 1, to capture local neighborhood
structure around a node at each time step. Note that the embeddings input to a layer can potentially
vary across different snapshots. We denote the node representations output by the structural attention
block, as {h1v, hv2, . . . , hvT}, htv ∈ Rf, which feed as input to the temporal attention block.
Temporal attention block. First, we equip the temporal attention module with a sense of ordering
through position embeddings (Gehring et al., 2017), {p1, . . . ,pT}, pt ∈ Rf, which embed the
absolute temporal position of each snapshot. The position embeddings are then combined with
the output of the structural attention block to obtain a sequence of input representations: {hv1 +
p1, h2v + p2, . . . , hvT +pT} for node v across multiple time steps. This block also follows a similar
structure with multiple stacked temporal self-attention layers. The outputs of the final layer pass into
a position-wise feed-forward layer to give the final node representations {ev1 , ev2, . . . , evT} ∀v ∈ V .
Graph context prediction. To ensure that the learned representations capture both structural and
temporal information, we define an objective function that preserves the local structure around a
node, across multiple time steps. We use the dynamic representations of a node v at time step t,
etv to predict the occurrence of nodes appearing the local neighborhood around v at t. In particular,
we use a binary cross-entropy loss function at each time step to encourage nodes co-occurring in
fixed-length random walks, to have similar representations.
T
Lv = X	X	- log(σ(<	eU, eV	>)) -	wn	∙ X log(1 - σ(<	eu	, eV	>))	⑸
t=1 u∈Nwt alk(v)	u0 ∈Pnt (v)
where σ is the sigmoid function, < . > denotes the inner product operation, Nwtalk(v) is the set of
nodes that co-occur with v on fixed-length random walks at snapshot t, Pnt is a negative sampling
distribution for snapshot Gt, and wn, negative sampling ratio, is a tunable hyper-parameter to balance
the positive and negative samples.
5	Experiments
We evaluate the quality of our learned node representations on the fundamental task of dynamic
link prediction. We choose this task since it has been widely used (Trivedi et al., 2017; Goyal
et al., 2017; Li et al., 2018) in evaluating the quality of dynamic node representations to predict the
temporal evolution in graph structure.
In our experiments, we compare the performance of DySAT against a variety of static and dynamic
graph representation learning baselines. Our experimental results on four publicly available bench-
marks indicate that DySAT achieves significant performance gains over other methods.
5
Under review as a conference paper at ICLR 2019
Communication Networks	Rating Networks
Dataset	Enron	UCI	Yelp	ML-10M
# Nodes	143	1,809	6,569	20,537
# Links	2,347	16,822	95,361	43,760
# Time steps	10	13	12	13
Table 1: Statistics of the datasets used in our experiments
5.1	Datasets
We use four dynamic graph datasets with two communication and bipartite rating networks each.
Communication networks. We consider two publicly available communication network datasets:
Enron (Klimt & Yang, 2004) and UCI (Panzarasa et al., 2009). In Enron, the communication links
are email interactions between core employees and the links in UCI represent messages sent between
users on an online social network platform.
Rating networks. We use two bipartite rating networks from Yelp1 and MovieLens (Harper &
Konstan, 2016). In Yelp, the dynamic graph comprises links between two types of nodes, users and
businesses, derived from the observed ratings over time. ML-10M consists of a user-tag interaction
network where user-tag links connects users with the tags they applied on certain movies.
In each dataset, multiple graph snapshots are created based on the observed interactions in fixed-
length time windows. Dataset statistics are shown in Table 1, while Appendix G has further details.
5.2	Experimental Setup
We conduct experiments on the task of link prediction in dynamic graphs, where we learn dynamic
node representations on snapshots {G1, . . . , Gt} and use {etv, ∀v ∈ V} to predict the links at Gt+1
during evaluation. We compare different models based on their ability to correctly classify each
example (node pair) into links and non-links. To further analyze predictive capability, we also
evaluate new link prediction, with a focus on new links that appear at each time step, (Appendix B).
We evaluate the performance of different models by training a logistic regression classifier for dy-
namic link prediction (Zhou et al., 2018). We create evaluation examples from the links in Gt+1 and
an equal number of randomly sampled pairs of unconnected nodes (non-links). A held-out valida-
tion set (20% links) is used to tune the hyper-parameters across all models, which is later discarded.
We randomly sample 25% of the examples for training and use the remaining 75% as our test set.
We repeat this for 10 randomized runs and report the average performance in our results.
We follow the strategy recommended by Grover & Leskovec (2016) to compute the feature repre-
sentation for a pair of nodes, using the Hadamard Operator (etu etv), for all methods unless explic-
itly specified otherwise. The Hadamard operator computes the element-wise product of two vectors
and closely mirrors the widely used inner product operation in learning node embeddings. We eval-
uate the performance of link prediction using Area Under the ROC Curve (AUC) scores (Grover &
Leskovec, 2016). We also report the average precision scores in Table 6 of the Appendix.
We implement DySAT in Tensorflow (Abadi et al., 2016) and employ mini-batch gradient descent
with Adam optimizer (Kingma & Ba, 2015) for training. For Enron, we use a single layer in both the
structural and temporal blocks, with each layer comprising 16 attention heads computing 8 features
apiece (for a total of 128 dimensions). In the other datasets, we use two structural self-attentional
layers with 16 and 8 heads respectively, each computing 16 features (layer sizes of 256, 128). The
model is trained for a maximum of 200 epochs with a batch size of 256 nodes and the best performing
model on the validation set, is chosen for evaluation.
5.3	Baseline
We compare the performance of DySAT with several state-of-the-art dynamic graph embedding
techniques. In addition, we include several static graph embedding methods in comparison to ana-
1https://www.yelp.com/dataset/challenge
6
Under review as a conference paper at ICLR 2019
Method	Enron		UCI		Yelp		ML-10M	
	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC
node2vec	83.72 ± 0.7	83.05 ± 1.2	79.99 ± 0.4	80.49 ± 0.6	67.86 ± 0.2	65.34 ± 0.2	87.74 ± 0.2	87.52 ± 0.3
G-SAGE	82.48* ± 0.6	81.88* ±0.5	79.15* ± 0.4	82.89* ± 0.2	60.95t ± 0.1	58.56∣ ± 0.2	86.19± ± 0.3	89.92± ± 0.1
G-SAGE + GAT	72.52 ± 0.4	73.34 ± 0.6	74.03 ± 0.4	79.83 ± 0.2	66.15 ± 0.1	65.09 ± 0.2	83.97 ± 0.3	84.93 ± 0.1
GCN-AE	81.55 ± 1.5	81.71 ± 1.5	80.53 ± 0.3	83.50 ± 0.5	66.71 ± 0.2	65.82 ± 0.2	85.49 ± 0.1	85.74 ± 0.1
GAT-AE	75.71 ± 1.1	75.97 ± 1.4	79.98 ± 0.2	81.86 ± 0.3	65.92 ± 0.1	65.37 ± 0.1	87.01 ± 0.2	86.75 ± 0.2
DynamicTriad	80.26 ± 0.8	78.98 ± 0.9	77.59 ± 0.6	80.28 ± 0.5	63.53 ± 0.3	62.69 ± 0.3	88.71 ± 0.2	88.43 ± 0.1
Know-Evolve	61.57 ± 1.1	62.28 ± 1.5	71.20 ± 0.5	80.93 ± 0.2	56.88 ± 0.2	59.68 ± 0.2	78.80 ± 0.5	83.70 ± 0.2
DynGEM	67.83 ± 0.6	69.72 ± 1.3	77.49 ± 0.3	79.82 ± 0.5	66.02 ± 0.2	65.94 ± 0.2	73.69 ± 1.2	85.96 ± 0.3
DySAT	85.71 ± 0.3	86.60 ± 0.2	81.03 ± 0.2	85.81 ± 0.1	70.15 ± 0.1	69.87 ± 0.1	90.82 ± 0.3	93.68 ± 0.1
Table 2: Experiment results on dynamic link prediction (micro and macro averaged AUC with stan-
dard deviation). We show GraphSAGE (denoted by G-SAGE) results with the best performing
aggregators for each dataset (* represents GCN, f represents LSTM, and 去 represents max-pooling).
lyze the gains of using temporal information for dynamic link prediction. To make a fair comparison
with static methods, we provide access to the entire history of snapshots by constructing an aggre-
gated graph upto time t, where the weight of each link is defined as the cumulative weight till t
agnostic of its occurrence times. We use author-provided implementations for all the baselines and
set the final embedding dimension d = 128.
We compare against several state-of-the-art unsupervised static embedding methods:
node2vec (Grover & Leskovec, 2016), GraphSAGE (Hamilton et al., 2017b) and graph au-
toencoders (Hamilton et al., 2017a). We experiment with different aggregators in GraphSAGE,
namely, GCN, mean-pooling, max-pooling, and LSTM, to report the performance of the best
performing aggregator in each dataset. To provide a fair comparison with GAT (Velickovic et al.,
2018), which originally conduct experiments only on node classification, we implement a graph
attention layer as an additional aggregator in GraphSAGE, which we denote by GraphSAGE +
GAT. We also train GCN and GAT as autoencoders for link prediction along the suggested lines
of (Zitnik et al., 2018), denoted by GCN-AE and GAT-AE respectively. In the dynamic setting,
we evaluate DySAT against the most recent studies on dynamic graph embedding including
Know-Evolve (Trivedi et al., 2017), DynamicTriad (Zhou et al., 2018), and DynGEM (Goyal et al.,
2017). The details of hyper-parameter tuning for all methods can be found in Appendix F.
5.4	Experimental Results
We evaluate the models at each time step t by training separate models up to snapshot t and evaluate
at t + 1 for each t = 1, . . . , T. We summarize the micro and macro averaged AUC scores (across all
time steps) for all models in Table 2. From the results, we observe that DySAT achieves consistent
gains of 3-4% macro-AUC, in comparison to the best baseline across all datasets.
Further, we compare the model performance at each time step (Figure 2), to obtain a deep under-
standing of their temporal behaviors. We fine the performance of DySAT to be relatively more stable
than other methods. This contrast is pronounced in the communication networks (Enron and UCI),
where we observe drastic drops in performance of static embedding methods at certain time steps.
The runtime per mini-batch of DySAT on ML-10M, using a machine with Nvidia Tesla V100 GPU
and 28 CPU cores, is 0.72 seconds. In comparison, a model variant without temporal attention
(Appendix A) takes 0.51 seconds, which illustrates the relatively low cost of temporal attention.
6	Discussion
Our experimental results provide several interesting observations and insights to the performance of
different graph embedding techniques.
First, we observe that GraphSAGE achieves comparable performance to DynamicTriad across dif-
ferent datasets, despite being trained only on static graphs. One possible explanation may be that
GraphSAGE uses trainable neighbor-aggregation functions, while DynamicTriad employs Skip-
gram based methods augmented with temporal smoothness. This leads us to conjecture that the
combination of structural and temporal modeling with expressive aggregation functions, such as
7
Under review as a conference paper at ICLR 2019
---DySAT——node2vec——GraphSAGE GraphSAGE+GAT
DynGEM
DynamicTriad
Kπow-evolve
Enron
UCI
100
100
90
90
80
70-
60
50
60
80
100
90
80
70-
O
σ160
GCN-AE——GAT-AE
6
Φ 70-
< -
O
< 80
Yelp
ML-IOM
50
70
Time steps
Time steps
Figure 2: Performance comparison of DySAT with different models across multiple time steps: the
solid line represents DySAT; dashed lines represent static graph embedding models; and dotted lines
represent dynamic graph embedding models. We truncate the y-axis to avoid visual clutter.
multi-head attention, is responsible for the consistently superior performance of DySAT on dynamic
link prediction. We also observe that node2vec achieves consistent performance agnostic of tempo-
ral information, which demonstrates the effectiveness of second-order random walk sampling. This
observation points to the direction of applying sampling techniques to further improve DySAT.
In DySAT, we employ structural attention layers followed by temporal attention layers. We choose
this design because graph structures are not stable over time, which makes directly employing struc-
tural attention layers after temporal attention layers infeasible. We also consider another alternative
design choice that applies self-attention along the two dimensions of neighbors and time together
following the strategy similar to (Shen et al., 2018). In practice, this would be computationally ex-
pensive due to variable number of neighbors per node across multiple snapshots. We leave exploring
other architectural design choices based on structural and temporal self-attentions as future work.
In the current setup, we store the adjacency matrix of each snapshot in memory using sparse ma-
trix, which may pose memory challenges when scaling to large graphs. In the future, we plan
to explore DySAT with memory-efficient mini-batch training strategy along the lines of Graph-
SAGE (Hamilton et al., 2017b). Further, we develop an incremental self-attention network (IncSAT)
that is efficient in both computation and memory cost as a direct extension of DySAT. Our initial
results are promising as reported in Appendix E, which opens the door to future exploration of
self-attentional architectures for incremental (or streaming) graph representation learning. We also
evaluate the capability of DySAT on multi-step link prediction or forecasting and observe significant
relative improvements of 6% AUC on average over existing methods, as reported in Appendix C.
7	Conclusion
In this paper, we introduce a novel self-attentional neural network architecture named DySAT to
learn node representations in dynamic graphs. Specifically, DySAT computes dynamic node rep-
resentations using self-attention over the (1) structural neighborhood and (2) historical node rep-
resentations, thus effectively captures the temporal evolutionary patterns of graph structures. Our
experiment results on various real-world dynamic graph datasets indicate that DySAT achieves sig-
nificant performance gains over several state-of-the-art static and dynamic graph embedding base-
lines. Though our experiments are conducted on graphs without node features, DySAT can be easily
generalized on feature-rich graphs. Another interesting direction is exploring continuous-time gen-
eralization of our framework to incorporate more fine-grained temporal variations.
8
Under review as a conference paper at ICLR 2019
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Va-
sudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system
for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and
Implementation, OSDI2016, Savannah, GA, USA, November2-4, 2016., pp. 265-283, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations (ICLR),
2015.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding
and clustering. In Advances in Neural Information Processing Systems 14 [Neural Information
Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British
Columbia, Canada], pp. 585-591, 2001.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-
10, 2016, Barcelona, Spain, pp. 3837-3845, 2016.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional
sequence to sequence learning. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1243-1252, 2017.
Palash Goyal, Nitin Kamra, Xinran He, and Yan Liu. Dyngem: Deep embedding method for dy-
namic graphs. In IJCAI International Workshop on Representation Learning for Graphs (ReLiG),
August 2017.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
San Francisco, CA, USA, August 13-17, 2016, pp. 855-864, 2016.
William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. arXiv preprint arXiv:1709.05584, 2017a.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 1025-
1035, 2017b.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. ACM
Transactions on Interactive Intelligent Systems (TIIS), 5(4):19, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference for Learning Representations (ICLR), 2017.
Bryan Klimt and Yiming Yang. Introducing the enron corpus. In CEAS 2004 - First Conference on
Email and Anti-Spam, July 30-31, 2004, Mountain View, California, USA, 2004.
Gueorgi Kossinets and Duncan J Watts. Empirical analysis of an evolving social network. science,
311(5757):88-90, 2006.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densification and shrinking
diameters. ACM Transactions on Knowledge Discovery from Data (TKDD), 1(1):2, 2007.
Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, and Huan Liu. Attributed network em-
bedding for learning in a dynamic environment. In Proceedings of the 2017 ACM on Conference
on Information and Knowledge Management, CIKM 2017, Singapore, November 06 - 10, 2017,
pp. 387-396, 2017.
9
Under review as a conference paper at ICLR 2019
Jundong Li, Kewei Cheng, Liang Wu, and Huan Liu. Streaming link prediction on dynamic at-
tributed networks. In Proceedings of the Eleventh ACM International Conference on Web Search
and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018, pp. 369-377,
2018.
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. A structured self-attentive sentence embedding. In International Conference on
Learning Representations (ICLR), 2017.
Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, and
Sungchul Kim. Continuous-time dynamic network embeddings. In 3rd International Workshop
on Learning Representations for Big Networks (WWW BigNet), 2018.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
works for graphs. In Proceedings of the 33nd International Conference on Machine Learning,
ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 2014-2023, 2016.
Pietro Panzarasa, Tore Opsahl, and Kathleen M. Carley. Patterns and dynamics of users’ behavior
and interaction: Network analysis of an online community. JASIST, 60(5):911-932, 2009.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representa-
tions. In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, KDD ’14, New York, NY, USA - August 24 - 27, 2014, pp. 701-710, 2014.
Aravind Sankar, Xinyang Zhang, and Kevin Chen-Chuan Chang. Motif-based convolutional neural
network on graphs. CoRR, abs/1711.05697, 2017.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position represen-
tations. In Proceedings of the 2018 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans,
Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), pp. 464-468, 2018.
Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang. Disan: Di-
rectional self-attention network for rnn/cnn-free language understanding. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, Febru-
ary 2-7, 2018, 2018.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(1):1929-1958, 2014.
Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, and Xiaodong Shi. Deep semantic role
labeling with self-attention. In Proceedings of the Thirty-Second AAAI Conference on Artificial
Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale
information network embedding. In Proceedings of the 24th International Conference on World
Wide Web, WWW 2015, Florence, Italy, May 18-22, 2015, pp. 1067-1077, 2015. doi: 10.1145/
2736277.2741093.
Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for
nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.
Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-evolve: Deep temporal reasoning
for dynamic knowledge graphs. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 3462-3471, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems 30: Annual Conference on Neural Information Processing Systems
2017, 4-9 December 2017, Long Beach, CA, USA, pp. 6000-6010, 2017.
10
Under review as a conference paper at ICLR 2019
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations
(ICLR), 2018.
Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San
Francisco, CA, USA, August 13-17, 2016, pp. 1225-1234, 2016. doi: 10.1145/2939672.2939753.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by trans-
lating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intel-
ligence, July27 -31,2014, Quebec City, Quebec, Canada. ,pp.1112-1119, 2014.
Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,
and Quoc V. Le. Qanet: Combining local convolution with global self-attention for reading
comprehension. In International Conference on Learning Representations (ICLR), 2018.
Le-kui Zhou, Yang Yang, Xiang Ren, Fei Wu, and Yueting Zhuang. Dynamic network embedding
by modeling triadic closure process. In Proceedings of the Thirty-Second AAAI Conference on
Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018.
Linhong Zhu, Dong Guo, Junming Yin, Greg Ver Steeg, and Aram Galstyan. Scalable temporal
latent space inference for link prediction in dynamic social networks. IEEE Trans. Knowl. Data
Eng., 28(10):2765-2777, 2016.
Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side effects with
graph convolutional networks. Bioinformatics, 34(13):457466, 2018.
Yuan Zuo, Guannan Liu, Hao Lin, Jia Guo, Xiaoqian Hu, and Junjie Wu. Embedding temporal
network via neighborhood formation. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pp. 2857-2866. ACM, 2018.
11
Under review as a conference paper at ICLR 2019
A Analysis of Temporal Attention
In this section, we conduct an in-depth analysis of the proposed temporal attention layer, to demon-
strate its utility and examine the distribution of temporal attention weights.
A.1 Effect of Removing Temporal Layers
To demonstrate the effectiveness of temporal self-attention layers, we conduct an experimental
study that removes the temporal attention block from DySAT to create a simpler architecture. This
model is optimized using the same loss function (Eqn. 5) applied on the intermediate representations
{hv1 , hv2, . . . , hvT} for each node v ∈ V. Note that this model is different from static methods since
the structural self-attention block is jointly optimized (using Eqn. 5) across all snapshots without
any explicit temporal modeling. We use the best configuration of DySAT on each dataset from the
original experiments to initialize the new model. The performance comparison is shown in Table 3.
We observe that in some datasets, the structural attention block is able to learn some temporal evo-
lution patterns in graph structure, despite the lack of explicit temporal modeling. However, the new
model is consistently inferior to DySAT and we observe that the original DySAT has a 3% average
gain in Macro-AUC, which validates our choice of using temporal self-attentional layers.
Method	Enron		UCI		Yelp		ML-10M	
	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC
Original	85.71 ± 0.3	86.60 ± 0.2	81.03 ± 0.2	85.81 ± 0.1	70.15 ± 0.1	69.87 ± 0.1	90.82 ± 0.3	93.68 ± 0.1
No Temporal	84.50 ± 0.3	85.68 ± 0.4	76.61 ± 0.2	79.97 ± 0.3	68.34 ± 0.1	67.20 ± 0.3	89.61 ± 0.4	91.10 ± 0.2
Table 3: Experimental study on removing temporal attention layers from DySAT (micro and macro
averaged AUC with standard deviation)
A.2 Visualization of Temporal Attention Weights
We conduct a qualitative analysis to obtain deeper insights into the distribution of temporal atten-
tion weights learned by DySAT. In this experiment, we examine the temporal attention coefficients
learned at each time step t, which indicate the relative importance of each historical snapshot (< t)
in predicting the links at t. We choose the Enron dataset to visualize the mean and standard deviation
of temporal attention coefficients, over all the nodes. Figure 3 visualizes a heatmap of the learned
temporal attention weights on Enron dataset for the first 10 time steps.
From Figure 3, we observe that the mean temporal attention weights are mildly biased towards recent
snapshots, while the historical snaphots vary in their importance across different time steps. Further,
we find that the standard deviation of attention weights across different nodes is generally high and
exhibits more variability. Thus, the temporal attention weights are well distributed across historical
snapshots, with significant variance across different nodes in the graph. While this analysis attempts
to provide a high-level perspective on the weights learned by temporal attention, an appropriate
interpretation of these coefficients (as done by e.g., (Bahdanau et al., 2015)) requires further domain
knowlege about the dataset under study, and is left as future work.
Temporal Attention Weights Mean (Enron)
ZE ks9r-86
(p(u:p_p(u」d) d(υ⅛(υELL
123456789 10
Time step (history)
0.0
TemPOral AttentlOn WelghtS Std. (Enron)
(P 罚t⅛si-⅛①E-匚一
-0.40
032
-0.24
123456789 10 ɑ ɑɑ
Time step (history)
Figure 3: Heatmap visualizing mean and standard deviation of temporal attention weights over all
nodes in Enron dataset.
12
Under review as a conference paper at ICLR 2019
B Dynamic New Link Prediction
In this section, we additionally report the results of dynamic link prediction evaluated only on the
new links at each time step. This provides an in-depth analysis on the capabilities of different
methods in predicting relatively unseen links. We follow the same evaluation setup of training a
downstream logistic regression classifier for dynamic link prediction. However, a key difference is
that the evaluation examples comprise new links at Gt+1 (that are not in Gt) and an equal number of
randomly sampled non-links.
Table 4 summarizes the micro and macro averaged AUC scores for different methods on the four
datasets. The absolute performance numbers of all methods are lower than the original evaluation
setup of using all links at Gt+1, which is reasonable since accurate prediction of new links at Gt+1
is expected to be slightly more challenging in comparison to predicting all the links at Gt+1. From
Table 2), We find that DySAT achieves consistent relative gains of 3-5% Macro-AUC over the best
baselines on dynamic new link prediction as well, thus validating its effectiveness in accurately
capturing temporal context for neW link prediction.
Method	Enron		UCI		Yelp		ML-10M	
	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC
node2vec	76.92 ± 1.2	75.86 ± 0.5	73.67 ± 0.3	74.76 ± 0.8	67.36 ± 0.2	65.17 ± 0.2	85.22 ± 0.2	84.89 ± 0.1
G-SAGE	73.92* ± 0.7	74.67* ± 0.6	76.69* ± 0.3	79.41* ± 0.1	62.25t ± 0.2	58.81t ± 0.3	85.23± ± 0.3	89.14± ± 0.2
G-SAGE + GAT	67.02 ± 0.8	68.32 ± 0.7	73.18 ± 0.4	76.79 ± 0.2	66.53 ± 0.2	65.45 ± 0.1	80.84 ± 0.3	82.53 ± 0.1
GCN-AE	74.46 ± 1.1	74.02 ± 1.6	74.76 ± 0.1	76.75 ± 0.6	66.18 ± 0.2	65.77 ± 0.3	82.45 ± 0.3	82.48 ± 0.2
GAT-AE	69.75 ± 2.2	69.25 ± 1.9	72.52 ± 0.4	73.78 ± 0.7	66.07 ± 0.1	65.91 ± 0.2	84.98 ± 0.2	84.51 ± 0.3
DynamicTriad	69.59 ± 1.2	68.77 ± 1.7	67.97 ± 0.7	71.67 ± 0.9	63.76 ± 0.2	62.83 ± 0.3	84.72 ± 0.2	84.32 ± 0.2
KnoW-Evolve	59.05 ± 2.7	59.63 ± 2.7	69.10 ± 0.3	77.48 ± 0.2	56.95 ± 0.2	59.72 ± 0.5	76.83 ± 0.5	82.23 ± 0.2
DynGEM	60.73 ± 1.1	62.85 ± 1.9	77.49 ± 0.3	79.82 ± 0.5	66.42 ± 0.2	66.84 ± 0.2	73.77 ± 0.7	83.51 ± 0.3
DySAT	78.87 ± 0.6	78.58 ± 0.6	79.24 ± 0.3	83.66 ± 0.2	69.46 ± 0.1	69.14 ± 0.1	89.29 ± 0.2	92.65 ± 0.1
Table 4: Experiment results on dynamic new link prediction (micro and macro averaged AUC With
standard deviation). We shoW GraphSAGE (denoted by G-SAGE) results With the best performing
aggregators for each dataset (* represents GCN, f represents LSTM, and 去 represents max-pooling).
C Multi-Step Link Prediction
In this section, We evaluate various dynamic graph representation learning methods on the task of
multi-step link prediction or forecasting. Here, each model is trained for a fixed number of time
steps, and the latest embeddings are used to predict links at multiple future time steps. In each
dataset, We choose the last 6 snapshots to evaluate multi-step link prediction. The model is trained
on the previous remaining snapshots, and the latest embeddings are used to forecast links at future
time steps. For each future time step t+∆ (1 ≤ ∆ ≤ 6), We create examples from the links in Gt+∆
and an equal number of randomly sampled pairs of unconnected nodes (non-links). We otherWise
use the same evaluation setup of training a doWnstream logistic regression classifier to evaluate link
prediction. In this experiment, We exclude the links formed by nodes that neWly appear in the future
evaluation snapshots, since most methods cannot be easily support updates for neW nodes.
Figure. 4 depicts the variation in model performance of different methods over the 6 evaluation
snapshots. As expected, We observe a slight decay in performance over time for all the models.
DySATachieves significant performance gains over all other baselines and maintains a highly stable
link prediction performance over multiple future time steps. Static embedding methods often exhibit
large variations in performance over time steps, While DySAT achieves a stable consistent perfor-
mance. The historical context captured by the dynamic node embeddings of DySAT, is one of the
most likely reasons for its stable multi-step forecasting performance.
13
Under review as a conference paper at ICLR 2019
---DySAT----node2vec----GraphSAGE-- GraphSAGE+GAT--- GCN-AE------GAT-AE DynamicTnac Know-evolve DynGEM
0⊃< ωmsω><
7	8	9	10	11	12	8	9	10	11	12	13
Time steps	Time steps
Figure 4: Performance comparison of DySAT with different models on multi-step link prediction
for 6 future time steps on all datasets
D Impact of unseen nodes on Dynamic Link Prediction
In this section, we analyze the sensitivity of different graph representation learning on link prediction
for previously unseen nodes that appear newly at time t.
---DySAT--node2vec--GraphSAGE-GraphSAGE+GAT GCN-AE-GAT-AF-- DynamicTriad Know-evolve DynGEM
O⊃< ωm2ω><
Number of new nodes
478	248	278	154	5,0	17	19	9	2,4	7	14
ioo	ucι
90
80
70-
60
40-
0--0--∩-
7 6H
0⊃< ωm2ω><
3	'	5	'	7	'	9	'	11	'	13
Time steps
Figure 5: Performance comparison of DySAT with different models on link prediction restricted to
new nodes at each time step
3	'	5	'	7	'	9	'	11	'	13
Time steps
14
Under review as a conference paper at ICLR 2019
Figure 6: Neural architecture of IncSAT: the components that are excluded from DySAT are wrapped
by dashed blue rectangles. The intermediate node representations are directly loaded from saved
models trained previously.
A node is considered as a new node at time step t in Gt if it has not appeared (has no links) in any of
the previous t - 1 snapshots. In this experiment, the evaluation set at time step t only comprises the
subset of links at Gt+1 among the new nodes in Gt and corresponding randomly sampled non-links.
Since the number of nodes varies significantly across different time steps, we report the performance
of each method along with the number of new nodes at each time step, in Figure 5.
From Figure 5, we observe that DySAT outperforms other baselines in most datasets, demonstrating
the ability to characterize new or previously unseen nodes despite their limited history. Although the
temporal attention will focus on the latest representation of a new node v due to absence of history,
the structural embedding of v recieves backpropagation signals through the temporal attention on
neighboring nodes, which indirectly affects the final embedding of v . We hypothesize that this
indirect temporal signal is one of the reasons for DySAT to achieve performance improvements over
baselines, albeit not designed to explicitly model historical context for previously unseen nodes.
E Incremental Self-Attention Network
In this section, we describe an extension of our dynamic self-attentional architecture to learn incre-
mental node representations. The motivation of incremental learning arises due to the proliferation
in sizes of real-world graphs, making it difficult to store multiple snapshots in memory. Thus, the
incremental graph representation learning problem imposes the restriction of no access to historical
graph snapshots, in contrast to most dynamic graph embedding methods. Specifically, to learn the
node embeddings {etv ∈ Rd ∀v ∈ V} at time T , we require a model to only access to the snapshot
GT and a summary of the historical snapshots. For example, DynGEM (Goyal et al., 2017) is an
example of an incremental embedding method that uses the embeddings learned at step t - 1, as
initialization to learn the embeddings at t.
We propose an extension of our self-attentional architecture named IncSAT to explore solving the
incremental graph representation learning problem. To learn node representations at T , we first
incrementally train multiple models at 1 ≤ t ≤ T . Unlike the original DySAT where structural
self-attention is applied at each snapshot, IncSAT applies the structural block only at the latest graph
snapshot Gt . We enable incremental learning by storing the intermediate output representations
{hvT ∀v ∈ V } of the structural block. As illustrated in Figure 6, these intermediate output repre-
sentations of historical snapshots (1 ≤ t < T ) can be directly loaded from previously saved results
at 1 ≤ t < T . Thus, the structural information of the previous historical snapshots are summarized
in the stored intermediate representations. The temporal self-attention is only applied to the current
15
Under review as a conference paper at ICLR 2019
								
Method	Enron		UCI		Yelp		ML-10M	
	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC	Micro-AUC	Macro-AUC
DySAT	85.71 ± 0.3	86.60 ± 0.2	81.03 ± 0.2	85.81 ± 0.1	70.15 ± 0.1	69.87 ± 0.1	90.82 ± 0.3	93.68 ± 0.1
DynGEM	67.83 ± 0.6	69.72 ± 1.3	77.49 ± 0.3	79.82 ± 0.5	66.02 ± 0.2	65.94 ± 0.2	73.69 ± 1.2	85.96 ± 0.3
IncSAT	84.36 ± 0.2	85.43 ± 0.3	76.18 ± 0.5	85.37 ± 0.2	69.54 ± 0.1	68.73 ± 0.3	80.13 ± 0.4	91.14 ± 0.2
Table 5: Experimental results of IncSAT in comparison to DySAT (micro and macro averaged AUC
with standard deviation)
snapshot GT over the historical representations of each node to compute the final node embeddings
{evT ∀v ∈ V} at T, which are trained on random walks sampled from GT.
We evaluate IncSAT using the same experimental setup, with minor modifications in hyper-
parameters. We use a dropout rate of 0.4 in both the structural and temporal self-attention layers.
From our preliminary experiments, we find that a higher dropout rate in the structural block can
facilitate avoiding over-fitting the model to the current graph snapshot. In Table 5, we report the per-
formance of IncSAT in comparison to DySAT and DynGEM, which is the only one that can support
incremental training from our baseline models. The results show that IncSAT achieves comparable
performance to DySAT on most datasets while significantly outperforming DynGEM, albeit with
minimal hyper-parameter tuning.
F Details on Hyper-parameter Settings and Tuning
In DySAT, the objective function (Eqn. 5) utilizes positive pairs of nodes co-occurring in fixed-length
random walks. We follow the strategy of Deepwalk (Perozzi et al., 2014) to sample walks 10 walks
of length 40 per node, each with a context window size of 10. We use 10 negative samples per pos-
itive pair, with context distribution (Pnt) smoothing over node degrees with a smoothing parameter
of 0.75, following (Perozzi et al., 2014; Grover & Leskovec, 2016; Hamilton et al., 2017b). During
training, we apply L2 regularization with λ = 5 × 10-4 and use dropout rates (Srivastava et al.,
2014) of 0.1 and 0.5 in the self-attention layers of the structural and temporal blocks respectively.
We use the validation set for tuning the learning rate in the range of {10-4, 10-3} and negative
sampling ratio wn in the range {0.01, 0.1, 1}.
We tune the hyper-parameters of all baselines following their recommended guidelines. For
node2vec, we use the default settings as in the paper, with 10 random walks of length 80 per node and
context window of 10, trained for a single epoch. We tune the in-out and return hyper-parameters,
p, q using grid-search, in the range {0.25, 0.50, 1, 2, 4} and report the best results. In case of Graph-
SAGE, we train a two layer model with respective neighborhood sample sizes 25 and 10, for 10
epochs, as described in the original paper. We evaluate the embeddings at each epoch on the vali-
dation set, and choose the best for final evaluation. Note that the results of GraphSAGE reported in
Table 2 represent that of best-performing aggregator in each dataset.
For Know-evolve, we tune the two weight-scale hyper-parameters in the range
{10-4, 10-3, 0.01, 0.1}, learning rates in {10-4, 10-3} and choose the best performing model.
DynamicTriad (Zhou et al., 2018) was tuned using their two key hyper-parameters determining the
effect of smoothness and triadic closure, β0 and β1 in the range {0.01, 0.1, 1, 10}, as advised, while
using recommended settings otherwise. We use the L1 operator (|etu - etu |) instead of Hadamard,
as recommended in the paper, which also gives better performance. For DynGEM, we tune the
different scaling and regularization hyper-parameters, α ∈ {10-6, 10-5}, β ∈ {0.1, 1, 2, 5},
ν1 ∈ {10-6, 10-4} and ν2 ∈ {10-6, 10-4}, while using other default configurations.
G Additional Dataset Details
In this section, we provide some additional, relevant dataset details. Since dynamic graphs often con-
tain continuous timestamps, we split the data into multiple snapshots using suitable time-windows
such each snapshot has an equitable yet reasonable number of interactions (communication/ratings).
In each snapshot, the weight of a link is determined by the number of interactions between the cor-
16
Under review as a conference paper at ICLR 2019
Method	Enron		UCI		Yelp		ML-10M	
	Micro-AP	Macro-AP	Micro-AP	Macro-AP	Micro-AP	Macro-AP	Micro-AP	Macro-AP
node2vec	84.26 ± 0.8	84.11 ± 1.1	80.22 ± 0.4	81.12 ± 0.5	66.46 ± 0.2	63.82 ± 0.2	88.86 ± 0.2	88.71 ± 0.3
G-SAGE	83.99* ± 0.6	84.02* ± 0.6	75.91* ± 0.6	82.36* ± 0.2	58.81t ± 0.1	55.84t ± 0.2	85.45± ± 0.3	90.26± ± 0.2
G-SAGE + GAT	72.60 ± 0.5	74.75 ± 0.9	66.77 ± 0.4	76.30 ± 0.4	62.43 ± 0.1	61.49 ± 0.3	81.69 ± 0.6	82.35 ± 0.2
GCN-AE	81.97 ± 1.5	83.08 ± 1.4	80.73 ± 0.4	84.16 ± 0.6	65.92 ± 0.2	65.39 ± 0.2	86.85 ± 0.2	87.43 ± 0.1
GAT-AE	76.98 ± 1.1	78.18 ± 0.8	80.14 ± 0.4	83.75 ± 0.3	65.45 ± 0.2	65.01 ± 0.2	88.42 ± 0.2	88.14 ± 0.2
DynamicTriad	82.06 ± 0.9	81.22 ± 0.9	76.21 ± 0.8	80.05 ± 0.6	61.29 ± 0.3	60.79 ± 0.3	89.91 ± 0.2	89.61 ± 0.3
Know-Evolve	57.68 ± 1.2	60.71 ± 1.7	66.99 ± 0.5	77.49 ± 0.2	53.98 ± 0.2	56.44 ± 0.2	75.64 ± 0.5	79.97 ± 0.3
DynGEM	70.37 ± 0.5	72.35 ± 1.0	78.78 ± 0.3	81.71 ± 0.3	68.02 ± 0.2	68.09 ± 0.2	80.65 ± 0.9	89.43 ± 0.2
DySAT	86.82 ± 0.3	88.25 ± 0.2	80.88 ± 0.2	85.96 ± 0.1	65.81 ± 0.1	66.76 ± 0.1	93.03 ± 0.2	94.92 ± 0.1
Table 6: Experiment results on dynamic link prediction (micro and macro average precision with
standard deviation). We show GraphSAGE (denoted by G-SAGE) results with the base performing
aggregators for each dataset (* represents GCN, f represents LSTM, and 去 represents max-pooling).
responding pair of users during that time-period. The pre-processed versions of all datasets will be
made publicly available, along with the scripts used for processing the raw data.
Communication Networks. The original un-processed Enron dataset is available at https:
//www.cs.cmu.edu/- ./enron/. We use only the email CommUncationS that are between
Enron employees, i.e., sent by an Enron employee and have at least one recipient who is an Enron
employee. A time-window of2 months is used to construct 16 snapshots, where the first 5 are used
as warm-up (due to sparsity) and the remaining 11 snapshots for evaluation.
The UCI dataset was downloaded from http://networkrepository.com/opsahl_
ucsocial.php. This dataset contains private messages sent between users over a span of six
months, on an online social network platform at the University of California, Irvine. The snapshots
are created using their communication history with a time-window of 10 days. We discard/merge
the terminal snapshots if they do not contain sufficient communications.
Rating Networks. We use the Round 11 version of the Yelp Dataset Challenge https://www.
yelp.com/dataset/challenge. To extract a cohesive subset of user-business ratings, we
first select all businesses in the state of Arizona (the state with the largest number of ratings) with
a selected set of restaurant categories. Further, we filter the data to retain only users and business
which have at-least 15 ratings. Finally, we use a time-window of 6 months to extract 12 snapshots
in the period of 2009 to 2015.
The ML-10m dynamic user-tag interaction network was downloaded from http://
networkrepository.com/ia-movielens-user2tags-10m.php. This dataset depicts
the tagging behavior of MovieLens users, with the tags applied by a user on her rated movies. We
use a time-window of 3 months to extract 13 snaphots over the course of 3 years.
17