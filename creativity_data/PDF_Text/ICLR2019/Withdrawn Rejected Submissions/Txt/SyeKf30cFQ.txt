Under review as a conference paper at ICLR 2019
A Theoretical Framework for Deep and Lo-
cally Connected ReLU Network
Anonymous authors
Paper under double-blind review
Ab stract
Understanding theoretical properties of deep and locally connected nonlinear
network, such as deep convolutional neural network (DCNN), is still a hard prob-
lem despite its empirical success. In this paper, we propose a novel theoretical
framework for such networks with ReLU nonlinearity. The framework bridges
data distribution with gradient descent rules, favors disentangled representations
and is compatible with common regularization techniques such as Batch Norm,
after a novel discovery of its projection nature. The framework is built upon
teacher-student setting, by projecting the student’s forward/backward pass onto the
teacher’s computational graph. We do not impose unrealistic assumptions (e.g.,
Gaussian inputs, independence of activation, etc). Our framework could help facili-
tate theoretical analysis of many practical issues, e.g. disentangled representations
in deep networks.
1	Introduction
Deep Convolutional Neural Network (DCNN) has achieved a huge empirical success in multiple
disciplines (e.g., computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al.,
2016), Computer Go (Silver et al., 2016; 2017; Tian & Zhu, 2016), and so on). On the other hand, its
theoretical properties remain an open problem and an active research topic.
Learning deep models are often treated as non-convex optimization in a high-dimensional space.
From this perspective, many properties in deep models have been analyzed: landscapes of loss
functions (Choromanska et al., 2015b; Li et al., 2017; Mei et al., 2016), saddle points (Du et al.,
2017; Dauphin et al., 2014), relationships between local minima and global minimum (Kawaguchi,
2016; Hardt & Ma, 2017; Safran & Shamir, 2017), trajectories of gradient descent (Goodfellow et al.,
2014), path between local minima (Venturi et al., 2018), etc.
However, such a modeling misses two components: neither specific network structures nor input data
distribution is considered. Both are critical in practice. Empirically, deep models work particular well
for certain forms of data (e.g., images); theoretically, for certain data distribution, popular methods
like gradient descent is shown to fail to recover network parameters (Brutzkus & Globerson, 2017).
Along this direction, previous theoretical works assume specific data distributions like spherical
Gaussian and focus on shallow nonlinear networks (Tian, 2017; Brutzkus & Globerson, 2017; Du
et al., 2018). These assumptions yield nice gradient forms and enable analysis of many properties
such as global convergence. However, it is also nontrivial to extend such approaches to deep nonlinear
neural networks that yield strong empirical performance.
In this paper, we propose a novel theoretical framework for deep and locally connected ReLU network
that is applicable to general data distributions. Specifically, we embrace a teacher-student setting.
The teacher computes classification labels via a computational graph that has local structures (e.g.,
CNN): intermediate variables in the graph, (called summarization variables), are computed from
a subset of the input dimensions. The student network, with similar local structures, updates the
weights to fit teacher’s labels with gradient descent, without knowing the summarization variables.
One ultimate goal is to show that after training, each node in the student network is highly selective
with respect to the summarization variable in the teacher. Achieving this goal will shed light to how
the training of practically effective methods like CNN works, which remains a grand challenge. As a
first step, we reformulate the forward/backward pass in gradient descent by marginalizing out the
input data conditioned on the graph variables of the teacher at each layer. The reformulation has nice
properties: (1) it relates data distribution with gradient update rules, (2) it is compatible with existing
1
Under review as a conference paper at ICLR 2019
LoCally connected NeUral NetworK
Figure 1: Problem setting. (a) We use Greek letters {α,β,... ω} to represent receptive fields.
Receptive fields form a hierarchy. The entire input is denoted as X (or Xω). A local region of an input
X is denoted as Xa. (b) For each region a, we have a latent multinomial discrete variable Za which is
computed from its immediate children {zβ}β∈ch(α). Given the input x, Za = Za(Xa) is a function
of the image content Xa at a. Finally, Zω at the top level is the class label. (c) A locally connected
neural network is trained with pairs (x, Zω(x)), where Zω(x) is the class label generated from the
teacher. (d) For each node j, fj (x) is the activation while gj (x) is the back-propagated gradient, both
as function of input X (and weights at different layers).
(d) Gradient Descent in Student
state-of-the-art regularization techniques such as Batch Normalization (Ioffe & Szegedy, 2015), and
(3) it favors disentangled representation when data distributions have factorizable structures. To our
best knowledge, our work is the first theoretical framework to achieve these properties for deep and
locally connected nonlinear networks.
Previous works have also proposed framework to explain deep networks, e.g., renormalization group
for restricted Boltzmann machines (Mehta & Schwab, 2014), spin-glass models (Amit et al., 1985;
Choromanska et al., 2015a), transient chaos models (Poole et al., 2016), differential equations (Su
et al., 2014; Saxe et al., 2013), information bottleneck (Achille & Soatto, 2017; Tishby & Zaslavsky,
2015; Saxe et al., 2018), etc. In comparison, our framework (1) imposes mild assumptions rather
than unrealistic ones (e.g., independence of activations), (2) explicitly deals with back-propagation
which is the dominant approach used for training in practice, and relates it with data distribution, and
(3) considers spatial locality of neurons, an important component in practical deep models.
2	Problem Setting
We consider multi-layer (deep) and locally connected network with ReLU nonlinearity. We consider
supervised setting, in which we have a dataset {(x, y)}, where x is the input image and y is its label
computed from x deterministically. It is hard to analyze y which does not have a structure (e.g.,
random labels). Here our analysis assumes the generation of y from x has a specific hierarchical
structure. We use teacher-student setting to study the property: a student network learns teacher’s
label y via gradient descent, without knowing teacher’s internal representations.
2.1	Receptive fields
An interesting characteristics in locally connected network is that each neuron only covers a fraction
of the input dimension. Furthermore, for deep and locally connected network, neurons in the lower
layer cover a small region while neurons in the upper layer cover a large region.
We use Greek letters {α, β, . . . , ω} to represent receptive fields. For a receptive field α, xa is the
content in that region. We use ω to represent the entire image (Fig. 1(a)).
Receptive fields form a hierarchy: α is a parent of β, denoted as α ∈ pa(β) or β ∈ ch(α), if α ⊇ β
and there exists no other receptive field γ ∈/ {α, β} so that α ⊇ γ ⊇ β. Note that siblings can have
substantial overlaps (e.g., β1 and β2 in Fig. 1(a)). With this partial ordering, we can attach layer
number l to each receptive field: α ∈ pa(β) implies l(β) = l(α) + 1. For top-most layer (closest to
classification label), l = 0 and for bottom-most layer, l = L.
For locally connected network, a neuron (or node) j ∈ α means its receptive field is α. Denote na
as the number of nodes covering the same region (e.g., multi-channel case, Fig. 2(a)). The image
content is xa(j), abbreviated as xj ifno ambiguity. The parentj’s receptive field covers its children’s.
2.2	The teacher
We assume the label y of the input x is computed by a teacher in a bottom-up manner: for each
region α, we compute a summarization variable Za from the summarization variables of its children:
2
Under review as a conference paper at ICLR 2019
(b)
明	«2
!O Ω O ' ；C C C；
∣O O O I IO O O ； ∣O O O ；
βl β2 βz
Figure 2: (a) Multiple nodes (neurons) share the same receptive field a. Note that na is the number
of nodes sharing the receptive field α. (b) Grouping nodes with the same receptive fields together. By
abuse of notation, α also represents the collection of all nodes with the same receptive field.
Za = φα({zβ }β∈ch(α)). ThiS procedure is repeated until the top-level summarization Zω is computed,
which is the class label y. We denote φ = {φa } as the collection of all summarization functions.
For convenience, we assume Za be discrete variables that takes m^ possible values. Intuitively, m^
is exponential w.r.t the area of the receptive field sz(α), for binary input, m@ ≤ 2sz(a). We call a
particular assignment of Za, Za = a, an event. For the bottom-most layers, Z is just the (discretized)
value in each dimension.
At each stage, the upward function is deterministic but lossy: Za does not contain all the information
in {Zβ} for β ∈ ch(α). Indeed, it keeps relevant information in the input region Xa with respect to the
class label, and discards the irrelevant part. During training, all summarization variables Z = {Za}
are unknown to the student, except for the label y.
Example of teacher networks. Locally connected network itself is a good example of teacher net-
work, in which nodes of different channels located at one specific spatial location form some encoding
of the variable Za . Note that the relationship between a particular input x and the corresponding
values of the summarization variable Z at each layer is purely deterministic.
The reason why probabilistic quantities (e.g., P(Za) and P(Za∣Zβ)) appear in our formulation, is due
to marginalization over Z (or x). This marginalization implicitly establishes a relationship between
the conditional probabilities P(Za∣Zβ) and the input data distribution P(x). If we have specified
P(ZaIZe) at each layer, then we implicitly specify a certain kind of data distribution P(x). Conversely,
given a certain kind of P(X) and summarization function φ, we can compute P(ZaIZe) by sampling
x, compute summarization variable Za, and accumulate frequency statistics of P(ZaIZβ). If there is
an overlap between sibling receptive fields, then it is likely that some relationship among P(ZaIZe)
might exist, which we leave for future work.
Although such an indirect specification may not be as intuitive and mathematically easy to deal with
as common assumptions used in previous works (e.g., assuming Gaussian input (Tian, 2017; Du et al.,
2018; Brutzkus & Globerson, 2017)), it gives much more flexibility of the distribution X and is more
likely to be true empirically.
Comparison with top-down generative model. An alternative (and more traditional) way to specify
data distribution is to use a top-down generative model: first sample the label y, then sample the
latent variables Za at each layer in a top-down manner, until the input layer. Marginalizing over all
the latent variables Za yields a class-conditioned data distribution P(XIy).
The main difficulty of this top-down modeling is that when the receptive fields α and α0 of sibling
latent variables overlap, the underlying graphical model becomes loopy. This makes the population
loss function, which involves an integral over the input data X, very difficult to deal with. As a result,
it is nontrivial to find a concise relationship between the parameters in the top-down modeling (e.g.,
conditional probability) and the optimization techniques applied to neural network (e.g., gradient
descent). In contrast, as we will see in Sec. 3, our modeling naturally gives relationship between
gradient descent rules and conditional probability between nearby summarization variables.
2.3	The student
We consider a neuron (or node) j . Denote fj as its activation after nonlinearity and gj as the
(input) gradient it receives after filtered by ReLU’s gating (Fig. 1(d)). Note that both fj and gj are
deterministic functions of the input X and label y, and are abbreviated as fj (X) and gj (X). 1.
1 Note that all analysis still holds with bias terms. We omit them for brevity.
3
Under review as a conference paper at ICLR 2019
The activation fj and gradient gk can be written as (note that fj0 is the binary gating function):
fj (x) = fj0(x) wjkfk(x), gk(x) = fk0 (x) wjkgj(x)	(1)
k∈ch(j)	j∈pa(k)
And the weight update for gradient descent is ∆wjk = Ex [fk (x)gj (x)]. Here is the expectation is
with respect to a training dataset (or a batch), depending on whether GD or SGD has been used. We
also use fjraw and gjraw as the counterpart of fj and gj before nonlinearity.
For locally connected network, the activation fj of node j is only dependent on the region xj , rather
than the entire image x. This means that fj (x) = fj (xj) and fj (xj) = fj0 (xj) Pk wjkfk (xk).
However, the gradient gj is determined by the entire image x, and its label y, i.e., gj = gj (x, y).
Note that since the label y is a deterministic (but unknown) function of x, for gradient we just write
gj = gj (x).
Marginalized Gradient. For locally connected network, the gradient gj has some nice structures.
From Eqn. 17 we knows that ∆wjk = Ex [fk (x)gj (x)] = Exk fk(xk)Ex-k|xk [gj(x)]. Define
x-k = x\xk as the input image x except for xk. Then we can define the marginalized gradient:
gj(xk) = Ex-k|xk [gj (x)]	(2)
as the marginalization (average) of x-k, while keep xk fixed. With this notation, we can write
∆wjk = Exk [fk (xk )gj (xk)].
On the other hand, the gradient which back-propagates to a node k can be written as
gk(x) = fk0 (x)	wjkgj(x) = fk0 (xk)	wjkgj(x)	(3)
j∈pa(k)	j
where fk0 is the derivative of activation function of node k (for ReLU it is just a gating function). If
we take expectation with respect to x-k|xk on both side, we get
gk(xk) = fk0 (xk)gkraw(xk) = fk0 (xk)	wjkgj(xk)	(4)
j∈pa(k)
Note that all marginalized gradients gj (xk) are independently computed by marginalizing with
respect to all regions that are outside the receptive field xk . Interestingly, there is a relationship
between these gradients that respects the locality structure:
Theorem 1 (Recursive Property of marginalized gradient). gj(xk) = Exj,-k|xk [gj (xj)]
This shows that there is a recursive structure in marginal gradient: we can first compute gj (xj) for
top node j, then by marginalizing over the region within xj but outside xk , we get its projection
gj (xk) on child k, then by Eqn. 20 we collect all projections from all the parents of node k, to get
gk(xk). This procedure can be repeated until we arrive at the leaf nodes.
3	The Framework and its Goal
Let’s first consider the following quantity. For each neural node j , we want to compute the expected
gradient given a particular factor zα, where α = rf(j) (the reception field of node j):
gj (Za) ≡ EXj ∣Za [gj (Xj)] = J gj (Xj )P(Xj Iza)dxj	(S)
And gj (za) = gj(za)P(za). Similarly, f (za) = EX^ [fj(Xj)] and fj(za) = EX^ fj(Xj)].
Note that P(Xj Iza) is the frequency count of Xj for za. If za captures all information of Xj, then
P(Xj Iza) is a delta function. Throughout the paper, we use frequentist interpretation of probabilities.
Goal. Intuitively, if we have gj (za = a) > 0 and gj(za 6= a) < 0, then the node j learns about the
hidden event za = a. For multi-class classification, the top level nodes (just below the softmax layer)
already embrace such correlations (here j is the class label): gj(y = j) > 0 and gj(y 6= j) < 0,
where we know zω = y is the top level factor. A natural question now arises:
Does gradient descent automatically push gj(za) to be correlated with the factor za?
4
Under review as a conference paper at ICLR 2019
	Dimension	Description
Fa, Ga, Da	ma-by-na	Activation fj(Za), gradient Oj(Za) and gating prob fj(Za) at group α.
~Wa	ne -by-na 一	Weight matrix that links group α and β
Pae	ma-by-m^	Prob P(Ze ∣Za) of events at group a and β
Table 1: Matrix Notation. See Eqn. 8 and Eqn. 59.
If this is true, then gradient descent on deep models is essentially a weak-supervised approach that
automatically learns the intermediate events at different levels. Giving a complete answer of this
question is very difficult and is beyond the scope of this paper. As a first step, we build a theoretical
framework that enables such analysis. We start with the relationship between neighboring layers:
Theorem 2 (Reformulation). For node j and k and their receptive field α and β. If the following
two conditions holds:
1)	Onsite Conditional independence. P(xj |zrf(j) , z...) = P(xj |zrf(j)).
2)	Decorrelation. Given z§, gkaw(∙) and fkaw(∙) are uncorrelated with fk (∙):
Eχk∣zβ [fkgkaw]	= Eχk∣zβ	[fk] Eχk∣zβ	[gkaw],	Eχk∣zβ	[fkfkaw]	= Eχk∣zβ	[fk] Eχk∣zβ	[fkaw]	(6)
Then the following iterative equations hold:
fj (Za)= fj (Za) ɪ2 Wjk Eze ∣Za [fk (Ze )]，	gk(Ze) = fk (Ze )	Wjk EzaIze [gj (Za)]	⑺
k∈ch(j)	j∈pa(k)
The reformulation becomes exact if Za contains all information of the region.
Theorem 3.	If P(xj |Za ) is a delta function for all α, then all conditions in Thm. 2 hold.
While Thm. 3 holds in the ideal (and maybe trivial) case, both assumptions are still practically
reasonable. For assumption (1), the main idea is that the image content xa is most related to the
summarization variable Za located at the same receptive field α, and less related to others. On the
other hand, assumptions (2) holds approximately if the summarization variable is fine-grained.
Intuitively, P(xj |Za) is a distribution encoding how much information gets lost if we only know the
factor Za . Climbing up the ladder, more and more information is lost while keeping the critical part
for the classification. This is consistent with empirical observations (Bau et al., 2017), in which the
low-level features in DCNN are generic, and high-level features are more class-specific.
One key property of this formulation is that, it relates conditional probabilities P(Za, Zβ), and
thus input data distribution P(x) into the gradient descent rules. This is important since running
backpropagation on different dataset is now formulated into the same framework with different
probability, i.e., frequency counts of events. By studying which family of distribution leads to the
desired property, we could understand backpropagation better.
Furthermore, the property of stochastic gradient descent (SGD) can be modeled as using an imperfect
estimate P(Za, Zβ) of the true probability P(Za, Zβ) when running backpropagation. This is because
each batch is a rough sample of the data distribution so the resulting P(Za, Zβ) will also be different.
This could also unite GD and SGD analysis.
For boundary conditions, in the lowest level L, we could treat each input pixel (or a group of pixels)
as a single event: fk(Zβ) = I [k = Zβ]. For top level, each node j corresponds to a class label j while
the summarization variable Za also take class labels: gj (Za) = a1I [j = Za] - a2I [j 6= Za].
If we group the nodes with the same reception field at the same level together (Fig. 2), we have the
matrix form of Eqn. 7 (◦ is element-wise multiplication):
Theorem 4	(Matrix Representation of Reformulation).
Fa = Da ◦ X Pae Fe Wβa,	G β =。6◦ X Pae GaWTm	∆Wβa = (Pae Fe )T Ga (8)
β∈ch(a)	a∈pa(β)
See Tbl. 3 for the notation. For this dynamics, We want Fω = Inω, i.e., the top nω neurons faithfully
represents the classification labels. Therefore, the top level gradient is Gω = Inω - Fω . On the
other side, for each region β at the bottom layer, we have Fe = Inβ, i.e., the input contains all the
preliminary factors. For all regions α in the top-most and bottom-most layers, we have na = ma .
5
Under review as a conference paper at ICLR 2019
w/o BN
with BN
可/1g7 9c°= 1回
∖	∖ 9c1 = f gbn
_______S subspace {f, 1}
(c) Projected Gradient
Figure 3: Batch Normalization (BN) as a projection. (a) Add BN by inserting a new node jbn. (b)
ForWard/backward pass in BN and relevant quantities. (c) The gradient g that is propagated down is
a projection of input gradient gbn onto the orthogonal complementary space spanned by {f, 1}.
(a) Network before/after adding BN (b) ForWardZbaCkWard in BN
4	Batch Normalization under Reformulation
Our reformulation naturally incorporates empirical regularization technique like Batch Normalization
(BN) (Ioffe & Szegedy, 2015).
4.1	Batch Normalization as A Projection
We start with a novel finding of Batch Norm: the back-propagated gradient through Batch Norm
layer at a node j is a projection onto the orthogonal complementary subspace spanned by all one
vectors and the current activations of node j.
Denote pre-batchnorm activations as f = [fj∙ (x1),... f (XN)] where N is the batchsize. In Batch
Norm, f is whitened to be f, then linearly transformed to yield the output fbn (note that we omit node
subscript j for clarity):
^ ~ ^ , :	一
f = f - μ1, f = f/σ, fbn = cιf + co	(9)
where μ = N f T1 and σ2 = N f Tf and c1 , c0 are learnable parameters.
The original Batch Norm paper (ioffe & Szegedy, 2015) derives complicated and unintuitive weight
update rules. With vector notation, the update has a compact form with a clear geometric meaning.
Theorem 5 (Backpropagation of Batch Norm). For a top-down pre-BN gradient gbn (a vector of
size N -by-1,N is the batchsize), the gradient after passing BN layer is the following:
g = JBN ⑴gbn = cσp⊥1 gbn,	gc ≡ Oi ,gc0 ]T = S⑴Tgbn	(10)
Here P⊥1 is the orthogonal complementary projection onto subspace {f, 1} and S(f) ≡ [f, 1].
intuitively, the back-propagated gradient g is zero-mean and perpendicular to the input activation f
of BN layer, as illustrated in Fig. 3. unlike (Kohler et al., 2018) that analyzes BN in an approximate
manner, in Thm. 5 we do not impose any assumptions.
4.2	Batch Norm under the reformulation
in our reformulation, we take the expectation of input x so there is no explicit notation of batch.
However, we could regard each sample in the batch as i.i.d. samples from the data distribution P(x).
Then the analysis of Batch Norm in Sec. 4.1 could be applied in the reformulation and yield similar
results, using the quantity that Ex [fj (x)] = Ezα [fj (zα )].
In this case, we have μ = Ezα [fj] and σ2 = Ezα [(fj(Za) - μ)2], and JBN(f) = c1Pf⊥1zα. Note
that the projection matrix Pf⊥,1,zα is under the new inner product hfj , gj izα = Ezα [fj (zα)gj (zα)] and
norm ∣∣f kzα =(f, f)；/2. In comparison, Sec. 4.1 is a special case with P(Za) = N PiNL1 δ(za —
φα(xi )), where x1, . . . , xN are the batch samples.
One consequence is that for G a, we have 1T G a = [Ezα [g1(za)],..., Eza [gnα (Za)]] = 0 since
gj (∙) is in the null space of 1 under the inner product (∙, ∙)za. This property will be used in Sec. 5.2.
6
Under review as a conference paper at ICLR 2019
1o!o:o:
L______」
1
ɑ
)
a
一oio!o!
L_____」
2
ɑ
o'o!oiΛ,
oio;δ⅛,
1o!o!δ 一⅛
L_____』
S 一/31/32
Λ 4分
小/32
1⅞⅞1[o1iiqi;o=⅛
皿2二=
硒,⅛∖一
©回一/
⅛
隹/32
α 1 a 2
S S
1
(C)
(d)
b
Q
P
ɑ 1
S

Figure 4: Disentangled representation. (a) Nodes are grouped according to regions. (b) An example
of one parent region α (2 nodes) and two child regions β1 and β2 (5 nodes each). We assume
factorization property of data distribution P. (C) disentangled activations, (d) Separable weights.
5	Example applications of proposed theoretical framework
With the help of the theoretical framework, we now can analyze interesting structures of gradient
descent in deep models, when the data distribution P(zα , zβ) satisfies specific conditions. Here we
give two concrete examples: the role played by nonlinearity and in which condition disentangled
representation can be achieved. Besides, from the theoretical framework, we also give general
comments on multiple issues (e.g., overfitting, GD versus sGD) in deep learning.
5.1	Nonlinear versus linear
in the formulation, mα is the number of possible events within a region α, which is often exponential
with respect to the size sz(α) of the region. The following analysis shows that a linear model cannot
handle it, even with exponential number of nodes nα, while a nonlinear one with ReLU can.
Definition 1 (Convex Hull of a set). We define the convex hull Conv(P) of m points P ⊂ Rn to be
Conv(P) = {Pa, a ∈ ∆n-1}, where ∆n-1 = {a ∈ Rn, ai ≥ 0, Pi ai = 1}. A row Pj is called
vertex if pj ∈/ Conv(P \pj ).
Definition 2. A matrix P of size m-by-n is called k-vert, or vert(P) = k ≤ m, if its k rows are
vertices of the convex hull generated by its rows. P is called all-vert if k = m.
Theorem 6 (Expressibility of ReLU Nonlinearity). Assuming mα = nα = O(exp(sz(α))), where
sz(α) is the size of receptive field of α. If each Pαβ is all-vert, then: (ω is top-level receptive field)
min LossReLU (W) = 0, min LossLinear (W) = O(exp(sz(ω)))	(11)
Note that here Loss(W) ≡ kFω - Ik2F. This shows the power of nonlinearity, which guarantees
full rank of output, even if the matrices involved in the multiplication are low-rank. The following
theorem shows that for intermediate layers whose input is not identity, the all-vert property remains.
Theorem 7. (1) If F is full row rank, then vert(P F) = vert(P). (2) PF is all-vert iff P is all-vert.
This means that if all Pαβ are all-vert and its input Fβ is full-rank, then with the same construction of
Thm. 6, Fα can be made identity. in particular, if we sample W randomly, then with probability 1,
all Fβ are full-rank, in particular the top-level input F1. Therefore, using top-level W1 alone would
be sufficient to yield zero generalization error, as shown in the previous works that random projection
could work well.
5.2	Disentangled Representation
The analysis in sec. 5.1 assumes that nα = mα, which means that we have sufficient nodes, one
neuron for one event, to convey the information forward to the classification level. in practice,
this is never the case. When nα mα = O(exp(sz(α))) and the network needs to represent the
information in a proper way so that it can be sent to the top level. ideally, if the factor zα can be
written down as a list of binary factors: zα = zα[1] , zα[2] , . . . , zα[j] , the output of a node j could
represent zα[j], so that all mα events can be represented concisely with nα nodes.
7
Under review as a conference paper at ICLR 2019
Symbols	Description
	Zaji		The j-th binary factor of region a. Za j] Can take 0 or 1.
	Pa [j]		2-by-1 marginal PrObabiIity vector Ofbinary factor Zaj].	
fa[j], ga[j], ga[j]	The j-th column of Fa, Ga and Ga COrreSPOnding to j-th binary factor Za j].
1 / 0	All-1 / All-0 vector. Its dimension depends on context.
Fi 0 F2	Out (or tensor) product of Fi and F2. 0K=1Fk is the product of multiple Fk.
{Siaβ }	Disjoint collection of na indices sets. Sae are the indices of downstream nodes in β to i-th binary factor in α (Fig. 4(b)).
Wβa[Saβ ,j]-	The j-th subcolumn of weight matrix Wβa, whose rows are selected by Sae.
Table 2: Symbols used in Sec. 5.2.
To come up with a complete theory for disentangled representation in deep nonlinear network is
far from trivial and beyond the scope of this paper. In the following, we make an initial attempt by
constructing factorizable Pαβ so that disentangled representation is possible in the forward pass. First
we need to formally define what is disentangled representation:
Definition 3. The activation Fa is disentangled,if its j-th column Fa,：j = 1 0 .. .0fɑ[j] 0... 0 L
where each fα[j] and 1 is a 2-by-1 vector.
Definition 4. The gradient Ga IS disentangled, if its j -th column Ga,：j = Pa[i] 0 ∙∙∙0gaj] 0∙∙∙ 0
Pa[na], where Pa[j] = [P(α[j] = 0),P(α[j] = 1)]T and ga[j] isa 2-by-1 vector
Intuitively, this means that each node j represents the binary factor za [j]. A follow-up question is
whether such disentangled properties carries over layers in the forward pass. It turns out that the
disentangled structure carries if the data distribution and weights have compatible structures:
Definition 5. The weights Wβa is separable with respect to a disjoint set {Siaβ}, if Wβa =
diag Wβa[S1aβ, 1], Wβa[S2aβ, 2],..., Wβa[Snaαβ, na].
Theorem 8 (Disentangled Forward). If for each β ∈ ch(α), Paβ can be written as a tensor product
Paβ = Ni Pa[i]β[Sαβ] where {Siaβ} are αβ -dependent disjointed set, Wβa is separable with respect
to {Siaβ}, Fβ is disentangled, then Fa is also disentangled (with/without ReLU /Batch Norm).
If the bottom activations are disentangled, by induction, all activations will be disentangled. The next
question is whether gradient descent preserves such a structure. The answer is also conditionally yes:
Theorem 9 (Separable Weight Update). If Pae = Ni Pa[i]β[Si], Fe and Ga are both disentangled,
1TGa = 0, then the gradient update ∆Wea is separable with respect to {Si }.
Therefore, with disentangled Fe and Ga and centered gradient ITGa = 0, the separable structure
is conserved over gradient descent, given the initial We(0a) is separable. Note that centered gradient
is guaranteed if we insert Batch Norm (Eqn. 83) after linear layers. And the activation F remains
disentangled if the weights are separable.
The hard part is whether Ge remains disentangled during backpropagation, if {Ga }a∈pa(e) are all
disentangled. If so, then the disentangled representation is self-sustainable under gradient descent.
This is a non-trivial problem and generally requires structures of data distribution. We put some
discussion in the Appendix and leave this topic for future work.
6	Explanation of common behaviors in Deep Learning
In the proposed formulation, the input x in Eqn. 7 is integrated out, and the data distribution is
now encoded into the probabilistic distribution P(za, ze), and their marginals. A change of such
distribution means the input distribution has changed. For the first time, we can now analyze many
practical factors and behaviors in the DL training that is traditionally not included in the formulation.
Over-fitting. Given finite number of training samples, there is always error in estimated factor-factor
distribution P(za, ze) and factor-observation distribution P(xa|za). In some cases, a slight change
of distribution would drastically change the optimal weights for prediction, which is overfitting.
8
Under review as a conference paper at ICLR 2019
Figure 5: Overfitting Example
Here is one example. Suppose there are two different kinds of events at two disjoint reception fields:
Za and zγ. The class label is zω, which equals Za but is not related to zγ. Therefore, we have:
~, . . ~, . .
P(Zω = 1|za = 1) = 1, P(Zω = 1|za = 0)=0	(12)
Although Zγ is unrelated to the class label Zω , with finite samples Zγ could show spurious correlation:
~ Z	.	.	.、	一	~ Z	..	一、	一	....
P(zω = 1 ∣Zγ = 1) = 0.5 + e,	P(zω = 1 ∣Zγ = 0) = 0.5 — e	(13)
On the other hand, as shown in Fig. 5, P(xa|Za) contains a lot of detailed structures and is almost
impossible to separate in the finite sample case, while P(χγ ∣Zγ) could be well separated for ZY = 0/1.
Therefore, for node j with rf(j) = α, fj(Za) ≈ constant (input almost indistinguishable):
∆wj = Ezα [fj(Za)g0(Za)] ≈ 0	(14)
where go(Za) = Ezω∣zα [go(Zω)] = { -1 Za = 0 , which is a strong gradient signal backpropa-
gated from the top softmax level, since Za is strongly correlated with Zω. For node k with rf(k) = γ,
an easy separation of the input (e.g., random initialization) yields distinctive fk (Zγ). Therefore,
∆wk = Ezγ [fj (Zγ)g0 (Zγ)] > 0
(15)
where go(Zγ) = Ezωj [go(Zω)]
Zγ = 1
Zγ = 0
a weak signal because of Zγ is (almost)
unrelated to the label. Therefore, we see that the weight wj that links to meaningful receptive field Za
does not receive strong gradient, while the weight wk that links to irrelevant (but spurious) receptive
field Zγ receives strong gradient. This will lead to overfitting.
With more data, over-fitting is alleviated since (1) P(Zω ∣Zγ) becomes more accurate and e → 0; (2)
P(xa|Za) starts to show statistical difference for Za = 0/1 and thus fj(Za) shows distinctiveness.
Note that there exists a second explanation: we could argue that Zγ is a true but weak factor that
contributes to the label, while Za is a fictitious discriminative factor, since the appearance difference
between Za = 0 and Za = 1 (i.e., P(χa∣Za) for ɑ = 0/1) could be purely due to noise and thus
should be neglected. With finite number of samples, these two cases are essentially indistinguishable.
Models with different induction bias might prefer one to the other, yielding drastically different
generalization error. For neural network, SGD prefers the second explanation but if under the pressure
of training, it may also explore the first one by pushing gradient down to distinguish subtle difference
in the input. This may explain why the same neural networks can fit random-labeled data, and
generalize well for real data (Zhang et al., 2016).
Gradient Descent: Stochastic or not? Previous works (Keskar et al., 2017) show that empirically
stochastic gradient decent (SGD) with small batch size tends to converge to “flat” minima and offers
better generalizable solution than those uses larger batches to compute the gradient.
From our framework, SGD update with small batch size is equivalent to using a perturbed/noisy
version of P(Za, Zβ) at each iteration. Such an approach naturally reduces aforementioned over-fitting
issues, which is due to hyper-sensitivity of data distribution and makes the final weight solution
invariant to changes in P(Za, Zβ), yielding a “flat” solution.
7	Conclusion and future work
In this paper, we propose a novel theoretical framework for deep (multi-layered) nonlinear network
with ReLU activation and local receptive fields. The framework utilizes the specific structure of
9
Under review as a conference paper at ICLR 2019
neural networks, and formulates input data distributions explicitly. Compared to modeling deep
models as non-convex problems, our framework reveals more structures of the network; compared
to recent works that also take data distribution into considerations, our theoretical framework can
model deep networks without imposing idealistic analytic distribution of data like Gaussian inputs or
independent activations. Besides, we also analyze regularization techniques like Batch Norm, depicts
its underlying geometrical intuition, and shows that BN is compatible with our framework.
Using this novel framework, we have made an initial attempt to analyze many important and practical
issues in deep models, and provides a novel perspective on overfitting, generalization, disentangled
representation, etc. We emphasize that in this work, we barely touch the surface of these core issues
in deep learning. As a future work, we aim to explore them in a deeper and more thorough manner,
by using the powerful theoretical framework proposed in this paper.
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentangling in deep represen-
tations. arXiv preprint arXiv:1706.01350, 2017.
Daniel J Amit, Hanoch Gutfreund, and Haim Sompolinsky. Spin-glass models of neural networks.
Physical Review A, 32(2):1007, 1985.
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection:
Quantifying interpretability of deep visual representations. CVPR, 2017.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. ICML, 2017.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192-204, 2015a.
Anna Choromanska, Yann LeCun, and Gerard Ben Arous. Open problem: The landscape of the loss
surfaces of multilayer networks. In Conference on Learning Theory, pp. 1756-1760, 2015b.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient
descent can take exponential time to escape saddle points. In Advances in Neural Information
Processing Systems, pp. 1067-1077, 2017.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent
learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. ICML, 2018.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network
optimization problems. arXiv preprint arXiv:1412.6544, 2014.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. ICLR, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. ICML, 2015.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR,
2017.
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas
Hofmann. Towards a theoretical understanding of batch normalization. arXiv preprint
arXiv:1805.10694, 2018.
10
Under review as a conference paper at ICLR 2019
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
Honal neural networks. In Advances in neural information processing Systems, PP∙ 1097-1105,
2012.
Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscaPe of neural nets.
arXiv preprint arXiv:1712.09913, 2017.
Pankaj Mehta and David J Schwab. An exact maPPing between the variational renormalization grouP
and deeP learning. arXiv preprint arXiv:1410.3831, 2014.
Song Mei, Yu Bai, and Andrea Montanari. The landscaPe of emPirical risk for non-convex losses.
arXiv preprint arXiv:1607.06534, 2016.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. ExPonential
exPressivity in deeP neural networks through transient chaos. In Advances in neural information
processing systems, PP. 3360-3368, 2016.
Itay Safran and Ohad Shamir. SPurious local minima are common in two-layer relu neural networks.
CoRR, abs/1712.08968, 2017. URL http://arxiv.org/abs/1712.08968.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deeP linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Andrew Michael Saxe, Yamini Bansal, Joel DaPello, Madhu Advani, Artemy Kolchinsky, Bren-
dan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deeP
learning. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=ry_WPG-A-.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deeP neural networks and tree search. nature, 529(7587):484, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354, 2017.
Karen Simonyan and Andrew Zisserman. Very deeP convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Weijie Su, StePhen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s
accelerated gradient method: Theory and insights. In Advances in Neural Information Processing
Systems, PP. 2510-2518, 2014.
Yuandong Tian. An analytical formula of PoPulation gradient for two-layered relu network and its
aPPlications in convergence and critical Point analysis. ICML, 2017.
Yuandong Tian and Yan Zhu. Better comPuter go Player with neural network and long-term Prediction.
ICLR, 2016.
Naftali Tishby and Noga Zaslavsky. DeeP learning and the information bottleneck PrinciPle. In
Information Theory Workshop (ITW), 2015 IEEE, PP. 1-5. IEEE, 2015.
Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension
have no sPurious valleys. arXiv preprint arXiv:1802.06384, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deeP learning requires rethinking generalization. ICLR, 2016.
11
Under review as a conference paper at ICLR 2019
8	Appendix
8.1	Problem Setting
We consider a neuron (or node) j . Denote fj as its activation after nonlinearity and gj as the (input)
gradient it receives after filtered by ReLU’s gating. Note that both fj and gj are deterministic
functions of the input x and label y. Since y is a deterministic function of x, we can write fj = fj (x)
and gj = gj (x). Note that all analysis still holds with bias terms. We omit them for brevity.
The activation fj and gradient gk can be written as (note that fj0 is the binary gating function):
fj (x) = fj0(x)	wjkfk(x), gk(x) = fk0 (x)	wjkgj (x)	(16)
k∈ch(j)	j∈pa(k)
And the weight update for gradient descent is:
∆wjk = Ex [fk (x)gj (x)]	(17)
Here is the expectation is with respect to a training dataset (or a batch), depending on whether GD or
SGD has been used. We also use fjraw and gjraw as the counterpart of fj and gj before nonlinearity.
8.2	Marginalized Gradient
Given the structure of locally connected network, the gradient gj has some nice structures. From
Eqn. 17 we knows that ∆wjk = Ex [fk (x)gj (x)] = Exk fk(xk)Ex-k|xk [gj (x)]. Define x-k =
x\xk as the input image x except for xk. Then we can define the marginalized gradient:
gj(xk) = Ex-k|xk [gj (x)]	(18)
as the marginalization (average) of x-k, while keep xk fixed. With this notation, we can write
∆wjk = Exk [fk (xk )gj (xk)].
On the other hand, the gradient which back-propagates to a node k can be written as
gk(x) = fk0 (x)	wjkgj(x) = fk0 (xk)	wjkgj(x)	(19)
j∈pa(k)	j
where fk0 is the derivative of activation function of node k (for ReLU it is just a gating function). If
we take expectation with respect to x-k|xk on both side, we get
gk(xk) = fk0 (xk)gkraw(xk) = fk0 (xk)	wjkgj(xk)	(20)
j∈pa(k)
Note that all marginalized gradients gj (xk) are independently computed by marginalizing with
respect to all regions that are outside the receptive field Xk. Interestingly, there is a relationship
between these gradients that respects the locality structure:
Figure 6: Notation used in Thm. 1.
Theorem 1 (Recursive Property of marginalized gradient).
gj (Xk) =Eχj,-k∣χk [gj (Xj)]	(21)
12
Under review as a conference paper at ICLR 2019
Proof. We have:
gj(xk) = Ex-k|xk [gj (x)]
= Ex-j,xj,-k|xk [gj(x)]
= Ex-j |xj,-k ,xk Exj,-k |xk [gj (x)]
= Ex-j |xj Exj,-k |xk [gj (x)]
= Exj,-k |xk Ex-j |xj [gj (x)]
= Exj,-k |xk [gj (xj)]
□
8.3 Network theorem
Theorem 2 (Reformulation). Denote α = rf(j) and β = rf (k). k is a child of j. If the following
two conditions hold:
•	FocusofknowIedge. P(Xk ∣Za,zβ) = P(Xk ∣zβ).
•	Broadness of knowledge. P(Xj ∣zɑ,zβ) = P(Xj ∣zɑ).
•	Decorrelation. Given zβ,(gkaw(∙) and fk(∙)) and (fkaw(∙) and fk(∙))are uncorrelated
Then the following two conditions holds:
fj(Za) = fj(zα) X WjkEze∣zα [fk(zβ)]	(22a)
k∈ch(j)
gk (Ze) = fk (Ze)〉： WjkEzaIze [gj (Za)]	(22b)
j∈pa(k)
Proof. For Eqn. 22a, we have:
fjraw(Za) =	fjraw(X)P(X∣Za)dX
= Z fjraw(Xj)P(Xj ∣Za)dXj
=	Wjk fk (Xk )	P(Xj ∣Za )dXj
k∈ch(j)
And for each of the entry, we have:
fk (Xk )P(Xj ∣Za )dXj =	fk (Xk )P(Xk ∣Za )dXk
For P(Xk∣Za), using focus of knowledge, we have:
P(Xk∣Za) =	P(Xk,Zβ∣Za)
ze
= X P(Xk ∣Zβ, Za)P(Zβ∣Za)
ze
= XP(Xk∣Zβ)P(Zβ∣Za)
ze
(23)
(24)
(25)
(26)
(27)
(28)
(29)
13
Under review as a conference paper at ICLR 2019
Therefore, following Eqn. 26, we have:
f fk (Xk)P(xk ∣Zα)dXk = / fk(Xk) X P(Xk ∣zβ)P(zβ ∣Zɑ)dXk zβ	(30)
=X( f fk (Xk)P(xk ∣zβ)dxk) P(zβ∣Zɑ)	(31)
=X fk (zβ)P(zβ∣Zɑ) z	(32)
zβ =Eze ∣Za [fk (Ze )]	(33)
Putting it back to Eqn. 25 and we have:	
fjraw (Za)= X WjkEzβ∣za [fk(zβ)] k∈ch(j)	(34)
For Eqn. 22b, similarly we have:	
gkaw(zβ) = / gkaw (X)P(XIZe )dx	(35)
=	E Wjkgj(X)P(XIZe)dX j∈pa(k)	(36)
Notice that we have:	
P(XIZe) = P(XjIZe)P(X-jIXj,Ze) = P(XjIZe)P(X-jIXj)	(37)
since Xj covers Xk which determines Ze . Therefore, for each item we have:	
gj (X)P(XIZe)dX =	gj (X)P(Xj IZe)P(X-j IXj)dX	(38)
=	gj(X)P(X-jIXj)dX-j P(XjIZe)dXj	(39)
=	gj (Xj)P(Xj IZe)dXj	(40)
Then we use the broadness of knowledge:	
P(Xj IZe) = XP(Xj,ZαIZe) zα	(41)
= XP(XjIZα,Ze)P(ZαIZe) zα	(42)
= X P(Xj IZα)P(Zα IZe)	(43)
zα
Following Eqn. 40, we now have:
gj (X)P(XIZe)dX	gj(Xj) X P(Xj IZα)P(Zα IZe)dXj zα	(44)
	X	gj(Xj)P(XjIZα)dXj P(ZαIZe) zα	(45)
	gj(Zα)P(ZαIZe)	(46)
	zα E EzaIze [gj (za)]	(47)
14
Under review as a conference paper at ICLR 2019
Putting it back to Eqn. 36 and we have:
gkaw(zβ) = X WjkEzaIze [gj (Za)]	(48)
j∈pa(k)
Using the definition of gk(zβ):
gk(zβ) = J gk(Xk)P(xk∣zβ)dxk	(49)
=/ fk (Xk )gkaw(xk )P(xk∣zβ )dxk	(50)
=EXkIze [fk(Xk)gkaw(Xk)]	(51)
The Un-Corre山tion between gkaw(∙) and fk(∙) means that
EXkIze [fkgkaw] = EXkIze fk] ∙ EXkIze [gkaw]	(52)
Similarly for fj(za).	□
8.4 Exactness of reformulation
The following theorem shows that the reformUlation is exact if za has all information of the region.
Theorem 3. If P(Xj |za) is a delta function for all α, then the conditions of Thm. 2 hold and the
reformulation becomes exact.
Proof. The fact that P(Xj |za) is a delta fUnction means that there exists a fUnction φj so that:
P(Xj|za) = δ(Xj - φj(za))	(53)
That is, za contains all information of Xj (or Xa). Therefore,
•	Broadness of knowledge. za contains strictly more information than zβ for β ∈ ch(α),
therefore P(Xj|za, zβ) = P(Xj|za).
•	Focus of knowledge. z§ captures all information of Zk, so P(Xk ∣Za, zβ) = P(Xk ∣zβ).
•	Decorrelation. For any h1(Xj) and h2(Xj) we have
EXj Izα [h1h2] =
h1 (Xj)h2 (Xj)P(Xj |Za)dXj
h1(Xj)h2(Xj)δ(Xj - φj(Za))dXj
h1(φj(Za))h2(φj(Za))
h1 (Xj)P(Xj |Za)dXj	h2 (Xj)P(Xj |Za)dXj
EXjIzα [h1] EXjIzα [h2]
(54)
(55)
(56)
(57)
(58)
□
8.5 Matrix form
Theorem 4 (Matrix Representation of Reformulation).
Fa = Da ◦ X Pae Fe Wβa, G β = De ◦ X Pae G aWβTa, ∆Wβa = (Pae Fe )T Ga (59)
β∈ch(a)	a∈pa(β)
15
Under review as a conference paper at ICLR 2019
	Dimension	Description
Fa, Da	ma-by-na	-AcSvaHonfj(ZoyandgaSEg^prob7jτza∏n^groupα
Ga, GGa	ma-by-na	Gradient gj (Za) and UnnormaIized gradient Gj (Za) in group α.
~Wa	ne -by-na	Weight matrix that links group β and α.
Pae, Pae	ma-by-me	Prob P(Ze∣Za), P(ZaIZe) of events between group β and α.
Λa		ma-by-ma~	Diagonal matrix encoding prior prob P(Za).
Table 3: Matrix Notation. See Eqn. 59.
Proof. We first consider one certain group α and β, which uses xα and xβ as the receptive field. For
this pair, we can write Eqn. 22 in the following matrix form:
Fαraw = Pαβ Fβ Wβα
Fα = Fαraw ◦ Dα
(60a)
(60b)
(60c)
(60d)
Using Λβ(Pbe)T = POgΛα and Gα = ΛαGα, We could simplify Eqn. 60 as follows:
Fα = Pαβ Fβ Wβα ◦ Dα
Ge = (Pae)T Gβ WTa ◦ De
(61a)
(61b)
Therefore, using the fact that j∈pa(k) = α∈pa(e) j∈α (where β = rf(k)) and k∈ch(j) =
Pe∈ch(α) Pk∈e (where α = rf(j)), and group all nodes that share the receptive field together, we
have:
Fα = Dα ◦	PαeFeWeα	(62a)
e∈ch(α)
Ge = De ◦ X PT GaWeb	(62b)
α∈pa(e)
For the gradient update rule, from Eqn. 17 notice that:
∆wjk = Ex [fk (x)gj (x)]	(63)
=	fk(x)gj (x)P(x)dx	(64)
=	fk(x)gj(x) XP(xIZa)P(Za)dx zα	(65)
= X	fk(x)gj(x)P(xIZa)P(Za)dx zα	(66)
We assume decorrelation so we have:
∆wjk = X Eχ∣Zα [fk (x)] gj (Za)P(Za)	(67)
zα
=X EXk ∣Zα [fk (Xk )] Gj (Za)	(68)
zα
16
Under review as a conference paper at ICLR 2019
For EXk ∣zα [fk (Xk)], again We Use focus of knowledge:
EXk ∣zα [fk(Xk)] = Jfk(Xk)P(Xk ∣Za)dXk	(69)
=	fk(xk)P(xk|Za,Ze)P(Ze|Za)dxk zβ	(70)
= X	fk(Xk)P(Xk|Ze)P(Ze|Za)dXk zβ	(71)
= X fk (Ze )P(Ze |Za ) zβ	(72)
Put them together and we have:
∆wjk = XX
fk (Ze)gj (Za)P(Ze |za) = Eza ,zβ [fk (Ze)gj (Za)]	(73)
zα zβ
Write it in concise matrix form and we get:
∆Wβa = (Pae Fe )T G a	(74)
□
8.6 Batch Norm as a projection
Theorem 5 (Backpropagation of Batch Norm). For a top-down gradient g, BN layer gives the
following gradient update (Pf⊥,1 is the orthogonal complementary projection of subspace {f, 1}):
g = JBN (f )gbn = cσPf⊥1 gbn,	gc = Sf)'gbn	(75)
Proof. We denote pre-batchnorm activations as f(i) = fj (xi) (i = 1 . . . N). In Batch Norm, f(i) is
whitened to be f(i), then linearly transformed to yield the output fb?:
N= f ⑶-μ,	f⑶=fi∕σ,	fbn) = ci f(i)+ co	(76)
where μ = N Pi f (i) and σ2 = N Pi(f (i) - μ)2 and ci, c0 are learnable parameters.
While in the original batch norm paper, the weight update rules are super complicated and unintuitive
(listed here for a reference):
⅛¾⅛⅛舞
Figure 7: Original BN rule from (Ioffe & Szegedy, 2015).
It turns out that with vector notation, the update equations have a compact vector form with clear
geometric meaning.
17
Under review as a conference paper at ICLR 2019
w/o BN
with BN
(a) Network before/after adding BN
(b) ForWard/backward in BN
subspace {f, 1}
(C) Projected Gradient
Figure 8: Batch Normalization (BN) as a projection. (a) Add BN by inserting a new node jbn. (b)
ForWard/backward pass in BN and relevant quantities. (c) The gradient g that is propagated down is
a projection of input gradient gbn onto the orthogonal complementary space spanned by {f, 1}.
To achieve that, we first write down the vector form of forward pass of batch normalization:
f = P⊥f, f = f/kf kuni, fbn = Clf + C01 = S (f)c	(77)
I	11t
where f, f, f and fbn are vectors of SiZe N, P⊥ ≡ I - ANr is the projection matrix that centers
the data, σ = ∣∣f ∣∣uni = √√N ∣∣f ∣∣2 and C ≡ [c1, c0]T are the parameters in Batch Normalization and
S(f) ≡ [f, 1] is the standardized data. Note that S(f )TS(f) = N ∙ I2 (I2 is 2-by-2 identity matrix)
and thus S(x) is an column-orthogonal N-by-2 matrix. If we put everything together, then we have:
C	P⊥ f
fbn = c1 P⊥f∣U + c01
(78)
Using this notation, we can compute the Jacobian of batch normalization layer. Specifically, for any
vector f, we have:
d (kf
df
ffT
IW V -IW
1⅛ Pf⊥
(79)
1
where PfI projects a vector into the orthogonal complementary space of f. Therefore we have:
JBN (f ) = dfbn	= C df	=	C df df	=	C1 pc
J (f )= df	= c1 df	=	c1 df	df	=	σPf ,1
(80)
where P⊥1 = I - S(f )N(f) is a symmetric projection matrix that projects the input gradient to the
orthogonal complement space spanned by X and 1 (Fig. 3(b)). Note that the space spanned by f and
1 is also the space spanned by f and 1, since f = (f — μ1)∕σ can be represented linearly by f and 1.
Therefore P I = PfI1 .
f,1 f,1
An interesting property is that since fbn returns a vector in the subspace of f and 1, for the N -by-N
Jacobian matrix of Batch Normalization, we have:
JBN (f)fbn = JBN (f)1 = JBN (f)f = 0	(81)
Following the backpropagation rule, we get the following gradient update for batch normalization. If
gbn = ∂L∕∂f is the gradient from top, then
gc = S(f)Tgbn,	g = JBN (f)gbn
(82)
Therefore, any gradient (vector of size N) that is back-propagated to the input of BN layer will be
automatically orthogonal to that activation (which is also a vector of size N).	□
8.6	. 1 Batch Normalization in the Reformulation
The analysis of Batch Norm is compatible with the reformulation and we arrive at similar backpropa-
gation rule, by noticing that Ex [fj (x)] = Ezα [fj (zα )]:
μ =	Eza[fj],	σ2	= Ezα[(fj	(Za)	-	μ)2]	, JBN(f) = ?P⊥1	(83)
Note that we still have the projection property, but under the new inner product hfj , gj izα =
EZa [fj (Za)gj (Za)] andnormkfkza = hf,fi1α2.
18
Under review as a conference paper at ICLR 2019
8.6.2 Conserved Quantity in Batch Normalization for ReLU activation
One can find an interesting quantity, by multiplying gj(x) on both side of the forward equation in
Eqn. 16 and taking expectation:
Ex [gj fj] = Ex	wjkfkgj =	wjk∆wjk
k∈ch(j)	k∈ch(j)
(84)
Using the language of differential equation, we know that:
Z tEx hgj(t0)fj(t0)i
dt0=Ej(t)-Ej(0)
(85)
where Ej = 2 Pk∈chj) wjk = 2IlWj∙ k2. If We place Batch Normalization Iayerjust after ReLU
activation and linear layer, by BN property, since Ex [gj fj] ≡ 0 for all iterations, the row energy
Ej (t) of weight matrix W of the linear layer is conserved over time. This might be part of the reason
why BN helps stabilize the training. Otherwise energy might “leak” from one layer to nearby layers.
8.7	Example applications of proposed theoretical framework
With the help of the theoretical framework, we now can analyze interesting structures of gradient
descent in deep models, when the data distribution P(zα , zβ ) satisfies specific conditions. Here we
give two concrete examples: the role played by nonlinearity and in which condition disentangled
representation can be achieved. Besides, from the theoretical framework, we also give general
comments on multiple issues (e.g., overfitting, GD versus SGD) in deep learning.
8.7.1	Nonlinear versus linear
In the formulation, mα is the number of possible events within a region α, which is often exponential
with respect to the size sz(α) of the region. The following analysis shows that a linear model cannot
handle it, even with exponential number of nodes nα, while a nonlinear one with ReLU can.
Definition 1 (Convex Hull of a Set). We define the convex hull Conv(P) of m points P ⊂ Rn to be
Conv(P) = {Pa, a ∈ ∆n-1}, where ∆n-1 = {a ∈ Rn,ai ≥ 0, Pi a% = 1}. A row Pjis called
vertex ifpj ∈/ Conv(P \pj).
Definition 2. A matrix P of size m-by-n is called k-vert, or vert(P) = k ≤ m, if its k rows are
vertices of the convex hull generated by its rows. P is called all-vert if k = m.
Theorem 6 (Expressibility of ReLU Nonlinearity). Assuming mα = nα = O(exp(sz(α))), where
sz(α) is the size of receptive field of α. If each Pαβ is all-vert, then: (ω is top-level receptive field)
min LossReLU (W) = 0,	min LossLinear (W) = O(exp(sz(ω)))	(86)
Here we define Loss(W ) ≡ IFω - II2F.
Proof. We prove that in the case of nonlinearity, there exists a weight so that the activation Fα = I
for all α. We prove by induction. The base case is trivial since we already know that Fα = I for all
leaf regions.
Suppose Fβ = I for any β ∈ ch(α). Since Pαβ is all-vert, every row is a vertex of the convex hull,
which means that for i-th row pi, there exists a weight Wi and b so that WTPi + b = 1/ ∣ch(α) | > 0
and wiT pj + bi < 0 for j 6= i. Put these weights and biases together into Wβα and we have
Fαraw =	Pαβ Fβ Wβα =	Pαβ Wβα
ββ
(87)
All diagonal elements of Fαraw are 1 while all off-diagonal elements are negative. Therefore, af-
ter ReLU, Fα = I . Applying induction, we get Fω = I and Gω = I - Fω = 0. Therefore,
LossReLU(W ) = IGω I2F = 0.
In the linear case, we know that rank(Fα) ≤ β rank(Pαβ Fβ Wβα) ≤ β rank(Fβ), which is on
the order of the size sz(α) of a's receptive field (Note that the constant relies on the overlap between
19
Under review as a conference paper at ICLR 2019
1o!o:o:
L______」
1
a
)
a
一oio!o!
L_____」
2
Q
o'o!oiΛ,
oio;δ⅛,
1o!o!δ 一⅛
L_____』
S 一/31/32
Λ 4分
小/32
1⅞⅞1[o1iiqi;o=⅛
皿2二=
硒,⅛∖一
©回一/
⅛
隹/32
a 1 a 2
S S
Ooo-I3"δl
1
(C)
(d)
b
a
万
ɑ 1
S

Figure 9: Disentangled representation. (a) Nodes are grouped according to regions. (b) An example
of one parent region α (2 nodes) and two child regions β1 and β2 (5 nodes each). We assume
factorization property of data distribution P. (C) disentangled activations, (d) Separable weights.
receptive fields). However, at the top-level, mω = nω = O(exp(sz(ω))), i.e., the information
contained in α is exponential with respect to the size of the receptive field. By Eckart-Young-Mirsky
theorem, we know that there is a lower bound for low-rank approximation. Therefore, the loss for
linear network Losslinear is at least on the order of m0, i.e., Losslinear = O(mω). Note that this also
works if we have BN layer in-between, since BN does a linear transform in the forward pass. □
This shows the power of nonlinearity, which guarantees full rank of output, even if the matrices
involved in the multiplication are low-rank. The following theorem shows that for intermediate layers
whose input is not identity, the all-vert property remains.
Theorem 7. (1) If F is full row rank, then vert(P F) = vert(P). (2) PF is all-vert iff P is all-vert.
Proof. For (1), note that each row of PF is piTF. If F is row full rank, then F has pseudo-inverse
F0 so that FF0 = I. Therefore, ifpi is not a vertex:
pi =	ajpj ,	aj = 1, aj ≥ 0,	(88)
j 6=i	j
then piT F is also not a vertex and vice versa. Therefore, vert(P F ) = vert(P). (2) follows from
(1).	□
This means that if all Pαβ are all-vert and its input Fβ is full-rank, then with the same construction of
Thm. 6, Fα can be made identity. In particular, if we sample W randomly, then with probability 1,
all Fβ are full-rank, in particular the top-level input F1. Therefore, using top-level W1 alone would
be sufficient to yield zero generalization error, as shown in the previous works that random projection
could work well.
8.7.2	Disentangled Representation
The analysis in the previous section assumes that nα = mα , which means that we have sufficient
nodes, one neuron for one event, to convey the information forward to the classification level. In
practice, this is never the case. When nα mα = O(exp(sz(α))) and the network needs to
represent the information in a proper way so that it can be sent to the top level. Ideally, if the factor
zα can be written down as a list of binary factors: zα = zα[1] , zα[2] , . . . , zα[j] , the output of a node
j could represent zα[j], so that all mα events can be represented concisely with nα nodes.
To come up with a complete theory for disentangled representation in deep nonlinear network is
far from trivial and beyond the scope of this paper. In the following, we make an initial attempt by
constructing factorizable Pαβ so that disentangled representation is possible in the forward pass. First
we need to formally define what is disentangled representation:
Definition 3. The activation Fa is disentangled,if its j -th column Fα,j = 1 0 ... 0 fαj] 0... 01,
where each fα[j] and 1 is a 2-by-1 vector.
Definition 4. The gradient G a is disentangled, if its j-th column G α,j∙ = pa[1] 0 ... 0 gaj] 0... 0
Pa[nα]，where Pa j] = [P(α[j] = 0), P(α[j] = 1)]T and ga[j] is a 2-by-1 vector.
20
Under review as a conference paper at ICLR 2019
Intuitively, this means that each node j represents the binary factor zα [j]. A follow-up question is
whether such disentangled properties carries over layers in the forward pass. It turns out that the
disentangled structure carries if the data distribution and weights have compatible structures:
Definition 5. The weights Wβα is separable with respect to a disjoint set {Siαβ}, if Wβα =
diag Wβα[S1αβ, 1], Wβα[S2αβ, 2],..., Wβα[Snααβ, nα].
If the bottom activations are disentangled, by induction, all activations should be disentangled. The
next question is whether gradient descent preserves such a structure. Here we provide a few theorems
to discuss such issues.
We first start with two lemmas. Both of them have simple proofs.
Lemma 1. Distribution representations have the following property:
(1)	If Fα(i) is disentangled, Fα = Pi wi Fα(i) is also disentangled.
(2)	If Fα is disentangled and h is any per-column element-wise function, then h(Fα) is disen-
tangled.
(3)	If Fα(i) are disentangled, hi are per-column element-wise function, then h1 (Fα(1)) ◦
h2(Fα(2)) . . . ◦ hn(Fα(n)) is disentangled.
Proof. (1) follows from properties of tensor product. For (2) and (3), note that the j-th column of Fα
is Fα,j = 10... fj ... 01, therefore hj (Fα,j) = 10... hj(fj)... 01, and h1(F01j) ◦无2(尸!2或)=
1 0 ...h1(f；I)) ◦ h2(f；2))... 0 1.	□
Given one child β ∈ ch(α), denote
PSj	=	Pɑ[j]β[Sj ] = [P(zβ[Sj ]lzɑ[j])]	(89)
wSj	=	Wβα[Sj,j]	(90)
pα[j]	=	[P(α[j] = 0), P(α[j] = 1)]T	(91)
We have PSj 1 = 1 and 1T pα[j] = 1. Note here for simplicity, 1 represents all-one vectors of any
length, determined by the context.
Since Fα and Gβ are disentangled, their j-th column can be written as:
Fe,：j =	1 0 ... 0 fj 0 ... 0 1	(92)
∕^r	∙~,	∙~, ~	∙~,	∙~,	∕cc∖
Ga,:j	= Pα[1]	0	. . . 0 gj 0	. . . 0 Pα[nα]	(93)
For simplicity, in the following proofs, we just show the case that nα = 2, nβ = 3, zα = zα[1] , zα[2]
and S = {S1 , S2 } = {{1, 2}, {3}}. We write f1,2 = [f1 0 1, 1 0 f2] as a 2-column matrix. The
general case is similar and we omit here for brevity.
Theorem 8 (Disentangled Forward). If for each β ∈ ch(α), Pαβ can be written as a tensor product
Pαβ = Ni Pα[i]β[Sαβ] where {Siαβ} are αβ -dependent disjointed set, Wβα is separable with respect
to {Siαβ}, Fβ is disentangled, then Fα is also disentangled (with/without ReLU /Batch Norm).
Proof. For a certain β ∈ ch(α), we first compute the quantity PαβFβ:
PαβFβ = (P1,2 0 P3) [f1,2 0 1, 1 0 f3] = [P1,2f1,2 0 1, 1 0 P3f3]	(94)
Therefore, the forward information sent from β to α is:
Fβra→wα	=	PαβFβWβα = [P1,2f1,2 0 1, 10P3f3]	w01,2	w03	(95)
=	[P1,2f1,2w1,2 0 1, 1 0 P3f3w3]	(96)
21
Under review as a conference paper at ICLR 2019
Note that both P1,2f1,2w1,2 and P3f3w3 are 2-by-1 vectors. Therefore, for each β ∈ ch(α), Fβra→wα
is disentangled. By Lemma 1, both Fαraw = Pβ∈ch(α) Fβra→wα and the nonlinear response Fα are
disentangled. By Eqn. 83, the forward pass of Batch Norm is a Per-Column element-wise function, so
BN also preserves disentangledness.	□
Theorem 9 (Separable Weight Update). If Pae = Ni Pɑ[i]β[Si], both Fe and Gɑ are disentangled,
1TGα = 0, then the gradient update ∆Weα is separable with respect to {Si}.
Proof. Following Eqn. 62 and Eqn. 94, we have:
∆Wβα =	(Pae Fe )T G a
=	(PI^(P3/	[g1 乳 Pa[2]，Pa[1]乳 g2]
=一(Pl,2fl,2)τgl	(Pl,2fl,2)τP叫]乳 1Tg2
一_ ITgi 氧(P3f3)TPa[2]	(P3f3)Tg2
Since 1T Ga = 0, we have for any j, 1TGα,j = 0 and thus 1Tgj = 0. Therefore,
∆Wβα = diag ((P1,2f1,2)Tg1, (P3f3)Tg2)
which is separable with respect to S. In particular:
∆wi,2 = (Pl,2fl,2)T g1, ∆w3 = (P3f3)τ g2
(97)
(98)
(99)
(100)
(101)
□
8.7.3 Discussion about backpropagation of disentangled gradient
One problem remains. If {Ga}a∈pa(e) are all disentangled, whether Ge is disentangled? We can try
computing the following quality:
G α→e	=	PT GaWTa
=(PT2 乳 PT) [g1 乳 Pa[2], Pa[1]乳 g21w0,2	WT
=[PT2g1 ㊈ Pβ[3], Pβ[1,2] ® P3τg2]	W0，2	:T
0	w3
=[PT2g1wf2 乳 Pe[3], Pβ[i,2]乳 P3 g2wT]
Note that here we use the following equality from total probability rule:
P3T Pa[2] = Pe[3] ,	PiT,2Pa[i] = Pe[i,2]
where Pe[i,2] is a 4-by-1 vector:
-P(Ze[i] = 0,ze[2] = 0)
P =	P(Ze[i] = 0,ze[2] = I)
Pe[1,2] —	P(Ze[i] = 1,ze[2] = O)
P(Ze[i] = 1, Ze[2] = 1)
(102)
(103)
(104)
(105)
(106)
(107)
Note that the ordering of these joint probability corresponds to the column order of Pi,2 .
Now with this example, we see that the backward case ( Eqn. 106) is very different from the forward
case (Eqn. 96), in which Ga→e is no longer disentangled. Indeed, PT2g1wT,2 is a 2-column matrix
and Pe[i,2] is not a rank-1 tensor anymore. Intuitively this makes sense, if two low-level attributes
have very similar behaviors, there is no way to distinguish the two via backpropagation.
Note that we also cannot assume independence: Pe[i,2] = Pe[i] 0 Pe[2] since the independence
property is in general not carried from layer to layer.
22
Under review as a conference paper at ICLR 2019
1 -	1	片 ra，" . 1	,1 Cll ♦ r∙
For general cases, Grαa→wβ takes the following form:
nα	nα
G~ raw _ DT ≈a,,rT ZO ∕C^∖ __	9	G DT ≈a,,rT	ZO ∕C^∖ __
α→β = PSaegιWSaeXpβ[sɑβ],	pβ[sfβ]XPSaeg2WSaeXQ^Pe[sɑβ],	∙∙∙
1	1 j=2 j	2	2	j=3	j
(108)
One hope here is that if we consider α∈pa(β) Grαa→wβ, the summation over parent α could lead to a
better structure, even for individual a, PTl gfwS is not 1-order tensor. For example, if Sae = Sj,
then for the first column in Si, due to 1tgj = 0, we know that:
X PTSI gαwT,Sι[1] = X cɑ(v+,Sι - v-,Sι)	(109)
α∈pa(β)	α∈pa(β)
where v+,Sι = P(Zβ[SιUzα[1] = 1) and v-,Sι = P(Ze[Sι]∣zα[1] = 0) and PaS =	v+,S1 .
, 1	, 1	vα,S1
If each α ∈ pa(β) is informative in a diverse way, and |S1 | is relatively small (e.g., 4), then
vα+,S - vα-,S 6= 0 and spans the probability space of dimension 2|S1 | - 1. Then we can always
find cα (or equivalently, weights) so that Eqn. 109 becomes rank-1 tensor (or disentangled). Besides,
the gating Dβ , which is disentangled as it is an element-wise function of Fβ, will also play a role in
regularizing Gβ.
We will leave this part to future work.
23